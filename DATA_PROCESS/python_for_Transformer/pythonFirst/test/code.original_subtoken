def get vid from url ( url ) : return match1 ( url , r'youtu\.be/([^?/]+)' ) or match1 ( url , r'youtube\.com/embed/([^/?]+)' ) or match1 ( url , r'youtube\.com/v/([^/?]+)' ) or match1 ( url , r'youtube\.com/watch/([^/?]+)' ) or parse query param ( url , 'v' ) or parse query param ( parse query param ( url , 'u' ) , 'v' )
def dailymotion download ( url , output dir = '.' , merge = True , info only = False , * * kwargs ) : html = get content ( rebuilt url ( url ) ) info = json . loads ( match1 ( html , r'qualities":({.+?}),"' ) ) title = match1 ( html , r'"video title"\s*:\s*"([^"]+)"' ) or match1 ( html , r'"title"\s*:\s*"([^"]+)"' ) title = unicodize ( title ) for quality in [ '1080' , '720' , '480' , '380' , '240' , '144' , 'auto' ] : try : real url = info [ quality ] [ 1 ] [ "url" ] if real url : break except Key Error : pass mime , ext , size = url info ( real url ) print info ( site info , title , mime , size ) if not info only : download urls ( [ real url ] , title , ext , size , output dir = output dir , merge = merge )
def sina download ( url , output dir = '.' , merge = True , info only = False , * * kwargs ) : if 'news.sina.com.cn/zxt' in url : sina zxt ( url , output dir = output dir , merge = merge , info only = info only , * * kwargs ) return vid = match1 ( url , r'vid=(\d+)' ) if vid is None : video page = get content ( url ) vid = hd vid = match1 ( video page , r'hd vid\s*:\s*\'([^\']+)\'' ) if hd vid == '0' : vids = match1 ( video page , r'[^\w]vid\s*:\s*\'([^\']+)\'' ) . split ( '|' ) vid = vids [ - 1 ] if vid is None : vid = match1 ( video page , r'vid:"?(\d+)"?' ) if vid : #title = match1(video page, r'title\s*:\s*\'([^\']+)\'') sina download by vid ( vid , output dir = output dir , merge = merge , info only = info only ) else : vkey = match1 ( video page , r'vkey\s*:\s*"([^"]+)"' ) if vkey is None : vid = match1 ( url , r'#(\d+)' ) sina download by vid ( vid , output dir = output dir , merge = merge , info only = info only ) return title = match1 ( video page , r'title\s*:\s*"([^"]+)"' ) sina download by vkey ( vkey , title = title , output dir = output dir , merge = merge , info only = info only )
def get vid from url ( self , url ) : hit = re . search ( r'live.qq.com/(\d+)' , url ) if hit is not None : return hit . group ( 1 ) hit = re . search ( r'live.qq.com/directory/match/(\d+)' , url ) if hit is not None : return self . get room id from url ( hit . group ( 1 ) ) html = get content ( url ) room id = match1 ( html , r'room id\":(\d+)' ) if room id is None : log . wtf ( 'Unknown page {}' . format ( url ) ) return room id
def sprint ( text , * colors ) : return "\33[{}m{content}\33[{}m" . format ( ";" . join ( [ str ( color ) for color in colors ] ) , RESET , content = text ) if IS ANSI TERMINAL and colors else text
def print log ( text , * colors ) : sys . stderr . write ( sprint ( "{}: {}" . format ( script name , text ) , * colors ) + "\n" )
def e ( message , exit code = None ) : print log ( message , YELLOW , BOLD ) if exit code is not None : sys . exit ( exit code )
def wtf ( message , exit code = 1 ) : print log ( message , RED , BOLD ) if exit code is not None : sys . exit ( exit code )
def detect os ( ) : # Inspired by: syst = system ( ) . lower ( ) os = 'unknown' if 'cygwin' in syst : os = 'cygwin' elif 'darwin' in syst : os = 'mac' elif 'linux' in syst : os = 'linux' try : with open ( '/proc/version' , 'r' ) as f : if 'microsoft' in f . read ( ) . lower ( ) : os = 'wsl' except : pass elif 'windows' in syst : os = 'windows' elif 'bsd' in syst : os = 'bsd' return os
def get video url from video id ( video id ) : # from js data = [ "" ] * 256 for index , in enumerate ( data ) : t = index for i in range ( 8 ) : t = - 306674912 ^ unsigned right shitf ( t , 1 ) if 1 & t else unsigned right shitf ( t , 1 ) data [ index ] = t def tmp ( ) : rand num = random . random ( ) path = "/video/urls/v/1/toutiao/mp4/{video id}?r={random num}" . format ( video id = video id , random num = str ( rand num ) [ 2 : ] ) e = o = r = - 1 i , a = 0 , len ( path ) while i < a : e = ord ( path [ i ] ) i += 1 if e < 128 : r = unsigned right shitf ( r , 8 ) ^ data [ 255 & ( r ^ e ) ] else : if e < 2048 : r = unsigned right shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 192 | e >> 6 & 31 ) ) ] r = unsigned right shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] else : if 55296 <= e < 57344 : e = ( 1023 & e ) + 64 i += 1 o = 1023 & t . url ( i ) r = unsigned right shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 240 | e >> 8 & 7 ) ) ] r = unsigned right shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 2 & 63 ) ) ] r = unsigned right shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | o >> 6 & 15 | ( 3 & e ) << 4 ) ) ] r = unsigned right shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & o ) ) ] else : r = unsigned right shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 224 | e >> 12 & 15 ) ) ] r = unsigned right shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 6 & 63 ) ) ] r = unsigned right shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] return "https://ib.365yg.com{path}&s={param}" . format ( path = path , param = unsigned right shitf ( r ^ - 1 , 0 ) ) while 1 : url = tmp ( ) if url . split ( "=" ) [ - 1 ] [ 0 ] != "-" : # 参数s不能为负数 return url
def get vid from url ( url ) : vid = match1 ( url , 'https?://www.mgtv.com/(?:b|l)/\d+/(\d+).html' ) if not vid : vid = match1 ( url , 'https?://www.mgtv.com/hz/bdpz/\d+/(\d+).html' ) return vid
def legitimize ( text , os = detect os ( ) ) : # POSIX systems text = text . translate ( { 0 : None , ord ( '/' ) : '-' , ord ( '|' ) : '-' , } ) # FIXME: do some filesystem detection if os == 'windows' or os == 'cygwin' or os == 'wsl' : # Windows (non-POSIX namespace) text = text . translate ( { # Reserved in Windows VFAT and NTFS ord ( ':' ) : '-' , ord ( '*' ) : '-' , ord ( '?' ) : '-' , ord ( '\\' ) : '-' , ord ( '\"' ) : '\'' , # Reserved in Windows VFAT ord ( '+' ) : '-' , ord ( '<' ) : '-' , ord ( '>' ) : '-' , ord ( '[' ) : '(' , ord ( ']' ) : ')' , ord ( '\t' ) : ' ' , } ) else : # *nix if os == 'mac' : # Mac OS HFS+ text = text . translate ( { ord ( ':' ) : '-' , } ) # Remove leading . if text . startswith ( "." ) : text = text [ 1 : ] text = text [ : 80 ] # Trim to 82 Unicode characters long return text
def cbs download ( url , output dir = '.' , merge = True , info only = False , * * kwargs ) : html = get content ( url ) pid = match1 ( html , r'video\.settings\.pid\s*=\s*\'([^\']+)\'' ) title = match1 ( html , r'video\.settings\.title\s*=\s*\"([^\"]+)\"' ) theplatform download by pid ( pid , title , output dir = output dir , merge = merge , info only = info only )
def parse host ( host ) : if re . match ( r'^(\d+)$' , host ) is not None : return ( "0.0.0.0" , int ( host ) ) if re . match ( r'^(\w+)://' , host ) is None : host = "//" + host o = parse . urlparse ( host ) hostname = o . hostname or "0.0.0.0" port = o . port or 0 return ( hostname , port )
def get conn ( self ) : conn config = self . get conn params ( ) conn = snowflake . connector . connect ( * * conn config ) return conn
def bulk load ( self , table , tmp file ) : self . copy expert ( "COPY {table} FROM STDIN" . format ( table = table ) , tmp file )
def bulk dump ( self , table , tmp file ) : self . copy expert ( "COPY {table} TO STDOUT" . format ( table = table ) , tmp file )
def execute ( self , context ) : hook = Google Cloud Storage Hook ( google cloud storage conn id = self . google cloud storage conn id , delegate to = self . delegate to ) hook . upload ( bucket name = self . bucket , object name = self . dst , mime type = self . mime type , filename = self . src , gzip = self . gzip , )
def get conn ( self ) : conn = self . get connection ( self . mysql conn id ) conn config = { "user" : conn . login , "passwd" : conn . password or '' , "host" : conn . host or 'localhost' , "db" : self . schema or conn . schema or '' } if not conn . port : conn config [ "port" ] = 3306 else : conn config [ "port" ] = int ( conn . port ) if conn . extra dejson . get ( 'charset' , False ) : conn config [ "charset" ] = conn . extra dejson [ "charset" ] if ( conn config [ "charset" ] ) . lower ( ) == 'utf8' or ( conn config [ "charset" ] ) . lower ( ) == 'utf-8' : conn config [ "use unicode" ] = True if conn . extra dejson . get ( 'cursor' , False ) : if ( conn . extra dejson [ "cursor" ] ) . lower ( ) == 'sscursor' : conn config [ "cursorclass" ] = My SQ Ldb . cursors . SS Cursor elif ( conn . extra dejson [ "cursor" ] ) . lower ( ) == 'dictcursor' : conn config [ "cursorclass" ] = My SQ Ldb . cursors . Dict Cursor elif ( conn . extra dejson [ "cursor" ] ) . lower ( ) == 'ssdictcursor' : conn config [ "cursorclass" ] = My SQ Ldb . cursors . SS Dict Cursor local infile = conn . extra dejson . get ( 'local infile' , False ) if conn . extra dejson . get ( 'ssl' , False ) : # SSL parameter for My SQL has to be a dictionary and in case # of extra/dejson we can get string if extra is passed via # URL parameters dejson ssl = conn . extra dejson [ 'ssl' ] if isinstance ( dejson ssl , six . string types ) : dejson ssl = json . loads ( dejson ssl ) conn config [ 'ssl' ] = dejson ssl if conn . extra dejson . get ( 'unix socket' ) : conn config [ 'unix socket' ] = conn . extra dejson [ 'unix socket' ] if local infile : conn config [ "local infile" ] = 1 conn = My SQ Ldb . connect ( * * conn config ) return conn
def bulk load ( self , table , tmp file ) : conn = self . get conn ( ) cur = conn . cursor ( ) cur . execute ( . format ( tmp file = tmp file , table = table ) ) conn . commit ( )
def get proxy version ( self ) : self . download sql proxy if needed ( ) command to run = [ self . sql proxy path ] command to run . extend ( [ '--version' ] ) command to run . extend ( self . get credential parameters ( ) ) result = subprocess . check output ( command to run ) . decode ( 'utf-8' ) pattern = re . compile ( "^.*[V|v]ersion ([^;]*);.*$" ) m = pattern . match ( result ) if m : return m . group ( 1 ) else : return None
def cleanup database hook ( self ) : if self . database type == 'postgres' : if hasattr ( self . db hook , 'conn' ) and self . db hook . conn and self . db hook . conn . notices : for output in self . db hook . conn . notices : self . log . info ( output )
def reserve free tcp port ( self ) : self . reserved tcp socket = socket . socket ( socket . AF INET , socket . SOCK STREAM ) self . reserved tcp socket . bind ( ( '127.0.0.1' , 0 ) ) self . sql proxy tcp port = self . reserved tcp socket . getsockname ( ) [ 1 ]
def get error code ( self , e ) : try : matches = self . error code pattern . match ( str ( e ) ) code = int ( matches . group ( 0 ) ) return code except Value Error : return e
def integrate plugins ( ) : import sys from airflow . plugins manager import sensors modules for sensors module in sensors modules : sys . modules [ sensors module . name ] = sensors module globals ( ) [ sensors module . name ] = sensors module
def clear dag runs ( ) : session = settings . Session ( ) drs = session . query ( Dag Run ) . filter ( Dag Run . dag id . in ( DAG IDS ) , ) . all ( ) for dr in drs : logging . info ( 'Deleting Dag Run :: {}' . format ( dr ) ) session . delete ( dr )
def clear dag task instances ( ) : session = settings . Session ( ) TI = Task Instance tis = ( session . query ( TI ) . filter ( TI . dag id . in ( DAG IDS ) ) . all ( ) ) for ti in tis : logging . info ( 'Deleting Task Instance :: {}' . format ( ti ) ) session . delete ( ti ) session . commit ( )
def set dags paused state ( is paused ) : session = settings . Session ( ) dms = session . query ( Dag Model ) . filter ( Dag Model . dag id . in ( DAG IDS ) ) for dm in dms : logging . info ( 'Setting DAG :: {} is paused={}' . format ( dm , is paused ) ) dm . is paused = is paused session . commit ( )
def print stats ( self ) : session = settings . Session ( ) TI = Task Instance tis = ( session . query ( TI ) . filter ( TI . dag id . in ( DAG IDS ) ) . all ( ) ) successful tis = [ x for x in tis if x . state == State . SUCCESS ] ti perf = [ ( ti . dag id , ti . task id , ti . execution date , ( ti . queued dttm - self . start date ) . total seconds ( ) , ( ti . start date - self . start date ) . total seconds ( ) , ( ti . end date - self . start date ) . total seconds ( ) , ti . duration ) for ti in successful tis ] ti perf df = pd . Data Frame ( ti perf , columns = [ 'dag id' , 'task id' , 'execution date' , 'queue delay' , 'start delay' , 'land time' , 'duration' ] ) print ( 'Performance Results' ) print ( '###################' ) for dag id in DAG IDS : print ( 'DAG {}' . format ( dag id ) ) print ( ti perf df [ ti perf df [ 'dag id' ] == dag id ] ) print ( '###################' ) if len ( tis ) > len ( successful tis ) : print ( "WARNING!! The following task instances haven't completed" ) print ( pd . Data Frame ( [ ( ti . dag id , ti . task id , ti . execution date , ti . state ) for ti in filter ( lambda x : x . state != State . SUCCESS , tis ) ] , columns = [ 'dag id' , 'task id' , 'execution date' , 'state' ] ) ) session . commit ( )
def heartbeat ( self ) : super ( Scheduler Metrics Job , self ) . heartbeat ( ) session = settings . Session ( ) # Get all the relevant task instances TI = Task Instance successful tis = ( session . query ( TI ) . filter ( TI . dag id . in ( DAG IDS ) ) . filter ( TI . state . in ( [ State . SUCCESS ] ) ) . all ( ) ) session . commit ( ) dagbag = Dag Bag ( SUBDIR ) dags = [ dagbag . dags [ dag id ] for dag id in DAG IDS ] # the tasks in perf dag 1 and per dag 2 have a daily schedule interval. num task instances = sum ( [ ( timezone . utcnow ( ) - task . start date ) . days for dag in dags for task in dag . tasks ] ) if ( len ( successful tis ) == num task instances or ( timezone . utcnow ( ) - self . start date ) . total seconds ( ) > MAX RUNTIME SECS ) : if len ( successful tis ) == num task instances : self . log . info ( "All tasks processed! Printing stats." ) else : self . log . info ( "Test timeout reached. Printing available stats." ) self . print stats ( ) set dags paused state ( True ) sys . exit ( )
def get dag run state ( dag id , execution date ) : dagbag = Dag Bag ( ) # Check DAG exists. if dag id not in dagbag . dags : error message = "Dag id {} not found" . format ( dag id ) raise Dag Not Found ( error message ) # Get DAG object and check Task Exists dag = dagbag . get dag ( dag id ) # Get Dag Run object and check that it exists dagrun = dag . get dagrun ( execution date = execution date ) if not dagrun : error message = ( 'Dag Run for date {} not found in dag {}' . format ( execution date , dag id ) ) raise Dag Run Not Found ( error message ) return { 'state' : dagrun . get state ( ) }
def get conn ( self ) : conn = self . get connection ( self . druid broker conn id ) druid broker conn = connect ( host = conn . host , port = conn . port , path = conn . extra dejson . get ( 'endpoint' , '/druid/v2/sql' ) , scheme = conn . extra dejson . get ( 'schema' , 'http' ) ) self . log . info ( 'Get the connection to druid broker on %s' , conn . host ) return druid broker conn
def create session ( ) : session = settings . Session ( ) try : yield session session . commit ( ) except Exception : session . rollback ( ) raise finally : session . close ( )
def resetdb ( ) : from airflow import models # alembic adds significant import time, so we import it lazily from alembic . migration import Migration Context log . info ( "Dropping tables that exist" ) models . base . Base . metadata . drop all ( settings . engine ) mc = Migration Context . configure ( settings . engine ) if mc . version . exists ( settings . engine ) : mc . version . drop ( settings . engine ) from flask appbuilder . models . sqla import Base Base . metadata . drop all ( settings . engine ) initdb ( )
def execute ( self , context ) : hook = Wasb Hook ( wasb conn id = self . wasb conn id ) self . log . info ( 'Uploading %s to wasb://%s ' 'as %s' . format ( self . file path , self . container name , self . blob name ) ) hook . load file ( self . file path , self . container name , self . blob name , * * self . load options )
def get conn ( self ) : db = self . get connection ( self . presto conn id ) reqkwargs = None if db . password is not None : reqkwargs = { 'auth' : HTTP Basic Auth ( db . login , db . password ) } return presto . connect ( host = db . host , port = db . port , username = db . login , source = db . extra dejson . get ( 'source' , 'airflow' ) , protocol = db . extra dejson . get ( 'protocol' , 'http' ) , catalog = db . extra dejson . get ( 'catalog' , 'hive' ) , requests kwargs = reqkwargs , schema = db . schema )
def get pretty exception message ( e ) : if ( hasattr ( e , 'message' ) and 'error Name' in e . message and 'message' in e . message ) : return ( '{name}: {message}' . format ( name = e . message [ 'error Name' ] , message = e . message [ 'message' ] ) ) else : return str ( e )
def get records ( self , hql , parameters = None ) : try : return super ( ) . get records ( self . strip sql ( hql ) , parameters ) except Database Error as e : raise Presto Exception ( self . get pretty exception message ( e ) )
def get pandas df ( self , hql , parameters = None ) : import pandas cursor = self . get cursor ( ) try : cursor . execute ( self . strip sql ( hql ) , parameters ) data = cursor . fetchall ( ) except Database Error as e : raise Presto Exception ( self . get pretty exception message ( e ) ) column descriptions = cursor . description if data : df = pandas . Data Frame ( data ) df . columns = [ c [ 0 ] for c in column descriptions ] else : df = pandas . Data Frame ( ) return df
def run ( self , hql , parameters = None ) : return super ( ) . run ( self . strip sql ( hql ) , parameters )
def get conn ( self ) : if self . cosmos client is not None : return self . cosmos client # Initialize the Python Azure Cosmos DB client self . cosmos client = cosmos client . Cosmos Client ( self . endpoint uri , { 'master Key' : self . master key } ) return self . cosmos client
def does collection exist ( self , collection name , database name = None ) : if collection name is None : raise Airflow Bad Request ( "Collection name cannot be None." ) existing container = list ( self . get conn ( ) . Query Containers ( get database link ( self . get database name ( database name ) ) , { "query" : "SELECT * FROM r WHERE r.id=@id" , "parameters" : [ { "name" : "@id" , "value" : collection name } ] } ) ) if len ( existing container ) == 0 : return False return True
def create collection ( self , collection name , database name = None ) : if collection name is None : raise Airflow Bad Request ( "Collection name cannot be None." ) # We need to check to see if this container already exists so we don't try # to create it twice existing container = list ( self . get conn ( ) . Query Containers ( get database link ( self . get database name ( database name ) ) , { "query" : "SELECT * FROM r WHERE r.id=@id" , "parameters" : [ { "name" : "@id" , "value" : collection name } ] } ) ) # Only create if we did not find it already existing if len ( existing container ) == 0 : self . get conn ( ) . Create Container ( get database link ( self . get database name ( database name ) ) , { "id" : collection name } )
def does database exist ( self , database name ) : if database name is None : raise Airflow Bad Request ( "Database name cannot be None." ) existing database = list ( self . get conn ( ) . Query Databases ( { "query" : "SELECT * FROM r WHERE r.id=@id" , "parameters" : [ { "name" : "@id" , "value" : database name } ] } ) ) if len ( existing database ) == 0 : return False return True
def create database ( self , database name ) : if database name is None : raise Airflow Bad Request ( "Database name cannot be None." ) # We need to check to see if this database already exists so we don't try # to create it twice existing database = list ( self . get conn ( ) . Query Databases ( { "query" : "SELECT * FROM r WHERE r.id=@id" , "parameters" : [ { "name" : "@id" , "value" : database name } ] } ) ) # Only create if we did not find it already existing if len ( existing database ) == 0 : self . get conn ( ) . Create Database ( { "id" : database name } )
def delete database ( self , database name ) : if database name is None : raise Airflow Bad Request ( "Database name cannot be None." ) self . get conn ( ) . Delete Database ( get database link ( database name ) )
def delete collection ( self , collection name , database name = None ) : if collection name is None : raise Airflow Bad Request ( "Collection name cannot be None." ) self . get conn ( ) . Delete Container ( get collection link ( self . get database name ( database name ) , collection name ) )
def insert documents ( self , documents , database name = None , collection name = None ) : if documents is None : raise Airflow Bad Request ( "You cannot insert empty documents" ) created documents = [ ] for single document in documents : created documents . append ( self . get conn ( ) . Create Item ( get collection link ( self . get database name ( database name ) , self . get collection name ( collection name ) ) , single document ) ) return created documents
def delete document ( self , document id , database name = None , collection name = None ) : if document id is None : raise Airflow Bad Request ( "Cannot delete a document without an id" ) self . get conn ( ) . Delete Item ( get document link ( self . get database name ( database name ) , self . get collection name ( collection name ) , document id ) )
def get document ( self , document id , database name = None , collection name = None ) : if document id is None : raise Airflow Bad Request ( "Cannot get a document without an id" ) try : return self . get conn ( ) . Read Item ( get document link ( self . get database name ( database name ) , self . get collection name ( collection name ) , document id ) ) except HTTP Failure : return None
def get documents ( self , sql string , database name = None , collection name = None , partition key = None ) : if sql string is None : raise Airflow Bad Request ( "SQL query string cannot be None" ) # Query them in SQL query = { 'query' : sql string } try : result iterable = self . get conn ( ) . Query Items ( get collection link ( self . get database name ( database name ) , self . get collection name ( collection name ) ) , query , partition key ) return list ( result iterable ) except HTTP Failure : return None
def get code ( dag id ) : session = settings . Session ( ) DM = models . Dag Model dag = session . query ( DM ) . filter ( DM . dag id == dag id ) . first ( ) session . close ( ) # Check DAG exists. if dag is None : error message = "Dag id {} not found" . format ( dag id ) raise Dag Not Found ( error message ) try : with wwwutils . open maybe zipped ( dag . fileloc , 'r' ) as f : code = f . read ( ) return code except IO Error as e : error message = "Error {} while reading Dag id {} Code" . format ( str ( e ) , dag id ) raise Airflow Exception ( error message )
def get conn ( self ) : conn = self . get connection ( self . vertica conn id ) conn config = { "user" : conn . login , "password" : conn . password or '' , "database" : conn . schema , "host" : conn . host or 'localhost' } if not conn . port : conn config [ "port" ] = 5433 else : conn config [ "port" ] = int ( conn . port ) conn = connect ( * * conn config ) return conn
def flush ( self ) : if len ( self . buffer ) > 0 : self . logger . log ( self . level , self . buffer ) self . buffer = str ( )
def start ( self ) : self . process = self . launch process ( self . dag directory , self . file paths , self . max runs , self . processor factory , self . child signal conn , self . stat queue , self . result queue , self . async mode ) self . log . info ( "Launched Dag File Processor Manager with pid: %s" , self . process . pid )
def exit gracefully ( self , signum , frame ) : self . log . info ( "Exiting gracefully upon receiving signal %s" , signum ) self . terminate ( ) self . end ( ) self . log . debug ( "Finished terminating DAG processors." ) sys . exit ( os . EX OK )
def start in async ( self ) : while True : loop start time = time . time ( ) if self . signal conn . poll ( ) : agent signal = self . signal conn . recv ( ) if agent signal == Dag Parsing Signal . TERMINATE MANAGER : self . terminate ( ) break elif agent signal == Dag Parsing Signal . END MANAGER : self . end ( ) sys . exit ( os . EX OK ) self . refresh dag dir ( ) simple dags = self . heartbeat ( ) for simple dag in simple dags : self . result queue . put ( simple dag ) self . print stat ( ) all files processed = all ( self . get last finish time ( x ) is not None for x in self . file paths ) max runs reached = self . max runs reached ( ) dag parsing stat = Dag Parsing Stat ( self . file paths , self . get all pids ( ) , max runs reached , all files processed , len ( simple dags ) ) self . stat queue . put ( dag parsing stat ) if max runs reached : self . log . info ( "Exiting dag parsing loop as all files " "have been processed %s times" , self . max runs ) break loop duration = time . time ( ) - loop start time if loop duration < 1 : sleep length = 1 - loop duration self . log . debug ( "Sleeping for %.2f seconds to prevent excessive logging" , sleep length ) time . sleep ( sleep length )
def refresh dag dir ( self ) : elapsed time since refresh = ( timezone . utcnow ( ) - self . last dag dir refresh time ) . total seconds ( ) if elapsed time since refresh > self . dag dir list interval : # Build up a list of Python files that could contain DA Gs self . log . info ( "Searching for files in %s" , self . dag directory ) self . file paths = list py file paths ( self . dag directory ) self . last dag dir refresh time = timezone . utcnow ( ) self . log . info ( "There are %s files in %s" , len ( self . file paths ) , self . dag directory ) self . set file paths ( self . file paths ) try : self . log . debug ( "Removing old import errors" ) self . clear nonexistent import errors ( ) except Exception : self . log . exception ( "Error removing old import errors" )
def print stat ( self ) : if ( ( timezone . utcnow ( ) - self . last stat print time ) . total seconds ( ) > self . print stats interval ) : if len ( self . file paths ) > 0 : self . log file processing stats ( self . file paths ) self . last stat print time = timezone . utcnow ( )
def wait until finished ( self ) : for file path , processor in self . processors . items ( ) : while not processor . done : time . sleep ( 0.1 )
def open slots ( self , session ) : from airflow . models . taskinstance import Task Instance as TI # Avoid circular import used slots = session . query ( func . count ( ) ) . filter ( TI . pool == self . pool ) . filter ( TI . state . in ( [ State . RUNNING , State . QUEUED ] ) ) . scalar ( ) return self . slots - used slots
def run command ( command ) : process = subprocess . Popen ( shlex . split ( command ) , stdout = subprocess . PIPE , stderr = subprocess . PIPE , close fds = True ) output , stderr = [ stream . decode ( sys . getdefaultencoding ( ) , 'ignore' ) for stream in process . communicate ( ) ] if process . returncode != 0 : raise Airflow Config Exception ( "Cannot execute {}. Error code is: {}. Output: {}, Stderr: {}" . format ( command , process . returncode , output , stderr ) ) return output
def get task ( dag id , task id ) : dagbag = Dag Bag ( ) # Check DAG exists. if dag id not in dagbag . dags : error message = "Dag id {} not found" . format ( dag id ) raise Dag Not Found ( error message ) # Get DAG object and check Task Exists dag = dagbag . get dag ( dag id ) if not dag . has task ( task id ) : error message = 'Task {} not found in dag {}' . format ( task id , dag id ) raise Task Not Found ( error message ) # Return the task. return dag . get task ( task id )
def dispose orm ( ) : log . debug ( "Disposing DB connection pool (PID %s)" , os . getpid ( ) ) global engine global Session if Session : Session . remove ( ) Session = None if engine : engine . dispose ( ) engine = None
def prepare classpath ( ) : if DAGS FOLDER not in sys . path : sys . path . append ( DAGS FOLDER ) # Add ./config/ for loading custom log parsers etc, or # airflow local settings etc. config path = os . path . join ( AIRFLOW HOME , 'config' ) if config path not in sys . path : sys . path . append ( config path ) if PLUGINS FOLDER not in sys . path : sys . path . append ( PLUGINS FOLDER )
def alchemy to dict ( obj ) : if not obj : return None d = { } for c in obj . table . columns : value = getattr ( obj , c . name ) if type ( value ) == datetime : value = value . isoformat ( ) d [ c . name ] = value return d
def chunks ( items , chunk size ) : if chunk size <= 0 : raise Value Error ( 'Chunk size must be a positive integer' ) for i in range ( 0 , len ( items ) , chunk size ) : yield items [ i : i + chunk size ]
def get task instance ( dag id , task id , execution date ) : dagbag = Dag Bag ( ) # Check DAG exists. if dag id not in dagbag . dags : error message = "Dag id {} not found" . format ( dag id ) raise Dag Not Found ( error message ) # Get DAG object and check Task Exists dag = dagbag . get dag ( dag id ) if not dag . has task ( task id ) : error message = 'Task {} not found in dag {}' . format ( task id , dag id ) raise Task Not Found ( error message ) # Get Dag Run object and check that it exists dagrun = dag . get dagrun ( execution date = execution date ) if not dagrun : error message = ( 'Dag Run for date {} not found in dag {}' . format ( execution date , dag id ) ) raise Dag Run Not Found ( error message ) # Get task instance object and check that it exists task instance = dagrun . get task instance ( task id ) if not task instance : error message = ( 'Task {} instance for date {} not found' . format ( task id , execution date ) ) raise Task Instance Not Found ( error message ) return task instance
def integrate plugins ( ) : import sys from airflow . plugins manager import operators modules for operators module in operators modules : sys . modules [ operators module . name ] = operators module globals ( ) [ operators module . name ] = operators module
def get conn ( self ) : http authorized = self . authorize ( ) return build ( 'dataproc' , self . api version , http = http authorized , cache discovery = False )
def wait ( self , operation ) : submitted = Data Proc Operation ( self . get conn ( ) , operation , self . num retries ) submitted . wait for done ( )
def get conn ( self ) : authed http = self . authorize ( ) return build ( 'ml' , 'v1' , http = authed http , cache discovery = False )
def set default version ( self , project id , model name , version name ) : full version name = 'projects/{}/models/{}/versions/{}' . format ( project id , model name , version name ) request = self . mlengine . projects ( ) . models ( ) . versions ( ) . set Default ( name = full version name , body = { } ) try : response = request . execute ( ) self . log . info ( 'Successfully set version: %s to default' , response ) return response except Http Error as e : self . log . error ( 'Something went wrong: %s' , e ) raise
def list versions ( self , project id , model name ) : result = [ ] full parent name = 'projects/{}/models/{}' . format ( project id , model name ) request = self . mlengine . projects ( ) . models ( ) . versions ( ) . list ( parent = full parent name , page Size = 100 ) response = request . execute ( ) next page token = response . get ( 'next Page Token' , None ) result . extend ( response . get ( 'versions' , [ ] ) ) while next page token is not None : next request = self . mlengine . projects ( ) . models ( ) . versions ( ) . list ( parent = full parent name , page Token = next page token , page Size = 100 ) response = next request . execute ( ) next page token = response . get ( 'next Page Token' , None ) result . extend ( response . get ( 'versions' , [ ] ) ) time . sleep ( 5 ) return result
def delete version ( self , project id , model name , version name ) : full name = 'projects/{}/models/{}/versions/{}' . format ( project id , model name , version name ) delete request = self . mlengine . projects ( ) . models ( ) . versions ( ) . delete ( name = full name ) response = delete request . execute ( ) get request = self . mlengine . projects ( ) . operations ( ) . get ( name = response [ 'name' ] ) return poll with exponential delay ( request = get request , max n = 9 , is done func = lambda resp : resp . get ( 'done' , False ) , is error func = lambda resp : resp . get ( 'error' , None ) is not None )
def create model ( self , project id , model ) : if not model [ 'name' ] : raise Value Error ( "Model name must be provided and " "could not be an empty string" ) project = 'projects/{}' . format ( project id ) request = self . mlengine . projects ( ) . models ( ) . create ( parent = project , body = model ) return request . execute ( )
def get model ( self , project id , model name ) : if not model name : raise Value Error ( "Model name must be provided and " "it could not be an empty string" ) full model name = 'projects/{}/models/{}' . format ( project id , model name ) request = self . mlengine . projects ( ) . models ( ) . get ( name = full model name ) try : return request . execute ( ) except Http Error as e : if e . resp . status == 404 : self . log . error ( 'Model was not found: %s' , e ) return None raise
def write batch data ( self , items ) : dynamodb conn = self . get conn ( ) try : table = dynamodb conn . Table ( self . table name ) with table . batch writer ( overwrite by pkeys = self . table keys ) as batch : for item in items : batch . put item ( Item = item ) return True except Exception as general error : raise Airflow Exception ( 'Failed to insert items in dynamodb, error: {error}' . format ( error = str ( general error ) ) )
def integrate plugins ( ) : from airflow . plugins manager import executors modules for executors module in executors modules : sys . modules [ executors module . name ] = executors module globals ( ) [ executors module . name ] = executors module
def get default executor ( ) : global DEFAULT EXECUTOR if DEFAULT EXECUTOR is not None : return DEFAULT EXECUTOR executor name = configuration . conf . get ( 'core' , 'EXECUTOR' ) DEFAULT EXECUTOR = get executor ( executor name ) log = Logging Mixin ( ) . log log . info ( "Using executor %s" , executor name ) return DEFAULT EXECUTOR
def on error ( self , error , items ) : self . log . error ( 'Encountered Segment error: {segment error} with ' 'items: {with items}' . format ( segment error = error , with items = items ) ) raise Airflow Exception ( 'Segment error: {}' . format ( error ) )
def get conn ( self ) : conn = self . get connection ( self . mssql conn id ) conn = pymssql . connect ( server = conn . host , user = conn . login , password = conn . password , database = self . schema or conn . schema , port = conn . port ) return conn
def execute ( self , context ) : self . hook = Spark Submit Hook ( conf = self . conf , conn id = self . conn id , files = self . files , py files = self . py files , archives = self . archives , driver class path = self . driver class path , jars = self . jars , java class = self . java class , packages = self . packages , exclude packages = self . exclude packages , repositories = self . repositories , total executor cores = self . total executor cores , executor cores = self . executor cores , executor memory = self . executor memory , driver memory = self . driver memory , keytab = self . keytab , principal = self . principal , name = self . name , num executors = self . num executors , application args = self . application args , env vars = self . env vars , verbose = self . verbose , spark binary = self . spark binary ) self . hook . submit ( self . application )
def delete dag ( dag id ) : try : count = delete . delete dag ( dag id ) except Airflow Exception as err : log . error ( err ) response = jsonify ( error = "{}" . format ( err ) ) response . status code = err . status code return response return jsonify ( message = "Removed {} record(s)" . format ( count ) , count = count )
def get dag code ( dag id ) : try : return get code ( dag id ) except Airflow Exception as err : log . info ( err ) response = jsonify ( error = "{}" . format ( err ) ) response . status code = err . status code return response
def task info ( dag id , task id ) : try : info = get task ( dag id , task id ) except Airflow Exception as err : log . info ( err ) response = jsonify ( error = "{}" . format ( err ) ) response . status code = err . status code return response # JSO Nify and return. fields = { k : str ( v ) for k , v in vars ( info ) . items ( ) if not k . startswith ( ' ' ) } return jsonify ( fields )
def get pools ( ) : try : pools = pool api . get pools ( ) except Airflow Exception as err : log . error ( err ) response = jsonify ( error = "{}" . format ( err ) ) response . status code = err . status code return response else : return jsonify ( [ p . to json ( ) for p in pools ] )
def create pool ( ) : params = request . get json ( force = True ) try : pool = pool api . create pool ( * * params ) except Airflow Exception as err : log . error ( err ) response = jsonify ( error = "{}" . format ( err ) ) response . status code = err . status code return response else : return jsonify ( pool . to json ( ) )
def get task instances ( self , state = None , session = None ) : from airflow . models . taskinstance import Task Instance # Avoid circular import tis = session . query ( Task Instance ) . filter ( Task Instance . dag id == self . dag id , Task Instance . execution date == self . execution date , ) if state : if isinstance ( state , six . string types ) : tis = tis . filter ( Task Instance . state == state ) else : # this is required to deal with NULL values if None in state : tis = tis . filter ( or ( Task Instance . state . in ( state ) , Task Instance . state . is ( None ) ) ) else : tis = tis . filter ( Task Instance . state . in ( state ) ) if self . dag and self . dag . partial : tis = tis . filter ( Task Instance . task id . in ( self . dag . task ids ) ) return tis . all ( )
def get previous dagrun ( self , session = None ) : return session . query ( Dag Run ) . filter ( Dag Run . dag id == self . dag id , Dag Run . execution date < self . execution date ) . order by ( Dag Run . execution date . desc ( ) ) . first ( )
def get previous scheduled dagrun ( self , session = None ) : dag = self . get dag ( ) return session . query ( Dag Run ) . filter ( Dag Run . dag id == self . dag id , Dag Run . execution date == dag . previous schedule ( self . execution date ) ) . first ( )
def conditionally trigger ( context , dag run obj ) : c p = context [ 'params' ] [ 'condition param' ] print ( "Controller DAG : conditionally trigger = {}" . format ( c p ) ) if context [ 'params' ] [ 'condition param' ] : dag run obj . payload = { 'message' : context [ 'params' ] [ 'message' ] } pp . pprint ( dag run obj . payload ) return dag run obj
def get dag ( self , dag id ) : from airflow . models . dag import Dag Model # Avoid circular import # If asking for a known subdag, we want to refresh the parent root dag id = dag id if dag id in self . dags : dag = self . dags [ dag id ] if dag . is subdag : root dag id = dag . parent dag . dag id # If the dag corresponding to root dag id is absent or expired orm dag = Dag Model . get current ( root dag id ) if orm dag and ( root dag id not in self . dags or ( orm dag . last expired and dag . last loaded < orm dag . last expired ) ) : # Reprocess source file found dags = self . process file ( filepath = orm dag . fileloc , only if updated = False ) # If the source file no longer exports `dag id`, delete it from self.dags if found dags and dag id in [ found dag . dag id for found dag in found dags ] : return self . dags [ dag id ] elif dag id in self . dags : del self . dags [ dag id ] return self . dags . get ( dag id )
def dagbag report ( self ) : report = textwrap . dedent ( ) stats = self . dagbag stats return report . format ( dag folder = self . dag folder , duration = sum ( [ o . duration for o in stats ] ) , dag num = sum ( [ o . dag num for o in stats ] ) , task num = sum ( [ o . task num for o in stats ] ) , table = pprinttable ( stats ) , )
def execute ( self , context ) : self . hook = Spark JDBC Hook ( spark app name = self . spark app name , spark conn id = self . spark conn id , spark conf = self . spark conf , spark py files = self . spark py files , spark files = self . spark files , spark jars = self . spark jars , num executors = self . num executors , executor cores = self . executor cores , executor memory = self . executor memory , driver memory = self . driver memory , verbose = self . verbose , keytab = self . keytab , principal = self . principal , cmd type = self . cmd type , jdbc table = self . jdbc table , jdbc conn id = self . jdbc conn id , jdbc driver = self . jdbc driver , metastore table = self . metastore table , jdbc truncate = self . jdbc truncate , save mode = self . save mode , save format = self . save format , batch size = self . batch size , fetch size = self . fetch size , num partitions = self . num partitions , partition column = self . partition column , lower bound = self . lower bound , upper bound = self . upper bound , create table column types = self . create table column types ) self . hook . submit jdbc job ( )
def integrate plugins ( ) : import sys from airflow . plugins manager import macros modules for macros module in macros modules : sys . modules [ macros module . name ] = macros module globals ( ) [ macros module . name ] = macros module
def error ( self , session = None ) : self . log . error ( "Recording the task instance as FAILED" ) self . state = State . FAILED session . merge ( self ) session . commit ( )
def clear xcom data ( self , session = None ) : session . query ( X Com ) . filter ( X Com . dag id == self . dag id , X Com . task id == self . task id , X Com . execution date == self . execution date ) . delete ( ) session . commit ( )
def key ( self ) : return self . dag id , self . task id , self . execution date , self . try number
def init run context ( self , raw = False ) : self . raw = raw self . set context ( self )
def close ( self ) : # When application exit, system shuts down all handlers by # calling close method. Here we check if logger is already # closed to prevent uploading the log to remote storage multiple # times when `logging.shutdown` is called. if self . closed : return super ( ) . close ( ) if not self . upload on close : return local loc = os . path . join ( self . local base , self . log relative path ) remote loc = os . path . join ( self . remote base , self . log relative path ) if os . path . exists ( local loc ) : # read log and remove old logs to get just the latest additions with open ( local loc , 'r' ) as logfile : log = logfile . read ( ) self . wasb write ( log , remote loc , append = True ) if self . delete local copy : shutil . rmtree ( os . path . dirname ( local loc ) ) # Mark closed so we don't double write if close is called twice self . closed = True
def query cassandra ( self ) : self . hook = Cassandra Hook ( cassandra conn id = self . cassandra conn id ) session = self . hook . get conn ( ) cursor = session . execute ( self . cql ) return cursor
def execute ( self , context ) : self . hook = Spark Sql Hook ( sql = self . sql , conf = self . conf , conn id = self . conn id , total executor cores = self . total executor cores , executor cores = self . executor cores , executor memory = self . executor memory , keytab = self . keytab , principal = self . principal , name = self . name , num executors = self . num executors , master = self . master , yarn queue = self . yarn queue ) self . hook . run query ( )
def get conn ( self ) : conn = self . get connection ( self . conn id ) service options = conn . extra dejson self . account name = service options . get ( 'account name' ) adl Creds = lib . auth ( tenant id = service options . get ( 'tenant' ) , client secret = conn . password , client id = conn . login ) adls File System Client = core . Azure DL File System ( adl Creds , store name = self . account name ) adls File System Client . connect ( ) return adls File System Client
def execute ( self , context ) : self . hook = self . get hook ( ) self . hook . get conn ( ) self . query execution context [ 'Database' ] = self . database self . result configuration [ 'Output Location' ] = self . output location self . query execution id = self . hook . run query ( self . query , self . query execution context , self . result configuration , self . client request token ) query status = self . hook . poll query status ( self . query execution id , self . max tries ) if query status in AWS Athena Hook . FAILURE STATES : raise Exception ( 'Final state of Athena job is {}, query execution id is {}.' . format ( query status , self . query execution id ) ) elif not query status or query status in AWS Athena Hook . INTERMEDIATE STATES : raise Exception ( 'Final state of Athena job is {}. ' 'Max tries of poll status exceeded, query execution id is {}.' . format ( query status , self . query execution id ) )
def on kill ( self ) : if self . query execution id : self . log . info ( '⚰️⚰️⚰️ Received a kill Signal. Time to Die') self . log . info ( 'Stopping Query with execution Id - %s' , self . query execution id ) response = self . hook . stop query ( self . query execution id ) http status code = None try : http status code = response [ 'Response Metadata' ] [ 'HTTP Status Code' ] except Exception as ex : self . log . error ( 'Exception while cancelling query' , ex ) finally : if http status code is None or http status code != 200 : self . log . error ( 'Unable to request query cancel on athena. Exiting' ) else : self . log . info ( 'Polling Athena for query with id %s to reach final state' , self . query execution id ) self . hook . poll query status ( self . query execution id )
def uncompress file ( input file name , file extension , dest dir ) : if file extension . lower ( ) not in ( '.gz' , '.bz2' ) : raise Not Implemented Error ( "Received {} format. Only gz and bz2 " "files can currently be uncompressed." . format ( file extension ) ) if file extension . lower ( ) == '.gz' : fmodule = gzip . Gzip File elif file extension . lower ( ) == '.bz2' : fmodule = bz2 . BZ2File with fmodule ( input file name , mode = 'rb' ) as f compressed , Named Temporary File ( dir = dest dir , mode = 'wb' , delete = False ) as f uncompressed : shutil . copyfileobj ( f compressed , f uncompressed ) return f uncompressed . name
def get conn ( self ) : if not self . conn : connection = self . get connection ( self . conn id ) extras = connection . extra dejson self . conn = Salesforce ( username = connection . login , password = connection . password , security token = extras [ 'security token' ] , instance url = connection . host , sandbox = extras . get ( 'sandbox' , False ) ) return self . conn
def put records ( self , records ) : firehose conn = self . get conn ( ) response = firehose conn . put record batch ( Delivery Stream Name = self . delivery stream , Records = records ) return response
def send email ( to , subject , html content , files = None , dryrun = False , cc = None , bcc = None , mime subtype = 'mixed' , mime charset = 'utf-8' , * * kwargs ) : path , attr = configuration . conf . get ( 'email' , 'EMAIL BACKEND' ) . rsplit ( '.' , 1 ) module = importlib . import module ( path ) backend = getattr ( module , attr ) to = get email address list ( to ) to = ", " . join ( to ) return backend ( to , subject , html content , files = files , dryrun = dryrun , cc = cc , bcc = bcc , mime subtype = mime subtype , mime charset = mime charset , * * kwargs )
def get conn ( self ) : if self . conn is None : params = self . get connection ( self . ftp conn id ) pasv = params . extra dejson . get ( "passive" , True ) self . conn = ftplib . FTP ( params . host , params . login , params . password ) self . conn . set pasv ( pasv ) return self . conn
def execute ( self , context ) : self . hook = Discord Webhook Hook ( self . http conn id , self . webhook endpoint , self . message , self . username , self . avatar url , self . tts , self . proxy ) self . hook . execute ( )
def get conn ( self ) : conn = self . get connection ( self . conn id ) service options = conn . extra dejson return File Service ( account name = conn . login , account key = conn . password , * * service options )
def get conn ( self ) : if not self . conn : self . conn = storage . Client ( credentials = self . get credentials ( ) ) return self . conn
def describe training job with log ( self , job name , positions , stream names , instance count , state , last description , last describe job call ) : log group = '/aws/sagemaker/Training Jobs' if len ( stream names ) < instance count : # Log streams are created whenever a container starts writing to stdout/err, so this list # may be dynamic until we have a stream for every instance. logs conn = self . get log conn ( ) try : streams = logs conn . describe log streams ( log Group Name = log group , log Stream Name Prefix = job name + '/' , order By = 'Log Stream Name' , limit = instance count ) stream names = [ s [ 'log Stream Name' ] for s in streams [ 'log Streams' ] ] positions . update ( [ ( s , Position ( timestamp = 0 , skip = 0 ) ) for s in stream names if s not in positions ] ) except logs conn . exceptions . Resource Not Found Exception : # On the very first training job run on an account, there's no log group until # the container starts logging, so ignore any errors thrown about that pass if len ( stream names ) > 0 : for idx , event in self . multi stream iter ( log group , stream names , positions ) : self . log . info ( event [ 'message' ] ) ts , count = positions [ stream names [ idx ] ] if event [ 'timestamp' ] == ts : positions [ stream names [ idx ] ] = Position ( timestamp = ts , skip = count + 1 ) else : positions [ stream names [ idx ] ] = Position ( timestamp = event [ 'timestamp' ] , skip = 1 ) if state == Log State . COMPLETE : return state , last description , last describe job call if state == Log State . JOB COMPLETE : state = Log State . COMPLETE elif time . time ( ) - last describe job call >= 30 : description = self . describe training job ( job name ) last describe job call = time . time ( ) if secondary training status changed ( description , last description ) : self . log . info ( secondary training status message ( description , last description ) ) last description = description status = description [ 'Training Job Status' ] if status not in self . non terminal states : state = Log State . JOB COMPLETE return state , last description , last describe job call
def execute ( self , context ) : bucket helper = Google Cloud Bucket Helper ( self . gcp conn id , self . delegate to ) self . py file = bucket helper . google cloud to local ( self . py file ) hook = Data Flow Hook ( gcp conn id = self . gcp conn id , delegate to = self . delegate to , poll sleep = self . poll sleep ) dataflow options = self . dataflow default options . copy ( ) dataflow options . update ( self . options ) # Convert argument names from lower Camel Case to snake case. camel to snake = lambda name : re . sub ( r'[A-Z]' , lambda x : ' ' + x . group ( 0 ) . lower ( ) , name ) formatted options = { camel to snake ( key ) : dataflow options [ key ] for key in dataflow options } hook . start python dataflow ( self . job name , formatted options , self . py file , self . py options )
def prepare cli cmd ( self ) : conn = self . conn hive bin = 'hive' cmd extra = [ ] if self . use beeline : hive bin = 'beeline' jdbc url = "jdbc:hive2://{host}:{port}/{schema}" . format ( host = conn . host , port = conn . port , schema = conn . schema ) if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' : template = conn . extra dejson . get ( 'principal' , "hive/ HOST@EXAMPLE.COM" ) if " HOST" in template : template = utils . replace hostname pattern ( utils . get components ( template ) ) proxy user = "" # noqa if conn . extra dejson . get ( 'proxy user' ) == "login" and conn . login : proxy user = "hive.server2.proxy.user={0}" . format ( conn . login ) elif conn . extra dejson . get ( 'proxy user' ) == "owner" and self . run as : proxy user = "hive.server2.proxy.user={0}" . format ( self . run as ) jdbc url += ";principal={template};{proxy user}" . format ( template = template , proxy user = proxy user ) elif self . auth : jdbc url += ";auth=" + self . auth jdbc url = '"{}"' . format ( jdbc url ) cmd extra += [ '-u' , jdbc url ] if conn . login : cmd extra += [ '-n' , conn . login ] if conn . password : cmd extra += [ '-p' , conn . password ] hive params list = self . hive cli params . split ( ) return [ hive bin ] + cmd extra + hive params list
def get metastore client ( self ) : import hmsclient from thrift . transport import T Socket , T Transport from thrift . protocol import T Binary Protocol ms = self . metastore conn auth mechanism = ms . extra dejson . get ( 'auth Mechanism' , 'NOSASL' ) if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' : auth mechanism = ms . extra dejson . get ( 'auth Mechanism' , 'GSSAPI' ) kerberos service name = ms . extra dejson . get ( 'kerberos service name' , 'hive' ) socket = T Socket . T Socket ( ms . host , ms . port ) if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' and auth mechanism == 'GSSAPI' : try : import saslwrapper as sasl except Import Error : import sasl def sasl factory ( ) : sasl client = sasl . Client ( ) sasl client . set Attr ( "host" , ms . host ) sasl client . set Attr ( "service" , kerberos service name ) sasl client . init ( ) return sasl client from thrift sasl import T Sasl Client Transport transport = T Sasl Client Transport ( sasl factory , "GSSAPI" , socket ) else : transport = T Transport . T Buffered Transport ( socket ) protocol = T Binary Protocol . T Binary Protocol ( transport ) return hmsclient . HMS Client ( iprot = protocol )
def get tables ( self , db , pattern = '*' ) : with self . metastore as client : tables = client . get tables ( db name = db , pattern = pattern ) return client . get table objects by name ( db , tables )
def get conn ( self , schema = None ) : db = self . get connection ( self . hiveserver2 conn id ) auth mechanism = db . extra dejson . get ( 'auth Mechanism' , 'NONE' ) if auth mechanism == 'NONE' and db . login is None : # we need to give a username username = 'airflow' kerberos service name = None if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' : auth mechanism = db . extra dejson . get ( 'auth Mechanism' , 'KERBEROS' ) kerberos service name = db . extra dejson . get ( 'kerberos service name' , 'hive' ) # pyhive uses GSSAPI instead of KERBEROS as a auth mechanism identifier if auth mechanism == 'GSSAPI' : self . log . warning ( "Detected deprecated 'GSSAPI' for auth Mechanism " "for %s. Please use 'KERBEROS' instead" , self . hiveserver2 conn id ) auth mechanism = 'KERBEROS' from pyhive . hive import connect return connect ( host = db . host , port = db . port , auth = auth mechanism , kerberos service name = kerberos service name , username = db . login or username , password = db . password , database = schema or db . schema or 'default' )
def get endpoint ( self ) : conn = self . get connection ( self . http conn id ) token = conn . password if not token : raise Airflow Exception ( 'Dingding token is requests but get nothing, ' 'check you conn id configuration.' ) return 'robot/send?access token={}' . format ( token )
def bind parameters ( operation , parameters ) : # inspired by My SQL Python Connector (conversion.py) string parameters = { } for ( name , value ) in iteritems ( parameters ) : if value is None : string parameters [ name ] = 'NULL' elif isinstance ( value , basestring ) : string parameters [ name ] = "'" + escape ( value ) + "'" else : string parameters [ name ] = str ( value ) return operation % string parameters
def escape ( s ) : e = s e = e . replace ( '\\' , '\\\\' ) e = e . replace ( '\n' , '\\n' ) e = e . replace ( '\r' , '\\r' ) e = e . replace ( "'" , "\\'" ) e = e . replace ( '"' , '\\"' ) return e
def get conn ( self ) : service = self . get service ( ) project = self . get field ( 'project' ) return Big Query Connection ( service = service , project id = project , use legacy sql = self . use legacy sql , location = self . location , num retries = self . num retries )
def get service ( self ) : http authorized = self . authorize ( ) return build ( 'bigquery' , 'v2' , http = http authorized , cache discovery = False )
def cancel query ( self ) : jobs = self . service . jobs ( ) if ( self . running job id and not self . poll job complete ( self . running job id ) ) : self . log . info ( 'Attempting to cancel job : %s, %s' , self . project id , self . running job id ) if self . location : jobs . cancel ( project Id = self . project id , job Id = self . running job id , location = self . location ) . execute ( num retries = self . num retries ) else : jobs . cancel ( project Id = self . project id , job Id = self . running job id ) . execute ( num retries = self . num retries ) else : self . log . info ( 'No running Big Query jobs to cancel.' ) return # Wait for all the calls to cancel to finish max polling attempts = 12 polling attempts = 0 job complete = False while polling attempts < max polling attempts and not job complete : polling attempts = polling attempts + 1 job complete = self . poll job complete ( self . running job id ) if job complete : self . log . info ( 'Job successfully canceled: %s, %s' , self . project id , self . running job id ) elif polling attempts == max polling attempts : self . log . info ( "Stopping polling due to timeout. Job with id %s " "has not completed cancel and may or may not finish." , self . running job id ) else : self . log . info ( 'Waiting for canceled job with id %s to finish.' , self . running job id ) time . sleep ( 5 )
def query postgres ( self ) : postgres = Postgres Hook ( postgres conn id = self . postgres conn id ) conn = postgres . get conn ( ) cursor = conn . cursor ( ) cursor . execute ( self . sql , self . parameters ) return cursor
def integrate plugins ( ) : from airflow . plugins manager import hooks modules for hooks module in hooks modules : sys . modules [ hooks module . name ] = hooks module globals ( ) [ hooks module . name ] = hooks module
def on finish ( self ) : if self . cfg path and os . path . isfile ( self . cfg path ) : if self . run as user : subprocess . call ( [ 'sudo' , 'rm' , self . cfg path ] , close fds = True ) else : os . remove ( self . cfg path )
def main ( ) : # Parse arguments usage = "usage: nvd3.py [options]" parser = Option Parser ( usage = usage , version = ( "python-nvd3 - Charts generator with " "nvd3.js and d3.js" ) ) parser . add option ( "-q" , "--quiet" , action = "store false" , dest = "verbose" , default = True , help = "don't print messages to stdout" ) ( options , args ) = parser . parse args ( )
def buildhtmlheader ( self ) : self . htmlheader = '' # If the Java Script assets have already been injected, don't bother re-sourcing them. global js initialized if ' js initialized' not in globals ( ) or not js initialized : for css in self . header css : self . htmlheader += css for js in self . header js : self . htmlheader += js
def buildjschart ( self ) : self . jschart = '' # add custom tooltip string in jschart # default condition (if build custom tooltip is not called explicitly with date flag=True) if self . tooltip condition string == '' : self . tooltip condition string = 'var y = String(graph.point.y);\n' # Include data self . series js = json . dumps ( self . series )
def create x axis ( self , name , label = None , format = None , date = False , custom format = False ) : axis = { } if custom format and format : axis [ 'tick Format' ] = format elif format : if format == 'AM PM' : axis [ 'tick Format' ] = "function(d) { return get am pm(parse Int(d)); }" else : axis [ 'tick Format' ] = "d3.format(',%s')" % format if label : axis [ 'axis Label' ] = "'" + label + "'" if date : self . dateformat = format axis [ 'tick Format' ] = ( "function(d) { return d3.time.format('%s')" "(new Date(parse Int(d))) }\n" "" % self . dateformat ) # flag is the x Axis is a date if name [ 0 ] == 'x' : self . x axis date = True # Add new axis to list of axis self . axislist [ name ] = axis # Create x2Axis if focus enable if name == "x Axis" and self . focus enable : self . axislist [ 'x2Axis' ] = axis
def create y axis ( self , name , label = None , format = None , custom format = False ) : axis = { } if custom format and format : axis [ 'tick Format' ] = format elif format : axis [ 'tick Format' ] = "d3.format(',%s')" % format if label : axis [ 'axis Label' ] = "'" + label + "'" # Add new axis to list of axis self . axislist [ name ] = axis
def get conn ( self ) : conn = self . get connection ( self . sqlite conn id ) conn = sqlite3 . connect ( conn . host ) return conn
def action logging ( f ) : @ functools . wraps ( f ) def wrapper ( * args , * * kwargs ) : with create session ( ) as session : if g . user . is anonymous : user = 'anonymous' else : user = g . user . username log = Log ( event = f . name , task instance = None , owner = user , extra = str ( list ( request . args . items ( ) ) ) , task id = request . args . get ( 'task id' ) , dag id = request . args . get ( 'dag id' ) ) if 'execution date' in request . args : log . execution date = pendulum . parse ( request . args . get ( 'execution date' ) ) session . add ( log ) return f ( * args , * * kwargs ) return wrapper
def gzipped ( f ) : @ functools . wraps ( f ) def view func ( * args , * * kwargs ) : @ after this request def zipper ( response ) : accept encoding = request . headers . get ( 'Accept-Encoding' , '' ) if 'gzip' not in accept encoding . lower ( ) : return response response . direct passthrough = False if ( response . status code < 200 or response . status code >= 300 or 'Content-Encoding' in response . headers ) : return response gzip buffer = IO ( ) gzip file = gzip . Gzip File ( mode = 'wb' , fileobj = gzip buffer ) gzip file . write ( response . data ) gzip file . close ( ) response . data = gzip buffer . getvalue ( ) response . headers [ 'Content-Encoding' ] = 'gzip' response . headers [ 'Vary' ] = 'Accept-Encoding' response . headers [ 'Content-Length' ] = len ( response . data ) return response return f ( * args , * * kwargs ) return view func
def json response ( obj ) : return Response ( response = json . dumps ( obj , indent = 4 , cls = Airflow Json Encoder ) , status = 200 , mimetype = "application/json" )
def make cache key ( * args , * * kwargs ) : path = request . path args = str ( hash ( frozenset ( request . args . items ( ) ) ) ) return ( path + args ) . encode ( 'ascii' , 'ignore' )
def get api key ( self ) : conn = self . get connection ( self . http conn id ) api key = conn . password if not api key : raise Airflow Exception ( 'Opsgenie API Key is required for this hook, ' 'please check your conn id configuration.' ) return api key
def execute ( self , context ) : self . hook = Opsgenie Alert Hook ( self . opsgenie conn id ) self . hook . execute ( self . build opsgenie payload ( ) )
def get conn ( self ) : if self . conn is None : cnopts = pysftp . Cn Opts ( ) if self . no host key check : cnopts . hostkeys = None cnopts . compression = self . compress conn params = { 'host' : self . remote host , 'port' : self . port , 'username' : self . username , 'cnopts' : cnopts } if self . password and self . password . strip ( ) : conn params [ 'password' ] = self . password if self . key file : conn params [ 'private key' ] = self . key file if self . private key pass : conn params [ 'private key pass' ] = self . private key pass self . conn = pysftp . Connection ( * * conn params ) return self . conn
def execute ( self , context ) : s3 conn = S3Hook ( self . s3 conn id ) # Grab collection and execute query according to whether or not it is a pipeline if self . is pipeline : results = Mongo Hook ( self . mongo conn id ) . aggregate ( mongo collection = self . mongo collection , aggregate query = self . mongo query , mongo db = self . mongo db ) else : results = Mongo Hook ( self . mongo conn id ) . find ( mongo collection = self . mongo collection , query = self . mongo query , mongo db = self . mongo db ) # Performs transform then stringifies the docs results into json format docs str = self . stringify ( self . transform ( results ) ) # Load Into S3 s3 conn . load string ( string data = docs str , key = self . s3 key , bucket name = self . s3 bucket , replace = self . replace ) return True
def get pool ( name , session = None ) : if not ( name and name . strip ( ) ) : raise Airflow Bad Request ( "Pool name shouldn't be empty" ) pool = session . query ( Pool ) . filter by ( pool = name ) . first ( ) if pool is None : raise Pool Not Found ( "Pool '%s' doesn't exist" % name ) return pool
def create pool ( name , slots , description , session = None ) : if not ( name and name . strip ( ) ) : raise Airflow Bad Request ( "Pool name shouldn't be empty" ) try : slots = int ( slots ) except Value Error : raise Airflow Bad Request ( "Bad value for `slots`: %s" % slots ) session . expire on commit = False pool = session . query ( Pool ) . filter by ( pool = name ) . first ( ) if pool is None : pool = Pool ( pool = name , slots = slots , description = description ) session . add ( pool ) else : pool . slots = slots pool . description = description session . commit ( ) return pool
def delete pool ( name , session = None ) : if not ( name and name . strip ( ) ) : raise Airflow Bad Request ( "Pool name shouldn't be empty" ) pool = session . query ( Pool ) . filter by ( pool = name ) . first ( ) if pool is None : raise Pool Not Found ( "Pool '%s' doesn't exist" % name ) session . delete ( pool ) session . commit ( ) return pool
def execute ( self ) : proxies = { } if self . proxy : # we only need https proxy for Discord proxies = { 'https' : self . proxy } discord payload = self . build discord payload ( ) self . run ( endpoint = self . webhook endpoint , data = discord payload , headers = { 'Content-type' : 'application/json' } , extra options = { 'proxies' : proxies } )
def close ( self ) : # When application exit, system shuts down all handlers by # calling close method. Here we check if logger is already # closed to prevent uploading the log to remote storage multiple # times when `logging.shutdown` is called. if self . closed : return super ( ) . close ( ) if not self . upload on close : return local loc = os . path . join ( self . local base , self . log relative path ) remote loc = os . path . join ( self . remote base , self . log relative path ) if os . path . exists ( local loc ) : # read log and remove old logs to get just the latest additions with open ( local loc , 'r' ) as logfile : log = logfile . read ( ) self . s3 write ( log , remote loc ) # Mark closed so we don't double write if close is called twice self . closed = True
def get init containers ( self ) : # If we're using volume claims to mount the dags, no init container is needed if self . kube config . dags volume claim or self . kube config . dags volume host or self . kube config . dags in image : return [ ] # Otherwise, define a git-sync init container init environment = [ { 'name' : 'GIT SYNC REPO' , 'value' : self . kube config . git repo } , { 'name' : 'GIT SYNC BRANCH' , 'value' : self . kube config . git branch } , { 'name' : 'GIT SYNC ROOT' , 'value' : self . kube config . git sync root } , { 'name' : 'GIT SYNC DEST' , 'value' : self . kube config . git sync dest } , { 'name' : 'GIT SYNC DEPTH' , 'value' : '1' } , { 'name' : 'GIT SYNC ONE TIME' , 'value' : 'true' } ] if self . kube config . git user : init environment . append ( { 'name' : 'GIT SYNC USERNAME' , 'value' : self . kube config . git user } ) if self . kube config . git password : init environment . append ( { 'name' : 'GIT SYNC PASSWORD' , 'value' : self . kube config . git password } ) volume mounts = [ { 'mount Path' : self . kube config . git sync root , 'name' : self . dags volume name , 'read Only' : False } ] if self . kube config . git ssh key secret name : volume mounts . append ( { 'name' : self . git sync ssh secret volume name , 'mount Path' : '/etc/git-secret/ssh' , 'sub Path' : 'ssh' } ) init environment . extend ( [ { 'name' : 'GIT SSH KEY FILE' , 'value' : '/etc/git-secret/ssh' } , { 'name' : 'GIT SYNC SSH' , 'value' : 'true' } ] ) if self . kube config . git ssh known hosts configmap name : volume mounts . append ( { 'name' : self . git sync ssh known hosts volume name , 'mount Path' : '/etc/git-secret/known hosts' , 'sub Path' : 'known hosts' } ) init environment . extend ( [ { 'name' : 'GIT KNOWN HOSTS' , 'value' : 'true' } , { 'name' : 'GIT SSH KNOWN HOSTS FILE' , 'value' : '/etc/git-secret/known hosts' } ] ) else : init environment . append ( { 'name' : 'GIT KNOWN HOSTS' , 'value' : 'false' } ) return [ { 'name' : self . kube config . git sync init container name , 'image' : self . kube config . git sync container , 'security Context' : { 'run As User' : 65533 } , # git-sync user 'env' : init environment , 'volume Mounts' : volume mounts } ]
def get environment ( self ) : env = { } for env var name , env var val in six . iteritems ( self . kube config . kube env vars ) : env [ env var name ] = env var val env [ "AIRFLOW CORE EXECUTOR" ] = "Local Executor" if self . kube config . airflow configmap : env [ 'AIRFLOW HOME' ] = self . worker airflow home env [ 'AIRFLOW CORE DAGS FOLDER' ] = self . worker airflow dags if ( not self . kube config . airflow configmap and 'AIRFLOW CORE SQL ALCHEMY CONN' not in self . kube config . kube secrets ) : env [ 'AIRFLOW CORE SQL ALCHEMY CONN' ] = conf . get ( "core" , "SQL ALCHEMY CONN" ) if self . kube config . git dags folder mount point : # /root/airflow/dags/repo/dags dag volume mount path = os . path . join ( self . kube config . git dags folder mount point , self . kube config . git sync dest , # repo self . kube config . git subpath # dags ) env [ 'AIRFLOW CORE DAGS FOLDER' ] = dag volume mount path return env
def get secrets ( self ) : worker secrets = [ ] for env var name , obj key pair in six . iteritems ( self . kube config . kube secrets ) : k8s secret obj , k8s secret key = obj key pair . split ( '=' ) worker secrets . append ( Secret ( 'env' , env var name , k8s secret obj , k8s secret key ) ) if self . kube config . env from secret ref : for secret ref in self . kube config . env from secret ref . split ( ',' ) : worker secrets . append ( Secret ( 'env' , None , secret ref ) ) return worker secrets
def get security context ( self ) : security context = { } if self . kube config . worker run as user : security context [ 'run As User' ] = self . kube config . worker run as user if self . kube config . worker fs group : security context [ 'fs Group' ] = self . kube config . worker fs group # set fs group to 65533 if not explicitly specified and using git ssh keypair auth if self . kube config . git ssh key secret name and security context . get ( 'fs Group' ) is None : security context [ 'fs Group' ] = 65533 return security context
def start ( self ) : self . process = Dag File Processor . launch process ( self . result queue , self . file path , self . pickle dags , self . dag id white list , "Dag File Processor{}" . format ( self . instance id ) , self . zombies ) self . start time = timezone . utcnow ( )
def exit gracefully ( self , signum , frame ) : self . log . info ( "Exiting gracefully upon receiving signal %s" , signum ) if self . processor agent : self . processor agent . end ( ) sys . exit ( os . EX OK )
def process executor events ( self , simple dag bag , session = None ) : # TODO: this shares quite a lot of code with  manage executor state TI = models . Task Instance for key , state in list ( self . executor . get event buffer ( simple dag bag . dag ids ) . items ( ) ) : dag id , task id , execution date , try number = key self . log . info ( "Executor reports execution of %s.%s execution date=%s " "exited with status %s for try number %s" , dag id , task id , execution date , state , try number ) if state == State . FAILED or state == State . SUCCESS : qry = session . query ( TI ) . filter ( TI . dag id == dag id , TI . task id == task id , TI . execution date == execution date ) ti = qry . first ( ) if not ti : self . log . warning ( "Task Instance %s went missing from the database" , ti ) continue # TODO: should we fail RUNNING as well, as we do in Backfills? if ti . try number == try number and ti . state == State . QUEUED : msg = ( "Executor reports task instance {} finished ({}) " "although the task says its {}. Was the task " "killed externally?" . format ( ti , state , ti . state ) ) self . log . error ( msg ) try : simple dag = simple dag bag . get dag ( dag id ) dagbag = models . Dag Bag ( simple dag . full filepath ) dag = dagbag . get dag ( dag id ) ti . task = dag . get task ( task id ) ti . handle failure ( msg ) except Exception : self . log . error ( "Cannot load the dag bag to handle failure for %s" ". Setting task to FAILED without callbacks or " "retries. Do you have enough resources?" , ti ) ti . state = State . FAILED session . merge ( ti ) session . commit ( )
def heartbeat callback ( self , session = None ) : if self . terminating : # ensure termination if processes are created later self . task runner . terminate ( ) return self . task instance . refresh from db ( ) ti = self . task instance fqdn = get hostname ( ) same hostname = fqdn == ti . hostname same process = ti . pid == os . getpid ( ) if ti . state == State . RUNNING : if not same hostname : self . log . warning ( "The recorded hostname %s " "does not match this instance's hostname " "%s" , ti . hostname , fqdn ) raise Airflow Exception ( "Hostname of job runner does not match" ) elif not same process : current pid = os . getpid ( ) self . log . warning ( "Recorded pid %s does not match " "the current pid %s" , ti . pid , current pid ) raise Airflow Exception ( "PID of job runner does not match" ) elif ( self . task runner . return code ( ) is None and hasattr ( self . task runner , 'process' ) ) : self . log . warning ( "State of this instance has been externally set to %s. " "Taking the poison pill." , ti . state ) self . task runner . terminate ( ) self . terminating = True
def get conn ( self ) : if self . session and not self . session . is shutdown : return self . session self . session = self . cluster . connect ( self . keyspace ) return self . session
def query mysql ( self ) : mysql = My Sql Hook ( mysql conn id = self . mysql conn id ) conn = mysql . get conn ( ) cursor = conn . cursor ( ) cursor . execute ( self . sql ) return cursor
def get col type dict ( self ) : schema = [ ] if isinstance ( self . schema , string types ) : schema = json . loads ( self . schema ) elif isinstance ( self . schema , list ) : schema = self . schema elif self . schema is not None : self . log . warn ( 'Using default schema due to unexpected type.' 'Should be a string or list.' ) col type dict = { } try : col type dict = { col [ 'name' ] : col [ 'type' ] for col in schema } except Key Error : self . log . warn ( 'Using default schema due to missing name or type. Please ' 'refer to: https://cloud.google.com/bigquery/docs/schemas' '#specifying a json schema file' ) return col type dict
def extra dejson ( self ) : obj = { } if self . extra : try : obj = json . loads ( self . extra ) except Exception as e : self . log . exception ( e ) self . log . error ( "Failed parsing the json for conn id %s" , self . conn id ) return obj
def scale time units ( time seconds arr , unit ) : if unit == 'minutes' : return list ( map ( lambda x : x * 1.0 / 60 , time seconds arr ) ) elif unit == 'hours' : return list ( map ( lambda x : x * 1.0 / ( 60 * 60 ) , time seconds arr ) ) elif unit == 'days' : return list ( map ( lambda x : x * 1.0 / ( 24 * 60 * 60 ) , time seconds arr ) ) return time seconds arr
def get all permissions views ( self ) : perms views = set ( ) for role in self . get user roles ( ) : perms views . update ( { ( perm view . permission . name , perm view . view menu . name ) for perm view in role . permissions } ) return perms views
def has role ( self , role name or list ) : if not isinstance ( role name or list , list ) : role name or list = [ role name or list ] return any ( [ r . name in role name or list for r in self . get user roles ( ) ] )
def has perm ( self , permission name , view menu name ) : if hasattr ( self , 'perms' ) : if ( permission name , view menu name ) in self . perms : return True # rebuild the permissions set self . get and cache perms ( ) return ( permission name , view menu name ) in self . perms
def clean perms ( self ) : self . log . debug ( 'Cleaning faulty perms' ) sesh = self . get session pvms = ( sesh . query ( sqla models . Permission View ) . filter ( or ( sqla models . Permission View . permission == None , # NOQA sqla models . Permission View . view menu == None , # NOQA ) ) ) deleted count = pvms . delete ( ) sesh . commit ( ) if deleted count : self . log . info ( 'Deleted %s faulty permissions' , deleted count )
def create perm vm for all dag ( self ) : # create perm for global logical dag for dag vm in self . DAG VMS : for perm in self . DAG PERMS : self . merge perm ( permission name = perm , view menu name = dag vm )
def poke ( self , context ) : if '.' in self . table name : self . database name , self . table name = self . table name . split ( '.' ) self . log . info ( 'Poking for table %s. %s, expression %s' , self . database name , self . table name , self . expression ) return self . get hook ( ) . check for partition ( self . database name , self . table name , self . expression )
def get conn ( self ) : # When using HA Client, proxy user must be the same, so is ok to always # take the first. effective user = self . proxy user autoconfig = self . autoconfig use sasl = configuration . conf . get ( 'core' , 'security' ) == 'kerberos' try : connections = self . get connections ( self . hdfs conn id ) if not effective user : effective user = connections [ 0 ] . login if not autoconfig : autoconfig = connections [ 0 ] . extra dejson . get ( 'autoconfig' , False ) hdfs namenode principal = connections [ 0 ] . extra dejson . get ( 'hdfs namenode principal' ) except Airflow Exception : if not autoconfig : raise if autoconfig : # will read config info from $HADOOP HOME conf files client = Auto Config Client ( effective user = effective user , use sasl = use sasl ) elif len ( connections ) == 1 : client = Client ( connections [ 0 ] . host , connections [ 0 ] . port , effective user = effective user , use sasl = use sasl , hdfs namenode principal = hdfs namenode principal ) elif len ( connections ) > 1 : nn = [ Namenode ( conn . host , conn . port ) for conn in connections ] client = HA Client ( nn , effective user = effective user , use sasl = use sasl , hdfs namenode principal = hdfs namenode principal ) else : raise HDFS Hook Exception ( "conn id doesn't exist in the repository " "and autoconfig is not specified" ) return client
def get conn ( self ) : conn = self . get connection ( self . pinot broker conn id ) pinot broker conn = connect ( host = conn . host , port = conn . port , path = conn . extra dejson . get ( 'endpoint' , '/pql' ) , scheme = conn . extra dejson . get ( 'schema' , 'http' ) ) self . log . info ( 'Get the connection to pinot ' 'broker on {host}' . format ( host = conn . host ) ) return pinot broker conn
def convert date to dict ( field date ) : return { DAY : field date . day , MONTH : field date . month , YEAR : field date . year }
def convert time to dict ( time ) : return { HOURS : time . hour , MINUTES : time . minute , SECONDS : time . second }
def get conn ( self ) : conn = self . get connection ( self . redis conn id ) self . host = conn . host self . port = conn . port self . password = None if str ( conn . password ) . lower ( ) in [ 'none' , 'false' , '' ] else conn . password self . db = conn . extra dejson . get ( 'db' , None ) if not self . redis : self . log . debug ( 'Initializing redis object for conn id "%s" on %s:%s:%s' , self . redis conn id , self . host , self . port , self . db ) self . redis = Redis ( host = self . host , port = self . port , password = self . password , db = self . db ) return self . redis
def get conn ( self ) : db = self . get connection ( getattr ( self , self . conn name attr ) ) return self . connector . connect ( host = db . host , port = db . port , username = db . login , schema = db . schema )
def set autocommit ( self , conn , autocommit ) : if not self . supports autocommit and autocommit : self . log . warn ( ( "%s connection doesn't support " "autocommit but autocommit activated." ) , getattr ( self , self . conn name attr ) ) conn . autocommit = autocommit
def get query ( self ) : return ( super ( ) . get query ( ) . filter ( or ( models . Dag Model . is active , models . Dag Model . is paused ) ) . filter ( ~ models . Dag Model . is subdag ) )
def get count query ( self ) : return ( super ( ) . get count query ( ) . filter ( models . Dag Model . is active ) . filter ( ~ models . Dag Model . is subdag ) )
def execute ( self , context ) : self . hook = Slack Webhook Hook ( self . http conn id , self . webhook token , self . message , self . attachments , self . channel , self . username , self . icon emoji , self . link names , self . proxy ) self . hook . execute ( )
def get credentials ( self ) : key path = self . get field ( 'key path' , False ) keyfile dict = self . get field ( 'keyfile dict' , False ) scope = self . get field ( 'scope' , None ) if scope : scopes = [ s . strip ( ) for s in scope . split ( ',' ) ] else : scopes = DEFAULT SCOPES if not key path and not keyfile dict : self . log . info ( 'Getting connection using `google.auth.default()` ' 'since no key file is defined for hook.' ) credentials , = google . auth . default ( scopes = scopes ) elif key path : # Get credentials from a JSON file. if key path . endswith ( '.json' ) : self . log . debug ( 'Getting connection using JSON key file %s' % key path ) credentials = ( google . oauth2 . service account . Credentials . from service account file ( key path , scopes = scopes ) ) elif key path . endswith ( '.p12' ) : raise Airflow Exception ( 'Legacy P12 key file are not supported, ' 'use a JSON key file.' ) else : raise Airflow Exception ( 'Unrecognised extension for key file.' ) else : # Get credentials from JSON data provided in the UI. try : keyfile dict = json . loads ( keyfile dict ) # Depending on how the JSON was formatted, it may contain # escaped newlines. Convert those to actual newlines. keyfile dict [ 'private key' ] = keyfile dict [ 'private key' ] . replace ( '\\n' , '\n' ) credentials = ( google . oauth2 . service account . Credentials . from service account info ( keyfile dict , scopes = scopes ) ) except json . decoder . JSON Decode Error : raise Airflow Exception ( 'Invalid key JSON.' ) return credentials . with subject ( self . delegate to ) if self . delegate to else credentials
def read image file ( data dir , image ext , n ) : def PIL2array ( img ) : return np . array ( img . getdata ( ) , dtype = np . uint8 ) . reshape ( 64 , 64 ) def find files ( data dir , image ext ) : files = [ ] # find those files with the specified extension for file dir in os . listdir ( data dir ) : if file dir . endswith ( image ext ) : files . append ( os . path . join ( data dir , file dir ) ) return sorted ( files ) # sort files in ascend order to keep relations patches = [ ] list files = find files ( data dir , image ext ) for fpath in list files : img = Image . open ( fpath ) for y in range ( 0 , 1024 , 64 ) : for x in range ( 0 , 1024 , 64 ) : patch = img . crop ( ( x , y , x + 64 , y + 64 ) ) patches . append ( PIL2array ( patch ) ) return torch . Byte Tensor ( np . array ( patches [ : n ] ) )
def accuracy ( output , target , topk = ( 1 , ) ) : with torch . no grad ( ) : maxk = max ( topk ) batch size = target . size ( 0 ) , pred = output . topk ( maxk , 1 , True , True ) pred = pred . t ( ) correct = pred . eq ( target [ None ] ) res = [ ] for k in topk : correct k = correct [ : k ] . flatten ( ) . sum ( dtype = torch . float32 ) res . append ( correct k * ( 100.0 / batch size ) ) return res
def setup for distributed ( is master ) : import builtins as builtin builtin print = builtin . print def print ( * args , * * kwargs ) : force = kwargs . pop ( 'force' , False ) if is master or force : builtin print ( * args , * * kwargs ) builtin . print = print
def download ( self ) : import tarfile if self . check integrity ( ) : print ( 'Files already downloaded and verified' ) return download url ( self . url , self . root , self . filename , self . md5 checksum ) # Extract file with tarfile . open ( os . path . join ( self . root , self . filename ) , 'r:gz' ) as tar : tar . extractall ( path = self . root ) # Download individual photos with open ( os . path . join ( self . root , 'dataset' , 'SBU captioned photo dataset urls.txt' ) ) as fh : for line in fh : url = line . rstrip ( ) try : download url ( url , os . path . join ( self . root , 'dataset' ) ) except OS Error : # The images point to public images on Flickr. # Note: Images might be removed by users at anytime. pass
def download ( self ) : if self . check exists ( ) : return makedir exist ok ( self . raw folder ) makedir exist ok ( self . processed folder ) # download files for url in self . urls : filename = url . rpartition ( '/' ) [ 2 ] file path = os . path . join ( self . raw folder , filename ) download url ( url , root = self . raw folder , filename = filename , md5 = None ) self . extract gzip ( gzip path = file path , remove finished = True ) # process and save as torch files print ( 'Processing...' ) training set = ( read image file ( os . path . join ( self . raw folder , 'train-images-idx3-ubyte' ) ) , read label file ( os . path . join ( self . raw folder , 'train-labels-idx1-ubyte' ) ) ) test set = ( read image file ( os . path . join ( self . raw folder , 't10k-images-idx3-ubyte' ) ) , read label file ( os . path . join ( self . raw folder , 't10k-labels-idx1-ubyte' ) ) ) with open ( os . path . join ( self . processed folder , self . training file ) , 'wb' ) as f : torch . save ( training set , f ) with open ( os . path . join ( self . processed folder , self . test file ) , 'wb' ) as f : torch . save ( test set , f ) print ( 'Done!' )
def download ( self ) : import shutil import zipfile if self . check exists ( ) : return makedir exist ok ( self . raw folder ) makedir exist ok ( self . processed folder ) # download files filename = self . url . rpartition ( '/' ) [ 2 ] file path = os . path . join ( self . raw folder , filename ) download url ( self . url , root = self . raw folder , filename = filename , md5 = None ) print ( 'Extracting zip archive' ) with zipfile . Zip File ( file path ) as zip f : zip f . extractall ( self . raw folder ) os . unlink ( file path ) gzip folder = os . path . join ( self . raw folder , 'gzip' ) for gzip file in os . listdir ( gzip folder ) : if gzip file . endswith ( '.gz' ) : self . extract gzip ( gzip path = os . path . join ( gzip folder , gzip file ) ) # process and save as torch files for split in self . splits : print ( 'Processing ' + split ) training set = ( read image file ( os . path . join ( gzip folder , 'emnist-{}-train-images-idx3-ubyte' . format ( split ) ) ) , read label file ( os . path . join ( gzip folder , 'emnist-{}-train-labels-idx1-ubyte' . format ( split ) ) ) ) test set = ( read image file ( os . path . join ( gzip folder , 'emnist-{}-test-images-idx3-ubyte' . format ( split ) ) ) , read label file ( os . path . join ( gzip folder , 'emnist-{}-test-labels-idx1-ubyte' . format ( split ) ) ) ) with open ( os . path . join ( self . processed folder , self . training file ( split ) ) , 'wb' ) as f : torch . save ( training set , f ) with open ( os . path . join ( self . processed folder , self . test file ( split ) ) , 'wb' ) as f : torch . save ( test set , f ) shutil . rmtree ( gzip folder ) print ( 'Done!' )
def preferences ( ) : # save preferences if request . method == 'POST' : resp = make response ( redirect ( urljoin ( settings [ 'server' ] [ 'base url' ] , url for ( 'index' ) ) ) ) try : request . preferences . parse form ( request . form ) except Validation Exception : request . errors . append ( gettext ( 'Invalid settings, please edit your preferences' ) ) return resp return request . preferences . save ( resp ) # render preferences image proxy = request . preferences . get value ( 'image proxy' ) lang = request . preferences . get value ( 'language' ) disabled engines = request . preferences . engines . get disabled ( ) allowed plugins = request . preferences . plugins . get enabled ( ) # stats for preferences page stats = { } for c in categories : for e in categories [ c ] : stats [ e . name ] = { 'time' : None , 'warn timeout' : False , 'warn time' : False } if e . timeout > settings [ 'outgoing' ] [ 'request timeout' ] : stats [ e . name ] [ 'warn timeout' ] = True stats [ e . name ] [ 'supports selected language' ] = is selected language supported ( e , request . preferences ) # get first element [0], the engine time, # and then the second element [1] : the time (the first one is the label) for engine stat in get engines stats ( ) [ 0 ] [ 1 ] : stats [ engine stat . get ( 'name' ) ] [ 'time' ] = round ( engine stat . get ( 'avg' ) , 3 ) if engine stat . get ( 'avg' ) > settings [ 'outgoing' ] [ 'request timeout' ] : stats [ engine stat . get ( 'name' ) ] [ 'warn time' ] = True # end of stats return render ( 'preferences.html' , locales = settings [ 'locales' ] , current locale = get locale ( ) , image proxy = image proxy , engines by category = categories , stats = stats , answerers = [ { 'info' : a . self info ( ) , 'keywords' : a . keywords } for a in answerers ] , disabled engines = disabled engines , autocomplete backends = autocomplete backends , shortcuts = { y : x for x , y in engine shortcuts . items ( ) } , themes = themes , plugins = plugins , doi resolvers = settings [ 'doi resolvers' ] , current doi resolver = get doi resolver ( request . args , request . preferences . get value ( 'doi resolver' ) ) , allowed plugins = allowed plugins , theme = get current theme name ( ) , preferences url params = request . preferences . get as url params ( ) , base url = get base url ( ) , preferences = True )
def get themes ( templates path ) : themes = os . listdir ( templates path ) if ' common ' in themes : themes . remove ( ' common ' ) return themes
def searx bang ( full query ) : # check if there is a query which can be parsed if len ( full query . get Search Query ( ) ) == 0 : return [ ] results = [ ] # check if current query stats with !bang first char = full query . get Search Query ( ) [ 0 ] if first char == '!' or first char == '?' : if len ( full query . get Search Query ( ) ) == 1 : # show some example queries # TODO, check if engine is not avaliable results . append ( first char + "images" ) results . append ( first char + "wikipedia" ) results . append ( first char + "osm" ) else : engine query = full query . get Search Query ( ) [ 1 : ] # check if query starts with categorie name for categorie in categories : if categorie . startswith ( engine query ) : results . append ( first char + '{categorie}' . format ( categorie = categorie ) ) # check if query starts with engine name for engine in engines : if engine . startswith ( engine query . replace ( ' ' , ' ' ) ) : results . append ( first char + '{engine}' . format ( engine = engine . replace ( ' ' , ' ' ) ) ) # check if query starts with engine shortcut for engine shortcut in engine shortcuts : if engine shortcut . startswith ( engine query ) : results . append ( first char + '{engine shortcut}' . format ( engine shortcut = engine shortcut ) ) # check if current query stats with :bang elif first char == ':' : if len ( full query . get Search Query ( ) ) == 1 : # show some example queries results . append ( ":en" ) results . append ( ":en us" ) results . append ( ":english" ) results . append ( ":united kingdom" ) else : engine query = full query . get Search Query ( ) [ 1 : ] for lc in language codes : lang id , lang name , country , english name = map ( unicode . lower , lc ) # check if query starts with language-id if lang id . startswith ( engine query ) : if len ( engine query ) <= 2 : results . append ( u':{lang id}' . format ( lang id = lang id . split ( '-' ) [ 0 ] ) ) else : results . append ( u':{lang id}' . format ( lang id = lang id ) ) # check if query starts with language name if lang name . startswith ( engine query ) or english name . startswith ( engine query ) : results . append ( u':{lang name}' . format ( lang name = lang name ) ) # check if query starts with country if country . startswith ( engine query . replace ( ' ' , ' ' ) ) : results . append ( u':{country}' . format ( country = country . replace ( ' ' , ' ' ) ) ) # remove duplicates result set = set ( results ) # remove results which are already contained in the query for query part in full query . query parts : if query part in result set : result set . remove ( query part ) # convert result set back to list return list ( result set )
def response ( resp ) : json resp = resp . text [ resp . text . find ( '\n' ) + 1 : resp . text . rfind ( '\n' ) - 2 ] results = [ ] try : conversion rate = float ( json . loads ( json resp ) [ 'conversion' ] [ 'converted-amount' ] ) except : return results answer = '{0} {1} = {2} {3}, 1 {1} ({5}) = {4} {3} ({6})' . format ( resp . search params [ 'amount' ] , resp . search params [ 'from' ] , resp . search params [ 'amount' ] * conversion rate , resp . search params [ 'to' ] , conversion rate , resp . search params [ 'from name' ] , resp . search params [ 'to name' ] , ) url = 'https://duckduckgo.com/js/spice/currency/1/{0}/{1}' . format ( resp . search params [ 'from' ] . upper ( ) , resp . search params [ 'to' ] ) results . append ( { 'answer' : answer , 'url' : url } ) return results
def mvn ( * args , * * kwargs ) : # Faster than using `tfd.Multivariate Normal Diag`. return tfd . Independent ( tfd . Normal ( * args , * * kwargs ) , reinterpreted batch ndims = 1 )
def eight schools joint log prob ( treatment effects , treatment stddevs , avg effect , avg stddev , school effects standard ) : rv avg effect = tfd . Normal ( loc = 0. , scale = 10. ) rv avg stddev = tfd . Normal ( loc = 5. , scale = 1. ) rv school effects standard = mvn ( loc = tf . zeros like ( school effects standard ) , scale = tf . ones like ( school effects standard ) ) rv treatment effects = mvn ( loc = ( avg effect + tf . exp ( avg stddev ) * school effects standard ) , scale = treatment stddevs ) return ( rv avg effect . log prob ( avg effect ) + rv avg stddev . log prob ( avg stddev ) + rv school effects standard . log prob ( school effects standard ) + rv treatment effects . log prob ( treatment effects ) )
def benchmark eight schools hmc ( num results = int ( 5e3 ) , num burnin steps = int ( 3e3 ) , num leapfrog steps = 3 , step size = 0.4 ) : num schools = 8 treatment effects = tf . constant ( [ 28 , 8 , - 3 , 7 , - 1 , 1 , 18 , 12 ] , dtype = np . float32 , name = 'treatment effects' ) treatment stddevs = tf . constant ( [ 15 , 10 , 16 , 11 , 9 , 11 , 10 , 18 ] , dtype = np . float32 , name = 'treatment stddevs' ) def unnormalized posterior log prob ( avg effect , avg stddev , school effects standard ) : """Eight-schools unnormalized log posterior.""" return eight schools joint log prob ( treatment effects , treatment stddevs , avg effect , avg stddev , school effects standard ) if tf . executing eagerly ( ) : sample chain = tf . function ( tfp . mcmc . sample chain ) else : sample chain = tfp . mcmc . sample chain def computation ( ) : """The benchmark computation.""" , kernel results = sample chain ( num results = num results , num burnin steps = num burnin steps , current state = ( tf . zeros ( [ ] , name = 'init avg effect' ) , tf . zeros ( [ ] , name = 'init avg stddev' ) , tf . ones ( [ num schools ] , name = 'init school effects standard' ) , ) , kernel = tfp . mcmc . Hamiltonian Monte Carlo ( target log prob fn = unnormalized posterior log prob , step size = step size , num leapfrog steps = num leapfrog steps ) ) return kernel results . is accepted # Let's force evaluation of graph to ensure build time is not part of our time # trial. is accepted tensor = computation ( ) if not tf . executing eagerly ( ) : session = tf . compat . v1 . Session ( ) session . run ( is accepted tensor ) start time = time . time ( ) if tf . executing eagerly ( ) : is accepted = computation ( ) else : is accepted = session . run ( is accepted tensor ) wall time = time . time ( ) - start time num accepted = np . sum ( is accepted ) acceptance rate = np . float32 ( num accepted ) / np . float32 ( num results ) return dict ( iters = ( num results + num burnin steps ) * num leapfrog steps , extras = { 'acceptance rate' : acceptance rate } , wall time = wall time )
def build custom rv ( distribution , sample shape , value , name ) : # Program transformations (e.g., `make log joint fn`) assume that # the traced constructor has `name` and `value` kwargs, enabling # them to override the value of an RV according to its name. # User-defined R Vs inherit their name from the provided # distribution; this helper method exposes the name as a dummy kwarg # so that it's visible to program transformations. del name # unused return Random Variable ( distribution = distribution , sample shape = sample shape , value = value )
def make random variable ( distribution cls ) : @ interceptable @ functools . wraps ( distribution cls , assigned = ( ' module ' , ' name ' ) ) @ docstring util . expand docstring ( cls = distribution cls . name , doc = inspect . cleandoc ( distribution cls . init . doc or '' ) ) def func ( * args , * * kwargs ) : # pylint: disable=g-doc-args # pylint: enable=g-doc-args sample shape = kwargs . pop ( 'sample shape' , ( ) ) value = kwargs . pop ( 'value' , None ) return Random Variable ( distribution = distribution cls ( * args , * * kwargs ) , sample shape = sample shape , value = value ) return func
def max mask non finite ( x , axis = - 1 , keepdims = False , mask = 0 ) : m = np . max ( x , axis = astuple ( axis ) , keepdims = keepdims ) needs masking = ~ np . isfinite ( m ) if needs masking . ndim > 0 : m [ needs masking ] = mask elif needs masking : m = mask return m
def eval all one hot ( fn , dist , name = None ) : with tf . compat . v1 . name scope ( name , 'eval all one hot' ) : event size = dist . event shape tensor ( ) [ - 1 ] batch ndims = tf . size ( input = dist . batch shape tensor ( ) ) # Reshape `eye(d)` to: `[d] + [1]*batch ndims + [d]`. x = tf . reshape ( tf . eye ( event size , dtype = dist . dtype ) , shape = tf . pad ( tensor = tf . ones ( batch ndims , tf . int32 ) , paddings = [ [ 1 , 1 ] ] , constant values = event size ) ) # Compute `fn(x)` then cyclically left-transpose one dim. perm = tf . pad ( tensor = tf . range ( 1 , batch ndims + 1 ) , paddings = [ [ 0 , 1 ] ] ) return tf . transpose ( a = fn ( dist , x ) , perm = perm )
def get convert to tensor fn ( identifier ) : if identifier is None : return None if isinstance ( identifier , six . string types ) : identifier = str ( identifier ) return deserialize ( identifier ) if isinstance ( identifier , dict ) : return deserialize ( identifier ) if isinstance ( identifier , property ) : identifier = identifier . fget if callable ( identifier ) : return identifier raise Value Error ( 'Could not interpret ' 'convert-to-tensor function identifier:' , identifier )
def new ( params , event size , validate args = False , name = None ) : with tf . compat . v1 . name scope ( name , 'Multivariate Normal Tri L' , [ params , event size ] ) : params = tf . convert to tensor ( value = params , name = 'params' ) scale tril = tfb . Scale Tri L ( diag shift = np . array ( 1e-5 , params . dtype . as numpy dtype ( ) ) , validate args = validate args ) return tfd . Multivariate Normal Tri L ( loc = params [ ... , : event size ] , scale tril = scale tril ( params [ ... , event size : ] ) , validate args = validate args )
def params size ( event size , name = None ) : with tf . compat . v1 . name scope ( name , 'Multivariate Normal Tri L params size' , [ event size ] ) : return event size + event size * ( event size + 1 ) // 2
def new ( params , event size , dtype = None , validate args = False , name = None ) : with tf . compat . v1 . name scope ( name , 'One Hot Categorical' , [ params , event size ] ) : return tfd . One Hot Categorical ( logits = params , dtype = dtype or params . dtype . base dtype , validate args = validate args )
def new ( params , event size , num components , dtype = None , validate args = False , name = None ) : with tf . compat . v1 . name scope ( name , 'Categorical Mixture Of One Hot Categorical' , [ params , event size , num components ] ) : dist = Mixture Same Family . new ( params , num components , One Hot Categorical ( event size , validate args = False , # So we can eval on simplex interior. name = name ) , validate args = validate args , name = name ) # pylint: disable=protected-access dist . mean = functools . partial ( eval all one hot , tfd . Distribution . prob , dist ) dist . log mean = functools . partial ( eval all one hot , tfd . Distribution . log prob , dist ) # pylint: enable=protected-access return dist
def params size ( event size , num components , name = None ) : with tf . compat . v1 . name scope ( name , 'Categorical Mixture Of One Hot Categorical params size' , [ event size , num components ] ) : return Mixture Same Family . params size ( num components , One Hot Categorical . params size ( event size , name = name ) , name = name )
def new ( params , event shape = ( ) , dtype = None , validate args = False , name = None ) : with tf . compat . v1 . name scope ( name , 'Independent Bernoulli' , [ params , event shape ] ) : params = tf . convert to tensor ( value = params , name = 'params' ) event shape = dist util . expand to vector ( tf . convert to tensor ( value = event shape , name = 'event shape' , dtype hint = tf . int32 ) , tensor name = 'event shape' ) new shape = tf . concat ( [ tf . shape ( input = params ) [ : - 1 ] , event shape , ] , axis = 0 ) dist = tfd . Independent ( tfd . Bernoulli ( logits = tf . reshape ( params , new shape ) , dtype = dtype or params . dtype . base dtype , validate args = validate args ) , reinterpreted batch ndims = tf . size ( input = event shape ) , validate args = validate args ) dist . logits = dist . distribution . logits # pylint: disable=protected-access dist . probs = dist . distribution . probs # pylint: disable=protected-access dist . logits = tfd . Bernoulli . logits dist . probs = tfd . Bernoulli . probs return dist
def new ( params , event shape = ( ) , validate args = False , name = None ) : with tf . compat . v1 . name scope ( name , 'Independent Logistic' , [ params , event shape ] ) : params = tf . convert to tensor ( value = params , name = 'params' ) event shape = dist util . expand to vector ( tf . convert to tensor ( value = event shape , name = 'event shape' , dtype hint = tf . int32 ) , tensor name = 'event shape' ) output shape = tf . concat ( [ tf . shape ( input = params ) [ : - 1 ] , event shape , ] , axis = 0 ) loc params , scale params = tf . split ( params , 2 , axis = - 1 ) return tfd . Independent ( tfd . Logistic ( loc = tf . reshape ( loc params , output shape ) , scale = tf . math . softplus ( tf . reshape ( scale params , output shape ) ) , validate args = validate args ) , reinterpreted batch ndims = tf . size ( input = event shape ) , validate args = validate args )
def params size ( event shape = ( ) , name = None ) : with tf . compat . v1 . name scope ( name , 'Independent Normal params size' , [ event shape ] ) : event shape = tf . convert to tensor ( value = event shape , name = 'event shape' , dtype hint = tf . int32 ) return 2 * event size ( event shape , name = name or 'Independent Normal params size' )
def new ( params , event shape = ( ) , validate args = False , name = None ) : with tf . compat . v1 . name scope ( name , 'Independent Poisson' , [ params , event shape ] ) : params = tf . convert to tensor ( value = params , name = 'params' ) event shape = dist util . expand to vector ( tf . convert to tensor ( value = event shape , name = 'event shape' , dtype hint = tf . int32 ) , tensor name = 'event shape' ) output shape = tf . concat ( [ tf . shape ( input = params ) [ : - 1 ] , event shape , ] , axis = 0 ) return tfd . Independent ( tfd . Poisson ( log rate = tf . reshape ( params , output shape ) , validate args = validate args ) , reinterpreted batch ndims = tf . size ( input = event shape ) , validate args = validate args )
def new ( params , num components , component layer , validate args = False , name = None ) : with tf . compat . v1 . name scope ( name , 'Mixture Same Family' , [ params , num components , component layer ] ) : params = tf . convert to tensor ( value = params , name = 'params' ) num components = tf . convert to tensor ( value = num components , name = 'num components' , dtype hint = tf . int32 ) components dist = component layer ( tf . reshape ( params [ ... , num components : ] , tf . concat ( [ tf . shape ( input = params ) [ : - 1 ] , [ num components , - 1 ] ] , axis = 0 ) ) ) mixture dist = tfd . Categorical ( logits = params [ ... , : num components ] ) return tfd . Mixture Same Family ( mixture dist , components dist , # TODO(b/120154797): Change following to `validate args=True` after # fixing: "Value Error: `mixture distribution` must have scalar # `event dim`s." assertion in Mixture Same Family. validate args = False )
def params size ( num components , event shape = ( ) , name = None ) : return Mixture Same Family . params size ( num components , Independent Normal . params size ( event shape , name = name ) , name = name )
def new ( params , num components , event shape = ( ) , validate args = False , name = None ) : return Mixture Same Family . new ( params , num components , Independent Logistic ( event shape , validate args = validate args , name = name ) , validate args = validate args , name = name )
def params size ( num components , event shape = ( ) , name = None ) : return Mixture Same Family . params size ( num components , Independent Logistic . params size ( event shape , name = name ) , name = name )
def maybe check valid map values ( map values , validate args ) : assertions = [ ] message = 'Rank of map values must be 1.' if tensorshape util . rank ( map values . shape ) is not None : if tensorshape util . rank ( map values . shape ) != 1 : raise Value Error ( message ) elif validate args : assertions . append ( assert util . assert rank ( map values , 1 , message = message ) ) message = 'Size of map values must be greater than 0.' if tensorshape util . num elements ( map values . shape ) is not None : if tensorshape util . num elements ( map values . shape ) == 0 : raise Value Error ( message ) elif validate args : assertions . append ( assert util . assert greater ( tf . size ( input = map values ) , 0 , message = message ) ) if validate args : assertions . append ( assert util . assert equal ( tf . math . is strictly increasing ( map values ) , True , message = 'map values is not strictly increasing.' ) ) return assertions
def as tensor ( x , name , dtype ) : return None if x is None else tf . convert to tensor ( value = x , name = name , dtype = dtype )
def get default reinterpreted batch ndims ( self , distribution ) : ndims = prefer static . rank from shape ( distribution . batch shape tensor , distribution . batch shape ) return prefer static . maximum ( 0 , ndims - 1 )
def cat probs ( self , log probs ) : which softmax = tf . nn . log softmax if log probs else tf . nn . softmax cat probs = which softmax ( self . cat . logits ) cat probs = tf . unstack ( cat probs , num = self . num components , axis = - 1 ) return cat probs
def maybe validate args ( outcomes , logits , probs , validate args ) : assertions = [ ] def validate equal last dim ( tensor a , tensor b , message ) : if tensor a . shape . is fully defined ( ) and tensor b . shape . is fully defined ( ) : if tensor a . shape [ - 1 ] != tensor b . shape [ - 1 ] : raise Value Error ( message ) elif validate args : assertions . append ( tf . compat . v1 . assert equal ( tf . shape ( input = tensor a ) [ - 1 ] , tf . shape ( input = tensor b ) [ - 1 ] , message = message ) ) if logits is not None : validate equal last dim ( outcomes , logits , message = 'Last dimension of outcomes and logits must be equal size.' ) if probs is not None : validate equal last dim ( outcomes , probs , message = 'Last dimension of outcomes and probs must be equal size.' ) message = 'Rank of outcomes must be 1.' if outcomes . shape . ndims is not None : if outcomes . shape . ndims != 1 : raise Value Error ( message ) elif validate args : assertions . append ( tf . compat . v1 . assert rank ( outcomes , 1 , message = message ) ) message = 'Size of outcomes must be greater than 0.' if outcomes . shape . num elements ( ) is not None : if outcomes . shape . num elements ( ) == 0 : raise Value Error ( message ) elif validate args : assertions . append ( tf . compat . v1 . assert greater ( tf . size ( input = outcomes ) , 0 , message = message ) ) if validate args : assertions . append ( tf . compat . v1 . assert equal ( tf . math . is strictly increasing ( outcomes ) , True , message = 'outcomes is not strictly increasing.' ) ) return assertions
def logistic regression ( features ) : coeffs = ed . Multivariate Normal Diag ( loc = tf . zeros ( features . shape [ 1 ] ) , name = "coeffs" ) labels = ed . Bernoulli ( logits = tf . tensordot ( features , coeffs , [ [ 1 ] , [ 0 ] ] ) , name = "labels" ) return labels
def covertype ( ) : import sklearn . datasets # pylint: disable=g-import-not-at-top data = sklearn . datasets . covtype . fetch covtype ( ) features = data . data labels = data . target # Normalize features and append a column of ones for the intercept. features -= features . mean ( 0 ) features /= features . std ( 0 ) features = np . hstack ( [ features , np . ones ( [ features . shape [ 0 ] , 1 ] ) ] ) features = tf . cast ( features , dtype = tf . float32 ) # Binarize outcomes on whether it is a specific category. , counts = np . unique ( labels , return counts = True ) specific category = np . argmax ( counts ) labels = ( labels == specific category ) labels = tf . cast ( labels , dtype = tf . int32 ) return features , labels
def maybe assert valid concentration ( self , concentration , validate args ) : if not validate args : return concentration return distribution util . with dependencies ( [ assert util . assert positive ( concentration , message = "Concentration parameter must be positive." ) , assert util . assert rank at least ( concentration , 1 , message = "Concentration parameter must have >=1 dimensions." ) , assert util . assert less ( 1 , tf . shape ( input = concentration ) [ - 1 ] , message = "Concentration parameter must have event size >= 2." ) , ] , concentration )
def maybe assert valid sample ( self , x ) : if not self . validate args : return x return distribution util . with dependencies ( [ assert util . assert positive ( x , message = "samples must be positive" ) , assert util . assert near ( tf . ones ( [ ] , dtype = self . dtype ) , tf . reduce sum ( input tensor = x , axis = - 1 ) , message = "sample last-dimension must sum to `1`" ) , ] , x )
def make positive axis ( axis , ndims ) : axis = make list or 1d tensor ( axis ) ndims = tf . convert to tensor ( value = ndims , name = 'ndims' , dtype = tf . int32 ) ndims = tf . get static value ( ndims ) if is list like ( axis ) and ndims is not None : # Static case positive axis = [ ] for a in axis : if a < 0 : a = ndims + a positive axis . append ( a ) else : # Dynamic case axis = tf . convert to tensor ( value = axis , name = 'axis' , dtype = tf . int32 ) positive axis = tf . where ( axis >= 0 , axis , axis + ndims ) return positive axis
def squeeze ( x , axis ) : x = tf . convert to tensor ( value = x , name = 'x' ) if axis is None : return tf . squeeze ( x , axis = None ) axis = tf . convert to tensor ( value = axis , name = 'axis' , dtype = tf . int32 ) axis += tf . zeros ( [ 1 ] , dtype = axis . dtype ) # Make axis at least 1d. keep axis , = tf . compat . v1 . setdiff1d ( tf . range ( 0 , tf . rank ( x ) ) , axis ) return tf . reshape ( x , tf . gather ( tf . shape ( input = x ) , keep axis ) )
def z ( self , x ) : with tf . name scope ( "standardize" ) : return ( x - self . loc ) / self . scale
def inv z ( self , z ) : with tf . name scope ( "reconstruct" ) : return z * self . scale + self . loc
def semilocal linear trend transition matrix ( autoregressive coef ) : # We want to write the following 2 x 2 matrix: #  [[1., 1., ],    # level(t+1) = level(t) + slope(t) #   [0., ar coef], # slope(t+1) = ar coef * slope(t) # but it's slightly tricky to properly incorporate the batch shape of # autoregressive coef. E.g., if autoregressive coef has shape [4,6], we want # to return shape [4, 6, 2, 2]. We do this by breaking the matrix into its # fixed entries, written explicitly, and then the autoregressive coef part # which we add in after using a mask to broadcast to the correct matrix shape. fixed entries = tf . constant ( [ [ 1. , 1. ] , [ 0. , 0. ] ] , dtype = autoregressive coef . dtype ) autoregressive coef mask = tf . constant ( [ [ 0. , 0. ] , [ 0. , 1. ] ] , dtype = autoregressive coef . dtype ) bottom right entry = ( autoregressive coef [ ... , tf . newaxis , tf . newaxis ] * autoregressive coef mask ) return tf . linalg . Linear Operator Full Matrix ( fixed entries + bottom right entry )
def semilocal linear trend transition noise ( level scale , slope mean , slope scale , autoregressive coef ) : # At each timestep, the stochasticity of `level` and `slope` are given # by `level scale` and `slope scale` respectively. broadcast batch shape = dist util . get broadcast shape ( level scale , slope mean , slope scale , autoregressive coef ) broadcast ones = tf . ones ( broadcast batch shape , dtype = level scale . dtype ) scale diag = tf . stack ( [ level scale * broadcast ones , slope scale * broadcast ones ] , axis = - 1 ) # We additionally fold in a bias term implementing the nonzero `slope mean`. # The overall `slope` update is (from `Semi Local Linear Trend` docstring) #   slope[t] = (slope mean + #               autoregressive coef * (slope[t-1] - slope mean) + #               Normal(0., slope scale)) # which we rewrite as #   slope[t] = ( #    autoregressive coef * slope[t-1] +                  # linear transition #    Normal(loc=slope mean - autoregressive coef * slope mean,  # noise bias #           scale=slope scale))                                 # noise scale bias = tf . stack ( [ tf . zeros like ( broadcast ones ) , slope mean * ( 1 - autoregressive coef ) * broadcast ones ] , axis = - 1 ) return tfd . Multivariate Normal Diag ( loc = bias , scale diag = scale diag )
def machine eps ( dtype ) : if isinstance ( dtype , tf . D Type ) : dtype = dtype . as numpy dtype ( ) return np . finfo ( dtype ) . eps
def fix step size ( value and gradients function , val c input , active , step size shrink param ) : # The maximum iterations permitted are determined as the number of halvings # it takes to reduce 1 to 0 in the given dtype. iter max = np . ceil ( - np . log2 ( machine eps ( val c input . x . dtype ) ) ) def cond ( i , val c , to fix ) : del val c # Unused. return ( i < iter max ) & tf . reduce any ( input tensor = to fix ) def body ( i , val c , to fix ) : next c = tf . where ( to fix , val c . x * step size shrink param , val c . x ) next val c = value and gradients function ( next c ) still to fix = to fix & ~ hzl . is finite ( next val c ) return ( i + 1 , next val c , still to fix ) to fix = active & ~ hzl . is finite ( val c input ) return tf . while loop ( cond = cond , body = body , loop vars = ( 0 , val c input , to fix ) )
def line search inner bisection ( value and gradients function , search interval , active , f lim ) : midpoint = ( search interval . left . x + search interval . right . x ) / 2 val mid = value and gradients function ( midpoint ) is valid mid = hzl . is finite ( val mid ) still active = active & is valid mid new failed = active & ~ is valid mid next inteval = search interval . replace ( failed = search interval . failed | new failed , func evals = search interval . func evals + 1 ) def apply update ( ) : update result = hzl . update ( value and gradients function , next inteval . left , next inteval . right , val mid , f lim , active = still active ) return Hager Zhang Line Search Result ( converged = next inteval . converged , failed = next inteval . failed | update result . failed , iterations = next inteval . iterations + update result . iteration , func evals = next inteval . func evals + update result . num evals , left = update result . left , right = update result . right ) return prefer static . cond ( tf . reduce any ( input tensor = still active ) , apply update , lambda : next inteval )
def print ( pass through tensor , values ) : flat values = [ ] for value in values : # Checks if it is a namedtuple. if hasattr ( value , ' fields' ) : for field in value . fields : flat values . extend ( [ field , to str ( getattr ( value , field ) ) ] ) continue if isinstance ( value , ( list , tuple ) ) : for v in value : flat values . append ( to str ( v ) ) continue flat values . append ( to str ( value ) ) return tf . compat . v1 . Print ( pass through tensor , flat values )
def maybe check quadrature param ( param , name , validate args ) : with tf . name scope ( "check " + name ) : assertions = [ ] if tensorshape util . rank ( param . shape ) is not None : if tensorshape util . rank ( param . shape ) == 0 : raise Value Error ( "Mixing params must be a (batch of) vector; " "{}.rank={} is not at least one." . format ( name , tensorshape util . rank ( param . shape ) ) ) elif validate args : assertions . append ( assert util . assert rank at least ( param , 1 , message = ( "Mixing params must be a (batch of) vector; " "{}.rank is not at least one." . format ( name ) ) ) ) # TODO(jvdillon): Remove once we support k-mixtures. if tensorshape util . with rank at least ( param . shape , 1 ) [ - 1 ] is not None : if tf . compat . dimension value ( param . shape [ - 1 ] ) != 1 : raise Not Implemented Error ( "Currently only bimixtures are supported; " "{}.shape[-1]={} is not 1." . format ( name , tf . compat . dimension value ( param . shape [ - 1 ] ) ) ) elif validate args : assertions . append ( assert util . assert equal ( tf . shape ( input = param ) [ - 1 ] , 1 , message = ( "Currently only bimixtures are supported; " "{}.shape[-1] is not 1." . format ( name ) ) ) ) if assertions : return distribution util . with dependencies ( assertions , param ) return param
def determine batch event shapes ( grid , endpoint affine ) : with tf . name scope ( "determine batch event shapes" ) : # grid  # shape: [B, k, q] # endpoint affine     # len=k, shape: [B, d, d] batch shape = grid . shape [ : - 2 ] batch shape tensor = tf . shape ( input = grid ) [ : - 2 ] event shape = None event shape tensor = None def set event shape ( shape , shape tensor ) : if event shape is None : return shape , shape tensor return ( tf . broadcast static shape ( event shape , shape ) , tf . broadcast dynamic shape ( event shape tensor , shape tensor ) ) for aff in endpoint affine : if aff . shift is not None : batch shape = tf . broadcast static shape ( batch shape , aff . shift . shape [ : - 1 ] ) batch shape tensor = tf . broadcast dynamic shape ( batch shape tensor , tf . shape ( input = aff . shift ) [ : - 1 ] ) event shape , event shape tensor = set event shape ( aff . shift . shape [ - 1 : ] , tf . shape ( input = aff . shift ) [ - 1 : ] ) if aff . scale is not None : batch shape = tf . broadcast static shape ( batch shape , aff . scale . batch shape ) batch shape tensor = tf . broadcast dynamic shape ( batch shape tensor , aff . scale . batch shape tensor ( ) ) event shape , event shape tensor = set event shape ( tf . Tensor Shape ( [ aff . scale . range dimension ] ) , aff . scale . range dimension tensor ( ) [ tf . newaxis ] ) return batch shape , batch shape tensor , event shape , event shape tensor
def interpolate loc ( grid , loc ) : if len ( loc ) != 2 : raise Not Implemented Error ( "Currently only bimixtures are supported; " "len(scale)={} is not 2." . format ( len ( loc ) ) ) deg = tf . compat . dimension value ( tensorshape util . with rank at least ( grid . shape , 1 ) [ - 1 ] ) if deg is None : raise Value Error ( "Num quadrature grid points must be known prior " "to graph execution." ) with tf . name scope ( "interpolate loc" ) : if loc is None or loc [ 0 ] is None and loc [ 1 ] is None : return [ None ] * deg # shape: [B, 1, k, deg] w = grid [ ... , tf . newaxis , : , : ] loc = [ x [ ... , tf . newaxis ] # shape: [B, e, 1] if x is not None else None for x in loc ] if loc [ 0 ] is None : x = w [ ... , 1 , : ] * loc [ 1 ] # shape: [B, e, deg] elif loc [ 1 ] is None : x = w [ ... , 0 , : ] * loc [ 0 ] # shape: [B, e, deg] else : delta = loc [ 0 ] - loc [ 1 ] x = w [ ... , 0 , : ] * delta + loc [ 1 ] # shape: [B, e, deg] return [ x [ ... , k ] for k in range ( deg ) ]
def interpolate scale ( grid , scale ) : if len ( scale ) != 2 : raise Not Implemented Error ( "Currently only bimixtures are supported; " "len(scale)={} is not 2." . format ( len ( scale ) ) ) deg = tf . compat . dimension value ( tensorshape util . with rank at least ( grid . shape , 1 ) [ - 1 ] ) if deg is None : raise Value Error ( "Num quadrature grid points must be known prior " "to graph execution." ) with tf . name scope ( "interpolate scale" ) : return [ linop add lib . add operators ( [ linop scale ( grid [ ... , k , q ] , s ) for k , s in enumerate ( scale ) ] ) [ 0 ] for q in range ( deg ) ]
def linop scale ( w , op ) : # We assume w > 0. (This assumption only relates to the is * attributes.) with tf . name scope ( "linop scale" ) : # TODO(b/35301104): Linear Operator Composition doesn't combine operators, so # special case combinations here. Once it does, this function can be # replaced by: #     return linop composition lib.Linear Operator Composition([ #         scaled identity(w), op]) def scaled identity ( w ) : return tf . linalg . Linear Operator Scaled Identity ( num rows = op . range dimension tensor ( ) , multiplier = w , is non singular = op . is non singular , is self adjoint = op . is self adjoint , is positive definite = op . is positive definite ) if isinstance ( op , tf . linalg . Linear Operator Identity ) : return scaled identity ( w ) if isinstance ( op , tf . linalg . Linear Operator Scaled Identity ) : return scaled identity ( w * op . multiplier ) if isinstance ( op , tf . linalg . Linear Operator Diag ) : return tf . linalg . Linear Operator Diag ( diag = w [ ... , tf . newaxis ] * op . diag part ( ) , is non singular = op . is non singular , is self adjoint = op . is self adjoint , is positive definite = op . is positive definite ) if isinstance ( op , tf . linalg . Linear Operator Lower Triangular ) : return tf . linalg . Linear Operator Lower Triangular ( tril = w [ ... , tf . newaxis , tf . newaxis ] * op . to dense ( ) , is non singular = op . is non singular , is self adjoint = op . is self adjoint , is positive definite = op . is positive definite ) raise Not Implemented Error ( "Unsupported Linop type ({})" . format ( type ( op ) . name ) )
def concat vectors ( * args ) : args = [ tf . get static value ( x ) for x in args ] if any ( vec is None for vec in args ) : return tf . concat ( args , axis = 0 ) return [ val for vec in args for val in vec ]
def log vector matrix ( vs , ms ) : return tf . reduce logsumexp ( input tensor = vs [ ... , tf . newaxis ] + ms , axis = - 2 )
def log matrix vector ( ms , vs ) : return tf . reduce logsumexp ( input tensor = ms + vs [ ... , tf . newaxis , : ] , axis = - 1 )
def vector matrix ( vs , ms ) : return tf . reduce sum ( input tensor = vs [ ... , tf . newaxis ] * ms , axis = - 2 )
def extract log probs ( num states , dist ) : states = tf . reshape ( tf . range ( num states ) , tf . concat ( [ [ num states ] , tf . ones like ( dist . batch shape tensor ( ) ) ] , axis = 0 ) ) return distribution util . move dimension ( dist . log prob ( states ) , 0 , - 1 )
def marginal hidden probs ( self ) : initial log probs = tf . broadcast to ( self . log init , tf . concat ( [ self . batch shape tensor ( ) , [ self . num states ] ] , axis = 0 ) ) # initial log probs :: batch shape num states if self . num steps > 1 : transition log probs = self . log trans def forward step ( log probs , ) : return log vector matrix ( log probs , transition log probs ) dummy index = tf . zeros ( self . num steps - 1 , dtype = tf . float32 ) forward log probs = tf . scan ( forward step , dummy index , initializer = initial log probs , name = "forward log probs" ) forward log probs = tf . concat ( [ [ initial log probs ] , forward log probs ] , axis = 0 ) else : forward log probs = initial log probs [ tf . newaxis , ... ] # returns :: num steps batch shape num states return tf . exp ( forward log probs )
def choose random direction ( current state parts , batch rank , seed = None ) : seed gen = distributions . Seed Stream ( seed , salt = ' choose random direction' ) # Chooses the random directions across each of the input components. rnd direction parts = [ tf . random . normal ( tf . shape ( input = current state part ) , dtype = tf . float32 , seed = seed gen ( ) ) for current state part in current state parts ] # Sum squares over all of the input components. Note this takes all # components into account. sum squares = sum ( tf . reduce sum ( input tensor = rnd direction ** 2. , axis = tf . range ( batch rank , tf . rank ( rnd direction ) ) , keepdims = True ) for rnd direction in rnd direction parts ) # Normalizes the random direction fragments. rnd direction parts = [ rnd direction / tf . sqrt ( sum squares ) for rnd direction in rnd direction parts ] return rnd direction parts
def maybe call fn ( fn , fn arg list , fn result = None , description = 'target log prob' ) : fn arg list = ( list ( fn arg list ) if mcmc util . is list like ( fn arg list ) else [ fn arg list ] ) if fn result is None : fn result = fn ( * fn arg list ) if not fn result . dtype . is floating : raise Type Error ( '`{}` must be a `Tensor` with `float` `dtype`.' . format ( description ) ) return fn result
def prepare args ( target log prob fn , state , step size , target log prob = None , maybe expand = False , description = 'target log prob' ) : state parts = list ( state ) if mcmc util . is list like ( state ) else [ state ] state parts = [ tf . convert to tensor ( value = s , name = 'current state' ) for s in state parts ] target log prob = maybe call fn ( target log prob fn , state parts , target log prob , description ) step sizes = ( list ( step size ) if mcmc util . is list like ( step size ) else [ step size ] ) step sizes = [ tf . convert to tensor ( value = s , name = 'step size' , dtype = target log prob . dtype ) for s in step sizes ] if len ( step sizes ) == 1 : step sizes *= len ( state parts ) if len ( state parts ) != len ( step sizes ) : raise Value Error ( 'There should be exactly one `step size` or it should ' 'have same length as `current state`.' ) def maybe flatten ( x ) : return x if maybe expand or mcmc util . is list like ( state ) else x [ 0 ] return [ maybe flatten ( state parts ) , maybe flatten ( step sizes ) , target log prob ]
def build trainable posterior ( param , initial loc fn ) : loc = tf . compat . v1 . get variable ( param . name + ' loc' , initializer = lambda : initial loc fn ( param ) , dtype = param . prior . dtype , use resource = True ) scale = tf . nn . softplus ( tf . compat . v1 . get variable ( param . name + ' scale' , initializer = lambda : - 4 * tf . ones like ( initial loc fn ( param ) ) , dtype = param . prior . dtype , use resource = True ) ) q = tfd . Normal ( loc = loc , scale = scale ) # Ensure the `event shape` of the variational distribution matches the # parameter. if ( param . prior . event shape . ndims is None or param . prior . event shape . ndims > 0 ) : q = tfd . Independent ( q , reinterpreted batch ndims = param . prior . event shape . ndims ) # Transform to constrained parameter space. return tfd . Transformed Distribution ( q , param . bijector )
def minimize in graph ( build loss fn , num steps = 200 , optimizer = None ) : optimizer = tf . compat . v1 . train . Adam Optimizer ( 0.1 ) if optimizer is None else optimizer def train loop body ( step ) : train op = optimizer . minimize ( build loss fn if tf . executing eagerly ( ) else build loss fn ( ) ) return tf . tuple ( tensors = [ tf . add ( step , 1 ) ] , control inputs = [ train op ] ) minimize op = tf . compat . v1 . while loop ( cond = lambda step : step < num steps , body = train loop body , loop vars = [ tf . constant ( 0 ) ] , return same structure = True ) [ 0 ] # Always return a single op. return minimize op
def broadcast batch shape ( distributions ) : # Static case batch shape = distributions [ 0 ] . batch shape for distribution in distributions : batch shape = tf . broadcast static shape ( batch shape , distribution . batch shape ) if batch shape . is fully defined ( ) : return batch shape . as list ( ) # Fallback on dynamic. batch shape = distributions [ 0 ] . batch shape tensor ( ) for distribution in distributions : batch shape = tf . broadcast dynamic shape ( batch shape , distribution . batch shape tensor ( ) ) return tf . convert to tensor ( value = batch shape )
def range ( self , name = "range" ) : with self . name scope ( name ) : return self . high - self . low
def make summary statistic ( attr ) : def fn ( self ) : if any ( self . dist fn args ) : # pylint: disable=protected-access raise Value Error ( 'Can only compute ' + attr + ' when all distributions are ' 'independent; {}' . format ( self . model ) ) return self . unflatten ( getattr ( d ( ) , attr ) ( ) for d in self . dist fn wrapped ) # pylint: disable=protected-access return fn
def resolve distribution names ( dist fn args , dist names , leaf name ) : if dist names is None : dist names = [ ] else : dist names = dist names . copy ( ) n = len ( dist fn args ) dist names . extend ( [ None ] * ( n - len ( dist names ) ) ) for i , args in enumerate ( reversed ( dist fn args ) ) : if not args : continue # There's no args to analyze. i = n - i - 1 for j , arg name in enumerate ( args ) : dist names [ i - j - 1 ] = arg name j = 0 for i in range ( len ( dist names ) ) : i = n - i - 1 if dist names [ i ] is None : dist names [ i ] = leaf name if j == 0 else leaf name + str ( j ) j += 1 return tuple ( dist names )
def get required args ( fn ) : argspec = tf inspect . getfullargspec ( fn ) args = argspec . args if tf inspect . isclass ( fn ) : args = args [ 1 : ] # Remove the `self` arg. if argspec . defaults : # Remove the args which have defaults. By convention we only feed # *required args*. This means some distributions must always be wrapped # with a `lambda`, e.g., `lambda logits: tfd.Bernoulli(logits=logits)` # or `lambda probs: tfd.Bernoulli(probs=probs)`. args = args [ : - len ( argspec . defaults ) ] return tuple ( args )
def build ( self , model ) : if not isinstance ( model , collections . Sequence ) : raise Type Error ( '`model` must be `list`-like (saw: {}).' . format ( type ( model ) . name ) ) self . dist fn = model self . dist fn wrapped , self . dist fn args = zip ( * [ unify call signature ( i , dist fn ) for i , dist fn in enumerate ( model ) ] )
def entropy ( self ) : if any ( self . dist fn args ) : raise Value Error ( 'Can only compute entropy when all distributions are independent.' ) return sum ( joint distribution lib . maybe check wont broadcast ( ( d ( ) . entropy ( ) for d in self . dist fn wrapped ) , self . validate args ) )
def prepare args ( log likelihood fn , state , log likelihood = None , description = 'log likelihood' ) : state parts = list ( state ) if mcmc util . is list like ( state ) else [ state ] state parts = [ tf . convert to tensor ( s , name = 'current state' ) for s in state parts ] log likelihood = maybe call fn ( log likelihood fn , state parts , log likelihood , description ) return [ state parts , log likelihood ]
def vector size to square matrix size ( d , validate args , name = None ) : if isinstance ( d , ( float , int , np . generic , np . ndarray ) ) : n = ( - 1 + np . sqrt ( 1 + 8 * d ) ) / 2. if float ( int ( n ) ) != n : raise Value Error ( "Vector length is not a triangular number." ) return int ( n ) else : with tf . name scope ( name or "vector size to square matrix size" ) as name : n = ( - 1. + tf . sqrt ( 1 + 8. * tf . cast ( d , dtype = tf . float32 ) ) ) / 2. if validate args : with tf . control dependencies ( [ assert util . assert equal ( tf . cast ( tf . cast ( n , dtype = tf . int32 ) , dtype = tf . float32 ) , n , message = "Vector length is not a triangular number" ) ] ) : n = tf . identity ( n ) return tf . cast ( n , d . dtype )
def argsort ( values , axis = - 1 , direction = 'ASCENDING' , stable = False , name = None ) : # pylint: disable=unused-argument if direction == 'ASCENDING' : pass elif direction == 'DESCENDING' : values = np . negative ( values ) else : raise Value Error ( 'Unrecognized direction: {}.' . format ( direction ) ) return np . argsort ( values , axis , kind = 'stable' if stable else 'quicksort' )
def sort ( values , axis = - 1 , direction = 'ASCENDING' , stable = False , name = None ) : # pylint: disable=unused-argument if direction == 'ASCENDING' : pass elif direction == 'DESCENDING' : values = np . negative ( values ) else : raise Value Error ( 'Unrecognized direction: {}.' . format ( direction ) ) result = np . sort ( values , axis , kind = 'stable' if stable else 'quicksort' ) if direction == 'DESCENDING' : return np . negative ( result ) return result
def ndtr ( x ) : half sqrt 2 = tf . constant ( 0.5 * np . sqrt ( 2. ) , dtype = x . dtype , name = "half sqrt 2" ) w = x * half sqrt 2 z = tf . abs ( w ) y = tf . where ( tf . less ( z , half sqrt 2 ) , 1. + tf . math . erf ( w ) , tf . where ( tf . greater ( w , 0. ) , 2. - tf . math . erfc ( z ) , tf . math . erfc ( z ) ) ) return 0.5 * y
def ndtri ( p ) : # Constants used in piece-wise rational approximations. Taken from the cephes # library: # https://root.cern.ch/doc/v608/Spec Func Cephes Inv 8cxx source.html p0 = list ( reversed ( [ - 5.99633501014107895267E1 , 9.80010754185999661536E1 , - 5.66762857469070293439E1 , 1.39312609387279679503E1 , - 1.23916583867381258016E0 ] ) ) q0 = list ( reversed ( [ 1.0 , 1.95448858338141759834E0 , 4.67627912898881538453E0 , 8.63602421390890590575E1 , - 2.25462687854119370527E2 , 2.00260212380060660359E2 , - 8.20372256168333339912E1 , 1.59056225126211695515E1 , - 1.18331621121330003142E0 ] ) ) p1 = list ( reversed ( [ 4.05544892305962419923E0 , 3.15251094599893866154E1 , 5.71628192246421288162E1 , 4.40805073893200834700E1 , 1.46849561928858024014E1 , 2.18663306850790267539E0 , - 1.40256079171354495875E-1 , - 3.50424626827848203418E-2 , - 8.57456785154685413611E-4 ] ) ) q1 = list ( reversed ( [ 1.0 , 1.57799883256466749731E1 , 4.53907635128879210584E1 , 4.13172038254672030440E1 , 1.50425385692907503408E1 , 2.50464946208309415979E0 , - 1.42182922854787788574E-1 , - 3.80806407691578277194E-2 , - 9.33259480895457427372E-4 ] ) ) p2 = list ( reversed ( [ 3.23774891776946035970E0 , 6.91522889068984211695E0 , 3.93881025292474443415E0 , 1.33303460815807542389E0 , 2.01485389549179081538E-1 , 1.23716634817820021358E-2 , 3.01581553508235416007E-4 , 2.65806974686737550832E-6 , 6.23974539184983293730E-9 ] ) ) q2 = list ( reversed ( [ 1.0 , 6.02427039364742014255E0 , 3.67983563856160859403E0 , 1.37702099489081330271E0 , 2.16236993594496635890E-1 , 1.34204006088543189037E-2 , 3.28014464682127739104E-4 , 2.89247864745380683936E-6 , 6.79019408009981274425E-9 ] ) ) def create polynomial ( var , coeffs ) : """Compute n th order polynomial via Horner's method.""" coeffs = np . array ( coeffs , dtype util . as numpy dtype ( var . dtype ) ) if not coeffs . size : return tf . zeros like ( var ) return coeffs [ 0 ] + create polynomial ( var , coeffs [ 1 : ] ) * var maybe complement p = tf . where ( p > - np . expm1 ( - 2. ) , 1. - p , p ) # Write in an arbitrary value in place of 0 for p since 0 will cause Na Ns # later on. The result from the computation when p == 0 is not used so any # number that doesn't result in Na Ns is fine. sanitized mcp = tf . where ( maybe complement p <= 0. , tf . fill ( tf . shape ( input = p ) , dtype util . as numpy dtype ( p . dtype ) ( 0.5 ) ) , maybe complement p ) # Compute x for p > exp(-2): x/sqrt(2pi) = w + w**3 P0(w**2)/Q0(w**2). w = sanitized mcp - 0.5 ww = w ** 2 x for big p = w + w * ww * ( create polynomial ( ww , p0 ) / create polynomial ( ww , q0 ) ) x for big p *= - np . sqrt ( 2. * np . pi ) # Compute x for p <= exp(-2): x = z - log(z)/z - (1/z) P(1/z) / Q(1/z), # where z = sqrt(-2. * log(p)), and P/Q are chosen between two different # arrays based on whether p < exp(-32). z = tf . sqrt ( - 2. * tf . math . log ( sanitized mcp ) ) first term = z - tf . math . log ( z ) / z second term small p = ( create polynomial ( 1. / z , p2 ) / create polynomial ( 1. / z , q2 ) / z ) second term otherwise = ( create polynomial ( 1. / z , p1 ) / create polynomial ( 1. / z , q1 ) / z ) x for small p = first term - second term small p x otherwise = first term - second term otherwise x = tf . where ( sanitized mcp > np . exp ( - 2. ) , x for big p , tf . where ( z >= 8.0 , x for small p , x otherwise ) ) x = tf . where ( p > 1. - np . exp ( - 2. ) , x , - x ) infinity scalar = tf . constant ( np . inf , dtype = p . dtype ) infinity = tf . fill ( tf . shape ( input = p ) , infinity scalar ) x nan replaced = tf . where ( p <= 0.0 , - infinity , tf . where ( p >= 1.0 , infinity , x ) ) return x nan replaced
def log ndtr asymptotic series ( x , series order ) : npdt = dtype util . as numpy dtype ( x . dtype ) if series order <= 0 : return npdt ( 1 ) x 2 = tf . square ( x ) even sum = tf . zeros like ( x ) odd sum = tf . zeros like ( x ) x 2n = x 2 # Start with x^{2*1} = x^{2*n} with n = 1. for n in range ( 1 , series order + 1 ) : y = npdt ( double factorial ( 2 * n - 1 ) ) / x 2n if n % 2 : odd sum += y else : even sum += y x 2n *= x 2 return 1. + even sum - odd sum
def text messages joint log prob ( count data , lambda 1 , lambda 2 , tau ) : alpha = ( 1. / tf . reduce mean ( input tensor = count data ) ) rv lambda = tfd . Exponential ( rate = alpha ) rv tau = tfd . Uniform ( ) lambda = tf . gather ( [ lambda 1 , lambda 2 ] , indices = tf . cast ( tau * tf . cast ( tf . size ( input = count data ) , dtype = tf . float32 ) <= tf . cast ( tf . range ( tf . size ( input = count data ) ) , dtype = tf . float32 ) , dtype = tf . int32 ) ) rv observation = tfd . Poisson ( rate = lambda ) return ( rv lambda . log prob ( lambda 1 ) + rv lambda . log prob ( lambda 2 ) + rv tau . log prob ( tau ) + tf . reduce sum ( input tensor = rv observation . log prob ( count data ) ) )
def benchmark text messages hmc ( num results = int ( 3e3 ) , num burnin steps = int ( 3e3 ) , num leapfrog steps = 3 ) : if not tf . executing eagerly ( ) : tf . compat . v1 . reset default graph ( ) # Build a static, pretend dataset. count data = tf . cast ( tf . concat ( [ tfd . Poisson ( rate = 15. ) . sample ( 43 ) , tfd . Poisson ( rate = 25. ) . sample ( 31 ) ] , axis = 0 ) , dtype = tf . float32 ) if tf . executing eagerly ( ) : count data = count data . numpy ( ) else : with tf . compat . v1 . Session ( ) : count data = count data . eval ( ) # Define a closure over our joint log prob. def unnormalized log posterior ( lambda1 , lambda2 , tau ) : return text messages joint log prob ( count data , lambda1 , lambda2 , tau ) if tf . executing eagerly ( ) : sample chain = tf . function ( tfp . mcmc . sample chain ) else : sample chain = tfp . mcmc . sample chain # Initialize the step size. (It will be automatically adapted.) step size = tf . compat . v2 . Variable ( name = 'step size' , initial value = tf . constant ( 0.05 , dtype = tf . float32 ) , trainable = False ) def computation ( ) : """The benchmark computation.""" initial chain state = [ tf . constant ( count data . mean ( ) , name = 'init lambda1' ) , tf . constant ( count data . mean ( ) , name = 'init lambda2' ) , tf . constant ( 0.5 , name = 'init tau' ) , ] unconstraining bijectors = [ tfp . bijectors . Exp ( ) , # Maps a positive real to R. tfp . bijectors . Exp ( ) , # Maps a positive real to R. tfp . bijectors . Sigmoid ( ) , # Maps [0,1] to R. ] , kernel results = sample chain ( num results = num results , num burnin steps = num burnin steps , current state = initial chain state , kernel = tfp . mcmc . Transformed Transition Kernel ( inner kernel = tfp . mcmc . Hamiltonian Monte Carlo ( target log prob fn = unnormalized log posterior , num leapfrog steps = num leapfrog steps , step size = step size , step size update fn = tfp . mcmc . make simple step size update policy ( num burnin steps ) , state gradients are stopped = True ) , bijector = unconstraining bijectors ) ) return kernel results . inner results . is accepted # Let's force evaluation of graph to ensure build time is not part of our time # trial. is accepted tensor = computation ( ) if not tf . executing eagerly ( ) : session = tf . compat . v1 . Session ( ) session . run ( tf . compat . v1 . global variables initializer ( ) ) session . run ( is accepted tensor ) start time = time . time ( ) if tf . executing eagerly ( ) : is accepted = computation ( ) else : is accepted = session . run ( is accepted tensor ) wall time = time . time ( ) - start time num accepted = np . sum ( is accepted ) acceptance rate = np . float32 ( num accepted ) / np . float32 ( num results ) return dict ( iters = ( num results + num burnin steps ) * num leapfrog steps , extras = { 'acceptance rate' : acceptance rate } , wall time = wall time )
def outer squared difference ( x , y ) : z = x - y return z [ ... , tf . newaxis , : ] * z [ ... , tf . newaxis ]
def split covariance into marginals ( covariance , block sizes ) : start dim = 0 marginals = [ ] for size in block sizes : end dim = start dim + size marginals . append ( covariance [ ... , start dim : end dim , start dim : end dim ] ) start dim = end dim return marginals
def numpy text ( tensor , is repr = False ) : if tensor . dtype . is numpy compatible : text = repr ( tensor . numpy ( ) ) if is repr else str ( tensor . numpy ( ) ) else : text = "<unprintable>" if "\n" in text : text = "\n" + text return text
def sample shape ( self ) : if isinstance ( self . sample shape , tf . Tensor ) : return tf . Tensor Shape ( tf . get static value ( self . sample shape ) ) return tf . Tensor Shape ( self . sample shape )
def value ( self ) : if self . value is None : try : self . value = self . distribution . sample ( self . sample shape tensor ( ) ) except Not Implemented Error : raise Not Implemented Error ( "sample is not implemented for {0}. You must either pass in the " "value argument or implement sample for {0}." . format ( self . distribution . class . name ) ) return self . value
def numpy ( self ) : if not isinstance ( self . value , ops . Eager Tensor ) : raise Not Implemented Error ( "value argument must be a Eager Tensor." ) return self . value . numpy ( )
def uniform unit norm ( dimension , shape , dtype , seed ) : # This works because the Gaussian distribution is spherically symmetric. # raw shape: shape + [dimension] raw = normal . Normal ( loc = dtype util . as numpy dtype ( dtype ) ( 0 ) , scale = dtype util . as numpy dtype ( dtype ) ( 1 ) ) . sample ( tf . concat ( [ shape , [ dimension ] ] , axis = 0 ) , seed = seed ( ) ) unit norm = raw / tf . norm ( tensor = raw , ord = 2 , axis = - 1 ) [ ... , tf . newaxis ] return unit norm
def common dtype ( args list , preferred dtype = None ) : dtype = None preferred dtype = ( None if preferred dtype is None else tf . as dtype ( preferred dtype ) ) for a in tf . nest . flatten ( args list ) : if hasattr ( a , 'dtype' ) : dt = tf . as dtype ( a . dtype ) else : continue if dtype is None : dtype = dt elif dtype != dt : raise Type Error ( 'Found incompatible dtypes, {} and {}.' . format ( dtype , dt ) ) if dtype is None and preferred dtype is None : return None return ( preferred dtype if dtype is None else dtype ) . as numpy dtype
def make summary statistic ( attr ) : def fn ( self , * * kwargs ) : """Implements summary statistic, eg, mean, stddev, mode.""" x = getattr ( self . distribution , attr ) ( * * kwargs ) shape = prefer static . concat ( [ self . distribution . batch shape tensor ( ) , prefer static . ones ( prefer static . rank from shape ( self . sample shape ) , dtype = self . sample shape . dtype ) , self . distribution . event shape tensor ( ) , ] , axis = 0 ) x = tf . reshape ( x , shape = shape ) shape = prefer static . concat ( [ self . distribution . batch shape tensor ( ) , self . sample shape , self . distribution . event shape tensor ( ) , ] , axis = 0 ) return tf . broadcast to ( x , shape ) return fn
def broadcast to ( tensor to broadcast , target tensors ) : output = tensor to broadcast for tensor in target tensors : output += tf . zeros like ( tensor ) return output
def pdf at peak ( self ) : return ( self . peak - self . low ) / ( self . high - self . low )
def effective sample size single state ( states , filter beyond lag , filter threshold ) : with tf . compat . v1 . name scope ( 'effective sample size single state' , values = [ states , filter beyond lag , filter threshold ] ) : states = tf . convert to tensor ( value = states , name = 'states' ) dt = states . dtype # filter beyond lag == None ==> auto corr is the full sequence. auto corr = stats . auto correlation ( states , axis = 0 , max lags = filter beyond lag ) if filter threshold is not None : filter threshold = tf . convert to tensor ( value = filter threshold , dtype = dt , name = 'filter threshold' ) # Get a binary mask to zero out values of auto corr below the threshold. #   mask[i, ...] = 1 if auto corr[j, ...] > threshold for all j <= i, #   mask[i, ...] = 0, otherwise. # So, along dimension zero, the mask will look like [1, 1, ..., 0, 0,...] # Building step by step, #   Assume auto corr = [1, 0.5, 0.0, 0.3], and filter threshold = 0.2. # Step 1:  mask = [False, False, True, False] mask = auto corr < filter threshold # Step 2:  mask = [0, 0, 1, 1] mask = tf . cast ( mask , dtype = dt ) # Step 3:  mask = [0, 0, 1, 2] mask = tf . cumsum ( mask , axis = 0 ) # Step 4:  mask = [1, 1, 0, 0] mask = tf . maximum ( 1. - mask , 0. ) auto corr *= mask # With R[k] := auto corr[k, ...], # ESS = N / {1 + 2 * Sum {k=1}^N (N - k) / N * R[k]} #     = N / {-1 + 2 * Sum {k=0}^N (N - k) / N * R[k]} (since R[0] = 1) #     approx N / {-1 + 2 * Sum {k=0}^M (N - k) / N * R[k]} # where M is the filter beyond lag truncation point chosen above. # Get the factor (N - k) / N, and give it shape [M, 1,...,1], having total # ndims the same as auto corr n = axis size ( states , axis = 0 ) k = tf . range ( 0. , axis size ( auto corr , axis = 0 ) ) nk factor = ( n - k ) / n if auto corr . shape . ndims is not None : new shape = [ - 1 ] + [ 1 ] * ( auto corr . shape . ndims - 1 ) else : new shape = tf . concat ( ( [ - 1 ] , tf . ones ( [ tf . rank ( auto corr ) - 1 ] , dtype = tf . int32 ) ) , axis = 0 ) nk factor = tf . reshape ( nk factor , new shape ) return n / ( - 1 + 2 * tf . reduce sum ( input tensor = nk factor * auto corr , axis = 0 ) )
def potential scale reduction single state ( state , independent chain ndims ) : with tf . compat . v1 . name scope ( 'potential scale reduction single state' , values = [ state , independent chain ndims ] ) : # We assume exactly one leading dimension indexes e.g. correlated samples # from each Markov chain. state = tf . convert to tensor ( value = state , name = 'state' ) sample ndims = 1 sample axis = tf . range ( 0 , sample ndims ) chain axis = tf . range ( sample ndims , sample ndims + independent chain ndims ) sample and chain axis = tf . range ( 0 , sample ndims + independent chain ndims ) n = axis size ( state , sample axis ) m = axis size ( state , chain axis ) # In the language of Brooks and Gelman (1998), # B / n is the between chain variance, the variance of the chain means. # W is the within sequence variance, the mean of the chain variances. b div n = reduce variance ( tf . reduce mean ( input tensor = state , axis = sample axis , keepdims = True ) , sample and chain axis , biased = False ) w = tf . reduce mean ( input tensor = reduce variance ( state , sample axis , keepdims = True , biased = True ) , axis = sample and chain axis ) # sigma^2 + is an estimate of the true variance, which would be unbiased if # each chain was drawn from the target.  c.f. "law of total variance." sigma 2 plus = w + b div n return ( ( m + 1. ) / m ) * sigma 2 plus / w - ( n - 1. ) / ( m * n )
def axis size ( x , axis = None ) : if axis is None : return tf . cast ( tf . size ( input = x ) , x . dtype ) return tf . cast ( tf . reduce prod ( input tensor = tf . gather ( tf . shape ( input = x ) , axis ) ) , x . dtype )
def broadcast maybelist arg ( states , secondary arg , name ) : if is list like ( secondary arg ) : if len ( secondary arg ) != len ( states ) : raise Value Error ( 'Argument `%s` was a list of different length ({}) than ' '`states` ({})' . format ( name , len ( states ) ) ) else : secondary arg = [ secondary arg ] * len ( states ) return secondary arg
def remove ( self , field ) : return Mapping ( x = None if field == "x" else self . x , y = None if field == "y" else self . y , ildj = self . ildj , kwargs = self . kwargs )
def merge ( self , old , new , use equals = False ) : if old is None : return new if new is None : return old if ( old == new ) if use equals else ( old is new ) : return old raise Value Error ( "Incompatible values: %s != %s" % ( old , new ) )
def deep tuple ( self , x ) : if isinstance ( x , dict ) : return self . deep tuple ( tuple ( sorted ( x . items ( ) ) ) ) elif isinstance ( x , ( list , tuple ) ) : return tuple ( map ( self . deep tuple , x ) ) return x
def vggconv block ( x , filters , kernel , stride , kernel posterior fn ) : out = tfp . layers . Convolution2D Flipout ( filters , kernel , padding = 'same' , kernel posterior fn = kernel posterior fn ) ( x ) out = tf . keras . layers . Batch Normalization ( ) ( out ) out = tf . keras . layers . Activation ( 'relu' ) ( out ) out = tfp . layers . Convolution2D Flipout ( filters , kernel , padding = 'same' , kernel posterior fn = kernel posterior fn ) ( out ) out = tf . keras . layers . Batch Normalization ( ) ( out ) out = tf . keras . layers . Activation ( 'relu' ) ( out ) out = tf . keras . layers . Max Pooling2D ( pool size = ( 2 , 2 ) , strides = stride ) ( out ) return out
def embed no none gradient check ( value and gradients fn ) : @ functools . wraps ( value and gradients fn ) def func wrapped ( * args , * * kwargs ) : """Wrapped function which checks for None gradients.""" value , grads = value and gradients fn ( * args , * * kwargs ) if any ( grad is None for grad in grads ) : raise Value Error ( "Gradient is None for a state." ) return value , grads return func wrapped
def has no u turn ( state one , state two , momentum ) : dot product = sum ( [ tf . reduce sum ( input tensor = ( s1 - s2 ) * m ) for s1 , s2 , m in zip ( state one , state two , momentum ) ] ) return dot product > 0
def leapfrog ( value and gradients fn , current state , current grads target log prob , current momentum , step size ) : mid momentum = [ m + 0.5 * step * g for m , step , g in zip ( current momentum , step size , current grads target log prob ) ] next state = [ s + step * m for s , step , m in zip ( current state , step size , mid momentum ) ] next target log prob , next grads target log prob = value and gradients fn ( * next state ) next momentum = [ m + 0.5 * step * g for m , step , g in zip ( mid momentum , step size , next grads target log prob ) ] return [ next state , next target log prob , next grads target log prob , next momentum , ]
def log joint ( current target log prob , current momentum ) : momentum log prob = - sum ( [ tf . reduce sum ( input tensor = 0.5 * ( m ** 2. ) ) for m in current momentum ] ) return current target log prob + momentum log prob
def random bernoulli ( shape , probs , dtype = tf . int32 , seed = None , name = None ) : with tf . compat . v1 . name scope ( name , "random bernoulli" , [ shape , probs ] ) : probs = tf . convert to tensor ( value = probs ) random uniform = tf . random . uniform ( shape , dtype = probs . dtype , seed = seed ) return tf . cast ( tf . less ( random uniform , probs ) , dtype )
def expand as args ( args ) : return ( isinstance ( args , collections . Sequence ) and not is namedtuple ( args ) and not force leaf ( args ) )
def nested convert to tensor ( struct , dtype = None , name = None ) : if dtype is not None or not tf . nest . is nested ( struct ) : return tf . convert to tensor ( struct , dtype = dtype ) if maybe convertible to tensor ( struct ) : try : # Try converting the structure wholesale. return tf . convert to tensor ( value = struct , name = name ) except ( Value Error , Type Error ) : # Unfortunately Eager/Graph mode don't agree on the error type. pass # Try converting all of its children. shallow struct = get shallow structure ( struct ) return nest . map structure up to ( shallow struct , lambda s : nested convert to tensor ( s , name = name ) , struct )
def get tensor like attributes ( ) : # Enable "Tensor semantics" for distributions. # See tensorflow/python/framework/ops.py `class Tensor` for details. attrs = dict ( ) # Setup overloadable operators and white-listed members / properties. attrs . update ( ( attr , wrap method ( tf . Tensor , attr ) ) for attr in tf . Tensor . OVERLOADABLE OPERATORS . union ( { ' iter ' } ) ) # Copy some members straight-through. attrs . update ( ( attr , getattr ( tf . Tensor , attr ) ) for attr in { ' nonzero ' , ' bool ' , ' array priority ' } ) return attrs
def pack images ( images , rows , cols ) : shape = tf . shape ( input = images ) width = shape [ - 3 ] height = shape [ - 2 ] depth = shape [ - 1 ] images = tf . reshape ( images , ( - 1 , width , height , depth ) ) batch = tf . shape ( input = images ) [ 0 ] rows = tf . minimum ( rows , batch ) cols = tf . minimum ( batch // rows , cols ) images = images [ : rows * cols ] images = tf . reshape ( images , ( rows , cols , width , height , depth ) ) images = tf . transpose ( a = images , perm = [ 0 , 2 , 1 , 3 , 4 ] ) images = tf . reshape ( images , [ 1 , rows * width , cols * height , depth ] ) return images
def download ( directory , filename ) : filepath = os . path . join ( directory , filename ) if tf . io . gfile . exists ( filepath ) : return filepath if not tf . io . gfile . exists ( directory ) : tf . io . gfile . makedirs ( directory ) url = os . path . join ( ROOT PATH , filename ) print ( "Downloading %s to %s" % ( url , filepath ) ) urllib . request . urlretrieve ( url , filepath ) return filepath
def build fake input fns ( batch size ) : random sample = np . random . rand ( batch size , * IMAGE SHAPE ) . astype ( "float32" ) def train input fn ( ) : dataset = tf . data . Dataset . from tensor slices ( random sample ) . map ( lambda row : ( row , 0 ) ) . batch ( batch size ) . repeat ( ) return tf . compat . v1 . data . make one shot iterator ( dataset ) . get next ( ) def eval input fn ( ) : dataset = tf . data . Dataset . from tensor slices ( random sample ) . map ( lambda row : ( row , 0 ) ) . batch ( batch size ) return tf . compat . v1 . data . make one shot iterator ( dataset ) . get next ( ) return train input fn , eval input fn
def build input fns ( data dir , batch size ) : # Build an iterator over training batches. def train input fn ( ) : dataset = static mnist dataset ( data dir , "train" ) dataset = dataset . shuffle ( 50000 ) . repeat ( ) . batch ( batch size ) return tf . compat . v1 . data . make one shot iterator ( dataset ) . get next ( ) # Build an iterator over the heldout set. def eval input fn ( ) : eval dataset = static mnist dataset ( data dir , "valid" ) eval dataset = eval dataset . batch ( batch size ) return tf . compat . v1 . data . make one shot iterator ( eval dataset ) . get next ( ) return train input fn , eval input fn
def validate block sizes ( block sizes , bijectors , validate args ) : block sizes shape = block sizes . shape if tensorshape util . is fully defined ( block sizes shape ) : if ( tensorshape util . rank ( block sizes shape ) != 1 or ( tensorshape util . num elements ( block sizes shape ) != len ( bijectors ) ) ) : raise Value Error ( '`block sizes` must be `None`, or a vector of the same length as ' '`bijectors`. Got a `Tensor` with shape {} and `bijectors` of ' 'length {}' . format ( block sizes shape , len ( bijectors ) ) ) return block sizes elif validate args : message = ( '`block sizes` must be `None`, or a vector of the same length ' 'as `bijectors`.' ) with tf . control dependencies ( [ assert util . assert equal ( tf . size ( input = block sizes ) , len ( bijectors ) , message = message ) , assert util . assert equal ( tf . rank ( block sizes ) , 1 ) ] ) : return tf . identity ( block sizes ) else : return block sizes
def maybe check wont broadcast ( flat xs , validate args ) : flat xs = tuple ( flat xs ) # So we can receive generators. if not validate args : # Note: we don't try static validation because it is theoretically # possible that a user wants to take advantage of broadcasting. # Only when `validate args` is `True` do we enforce the validation. return flat xs msg = 'Broadcasting probably indicates an error in model specification.' s = tuple ( x . shape for x in flat xs ) if all ( tensorshape util . is fully defined ( s ) for s in s ) : if not all ( a == b for a , b in zip ( s [ 1 : ] , s [ : - 1 ] ) ) : raise Value Error ( msg ) return flat xs assertions = [ assert util . assert equal ( a , b , message = msg ) for a , b in zip ( s [ 1 : ] , s [ : - 1 ] ) ] with tf . control dependencies ( assertions ) : return tuple ( tf . identity ( x ) for x in flat xs )
def maybe call volatility fn and grads ( volatility fn , state , volatility fn results = None , grads volatility fn = None , sample shape = None , parallel iterations = 10 ) : state parts = list ( state ) if mcmc util . is list like ( state ) else [ state ] needs volatility fn gradients = grads volatility fn is None # Convert `volatility fn results` to a list if volatility fn results is None : volatility fn results = volatility fn ( * state parts ) volatility fn results = ( list ( volatility fn results ) if mcmc util . is list like ( volatility fn results ) else [ volatility fn results ] ) if len ( volatility fn results ) == 1 : volatility fn results *= len ( state parts ) if len ( state parts ) != len ( volatility fn results ) : raise Value Error ( '`volatility fn` should return a tensor or a list ' 'of the same length as `current state`.' ) # The shape of 'volatility parts' needs to have the number of chains as a # leading dimension. For determinism we broadcast 'volatility parts' to the # shape of `state parts` since each dimension of `state parts` could have a # different volatility value. volatility fn results = maybe broadcast volatility ( volatility fn results , state parts ) if grads volatility fn is None : [ , grads volatility fn , ] = diag jacobian ( xs = state parts , ys = volatility fn results , sample shape = sample shape , parallel iterations = parallel iterations , fn = volatility fn ) # Compute gradient of `volatility parts**2` if needs volatility fn gradients : grads volatility fn = [ 2. * g * volatility if g is not None else tf . zeros like ( fn arg , dtype = fn arg . dtype . base dtype ) for g , volatility , fn arg in zip ( grads volatility fn , volatility fn results , state parts ) ] return volatility fn results , grads volatility fn
def maybe broadcast volatility ( volatility parts , state parts ) : return [ v + tf . zeros like ( sp , dtype = sp . dtype . base dtype ) for v , sp in zip ( volatility parts , state parts ) ]
def prepare args ( target log prob fn , volatility fn , state , step size , target log prob = None , grads target log prob = None , volatility = None , grads volatility fn = None , diffusion drift = None , parallel iterations = 10 ) : state parts = list ( state ) if mcmc util . is list like ( state ) else [ state ] [ target log prob , grads target log prob , ] = mcmc util . maybe call fn and grads ( target log prob fn , state parts , target log prob , grads target log prob ) [ volatility parts , grads volatility , ] = maybe call volatility fn and grads ( volatility fn , state parts , volatility , grads volatility fn , distribution util . prefer static shape ( target log prob ) , parallel iterations ) step sizes = ( list ( step size ) if mcmc util . is list like ( step size ) else [ step size ] ) step sizes = [ tf . convert to tensor ( value = s , name = 'step size' , dtype = target log prob . dtype ) for s in step sizes ] if len ( step sizes ) == 1 : step sizes *= len ( state parts ) if len ( state parts ) != len ( step sizes ) : raise Value Error ( 'There should be exactly one `step size` or it should ' 'have same length as `current state`.' ) if diffusion drift is None : diffusion drift parts = get drift ( step sizes , volatility parts , grads volatility , grads target log prob ) else : diffusion drift parts = ( list ( diffusion drift ) if mcmc util . is list like ( diffusion drift ) else [ diffusion drift ] ) if len ( state parts ) != len ( diffusion drift ) : raise Value Error ( 'There should be exactly one `diffusion drift` or it ' 'should have same length as list-like `current state`.' ) return [ state parts , step sizes , target log prob , grads target log prob , volatility parts , grads volatility , diffusion drift parts , ]
def validate init args statically ( distribution , batch shape ) : if tensorshape util . rank ( batch shape . shape ) is not None : if tensorshape util . rank ( batch shape . shape ) != 1 : raise Value Error ( "`batch shape` must be a vector " "(saw rank: {})." . format ( tensorshape util . rank ( batch shape . shape ) ) ) batch shape static = tensorshape util . constant value as shape ( batch shape ) batch size static = tensorshape util . num elements ( batch shape static ) dist batch size static = tensorshape util . num elements ( distribution . batch shape ) if batch size static is not None and dist batch size static is not None : if batch size static != dist batch size static : raise Value Error ( "`batch shape` size ({}) must match " "`distribution.batch shape` size ({})." . format ( batch size static , dist batch size static ) ) if tensorshape util . dims ( batch shape static ) is not None : if any ( tf . compat . dimension value ( dim ) is not None and tf . compat . dimension value ( dim ) < 1 for dim in batch shape static ) : raise Value Error ( "`batch shape` elements must be >=-1." )
def sample shape ( self , x ) : x ndims = ( tf . rank ( x ) if tensorshape util . rank ( x . shape ) is None else tensorshape util . rank ( x . shape ) ) event ndims = ( tf . size ( input = self . event shape tensor ( ) ) if tensorshape util . rank ( self . event shape ) is None else tensorshape util . rank ( self . event shape ) ) batch ndims = ( tf . size ( input = self . batch shape unexpanded ) if tensorshape util . rank ( self . batch shape ) is None else tensorshape util . rank ( self . batch shape ) ) sample ndims = x ndims - batch ndims - event ndims if isinstance ( sample ndims , int ) : static sample shape = x . shape [ : sample ndims ] else : static sample shape = tf . Tensor Shape ( None ) if tensorshape util . is fully defined ( static sample shape ) : sample shape = np . int32 ( static sample shape ) else : sample shape = tf . shape ( input = x ) [ : sample ndims ] return sample shape , static sample shape
def call reshape input output ( self , fn , x , extra kwargs = None ) : # Note: we take `extra kwargs` as a dict rather than `**extra kwargs` # because it is possible the user provided extra kwargs would itself # have `fn` and/or `x` as a key. with tf . control dependencies ( self . runtime assertions + self . validate sample arg ( x ) ) : sample shape , static sample shape = self . sample shape ( x ) old shape = tf . concat ( [ sample shape , self . distribution . batch shape tensor ( ) , self . event shape tensor ( ) , ] , axis = 0 ) x reshape = tf . reshape ( x , old shape ) result = fn ( x reshape , * * extra kwargs ) if extra kwargs else fn ( x reshape ) new shape = tf . concat ( [ sample shape , self . batch shape unexpanded , ] , axis = 0 ) result = tf . reshape ( result , new shape ) if ( tensorshape util . rank ( static sample shape ) is not None and tensorshape util . rank ( self . batch shape ) is not None ) : new shape = tensorshape util . concatenate ( static sample shape , self . batch shape ) tensorshape util . set shape ( result , new shape ) return result
def call and reshape output ( self , fn , event shape list = None , static event shape list = None , extra kwargs = None ) : # Note: we take `extra kwargs` as a dict rather than `**extra kwargs` # because it is possible the user provided extra kwargs would itself # have `fn`, `event shape list`, `static event shape list` and/or # `extra kwargs` as keys. with tf . control dependencies ( self . runtime assertions ) : if event shape list is None : event shape list = [ self . event shape tensor ( ) ] if static event shape list is None : static event shape list = [ self . event shape ] new shape = tf . concat ( [ self . batch shape unexpanded ] + event shape list , axis = 0 ) result = tf . reshape ( fn ( * * extra kwargs ) if extra kwargs else fn ( ) , new shape ) if ( tensorshape util . rank ( self . batch shape ) is not None and tensorshape util . rank ( self . event shape ) is not None ) : event shape = tf . Tensor Shape ( [ ] ) for rss in static event shape list : event shape = tensorshape util . concatenate ( event shape , rss ) static shape = tensorshape util . concatenate ( self . batch shape , event shape ) tensorshape util . set shape ( result , static shape ) return result
def validate sample arg ( self , x ) : with tf . name scope ( "validate sample arg" ) : x ndims = ( tf . rank ( x ) if tensorshape util . rank ( x . shape ) is None else tensorshape util . rank ( x . shape ) ) event ndims = ( tf . size ( input = self . event shape tensor ( ) ) if tensorshape util . rank ( self . event shape ) is None else tensorshape util . rank ( self . event shape ) ) batch ndims = ( tf . size ( input = self . batch shape unexpanded ) if tensorshape util . rank ( self . batch shape ) is None else tensorshape util . rank ( self . batch shape ) ) expected batch event ndims = batch ndims + event ndims if ( isinstance ( x ndims , int ) and isinstance ( expected batch event ndims , int ) ) : if x ndims < expected batch event ndims : raise Not Implemented Error ( "Broadcasting is not supported; too few batch and event dims " "(expected at least {}, saw {})." . format ( expected batch event ndims , x ndims ) ) ndims assertion = [ ] elif self . validate args : ndims assertion = [ assert util . assert greater equal ( x ndims , expected batch event ndims , message = ( "Broadcasting is not supported; too few " "batch and event dims." ) , name = "assert batch and event ndims large enough" ) , ] if ( tensorshape util . is fully defined ( self . batch shape ) and tensorshape util . is fully defined ( self . event shape ) ) : expected batch event shape = np . int32 ( tensorshape util . concatenate ( self . batch shape , self . event shape ) ) else : expected batch event shape = tf . concat ( [ self . batch shape tensor ( ) , self . event shape tensor ( ) , ] , axis = 0 ) sample ndims = x ndims - expected batch event ndims if isinstance ( sample ndims , int ) : sample ndims = max ( sample ndims , 0 ) if ( isinstance ( sample ndims , int ) and tensorshape util . is fully defined ( x . shape [ sample ndims : ] ) ) : actual batch event shape = np . int32 ( x . shape [ sample ndims : ] ) else : sample ndims = tf . maximum ( sample ndims , 0 ) actual batch event shape = tf . shape ( input = x ) [ sample ndims : ] if ( isinstance ( expected batch event shape , np . ndarray ) and isinstance ( actual batch event shape , np . ndarray ) ) : if any ( expected batch event shape != actual batch event shape ) : raise Not Implemented Error ( "Broadcasting is not supported; " "unexpected batch and event shape " "(expected {}, saw {})." . format ( expected batch event shape , actual batch event shape ) ) # We need to set the final runtime-assertions to `ndims assertion` since # its possible this assertion was created. We could add a condition to # only do so if `self.validate args == True`, however this is redundant # as `ndims assertion` already encodes this information. runtime assertions = ndims assertion elif self . validate args : # We need to make the `ndims assertion` a control dep because otherwise # TF itself might raise an exception owing to this assertion being # ill-defined, ie, one cannot even compare different rank Tensors. with tf . control dependencies ( ndims assertion ) : shape assertion = assert util . assert equal ( expected batch event shape , actual batch event shape , message = ( "Broadcasting is not supported; " "unexpected batch and event shape." ) , name = "assert batch and event shape same" ) runtime assertions = [ shape assertion ] else : runtime assertions = [ ] return runtime assertions
def maybe assert valid sample ( self , counts ) : if not self . validate args : return counts counts = distribution util . embed check nonnegative integer form ( counts ) return distribution util . with dependencies ( [ assert util . assert less equal ( counts , self . total count , message = "counts are not less than or equal to n." ) , ] , counts )
def flat sample distributions ( self , sample shape = ( ) , seed = None , value = None ) : ds = [ ] values out = [ ] seed = seed stream . Seed Stream ( 'Joint Distribution Coroutine' , seed ) gen = self . model ( ) index = 0 d = next ( gen ) try : while True : actual distribution = d . distribution if isinstance ( d , self . Root ) else d ds . append ( actual distribution ) if ( value is not None and len ( value ) > index and value [ index ] is not None ) : seed ( ) next value = value [ index ] else : next value = actual distribution . sample ( sample shape = sample shape if isinstance ( d , self . Root ) else ( ) , seed = seed ( ) ) values out . append ( next value ) index += 1 d = gen . send ( next value ) except Stop Iteration : pass return ds , values out
def newsgroups dataset ( directory , split name , num words , shuffle and repeat ) : data = np . load ( download ( directory , FILE TEMPLATE . format ( split = split name ) ) ) # The last row is empty in both train and test. data = data [ : - 1 ] # Each row is a list of word ids in the document. We first convert this to # sparse COO matrix (which automatically sums the repeating words). Then, # we convert this COO matrix to CSR format which allows for fast querying of # documents. num documents = data . shape [ 0 ] indices = np . array ( [ ( row idx , column idx ) for row idx , row in enumerate ( data ) for column idx in row ] ) sparse matrix = scipy . sparse . coo matrix ( ( np . ones ( indices . shape [ 0 ] ) , ( indices [ : , 0 ] , indices [ : , 1 ] ) ) , shape = ( num documents , num words ) , dtype = np . float32 ) sparse matrix = sparse matrix . tocsr ( ) dataset = tf . data . Dataset . range ( num documents ) # For training, we shuffle each epoch and repeat the epochs. if shuffle and repeat : dataset = dataset . shuffle ( num documents ) . repeat ( ) # Returns a single document as a dense Tensor Flow tensor. The dataset is # stored as a sparse matrix outside of the graph. def get row py func ( idx ) : def get row python ( idx py ) : return np . squeeze ( np . array ( sparse matrix [ idx py ] . todense ( ) ) , axis = 0 ) py func = tf . compat . v1 . py func ( get row python , [ idx ] , tf . float32 , stateful = False ) py func . set shape ( ( num words , ) ) return py func dataset = dataset . map ( get row py func ) return dataset
def build fake input fns ( batch size ) : num words = 1000 vocabulary = [ str ( i ) for i in range ( num words ) ] random sample = np . random . randint ( 10 , size = ( batch size , num words ) ) . astype ( np . float32 ) def train input fn ( ) : dataset = tf . data . Dataset . from tensor slices ( random sample ) dataset = dataset . batch ( batch size ) . repeat ( ) return tf . compat . v1 . data . make one shot iterator ( dataset ) . get next ( ) def eval input fn ( ) : dataset = tf . data . Dataset . from tensor slices ( random sample ) dataset = dataset . batch ( batch size ) return tf . compat . v1 . data . make one shot iterator ( dataset ) . get next ( ) return train input fn , eval input fn , vocabulary
def load bernoulli mnist dataset ( directory , split name ) : amat file = download ( directory , FILE TEMPLATE . format ( split = split name ) ) dataset = tf . data . Text Line Dataset ( amat file ) str to arr = lambda string : np . array ( [ c == b"1" for c in string . split ( ) ] ) def parser ( s ) : booltensor = tf . compat . v1 . py func ( str to arr , [ s ] , tf . bool ) reshaped = tf . reshape ( booltensor , [ 28 , 28 , 1 ] ) return tf . cast ( reshaped , dtype = tf . float32 ) , tf . constant ( 0 , tf . int32 ) return dataset . map ( parser )
def build input pipeline ( data dir , batch size , heldout size , mnist type ) : # Build an iterator over training batches. if mnist type in [ Mnist Type . FAKE DATA , Mnist Type . THRESHOLD ] : if mnist type == Mnist Type . FAKE DATA : mnist data = build fake data ( ) else : mnist data = mnist . read data sets ( data dir ) training dataset = tf . data . Dataset . from tensor slices ( ( mnist data . train . images , np . int32 ( mnist data . train . labels ) ) ) heldout dataset = tf . data . Dataset . from tensor slices ( ( mnist data . validation . images , np . int32 ( mnist data . validation . labels ) ) ) elif mnist type == Mnist Type . BERNOULLI : training dataset = load bernoulli mnist dataset ( data dir , "train" ) heldout dataset = load bernoulli mnist dataset ( data dir , "valid" ) else : raise Value Error ( "Unknown MNIST type." ) training batches = training dataset . repeat ( ) . batch ( batch size ) training iterator = tf . compat . v1 . data . make one shot iterator ( training batches ) # Build a iterator over the heldout set with batch size=heldout size, # i.e., return the entire heldout set as a constant. heldout frozen = ( heldout dataset . take ( heldout size ) . repeat ( ) . batch ( heldout size ) ) heldout iterator = tf . compat . v1 . data . make one shot iterator ( heldout frozen ) # Combine these into a feedable iterator that can switch between training # and validation inputs. handle = tf . compat . v1 . placeholder ( tf . string , shape = [ ] ) feedable iterator = tf . compat . v1 . data . Iterator . from string handle ( handle , training batches . output types , training batches . output shapes ) images , labels = feedable iterator . get next ( ) # Reshape as a pixel image and binarize pixels. images = tf . reshape ( images , shape = [ - 1 ] + IMAGE SHAPE ) if mnist type in [ Mnist Type . FAKE DATA , Mnist Type . THRESHOLD ] : images = tf . cast ( images > 0.5 , dtype = tf . int32 ) return images , labels , handle , training iterator , heldout iterator
def as numpy dtype ( dtype ) : dtype = tf . as dtype ( dtype ) if hasattr ( dtype , 'as numpy dtype' ) : return dtype . as numpy dtype return dtype
def base dtype ( dtype ) : dtype = tf . as dtype ( dtype ) if hasattr ( dtype , 'base dtype' ) : return dtype . base dtype return dtype
def is bool ( dtype ) : dtype = tf . as dtype ( dtype ) if hasattr ( dtype , 'is bool' ) : return dtype . is bool # We use `kind` because: # np.issubdtype(np.uint8, np.bool) == True. return np . dtype ( dtype ) . kind == 'b'
def is complex ( dtype ) : dtype = tf . as dtype ( dtype ) if hasattr ( dtype , 'is complex' ) : return dtype . is complex return np . issubdtype ( np . dtype ( dtype ) , np . complex )
def max ( dtype ) : # pylint: disable=redefined-builtin dtype = tf . as dtype ( dtype ) if hasattr ( dtype , 'max' ) : return dtype . max use finfo = is floating ( dtype ) or is complex ( dtype ) return np . finfo ( dtype ) . max if use finfo else np . iinfo ( dtype ) . max
def name ( dtype ) : dtype = tf . as dtype ( dtype ) if hasattr ( dtype , 'name' ) : return dtype . name if hasattr ( dtype , ' name ' ) : return dtype . name return str ( dtype )
def size ( dtype ) : dtype = tf . as dtype ( dtype ) if hasattr ( dtype , 'size' ) : return dtype . size return np . dtype ( dtype ) . itemsize
def nelder mead one step ( current simplex , current objective values , objective function = None , dim = None , func tolerance = None , position tolerance = None , batch evaluate objective = False , reflection = None , expansion = None , contraction = None , shrinkage = None , name = None ) : with tf . compat . v1 . name scope ( name , 'nelder mead one step' ) : domain dtype = current simplex . dtype . base dtype order = tf . argsort ( current objective values , direction = 'ASCENDING' , stable = True ) ( best index , worst index , second worst index ) = order [ 0 ] , order [ - 1 ] , order [ - 2 ] worst vertex = current simplex [ worst index ] ( best objective value , worst objective value , second worst objective value ) = ( current objective values [ best index ] , current objective values [ worst index ] , current objective values [ second worst index ] ) # Compute the centroid of the face opposite the worst vertex. face centroid = tf . reduce sum ( input tensor = current simplex , axis = 0 ) - worst vertex face centroid /= tf . cast ( dim , domain dtype ) # Reflect the worst vertex through the opposite face. reflected = face centroid + reflection * ( face centroid - worst vertex ) objective at reflected = objective function ( reflected ) num evaluations = 1 has converged = check convergence ( current simplex , current simplex [ best index ] , best objective value , worst objective value , func tolerance , position tolerance ) def converged fn ( ) : return ( True , current simplex , current objective values , 0 ) case0 = has converged , converged fn accept reflected = ( ( objective at reflected < second worst objective value ) & ( objective at reflected >= best objective value ) ) accept reflected fn = accept reflected fn ( current simplex , current objective values , worst index , reflected , objective at reflected ) case1 = accept reflected , accept reflected fn do expansion = objective at reflected < best objective value expansion fn = expansion fn ( objective function , current simplex , current objective values , worst index , reflected , objective at reflected , face centroid , expansion ) case2 = do expansion , expansion fn do outside contraction = ( ( objective at reflected < worst objective value ) & ( objective at reflected >= second worst objective value ) ) outside contraction fn = outside contraction fn ( objective function , current simplex , current objective values , face centroid , best index , worst index , reflected , objective at reflected , contraction , shrinkage , batch evaluate objective ) case3 = do outside contraction , outside contraction fn default fn = inside contraction fn ( objective function , current simplex , current objective values , face centroid , best index , worst index , worst objective value , contraction , shrinkage , batch evaluate objective ) ( converged , next simplex , next objective at simplex , case evals ) = prefer static . case ( [ case0 , case1 , case2 , case3 ] , default = default fn , exclusive = False ) next simplex . set shape ( current simplex . shape ) next objective at simplex . set shape ( current objective values . shape ) return ( converged , next simplex , next objective at simplex , num evaluations + case evals )
def accept reflected fn ( simplex , objective values , worst index , reflected , objective at reflected ) : def replace worst with reflected ( ) : next simplex = replace at index ( simplex , worst index , reflected ) next objective values = replace at index ( objective values , worst index , objective at reflected ) return False , next simplex , next objective values , 0 return replace worst with reflected
def expansion fn ( objective function , simplex , objective values , worst index , reflected , objective at reflected , face centroid , expansion ) : def expand and maybe replace ( ) : """Performs the expansion step.""" expanded = face centroid + expansion * ( reflected - face centroid ) expanded objective value = objective function ( expanded ) expanded is better = ( expanded objective value < objective at reflected ) accept expanded fn = lambda : ( expanded , expanded objective value ) accept reflected fn = lambda : ( reflected , objective at reflected ) next pt , next objective value = prefer static . cond ( expanded is better , accept expanded fn , accept reflected fn ) next simplex = replace at index ( simplex , worst index , next pt ) next objective at simplex = replace at index ( objective values , worst index , next objective value ) return False , next simplex , next objective at simplex , 1 return expand and maybe replace
def outside contraction fn ( objective function , simplex , objective values , face centroid , best index , worst index , reflected , objective at reflected , contraction , shrinkage , batch evaluate objective ) : def contraction ( ) : """Performs a contraction.""" contracted = face centroid + contraction * ( reflected - face centroid ) objective at contracted = objective function ( contracted ) is contracted acceptable = objective at contracted <= objective at reflected def accept contraction ( ) : next simplex = replace at index ( simplex , worst index , contracted ) objective at next simplex = replace at index ( objective values , worst index , objective at contracted ) return ( False , next simplex , objective at next simplex , 1 ) def reject contraction ( ) : return shrink towards best ( objective function , simplex , best index , shrinkage , batch evaluate objective ) return prefer static . cond ( is contracted acceptable , accept contraction , reject contraction ) return contraction
def shrink towards best ( objective function , simplex , best index , shrinkage , batch evaluate objective ) : # If the contraction step fails to improve the average objective enough, # the simplex is shrunk towards the best vertex. best vertex = simplex [ best index ] shrunk simplex = best vertex + shrinkage * ( simplex - best vertex ) objective at shrunk simplex , evals = evaluate objective multiple ( objective function , shrunk simplex , batch evaluate objective ) return ( False , shrunk simplex , objective at shrunk simplex , evals )
def replace at index ( x , index , replacement ) : x new = tf . concat ( [ x [ : index ] , tf . expand dims ( replacement , axis = 0 ) , x [ ( index + 1 ) : ] ] , axis = 0 ) return x new
def prepare args with initial simplex ( objective function , initial simplex , objective at initial simplex , batch evaluate objective ) : initial simplex = tf . convert to tensor ( value = initial simplex ) # If d is the dimension of the problem, the number of vertices in the # simplex should be d+1. From this, we can infer the number of dimensions # as n - 1 where n is the number of vertices specified. num vertices = tf . shape ( input = initial simplex ) [ 0 ] dim = num vertices - 1 num evaluations = 0 if objective at initial simplex is None : objective at initial simplex , n evals = evaluate objective multiple ( objective function , initial simplex , batch evaluate objective ) num evaluations += n evals objective at initial simplex = tf . convert to tensor ( value = objective at initial simplex ) return ( dim , num vertices , initial simplex , objective at initial simplex , num evaluations )
def prepare args with initial vertex ( objective function , initial vertex , step sizes , objective at initial vertex , batch evaluate objective ) : dim = tf . size ( input = initial vertex ) num vertices = dim + 1 unit vectors along axes = tf . reshape ( tf . eye ( dim , dim , dtype = initial vertex . dtype . base dtype ) , tf . concat ( [ [ dim ] , tf . shape ( input = initial vertex ) ] , axis = 0 ) ) # If step sizes does not broadcast to initial vertex, the multiplication # in the second term will fail. simplex face = initial vertex + step sizes * unit vectors along axes simplex = tf . concat ( [ tf . expand dims ( initial vertex , axis = 0 ) , simplex face ] , axis = 0 ) num evaluations = 0 # Evaluate the objective function at the simplex vertices. if objective at initial vertex is None : objective at initial vertex = objective function ( initial vertex ) num evaluations += 1 objective at simplex face , num evals = evaluate objective multiple ( objective function , simplex face , batch evaluate objective ) num evaluations += num evals objective at simplex = tf . concat ( [ tf . expand dims ( objective at initial vertex , axis = 0 ) , objective at simplex face ] , axis = 0 ) return ( dim , num vertices , simplex , objective at simplex , num evaluations )
def build input pipeline ( mnist data , batch size , heldout size ) : # Build an iterator over training batches. training dataset = tf . data . Dataset . from tensor slices ( ( mnist data . train . images , np . int32 ( mnist data . train . labels ) ) ) training batches = training dataset . shuffle ( 50000 , reshuffle each iteration = True ) . repeat ( ) . batch ( batch size ) training iterator = tf . compat . v1 . data . make one shot iterator ( training batches ) # Build a iterator over the heldout set with batch size=heldout size, # i.e., return the entire heldout set as a constant. heldout dataset = tf . data . Dataset . from tensor slices ( ( mnist data . validation . images , np . int32 ( mnist data . validation . labels ) ) ) heldout frozen = ( heldout dataset . take ( heldout size ) . repeat ( ) . batch ( heldout size ) ) heldout iterator = tf . compat . v1 . data . make one shot iterator ( heldout frozen ) # Combine these into a feedable iterator that can switch between training # and validation inputs. handle = tf . compat . v1 . placeholder ( tf . string , shape = [ ] ) feedable iterator = tf . compat . v1 . data . Iterator . from string handle ( handle , training batches . output types , training batches . output shapes ) images , labels = feedable iterator . get next ( ) return images , labels , handle , training iterator , heldout iterator
def build fake data ( num examples = 10 ) : class Dummy ( object ) : pass num examples = 10 mnist data = Dummy ( ) mnist data . train = Dummy ( ) mnist data . train . images = np . float32 ( np . random . randn ( num examples , * IMAGE SHAPE ) ) mnist data . train . labels = np . int32 ( np . random . permutation ( np . arange ( num examples ) ) ) mnist data . train . num examples = num examples mnist data . validation = Dummy ( ) mnist data . validation . images = np . float32 ( np . random . randn ( num examples , * IMAGE SHAPE ) ) mnist data . validation . labels = np . int32 ( np . random . permutation ( np . arange ( num examples ) ) ) mnist data . validation . num examples = num examples return mnist data
def get config ( self ) : return { 'initializers' : [ tf . compat . v2 . initializers . serialize ( tf . keras . initializers . get ( init ) ) for init in self . initializers ] , 'sizes' : self . sizes , 'validate args' : self . validate args , }
def from config ( cls , config ) : return cls ( * * { 'initializers' : [ tf . compat . v2 . initializers . deserialize ( init ) for init in config . get ( 'initializers' , [ ] ) ] , 'sizes' : config . get ( 'sizes' , [ ] ) , 'validate args' : config . get ( 'validate args' , False ) , } )
def matmul ( a , b , transpose a = False , transpose b = False , adjoint a = False , adjoint b = False , a is sparse = False , b is sparse = False , name = None ) : # pylint: disable=unused-argument if a is sparse or b is sparse : raise Not Implemented Error ( 'Numpy backend does not support sparse matmul.' ) if transpose a or adjoint a : a = matrix transpose ( a , conjugate = adjoint a ) if transpose b or adjoint b : b = matrix transpose ( b , conjugate = adjoint b ) return np . matmul ( a , b )
def std var helper ( self , statistic , statistic name , statistic ndims , df factor fn ) : df = tf . reshape ( self . df , tf . concat ( [ tf . shape ( input = self . df ) , tf . ones ( [ statistic ndims ] , dtype = tf . int32 ) ] , - 1 ) ) df = broadcast to shape ( df , tf . shape ( input = statistic ) ) # We need to put the tf.where inside the outer tf.where to ensure we never # hit a Na N in the gradient. denom = tf . where ( df > 2. , df - 2. , tf . ones like ( df ) ) statistic = statistic * df factor fn ( df / denom ) # When 1 < df <= 2, stddev/variance are infinite. inf = dtype util . as numpy dtype ( self . dtype ) ( np . inf ) result where defined = tf . where ( df > 2. , statistic , tf . fill ( tf . shape ( input = statistic ) , inf , name = "inf" ) ) if self . allow nan stats : nan = dtype util . as numpy dtype ( self . dtype ) ( np . nan ) return tf . where ( df > 1. , result where defined , tf . fill ( tf . shape ( input = statistic ) , nan , name = "nan" ) ) else : with tf . control dependencies ( [ assert util . assert less ( tf . cast ( 1. , self . dtype ) , df , message = statistic name + " not defined for components of df <= 1" ) , ] ) : return tf . identity ( result where defined )
def pick scalar condition ( pred , cond true , cond false ) : # Note: This function is only valid if all of pred, cond true, and cond false # are scalars. This means its semantics are arguably more like tf.cond than # tf.where even though we use tf.where to implement it. pred = tf . get static value ( tf . convert to tensor ( value = pred ) ) if pred is None : return tf . where ( pred , cond true , cond false ) return cond true if pred else cond false
def finish log prob for one fiber ( self , y , x , ildj , event ndims , * * distribution kwargs ) : x = self . maybe rotate dims ( x , rotate right = True ) log prob = self . distribution . log prob ( x , * * distribution kwargs ) if self . is maybe event override : log prob = tf . reduce sum ( input tensor = log prob , axis = self . reduce event indices ) log prob += tf . cast ( ildj , log prob . dtype ) if self . is maybe event override and isinstance ( event ndims , int ) : tensorshape util . set shape ( log prob , tf . broadcast static shape ( tensorshape util . with rank at least ( y . shape , 1 ) [ : - event ndims ] , self . batch shape ) ) return log prob
def finish prob for one fiber ( self , y , x , ildj , event ndims , * * distribution kwargs ) : x = self . maybe rotate dims ( x , rotate right = True ) prob = self . distribution . prob ( x , * * distribution kwargs ) if self . is maybe event override : prob = tf . reduce prod ( input tensor = prob , axis = self . reduce event indices ) prob *= tf . exp ( tf . cast ( ildj , prob . dtype ) ) if self . is maybe event override and isinstance ( event ndims , int ) : tensorshape util . set shape ( prob , tf . broadcast static shape ( tensorshape util . with rank at least ( y . shape , 1 ) [ : - event ndims ] , self . batch shape ) ) return prob
def maybe rotate dims ( self , x , rotate right = False ) : needs rotation const = tf . get static value ( self . needs rotation ) if needs rotation const is not None and not needs rotation const : return x ndims = prefer static . rank ( x ) n = ( ndims - self . rotate ndims ) if rotate right else self . rotate ndims perm = prefer static . concat ( [ prefer static . range ( n , ndims ) , prefer static . range ( 0 , n ) ] , axis = 0 ) return tf . transpose ( a = x , perm = perm )
def apply single step ( dist , params event ndims , slices , params overrides ) : if len ( slices ) == 1 and slices [ 0 ] == Ellipsis : # The path used by Distribution.copy: batch slice(...args..., Ellipsis) override dict = { } else : override dict = slice params to dict ( dist , params event ndims , slices ) override dict . update ( params overrides ) parameters = dict ( dist . parameters , * * override dict ) new dist = type ( dist ) ( * * parameters ) return new dist
def apply slice sequence ( dist , params event ndims , slice overrides seq ) : for slices , overrides in slice overrides seq : dist = apply single step ( dist , params event ndims , slices , overrides ) return dist
def num cols ( x ) : if tf . compat . dimension value ( x . shape [ - 1 ] ) is not None : return tf . compat . dimension value ( x . shape [ - 1 ] ) return tf . shape ( input = x ) [ - 1 ]
def prefer static ( original fn , static fn ) : original spec = tf inspect . getfullargspec ( original fn ) static spec = tf inspect . getfullargspec ( static fn ) if original spec != static spec : raise Value Error ( 'Arg specs do not match: original={}, static={}, fn={}' . format ( original spec , static spec , original fn ) ) @ decorator . decorator def wrap ( wrapped fn , * args , * * kwargs ) : del wrapped fn [ args , kwargs ] , all static = maybe get static args ( [ args , kwargs ] ) if all static : return static fn ( * args , * * kwargs ) return original fn ( * args , * * kwargs ) return wrap ( original fn )
def copy docstring ( original fn , new fn ) : original spec = tf inspect . getfullargspec ( original fn ) new spec = tf inspect . getfullargspec ( new fn ) if original spec != new spec : raise Value Error ( 'Arg specs do not match: original={}, new={}, fn={}' . format ( original spec , new spec , original fn ) ) @ decorator . decorator def wrap ( wrapped fn , * args , * * kwargs ) : del wrapped fn return new fn ( * args , * * kwargs ) return wrap ( original fn )
def get static predicate ( pred ) : if pred in { 0 , 1 } : # Accept 1/0 as valid boolean values pred value = bool ( pred ) elif isinstance ( pred , bool ) : pred value = pred elif isinstance ( pred , tf . Tensor ) : pred value = tf . get static value ( pred ) # TODO(jamieas): remove the dependency on `pywrap tensorflow`. # pylint: disable=protected-access if pred value is None : pred value = c api . TF Try Evaluate Constant wrapper ( pred . graph . c graph , pred . as tf output ( ) ) # pylint: enable=protected-access else : raise Type Error ( '`pred` must be a Tensor, or a Python bool, or 1 or 0. ' 'Found instead: {}' . format ( pred ) ) return pred value
def rank from shape ( shape tensor fn , tensorshape = None ) : if tensorshape is None : shape tensor = ( shape tensor fn ( ) if callable ( shape tensor fn ) else shape tensor fn ) if ( hasattr ( shape tensor , 'shape' ) and hasattr ( shape tensor . shape , 'num elements' ) ) : ndims = tensorshape util . num elements ( shape tensor . shape ) else : ndims = len ( shape tensor ) ndims fn = lambda : tf . size ( input = shape tensor ) else : ndims = tensorshape util . rank ( tensorshape ) ndims fn = lambda : tf . size ( input = shape tensor fn ( ) # pylint: disable=g-long-lambda if callable ( shape tensor fn ) else shape tensor fn ) return ndims fn ( ) if ndims is None else ndims
def name scope ( self , name = None , default name = None , values = None ) : with tf . compat . v1 . name scope ( self . name ) : with tf . compat . v1 . name scope ( name , default name , values = values or [ ] ) as scope : yield scope
def embed check nonnegative integer form ( x , name = "embed check nonnegative integer form" ) : with tf . name scope ( name ) : x = tf . convert to tensor ( value = x , name = "x" ) assertions = [ assert util . assert non negative ( x , message = "'{}' must be non-negative." . format ( x ) ) , ] if not dtype util . is integer ( x . dtype ) : assertions += [ assert integer form ( x , message = "'{}' cannot contain fractional components." . format ( x ) ) , ] return with dependencies ( assertions , x )
def is known unsigned by dtype ( dt ) : return { tf . bool : True , tf . uint8 : True , tf . uint16 : True , } . get ( dt . base dtype , False )
def is known signed by dtype ( dt ) : return { tf . float16 : True , tf . float32 : True , tf . float64 : True , tf . int8 : True , tf . int16 : True , tf . int32 : True , tf . int64 : True , } . get ( dt . base dtype , False )
def largest integer by dtype ( dt ) : if not is known dtype ( dt ) : raise Type Error ( "Unrecognized dtype: {}" . format ( dt . name ) ) if dt . is floating : return int ( 2 ** ( np . finfo ( dt . as numpy dtype ) . nmant + 1 ) ) if dt . is integer : return np . iinfo ( dt . as numpy dtype ) . max if dt . base dtype == tf . bool : return int ( 1 ) # We actually can't land here but keep the case for completeness. raise Type Error ( "Unrecognized dtype: {}" . format ( dt . name ) )
def smallest integer by dtype ( dt ) : if not is known dtype ( dt ) : raise Type Error ( "Unrecognized dtype: {}" . format ( dt . name ) ) if is known unsigned by dtype ( dt ) : return 0 return - 1 * largest integer by dtype ( dt )
def is integer like by dtype ( dt ) : if not is known dtype ( dt ) : raise Type Error ( "Unrecognized dtype: {}" . format ( dt . name ) ) return dt . is integer or dt . base dtype == tf . bool
def gen new seed ( seed , salt ) : if seed is None : return None string = ( str ( seed ) + salt ) . encode ( "utf-8" ) return int ( hashlib . md5 ( string ) . hexdigest ( ) [ : 8 ] , 16 ) & 0x7FFFFFFF
def dimension size ( x , axis ) : # Since tf.gather isn't "constant-in, constant-out", we must first check the # static shape or fallback to dynamic shape. s = tf . compat . dimension value ( tensorshape util . with rank at least ( x . shape , np . abs ( axis ) ) [ axis ] ) if s is not None : return s return tf . shape ( input = x ) [ axis ]
def maybe validate rightmost transposed ndims ( rightmost transposed ndims , validate args , name = None ) : with tf . name scope ( name or 'maybe validate rightmost transposed ndims' ) : assertions = [ ] if not dtype util . is integer ( rightmost transposed ndims . dtype ) : raise Type Error ( '`rightmost transposed ndims` must be integer type.' ) if tensorshape util . rank ( rightmost transposed ndims . shape ) is not None : if tensorshape util . rank ( rightmost transposed ndims . shape ) != 0 : raise Value Error ( '`rightmost transposed ndims` must be a scalar, ' 'saw rank: {}.' . format ( tensorshape util . rank ( rightmost transposed ndims . shape ) ) ) elif validate args : assertions += [ assert util . assert rank ( rightmost transposed ndims , 0 ) ] rightmost transposed ndims = tf . get static value ( rightmost transposed ndims ) msg = '`rightmost transposed ndims` must be non-negative.' if rightmost transposed ndims is not None : if rightmost transposed ndims < 0 : raise Value Error ( msg [ : - 1 ] + ', saw: {}.' . format ( rightmost transposed ndims ) ) elif validate args : assertions += [ assert util . assert non negative ( rightmost transposed ndims , message = msg ) ] return assertions
def maybe validate perm ( perm , validate args , name = None ) : with tf . name scope ( name or 'maybe validate perm' ) : assertions = [ ] if not dtype util . is integer ( perm . dtype ) : raise Type Error ( '`perm` must be integer type' ) msg = '`perm` must be a vector.' if tensorshape util . rank ( perm . shape ) is not None : if tensorshape util . rank ( perm . shape ) != 1 : raise Value Error ( msg [ : - 1 ] + ', saw rank: {}.' . format ( tensorshape util . rank ( perm . shape ) ) ) elif validate args : assertions += [ assert util . assert rank ( perm , 1 , message = msg ) ] perm = tf . get static value ( perm ) msg = '`perm` must be a valid permutation vector.' if perm is not None : if not np . all ( np . arange ( np . size ( perm ) ) == np . sort ( perm ) ) : raise Value Error ( msg [ : - 1 ] + ', saw: {}.' . format ( perm ) ) elif validate args : assertions += [ assert util . assert equal ( tf . sort ( perm ) , tf . range ( tf . size ( input = perm ) ) , message = msg ) ] return assertions
def event shape ( self , shape , static perm to shape ) : rightmost = tf . get static value ( self . rightmost transposed ndims ) if tensorshape util . rank ( shape ) is None or rightmost is None : return tf . Tensor Shape ( None ) if tensorshape util . rank ( shape ) < rightmost : raise Value Error ( 'Invalid shape: min event ndims={} but got {}' . format ( rightmost , shape ) ) perm = tf . get static value ( self . perm , partial = True ) if perm is None : return shape [ : tensorshape util . rank ( shape ) - rightmost ] . concatenate ( [ None ] * int ( rightmost ) ) # We can use elimination to reidentify a single None dimension. if sum ( p is None for p in perm ) == 1 : present = np . argsort ( [ - 1 if p is None else p for p in perm ] ) for i , p in enumerate ( present [ 1 : ] ) : # The -1 sorts to position 0. if i != p : perm = [ i if p is None else p for p in perm ] break return shape [ : tensorshape util . rank ( shape ) - rightmost ] . concatenate ( static perm to shape ( shape [ tensorshape util . rank ( shape ) - rightmost : ] , perm ) )
def check equal shape ( name , static shape , dynamic shape , static target shape , dynamic target shape = None ) : static target shape = tf . Tensor Shape ( static target shape ) if tensorshape util . is fully defined ( static shape ) and tensorshape util . is fully defined ( static target shape ) : if static shape != static target shape : raise Value Error ( "{}: required shape {} but found {}" . format ( name , static target shape , static shape ) ) return None else : if dynamic target shape is None : if tensorshape util . is fully defined ( static target shape ) : dynamic target shape = tensorshape util . as list ( static target shape ) else : raise Value Error ( "{}: cannot infer target shape: no dynamic shape " "specified and static shape {} is not fully defined" . format ( name , static target shape ) ) return assert util . assert equal ( dynamic shape , dynamic target shape , message = ( "{}: required shape {}" . format ( name , static target shape ) ) )
def kalman transition ( filtered mean , filtered cov , transition matrix , transition noise ) : predicted mean = propagate mean ( filtered mean , transition matrix , transition noise ) predicted cov = propagate cov ( filtered cov , transition matrix , transition noise ) return predicted mean , predicted cov
def propagate mean ( mean , linop , dist ) : return linop . matmul ( mean ) + dist . mean ( ) [ ... , tf . newaxis ]
def propagate cov ( cov , linop , dist ) : # For linop A and input cov P, returns `A P A' + dist.cov()` return linop . matmul ( linop . matmul ( cov ) , adjoint arg = True ) + dist . covariance ( )
def joint sample n ( self , n , seed = None ) : with tf . name scope ( "sample n joint" ) : stream = seed stream . Seed Stream ( seed , salt = "Linear Gaussian State Space Model sample n joint" ) sample and batch shape = distribution util . prefer static value ( tf . concat ( [ [ n ] , self . batch shape tensor ( ) ] , axis = 0 ) ) # Sample the initial timestep from the prior.  Since we want # this sample to have full batch shape (not just the batch shape # of the self.initial state prior object which might in general be # smaller), we augment the sample shape to include whatever # extra batch dimensions are required. with tf . control dependencies ( self . runtime assertions ) : initial latent = self . initial state prior . sample ( sample shape = augment sample shape ( self . initial state prior , sample and batch shape , self . validate args ) , seed = stream ( ) ) # Add a dummy dimension so that matmul() does matrix-vector # multiplication. initial latent = initial latent [ ... , tf . newaxis ] initial observation matrix = ( self . get observation matrix for timestep ( self . initial step ) ) initial observation noise = ( self . get observation noise for timestep ( self . initial step ) ) initial observation pred = initial observation matrix . matmul ( initial latent ) initial observation = ( initial observation pred + initial observation noise . sample ( sample shape = augment sample shape ( initial observation noise , sample and batch shape , self . validate args ) , seed = stream ( ) ) [ ... , tf . newaxis ] ) sample step = build kalman sample step ( self . get transition matrix for timestep , self . get transition noise for timestep , self . get observation matrix for timestep , self . get observation noise for timestep , full sample and batch shape = sample and batch shape , stream = stream , validate args = self . validate args ) # Scan over all timesteps to sample latents and observations. ( latents , observations ) = tf . scan ( sample step , elems = tf . range ( self . initial step + 1 , self . final step ) , initializer = ( initial latent , initial observation ) ) # Combine the initial sampled timestep with the remaining timesteps. latents = tf . concat ( [ initial latent [ tf . newaxis , ... ] , latents ] , axis = 0 ) observations = tf . concat ( [ initial observation [ tf . newaxis , ... ] , observations ] , axis = 0 ) # Put dimensions back in order. The samples we've computed are # ordered by timestep, with shape `[num timesteps, num samples, # batch shape, size, 1]` where `size` represents `latent size` # or `observation size` respectively. But timesteps are really # part of each probabilistic event, so we need to return a Tensor # of shape `[num samples, batch shape, num timesteps, size]`. latents = tf . squeeze ( latents , - 1 ) latents = distribution util . move dimension ( latents , 0 , - 2 ) observations = tf . squeeze ( observations , - 1 ) observations = distribution util . move dimension ( observations , 0 , - 2 ) return latents , observations
def log normalization ( self ) : event dim = tf . compat . dimension value ( self . event shape [ 0 ] ) if event dim is None : raise Value Error ( 'v MF  log normalizer currently only supports ' 'statically known event shape' ) safe conc = tf . where ( self . concentration > 0 , self . concentration , tf . ones like ( self . concentration ) ) safe lognorm = ( ( event dim / 2 - 1 ) * tf . math . log ( safe conc ) - ( event dim / 2 ) * np . log ( 2 * np . pi ) - tf . math . log ( bessel ive ( event dim / 2 - 1 , safe conc ) ) - tf . abs ( safe conc ) ) log nsphere surface area = ( np . log ( 2. ) + ( event dim / 2 ) * np . log ( np . pi ) - tf . math . lgamma ( tf . cast ( event dim / 2 , self . dtype ) ) ) return tf . where ( self . concentration > 0 , - safe lognorm , log nsphere surface area * tf . ones like ( safe lognorm ) )
def maybe assert valid sample ( self , samples ) : if not self . validate args : return samples with tf . control dependencies ( [ assert util . assert near ( 1. , tf . linalg . norm ( tensor = samples , axis = - 1 ) , message = 'samples must be unit length' ) , assert util . assert equal ( tf . shape ( input = samples ) [ - 1 : ] , self . event shape tensor ( ) , message = ( 'samples must have innermost dimension matching that of ' '`self.mean direction`' ) ) , ] ) : return tf . identity ( samples )
def mode ( self ) : return ( self . mean direction + tf . zeros like ( self . concentration ) [ ... , tf . newaxis ] )
def rotate ( self , samples ) : event dim = ( tf . compat . dimension value ( self . event shape [ 0 ] ) or self . event shape tensor ( ) [ 0 ] ) basis = tf . concat ( [ [ 1. ] , tf . zeros ( [ event dim - 1 ] , dtype = self . dtype ) ] , axis = 0 ) , u = tf . nn . l2 normalize ( basis - self . mean direction , axis = - 1 ) return samples - 2 * tf . reduce sum ( input tensor = samples * u , axis = - 1 , keepdims = True ) * u
def sample 3d ( self , n , seed = None ) : seed = seed stream . Seed Stream ( seed , salt = 'von mises fisher 3d' ) u shape = tf . concat ( [ [ n ] , self . batch shape tensor ( ) ] , axis = 0 ) z = tf . random . uniform ( u shape , seed = seed ( ) , dtype = self . dtype ) # TODO(bjp): Higher-order odd dim analytic CD Fs are available in [1], could # be bisected for bounded sampling runtime (i.e. not rejection sampling). # [1]: Inversion sampler via: https://ieeexplore.ieee.org/document/7347705/ # The inversion is: u = 1 + log(z + (1-z)*exp(-2*kappa)) / kappa # We must protect against both kappa and z being zero. safe conc = tf . where ( self . concentration > 0 , self . concentration , tf . ones like ( self . concentration ) ) safe z = tf . where ( z > 0 , z , tf . ones like ( z ) ) safe u = 1 + tf . reduce logsumexp ( input tensor = [ tf . math . log ( safe z ) , tf . math . log1p ( - safe z ) - 2 * safe conc ] , axis = 0 ) / safe conc # Limit of the above expression as kappa->0 is 2*z-1 u = tf . where ( self . concentration > tf . zeros like ( safe u ) , safe u , 2 * z - 1 ) # Limit of the expression as z->0 is -1. u = tf . where ( tf . equal ( z , 0 ) , - tf . ones like ( u ) , u ) if not self . allow nan stats : u = tf . debugging . check numerics ( u , 'u in  sample 3d' ) return u [ ... , tf . newaxis ]
def remove dict keys with value ( dict , val ) : return { k : v for k , v in dict . items ( ) if v is not val }
def recursively replace dict for pretty dict ( x ) : # We use "Pretty Dict" because collections.Ordered Dict repr/str has the word # "Ordered Dict" in it. We only want to print "Ordered Dict" if in fact the # input really is an Ordered Dict. if isinstance ( x , dict ) : return Pretty Dict ( { k : recursively replace dict for pretty dict ( v ) for k , v in x . items ( ) } ) if ( isinstance ( x , collections . Sequence ) and not isinstance ( x , six . string types ) ) : args = ( recursively replace dict for pretty dict ( x ) for x in x ) is named tuple = ( isinstance ( x , tuple ) and hasattr ( x , " asdict" ) and hasattr ( x , " fields" ) ) return type ( x ) ( * args ) if is named tuple else type ( x ) ( args ) if isinstance ( x , collections . Mapping ) : return type ( x ) ( * * { k : recursively replace dict for pretty dict ( v ) for k , v in x . items ( ) } ) return x
def get samples ( dist , z , n , seed ) : with tf . compat . v1 . name scope ( 'get samples' , values = [ z , n ] ) : if ( n is None ) == ( z is None ) : raise Value Error ( 'Must specify exactly one of arguments "n" and "z".  Found: ' 'n = %s, z = %s' % ( n , z ) ) if n is not None : return dist . sample ( n , seed = seed ) else : return tf . convert to tensor ( value = z , name = 'z' )
def is namedtuple like ( x ) : try : for fn in x . fields : = getattr ( x , fn ) return True except Attribute Error : return False
def make name ( super name , default super name , sub name ) : name = super name if super name is not None else default super name if sub name is not None : name += ' ' + sub name return name
def choose base case ( is accepted , accepted , rejected , name = None ) : def expand is accepted like ( x ) : """Helper to expand `is accepted` like the shape of some input arg.""" with tf . compat . v1 . name scope ( 'expand is accepted like' ) : expand shape = tf . concat ( [ tf . shape ( input = is accepted ) , tf . ones ( [ tf . rank ( x ) - tf . rank ( is accepted ) ] , dtype = tf . int32 ) , ] , axis = 0 ) multiples = tf . concat ( [ tf . ones ( [ tf . rank ( is accepted ) ] , dtype = tf . int32 ) , tf . shape ( input = x ) [ tf . rank ( is accepted ) : ] , ] , axis = 0 ) m = tf . tile ( tf . reshape ( is accepted , expand shape ) , multiples ) m . set shape ( m . shape . merge with ( x . shape ) ) return m def where ( accepted , rejected ) : if accepted is rejected : return accepted accepted = tf . convert to tensor ( value = accepted , name = 'accepted' ) rejected = tf . convert to tensor ( value = rejected , name = 'rejected' ) r = tf . where ( expand is accepted like ( accepted ) , accepted , rejected ) r . set shape ( r . shape . merge with ( accepted . shape . merge with ( rejected . shape ) ) ) return r with tf . compat . v1 . name scope ( name , 'choose' , values = [ is accepted , accepted , rejected ] ) : if not is list like ( accepted ) : return where ( accepted , rejected ) return [ ( choose ( is accepted , a , r , name = name ) if is namedtuple like ( a ) else where ( a , r ) ) for a , r in zip ( accepted , rejected ) ]
def choose ( is accepted , accepted , rejected , name = None ) : if not is namedtuple like ( accepted ) : return choose base case ( is accepted , accepted , rejected , name = name ) if not isinstance ( accepted , type ( rejected ) ) : raise Type Error ( 'Type of `accepted` ({}) must be identical to ' 'type of `rejected` ({})' . format ( type ( accepted ) . name , type ( rejected ) . name ) ) return type ( accepted ) ( * * dict ( [ ( fn , choose ( is accepted , getattr ( accepted , fn ) , getattr ( rejected , fn ) , name = name ) ) for fn in accepted . fields ] ) )
def value and gradients ( fn , fn arg list , result = None , grads = None , name = None ) : with tf . compat . v1 . name scope ( name , 'value and gradients' , [ fn arg list , result , grads ] ) : def convert to tensor ( x , name ) : ctt = lambda x : x if x is None else tf . convert to tensor ( value = x , name = name ) return [ ctt ( x ) for x in x ] if is list like ( x ) else ctt ( x ) fn arg list = ( list ( fn arg list ) if is list like ( fn arg list ) else [ fn arg list ] ) fn arg list = convert to tensor ( fn arg list , 'fn arg' ) if result is None : result = fn ( * fn arg list ) if grads is None and tf . executing eagerly ( ) : # Ensure we disable bijector cacheing in eager mode. # TODO(b/72831017): Remove this once bijector cacheing is fixed for # eager mode. fn arg list = [ 0 + x for x in fn arg list ] result = convert to tensor ( result , 'fn result' ) if grads is not None : grads = convert to tensor ( grads , 'fn grad' ) return result , grads if is list like ( result ) and len ( result ) == len ( fn arg list ) : # Compute the block diagonal of Jacobian. # TODO(b/79158574): Guard this calculation by an arg which explicitly # requests block diagonal Jacobian calculation. def fn slice ( i ) : """Needed to prevent `cell-var-from-loop` pylint warning.""" return lambda x : fn ( * ( fn arg list [ : i ] + [ x ] + fn arg list [ i + 1 : ] ) ) grads = [ tfp math value and gradients ( fn slice ( i ) , fn arg list [ i ] ) [ 1 ] for i in range ( len ( result ) ) ] else : , grads = tfp math value and gradients ( fn , fn arg list ) return result , grads
def maybe call fn and grads ( fn , fn arg list , result = None , grads = None , check non none grads = True , name = None ) : with tf . compat . v1 . name scope ( name , 'maybe call fn and grads' , [ fn arg list , result , grads ] ) : fn arg list = ( list ( fn arg list ) if is list like ( fn arg list ) else [ fn arg list ] ) result , grads = value and gradients ( fn , fn arg list , result , grads ) if not all ( r . dtype . is floating for r in ( result if is list like ( result ) else [ result ] ) ) : # pylint: disable=superfluous-parens raise Type Error ( 'Function result must be a `Tensor` with `float` ' '`dtype`.' ) if len ( fn arg list ) != len ( grads ) : raise Value Error ( 'Function args must be in one-to-one correspondence ' 'with grads.' ) if check non none grads and any ( g is None for g in grads ) : raise Value Error ( 'Encountered `None` gradient.\n' '  fn arg list: {}\n' '  grads: {}' . format ( fn arg list , grads ) ) return result , grads
def maybe check valid shape ( shape , validate args ) : if not dtype util . is integer ( shape . dtype ) : raise Type Error ( '{} dtype ({}) should be `int`-like.' . format ( shape , dtype util . name ( shape . dtype ) ) ) assertions = [ ] message = '`{}` rank should be <= 1.' if tensorshape util . rank ( shape . shape ) is not None : if tensorshape util . rank ( shape . shape ) > 1 : raise Value Error ( message . format ( shape ) ) elif validate args : assertions . append ( assert util . assert less ( tf . rank ( shape ) , 2 , message = message . format ( shape ) ) ) shape = tf . get static value ( shape ) message = '`{}` elements must have at most one `-1`.' if shape is not None : if sum ( shape == - 1 ) > 1 : raise Value Error ( message . format ( shape ) ) elif validate args : assertions . append ( assert util . assert less ( tf . reduce sum ( input tensor = tf . cast ( tf . equal ( shape , - 1 ) , tf . int32 ) ) , 2 , message = message . format ( shape ) ) ) message = '`{}` elements must be either positive integers or `-1`.' if shape is not None : if np . any ( shape < - 1 ) : raise Value Error ( message . format ( shape ) ) elif validate args : assertions . append ( assert util . assert greater ( shape , - 2 , message = message . format ( shape ) ) ) return assertions
def maybe assert valid sample ( self , x ) : if not self . validate args : return x return distribution util . with dependencies ( [ assert util . assert positive ( x , message = "sample must be positive" ) , assert util . assert less ( x , 1. , message = "sample must be less than `1`." ) , ] , x )
def converged any ( converged , failed ) : return ( tf . reduce any ( input tensor = converged ) | tf . reduce all ( input tensor = failed ) )
def update position ( state , position delta , next objective , next gradient , grad tolerance , f relative tolerance , x tolerance ) : failed = state . failed | ~ tf . math . is finite ( next objective ) | ~ tf . reduce all ( input tensor = tf . math . is finite ( next gradient ) , axis = - 1 ) next position = state . position + position delta converged = ~ failed & check convergence ( state . position , next position , state . objective value , next objective , next gradient , grad tolerance , f relative tolerance , x tolerance ) return update fields ( state , converged = state . converged | converged , failed = failed , position = next position , objective value = next objective , objective gradient = next gradient )
def check convergence ( current position , next position , current objective , next objective , next gradient , grad tolerance , f relative tolerance , x tolerance ) : grad converged = norm ( next gradient , dims = 1 ) <= grad tolerance x converged = norm ( next position - current position , dims = 1 ) <= x tolerance f converged = ( norm ( next objective - current objective , dims = 0 ) <= f relative tolerance * current objective ) return grad converged | x converged | f converged
def get field ( kernel results , field name ) : if hasattr ( kernel results , field name ) : return getattr ( kernel results , field name ) if hasattr ( kernel results , 'accepted results' ) : return getattr ( kernel results . accepted results , field name ) raise Type Error ( 'Cannot extract %s from %s' % ( field name , kernel results ) )
def get exchanged states ( self , old states , exchange proposed , exchange proposed n , sampled replica states , sampled replica results ) : with tf . compat . v1 . name scope ( 'get exchanged states' ) : target log probs = [ ] for replica in range ( self . num replica ) : replica log prob = get field ( sampled replica results [ replica ] , 'target log prob' ) inverse temp = self . inverse temperatures [ replica ] target log probs . append ( replica log prob / inverse temp ) target log probs = tf . stack ( target log probs , axis = 0 ) dtype = target log probs . dtype num state parts = len ( sampled replica states [ 0 ] ) # exchanged states[k][i] is Tensor of (new) state part k, for replica i. # The `k` will be known statically, and `i` is a Tensor. # We will insert values into indices `i` for every replica with a proposed # exchange. exchanged states = [ tf . Tensor Array ( dtype , size = self . num replica , dynamic size = False , tensor array name = 'exchanged states' , # State part k has same shape, regardless of replica.  So use 0. element shape = sampled replica states [ 0 ] [ k ] . shape ) for k in range ( num state parts ) ] # Draw random variables here, to avoid sampling in the loop (and losing # reproducibility).  This may mean we sample too many, but we will always # have enough. sample shape = tf . concat ( ( [ self . num replica // 2 ] , tf . shape ( input = target log probs ) [ 1 : ] ) , axis = 0 ) log uniforms = tf . math . log ( tf . random . uniform ( shape = sample shape , dtype = dtype , seed = self . seed stream ( ) ) ) def swap ( is exchange accepted , x , y ) : """Swap batches of x, y where accepted.""" with tf . compat . v1 . name scope ( 'swap where exchange accepted' ) : new x = mcmc util . choose ( is exchange accepted , y , x ) new y = mcmc util . choose ( is exchange accepted , x , y ) return new x , new y def cond ( i , unused exchanged states ) : return i < exchange proposed n def body ( i , exchanged states ) : """Body of while loop for exchanging states.""" # Propose exchange between replicas indexed by m and n. m , n = tf . unstack ( exchange proposed [ i ] ) # Construct log accept ratio:  -temp diff * target log prob diff. # Note target log prob diff = -Energy Diff (common definition is in terms # of energy). temp diff = self . inverse temperatures [ m ] - self . inverse temperatures [ n ] # Difference of target log probs may be +- Inf or Na N.  We want the # product of this with the temperature difference to have "alt value" of # -Inf. log accept ratio = mcmc util . safe sum ( [ - temp diff * target log probs [ m ] , temp diff * target log probs [ n ] ] ) is exchange accepted = log uniforms [ i ] < log accept ratio for k in range ( num state parts ) : new m , new n = swap ( is exchange accepted , old states [ k ] . read ( m ) , old states [ k ] . read ( n ) ) exchanged states [ k ] = exchanged states [ k ] . write ( m , new m ) exchanged states [ k ] = exchanged states [ k ] . write ( n , new n ) return i + 1 , exchanged states # At this point, exchanged states[k] is a length num replicas Tensor Array. return tf . while loop ( cond = cond , body = body , loop vars = [ tf . constant ( 0 ) , exchanged states ] ) [ 1 ]
def variance scale term ( self ) : # Expand back the last dim so the shape of  variance scale term matches the # shape of self.concentration. c0 = self . total concentration [ ... , tf . newaxis ] return tf . sqrt ( ( 1. + c0 / self . total count [ ... , tf . newaxis ] ) / ( 1. + c0 ) )
def maybe assert valid concentration ( self , concentration , validate args ) : if not validate args : return concentration concentration = distribution util . embed check categorical event shape ( concentration ) return distribution util . with dependencies ( [ assert util . assert positive ( concentration , message = "Concentration parameter must be positive." ) , ] , concentration )
def maybe assert valid sample ( self , counts ) : if not self . validate args : return counts counts = distribution util . embed check nonnegative integer form ( counts ) return distribution util . with dependencies ( [ assert util . assert equal ( self . total count , tf . reduce sum ( input tensor = counts , axis = - 1 ) , message = "counts last-dimension must sum to `self.total count`" ) , ] , counts )
def forward log det jacobian fn ( bijector ) : if not mcmc util . is list like ( bijector ) : bijector = [ bijector ] def fn ( transformed state parts , event ndims ) : return sum ( [ b . forward log det jacobian ( sp , event ndims = e ) for b , e , sp in zip ( bijector , event ndims , transformed state parts ) ] ) return fn
def forward transform fn ( bijector ) : if not mcmc util . is list like ( bijector ) : bijector = [ bijector ] def fn ( transformed state parts ) : return [ b . forward ( sp ) for b , sp in zip ( bijector , transformed state parts ) ] return fn
def inverse transform fn ( bijector ) : if not mcmc util . is list like ( bijector ) : bijector = [ bijector ] def fn ( state parts ) : return [ b . inverse ( sp ) for b , sp in zip ( bijector , state parts ) ] return fn
def val where ( cond , tval , fval ) : if isinstance ( tval , tf . Tensor ) : return tf . where ( cond , tval , fval ) elif isinstance ( tval , tuple ) : cls = type ( tval ) return cls ( * ( val where ( cond , t , f ) for t , f in zip ( tval , fval ) ) ) else : raise Exception ( Type Error )
def secant2 inner ( value and gradients function , initial args , val 0 , val c , f lim , sufficient decrease param , curvature param ) : # Apply the `update` function on active branch members to squeeze their # bracketing interval. update result = update ( value and gradients function , initial args . left , initial args . right , val c , f lim , active = initial args . active ) # Update active and failed flags, update left/right on non-failed entries. active = initial args . active & ~ update result . failed failed = initial args . failed | update result . failed val left = val where ( active , update result . left , initial args . left ) val right = val where ( active , update result . right , initial args . right ) # Check if new `c` points should be generated. updated left = active & tf . equal ( val left . x , val c . x ) updated right = active & tf . equal ( val right . x , val c . x ) is new = updated left | updated right next c = tf . where ( updated left , secant ( initial args . left , val left ) , val c . x ) next c = tf . where ( updated right , secant ( initial args . right , val right ) , next c ) in range = ( val left . x <= next c ) & ( next c <= val right . x ) # Figure out if an extra function evaluation is needed for new `c` points. needs extra eval = tf . reduce any ( input tensor = in range & is new ) num evals = initial args . num evals + update result . num evals num evals = num evals + tf . cast ( needs extra eval , num evals . dtype ) next args = Secant2Result ( active = active & in range , # No longer active if `c` is out of range. converged = initial args . converged , failed = failed , num evals = num evals , left = val left , right = val right ) def apply inner update ( ) : next val c = prefer static . cond ( needs extra eval , ( lambda : value and gradients function ( next c ) ) , ( lambda : val c ) ) return secant2 inner update ( value and gradients function , next args , val 0 , next val c , f lim , sufficient decrease param , curvature param ) return prefer static . cond ( tf . reduce any ( input tensor = next args . active ) , apply inner update , lambda : next args )
def secant2 inner update ( value and gradients function , initial args , val 0 , val c , f lim , sufficient decrease param , curvature param ) : # Fail if `val c` is no longer finite. new failed = initial args . active & ~ is finite ( val c ) active = initial args . active & ~ new failed failed = initial args . failed | new failed # We converge when we find a point satisfying the Wolfe conditions, in those # cases we set `val left = val right = val c`. found wolfe = active & satisfies wolfe ( val 0 , val c , f lim , sufficient decrease param , curvature param ) val left = val where ( found wolfe , val c , initial args . left ) val right = val where ( found wolfe , val c , initial args . right ) converged = initial args . converged | found wolfe active = active & ~ found wolfe # If any active batch members remain, we apply the `update` function to # squeeze further their corresponding left/right bracketing interval. def apply update ( ) : update result = update ( value and gradients function , val left , val right , val c , f lim , active = active ) return Secant2Result ( active = tf . zeros like ( active ) , # End of secant2, no actives anymore. converged = converged , failed = failed | update result . failed , num evals = initial args . num evals + update result . num evals , left = update result . left , right = update result . right ) # Otherwise just return the current results. def default ( ) : return Secant2Result ( active = active , converged = converged , failed = failed , num evals = initial args . num evals , left = val left , right = val right ) return prefer static . cond ( tf . reduce any ( input tensor = active ) , apply update , default )
def bisect ( value and gradients function , initial args , f lim ) : def loop cond ( curr ) : # TODO(b/112524024): Also take into account max iterations. return ~ tf . reduce all ( input tensor = curr . stopped ) def loop body ( curr ) : """Narrow down interval to satisfy opposite slope conditions.""" mid = value and gradients function ( ( curr . left . x + curr . right . x ) / 2 ) # Fail if function values at mid point are no longer finite; or left/right # points are so close to it that we can't distinguish them any more. failed = ( curr . failed | ~ is finite ( mid ) | tf . equal ( mid . x , curr . left . x ) | tf . equal ( mid . x , curr . right . x ) ) # If mid point has a negative slope and the function value at that point is # small enough, we can use it as a new left end point to narrow down the # interval. If mid point has a positive slope, then we have found a suitable # right end point to bracket a minima within opposite slopes. Otherwise, the # mid point has a negative slope but the function value at that point is too # high to work as left end point, we are in the same situation in which we # started the loop so we just update the right end point and continue. to update = ~ ( curr . stopped | failed ) update left = ( mid . df < 0 ) & ( mid . f <= f lim ) left = val where ( to update & update left , mid , curr . left ) right = val where ( to update & ~ update left , mid , curr . right ) # We're done when the right end point has a positive slope. stopped = curr . stopped | failed | ( right . df >= 0 ) return [ Intermediate Result ( iteration = curr . iteration , stopped = stopped , failed = failed , num evals = curr . num evals + 1 , left = left , right = right ) ] # The interval needs updating if the right end point has a negative slope and # the value of the function at that point is too high. It is not a valid left # end point but along with the current left end point, it encloses another # minima. The loop above tries to narrow the interval so that it satisfies the # opposite slope conditions. return tf . while loop ( cond = loop cond , body = loop body , loop vars = [ initial args ] ) [ 0 ]
def prepare args ( target log prob fn , state , step size , target log prob = None , grads target log prob = None , maybe expand = False , state gradients are stopped = False ) : state parts = list ( state ) if mcmc util . is list like ( state ) else [ state ] state parts = [ tf . convert to tensor ( value = s , name = 'current state' ) for s in state parts ] if state gradients are stopped : state parts = [ tf . stop gradient ( x ) for x in state parts ] target log prob , grads target log prob = mcmc util . maybe call fn and grads ( target log prob fn , state parts , target log prob , grads target log prob ) step sizes = ( list ( step size ) if mcmc util . is list like ( step size ) else [ step size ] ) step sizes = [ tf . convert to tensor ( value = s , name = 'step size' , dtype = target log prob . dtype ) for s in step sizes ] if len ( step sizes ) == 1 : step sizes *= len ( state parts ) if len ( state parts ) != len ( step sizes ) : raise Value Error ( 'There should be exactly one `step size` or it should ' 'have same length as `current state`.' ) def maybe flatten ( x ) : return x if maybe expand or mcmc util . is list like ( state ) else x [ 0 ] return [ maybe flatten ( state parts ) , maybe flatten ( step sizes ) , target log prob , grads target log prob , ]
def bootstrap results ( self , init state ) : kernel results = self . impl . bootstrap results ( init state ) if self . step size update fn is not None : step size assign = self . step size update fn ( self . step size , None ) # pylint: disable=not-callable kernel results = kernel results . replace ( extra = Hamiltonian Monte Carlo Extra Kernel Results ( step size assign = step size assign ) ) return kernel results
def resnet block ( x , filters , kernel , stride , kernel posterior fn ) : x = tf . keras . layers . Batch Normalization ( ) ( x ) x = tf . keras . layers . Activation ( 'relu' ) ( x ) if stride != 1 or filters != x . shape [ 1 ] : shortcut = projection shortcut ( x , filters , stride , kernel posterior fn ) else : shortcut = x x = tfp . layers . Convolution2D Flipout ( filters , kernel , strides = stride , padding = 'same' , kernel posterior fn = kernel posterior fn ) ( x ) x = tf . keras . layers . Batch Normalization ( ) ( x ) x = tf . keras . layers . Activation ( 'relu' ) ( x ) x = tfp . layers . Convolution2D Flipout ( filters , kernel , strides = 1 , padding = 'same' , kernel posterior fn = kernel posterior fn ) ( x ) x = tf . keras . layers . add ( [ x , shortcut ] ) return x
def deep exponential family ( data size , feature size , units , shape ) : w2 = ed . Gamma ( 0.1 , 0.3 , sample shape = [ units [ 2 ] , units [ 1 ] ] , name = "w2" ) w1 = ed . Gamma ( 0.1 , 0.3 , sample shape = [ units [ 1 ] , units [ 0 ] ] , name = "w1" ) w0 = ed . Gamma ( 0.1 , 0.3 , sample shape = [ units [ 0 ] , feature size ] , name = "w0" ) z2 = ed . Gamma ( 0.1 , 0.1 , sample shape = [ data size , units [ 2 ] ] , name = "z2" ) z1 = ed . Gamma ( shape , shape / tf . matmul ( z2 , w2 ) , name = "z1" ) z0 = ed . Gamma ( shape , shape / tf . matmul ( z1 , w1 ) , name = "z0" ) x = ed . Poisson ( tf . matmul ( z0 , w0 ) , name = "x" ) return x
def trainable positive deterministic ( shape , min loc = 1e-3 , name = None ) : with tf . compat . v1 . variable scope ( None , default name = "trainable positive deterministic" ) : unconstrained loc = tf . compat . v1 . get variable ( "unconstrained loc" , shape ) loc = tf . maximum ( tf . nn . softplus ( unconstrained loc ) , min loc ) rv = ed . Deterministic ( loc = loc , name = name ) return rv
def trainable gamma ( shape , min concentration = 1e-3 , min scale = 1e-5 , name = None ) : with tf . compat . v1 . variable scope ( None , default name = "trainable gamma" ) : unconstrained concentration = tf . compat . v1 . get variable ( "unconstrained concentration" , shape , initializer = tf . compat . v1 . initializers . random normal ( mean = 0.5 , stddev = 0.1 ) ) unconstrained scale = tf . compat . v1 . get variable ( "unconstrained scale" , shape , initializer = tf . compat . v1 . initializers . random normal ( stddev = 0.1 ) ) concentration = tf . maximum ( tf . nn . softplus ( unconstrained concentration ) , min concentration ) rate = tf . maximum ( 1. / tf . nn . softplus ( unconstrained scale ) , 1. / min scale ) rv = ed . Gamma ( concentration = concentration , rate = rate , name = name ) return rv
def registered kl ( type a , type b ) : hierarchy a = tf inspect . getmro ( type a ) hierarchy b = tf inspect . getmro ( type b ) dist to children = None kl fn = None for mro to a , parent a in enumerate ( hierarchy a ) : for mro to b , parent b in enumerate ( hierarchy b ) : candidate dist = mro to a + mro to b candidate kl fn = DIVERGENCES . get ( ( parent a , parent b ) , None ) if not kl fn or ( candidate kl fn and candidate dist < dist to children ) : dist to children = candidate dist kl fn = candidate kl fn return kl fn
def read image ( filepath ) : im bytes = tf . io . read file ( filepath ) im = tf . image . decode image ( im bytes , channels = CHANNELS ) im = tf . image . convert image dtype ( im , tf . float32 ) return im
def download sprites ( ) : filepath = os . path . join ( FLAGS . data dir , DATA SPRITES DIR ) if not tf . io . gfile . exists ( filepath ) : if not tf . io . gfile . exists ( FLAGS . data dir ) : tf . io . gfile . makedirs ( FLAGS . data dir ) zip name = "{}.zip" . format ( filepath ) urllib . request . urlretrieve ( DATA SPRITES URL , zip name ) with zipfile . Zip File ( zip name , "r" ) as zip file : zip file . extractall ( FLAGS . data dir ) tf . io . gfile . remove ( zip name ) return filepath
def create character ( skin , hair , top , pants ) : dtype = skin . dtype hair mask = tf . cast ( hair [ ... , - 1 : ] <= 0 , dtype ) top mask = tf . cast ( top [ ... , - 1 : ] <= 0 , dtype ) pants mask = tf . cast ( pants [ ... , - 1 : ] <= 0 , dtype ) char = ( skin * hair mask ) + hair char = ( char * top mask ) + top char = ( char * pants mask ) + pants return char
def create random seq ( character , action metadata , direction , length = 8 ) : start = tf . random . uniform ( [ ] , maxval = action metadata [ 1 ] , dtype = tf . int32 ) return create seq ( character , action metadata , direction , length , start )
def maybe validate distributions ( distributions , dtype override , validate args ) : assertions = [ ] if not is iterable ( distributions ) or not distributions : raise Value Error ( '`distributions` must be a list of one or more ' 'distributions.' ) if dtype override is None : dts = [ dtype util . base dtype ( d . dtype ) for d in distributions if d . dtype is not None ] if dts [ 1 : ] != dts [ : - 1 ] : raise Type Error ( 'Distributions must have same dtype; found: {}.' . format ( set ( dtype util . name ( dt ) for dt in dts ) ) ) # Validate event ndims. for d in distributions : if tensorshape util . rank ( d . event shape ) is not None : if tensorshape util . rank ( d . event shape ) != 1 : raise Value Error ( '`Distribution` must be vector variate, ' 'found event nimds: {}.' . format ( tensorshape util . rank ( d . event shape ) ) ) elif validate args : assertions . append ( assert util . assert equal ( 1 , tf . size ( input = d . event shape tensor ( ) ) , message = '`Distribution` must be vector variate.' ) ) batch shapes = [ d . batch shape for d in distributions ] if all ( tensorshape util . is fully defined ( b ) for b in batch shapes ) : if batch shapes [ 1 : ] != batch shapes [ : - 1 ] : raise Value Error ( 'Distributions must have the same `batch shape`; ' 'found: {}.' . format ( batch shapes ) ) elif validate args : batch shapes = [ tensorshape util . as list ( d . batch shape ) # pylint: disable=g-complex-comprehension if tensorshape util . is fully defined ( d . batch shape ) else d . batch shape tensor ( ) for d in distributions ] assertions . extend ( assert util . assert equal ( # pylint: disable=g-complex-comprehension b1 , b2 , message = 'Distribution `batch shape`s must be identical.' ) for b1 , b2 in zip ( batch shapes [ 1 : ] , batch shapes [ : - 1 ] ) ) return assertions
def build input pipeline ( x train , x test , y train , y test , batch size , valid size ) : x train = x train . astype ( "float32" ) x test = x test . astype ( "float32" ) x train /= 255 x test /= 255 y train = y train . flatten ( ) y test = y test . flatten ( ) if FLAGS . subtract pixel mean : x train mean = np . mean ( x train , axis = 0 ) x train -= x train mean x test -= x train mean print ( "x train shape:" + str ( x train . shape ) ) print ( str ( x train . shape [ 0 ] ) + " train samples" ) print ( str ( x test . shape [ 0 ] ) + " test samples" ) # Build an iterator over training batches. training dataset = tf . data . Dataset . from tensor slices ( ( x train , np . int32 ( y train ) ) ) training batches = training dataset . shuffle ( 50000 , reshuffle each iteration = True ) . repeat ( ) . batch ( batch size ) training iterator = tf . compat . v1 . data . make one shot iterator ( training batches ) # Build a iterator over the heldout set with batch size=heldout size, # i.e., return the entire heldout set as a constant. heldout dataset = tf . data . Dataset . from tensor slices ( ( x test , np . int32 ( y test ) ) ) heldout batches = heldout dataset . repeat ( ) . batch ( valid size ) heldout iterator = tf . compat . v1 . data . make one shot iterator ( heldout batches ) # Combine these into a feedable iterator that can switch between training # and validation inputs. handle = tf . compat . v1 . placeholder ( tf . string , shape = [ ] ) feedable iterator = tf . compat . v1 . data . Iterator . from string handle ( handle , training batches . output types , training batches . output shapes ) images , labels = feedable iterator . get next ( ) return images , labels , handle , training iterator , heldout iterator
def build fake data ( ) : num examples = 10 x train = np . random . rand ( num examples , * IMAGE SHAPE ) . astype ( np . float32 ) y train = np . random . permutation ( np . arange ( num examples ) ) . astype ( np . int32 ) x test = np . random . rand ( num examples , * IMAGE SHAPE ) . astype ( np . float32 ) y test = np . random . permutation ( np . arange ( num examples ) ) . astype ( np . int32 ) return ( x train , y train ) , ( x test , y test )
def sort tensor ( tensor ) : sorted , = tf . nn . top k ( tensor , k = tf . shape ( input = tensor ) [ - 1 ] ) sorted . set shape ( tensor . shape ) return sorted
def batch interp with gather nd ( x , x ref min , x ref max , y ref , nd , fill value , batch dims ) : dtype = x . dtype # In this function, # x.shape = [A1, ..., An, D, nd], where n = batch dims # and # y ref.shape = [A1, ..., An, C1, C2,..., Cnd, B1,...,BM] # y ref[A1, ..., An, i1,...,ind] is a shape [B1,...,BM] Tensor with the value # at index [i1,...,ind] in the interpolation table. #  and x ref max have shapes [A1, ..., An, nd]. # ny[k] is number of y reference points in interp dim k. ny = tf . cast ( tf . shape ( input = y ref ) [ batch dims : batch dims + nd ] , dtype ) # Map [x ref min, x ref max] to [0, ny - 1]. # This is the (fractional) index of x. # x idx unclipped[A1, ..., An, d, k] is the fractional index into dim k of # interpolation table for the dth x value. x ref min expanded = tf . expand dims ( x ref min , axis = - 2 ) x ref max expanded = tf . expand dims ( x ref max , axis = - 2 ) x idx unclipped = ( ny - 1 ) * ( x - x ref min expanded ) / ( x ref max expanded - x ref min expanded ) # Wherever x is Na N, x idx unclipped will be Na N as well. # Keep track of the nan indices here (so we can impute Na N later). # Also eliminate any Na N indices, since there is not Na N in 32bit. nan idx = tf . math . is nan ( x idx unclipped ) x idx unclipped = tf . where ( nan idx , tf . zeros like ( x idx unclipped ) , x idx unclipped ) # x idx.shape = [A1, ..., An, D, nd] x idx = tf . clip by value ( x idx unclipped , tf . zeros ( ( ) , dtype = dtype ) , ny - 1 ) # Get the index above and below x idx. # Naively we could set idx below = floor(x idx), idx above = ceil(x idx), # however, this results in idx below == idx above whenever x is on a grid. # This in turn results in y ref below == y ref above, and then the gradient # at this point is zero.  So here we "jitter" one of idx below, idx above, # so that they are at different values.  This jittering does not affect the # interpolated value, but does make the gradient nonzero (unless of course # the y ref values are the same). idx below = tf . floor ( x idx ) idx above = tf . minimum ( idx below + 1 , ny - 1 ) idx below = tf . maximum ( idx above - 1 , 0 ) # These are the values of y ref corresponding to above/below indices. # idx below int32.shape = x.shape[:-1] + [nd] idx below int32 = tf . cast ( idx below , dtype = tf . int32 ) idx above int32 = tf . cast ( idx above , dtype = tf . int32 ) # idx below list is a length nd list of shape x.shape[:-1] int32 tensors. idx below list = tf . unstack ( idx below int32 , axis = - 1 ) idx above list = tf . unstack ( idx above int32 , axis = - 1 ) # Use t to get a convex combination of the below/above values. # t.shape = [A1, ..., An, D, nd] t = x idx - idx below # x, and tensors shaped like x, need to be added to, and selected with # (using tf.where) the output y.  This requires appending singletons. def expand x fn ( tensor ) : # Reshape tensor to tensor.shape + [1] * M. extended shape = tf . concat ( [ tf . shape ( input = tensor ) , tf . ones like ( tf . shape ( input = y ref ) [ batch dims + nd : ] ) ] , axis = 0 ) return tf . reshape ( tensor , extended shape ) # Now, t.shape = [A1, ..., An, D, nd] + [1] * (rank(y ref) - nd - batch dims) t = expand x fn ( t ) s = 1 - t # Re-insert Na N wherever x was Na N. nan idx = expand x fn ( nan idx ) t = tf . where ( nan idx , tf . fill ( tf . shape ( input = t ) , tf . constant ( np . nan , dtype ) ) , t ) terms = [ ] # Our work above has located x's fractional index inside a cube of above/below # indices. The distance to the below indices is t, and to the above indices # is s. # Drawing lines from x to the cube walls, we get 2**nd smaller cubes. Each # term in the result is a product of a reference point, gathered from y ref, # multiplied by a volume.  The volume is that of the cube opposite to the # reference point.  E.g. if the reference point is below x in every axis, the # volume is that of the cube with corner above x in every axis, s[0]*...*s[nd] # We could probably do this with one massive gather, but that would be very # unreadable and un-debuggable.  It also would create a large Tensor. for zero ones list in binary count ( nd ) : gather from y ref idx = [ ] opposite volume t idx = [ ] opposite volume s idx = [ ] for k , zero or one in enumerate ( zero ones list ) : if zero or one == 0 : # If the kth iterate has zero or one = 0, # Will gather from the "below" reference point along axis k. gather from y ref idx . append ( idx below list [ k ] ) # Now append the index to gather for computing opposite volume. # This could be done by initializing opposite volume to 1, then here: #  opposite volume *= tf.gather(s, indices=k, axis=tf.rank(x) - 1) # but that puts a gather in the "inner loop."  Better to append the # index and do one larger gather down below. opposite volume s idx . append ( k ) else : gather from y ref idx . append ( idx above list [ k ] ) # Append an index to gather, having the same effect as #   opposite volume *= tf.gather(t, indices=k, axis=tf.rank(x) - 1) opposite volume t idx . append ( k ) # Compute opposite volume (volume of cube opposite the ref point): # Recall t.shape = s.shape = [D, nd] + [1, ..., 1] # Gather from t and s along the "nd" axis, which is rank(x) - 1. ov axis = tf . rank ( x ) - 1 opposite volume = ( tf . reduce prod ( input tensor = tf . gather ( t , indices = tf . cast ( opposite volume t idx , dtype = tf . int32 ) , axis = ov axis ) , axis = ov axis ) * tf . reduce prod ( input tensor = tf . gather ( s , indices = tf . cast ( opposite volume s idx , dtype = tf . int32 ) , axis = ov axis ) , axis = ov axis ) ) # pyformat: disable y ref pt = tf . gather nd ( y ref , tf . stack ( gather from y ref idx , axis = - 1 ) , batch dims = batch dims ) terms . append ( y ref pt * opposite volume ) y = tf . math . add n ( terms ) if tf . debugging . is numeric tensor ( fill value ) : # Recall x idx unclipped.shape = [D, nd], # so here we check if it was out of bounds in any of the nd dims. # Thus, oob idx.shape = [D]. oob idx = tf . reduce any ( input tensor = ( x idx unclipped < 0 ) | ( x idx unclipped > ny - 1 ) , axis = - 1 ) # Now, y.shape = [D, B1,...,BM], so we'll have to broadcast oob idx. oob idx = expand x fn ( oob idx ) # Shape [D, 1,...,1] oob idx |= tf . fill ( tf . shape ( input = y ) , False ) y = tf . where ( oob idx , tf . fill ( tf . shape ( input = y ) , fill value ) , y ) return y
def assert ndims statically ( x , expect ndims = None , expect ndims at least = None , expect static = False ) : ndims = x . shape . ndims if ndims is None : if expect static : raise Value Error ( 'Expected static ndims. Found: {}' . format ( x ) ) return if expect ndims is not None and ndims != expect ndims : raise Value Error ( 'ndims must be {}.  Found: {}' . format ( expect ndims , ndims ) ) if expect ndims at least is not None and ndims < expect ndims at least : raise Value Error ( 'ndims must be at least {}. Found {}' . format ( expect ndims at least , ndims ) )
def batch gather with broadcast ( params , indices , axis ) : # batch gather assumes... #   params.shape =  [A1,...,AN, B1,...,BM] #   indices.shape = [A1,...,AN, C] # which gives output of shape #                   [A1,...,AN, C, B1,...,BM] # Here we broadcast dims of each to the left of `axis` in params, and left of # the rightmost dim in indices, e.g. we can # have #   params.shape =  [A1,...,AN, B1,...,BM] #   indices.shape = [a1,...,a N, C], # where ai broadcasts with Ai. # leading bcast shape is the broadcast of [A1,...,AN] and [a1,...,a N]. leading bcast shape = tf . broadcast dynamic shape ( tf . shape ( input = params ) [ : axis ] , tf . shape ( input = indices ) [ : - 1 ] ) params += tf . zeros ( tf . concat ( ( leading bcast shape , tf . shape ( input = params ) [ axis : ] ) , axis = 0 ) , dtype = params . dtype ) indices += tf . zeros ( tf . concat ( ( leading bcast shape , tf . shape ( input = indices ) [ - 1 : ] ) , axis = 0 ) , dtype = indices . dtype ) return tf . compat . v1 . batch gather ( params , indices )
def broadcast cat event and params ( event , params , base dtype ) : if dtype util . is integer ( event . dtype ) : pass elif dtype util . is floating ( event . dtype ) : # When `validate args=True` we've already ensured int/float casting # is closed. event = tf . cast ( event , dtype = tf . int32 ) else : raise Type Error ( "`value` should have integer `dtype` or " "`self.dtype` ({})" . format ( base dtype ) ) shape known statically = ( tensorshape util . rank ( params . shape ) is not None and tensorshape util . is fully defined ( params . shape [ : - 1 ] ) and tensorshape util . is fully defined ( event . shape ) ) if not shape known statically or params . shape [ : - 1 ] != event . shape : params *= tf . ones like ( event [ ... , tf . newaxis ] , dtype = params . dtype ) params shape = tf . shape ( input = params ) [ : - 1 ] event *= tf . ones ( params shape , dtype = event . dtype ) if tensorshape util . rank ( params . shape ) is not None : tensorshape util . set shape ( event , params . shape [ : - 1 ] ) return event , params
def broadcast event and samples ( event , samples , event ndims ) : # This is the shape of self.samples, without the samples axis, i.e. the shape # of the result of a call to dist.sample(). This way we can broadcast it with # event to get a properly-sized event, then add the singleton dim back at # -event ndims - 1. samples shape = tf . concat ( [ tf . shape ( input = samples ) [ : - event ndims - 1 ] , tf . shape ( input = samples ) [ tf . rank ( samples ) - event ndims : ] ] , axis = 0 ) event *= tf . ones ( samples shape , dtype = event . dtype ) event = tf . expand dims ( event , axis = - event ndims - 1 ) samples *= tf . ones like ( event , dtype = samples . dtype ) return event , samples
def update inv hessian ( prev state , next state ) : # Only update the inverse Hessian if not already failed or converged. should update = ~ next state . converged & ~ next state . failed # Compute the normalization term (y^T . s), should not update if is singular. gradient delta = next state . objective gradient - prev state . objective gradient position delta = next state . position - prev state . position normalization factor = tf . reduce sum ( input tensor = gradient delta * position delta , axis = - 1 ) should update = should update & ~ tf . equal ( normalization factor , 0 ) def do update inv hessian ( ) : next inv hessian = bfgs inv hessian update ( gradient delta , position delta , normalization factor , prev state . inverse hessian estimate ) return bfgs utils . update fields ( next state , inverse hessian estimate = tf . where ( should update , next inv hessian , prev state . inverse hessian estimate ) ) return prefer static . cond ( tf . reduce any ( input tensor = should update ) , do update inv hessian , lambda : next state )
def get initial state ( value and gradients function , initial position , num correction pairs , tolerance ) : init args = bfgs utils . get initial state args ( value and gradients function , initial position , tolerance ) empty queue = make empty queue for ( num correction pairs , initial position ) init args . update ( position deltas = empty queue , gradient deltas = empty queue ) return L Bfgs Optimizer Results ( * * init args )
def von mises cdf series ( x , concentration , num terms , dtype ) : # Keep the number of terms as a float. It should be a small integer, so # exactly representable as a float. num terms = tf . cast ( num terms , dtype = dtype ) def loop body ( n , rn , drn dconcentration , vn , dvn dconcentration ) : """One iteration of the series loop.""" denominator = 2. * n / concentration + rn ddenominator dk = - 2. * n / concentration ** 2 + drn dconcentration rn = 1. / denominator drn dconcentration = - ddenominator dk / denominator ** 2 multiplier = tf . sin ( n * x ) / n + vn vn = rn * multiplier dvn dconcentration = ( drn dconcentration * multiplier + rn * dvn dconcentration ) n -= 1. return n , rn , drn dconcentration , vn , dvn dconcentration ( , , , vn , dvn dconcentration ) = tf . while loop ( cond = lambda n , * : n > 0. , body = loop body , loop vars = ( num terms , # n tf . zeros like ( x , name = "rn" ) , tf . zeros like ( x , name = "drn dconcentration" ) , tf . zeros like ( x , name = "vn" ) , tf . zeros like ( x , name = "dvn dconcentration" ) , ) , ) cdf = .5 + x / ( 2. * np . pi ) + vn / np . pi dcdf dconcentration = dvn dconcentration / np . pi # Clip the result to [0, 1]. cdf clipped = tf . clip by value ( cdf , 0. , 1. ) # The clipped values do not depend on concentration anymore, so set their # derivative to zero. dcdf dconcentration *= tf . cast ( ( cdf >= 0. ) & ( cdf <= 1. ) , dtype ) return cdf clipped , dcdf dconcentration
def von mises cdf normal ( x , concentration , dtype ) : def cdf func ( concentration ) : """A helper function that is passed to value and gradient.""" # z is an "almost Normally distributed" random variable. z = ( ( np . sqrt ( 2. / np . pi ) / tf . math . bessel i0e ( concentration ) ) * tf . sin ( .5 * x ) ) # This is the correction described in [1] which reduces the error # of the Normal approximation. z2 = z ** 2 z3 = z2 * z z4 = z2 ** 2 c = 24. * concentration c1 = 56. xi = z - z3 / ( ( c - 2. * z2 - 16. ) / 3. - ( z4 + ( 7. / 4. ) * z2 + 167. / 2. ) / ( c - c1 - z2 + 3. ) ) ** 2 distrib = normal . Normal ( tf . cast ( 0. , dtype ) , tf . cast ( 1. , dtype ) ) return distrib . cdf ( xi ) return value and gradient ( cdf func , concentration )
def get initial args ( objective function , initial population , initial position , population size , population stddev , max iterations , func tolerance , position tolerance , differential weight , crossover prob , seed ) : was iterable = False if initial position is not None : initial position , was iterable = ensure list ( initial position ) if initial population is not None : initial population , was iterable = ensure list ( initial population ) population = get starting population ( initial population , initial position , population size , population stddev , seed = seed ) differential weight = tf . convert to tensor ( value = differential weight , dtype = population [ 0 ] . dtype . base dtype ) crossover prob = tf . convert to tensor ( value = crossover prob ) population values = objective function ( * population ) if max iterations is not None : max iterations = tf . convert to tensor ( value = max iterations ) func tolerance = tf . convert to tensor ( value = func tolerance , dtype = population values . dtype . base dtype ) position tolerance = tf . convert to tensor ( value = position tolerance , dtype = population [ 0 ] . dtype . base dtype ) return ( was iterable , population , population values , max iterations , func tolerance , position tolerance , differential weight , crossover prob )
def find best in population ( population , values ) : best value = tf . math . reduce min ( input tensor = values ) best index = tf . where ( tf . math . equal ( values , best value ) ) [ 0 , 0 ] return ( [ population part [ best index ] for population part in population ] , best value )
def check convergence ( population , population values , func tolerance , position tolerance ) : # Check func tolerance value range = tf . math . abs ( tf . math . reduce max ( input tensor = population values ) - tf . math . reduce min ( input tensor = population values ) ) value converged = value range <= func tolerance # Ideally, we would compute the position convergence by computing the # pairwise distance between every member of the population and checking if # the maximum of those is less than the supplied tolerance. However, this is # completely infeasible in terms of performance. We adopt a more conservative # approach which checks the distance between the first population member # with the rest of the population. If the largest such distance is less than # half the supplied tolerance, we stop. The reason why this is sufficient is # as follows. For any pair of distinct points (a, b) in the population, we # have the relation:  |a - b| <= |x0 - a| + |x0 - b|, where x0 is any # other point. In particular, let x0 be the first element of the population # and suppose that the largest distance between this point and any other # member is epsilon. Then, for any pair of points (a, b), # |a - b| <= 2 * epsilon and hence, the maximum distance between any pair of # points in the population is bounded above by twice the distance between # the first point and other points. half tol = position tolerance / 2 def part converged ( part ) : return tf . math . reduce max ( input tensor = tf . math . abs ( part - part [ 0 ] ) ) <= half tol x converged = tf . math . reduce all ( input tensor = [ part converged ( part ) for part in population ] ) return value converged | x converged
def get tol ( tol , dtype , validate args ) : if tol is None : return tf . convert to tensor ( value = 0 , dtype = dtype ) tol = tf . convert to tensor ( value = tol , dtype = dtype ) if validate args : tol = distribution util . with dependencies ( [ assert util . assert non negative ( tol , message = "Argument 'tol' must be non-negative" ) ] , tol ) return tol
def build input pipeline ( train images , batch size ) : training dataset = tf . data . Dataset . from tensor slices ( train images ) training batches = training dataset . shuffle ( 50000 , reshuffle each iteration = True ) . repeat ( ) . batch ( batch size ) training iterator = tf . compat . v1 . data . make one shot iterator ( training batches ) images = training iterator . get next ( ) return images
def hat integral inverse ( self , x ) : x = tf . cast ( x , self . power . dtype ) t = self . power - 1. return tf . math . expm1 ( - ( tf . math . log ( t ) + tf . math . log ( x ) ) / t )
def lu reconstruct assertions ( lower upper , perm , validate args ) : assertions = [ ] message = 'Input `lower upper` must have at least 2 dimensions.' if lower upper . shape . ndims is not None : if lower upper . shape . ndims < 2 : raise Value Error ( message ) elif validate args : assertions . append ( tf . compat . v1 . assert rank at least ( lower upper , rank = 2 , message = message ) ) message = '`rank(lower upper)` must equal `rank(perm) + 1`' if lower upper . shape . ndims is not None and perm . shape . ndims is not None : if lower upper . shape . ndims != perm . shape . ndims + 1 : raise Value Error ( message ) elif validate args : assertions . append ( tf . compat . v1 . assert rank ( lower upper , rank = tf . rank ( perm ) + 1 , message = message ) ) message = '`lower upper` must be square.' if lower upper . shape [ : - 2 ] . is fully defined ( ) : if lower upper . shape [ - 2 ] != lower upper . shape [ - 1 ] : raise Value Error ( message ) elif validate args : m , n = tf . split ( tf . shape ( input = lower upper ) [ - 2 : ] , num or size splits = 2 ) assertions . append ( tf . compat . v1 . assert equal ( m , n , message = message ) ) return assertions
def lu solve assertions ( lower upper , perm , rhs , validate args ) : assertions = lu reconstruct assertions ( lower upper , perm , validate args ) message = 'Input `rhs` must have at least 2 dimensions.' if rhs . shape . ndims is not None : if rhs . shape . ndims < 2 : raise Value Error ( message ) elif validate args : assertions . append ( tf . compat . v1 . assert rank at least ( rhs , rank = 2 , message = message ) ) message = '`lower upper.shape[-1]` must equal `rhs.shape[-1]`.' if ( tf . compat . dimension value ( lower upper . shape [ - 1 ] ) is not None and tf . compat . dimension value ( rhs . shape [ - 2 ] ) is not None ) : if lower upper . shape [ - 1 ] != rhs . shape [ - 2 ] : raise Value Error ( message ) elif validate args : assertions . append ( tf . compat . v1 . assert equal ( tf . shape ( input = lower upper ) [ - 1 ] , tf . shape ( input = rhs ) [ - 2 ] , message = message ) ) return assertions
def maybe validate matrix ( a , validate args ) : assertions = [ ] if not a . dtype . is floating : raise Type Error ( 'Input `a` must have `float`-like `dtype` ' '(saw {}).' . format ( a . dtype . name ) ) if a . shape . ndims is not None : if a . shape . ndims < 2 : raise Value Error ( 'Input `a` must have at least 2 dimensions ' '(saw: {}).' . format ( a . shape . ndims ) ) elif validate args : assertions . append ( tf . compat . v1 . assert rank at least ( a , rank = 2 , message = 'Input `a` must have at least 2 dimensions.' ) ) return assertions
def gen slices ( num blocks , n in , n out , mask type = MASK EXCLUSIVE ) : # TODO(b/67594795): Better support of dynamic shape. slices = [ ] col = 0 d in = n in // num blocks d out = n out // num blocks row = d out if mask type == MASK EXCLUSIVE else 0 for in range ( num blocks ) : row slice = slice ( row , None ) col slice = slice ( col , col + d in ) slices . append ( [ row slice , col slice ] ) col += d in row += d out return slices
def gen mask ( num blocks , n in , n out , mask type = MASK EXCLUSIVE , dtype = tf . float32 ) : # TODO(b/67594795): Better support of dynamic shape. mask = np . zeros ( [ n out , n in ] , dtype = dtype . as numpy dtype ( ) ) slices = gen slices ( num blocks , n in , n out , mask type = mask type ) for [ row slice , col slice ] in slices : mask [ row slice , col slice ] = 1 return mask
def create input order ( input size , input order = "left-to-right" ) : if isinstance ( input order , six . string types ) : if input order == "left-to-right" : return np . arange ( start = 1 , stop = input size + 1 ) elif input order == "right-to-left" : return np . arange ( start = input size , stop = 0 , step = - 1 ) elif input order == "random" : ret = np . arange ( start = 1 , stop = input size + 1 ) np . random . shuffle ( ret ) return ret elif np . all ( np . sort ( input order ) == np . arange ( 1 , input size + 1 ) ) : return np . array ( input order ) raise Value Error ( "Invalid input order: '{}'." . format ( input order ) )
def create masks ( degrees ) : return [ # Create input->hidden and hidden->hidden masks. inp [ : , np . newaxis ] <= out for inp , out in zip ( degrees [ : - 1 ] , degrees [ 1 : ] ) ] + [ # Create hidden->output mask. degrees [ - 1 ] [ : , np . newaxis ] < degrees [ 0 ] ]
def make masked initializer ( mask , initializer ) : initializer = tf . keras . initializers . get ( initializer ) def masked initializer ( shape , dtype = None , partition info = None ) : # If no `partition info` is given, then don't pass it to `initializer`, as # `initializer` may be a `tf.compat.v2.initializers.Initializer` (which # don't accept a `partition info` argument). if partition info is None : x = initializer ( shape , dtype ) else : x = initializer ( shape , dtype , partition info ) return tf . cast ( mask , x . dtype ) * x return masked initializer
def build ( self , input shape ) : if self . event shape is None : # `event shape` wasn't specied at  init , so infer from `input shape`. self . event shape = [ tf . compat . dimension value ( input shape [ - 1 ] ) ] self . event size = self . event shape [ - 1 ] self . event ndims = len ( self . event shape ) # Should we throw if input shape has rank > 2? if input shape [ - 1 ] != self . event shape [ - 1 ] : raise Value Error ( "Invalid final dimension of `input shape`. " "Expected `{!r}`, but got `{!r}`" . format ( self . event shape [ - 1 ] , input shape [ - 1 ] ) ) # Construct the masks. self . input order = create input order ( self . event size , self . input order param ) self . masks = create masks ( create degrees ( input size = self . event size , hidden units = self . hidden units , input order = self . input order , hidden degrees = self . hidden degrees ) ) # In the final layer, we will produce `self. params` outputs for each of the # `self. event size` inputs to `Autoregressive Layer`.  But `masks[-1]` has # shape `[self. hidden units[-1], self. event size]`.  Thus, we need to # expand the mask to `[hidden units[-1], event size * self. params]` such # that all units for the same input are masked identically.  In particular, # we tile the mask so the j-th element of `tf.unstack(output, axis=-1)` is a # tensor of the j-th parameter/unit for each input. # # NOTE: Other orderings of the output could be faster -- should benchmark. self . masks [ - 1 ] = np . reshape ( np . tile ( self . masks [ - 1 ] [ ... , tf . newaxis ] , [ 1 , 1 , self . params ] ) , [ self . masks [ - 1 ] . shape [ 0 ] , self . event size * self . params ] ) self . network = tf . keras . Sequential ( [ # Starting this model with an `Input Layer` ensures that Keras will build # and propagate our `dtype` to each layer we add. tf . keras . layers . Input Layer ( ( self . event size , ) , dtype = self . dtype ) ] ) # Input-to-hidden, hidden-to-hidden, and hidden-to-output layers: #  [..., self. event size] -> [..., self. hidden units[0]]. #  [..., self. hidden units[k-1]] -> [..., self. hidden units[k]]. #  [..., self. hidden units[-1]] -> [..., event size * self. params]. layer output sizes = self . hidden units + [ self . event size * self . params ] for k in range ( len ( self . masks ) ) : self . network . add ( tf . keras . layers . Dense ( layer output sizes [ k ] , kernel initializer = make masked initializer ( self . masks [ k ] , self . kernel initializer ) , kernel constraint = make masked constraint ( self . masks [ k ] ) , activation = self . activation if k + 1 < len ( self . masks ) else None , use bias = self . use bias , * * self . kwargs ) ) # Record that the layer has been built. super ( Autoregressive Layer , self ) . build ( input shape )
def call ( self , x ) : with tf . compat . v2 . name scope ( self . name or "Autoregressive Layer call" ) : x = tf . convert to tensor ( value = x , dtype = self . dtype , name = "x" ) input shape = tf . shape ( input = x ) # TODO(b/67594795): Better support for dynamic shapes. if tensorshape util . rank ( x . shape ) == 1 : x = x [ tf . newaxis , ... ] return tf . reshape ( self . network ( x ) , tf . concat ( [ input shape , [ self . params ] ] , axis = 0 ) )
def zero dimensional mvndiag ( dtype ) : dummy mvndiag = tfd . Multivariate Normal Diag ( scale diag = tf . ones ( [ 0 ] , dtype = dtype ) ) dummy mvndiag . covariance = lambda : dummy mvndiag . variance ( ) [ ... , tf . newaxis ] return dummy mvndiag
def observe timeseries fn ( timeseries ) : def observation noise fn ( t ) : current slice = timeseries [ ... , t , : ] return tfd . Multivariate Normal Diag ( loc = current slice , scale diag = tf . zeros like ( current slice ) ) return observation noise fn
def params to weights ( self , global scale variance , global scale noncentered , local scale variances , local scales noncentered , weights noncentered ) : global scale = ( global scale noncentered * tf . sqrt ( global scale variance ) * self . weights prior scale ) local scales = local scales noncentered * tf . sqrt ( local scale variances ) return weights noncentered * local scales * global scale [ ... , tf . newaxis ]
def depth ( g ) : def explore ( v ) : if v . depth < 0 : v . depth = ( ( 1 + max ( [ - 1 ] + [ explore ( annotated graph [ u ] ) for u in v . parents ] ) ) if v . parents else 0 ) return v . depth annotated graph = { k : Node ( k , v ) for k , v in g . items ( ) } for v in annotated graph . values ( ) : explore ( v ) return annotated graph
def best order ( g ) : def explore ( u ) : """Recursive function to ascend up through unvisited dependencies.""" if u . depth < 0 : return # Already visited. if not u . parents : result . append ( ( u . name , u . parents ) ) u . depth = - 1 # Mark visited. return b = ( u . name , [ ] ) result . append ( b ) u . depth = - 1 # Mark visited. d = 0 for v in sorted ( ( g . get ( p ) for p in u . parents ) , key = lambda v : v . depth ) : n0 = len ( result ) explore ( v ) n1 = len ( result ) b [ 1 ] . extend ( [ ' ' ] * d + [ v . name ] ) d = n1 - n0 - 1 g = depth ( g ) result = [ ] for u in sorted ( g . values ( ) , key = lambda v : v . depth , reverse = True ) : explore ( u ) return tuple ( reversed ( result ) )
def prob chain rule flatten ( named makers ) : def make ( dist fn , args ) : if args is None : return lambda * : dist fn if not args : return lambda * : dist fn ( ) def fn ( * xs ) : kwargs = dict ( zip ( args , reversed ( xs [ - len ( args ) : ] ) ) ) kwargs . pop ( ' ' , None ) return dist fn ( * * kwargs ) return fn named makers = convert to dict ( named makers ) g = { k : ( None if distribution util . is distribution instance ( v ) else joint distribution sequential . get required args ( v ) ) # pylint: disable=protected-access for k , v in named makers . items ( ) } g = best order ( g ) dist fn name , dist fn args = zip ( * g ) dist fn args = tuple ( None if a is None else tuple ( a ) for a in dist fn args ) dist fn wrapped = tuple ( make ( named makers [ name ] , parents ) for ( name , parents ) in g ) dist fn = tuple ( named makers . get ( n ) for n in dist fn name ) return dist fn , dist fn wrapped , dist fn args , dist fn name
def build ( self , model ) : if not is dict like ( model ) : raise Type Error ( '`model` must be convertible to `dict` (saw: {}).' . format ( type ( model ) . name ) ) [ self . dist fn , self . dist fn wrapped , self . dist fn args , self . dist fn name , # Joint Distribution Sequential doesn't have this. ] = prob chain rule flatten ( model )
def build is last day of season ( num steps per season ) : num steps per cycle = np . sum ( num steps per season ) changepoints = np . cumsum ( np . ravel ( num steps per season ) ) - 1 def is last day of season ( t ) : t = dist util . maybe get static value ( t ) if t is not None : # static case step in cycle = t % num steps per cycle return any ( step in cycle == changepoints ) else : step in cycle = tf . math . floormod ( t , num steps per cycle ) return tf . reduce any ( input tensor = tf . equal ( step in cycle , changepoints ) ) return is last day of season
def build seasonal transition matrix ( num seasons , is last day of season , dtype , basis change matrix = None , basis change matrix inv = None ) : with tf . compat . v1 . name scope ( 'build seasonal transition matrix' ) : # If the season is changing, the transition matrix permutes the latent # state to shift all seasons up by a dimension, and sends the current # season's effect to the bottom. seasonal permutation = np . concatenate ( [ np . arange ( 1 , num seasons ) , [ 0 ] ] , axis = 0 ) seasonal permutation matrix = tf . constant ( np . eye ( num seasons ) [ seasonal permutation ] , dtype = dtype ) # Optionally transform the transition matrix into a reparameterized space, # enforcing the zero-sum constraint for Constrained Seasonal State Space Model. if basis change matrix is not None : seasonal permutation matrix = tf . matmul ( basis change matrix , tf . matmul ( seasonal permutation matrix , basis change matrix inv ) ) identity matrix = tf . eye ( tf . shape ( input = seasonal permutation matrix ) [ - 1 ] , dtype = dtype ) def seasonal transition matrix ( t ) : return tf . linalg . Linear Operator Full Matrix ( matrix = dist util . pick scalar condition ( is last day of season ( t ) , seasonal permutation matrix , identity matrix ) ) return seasonal transition matrix
def build seasonal transition noise ( drift scale , num seasons , is last day of season ) : # If the current season has just ended, increase the variance of its effect # following drift scale. (the just-ended seasonal effect will always be the # bottom element of the vector). Otherwise, do nothing. drift scale diag = tf . stack ( [ tf . zeros like ( drift scale ) ] * ( num seasons - 1 ) + [ drift scale ] , axis = - 1 ) def seasonal transition noise ( t ) : noise scale diag = dist util . pick scalar condition ( is last day of season ( t ) , drift scale diag , tf . zeros like ( drift scale diag ) ) return tfd . Multivariate Normal Diag ( loc = tf . zeros ( num seasons , dtype = drift scale . dtype ) , scale diag = noise scale diag ) return seasonal transition noise
def build constrained seasonal transition noise ( drift scale , num seasons , is last day of season ) : # Conceptually, this method takes the noise covariance on effects L @ L' # computed by `build seasonal transition noise`, with scale factor #       L = [ 0, 0, ..., 0 #             ... #             0, 0, ..., drift scale], # and transforms it to act on the constrained-residual representation. # # The resulting noise covariance M @ M' is equivalent to #    M @ M' = effects to residuals @ LL' @ residuals to effects # where `@` is matrix multiplication. However because this matrix is # rank-deficient, we can't take its Cholesky decomposition directly, so we'll # construct its lower-triangular scale factor `M` by hand instead. # # Concretely, let `M = P @ R @ L` be the scale factor in the # transformed space, with matrices `R`, `P` applying the reparameterization # and zero-mean constraint respectively as defined in the # "Mathematical Details" section of `Constrained Seasonal State Space Model`. It's # easy to see (*) that the implied covariance # `M @ M' = P @ R @ L @ L' @ R' @ P'` is just the constant matrix #  `M @ M' = [ 1, 1, ..., 1, 0 #              1, 1, ..., 1, 0 #              ... #              1, 1, ..., 1, 0 #              0, 0, ..., 0, 0] * (drift scale / num seasons)**2` # with zeros in the final row and column. So we can directly construct # the lower-triangular factor #  `Q = [ 1, 0, ...  0 #         1, 0, ..., 0 #         ... #         1, 0, ..., 0 #         0, 0, ..., 0 ] * drift scale/num seasons` # such that Q @ Q' = M @ M'. In practice, we don't reify the final row and # column full of zeroes, i.e., we construct # `Q[:num seasons-1, :num seasons-1]` as the scale-Tri L covariance factor. # # (*) Argument: `L` is zero everywhere but the last column, so `R @ L` will be # too. Since the last column of `R` is the constant `-1/num seasons`, `R @ L` # is simply the matrix with constant `-drift scale/num seasons` in the final # column (except the final row, which is negated) and zero in all other # columns, and `M = P @ R @ L` additionally zeroes out the final row. Then # M @ M' is just the outer product of that final column with itself (since all # other columns are zero), which gives the matrix shown above. drift scale tril nonzeros = tf . concat ( [ tf . ones ( [ num seasons - 1 , 1 ] , dtype = drift scale . dtype ) , tf . zeros ( [ num seasons - 1 , num seasons - 2 ] , dtype = drift scale . dtype ) ] , axis = - 1 ) drift scale tril = ( drift scale tril nonzeros * drift scale [ ... , tf . newaxis , tf . newaxis ] / num seasons ) # Inject transition noise iff it is the last day of the season. def seasonal transition noise ( t ) : noise scale tril = dist util . pick scalar condition ( is last day of season ( t ) , drift scale tril , tf . zeros like ( drift scale tril ) ) return tfd . Multivariate Normal Tri L ( loc = tf . zeros ( num seasons - 1 , dtype = drift scale . dtype ) , scale tril = noise scale tril ) return seasonal transition noise
def optimize ( self ) : jmodel = call Java Func ( self . value . optimize ) from bigdl . nn . layer import Layer return Layer . of ( jmodel )
def get end trigger ( options ) : if options . end Trigger Type . lower ( ) == "epoch" : return Max Epoch ( options . end Trigger Num ) else : return Max Iteration ( options . end Trigger Num )
def validate optimizer ( optimizer , test data , options ) : optimizer . set validation ( batch size = options . batch Size , val rdd = test data , trigger = Every Epoch ( ) , val method = [ Top1Accuracy ( ) ] ) optimizer . set checkpoint ( Every Epoch ( ) , options . checkpoint Path )
def value ( self ) : if not hasattr ( self , " value" ) and self . path is not None : self . value = self . load ( self . path ) return self . value
def call Big Dl Func ( bigdl type , name , * args ) : gateway = get gateway ( ) error = Exception ( "Cannot find function: %s" % name ) for jinvoker in Java Creator . instance ( bigdl type , gateway ) . value : # hasattr(jinvoker, name) always return true here, # so you need to invoke the method to check if it exist or not try : api = getattr ( jinvoker , name ) result = call Java Func ( api , * args ) except Exception as e : error = e if "does not exist" not in str ( e ) : raise e else : return result raise error
def py2java ( gateway , obj ) : if isinstance ( obj , RDD ) : obj = to java object rdd ( obj ) elif isinstance ( obj , Data Frame ) : obj = obj . jdf elif isinstance ( obj , Spark Context ) : obj = obj . jsc elif isinstance ( obj , ( list , tuple ) ) : obj = List Converter ( ) . convert ( [ py2java ( gateway , x ) for x in obj ] , gateway . gateway client ) elif isinstance ( obj , dict ) : result = { } for ( key , value ) in obj . items ( ) : result [ key ] = py2java ( gateway , value ) obj = Map Converter ( ) . convert ( result , gateway . gateway client ) elif isinstance ( obj , Java Value ) : obj = obj . value elif isinstance ( obj , Java Object ) : pass elif isinstance ( obj , ( int , long , float , bool , bytes , unicode ) ) : pass else : data = bytearray ( Pickle Serializer ( ) . dumps ( obj ) ) obj = gateway . jvm . org . apache . spark . bigdl . api . python . Big DL Ser De . loads ( data ) return obj
def get label ( self ) : label = call Big Dl Func ( self . bigdl type , "image Feature To Label Tensor" , self . value ) return label . to ndarray ( )
def read parquet ( cls , path , sc , bigdl type = "float" ) : return Distributed Image Frame ( jvalue = call Big Dl Func ( bigdl type , "read Parquet" , path , sc ) )
def write parquet ( cls , path , output , sc , partition num = 1 , bigdl type = "float" ) : return call Big Dl Func ( bigdl type , "write Parquet" , path , output , sc , partition num )
def get image ( self , float key = "floats" , to chw = True ) : return self . image frame . get image ( float key , to chw )
def get image ( self , float key = "floats" , to chw = True ) : tensors = call Big Dl Func ( self . bigdl type , "local Image Frame To Image Tensor" , self . value , float key , to chw ) return map ( lambda tensor : tensor . to ndarray ( ) , tensors )
def get label ( self ) : tensor rdd = call Big Dl Func ( self . bigdl type , "distributed Image Frame To Label Tensor Rdd" , self . value ) return tensor rdd . map ( lambda tensor : tensor . to ndarray ( ) )
def get predict ( self , key = "predict" ) : predicts = call Big Dl Func ( self . bigdl type , "distributed Image Frame To Predict" , self . value , key ) return predicts . map ( lambda predict : ( predict [ 0 ] , predict [ 1 ] . to ndarray ( ) ) if predict [ 1 ] else ( predict [ 0 ] , None ) )
def save keras definition ( keras model , path ) : model json = keras model . to json ( ) with open ( path , "w" ) as json file : json file . write ( model json )
def build keras model ( ) : from keras . models import Sequential from keras . layers import Dense , Dropout , Activation , Flatten from keras . layers import Convolution2D , Max Pooling2D keras model = Sequential ( ) keras model . add ( Convolution2D ( 32 , 3 , 3 , border mode = 'valid' , input shape = input shape ) ) keras model . add ( Activation ( 'relu' ) ) keras model . add ( Convolution2D ( 32 , 3 , 3 ) ) keras model . add ( Activation ( 'relu' ) ) keras model . add ( Max Pooling2D ( pool size = ( 2 , 2 ) ) ) keras model . add ( Dropout ( 0.25 ) ) keras model . add ( Flatten ( ) ) keras model . add ( Dense ( 128 ) ) keras model . add ( Activation ( 'relu' ) ) keras model . add ( Dropout ( 0.5 ) ) keras model . add ( Dense ( 10 ) ) keras model . add ( Activation ( 'softmax' ) ) return keras model
def training ( self , is training = True ) : if is training : call Java Func ( self . value . training ) else : call Java Func ( self . value . evaluate ) return self
def build keras model ( ) : from keras . models import Sequential from keras . layers import Dense , Dropout , Activation from keras . layers import Embedding from keras . layers import LSTM from keras . layers import Convolution1D , Max Pooling1D keras model = Sequential ( ) keras model . add ( Embedding ( 20000 , 128 , input length = 100 ) ) keras model . add ( Dropout ( 0.25 ) ) keras model . add ( Convolution1D ( nb filter = 64 , filter length = 5 , border mode = 'valid' , activation = 'relu' , subsample length = 1 ) ) keras model . add ( Max Pooling1D ( pool length = 4 ) ) keras model . add ( LSTM ( 70 ) ) keras model . add ( Dense ( 1 ) ) keras model . add ( Activation ( 'sigmoid' ) ) return keras model
def get bigdl classpath ( ) : if os . getenv ( "BIGDL CLASSPATH" ) : return os . environ [ "BIGDL CLASSPATH" ] jar dir = os . path . abspath ( file + "/../../" ) jar paths = glob . glob ( os . path . join ( jar dir , "share/lib/*.jar" ) ) if jar paths : assert len ( jar paths ) == 1 , "Expecting one jar: %s" % len ( jar paths ) return jar paths [ 0 ] return ""
def is spark below 2 2 ( ) : import pyspark if ( hasattr ( pyspark , "version" ) ) : full version = pyspark . version . version # We only need the general spark version (eg, 1.6, 2.2). parts = full version . split ( "." ) spark version = parts [ 0 ] + "." + parts [ 1 ] if ( compare version ( spark version , "2.2" ) >= 0 ) : return False return True
def attention ( inputs , state , att size , mask , scope = "attention" ) : with tf . variable scope ( scope ) : u = tf . concat ( [ tf . tile ( tf . expand dims ( state , axis = 1 ) , [ 1 , tf . shape ( inputs ) [ 1 ] , 1 ] ) , inputs ] , axis = 2 ) logits = tf . layers . dense ( tf . layers . dense ( u , att size , activation = tf . nn . tanh ) , 1 , use bias = False ) logits = softmax mask ( tf . squeeze ( logits , [ 2 ] ) , mask ) att weights = tf . expand dims ( tf . nn . softmax ( logits ) , axis = 2 ) res = tf . reduce sum ( att weights * inputs , axis = 1 ) return res , logits
def summary gradient updates ( grads , opt , lr ) : # strategy: # make a dict of variable name -> [variable, grad, adagrad slot] vars grads = { } for v in tf . trainable variables ( ) : vars grads [ v . name ] = [ v , None , None ] for g , v in grads : vars grads [ v . name ] [ 1 ] = g vars grads [ v . name ] [ 2 ] = opt . get slot ( v , 'accumulator' ) # now make summaries ret = [ ] for vname , ( v , g , a ) in vars grads . items ( ) : if g is None : continue if isinstance ( g , tf . Indexed Slices ) : # a sparse gradient - only take norm of params that are updated updates = lr * g . values if a is not None : updates /= tf . sqrt ( tf . gather ( a , g . indices ) ) else : updates = lr * g if a is not None : updates /= tf . sqrt ( a ) values norm = tf . sqrt ( tf . reduce sum ( v * v ) ) + 1.0e-7 updates norm = tf . sqrt ( tf . reduce sum ( updates * updates ) ) ret . append ( tf . summary . scalar ( 'UPDATE/' + vname . replace ( ":" , " " ) , updates norm / values norm ) ) return ret
def dump weights ( tf save dir , outfile , options ) : def get outname ( tf name ) : outname = re . sub ( ':0$' , '' , tf name ) outname = outname . lstrip ( 'lm/' ) outname = re . sub ( '/rnn/' , '/RNN/' , outname ) outname = re . sub ( '/multi rnn cell/' , '/Multi RNN Cell/' , outname ) outname = re . sub ( '/cell ' , '/Cell' , outname ) outname = re . sub ( '/lstm cell/' , '/LSTM Cell/' , outname ) if '/RNN/' in outname : if 'projection' in outname : outname = re . sub ( 'projection/kernel' , 'W P 0' , outname ) else : outname = re . sub ( '/kernel' , '/W 0' , outname ) outname = re . sub ( '/bias' , '/B' , outname ) return outname ckpt file = tf . train . latest checkpoint ( tf save dir ) config = tf . Config Proto ( allow soft placement = True ) with tf . Graph ( ) . as default ( ) : with tf . Session ( config = config ) as sess : with tf . variable scope ( 'lm' ) : Language Model ( options , False ) # Create graph # we use the "Saver" class to load the variables loader = tf . train . Saver ( ) loader . restore ( sess , ckpt file ) with h5py . File ( outfile , 'w' ) as fout : for v in tf . trainable variables ( ) : if v . name . find ( 'softmax' ) >= 0 : # don't dump these continue outname = get outname ( v . name ) # print("Saving variable {0} with name {1}".format( #     v.name, outname)) shape = v . get shape ( ) . as list ( ) dset = fout . create dataset ( outname , shape , dtype = 'float32' ) values = sess . run ( [ v ] ) [ 0 ] dset [ ... ] = values
def read data by config ( config : dict ) : dataset config = config . get ( 'dataset' , None ) if dataset config : config . pop ( 'dataset' ) ds type = dataset config [ 'type' ] if ds type == 'classification' : reader = { 'class name' : 'basic classification reader' } iterator = { 'class name' : 'basic classification iterator' } config [ 'dataset reader' ] = { * * dataset config , * * reader } config [ 'dataset iterator' ] = { * * dataset config , * * iterator } else : raise Exception ( "Unsupported dataset type: {}" . format ( ds type ) ) try : reader config = dict ( config [ 'dataset reader' ] ) except Key Error : raise Config Error ( "No dataset reader is provided in the JSON config." ) reader = get model ( reader config . pop ( 'class name' ) ) ( ) data path = reader config . pop ( 'data path' , '' ) if isinstance ( data path , list ) : data path = [ expand path ( x ) for x in data path ] else : data path = expand path ( data path ) return reader . read ( data path , * * reader config )
def train evaluate model from config ( config : Union [ str , Path , dict ] , iterator : Union [ Data Learning Iterator , Data Fitting Iterator ] = None , * , to train : bool = True , evaluation targets : Optional [ Iterable [ str ] ] = None , to validate : Optional [ bool ] = None , download : bool = False , start epoch num : Optional [ int ] = None , recursive : bool = False ) -> Dict [ str , Dict [ str , float ] ] : config = parse config ( config ) if download : deep download ( config ) if to train and recursive : for subconfig in get all elems from json ( config [ 'chainer' ] , 'config path' ) : log . info ( f'Training "{subconfig}"' ) train evaluate model from config ( subconfig , download = False , recursive = True ) import packages ( config . get ( 'metadata' , { } ) . get ( 'imports' , [ ] ) ) if iterator is None : try : data = read data by config ( config ) except Config Error as e : to train = False log . warning ( f'Skipping training. {e.message}' ) else : iterator = get iterator from config ( config , data ) if 'train' not in config : log . warning ( 'Train config is missing. Populating with default values' ) train config = config . get ( 'train' ) if start epoch num is not None : train config [ 'start epoch num' ] = start epoch num if 'evaluation targets' not in train config and ( 'validate best' in train config or 'test best' in train config ) : log . warning ( '"validate best" and "test best" parameters are deprecated.' ' Please, use "evaluation targets" list instead' ) train config [ 'evaluation targets' ] = [ ] if train config . pop ( 'validate best' , True ) : train config [ 'evaluation targets' ] . append ( 'valid' ) if train config . pop ( 'test best' , True ) : train config [ 'evaluation targets' ] . append ( 'test' ) trainer class = get model ( train config . pop ( 'class name' , 'nn trainer' ) ) trainer = trainer class ( config [ 'chainer' ] , * * train config ) if to train : trainer . train ( iterator ) res = { } if iterator is not None : if to validate is not None : if evaluation targets is None : log . warning ( '"to validate" parameter is deprecated and will be removed in future versions.' ' Please, use "evaluation targets" list instead' ) evaluation targets = [ 'test' ] if to validate : evaluation targets . append ( 'valid' ) else : log . warn ( 'Both "evaluation targets" and "to validate" parameters are specified.' ' "to validate" is deprecated and will be ignored' ) res = trainer . evaluate ( iterator , evaluation targets , print reports = True ) trainer . get chainer ( ) . destroy ( ) res = { k : v [ 'metrics' ] for k , v in res . items ( ) } return res
def load ( self ) -> None : # Checks presence of the model files if self . load path . exists ( ) : path = str ( self . load path . resolve ( ) ) log . info ( '[loading model from {}]' . format ( path ) ) self . net . load ( path )
def build ( self ) : word inputs = kl . Input ( shape = ( None , MAX WORD LENGTH + 2 ) , dtype = "int32" ) inputs = [ word inputs ] word outputs = self . build word cnn ( word inputs ) if len ( self . word vectorizers ) > 0 : additional word inputs = [ kl . Input ( shape = ( None , input dim ) , dtype = "float32" ) for input dim , dense dim in self . word vectorizers ] inputs . extend ( additional word inputs ) additional word embeddings = [ kl . Dense ( dense dim ) ( additional word inputs [ i ] ) for i , ( , dense dim ) in enumerate ( self . word vectorizers ) ] word outputs = kl . Concatenate ( ) ( [ word outputs ] + additional word embeddings ) outputs , lstm outputs = self . build basic network ( word outputs ) compile args = { "optimizer" : ko . nadam ( lr = 0.002 , clipnorm = 5.0 ) , "loss" : "categorical crossentropy" , "metrics" : [ "accuracy" ] } self . model = Model ( inputs , outputs ) self . model . compile ( * * compile args ) if self . verbose > 0 : self . model . summary ( print fn = log . info ) return self
def build word cnn ( self , inputs ) : inputs = kl . Lambda ( kb . one hot , arguments = { "num classes" : self . symbols number } , output shape = lambda x : tuple ( x ) + ( self . symbols number , ) ) ( inputs ) char embeddings = kl . Dense ( self . char embeddings size , use bias = False ) ( inputs ) conv outputs = [ ] self . char output dim = 0 for window size , filters number in zip ( self . char window size , self . char filters ) : curr output = char embeddings curr filters number = ( min ( self . char filter multiple * window size , 200 ) if filters number is None else filters number ) for in range ( self . char conv layers - 1 ) : curr output = kl . Conv2D ( curr filters number , ( 1 , window size ) , padding = "same" , activation = "relu" , data format = "channels last" ) ( curr output ) if self . conv dropout > 0.0 : curr output = kl . Dropout ( self . conv dropout ) ( curr output ) curr output = kl . Conv2D ( curr filters number , ( 1 , window size ) , padding = "same" , activation = "relu" , data format = "channels last" ) ( curr output ) conv outputs . append ( curr output ) self . char output dim += curr filters number if len ( conv outputs ) > 1 : conv output = kl . Concatenate ( axis = - 1 ) ( conv outputs ) else : conv output = conv outputs [ 0 ] highway input = kl . Lambda ( kb . max , arguments = { "axis" : - 2 } ) ( conv output ) if self . intermediate dropout > 0.0 : highway input = kl . Dropout ( self . intermediate dropout ) ( highway input ) for i in range ( self . char highway layers - 1 ) : highway input = Highway ( activation = "relu" ) ( highway input ) if self . highway dropout > 0.0 : highway input = kl . Dropout ( self . highway dropout ) ( highway input ) highway output = Highway ( activation = "relu" ) ( highway input ) return highway output
def main ( ) : args = parser . parse args ( ) path = get settings path ( ) if args . default : if populate settings dir ( force = True ) : print ( f'Populated {path} with default settings files' ) else : print ( f'{path} is already a default settings directory' ) else : print ( f'Current Deep Pavlov settings path: {path}' )
def graph wrap ( func , graph ) : @ wraps ( func ) def wrapped ( * args , * * kwargs ) : with graph . as default ( ) : return func ( * args , * * kwargs ) return wrapped
def keras wrap ( func , graph , session ) : import keras . backend as K @ wraps ( func ) def wrapped ( * args , * * kwargs ) : with graph . as default ( ) : K . set session ( session ) return func ( * args , * * kwargs ) return wrapped
def prettify metrics ( metrics : List [ Tuple [ str , float ] ] , precision : int = 4 ) -> Ordered Dict : prettified metrics = Ordered Dict ( ) for key , value in metrics : value = round ( value , precision ) prettified metrics [ key ] = value return prettified metrics
def load ( self , exclude scopes : tuple = ( 'Optimizer' , ) ) -> None : if not hasattr ( self , 'sess' ) : raise Runtime Error ( 'Your Tensor Flow model {} must' ' have sess attribute!' . format ( self . class . name ) ) path = str ( self . load path . resolve ( ) ) # Check presence of the model files if tf . train . checkpoint exists ( path ) : log . info ( '[loading model from {}]' . format ( path ) ) # Exclude optimizer variables from saved variables var list = self . get saveable variables ( exclude scopes ) saver = tf . train . Saver ( var list ) saver . restore ( self . sess , path )
def save ( self , exclude scopes : tuple = ( 'Optimizer' , ) ) -> None : if not hasattr ( self , 'sess' ) : raise Runtime Error ( 'Your Tensor Flow model {} must' ' have sess attribute!' . format ( self . class . name ) ) path = str ( self . save path . resolve ( ) ) log . info ( '[saving model to {}]' . format ( path ) ) var list = self . get saveable variables ( exclude scopes ) saver = tf . train . Saver ( var list ) saver . save ( self . sess , path )
def print number of parameters ( ) : log . info ( 'Number of parameters: ' ) variables = tf . trainable variables ( ) blocks = defaultdict ( int ) for var in variables : # Get the top level scope name of variable block name = var . name . split ( '/' ) [ 0 ] number of parameters = np . prod ( var . get shape ( ) . as list ( ) ) blocks [ block name ] += number of parameters for block name , cnt in blocks . items ( ) : log . info ( "{} - {}." . format ( block name , cnt ) ) total num parameters = np . sum ( list ( blocks . values ( ) ) ) log . info ( 'Total number of parameters equal {}' . format ( total num parameters ) )
def search ( self , word , d , allow spaces = True , return cost = True ) : if not all ( ( c in self . alphabet or ( c == " " and self . allow spaces ) ) for c in word ) : return [ ] # raise Value Error("{0} contains an incorrect symbol".format(word)) return self . trie search ( word , d , allow spaces = allow spaces , return cost = return cost )
def make default operation costs ( self , allow spaces = False ) : self . operation costs = dict ( ) self . operation costs [ "" ] = { c : 1.0 for c in list ( self . alphabet ) + [ ' ' ] } for a in self . alphabet : current costs = { c : 1.0 for c in self . alphabet } current costs [ a ] = 0.0 current costs [ "" ] = 1.0 if allow spaces : current costs [ " " ] = 1.0 self . operation costs [ a ] = current costs # транспозиции for a , b in itertools . permutations ( self . alphabet , 2 ) : self . operation costs [ a + b ] = { b + a : 1.0 } # пробелы if allow spaces : self . operation costs [ " " ] = { c : 1.0 for c in self . alphabet } self . operation costs [ " " ] [ "" ] = 1.0
def start timer ( self ) -> None : self . timer = Timer ( self . config [ 'conversation lifetime' ] , self . self destruct callback ) self . timer . start ( )
def build model ( config : Union [ str , Path , dict ] , mode : str = 'infer' , load trained : bool = False , download : bool = False , serialized : Optional [ bytes ] = None ) -> Chainer : config = parse config ( config ) if serialized : serialized : list = pickle . loads ( serialized ) if download : deep download ( config ) import packages ( config . get ( 'metadata' , { } ) . get ( 'imports' , [ ] ) ) model config = config [ 'chainer' ] model = Chainer ( model config [ 'in' ] , model config [ 'out' ] , model config . get ( 'in y' ) ) for component config in model config [ 'pipe' ] : if load trained and ( 'fit on' in component config or 'in y' in component config ) : try : component config [ 'load path' ] = component config [ 'save path' ] except Key Error : log . warning ( 'No "save path" parameter for the {} component, so "load path" will not be renewed' . format ( component config . get ( 'class name' , component config . get ( 'ref' , 'UNKNOWN' ) ) ) ) if serialized and 'in' in component config : component serialized = serialized . pop ( 0 ) else : component serialized = None component = from params ( component config , mode = mode , serialized = component serialized ) if 'in' in component config : c in = component config [ 'in' ] c out = component config [ 'out' ] in y = component config . get ( 'in y' , None ) main = component config . get ( 'main' , False ) model . append ( component , c in , c out , in y , main ) return model
def interact model ( config : Union [ str , Path , dict ] ) -> None : model = build model ( config ) while True : args = [ ] for in x in model . in x : args . append ( ( input ( '{}::' . format ( in x ) ) , ) ) # check for exit command if args [ - 1 ] [ 0 ] in { 'exit' , 'stop' , 'quit' , 'q' } : return pred = model ( * args ) if len ( model . out params ) > 1 : pred = zip ( * pred ) print ( '>>' , * pred )
def predict on stream ( config : Union [ str , Path , dict ] , batch size : int = 1 , file path : Optional [ str ] = None ) -> None : if file path is None or file path == '-' : if sys . stdin . isatty ( ) : raise Runtime Error ( 'To process data from terminal please use interact mode' ) f = sys . stdin else : f = open ( file path , encoding = 'utf8' ) model : Chainer = build model ( config ) args count = len ( model . in x ) while True : batch = list ( ( l . strip ( ) for l in islice ( f , batch size * args count ) ) ) if not batch : break args = [ ] for i in range ( args count ) : args . append ( batch [ i : : args count ] ) res = model ( * args ) if len ( model . out params ) == 1 : res = [ res ] for res in zip ( * res ) : res = json . dumps ( res , ensure ascii = False ) print ( res , flush = True ) if f is not sys . stdin : f . close ( )
def fn from str ( name : str ) -> Callable [ ... , Any ] : try : module name , fn name = name . split ( ':' ) except Value Error : raise Config Error ( 'Expected function description in a `module.submodules:function name` form, but got `{}`' . format ( name ) ) return getattr ( importlib . import module ( module name ) , fn name )
def register metric ( metric name : str ) -> Callable [ ... , Any ] : def decorate ( fn ) : fn name = fn . module + ':' + fn . name if metric name in REGISTRY and REGISTRY [ metric name ] != fn name : log . warning ( '"{}" is already registered as a metric name, the old function will be ignored' . format ( metric name ) ) REGISTRY [ metric name ] = fn name return fn return decorate
def get metric by name ( name : str ) -> Callable [ ... , Any ] : if name not in REGISTRY : raise Config Error ( f'"{name}" is not registered as a metric' ) return fn from str ( REGISTRY [ name ] )
def read requirements ( ) : reqs path = os . path . join ( location , 'requirements.txt' ) with open ( reqs path , encoding = 'utf8' ) as f : reqs = [ line . strip ( ) for line in f if not line . strip ( ) . startswith ( '#' ) ] names = [ ] links = [ ] for req in reqs : if '://' in req : links . append ( req ) else : names . append ( req ) return { 'install requires' : names , 'dependency links' : links }
def export2hub ( weight file , hub dir , options ) : spec = make module spec ( options , str ( weight file ) ) try : with tf . Graph ( ) . as default ( ) : module = hub . Module ( spec ) with tf . Session ( ) as sess : sess . run ( tf . global variables initializer ( ) ) if hub dir . exists ( ) : shutil . rmtree ( hub dir ) module . export ( str ( hub dir ) , sess ) finally : pass
def main ( ) : args = parser . parse args ( ) run ms bot framework server ( agent generator = make agent , app id = args . ms id , app secret = args . ms secret , stateful = True )
def check gpu existence ( ) : global gpu available if gpu available is None : sess config = tf . Config Proto ( ) sess config . gpu options . allow growth = True try : with tf . Session ( config = sess config ) : device list = device lib . list local devices ( ) gpu available = any ( device . device type == 'GPU' for device in device list ) except Attribute Error as e : log . warning ( f'Got an Attribute Error `{e}`, assuming documentation building' ) gpu available = False return gpu available
def parse config property ( item : T , variables : Dict [ str , Union [ str , Path , float , bool , None ] ] ) -> T : if isinstance ( item , str ) : return item . format ( * * variables ) elif isinstance ( item , list ) : return [ parse config property ( item , variables ) for item in item ] elif isinstance ( item , dict ) : return { k : parse config property ( v , variables ) for k , v in item . items ( ) } else : return item
def parse config ( config : Union [ str , Path , dict ] ) -> dict : if isinstance ( config , ( str , Path ) ) : config = read json ( find config ( config ) ) variables = { 'DEEPPAVLOV PATH' : os . getenv ( f'DP DEEPPAVLOV PATH' , Path ( file ) . parent . parent . parent ) } for name , value in config . get ( 'metadata' , { } ) . get ( 'variables' , { } ) . items ( ) : env name = f'DP {name}' if env name in os . environ : value = os . getenv ( env name ) variables [ name ] = value . format ( * * variables ) return parse config property ( config , variables )
def expand path ( path : Union [ str , Path ] ) -> Path : return Path ( path ) . expanduser ( ) . resolve ( )
def from params ( params : Dict , mode : str = 'infer' , serialized : Any = None , * * kwargs ) -> Component : # what is passed in json: config params = { k : resolve ( v ) for k , v in params . items ( ) } # get component by reference (if any) if 'ref' in config params : try : component = refs [ config params [ 'ref' ] ] if serialized is not None : component . deserialize ( serialized ) return component except Key Error : e = Config Error ( 'Component with id "{id}" was referenced but not initialized' . format ( id = config params [ 'ref' ] ) ) log . exception ( e ) raise e elif 'config path' in config params : from deeppavlov . core . commands . infer import build model refs = refs . copy ( ) refs . clear ( ) config = parse config ( expand path ( config params [ 'config path' ] ) ) model = build model ( config , serialized = serialized ) refs . clear ( ) refs . update ( refs ) try : refs [ config params [ 'id' ] ] = model except Key Error : pass return model cls name = config params . pop ( 'class name' , None ) if not cls name : e = Config Error ( 'Component config has no `class name` nor `ref` fields' ) log . exception ( e ) raise e cls = get model ( cls name ) # find the submodels params recursively config params = { k : init param ( v , mode ) for k , v in config params . items ( ) } try : spec = inspect . getfullargspec ( cls ) if 'mode' in spec . args + spec . kwonlyargs or spec . varkw is not None : kwargs [ 'mode' ] = mode component = cls ( * * dict ( config params , * * kwargs ) ) try : refs [ config params [ 'id' ] ] = component except Key Error : pass except Exception : log . exception ( "Exception in {}" . format ( cls ) ) raise if serialized is not None : component . deserialize ( serialized ) return component
def run ( self ) -> None : while True : request = self . input queue . get ( ) response = self . handle request ( request ) self . output queue . put ( response )
def refresh valid certs ( self ) -> None : self . timer = Timer ( REFRESH VALID CERTS PERIOD SECS , self . refresh valid certs ) self . timer . start ( ) expired certificates = [ ] for valid cert url , valid cert in self . valid certificates . items ( ) : valid cert : Validated Cert = valid cert cert expiration time : datetime = valid cert . expiration timestamp if datetime . utcnow ( ) > cert expiration time : expired certificates . append ( valid cert url ) for expired cert url in expired certificates : del self . valid certificates [ expired cert url ] log . info ( f'Validation period of {expired cert url} certificate expired' )
def cls from str ( name : str ) -> type : try : module name , cls name = name . split ( ':' ) except Value Error : raise Config Error ( 'Expected class description in a `module.submodules:Class Name` form, but got `{}`' . format ( name ) ) return getattr ( importlib . import module ( module name ) , cls name )
def get model ( name : str ) -> type : if name not in REGISTRY : if ':' not in name : raise Config Error ( "Model {} is not registered." . format ( name ) ) return cls from str ( name ) return cls from str ( REGISTRY [ name ] )
def list jobs ( self ) : res = h2o . api ( "GET /3/Jobs" ) table = [ [ "type" ] , [ "dest" ] , [ "description" ] , [ "status" ] ] for job in res [ "jobs" ] : job dest = job [ "dest" ] table [ 0 ] . append ( self . translate job type ( job dest [ "type" ] ) ) table [ 1 ] . append ( job dest [ "name" ] ) table [ 2 ] . append ( job [ "description" ] ) table [ 3 ] . append ( job [ "status" ] ) return table
def list timezones ( self ) : from h2o . expr import Expr Node return h2o . H2O Frame . expr ( expr = Expr Node ( "list Time Zones" ) ) . frame ( )
def parse ( self , key , hex key = None , column Type Dict = None , timeout Secs = 300 , retry Delay Secs = 0.2 , initial Delay Secs = None , poll Timeout Secs = 180 , noise = None , benchmark Logging = None , no Poll = False , intermediate Results = False , * * kwargs ) : # these should override what parse setup gets below params dict = { 'source frames' : None , 'destination frame' : hex key , 'parse type' : None , # file type  'separator' : None , 'single quotes' : None , 'check header' : None , # forces first line to be seen as column names  'number columns' : None , 'column names' : None , # a list 'column types' : None , # a list. or can use column Type Dict param (see below) 'na strings' : None , # a list 'chunk size' : None , # are these two no longer supported? 'delete on done' : None , 'blocking' : None , } # if key is a list, create a comma separated string # list or tuple but not string if not isinstance ( key , basestring ) : # it's a list of some kind (tuple ok?) # if len(key) > 1: #     print "I noticed you're giving me a list of > 1 keys %s to parse:" % len(key), key # len 1 is ok here. 0 not. what if None or [None] here if not key : raise Exception ( "key seems to be bad in parse. Should be list or string. %s" % key ) # have to put double quotes around the individual list items (single not legal) source frames = "[" + "," . join ( map ( ( lambda x : '"' + x + '"' ) , key ) ) + "]" else : # what if None here source frames = '["' + key + '"]' # quotes required on key params dict [ 'source frames' ] = source frames # merge kwargs into params dict # =None overwrites params dict # column Type Dict not used here h2o methods . check params update kwargs ( params dict , kwargs , 'parse before setup merge' , print params = False ) # Call Parse Setup?source frames=[keys] . . . # if benchmark Logging: #     cloud Perf H2O.get log save(init Only=True) # h2o methods.check params update kwargs(params dict, kwargs, 'parse setup', print params=True) params setup = { 'source frames' : source frames } setup result = self . do json request ( json Request = "3/Parse Setup.json" , cmd = 'post' , timeout = timeout Secs , post Data = params setup ) h2o sandbox . check sandbox for errors ( ) verboseprint ( "Parse Setup result:" , dump json ( setup result ) ) # this should match what we gave as input? if setup result [ 'source frames' ] : # should these be quoted? source frames Str = "[" + "," . join ( [ ( '"%s"' % src [ 'name' ] ) for src in setup result [ 'source frames' ] ] ) + "]" else : source frames Str = None # I suppose we need a way for parameters to parse() to override these # should it be an array or a dict? if setup result [ 'column names' ] : # single quotes not legal..need double quotes column Names Str = "[" + "," . join ( map ( ( lambda x : '"' + x + '"' ) , setup result [ 'column names' ] ) ) + "]" else : column Names Str = None column Types = setup result [ 'column types' ] assert column Types is not None , "%s %s" % ( "column types:" , column Types ) if setup result [ 'na strings' ] : # single quotes not legal..need double quotes na Strings = "[" + "," . join ( map ( ( lambda x : '"' + x + '"' if x != None else '""' ) , setup result [ 'na strings' ] ) ) + "]" else : na Strings = None # dict parameter to update column Type Dict? # but we don't pass column Names like this? ct = setup result [ 'column types' ] column Names = setup result [ 'column names' ] if column Type Dict : for k , v in column Type Dict . iteritems ( ) : if isinstance ( k , int ) : # if a column index if k >= 0 and k < len ( ct ) : ct [ k ] = v else : raise Exception ( "bad col index %s in column Type Dict param %s" % ( k , column Type Dict ) ) # if a column name elif isinstance ( k , basestring ) : # find the index if k not in column Names : raise Exception ( "bad col name %s in column Type Dict param %s. column Names: %s" % ( k , column Type Dict , column Names ) ) ci = column Names . index ( k ) ct [ ci ] = v else : raise Exception ( "%s %s should be int or string" % ( k , type ( k ) ) ) column Types Str = "[" + "," . join ( map ( ( lambda x : '"' + x + '"' ) , ct ) ) + "]" parse params = { 'source frames' : source frames Str , 'destination frame' : setup result [ 'destination frame' ] , 'parse type' : setup result [ 'parse type' ] , 'separator' : setup result [ 'separator' ] , 'single quotes' : setup result [ 'single quotes' ] , 'check header' : setup result [ 'check header' ] , 'number columns' : setup result [ 'number columns' ] , 'column names' : column Names Str , 'column types' : column Types Str , 'na strings' : na Strings , 'chunk size' : setup result [ 'chunk size' ] , # No longer supported? how come these aren't in setup result? 'delete on done' : params dict [ 'delete on done' ] , 'blocking' : params dict [ 'blocking' ] , } # HACK: if there are too many column names..don't print! it is crazy output # just check the output of parse setup. Don't worry about column Names passed as params here.  too Many Col Names To Print = setup result [ 'column names' ] and len ( setup result [ 'column names' ] ) > 2000 if too Many Col Names To Print : h2p . yellow print ( "Not printing the parameters to Parse because the column Names are too lengthy." ) h2p . yellow print ( "See sandbox/commands.log" ) # merge params dict into parse params # don't want =None to overwrite parse params h2o methods . check params update kwargs ( parse params , params dict , 'parse after merge into parse setup' , print params = not too Many Col Names To Print , ignore None = True ) print "parse source frames is length:" , len ( parse params [ 'source frames' ] ) # This can be null now? parse Setup doesn't return default colnames? # print "parse column names is length:", len(parse params['column names']) # none of the kwargs passed to here! parse result = self . do json request ( json Request = "3/Parse.json" , cmd = 'post' , post Data = parse params , timeout = timeout Secs ) verboseprint ( "Parse result:" , dump json ( parse result ) ) job key = parse result [ 'job' ] [ 'key' ] [ 'name' ] hex key = parse params [ 'destination frame' ] # TODO: dislike having different shapes for no Poll and poll if no Poll : # ?? h2o sandbox . check sandbox for errors ( ) # return self.jobs(job key) return parse result # does Frame also, while polling if intermediate Results : key = hex key else : key = None job result = self . poll job ( job key , timeout Secs = timeout Secs , key = key ) if job result : jobs = job result [ 'jobs' ] [ 0 ] description = jobs [ 'description' ] dest = jobs [ 'dest' ] msec = jobs [ 'msec' ] status = jobs [ 'status' ] progress = jobs [ 'progress' ] dest key = dest [ 'name' ] # can condition this with a parameter if some FAILED are expected by tests. if status == 'FAILED' : print dump json ( job result ) raise Exception ( "Taking exception on parse job status: %s %s %s %s %s" % ( status , progress , msec , dest key , description ) ) return self . frames ( dest key ) else : # ? we should always get a job json result raise Exception ( "parse didn't get a job result when it expected one" )
def summary ( self , key , column = "C1" , timeout Secs = 10 , * * kwargs ) : params dict = { # 'offset': 0, # 'len': 100 } h2o methods . check params update kwargs ( params dict , kwargs , 'summary' , True ) result = self . do json request ( '3/Frames.json/%s/columns/%s/summary' % ( key , column ) , timeout = timeout Secs , params = params dict ) h2o sandbox . check sandbox for errors ( ) return result
def delete frame ( self , key , ignore Missing Key = True , timeout Secs = 60 , * * kwargs ) : assert key is not None , '"key" parameter is null' result = self . do json request ( '/3/Frames.json/' + key , cmd = 'delete' , timeout = timeout Secs ) # TODO: look for what? if not ignore Missing Key and 'f00b4r' in result : raise Value Error ( 'Frame key not found: ' + key ) return result
def compute model metrics ( self , model , frame , timeout Secs = 60 , * * kwargs ) : assert model is not None , '"model" parameter is null' assert frame is not None , '"frame" parameter is null' models = self . models ( key = model , timeout Secs = timeout Secs ) assert models is not None , "/Models REST call failed" assert models [ 'models' ] [ 0 ] [ 'model id' ] [ 'name' ] == model , "/Models/{0} returned Model {1} rather than Model {2}" . format ( model , models [ 'models' ] [ 0 ] [ 'key' ] [ 'name' ] , model ) # TODO: test this assert, I don't think this is working. . . frames = self . frames ( key = frame ) assert frames is not None , "/Frames/{0} REST call failed" . format ( frame ) print "frames:" , dump json ( frames ) # is the name not there? # assert frames['frames'][0]['model id']['name'] == frame, "/Frames/{0} returned Frame {1} rather than Frame {2}".format(frame, models['models'][0]['key']['name'], frame) result = self . do json request ( '/3/Model Metrics.json/models/' + model + '/frames/' + frame , cmd = 'post' , timeout = timeout Secs ) mm = result [ 'model metrics' ] [ 0 ] verboseprint ( "model metrics: " + repr ( mm ) ) h2o sandbox . check sandbox for errors ( ) return mm
def delete model ( self , key , ignore Missing Key = True , timeout Secs = 60 , * * kwargs ) : assert key is not None , '"key" parameter is null' result = self . do json request ( '/3/Models.json/' + key , cmd = 'delete' , timeout = timeout Secs ) # TODO: look for what? if not ignore Missing Key and 'f00b4r' in result : raise Value Error ( 'Model key not found: ' + key ) verboseprint ( "delete model result:" , dump json ( result ) ) return result
def tabulate ( self , tablefmt = "simple" , rollups = False , rows = 10 ) : if not self . is valid ( ) : self . fill ( rows = rows ) # Pretty print cached data d = collections . Ordered Dict ( ) # If also printing the rollup stats, build a full row-header if rollups : col = next ( iter ( viewvalues ( self . data ) ) ) # Get a sample column lrows = len ( col [ 'data' ] ) # Cached rows being displayed d [ "" ] = [ "type" , "mins" , "mean" , "maxs" , "sigma" , "zeros" , "missing" ] + list ( map ( str , range ( lrows ) ) ) # For all columns... for k , v in viewitems ( self . data ) : x = v [ 'data' ] # Data to display t = v [ "type" ] # Column type if t == "enum" : domain = v [ 'domain' ] # Map to cat strings as needed x = [ "" if math . isnan ( idx ) else domain [ int ( idx ) ] for idx in x ] elif t == "time" : x = [ "" if math . isnan ( z ) else time . strftime ( "%Y-%m-%d %H:%M:%S" , time . gmtime ( z / 1000 ) ) for z in x ] if rollups : # Rollups, if requested mins = v [ 'mins' ] [ 0 ] if v [ 'mins' ] and v [ "type" ] != "enum" else None maxs = v [ 'maxs' ] [ 0 ] if v [ 'maxs' ] and v [ "type" ] != "enum" else None #Cross check type with mean and sigma. Set to None if of type enum. if v [ 'type' ] == "enum" : v [ 'mean' ] = v [ 'sigma' ] = v [ 'zero count' ] = None x = [ v [ 'type' ] , mins , v [ 'mean' ] , maxs , v [ 'sigma' ] , v [ 'zero count' ] , v [ 'missing count' ] ] + x d [ k ] = x # Insert into ordered-dict return tabulate . tabulate ( d , headers = "keys" , tablefmt = tablefmt )
def run instances ( count , ec2 config , region , wait For SSH = True , tags = None ) : ec2params = inheritparams ( ec2 config , EC2 API RUN INSTANCE ) ec2params . setdefault ( 'min count' , count ) ec2params . setdefault ( 'max count' , count ) reservation = None conn = ec2 connect ( region ) try : reservation = conn . run instances ( * * ec2params ) log ( 'Reservation: {0}' . format ( reservation . id ) ) log ( 'Waiting for {0} EC2 instances {1} to come up, this can take 1-2 minutes.' . format ( len ( reservation . instances ) , reservation . instances ) ) start = time . time ( ) time . sleep ( 1 ) for instance in reservation . instances : while instance . update ( ) == 'pending' : time . sleep ( 1 ) h2o cmd . dot ( ) if not instance . state == 'running' : raise Exception ( '\033[91m[ec2] Error waiting for running state. Instance is in state {0}.\033[0m' . format ( instance . state ) ) log ( 'Instances started in {0} seconds' . format ( time . time ( ) - start ) ) log ( 'Instances: ' ) for inst in reservation . instances : log ( "   {0} ({1}) : public ip: {2}, private ip: {3}" . format ( inst . public dns name , inst . id , inst . ip address , inst . private ip address ) ) if wait For SSH : # kbn: changing to private address, so it should fail if not in right domain # used to have the public ip address wait for ssh ( [ i . private ip address for i in reservation . instances ] ) # Tag instances try : if tags : conn . create tags ( [ i . id for i in reservation . instances ] , tags ) except : warn ( 'Something wrong during tagging instances. Exceptions IGNORED!' ) print sys . exc info ( ) pass return reservation except : print "\033[91m Unexpected error\033[0m :" , sys . exc info ( ) if reservation : terminate reservation ( reservation , region ) raise
def terminate instances ( instances , region ) : if not instances : return conn = ec2 connect ( region ) log ( "Terminating instances {0}." . format ( instances ) ) conn . terminate instances ( instances ) log ( "Done" )
def stop instances ( instances , region ) : if not instances : return conn = ec2 connect ( region ) log ( "Stopping instances {0}." . format ( instances ) ) conn . stop instances ( instances ) log ( "Done" )
def start instances ( instances , region ) : if not instances : return conn = ec2 connect ( region ) log ( "Starting instances {0}." . format ( instances ) ) conn . start instances ( instances ) log ( "Done" )
def reboot instances ( instances , region ) : if not instances : return conn = ec2 connect ( region ) log ( "Rebooting instances {0}." . format ( instances ) ) conn . reboot instances ( instances ) log ( "Done" )
def wait for ssh ( ips , port = 22 , skip Alive = True , requiredsuccess = 3 ) : log ( 'Waiting for SSH on following hosts: {0}' . format ( ips ) ) for ip in ips : if not skip Alive or not ssh live ( ip , port ) : log ( 'Waiting for SSH on instance {0}...' . format ( ip ) ) count = 0 while count < requiredsuccess : if ssh live ( ip , port ) : count += 1 else : count = 0 time . sleep ( 1 ) h2o cmd . dot ( )
def join ( self ) : self . future = False self . job . poll ( ) model key = self . job . dest key self . job = None model json = h2o . api ( "GET /%d/Models/%s" % ( self . rest version , model key ) ) [ "models" ] [ 0 ] self . resolve model ( model key , model json )
def signal handler ( signum , stackframe ) : global g runner global g handling signal if g handling signal : # Don't do this recursively. return g handling signal = True print ( "" ) print ( "----------------------------------------------------------------------" ) print ( "" ) print ( "SIGNAL CAUGHT (" + str ( signum ) + ").  TEARING DOWN CLOUDS." ) print ( "" ) print ( "----------------------------------------------------------------------" ) g runner . terminate ( )
def usage ( ) : print ( "" ) print ( "Usage:  " + g script name + " [...options...]" ) print ( "" ) print ( "    (Output dir is: " + str ( g output dir ) + ")" ) print ( "    (Default number of clouds is: " + str ( g num clouds ) + ")" ) print ( "" ) print ( "    --wipeall        Remove all prior test state before starting, particularly" ) print ( "                     random seeds." ) print ( "                     (Removes master seed file and all Rsandbox directories." ) print ( "                     Also wipes the output dir before starting.)" ) print ( "" ) print ( "    --wipe           Wipes the output dir before starting.  Keeps old random seeds." ) print ( "" ) print ( "    --baseport       The first port at which H2O starts searching for free ports." ) print ( "" ) print ( "    --numclouds      The number of clusters to start." ) print ( "                     Each test is randomly assigned to a cluster." ) print ( "" ) print ( "    --numnodes       The number of nodes in the cluster." ) print ( "                     When this is specified, numclouds must be 1." ) print ( "" ) print ( "    --test           If you only want to run one test, specify it like this." ) print ( "" ) print ( "    --testlist       A file containing a list of tests to run (for example the" ) print ( "                     'failed.txt' file from the output directory)." ) print ( "    --excludelist    A file containing a list of tests to NOT run." ) print ( "" ) print ( "    --testgroup      Test a group of tests by function:" ) print ( "                     pca, glm, kmeans, gbm, rf, deeplearning, algos, golden, munging, parser" ) print ( "" ) print ( "    --testsize       Sizes (and by extension length) of tests to run:" ) print ( "                     s=small (seconds), m=medium (a minute or two), l=large (longer), x=xlarge (very big)" ) print ( "                     (Default is to run all tests.)" ) print ( "" ) print ( "    --usecloud       ip:port of cluster to send tests to instead of starting clusters." ) print ( "                     (When this is specified, numclouds is ignored.)" ) print ( "" ) print ( "    --usecloud2      cloud.cfg: Use a set clusters defined in cloud.config to run tests on." ) print ( "                     (When this is specified, numclouds, numnodes, and usecloud are ignored.)" ) print ( "" ) print ( "    --client         Send REST API commands through client mode." ) print ( "" ) print ( "    --norun          Perform side effects like wipe, but don't actually run tests." ) print ( "" ) print ( "    --jvm.xmx        Configure size of launched JVM running H2O. E.g. '--jvm.xmx 3g'" ) print ( "" ) print ( "    --jvm.cp         Classpath argument, in addition to h2o.jar path. E.g. " "'--jvm.cp /Users/h2o/mysql-connector-java-5.1.38-bin.jar'" ) print ( "" ) print ( "    --nopass         Run the NOPASS and NOFEATURE tests only and do not ignore any failures." ) print ( "" ) print ( "    --nointernal     Don't run the INTERNAL tests." ) print ( "" ) print ( "    --c              Start the JV Ms in a convenient location." ) print ( "" ) print ( "    --h2ojar         Supply a path to the H2O jar file." ) print ( "" ) print ( "    --tar            Supply a path to the R TAR." ) print ( "" ) print ( "" ) print ( "    --pto            The phantomjs timeout in seconds. Default is 3600 (1hr)." ) print ( "" ) print ( "    --noxunit        Do not produce x Unit reports." ) print ( "" ) print ( "    --r Pkg Ver Chk     Check that Jenkins-approved R packages/versions are present" ) print ( "" ) print ( "    --on Hadoop       Indication that tests will be run on h2o multinode hadoop clusters." ) print ( "                     `locate` and `sandbox` runit/pyunit test utilities use this indication in order to" ) print ( "                     behave properly. --hadoop Namenode must be specified if --on Hadoop option is used." ) print ( "    --hadoop Namenode Specifies that the runit/pyunit tests have access to this hadoop namenode." ) print ( "                     runit/pyunit test utilities have ability to retrieve this value." ) print ( "" ) print ( "    --perf           Save Jenkins build id, date, machine ip, git hash, name, start time, finish time," ) print ( "                     pass, ncpus, os, and job name of each test to perf.csv in the results directory." ) print ( "                     Takes three parameters: git hash, git branch, and build id, job name in that order." ) print ( "" ) print ( "    --jacoco         Generate code coverage data using Ja Co Co. Class includes and excludes may optionally" ) print ( "                     follow in the format of [includes]:[excludes] where [...] denotes a list of" ) print ( "                     classes, each separated by a comma (,). Wildcard characters (* and ?) may be used." ) print ( "" ) print ( "    --geterrs        Generate xml file that contains the actual unit test errors and the actual Java error." ) print ( "" ) print ( "    --test.ssl       Runs all the nodes with SSL enabled." ) print ( "" ) print ( "    --ldap.username  Username for LDAP." ) print ( "" ) print ( "    --ldap.password  Password for LDAP." ) print ( "" ) print ( "    --ldap.config    Path to LDAP config. If set, all nodes will be started with LDAP support." ) print ( "" ) print ( "    --kerb.principal  Kerberos service principal." ) print ( "" ) print ( "    --jvm.opts       Additional JVM options." ) print ( "" ) print ( "    --rest Log        If set, enable REST API logging. Logs will be available at <results Dir>/rest.log." ) print ( "                     Please note, that enablig REST API logging will increase the execution time and that" ) print ( "                     the log file might be large (> 2GB)." ) print ( "    If neither --test nor --testlist is specified, then the list of tests is" ) print ( "    discovered automatically as files matching '*runit*.R'." ) print ( "" ) print ( "" ) print ( "Examples:" ) print ( "" ) print ( "    Just accept the defaults and go (note: output dir must not exist):" ) print ( "        " + g script name ) print ( "" ) print ( "    Remove all random seeds (i.e. make new ones) but don't run any tests:" ) print ( "        " + g script name + " --wipeall --norun" ) print ( "" ) print ( "    For a powerful laptop with 8 cores (keep default numclouds):" ) print ( "        " + g script name + " --wipeall" ) print ( "" ) print ( "    For a big server with 32 cores:" ) print ( "        " + g script name + " --wipeall --numclouds 16" ) print ( "" ) print ( "    Just run the tests that finish quickly" ) print ( "        " + g script name + " --wipeall --testsize s" ) print ( "" ) print ( "    Run one specific test, keeping old random seeds:" ) print ( "        " + g script name + " --wipe --test path/to/test.R" ) print ( "" ) print ( "    Rerunning failures from a previous run, keeping old random seeds:" ) print ( "        # Copy failures.txt, otherwise --wipe removes the directory with the list!" ) print ( "        cp " + os . path . join ( g output dir , "failures.txt" ) + " ." ) print ( "        " + g script name + " --wipe --numclouds 16 --testlist failed.txt" ) print ( "" ) print ( "    Run tests on a pre-existing cloud (e.g. in a debugger), keeping old random seeds:" ) print ( "        " + g script name + " --wipe --usecloud ip:port" ) print ( "" ) print ( "    Run tests with Ja Co Co enabled, excluding org.example1 and org.example2" ) print ( "        " + g script name + " --jacoco :org.example1,org.example2" ) sys . exit ( 1 )
def wipe output dir ( ) : print ( "Wiping output directory." ) try : if os . path . exists ( g output dir ) : shutil . rmtree ( str ( g output dir ) ) except OS Error as e : print ( "ERROR: Removing output directory %s failed: " % g output dir ) print ( "       (errno {0}): {1}" . format ( e . errno , e . strerror ) ) print ( "" ) sys . exit ( 1 )
def get ip ( self ) : if len ( self . client nodes ) > 0 : node = self . client nodes [ 0 ] else : node = self . nodes [ 0 ] return node . get ip ( )
def get port ( self ) : if len ( self . client nodes ) > 0 : node = self . client nodes [ 0 ] else : node = self . nodes [ 0 ] return node . get port ( )
def determine vec size ( self ) : first column = self . pre trained . types [ self . pre trained . columns [ 0 ] ] if first column != 'string' : raise H2O Value Error ( "First column of given pre trained model %s is required to be a String" , self . pre trained . frame id ) if list ( self . pre trained . types . values ( ) ) . count ( 'string' ) > 1 : raise H2O Value Error ( "There are multiple columns in given pre trained model %s with a String type." , self . pre trained . frame id ) self . vec size = self . pre trained . dim [ 1 ] - 1
def get lambda source code ( lambda fn , src ) : def gen lambdas ( ) : def gen ( ) : yield src + "\n" g = gen ( ) step = 0 tokens = [ ] for tok in tokenize . generate tokens ( getattr ( g , "next" , getattr ( g , " next " , None ) ) ) : if step == 0 : if tok [ 0 ] == tokenize . NAME and tok [ 1 ] == "lambda" : step = 1 tokens = [ tok ] level = 0 elif step == 1 : if tok [ 0 ] == tokenize . NAME : tokens . append ( tok ) step = 2 else : step = 0 elif step == 2 : if tok [ 0 ] == tokenize . OP and tok [ 1 ] == ":" : tokens . append ( tok ) step = 3 else : step = 0 elif step == 3 : if level == 0 and ( tok [ 0 ] == tokenize . OP and tok [ 1 ] in ",)" or tok [ 0 ] == tokenize . ENDMARKER ) : yield tokenize . untokenize ( tokens ) . strip ( ) step = 0 else : tokens . append ( tok ) if tok [ 0 ] == tokenize . OP : if tok [ 1 ] in "[({" : level += 1 if tok [ 1 ] in "])}" : level -= 1 assert not tokens actual code = lambda fn . code . co code for lambda src in gen lambdas ( ) : try : fn = eval ( lambda src , globals ( ) , locals ( ) ) if fn . code . co code == actual code : return lambda src . split ( ":" , 1 ) [ 1 ] . strip ( ) except Exception : pass return "<lambda>"
def name ( self , src = None ) : res = [ get type name ( tt , src ) for tt in self . types ] if len ( res ) == 2 and "None" in res : res . remove ( "None" ) return "?" + res [ 0 ] else : return " | " . join ( res )
def check ( self , var ) : return all ( check type ( var , tt ) for tt in self . types )
def name ( self , src = None ) : return " & " . join ( get type name ( tt , src ) for tt in self . types )
def check ( self , var ) : return not any ( check type ( var , tt ) for tt in self . types )
def name ( self , src = None ) : if len ( self . types ) > 1 : return "!(%s)" % str ( "|" . join ( get type name ( tt , src ) for tt in self . types ) ) else : return "!" + get type name ( self . types [ 0 ] , src )
def check ( self , var ) : return isinstance ( var , tuple ) and all ( check type ( t , self . element type ) for t in var )
def check ( self , var ) : if not isinstance ( var , dict ) : return False if any ( key not in self . types for key in var ) : return False for key , ktype in viewitems ( self . types ) : val = var . get ( key , None ) if not check type ( val , ktype ) : return False return True
def name ( self , src = None ) : return "{%s}" % ", " . join ( "%s: %s" % ( key , get type name ( ktype , src ) ) for key , ktype in viewitems ( self . types ) )
def check ( self , var ) : return ( isinstance ( var , int type ) and ( self . lower bound is None or var >= self . lower bound ) and ( self . upper bound is None or var <= self . upper bound ) )
def name ( self , src = None ) : if self . upper bound is None and self . lower bound is None : return "int" if self . upper bound is None : if self . lower bound == 1 : return "int>0" return "int≥%d" % s lf. l ower bound if self . lower bound is None : return "int≤%d" % s lf. u pper bound return "int[%d…%d]" % ( e lf. l ower bound,  s lf. u pper bound)
def check ( self , var ) : return ( isinstance ( var , num type ) and ( self . lower bound is None or var >= self . lower bound ) and ( self . upper bound is None or var <= self . upper bound ) )
def check ( self , var ) : if self . class is None : self . init ( ) return self . class and self . checker ( var , self . class )
def check ( self , var ) : if not isinstance ( var , str type ) : return False return enum mangle ( var ) in self . consts
def get config ( ) : self = H2O Config Reader . get instance ( ) if not self . config loaded : self . read config ( ) return self . config
def read config ( self ) : self . config loaded = True conf = [ ] for f in self . candidate log files ( ) : if os . path . isfile ( f ) : self . logger . info ( "Reading config file %s" % f ) section rx = re . compile ( r"^\[(\w+)\]$" ) keyvalue rx = re . compile ( r"^(\w+:)?([\w.]+)\s*=(.*)$" ) with io . open ( f , "rt" , encoding = "utf-8" ) as config file : section name = None for lineno , line in enumerate ( config file ) : line = line . strip ( ) if line == "" or line . startswith ( "#" ) : continue m1 = section rx . match ( line ) if m1 : section name = m1 . group ( 1 ) continue m2 = keyvalue rx . match ( line ) if m2 : lng = m2 . group ( 1 ) key = m2 . group ( 2 ) val = m2 . group ( 3 ) . strip ( ) if lng and lng . lower ( ) != "py:" : continue if section name : key = section name + "." + key if key in H2O Config Reader . allowed config keys : conf . append ( ( key , val ) ) else : self . logger . error ( "Key %s is not a valid config key" % key ) continue self . logger . error ( "Syntax error in config file line %d: %s" % ( lineno , line ) ) self . config = dict ( conf ) return
def candidate log files ( ) : # Search for .h2oconfig in the current directory and all parent directories relpath = ".h2oconfig" prevpath = None while True : abspath = os . path . abspath ( relpath ) if abspath == prevpath : break prevpath = abspath relpath = "../" + relpath yield abspath # Also check if .h2oconfig exists in the user's directory yield os . path . expanduser ( "~/.h2oconfig" )
def recalculate model parameters ( self , now ) : time until end = self . estimate progress completion time ( now ) - now assert time until end >= 0 , "Estimated progress completion cannot be in the past." x real = self . get real progress ( ) if x real == 1 : t0 , x0 , v0 , ve = now , 1 , 0 , 0 else : x0 , v0 = self . compute progress at time ( now ) t0 = now if x0 >= 1 : # On rare occasion, the model's progress may have reached 100% by ``now``. This can happen if # (1) the progress is close to 100% initially and has high speed, (2) on the previous call we # estimated that the process completion time will be right after the next poll time, and (3) # the polling itself took so much time that the process effectively "overshoot". # If this happens, then we adjust x0, v0 to the previous valid data checkpoint. t0 , x0 , v0 = self . t0 , self . x0 , self . v0 time until end += now - t0 z = self . BETA * time until end max speed = ( 1 - x real ** 2 ) / self . FINISH DELAY ve = v0 + ( self . BETA * ( 1 - x0 ) - v0 * z ) / ( z - 1 + math . exp ( - z ) ) if ve < 0 : # Current speed is too high -- reduce v0 (violate non-smoothness of speed) v0 = self . BETA * ( 1 - x0 ) / ( 1 - math . exp ( - z ) ) ve = 0 if ve > max speed : # Current speed is too low: finish later, but do not allow ``ve`` to be higher than ``max speed`` ve = max speed self . t0 , self . x0 , self . v0 , self . ve = t0 , x0 , v0 , ve
def draw ( self , txt , final = False ) : if not self . file mode : # If the user presses Ctrl+C this ensures we still start writing from the beginning of the line sys . stdout . write ( "\r" ) sys . stdout . write ( txt ) if final and not isinstance ( self . widget , Hidden Widget ) : sys . stdout . write ( "\n" ) else : if not self . file mode : sys . stdout . write ( "\r" ) sys . stdout . flush ( )
def render ( self , progress , width = None , status = None ) : results = [ widget . render ( progress , width = self . widget lengths [ i ] , status = status ) for i , widget in enumerate ( self . widgets ) ] if self . file mode : res = "" for i , result in enumerate ( results ) : res += result . rendered if result . length < self . widget lengths [ i ] and progress < 1 : break res += " " if i < len ( results ) - 1 else "" rendered str = res [ len ( self . rendered ) : ] self . rendered = res else : rendered str = " " . join ( r . rendered for r in results ) if self . to render : rendered str = self . to render + rendered str self . to render = None next progress = min ( r . next progress for r in results ) next time = min ( r . next time for r in results ) return Render Result ( rendered str , next progress = next progress , next time = next time )
def compute widget sizes ( self ) : wl = [ 0 ] * len ( self . widgets ) flex count = 0 # First render all non-flexible widgets for i , widget in enumerate ( self . widgets ) : if isinstance ( widget , Progress Bar Flexible Widget ) : flex count += 1 else : wl [ i ] = widget . render ( 1 ) . length remaining width = self . width - sum ( wl ) remaining width -= len ( self . widgets ) - 1 # account for 1-space interval between widgets if remaining width < 10 * flex count : if self . file mode : remaining width = 10 * flex count else : # The window is too small to accomodate the widget: try to split it into several lines, otherwise # switch to "file mode". If we don't do this, then rendering the widget will cause it to wrap, and # then when we use \r to go to the beginning of the line, only part of the widget will be overwritten, # which means we'll have many (possibly hundreds) of progress bar lines in the end. widget0 = self . widgets [ 0 ] if isinstance ( widget0 , PBW String ) and remaining width + widget0 . render ( 0 ) . length >= 10 * flex count : remaining width += widget0 . render ( 0 ) . length + 1 self . to render = widget0 . render ( 0 ) . rendered + "\n" self . widgets = self . widgets [ 1 : ] if remaining width < 10 * flex count : self . file mode = True remaining width = 10 * flex count remaining width = max ( remaining width , 10 * flex count ) # Ensure at least 10 chars per flexible widget for i , widget in enumerate ( self . widgets ) : if isinstance ( widget , Progress Bar Flexible Widget ) : target length = int ( remaining width / flex count ) result = widget . render ( 1 , target length ) wl [ i ] = result . length remaining width -= result . length flex count -= 1 return wl
def get terminal size ( ) : # If output is not terminal but a regular file, assume 100 chars width if not sys . stdout . isatty ( ) : return 80 # Otherwise, first try getting the dimensions from shell command `stty`: try : import subprocess ret = subprocess . check output ( [ "stty" , "size" ] ) . strip ( ) . split ( " " ) if len ( ret ) == 2 : return int ( ret [ 1 ] ) except : pass # Otherwise try using ioctl try : from termios import TIOCGWINSZ from fcntl import ioctl from struct import unpack res = unpack ( "hh" , ioctl ( sys . stdout , TIOCGWINSZ , b"1234" ) ) return int ( res [ 1 ] ) except : pass # Finally check the COLUMNS environment variable return int ( os . environ . get ( "COLUMNS" , 80 ) )
def render ( self , progress , width = None , status = None ) : if width <= 3 : return Render Result ( ) bar width = width - 2 # Total width minus the bar ends n chars = int ( progress * bar width + 0.001 ) endf , endl = self . bar ends if self . file mode : out = endf out += self . bar symbols [ - 1 ] * n chars out += endl if progress == 1 else "" if status : out += " (%s)" % status next progress = ( n chars + 1 ) / bar width rendered len = len ( out ) else : frac chars = int ( ( progress * bar width - n chars ) * len ( self . bar symbols ) ) out = endf out += self . bar symbols [ - 1 ] * n chars out += self . bar symbols [ frac chars - 1 ] if frac chars > 0 else "" rendered len = len ( out ) if status : out += colorama . Fore . RED + " (" + status + ")" + colorama . Style . RESET ALL rendered len += 3 + len ( status ) out += " " * ( width - 1 - rendered len ) out += endl next progress = ( n chars + ( frac chars + 1 ) / len ( self . bar symbols ) ) / bar width rendered len += max ( 0 , width - 1 - rendered len ) + 1 return Render Result ( rendered = out , length = rendered len , next progress = next progress )
def set encoding ( self , encoding ) : self . bar ends = "[]" self . bar symbols = "#" if not encoding : return s1 = "\u258F\u258E\u258D\u258C\u258B\u258A\u2589\u2588" s2 = "\u258C\u2588" s3 = "\u2588" if self . file mode : s1 = s2 = None assert len ( s3 ) == 1 for s in ( s1 , s2 , s3 ) : if s is None : continue try : s . encode ( encoding ) self . bar ends = "||" self . bar symbols = s return except Unicode Encode Error : pass except Lookup Error : print ( "Warning: unknown encoding %s" % encoding )
def render ( self , progress , width = None , status = None ) : current pct = int ( progress * 100 + 0.1 ) return Render Result ( rendered = "%3d%%" % current pct , next progress = ( current pct + 1 ) / 100 )
def refresh ( self ) : self . ex . cache . flush ( ) self . frame ( fill cache = True )
def structure ( self ) : df = self . as data frame ( use pandas = False ) cn = df . pop ( 0 ) nr = self . nrow nc = self . ncol width = max ( [ len ( c ) for c in cn ] ) isfactor = self . isfactor ( ) numlevels = self . nlevels ( ) lvls = self . levels ( ) print ( "H2O Frame: '{}' \n Dimensions: {} obs. of {} variables" . format ( self . frame id , nr , nc ) ) for i in range ( nc ) : print ( "$ {} {}: " . format ( cn [ i ] , ' ' * ( width - max ( 0 , len ( cn [ i ] ) ) ) ) , end = ' ' ) if isfactor [ i ] : nl = numlevels [ i ] print ( "Factor w/ {} level(s) {} " . format ( nl , '"' + '","' . join ( lvls [ i ] ) + '"' ) , end = '\n' ) else : print ( "num {}" . format ( " " . join ( it [ 0 ] if it else "nan" for it in h2o . as list ( self [ : 10 , i ] , False ) [ 1 : ] ) ) )
def parse text ( text ) : assert isinstance ( text , str type ) , "`text` parameter should be a string, got %r" % type ( text ) gen = iter ( text . splitlines ( True ) ) # True = keep newlines readline = gen . next if hasattr ( gen , "next" ) else gen . next return Code ( tokenize ( readline ) )
def parse file ( filename ) : assert isinstance ( filename , str type ) , "`filename` parameter should be a string, got %r" % type ( filename ) with open ( filename , "rt" , encoding = "utf-8" ) as f : return Code ( tokenize ( f . readline ) )
def move ( self , drow , dcol = 0 ) : self . start row += drow self . start col += dcol self . end row += drow self . end col += dcol
def unparse ( self ) : ut = Untokenizer ( start row = self . tokens [ 0 ] . start row ) self . unparse ( ut ) return ut . result ( )
def centers ( self ) : o = self . model json [ "output" ] cvals = o [ "centers" ] . cell values centers = [ list ( cval [ 1 : ] ) for cval in cvals ] return centers
def centers std ( self ) : o = self . model json [ "output" ] cvals = o [ "centers std" ] . cell values centers std = [ list ( cval [ 1 : ] ) for cval in cvals ] centers std = [ list ( x ) for x in zip ( * centers std ) ] return centers std
def version check ( ) : from . init import version as ver pkg ci = h2oconn . cluster if not ci : raise H2O Connection Error ( "Connection not initialized. Did you run h2o.connect()?" ) ver h2o = ci . version if ver pkg == "SUBST PROJECT VERSION" : ver pkg = "UNKNOWN" if str ( ver h2o ) != str ( ver pkg ) : branch name h2o = ci . branch name build number h2o = ci . build number if build number h2o is None or build number h2o == "unknown" : raise H2O Connection Error ( "Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. " "Upgrade H2O and h2o-Python to latest stable version - " "http://h2o-release.s3.amazonaws.com/h2o/latest stable.html" "" . format ( ver h2o , ver pkg ) ) elif build number h2o == "99999" : raise H2O Connection Error ( "Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. " "This is a developer build, please contact your developer." "" . format ( ver h2o , ver pkg ) ) else : raise H2O Connection Error ( "Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. " "Install the matching h2o-Python version from - " "http://h2o-release.s3.amazonaws.com/h2o/{2}/{3}/index.html." "" . format ( ver h2o , ver pkg , branch name h2o , build number h2o ) ) # Check age of the install if ci . build too old : print ( "Warning: Your H2O cluster version is too old ({})! Please download and install the latest " "version from http://h2o.ai/download/" . format ( ci . build age ) )
def load dataset ( relative path ) : assert is type ( relative path , str ) h2o dir = os . path . split ( file ) [ 0 ] for possible file in [ os . path . join ( h2o dir , relative path ) , os . path . join ( h2o dir , "h2o data" , relative path ) , os . path . join ( h2o dir , "h2o data" , relative path + ".csv" ) ] : if os . path . exists ( possible file ) : return upload file ( possible file ) # File not found -- raise an error! raise H2O Value Error ( "Data file %s cannot be found" % relative path )
def check frame id ( frame id ) : if frame id is None : return if frame id . strip ( ) == "" : raise H2O Value Error ( "Frame id cannot be an empty string: %r" % frame id ) for i , ch in enumerate ( frame id ) : # '$' character has special meaning at the beginning of the string; and prohibited anywhere else if ch == "$" and i == 0 : continue if ch not in id allowed characters : raise H2O Value Error ( "Character '%s' is illegal in frame id: %s" % ( ch , frame id ) ) if re . match ( r"-?[0-9]" , frame id ) : raise H2O Value Error ( "Frame id cannot start with a number: %s" % frame id )
def slice is normalized ( s ) : return ( s . start is not None and s . stop is not None and s . step is not None and s . start <= s . stop )
def deprecated ( message ) : from traceback import extract stack assert message , "`message` argument in @deprecated is required." def deprecated decorator ( fun ) : def decorator invisible ( * args , * * kwargs ) : stack = extract stack ( ) assert len ( stack ) >= 2 and stack [ - 1 ] [ 2 ] == "decorator invisible" , "Got confusing stack... %r" % stack print ( "[WARNING] in %s line %d:" % ( stack [ - 2 ] [ 0 ] , stack [ - 2 ] [ 1 ] ) ) print ( "    >>> %s" % ( stack [ - 2 ] [ 3 ] or "????" ) ) print ( "        ^^^^ %s" % message ) return fun ( * args , * * kwargs ) decorator invisible . doc = message decorator invisible . name = fun . name decorator invisible . module = fun . module decorator invisible . deprecated = True return decorator invisible return deprecated decorator
def join ( self ) : self . future = False self . job . poll ( ) self . job = None
def summary ( self , header = True ) : table = [ ] for model in self . models : model summary = model . model json [ "output" ] [ "model summary" ] r values = list ( model summary . cell values [ 0 ] ) r values [ 0 ] = model . model id table . append ( r values ) # if h2o.can use pandas(): #  import pandas #  pandas.options.display.max rows = 20 #  print pandas.Data Frame(table,columns=self.col header) #  return print ( ) if header : print ( 'Grid Summary:' ) print ( ) H2O Display ( table , [ 'Model Id' ] + model summary . col header [ 1 : ] , numalign = "left" , stralign = "left" )
def show ( self ) : hyper combos = itertools . product ( * list ( self . hyper params . values ( ) ) ) if not self . models : c values = [ [ idx + 1 , list ( val ) ] for idx , val in enumerate ( hyper combos ) ] print ( H2O Two Dim Table ( col header = [ 'Model' , 'Hyperparameters: [' + ', ' . join ( list ( self . hyper params . keys ( ) ) ) + ']' ] , table header = 'Grid Search of Model ' + self . model . class . name , cell values = c values ) ) else : print ( self . sorted metric table ( ) )
def parse ( self ) : f = open ( self . parse log path , "r" ) self . parse2 ( f ) f . close ( )
def parse2 ( self , f ) : line num = 0 s = f . readline ( ) while ( len ( s ) > 0 ) : line num += 1 # Check for beginning of parsed data set. match groups = re . search ( r"Parse result for (.*) .(\d+) rows.:" , s ) if ( match groups is not None ) : dataset name = match groups . group ( 1 ) if ( self . dataset name is not None ) : print ( "ERROR: Too many datasets found on file {} line {}" . format ( self . parse log path , line num ) ) sys . exit ( 1 ) self . dataset name = dataset name num rows = int ( match groups . group ( 2 ) ) self . num rows = num rows s = f . readline ( ) continue # Check for numeric column. match groups = re . search ( r"INFO WATER:" + r"\s*C(\d+):" + r"\s*numeric" + r"\s*min\((\S*)\)" + r"\s*max\((\S*).\)" + r"\s*(na\((\S+)\))?" + r"\s*(constant)?" , s ) if ( match groups is not None ) : col num = int ( match groups . group ( 1 ) ) min val = float ( match groups . group ( 2 ) ) max val = float ( match groups . group ( 3 ) ) # four = match groups.group(4) na count = match groups . group ( 5 ) if ( na count is None ) : na count = 0 else : na count = int ( na count ) constant str = match groups . group ( 6 ) is constant = constant str is not None if ( is constant ) : if ( min val != max val ) : print ( "ERROR: is constant mismatch on file {} line {}" . format ( self . parse log path , line num ) ) sys . exit ( 1 ) na fraction = float ( na count ) / float ( self . num rows ) is min integer = float ( int ( min val ) ) == float ( min val ) is max integer = float ( int ( min val ) ) == float ( min val ) is integer = is min integer and is max integer c = Real Column ( col num , "C" + str ( col num ) , min val , max val , na fraction , is integer ) self . add col ( c ) s = f . readline ( ) continue # Check for categorical column. match groups = re . search ( r"INFO WATER:" + r"\s*C(\d+):" + r"\s*categorical" + r"\s*min\((\S*)\)" + r"\s*max\((\S*).\)" + r"\s*(na\((\S+)\))?" + r"\s*(constant)?" + r"\s*cardinality\((\d+)\)" , s ) if ( match groups is not None ) : col num = int ( match groups . group ( 1 ) ) min val = float ( match groups . group ( 2 ) ) max val = float ( match groups . group ( 3 ) ) # four = match groups.group(4) na count = match groups . group ( 5 ) if ( na count is None ) : na count = 0 else : na count = int ( na count ) constant str = match groups . group ( 6 ) is constant = constant str is not None if ( is constant ) : if ( min val != max val ) : print ( "ERROR: is constant mismatch on file {} line {}" . format ( self . parse log path , line num ) ) sys . exit ( 1 ) num levels = int ( match groups . group ( 7 ) ) if ( is constant ) : if ( num levels != 1 ) : print ( "ERROR: num levels mismatch on file {} line {}" . format ( self . parse log path , line num ) ) sys . exit ( 1 ) na fraction = float ( na count ) / float ( self . num rows ) c = Categorical Column ( col num , "C" + str ( col num ) , num levels , na fraction ) self . add col ( c ) s = f . readline ( ) continue print ( "ERROR: Unrecognized regexp pattern on file {} line {}" . format ( self . parse log path , line num ) ) sys . exit ( 1 )
def log start transaction ( self , endpoint , data , json , files , params ) : # TODO: add information about the caller, i.e. which module + line of code called the .request() method #       This can be done by fetching current traceback and then traversing it until we find the request function self . requests counter += 1 if not self . is logging : return msg = "\n---- %d --------------------------------------------------------\n" % self . requests counter msg += "[%s] %s\n" % ( time . strftime ( "%H:%M:%S" ) , endpoint ) if params is not None : msg += "     params: {%s}\n" % ", " . join ( "%s:%s" % item for item in viewitems ( params ) ) if data is not None : msg += "     body: {%s}\n" % ", " . join ( "%s:%s" % item for item in viewitems ( data ) ) if json is not None : import json as j msg += "     json: %s\n" % j . dumps ( json ) if files is not None : msg += "     file: %s\n" % ", " . join ( f . name for f in viewvalues ( files ) ) self . log message ( msg + "\n" )
def log end transaction ( self , start time , response ) : if not self . is logging : return elapsed time = int ( ( time . time ( ) - start time ) * 1000 ) msg = "<<< HTTP %d %s   (%d ms)\n" % ( response . status code , response . reason , elapsed time ) if "Content-Type" in response . headers : msg += "    Content-Type: %s\n" % response . headers [ "Content-Type" ] msg += response . text self . log message ( msg + "\n\n" )
def print ( self , msg , flush = False , end = "\n" ) : if self . verbose : print2 ( msg , end = end , flush = flush )
def normalize enum constant ( s ) : if s . islower ( ) : return s if s . isupper ( ) : return s . lower ( ) return "" . join ( ch if ch . islower ( ) else " " + ch . lower ( ) for ch in s ) . strip ( " " )
def default params ( self ) : params = { } for p in self . parms : params [ p ] = self . parms [ p ] [ "default value" ] return params
def actual params ( self ) : params to select = { "model id" : "name" , "response column" : "column name" , "training frame" : "name" , "validation frame" : "name" } params = { } for p in self . parms : if p in params to select . keys ( ) : params [ p ] = self . parms [ p ] [ "actual value" ] . get ( params to select [ p ] , None ) else : params [ p ] = self . parms [ p ] [ "actual value" ] return params
def show ( self ) : if self . future : self . job . poll once ( ) return if self . model json is None : print ( "No model trained yet" ) return if self . model id is None : print ( "This H2O Estimator has been removed." ) return model = self . model json [ "output" ] print ( "Model Details" ) print ( "=============" ) print ( self . class . name , ": " , self . model json [ "algo full name" ] ) print ( "Model Key: " , self . id ) self . summary ( ) print ( ) # training metrics tm = model [ "training metrics" ] if tm : tm . show ( ) vm = model [ "validation metrics" ] if vm : vm . show ( ) xm = model [ "cross validation metrics" ] if xm : xm . show ( ) xms = model [ "cross validation metrics summary" ] if xms : xms . show ( ) if "scoring history" in model and model [ "scoring history" ] : model [ "scoring history" ] . show ( ) if "variable importances" in model and model [ "variable importances" ] : model [ "variable importances" ] . show ( )
def gbm ( interactive = True , echo = True , testing = False ) : def demo body ( go ) : go ( ) # Connect to H2O h2o . init ( ) go ( ) # Upload the prostate dataset that comes included in the h2o python package prostate = h2o . load dataset ( "prostate" ) go ( ) # Print a description of the prostate data prostate . describe ( ) go ( ) # Randomly split the dataset into ~70/30, training/test sets train , test = prostate . split frame ( ratios = [ 0.70 ] ) go ( ) # Convert the response columns to factors (for binary classification problems) train [ "CAPSULE" ] = train [ "CAPSULE" ] . asfactor ( ) test [ "CAPSULE" ] = test [ "CAPSULE" ] . asfactor ( ) go ( ) # Build a (classification) GLM from h2o . estimators import H2O Gradient Boosting Estimator prostate gbm = H2O Gradient Boosting Estimator ( distribution = "bernoulli" , ntrees = 10 , max depth = 8 , min rows = 10 , learn rate = 0.2 ) prostate gbm . train ( x = [ "AGE" , "RACE" , "PSA" , "VOL" , "GLEASON" ] , y = "CAPSULE" , training frame = train ) go ( ) # Show the model prostate gbm . show ( ) go ( ) # Predict on the test set and show the first ten predictions predictions = prostate gbm . predict ( test ) predictions . show ( ) go ( ) # Fetch a tree, print number of tree nodes, show root node description from h2o . tree import H2O Tree , H2O Node tree = H2O Tree ( prostate gbm , 0 , "0" ) len ( tree ) tree . left children tree . right children tree . root node . show ( ) go ( ) # Show default performance metrics performance = prostate gbm . model performance ( test ) performance . show ( ) # Execute: run demo ( demo body , interactive , echo , testing )
def deeplearning ( interactive = True , echo = True , testing = False ) : def demo body ( go ) : go ( ) # Connect to H2O h2o . init ( ) go ( ) # Upload the prostate dataset that comes included in the h2o python package prostate = h2o . load dataset ( "prostate" ) go ( ) # Print a description of the prostate data prostate . describe ( ) go ( ) # Randomly split the dataset into ~70/30, training/test sets train , test = prostate . split frame ( ratios = [ 0.70 ] ) go ( ) # Convert the response columns to factors (for binary classification problems) train [ "CAPSULE" ] = train [ "CAPSULE" ] . asfactor ( ) test [ "CAPSULE" ] = test [ "CAPSULE" ] . asfactor ( ) go ( ) # Build a (classification) GLM from h2o . estimators import H2O Deep Learning Estimator prostate dl = H2O Deep Learning Estimator ( activation = "Tanh" , hidden = [ 10 , 10 , 10 ] , epochs = 10000 ) prostate dl . train ( x = list ( set ( prostate . col names ) - { "ID" , "CAPSULE" } ) , y = "CAPSULE" , training frame = train ) go ( ) # Show the model prostate dl . show ( ) go ( ) # Predict on the test set and show the first ten predictions predictions = prostate dl . predict ( test ) predictions . show ( ) go ( ) # Show default performance metrics performance = prostate dl . model performance ( test ) performance . show ( ) # Execute: run demo ( demo body , interactive , echo , testing )
def glm ( interactive = True , echo = True , testing = False ) : def demo body ( go ) : go ( ) # Connect to H2O h2o . init ( ) go ( ) # Upload the prostate dataset that comes included in the h2o python package prostate = h2o . load dataset ( "prostate" ) go ( ) # Print a description of the prostate data prostate . describe ( ) go ( ) # Randomly split the dataset into ~70/30, training/test sets train , test = prostate . split frame ( ratios = [ 0.70 ] ) go ( ) # Convert the response columns to factors (for binary classification problems) train [ "CAPSULE" ] = train [ "CAPSULE" ] . asfactor ( ) test [ "CAPSULE" ] = test [ "CAPSULE" ] . asfactor ( ) go ( ) # Build a (classification) GLM from h2o . estimators import H2O Generalized Linear Estimator prostate glm = H2O Generalized Linear Estimator ( family = "binomial" , alpha = [ 0.5 ] ) prostate glm . train ( x = [ "AGE" , "RACE" , "PSA" , "VOL" , "GLEASON" ] , y = "CAPSULE" , training frame = train ) go ( ) # Show the model prostate glm . show ( ) go ( ) # Predict on the test set and show the first ten predictions predictions = prostate glm . predict ( test ) predictions . show ( ) go ( ) # Show default performance metrics performance = prostate glm . model performance ( test ) performance . show ( ) # Execute: run demo ( demo body , interactive , echo , testing )
def as data frame ( self ) : if can use pandas ( ) : import pandas pandas . options . display . max colwidth = 70 return pandas . Data Frame ( self . cell values , columns = self . col header ) return self
def show ( self , header = True ) : # if h2o.can use pandas(): #  import pandas #  pandas.options.display.max rows = 20 #  print pandas.Data Frame(self. cell values,columns=self. col header) #  return if header and self . table header : print ( self . table header + ":" , end = ' ' ) if self . table description : print ( self . table description ) print ( ) table = copy . deepcopy ( self . cell values ) nr = 0 if is list of lists ( table ) : nr = len ( table ) # only set if we truly have multiple rows... not just one long row :) if nr > 20 : # create a truncated view of the table, first/last 5 rows trunc table = [ ] trunc table += [ v for v in table [ : 5 ] ] trunc table . append ( [ "---" ] * len ( table [ 0 ] ) ) trunc table += [ v for v in table [ ( nr - 5 ) : ] ] table = trunc table H2O Display ( table , self . col header , numalign = "left" , stralign = "left" ) if nr > 20 and can use pandas ( ) : print ( '\n See the whole table with table.as data frame()' )
def jar paths ( ) : # PUBDEV-3534 hook to use arbitrary h2o.jar own jar = os . getenv ( "H2O JAR PATH" , "" ) if own jar != "" : if not os . path . isfile ( own jar ) : raise H2O Startup Error ( "Environment variable H2O JAR PATH is set to '%d' but file does not exists, unset environment variable or provide valid path to h2o.jar file." % own jar ) yield own jar # Check if running from an h2o-3 src folder (or any subfolder), in which case use the freshly-built h2o.jar cwd chunks = os . path . abspath ( "." ) . split ( os . path . sep ) for i in range ( len ( cwd chunks ) , 0 , - 1 ) : if cwd chunks [ i - 1 ] == "h2o-3" : yield os . path . sep . join ( cwd chunks [ : i ] + [ "build" , "h2o.jar" ] ) # Then check the backend/bin folder: # (the following works assuming this code is located in h2o/backend/server.py file) backend dir = os . path . split ( os . path . realpath ( file ) ) [ 0 ] yield os . path . join ( backend dir , "bin" , "h2o.jar" ) # Then try several old locations where h2o.jar might have been installed prefix1 = prefix2 = sys . prefix # On Unix-like systems Python typically gets installed into /Library/... or /System/Library/... If one of # those paths is sys.prefix, then we also build its counterpart. if prefix1 . startswith ( os . path . sep + "Library" ) : prefix2 = os . path . join ( "" , "System" , prefix1 ) elif prefix1 . startswith ( os . path . sep + "System" ) : prefix2 = prefix1 [ len ( os . path . join ( "" , "System" ) ) : ] yield os . path . join ( prefix1 , "h2o jar" , "h2o.jar" ) yield os . path . join ( os . path . abspath ( os . sep ) , "usr" , "local" , "h2o jar" , "h2o.jar" ) yield os . path . join ( prefix1 , "local" , "h2o jar" , "h2o.jar" ) yield os . path . join ( get config var ( "userbase" ) , "h2o jar" , "h2o.jar" ) yield os . path . join ( prefix2 , "h2o jar" , "h2o.jar" )
def csv dict writer ( f , fieldnames , * * kwargs ) : import csv if "delimiter" in kwargs : kwargs [ "delimiter" ] = str ( kwargs [ "delimiter" ] ) return csv . Dict Writer ( f , fieldnames , * * kwargs )
def path2uri ( self , dirpath ) : relpath = dirpath . replace ( self . root path , self . package name ) if relpath . startswith ( os . path . sep ) : relpath = relpath [ 1 : ] return relpath . replace ( os . path . sep , '.' )
def parse module ( self , uri ) : filename = self . uri2path ( uri ) if filename is None : # nothing that we could handle here. return ( [ ] , [ ] ) f = open ( filename , 'rt' ) functions , classes = self . parse lines ( f ) f . close ( ) return functions , classes
def parse lines ( self , linesource ) : functions = [ ] classes = [ ] for line in linesource : if line . startswith ( 'def ' ) and line . count ( '(' ) : # exclude private stuff name = self . get object name ( line ) if not name . startswith ( ' ' ) : functions . append ( name ) elif line . startswith ( 'class ' ) : # exclude private stuff name = self . get object name ( line ) if not name . startswith ( ' ' ) : classes . append ( name ) else : pass functions . sort ( ) classes . sort ( ) return functions , classes
def to list ( self ) : return [ [ int ( self . table . cell values [ 0 ] [ 1 ] ) , int ( self . table . cell values [ 0 ] [ 2 ] ) ] , [ int ( self . table . cell values [ 1 ] [ 1 ] ) , int ( self . table . cell values [ 1 ] [ 2 ] ) ] ]
def locate files ( root dir ) : all files = [ ] root dir = os . path . abspath ( root dir ) for dir name , subdirs , files in os . walk ( root dir ) : for f in files : if f . endswith ( ".py" ) : all files . append ( os . path . join ( dir name , f ) ) return all files
def main ( ) : # magic files = {} for filename in locate files ( ROOT DIR ) : print ( "Processing %s" % filename ) with open ( filename , "rt" ) as f : tokens = list ( tokenize . generate tokens ( f . readline ) ) text1 = tokenize . untokenize ( tokens ) ntokens = normalize tokens ( tokens ) text2 = tokenize . untokenize ( ntokens ) assert text1 == text2
def show ( self ) : if self . metric json == None : print ( "WARNING: Model metrics cannot be calculated and metric json is empty due to the absence of the response column in your dataset." ) return metric type = self . metric json [ ' meta' ] [ 'schema type' ] types w glm = [ 'Model Metrics Regression GLM' , 'Model Metrics Binomial GLM' ] types w clustering = [ 'Model Metrics Clustering' ] types w mult = [ 'Model Metrics Multinomial' ] types w ord = [ 'Model Metrics Ordinal' ] types w bin = [ 'Model Metrics Binomial' , 'Model Metrics Binomial GLM' ] types w r2 = [ 'Model Metrics Regression GLM' ] types w mean residual deviance = [ 'Model Metrics Regression GLM' , 'Model Metrics Regression' ] types w mean absolute error = [ 'Model Metrics Regression GLM' , 'Model Metrics Regression' ] types w logloss = types w bin + types w mult + types w ord types w dim = [ "Model Metrics GLRM" ] types w anomaly = [ 'Model Metrics Anomaly' ] print ( ) print ( metric type + ": " + self . algo ) reported on = "** Reported on {} data. **" if self . on train : print ( reported on . format ( "train" ) ) elif self . on valid : print ( reported on . format ( "validation" ) ) elif self . on xval : print ( reported on . format ( "cross-validation" ) ) else : print ( reported on . format ( "test" ) ) print ( ) if metric type not in types w anomaly : print ( "MSE: " + str ( self . mse ( ) ) ) print ( "RMSE: " + str ( self . rmse ( ) ) ) if metric type in types w mean absolute error : print ( "MAE: " + str ( self . mae ( ) ) ) print ( "RMSLE: " + str ( self . rmsle ( ) ) ) if metric type in types w r2 : print ( "R^2: " + str ( self . r2 ( ) ) ) if metric type in types w mean residual deviance : print ( "Mean Residual Deviance: " + str ( self . mean residual deviance ( ) ) ) if metric type in types w logloss : print ( "Log Loss: " + str ( self . logloss ( ) ) ) if metric type == 'Model Metrics Binomial' : # second element for first threshold is the actual mean per class error print ( "Mean Per-Class Error: %s" % self . mean per class error ( ) [ 0 ] [ 1 ] ) if metric type == 'Model Metrics Multinomial' or metric type == 'Model Metrics Ordinal' : print ( "Mean Per-Class Error: " + str ( self . mean per class error ( ) ) ) if metric type in types w glm : print ( "Null degrees of freedom: " + str ( self . null degrees of freedom ( ) ) ) print ( "Residual degrees of freedom: " + str ( self . residual degrees of freedom ( ) ) ) print ( "Null deviance: " + str ( self . null deviance ( ) ) ) print ( "Residual deviance: " + str ( self . residual deviance ( ) ) ) print ( "AIC: " + str ( self . aic ( ) ) ) if metric type in types w bin : print ( "AUC: " + str ( self . auc ( ) ) ) print ( "pr auc: " + str ( self . pr auc ( ) ) ) print ( "Gini: " + str ( self . gini ( ) ) ) self . confusion matrix ( ) . show ( ) self . metric json [ "max criteria and metric scores" ] . show ( ) if self . gains lift ( ) : print ( self . gains lift ( ) ) if metric type in types w anomaly : print ( "Anomaly Score: " + str ( self . mean score ( ) ) ) print ( "Normalized Anomaly Score: " + str ( self . mean normalized score ( ) ) ) if ( metric type in types w mult ) or ( metric type in types w ord ) : self . confusion matrix ( ) . show ( ) self . hit ratio table ( ) . show ( ) if metric type in types w clustering : print ( "Total Within Cluster Sum of Square Error: " + str ( self . tot withinss ( ) ) ) print ( "Total Sum of Square Error to Grand Mean: " + str ( self . totss ( ) ) ) print ( "Between Cluster Sum of Square Error: " + str ( self . betweenss ( ) ) ) self . metric json [ 'centroid stats' ] . show ( ) if metric type in types w dim : print ( "Sum of Squared Error (Numeric): " + str ( self . num err ( ) ) ) print ( "Misclassification Error (Categorical): " + str ( self . cat err ( ) ) ) if self . custom metric name ( ) : print ( "{}: {}" . format ( self . custom metric name ( ) , self . custom metric value ( ) ) )
def generate schema ( class name , schema ) : has map = False for field in schema [ "fields" ] : if field [ "type" ] . startswith ( "Map" ) : has map = True superclass = schema [ "superclass" ] if superclass == "Iced" : superclass = "Object" yield "/**" yield " * This file is auto-generated by h2o-3/h2o-bindings/bin/gen csharp.py" yield " * Copyright 2016 H2O.ai;  Apache License Version 2.0 (see LICENSE for details)" yield " */" yield "namespace ai.h2o" yield "{" yield "  using System;" yield "  using System.Collections.Generic;" if has map else None yield "" yield "  public class {name}: {super} {{" . format ( name = class name , super = superclass ) for field in schema [ "fields" ] : if field [ "name" ] == " meta" : continue csharp type = translate type ( field [ "type" ] , field [ "schema name" ] ) yield "    /// <summary>" yield bi . wrap ( field [ "help" ] , "    ///   " ) yield "    /// </summary>" yield "    public {type} {name} {{ get; set; }}" . format ( type = csharp type , name = field [ "name" ] ) yield "" yield "  }" yield "}"
def available ( ) : builder json = h2o . api ( "GET /3/Model Builders" , data = { "algo" : "deepwater" } ) visibility = builder json [ "model builders" ] [ "deepwater" ] [ "visibility" ] if visibility == "Experimental" : print ( "Cannot build a Deep Water model - no backend found." ) return False else : return True
def endpoint groups ( ) : groups = defaultdict ( list ) for e in endpoints ( ) : groups [ e [ "class name" ] ] . append ( e ) return groups
def update site forward ( apps , schema editor ) : Site = apps . get model ( "sites" , "Site" ) Site . objects . update or create ( id = settings . SITE ID , defaults = { "domain" : "{{cookiecutter.domain name}}" , "name" : "{{cookiecutter.project name}}" , } , )
def json data ( self , data = None ) : if data is None : data = { } data . update ( self . default data ) return json . dumps ( data )
def comment user ( self , user id , amount = None ) : if not self . check user ( user id , filter closed acc = True ) : return False self . logger . info ( "Going to comment user %s's feed:" % user id ) user id = self . convert to user id ( user id ) medias = self . get user medias ( user id , is comment = True ) if not medias : self . logger . info ( "None medias received: account is closed or medias have been filtered." ) return False return self . comment medias ( medias [ : amount ] )
def get credentials ( username = None ) : while not check secret ( ) : pass while True : try : with open ( SECRET FILE , "r" ) as f : lines = [ line . strip ( ) . split ( ":" , 2 ) for line in f . readlines ( ) ] except Value Error : msg = 'Problem with opening `{}`, will remove the file.' raise Exception ( msg . format ( SECRET FILE ) ) if username is not None : for login , password in lines : if login == username . strip ( ) : return login , password print ( "Which account do you want to use? (Type number)" ) for ind , ( login , password ) in enumerate ( lines ) : print ( "%d: %s" % ( ind + 1 , login ) ) print ( "%d: %s" % ( 0 , "add another account." ) ) print ( "%d: %s" % ( - 1 , "delete all accounts." ) ) try : ind = int ( sys . stdin . readline ( ) ) if ind == 0 : add credentials ( ) continue elif ind == - 1 : delete credentials ( ) check secret ( ) continue elif 0 <= ind - 1 < len ( lines ) : return lines [ ind - 1 ] except Exception : print ( "Wrong input, enter the number of the account to use." )
def like user ( self , user id , amount = None , filtration = True ) : if filtration : if not self . check user ( user id ) : return False self . logger . info ( "Liking user %s's feed:" % user id ) user id = self . convert to user id ( user id ) medias = self . get user medias ( user id , filtration = filtration ) if not medias : self . logger . info ( "None medias received: account is closed or medias have been filtered." ) return False return self . like medias ( medias [ : amount ] )
def like hashtag ( self , hashtag , amount = None ) : self . logger . info ( "Going to like media with hashtag #%s." % hashtag ) medias = self . get total hashtag medias ( hashtag , amount ) return self . like medias ( medias )
def check not bot ( self , user id ) : self . small delay ( ) user id = self . convert to user id ( user id ) if not user id : return False if user id in self . whitelist : return True if user id in self . blacklist : return False user info = self . get user info ( user id ) if not user info : return True # closed acc skipped = self . skipped file if "following count" in user info and user info [ "following count" ] > self . max following to block : msg = 'following count > bot.max following to block, skipping!' self . console print ( msg , 'red' ) skipped . append ( user id ) return False # massfollower if search stop words in user ( self , user info ) : msg = '`bot.search stop words in user` found in user, skipping!' skipped . append ( user id ) return False return True
def get tweets ( user , pages = 25 ) : url = f'https://twitter.com/i/profiles/show/{user}/timeline/tweets?include available features=1&include entities=1&include new items bar=true' headers = { 'Accept' : 'application/json, text/javascript, */*; q=0.01' , 'Referer' : f'https://twitter.com/{user}' , 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10 12 6) Apple Web Kit/603.3.8 (KHTML, like Gecko) Version/10.1.2 Safari/603.3.8' , 'X-Twitter-Active-User' : 'yes' , 'X-Requested-With' : 'XML Http Request' , 'Accept-Language' : 'en-US' } def gen tweets ( pages ) : r = session . get ( url , headers = headers ) while pages > 0 : try : html = HTML ( html = r . json ( ) [ 'items html' ] , url = 'bunk' , default encoding = 'utf-8' ) except Key Error : raise Value Error ( f'Oops! Either "{user}" does not exist or is private.' ) comma = "," dot = "." tweets = [ ] for tweet in html . find ( 'html > .stream-item' ) : # 10~11 html elements have `.stream-item` class and also their `data-item-type` is `tweet` # but their content doesn't look like a tweet's content try : text = tweet . find ( '.tweet-text' ) [ 0 ] . full text except Index Error : # issue #50 continue tweet id = tweet . find ( '.js-permalink' ) [ 0 ] . attrs [ 'data-conversation-id' ] time = datetime . fromtimestamp ( int ( tweet . find ( '. timestamp' ) [ 0 ] . attrs [ 'data-time-ms' ] ) / 1000.0 ) interactions = [ x . text for x in tweet . find ( '.Profile Tweet-action Count' ) ] replies = int ( interactions [ 0 ] . split ( ' ' ) [ 0 ] . replace ( comma , '' ) . replace ( dot , '' ) or interactions [ 3 ] ) retweets = int ( interactions [ 1 ] . split ( ' ' ) [ 0 ] . replace ( comma , '' ) . replace ( dot , '' ) or interactions [ 4 ] or interactions [ 5 ] ) likes = int ( interactions [ 2 ] . split ( ' ' ) [ 0 ] . replace ( comma , '' ) . replace ( dot , '' ) or interactions [ 6 ] or interactions [ 7 ] ) hashtags = [ hashtag node . full text for hashtag node in tweet . find ( '.twitter-hashtag' ) ] urls = [ url node . attrs [ 'data-expanded-url' ] for url node in tweet . find ( 'a.twitter-timeline-link:not(.u-hidden)' ) ] photos = [ photo node . attrs [ 'data-image-url' ] for photo node in tweet . find ( '.Adaptive Media-photo Container' ) ] videos = [ ] video nodes = tweet . find ( ".Playable Media-player" ) for node in video nodes : styles = node . attrs [ 'style' ] . split ( ) for style in styles : if style . startswith ( 'background' ) : tmp = style . split ( '/' ) [ - 1 ] video id = tmp [ : tmp . index ( '.jpg' ) ] videos . append ( { 'id' : video id } ) tweets . append ( { 'tweet Id' : tweet id , 'time' : time , 'text' : text , 'replies' : replies , 'retweets' : retweets , 'likes' : likes , 'entries' : { 'hashtags' : hashtags , 'urls' : urls , 'photos' : photos , 'videos' : videos } } ) last tweet = html . find ( '.stream-item' ) [ - 1 ] . attrs [ 'data-item-id' ] for tweet in tweets : if tweet : tweet [ 'text' ] = re . sub ( 'http' , ' http' , tweet [ 'text' ] , 1 ) yield tweet r = session . get ( url , params = { 'max position' : last tweet } , headers = headers ) pages += - 1 yield from gen tweets ( pages )
def get uri ( self , request ) : protocol = request . protocol override if request . protocol override else self . protocol protocol = protocol . lower ( ) port = HTTP PORT if protocol == 'http' else HTTPS PORT return protocol + '://' + request . host + ':' + str ( port ) + request . path
def get connection ( self , request ) : protocol = request . protocol override if request . protocol override else self . protocol protocol = protocol . lower ( ) target host = request . host # target port = HTTP PORT if protocol == 'http' else HTTPS PORT connection = Requests Connection ( target host , protocol , self . request session , self . timeout ) proxy host = self . proxy host proxy port = self . proxy port if self . proxy host : headers = None if self . proxy user and self . proxy password : auth = base64 . b64encode ( "{0}:{1}" . format ( self . proxy user , self . proxy password ) . encode ( ) ) headers = { 'Proxy-Authorization' : 'Basic {0}' . format ( auth . decode ( ) ) } connection . set tunnel ( proxy host , int ( proxy port ) , headers ) return connection
def perform request ( self , request ) : connection = self . get connection ( request ) try : connection . putrequest ( request . method , request . path ) self . send request headers ( connection , request . headers ) self . send request body ( connection , request . body ) if DEBUG REQUESTS and request . body : print ( 'request:' ) try : print ( request . body ) except : # pylint: disable=bare-except pass resp = connection . getresponse ( ) status = int ( resp . status ) message = resp . reason respheaders = resp . getheaders ( ) # for consistency across platforms, make header names lowercase for i , value in enumerate ( respheaders ) : respheaders [ i ] = ( value [ 0 ] . lower ( ) , value [ 1 ] ) respbody = None if resp . length is None : respbody = resp . read ( ) elif resp . length > 0 : respbody = resp . read ( resp . length ) if DEBUG RESPONSES and respbody : print ( 'response:' ) try : print ( respbody ) except : # pylint: disable=bare-except pass response = HTTP Response ( status , resp . reason , respheaders , respbody ) if status == 307 : new url = urlparse ( dict ( respheaders ) [ 'location' ] ) request . host = new url . hostname request . path = new url . path request . path , request . query = self . update request uri query ( request ) return self . perform request ( request ) if status >= 300 : raise HTTP Error ( status , message , respheaders , respbody ) return response finally : connection . close ( )
def get authorization server ( self ) : value = '' for key in [ 'authorization uri' , 'authorization' ] : value = self . get value ( key ) or '' if value : break return value
def validate request uri ( self , uri ) : if not uri : raise Value Error ( 'request uri cannot be empty' ) uri = parse . urlparse ( uri ) if not uri . netloc : raise Value Error ( 'request uri must be an absolute URI' ) if uri . scheme . lower ( ) not in [ 'http' , 'https' ] : raise Value Error ( 'request uri must be HTTP or HTTPS' ) return uri . netloc
def set timeout ( self , timeout in seconds ) : timeout in ms = int ( timeout in seconds * 1000 ) Win Http Request . Set Timeouts ( self , 0 , timeout in ms , timeout in ms , timeout in ms )
def set request header ( self , name , value ) : name = BSTR ( name ) value = BSTR ( value ) Win Http Request . Set Request Header ( self , name , value )
def get all response headers ( self ) : bstr headers = c void p ( ) Win Http Request . Get All Response Headers ( self , byref ( bstr headers ) ) bstr headers = ctypes . cast ( bstr headers , c wchar p ) headers = bstr headers . value Sys Free String ( bstr headers ) return headers
def send ( self , request = None ) : # Sends VT EMPTY if it is GET, HEAD request. if request is None : var empty = VARIANT . create empty ( ) Win Http Request . Send ( self , var empty ) else : # Sends request body as SAFE Array. request = VARIANT . create safearray from str ( request ) Win Http Request . Send ( self , request )
def status ( self ) : status = c long ( ) Win Http Request . Status ( self , byref ( status ) ) return int ( status . value )
def status text ( self ) : bstr status text = c void p ( ) Win Http Request . Status Text ( self , byref ( bstr status text ) ) bstr status text = ctypes . cast ( bstr status text , c wchar p ) status text = bstr status text . value Sys Free String ( bstr status text ) return status text
def response body ( self ) : var respbody = VARIANT ( ) Win Http Request . Response Body ( self , byref ( var respbody ) ) if var respbody . is safearray of bytes ( ) : respbody = var respbody . str from safearray ( ) return respbody else : return ''
def set client certificate ( self , certificate ) : certificate = BSTR ( certificate ) Win Http Request . Set Client Certificate ( self , certificate )
def set tunnel ( self , host , port ) : url = host if port : url = url + u':' + port var host = VARIANT . create bstr from str ( url ) var empty = VARIANT . create empty ( ) Win Http Request . Set Proxy ( self , HTTPREQUEST PROXYSETTING PROXY , var host , var empty )
def set tunnel ( self , host , port = None , headers = None ) : self . httprequest . set tunnel ( unicode ( host ) , unicode ( str ( port ) ) )
def putrequest ( self , method , uri ) : protocol = unicode ( self . protocol + '://' ) url = protocol + self . host + unicode ( uri ) self . httprequest . set timeout ( self . timeout ) self . httprequest . open ( unicode ( method ) , url ) # sets certificate for the connection if cert file is set. if self . cert file is not None : self . httprequest . set client certificate ( unicode ( self . cert file ) )
def putheader ( self , name , value ) : if sys . version info < ( 3 , ) : name = str ( name ) . decode ( 'utf-8' ) value = str ( value ) . decode ( 'utf-8' ) self . httprequest . set request header ( name , value )
def send ( self , request body ) : if not request body : self . httprequest . send ( ) else : self . httprequest . send ( request body )
def getresponse ( self ) : status = self . httprequest . status ( ) status text = self . httprequest . status text ( ) resp headers = self . httprequest . get all response headers ( ) fixed headers = [ ] for resp header in resp headers . split ( '\n' ) : if ( resp header . startswith ( '\t' ) or resp header . startswith ( ' ' ) ) and fixed headers : # append to previous header fixed headers [ - 1 ] += resp header else : fixed headers . append ( resp header ) headers = [ ] for resp header in fixed headers : if ':' in resp header : pos = resp header . find ( ':' ) headers . append ( ( resp header [ : pos ] . lower ( ) , resp header [ pos + 1 : ] . strip ( ) ) ) body = self . httprequest . response body ( ) length = len ( body ) return Response ( status , status text , length , headers , body )
def get readable id ( id name , id prefix to skip ) : # id name is in the form 'https://namespace.host.suffix/name' # where name may contain a forward slash! pos = id name . find ( '//' ) if pos != - 1 : pos += 2 if id prefix to skip : pos = id name . find ( id prefix to skip , pos ) if pos != - 1 : pos += len ( id prefix to skip ) pos = id name . find ( '/' , pos ) if pos != - 1 : return id name [ pos + 1 : ] return id name
def get serialization name ( element name ) : known = KNOWN SERIALIZATION XFORMS . get ( element name ) if known is not None : return known if element name . startswith ( 'x ms ' ) : return element name . replace ( ' ' , '-' ) if element name . endswith ( ' id' ) : element name = element name . replace ( ' id' , 'ID' ) for name in [ 'content ' , 'last modified' , 'if ' , 'cache control' ] : if element name . startswith ( name ) : element name = element name . replace ( ' ' , '- ' ) return '' . join ( name . capitalize ( ) for name in element name . split ( ' ' ) )
def get entry properties from node ( entry , include id , id prefix to skip = None , use title as id = False ) : properties = { } etag = entry . get Attribute NS ( METADATA NS , 'etag' ) if etag : properties [ 'etag' ] = etag for updated in Minidom Xml To Object . get child nodes ( entry , 'updated' ) : properties [ 'updated' ] = updated . first Child . node Value for name in Minidom Xml To Object . get children from path ( entry , 'author' , 'name' ) : if name . first Child is not None : properties [ 'author' ] = name . first Child . node Value if include id : if use title as id : for title in Minidom Xml To Object . get child nodes ( entry , 'title' ) : properties [ 'name' ] = title . first Child . node Value else : # TODO: check if this is used for id in Minidom Xml To Object . get child nodes ( entry , 'id' ) : properties [ 'name' ] = get readable id ( id . first Child . node Value , id prefix to skip ) return properties
def parse response body from xml node ( node , return type ) : return obj = return type ( ) Minidom Xml To Object . fill data to return object ( node , return obj ) return return obj
def fill instance child ( xmldoc , element name , return type ) : xmlelements = Minidom Xml To Object . get child nodes ( xmldoc , get serialization name ( element name ) ) if not xmlelements : return None return obj = return type ( ) Minidom Xml To Object . fill data to return object ( xmlelements [ 0 ] , return obj ) return return obj
def build package from pr number ( gh token , sdk id , pr number , output folder , * , with comment = False ) : con = Github ( gh token ) repo = con . get repo ( sdk id ) sdk pr = repo . get pull ( pr number ) # "get files" of Github only download the first 300 files. Might not be enough. package names = { f . filename . split ( '/' ) [ 0 ] for f in sdk pr . get files ( ) if f . filename . startswith ( "azure" ) } absolute output folder = Path ( output folder ) . resolve ( ) with tempfile . Temporary Directory ( ) as temp dir , manage git folder ( gh token , Path ( temp dir ) / Path ( "sdk" ) , sdk id , pr number = pr number ) as sdk folder : for package name in package names : LOGGER . debug ( "Build {}" . format ( package name ) ) execute simple command ( [ "python" , "./build package.py" , "--dest" , str ( absolute output folder ) , package name ] , cwd = sdk folder ) LOGGER . debug ( "Build finished: {}" . format ( package name ) ) if with comment : files = [ f . name for f in absolute output folder . iterdir ( ) ] comment message = None dashboard = Dashboard Commentable Object ( sdk pr , "(message created by the CI based on PR content)" ) try : installation message = build installation message ( sdk pr ) download message = build download message ( sdk pr , files ) comment message = installation message + "\n\n" + download message dashboard . create comment ( comment message ) except Exception : LOGGER . critical ( "Unable to do PR comment:\n%s" , comment message )
def extract api version from code ( function ) : try : srccode = inspect . getsource ( function ) try : ast tree = ast . parse ( srccode ) except Indentation Error : ast tree = ast . parse ( 'with 0:\n' + srccode ) api version visitor = Api Version Extractor ( ) api version visitor . visit ( ast tree ) return api version visitor . api version except Exception : raise
def build receiver ( self ) : # pylint: disable=protected-access self . handler . message handler = self . handler . receiver type ( self . handler . session , self . handler . remote address , self . handler . name , on message received = self . handler . message received , name = 'receiver-link-{}' . format ( uuid . uuid4 ( ) ) , debug = self . handler . debug trace , prefetch = self . handler . prefetch , max message size = self . handler . max message size , properties = self . handler . link properties , error policy = self . handler . error policy , encoding = self . handler . encoding ) if self . mode != Receive Settle Mode . Peek Lock : self . handler . message handler . send settle mode = constants . Sender Settle Mode . Settled self . handler . message handler . receive settle mode = constants . Receiver Settle Mode . Receive And Delete self . handler . message handler . settle mode = constants . Receiver Settle Mode . Receive And Delete self . handler . message handler . open ( )
def parse response for async op ( response ) : if response is None : return None result = Asynchronous Operation Result ( ) if response . headers : for name , value in response . headers : if name . lower ( ) == 'x-ms-request-id' : result . request id = value return result
def update management header ( self , request , x ms version ) : if request . method in [ 'PUT' , 'POST' , 'MERGE' , 'DELETE' ] : request . headers . append ( ( 'Content-Length' , str ( len ( request . body ) ) ) ) # append additional headers base on the service request . headers . append ( ( 'x-ms-version' , x ms version or self . x ms version ) ) # if it is not GET or HEAD request, must set content-type. if not request . method in [ 'GET' , 'HEAD' ] : for name , in request . headers : if 'content-type' == name . lower ( ) : break else : request . headers . append ( ( 'Content-Type' , self . content type ) ) return request . headers
def get regions ( self ) : response = self . perform get ( self . get path ( 'services/service Bus/Regions/' , None ) , None ) return Minidom Xml To Object . convert response to feeds ( response , Service Bus Management Xml Serializer . xml to region )
def list namespaces ( self ) : response = self . perform get ( self . get path ( 'services/service Bus/Namespaces/' , None ) , None ) return Minidom Xml To Object . convert response to feeds ( response , Service Bus Management Xml Serializer . xml to namespace )
def create ( env dir , system site packages = False , clear = False , symlinks = False , with pip = False , prompt = None ) : builder = Extended Env Builder ( system site packages = system site packages , clear = clear , symlinks = symlinks , with pip = with pip , prompt = prompt ) builder . create ( env dir ) return builder . context
def list databases ( self , name ) : response = self . perform get ( self . get list databases path ( name ) , None ) return Minidom Xml To Object . parse service resources response ( response , Database )
def validate challenge ( self , challenge ) : bearer string = 'Bearer ' if not challenge : raise Value Error ( 'Challenge cannot be empty' ) challenge = challenge . strip ( ) if not challenge . startswith ( bearer string ) : raise Value Error ( 'Challenge is not Bearer' ) return challenge [ len ( bearer string ) : ]
def list queues ( self ) : request = HTTP Request ( ) request . method = 'GET' request . host = self . get host ( ) request . path = '/$Resources/Queues' request . path , request . query = self . httpclient . update request uri query ( request ) # pylint: disable=protected-access request . headers = self . update service bus header ( request ) response = self . perform request ( request ) return E Tree Xml To Object . convert response to feeds ( response , convert etree element to queue )
def send event ( self , hub name , message , device id = None , broker properties = None ) : validate not none ( 'hub name' , hub name ) request = HTTP Request ( ) request . method = 'POST' request . host = self . get host ( ) if device id : request . path = '/{0}/publishers/{1}/messages?api-version=2014-01' . format ( hub name , device id ) else : request . path = '/{0}/messages?api-version=2014-01' . format ( hub name ) if broker properties : request . headers . append ( ( 'Broker Properties' , str ( broker properties ) ) ) request . body = get request body ( message ) request . path , request . query = self . httpclient . update request uri query ( request ) # pylint: disable=protected-access request . headers = self . update service bus header ( request ) self . perform request ( request )
def update service bus header ( self , request ) : if request . method in [ 'PUT' , 'POST' , 'MERGE' , 'DELETE' ] : request . headers . append ( ( 'Content-Length' , str ( len ( request . body ) ) ) ) # if it is not GET or HEAD request, must set content-type. if not request . method in [ 'GET' , 'HEAD' ] : for name , in request . headers : if name . lower ( ) == 'content-type' : break else : request . headers . append ( ( 'Content-Type' , 'application/atom+xml;type=entry;charset=utf-8' ) ) # Adds authorization header for authentication. self . authentication . sign request ( request , self . httpclient ) return request . headers
def get authorization ( self , request , httpclient ) : return 'WRAP access token="' + self . get token ( request . host , request . path , httpclient ) + '"'
def token is expired ( self , token ) : # pylint: disable=no-self-use time pos begin = token . find ( 'Expires On=' ) + len ( 'Expires On=' ) time pos end = token . find ( '&' , time pos begin ) token expire time = int ( token [ time pos begin : time pos end ] ) time now = time . mktime ( time . localtime ( ) ) # Adding 30 seconds so the token wouldn't be expired when we send the # token to server. return ( token expire time - time now ) < 30
def add headers ( self , request ) : # Adds custom properties if self . custom properties : for name , value in self . custom properties . items ( ) : request . headers . append ( ( name , self . serialize escaped properties value ( value ) ) ) # Adds content-type request . headers . append ( ( 'Content-Type' , self . type ) ) # Adds Broker Properties if self . broker properties : if hasattr ( self . broker properties , 'items' ) : broker properties = { name : self . serialize basic properties value ( value ) for name , value in self . broker properties . items ( ) } broker properties = json . dumps ( broker properties ) else : broker properties = self . broker properties request . headers . append ( ( 'Broker Properties' , str ( broker properties ) ) ) return request . headers
def as batch body ( self ) : if sys . version info >= ( 3 , ) and isinstance ( self . body , bytes ) : # It HAS to be string to be serialized in JSON body = self . body . decode ( 'utf-8' ) else : # Python 2.7 people handle this themself body = self . body result = { 'Body' : body } # Adds custom properties if self . custom properties : result [ 'User Properties' ] = { name : self . serialize basic properties value ( value ) for name , value in self . custom properties . items ( ) } # Adds Broker Properties if self . broker properties : result [ 'Broker Properties' ] = { name : self . serialize basic properties value ( value ) for name , value in self . broker properties . items ( ) } return result
def general error handler ( http error ) : message = str ( http error ) if http error . respbody is not None : message += '\n' + http error . respbody . decode ( 'utf-8-sig' ) raise Azure Http Error ( message , http error . status )
def handle redirect ( self , r , * * kwargs ) : if r . is redirect : self . thread local . auth attempted = False
def use ( self , profile ) : if not isinstance ( profile , ( Known Profiles , Profile Definition ) ) : raise Value Error ( "Can only set as default a Profile Definition or a Known Profiles" ) type ( self ) . profile = profile
def build config ( config : Dict [ str , Any ] ) -> Dict [ str , str ] : result = config . copy ( ) # Manage the classifier stable/beta is stable = result . pop ( "is stable" , False ) if is stable : result [ "classifier" ] = "Development Status :: 5 - Production/Stable" else : result [ "classifier" ] = "Development Status :: 4 - Beta" # Manage the nspkg package name = result [ "package name" ] result [ "package nspkg" ] = result . pop ( "package nspkg" , package name [ : package name . rindex ( '-' ) ] + "-nspkg" ) # ARM? result [ 'is arm' ] = result . pop ( "is arm" , True ) # Do I need msrestazure for this package? result [ 'need msrestazure' ] = result . pop ( "need msrestazure" , True ) # Pre-compute some Jinja variable that are complicated to do inside the templates package parts = result [ "package nspkg" ] [ : - len ( '-nspkg' ) ] . split ( '-' ) result [ 'nspkg names' ] = [ "." . join ( package parts [ : i + 1 ] ) for i in range ( len ( package parts ) ) ] result [ 'init names' ] = [ "/" . join ( package parts [ : i + 1 ] ) + "/ init .py" for i in range ( len ( package parts ) ) ] # Return result return result
def get entry properties from element ( element , include id , id prefix to skip = None , use title as id = False ) : properties = { } etag = element . attrib . get ( make etree ns attr name ( etree entity feed namespaces [ 'm' ] , 'etag' ) , None ) if etag is not None : properties [ 'etag' ] = etag updated = element . findtext ( './atom:updated' , '' , etree entity feed namespaces ) if updated : properties [ 'updated' ] = updated author name = element . findtext ( './atom:author/atom:name' , '' , etree entity feed namespaces ) if author name : properties [ 'author' ] = author name if include id : if use title as id : title = element . findtext ( './atom:title' , '' , etree entity feed namespaces ) if title : properties [ 'name' ] = title else : element id = element . findtext ( './atom:id' , '' , etree entity feed namespaces ) if element id : properties [ 'name' ] = get readable id ( element id , id prefix to skip ) return properties
def parse response body from xml node ( node , return type ) : return obj = return type ( ) E Tree Xml To Object . fill data to return object ( node , return obj ) return return obj
def fill instance child ( xmldoc , element name , return type ) : element = xmldoc . find ( get serialization name ( element name ) ) if element is None : return None return obj = return type ( ) E Tree Xml To Object . fill data to return object ( element , return obj ) return return obj
def terminal width ( value ) : if isinstance ( value , bytes ) : value = value . decode ( "utf8" , "ignore" ) return sum ( map ( get width , map ( ord , value ) ) )
def get cut prefix ( value , max len ) : should convert = isinstance ( value , bytes ) if should convert : value = value . decode ( "utf8" , "ignore" ) for i in range ( len ( value ) ) : if terminal width ( value [ i : ] ) <= max len : break return value [ i : ] . encode ( "utf8" , "ignore" ) if should convert else value [ i : ]
def print inplace ( msg ) : term width = get terminal size ( ) . columns spacing = term width - terminal width ( msg ) # On windows we need one less space or we overflow the line for some reason. if is win32 : spacing -= 1 sys . stderr . write ( "\r{0}" . format ( msg ) ) sys . stderr . write ( " " * max ( 0 , spacing ) ) sys . stderr . flush ( )
def format filesize ( size ) : for suffix in ( "bytes" , "KB" , "MB" , "GB" , "TB" ) : if size < 1024.0 : if suffix in ( "GB" , "TB" ) : return "{0:3.2f} {1}" . format ( size , suffix ) else : return "{0:3.1f} {1}" . format ( size , suffix ) size /= 1024.0
def format time ( elapsed ) : hours = int ( elapsed / ( 60 * 60 ) ) minutes = int ( ( elapsed % ( 60 * 60 ) ) / 60 ) seconds = int ( elapsed % 60 ) rval = "" if hours : rval += "{0}h" . format ( hours ) if elapsed > 60 : rval += "{0}m" . format ( minutes ) rval += "{0}s" . format ( seconds ) return rval
def create status line ( * * params ) : max size = get terminal size ( ) . columns - 1 for fmt in PROGRESS FORMATS : status = fmt . format ( * * params ) if len ( status ) <= max size : break return status
def close ( self ) : if not self . closed : log . debug ( "Closing worker thread" ) self . closed = True if self . wait : self . wait . set ( )
def close ( self ) : if not self . closed : log . debug ( "Closing writer thread" ) self . closed = True self . reader . buffer . close ( ) self . executor . shutdown ( wait = False ) if concurrent . futures . thread . threads queues : concurrent . futures . thread . threads queues . clear ( )
def put ( self , segment ) : if self . closed : return if segment is not None : future = self . executor . submit ( self . fetch , segment , retries = self . retries ) else : future = None self . queue ( self . futures , ( segment , future ) )
def queue ( self , queue , value ) : while not self . closed : try : queue . put ( value , block = True , timeout = 1 ) return except queue . Full : continue
def pkcs7 decode ( padded Data , key Size = 16 ) : # Use ord + [-1:] to support both python 2 and 3 val = ord ( padded Data [ - 1 : ] ) if val > key Size : raise Stream Error ( "Input is not padded or padding is corrupt, got padding size of {0}" . format ( val ) ) return padded Data [ : - val ]
def prepend www ( url ) : parsed = urlparse ( url ) if parsed . netloc . split ( "." ) [ 0 ] != "www" : return parsed . scheme + "://www." + parsed . netloc + parsed . path else : return url
def json ( cls , res , * args , * * kwargs ) : # if an encoding is already set then use the provided encoding if res . encoding is None : res . encoding = cls . determine json encoding ( res . content [ : 4 ] ) return parse json ( res . text , * args , * * kwargs )
def xml ( cls , res , * args , * * kwargs ) : return parse xml ( res . text , * args , * * kwargs )
def get streams ( self ) : token = self . login ( self . get option ( "username" ) , self . get option ( "password" ) ) m = self . url re . match ( self . url ) scode = m and m . group ( "scode" ) or self . get option ( "station code" ) res = self . session . http . get ( self . guide url , params = dict ( token = token ) ) channels = Ordered Dict ( ) for t in itertags ( res . text , "a" ) : if t . attributes . get ( 'cs' ) : channels [ t . attributes . get ( 'cs' ) . lower ( ) ] = t . attributes . get ( 'title' ) . replace ( "Watch " , "" ) . strip ( ) if not scode : log . error ( "Station code not provided, use --ustvnow-station-code." ) log . info ( "Available stations are: \n{0} " . format ( '\n' . join ( '    {0} ({1})' . format ( c , n ) for c , n in channels . items ( ) ) ) ) return if scode in channels : log . debug ( "Finding streams for: {0}" , channels . get ( scode ) ) r = self . session . http . get ( self . stream url , params = { "scode" : scode , "token" : token , "br n" : "Firefox" , "br v" : "52" , "br d" : "desktop" } , headers = { "User-Agent" : useragents . FIREFOX } ) data = self . session . http . json ( r ) return HLS Stream . parse variant playlist ( self . session , data [ "stream" ] ) else : log . error ( "Invalid station-code: {0}" , scode )
def login ( self ) : email = self . get option ( "email" ) password = self . get option ( "password" ) if email and password : res = self . session . http . get ( self . login url ) csrf match = self . csrf re . search ( res . text ) token = csrf match and csrf match . group ( 1 ) self . logger . debug ( "Attempting login as {0} (token={1})" , email , token ) res = self . session . http . post ( self . login url , data = dict ( login = email , password = password , csrfmiddlewaretoken = token ) , allow redirects = False , raise for status = False , headers = { "Referer" : self . login url } ) if res . status code != 302 : self . logger . error ( "Failed to login to Live Edu account: {0}" , email )
def output stream http ( plugin , initial streams , external = False , port = 0 ) : global output if not external : if not args . player : console . exit ( "The default player (VLC) does not seem to be " "installed. You must specify the path to a player " "executable with --player." ) title = create title ( plugin ) server = create http server ( ) player = output = Player Output ( args . player , args = args . player args , filename = server . url , quiet = not args . verbose player , title = title ) try : log . info ( "Starting player: {0}" , args . player ) if player : player . open ( ) except OS Error as err : console . exit ( "Failed to start player: {0} ({1})" , args . player , err ) else : server = create http server ( host = None , port = port ) player = None log . info ( "Starting server, access with one of:" ) for url in server . urls : log . info ( " " + url ) for req in iter http requests ( server , player ) : user agent = req . headers . get ( "User-Agent" ) or "unknown player" log . info ( "Got HTTP request from {0}" . format ( user agent ) ) stream fd = prebuffer = None while not stream fd and ( not player or player . running ) : try : streams = initial streams or fetch streams ( plugin ) initial streams = None for stream name in ( resolve stream name ( streams , s ) for s in args . stream ) : if stream name in streams : stream = streams [ stream name ] break else : log . info ( "Stream not available, will re-fetch " "streams in 10 sec" ) sleep ( 10 ) continue except Plugin Error as err : log . error ( u"Unable to fetch new streams: {0}" , err ) continue try : log . info ( "Opening stream: {0} ({1})" , stream name , type ( stream ) . shortname ( ) ) stream fd , prebuffer = open stream ( stream ) except Stream Error as err : log . error ( "{0}" , err ) if stream fd and prebuffer : log . debug ( "Writing stream to player" ) read stream ( stream fd , server , prebuffer ) server . close ( True ) player . close ( ) server . close ( )
def output stream passthrough ( plugin , stream ) : global output title = create title ( plugin ) filename = '"{0}"' . format ( stream to url ( stream ) ) output = Player Output ( args . player , args = args . player args , filename = filename , call = True , quiet = not args . verbose player , title = title ) try : log . info ( "Starting player: {0}" , args . player ) output . open ( ) except OS Error as err : console . exit ( "Failed to start player: {0} ({1})" , args . player , err ) return False return True
def output stream ( plugin , stream ) : global output success open = False for i in range ( args . retry open ) : try : stream fd , prebuffer = open stream ( stream ) success open = True break except Stream Error as err : log . error ( "Try {0}/{1}: Could not open stream {2} ({3})" , i + 1 , args . retry open , stream , err ) if not success open : console . exit ( "Could not open stream {0}, tried {1} times, exiting" , stream , args . retry open ) output = create output ( plugin ) try : output . open ( ) except ( IO Error , OS Error ) as err : if isinstance ( output , Player Output ) : console . exit ( "Failed to start player: {0} ({1})" , args . player , err ) else : console . exit ( "Failed to open output: {0} ({1})" , args . output , err ) with closing ( output ) : log . debug ( "Writing stream to output" ) read stream ( stream fd , output , prebuffer ) return True
def read stream ( stream , output , prebuffer , chunk size = 8192 ) : is player = isinstance ( output , Player Output ) is http = isinstance ( output , HTTP Server ) is fifo = is player and output . namedpipe show progress = isinstance ( output , File Output ) and output . fd is not stdout and sys . stdout . isatty ( ) show record progress = hasattr ( output , "record" ) and isinstance ( output . record , File Output ) and output . record . fd is not stdout and sys . stdout . isatty ( ) stream iterator = chain ( [ prebuffer ] , iter ( partial ( stream . read , chunk size ) , b"" ) ) if show progress : stream iterator = progress ( stream iterator , prefix = os . path . basename ( args . output ) ) elif show record progress : stream iterator = progress ( stream iterator , prefix = os . path . basename ( args . record ) ) try : for data in stream iterator : # We need to check if the player process still exists when # using named pipes on Windows since the named pipe is not # automatically closed by the player. if is win32 and is fifo : output . player . poll ( ) if output . player . returncode is not None : log . info ( "Player closed" ) break try : output . write ( data ) except IO Error as err : if is player and err . errno in ACCEPTABLE ERRNO : log . info ( "Player closed" ) elif is http and err . errno in ACCEPTABLE ERRNO : log . info ( "HTTP connection closed" ) else : console . exit ( "Error when writing to output: {0}, exiting" , err ) break except IO Error as err : console . exit ( "Error when reading from stream: {0}, exiting" , err ) finally : stream . close ( ) log . info ( "Stream ended" )
def fetch streams ( plugin ) : return plugin . streams ( stream types = args . stream types , sorting excludes = args . stream sorting excludes )
def resolve stream name ( streams , stream name ) : if stream name in STREAM SYNONYMS and stream name in streams : for name , stream in streams . items ( ) : if stream is streams [ stream name ] and name not in STREAM SYNONYMS : return name return stream name
def print plugins ( ) : pluginlist = list ( streamlink . get plugins ( ) . keys ( ) ) pluginlist formatted = ", " . join ( sorted ( pluginlist ) ) if console . json : console . msg json ( pluginlist ) else : console . msg ( "Loaded plugins: {0}" , pluginlist formatted )
def load plugins ( dirs ) : dirs = [ os . path . expanduser ( d ) for d in dirs ] for directory in dirs : if os . path . isdir ( directory ) : streamlink . load plugins ( directory ) else : log . warning ( "Plugin path {0} does not exist or is not " "a directory!" , directory )
def setup http session ( ) : if args . http proxy : streamlink . set option ( "http-proxy" , args . http proxy ) if args . https proxy : streamlink . set option ( "https-proxy" , args . https proxy ) if args . http cookie : streamlink . set option ( "http-cookies" , dict ( args . http cookie ) ) if args . http header : streamlink . set option ( "http-headers" , dict ( args . http header ) ) if args . http query param : streamlink . set option ( "http-query-params" , dict ( args . http query param ) ) if args . http ignore env : streamlink . set option ( "http-trust-env" , False ) if args . http no ssl verify : streamlink . set option ( "http-ssl-verify" , False ) if args . http disable dh : streamlink . set option ( "http-disable-dh" , True ) if args . http ssl cert : streamlink . set option ( "http-ssl-cert" , args . http ssl cert ) if args . http ssl cert crt key : streamlink . set option ( "http-ssl-cert" , tuple ( args . http ssl cert crt key ) ) if args . http timeout : streamlink . set option ( "http-timeout" , args . http timeout ) if args . http cookies : streamlink . set option ( "http-cookies" , args . http cookies ) if args . http headers : streamlink . set option ( "http-headers" , args . http headers ) if args . http query params : streamlink . set option ( "http-query-params" , args . http query params )
def setup plugins ( extra plugin dir = None ) : if os . path . isdir ( PLUGINS DIR ) : load plugins ( [ PLUGINS DIR ] ) if extra plugin dir : load plugins ( extra plugin dir )
def setup options ( ) : if args . hls live edge : streamlink . set option ( "hls-live-edge" , args . hls live edge ) if args . hls segment attempts : streamlink . set option ( "hls-segment-attempts" , args . hls segment attempts ) if args . hls playlist reload attempts : streamlink . set option ( "hls-playlist-reload-attempts" , args . hls playlist reload attempts ) if args . hls segment threads : streamlink . set option ( "hls-segment-threads" , args . hls segment threads ) if args . hls segment timeout : streamlink . set option ( "hls-segment-timeout" , args . hls segment timeout ) if args . hls segment ignore names : streamlink . set option ( "hls-segment-ignore-names" , args . hls segment ignore names ) if args . hls segment key uri : streamlink . set option ( "hls-segment-key-uri" , args . hls segment key uri ) if args . hls timeout : streamlink . set option ( "hls-timeout" , args . hls timeout ) if args . hls audio select : streamlink . set option ( "hls-audio-select" , args . hls audio select ) if args . hls start offset : streamlink . set option ( "hls-start-offset" , args . hls start offset ) if args . hls duration : streamlink . set option ( "hls-duration" , args . hls duration ) if args . hls live restart : streamlink . set option ( "hls-live-restart" , args . hls live restart ) if args . hds live edge : streamlink . set option ( "hds-live-edge" , args . hds live edge ) if args . hds segment attempts : streamlink . set option ( "hds-segment-attempts" , args . hds segment attempts ) if args . hds segment threads : streamlink . set option ( "hds-segment-threads" , args . hds segment threads ) if args . hds segment timeout : streamlink . set option ( "hds-segment-timeout" , args . hds segment timeout ) if args . hds timeout : streamlink . set option ( "hds-timeout" , args . hds timeout ) if args . http stream timeout : streamlink . set option ( "http-stream-timeout" , args . http stream timeout ) if args . ringbuffer size : streamlink . set option ( "ringbuffer-size" , args . ringbuffer size ) if args . rtmp proxy : streamlink . set option ( "rtmp-proxy" , args . rtmp proxy ) if args . rtmp rtmpdump : streamlink . set option ( "rtmp-rtmpdump" , args . rtmp rtmpdump ) if args . rtmp timeout : streamlink . set option ( "rtmp-timeout" , args . rtmp timeout ) if args . stream segment attempts : streamlink . set option ( "stream-segment-attempts" , args . stream segment attempts ) if args . stream segment threads : streamlink . set option ( "stream-segment-threads" , args . stream segment threads ) if args . stream segment timeout : streamlink . set option ( "stream-segment-timeout" , args . stream segment timeout ) if args . stream timeout : streamlink . set option ( "stream-timeout" , args . stream timeout ) if args . ffmpeg ffmpeg : streamlink . set option ( "ffmpeg-ffmpeg" , args . ffmpeg ffmpeg ) if args . ffmpeg verbose : streamlink . set option ( "ffmpeg-verbose" , args . ffmpeg verbose ) if args . ffmpeg verbose path : streamlink . set option ( "ffmpeg-verbose-path" , args . ffmpeg verbose path ) if args . ffmpeg video transcode : streamlink . set option ( "ffmpeg-video-transcode" , args . ffmpeg video transcode ) if args . ffmpeg audio transcode : streamlink . set option ( "ffmpeg-audio-transcode" , args . ffmpeg audio transcode ) streamlink . set option ( "subprocess-errorlog" , args . subprocess errorlog ) streamlink . set option ( "subprocess-errorlog-path" , args . subprocess errorlog path ) streamlink . set option ( "locale" , args . locale )
def setup plugin args ( session , parser ) : plugin args = parser . add argument group ( "Plugin options" ) for pname , plugin in session . plugins . items ( ) : defaults = { } for parg in plugin . arguments : plugin args . add argument ( parg . argument name ( pname ) , * * parg . options ) defaults [ parg . dest ] = parg . default plugin . options = Plugin Options ( defaults )
def setup plugin options ( session , plugin ) : pname = plugin . module required = Ordered Dict ( { } ) for parg in plugin . arguments : if parg . options . get ( "help" ) != argparse . SUPPRESS : if parg . required : required [ parg . name ] = parg value = getattr ( args , parg . namespace dest ( pname ) ) session . set plugin option ( pname , parg . dest , value ) # if the value is set, check to see if any of the required arguments are not set if parg . required or value : try : for rparg in plugin . arguments . requires ( parg . name ) : required [ rparg . name ] = rparg except Runtime Error : console . logger . error ( "{0} plugin has a configuration error and the arguments " "cannot be parsed" . format ( pname ) ) break if required : for req in required . values ( ) : if not session . get plugin option ( pname , req . dest ) : prompt = req . prompt or "Enter {0} {1}" . format ( pname , req . name ) session . set plugin option ( pname , req . dest , console . askpass ( prompt + ": " ) if req . sensitive else console . ask ( prompt + ": " ) )
def log current versions ( ) : if logger . root . is Enabled For ( logging . DEBUG ) : # MAC OS X if sys . platform == "darwin" : os version = "mac OS {0}" . format ( platform . mac ver ( ) [ 0 ] ) # Windows elif sys . platform . startswith ( "win" ) : os version = "{0} {1}" . format ( platform . system ( ) , platform . release ( ) ) # linux / other else : os version = platform . platform ( ) log . debug ( "OS:         {0}" . format ( os version ) ) log . debug ( "Python:     {0}" . format ( platform . python version ( ) ) ) log . debug ( "Streamlink: {0}" . format ( streamlink version ) ) log . debug ( "Requests({0}), Socks({1}), Websocket({2})" . format ( requests . version , socks version , websocket version ) )
def get stream id ( self , text ) : m = self . image re . search ( text ) if m : return m . group ( "stream id" )
def get iframe ( self , text ) : m = self . iframe re . search ( text ) if m : return self . session . streams ( m . group ( "url" ) )
def startswith ( string ) : def starts with ( value ) : validate ( text , value ) if not value . startswith ( string ) : raise Value Error ( "'{0}' does not start with '{1}'" . format ( value , string ) ) return True return starts with
def endswith ( string ) : def ends with ( value ) : validate ( text , value ) if not value . endswith ( string ) : raise Value Error ( "'{0}' does not end with '{1}'" . format ( value , string ) ) return True return ends with
def contains ( string ) : def contains str ( value ) : validate ( text , value ) if string not in value : raise Value Error ( "'{0}' does not contain '{1}'" . format ( value , string ) ) return True return contains str
def url ( * * attributes ) : def check url ( value ) : validate ( text , value ) parsed = urlparse ( value ) if not parsed . netloc : raise Value Error ( "'{0}' is not a valid URL" . format ( value ) ) for name , schema in attributes . items ( ) : if not hasattr ( parsed , name ) : raise Value Error ( "Invalid URL attribute '{0}'" . format ( name ) ) try : validate ( schema , getattr ( parsed , name ) ) except Value Error as err : raise Value Error ( "Unable to validate URL attribute '{0}': {1}" . format ( name , err ) ) return True # Convert "http" to be either any("http", "https") for convenience if attributes . get ( "scheme" ) == "http" : attributes [ "scheme" ] = any ( "http" , "https" ) return check url
def xml find ( xpath ) : def xpath find ( value ) : validate ( ET . iselement , value ) value = value . find ( xpath ) if value is None : raise Value Error ( "X Path '{0}' did not return an element" . format ( xpath ) ) return validate ( ET . iselement , value ) return transform ( xpath find )
def xml findall ( xpath ) : def xpath findall ( value ) : validate ( ET . iselement , value ) return value . findall ( xpath ) return transform ( xpath findall )
def dologin ( self , email , password , emailauth = "" , emailsteamid = "" , captchagid = "-1" , captcha text = "" , twofactorcode = "" ) : epassword , rsatimestamp = self . encrypt password ( email , password ) login data = { 'username' : email , "password" : epassword , "emailauth" : emailauth , "loginfriendlyname" : "Streamlink" , "captchagid" : captchagid , "captcha text" : captcha text , "emailsteamid" : emailsteamid , "rsatimestamp" : rsatimestamp , "remember login" : True , "donotcache" : self . donotcache , "twofactorcode" : twofactorcode } res = self . session . http . post ( self . dologin url , data = login data ) resp = self . session . http . json ( res , schema = self . dologin schema ) if not resp [ u"success" ] : if resp . get ( u"captcha needed" ) : # special case for captcha captchagid = resp [ u"captcha gid" ] log . error ( "Captcha result required, open this URL to see the captcha: {}" . format ( self . captcha url . format ( captchagid ) ) ) try : captcha text = self . input ask ( "Captcha text" ) except Fatal Plugin Error : captcha text = None if not captcha text : return False else : # If the user must enter the code that was emailed to them if resp . get ( u"emailauth needed" ) : if not emailauth : try : emailauth = self . input ask ( "Email auth code required" ) except Fatal Plugin Error : emailauth = None if not emailauth : return False else : raise Steam Login Failed ( "Email auth key error" ) # If the user must enter a two factor auth code if resp . get ( u"requires twofactor" ) : try : twofactorcode = self . input ask ( "Two factor auth code required" ) except Fatal Plugin Error : twofactorcode = None if not twofactorcode : return False if resp . get ( u"message" ) : raise Steam Login Failed ( resp [ u"message" ] ) return self . dologin ( email , password , emailauth = emailauth , emailsteamid = resp . get ( u"emailsteamid" , u"" ) , captcha text = captcha text , captchagid = captchagid , twofactorcode = twofactorcode ) elif resp . get ( "login complete" ) : return True else : log . error ( "Something when wrong when logging in to Steam" ) return False
def get stream id ( self , html ) : stream id = stream id pattern . search ( html ) if not stream id : self . logger . error ( "Failed to extract stream id." ) return stream id . group ( "stream id" )
def login ( self , username , password ) : self . logger . debug ( 'login ...' ) res = self . session . http . get ( self . login url ) input list = self . input re . findall ( res . text ) if not input list : raise Plugin Error ( 'Missing input data on login website.' ) data = { } for input data in input list : try : input name = self . name re . search ( input data ) . group ( 1 ) except Attribute Error : continue try : input value = self . value re . search ( input data ) . group ( 1 ) except Attribute Error : input value = '' data [ input name ] = input value login data = { 'ctl00$Login1$User Name' : username , 'ctl00$Login1$Password' : password , 'ctl00$Login1$Login Button.x' : '0' , 'ctl00$Login1$Login Button.y' : '0' } data . update ( login data ) res = self . session . http . post ( self . login url , data = data ) for cookie in self . session . http . cookies : self . session attributes . set ( cookie . name , cookie . value , expires = 3600 * 24 ) if self . session attributes . get ( 'ASP.NET Session Id' ) and self . session attributes . get ( '.abportail1' ) : self . logger . debug ( 'New session data' ) self . set expires time cache ( ) return True else : self . logger . error ( 'Failed to login, check your username/password' ) return False
def output Char Formatter ( c ) : #TODO 2: allow hex only output if 32 < c < 127 : return chr ( c ) elif c == 10 : return '\\n' elif c == 13 : return '\\r' elif c == 32 : return '" "' else : return '\\x{:02x}' . format ( c )
def output Formatter ( s ) : result = '' def format Sub String ( s ) : for c in s : if c == 32 : yield ' ' else : yield output Char Formatter ( c ) if len ( result ) < 200 : return '' . join ( format Sub String ( s ) ) else : return '' . join ( format Sub String ( s [ : 100 ] ) ) + '...' + '' . join ( format Sub String ( s [ - 100 : ] ) )
def read Bytes ( self , n ) : if self . pos & 7 : raise Value Error ( 'read Bytes: need byte boundary' ) result = self . data [ self . pos >> 3 : ( self . pos >> 3 ) + n ] self . pos += 8 * n return result
def show Code ( self , width = 80 ) : #make table of all symbols with binary strings symbol Strings = [ ( self . bit Pattern ( s . index ) , self . mnemonic ( s . index ) ) for s in self ] #determine column widths the way Lisp programmers do it left Col Width , right Col Width = map ( max , map ( map , repeat ( len ) , zip ( * symbol Strings ) ) ) colwidth = left Col Width + right Col Width columns = 81 // ( colwidth + 2 ) rows = - ( - len ( symbol Strings ) // columns ) def justify ( bs ) : b , s = bs return b . rjust ( left Col Width ) + ':' + s . ljust ( right Col Width ) for i in range ( rows ) : print ( ' ' . join ( map ( justify , symbol Strings [ i : : rows ] ) ) . rstrip ( ) )
def read Tuple ( self , stream ) : length , symbol = self . decode Peek ( stream . peek ( self . max Length ) ) stream . pos += length return length , symbol
def value ( self , index , extra ) : lower , upper = self . span ( index ) value = lower + ( extra or 0 ) if value > upper : raise Value Error ( 'value: extra out of range' ) return value
def value ( self , index , extra ) : index = index if index == 0 : return 1 , 0 if index <= self . RLEMAX : return ( 1 << index ) + extra , 0 return 1 , index - self . RLEMAX
def mnemonic ( self , index ) : i , c , d0 = self . split Symbol ( index ) i Lower , = i . code . span ( i . index ) i Extra = i . extra Bits ( ) c Lower , = c . code . span ( c . index ) c Extra = c . extra Bits ( ) return 'I{}{}{}C{}{}{}{}' . format ( i Lower , '+' if i Extra else '' , 'x' * i Extra if i Extra < 6 else '[{}*x]' . format ( i Extra ) , c Lower , '+' if c Extra else '' , 'x' * c Extra if c Extra < 6 else '[{}*x]' . format ( c Extra ) , '&D=0' if d0 else '' )
def compile Actions ( self ) : import re self . action List = actions = [ None ] * 121 #Action 73, which is too long, looks like this when expanded: actions [ 73 ] = "b' the '+w+b' of the '" #find out what the columns are action Lines = self . action Table . splitlines ( ) colon Positions = [ m . start ( ) for m in re . finditer ( ':' , action Lines [ 1 ] ) ] + [ 100 ] columns = [ ( colon Positions [ i ] - 3 , colon Positions [ i + 1 ] - 3 ) for i in range ( len ( colon Positions ) - 1 ) ] for line in self . action Table . splitlines ( keepends = False ) : for start , end in columns : action = line [ start : end ] #skip empty actions if not action or action . isspace ( ) : continue #chop it up, and check if the colon is properly placed index , colon , action = action [ : 3 ] , action [ 3 ] , action [ 4 : ] assert colon == ':' #remove filler spaces at right action = action . rstrip ( ) #replace space symbols action = action . replace ( ' ' , ' ' ) w Pos = action . index ( 'w' ) #add quotes around left string when present #translation: any pattern from beginning, up to #(but not including) a + following by a w later on action = re . sub ( r"^(.*)(?=\+[U(]*w)" , r"b'\1'" , action ) #add quotes around right string when present #translation: anything with a w in it, followed by a + #and a pattern up to the end #(there is no variable lookbehind assertion, #so we have to copy the pattern) action = re . sub ( r"(w[[:\-1\]).U]*)\+(.*)$" , r"\1+b'\2'" , action ) #expand shortcut for uppercase All action = action . replace ( ".U" , ".upper()" ) #store action actions [ int ( index ) ] = action
def do Action ( self , w , action ) : #set environment for the Upper Case First U = self . upper Case1 return eval ( self . action List [ action ] , locals ( ) )
def process Stream ( self ) : print ( 'addr  hex{:{}s}binary context explanation' . format ( '' , self . width - 10 ) ) print ( 'Stream header' . center ( 60 , '-' ) ) self . window Size = self . verbose Read ( Window Size Alphabet ( ) ) print ( 'Metablock header' . center ( 60 , '=' ) ) self . ISLAST = False self . output = bytearray ( ) while not self . ISLAST : self . ISLAST = self . verbose Read ( Bool Code ( 'LAST' , description = "Last block" ) ) if self . ISLAST : if self . verbose Read ( Bool Code ( 'EMPTY' , description = "Empty block" ) ) : break if self . metablock Length ( ) : continue if not self . ISLAST and self . uncompressed ( ) : continue print ( 'Block type descriptors' . center ( 60 , '-' ) ) self . number Of Block Types = { } self . current Block Counts = { } self . block Type Codes = { } self . block Count Codes = { } for block Type in ( L , I , D ) : self . block Type ( block Type ) print ( 'Distance code parameters' . center ( 60 , '-' ) ) self . NPOSTFIX , self . NDIRECT = self . verbose Read ( Distance Param Alphabet ( ) ) self . read Literal Context Modes ( ) print ( 'Context maps' . center ( 60 , '-' ) ) self . cmaps = { } #keep the number of each kind of prefix tree for the last loop number Of Trees = { I : self . number Of Block Types [ I ] } for block Type in ( L , D ) : number Of Trees [ block Type ] = self . context Map ( block Type ) print ( 'Prefix code lists' . center ( 60 , '-' ) ) self . prefix Codes = { } for block Type in ( L , I , D ) : self . read Prefix Array ( block Type , number Of Trees [ block Type ] ) self . metablock ( )
def uncompressed ( self ) : ISUNCOMPRESSED = self . verbose Read ( Bool Code ( 'UNCMPR' , description = 'Is uncompressed?' ) ) if ISUNCOMPRESSED : self . verbose Read ( Filler Alphabet ( stream Pos = self . stream . pos ) ) print ( 'Uncompressed data:' ) self . output += self . stream . read Bytes ( self . MLEN ) print ( output Formatter ( self . output [ - self . MLEN : ] ) ) return ISUNCOMPRESSED
def block Type ( self , kind ) : NBLTYPES = self . verbose Read ( Type Count Alphabet ( 'BT#' + kind [ 0 ] . upper ( ) , description = '{} block types' . format ( kind ) , ) ) self . number Of Block Types [ kind ] = NBLTYPES if NBLTYPES >= 2 : self . block Type Codes [ kind ] = self . read Prefix Code ( Block Type Alphabet ( 'BT' + kind [ 0 ] . upper ( ) , NBLTYPES ) ) self . block Count Codes [ kind ] = self . read Prefix Code ( Block Count Alphabet ( 'BC' + kind [ 0 ] . upper ( ) ) ) block Count = self . verbose Read ( self . block Count Codes [ kind ] ) else : block Count = 1 << 24 self . current Block Counts [ kind ] = block Count
def IMTF ( v ) : #mtf is initialized virtually with range(infinity) mtf = [ ] for i , vi in enumerate ( v ) : #get old value from mtf. If never seen, take virtual value try : value = mtf . pop ( vi ) except Index Error : value = vi #put value at front mtf . insert ( 0 , value ) #replace transformed value v [ i ] = value
def read Prefix Array ( self , kind , number Of Trees ) : prefixes = [ ] for i in range ( number Of Trees ) : if kind == L : alphabet = Literal Alphabet ( i ) elif kind == I : alphabet = Insert And Copy Alphabet ( i ) elif kind == D : alphabet = Distance Alphabet ( i , NPOSTFIX = self . NPOSTFIX , NDIRECT = self . NDIRECT ) self . read Prefix Code ( alphabet ) prefixes . append ( alphabet ) self . prefix Codes [ kind ] = prefixes
def arrow table from vaex df ( ds , column names = None , selection = None , strings = True , virtual = False ) : names = [ ] arrays = [ ] for name , array in ds . to items ( column names = column names , selection = selection , strings = strings , virtual = virtual ) : names . append ( name ) arrays . append ( arrow array from numpy array ( array ) ) return pyarrow . Table . from arrays ( arrays , names )
def patch ( f ) : name = f . name Dataset . hidden [ name ] = f return f
def graphviz ( self , dot = None ) : from graphviz import Graph , Digraph node = self . graph ( ) dot = dot or Digraph ( comment = self . expression ) def walk ( node ) : if isinstance ( node , six . string types ) : dot . node ( node , node ) return node , node else : node repr , fname , fobj , deps = node node id = node repr dot . node ( node id , node repr ) for dep in deps : dep id , dep = walk ( dep ) dot . edge ( node id , dep id ) return node id , node walk ( node ) return dot
def from astropy table ( table ) : import vaex . file . other return vaex . file . other . Dataset Astropy Table ( table = table )
def zeldovich ( dim = 2 , N = 256 , n = - 2.5 , t = None , scale = 1 , seed = None ) : import vaex . file return vaex . file . other . Zeldovich ( dim = dim , N = N , n = n , t = t , scale = scale )
def vrange ( start , stop , step = 1 , dtype = 'f8' ) : from . column import Column Virtual Range return Column Virtual Range ( start , stop , step , dtype )
def open ( self , path ) : logger . debug ( "open dataset: %r" , path ) if path . startswith ( "http" ) or path . startswith ( "ws" ) : dataset = vaex . open ( path , thread mover = self . call in main thread ) else : dataset = vaex . open ( path ) self . add recently opened ( path ) self . dataset selector . add ( dataset ) return dataset
def evaluate ( self , expression , i1 = None , i2 = None , out = None , selection = None , delay = False ) : expression = ensure strings from expressions ( expression ) result = self . server . call dataset ( "evaluate" , self , expression = expression , i1 = i1 , i2 = i2 , selection = selection , delay = delay ) # TODO: we ignore out return result
def depending columns ( self , ds ) : depending = set ( ) for expression in self . expressions : expression = ds . expr ( expression ) # make sure it is an expression depending |= expression . variables ( ) if self . previous selection : depending |= self . previous selection . depending columns ( ds ) return depending
def task ( self , task , progressbar = False ) : if self . delay : # should return a task or a promise nesting it return self . executor . schedule ( task ) else : import vaex . utils callback = None try : if progressbar == True : def update ( fraction ) : bar . update ( fraction ) return True bar = vaex . utils . progressbar ( task . name ) callback = self . executor . signal progress . connect ( update ) elif progressbar : callback = self . executor . signal progress . connect ( progressbar ) result = self . executor . run ( task ) if progressbar == True : bar . finish ( ) sys . stdout . write ( '\n' ) return result finally : if callback : self . executor . signal progress . disconnect ( callback )
def sort ( self , Ncol , order ) : self . emit ( Qt Core . SIGNAL ( "layout About To Be Changed()" ) ) if Ncol == 0 : print ( "by name" ) # get indices, sorted by pair name sortlist = list ( zip ( self . pairs , list ( range ( len ( self . pairs ) ) ) ) ) print ( sortlist ) sortlist . sort ( key = operator . itemgetter ( 0 ) ) print ( sortlist ) self . indices = list ( map ( operator . itemgetter ( 1 ) , sortlist ) ) print ( ( self . indices ) ) if Ncol == 1 : # get indices, sorted by ranking, or no sorting if None not in self . ranking : sortlist = list ( zip ( self . ranking , list ( range ( len ( self . pairs ) ) ) ) ) sortlist . sort ( key = operator . itemgetter ( 0 ) ) self . indices = list ( map ( operator . itemgetter ( 1 ) , sortlist ) ) else : self . indices = list ( range ( len ( self . pairs ) ) ) print ( ( self . indices ) ) if order == Qt Core . Qt . Descending Order : self . indices . reverse ( ) print ( ( self . indices ) ) self . emit ( Qt Core . SIGNAL ( "layout Changed()" ) )
def wait ( self ) : logger . debug ( "will wait for last plot to finish" ) self . plot event = threading . Event ( ) self . queue update . wait ( ) self . queue replot . wait ( ) self . queue redraw . wait ( ) qt app = Qt Core . Q Core Application . instance ( ) sleep = 10 while not self . plot event . is set ( ) : logger . debug ( "waiting for last plot to finish" ) qt app . process Events ( ) Qt Test . Q Test . q Sleep ( sleep ) logger . debug ( "waiting for plot finished" )
def os open ( document ) : osname = platform . system ( ) . lower ( ) if osname == "darwin" : os . system ( "open \"" + document + "\"" ) if osname == "linux" : cmd = "xdg-open \"" + document + "\"&" os . system ( cmd ) if osname == "windows" : os . system ( "start \"" + document + "\"" )
def write to ( f , mode ) : if hasattr ( f , 'write' ) : yield f else : f = open ( f , mode ) yield f f . close ( )
def split and combine mask ( arrays ) : masks = [ np . ma . getmaskarray ( block ) for block in arrays if np . ma . is Masked Array ( block ) ] arrays = [ block . data if np . ma . is Masked Array ( block ) else block for block in arrays ] mask = None if masks : mask = masks [ 0 ] . copy ( ) for other in masks [ 1 : ] : mask |= other return arrays , mask
def nop ( self , expression , progress = False , delay = False ) : expression = ensure string from expression ( expression ) def map ( ar ) : pass def reduce ( a , b ) : pass return self . map reduce ( map , reduce , [ expression ] , delay = delay , progress = progress , name = 'nop' , to numpy = False )
def plot3d ( self , x , y , z , vx = None , vy = None , vz = None , vwhat = None , limits = None , grid = None , what = "count(*)" , shape = 128 , selection = [ None , True ] , f = None , vcount limits = None , smooth pre = None , smooth post = None , grid limits = None , normalize = "normalize" , colormap = "afmhot" , figure key = None , fig = None , lighting = True , level = [ 0.1 , 0.5 , 0.9 ] , opacity = [ 0.01 , 0.05 , 0.1 ] , level width = 0.1 , show = True , * * kwargs ) : import vaex . ext . ipyvolume # vaex.ext.ipyvolume. cls = vaex . ext . ipyvolume . Plot Default plot3d = cls ( df = self , x = x , y = y , z = z , vx = vx , vy = vy , vz = vz , grid = grid , shape = shape , limits = limits , what = what , f = f , figure key = figure key , fig = fig , selection = selection , smooth pre = smooth pre , smooth post = smooth post , grid limits = grid limits , vcount limits = vcount limits , normalize = normalize , colormap = colormap , * * kwargs ) if show : plot3d . show ( ) return plot3d
def dtype ( self , expression , internal = False ) : expression = ensure string from expression ( expression ) if expression in self . variables : return np . float64 ( 1 ) . dtype elif expression in self . columns . keys ( ) : column = self . columns [ expression ] data = column [ 0 : 1 ] dtype = data . dtype else : data = self . evaluate ( expression , 0 , 1 , filtered = False ) dtype = data . dtype if not internal : if dtype != str type : if dtype . kind in 'US' : return str type if dtype . kind == 'O' : # we lie about arrays containing strings if isinstance ( data [ 0 ] , six . string types ) : return str type return dtype
def remove virtual meta ( self ) : dir = self . get private dir ( create = True ) path = os . path . join ( dir , "virtual meta.yaml" ) try : if os . path . exists ( path ) : os . remove ( path ) if not os . listdir ( dir ) : os . rmdir ( dir ) except : logger . exception ( "error while trying to remove %s or %s" , path , dir )
def evaluate variable ( self , name ) : if isinstance ( self . variables [ name ] , six . string types ) : # TODO: this does not allow more than one level deep variable, like a depends on b, b on c, c is a const value = eval ( self . variables [ name ] , expression namespace , self . variables ) return value else : return self . variables [ name ]
def evaluate selection mask ( self , name = "default" , i1 = None , i2 = None , selection = None , cache = False ) : i1 = i1 or 0 i2 = i2 or len ( self ) scope = scopes . Block Scope Selection ( self , i1 , i2 , selection , cache = cache ) return scope . evaluate ( name )
def add column ( self , name , f or array ) : if isinstance ( f or array , ( np . ndarray , Column ) ) : data = ar = f or array # it can be None when we have an 'empty' Data Frame Arrays if self . length original is None : self . length unfiltered = len ( data ) self . length original = len ( data ) self . index end = self . length unfiltered if len ( ar ) != self . length original ( ) : if self . filtered : # give a better warning to avoid confusion if len ( self ) == len ( ar ) : raise Value Error ( "Array is of length %s, while the length of the Data Frame is %s due to the filtering, the (unfiltered) length is %s." % ( len ( ar ) , len ( self ) , self . length unfiltered ( ) ) ) raise Value Error ( "array is of length %s, while the length of the Data Frame is %s" % ( len ( ar ) , self . length original ( ) ) ) # assert self.length unfiltered() == len(data), "columns should be of equal length, length should be %d, while it is %d" % ( self.length unfiltered(), len(data)) self . columns [ name ] = f or array if name not in self . column names : self . column names . append ( name ) else : raise Value Error ( "functions not yet implemented" ) self . save assign expression ( name , Expression ( self , name ) )
def rename column ( self , name , new name , unique = False , store in state = True ) : new name = vaex . utils . find valid name ( new name , used = [ ] if not unique else list ( self ) ) data = self . columns . get ( name ) if data is not None : del self . columns [ name ] self . column names [ self . column names . index ( name ) ] = new name self . columns [ new name ] = data else : expression = self . virtual columns [ name ] del self . virtual columns [ name ] self . virtual columns [ new name ] = expression if store in state : self . renamed columns . append ( ( name , new name ) ) for d in [ self . ucds , self . units , self . descriptions ] : if name in d : d [ new name ] = d [ name ] del d [ name ] return new name
def delete virtual column ( self , name ) : del self . virtual columns [ name ] self . signal column changed . emit ( self , name , "delete" )
def delete variable ( self , name ) : del self . variables [ name ] self . signal variable changed . emit ( self , name , "delete" )
def tail ( self , n = 10 ) : N = len ( self ) # self.cat(i1=max(0, N-n), i2=min(len(self), N)) return self [ max ( 0 , N - n ) : min ( len ( self ) , N ) ]
def head and tail print ( self , n = 5 ) : from I Python import display display . display ( display . HTML ( self . head and tail table ( n ) ) )
def set current row ( self , value ) : if ( value is not None ) and ( ( value < 0 ) or ( value >= len ( self ) ) ) : raise Index Error ( "index %d out of range [0,%d]" % ( value , len ( self ) ) ) self . current row = value self . signal pick . emit ( self , value )
def selection undo ( self , name = "default" , executor = None ) : logger . debug ( "undo" ) executor = executor or self . executor assert self . selection can undo ( name = name ) selection history = self . selection histories [ name ] index = self . selection history indices [ name ] self . selection history indices [ name ] -= 1 self . signal selection changed . emit ( self ) logger . debug ( "undo: selection history is %r, index is %r" , selection history , self . selection history indices [ name ] )
def selection redo ( self , name = "default" , executor = None ) : logger . debug ( "redo" ) executor = executor or self . executor assert self . selection can redo ( name = name ) selection history = self . selection histories [ name ] index = self . selection history indices [ name ] next = selection history [ index + 1 ] self . selection history indices [ name ] += 1 self . signal selection changed . emit ( self ) logger . debug ( "redo: selection history is %r, index is %r" , selection history , index )
def selection can redo ( self , name = "default" ) : return ( self . selection history indices [ name ] + 1 ) < len ( self . selection histories [ name ] )
def selection ( self , create selection , name , executor = None , execute fully = False ) : selection history = self . selection histories [ name ] previous index = self . selection history indices [ name ] current = selection history [ previous index ] if selection history else None selection = create selection ( current ) executor = executor or self . executor selection history . append ( selection ) self . selection history indices [ name ] += 1 # clip any redo history del selection history [ self . selection history indices [ name ] : - 1 ] if 0 : if self . is local ( ) : if selection : # result = selection.execute(executor=executor, execute fully=execute fully) result = vaex . promise . Promise . fulfilled ( None ) self . signal selection changed . emit ( self ) else : result = vaex . promise . Promise . fulfilled ( None ) self . signal selection changed . emit ( self ) else : self . signal selection changed . emit ( self ) result = vaex . promise . Promise . fulfilled ( None ) self . signal selection changed . emit ( self ) result = vaex . promise . Promise . fulfilled ( None ) logger . debug ( "select selection history is %r, index is %r" , selection history , self . selection history indices [ name ] ) return result
def find valid name ( self , initial name ) : return vaex . utils . find valid name ( initial name , used = self . get column names ( hidden = True ) )
def root nodes ( self ) : # these lists (~used as ordered set) keep track of leafes and root nodes # root nodes root nodes = [ ] leafes = [ ] def walk ( node ) : # this function recursively walks the expression graph if isinstance ( node , six . string types ) : # we end up at a leaf leafes . append ( node ) if node in root nodes : # so it cannot be a root node root nodes . remove ( node ) else : node repr , fname , fobj , deps = node if node repr in self . virtual columns : # we encountered a virtual column, similar behaviour as leaf leafes . append ( node repr ) if node repr in root nodes : root nodes . remove ( node repr ) # resursive part for dep in deps : walk ( dep ) for column in self . virtual columns . keys ( ) : if column not in leafes : root nodes . append ( column ) node = self [ column ] . graph ( ) # we don't do the virtual column itself, just it's depedencies node repr , fname , fobj , deps = node for dep in deps : walk ( dep ) return root nodes
def graphviz ( self , dot = None ) : from graphviz import Digraph dot = dot or Digraph ( comment = 'whole dataframe' ) root nodes = self . root nodes ( ) for column in root nodes : self [ column ] . graphviz ( dot = dot ) return dot
def categorize ( self , column , labels = None , check = True ) : column = ensure string from expression ( column ) if check : vmin , vmax = self . minmax ( column ) if labels is None : N = int ( vmax + 1 ) labels = list ( map ( str , range ( N ) ) ) if ( vmax - vmin ) >= len ( labels ) : raise Value Error ( 'value of {} found, which is larger than number of labels {}' . format ( vmax , len ( labels ) ) ) self . categories [ column ] = dict ( labels = labels , N = len ( labels ) )
def hstack ( self , other , prefix = None ) : assert len ( self ) == len ( other ) , "does not make sense to horizontally stack Data Frames with different lengths" for name in other . get column names ( ) : if prefix : new name = prefix + name else : new name = name self . add column ( new name , other . columns [ name ] )
def compare ( self , other , report missing = True , report difference = False , show = 10 , orderby = None , column names = None ) : if column names is None : column names = self . get column names ( virtual = False ) for other column name in other . get column names ( virtual = False ) : if other column name not in column names : column names . append ( other column name ) different values = [ ] missing = [ ] type mismatch = [ ] meta mismatch = [ ] assert len ( self ) == len ( other ) if orderby : index1 = np . argsort ( self . columns [ orderby ] ) index2 = np . argsort ( other . columns [ orderby ] ) for column name in column names : if column name not in self . get column names ( virtual = False ) : missing . append ( column name ) if report missing : print ( "%s missing from this Data Frame" % column name ) elif column name not in other . get column names ( virtual = False ) : missing . append ( column name ) if report missing : print ( "%s missing from other Data Frame" % column name ) else : ucd1 = self . ucds . get ( column name ) ucd2 = other . ucds . get ( column name ) if ucd1 != ucd2 : print ( "ucd mismatch : %r vs %r for %s" % ( ucd1 , ucd2 , column name ) ) meta mismatch . append ( column name ) unit1 = self . units . get ( column name ) unit2 = other . units . get ( column name ) if unit1 != unit2 : print ( "unit mismatch : %r vs %r for %s" % ( unit1 , unit2 , column name ) ) meta mismatch . append ( column name ) type1 = self . dtype ( column name ) if type1 != str type : type1 = type1 . type type2 = other . dtype ( column name ) if type2 != str type : type2 = type2 . type if type1 != type2 : print ( "different dtypes: %s vs %s for %s" % ( self . dtype ( column name ) , other . dtype ( column name ) , column name ) ) type mismatch . append ( column name ) else : # a = self.columns[column name] # b = other.columns[column name] # if self.filtered: #   a = a[self.evaluate selection mask(None)] # if other.filtered: #   b = b[other.evaluate selection mask(None)] a = self . evaluate ( column name ) b = other . evaluate ( column name ) if orderby : a = a [ index1 ] b = b [ index2 ] def normalize ( ar ) : if ar . dtype == str type : return ar if ar . dtype . kind == "f" and hasattr ( ar , "mask" ) : mask = ar . mask ar = ar . copy ( ) ar [ mask ] = np . nan if ar . dtype . kind in "SU" : if hasattr ( ar , "mask" ) : data = ar . data else : data = ar values = [ value . strip ( ) for value in data . tolist ( ) ] if hasattr ( ar , "mask" ) : ar = np . ma . masked array ( values , ar . mask ) else : ar = np . array ( values ) return ar def equal mask ( a , b ) : a = normalize ( a ) b = normalize ( b ) boolean mask = ( a == b ) if self . dtype ( column name ) != str type and self . dtype ( column name ) . kind == 'f' : # floats with nan won't equal itself, i.e. Na N != Na N boolean mask |= ( np . isnan ( a ) & np . isnan ( b ) ) return boolean mask boolean mask = equal mask ( a , b ) all equal = np . all ( boolean mask ) if not all equal : count = np . sum ( ~ boolean mask ) print ( "%s does not match for both Data Frames, %d rows are diffent out of %d" % ( column name , count , len ( self ) ) ) different values . append ( column name ) if report difference : indices = np . arange ( len ( self ) ) [ ~ boolean mask ] values1 = self . columns [ column name ] [ : ] [ ~ boolean mask ] values2 = other . columns [ column name ] [ : ] [ ~ boolean mask ] print ( "\tshowing difference for the first 10" ) for i in range ( min ( len ( values1 ) , show ) ) : try : diff = values1 [ i ] - values2 [ i ] except : diff = "does not exists" print ( "%s[%d] == %s != %s other.%s[%d] (diff = %s)" % ( column name , indices [ i ] , values1 [ i ] , values2 [ i ] , column name , indices [ i ] , diff ) ) return different values , missing , type mismatch , meta mismatch
def patch ( f ) : name = f . name setattr ( Data Frame , name , f ) return f
def as recarray ( self ) : dtype = [ ( k , v . dtype ) for k , v in self . dict . iteritems ( ) ] R = numpy . recarray ( len ( self . dict [ k ] ) , dtype = dtype ) for key in self . dict : R [ key ] = self . dict [ key ] return R
def show versions ( ) : core deps = [ 'audioread' , 'numpy' , 'scipy' , 'sklearn' , 'joblib' , 'decorator' , 'six' , 'soundfile' , 'resampy' , 'numba' ] extra deps = [ 'numpydoc' , 'sphinx' , 'sphinx rtd theme' , 'sphinxcontrib.versioning' , 'sphinx-gallery' , 'pytest' , 'pytest-mpl' , 'pytest-cov' , 'matplotlib' ] print ( 'INSTALLED VERSIONS' ) print ( '------------------' ) print ( 'python: {}\n' . format ( sys . version ) ) print ( 'librosa: {}\n' . format ( version ) ) for dep in core deps : print ( '{}: {}' . format ( dep , get mod version ( dep ) ) ) print ( '' ) for dep in extra deps : print ( '{}: {}' . format ( dep , get mod version ( dep ) ) ) pass
def adjust tuning ( input file , output file ) : print ( 'Loading ' , input file ) y , sr = librosa . load ( input file ) print ( 'Separating harmonic component ... ' ) y harm = librosa . effects . harmonic ( y ) print ( 'Estimating tuning ... ' ) # Just track the pitches associated with high magnitude tuning = librosa . estimate tuning ( y = y harm , sr = sr ) print ( '{:+0.2f} cents' . format ( 100 * tuning ) ) print ( 'Applying pitch-correction of {:+0.2f} cents' . format ( - 100 * tuning ) ) y tuned = librosa . effects . pitch shift ( y , sr , - tuning ) print ( 'Saving tuned audio to: ' , output file ) librosa . output . write wav ( output file , y tuned , sr )
def cqt filter fft ( sr , fmin , n bins , bins per octave , tuning , filter scale , norm , sparsity , hop length = None , window = 'hann' ) : basis , lengths = filters . constant q ( sr , fmin = fmin , n bins = n bins , bins per octave = bins per octave , tuning = tuning , filter scale = filter scale , norm = norm , pad fft = True , window = window ) # Filters are padded up to the nearest integral power of 2 n fft = basis . shape [ 1 ] if ( hop length is not None and n fft < 2.0 ** ( 1 + np . ceil ( np . log2 ( hop length ) ) ) ) : n fft = int ( 2.0 ** ( 1 + np . ceil ( np . log2 ( hop length ) ) ) ) # re-normalize bases with respect to the FFT window length basis *= lengths [ : , np . newaxis ] / float ( n fft ) # FFT and retain only the non-negative frequencies fft = get fftlib ( ) fft basis = fft . fft ( basis , n = n fft , axis = 1 ) [ : , : ( n fft // 2 ) + 1 ] # sparsify the basis fft basis = util . sparsify rows ( fft basis , quantile = sparsity ) return fft basis , n fft , lengths
def trim stack ( cqt resp , n bins ) : # cleanup any framing errors at the boundaries max col = min ( x . shape [ 1 ] for x in cqt resp ) cqt resp = np . vstack ( [ x [ : , : max col ] for x in cqt resp ] [ : : - 1 ] ) # Finally, clip out any bottom frequencies that we don't really want # Transpose magic here to ensure column-contiguity return np . ascontiguousarray ( cqt resp [ - n bins : ] . T ) . T
def cqt response ( y , n fft , hop length , fft basis , mode ) : # Compute the STFT matrix D = stft ( y , n fft = n fft , hop length = hop length , window = 'ones' , pad mode = mode ) # And filter response energy return fft basis . dot ( D )
def early downsample count ( nyquist , filter cutoff , hop length , n octaves ) : downsample count1 = max ( 0 , int ( np . ceil ( np . log2 ( audio . BW FASTEST * nyquist / filter cutoff ) ) - 1 ) - 1 ) num twos = num two factors ( hop length ) downsample count2 = max ( 0 , num twos - n octaves + 1 ) return min ( downsample count1 , downsample count2 )
def early downsample ( y , sr , hop length , res type , n octaves , nyquist , filter cutoff , scale ) : downsample count = early downsample count ( nyquist , filter cutoff , hop length , n octaves ) if downsample count > 0 and res type == 'kaiser fast' : downsample factor = 2 ** ( downsample count ) hop length //= downsample factor if len ( y ) < downsample factor : raise Parameter Error ( 'Input signal length={:d} is too short for ' '{:d}-octave CQT' . format ( len ( y ) , n octaves ) ) new sr = sr / float ( downsample factor ) y = audio . resample ( y , sr , new sr , res type = res type , scale = True ) # If we're not going to length-scale after CQT, we # need to compensate for the downsampling factor here if not scale : y *= np . sqrt ( downsample factor ) sr = new sr return y , sr , hop length
def check axes ( axes ) : if axes is None : import matplotlib . pyplot as plt axes = plt . gca ( ) elif not isinstance ( axes , Axes ) : raise Value Error ( "`axes` must be an instance of matplotlib.axes.Axes. " "Found type(axes)={}" . format ( type ( axes ) ) ) return axes
def scale axes ( axes , ax type , which ) : kwargs = dict ( ) if which == 'x' : thresh = 'linthreshx' base = 'basex' scale = 'linscalex' scaler = axes . set xscale limit = axes . set xlim else : thresh = 'linthreshy' base = 'basey' scale = 'linscaley' scaler = axes . set yscale limit = axes . set ylim # Map ticker scales if ax type == 'mel' : mode = 'symlog' kwargs [ thresh ] = 1000.0 kwargs [ base ] = 2 elif ax type == 'log' : mode = 'symlog' kwargs [ base ] = 2 kwargs [ thresh ] = core . note to hz ( 'C2' ) kwargs [ scale ] = 0.5 elif ax type in [ 'cqt' , 'cqt hz' , 'cqt note' ] : mode = 'log' kwargs [ base ] = 2 elif ax type == 'tempo' : mode = 'log' kwargs [ base ] = 2 limit ( 16 , 480 ) else : return scaler ( mode , * * kwargs )
def decorate axis ( axis , ax type ) : if ax type == 'tonnetz' : axis . set major formatter ( Tonnetz Formatter ( ) ) axis . set major locator ( Fixed Locator ( 0.5 + np . arange ( 6 ) ) ) axis . set label text ( 'Tonnetz' ) elif ax type == 'chroma' : axis . set major formatter ( Chroma Formatter ( ) ) axis . set major locator ( Fixed Locator ( 0.5 + np . add . outer ( 12 * np . arange ( 10 ) , [ 0 , 2 , 4 , 5 , 7 , 9 , 11 ] ) . ravel ( ) ) ) axis . set label text ( 'Pitch class' ) elif ax type == 'tempo' : axis . set major formatter ( Scalar Formatter ( ) ) axis . set major locator ( Log Locator ( base = 2.0 ) ) axis . set label text ( 'BPM' ) elif ax type == 'time' : axis . set major formatter ( Time Formatter ( unit = None , lag = False ) ) axis . set major locator ( Max N Locator ( prune = None , steps = [ 1 , 1.5 , 5 , 6 , 10 ] ) ) axis . set label text ( 'Time' ) elif ax type == 's' : axis . set major formatter ( Time Formatter ( unit = 's' , lag = False ) ) axis . set major locator ( Max N Locator ( prune = None , steps = [ 1 , 1.5 , 5 , 6 , 10 ] ) ) axis . set label text ( 'Time (s)' ) elif ax type == 'ms' : axis . set major formatter ( Time Formatter ( unit = 'ms' , lag = False ) ) axis . set major locator ( Max N Locator ( prune = None , steps = [ 1 , 1.5 , 5 , 6 , 10 ] ) ) axis . set label text ( 'Time (ms)' ) elif ax type == 'lag' : axis . set major formatter ( Time Formatter ( unit = None , lag = True ) ) axis . set major locator ( Max N Locator ( prune = None , steps = [ 1 , 1.5 , 5 , 6 , 10 ] ) ) axis . set label text ( 'Lag' ) elif ax type == 'lag s' : axis . set major formatter ( Time Formatter ( unit = 's' , lag = True ) ) axis . set major locator ( Max N Locator ( prune = None , steps = [ 1 , 1.5 , 5 , 6 , 10 ] ) ) axis . set label text ( 'Lag (s)' ) elif ax type == 'lag ms' : axis . set major formatter ( Time Formatter ( unit = 'ms' , lag = True ) ) axis . set major locator ( Max N Locator ( prune = None , steps = [ 1 , 1.5 , 5 , 6 , 10 ] ) ) axis . set label text ( 'Lag (ms)' ) elif ax type == 'cqt note' : axis . set major formatter ( Note Formatter ( ) ) axis . set major locator ( Log Locator ( base = 2.0 ) ) axis . set minor formatter ( Note Formatter ( major = False ) ) axis . set minor locator ( Log Locator ( base = 2.0 , subs = 2.0 ** ( np . arange ( 1 , 12 ) / 12.0 ) ) ) axis . set label text ( 'Note' ) elif ax type in [ 'cqt hz' ] : axis . set major formatter ( Log Hz Formatter ( ) ) axis . set major locator ( Log Locator ( base = 2.0 ) ) axis . set minor formatter ( Log Hz Formatter ( major = False ) ) axis . set minor locator ( Log Locator ( base = 2.0 , subs = 2.0 ** ( np . arange ( 1 , 12 ) / 12.0 ) ) ) axis . set label text ( 'Hz' ) elif ax type in [ 'mel' , 'log' ] : axis . set major formatter ( Scalar Formatter ( ) ) axis . set major locator ( Symmetrical Log Locator ( axis . get transform ( ) ) ) axis . set label text ( 'Hz' ) elif ax type in [ 'linear' , 'hz' ] : axis . set major formatter ( Scalar Formatter ( ) ) axis . set label text ( 'Hz' ) elif ax type in [ 'frames' ] : axis . set label text ( 'Frames' ) elif ax type in [ 'off' , 'none' , None ] : axis . set label text ( '' ) axis . set ticks ( [ ] )
def coord fft hz ( n , sr = 22050 , * * kwargs ) : n fft = 2 * ( n - 1 ) # The following code centers the FFT bins at their frequencies # and clips to the non-negative frequency range [0, nyquist] basis = core . fft frequencies ( sr = sr , n fft = n fft ) fmax = basis [ - 1 ] basis -= 0.5 * ( basis [ 1 ] - basis [ 0 ] ) basis = np . append ( np . maximum ( 0 , basis ) , [ fmax ] ) return basis
def coord mel hz ( n , fmin = 0 , fmax = 11025.0 , * * kwargs ) : if fmin is None : fmin = 0 if fmax is None : fmax = 11025.0 basis = core . mel frequencies ( n , fmin = fmin , fmax = fmax ) basis [ 1 : ] -= 0.5 * np . diff ( basis ) basis = np . append ( np . maximum ( 0 , basis ) , [ fmax ] ) return basis
def coord cqt hz ( n , fmin = None , bins per octave = 12 , * * kwargs ) : if fmin is None : fmin = core . note to hz ( 'C1' ) # we drop by half a bin so that CQT bins are centered vertically return core . cqt frequencies ( n + 1 , fmin = fmin / 2.0 ** ( 0.5 / bins per octave ) , bins per octave = bins per octave )
def coord chroma ( n , bins per octave = 12 , * * kwargs ) : return np . linspace ( 0 , ( 12.0 * n ) / bins per octave , num = n + 1 , endpoint = True )
def coord time ( n , sr = 22050 , hop length = 512 , * * kwargs ) : return core . frames to time ( np . arange ( n + 1 ) , sr = sr , hop length = hop length )
def window ss fill ( x , win sq , n frames , hop length ) : # pragma: no cover n = len ( x ) n fft = len ( win sq ) for i in range ( n frames ) : sample = i * hop length x [ sample : min ( n , sample + n fft ) ] += win sq [ : max ( 0 , min ( n fft , n - sample ) ) ]
def match interval overlaps ( query , intervals to , candidates ) : # pragma: no cover best score = - 1 best idx = - 1 for idx in candidates : score = jaccard ( query , intervals to [ idx ] ) if score > best score : best score , best idx = score , idx return best idx
def match intervals ( intervals from , intervals to , strict = True ) : # pragma: no cover # sort index of the interval starts start index = np . argsort ( intervals to [ : , 0 ] ) # sort index of the interval ends end index = np . argsort ( intervals to [ : , 1 ] ) # and sorted values of starts start sorted = intervals to [ start index , 0 ] # and ends end sorted = intervals to [ end index , 1 ] search ends = np . searchsorted ( start sorted , intervals from [ : , 1 ] , side = 'right' ) search starts = np . searchsorted ( end sorted , intervals from [ : , 0 ] , side = 'left' ) output = np . empty ( len ( intervals from ) , dtype = numba . uint32 ) for i in range ( len ( intervals from ) ) : query = intervals from [ i ] # Find the intervals that start after our query ends after query = search ends [ i ] # And the intervals that end after our query begins before query = search starts [ i ] # Candidates for overlapping have to (end after we start) and (begin before we end) candidates = set ( start index [ : after query ] ) & set ( end index [ before query : ] ) # Proceed as before if len ( candidates ) > 0 : output [ i ] = match interval overlaps ( query , intervals to , candidates ) elif strict : # Numba only lets us use compile-time constants in exception messages raise Parameter Error else : # Find the closest interval # (start index[after query] - query[1]) is the distance to the next interval # (query[0] - end index[before query]) dist before = np . inf dist after = np . inf if search starts [ i ] > 0 : dist before = query [ 0 ] - end sorted [ search starts [ i ] - 1 ] if search ends [ i ] + 1 < len ( intervals to ) : dist after = start sorted [ search ends [ i ] + 1 ] - query [ 1 ] if dist before < dist after : output [ i ] = end index [ search starts [ i ] - 1 ] else : output [ i ] = start index [ search ends [ i ] + 1 ] return output
def get files ( dir name , extensions ) : # Expand out the directory dir name = os . path . abspath ( os . path . expanduser ( dir name ) ) myfiles = set ( ) for sub ext in extensions : globstr = os . path . join ( dir name , '*' + os . path . extsep + sub ext ) myfiles |= set ( glob . glob ( globstr ) ) return myfiles
def process arguments ( args ) : parser = argparse . Argument Parser ( description = 'Time stretching example' ) parser . add argument ( 'input file' , action = 'store' , help = 'path to the input file (wav, mp3, etc)' ) parser . add argument ( 'output file' , action = 'store' , help = 'path to the stretched output (wav)' ) parser . add argument ( '-s' , '--speed' , action = 'store' , type = float , default = 2.0 , required = False , help = 'speed' ) return vars ( parser . parse args ( args ) )
def beat local score ( onset envelope , period ) : window = np . exp ( - 0.5 * ( np . arange ( - period , period + 1 ) * 32.0 / period ) ** 2 ) return scipy . signal . convolve ( normalize onsets ( onset envelope ) , window , 'same' )
def beat track dp ( localscore , period , tightness ) : backlink = np . zeros like ( localscore , dtype = int ) cumscore = np . zeros like ( localscore ) # Search range for previous beat window = np . arange ( - 2 * period , - np . round ( period / 2 ) + 1 , dtype = int ) # Make a score window, which begins biased toward start bpm and skewed if tightness <= 0 : raise Parameter Error ( 'tightness must be strictly positive' ) txwt = - tightness * ( np . log ( - window / period ) ** 2 ) # Are we on the first beat? first beat = True for i , score i in enumerate ( localscore ) : # Are we reaching back before time 0? z pad = np . maximum ( 0 , min ( - window [ 0 ] , len ( window ) ) ) # Search over all possible predecessors candidates = txwt . copy ( ) candidates [ z pad : ] = candidates [ z pad : ] + cumscore [ window [ z pad : ] ] # Find the best preceding beat beat location = np . argmax ( candidates ) # Add the local score cumscore [ i ] = score i + candidates [ beat location ] # Special case the first onset.  Stop if the localscore is small if first beat and score i < 0.01 * localscore . max ( ) : backlink [ i ] = - 1 else : backlink [ i ] = window [ beat location ] first beat = False # Update the time range window = window + 1 return backlink , cumscore
def last beat ( cumscore ) : maxes = util . localmax ( cumscore ) med score = np . median ( cumscore [ np . argwhere ( maxes ) ] ) # The last of these is the last beat (since score generally increases) return np . argwhere ( ( cumscore * maxes * 2 > med score ) ) . max ( )
def conv3x3 ( in planes , out planes , dilation = 1 ) : return nn . Conv2d ( in planes , out planes , kernel size = 3 , padding = dilation , dilation = dilation )
def average ( self , n = 0 ) : assert n >= 0 for key in self . val history : values = np . array ( self . val history [ key ] [ - n : ] ) nums = np . array ( self . n history [ key ] [ - n : ] ) avg = np . sum ( values * nums ) / np . sum ( nums ) self . output [ key ] = avg self . ready = True
def scatter ( input , devices , streams = None ) : if streams is None : streams = [ None ] * len ( devices ) if isinstance ( input , list ) : chunk size = ( len ( input ) - 1 ) // len ( devices ) + 1 outputs = [ scatter ( input [ i ] , [ devices [ i // chunk size ] ] , [ streams [ i // chunk size ] ] ) for i in range ( len ( input ) ) ] return outputs elif isinstance ( input , torch . Tensor ) : output = input . contiguous ( ) # TODO: copy to a pinned buffer first (if copying from CPU) stream = streams [ 0 ] if output . numel ( ) > 0 else None with torch . cuda . device ( devices [ 0 ] ) , torch . cuda . stream ( stream ) : output = output . cuda ( devices [ 0 ] , non blocking = True ) return output else : raise Exception ( 'Unknown type {}.' . format ( type ( input ) ) )
def start ( self ) : if not self . is running : self . t start = time ( ) self . is running = True self . t last = time ( )
def scatter kwargs ( inputs , kwargs , target gpus , dim = 0 ) : inputs = scatter ( inputs , target gpus , dim ) if inputs else [ ] kwargs = scatter ( kwargs , target gpus , dim ) if kwargs else [ ] if len ( inputs ) < len ( kwargs ) : inputs . extend ( [ ( ) for in range ( len ( kwargs ) - len ( inputs ) ) ] ) elif len ( kwargs ) < len ( inputs ) : kwargs . extend ( [ { } for in range ( len ( inputs ) - len ( kwargs ) ) ] ) inputs = tuple ( inputs ) kwargs = tuple ( kwargs ) return inputs , kwargs
async def json ( self , * , encoding : str = None , loads : JSON Decoder = DEFAULT JSON DECODER , content type : Optional [ str ] = 'application/json' ) -> Any : return await self . aws json ( encoding = encoding , loads = loads , content type = content type )
async def text ( self , * , encoding : Optional [ str ] = None , errors : str = 'strict' ) -> str : return await self . aws text ( encoding = encoding , errors = errors )
async def handle callback ( self , aws callback : typing . Coroutine , response ) : callback result = None try : callback result = await aws callback except Nothing Matched Error as e : self . logger . error ( f'<Item: {str(e).lower()}>' ) except Exception as e : self . logger . error ( f'<Callback[{aws callback. name }]: {e}' ) return callback result , response
async def multiple request ( self , urls , is gather = False , * * kwargs ) : if is gather : resp results = await asyncio . gather ( * [ self . handle request ( self . request ( url = url , * * kwargs ) ) for url in urls ] , return exceptions = True ) for index , task result in enumerate ( resp results ) : if not isinstance ( task result , Runtime Error ) and task result : , response = task result response . index = index yield response else : for index , url in enumerate ( urls ) : , response = await self . handle request ( self . request ( url = url , * * kwargs ) ) response . index = index yield response
def request ( self , url : str , method : str = 'GET' , * , callback = None , encoding : typing . Optional [ str ] = None , headers : dict = None , metadata : dict = None , request config : dict = None , request session = None , * * kwargs ) : headers = headers or { } metadata = metadata or { } request config = request config or { } request session = request session or self . request session headers . update ( self . headers . copy ( ) ) request config . update ( self . request config . copy ( ) ) kwargs . update ( self . kwargs . copy ( ) ) return Request ( url = url , method = method , callback = callback , encoding = encoding , headers = headers , metadata = metadata , request config = request config , request session = request session , * * kwargs )
async def start master ( self ) : for url in self . start urls : request ins = self . request ( url = url , callback = self . parse , metadata = self . metadata ) self . request queue . put nowait ( self . handle request ( request ins ) ) workers = [ asyncio . ensure future ( self . start worker ( ) ) for i in range ( self . worker numbers ) ] for worker in workers : self . logger . info ( f"Worker started: {id(worker)}" ) await self . request queue . join ( ) if not self . is async start : await self . stop ( SIGINT ) else : await self . cancel tasks ( )
def normalize task v2 ( task ) : result = dict ( ) mod arg parser = Module Args Parser ( task ) try : action , arguments , result [ 'delegate to' ] = mod arg parser . parse ( ) except Ansible Parser Error as e : try : task info = "%s:%s" % ( task [ FILENAME KEY ] , task [ LINE NUMBER KEY ] ) del task [ FILENAME KEY ] del task [ LINE NUMBER KEY ] except Key Error : task info = "Unknown" try : import pprint pp = pprint . Pretty Printer ( indent = 2 ) task pprint = pp . pformat ( task ) except Import Error : task pprint = task raise System Exit ( "Couldn't parse task at %s (%s)\n%s" % ( task info , e . message , task pprint ) ) # denormalize shell -> command conversion if ' uses shell' in arguments : action = 'shell' del ( arguments [ ' uses shell' ] ) for ( k , v ) in list ( task . items ( ) ) : if k in ( 'action' , 'local action' , 'args' , 'delegate to' ) or k == action : # we don't want to re-assign these values, which were # determined by the Module Args Parser() above continue else : result [ k ] = v result [ 'action' ] = dict ( ansible module = action ) if ' raw params' in arguments : result [ 'action' ] [ ' ansible arguments ' ] = arguments [ ' raw params' ] . split ( ' ' ) del ( arguments [ ' raw params' ] ) else : result [ 'action' ] [ ' ansible arguments ' ] = list ( ) if 'argv' in arguments and not result [ 'action' ] [ ' ansible arguments ' ] : result [ 'action' ] [ ' ansible arguments ' ] = arguments [ 'argv' ] del ( arguments [ 'argv' ] ) result [ 'action' ] . update ( arguments ) return result
def wheel dist name ( self ) : return '-' . join ( ( safer name ( self . distribution . get name ( ) ) , safer version ( self . distribution . get version ( ) ) ) )
def get archive basename ( self ) : impl tag , abi tag , plat tag = self . get tag ( ) archive basename = "%s-%s-%s-%s" % ( self . wheel dist name , impl tag , abi tag , plat tag ) return archive basename
def add requirements ( self , metadata path ) : additional = list ( self . setupcfg requirements ( ) ) if not additional : return pkg info = read pkg info ( metadata path ) if 'Provides-Extra' in pkg info or 'Requires-Dist' in pkg info : warnings . warn ( 'setup.cfg requirements overwrite values from setup.py' ) del pkg info [ 'Provides-Extra' ] del pkg info [ 'Requires-Dist' ] for k , v in additional : pkg info [ k ] = v write pkg info ( metadata path , pkg info )
def egg2dist ( self , egginfo path , distinfo path ) : def adios ( p ) : """Appropriately delete directory, file or link.""" if os . path . exists ( p ) and not os . path . islink ( p ) and os . path . isdir ( p ) : shutil . rmtree ( p ) elif os . path . exists ( p ) : os . unlink ( p ) adios ( distinfo path ) if not os . path . exists ( egginfo path ) : # There is no egg-info. This is probably because the egg-info # file/directory is not named matching the distribution name used # to name the archive file. Check for this case and report # accordingly. import glob pat = os . path . join ( os . path . dirname ( egginfo path ) , '*.egg-info' ) possible = glob . glob ( pat ) err = "Egg metadata expected at %s but not found" % ( egginfo path , ) if possible : alt = os . path . basename ( possible [ 0 ] ) err += " (%s found - possible misnamed archive file?)" % ( alt , ) raise Value Error ( err ) if os . path . isfile ( egginfo path ) : # .egg-info is a single file pkginfo path = egginfo path pkg info = self . pkginfo to metadata ( egginfo path , egginfo path ) os . mkdir ( distinfo path ) else : # .egg-info is a directory pkginfo path = os . path . join ( egginfo path , 'PKG-INFO' ) pkg info = self . pkginfo to metadata ( egginfo path , pkginfo path ) # ignore common egg metadata that is useless to wheel shutil . copytree ( egginfo path , distinfo path , ignore = lambda x , y : set ( ( 'PKG-INFO' , 'requires.txt' , 'SOURCES.txt' , 'not-zip-safe' , ) ) ) # delete dependency links if it is only whitespace dependency links path = os . path . join ( distinfo path , 'dependency links.txt' ) with open ( dependency links path , 'r' ) as dependency links file : dependency links = dependency links file . read ( ) . strip ( ) if not dependency links : adios ( dependency links path ) write pkg info ( os . path . join ( distinfo path , 'METADATA' ) , pkg info ) # XXX deprecated. Still useful for current distribute/setuptools. metadata path = os . path . join ( distinfo path , 'METADATA' ) self . add requirements ( metadata path ) # XXX intentionally a different path than the PEP. metadata json path = os . path . join ( distinfo path , 'metadata.json' ) pymeta = pkginfo to dict ( metadata path , distribution = self . distribution ) if 'description' in pymeta : description filename = 'DESCRIPTION.rst' description text = pymeta . pop ( 'description' ) description path = os . path . join ( distinfo path , description filename ) with open ( description path , "wb" ) as description file : description file . write ( description text . encode ( 'utf-8' ) ) pymeta [ 'extensions' ] [ 'python.details' ] [ 'document names' ] [ 'description' ] = description filename # XXX heuristically copy any LICENSE/LICENSE.txt? license = self . license file ( ) if license : license filename = 'LICENSE.txt' shutil . copy ( license , os . path . join ( self . distinfo dir , license filename ) ) pymeta [ 'extensions' ] [ 'python.details' ] [ 'document names' ] [ 'license' ] = license filename with open ( metadata json path , "w" ) as metadata json : json . dump ( pymeta , metadata json , sort keys = True ) adios ( egginfo path )
def telemetry client ( self , value : Bot Telemetry Client ) -> None : if value is None : self . telemetry client = Null Telemetry Client ( ) else : self . telemetry client = value
def create db and container ( self ) : db id = self . config . database container name = self . config . container self . db = self . get or create database ( self . client , db id ) self . container = self . get or create container ( self . client , container name )
def get step name ( self , index : int ) -> str : step name = self . steps [ index ] . qualname if not step name or ">" in step name : step name = f"Step{index + 1}of{len(self. steps)}" return step name
def c if ( self , classical , val ) : if not isinstance ( classical , Classical Register ) : raise Qiskit Error ( "c if must be used with a classical register" ) if val < 0 : raise Qiskit Error ( "control value should be non-negative" ) self . control = ( classical , val ) return self
def qasmif ( self , string ) : if self . control is None : return string return "if(%s==%d) " % ( self . control [ 0 ] . name , self . control [ 1 ] ) + string
def u base ( self , theta , phi , lam , q ) : return self . append ( U Base ( theta , phi , lam ) , [ q ] , [ ] )
def to matrix ( self ) : theta , phi , lam = self . params return numpy . array ( [ [ numpy . cos ( theta / 2 ) , - numpy . exp ( 1j * lam ) * numpy . sin ( theta / 2 ) ] , [ numpy . exp ( 1j * phi ) * numpy . sin ( theta / 2 ) , numpy . exp ( 1j * ( phi + lam ) ) * numpy . cos ( theta / 2 ) ] ] , dtype = complex )
def exp fit fun ( x , a , tau , c ) : # pylint: disable=invalid-name return a * np . exp ( - x / tau ) + c
def osc fit fun ( x , a , tau , f , phi , c ) : # pylint: disable=invalid-name return a * np . exp ( - x / tau ) * np . cos ( 2 * np . pi * f * x + phi ) + c
def run ( self , dag ) : runs = dag . collect runs ( [ "u1" , "u2" , "u3" , "id" ] ) runs = split runs on parameters ( runs ) for run in runs : right name = "u1" right parameters = ( 0 , 0 , 0 ) # (theta, phi, lambda) for current node in run : left name = current node . name if ( current node . condition is not None or len ( current node . qargs ) != 1 or left name not in [ "u1" , "u2" , "u3" , "id" ] ) : raise Transpiler Error ( "internal error" ) if left name == "u1" : left parameters = ( 0 , 0 , current node . op . params [ 0 ] ) elif left name == "u2" : left parameters = ( np . pi / 2 , current node . op . params [ 0 ] , current node . op . params [ 1 ] ) elif left name == "u3" : left parameters = tuple ( current node . op . params ) else : left name = "u1" # replace id with u1 left parameters = ( 0 , 0 , 0 ) # If there are any sympy objects coming from the gate convert # to numpy. left parameters = tuple ( [ float ( x ) for x in left parameters ] ) # Compose gates name tuple = ( left name , right name ) if name tuple == ( "u1" , "u1" ) : # u1(lambda1) * u1(lambda2) = u1(lambda1 + lambda2) right parameters = ( 0 , 0 , right parameters [ 2 ] + left parameters [ 2 ] ) elif name tuple == ( "u1" , "u2" ) : # u1(lambda1) * u2(phi2, lambda2) = u2(phi2 + lambda1, lambda2) right parameters = ( np . pi / 2 , right parameters [ 1 ] + left parameters [ 2 ] , right parameters [ 2 ] ) elif name tuple == ( "u2" , "u1" ) : # u2(phi1, lambda1) * u1(lambda2) = u2(phi1, lambda1 + lambda2) right name = "u2" right parameters = ( np . pi / 2 , left parameters [ 1 ] , right parameters [ 2 ] + left parameters [ 2 ] ) elif name tuple == ( "u1" , "u3" ) : # u1(lambda1) * u3(theta2, phi2, lambda2) = #     u3(theta2, phi2 + lambda1, lambda2) right parameters = ( right parameters [ 0 ] , right parameters [ 1 ] + left parameters [ 2 ] , right parameters [ 2 ] ) elif name tuple == ( "u3" , "u1" ) : # u3(theta1, phi1, lambda1) * u1(lambda2) = #     u3(theta1, phi1, lambda1 + lambda2) right name = "u3" right parameters = ( left parameters [ 0 ] , left parameters [ 1 ] , right parameters [ 2 ] + left parameters [ 2 ] ) elif name tuple == ( "u2" , "u2" ) : # Using Ry(pi/2).Rz(2*lambda).Ry(pi/2) = #    Rz(pi/2).Ry(pi-2*lambda).Rz(pi/2), # u2(phi1, lambda1) * u2(phi2, lambda2) = #    u3(pi - lambda1 - phi2, phi1 + pi/2, lambda2 + pi/2) right name = "u3" right parameters = ( np . pi - left parameters [ 2 ] - right parameters [ 1 ] , left parameters [ 1 ] + np . pi / 2 , right parameters [ 2 ] + np . pi / 2 ) elif name tuple [ 1 ] == "nop" : right name = left name right parameters = left parameters else : # For composing u3's or u2's with u3's, use # u2(phi, lambda) = u3(pi/2, phi, lambda) # together with the qiskit.mapper.compose u3 method. right name = "u3" # Evaluate the symbolic expressions for efficiency right parameters = Optimize1q Gates . compose u3 ( left parameters [ 0 ] , left parameters [ 1 ] , left parameters [ 2 ] , right parameters [ 0 ] , right parameters [ 1 ] , right parameters [ 2 ] ) # Why evalf()? This program: #   OPENQASM 2.0; #   include "qelib1.inc"; #   qreg q[2]; #   creg c[2]; #   u3(0.518016983430947*pi,1.37051598592907*pi,1.36816383603222*pi) q[0]; #   u3(1.69867232277986*pi,0.371448347747471*pi,0.461117217930936*pi) q[0]; #   u3(0.294319836336836*pi,0.450325871124225*pi,1.46804720442555*pi) q[0]; #   measure q -> c; # took >630 seconds (did not complete) to optimize without # calling evalf() at all, 19 seconds to optimize calling # evalf() AFTER compose u3, and 1 second to optimize # calling evalf() BEFORE compose u3. # 1. Here down, when we simplify, we add f(theta) to lambda to # correct the global phase when f(theta) is 2*pi. This isn't # necessary but the other steps preserve the global phase, so # we continue in that manner. # 2. The final step will remove Z rotations by 2*pi. # 3. Note that is zero is true only if the expression is exactly # zero. If the input expressions have already been evaluated # then these final simplifications will not occur. # TODO After we refactor, we should have separate passes for # exact and approximate rewriting. # Y rotation is 0 mod 2*pi, so the gate is a u1 if np . mod ( right parameters [ 0 ] , ( 2 * np . pi ) ) == 0 and right name != "u1" : right name = "u1" right parameters = ( 0 , 0 , right parameters [ 1 ] + right parameters [ 2 ] + right parameters [ 0 ] ) # Y rotation is pi/2 or -pi/2 mod 2*pi, so the gate is a u2 if right name == "u3" : # theta = pi/2 + 2*k*pi if np . mod ( ( right parameters [ 0 ] - np . pi / 2 ) , ( 2 * np . pi ) ) == 0 : right name = "u2" right parameters = ( np . pi / 2 , right parameters [ 1 ] , right parameters [ 2 ] + ( right parameters [ 0 ] - np . pi / 2 ) ) # theta = -pi/2 + 2*k*pi if np . mod ( ( right parameters [ 0 ] + np . pi / 2 ) , ( 2 * np . pi ) ) == 0 : right name = "u2" right parameters = ( np . pi / 2 , right parameters [ 1 ] + np . pi , right parameters [ 2 ] - np . pi + ( right parameters [ 0 ] + np . pi / 2 ) ) # u1 and lambda is 0 mod 2*pi so gate is nop (up to a global phase) if right name == "u1" and np . mod ( right parameters [ 2 ] , ( 2 * np . pi ) ) == 0 : right name = "nop" # Replace the the first node in the run with a dummy DAG which contains a dummy # qubit. The name is irrelevant, because substitute node with dag will take care of # putting it in the right place. run qarg = ( Quantum Register ( 1 , 'q' ) , 0 ) new op = Gate ( name = "" , num qubits = 1 , params = [ ] ) if right name == "u1" : new op = U1Gate ( right parameters [ 2 ] ) if right name == "u2" : new op = U2Gate ( right parameters [ 1 ] , right parameters [ 2 ] ) if right name == "u3" : new op = U3Gate ( * right parameters ) if right name != 'nop' : new dag = DAG Circuit ( ) new dag . add qreg ( run qarg [ 0 ] ) new dag . apply operation back ( new op , [ run qarg ] , [ ] ) dag . substitute node with dag ( run [ 0 ] , new dag ) # Delete the other nodes in the run for current node in run [ 1 : ] : dag . remove op node ( current node ) if right name == "nop" : dag . remove op node ( run [ 0 ] ) return dag
def trim ( image ) : background = PIL . Image . new ( image . mode , image . size , image . getpixel ( ( 0 , 0 ) ) ) diff = PIL . Image Chops . difference ( image , background ) diff = PIL . Image Chops . add ( diff , diff , 2.0 , - 100 ) bbox = diff . getbbox ( ) if bbox : image = image . crop ( bbox ) return image
def get gate span ( qregs , instruction ) : min index = len ( qregs ) max index = 0 for qreg in instruction . qargs : index = qregs . index ( qreg ) if index < min index : min index = index if index > max index : max index = index if instruction . cargs : return qregs [ min index : ] return qregs [ min index : max index + 1 ]
def is cptp ( self , atol = None , rtol = None ) : if self . data [ 1 ] is not None : return False if atol is None : atol = self . atol if rtol is None : rtol = self . rtol accum = 0j for op in self . data [ 0 ] : accum += np . dot ( np . transpose ( np . conj ( op ) ) , op ) return is identity matrix ( accum , rtol = rtol , atol = atol )
def conjugate ( self ) : kraus l , kraus r = self . data kraus l = [ k . conj ( ) for k in kraus l ] if kraus r is not None : kraus r = [ k . conj ( ) for k in kraus r ] return Kraus ( ( kraus l , kraus r ) , self . input dims ( ) , self . output dims ( ) )
def transpose ( self ) : kraus l , kraus r = self . data kraus l = [ k . T for k in kraus l ] if kraus r is not None : kraus r = [ k . T for k in kraus r ] return Kraus ( ( kraus l , kraus r ) , input dims = self . output dims ( ) , output dims = self . input dims ( ) )
def real ( self , nested scope = None ) : operation = self . children [ 0 ] . operation ( ) lhs = self . children [ 1 ] . real ( nested scope ) rhs = self . children [ 2 ] . real ( nested scope ) return operation ( lhs , rhs )
def sym ( self , nested scope = None ) : operation = self . children [ 0 ] . operation ( ) lhs = self . children [ 1 ] . sym ( nested scope ) rhs = self . children [ 2 ] . sym ( nested scope ) return operation ( lhs , rhs )
def process custom unitary ( self , node ) : name = node . name if node . arguments is not None : args = self . process node ( node . arguments ) else : args = [ ] bits = [ self . process bit id ( node element ) for node element in node . bitlist . children ] if name in self . gates : gargs = self . gates [ name ] [ "args" ] gbits = self . gates [ name ] [ "bits" ] # Loop over register arguments, if any. maxidx = max ( map ( len , bits ) ) for idx in range ( maxidx ) : self . arg stack . append ( { gargs [ j ] : args [ j ] for j in range ( len ( gargs ) ) } ) # Only index into register arguments. element = [ idx * x for x in [ len ( bits [ j ] ) > 1 for j in range ( len ( bits ) ) ] ] self . bit stack . append ( { gbits [ j ] : bits [ j ] [ element [ j ] ] for j in range ( len ( gbits ) ) } ) self . create dag op ( name , [ self . arg stack [ - 1 ] [ s ] . sym ( ) for s in gargs ] , [ self . bit stack [ - 1 ] [ s ] for s in gbits ] ) self . arg stack . pop ( ) self . bit stack . pop ( ) else : raise Qiskit Error ( "internal error undefined gate:" , "line=%s" % node . line , "file=%s" % node . file )
def process cnot ( self , node ) : id0 = self . process bit id ( node . children [ 0 ] ) id1 = self . process bit id ( node . children [ 1 ] ) if not ( len ( id0 ) == len ( id1 ) or len ( id0 ) == 1 or len ( id1 ) == 1 ) : raise Qiskit Error ( "internal error: qreg size mismatch" , "line=%s" % node . line , "file=%s" % node . file ) maxidx = max ( [ len ( id0 ) , len ( id1 ) ] ) for idx in range ( maxidx ) : if len ( id0 ) > 1 and len ( id1 ) > 1 : self . dag . apply operation back ( CX Base ( ) , [ id0 [ idx ] , id1 [ idx ] ] , [ ] , self . condition ) elif len ( id0 ) > 1 : self . dag . apply operation back ( CX Base ( ) , [ id0 [ idx ] , id1 [ 0 ] ] , [ ] , self . condition ) else : self . dag . apply operation back ( CX Base ( ) , [ id0 [ 0 ] , id1 [ idx ] ] , [ ] , self . condition )
def process measure ( self , node ) : id0 = self . process bit id ( node . children [ 0 ] ) id1 = self . process bit id ( node . children [ 1 ] ) if len ( id0 ) != len ( id1 ) : raise Qiskit Error ( "internal error: reg size mismatch" , "line=%s" % node . line , "file=%s" % node . file ) for idx , idy in zip ( id0 , id1 ) : self . dag . apply operation back ( Measure ( ) , [ idx ] , [ idy ] , self . condition )
def process if ( self , node ) : creg name = node . children [ 0 ] . name creg = self . dag . cregs [ creg name ] cval = node . children [ 1 ] . value self . condition = ( creg , cval ) self . process node ( node . children [ 2 ] ) self . condition = None
def process node ( self , node ) : if node . type == "program" : self . process children ( node ) elif node . type == "qreg" : qreg = Quantum Register ( node . index , node . name ) self . dag . add qreg ( qreg ) elif node . type == "creg" : creg = Classical Register ( node . index , node . name ) self . dag . add creg ( creg ) elif node . type == "id" : raise Qiskit Error ( "internal error:  process node on id" ) elif node . type == "int" : raise Qiskit Error ( "internal error:  process node on int" ) elif node . type == "real" : raise Qiskit Error ( "internal error:  process node on real" ) elif node . type == "indexed id" : raise Qiskit Error ( "internal error:  process node on indexed id" ) elif node . type == "id list" : # We process id list nodes when they are leaves of barriers. return [ self . process bit id ( node children ) for node children in node . children ] elif node . type == "primary list" : # We should only be called for a barrier. return [ self . process bit id ( m ) for m in node . children ] elif node . type == "gate" : self . process gate ( node ) elif node . type == "custom unitary" : self . process custom unitary ( node ) elif node . type == "universal unitary" : args = self . process node ( node . children [ 0 ] ) qid = self . process bit id ( node . children [ 1 ] ) for element in qid : self . dag . apply operation back ( U Base ( * args , element ) , self . condition ) elif node . type == "cnot" : self . process cnot ( node ) elif node . type == "expression list" : return node . children elif node . type == "binop" : raise Qiskit Error ( "internal error:  process node on binop" ) elif node . type == "prefix" : raise Qiskit Error ( "internal error:  process node on prefix" ) elif node . type == "measure" : self . process measure ( node ) elif node . type == "format" : self . version = node . version ( ) elif node . type == "barrier" : ids = self . process node ( node . children [ 0 ] ) qubits = [ ] for qubit in ids : for j , in enumerate ( qubit ) : qubits . append ( qubit [ j ] ) self . dag . apply operation back ( Barrier ( len ( qubits ) ) , qubits , [ ] ) elif node . type == "reset" : id0 = self . process bit id ( node . children [ 0 ] ) for i , in enumerate ( id0 ) : self . dag . apply operation back ( Reset ( ) , [ id0 [ i ] ] , [ ] , self . condition ) elif node . type == "if" : self . process if ( node ) elif node . type == "opaque" : self . process gate ( node , opaque = True ) elif node . type == "external" : raise Qiskit Error ( "internal error:  process node on external" ) else : raise Qiskit Error ( "internal error: undefined node type" , node . type , "line=%s" % node . line , "file=%s" % node . file ) return None
def qasm ( self , prec = 15 ) : return "measure " + self . children [ 0 ] . qasm ( prec ) + " -> " + self . children [ 1 ] . qasm ( prec ) + ";"
def to string ( self , indent ) : ind = indent * ' ' print ( ind , 'indexed id' , self . name , self . index )
def validate ( instance ) : try : = instance . schema . validate ( instance . to dict ( ) ) except Validation Error as ex : raise Model Validation Error ( ex . messages , ex . field names , ex . fields , ex . data , * * ex . kwargs )
def validate after init ( init method ) : @ wraps ( init method ) def decorated ( self , * * kwargs ) : try : = self . shallow schema . validate ( kwargs ) except Validation Error as ex : raise Model Validation Error ( ex . messages , ex . field names , ex . fields , ex . data , * * ex . kwargs ) from None init method ( self , * * kwargs ) return decorated
def qft ( circ , q , n ) : for j in range ( n ) : for k in range ( j ) : circ . cu1 ( math . pi / float ( 2 ** ( j - k ) ) , q [ j ] , q [ k ] ) circ . h ( q [ j ] )
def random unitary matrix ( dim , seed = None ) : warnings . warn ( 'The random unitary matrix() function in qiskit.tools.qi has been ' 'deprecated and will be removed in the future. Instead use ' 'the function in qiskit.quantum info.random' , Deprecation Warning ) return random . random unitary ( dim , seed ) . data
def random density matrix ( length , rank = None , method = 'Hilbert-Schmidt' , seed = None ) : warnings . warn ( 'The random density matrix() function in qiskit.tools.qi has been ' 'deprecated and will be removed in the future. Instead use ' 'the function in qiskit.quantum info.random' , Deprecation Warning ) return random . random density matrix ( length , rank , method , seed )
def u3 ( self , theta , phi , lam , q ) : return self . append ( U3Gate ( theta , phi , lam ) , [ q ] , [ ] )
def check type ( self , value , attr , data ) : # Check the type in the standard way first, in order to fail quickly # in case of invalid values. root value = super ( Instruction Parameter , self ) . check type ( value , attr , data ) if is collection ( value ) : = [ super ( Instruction Parameter , self ) . check type ( item , attr , data ) for item in value ] return root value
def check range ( self , j ) : if isinstance ( j , int ) : if j < 0 or j >= self . size : raise Qiskit Index Error ( "register index out of range" ) elif isinstance ( j , slice ) : if j . start < 0 or j . stop >= self . size or ( j . step is not None and j . step <= 0 ) : raise Qiskit Index Error ( "register index slice out of range" )
def to string ( self , indent ) : ind = indent * ' ' if self . root : print ( ind , self . type , '---' , self . root ) else : print ( ind , self . type ) indent = indent + 3 ind = indent * ' ' for children in self . children : if children is None : print ( "OOPS! type of parent is" , type ( self ) ) print ( self . children ) if isinstance ( children , str ) : print ( ind , children ) elif isinstance ( children , int ) : print ( ind , str ( children ) ) elif isinstance ( children , float ) : print ( ind , str ( children ) ) else : children . to string ( indent )
def is square matrix ( mat ) : mat = np . array ( mat ) if mat . ndim != 2 : return False shape = mat . shape return shape [ 0 ] == shape [ 1 ]
def is diagonal matrix ( mat , rtol = RTOL DEFAULT , atol = ATOL DEFAULT ) : if atol is None : atol = ATOL DEFAULT if rtol is None : rtol = RTOL DEFAULT mat = np . array ( mat ) if mat . ndim != 2 : return False return np . allclose ( mat , np . diag ( np . diagonal ( mat ) ) , rtol = rtol , atol = atol )
def is symmetric matrix ( op , rtol = RTOL DEFAULT , atol = ATOL DEFAULT ) : if atol is None : atol = ATOL DEFAULT if rtol is None : rtol = RTOL DEFAULT mat = np . array ( op ) if mat . ndim != 2 : return False return np . allclose ( mat , mat . T , rtol = rtol , atol = atol )
def is hermitian matrix ( mat , rtol = RTOL DEFAULT , atol = ATOL DEFAULT ) : if atol is None : atol = ATOL DEFAULT if rtol is None : rtol = RTOL DEFAULT mat = np . array ( mat ) if mat . ndim != 2 : return False return np . allclose ( mat , np . conj ( mat . T ) , rtol = rtol , atol = atol )
def is positive semidefinite matrix ( mat , rtol = RTOL DEFAULT , atol = ATOL DEFAULT ) : if atol is None : atol = ATOL DEFAULT if rtol is None : rtol = RTOL DEFAULT if not is hermitian matrix ( mat , rtol = rtol , atol = atol ) : return False # Check eigenvalues are all positive vals = np . linalg . eigvalsh ( mat ) for v in vals : if v < - atol : return False return True
def is identity matrix ( mat , ignore phase = False , rtol = RTOL DEFAULT , atol = ATOL DEFAULT ) : if atol is None : atol = ATOL DEFAULT if rtol is None : rtol = RTOL DEFAULT mat = np . array ( mat ) if mat . ndim != 2 : return False if ignore phase : # If the matrix is equal to an identity up to a phase, we can # remove the phase by multiplying each entry by the complex # conjugate of the phase of the [0, 0] entry. theta = np . angle ( mat [ 0 , 0 ] ) mat = np . exp ( - 1j * theta ) * mat # Check if square identity iden = np . eye ( len ( mat ) ) return np . allclose ( mat , iden , rtol = rtol , atol = atol )
def is unitary matrix ( mat , rtol = RTOL DEFAULT , atol = ATOL DEFAULT ) : if atol is None : atol = ATOL DEFAULT if rtol is None : rtol = RTOL DEFAULT mat = np . array ( mat ) # Compute A^dagger.A and see if it is identity matrix mat = np . conj ( mat . T ) . dot ( mat ) return is identity matrix ( mat , ignore phase = False , rtol = rtol , atol = atol )
def run ( self , dag ) : swaps = dag . op nodes ( Swap Gate ) for swap in swaps : final successor = [ ] for successor in dag . successors ( swap ) : final successor . append ( successor . type == 'out' or ( successor . type == 'op' and successor . op . name == 'measure' ) ) if all ( final successor ) : # the node swap needs to be removed and, if a measure follows, needs to be adapted swap qargs = swap . qargs measure layer = DAG Circuit ( ) for qreg in dag . qregs . values ( ) : measure layer . add qreg ( qreg ) for creg in dag . cregs . values ( ) : measure layer . add creg ( creg ) for successor in dag . successors ( swap ) : if successor . type == 'op' and successor . op . name == 'measure' : # replace measure node with a new one, where qargs is set with the "other" # swap qarg. dag . remove op node ( successor ) old measure qarg = successor . qargs [ 0 ] new measure qarg = swap qargs [ swap qargs . index ( old measure qarg ) - 1 ] measure layer . apply operation back ( Measure ( ) , [ new measure qarg ] , [ successor . cargs [ 0 ] ] ) dag . extend back ( measure layer ) dag . remove op node ( swap ) return dag
def to choi ( rep , data , input dim , output dim ) : if rep == 'Choi' : return data if rep == 'Operator' : return from operator ( 'Choi' , data , input dim , output dim ) if rep == 'Super Op' : return superop to choi ( data , input dim , output dim ) if rep == 'Kraus' : return kraus to choi ( data , input dim , output dim ) if rep == 'Chi' : return chi to choi ( data , input dim , output dim ) if rep == 'PTM' : data = ptm to superop ( data , input dim , output dim ) return superop to choi ( data , input dim , output dim ) if rep == 'Stinespring' : return stinespring to choi ( data , input dim , output dim ) raise Qiskit Error ( 'Invalid Quantum Channel {}' . format ( rep ) )
def to superop ( rep , data , input dim , output dim ) : if rep == 'Super Op' : return data if rep == 'Operator' : return from operator ( 'Super Op' , data , input dim , output dim ) if rep == 'Choi' : return choi to superop ( data , input dim , output dim ) if rep == 'Kraus' : return kraus to superop ( data , input dim , output dim ) if rep == 'Chi' : data = chi to choi ( data , input dim , output dim ) return choi to superop ( data , input dim , output dim ) if rep == 'PTM' : return ptm to superop ( data , input dim , output dim ) if rep == 'Stinespring' : return stinespring to superop ( data , input dim , output dim ) raise Qiskit Error ( 'Invalid Quantum Channel {}' . format ( rep ) )
def to kraus ( rep , data , input dim , output dim ) : if rep == 'Kraus' : return data if rep == 'Stinespring' : return stinespring to kraus ( data , input dim , output dim ) if rep == 'Operator' : return from operator ( 'Kraus' , data , input dim , output dim ) # Convert via Choi and Kraus if rep != 'Choi' : data = to choi ( rep , data , input dim , output dim ) return choi to kraus ( data , input dim , output dim )
def to chi ( rep , data , input dim , output dim ) : if rep == 'Chi' : return data # Check valid n-qubit input check nqubit dim ( input dim , output dim ) if rep == 'Operator' : return from operator ( 'Chi' , data , input dim , output dim ) # Convert via Choi representation if rep != 'Choi' : data = to choi ( rep , data , input dim , output dim ) return choi to chi ( data , input dim , output dim )
def to ptm ( rep , data , input dim , output dim ) : if rep == 'PTM' : return data # Check valid n-qubit input check nqubit dim ( input dim , output dim ) if rep == 'Operator' : return from operator ( 'PTM' , data , input dim , output dim ) # Convert via Superoperator representation if rep != 'Super Op' : data = to superop ( rep , data , input dim , output dim ) return superop to ptm ( data , input dim , output dim )
def to stinespring ( rep , data , input dim , output dim ) : if rep == 'Stinespring' : return data if rep == 'Operator' : return from operator ( 'Stinespring' , data , input dim , output dim ) # Convert via Superoperator representation if rep != 'Kraus' : data = to kraus ( rep , data , input dim , output dim ) return kraus to stinespring ( data , input dim , output dim )
def to operator ( rep , data , input dim , output dim ) : if rep == 'Operator' : return data if rep == 'Stinespring' : return stinespring to operator ( data , input dim , output dim ) # Convert via Kraus representation if rep != 'Kraus' : data = to kraus ( rep , data , input dim , output dim ) return kraus to operator ( data , input dim , output dim )
def from operator ( rep , data , input dim , output dim ) : if rep == 'Operator' : return data if rep == 'Super Op' : return np . kron ( np . conj ( data ) , data ) if rep == 'Choi' : vec = np . ravel ( data , order = 'F' ) return np . outer ( vec , np . conj ( vec ) ) if rep == 'Kraus' : return ( [ data ] , None ) if rep == 'Stinespring' : return ( data , None ) if rep == 'Chi' : check nqubit dim ( input dim , output dim ) data = from operator ( 'Choi' , data , input dim , output dim ) return choi to chi ( data , input dim , output dim ) if rep == 'PTM' : check nqubit dim ( input dim , output dim ) data = from operator ( 'Super Op' , data , input dim , output dim ) return superop to ptm ( data , input dim , output dim ) raise Qiskit Error ( 'Invalid Quantum Channel {}' . format ( rep ) )
def stinespring to operator ( data , input dim , output dim ) : trace dim = data [ 0 ] . shape [ 0 ] // output dim if data [ 1 ] is not None or trace dim != 1 : raise Qiskit Error ( 'Channel cannot be converted to Operator representation' ) return data [ 0 ]
def superop to choi ( data , input dim , output dim ) : shape = ( output dim , output dim , input dim , input dim ) return reshuffle ( data , shape )
def choi to superop ( data , input dim , output dim ) : shape = ( input dim , output dim , input dim , output dim ) return reshuffle ( data , shape )
def kraus to choi ( data , input dim , output dim ) : choi = 0 kraus l , kraus r = data if kraus r is None : for i in kraus l : vec = i . ravel ( order = 'F' ) choi += np . outer ( vec , vec . conj ( ) ) else : for i , j in zip ( kraus l , kraus r ) : choi += np . outer ( i . ravel ( order = 'F' ) , j . ravel ( order = 'F' ) . conj ( ) ) return choi
def choi to kraus ( data , input dim , output dim , atol = ATOL DEFAULT ) : # Check if hermitian matrix if is hermitian matrix ( data , atol = atol ) : # Get eigen-decomposition of Choi-matrix w , v = la . eigh ( data ) # Check eigenvaleus are non-negative if len ( w [ w < - atol ] ) == 0 : # CP-map Kraus representation kraus = [ ] for val , vec in zip ( w , v . T ) : if abs ( val ) > atol : k = np . sqrt ( val ) * vec . reshape ( ( output dim , input dim ) , order = 'F' ) kraus . append ( k ) # If we are converting a zero matrix, we need to return a Kraus set # with a single zero-element Kraus matrix if not kraus : kraus . append ( np . zeros ( ( output dim , input dim ) , dtype = complex ) ) return ( kraus , None ) # Non-CP-map generalized Kraus representation mat u , svals , mat vh = la . svd ( data ) kraus l = [ ] kraus r = [ ] for val , vec l , vec r in zip ( svals , mat u . T , mat vh . conj ( ) ) : kraus l . append ( np . sqrt ( val ) * vec l . reshape ( ( output dim , input dim ) , order = 'F' ) ) kraus r . append ( np . sqrt ( val ) * vec r . reshape ( ( output dim , input dim ) , order = 'F' ) ) return ( kraus l , kraus r )
def stinespring to kraus ( data , input dim , output dim ) : kraus pair = [ ] for stine in data : if stine is None : kraus pair . append ( None ) else : trace dim = stine . shape [ 0 ] // output dim iden = np . eye ( output dim ) kraus = [ ] for j in range ( trace dim ) : vec = np . zeros ( trace dim ) vec [ j ] = 1 kraus . append ( np . kron ( iden , vec [ None , : ] ) . dot ( stine ) ) kraus pair . append ( kraus ) return tuple ( kraus pair )
def stinespring to choi ( data , input dim , output dim ) : trace dim = data [ 0 ] . shape [ 0 ] // output dim stine l = np . reshape ( data [ 0 ] , ( output dim , trace dim , input dim ) ) if data [ 1 ] is None : stine r = stine l else : stine r = np . reshape ( data [ 1 ] , ( output dim , trace dim , input dim ) ) return np . reshape ( np . einsum ( 'i Aj,k Al->jilk' , stine l , stine r . conj ( ) ) , 2 * [ input dim * output dim ] )
def kraus to stinespring ( data , input dim , output dim ) : stine pair = [ None , None ] for i , kraus in enumerate ( data ) : if kraus is not None : num kraus = len ( kraus ) stine = np . zeros ( ( output dim * num kraus , input dim ) , dtype = complex ) for j , mat in enumerate ( kraus ) : vec = np . zeros ( num kraus ) vec [ j ] = 1 stine += np . kron ( mat , vec [ : , None ] ) stine pair [ i ] = stine return tuple ( stine pair )
def kraus to superop ( data , input dim , output dim ) : kraus l , kraus r = data superop = 0 if kraus r is None : for i in kraus l : superop += np . kron ( np . conj ( i ) , i ) else : for i , j in zip ( kraus l , kraus r ) : superop += np . kron ( np . conj ( j ) , i ) return superop
def chi to choi ( data , input dim , output dim ) : num qubits = int ( np . log2 ( input dim ) ) return transform from pauli ( data , num qubits )
def choi to chi ( data , input dim , output dim ) : num qubits = int ( np . log2 ( input dim ) ) return transform to pauli ( data , num qubits )
def reravel ( mat1 , mat2 , shape1 , shape2 ) : # Reshuffle indicies left dims = shape1 [ : 2 ] + shape2 [ : 2 ] right dims = shape1 [ 2 : ] + shape2 [ 2 : ] tensor shape = left dims + right dims final shape = ( np . product ( left dims ) , np . product ( right dims ) ) # Tensor product matrices data = np . kron ( mat1 , mat2 ) data = np . reshape ( np . transpose ( np . reshape ( data , tensor shape ) , ( 0 , 2 , 1 , 3 , 4 , 6 , 5 , 7 ) ) , final shape ) return data
def transform from pauli ( data , num qubits ) : # Change basis: sum {i=0}^3 =|\sigma i>><i| basis mat = np . array ( [ [ 1 , 0 , 0 , 1 ] , [ 0 , 1 , 1j , 0 ] , [ 0 , 1 , - 1j , 0 ] , [ 1 , 0j , 0 , - 1 ] ] , dtype = complex ) # Note that we manually renormalized after change of basis # to avoid rounding errors from square-roots of 2. cob = basis mat for in range ( num qubits - 1 ) : dim = int ( np . sqrt ( len ( cob ) ) ) cob = np . reshape ( np . transpose ( np . reshape ( np . kron ( basis mat , cob ) , ( 2 , 2 , dim , dim , 4 , dim * dim ) ) , ( 0 , 2 , 1 , 3 , 4 , 5 ) ) , ( 4 * dim * dim , 4 * dim * dim ) ) return np . dot ( np . dot ( cob , data ) , cob . conj ( ) . T ) / 2 ** num qubits
def check nqubit dim ( input dim , output dim ) : if input dim != output dim : raise Qiskit Error ( 'Not an n-qubit channel: input dim' + ' ({}) != output dim ({})' . format ( input dim , output dim ) ) num qubits = int ( np . log2 ( input dim ) ) if 2 ** num qubits != input dim : raise Qiskit Error ( 'Not an n-qubit channel: input dim != 2 ** n' )
def hide tick lines and labels ( axis ) : for item in axis . get ticklines ( ) + axis . get ticklabels ( ) : item . set visible ( False )
def clear ( self ) : self . points = [ ] self . vectors = [ ] self . point style = [ ] self . annotations = [ ]
def render ( self , title = '' ) : if self . rendered : self . axes . clear ( ) self . rendered = True # Figure instance for Bloch sphere plot if not self . ext fig : self . fig = plt . figure ( figsize = self . figsize ) if not self . ext axes : self . axes = Axes3D ( self . fig , azim = self . view [ 0 ] , elev = self . view [ 1 ] ) if self . background : self . axes . clear ( ) self . axes . set xlim3d ( - 1.3 , 1.3 ) self . axes . set ylim3d ( - 1.3 , 1.3 ) self . axes . set zlim3d ( - 1.3 , 1.3 ) else : self . plot axes ( ) self . axes . set axis off ( ) self . axes . set xlim3d ( - 0.7 , 0.7 ) self . axes . set ylim3d ( - 0.7 , 0.7 ) self . axes . set zlim3d ( - 0.7 , 0.7 ) self . axes . grid ( False ) self . plot back ( ) self . plot points ( ) self . plot vectors ( ) self . plot front ( ) self . plot axes labels ( ) self . plot annotations ( ) self . axes . set title ( title , fontsize = self . font size , y = 1.08 )
def plot front ( self ) : u angle = np . linspace ( - np . pi , 0 , 25 ) v angle = np . linspace ( 0 , np . pi , 25 ) x dir = np . outer ( np . cos ( u angle ) , np . sin ( v angle ) ) y dir = np . outer ( np . sin ( u angle ) , np . sin ( v angle ) ) z dir = np . outer ( np . ones ( u angle . shape [ 0 ] ) , np . cos ( v angle ) ) self . axes . plot surface ( x dir , y dir , z dir , rstride = 2 , cstride = 2 , color = self . sphere color , linewidth = 0 , alpha = self . sphere alpha ) # wireframe self . axes . plot wireframe ( x dir , y dir , z dir , rstride = 5 , cstride = 5 , color = self . frame color , alpha = self . frame alpha ) # equator self . axes . plot ( 1.0 * np . cos ( u angle ) , 1.0 * np . sin ( u angle ) , zs = 0 , zdir = 'z' , lw = self . frame width , color = self . frame color ) self . axes . plot ( 1.0 * np . cos ( u angle ) , 1.0 * np . sin ( u angle ) , zs = 0 , zdir = 'x' , lw = self . frame width , color = self . frame color )
def show ( self , title = '' ) : self . render ( title = title ) if self . fig : plt . show ( self . fig )
def two qubit kak ( unitary matrix , verify gate sequence = False ) : warnings . warn ( "two qubit kak function is now accessible under " "qiskit.quantum info.synthesis" , Deprecation Warning ) return synthesis . two qubit kak ( unitary matrix )
def top ( self ) : ret = self . top format % self . top connect . center ( self . width , self . top pad ) if self . right fill : ret = ret . ljust ( self . right fill , self . top pad ) if self . left fill : ret = ret . rjust ( self . left fill , self . top pad ) ret = ret . center ( self . layer width , self . top bck ) return ret
def mid ( self ) : ret = self . mid format % self . mid content . center ( self . width , self . mid padding ) if self . right fill : ret = ret . ljust ( self . right fill , self . mid padding ) if self . left fill : ret = ret . rjust ( self . left fill , self . mid padding ) ret = ret . center ( self . layer width , self . mid bck ) return ret
def bot ( self ) : ret = self . bot format % self . bot connect . center ( self . width , self . bot pad ) if self . right fill : ret = ret . ljust ( self . right fill , self . bot pad ) if self . left fill : ret = ret . rjust ( self . left fill , self . bot pad ) ret = ret . center ( self . layer width , self . bot bck ) return ret
def length ( self ) : return max ( len ( self . top ) , len ( self . mid ) , len ( self . bot ) )
def label for box ( instruction ) : label = instruction . name . capitalize ( ) params = Text Drawing . params for label ( instruction ) if params : label += "(%s)" % ',' . join ( params ) return label
def latex ( self , prec = 15 , nested scope = None ) : if not nested scope : return "\textrm{" + self . name + "}" else : if self . name not in nested scope [ - 1 ] : raise Node Exception ( "Expected local parameter name: " , "name=%s, " % self . name , "line=%s, " % self . line , "file=%s" % self . file ) else : return nested scope [ - 1 ] [ self . name ] . latex ( prec , nested scope [ 0 : - 1 ] )
def sym ( self , nested scope = None ) : if not nested scope or self . name not in nested scope [ - 1 ] : raise Node Exception ( "Expected local parameter name: " , "name=%s, line=%s, file=%s" % ( self . name , self . line , self . file ) ) else : return nested scope [ - 1 ] [ self . name ] . sym ( nested scope [ 0 : - 1 ] )
def real ( self , nested scope = None ) : if not nested scope or self . name not in nested scope [ - 1 ] : raise Node Exception ( "Expected local parameter name: " , "name=%s, line=%s, file=%s" % ( self . name , self . line , self . file ) ) else : return nested scope [ - 1 ] [ self . name ] . real ( nested scope [ 0 : - 1 ] )
def attach ( self , instruction , qargs , cargs ) : self . append ( instruction , qargs , cargs )
def check dups ( self , qubits ) : squbits = set ( qubits ) if len ( squbits ) != len ( qubits ) : raise Qiskit Error ( "duplicate qubit arguments" )
def check qargs ( self , qargs ) : if not all ( isinstance ( i , tuple ) and isinstance ( i [ 0 ] , Quantum Register ) and isinstance ( i [ 1 ] , int ) for i in qargs ) : raise Qiskit Error ( "qarg not (Quantum Register, int) tuple" ) if not all ( self . has register ( i [ 0 ] ) for i in qargs ) : raise Qiskit Error ( "register not in this circuit" ) for qubit in qargs : qubit [ 0 ] . check range ( qubit [ 1 ] )
def check cargs ( self , cargs ) : if not all ( isinstance ( i , tuple ) and isinstance ( i [ 0 ] , Classical Register ) and isinstance ( i [ 1 ] , int ) for i in cargs ) : raise Qiskit Error ( "carg not (Classical Register, int) tuple" ) if not all ( self . has register ( i [ 0 ] ) for i in cargs ) : raise Qiskit Error ( "register not in this circuit" ) for clbit in cargs : clbit [ 0 ] . check range ( clbit [ 1 ] )
def check compatible regs ( self , rhs ) : list1 = self . qregs + self . cregs list2 = rhs . qregs + rhs . cregs for element1 in list1 : for element2 in list2 : if element2 . name == element1 . name : if element1 != element2 : raise Qiskit Error ( "circuits are not compatible" )
def qasm ( self ) : string temp = self . header + "\n" string temp += self . extension lib + "\n" for register in self . qregs : string temp += register . qasm ( ) + "\n" for register in self . cregs : string temp += register . qasm ( ) + "\n" for instruction , qargs , cargs in self . data : if instruction . name == 'measure' : qubit = qargs [ 0 ] clbit = cargs [ 0 ] string temp += "%s %s[%d] -> %s[%d];\n" % ( instruction . qasm ( ) , qubit [ 0 ] . name , qubit [ 1 ] , clbit [ 0 ] . name , clbit [ 1 ] ) else : string temp += "%s %s;\n" % ( instruction . qasm ( ) , "," . join ( [ "%s[%d]" % ( j [ 0 ] . name , j [ 1 ] ) for j in qargs + cargs ] ) ) return string temp
def bind parameter ( self , parameter , value ) : for ( instr , param index ) in self . parameter table [ parameter ] : instr . params [ param index ] = value
def score step ( step ) : # Each added swap will add 3 ops to gates mapped, so subtract 3. return len ( [ g for g in step [ 'gates mapped' ] if len ( g . qargs ) == 2 ] ) - 3 * step [ 'swaps added' ]
def transform gate for layout ( gate , layout ) : mapped op node = deepcopy ( [ n for n in gate [ 'graph' ] . nodes ( ) if n . type == 'op' ] [ 0 ] ) # Workaround until #1816, apply mapped to qargs to both DAG Node and op device qreg = Quantum Register ( len ( layout . get physical bits ( ) ) , 'q' ) mapped qargs = [ ( device qreg , layout [ a ] ) for a in mapped op node . qargs ] mapped op node . qargs = mapped op node . op . qargs = mapped qargs mapped op node . pop ( 'name' ) return mapped op node
def swap ops from edge ( edge , layout ) : device qreg = Quantum Register ( len ( layout . get physical bits ( ) ) , 'q' ) qreg edge = [ ( device qreg , i ) for i in edge ] # TODO shouldn't be making other nodes not by the DAG!! return [ DAG Node ( { 'op' : Swap Gate ( ) , 'qargs' : qreg edge , 'cargs' : [ ] , 'type' : 'op' } ) ]
def physical qubits ( self ) : if self . qubit list is None : self . qubit list = sorted ( [ pqubit for pqubit in self . graph . nodes ] ) return self . qubit list
def cu1 ( self , theta , ctl , tgt ) : return self . append ( Cu1Gate ( theta ) , [ ctl , tgt ] , [ ] )
def inverse ( self ) : for index , instruction in enumerate ( self . instructions ) : self . instructions [ index ] = instruction . inverse ( ) return self
def q if ( self , * qregs ) : for gate in self . instructions : gate . q if ( * qregs ) return self
def c if ( self , classical , val ) : for gate in self . instructions : gate . c if ( classical , val ) return self
def initialize ( self , params , qubits ) : if isinstance ( qubits , Quantum Register ) : qubits = qubits [ : ] else : qubits = convert to bits ( [ qubits ] , [ qbit for qreg in self . qregs for qbit in qreg ] ) [ 0 ] return self . append ( Initialize ( params ) , qubits )
def is virtual ( value ) : return value is None or isinstance ( value , tuple ) and len ( value ) == 2 and isinstance ( value [ 0 ] , Register ) and isinstance ( value [ 1 ] , int )
def copy ( self ) : layout copy = type ( self ) ( ) layout copy . p2v = self . p2v . copy ( ) layout copy . v2p = self . v2p . copy ( ) return layout copy
def ccx ( self , ctl1 , ctl2 , tgt ) : return self . append ( Toffoli Gate ( ) , [ ctl1 , ctl2 , tgt ] , [ ] )
def u2 ( self , phi , lam , q ) : return self . append ( U2Gate ( phi , lam ) , [ q ] , [ ] )
def to matrix ( self ) : isqrt2 = 1 / numpy . sqrt ( 2 ) phi , lam = self . params phi , lam = float ( phi ) , float ( lam ) return numpy . array ( [ [ isqrt2 , - numpy . exp ( 1j * lam ) * isqrt2 ] , [ numpy . exp ( 1j * phi ) * isqrt2 , numpy . exp ( 1j * ( phi + lam ) ) * isqrt2 ] ] , dtype = complex )
def is cptp ( self , atol = None , rtol = None ) : if atol is None : atol = self . atol if rtol is None : rtol = self . rtol if self . data [ 1 ] is not None : return False check = np . dot ( np . transpose ( np . conj ( self . data [ 0 ] ) ) , self . data [ 0 ] ) return is identity matrix ( check , rtol = self . rtol , atol = self . atol )
def conjugate ( self ) : # pylint: disable=assignment-from-no-return stine l = np . conjugate ( self . data [ 0 ] ) stine r = None if self . data [ 1 ] is not None : stine r = np . conjugate ( self . data [ 1 ] ) return Stinespring ( ( stine l , stine r ) , self . input dims ( ) , self . output dims ( ) )
def transpose ( self ) : din , dout = self . dim dtr = self . data [ 0 ] . shape [ 0 ] // dout stine = [ None , None ] for i , mat in enumerate ( self . data ) : if mat is not None : stine [ i ] = np . reshape ( np . transpose ( np . reshape ( mat , ( dout , dtr , din ) ) , ( 2 , 1 , 0 ) ) , ( din * dtr , dout ) ) return Stinespring ( tuple ( stine ) , input dims = self . output dims ( ) , output dims = self . input dims ( ) )
def to operator ( self ) : # Place import here to avoid cyclic import from circuit visualization from qiskit . quantum info . operators . operator import Operator return Operator ( self . to matrix ( ) )
def to instruction ( self ) : from qiskit . circuit import Quantum Circuit , Quantum Register from qiskit . extensions . standard import Id Gate , X Gate , Y Gate , Z Gate gates = { 'I' : Id Gate ( ) , 'X' : X Gate ( ) , 'Y' : Y Gate ( ) , 'Z' : Z Gate ( ) } label = self . to label ( ) n qubits = self . numberofqubits qreg = Quantum Register ( n qubits ) circuit = Quantum Circuit ( qreg , name = 'Pauli:{}' . format ( label ) ) for i , pauli in enumerate ( reversed ( label ) ) : circuit . append ( gates [ pauli ] , [ qreg [ i ] ] ) return circuit . to instruction ( )
def validate initial statevector ( self ) : # If initial statevector isn't set we don't need to validate if self . initial statevector is None : return # Check statevector is correct length for number of qubits length = len ( self . initial statevector ) required dim = 2 ** self . number of qubits if length != required dim : raise Basic Aer Error ( 'initial statevector is incorrect length: ' + '{} != {}' . format ( length , required dim ) )
def set options ( self , qobj config = None , backend options = None ) : # Reset default options self . initial statevector = self . DEFAULT OPTIONS [ "initial statevector" ] self . chop threshold = self . DEFAULT OPTIONS [ "chop threshold" ] if backend options is None : backend options = { } # Check for custom initial statevector in backend options first, # then config second if 'initial statevector' in backend options : self . initial statevector = np . array ( backend options [ 'initial statevector' ] , dtype = complex ) elif hasattr ( qobj config , 'initial statevector' ) : self . initial statevector = np . array ( qobj config . initial statevector , dtype = complex ) if self . initial statevector is not None : # Check the initial statevector is normalized norm = np . linalg . norm ( self . initial statevector ) if round ( norm , 12 ) != 1 : raise Basic Aer Error ( 'initial statevector is not normalized: ' + 'norm {} != 1' . format ( norm ) ) # Check for custom chop threshold # Replace with custom options if 'chop threshold' in backend options : self . chop threshold = backend options [ 'chop threshold' ] elif hasattr ( qobj config , 'chop threshold' ) : self . chop threshold = qobj config . chop threshold
def initialize statevector ( self ) : if self . initial statevector is None : # Set to default state of all qubits in |0> self . statevector = np . zeros ( 2 ** self . number of qubits , dtype = complex ) self . statevector [ 0 ] = 1 else : self . statevector = self . initial statevector . copy ( ) # Reshape to rank-N tensor self . statevector = np . reshape ( self . statevector , self . number of qubits * [ 2 ] )
def get statevector ( self ) : vec = np . reshape ( self . statevector , 2 ** self . number of qubits ) # Expand complex numbers vec = np . stack ( [ vec . real , vec . imag ] , axis = 1 ) # Truncate small values vec [ abs ( vec ) < self . chop threshold ] = 0.0 return vec
def validate ( self , qobj ) : n qubits = qobj . config . n qubits max qubits = self . configuration ( ) . n qubits if n qubits > max qubits : raise Basic Aer Error ( 'Number of qubits {} ' . format ( n qubits ) + 'is greater than maximum ({}) ' . format ( max qubits ) + 'for "{}".' . format ( self . name ( ) ) ) for experiment in qobj . experiments : name = experiment . header . name if experiment . config . memory slots == 0 : logger . warning ( 'No classical registers in circuit "%s", ' 'counts will be empty.' , name ) elif 'measure' not in [ op . name for op in experiment . instructions ] : logger . warning ( 'No measurements in circuit "%s", ' 'classical register will remain all zeros.' , name )
def validate initial unitary ( self ) : # If initial unitary isn't set we don't need to validate if self . initial unitary is None : return # Check unitary is correct length for number of qubits shape = np . shape ( self . initial unitary ) required shape = ( 2 ** self . number of qubits , 2 ** self . number of qubits ) if shape != required shape : raise Basic Aer Error ( 'initial unitary is incorrect shape: ' + '{} != 2 ** {}' . format ( shape , required shape ) )
def set options ( self , qobj config = None , backend options = None ) : # Reset default options self . initial unitary = self . DEFAULT OPTIONS [ "initial unitary" ] self . chop threshold = self . DEFAULT OPTIONS [ "chop threshold" ] if backend options is None : backend options = { } # Check for custom initial statevector in backend options first, # then config second if 'initial unitary' in backend options : self . initial unitary = np . array ( backend options [ 'initial unitary' ] , dtype = complex ) elif hasattr ( qobj config , 'initial unitary' ) : self . initial unitary = np . array ( qobj config . initial unitary , dtype = complex ) if self . initial unitary is not None : # Check the initial unitary is actually unitary shape = np . shape ( self . initial unitary ) if len ( shape ) != 2 or shape [ 0 ] != shape [ 1 ] : raise Basic Aer Error ( "initial unitary is not a square matrix" ) iden = np . eye ( len ( self . initial unitary ) ) u dagger u = np . dot ( self . initial unitary . T . conj ( ) , self . initial unitary ) norm = np . linalg . norm ( u dagger u - iden ) if round ( norm , 10 ) != 0 : raise Basic Aer Error ( "initial unitary is not unitary" ) # Check the initial statevector is normalized # Check for custom chop threshold # Replace with custom options if 'chop threshold' in backend options : self . chop threshold = backend options [ 'chop threshold' ] elif hasattr ( qobj config , 'chop threshold' ) : self . chop threshold = qobj config . chop threshold
def initialize unitary ( self ) : self . validate initial unitary ( ) if self . initial unitary is None : # Set to identity matrix self . unitary = np . eye ( 2 ** self . number of qubits , dtype = complex ) else : self . unitary = self . initial unitary . copy ( ) # Reshape to rank-N tensor self . unitary = np . reshape ( self . unitary , self . number of qubits * [ 2 , 2 ] )
def get unitary ( self ) : unitary = np . reshape ( self . unitary , 2 * [ 2 ** self . number of qubits ] ) # Expand complex numbers unitary = np . stack ( ( unitary . real , unitary . imag ) , axis = - 1 ) # Truncate small values unitary [ abs ( unitary ) < self . chop threshold ] = 0.0 return unitary
def is bit ( obj ) : # If there is a bit type this could be replaced by isinstance. if isinstance ( obj , tuple ) and len ( obj ) == 2 : if isinstance ( obj [ 0 ] , Register ) and isinstance ( obj [ 1 ] , int ) and obj [ 1 ] < len ( obj [ 0 ] ) : return True return False
def to matrix ( self ) : lam = self . params [ 0 ] lam = float ( lam ) return numpy . array ( [ [ 1 , 0 ] , [ 0 , numpy . exp ( 1j * lam ) ] ] , dtype = complex )
def real ( self , nested scope = None ) : op = self . children [ 0 ] . name expr = self . children [ 1 ] dispatch = { 'sin' : sympy . sin , 'cos' : sympy . cos , 'tan' : sympy . tan , 'asin' : sympy . asin , 'acos' : sympy . acos , 'atan' : sympy . atan , 'exp' : sympy . exp , 'ln' : sympy . log , 'sqrt' : sympy . sqrt } if op in dispatch : arg = expr . real ( nested scope ) return dispatch [ op ] ( arg ) else : raise Node Exception ( "internal error: undefined external" )
def rzz ( self , theta , qubit1 , qubit2 ) : return self . append ( RZZ Gate ( theta ) , [ qubit1 , qubit2 ] , [ ] )
def cswap ( self , ctl , tgt1 , tgt2 ) : return self . append ( Fredkin Gate ( ) , [ ctl , tgt1 , tgt2 ] , [ ] )
def initialize backend prop ( self ) : backend prop = self . backend prop for ginfo in backend prop . gates : if ginfo . gate == 'cx' : for item in ginfo . parameters : if item . name == 'gate error' : g reliab = 1.0 - item . value break else : g reliab = 1.0 swap reliab = - math . log ( pow ( g reliab , 3 ) ) self . swap graph . add edge ( ginfo . qubits [ 0 ] , ginfo . qubits [ 1 ] , weight = swap reliab ) self . swap graph . add edge ( ginfo . qubits [ 1 ] , ginfo . qubits [ 0 ] , weight = swap reliab ) self . cx errors [ ( ginfo . qubits [ 0 ] , ginfo . qubits [ 1 ] ) ] = g reliab self . gate list . append ( ( ginfo . qubits [ 0 ] , ginfo . qubits [ 1 ] ) ) idx = 0 for q in backend prop . qubits : for nduv in q : if nduv . name == 'readout error' : self . readout errors [ idx ] = 1.0 - nduv . value self . available hw qubits . append ( idx ) idx += 1 for edge in self . cx errors : self . gate cost [ edge ] = self . cx errors [ edge ] * self . readout errors [ edge [ 0 ] ] * self . readout errors [ edge [ 1 ] ] self . swap paths , swap costs temp = nx . algorithms . shortest paths . dense . floyd warshall predecessor and distance ( self . swap graph , weight = 'weight' ) for i in swap costs temp : self . swap costs [ i ] = { } for j in swap costs temp [ i ] : if ( i , j ) in self . cx errors : self . swap costs [ i ] [ j ] = self . cx errors [ ( i , j ) ] elif ( j , i ) in self . cx errors : self . swap costs [ i ] [ j ] = self . cx errors [ ( j , i ) ] else : best reliab = 0.0 for n in self . swap graph . neighbors ( j ) : if ( n , j ) in self . cx errors : reliab = math . exp ( - swap costs temp [ i ] [ n ] ) * self . cx errors [ ( n , j ) ] else : reliab = math . exp ( - swap costs temp [ i ] [ n ] ) * self . cx errors [ ( j , n ) ] if reliab > best reliab : best reliab = reliab self . swap costs [ i ] [ j ] = best reliab
def select best remaining cx ( self ) : candidates = [ ] for gate in self . gate list : chk1 = gate [ 0 ] in self . available hw qubits chk2 = gate [ 1 ] in self . available hw qubits if chk1 and chk2 : candidates . append ( gate ) best reliab = 0 best item = None for item in candidates : if self . gate cost [ item ] > best reliab : best reliab = self . gate cost [ item ] best item = item return best item
def select best remaining qubit ( self , prog qubit ) : reliab store = { } for hw qubit in self . available hw qubits : reliab = 1 for n in self . prog graph . neighbors ( prog qubit ) : if n in self . prog2hw : reliab *= self . swap costs [ self . prog2hw [ n ] ] [ hw qubit ] reliab *= self . readout errors [ hw qubit ] reliab store [ hw qubit ] = reliab max reliab = 0 best hw qubit = None for hw qubit in reliab store : if reliab store [ hw qubit ] > max reliab : max reliab = reliab store [ hw qubit ] best hw qubit = hw qubit return best hw qubit
def run ( self , dag ) : self . initialize backend prop ( ) num qubits = self . create program graph ( dag ) if num qubits > len ( self . swap graph ) : raise Transpiler Error ( 'Number of qubits greater than device.' ) for end1 , end2 , in sorted ( self . prog graph . edges ( data = True ) , key = lambda x : x [ 2 ] [ 'weight' ] , reverse = True ) : self . pending program edges . append ( ( end1 , end2 ) ) while self . pending program edges : edge = self . select next edge ( ) q1 mapped = edge [ 0 ] in self . prog2hw q2 mapped = edge [ 1 ] in self . prog2hw if ( not q1 mapped ) and ( not q2 mapped ) : best hw edge = self . select best remaining cx ( ) self . prog2hw [ edge [ 0 ] ] = best hw edge [ 0 ] self . prog2hw [ edge [ 1 ] ] = best hw edge [ 1 ] self . available hw qubits . remove ( best hw edge [ 0 ] ) self . available hw qubits . remove ( best hw edge [ 1 ] ) elif not q1 mapped : best hw qubit = self . select best remaining qubit ( edge [ 0 ] ) self . prog2hw [ edge [ 0 ] ] = best hw qubit self . available hw qubits . remove ( best hw qubit ) else : best hw qubit = self . select best remaining qubit ( edge [ 1 ] ) self . prog2hw [ edge [ 1 ] ] = best hw qubit self . available hw qubits . remove ( best hw qubit ) new edges = [ x for x in self . pending program edges if not ( x [ 0 ] in self . prog2hw and x [ 1 ] in self . prog2hw ) ] self . pending program edges = new edges for qid in self . qarg to id . values ( ) : if qid not in self . prog2hw : self . prog2hw [ qid ] = self . available hw qubits [ 0 ] self . available hw qubits . remove ( self . prog2hw [ qid ] ) layout = Layout ( ) for q in dag . qubits ( ) : pid = self . qarg to id ( q ) hwid = self . prog2hw [ pid ] layout [ ( q [ 0 ] , q [ 1 ] ) ] = hwid self . property set [ 'layout' ] = layout
def inverse ( self ) : self . data = [ gate . inverse ( ) for gate in reversed ( self . data ) ] self . inverse flag = not self . inverse flag return self
def q if ( self , * qregs ) : self . data = [ gate . q if ( qregs ) for gate in self . data ] return self
def c if ( self , classical , val ) : self . data = [ gate . c if ( classical , val ) for gate in self . data ] return self
def is unitary ( self , atol = None , rtol = None ) : if atol is None : atol = self . atol if rtol is None : rtol = self . rtol return is unitary matrix ( self . data , rtol = rtol , atol = atol )
def conjugate ( self ) : return Operator ( np . conj ( self . data ) , self . input dims ( ) , self . output dims ( ) )
def transpose ( self ) : return Operator ( np . transpose ( self . data ) , self . input dims ( ) , self . output dims ( ) )
def shape ( self ) : return tuple ( reversed ( self . output dims ( ) ) ) + tuple ( reversed ( self . input dims ( ) ) )
def format state ( self , state ) : state = np . array ( state ) shape = state . shape ndim = state . ndim if ndim > 2 : raise Qiskit Error ( 'Input state is not a vector or matrix.' ) # Flatten column-vector to vector if ndim == 2 : if shape [ 1 ] != 1 and shape [ 1 ] != shape [ 0 ] : raise Qiskit Error ( 'Input state is not a vector or matrix.' ) if shape [ 1 ] == 1 : # flatten colum-vector to vector state = np . reshape ( state , shape [ 0 ] ) return state
def instruction to operator ( cls , instruction ) : # Convert circuit to an instruction if isinstance ( instruction , Quantum Circuit ) : instruction = instruction . to instruction ( ) # Initialize an identity operator of the correct size of the circuit op = Operator ( np . eye ( 2 ** instruction . num qubits ) ) op . append instruction ( instruction ) return op
def append instruction ( self , obj , qargs = None ) : if isinstance ( obj , Instruction ) : mat = None if hasattr ( obj , 'to matrix' ) : # If instruction is a gate first we see if it has a # `to matrix` definition and if so use that. try : mat = obj . to matrix ( ) except Qiskit Error : pass if mat is not None : # Perform the composition and inplace update the current state # of the operator op = self . compose ( mat , qargs = qargs ) self . data = op . data else : # If the instruction doesn't have a matrix defined we use its # circuit decomposition definition if it exists, otherwise we # cannot compose this gate and raise an error. if obj . definition is None : raise Qiskit Error ( 'Cannot apply Instruction: {}' . format ( obj . name ) ) for instr , qregs , cregs in obj . definition : if cregs : raise Qiskit Error ( 'Cannot apply instruction with classical registers: {}' . format ( instr . name ) ) # Get the integer position of the flat register new qargs = [ tup [ 1 ] for tup in qregs ] self . append instruction ( instr , qargs = new qargs ) else : raise Qiskit Error ( 'Input is not an instruction.' )
def real ( self , nested scope = None ) : operation = self . children [ 0 ] . operation ( ) expr = self . children [ 1 ] . real ( nested scope ) return operation ( expr )
def sym ( self , nested scope = None ) : operation = self . children [ 0 ] . operation ( ) expr = self . children [ 1 ] . sym ( nested scope ) return operation ( expr )
def separate bitstring ( bitstring , creg sizes ) : substrings = [ ] running index = 0 for , size in reversed ( creg sizes ) : substrings . append ( bitstring [ running index : running index + size ] ) running index += size return ' ' . join ( substrings )
def bit string index ( text ) : n = len ( text ) k = text . count ( "1" ) if text . count ( "0" ) != n - k : raise Visualization Error ( "s must be a string of 0 and 1" ) ones = [ pos for pos , char in enumerate ( text ) if char == "1" ] return lex index ( n , k , ones )
def bit string index ( s ) : n = len ( s ) k = s . count ( "1" ) if s . count ( "0" ) != n - k : raise Visualization Error ( "s must be a string of 0 and 1" ) ones = [ pos for pos , char in enumerate ( s ) if char == "1" ] return lex index ( n , k , ones )
def op ( self ) : if 'type' not in self . data dict or self . data dict [ 'type' ] != 'op' : raise Qiskit Error ( "The node %s is not an op node" % ( str ( self ) ) ) return self . data dict . get ( 'op' )
def run ( self , dag ) : diagonal 1q gates = ( RZ Gate , Z Gate , T Gate , S Gate , Tdg Gate , Sdg Gate , U1Gate ) diagonal 2q gates = ( Cz Gate , Crz Gate , Cu1Gate , RZZ Gate ) nodes to remove = set ( ) for measure in dag . op nodes ( Measure ) : predecessor = dag . quantum predecessors ( measure ) [ 0 ] if predecessor . type == 'op' and isinstance ( predecessor . op , diagonal 1q gates ) : nodes to remove . add ( predecessor ) if predecessor . type == 'op' and isinstance ( predecessor . op , diagonal 2q gates ) : successors = dag . quantum successors ( predecessor ) if all ( [ s . type == 'op' and isinstance ( s . op , Measure ) for s in successors ] ) : nodes to remove . add ( predecessor ) for node to remove in nodes to remove : dag . remove op node ( node to remove ) return dag
def to string ( self , indent ) : ind = indent * ' ' print ( ind , 'qreg' ) self . children [ 0 ] . to string ( indent + 3 )
def remove all ops named ( self , opname ) : for n in self . named nodes ( opname ) : self . remove op node ( n )
def add qreg ( self , qreg ) : if not isinstance ( qreg , Quantum Register ) : raise DAG Circuit Error ( "not a Quantum Register instance." ) if qreg . name in self . qregs : raise DAG Circuit Error ( "duplicate register %s" % qreg . name ) self . qregs [ qreg . name ] = qreg for j in range ( qreg . size ) : self . add wire ( ( qreg , j ) )
def add creg ( self , creg ) : if not isinstance ( creg , Classical Register ) : raise DAG Circuit Error ( "not a Classical Register instance." ) if creg . name in self . cregs : raise DAG Circuit Error ( "duplicate register %s" % creg . name ) self . cregs [ creg . name ] = creg for j in range ( creg . size ) : self . add wire ( ( creg , j ) )
def extend back ( self , dag , edge map = None ) : edge map = edge map or { } for qreg in dag . qregs . values ( ) : if qreg . name not in self . qregs : self . add qreg ( Quantum Register ( qreg . size , qreg . name ) ) edge map . update ( [ ( qbit , qbit ) for qbit in qreg if qbit not in edge map ] ) for creg in dag . cregs . values ( ) : if creg . name not in self . cregs : self . add creg ( Classical Register ( creg . size , creg . name ) ) edge map . update ( [ ( cbit , cbit ) for cbit in creg if cbit not in edge map ] ) self . compose back ( dag , edge map )
def named nodes ( self , * names ) : named nodes = [ ] for node in self . multi graph . nodes ( ) : if node . type == 'op' and node . op . name in names : named nodes . append ( node ) return named nodes
def two Q gates ( self ) : two q gates = [ ] for node in self . gate nodes ( ) : if len ( node . qargs ) == 2 : two q gates . append ( node ) return two q gates
def predecessors ( self , node ) : if isinstance ( node , int ) : warnings . warn ( 'Calling predecessors() with a node id is deprecated,' ' use a DAG Node instead' , Deprecation Warning , 2 ) node = self . id to node [ node ] return self . multi graph . predecessors ( node )
def ancestors ( self , node ) : if isinstance ( node , int ) : warnings . warn ( 'Calling ancestors() with a node id is deprecated,' ' use a DAG Node instead' , Deprecation Warning , 2 ) node = self . id to node [ node ] return nx . ancestors ( self . multi graph , node )
def remove ancestors of ( self , node ) : if isinstance ( node , int ) : warnings . warn ( 'Calling remove ancestors of() with a node id is deprecated,' ' use a DAG Node instead' , Deprecation Warning , 2 ) node = self . id to node [ node ] anc = nx . ancestors ( self . multi graph , node ) # TODO: probably better to do all at once using # multi graph.remove nodes from; same for related functions ... for anc node in anc : if anc node . type == "op" : self . remove op node ( anc node )
def remove descendants of ( self , node ) : if isinstance ( node , int ) : warnings . warn ( 'Calling remove descendants of() with a node id is deprecated,' ' use a DAG Node instead' , Deprecation Warning , 2 ) node = self . id to node [ node ] desc = nx . descendants ( self . multi graph , node ) for desc node in desc : if desc node . type == "op" : self . remove op node ( desc node )
def remove nonancestors of ( self , node ) : if isinstance ( node , int ) : warnings . warn ( 'Calling remove nonancestors of() with a node id is deprecated,' ' use a DAG Node instead' , Deprecation Warning , 2 ) node = self . id to node [ node ] anc = nx . ancestors ( self . multi graph , node ) comp = list ( set ( self . multi graph . nodes ( ) ) - set ( anc ) ) for n in comp : if n . type == "op" : self . remove op node ( n )
def remove nondescendants of ( self , node ) : if isinstance ( node , int ) : warnings . warn ( 'Calling remove nondescendants of() with a node id is deprecated,' ' use a DAG Node instead' , Deprecation Warning , 2 ) node = self . id to node [ node ] dec = nx . descendants ( self . multi graph , node ) comp = list ( set ( self . multi graph . nodes ( ) ) - set ( dec ) ) for n in comp : if n . type == "op" : self . remove op node ( n )
def multigraph layers ( self ) : predecessor count = dict ( ) # Dict[node, predecessors not visited] cur layer = [ node for node in self . input map . values ( ) ] yield cur layer next layer = [ ] while cur layer : for node in cur layer : # Count multiedges with multiplicity. for successor in self . multi graph . successors ( node ) : multiplicity = self . multi graph . number of edges ( node , successor ) if successor in predecessor count : predecessor count [ successor ] -= multiplicity else : predecessor count [ successor ] = self . multi graph . in degree ( successor ) - multiplicity if predecessor count [ successor ] == 0 : next layer . append ( successor ) del predecessor count [ successor ] yield next layer cur layer = next layer next layer = [ ]
def properties ( self ) : summary = { "size" : self . size ( ) , "depth" : self . depth ( ) , "width" : self . width ( ) , "bits" : self . num cbits ( ) , "factors" : self . num tensor factors ( ) , "operations" : self . count ops ( ) } return summary
def pauli prep gates ( circuit , qreg , op ) : bas , proj = op if bas not in [ 'X' , 'Y' , 'Z' ] : raise Qiskit Error ( "There's no X, Y or Z basis for this Pauli " "preparation" ) if bas == "X" : if proj == 1 : circuit . u2 ( np . pi , np . pi , qreg ) # H.X else : circuit . u2 ( 0. , np . pi , qreg ) # H elif bas == "Y" : if proj == 1 : circuit . u2 ( - 0.5 * np . pi , np . pi , qreg ) # S.H.X else : circuit . u2 ( 0.5 * np . pi , np . pi , qreg ) # S.H elif bas == "Z" and proj == 1 : circuit . u3 ( np . pi , 0. , np . pi , qreg )
def pauli meas gates ( circuit , qreg , op ) : if op not in [ 'X' , 'Y' , 'Z' ] : raise Qiskit Error ( "There's no X, Y or Z basis for this Pauli " "measurement" ) if op == "X" : circuit . u2 ( 0. , np . pi , qreg ) # H elif op == "Y" : circuit . u2 ( 0. , 0.5 * np . pi , qreg )
def sic prep gates ( circuit , qreg , op ) : bas , proj = op if bas != 'S' : raise Qiskit Error ( 'Not in SIC basis!' ) theta = - 2 * np . arctan ( np . sqrt ( 2 ) ) if proj == 1 : circuit . u3 ( theta , np . pi , 0.0 , qreg ) elif proj == 2 : circuit . u3 ( theta , np . pi / 3 , 0.0 , qreg ) elif proj == 3 : circuit . u3 ( theta , - np . pi / 3 , 0.0 , qreg )
def projector ( op list , basis ) : ret = 1 # list is from qubit 0 to 1 for op in op list : label , eigenstate = op ret = np . kron ( basis [ label ] [ eigenstate ] , ret ) return ret
def run ( self , dag ) : resets = dag . op nodes ( Reset ) for reset in resets : predecessor = next ( dag . predecessors ( reset ) ) if predecessor . type == 'in' : dag . remove op node ( reset ) return dag
def cu3 ( self , theta , phi , lam , ctl , tgt ) : return self . append ( Cu3Gate ( theta , phi , lam ) , [ ctl , tgt ] , [ ] )
def build bell circuit ( ) : q = Quantum Register ( 2 ) c = Classical Register ( 2 ) qc = Quantum Circuit ( q , c ) qc . h ( q [ 0 ] ) qc . cx ( q [ 0 ] , q [ 1 ] ) qc . measure ( q , c ) return qc
def drive ( self ) -> Drive Channel : if self . drives : return self . drives [ 0 ] else : raise Pulse Error ( "No drive channels in q[%d]" % self . index )
def control ( self ) -> Control Channel : if self . controls : return self . controls [ 0 ] else : raise Pulse Error ( "No control channels in q[%d]" % self . index )
def measure ( self ) -> Measure Channel : if self . measures : return self . measures [ 0 ] else : raise Pulse Error ( "No measurement channels in q[%d]" % self . index )
def acquire ( self ) -> Acquire Channel : if self . acquires : return self . acquires [ 0 ] else : raise Pulse Error ( "No acquire channels in q[%d]" % self . index )
def input state ( circ , q , n ) : for j in range ( n ) : circ . h ( q [ j ] ) circ . u1 ( math . pi / float ( 2 ** ( j ) ) , q [ j ] ) . inverse ( )
def unset qiskit logger ( ) : qiskit logger = logging . get Logger ( 'qiskit' ) for handler in qiskit logger . handlers : qiskit logger . remove Handler ( handler )
def input ( self , data ) : self . data = data self . lexer . input ( data )
def pop ( self ) : self . lexer = self . stack . pop ( ) self . filename = self . lexer . qasm file self . lineno = self . lexer . qasm line
def push ( self , filename ) : self . lexer . qasm file = self . filename self . lexer . qasm line = self . lineno self . stack . append ( self . lexer ) self . mklexer ( filename )
def get bound method ( self , instruction ) : try : return self . bound instructions [ type ( instruction ) ] except Key Error : raise Pulse Error ( 'Qobj conversion method for %s is not found.' % instruction )
def verify declared bit ( self , obj ) : # We are verifying gate args against the formal parameters of a # gate prototype. if obj . name not in self . current symtab : raise Qasm Error ( "Cannot find symbol '" + obj . name + "' in argument list for gate, line" , str ( obj . line ) , 'file' , obj . file ) # This insures the thing is from the bitlist and not from the # argument list. sym = self . current symtab [ obj . name ] if not ( sym . type == 'id' and sym . is bit ) : raise Qasm Error ( "Bit" , obj . name , 'is not declared as a bit in the gate.' )
def verify exp list ( self , obj ) : # A tad harder.  This is a list of expressions each of which could be # the head of a tree. We need to recursively walk each of these and # ensure that any Id elements resolve to the current stack. # # I believe we only have to look at the current symtab. if obj . children is not None : for children in obj . children : if isinstance ( children , node . Id ) : if children . name in self . external functions : continue if children . name not in self . current symtab : raise Qasm Error ( "Argument '" + children . name + "' in expression cannot be " + "found, line" , str ( children . line ) , "file" , children . file ) else : if hasattr ( children , "children" ) : self . verify exp list ( children )
def verify as gate ( self , obj , bitlist , arglist = None ) : if obj . name not in self . global symtab : raise Qasm Error ( "Cannot find gate definition for '" + obj . name + "', line" , str ( obj . line ) , 'file' , obj . file ) g sym = self . global symtab [ obj . name ] if not ( g sym . type == 'gate' or g sym . type == 'opaque' ) : raise Qasm Error ( "'" + obj . name + "' is used as a gate " + "or opaque call but the symbol is neither;" + " it is a '" + g sym . type + "' line" , str ( obj . line ) , 'file' , obj . file ) if g sym . n bits ( ) != bitlist . size ( ) : raise Qasm Error ( "Gate or opaque call to '" + obj . name + "' uses" , str ( bitlist . size ( ) ) , "qubits but is declared for" , str ( g sym . n bits ( ) ) , "qubits" , "line" , str ( obj . line ) , 'file' , obj . file ) if arglist : if g sym . n args ( ) != arglist . size ( ) : raise Qasm Error ( "Gate or opaque call to '" + obj . name + "' uses" , str ( arglist . size ( ) ) , "qubits but is declared for" , str ( g sym . n args ( ) ) , "qubits" , "line" , str ( obj . line ) , 'file' , obj . file ) else : if g sym . n args ( ) > 0 : raise Qasm Error ( "Gate or opaque call to '" + obj . name + "' has no arguments but is declared for" , str ( g sym . n args ( ) ) , "qubits" , "line" , str ( obj . line ) , 'file' , obj . file )
def verify reg ( self , obj , object type ) : # How to verify: #    types must match #    indexes must be checked if obj . name not in self . global symtab : raise Qasm Error ( 'Cannot find definition for' , object type , "'" + obj . name + "'" , 'at line' , str ( obj . line ) , 'file' , obj . file ) g sym = self . global symtab [ obj . name ] if g sym . type != object type : raise Qasm Error ( "Type for '" + g sym . name + "' should be '" + object type + "' but was found to be '" + g sym . type + "'" , "line" , str ( obj . line ) , "file" , obj . file ) if obj . type == 'indexed id' : bound = g sym . index ndx = obj . index if ndx < 0 or ndx >= bound : raise Qasm Error ( "Register index for '" + g sym . name + "' out of bounds. Index is" , str ( ndx ) , "bound is 0 <= index <" , str ( bound ) , "at line" , str ( obj . line ) , "file" , obj . file )
def verify reg list ( self , obj , object type ) : # We expect the object to be a bitlist or an idlist, we don't care. # We will iterate it and ensure everything in it is declared as a bit, # and throw if not. for children in obj . children : self . verify reg ( children , object type )
def get tokens ( self ) : try : while True : token = self . lexer . token ( ) if not token : break yield token except Qasm Error as e : print ( 'Exception tokenizing qasm file:' , e . msg )
def parse debug ( self , val ) : if val is True : self . parse deb = True elif val is False : self . parse deb = False else : raise Qasm Error ( "Illegal debug value '" + str ( val ) + "' must be True or False." )
def parse ( self , data ) : self . parser . parse ( data , lexer = self . lexer , debug = self . parse deb ) if self . qasm is None : raise Qasm Error ( "Uncaught exception in parser; " + "see previous messages for details." ) return self . qasm
def get tokens ( self ) : if self . filename : with open ( self . filename ) as ifile : self . data = ifile . read ( ) with Qasm Parser ( self . filename ) as qasm p : return qasm p . get tokens ( )
def parse ( self ) : if self . filename : with open ( self . filename ) as ifile : self . data = ifile . read ( ) with Qasm Parser ( self . filename ) as qasm p : qasm p . parse debug ( False ) return qasm p . parse ( self . data )
def crz ( self , theta , ctl , tgt ) : return self . append ( Crz Gate ( theta ) , [ ctl , tgt ] , [ ] )
def qasm ( self , prec = 15 ) : string = "gate " + self . name if self . arguments is not None : string += "(" + self . arguments . qasm ( prec ) + ")" string += " " + self . bitlist . qasm ( prec ) + "\n" string += "{\n" + self . body . qasm ( prec ) + "}" return string
def backend widget ( backend ) : config = backend . configuration ( ) . to dict ( ) props = backend . properties ( ) . to dict ( ) name = widgets . HTML ( value = "<h4>{name}</h4>" . format ( name = backend . name ( ) ) , layout = widgets . Layout ( ) ) n qubits = config [ 'n qubits' ] qubit count = widgets . HTML ( value = "<h5><b>{qubits}</b></h5>" . format ( qubits = n qubits ) , layout = widgets . Layout ( justify content = 'center' ) ) cmap = widgets . Output ( layout = widgets . Layout ( min width = '250px' , max width = '250px' , max height = '250px' , min height = '250px' , justify content = 'center' , align items = 'center' , margin = '0px 0px 0px 0px' ) ) with cmap : cmap fig = plot gate map ( backend , plot directed = False , label qubits = False ) if cmap fig is not None : display ( cmap fig ) # Prevents plot from showing up twice. plt . close ( cmap fig ) pending = generate jobs pending widget ( ) is oper = widgets . HTML ( value = "<h5></h5>" , layout = widgets . Layout ( justify content = 'center' ) ) least busy = widgets . HTML ( value = "<h5></h5>" , layout = widgets . Layout ( justify content = 'center' ) ) t1 units = props [ 'qubits' ] [ 0 ] [ 0 ] [ 'unit' ] avg t1 = round ( sum ( [ q [ 0 ] [ 'value' ] for q in props [ 'qubits' ] ] ) / n qubits , 1 ) t1 widget = widgets . HTML ( value = "<h5>{t1} {units}</h5>" . format ( t1 = avg t1 , units = t1 units ) , layout = widgets . Layout ( ) ) t2 units = props [ 'qubits' ] [ 0 ] [ 1 ] [ 'unit' ] avg t2 = round ( sum ( [ q [ 1 ] [ 'value' ] for q in props [ 'qubits' ] ] ) / n qubits , 1 ) t2 widget = widgets . HTML ( value = "<h5>{t2} {units}</h5>" . format ( t2 = avg t2 , units = t2 units ) , layout = widgets . Layout ( ) ) out = widgets . V Box ( [ name , cmap , qubit count , pending , least busy , is oper , t1 widget , t2 widget ] , layout = widgets . Layout ( display = 'inline-flex' , flex flow = 'column' , align items = 'center' ) ) out . is alive = True return out
def generate jobs pending widget ( ) : pbar = widgets . Int Progress ( value = 0 , min = 0 , max = 50 , description = '' , orientation = 'horizontal' , layout = widgets . Layout ( max width = '180px' ) ) pbar . style . bar color = '#71cddd' pbar current = widgets . Label ( value = str ( pbar . value ) , layout = widgets . Layout ( min width = 'auto' ) ) pbar max = widgets . Label ( value = str ( pbar . max ) , layout = widgets . Layout ( min width = 'auto' ) ) def on max change ( change ) : pbar max . value = str ( change [ 'new' ] ) def on val change ( change ) : pbar current . value = str ( change [ 'new' ] ) pbar . observe ( on max change , names = 'max' ) pbar . observe ( on val change , names = 'value' ) jobs widget = widgets . H Box ( [ pbar current , pbar , pbar max ] , layout = widgets . Layout ( max width = '250px' , min width = '250px' , justify content = 'center' ) ) return jobs widget
def bipartite shape ( self ) : return ( self . input dim , self . output dim , self . input dim , self . output dim )
def conjugate ( self ) : return Choi ( np . conj ( self . data ) , self . input dims ( ) , self . output dims ( ) )
def transpose ( self ) : # Make bipartite matrix d in , d out = self . dim data = np . reshape ( self . data , ( d in , d out , d in , d out ) ) # Swap input and output indicies on bipartite matrix data = np . transpose ( data , ( 1 , 0 , 3 , 2 ) ) # Transpose channel has input and output dimensions swapped data = np . reshape ( data , ( d in * d out , d in * d out ) ) return Choi ( data , input dims = self . output dims ( ) , output dims = self . input dims ( ) )
def load schemas and validators ( ) : schema base path = os . path . join ( os . path . dirname ( file ) , '../..' ) for name , path in DEFAULT SCHEMA PATHS . items ( ) : load schema ( os . path . join ( schema base path , path ) , name ) get validator ( name )
def qasm ( self , prec = 15 ) : return "," . join ( [ self . children [ j ] . qasm ( prec ) for j in range ( self . size ( ) ) ] )
def qasm ( self , prec = 15 ) : string = "" for children in self . children : string += "  " + children . qasm ( prec ) + "\n" return string
def calls ( self ) : lst = [ ] for children in self . children : if children . type == "custom unitary" : lst . append ( children . name ) return lst
def qasm ( self , prec = 15 ) : if self . value == pi : return "pi" return ccode ( self . value , precision = prec )
def conjugate ( self ) : return Super Op ( np . conj ( self . data ) , self . input dims ( ) , self . output dims ( ) )
def transpose ( self ) : return Super Op ( np . transpose ( self . data ) , input dims = self . output dims ( ) , output dims = self . input dims ( ) )
def compose subsystem ( self , other , qargs , front = False ) : # Compute tensor contraction indices from qargs input dims = list ( self . input dims ( ) ) output dims = list ( self . output dims ( ) ) if front : num indices = len ( self . input dims ( ) ) shift = 2 * len ( self . output dims ( ) ) right mul = True for pos , qubit in enumerate ( qargs ) : input dims [ qubit ] = other . input dims [ pos ] else : num indices = len ( self . output dims ( ) ) shift = 0 right mul = False for pos , qubit in enumerate ( qargs ) : output dims [ qubit ] = other . output dims [ pos ] # Reshape current matrix # Note that we must reverse the subsystem dimension order as # qubit 0 corresponds to the right-most position in the tensor # product, which is the last tensor wire index. tensor = np . reshape ( self . data , self . shape ) mat = np . reshape ( other . data , other . shape ) # Add first set of indicies indices = [ 2 * num indices - 1 - qubit for qubit in qargs ] + [ num indices - 1 - qubit for qubit in qargs ] final shape = [ np . product ( output dims ) ** 2 , np . product ( input dims ) ** 2 ] data = np . reshape ( self . einsum matmul ( tensor , mat , indices , shift , right mul ) , final shape ) return Super Op ( data , input dims , output dims )
def instruction to superop ( cls , instruction ) : # Convert circuit to an instruction if isinstance ( instruction , Quantum Circuit ) : instruction = instruction . to instruction ( ) # Initialize an identity superoperator of the correct size # of the circuit op = Super Op ( np . eye ( 4 ** instruction . num qubits ) ) op . append instruction ( instruction ) return op
def append instruction ( self , obj , qargs = None ) : if isinstance ( obj , Instruction ) : chan = None if obj . name == 'reset' : # For superoperator evolution we can simulate a reset as # a non-unitary supeorperator matrix chan = Super Op ( np . array ( [ [ 1 , 0 , 0 , 1 ] , [ 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 ] ] ) ) if obj . name == 'kraus' : kraus = obj . params dim = len ( kraus [ 0 ] ) chan = Super Op ( to superop ( 'Kraus' , ( kraus , None ) , dim , dim ) ) elif hasattr ( obj , 'to matrix' ) : # If instruction is a gate first we see if it has a # `to matrix` definition and if so use that. try : kraus = [ obj . to matrix ( ) ] dim = len ( kraus [ 0 ] ) chan = Super Op ( to superop ( 'Kraus' , ( kraus , None ) , dim , dim ) ) except Qiskit Error : pass if chan is not None : # Perform the composition and inplace update the current state # of the operator op = self . compose ( chan , qargs = qargs ) self . data = op . data else : # If the instruction doesn't have a matrix defined we use its # circuit decomposition definition if it exists, otherwise we # cannot compose this gate and raise an error. if obj . definition is None : raise Qiskit Error ( 'Cannot apply Instruction: {}' . format ( obj . name ) ) for instr , qregs , cregs in obj . definition : if cregs : raise Qiskit Error ( 'Cannot apply instruction with classical registers: {}' . format ( instr . name ) ) # Get the integer position of the flat register new qargs = [ tup [ 1 ] for tup in qregs ] self . append instruction ( instr , qargs = new qargs ) else : raise Qiskit Error ( 'Input is not an instruction.' )
def run ( self , dag ) : # Collect DAG nodes which are followed only by barriers or other measures. final op types = [ 'measure' , 'barrier' ] final ops = [ ] for candidate node in dag . named nodes ( * final op types ) : is final op = True for , child successors in dag . bfs successors ( candidate node ) : if any ( suc . type == 'op' and suc . name not in final op types for suc in child successors ) : is final op = False break if is final op : final ops . append ( candidate node ) if not final ops : return dag # Create a layer with the barrier and add registers from the original dag. barrier layer = DAG Circuit ( ) for qreg in dag . qregs . values ( ) : barrier layer . add qreg ( qreg ) for creg in dag . cregs . values ( ) : barrier layer . add creg ( creg ) final qubits = set ( final op . qargs [ 0 ] for final op in final ops ) barrier layer . apply operation back ( Barrier ( len ( final qubits ) ) , list ( final qubits ) , [ ] ) # Preserve order of final ops collected earlier from the original DAG. ordered final nodes = [ node for node in dag . topological op nodes ( ) if node in set ( final ops ) ] # Move final ops to the new layer and append the new layer to the DAG. for final node in ordered final nodes : barrier layer . apply operation back ( final node . op , final node . qargs , final node . cargs ) for final op in final ops : dag . remove op node ( final op ) dag . extend back ( barrier layer ) # Merge the new barrier into any other barriers adjacent pass = Merge Adjacent Barriers ( ) return adjacent pass . run ( dag )
def unitary ( self , obj , qubits , label = None ) : if isinstance ( qubits , Quantum Register ) : qubits = qubits [ : ] return self . append ( Unitary Gate ( obj , label = label ) , qubits , [ ] )
def define ( self ) : if self . num qubits == 1 : q = Quantum Register ( 1 , "q" ) angles = euler angles 1q ( self . to matrix ( ) ) self . definition = [ ( U3Gate ( * angles ) , [ q [ 0 ] ] , [ ] ) ] if self . num qubits == 2 : self . definition = two qubit kak ( self . to matrix ( ) )
def atol ( self , atol ) : # NOTE: that this overrides the class value so applies to all # instances of the class. max tol = self . class . MAX TOL if atol < 0 : raise Qiskit Error ( "Invalid atol: must be non-negative." ) if atol > max tol : raise Qiskit Error ( "Invalid atol: must be less than {}." . format ( max tol ) ) self . class . ATOL = atol
def rtol ( self , rtol ) : # NOTE: that this overrides the class value so applies to all # instances of the class. max tol = self . class . MAX TOL if rtol < 0 : raise Qiskit Error ( "Invalid rtol: must be non-negative." ) if rtol > max tol : raise Qiskit Error ( "Invalid rtol: must be less than {}." . format ( max tol ) ) self . class . RTOL = rtol
def input dims ( self , qargs = None ) : if qargs is None : return self . input dims return tuple ( self . input dims [ i ] for i in qargs )
def output dims ( self , qargs = None ) : if qargs is None : return self . output dims return tuple ( self . output dims [ i ] for i in qargs )
def copy ( self ) : # pylint: disable=no-value-for-parameter # The constructor of subclasses from raw data should be a copy return self . class ( self . data , self . input dims ( ) , self . output dims ( ) )
def automatic dims ( cls , dims , size ) : if dims is None : dims = size elif np . product ( dims ) != size : raise Qiskit Error ( "dimensions do not match size." ) if isinstance ( dims , ( int , np . integer ) ) : num qubits = int ( np . log2 ( dims ) ) if 2 ** num qubits == size : return num qubits * ( 2 , ) return ( dims , ) return tuple ( dims )
def deserialize ( self , value , attr , data ) : try : return super ( ) . deserialize ( value , attr , data ) except Validation Error as ex : if 'deserialization schema selector' in ex . messages [ 0 ] : ex . messages [ 0 ] = 'Cannot find a valid schema among the choices' raise
def serialize ( self , value , key , obj ) : try : return super ( ) . serialize ( value , key , obj ) except Type Error as ex : if 'serialization schema selector' in str ( ex ) : raise Validation Error ( 'Data from an invalid schema' ) raise
def inverse ( self ) : return Snapshot ( self . num qubits , self . num clbits , self . params [ 0 ] , self . params [ 1 ] )
def is unitary ( self , atol = None , rtol = None ) : try : op = self . to operator ( ) return op . is unitary ( atol = atol , rtol = rtol ) except Qiskit Error : return False
def to operator ( self ) : mat = to operator ( self . rep , self . data , * self . dim ) return Operator ( mat , self . input dims ( ) , self . output dims ( ) )
def format state ( self , state , density matrix = False ) : state = np . array ( state ) shape = state . shape ndim = state . ndim if ndim > 2 : raise Qiskit Error ( 'Input state is not a vector or matrix.' ) # Flatten column-vector to vector if ndim == 2 : if shape [ 1 ] != 1 and shape [ 1 ] != shape [ 0 ] : raise Qiskit Error ( 'Input state is not a vector or matrix.' ) if shape [ 1 ] == 1 : # flatten colum-vector to vector state = np . reshape ( state , shape [ 0 ] ) # Convert statevector to density matrix if required if density matrix and ndim == 1 : state = np . outer ( state , np . transpose ( np . conj ( state ) ) ) return state
def init transformer ( cls , data ) : # This handles common conversion for all Quantum Channel subclasses. # If the input is already a Quantum Channel subclass it will return # the original object if isinstance ( data , Quantum Channel ) : return data if hasattr ( data , 'to quantumchannel' ) : # If the data object is not a Quantum Channel it will give # preference to a 'to quantumchannel' attribute that allows # an arbitrary object to define its own conversion to any # quantum channel subclass. return data . to channel ( ) if hasattr ( data , 'to channel' ) : # TODO: this 'to channel' method is the same case as the above # but is used by current version of Aer. It should be removed # once Aer is nupdated to use `to quantumchannel` # instead of `to channel`, return data . to channel ( ) # Finally if the input is not a Quantum Channel and doesn't have a # 'to quantumchannel' conversion method we try and initialize it as a # regular matrix Operator which can be converted into a Quantum Channel. return Operator ( data )
def parse time ( self , date string , settings ) : date string = PATTERN . sub ( '' , date string ) date string = re . sub ( r'\b(?:ago|in)\b' , '' , date string ) try : return time parser ( date string ) except : pass
def read config ( self ) : self . threads = self . cfg [ "threads" ] or str ( int ( multiprocessing . cpu count ( ) / 2 ) + 1 ) self . phantom modules path = self . cfg [ "phantom modules path" ] self . additional libs = ' ' . join ( self . cfg [ "additional libs" ] ) self . answ log level = self . cfg [ "writelog" ] if self . answ log level . lower ( ) in [ '0' , 'false' ] : self . answ log level = 'none' elif self . answ log level . lower ( ) in [ '1' , 'true' ] : self . answ log level = 'all' self . timeout = parse duration ( self . cfg [ "timeout" ] ) if self . timeout > 120000 : logger . warning ( "You've set timeout over 2 minutes." " Are you a functional tester?" ) self . answ log = self . core . mkstemp ( ".log" , "answ " ) self . core . add artifact file ( self . answ log ) self . core . add artifact file ( self . phout file ) self . core . add artifact file ( self . stat log ) self . phantom log = self . core . mkstemp ( ".log" , "phantom " ) self . core . add artifact file ( self . phantom log ) main stream = Stream Config ( self . core , len ( self . streams ) , self . phout file , self . answ log , self . answ log level , self . timeout , self . cfg , True ) self . streams . append ( main stream ) for section in self . multi ( ) : self . streams . append ( Stream Config ( self . core , len ( self . streams ) , self . phout file , self . answ log , self . answ log level , self . timeout , section ) ) for stream in self . streams : stream . read config ( ) if any ( stream . ssl for stream in self . streams ) : self . additional libs += ' ssl io benchmark method stream transport ssl'
def compose config ( self ) : streams config = '' stat benchmarks = '' for stream in self . streams : streams config += stream . compose config ( ) if not stream . is main : stat benchmarks += " " + "benchmark io%s" % stream . sequence no kwargs = { } kwargs [ 'threads' ] = self . threads kwargs [ 'phantom log' ] = self . phantom log kwargs [ 'stat log' ] = self . stat log kwargs [ 'benchmarks block' ] = streams config kwargs [ 'stat benchmarks' ] = stat benchmarks kwargs [ 'additional libs' ] = self . additional libs kwargs [ 'phantom modules path' ] = self . phantom modules path filename = self . core . mkstemp ( ".conf" , "phantom " ) self . core . add artifact file ( filename ) logger . debug ( "Generating phantom config: %s" , filename ) template str = resource string ( name , "config/phantom.conf.tpl" ) tpl = string . Template ( template str ) config = tpl . substitute ( kwargs ) with open ( filename , 'w' ) as conffile : conffile . write ( config ) return filename
def get info ( self ) : result = copy . copy ( self . streams [ 0 ] ) result . stat log = self . stat log result . steps = [ ] result . ammo file = '' result . rps schedule = None result . ammo count = 0 result . duration = 0 result . instances = 0 result . loadscheme = [ ] result . loop count = 0 for stream in self . streams : sec no = 0 logger . debug ( "Steps: %s" , stream . stepper wrapper . steps ) for item in stream . stepper wrapper . steps : for x in range ( 0 , item [ 1 ] ) : if len ( result . steps ) > sec no : result . steps [ sec no ] [ 0 ] += item [ 0 ] else : result . steps . append ( [ item [ 0 ] , 1 ] ) sec no += 1 if result . rps schedule : result . rps schedule = [ ] else : result . rps schedule = stream . stepper wrapper . loadscheme if result . loadscheme : result . loadscheme = '' else : # FIXME: add formatted load scheme for server: # <step size,step type,first rps,last rps,original step params> # as a string result . loadscheme = '' if result . loop count : result . loop count = u'0' else : result . loop count = stream . stepper wrapper . loop count result . ammo file += '{} ' . format ( stream . stepper wrapper . ammo file ) result . ammo count += stream . stepper wrapper . ammo count result . duration = max ( result . duration , stream . stepper wrapper . duration ) result . instances += stream . instances if not result . ammo count : raise Value Error ( "Total ammo count cannot be zero" ) return result
def expand time ( str time , default unit = 's' , multiplier = 1 ) : parser = re . compile ( r'(\d+)([a-z A-Z]*)' ) parts = parser . findall ( str time ) result = 0.0 for value , unit in parts : value = int ( value ) unit = unit . lower ( ) if unit == '' : unit = default unit if unit == 'ms' : result += value * 0.001 continue elif unit == 's' : result += value continue elif unit == 'm' : result += value * 60 continue elif unit == 'h' : result += value * 60 * 60 continue elif unit == 'd' : result += value * 60 * 60 * 24 continue elif unit == 'w' : result += value * 60 * 60 * 24 * 7 continue else : raise Value Error ( "String contains unsupported unit %s: %s" % ( unit , str time ) ) return int ( result * multiplier )
def pid exists ( pid ) : if pid < 0 : return False try : os . kill ( pid , 0 ) except OS Error as exc : logging . debug ( "No process[%s]: %s" , exc . errno , exc ) return exc . errno == errno . EPERM else : p = psutil . Process ( pid ) return p . status != psutil . STATUS ZOMBIE
def read config ( self ) : self . log . info ( "Configuring Stepper Wrapper..." ) self . ammo file = self . get option ( self . OPTION AMMOFILE ) self . ammo type = self . get option ( 'ammo type' ) if self . ammo file : self . ammo file = os . path . expanduser ( self . ammo file ) self . loop limit = self . get option ( self . OPTION LOOP ) self . ammo limit = self . get option ( "ammo limit" ) self . load profile = Load Profile ( * * self . get option ( 'load profile' ) ) self . instances = int ( self . get option ( self . OPTION INSTANCES LIMIT , '1000' ) ) self . uris = self . get option ( "uris" , [ ] ) while '' in self . uris : self . uris . remove ( '' ) self . headers = self . get option ( "headers" ) self . http ver = self . get option ( "header http" ) self . autocases = self . get option ( "autocases" ) self . enum ammo = self . get option ( "enum ammo" ) self . use caching = self . get option ( "use caching" ) self . file cache = self . get option ( 'file cache' ) cache dir = self . get option ( "cache dir" ) or self . core . artifacts base dir self . cache dir = os . path . expanduser ( cache dir ) self . force stepping = self . get option ( "force stepping" ) if self . get option ( self . OPTION LOAD ) [ self . OPTION LOAD TYPE ] == 'stpd file' : self . stpd = self . get option ( self . OPTION LOAD ) [ self . OPTION SCHEDULE ] self . chosen cases = self . get option ( "chosen cases" ) . split ( ) if self . chosen cases : self . log . info ( "chosen cases LIMITS: %s" , self . chosen cases )
def prepare stepper ( self ) : def publish info ( stepper info ) : info . status . publish ( 'loadscheme' , stepper info . loadscheme ) info . status . publish ( 'loop count' , stepper info . loop count ) info . status . publish ( 'steps' , stepper info . steps ) info . status . publish ( 'duration' , stepper info . duration ) info . status . ammo count = stepper info . ammo count info . status . publish ( 'instances' , stepper info . instances ) self . core . publish ( 'stepper' , 'loadscheme' , stepper info . loadscheme ) self . core . publish ( 'stepper' , 'loop count' , stepper info . loop count ) self . core . publish ( 'stepper' , 'steps' , stepper info . steps ) self . core . publish ( 'stepper' , 'duration' , stepper info . duration ) self . core . publish ( 'stepper' , 'ammo count' , stepper info . ammo count ) self . core . publish ( 'stepper' , 'instances' , stepper info . instances ) return stepper info if not self . stpd : self . stpd = self . get stpd filename ( ) if self . use caching and not self . force stepping and os . path . exists ( self . stpd ) and os . path . exists ( self . si filename ( ) ) : self . log . info ( "Using cached stpd-file: %s" , self . stpd ) stepper info = self . read cached options ( ) if self . instances and self . load profile . is rps ( ) : self . log . info ( "rps schedule is set. Overriding cached instances param from config: %s" , self . instances ) stepper info = stepper info . replace ( instances = self . instances ) publish info ( stepper info ) else : if ( self . force stepping and os . path . exists ( self . si filename ( ) ) ) : os . remove ( self . si filename ( ) ) self . make stpd file ( ) stepper info = info . status . get info ( ) self . write cached options ( stepper info ) else : self . log . info ( "Using specified stpd-file: %s" , self . stpd ) stepper info = publish info ( self . read cached options ( ) ) self . ammo count = stepper info . ammo count self . duration = stepper info . duration self . loop count = stepper info . loop count self . loadscheme = stepper info . loadscheme self . steps = stepper info . steps if stepper info . instances : self . instances = stepper info . instances
def get stpd filename ( self ) : if self . use caching : sep = "|" hasher = hashlib . md5 ( ) hashed str = "cache version 6" + sep + ';' . join ( self . load profile . schedule ) + sep + str ( self . loop limit ) hashed str += sep + str ( self . ammo limit ) + sep + ';' . join ( self . load profile . schedule ) + sep + str ( self . autocases ) hashed str += sep + ";" . join ( self . uris ) + sep + ";" . join ( self . headers ) + sep + self . http ver + sep + ";" . join ( self . chosen cases ) hashed str += sep + str ( self . enum ammo ) + sep + str ( self . ammo type ) if self . load profile . is instances ( ) : hashed str += sep + str ( self . instances ) if self . ammo file : opener = resource . get opener ( self . ammo file ) hashed str += sep + opener . hash else : if not self . uris : raise Runtime Error ( "Neither ammofile nor uris specified" ) hashed str += sep + ';' . join ( self . uris ) + sep + ';' . join ( self . headers ) self . log . debug ( "stpd-hash source: %s" , hashed str ) hasher . update ( hashed str . encode ( 'utf8' ) ) if not os . path . exists ( self . cache dir ) : os . makedirs ( self . cache dir ) stpd = self . cache dir + '/' + os . path . basename ( self . ammo file ) + " " + hasher . hexdigest ( ) + ".stpd" else : stpd = os . path . realpath ( "ammo.stpd" ) self . log . debug ( "Generated cache file name: %s" , stpd ) return stpd
def read cached options ( self ) : self . log . debug ( "Reading cached stepper info: %s" , self . si filename ( ) ) with open ( self . si filename ( ) , 'r' ) as si file : si = info . Stepper Info ( * * json . load ( si file ) ) return si
def write cached options ( self , si ) : self . log . debug ( "Saving stepper info: %s" , self . si filename ( ) ) with open ( self . si filename ( ) , 'w' ) as si file : json . dump ( si . asdict ( ) , si file , indent = 4 )
def make stpd file ( self ) : self . log . info ( "Making stpd-file: %s" , self . stpd ) stepper = Stepper ( self . core , rps schedule = self . load profile . schedule if self . load profile . is rps ( ) else None , http ver = self . http ver , ammo file = self . ammo file , instances schedule = self . load profile . schedule if self . load profile . is instances ( ) else None , instances = self . instances , loop limit = self . loop limit , ammo limit = self . ammo limit , uris = self . uris , headers = [ header . strip ( '[]' ) for header in self . headers ] , autocases = self . autocases , enum ammo = self . enum ammo , ammo type = self . ammo type , chosen cases = self . chosen cases , use cache = self . use caching ) with open ( self . stpd , 'w' , self . file cache ) as os : stepper . write ( os )
def create ( rps schedule ) : if len ( rps schedule ) > 1 : lp = Composite ( [ Step Factory . produce ( step config ) for step config in rps schedule ] ) else : lp = Step Factory . produce ( rps schedule [ 0 ] ) info . status . publish ( 'duration' , lp . get duration ( ) / 1000 ) info . status . publish ( 'steps' , lp . get rps list ( ) ) info . status . lp len = len ( lp ) return lp
def rps at ( self , t ) : if 0 <= t <= self . duration : return self . minrps + float ( self . maxrps - self . minrps ) * t / self . duration else : return 0
def execute ( self , cmd ) : self . log . info ( "Executing: %s" , cmd ) retcode = execute ( cmd , shell = True , poll period = 0.1 , catch out = self . catch out ) [ 0 ] if retcode : raise Runtime Error ( "Subprocess returned %s" % retcode ) return retcode
def publish ( self , key , value ) : self . log . debug ( "Publishing status: %s/%s: %s" , self . class . name , key , value ) self . core . publish ( self . class . name , key , value )
def count matched codes ( codes regex , codes dict ) : total = 0 for code , count in codes dict . items ( ) : if codes regex . match ( str ( code ) ) : total += count return total
def stop ( self ) : self . quit . set ( ) # yapf:disable while sorted ( [ self . pool [ i ] . is alive ( ) for i in xrange ( len ( self . pool ) ) ] ) [ - 1 ] : time . sleep ( 1 ) # yapf:enable try : while not self . task queue . empty ( ) : self . task queue . get ( timeout = 0.1 ) self . task queue . close ( ) self . feeder . join ( ) except Exception as ex : logger . info ( ex )
def feed ( self ) : self . plan = Stpd Reader ( self . stpd filename ) if self . cached stpd : self . plan = list ( self . plan ) for task in self . plan : if self . quit . is set ( ) : logger . info ( "Stop feeding: gonna quit" ) return # try putting a task to a queue unless there is a quit flag # or all workers have exited while True : try : self . task queue . put ( task , timeout = 1 ) break except Full : if self . quit . is set ( ) or self . workers finished : return else : continue workers count = self . instances logger . info ( "Feeded all data. Publishing %d killer tasks" % ( workers count ) ) retry delay = 1 for in range ( 5 ) : try : [ self . task queue . put ( None , timeout = 1 ) for in xrange ( 0 , workers count ) ] break except Full : logger . debug ( "Couldn't post killer tasks" " because queue is full. Retrying in %ss" , retry delay ) time . sleep ( retry delay ) retry delay *= 2 try : logger . info ( "Waiting for workers" ) map ( lambda x : x . join ( ) , self . pool ) logger . info ( "All workers exited." ) self . workers finished = True except ( Keyboard Interrupt , System Exit ) : self . task queue . close ( ) self . results . close ( ) self . quit . set ( ) logger . info ( "Going to quit. Waiting for workers" ) map ( lambda x : x . join ( ) , self . pool ) self . workers finished = True
def worker ( self ) : logger . debug ( "Init shooter process" ) try : self . gun . setup ( ) except Exception : logger . exception ( "Couldn't initialize gun. Exit shooter process" ) return while not self . quit . is set ( ) : try : task = self . task queue . get ( timeout = 1 ) if not task : logger . debug ( "Got killer task." ) break timestamp , missile , marker = task planned time = self . start time + ( timestamp / 1000.0 ) delay = planned time - time . time ( ) if delay > 0 : time . sleep ( delay ) try : with self . instance counter . get lock ( ) : self . instance counter . value += 1 self . gun . shoot ( missile , marker ) finally : with self . instance counter . get lock ( ) : self . instance counter . value -= 1 except ( Keyboard Interrupt , System Exit ) : break except Empty : if self . quit . is set ( ) : logger . debug ( "Empty queue. Exiting process" ) return except Full : logger . warning ( "Couldn't put to result queue because it's full" ) except Exception : logger . exception ( "Bfg shoot exception" ) try : self . gun . teardown ( ) except Exception : logger . exception ( "Couldn't finalize gun. Exit shooter process" ) return logger . debug ( "Exit shooter process" )
def green worker ( self ) : while not self . quit . is set ( ) : try : task = self . green queue . get ( timeout = 1 ) timestamp , missile , marker = task planned time = self . start time + ( timestamp / 1000.0 ) delay = planned time - time . time ( ) if delay > 0 : time . sleep ( delay ) try : with self . instance counter . get lock ( ) : self . instance counter . value += 1 self . gun . shoot ( missile , marker ) finally : with self . instance counter . get lock ( ) : self . instance counter . value -= 1 self . free threads count += 1 except ( Keyboard Interrupt , System Exit ) : break except Empty : continue except Full : logger . warning ( "Couldn't put to result queue because it's full" ) except Exception : logger . exception ( "Bfg shoot exception" )
def add user options ( self ) : if self . options . get ( 'user options' , None ) : self . core . apply shorthand options ( self . options [ 'user options' ] )
def configure ( self , options ) : self . options = options if self . options . get ( 'lock dir' , None ) : self . core . set option ( self . core . SECTION , "lock dir" , self . options [ 'lock dir' ] ) if self . options . get ( 'ignore lock' , None ) : self . core . set option ( self . core . SECTION , 'ignore lock' , self . options [ 'ignore lock' ] ) while True : try : self . core . get lock ( ) break except Exception as exc : if self . options . get ( 'lock fail' , None ) : raise Runtime Error ( "Lock file present, cannot continue" ) self . log . info ( "Couldn't get lock. Will retry in 5 seconds... (%s)" , str ( exc ) ) time . sleep ( 5 ) configs = self . get default configs ( ) if self . options . get ( 'config' , None ) : configs . append ( self . options [ 'config' ] ) self . core . load configs ( configs ) self . add user options ( ) self . core . load plugins ( ) if self . options . get ( 'ignore lock' , None ) : self . core . set option ( self . core . SECTION , self . IGNORE LOCKS , "1" )
def collect data ( self , end = False ) : data = get nowait from queue ( self . results ) stats = get nowait from queue ( self . stats results ) logger . debug ( "Data timestamps: %s" % [ d . get ( 'ts' ) for d in data ] ) logger . debug ( "Stats timestamps: %s" % [ d . get ( 'ts' ) for d in stats ] ) for item in data : ts = item [ 'ts' ] if ts in self . stat cache : # send items data item = item stat item = self . stat cache . pop ( ts ) self . notify listeners ( data item , stat item ) else : self . data cache [ ts ] = item for item in stats : ts = item [ 'ts' ] if ts in self . data cache : # send items data item = self . data cache . pop ( ts ) stat item = item self . notify listeners ( data item , stat item ) else : self . stat cache [ ts ] = item if end and len ( self . data cache ) > 0 : logger . info ( 'Timestamps without stats:' ) for ts , data item in sorted ( self . data cache . items ( ) , key = lambda i : i [ 0 ] ) : logger . info ( ts ) self . notify listeners ( data item , Stats Reader . stats item ( ts , 0 , 0 ) )
def notify listeners ( self , data , stats ) : for listener in self . listeners : listener . on aggregated data ( data , stats )
def clean markup ( self , orig str ) : for val in [ self . YELLOW , self . RED , self . RESET , self . CYAN , self . BG MAGENTA , self . WHITE , self . BG GREEN , self . GREEN , self . BG BROWN , self . RED DARK , self . MAGENTA , self . BG CYAN ] : orig str = orig str . replace ( val , '' ) return orig str
def uninstall ( self ) : if self . session : logger . info ( 'Waiting monitoring data...' ) self . session . terminate ( ) self . session . wait ( ) self . session = None log filename = "agent {host}.log" . format ( host = "localhost" ) data filename = "agent {host}.rawdata" . format ( host = "localhost" ) try : logger . info ( 'Saving monitoring artefacts from localhost' ) copyfile ( self . workdir + "/ agent.log" , log filename ) copyfile ( self . workdir + "/monitoring.rawdata" , data filename ) logger . info ( 'Deleting temp directory: %s' , self . workdir ) rmtree ( self . workdir ) except Exception : logger . error ( "Exception while uninstalling agent" , exc info = True ) logger . info ( "Removing agent from: localhost" ) return log filename , data filename
def install ( self ) : logger . info ( "Installing monitoring agent at %s@%s..." , self . username , self . host ) # create remote temp dir cmd = self . python + ' -c "import tempfile; print tempfile.mkdtemp();"' logger . info ( "Creating temp dir on %s" , self . host ) try : out , errors , err code = self . ssh . execute ( cmd ) except Exception : logger . error ( "Failed to install monitoring agent to %s" , self . host , exc info = True ) return None , None , None if errors : logger . error ( "[%s] error: '%s'" , self . host , errors ) logger . error ( "Cancelling agent installation on %s" , self . host ) return None , None , None if err code : logger . error ( "Failed to create remote dir via SSH at %s@%s, code %s: %s" % ( self . username , self . host , err code , out . strip ( ) ) ) return None , None , None remote dir = out . strip ( ) if remote dir : self . path [ 'AGENT REMOTE FOLDER' ] = remote dir self . agent remote folder = remote dir logger . debug ( "Remote dir at %s:%s" , self . host , self . path [ 'AGENT REMOTE FOLDER' ] ) # create collector config agent config = self . config . create collector config ( self . path [ 'AGENT REMOTE FOLDER' ] ) startup config = self . config . create startup config ( ) customs script = self . config . create custom exec script ( ) # trying to detect os version/architecture and get information about telegraf client # DO NOT DELETE indices in string format below. Python 2.6 does not # support string formatting without indices remote cmd = 'import os; print os.path.isfile("' + self . path [ 'TELEGRAF REMOTE PATH' ] + '")' cmd = self . python + ' -c \'{cmd}\'' . format ( cmd = remote cmd ) remote telegraf exists = "False" try : out , err , err code = self . ssh . execute ( cmd ) except Exception : logger . error ( "SSH execute error trying to check telegraf availability on host %s" , self . host , exc info = True ) else : if err : logger . error ( "[%s] error: '%s'" , self . host , errors ) if out . strip ( ) : remote telegraf exists = out . strip ( ) try : if remote telegraf exists in "True" : logger . debug ( 'Found telegraf client on %s..' , self . host ) else : logger . debug ( 'Not found telegraf client on %s, trying to install from tank. Copying..' , self . host ) if os . path . isfile ( self . path [ 'TELEGRAF LOCAL PATH' ] ) : self . ssh . send file ( self . path [ 'TELEGRAF LOCAL PATH' ] , self . path [ 'TELEGRAF REMOTE PATH' ] ) elif os . path . isfile ( "/usr/bin/telegraf" ) : self . ssh . send file ( '/usr/bin/telegraf' , self . path [ 'TELEGRAF REMOTE PATH' ] ) else : logger . error ( 'Telegraf binary not found neither on %s nor on localhost at specified path: %s\n' 'or install debian package: `telegraf`' , self . host , self . path [ 'TELEGRAF LOCAL PATH' ] ) return None , None , None self . ssh . send file ( os . path . join ( self . path [ 'AGENT LOCAL FOLDER' ] , self . AGENT FILENAME ) , os . path . join ( self . path [ 'AGENT REMOTE FOLDER' ] , self . AGENT FILENAME ) ) self . ssh . send file ( agent config , os . path . join ( self . path [ 'AGENT REMOTE FOLDER' ] , 'agent.cfg' ) ) self . ssh . send file ( startup config , os . path . join ( self . path [ 'AGENT REMOTE FOLDER' ] , 'agent startup.cfg' ) ) self . ssh . send file ( customs script , os . path . join ( self . path [ 'AGENT REMOTE FOLDER' ] , 'agent customs.sh' ) ) except Exception : logger . error ( "Failed to install agent on %s" , self . host , exc info = True ) return None , None , None return agent config , startup config , customs script
def uninstall ( self ) : log filename = "agent {host}.log" . format ( host = self . host ) data filename = "agent {host}.rawdata" . format ( host = self . host ) try : if self . session : self . session . send ( "stop\n" ) self . session . close ( ) self . session = None except Base Exception : logger . warning ( 'Unable to correctly stop monitoring agent - session is broken. Pay attention to agent log (%s).' , log filename , exc info = True ) else : try : self . ssh . get file ( os . path . join ( self . path [ 'AGENT REMOTE FOLDER' ] , " agent.log" ) , log filename ) self . ssh . get file ( os . path . join ( self . path [ 'AGENT REMOTE FOLDER' ] , "monitoring.rawdata" ) , data filename ) self . ssh . rm r ( self . path [ 'AGENT REMOTE FOLDER' ] ) except Exception : logger . error ( "Unable to get agent artefacts" , exc info = True ) self . kill agent ( ) return log filename , data filename
def add jmeter components ( self , jmx , jtl , variables ) : logger . debug ( "Original JMX: %s" , os . path . realpath ( jmx ) ) with open ( jmx , 'r' ) as src jmx : source lines = src jmx . readlines ( ) try : # In new Jmeter version (3.2 as example) Work Bench's plugin checkbox enabled by default # It totally crashes Yandex tank injection and raises XML Parse Exception closing = source lines . pop ( - 1 ) if "Work Bench Gui" in source lines [ - 5 ] : logger . info ( "Work Bench checkbox enabled...bypassing" ) last string count = 6 else : last string count = 2 while last string count > 0 : closing = source lines . pop ( - 1 ) + closing last string count -= 1 logger . debug ( "Closing statement: %s" , closing ) except Exception as exc : raise Runtime Error ( "Failed to find the end of JMX XML: %s" % exc ) udv tpl = resource string ( name , 'config/jmeter var template.xml' ) udv set = [ ] for var name , var value in variables . iteritems ( ) : udv set . append ( udv tpl % ( var name , var name , var value ) ) udv = "\n" . join ( udv set ) if self . jmeter ver >= 2.13 : save connect = '<connect Time>true</connect Time>' else : save connect = '' if self . ext log in [ 'errors' , 'all' ] : level map = { 'errors' : 'true' , 'all' : 'false' } tpl resource = 'jmeter writer ext.xml' tpl args = { 'jtl' : self . jtl file , 'udv' : udv , 'ext log' : self . ext log file , 'ext level' : level map [ self . ext log ] , 'save connect' : save connect } else : tpl resource = 'jmeter writer.xml' tpl args = { 'jtl' : self . jtl file , 'udv' : udv , 'save connect' : save connect } tpl = resource string ( name , 'config/' + tpl resource ) try : new jmx = self . core . mkstemp ( '.jmx' , 'modified ' , os . path . dirname ( os . path . realpath ( jmx ) ) ) except OS Error as exc : logger . debug ( "Can't create modified jmx near original: %s" , exc ) new jmx = self . core . mkstemp ( '.jmx' , 'modified ' ) logger . debug ( "Modified JMX: %s" , new jmx ) with open ( new jmx , "wb" ) as fh : fh . write ( '' . join ( source lines ) ) fh . write ( tpl % tpl args ) fh . write ( closing ) return new jmx
def terminate ( self ) : if self . stderr file : self . stderr file . close ( ) if not self . process : return waitfor = time . time ( ) + PROCESS KILL TIMEOUT while time . time ( ) < waitfor : try : self . process . terminate ( ) except Environment Error as e : if e . errno != errno . ESRCH : LOGGER . warning ( "Failed to terminate process '{}': {}" . format ( self . cmd , e ) ) return time . sleep ( 0.1 ) try : self . process . kill ( ) except Environment Error as e : if e . errno != errno . ESRCH : LOGGER . warning ( "Failed to kill process '{}': {}" . format ( self . cmd , e ) ) return
def read data ( self , lines ) : results = [ ] for line in lines : timestamp , rps , instances = line . split ( "\t" ) curr ts = int ( float ( timestamp ) ) # We allow floats here, but tank expects only seconds if self . last ts < curr ts : self . last ts = curr ts results . append ( self . stats item ( self . last ts , float ( rps ) , float ( instances ) ) ) return results
def create criterion ( self , criterion str ) : parsed = criterion str . split ( "(" ) type str = parsed [ 0 ] . strip ( ) . lower ( ) parsed [ 1 ] = parsed [ 1 ] . split ( ")" ) [ 0 ] . strip ( ) for criterion class in self . custom criterions : if criterion class . get type string ( ) == type str : return criterion class ( self , parsed [ 1 ] ) raise Value Error ( "Unsupported autostop criterion type: %s" % criterion str )
def getconfig ( self , filename , target hint ) : try : tree = self . parse xml ( filename ) except IO Error as exc : logger . error ( "Error loading config: %s" , exc ) raise Runtime Error ( "Can't read monitoring config %s" % filename ) hosts = tree . findall ( 'Host' ) config = [ ] for host in hosts : host config = self . get host config ( host , target hint ) config . append ( host config ) return config
def check disk ( self ) : cmd = "sh -c \"df --no-sync -m -P -l -x fuse -x tmpfs -x devtmpfs -x davfs -x nfs " cmd += self . core . artifacts base dir cmd += " | tail -n 1 | awk '{print \$4}' \"" res = execute ( cmd , True , 0.1 , True ) logging . debug ( "Result: %s" , res ) if not len ( res [ 1 ] ) : self . log . debug ( "No disk usage info: %s" , res [ 2 ] ) return disk free = res [ 1 ] self . log . debug ( "Disk free space: %s/%s" , disk free . strip ( ) , self . disk limit ) if int ( disk free . strip ( ) ) < self . disk limit : raise Runtime Error ( "Not enough local resources: disk space less than %s MB in %s: %s MB" % ( self . disk limit , self . core . artifacts base dir , int ( disk free . strip ( ) ) ) )
def check mem ( self ) : mem free = psutil . virtual memory ( ) . available / 2 ** 20 self . log . debug ( "Memory free: %s/%s" , mem free , self . mem limit ) if mem free < self . mem limit : raise Runtime Error ( "Not enough resources: free memory less " "than %s MB: %s MB" % ( self . mem limit , mem free ) )
def get terminal size ( ) : default size = ( 30 , 120 ) env = os . environ def ioctl gwinsz ( file d ) : try : sizes = struct . unpack ( 'hh' , fcntl . ioctl ( file d , termios . TIOCGWINSZ , '1234' ) ) except Exception : sizes = default size return sizes sizes = ioctl gwinsz ( 0 ) or ioctl gwinsz ( 1 ) or ioctl gwinsz ( 2 ) if not sizes : try : file d = os . open ( os . ctermid ( ) , os . O RDONLY ) sizes = ioctl gwinsz ( file d ) os . close ( file d . fileno ( ) ) except Exception : pass if not sizes : try : sizes = ( env [ 'LINES' ] , env [ 'COLUMNS' ] ) except Exception : sizes = default size return int ( sizes [ 1 ] ) , int ( sizes [ 0 ] )
def get right line ( self , widget output ) : right line = '' if widget output : right line = widget output . pop ( 0 ) if len ( right line ) > self . right panel width : right line plain = self . markup . clean markup ( right line ) if len ( right line plain ) > self . right panel width : right line = right line [ : self . right panel width ] + self . markup . RESET return right line
def truncate ( self , line arr , max width ) : def is space ( chunk ) : return all ( [ True if i == ' ' else False for i in chunk ] ) def is empty ( chunks , markups ) : result = [ ] for chunk in chunks : if chunk in markups : result . append ( True ) elif is space ( chunk ) : result . append ( True ) else : result . append ( False ) return all ( result ) left = max width result = '' markups = self . markup . get markup vars ( ) for num , chunk in enumerate ( line arr ) : if chunk in markups : result += chunk else : if left > 0 : if len ( chunk ) <= left : result += chunk left -= len ( chunk ) else : leftover = ( chunk [ left : ] , ) + line arr [ num + 1 : ] was cut = not is empty ( leftover , markups ) if was cut : result += chunk [ : left - 1 ] + self . markup . RESET + u'\u2026' else : result += chunk [ : left ] left = 0 return result
def render screen ( self ) : self . term width , self . term height = get terminal size ( ) self . log . debug ( "Terminal size: %sx%s" , self . term width , self . term height ) self . right panel width = int ( ( self . term width - len ( self . RIGHT PANEL SEPARATOR ) ) * ( float ( self . info panel percent ) / 100 ) ) - 1 if self . right panel width > 0 : self . left panel width = self . term width - self . right panel width - len ( self . RIGHT PANEL SEPARATOR ) - 2 else : self . right panel width = 0 self . left panel width = self . term width - 1 self . log . debug ( "Left/right panels width: %s/%s" , self . left panel width , self . right panel width ) widget output = [ ] if self . right panel width : widget output = [ ] self . log . debug ( "There are %d info widgets" % len ( self . info widgets ) ) for index , widget in sorted ( self . info widgets . iteritems ( ) , key = lambda item : ( item [ 1 ] . get index ( ) , item [ 0 ] ) ) : self . log . debug ( "Rendering info widget #%s: %s" , index , widget ) widget out = widget . render ( self ) . strip ( ) if widget out : widget output += widget out . split ( "\n" ) widget output += [ "" ] left lines = self . render left panel ( ) self . log . debug ( "Composing final screen output" ) output = [ ] for line no in range ( 1 , self . term height ) : line = " " if line no > 1 and left lines : left line = left lines . pop ( 0 ) left line plain = self . markup . clean markup ( left line ) left line += ( ' ' * ( self . left panel width - len ( left line plain ) ) ) line += left line else : line += ' ' * self . left panel width if self . right panel width : line += self . markup . RESET line += self . markup . WHITE line += self . RIGHT PANEL SEPARATOR line += self . markup . RESET right line = self . get right line ( widget output ) line += right line output . append ( line ) return self . markup . new line . join ( output ) + self . markup . new line
def add info widget ( self , widget ) : index = widget . get index ( ) while index in self . info widgets . keys ( ) : index += 1 self . info widgets [ widget . get index ( ) ] = widget
def fill rectangle ( self , prepared ) : result = [ ] width = max ( [ self . clean len ( line ) for line in prepared ] ) for line in prepared : spacer = ' ' * ( width - self . clean len ( line ) ) result . append ( line + ( self . screen . markup . RESET , spacer ) ) return ( width , result )
def clean len ( self , line ) : if isinstance ( line , basestring ) : return len ( self . screen . markup . clean markup ( line ) ) elif isinstance ( line , tuple ) or isinstance ( line , list ) : markups = self . screen . markup . get markup vars ( ) length = 0 for i in line : if i not in markups : length += len ( i ) return length
def add info widget ( self , widget ) : if not self . screen : self . log . debug ( "No screen instance to add widget" ) else : self . screen . add info widget ( widget )
def clean markup ( self , orig str ) : for val in self . get markup vars ( ) : orig str = orig str . replace ( val , '' ) return orig str
def make writer request ( self , params = None , json = None , http method = "POST" , trace = False ) : request = requests . Request ( http method , self . writer url , params = params , json = json , headers = { 'User-Agent' : self . user agent } ) ids = id gen ( str ( uuid . uuid4 ( ) ) ) network timeouts = self . network timeouts ( ) maintenance timeouts = self . maintenance timeouts ( ) while True : try : response = self . send single request ( request , ids . next ( ) , trace = trace ) return response except ( Timeout , Connection Error , Protocol Error ) : logger . warn ( traceback . format exc ( ) ) try : timeout = next ( network timeouts ) logger . warn ( "Network error, will retry in %ss..." % timeout ) time . sleep ( timeout ) continue except Stop Iteration : raise self . Network Error ( ) except self . Under Maintenance as e : try : timeout = next ( maintenance timeouts ) logger . warn ( "Writer is under maintenance, will retry in %ss..." % timeout ) time . sleep ( timeout ) continue except Stop Iteration : raise e
def load plugins ( self ) : logger . info ( "Loading plugins..." ) for ( plugin name , plugin path , plugin cfg ) in self . config . plugins : logger . debug ( "Loading plugin %s from %s" , plugin name , plugin path ) if plugin path == "yandextank.plugins.Overload" : logger . warning ( "Deprecated plugin name: 'yandextank.plugins.Overload'\n" "There is a new generic plugin now.\n" "Correcting to 'yandextank.plugins.Data Uploader overload'" ) plugin path = "yandextank.plugins.Data Uploader overload" try : plugin = il . import module ( plugin path ) except Import Error : logger . warning ( 'Plugin name %s path %s import error' , plugin name , plugin path ) logger . debug ( 'Plugin name %s path %s import error' , plugin name , plugin path , exc info = True ) raise try : instance = getattr ( plugin , 'Plugin' ) ( self , cfg = plugin cfg , name = plugin name ) except Attribute Error : logger . warning ( 'Plugin %s classname should be `Plugin`' , plugin name ) raise else : self . register plugin ( self . PLUGIN PREFIX + plugin name , instance ) logger . debug ( "Plugin instances: %s" , self . plugins )
def get plugin of type ( self , plugin class ) : logger . debug ( "Searching for plugin: %s" , plugin class ) matches = [ plugin for plugin in self . plugins . values ( ) if isinstance ( plugin , plugin class ) ] if matches : if len ( matches ) > 1 : logger . debug ( "More then one plugin of type %s found. Using first one." , plugin class ) return matches [ - 1 ] else : raise Key Error ( "Requested plugin type not found: %s" % plugin class )
def get plugins of type ( self , plugin class ) : logger . debug ( "Searching for plugins: %s" , plugin class ) matches = [ plugin for plugin in self . plugins . values ( ) if isinstance ( plugin , plugin class ) ] if matches : return matches else : raise Key Error ( "Requested plugin type not found: %s" % plugin class )
def collect file ( self , filename , keep original = False ) : dest = self . artifacts dir + '/' + os . path . basename ( filename ) logger . debug ( "Collecting file: %s to %s" , filename , dest ) if not filename or not os . path . exists ( filename ) : logger . warning ( "File not found to collect: %s" , filename ) return if os . path . exists ( dest ) : # FIXME: 3 find a way to store artifacts anyway logger . warning ( "File already exists: %s" , dest ) return if keep original : shutil . copy ( filename , self . artifacts dir ) else : shutil . move ( filename , self . artifacts dir ) os . chmod ( dest , 0o644 )
def add artifact file ( self , filename , keep original = False ) : if filename : logger . debug ( "Adding artifact file to collect (keep=%s): %s" , keep original , filename ) self . artifact files [ filename ] = keep original
def load files ( self , configs ) : logger . debug ( "Reading configs: %s" , configs ) config filenames = [ resource . resource filename ( config ) for config in configs ] try : self . config . read ( config filenames ) except Exception as ex : logger . error ( "Can't load configs: %s" , ex ) raise ex
def flush ( self , filename = None ) : if not filename : filename = self . file if filename : with open ( filename , 'w' ) as handle : self . config . write ( handle )
def get options ( self , section , prefix = '' ) : res = [ ] try : for option in self . config . options ( section ) : if not prefix or option . find ( prefix ) == 0 : res += [ ( option [ len ( prefix ) : ] , self . config . get ( section , option ) ) ] except Config Parser . No Section Error as ex : logger . warning ( "No section: %s" , ex ) logger . debug ( "Section: [%s] prefix: '%s' options:\n%s" , section , prefix , res ) return res
def find sections ( self , prefix ) : res = [ ] for section in self . config . sections ( ) : if section . startswith ( prefix ) : res . append ( section ) return res
def decode stat data ( self , chunk ) : for date str , statistics in chunk . iteritems ( ) : date obj = datetime . datetime . strptime ( date str . split ( "." ) [ 0 ] , '%Y-%m-%d %H:%M:%S' ) chunk date = int ( time . mktime ( date obj . timetuple ( ) ) ) instances = 0 for benchmark name , benchmark in statistics . iteritems ( ) : if not benchmark name . startswith ( "benchmark io" ) : continue for method , meth obj in benchmark . iteritems ( ) : if "mmtasks" in meth obj : instances += meth obj [ "mmtasks" ] [ 2 ] offset = chunk date - 1 - self . start time reqps = 0 if 0 <= offset < len ( self . phantom info . steps ) : reqps = self . phantom info . steps [ offset ] [ 0 ] yield self . stats item ( chunk date - 1 , instances , reqps )
def prepare ( self ) : # Parse config agent configs = [ ] if self . config : agent configs = self . config manager . getconfig ( self . config , self . default target ) # Creating agent for hosts for config in agent configs : if config [ 'host' ] in [ 'localhost' , '127.0.0.1' , '::1' ] : client = self . clients [ 'localhost' ] ( config , self . old style configs , kill old = self . kill old ) else : client = self . clients [ 'ssh' ] ( config , self . old style configs , timeout = 5 , kill old = self . kill old ) logger . debug ( 'Installing monitoring agent. Host: %s' , client . host ) agent config , startup config , customs script = client . install ( ) if agent config : self . agents . append ( client ) self . artifact files . append ( agent config ) if startup config : self . artifact files . append ( startup config ) if customs script : self . artifact files . append ( customs script )
def poll ( self ) : start time = time . time ( ) for agent in self . agents : for collect in agent . reader : # don't crush if trash or traceback came from agent to stdout if not collect : return 0 for chunk in collect : ts , prepared results = chunk if self . load start time and int ( ts ) >= self . load start time : ready to send = { "timestamp" : int ( ts ) , "data" : { self . hash hostname ( agent . host ) : { "comment" : agent . config . comment , "metrics" : prepared results } } } self . collected data . append ( ready to send ) logger . debug ( 'Polling/decoding agents data took: %.2fms' , ( time . time ( ) - start time ) * 1000 ) collected data length = len ( self . collected data ) if not self . first data received and self . collected data : self . first data received = True logger . info ( "Monitoring received first data." ) else : self . send collected data ( ) return collected data length
def send collected data ( self ) : data = self . collected data self . collected data = [ ] for listener in self . listeners : # deep copy to ensure each listener gets it's own copy listener . monitoring data ( copy . deepcopy ( data ) )
def decode agents data ( self , block ) : collect = [ ] if block : for chunk in block . split ( '\n' ) : try : if chunk : prepared results = { } jsn = json . loads ( chunk ) for ts , values in jsn . iteritems ( ) : for key , value in values . iteritems ( ) : # key sample: diskio-sda1 io time # key group sample: diskio # key name sample: io time try : key group , key name = key . split ( ' ' ) [ 0 ] . split ( '-' ) [ 0 ] , ' ' . join ( key . split ( ' ' ) [ 1 : ] ) except : # noqa: E722 key group , key name = key . split ( ' ' ) [ 0 ] , ' ' . join ( key . split ( ' ' ) [ 1 : ] ) if key group in decoder . diff metrics . keys ( ) : if key name in decoder . diff metrics [ key group ] : decoded key = decoder . find common names ( key ) if self . prev check : try : value = jsn [ ts ] [ key ] - self . prev check [ key ] except Key Error : logger . debug ( 'There is no diff value for metric %s.\n' 'Timestamp: %s. Is it initial data?' , key , ts , exc info = True ) value = 0 prepared results [ decoded key ] = value else : decoded key = decoder . find common names ( key ) prepared results [ decoded key ] = value else : decoded key = decoder . find common names ( key ) prepared results [ decoded key ] = value self . prev check = jsn [ ts ] collect . append ( ( ts , prepared results ) ) except Value Error : logger . error ( 'Telegraf agent send trash to output: %s' , chunk ) logger . debug ( 'Telegraf agent data block w/ trash: %s' , exc info = True ) return [ ] except Base Exception : logger . error ( 'Exception trying to parse agent data: %s' , chunk , exc info = True ) return [ ] if collect : return collect
async def close ( self ) : if self . ws is not None : await self . ws . close ( ) if self . polygon is not None : await self . polygon . close ( )
def submit order ( self , symbol , qty , side , type , time in force , limit price = None , stop price = None , client order id = None ) : params = { 'symbol' : symbol , 'qty' : qty , 'side' : side , 'type' : type , 'time in force' : time in force , } if limit price is not None : params [ 'limit price' ] = limit price if stop price is not None : params [ 'stop price' ] = stop price if client order id is not None : params [ 'client order id' ] = client order id resp = self . post ( '/orders' , params ) return Order ( resp )
def get position ( self , symbol ) : resp = self . get ( '/positions/{}' . format ( symbol ) ) return Position ( resp )
def list assets ( self , status = None , asset class = None ) : params = { 'status' : status , 'assert class' : asset class , } resp = self . get ( '/assets' , params ) return [ Asset ( o ) for o in resp ]
def construct event logger ( event record callback ) : check . callable param ( event record callback , 'event record callback' ) return construct single handler logger ( 'event-logger' , DEBUG , Structured Logger Handler ( lambda logger message : event record callback ( construct event record ( logger message ) ) ) , )
def construct json event logger ( json path ) : check . str param ( json path , 'json path' ) return construct single handler logger ( "json-event-record-logger" , DEBUG , Json Event Logger Handler ( json path , lambda record : construct event record ( Structured Logger Message ( name = record . name , message = record . msg , level = record . levelno , meta = record . dagster meta , record = record , ) ) , ) , )
def format config for graphql ( config ) : def format config subdict ( config , current indent = 0 ) : check . dict param ( config , 'config' , key type = str ) printer = Indenting String Io Printer ( indent level = 2 , current indent = current indent ) printer . line ( '{' ) n elements = len ( config ) for i , key in enumerate ( sorted ( config , key = lambda x : x [ 0 ] ) ) : value = config [ key ] with printer . with indent ( ) : formatted value = ( format config item ( value , current indent = printer . current indent ) . lstrip ( ' ' ) . rstrip ( '\n' ) ) printer . line ( '{key}: {formatted value}{comma}' . format ( key = key , formatted value = formatted value , comma = ',' if i != n elements - 1 else '' , ) ) printer . line ( '}' ) return printer . read ( ) def format config sublist ( config , current indent = 0 ) : printer = Indenting String Io Printer ( indent level = 2 , current indent = current indent ) printer . line ( '[' ) n elements = len ( config ) for i , value in enumerate ( config ) : with printer . with indent ( ) : formatted value = ( format config item ( value , current indent = printer . current indent ) . lstrip ( ' ' ) . rstrip ( '\n' ) ) printer . line ( '{formatted value}{comma}' . format ( formatted value = formatted value , comma = ',' if i != n elements - 1 else '' ) ) printer . line ( ']' ) return printer . read ( ) def format config item ( config , current indent = 0 ) : printer = Indenting String Io Printer ( indent level = 2 , current indent = current indent ) if isinstance ( config , dict ) : return format config subdict ( config , printer . current indent ) elif isinstance ( config , list ) : return format config sublist ( config , printer . current indent ) elif isinstance ( config , bool ) : return repr ( config ) . lower ( ) else : return repr ( config ) . replace ( '\'' , '"' ) check . dict param ( config , 'config' , key type = str ) if not isinstance ( config , dict ) : check . failed ( 'Expected a dict to format as config, got: {item}' . format ( item = repr ( config ) ) ) return format config subdict ( config )
def execute pipeline through queue ( repository info , pipeline name , solid subset , environment dict , run id , message queue , reexecution config , step keys to execute , ) : message queue . put ( Process Started Sentinel ( os . getpid ( ) ) ) run config = Run Config ( run id , event callback = message queue . put , executor config = In Process Executor Config ( raise on error = False ) , reexecution config = reexecution config , step keys to execute = step keys to execute , ) repository container = Repository Container ( repository info ) if repository container . repo error : message queue . put ( Multiprocessing Error ( serializable error info from exc info ( repository container . repo error ) ) ) return try : result = execute pipeline ( repository container . repository . get pipeline ( pipeline name ) . build sub pipeline ( solid subset ) , environment dict , run config = run config , ) return result except : # pylint: disable=W0702 error info = serializable error info from exc info ( sys . exc info ( ) ) message queue . put ( Multiprocessing Error ( error info ) ) finally : message queue . put ( Multiprocessing Done ( ) ) message queue . close ( )
def join ( self ) : while True : with self . processes lock : if not self . processes and self . processing semaphore . locked ( ) : return True gevent . sleep ( 0.1 )
def build ( self , pipeline def , artifacts persisted ) : # Construct dependency dictionary deps = { step . key : set ( ) for step in self . steps } for step in self . steps : for step input in step . step inputs : deps [ step . key ] . add ( step input . prev output handle . step key ) step dict = { step . key : step for step in self . steps } return Execution Plan ( pipeline def , step dict , deps , artifacts persisted )
def construct publish comands ( additional steps = None , nightly = False ) : publish commands = ( [ 'rm -rf dist' ] + ( additional steps if additional steps else [ ] ) + [ 'python setup.py sdist bdist wheel{nightly}' . format ( nightly = ' --nightly' if nightly else '' ) , 'twine upload dist/*' , ] ) return publish commands
def block ( self , text , prefix = '' ) : wrapper = Text Wrapper ( width = self . line length - len ( self . current indent str ) , initial indent = prefix , subsequent indent = prefix , break long words = False , break on hyphens = False , ) for line in wrapper . wrap ( text ) : self . line ( line )
def define shared fields ( ) : clustering fields = Field ( List ( String ) , description = , is optional = True , ) create disposition = Field ( BQ Create Disposition , description = , is optional = True , ) destination encryption configuration = Field ( String , description = , is optional = True , ) schema update options = Field ( List ( BQ Schema Update Option ) , description = , is optional = True , ) time partitioning = Field ( Dict ( fields = { 'expiration ms' : Field ( Int , description = , is optional = True , ) , 'field' : Field ( String , description = , is optional = True , ) , 'require partition filter' : Field ( Bool , description = , is optional = True , ) , } ) , description = 'Specifies time-based partitioning for the destination table.' , is optional = True , ) write disposition = Field ( BQ Write Disposition , description = , is optional = True , ) return { 'clustering fields' : clustering fields , 'create disposition' : create disposition , 'destination encryption configuration' : destination encryption configuration , 'schema update options' : schema update options , 'time partitioning' : time partitioning , 'write disposition' : write disposition , }
def mkdir p ( newdir , mode = 0o777 ) : try : os . makedirs ( newdir , mode ) except OS Error as err : # Reraise the error unless it's about an already existing directory if err . errno != errno . EEXIST or not os . path . isdir ( newdir ) : raise
def success ( self ) : any success = False for step event in itertools . chain ( self . input expectations , self . output expectations , self . transforms ) : if step event . event type == Dagster Event Type . STEP FAILURE : return False if step event . event type == Dagster Event Type . STEP SUCCESS : any success = True return any success
def skipped ( self ) : return all ( [ step event . event type == Dagster Event Type . STEP SKIPPED for step event in itertools . chain ( self . input expectations , self . output expectations , self . transforms ) ] )
def failure data ( self ) : for result in itertools . chain ( self . input expectations , self . output expectations , self . transforms ) : if result . event type == Dagster Event Type . STEP FAILURE : return result . step failure data
def is valid dataset ( config value ) : return re . match ( # regex matches: project.table -- OR -- table r'^' + RE PROJECT + r'\.' + RE DS TABLE + r'$|^' + RE DS TABLE + r'$' , config value , )
def is valid table ( config value ) : return re . match ( r'^' + RE PROJECT #  project + r'\.' #       . + RE DS TABLE # dataset + r'\.' #       . + RE DS TABLE # table + r'$|^' #      -- OR -- + RE DS TABLE # dataset + r'\.' #       . + RE DS TABLE # table + r'$' , config value , )
def coalesce execution steps ( execution plan ) : solid order = coalesce solid order ( execution plan ) steps = defaultdict ( list ) for solid name , solid steps in itertools . groupby ( execution plan . topological steps ( ) , lambda x : x . solid name ) : steps [ solid name ] += list ( solid steps ) return Ordered Dict ( [ ( solid name , steps [ solid name ] ) for solid name in solid order ] )
def create cursor ( self , name = None ) : return Cursor ( self . client connection , self . connection , self . djongo connection )
def close ( self ) : if self . connection : with self . wrap database errors : self . connection . client . close ( )
def make mdl ( model , model dict ) : for field name in model dict : field = model . meta . get field ( field name ) model dict [ field name ] = field . to python ( model dict [ field name ] ) return model ( * * model dict )
def formfield ( self , * * kwargs ) : defaults = { 'form class' : Array Form Field , 'model container' : self . model container , 'model form class' : self . model form class , 'name' : self . attname , 'mdl form kw l' : self . model form kwargs l } defaults . update ( kwargs ) return super ( ) . formfield ( * * defaults )
def apply rel filters ( self , queryset ) : queryset . add hints ( instance = self . instance ) if self . db : queryset = queryset . using ( self . db ) queryset = queryset . filter ( * * self . core filters ) return queryset
def calc c ( self , a1 , a2 , r1 , r2 ) : if r1 == 0.0 and r2 == 0.0 : # Find the limits of C1 and C2 as r1 -> 0 and r2 -> 0 # Since the b-value must be the same and r1 = r2, # we have A1(r1, b1) = A2(r2, b2) = A, # then the limits for both C1 and C2 are A. return a1 , a2 div = 1 / ( r1 + r2 ) c1 = ( a1 * r2 + a2 * r1 ) * div c2 = ( a1 * r1 + a2 * r2 ) * div return c1 , c2
def clear ( self ) : self . reg = np . zeros ( ( self . m , ) , dtype = np . int8 )
def index ( self ) : for i , hashtable in enumerate ( self . hashtables ) : self . sorted hashtables [ i ] = [ H for H in hashtable . keys ( ) ] self . sorted hashtables [ i ] . sort ( )
async def close ( self ) : async with self . lock : for t in self . hashtables : await t . close ( ) if self . keys is not None : await self . keys . close ( ) self . initialized = False
def parse scoped selector ( scoped selector ) : # Conver Macro (%scope/name) to (scope/name/macro.value) if scoped selector [ 0 ] == '%' : if scoped selector . endswith ( '.value' ) : err str = '{} is invalid cannot use % and end with .value' raise Value Error ( err str . format ( scoped selector ) ) scoped selector = scoped selector [ 1 : ] + '/macro.value' scope selector list = scoped selector . rsplit ( '/' , 1 ) scope = '' . join ( scope selector list [ : - 1 ] ) selector = scope selector list [ - 1 ] return scope , selector
def advance one line ( self ) : current line = self . current token . line number while current line == self . current token . line number : self . current token = Config Parser . Token ( * next ( self . token generator ) )
def augment exception message and reraise ( exception , message ) : class Exception Proxy ( type ( exception ) ) : """Acts as a proxy for an exception with an augmented message.""" module = type ( exception ) . module def init ( self ) : pass def getattr ( self , attr name ) : return getattr ( exception , attr name ) def str ( self ) : return str ( exception ) + message Exception Proxy . name = type ( exception ) . name proxy = Exception Proxy ( ) if six . PY3 : Exception Proxy . qualname = type ( exception ) . qualname six . raise from ( proxy . with traceback ( exception . traceback ) , None ) else : six . reraise ( proxy , None , sys . exc info ( ) [ 2 ] )
def markdownify operative config str ( self , string ) : # TODO: Total hack below. Implement more principled formatting. def process ( line ) : """Convert a single line to markdown format.""" if not line . startswith ( '#' ) : return '    ' + line line = line [ 2 : ] if line . startswith ( '====' ) : return '' if line . startswith ( 'None' ) : return '    # None.' if line . endswith ( ':' ) : return '#### ' + line return line output lines = [ ] for line in string . splitlines ( ) : procd line = process ( line ) if procd line is not None : output lines . append ( procd line ) return '\n' . join ( output lines )
def after create session ( self , session = None , coord = None ) : config str = config . operative config str ( ) if not tf . gfile . Is Directory ( self . output dir ) : tf . gfile . Make Dirs ( self . output dir ) global step val = 0 if session is not None : global step = tf . train . get global step ( ) if global step is not None : global step val = session . run ( global step ) filename = '%s-%s.gin' % ( self . base name , global step val ) config path = os . path . join ( self . output dir , filename ) with tf . gfile . G File ( config path , 'w' ) as f : f . write ( config str ) if self . summarize config : md config str = self . markdownify operative config str ( config str ) summary metadata = summary pb2 . Summary Metadata ( ) summary metadata . plugin data . plugin name = 'text' summary metadata . plugin data . content = b'{}' text tensor = tf . make tensor proto ( md config str ) summary = summary pb2 . Summary ( ) summary . value . add ( tag = 'gin/' + self . base name , tensor = text tensor , metadata = summary metadata ) if not self . summary writer : # Creating the File Writer also creates the events file, so it should be # done here (where it is most likely to only occur on chief workers), as # opposed to in the constructor. self . summary writer = tf . summary . File Writer Cache . get ( self . output dir ) self . summary writer . add summary ( summary , global step val ) self . summary writer . flush ( )
def find class construction fn ( cls ) : for base in type . mro ( cls ) : if ' init ' in base . dict : return base . init if ' new ' in base . dict : return base . new
def ensure wrappability ( fn ) : # Handle "wrapped descriptor" and "method-wrapper" types. if isinstance ( fn , ( type ( object . init ) , type ( object . call ) ) ) : # pylint: disable=unnecessary-lambda wrappable fn = lambda * args , * * kwargs : fn ( * args , * * kwargs ) wrappable fn . name = fn . name wrappable fn . doc = fn . doc wrappable fn . module = '' # These types have no  module , sigh. wrappable fn . wrapped = fn return wrappable fn # Otherwise we're good to go... return fn
def get cached arg spec ( fn ) : arg spec = ARG SPEC CACHE . get ( fn ) if arg spec is None : arg spec fn = inspect . getfullargspec if six . PY3 else inspect . getargspec try : arg spec = arg spec fn ( fn ) except Type Error : # `fn` might be a callable object. arg spec = arg spec fn ( fn . call ) ARG SPEC CACHE [ fn ] = arg spec return arg spec
def get supplied positional parameter names ( fn , args ) : arg spec = get cached arg spec ( fn ) # May be shorter than len(args) if args contains vararg (*args) arguments. return arg spec . args [ : len ( args ) ]
def get all positional parameter names ( fn ) : arg spec = get cached arg spec ( fn ) args = arg spec . args if arg spec . defaults : args = args [ : - len ( arg spec . defaults ) ] return args
def parse value ( value ) : if not isinstance ( value , six . string types ) : raise Value Error ( 'value ({}) should be a string type.' . format ( value ) ) return config parser . Config Parser ( value , Parser Delegate ( ) ) . parse value ( )
def iterate flattened values ( value ) : if isinstance ( value , six . string types ) : yield value return if isinstance ( value , collections . Mapping ) : value = collections . Values View ( value ) if isinstance ( value , collections . Iterable ) : for nested value in value : for nested nested value in iterate flattened values ( nested value ) : yield nested nested value yield value
def get all matches ( self , partial selector ) : matching selectors = self . matching selectors ( partial selector ) return [ self . selector map [ selector ] for selector in matching selectors ]
def sp search query ( query ) : result = [ ] for ( field , values ) in query . items ( ) : field = SEARCH FIELD MAP . get ( field , field ) if field is None : continue for value in values : if field == 'year' : value = transform year ( value ) if value is not None : result . append ( '%s:%d' % ( field , value ) ) elif field == 'any' : result . append ( '"%s"' % value ) else : result . append ( '%s:"%s"' % ( field , value ) ) return ' ' . join ( result )
def parse retry after ( self , response ) : value = response . headers . get ( 'Retry-After' ) if not value : seconds = 0 elif re . match ( r'^\s*[0-9]+\s*$' , value ) : seconds = int ( value ) else : date tuple = email . utils . parsedate ( value ) if date tuple is None : seconds = 0 else : seconds = time . mktime ( date tuple ) - time . time ( ) return max ( 0 , seconds )
def set default headers ( self , * args , * * kwargs ) : self . set header ( 'Access-Control-Allow-Origin' , '*' ) self . set header ( 'Access-Control-Allow-Headers' , 'Origin, X-Requested-With, Content-Type, Accept' ) self . set header ( 'Access-Control-Allow-Methods' , 'GET, HEAD, PUT, POST, DELETE' )
def prepare ( self ) : host = self . request . headers . get ( 'Host' , None ) if host is not None and host in self . hosts : return raise tornado . web . HTTP Error ( 403 )
def start ( self ) : self . service info = Service Info ( ' webthing. tcp.local.' , '{}. webthing. tcp.local.' . format ( self . name ) , address = socket . inet aton ( get ip ( ) ) , port = self . port , properties = { 'path' : '/' , } , server = '{}.local.' . format ( socket . gethostname ( ) ) ) self . zeroconf = Zeroconf ( ) self . zeroconf . register service ( self . service info ) self . server . listen ( self . port ) tornado . ioloop . IO Loop . current ( ) . start ( )
def start ( self ) : self . status = 'pending' self . thing . action notify ( self ) self . perform action ( ) self . finish ( )
def finish ( self ) : self . status = 'completed' self . time completed = timestamp ( ) self . thing . action notify ( self )
def update ( self , * * fields ) : # build up the query to execute self . for write = True if django . VERSION >= ( 2 , 0 ) : query = self . query . chain ( Update Query ) else : query = self . query . clone ( Update Query ) query . annotations = None query . add update values ( fields ) # build the compiler for for the query connection = django . db . connections [ self . db ] compiler = Postgres Returning Update Compiler ( query , connection , self . db ) # execute the query with transaction . atomic ( using = self . db , savepoint = False ) : rows = compiler . execute sql ( CURSOR ) self . result cache = None # send out a signal for each row for row in rows : signals . update . send ( self . model , pk = row [ 0 ] ) # the original update(..) returns the amount of rows # affected, let's do the same return len ( rows )
def on model save ( sender , * * kwargs ) : created , instance = kwargs [ 'created' ] , kwargs [ 'instance' ] if created : signals . create . send ( sender , pk = instance . pk ) else : signals . update . send ( sender , pk = instance . pk )
def on model delete ( sender , * * kwargs ) : instance = kwargs [ 'instance' ] signals . delete . send ( sender , pk = instance . pk )
def resolve expression ( self , * args , * * kwargs ) : result = dict ( ) for key , value in self . value . items ( ) : if hasattr ( value , 'resolve expression' ) : result [ key ] = value . resolve expression ( * args , * * kwargs ) else : result [ key ] = value return H Store Value ( result )
def as sql ( self , compiler , connection ) : qn = compiler . quote name unless alias return "%s.%s->'%s'" % ( qn ( self . alias ) , qn ( self . target . column ) , self . hstore key ) , [ ]
def relabeled clone ( self , relabels ) : return self . class ( relabels . get ( self . alias , self . alias ) , self . target , self . hstore key , self . output field )
def as sql ( self , compiler , connection ) : sql , params = super ( ) . as sql ( compiler , connection ) return 'EXTRACT(epoch FROM {})' . format ( sql ) , params
def create model ( self , model ) : for field in model . meta . local fields : if not isinstance ( field , H Store Field ) : continue self . add field ( model , field )
def delete model ( self , model ) : for field in model . meta . local fields : if not isinstance ( field , H Store Field ) : continue self . remove field ( model , field )
def alter db table ( self , model , old db table , new db table ) : for field in model . meta . local fields : if not isinstance ( field , H Store Field ) : continue for key in self . iterate required keys ( field ) : self . rename hstore required ( old db table , new db table , field , field , key )
def add field ( self , model , field ) : for key in self . iterate required keys ( field ) : self . create hstore required ( model . meta . db table , field , key )
def remove field ( self , model , field ) : for key in self . iterate required keys ( field ) : self . drop hstore required ( model . meta . db table , field , key )
def alter field ( self , model , old field , new field , strict = False ) : is old field hstore = isinstance ( old field , H Store Field ) is new field hstore = isinstance ( new field , H Store Field ) if not is old field hstore and not is new field hstore : return old required = getattr ( old field , 'required' , [ ] ) or [ ] new required = getattr ( new field , 'required' , [ ] ) or [ ] # handle field renames before moving on if str ( old field . column ) != str ( new field . column ) : for key in self . iterate required keys ( old field ) : self . rename hstore required ( model . meta . db table , model . meta . db table , old field , new field , key ) # drop the constraints for keys that have been removed for key in old required : if key not in new required : self . drop hstore required ( model . meta . db table , old field , key ) # create new constraints for keys that have been added for key in new required : if key not in old required : self . create hstore required ( model . meta . db table , new field , key )
def create hstore required ( self , table name , field , key ) : name = self . required constraint name ( table name , field , key ) sql = self . sql hstore required create . format ( name = self . quote name ( name ) , table = self . quote name ( table name ) , field = self . quote name ( field . column ) , key = key ) self . execute ( sql )
def drop hstore required ( self , table name , field , key ) : name = self . required constraint name ( table name , field , key ) sql = self . sql hstore required drop . format ( table = self . quote name ( table name ) , name = self . quote name ( name ) ) self . execute ( sql )
def create sql ( self , model , schema editor , using = '' ) : if django . VERSION >= ( 2 , 0 ) : statement = super ( ) . create sql ( model , schema editor , using ) statement . template = self . sql create index statement . parts [ 'condition' ] = self . condition return statement else : sql create index = self . sql create index sql parameters = { * * Index . get sql create template values ( self , model , schema editor , using ) , 'condition' : self . condition } return sql create index % sql parameters
def create command ( text , commands ) : class Custom Command ( Base Command ) : description = text def run ( self ) : for cmd in commands : subprocess . check call ( cmd ) return Custom Command
def create model ( self , model ) : super ( ) . create model ( model ) for mixin in self . post processing mixins : mixin . create model ( model )
def delete model ( self , model ) : for mixin in self . post processing mixins : mixin . delete model ( model ) super ( ) . delete model ( model )
def alter db table ( self , model , old db table , new db table ) : super ( Schema Editor , self ) . alter db table ( model , old db table , new db table ) for mixin in self . post processing mixins : mixin . alter db table ( model , old db table , new db table )
def add field ( self , model , field ) : super ( Schema Editor , self ) . add field ( model , field ) for mixin in self . post processing mixins : mixin . add field ( model , field )
def remove field ( self , model , field ) : for mixin in self . post processing mixins : mixin . remove field ( model , field ) super ( Schema Editor , self ) . remove field ( model , field )
def alter field ( self , model , old field , new field , strict = False ) : super ( Schema Editor , self ) . alter field ( model , old field , new field , strict ) for mixin in self . post processing mixins : mixin . alter field ( model , old field , new field , strict )
def form returning ( self ) : qn = self . connection . ops . quote name return ' RETURNING %s' % qn ( self . query . model . meta . pk . attname )
def as sql ( self , return id = False ) : queries = [ self . rewrite insert ( sql , params , return id ) for sql , params in super ( ) . as sql ( ) ] return queries
def alter db table ( self , model , old db table , new db table ) : for field in model . meta . local fields : if not isinstance ( field , H Store Field ) : continue for keys in self . iterate uniqueness keys ( field ) : self . rename hstore unique ( old db table , new db table , field , field , keys )
def add field ( self , model , field ) : for keys in self . iterate uniqueness keys ( field ) : self . create hstore unique ( model , field , keys )
def remove field ( self , model , field ) : for keys in self . iterate uniqueness keys ( field ) : self . drop hstore unique ( model , field , keys )
def alter field ( self , model , old field , new field , strict = False ) : is old field hstore = isinstance ( old field , H Store Field ) is new field hstore = isinstance ( new field , H Store Field ) if not is old field hstore and not is new field hstore : return old uniqueness = getattr ( old field , 'uniqueness' , [ ] ) or [ ] new uniqueness = getattr ( new field , 'uniqueness' , [ ] ) or [ ] # handle field renames before moving on if str ( old field . column ) != str ( new field . column ) : for keys in self . iterate uniqueness keys ( old field ) : self . rename hstore unique ( model . meta . db table , model . meta . db table , old field , new field , keys ) # drop the indexes for keys that have been removed for keys in old uniqueness : if keys not in new uniqueness : self . drop hstore unique ( model , old field , self . compose keys ( keys ) ) # create new indexes for keys that have been added for keys in new uniqueness : if keys not in old uniqueness : self . create hstore unique ( model , new field , self . compose keys ( keys ) )
def create hstore unique ( self , model , field , keys ) : name = self . unique constraint name ( model . meta . db table , field , keys ) columns = [ '(%s->\'%s\')' % ( field . column , key ) for key in keys ] sql = self . sql hstore unique create . format ( name = self . quote name ( name ) , table = self . quote name ( model . meta . db table ) , columns = ',' . join ( columns ) ) self . execute ( sql )
def drop hstore unique ( self , model , field , keys ) : name = self . unique constraint name ( model . meta . db table , field , keys ) sql = self . sql hstore unique drop . format ( name = self . quote name ( name ) ) self . execute ( sql )
def as sql ( self , compiler , connection ) -> Tuple [ str , List [ Any ] ] : sql , params = super ( ) . as sql ( compiler , connection ) qn = compiler . quote name unless alias # generate the extra conditions extra conditions = ' AND ' . join ( [ '{}.{} = %s' . format ( qn ( self . table name ) , qn ( field . column ) ) for field , value in self . extra conditions ] ) # add to the existing params, so the connector will # actually nicely format the value for us for , value in self . extra conditions : params . append ( value ) # rewrite the sql to include the extra conditions rewritten sql = sql . replace ( ')' , ' AND {})' . format ( extra conditions ) ) return rewritten sql , params
def select ( self , board ) : if self . unexplored : i = random . randrange ( len ( self . unexplored ) ) pos = self . unexplored [ i ] self . unexplored [ i ] = self . unexplored [ len ( self . unexplored ) - 1 ] self . unexplored . pop ( ) return pos elif self . bestchild : return self . bestchild . pos else : return PASS
def random playout ( self , board ) : for x in range ( MAXMOVES ) : # XXX while not self.finished? if board . finished : break board . move ( board . random move ( ) )
def Get Domain ( self ) : return ( self . knots [ self . degree - 1 ] , self . knots [ len ( self . knots ) - self . degree ] )
def parse posts ( self , raw posts ) : parsed posts = self . parse json ( raw posts ) # Posts are not sorted. The order is provided by # 'order' key. for post id in parsed posts [ 'order' ] : yield parsed posts [ 'posts' ] [ post id ]
def posts ( self , channel , page = None ) : entrypoint = self . RCHANNELS + '/' + channel + '/' + self . RPOSTS params = { self . PPER PAGE : self . max items } if page is not None : params [ self . PPAGE ] = page response = self . fetch ( entrypoint , params ) return response
def user ( self , user ) : entrypoint = self . RUSERS + '/' + user response = self . fetch ( entrypoint , None ) return response
def pre init ( self ) : if not self . parsed args . mboxes path : base path = os . path . expanduser ( '~/.perceval/mailinglists/' ) dirpath = os . path . join ( base path , self . parsed args . url ) else : dirpath = self . parsed args . mboxes path setattr ( self . parsed args , 'dirpath' , dirpath )
def setup cmd parser ( cls ) : parser = Backend Command Argument Parser ( cls . BACKEND . CATEGORIES , archive = True ) # Required arguments parser . parser . add argument ( 'url' , help = "URL of the RSS feed" ) return parser
def fetch merge requests ( self , from date ) : merges groups = self . client . merges ( from date = from date ) for raw merges in merges groups : merges = json . loads ( raw merges ) for merge in merges : merge id = merge [ 'iid' ] if self . blacklist ids and merge id in self . blacklist ids : logger . warning ( "Skipping blacklisted merge request %s" , merge id ) continue # The single merge request API call returns a more # complete merge request, thus we inflate it with # other data (e.g., notes, emojis, versions) merge full raw = self . client . merge ( merge id ) merge full = json . loads ( merge full raw ) self . init merge extra fields ( merge full ) merge full [ 'notes data' ] = self . get merge notes ( merge id ) merge full [ 'award emoji data' ] = self . get award emoji ( Git Lab Client . MERGES , merge id ) merge full [ 'versions data' ] = self . get merge versions ( merge id ) yield merge full
def issues ( self , from date = None ) : payload = { 'state' : 'all' , 'order by' : 'updated at' , 'sort' : 'asc' , 'per page' : PER PAGE } if from date : payload [ 'updated after' ] = from date . isoformat ( ) return self . fetch items ( Git Lab Client . ISSUES , payload )
def merges ( self , from date = None ) : payload = { 'state' : 'all' , 'order by' : 'updated at' , 'sort' : 'asc' , 'view' : 'simple' , 'per page' : PER PAGE } if from date : payload [ 'updated after' ] = from date . isoformat ( ) return self . fetch items ( Git Lab Client . MERGES , payload )
def merge ( self , merge id ) : path = urijoin ( self . base url , Git Lab Client . PROJECTS , self . owner + '%2F' + self . repository , Git Lab Client . MERGES , merge id ) response = self . fetch ( path ) return response . text
def merge versions ( self , merge id ) : payload = { 'order by' : 'updated at' , 'sort' : 'asc' , 'per page' : PER PAGE } path = urijoin ( Git Lab Client . MERGES , str ( merge id ) , Git Lab Client . VERSIONS ) return self . fetch items ( path , payload )
def merge version ( self , merge id , version id ) : path = urijoin ( self . base url , Git Lab Client . PROJECTS , self . owner + '%2F' + self . repository , Git Lab Client . MERGES , merge id , Git Lab Client . VERSIONS , version id ) response = self . fetch ( path ) return response . text
def notes ( self , item type , item id ) : payload = { 'order by' : 'updated at' , 'sort' : 'asc' , 'per page' : PER PAGE } path = urijoin ( item type , str ( item id ) , Git Lab Client . NOTES ) return self . fetch items ( path , payload )
def emojis ( self , item type , item id ) : payload = { 'order by' : 'updated at' , 'sort' : 'asc' , 'per page' : PER PAGE } path = urijoin ( item type , str ( item id ) , Git Lab Client . EMOJI ) return self . fetch items ( path , payload )
def note emojis ( self , item type , item id , note id ) : payload = { 'order by' : 'updated at' , 'sort' : 'asc' , 'per page' : PER PAGE } path = urijoin ( item type , str ( item id ) , Git Lab Client . NOTES , str ( note id ) , Git Lab Client . EMOJI ) return self . fetch items ( path , payload )
def fetch items ( self , path , payload ) : page = 0 # current page last page = None # last page url next = urijoin ( self . base url , Git Lab Client . PROJECTS , self . owner + '%2F' + self . repository , path ) logger . debug ( "Get Git Lab paginated items from " + url next ) response = self . fetch ( url next , payload = payload ) items = response . text page += 1 if 'last' in response . links : last url = response . links [ 'last' ] [ 'url' ] last page = last url . split ( '&page=' ) [ 1 ] . split ( '&' ) [ 0 ] last page = int ( last page ) logger . debug ( "Page: %i/%i" % ( page , last page ) ) while items : yield items items = None if 'next' in response . links : url next = response . links [ 'next' ] [ 'url' ] # Loving requests :) response = self . fetch ( url next , payload = payload ) page += 1 items = response . text logger . debug ( "Page: %i/%i" % ( page , last page ) )
def init rate limit ( self ) : url = urijoin ( self . base url , 'projects' , self . owner + '%2F' + self . repository ) try : response = super ( ) . fetch ( url ) self . update rate limit ( response ) except requests . exceptions . HTTP Error as error : if error . response . status code == 401 : raise error else : logger . warning ( "Rate limit not initialized: %s" , error )
def setup cmd parser ( cls ) : parser = Backend Command Argument Parser ( cls . BACKEND . CATEGORIES , from date = True , token auth = True , archive = True ) # Git Lab options group = parser . parser . add argument group ( 'Git Lab arguments' ) group . add argument ( '--enterprise-url' , dest = 'base url' , help = "Base URL for Git Lab Enterprise instance" ) group . add argument ( '--sleep-for-rate' , dest = 'sleep for rate' , action = 'store true' , help = "sleep for getting more rate" ) group . add argument ( '--min-rate-to-sleep' , dest = 'min rate to sleep' , default = MIN RATE LIMIT , type = int , help = ) group . add argument ( '--blacklist-ids' , dest = 'blacklist ids' , nargs = '*' , type = int , help = "Ids of items that must not be retrieved." ) # Generic client options group . add argument ( '--max-retries' , dest = 'max retries' , default = MAX RETRIES , type = int , help = "number of API call retries" ) group . add argument ( '--sleep-time' , dest = 'sleep time' , default = DEFAULT SLEEP TIME , type = int , help = "sleeping time between API call retries" ) # Positional arguments parser . parser . add argument ( 'owner' , help = "Git Lab owner" ) parser . parser . add argument ( 'repository' , help = "Git Lab repository" ) return parser
def channel info ( self , channel ) : resource = self . RCHANNEL INFO params = { self . PCHANNEL : channel , } response = self . fetch ( resource , params ) return response
def history ( self , channel , oldest = None , latest = None ) : resource = self . RCHANNEL HISTORY params = { self . PCHANNEL : channel , self . PCOUNT : self . max items } if oldest is not None : params [ self . POLDEST ] = oldest if latest is not None : params [ self . PLATEST ] = latest response = self . fetch ( resource , params ) return response
def user ( self , user id ) : resource = self . RUSER INFO params = { self . PUSER : user id } response = self . fetch ( resource , params ) return response
def setup cmd parser ( cls ) : parser = Backend Command Argument Parser ( cls . BACKEND . CATEGORIES , from date = True , token auth = True , archive = True ) # Backend token is required action = parser . parser . option string actions [ '--api-token' ] action . required = True # Slack options group = parser . parser . add argument group ( 'Slack arguments' ) group . add argument ( '--max-items' , dest = 'max items' , type = int , default = MAX ITEMS , help = "Maximum number of items requested on the same query" ) # Required arguments parser . parser . add argument ( 'channel' , help = "Slack channel identifier" ) return parser
def logout ( self ) : params = { self . PLOGOUT : '1' } self . call ( self . CGI LOGIN , params ) self . close http session ( ) logger . debug ( "Bugzilla user logged out from %s" , self . base url )
def metadata ( self ) : params = { self . PCTYPE : self . CTYPE XML } response = self . call ( self . CGI BUG , params ) return response
def events ( self , group , from date = DEFAULT DATETIME ) : date = datetime to utc ( from date ) date = date . strftime ( "since:%Y-%m-%d T%H:%M:%S.000Z" ) resource = urijoin ( group , self . REVENTS ) # Hack required due to Metup API does not support list # values with the format `?param=value1&param=value2`. # It only works with `?param=value1,value2`. # Morever, urrlib3 encodes comma characters when values # are given using params dict, which it doesn't work # with Meetup, either. fixed params = '?' + self . PFIELDS + '=' + ',' . join ( self . VEVENT FIELDS ) fixed params += '&' + self . PSTATUS + '=' + ',' . join ( self . VSTATUS ) resource += fixed params params = { self . PORDER : self . VUPDATED , self . PSCROLL : date , self . PPAGE : self . max items } try : for page in self . fetch ( resource , params ) : yield page except requests . exceptions . HTTP Error as error : if error . response . status code == 410 : msg = "Group is no longer accessible: {}" . format ( error ) raise Repository Error ( cause = msg ) else : raise error
def comments ( self , group , event id ) : resource = urijoin ( group , self . REVENTS , event id , self . RCOMMENTS ) params = { self . PPAGE : self . max items } for page in self . fetch ( resource , params ) : yield page
def rsvps ( self , group , event id ) : resource = urijoin ( group , self . REVENTS , event id , self . RRSVPS ) # Same hack that in 'events' method fixed params = '?' + self . PFIELDS + '=' + ',' . join ( self . VRSVP FIELDS ) fixed params += '&' + self . PRESPONSE + '=' + ',' . join ( self . VRESPONSE ) resource += fixed params params = { self . PPAGE : self . max items } for page in self . fetch ( resource , params ) : yield page
def parse reviews ( raw data ) : # Join isolated reviews in JSON in array for parsing items raw = "[" + raw data . replace ( "\n" , "," ) + "]" items raw = items raw . replace ( ",]" , "]" ) items = json . loads ( items raw ) reviews = [ ] for item in items : if 'project' in item . keys ( ) : reviews . append ( item ) return reviews
def version ( self ) : if self . version : return self . version cmd = self . gerrit cmd + " %s " % ( Gerrit Client . CMD VERSION ) logger . debug ( "Getting version: %s" % ( cmd ) ) raw data = self . execute ( cmd ) raw data = str ( raw data , "UTF-8" ) logger . debug ( "Gerrit version: %s" % ( raw data ) ) # output: gerrit version 2.10-rc1-988-g333a9dd m = re . match ( Gerrit Client . VERSION REGEX , raw data ) if not m : cause = "Invalid gerrit version %s" % raw data raise Backend Error ( cause = cause ) try : mayor = int ( m . group ( 1 ) ) minor = int ( m . group ( 2 ) ) except Exception : cause = "Gerrit client could not determine the server version." raise Backend Error ( cause = cause ) self . version = [ mayor , minor ] return self . version
def reviews ( self , last item , filter = None ) : cmd = self . get gerrit cmd ( last item , filter ) logger . debug ( "Getting reviews with command: %s" , cmd ) raw data = self . execute ( cmd ) raw data = str ( raw data , "UTF-8" ) return raw data
def next retrieve group item ( self , last item = None , entry = None ) : next item = None gerrit version = self . version if gerrit version [ 0 ] == 2 and gerrit version [ 1 ] > 9 : if last item is None : next item = 0 else : next item = last item elif gerrit version [ 0 ] == 2 and gerrit version [ 1 ] == 9 : # https://groups.google.com/forum/#!topic/repo-discuss/y Qg RR5hl S3E cause = "Gerrit 2.9.0 does not support pagination" raise Backend Error ( cause = cause ) else : if entry is not None : next item = entry [ 'sort Key' ] return next item
def execute from archive ( self , cmd ) : cmd = self . sanitize for archive ( cmd ) response = self . archive . retrieve ( cmd , None , None ) if isinstance ( response , Runtime Error ) : raise response return response
def execute from remote ( self , cmd ) : result = None # data result from the cmd execution retries = 0 while retries < self . MAX RETRIES : try : result = subprocess . check output ( cmd , shell = True ) break except subprocess . Called Process Error as ex : logger . error ( "gerrit cmd %s failed: %s" , cmd , ex ) time . sleep ( self . RETRY WAIT * retries ) retries += 1 if result is None : result = Runtime Error ( cmd + " failed " + str ( self . MAX RETRIES ) + " times. Giving up!" ) if self . archive : cmd = self . sanitize for archive ( cmd ) self . archive . store ( cmd , None , None , result ) if isinstance ( result , Runtime Error ) : raise result return result
def setup cmd parser ( cls ) : parser = Backend Command Argument Parser ( cls . BACKEND . CATEGORIES , from date = True , archive = True ) # Gerrit options group = parser . parser . add argument group ( 'Gerrit arguments' ) group . add argument ( '--user' , dest = 'user' , help = "Gerrit ssh user" ) group . add argument ( '--max-reviews' , dest = 'max reviews' , type = int , default = MAX REVIEWS , help = "Max number of reviews per ssh query." ) group . add argument ( '--blacklist-reviews' , dest = 'blacklist reviews' , nargs = '*' , help = "Wrong reviews that must not be retrieved." ) group . add argument ( '--disable-host-key-check' , dest = 'disable host key check' , action = 'store true' , help = "Don't check remote host identity" ) group . add argument ( '--ssh-port' , dest = 'port' , default = PORT , type = int , help = "Set SSH port of the Gerrit server" ) # Required arguments parser . parser . add argument ( 'hostname' , help = "Hostname of the Gerrit server" ) return parser
def fetch issue data ( self , issue id ) : raw issue = self . client . issue ( issue id ) issue = json . loads ( raw issue ) return issue
def fetch issue attachments ( self , issue id ) : for attachments raw in self . client . issue collection ( issue id , "attachments" ) : attachments = json . loads ( attachments raw ) for attachment in attachments [ 'entries' ] : yield attachment
def fetch issue messages ( self , issue id ) : for messages raw in self . client . issue collection ( issue id , "messages" ) : messages = json . loads ( messages raw ) for msg in messages [ 'entries' ] : msg [ 'owner data' ] = self . fetch user data ( '{OWNER}' , msg [ 'owner link' ] ) yield msg
def fetch issue activities ( self , issue id ) : for activities raw in self . client . issue collection ( issue id , "activity" ) : activities = json . loads ( activities raw ) for act in activities [ 'entries' ] : act [ 'person data' ] = self . fetch user data ( '{PERSON}' , act [ 'person link' ] ) yield act
def fetch user data ( self , tag type , user link ) : user name = self . client . user name ( user link ) user = { } if not user name : return user user raw = self . client . user ( user name ) user = json . loads ( user raw ) return user
def issues ( self , start = None ) : payload = self . build payload ( size = self . items per page , operation = True , startdate = start ) path = self . get url project ( ) return self . fetch items ( path = path , payload = payload )
def user ( self , user name ) : user = None if user name in self . users : return self . users [ user name ] url user = self . get url ( "~" + user name ) logger . info ( "Getting info for %s" % ( url user ) ) try : raw user = self . send request ( url user ) user = raw user except requests . exceptions . HTTP Error as e : if e . response . status code in [ 404 , 410 ] : logger . warning ( "Data is not available - %s" , url user ) user = '{}' else : raise e self . users [ user name ] = user return user
def issue ( self , issue id ) : path = urijoin ( "bugs" , str ( issue id ) ) url issue = self . get url ( path ) raw text = self . send request ( url issue ) return raw text
def issue collection ( self , issue id , collection name ) : path = urijoin ( "bugs" , str ( issue id ) , collection name ) url collection = self . get url ( path ) payload = { 'ws.size' : self . items per page , 'ws.start' : 0 , 'order by' : 'date last updated' } raw items = self . fetch items ( path = url collection , payload = payload ) return raw items
def fetch items ( self , path , payload ) : page = 0 # current page url next = path fetch data = True while fetch data : logger . debug ( "Fetching page: %i" , page ) try : raw content = self . send request ( url next , payload ) content = json . loads ( raw content ) except requests . exceptions . HTTP Error as e : if e . response . status code in [ 410 ] : logger . warning ( "Data is not available - %s" , url next ) raw content = '{"total size": 0, "start": 0, "entries": []}' content = json . loads ( raw content ) else : raise e if 'next collection link' in content : url next = content [ 'next collection link' ] payload = None else : fetch data = False yield raw content page += 1
def find group id ( self ) : group subscriptions = self . subscriptions ( self . auth ) for subscriptions in group subscriptions : for sub in subscriptions : if sub [ 'group name' ] == self . group name : return sub [ 'group id' ] msg = "Group id not found for group name %s" % self . group name raise Backend Error ( cause = msg )
def fetch ( self , url , payload ) : r = requests . get ( url , params = payload , auth = self . auth , verify = self . verify ) try : r . raise for status ( ) except requests . exceptions . HTTP Error as e : raise e return r
def pre init ( self ) : if not self . parsed args . mboxes path : base path = os . path . expanduser ( '~/.perceval/mailinglists/' ) dirpath = os . path . join ( base path , GROUPSIO URL , 'g' , self . parsed args . group name ) else : dirpath = self . parsed args . mboxes path setattr ( self . parsed args , 'dirpath' , dirpath )
def setup cmd parser ( cls ) : parser = Backend Command Argument Parser ( cls . BACKEND . CATEGORIES , from date = True , token auth = True ) # Backend token is required action = parser . parser . option string actions [ '--api-token' ] action . required = True # Optional arguments group = parser . parser . add argument group ( 'Groupsio arguments' ) group . add argument ( '--mboxes-path' , dest = 'mboxes path' , help = "Path where mbox files will be stored" ) group . add argument ( '--no-verify' , dest = 'verify' , action = 'store false' , help = "Value 'True' enable SSL verification" ) # Required arguments parser . parser . add argument ( 'group name' , help = "Name of the group on Groups.io" ) return parser
def set auth arguments ( self , basic auth = True , token auth = False ) : group = self . parser . add argument group ( 'authentication arguments' ) if basic auth : group . add argument ( '-u' , '--backend-user' , dest = 'user' , help = "backend user" ) group . add argument ( '-p' , '--backend-password' , dest = 'password' , help = "backend password" ) if token auth : group . add argument ( '-t' , '--api-token' , dest = 'api token' , help = "backend authentication token / API key" )
def set archive arguments ( self ) : group = self . parser . add argument group ( 'archive arguments' ) group . add argument ( '--archive-path' , dest = 'archive path' , default = None , help = "directory path to the archives" ) group . add argument ( '--no-archive' , dest = 'no archive' , action = 'store true' , help = "do not archive data" ) group . add argument ( '--fetch-archive' , dest = 'fetch archive' , action = 'store true' , help = "fetch data from the archives" ) group . add argument ( '--archived-since' , dest = 'archived since' , default = '1970-01-01' , help = "retrieve items archived since the given date" )
def set output arguments ( self ) : group = self . parser . add argument group ( 'output arguments' ) group . add argument ( '-o' , '--output' , type = argparse . File Type ( 'w' ) , dest = 'outfile' , default = sys . stdout , help = "output file" ) group . add argument ( '--json-line' , dest = 'json line' , action = 'store true' , help = "produce a JSON line for each output item" )
def initialize archive ( self ) : if 'archive path' not in self . parsed args : manager = None elif self . parsed args . no archive : manager = None else : if not self . parsed args . archive path : archive path = os . path . expanduser ( ARCHIVES DEFAULT PATH ) else : archive path = self . parsed args . archive path manager = Archive Manager ( archive path ) self . archive manager = manager
def fetch and parse messages ( self , mailing list , from date ) : from date = datetime to utc ( from date ) nmsgs , imsgs , tmsgs = ( 0 , 0 , 0 ) for mbox in mailing list . mboxes : tmp path = None try : tmp path = self . copy mbox ( mbox ) for message in self . parse mbox ( tmp path ) : tmsgs += 1 if not self . validate message ( message ) : imsgs += 1 continue # Ignore those messages sent before the given date dt = str to datetime ( message [ M Box . DATE FIELD ] ) if dt < from date : logger . debug ( "Message %s sent before %s; skipped" , message [ 'unixfrom' ] , str ( from date ) ) tmsgs -= 1 continue # Convert 'Case Insensitive Dict' to dict message = self . casedict to dict ( message ) nmsgs += 1 logger . debug ( "Message %s parsed" , message [ 'unixfrom' ] ) yield message except ( OS Error , EOF Error ) as e : logger . warning ( "Ignoring %s mbox due to: %s" , mbox . filepath , str ( e ) ) except Exception as e : if tmp path and os . path . exists ( tmp path ) : os . remove ( tmp path ) raise e finally : if tmp path and os . path . exists ( tmp path ) : os . remove ( tmp path ) logger . info ( "Done. %s/%s messages fetched; %s ignored" , nmsgs , tmsgs , imsgs )
def copy mbox ( self , mbox ) : tmp path = tempfile . mktemp ( prefix = 'perceval ' ) with mbox . container as f in : with open ( tmp path , mode = 'wb' ) as f out : for l in f in : f out . write ( l ) return tmp path
def validate message ( self , message ) : # This check is "case insensitive" because we're # using 'Case Insensitive Dict' from requests.structures # module to store the contents of a message. if self . MESSAGE ID FIELD not in message : logger . warning ( "Field 'Message-ID' not found in message %s; ignoring" , message [ 'unixfrom' ] ) return False if not message [ self . MESSAGE ID FIELD ] : logger . warning ( "Field 'Message-ID' is empty in message %s; ignoring" , message [ 'unixfrom' ] ) return False if self . DATE FIELD not in message : logger . warning ( "Field 'Date' not found in message %s; ignoring" , message [ 'unixfrom' ] ) return False if not message [ self . DATE FIELD ] : logger . warning ( "Field 'Date' is empty in message %s; ignoring" , message [ 'unixfrom' ] ) return False try : str to datetime ( message [ self . DATE FIELD ] ) except Invalid Date Error : logger . warning ( "Invalid date %s in message %s; ignoring" , message [ self . DATE FIELD ] , message [ 'unixfrom' ] ) return False return True
def get message ( self , key ) : start , stop = self . lookup ( key ) self . file . seek ( start ) from line = self . file . readline ( ) . replace ( mailbox . linesep , b'' ) string = self . file . read ( stop - self . file . tell ( ) ) msg = self . message factory ( string . replace ( mailbox . linesep , b'\n' ) ) try : msg . set from ( from line [ 5 : ] . decode ( 'ascii' ) ) return msg except Unicode Decode Error : pass try : msg . set from ( from line [ 5 : ] . decode ( 'utf-8' ) ) except Unicode Decode Error : msg . set from ( from line [ 5 : ] . decode ( 'iso-8859-1' ) ) return msg
def pre init ( self ) : if self . parsed args . git log : git path = self . parsed args . git log elif not self . parsed args . git path : base path = os . path . expanduser ( '~/.perceval/repositories/' ) processed uri = self . parsed args . uri . lstrip ( '/' ) git path = os . path . join ( base path , processed uri ) + '-git' else : git path = self . parsed args . git path setattr ( self . parsed args , 'gitpath' , git path )
def setup cmd parser ( cls ) : parser = Backend Command Argument Parser ( cls . BACKEND . CATEGORIES , from date = True , to date = True ) # Optional arguments group = parser . parser . add argument group ( 'Git arguments' ) group . add argument ( '--branches' , dest = 'branches' , nargs = '+' , type = str , default = None , help = "Fetch commits only from these branches" ) # Mutual exclusive parameters exgroup = group . add mutually exclusive group ( ) exgroup . add argument ( '--git-path' , dest = 'git path' , help = "Path where the Git repository will be cloned" ) exgroup . add argument ( '--git-log' , dest = 'git log' , help = "Path to the Git log file" ) exgroup fetch = group . add mutually exclusive group ( ) exgroup fetch . add argument ( '--latest-items' , dest = 'latest items' , action = 'store true' , help = "Fetch latest commits added to the repository" ) exgroup fetch . add argument ( '--no-update' , dest = 'no update' , action = 'store true' , help = "Fetch all commits without updating the repository" ) # Required arguments parser . parser . add argument ( 'uri' , help = "URI of the Git log repository" ) return parser
def parse ( self ) : for line in self . stream : line = line . rstrip ( '\n' ) parsed = False self . nline += 1 while not parsed : parsed = self . handlers [ self . state ] ( line ) if self . state == self . COMMIT and self . commit : commit = self . build commit ( ) logger . debug ( "Commit %s parsed" , commit [ 'commit' ] ) yield commit # Return the last commit, if any if self . commit : commit = self . build commit ( ) logger . debug ( "Commit %s parsed" , commit [ 'commit' ] ) yield commit
def fetch pack ( self ) : def prepare refs ( refs ) : return [ ref . hash . encode ( 'utf-8' ) for ref in refs if not ref . refname . endswith ( '^{}' ) ] def determine wants ( refs ) : remote refs = prepare refs ( self . discover refs ( remote = True ) ) local refs = prepare refs ( self . discover refs ( ) ) wants = [ ref for ref in remote refs if ref not in local refs ] return wants client , repo path = dulwich . client . get transport and path ( self . uri ) repo = dulwich . repo . Repo ( self . dirpath ) fd = io . Bytes IO ( ) local refs = self . discover refs ( ) graph walker = Graph Walker ( local refs ) result = client . fetch pack ( repo path , determine wants , graph walker , fd . write ) refs = [ Git Ref ( ref hash . decode ( 'utf-8' ) , ref name . decode ( 'utf-8' ) ) for ref name , ref hash in result . refs . items ( ) ] if len ( fd . getvalue ( ) ) > 0 : fd . seek ( 0 ) pack = repo . object store . add thin pack ( fd . read , None ) pack name = pack . name ( ) . decode ( 'utf-8' ) else : pack name = None return ( pack name , refs )
def read commits from pack ( self , packet name ) : filepath = 'objects/pack/pack-' + packet name cmd verify pack = [ 'git' , 'verify-pack' , '-v' , filepath ] outs = self . exec ( cmd verify pack , cwd = self . dirpath , env = self . gitenv ) outs = outs . decode ( 'utf-8' , errors = 'surrogateescape' ) . rstrip ( ) lines = [ line . split ( ' ' ) for line in outs . split ( '\n' ) ] # Commits usually come in the pack ordered from newest to oldest commits = [ parts [ 0 ] for parts in lines if parts [ 1 ] == 'commit' ] commits . reverse ( ) return commits
def update references ( self , refs ) : new refs = [ ref . refname for ref in refs ] # Delete old references for old ref in self . discover refs ( ) : if not old ref . refname . startswith ( 'refs/heads/' ) : continue if old ref . refname in new refs : continue self . update ref ( old ref , delete = True ) # Update new references for new ref in refs : refname = new ref . refname if refname . endswith ( '^{}' ) : logger . debug ( "Annotated tag %s ignored for updating in sync process" , refname ) continue elif not refname . startswith ( 'refs/heads/' ) and not refname . startswith ( 'refs/tags/' ) : logger . debug ( "Reference %s not needed; ignored for updating in sync process" , refname ) continue else : self . update ref ( new ref ) # Prune repository to remove old branches cmd = [ 'git' , 'remote' , 'prune' , 'origin' ] self . exec ( cmd , cwd = self . dirpath , env = self . gitenv )
def discover refs ( self , remote = False ) : if remote : cmd refs = [ 'git' , 'ls-remote' , '-h' , '-t' , '--exit-code' , 'origin' ] sep = '\t' ignored error codes = [ 2 ] else : # Check first whether the local repo is empty; # Running 'show-ref' in empty repos gives an error if self . is empty ( ) : raise Empty Repository Error ( repository = self . uri ) cmd refs = [ 'git' , 'show-ref' , '--heads' , '--tags' ] sep = ' ' ignored error codes = [ 1 ] # Error codes returned when no matching refs (i.e, no heads # or tags) are found in a repository will be ignored. Otherwise, # the full process would fail for those situations. outs = self . exec ( cmd refs , cwd = self . dirpath , env = self . gitenv , ignored error codes = ignored error codes ) outs = outs . decode ( 'utf-8' , errors = 'surrogateescape' ) . rstrip ( ) outs = outs . split ( '\n' ) if outs else [ ] refs = [ ] for line in outs : data = line . split ( sep ) ref = Git Ref ( data [ 0 ] , data [ 1 ] ) refs . append ( ref ) return refs
def update ref ( self , ref , delete = False ) : cmd = [ 'git' , 'update-ref' ] if delete : cmd . extend ( [ '-d' , ref . refname ] ) action = 'deleted' else : cmd . extend ( [ ref . refname , ref . hash ] ) action = 'updated to %s' % ref . hash try : self . exec ( cmd , cwd = self . dirpath , env = self . gitenv ) except Repository Error as e : logger . warning ( "Git %s ref could not be %s during sync process in %s (%s); skipped" , ref . refname , action , self . uri , self . dirpath ) else : logger . debug ( "Git %s ref %s in %s (%s)" , ref . refname , action , self . uri , self . dirpath )
def setup cmd parser ( cls ) : parser = Backend Command Argument Parser ( cls . BACKEND . CATEGORIES , token auth = True , archive = True ) # Backend token is required action = parser . parser . option string actions [ '--api-token' ] action . required = True # Meetup options group = parser . parser . add argument group ( 'Twitter arguments' ) group . add argument ( '--max-items' , dest = 'max items' , type = int , default = MAX ITEMS , help = "Maximum number of items requested on the same query" ) group . add argument ( '--no-entities' , dest = 'include entities' , action = 'store false' , help = " Exclude entities node" ) group . add argument ( '--geo-code' , dest = 'geocode' , help = "Select tweets by users located at latitude,longitude,radius" ) group . add argument ( '--lang' , dest = 'lang' , help = "Select tweets to the given language in ISO 639-1 code" ) group . add argument ( '--tweets-type' , dest = 'tweets type' , default = TWEET TYPE MIXED , help = "Type of tweets returned. Default is 'mixed', others are 'recent' and 'popular'" ) group . add argument ( '--sleep-for-rate' , dest = 'sleep for rate' , action = 'store true' , help = "sleep for getting more rate" ) group . add argument ( '--min-rate-to-sleep' , dest = 'min rate to sleep' , default = MIN RATE LIMIT , type = int , help = "sleep until reset when the rate limit reaches this value" ) group . add argument ( '--sleep-time' , dest = 'sleep time' , default = SLEEP TIME , type = int , help = "minimun sleeping time to avoid too many request exception" ) # Required arguments parser . parser . add argument ( 'query' , help = "Search query including operators, max 500 chars" ) return parser
def parse hits ( self , hit raw ) : # Create the soup and get the desired div bs result = bs4 . Beautiful Soup ( hit raw , 'html.parser' ) hit string = bs result . find ( "div" , id = "result Stats" ) . text # Remove commas or dots hit string = hit string . replace ( ',' , u'' ) hit string = hit string . replace ( '.' , u'' ) fetched on = datetime utcnow ( ) . timestamp ( ) id args = self . keywords [ : ] id args . append ( str ( fetched on ) ) hits json = { 'fetched on' : fetched on , 'id' : uuid ( * id args ) , 'keywords' : self . keywords , 'type' : 'google Search Hits' } if not hit string : logger . warning ( "No hits for %s" , self . keywords ) hits json [ 'hits' ] = 0 return hits json str hits = re . search ( r'\d+' , hit string ) . group ( 0 ) hits = int ( str hits ) hits json [ 'hits' ] = hits return hits json
def hits ( self , keywords ) : if len ( keywords ) == 1 : query str = keywords [ 0 ] else : query str = ' ' . join ( [ k for k in keywords ] ) logger . info ( "Fetching hits for '%s'" , query str ) params = { 'q' : query str } # Make the request req = self . fetch ( GOOGLE SEARCH URL , payload = params ) return req . text
def fetch pull requests ( self , from date , to date ) : raw pulls = self . client . pulls ( from date = from date ) for raw pull in raw pulls : pull = json . loads ( raw pull ) if str to datetime ( pull [ 'updated at' ] ) > to date : return self . init extra pull fields ( pull ) for field in TARGET PULL FIELDS : if not pull [ field ] : continue if field == 'user' : pull [ field + ' data' ] = self . get user ( pull [ field ] [ 'login' ] ) elif field == 'merged by' : pull [ field + ' data' ] = self . get user ( pull [ field ] [ 'login' ] ) elif field == 'review comments' : pull [ field + ' data' ] = self . get pull review comments ( pull [ 'number' ] ) elif field == 'requested reviewers' : pull [ field + ' data' ] = self . get pull requested reviewers ( pull [ 'number' ] ) elif field == 'commits' : pull [ field + ' data' ] = self . get pull commits ( pull [ 'number' ] ) yield pull
def fetch repo info ( self ) : raw repo = self . client . repo ( ) repo = json . loads ( raw repo ) fetched on = datetime utcnow ( ) repo [ 'fetched on' ] = fetched on . timestamp ( ) yield repo
def get issue comment reactions ( self , comment id , total count ) : reactions = [ ] if total count == 0 : return reactions group reactions = self . client . issue comment reactions ( comment id ) for raw reactions in group reactions : for reaction in json . loads ( raw reactions ) : reaction [ 'user data' ] = self . get user ( reaction [ 'user' ] [ 'login' ] ) reactions . append ( reaction ) return reactions
def get pull requested reviewers ( self , pr number ) : requested reviewers = [ ] group requested reviewers = self . client . pull requested reviewers ( pr number ) for raw requested reviewers in group requested reviewers : group requested reviewers = json . loads ( raw requested reviewers ) for requested reviewer in group requested reviewers [ 'users' ] : user data = self . get user ( requested reviewer [ 'login' ] ) requested reviewers . append ( user data ) return requested reviewers
def get pull commits ( self , pr number ) : hashes = [ ] group pull commits = self . client . pull commits ( pr number ) for raw pull commits in group pull commits : for commit in json . loads ( raw pull commits ) : commit hash = commit [ 'sha' ] hashes . append ( commit hash ) return hashes
def get pull review comments ( self , pr number ) : comments = [ ] group comments = self . client . pull review comments ( pr number ) for raw comments in group comments : for comment in json . loads ( raw comments ) : comment id = comment . get ( 'id' ) user = comment . get ( 'user' , None ) if not user : logger . warning ( "Missing user info for %s" , comment [ 'url' ] ) comment [ 'user data' ] = None else : comment [ 'user data' ] = self . get user ( user [ 'login' ] ) comment [ 'reactions data' ] = self . get pull review comment reactions ( comment id , comment [ 'reactions' ] [ 'total count' ] ) comments . append ( comment ) return comments
def get pull review comment reactions ( self , comment id , total count ) : reactions = [ ] if total count == 0 : return reactions group reactions = self . client . pull review comment reactions ( comment id ) for raw reactions in group reactions : for reaction in json . loads ( raw reactions ) : reaction [ 'user data' ] = self . get user ( reaction [ 'user' ] [ 'login' ] ) reactions . append ( reaction ) return reactions
def get user ( self , login ) : user = { } if not login : return user user raw = self . client . user ( login ) user = json . loads ( user raw ) user orgs raw = self . client . user orgs ( login ) user [ 'organizations' ] = json . loads ( user orgs raw ) return user
def issue reactions ( self , issue number ) : payload = { 'per page' : PER PAGE , 'direction' : 'asc' , 'sort' : 'updated' } path = urijoin ( "issues" , str ( issue number ) , "reactions" ) return self . fetch items ( path , payload )
def pull requested reviewers ( self , pr number ) : requested reviewers url = urijoin ( "pulls" , str ( pr number ) , "requested reviewers" ) return self . fetch items ( requested reviewers url , { } )
def pull commits ( self , pr number ) : payload = { 'per page' : PER PAGE , } commit url = urijoin ( "pulls" , str ( pr number ) , "commits" ) return self . fetch items ( commit url , payload )
def pull review comments ( self , pr number ) : payload = { 'per page' : PER PAGE , 'direction' : 'asc' , 'sort' : 'updated' } comments url = urijoin ( "pulls" , str ( pr number ) , "comments" ) return self . fetch items ( comments url , payload )
def pull review comment reactions ( self , comment id ) : payload = { 'per page' : PER PAGE , 'direction' : 'asc' , 'sort' : 'updated' } path = urijoin ( "pulls" , "comments" , str ( comment id ) , "reactions" ) return self . fetch items ( path , payload )
def user ( self , login ) : user = None if login in self . users : return self . users [ login ] url user = urijoin ( self . base url , 'users' , login ) logging . info ( "Getting info for %s" % ( url user ) ) r = self . fetch ( url user ) user = r . text self . users [ login ] = user return user
def user orgs ( self , login ) : if login in self . users orgs : return self . users orgs [ login ] url = urijoin ( self . base url , 'users' , login , 'orgs' ) try : r = self . fetch ( url ) orgs = r . text except requests . exceptions . HTTP Error as error : # 404 not found is wrongly received sometimes if error . response . status code == 404 : logger . error ( "Can't get github login orgs: %s" , error ) orgs = '[]' else : raise error self . users orgs [ login ] = orgs return orgs
def get token rate limit ( self , token ) : rate url = urijoin ( self . base url , "rate limit" ) self . session . headers . update ( { 'Authorization' : 'token ' + token } ) remaining = 0 try : headers = super ( ) . fetch ( rate url ) . headers if self . rate limit header in headers : remaining = int ( headers [ self . rate limit header ] ) except requests . exceptions . HTTP Error as error : logger . warning ( "Rate limit not initialized: %s" , error ) return remaining
def get tokens rate limits ( self ) : remainings = [ 0 ] * self . n tokens # Turn off archiving when checking rates, because that would cause # archive key conflict (the same UR Ls giving different responses) arch = self . archive self . archive = None for idx , token in enumerate ( self . tokens ) : # Pass flag to skip disabling archiving because this function doies it remainings [ idx ] = self . get token rate limit ( token ) # Restore archiving to whatever state it was self . archive = arch logger . debug ( "Remaining API points: {}" . format ( remainings ) ) return remainings
def choose best api token ( self ) : # Return if no tokens given if self . n tokens == 0 : return # If multiple tokens given, choose best token idx = 0 if self . n tokens > 1 : remainings = self . get tokens rate limits ( ) token idx = remainings . index ( max ( remainings ) ) logger . debug ( "Remaining API points: {}, choosen index: {}" . format ( remainings , token idx ) ) # If we have any tokens - use best of them self . current token = self . tokens [ token idx ] self . session . headers . update ( { 'Authorization' : 'token ' + self . current token } ) # Update rate limit data for the current token self . update current rate limit ( )
def need check tokens ( self ) : if self . n tokens <= 1 or self . rate limit is None : return False elif self . last rate limit checked is None : self . last rate limit checked = self . rate limit return True # If approaching minimum rate limit for sleep approaching limit = float ( self . min rate to sleep ) * ( 1.0 + TOKEN USAGE BEFORE SWITCH ) + 1 if self . rate limit <= approaching limit : self . last rate limit checked = self . rate limit return True # Only switch token when used predefined factor of the current token's remaining API points ratio = float ( self . rate limit ) / float ( self . last rate limit checked ) if ratio < 1.0 - TOKEN USAGE BEFORE SWITCH : self . last rate limit checked = self . rate limit return True elif ratio > 1.0 : self . last rate limit checked = self . rate limit return False else : return False
def update current rate limit ( self ) : url = urijoin ( self . base url , "rate limit" ) try : # Turn off archiving when checking rates, because that would cause # archive key conflict (the same UR Ls giving different responses) arch = self . archive self . archive = None response = super ( ) . fetch ( url ) self . archive = arch self . update rate limit ( response ) self . last rate limit checked = self . rate limit except requests . exceptions . HTTP Error as error : if error . response . status code == 404 : logger . warning ( "Rate limit not initialized: %s" , error ) else : raise error
def load metadata ( self ) : logger . debug ( "Loading metadata infomation of archive %s" , self . archive path ) cursor = self . db . cursor ( ) select stmt = "SELECT origin, backend name, backend version, " "category, backend params, created on " "FROM " + self . METADATA TABLE + " " "LIMIT 1" cursor . execute ( select stmt ) row = cursor . fetchone ( ) cursor . close ( ) if row : self . origin = row [ 0 ] self . backend name = row [ 1 ] self . backend version = row [ 2 ] self . category = row [ 3 ] self . backend params = pickle . loads ( row [ 4 ] ) self . created on = str to datetime ( row [ 5 ] ) else : logger . debug ( "Metadata of archive %s was empty" , self . archive path ) logger . debug ( "Metadata of archive %s loaded" , self . archive path )
def count table rows ( self , table name ) : cursor = self . db . cursor ( ) select stmt = "SELECT COUNT(*) FROM " + table name try : cursor . execute ( select stmt ) row = cursor . fetchone ( ) except sqlite3 . Database Error as e : msg = "invalid archive file; cause: %s" % str ( e ) raise Archive Error ( cause = msg ) finally : cursor . close ( ) return row [ 0 ]
def search archives ( self , origin , backend name , category , archived after ) : for archive path in self . search files ( ) : try : archive = Archive ( archive path ) except Archive Error : continue match = archive . origin == origin and archive . backend name == backend name and archive . category == category and archive . created on >= archived after if not match : continue yield archive path , archive . created on
def search files ( self ) : for root , , files in os . walk ( self . dirpath ) : for filename in files : location = os . path . join ( root , filename ) yield location
def repository ( self , owner , repository ) : url = urijoin ( self . base url , self . RREPOSITORY , owner , repository ) logger . debug ( "Docker Hub client requests: %s" , url ) response = self . fetch ( url ) return response . text
def get fields ( self ) : url = urijoin ( self . base url , self . RESOURCE , self . VERSION API , 'field' ) req = self . fetch ( url ) return req . text
def get builds ( self , job name ) : if self . blacklist jobs and job name in self . blacklist jobs : logger . warning ( "Not getting blacklisted job: %s" , job name ) return payload = { 'depth' : self . detail depth } url build = urijoin ( self . base url , "job" , job name , "api" , "json" ) response = self . fetch ( url build , payload = payload ) return response . text
def setup cmd parser ( cls ) : parser = Backend Command Argument Parser ( cls . BACKEND . CATEGORIES , from date = True , token auth = True , archive = True ) # Stack Exchange options group = parser . parser . add argument group ( 'Stack Exchange arguments' ) group . add argument ( '--site' , dest = 'site' , required = True , help = "Stack Exchange site" ) group . add argument ( '--tagged' , dest = 'tagged' , help = "filter items by question Tag" ) group . add argument ( '--max-questions' , dest = 'max questions' , type = int , default = MAX QUESTIONS , help = "Maximum number of questions requested in the same query" ) return parser
def get max date ( self , reviews ) : max ts = 0 for review in reviews : ts = str to datetime ( review [ 'timestamp' ] ) ts = datetime to utc ( ts ) if ts . timestamp ( ) > max ts : max ts = ts . timestamp ( ) return max ts
def get pages ( self , namespace , apcontinue = '' ) : params = { "action" : "query" , "list" : "allpages" , "aplimit" : self . limit , "apnamespace" : namespace , "format" : "json" } if apcontinue : params [ 'apcontinue' ] = apcontinue return self . call ( params )
def get recent pages ( self , namespaces , rccontinue = '' ) : namespaces . sort ( ) params = { "action" : "query" , "list" : "recentchanges" , "rclimit" : self . limit , "rcnamespace" : "|" . join ( namespaces ) , "rcprop" : "title|timestamp|ids" , "format" : "json" } if rccontinue : params [ 'rccontinue' ] = rccontinue return self . call ( params )
def retrieve archives ( self , from date ) : archives = [ ] candidates = self . list supybot archives ( ) for candidate in candidates : dt = self . parse date from filepath ( candidate ) if dt . date ( ) >= from date . date ( ) : archives . append ( ( dt , candidate ) ) else : logger . debug ( "Archive %s stored before %s; skipped" , candidate , str ( from date ) ) archives . sort ( key = lambda x : x [ 0 ] ) return [ archive [ 1 ] for archive in archives ]
def list supybot archives ( self ) : archives = [ ] for root , , files in os . walk ( self . dirpath ) : for filename in files : location = os . path . join ( root , filename ) archives . append ( location ) return archives
def capabilities url ( self , service url ) : qs = [ ] if service url . find ( '?' ) != - 1 : qs = cgi . parse qsl ( service url . split ( '?' ) [ 1 ] ) params = [ x [ 0 ] for x in qs ] if 'service' not in params : qs . append ( ( 'service' , 'WFS' ) ) if 'request' not in params : qs . append ( ( 'request' , 'Get Capabilities' ) ) if 'version' not in params : qs . append ( ( 'version' , self . version ) ) urlqs = urlencode ( tuple ( qs ) ) return service url . split ( '?' ) [ 0 ] + '?' + urlqs
def parse result ( self ) : if self . result is not None : result = self . result . find ( nspv ( "wml2:Measurement Timeseries" ) ) self . result = Measurement Timeseries ( result )
def complex input with reference ( ) : print ( "\ncomplex input with reference ..." ) wps = Web Processing Service ( 'http://localhost:8094/wps' , verbose = verbose ) processid = 'wordcount' textdoc = Complex Data Input ( "http://www.gutenberg.org/files/28885/28885-h/28885-h.htm" ) # alice in wonderland inputs = [ ( "text" , textdoc ) ] # list of tuple (output identifier, as Reference attribute, mime Type attribute) # when as Reference or mime Type is None - the wps service will use its default option outputs = [ ( "output" , True , 'some/mime-type' ) ] execution = wps . execute ( processid , inputs , output = outputs ) monitor Execution ( execution ) # show status print ( 'percent complete' , execution . percent Completed ) print ( 'status message' , execution . status Message ) for output in execution . process Outputs : print ( 'identifier=%s, data Type=%s, data=%s, reference=%s' % ( output . identifier , output . data Type , output . data , output . reference ) )
def normalize ( s ) : # Added to bypass NIST-style pre-processing of hyp and ref files -- wade if ( nonorm ) : return s . split ( ) try : s . split ( ) except : s = " " . join ( s ) # language-independent part: for ( pattern , replace ) in normalize1 : s = re . sub ( pattern , replace , s ) s = xml . sax . saxutils . unescape ( s , { '&quot;' : '"' } ) # language-dependent part (assuming Western languages): s = " %s " % s if not preserve case : s = s . lower ( ) # this might not be identical to the original return [ tok for tok in normalize3 . split ( s ) if tok and tok != ' ' ]
def erfcc ( x ) : z = abs ( x ) t = 1 / ( 1 + 0.5 * z ) r = t * math . exp ( - z * z - 1.26551223 + t * ( 1.00002368 + t * ( .37409196 + t * ( .09678418 + t * ( - .18628806 + t * ( .27886807 + t * ( - 1.13520398 + t * ( 1.48851587 + t * ( - .82215223 + t * .17087277 ) ) ) ) ) ) ) ) ) if ( x >= 0. ) : return r else : return 2. - r
def log calls ( func ) : def wrapper ( * args , * * kargs ) : call Str = "%s(%s)" % ( func . name , ", " . join ( [ repr ( p ) for p in args ] + [ "%s=%s" % ( k , repr ( v ) ) for ( k , v ) in list ( kargs . items ( ) ) ] ) ) debug ( ">> %s" , call Str ) ret = func ( * args , * * kargs ) debug ( "<< %s: %s" , call Str , repr ( ret ) ) return ret return wrapper
def synchronized ( func ) : func . lock = threading . Lock ( ) def synced func ( * args , * * kargs ) : with func . lock : return func ( * args , * * kargs ) return synced func
def message ( msg , * args ) : clear progress ( ) text = ( msg % args ) sys . stdout . write ( text + '\n' )
def tempfile get ( target ) : fn = '%s-%s.tmp' % ( target , '' . join ( random . Random ( ) . sample ( "0123456789abcdefghijklmnopqrstuvwxyz" , 15 ) ) ) TEMP FILES . add ( fn ) return fn
def tempfile set ( tempfile , target ) : if target : os . rename ( tempfile , target ) else : os . unlink ( tempfile ) if target in TEMP FILES : TEMP FILES . remove ( tempfile )
def clean tempfiles ( ) : for fn in TEMP FILES : if os . path . exists ( fn ) : os . unlink ( fn )
def get fixed path ( self ) : pi = self . path . split ( PATH SEP ) fi = [ ] for p in pi : if '*' in p or '?' in p : break fi . append ( p ) return PATH SEP . join ( fi )
def get legal params ( self , method ) : if method not in self . client . meta . method to api mapping : # Injected methods. Ignore. return [ ] api = self . client . meta . method to api mapping [ method ] shape = self . client . meta . service model . operation model ( api ) . input shape if shape is None : # No params needed for this API. return [ ] return shape . members . keys ( )
def add options ( parser ) : for param , param type , param doc in Boto Client . EXTRA CLIENT PARAMS : parser . add option ( '--API-' + param , help = param doc , type = param type , dest = param )
def add task ( self , func name , * args , * * kargs ) : self . tasks . put ( ( func name , 0 , args , kargs ) )
def join ( self ) : self . tasks . join ( ) # Force each thread to break loop. for worker in self . workers : self . tasks . put ( None ) # Wait for all thread to terminate. for worker in self . workers : worker . join ( ) worker . s3 = None
def processed ( self ) : self . processed tasks += 1 qsize = self . tasks . qsize ( ) if qsize > 0 : progress ( '[%d task(s) completed, %d remaining, %d thread(s)]' , self . processed tasks , qsize , len ( self . workers ) ) else : progress ( '[%d task(s) completed, %d thread(s)]' , self . processed tasks , len ( self . workers ) )
def s3 keys from env ( ) : env = os . environ if S3 ACCESS KEY NAME in env and S3 SECRET KEY NAME in env : keys = ( env [ S3 ACCESS KEY NAME ] , env [ S3 SECRET KEY NAME ] ) debug ( "read S3 keys from environment" ) return keys else : return None
def s3 keys from cmdline ( opt ) : if opt . access key != None and opt . secret key != None : keys = ( opt . access key , opt . secret key ) debug ( "read S3 keys from commandline" ) return keys else : return None
def s3 keys from s3cfg ( opt ) : try : if opt . s3cfg != None : s3cfg path = "%s" % opt . s3cfg else : s3cfg path = "%s/.s3cfg" % os . environ [ "HOME" ] if not os . path . exists ( s3cfg path ) : return None config = Config Parser . Config Parser ( ) config . read ( s3cfg path ) keys = config . get ( "default" , "access key" ) , config . get ( "default" , "secret key" ) debug ( "read S3 keys from %s file" , s3cfg path ) return keys except Exception as e : info ( "could not read S3 keys from %s file; skipping (%s)" , s3cfg path , e ) return None
def init s3 keys ( opt ) : S3Handler . S3 KEYS = S3Handler . s3 keys from cmdline ( opt ) or S3Handler . s3 keys from env ( ) or S3Handler . s3 keys from s3cfg ( opt )
def connect ( self ) : try : if S3Handler . S3 KEYS : self . s3 = Boto Client ( self . opt , S3Handler . S3 KEYS [ 0 ] , S3Handler . S3 KEYS [ 1 ] ) else : self . s3 = Boto Client ( self . opt ) except Exception as e : raise Retry Failure ( 'Unable to connect to s3: %s' % e )
def local walk ( self , basedir ) : result = [ ] for root , dirs , files in os . walk ( basedir ) : for f in files : result . append ( os . path . join ( root , f ) ) return result
def put single file ( self , pool , source , target ) : if os . path . isdir ( source ) : if self . opt . recursive : for f in ( f for f in self . local walk ( source ) if not os . path . isdir ( f ) ) : target url = S3URL ( target ) # deal with ./ or ../ here by normalizing the path. joined path = os . path . normpath ( os . path . join ( target url . path , os . path . relpath ( f , source ) ) ) pool . upload ( f , S3URL . combine ( 's3' , target url . bucket , joined path ) ) else : message ( 'omitting directory "%s".' % source ) else : pool . upload ( source , target )
def create bucket ( self , source ) : s3url = S3URL ( source ) message ( 'Creating %s' , source ) if not self . opt . dry run : resp = self . s3 . create bucket ( Bucket = s3url . bucket ) if resp [ 'Response Metadata' ] [ "HTTP Status Code" ] == 200 : message ( 'Done.' ) else : raise Failure ( 'Unable to create bucket %s' % source )
def update privilege ( self , obj , target ) : if 'privilege' in obj [ 'Metadata' ] : os . chmod ( target , int ( obj [ 'Metadata' ] [ 'privilege' ] , 8 ) )
def print files ( self , source ) : sources = self . source expand ( source ) for source in sources : s3url = S3URL ( source ) response = self . s3 . get object ( Bucket = s3url . bucket , Key = s3url . path ) message ( '%s' , response [ 'Body' ] . read ( ) )
def get single file ( self , pool , source , target ) : if source [ - 1 ] == PATH SEP : if self . opt . recursive : basepath = S3URL ( source ) . path for f in ( f for f in self . s3walk ( source ) if not f [ 'is dir' ] ) : pool . download ( f [ 'name' ] , os . path . join ( target , os . path . relpath ( S3URL ( f [ 'name' ] ) . path , basepath ) ) ) else : message ( 'omitting directory "%s".' % source ) else : pool . download ( source , target )
def cp single file ( self , pool , source , target , delete source ) : if source [ - 1 ] == PATH SEP : if self . opt . recursive : basepath = S3URL ( source ) . path for f in ( f for f in self . s3walk ( source ) if not f [ 'is dir' ] ) : pool . copy ( f [ 'name' ] , os . path . join ( target , os . path . relpath ( S3URL ( f [ 'name' ] ) . path , basepath ) ) , delete source = delete source ) else : message ( 'omitting directory "%s".' % source ) else : pool . copy ( source , target , delete source = delete source )
def del files ( self , source ) : src files = [ ] for obj in self . s3walk ( source ) : if not obj [ 'is dir' ] : # ignore directories src files . append ( obj [ 'name' ] ) pool = Thread Pool ( Thread Util , self . opt ) pool . batch delete ( src files ) pool . join ( )
def dsync files ( self , source , target ) : src s3 url = S3URL . is valid ( source ) dst s3 url = S3URL . is valid ( target ) source list = self . relative dir walk ( source ) if len ( source list ) == 0 or '.' in source list : raise Failure ( 'Sync command need to sync directory to directory.' ) sync list = [ ( os . path . join ( source , f ) , os . path . join ( target , f ) ) for f in source list ] pool = Thread Pool ( Thread Util , self . opt ) if src s3 url and not dst s3 url : for src , dest in sync list : pool . download ( src , dest ) elif not src s3 url and dst s3 url : for src , dest in sync list : pool . upload ( src , dest ) elif src s3 url and dst s3 url : for src , dest in sync list : pool . copy ( src , dest ) else : raise Invalid Argument ( 'Cannot sync two local directories.' ) pool . join ( ) if self . opt . delete removed : target list = self . relative dir walk ( target ) remove list = [ os . path . join ( target , f ) for f in ( set ( target list ) - set ( source list ) ) ] if S3URL . is valid ( target ) : pool = Thread Pool ( Thread Util , self . opt ) pool . batch delete ( remove list ) pool . join ( ) else : for f in remove list : try : os . unlink ( f ) message ( 'Delete %s' , f ) except : pass
def file hash ( self , filename , block size = 2 ** 20 ) : m = hashlib . md5 ( ) with open ( filename , 'rb' ) as f : while True : data = f . read ( block size ) if not data : break m . update ( data ) return m . hexdigest ( )
def get md5 ( self ) : if self . md5 is None : self . md5 = self . file hash ( self . filename ) return self . md5
def mkdirs ( self , target ) : path = os . path . dirname ( target ) if path and path != PATH SEP and not os . path . isdir ( path ) : # Multi-threading means there will be intervleaved execution # between the check and creation of the directory. try : os . makedirs ( path ) except OS Error as ose : if ose . errno != errno . EEXIST : raise Failure ( 'Unable to create directory (%s)' % ( path , ) )
def conditional ( self , result , obj ) : fileonly = ( self . opt . last modified before is not None ) or ( self . opt . last modified after is not None ) if obj [ 'is dir' ] : if not fileonly : result . append ( obj ) return if ( self . opt . last modified before is not None ) and obj [ 'last modified' ] >= self . opt . last modified before : return if ( self . opt . last modified after is not None ) and obj [ 'last modified' ] <= self . opt . last modified after : return result . append ( obj )
def get file privilege ( self , source ) : try : return str ( oct ( os . stat ( source ) . st mode ) [ - 3 : ] ) except Exception as e : raise Failure ( 'Could not get stat for %s, error message = %s' , source , e )
def lookup ( self , s3url ) : try : return self . s3 . head object ( Bucket = s3url . bucket , Key = s3url . path ) except Boto Client . Client Error as e : if e . response [ 'Response Metadata' ] [ 'HTTP Status Code' ] == 404 : return None else : raise e
def read file chunk ( self , source , pos , chunk ) : if chunk == 0 : return String IO ( ) data = None with open ( source , 'rb' ) as f : f . seek ( pos ) data = f . read ( chunk ) if not data : raise Failure ( 'Unable to read data from source: %s' % source ) return String IO ( data )
def upload ( self , source , target , mpi = None , pos = 0 , chunk = 0 , part = 0 ) : s3url = S3URL ( target ) obj = self . lookup ( s3url ) # Initialization: Set up multithreaded uploads. if not mpi : fsize = os . path . getsize ( source ) md5cache = Local MD5Cache ( source ) # optional checks if self . opt . dry run : message ( '%s => %s' , source , target ) return elif self . opt . sync check and self . sync check ( md5cache , obj ) : message ( '%s => %s (synced)' , source , target ) return elif not self . opt . force and obj : raise Failure ( 'File already exists: %s' % target ) if fsize < self . opt . max singlepart upload size : data = self . read file chunk ( source , 0 , fsize ) self . s3 . put object ( Bucket = s3url . bucket , Key = s3url . path , Body = data , Metadata = { 'md5' : md5cache . get md5 ( ) , 'privilege' : self . get file privilege ( source ) } ) message ( '%s => %s' , source , target ) return # Here we need to have our own md5 value because multipart upload calculates # different md5 values. response = self . s3 . create multipart upload ( Bucket = s3url . bucket , Key = s3url . path , Metadata = { 'md5' : md5cache . get md5 ( ) , 'privilege' : self . get file privilege ( source ) } ) upload id = response [ 'Upload Id' ] for args in self . get file splits ( upload id , source , target , fsize , self . opt . multipart split size ) : self . pool . upload ( * args ) return data = self . read file chunk ( source , pos , chunk ) response = self . s3 . upload part ( Bucket = s3url . bucket , Key = s3url . path , Upload Id = mpi . id , Body = data , Part Number = part ) # Finalize if mpi . complete ( { 'E Tag' : response [ 'E Tag' ] , 'Part Number' : part } ) : try : self . s3 . complete multipart upload ( Bucket = s3url . bucket , Key = s3url . path , Upload Id = mpi . id , Multipart Upload = { 'Parts' : mpi . sorted parts ( ) } ) message ( '%s => %s' , source , target ) except Exception as e : message ( 'Unable to complete upload: %s' , str ( e ) ) self . s3 . abort multipart upload ( Bucket = s3url . bucket , Key = s3url . path , Upload Id = mpi . id ) raise Retry Failure ( 'Upload failed: Unable to complete upload %s.' % source )
def verify file size ( self , obj , downloaded file ) : file size = os . path . getsize ( downloaded file ) if int ( obj [ 'Content Length' ] ) != file size : raise Retry Failure ( 'Downloaded file size inconsistent: %s' % ( repr ( obj ) ) )
def write file chunk ( self , target , pos , chunk , body ) : fd = os . open ( target , os . O CREAT | os . O WRONLY ) try : os . lseek ( fd , pos , os . SEEK SET ) data = body . read ( chunk ) num bytes written = os . write ( fd , data ) if ( num bytes written != len ( data ) ) : raise Retry Failure ( 'Number of bytes written inconsistent: %s != %s' % ( num bytes written , sys . getsizeof ( data ) ) ) finally : os . close ( fd )
def download ( self , source , target , mpi = None , pos = 0 , chunk = 0 , part = 0 ) : s3url = S3URL ( source ) obj = self . lookup ( s3url ) if obj is None : raise Failure ( 'The obj "%s" does not exists.' % ( s3url . path , ) ) # Initialization: Set up multithreaded downloads. if not mpi : # optional checks if self . opt . dry run : message ( '%s => %s' , source , target ) return elif self . opt . sync check and self . sync check ( Local MD5Cache ( target ) , obj ) : message ( '%s => %s (synced)' , source , target ) return elif not self . opt . force and os . path . exists ( target ) : raise Failure ( 'File already exists: %s' % target ) fsize = int ( obj [ 'Content Length' ] ) # Small file optimization. if fsize < self . opt . max singlepart download size : # Create a single part to chain back main download operation. mpi = Thread Util . Multipart Item ( tempfile get ( target ) ) mpi . total = 1 pos = 0 chunk = fsize # Continue as one part download. else : # Here we use temp filename as the id of mpi. for args in self . get file splits ( tempfile get ( target ) , source , target , fsize , self . opt . multipart split size ) : self . pool . download ( * args ) return tempfile = mpi . id if self . opt . recursive : self . mkdirs ( tempfile ) # Download part of the file, range is inclusive. response = self . s3 . get object ( Bucket = s3url . bucket , Key = s3url . path , Range = 'bytes=%d-%d' % ( pos , pos + chunk - 1 ) ) self . write file chunk ( tempfile , pos , chunk , response [ 'Body' ] ) # Finalize if mpi . complete ( { 'Part Number' : part } ) : try : self . update privilege ( obj , tempfile ) self . verify file size ( obj , tempfile ) tempfile set ( tempfile , target ) message ( '%s => %s' , source , target ) except Exception as e : # Note that we don't retry in this case, because # We are going to remove the temp file, and if we # retry here with original parameters (wrapped in # the task item), it would fail anyway tempfile set ( tempfile , None ) raise Failure ( 'Download Failure: %s, Source: %s.' % ( e . message , source ) )
def copy ( self , source , target , mpi = None , pos = 0 , chunk = 0 , part = 0 , delete source = False ) : if self . opt . dry run : message ( '%s => %s' % ( source , target ) ) return source url = S3URL ( source ) target url = S3URL ( target ) if not mpi : obj = self . lookup ( source url ) fsize = int ( obj [ 'Content Length' ] ) if fsize < self . opt . max singlepart copy size : self . s3 . copy object ( Bucket = target url . bucket , Key = target url . path , Copy Source = { 'Bucket' : source url . bucket , 'Key' : source url . path } ) message ( '%s => %s' % ( source , target ) ) if delete source : self . delete ( source ) return response = self . s3 . create multipart upload ( Bucket = target url . bucket , Key = target url . path , Metadata = obj [ 'Metadata' ] ) upload id = response [ 'Upload Id' ] for args in self . get file splits ( upload id , source , target , fsize , self . opt . multipart split size ) : self . pool . copy ( * args , delete source = delete source ) return response = self . s3 . upload part copy ( Bucket = target url . bucket , Key = target url . path , Copy Source = { 'Bucket' : source url . bucket , 'Key' : source url . path } , Copy Source Range = 'bytes=%d-%d' % ( pos , pos + chunk - 1 ) , Upload Id = mpi . id , Part Number = part ) if mpi . complete ( { 'E Tag' : response [ 'Copy Part Result' ] [ 'E Tag' ] , 'Part Number' : part } ) : try : # Finalize copy operation. self . s3 . complete multipart upload ( Bucket = target url . bucket , Key = target url . path , Upload Id = mpi . id , Multipart Upload = { 'Parts' : mpi . sorted parts ( ) } ) if delete source : self . delete ( source ) message ( '%s => %s' % ( source , target ) ) except Exception as e : message ( 'Unable to complete upload: %s' , str ( e ) ) self . s3 . abort multipart upload ( Bucket = source url . bucket , Key = source url . path , Upload Id = mpi . id ) raise Retry Failure ( 'Copy failed: Unable to complete copy %s.' % source )
def delete ( self , source ) : s3url = S3URL ( source ) message ( 'Delete %s' , source ) if not self . opt . dry run : self . s3 . delete object ( Bucket = s3url . bucket , Key = s3url . path )
def run ( self , args ) : if len ( args ) == 0 : raise Invalid Argument ( 'No command provided' ) cmd = args [ 0 ] if cmd + ' handler' in Command Handler . dict : Command Handler . dict [ cmd + ' handler' ] ( self , args ) else : raise Invalid Argument ( 'Unknown command %s' % cmd )
def ls handler ( self , args ) : if len ( args ) == 1 : self . pretty print ( self . s3handler ( ) . list buckets ( ) ) return self . validate ( 'cmd|s3' , args ) self . pretty print ( self . s3handler ( ) . s3walk ( args [ 1 ] ) )
def mb handler ( self , args ) : if len ( args ) == 1 : raise Invalid Argument ( 'No s3 bucketname provided' ) self . validate ( 'cmd|s3' , args ) self . s3handler ( ) . create bucket ( args [ 1 ] )
def put handler ( self , args ) : # Special check for shell expansion if len ( args ) < 3 : raise Invalid Argument ( 'Invalid number of parameters' ) self . validate ( '|' . join ( [ 'cmd' ] + [ 'local' ] * ( len ( args ) - 2 ) + [ 's3' ] ) , args ) source = args [ 1 : - 1 ] # shell expansion target = args [ - 1 ] self . s3handler ( ) . put files ( source , target )
def get handler ( self , args ) : # Special case when we don't have target directory. if len ( args ) == 2 : args += [ '.' ] self . validate ( 'cmd|s3|local' , args ) source = args [ 1 ] target = args [ 2 ] self . s3handler ( ) . get files ( source , target )
def cat handler ( self , args ) : self . validate ( 'cmd|s3' , args ) source = args [ 1 ] self . s3handler ( ) . print files ( source )
def dsync handler ( self , args ) : self . opt . recursive = True self . opt . sync check = True self . opt . force = True self . validate ( 'cmd|s3,local|s3,local' , args ) source = args [ 1 ] target = args [ 2 ] self . s3handler ( ) . dsync files ( source , target )
def cp handler ( self , args ) : self . validate ( 'cmd|s3|s3' , args ) source = args [ 1 ] target = args [ 2 ] self . s3handler ( ) . cp files ( source , target )
def mv handler ( self , args ) : self . validate ( 'cmd|s3|s3' , args ) source = args [ 1 ] target = args [ 2 ] self . s3handler ( ) . cp files ( source , target , delete source = True )
def del handler ( self , args ) : self . validate ( 'cmd|s3' , args ) source = args [ 1 ] self . s3handler ( ) . del files ( source )
def du handler ( self , args ) : for src , size in self . s3handler ( ) . size ( args [ 1 : ] ) : message ( '%s\t%s' % ( size , src ) )
def totalsize handler ( self , args ) : total size = 0 for src , size in self . s3handler ( ) . size ( args [ 1 : ] ) : total size += size message ( str ( total size ) )
def match date ( self , value ) : m = self . REGEX DATE . search ( value ) date = datetime . datetime . utcnow ( ) . date ( ) if m : date = datetime . date ( int ( m . group ( 1 ) ) , int ( m . group ( 2 ) ) , int ( m . group ( 3 ) ) ) value = self . REGEX DATE . sub ( '' , value ) return ( date , value )
def match time ( self , value ) : m = self . REGEX TIME . search ( value ) time = datetime . datetime . utcnow ( ) . time ( ) if m : time = datetime . time ( int ( m . group ( 1 ) ) , int ( m . group ( 2 ) ) ) value = self . REGEX TIME . sub ( '' , value ) return ( time , value )
def match delta ( self , value ) : m = self . REGEX DELTA . search ( value ) delta = datetime . timedelta ( days = 0 ) if m : d = int ( m . group ( 1 ) ) if m . group ( 3 ) == 'ago' or m . group ( 3 ) == 'before' : d = - d if m . group ( 2 ) == 'minute' : delta = datetime . timedelta ( minutes = d ) elif m . group ( 2 ) == 'hour' : delta = datetime . timedelta ( hours = d ) elif m . group ( 2 ) == 'day' : delta = datetime . timedelta ( days = d ) elif m . group ( 2 ) == 'week' : delta = datetime . timedelta ( weeks = d ) value = self . REGEX DELTA . sub ( '' , value ) return ( delta , value )
def check dict ( self , opt , value ) : try : return json . loads ( value ) except : raise optparse . Option Value Error ( "Option %s: invalid dict value: %r" % ( opt , value ) )
def discover gateways ( self ) : socket = socket . socket ( socket . AF INET , socket . SOCK DGRAM ) socket . settimeout ( 5.0 ) if self . interface != 'any' : socket . bind ( ( self . interface , 0 ) ) for gateway in self . gateways config : host = gateway . get ( 'host' ) port = gateway . get ( 'port' ) sid = gateway . get ( 'sid' ) if not ( host and port and sid ) : continue try : ip address = socket . gethostbyname ( host ) if gateway . get ( 'disable' ) : LOGGER . info ( 'Xiaomi Gateway %s is disabled by configuration' , sid ) self . disabled gateways . append ( ip address ) continue LOGGER . info ( 'Xiaomi Gateway %s configured at IP %s:%s' , sid , ip address , port ) self . gateways [ ip address ] = Xiaomi Gateway ( ip address , port , sid , gateway . get ( 'key' ) , self . device discovery retries , self . interface , gateway . get ( 'proto' ) ) except OS Error as error : LOGGER . error ( "Could not resolve %s: %s" , host , error ) try : socket . sendto ( '{"cmd":"whois"}' . encode ( ) , ( self . MULTICAST ADDRESS , self . GATEWAY DISCOVERY PORT ) ) while True : data , ( ip add , ) = socket . recvfrom ( 1024 ) if len ( data ) is None or ip add in self . gateways : continue if ip add in self . gateways . keys ( ) or ip add in self . disabled gateways : continue resp = json . loads ( data . decode ( ) ) if resp [ "cmd" ] != 'iam' : LOGGER . error ( "Response does not match return cmd" ) continue if resp [ "model" ] not in GATEWAY MODELS : LOGGER . error ( "Response must be gateway model" ) continue disabled = False gateway key = None for gateway in self . gateways config : sid = gateway . get ( 'sid' ) if sid is None or sid == resp [ "sid" ] : gateway key = gateway . get ( 'key' ) if sid and sid == resp [ 'sid' ] and gateway . get ( 'disable' ) : disabled = True sid = resp [ "sid" ] if disabled : LOGGER . info ( "Xiaomi Gateway %s is disabled by configuration" , sid ) self . disabled gateways . append ( ip add ) else : LOGGER . info ( 'Xiaomi Gateway %s found at IP %s' , sid , ip add ) self . gateways [ ip add ] = Xiaomi Gateway ( ip add , resp [ "port" ] , sid , gateway key , self . device discovery retries , self . interface , resp [ "proto version" ] if "proto version" in resp else None ) except socket . timeout : LOGGER . info ( "Gateway discovery finished in 5 seconds" ) socket . close ( )
def get from hub ( self , sid ) : cmd = '{ "cmd":"read","sid":"' + sid + '"}' resp = self . send cmd ( cmd , "read ack" ) if int ( self . proto [ 0 : 1 ] ) == 1 else self . send cmd ( cmd , "read rsp" ) LOGGER . debug ( "read ack << %s" , resp ) return self . push data ( resp )
def push data ( self , data ) : if not validate data ( data ) : return False jdata = json . loads ( data [ 'data' ] ) if int ( self . proto [ 0 : 1 ] ) == 1 else list2map ( data [ 'params' ] ) if jdata is None : return False sid = data [ 'sid' ] for func in self . callbacks [ sid ] : func ( jdata , data ) return True
def get key ( self ) : init vector = bytes ( bytearray . fromhex ( '17996d093d28ddb3ba695a2e6f58562e' ) ) encryptor = Cipher ( algorithms . AES ( self . key . encode ( ) ) , modes . CBC ( init vector ) , backend = default backend ( ) ) . encryptor ( ) ciphertext = encryptor . update ( self . token . encode ( ) ) + encryptor . finalize ( ) if isinstance ( ciphertext , str ) : # For Python 2 compatibility return '' . join ( '{:02x}' . format ( ord ( x ) ) for x in ciphertext ) return '' . join ( '{:02x}' . format ( x ) for x in ciphertext )
def ensure log handler ( self ) : if log . handlers : return handler = logging . Stream Handler ( ) formatter = logging . Formatter ( '%(asctime)s %(levelname)-5.5s [%(name)s][%(thread Name)s] %(message)s' ) handler . set Formatter ( formatter ) log . add Handler ( handler )
def lambda function ( f ) : @ functools . wraps ( f ) def wrapper ( event , context ) : global CURRENT LAMBDA CONTEXT CURRENT LAMBDA CONTEXT = context try : result = f ( event , context ) return wait ( lambda : result ) except : cls , exc , trace = sys . exc info ( ) report exc info ( ( cls , exc , trace . tb next ) ) wait ( ) raise return wrapper
def create agent log ( ) : log file = SETTINGS [ 'agent.log file' ] if not log file . endswith ( '.rollbar' ) : log . error ( "Provided agent log file does not end with .rollbar, which it must. " "Using default instead." ) log file = DEFAULTS [ 'agent.log file' ] retval = logging . get Logger ( 'rollbar agent' ) handler = logging . File Handler ( log file , 'a' , 'utf-8' ) formatter = logging . Formatter ( '%(message)s' ) handler . set Formatter ( formatter ) retval . add Handler ( handler ) retval . set Level ( logging . WARNING ) return retval
def add lambda context data ( data ) : global CURRENT LAMBDA CONTEXT context = CURRENT LAMBDA CONTEXT if context is None : return try : lambda data = { 'lambda' : { 'remaining time in millis' : context . get remaining time in millis ( ) , 'function name' : context . function name , 'function version' : context . function version , 'arn' : context . invoked function arn , 'request id' : context . aws request id , } } if 'custom' in data : data [ 'custom' ] = dict merge ( data [ 'custom' ] , lambda data ) else : data [ 'custom' ] = lambda data except Exception as e : log . exception ( "Exception while adding lambda context data: %r" , e ) finally : CURRENT LAMBDA CONTEXT = None
def add request data ( data , request ) : try : request data = build request data ( request ) except Exception as e : log . exception ( "Exception while building request data for Rollbar payload: %r" , e ) else : if request data : filter ip ( request data , SETTINGS [ 'capture ip' ] ) data [ 'request' ] = request data
def check add locals ( frame , frame num , total frames ) : # Include the last frames locals # Include any frame locals that came from a file in the project's root return any ( ( ( frame num == total frames - 1 ) , ( 'root' in SETTINGS and ( frame . get ( 'filename' ) or '' ) . lower ( ) . startswith ( ( SETTINGS [ 'root' ] or '' ) . lower ( ) ) ) ) )
def build server data ( ) : # server environment server data = { 'host' : socket . gethostname ( ) , 'pid' : os . getpid ( ) } # argv does not always exist in embedded python environments argv = getattr ( sys , 'argv' , None ) if argv : server data [ 'argv' ] = argv for key in [ 'branch' , 'root' ] : if SETTINGS . get ( key ) : server data [ key ] = SETTINGS [ key ] return server data
def build payload ( data ) : for k , v in iteritems ( data ) : data [ k ] = transform ( v , key = ( k , ) ) payload = { 'access token' : SETTINGS [ 'access token' ] , 'data' : data } return payload
def main ( ) : rollbar . init ( 'ACCESS TOKEN' , environment = 'test' , handler = 'twisted' ) factory = protocol . Server Factory ( ) factory . protocol = Echo reactor . listen TCP ( 8000 , factory ) reactor . run ( )
def decompose ( hangul letter ) : from . import checker if len ( hangul letter ) < 1 : raise Not Letter Exception ( '' ) elif not checker . is hangul ( hangul letter ) : raise Not Hangul Exception ( '' ) if hangul letter in CHO : return hangul letter , '' , '' if hangul letter in JOONG : return '' , hangul letter , '' if hangul letter in JONG : return '' , '' , hangul letter code = hangul index ( hangul letter ) cho , joong , jong = decompose index ( code ) if cho < 0 : cho = 0 try : return CHO [ cho ] , JOONG [ joong ] , JONG [ jong ] except : print ( "%d / %d  / %d" % ( cho , joong , jong ) ) print ( "%s / %s " % ( JOONG [ joong ] . encode ( "utf8" ) , JONG [ jong ] . encode ( 'utf8' ) ) ) raise Exception ( )
def has jongsung ( letter ) : if len ( letter ) != 1 : raise Exception ( 'The target string must be one letter.' ) if not is hangul ( letter ) : raise Not Hangul Exception ( 'The target string must be Hangul' ) code = lt . hangul index ( letter ) return code % NUM JONG > 0
def attach ( word , josa = EUN NEUN ) : last letter = word . strip ( ) [ - 1 ] try : , , letter jong = letter . decompose ( last letter ) except Not Hangul Exception : letter jong = letter . get substituent of ( last letter ) if letter jong in ( '' , josa [ 'except' ] ) : return word + josa [ 'has' ] return word + josa [ 'not' ]
def is inside except ( node ) : current = node while current and not isinstance ( current . parent , astroid . Except Handler ) : current = current . parent return current and current is current . parent . name
def is inside lambda ( node : astroid . node classes . Node NG ) -> bool : parent = node . parent while parent is not None : if isinstance ( parent , astroid . Lambda ) : return True parent = parent . parent return False
def get all elements ( node : astroid . node classes . Node NG ) -> Iterable [ astroid . node classes . Node NG ] : if isinstance ( node , ( astroid . Tuple , astroid . List ) ) : for child in node . elts : for e in get all elements ( child ) : yield e else : yield node
def is super ( node : astroid . node classes . Node NG ) -> bool : if getattr ( node , "name" , None ) == "super" and node . root ( ) . name == BUILTINS NAME : return True return False
def is error ( node : astroid . node classes . Node NG ) -> bool : for child node in node . get children ( ) : if isinstance ( child node , astroid . Raise ) : return True return False
def is builtin object ( node : astroid . node classes . Node NG ) -> bool : return node and node . root ( ) . name == BUILTINS NAME
def is func decorator ( node : astroid . node classes . Node NG ) -> bool : parent = node . parent while parent is not None : if isinstance ( parent , astroid . Decorators ) : return True if parent . is statement or isinstance ( parent , ( astroid . Lambda , scoped nodes . Comprehension Scope , scoped nodes . List Comp ) , ) : break parent = parent . parent return False
def assign parent ( node : astroid . node classes . Node NG ) -> astroid . node classes . Node NG : while node and isinstance ( node , ( astroid . Assign Name , astroid . Tuple , astroid . List ) ) : node = node . parent return node
def check messages ( * messages : str ) -> Callable : def store messages ( func ) : func . checks msgs = messages return func return store messages
def decorated with property ( node : astroid . Function Def ) -> bool : if not node . decorators : return False for decorator in node . decorators . nodes : if not isinstance ( decorator , astroid . Name ) : continue try : if is property decorator ( decorator ) : return True except astroid . Inference Error : pass return False
def decorated with ( func : astroid . Function Def , qnames : Iterable [ str ] ) -> bool : decorators = func . decorators . nodes if func . decorators else [ ] for decorator node in decorators : try : if any ( i is not None and i . qname ( ) in qnames for i in decorator node . infer ( ) ) : return True except astroid . Inference Error : continue return False
def find try except wrapper node ( node : astroid . node classes . Node NG ) -> Union [ astroid . Except Handler , astroid . Try Except ] : current = node ignores = ( astroid . Except Handler , astroid . Try Except ) while current and not isinstance ( current . parent , ignores ) : current = current . parent if current and isinstance ( current . parent , ignores ) : return current . parent return None
def is from fallback block ( node : astroid . node classes . Node NG ) -> bool : context = find try except wrapper node ( node ) if not context : return False if isinstance ( context , astroid . Except Handler ) : other body = context . parent . body handlers = context . parent . handlers else : other body = itertools . chain . from iterable ( handler . body for handler in context . handlers ) handlers = context . handlers has fallback imports = any ( isinstance ( import node , ( astroid . Import From , astroid . Import ) ) for import node in other body ) ignores import error = except handlers ignores exception ( handlers , Import Error ) return ignores import error or has fallback imports
def is registered in singledispatch function ( node : astroid . Function Def ) -> bool : singledispatch qnames = ( "functools.singledispatch" , "singledispatch.singledispatch" , ) if not isinstance ( node , astroid . Function Def ) : return False decorators = node . decorators . nodes if node . decorators else [ ] for decorator in decorators : # func.register are function calls if not isinstance ( decorator , astroid . Call ) : continue func = decorator . func if not isinstance ( func , astroid . Attribute ) or func . attrname != "register" : continue try : func def = next ( func . expr . infer ( ) ) except astroid . Inference Error : continue if isinstance ( func def , astroid . Function Def ) : # pylint: disable=redundant-keyword-arg; some flow inference goes wrong here return decorated with ( func def , singledispatch qnames ) return False
def is postponed evaluation enabled ( node : astroid . node classes . Node NG ) -> bool : name = "annotations" module = node . root ( ) stmt = module . locals . get ( name ) return ( stmt and isinstance ( stmt [ 0 ] , astroid . Import From ) and stmt [ 0 ] . modname == " future " )
def repr tree defs ( data , indent str = None ) : lines = [ ] nodes = data . items ( ) for i , ( mod , ( sub , files ) ) in enumerate ( sorted ( nodes , key = lambda x : x [ 0 ] ) ) : if not files : files = "" else : files = "(%s)" % "," . join ( sorted ( files ) ) if indent str is None : lines . append ( "%s %s" % ( mod , files ) ) sub indent str = "  " else : lines . append ( r"%s\-%s %s" % ( indent str , mod , files ) ) if i == len ( nodes ) - 1 : sub indent str = "%s  " % indent str else : sub indent str = "%s| " % indent str if sub : lines . append ( repr tree defs ( sub , sub indent str ) ) return "\n" . join ( lines )
def visit import ( self , node ) : self . check reimport ( node ) self . check import as rename ( node ) modnode = node . root ( ) names = [ name for name , in node . names ] if len ( names ) >= 2 : self . add message ( "multiple-imports" , args = ", " . join ( names ) , node = node ) for name in names : self . check deprecated module ( node , name ) self . check preferred module ( node , name ) imported module = self . get imported module ( node , name ) if isinstance ( node . parent , astroid . Module ) : # Allow imports nested self . check position ( node ) if isinstance ( node . scope ( ) , astroid . Module ) : self . record import ( node , imported module ) if imported module is None : continue self . check relative import ( modnode , node , imported module , name ) self . add imported module ( node , imported module . name )
def visit importfrom ( self , node ) : basename = node . modname imported module = self . get imported module ( node , basename ) self . check import as rename ( node ) self . check misplaced future ( node ) self . check deprecated module ( node , basename ) self . check preferred module ( node , basename ) self . check wildcard imports ( node , imported module ) self . check same line imports ( node ) self . check reimport ( node , basename = basename , level = node . level ) if isinstance ( node . parent , astroid . Module ) : # Allow imports nested self . check position ( node ) if isinstance ( node . scope ( ) , astroid . Module ) : self . record import ( node , imported module ) if imported module is None : return modnode = node . root ( ) self . check relative import ( modnode , node , imported module , basename ) for name , in node . names : if name != "*" : self . add imported module ( node , "%s.%s" % ( imported module . name , name ) ) else : self . add imported module ( node , imported module . name )
def record import ( self , node , importedmodnode ) : if isinstance ( node , astroid . Import From ) : importedname = node . modname else : importedname = importedmodnode . name if importedmodnode else None if not importedname : importedname = node . names [ 0 ] [ 0 ] . split ( "." ) [ 0 ] if isinstance ( node , astroid . Import From ) and ( node . level or 0 ) >= 1 : # We need the importedname with first point to detect local package # Example of node: #  'from .my package1 import My Class1' #  the output should be '.my package1' instead of 'my package1' # Example of node: #  'from . import my package2' #  the output should be '.my package2' instead of '{pyfile}' importedname = "." + importedname self . imports stack . append ( ( node , importedname ) )
def add imported module ( self , node , importedmodname ) : module file = node . root ( ) . file context name = node . root ( ) . name base = os . path . splitext ( os . path . basename ( module file ) ) [ 0 ] try : importedmodname = astroid . modutils . get module part ( importedmodname , module file ) except Import Error : pass if context name == importedmodname : self . add message ( "import-self" , node = node ) elif not astroid . modutils . is standard module ( importedmodname ) : # if this is not a package  init  module if base != " init " and context name not in self . module pkg : # record the module's parent, or the module itself if this is # a top level module, as the package it belongs to self . module pkg [ context name ] = context name . rsplit ( "." , 1 ) [ 0 ] # handle dependencies importedmodnames = self . stats [ "dependencies" ] . setdefault ( importedmodname , set ( ) ) if context name not in importedmodnames : importedmodnames . add ( context name ) # update import graph self . import graph [ context name ] . add ( importedmodname ) if not self . linter . is message enabled ( "cyclic-import" , line = node . lineno ) : self . excluded edges [ context name ] . add ( importedmodname )
def check deprecated module ( self , node , mod path ) : for mod name in self . config . deprecated modules : if mod path == mod name or mod path . startswith ( mod name + "." ) : self . add message ( "deprecated-module" , node = node , args = mod path )
def check preferred module ( self , node , mod path ) : if mod path in self . preferred modules : self . add message ( "preferred-module" , node = node , args = ( self . preferred modules [ mod path ] , mod path ) , )
def report external dependencies ( self , sect , , dummy ) : dep info = make tree defs ( self . external dependencies info ( ) . items ( ) ) if not dep info : raise Empty Report Error ( ) tree str = repr tree defs ( dep info ) sect . append ( Verbatim Text ( tree str ) )
def filter dependencies graph ( self , internal ) : graph = collections . defaultdict ( set ) for importee , importers in self . stats [ "dependencies" ] . items ( ) : for importer in importers : package = self . module pkg . get ( importer , importer ) is inside = importee . startswith ( package ) if is inside and internal or not is inside and not internal : graph [ importee ] . add ( importer ) return graph
def get default options ( ) : options = [ ] home = os . environ . get ( "HOME" , "" ) if home : rcfile = os . path . join ( home , RCFILE ) try : options = open ( rcfile ) . read ( ) . split ( ) except IO Error : pass # ignore if no config file found return options
def insert default options ( ) : options = get default options ( ) options . reverse ( ) for arg in options : sys . argv . insert ( 1 , arg )
def show attr ( self , node ) : visibility = get visibility ( getattr ( node , "name" , node ) ) return not self . mode & VIS MOD [ visibility ]
def get callbacks ( self , node ) : klass = node . class methods = self . cache . get ( klass ) if methods is None : handler = self . handler kid = klass . name . lower ( ) e method = getattr ( handler , "visit %s" % kid , getattr ( handler , "visit default" , None ) ) l method = getattr ( handler , "leave %s" % kid , getattr ( handler , "leave default" , None ) ) self . cache [ klass ] = ( e method , l method ) else : e method , l method = methods return e method , l method
def visit ( self , node ) : if node in self . visited : return None self . visited [ node ] = 1 # FIXME: use set ? methods = self . get callbacks ( node ) if methods [ 0 ] is not None : methods [ 0 ] ( node ) if hasattr ( node , "locals" ) : # skip Instance and other proxy for local node in node . values ( ) : self . visit ( local node ) if methods [ 1 ] is not None : return methods [ 1 ] ( node ) return None
def visit call ( self , node ) : try : for inferred in node . func . infer ( ) : if inferred is astroid . Uninferable : continue elif inferred . root ( ) . name == OPEN MODULE : if getattr ( node . func , "name" , None ) in OPEN FILES : self . check open mode ( node ) elif inferred . root ( ) . name == UNITTEST CASE : self . check redundant assert ( node , inferred ) elif isinstance ( inferred , astroid . Class Def ) : if inferred . qname ( ) == THREADING THREAD : self . check bad thread instantiation ( node ) elif inferred . qname ( ) == SUBPROCESS POPEN : self . check for preexec fn in popen ( node ) elif isinstance ( inferred , astroid . Function Def ) : name = inferred . qname ( ) if name == COPY COPY : self . check shallow copy environ ( node ) elif name in ENV GETTERS : self . check env function ( node , inferred ) elif name == SUBPROCESS RUN and PY35 : self . check for check kw in run ( node ) self . check deprecated method ( node , inferred ) except astroid . Inference Error : return
def check open mode ( self , node ) : try : mode arg = utils . get argument from call ( node , position = 1 , keyword = "mode" ) except utils . No Such Argument Error : return if mode arg : mode arg = utils . safe infer ( mode arg ) if isinstance ( mode arg , astroid . Const ) and not check mode str ( mode arg . value ) : self . add message ( "bad-open-mode" , node = node , args = mode arg . value )
def handle message ( self , msg ) : self . messages . append ( { "type" : msg . category , "module" : msg . module , "obj" : msg . obj , "line" : msg . line , "column" : msg . column , "path" : msg . path , "symbol" : msg . symbol , "message" : html . escape ( msg . msg or "" , quote = False ) , "message-id" : msg . msg id , } )
def get title ( self , node ) : title = node . name if self . module names : title = "%s.%s" % ( node . root ( ) . name , title ) return title
def set default options ( self ) : self . module names = self . set option ( self . config . module names ) all ancestors = self . set option ( self . config . all ancestors ) all associated = self . set option ( self . config . all associated ) anc level , association level = ( 0 , 0 ) if all ancestors : anc level = - 1 if all associated : association level = - 1 if self . config . show ancestors is not None : anc level = self . config . show ancestors if self . config . show associated is not None : association level = self . config . show associated self . anc level , self . association level = anc level , association level
def show node ( self , node ) : if self . config . show builtin : return True return node . root ( ) . name != BUILTINS NAME
def add class ( self , node ) : self . linker . visit ( node ) self . classdiagram . add object ( self . get title ( node ) , node )
def get ancestors ( self , node , level ) : if level == 0 : return for ancestor in node . ancestors ( recurs = False ) : if not self . show node ( ancestor ) : continue yield ancestor
def get associated ( self , klass node , level ) : if level == 0 : return for association nodes in list ( klass node . instance attrs type . values ( ) ) + list ( klass node . locals type . values ( ) ) : for node in association nodes : if isinstance ( node , astroid . Instance ) : node = node . proxied if not ( isinstance ( node , astroid . Class Def ) and self . show node ( node ) ) : continue yield node
def extract classes ( self , klass node , anc level , association level ) : if self . classdiagram . has node ( klass node ) or not self . show node ( klass node ) : return self . add class ( klass node ) for ancestor in self . get ancestors ( klass node , anc level ) : self . extract classes ( ancestor , anc level - 1 , association level ) for node in self . get associated ( klass node , association level ) : self . extract classes ( node , anc level , association level - 1 )
def visit importfrom ( self , node ) : if self . pkgdiagram : self . pkgdiagram . add from depend ( node , node . modname )
def has parent of type ( node , node type , statement ) : parent = node . parent while not isinstance ( parent , node type ) and statement . parent of ( parent ) : parent = parent . parent return isinstance ( parent , node type )
def is name used as variadic ( name , variadics ) : return any ( variadic . value == name or variadic . value . parent of ( name ) for variadic in variadics )
def register ( linter ) : linter . register checker ( Type Checker ( linter ) ) linter . register checker ( Iterable Checker ( linter ) )
def visit unaryop ( self , node ) : for error in node . type errors ( ) : # Let the error customize its output. self . add message ( "invalid-unary-operand-type" , args = str ( error ) , node = node )
def interfaces ( node , herited = True , handler func = iface hdlr ) : # FIXME: what if  implements  = (My I Face, My Parent. implements )... try : implements = bases . Instance ( node ) . getattr ( " implements " ) [ 0 ] except exceptions . Not Found Error : return if not herited and implements . frame ( ) is not node : return found = set ( ) missing = False for iface in node classes . unpack infer ( implements ) : if iface is astroid . Uninferable : missing = True continue if iface not in found and handler func ( iface ) : found . add ( iface ) yield iface if missing : raise exceptions . Inference Error ( )
def project from files ( files , func wrapper = astroid wrapper , project name = "no name" , black list = ( "CVS" , ) ) : # build the project representation astroid manager = manager . Astroid Manager ( ) project = Project ( project name ) for something in files : if not os . path . exists ( something ) : fpath = modutils . file from modpath ( something . split ( "." ) ) elif os . path . isdir ( something ) : fpath = os . path . join ( something , " init .py" ) else : fpath = something ast = func wrapper ( astroid manager . ast from file , fpath ) if ast is None : continue # XXX why is first file defining the project.path ? project . path = project . path or ast . file project . add module ( ast ) base name = ast . name # recurse in package except if  init  was explicitly given if ast . package and something . find ( " init " ) == - 1 : # recurse on others packages / modules if this is a package for fpath in modutils . get module files ( os . path . dirname ( ast . file ) , black list ) : ast = func wrapper ( astroid manager . ast from file , fpath ) if ast is None or ast . name == base name : continue project . add module ( ast ) return project
def compute module ( self , context name , mod path ) : package dir = os . path . dirname ( self . project . path ) if context name == mod path : return 0 if modutils . is standard module ( mod path , ( package dir , ) ) : return 1 return 0
def imported module ( self , node , mod path , relative ) : module = node . root ( ) context name = module . name if relative : mod path = "%s.%s" % ( "." . join ( context name . split ( "." ) [ : - 1 ] ) , mod path ) if self . compute module ( context name , mod path ) : # handle dependencies if not hasattr ( module , "depends" ) : module . depends = [ ] mod paths = module . depends if mod path not in mod paths : mod paths . append ( mod path )
def register ( linter ) : linter . register reporter ( Text Reporter ) linter . register reporter ( Parseable Text Reporter ) linter . register reporter ( VS Text Reporter ) linter . register reporter ( Colorized Text Reporter )
def handle message ( self , msg ) : if msg . module not in self . modules : if msg . module : self . writeln ( "************* Module %s" % msg . module ) self . modules . add ( msg . module ) else : self . writeln ( "************* " ) self . write message ( msg )
def open graph ( self , * * args ) : self . stream . write ( "%sgraph:{\n" % self . indent ) self . inc indent ( ) self . write attributes ( GRAPH ATTRS , * * args )
def edge ( self , from node , to node , edge type = "" , * * args ) : self . stream . write ( '%s%sedge: {sourcename:"%s" targetname:"%s"' % ( self . indent , edge type , from node , to node ) ) self . write attributes ( EDGE ATTRS , * * args ) self . stream . write ( "}\n" )
def write attributes ( self , attributes dict , * * args ) : for key , value in args . items ( ) : try : type = attributes dict [ key ] except Key Error : raise Exception ( % ( key , attributes dict . keys ( ) ) ) if not type : self . stream . write ( '%s%s:"%s"\n' % ( self . indent , key , value ) ) elif type == 1 : self . stream . write ( "%s%s:%s\n" % ( self . indent , key , int ( value ) ) ) elif value in type : self . stream . write ( "%s%s:%s\n" % ( self . indent , key , value ) ) else : raise Exception ( % ( value , key , type ) )
def register ( linter ) : linter . register checker ( String Format Checker ( linter ) ) linter . register checker ( String Constant Checker ( linter ) )
def check new format ( self , node , func ) : # TODO: skip (for now) format nodes which don't have #       an explicit string on the left side of the format operation. #       We do this because our inference engine can't properly handle #       redefinitions of the original string. #       For more details, see issue 287. # # Note that there may not be any left side at all, if the format method # has been assigned to another variable. See issue 351. For example: # #    fmt = 'some string {}'.format #    fmt('arg') if isinstance ( node . func , astroid . Attribute ) and not isinstance ( node . func . expr , astroid . Const ) : return if node . starargs or node . kwargs : return try : strnode = next ( func . bound . infer ( ) ) except astroid . Inference Error : return if not ( isinstance ( strnode , astroid . Const ) and isinstance ( strnode . value , str ) ) : return try : call site = Call Site . from call ( node ) except astroid . Inference Error : return try : fields , num args , manual pos = utils . parse format method string ( strnode . value ) except utils . Incomplete Format String : self . add message ( "bad-format-string" , node = node ) return positional arguments = call site . positional arguments named arguments = call site . keyword arguments named fields = { field [ 0 ] for field in fields if isinstance ( field [ 0 ] , str ) } if num args and manual pos : self . add message ( "format-combined-specification" , node = node ) return check args = False # Consider "{[0]} {[1]}" as num args. num args += sum ( 1 for field in named fields if field == "" ) if named fields : for field in named fields : if field and field not in named arguments : self . add message ( "missing-format-argument-key" , node = node , args = ( field , ) ) for field in named arguments : if field not in named fields : self . add message ( "unused-format-string-argument" , node = node , args = ( field , ) ) # num args can be 0 if manual pos is not. num args = num args or manual pos if positional arguments or num args : empty = any ( True for field in named fields if field == "" ) if named arguments or empty : # Verify the required number of positional arguments # only if the .format got at least one keyword argument. # This means that the format strings accepts both # positional and named fields and we should warn # when one of the them is missing or is extra. check args = True else : check args = True if check args : # num args can be 0 if manual pos is not. num args = num args or manual pos if len ( positional arguments ) > num args : self . add message ( "too-many-format-args" , node = node ) elif len ( positional arguments ) < num args : self . add message ( "too-few-format-args" , node = node ) self . detect vacuous formatting ( node , positional arguments ) self . check new format specifiers ( node , fields , named arguments )
def visit section ( self , layout ) : self . section += 1 self . writeln ( ) self . format children ( layout ) self . section -= 1 self . writeln ( )
def visit evaluationsection ( self , layout ) : self . section += 1 self . format children ( layout ) self . section -= 1 self . writeln ( )
def visit table ( self , layout ) : table content = self . get table content ( layout ) # get columns width cols width = [ 0 ] * len ( table content [ 0 ] ) for row in table content : for index , col in enumerate ( row ) : cols width [ index ] = max ( cols width [ index ] , len ( col ) ) self . default table ( layout , table content , cols width ) self . writeln ( )
def check symbol ( self , msgid , symbol ) : other message = self . messages definitions . get ( symbol ) if other message : self . raise duplicate msg id ( symbol , msgid , other message . msgid ) else : alternative msgid = None alternative message = self . alternative names . get ( symbol ) if alternative message : if alternative message . symbol == symbol : alternative msgid = alternative message . msgid else : for old msgid , old symbol in alternative message . old names : if old symbol == symbol : alternative msgid = old msgid break if msgid != alternative msgid : self . raise duplicate msg id ( symbol , msgid , alternative msgid )
def help message ( self , msgids ) : for msgid in msgids : try : for message definition in self . get message definitions ( msgid ) : print ( message definition . format help ( checkerref = True ) ) print ( "" ) except Unknown Message Error as ex : print ( ex ) print ( "" ) continue
def list messages ( self ) : messages = sorted ( self . messages definitions . values ( ) , key = lambda m : m . msgid ) for message in messages : if not message . may be emitted ( ) : continue print ( message . format help ( checkerref = False ) ) print ( "" )
def builder inited ( app ) : # PACKAGE/docs/exts/pylint extensions.py --> PACKAGE/ base path = os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( file ) ) ) ) # PACKAGE/ --> PACKAGE/pylint/extensions ext path = os . path . join ( base path , "pylint" , "extensions" ) modules = [ ] doc files = { } for filename in os . listdir ( ext path ) : name , ext = os . path . splitext ( filename ) if name [ 0 ] == " " or name in DEPRECATED MODULES : continue if ext == ".py" : modules . append ( "pylint.extensions.%s" % name ) elif ext == ".rst" : doc files [ "pylint.extensions." + name ] = os . path . join ( ext path , filename ) modules . sort ( ) if not modules : sys . exit ( "No Pylint extensions found?" ) linter = Py Linter ( ) linter . load plugin modules ( modules ) extensions doc = os . path . join ( base path , "doc" , "technical reference" , "extensions.rst" ) with open ( extensions doc , "w" ) as stream : stream . write ( "Optional Pylint checkers in the extensions module\n" ) stream . write ( "=================================================\n\n" ) stream . write ( "Pylint provides the following optional plugins:\n\n" ) for module in modules : stream . write ( "- :ref:`{}`\n" . format ( module ) ) stream . write ( "\n" ) stream . write ( "You can activate any or all of these extensions " "by adding a ``load-plugins`` line to the ``MASTER`` " "section of your ``.pylintrc``, for example::\n" ) stream . write ( "\n    load-plugins=pylint.extensions.docparams," "pylint.extensions.docstyle\n\n" ) by module = get plugins info ( linter , doc files ) for module , info in sorted ( by module . items ( ) ) : linter . print checker doc ( info [ "name" ] , info , stream = stream )
def cpu count ( ) -> int : sched getaffinity = getattr ( os , "sched getaffinity" , None ) # pylint: disable=not-callable,using-constant-test if sched getaffinity : return len ( sched getaffinity ( 0 ) ) if multiprocessing : return multiprocessing . cpu count ( ) return 1
def report messages stats ( sect , stats , ) : if not stats [ "by msg" ] : # don't print this report when we didn't detected any errors raise exceptions . Empty Report Error ( ) in order = sorted ( [ ( value , msg id ) for msg id , value in stats [ "by msg" ] . items ( ) if not msg id . startswith ( "I" ) ] ) in order . reverse ( ) lines = ( "message id" , "occurrences" ) for value , msg id in in order : lines += ( msg id , str ( value ) ) sect . append ( report nodes . Table ( children = lines , cols = 2 , rheaders = 1 ) )
def python3 porting mode ( self ) : self . disable ( "all" ) self . enable ( "python3" ) if self . error mode : # The error mode was activated, using the -E flag. # So we'll need to enable only the errors from the # Python 3 porting checker. for msg id in self . checker messages ( "python3" ) : if msg id . startswith ( "E" ) : self . enable ( msg id ) else : self . disable ( msg id ) config parser = self . cfgfile parser if config parser . has option ( "MESSAGES CONTROL" , "disable" ) : value = config parser . get ( "MESSAGES CONTROL" , "disable" ) self . global set option ( "disable" , value ) self . python3 porting mode = True
def get checkers ( self ) : return [ self ] + [ c for checkers in self . checkers . values ( ) for c in checkers if c is not self ]
def get checker names ( self ) : current checkers = self . get checkers ( ) return sorted ( { check . name for check in current checkers if check . name != "master" } )
def prepare checkers ( self ) : if not self . config . reports : self . disable reporters ( ) # get needed checkers neededcheckers = [ self ] for checker in self . get checkers ( ) [ 1 : ] : messages = { msg for msg in checker . msgs if self . is message enabled ( msg ) } if messages or any ( self . report is enabled ( r [ 0 ] ) for r in checker . reports ) : neededcheckers . append ( checker ) # Sort checkers by priority neededcheckers = sorted ( neededcheckers , key = operator . attrgetter ( "priority" ) , reverse = True ) return neededcheckers
def expand files ( self , modules ) : result , errors = utils . expand modules ( modules , self . config . black list , self . config . black list re ) for error in errors : message = modname = error [ "mod" ] key = error [ "key" ] self . set current module ( modname ) if key == "fatal" : message = str ( error [ "ex" ] ) . replace ( os . getcwd ( ) + os . sep , "" ) self . add message ( key , args = message ) return result
def check astroid module ( self , ast node , walker , rawcheckers , tokencheckers ) : try : tokens = utils . tokenize module ( ast node ) except tokenize . Token Error as ex : self . add message ( "syntax-error" , line = ex . args [ 1 ] [ 0 ] , args = ex . args [ 0 ] ) return None if not ast node . pure python : self . add message ( "raw-checker-failed" , args = ast node . name ) else : # assert astroid.file.endswith('.py') # invoke I Token Checker interface on self to fetch module/block # level options self . process tokens ( tokens ) if self . ignore file : return False # walk ast to collect line numbers self . file state . collect block lines ( self . msgs store , ast node ) # run raw and tokens checkers for checker in rawcheckers : checker . process module ( ast node ) for checker in tokencheckers : checker . process tokens ( tokens ) # generate events to astroid checkers walker . walk ( ast node ) return True
def report evaluation ( self ) : # check with at least check 1 statements (usually 0 when there is a # syntax error preventing pylint from further processing) previous stats = config . load results ( self . file state . base name ) if self . stats [ "statement" ] == 0 : return # get a global note for the code evaluation = self . config . evaluation try : note = eval ( evaluation , { } , self . stats ) # pylint: disable=eval-used except Exception as ex : msg = "An exception occurred while rating: %s" % ex else : self . stats [ "global note" ] = note msg = "Your code has been rated at %.2f/10" % note pnote = previous stats . get ( "global note" ) if pnote is not None : msg += " (previous run: %.2f/10, %+.2f)" % ( pnote , note - pnote ) if self . config . score : sect = report nodes . Evaluation Section ( msg ) self . reporter . display reports ( sect )
def cb generate config ( self , * args , * * kwargs ) : self . linter . generate config ( skipsections = ( "COMMANDS" , ) ) sys . exit ( 0 )
def cb generate manpage ( self , * args , * * kwargs ) : from pylint import pkginfo self . linter . generate manpage ( pkginfo ) sys . exit ( 0 )
def cb help message ( self , option , optname , value , parser ) : self . linter . msgs store . help message ( utils . splitstrip ( value ) ) sys . exit ( 0 )
def cb full documentation ( self , option , optname , value , parser ) : self . linter . print full documentation ( ) sys . exit ( 0 )
def cb list messages ( self , option , optname , value , parser ) : # FIXME self . linter . msgs store . list messages ( ) sys . exit ( 0 )
def normalize text ( text , line len = 80 , indent = "" ) : return "\n" . join ( textwrap . wrap ( text , width = line len , initial indent = indent , subsequent indent = indent ) )
def get module and frameid ( node ) : frame = node . frame ( ) module , obj = "" , [ ] while frame : if isinstance ( frame , Module ) : module = frame . name else : obj . append ( getattr ( frame , "name" , "<lambda>" ) ) try : frame = frame . parent . frame ( ) except Attribute Error : frame = None obj . reverse ( ) return module , "." . join ( obj )
def safe decode ( line , encoding , * args , * * kwargs ) : try : return line . decode ( encoding or sys . getdefaultencoding ( ) , * args , * * kwargs ) except Lookup Error : return line . decode ( sys . getdefaultencoding ( ) , * args , * * kwargs )
def comment ( string ) : lines = [ line . strip ( ) for line in string . splitlines ( ) ] return "# " + ( "%s# " % linesep ) . join ( lines )
def format option value ( optdict , value ) : if isinstance ( value , ( list , tuple ) ) : value = "," . join ( format option value ( optdict , item ) for item in value ) elif isinstance ( value , dict ) : value = "," . join ( "%s:%s" % ( k , v ) for k , v in value . items ( ) ) elif hasattr ( value , "match" ) : # optdict.get('type') == 'regexp' # compiled regexp value = value . pattern elif optdict . get ( "type" ) == "yn" : value = "yes" if value else "no" elif isinstance ( value , str ) and value . isspace ( ) : value = "'%s'" % value return value
def format section ( stream , section , options , doc = None ) : if doc : print ( comment ( doc ) , file = stream ) print ( "[%s]" % section , file = stream ) ini format ( stream , options )
def ini format ( stream , options ) : for optname , optdict , value in options : value = format option value ( optdict , value ) help opt = optdict . get ( "help" ) if help opt : help opt = normalize text ( help opt , line len = 79 , indent = "# " ) print ( file = stream ) print ( help opt , file = stream ) else : print ( file = stream ) if value is None : print ( "#%s=" % optname , file = stream ) else : value = str ( value ) . strip ( ) if re . match ( r"^([\w-]+,)+[\w-]+$" , str ( value ) ) : separator = "\n " + " " * len ( optname ) value = separator . join ( x + "," for x in str ( value ) . split ( "," ) ) # remove trailing ',' from last element of the list value = value [ : - 1 ] print ( "%s=%s" % ( optname , value ) , file = stream )
def insert ( self , index , child ) : self . children . insert ( index , child ) child . parent = self
def append ( self , child ) : assert child not in self . parents ( ) V Node . append ( self , child )
def parents ( self ) : assert self . parent is not self if self . parent is None : return [ ] return [ self . parent ] + self . parent . parents ( )
def collect block lines ( self , msgs store , module node ) : for msg , lines in self . module msgs state . items ( ) : self . raw module msgs state [ msg ] = lines . copy ( ) orig state = self . module msgs state . copy ( ) self . module msgs state = { } self . suppression mapping = { } self . effective max line number = module node . tolineno self . collect block lines ( msgs store , module node , orig state )
def enable report ( self , reportid ) : reportid = reportid . upper ( ) self . reports state [ reportid ] = True
def disable report ( self , reportid ) : reportid = reportid . upper ( ) self . reports state [ reportid ] = False
def register ( linter ) : linter . register checker ( Encoding Checker ( linter ) ) linter . register checker ( By Id Managed Messages Checker ( linter ) )
def process module ( self , module ) : managed msgs = Messages Handler Mix In . get by id managed msgs ( ) for ( mod name , msg id , msg symbol , lineno , is disabled ) in managed msgs : if mod name == module . name : if is disabled : txt = "Id '{ident}' is used to disable '{symbol}' message emission" . format ( ident = msg id , symbol = msg symbol ) else : txt = "Id '{ident}' is used to enable '{symbol}' message emission" . format ( ident = msg id , symbol = msg symbol ) self . add message ( "use-symbolic-message-instead" , line = lineno , args = txt ) Messages Handler Mix In . clear by id managed msgs ( )
def process module ( self , module ) : if module . file encoding : encoding = module . file encoding else : encoding = "ascii" with module . stream ( ) as stream : for lineno , line in enumerate ( stream ) : self . check encoding ( lineno + 1 , line , encoding )
def process tokens ( self , tokens ) : if not self . config . notes : return comments = ( token info for token info in tokens if token info . type == tokenize . COMMENT ) for comment in comments : comment text = comment . string [ 1 : ] . lstrip ( ) # trim '#' and whitespaces # handle pylint disable clauses disable option match = OPTION RGX . search ( comment text ) if disable option match : try : , value = disable option match . group ( 1 ) . split ( "=" , 1 ) values = [ val . strip ( ) . upper ( ) for val in value . split ( "," ) ] if set ( values ) & set ( self . config . notes ) : continue except Value Error : self . add message ( "bad-inline-option" , args = disable option match . group ( 1 ) . strip ( ) , line = comment . string , ) continue # emit warnings if necessary match = self . fixme pattern . search ( "#" + comment text . lower ( ) ) if match : note = match . group ( 1 ) self . add message ( "fixme" , col offset = comment . string . lower ( ) . index ( note . lower ( ) ) , args = comment text , line = comment . start [ 0 ] , )
def is from future import ( stmt , name ) : try : module = stmt . do import module ( stmt . modname ) except astroid . Astroid Building Exception : return None for local node in module . locals . get ( name , [ ] ) : if isinstance ( local node , astroid . Import From ) and local node . modname == FUTURE : return True return None
def in for else branch ( parent , stmt ) : return isinstance ( parent , astroid . For ) and any ( else stmt . parent of ( stmt ) or else stmt == stmt for else stmt in parent . orelse )
def overridden method ( klass , name ) : try : parent = next ( klass . local attr ancestors ( name ) ) except ( Stop Iteration , Key Error ) : return None try : meth node = parent [ name ] except Key Error : # We have found an ancestor defining <name> but it's not in the local # dictionary. This may happen with astroid built from living objects. return None if isinstance ( meth node , astroid . Function Def ) : return meth node return None
def assigned locally ( name node ) : assign stmts = name node . scope ( ) . nodes of class ( astroid . Assign Name ) return any ( a . name == name node . name for a in assign stmts )
def visit global ( self , node ) : frame = node . frame ( ) if isinstance ( frame , astroid . Module ) : self . add message ( "global-at-module-level" , node = node ) return module = frame . root ( ) default message = True locals = node . scope ( ) . locals for name in node . names : try : assign nodes = module . getattr ( name ) except astroid . Not Found Error : # unassigned global, skip assign nodes = [ ] not defined locally by import = not any ( isinstance ( local , astroid . node classes . Import ) for local in locals . get ( name , ( ) ) ) if not assign nodes and not defined locally by import : self . add message ( "global-variable-not-assigned" , args = name , node = node ) default message = False continue for anode in assign nodes : if ( isinstance ( anode , astroid . Assign Name ) and anode . name in module . special attributes ) : self . add message ( "redefined-builtin" , args = name , node = node ) break if anode . frame ( ) is module : # module level assignment break else : if not defined locally by import : # global undefined at the module scope self . add message ( "global-variable-undefined" , args = name , node = node ) default message = False if default message : self . add message ( "global-statement" , node = node )
def visit import ( self , node ) : if not self . analyse fallback blocks and utils . is from fallback block ( node ) : # No need to verify this, since Import Error is already # handled by the client code. return for name , in node . names : parts = name . split ( "." ) try : module = next ( infer name module ( node , parts [ 0 ] ) ) except astroid . Resolve Error : continue self . check module attrs ( node , module , parts [ 1 : ] )
def visit importfrom ( self , node ) : if not self . analyse fallback blocks and utils . is from fallback block ( node ) : # No need to verify this, since Import Error is already # handled by the client code. return name parts = node . modname . split ( "." ) try : module = node . do import module ( name parts [ 0 ] ) except astroid . Astroid Building Exception : return module = self . check module attrs ( node , module , name parts [ 1 : ] ) if not module : return for name , in node . names : if name == "*" : continue self . check module attrs ( node , module , name . split ( "." ) )
def check metaclasses ( self , node ) : consumed = [ ] # [(scope locals, consumed key)] for child node in node . get children ( ) : if isinstance ( child node , astroid . Class Def ) : consumed . extend ( self . check classdef metaclasses ( child node , node ) ) # Pop the consumed items, in order to avoid having # unused-import and unused-variable false positives for scope locals , name in consumed : scope locals . pop ( name , None )
def get packages ( directory , prefix ) : result = [ ] for package in os . listdir ( directory ) : absfile = join ( directory , package ) if isdir ( absfile ) : if exists ( join ( absfile , " init .py" ) ) : if prefix : result . append ( "%s.%s" % ( prefix , package ) ) else : result . append ( package ) result += get packages ( absfile , result [ - 1 ] ) return result
def run ( self ) : install lib . install lib . run ( self ) # manually install included directories if any if include dirs : for directory in include dirs : dest = join ( self . install dir , directory ) if sys . version info >= ( 3 , 0 ) : exclude = { "invalid encoded data*" , "unknown encoding*" } else : exclude = set ( ) shutil . rmtree ( dest , ignore errors = True ) shutil . copytree ( directory , dest , ignore = shutil . ignore patterns ( * exclude ) )
def report similarities ( sect , stats , old stats ) : lines = [ "" , "now" , "previous" , "difference" ] lines += table lines from stats ( stats , old stats , ( "nb duplicated lines" , "percent duplicated lines" ) ) sect . append ( Table ( children = lines , cols = 4 , rheaders = 1 , cheaders = 1 ) )
def Run ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] from getopt import getopt s opts = "hdi" l opts = ( "help" , "duplicates=" , "ignore-comments" , "ignore-imports" , "ignore-docstrings" , ) min lines = 4 ignore comments = False ignore docstrings = False ignore imports = False opts , args = getopt ( argv , s opts , l opts ) for opt , val in opts : if opt in ( "-d" , "--duplicates" ) : min lines = int ( val ) elif opt in ( "-h" , "--help" ) : usage ( ) elif opt in ( "-i" , "--ignore-comments" ) : ignore comments = True elif opt in ( "--ignore-docstrings" , ) : ignore docstrings = True elif opt in ( "--ignore-imports" , ) : ignore imports = True if not args : usage ( 1 ) sim = Similar ( min lines , ignore comments , ignore docstrings , ignore imports ) for filename in args : with open ( filename ) as stream : sim . append stream ( filename , stream ) sim . run ( ) sys . exit ( 0 )
def append stream ( self , streamid , stream , encoding = None ) : if encoding is None : readlines = stream . readlines else : readlines = decoding stream ( stream , encoding ) . readlines try : self . linesets . append ( Line Set ( streamid , readlines ( ) , self . ignore comments , self . ignore docstrings , self . ignore imports , ) ) except Unicode Decode Error : pass
def compute sims ( self ) : no duplicates = defaultdict ( list ) for num , lineset1 , idx1 , lineset2 , idx2 in self . iter sims ( ) : duplicate = no duplicates [ num ] for couples in duplicate : if ( lineset1 , idx1 ) in couples or ( lineset2 , idx2 ) in couples : couples . add ( ( lineset1 , idx1 ) ) couples . add ( ( lineset2 , idx2 ) ) break else : duplicate . append ( { ( lineset1 , idx1 ) , ( lineset2 , idx2 ) } ) sims = [ ] for num , ensembles in no duplicates . items ( ) : for couples in ensembles : sims . append ( ( num , couples ) ) sims . sort ( ) sims . reverse ( ) return sims
def display sims ( self , sims ) : nb lignes dupliquees = 0 for num , couples in sims : print ( ) print ( num , "similar lines in" , len ( couples ) , "files" ) couples = sorted ( couples ) for lineset , idx in couples : print ( "==%s:%s" % ( lineset . name , idx ) ) # pylint: disable=W0631 for line in lineset . real lines [ idx : idx + num ] : print ( "  " , line . rstrip ( ) ) nb lignes dupliquees += num * ( len ( couples ) - 1 ) nb total lignes = sum ( [ len ( lineset ) for lineset in self . linesets ] ) print ( "TOTAL lines=%s duplicates=%s percent=%.2f" % ( nb total lignes , nb lignes dupliquees , nb lignes dupliquees * 100.0 / nb total lignes , ) )
def find common ( self , lineset1 , lineset2 ) : lines1 = lineset1 . enumerate stripped lines2 = lineset2 . enumerate stripped find = lineset2 . find index1 = 0 min lines = self . min lines while index1 < len ( lineset1 ) : skip = 1 num = 0 for index2 in find ( lineset1 [ index1 ] ) : non blank = 0 for num , ( ( , line1 ) , ( , line2 ) ) in enumerate ( zip ( lines1 ( index1 ) , lines2 ( index2 ) ) ) : if line1 != line2 : if non blank > min lines : yield num , lineset1 , index1 , lineset2 , index2 skip = max ( skip , num ) break if line1 : non blank += 1 else : # we may have reach the end num += 1 if non blank > min lines : yield num , lineset1 , index1 , lineset2 , index2 skip = max ( skip , num ) index1 += skip
def mk index ( self ) : index = defaultdict ( list ) for line no , line in enumerate ( self . stripped lines ) : if line : index [ line ] . append ( line no ) return index
def definition equivalent to call ( definition , call ) : if definition . kwargs : same kw variadics = definition . kwargs in call . starred kws else : same kw variadics = not call . starred kws if definition . varargs : same args variadics = definition . varargs in call . starred args else : same args variadics = not call . starred args same kwonlyargs = all ( kw in call . kws for kw in definition . kwonlyargs ) same args = definition . args == call . args no additional kwarg arguments = True if call . kws : for keyword in call . kws : is arg = keyword in call . args is kwonly = keyword in definition . kwonlyargs if not is arg and not is kwonly : # Maybe this argument goes into **kwargs, # or it is an extraneous argument. # In any case, the signature is different than # the call site, which stops our search. no additional kwarg arguments = False break return all ( ( same args , same kwonlyargs , same args variadics , same kw variadics , no additional kwarg arguments , ) )
def register ( linter ) : linter . register checker ( Class Checker ( linter ) ) linter . register checker ( Special Methods Checker ( linter ) )
def set accessed ( self , node ) : frame = node frame class ( node ) if frame is None : # The node does not live in a class. return self . scopes [ frame ] [ node . attrname ] . append ( node )
def visit classdef ( self , node ) : self . check bases classes ( node ) # if not an exception or a metaclass if node . type == "class" and has known bases ( node ) : try : node . local attr ( " init " ) except astroid . Not Found Error : self . add message ( "no-init" , args = node , node = node ) self . check slots ( node ) self . check proper bases ( node ) self . check consistent mro ( node )
def check consistent mro ( self , node ) : try : node . mro ( ) except Inconsistent Mro Error : self . add message ( "inconsistent-mro" , args = node . name , node = node ) except Duplicate Bases Error : self . add message ( "duplicate-bases" , args = node . name , node = node ) except Not Implemented Error : # Old style class, there's no mro so don't do anything. pass
def visit functiondef ( self , node ) : # ignore actual functions if not node . is method ( ) : return self . check useless super delegation ( node ) klass = node . parent . frame ( ) self . meth could be func = True # check first argument is self if this is actually a method self . check first arg for type ( node , klass . type == "metaclass" ) if node . name == " init " : self . check init ( node ) return # check signature if the method overloads inherited method for overridden in klass . local attr ancestors ( node . name ) : # get astroid for the searched method try : meth node = overridden [ node . name ] except Key Error : # we have found the method but it's not in the local # dictionary. # This may happen with astroid build from living objects continue if not isinstance ( meth node , astroid . Function Def ) : continue self . check signature ( node , meth node , "overridden" , klass ) break if node . decorators : for decorator in node . decorators . nodes : if isinstance ( decorator , astroid . Attribute ) and decorator . attrname in ( "getter" , "setter" , "deleter" , ) : # attribute affectation will call this method, not hiding it return if isinstance ( decorator , astroid . Name ) : if decorator . name == "property" : # attribute affectation will either call a setter or raise # an attribute error, anyway not hiding the function return # Infer the decorator and see if it returns something useful inferred = safe infer ( decorator ) if not inferred : return if isinstance ( inferred , astroid . Function Def ) : # Okay, it's a decorator, let's see what it can infer. try : inferred = next ( inferred . infer call result ( inferred ) ) except astroid . Inference Error : return try : if ( isinstance ( inferred , ( astroid . Instance , astroid . Class Def ) ) and inferred . getattr ( " get " ) and inferred . getattr ( " set " ) ) : return except astroid . Attribute Inference Error : pass # check if the method is hidden by an attribute try : overridden = klass . instance attr ( node . name ) [ 0 ] # XXX overridden frame = overridden . frame ( ) if ( isinstance ( overridden frame , astroid . Function Def ) and overridden frame . type == "method" ) : overridden frame = overridden frame . parent . frame ( ) if isinstance ( overridden frame , astroid . Class Def ) and klass . is subtype of ( overridden frame . qname ( ) ) : args = ( overridden . root ( ) . name , overridden . fromlineno ) self . add message ( "method-hidden" , args = args , node = node ) except astroid . Not Found Error : pass
def check accessed members ( self , node , accessed ) : # XXX refactor, probably much simpler now that E0201 is in type checker excs = ( "Attribute Error" , "Exception" , "Base Exception" ) for attr , nodes in accessed . items ( ) : try : # is it a class attribute ? node . local attr ( attr ) # yes, stop here continue except astroid . Not Found Error : pass # is it an instance attribute of a parent class ? try : next ( node . instance attr ancestors ( attr ) ) # yes, stop here continue except Stop Iteration : pass # is it an instance attribute ? try : defstmts = node . instance attr ( attr ) except astroid . Not Found Error : pass else : # filter out augment assignment nodes defstmts = [ stmt for stmt in defstmts if stmt not in nodes ] if not defstmts : # only augment assignment for this node, no-member should be # triggered by the typecheck checker continue # filter defstmts to only pick the first one when there are # several assignments in the same scope scope = defstmts [ 0 ] . scope ( ) defstmts = [ stmt for i , stmt in enumerate ( defstmts ) if i == 0 or stmt . scope ( ) is not scope ] # if there are still more than one, don't attempt to be smarter # than we can be if len ( defstmts ) == 1 : defstmt = defstmts [ 0 ] # check that if the node is accessed in the same method as # it's defined, it's accessed after the initial assignment frame = defstmt . frame ( ) lno = defstmt . fromlineno for node in nodes : if ( node . frame ( ) is frame and node . fromlineno < lno and not astroid . are exclusive ( node . statement ( ) , defstmt , excs ) ) : self . add message ( "access-member-before-definition" , node = node , args = ( attr , lno ) , )
def check signature ( self , method1 , refmethod , class type , cls ) : if not ( isinstance ( method1 , astroid . Function Def ) and isinstance ( refmethod , astroid . Function Def ) ) : self . add message ( "method-check-failed" , args = ( method1 , refmethod ) , node = method1 ) return instance = cls . instantiate class ( ) method1 = function to method ( method1 , instance ) refmethod = function to method ( refmethod , instance ) # Don't care about functions with unknown argument (builtins). if method1 . args . args is None or refmethod . args . args is None : return # Ignore private to class methods. if is attr private ( method1 . name ) : return # Ignore setters, they have an implicit extra argument, # which shouldn't be taken in consideration. if method1 . decorators : for decorator in method1 . decorators . nodes : if ( isinstance ( decorator , astroid . Attribute ) and decorator . attrname == "setter" ) : return if different parameters ( refmethod , method1 , dummy parameter regex = self . dummy rgx ) : self . add message ( "arguments-differ" , args = ( class type , method1 . name ) , node = method1 ) elif len ( method1 . args . defaults ) < len ( refmethod . args . defaults ) : self . add message ( "signature-differs" , args = ( class type , method1 . name ) , node = method1 )
def is raising ( body : typing . List ) -> bool : for node in body : if isinstance ( node , astroid . Raise ) : return True return False
def visit tryexcept ( self , node ) : self . check try except raise ( node ) exceptions classes = [ ] nb handlers = len ( node . handlers ) for index , handler in enumerate ( node . handlers ) : if handler . type is None : if not is raising ( handler . body ) : self . add message ( "bare-except" , node = handler ) # check if an "except:" is followed by some other # except if index < ( nb handlers - 1 ) : msg = "empty except clause should always appear last" self . add message ( "bad-except-order" , node = node , args = msg ) elif isinstance ( handler . type , astroid . Bool Op ) : self . add message ( "binary-op-exception" , node = handler , args = handler . type . op ) else : try : excs = list ( annotated unpack infer ( handler . type ) ) except astroid . Inference Error : continue for part , exc in excs : if exc is astroid . Uninferable : continue if isinstance ( exc , astroid . Instance ) and utils . inherit from std ex ( exc ) : # pylint: disable=protected-access exc = exc . proxied self . check catching non exception ( handler , exc , part ) if not isinstance ( exc , astroid . Class Def ) : continue exc ancestors = [ anc for anc in exc . ancestors ( ) if isinstance ( anc , astroid . Class Def ) ] for previous exc in exceptions classes : if previous exc in exc ancestors : msg = "%s is an ancestor class of %s" % ( previous exc . name , exc . name , ) self . add message ( "bad-except-order" , node = handler . type , args = msg ) if ( exc . name in self . config . overgeneral exceptions and exc . root ( ) . name == utils . EXCEPTIONS MODULE and not is raising ( handler . body ) ) : self . add message ( "broad-except" , args = exc . name , node = handler . type ) if exc in exceptions classes : self . add message ( "duplicate-except" , args = exc . name , node = handler . type ) exceptions classes += [ exc for , exc in excs ]
def visit functiondef ( self , node ) : # ignore actual functions or method within a new style class if not node . is method ( ) : return klass = node . parent . frame ( ) for stmt in node . nodes of class ( astroid . Call ) : if node frame class ( stmt ) != node frame class ( node ) : # Don't look down in other scopes. continue expr = stmt . func if not isinstance ( expr , astroid . Attribute ) : continue call = expr . expr # skip the test if using super if not ( isinstance ( call , astroid . Call ) and isinstance ( call . func , astroid . Name ) and call . func . name == "super" ) : continue if not klass . newstyle and has known bases ( klass ) : # super should not be used on an old style class continue else : # super first arg should be the class if not call . args : if sys . version info [ 0 ] == 3 : # unless Python 3 continue else : self . add message ( "missing-super-argument" , node = call ) continue # calling super(type(self), self) can lead to recursion loop # in derived classes arg0 = call . args [ 0 ] if ( isinstance ( arg0 , astroid . Call ) and isinstance ( arg0 . func , astroid . Name ) and arg0 . func . name == "type" ) : self . add message ( "bad-super-call" , node = call , args = ( "type" , ) ) continue # calling super(self. class , self) can lead to recursion loop # in derived classes if ( len ( call . args ) >= 2 and isinstance ( call . args [ 1 ] , astroid . Name ) and call . args [ 1 ] . name == "self" and isinstance ( arg0 , astroid . Attribute ) and arg0 . attrname == " class " ) : self . add message ( "bad-super-call" , node = call , args = ( "self. class " , ) ) continue try : supcls = call . args and next ( call . args [ 0 ] . infer ( ) , None ) except astroid . Inference Error : continue if klass is not supcls : name = None # if supcls is not Uninferable, then supcls was infered # and use its name. Otherwise, try to look # for call.args[0].name if supcls : name = supcls . name elif call . args and hasattr ( call . args [ 0 ] , "name" ) : name = call . args [ 0 ] . name if name : self . add message ( "bad-super-call" , node = call , args = ( name , ) )
def display reports ( self , layout ) : self . section = 0 if hasattr ( layout , "report id" ) : layout . children [ 0 ] . children [ 0 ] . data += " (%s)" % layout . report id self . display ( layout )
def is typing namedtuple ( node : astroid . Class Def ) -> bool : for base in node . ancestors ( ) : if base . qname ( ) == TYPING NAMEDTUPLE : return True return False
def visit classdef ( self , node ) : nb parents = len ( list ( node . ancestors ( ) ) ) if nb parents > self . config . max parents : self . add message ( "too-many-ancestors" , node = node , args = ( nb parents , self . config . max parents ) , ) if len ( node . instance attrs ) > self . config . max attributes : self . add message ( "too-many-instance-attributes" , node = node , args = ( len ( node . instance attrs ) , self . config . max attributes ) , )
def leave classdef ( self , node ) : my methods = sum ( 1 for method in node . mymethods ( ) if not method . name . startswith ( " " ) ) # Does the class contain less than n public methods ? # This checks only the methods defined in the current class, # since the user might not have control over the classes # from the ancestors. It avoids some false positives # for classes such as unittest.Test Case, which provides # a lot of assert methods. It doesn't make sense to warn # when the user subclasses Test Case to add his own tests. if my methods > self . config . max public methods : self . add message ( "too-many-public-methods" , node = node , args = ( my methods , self . config . max public methods ) , ) # Stop here for exception, metaclass, interface classes and other # classes for which we don't need to count the methods. if ( node . type != "class" or is enum class ( node ) or is dataclass ( node ) or is typing namedtuple ( node ) ) : return # Does the class contain more than n public methods ? # This checks all the methods defined by ancestors and # by the current class. all methods = count methods in class ( node ) if all methods < self . config . min public methods : self . add message ( "too-few-public-methods" , node = node , args = ( all methods , self . config . min public methods ) , )
def visit tryexcept ( self , node ) : branches = len ( node . handlers ) if node . orelse : branches += 1 self . inc branch ( node , branches ) self . inc all stmts ( branches )
def visit if ( self , node ) : self . check boolean expressions ( node ) branches = 1 # don't double count If nodes coming from some 'elif' if node . orelse and ( len ( node . orelse ) > 1 or not isinstance ( node . orelse [ 0 ] , If ) ) : branches += 1 self . inc branch ( node , branches ) self . inc all stmts ( branches )
def visit while ( self , node ) : branches = 1 if node . orelse : branches += 1 self . inc branch ( node , branches )
def check docstring ( self , node ) : docstring = node . doc if not docstring : return start line = node . lineno + 1 # Go through lines of docstring for idx , line in enumerate ( docstring . splitlines ( ) ) : self . check spelling ( "wrong-spelling-in-docstring" , line , start line + idx )
def register ( linter ) : linter . register checker ( Refactoring Checker ( linter ) ) linter . register checker ( Not Checker ( linter ) ) linter . register checker ( Recommandation Checker ( linter ) ) linter . register checker ( Len Checker ( linter ) )
def check stop iteration inside generator ( self , node ) : frame = node . frame ( ) if not isinstance ( frame , astroid . Function Def ) or not frame . is generator ( ) : return if utils . node ignores exception ( node , Stop Iteration ) : return if not node . exc : return exc = utils . safe infer ( node . exc ) if exc is None or exc is astroid . Uninferable : return if self . check exception inherit from stopiteration ( exc ) : self . add message ( "stop-iteration-return" , node = node )
def check exception inherit from stopiteration ( exc ) : stopiteration qname = "{}.Stop Iteration" . format ( utils . EXCEPTIONS MODULE ) return any ( class . qname ( ) == stopiteration qname for class in exc . mro ( ) )
def check nested blocks ( self , node ) : # only check block levels inside functions or methods if not isinstance ( node . scope ( ) , astroid . Function Def ) : return # messages are triggered on leaving the nested block. Here we save the # stack in case the current node isn't nested in the previous one nested blocks = self . nested blocks [ : ] if node . parent == node . scope ( ) : self . nested blocks = [ node ] else : # go through ancestors from the most nested to the less for ancestor node in reversed ( self . nested blocks ) : if ancestor node == node . parent : break self . nested blocks . pop ( ) # if the node is an elif, this should not be another nesting level if isinstance ( node , astroid . If ) and self . is actual elif ( node ) : if self . nested blocks : self . nested blocks . pop ( ) self . nested blocks . append ( node ) # send message only once per group of nested blocks if len ( nested blocks ) > len ( self . nested blocks ) : self . emit nested blocks message if needed ( nested blocks )
def check consider merging isinstance ( self , node ) : if node . op != "or" : return first args = self . duplicated isinstance types ( node ) for duplicated name , class names in first args . items ( ) : names = sorted ( name for name in class names ) self . add message ( "consider-merging-isinstance" , node = node , args = ( duplicated name , ", " . join ( names ) ) , )
def visit for ( self , node ) : # Verify that we have a `range([start], len(...), [stop])` call and # that the object which is iterated is used as a subscript in the # body of the for. # Is it a proper range call? if not isinstance ( node . iter , astroid . Call ) : return if not self . is builtin ( node . iter . func , "range" ) : return if len ( node . iter . args ) == 2 and not is constant zero ( node . iter . args [ 0 ] ) : return if len ( node . iter . args ) > 2 : return # Is it a proper len call? if not isinstance ( node . iter . args [ - 1 ] , astroid . Call ) : return second func = node . iter . args [ - 1 ] . func if not self . is builtin ( second func , "len" ) : return len args = node . iter . args [ - 1 ] . args if not len args or len ( len args ) != 1 : return iterating object = len args [ 0 ] if not isinstance ( iterating object , astroid . Name ) : return # If we're defining  iter  on self, enumerate won't work scope = node . scope ( ) if iterating object . name == "self" and scope . name == " iter " : return # Verify that the body of the for loop uses a subscript # with the object that was iterated. This uses some heuristics # in order to make sure that the same object is used in the # for body. for child in node . body : for subscript in child . nodes of class ( astroid . Subscript ) : if not isinstance ( subscript . value , astroid . Name ) : continue if not isinstance ( subscript . slice , astroid . Index ) : continue if not isinstance ( subscript . slice . value , astroid . Name ) : continue if subscript . slice . value . name != node . target . name : continue if iterating object . name != subscript . value . name : continue if subscript . value . scope ( ) != node . scope ( ) : # Ignore this subscript if it's not in the same # scope. This means that in the body of the for # loop, another scope was created, where the same # name for the iterating object was used. continue self . add message ( "consider-using-enumerate" , node = node ) return
def check graphviz available ( output format ) : try : subprocess . call ( [ "dot" , "-V" ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) except OS Error : print ( "The output format '%s' is currently not available.\n" "Please install 'Graphviz' to have other output formats " "than 'dot' or 'vcg'." % output format ) sys . exit ( 32 )
def run ( self , args ) : if not args : print ( self . help ( ) ) return 1 # insert current working directory to the python path to recognize # dependencies to local modules even if cwd is not in the PYTHONPATH sys . path . insert ( 0 , os . getcwd ( ) ) try : project = project from files ( args , project name = self . config . project , black list = self . config . black list , ) linker = Linker ( project , tag = True ) handler = Diadefs Handler ( self . config ) diadefs = handler . get diadefs ( project , linker ) finally : sys . path . pop ( 0 ) if self . config . output format == "vcg" : writer . VCG Writer ( self . config ) . write ( diadefs ) else : writer . Dot Writer ( self . config ) . write ( diadefs ) return 0
def visit tryexcept ( self , node ) : for handler in node . handlers : if handler . type is None : continue if isinstance ( handler . type , astroid . Bool Op ) : continue try : excs = list ( annotated unpack infer ( handler . type ) ) except astroid . Inference Error : continue handled in clause = [ ] for part , exc in excs : if exc is astroid . Uninferable : continue if isinstance ( exc , astroid . Instance ) and utils . inherit from std ex ( exc ) : # pylint: disable=protected-access exc = exc . proxied if not isinstance ( exc , astroid . Class Def ) : continue exc ancestors = [ anc for anc in exc . ancestors ( ) if isinstance ( anc , astroid . Class Def ) ] for prev part , prev exc in handled in clause : prev exc ancestors = [ anc for anc in prev exc . ancestors ( ) if isinstance ( anc , astroid . Class Def ) ] if exc == prev exc : self . add message ( "overlapping-except" , node = handler . type , args = "%s and %s are the same" % ( prev part . as string ( ) , part . as string ( ) ) , ) elif prev exc in exc ancestors or exc in prev exc ancestors : ancestor = part if exc in prev exc ancestors else prev part descendant = part if prev exc in exc ancestors else prev part self . add message ( "overlapping-except" , node = handler . type , args = "%s is an ancestor class of %s" % ( ancestor . as string ( ) , descendant . as string ( ) ) , ) handled in clause += [ ( part , exc ) ]
def write packages ( self , diagram ) : # sorted to get predictable (hence testable) results for i , obj in enumerate ( sorted ( diagram . modules ( ) , key = lambda x : x . title ) ) : self . printer . emit node ( i , label = self . get title ( obj ) , shape = "box" ) obj . fig id = i # package dependencies for rel in diagram . get relationships ( "depends" ) : self . printer . emit edge ( rel . from object . fig id , rel . to object . fig id , * * self . pkg edges )
def write classes ( self , diagram ) : # sorted to get predictable (hence testable) results for i , obj in enumerate ( sorted ( diagram . objects , key = lambda x : x . title ) ) : self . printer . emit node ( i , * * self . get values ( obj ) ) obj . fig id = i # inheritance links for rel in diagram . get relationships ( "specialization" ) : self . printer . emit edge ( rel . from object . fig id , rel . to object . fig id , * * self . inh edges ) # implementation links for rel in diagram . get relationships ( "implements" ) : self . printer . emit edge ( rel . from object . fig id , rel . to object . fig id , * * self . imp edges ) # generate associations for rel in diagram . get relationships ( "association" ) : self . printer . emit edge ( rel . from object . fig id , rel . to object . fig id , label = rel . name , * * self . association edges )
def set printer ( self , file name , basename ) : layout = dict ( rankdir = "BT" ) self . printer = Dot Backend ( basename , additional param = layout ) self . file name = file name
def set printer ( self , file name , basename ) : self . graph file = open ( file name , "w+" ) self . printer = VCG Printer ( self . graph file ) self . printer . open graph ( title = basename , layoutalgorithm = "dfs" , late edge labels = "yes" , port sharing = "no" , manhattan edges = "yes" , ) self . printer . emit node = self . printer . node self . printer . emit edge = self . printer . edge
def may be emitted ( self ) : if self . minversion is not None and self . minversion > sys . version info : return False if self . maxversion is not None and self . maxversion <= sys . version info : return False return True
def format help ( self , checkerref = False ) : desc = self . descr if checkerref : desc += " This message belongs to the %s checker." % self . checker . name title = self . msg if self . symbol : msgid = "%s (%s)" % ( self . symbol , self . msgid ) else : msgid = self . msgid if self . minversion or self . maxversion : restr = [ ] if self . minversion : restr . append ( "< %s" % "." . join ( [ str ( n ) for n in self . minversion ] ) ) if self . maxversion : restr . append ( ">= %s" % "." . join ( [ str ( n ) for n in self . maxversion ] ) ) restr = " or " . join ( restr ) if checkerref : desc += " It can't be emitted when using Python %s." % restr else : desc += " This message can't be emitted when using Python %s." % restr desc = normalize text ( " " . join ( desc . split ( ) ) , indent = "  " ) if title != "%s" : title = title . splitlines ( ) [ 0 ] return ":%s: *%s*\n%s" % ( msgid , title . rstrip ( " " ) , desc ) return ":%s:\n%s" % ( msgid , desc )
def get cycles ( graph dict , path , visited , result , vertice ) : if vertice in path : cycle = [ vertice ] for node in path [ : : - 1 ] : if node == vertice : break cycle . insert ( 0 , node ) # make a canonical representation start from = min ( cycle ) index = cycle . index ( start from ) cycle = cycle [ index : ] + cycle [ 0 : index ] # append it to result if not already in if cycle not in result : result . append ( cycle ) return path . append ( vertice ) try : for node in graph dict [ vertice ] : # don't check already visited nodes again if node not in visited : get cycles ( graph dict , path , visited , result , node ) visited . add ( node ) except Key Error : pass path . pop ( )
def get source ( self ) : if self . source is None : self . emit ( "}\n" ) self . source = "\n" . join ( self . lines ) del self . lines return self . source
def rest format section ( stream , section , options , doc = None ) : if section : print ( "%s\n%s" % ( section , "'" * len ( section ) ) , file = stream ) if doc : print ( normalize text ( doc , line len = 79 , indent = "" ) , file = stream ) print ( file = stream ) for optname , optdict , value in options : help opt = optdict . get ( "help" ) print ( ":%s:" % optname , file = stream ) if help opt : help opt = normalize text ( help opt , line len = 79 , indent = "  " ) print ( help opt , file = stream ) if value : value = str ( format option value ( optdict , value ) ) print ( file = stream ) print ( "  Default: ``%s``" % value . replace ( "`` " , "```` ``" ) , file = stream )
def disable ( self , msgid , scope = "package" , line = None , ignore unknown = False ) : self . set msg status ( msgid , enable = False , scope = scope , line = line , ignore unknown = ignore unknown ) self . register by id managed msg ( msgid , line )
def enable ( self , msgid , scope = "package" , line = None , ignore unknown = False ) : self . set msg status ( msgid , enable = True , scope = scope , line = line , ignore unknown = ignore unknown ) self . register by id managed msg ( msgid , line , is disabled = False )
def print full documentation ( self , stream = None ) : if not stream : stream = sys . stdout print ( "Pylint global options and switches" , file = stream ) print ( "----------------------------------" , file = stream ) print ( "" , file = stream ) print ( "Pylint provides global options and switches." , file = stream ) print ( "" , file = stream ) by checker = { } for checker in self . get checkers ( ) : if checker . name == "master" : if checker . options : for section , options in checker . options by section ( ) : if section is None : title = "General options" else : title = "%s options" % section . capitalize ( ) print ( title , file = stream ) print ( "~" * len ( title ) , file = stream ) rest format section ( stream , None , options ) print ( "" , file = stream ) else : name = checker . name try : by checker [ name ] [ "options" ] += checker . options and values ( ) by checker [ name ] [ "msgs" ] . update ( checker . msgs ) by checker [ name ] [ "reports" ] += checker . reports except Key Error : by checker [ name ] = { "options" : list ( checker . options and values ( ) ) , "msgs" : dict ( checker . msgs ) , "reports" : list ( checker . reports ) , } print ( "Pylint checkers' options and switches" , file = stream ) print ( "-------------------------------------" , file = stream ) print ( "" , file = stream ) print ( "Pylint checkers can provide three set of features:" , file = stream ) print ( "" , file = stream ) print ( "* options that control their execution," , file = stream ) print ( "* messages that they can raise," , file = stream ) print ( "* reports that they can generate." , file = stream ) print ( "" , file = stream ) print ( "Below is a list of all checkers and their features." , file = stream ) print ( "" , file = stream ) for checker , info in sorted ( by checker . items ( ) ) : self . print checker doc ( checker , info , stream = stream )
def get indent length ( line ) : result = 0 for char in line : if char == " " : result += 1 elif char == "\t" : result += TAB LENGTH else : break return result
def get indent hint line ( bar positions , bad position ) : if not bar positions : return ( "" , "" ) # TODO tabs should not be replaced by some random (8) number of spaces bar positions = [ get indent length ( indent ) for indent in bar positions ] bad position = get indent length ( bad position ) delta message = "" markers = [ ( pos , "|" ) for pos in bar positions ] if len ( markers ) == 1 : # if we have only one marker we'll provide an extra hint on how to fix expected position = markers [ 0 ] [ 0 ] delta = abs ( expected position - bad position ) direction = "add" if expected position > bad position else "remove" delta message = CONTINUATION HINT MESSAGE % ( direction , delta , "s" if delta > 1 else "" , ) markers . append ( ( bad position , "^" ) ) markers . sort ( ) line = [ " " ] * ( markers [ - 1 ] [ 0 ] + 1 ) for position , marker in markers : line [ position ] = marker return ( "" . join ( line ) , delta message )
def handle line start ( self , pos ) : if self . line start > - 1 : return check token position = pos if self . tokens . token ( pos ) == ASYNC TOKEN : check token position += 1 self . is block opener = ( self . tokens . token ( check token position ) in CONTINUATION BLOCK OPENERS ) self . line start = pos
def get valid indentations ( self , idx ) : # The closing brace on a dict or the 'for' in a dict comprehension may # reset two indent levels because the dict value is ended implicitly stack top = - 1 if ( self . tokens . token ( idx ) in ( "}" , "for" ) and self . cont stack [ - 1 ] . token == ":" ) : stack top = - 2 indent = self . cont stack [ stack top ] if self . tokens . token ( idx ) in CLOSING BRACKETS : valid indentations = indent . valid outdent strings else : valid indentations = indent . valid continuation strings return indent , valid indentations . copy ( )
def continuation inside bracket ( self , bracket , position ) : indentation = self . tokens . line indent ( position ) token indent = self . tokens . token indent ( position ) next token indent = self . tokens . token indent ( position + 1 ) if ( self . is block opener and next token indent == indentation + self . block indent string ) : return Continued Indent ( CONTINUED BLOCK , bracket , position , Indentations ( token indent ) , Before Block Indentations ( next token indent , next token indent + self . continuation string ) , ) return Continued Indent ( CONTINUED , bracket , position , Indentations ( token indent , next token indent ) , Indentations ( next token indent ) , )
def new line ( self , tokens , line end , line start ) : if last token on line is ( tokens , line end , ";" ) : self . add message ( "unnecessary-semicolon" , line = tokens . start line ( line end ) ) line num = tokens . start line ( line start ) line = tokens . line ( line start ) if tokens . type ( line start ) not in JUNK TOKENS : self . lines [ line num ] = line . split ( "\n" ) [ 0 ] self . check lines ( line , line num )
def has valid type annotation ( self , tokens , i ) : if not self . inside brackets ( "(" ) : return False # token info # type string start end line #  0      1     2    3    4 bracket level = 0 for token in tokens [ i - 1 : : - 1 ] : if token [ 1 ] == ":" : return True if token [ 1 ] == "(" : return False if token [ 1 ] == "]" : bracket level += 1 elif token [ 1 ] == "[" : bracket level -= 1 elif token [ 1 ] == "," : if not bracket level : return False elif token [ 1 ] in ( "." , "..." ) : continue elif token [ 0 ] not in ( tokenize . NAME , tokenize . STRING , tokenize . NL ) : return False return False
def check equals spacing ( self , tokens , i ) : if self . has valid type annotation ( tokens , i ) : self . check space ( tokens , i , ( MUST , MUST ) ) elif self . inside brackets ( "(" ) or self . inside brackets ( "lambda" ) : self . check space ( tokens , i , ( MUST NOT , MUST NOT ) ) else : self . check space ( tokens , i , ( MUST , MUST ) )
def check surrounded by space ( self , tokens , i ) : self . check space ( tokens , i , ( MUST , MUST ) )
def visit default ( self , node ) : if not node . is statement : return if not node . root ( ) . pure python : return # XXX block visit of child nodes prev sibl = node . previous sibling ( ) if prev sibl is not None : prev line = prev sibl . fromlineno else : # The line on which a finally: occurs in a try/finally # is not directly represented in the AST. We infer it # by taking the last line of the body and adding 1, which # should be the line of finally: if ( isinstance ( node . parent , nodes . Try Finally ) and node in node . parent . finalbody ) : prev line = node . parent . body [ 0 ] . tolineno + 1 else : prev line = node . parent . statement ( ) . fromlineno line = node . fromlineno assert line , node if prev line == line and self . visited lines . get ( line ) != 2 : self . check multi statement line ( node , line ) return if line in self . visited lines : return try : tolineno = node . blockstart tolineno except Attribute Error : tolineno = node . tolineno assert tolineno , node lines = [ ] for line in range ( line , tolineno + 1 ) : self . visited lines [ line ] = 1 try : lines . append ( self . lines [ line ] . rstrip ( ) ) except Key Error : lines . append ( "" )
def check multi statement line ( self , node , line ) : # Do not warn about multiple nested context managers # in with statements. if isinstance ( node , nodes . With ) : return # For try... except... finally..., the two nodes # appear to be on the same line due to how the AST is built. if isinstance ( node , nodes . Try Except ) and isinstance ( node . parent , nodes . Try Finally ) : return if ( isinstance ( node . parent , nodes . If ) and not node . parent . orelse and self . config . single line if stmt ) : return if ( isinstance ( node . parent , nodes . Class Def ) and len ( node . parent . body ) == 1 and self . config . single line class stmt ) : return self . add message ( "multiple-statements" , node = node ) self . visited lines [ line ] = 2
def check lines ( self , lines , i ) : max chars = self . config . max line length ignore long line = self . config . ignore long lines def check line ( line , i ) : if not line . endswith ( "\n" ) : self . add message ( "missing-final-newline" , line = i ) else : # exclude \f (formfeed) from the rstrip stripped line = line . rstrip ( "\t\n\r\v " ) if not stripped line and EMPTY LINE in self . config . no space check : # allow empty lines pass elif line [ len ( stripped line ) : ] not in ( "\n" , "\r\n" ) : self . add message ( "trailing-whitespace" , line = i , col offset = len ( stripped line ) ) # Don't count excess whitespace in the line length. line = stripped line mobj = OPTION RGX . search ( line ) if mobj and "=" in line : front of equal , , back of equal = mobj . group ( 1 ) . partition ( "=" ) if front of equal . strip ( ) == "disable" : if "line-too-long" in { msg id . strip ( ) for msg id in back of equal . split ( "," ) } : return None line = line . rsplit ( "#" , 1 ) [ 0 ] . rstrip ( ) if len ( line ) > max chars and not ignore long line . search ( line ) : self . add message ( "line-too-long" , line = i , args = ( len ( line ) , max chars ) ) return i + 1 unsplit ends = { "\v" , "\x0b" , "\f" , "\x0c" , "\x1c" , "\x1d" , "\x1e" , "\x85" , "\u2028" , "\u2029" , } unsplit = [ ] for line in lines . splitlines ( True ) : if line [ - 1 ] in unsplit ends : unsplit . append ( line ) continue if unsplit : unsplit . append ( line ) line = "" . join ( unsplit ) unsplit = [ ] i = check line ( line , i ) if i is None : break if unsplit : check line ( "" . join ( unsplit ) , i )
def check indent level ( self , string , expected , line num ) : indent = self . config . indent string if indent == "\\t" : # \t is not interpreted in the configuration file indent = "\t" level = 0 unit size = len ( indent ) while string [ : unit size ] == indent : string = string [ unit size : ] level += 1 suppl = "" while string and string [ 0 ] in " \t" : if string [ 0 ] != indent [ 0 ] : if string [ 0 ] == "\t" : args = ( "tab" , "space" ) else : args = ( "space" , "tab" ) self . add message ( "mixed-indentation" , args = args , line = line num ) return level suppl += string [ 0 ] string = string [ 1 : ] if level != expected or suppl : i type = "spaces" if indent [ 0 ] == "\t" : i type = "tabs" self . add message ( "bad-indentation" , line = line num , args = ( level * unit size + len ( suppl ) , i type , expected * unit size ) , ) return None
def is conditional import ( node ) : parent = node . parent return isinstance ( parent , ( astroid . Try Except , astroid . Except Handler , astroid . If , astroid . If Exp ) )
def visit name ( self , node ) : found node , = node . lookup ( node . name ) if not is builtin ( found node ) : return if node . name not in self . bad builtins : return if node ignores exception ( node ) or isinstance ( find try except wrapper node ( node ) , astroid . Except Handler ) : return message = node . name . lower ( ) + "-builtin" self . add message ( message , node = node )
def visit subscript ( self , node ) : try : for inferred in node . value . infer ( ) : if not isinstance ( inferred , astroid . Instance ) : continue if utils . inherit from std ex ( inferred ) : self . add message ( "indexing-exception" , node = node ) except astroid . Inference Error : return
def visit attribute ( self , node ) : if node . attrname == "xreadlines" : self . add message ( "xreadlines-attribute" , node = node ) return exception message = "message" try : for inferred in node . expr . infer ( ) : if isinstance ( inferred , astroid . Instance ) and utils . inherit from std ex ( inferred ) : if node . attrname == exception message : # Exceptions with .message clearly defined are an exception if exception message in inferred . instance attrs : continue self . add message ( "exception-message-attribute" , node = node ) if isinstance ( inferred , astroid . Module ) : self . warn if deprecated ( node , inferred . name , { node . attrname } , report on modules = False ) except astroid . Inference Error : return
def visit excepthandler ( self , node ) : def is used in except block ( node ) : scope = node . scope ( ) current = node while ( current and current != scope and not isinstance ( current , astroid . Except Handler ) ) : current = current . parent return isinstance ( current , astroid . Except Handler ) and current . type != node if isinstance ( node . name , ( astroid . Tuple , astroid . List ) ) : self . add message ( "unpacking-in-except" , node = node ) return if not node . name : return # Find any names scope = node . parent . scope ( ) scope names = scope . nodes of class ( astroid . Name , skip klass = astroid . Function Def ) scope names = list ( scope names ) potential leaked names = [ scope name for scope name in scope names if scope name . name == node . name . name and scope name . lineno > node . lineno and not is used in except block ( scope name ) ] reassignments for same name = { assign name . lineno for assign name in scope . nodes of class ( astroid . Assign Name , skip klass = astroid . Function Def ) if assign name . name == node . name . name } for leaked name in potential leaked names : if any ( node . lineno < elem < leaked name . lineno for elem in reassignments for same name ) : continue self . add message ( "exception-escape" , node = leaked name )
def find pylintrc ( ) : # is there a pylint rc file in the current directory ? if os . path . exists ( "pylintrc" ) : return os . path . abspath ( "pylintrc" ) if os . path . exists ( ".pylintrc" ) : return os . path . abspath ( ".pylintrc" ) if os . path . isfile ( " init .py" ) : curdir = os . path . abspath ( os . getcwd ( ) ) while os . path . isfile ( os . path . join ( curdir , " init .py" ) ) : curdir = os . path . abspath ( os . path . join ( curdir , ".." ) ) if os . path . isfile ( os . path . join ( curdir , "pylintrc" ) ) : return os . path . join ( curdir , "pylintrc" ) if os . path . isfile ( os . path . join ( curdir , ".pylintrc" ) ) : return os . path . join ( curdir , ".pylintrc" ) if "PYLINTRC" in os . environ and os . path . exists ( os . environ [ "PYLINTRC" ] ) : pylintrc = os . environ [ "PYLINTRC" ] else : user home = os . path . expanduser ( "~" ) if user home in ( "~" , "/root" ) : pylintrc = ".pylintrc" else : pylintrc = os . path . join ( user home , ".pylintrc" ) if not os . path . isfile ( pylintrc ) : pylintrc = os . path . join ( user home , ".config" , "pylintrc" ) if not os . path . isfile ( pylintrc ) : if os . path . isfile ( "/etc/pylintrc" ) : pylintrc = "/etc/pylintrc" else : pylintrc = None return pylintrc
def register options provider ( self , provider , own group = True ) : assert provider . priority <= 0 , "provider's priority can't be >= 0" for i in range ( len ( self . options providers ) ) : if provider . priority > self . options providers [ i ] . priority : self . options providers . insert ( i , provider ) break else : self . options providers . append ( provider ) non group spec options = [ option for option in provider . options if "group" not in option [ 1 ] ] groups = getattr ( provider , "option groups" , ( ) ) if own group and non group spec options : self . add option group ( provider . name . upper ( ) , provider . doc , non group spec options , provider , ) else : for opt , optdict in non group spec options : self . add optik option ( provider , self . cmdline parser , opt , optdict ) for gname , gdoc in groups : gname = gname . upper ( ) goptions = [ option for option in provider . options if option [ 1 ] . get ( "group" , "" ) . upper ( ) == gname ] self . add option group ( gname , gdoc , goptions , provider )
def cb set provider option ( self , option , opt , value , parser ) : if opt . startswith ( "--" ) : # remove -- on long option opt = opt [ 2 : ] else : # short option, get its long equivalent opt = self . short options [ opt [ 1 : ] ] # trick since we can't set action='store true' on options if value is None : value = 1 self . global set option ( opt , value )
def global set option ( self , opt , value ) : self . all options [ opt ] . set option ( opt , value )
def add help section ( self , title , description , level = 0 ) : group = optparse . Option Group ( self . cmdline parser , title = title . capitalize ( ) , description = description ) group . level = level self . maxlevel = max ( self . maxlevel , level ) self . cmdline parser . add option group ( group )
def help ( self , level = 0 ) : self . cmdline parser . formatter . output level = level with patch optparse ( ) : return self . cmdline parser . format help ( )
def load defaults ( self ) : for opt , optdict in self . options : action = optdict . get ( "action" ) if action != "callback" : # callback action have no default if optdict is None : optdict = self . get option def ( opt ) default = optdict . get ( "default" ) self . set option ( opt , default , action , optdict )
def option attrname ( self , opt , optdict = None ) : if optdict is None : optdict = self . get option def ( opt ) return optdict . get ( "dest" , opt . replace ( "-" , " " ) )
def get option def ( self , opt ) : assert self . options for option in self . options : if option [ 0 ] == opt : return option [ 1 ] raise optparse . Option Error ( "no such option %s in section %r" % ( opt , self . name ) , opt )
def visit module ( self , node ) : # pylint: disable=unused-argument # The code being checked can just as easily "import logging as foo", # so it is necessary to process the imports and store in this field # what name the logging module is actually given. self . logging names = set ( ) logging mods = self . config . logging modules self . format style = self . config . logging format style self . logging modules = set ( logging mods ) self . from imports = { } for logging mod in logging mods : parts = logging mod . rsplit ( "." , 1 ) if len ( parts ) > 1 : self . from imports [ parts [ 0 ] ] = parts [ 1 ]
def visit importfrom ( self , node ) : try : logging name = self . from imports [ node . modname ] for module , as name in node . names : if module == logging name : self . logging names . add ( as name or module ) except Key Error : pass
def visit import ( self , node ) : for module , as name in node . names : if module in self . logging modules : self . logging names . add ( as name or module )
def visit call ( self , node ) : def is logging name ( ) : return ( isinstance ( node . func , astroid . Attribute ) and isinstance ( node . func . expr , astroid . Name ) and node . func . expr . name in self . logging names ) def is logger class ( ) : try : for inferred in node . func . infer ( ) : if isinstance ( inferred , astroid . Bound Method ) : parent = inferred . proxied . parent if isinstance ( parent , astroid . Class Def ) and ( parent . qname ( ) == "logging.Logger" or any ( ancestor . qname ( ) == "logging.Logger" for ancestor in parent . ancestors ( ) ) ) : return True , inferred . proxied . name except astroid . exceptions . Inference Error : pass return False , None if is logging name ( ) : name = node . func . attrname else : result , name = is logger class ( ) if not result : return self . check log method ( node , name )
def in loop ( node ) : parent = node . parent while parent is not None : if isinstance ( parent , ( astroid . For , astroid . List Comp , astroid . Set Comp , astroid . Dict Comp , astroid . Generator Exp , ) , ) : return True parent = parent . parent return False
def register ( linter ) : linter . register checker ( Basic Error Checker ( linter ) ) linter . register checker ( Basic Checker ( linter ) ) linter . register checker ( Name Checker ( linter ) ) linter . register checker ( Doc String Checker ( linter ) ) linter . register checker ( Pass Checker ( linter ) ) linter . register checker ( Comparison Checker ( linter ) )
def visit starred ( self , node ) : if isinstance ( node . parent , astroid . Call ) : # f(*args) is converted to Call(args=[Starred]), so ignore # them for this check. return if PY35 and isinstance ( node . parent , ( astroid . List , astroid . Tuple , astroid . Set , astroid . Dict ) ) : # PEP 448 unpacking. return stmt = node . statement ( ) if not isinstance ( stmt , astroid . Assign ) : return if stmt . value is node or stmt . value . parent of ( node ) : self . add message ( "star-needs-assignment-target" , node = node )
def check nonlocal and global ( self , node ) : def same scope ( current ) : return current . scope ( ) is node from iter = itertools . chain . from iterable nonlocals = set ( from iter ( child . names for child in node . nodes of class ( astroid . Nonlocal ) if same scope ( child ) ) ) if not nonlocals : return global vars = set ( from iter ( child . names for child in node . nodes of class ( astroid . Global ) if same scope ( child ) ) ) for name in nonlocals . intersection ( global vars ) : self . add message ( "nonlocal-and-global" , args = ( name , ) , node = node )
def visit unaryop ( self , node ) : if ( ( node . op in "+-" ) and isinstance ( node . operand , astroid . Unary Op ) and ( node . operand . op == node . op ) ) : self . add message ( "nonexistent-operator" , node = node , args = node . op * 2 )
def check else on loop ( self , node ) : if node . orelse and not loop exits early ( node ) : self . add message ( "useless-else-on-loop" , node = node , # This is not optimal, but the line previous # to the first statement in the else clause # will usually be the one that contains the else:. line = node . orelse [ 0 ] . lineno - 1 , )
def check in loop ( self , node , node name ) : node = node . parent while node : if isinstance ( node , ( astroid . For , astroid . While ) ) : if node not in node . orelse : return if isinstance ( node , ( astroid . Class Def , astroid . Function Def ) ) : break if ( isinstance ( node , astroid . Try Finally ) and node in node . finalbody and isinstance ( node , astroid . Continue ) ) : self . add message ( "continue-in-finally" , node = node ) node = node . parent self . add message ( "not-in-loop" , node = node , args = node name )
def open ( self ) : self . tryfinallys = [ ] self . stats = self . linter . add stats ( module = 0 , function = 0 , method = 0 , class = 0 )
def visit expr ( self , node ) : expr = node . value if isinstance ( expr , astroid . Const ) and isinstance ( expr . value , str ) : # treat string statement in a separated message # Handle PEP-257 attribute docstrings. # An attribute docstring is defined as being a string right after # an assignment at the module level, class level or  init  level. scope = expr . scope ( ) if isinstance ( scope , ( astroid . Class Def , astroid . Module , astroid . Function Def ) ) : if isinstance ( scope , astroid . Function Def ) and scope . name != " init " : pass else : sibling = expr . previous sibling ( ) if ( sibling is not None and sibling . scope ( ) is scope and isinstance ( sibling , ( astroid . Assign , astroid . Ann Assign ) ) ) : return self . add message ( "pointless-string-statement" , node = node ) return # Ignore if this is : # * a direct function call # * the unique child of a try/except body # * a yieldd statement # * an ellipsis (which can be used on Python 3 instead of pass) # warn W0106 if we have any underlying function call (we can't predict # side effects), else pointless-statement if isinstance ( expr , ( astroid . Yield , astroid . Await , astroid . Ellipsis , astroid . Call ) ) or ( isinstance ( node . parent , astroid . Try Except ) and node . parent . body == [ node ] ) : return if any ( expr . nodes of class ( astroid . Call ) ) : self . add message ( "expression-not-assigned" , node = node , args = expr . as string ( ) ) else : self . add message ( "pointless-statement" , node = node )
def visit lambda ( self , node ) : # if the body of the lambda is a call expression with the same # argument list as the lambda itself, then the lambda is # possibly unnecessary and at least suspicious. if node . args . defaults : # If the arguments of the lambda include defaults, then a # judgment cannot be made because there is no way to check # that the defaults defined by the lambda are the same as # the defaults defined by the function called in the body # of the lambda. return call = node . body if not isinstance ( call , astroid . Call ) : # The body of the lambda must be a function call expression # for the lambda to be unnecessary. return if isinstance ( node . body . func , astroid . Attribute ) and isinstance ( node . body . func . expr , astroid . Call ) : # Chained call, the intermediate call might # return something else (but we don't check that, yet). return call site = Call Site . from call ( call ) ordinary args = list ( node . args . args ) new call args = list ( self . filter vararg ( node , call . args ) ) if node . args . kwarg : if self . has variadic argument ( call . kwargs , node . args . kwarg ) : return if node . args . vararg : if self . has variadic argument ( call . starargs , node . args . vararg ) : return elif call . starargs : return if call . keywords : # Look for additional keyword arguments that are not part # of the lambda's signature lambda kwargs = { keyword . name for keyword in node . args . defaults } if len ( lambda kwargs ) != len ( call site . keyword arguments ) : # Different lengths, so probably not identical return if set ( call site . keyword arguments ) . difference ( lambda kwargs ) : return # The "ordinary" arguments must be in a correspondence such that: # ordinary args[i].name == call.args[i].name. if len ( ordinary args ) != len ( new call args ) : return for arg , passed arg in zip ( ordinary args , new call args ) : if not isinstance ( passed arg , astroid . Name ) : return if arg . name != passed arg . name : return self . add message ( "unnecessary-lambda" , line = node . fromlineno , node = node )
def visit assert ( self , node ) : if ( node . fail is None and isinstance ( node . test , astroid . Tuple ) and len ( node . test . elts ) == 2 ) : self . add message ( "assert-on-tuple" , node = node )
def visit dict ( self , node ) : keys = set ( ) for k , in node . items : if isinstance ( k , astroid . Const ) : key = k . value if key in keys : self . add message ( "duplicate-key" , node = node , args = key ) keys . add ( key )
def check reversed ( self , node ) : try : argument = utils . safe infer ( utils . get argument from call ( node , position = 0 ) ) except utils . No Such Argument Error : pass else : if argument is astroid . Uninferable : return if argument is None : # Nothing was infered. # Try to see if we have iter(). if isinstance ( node . args [ 0 ] , astroid . Call ) : try : func = next ( node . args [ 0 ] . func . infer ( ) ) except astroid . Inference Error : return if getattr ( func , "name" , None ) == "iter" and utils . is builtin object ( func ) : self . add message ( "bad-reversed-sequence" , node = node ) return if isinstance ( argument , ( astroid . List , astroid . Tuple ) ) : return if isinstance ( argument , astroid . Instance ) : if argument . proxied . name == "dict" and utils . is builtin object ( argument . proxied ) : self . add message ( "bad-reversed-sequence" , node = node ) return if any ( ancestor . name == "dict" and utils . is builtin object ( ancestor ) for ancestor in argument . proxied . ancestors ( ) ) : # Mappings aren't accepted by reversed(), unless # they provide explicitly a  reversed  method. try : argument . locals [ REVERSED PROTOCOL METHOD ] except Key Error : self . add message ( "bad-reversed-sequence" , node = node ) return if hasattr ( argument , "getattr" ) : # everything else is not a proper sequence for reversed() for methods in REVERSED METHODS : for meth in methods : try : argument . getattr ( meth ) except astroid . Not Found Error : break else : break else : self . add message ( "bad-reversed-sequence" , node = node ) else : self . add message ( "bad-reversed-sequence" , node = node )
def visit assignname ( self , node ) : self . check assign to new keyword violation ( node . name , node ) frame = node . frame ( ) assign type = node . assign type ( ) if isinstance ( assign type , astroid . Comprehension ) : self . check name ( "inlinevar" , node . name , node ) elif isinstance ( frame , astroid . Module ) : if isinstance ( assign type , astroid . Assign ) and not in loop ( assign type ) : if isinstance ( utils . safe infer ( assign type . value ) , astroid . Class Def ) : self . check name ( "class" , node . name , node ) else : if not redefines import ( node ) : # Don't emit if the name redefines an import # in an Import Error except handler. self . check name ( "const" , node . name , node ) elif isinstance ( assign type , astroid . Except Handler ) : self . check name ( "variable" , node . name , node ) elif isinstance ( frame , astroid . Function Def ) : # global introduced variable aren't in the function locals if node . name in frame and node . name not in frame . argnames ( ) : if not redefines import ( node ) : self . check name ( "variable" , node . name , node ) elif isinstance ( frame , astroid . Class Def ) : if not list ( frame . local attr ancestors ( node . name ) ) : self . check name ( "class attribute" , node . name , node )
def check name ( self , node type , name , node , confidence = interfaces . HIGH ) : def should exempt from invalid name ( node ) : if node type == "variable" : inferred = utils . safe infer ( node ) if isinstance ( inferred , astroid . Class Def ) : return True return False if utils . is inside except ( node ) : clobbering , = utils . clobber in except ( node ) if clobbering : return if name in self . config . good names : return if name in self . config . bad names : self . stats [ "badname " + node type ] += 1 self . add message ( "blacklisted-name" , node = node , args = name ) return regexp = self . name regexps [ node type ] match = regexp . match ( name ) if is multi naming match ( match , node type , confidence ) : name group = self . find name group ( node type ) bad name group = self . bad names . setdefault ( name group , { } ) warnings = bad name group . setdefault ( match . lastgroup , [ ] ) warnings . append ( ( node , node type , name , confidence ) ) if match is None and not should exempt from invalid name ( node ) : self . raise name warning ( node , node type , name , confidence )
def check docstring ( self , node type , node , report missing = True , confidence = interfaces . HIGH ) : docstring = node . doc if docstring is None : if not report missing : return lines = utils . get node last lineno ( node ) - node . lineno if node type == "module" and not lines : # If the module has no body, there's no reason # to require a docstring. return max lines = self . config . docstring min length if node type != "module" and max lines > - 1 and lines < max lines : return self . stats [ "undocumented " + node type ] += 1 if ( node . body and isinstance ( node . body [ 0 ] , astroid . Expr ) and isinstance ( node . body [ 0 ] . value , astroid . Call ) ) : # Most likely a string with a format call. Let's see. func = utils . safe infer ( node . body [ 0 ] . value . func ) if isinstance ( func , astroid . Bound Method ) and isinstance ( func . bound , astroid . Instance ) : # Strings in Python 3, others in Python 2. if PY3K and func . bound . name == "str" : return if func . bound . name in ( "str" , "unicode" , "bytes" ) : return self . add message ( "missing-docstring" , node = node , args = ( node type , ) , confidence = confidence ) elif not docstring . strip ( ) : self . stats [ "undocumented " + node type ] += 1 self . add message ( "empty-docstring" , node = node , args = ( node type , ) , confidence = confidence )
def check literal comparison ( self , literal , node ) : nodes = ( astroid . List , astroid . Tuple , astroid . Dict , astroid . Set ) is other literal = isinstance ( literal , nodes ) is const = False if isinstance ( literal , astroid . Const ) : if isinstance ( literal . value , bool ) or literal . value is None : # Not interested in this values. return is const = isinstance ( literal . value , ( bytes , str , int , float ) ) if is const or is other literal : self . add message ( "literal-comparison" , node = node )
def subgraph ( self , node , name , extra blocks = ( ) ) : if self . graph is None : # global loop self . graph = Path Graph ( node ) self . subgraph parse ( node , node , extra blocks ) self . graphs [ "%s%s" % ( self . classname , name ) ] = self . graph self . reset ( ) else : self . append node ( node ) self . subgraph parse ( node , node , extra blocks )
def subgraph parse ( self , node , pathnode , extra blocks ) : # pylint: disable=unused-argument loose ends = [ ] self . tail = node self . dispatch list ( node . body ) loose ends . append ( self . tail ) for extra in extra blocks : self . tail = node self . dispatch list ( extra . body ) loose ends . append ( self . tail ) if node . orelse : self . tail = node self . dispatch list ( node . orelse ) loose ends . append ( self . tail ) else : loose ends . append ( node ) if node : bottom = "%s" % self . bottom counter self . bottom counter += 1 for le in loose ends : self . graph . connect ( le , bottom ) self . tail = bottom
def add checker ( self , checker ) : # XXX : should be possible to merge needed checkers and add checker vcids = set ( ) lcids = set ( ) visits = self . visit events leaves = self . leave events for member in dir ( checker ) : cid = member [ 6 : ] if cid == "default" : continue if member . startswith ( "visit " ) : v meth = getattr ( checker , member ) # don't use visit methods with no activated message: if self . is method enabled ( v meth ) : visits [ cid ] . append ( v meth ) vcids . add ( cid ) elif member . startswith ( "leave " ) : l meth = getattr ( checker , member ) # don't use leave methods with no activated message: if self . is method enabled ( l meth ) : leaves [ cid ] . append ( l meth ) lcids . add ( cid ) visit default = getattr ( checker , "visit default" , None ) if visit default : for cls in nodes . ALL NODE CLASSES : cid = cls . name . lower ( ) if cid not in vcids : visits [ cid ] . append ( visit default )
def add relationship ( self , from object , to object , relation type , name = None ) : rel = Relationship ( from object , to object , relation type , name ) self . relationships . setdefault ( relation type , [ ] ) . append ( rel )
def get relationship ( self , from object , relation type ) : for rel in self . relationships . get ( relation type , ( ) ) : if rel . from object is from object : return rel raise Key Error ( relation type )
def get attrs ( self , node ) : attrs = [ ] properties = [ ( n , m ) for n , m in node . items ( ) if isinstance ( m , astroid . Function Def ) and decorated with property ( m ) ] for node name , associated nodes in ( list ( node . instance attrs type . items ( ) ) + list ( node . locals type . items ( ) ) + properties ) : if not self . show attr ( node name ) : continue names = self . class names ( associated nodes ) if names : node name = "%s : %s" % ( node name , ", " . join ( names ) ) attrs . append ( node name ) return sorted ( attrs )
def add object ( self , title , node ) : assert node not in self . nodes ent = Diagram Entity ( title , node ) self . nodes [ node ] = ent self . objects . append ( ent )
def class names ( self , nodes ) : names = [ ] for node in nodes : if isinstance ( node , astroid . Instance ) : node = node . proxied if ( isinstance ( node , astroid . Class Def ) and hasattr ( node , "name" ) and not self . has node ( node ) ) : if node . name not in names : node name = node . name names . append ( node name ) return names
def classes ( self ) : return [ o for o in self . objects if isinstance ( o . node , astroid . Class Def ) ]
def classe ( self , name ) : for klass in self . classes ( ) : if klass . node . name == name : return klass raise Key Error ( name )
def extract relationships ( self ) : for obj in self . classes ( ) : node = obj . node obj . attrs = self . get attrs ( node ) obj . methods = self . get methods ( node ) # shape if is interface ( node ) : obj . shape = "interface" else : obj . shape = "class" # inheritance link for par node in node . ancestors ( recurs = False ) : try : par obj = self . object from node ( par node ) self . add relationship ( obj , par obj , "specialization" ) except Key Error : continue # implements link for impl node in node . implements : try : impl obj = self . object from node ( impl node ) self . add relationship ( obj , impl obj , "implements" ) except Key Error : continue # associations link for name , values in list ( node . instance attrs type . items ( ) ) + list ( node . locals type . items ( ) ) : for value in values : if value is astroid . Uninferable : continue if isinstance ( value , astroid . Instance ) : value = value . proxied try : associated obj = self . object from node ( value ) self . add relationship ( associated obj , obj , "association" , name ) except Key Error : continue
def modules ( self ) : return [ o for o in self . objects if isinstance ( o . node , astroid . Module ) ]
def module ( self , name ) : for mod in self . modules ( ) : if mod . node . name == name : return mod raise Key Error ( name )
def add from depend ( self , node , from module ) : mod name = node . root ( ) . name obj = self . module ( mod name ) if from module not in obj . node . depends : obj . node . depends . append ( from module )
def extract relationships ( self ) : Class Diagram . extract relationships ( self ) for obj in self . classes ( ) : # ownership try : mod = self . object from node ( obj . node . root ( ) ) self . add relationship ( obj , mod , "ownership" ) except Key Error : continue for obj in self . modules ( ) : obj . shape = "package" # dependencies for dep name in obj . node . depends : try : dep = self . get module ( dep name , obj . node ) except Key Error : continue self . add relationship ( obj , dep , "depends" )
def query ( self ) : if hasattr ( self . model , 'query' ) : return self . model . query else : return self . session . query ( self . model )
def prepare request ( uri , headers = None , data = None , method = None ) : if headers is None : headers = { } if data and not method : method = 'POST' elif not method : method = 'GET' if method == 'GET' and data : uri = add params to uri ( uri , data ) data = None return uri , headers , data , method
def handle oauth1 response ( self , args ) : client = self . make client ( ) client . verifier = args . get ( 'oauth verifier' ) tup = session . get ( '%s oauthtok' % self . name ) if not tup : raise O Auth Exception ( 'Token not found, maybe you disabled cookie' , type = 'token not found' ) client . resource owner key = tup [ 0 ] client . resource owner secret = tup [ 1 ] uri , headers , data = client . sign ( self . expand url ( self . access token url ) , encode ( self . access token method ) ) headers . update ( self . access token headers ) resp , content = self . http request ( uri , headers , to bytes ( data , self . encoding ) , method = self . access token method ) data = parse response ( resp , content ) if resp . code not in ( 200 , 201 ) : raise O Auth Exception ( 'Invalid response from %s' % self . name , type = 'invalid response' , data = data ) return data
def handle oauth2 response ( self , args ) : client = self . make client ( ) remote args = { 'code' : args . get ( 'code' ) , 'client secret' : self . consumer secret , 'redirect uri' : session . get ( '%s oauthredir' % self . name ) } log . debug ( 'Prepare oauth2 remote args %r' , remote args ) remote args . update ( self . access token params ) headers = copy ( self . access token headers ) if self . access token method == 'POST' : headers . update ( { 'Content-Type' : 'application/x-www-form-urlencoded' } ) body = client . prepare request body ( * * remote args ) resp , content = self . http request ( self . expand url ( self . access token url ) , headers = headers , data = to bytes ( body , self . encoding ) , method = self . access token method , ) elif self . access token method == 'GET' : qs = client . prepare request body ( * * remote args ) url = self . expand url ( self . access token url ) url += ( '?' in url and '&' or '?' ) + qs resp , content = self . http request ( url , headers = headers , method = self . access token method , ) else : raise O Auth Exception ( 'Unsupported access token method: %s' % self . access token method ) data = parse response ( resp , content , content type = self . content type ) if resp . code not in ( 200 , 201 ) : raise O Auth Exception ( 'Invalid response from %s' % self . name , type = 'invalid response' , data = data ) return data
def authorized response ( self , args = None ) : if args is None : args = request . args if 'oauth verifier' in args : data = self . handle oauth1 response ( args ) elif 'code' in args : data = self . handle oauth2 response ( args ) else : data = self . handle unknown response ( ) # free request token session . pop ( '%s oauthtok' % self . name , None ) session . pop ( '%s oauthredir' % self . name , None ) return data
def make client with token ( self , token ) : cached clients = getattr ( self , 'clients' , None ) hashed token = hash token ( self , token ) if cached clients and hashed token in cached clients : return cached clients [ hashed token ] client = self . make client ( token ) # implemented in subclasses if cached clients : cached clients [ hashed token ] = client return client
def confirm authorization request ( self ) : server = self . server uri , http method , body , headers = extract params ( ) try : realms , credentials = server . get realms and credentials ( uri , http method = http method , body = body , headers = headers ) ret = server . create authorization response ( uri , http method , body , headers , realms , credentials ) log . debug ( 'Authorization successful.' ) return create response ( * ret ) except errors . O Auth1Error as e : return redirect ( e . in uri ( self . error uri ) ) except errors . Invalid Client Error as e : return redirect ( e . in uri ( self . error uri ) )
def require oauth ( self , * realms , * * kwargs ) : def wrapper ( f ) : @ wraps ( f ) def decorated ( * args , * * kwargs ) : for func in self . before request funcs : func ( ) if hasattr ( request , 'oauth' ) and request . oauth : return f ( * args , * * kwargs ) server = self . server uri , http method , body , headers = extract params ( ) try : valid , req = server . validate protected resource request ( uri , http method , body , headers , realms ) except Exception as e : log . warn ( 'Exception: %r' , e ) e . urlencoded = urlencode ( [ ( 'error' , 'unknown' ) ] ) e . status code = 400 return error response ( e ) for func in self . after request funcs : valid , req = func ( valid , req ) if not valid : return abort ( 401 ) # alias user for convenience req . user = req . access token . user request . oauth = req return f ( * args , * * kwargs ) return decorated return wrapper
def get default realms ( self , client key , request ) : log . debug ( 'Get realms for %r' , client key ) if not request . client : request . client = self . clientgetter ( client key = client key ) client = request . client if hasattr ( client , 'default realms' ) : return client . default realms return [ ]
def get realms ( self , token , request ) : log . debug ( 'Get realms of %r' , token ) tok = request . request token or self . grantgetter ( token = token ) if not tok : return [ ] request . request token = tok if hasattr ( tok , 'realms' ) : return tok . realms or [ ] return [ ]
def get redirect uri ( self , token , request ) : log . debug ( 'Get redirect uri of %r' , token ) tok = request . request token or self . grantgetter ( token = token ) return tok . redirect uri
def get rsa key ( self , client key , request ) : if not request . client : request . client = self . clientgetter ( client key = client key ) if hasattr ( request . client , 'rsa key' ) : return request . client . rsa key return None
def validate client key ( self , client key , request ) : log . debug ( 'Validate client key for %r' , client key ) if not request . client : request . client = self . clientgetter ( client key = client key ) if request . client : return True return False
def validate request token ( self , client key , token , request ) : log . debug ( 'Validate request token %r for %r' , token , client key ) tok = request . request token or self . grantgetter ( token = token ) if tok and tok . client key == client key : request . request token = tok return True return False
def validate access token ( self , client key , token , request ) : log . debug ( 'Validate access token %r for %r' , token , client key ) tok = request . access token or self . tokengetter ( client key = client key , token = token , ) if tok : request . access token = tok return True return False
def validate timestamp and nonce ( self , client key , timestamp , nonce , request , request token = None , access token = None ) : log . debug ( 'Validate timestamp and nonce %r' , client key ) nonce exists = self . noncegetter ( client key = client key , timestamp = timestamp , nonce = nonce , request token = request token , access token = access token ) if nonce exists : return False self . noncesetter ( client key = client key , timestamp = timestamp , nonce = nonce , request token = request token , access token = access token ) return True
def validate redirect uri ( self , client key , redirect uri , request ) : log . debug ( 'Validate redirect uri %r for %r' , redirect uri , client key ) if not request . client : request . client = self . clientgetter ( client key = client key ) if not request . client : return False if not request . client . redirect uris and redirect uri is None : return True request . redirect uri = redirect uri return redirect uri in request . client . redirect uris
def validate realms ( self , client key , token , request , uri = None , realms = None ) : log . debug ( 'Validate realms %r for %r' , realms , client key ) if request . access token : tok = request . access token else : tok = self . tokengetter ( client key = client key , token = token ) request . access token = tok if not tok : return False return set ( tok . realms ) . issuperset ( set ( realms ) )
def validate verifier ( self , client key , token , verifier , request ) : log . debug ( 'Validate verifier %r for %r' , verifier , client key ) data = self . verifiergetter ( verifier = verifier , token = token ) if not data : return False if not hasattr ( data , 'user' ) : log . debug ( 'Verifier should has user attribute' ) return False request . user = data . user if hasattr ( data , 'client key' ) : return data . client key == client key return True
def verify request token ( self , token , request ) : log . debug ( 'Verify request token %r' , token ) tok = request . request token or self . grantgetter ( token = token ) if tok : request . request token = tok return True return False
def verify realms ( self , token , realms , request ) : log . debug ( 'Verify realms %r' , realms ) tok = request . request token or self . grantgetter ( token = token ) if not tok : return False request . request token = tok if not hasattr ( tok , 'realms' ) : # realms not enabled return True return set ( tok . realms ) == set ( realms )
def confirm authorization request ( self ) : server = self . server scope = request . values . get ( 'scope' ) or '' scopes = scope . split ( ) credentials = dict ( client id = request . values . get ( 'client id' ) , redirect uri = request . values . get ( 'redirect uri' , None ) , response type = request . values . get ( 'response type' , None ) , state = request . values . get ( 'state' , None ) ) log . debug ( 'Fetched credentials from request %r.' , credentials ) redirect uri = credentials . get ( 'redirect uri' ) log . debug ( 'Found redirect uri %s.' , redirect uri ) uri , http method , body , headers = extract params ( ) try : ret = server . create authorization response ( uri , http method , body , headers , scopes , credentials ) log . debug ( 'Authorization successful.' ) return create response ( * ret ) except oauth2 . Fatal Client Error as e : log . debug ( 'Fatal client error %r' , e , exc info = True ) return self . on exception ( e , e . in uri ( self . error uri ) ) except oauth2 . O Auth2Error as e : log . debug ( 'O Auth2Error: %r' , e , exc info = True ) # on auth error, we should preserve state if it's present according to RFC 6749 state = request . values . get ( 'state' ) if state and not e . state : e . state = state # set e.state so e.in uri() can add the state query parameter to redirect uri return self . on exception ( e , e . in uri ( redirect uri or self . error uri ) ) except Exception as e : log . exception ( e ) return self . on exception ( e , add params to uri ( self . error uri , { 'error' : str ( e ) } ) )
def require oauth ( self , * scopes ) : def wrapper ( f ) : @ wraps ( f ) def decorated ( * args , * * kwargs ) : for func in self . before request funcs : func ( ) if hasattr ( request , 'oauth' ) and request . oauth : return f ( * args , * * kwargs ) valid , req = self . verify request ( scopes ) for func in self . after request funcs : valid , req = func ( valid , req ) if not valid : if self . invalid response : return self . invalid response ( req ) return abort ( 401 ) request . oauth = req return f ( * args , * * kwargs ) return decorated return wrapper
def get default redirect uri ( self , client id , request , * args , * * kwargs ) : request . client = request . client or self . clientgetter ( client id ) redirect uri = request . client . default redirect uri log . debug ( 'Found default redirect uri %r' , redirect uri ) return redirect uri
def get default scopes ( self , client id , request , * args , * * kwargs ) : request . client = request . client or self . clientgetter ( client id ) scopes = request . client . default scopes log . debug ( 'Found default scopes %r' , scopes ) return scopes
def save authorization code ( self , client id , code , request , * args , * * kwargs ) : log . debug ( 'Persist authorization code %r for client %r' , code , client id ) request . client = request . client or self . clientgetter ( client id ) self . grantsetter ( client id , code , request , * args , * * kwargs ) return request . client . default redirect uri
def save bearer token ( self , token , request , * args , * * kwargs ) : log . debug ( 'Save bearer token %r' , token ) self . tokensetter ( token , request , * args , * * kwargs ) return request . client . default redirect uri
def validate client id ( self , client id , request , * args , * * kwargs ) : log . debug ( 'Validate client %r' , client id ) client = request . client or self . clientgetter ( client id ) if client : # attach client to request object request . client = client return True return False
def validate code ( self , client id , code , client , request , * args , * * kwargs ) : client = client or self . clientgetter ( client id ) log . debug ( 'Validate code for client %r and code %r' , client . client id , code ) grant = self . grantgetter ( client id = client . client id , code = code ) if not grant : log . debug ( 'Grant not found.' ) return False if hasattr ( grant , 'expires' ) and datetime . datetime . utcnow ( ) > grant . expires : log . debug ( 'Grant is expired.' ) return False request . state = kwargs . get ( 'state' ) request . user = grant . user request . scopes = grant . scopes return True
def validate scopes ( self , client id , scopes , client , request , * args , * * kwargs ) : if hasattr ( client , 'validate scopes' ) : return client . validate scopes ( scopes ) return set ( client . default scopes ) . issuperset ( set ( scopes ) )
def revoke token ( self , token , token type hint , request , * args , * * kwargs ) : if token type hint : tok = self . tokengetter ( * * { token type hint : token } ) else : tok = self . tokengetter ( access token = token ) if not tok : tok = self . tokengetter ( refresh token = token ) if tok : request . client id = tok . client id request . user = tok . user tok . delete ( ) return True msg = 'Invalid token supplied.' log . debug ( msg ) request . error message = msg return False
def update qq api request data ( data = { } ) : defaults = { 'openid' : session . get ( 'qq openid' ) , 'access token' : session . get ( 'qq token' ) [ 0 ] , 'oauth consumer key' : QQ APP ID , } defaults . update ( data ) return defaults
def convert keys to string ( dictionary ) : if not isinstance ( dictionary , dict ) : return dictionary return dict ( ( str ( k ) , convert keys to string ( v ) ) for k , v in dictionary . items ( ) )
def register to ( self , oauth , name = None , * * kwargs ) : kwargs = self . process kwargs ( name = ( name or self . default name ) , * * kwargs ) return oauth . remote app ( * * kwargs )
def create ( self , oauth , * * kwargs ) : kwargs = self . process kwargs ( name = self . default name , register = False , * * kwargs ) return oauth . remote app ( * * kwargs )
def extract params ( ) : uri = get uri from request ( request ) http method = request . method headers = dict ( request . headers ) if 'wsgi.input' in headers : del headers [ 'wsgi.input' ] if 'wsgi.errors' in headers : del headers [ 'wsgi.errors' ] # Werkzeug, and subsequently Flask provide a safe Authorization header # parsing, so we just replace the Authorization header with the extraced # info if it was successfully parsed. if request . authorization : headers [ 'Authorization' ] = request . authorization body = request . form . to dict ( ) return uri , http method , body , headers
def to bytes ( text , encoding = 'utf-8' ) : if not text : return text if not isinstance ( text , bytes type ) : text = text . encode ( encoding ) return text
def decode base64 ( text , encoding = 'utf-8' ) : text = to bytes ( text , encoding ) return to unicode ( base64 . b64decode ( text ) , encoding )
def create response ( headers , body , status ) : response = Response ( body or '' ) for k , v in headers . items ( ) : response . headers [ str ( k ) ] = v response . status code = status return response
def get cached clients ( ) : if O Auth . state key not in current app . extensions : raise Runtime Error ( '%r is not initialized.' % current app ) state = current app . extensions [ O Auth . state key ] return state . cached clients
def check exception ( self ) : for i in xrange ( self . iterations ) : cert = X509 ( ) try : cert . get pubkey ( ) except Error : pass
def check success ( self ) : small = xrange ( 3 ) for i in xrange ( self . iterations ) : key = P Key ( ) key . generate key ( TYPE DSA , 256 ) for i in small : cert = X509 ( ) cert . set pubkey ( key ) for i in small : cert . get pubkey ( )
def check load privatekey callback ( self ) : for i in xrange ( self . iterations * 10 ) : load privatekey ( FILETYPE PEM , self . ENCRYPTED PEM , lambda * args : "hello, secret" )
def bio to string ( bio ) : result buffer = ffi . new ( 'char**' ) buffer length = lib . BIO get mem data ( bio , result buffer ) return ffi . buffer ( result buffer [ 0 ] , buffer length ) [ : ]
def print token factory ( col ) : def helper ( msg ) : style = style from dict ( { Token . Color : col , } ) tokens = [ ( Token . Color , msg ) ] print tokens ( tokens , style = style ) def helper no terminal ( msg ) : # workaround if we have no terminal print ( msg ) if sys . stdout . isatty ( ) : return helper else : return helper no terminal
def get service metadata ( self ) : return { 'import labels as tags' : self . config . get ( 'import labels as tags' , False , asbool ) , 'label template' : self . config . get ( 'label template' , DEFAULT LABEL TEMPLATE ) , }
def issues ( self ) : for board in self . get boards ( ) : for lst in self . get lists ( board [ 'id' ] ) : listextra = dict ( boardname = board [ 'name' ] , listname = lst [ 'name' ] ) for card in self . get cards ( lst [ 'id' ] ) : issue = self . get issue for record ( card , extra = listextra ) issue . update extra ( { "annotations" : self . annotations ( card ) } ) yield issue
def get comments ( self , card id ) : params = { 'filter' : 'comment Card' , 'member Creator fields' : 'username' } comments = self . api request ( "/1/cards/{card id}/actions" . format ( card id = card id ) , * * params ) for comment in comments : assert comment [ 'type' ] == 'comment Card' yield comment
def get issues ( self , repo , keys ) : key1 , key2 = keys key3 = key1 [ : - 1 ] # Just the singular form of key1 url = self . base url + "/api/0/" + repo + "/" + key1 response = self . session . get ( url , params = dict ( status = 'Open' ) ) if not bool ( response ) : error = response . json ( ) code = error [ 'error code' ] if code == 'ETRACKERDISABLED' : return [ ] else : raise IO Error ( 'Failed to talk to %r %r' % ( url , error ) ) issues = [ ] for result in response . json ( ) [ key2 ] : idx = six . text type ( result [ 'id' ] ) result [ 'html url' ] = "/" . join ( [ self . base url , repo , key3 , idx ] ) issues . append ( ( repo , result ) ) return issues
def api url ( self , path , * * context ) : if self . host == 'github.com' : baseurl = "https://api.github.com" else : baseurl = "https://{}/api/v3" . format ( self . host ) return baseurl + path . format ( * * context )
def getter ( self , url , subkey = None ) : kwargs = { } if 'basic' in self . auth : kwargs [ 'auth' ] = self . auth [ 'basic' ] results = [ ] link = dict ( next = url ) while 'next' in link : response = self . session . get ( link [ 'next' ] , * * kwargs ) # Warn about the mis-leading 404 error code.  See: if response . status code == 404 and 'token' in self . auth : log . warn ( "A '404' from github may indicate an auth " "failure. Make sure both that your token is correct " "and that it has 'public repo' and not 'public " "access' rights." ) json res = self . json response ( response ) if subkey is not None : json res = json res [ subkey ] results += json res link = self . link field to dict ( response . headers . get ( 'link' , None ) ) return results
def get owned repo issues ( self , tag ) : issues = { } for issue in self . client . get issues ( * tag . split ( '/' ) ) : issues [ issue [ 'url' ] ] = ( tag , issue ) return issues
def get query ( self , query ) : issues = { } for issue in self . client . get query ( query ) : url = issue [ 'html url' ] try : repo = self . get repository from issue ( issue ) except Value Error as e : log . critical ( e ) else : issues [ url ] = ( repo , issue ) return issues
def reqs ( self , tag ) : return [ ( tag , i ) for i in self . client . get pulls ( * tag . split ( '/' ) ) ]
def aggregate issues ( conf , main section , debug ) : log . info ( "Starting to aggregate remote issues." ) # Create and call service objects for every target in the config targets = aslist ( conf . get ( main section , 'targets' ) ) queue = multiprocessing . Queue ( ) log . info ( "Spawning %i workers." % len ( targets ) ) processes = [ ] if debug : for target in targets : aggregate issues ( conf , main section , target , queue , conf . get ( target , 'service' ) ) else : for target in targets : proc = multiprocessing . Process ( target = aggregate issues , args = ( conf , main section , target , queue , conf . get ( target , 'service' ) ) ) proc . start ( ) processes . append ( proc ) # Sleep for 1 second here to try and avoid a race condition where # all N workers start up and ask the gpg-agent process for # information at the same time.  This causes gpg-agent to fumble # and tell some of our workers some incomplete things. time . sleep ( 1 ) currently running = len ( targets ) while currently running > 0 : issue = queue . get ( True ) if isinstance ( issue , tuple ) : completion type , args = issue if completion type == SERVICE FINISHED ERROR : target , e = args log . info ( "Terminating workers" ) for process in processes : process . terminate ( ) raise Runtime Error ( "critical error in target '{}'" . format ( target ) ) currently running -= 1 continue yield issue log . info ( "Done aggregating remote issues." )
def get config or default ( self , key , default , as type = lambda x : x ) : if self . main config . has option ( self . main section , key ) : return as type ( self . main config . get ( self . main section , key ) ) return default
def validate config ( cls , service config , target ) : if service config . has option ( target , 'only if assigned' ) : die ( "[%s] has an 'only if assigned' option.  Should be " "'%s.only if assigned'." % ( target , cls . CONFIG PREFIX ) ) if service config . has option ( target , 'also unassigned' ) : die ( "[%s] has an 'also unassigned' option.  Should be " "'%s.also unassigned'." % ( target , cls . CONFIG PREFIX ) ) if service config . has option ( target , 'default priority' ) : die ( "[%s] has a 'default priority' option.  Should be " "'%s.default priority'." % ( target , cls . CONFIG PREFIX ) ) if service config . has option ( target , 'add tags' ) : die ( "[%s] has an 'add tags' option.  Should be " "'%s.add tags'." % ( target , cls . CONFIG PREFIX ) )
def include ( self , issue ) : only if assigned = self . config . get ( 'only if assigned' , None ) if only if assigned : owner = self . get owner ( issue ) include owners = [ only if assigned ] if self . config . get ( 'also unassigned' , None , asbool ) : include owners . append ( None ) return owner in include owners only if author = self . config . get ( 'only if author' , None ) if only if author : return self . get author ( issue ) == only if author return True
def oracle eval ( command ) : p = subprocess . Popen ( command , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) p . wait ( ) if p . returncode == 0 : return p . stdout . readline ( ) . strip ( ) . decode ( 'utf-8' ) else : die ( "Error retrieving password: `{command}` returned '{error}'" . format ( command = command , error = p . stderr . read ( ) . strip ( ) ) )
def getint ( self , section , option ) : try : return super ( Bugwarrior Config Parser , self ) . getint ( section , option ) except Value Error : if self . get ( section , option ) == u'' : return None else : raise Value Error ( "{section}.{option} must be an integer or empty." . format ( section = section , option = option ) )
def get data ( self , url ) : return self . json response ( requests . get ( url , * * self . requests kwargs ) )
def hamdist ( str1 , str2 ) : diffs = 0 for ch1 , ch2 in zip ( str1 , str2 ) : if ch1 != ch2 : diffs += 1 return diffs
def fdrcorrection ( pvals , alpha = 0.05 ) : # Implement copy from GOA Tools. pvals = np . asarray ( pvals ) pvals sortind = np . argsort ( pvals ) pvals sorted = np . take ( pvals , pvals sortind ) ecdffactor = ecdf ( pvals sorted ) reject = pvals sorted <= ecdffactor * alpha if reject . any ( ) : rejectmax = max ( np . nonzero ( reject ) [ 0 ] ) reject [ : rejectmax ] = True pvals corrected raw = pvals sorted / ecdffactor pvals corrected = np . minimum . accumulate ( pvals corrected raw [ : : - 1 ] ) [ : : - 1 ] del pvals corrected raw pvals corrected [ pvals corrected > 1 ] = 1 pvals corrected = np . empty like ( pvals corrected ) pvals corrected [ pvals sortind ] = pvals corrected del pvals corrected reject = np . empty like ( reject ) reject [ pvals sortind ] = reject return reject , pvals corrected
def prepare argparser ( ) : description = "%(prog)s -- Gene Set Enrichment Analysis in Python" epilog = "For command line options of each command, type: %(prog)s COMMAND -h" # top-level parser argparser = ap . Argument Parser ( description = description , epilog = epilog ) argparser . add argument ( "--version" , action = "version" , version = "%(prog)s " + version ) subparsers = argparser . add subparsers ( dest = 'subcommand name' ) #help="sub-command help") # command for 'gsea' add gsea parser ( subparsers ) # command for 'prerank' add prerank parser ( subparsers ) # command for 'ssgsea' add singlesample parser ( subparsers ) # command for 'plot' add plot parser ( subparsers ) # command for 'enrichr' add enrichr parser ( subparsers ) # command for 'biomart' add biomart parser ( subparsers ) return argparser
def add gsea parser ( subparsers ) : argparser gsea = subparsers . add parser ( "gsea" , help = "Main GSE Apy Function: run GSE Apy instead of GSEA." ) # group for input files group input = argparser gsea . add argument group ( "Input files arguments" ) group input . add argument ( "-d" , "--data" , dest = "data" , action = "store" , type = str , required = True , help = "Input gene expression dataset file in txt format.Same with GSEA." ) group input . add argument ( "-c" , "--cls" , dest = "cls" , action = "store" , type = str , required = True , help = "Input class vector (phenotype) file in CLS format. Same with GSEA." ) group input . add argument ( "-g" , "--gmt" , dest = "gmt" , action = "store" , type = str , required = True , help = "Gene set database in GMT format. Same with GSEA." ) group input . add argument ( "-t" , "--permu-type" , action = "store" , dest = "type" , type = str , metavar = 'per Type' , choices = ( "gene set" , "phenotype" ) , default = "gene set" , help = "Permutation type. Same with GSEA, choose from {'gene set', 'phenotype'}" ) # group for output files group output = argparser gsea . add argument group ( "Output arguments" ) add output option ( group output ) # group for General options. group opt = argparser gsea . add argument group ( "GSEA advanced arguments" ) group opt . add argument ( "-n" , "--permu-num" , dest = "n" , action = "store" , type = int , default = 1000 , metavar = 'nperm' , help = "Number of random permutations. For calculating esnulls. Default: 1000" ) group opt . add argument ( "--min-size" , dest = "mins" , action = "store" , type = int , default = 15 , metavar = 'int' , help = "Min size of input genes presented in Gene Sets. Default: 15" ) group opt . add argument ( "--max-size" , dest = "maxs" , action = "store" , type = int , default = 500 , metavar = 'int' , help = "Max size of input genes presented in Gene Sets. Default: 500" ) group opt . add argument ( "-w" , "--weight" , action = 'store' , dest = 'weight' , default = 1.0 , type = float , metavar = 'float' , help = 'Weighted score of rank metrics. For weighting input genes. Choose from {0, 1, 1.5, 2}. Default: 1' , ) group opt . add argument ( "-m" , "--method" , action = "store" , dest = "method" , type = str , metavar = '' , choices = ( "signal to noise" , "t test" , "ratio of classes" , "diff of classes" , "log2 ratio of classes" ) , default = "log2 ratio of classes" , help = ) group opt . add argument ( "-a" , "--ascending" , action = 'store true' , dest = 'ascending' , default = False , help = 'Rank metric sorting order. If the -a flag was chosen, then ascending equals to True. Default: False.' ) group opt . add argument ( "-s" , "--seed" , dest = "seed" , action = "store" , type = int , default = None , metavar = '' , help = "Number of random seed. Default: None" ) group opt . add argument ( "-p" , "--threads" , dest = "threads" , action = "store" , type = int , default = 1 , metavar = 'procs' , help = "Number of Processes you are going to use. Default: 1" ) return
def add prerank parser ( subparsers ) : argparser prerank = subparsers . add parser ( "prerank" , help = "Run GSE Apy Prerank tool on preranked gene list." ) # group for input files prerank input = argparser prerank . add argument group ( "Input files arguments" ) prerank input . add argument ( "-r" , "--rnk" , dest = "rnk" , action = "store" , type = str , required = True , help = "Ranking metric file in .rnk format. Same with GSEA." ) prerank input . add argument ( "-g" , "--gmt" , dest = "gmt" , action = "store" , type = str , required = True , help = "Gene set database in GMT format. Same with GSEA." ) prerank input . add argument ( "-l" , "--label" , action = 'store' , nargs = 2 , dest = 'label' , metavar = ( 'pos' , 'neg' ) , type = str , default = ( 'Pos' , 'Neg' ) , help = "The phenotype label argument need two parameters to define. Default: ('Pos','Neg')" ) # group for output files prerank output = argparser prerank . add argument group ( "Output arguments" ) add output option ( prerank output ) # group for General options. prerank opt = argparser prerank . add argument group ( "GSEA advanced arguments" ) prerank opt . add argument ( "-n" , "--permu-num" , dest = "n" , action = "store" , type = int , default = 1000 , metavar = 'nperm' , help = "Number of random permutations. For calculating esnulls. Default: 1000" ) prerank opt . add argument ( "--min-size" , dest = "mins" , action = "store" , type = int , default = 15 , metavar = 'int' , help = "Min size of input genes presented in Gene Sets. Default: 15" ) prerank opt . add argument ( "--max-size" , dest = "maxs" , action = "store" , type = int , default = 500 , metavar = 'int' , help = "Max size of input genes presented in Gene Sets. Default: 500" ) prerank opt . add argument ( "-w" , "--weight" , action = 'store' , dest = 'weight' , default = 1.0 , type = float , metavar = 'float' , help = 'Weighted score of rank metrics. For weighting input genes. Choose from {0, 1, 1.5, 2}. Default: 1' , ) prerank opt . add argument ( "-a" , "--ascending" , action = 'store true' , dest = 'ascending' , default = False , help = 'Rank metric sorting order. If the -a flag was chosen, then ascending equals to True. Default: False.' ) prerank opt . add argument ( "-s" , "--seed" , dest = "seed" , action = "store" , type = int , default = None , metavar = '' , help = "Number of random seed. Default: None" ) prerank opt . add argument ( "-p" , "--threads" , dest = "threads" , action = "store" , type = int , default = 1 , metavar = 'procs' , help = "Number of Processes you are going to use. Default: 1" ) return
def add plot parser ( subparsers ) : argparser replot = subparsers . add parser ( "replot" , help = "Reproduce GSEA desktop output figures." ) group replot = argparser replot . add argument group ( "Input arguments" ) group replot . add argument ( "-i" , "--indir" , action = "store" , dest = "indir" , required = True , metavar = 'GSEA dir' , help = "The GSEA desktop results directroy that you want to reproduce the figure " ) add output option ( group replot ) #add output group( argparser plot ) group replot . add argument ( "-w" , "--weight" , action = 'store' , dest = 'weight' , default = 1.0 , type = float , metavar = 'float' , help = 'Weighted score of rank metrics. Please Use the same value in GSEA. Choose from (0, 1, 1.5, 2),default: 1' , ) return
def add enrichr parser ( subparsers ) : argparser enrichr = subparsers . add parser ( "enrichr" , help = "Using Enrichr API to perform GO analysis." ) # group for required options. enrichr opt = argparser enrichr . add argument group ( "Input arguments" ) enrichr opt . add argument ( "-i" , "--input-list" , action = "store" , dest = "gene list" , type = str , required = True , metavar = 'I Ds' , help = "Enrichr uses a list of gene names as input." ) enrichr opt . add argument ( "-g" , "--gene-sets" , action = "store" , dest = "library" , type = str , required = True , metavar = 'GMT' , help = "Enrichr library name(s) required. Separate each name by comma." ) enrichr opt . add argument ( "--org" , "--organism" , action = "store" , dest = "organism" , type = str , default = '' , help = "Enrichr supported organism name. Default: human. See here: https://amp.pharm.mssm.edu/mod Enrichr." ) enrichr opt . add argument ( "--ds" , "--description" , action = "store" , dest = "descrip" , type = str , default = 'enrichr' , metavar = 'STRING' , help = ) enrichr opt . add argument ( "--cut" , "--cut-off" , action = "store" , dest = "thresh" , metavar = 'float' , type = float , default = 0.05 , help = "Adjust-Pval cutoff, used for generating plots. Default: 0.05." ) enrichr opt . add argument ( "--bg" , "--background" , action = "store" , dest = "bg" , default = 'hsapiens gene ensembl' , metavar = 'BGNUM' , help = "Bio Mart Dataset name or Background total genes number. Default: None" ) enrichr opt . add argument ( "-t" , "--top-term" , dest = "term" , action = "store" , type = int , default = 10 , metavar = 'int' , help = "Numbers of top terms shown in the plot. Default: 10" ) # enrichr opt.add argument("--scale", dest = "scale", action="store", type=float, default=0.5, metavar='float', #                          help="scatter dot scale in the dotplot. Default: 0.5") # enrichr opt.add argument("--no-plot", action='store true', dest='no plot', default=False, #                           help="Suppress the plot output.This is useful only if data are interested. Default: False.") enrichr output = argparser enrichr . add argument group ( "Output figure arguments" ) add output option ( enrichr output ) return
def add biomart parser ( subparsers ) : argparser biomart = subparsers . add parser ( "biomart" , help = "Using Bio Mart API to convert gene ids." ) # group for required options. biomart opt = argparser biomart . add argument group ( "Input arguments" ) biomart opt . add argument ( "-f" , "--filter" , action = 'store' , nargs = 2 , dest = 'filter' , required = True , metavar = ( 'NAME' , 'VALUE' ) , help = ) biomart opt . add argument ( "-a" , "--attributes" , action = "store" , dest = "attrs" , type = str , required = True , metavar = 'ATTR' , help = "Which attribute(s) to retrieve. Separate each attr by comma." ) biomart opt . add argument ( "-o" , "--ofile" , dest = "ofile" , type = str , required = True , help = "Output file name" ) biomart opt . add argument ( "-d" , "--dataset" , action = "store" , dest = "bg" , type = str , default = 'hsapiens gene ensembl' , metavar = 'DATA' , help = "Which dataset to use. Default: hsapiens gene ensembl" ) biomart opt . add argument ( "--host" , action = "store" , dest = "host" , type = str , default = 'www.ensembl.org' , metavar = 'HOST' , help = "Which host to use. Select from {'www.ensembl.org', 'asia.ensembl.org', 'useast.ensembl.org'}." ) biomart opt . add argument ( "-m" , "--mart" , action = "store" , dest = "mart" , type = str , metavar = 'MART' , default = "ENSEMBL MART ENSEMBL" , help = "Which mart to use. Default: ENSEMBL MART ENSEMBL." ) biomart opt . add argument ( "-v" , "--verbose" , action = "store true" , default = False , dest = 'verbose' , help = "Increase output verbosity, print out progress of your job" , )
def get marts ( self ) : mart names = pd . Series ( self . names , name = "Name" ) mart descriptions = pd . Series ( self . display Names , name = "Description" ) return pd . concat ( [ mart names , mart descriptions ] , axis = 1 )
def get datasets ( self , mart = 'ENSEMBL MART ENSEMBL' ) : datasets = self . datasets ( mart , raw = True ) return pd . read csv ( String IO ( datasets ) , header = None , usecols = [ 1 , 2 ] , names = [ "Name" , "Description" ] , sep = "\t" )
def get attributes ( self , dataset ) : attributes = self . attributes ( dataset ) attr = [ ( k , v [ 0 ] ) for k , v in attributes . items ( ) ] return pd . Data Frame ( attr , columns = [ "Attribute" , "Description" ] )
def get filters ( self , dataset ) : filters = self . filters ( dataset ) filt = [ ( k , v [ 0 ] ) for k , v in filters . items ( ) ] return pd . Data Frame ( filt , columns = [ "Filter" , "Description" ] )
def prepare outdir ( self ) : self . outdir = self . outdir if self . outdir is None : self . tmpdir = Temporary Directory ( ) self . outdir = self . tmpdir . name elif isinstance ( self . outdir , str ) : mkdirs ( self . outdir ) else : raise Exception ( "Error parsing outdir: %s" % type ( self . outdir ) ) # handle gmt type if isinstance ( self . gene sets , str ) : gset = os . path . split ( self . gene sets ) [ - 1 ] . lower ( ) . rstrip ( ".gmt" ) elif isinstance ( self . gene sets , dict ) : gset = "blank name" else : raise Exception ( "Error parsing gene sets parameter for gene sets" ) logfile = os . path . join ( self . outdir , "gseapy.%s.%s.log" % ( self . module , gset ) ) return logfile
def set cores ( self ) : cpu num = cpu count ( ) - 1 if self . processes > cpu num : cores = cpu num elif self . processes < 1 : cores = 1 else : cores = self . processes # have to be int if user input is float self . processes = int ( cores )
def load gmt ( self , gene list , gmt ) : if isinstance ( gmt , dict ) : genesets dict = gmt elif isinstance ( gmt , str ) : genesets dict = self . parse gmt ( gmt ) else : raise Exception ( "Error parsing gmt parameter for gene sets" ) subsets = list ( genesets dict . keys ( ) ) self . n genesets = len ( subsets ) for subset in subsets : subset list = genesets dict . get ( subset ) if isinstance ( subset list , set ) : subset list = list ( subset list ) genesets dict [ subset ] = subset list tag indicator = np . in1d ( gene list , subset list , assume unique = True ) tag len = tag indicator . sum ( ) if self . min size <= tag len <= self . max size : continue del genesets dict [ subset ] filsets num = len ( subsets ) - len ( genesets dict ) self . logger . info ( "%04d gene sets have been filtered out when max size=%s and min size=%s" % ( filsets num , self . max size , self . min size ) ) if filsets num == len ( subsets ) : self . logger . error ( "No gene sets passed through filtering condition!!!, try new parameters again!\n" + "Note: check gene name, gmt file format, or filtering size." ) sys . exit ( 0 ) self . gmtdct = genesets dict return genesets dict
def get libraries ( self , database = '' ) : lib url = 'http://amp.pharm.mssm.edu/%s Enrichr/dataset Statistics' % database libs json = json . loads ( requests . get ( lib url ) . text ) libs = [ lib [ 'library Name' ] for lib in libs json [ 'statistics' ] ] return sorted ( libs )
def download libraries ( self , libname ) : self . logger . info ( "Downloading and generating Enrichr library gene sets......" ) s = retry ( 5 ) # queery string ENRICHR URL = 'http://amp.pharm.mssm.edu/Enrichr/gene Set Library' query string = '?mode=text&library Name=%s' # get response = s . get ( ENRICHR URL + query string % libname , timeout = None ) if not response . ok : raise Exception ( 'Error fetching enrichment results, check internet connection first.' ) # reformat to dict and save to disk mkdirs ( DEFAULT CACHE PATH ) genesets dict = { } outname = "enrichr.%s.gmt" % libname gmtout = open ( os . path . join ( DEFAULT CACHE PATH , outname ) , "w" ) for line in response . iter lines ( chunk size = 1024 , decode unicode = 'utf-8' ) : line = line . strip ( ) k = line . split ( "\t" ) [ 0 ] v = list ( map ( lambda x : x . split ( "," ) [ 0 ] , line . split ( "\t" ) [ 2 : ] ) ) genesets dict . update ( { k : v } ) outline = "%s\t\t%s\n" % ( k , "\t" . join ( v ) ) gmtout . write ( outline ) gmtout . close ( ) return genesets dict
def heatmat ( self , df , classes , pheno pos , pheno neg ) : width = len ( classes ) if len ( classes ) >= 6 else 5 cls boo A = list ( map ( lambda x : True if x == pheno pos else False , classes ) ) cls boo B = list ( map ( lambda x : True if x == pheno neg else False , classes ) ) dat A = df . loc [ : , cls boo A ] dat B = df . loc [ : , cls boo B ] dat AB = pd . concat ( [ dat A , dat B ] , axis = 1 ) self . width = width self . heatmat = dat AB return
def save results ( self , zipdata , outdir , module , gmt , rank metric , permutation type ) : res = Ordered Dict ( ) for gs , gseale , ind , RES in zipdata : rdict = Ordered Dict ( ) rdict [ 'es' ] = gseale [ 0 ] rdict [ 'nes' ] = gseale [ 1 ] rdict [ 'pval' ] = gseale [ 2 ] rdict [ 'fdr' ] = gseale [ 3 ] rdict [ 'geneset size' ] = len ( gmt [ gs ] ) rdict [ 'matched size' ] = len ( ind ) #reformat gene list. genes = rank metric . index . values [ ind ] rdict [ 'genes' ] = ";" . join ( [ str ( g ) . strip ( ) for g in genes ] ) if self . module != 'ssgsea' : # extract leading edge genes if rdict [ 'es' ] > 0 : # RES -> ndarray, ind -> list idx = RES . argmax ( ) ldg pos = list ( filter ( lambda x : x <= idx , ind ) ) elif rdict [ 'es' ] < 0 : idx = RES . argmin ( ) ldg pos = list ( filter ( lambda x : x >= idx , ind ) ) else : ldg pos = ind # es == 0 ? rdict [ 'ledge genes' ] = ';' . join ( list ( map ( str , rank metric . iloc [ ldg pos ] . index ) ) ) rdict [ 'RES' ] = RES rdict [ 'hits indices' ] = ind # save to one odict res [ gs ] = rdict # save self . results = res # save to dataframe res df = pd . Data Frame . from dict ( res , orient = 'index' ) res df . index . name = 'Term' res df . drop ( [ 'RES' , 'hits indices' ] , axis = 1 , inplace = True ) res df . sort values ( by = [ 'fdr' , 'pval' ] , inplace = True ) self . res2d = res df if self . outdir is None : return out = os . path . join ( outdir , 'gseapy.{b}.{c}.report.csv' . format ( b = module , c = permutation type ) ) if self . module == 'ssgsea' : out = out . replace ( ".csv" , ".txt" ) with open ( out , 'a' ) as f : f . write ( '# normalize enrichment scores by random permutation procedure (GSEA method)\n' ) f . write ( "# might not proper for publication\n" ) res df . to csv ( f , sep = '\t' ) else : res df . to csv ( out ) return
def load data ( self , cls vec ) : # read data in if isinstance ( self . data , pd . Data Frame ) : exprs = self . data . copy ( ) # handle index is gene names if exprs . index . dtype == 'O' : exprs = exprs . reset index ( ) elif os . path . isfile ( self . data ) : # GCT input format? if self . data . endswith ( "gct" ) : exprs = pd . read csv ( self . data , skiprows = 1 , comment = '#' , sep = "\t" ) else : exprs = pd . read csv ( self . data , comment = '#' , sep = "\t" ) else : raise Exception ( 'Error parsing gene expression Data Frame!' ) #drop duplicated gene names if exprs . iloc [ : , 0 ] . duplicated ( ) . sum ( ) > 0 : self . logger . warning ( "Warning: dropping duplicated gene names, only keep the first values" ) exprs . drop duplicates ( subset = exprs . columns [ 0 ] , inplace = True ) #drop duplicate gene names. if exprs . isnull ( ) . any ( ) . sum ( ) > 0 : self . logger . warning ( "Warning: Input data contains NA, filled NA with 0" ) exprs . dropna ( how = 'all' , inplace = True ) #drop rows with all N As exprs = exprs . fillna ( 0 ) # set gene name as index exprs . set index ( keys = exprs . columns [ 0 ] , inplace = True ) # select numberic columns df = exprs . select dtypes ( include = [ np . number ] ) # drop any genes which std ==0 df std = df . groupby ( by = cls vec , axis = 1 ) . std ( ) df = df [ ~ df std . isin ( [ 0 ] ) . any ( axis = 1 ) ] df = df + 0.00001 # we don't like zeros!!! return df
def run Samples Permu ( self , df , gmt = None ) : assert self . min size <= self . max size mkdirs ( self . outdir ) self . results On Samples = Ordered Dict ( ) outdir = self . outdir # iter throught each sample for name , ser in df . iteritems ( ) : self . outdir = os . path . join ( outdir , str ( name ) ) self . logger . info ( "Run Sample: %s " % name ) mkdirs ( self . outdir ) # sort ranking values from high to low or reverse dat2 = ser . sort values ( ascending = self . ascending ) # reset integer index, or caused unwanted problems # df.reset index(drop=True, inplace=True) # compute ES, NES, pval, FDR, RES gsea results , hit ind , rank ES , subsets = gsea compute ( data = dat2 , n = self . permutation num , gmt = gmt , weighted score type = self . weighted score type , permutation type = 'gene set' , method = None , pheno pos = '' , pheno neg = '' , classes = None , ascending = self . ascending , processes = self . processes , seed = self . seed , single = True , scale = self . scale ) # write file res zip = zip ( subsets , list ( gsea results ) , hit ind , rank ES ) self . save results ( zipdata = res zip , outdir = self . outdir , module = self . module , gmt = gmt , rank metric = dat2 , permutation type = "gene sets" ) self . results On Samples [ name ] = self . res2d . es # plotting if self . noplot : continue self . logger . info ( "Plotting Sample: %s \n" % name ) self . plotting ( rank metric = dat2 , results = self . results , graph num = self . graph num , outdir = self . outdir , figsize = self . figsize , format = self . format ) # save es, nes to file self . save ( outdir ) return
def save ( self , outdir ) : # save raw ES to one csv file samples Raw ES = pd . Data Frame ( self . results On Samples ) samples Raw ES . index . name = 'Term|ES' # normalize enrichment scores by using the entire data set, as indicated # by Barbie et al., 2009, online methods, pg. 2 samples NES = samples Raw ES / ( samples Raw ES . values . max ( ) - samples Raw ES . values . min ( ) ) samples NES = samples NES . copy ( ) samples NES . index . rename ( 'Term|NES' , inplace = True ) self . res2d = samples NES self . logger . info ( "Congratulations. GSE Apy runs successfully................\n" ) if self . outdir is None : return # write es out E Sfile = os . path . join ( outdir , "gseapy.samples.raw.es.txt" ) with open ( out E Sfile , 'a' ) as f : if self . scale : f . write ( '# scale the enrichment scores by number of genes in the gene sets\n' ) f . write ( '# this normalization has not effects on the final NES, ' + 'as indicated by Barbie et al., 2009, online methods, pg. 2\n' ) else : f . write ( '# raw enrichment scores of all data\n' ) f . write ( '# no scale es by numbers of genes in the gene sets\n' ) samples Raw ES . to csv ( f , sep = '\t' ) out NE Sfile = os . path . join ( outdir , "gseapy.samples.normalized.es.txt" ) with open ( out NE Sfile , 'a' ) as f : f . write ( '# normalize enrichment scores by using the entire data set\n' ) f . write ( '# as indicated by Barbie et al., 2009, online methods, pg. 2\n' ) samples NES . to csv ( f , sep = '\t' ) return
def prepare outdir ( self ) : self . outdir = self . outdir if self . outdir is None : self . tmpdir = Temporary Directory ( ) self . outdir = self . tmpdir . name elif isinstance ( self . outdir , str ) : mkdirs ( self . outdir ) else : raise Exception ( "Error parsing outdir: %s" % type ( self . outdir ) ) # handle gene sets logfile = os . path . join ( self . outdir , "gseapy.%s.%s.log" % ( self . module , self . descriptions ) ) return logfile
def parse genesets ( self ) : enrichr library = self . get libraries ( ) if isinstance ( self . gene sets , list ) : gss = self . gene sets elif isinstance ( self . gene sets , str ) : gss = [ g . strip ( ) for g in self . gene sets . strip ( ) . split ( "," ) ] elif isinstance ( self . gene sets , dict ) : gss = [ self . gene sets ] else : raise Exception ( "Error parsing enrichr libraries, please provided corrected one" ) # gss: a list contain .gmt, dict, enrichr liraries. # now, convert .gmt to dict gss exist = [ ] for g in gss : if isinstance ( g , dict ) : gss exist . append ( g ) continue if isinstance ( g , str ) : if g in enrichr library : gss exist . append ( g ) continue if g . lower ( ) . endswith ( ".gmt" ) and os . path . exists ( g ) : self . logger . info ( "User Defined gene sets is given: %s" % g ) with open ( g ) as genesets : g dict = { line . strip ( ) . split ( "\t" ) [ 0 ] : line . strip ( ) . split ( "\t" ) [ 2 : ] for line in genesets . readlines ( ) } gss exist . append ( g dict ) return gss exist
def send genes ( self , gene list , url ) : payload = { 'list' : ( None , gene list ) , 'description' : ( None , self . descriptions ) } # response response = requests . post ( url , files = payload ) if not response . ok : raise Exception ( 'Error analyzing gene list' ) sleep ( 1 ) job id = json . loads ( response . text ) return job id
def check genes ( self , gene list , usr list id ) : response = requests . get ( 'http://amp.pharm.mssm.edu/Enrichr/view?user List Id=%s' % usr list id ) if not response . ok : raise Exception ( 'Error getting gene list back' ) returned L = json . loads ( response . text ) [ "genes" ] returned N = sum ( [ 1 for gene in gene list if gene in returned L ] ) self . logger . info ( '{} genes successfully recognized by Enrichr' . format ( returned N ) )
def run ( self ) : # set organism self . get organism ( ) # read input file genes list = self . parse genelists ( ) gss = self . parse genesets ( ) # if gmt self . logger . info ( "Connecting to Enrichr Server to get latest library names" ) if len ( gss ) < 1 : sys . stderr . write ( "Not validated Enrichr library name provided\n" ) sys . stdout . write ( "Hint: use get library name() to view full list of supported names" ) sys . exit ( 1 ) self . results = pd . Data Frame ( ) for g in gss : if isinstance ( g , dict ) : ## local mode res = self . enrich ( g ) short ID , self . gs = str ( id ( g ) ) , "CUSTOM%s" % id ( g ) if res is None : self . logger . info ( "No hits return, for gene set: Custom%s" % short ID ) continue else : ## online mode self . gs = str ( g ) self . logger . debug ( "Start Enrichr using library: %s" % ( self . gs ) ) self . logger . info ( 'Analysis name: %s, Enrichr Library: %s' % ( self . descriptions , self . gs ) ) short ID , res = self . get results ( genes list ) # Remember gene set library used res . insert ( 0 , "Gene set" , self . gs ) # Append to master dataframe self . results = self . results . append ( res , ignore index = True , sort = True ) self . res2d = res if self . outdir is None : continue self . logger . info ( 'Save file of enrichment results: Job Id:' + str ( short ID ) ) outfile = "%s/%s.%s.%s.reports.txt" % ( self . outdir , self . gs , self . descriptions , self . module ) self . res2d . to csv ( outfile , index = False , encoding = 'utf-8' , sep = "\t" ) # plotting if not self . no plot : msg = barplot ( df = res , cutoff = self . cutoff , figsize = self . figsize , top term = self . top term , color = 'salmon' , title = self . gs , ofname = outfile . replace ( "txt" , self . format ) ) if msg is not None : self . logger . warning ( msg ) self . logger . info ( 'Done.\n' ) # clean up tmpdir if self . outdir is None : self . tmpdir . cleanup ( ) return
def annulus hires ( script , radius = None , radius1 = None , radius2 = None , diameter = None , diameter1 = None , diameter2 = None , cir segments = 48 , rad segments = 1 , color = None ) : if radius is not None and diameter is None : if radius1 is None and diameter1 is None : radius1 = radius if radius2 is None and diameter2 is None : radius2 = 0 if diameter is not None : if radius1 is None and diameter1 is None : radius1 = diameter / 2 if radius2 is None and diameter2 is None : radius2 = 0 if diameter1 is not None : radius1 = diameter1 / 2 if diameter2 is not None : radius2 = diameter2 / 2 if radius1 is None : radius1 = 1 if radius2 is None : radius2 = 0 ring = ( radius1 - radius2 ) / rad segments for i in range ( 0 , rad segments ) : annulus ( script , radius1 = radius1 - i * ring , radius2 = radius1 - ( i + 1 ) * ring , cir segments = cir segments ) layers . join ( script , merge vert = True ) if color is not None : vert color . function ( script , color = color ) return None
def tube hires ( script , height = 1.0 , radius = None , radius1 = None , radius2 = None , diameter = None , diameter1 = None , diameter2 = None , cir segments = 32 , rad segments = 1 , height segments = 1 , center = False , simple bottom = False , color = None ) : # TODO: add option to round the top of the cylinder, i.e. deform spherically # TODO: add warnings if values are ignored, e.g. if you specify both radius # and diameter. if radius is not None and diameter is None : if radius1 is None and diameter1 is None : radius1 = radius if radius2 is None and diameter2 is None : radius2 = 0 if diameter is not None : if radius1 is None and diameter1 is None : radius1 = diameter / 2 if radius2 is None and diameter2 is None : radius2 = 0 if diameter1 is not None : radius1 = diameter1 / 2 if diameter2 is not None : radius2 = diameter2 / 2 if radius1 is None : radius1 = 1 if radius2 is None : radius2 = 0 # Create top annulus hires ( script , radius1 = radius1 , radius2 = radius2 , cir segments = cir segments , rad segments = rad segments ) transform . translate ( script , [ 0 , 0 , height ] ) # Create bottom if simple bottom : annulus ( script , radius1 = radius1 , radius2 = radius2 , cir segments = cir segments ) else : layers . duplicate ( script ) transform . translate ( script , [ 0 , 0 , - height ] ) # Rotate to correct normals transform . rotate ( script , 'x' , 180 ) # Create outer tube cylinder open hires ( script , height , radius1 , cir segments = cir segments , height segments = height segments ) # Create inner tube if radius2 != 0 : cylinder open hires ( script , height , radius2 , cir segments = cir segments , height segments = height segments , invert normals = True ) # Join everything together layers . join ( script ) # Need some tolerance on merge vert due to rounding errors clean . merge vert ( script , threshold = 0.00002 ) if center : transform . translate ( script , [ 0 , 0 , - height / 2 ] ) if color is not None : vert color . function ( script , color = color ) return None
def save to file ( self , script file ) : # TODO: rasie exception here instead? if not self . filters : print ( 'WARNING: no filters to save to file!' ) script file descriptor = open ( script file , 'w' ) script file descriptor . write ( '' . join ( self . opening + self . filters + self . closing ) ) script file descriptor . close ( )
def per triangle ( script , sidedim = 0 , textdim = 1024 , border = 2 , method = 1 ) : filter xml = '' . join ( [ '  <filter name="Parametrization: Trivial Per-Triangle ">\n' , '    <Param name="sidedim"' , 'value="%d"' % sidedim , 'description="Quads per line"' , 'type="Rich Int"' , 'tooltip="Indicates how many triangles have to be put on each line (every quad contains two triangles). Leave 0 for automatic calculation"' , '/>\n' , '    <Param name="textdim"' , 'value="%d"' % textdim , 'description="Texture Dimension (px)"' , 'type="Rich Int"' , 'tooltip="Gives an indication on how big the texture is"' , '/>\n' , '    <Param name="border"' , 'value="%d"' % border , 'description="Inter-Triangle border (px)"' , 'type="Rich Int"' , 'tooltip="Specifies how many pixels to be left between triangles in parametrization domain"' , '/>\n' , '    <Param name="method"' , 'value="%d"' % method , 'description="Method"' , 'enum val0="Basic"' , 'enum val1="Space-optimizing"' , 'enum cardinality="2"' , 'type="Rich Enum"' , 'tooltip="Choose space optimizing to map smaller faces into smaller triangles in parametrizazion domain"' '/>\n' , '  </filter>\n' ] ) util . write filter ( script , filter xml ) return None
def v multiply ( scalar , v1 ) : vector = [ ] for i , x in enumerate ( v1 ) : vector . append ( '(({})*({}))' . format ( scalar , v1 [ i ] ) ) return vector
def measure all ( fbasename = None , log = None , ml version = ml version ) : ml script1 file = 'TEMP3D measure g And T.mlx' if ml version == '1.3.4BETA' : file out = 'TEMP3D aabb.xyz' else : file out = None ml script1 = mlx . Filter Script ( file in = fbasename , file out = file out , ml version = ml version ) compute . measure geometry ( ml script1 ) compute . measure topology ( ml script1 ) ml script1 . save to file ( ml script1 file ) ml script1 . run script ( log = log , script file = ml script1 file ) geometry = ml script1 . geometry topology = ml script1 . topology if ml version == '1.3.4BETA' : if log is not None : log file = open ( log , 'a' ) log file . write ( '***Axis Aligned Bounding Results for file "%s":\n' % fbasename ) log file . close ( ) aabb = measure aabb ( file out , log ) else : aabb = geometry [ 'aabb' ] return aabb , geometry , topology
def measure dimension ( fbasename = None , log = None , axis1 = None , offset1 = 0.0 , axis2 = None , offset2 = 0.0 , ml version = ml version ) : axis1 = axis1 . lower ( ) axis2 = axis2 . lower ( ) ml script1 file = 'TEMP3D measure dimension.mlx' file out = 'TEMP3D measure dimension.xyz' ml script1 = mlx . Filter Script ( file in = fbasename , file out = file out , ml version = ml version ) compute . section ( ml script1 , axis1 , offset1 , surface = True ) compute . section ( ml script1 , axis2 , offset2 , surface = False ) layers . delete lower ( ml script1 ) ml script1 . save to file ( ml script1 file ) ml script1 . run script ( log = log , script file = ml script1 file ) for val in ( 'x' , 'y' , 'z' ) : if val not in ( axis1 , axis2 ) : axis = val # ord: Get number that represents letter in ASCII # Here we find the offset from 'x' to determine the list reference # i.e. 0 for x, 1 for y, 2 for z axis num = ord ( axis ) - ord ( 'x' ) aabb = measure aabb ( file out , log ) dimension = { 'min' : aabb [ 'min' ] [ axis num ] , 'max' : aabb [ 'max' ] [ axis num ] , 'length' : aabb [ 'size' ] [ axis num ] , 'axis' : axis } if log is None : print ( '\n For file "%s"' % fbasename ) print ( 'Dimension parallel to %s with %s=%s & %s=%s:' % ( axis , axis1 , offset1 , axis2 , offset2 ) ) print ( '  Min = %s, Max = %s, Total length = %s' % ( dimension [ 'min' ] , dimension [ 'max' ] , dimension [ 'length' ] ) ) else : log file = open ( log , 'a' ) log file . write ( '\n For file "%s"\n' % fbasename ) log file . write ( 'Dimension parallel to %s with %s=%s & %s=%s:\n' % ( axis , axis1 , offset1 , axis2 , offset2 ) ) log file . write ( 'min = %s\n' % dimension [ 'min' ] ) log file . write ( 'max = %s\n' % dimension [ 'max' ] ) log file . write ( 'Total length = %s\n' % dimension [ 'length' ] ) log file . close ( ) return dimension
def get vprof version ( filename ) : with open ( filename ) as src file : version match = re . search ( r"^ version  = ['\"]([^'\"]*)['\"]" , src file . read ( ) , re . M ) if version match : return version match . group ( 1 ) raise Runtime Error ( 'Unable to find version info.' )
def get obj count difference ( objs1 , objs2 ) : clean obj list1 = process in memory objects ( objs1 ) clean obj list2 = process in memory objects ( objs2 ) obj count 1 = get object count by type ( clean obj list1 ) obj count 2 = get object count by type ( clean obj list2 ) return obj count 1 - obj count 2
def format obj count ( objects ) : result = [ ] regex = re . compile ( r'<(?P<type>\w+) \'(?P<name>\S+)\'>' ) for obj type , obj count in objects . items ( ) : if obj count != 0 : match = re . findall ( regex , repr ( obj type ) ) if match : obj type , obj name = match [ 0 ] result . append ( ( "%s %s" % ( obj type , obj name ) , obj count ) ) return sorted ( result , key = operator . itemgetter ( 1 ) , reverse = True )
def trace memory usage ( self , frame , event , arg ) : #pylint: disable=unused-argument if event == 'line' and frame . f code . co filename in self . target modules : self . events list . append ( ( frame . f lineno , self . process . memory info ( ) . rss , frame . f code . co name , frame . f code . co filename ) ) return self . trace memory usage
def code events ( self ) : if self . resulting events : return self . resulting events for i , ( lineno , mem , func , fname ) in enumerate ( self . events list ) : mem in mb = float ( mem - self . mem overhead ) / BYTES IN MB if ( self . resulting events and self . resulting events [ - 1 ] [ 0 ] == lineno and self . resulting events [ - 1 ] [ 2 ] == func and self . resulting events [ - 1 ] [ 3 ] == fname and self . resulting events [ - 1 ] [ 1 ] < mem in mb ) : self . resulting events [ - 1 ] [ 1 ] = mem in mb else : self . resulting events . append ( [ i + 1 , lineno , mem in mb , func , fname ] ) return self . resulting events
def compute mem overhead ( self ) : self . mem overhead = ( self . process . memory info ( ) . rss - builtins . initial rss size )
def profile package ( self ) : target modules = base profiler . get pkg module names ( self . run object ) try : with Code Events Tracker ( target modules ) as prof : prof . compute mem overhead ( ) runpy . run path ( self . run object , run name = ' main ' ) except System Exit : pass return prof , None
def profile module ( self ) : target modules = { self . run object } try : with open ( self . run object , 'rb' ) as srcfile , Code Events Tracker ( target modules ) as prof : code = compile ( srcfile . read ( ) , self . run object , 'exec' ) prof . compute mem overhead ( ) exec ( code , self . globs , None ) except System Exit : pass return prof , None
def profile function ( self ) : target modules = { self . run object . code . co filename } with Code Events Tracker ( target modules ) as prof : prof . compute mem overhead ( ) result = self . run object ( * self . run args , * * self . run kwargs ) return prof , result
def run ( self ) : existing objects = get in memory objects ( ) prof , result = self . profile ( ) new objects = get in memory objects ( ) new obj count = get obj count difference ( new objects , existing objects ) result obj count = new obj count - prof . obj overhead # existing objects list is also profiler overhead result obj count [ list ] -= 1 pretty obj count = format obj count ( result obj count ) return { 'object Name' : self . object name , 'code Events' : prof . code events , 'total Events' : len ( prof . code events ) , 'objects Count' : pretty obj count , 'result' : result , 'timestamp' : int ( time . time ( ) ) }
def get run object type ( run object ) : if isinstance ( run object , tuple ) : return 'function' run object , , = run object . partition ( ' ' ) if os . path . isdir ( run object ) : return 'package' return 'module'
def init module ( self , run object ) : self . profile = self . profile module self . run object , , self . run args = run object . partition ( ' ' ) self . object name = '%s (module)' % self . run object self . globs = { ' file ' : self . run object , ' name ' : ' main ' , ' package ' : None , } program path = os . path . dirname ( self . run object ) if sys . path [ 0 ] != program path : sys . path . insert ( 0 , program path ) self . replace sysargs ( )
def init package ( self , run object ) : self . profile = self . profile package self . run object , , self . run args = run object . partition ( ' ' ) self . object name = '%s (package)' % self . run object self . replace sysargs ( )
def init function ( self , run object ) : self . profile = self . profile function self . run object , self . run args , self . run kwargs = run object filename = inspect . getsourcefile ( self . run object ) self . object name = '%s @ %s (function)' % ( self . run object . name , filename )
def replace sysargs ( self ) : sys . argv [ : ] = [ self . run object ] if self . run args : sys . argv += self . run args . split ( )
def fill sample count ( self , node ) : node [ 'sample Count' ] += sum ( self . fill sample count ( child ) for child in node [ 'children' ] ) return node [ 'sample Count' ]
def format tree ( self , node , total samples ) : funcname , filename , = node [ 'stack' ] sample percent = self . get percentage ( node [ 'sample Count' ] , total samples ) color hash = base profiler . hash name ( '%s @ %s' % ( funcname , filename ) ) return { 'stack' : node [ 'stack' ] , 'children' : [ self . format tree ( child , total samples ) for child in node [ 'children' ] ] , 'sample Count' : node [ 'sample Count' ] , 'sample Percentage' : sample percent , 'color Hash' : color hash }
def call tree ( self ) : call tree = { 'stack' : 'base' , 'sample Count' : 0 , 'children' : [ ] } for stack , sample count in self . stats . items ( ) : self . insert stack ( reversed ( stack ) , sample count , call tree ) self . fill sample count ( call tree ) if not call tree [ 'children' ] : return { } return self . format tree ( call tree [ 'children' ] [ 0 ] , call tree [ 'sample Count' ] )
def profile package ( self ) : with Stat Profiler ( ) as prof : prof . base frame = inspect . currentframe ( ) try : runpy . run path ( self . run object , run name = ' main ' ) except System Exit : pass call tree = prof . call tree return { 'object Name' : self . object name , 'sample Interval' : SAMPLE INTERVAL , 'run Time' : prof . run time , 'call Stats' : call tree , 'total Samples' : call tree . get ( 'sample Count' , 0 ) , 'timestamp' : int ( time . time ( ) ) }
def profile module ( self ) : with open ( self . run object , 'rb' ) as srcfile , Stat Profiler ( ) as prof : code = compile ( srcfile . read ( ) , self . run object , 'exec' ) prof . base frame = inspect . currentframe ( ) try : exec ( code , self . globs , None ) except System Exit : pass call tree = prof . call tree return { 'object Name' : self . object name , 'sample Interval' : SAMPLE INTERVAL , 'run Time' : prof . run time , 'call Stats' : call tree , 'total Samples' : call tree . get ( 'sample Count' , 0 ) , 'timestamp' : int ( time . time ( ) ) }
def profile function ( self ) : with Stat Profiler ( ) as prof : result = self . run object ( * self . run args , * * self . run kwargs ) call tree = prof . call tree return { 'object Name' : self . object name , 'sample Interval' : SAMPLE INTERVAL , 'run Time' : prof . run time , 'call Stats' : call tree , 'total Samples' : call tree . get ( 'sample Count' , 0 ) , 'result' : result , 'timestamp' : int ( time . time ( ) ) }
def transform stats ( prof ) : records = [ ] for info , params in prof . stats . items ( ) : filename , lineno , funcname = info cum calls , num calls , time per call , cum time , = params if prof . total tt == 0 : percentage = 0 else : percentage = round ( 100 * ( cum time / prof . total tt ) , 4 ) cum time = round ( cum time , 4 ) func name = '%s @ %s' % ( funcname , filename ) color hash = base profiler . hash name ( func name ) records . append ( ( filename , lineno , funcname , cum time , percentage , num calls , cum calls , time per call , filename , color hash ) ) return sorted ( records , key = operator . itemgetter ( 4 ) , reverse = True )
def profile package ( self ) : prof = c Profile . Profile ( ) prof . enable ( ) try : runpy . run path ( self . run object , run name = ' main ' ) except System Exit : pass prof . disable ( ) prof stats = pstats . Stats ( prof ) prof stats . calc callees ( ) return { 'object Name' : self . object name , 'call Stats' : self . transform stats ( prof stats ) , 'total Time' : prof stats . total tt , 'primitive Calls' : prof stats . prim calls , 'total Calls' : prof stats . total calls , 'timestamp' : int ( time . time ( ) ) }
def profile module ( self ) : prof = c Profile . Profile ( ) try : with open ( self . run object , 'rb' ) as srcfile : code = compile ( srcfile . read ( ) , self . run object , 'exec' ) prof . runctx ( code , self . globs , None ) except System Exit : pass prof stats = pstats . Stats ( prof ) prof stats . calc callees ( ) return { 'object Name' : self . object name , 'call Stats' : self . transform stats ( prof stats ) , 'total Time' : prof stats . total tt , 'primitive Calls' : prof stats . prim calls , 'total Calls' : prof stats . total calls , 'timestamp' : int ( time . time ( ) ) }
def profile function ( self ) : prof = c Profile . Profile ( ) prof . enable ( ) result = self . run object ( * self . run args , * * self . run kwargs ) prof . disable ( ) prof stats = pstats . Stats ( prof ) prof stats . calc callees ( ) return { 'object Name' : self . object name , 'call Stats' : self . transform stats ( prof stats ) , 'total Time' : prof stats . total tt , 'primitive Calls' : prof stats . prim calls , 'total Calls' : prof stats . total calls , 'result' : result , 'timestamp' : int ( time . time ( ) ) }
def show guestbook ( ) : cursor = flask . g . db . execute ( 'SELECT name, message FROM entry ORDER BY id DESC;' ) entries = [ { 'name' : row [ 0 ] , 'message' : row [ 1 ] } for row in cursor . fetchall ( ) ] return jinja2 . Template ( LAYOUT ) . render ( entries = entries )
def add entry ( ) : name , msg = flask . request . form [ 'name' ] , flask . request . form [ 'message' ] flask . g . db . execute ( 'INSERT INTO entry (name, message) VALUES (?, ?)' , ( name , msg ) ) flask . g . db . commit ( ) return flask . redirect ( '/' )
def handle root ( ) : res filename = os . path . join ( os . path . dirname ( file ) , PROFILE HTML ) with io . open ( res filename , 'rb' ) as res file : content = res file . read ( ) return content , 'text/html'
def handle other ( self ) : res filename = os . path . join ( os . path . dirname ( file ) , STATIC DIR , self . path [ 1 : ] ) with io . open ( res filename , 'rb' ) as res file : content = res file . read ( ) , extension = os . path . splitext ( self . path ) return content , 'text/%s' % extension [ 1 : ]
def do GET ( self ) : handler = self . uri map . get ( self . path ) or self . handle other content , content type = handler ( ) compressed content = gzip . compress ( content ) self . send response ( 200 , headers = ( ( 'Content-type' , '%s; charset=utf-8' % content type ) , ( 'Content-Encoding' , 'gzip' ) , ( 'Content-Length' , len ( compressed content ) ) ) ) self . wfile . write ( compressed content )
def do POST ( self ) : post data = self . rfile . read ( int ( self . headers [ 'Content-Length' ] ) ) json data = gzip . decompress ( post data ) self . profile json . update ( json . loads ( json data . decode ( 'utf-8' ) ) ) self . send response ( 200 , headers = ( ( 'Content-type' , '%s; charset=utf-8' % 'text/json' ) , ( 'Content-Encoding' , 'gzip' ) , ( 'Content-Length' , len ( post data ) ) ) )
def send response ( self , http code , message = None , headers = None ) : self . send response ( http code , message ) if headers : for header in headers : self . send header ( * header ) self . end headers ( )
def main ( ) : parser = argparse . Argument Parser ( prog = PROGRAN NAME , description = MODULE DESC , formatter class = argparse . Raw Text Help Formatter ) launch modes = parser . add mutually exclusive group ( required = True ) launch modes . add argument ( '-r' , '--remote' , dest = 'remote' , action = 'store true' , default = False , help = 'launch in remote mode' ) launch modes . add argument ( '-i' , '--input-file' , dest = 'input file' , type = str , default = '' , help = 'render UI from file' ) launch modes . add argument ( '-c' , '--config' , nargs = 2 , dest = 'config' , help = CONFIG DESC , metavar = ( 'CONFIG' , 'SRC' ) ) parser . add argument ( '-H' , '--host' , dest = 'host' , default = HOST , type = str , help = 'set internal webserver host' ) parser . add argument ( '-p' , '--port' , dest = 'port' , default = PORT , type = int , help = 'set internal webserver port' ) parser . add argument ( '-n' , '--no-browser' , dest = 'dont start browser' , action = 'store true' , default = False , help = "don't start browser automatically" ) parser . add argument ( '-o' , '--output-file' , dest = 'output file' , type = str , default = '' , help = 'save profile to file' ) parser . add argument ( '--debug' , dest = 'debug mode' , action = 'store true' , default = False , help = "don't suppress error messages" ) parser . add argument ( '--version' , action = 'version' , version = 'vprof %s' % version ) args = parser . parse args ( ) # Render UI from file. if args . input file : with open ( args . input file ) as ifile : saved stats = json . loads ( ifile . read ( ) ) if saved stats [ 'version' ] != version : print ( 'Incorrect profiler version - %s. %s is required.' % ( saved stats [ 'version' ] , version ) ) sys . exit ( ERR CODES [ 'input file error' ] ) stats server . start ( args . host , args . port , saved stats , args . dont start browser , args . debug mode ) # Launch in remote mode. elif args . remote : stats server . start ( args . host , args . port , { } , args . dont start browser , args . debug mode ) # Profiler mode. else : config , source = args . config try : program stats = runner . run profilers ( source , config , verbose = True ) except runner . Ambiguous Configuration Error : print ( 'Profiler configuration %s is ambiguous. ' 'Please, remove duplicates.' % config ) sys . exit ( ERR CODES [ 'ambiguous configuration' ] ) except runner . Bad Option Error as exc : print ( exc ) sys . exit ( ERR CODES [ 'bad option' ] ) if args . output file : with open ( args . output file , 'w' ) as outfile : program stats [ 'version' ] = version outfile . write ( json . dumps ( program stats , indent = 2 ) ) else : stats server . start ( args . host , args . port , program stats , args . dont start browser , args . debug mode )
def check standard dir ( module path ) : if 'site-packages' in module path : return True for stdlib path in STDLIB PATHS : if fnmatch . fnmatchcase ( module path , stdlib path + '*' ) : return True return False
def record line ( self , frame , event , arg ) : # pylint: disable=unused-argument if event == 'line' : if self . prev timestamp : runtime = time . time ( ) - self . prev timestamp self . lines . append ( [ self . prev path , self . prev lineno , runtime ] ) self . prev lineno = frame . f lineno self . prev path = frame . f code . co filename self . prev timestamp = time . time ( ) return self . record line
def lines without stdlib ( self ) : prev line = None current module path = inspect . getabsfile ( inspect . currentframe ( ) ) for module path , lineno , runtime in self . lines : module abspath = os . path . abspath ( module path ) if not prev line : prev line = [ module abspath , lineno , runtime ] else : if ( not check standard dir ( module path ) and module abspath != current module path ) : yield prev line prev line = [ module abspath , lineno , runtime ] else : prev line [ 2 ] += runtime yield prev line
def fill heatmap ( self ) : for module path , lineno , runtime in self . lines without stdlib : self . execution count [ module path ] [ lineno ] += 1 self . heatmap [ module path ] [ lineno ] += runtime
def skip lines ( src code , skip map ) : if not skip map : return [ [ 'line' , j + 1 , l ] for j , l in enumerate ( src code ) ] code with skips , i = [ ] , 0 for line , length in skip map : code with skips . extend ( [ 'line' , i + j + 1 , l ] for j , l in enumerate ( src code [ i : line ] ) ) if ( code with skips and code with skips [ - 1 ] [ 0 ] == 'skip' ) : # Merge skips. code with skips [ - 1 ] [ 1 ] += length else : code with skips . append ( [ 'skip' , length ] ) i = line + length code with skips . extend ( [ 'line' , i + j + 1 , l ] for j , l in enumerate ( src code [ i : ] ) ) return code with skips
def profile package ( self ) : with Code Heatmap Calculator ( ) as prof : try : runpy . run path ( self . run object , run name = ' main ' ) except System Exit : pass heatmaps = [ ] for filename , heatmap in prof . heatmap . items ( ) : if os . path . isfile ( filename ) : heatmaps . append ( self . format heatmap ( filename , heatmap , prof . execution count [ filename ] ) ) run time = sum ( heatmap [ 'run Time' ] for heatmap in heatmaps ) return { 'object Name' : self . run object , 'run Time' : run time , 'heatmaps' : heatmaps }
def format heatmap ( self , filename , heatmap , execution count ) : with open ( filename ) as src file : file source = src file . read ( ) . split ( '\n' ) skip map = self . calc skips ( heatmap , len ( file source ) ) run time = sum ( time for time in heatmap . values ( ) ) return { 'name' : filename , 'heatmap' : heatmap , 'execution Count' : execution count , 'src Code' : self . skip lines ( file source , skip map ) , 'run Time' : run time }
def profile module ( self ) : with open ( self . run object , 'r' ) as srcfile : src code = srcfile . read ( ) code = compile ( src code , self . run object , 'exec' ) try : with Code Heatmap Calculator ( ) as prof : exec ( code , self . globs , None ) except System Exit : pass heatmaps = [ ] for filename , heatmap in prof . heatmap . items ( ) : if os . path . isfile ( filename ) : heatmaps . append ( self . format heatmap ( filename , heatmap , prof . execution count [ filename ] ) ) run time = sum ( heatmap [ 'run Time' ] for heatmap in heatmaps ) return { 'object Name' : self . run object , 'run Time' : run time , 'heatmaps' : heatmaps }
def profile function ( self ) : with Code Heatmap Calculator ( ) as prof : result = self . run object ( * self . run args , * * self . run kwargs ) code lines , start line = inspect . getsourcelines ( self . run object ) source lines = [ ] for line in code lines : source lines . append ( ( 'line' , start line , line ) ) start line += 1 filename = os . path . abspath ( inspect . getsourcefile ( self . run object ) ) heatmap = prof . heatmap [ filename ] run time = sum ( time for time in heatmap . values ( ) ) return { 'object Name' : self . object name , 'run Time' : run time , 'result' : result , 'timestamp' : int ( time . time ( ) ) , 'heatmaps' : [ { 'name' : self . object name , 'heatmap' : heatmap , 'execution Count' : prof . execution count [ filename ] , 'src Code' : source lines , 'run Time' : run time } ] }
def count vocab ( self , analyzed docs ) : vocabulary = self . vocabulary j indices = make int array ( ) indptr = make int array ( ) indptr . append ( 0 ) for doc in analyzed docs : for feature in doc : try : j indices . append ( vocabulary [ feature ] ) except Key Error : # Ignore out-of-vocabulary items for fixed vocab=True continue indptr . append ( len ( j indices ) ) j indices = frombuffer empty ( j indices , dtype = np . intc ) indptr = np . frombuffer ( indptr , dtype = np . intc ) values = np . ones ( len ( j indices ) ) X = sp . csr matrix ( ( values , j indices , indptr ) , shape = ( len ( indptr ) - 1 , len ( vocabulary ) ) , dtype = self . dtype ) X . sum duplicates ( ) if self . binary : X . data . fill ( 1 ) return X
def to scikit ( self ) : scaler = Standard Scaler ( with mean = self . with mean , with std = self . with std , copy = self . copy ) scaler . dict = self . dict return scaler
def fit ( self , Z , parameter iterable ) : self . scorer = check scoring ( self . estimator , scoring = self . scoring ) cv = self . cv cv = check cv ( cv , Z ) if self . verbose > 0 : if isinstance ( parameter iterable , Sized ) : n candidates = len ( parameter iterable ) print ( "Fitting {0} folds for each of {1} candidates, totalling" " {2} fits" . format ( len ( cv ) , n candidates , n candidates * len ( cv ) ) ) base estimator = clone ( self . estimator ) pre dispatch = self . pre dispatch out = Parallel ( n jobs = self . n jobs , verbose = self . verbose , pre dispatch = pre dispatch , backend = "threading" ) ( delayed ( fit and score ) ( clone ( base estimator ) , Z , self . scorer , train , test , self . verbose , parameters , self . fit params , return parameters = True , error score = self . error score ) for parameters in parameter iterable for train , test in cv ) # Out is a list of triplet: score, estimator, n test samples n fits = len ( out ) n folds = len ( cv ) scores = list ( ) grid scores = list ( ) for grid start in range ( 0 , n fits , n folds ) : n test samples = 0 score = 0 all scores = [ ] for this score , this n test samples , , parameters in out [ grid start : grid start + n folds ] : all scores . append ( this score ) if self . iid : this score *= this n test samples n test samples += this n test samples score += this score if self . iid : score /= float ( n test samples ) else : score /= float ( n folds ) scores . append ( ( score , parameters ) ) # TODO: shall we also store the test fold sizes? grid scores . append ( CV Score Tuple ( parameters , score , np . array ( all scores ) ) ) # Store the computed scores self . grid scores = grid scores # Find the best parameters by comparing on the mean validation score: # note that `sorted` is deterministic in the way it breaks ties best = sorted ( grid scores , key = lambda x : x . mean validation score , reverse = True ) [ 0 ] self . best params = best . parameters self . best score = best . mean validation score if self . refit : # fit the best estimator using the entire dataset # clone first to work around broken estimators best estimator = clone ( base estimator ) . set params ( * * best . parameters ) best estimator . fit ( Z , * * self . fit params ) self . best estimator = best estimator return self
def score ( estimator , Z test , scorer ) : score = scorer ( estimator , Z test ) if not isinstance ( score , numbers . Number ) : raise Value Error ( "scoring must return a number, got %s (%s) instead." % ( str ( score ) , type ( score ) ) ) return score
def block collection ( iterator , dtype , bsize = - 1 ) : i = 0 accumulated = [ ] for a in iterator : if ( bsize > 0 ) and ( i >= bsize ) : yield pack accumulated ( accumulated , dtype ) accumulated = [ ] i = 0 accumulated . append ( a ) i += 1 if i > 0 : yield pack accumulated ( accumulated , dtype )
def block tuple ( iterator , dtypes , bsize = - 1 ) : i = 0 blocked tuple = None for tuple i in iterator : if blocked tuple is None : blocked tuple = tuple ( [ ] for in range ( len ( tuple i ) ) ) if ( bsize > 0 ) and ( i >= bsize ) : yield tuple ( pack accumulated ( x , dtype ) for x , dtype in zip ( blocked tuple , dtypes ) ) blocked tuple = tuple ( [ ] for in range ( len ( tuple i ) ) ) i = 0 for x j , x in zip ( tuple i , blocked tuple ) : x . append ( x j ) i += 1 if i > 0 : yield tuple ( pack accumulated ( x , dtype ) for x , dtype in zip ( blocked tuple , dtypes ) )
def shape ( self ) : # TODO cache first = self . first ( ) . shape shape = self . rdd . map ( lambda x : x . shape [ 0 ] ) . sum ( ) return ( shape , ) + first [ 1 : ]
def toarray ( self ) : rdd = self . rdd . map ( lambda x : x . toarray ( ) ) return np . concatenate ( rdd . collect ( ) )
def convert ( self , txn ) : ofxid = self . mk ofxid ( txn . id ) metadata = { } posting metadata = { "ofxid" : ofxid } if isinstance ( txn , Ofx Transaction ) : posting = Posting ( self . name , Amount ( txn . amount , self . currency ) , metadata = posting metadata ) return Transaction ( date = txn . date , payee = self . format payee ( txn ) , postings = [ posting , posting . clone inverted ( self . mk dynamic account ( self . format payee ( txn ) , exclude = self . name ) ) ] ) elif isinstance ( txn , Investment Transaction ) : acct1 = self . name acct2 = self . name posting1 = None posting2 = None security = self . maybe get ticker ( txn . security ) if isinstance ( txn . type , str ) : # recent versions of ofxparse if re . match ( '^(buy|sell)' , txn . type ) : acct2 = self . unknownaccount or 'Assets:Unknown' elif txn . type == 'transfer' : acct2 = 'Transfer' elif txn . type == 'reinvest' : # reinvestment of income # TODO: make this configurable acct2 = 'Income:Interest' elif txn . type == 'income' and txn . income type == 'DIV' : # Fidelity lists non-reinvested dividend income as # type: income, income type: DIV # TODO: determine how dividend income is listed from other institutions # income/DIV transactions do not involve buying or selling a security # so their postings need special handling compared to # others metadata [ 'dividend from' ] = security acct2 = 'Income:Dividends' posting1 = Posting ( acct1 , Amount ( txn . total , self . currency ) , metadata = posting metadata ) posting2 = posting1 . clone inverted ( acct2 ) else : # ??? pass else : # Old version of ofxparse if ( txn . type in [ 0 , 1 , 3 , 4 ] ) : # buymf, sellmf, buystock, sellstock acct2 = self . unknownaccount or 'Assets:Unknown' elif ( txn . type == 2 ) : # reinvest acct2 = 'Income:Interest' else : # ??? pass aux date = None if txn . settle Date is not None and txn . settle Date != txn . trade Date : aux date = txn . settle Date # income/DIV already defined above; # this block defines all other posting types if posting1 is None and posting2 is None : posting1 = Posting ( acct1 , Amount ( txn . units , security , unlimited = True ) , unit price = Amount ( txn . unit price , self . currency , unlimited = True ) , metadata = posting metadata ) posting2 = Posting ( acct2 , Amount ( txn . units * txn . unit price , self . currency , reverse = True ) ) else : # Previously defined if type:income income type/DIV pass return Transaction ( date = txn . trade Date , aux date = aux date , payee = self . format payee ( txn ) , metadata = metadata , postings = [ posting1 , posting2 ] )
def compatibility ( session , install ) : session . install ( '-e' , '.[dev]' ) session . install ( install ) run tests ( session )
def text width ( self , text : str ) -> float : width , = self . font . getsize ( text ) return width
def text width ( self , text : str ) -> float : width = 0 for index , c in enumerate ( text ) : width += self . char to width . get ( c , self . default character width ) width -= self . pair to kern . get ( text [ index : index + 2 ] , 0 ) return width
def default ( cls ) -> 'Precalculated Text Measurer' : if cls . default cache is not None : return cls . default cache if pkg resources . resource exists ( name , 'default-widths.json.xz' ) : import lzma with pkg resources . resource stream ( name , 'default-widths.json.xz' ) as f : with lzma . open ( f , "rt" ) as g : cls . default cache = Precalculated Text Measurer . from json ( cast ( Text IO , g ) ) return cls . default cache elif pkg resources . resource exists ( name , 'default-widths.json' ) : with pkg resources . resource stream ( name , 'default-widths.json' ) as f : cls . default cache = Precalculated Text Measurer . from json ( io . Text IO Wrapper ( f , encoding = 'utf-8' ) ) return cls . default cache else : raise Value Error ( 'could not load default-widths.json' )
def generate supported characters ( deja vu sans path : str ) -> Iterable [ str ] : font = tt Lib . TT Font ( deja vu sans path ) for cmap in font [ 'cmap' ] . tables : if cmap . is Unicode ( ) : for code in cmap . cmap : yield chr ( code )
def write json ( f : Text IO , deja vu sans path : str , measurer : text measurer . Text Measurer , encodings : Iterable [ str ] ) -> None : supported characters = list ( generate supported characters ( deja vu sans path ) ) kerning characters = '' . join ( generate encodeable characters ( supported characters , encodings ) ) char to length = calculate character to length mapping ( measurer , supported characters ) pair to kerning = calculate pair to kern mapping ( measurer , char to length , kerning characters ) json . dump ( { 'mean-character-length' : statistics . mean ( char to length . values ( ) ) , 'character-lengths' : char to length , 'kerning-characters' : kerning characters , 'kerning-pairs' : pair to kerning } , f , sort keys = True , indent = 1 )
def convolve gaussian 2d ( image , gaussian kernel 1d ) : result = scipy . ndimage . filters . correlate1d ( image , gaussian kernel 1d , axis = 0 ) result = scipy . ndimage . filters . correlate1d ( result , gaussian kernel 1d , axis = 1 ) return result
def get gaussian kernel ( gaussian kernel width = 11 , gaussian kernel sigma = 1.5 ) : # 1D Gaussian kernel definition gaussian kernel 1d = numpy . ndarray ( ( gaussian kernel width ) ) norm mu = int ( gaussian kernel width / 2 ) # Fill Gaussian kernel for i in range ( gaussian kernel width ) : gaussian kernel 1d [ i ] = ( exp ( - ( ( ( i - norm mu ) ** 2 ) ) / ( 2 * ( gaussian kernel sigma ** 2 ) ) ) ) return gaussian kernel 1d / numpy . sum ( gaussian kernel 1d )
def main ( ) : description = '\n' . join ( [ 'Compares an image with a list of images using the SSIM metric.' , '  Example:' , '    pyssim test-images/test1-1.png "test-images/*"' ] ) parser = argparse . Argument Parser ( prog = 'pyssim' , formatter class = argparse . Raw Text Help Formatter , description = description ) parser . add argument ( '--cw' , help = 'compute the complex wavelet SSIM' , action = 'store true' ) parser . add argument ( 'base image' , metavar = 'image1.png' , type = argparse . File Type ( 'r' ) ) parser . add argument ( 'comparison images' , metavar = 'image path with* or image2.png' ) parser . add argument ( '--width' , type = int , default = None , help = 'scales the image before computing SSIM' ) parser . add argument ( '--height' , type = int , default = None , help = 'scales the image before computing SSIM' ) args = parser . parse args ( ) if args . width and args . height : size = ( args . width , args . height ) else : size = None if not args . cw : gaussian kernel sigma = 1.5 gaussian kernel width = 11 gaussian kernel 1d = get gaussian kernel ( gaussian kernel width , gaussian kernel sigma ) comparison images = glob . glob ( args . comparison images ) is a single image = len ( comparison images ) == 1 for comparison image in comparison images : if args . cw : ssim = SSIM ( args . base image . name , size = size ) ssim value = ssim . cw ssim value ( comparison image ) else : ssim = SSIM ( args . base image . name , gaussian kernel 1d , size = size ) ssim value = ssim . ssim value ( comparison image ) if is a single image : sys . stdout . write ( '%.7g' % ssim value ) else : sys . stdout . write ( '%s - %s: %.7g' % ( args . base image . name , comparison image , ssim value ) ) sys . stdout . write ( '\n' )
def destroy ( self ) : if self . conf . auto Tick : self . destroying = True else : self . do Destroy ( )
def get Status ( self ) : status = { } status [ 'version' ] = VERSION status [ 'revision' ] = REVISION status [ 'self' ] = self . self Node status [ 'state' ] = self . raft State status [ 'leader' ] = self . raft Leader status [ 'partner nodes count' ] = len ( self . other Nodes ) for node in self . other Nodes : status [ 'partner node status server ' + node . id ] = 2 if node in self . connected Nodes else 0 status [ 'readonly nodes count' ] = len ( self . readonly Nodes ) for node in self . readonly Nodes : status [ 'readonly node status server ' + node . id ] = 2 if node in self . connected Nodes else 0 status [ 'log len' ] = len ( self . raft Log ) status [ 'last applied' ] = self . raft Last Applied status [ 'commit idx' ] = self . raft Commit Index status [ 'raft term' ] = self . raft Current Term status [ 'next node idx count' ] = len ( self . raft Next Index ) for node , idx in iteritems ( self . raft Next Index ) : status [ 'next node idx server ' + node . id ] = idx status [ 'match idx count' ] = len ( self . raft Match Index ) for node , idx in iteritems ( self . raft Match Index ) : status [ 'match idx server ' + node . id ] = idx status [ 'leader commit idx' ] = self . leader Commit Index status [ 'uptime' ] = int ( time . time ( ) - self . start Time ) status [ 'self code version' ] = self . self Code Version status [ 'enabled code version' ] = self . enabled Code Version return status
def print Status ( self ) : status = self . get Status ( ) for k , v in iteritems ( status ) : logging . info ( '%s: %s' % ( str ( k ) , str ( v ) ) )
def check ( func ) : def wrapped ( * args , * * kwargs ) : check name = func . name arg name = None if args : arg name = args [ 0 ] try : if arg name : logger . debug ( "Checking '%s' for '%s'" , check name , arg name ) else : logger . debug ( "Checking '%s'" , check name ) response = func ( * args , * * kwargs ) except Exception as e : message = str ( e ) response = { "ok" : False , "error" : message , "stacktrace" : traceback . format exc ( ) , } # The check contains several individual checks (e.g., one per # database). Preface the results by name. if arg name : response = { arg name : response } logger . exception ( "Error calling '%s' for '%s': %s" , check name , arg name , message ) else : logger . exception ( "Error calling '%s': %s" , check name , message ) return response return wrapped
def str to list ( s ) : list = s . split ( "," ) return list ( map ( lambda i : i . lstrip ( ) , list ) )
def cli parse ( file path , sa , nameservers , dns timeout , parallel = False ) : try : file results = parse report file ( file path , nameservers = nameservers , dns timeout = dns timeout , strip attachment payloads = sa , parallel = parallel ) except Parser Error as error : return error , file path finally : global counter with counter . get lock ( ) : counter . value += 1 return file results , file path
def main ( ) : def process reports ( reports ) : output str = "{0}\n" . format ( json . dumps ( reports , ensure ascii = False , indent = 2 ) ) if not opts . silent : print ( output str ) if opts . kafka hosts : try : ssl context = None if opts . kafka skip certificate verification : logger . debug ( "Skipping Kafka certificate verification" ) ssl context = create default context ( ) ssl context . check hostname = False ssl context . verify mode = CERT NONE kafka client = kafkaclient . Kafka Client ( opts . kafka hosts , username = opts . kafka username , password = opts . kafka password , ssl context = ssl context ) except Exception as error : logger . error ( "Kafka Error: {0}" . format ( error . str ( ) ) ) if opts . save aggregate : for report in reports [ "aggregate reports" ] : try : if opts . elasticsearch hosts : elastic . save aggregate report to elasticsearch ( report , index suffix = opts . elasticsearch index suffix , monthly indexes = opts . elasticsearch monthly indexes ) except elastic . Already Saved as warning : logger . warning ( warning . str ( ) ) except elastic . Elasticsearch Error as error : logger . error ( "Elasticsearch Error: {0}" . format ( error . str ( ) ) ) try : if opts . kafka hosts : kafka client . save aggregate reports to kafka ( report , kafka aggregate topic ) except Exception as error : logger . error ( "Kafka Error: {0}" . format ( error . str ( ) ) ) if opts . hec : try : aggregate reports = reports [ "aggregate reports" ] if len ( aggregate reports ) > 0 : hec client . save aggregate reports to splunk ( aggregate reports ) except splunk . Splunk Error as e : logger . error ( "Splunk HEC error: {0}" . format ( e . str ( ) ) ) if opts . save forensic : for report in reports [ "forensic reports" ] : try : if opts . elasticsearch hosts : elastic . save forensic report to elasticsearch ( report , index suffix = opts . elasticsearch index suffix , monthly indexes = opts . elasticsearch monthly indexes ) except elastic . Already Saved as warning : logger . warning ( warning . str ( ) ) except elastic . Elasticsearch Error as error : logger . error ( "Elasticsearch Error: {0}" . format ( error . str ( ) ) ) except Invalid DMARC Report as error : logger . error ( error . str ( ) ) try : if opts . kafka hosts : kafka client . save forensic reports to kafka ( report , kafka forensic topic ) except Exception as error : logger . error ( "Kafka Error: {0}" . format ( error . str ( ) ) ) if opts . hec : try : forensic reports = reports [ "forensic reports" ] if len ( forensic reports ) > 0 : hec client . save forensic reports to splunk ( forensic reports ) except splunk . Splunk Error as e : logger . error ( "Splunk HEC error: {0}" . format ( e . str ( ) ) ) arg parser = Argument Parser ( description = "Parses DMARC reports" ) arg parser . add argument ( "-c" , "--config-file" , help = "A path to a configuration file " "(--silent implied)" ) arg parser . add argument ( "file path" , nargs = "*" , help = "one or more paths to aggregate or forensic " "report files or emails" ) strip attachment help = "remove attachment payloads from forensic " "report output" arg parser . add argument ( "--strip-attachment-payloads" , help = strip attachment help , action = "store true" ) arg parser . add argument ( "-o" , "--output" , help = "write output files to the given directory" ) arg parser . add argument ( "-n" , "--nameservers" , nargs = "+" , help = "nameservers to query " "(default is Cloudflare's nameservers)" ) arg parser . add argument ( "-t" , "--dns timeout" , help = "number of seconds to wait for an answer " "from DNS (default: 6.0)" , type = float , default = 6.0 ) arg parser . add argument ( "-s" , "--silent" , action = "store true" , help = "only print errors and warnings" ) arg parser . add argument ( "--debug" , action = "store true" , help = "print debugging information" ) arg parser . add argument ( "--log-file" , default = None , help = "output logging to a file" ) arg parser . add argument ( "-v" , "--version" , action = "version" , version = version ) aggregate reports = [ ] forensic reports = [ ] args = arg parser . parse args ( ) opts = Namespace ( file path = args . file path , config file = args . config file , strip attachment payloads = args . strip attachment payloads , output = args . output , nameservers = args . nameservers , silent = args . silent , dns timeout = args . dns timeout , debug = args . debug , save aggregate = False , save forensic = False , imap host = None , imap skip certificate verification = False , imap ssl = True , imap port = 993 , imap user = None , imap password = None , imap reports folder = "INBOX" , imap archive folder = "Archive" , imap watch = False , imap delete = False , imap test = False , hec = None , hec token = None , hec index = None , hec skip certificate verification = False , elasticsearch hosts = None , elasticsearch index suffix = None , elasticsearch ssl = True , elasticsearch ssl cert path = None , elasticsearch monthly indexes = False , kafka hosts = None , kafka username = None , kafka password = None , kafka aggregate topic = None , kafka forensic topic = None , kafka ssl = False , kafka skip certificate verification = False , smtp host = None , smtp port = 25 , smtp ssl = False , smtp skip certificate verification = False , smtp user = None , smtp password = None , smtp from = None , smtp to = [ ] , smtp subject = "parsedmarc report" , smtp message = "Please see the attached DMARC results." , log file = args . log file , n procs = 1 , chunk size = 1 ) args = arg parser . parse args ( ) if args . config file : abs path = os . path . abspath ( args . config file ) if not os . path . exists ( abs path ) : logger . error ( "A file does not exist at {0}" . format ( abs path ) ) exit ( - 1 ) opts . silent = True config = Config Parser ( ) config . read ( args . config file ) if "general" in config . sections ( ) : general config = config [ "general" ] if "strip attachment payloads" in general config : opts . strip attachment payloads = general config [ "strip attachment payloads" ] if "output" in general config : opts . output = general config [ "output" ] if "nameservers" in general config : opts . nameservers = str to list ( general config [ "nameservers" ] ) if "dns timeout" in general config : opts . dns timeout = general config . getfloat ( "dns timeout" ) if "save aggregate" in general config : opts . save aggregate = general config [ "save aggregate" ] if "save forensic" in general config : opts . save forensic = general config [ "save forensic" ] if "debug" in general config : opts . debug = general config . getboolean ( "debug" ) if "silent" in general config : opts . silent = general config . getboolean ( "silent" ) if "log file" in general config : opts . log file = general config [ "log file" ] if "n procs" in general config : opts . n procs = general config . getint ( "n procs" ) if "chunk size" in general config : opts . chunk size = general config . getint ( "chunk size" ) if "imap" in config . sections ( ) : imap config = config [ "imap" ] if "host" in imap config : opts . imap host = imap config [ "host" ] else : logger . error ( "host setting missing from the " "imap config section" ) exit ( - 1 ) if "port" in imap config : opts . imap port = imap config [ "port" ] if "ssl" in imap config : opts . imap ssl = imap config . getboolean ( "ssl" ) if "skip certificate verification" in imap config : imap verify = imap config . getboolean ( "skip certificate verification" ) opts . imap skip certificate verification = imap verify if "user" in imap config : opts . imap user = imap config [ "user" ] else : logger . critical ( "user setting missing from the " "imap config section" ) exit ( - 1 ) if "password" in imap config : opts . imap password = imap config [ "password" ] else : logger . critical ( "password setting missing from the " "imap config section" ) exit ( - 1 ) if "reports folder" in imap config : opts . imap reports folder = imap config [ "reports folder" ] if "archive folder" in imap config : opts . imap archive folder = imap config [ "archive folder" ] if "watch" in imap config : opts . imap watch = imap config . getboolean ( "watch" ) if "delete" in imap config : opts . imap delete = imap config . getboolean ( "delete" ) if "test" in imap config : opts . imap test = imap config . getboolean ( "test" ) if "elasticsearch" in config : elasticsearch config = config [ "elasticsearch" ] if "hosts" in elasticsearch config : opts . elasticsearch hosts = str to list ( elasticsearch config [ "hosts" ] ) else : logger . critical ( "hosts setting missing from the " "elasticsearch config section" ) exit ( - 1 ) if "index suffix" in elasticsearch config : opts . elasticsearch index suffix = elasticsearch config [ "index suffix" ] if "monthly indexes" in elasticsearch config : monthly = elasticsearch config . getboolean ( "monthly indexes" ) opts . elasticsearch monthly indexes = monthly if "ssl" in elasticsearch config : opts . elasticsearch ssl = elasticsearch config . getboolean ( "ssl" ) if "cert path" in elasticsearch config : opts . elasticsearch ssl cert path = elasticsearch config [ "cert path" ] if "splunk hec" in config . sections ( ) : hec config = config [ "splunk hec" ] if "url" in hec config : opts . hec = hec config [ "url" ] else : logger . critical ( "url setting missing from the " "splunk hec config section" ) exit ( - 1 ) if "token" in hec config : opts . hec token = hec config [ "token" ] else : logger . critical ( "token setting missing from the " "splunk hec config section" ) exit ( - 1 ) if "index" in hec config : opts . hec index = hec config [ "index" ] else : logger . critical ( "index setting missing from the " "splunk hec config section" ) exit ( - 1 ) if "skip certificate verification" in hec config : opts . hec skip certificate verification = hec config [ "skip certificate verification" ] if "kafka" in config . sections ( ) : kafka config = config [ "kafka" ] if "hosts" in kafka config : opts . kafka hosts = str to list ( kafka config [ "hosts" ] ) else : logger . critical ( "hosts setting missing from the " "kafka config section" ) exit ( - 1 ) if "user" in kafka config : opts . kafka username = kafka config [ "user" ] else : logger . critical ( "user setting missing from the " "kafka config section" ) exit ( - 1 ) if "password" in kafka config : opts . kafka password = kafka config [ "password" ] else : logger . critical ( "password setting missing from the " "kafka config section" ) exit ( - 1 ) if "ssl" in kafka config : opts . kafka ssl = kafka config [ "ssl" ] . getboolean ( ) if "skip certificate verification" in kafka config : kafka verify = kafka config . getboolean ( "skip certificate verification" ) opts . kafka skip certificate verification = kafka verify if "aggregate topic" in kafka config : opts . kafka aggregate = kafka config [ "aggregate topic" ] else : logger . critical ( "aggregate topic setting missing from the " "kafka config section" ) exit ( - 1 ) if "forensic topic" in kafka config : opts . kafka username = kafka config [ "forensic topic" ] else : logger . critical ( "forensic topic setting missing from the " "splunk hec config section" ) if "smtp" in config . sections ( ) : smtp config = config [ "smtp" ] if "host" in smtp config : opts . smtp host = smtp config [ "host" ] else : logger . critical ( "host setting missing from the " "smtp config section" ) exit ( - 1 ) if "port" in smtp config : opts . smtp port = smtp config [ "port" ] if "ssl" in smtp config : opts . smtp ssl = smtp config . getboolean ( "ssl" ) if "skip certificate verification" in smtp config : smtp verify = smtp config . getboolean ( "skip certificate verification" ) opts . smtp skip certificate verification = smtp verify if "user" in smtp config : opts . smtp user = smtp config [ "user" ] else : logger . critical ( "user setting missing from the " "smtp config section" ) exit ( - 1 ) if "password" in smtp config : opts . smtp password = smtp config [ "password" ] else : logger . critical ( "password setting missing from the " "smtp config section" ) exit ( - 1 ) if "from" in smtp config : opts . smtp from = smtp config [ "from" ] else : logger . critical ( "from setting missing from the " "smtp config section" ) if "to" in smtp config : opts . smtp to = str to list ( smtp config [ "to" ] ) else : logger . critical ( "to setting missing from the " "smtp config section" ) if "subject" in smtp config : opts . smtp subject = smtp config [ "subject" ] if "attachment" in smtp config : opts . smtp attachment = smtp config [ "attachment" ] if "message" in smtp config : opts . smtp message = smtp config [ "message" ] logging . basic Config ( level = logging . WARNING ) logger . set Level ( logging . WARNING ) if opts . debug : logging . basic Config ( level = logging . DEBUG ) logger . set Level ( logging . DEBUG ) if opts . log file : fh = logging . File Handler ( opts . log file ) formatter = logging . Formatter ( '%(asctime)s - ' '%(levelname)s - [%(filename)s:%(lineno)d] - %(message)s' ) fh . set Formatter ( formatter ) logger . add Handler ( fh ) if opts . imap host is None and len ( opts . file path ) == 0 : logger . error ( "You must supply input files, or an IMAP configuration" ) exit ( 1 ) if opts . save aggregate or opts . save forensic : try : if opts . elasticsearch hosts : es aggregate index = "dmarc aggregate" es forensic index = "dmarc forensic" if opts . elasticsearch index suffix : suffix = opts . elasticsearch index suffix es aggregate index = "{0} {1}" . format ( es aggregate index , suffix ) es forensic index = "{0} {1}" . format ( es forensic index , suffix ) elastic . set hosts ( opts . elasticsearch hosts , opts . elasticsearch ssl , opts . elasticsearch ssl cert path ) elastic . migrate indexes ( aggregate indexes = [ es aggregate index ] , forensic indexes = [ es forensic index ] ) except elastic . Elasticsearch Error as error : logger . error ( "Elasticsearch Error: {0}" . format ( error . str ( ) ) ) exit ( 1 ) if opts . hec : if opts . hec token is None or opts . hec index is None : logger . error ( "HEC token and HEC index are required when " "using HEC URL" ) exit ( 1 ) verify = True if opts . hec skip certificate verification : verify = False hec client = splunk . HEC Client ( opts . hec , opts . hec token , opts . hec index , verify = verify ) kafka aggregate topic = opts . kafka aggregate topic kafka forensic topic = opts . kafka forensic topic file paths = [ ] for file path in args . file path : file paths += glob ( file path ) file paths = list ( set ( file paths ) ) counter = Value ( 'i' , 0 ) pool = Pool ( opts . n procs , initializer = init , initargs = ( counter , ) ) results = pool . starmap async ( cli parse , zip ( file paths , repeat ( opts . strip attachment payloads ) , repeat ( opts . nameservers ) , repeat ( opts . dns timeout ) , repeat ( opts . n procs >= 1 ) ) , opts . chunk size ) pbar = tqdm ( total = len ( file paths ) ) while not results . ready ( ) : pbar . update ( counter . value - pbar . n ) time . sleep ( 0.1 ) pbar . close ( ) results = results . get ( ) pool . close ( ) pool . join ( ) for result in results : if type ( result [ 0 ] ) is Invalid DMARC Report : logger . error ( "Failed to parse {0} - {1}" . format ( result [ 1 ] , result [ 0 ] ) ) else : if result [ 0 ] [ "report type" ] == "aggregate" : aggregate reports . append ( result [ 0 ] [ "report" ] ) elif result [ 0 ] [ "report type" ] == "forensic" : forensic reports . append ( result [ 0 ] [ "report" ] ) if opts . imap host : try : if opts . imap user is None or opts . imap password is None : logger . error ( "IMAP user and password must be specified if" "host is specified" ) rf = opts . imap reports folder af = opts . imap archive folder ns = opts . nameservers sa = opts . strip attachment payloads ssl = True ssl context = None if opts . imap skip certificate verification : logger . debug ( "Skipping IMAP certificate verification" ) ssl context = create default context ( ) ssl context . check hostname = False ssl context . verify mode = CERT NONE if opts . imap ssl is False : ssl = False reports = get dmarc reports from inbox ( host = opts . imap host , port = opts . imap port , ssl = ssl , ssl context = ssl context , user = opts . imap user , password = opts . imap password , reports folder = rf , archive folder = af , delete = opts . imap delete , nameservers = ns , test = opts . imap test , strip attachment payloads = sa ) aggregate reports += reports [ "aggregate reports" ] forensic reports += reports [ "forensic reports" ] except IMAP Error as error : logger . error ( "IMAP Error: {0}" . format ( error . str ( ) ) ) exit ( 1 ) results = Ordered Dict ( [ ( "aggregate reports" , aggregate reports ) , ( "forensic reports" , forensic reports ) ] ) if opts . output : save output ( results , output directory = opts . output ) process reports ( results ) if opts . smtp host : try : ssl context = None if opts . smtp skip certificate verification : logger . debug ( "Skipping SMTP certificate verification" ) ssl context = create default context ( ) ssl context . check hostname = False ssl context . verify mode = CERT NONE email results ( results , opts . smtp host , opts . smtp from , opts . smtp to , ssl = opts . smtp ssl , user = opts . smtp user , password = opts . smtp password , subject = opts . smtp subject , ssl context = ssl context ) except SMTP Error as error : logger . error ( "SMTP Error: {0}" . format ( error . str ( ) ) ) exit ( 1 ) if opts . imap host and opts . imap watch : logger . info ( "Watching for email - Quit with ctrl-c" ) ssl = True ssl context = None if opts . imap skip certificate verification : logger . debug ( "Skipping IMAP certificate verification" ) ssl context = create default context ( ) ssl context . check hostname = False ssl context . verify mode = CERT NONE if opts . imap ssl is False : ssl = False try : sa = opts . strip attachment payloads watch inbox ( opts . imap host , opts . imap user , opts . imap password , process reports , port = opts . imap port , ssl = ssl , ssl context = ssl context , reports folder = opts . imap reports folder , archive folder = opts . imap archive folder , delete = opts . imap delete , test = opts . imap test , nameservers = opts . nameservers , dns timeout = opts . dns timeout , strip attachment payloads = sa ) except IMAP Error as error : logger . error ( "IMAP error: {0}" . format ( error . str ( ) ) ) exit ( 1 )
def publish ( self , subject , reply , payload , payload size ) : if subject == "" : # Avoid sending messages with empty replies. raise Err Bad Subject payload size bytes = ( "%d" % payload size ) . encode ( ) pub cmd = b'' . join ( [ PUB OP , SPC , subject . encode ( ) , SPC , reply , SPC , payload size bytes , CRLF , payload , CRLF ] ) self . stats [ 'out msgs' ] += 1 self . stats [ 'out bytes' ] += payload size yield from self . send command ( pub cmd ) if self . flush queue . empty ( ) : yield from self . flush pending ( )
def process pong ( self ) : if len ( self . pongs ) > 0 : future = self . pongs . pop ( 0 ) future . set result ( True ) self . pongs received += 1 self . pings outstanding -= 1
def process msg ( self , sid , subject , reply , data ) : payload size = len ( data ) self . stats [ 'in msgs' ] += 1 self . stats [ 'in bytes' ] += payload size sub = self . subs . get ( sid ) if sub is None : # Skip in case no subscription present. return sub . received += 1 if sub . max msgs > 0 and sub . received >= sub . max msgs : # Enough messages so can throwaway subscription now. self . subs . pop ( sid , None ) msg = self . build message ( subject , reply , data ) # Check if it is an old style request. if sub . future is not None : if sub . future . cancelled ( ) : # Already gave up, nothing to do. return sub . future . set result ( msg ) return # Let subscription wait for msgs coroutine process the messages, # but in case sending to the subscription task would block, # then consider it to be an slow consumer and drop the message. try : sub . pending size += payload size if sub . pending size >= sub . pending bytes limit : # Substract again the bytes since throwing away # the message so would not be pending data. sub . pending size -= payload size if self . error cb is not None : yield from self . error cb ( Err Slow Consumer ( subject = subject , sid = sid ) ) return sub . pending queue . put nowait ( msg ) except asyncio . Queue Full : if self . error cb is not None : yield from self . error cb ( Err Slow Consumer ( subject = subject , sid = sid ) )
def load features from array ( self , features ) : self . feature images = np . load ( features ) self . feature names = range ( self . feature images . shape [ 1 ] )
def dot product ( self , imgs to decode ) : return np . dot ( imgs to decode . T , self . feature images ) . T
def feature selection ( feat select , X , y ) : # K-best if re . match ( '.*-best' , feat select ) is not None : n = int ( feat select . split ( '-' ) [ 0 ] ) selector = Select K Best ( k = n ) import warnings with warnings . catch warnings ( ) : warnings . simplefilter ( 'ignore' , category = User Warning ) features selected = np . where ( selector . fit ( X , y ) . get support ( ) is True ) [ 0 ] elif re . match ( '.*-randombest' , feat select ) is not None : n = int ( feat select . split ( '-' ) [ 0 ] ) from random import shuffle features = range ( 0 , X . shape [ 1 ] ) shuffle ( features ) features selected = features [ : n ] return features selected
def fit ( self , X , y , cv = None , class weight = 'auto' ) : # Incorporate error checking such as : # if isinstance(self.classifier, Scikit Classifier): #     do one thing None # otherwise None. self . X = X self . y = y self . set class weight ( class weight = class weight , y = y ) self . clf = self . clf . fit ( X , y ) return self . clf
def set class weight ( self , class weight = 'auto' , y = None ) : if class weight is None : cw = None try : self . clf . set params ( class weight = cw ) except Value Error : pass elif class weight == 'auto' : c = np . bincount ( y ) ii = np . nonzero ( c ) [ 0 ] c = c / float ( c . sum ( ) ) cw = dict ( zip ( ii [ : : - 1 ] , c [ ii ] ) ) try : self . clf . set params ( class weight = cw ) except Value Error : import warnings warnings . warn ( "Tried to set class weight, but failed. The classifier " "probably doesn't support it" )
def cross val fit ( self , X , y , cross val = '4-Fold' , scoring = 'accuracy' , feat select = None , class weight = 'auto' ) : from sklearn import cross validation self . X = X self . y = y self . set class weight ( class weight = class weight , y = y ) # Set cross validator if isinstance ( cross val , string types ) : if re . match ( '.*-Fold' , cross val ) is not None : n = int ( cross val . split ( '-' ) [ 0 ] ) self . cver = cross validation . Stratified K Fold ( self . y , n ) else : raise Exception ( 'Unrecognized cross validation method' ) else : self . cver = cross val if feat select is not None : self . features selected = [ ] # Perform cross-validated classification from sklearn . grid search import Grid Search CV if isinstance ( self . clf , Grid Search CV ) : import warnings if feat select is not None : warnings . warn ( "Cross-validated feature selection not supported with " "Grid Search CV" ) self . clf . set params ( cv = self . cver , scoring = scoring ) with warnings . catch warnings ( ) : warnings . simplefilter ( 'ignore' , category = User Warning ) self . clf = self . clf . fit ( X , y ) self . cvs = self . clf . best score else : self . cvs = self . feat select cvs ( feat select = feat select , scoring = scoring ) if feat select is not None : fs = feature selection ( feat select , X , y ) self . features selected . append ( fs ) X = X [ : , fs ] self . clf . fit ( X , y ) return self . cvs . mean ( )
def fit dataset ( self , dataset , y , features = None , feature type = 'features' ) : # Get data from dataset if feature type == 'features' : X = np . rot90 ( dataset . feature table . data . toarray ( ) ) elif feature type == 'voxels' : X = np . rot90 ( dataset . image table . data . toarray ( ) ) self . sk classifier . fit ( X , y )
def get top words ( model , feature names , n top words = 40 ) : topic words = [ ] for topic in model . components : top words = [ feature names [ i ] for i in topic . argsort ( ) [ : - n top words - 1 : - 1 ] ] topic words += [ top words ] return topic words
def pearson ( x , y ) : data = np . vstack ( ( x , y ) ) ms = data . mean ( axis = 1 ) [ ( slice ( None , None , None ) , None ) ] datam = data - ms datass = np . sqrt ( np . sum ( datam ** 2 , axis = 1 ) ) temp = np . dot ( datam [ 1 : ] , datam [ 0 ] . T ) rs = temp / ( datass [ 1 : ] * datass [ 0 ] ) return rs
def load ( cls , filename ) : try : dataset = pickle . load ( open ( filename , 'rb' ) ) except Unicode Decode Error : # Need to try this for python3 dataset = pickle . load ( open ( filename , 'rb' ) , encoding = 'latin' ) if hasattr ( dataset , 'feature table' ) : dataset . feature table . csr to sdf ( ) return dataset
def save ( self , filename ) : if hasattr ( self , 'feature table' ) : self . feature table . sdf to csr ( ) pickle . dump ( self , open ( filename , 'wb' ) , - 1 ) if hasattr ( self , 'feature table' ) : self . feature table . csr to sdf ( )
def get ids by expression ( self , expression , threshold = 0.001 , func = np . sum ) : lexer = lp . Lexer ( ) lexer . build ( ) parser = lp . Parser ( lexer , self . dataset , threshold = threshold , func = func ) parser . build ( ) return parser . parse ( expression ) . keys ( ) . values
def sdf to csr ( self ) : data = self . data . to dense ( ) self . data = { 'columns' : list ( data . columns ) , 'index' : list ( data . index ) , 'values' : sparse . csr matrix ( data . values ) }
def xyz to mat ( foci , xyz dims = None , mat dims = None ) : foci = np . hstack ( ( foci , np . ones ( ( foci . shape [ 0 ] , 1 ) ) ) ) mat = np . array ( [ [ - 0.5 , 0 , 0 , 45 ] , [ 0 , 0.5 , 0 , 63 ] , [ 0 , 0 , 0.5 , 36 ] ] ) . T result = np . dot ( foci , mat ) [ : , : : - 1 ] # multiply and reverse column order return np . round ( result ) . astype ( int )
def save img ( data , filename , masker , header = None ) : if not header : header = masker . get header ( ) header . set data dtype ( data . dtype ) # Avoids loss of precision # Update min/max -- this should happen on save, but doesn't seem to header [ 'cal max' ] = data . max ( ) header [ 'cal min' ] = data . min ( ) img = nifti1 . Nifti1Image ( masker . unmask ( data ) , None , header ) img . to filename ( filename )
def dict to object ( item , object name ) : fields = item . keys ( ) values = item . values ( ) return json . loads ( json . dumps ( item ) , object hook = lambda d : namedtuple ( object name , fields ) ( * values ) )
async def get bearer info ( self ) : if self . client id is None : raise Spotify Exception ( GET BEARER ERR % 'client id' ) elif self . client secret is None : raise Spotify Exception ( GET BEARER ERR % 'client secret' ) token = b64encode ( ':' . join ( ( self . client id , self . client secret ) ) . encode ( ) ) kwargs = { 'url' : 'https://accounts.spotify.com/api/token' , 'data' : { 'grant type' : 'client credentials' } , 'headers' : { 'Authorization' : 'Basic ' + token . decode ( ) } } async with self . session . post ( * * kwargs ) as resp : return json . loads ( await resp . text ( encoding = 'utf-8' ) )
def assert hasattr ( attr : str , msg : str , tp : Base Exception = Spotify Exception ) -> Callable : def decorator ( func : Callable ) -> Callable : @ functools . wraps ( func ) def decorated ( self , * args , * * kwargs ) : if not hasattr ( self , attr ) : raise tp ( msg ) return func ( self , * args , * * kwargs ) if inspect . iscoroutinefunction ( func ) : @ functools . wraps ( func ) async def decorated ( * args , * * kwargs ) : return await decorated ( * args , * * kwargs ) return decorated return decorator
def from client ( cls , client , * args , * * kwargs ) : return cls ( client . http . client id , * args , * * kwargs )
def url ( client id : str , redirect uri : str , * , scope : str = None , state : str = None , secure : bool = True ) -> str : attrs = { 'client id' : client id , 'redirect uri' : quote ( redirect uri ) } if scope is not None : attrs [ 'scope' ] = quote ( scope ) if state is not None : attrs [ 'state' ] = state parameters = '&' . join ( '{0}={1}' . format ( * item ) for item in attrs . items ( ) ) return O Auth2 . BASE . format ( parameters = parameters )
def attrs ( self ) : data = { 'client id' : self . client id , 'redirect uri' : quote ( self . redirect uri ) , } if self . scope is not None : data [ 'scope' ] = quote ( self . scope ) if self . state is not None : data [ 'state' ] = self . state return data
def parameters ( self ) -> str : return '&' . join ( '{0}={1}' . format ( * item ) for item in self . attrs . items ( ) )
async def from href ( self ) : if not hasattr ( self , 'href' ) : raise Type Error ( 'Spotify object has no `href` attribute, therefore cannot be retrived' ) elif hasattr ( self , 'http' ) : return await self . http . request ( ( 'GET' , self . href ) ) else : cls = type ( self ) try : client = getattr ( self , ' {0} client' . format ( cls . name ) ) except Attribute Error : raise Type Error ( 'Spotify object has no way to access a HTTP Client.' ) else : http = client . http data = await http . request ( ( 'GET' , self . href ) ) return cls ( client , data )
def update code urls ( self ) : to ignore = [ ".gitignore" , ".keep" ] for root , , files in Py Funceble . walk ( Py Funceble . CURRENT DIRECTORY + Py Funceble . directory separator + "Py Funceble" + Py Funceble . directory separator ) : # We loop through every directories and files in the `Py Funceble` directory. for file in files : # We loop through the list of files of the currently read directory. if file not in to ignore and " pycache " not in root : # * The filename is not into the list of file to ignore. # and # * The directory we are reading is not ` pycache `. if root . endswith ( Py Funceble . directory separator ) : # The root directory ends with the directory separator. # We fix the path in the currently read file. self . update docs ( root + file ) else : # The root directory does not ends with the directory separator. # We fix the path in the currently read file. # (after appending the directory separator between the root and file) self . update docs ( root + Py Funceble . directory separator + file ) for root , , files in Py Funceble . walk ( Py Funceble . CURRENT DIRECTORY + Py Funceble . directory separator + "tests" + Py Funceble . directory separator ) : # We loop through every directories and files in the `tests` directory. for file in files : # We loop through the list of files of the currently read directory. if file not in to ignore and " pycache " not in root : # * The filename is not into the list of file to ignore. # and # * The directory we are reading is not ` pycache `. if root . endswith ( Py Funceble . directory separator ) : # The root directory ends with the directory separator. # We fix the path in the currently read file. self . update docs ( root + file ) else : # The root directory does not ends with the directory separator. # We fix the path in the currently read file. # (after appending the directory separator between the root and file) self . update docs ( root + Py Funceble . directory separator + file )
def is version greater ( self ) : # we compare the 2 versions. checked = Version ( True ) . check versions ( self . current version [ 0 ] , self . version yaml ) if checked is not None and not checked : # The current version is greater as the older one. # We return True. return True # We return False return False
def is dev version ( cls ) : # We initiate the command we have to run in order to # get the branch we are currently working with. command = "git branch" # We execute and get the command output. command result = Command ( command ) . execute ( ) for branch in command result . split ( "\n" ) : # We loop through each line of the command output. if branch . startswith ( "*" ) and "dev" in branch : # The current branch is `dev`. # We return True. return True # The current branch is not `dev`. # We return False. return False
def does require deprecation ( self ) : for index , version number in enumerate ( self . current version [ 0 ] [ : 2 ] ) : # We loop through the 2 last elements of the version. if version number > self . version yaml [ index ] : # The currently read version number is greater than the one we have in # the version.yaml. # We return True. return True # We return False, we do not need to deprecate anything. return False
def backup ( self ) : if Py Funceble . CONFIGURATION [ "auto continue" ] : # The auto continue subsystem is activated. # We initiate the location where we are going to save the data to backup. data to backup = { } # We get the current counter states. configuration counter = Py Funceble . INTERN [ "counter" ] [ "number" ] # We initiate the data we have to backup. data to backup [ Py Funceble . INTERN [ "file to test" ] ] = { # We backup the number of tested. "tested" : configuration counter [ "tested" ] , # We backup the number of up. "up" : configuration counter [ "up" ] , # We backup the number of down. "down" : configuration counter [ "down" ] , # We backup the number of invalid. "invalid" : configuration counter [ "invalid" ] , } # We initiate the final data we have to save. # We initiate this variable instead of updating backup content because # we do not want to touch the backup content. to save = { } # We add the backup content into to save. to save . update ( self . backup content ) # And we overwrite with the newly data to backup. to save . update ( data to backup ) # Finaly, we save our informations into the log file. Dict ( to save ) . to json ( self . autocontinue log file )
def restore ( self ) : if Py Funceble . CONFIGURATION [ "auto continue" ] and self . backup content : # The auto continue subsystem is activated and the backup content # is not empty. # We get the file we have to restore. file to restore = Py Funceble . INTERN [ "file to test" ] if file to restore in self . backup content : # The file we are working with is already into the backup content. # We initiate the different status to set. to initiate = [ "up" , "down" , "invalid" , "tested" ] # Because at some time it was not the current status, we have to map # the new with the old. This way, if someone is running the latest # version but with old data, we still continue like nothing happend. alternatives = { "up" : "number of up" , "down" : "number of down" , "invalid" : "number of invalid" , "tested" : "number of tested" , } for string in to initiate : # We loop over the status we have to initiate. try : # We try to update the counters by using the currently read status. Py Funceble . INTERN [ "counter" ] [ "number" ] . update ( { string : self . backup content [ file to restore ] [ string ] } ) except Key Error : # But if the status is not present, we try with the older index # we mapped previously. Py Funceble . INTERN [ "counter" ] [ "number" ] . update ( { string : self . backup content [ file to restore ] [ alternatives [ string ] ] } )
def stay safe ( ) : # pragma: no cover random = int ( choice ( str ( int ( time ( ) ) ) ) ) if not CONFIGURATION [ "quiet" ] and random % 3 == 0 : print ( "\n" + Fore . GREEN + Style . BRIGHT + "Thanks for using Py Funceble!" ) print ( Fore . YELLOW + Style . BRIGHT + "Share your experience on " + Fore . CYAN + "Twitter" + Fore . YELLOW + " with " + Fore . CYAN + "#Py Funceble" + Fore . YELLOW + "!" ) print ( Fore . GREEN + Style . BRIGHT + "Have a feedback, an issue or an improvement idea ?" ) print ( Fore . YELLOW + Style . BRIGHT + "Let us know on " + Fore . CYAN + "Git Hub" + Fore . YELLOW + "!" )
def command line ( ) : # pragma: no cover pylint: disable=too-many-branches,too-many-statements if name == "Py Funceble" : # We initiate the end of the coloration at the end of each line. initiate ( autoreset = True ) # We load the configuration and the directory structure. load config ( True ) try : # The following handle the command line argument. try : PARSER = argparse . Argument Parser ( epilog = "Crafted with %s by %s" % ( Fore . RED + "♥" + F re.R E SET, Style . BRIGHT + Fore . CYAN + "Nissar Chababy (Funilrys) " + Style . RESET ALL + "with the help of " + Style . BRIGHT + Fore . GREEN + "https://pyfunceble.rtfd.io/en/master/contributors.html " + Style . RESET ALL + "&& " + Style . BRIGHT + Fore . GREEN + "https://pyfunceble.rtfd.io/en/master/special-thanks.html" , ) , add help = False , ) CURRENT VALUE FORMAT = ( Fore . YELLOW + Style . BRIGHT + "Configured value: " + Fore . BLUE ) PARSER . add argument ( "-ad" , "--adblock" , action = "store true" , help = "Switch the decoding of the adblock format. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "adblock" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "-a" , "--all" , action = "store false" , help = "Output all available information on the screen. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "less" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "" "-c" , "--auto-continue" , "--continue" , action = "store true" , help = "Switch the value of the auto continue mode. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "auto continue" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "--autosave-minutes" , type = int , help = "Update the minimum of minutes before we start " "committing to upstream under Travis CI. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "travis autosave minutes" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "--clean" , action = "store true" , help = "Clean all files under output." ) PARSER . add argument ( "--clean-all" , action = "store true" , help = "Clean all files under output and all file generated by Py Funceble." , ) PARSER . add argument ( "--cmd" , type = str , help = "Pass a command to run before each commit " "(except the final one) under the Travis mode. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "command before end" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "--cmd-before-end" , type = str , help = "Pass a command to run before the results " "(final) commit under the Travis mode. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "command before end" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "--commit-autosave-message" , type = str , help = "Replace the default autosave commit message. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "travis autosave commit" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "--commit-results-message" , type = str , help = "Replace the default results (final) commit message. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "travis autosave final commit" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "-d" , "--domain" , type = str , help = "Set and test the given domain." ) PARSER . add argument ( "-db" , "--database" , action = "store true" , help = "Switch the value of the usage of a database to store " "inactive domains of the currently tested list. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "inactive database" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "-dbr" , "--days-between-db-retest" , type = int , help = "Set the numbers of days between each retest of domains present " "into inactive-db.json. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "days between db retest" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "--debug" , action = "store true" , help = "Switch the value of the debug mode. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "debug" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "--directory-structure" , action = "store true" , help = "Generate the directory and files that are needed and which does " "not exist in the current directory." , ) PARSER . add argument ( "-ex" , "--execution" , action = "store true" , help = "Switch the default value of the execution time showing. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "show execution time" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "-f" , "--file" , type = str , help = "Read the given file and test all domains inside it. " "If a URL is given we download and test the content of the given URL." , # pylint: disable=line-too-long ) PARSER . add argument ( "--filter" , type = str , help = "Domain to filter (regex)." ) PARSER . add argument ( "--help" , action = "help" , default = argparse . SUPPRESS , help = "Show this help message and exit." , ) PARSER . add argument ( "--hierarchical" , action = "store true" , help = "Switch the value of the hierarchical sorting of the tested file. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "hierarchical sorting" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "-h" , "--host" , action = "store true" , help = "Switch the value of the generation of hosts file. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "generate hosts" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "--http" , action = "store true" , help = "Switch the value of the usage of HTTP code. %s" % ( CURRENT VALUE FORMAT + repr ( HTTP CODE [ "active" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "--iana" , action = "store true" , help = "Update/Generate `iana-domains-db.json`." , ) PARSER . add argument ( "--idna" , action = "store true" , help = "Switch the value of the IDNA conversion. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "idna conversion" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "-ip" , type = str , help = "Change the IP to print in the hosts files with the given one. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "custom ip" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "--json" , action = "store true" , help = "Switch the value of the generation " "of the JSON formatted list of domains. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "generate json" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "--less" , action = "store true" , help = "Output less informations on screen. %s" % ( CURRENT VALUE FORMAT + repr ( Core . switch ( "less" ) ) + Style . RESET ALL ) , ) PARSER . add argument ( "--local" , action = "store true" , help = "Switch the value of the local network testing. %s" % ( CURRENT VALUE FORMAT + repr ( Core . switch ( "local" ) ) + Style . RESET ALL ) , ) PARSER . add argument ( "--link" , type = str , help = "Download and test the given file." ) PARSER . add argument ( "-m" , "--mining" , action = "store true" , help = "Switch the value of the mining subsystem usage. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "mining" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "-n" , "--no-files" , action = "store true" , help = "Switch the value of the production of output files. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "no files" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "-nl" , "--no-logs" , action = "store true" , help = "Switch the value of the production of logs files " "in the case we encounter some errors. %s" % ( CURRENT VALUE FORMAT + repr ( not CONFIGURATION [ "logs" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "-ns" , "--no-special" , action = "store true" , help = "Switch the value of the usage of the SPECIAL rules. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "no special" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "-nu" , "--no-unified" , action = "store true" , help = "Switch the value of the production unified logs " "under the output directory. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "unified" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "-nw" , "--no-whois" , action = "store true" , help = "Switch the value the usage of whois to test domain's status. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "no whois" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "-p" , "--percentage" , action = "store true" , help = "Switch the value of the percentage output mode. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "show percentage" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "--plain" , action = "store true" , help = "Switch the value of the generation " "of the plain list of domains. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "plain list domain" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "--production" , action = "store true" , help = "Prepare the repository for production." , ) PARSER . add argument ( "-psl" , "--public-suffix" , action = "store true" , help = "Update/Generate `public-suffix.json`." , ) PARSER . add argument ( "-q" , "--quiet" , action = "store true" , help = "Run the script in quiet mode. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "quiet" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "--share-logs" , action = "store true" , help = "Switch the value of the sharing of logs. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "share logs" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "-s" , "--simple" , action = "store true" , help = "Switch the value of the simple output mode. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "simple" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "--split" , action = "store true" , help = "Switch the value of the split of the generated output files. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "inactive database" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "--syntax" , action = "store true" , help = "Switch the value of the syntax test mode. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "syntax" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "-t" , "--timeout" , type = int , default = 3 , help = "Switch the value of the timeout. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "seconds before http timeout" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "--travis" , action = "store true" , help = "Switch the value of the Travis mode. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "travis" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "--travis-branch" , type = str , default = "master" , help = "Switch the branch name where we are going to push. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "travis branch" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "-u" , "--url" , type = str , help = "Analyze the given URL." ) PARSER . add argument ( "-uf" , "--url-file" , type = str , help = "Read and test the list of URL of the given file. " "If a URL is given we download and test the content of the given URL." , # pylint: disable=line-too-long ) PARSER . add argument ( "-ua" , "--user-agent" , type = str , help = "Set the user-agent to use and set every time we " "interact with everything which is not our logs sharing system." , # pylint: disable=line-too-long ) PARSER . add argument ( "-v" , "--version" , help = "Show the version of Py Funceble and exit." , action = "version" , version = "%(prog)s " + VERSION , ) PARSER . add argument ( "-vsc" , "--verify-ssl-certificate" , action = "store true" , help = "Switch the value of the verification of the " "SSL/TLS certificate when testing for URL. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "verify ssl certificate" ] ) + Style . RESET ALL ) , ) PARSER . add argument ( "-wdb" , "--whois-database" , action = "store true" , help = "Switch the value of the usage of a database to store " "whois data in order to avoid whois servers rate limit. %s" % ( CURRENT VALUE FORMAT + repr ( CONFIGURATION [ "whois database" ] ) + Style . RESET ALL ) , ) ARGS = PARSER . parse args ( ) if ARGS . less : CONFIGURATION . update ( { "less" : ARGS . less } ) elif not ARGS . all : CONFIGURATION . update ( { "less" : ARGS . all } ) if ARGS . adblock : CONFIGURATION . update ( { "adblock" : Core . switch ( "adblock" ) } ) if ARGS . auto continue : CONFIGURATION . update ( { "auto continue" : Core . switch ( "auto continue" ) } ) if ARGS . autosave minutes : CONFIGURATION . update ( { "travis autosave minutes" : ARGS . autosave minutes } ) if ARGS . clean : Clean ( None ) if ARGS . clean all : Clean ( None , ARGS . clean all ) if ARGS . cmd : CONFIGURATION . update ( { "command" : ARGS . cmd } ) if ARGS . cmd before end : CONFIGURATION . update ( { "command before end" : ARGS . cmd before end } ) if ARGS . commit autosave message : CONFIGURATION . update ( { "travis autosave commit" : ARGS . commit autosave message } ) if ARGS . commit results message : CONFIGURATION . update ( { "travis autosave final commit" : ARGS . commit results message } ) if ARGS . database : CONFIGURATION . update ( { "inactive database" : Core . switch ( "inactive database" ) } ) if ARGS . days between db retest : CONFIGURATION . update ( { "days between db retest" : ARGS . days between db retest } ) if ARGS . debug : CONFIGURATION . update ( { "debug" : Core . switch ( "debug" ) } ) if ARGS . directory structure : Directory Structure ( ) if ARGS . execution : CONFIGURATION . update ( { "show execution time" : Core . switch ( "show execution time" ) } ) if ARGS . filter : CONFIGURATION . update ( { "filter" : ARGS . filter } ) if ARGS . hierarchical : CONFIGURATION . update ( { "hierarchical sorting" : Core . switch ( "hierarchical sorting" ) } ) if ARGS . host : CONFIGURATION . update ( { "generate hosts" : Core . switch ( "generate hosts" ) } ) if ARGS . http : HTTP CODE . update ( { "active" : Core . switch ( HTTP CODE [ "active" ] , True ) } ) if ARGS . iana : IANA ( ) . update ( ) if ARGS . idna : CONFIGURATION . update ( { "idna conversion" : Core . switch ( "idna conversion" ) } ) if ARGS . ip : CONFIGURATION . update ( { "custom ip" : ARGS . ip } ) if ARGS . json : CONFIGURATION . update ( { "generate json" : Core . switch ( "generate json" ) } ) if ARGS . local : CONFIGURATION . update ( { "local" : Core . switch ( "local" ) } ) if ARGS . mining : CONFIGURATION . update ( { "mining" : Core . switch ( "mining" ) } ) if ARGS . no files : CONFIGURATION . update ( { "no files" : Core . switch ( "no files" ) } ) if ARGS . no logs : CONFIGURATION . update ( { "logs" : Core . switch ( "logs" ) } ) if ARGS . no special : CONFIGURATION . update ( { "no special" : Core . switch ( "no special" ) } ) if ARGS . no unified : CONFIGURATION . update ( { "unified" : Core . switch ( "unified" ) } ) if ARGS . no whois : CONFIGURATION . update ( { "no whois" : Core . switch ( "no whois" ) } ) if ARGS . percentage : CONFIGURATION . update ( { "show percentage" : Core . switch ( "show percentage" ) } ) if ARGS . plain : CONFIGURATION . update ( { "plain list domain" : Core . switch ( "plain list domain" ) } ) if ARGS . production : Production ( ) if ARGS . public suffix : Public Suffix ( ) . update ( ) if ARGS . quiet : CONFIGURATION . update ( { "quiet" : Core . switch ( "quiet" ) } ) if ARGS . share logs : CONFIGURATION . update ( { "share logs" : Core . switch ( "share logs" ) } ) if ARGS . simple : CONFIGURATION . update ( { "simple" : Core . switch ( "simple" ) , "quiet" : Core . switch ( "quiet" ) } ) if ARGS . split : CONFIGURATION . update ( { "split" : Core . switch ( "split" ) } ) if ARGS . syntax : CONFIGURATION . update ( { "syntax" : Core . switch ( "syntax" ) } ) if ARGS . timeout and ARGS . timeout % 3 == 0 : CONFIGURATION . update ( { "seconds before http timeout" : ARGS . timeout } ) if ARGS . travis : CONFIGURATION . update ( { "travis" : Core . switch ( "travis" ) } ) if ARGS . travis branch : CONFIGURATION . update ( { "travis branch" : ARGS . travis branch } ) if ARGS . user agent : CONFIGURATION . update ( { "user agent" : ARGS . user agent } ) if ARGS . verify ssl certificate : CONFIGURATION . update ( { "verify ssl certificate" : ARGS . verify ssl certificate } ) if ARGS . whois database : CONFIGURATION . update ( { "whois database" : Core . switch ( "whois database" ) } ) if not CONFIGURATION [ "quiet" ] : Core . colorify logo ( home = True ) # We compare the versions (upstream and local) and in between. Version ( ) . compare ( ) # We call our Core which will handle all case depending of the configuration or # the used command line arguments. Core ( domain or ip to test = ARGS . domain , file path = ARGS . file , url to test = ARGS . url , url file = ARGS . url file , link to test = ARGS . link , ) except Key Error as e : if not Version ( True ) . is cloned ( ) : # We are not into the cloned version. # We merge the local with the upstream configuration. Merge ( CURRENT DIRECTORY ) else : # We are in the cloned version. # We raise the exception. # # Note: The purpose of this is to avoid having # to search for a mistake while developing. raise e except Keyboard Interrupt : stay safe ( )
def entry management url ( self ) : if ( self . url file # pylint: disable=no-member and not self . entry management url download ( self . url file # pylint: disable=no-member ) ) : # pylint: disable=no-member # The current url file is not a URL. # We initiate the filename as the file we have to test. Py Funceble . INTERN [ "file to test" ] = self . url file
def print header ( cls ) : if ( not Py Funceble . CONFIGURATION [ "quiet" ] and not Py Funceble . CONFIGURATION [ "header printed" ] ) : # * The quiet mode is not activated. # and # * The header has not been already printed. # We print a new line. print ( "\n" ) if Py Funceble . CONFIGURATION [ "less" ] : # We have to show less informations on screen. # We print the `Less` header. Prints ( None , "Less" ) . header ( ) else : # We have to show every informations on screen. # We print the `Generic` header. Prints ( None , "Generic" ) . header ( ) # The header was printed. # We initiate the variable which say that the header has been printed to True. Py Funceble . CONFIGURATION [ "header printed" ] = True
def handle ( self ) : # We initiate the source we are going to parse to the Generate class. source = "URL" if self . catched . lower ( ) not in Py Funceble . STATUS [ "list" ] [ "invalid" ] : # The parsed status is not in the list of invalid. # We generate the status file with the catched status. Generate ( self . catched , source ) . status file ( ) else : # The parsed status is in the list of invalid. # We generate the status file with the parsed status. Generate ( self . catched , "SYNTAX" ) . status file ( ) # We return the parsed status. return self . catched
def delete uneeded ( self ) : # We get the structure we have to apply. structure = self . get structure ( ) # We get the list of key which is implicitly the list of directory we do not bave to delete. list of key = list ( structure . keys ( ) ) # We move to the content of the parent as we know that we are creating only one directory. # Note: if one day we will have to create multiple directory, we will have to change # the following. structure = structure [ list of key [ 0 ] ] # We also set the parent directory as we are going to construct its childen. parent path = list of key [ 0 ] if not parent path . endswith ( Py Funceble . directory separator ) : parent path += Py Funceble . directory separator for root , , in Py Funceble . walk ( parent path ) : # We loop through each directories of the parent path. # We fix the path in order to avoid issues. root = Directory ( root ) . fix path ( ) if root . replace ( parent path , "" ) not in structure : # The currently read directory is not in our structure. # We delete it. Py Funceble . rmtree ( root )
def load config file ( self ) : try : # We try to load the configuration file. Py Funceble . CONFIGURATION . update ( Dict . from yaml ( File ( self . path to config ) . read ( ) ) ) # We install the latest iana configuration file. self . install iana config ( ) # We install the latest public suffix configuration file. self . install psl config ( ) # We install the latest directory structure file. self . install directory structure file ( ) except File Not Found Error as exception : # But if the configuration file is not found. if Py Funceble . path . isfile ( self . path to default config ) : # The `DEFAULT CONFIGURATION FILENAME` file exists. # We copy it as the configuration file. File ( self . path to default config ) . copy ( self . path to config ) # And we load the configuration file as it does exist (yet). self . load config file ( ) else : # The `DEFAULT CONFIGURATION FILENAME` file does not exists. # We raise the exception we were handling. raise exception
def install iana config ( cls ) : # We initiate the link to the iana configuration. # It is not hard coded because this method is called only if we # are sure that the configuration file exist. iana link = Py Funceble . CONFIGURATION [ "links" ] [ "iana" ] # We update the link according to our current version. iana link = Version ( True ) . right url from version ( iana link ) # We set the destination of the downloaded file. destination = Py Funceble . CURRENT DIRECTORY + "iana-domains-db.json" if not Version ( True ) . is cloned ( ) or not Py Funceble . path . isfile ( destination ) : # The current version is not the cloned version. # We Download the link content and return the download status. return Download ( iana link , destination ) . text ( ) # We are in the cloned version. # We do not need to download the file, so we are returning None. return None
def install psl config ( cls ) : # We initiate the link to the public suffix configuration. # It is not hard coded because this method is called only if we # are sure that the configuration file exist. psl link = Py Funceble . CONFIGURATION [ "links" ] [ "psl" ] # We update the link according to our current version. psl link = Version ( True ) . right url from version ( psl link ) # We set the destination of the downloaded file. destination = ( Py Funceble . CURRENT DIRECTORY + Py Funceble . CONFIGURATION [ "outputs" ] [ "default files" ] [ "public suffix" ] ) if not Version ( True ) . is cloned ( ) or not Py Funceble . path . isfile ( destination ) : # The current version is not the cloned version. # We Download the link content and return the download status. return Download ( psl link , destination ) . text ( ) # We are in the cloned version. # We do not need to download the file, so we are returning None. return None
def install directory structure file ( cls ) : # We initiate the link to the public suffix configuration. # It is not hard coded because this method is called only if we # are sure that the configuration file exist. dir structure link = Py Funceble . CONFIGURATION [ "links" ] [ "dir structure" ] # We update the link according to our current version. dir structure link = Version ( True ) . right url from version ( dir structure link ) # We set the destination of the downloaded file. destination = ( Py Funceble . CURRENT DIRECTORY + Py Funceble . CONFIGURATION [ "outputs" ] [ "default files" ] [ "dir structure" ] ) if not Version ( True ) . is cloned ( ) or not Py Funceble . path . isfile ( destination ) : # The current version is not the cloned version. # We Download the link content and return the download status. data = Download ( dir structure link , destination , return data = True ) . text ( ) File ( destination ) . write ( data , overwrite = True ) return True # We are in the cloned version. # We do not need to download the file, so we are returning None. return None
def merge values ( self ) : to remove = [ ] self . new config = Dict ( Dict ( self . upstream config ) . merge ( Py Funceble . CONFIGURATION ) ) . remove key ( to remove )
def load ( self ) : if "PYFUNCEBLE AUTO CONFIGURATION" not in Py Funceble . environ : # The auto configuration environment variable is not set. while True : # We infinitly loop until we get a reponse which is `y|Y` or `n|N`. # We ask the user if we should install and load the default configuration. response = input ( Py Funceble . Style . BRIGHT + Py Funceble . Fore . RED + "A configuration key is missing.\n" + Py Funceble . Fore . RESET + "Try to merge upstream configuration file into %s ? [y/n] " % ( Py Funceble . Style . BRIGHT + self . path to config + Py Funceble . Style . RESET ALL ) ) if isinstance ( response , str ) : # The response is a string if response . lower ( ) == "y" : # The response is a `y` or `Y`. # We merge the old values inside the new one. self . merge values ( ) # And we save. self . save ( ) print ( Py Funceble . Style . BRIGHT + Py Funceble . Fore . GREEN + "Done!\n" "Please try again, if it happens again," " please fill a new issue." ) # And we break the loop as we got a satisfied response. break elif response . lower ( ) == "n" : # The response is a `n` or `N`. # We inform the user that something went wrong. raise Exception ( "Configuration key still missing." ) else : # The auto configuration environment variable is set. # We merge the old values inside the new one. self . merge values ( ) # And we save. self . save ( )
def compare ( self ) : if self . upstream data [ "force update" ] [ "status" ] : # The force update status is set to True. for minimal in self . upstream data [ "force update" ] [ "minimal version" ] : # We loop through the list of minimal version which trigger the # the force update message. # We compare the local with the currently read minimal version. checked = self . check versions ( self . local splited , self . split versions ( minimal ) ) if not Py Funceble . CONFIGURATION [ "quiet" ] : # The quiet mode is not activated. if checked or checked is not False and not checked : # The current version is less or equal to # the minimal version. # We initiate the message we are going to return to # the user. message = ( Py Funceble . Style . BRIGHT + Py Funceble . Fore . RED + "A critical issue has been fixed.\n" + Py Funceble . Style . RESET ALL ) # pylint:disable=line-too-long message += ( Py Funceble . Style . BRIGHT + Py Funceble . Fore . GREEN + "Please take the time to update Py Funceble!\n" + Py Funceble . Style . RESET ALL ) # pylint:disable=line-too-long # We print the message on screen. print ( message ) # We exit Py Funceble with the code 1. exit ( 1 ) elif checked or checked is not False and not checked : # The quiet mode is activated and the current version # is less or equal to the minimal version. # We raise an exception telling the user to update their # instance of Py Funceble. raise Exception ( "A critical issue has been fixed. Please take the time to update Py Funceble!" # pylint:disable=line-too-long ) for version in self . upstream data [ "deprecated" ] : # We loop through the list of deprecated versions. # We compare the local with the currently read deprecated version. checked = self . check versions ( self . local splited , self . split versions ( version ) ) if ( not Py Funceble . CONFIGURATION [ "quiet" ] and checked or checked is not False and not checked ) : # The quiet mode is not activated and the local version is # less or equal to the currently read deprecated version. # We initiate the message we are going to return to the user. message = ( Py Funceble . Style . BRIGHT + Py Funceble . Fore . RED + "Your current version is considered as deprecated.\n" + Py Funceble . Style . RESET ALL ) # pylint:disable=line-too-long message += ( Py Funceble . Style . BRIGHT + Py Funceble . Fore . GREEN + "Please take the time to update Py Funceble!\n" + Py Funceble . Style . RESET ALL ) # pylint:disable=line-too-long # We print the message. print ( message ) # And we continue to the next logic. There is no need to # shutdown Py Funceble as it's just for information. return # The quiet mode is activated. if checked or checked is not False and not checked : # The local version is  less or equal to the currently # read deprecated version. print ( "Version deprecated." ) # And we continue to the next logic. There is no need to # shutdown Py Funceble as it's just for information. return # We compare the local version with the upstream version. status = self . check versions ( self . local splited , self . split versions ( self . upstream data [ "current version" ] ) , ) if status is not None and not status and not Py Funceble . CONFIGURATION [ "quiet" ] : # The quiet mode is not activate and the current version is greater than # the upstream version. # We initiate the message we are going to return to the user. message = ( Py Funceble . Style . BRIGHT + Py Funceble . Fore . CYAN + "Your version is more recent!\n You should really think about sharing your changes with the community!\n" # pylint:disable=line-too-long + Py Funceble . Style . RESET ALL ) message += ( Py Funceble . Style . BRIGHT + "Your version: " + Py Funceble . Style . RESET ALL + Py Funceble . VERSION + "\n" ) message += ( Py Funceble . Style . BRIGHT + "Upstream version: " + Py Funceble . Style . RESET ALL + self . upstream data [ "current version" ] + "\n" ) # We print the message. print ( message ) elif status : # The current version is less that the upstream version. if not Py Funceble . CONFIGURATION [ "quiet" ] : # The quiet mode is not activated. # We initiate the message we are going to return to the user. message = ( Py Funceble . Style . BRIGHT + Py Funceble . Fore . YELLOW + "Please take the time to update Py Funceble!\n" + Py Funceble . Style . RESET ALL ) # pylint:disable=line-too-long message += ( Py Funceble . Style . BRIGHT + "Your version: " + Py Funceble . Style . RESET ALL + Py Funceble . VERSION + "\n" ) # pylint:disable=line-too-long message += ( Py Funceble . Style . BRIGHT + "Upstream version: " + Py Funceble . Style . RESET ALL + self . upstream data [ # pylint:disable=line-too-long "current version" ] + "\n" ) # We print the message. print ( message ) else : # The quiet mode is activated. # We print the message. print ( "New version available." ) # And we continue to the next app logic. There is no need to # shutdown Py Funceble as it's just for information. return
def handle non existant index ( cls ) : try : # We try to call the http code. Py Funceble . INTERN [ "http code" ] except Key Error : # If it is not found. # We initiate an empty http code. Py Funceble . INTERN [ "http code" ] = "*" * 3 try : # We try to call the referer. Py Funceble . INTERN [ "referer" ] except Key Error : # If it is not found. # We initate an `Unknown` referer. Py Funceble . INTERN [ "referer" ] = "Unknown"
def info files ( self ) : # pylint: disable=inconsistent-return-statements if self . do not produce file ( ) : # We do not have to produce file. # We return false. return False if ( "file to test" in Py Funceble . INTERN and Py Funceble . INTERN [ "file to test" ] and ( Py Funceble . CONFIGURATION [ "generate hosts" ] or Py Funceble . CONFIGURATION [ "plain list domain" ] or Py Funceble . CONFIGURATION [ "generate json" ] ) ) : # * We are not testing as an imported module. # and # * The hosts file generation is activated. # or # * The plain list generation is activated. # We initiate a variable which whill save the splited testination. splited destination = "" # We initiate the list of all analytic related statuses. http list = [ ] http list . extend ( Py Funceble . STATUS [ "list" ] [ "potentially up" ] ) http list . extend ( Py Funceble . STATUS [ "list" ] [ "potentially down" ] ) http list . extend ( Py Funceble . STATUS [ "list" ] [ "http active" ] ) http list . extend ( Py Funceble . STATUS [ "list" ] [ "suspicious" ] ) # We partially initiate the path to the hosts file. output hosts = ( self . output parent dir + Py Funceble . OUTPUTS [ "hosts" ] [ "directory" ] + "%s" + directory separator + Py Funceble . OUTPUTS [ "hosts" ] [ "filename" ] ) # We partially initiate the path to the plain list file. output domains = ( self . output parent dir + Py Funceble . OUTPUTS [ "domains" ] [ "directory" ] + "%s" + directory separator + Py Funceble . OUTPUTS [ "domains" ] [ "filename" ] ) # We partially intiate the path to the json list file. output json = ( self . output parent dir + Py Funceble . OUTPUTS [ "json" ] [ "directory" ] + "%s" + directory separator + Py Funceble . OUTPUTS [ "json" ] [ "filename" ] ) if self . domain status . lower ( ) in Py Funceble . STATUS [ "list" ] [ "up" ] : # The status is in the list of up list. # We complete the path to the hosts file. hosts destination = output hosts % Py Funceble . STATUS [ "official" ] [ "up" ] # We complete the path to the plain list file. plain destination = output domains % Py Funceble . STATUS [ "official" ] [ "up" ] # We complete the path to the json list file. json destination = output json % Py Funceble . STATUS [ "official" ] [ "up" ] elif self . domain status . lower ( ) in Py Funceble . STATUS [ "list" ] [ "valid" ] : # The status is in the list of valid list. # We complete the path to the hosts file. hosts destination = ( output hosts % Py Funceble . STATUS [ "official" ] [ "valid" ] ) # We complete the path to the plain list file. plain destination = ( output domains % Py Funceble . STATUS [ "official" ] [ "valid" ] ) # We complete the path to the json list file. json destination = output json % Py Funceble . STATUS [ "official" ] [ "valid" ] elif self . domain status . lower ( ) in Py Funceble . STATUS [ "list" ] [ "down" ] : # The status is in the list of down list. # We complete the path to the hosts file. hosts destination = output hosts % Py Funceble . STATUS [ "official" ] [ "down" ] # We complete the path to the plain list file. plain destination = ( output domains % Py Funceble . STATUS [ "official" ] [ "down" ] ) # We complete the path to the json list file. json destination = output json % Py Funceble . STATUS [ "official" ] [ "down" ] elif self . domain status . lower ( ) in Py Funceble . STATUS [ "list" ] [ "invalid" ] : # The status is in the list of invalid list. # We complete the path to the hosts file. hosts destination = ( output hosts % Py Funceble . STATUS [ "official" ] [ "invalid" ] ) # We complete the path to the plain list file. plain destination = ( output domains % Py Funceble . STATUS [ "official" ] [ "invalid" ] ) # We complete the path to the json list file. json destination = ( output json % Py Funceble . STATUS [ "official" ] [ "invalid" ] ) elif self . domain status . lower ( ) in http list : # The status is in the list of analytic status. # We construct the path to the analytic directory. output dir = self . analytic host file directory ( ) if not output dir . endswith ( directory separator ) : # The output directory does not ends with the directory separator. # We append the directory separator at the end of the output directory. output dir += directory separator # We initiate the hosts file path. hosts destination = output dir + Py Funceble . OUTPUTS [ "hosts" ] [ "filename" ] # We initiate the plain list file path. plain destination = ( output dir + Py Funceble . OUTPUTS [ "domains" ] [ "filename" ] ) # We complete the path to the json list file. json destination = output dir + Py Funceble . OUTPUTS [ "json" ] [ "filename" ] # We initiate the path to the http code file. # Note: We generate the http code file so that # we can have each domain in a file which is the # extracted http code. splited destination = output dir + str ( Py Funceble . INTERN [ "http code" ] ) if Py Funceble . CONFIGURATION [ "generate hosts" ] : # The hosts file generation is activated. # We generate/append the currently tested element in its # final location. (hosts file format) # We print on screen and on file. Prints ( [ Py Funceble . CONFIGURATION [ "custom ip" ] , self . tested ] , "Full Hosts" , hosts destination , ) . data ( ) if Py Funceble . CONFIGURATION [ "plain list domain" ] : # The plain list generation is activated. # We generate/append the currently tested element in its # final location. (the plain list format) # We print on file. Prints ( [ self . tested ] , "Plain Domain" , plain destination ) . data ( ) if Py Funceble . CONFIGURATION [ "split" ] and splited destination : # The splited list generation is activated. # We generate/append the currently tested element in its # final location. (the split list format) # We print on file. Prints ( [ self . tested ] , "Plain Domain" , splited destination ) . data ( ) if Py Funceble . CONFIGURATION [ "generate json" ] : # The jsaon list generation is activated. # We generate/append the currently tested element in its # final location. (the json format) # We print on file. Prints ( [ self . tested ] , "JSON" , json destination ) . data ( )
def status file ( self ) : # pylint: disable=inconsistent-return-statements if "file to test" in Py Funceble . INTERN : # We are not testing as an imported module. # We generate the hosts file. Generate ( self . domain status , self . source , self . expiration date ) . info files ( ) # We are testing a file content. # We increase the percentage count. Percentage ( self . domain status ) . count ( ) # We print on screen if needed. self . prints status screen ( ) if self . do not produce file ( ) : return None if ( not Py Funceble . CONFIGURATION [ "no files" ] and Py Funceble . CONFIGURATION [ "split" ] ) : # * The file non-generation of file is globaly deactivated. # and # * We have to split the outputs. # We print or generate the files. self . prints status file ( ) else : # * The file non-generation of file is globaly activated. # or # * We do not have to split the outputs. # We print or generate the unified files. self . unified file ( )
def load ( self ) : if not Py Funceble . INTERN [ "psl db" ] : # The public database was not already loaded. # * We read, convert to dict and return the file content. # and # * We fill/create the database. Py Funceble . INTERN [ "psl db" ] = Dict ( ) . from json ( File ( self . destination ) . read ( ) )
def load ( self ) : if "iana db" not in Py Funceble . INTERN or not Py Funceble . INTERN [ "iana db" ] : # The global database is empty, None or does not exist. # We update it with the database content. Py Funceble . INTERN [ "iana db" ] = self . iana db
def update ( self ) : if not Py Funceble . CONFIGURATION [ "quiet" ] : # * The quiet mode is not activated. # We print on screen what we are doing. print ( "Update of iana-domains-db" , end = " " ) # We loop through the line of the iana website. for extension , referer in self . extensions ( ) : if extension not in self . iana db or self . iana db [ extension ] != referer : # We add the extension to the databae. self . iana db [ extension ] = referer # We save the content of the constructed database. Dict ( self . iana db ) . to json ( self . destination ) if not Py Funceble . CONFIGURATION [ "quiet" ] : # The quiet mode is not activated. # We indicate that the work is done without any issue. print ( Py Funceble . INTERN [ "done" ] )
def retrieve ( self ) : if Py Funceble . CONFIGURATION [ "mining" ] : # The mining is activated. if "mined" not in Py Funceble . INTERN : Py Funceble . INTERN [ "mined" ] = { } if Py Funceble . path . isfile ( self . file ) : # Our backup file exist. # We return the information from our backup. data = Dict ( ) . from json ( File ( self . file ) . read ( ) ) # We clean the empty elements. for file path in data : Py Funceble . INTERN [ "mined" ] [ file path ] = { } for element in data [ file path ] : if data [ file path ] [ element ] : Py Funceble . INTERN [ "mined" ] [ file path ] [ element ] = data [ file path ] [ element ] return # * The mining is not activated. # or # * Our backup file does not exist. # We return nothing. Py Funceble . INTERN [ "mined" ] = { } return
def backup ( self ) : if Py Funceble . CONFIGURATION [ "mining" ] : # The mining is activated. # We backup our mined informations. Dict ( Py Funceble . INTERN [ "mined" ] ) . to json ( self . file )
def process ( self ) : # pragma: no cover if Py Funceble . CONFIGURATION [ "mining" ] : # The mining is activated. # We load the mining logic. mined = self . mine ( ) if mined : # The mined data is not empty or None. # We add the mined data to the global database. self . add ( mined ) # And we finally backup everything. self . backup ( )
def json print ( self ) : # pragma: no cover if self . output : # The given output is not empty. if Py Funceble . path . isfile ( self . output ) : # The given output already exist. # We get the content of the output. content = Dict ( ) . from json ( File ( self . output ) . read ( ) ) if isinstance ( content , list ) : # The content is a list. # We extend the content with our data to print. content . extend ( self . data to print ) # We format our list. content = List ( content ) . custom format ( Sort . standard ) if Py Funceble . CONFIGURATION [ "hierarchical sorting" ] : # The hierarchical sorting is activated. # We format our content hierarchicaly content = List ( content ) . custom format ( Sort . hierarchical ) # We finally save our content into the file. Dict ( content ) . to json ( self . output ) else : # The content is not a list. # We raise an exception. raise Exception ( "Output not correctly formatted." ) else : # The given output does not already exist. # We save our data to print into the output. # # Note: We do not have to take care if self.data to print is a list # formatted or not because this method should not be called if it is # not the case. Dict ( self . data to print ) . to json ( self . output ) else : # The given output is empty. # We raise an exception. raise Exception ( "Empty output given." )
def file to delete ( cls ) : # We initiate the directory we have to look for. directory = Py Funceble . OUTPUT DIRECTORY + Py Funceble . OUTPUTS [ "parent directory" ] if not directory . endswith ( Py Funceble . directory separator ) : # pragma: no cover # For safety, if it does not ends with the directory separator, we append it # to its end. directory += Py Funceble . directory separator # We initiate a variable which will save the list of file to delete. result = [ ] for root , , files in Py Funceble . walk ( directory ) : # We walk in the directory and get all files and sub-directories. for file in files : # If there is files in the current sub-directory, we loop # through the list of files. if file not in [ ".gitignore" , ".keep" ] : # The file is not into our list of file we do not have to delete. if root . endswith ( Py Funceble . directory separator ) : # The root ends with the directory separator. # We construct the path and append the full path to the result. result . append ( root + file ) else : # The root directory does not ends with the directory separator. # We construct the path by appending the directory separator # between the root and the filename and append the full path to # the result. result . append ( root + Py Funceble . directory separator + file ) # pragma: no cover # We return our list of file to delete. return result
def databases to delete ( cls ) : # pragma: no cover # We initiate the directory we have to look for. directory = Py Funceble . CURRENT DIRECTORY # We initate the result variable. result = [ ] # We append the dir structure file. result . append ( directory + Py Funceble . CONFIGURATION [ "outputs" ] [ "default files" ] [ "dir structure" ] ) # We append the iana file. result . append ( directory + Py Funceble . CONFIGURATION [ "outputs" ] [ "default files" ] [ "iana" ] ) # We append the public suffix file. result . append ( directory + Py Funceble . CONFIGURATION [ "outputs" ] [ "default files" ] [ "public suffix" ] ) # We append the inactive database file. result . append ( directory + Py Funceble . CONFIGURATION [ "outputs" ] [ "default files" ] [ "inactive db" ] ) # We append the mining database file. result . append ( directory + Py Funceble . CONFIGURATION [ "outputs" ] [ "default files" ] [ "mining" ] ) # We append the whois database file. result . append ( directory + Py Funceble . CONFIGURATION [ "outputs" ] [ "default files" ] [ "whois db" ] ) return result
def get ( self ) : # We initiate a variable which will save the result we are going # to return. result = { } if self . algorithm in self . valid algorithms : # * The parsed path exist. # and # * The parsed algorithm is in the list of valid algorithms. if self . algorithm == "all" : # The parsed algorithm is `all`. # We remove `all` (the first element of the list) from # the list of valid algorithms because we are going to # loop through the list of valid algorithms. del self . valid algorithms [ 0 ] for algo in self . valid algorithms : # We loop through the list of valid algorithms. if self . path and path . isfile ( self . path ) : # The file path exist. # We save the hash into the result variable. result [ algo ] = self . hash file ( algo ) elif self . data : # * The path does not exist. # and # * The given data is not empty. # We save the hash into the result variable. result [ algo ] = self . hash data ( algo ) else : # pragma: no cover # All other case are met. # We return None. return None else : # The parsed algorithm is a specific one. if self . path and path . isfile ( self . path ) : # The file path exist. # We save the hash into the result variable. result [ self . algorithm ] = self . hash file ( self . algorithm ) elif self . data : # * The path does not exist. # and # * The given data is not empty. # We save the hash into the result variable. result [ self . algorithm ] = self . hash data ( self . algorithm ) else : # All the other case are met. # We return None. return None else : # pragma: no cover # The parsed algorithm is not in the list of valid algorithms. return None if self . algorithm != "all" and self . only hash : # * The parsed algorithm is not equal to `all`. # and # * We only have to return the selected hash. # We return the selected algorithm. return result [ self . algorithm ] # * The parsed algorithm is equal to `all`. # or # * We do not have to return the selected hash. # We return all hashes. return result
def count ( self ) : if self . status : # The status is parsed. # We increase the number of tested. Py Funceble . INTERN [ "counter" ] [ "number" ] [ "tested" ] += 1 if ( self . status . lower ( ) in Py Funceble . STATUS [ "list" ] [ "up" ] or self . status . lower ( ) in Py Funceble . STATUS [ "list" ] [ "valid" ] ) : # The status is in the list of up status. # We increase the number of up. Py Funceble . INTERN [ "counter" ] [ "number" ] [ "up" ] += 1 elif self . status . lower ( ) in Py Funceble . STATUS [ "list" ] [ "down" ] : # The status is in the list of down status. # We increase the number of down. Py Funceble . INTERN [ "counter" ] [ "number" ] [ "down" ] += 1 else : # The status is not in the list of up nor down status. # We increase the number of invalid. Py Funceble . INTERN [ "counter" ] [ "number" ] [ "invalid" ] += 1
def calculate ( cls ) : # We map the current state/counters of the different status. percentages = { "up" : Py Funceble . INTERN [ "counter" ] [ "number" ] [ "up" ] , "down" : Py Funceble . INTERN [ "counter" ] [ "number" ] [ "down" ] , "invalid" : Py Funceble . INTERN [ "counter" ] [ "number" ] [ "invalid" ] , } for percentage in percentages : # We loop through our map index. # We calculate the percentage. calculation = ( percentages [ percentage ] * 100 // Py Funceble . INTERN [ "counter" ] [ "number" ] [ "tested" ] ) # And we update the percentage counter of the actual status. Py Funceble . INTERN [ "counter" ] [ "percentage" ] . update ( { percentage : calculation } )
def log ( self ) : if ( Py Funceble . CONFIGURATION [ "show percentage" ] and Py Funceble . INTERN [ "counter" ] [ "number" ] [ "tested" ] > 0 ) : # * We are allowed to show the percentage on screen. # and # * The number of tested is greater than 0. # We initiate the output file. output = ( Py Funceble . OUTPUT DIRECTORY + Py Funceble . OUTPUTS [ "parent directory" ] + Py Funceble . OUTPUTS [ "logs" ] [ "directories" ] [ "parent" ] + Py Funceble . OUTPUTS [ "logs" ] [ "directories" ] [ "percentage" ] + Py Funceble . OUTPUTS [ "logs" ] [ "filenames" ] [ "percentage" ] ) # We delete the output file if it does exist. File ( output ) . delete ( ) # We calculate the percentage of each statuses. self . calculate ( ) if not Py Funceble . CONFIGURATION [ "quiet" ] : # The quiet mode is activated. # We print a new line. print ( "\n" ) # We print the percentage header on file and screen. Prints ( None , "Percentage" , output ) . header ( ) # We construct the different lines/data to print on screen and file. lines to print = [ [ Py Funceble . STATUS [ "official" ] [ "up" ] , str ( Py Funceble . INTERN [ "counter" ] [ "percentage" ] [ "up" ] ) + "%" , Py Funceble . INTERN [ "counter" ] [ "number" ] [ "up" ] , ] , [ Py Funceble . STATUS [ "official" ] [ "down" ] , str ( Py Funceble . INTERN [ "counter" ] [ "percentage" ] [ "down" ] ) + "%" , Py Funceble . INTERN [ "counter" ] [ "number" ] [ "down" ] , ] , [ Py Funceble . STATUS [ "official" ] [ "invalid" ] , str ( Py Funceble . INTERN [ "counter" ] [ "percentage" ] [ "invalid" ] ) + "%" , Py Funceble . INTERN [ "counter" ] [ "number" ] [ "invalid" ] , ] , ] if Py Funceble . CONFIGURATION [ "syntax" ] : # We are checking for syntax. # We update the denomination of the UP. lines to print [ 0 ] [ 0 ] = Py Funceble . STATUS [ "official" ] [ "valid" ] # And we unset the INACTIVE line. del lines to print [ 1 ] for to print in lines to print : # We loop throught the different line to print. # (one line for each status.) # And we print the current status line on file and screen. Prints ( to print , "Percentage" , output ) . data ( ) elif Py Funceble . INTERN [ "counter" ] [ "number" ] [ "tested" ] > 0 : # * We are not allowed to show the percentage on screen. # but # * The number of tested is greater than 0. # We run the calculation. # Note: The following is needed, because all counter calculation are # done by this class. self . calculate ( )
def reformat historical formating error ( self ) : # pragma: no cover if Py Funceble . CONFIGURATION [ "inactive database" ] : # The database subsystem is activated. # We construct the possible path to an older version of the database. historical formating error = ( Py Funceble . CURRENT DIRECTORY + "inactive-db.json" ) if Py Funceble . path . isfile ( historical formating error ) : # The histortical file already exists. # We get its content. data = Dict ( ) . from json ( File ( historical formating error ) . read ( ) ) # We initiate a variable which will save the data that is going # to be merged. data to parse = { } # We get the database keybase. top keys = data . keys ( ) for top key in top keys : # We loop through the list of upper keys. # We get the lowest keys. low keys = data [ top key ] . keys ( ) # We initiate the data to parse. data to parse [ top key ] = { } for low key in low keys : # We loop through the list of lower keys. if low key . isdigit ( ) : # The current low key is a digit. # We parse its content (from the old) into the new format. # In between, we remove 30 days from the low key so that # it become in the past. This way they will be retested # automatically. data to parse [ top key ] [ int ( low key ) - ( self . one day in seconds * 30 ) ] = data [ top key ] [ low key ] else : # The current low key is not a digit. # We parse its content (from the old) into the new format. # In between, we remove 30 days from the current time so that # it become in the past. This way they will be retested # automatically. data to parse [ top key ] [ int ( Py Funceble . time ( ) ) - ( self . one day in seconds * 30 ) ] = data [ top key ] [ low key ] if "inactive db" in Py Funceble . INTERN : # The current (new) database is not empty. # We update add the content of the old into the current database. Py Funceble . INTERN [ "inactive db" ] . update ( data to parse ) else : # The current (new) database is empty. # We replace the content with the data to parse as it is complient # with the new format. Py Funceble . INTERN [ "inactive db" ] = data to parse # We delete the old database file. File ( historical formating error ) . delete ( )
def retrieve ( self ) : if Py Funceble . CONFIGURATION [ "inactive database" ] : # The database subsystem is activated. # We get, format and initiate the historical database file. self . reformat historical formating error ( ) if Py Funceble . path . isfile ( self . inactive db path ) : # The database file exist. # We merge our current database into already initiated one. self . merge ( )
def backup ( self ) : if Py Funceble . CONFIGURATION [ "inactive database" ] : # The database subsystem is activated. # We save the current database state into the database file. Dict ( Py Funceble . INTERN [ "inactive db" ] ) . to json ( self . inactive db path )
def is present ( cls ) : if Py Funceble . CONFIGURATION [ "inactive database" ] : # The database subsystem is activated. if Py Funceble . INTERN [ "to test" ] in Py Funceble . INTERN [ "flatten inactive db" ] or ( Py Funceble . INTERN [ "file to test" ] in Py Funceble . INTERN [ "inactive db" ] and Py Funceble . INTERN [ "inactive db" ] [ Py Funceble . INTERN [ "file to test" ] ] and "to test" in Py Funceble . INTERN [ "inactive db" ] [ Py Funceble . INTERN [ "file to test" ] ] and Py Funceble . INTERN [ "to test" ] in Py Funceble . INTERN [ "inactive db" ] [ Py Funceble . INTERN [ "file to test" ] ] [ "to test" ] ) : return True return False
def retrieve ( self ) : if self . authorization ( ) and "whois db" not in Py Funceble . INTERN : # The usage of the whois database is activated. if Py Funceble . path . isfile ( self . whois db path ) : # The database file exist. # We merge our current database into already initiated one. Py Funceble . INTERN [ "whois db" ] = Dict ( ) . from json ( File ( self . whois db path ) . read ( ) ) else : # The database file does not exist. # We initiate an empty database. Py Funceble . INTERN [ "whois db" ] = { }
def backup ( self ) : if self . authorization ( ) : # We are authorized to work. # We backup the current state of the datbase. Dict ( Py Funceble . INTERN [ "whois db" ] ) . to json ( self . whois db path )
def is in database ( self ) : if ( self . authorization ( ) and Py Funceble . INTERN [ "file to test" ] in Py Funceble . INTERN [ "whois db" ] and Py Funceble . INTERN [ "to test" ] in Py Funceble . INTERN [ "whois db" ] [ Py Funceble . INTERN [ "file to test" ] ] ) : # * We are authorized to work. # and # * The given file path exist in the database. # and # * The element we are testing is in the database related to the # given file path. # We return True, the element we are testing is into the database. return True # * We are not authorized to work. # or # * The given file path does not exist in the database. # or # * The element we are testing is not in the database related to the # given file path. # We return False,the element we are testing is not into the database. return False
def is time older ( self ) : if ( self . authorization ( ) and self . is in database ( ) and int ( Py Funceble . INTERN [ "whois db" ] [ Py Funceble . INTERN [ "file to test" ] ] [ Py Funceble . INTERN [ "to test" ] ] [ "epoch" ] ) < int ( Py Funceble . time ( ) ) ) : # * We are authorized to work. # and # * The element we are testing is in the database. # and # * The epoch of the expiration date is less than our current epoch. # The expiration date is in the past, we return True. return True # The expiration date is in the future, we return False. return False
def add ( self ) : if self . authorization ( ) : # We are authorized to work. if self . epoch < int ( Py Funceble . time ( ) ) : state = "past" else : state = "future" if self . is in database ( ) : # The element we are working with is in the database. if ( str ( self . epoch ) != Py Funceble . INTERN [ "whois db" ] [ Py Funceble . INTERN [ "file to test" ] ] [ Py Funceble . INTERN [ "to test" ] ] [ "epoch" ] ) : # The given epoch is diffent from the one saved. # We update it. Py Funceble . INTERN [ "whois db" ] [ Py Funceble . INTERN [ "file to test" ] ] [ Py Funceble . INTERN [ "to test" ] ] . update ( { "epoch" : str ( self . epoch ) , "state" : state , "expiration date" : self . expiration date , } ) elif self . is time older ( ) : # The expiration date from the database is in the past. if ( Py Funceble . INTERN [ "whois db" ] [ Py Funceble . INTERN [ "file to test" ] ] [ Py Funceble . INTERN [ "to test" ] ] [ "state" ] != "past" ) : # pragma: no cover # The state of the element in the datbase is not # equal to `past`. # We update it to `past`. Py Funceble . INTERN [ "whois db" ] [ Py Funceble . INTERN [ "file to test" ] ] [ Py Funceble . INTERN [ "to test" ] ] . update ( { "state" : "past" } ) elif ( Py Funceble . INTERN [ "whois db" ] [ Py Funceble . INTERN [ "file to test" ] ] [ Py Funceble . INTERN [ "to test" ] ] [ "state" ] != "future" ) : # * The expiration date from the database is in the future. # and # * The state of the element in the database is not # equal to `future`. # We update it to `future`. Py Funceble . INTERN [ "whois db" ] [ Py Funceble . INTERN [ "file to test" ] ] [ Py Funceble . INTERN [ "to test" ] ] . update ( { "state" : "future" } ) else : # The element we are working with is not in the database. if ( not Py Funceble . INTERN [ "file to test" ] in Py Funceble . INTERN [ "whois db" ] ) : # The file path is not in the database. # We initiate it. Py Funceble . INTERN [ "whois db" ] [ Py Funceble . INTERN [ "file to test" ] ] = { } # We create the first dataset. Py Funceble . INTERN [ "whois db" ] [ Py Funceble . INTERN [ "file to test" ] ] . update ( { Py Funceble . INTERN [ "to test" ] : { "epoch" : str ( self . epoch ) , "state" : state , "expiration date" : self . expiration date , } } ) # We do a safety backup of our database. self . backup ( )
def travis permissions ( cls ) : if Py Funceble . CONFIGURATION [ "travis" ] : try : build dir = Py Funceble . environ [ "TRAVIS BUILD DIR" ] commands = [ "sudo chown -R travis:travis %s" % ( build dir ) , "sudo chgrp -R travis %s" % ( build dir ) , "sudo chmod -R g+rw X %s" % ( build dir ) , "sudo chmod 777 -Rf %s.git" % ( build dir + Py Funceble . directory separator ) , r"sudo find %s -type d -exec chmod g+x '{}' \;" % ( build dir ) , ] for command in commands : Command ( command ) . execute ( ) if Command ( "git config core.shared Repository" ) . execute ( ) == "" : Command ( "git config core.shared Repository group" ) . execute ( ) except Key Error : pass
def travis ( self ) : if Py Funceble . CONFIGURATION [ "travis" ] : try : = Py Funceble . environ [ "TRAVIS BUILD DIR" ] time autorisation = False try : time autorisation = int ( Py Funceble . time ( ) ) >= int ( Py Funceble . INTERN [ "start" ] ) + ( int ( Py Funceble . CONFIGURATION [ "travis autosave minutes" ] ) * 60 ) except Key Error : if self . last and not self . bypass : raise Exception ( "Please review the way `Execution Time()` is called." ) if self . last or time autorisation or self . bypass : Percentage ( ) . log ( ) self . travis permissions ( ) command = 'git add --all && git commit -a -m "%s"' if self . last or self . bypass : if Py Funceble . CONFIGURATION [ "command before end" ] : for line in Command ( Py Funceble . CONFIGURATION [ "command before end" ] ) . run ( ) : sys stdout . write ( "{}\n" . format ( line ) ) self . travis permissions ( ) message = ( Py Funceble . CONFIGURATION [ "travis autosave final commit" ] + " [ci skip]" ) Command ( command % message ) . execute ( ) else : if Py Funceble . CONFIGURATION [ "command" ] : for line in Command ( Py Funceble . CONFIGURATION [ "command" ] ) . run ( ) : sys stdout . write ( "{}\n" . format ( line ) ) self . travis permissions ( ) Command ( command % Py Funceble . CONFIGURATION [ "travis autosave commit" ] ) . execute ( ) print ( Command ( "git push origin %s" % Py Funceble . CONFIGURATION [ "travis branch" ] ) . execute ( ) ) exit ( 0 ) except Key Error : pass
def nslookup ( cls ) : try : # We try to get the addresse information of the given domain or IP. if "current test data" in Py Funceble . INTERN : # pragma: no cover # The end-user want more information whith his test. if not Check ( ) . is ip valid ( ) : # The element we are testing is not an IP. # We request the address informations. request = Py Funceble . socket . getaddrinfo ( Py Funceble . INTERN [ "to test" ] , 80 , 0 , 0 , Py Funceble . socket . IPPROTO TCP , ) for sequence in request : # We loop through the sequence returned by the request. # We append the NS informations into the nslookup index. Py Funceble . INTERN [ "current test data" ] [ "nslookup" ] . append ( sequence [ - 1 ] [ 0 ] ) else : # The element we are testing is an IP. request = Py Funceble . socket . gethostbyaddr ( Py Funceble . INTERN [ "to test" ] ) # We append the NS informations into the nslookup index. Py Funceble . INTERN [ "current test data" ] [ "nslookup" ] [ "hostname" ] = request [ 0 ] Py Funceble . INTERN [ "current test data" ] [ "nslookup" ] [ "aliases" ] = request [ 1 ] Py Funceble . INTERN [ "current test data" ] [ "nslookup" ] [ "ips" ] = request [ 2 ] else : if not Check ( ) . is ip valid ( ) : # The element we are testing is not an IP. Py Funceble . socket . getaddrinfo ( Py Funceble . INTERN [ "to test" ] , 80 , 0 , 0 , Py Funceble . socket . IPPROTO TCP , ) else : # The element we are testing is an IP. Py Funceble . socket . gethostbyaddr ( Py Funceble . INTERN [ "to test" ] ) # It was done successfuly, we return True. # Note: we don't need to read the addresses so we consider as successful # as long as there is no error. return True except ( OS Error , Py Funceble . socket . herror , Py Funceble . socket . gaierror ) : # One of the listed exception is matched. # It was done unsuccesfuly, we return False. return False
def get ( self ) : if not Py Funceble . CONFIGURATION [ "local" ] : # We are not running a test in a local network. if self . domain extension not in self . ignored extension : # The extension of the domain we are testing is not into # the list of ignored extensions. # We set the referer to None as we do not have any. referer = None if self . domain extension in Py Funceble . INTERN [ "iana db" ] : # The domain extension is in the iana database. if not Py Funceble . CONFIGURATION [ "no whois" ] : # We are authorized to use WHOIS for the test result. # We get the referer from the database. referer = Py Funceble . INTERN [ "iana db" ] [ self . domain extension ] if not referer : # The referer is not filled. # We log the case of the current extension. Logs ( ) . referer not found ( self . domain extension ) # And we handle and return None status. return None # The referer is into the database. # We return the extracted referer. return referer # We are not authorized to use WHOIS for the test result. # We return None. return None # The domain extension is not in the iana database. # We return False, it is an invalid domain. return False # The extension of the domain we are testing is not into # the list of ignored extensions. # We return None, the domain does not have a whois server. return None # We are running a test in a local network. # We return None. return None
def standard paths ( ) : for is plat spec in [ True , False ] : path = distutils . sysconfig . get python lib ( standard lib = True , plat specific = is plat spec ) for name in os . listdir ( path ) : yield name try : for name in os . listdir ( os . path . join ( path , 'lib-dynload' ) ) : yield name except OS Error : # pragma: no cover pass
def standard package names ( ) : for name in standard paths ( ) : if name . startswith ( ' ' ) or '-' in name : continue if '.' in name and name . rsplit ( '.' ) [ - 1 ] not in [ 'so' , 'py' , 'pyc' ] : continue yield name . split ( '.' ) [ 0 ]
def unused import line numbers ( messages ) : for message in messages : if isinstance ( message , pyflakes . messages . Unused Import ) : yield message . lineno
def unused import module name ( messages ) : pattern = r'\'(.+?)\'' for message in messages : if isinstance ( message , pyflakes . messages . Unused Import ) : module name = re . search ( pattern , str ( message ) ) module name = module name . group ( ) [ 1 : - 1 ] if module name : yield ( message . lineno , module name )
def star import used line numbers ( messages ) : for message in messages : if isinstance ( message , pyflakes . messages . Import Star Used ) : yield message . lineno
def star import usage undefined name ( messages ) : for message in messages : if isinstance ( message , pyflakes . messages . Import Star Usage ) : undefined name = message . message args [ 0 ] module name = message . message args [ 1 ] yield ( message . lineno , undefined name , module name )
def unused variable line numbers ( messages ) : for message in messages : if isinstance ( message , pyflakes . messages . Unused Variable ) : yield message . lineno
def duplicate key line numbers ( messages , source ) : messages = [ message for message in messages if isinstance ( message , pyflakes . messages . Multi Value Repeated Key Literal ) ] if messages : # Filter out complex cases. We don't want to bother trying to parse # this stuff and get it right. We can do it on a key-by-key basis. key to messages = create key to messages dict ( messages ) lines = source . split ( '\n' ) for ( key , messages ) in key to messages . items ( ) : good = True for message in messages : line = lines [ message . lineno - 1 ] key = message . message args [ 0 ] if not dict entry has key ( line , key ) : good = False if good : for message in messages : yield message . lineno
def create key to messages dict ( messages ) : dictionary = collections . defaultdict ( lambda : [ ] ) for message in messages : dictionary [ message . message args [ 0 ] ] . append ( message ) return dictionary
def check ( source ) : if sys . version info [ 0 ] == 2 and isinstance ( source , unicode ) : # Convert back to original byte string encoding, otherwise pyflakes # call to compile() will complain. See PEP 263. This only affects # Python 2. try : source = source . encode ( 'utf-8' ) except Unicode Error : # pragma: no cover return [ ] reporter = List Reporter ( ) try : pyflakes . api . check ( source , filename = '<string>' , reporter = reporter ) except ( Attribute Error , Recursion Error , Unicode Decode Error ) : pass return reporter . messages
def extract package name ( line ) : assert '\\' not in line assert '(' not in line assert ')' not in line assert ';' not in line if line . lstrip ( ) . startswith ( ( 'import' , 'from' ) ) : word = line . split ( ) [ 1 ] else : # Ignore doctests. return None package = word . split ( '.' ) [ 0 ] assert ' ' not in package return package
def multiline import ( line , previous line = '' ) : for symbol in '()' : if symbol in line : return True # Ignore doctests. if line . lstrip ( ) . startswith ( '>' ) : return True return multiline statement ( line , previous line )
def multiline statement ( line , previous line = '' ) : for symbol in '\\:;' : if symbol in line : return True sio = io . String IO ( line ) try : list ( tokenize . generate tokens ( sio . readline ) ) return previous line . rstrip ( ) . endswith ( '\\' ) except ( Syntax Error , tokenize . Token Error ) : return True
def break up import ( line ) : assert '\\' not in line assert '(' not in line assert ')' not in line assert ';' not in line assert '#' not in line assert not line . lstrip ( ) . startswith ( 'from' ) newline = get line ending ( line ) if not newline : return line ( indentation , imports ) = re . split ( pattern = r'\bimport\b' , string = line , maxsplit = 1 ) indentation += 'import ' assert newline return '' . join ( [ indentation + i . strip ( ) + newline for i in sorted ( imports . split ( ',' ) ) ] )
def filter code ( source , additional imports = None , expand star imports = False , remove all unused imports = False , remove duplicate keys = False , remove unused variables = False , ignore init module imports = False , ) : imports = SAFE IMPORTS if additional imports : imports |= frozenset ( additional imports ) del additional imports messages = check ( source ) if ignore init module imports : marked import line numbers = frozenset ( ) else : marked import line numbers = frozenset ( unused import line numbers ( messages ) ) marked unused module = collections . defaultdict ( lambda : [ ] ) for line number , module name in unused import module name ( messages ) : marked unused module [ line number ] . append ( module name ) if expand star imports and not ( # See explanations in #18. re . search ( r'\b all \b' , source ) or re . search ( r'\bdel\b' , source ) ) : marked star import line numbers = frozenset ( star import used line numbers ( messages ) ) if len ( marked star import line numbers ) > 1 : # Auto expanding only possible for single star import marked star import line numbers = frozenset ( ) else : undefined names = [ ] for line number , undefined name , in star import usage undefined name ( messages ) : undefined names . append ( undefined name ) if not undefined names : marked star import line numbers = frozenset ( ) else : marked star import line numbers = frozenset ( ) if remove unused variables : marked variable line numbers = frozenset ( unused variable line numbers ( messages ) ) else : marked variable line numbers = frozenset ( ) if remove duplicate keys : marked key line numbers = frozenset ( duplicate key line numbers ( messages , source ) ) else : marked key line numbers = frozenset ( ) line messages = get messages by line ( messages ) sio = io . String IO ( source ) previous line = '' for line number , line in enumerate ( sio . readlines ( ) , start = 1 ) : if '#' in line : yield line elif line number in marked import line numbers : yield filter unused import ( line , unused module = marked unused module [ line number ] , remove all unused imports = remove all unused imports , imports = imports , previous line = previous line ) elif line number in marked variable line numbers : yield filter unused variable ( line ) elif line number in marked key line numbers : yield filter duplicate key ( line , line messages [ line number ] , line number , marked key line numbers , source ) elif line number in marked star import line numbers : yield filter star import ( line , undefined names ) else : yield line previous line = line
def get messages by line ( messages ) : line messages = { } for message in messages : line messages [ message . lineno ] = message return line messages
def filter star import ( line , marked star import undefined name ) : undefined name = sorted ( set ( marked star import undefined name ) ) return re . sub ( r'\*' , ', ' . join ( undefined name ) , line )
def filter unused import ( line , unused module , remove all unused imports , imports , previous line = '' ) : if multiline import ( line , previous line ) : return line is from import = line . lstrip ( ) . startswith ( 'from' ) if ',' in line and not is from import : return break up import ( line ) package = extract package name ( line ) if not remove all unused imports and package not in imports : return line if ',' in line : assert is from import return filter from import ( line , unused module ) else : # We need to replace import with "pass" in case the import is the # only line inside a block. For example, # "if True:\n    import os". In such cases, if the import is # removed, the block will be left hanging with no body. return ( get indentation ( line ) + 'pass' + get line ending ( line ) )
def filter unused variable ( line , previous line = '' ) : if re . match ( EXCEPT REGEX , line ) : return re . sub ( r' as \w+:$' , ':' , line , count = 1 ) elif multiline statement ( line , previous line ) : return line elif line . count ( '=' ) == 1 : split line = line . split ( '=' ) assert len ( split line ) == 2 value = split line [ 1 ] . lstrip ( ) if ',' in split line [ 0 ] : return line if is literal or name ( value ) : # Rather than removing the line, replace with it "pass" to avoid # a possible hanging block with no body. value = 'pass' + get line ending ( line ) return get indentation ( line ) + value else : return line
def filter duplicate key ( line , message , line number , marked line numbers , source , previous line = '' ) : if marked line numbers and line number == sorted ( marked line numbers ) [ 0 ] : return '' return line
def is literal or name ( value ) : try : ast . literal eval ( value ) return True except ( Syntax Error , Value Error ) : pass if value . strip ( ) in [ 'dict()' , 'list()' , 'set()' ] : return True # Support removal of variables on the right side. But make sure # there are no dots, which could mean an access of a property. return re . match ( r'^\w+\s*$' , value )
def useless pass line numbers ( source ) : sio = io . String IO ( source ) previous token type = None last pass row = None last pass indentation = None previous line = '' for token in tokenize . generate tokens ( sio . readline ) : token type = token [ 0 ] start row = token [ 2 ] [ 0 ] line = token [ 4 ] is pass = ( token type == tokenize . NAME and line . strip ( ) == 'pass' ) # Leading "pass". if ( start row - 1 == last pass row and get indentation ( line ) == last pass indentation and token type in ATOMS and not is pass ) : yield start row - 1 if is pass : last pass row = start row last pass indentation = get indentation ( line ) # Trailing "pass". if ( is pass and previous token type != tokenize . INDENT and not previous line . rstrip ( ) . endswith ( '\\' ) ) : yield start row previous token type = token type previous line = line
def filter useless pass ( source ) : try : marked lines = frozenset ( useless pass line numbers ( source ) ) except ( Syntax Error , tokenize . Token Error ) : marked lines = frozenset ( ) sio = io . String IO ( source ) for line number , line in enumerate ( sio . readlines ( ) , start = 1 ) : if line number not in marked lines : yield line
def get indentation ( line ) : if line . strip ( ) : non whitespace index = len ( line ) - len ( line . lstrip ( ) ) return line [ : non whitespace index ] else : return ''
def get line ending ( line ) : non whitespace index = len ( line . rstrip ( ) ) - len ( line ) if not non whitespace index : return '' else : return line [ non whitespace index : ]
def fix code ( source , additional imports = None , expand star imports = False , remove all unused imports = False , remove duplicate keys = False , remove unused variables = False , ignore init module imports = False ) : if not source : return source # pyflakes does not handle "nonlocal" correctly. if 'nonlocal' in source : remove unused variables = False filtered source = None while True : filtered source = '' . join ( filter useless pass ( '' . join ( filter code ( source , additional imports = additional imports , expand star imports = expand star imports , remove all unused imports = remove all unused imports , remove duplicate keys = remove duplicate keys , remove unused variables = remove unused variables , ignore init module imports = ignore init module imports , ) ) ) ) if filtered source == source : break source = filtered source return filtered source
def detect encoding ( filename , limit byte check = - 1 ) : try : with open ( filename , 'rb' ) as input file : encoding = detect encoding ( input file . readline ) # Check for correctness of encoding. with open with encoding ( filename , encoding ) as input file : input file . read ( limit byte check ) return encoding except ( Lookup Error , Syntax Error , Unicode Decode Error ) : return 'latin-1'
def detect encoding ( readline ) : try : from lib2to3 . pgen2 import tokenize as lib2to3 tokenize encoding = lib2to3 tokenize . detect encoding ( readline ) [ 0 ] return encoding except ( Lookup Error , Syntax Error , Unicode Decode Error ) : return 'latin-1'
def split comma separated ( string ) : return set ( text . strip ( ) for text in string . split ( ',' ) if text . strip ( ) )
def is python file ( filename ) : if filename . endswith ( '.py' ) : return True try : with open with encoding ( filename , None , limit byte check = MAX PYTHON FILE DETECTION BYTES ) as f : text = f . read ( MAX PYTHON FILE DETECTION BYTES ) if not text : return False first line = text . splitlines ( ) [ 0 ] except ( IO Error , Index Error ) : return False if not PYTHON SHEBANG REGEX . match ( first line ) : return False return True
def is exclude file ( filename , exclude ) : base name = os . path . basename ( filename ) if base name . startswith ( '.' ) : return True for pattern in exclude : if fnmatch . fnmatch ( base name , pattern ) : return True if fnmatch . fnmatch ( filename , pattern ) : return True return False
def create ( cls , name value , name type ) : if isinstance ( name value , Name . Name Value ) : value = name value elif isinstance ( name value , str ) : value = cls . Name Value ( name value ) else : name = 'Name' msg = exceptions . Error Strings . BAD EXP RECV member = 'name value' raise Type Error ( msg . format ( '{0}.{1}' . format ( name , member ) , 'name value' , type ( Name . Name Value ) , type ( name value ) ) ) if isinstance ( name type , Name . Name Type ) : n type = name type elif isinstance ( name type , Enum ) : n type = cls . Name Type ( name type ) else : name = 'Name' msg = exceptions . Error Strings . BAD EXP RECV member = 'name type' raise Type Error ( msg . format ( '{0}.{1}' . format ( name , member ) , 'name type' , type ( Name . Name Type ) , type ( name type ) ) ) return Name ( name value = value , name type = n type )
def get attribute from managed object ( self , managed object , attr name ) : if attr name == 'Unique Identifier' : return str ( managed object . unique identifier ) elif attr name == 'Name' : names = list ( ) for name in managed object . names : name = attributes . Name ( attributes . Name . Name Value ( name ) , attributes . Name . Name Type ( enums . Name Type . UNINTERPRETED TEXT STRING ) ) names . append ( name ) return names elif attr name == 'Object Type' : return managed object . object type elif attr name == 'Cryptographic Algorithm' : return managed object . cryptographic algorithm elif attr name == 'Cryptographic Length' : return managed object . cryptographic length elif attr name == 'Cryptographic Parameters' : return None elif attr name == 'Cryptographic Domain Parameters' : return None elif attr name == 'Certificate Type' : return managed object . certificate type elif attr name == 'Certificate Length' : return None elif attr name == 'X.509 Certificate Identifier' : return None elif attr name == 'X.509 Certificate Subject' : return None elif attr name == 'X.509 Certificate Issuer' : return None elif attr name == 'Certificate Identifier' : return None elif attr name == 'Certificate Subject' : return None elif attr name == 'Certificate Issuer' : return None elif attr name == 'Digital Signature Algorithm' : return None elif attr name == 'Digest' : return None elif attr name == 'Operation Policy Name' : return managed object . operation policy name elif attr name == 'Cryptographic Usage Mask' : return managed object . cryptographic usage masks elif attr name == 'Lease Time' : return None elif attr name == 'Usage Limits' : return None elif attr name == 'State' : return managed object . state elif attr name == 'Initial Date' : return managed object . initial date elif attr name == 'Activation Date' : return None elif attr name == 'Process Start Date' : return None elif attr name == 'Protect Stop Date' : return None elif attr name == 'Deactivation Date' : return None elif attr name == 'Destroy Date' : return None elif attr name == 'Compromise Occurrence Date' : return None elif attr name == 'Compromise Date' : return None elif attr name == 'Revocation Reason' : return None elif attr name == 'Archive Date' : return None elif attr name == 'Object Group' : return None elif attr name == 'Fresh' : return None elif attr name == 'Link' : return None elif attr name == 'Application Specific Information' : return None elif attr name == 'Contact Information' : return None elif attr name == 'Last Change Date' : return None else : # Since custom attribute names are possible, just return None # for unrecognized attributes. This satisfies the spec. return None
def set attribute on managed object ( self , managed object , attribute ) : attribute name = attribute [ 0 ] attribute value = attribute [ 1 ] if self . attribute policy . is attribute multivalued ( attribute name ) : if attribute name == 'Name' : managed object . names . extend ( [ x . name value . value for x in attribute value ] ) for name in managed object . names : if managed object . names . count ( name ) > 1 : raise exceptions . Invalid Field ( "Cannot set duplicate name values." ) else : # TODO (peterhamilton) Remove when all attributes are supported raise exceptions . Invalid Field ( "The {0} attribute is unsupported." . format ( attribute name ) ) else : field = None value = attribute value . value if attribute name == 'Cryptographic Algorithm' : field = 'cryptographic algorithm' elif attribute name == 'Cryptographic Length' : field = 'cryptographic length' elif attribute name == 'Cryptographic Usage Mask' : field = 'cryptographic usage masks' value = list ( ) for e in enums . Cryptographic Usage Mask : if e . value & attribute value . value : value . append ( e ) elif attribute name == 'Operation Policy Name' : field = 'operation policy name' if field : existing value = getattr ( managed object , field ) if existing value : if existing value != value : raise exceptions . Invalid Field ( "Cannot overwrite the {0} attribute." . format ( attribute name ) ) else : setattr ( managed object , field , value ) else : # TODO (peterhamilton) Remove when all attributes are supported raise exceptions . Invalid Field ( "The {0} attribute is unsupported." . format ( attribute name ) )
def validate ( self ) : if self . unique identifier is not None : if not isinstance ( self . unique identifier , attributes . Unique Identifier ) : msg = "invalid unique identifier" raise Type Error ( msg ) if self . compromise occurrence date is not None : if not isinstance ( self . compromise occurrence date , primitives . Date Time ) : msg = "invalid compromise time" raise Type Error ( msg ) if not isinstance ( self . revocation reason , objects . Revocation Reason ) : msg = "invalid revocation reason" raise Type Error ( msg )
def key wrapping data ( self , value ) : if value is None : value = { } elif not isinstance ( value , dict ) : raise Type Error ( "Key wrapping data must be a dictionary." ) self . kdw wrapping method = value . get ( 'wrapping method' ) eki = value . get ( 'encryption key information' ) if eki is None : eki = { } self . kdw eki unique identifier = eki . get ( 'unique identifier' ) eki cp = eki . get ( 'cryptographic parameters' ) if eki cp is None : eki cp = { } self . kdw eki cp block cipher mode = eki cp . get ( 'block cipher mode' ) self . kdw eki cp padding method = eki cp . get ( 'padding method' ) self . kdw eki cp hashing algorithm = eki cp . get ( 'hashing algorithm' ) self . kdw eki cp key role type = eki cp . get ( 'key role type' ) self . kdw eki cp digital signature algorithm = eki cp . get ( 'digital signature algorithm' ) self . kdw eki cp cryptographic algorithm = eki cp . get ( 'cryptographic algorithm' ) self . kdw eki cp random iv = eki cp . get ( 'random iv' ) self . kdw eki cp iv length = eki cp . get ( 'iv length' ) self . kdw eki cp tag length = eki cp . get ( 'tag length' ) self . kdw eki cp fixed field length = eki cp . get ( 'fixed field length' ) self . kdw eki cp invocation field length = eki cp . get ( 'invocation field length' ) self . kdw eki cp counter length = eki cp . get ( 'counter length' ) self . kdw eki cp initial counter value = eki cp . get ( 'initial counter value' ) mski = value . get ( 'mac signature key information' ) if mski is None : mski = { } self . kdw mski unique identifier = mski . get ( 'unique identifier' ) mski cp = mski . get ( 'cryptographic parameters' ) if mski cp is None : mski cp = { } self . kdw mski cp block cipher mode = mski cp . get ( 'block cipher mode' ) self . kdw mski cp padding method = mski cp . get ( 'padding method' ) self . kdw mski cp hashing algorithm = mski cp . get ( 'hashing algorithm' ) self . kdw mski cp key role type = mski cp . get ( 'key role type' ) self . kdw mski cp digital signature algorithm = mski cp . get ( 'digital signature algorithm' ) self . kdw mski cp cryptographic algorithm = mski cp . get ( 'cryptographic algorithm' ) self . kdw mski cp random iv = mski cp . get ( 'random iv' ) self . kdw mski cp iv length = mski cp . get ( 'iv length' ) self . kdw mski cp tag length = mski cp . get ( 'tag length' ) self . kdw mski cp fixed field length = mski cp . get ( 'fixed field length' ) self . kdw mski cp invocation field length = mski cp . get ( 'invocation field length' ) self . kdw mski cp counter length = mski cp . get ( 'counter length' ) self . kdw mski cp initial counter value = mski cp . get ( 'initial counter value' ) self . kdw mac signature = value . get ( 'mac signature' ) self . kdw iv counter nonce = value . get ( 'iv counter nonce' ) self . kdw encoding option = value . get ( 'encoding option' )
def get json files ( p ) : f = [ os . path . join ( p , x ) for x in os . listdir ( p ) if x . endswith ( ".json" ) ] return sorted ( f )
def scan policies ( self ) : policy files = get json files ( self . policy directory ) for f in set ( policy files ) - set ( self . policy files ) : self . file timestamps [ f ] = 0 for f in set ( self . policy files ) - set ( policy files ) : self . logger . info ( "Removing policies for file: {}" . format ( f ) ) self . file timestamps . pop ( f , None ) for p in self . policy cache . keys ( ) : self . disassociate policy and file ( p , f ) for p in [ k for k , v in self . policy map . items ( ) if v == f ] : self . restore or delete policy ( p ) self . policy files = policy files for f in sorted ( self . file timestamps . keys ( ) ) : t = os . path . getmtime ( f ) if t > self . file timestamps [ f ] : self . logger . info ( "Loading policies for file: {}" . format ( f ) ) self . file timestamps [ f ] = t old p = [ k for k , v in self . policy map . items ( ) if v == f ] try : new p = operation policy . read policy from file ( f ) except Value Error : self . logger . error ( "Failure loading file: {}" . format ( f ) ) self . logger . debug ( "" , exc info = True ) continue for p in new p . keys ( ) : self . logger . info ( "Loading policy: {}" . format ( p ) ) if p in self . reserved policies : self . logger . warning ( "Policy '{}' overwrites a reserved policy and " "will be thrown out." . format ( p ) ) continue if p in sorted ( self . policy store . keys ( ) ) : self . logger . debug ( "Policy '{}' overwrites an existing " "policy." . format ( p ) ) if f != self . policy map . get ( p ) : self . policy cache . get ( p ) . append ( ( time . time ( ) , self . policy map . get ( p ) , self . policy store . get ( p ) ) ) else : self . policy cache [ p ] = [ ] self . policy store [ p ] = new p . get ( p ) self . policy map [ p ] = f for p in set ( old p ) - set ( new p . keys ( ) ) : self . disassociate policy and file ( p , f ) self . restore or delete policy ( p )
def run ( self ) : self . initialize tracking structures ( ) if self . live monitoring : self . logger . info ( "Starting up the operation policy file monitor." ) while not self . halt trigger . is set ( ) : time . sleep ( 1 ) self . scan policies ( ) self . logger . info ( "Stopping the operation policy file monitor." ) else : self . scan policies ( )
def get certificate from connection ( connection ) : certificate = connection . getpeercert ( binary form = True ) if certificate : return x509 . load der x509 certificate ( certificate , backends . default backend ( ) ) return None
def get common names from certificate ( certificate ) : common names = certificate . subject . get attributes for oid ( x509 . oid . Name OID . COMMON NAME ) return [ common name . value for common name in common names ]
def get client identity from certificate ( certificate ) : client ids = get common names from certificate ( certificate ) if len ( client ids ) > 0 : if len ( client ids ) > 1 : raise exceptions . Permission Denied ( "Multiple client identities found." ) return client ids [ 0 ] else : raise exceptions . Permission Denied ( "The certificate does not define any subject common names. " "Client identity unavailable." )
def validate ( self ) : if not isinstance ( self . revocation code , Revocation Reason Code ) : msg = "Revocation Reaon Code expected" raise Type Error ( msg ) if self . revocation message is not None : if not isinstance ( self . revocation message , Text String ) : msg = "Text String expect" raise Type Error ( msg )
def validate ( self ) : if self . unique identifier is not None : if not isinstance ( self . unique identifier , attributes . Unique Identifier ) : msg = "invalid unique identifier" raise Type Error ( msg )
def load ( self ) : mod = import module ( self . module name ) obj = mod if self . object name : for attr in self . object name . split ( '.' ) : obj = getattr ( obj , attr ) return obj
def generate controller ( args ) : controller template = os . path . join ( dirname ( abspath ( file ) ) , 'templates/controller.py' ) test template = os . path . join ( dirname ( abspath ( file ) ) , 'templates/unittest.py' ) controller name = args . get ( '<controller>' ) current path = os . getcwd ( ) logger . info ( 'Start generating controller.' ) if not controller name : logger . warning ( 'Controller name cannot be empty.' ) return # controller file with open ( controller template , 'r' ) as template file : controller file path = os . path . join ( current path , 'application/controllers' , controller name + '.py' ) with open ( controller file path , 'w+' ) as controller file : for line in template file : new line = line . replace ( '#{controller}' , controller name ) controller file . write ( new line ) logger . info ( "New: %s" % relative path ( controller file path ) ) # test file with open ( test template , 'r' ) as template file : test file path = os . path . join ( current path , 'tests' , 'test %s.py' % controller name ) with open ( test file path , 'w+' ) as test file : for line in template file : new line = line . replace ( '#{controller}' , controller name ) . replace ( '#{controller|title}' , controller name . title ( ) ) test file . write ( new line ) logger . info ( "New: %s" % relative path ( test file path ) ) # assets dir assets dir path = os . path . join ( current path , 'application/pages/%s' % controller name ) mkdir p ( assets dir path ) # form file generate form ( controller name ) logger . info ( 'Finish generating controller.' )
def mkdir p ( path ) : try : os . makedirs ( path ) except OS Error as exc : if exc . errno == errno . EEXIST and os . path . isdir ( path ) : pass else : raise else : logger . info ( "New: %s%s" , path , os . path . sep )
def rewrite and copy ( src file , dst file , project name ) : # Create temp file fh , abs path = mkstemp ( ) with io . open ( abs path , 'w' , encoding = 'utf-8' ) as new file : with io . open ( src file , 'r' , encoding = 'utf-8' ) as old file : for line in old file : new line = line . replace ( '#{project}' , project name ) . replace ( '#{project|title}' , project name . title ( ) ) new file . write ( new line ) # Copy to new file shutil . copy ( abs path , dst file ) os . close ( fh )
def check url ( form , field ) : url = field . data . strip ( ) if not url : return result = urlparse ( url ) if result . scheme == "" : field . data = "http://%s" % re . sub ( r'^:?/*' , '' , url )
def encode ( something ) : secret key = current app . config . get ( 'SECRET KEY' ) s = URL Safe Serializer ( secret key ) return s . dumps ( something )
def decode ( something ) : secret key = current app . config . get ( 'SECRET KEY' ) s = URL Safe Serializer ( secret key ) try : return s . loads ( something ) except Bad Signature : return None
def absolute url for ( endpoint , * * values ) : config = current app . config site domain = config . get ( 'SITE DOMAIN' ) relative url = url for ( endpoint , * * values ) return join url ( site domain , relative url )
def signin user ( user , permenent = True ) : session . permanent = permenent session [ 'user id' ] = user . id
def get current user ( ) : if not 'user id' in session : return None user = User . query . filter ( User . id == session [ 'user id' ] ) . first ( ) if not user : signout user ( ) return None return user
def create app ( ) : config = load config ( ) app = Flask ( name ) app . config . from object ( config ) # Proxy fix app . wsgi app = Proxy Fix ( app . wsgi app ) # CSRF protect Csrf Protect ( app ) if app . debug or app . testing : Debug Toolbar Extension ( app ) # Serve static files app . wsgi app = Shared Data Middleware ( app . wsgi app , { '/pages' : os . path . join ( app . config . get ( 'PROJECT PATH' ) , 'application/pages' ) } ) else : # Log errors to stderr in production mode app . logger . add Handler ( logging . Stream Handler ( ) ) app . logger . set Level ( logging . ERROR ) # Enable Sentry if app . config . get ( 'SENTRY DSN' ) : from . utils . sentry import sentry sentry . init app ( app , dsn = app . config . get ( 'SENTRY DSN' ) ) # Serve static files app . wsgi app = Shared Data Middleware ( app . wsgi app , { '/static' : os . path . join ( app . config . get ( 'PROJECT PATH' ) , 'output/static' ) , '/pkg' : os . path . join ( app . config . get ( 'PROJECT PATH' ) , 'output/pkg' ) , '/pages' : os . path . join ( app . config . get ( 'PROJECT PATH' ) , 'output/pages' ) } ) # Register components register db ( app ) register routes ( app ) register jinja ( app ) register error handle ( app ) register hooks ( app ) return app
def register jinja ( app ) : import jinja2 from . utils import filters , permissions , helpers if app . debug or app . testing : my loader = jinja2 . Choice Loader ( [ app . jinja loader , jinja2 . File System Loader ( [ os . path . join ( app . config . get ( 'PROJECT PATH' ) , 'application/macros' ) , os . path . join ( app . config . get ( 'PROJECT PATH' ) , 'application/pages' ) ] ) ] ) else : my loader = jinja2 . Choice Loader ( [ app . jinja loader , jinja2 . File System Loader ( [ os . path . join ( app . config . get ( 'PROJECT PATH' ) , 'output/macros' ) , os . path . join ( app . config . get ( 'PROJECT PATH' ) , 'output/pages' ) ] ) ] ) app . jinja loader = my loader app . jinja env . filters . update ( { 'timesince' : filters . timesince } ) def url for other page ( page ) : """Generate url for pagination.""" view args = request . view args . copy ( ) args = request . args . copy ( ) . to dict ( ) combined args = dict ( view args . items ( ) + args . items ( ) ) combined args [ 'page' ] = page return url for ( request . endpoint , * * combined args ) rules = { } for endpoint , rules in iteritems ( app . url map . rules by endpoint ) : if any ( item in endpoint for item in [ ' debug toolbar' , 'debugtoolbar' , 'static' ] ) : continue rules [ endpoint ] = [ { 'rule' : rule . rule } for rule in rules ] app . jinja env . globals . update ( { 'absolute url for' : helpers . absolute url for , 'url for other page' : url for other page , 'rules' : rules , 'permissions' : permissions } )
def register error handle ( app ) : @ app . errorhandler ( 403 ) def page 403 ( error ) : return render template ( 'site/403/403.html' ) , 403 @ app . errorhandler ( 404 ) def page 404 ( error ) : return render template ( 'site/404/404.html' ) , 404 @ app . errorhandler ( 500 ) def page 500 ( error ) : return render template ( 'site/500/500.html' ) , 500
def dataframe to csv ( writer , dataframe , delimiter , with header ) : encoding writer = codecs . getwriter ( 'utf-8' ) ( writer ) dataframe . to csv ( path or buf = encoding writer , sep = delimiter , header = with header , index = False )
def dataframe from csv ( reader , delimiter , with header , skipspace ) : sep = delimiter header = 0 if not with header : header = None return pd . read csv ( reader , header = header , sep = sep , skipinitialspace = skipspace , encoding = 'utf-8-sig' )
def contents url ( self ) : loc = self . download location return loc . base uri + loc . location + loc . access credential
def open ( self ) : return self . workspace . rest . open intermediate dataset contents ( self . workspace . workspace id , self . experiment . experiment id , self . node id , self . port name )
def read as binary ( self ) : return self . workspace . rest . read intermediate dataset contents binary ( self . workspace . workspace id , self . experiment . experiment id , self . node id , self . port name )
def read as text ( self ) : return self . workspace . rest . read intermediate dataset contents text ( self . workspace . workspace id , self . experiment . experiment id , self . node id , self . port name )
def to dataframe ( self ) : #TODO: figure out why passing in the opened stream directly gives invalid data data = self . read as binary ( ) reader = Bytes IO ( data ) return deserialize dataframe ( reader , self . data type id )
def get experiments ( self , workspace id ) : api path = self . EXPERIMENTS URI FMT . format ( workspace id ) return self . send get req ( api path )
def get datasets ( self , workspace id ) : api path = self . DATASOURCES URI FMT . format ( workspace id ) return self . send get req ( api path )
def get dataset ( self , workspace id , dataset id ) : api path = self . DATASOURCE URI FMT . format ( workspace id , dataset id ) return self . send get req ( api path )
def find globals ( code ) : cur byte = 0 byte code = code . co code names = set ( ) while cur byte < len ( byte code ) : op = ord ( byte code [ cur byte ] ) if op >= dis . HAVE ARGUMENT : if op == LOAD GLOBAL : oparg = ord ( byte code [ cur byte + 1 ] ) + ( ord ( byte code [ cur byte + 2 ] ) << 8 ) name = code . co names [ oparg ] names . add ( name ) cur byte += 2 cur byte += 1 return names
def copy ( self ) : pen = Pen ( ) pen . dict = self . dict . copy ( ) return pen
def draw ( self , cr , highlight = False , bounding = None ) : if bounding is None or self . intersects ( bounding ) : self . draw ( cr , highlight , bounding )
def build choices ( self ) : tree token = u'sitetree tree from "%s" template "%s"' % ( self . tree , self . template ) context kwargs = { 'current app' : 'admin' } context = template . Context ( context kwargs ) if VERSION >= ( 1 , 8 ) else template . Context ( * * context kwargs ) context . update ( { 'request' : object ( ) } ) choices str = sitetree tree ( Parser ( None ) , Token ( token type = TOKEN BLOCK , contents = tree token ) ) . render ( context ) tree choices = [ ( ITEMS FIELD ROOT ID , self . root title ) ] for line in choices str . splitlines ( ) : if line . strip ( ) : splitted = line . split ( ':::' ) tree choices . append ( ( splitted [ 0 ] , mark safe ( splitted [ 1 ] ) ) ) return tree choices
def init ( self ) : # Drop cache flag set by .reset() method. cache . get ( 'sitetrees reset' ) and self . empty ( init = False ) self . cache = cache . get ( 'sitetrees' , { 'sitetrees' : { } , 'parents' : { } , 'items by ids' : { } , 'tree aliases' : { } } )
def empty ( self , * * kwargs ) : cache . delete ( 'sitetrees' ) cache . delete ( 'sitetrees reset' ) kwargs . get ( 'init' , True ) and self . init ( )
def for tag ( cls , parser , token , preposition , error hint ) : tokens = token . split contents ( ) if len ( tokens ) >= 3 and tokens [ 1 ] == preposition : as var = cls . get as var ( tokens ) tree alias = parser . compile filter ( tokens [ 2 ] ) return cls ( tree alias , as var ) raise template . Template Syntax Error ( '%r tag requires at least two arguments. E.g. {%% %s %%}.' % ( tokens [ 0 ] , error hint ) )
def get model url name ( model nfo , page , with namespace = False ) : prefix = '' if with namespace : prefix = 'admin:' return ( '%s%s %s' % ( prefix , '%s %s' % model nfo , page ) ) . lower ( )
def reregister tree admin ( ) : try : admin . site . unregister ( MODEL TREE CLASS ) except Not Registered : pass admin . site . register ( MODEL TREE CLASS , TREE ADMIN ( ) )
def redirect ( self , request , response ) : if ' addanother' in request . POST : return Http Response Redirect ( '../item add/' ) elif ' save' in request . POST : return Http Response Redirect ( '../' ) elif ' continue' in request . POST : return response return Http Response Redirect ( '' )
def get tree ( self , request , tree id , item id = None ) : if tree id is None : tree id = self . get object ( request , item id ) . tree id self . tree = MODEL TREE CLASS . default manager . get ( pk = tree id ) self . tree . verbose name plural = self . tree . meta . verbose name plural self . tree . urls = TREE URLS return self . tree
def item move ( self , request , tree id , item id , direction ) : current item = MODEL TREE ITEM CLASS . default manager . get ( pk = item id ) if direction == 'up' : sort order = 'sort order' else : sort order = '-sort order' siblings = MODEL TREE ITEM CLASS . default manager . filter ( parent = current item . parent , tree = current item . tree ) . order by ( sort order ) previous item = None for item in siblings : if item != current item : previous item = item else : break if previous item is not None : current item sort order = current item . sort order previous item sort order = previous item . sort order current item . sort order = previous item sort order previous item . sort order = current item sort order current item . save ( ) previous item . save ( ) return Http Response Redirect ( '../../' )
def get urls ( self ) : urls = super ( Tree Admin , self ) . get urls ( ) prefix change = 'change/' if DJANGO POST 19 else '' sitetree urls = [ url ( r'^change/$' , redirects handler , name = get tree item url name ( 'changelist' ) ) , url ( r'^((?P<tree id>\d+)/)?%sitem add/$' % prefix change , self . admin site . admin view ( self . tree admin . item add ) , name = get tree item url name ( 'add' ) ) , url ( r'^(?P<tree id>\d+)/%sitem (?P<item id>\d+)/$' % prefix change , self . admin site . admin view ( self . tree admin . item edit ) , name = get tree item url name ( 'change' ) ) , url ( r'^%sitem (?P<item id>\d+)/$' % prefix change , self . admin site . admin view ( self . tree admin . item edit ) , name = get tree item url name ( 'change' ) ) , url ( r'^((?P<tree id>\d+)/)?%sitem (?P<item id>\d+)/delete/$' % prefix change , self . admin site . admin view ( self . tree admin . item delete ) , name = get tree item url name ( 'delete' ) ) , url ( r'^((?P<tree id>\d+)/)?%sitem (?P<item id>\d+)/history/$' % prefix change , self . admin site . admin view ( self . tree admin . item history ) , name = get tree item url name ( 'history' ) ) , url ( r'^(?P<tree id>\d+)/%sitem (?P<item id>\d+)/move (?P<direction>(up|down))/$' % prefix change , self . admin site . admin view ( self . tree admin . item move ) , name = get tree item url name ( 'move' ) ) , ] if not DJANGO POST 19 : sitetree urls = patterns func ( '' , * sitetree urls ) if SMUGGLER INSTALLED : sitetree urls += ( url ( r'^dump all/$' , self . admin site . admin view ( self . dump view ) , name = 'sitetree dump' ) , ) return sitetree urls + urls
async def asgi send ( self , message : dict ) -> None : if message [ "type" ] == "http.response.start" and self . state == ASGIHTTP State . REQUEST : self . response = message elif message [ "type" ] == "http.response.body" and self . state in { ASGIHTTP State . REQUEST , ASGIHTTP State . RESPONSE , } : if self . state == ASGIHTTP State . REQUEST : headers = build and validate headers ( self . response [ "headers" ] ) headers . extend ( self . response headers ( ) ) await self . asend ( h11 . Response ( status code = int ( self . response [ "status" ] ) , headers = headers ) ) self . state = ASGIHTTP State . RESPONSE if ( not suppress body ( self . scope [ "method" ] , int ( self . response [ "status" ] ) ) and message . get ( "body" , b"" ) != b"" ) : await self . asend ( h11 . Data ( data = bytes ( message [ "body" ] ) ) ) if not message . get ( "more body" , False ) : if self . state != ASGIHTTP State . CLOSED : await self . asend ( h11 . End Of Message ( ) ) await self . asgi put ( { "type" : "http.disconnect" } ) self . state = ASGIHTTP State . CLOSED else : raise Unexpected Message ( self . state , message [ "type" ] )
async def asgi send ( self , message : dict ) -> None : if message [ "type" ] == "websocket.accept" and self . state == ASGI Websocket State . HANDSHAKE : headers = build and validate headers ( message . get ( "headers" , [ ] ) ) raise if subprotocol present ( headers ) headers . extend ( self . response headers ( ) ) await self . asend ( Accept Connection ( extensions = [ Per Message Deflate ( ) ] , extra headers = headers , subprotocol = message . get ( "subprotocol" ) , ) ) self . state = ASGI Websocket State . CONNECTED self . config . access logger . access ( self . scope , { "status" : 101 , "headers" : [ ] } , time ( ) - self . start time ) elif ( message [ "type" ] == "websocket.http.response.start" and self . state == ASGI Websocket State . HANDSHAKE ) : self . response = message self . config . access logger . access ( self . scope , self . response , time ( ) - self . start time ) elif message [ "type" ] == "websocket.http.response.body" and self . state in { ASGI Websocket State . HANDSHAKE , ASGI Websocket State . RESPONSE , } : await self . asgi send rejection ( message ) elif message [ "type" ] == "websocket.send" and self . state == ASGI Websocket State . CONNECTED : data : Union [ bytes , str ] if message . get ( "bytes" ) is not None : await self . asend ( Bytes Message ( data = bytes ( message [ "bytes" ] ) ) ) elif not isinstance ( message [ "text" ] , str ) : raise Type Error ( f"{message['text']} should be a str" ) else : await self . asend ( Text Message ( data = message [ "text" ] ) ) elif message [ "type" ] == "websocket.close" and self . state == ASGI Websocket State . HANDSHAKE : await self . send http error ( 403 ) self . state = ASGI Websocket State . HTTPCLOSED elif message [ "type" ] == "websocket.close" : await self . asend ( Close Connection ( code = int ( message [ "code" ] ) ) ) self . state = ASGI Websocket State . CLOSED else : raise Unexpected Message ( self . state , message [ "type" ] )
def update binary annotations ( self , extra annotations ) : if not self . logging context : # This is not the root span, so binary annotations will be added # to the log handler when this span context exits. self . binary annotations . update ( extra annotations ) else : # Otherwise, we're in the context of the root span, so just update # the binary annotations for the logging context directly. self . logging context . tags . update ( extra annotations )
def encode span ( self , v2 span ) : span = v2 span . build v1 span ( ) thrift endpoint = thrift . create endpoint ( span . endpoint . port , span . endpoint . service name , span . endpoint . ipv4 , span . endpoint . ipv6 , ) thrift annotations = thrift . annotation list builder ( span . annotations , thrift endpoint , ) thrift binary annotations = thrift . binary annotation list builder ( span . binary annotations , thrift endpoint , ) # Add sa/ca binary annotations if v2 span . remote endpoint : self . encode remote endpoint ( v2 span . remote endpoint , v2 span . kind , thrift binary annotations , ) thrift span = thrift . create span ( span . id , span . parent id , span . trace id , span . name , thrift annotations , thrift binary annotations , span . timestamp , span . duration , ) encoded span = thrift . span to bytes ( thrift span ) return encoded span
def encode span ( self , v2 span ) : span = v2 span . build v1 span ( ) json span = { 'trace Id' : span . trace id , 'name' : span . name , 'id' : span . id , 'annotations' : [ ] , 'binary Annotations' : [ ] , } if span . parent id : json span [ 'parent Id' ] = span . parent id if span . timestamp : json span [ 'timestamp' ] = int ( span . timestamp * 1000000 ) if span . duration : json span [ 'duration' ] = int ( span . duration * 1000000 ) v1 endpoint = self . create json endpoint ( span . endpoint , True ) for key , timestamp in span . annotations . items ( ) : json span [ 'annotations' ] . append ( { 'endpoint' : v1 endpoint , 'timestamp' : int ( timestamp * 1000000 ) , 'value' : key , } ) for key , value in span . binary annotations . items ( ) : json span [ 'binary Annotations' ] . append ( { 'key' : key , 'value' : value , 'endpoint' : v1 endpoint , } ) # Add sa/ca binary annotations if v2 span . remote endpoint : self . encode remote endpoint ( v2 span . remote endpoint , v2 span . kind , json span [ 'binary Annotations' ] , ) encoded span = json . dumps ( json span ) return encoded span
def encode span ( self , span ) : json span = { 'trace Id' : span . trace id , 'id' : span . span id , } if span . name : json span [ 'name' ] = span . name if span . parent id : json span [ 'parent Id' ] = span . parent id if span . timestamp : json span [ 'timestamp' ] = int ( span . timestamp * 1000000 ) if span . duration : json span [ 'duration' ] = int ( span . duration * 1000000 ) if span . shared is True : json span [ 'shared' ] = True if span . kind and span . kind . value is not None : json span [ 'kind' ] = span . kind . value if span . local endpoint : json span [ 'local Endpoint' ] = self . create json endpoint ( span . local endpoint , False , ) if span . remote endpoint : json span [ 'remote Endpoint' ] = self . create json endpoint ( span . remote endpoint , False , ) if span . tags and len ( span . tags ) > 0 : json span [ 'tags' ] = span . tags if span . annotations : json span [ 'annotations' ] = [ { 'timestamp' : int ( timestamp * 1000000 ) , 'value' : key , } for key , timestamp in span . annotations . items ( ) ] encoded span = json . dumps ( json span ) return encoded span
def fits ( self , current count , current size , max size , new span ) : return current size + len ( new span ) <= max size
def encode span ( self , span ) : if not protobuf . installed ( ) : raise Zipkin Error ( 'protobuf encoding requires installing the protobuf\'s extra ' 'requirements. Use py-zipkin[protobuf] in your requirements.txt.' ) pb span = protobuf . create protobuf span ( span ) return protobuf . encode pb list ( [ pb span ] )
def join lines ( string , strip = Strip . BOTH ) : lines = [ ] for line in string . splitlines ( ) : if strip & Strip . RIGHT : line = line . rstrip ( ) if strip & Strip . LEFT : line = line . lstrip ( ) lines . append ( line ) return '' . join ( lines )
async def json or text ( response ) : text = await response . text ( ) if response . headers [ 'Content-Type' ] == 'application/json; charset=utf-8' : return json . loads ( text ) return text
async def limited ( until ) : duration = int ( round ( until - time . time ( ) ) ) mins = duration / 60 fmt = 'We have exhausted a ratelimit quota. Retrying in %.2f seconds (%.3f minutes).' log . warn ( fmt , duration , mins )
async def request ( self , method , url , * * kwargs ) : rate limiter = Rate Limiter ( max calls = 59 , period = 60 , callback = limited ) # handles ratelimits. max calls is set to 59 because current implementation will retry in 60s after 60 calls is reached. DBL has a 1h block so obviously this doesn't work well, as it will get a 429 when 60 is reached. async with rate limiter : # this works but doesn't 'save' over restart. need a better implementation. if not self . token : raise Unauthorized Detected ( 'Unauthorized Detected (status code: 401): No TOKEN provided' ) headers = { 'User-Agent' : self . user agent , 'Content-Type' : 'application/json' } if 'json' in kwargs : kwargs [ 'data' ] = to json ( kwargs . pop ( 'json' ) ) kwargs [ 'headers' ] = headers headers [ 'Authorization' ] = self . token for tries in range ( 5 ) : async with self . session . request ( method , url , * * kwargs ) as resp : log . debug ( '%s %s with %s has returned %s' , method , url , kwargs . get ( 'data' ) , resp . status ) data = await json or text ( resp ) if 300 > resp . status >= 200 : return data if resp . status == 429 : # we are being ratelimited fmt = 'We are being rate limited. Retrying in %.2f seconds (%.3f minutes).' # sleep a bit retry after = json . loads ( resp . headers . get ( 'Retry-After' ) ) mins = retry after / 60 log . warning ( fmt , retry after , mins ) # check if it's a global rate limit (True as only 1 ratelimit atm - /api/bots) is global = True # is global = data.get('global', False) if is global : self . global over . clear ( ) await asyncio . sleep ( retry after , loop = self . loop ) log . debug ( 'Done sleeping for the rate limit. Retrying...' ) # release the global lock now that the # global rate limit has passed if is global : self . global over . set ( ) log . debug ( 'Global rate limit is now over.' ) continue if resp . status == 400 : raise HTTP Exception ( resp , data ) elif resp . status == 401 : raise Unauthorized ( resp , data ) elif resp . status == 403 : raise Forbidden ( resp , data ) elif resp . status == 404 : raise Not Found ( resp , data ) else : raise HTTP Exception ( resp , data ) # We've run out of retries, raise. raise HTTP Exception ( resp , data )
async def get bot info ( self , bot id ) : resp = await self . request ( 'GET' , '{}/bots/{}' . format ( self . BASE , bot id ) ) resp [ 'date' ] = datetime . strptime ( resp [ 'date' ] , '%Y-%m-%d T%H:%M:%S.%f Z' ) for k in resp : if resp [ k ] == '' : resp [ k ] = None return resp
async def get bots ( self , limit , offset ) : if limit > 500 : limit = 50 return await self . request ( 'GET' , '{}/bots?limit={}&offset={}' . format ( self . BASE , limit , offset ) )
def read ( self ) : packet = self . packet with self . read lock : buffer = self . buffer while len ( buffer ) < packet : buffer += self . read data ( ) length = self . unpack ( buffer [ : packet ] ) [ 0 ] + packet while len ( buffer ) < length : buffer += self . read data ( ) term , self . buffer = decode ( buffer [ packet : ] ) return term
def write ( self , message ) : data = encode ( message , compressed = self . compressed ) length = len ( data ) data = self . pack ( length ) + data with self . write lock : while data : try : n = os . write ( self . out d , data ) except OS Error as why : if why . errno in ( errno . EPIPE , errno . EINVAL ) : raise EOF Error ( ) raise if not n : raise EOF Error ( ) data = data [ n : ] return length + self . packet
def decode ( string ) : if not string : raise Incomplete Data ( string ) if string [ 0 ] != 131 : raise Value Error ( "unknown protocol version: %r" % string [ 0 ] ) if string [ 1 : 2 ] == b'P' : # compressed term if len ( string ) < 16 : raise Incomplete Data ( string ) d = decompressobj ( ) term string = d . decompress ( string [ 6 : ] ) + d . flush ( ) uncompressed size , = int4 unpack ( string [ 2 : 6 ] ) if len ( term string ) != uncompressed size : raise Value Error ( "invalid compressed tag, " "%d bytes but got %d" % ( uncompressed size , len ( term string ) ) ) # tail data returned by decode term() can be simple ignored term , tail = decode term ( term string ) return term , d . unused data return decode term ( string [ 1 : ] )
def encode ( term , compressed = False ) : encoded term = encode term ( term ) # False and 0 do not attempt compression. if compressed : if compressed is True : # default compression level of 6 compressed = 6 elif compressed < 0 or compressed > 9 : raise Value Error ( "invalid compression level: %r" % ( compressed , ) ) zlib term = compress ( encoded term , compressed ) ln = len ( encoded term ) if len ( zlib term ) + 5 <= ln : # Compressed term should be smaller return b"\x83P" + int4 pack ( ln ) + zlib term return b"\x83" + encoded term
def add Source Addr ( self , addr ) : try : self . multi In Socket . setsockopt ( socket . IPPROTO IP , socket . IP ADD MEMBERSHIP , self . make Mreq ( addr ) ) except socket . error : # if 1 interface has more than 1 address, exception is raised for the second pass sock = self . create Multicast Out Socket ( addr , self . observer . ttl ) self . multi Out Uni In Sockets [ addr ] = sock self . poll . register ( sock , select . POLLIN )
def send Pending Messages ( self ) : if len ( self . queue ) == 0 : time . sleep ( 0.1 ) return msg = self . queue . pop ( 0 ) if msg . can Send ( ) : self . send Msg ( msg ) msg . refresh ( ) if not ( msg . is Finished ( ) ) : self . queue . append ( msg ) else : self . queue . append ( msg ) time . sleep ( 0.01 )
def stop ( self ) : self . clear Remote Services ( ) self . clear Local Services ( ) self . stop Threads ( ) self . server Started = False
def clear Local Services ( self ) : for service in list ( self . local Services . values ( ) ) : self . send Bye ( service ) self . local Services . clear ( )
def search Services ( self , types = None , scopes = None , timeout = 3 ) : if not self . server Started : raise Exception ( "Server not started" ) self . send Probe ( types , scopes ) time . sleep ( timeout ) return self . filter Services ( list ( self . remote Services . values ( ) ) , types , scopes )
def create SOAP Message ( env ) : if env . get Action ( ) == ACTION PROBE : return create Probe Message ( env ) if env . get Action ( ) == ACTION PROBE MATCH : return create Probe Match Message ( env ) if env . get Action ( ) == ACTION RESOLVE : return create Resolve Message ( env ) if env . get Action ( ) == ACTION RESOLVE MATCH : return create Resolve Match Message ( env ) if env . get Action ( ) == ACTION HELLO : return create Hello Message ( env ) if env . get Action ( ) == ACTION BYE : return create Bye Message ( env )
def discover ( scope , loglevel , capture ) : if loglevel : level = getattr ( logging , loglevel , None ) if not level : print ( "Invalid log level '%s'" % loglevel ) return logger . set Level ( level ) run ( scope = scope , capture = capture )
def save ( self , * * kwargs ) : child relation names = [ rel . get accessor name ( ) for rel in get all child relations ( self ) ] child m2m field names = [ field . name for field in get all child m2m relations ( self ) ] update fields = kwargs . pop ( 'update fields' , None ) if update fields is None : real update fields = None relations to commit = child relation names m2m fields to commit = child m2m field names else : real update fields = [ ] relations to commit = [ ] m2m fields to commit = [ ] for field in update fields : if field in child relation names : relations to commit . append ( field ) elif field in child m2m field names : m2m fields to commit . append ( field ) else : real update fields . append ( field ) super ( Clusterable Model , self ) . save ( update fields = real update fields , * * kwargs ) for relation in relations to commit : getattr ( self , relation ) . commit ( ) for field in m2m fields to commit : getattr ( self , field ) . commit ( )
def validate unique ( self ) : # Collect unique checks and to run from all the forms. all unique checks = set ( ) all date checks = set ( ) forms to delete = self . deleted forms valid forms = [ form for form in self . forms if form . is valid ( ) and form not in forms to delete ] for form in valid forms : unique checks , date checks = form . instance . get unique checks ( ) all unique checks . update ( unique checks ) all date checks . update ( date checks ) errors = [ ] # Do each of the unique checks (unique and unique together) for uclass , unique check in all unique checks : seen data = set ( ) for form in valid forms : # Get the data for the set of fields that must be unique among the forms. row data = ( field if field in self . unique fields else form . cleaned data [ field ] for field in unique check if field in form . cleaned data ) # Reduce Model instances to their primary key values row data = tuple ( d . get pk val ( ) if hasattr ( d , ' get pk val' ) else d for d in row data ) if row data and None not in row data : # if we've already seen it then we have a uniqueness failure if row data in seen data : # poke error messages into the right places and mark # the form as invalid errors . append ( self . get unique error message ( unique check ) ) form . errors [ NON FIELD ERRORS ] = self . error class ( [ self . get form error ( ) ] ) # remove the data from the cleaned data dict since it was invalid for field in unique check : if field in form . cleaned data : del form . cleaned data [ field ] # mark the data as seen seen data . add ( row data ) if errors : raise Validation Error ( errors )
def has changed ( self ) : # Need to recurse over nested formsets so that the form is saved if there are changes # to child forms but not the parent if self . formsets : for formset in self . formsets . values ( ) : for form in formset . forms : if form . has changed ( ) : return True return bool ( self . changed data )
def with valid checksum ( self ) : # type: () -> Address return Address ( trytes = self . address + self . generate checksum ( ) , # Make sure to copy all of the ancillary attributes, too! balance = self . balance , key index = self . key index , security level = self . security level , )
def generate checksum ( self ) : # type: () -> Address Checksum checksum trits = [ ] # type: Mutable Sequence[int] sponge = Kerl ( ) sponge . absorb ( self . address . as trits ( ) ) sponge . squeeze ( checksum trits ) checksum length = Address Checksum . LEN * TRITS PER TRYTE return Address Checksum . from trits ( checksum trits [ - checksum length : ] )
def prompt for seed ( ) : # type: () -> Seed seed = secure input ( 'Enter seed and press return (typing will not be shown).\n' 'If no seed is specified, a random one will be used instead.\n' ) if isinstance ( seed , text type ) : seed = seed . encode ( 'ascii' ) return Seed ( seed ) if seed else Seed . random ( )
def create sponge ( self , index ) : # type: (int) -> Kerl seed = self . seed as trits [ : ] sponge = Kerl ( ) sponge . absorb ( add trits ( seed , trits from int ( index ) ) ) # Squeeze all of the trits out of the sponge and re-absorb them. # Note that the sponge transforms several times per operation, # so this sequence is not as redundant as it looks at first # glance. sponge . squeeze ( seed ) sponge . reset ( ) sponge . absorb ( seed ) return sponge
def transform ( self ) : # type: () -> None # Copy some values locally so we can avoid global lookups in the # inner loop. # # References: # # - https://wiki.python.org/moin/Python Speed/Performance Tips#Local Variables state length = STATE LENGTH truth table = TRUTH TABLE # Operate on a copy of ``self. state`` to eliminate dot lookups # in the inner loop. # # References: # # - https://wiki.python.org/moin/Python Speed/Performance Tips#Avoiding dots... # - http://stackoverflow.com/a/2612990/ prev state = self . state [ : ] new state = prev state [ : ] # Note: This code looks significantly different from the C # implementation because it has been optimized to limit the # number of list item lookups (these are relatively slow in # Python). index = 0 for in range ( NUMBER OF ROUNDS ) : prev trit = prev state [ index ] for pos in range ( state length ) : index += ( 364 if index < 365 else - 365 ) new trit = prev state [ index ] new state [ pos ] = truth table [ prev trit + ( 3 * new trit ) + 4 ] prev trit = new trit prev state = new state new state = new state [ : ] self . state = new state
def full add trits ( left , right , carry ) : # type: (int, int, int) -> Tuple[int, int] sum both = add trits ( left , right ) cons left = cons trits ( left , right ) cons right = cons trits ( sum both , carry ) return add trits ( sum both , carry ) , any trits ( cons left , cons right )
def resolve adapter ( uri ) : # type: (Adapter Spec) -> Base Adapter if isinstance ( uri , Base Adapter ) : return uri parsed = compat . urllib parse . urlsplit ( uri ) # type: Split Result if not parsed . scheme : raise with context ( exc = Invalid Uri ( 'URI must begin with "<protocol>://" (e.g., "udp://").' , ) , context = { 'parsed' : parsed , 'uri' : uri , } , ) try : adapter type = adapter registry [ parsed . scheme ] except Key Error : raise with context ( exc = Invalid Uri ( 'Unrecognized protocol {protocol!r}.' . format ( protocol = parsed . scheme , ) ) , context = { 'parsed' : parsed , 'uri' : uri , } , ) return adapter type . configure ( parsed )
def log ( self , level , message , context = None ) : # type: (int, Text, Optional[dict]) -> None if self . logger : self . logger . log ( level , message , extra = { 'context' : context or { } } )
def address from digest ( digest ) : # type: (Digest) -> Address address trits = [ 0 ] * ( Address . LEN * TRITS PER TRYTE ) # type: List[int] sponge = Kerl ( ) sponge . absorb ( digest . as trits ( ) ) sponge . squeeze ( address trits ) return Address . from trits ( trits = address trits , key index = digest . key index , security level = digest . security level , )
def encode ( self , input , errors = 'strict' ) : if isinstance ( input , memoryview ) : input = input . tobytes ( ) if not isinstance ( input , ( binary type , bytearray ) ) : raise with context ( exc = Type Error ( "Can't encode {type}; byte string expected." . format ( type = type ( input ) . name , ) ) , context = { 'input' : input , } , ) # :bc: In Python 2, iterating over a byte string yields # characters instead of integers. if not isinstance ( input , bytearray ) : input = bytearray ( input ) trytes = bytearray ( ) for c in input : second , first = divmod ( c , len ( self . alphabet ) ) trytes . append ( self . alphabet [ first ] ) trytes . append ( self . alphabet [ second ] ) return binary type ( trytes ) , len ( input )
def decode ( self , input , errors = 'strict' ) : if isinstance ( input , memoryview ) : input = input . tobytes ( ) if not isinstance ( input , ( binary type , bytearray ) ) : raise with context ( exc = Type Error ( "Can't decode {type}; byte string expected." . format ( type = type ( input ) . name , ) ) , context = { 'input' : input , } , ) # :bc: In Python 2, iterating over a byte string yields # characters instead of integers. if not isinstance ( input , bytearray ) : input = bytearray ( input ) bytes = bytearray ( ) for i in range ( 0 , len ( input ) , 2 ) : try : first , second = input [ i : i + 2 ] except Value Error : if errors == 'strict' : raise with context ( exc = Trytes Decode Error ( "'{name}' codec can't decode value; " "tryte sequence has odd length." . format ( name = self . name , ) , ) , context = { 'input' : input , } , ) elif errors == 'replace' : bytes += b'?' continue try : bytes . append ( self . index [ first ] + ( self . index [ second ] * len ( self . index ) ) ) except Value Error : # This combination of trytes yields a value > 255 when # decoded. # Naturally, we can't represent this using ASCII. if errors == 'strict' : raise with context ( exc = Trytes Decode Error ( "'{name}' codec can't decode trytes {pair} " "at position {i}-{j}: " "ordinal not in range(255)" . format ( name = self . name , pair = chr ( first ) + chr ( second ) , i = i , j = i + 1 , ) , ) , context = { 'input' : input , } ) elif errors == 'replace' : bytes += b'?' return binary type ( bytes ) , len ( input )
def find addresses ( self , seed , index , count , security level , checksum ) : # type: (Seed, int, Optional[int], int, bool) -> List[Address] generator = Address Generator ( seed , security level , checksum ) if count is None : # Connect to Tangle and find the first address without any # transactions. for addy in generator . create iterator ( start = index ) : # We use addy.address here because Find Transactions does # not work on an address with a checksum response = Find Transactions Command ( self . adapter ) ( addresses = [ addy . address ] , ) if not response . get ( 'hashes' ) : return [ addy ] return generator . get addresses ( start = index , count = count )
def as tryte string ( self ) : # type: () -> Transaction Trytes return Transaction Trytes ( self . signature message fragment + self . address . address + self . value as trytes + self . legacy tag + self . timestamp as trytes + self . current index as trytes + self . last index as trytes + self . bundle hash + self . trunk transaction hash + self . branch transaction hash + self . tag + self . attachment timestamp as trytes + self . attachment timestamp lower bound as trytes + self . attachment timestamp upper bound as trytes + self . nonce )
def is confirmed ( self , new is confirmed ) : # type: (bool) -> None self . is confirmed = new is confirmed for txn in self : txn . is confirmed = new is confirmed
def group transactions ( self ) : # type: () -> List[List[Transaction]] groups = [ ] if self : last txn = self . tail transaction current group = [ last txn ] for current txn in self . transactions [ 1 : ] : # Transactions are grouped by address, so as long as the # address stays consistent from one transaction to # another, we are still in the same group. if current txn . address == last txn . address : current group . append ( current txn ) else : groups . append ( current group ) current group = [ current txn ] last txn = current txn if current group : groups . append ( current group ) return groups
def errors ( self ) : # type: () -> List[Text] try : self . errors . extend ( self . validator ) # type: List[Text] except Stop Iteration : pass return self . errors
def is valid ( self ) : # type: () -> bool if not self . errors : try : # We only have to check for a single error to determine # if the bundle is valid or not. self . errors . append ( next ( self . validator ) ) except Stop Iteration : pass return not self . errors
def create validator ( self ) : # type: () -> Generator[Text, None, None] # Group transactions by address to make it easier to iterate # over inputs. grouped transactions = self . bundle . group transactions ( ) # Define a few expected values. bundle hash = self . bundle . hash last index = len ( self . bundle ) - 1 # Track a few others as we go along. balance = 0 # Check indices and balance first. # Note that we use a counter to keep track of the current index, # since at this point we can't trust that the transactions have # correct ``current index`` values. counter = 0 for group in grouped transactions : for txn in group : balance += txn . value if txn . bundle hash != bundle hash : yield 'Transaction {i} has invalid bundle hash.' . format ( i = counter , ) if txn . current index != counter : yield ( 'Transaction {i} has invalid current index value ' '(expected {i}, actual {actual}).' . format ( actual = txn . current index , i = counter , ) ) if txn . last index != last index : yield ( 'Transaction {i} has invalid last index value ' '(expected {expected}, actual {actual}).' . format ( actual = txn . last index , expected = last index , i = counter , ) ) counter += 1 # Bundle must be balanced (spends must match inputs). if balance != 0 : yield ( 'Bundle has invalid balance ' '(expected 0, actual {actual}).' . format ( actual = balance , ) ) # Signature validation is only meaningful if the transactions # are otherwise valid. if not self . errors : signature validation queue = [ ] # type: List[List[Transaction]] for group in grouped transactions : # Signature validation only applies to inputs. if group [ 0 ] . value >= 0 : continue validate group signature = True for j , txn in enumerate ( group ) : if ( j > 0 ) and ( txn . value != 0 ) : # Input is malformed; signature fragments after # the first should have zero value. yield ( 'Transaction {i} has invalid value ' '(expected 0, actual {actual}).' . format ( actual = txn . value , # If we get to this point, we know that # the ``current index`` value for each # transaction can be trusted. i = txn . current index , ) ) # We won't be able to validate the signature, # but continue anyway, so that we can check that # the other transactions in the group have the # correct ``value``. validate group signature = False continue # After collecting the signature fragment from each # transaction in the group, queue them up to run through # the validator. # # We have to perform signature validation separately so # that we can try different algorithms (for # backwards-compatibility). # # References: # if validate group signature : signature validation queue . append ( group ) # Once we've finished checking the attributes from each # transaction in the bundle, go back and validate # signatures. if signature validation queue : # ``yield from`` is an option here, but for # compatibility with Python 2 clients, we will do it the # old-fashioned way. for error in self . get bundle signature errors ( signature validation queue ) : yield error
def start repl ( api ) : # type: (Iota) -> None banner = ( 'IOTA API client for {uri} ({testnet}) ' 'initialized as variable `api`.\n' 'Type `help(api)` for list of API commands.' . format ( testnet = 'testnet' if api . testnet else 'mainnet' , uri = api . adapter . get uri ( ) , ) ) scope vars = { 'api' : api } try : # noinspection Py Unresolved References import I Python except Import Error : # I Python not available; use regular Python REPL. from code import Interactive Console Interactive Console ( locals = scope vars ) . interact ( banner , '' ) else : print ( banner ) I Python . start ipython ( argv = [ ] , user ns = scope vars )
def Security Level ( ) : return ( f . Type ( int ) | f . Min ( 1 ) | f . Max ( 3 ) | f . Optional ( default = Address Generator . DEFAULT SECURITY LEVEL ) )
def as tryte string ( self ) : # type: () -> Tryte String if not self . bundle hash : raise with context ( exc = Runtime Error ( 'Cannot get Tryte String representation of {cls} instance ' 'without a bundle hash; call ``bundle.finalize()`` first ' '(``exc.context`` has more info).' . format ( cls = type ( self ) . name , ) , ) , context = { 'transaction' : self , } , ) return super ( Proposed Transaction , self ) . as tryte string ( )
def tag ( self ) : # type: () -> Tag for txn in reversed ( self ) : # type: Proposed Transaction if txn . tag : return txn . tag return Tag ( b'' )
def finalize ( self ) : # type: () -> None if self . hash : raise Runtime Error ( 'Bundle is already finalized.' ) if not self : raise Value Error ( 'Bundle has no transactions.' ) # Quick validation. balance = self . balance if balance < 0 : if self . change address : self . add transaction ( Proposed Transaction ( address = self . change address , value = - balance , tag = self . tag , ) ) else : raise Value Error ( 'Bundle has unspent inputs (balance: {balance}); ' 'use ``send unspent inputs to`` to create ' 'change transaction.' . format ( balance = balance , ) , ) elif balance > 0 : raise Value Error ( 'Inputs are insufficient to cover bundle spend ' '(balance: {balance}).' . format ( balance = balance , ) , ) # Generate bundle hash. while True : sponge = Kerl ( ) last index = len ( self ) - 1 for i , txn in enumerate ( self ) : txn . current index = i txn . last index = last index sponge . absorb ( txn . get signature validation trytes ( ) . as trits ( ) ) bundle hash trits = [ 0 ] * HASH LENGTH sponge . squeeze ( bundle hash trits ) bundle hash = Bundle Hash . from trits ( bundle hash trits ) # Check that we generated a secure bundle hash. if any ( 13 in part for part in normalize ( bundle hash ) ) : # Increment the legacy tag and try again. tail transaction = ( self . tail transaction ) # type: Proposed Transaction tail transaction . increment legacy tag ( ) else : break # Copy bundle hash to individual transactions. for txn in self : txn . bundle hash = bundle hash # Initialize signature/message fragment. txn . signature message fragment = Fragment ( txn . message or b'' )
def sign inputs ( self , key generator ) : # type: (Key Generator) -> None if not self . hash : raise Runtime Error ( 'Cannot sign inputs until bundle is finalized.' ) # Use a counter for the loop so that we can skip ahead as we go. i = 0 while i < len ( self ) : txn = self [ i ] if txn . value < 0 : # In order to sign the input, we need to know the index # of the private key used to generate it. if txn . address . key index is None : raise with context ( exc = Value Error ( 'Unable to sign input {input}; ' '``key index`` is None ' '(``exc.context`` has more info).' . format ( input = txn . address , ) , ) , context = { 'transaction' : txn , } , ) if txn . address . security level is None : raise with context ( exc = Value Error ( 'Unable to sign input {input}; ' '``security level`` is None ' '(``exc.context`` has more info).' . format ( input = txn . address , ) , ) , context = { 'transaction' : txn , } , ) self . sign input at ( i , key generator . get key for ( txn . address ) ) i += txn . address . security level else : # No signature needed (nor even possible, in some # cases); skip this transaction. i += 1
def create input transactions ( self , addy ) : # type: (Address) -> None self . transactions . append ( Proposed Transaction ( address = addy , tag = self . tag , # Spend the entire address balance; if necessary, we will # add a change transaction to the bundle. value = - addy . balance , ) ) # Signatures require additional transactions to store, due to # transaction length limit. # Subtract 1 to account for the transaction we just added. for in range ( addy . security level - 1 ) : self . transactions . append ( Proposed Transaction ( address = addy , tag = self . tag , # Note zero value; this is a meta transaction. value = 0 , ) )
def decompress G1 ( z : G1Compressed ) -> G1Uncompressed : # b flag == 1 indicates the infinity point b flag = ( z % POW 2 383 ) // POW 2 382 if b flag == 1 : return Z1 x = z % POW 2 381 # Try solving y coordinate from the equation Y^2 = X^3 + b # using quadratic residue y = pow ( ( x ** 3 + b . n ) % q , ( q + 1 ) // 4 , q ) if pow ( y , 2 , q ) != ( x ** 3 + b . n ) % q : raise Value Error ( "The given point is not on G1: y**2 = x**3 + b" ) # Choose the y whose leftmost bit is equal to the a flag a flag = ( z % POW 2 382 ) // POW 2 381 if ( y * 2 ) // q != a flag : y = q - y return ( FQ ( x ) , FQ ( y ) , FQ ( 1 ) )
def prime field inv ( a : int , n : int ) -> int : if a == 0 : return 0 lm , hm = 1 , 0 low , high = a % n , n while low > 1 : r = high // low nm , new = hm - lm * r , high - low * r lm , low , hm , high = nm , new , lm , low return lm % n
def repr html ( self ) : rows , c = '' , '' s = '<tr><td><strong>{k}</strong></td><td style="{stl}">{v}</td></tr>' for k , v in self . dict . items ( ) : if k == ' colour' : k = 'colour' c = utils . text colour for hex ( v ) style = 'color:{}; background-color:{}' . format ( c , v ) else : style = 'color:black; background-color:white' if k == 'component' : try : v = v . repr html ( ) except Attribute Error : v = v . repr ( ) rows += s . format ( k = k , v = v , stl = style ) html = '<table>{}</table>' . format ( rows ) return html
def random ( cls , component ) : colour = random . sample ( [ i for i in range ( 256 ) ] , 3 ) return cls ( { 'colour' : colour , 'component' : component , 'width' : 1.0 } )
def repr html ( self ) : all keys = list ( set ( itertools . chain ( * [ d . keys for d in self ] ) ) ) rows = '' for decor in self : th , tr = decor . repr html row ( keys = all keys ) rows += '<tr>{}</tr>' . format ( tr ) header = '<tr>{}</tr>' . format ( th ) html = '<table>{}{}</table>' . format ( header , rows ) return html
def repr html ( self ) : rows = '' s = '<tr><td><strong>{k}</strong></td><td>{v}</td></tr>' for k , v in self . dict . items ( ) : rows += s . format ( k = k , v = v ) html = '<table>{}</table>' . format ( rows ) return html
def Rock ( * args , * * kwargs ) : with warnings . catch warnings ( ) : warnings . simplefilter ( "always" ) w = "The 'Rock' class was renamed 'Component'. " w += "Please update your code." warnings . warn ( w , Deprecation Warning , stacklevel = 2 ) return Component ( * args , * * kwargs )
def process row ( text , columns ) : if not text : return # Construct the column dictionary that maps each field to # its start, its length, and its read and write functions. coldict = { k : { 'start' : s , 'len' : l , 'read' : r , 'write' : w } for k , ( s , l , r , w ) in columns . items ( ) } # Now collect the item item = { } for field in coldict : value = get field ( text , coldict , field ) if value is not None : item [ field ] = value return item
def parse canstrat ( text ) : result = { } for row in text . split ( '\n' ) : if not row : continue if len ( row ) < 8 : # Not a real record. continue # Read the metadata for this row/ row header = process row ( row , columns ) or { 'card' : None } card = row header [ 'card' ] # Now we know the card type for this row, we can process it. if card is not None : item = process row ( row , columns [ card ] ) this list = result . get ( card , [ ] ) this list . append ( item ) result [ card ] = this list # Flatten if possible. for c , d in result . items ( ) : if len ( d ) == 1 : result [ c ] = d [ 0 ] return result
def get template ( name ) : text = re . sub ( r'\r\n' , r'\n' , name ) text = re . sub ( r'\{([FISDE°].*?)\}',   '{{\1}}',   ext) return text
def clean longitudinal data ( cls , data , null = None ) : # Rename 'depth' or 'MD' if ( 'top' not in data . keys ( ) ) : data [ 'top' ] = data . pop ( 'depth' , data . pop ( 'MD' , None ) ) # Sort everything idx = list ( data . keys ( ) ) . index ( 'top' ) values = sorted ( zip ( * data . values ( ) ) , key = lambda x : x [ idx ] ) data = { k : list ( v ) for k , v in zip ( data . keys ( ) , zip ( * values ) ) } if data [ 'top' ] is None : raise Striplog Error ( 'Could not get tops.' ) # Get rid of null-like values if specified. if null is not None : for k , v in data . items ( ) : data [ k ] = [ i if i != null else None for i in v ] return data
def from csv ( cls , filename = None , text = None , dlm = ',' , lexicon = None , points = False , include = None , exclude = None , remap = None , function = None , null = None , ignore = None , source = None , stop = None , fieldnames = None ) : if ( filename is None ) and ( text is None ) : raise Striplog Error ( "You must provide a filename or CSV text." ) if ( filename is not None ) : if source is None : source = filename with open ( filename , 'r' ) as f : text = f . read ( ) source = source or 'CSV' # Deal with multiple spaces in space delimited file. if dlm == ' ' : text = re . sub ( r'[ \t]+' , ' ' , text ) if fieldnames is not None : text = dlm . join ( fieldnames ) + '\n' + text try : f = String IO ( text ) # Python 3 except Type Error : f = String IO ( unicode ( text ) ) # Python 2 reader = csv . Dict Reader ( f , delimiter = dlm ) # Reorganize the data to make fixing it easier. reorg = { k . strip ( ) . lower ( ) : [ ] for k in reader . fieldnames if k is not None } t = f . tell ( ) for key in reorg : f . seek ( t ) for r in reader : s = { k . strip ( ) . lower ( ) : v . strip ( ) for k , v in r . items ( ) } try : reorg [ key ] . append ( float ( s [ key ] ) ) except Value Error : reorg [ key ] . append ( s [ key ] ) f . close ( ) remap = remap or { } for k , v in remap . items ( ) : reorg [ v ] = reorg . pop ( k ) data = cls . clean longitudinal data ( reorg , null = null ) list of Intervals = cls . build list of Intervals ( data , points = points , lexicon = lexicon , include = include , exclude = exclude , ignore = ignore , stop = stop ) return cls ( list of Intervals , source = source )
def from img ( cls , * args , * * kwargs ) : with warnings . catch warnings ( ) : warnings . simplefilter ( "always" ) w = "from img() is deprecated; please use from image()" warnings . warn ( w ) return cls . from image ( * args , * * kwargs )
def from canstrat ( cls , filename , source = 'canstrat' ) : with open ( filename ) as f : dat = f . read ( ) data = parse canstrat ( dat ) list of Intervals = [ ] for d in data [ 7 ] : # 7 is the 'card type' for lithology info. if d . pop ( 'skip' ) : continue top = d . pop ( 'top' ) base = d . pop ( 'base' ) comps = [ Component ( { 'lithology' : d [ 'rtc' ] , 'colour' : d [ 'colour name' ] } ) ] iv = Interval ( top = top , base = base , components = comps , data = d ) list of Intervals . append ( iv ) return cls ( list of Intervals , source = source )
def copy ( self ) : return Striplog ( [ i . copy ( ) for i in self ] , order = self . order , source = self . source )
def get data ( self , field , function = None , default = None ) : f = function or utils . null data = [ ] for iv in self : d = iv . data . get ( field ) if d is None : if default is not None : d = default else : d = np . nan data . append ( f ( d ) ) return np . array ( data )
def depth ( self , d ) : with warnings . catch warnings ( ) : warnings . simplefilter ( "always" ) w = "depth() is deprecated; please use read at()" warnings . warn ( w ) return self . read at ( d )
def dict repr html ( dictionary ) : rows = '' s = '<tr><td><strong>{k}</strong></td><td>{v}</td></tr>' for k , v in dictionary . items ( ) : rows += s . format ( k = k , v = v ) html = '<table>{}</table>' . format ( rows ) return html
def convert field ( self , value , conversion ) : try : # If the normal behaviour works, do it. s = super ( Custom Formatter , self ) return s . convert field ( value , conversion ) except Value Error : funcs = { 's' : str , # Default. 'r' : repr , # Default. 'a' : ascii , # Default. 'u' : str . upper , 'l' : str . lower , 'c' : str . capitalize , 't' : str . title , 'm' : np . mean , 'µ':   p. m ean, 'v' : np . var , 'd' : np . std , '+' : np . sum , '∑':  n .s u m, 'x' : np . product , } return funcs . get ( conversion ) ( value )
def get random ( self , obj type ) : return self . mutator [ obj type ] [ random . randint ( 0 , self . config . level ) ]
def get mutator ( self , obj , obj type ) : if obj type == unicode : obj type = str obj = str ( obj ) return self . get random ( obj type ) ( obj )
def get string polyglot attack ( self , obj ) : return self . polyglot attacks [ random . choice ( self . config . techniques ) ] % obj
def random action ( self , b ) : action = random . choice ( [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] ) if len ( b ) >= 3 : pos = random . randint ( 0 , len ( b ) - 2 ) if action == 1 : rbyte = random . randrange ( 256 ) rn = random . randrange ( len ( b ) ) b [ rn ] = "%c" % rbyte elif action == 2 : howmany = random . randint ( 1 , 100 ) curpos = pos for in range ( 0 , howmany ) : b . insert ( curpos , b [ pos ] ) pos += 1 elif action == 3 : n = random . choice ( [ 1 , 2 , 4 ] ) for in range ( 0 , n ) : if len ( b ) > pos + 1 : tmp = b [ pos ] b [ pos ] = b [ pos + 1 ] b [ pos + 1 ] = tmp pos += 1 else : pos -= 2 tmp = b [ pos ] b [ pos ] = b [ pos + 1 ] b [ pos + 1 ] = tmp pos += 1 elif action in [ 4 , 5 ] : op = { 4 : lambda x , y : ord ( x ) << y , 5 : lambda x , y : ord ( x ) >> y , } n = random . choice ( [ 1 , 2 , 4 ] ) if len ( b ) < pos + n : pos = len ( b ) - ( pos + n ) if n == 1 : f = "<B" s = op [ action ] ( b [ pos ] , n ) % 0xff elif n == 2 : f = "<H" s = op [ action ] ( b [ pos ] , n ) % 0xffff elif n == 4 : f = "<I" s = op [ action ] ( b [ pos ] , n ) % 0xffffff val = struct . pack ( f , s ) for v in val : if isinstance ( v , int ) : v = chr ( v ) b [ pos ] = v pos += 1 elif action == 6 : b . insert ( random . randint ( 0 , len ( b ) - 1 ) , random . choice ( [ "\"" , "[" , "]" , "+" , "-" , "}" , "{" ] ) ) elif action == 7 : del b [ random . randint ( 0 , len ( b ) - 1 ) ] elif action in [ 8 , 9 ] : block = random . choice ( [ ( r"\"" , r"\"" ) , ( r"\[" , r"\]" ) , ( r"\{" , r"\}" ) ] ) b str = self . safe join ( b ) block re = re . compile ( str ( ".+({0}[^{2}{3}]+{1}).+" ) . format ( block [ 0 ] , block [ 1 ] , block [ 0 ] , block [ 1 ] ) ) if block re . search ( b str ) : r = random . choice ( block re . findall ( b str ) ) random re = re . compile ( "({0})" . format ( re . escape ( r ) ) ) if random re . search ( b str ) : if action == 8 : newarr = list ( random re . sub ( "" , b str ) ) b [ : ] = newarr else : newarr = list ( random re . sub ( "\\1" * random . randint ( 1 , 10 ) , b str , 1 ) ) b [ : ] = newarr elif action == 10 : b str = self . safe join ( b ) limit choice = random . choice ( [ 0x7FFFFFFF , - 0x80000000 , 0xff , - 0xff , ] ) block re = re . compile ( "(\-?[0-9]+)" ) if block re . search ( b str ) : block = random . choice ( [ m for m in block re . finditer ( b str ) ] ) new = b str [ 0 : block . start ( ) ] + str ( int ( block . group ( ) ) * limit choice ) + b str [ block . start ( ) + len ( block . group ( ) ) : ] b [ : ] = list ( new )
def safe unicode ( self , buf ) : tmp = "" buf = "" . join ( b for b in buf ) for character in buf : tmp += character return tmp
def custom html ( self , filepath ) : try : response . headers . append ( "Access-Control-Allow-Origin" , "*" ) response . headers . append ( "Accept-Encoding" , "identity" ) response . headers . append ( "Content-Type" , "text/html" ) return static file ( filepath , root = self . config . html ) except Exception as e : raise PJF Base Exception ( e . message if hasattr ( e , "message" ) else str ( e ) )
def serve ( self ) : try : fuzzed = self . json . fuzzed if self . config . fuzz web : self . client queue . put ( ( request . environ . get ( 'REMOTE ADDR' ) , fuzzed ) ) response . headers . append ( "Access-Control-Allow-Origin" , "*" ) response . headers . append ( "Accept-Encoding" , "identity" ) response . headers . append ( "Content-Type" , self . config . content type ) if self . config . notify : PJF Testcase Server . send testcase ( fuzzed , '127.0.0.1' , self . config . ports [ "servers" ] [ "TCASE PORT" ] ) yield fuzzed except Exception as e : raise PJF Base Exception ( e . message if hasattr ( e , "message" ) else str ( e ) )
def fuzz ( self , obj ) : decorators = self . decorators @ decorators . mutate object decorate def mutate ( ) : return obj return mutate ( )
def spawn ( self , cmd , stdin content = "" , stdin = False , shell = False , timeout = 2 ) : try : if type ( cmd ) != list : raise PJF Invalid Type ( type ( cmd ) , list ) if type ( stdin content ) != str : raise PJF Invalid Type ( type ( stdin content ) , str ) if type ( stdin ) != bool : raise PJF Invalid Type ( type ( stdin ) , bool ) self . in = stdin content try : self . process = subprocess . Popen ( cmd , stdout = PIPE , stderr = PIPE , stdin = PIPE , shell = shell ) self . finish read ( timeout , stdin content , stdin ) if self . process . poll ( ) is not None : self . close ( ) except Keyboard Interrupt : return except OS Error : raise PJF Process Execution Error ( "Binary <%s> does not exist" % cmd [ 0 ] ) except Exception as e : raise PJF Base Exception ( e . message if hasattr ( e , "message" ) else str ( e ) )
def get output ( self , stdin content , stdin ) : try : if stdin : if sys . version info >= ( 3 , 0 ) : self . process . stdin . write ( bytes ( stdin content , "utf-8" ) ) else : self . process . stdin . write ( stdin content ) self . out = self . process . communicate ( ) [ 0 ] except ( error , IO Error ) : self . out = self . in pass
def finish read ( self , timeout = 2 , stdin content = "" , stdin = False ) : process = Thread ( target = self . get output , args = ( stdin content , stdin ) ) process . start ( ) if timeout > 0 : process . join ( timeout ) else : process . join ( ) if process . is alive ( ) : self . close ( ) self . return code = - signal . SIGHUP else : self . return code = self . process . returncode
def close ( self ) : try : self . process . terminate ( ) self . return code = self . process . returncode except OS Error : pass self . process . stdin . close ( ) self . process . stdout . close ( ) self . process . stderr . close ( ) self . logger . debug ( "[{0}] - PJF Executor successfully completed" . format ( time . strftime ( "%H:%M:%S" ) ) )
def start ( self ) : from . pjf worker import PJF Worker worker = PJF Worker ( self ) if self . update pjf : worker . update library ( ) elif self . browser auto : worker . browser autopwn ( ) elif self . fuzz web : worker . web fuzzer ( ) elif self . json : if not self . web server and not self . ext fuzz and not self . cmd fuzz : worker . fuzz ( ) elif self . ext fuzz : if self . stdin : worker . fuzz stdin ( ) else : worker . fuzz command line ( ) elif self . cmd fuzz : if self . stdin : worker . fuzz external ( True ) else : worker . fuzz external ( ) else : worker . start http server ( ) elif self . json file : worker . start file fuzz ( ) elif self . process to monitor : worker . start process monitor ( )
def execute ( self , obj ) : try : if self . config . stdin : self . spawn ( self . config . command , stdin content = obj , stdin = True , timeout = 1 ) else : if "@@" not in self . config . command : raise PJF Missing Argument ( "Missing @@ filename indicator while using non-stdin fuzzing method" ) for x in self . config . command : if "@@" in x : self . config . command [ self . config . command . index ( x ) ] = x . replace ( "@@" , obj ) self . spawn ( self . config . command , timeout = 2 ) self . logger . debug ( "[{0}] - PJF External Fuzzer successfully completed" . format ( time . strftime ( "%H:%M:%S" ) ) ) return self . out except Keyboard Interrupt : return "" except Exception as e : raise PJF Base Exception ( e . message if hasattr ( e , "message" ) else str ( e ) )
def json encode ( func ) : def func wrapper ( self , indent , utf8 ) : if utf8 : encoding = "\\x%02x" else : encoding = "\\u%04x" hex regex = re . compile ( r"(\\\\x[a-f A-F0-9]{2})" ) unicode regex = re . compile ( r"(\\u[a-f A-F0-9]{4})" ) def encode decode all ( d , decode = True ) : if type ( d ) == dict : for k in d : if type ( d [ k ] ) in [ dict , list ] : if decode : d [ k ] = encode decode all ( d [ k ] ) else : d [ k ] = encode decode all ( d [ k ] , decode = False ) elif type ( d [ k ] ) == str : if decode : d [ k ] = decode ( d [ k ] ) else : d [ k ] = encode ( d [ k ] ) elif type ( d ) == list : arr = [ ] for e in d : if type ( e ) == str : if decode : arr . append ( decode ( e ) ) else : arr . append ( encode ( e ) ) elif type ( e ) in [ dict , list ] : if decode : arr . append ( encode decode all ( e ) ) else : arr . append ( encode decode all ( e , decode = False ) ) else : arr . append ( e ) return arr else : if decode : return decode ( d ) else : return encode ( d ) return d def decode ( x ) : tmp = "" . join ( encoding % ord ( c ) if c not in p else c for c in x ) if sys . version info >= ( 3 , 0 ) : return str ( tmp ) else : for encoded in unicode regex . findall ( tmp ) : tmp = tmp . replace ( encoded , encoded . decode ( "unicode escape" ) ) return unicode ( tmp ) def encode ( x ) : for encoded in hex regex . findall ( x ) : if sys . version info >= ( 3 , 0 ) : x = x . replace ( encoded , bytes ( str ( encoded ) . replace ( "\\\\x" , "\\x" ) , "utf-8" ) . decode ( "unicode escape" ) ) else : x = x . replace ( encoded , str ( encoded ) . replace ( "\\\\x" , "\\x" ) . decode ( "string escape" ) ) return x if indent : return encode decode all ( "{0}" . format ( json . dumps ( encode decode all ( func ( self ) ) , indent = 5 ) ) , decode = False ) else : return encode decode all ( "{0}" . format ( json . dumps ( encode decode all ( func ( self ) ) ) ) , decode = False ) return func wrapper
def shutdown ( self , * args ) : try : self . shutdown ( ) if self . process : self . process . wait ( ) self . process . stdout . close ( ) self . process . stdin . close ( ) self . process . stderr . close ( ) self . finished = True self . send testcase ( '' , '127.0.0.1' , self . config . ports [ "servers" ] [ "TCASE PORT" ] ) self . logger . debug ( "[{0}] - PJF Process Monitor successfully completed" . format ( time . strftime ( "%H:%M:%S" ) ) ) except Exception as e : raise PJF Base Exception ( e . message if hasattr ( e , "message" ) else str ( e ) )
def run and monitor ( self ) : signal . signal ( signal . SIGINT , self . shutdown ) self . spawn ( self . config . process to monitor , timeout = 0 ) return self . is sigsegv ( self . return code )
def start monitor ( self , standalone = True ) : try : self . start ( ) cmdline = shlex . split ( self . config . process to monitor ) if standalone : signal . signal ( signal . SIGINT , self . shutdown ) self . process = subprocess . Popen ( cmdline , stdin = PIPE , stdout = PIPE , stderr = PIPE ) while self . process and not self . finished : self . process . wait ( ) if self . is sigsegv ( self . process . returncode ) : if self . config . debug : print ( "[\033[92m INFO\033[0m] Process crashed with \033[91m SIGSEGV\033[0m, waiting for testcase..." ) while not self . got testcase ( ) : time . sleep ( 1 ) self . save testcase ( self . testcase [ - 10 : ] ) # just take last 10 testcases if self . process : self . process = subprocess . Popen ( cmdline , stdin = PIPE , stdout = PIPE , stderr = PIPE ) except OS Error : self . shutdown ( ) self . process = False self . got testcase = lambda : True raise PJF Process Execution Error ( "Binary <%s> does not exist" % cmdline [ 0 ] ) except Exception as e : raise PJF Base Exception ( "Unknown error please send log to author" )
def fuzz elements ( self , element ) : try : if type ( element ) == dict : tmp element = { } for key in element : if len ( self . config . parameters ) > 0 : if self . config . exclude parameters : fuzz = key not in self . config . parameters else : fuzz = key in self . config . parameters else : fuzz = True if fuzz : if type ( element [ key ] ) == dict : tmp element . update ( { key : self . fuzz elements ( element [ key ] ) } ) elif type ( element [ key ] ) == list : tmp element . update ( { key : self . fuzz elements ( element [ key ] ) } ) else : tmp element . update ( { key : self . mutator . fuzz ( element [ key ] ) } ) else : tmp element . update ( { key : self . fuzz elements ( element [ key ] ) } ) element = tmp element del tmp element elif type ( element ) == list : arr = [ ] for key in element : if type ( key ) == dict : arr . append ( self . fuzz elements ( key ) ) elif type ( key ) == list : arr . append ( self . fuzz elements ( key ) ) else : if len ( self . config . parameters ) <= 0 : arr . append ( self . mutator . fuzz ( key ) ) else : arr . append ( key ) element = arr del arr except Exception as e : raise PJF Base Exception ( e . message if hasattr ( e , "message" ) else str ( e ) ) return element
def fuzzed ( self ) : try : if self . config . strong fuzz : fuzzer = PJF Mutators ( self . config ) if self . config . url encode : if sys . version info >= ( 3 , 0 ) : return urllib . parse . quote ( fuzzer . fuzz ( json . dumps ( self . config . json ) ) ) else : return urllib . quote ( fuzzer . fuzz ( json . dumps ( self . config . json ) ) ) else : if type ( self . config . json ) in [ list , dict ] : return fuzzer . fuzz ( json . dumps ( self . config . json ) ) else : return fuzzer . fuzz ( self . config . json ) else : if self . config . url encode : if sys . version info >= ( 3 , 0 ) : return urllib . parse . quote ( self . get fuzzed ( self . config . indent , self . config . utf8 ) ) else : return urllib . quote ( self . get fuzzed ( self . config . indent , self . config . utf8 ) ) else : return self . get fuzzed ( self . config . indent , self . config . utf8 ) except Exception as e : raise PJF Base Exception ( e . message if hasattr ( e , "message" ) else str ( e ) )
def get fuzzed ( self , indent = False , utf8 = False ) : try : if "array" in self . json : return self . fuzz elements ( dict ( self . json ) ) [ "array" ] else : return self . fuzz elements ( dict ( self . json ) ) except Exception as e : raise PJF Base Exception ( e . message if hasattr ( e , "message" ) else str ( e ) )
def mutate object decorate ( self , func ) : def mutate ( ) : obj = func ( ) return self . Mutators . get mutator ( obj , type ( obj ) ) return mutate
def get User Id ( self ) : self . user Id = self ( "GET" , "{0}/users/self/profile" . format ( self . API USER ) , auth = self . Auth . Skype Token ) . json ( ) . get ( "username" )
def sync Endpoints ( self ) : self . endpoints [ "all" ] = [ ] for json in self ( "GET" , "{0}/users/ME/presence Docs/messaging Service" . format ( self . msgs Host ) , params = { "view" : "expanded" } , auth = self . Auth . Reg Token ) . json ( ) . get ( "endpoint Presence Docs" , [ ] ) : id = json . get ( "link" , "" ) . split ( "/" ) [ 7 ] self . endpoints [ "all" ] . append ( Skype Endpoint ( self , id ) )
def get Mac256Hash ( challenge , app Id = "msmsgs@msnmsgr.com" , key = "Q1P7W2E4J9R8U3S5" ) : clear Text = challenge + app Id clear Text += "0" * ( 8 - len ( clear Text ) % 8 ) def int32To Hex String ( n ) : hex Chars = "0123456789abcdef" hex String = "" for i in range ( 4 ) : hex String += hex Chars [ ( n >> ( i * 8 + 4 ) ) & 15 ] hex String += hex Chars [ ( n >> ( i * 8 ) ) & 15 ] return hex String def int64Xor ( a , b ) : s A = "{0:b}" . format ( a ) s B = "{0:b}" . format ( b ) s C = "" s D = "" diff = abs ( len ( s A ) - len ( s B ) ) for i in range ( diff ) : s D += "0" if len ( s A ) < len ( s B ) : s D += s A s A = s D elif len ( s B ) < len ( s A ) : s D += s B s B = s D for i in range ( len ( s A ) ) : s C += "0" if s A [ i ] == s B [ i ] else "1" return int ( s C , 2 ) def c S64 ( pdw Data , p In Hash ) : MODULUS = 2147483647 CS64 a = p In Hash [ 0 ] & MODULUS CS64 b = p In Hash [ 1 ] & MODULUS CS64 c = p In Hash [ 2 ] & MODULUS CS64 d = p In Hash [ 3 ] & MODULUS CS64 e = 242854337 pos = 0 qw Datum = 0 qw MAC = 0 qw Sum = 0 for i in range ( len ( pdw Data ) // 2 ) : qw Datum = int ( pdw Data [ pos ] ) pos += 1 qw Datum *= CS64 e qw Datum = qw Datum % MODULUS qw MAC += qw Datum qw MAC *= CS64 a qw MAC += CS64 b qw MAC = qw MAC % MODULUS qw Sum += qw MAC qw MAC += int ( pdw Data [ pos ] ) pos += 1 qw MAC *= CS64 c qw MAC += CS64 d qw MAC = qw MAC % MODULUS qw Sum += qw MAC qw MAC += CS64 b qw MAC = qw MAC % MODULUS qw Sum += CS64 d qw Sum = qw Sum % MODULUS return [ qw MAC , qw Sum ] cch Clear Text = len ( clear Text ) // 4 p Clear Text = [ ] for i in range ( cch Clear Text ) : p Clear Text = p Clear Text [ : i ] + [ 0 ] + p Clear Text [ i : ] for pos in range ( 4 ) : p Clear Text [ i ] += ord ( clear Text [ 4 * i + pos ] ) * ( 256 ** pos ) sha256Hash = [ 0 , 0 , 0 , 0 ] hash = hashlib . sha256 ( ( challenge + key ) . encode ( "utf-8" ) ) . hexdigest ( ) . upper ( ) for i in range ( len ( sha256Hash ) ) : sha256Hash [ i ] = 0 for pos in range ( 4 ) : dpos = 8 * i + pos * 2 sha256Hash [ i ] += int ( hash [ dpos : dpos + 2 ] , 16 ) * ( 256 ** pos ) mac Hash = c S64 ( p Clear Text , sha256Hash ) mac Parts = [ mac Hash [ 0 ] , mac Hash [ 1 ] , mac Hash [ 0 ] , mac Hash [ 1 ] ] return "" . join ( map ( int32To Hex String , map ( int64Xor , sha256Hash , mac Parts ) ) )
def u ( text , encoding = 'utf-8' ) : if isinstance ( text , six . binary type ) : text = text . decode ( encoding ) # it's already unicode text = text . replace ( '\r\n' , '\n' ) return text
def to dict ( self ) : d = self . metadata . copy ( ) d [ 'content' ] = self . content return d
def load ( self , fm , * * kwargs ) : kwargs . setdefault ( 'Loader' , Safe Loader ) return yaml . load ( fm , * * kwargs )
def export ( self , metadata , * * kwargs ) : kwargs . setdefault ( 'Dumper' , Safe Dumper ) kwargs . setdefault ( 'default flow style' , False ) kwargs . setdefault ( 'allow unicode' , True ) metadata = yaml . dump ( metadata , * * kwargs ) . strip ( ) return u ( metadata )
def export ( self , metadata , * * kwargs ) : kwargs . setdefault ( 'indent' , 4 ) metadata = json . dumps ( metadata , * * kwargs ) return u ( metadata )
def match ( self ) : cache match , cache string = self . match cache string = self . string if cache string == string : return cache match cache match = fullmatch ( LIST PATTERN FORMAT . replace ( b'{pattern}' , self . pattern . encode ( ) ) , self . shadow , MULTILINE , ) self . match cache = cache match , string return cache match
def convert ( self , newstart : str ) -> None : match = self . match ms = match . start ( ) for s , e in reversed ( match . spans ( 'pattern' ) ) : self [ s - ms : e - ms ] = newstart self . pattern = escape ( newstart )
def arguments ( self ) -> List [ Argument ] : shadow = self . shadow split spans = self . args matcher ( shadow ) . spans ( 'arg' ) if not split spans : return [ ] arguments = [ ] arguments append = arguments . append type to spans = self . type to spans ss , se = span = self . span type = id ( span ) lststr = self . lststr string = lststr [ 0 ] arg spans = type to spans . setdefault ( type , [ ] ) span tuple to span get = { ( s [ 0 ] , s [ 1 ] ) : s for s in arg spans } . get for arg self start , arg self end in split spans : s , e = arg span = [ ss + arg self start , ss + arg self end ] old span = span tuple to span get ( ( s , e ) ) if old span is None : insort ( arg spans , arg span ) else : arg span = old span arg = Argument ( lststr , type to spans , arg span , type ) arg . shadow cache = ( string [ s : e ] , shadow [ arg self start : arg self end ] ) arguments append ( arg ) return arguments
def pattern ( trie : dict ) -> str : if '' in trie : if len ( trie ) == 1 : return '' optional = True del trie [ '' ] else : optional = False subpattern to chars = defaultdict ( list ) for char , sub trie in trie . items ( ) : subpattern = pattern ( sub trie ) subpattern to chars [ subpattern ] . append ( char ) alts = [ ] for subpattern , chars in subpattern to chars . items ( ) : if len ( chars ) == 1 : alts . append ( chars [ 0 ] + subpattern ) else : chars . sort ( reverse = True ) alts . append ( '[' + '' . join ( chars ) + ']' + subpattern ) if len ( alts ) == 1 : result = alts [ 0 ] if optional : if len ( result ) == 1 : result += '?+' else : # more than one character in alts[0] result = '(?:' + result + ')?+' else : alts . sort ( reverse = True ) result = '(?>' + '|' . join ( alts ) + ')' if optional : result += '?+' return result
def atomic partition ( self , char : int ) -> Tuple [ str , str , str ] : s , e = self . span index = self . shadow . find ( char ) if index == - 1 : return self . lststr [ 0 ] [ s : e ] , '' , '' lststr0 = self . lststr [ 0 ] return lststr0 [ s : s + index ] , chr ( char ) , lststr0 [ s + index + 1 : e ]
def subspans ( self , type : str ) -> List [ List [ int ] ] : return self . type to spans [ type ]
def insert update ( self , index : int , length : int ) -> None : ss , se = self . span for spans in self . type to spans . values ( ) : for span in spans : if index < span [ 1 ] or span [ 1 ] == index == se : span [ 1 ] += length # index is before s, or at s but not on self span if index < span [ 0 ] or span [ 0 ] == index != ss : span [ 0 ] += length
def pprint ( self , indent : str = '    ' , remove comments = False ) : warn ( 'pprint method is deprecated, use pformat instead.' , Deprecation Warning , ) return self . pformat ( indent , remove comments )
def parameters ( self ) -> List [ 'Parameter' ] : lststr = self . lststr type to spans = self . type to spans return [ Parameter ( lststr , type to spans , span , 'Parameter' ) for span in self . subspans ( 'Parameter' ) ]
def parser functions ( self ) -> List [ 'Parser Function' ] : lststr = self . lststr type to spans = self . type to spans return [ Parser Function ( lststr , type to spans , span , 'Parser Function' ) for span in self . subspans ( 'Parser Function' ) ]
def templates ( self ) -> List [ 'Template' ] : lststr = self . lststr type to spans = self . type to spans return [ Template ( lststr , type to spans , span , 'Template' ) for span in self . subspans ( 'Template' ) ]
def wikilinks ( self ) -> List [ 'Wiki Link' ] : lststr = self . lststr type to spans = self . type to spans return [ Wiki Link ( lststr , type to spans , span , 'Wiki Link' ) for span in self . subspans ( 'Wiki Link' ) ]
def comments ( self ) -> List [ 'Comment' ] : lststr = self . lststr type to spans = self . type to spans return [ Comment ( lststr , type to spans , span , 'Comment' ) for span in self . subspans ( 'Comment' ) ]
def tables ( self ) -> List [ 'Table' ] : tables = [ ] # type: List['Table'] tables append = tables . append type to spans = self . type to spans lststr = self . lststr shadow = self . shadow [ : ] ss , se = self . span spans = type to spans . setdefault ( 'Table' , [ ] ) if not spans : # All the added spans will be new. m = True # type: Any while m : m = False for m in TABLE FINDITER ( shadow ) : ms , me = m . span ( ) # Ignore leading whitespace using len(m[1]). span = [ ss + ms + len ( m [ 1 ] ) , ss + me ] spans . append ( span ) tables append ( Table ( lststr , type to spans , span , 'Table' ) ) shadow [ ms : me ] = b' ' * ( me - ms ) return tables # There are already exists some spans. Try to use the already existing # before appending new spans. span tuple to span get = { ( s [ 0 ] , s [ 1 ] ) : s for s in spans } . get m = True while m : m = False for m in TABLE FINDITER ( shadow ) : ms , me = m . span ( ) # Ignore leading whitespace using len(m[1]). s , e = ss + ms + len ( m [ 1 ] ) , ss + me old span = span tuple to span get ( ( s , e ) ) if old span is None : span = [ s , e ] insort ( spans , span ) else : span = old span tables append ( Table ( lststr , type to spans , span , 'Table' ) ) shadow [ ms : me ] = b' ' * ( me - ms ) return tables
def tags ( self , name = None ) -> List [ 'Tag' ] : lststr = self . lststr type to spans = self . type to spans if name : if name in tag extensions : string = lststr [ 0 ] return [ Tag ( lststr , type to spans , span , 'Extension Tag' ) for span in type to spans [ 'Extension Tag' ] if string . startswith ( '<' + name , span [ 0 ] ) ] tags = [ ] # type: List['Tag'] else : # There is no name, add all extension tags. Before using shadow. tags = [ Tag ( lststr , type to spans , span , 'Extension Tag' ) for span in type to spans [ 'Extension Tag' ] ] tags append = tags . append # Get the left-most start tag, match it to right-most end tag # and so on. ss = self . span [ 0 ] shadow = self . shadow if name : # There is a name but it is not in TAG EXTENSIONS. reversed start matches = reversed ( [ m for m in regex compile ( START TAG PATTERN . replace ( rb'{name}' , rb'(?P<name>' + name . encode ( ) + rb')' ) ) . finditer ( shadow ) ] ) end search = regex compile ( END TAG PATTERN . replace ( b'{name}' , name . encode ( ) ) ) . search else : reversed start matches = reversed ( [ m for m in START TAG FINDITER ( shadow ) ] ) shadow copy = shadow [ : ] spans = type to spans . setdefault ( 'Tag' , [ ] ) span tuple to span get = { ( s [ 0 ] , s [ 1 ] ) : s for s in spans } . get spans append = spans . append for start match in reversed start matches : if start match [ 'self closing' ] : # Don't look for the end tag s , e = start match . span ( ) span = [ ss + s , ss + e ] else : # look for the end-tag if name : # the end search is already available # noinspection Py Unbound Local Variable end match = end search ( shadow copy , start match . end ( ) ) else : # build end search according to start tag name end match = search ( END TAG PATTERN . replace ( b'{name}' , start match [ 'name' ] ) , shadow copy ) if end match : s , e = end match . span ( ) shadow copy [ s : e ] = b' ' * ( e - s ) span = [ ss + start match . start ( ) , ss + e ] else : # Assume start-only tag. s , e = start match . span ( ) span = [ ss + s , ss + e ] old span = span tuple to span get ( ( span [ 0 ] , span [ 1 ] ) ) if old span is None : spans append ( span ) else : span = old span tags append ( Tag ( lststr , type to spans , span , 'Tag' ) ) return sorted ( tags , key = attrgetter ( ' span' ) )
def subspans ( self , type : str ) -> Generator [ int , None , None ] : ss , se = self . span spans = self . type to spans [ type ] # Do not yield self. span by bisecting for s < ss. # The second bisect is an optimization and should be on [se + 1], # but empty spans are not desired thus [se] is used. b = bisect ( spans , [ ss ] ) for span in spans [ b : bisect ( spans , [ se ] , b ) ] : if span [ 1 ] <= se : yield span
def del arg ( self , name : str ) -> None : for arg in reversed ( self . arguments ) : if arg . name . strip ( WS ) == name . strip ( WS ) : del arg [ : ]
def to ogc wkt ( self ) : return 'GEOGCS["%s", %s, %s, %s, AXIS["Lon", %s], AXIS["Lat", %s]]' % ( self . name , self . datum . to ogc wkt ( ) , self . prime mer . to ogc wkt ( ) , self . angunit . to ogc wkt ( ) , self . twin ax [ 0 ] . ogc wkt , self . twin ax [ 1 ] . ogc wkt )
def to esri wkt ( self ) : return 'GEOGCS["%s", %s, %s, %s, AXIS["Lon", %s], AXIS["Lat", %s]]' % ( self . name , self . datum . to esri wkt ( ) , self . prime mer . to esri wkt ( ) , self . angunit . to esri wkt ( ) , self . twin ax [ 0 ] . esri wkt , self . twin ax [ 1 ] . esri wkt )
def to ogc wkt ( self ) : string = 'PROJCS["%s", %s, %s, ' % ( self . name , self . geogcs . to ogc wkt ( ) , self . proj . to ogc wkt ( ) ) string += ", " . join ( param . to ogc wkt ( ) for param in self . params ) string += ', %s' % self . unit . to ogc wkt ( ) string += ', AXIS["X", %s], AXIS["Y", %s]]' % ( self . twin ax [ 0 ] . ogc wkt , self . twin ax [ 1 ] . ogc wkt ) return string
def to esri wkt ( self ) : string = 'PROJCS["%s", %s, %s, ' % ( self . name , self . geogcs . to esri wkt ( ) , self . proj . to esri wkt ( ) ) string += ", " . join ( param . to esri wkt ( ) for param in self . params ) string += ', %s' % self . unit . to esri wkt ( ) string += ', AXIS["X", %s], AXIS["Y", %s]]' % ( self . twin ax [ 0 ] . esri wkt , self . twin ax [ 1 ] . esri wkt ) return string
def parse geo tiff ( key dir vlr : Geo Key Directory Vlr , double vlr : Geo Double Params Vlr , ascii vlr : Geo Ascii Params Vlr , ) -> List [ Geo Tiff Key ] : geotiff keys = [ ] for k in key dir vlr . geo keys : if k . tiff tag location == 0 : value = k . value offset elif k . tiff tag location == 34736 : value = double vlr . doubles [ k . value offset ] elif k . tiff tag location == 34737 : try : value = ascii vlr . strings [ k . value offset ] [ k . count : ] except Index Error : # Maybe I'm just misunderstanding the specification :thinking: value = ascii vlr . strings [ 0 ] [ k . value offset : k . value offset + k . count ] else : logger . warning ( "Geo Tiff Key with unknown tiff tag location ({})" . format ( k . tiff tag location ) ) continue geotiff keys . append ( Geo Tiff Key ( k . id , value ) ) return geotiff keys
def copy fields from ( self , other record ) : for dim name in self . dimensions names : try : self [ dim name ] = other record [ dim name ] except Value Error : pass
def from stream ( cls , stream , point format , count ) : points dtype = point format . dtype point data buffer = bytearray ( stream . read ( count * points dtype . itemsize ) ) try : data = np . frombuffer ( point data buffer , dtype = points dtype , count = count ) except Value Error : expected bytes len = count * points dtype . itemsize if len ( point data buffer ) % points dtype . itemsize != 0 : missing bytes len = expected bytes len - len ( point data buffer ) raise not enough bytes error ( expected bytes len , missing bytes len , len ( point data buffer ) , points dtype , ) else : actual count = len ( point data buffer ) // points dtype . itemsize logger . critical ( "Expected {} points, there are {} ({} missing)" . format ( count , actual count , count - actual count ) ) data = np . frombuffer ( point data buffer , dtype = points dtype , count = actual count ) return cls ( data , point format )
def x ( self ) : return scale dimension ( self . X , self . header . x scale , self . header . x offset )
def y ( self ) : return scale dimension ( self . Y , self . header . y scale , self . header . y offset )
def z ( self ) : return scale dimension ( self . Z , self . header . z scale , self . header . z offset )
def min file version for point format ( point format id ) : for version , point formats in sorted ( VERSION TO POINT FMT . items ( ) ) : if point format id in point formats : return version else : raise errors . Point Format Not Supported ( point format id )
def is point fmt compatible with version ( point format id , file version ) : try : return point format id in VERSION TO POINT FMT [ str ( file version ) ] except Key Error : raise errors . File Version Not Supported ( file version )
def files have same point format id ( las files ) : point format found = { las . header . point format id for las in las files } return len ( point format found ) == 1
def files have same dtype ( las files ) : dtypes = { las . points . dtype for las in las files } return len ( dtypes ) == 1
def raise if wrong file signature ( stream ) : file sig = stream . read ( len ( headers . LAS FILE SIGNATURE ) ) if file sig != headers . LAS FILE SIGNATURE : raise errors . Pylas Error ( "File Signature ({}) is not {}" . format ( file sig , headers . LAS FILE SIGNATURE ) )
def read header ( self ) : self . stream . seek ( self . start pos ) return headers . Header Factory ( ) . read from stream ( self . stream )
def read vlrs ( self ) : self . stream . seek ( self . start pos + self . header . size ) return VLR List . read from ( self . stream , num to read = self . header . number of vlr )
def read compressed points data ( self , laszip vlr , point format ) : offset to chunk table = struct . unpack ( "<q" , self . stream . read ( 8 ) ) [ 0 ] size of point data = offset to chunk table - self . stream . tell ( ) if offset to chunk table <= 0 : logger . warning ( "Strange offset to chunk table: {}, ignoring it.." . format ( offset to chunk table ) ) size of point data = - 1 # Read everything points = record . Packed Point Record . from compressed buffer ( self . stream . read ( size of point data ) , point format , self . header . point count , laszip vlr , ) return points
def read internal waveform packet ( self ) : # This is strange, the spec says, waveform data packet is in a EVLR #  but in the 2 samples I have its a VLR # but also the 2 samples have a wrong user id (LAS Spec instead of LASF Spec) b = bytearray ( self . stream . read ( rawvlr . VLR HEADER SIZE ) ) waveform header = rawvlr . Raw VLR Header . from buffer ( b ) waveform record = self . stream . read ( ) logger . debug ( "Read: {} M Bytes of waveform record" . format ( len ( waveform record ) / 10 ** 6 ) ) return waveform header , waveform record
def warn if not at expected pos ( self , expected pos , end of , start of ) : diff = expected pos - self . stream . tell ( ) if diff != 0 : logger . warning ( "There are {} bytes between {} and {}" . format ( diff , end of , start of ) )
def date ( self , date ) : self . creation year = date . year self . creation day of year = date . timetuple ( ) . tm yday
def mins ( self ) : return np . array ( [ self . x min , self . y min , self . z min ] )
def mins ( self , value ) : self . x min , self . y min , self . z min = value
def maxs ( self ) : return np . array ( [ self . x max , self . y max , self . z max ] )
def maxs ( self , value ) : self . x max , self . y max , self . z max = value
def scales ( self ) : return np . array ( [ self . x scale , self . y scale , self . z scale ] )
def offsets ( self ) : return np . array ( [ self . x offset , self . y offset , self . z offset ] )
def num extra bytes ( self ) : return sum ( np . dtype ( extra dim [ 1 ] ) . itemsize for extra dim in self . extra dims )
def has waveform packet ( self ) : dimensions = set ( self . dimension names ) return all ( name in dimensions for name in dims . WAVEFORM FIELDS NAMES )
def main ( port , ip , command , loglevel ) : numeric level = getattr ( logging , loglevel . upper ( ) , None ) if not isinstance ( numeric level , int ) : raise Value Error ( 'Invalid log level: %s' % loglevel ) logging . basic Config ( level = numeric level ) click . echo ( "Demo of satel integra library" ) if command == "demo" : demo ( ip , port )
def checksum ( command ) : crc = 0x147A for b in command : # rotate (crc 1 bit left) crc = ( ( crc << 1 ) & 0x FFFF ) | ( crc & 0x8000 ) >> 15 crc = crc ^ 0x FFFF crc = ( crc + ( crc >> 8 ) + b ) & 0x FFFF return crc
def print hex ( data ) : hex msg = "" for c in data : hex msg += "\\x" + format ( c , "02x" ) LOGGER . debug ( hex msg )
def verify and strip ( resp ) : if resp [ 0 : 2 ] != b'\x FE\x FE' : LOGGER . error ( "Houston, we got problem:" ) print hex ( resp ) raise Exception ( "Wrong header - got %X%X" % ( resp [ 0 ] , resp [ 1 ] ) ) if resp [ - 2 : ] != b'\x FE\x0D' : raise Exception ( "Wrong footer - got %X%X" % ( resp [ - 2 ] , resp [ - 1 ] ) ) output = resp [ 2 : - 2 ] . replace ( b'\x FE\x F0' , b'\x FE' ) c = checksum ( bytearray ( output [ 0 : - 2 ] ) ) if ( 256 * output [ - 2 : - 1 ] [ 0 ] + output [ - 1 : ] [ 0 ] ) != c : raise Exception ( "Wrong checksum - got %d expected %d" % ( ( 256 * output [ - 2 : - 1 ] [ 0 ] + output [ - 1 : ] [ 0 ] ) , c ) ) return output [ 0 : - 2 ]
def generate query ( command ) : data = bytearray ( command ) c = checksum ( data ) data . append ( c >> 8 ) data . append ( c & 0x FF ) data . replace ( b'\x FE' , b'\x FE\x F0' ) data = bytearray . fromhex ( "FEFE" ) + data + bytearray . fromhex ( "FE0D" ) return data
def demo ( host , port ) : # logging.basic Config(level=logging.DEBUG) loop = asyncio . get event loop ( ) stl = Async Satel ( host , port , loop , [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 25 , 26 , 27 , 28 , 29 , 30 ] , [ 8 , 9 , 10 ] ) loop . run until complete ( stl . connect ( ) ) loop . create task ( stl . arm ( "3333" , 1 ) ) loop . create task ( stl . disarm ( "3333" ) ) loop . create task ( stl . keep alive ( ) ) loop . create task ( stl . monitor status ( ) ) loop . run forever ( ) loop . close ( )
async def connect ( self ) : LOGGER . debug ( "Connecting..." ) try : self . reader , self . writer = await asyncio . open connection ( self . host , self . port , loop = self . loop ) LOGGER . debug ( "sucess connecting..." ) except Exception as e : LOGGER . warning ( "Exception during connecting: %s." , e ) self . writer = None self . reader = None return False return True
async def start monitoring ( self ) : data = generate query ( b'\x7F\x01\x DC\x99\x80\x00\x04\x00\x00\x00\x00\x00\x00' ) await self . send data ( data ) resp = await self . read data ( ) if resp is None : LOGGER . warning ( "Start monitoring - no data!" ) return if resp [ 1 : 2 ] != b'\x FF' : LOGGER . warning ( "Monitoring not accepted." )
async def disarm ( self , code , partition list ) : LOGGER . info ( "Sending disarm command." ) while len ( code ) < 16 : code += 'F' code bytes = bytearray . fromhex ( code ) data = generate query ( b'\x84' + code bytes + partition bytes ( partition list ) ) await self . send data ( data )
async def clear alarm ( self , code , partition list ) : LOGGER . info ( "Sending clear the alarm command." ) while len ( code ) < 16 : code += 'F' code bytes = bytearray . fromhex ( code ) data = generate query ( b'\x85' + code bytes + partition bytes ( partition list ) ) await self . send data ( data )
async def set output ( self , code , output id , state ) : LOGGER . debug ( "Turn on, output: %s, code: %s" , output id , code ) while len ( code ) < 16 : code += 'F' code bytes = bytearray . fromhex ( code ) mode command = 0x88 if state else 0x89 data = generate query ( mode command . to bytes ( 1 , 'big' ) + code bytes + output bytes ( output id ) ) await self . send data ( data )
def close ( self ) : LOGGER . debug ( "Closing..." ) self . closed = True if self . connected : self . writer . close ( )
def purge db ( self ) : with self . engine . begin ( ) as db : purge user ( db , self . user id )
def get notebook ( self , path , content , format ) : with self . engine . begin ( ) as db : try : record = get file ( db , self . user id , path , content , self . crypto . decrypt , ) except No Such File : self . no such entity ( path ) return self . notebook model from db ( record , content )
def notebook model from db ( self , record , content ) : path = to api path ( record [ 'parent name' ] + record [ 'name' ] ) model = base model ( path ) model [ 'type' ] = 'notebook' model [ 'last modified' ] = model [ 'created' ] = record [ 'created at' ] if content : content = reads base64 ( record [ 'content' ] ) self . mark trusted cells ( content , path ) model [ 'content' ] = content model [ 'format' ] = 'json' self . validate notebook model ( model ) return model
def get directory ( self , path , content , format ) : with self . engine . begin ( ) as db : try : record = get directory ( db , self . user id , path , content ) except No Such Directory : if self . file exists ( path ) : # TODO: It's awkward/expensive to have to check this to # return a 400 instead of 404. Consider just 404ing. self . do 400 ( "Wrong type: %s" % path ) else : self . no such entity ( path ) return self . directory model from db ( record , content )
def directory model from db ( self , record , content ) : model = base directory model ( to api path ( record [ 'name' ] ) ) if content : model [ 'format' ] = 'json' model [ 'content' ] = list ( chain ( self . convert file records ( record [ 'files' ] ) , ( self . directory model from db ( subdir , False ) for subdir in record [ 'subdirs' ] ) , ) ) return model
def file model from db ( self , record , content , format ) : # TODO: Most of this is shared with  notebook model from db. path = to api path ( record [ 'parent name' ] + record [ 'name' ] ) model = base model ( path ) model [ 'type' ] = 'file' model [ 'last modified' ] = model [ 'created' ] = record [ 'created at' ] if content : bcontent = record [ 'content' ] model [ 'content' ] , model [ 'format' ] , model [ 'mimetype' ] = from b64 ( path , bcontent , format , ) return model
def save file ( self , db , model , path ) : save file ( db , self . user id , path , to b64 ( model [ 'content' ] , model . get ( 'format' , None ) ) , self . crypto . encrypt , self . max file size bytes , ) return None
def delete file ( self , path ) : if self . file exists ( path ) : self . delete non directory ( path ) elif self . dir exists ( path ) : self . delete directory ( path ) else : self . no such entity ( path )
def ensure db user ( db , user id ) : with ignore unique violation ( ) : db . execute ( users . insert ( ) . values ( id = user id ) , )
def purge user ( db , user id ) : db . execute ( files . delete ( ) . where ( files . c . user id == user id ) ) db . execute ( directories . delete ( ) . where ( directories . c . user id == user id ) ) db . execute ( users . delete ( ) . where ( users . c . id == user id ) )
def create directory ( db , user id , api path ) : name = from api dirname ( api path ) if name == '/' : parent name = null ( ) parent user id = null ( ) else : # Convert '/foo/bar/buzz/' -> '/foo/bar/' parent name = name [ : name . rindex ( '/' , 0 , - 1 ) + 1 ] parent user id = user id db . execute ( directories . insert ( ) . values ( name = name , user id = user id , parent name = parent name , parent user id = parent user id , ) )
def delete directory ( db , user id , api path ) : db dirname = from api dirname ( api path ) try : result = db . execute ( directories . delete ( ) . where ( and ( directories . c . user id == user id , directories . c . name == db dirname , ) ) ) except Integrity Error as error : if is foreign key violation ( error ) : raise Directory Not Empty ( api path ) else : raise rowcount = result . rowcount if not rowcount : raise No Such Directory ( api path ) return rowcount
def files in directory ( db , user id , db dirname ) : fields = file default fields ( ) rows = db . execute ( select ( fields , ) . where ( is in directory ( files , user id , db dirname ) , ) . order by ( files . c . user id , files . c . parent name , files . c . name , files . c . created at , ) . distinct ( files . c . user id , files . c . parent name , files . c . name , ) ) return [ to dict no content ( fields , row ) for row in rows ]
def directories in directory ( db , user id , db dirname ) : fields = directory default fields ( ) rows = db . execute ( select ( fields , ) . where ( is in directory ( directories , user id , db dirname ) , ) ) return [ to dict no content ( fields , row ) for row in rows ]
def file where ( user id , api path ) : directory , name = split api filepath ( api path ) return and ( files . c . name == name , files . c . user id == user id , files . c . parent name == directory , )
def select file ( user id , api path , fields , limit ) : query = select ( fields ) . where ( file where ( user id , api path ) , ) . order by ( file creation order ( ) , ) if limit is not None : query = query . limit ( limit ) return query
def file default fields ( ) : return [ files . c . name , files . c . created at , files . c . parent name , ]
def file exists ( db , user id , path ) : try : get file ( db , user id , path , include content = False , decrypt func = unused decrypt func , ) return True except No Such File : return False
def rename file ( db , user id , old api path , new api path ) : # Overwriting existing files is disallowed. if file exists ( db , user id , new api path ) : raise File Exists ( new api path ) old dir , old name = split api filepath ( old api path ) new dir , new name = split api filepath ( new api path ) if old dir != new dir : raise Value Error ( dedent ( . format ( old api path = old api path , new api path = new api path ) ) ) db . execute ( files . update ( ) . where ( file where ( user id , old api path ) , ) . values ( name = new name , created at = func . now ( ) , ) )
def rename directory ( db , user id , old api path , new api path ) : old db path = from api dirname ( old api path ) new db path = from api dirname ( new api path ) if old db path == '/' : raise Rename Root ( 'Renaming the root directory is not permitted.' ) # Overwriting existing directories is disallowed. if dir exists ( db , user id , new db path ) : raise Directory Exists ( new api path ) # Set this foreign key constraint to deferred so it's not violated # when we run the first statement to update the name of the directory. db . execute ( 'SET CONSTRAINTS ' 'pgcontents.directories parent user id fkey DEFERRED' ) # Update name column for the directory that's being renamed db . execute ( directories . update ( ) . where ( and ( directories . c . user id == user id , directories . c . name == old db path , ) ) . values ( name = new db path , ) ) # Update the name and parent name of any descendant directories.  Do # this in a single statement so the non-deferrable check constraint # is satisfied. db . execute ( directories . update ( ) . where ( and ( directories . c . user id == user id , directories . c . name . startswith ( old db path ) , directories . c . parent name . startswith ( old db path ) , ) ) . values ( name = func . concat ( new db path , func . right ( directories . c . name , - func . length ( old db path ) ) ) , parent name = func . concat ( new db path , func . right ( directories . c . parent name , - func . length ( old db path ) ) ) , ) )
def purge remote checkpoints ( db , user id ) : db . execute ( remote checkpoints . delete ( ) . where ( remote checkpoints . c . user id == user id , ) )
def reencrypt row content ( db , table , row id , decrypt func , encrypt func , logger ) : q = ( select ( [ table . c . content ] ) . with for update ( ) . where ( table . c . id == row id ) ) [ ( content , ) ] = db . execute ( q ) logger . info ( "Begin encrypting %s row %s." , table . name , row id ) db . execute ( table . update ( ) . where ( table . c . id == row id ) . values ( content = encrypt func ( decrypt func ( content ) ) ) ) logger . info ( "Done encrypting %s row %s." , table . name , row id )
def select file ids ( db , user id ) : return list ( db . execute ( select ( [ files . c . id ] ) . where ( files . c . user id == user id ) ) )
def select remote checkpoint ids ( db , user id ) : return list ( db . execute ( select ( [ remote checkpoints . c . id ] ) . where ( remote checkpoints . c . user id == user id ) ) )
def reencrypt user content ( engine , user id , old decrypt func , new encrypt func , logger ) : logger . info ( "Begin re-encryption for user %s" , user id ) with engine . begin ( ) as db : # NOTE: Doing both of these operations in one transaction depends for # correctness on the fact that the creation of new checkpoints always # involves writing new data into the database from Python, rather than # simply copying data inside the DB. # If we change checkpoint creation so that it does an in-database copy, # then we need to split this transaction to ensure that # file-reencryption is complete before checkpoint-reencryption starts. # If that doesn't happen, it will be possible for a user to create a # new checkpoint in a transaction that hasn't seen the completed # file-reencryption process, but we might not see that checkpoint here, # which means that we would never update the content of that checkpoint # to the new encryption key. logger . info ( "Re-encrypting files for %s" , user id ) for ( file id , ) in select file ids ( db , user id ) : reencrypt row content ( db , files , file id , old decrypt func , new encrypt func , logger , ) logger . info ( "Re-encrypting checkpoints for %s" , user id ) for ( cp id , ) in select remote checkpoint ids ( db , user id ) : reencrypt row content ( db , remote checkpoints , cp id , old decrypt func , new encrypt func , logger , ) logger . info ( "Finished re-encryption for user %s" , user id )
def memoize single arg ( f ) : memo = { } @ wraps ( f ) def memoized f ( arg ) : try : return memo [ arg ] except Key Error : result = memo [ arg ] = f ( arg ) return result return memoized f
def delete checkpoint ( self , checkpoint id , path ) : with self . engine . begin ( ) as db : return delete single remote checkpoint ( db , self . user id , path , checkpoint id , )
def get checkpoint content ( self , checkpoint id , path ) : with self . engine . begin ( ) as db : return get remote checkpoint ( db , self . user id , path , checkpoint id , self . crypto . decrypt , ) [ 'content' ]
def list checkpoints ( self , path ) : with self . engine . begin ( ) as db : return list remote checkpoints ( db , self . user id , path )
def rename all checkpoints ( self , old path , new path ) : with self . engine . begin ( ) as db : return move remote checkpoints ( db , self . user id , old path , new path , )
def delete all checkpoints ( self , path ) : with self . engine . begin ( ) as db : delete remote checkpoints ( db , self . user id , path )
def purge db ( self ) : with self . engine . begin ( ) as db : purge remote checkpoints ( db , self . user id )
def apply prefix ( prefix , model ) : if not isinstance ( model , dict ) : raise Type Error ( "Expected dict for model, got %s" % type ( model ) ) # We get unwanted leading/trailing slashes if prefix or model['path'] are # '', both of which are legal values. model [ 'path' ] = '/' . join ( ( prefix , model [ 'path' ] ) ) . strip ( '/' ) if model [ 'type' ] in ( 'notebook' , 'file' ) : return model if model [ 'type' ] != 'directory' : raise Value Error ( "Unknown model type %s." % type ( model ) ) content = model . get ( 'content' , None ) if content is not None : for sub model in content : apply prefix ( prefix , sub model ) return model
def path dispatch1 ( mname , returns model ) : def wrapper ( self , * args , * * kwargs ) : path , args = get arg ( 'path' , args , kwargs ) prefix , mgr , mgr path = resolve path ( path , self . managers ) result = getattr ( mgr , mname ) ( mgr path , * args , * * kwargs ) if returns model and prefix : return apply prefix ( prefix , result ) else : return result return wrapper
def path dispatch old new ( mname , returns model ) : def wrapper ( self , old path , new path , * args , * * kwargs ) : old prefix , old mgr , old mgr path = resolve path ( old path , self . managers ) new prefix , new mgr , new mgr path = resolve path ( new path , self . managers , ) if old mgr is not new mgr : # TODO: Consider supporting this via get+delete+save. raise HTTP Error ( 400 , "Can't move files between backends ({old} -> {new})" . format ( old = old path , new = new path , ) ) assert new prefix == old prefix result = getattr ( new mgr , mname ) ( old mgr path , new mgr path , * args , * * kwargs ) if returns model and new prefix : return apply prefix ( new prefix , result ) else : return result return wrapper
def managers changed ( self , name , old , new ) : for key in new : if '/' in key : raise Value Error ( "Expected directory names w/o slashes.  Got [%s]" % key ) self . managers = { k . strip ( '/' ) : v for k , v in new . items ( ) }
def get ( self , path , content = True , type = None , format = None ) : path = normalize api path ( path ) if path : return self . get ( path , content = content , type = type , format = format ) if not content : return base directory model ( '' ) extra content = self . extra root dirs ( ) rm = self . root manager if rm is None : root model = base directory model ( '' ) root model . update ( format = 'json' , content = extra content , ) else : root model = rm . get ( path , content = content , type = type , format = format , ) # Append the extra directories. root model [ 'content' ] . extend ( extra content ) return root model
def split api filepath ( path ) : parts = path . rsplit ( '/' , 1 ) if len ( parts ) == 1 : name = parts [ 0 ] dirname = '/' else : name = parts [ 1 ] dirname = parts [ 0 ] + '/' return from api dirname ( dirname ) , name
def writes base64 ( nb , version = NBFORMAT VERSION ) : return b64encode ( writes ( nb , version = version ) . encode ( 'utf-8' ) )
def reads base64 ( nb , as version = NBFORMAT VERSION ) : try : return reads ( b64decode ( nb ) . decode ( 'utf-8' ) , as version = as version ) except Exception as e : raise Corrupted File ( e )
def prefix dirs ( path ) : dirname = posixpath . dirname path = path . strip ( '/' ) out = [ ] while path != '' : path = dirname ( path ) out . append ( path ) return reversed ( out )
def outside root to 404 ( fn ) : @ wraps ( fn ) def wrapped ( * args , * * kwargs ) : try : return fn ( * args , * * kwargs ) except Path Outside Root as e : raise HTTP Error ( 404 , "Path outside root: [%s]" % e . args [ 0 ] ) return wrapped
def create user ( db url , user ) : Postgres Checkpoints ( db url = db url , user id = user , create user on startup = True , )
def walk dirs ( mgr , dirs ) : for directory in dirs : children = mgr . get ( directory , content = True , type = 'directory' , ) [ 'content' ] dirs , files = map ( sorted , separate dirs files ( children ) ) yield directory , dirs , files if dirs : for entry in walk dirs ( mgr , dirs ) : yield entry
def walk files ( mgr ) : for dir , subdirs , files in walk files ( mgr ) : for file in files : yield file
def walk files with content ( mgr ) : for , , files in walk ( mgr ) : for f in files : yield mgr . get ( f , content = True )
def reencrypt single user ( engine , user id , old crypto , new crypto , logger ) : # Use Fallback Crypto so that we're re-entrant if we halt partway through. crypto = Fallback Crypto ( [ new crypto , old crypto ] ) reencrypt user content ( engine = engine , user id = user id , old decrypt func = crypto . decrypt , new encrypt func = crypto . encrypt , logger = logger , )
def unencrypt single user ( engine , user id , old crypto , logger ) : reencrypt user content ( engine = engine , user id = user id , old decrypt func = old crypto . decrypt , new encrypt func = lambda s : s , logger = logger , )
def upgrade ( db url , revision ) : with temp alembic ini ( ALEMBIC DIR LOCATION , db url ) as alembic ini : subprocess . check call ( [ 'alembic' , '-c' , alembic ini , 'upgrade' , revision ] )
def queue instance ( self , embed type , data ) : serializer = self . serializers . get ( embed type , None ) if serializer is None : return instance id = serializer . get id ( data ) if embed type not in self . ids : self . ids [ embed type ] = [ ] self . ids [ embed type ] . append ( instance id )
def insert instance ( self , block ) : embed type = block . get ( 'type' , None ) data = block . get ( 'data' , { } ) serializer = self . serializers . get ( embed type , None ) if serializer is None : return block try : instance id = serializer . get id ( data ) instance = self . instances [ embed type ] [ instance id ] data [ embed type ] = serializer . serialize ( instance ) except : data [ embed type ] = None block [ 'data' ] = data return block
def load data ( self ) : for embed type in self . ids . keys ( ) : self . load instances ( embed type , self . ids [ embed type ] )
def validate ( self , data ) : from dispatch . theme import Theme Manager errors = { } if data . get ( 'widget' ) is not None : try : widget = Theme Manager . Widgets . get ( data [ 'widget' ] ) except Widget Not Found as e : errors [ 'widget' ] = str ( e ) else : for field in widget . fields : field data = data [ 'data' ] . get ( field . name ) if field data is not None : try : field . validate ( field data ) except Invalid Field as e : errors [ field . name ] = str ( e ) elif field . required : errors [ field . name ] = '%s is required' % field . label if errors : raise Validation Error ( errors ) return data
def admin ( request ) : context = { 'api url' : settings . API URL , 'app js bundle' : 'manager-%s.js' % dispatch . version , 'app css bundle' : 'manager-%s.css' % dispatch . version } return render to response ( 'manager/index.html' , context )
def to json ( self ) : result = { } for field in self . fields : result [ field . name ] = field . to json ( self . data . get ( field . name ) ) return result
def exclude fields ( self ) : request = self . context . get ( 'request' ) if request : exclude = request . query params . get ( 'exclude' , None ) if exclude is None : return excluded fields = exclude . split ( ',' ) for field in excluded fields : self . fields . pop ( field )
def get ( self , * args , * * kwargs ) : if 'pk' in kwargs : kwargs [ 'parent' ] = kwargs [ 'pk' ] kwargs [ 'head' ] = True del kwargs [ 'pk' ] if 'request' in kwargs : request = kwargs [ 'request' ] version = request . GET . get ( 'version' , None ) preview id = request . GET . get ( 'preview id' , None ) if ( version is not None ) and ( preview id is not None ) : kwargs [ 'revision id' ] = version kwargs [ 'preview id' ] = preview id del kwargs [ 'is published' ] del kwargs [ 'request' ] return super ( Publishable Manager , self ) . get ( * args , * * kwargs )
def get attribute ( self , instance ) : attr = super ( Null Boolean Field , self ) . get attribute ( instance ) return True if attr else False
def validate widget ( widget ) : if not has valid id ( widget ) : raise Invalid Widget ( "%s must contain a valid 'id' attribute" % widget . name ) if not has valid name ( widget ) : raise Invalid Widget ( "%s must contain a valid 'name' attribute" % widget . name ) if not has valid template ( widget ) : raise Invalid Widget ( "%s must contain a valid 'template' attribute" % widget . name ) if not hasattr ( widget , 'zones' ) or not widget . zones : raise Invalid Widget ( "%s must be compatible with at least one zone" % widget . name )
def validate zone ( zone ) : if not has valid id ( zone ) : raise Invalid Zone ( "%s must contain a valid 'id' attribute" % zone . name ) if not has valid name ( zone ) : raise Invalid Zone ( "%s must contain a valid 'name' attribute" % zone . name )
def is valid uuid ( id ) : if not isinstance ( id , basestring ) : return False try : val = UUID ( id , version = 4 ) except Value Error : return False return True
def get permissions ( self ) : permissions = '' if self . groups . filter ( name = 'Admin' ) . exists ( ) or self . is superuser : permissions = 'admin' return permissions
def modify permissions ( self , permissions ) : group = Group . objects . get ( name = 'Admin' ) if permissions == 'admin' : self . groups . add ( group ) else : self . groups . remove ( group )
def Author Validator ( data ) : if not isinstance ( data , list ) : # Convert single instance to a list data = [ data ] for author in data : if 'person' not in author : raise Validation Error ( 'An author must contain a person.' ) if 'type' in author and not isinstance ( author [ 'type' ] , basestring ) : # If type is defined, it should be a string raise Validation Error ( 'The author type must be a string.' )
def save ( self , validated data ) : ( zone , created ) = Zone Model . objects . get or create ( zone id = self . id ) zone . widget id = validated data [ 'widget' ] zone . data = validated data [ 'data' ] # Call widget before-save hook on nested widgets for key in list ( zone . data . keys ( ) ) : if isinstance ( zone . data [ key ] , dict ) and ( 'id' in zone . data [ key ] . keys ( ) ) and ( 'data' in zone . data [ key ] . keys ( ) ) : zone . data [ key ] [ 'data' ] = self . before save ( zone . data [ key ] [ 'id' ] , zone . data [ key ] [ 'data' ] ) # Call widget before-save hook zone . data = self . before save ( zone . widget id , zone . data ) return zone . save ( )
def get data ( self ) : result = { } for field in self . fields : result [ field . name ] = self . data . get ( field . name ) return result
def prepare data ( self ) : result = { } for field in self . fields : data = self . data . get ( field . name ) result [ field . name ] = field . prepare data ( data ) return result
def render ( self , data = None , add context = None ) : template = loader . get template ( self . template ) if not data : data = self . context ( self . prepare data ( ) ) if add context is not None : for key , value in add context . iteritems ( ) : if key in self . accepted keywords : data [ key ] = value return template . render ( data )
def callback ( cls , user , query ) : # Get settings for this integration settings = cls . get settings ( show hidden = True ) fb = Facebook ( ) payload = { 'client id' : settings [ 'client id' ] , 'client secret' : settings [ 'client secret' ] , 'code' : query [ 'code' ] , 'redirect uri' : cls . REDIRECT URI } try : # Authenticate with Facebook fb . get access token ( payload ) # Fetch pages belonging to authenticated user pages = fb . list pages ( 'me' ) except Facebook API Error , e : raise Integration Callback Error ( e . message ) return { 'pages' : pages }
def get settings ( self , integration id ) : try : integration = self . get ( integration id = integration id ) return json . loads ( integration . settings ) except ( self . model . Does Not Exist , Value Error ) : return { }
def update settings ( self , integration id , settings ) : ( integration , created ) = self . get or create ( integration id = integration id ) try : current settings = json . loads ( integration . settings ) except Value Error : current settings = { } current settings . update ( settings ) integration . settings = json . dumps ( current settings ) integration . save ( )
def signup ( request , uuid = None ) : invite = get object or 404 ( Invite . objects . all ( ) , id = uuid ) if invite . expiration date < timezone . now ( ) : invite . delete ( ) raise Http404 ( 'This page does not exist.' ) if request . method == 'POST' : form = Sign Up Form ( request . POST ) if form . is valid ( ) : user = form . save ( commit = False ) user . email = invite . email user . person = invite . person user . save ( ) if invite . permissions == 'admin' : group = Group . objects . get ( name = 'Admin' ) user . groups . add ( group ) invite . delete ( ) return redirect ( 'dispatch-admin' ) else : return render ( request , 'registration/signup.html' , { 'form' : form , 'email' : invite . email } ) else : form = Sign Up Form ( ) return render ( request , 'registration/signup.html' , { 'form' : form , 'email' : invite . email } )
def zone ( zone id , * * kwargs ) : try : zone = Theme Manager . Zones . get ( zone id ) except Zone Not Found : return '' try : return zone . widget . render ( add context = kwargs ) except ( Widget Not Found , Attribute Error ) : pass return ''
def save subsection ( self , subsection id ) : Article . objects . filter ( parent id = self . parent . id ) . update ( subsection id = subsection id )
def get extension ( self ) : ext = os . path . splitext ( self . img . name ) [ 1 ] if ext : # Remove period from extension return ext [ 1 : ] return ext
def get medium url ( self ) : if self . is gif ( ) : return self . get absolute url ( ) return '%s%s-%s.jpg' % ( settings . MEDIA URL , self . get name ( ) , 'medium' )
def save ( self , * * kwargs ) : is new = self . pk is None if is new : # Make filenames lowercase self . img . name = self . img . name . lower ( ) # Call super method super ( Image , self ) . save ( * * kwargs ) if is new and self . img : data = self . img . read ( ) if not data : return image = Img . open ( String IO . String IO ( data ) ) self . width , self . height = image . size super ( Image , self ) . save ( ) name = self . get name ( ) ext = self . get extension ( ) for size in self . SIZES . keys ( ) : self . save thumbnail ( image , self . SIZES [ size ] , name , size , ext )
def save thumbnail ( self , image , size , name , label , file type ) : width , height = size ( imw , imh ) = image . size # If image is larger than thumbnail size, resize image if ( imw > width ) or ( imh > height ) : image . thumbnail ( size , Img . ANTIALIAS ) # Attach new thumbnail label to image filename name = "%s-%s.jpg" % ( name , label ) # Image.save format takes JPEG not jpg if file type in self . JPG FORMATS : file type = 'JPEG' # Write new thumbnail to String IO object image io = String IO . String IO ( ) image . save ( image io , format = file type , quality = 75 ) # Convert String IO object to Django File object thumb file = In Memory Uploaded File ( image io , None , name , 'image/jpeg' , image io . len , None ) # Save the new file to the default storage system default storage . save ( name , thumb file )
def decrement ( self ) : with self . lock : if self . count == 0 : raise Runtime Error ( 'Counter is at zero. It cannot dip below zero' ) self . count -= 1 if self . is finalized and self . count == 0 : self . callback ( )
def set exception ( self , exception ) : if not self . done ( ) : raise Transfer Not Done Error ( 'set exception can only be called once the transfer is ' 'complete.' ) self . coordinator . set exception ( exception , override = True )
def add done callback ( self , function , * args , * * kwargs ) : with self . done callbacks lock : self . done callbacks . append ( Function Container ( function , * args , * * kwargs ) )
def add failure cleanup ( self , function , * args , * * kwargs ) : with self . failure cleanups lock : self . failure cleanups . append ( Function Container ( function , * args , * * kwargs ) )
def iter step func decorators ( self ) : func defs = [ func for func in self . py tree . iter funcdefs ( ) ] + [ func for cls in self . py tree . iter classdefs ( ) for func in cls . iter funcdefs ( ) ] for func in func defs : for decorator in func . get decorators ( ) : if decorator . children [ 1 ] . value == 'step' : yield func , decorator break
def iter steps ( self ) : for func , decorator in self . iter step func decorators ( ) : step = self . step decorator args ( decorator ) if step : span = self . span from pos ( decorator . start pos , func . end pos ) yield step , func . name . value , span
def find step node ( self , step text ) : for func , decorator in self . iter step func decorators ( ) : step = self . step decorator args ( decorator ) arg node = decorator . children [ 3 ] if step == step text : return arg node , func elif isinstance ( step , list ) and step text in step : idx = step . index ( step text ) step node = arg node . children [ 1 ] . children [ idx * 2 ] return step node , func return None , None
def iter step func decorators ( self ) : for node in self . py tree . find all ( 'def' ) : for decorator in node . decorators : if decorator . name . value == 'step' : yield node , decorator break
def step decorator args ( self , decorator ) : args = decorator . call . value step = None if len ( args ) == 1 : try : step = args [ 0 ] . value . to python ( ) except ( Value Error , Syntax Error ) : pass if isinstance ( step , six . string types + ( list , ) ) : return step logging . error ( , self . file path ) else : logging . error ( "Decorator step accepts only one argument - %s" , self . file path )
def iter steps ( self ) : for func , decorator in self . iter step func decorators ( ) : step = self . step decorator args ( decorator ) if step : yield step , func . name , self . span for node ( func , True )
def find step node ( self , step text ) : for func , decorator in self . iter step func decorators ( ) : step = self . step decorator args ( decorator ) arg node = decorator . call . value [ 0 ] . value if step == step text : return arg node , func elif isinstance ( step , list ) and step text in step : step node = arg node [ step . index ( step text ) ] return step node , func return None , None
def POST ( self ) : # Get the POST data sent from Webex Teams json data = web . data ( ) print ( "\n WEBHOOK POST RECEIVED:" ) print ( json data , "\n" ) # Create a Webhook object from the JSON data webhook obj = Webhook ( json data ) # Get the room details room = api . rooms . get ( webhook obj . data . room Id ) # Get the message details message = api . messages . get ( webhook obj . data . id ) # Get the sender's details person = api . people . get ( message . person Id ) print ( "NEW MESSAGE IN ROOM '{}'" . format ( room . title ) ) print ( "FROM '{}'" . format ( person . display Name ) ) print ( "MESSAGE '{}'\n" . format ( message . text ) ) # This is a VERY IMPORTANT loop prevention control step. # If you respond to all messages...  You will respond to the messages # that the bot posts and thereby create a loop condition. me = api . people . me ( ) if message . person Id == me . id : # Message was sent by me (bot); do not respond. return 'OK' else : # Message was sent by someone else; parse message and respond. if "/CAT" in message . text : print ( "FOUND '/CAT'" ) # Get a cat fact cat fact = get catfact ( ) print ( "SENDING CAT FACT '{}'" . format ( cat fact ) ) # Post the fact to the room where the request was received api . messages . create ( room . id , text = cat fact ) return 'OK'
def validate base url ( base url ) : parsed url = urllib . parse . urlparse ( base url ) if parsed url . scheme and parsed url . netloc : return parsed url . geturl ( ) else : error message = "base url must contain a valid scheme (protocol " "specifier) and network location (hostname)" raise Value Error ( error message )
def is web url ( string ) : assert isinstance ( string , basestring ) parsed url = urllib . parse . urlparse ( string ) return ( ( parsed url . scheme . lower ( ) == 'http' or parsed url . scheme . lower ( ) == 'https' ) and parsed url . netloc )
def open local file ( file path ) : assert isinstance ( file path , basestring ) assert is local file ( file path ) file name = os . path . basename ( file path ) file object = open ( file path , 'rb' ) content type = mimetypes . guess type ( file name ) [ 0 ] or 'text/plain' return Encodable File ( file name = file name , file object = file object , content type = content type )
def strptime ( cls , date string , format = WEBEX TEAMS DATETIME FORMAT ) : return super ( Webex Teams Date Time , cls ) . strptime ( date string , format ) . replace ( tzinfo = Zulu Time Zone ( ) )
def created ( self ) : created = self . json data . get ( 'created' ) if created : return Webex Teams Date Time . strptime ( created ) else : return None
def wait on rate limit ( self , value ) : check type ( value , bool , may be none = False ) self . wait on rate limit = value
def serialize ( cls , data ) : if hasattr ( data , " hash " ) and callable ( data . hash ) : # If the data is already hashable (should be immutable) return it return data elif isinstance ( data , list ) : # Freeze the elements of the list and return as a tuple return tuple ( ( cls . serialize ( item ) for item in data ) ) elif isinstance ( data , dict ) : # Freeze the elements of the dictionary, sort them, and return # them as a list of tuples key value tuples = [ ( key , cls . serialize ( value ) ) for key , value in data . items ( ) ] key value tuples . sort ( ) return tuple ( key value tuples ) else : raise Type Error ( "Unable to freeze {} data type." . format ( type ( data ) ) )
def last Activity ( self ) : last activity = self . json data . get ( 'last Activity' ) if last activity : return Webex Teams Date Time . strptime ( last activity ) else : return None
def post events service ( request ) : # Get the POST data sent from Webex Teams json data = request . json log . info ( "\n" ) log . info ( "WEBHOOK POST RECEIVED:" ) log . info ( json data ) log . info ( "\n" ) # Create a Webhook object from the JSON data webhook obj = Webhook ( json data ) # Get the room details room = api . rooms . get ( webhook obj . data . room Id ) # Get the message details message = api . messages . get ( webhook obj . data . id ) # Get the sender's details person = api . people . get ( message . person Id ) log . info ( "NEW MESSAGE IN ROOM '{}'" . format ( room . title ) ) log . info ( "FROM '{}'" . format ( person . display Name ) ) log . info ( "MESSAGE '{}'\n" . format ( message . text ) ) # This is a VERY IMPORTANT loop prevention control step. # If you respond to all messages...  You will respond to the messages # that the bot posts and thereby create a loop condition. me = api . people . me ( ) if message . person Id == me . id : # Message was sent by me (bot); do not respond. return { 'Message' : 'OK' } else : # Message was sent by someone else; parse message and respond. if "/CAT" in message . text : log . info ( "FOUND '/CAT'" ) # Get a cat fact catfact = get catfact ( ) log . info ( "SENDING CAT FACT'{}'" . format ( catfact ) ) # Post the fact to the room where the request was received api . messages . create ( room . id , text = catfact ) return { 'Message' : 'OK' }
def get ngrok public url ( ) : try : response = requests . get ( url = NGROK CLIENT API BASE URL + "/tunnels" , headers = { 'content-type' : 'application/json' } ) response . raise for status ( ) except requests . exceptions . Request Exception : print ( "Could not connect to the ngrok client API; " "assuming not running." ) return None else : for tunnel in response . json ( ) [ "tunnels" ] : if tunnel . get ( "public url" , "" ) . startswith ( "http://" ) : print ( "Found ngrok public HTTP URL:" , tunnel [ "public url" ] ) return tunnel [ "public url" ]
def delete webhooks with name ( api , name ) : for webhook in api . webhooks . list ( ) : if webhook . name == name : print ( "Deleting Webhook:" , webhook . name , webhook . target Url ) api . webhooks . delete ( webhook . id )
def create ngrok webhook ( api , ngrok public url ) : print ( "Creating Webhook..." ) webhook = api . webhooks . create ( name = WEBHOOK NAME , target Url = urljoin ( ngrok public url , WEBHOOK URL SUFFIX ) , resource = WEBHOOK RESOURCE , event = WEBHOOK EVENT , ) print ( webhook ) print ( "Webhook successfully created." ) return webhook
def main ( ) : api = Webex Teams API ( ) delete webhooks with name ( api , name = WEBHOOK NAME ) public url = get ngrok public url ( ) if public url is not None : create ngrok webhook ( api , public url )
def console ( ) : parser = argparse . Argument Parser ( description = console . doc ) parser . add argument ( '--device' , default = '/dev/tty USB0' , help = 'port to read DSMR data from' ) parser . add argument ( '--host' , default = None , help = 'alternatively connect using TCP host.' ) parser . add argument ( '--port' , default = None , help = 'TCP port to use for connection' ) parser . add argument ( '--version' , default = '2.2' , choices = [ '2.2' , '4' ] , help = 'DSMR version (2.2, 4)' ) parser . add argument ( '--verbose' , '-v' , action = 'count' ) args = parser . parse args ( ) if args . verbose : level = logging . DEBUG else : level = logging . ERROR logging . basic Config ( level = level ) loop = asyncio . get event loop ( ) def print callback ( telegram ) : """Callback that prints telegram values.""" for obiref , obj in telegram . items ( ) : if obj : print ( obj . value , obj . unit ) print ( ) # create tcp or serial connection depending on args if args . host and args . port : create connection = partial ( create tcp dsmr reader , args . host , args . port , args . version , print callback , loop = loop ) else : create connection = partial ( create dsmr reader , args . device , args . version , print callback , loop = loop ) try : # connect and keep connected until interrupted by ctrl-c while True : # create serial or tcp connection conn = create connection ( ) transport , protocol = loop . run until complete ( conn ) # wait until connection it closed loop . run until complete ( protocol . wait closed ( ) ) # wait 5 seconds before attempting reconnect loop . run until complete ( asyncio . sleep ( 5 ) ) except Keyboard Interrupt : # cleanup connection after user initiated shutdown transport . close ( ) loop . run until complete ( asyncio . sleep ( 0 ) ) finally : loop . close ( )
def create dsmr protocol ( dsmr version , telegram callback , loop = None ) : if dsmr version == '2.2' : specification = telegram specifications . V2 2 serial settings = SERIAL SETTINGS V2 2 elif dsmr version == '4' : specification = telegram specifications . V4 serial settings = SERIAL SETTINGS V4 elif dsmr version == '5' : specification = telegram specifications . V5 serial settings = SERIAL SETTINGS V5 else : raise Not Implemented Error ( "No telegram parser found for version: %s" , dsmr version ) protocol = partial ( DSMR Protocol , loop , Telegram Parser ( specification ) , telegram callback = telegram callback ) return protocol , serial settings
def create dsmr reader ( port , dsmr version , telegram callback , loop = None ) : protocol , serial settings = create dsmr protocol ( dsmr version , telegram callback , loop = None ) serial settings [ 'url' ] = port conn = create serial connection ( loop , protocol , * * serial settings ) return conn
def create tcp dsmr reader ( host , port , dsmr version , telegram callback , loop = None ) : protocol , = create dsmr protocol ( dsmr version , telegram callback , loop = None ) conn = loop . create connection ( protocol , host , port ) return conn
def data received ( self , data ) : data = data . decode ( 'ascii' ) self . log . debug ( 'received data: %s' , data ) self . telegram buffer . append ( data ) for telegram in self . telegram buffer . get all ( ) : self . handle telegram ( telegram )
def connection lost ( self , exc ) : if exc : self . log . exception ( 'disconnected due to exception' ) else : self . log . info ( 'disconnected because of close/abort.' ) self . closed . set ( )
def handle telegram ( self , telegram ) : self . log . debug ( 'got telegram: %s' , telegram ) try : parsed telegram = self . telegram parser . parse ( telegram ) except Invalid Checksum Error as e : self . log . warning ( str ( e ) ) except Parse Error : self . log . exception ( "failed to parse telegram" ) else : self . telegram callback ( parsed telegram )
def ensure python ( specs ) : if not isinstance ( specs , ( list , tuple ) ) : specs = [ specs ] v = sys . version info part = '%s.%s' % ( v . major , v . minor ) for spec in specs : if part == spec : return try : if eval ( part + spec ) : return except Syntax Error : pass raise Value Error ( 'Python version %s unsupported' % part )
def find packages ( top = HERE ) : packages = [ ] for d , dirs , in os . walk ( top , followlinks = True ) : if os . path . exists ( pjoin ( d , ' init .py' ) ) : packages . append ( os . path . relpath ( d , top ) . replace ( os . path . sep , '.' ) ) elif d != top : # Do not look for packages in subfolders if current is not a package dirs [ : ] = [ ] return packages
def command for func ( func ) : class Func Command ( Base Command ) : def run ( self ) : func ( ) update package data ( self . distribution ) return Func Command
def run ( cmd , * * kwargs ) : log . info ( '> ' + list2cmdline ( cmd ) ) kwargs . setdefault ( 'cwd' , HERE ) kwargs . setdefault ( 'shell' , os . name == 'nt' ) if not isinstance ( cmd , ( list , tuple ) ) and os . name != 'nt' : cmd = shlex . split ( cmd ) cmd [ 0 ] = which ( cmd [ 0 ] ) return subprocess . check call ( cmd , * * kwargs )
def get file handler ( package data spec , data files spec ) : class File Handler ( Base Command ) : def run ( self ) : package data = self . distribution . package data package spec = package data spec or dict ( ) for ( key , patterns ) in package spec . items ( ) : package data [ key ] = get package data ( key , patterns ) self . distribution . data files = get data files ( data files spec , self . distribution . data files ) return File Handler
def compile pattern ( pat , ignore case = True ) : if isinstance ( pat , bytes ) : pat str = pat . decode ( 'ISO-8859-1' ) res str = translate glob ( pat str ) res = res str . encode ( 'ISO-8859-1' ) else : res = translate glob ( pat ) flags = re . IGNORECASE if ignore case else 0 return re . compile ( res , flags = flags ) . match
def translate glob ( pat ) : translated parts = [ ] for part in iexplode path ( pat ) : translated parts . append ( translate glob part ( part ) ) os sep class = '[%s]' % re . escape ( SEPARATORS ) res = join translated ( translated parts , os sep class ) return '{res}\\Z(?ms)' . format ( res = res )
def translate glob part ( pat ) : # Code modified from Python 3 standard lib fnmatch: if pat == '**' : return '.*' i , n = 0 , len ( pat ) res = [ ] while i < n : c = pat [ i ] i = i + 1 if c == '*' : # Match anything but path separators: res . append ( '[^%s]*' % SEPARATORS ) elif c == '?' : res . append ( '[^%s]?' % SEPARATORS ) elif c == '[' : j = i if j < n and pat [ j ] == '!' : j = j + 1 if j < n and pat [ j ] == ']' : j = j + 1 while j < n and pat [ j ] != ']' : j = j + 1 if j >= n : res . append ( '\\[' ) else : stuff = pat [ i : j ] . replace ( '\\' , '\\\\' ) i = j + 1 if stuff [ 0 ] == '!' : stuff = '^' + stuff [ 1 : ] elif stuff [ 0 ] == '^' : stuff = '\\' + stuff res . append ( '[%s]' % stuff ) else : res . append ( re . escape ( c ) ) return '' . join ( res )
def qsize ( self , extra predicate = None ) : count = self . query queued ( 'COUNT(*) AS count' , extra predicate = extra predicate ) return count [ 0 ] . count
def enqueue ( self , data ) : jsonified data = json . dumps ( data ) with self . db conn ( ) as conn : return conn . execute ( 'INSERT INTO %s (created, data) VALUES (%%(created)s, %%(data)s)' % self . table name , created = datetime . utcnow ( ) , data = jsonified data )
def build extra predicate ( self , extra predicate ) : if extra predicate is None : return '' # if they don't have a supported format seq, wrap it for them if not isinstance ( extra predicate [ 1 ] , ( list , dict , tuple ) ) : extra predicate = [ extra predicate [ 0 ] , ( extra predicate [ 1 ] , ) ] extra predicate = database . escape query ( * extra predicate ) return 'AND (' + extra predicate + ')'
def simplejson datetime serializer ( obj ) : if hasattr ( obj , 'isoformat' ) : return obj . isoformat ( ) else : raise Type Error ( 'Object of type %s with value of %s is not JSON serializable' % ( type ( obj ) , repr ( obj ) ) )
def reconnect ( self ) : conn = mysql . connect ( * * self . db args ) if conn is not None : self . close ( ) self . db = conn
def get ( self , query , * parameters , * * kwparameters ) : rows = self . query ( query , parameters , kwparameters ) if not rows : return None elif not isinstance ( rows , list ) : raise My SQL Error ( "Query is not a select query" ) elif len ( rows ) > 1 : raise My SQL Error ( "Multiple rows returned for Database.get() query" ) else : return rows [ 0 ]
def execute ( self , query , * parameters , * * kwparameters ) : return self . execute lastrowid ( query , * parameters , * * kwparameters )
def execute lastrowid ( self , query , * parameters , * * kwparameters ) : self . execute ( query , parameters , kwparameters ) self . result = self . db . store result ( ) return self . db . insert id ( )
def get connection ( db = DATABASE ) : return database . connect ( host = HOST , port = PORT , user = USER , password = PASSWORD , database = db )
def run benchmark ( ) : stopping = threading . Event ( ) workers = [ Insert Worker ( stopping ) for in range ( NUM WORKERS ) ] print ( 'Launching %d workers' % NUM WORKERS ) [ worker . start ( ) for worker in workers ] time . sleep ( WORKLOAD TIME ) print ( 'Stopping workload' ) stopping . set ( ) [ worker . join ( ) for worker in workers ] with get connection ( ) as conn : count = conn . get ( "SELECT COUNT(*) AS count FROM %s" % TABLE ) . count print ( "%d rows inserted using %d workers" % ( count , NUM WORKERS ) ) print ( "%.1f rows per second" % ( count / float ( WORKLOAD TIME ) ) )
def connect ( self ) : with self . lock : if self . aggregator : try : return self . pool connect ( self . aggregator ) except Pool Connection Exception : self . aggregator = None if not len ( self . aggregators ) : with self . pool connect ( self . primary aggregator ) as conn : self . update aggregator list ( conn ) conn . expire ( ) random . shuffle ( self . aggregators ) last exception = None for aggregator in self . aggregators : self . logger . debug ( 'Attempting connection with %s:%s' % ( aggregator [ 0 ] , aggregator [ 1 ] ) ) try : conn = self . pool connect ( aggregator ) # connection successful! self . aggregator = aggregator return conn except Pool Connection Exception as e : # connection error last exception = e else : # bad news bears...  try again later self . aggregator = None self . aggregators = [ ] raise last exception
def lookup by number ( errno ) : for key , val in globals ( ) . items ( ) : if errno == val : print ( key )
def size ( self ) : return sum ( q . qsize ( ) for q in self . connections . values ( ) ) + len ( self . fairies )
def ping ( self ) : with self . db conn ( ) as conn : affected rows = conn . query ( % self . manager . table name , datetime . utcnow ( ) , self . lock id , self . lock hash ) return bool ( affected rows == 1 )
def release ( self ) : if self . valid ( ) : with self . db conn ( ) as conn : affected rows = conn . query ( % self . manager . table name , self . lock id , self . lock hash ) return bool ( affected rows == 1 ) else : return False
def connect ( self , host = '127.0.0.1' , port = 3306 , user = 'root' , password = '' , database = None ) : if database is None : raise exceptions . Requires Database ( ) self . db args = { 'host' : host , 'port' : port , 'user' : user , 'password' : password , 'database' : database } with self . db conn ( ) as conn : conn . query ( 'SELECT 1' ) return self
def setup ( self ) : with self . db conn ( ) as conn : for table defn in self . tables . values ( ) : conn . execute ( table defn ) return self
def destroy ( self ) : with self . db conn ( ) as conn : for table name in self . tables : conn . execute ( 'DROP TABLE IF EXISTS %s' % table name ) return self
def ready ( self ) : with self . db conn ( ) as conn : tables = [ row . t for row in conn . query ( , self . db args [ 'database' ] ) ] return all ( [ table name in tables for table name in self . tables ] )
def valid ( self ) : if self . finished is not None : return False with self . db conn ( ) as conn : row = conn . get ( % self . queue . table name , now = datetime . utcnow ( ) , ttl = self . queue . execution ttl , task id = self . task id , execution id = self . execution id ) return bool ( row is not None and row . valid )
def ping ( self ) : if self . finished is not None : raise Already Finished ( ) with self . db conn ( ) as conn : success = conn . query ( % self . queue . table name , now = datetime . utcnow ( ) , task id = self . task id , execution id = self . execution id , ttl = self . queue . execution ttl ) if success != 1 : raise Task Does Not Exist ( )
def start step ( self , step name ) : if self . finished is not None : raise Already Finished ( ) step data = self . get step ( step name ) if step data is not None : if 'stop' in step data : raise Step Already Finished ( ) else : raise Step Already Started ( ) steps = copy . deepcopy ( self . steps ) steps . append ( { "start" : datetime . utcnow ( ) , "name" : step name } ) self . save ( steps = steps )
def stop step ( self , step name ) : if self . finished is not None : raise Already Finished ( ) steps = copy . deepcopy ( self . steps ) step data = self . get step ( step name , steps = steps ) if step data is None : raise Step Not Started ( ) elif 'stop' in step data : raise Step Already Finished ( ) step data [ 'stop' ] = datetime . utcnow ( ) step data [ 'duration' ] = util . timedelta total seconds ( step data [ 'stop' ] - step data [ 'start' ] ) self . save ( steps = steps )
def create ( self , device Type ) : r = self . api Client . post ( "api/v0002/device/types" , device Type ) if r . status code == 201 : return Device Type ( api Client = self . api Client , * * r . json ( ) ) else : raise Api Exception ( r )
def update ( self , device Uid , metadata = None , device Info = None , status = None ) : if not isinstance ( device Uid , Device Uid ) and isinstance ( device Uid , dict ) : device Uid = Device Uid ( * * device Uid ) device Url = "api/v0002/device/types/%s/devices/%s" % ( device Uid . type Id , device Uid . device Id ) data = { "status" : status , "device Info" : device Info , "metadata" : metadata } r = self . api Client . put ( device Url , data ) if r . status code == 200 : return Device ( api Client = self . api Client , * * r . json ( ) ) else : raise Api Exception ( r )
def find ( self , status = None , connected After = None ) : query Parms = { } if status : query Parms [ "status" ] = status if connected After : query Parms [ "connected After" ] = connected After return Iterable Client Status List ( self . api Client , filters = query Parms )
def list ( self ) : url = "api/v0002/mgmt/custom/bundle" r = self . api Client . get ( url ) if r . status code == 200 : return r . json ( ) else : raise Api Exception ( r )
def update Schema ( self , schema Id , schema Definition ) : req = Api Client . one Schema Url % ( self . host , "/draft" , schema Id ) body = { "schema Definition" : schema Definition } resp = requests . put ( req , auth = self . credentials , headers = { "Content-Type" : "application/json" } , data = json . dumps ( body ) , verify = self . verify ) if resp . status code == 200 : self . logger . debug ( "Schema updated" ) else : raise ibmiotf . API Exception ( resp . status code , "HTTP error updating schema" , resp ) return resp . json ( )
def disconnect ( self ) : # self.logger.info("Closing connection to the IBM Watson Io T Platform") self . client . disconnect ( ) # If we don't call loop stop() it appears we end up with a zombie thread which continues to process # network traffic, preventing any subsequent attempt to reconnect using connect() self . client . loop stop ( ) self . logger . info ( "Closed connection to the IBM Watson Io T Platform" )
def get ( self , device Uid , event Id ) : if not isinstance ( device Uid , Device Uid ) and isinstance ( device Uid , dict ) : device Uid = Device Uid ( * * device Uid ) url = "api/v0002/device/types/%s/devices/%s/events/%s" % ( device Uid . type Id , device Uid . device Id , event Id ) r = self . api Client . get ( url ) if r . status code == 200 : return Last Event ( * * r . json ( ) ) else : raise Api Exception ( r )
def get All ( self , device Uid ) : if not isinstance ( device Uid , Device Uid ) and isinstance ( device Uid , dict ) : device Uid = Device Uid ( * * device Uid ) url = "api/v0002/device/types/%s/devices/%s/events" % ( device Uid . type Id , device Uid . device Id ) r = self . api Client . get ( url ) if r . status code == 200 : events = [ ] for event in r . json ( ) : events . append ( Last Event ( * * event ) ) return events else : raise Api Exception ( r )
def load Byte Array ( self , page , return Error ) : return Error . contents . value = self . Illegal State Error raise Not Implemented Error ( "You must override this method." ) return ''
def check return ( result , func , cargs ) : if result != 0 : s = rt . Error Get Last Error Msg ( ) . decode ( ) msg = 'LAS Error in "%s": %s' % ( func . name , s ) rt . Error Reset ( ) raise R Tree Error ( msg ) return True
def check void ( result , func , cargs ) : if not bool ( result ) : s = rt . Error Get Last Error Msg ( ) . decode ( ) msg = 'Error in "%s": %s' % ( func . name , s ) rt . Error Reset ( ) raise R Tree Error ( msg ) return result
def check void done ( result , func , cargs ) : if rt . Error Get Error Count ( ) : s = rt . Error Get Last Error Msg ( ) . decode ( ) msg = 'Error in "%s": %s' % ( func . name , s ) rt . Error Reset ( ) raise R Tree Error ( msg ) return result
def load ( self ) : if isinstance ( self . application , str ) : return util . import app ( self . application ) else : return self . application
def init app ( self , app ) : if not hasattr ( app , 'extensions' ) : app . extensions = { } if 'common' in app . extensions : raise Runtime Error ( "Flask-Common extension already initialized" ) app . extensions [ 'common' ] = self self . app = app if 'COMMON FILESERVER DISABLED' not in app . config : with app . test request context ( ) : # Configure White Noise. app . wsgi app = White Noise ( app . wsgi app , root = url for ( 'static' , filename = '' ) [ 1 : ] ) self . cache = Cache ( app , config = { 'CACHE TYPE' : app . config . get ( "COMMON CACHE TYPE" , 'simple' ) } ) @ app . before request def before request callback ( ) : request . start time = maya . now ( ) @ app . after request def after request callback ( response ) : if 'COMMON POWERED BY DISABLED' not in current app . config : response . headers [ 'X-Powered-By' ] = 'Flask' if 'COMMON PROCESSED TIME DISABLED' not in current app . config : response . headers [ 'X-Processed-Time' ] = maya . now ( ) . epoch - request . start time . epoch return response @ app . route ( '/favicon.ico' ) def favicon ( ) : return redirect ( url for ( 'static' , filename = 'favicon.ico' ) , code = 301 )
def serve ( self , workers = None , * * kwargs ) : if self . app . debug : print ( crayons . yellow ( 'Booting Flask development server...' ) ) self . app . run ( ) else : print ( crayons . yellow ( 'Booting Gunicorn...' ) ) # Start the web server. server = Gunicorn Server ( self . app , workers = workers or number of gunicorn workers ( ) , worker class = 'egg:meinheld#gunicorn worker' , * * kwargs ) server . run ( )
def process image ( self , image , image format , save kwargs = { } ) : imagefile = Bytes IO ( ) inv image = Image Ops . invert ( image ) inv image . save ( imagefile , * * save kwargs ) return imagefile
def to python ( self , data ) : if data is not None : if hasattr ( data , 'open' ) : data . open ( ) return super ( Versatile Image Form Field , self ) . to python ( data )
def pre save ( self , model instance , add ) : file = super ( Versatile Image Field , self ) . pre save ( model instance , add ) self . update ppoi field ( model instance ) return file
def formfield ( self , * * kwargs ) : # This is a fairly standard way to set up some defaults # while letting the caller override them. defaults = { } if self . ppoi field : defaults [ 'form class' ] = Sized Image Centerpoint Click Django Admin Field if kwargs . get ( 'widget' ) is Admin File Widget : # Ensuring default admin widget is skipped (in favor of using # Sized Image Centerpoint Click Django Admin Field's default widget as # the default widget choice for use in the admin). # This is for two reasons: # 1. To prevent 'typical' admin users (those who want to use #    the PPOI 'click' widget by default) from having to #    specify a formfield overrides for each Model Admin class #    used by each model that has a Versatile Image Field. # 2. If a Versatile Image Field does not have a ppoi field specified #    it will 'fall back' to a Clearable File Input anyways. # If admin users do, in fact, want to force use of the # Admin File Widget they can simply subclass Admin File Widget and # specify it in their Model Admin.formfield overrides (though, # if that's the case, why are they using Versatile Image Field in # the first place?) del kwargs [ 'widget' ] defaults . update ( kwargs ) return super ( Versatile Image Field , self ) . formfield ( * * defaults )
def value to string ( self , obj ) : if DJANGO VERSION > ( 1 , 9 ) : value = self . value from object ( obj ) else : value = self . get val from obj ( obj ) return self . get prep value ( value )
def build filters and sizers ( self , ppoi value , create on demand ) : name = self . name if not name and self . field . placeholder image name : name = self . field . placeholder image name self . filters = Filter Library ( name , self . storage , versatileimagefield registry , ppoi value , create on demand ) for ( attr name , sizedimage cls ) in iteritems ( versatileimagefield registry . sizedimage registry ) : setattr ( self , attr name , sizedimage cls ( path to image = name , storage = self . storage , create on demand = create on demand , ppoi = ppoi value ) )
def get filtered root folder ( self ) : folder , filename = os . path . split ( self . name ) return os . path . join ( folder , VERSATILEIMAGEFIELD FILTERED DIRNAME , '' )
def get sized root folder ( self ) : folder , filename = os . path . split ( self . name ) return os . path . join ( VERSATILEIMAGEFIELD SIZED DIRNAME , folder , '' )
def get filtered sized root folder ( self ) : sized root folder = self . get sized root folder ( ) return os . path . join ( sized root folder , VERSATILEIMAGEFIELD FILTERED DIRNAME )
def retrieve image ( self , path to image ) : image = self . storage . open ( path to image , 'rb' ) file ext = path to image . rsplit ( '.' ) [ - 1 ] image format , mime type = get image metadata from file ext ( file ext ) return ( Image . open ( image ) , file ext , image format , mime type )
def ppoi as str ( self ) : return "%s %s" % ( str ( self . ppoi [ 0 ] ) . replace ( '.' , '-' ) , str ( self . ppoi [ 1 ] ) . replace ( '.' , '-' ) )
def get context ( self , name , value , attrs ) : if self . has template widget rendering : context = super ( Clearable File Input With Image Preview , self ) . get context ( name , value , attrs ) else : # Build the context manually. context = { } context [ 'widget' ] = { 'name' : name , 'is hidden' : self . is hidden , 'required' : self . is required , 'value' : self . format value ( value ) , 'attrs' : self . build attrs ( self . attrs , attrs ) , 'template name' : self . template name , 'type' : self . input type , } # It seems Django 1.11's Clearable File Input doesn't add everything to the 'widget' key, so we can't use it # in Multi Widget. Add it manually here. checkbox name = self . clear checkbox name ( name ) checkbox id = self . clear checkbox id ( checkbox name ) context [ 'widget' ] . update ( { 'checkbox name' : checkbox name , 'checkbox id' : checkbox id , 'is initial' : self . is initial ( value ) , 'input text' : self . input text , 'initial text' : self . initial text , 'clear checkbox label' : self . clear checkbox label , } ) if value and hasattr ( value , "url" ) : context [ 'widget' ] . update ( { 'hidden field id' : self . get hidden field id ( name ) , 'point stage id' : self . get point stage id ( name ) , 'ppoi id' : self . get ppoi id ( name ) , 'sized url' : self . get sized url ( value ) , 'image preview id' : self . image preview id ( name ) , } ) return context
def build attrs ( self , base attrs , extra attrs = None ) : attrs = base attrs . copy ( ) if extra attrs is not None : attrs . update ( extra attrs ) return attrs
def get filtered path ( path to image , filename key , storage ) : containing folder , filename = os . path . split ( path to image ) filtered filename = get filtered filename ( filename , filename key ) path to return = os . path . join ( * [ containing folder , VERSATILEIMAGEFIELD FILTERED DIRNAME , filtered filename ] ) # Removing spaces so this path is memcached key friendly path to return = path to return . replace ( ' ' , '' ) return path to return
def get url from image key ( image instance , image key ) : img key split = image key . split ( ' ' ) if 'x' in img key split [ - 1 ] : size key = img key split . pop ( - 1 ) else : size key = None img url = reduce ( getattr , img key split , image instance ) if size key : img url = img url [ size key ] . url return img url
def decode bytecode ( bytecode ) : bytecode wnd = memoryview ( bytecode ) while bytecode wnd : opcode id = byte2int ( bytecode wnd [ 0 ] ) opcode = OPCODE MAP [ opcode id ] if opcode . imm struct is not None : offs , imm , = opcode . imm struct . from raw ( None , bytecode wnd [ 1 : ] ) else : imm = None offs = 0 insn len = 1 + offs yield Instruction ( opcode , imm , insn len ) bytecode wnd = bytecode wnd [ insn len : ]
def decode module ( module , decode name subsections = False ) : module wnd = memoryview ( module ) # Read & yield module header. hdr = Module Header ( ) hdr len , hdr data , = hdr . from raw ( None , module wnd ) yield Module Fragment ( hdr , hdr data ) module wnd = module wnd [ hdr len : ] # Read & yield sections. while module wnd : sec = Section ( ) sec len , sec data , = sec . from raw ( None , module wnd ) # If requested, decode name subsections when encountered. if ( decode name subsections and sec data . id == SEC UNK and sec data . name == SEC NAME ) : sec wnd = sec data . payload while sec wnd : subsec = Name Sub Section ( ) subsec len , subsec data , = subsec . from raw ( None , sec wnd ) yield Module Fragment ( subsec , subsec data ) sec wnd = sec wnd [ subsec len : ] else : yield Module Fragment ( sec , sec data ) module wnd = module wnd [ sec len : ]
def deprecated func ( func ) : # We use a mutable container here to work around Py2's lack of # the `nonlocal` keyword. first usage = [ True ] @ functools . wraps ( func ) def wrapper ( * args , * * kwargs ) : if first usage [ 0 ] : warnings . warn ( "Call to deprecated function {}." . format ( func . name ) , Deprecation Warning , ) first usage [ 0 ] = False return func ( * args , * * kwargs ) return wrapper
def connect ( self ) : if self . loop is None : # pragma: no cover self . loop = asyncio . get event loop ( ) t = asyncio . Task ( self . loop . create connection ( self . config [ 'protocol factory' ] , self . config [ 'host' ] , self . config [ 'port' ] , ssl = self . config [ 'ssl' ] ) , loop = self . loop ) t . add done callback ( self . connection made ) return t
def agi code check ( code = None , response = None , line = None ) : code = int ( code ) response = response or "" result = { 'status code' : code , 'result' : ( '' , '' ) , 'msg' : '' } if code == 100 : result [ 'msg' ] = line elif code == 200 : for key , value , data in re kv . findall ( response ) : result [ key ] = ( value , data ) # If user hangs up... we get 'hangup' in the data if data == 'hangup' : return { 'error' : 'AGI Result Hangup' , 'msg' : 'User hungup during execution' } elif key == 'result' and value == '-1' : return { 'error' : 'AGI App Error' , 'msg' : 'Error executing application, or hangup' } elif code == 510 : result [ 'error' ] = 'AGI Invalid Command' elif code == 520 : # AGI Usage error result [ 'error' ] = 'AGI Usage Error' result [ 'msg' ] = line else : # Unhandled code or undefined response result [ 'error' ] = 'AGI Unknown Error' result [ 'msg' ] = line return result
def get instances ( self ) : return [ "<%s prefix:%s (uid:%s)>" % ( self . class . name , i . prefix , self . uid ) for i in self . instances ]
def gc ( ) : def after delete ( database ) : click . echo ( "Deleted table %s" % database ) app = get app ( ) upgrade from old version ( app ) app . delete orphan snapshots ( after delete )
def snapshot ( name ) : app = get app ( ) upgrade from old version ( app ) name = name or app . default snapshot name if app . get snapshot ( name ) : click . echo ( "Snapshot with name %s already exists" % name ) sys . exit ( 1 ) else : def before copy ( table name ) : click . echo ( "Snapshotting database %s" % table name ) app . create snapshot ( name , before copy = before copy )
def list ( ) : snapshots = get app ( ) . get snapshots ( ) click . echo ( '\n' . join ( '%s: %s' % ( s . snapshot name , humanize . naturaltime ( datetime . utcnow ( ) - s . created at ) ) for s in snapshots ) )
def restore ( name ) : app = get app ( ) if not name : snapshot = app . get latest snapshot ( ) if not snapshot : click . echo ( "Couldn't find any snapshots for project %s" % load config ( ) [ 'project name' ] ) sys . exit ( 1 ) else : snapshot = app . get snapshot ( name ) if not snapshot : click . echo ( "Couldn't find snapshot with name %s.\n" "You can list snapshots with 'stellar list'" % name ) sys . exit ( 1 ) # Check if slaves are ready if not snapshot . slaves ready : if app . is copy process running ( snapshot ) : sys . stdout . write ( 'Waiting for background process(%s) to finish' % snapshot . worker pid ) sys . stdout . flush ( ) while not snapshot . slaves ready : sys . stdout . write ( '.' ) sys . stdout . flush ( ) sleep ( 1 ) app . db . session . refresh ( snapshot ) click . echo ( '' ) else : click . echo ( 'Background process missing, doing slow restore.' ) app . inline slave copy ( snapshot ) app . restore ( snapshot ) click . echo ( 'Restore complete.' )
def init ( ) : while True : url = click . prompt ( "Please enter the url for your database.\n\n" "For example:\n" "Postgre SQL: postgresql://localhost:5432/\n" "My SQL: mysql+pymysql://root@localhost/" ) if url . count ( '/' ) == 2 and not url . endswith ( '/' ) : url = url + '/' if ( url . count ( '/' ) == 3 and url . endswith ( '/' ) and url . startswith ( 'postgresql://' ) ) : connection url = url + 'template1' else : connection url = url engine = create engine ( connection url , echo = False ) try : conn = engine . connect ( ) except Operational Error as err : click . echo ( "Could not connect to database: %s" % url ) click . echo ( "Error message: %s" % err . message ) click . echo ( '' ) else : break if engine . dialect . name not in SUPPORTED DIALECTS : click . echo ( "Your engine dialect %s is not supported." % ( engine . dialect . name ) ) click . echo ( "Supported dialects: %s" % ( ', ' . join ( SUPPORTED DIALECTS ) ) ) if url . count ( '/' ) == 3 and url . endswith ( '/' ) : while True : click . echo ( "You have the following databases: %s" % ', ' . join ( [ db for db in list of databases ( conn ) if not db . startswith ( 'stellar ' ) ] ) ) db name = click . prompt ( "Please enter the name of the database (eg. projectdb)" ) if database exists ( conn , db name ) : break else : click . echo ( "Could not find database %s" % db name ) click . echo ( '' ) else : db name = url . rsplit ( '/' , 1 ) [ - 1 ] url = url . rsplit ( '/' , 1 ) [ 0 ] + '/' name = click . prompt ( 'Please enter your project name (used internally, eg. %s)' % db name , default = db name ) raw url = url if engine . dialect . name == 'postgresql' : raw url = raw url + 'template1' with open ( 'stellar.yaml' , 'w' ) as project file : project file . write ( . strip ( ) % { 'name' : name , 'raw url' : raw url , 'url' : url , 'db name' : db name } ) click . echo ( "Wrote stellar.yaml" ) click . echo ( '' ) if engine . dialect . name == 'mysql' : click . echo ( "Warning: My SQL support is still in beta." ) click . echo ( "Tip: You probably want to take a snapshot: stellar snapshot" )
def on epoch end ( self ) -> None : self . indexes = np . arange ( self . nrows ) if self . shuffle : np . random . shuffle ( self . indexes )
def process text constructor ( cleaner : Callable , tokenizer : Callable , append indicators : bool , start tok : str , end tok : str ) : def process text ( text ) : if append indicators : return [ [ start tok ] + tokenizer ( cleaner ( doc ) ) + [ end tok ] for doc in text ] return [ tokenizer ( cleaner ( doc ) ) for doc in text ] return process text
def process text ( self , text : List [ str ] ) -> List [ List [ str ] ] : process text = process text constructor ( cleaner = self . cleaner , tokenizer = self . tokenizer , append indicators = self . append indicators , start tok = self . start tok , end tok = self . end tok ) return process text ( text )
def generate doc length stats ( self ) : heuristic = self . heuristic pct histdf = ( pd . Data Frame ( [ ( a , b ) for a , b in self . document length histogram . items ( ) ] , columns = [ 'bin' , 'doc count' ] ) . sort values ( by = 'bin' ) ) histdf [ 'cumsum pct' ] = histdf . doc count . cumsum ( ) / histdf . doc count . sum ( ) self . document length stats = histdf self . doc length huerestic = histdf . query ( f'cumsum pct >= {heuristic}' ) . bin . head ( 1 ) . values [ 0 ] logging . warning ( ' ' . join ( [ "Setting maximum document length to" , f'{self.doc length huerestic} based upon' , f'heuristic of {heuristic} percentile.\n' , 'See full histogram by insepecting the' , "`document length stats` attribute." ] ) ) self . padding maxlen = self . doc length huerestic
def token count pandas ( self ) : freq df = pd . Data Frame . from dict ( self . indexer . word counts , orient = 'index' ) freq df . columns = [ 'count' ] return freq df . sort values ( 'count' , ascending = False )
def inv cls ( cls ) : if cls . fwdm cls is cls . invm cls : return cls if not getattr ( cls , ' inv cls ' , None ) : class Inv ( cls ) : fwdm cls = cls . invm cls invm cls = cls . fwdm cls inv cls = cls Inv . name = cls . name + 'Inv' cls . inv cls = Inv return cls . inv cls
def update with rollback ( self , on dup , * args , * * kw ) : writelog = [ ] appendlog = writelog . append dedup item = self . dedup item write item = self . write item for ( key , val ) in iteritems args kw ( * args , * * kw ) : try : dedup result = dedup item ( key , val , on dup ) except Duplication Error : undo write = self . undo write for dedup result , write result in reversed ( writelog ) : undo write ( dedup result , write result ) raise if dedup result is not NOOP : write result = write item ( key , val , dedup result ) appendlog ( ( dedup result , write result ) )
def copy ( self ) : # Could just ``return self. class (self)`` here instead, but the below is faster. It uses #  new  to create a copy instance while bypassing its  init , which would result # in copying this bidict's items into the copy instance one at a time. Instead, make whole # copies of each of the backing mappings, and make them the backing mappings of the copy, # avoiding copying items one at a time. copy = self . class . new ( self . class ) copy . fwdm = self . fwdm . copy ( ) # pylint: disable=protected-access copy . invm = self . invm . copy ( ) # pylint: disable=protected-access copy . init inv ( ) # pylint: disable=protected-access return copy
def copy ( self ) : # Fast copy implementation bypassing  init . See comments in :meth:`Bidict Base.copy`. copy = self . class . new ( self . class ) sntl = Sentinel ( ) fwdm = self . fwdm . copy ( ) invm = self . invm . copy ( ) cur = sntl nxt = sntl . nxt for ( key , val ) in iteritems ( self ) : nxt = Node ( cur , sntl ) cur . nxt = fwdm [ key ] = invm [ val ] = nxt cur = nxt sntl . prv = nxt copy . sntl = sntl # pylint: disable=protected-access copy . fwdm = fwdm # pylint: disable=protected-access copy . invm = invm # pylint: disable=protected-access copy . init inv ( ) # pylint: disable=protected-access return copy
def clear ( self ) : self . fwdm . clear ( ) self . invm . clear ( ) self . sntl . nxt = self . sntl . prv = self . sntl
def new contact ( cls , address book , supported private objects , version , localize dates ) : return cls ( address book , None , supported private objects , version , localize dates )
def from user input ( cls , address book , user input , supported private objects , version , localize dates ) : contact = cls ( address book , None , supported private objects , version , localize dates ) contact . process user input ( user input ) return contact
def add category ( self , categories ) : categories obj = self . vcard . add ( 'categories' ) categories obj . value = helpers . convert to vcard ( "category" , categories , Object Type . list with strings )
def avail archs ( self ) : return { ARM32 : ( KS ARCH ARM , KS MODE ARM ) , ARM64 : ( KS ARCH ARM64 , KS MODE LITTLE ENDIAN ) , ARM TB : ( KS ARCH ARM , KS MODE THUMB ) , HEXAGON : ( KS ARCH HEXAGON , KS MODE BIG ENDIAN ) , MIPS32 : ( KS ARCH MIPS , KS MODE MIPS32 ) , MIPS64 : ( KS ARCH MIPS , KS MODE MIPS64 ) , PPC32 : ( KS ARCH PPC , KS MODE PPC32 ) , PPC64 : ( KS ARCH PPC , KS MODE PPC64 ) , SPARC32 : ( KS ARCH SPARC , KS MODE SPARC32 ) , SPARC64 : ( KS ARCH SPARC , KS MODE SPARC64 ) , SYSTEMZ : ( KS ARCH SYSTEMZ , KS MODE BIG ENDIAN ) , X86 16 : ( KS ARCH X86 , KS MODE 16 ) , X86 32 : ( KS ARCH X86 , KS MODE 32 ) , X86 64 : ( KS ARCH X86 , KS MODE 64 ) , }
def avail archs ( self ) : return { ARM32 : ( CS ARCH ARM , CS MODE ARM ) , ARM64 : ( CS ARCH ARM64 , CS MODE LITTLE ENDIAN ) , ARM TB : ( CS ARCH ARM , CS MODE THUMB ) , MIPS32 : ( CS ARCH MIPS , CS MODE MIPS32 ) , MIPS64 : ( CS ARCH MIPS , CS MODE MIPS64 ) , SPARC32 : ( CS ARCH SPARC , CS MODE BIG ENDIAN ) , SPARC64 : ( CS ARCH SPARC , CS MODE V9 ) , SYSTEMZ : ( CS ARCH SYSZ , CS MODE BIG ENDIAN ) , X86 16 : ( CS ARCH X86 , CS MODE 16 ) , X86 32 : ( CS ARCH X86 , CS MODE 32 ) , X86 64 : ( CS ARCH X86 , CS MODE 64 ) , }
def safe input ( prompt ) : if sys . version info < ( 3 , 0 ) : if isinstance ( prompt , compat . text type ) : # Python 2.x: unicode →  bytes encoding = locale . getpreferredencoding ( ) or 'utf-8' prompt = prompt . encode ( encoding ) else : if not isinstance ( prompt , compat . text type ) : # Python 3.x: bytes →  unicode prompt = prompt . decode ( ) return input ( prompt )
def first ( self ) : lim = [ 0 , 1 ] if self . limit : lim [ 0 ] = self . limit [ 0 ] if not self . filters and not self . order by : for ent in self : return ent return None ids = self . limit ( * lim ) . search ( ) if ids : return self . model . get ( ids [ 0 ] ) return None
def redis prefix lua ( conn , dest , index , prefix , is first , pattern = None ) : tkey = '%s:%s' % ( index . partition ( ':' ) [ 0 ] , uuid . uuid4 ( ) ) start , end = start end ( prefix ) return redis prefix lua ( conn , [ dest , tkey , index ] , [ start , end , pattern or prefix , int ( pattern is not None ) , int ( bool ( is first ) ) ] )
def add ( self , obj ) : if self . null session : return self . init ( ) pk = obj . pk if not pk . endswith ( ':None' ) : self . known [ pk ] = obj self . wknown [ pk ] = obj
def get ( self , pk ) : self . init ( ) return self . known . get ( pk ) or self . wknown . get ( pk )
def register ( cls , type , reduce func ) : if sys . version info < ( 3 , ) : # Python 2 pickler dispatching is not explicitly customizable. # Let us use a closure to workaround this limitation. def dispatcher ( cls , obj ) : reduced = reduce func ( obj ) cls . save reduce ( obj = obj , * reduced ) cls . dispatch table [ type ] = dispatcher else : cls . dispatch table [ type ] = reduce func
def Queue ( self , maxsize = 0 , reducers = None ) : from . queues import Queue return Queue ( maxsize , reducers = reducers , ctx = self . get context ( ) )
def Simple Queue ( self , reducers = None ) : from . queues import Simple Queue return Simple Queue ( reducers = reducers , ctx = self . get context ( ) )
def sendback result ( result queue , work id , result = None , exception = None ) : try : result queue . put ( Result Item ( work id , result = result , exception = exception ) ) except Base Exception as e : exc = Exception With Traceback ( e ) result queue . put ( Result Item ( work id , exception = exc ) )
def ensure executor running ( self ) : with self . processes management lock : if len ( self . processes ) != self . max workers : self . adjust process count ( ) self . start queue management thread ( )
def start ( self , initializer = None , initargs = ( ) ) : assert self . state . value == State . INITIAL if ( initializer is not None and not hasattr ( initializer , ' call ' ) ) : raise Type Error ( 'initializer must be a callable' ) # pipe over which we will retrieve address of server reader , writer = mp . Pipe ( duplex = False ) # spawn process which runs a server self . process = Process ( target = type ( self ) . run server , args = ( self . registry , self . address , bytes ( self . authkey ) , self . serializer , writer , initializer , initargs ) , ) ident = ':' . join ( str ( i ) for i in self . process . identity ) self . process . name = type ( self ) . name + '-' + ident self . process . start ( ) # get address of server writer . close ( ) self . address = reader . recv ( ) reader . close ( ) # register a finalizer self . state . value = State . STARTED self . shutdown = mp . util . Finalize ( self , type ( self ) . finalize manager , args = ( self . process , self . address , self . authkey , self . state , self . Client ) , exitpriority = 0 )
def Dup Fd ( fd ) : popen obj = get spawning popen ( ) if popen obj is not None : return popen obj . Dup Fd ( popen obj . duplicate for child ( fd ) ) elif HAVE SEND HANDLE and sys . version info [ : 2 ] > ( 3 , 3 ) : from multiprocessing import resource sharer return resource sharer . Dup Fd ( fd ) else : raise Type Error ( 'Cannot pickle connection object. This object can only be ' 'passed when spawning a new process' )
def wait job completion ( self ) : # Issue a warning to the user about the bad effect of this usage. if len ( self . pending work items ) > 0 : warnings . warn ( "Trying to resize an executor with running jobs: " "waiting for jobs completion before resizing." , User Warning ) mp . util . debug ( "Executor {} waiting for jobs completion before" " resizing" . format ( self . executor id ) ) # Wait for the completion of the jobs while len ( self . pending work items ) > 0 : time . sleep ( 1e-3 )
def get preparation data ( name , init main module = True ) : check not importing main ( ) d = dict ( log to stderr = util . log to stderr , authkey = bytes ( process . current process ( ) . authkey ) , ) if util . logger is not None : d [ 'log level' ] = util . logger . get Effective Level ( ) if len ( util . logger . handlers ) > 0 : h = util . logger . handlers [ 0 ] d [ 'log fmt' ] = h . formatter . fmt sys path = [ p for p in sys . path ] try : i = sys path . index ( '' ) except Value Error : pass else : sys path [ i ] = process . ORIGINAL DIR d . update ( name = name , sys path = sys path , sys argv = sys . argv , orig dir = process . ORIGINAL DIR , dir = os . getcwd ( ) ) if sys . platform != "win32" : # Pass the semaphore tracker pid to avoid re-spawning it in every child from . import semaphore tracker semaphore tracker . ensure running ( ) d [ 'tracker pid' ] = semaphore tracker . semaphore tracker . pid # Figure out whether to initialise main in the subprocess as a module # or through direct execution (or to leave it alone entirely) if init main module : main module = sys . modules [ ' main ' ] try : main mod name = getattr ( main module . spec , "name" , None ) except Base Exception : main mod name = None if main mod name is not None : d [ 'init main from name' ] = main mod name elif sys . platform != 'win32' or ( not WINEXE and not WINSERVICE ) : main path = getattr ( main module , ' file ' , None ) if main path is not None : if ( not os . path . isabs ( main path ) and process . ORIGINAL DIR is not None ) : main path = os . path . join ( process . ORIGINAL DIR , main path ) d [ 'init main from path' ] = os . path . normpath ( main path ) # Compat for python2.7 d [ 'main path' ] = d [ 'init main from path' ] return d
def prepare ( data ) : if 'name' in data : process . current process ( ) . name = data [ 'name' ] if 'authkey' in data : process . current process ( ) . authkey = data [ 'authkey' ] if 'log to stderr' in data and data [ 'log to stderr' ] : util . log to stderr ( ) if 'log level' in data : util . get logger ( ) . set Level ( data [ 'log level' ] ) if 'log fmt' in data : import logging util . get logger ( ) . handlers [ 0 ] . set Formatter ( logging . Formatter ( data [ 'log fmt' ] ) ) if 'sys path' in data : sys . path = data [ 'sys path' ] if 'sys argv' in data : sys . argv = data [ 'sys argv' ] if 'dir' in data : os . chdir ( data [ 'dir' ] ) if 'orig dir' in data : process . ORIGINAL DIR = data [ 'orig dir' ] if 'tracker pid' in data : from . import semaphore tracker semaphore tracker . semaphore tracker . pid = data [ "tracker pid" ] if 'init main from name' in data : fixup main from name ( data [ 'init main from name' ] ) elif 'init main from path' in data : fixup main from path ( data [ 'init main from path' ] )
def close fds ( keep fds ) : # pragma: no cover # Make sure to keep stdout and stderr open for logging purpose keep fds = set ( keep fds ) . union ( [ 1 , 2 ] ) # We try to retrieve all the open fds try : open fds = set ( int ( fd ) for fd in os . listdir ( '/proc/self/fd' ) ) except File Not Found Error : import resource max nfds = resource . getrlimit ( resource . RLIMIT NOFILE ) [ 0 ] open fds = set ( fd for fd in range ( 3 , max nfds ) ) open fds . add ( 0 ) for i in open fds - keep fds : try : os . close ( i ) except OS Error : pass
def recursive terminate without psutil ( process ) : try : recursive terminate ( process . pid ) except OS Error as e : warnings . warn ( "Failed to kill subprocesses on this platform. Please" ) # In case we cannot introspect the children, we fall back to the # classic Process.terminate. process . terminate ( ) process . join ( )
def recursive terminate ( pid ) : if sys . platform == "win32" : # On windows, the taskkill function with option `/T` terminate a given # process pid and its children. try : subprocess . check output ( [ "taskkill" , "/F" , "/T" , "/PID" , str ( pid ) ] , stderr = None ) except subprocess . Called Process Error as e : # In windows, taskkill return 1 for permission denied and 128, 255 # for no process found. if e . returncode not in [ 1 , 128 , 255 ] : raise elif e . returncode == 1 : # Try to kill the process without its descendants if taskkill # was denied permission. If this fails too, with an error # different from process not found, let the top level function # raise a warning and retry to kill the process. try : os . kill ( pid , signal . SIGTERM ) except OS Error as e : if e . errno != errno . ESRCH : raise else : try : children pids = subprocess . check output ( [ "pgrep" , "-P" , str ( pid ) ] , stderr = None ) except subprocess . Called Process Error as e : # `ps` returns 1 when no child process has been found if e . returncode == 1 : children pids = b'' else : raise # Decode the result, split the cpid and remove the trailing line children pids = children pids . decode ( ) . split ( '\n' ) [ : - 1 ] for cpid in children pids : cpid = int ( cpid ) recursive terminate ( cpid ) try : os . kill ( pid , signal . SIGTERM ) except OS Error as e : # if OS Error is raised with [Errno 3] no such process, the process # is already terminated, else, raise the error and let the top # level function raise a warning and retry to kill the process. if e . errno != errno . ESRCH : raise
def format exitcodes ( exitcodes ) : str exitcodes = [ "{}({})" . format ( get exitcode name ( e ) , e ) for e in exitcodes if e is not None ] return "{" + ", " . join ( str exitcodes ) + "}"
def main ( fd , verbose = 0 ) : # protect the process from ^C and "killall python" etc signal . signal ( signal . SIGINT , signal . SIG IGN ) signal . signal ( signal . SIGTERM , signal . SIG IGN ) if HAVE SIGMASK : signal . pthread sigmask ( signal . SIG UNBLOCK , IGNORED SIGNALS ) for f in ( sys . stdin , sys . stdout ) : try : f . close ( ) except Exception : pass if verbose : # pragma: no cover sys . stderr . write ( "Main semaphore tracker is running\n" ) sys . stderr . flush ( ) cache = set ( ) try : # keep track of registered/unregistered semaphores with os . fdopen ( fd , 'rb' ) as f : for line in f : try : cmd , name = line . strip ( ) . split ( b':' ) if cmd == b'REGISTER' : name = name . decode ( 'ascii' ) cache . add ( name ) if verbose : # pragma: no cover sys . stderr . write ( "[Semaphore Tracker] register {}\n" . format ( name ) ) sys . stderr . flush ( ) elif cmd == b'UNREGISTER' : name = name . decode ( 'ascii' ) cache . remove ( name ) if verbose : # pragma: no cover sys . stderr . write ( "[Semaphore Tracker] unregister {}" ": cache({})\n" . format ( name , len ( cache ) ) ) sys . stderr . flush ( ) elif cmd == b'PROBE' : pass else : raise Runtime Error ( 'unrecognized command %r' % cmd ) except Base Exception : try : sys . excepthook ( * sys . exc info ( ) ) except Base Exception : pass finally : # all processes have terminated; cleanup any remaining semaphores if cache : try : warnings . warn ( 'semaphore tracker: There appear to be %d ' 'leaked semaphores to clean up at shutdown' % len ( cache ) ) except Exception : pass for name in cache : # For some reason the process which created and registered this # semaphore has failed to unregister it. Presumably it has died. # We therefore unlink it. try : try : sem unlink ( name ) if verbose : # pragma: no cover sys . stderr . write ( "[Semaphore Tracker] unlink {}\n" . format ( name ) ) sys . stderr . flush ( ) except Exception as e : warnings . warn ( 'semaphore tracker: %s: %r' % ( name , e ) ) finally : pass if verbose : # pragma: no cover sys . stderr . write ( "semaphore tracker shut down\n" ) sys . stderr . flush ( )
def event processor ( self , frame , event , arg ) : out = self . debugger . intf [ - 1 ] . output lineno = frame . f lineno filename = self . core . canonic filename ( frame ) filename = self . core . filename ( filename ) if not out : print ( "%s - %s:%d" % ( event , filename , lineno ) ) else : out . write ( "%s - %s:%d" % ( event , filename , lineno ) ) if arg is not None : out . writeline ( ', %s ' % repr ( arg ) ) else : out . writeline ( '' ) pass pass return self . event processor
def run hooks ( obj , hooks , * args ) : for hook in hooks : if hook ( obj , * args ) : return True pass return False
def forget ( self ) : self . stack = [ ] self . curindex = 0 self . curframe = None self . thread name = None self . frame thread name = None return
def process commands ( self ) : if self . core . execution status != 'No program' : self . setup ( ) self . location ( ) pass leave loop = run hooks ( self , self . preloop hooks ) self . continue running = False while not leave loop : try : run hooks ( self , self . precmd hooks ) # bdb had a True return to leave loop. # A more straight-forward way is to set # instance variable self.continue running. leave loop = self . process command ( ) if leave loop or self . continue running : break except EOF Error : # If we have stacked interfaces, pop to the next # one.  If this is the last one however, we'll # just stick with that.  FIXME: Possibly we should # check to see if we are interactive.  and not # leave if that's the case. Is this the right # thing?  investigate and fix. if len ( self . debugger . intf ) > 1 : del self . debugger . intf [ - 1 ] self . last command = '' else : if self . debugger . intf [ - 1 ] . output : self . debugger . intf [ - 1 ] . output . writeline ( 'Leaving' ) raise Mexcept . Debugger Quit pass break pass pass return run hooks ( self , self . postcmd hooks )
def read history file ( self ) : histfile = self . debugger . intf [ - 1 ] . histfile try : import readline readline . read history file ( histfile ) except IO Error : pass except Import Error : pass return
def write history file ( self ) : settings = self . debugger . settings histfile = self . debugger . intf [ - 1 ] . histfile if settings [ 'hist save' ] : try : import readline try : readline . write history file ( histfile ) except IO Error : pass except Import Error : pass pass return
def errmsg ( self , msg , prefix = "** " ) : #  self.verbose shows lines so we don't have to duplicate info #  here. Perhaps there should be a 'terse' mode to never show #  position info. if not self . verbose : location = ( "%s:%s: Error in source command file" % ( self . script name , self . input lineno ) ) msg = "%s%s:\n%s%s" % ( prefix , location , prefix , msg ) else : msg = "%s%s" % ( prefix , msg ) pass self . msg ( msg ) if self . abort on error : raise EOF Error return
def close ( self ) : self . state = 'closing' if self . input : self . input . close ( ) pass if self . output : self . output . close ( ) pass self . state = 'disconnnected' return
def disassemble ( msg , msg nocr , section , co , lasti = - 1 , start line = - 1 , end line = None , relative pos = False , highlight = 'light' , start offset = 0 , end offset = None ) : return disassemble bytes ( msg , msg nocr , co . co code , lasti , co . co firstlineno , start line , end line , relative pos , co . co varnames , co . co names , co . co consts , co . co cellvars , co . co freevars , dict ( findlinestarts ( co ) ) , highlight , start offset = start offset , end offset = end offset )
def count frames ( frame , count start = 0 ) : count = - count start while frame : count += 1 frame = frame . f back return count
def print stack trace ( proc obj , count = None , color = 'plain' , opts = { } ) : if count is None : n = len ( proc obj . stack ) else : n = min ( len ( proc obj . stack ) , count ) try : for i in range ( n ) : print stack entry ( proc obj , i , color = color , opts = opts ) except Keyboard Interrupt : pass return
def eval print obj ( arg , frame , format = None , short = False ) : try : if not frame : # ?? Should we have set up a dummy globals # to have persistence? val = eval ( arg , None , None ) else : val = eval ( arg , frame . f globals , frame . f locals ) pass except : return 'No symbol "' + arg + '" in current context.' return print obj ( arg , val , format , short )
def print obj ( arg , val , format = None , short = False ) : what = arg if format : what = format + ' ' + arg val = Mprint . printf ( val , format ) pass s = '%s = %s' % ( what , val ) if not short : s += '\n  type = %s' % type ( val ) # Try to list the members of a class. # Not sure if this is correct or the # best way to do. s = print dict ( s , val , "object variables" ) if hasattr ( val , " class " ) : s = print dict ( s , val . class , "class variables" ) pass pass return s
def lookup ( self , subcmd prefix ) : for subcmd name in list ( self . subcmds . keys ( ) ) : if subcmd name . startswith ( subcmd prefix ) and len ( subcmd prefix ) >= self . subcmds [ subcmd name ] . class . min abbrev : return self . subcmds [ subcmd name ] pass return None
def short help ( self , subcmd cb , subcmd name , label = False ) : entry = self . lookup ( subcmd name ) if entry : if label : prefix = entry . name else : prefix = '' pass if hasattr ( entry , 'short help' ) : if prefix : prefix += ' -- ' self . cmd obj . msg ( prefix + entry . short help ) pass pass else : self . undefined subcmd ( "help" , subcmd name ) pass return
def run ( self , subcmd name , arg ) : entry = self . lookup ( subcmd name ) if entry : entry [ 'callback' ] ( arg ) else : self . cmdproc . undefined cmd ( entry . class . name , subcmd name ) pass return
def help ( self , * args ) : print ( args ) subcmd prefix = args [ 0 ] if not subcmd prefix or len ( subcmd prefix ) == 0 : self . msg ( self . doc ) self . msg ( % ( self . name ) ) for subcmd name in self . list ( ) : self . subcmd helper ( subcmd name , self , True , True ) return entry = self . lookup ( subcmd prefix ) if entry and hasattr ( entry , 'help' ) : entry . help ( args ) else : self . cmd obj . errmsg ( "Unknown 'help %s' subcommand %s" % ( self . name , subcmd prefix ) )
def list categories ( self ) : self . section ( "Classes of commands:" ) cats = list ( categories . keys ( ) ) cats . sort ( ) for cat in cats : # Foo! iteritems() doesn't do sorting self . msg ( "  %-13s -- %s" % ( cat , categories [ cat ] ) ) pass final msg = for line in re . compile ( '\n' ) . split ( final msg . rstrip ( '\n' ) ) : self . rst msg ( line ) pass return
def show category ( self , category , args ) : n2cmd = self . proc . commands names = list ( n2cmd . keys ( ) ) if len ( args ) == 1 and args [ 0 ] == '*' : self . section ( "Commands in class %s:" % category ) cmds = [ cmd for cmd in names if category == n2cmd [ cmd ] . category ] cmds . sort ( ) self . msg nocr ( self . columnize commands ( cmds ) ) return self . msg ( "%s.\n" % categories [ category ] ) self . section ( "List of commands:" ) names . sort ( ) for name in names : # Foo! iteritems() doesn't do sorting if category != n2cmd [ name ] . category : continue self . msg ( "%-13s -- %s" % ( name , n2cmd [ name ] . short help , ) ) pass return
def run ( self , args ) : # info line identifier if not self . proc . curframe : self . errmsg ( "No line number information available." ) return if len ( args ) == 3 : # lineinfo returns (item, file, lineno) or (None,) answer = self . lineinfo ( args [ 2 ] ) if answer [ 0 ] : item , filename , lineno = answer if not os . path . isfile ( filename ) : filename = Mclifns . search file ( filename , self . core . search path , self . main dirname ) self . msg ( 'Line %s of "%s" <%s>' % ( lineno , filename , item ) ) return filename = self . core . canonic filename ( self . proc . curframe ) if not os . path . isfile ( filename ) : filename = Mclifns . search file ( filename , self . core . search path , self . main dirname ) pass filename = self . core . canonic filename ( self . proc . curframe ) msg1 = 'Line %d of \"%s\"' % ( inspect . getlineno ( self . proc . curframe ) , self . core . filename ( filename ) ) msg2 = ( 'at instruction %d' % self . proc . curframe . f lasti ) if self . proc . event : msg2 += ', %s event' % self . proc . event pass self . msg ( Mmisc . wrapped lines ( msg1 , msg2 , self . settings [ 'width' ] ) ) return False
def map thread names ( ) : name2id = { } for thread id in list ( threading . active . keys ( ) ) : thread = threading . active [ thread id ] name = thread . get Name ( ) if name not in list ( name2id . keys ( ) ) : name2id [ name ] = thread id pass pass return name2id
def open ( self , inp , opts = None ) : if isinstance ( inp , list ) : self . input = inp else : raise IO Error ( "Invalid input type (%s) for %s" % ( type ( inp ) , inp ) ) return
def get int ( errmsg , arg , default = 1 , cmdname = None ) : if arg : try : # eval() is used so we will allow arithmetic expressions, # variables etc. default = int ( eval ( arg ) ) except ( Syntax Error , Name Error , Value Error ) : if cmdname : errmsg ( "Command '%s' expects an integer; got: %s." % ( cmdname , str ( arg ) ) ) else : errmsg ( 'Expecting an integer, got: %s.' % str ( arg ) ) pass raise Value Error return default
def run show int ( obj , what = None ) : val = obj . debugger . settings [ obj . name ] if not what : what = obj . name return obj . msg ( "%s is %d." % ( what , val ) )
def run show val ( obj , name ) : val = obj . debugger . settings [ obj . name ] obj . msg ( "%s is %s." % ( obj . name , obj . cmd . proc . saferepr ( val ) , ) ) return False
def is def stmt ( line , frame ) : # Should really also check that operand of 'LOAD CONST' is a code object return ( line and re def . match ( line ) and op at frame ( frame ) == 'LOAD CONST' and stmt contains opcode ( frame . f code , frame . f lineno , 'MAKE FUNCTION' ) )
def is class def ( line , frame ) : return ( line and re class . match ( line ) and stmt contains opcode ( frame . f code , frame . f lineno , 'BUILD CLASS' ) )
def nothread quit ( self , arg ) : self . debugger . core . stop ( ) self . debugger . core . execution status = 'Quit command' raise Mexcept . Debugger Quit
def threaded quit ( self , arg ) : threading list = threading . enumerate ( ) mythread = threading . current Thread ( ) for t in threading list : if t != mythread : ctype async raise ( t , Mexcept . Debugger Quit ) pass pass raise Mexcept . Debugger Quit
def main ( dbg = None , sys argv = list ( sys . argv ) ) : global title # Save the original just for use in the restart that works via exec. orig sys argv = list ( sys argv ) opts , dbg opts , sys argv = process options ( title , version , sys argv ) dbg opts [ 'orig sys argv' ] = sys argv dbg opts [ 'interface' ] = Mbullwinkle . BW Interface ( ) dbg opts [ 'processor' ] = 'bullwinkle' if dbg is None : dbg = Mdebugger . Trepan ( dbg opts ) dbg . core . add ignore ( main ) pass postprocess options ( dbg , opts ) # process options has munged sys.argv to remove any options that # options that belong to this debugger. The original options to # invoke the debugger and script are in global sys argv if len ( sys argv ) == 0 : # No program given to debug. Set to go into a command loop # anyway mainpyfile = None else : mainpyfile = sys argv [ 0 ] # Get script filename. if not os . path . isfile ( mainpyfile ) : mainpyfile = Mclifns . whence file ( mainpyfile ) is readable = Mfile . readable ( mainpyfile ) if is readable is None : print ( "%s: Python script file '%s' does not exist" % ( title , mainpyfile , ) ) sys . exit ( 1 ) elif not is readable : print ( "%s: Can't read Python script file '%s'" % ( title , mainpyfile , ) ) sys . exit ( 1 ) return # If mainpyfile is an optimized Python script try to find and # use non-optimized alternative. mainpyfile noopt = Mfile . file pyc2py ( mainpyfile ) if mainpyfile != mainpyfile noopt and Mfile . readable ( mainpyfile noopt ) : print ( "%s: Compiled Python script given and we can't use that." % title ) print ( "%s: Substituting non-compiled name: %s" % ( title , mainpyfile noopt , ) ) mainpyfile = mainpyfile noopt pass # Replace trepan's dir with script's dir in front of # module search path. sys . path [ 0 ] = dbg . main dirname = os . path . dirname ( mainpyfile ) # XXX If a signal has been received we continue in the loop, otherwise # the loop exits for some reason. dbg . sig received = False # if not mainpyfile: #     print('For now, you need to specify a Python script name!') #     sys.exit(2) #     pass while True : # Run the debugged script over and over again until we get it # right. try : if dbg . program sys argv and mainpyfile : normal termination = dbg . run script ( mainpyfile ) if not normal termination : break else : dbg . core . execution status = 'No program' dbg . core . processor . process commands ( ) pass dbg . core . execution status = 'Terminated' dbg . intf [ - 1 ] . msg ( "The program finished - quit or restart" ) dbg . core . processor . process commands ( ) except Mexcept . Debugger Quit : break except Mexcept . Debugger Restart : dbg . core . execution status = 'Restart requested' if dbg . program sys argv : sys . argv = list ( dbg . program sys argv ) part1 = ( 'Restarting %s with arguments:' % dbg . core . filename ( mainpyfile ) ) args = ' ' . join ( dbg . program sys argv [ 1 : ] ) dbg . intf [ - 1 ] . msg ( Mmisc . wrapped lines ( part1 , args , dbg . settings [ 'width' ] ) ) else : break except System Exit : # In most cases System Exit does not warrant a post-mortem session. break pass # Restore old sys.argv sys . argv = orig sys argv return
def signature ( frame ) : if not frame : return None code = frame . f code return ( code . co name , code . co filename , code . co firstlineno )
def all ( self ) : found = False s = [ ] for display in self . list : if not found : s . append ( ) found = True pass s . append ( display . format ( ) ) return s
def delete index ( self , display number ) : old size = len ( self . list ) self . list = [ disp for disp in self . list if display number != disp . number ] return old size != len ( self . list )
def display ( self , frame ) : if not frame : return s = [ ] sig = signature ( frame ) for display in self . list : if display . signature == sig and display . enabled : s . append ( display . to s ( frame ) ) pass pass return s
def debug ( frame = None ) : # ??? if frame is None : frame = frame ( ) . f back dbg = Remote Celery Trepan ( ) dbg . say ( BANNER . format ( self = dbg ) ) # dbg.say(SESSION STARTED.format(self=dbg)) trepan . api . debug ( dbg opts = dbg . dbg opts )
def run ( self , args ) : if len ( args ) < 2 : # We were given cmd without a subcommand; cmd is something # like "show", "info" or "set". Generally this means list # all of the subcommands. self . section ( "List of %s commands (with minimum abbreviation in " "parenthesis):" % self . name ) for subcmd name in self . cmds . list ( ) : # Some commands have lots of output. # they are excluded here because 'in list' is false. subcmd = self . cmds . subcmds [ subcmd name ] self . summary help ( subcmd name , subcmd ) pass return False subcmd prefix = args [ 1 ] # We were given: cmd subcmd ... # Run that. subcmd = self . cmds . lookup ( subcmd prefix ) if subcmd : nargs = len ( args ) - 2 if nargs < subcmd . min args : self . errmsg ( ( "Subcommand '%s %s' needs at least %d argument(s); " + "got %d." ) % ( self . name , subcmd . name , subcmd . min args , nargs ) ) return False if subcmd . max args is not None and nargs > subcmd . max args : self . errmsg ( ( "Subcommand '%s %s' takes at most %d argument(s); " + "got %d." ) % ( self . name , subcmd . name , subcmd . max args , nargs ) ) return False return subcmd . run ( args [ 2 : ] ) else : return self . undefined subcmd ( self . name , subcmd prefix ) return
def undefined subcmd ( self , cmd , subcmd ) : self . proc . intf [ - 1 ] . errmsg ( ( 'Undefined "%s" subcommand: "%s". ' + 'Try "help %s *".' ) % ( cmd , subcmd , cmd ) ) return
def main ( dbg = None , sys argv = list ( sys . argv ) ) : global title # Save the original just for use in the restart that works via exec. orig sys argv = list ( sys argv ) opts , dbg opts , sys argv = Moptions . process options ( title , VERSION , sys argv ) if opts . server is not None : if opts . server == 'tcp' : connection opts = { 'IO' : 'TCP' , 'PORT' : opts . port } else : connection opts = { 'IO' : 'FIFO' } intf = Mserver . Server Interface ( connection opts = connection opts ) dbg opts [ 'interface' ] = intf if 'FIFO' == intf . server type : print ( 'Starting FIFO server for process %s.' % os . getpid ( ) ) elif 'TCP' == intf . server type : print ( 'Starting TCP server listening on port %s.' % intf . inout . PORT ) pass elif opts . client : Mclient . run ( opts , sys argv ) return dbg opts [ 'orig sys argv' ] = orig sys argv if dbg is None : dbg = Mdebugger . Trepan ( dbg opts ) dbg . core . add ignore ( main ) pass Moptions . postprocess options ( dbg , opts ) # process options has munged sys.argv to remove any options that # options that belong to this debugger. The original options to # invoke the debugger and script are in global sys argv if len ( sys argv ) == 0 : # No program given to debug. Set to go into a command loop # anyway mainpyfile = None else : mainpyfile = sys argv [ 0 ] # Get script filename. if not osp . isfile ( mainpyfile ) : mainpyfile = Mclifns . whence file ( mainpyfile ) is readable = Mfile . readable ( mainpyfile ) if is readable is None : print ( "%s: Python script file '%s' does not exist" % ( title , mainpyfile , ) ) sys . exit ( 1 ) elif not is readable : print ( "%s: Can't read Python script file '%s'" % ( title , mainpyfile , ) ) sys . exit ( 1 ) return if Mfile . is compiled py ( mainpyfile ) : try : from xdis import load module , PYTHON VERSION , IS PYPY ( python version , timestamp , magic int , co , is pypy , source size ) = load module ( mainpyfile , code objects = None , fast load = True ) assert is pypy == IS PYPY assert python version == PYTHON VERSION , "bytecode is for version %s but we are version %s" % ( python version , PYTHON VERSION ) # We should we check version magic int py file = co . co filename if osp . isabs ( py file ) : try file = py file else : mainpydir = osp . dirname ( mainpyfile ) tag = sys . implementation . cache tag dirnames = [ osp . join ( mainpydir , tag ) , mainpydir ] + os . environ [ 'PATH' ] . split ( osp . pathsep ) + [ '.' ] try file = Mclifns . whence file ( py file , dirnames ) if osp . isfile ( try file ) : mainpyfile = try file pass else : # Move onto the except branch raise IO Error ( "Python file name embedded in code %s not found" % try file ) except IO Error : try : from uncompyle6 import decompile file except Import Error : print ( "%s: Compiled python file '%s', but uncompyle6 not found" % ( title , mainpyfile ) , file = sys . stderr ) sys . exit ( 1 ) return short name = osp . basename ( mainpyfile ) . strip ( '.pyc' ) fd = tempfile . Named Temporary File ( suffix = '.py' , prefix = short name + " " , delete = False ) old write = fd . file . write def write wrapper ( * args , * * kwargs ) : if isinstance ( args [ 0 ] , str ) : new args = list ( args ) new args [ 0 ] = args [ 0 ] . encode ( 'utf-8' ) old write ( * new args , * * kwargs ) else : old write ( * args , * * kwargs ) fd . file . write = write wrapper # from io import String IO # linemap io = String IO() try : decompile file ( mainpyfile , fd . file , mapstream = fd ) except : print ( "%s: error decompiling '%s'" % ( title , mainpyfile ) , file = sys . stderr ) sys . exit ( 1 ) return # # Get the line associations between the original and # # decompiled program # mapline = linemap io.getvalue() # fd.write(mapline + "\n\n") # linemap = eval(mapline[3:]) mainpyfile = fd . name fd . close ( ) # Since we are actually running the recreated source, # there is little no need to remap line numbers. # The mapping is given at the end of the file. # However we should consider adding this information # and original file name. print ( "%s: couldn't find Python source so we recreated it at '%s'" % ( title , mainpyfile ) , file = sys . stderr ) pass # If mainpyfile is an optimized Python script try to find and # use non-optimized alternative. mainpyfile noopt = pyficache . pyc2py ( mainpyfile ) if mainpyfile != mainpyfile noopt and Mfile . readable ( mainpyfile noopt ) : print ( "%s: Compiled Python script given and we can't use that." % title ) print ( "%s: Substituting non-compiled name: %s" % ( title , mainpyfile noopt , ) ) mainpyfile = mainpyfile noopt pass # Replace trepan's dir with script's dir in front of # module search path. sys . path [ 0 ] = dbg . main dirname = osp . dirname ( mainpyfile ) # XXX If a signal has been received we continue in the loop, otherwise # the loop exits for some reason. dbg . sig received = False # if not mainpyfile: #     print('For now, you need to specify a Python script name!') #     sys.exit(2) #     pass while True : # Run the debugged script over and over again until we get it # right. try : if dbg . program sys argv and mainpyfile : normal termination = dbg . run script ( mainpyfile ) if not normal termination : break else : dbg . core . execution status = 'No program' dbg . core . processor . process commands ( ) pass dbg . core . execution status = 'Terminated' dbg . intf [ - 1 ] . msg ( "The program finished - quit or restart" ) dbg . core . processor . process commands ( ) except Mexcept . Debugger Quit : break except Mexcept . Debugger Restart : dbg . core . execution status = 'Restart requested' if dbg . program sys argv : sys . argv = list ( dbg . program sys argv ) part1 = ( 'Restarting %s with arguments:' % dbg . core . filename ( mainpyfile ) ) args = ' ' . join ( dbg . program sys argv [ 1 : ] ) dbg . intf [ - 1 ] . msg ( Mmisc . wrapped lines ( part1 , args , dbg . settings [ 'width' ] ) ) else : break except System Exit : # In most cases System Exit does not warrant a post-mortem session. break pass # Restore old sys.argv sys . argv = orig sys argv return
def info signal ( self , args ) : if len ( args ) == 0 : return None signame = args [ 0 ] if signame in [ 'handle' , 'signal' ] : # This has come from dbgr's info command if len ( args ) == 1 : # Show all signal handlers self . dbgr . core . processor . section ( self . header ) for signame in self . siglist : self . print info signal entry ( signame ) return True else : signame = args [ 1 ] pass pass signame = self . is name or number ( signame ) self . dbgr . core . processor . section ( self . header ) self . print info signal entry ( signame ) return True
def handle print ( self , signame , set print ) : if set print : self . sigs [ signame ] . print method = self . dbgr . intf [ - 1 ] . msg else : self . sigs [ signame ] . print method = None pass return set print
def handle ( self , signum , frame ) : if self . print method : self . print method ( '\n Program received signal %s.' % self . signame ) if self . print stack : import traceback strings = traceback . format stack ( frame ) for s in strings : if s [ - 1 ] == '\n' : s = s [ 0 : - 1 ] self . print method ( s ) pass pass if self . b stop : core = self . dbgr . core old trace hook suspend = core . trace hook suspend core . trace hook suspend = True core . stop reason = ( 'intercepting signal %s (%d)' % ( self . signame , signum ) ) core . processor . event processor ( frame , 'signal' , signum ) core . trace hook suspend = old trace hook suspend pass if self . pass along : # pass the signal to the program if self . old handler : self . old handler ( signum , frame ) pass pass return
def file2module ( filename ) : basename = osp . basename ( filename ) if '.' in basename : pos = basename . rfind ( '.' ) return basename [ : pos ] else : return basename return None
def print obj ( arg , frame , format = None , short = False ) : try : if not frame : # ?? Should we have set up a dummy globals # to have persistence? obj = eval ( arg , None , None ) else : obj = eval ( arg , frame . f globals , frame . f locals ) pass except : return 'No symbol "' + arg + '" in current context.' # format and print what = arg if format : what = format + ' ' + arg obj = printf ( obj , format ) s = '%s = %s' % ( what , obj ) if not short : s += '\ntype = %s' % type ( obj ) if callable ( obj ) : argspec = print argspec ( obj , arg ) if argspec : s += ':\n\t' if inspect . isclass ( obj ) : s += 'Class constructor information:\n\t' obj = obj . init elif isinstance ( obj , types . Instance Type ) : obj = obj . call pass s += argspec pass # Try to list the members of a class. # Not sure if this is correct or the # best way to do. s = print dict ( s , obj , "object variables" ) if hasattr ( obj , " class " ) : s = print dict ( s , obj . class , "class variables" ) pass return s
def pyfiles ( callername , level = 2 ) : d = os . path . dirname ( callername ) # Get the name of our directory. # A glob pattern that will get all *.py files but not  init .py glob ( os . path . join ( d , '[a-z A-Z]*.py' ) ) py files = glob ( os . path . join ( d , '[a-z A-Z]*.py' ) ) return [ os . path . basename ( filename [ 0 : - 3 ] ) for filename in py files ]
def populate cmd lists ( self ) : self . commands = { } for cmd instance in self . cmd instances : cmd name = cmd instance . name self . commands [ cmd name ] = cmd instance pass return
def run ( self , args ) : mainfile = self . core . filename ( None ) if self . core . is running ( ) : if mainfile : part1 = "Python program '%s' is stopped" % mainfile else : part1 = 'Program is stopped' pass if self . proc . event : msg = 'via a %s event.' % self . proc . event else : msg = '.' self . msg ( Mmisc . wrapped lines ( part1 , msg , self . settings [ 'width' ] ) ) if self . proc . curframe : self . msg ( "PC offset is %d." % self . proc . curframe . f lasti ) if self . proc . event == 'return' : val = self . proc . event arg part1 = 'Return value is' self . msg ( Mmisc . wrapped lines ( part1 , self . proc . saferepr ( val ) , self . settings [ 'width' ] ) ) pass elif self . proc . event == 'exception' : exc type , exc value , exc tb = self . proc . event arg self . msg ( 'Exception type: %s' % self . proc . saferepr ( exc type ) ) if exc value : self . msg ( 'Exception value: %s' % self . proc . saferepr ( exc value ) ) pass pass self . msg ( 'It stopped %s.' % self . core . stop reason ) if self . proc . event in [ 'signal' , 'exception' , 'c exception' ] : self . msg ( 'Note: we are stopped *after* running the ' 'line shown.' ) pass else : if mainfile : part1 = "Python program '%s'" % mainfile msg = "is not currently running. " self . msg ( Mmisc . wrapped lines ( part1 , msg , self . settings [ 'width' ] ) ) else : self . msg ( 'No Python program is currently running.' ) pass self . msg ( self . core . execution status ) pass return False
def columnize commands ( self , commands ) : commands . sort ( ) width = self . debugger . settings [ 'width' ] return columnize . columnize ( commands , displaywidth = width , lineprefix = '    ' )
def close ( self ) : self . state = 'closing' if self . inout : self . inout . close ( ) pass self . state = 'closing connection' if self . conn : self . conn . close ( ) self . state = 'disconnected' return
def complete identifier ( cmd , prefix ) : if not cmd . proc . curframe : return [ None ] # Collect globals and locals.  It is usually not really sensible to also # complete builtins, and they clutter the namespace quite heavily, so we # leave them out. ns = cmd . proc . curframe . f globals . copy ( ) ns . update ( cmd . proc . curframe . f locals ) if '.' in prefix : # Walk an attribute chain up to the last part, similar to what # rlcompleter does.  This will bail if any of the parts are not # simple attribute access, which is what we want. dotted = prefix . split ( '.' ) try : obj = ns [ dotted [ 0 ] ] for part in dotted [ 1 : - 1 ] : obj = getattr ( obj , part ) except ( Key Error , Attribute Error ) : return [ ] pre prefix = '.' . join ( dotted [ : - 1 ] ) + '.' return [ pre prefix + n for n in dir ( obj ) if n . startswith ( dotted [ - 1 ] ) ] else : # Complete a simple name. return Mcomplete . complete token ( ns . keys ( ) , prefix )
def nothread quit ( self , arg ) : self . debugger . core . stop ( ) self . debugger . core . execution status = 'Quit command' self . proc . response [ 'event' ] = 'terminated' self . proc . response [ 'name' ] = 'status' self . proc . intf [ - 1 ] . msg ( self . proc . response ) raise Mexcept . Debugger Quit
def is started ( self ) : return ( tracer . is started ( ) and not self . trace hook suspend and tracer . find hook ( self . trace dispatch ) )
def set next ( self , frame , step ignore = 0 , step events = None ) : self . step events = None # Consider all events self . stop level = Mstack . count frames ( frame ) self . last level = self . stop level self . last frame = frame self . stop on finish = False self . step ignore = step ignore return
def stack trace ( self , f ) : while f : if ( not self . core . ignore filter . is included ( f ) or self . settings [ 'dbg trepan' ] ) : s = Mstack . format stack entry ( self , ( f , f . f lineno ) ) self . msg ( " " * 4 + s ) pass f = f . f back pass return
def checkfuncname ( b , frame ) : if not b . funcname : # Breakpoint was set via line number. if b . line != frame . f lineno : # Breakpoint was set at a line with a def statement and the function # defined is called: don't break. return False return True # Breakpoint set via function name. if frame . f code . co name != b . funcname : # It's not a function call, but rather execution of def statement. return False # We are in the right frame. if not b . func first executable line : # The function is entered for the 1st time. b . func first executable line = frame . f lineno if b . func first executable line != frame . f lineno : # But we are not at the first line number: don't break. return False return True
def delete breakpoint by number ( self , bpnum ) : success , msg , bp = self . get breakpoint ( bpnum ) if not success : return False , msg self . delete breakpoint ( bp ) return ( True , '' )
def en disable all breakpoints ( self , do enable = True ) : bp list = [ bp for bp in self . bpbynumber if bp ] bp nums = [ ] if do enable : endis = 'en' else : endis = 'dis' pass if not bp list : return "No breakpoints to %sable" % endis for bp in bp list : bp . enabled = do enable bp nums . append ( str ( bp . number ) ) pass return ( "Breakpoints %sabled: %s" % ( endis , ", " . join ( bp nums ) ) )
def en disable breakpoint by number ( self , bpnum , do enable = True ) : success , msg , bp = self . get breakpoint ( bpnum ) if not success : return success , msg if do enable : endis = 'en' else : endis = 'dis' pass if bp . enabled == do enable : return ( False , ( 'Breakpoint (%r) previously %sabled' % ( str ( bpnum ) , endis , ) ) ) bp . enabled = do enable return ( True , '' )
def open ( self , inp , opts = None ) : if isinstance ( inp , io . Text IO Wrapper ) : self . input = inp elif isinstance ( inp , 'string' . class ) : # FIXME self . name = inp self . input = open ( inp , 'r' ) else : raise IO Error ( "Invalid input type (%s) for %s" % ( inp . class . name , inp ) ) return
def restore original login ( request ) : original session = request . session . get ( la settings . USER SESSION FLAG ) logout ( request ) if not original session : return try : original user pk = signer . unsign ( original session , max age = timedelta ( days = la settings . USER SESSION DAYS TIMESTAMP ) . total seconds ( ) ) user = get user model ( ) . objects . get ( pk = original user pk ) messages . info ( request , la settings . MESSAGE LOGIN REVERT . format ( username = user . dict [ username field ] ) , extra tags = la settings . MESSAGE EXTRA TAGS , ) login as ( user , request , store original user = False ) if la settings . USER SESSION FLAG in request . session : del request . session [ la settings . USER SESSION FLAG ] except Signature Expired : pass
def load module ( path ) : i = path . rfind ( "." ) module , attr = path [ : i ] , path [ i + 1 : ] try : mod = import module ( module ) except Import Error : raise Improperly Configured ( "Error importing CAN LOGIN AS function: {}" . format ( module ) ) except Value Error : raise Improperly Configured ( "Error importing CAN LOGIN AS" " function. Is CAN LOGIN AS a" " string?" ) try : can login as = getattr ( mod , attr ) except Attribute Error : raise Improperly Configured ( "Module {0} does not define a {1} " "function." . format ( module , attr ) ) return can login as
def main ( argv ) : parser = argparse . Argument Parser ( description = DESCRIPTION , formatter class = argparse . Raw Description Help Formatter ) parser . add argument ( '-b' , '--base-url' , default = URL BASE , help = 'API root url, default: %s' % URL BASE , ) parser . add argument ( '-e' , '--expanded' , help = "Include Luminoso's analysis of each document, such as terms and" ' document vectors' , action = 'store true' , ) parser . add argument ( '-t' , '--token' , help = 'API authentication token' ) parser . add argument ( '-s' , '--save-token' , action = 'store true' , help = 'save --token for --base-url to ~/.luminoso/tokens.json' , ) parser . add argument ( 'project id' , help = 'The ID of the project in the Daylight API' ) parser . add argument ( 'output file' , nargs = '?' , default = None , help = 'The JSON lines (.jsons) file to write to' ) args = parser . parse args ( argv ) if args . save token : if not args . token : raise Value Error ( "error: no token provided" ) Luminoso Client . save token ( args . token , domain = urlparse ( args . base url ) . netloc ) client = Luminoso Client . connect ( url = args . base url , token = args . token ) proj client = client . client for path ( 'projects/{}' . format ( args . project id ) ) download docs ( proj client , args . output file , args . expanded )
def stream json lines ( file ) : if isinstance ( file , string type ) : file = open ( file , 'rb' ) for line in file : line = line . strip ( ) if line : if isinstance ( line , bytes ) : line = line . decode ( 'utf-8' ) yield json . loads ( line )
def get default account ( self ) : newclient = self . class ( self . session , self . root url ) account info = newclient . get ( '/accounts/' ) if account info [ 'default account' ] is not None : return account info [ 'default account' ] valid accounts = [ a [ 'account id' ] for a in account info [ 'accounts' ] if a [ 'account id' ] != 'public' ] if len ( valid accounts ) == 0 : raise Value Error ( "Can't determine your default URL. " "Please request a specific URL or ask " "Luminoso for support." ) return valid accounts [ 0 ]
def documentation ( self ) : newclient = self . class ( self . session , self . root url ) return newclient . get raw ( '/' )
def print csv ( result ) : if type ( result ) is not list : raise Type Error ( "output not able to be displayed as CSV." ) first line = result [ 0 ] w = csv . Dict Writer ( sys . stdout , fieldnames = sorted ( first line . keys ( ) ) ) w . writeheader ( ) for line in result : w . writerow ( line )
def read params ( input file , json body , p params ) : params = { } try : if input file : params . update ( json . load ( input file ) ) if json body is not None : params . update ( json . loads ( json body ) ) except Value Error as e : raise Value Error ( "input is not valid JSON: %s" % e ) try : params . update ( { p . split ( '=' , 1 ) [ 0 ] : p . split ( '=' , 1 ) [ 1 ] for p in p params } ) except Index Error : raise Value Error ( "--param arguments must have key=value format" ) return params
def batches ( iterable , size ) : sourceiter = iter ( iterable ) while True : try : batchiter = islice ( sourceiter , size ) yield chain ( [ next ( batchiter ) ] , batchiter ) except Stop Iteration : return
def simplify doc ( doc ) : # Mutate a copy of the document to fill in missing fields doc = dict ( doc ) if 'text' not in doc : raise Value Error ( "The document {!r} has no text field" . format ( doc ) ) return { 'text' : doc [ 'text' ] , 'metadata' : doc . get ( 'metadata' , [ ] ) , 'title' : doc . get ( 'title' , '' ) }
def create project with docs ( client , docs , language , name , account = None , progress = False ) : description = 'Uploaded using lumi-upload at {}' . format ( time . asctime ( ) ) if account is not None : proj record = client . post ( 'projects' , name = name , language = language , description = description , account id = account , ) else : proj record = client . post ( 'projects' , name = name , language = language , description = description ) proj id = proj record [ 'project id' ] proj client = client . client for path ( 'projects/' + proj id ) try : if progress : progress bar = tqdm ( desc = 'Uploading documents' ) else : progress bar = None for batch in batches ( docs , BATCH SIZE ) : docs to upload = [ simplify doc ( doc ) for doc in batch ] proj client . post ( 'upload' , docs = docs to upload ) if progress : progress bar . update ( BATCH SIZE ) finally : if progress : progress bar . close ( ) print ( 'The server is building project {!r}.' . format ( proj id ) ) proj client . post ( 'build' ) while True : time . sleep ( 10 ) proj status = proj client . get ( ) build info = proj status [ 'last build info' ] if 'success' in build info : if not build info [ 'success' ] : raise Luminoso Server Error ( build info [ 'reason' ] ) return proj status
def main ( argv ) : parser = argparse . Argument Parser ( description = DESCRIPTION , formatter class = argparse . Raw Description Help Formatter , ) parser . add argument ( '-b' , '--base-url' , default = URL BASE , help = 'API root url, default: %s' % URL BASE , ) parser . add argument ( '-a' , '--account-id' , default = None , help = 'Account ID that should own the project, if not the default' , ) parser . add argument ( '-l' , '--language' , default = 'en' , help = 'The language code for the language the text is in. Default: en' , ) parser . add argument ( '-t' , '--token' , help = "API authentication token" ) parser . add argument ( '-s' , '--save-token' , action = 'store true' , help = 'save --token for --base-url to ~/.luminoso/tokens.json' , ) parser . add argument ( 'input filename' , help = 'The JSON-lines (.jsons) file of documents to upload' , ) parser . add argument ( 'project name' , nargs = '?' , default = None , help = 'What the project should be called' , ) args = parser . parse args ( argv ) if args . save token : if not args . token : raise Value Error ( "error: no token provided" ) Luminoso Client . save token ( args . token , domain = urlparse ( args . base url ) . netloc ) client = Luminoso Client . connect ( url = args . base url , token = args . token ) name = args . project name if name is None : name = input ( 'Enter a name for the project: ' ) if not name : print ( 'Aborting because no name was provided.' ) return result = upload docs ( client , args . input filename , args . language , name , account = args . account id , progress = True , ) print ( 'Project {!r} created with {} documents' . format ( result [ 'project id' ] , result [ 'document count' ] ) )
def batches ( iterable , size ) : sourceiter = iter ( iterable ) while True : batchiter = islice ( sourceiter , size ) yield chain ( [ next ( batchiter ) ] , batchiter )
def post login page ( self ) : data = { 'ID Token1' : self . username , 'ID Token2' : self . password , 'Sun Query Params String' : base64 . b64encode ( b'realm=particuliers' ) , 'encoded' : 'true' , 'gx charset' : 'UTF-8' } try : self . session . post ( LOGIN URL , data = data , allow redirects = False , timeout = self . timeout ) except OS Error : raise Py Linky Error ( "Can not submit login form" ) if 'i Planet Directory Pro' not in self . session . cookies : raise Py Linky Error ( "Login error: Please check your username/password." ) return True
def fetch data ( self ) : for t in [ HOURLY , DAILY , MONTHLY , YEARLY ] : self . data [ t ] = self . get data per period ( t )
def prepare ( self ) : if self . class . view : return #: Load the View class from the dotted view name with enaml . imports ( ) : View = pydoc . locate ( self . page . view ) assert View , "Failed to import View: {}" . format ( self . page . view ) #: Set initial view properties self . class . view = View ( site = self . site , page = self . page , request = self . request , )
def initialize ( self ) : if self . class . view : self . view . handler = self self . view . request = self . request return #: Load the View class from the dotted view name with enaml . imports ( ) : from views . index import View #: Set initial view properties self . class . view = View ( company = current company , request = self . request , handler = self , )
def get ( self , * args , * * kwargs ) : #: Render view for get request, view is cached for websocket if self . is websocket ( ) : return super ( Demo Handler , self ) . get ( * args , * * kwargs ) else : #return tornado.web.Request Handler.get(self, *args, **kwargs) self . write ( self . view . render ( ) )
def on message ( self , message ) : #: Decode message change = tornado . escape . json decode ( message ) #print change #: Get the owner ID ref = change . get ( 'ref' ) if not ref : return #: Get the server side representation of the node #: If found will return the View declaration node node = self . view . xpath ( '//*[@ref="{}"]' . format ( ref ) , first = True ) if node is None : return #: Handle the event if change . get ( 'type' ) and change . get ( 'name' ) : if change [ 'type' ] == 'event' : #: Trigger the event trigger = getattr ( node , change [ 'name' ] ) trigger ( ) if change [ 'type' ] == 'update' : #: Trigger the update setattr ( node , change [ 'name' ] , change [ 'value' ] )
def update menus ( self , change ) : menus = { } #: Get all links links = [ p . link for p in self . pages if p . link ] + self . links #: Put all links in the correct menu for link in links : for menu in link . menus : if menu not in menus : menus [ menu ] = [ ] menus [ menu ] . append ( link ) #: Update the menus for name , menu in menus . items ( ) : k = '{} menu' . format ( name ) if hasattr ( self , k ) : setattr ( self , k , menu )
def default handlers ( self ) : static path = os . path . abspath ( os . path . join ( os . path . dirname ( file ) , "static" ) ) urls = [ ( r"/static/(.*)" , cyclone . web . Static File Handler , { "path" : static path } ) , ] for p in self . pages : handler = p . handler handler . site = self handler . page = p urls . append ( ( p . link . url , handler ) ) return urls
def set attribute ( self , name , value ) : if value is True : self . widget . set ( name , name ) elif value is False : del self . widget . attrib [ name ] else : self . widget . set ( name , str ( value ) )
def xpath ( self , query , * * kwargs ) : nodes = self . proxy . find ( query , * * kwargs ) return [ n . declaration for n in nodes ]
def init widget ( self ) : d = self . declaration if d . source : self . set source ( d . source ) else : super ( Raw Component , self ) . init widget ( )
def observe mode ( self , change ) : block = self . block if block and self . is initialized and change [ 'type' ] == 'update' : if change [ 'oldvalue' ] == 'replace' : raise Not Implemented Error for c in self . children : block . children . remove ( c ) c . set parent ( None ) self . refresh items ( )
def read ( * pathcomponents ) : with open ( join ( abspath ( dirname ( file ) ) , * pathcomponents ) ) as thefile : return thefile . read ( )
def print obj ( obj , verbose , metadata , mongo version ) : if verbose : sys . stdout . write ( json encoder . encode ( obj ) + '\n' ) sys . stdout . flush ( ) else : try : ts time = obj [ 'ts' ] operation = obj [ 'op' ] doc = None if operation == 'query' : if mongo version < "3.2" : doc = obj [ 'ns' ] . split ( "." ) [ - 1 ] query = json encoder . encode ( obj [ 'query' ] ) if 'query' in obj else "{}" else : if "query" in obj : cmd = obj [ 'query' ] # Mongo 3.2 - 3.4 else : cmd = obj [ 'command' ] # Mongo 3.6+ doc = cmd [ 'find' ] query = json encoder . encode ( cmd [ 'filter' ] ) if 'filter' in cmd else "{}" if 'sort' in cmd : query += ', sort: ' + json encoder . encode ( cmd [ 'sort' ] ) query += '. %s returned.' % obj [ 'nreturned' ] elif operation == 'update' : doc = obj [ 'ns' ] . split ( "." ) [ - 1 ] if mongo version < "3.6" : query = json encoder . encode ( obj [ 'query' ] ) if 'query' in obj else "{}" query += ', ' + json encoder . encode ( obj [ 'updateobj' ] ) else : query = json encoder . encode ( obj [ 'command' ] [ 'q' ] ) if 'command' in obj and 'q' in obj [ 'command' ] else "{}" query += ', ' + json encoder . encode ( obj [ 'command' ] [ 'u' ] ) if 'n Modified' in obj : query += '. %s updated.' % obj [ 'n Modified' ] elif 'n Matched' in obj : query += '. %s updated.' % obj [ 'n Matched' ] elif operation == 'insert' : if mongo version < "3.2" : doc = obj [ 'ns' ] . split ( "." ) [ - 1 ] query = json encoder . encode ( obj [ 'query' ] ) if 'query' in obj else "{}" else : if 'query' in obj : doc = obj [ 'query' ] [ 'insert' ] if 'documents' in obj [ 'query' ] : if isinstance ( obj [ 'query' ] [ 'documents' ] , collections . Iterable ) and len ( obj [ 'query' ] [ 'documents' ] ) > 1 : query = json encoder . encode ( obj [ 'query' ] [ 'documents' ] ) + ". " else : query = json encoder . encode ( obj [ 'query' ] [ 'documents' ] [ 0 ] ) + ". " else : query = "" else : # Mongo 3.6+ profiler looks like doens't record insert details (document object), and # some tools like Robo 3T (formerly Robomongo) allows to duplicate collections # but the profiler doesn't record the element inserted doc = obj [ 'ns' ] . split ( "." ) [ - 1 ] query = "" query += '%s inserted.' % obj [ 'ninserted' ] elif operation == 'remove' : doc = obj [ 'ns' ] . split ( "." ) [ - 1 ] if mongo version < "3.6" : query = json encoder . encode ( obj [ 'query' ] ) if 'query' in obj else "{}" else : query = json encoder . encode ( obj [ 'command' ] [ 'q' ] ) if 'command' in obj and 'q' in obj [ 'command' ] else "{}" query += '. %s deleted.' % obj [ 'ndeleted' ] elif operation == "command" : if 'count' in obj [ "command" ] : operation = "count" query = json encoder . encode ( obj [ 'command' ] [ 'query' ] ) elif 'aggregate' in obj [ "command" ] : operation = "aggregate" query = json encoder . encode ( obj [ 'command' ] [ 'pipeline' ] ) elif 'distinct' in obj [ "command" ] : operation = "distinct" query = json encoder . encode ( obj [ 'command' ] [ 'query' ] ) query = '"%s", %s' % ( obj [ 'command' ] [ 'key' ] , query ) elif 'drop' in obj [ "command" ] : operation = "drop" query = "" elif 'findandmodify' in obj [ "command" ] : operation = "findandmodify" query = "query: " + json encoder . encode ( obj [ 'command' ] [ 'query' ] ) if 'sort' in obj [ "command" ] : query += ", sort: " + json encoder . encode ( obj [ 'command' ] [ 'sort' ] ) if 'update' in obj [ "command" ] : query += ", update: " + json encoder . encode ( obj [ 'command' ] [ 'update' ] ) if 'remove' in obj [ "command" ] : query += ", remove: " + str ( obj [ 'command' ] [ 'remove' ] ) . lower ( ) if 'fields' in obj [ "command" ] : query += ", fields: " + json encoder . encode ( obj [ 'command' ] [ 'fields' ] ) if 'upsert' in obj [ "command" ] : query += ", upsert: " + str ( obj [ 'command' ] [ 'upsert' ] ) . lower ( ) if 'new' in obj [ "command" ] : query += ", new: " + str ( obj [ 'command' ] [ 'new' ] ) . lower ( ) elif 'group' in obj [ "command" ] : operation = "group" doc = obj [ "command" ] [ 'group' ] [ "ns" ] if 'key' in obj [ 'command' ] [ 'group' ] : key = "key: " + json encoder . encode ( obj [ 'command' ] [ 'group' ] [ 'key' ] ) else : key = None if 'initial' in obj [ 'command' ] [ 'group' ] : initial = "initial: " + json encoder . encode ( obj [ 'command' ] [ 'group' ] [ 'initial' ] ) else : initial = None if 'cond' in obj [ 'command' ] [ 'group' ] : cond = "cond: " + json encoder . encode ( obj [ 'command' ] [ 'group' ] [ 'cond' ] ) else : cond = None if '$keyf' in obj [ 'command' ] [ 'group' ] : key function = "keyf: " + min script ( obj [ 'command' ] [ 'group' ] [ '$keyf' ] ) else : key function = None if '$reduce' in obj [ 'command' ] [ 'group' ] : reduce func = "reduce: " + min script ( obj [ 'command' ] [ 'group' ] [ '$reduce' ] ) else : reduce func = None if 'finalize' in obj [ 'command' ] [ 'group' ] : finalize func = "finalize: " + min script ( obj [ 'command' ] [ 'group' ] [ 'finalize' ] ) else : finalize func = None query = ", " . join ( list ( filter ( lambda x : x , ( key , reduce func , initial , key function , cond , finalize func ) ) ) ) elif 'map' in obj [ "command" ] : operation = "map" doc = obj [ "command" ] [ "mapreduce" ] del obj [ "command" ] [ "mapreduce" ] map func = min script ( obj [ 'command' ] [ "map" ] ) del obj [ 'command' ] [ "map" ] reduce func = min script ( obj [ 'command' ] [ "reduce" ] ) del obj [ 'command' ] [ "reduce" ] query = "{%s, %s, %s}" % ( map func , reduce func , json encoder . encode ( obj [ 'command' ] ) ) else : warn ( 'Unknown command operation\n Dump: %s' % json encoder . encode ( obj ) ) if not doc : doc = obj [ "command" ] [ operation ] else : warn ( 'Unknown operation "%s"\n Dump: %s' % ( operation , json encoder . encode ( obj ) ) ) if metadata : met = [ ] for m in metadata : if m in obj and obj [ m ] != { } : q = m + ": " if isinstance ( obj [ m ] , str ) : q += '"%s"' % obj [ m ] elif isinstance ( obj [ m ] , dict ) : q += json encoder . encode ( obj [ m ] ) else : q += str ( obj [ m ] ) met . append ( q ) if met : if not query . endswith ( "." ) : query += ". " if not query . endswith ( " " ) : query += " " query += ", " . join ( met ) sys . stdout . write ( "%s %s [%s] : %s\n" % ( ts time . strftime ( "%Y-%m-%d %H:%M:%S.%f" ) [ : - 3 ] , operation . upper ( ) . ljust ( 9 ) , doc , query ) ) sys . stdout . flush ( ) # Allows pipe the output during the execution with others tools like 'grep' except ( Key Error , Type Error ) : warn ( 'Unknown registry\n Dump: %s' % json encoder . encode ( obj ) )
def error ( msg , exit code ) : sys . stderr . write ( "%s\ntry 'mongotail --help' for more information\n" % msg ) sys . stderr . flush ( ) exit ( exit code )
def error parsing ( msg = "unknown options" ) : sys . stderr . write ( "Error parsing command line: %s\ntry 'mongotail --help' for more information\n" % msg ) sys . stderr . flush ( ) exit ( EINVAL )
def draw ( self ) : self . screen . border ( 0 ) if self . title is not None : self . screen . addstr ( 2 , 2 , self . title , curses . A STANDOUT ) if self . subtitle is not None : self . screen . addstr ( 4 , 2 , self . subtitle , curses . A BOLD ) for index , item in enumerate ( self . items ) : if self . current option == index : text style = self . highlight else : text style = self . normal self . screen . addstr ( 5 + index , 4 , item . show ( index ) , text style ) screen rows , screen cols = Curses Menu . stdscr . getmaxyx ( ) top row = 0 if 6 + len ( self . items ) > screen rows : if screen rows + self . current option < 6 + len ( self . items ) : top row = self . current option else : top row = 6 + len ( self . items ) - screen rows self . screen . refresh ( top row , 0 , 0 , 0 , screen rows - 1 , screen cols - 1 )
def process user input ( self ) : user input = self . get input ( ) go to max = ord ( "9" ) if len ( self . items ) >= 9 else ord ( str ( len ( self . items ) ) ) if ord ( '1' ) <= user input <= go to max : self . go to ( user input - ord ( '0' ) - 1 ) elif user input == curses . KEY DOWN : self . go down ( ) elif user input == curses . KEY UP : self . go up ( ) elif user input == ord ( "\n" ) : self . select ( ) return user input
def select ( self ) : self . selected option = self . current option self . selected item . set up ( ) self . selected item . action ( ) self . selected item . clean up ( ) self . returned value = self . selected item . get return ( ) self . should exit = self . selected item . should exit if not self . should exit : self . draw ( )
def show ( self , index ) : if self . menu and self . menu . parent : self . text = "Return to %s menu" % self . menu . parent . title else : self . text = "Exit" return super ( Exit Item , self ) . show ( index )
def action ( self ) : self . return value = self . function ( * self . args , * * self . kwargs )
def action ( self ) : commandline = "{0} {1}" . format ( self . command , " " . join ( self . arguments ) ) try : completed process = subprocess . run ( commandline , shell = True ) self . exit status = completed process . returncode except Attribute Error : self . exit status = subprocess . call ( commandline , shell = True )
def set up ( self ) : self . menu . pause ( ) curses . def prog mode ( ) self . menu . clear screen ( )
def clean up ( self ) : self . submenu . join ( ) self . menu . clear screen ( ) curses . reset prog mode ( ) curses . curs set ( 1 ) # reset doesn't do this right curses . curs set ( 0 ) self . menu . resume ( )
def add ( df , new column , column 1 , column 2 ) : return basic math operation ( df , new column , column 1 , column 2 , op = 'add' )
def subtract ( df , new column , column 1 , column 2 ) : return basic math operation ( df , new column , column 1 , column 2 , op = 'sub' )
def multiply ( df , new column , column 1 , column 2 ) : return basic math operation ( df , new column , column 1 , column 2 , op = 'mul' )
def divide ( df , new column , column 1 , column 2 ) : return basic math operation ( df , new column , column 1 , column 2 , op = 'truediv' )
def cumsum ( df , new column : str , column : str , index : list , date column : str , date format : str ) : logging . get Logger ( name ) . warning ( f"DEPRECATED: use compute cumsum" ) date temp = ' date temp ' if isinstance ( index , str ) : index = [ index ] levels = list ( range ( 0 , len ( index ) ) ) df [ date temp ] = pd . to datetime ( df [ date column ] , format = date format ) reference cols = [ date temp , date column ] df = df . groupby ( index + reference cols ) . sum ( ) df [ new column ] = df . groupby ( level = levels ) [ column ] . cumsum ( ) df . reset index ( inplace = True ) del df [ date temp ] return df
def log message ( logger , message = "" ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : log message ( logger , func . name , message ) result = func ( * args , * * kwargs ) return result return wrapper return decorator
def log time ( logger ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : start = time . time ( ) result = func ( * args , * * kwargs ) end = time . time ( ) log time ( logger , func . name , start , end ) return result return wrapper return decorator
def clean cachedir old entries ( cachedir : Store Backend Base , func name : str , limit : int ) -> int : if limit < 1 : raise Value Error ( "'limit' must be greater or equal to 1" ) cache entries = get cachedir entries ( cachedir , func name ) cache entries = sorted ( cache entries , key = lambda e : e . last access , reverse = True ) cache entries to remove = cache entries [ limit : ] for entry in cache entries to remove : shutil . rmtree ( entry . path , ignore errors = True ) return len ( cache entries to remove )
def ada family core ( params , gparams , learning rate = 0.01 , eps = 1e-6 , rho = 0.95 , method = "ADADELTA" , beta = 0.0 , gsum regularization = 0.0001 ) : , , , args = inspect . getargvalues ( inspect . currentframe ( ) ) logging . info ( "ada family core: %s" % str ( args . items ( ) ) ) free parameters = [ ] if method == "FINETUNING ADAGRAD" : method = "ADAGRAD" gsum regularization = 0 one Minus Beta = 1 - beta gsums = [ theano . shared ( np . zeros like ( param . get value ( borrow = True ) , dtype = FLOATX ) , name = "gsum %s" % param . name ) if ( method == 'ADADELTA' or method == 'ADAGRAD' ) else None for param in params ] xsums = [ theano . shared ( np . zeros like ( param . get value ( borrow = True ) , dtype = FLOATX ) , name = "xsum %s" % param . name ) if method == 'ADADELTA' else None for param in params ] # Fix for Ada Grad, init gsum to 1 if method == 'ADAGRAD' : for gsum in gsums : gsum . set value ( gsum . get value ( ) ** 0 ) updates = Ordered Dict ( ) # Updates for gparam , param , gsum , xsum in zip ( gparams , params , gsums , xsums ) : if method == 'ADADELTA' : updates [ gsum ] = rho * gsum + ( 1. - rho ) * ( gparam ** 2 ) dparam = - T . sqrt ( ( xsum + eps ) / ( updates [ gsum ] + eps ) ) * gparam updates [ xsum ] = rho * xsum + ( 1. - rho ) * ( dparam ** 2 ) updates [ param ] = param * one Minus Beta + dparam elif method == 'ADAGRAD' : updates [ gsum ] = gsum + ( gparam ** 2 ) - gsum regularization * gsum updates [ param ] = param * one Minus Beta - learning rate * ( gparam / ( T . sqrt ( updates [ gsum ] + eps ) ) ) else : updates [ param ] = param * one Minus Beta - gparam * learning rate # Add free parameters if method == 'ADADELTA' : free parameters . extend ( gsums + xsums ) elif method == 'ADAGRAD' : free parameters . extend ( gsums ) # Check dtype for k in updates : if updates [ k ] . dtype != FLOATX : updates [ k ] = updates [ k ] . astype ( FLOATX ) return updates . items ( ) , free parameters
def learning updates ( self ) : params = self . training params ( ) gradients = self . get gradients ( params ) return self . optimization updates ( params , gradients )
def training params ( self ) : params = self . network . parameters # Freeze parameters if self . config . fixed parameters : logging . info ( "fixed parameters: %s" % ", " . join ( map ( str , self . config . fixed parameters ) ) ) params = [ p for p in params if p not in self . config . fixed parameters ] return params
def optimization updates ( self , params , gradients ) : updates , free parameters = optimize updates ( params , gradients , self . config ) self . network . free parameters . extend ( free parameters ) logging . info ( "Added %d free parameters for optimization" % len ( free parameters ) ) return updates
def first glimpse sensor ( self , x t ) : downsampled img = theano . tensor . signal . downsample . max pool 2d ( x t , ( 4 , 4 ) ) downsampled img = downsampled img . flatten ( ) first l = T . dot ( downsampled img , self . W f ) if self . disable reinforce : wf grad = self . W f if self . random glimpse : first l = self . srng . uniform ( ( 2 , ) , low = - 1.7 , high = 1.7 ) else : sampled l t = self . sample gaussian ( first l , self . cov ) sampled pdf = self . multi gaussian pdf ( disconnected grad ( sampled l t ) , first l ) wf grad = T . grad ( T . log ( sampled pdf ) , self . W f ) first l = sampled l t return first l , wf grad
def prepare ( self ) : self . output dim = 10 self . encoder = Chain ( self . input dim ) . stack ( Dense ( self . internal layer size , 'tanh' ) ) self . decoder = Chain ( self . internal layer size ) . stack ( Dense ( self . input dim ) ) self . classifier = Chain ( self . internal layer size ) . stack ( Dense ( 50 , 'tanh' ) , Dense ( self . output dim ) , Softmax ( ) ) self . register inner layers ( self . encoder , self . decoder , self . classifier ) self . target input = T . ivector ( 'target' ) self . register external inputs ( self . target input )
def compute tensor ( self , x ) : internal variable = self . encoder . compute tensor ( x ) decoding output = self . decoder . compute tensor ( internal variable ) classification output = self . classifier . compute tensor ( internal variable ) auto encoder cost = Auto Encoder Cost ( decoding output , x ) . get ( ) classification cost = Cross Entropy Cost ( classification output , self . target input ) . get ( ) final cost = 0.01 * auto encoder cost + classification cost error rate = Error Rate Cost ( classification output , self . target input ) . get ( ) self . register monitors ( ( "err" , error rate ) , ( "encoder cost" , auto encoder cost ) , ( "classify cost" , classification cost ) ) return final cost
def vectorize target ( self , size ) : if self . train set : self . train set = self . vectorize set ( self . train set , size ) if self . valid set : self . valid set = self . vectorize set ( self . valid set , size ) if self . test set : self . test set = self . vectorize set ( self . test set , size )
def report ( self ) : logging . info ( "%s train=%d valid=%d test=%d" % ( self . class . name , len ( list ( self . train set ) ) if self . train set else 0 , len ( list ( self . valid set ) ) if self . valid set else 0 , len ( list ( self . test set ) ) if self . test set else 0 ) )
def train ( self , train set , valid set = None , test set = None , train size = None ) : iteration = 0 while True : if not iteration % self . config . test frequency and test set : try : self . test ( iteration , test set ) except Keyboard Interrupt : logging . info ( 'interrupted!' ) break if not iteration % self . validation frequency and valid set : try : if not self . evaluate ( iteration , valid set ) : logging . info ( 'patience elapsed, bailing out' ) break except Keyboard Interrupt : logging . info ( 'interrupted!' ) break train message = "" try : train message = self . train func ( train set ) except Keyboard Interrupt : logging . info ( 'interrupted!' ) break if not iteration % self . config . monitor frequency : logging . info ( 'monitor (iter=%i) %s' , iteration + 1 , train message ) iteration += 1 if hasattr ( self . network , "iteration callback" ) : self . network . iteration callback ( ) yield train message if valid set : self . set params ( self . best params ) if test set : self . test ( 0 , test set )
def sample ( self , input , steps ) : inputs = [ [ onehot ( self . input dim , x ) for x in input ] ] for in range ( steps ) : target = self . compute ( inputs ) [ 0 , - 1 ] . argmax ( ) input . append ( target ) inputs [ 0 ] . append ( onehot ( self . input dim , target ) ) return input
def compute alignments ( self , prev state , precomputed values , mask = None ) : Wa Sp = T . dot ( prev state , self . Wa ) Ua H = precomputed values # For test time the Ua H will be (time, output dim) if Ua H . ndim == 2 : preact = Wa Sp [ : , None , : ] + Ua H [ None , : , : ] else : preact = Wa Sp [ : , None , : ] + Ua H act = T . activate ( preact , 'tanh' ) align scores = T . dot ( act , self . Va ) # ~ (batch, time) if mask : mask = ( 1 - mask ) * - 99.00 if align scores . ndim == 3 : align scores += mask [ None , : ] else : align scores += mask align weights = T . nnet . softmax ( align scores ) return align weights
def compute context vector ( self , prev state , inputs , precomputed values = None , mask = None ) : precomputed values = precomputed values if precomputed values else self . precompute ( inputs ) align weights = self . compute alignments ( prev state , precomputed values , mask ) context vector = T . sum ( align weights [ : , : , None ] * inputs , axis = 1 ) return context vector
def train ( self , train set , valid set = None , test set = None , train size = None ) : from platoon . channel import Worker from platoon . param sync import EASGD , ASGD server port = self . port param map = self . create param map ( ) # Initialize the worker worker = Worker ( control port = server port ) if self . config . learning rate : worker . send req ( { 'init schedule' : self . schedule params } ) self . sync hyperparams ( worker . send req ( 'sync hyperparams' ) [ 'sync hyperparams' ] ) easgd alpha = worker . send req ( 'get easgd alpha' ) if self . using easgd : self . logger . info ( "using EASGD with alpha={}" . format ( easgd alpha ) ) else : self . logger . info ( "using ASGD rule" ) rule = EASGD ( easgd alpha ) if self . using easgd else ASGD ( ) worker . init shared params ( param map . values ( ) , param sync rule = rule ) worker . send req ( { "set names" : None , "training names" : self . training names , "evaluation names" : self . evaluation names } ) # Load all training batches, consume vast memory here self . logger . info ( "started process {}" . format ( os . getpid ( ) ) ) self . logger . info ( "(proc {}) load training data" . format ( os . getpid ( ) ) ) train batches = list ( train set ) network callback = bool ( self . network . training callbacks ) trainer callback = bool ( self . iter controllers ) # Start from valid, so the performance when a worked join can be known worker . copy to local ( ) if valid set : self . run valid ( self . epoch , valid set , dry run = True ) self . fix costs ( ) worker . send req ( { "valid done" : None , "valid costs" : self . last run costs , "auto save" : self . config . auto save } ) worker . copy to local ( ) # Begin the loop while True : resp = worker . send req ( 'next' ) if resp == 'stop' : break elif resp == 'wait' : time . sleep ( 1 ) elif resp == 'get num batches' : worker . send req ( { 'get num batches done' : len ( train batches ) } ) elif 'eval' in resp : self . best cost = resp [ 'best valid cost' ] worker . copy to local ( ) valid costs = None test costs = None if valid set : self . run valid ( self . epoch , valid set ) self . fix costs ( ) valid costs = self . last run costs if test set : self . run test ( self . epoch , test set ) self . fix costs ( ) test costs = self . last run costs worker . send req ( { "eval done" : None , "valid costs" : valid costs , "test costs" : test costs , "auto save" : self . config . auto save } ) elif 'valid' in resp : self . best cost = resp [ 'best valid cost' ] worker . copy to local ( ) if valid set : self . run valid ( self . epoch , valid set , dry run = True ) self . fix costs ( ) worker . send req ( { "valid done" : None , "valid costs" : self . last run costs , "auto save" : self . config . auto save } ) elif 'train' in resp : batch ids = resp [ 'train' ] batch costs = [ [ ] for in self . training names ] for batch id in batch ids : x = train batches [ batch id ] cost x = self . learn ( * x ) for i , cost in enumerate ( cost x ) : batch costs [ i ] . append ( cost ) self . last cost = cost x [ 0 ] if network callback : self . network . training callback ( ) if trainer callback : for func in self . iter controllers : func ( self ) worker . sync params ( synchronous = True ) worker . send req ( { 'train done' : None , 'costs' : [ float ( np . mean ( c ) ) for c in batch costs ] } ) elif 'sync hyperparams' in resp : self . sync hyperparams ( resp [ 'sync hyperparams' ] ) worker . close ( ) return [ ]
def concatenate ( vars , axis = - 1 ) : from deepy . core . neural var import Neural Variable if isinstance ( vars [ 0 ] , Neural Variable ) : concat var = Concatenate ( axis = axis ) . compute ( * vars ) if axis == - 1 or axis == vars [ 0 ] . tensor . ndim - 1 : concat var . output dim = sum ( [ x . output dim for x in vars ] , 0 ) else : concat var = TT . concatenate ( vars , axis ) return concat var
def pad ( self , side , length ) : if self . train set : self . train set = pad dataset ( self . train set , side , length ) if self . valid set : self . valid set = pad dataset ( self . valid set , side , length ) if self . test set : self . test set = pad dataset ( self . test set , side , length )
def rmsprop core ( params , gradients , momentum = 0.9 , learning rate = 0.01 ) : for param , grad in zip ( params , gradients ) : rms = theano . shared ( np . zeros like ( param . get value ( ) ) , name = param . name + ' rms' ) rms = momentum * rms + ( 1 - momentum ) * grad * grad yield rms , rms yield param , param - learning rate * grad / T . sqrt ( rms + 1e-8 )
def report ( self ) : if not self . end time : self . end ( ) print ( "Time: {} mins" . format ( ( self . end time - self . start time ) / 60 ) )
def run ( self , data x ) : output vars = self . compute ( * data x ) return self . extract costs ( output vars )
def invoke ( self ) : self . counter += 1 if self . counter % self . freq == 0 : cnt = 0. sum map = defaultdict ( float ) for x in self . trainer . get data ( self . data split ) : val map = self . run ( x ) if not isinstance ( val map , dict ) : raise Exception ( "Monitor.run must return a dict." ) for k , val in val map . items ( ) : sum map [ k ] += val cnt += 1 for k in sum map : sum map [ k ] /= cnt new best = self . compare ( sum map ) self . trainer . report ( sum map , self . data split , new best = new best ) if new best : self . trainer . save checkpoint ( self . save path )
def build loop vars ( self ) : from theano . tensor . var import Tensor Variable from deepy . core . neural var import Neural Variable if not self . loop vars : self . ordered out keys = self . outputs . keys ( ) seq keys = self . sequences . keys ( ) filled out keys = [ k for k in self . ordered out keys if self . outputs [ k ] ] nonseq keys = self . non sequences . keys ( ) dummy tensors , self . scan local vars = get dummy args ( sequences = [ self . sequences [ k ] . tensor for k in seq keys ] , outputs info = [ self . outputs [ k ] . tensor for k in self . ordered out keys ] , non sequences = [ self . non sequences [ k ] . tensor for k in nonseq keys ] , * * self . kwargs ) dummy map = dict ( zip ( seq keys + filled out keys + nonseq keys , dummy tensors ) ) arg map = self . sequences . copy ( ) arg map . update ( self . outputs ) arg map . update ( self . non sequences ) self . loop vars = Loop Vars ( ) for k , dummy tensor in dummy map . items ( ) : dummy var = Neural Variable ( dummy tensor , dim = arg map [ k ] . dim ( ) ) self . loop vars [ k ] = dummy var
def scan step ( self , vars ) : from neural var import Neural Variable if not self . loop vars : raise Exception ( "The loop is not initialized. To initialize the loop, use `with loop as vars`" ) replace map = { } for k , var in vars . items ( ) : if var is not None : replace map [ self . dummy nodes [ k ] . tensor ] = var . tensor outputs = { } for k in self . outputs : if k not in self . loop vars : raise Exception ( "{} can not be found in loop vars." . format ( k ) ) output node = theano . clone ( self . loop vars [ k ] . tensor , replace map ) outputs [ k ] = Neural Variable ( output node , self . loop vars [ k ] . dim ( ) ) return outputs
def momentum core ( params , gradients , momentum = 0.9 , learning rate = 0.01 ) : free parameters = [ ] updates = [ ] for param , grad in zip ( params , gradients ) : delta = learning rate * grad velocity = theano . shared ( np . zeros like ( param . get value ( ) ) , name = param . name + ' vel' ) updates . append ( ( velocity , momentum * velocity - delta ) ) updates . append ( ( param , param + velocity ) ) free parameters . append ( velocity ) return updates , free parameters
def iftrain ( self , then branch , else branch ) : return ifelse ( self . training flag , then branch , else branch , name = "iftrain" )
def skip ( self , n batches , n epochs = 0 ) : logging . info ( "skip %d epochs and %d batches" % ( n epochs , n batches ) ) self . skip batches = n batches self . skip epochs = n epochs
def train ( self , train set , valid set = None , test set = None , train size = None ) : self . epoch = 0 while True : if self . skip epochs > 0 : logging . info ( "skipping one epoch ..." ) self . skip epochs -= 1 self . epoch += 1 yield None continue # Test if not self . epoch % self . config . test frequency and test set : try : self . run test ( self . epoch , test set ) except Keyboard Interrupt : logging . info ( 'interrupted!' ) break # Validate if not self . epoch % self . validation frequency and valid set : try : if not self . run valid ( self . epoch , valid set ) : logging . info ( 'patience elapsed, bailing out' ) break except Keyboard Interrupt : logging . info ( 'interrupted!' ) break # Train one step try : costs = self . run train ( self . epoch , train set , train size ) except Keyboard Interrupt : logging . info ( 'interrupted!' ) break # Check costs if np . isnan ( costs [ 0 ] [ 1 ] ) : logging . info ( "Na N detected in costs, rollback to last parameters" ) self . set params ( * self . checkpoint ) else : self . epoch += 1 self . network . epoch callback ( ) yield dict ( costs ) if valid set and self . config . get ( "save best parameters" , True ) : self . set params ( * self . best params ) if test set : self . run test ( - 1 , test set )
def run train ( self , epoch , train set , train size = None ) : self . network . train logger . record epoch ( epoch + 1 ) costs = self . train step ( train set , train size ) if not epoch % self . config . monitor frequency : self . report ( dict ( costs ) , "train" , epoch ) self . last run costs = costs return costs
def run valid ( self , epoch , valid set , dry run = False , save path = None ) : costs = self . valid step ( valid set ) # this is the same as: (J i - J f) / J i > min improvement , J = costs [ 0 ] new best = False if self . best cost - J > self . best cost * self . min improvement : # save the best cost and parameters self . best params = self . copy params ( ) new best = True if not dry run : self . best cost = J self . best epoch = epoch self . save checkpoint ( save path ) self . report ( dict ( costs ) , type = "valid" , epoch = 0 if dry run else epoch , new best = new best ) self . last run costs = costs return epoch - self . best epoch < self . patience
def report ( self , score map , type = "valid" , epoch = - 1 , new best = False ) : type str = type if len ( type str ) < 5 : type str += " " * ( 5 - len ( type str ) ) info = " " . join ( "%s=%.2f" % el for el in score map . items ( ) ) current epoch = epoch if epoch > 0 else self . current epoch ( ) epoch str = "epoch={}" . format ( current epoch + 1 ) if epoch < 0 : epoch str = "dryrun" sys . stdout . write ( "\r" ) sys . stdout . flush ( ) marker = " *" if new best else "" message = "{} ({}) {}{}" . format ( type str , epoch str , info , marker ) self . network . train logger . record ( message ) logging . info ( message )
def get data ( self , data split = "train" ) : if data split == 'train' : return self . current train set elif data split == 'valid' : return self . current valid set elif data split == 'test' : return self . current test set else : return None
def apply ( self , func , dim = None ) : output dim = dim if dim else self . output dim return Neural Variable ( func ( self . tensor ) , output dim )
def report ( self ) : if self . logger : self . logger . info ( "accessed parameters:" ) for key in self . used parameters : self . logger . info ( " - %s %s" % ( key , "(undefined)" if key in self . undefined parameters else "" ) )
def var ( self , tensor type , last dim = 0 , test shape = None ) : from deepy . tensor import var return var ( tensor type , last dim = last dim , test shape = test shape )
def shared ( self , value , name = None ) : if type ( value ) == int : final value = np . array ( value , dtype = "int32" ) elif type ( value ) == float : final value = np . array ( value , dtype = env . FLOATX ) else : final value = value return theano . shared ( final value , name = name )
def invoke ( self ) : self . iter += 1 if self . iter - max ( self . trainer . best iter , self . annealed iter ) >= self . patience : if self . annealed times >= self . anneal times : logging . info ( "ending" ) self . trainer . exit ( ) else : self . trainer . set params ( * self . trainer . best params ) self . learning rate . set value ( self . learning rate . get value ( ) * 0.5 ) self . annealed times += 1 self . annealed iter = self . iter logging . info ( "annealed learning rate to %f" % self . learning rate . get value ( ) )
def invoke ( self ) : self . iter += 1 logging . info ( "{} epochs left to run" . format ( self . patience - self . iter ) ) if self . iter >= self . patience : self . trainer . exit ( )
def stack encoders ( self , * layers ) : self . stack ( * layers ) self . encoding layes . extend ( layers )
def stack decoders ( self , * layers ) : self . stack ( * layers ) self . decoding layers . extend ( layers )
def encode ( self , x ) : if not self . encoding network : self . encoding network = Neural Network ( self . input dim , self . input tensor ) self . encoding network . input variables = self . input variables for layer in self . encoding layes : self . encoding network . stack layer ( layer , no setup = True ) return self . encoding network . compute ( * x )
def decode ( self , x ) : if not self . rep dim : raise Exception ( "rep dim must be set to decode." ) if not self . decoding network : self . decoding network = Neural Network ( self . rep dim ) for layer in self . decoding layers : self . decoding network . stack layer ( layer , no setup = True ) return self . decoding network . compute ( x )
def all parameters ( self ) : params = [ ] params . extend ( self . parameters ) params . extend ( self . free parameters ) return params
def setup variables ( self ) : if self . input tensor : if type ( self . input tensor ) == int : x = dim to var ( self . input tensor , name = "x" ) else : x = self . input tensor else : x = T . matrix ( 'x' ) self . input variables . append ( x ) self . output = x self . test output = x
def compute ( self , * x ) : self . compile ( ) outs = self . compute ( * x ) if self . output keys : return Map Dict ( dict ( zip ( self . output keys , outs ) ) ) else : return outs
def save params ( self , path , new thread = False ) : save logger . info ( path ) param variables = self . all parameters params = [ p . get value ( ) . copy ( ) for p in param variables ] if new thread : thread = Thread ( target = save network params , args = ( params , path ) ) thread . start ( ) else : save network params ( params , path ) self . train logger . save ( path )
def load params ( self , path , exclude free params = False ) : if not os . path . exists ( path ) : return logging . info ( "loading parameters from %s" % path ) # Decide which parameters to load if exclude free params : params to load = self . parameters else : params to load = self . all parameters # Load parameters if path . endswith ( ".gz" ) : opener = gzip . open if path . lower ( ) . endswith ( '.gz' ) else open handle = opener ( path , 'rb' ) saved params = pickle . load ( handle ) handle . close ( ) # Write parameters for target , source in zip ( params to load , saved params ) : logging . info ( '%s: setting value %s' , target . name , source . shape ) target . set value ( source ) elif path . endswith ( ".npz" ) : arrs = np . load ( path ) # Write parameters for target , idx in zip ( params to load , range ( len ( arrs . keys ( ) ) ) ) : source = arrs [ 'arr %d' % idx ] logging . info ( '%s: setting value %s' , target . name , source . shape ) target . set value ( source ) else : raise Exception ( "File format of %s is not supported, use '.gz' or '.npz' or '.uncompressed.gz'" % path ) self . train logger . load ( path )
def report ( self ) : logging . info ( "network inputs: %s" , " " . join ( map ( str , self . input variables ) ) ) logging . info ( "network targets: %s" , " " . join ( map ( str , self . target variables ) ) ) logging . info ( "network parameters: %s" , " " . join ( map ( str , self . all parameters ) ) ) logging . info ( "parameter count: %d" , self . parameter count )
def register updates ( self , * updates ) : for key , node in updates : if key not in self . registered updates : self . updates . append ( ( key , node ) ) self . registered updates . add ( key )
def register training updates ( self , * updates ) : for key , node in updates : if key not in self . registered training updates : self . training updates . append ( ( key , node ) ) self . registered training updates . add ( key )
def register monitors ( self , * monitors ) : for key , node in monitors : if key not in self . registered monitors : node *= 1.0 # Avoid Cuda Ndarray self . training monitors . append ( ( key , node ) ) self . testing monitors . append ( ( key , node ) ) self . registered monitors . add ( key )
def dump one ( elt to pickle , file obj ) : pickled elt str = dumps ( elt to pickle ) file obj . write ( pickled elt str ) # record separator is a blank line # (since pickled elt str might contain its own newlines) file obj . write ( '\n\n' )
def load params ( self , path , exclude free params = False ) : from deepy . core import graph from deepy . core . comp graph import Computational Graph model = graph . compile ( blocks = [ self ] ) model . load params ( path , exclude free params = exclude free params )
def onehot tensor ( i matrix , vocab size ) : dim0 , dim1 = i matrix . shape i vector = i matrix . reshape ( ( - 1 , ) ) hot matrix = T . extra ops . to one hot ( i vector , vocab size ) . reshape ( ( dim0 , dim1 , vocab size ) ) return hot matrix
def create request elements ( cls , request type , credentials , url , method = 'GET' , params = None , headers = None , body = '' , secret = None , redirect uri = '' , scope = '' , csrf = '' , user state = '' ) : headers = headers or { } params = params or { } consumer key = credentials . consumer key or '' consumer secret = credentials . consumer secret or '' token = credentials . token or '' refresh token = credentials . refresh token or credentials . token or '' # Separate url base and query parameters. url , base params = cls . split url ( url ) # Add params extracted from URL. params . update ( dict ( base params ) ) if request type == cls . USER AUTHORIZATION REQUEST TYPE : # User authorization request. # TODO: Raise error for specific message for each missing argument. if consumer key and redirect uri and ( csrf or not cls . supports csrf protection ) : params [ 'client id' ] = consumer key params [ 'redirect uri' ] = redirect uri params [ 'scope' ] = scope if cls . supports user state : params [ 'state' ] = base64 . urlsafe b64encode ( json . dumps ( { "csrf" : csrf , "user state" : user state } ) . encode ( 'utf-8' ) ) else : params [ 'state' ] = csrf params [ 'response type' ] = 'code' # Add authorization header headers . update ( cls . authorization header ( credentials ) ) else : raise O Auth2Error ( 'Credentials with valid consumer key and arguments ' 'redirect uri, scope and state are required to create ' 'O Auth 2.0 user authorization request elements!' ) elif request type == cls . ACCESS TOKEN REQUEST TYPE : # Access token request. if consumer key and consumer secret : params [ 'code' ] = token params [ 'client id' ] = consumer key params [ 'client secret' ] = consumer secret params [ 'redirect uri' ] = redirect uri params [ 'grant type' ] = 'authorization code' # TODO: Check whether all providers accept it headers . update ( cls . authorization header ( credentials ) ) else : raise O Auth2Error ( 'Credentials with valid token, consumer key, ' 'consumer secret and argument redirect uri are required ' 'to create O Auth 2.0 access token request elements!' ) elif request type == cls . REFRESH TOKEN REQUEST TYPE : # Refresh access token request. if refresh token and consumer key and consumer secret : params [ 'refresh token' ] = refresh token params [ 'client id' ] = consumer key params [ 'client secret' ] = consumer secret params [ 'grant type' ] = 'refresh token' else : raise O Auth2Error ( 'Credentials with valid refresh token, consumer key, ' 'consumer secret are required to create O Auth 2.0 ' 'refresh token request elements!' ) elif request type == cls . PROTECTED RESOURCE REQUEST TYPE : # Protected resource request. # Add Authorization header. See: # http://tools.ietf.org/html/rfc6749#section-7.1 if credentials . token type == cls . BEARER : # http://tools.ietf.org/html/rfc6750#section-2.1 headers . update ( { 'Authorization' : 'Bearer {0}' . format ( credentials . token ) } ) elif token : params [ 'access token' ] = token else : raise O Auth2Error ( 'Credentials with valid token are required to create ' 'O Auth 2.0 protected resources request elements!' ) request elements = core . Request Elements ( url , method , params , headers , body ) return cls . x request elements filter ( request type , request elements , credentials )
def x credentials parser ( credentials , data ) : # Facebook returns "expires" instead of "expires in". credentials . expire in = data . get ( 'expires' ) if data . get ( 'token type' ) == 'bearer' : # TODO: cls is not available here, hardcode for now. credentials . token type = 'Bearer' return credentials
def login ( provider name ) : # We need response object for the Werkzeug Adapter. response = make response ( ) # Log the user in, pass it the adapter and the provider name. result = authomatic . login ( Werkzeug Adapter ( request , response ) , provider name ) # If there is no Login Result object, the login procedure is still pending. if result : if result . user : # We need to update the user to get more info. result . user . update ( ) # The rest happens inside the template. return render template ( 'login.html' , result = result ) # Don't forget to return the response. return response
def save ( self ) : if self . data : cookie = self . create cookie ( ) cookie len = len ( cookie ) if cookie len > 4093 : raise Session Error ( 'Cookie too long! The cookie size {0} ' 'is more than 4093 bytes.' . format ( cookie len ) ) self . adapter . set header ( 'Set-Cookie' , cookie ) # Reset data self . data = { }
def get data ( self ) : cookie = self . adapter . cookies . get ( self . name ) return self . deserialize ( cookie ) if cookie else { }
def data ( self ) : if not self . data : self . data = self . get data ( ) # Always return a dict, even if deserialization returned nothing if self . data is None : self . data = { } return self . data
def signature ( self , * parts ) : signature = hmac . new ( six . b ( self . secret ) , digestmod = hashlib . sha1 ) signature . update ( six . b ( '|' . join ( parts ) ) ) return signature . hexdigest ( )
def valid ( self ) : if self . expiration time : return self . expiration time > int ( time . time ( ) ) else : return True
def is binary string ( content ) : textchars = ( bytearray ( [ 7 , 8 , 9 , 10 , 12 , 13 , 27 ] ) + bytearray ( range ( 0x20 , 0x100 ) ) ) return bool ( content . translate ( None , textchars ) )
def content ( self ) : if not self . content : content = self . httplib response . read ( ) if self . is binary string ( content ) : self . content = content else : self . content = content . decode ( 'utf-8' ) return self . content
def create request elements ( cls , request type , credentials , url , params = None , headers = None , body = '' , method = 'GET' , verifier = '' , callback = '' ) : params = params or { } headers = headers or { } consumer key = credentials . consumer key or '' consumer secret = credentials . consumer secret or '' token = credentials . token or '' token secret = credentials . token secret or '' # separate url base and query parameters url , base params = cls . split url ( url ) # add extracted params to future params params . update ( dict ( base params ) ) if request type == cls . USER AUTHORIZATION REQUEST TYPE : # no need for signature if token : params [ 'oauth token' ] = token else : raise O Auth1Error ( 'Credentials with valid token are required to create ' 'User Authorization URL!' ) else : # signature needed if request type == cls . REQUEST TOKEN REQUEST TYPE : # Request Token URL if consumer key and consumer secret and callback : params [ 'oauth consumer key' ] = consumer key params [ 'oauth callback' ] = callback else : raise O Auth1Error ( 'Credentials with valid consumer key, consumer secret ' 'and callback are required to create Request Token ' 'URL!' ) elif request type == cls . ACCESS TOKEN REQUEST TYPE : # Access Token URL if consumer key and consumer secret and token and verifier : params [ 'oauth token' ] = token params [ 'oauth consumer key' ] = consumer key params [ 'oauth verifier' ] = verifier else : raise O Auth1Error ( 'Credentials with valid consumer key, ' 'consumer secret, token and argument verifier' ' are required to create Access Token URL!' ) elif request type == cls . PROTECTED RESOURCE REQUEST TYPE : # Protected Resources URL if consumer key and consumer secret and token and token secret : params [ 'oauth token' ] = token params [ 'oauth consumer key' ] = consumer key else : raise O Auth1Error ( 'Credentials with valid consumer key, ' + 'consumer secret, token and token secret are required ' 'to create Protected Resources URL!' ) # Sign request. # http://oauth.net/core/1.0a/#anchor13 # Prepare parameters for signature base string # http://oauth.net/core/1.0a/#rfc.section.9.1 params [ 'oauth signature method' ] = cls . signature generator . method params [ 'oauth timestamp' ] = str ( int ( time . time ( ) ) ) params [ 'oauth nonce' ] = cls . csrf generator ( str ( uuid . uuid4 ( ) ) ) params [ 'oauth version' ] = '1.0' # add signature to params params [ 'oauth signature' ] = cls . signature generator . create signature ( # noqa method , url , params , consumer secret , token secret ) request elements = core . Request Elements ( url , method , params , headers , body ) return cls . x request elements filter ( request type , request elements , credentials )
def access user info ( self ) : response = super ( Bitbucket , self ) . access user info ( ) response . data . setdefault ( "email" , None ) email response = self . access ( self . user email url ) if email response . data : for item in email response . data : if item . get ( "primary" , False ) : response . data . update ( email = item . get ( "email" , None ) ) return response
def login ( self , * login args , * * login kwargs ) : def decorator ( f ) : @ wraps ( f ) def decorated ( * args , * * kwargs ) : self . response = make response ( ) adapter = Werkzeug Adapter ( request , self . response ) login kwargs . setdefault ( 'session' , session ) login kwargs . setdefault ( 'session saver' , self . session saver ) self . result = super ( Flask Authomatic , self ) . login ( adapter , * login args , * * login kwargs ) return f ( * args , * * kwargs ) return decorated return decorator
def login ( self ) : if self . params . get ( self . identifier param ) : # ================================================================= # Phase 1 before redirect. # ================================================================= self . log ( logging . INFO , u'Starting Open ID authentication procedure.' ) url = users . create login url ( dest url = self . url , federated identity = self . identifier ) self . log ( logging . INFO , u'Redirecting user to {0}.' . format ( url ) ) self . redirect ( url ) else : # ================================================================= # Phase 2 after redirect. # ================================================================= self . log ( logging . INFO , u'Continuing Open ID authentication procedure after redirect.' ) user = users . get current user ( ) if user : self . log ( logging . INFO , u'Authentication successful.' ) self . log ( logging . INFO , u'Creating user.' ) self . user = core . User ( self , id = user . federated identity ( ) , email = user . email ( ) , gae user = user ) # ============================================================= # We're done # ============================================================= else : raise Failure Error ( 'Unable to authenticate identifier "{0}"!' . format ( self . identifier ) )
def session set ( self , key , value ) : self . session [ self . session key ( key ) ] = value
def split url ( url ) : split = parse . urlsplit ( url ) base = parse . urlunsplit ( ( split . scheme , split . netloc , split . path , 0 , 0 ) ) params = parse . parse qsl ( split . query , True ) return base , params
def get app kwarg dict ( app Instance ) : # In order to support blueprints which do not have a config attribute app config = getattr ( app Instance , 'config' , { } ) return dict ( ( k . lower ( ) . replace ( 'cors ' , '' ) , app config . get ( k ) ) for k in CONFIG OPTIONS if app config . get ( k ) is not None )
def ensure iterable ( inst ) : if isinstance ( inst , str ) : return [ inst ] elif not isinstance ( inst , collections . abc . Iterable ) : return [ inst ] else : return inst
def isclose ( a , b , * , rel tol = 1e-09 , abs tol = 0.0 ) : try : return math . isclose ( a , b , rel tol = rel tol , abs tol = abs tol ) except Attribute Error : # Running on older version of python, fall back to hand-rolled implementation if ( rel tol < 0.0 ) or ( abs tol < 0.0 ) : raise Value Error ( "Tolerances must be non-negative, but are rel tol: {} and abs tol: {}" . format ( rel tol , abs tol ) ) if math . isnan ( a ) or math . isnan ( b ) : return False # Na Ns are never close to anything, even other Na Ns if ( a == b ) : return True if math . isinf ( a ) or math . isinf ( b ) : return False # Infinity is only close to itself, and we already handled that case diff = abs ( a - b ) return ( diff <= rel tol * abs ( b ) ) or ( diff <= rel tol * abs ( a ) ) or ( diff <= abs tol )
def get corresponding offsets ( onset fronts , onset front id , onsets , offsets ) : corresponding offsets = [ ] for index in get front idxs from id ( onset fronts , onset front id ) : offset fidx , offset sidx = lookup offset by onset idx ( index , onsets , offsets ) corresponding offsets . append ( ( offset fidx , offset sidx ) ) return corresponding offsets
def remove overlaps ( segmentation mask , fronts ) : fidxs , sidxs = np . where ( ( segmentation mask != fronts ) & ( segmentation mask != 0 ) & ( fronts != 0 ) ) fronts [ fidxs , sidxs ] = 0
def merge adjacent segments ( mask ) : mask ids = [ id for id in np . unique ( mask ) if id != 0 ] for id in mask ids : myfidxs , mysidxs = np . where ( mask == id ) for other in mask ids : # Ugh, brute force O(N^2) algorithm.. gross.. if id == other : continue else : other fidxs , other sidxs = np . where ( mask == other ) if segments are adjacent ( ( myfidxs , mysidxs ) , ( other fidxs , other sidxs ) ) : mask [ other fidxs , other sidxs ] = id
def asa task ( q , masks , stft , sample width , frame rate , nsamples for each fft ) : # Convert each mask to (1 or 0) rather than (ID or 0) for mask in masks : mask = np . where ( mask > 0 , 1 , 0 ) # Multiply the masks against STF Ts masks = [ mask * stft for mask in masks ] nparrs = [ ] dtype dict = { 1 : np . int8 , 2 : np . int16 , 4 : np . int32 } dtype = dtype dict [ sample width ] for m in masks : times , nparr = signal . istft ( m , frame rate , nperseg = nsamples for each fft ) nparr = nparr . astype ( dtype ) nparrs . append ( nparr ) for m in nparrs : q . put ( m ) q . put ( "DONE" )
def list to tf input ( data , response index , num outcomes ) : matrix = np . matrix ( [ row [ : response index ] + row [ response index + 1 : ] for row in data ] ) outcomes = np . asarray ( [ row [ response index ] for row in data ] , dtype = np . uint8 ) outcomes onehot = ( np . arange ( num outcomes ) == outcomes [ : , None ] ) . astype ( np . float32 ) return matrix , outcomes onehot
def expand and standardize dataset ( response index , response header , data set , col vals , headers , standardizers , feats to ignore , columns to expand , outcome trans dict ) : # expand and standardize modified set = [ ] for row index , row in enumerate ( data set ) : new row = [ ] for col index , val in enumerate ( row ) : header = headers [ col index ] # Outcome feature -> index outcome if col index == response index : new outcome = outcome trans dict [ val ] new row . append ( new outcome ) # Ignored feature -> pass elif header in feats to ignore : pass # Categorical feature -> create new binary column for each possible value of the column elif header in columns to expand : for poss val in col vals [ header ] : if val == poss val : new cat val = 1.0 else : new cat val = - 1.0 new row . append ( new cat val ) # Continuous feature -> standardize value with respect to its column else : new cont val = float ( ( val - standardizers [ header ] [ 'mean' ] ) / standardizers [ header ] [ 'std dev' ] ) new row . append ( new cont val ) modified set . append ( new row ) # update headers to reflect column expansion expanded headers = [ ] for header in headers : if header in feats to ignore : pass elif ( header in columns to expand ) and ( header is not response header ) : for poss val in col vals [ header ] : new header = '{} {}' . format ( header , poss val ) expanded headers . append ( new header ) else : expanded headers . append ( header ) return modified set , expanded headers
def list to tf input ( data , response index , num outcomes ) : matrix = np . matrix ( [ row [ : response index ] + row [ response index + 1 : ] for row in data ] ) outcomes = np . asarray ( [ row [ response index ] for row in data ] , dtype = np . uint8 ) return matrix , outcomes
def update index url from configs ( self ) : if 'VIRTUAL ENV' in os . environ : self . pip config locations . append ( os . path . join ( os . environ [ 'VIRTUAL ENV' ] , 'pip.conf' ) ) self . pip config locations . append ( os . path . join ( os . environ [ 'VIRTUAL ENV' ] , 'pip.ini' ) ) if site config files : self . pip config locations . extend ( site config files ) index url = None custom config = None if 'PIP INDEX URL' in os . environ and os . environ [ 'PIP INDEX URL' ] : # environ variable takes priority index url = os . environ [ 'PIP INDEX URL' ] custom config = 'PIP INDEX URL environment variable' else : for pip config filename in self . pip config locations : if pip config filename . startswith ( '~' ) : pip config filename = os . path . expanduser ( pip config filename ) if os . path . isfile ( pip config filename ) : config = Config Parser ( ) config . read ( [ pip config filename ] ) try : index url = config . get ( 'global' , 'index-url' ) custom config = pip config filename break # stop on first detected, because config locations have a priority except ( No Option Error , No Section Error ) : # pragma: nocover pass if index url : self . PYPI API URL = self . prepare api url ( index url ) print ( Color ( 'Setting API url to {{autoyellow}}{}{{/autoyellow}} as found in {{autoyellow}}{}{{/autoyellow}}' '. Use --default-index-url to use pypi default index' . format ( self . PYPI API URL , custom config ) ) )
def main ( ) : options = get options ( ) Windows . enable ( auto colors = True , reset atexit = True ) try : # maybe check if virtualenv is not activated check for virtualenv ( options ) # 1. detect requirements files filenames = Requirements Detector ( options . get ( '<requirements file>' ) ) . get filenames ( ) if filenames : print ( Color ( '{{autoyellow}}Found valid requirements file(s):{{/autoyellow}} ' '{{autocyan}}\n{}{{/autocyan}}' . format ( '\n' . join ( filenames ) ) ) ) else : # pragma: nocover print ( Color ( '{autoyellow}No requirements files found in current directory. CD into your project ' 'or manually specify requirements files as arguments.{/autoyellow}' ) ) return # 2. detect all packages inside requirements packages = Packages Detector ( filenames ) . get packages ( ) # 3. query pypi API, see which package has a newer version vs the one in requirements (or current env) packages status map = Packages Status Detector ( packages , options . get ( '--use-default-index' ) ) . detect available upgrades ( options ) # 4. [optionally], show interactive screen when user can choose which packages to upgrade selected packages = Package Interactive Selector ( packages status map , options ) . get packages ( ) # 5. having the list of packages, do the actual upgrade and replace the version inside all filenames upgraded packages = Packages Upgrader ( selected packages , filenames , options ) . do upgrade ( ) print ( Color ( '{{autogreen}}Successfully upgraded (and updated requirements) for the following packages: ' '{}{{/autogreen}}' . format ( ',' . join ( [ package [ 'name' ] for package in upgraded packages ] ) ) ) ) if options [ '--dry-run' ] : print ( Color ( '{automagenta}Actually, no, because this was a simulation using --dry-run{/automagenta}' ) ) except Keyboard Interrupt : # pragma: nocover print ( Color ( '\n{autored}Upgrade interrupted.{/autored}' ) )
def autodetect files ( self ) : if self . is valid requirements file ( 'requirements.txt' ) : self . filenames . append ( 'requirements.txt' ) if self . is valid requirements file ( 'requirements.pip' ) : # pragma: nocover self . filenames . append ( 'requirements.pip' ) if os . path . isdir ( 'requirements' ) : for filename in os . listdir ( 'requirements' ) : file path = os . path . join ( 'requirements' , filename ) if self . is valid requirements file ( file path ) : self . filenames . append ( file path ) self . check inclusions recursively ( )
def handle error ( errcode ) : if type ( errcode ) is c int : errcode = errcode . value if errcode == 0 : pass # no error elif errcode == - 1 : raise Timeout Error ( "the operation failed due to a timeout." ) elif errcode == - 2 : raise Lost Error ( "the stream has been lost." ) elif errcode == - 3 : raise Invalid Argument Error ( "an argument was incorrectly specified." ) elif errcode == - 4 : raise Internal Error ( "an internal error has occurred." ) elif errcode < 0 : raise Runtime Error ( "an unknown error has occurred." )
def child ( self , name ) : return XML Element ( lib . lsl child ( self . e , str . encode ( name ) ) )
def set name ( self , name ) : return bool ( lib . lsl set name ( self . e , str . encode ( name ) ) )
def set value ( self , value ) : return bool ( lib . lsl set value ( self . e , str . encode ( value ) ) )
def append child ( self , name ) : return XML Element ( lib . lsl append child ( self . e , str . encode ( name ) ) )
def prepend child ( self , name ) : return XML Element ( lib . lsl prepend child ( self . e , str . encode ( name ) ) )
def append copy ( self , elem ) : return XML Element ( lib . lsl append copy ( self . e , elem . e ) )
def prepend copy ( self , elem ) : return XML Element ( lib . lsl prepend copy ( self . e , elem . e ) )
def remove child ( self , rhs ) : if type ( rhs ) is XML Element : lib . lsl remove child ( self . e , rhs . e ) else : lib . lsl remove child n ( self . e , rhs )
def do AUTOCOMPLETE ( cmd , s ) : s = list ( preprocess query ( s ) ) [ 0 ] keys = [ k . decode ( ) for k in DB . smembers ( edge ngram key ( s ) ) ] print ( white ( keys ) ) print ( magenta ( '({} elements)' . format ( len ( keys ) ) ) )
def compute edge ngrams ( token , min = None ) : if min is None : min = config . MIN EDGE NGRAMS token = token [ : config . MAX EDGE NGRAMS + 1 ] return [ token [ : i ] for i in range ( min , len ( token ) ) ]
def iter pipe ( pipe , processors ) : if isinstance ( pipe , str ) : pipe = [ pipe ] for it in processors : pipe = it ( pipe ) yield from pipe
def make fuzzy ( word , max = 1 ) : # inversions neighbors = [ ] for i in range ( 0 , len ( word ) - 1 ) : neighbor = list ( word ) neighbor [ i ] , neighbor [ i + 1 ] = neighbor [ i + 1 ] , neighbor [ i ] neighbors . append ( '' . join ( neighbor ) ) # substitutions for letter in string . ascii lowercase : for i in range ( 0 , len ( word ) ) : neighbor = list ( word ) if letter != neighbor [ i ] : neighbor [ i ] = letter neighbors . append ( '' . join ( neighbor ) ) # insertions for letter in string . ascii lowercase : for i in range ( 0 , len ( word ) + 1 ) : neighbor = list ( word ) neighbor . insert ( i , letter ) neighbors . append ( '' . join ( neighbor ) ) if len ( word ) > 3 : # removal for i in range ( 0 , len ( word ) ) : neighbor = list ( word ) del neighbor [ i ] neighbors . append ( '' . join ( neighbor ) ) return neighbors
def do help ( self , command ) : if command : doc = getattr ( self , 'do ' + command ) . doc print ( cyan ( doc . replace ( ' ' * 8 , '' ) ) ) else : print ( magenta ( 'Available commands:' ) ) print ( magenta ( 'Type "HELP <command>" to get more info.' ) ) names = self . get names ( ) names . sort ( ) for name in names : if name [ : 3 ] != 'do ' : continue doc = getattr ( self , name ) . doc doc = doc . split ( '\n' ) [ 0 ] print ( '{} {}' . format ( yellow ( name [ 3 : ] ) , cyan ( doc . replace ( ' ' * 8 , ' ' ) . replace ( '\n' , '' ) ) ) )
def do DBINFO ( self , * args ) : info = DB . info ( ) keys = [ 'keyspace misses' , 'keyspace hits' , 'used memory human' , 'total commands processed' , 'total connections received' , 'connected clients' ] for key in keys : print ( '{}: {}' . format ( white ( key ) , blue ( info [ key ] ) ) ) nb of redis db = int ( DB . config get ( 'databases' ) [ 'databases' ] ) for db index in range ( nb of redis db - 1 ) : db name = 'db{}' . format ( db index ) if db name in info : label = white ( 'nb keys (db {})' . format ( db index ) ) print ( '{}: {}' . format ( label , blue ( info [ db name ] [ 'keys' ] ) ) )
def send ( r , stream = False ) : r . send ( stream = stream ) return r . response
def reinterptet harray to bits ( type From , sig Or Val , bits T ) : size = int ( type From . size ) width Of Elm = type From . elm Type . bit length ( ) w = bits T . bit length ( ) if size * width Of Elm != w : raise Type Conversion Err ( "Size of types is different" , size * width Of Elm , w ) part T = Bits ( width Of Elm ) parts = [ p . reinterpret cast ( part T ) for p in sig Or Val ] return Concat ( * reversed ( parts ) ) . reinterpret cast ( bits T )
def slice to SLICE ( slice Vals , width ) : if slice Vals . step is not None : raise Not Implemented Error ( ) start = slice Vals . start stop = slice Vals . stop if slice Vals . start is None : start = INT . from Py ( width ) else : start = to H Val ( slice Vals . start ) if slice Vals . stop is None : stop = INT . from Py ( 0 ) else : stop = to H Val ( slice Vals . stop ) start Is Val = isinstance ( start , Value ) stop Is Val = isinstance ( stop , Value ) indexes Are Values = start Is Val and stop Is Val if indexes Are Values : update Time = max ( start . update Time , stop . update Time ) else : update Time = - 1 return Slice . get Value Cls ( ) ( ( start , stop ) , SLICE , 1 , update Time )
def find files ( directory , pattern , recursive = True ) : if not os . path . isdir ( directory ) : if os . path . exists ( directory ) : raise IO Error ( directory + ' is not directory' ) else : raise IO Error ( directory + " does not exists" ) if recursive : for root , , files in os . walk ( directory ) : for basename in files : if fnmatch . fnmatch ( basename , pattern ) : filename = os . path . join ( root , basename ) yield filename else : root = directory for basename in os . listdir ( root ) : if fnmatch . fnmatch ( basename , pattern ) : filename = os . path . join ( root , basename ) if os . path . isfile ( filename ) : yield filename
def is Pow2 ( num ) -> bool : if not isinstance ( num , int ) : num = int ( num ) return num != 0 and ( ( num & ( num - 1 ) ) == 0 )
def Case ( self , case Val , * statements ) : assert self . parent Stm is None case Val = to H Val ( case Val , self . switch On . dtype ) assert isinstance ( case Val , Value ) , case Val assert case Val . is Full Vld ( ) , "Cmp with invalid value" assert case Val not in self . case value index , ( "Switch statement already has case for value " , case Val ) self . rank += 1 case = [ ] self . case value index [ case Val ] = len ( self . cases ) self . cases . append ( ( case Val , case ) ) cond = self . switch On . eq ( case Val ) self . inputs . append ( cond ) cond . endpoints . append ( self ) self . register stements ( statements , case ) return self
def Default ( self , * statements ) : assert self . parent Stm is None self . rank += 1 self . default = [ ] self . register stements ( statements , self . default ) return self
def vcd Register Interfaces ( self , obj : Union [ Interface , Unit ] , parent : Optional [ Vcd Var Writing Scope ] ) : if hasattr ( obj , " interfaces" ) and obj . interfaces : name = obj . name parent = self . vcd Writer if parent is None else parent sub Scope = parent . var Scope ( name ) self . obj2scope [ obj ] = sub Scope with sub Scope : # register all subinterfaces for ch Intf in obj . interfaces : self . vcd Register Interfaces ( ch Intf , sub Scope ) if isinstance ( obj , ( Unit , Sim Model ) ) : # register interfaces from all subunits for u in obj . units : self . vcd Register Interfaces ( u , sub Scope ) return sub Scope else : t = obj . dtype if isinstance ( t , self . supported type classes ) : t Name , width , formatter = vcd Type Info For H Type ( t ) try : parent . add Var ( obj , get Signal Name ( obj ) , t Name , width , formatter ) except Var Already Registered : pass
def before Sim ( self , simulator , synthesised Unit ) : vcd = self . vcd Writer vcd . date ( datetime . now ( ) ) vcd . timescale ( 1 ) self . vcd Register Interfaces ( synthesised Unit , None ) self . vcd Register Remaining Signals ( synthesised Unit ) vcd . enddefinitions ( )
def log Change ( self , now Time , sig , next Val ) : try : self . vcd Writer . log Change ( now Time , sig , next Val ) except Key Error : # not every signal has to be registered pass
def distinct By ( iterable , fn ) : s = set ( ) for i in iterable : r = fn ( i ) if r not in s : s . add ( r ) yield i
def remove Unconnected Signals ( netlist ) : to Delete = set ( ) to Search = netlist . signals while to Search : to Search = set ( ) for sig in to Search : if not sig . endpoints : try : if sig . interface is not None : # skip interfaces before we want to check them, # they should not be optimized out from design continue except Attribute Error : pass for e in sig . drivers : # drivers of this signal are useless rm them if isinstance ( e , Operator ) : inputs = e . operands if e . result is sig : e . result = None else : inputs = e . inputs netlist . statements . discard ( e ) for op in inputs : if not isinstance ( op , Value ) : try : op . endpoints . remove ( e ) except Key Error : # this operator has 2x+ same operand continue to Search . add ( op ) to Delete . add ( sig ) if to Delete : for sig in to Delete : if sig . ctx == netlist : netlist . signals . remove ( sig ) to Search . discard ( sig ) to Delete = set ( ) to Search = to Search
def on Write Req ( self , sim , addr , data ) : self . requests . append ( ( WRITE , addr , data ) )
def name for process and mark outputs ( statements : List [ Hdl Statement ] ) -> str : out names = [ ] for stm in statements : for sig in stm . outputs : if not sig . has Generic Name : out names . append ( sig . name ) if out names : return min ( out names ) else : return ""
def cut off drivers of ( dst Signal , statements ) : separated = [ ] stm filter = [ ] for stm in statements : stm . clean signal meta ( ) d = stm . cut off drivers of ( dst Signal ) if d is not None : separated . append ( d ) f = d is not stm stm filter . append ( f ) return list ( compress ( statements , stm filter ) ) , separated
def synthesize ( self , name , interfaces , target Platform ) : ent = Entity ( name ) ent . name = name + " inst" # instance name # create generics for , v in self . params . items ( ) : ent . generics . append ( v ) # interface set for faster lookup if isinstance ( interfaces , set ) : intf Set = interfaces else : intf Set = set ( interfaces ) # create ports for s in interfaces : pi = port Itemfrom Signal ( s , ent ) pi . register Intern Sig ( s ) ent . ports . append ( pi ) s . hidden = False remove Unconnected Signals ( self ) mark Visibility Of Signals ( self , name , self . signals , intf Set ) for proc in target Platform . before Hdl Arch Generation : proc ( self ) arch = Architecture ( ent ) for p in statements to HW Processes ( self . statements ) : arch . processes . append ( p ) # add signals, variables etc. in architecture for s in self . signals : if s not in intf Set and not s . hidden : arch . variables . append ( s ) # instantiate sub Units in architecture for u in self . sub Units : arch . component Instances . append ( u ) # add components in architecture for su in distinct By ( self . sub Units , lambda x : x . name ) : arch . components . append ( su ) self . synthesised = True return [ ent , arch ]
def get Max Stm Id For Stm ( stm ) : max Id = 0 if isinstance ( stm , Assignment ) : return stm . inst Id elif isinstance ( stm , Wait Stm ) : return max Id else : for stm in stm . iter stms ( ) : max Id = max ( max Id , get Max Stm Id For Stm ( stm ) ) return max Id
def monitor ( self , sim ) : if self . not Reset ( sim ) and self . enabled : self . wr Rd ( sim . write , 1 ) yield sim . wait On Comb Update ( ) d = self . do Read ( sim ) self . data . append ( d ) else : self . wr Rd ( sim . write , 0 )
def do Write ( self , sim , data ) : sim . write ( data , self . intf . data )
def driver ( self , sim ) : r = sim . read if self . actual Data is NOP and self . data : self . actual Data = self . data . popleft ( ) do = self . actual Data is not NOP if do : self . do Write ( sim , self . actual Data ) else : self . do Write ( sim , None ) en = self . not Reset ( sim ) and self . enabled if not ( en and do ) : return yield sim . wait On Comb Update ( ) rd = self . is Rd ( r ) if en : assert rd . vld Mask , ( ( "%r: ready signal for interface %r is in invalid state," " this would cause desynchronization" ) % ( sim . now , self . intf ) ) if rd . val : if self . debug Output is not None : self . debug Output . write ( "%s, wrote, %d: %r\n" % ( self . intf . get Full Name ( ) , sim . now , self . actual Data ) ) if self . data : self . actual Data = self . data . popleft ( ) else : self . actual Data = NOP
def get Physical Name ( self ) : if hasattr ( self , " bounded Entity Port" ) : return self . bounded Entity Port . name else : return self . get Full Name ( ) . replace ( '.' , self . NAME SEPARATOR )
def bit length ( self ) : try : interfaces = self . interfaces except Attribute Error : interfaces = None if interfaces is None : # not loaded interface intf = self . clone ( ) intf . load Declarations ( ) interfaces = intf . interfaces if interfaces : w = 0 for i in interfaces : w += i . bit length ( ) return w else : return self . dtype . bit length ( )
def sensitivity By Op ( op ) : if op == All Ops . RISING EDGE : return SENSITIVITY . RISING elif op == All Ops . FALLING EDGE : return SENSITIVITY . FALLING else : raise Type Error ( )
def eval ( self , operator , simulator = None ) : def get Val ( v ) : while not isinstance ( v , Value ) : v = v . val return v operands = list ( map ( get Val , operator . operands ) ) if is Event Dependent Op ( operator . operator ) : operands . append ( simulator . now ) elif operator . operator == All Ops . Int To Bits : operands . append ( operator . result . dtype ) return self . eval Fn ( * operands )
def convert Bits ( self , sig Or Val , to Type ) : if isinstance ( sig Or Val , Value ) : return convert Bits val ( self , sig Or Val , to Type ) elif isinstance ( to Type , H Bool ) : if self . bit length ( ) == 1 : v = 0 if sig Or Val . dtype . negated else 1 return sig Or Val . eq ( self . get Value Cls ( ) . from Py ( v , self ) ) elif isinstance ( to Type , Bits ) : if self . bit length ( ) == to Type . bit length ( ) : return sig Or Val . conv Sign ( to Type . signed ) elif to Type == INT : return Operator . with Res ( All Ops . Bits To Int , [ sig Or Val ] , to Type ) return default auto cast fn ( self , sig Or Val , to Type )
def reinterpret bits to hstruct ( sig Or Val , h Struct T ) : container = h Struct T . from Py ( None ) offset = 0 for f in h Struct T . fields : t = f . dtype width = t . bit length ( ) if f . name is not None : s = sig Or Val [ ( width + offset ) : offset ] s = s . reinterpret cast ( t ) setattr ( container , f . name , s ) offset += width return container
def full Word Cnt ( self , start : int , end : int ) : assert end >= start , ( start , end ) gap = max ( 0 , ( end - start ) - ( start % self . word Width ) ) return gap // self . word Width
def discover sensitivity seq ( self , signals : List [ Rtl Signal Base ] , seen : set , ctx : Sensitivity Ctx ) -> None : casual Sensitivity = set ( ) for s in signals : s . walk sensitivity ( casual Sensitivity , seen , ctx ) if ctx . contains ev dependency : break # if event dependent sensitivity found do not add other sensitivity if not ctx . contains ev dependency : ctx . extend ( casual Sensitivity )
def get rtl context ( self ) : for sig in chain ( self . inputs , self . outputs ) : if sig . ctx : return sig . ctx else : # Param instances does not have context continue raise Hwt Syntax Error ( "Statement does not have any signal in any context" , self )
def is mergable statement list ( cls , stms A , stms B ) : if stms A is None and stms B is None : return True elif stms A is None or stms B is None : return False a it = iter ( stms A ) b it = iter ( stms B ) a = get stm with branches ( a it ) b = get stm with branches ( b it ) while a is not None or b is not None : if a is None or b is None or not a . is mergable ( b ) : return False a = get stm with branches ( a it ) b = get stm with branches ( b it ) # lists are empty return True
def try reduce list ( statements : List [ "Hdl Statement" ] ) : io change = False new statements = [ ] for stm in statements : reduced , io change = stm . try reduce ( ) new statements . extend ( reduced ) io change |= io change new statements , rank decrease = Hdl Statement . merge statements ( new statements ) return new statements , rank decrease , io change
def set parent stm ( self , parent Stm : "Hdl Statement" ) : was top = self . parent Stm is None self . parent Stm = parent Stm if not self . now is event dependent and parent Stm . now is event dependent : self . on parent event dependent ( ) top Statement = parent Stm while top Statement . parent Stm is not None : top Statement = top Statement . parent Stm parent out add = top Statement . outputs . append parent in add = top Statement . inputs . append if was top : for inp in self . inputs : inp . endpoints . discard ( self ) inp . endpoints . append ( top Statement ) parent in add ( inp ) for outp in self . outputs : outp . drivers . discard ( self ) outp . drivers . append ( top Statement ) parent out add ( outp ) ctx = self . get rtl context ( ) ctx . statements . discard ( self ) parent Stm . rank += self . rank
def sig ( self , name , dtype = BIT , def Val = None ) : if isinstance ( dtype , H Struct ) : if def Val is not None : raise Not Implemented Error ( ) container = dtype . from Py ( None ) for f in dtype . fields : if f . name is not None : r = self . sig ( "%s %s" % ( name , f . name ) , f . dtype ) setattr ( container , f . name , r ) return container return self . ctx . sig ( name , dtype = dtype , def Val = def Val )
def clean As Subunit ( self ) : for pi in self . entity . ports : pi . connect Intern Sig ( ) for i in chain ( self . interfaces , self . private interfaces ) : i . clean ( )
def walk Flatten Fields ( sig Or Val , skip Padding = True ) : t = sig Or Val . dtype if isinstance ( t , Bits ) : yield sig Or Val elif isinstance ( t , H Union ) : yield from walk Flatten Fields ( sig Or Val . val , skip Padding = skip Padding ) elif isinstance ( t , H Struct ) : for f in t . fields : is Padding = f . name is None if not is Padding or not skip Padding : if is Padding : v = f . dtype . from Py ( None ) else : v = getattr ( sig Or Val , f . name ) yield from walk Flatten Fields ( v ) elif isinstance ( t , H Array ) : for item in sig Or Val : yield from walk Flatten Fields ( item ) else : raise Not Implemented Error ( t )
def sensitivity ( proc : HW Process , * sensitive To ) : for s in sensitive To : if isinstance ( s , tuple ) : sen , s = s if sen == SENSITIVITY . ANY : s . sim Sens Procs . add ( proc ) elif sen == SENSITIVITY . RISING : s . sim Rising Sens Procs . add ( proc ) elif sen == SENSITIVITY . FALLING : s . sim Falling Sens Procs . add ( proc ) else : raise Assertion Error ( sen ) else : s . sim Sens Procs . add ( proc )
def sim Eval Cond ( simulator , * conds ) : cond = True vld = True for v in conds : val = bool ( v . val ) full Vld = v . vld Mask == 1 if full Vld : if not val : return False , True else : return False , False cond = cond and val vld = vld and full Vld return cond , vld
def connect Sim Port ( sim Unit , sub Sim Unit , src Name , dst Name , direction ) : if direction == DIRECTION . OUT : orig Port = getattr ( sub Sim Unit , src Name ) new Port = getattr ( sim Unit , dst Name ) setattr ( sub Sim Unit , src Name , new Port ) else : orig Port = getattr ( sub Sim Unit , dst Name ) new Port = getattr ( sim Unit , src Name ) setattr ( sub Sim Unit , dst Name , new Port ) sub Sim Unit . ctx . signals . remove ( orig Port )
def vec ( val , width , signed = None ) : return Bits ( width , signed , force Vector = True ) . from Py ( val )
def monitor ( self , sim ) : r = sim . read if self . not Reset ( sim ) : # update rd signal only if required if self . last Rd is not 1 : self . wr Rd ( sim . write , 1 ) self . last Rd = 1 # try to run on Monitor Ready if there is any try : on Monitor Ready = self . on Monitor Ready except Attribute Error : on Monitor Ready = None if on Monitor Ready is not None : on Monitor Ready ( sim ) # wait for response of master yield sim . wait On Comb Update ( ) vld = self . is Vld ( r ) assert vld . vld Mask , ( sim . now , self . intf , "vld signal is in invalid state" ) if vld . val : # master responded with positive ack, do read data d = self . do Read ( sim ) if self . debug Output is not None : self . debug Output . write ( "%s, read, %d: %r\n" % ( self . intf . get Full Name ( ) , sim . now , d ) ) self . data . append ( d ) if self . after Read is not None : self . after Read ( sim ) else : if self . last Rd is not 0 : # can not receive, say it to masters self . wr Rd ( sim . write , 0 ) self . last Rd = 0
def HW Process ( cls , proc : HW Process , ctx : Resource Context ) -> None : seen = ctx . seen for stm in proc . statements : encl = stm . enclosed for full ev dep = stm . is completly event dependent now ev dep = stm . now is event dependent ev dep = full ev dep or now ev dep out mux dim = count mux inputs for outputs ( stm ) for o in stm . outputs : if o in seen : continue i = out mux dim [ o ] if isinstance ( o . dtype , H Array ) : assert i == 1 , ( o , i , " only one ram port per HW Process" ) for a in walk assignments ( stm , o ) : assert len ( a . indexes ) == 1 , "one address per RAM port" addr = a . indexes [ 0 ] ctx . register RAM write port ( o , addr , ev dep ) elif ev dep : ctx . register FF ( o ) if i > 1 : ctx . register MUX ( stm , o , i ) elif o not in encl : ctx . register Latch ( o ) if i > 1 : ctx . register MUX ( stm , o , i ) elif i > 1 : ctx . register MUX ( stm , o , i ) else : # just a connection continue if isinstance ( stm , Switch Container ) : case Eqs = set ( [ stm . switch On . eq ( c [ 0 ] ) for c in stm . cases ] ) inputs = chain ( [ sig for sig in stm . inputs if sig not in case Eqs ] , [ stm . switch On ] ) else : inputs = stm . inputs for i in inputs : # discover only internal signals in this statements for # operators if not i . hidden or i in seen : continue cls . HW Process operators ( i , ctx , ev dep )
def eval Param ( p ) : while isinstance ( p , Param ) : p = p . get ( ) if isinstance ( p , Rtl Signal Base ) : return p . static Eval ( ) # use rather param inheritance instead of param as param value return to H Val ( p )
def set ( self , val ) : assert not self . is Read Only , ( "This parameter(%s) was locked" " and now it can not be changed" % self . name ) assert self . replaced With is None , ( "This param was replaced with new one and this " "should not exists" ) val = to H Val ( val ) self . def Val = val self . val = val . static Eval ( ) self . dtype = self . val . dtype
def finalize ( self ) : ff to remove = 0 res = self . resources for m , addr Dict in self . memories . items ( ) : rw Sync Ports , r Sync Ports , w Sync Ports = 0 , 0 , 0 rw Async Ports , r Async Ports , w Async Ports = 0 , 0 , 0 r Sync w Async Ports , r Async w Sync Ports = 0 , 0 for , ( r Sync , w Sync , r Async , w Async ) in addr Dict . items ( ) : if r Sync : ff to remove += r Sync * m . dtype . elm Type . bit length ( ) # resolve port count for this addr signal rw Sync = min ( r Sync , w Sync ) r Sync -= rw Sync w Sync -= rw Sync rw Async = min ( r Async , w Async ) r Async -= rw Async w Async -= rw Async r Sync w Async = min ( r Sync , w Async ) r Sync -= r Sync w Async w Async -= r Sync w Async r Async w Sync = min ( r Async , w Sync ) r Async -= r Async w Sync w Sync -= r Async w Sync # update port counts for mem rw Sync Ports += rw Sync r Sync Ports += r Sync w Sync Ports += w Sync rw Async Ports += rw Async r Async Ports += r Async w Async Ports += w Async r Sync w Async Ports += r Sync w Async r Async w Sync Ports += r Async w Sync k = Resource RAM ( m . dtype . elm Type . bit length ( ) , int ( m . dtype . size ) , rw Sync Ports , r Sync Ports , w Sync Ports , r Sync w Async Ports , rw Async Ports , r Async Ports , w Async Ports , r Async w Sync Ports ) res [ k ] = res . get ( k , 0 ) + 1 self . memories . clear ( ) # remove register on read ports which will be merged into ram if ff to remove : ff cnt = res [ Resource FF ] ff cnt -= ff to remove if ff cnt : res [ Resource FF ] = ff cnt else : del res [ Resource FF ]
def eq ( self , other ) : return self . nary Op ( All Ops . EQ , tv ( self ) . eq , other )
def get Index Cascade ( self ) : try : # now I am result of the index  xxx[xx] <= source # get index op d = self . single Driver ( ) try : op = d . operator except Attribute Error : return if op == All Ops . INDEX : # get signal on which is index applied indexed On = d . operands [ 0 ] if isinstance ( indexed On , Rtl Signal Base ) : # [TODO] multidimensional indexing return indexed On , [ d . operands [ 1 ] ] else : raise Exception ( "can not drive static value %r" % indexed On ) except ( Multiple Drivers Err , No Driver Err ) : pass
def walk Params ( intf , discovered ) : for si in intf . interfaces : yield from walk Params ( si , discovered ) for p in intf . params : if p not in discovered : discovered . add ( p ) yield p
def register Intf In Impl ( self , i Name , intf ) : self . register Interface ( i Name , intf , is Private = True ) self . load Interface ( intf , False ) intf . signals For Interface ( self . ctx )
def get Base Name Scope ( cls ) : s = Name Scope ( False ) s . set Level ( 1 ) s [ 0 ] . update ( cls . keywords dict ) return s
def get Base Cond ( c ) : is Negated = False try : drivers = c . drivers except Attribute Error : return ( c , is Negated ) if len ( drivers ) == 1 : d = list ( c . drivers ) [ 0 ] if isinstance ( d , Operator ) and d . operator == All Ops . NOT : c = d . operands [ 0 ] is Negated = True return ( c , is Negated )
def sim Bits T ( width : int , signed : Union [ bool , None ] ) : k = ( width , signed ) try : return sim Bits T Cache [ k ] except Key Error : t = Sim Bits T ( width , signed ) sim Bits T Cache [ k ] = t return t
def cut off drivers of ( self , sig : Rtl Signal Base ) : if self . dst is sig : self . parent Stm = None return self else : return None
def load From H Type ( self , dtype : Hdl Type , bit Addr : int ) -> None : self . bit Addr = bit Addr children Are Choice = False if isinstance ( dtype , Bits ) : ld = self . load From Bits elif isinstance ( dtype , H Struct ) : ld = self . load From H Struct elif isinstance ( dtype , H Array ) : ld = self . load From Array elif isinstance ( dtype , H Stream ) : ld = self . load From H Stream elif isinstance ( dtype , H Union ) : ld = self . load From Union children Are Choice = True else : raise Type Error ( "expected instance of Hdl Type" , dtype ) self . bit Addr End = ld ( dtype , bit Addr ) self . children Are Choice = children Are Choice
def sign Fix ( val , width ) : if val > 0 : msb = 1 << ( width - 1 ) if val & msb : val -= mask ( width ) + 1 return val
def merge with other stm ( self , other : "If Container" ) -> None : merge = self . merge statement lists new Cases = [ ] for ( c , case A ) , ( , case B ) in zip ( self . cases , other . cases ) : new Cases . append ( ( c , merge ( case A , case B ) ) ) self . cases = new Cases if self . default is not None : self . default = merge ( self . default , other . default ) self . on merge ( other )
def get Indent ( indent Num ) : try : return indent Cache [ indent Num ] except Key Error : i = "" . join ( [ indent for in range ( indent Num ) ] ) indent Cache [ indent Num ] = i return i
def verilog Type Of Sig ( signal Item ) : driver cnt = len ( signal Item . drivers ) if signal Item . const or driver cnt > 1 or arr any ( signal Item . drivers , is Event Dependent Driver ) : return SIGNAL TYPE . REG else : if driver cnt == 1 : d = signal Item . drivers [ 0 ] if not isinstance ( d , ( Assignment , Port Item ) ) : return SIGNAL TYPE . REG return SIGNAL TYPE . WIRE
def name Availability Check ( obj , prop Name , prop ) : if getattr ( obj , prop Name , None ) is not None : raise Intf Lvl Conf Err ( "%r already has property %s old:%s new:%s" % ( obj , prop Name , repr ( getattr ( obj , prop Name ) ) , prop ) )
def register Parameter ( self , p Name , parameter ) -> None : name Availability Check ( self , p Name , parameter ) # resolve name in this scope try : has Name = parameter . name is not None except Attribute Error : has Name = False if not has Name : parameter . name = p Name # add name in this scope parameter . register Scope ( p Name , self ) if parameter . has Generic Name : parameter . name = p Name if parameter . parent is None : parameter . parent = self self . params . append ( parameter )
def register Unit ( self , u Name , unit ) : name Availability Check ( self , u Name , unit ) assert unit . parent is None unit . parent = self unit . name = u Name self . units . append ( unit )
def register Interface ( self , i Name , intf , is Private = False ) : name Availability Check ( self , i Name , intf ) assert intf . parent is None intf . parent = self intf . name = i Name intf . ctx = self . ctx if is Private : self . private interfaces . append ( intf ) intf . is Extern = False else : self . interfaces . append ( intf ) intf . is Extern = True
def register Array ( self , name , items ) : items . parent = self items . name = name for i , item in enumerate ( items ) : setattr ( self , "%s %d" % ( name , i ) , item )
def single Driver ( self ) : # [TODO] no driver exception drv cnt = len ( self . drivers ) if not drv cnt : raise No Driver Err ( self ) elif drv cnt != 1 : raise Multiple Drivers Err ( self ) return self . drivers [ 0 ]
def static Eval ( self ) : for o in self . operands : o . static Eval ( ) self . result . val = self . eval Fn ( )
def with Indent ( self , indent = 1 ) : ctx = copy ( self ) ctx . indent += indent return ctx
def propagate Clk ( obj ) : clk = obj . clk for u in obj . units : try Connect ( clk , u , 'clk' )
def propagate Clk Rst ( obj ) : clk = obj . clk rst = obj . rst for u in obj . units : try Connect ( clk , u , 'clk' ) try Connect ( ~ rst , u , 'rst n' ) try Connect ( rst , u , 'rst' )
def get Full Name ( self ) : name = "" tmp = self while isinstance ( tmp , ( Interface Base , H Obj List ) ) : if hasattr ( tmp , " name" ) : n = tmp . name else : n = '' if name == '' : name = n else : name = n + '.' + name if hasattr ( tmp , " parent" ) : tmp = tmp . parent else : tmp = None return name
def on T Write Callback init ( self , sim ) : yield from self . on T Write Callback ( sim ) self . intf . t . sig Inside . register Write Callback ( self . on T Write Callback , self . get Enable ) self . intf . o . sig Inside . register Write Callback ( self . on T Write Callback , self . get Enable )
def connect Sig ( self , signal ) : if self . direction == DIRECTION . IN : if self . src is not None : raise Hwt Syntax Error ( "Port %s is already associated with %r" % ( self . name , self . src ) ) self . src = signal signal . endpoints . append ( self ) elif self . direction == DIRECTION . OUT : if self . dst is not None : raise Hwt Syntax Error ( "Port %s is already associated with %r" % ( self . name , self . dst ) ) self . dst = signal signal . drivers . append ( self ) else : raise Not Implemented Error ( self ) signal . hidden = False signal . ctx . sub Units . add ( self . unit )
def connect Intern Sig ( self ) : d = self . direction if d == DIRECTION . OUT : self . src . endpoints . append ( self ) elif d == DIRECTION . IN or d == DIRECTION . INOUT : self . dst . drivers . append ( self ) else : raise Not Implemented Error ( d )
def get Intern Sig ( self ) : d = self . direction if d == DIRECTION . IN : return self . dst elif d == DIRECTION . OUT : return self . src else : raise Not Implemented Error ( d )
def is Ev Dependent On ( sig , process ) -> bool : if sig is None : return False return process in sig . sim Falling Sens Procs or process in sig . sim Rising Sens Procs
def add process ( self , proc , priority ) -> None : self . events . push ( self . now , priority , proc )
def schedule Apply Values ( self ) -> None : assert not self . apply Val Planed , self . now self . add process ( self . apply Values ( ) , PRIORITY APPLY COMB ) self . apply Val Planed = True if self . run Seq Processes Planed : # if run Seq Processes is already scheduled return assert not self . seq Procs To Run and not self . run Seq Processes Planed , self . now self . add process ( self . run Seq Processes ( ) , PRIORITY APPLY SEQ ) self . run Seq Processes Planed = True
def run Comb Processes ( self ) -> None : for proc in self . comb Procs To Run : cont = self . output Containers [ proc ] proc ( self , cont ) for sig Name , sig in cont . all signals : new Val = getattr ( cont , sig Name ) if new Val is not None : res = self . conflict Resolve Strategy ( new Val ) # prepare update updater , is Ev Dependent = res self . values To Apply . append ( ( sig , updater , is Ev Dependent , proc ) ) setattr ( cont , sig Name , None ) # else value is latched self . comb Procs To Run = Uniq List ( )
def run Seq Processes ( self ) -> Generator [ None , None , None ] : updates = [ ] for proc in self . seq Procs To Run : try : out Container = self . output Containers [ proc ] except Key Error : # processes does not have to have outputs out Container = None proc ( self , out Container ) if out Container is not None : updates . append ( out Container ) self . seq Procs To Run = Uniq List ( ) self . run Seq Processes Planed = False for cont in updates : for sig Name , sig in cont . all signals : new Val = getattr ( cont , sig Name ) if new Val is not None : v = self . conflict Resolve Strategy ( new Val ) updater , = v sig . sim Update Val ( self , updater ) setattr ( cont , sig Name , None ) return yield
def apply Values ( self ) -> Generator [ None , None , None ] : va = self . values To Apply self . apply Val Planed = False # log if there are items to log lav = self . config . log Applying Values if va and lav : lav ( self , va ) self . values To Apply = [ ] # apply values to signals, values can overwrite each other # but each signal should be driven by only one process and # it should resolve value collision add Sp = self . seq Procs To Run . append for s , v Updater , is Event Dependent , comes From in va : if is Event Dependent : # now=0 and this was process initialization or async reg add Sp ( comes From ) else : # regular combinational process s . sim Update Val ( self , v Updater ) self . run Comb Processes ( ) # processes triggered from sim Update Val can add new values if self . values To Apply and not self . apply Val Planed : self . schedule Apply Values ( ) return yield
def read ( self , sig ) -> Value : try : v = sig . val except Attribute Error : v = sig . sig Inside . val return v . clone ( )
def write ( self , val , sig : Sim Signal ) -> None : # get target Rtl Signal try : sim Sens Procs = sig . sim Sens Procs except Attribute Error : sig = sig . sig Inside sim Sens Procs = sig . sim Sens Procs # type cast of input value t = sig . dtype if isinstance ( val , Value ) : v = val . clone ( ) v = v . auto cast ( t ) else : v = t . from Py ( val ) # can not update value in signal directly due singnal proxies sig . sim Update Val ( self , lambda curent V : ( value Has Changed ( curent V , v ) , v ) ) if not self . apply Val Planed : if not ( sim Sens Procs or sig . sim Rising Sens Procs or sig . sim Falling Sens Procs ) : # signal value was changed but there are no sensitive processes # to it because of this  apply Values is never planed # and should be self . schedule Apply Values ( ) elif ( sig . write Callbacks or sig . write Callbacks To En ) : # signal write did not caused any change on any other signal # but there are still simulation agets waiting on # update Complete event self . schedule Apply Values ( )
def add process ( self , proc ) -> None : self . events . push ( self . now , PRIORITY NORMAL , proc )
def sim Unit ( self , synthesised Unit : Unit , until : float , extra Processes = [ ] ) : before Sim = self . config . before Sim if before Sim is not None : before Sim ( self , synthesised Unit ) add proc = self . add process for p in extra Processes : add proc ( p ( self ) ) self . init Unit Signals ( synthesised Unit ) self . run ( until )
def system C Type Of Sig ( signal Item ) : if signal Item . const or arr any ( signal Item . drivers , lambda d : isinstance ( d , Hdl Statement ) and d . now is event dependent ) : return SIGNAL TYPE . REG else : return SIGNAL TYPE . WIRE
def ternary Ops To If ( statements ) : stms = [ ] for st in statements : if isinstance ( st , Assignment ) : try : if not isinstance ( st . src , Rtl Signal Base ) : raise Does Not Contains Ternary ( ) d = st . src . single Driver ( ) if not isinstance ( d , Operator ) or d . operator != All Ops . TERNARY : raise Does Not Contains Ternary ( ) else : ops = d . operands ifc = If Container ( ops [ 0 ] , [ Assignment ( ops [ 1 ] , st . dst ) ] , [ Assignment ( ops [ 2 ] , st . dst ) ] ) stms . append ( ifc ) continue except ( Multiple Drivers Err , Does Not Contains Ternary ) : pass except No Driver Err : assert ( hasattr ( st . src , " interface" ) and st . src . interface is not None ) or st . src . def Val . vld Mask , st . src stms . append ( st ) return stms
def hash distance ( left hash , right hash ) : if len ( left hash ) != len ( right hash ) : raise Value Error ( 'Hamming distance requires two strings of equal length' ) return sum ( map ( lambda x : 0 if x [ 0 ] == x [ 1 ] else 1 , zip ( left hash , right hash ) ) )
def average hash ( image path , hash size = 8 ) : with open ( image path , 'rb' ) as f : # Open the image, resize it and convert it to black & white. image = Image . open ( f ) . resize ( ( hash size , hash size ) , Image . ANTIALIAS ) . convert ( 'L' ) pixels = list ( image . getdata ( ) ) avg = sum ( pixels ) / len ( pixels ) # Compute the hash based on each pixels value compared to the average. bits = "" . join ( map ( lambda pixel : '1' if pixel > avg else '0' , pixels ) ) hashformat = "0{hashlength}x" . format ( hashlength = hash size ** 2 // 4 ) return int ( bits , 2 ) . format ( hashformat )
def distance ( image path , other image path ) : image hash = average hash ( image path ) other image hash = average hash ( other image path ) return hash distance ( image hash , other image hash )
def setup platform ( hass , config , add entities , discovery info = None ) : host = config . get ( CONF HOST ) token = config . get ( CONF ACCESS TOKEN ) name = config . get ( CONF NAME ) volume step = config . get ( CONF VOLUME STEP ) device type = config . get ( CONF DEVICE CLASS ) device = Vizio Device ( host , token , name , volume step , device type ) if device . validate setup ( ) is False : LOGGER . error ( "Failed to set up Vizio platform, " "please check if host and API key are correct" ) return elif ( token is None or token == "" ) and device type == "tv" : LOGGER . error ( "Failed to set up Vizio platform, " "if device class is 'tv' then an auth token needs " "to be provided, otherwise if device class is " "'soundbar' then add the right device class to config" ) return if config . get ( CONF SUPPRESS WARNING ) : from requests . packages import urllib3 LOGGER . warning ( "Insecure Request Warning is disabled " "because of Vizio platform configuration" ) urllib3 . disable warnings ( urllib3 . exceptions . Insecure Request Warning ) add entities ( [ device ] , True )
def update ( self ) : is on = self . device . get power state ( ) if is on : self . state = STATE ON volume = self . device . get current volume ( ) if volume is not None : self . volume level = float ( volume ) / self . max volume input = self . device . get current input ( ) if input is not None : self . current input = input . meta name inputs = self . device . get inputs ( ) if inputs is not None : self . available inputs = [ input . name for input in inputs ] else : if is on is None : self . state = None else : self . state = STATE OFF self . volume level = None self . current input = None self . available inputs = None
def mute volume ( self , mute ) : if mute : self . device . mute on ( ) else : self . device . mute off ( )
def volume up ( self ) : self . volume level += self . volume step / self . max volume self . device . vol up ( num = self . volume step )
def volume down ( self ) : self . volume level -= self . volume step / self . max volume self . device . vol down ( num = self . volume step )
def set volume level ( self , volume ) : if self . volume level is not None : if volume > self . volume level : num = int ( self . max volume * ( volume - self . volume level ) ) self . volume level = volume self . device . vol up ( num = num ) elif volume < self . volume level : num = int ( self . max volume * ( self . volume level - volume ) ) self . volume level = volume self . device . vol down ( num = num )
def reset ( self ) : self . piece bb = [ BB VOID , # NONE BB RANK C | BB RANK G , # PAWN BB A1 | BB I1 | BB A9 | BB I9 , # LANCE BB A2 | BB A8 | BB I2 | BB I8 , # KNIGHT BB A3 | BB A7 | BB I3 | BB I7 , # SILVER BB A4 | BB A6 | BB I4 | BB I6 , # GOLD BB B2 | BB H8 , # BISHOP BB B8 | BB H2 , # ROOK BB A5 | BB I5 , # KING BB VOID , # PROM PAWN BB VOID , # PROM LANCE BB VOID , # PROM KNIGHT BB VOID , # PROM SILVER BB VOID , # PROM BISHOP BB VOID , # PROM ROOK ] self . pieces in hand = [ collections . Counter ( ) , collections . Counter ( ) ] self . occupied = Occupied ( BB RANK G | BB H2 | BB H8 | BB RANK I , BB RANK A | BB B2 | BB B8 | BB RANK C ) self . king squares = [ I5 , A5 ] self . pieces = [ NONE for i in SQUARES ] for i in SQUARES : mask = BB SQUARES [ i ] for piece type in PIECE TYPES : if mask & self . piece bb [ piece type ] : self . pieces [ i ] = piece type self . turn = BLACK self . move number = 1 self . captured piece stack = collections . deque ( ) self . move stack = collections . deque ( ) self . incremental zobrist hash = self . board zobrist hash ( DEFAULT RANDOM ARRAY ) self . transpositions = collections . Counter ( ( self . zobrist hash ( ) , ) )
def piece at ( self , square ) : mask = BB SQUARES [ square ] color = int ( bool ( self . occupied [ WHITE ] & mask ) ) piece type = self . piece type at ( square ) if piece type : return Piece ( piece type , color )
def remove piece at ( self , square , into hand = False ) : piece type = self . piece type at ( square ) if piece type == NONE : return if into hand : self . add piece into hand ( piece type , self . turn ) mask = BB SQUARES [ square ] self . piece bb [ piece type ] ^= mask color = int ( bool ( self . occupied [ WHITE ] & mask ) ) self . pieces [ square ] = NONE self . occupied . ixor ( mask , color , square ) # Update incremental zobrist hash. if color == BLACK : piece index = ( piece type - 1 ) * 2 else : piece index = ( piece type - 1 ) * 2 + 1 self . incremental zobrist hash ^= DEFAULT RANDOM ARRAY [ 81 * piece index + 9 * rank index ( square ) + file index ( square ) ]
def set piece at ( self , square , piece , from hand = False , into hand = False ) : if from hand : self . remove piece from hand ( piece . piece type , self . turn ) self . remove piece at ( square , into hand ) self . pieces [ square ] = piece . piece type mask = BB SQUARES [ square ] piece type = piece . piece type self . piece bb [ piece type ] |= mask if piece type == KING : self . king squares [ piece . color ] = square self . occupied . ixor ( mask , piece . color , square ) # Update incremental zorbist hash. if piece . color == BLACK : piece index = ( piece . piece type - 1 ) * 2 else : piece index = ( piece . piece type - 1 ) * 2 + 1 self . incremental zobrist hash ^= DEFAULT RANDOM ARRAY [ 81 * piece index + 9 * rank index ( square ) + file index ( square ) ]
def is checkmate ( self ) : if not self . is check ( ) : return False try : next ( self . generate legal moves ( ) . iter ( ) ) return False except Stop Iteration : return True
def pop ( self ) : move = self . move stack . pop ( ) # Update transposition table. self . transpositions . subtract ( ( self . zobrist hash ( ) , ) ) # Decrement move number. self . move number -= 1 # Restore state. captured piece type = self . captured piece stack . pop ( ) captured piece color = self . turn # On a null move simply swap the turn. if not move : self . turn ^= 1 return move # Restore the source square. piece type = self . piece type at ( move . to square ) if move . promotion : piece type = PIECE PROMOTED . index ( piece type ) if move . from square is None : self . add piece into hand ( piece type , self . turn ^ 1 ) else : self . set piece at ( move . from square , Piece ( piece type , self . turn ^ 1 ) ) # Restore target square. if captured piece type : self . remove piece from hand ( captured piece type , captured piece color ^ 1 ) self . set piece at ( move . to square , Piece ( captured piece type , captured piece color ) ) else : self . remove piece at ( move . to square ) # Swap turn. self . turn ^= 1 return move
def sfen ( self ) : sfen = [ ] empty = 0 # Position part. for square in SQUARES : piece = self . piece at ( square ) if not piece : empty += 1 else : if empty : sfen . append ( str ( empty ) ) empty = 0 sfen . append ( piece . symbol ( ) ) if BB SQUARES [ square ] & BB FILE 1 : if empty : sfen . append ( str ( empty ) ) empty = 0 if square != I1 : sfen . append ( '/' ) sfen . append ( ' ' ) # Side to move. if self . turn == WHITE : sfen . append ( 'w' ) else : sfen . append ( 'b' ) sfen . append ( ' ' ) # Pieces in hand pih len = 0 for color in COLORS : p = self . pieces in hand [ color ] pih len += len ( p ) for piece type in sorted ( p . keys ( ) , reverse = True ) : if p [ piece type ] >= 1 : if p [ piece type ] > 1 : sfen . append ( str ( p [ piece type ] ) ) piece = Piece ( piece type , color ) sfen . append ( piece . symbol ( ) ) if pih len == 0 : sfen . append ( '-' ) sfen . append ( ' ' ) # Move count sfen . append ( str ( self . move number ) ) return '' . join ( sfen )
def zobrist hash ( self , array = None ) : # Hash in the board setup. zobrist hash = self . board zobrist hash ( array ) if array is None : array = DEFAULT RANDOM ARRAY if self . turn == WHITE : zobrist hash ^= array [ 2268 ] # pieces in hand pattern is # 19 * 5 * 5 * 5 * 5 * 3 * 3 = 106875 < pow(2, 17) # just checking black side is okay in normal state i = ( self . pieces in hand [ BLACK ] [ ROOK ] * 35625 + self . pieces in hand [ BLACK ] [ BISHOP ] * 11875 + self . pieces in hand [ BLACK ] [ GOLD ] * 2375 + self . pieces in hand [ BLACK ] [ SILVER ] * 475 + self . pieces in hand [ BLACK ] [ KNIGHT ] * 95 + self . pieces in hand [ BLACK ] [ LANCE ] * 19 + self . pieces in hand [ BLACK ] [ PAWN ] ) bit = bit scan ( i ) while bit != - 1 and bit is not None : zobrist hash ^= array [ 2269 + bit ] bit = bit scan ( i , bit + 1 ) return zobrist hash
def symbol ( self ) : if self . color == BLACK : return PIECE SYMBOLS [ self . piece type ] . upper ( ) else : return PIECE SYMBOLS [ self . piece type ]
def load config from cli ( config : Good Conf , argv : List [ str ] ) -> List [ str ] : # Monkey patch Django's command parser from django . core . management . base import Base Command original parser = Base Command . create parser def patched parser ( self , prog name , subcommand ) : parser = original parser ( self , prog name , subcommand ) argparser add argument ( parser , config ) return parser Base Command . create parser = patched parser try : parser = argparse . Argument Parser ( add help = False ) argparser add argument ( parser , config ) config arg , default args = parser . parse known args ( argv ) config . load ( config arg . config ) yield default args finally : # Put that create parser back where it came from or so help me! Base Command . create parser = original parser
def execute from command line with config ( config : Good Conf , argv : List [ str ] ) : with load config from cli ( config , argv ) as args : from django . core . management import execute from command line execute from command line ( args )
def argparser add argument ( parser : argparse . Argument Parser , config : Good Conf ) : help = "Config file." if config . file env var : help += ( " Can also be configured via the " "environment variable: {}" . format ( config . file env var ) ) if config . default files : help += ( " Defaults to the first file that exists from " "[{}]." . format ( ', ' . join ( config . default files ) ) ) parser . add argument ( '-C' , '--config' , metavar = 'FILE' , help = help )
def load ( self , filename : str = None ) : if filename : self . config file = find file ( filename ) else : if self . file env var and self . file env var in os . environ : self . config file = find file ( os . environ [ self . file env var ] ) if not self . config file : for filename in self . default files : self . config file = find file ( filename , require = False ) if self . config file : break if self . config file : config = load config ( self . config file ) log . info ( "Loading config from %s" , self . config file ) else : config = { } log . info ( "No config file specified. " "Loading with environment variables." ) self . set values ( config )
def generate yaml ( cls , * * override ) : import ruamel . yaml yaml = ruamel . yaml . YAML ( ) yaml str = String IO ( ) yaml . dump ( cls . get initial ( * * override ) , stream = yaml str ) yaml str . seek ( 0 ) dict from yaml = yaml . load ( yaml str ) if cls . doc : dict from yaml . yaml set start comment ( '\n' + cls . doc + '\n\n' ) for k in dict from yaml . keys ( ) : if cls . values [ k ] . help : dict from yaml . yaml set comment before after key ( k , before = '\n' + cls . values [ k ] . help ) yaml str = String IO ( ) yaml . dump ( dict from yaml , yaml str ) yaml str . seek ( 0 ) return yaml str . read ( )
def generate markdown ( cls ) : lines = [ ] if cls . doc : lines . extend ( [ '# {}' . format ( cls . doc ) , '' ] ) for k , v in cls . values . items ( ) : lines . append ( '* **{}**  ' . format ( k ) ) if v . required : lines [ - 1 ] = lines [ - 1 ] + ' REQUIRED   ' if v . help : lines . append ( '  {}  ' . format ( v . help ) ) lines . append ( '  type: `{}`  ' . format ( v . cast as . name ) ) if v . default is not None : lines . append ( '  default: `{}`  ' . format ( v . default ) ) return '\n' . join ( lines )
def cast ( self , val : str ) : try : return getattr ( self , 'cast as {}' . format ( self . cast as . name . lower ( ) ) ) ( val ) except Attribute Error : return self . cast as ( val )
def list dates between ( first date , last date ) : return [ first date + timedelta ( days = n ) for n in range ( 1 + ( last date - first date ) . days ) ]
def parse date ( s ) : try : return datetime . date ( int ( s [ : 4 ] ) , int ( s [ 5 : 7 ] ) , int ( s [ 8 : 10 ] ) ) except Value Error : # other accepted format used in one-day data set return datetime . datetime . strptime ( s , '%d %B %Y' ) . date ( )
def load file ( self , currency file ) : if currency file . startswith ( ( 'http://' , 'https://' ) ) : content = urlopen ( currency file ) . read ( ) else : with open ( currency file , 'rb' ) as f : content = f . read ( ) if currency file . endswith ( '.zip' ) : self . load lines ( get lines from zip ( content ) ) else : self . load lines ( content . decode ( 'utf-8' ) . splitlines ( ) )
def set missing to none ( self , currency ) : rates = self . rates [ currency ] first date , last date = self . bounds [ currency ] for date in list dates between ( first date , last date ) : if date not in rates : rates [ date ] = None if self . verbose : missing = len ( [ r for r in itervalues ( rates ) if r is None ] ) if missing : print ( '{0}: {1} missing rates from {2} to {3} ({4} days)' . format ( currency , missing , first date , last date , 1 + ( last date - first date ) . days ) )
def read record ( self , n ) : self . file . seek ( n * K - K ) return self . file . read ( K )
def write record ( self , n , data ) : self . file . seek ( n * K - K ) return self . file . write ( data )
def comments ( self ) : record numbers = range ( 2 , self . fward ) if not record numbers : return '' data = b'' . join ( self . read record ( n ) [ 0 : 1000 ] for n in record numbers ) try : return data [ : data . find ( b'\4' ) ] . decode ( 'ascii' ) . replace ( '\0' , '\n' ) except Index Error : raise Value Error ( 'DAF file comment area is missing its EOT byte' ) except Unicode Decode Error : raise Value Error ( 'DAF file comment area is not ASCII text' )
def close ( self ) : self . daf . file . close ( ) for segment in self . segments : if hasattr ( segment , ' data' ) : del segment . data self . daf . array = None self . daf . map = None
def describe ( self , verbose = True ) : center = titlecase ( target names . get ( self . center , 'Unknown center' ) ) target = titlecase ( target names . get ( self . target , 'Unknown target' ) ) text = ( '{0.start jd:.2f}..{0.end jd:.2f}  {1} ({0.center})' ' -> {2} ({0.target})' . format ( self , center , target ) ) if verbose : text += ( '\n  frame={0.frame} data type={0.data type} source={1}' . format ( self , self . source . decode ( 'ascii' ) ) ) return text
def compute ( self , tdb , tdb2 = 0.0 ) : for position in self . generate ( tdb , tdb2 ) : return position
def close ( self ) : self . daf . file . close ( ) for segment in self . segments : if hasattr ( segment , ' data' ) : del segment . data
def describe ( self , verbose = True ) : body = titlecase ( target names . get ( self . body , 'Unknown body' ) ) text = ( '{0.start jd:.2f}..{0.end jd:.2f} frame={0.frame}' '  {1} ({0.body})' . format ( self , body ) ) if verbose : text += ( '\n  data type={0.data type} source={1}' . format ( self , self . source . decode ( 'ascii' ) ) ) return text
def load ( self ) : if self . data type == 2 : component count = 3 else : raise Value Error ( 'only binary PCK data type 2 is supported' ) init , intlen , rsize , n = self . daf . read array ( self . end i - 3 , self . end i ) initial epoch = jd ( init ) interval length = intlen / S PER DAY coefficient count = int ( rsize - 2 ) // component count coefficients = self . daf . map array ( self . start i , self . end i - 4 ) coefficients . shape = ( int ( n ) , int ( rsize ) ) coefficients = coefficients [ : , 2 : ] # ignore MID and RADIUS elements coefficients . shape = ( int ( n ) , component count , coefficient count ) coefficients = rollaxis ( coefficients , 1 ) return initial epoch , interval length , coefficients
def visit Bin Op ( self , node ) : if self . within logging statement ( ) and self . within logging argument ( ) : # handle percent format if isinstance ( node . op , Mod ) : self . violations . append ( ( node , PERCENT FORMAT VIOLATION ) ) # handle string concat if isinstance ( node . op , Add ) : self . violations . append ( ( node , STRING CONCAT VIOLATION ) ) super ( Logging Visitor , self ) . generic visit ( node )
def visit Dict ( self , node ) : if self . should check whitelist ( node ) : for key in node . keys : if key . s in self . whitelist or key . s . startswith ( "debug " ) : continue self . violations . append ( ( self . current logging call , WHITELIST VIOLATION . format ( key . s ) ) ) if self . should check extra exception ( node ) : for value in node . values : self . check exception arg ( value ) super ( Logging Visitor , self ) . generic visit ( node )
def visit Joined Str ( self , node ) : if version info >= ( 3 , 6 ) : if self . within logging statement ( ) : if any ( isinstance ( i , Formatted Value ) for i in node . values ) : if self . within logging argument ( ) : self . violations . append ( ( node , FSTRING VIOLATION ) ) super ( Logging Visitor , self ) . generic visit ( node )
def visit keyword ( self , node ) : if self . should check whitelist ( node ) : if node . arg not in self . whitelist and not node . arg . startswith ( "debug " ) : self . violations . append ( ( self . current logging call , WHITELIST VIOLATION . format ( node . arg ) ) ) if self . should check extra exception ( node ) : self . check exception arg ( node . value ) super ( Logging Visitor , self ) . generic visit ( node )
def visit Except Handler ( self , node ) : name = self . get except handler name ( node ) if not name : super ( Logging Visitor , self ) . generic visit ( node ) return self . current except names . append ( name ) super ( Logging Visitor , self ) . generic visit ( node ) self . current except names . pop ( )
def detect logging level ( self , node ) : try : if self . get id attr ( node . func . value ) == "warnings" : return None # NB: We could also look at the argument signature or the target attribute if node . func . attr in LOGGING LEVELS : return node . func . attr except Attribute Error : pass return None
def get except handler name ( self , node ) : name = node . name if not name : return None if version info < ( 3 , ) : return name . id return name
def is bare exception ( self , node ) : return isinstance ( node , Name ) and node . id in self . current except names
def check exc info ( self , node ) : if self . current logging level not in ( 'error' , 'exception' ) : return for kw in node . keywords : if kw . arg == 'exc info' : if self . current logging level == 'error' : violation = ERROR EXC INFO VIOLATION else : violation = REDUNDANT EXC INFO VIOLATION self . violations . append ( ( node , violation ) )
def db file widget ( cls ) : def get link display ( url ) : unquoted = unquote ( url . split ( '%2F' ) [ - 1 ] ) if sys . version info . major == 2 : # python 2 from django . utils . encoding import force unicode unquoted = force unicode ( unquoted ) return escape ( unquoted ) def get template substitution values ( self , value ) : # Used by Django < 1.11 subst = super ( cls , self ) . get template substitution values ( value ) subst [ 'initial' ] = get link display ( value . url ) return subst setattr ( cls , 'get template substitution values' , get template substitution values ) def get context ( self , name , value , attrs ) : context = super ( cls , self ) . get context ( name , value , attrs ) if value and hasattr ( value , 'url' ) : context [ 'widget' ] [ 'display' ] = get link display ( value . url ) return context setattr ( cls , 'get context' , get context ) return cls
def render to response ( self , context , * * response kwargs ) : filename = response kwargs . pop ( 'filename' , None ) cmd options = response kwargs . pop ( 'cmd options' , None ) if issubclass ( self . response class , PDF Template Response ) : if filename is None : filename = self . get filename ( ) if cmd options is None : cmd options = self . get cmd options ( ) return super ( PDF Template View , self ) . render to response ( context = context , filename = filename , show content in browser = self . show content in browser , header template = self . header template , footer template = self . footer template , cmd options = cmd options , cover template = self . cover template , * * response kwargs ) else : return super ( PDF Template View , self ) . render to response ( context = context , * * response kwargs )
def parse file ( self , file path , currency ) -> List [ Price Model ] : # load file # read csv into memory? contents = self . load file ( file path ) prices = [ ] # parse price elements for line in contents : price = self . parse line ( line ) assert isinstance ( price , Price Model ) price . currency = currency prices . append ( price ) return prices
def load file ( self , file path ) -> List [ str ] : content = [ ] content = read lines from file ( file path ) return content
def parse line ( self , line : str ) -> Price Model : line = line . rstrip ( ) parts = line . split ( ',' ) result = Price Model ( ) # symbol result . symbol = self . translate symbol ( parts [ 0 ] ) # value result . value = Decimal ( parts [ 1 ] ) # date date str = parts [ 2 ] date str = date str . replace ( '"' , '' ) date parts = date str . split ( '/' ) year str = date parts [ 2 ] month str = date parts [ 1 ] day str = date parts [ 0 ] logging . debug ( f"parsing {date parts} into date" ) result . datetime = datetime ( int ( year str ) , int ( month str ) , int ( day str ) ) return result
def translate symbol ( self , in symbol : str ) -> str : # read all mappings from the db if not self . symbol maps : self . load symbol maps ( ) # translate the incoming symbol result = self . symbol maps [ in symbol ] if in symbol in self . symbol maps else in symbol return result
def load symbol maps ( self ) : repo = Symbol Map Repository ( self . get session ( ) ) all maps = repo . get all ( ) self . symbol maps = { } for item in all maps : self . symbol maps [ item . in symbol ] = item . out symbol
def get session ( self ) : if not self . session : self . session = dal . get default session ( ) return self . session
def import csv ( filepath : str , currency : str ) : logger . debug ( f"currency = {currency}" ) # auto-convert to uppercase. currency = currency . upper ( ) app = Price Db Application ( ) app . logger = logger app . import prices ( filepath , currency )
def last ( symbol : str ) : app = Price Db Application ( ) # convert to uppercase if symbol : symbol = symbol . upper ( ) # extract namespace sec symbol = Security Symbol ( "" , "" ) sec symbol . parse ( symbol ) latest = app . get latest price ( sec symbol ) assert isinstance ( latest , Price Model ) print ( f"{latest}" ) else : # Show the latest prices available for all securities. latest = app . get latest prices ( ) for price in latest : print ( f"{price}" )
def download ( ctx , help : bool , symbol : str , namespace : str , agent : str , currency : str ) : if help : click . echo ( ctx . get help ( ) ) ctx . exit ( ) app = Price Db Application ( ) app . logger = logger if currency : currency = currency . strip ( ) currency = currency . upper ( ) # Otherwise download the prices for securities listed in the database. app . download prices ( currency = currency , agent = agent , symbol = symbol , namespace = namespace )
def prune ( symbol : str , all : str ) : app = Price Db Application ( ) app . logger = logger count = 0 if symbol is not None : sec symbol = Security Symbol ( "" , "" ) sec symbol . parse ( symbol ) deleted = app . prune ( sec symbol ) if deleted : count = 1 else : count = app . prune all ( ) print ( f"Removed {count} old price entries." )
def get default session ( ) : from . config import Config , Config Keys db path = Config ( ) . get ( Config Keys . price database ) if not db path : raise Value Error ( "Price database not set in the configuration file!" ) return get session ( db path )
def add map ( incoming , outgoing ) : db path = Config ( ) . get ( Config Keys . pricedb path ) session = get session ( db path ) new map = Symbol Map ( ) new map . in symbol = incoming new map . out symbol = outgoing session . add ( new map ) session . commit ( ) click . echo ( "Record saved." )
def list maps ( ) : db path = Config ( ) . get ( Config Keys . price database ) session = get session ( db path ) maps = session . query ( Symbol Map ) . all ( ) for item in maps : click . echo ( item )
def get by id ( self , symbol : str ) -> Symbol Map : return self . query . filter ( Symbol Map . in symbol == symbol ) . first ( )
def read lines from file ( file path : str ) -> List [ str ] : # check if the file exists? with open ( file path ) as csv file : content = csv file . readlines ( ) return content
def map entity ( self , entity : dal . Price ) -> Price Model : if not entity : return None result = Price Model ( ) result . currency = entity . currency # date/time dt string = entity . date format string = "%Y-%m-%d" if entity . time : dt string += f"T{entity.time}" format string += "T%H:%M:%S" price datetime = datetime . strptime ( dt string , format string ) result . datum = Datum ( ) result . datum . from datetime ( price datetime ) assert isinstance ( result . datum , Datum ) #result.namespace = entity.namespace #result.symbol = entity.symbol result . symbol = Security Symbol ( entity . namespace , entity . symbol ) # Value value = Decimal ( entity . value ) / Decimal ( entity . denom ) result . value = Decimal ( value ) return result
def map model ( self , model : Price Model ) -> Price : # assert isinstance(model, Price Model) assert isinstance ( model . symbol , Security Symbol ) assert isinstance ( model . datum , Datum ) entity = Price ( ) # Format date as ISO string date iso = f"{model.datum.value.year}-{model.datum.value.month:02d}-{model.datum.value.day:02d}" entity . date = date iso entity . time = f"{model.datum.value.hour:02d}:{model.datum.value.minute:02d}:{model.datum.value.second:02d}" # Symbol # properly mapped symbols have a namespace, except for the US markets # TODO check this with .csv import if model . symbol . namespace : entity . namespace = model . symbol . namespace . upper ( ) entity . symbol = model . symbol . mnemonic . upper ( ) assert isinstance ( model . value , Decimal ) # Find number of decimal places dec places = abs ( model . value . as tuple ( ) . exponent ) entity . denom = 10 ** dec places # Price value entity . value = int ( model . value * entity . denom ) # Currency entity . currency = model . currency . upper ( ) # self.logger.debug(f"{entity}") return entity
def read config ( self , file path : str ) : if not os . path . exists ( file path ) : raise File Not Found Error ( f"File path not found: {file path}" ) # check if file exists if not os . path . isfile ( file path ) : self . logger . error ( f"file not found: {file path}" ) raise File Not Found Error ( f"configuration file not found {file path}" ) self . config . read ( file path )
def get config template path ( self ) -> str : filename = resource filename ( Requirement . parse ( package name ) , template path + config filename ) return filename
def create user config ( self ) : src path = self . get config template path ( ) src = os . path . abspath ( src path ) if not os . path . exists ( src ) : message = f"Config template not found {src}" self . logger . error ( message ) raise File Not Found Error ( message ) dst = os . path . abspath ( self . get config path ( ) ) shutil . copyfile ( src , dst ) if not os . path . exists ( dst ) : raise File Not Found Error ( "Config file could not be copied to user dir!" )
def get contents ( self ) -> str : content = None # with open(file path) as cfg file: #     contents = cfg file.read() # Dump the current contents into an in-memory file. in memory = io . String IO ( "" ) self . config . write ( in memory ) in memory . seek ( 0 ) content = in memory . read ( ) #     log(DEBUG, "config content: %s", content) in memory . close ( ) return content
def set ( self , option : Config Keys , value ) : assert isinstance ( option , Config Keys ) # As currently we only have 1 section. section = SECTION self . config . set ( section , option . name , value ) self . save ( )
def get ( self , option : Config Keys ) : assert isinstance ( option , Config Keys ) # Currently only one section is used section = SECTION return self . config . get ( section , option . name )
def save ( self ) : file path = self . get config path ( ) contents = self . get contents ( ) with open ( file path , mode = 'w' ) as cfg file : cfg file . write ( contents )
def parse ( self , symbol : str ) -> ( str , str ) : symbol parts = symbol . split ( ":" ) namespace = None mnemonic = symbol if len ( symbol parts ) > 1 : namespace = symbol parts [ 0 ] mnemonic = symbol parts [ 1 ] self . namespace = namespace self . mnemonic = mnemonic return namespace , mnemonic
def add price ( self , price : Price Model ) : # assert isinstance(price, Price Model) if not price : raise Value Error ( "Cannot add price. The received model is null!" ) mapper = mappers . Price Mapper ( ) entity = mapper . map model ( price ) self . add price entity ( entity )
def download price ( self , symbol : str , currency : str , agent : str ) -> Price Model : price = self . download price ( symbol , currency , agent ) self . save ( ) return price
def session ( self ) : if not self . session : self . session = dal . get default session ( ) return self . session
def get prices ( self , date : str , currency : str ) -> List [ Price Model ] : from . repositories import Price Repository session = self . session repo = Price Repository ( session ) query = repo . query if date : query = query . filter ( dal . Price . date == date ) if currency : query = query . filter ( dal . Price . currency == currency ) # Sort by symbol. query = query . order by ( dal . Price . namespace , dal . Price . symbol ) price entities = query . all ( ) mapper = mappers . Price Mapper ( ) result = [ ] for entity in price entities : model = mapper . map entity ( entity ) result . append ( model ) return result
def get prices on ( self , on date : str , namespace : str , symbol : str ) : repo = self . get price repository ( ) query = ( repo . query . filter ( dal . Price . namespace == namespace ) . filter ( dal . Price . symbol == symbol ) . filter ( dal . Price . date == on date ) . order by ( dal . Price . time . desc ( ) ) ) result = query . first ( ) # logging.debug(result) return result
def download price ( self , symbol : str , currency : str , agent : str ) : from finance quote python import Quote assert isinstance ( symbol , str ) assert isinstance ( currency , str ) assert isinstance ( agent , str ) if not symbol : return None #self.logger.info(f"Downloading {symbol}... ") dl = Quote ( ) dl . logger = self . logger dl . set source ( agent ) dl . set currency ( currency ) result = dl . fetch ( agent , [ symbol ] ) if not result : raise Value Error ( f"Did not receive a response for {symbol}." ) price = result [ 0 ] if not price : raise Value Error ( f"Price not downloaded/parsed for {symbol}." ) else : # Create price data entity, to be inserted. self . add price ( price ) return price
def get securities ( self , currency : str , agent : str , symbol : str , namespace : str ) -> List [ dal . Security ] : repo = self . get security repository ( ) query = repo . query if currency is not None : query = query . filter ( dal . Security . currency == currency ) if agent is not None : query = query . filter ( dal . Security . updater == agent ) if symbol is not None : query = query . filter ( dal . Security . symbol == symbol ) if namespace is not None : query = query . filter ( dal . Security . namespace == namespace ) # Sorting query = query . order by ( dal . Security . namespace , dal . Security . symbol ) securities = query . all ( ) return securities
def partial ( self ) : ba = self . data [ "bound args" ] return state partial ( self . data [ "func" ] , * ba . args [ 1 : ] , * * ba . kwargs )
def update child calls ( self ) : for node in filter ( lambda n : len ( n . arg name ) , self . child list ) : self . data [ "bound args" ] . arguments [ node . arg name ] = node . partial ( ) self . updated = True
def descend ( self , include me = True ) : if include me : yield self for child in self . child list : yield child yield from child . descend ( )
def multi dec ( f ) : @ wraps ( f ) def wrapper ( * args , * * kwargs ) : args = ( args [ 0 ] if len ( args ) == 1 and isinstance ( args [ 0 ] , ( list , tuple ) ) else args ) for arg in args : if isinstance ( arg , Node ) and arg . parent . name is "root" : arg . parent . remove child ( arg ) arg . update child calls ( ) return f ( * args , * * kwargs ) return wrapper
def get Result From Process ( res , tempname , process ) : if not isinstance ( res , ( Undefined Value , Exception ) ) : value = get Representation ( tempname , process ) return value , res else : return res , str ( res )
def defined items ( self ) : return self . class ( [ ( k , v ) for k , v in self . items ( ) if v is not self . EMPTY ] , is empty = False )
def getx ( self , Parser , ext attr , tree ) : # return cached output if possible cache key = Parser . name + str ( hash ( tree ) ) if self . parser cache . get ( cache key ) : p = self . parser cache [ cache key ] else : # otherwise, run parser over tree p = Parser ( ) # set mappings for parsers that inspect attribute access if ext attr != "mappings" and Parser in [ Function Parser , Object Access Parser , ] : p . mappings = self . context mappings . copy ( ) # run parser p . visit ( tree ) # cache self . parser cache [ cache key ] = p return getattr ( p , ext attr )
def check part ( state , name , part msg , missing msg = None , expand msg = None ) : if missing msg is None : missing msg = "Are you sure you defined the {{part}}? " if expand msg is None : expand msg = "Did you correctly specify the {{part}}? " if not part msg : part msg = name append message = { "msg" : expand msg , "kwargs" : { "part" : part msg } } has part ( state , name , missing msg , append message [ "kwargs" ] ) stu part = state . student parts [ name ] sol part = state . solution parts [ name ] assert ast ( state , sol part , append message [ "kwargs" ] ) return part to child ( stu part , sol part , append message , state )
def detect openmp ( ) : compiler = new compiler ( ) print ( "Checking for Open MP support... " ) hasopenmp = hasfunction ( compiler , 'omp get num threads()' ) needs gomp = hasopenmp if not hasopenmp : compiler . add library ( 'gomp' ) hasopenmp = hasfunction ( compiler , 'omp get num threads()' ) needs gomp = hasopenmp if hasopenmp : print ( "Compiler supports Open MP" ) else : print ( "Did not detect Open MP support." ) return hasopenmp , needs gomp
def get true anomaly ( self ) : self . f = rsky . getf ( self . t supersample , self . t0 , self . per , self . a , self . inc * pi / 180. , self . ecc , self . w * pi / 180. , self . transittype , self . nthreads ) return self . f
def detect ( ) : compiler = new compiler ( ) hasopenmp = hasfunction ( compiler , 'omp get num threads()' ) needs gomp = hasopenmp if not hasopenmp : compiler . add library ( 'gomp' ) hasopenmp = hasfunction ( compiler , 'omp get num threads()' ) needs gomp = hasopenmp return hasopenmp
def teardown ( self , exception ) : ctx = stack . top if ctx is not None : if hasattr ( ctx , 'ldap3 manager connections' ) : for connection in ctx . ldap3 manager connections : self . destroy connection ( connection ) if hasattr ( ctx , 'ldap3 manager main connection' ) : log . debug ( "Unbinding a connection used within the request context." ) ctx . ldap3 manager main connection . unbind ( ) ctx . ldap3 manager main connection = None
def list all ( self , * * kwargs ) : quiet = False if "quiet" in kwargs : quiet = kwargs [ 'quiet' ] bot . spinner . start ( ) url = '%s/collections/' % self . base results = self . paginate get ( url ) bot . spinner . stop ( ) if len ( results ) == 0 : bot . info ( "No container collections found." ) sys . exit ( 1 ) rows = [ ] for result in results : if "containers" in result : if result [ 'id' ] not in [ 37 , 38 , 39 ] : for c in result [ 'containers' ] : rows . append ( [ c [ 'detail' ] , "%s:%s" % ( c [ 'name' ] , c [ 'tag' ] ) ] ) if quiet is False : bot . info ( "Collections" ) bot . table ( rows ) return rows
def update headers ( self , fields = None ) : do reset = True if hasattr ( self , 'headers' ) : if self . headers is not None : do reset = False if do reset is True : self . reset headers ( ) if fields is not None : for key , value in fields . items ( ) : self . headers [ key ] = value header names = "," . join ( list ( self . headers . keys ( ) ) ) bot . debug ( "Headers found: %s" % header names )
def post ( url , data = None , return json = True ) : bot . debug ( "POST %s" % url ) return call ( url , headers = headers , func = requests . post , data = data , return json = return json )
def get ( url , headers = None , token = None , data = None , return json = True ) : bot . debug ( "GET %s" % url ) return call ( url , headers = headers , func = requests . get , data = data , return json = return json )
def load secrets ( self ) : # Second priority: load from cache self . auth = self . get and update setting ( 'GLOBUS AUTH RESPONSE' ) self . transfer = self . get and update setting ( 'GLOBUS TRANSFER RESPONSE' )
def list logs ( self ) : results = [ ] for image in self . bucket . list blobs ( ) : if image . name . endswith ( 'log' ) : results . append ( image ) if len ( results ) == 0 : bot . info ( "No containers found, based on extension .log" ) return results
def init transfer client ( self ) : if self . tokens need update ( ) : self . update tokens ( ) access token = self . transfer [ 'access token' ] # Createe Refresh Token Authorizer authorizer = globus sdk . Refresh Token Authorizer ( self . transfer [ 'refresh token' ] , self . client , access token = self . transfer [ 'access token' ] , expires at = self . transfer [ 'expires at seconds' ] ) self . transfer client = globus sdk . Transfer Client ( authorizer = authorizer )
def status ( backend ) : print ( '[backend status]' ) settings = read client secrets ( ) print ( 'There are %s clients found in secrets.' % len ( settings ) ) if 'SREGISTRY CLIENT' in settings : print ( 'active: %s' % settings [ 'SREGISTRY CLIENT' ] ) update secrets ( settings ) else : print ( 'There is no active client.' )
def add ( backend , variable , value , force = False ) : print ( '[add]' ) settings = read client secrets ( ) # If the variable begins with the SREGISTRY <CLIENT> don't add it prefix = 'SREGISTRY %s ' % backend . upper ( ) if not variable . startswith ( prefix ) : variable = '%s%s' % ( prefix , variable ) # All must be uppercase variable = variable . upper ( ) bot . info ( "%s %s" % ( variable , value ) ) # Does the setting already exist? if backend in settings : if variable in settings [ backend ] and force is False : previous = settings [ backend ] [ variable ] bot . error ( '%s is already set as %s. Use --force to override.' % ( variable , previous ) ) sys . exit ( 1 ) if backend not in settings : settings [ backend ] = { } settings [ backend ] [ variable ] = value update secrets ( settings )
def remove ( backend , variable ) : print ( '[remove]' ) settings = read client secrets ( ) # If the variable begins with the SREGISTRY <CLIENT> don't add it prefixed = variable prefix = 'SREGISTRY %s ' % backend . upper ( ) if not variable . startswith ( prefix ) : prefixed = '%s%s' % ( prefix , variable ) # All must be uppercase variable = variable . upper ( ) bot . info ( variable ) # Does the setting already exist? if backend in settings : if variable in settings [ backend ] : del settings [ backend ] [ variable ] if prefixed in settings [ backend ] : del settings [ backend ] [ prefixed ] update secrets ( settings )
def activate ( backend ) : settings = read client secrets ( ) if backend is not None : settings [ 'SREGISTRY CLIENT' ] = backend update secrets ( settings ) print ( '[activate] %s' % backend )
def delete backend ( backend ) : settings = read client secrets ( ) if backend in settings : del settings [ backend ] # If the backend was the active client, remove too if 'SREGISTRY CLIENT' in settings : if settings [ 'SREGISTRY CLIENT' ] == backend : del settings [ 'SREGISTRY CLIENT' ] update secrets ( settings ) print ( '[delete] %s' % backend ) else : if backend is not None : print ( '%s is not a known client.' % backend ) else : print ( 'Please specify a backend to delete.' )
def delete ( self , url , headers = None , return json = True , default headers = True ) : bot . debug ( 'DELETE %s' % url ) return self . call ( url , headers = headers , func = requests . delete , return json = return json , default headers = default headers )
def head ( self , url ) : bot . debug ( 'HEAD %s' % url ) return self . call ( url , func = requests . head )
def post ( self , url , headers = None , data = None , return json = True , default headers = True ) : bot . debug ( "POST %s" % url ) return self . call ( url , headers = headers , func = requests . post , data = data , return json = return json , default headers = default headers )
def get ( self , url , headers = None , token = None , data = None , return json = True , default headers = True , quiet = False ) : bot . debug ( "GET %s" % url ) return self . call ( url , headers = headers , func = requests . get , data = data , return json = return json , default headers = default headers , quiet = quiet )
def paginate get ( self , url , headers = None , return json = True , start page = None ) : geturl = '%s&page=1' % ( url ) if start page is not None : geturl = '%s&page=%s' % ( url , start page ) results = [ ] while geturl is not None : result = self . get ( url , headers = headers , return json = return json ) # If we have pagination: if isinstance ( result , dict ) : if 'results' in result : results = results + result [ 'results' ] geturl = result [ 'next' ] # No pagination is a list else : return result return results
def remove ( self , image , force = False ) : q = parse image name ( remove uri ( image ) ) # If the registry is provided in the uri, use it if q [ 'registry' ] == None : q [ 'registry' ] = self . base # If the base doesn't start with http or https, add it q = self . add https ( q ) url = '%s/container/%s/%s:%s' % ( q [ 'registry' ] , q [ "collection" ] , q [ "image" ] , q [ "tag" ] ) SREGISTRY EVENT = self . authorize ( request type = "delete" , names = q ) headers = { 'Authorization' : SREGISTRY EVENT } self . update headers ( fields = headers ) continue delete = True if force is False : response = input ( "Are you sure you want to delete %s?" % q [ 'uri' ] ) while len ( response ) < 1 or response [ 0 ] . lower ( ) . strip ( ) not in "ynyesno" : response = input ( "Please answer yes or no: " ) if response [ 0 ] . lower ( ) . strip ( ) in "no" : continue delete = False if continue delete is True : response = self . delete ( url ) message = self . read response ( response ) bot . info ( "Response %s, %s" % ( response . status code , message ) ) else : bot . info ( "Delete cancelled." )
def get installdir ( ) : return os . path . abspath ( os . path . dirname ( os . path . dirname ( file ) ) )
def get collections ( self ) : collections = [ ] for container in self . conn . get account ( ) [ 1 ] : collections . append ( container [ 'name' ] ) return collections
def ipython ( args ) : # The client will announce itself (backend/database) unless it's get from sregistry . main import get client client = get client ( args . endpoint ) client . announce ( args . command ) from I Python import embed embed ( )
def update base ( self ) : self . base = self . get and update setting ( 'SREGISTRY GITLAB BASE' , "https://gitlab.com/" ) self . api base = "%s/api/v4" % self . base . strip ( '/' ) self . artifacts = self . get and update setting ( 'SREGISTRY GITLAB FOLDER' , 'build' ) self . job = self . get and update setting ( 'SREGISTRY GITLAB JOB' , 'build' ) bot . debug ( '      Api: %s' % self . api base ) bot . debug ( 'Artifacts: %s' % self . artifacts ) bot . debug ( '      Job: %s' % self . job )
def update secrets ( self ) : self . token = self . required get and update ( 'SREGISTRY GITLAB TOKEN' ) self . headers [ "Private-Token" ] = self . token
def update setting ( self , name , value ) : if value is not None : updates = { name : value } update client secrets ( backend = self . client name , updates = updates )
def search all ( self ) : results = set ( ) # Here we get names of collections, and then look up containers for container in self . conn . get account ( ) [ 1 ] : # The result here is just the name for result in self . conn . get container ( container [ 'name' ] ) [ 1 ] : results . add ( '%s/%s' % ( container [ 'name' ] , result [ 'name' ] ) ) if len ( results ) == 0 : bot . info ( "No container collections found." ) sys . exit ( 1 ) bot . info ( "Collections" ) bot . table ( [ [ x ] for x in list ( results ) ] ) return list ( results )
def search all ( self ) : results = [ ] # Parse through folders (collections): for entry in self . dbx . files list folder ( '' ) . entries : # Parse through containers for item in self . dbx . files list folder ( entry . path lower ) . entries : name = item . name . replace ( '.simg' , '' ) results . append ( [ "%s/%s" % ( entry . name , name ) ] ) if len ( results ) == 0 : bot . info ( "No container collections found." ) sys . exit ( 1 ) bot . info ( "Collections" ) bot . table ( results ) return results
def get build template ( ) : base = get installdir ( ) name = "%s/main/templates/build/singularity-cloudbuild.json" % base if os . path . exists ( name ) : bot . debug ( "Found template %s" % name ) return read json ( name ) bot . warning ( "Template %s not found." % name )
def get bucket ( self ) : # Case 1: The bucket already exists try : self . bucket = self . bucket service . get bucket ( self . bucket name ) # Case 2: The bucket needs to be created except google . cloud . exceptions . Not Found : self . bucket = self . bucket service . create bucket ( self . bucket name ) # Case 3: The bucket name is already taken except : bot . error ( 'Cannot get or create %s' % self . bucket name ) sys . exit ( 1 ) return self . bucket
def get subparsers ( parser ) : actions = [ action for action in parser . actions if isinstance ( action , argparse . Sub Parsers Action ) ] subparsers = dict ( ) for action in actions : # get all subparsers and print help for choice , subparser in action . choices . items ( ) : subparsers [ choice ] = subparser return subparsers
def get file hash ( filename ) : hasher = hashlib . sha256 ( ) with open ( filename , "rb" ) as f : for chunk in iter ( lambda : f . read ( 4096 ) , b"" ) : hasher . update ( chunk ) return hasher . hexdigest ( )
def clean up ( files ) : if not isinstance ( files , list ) : files = [ files ] for f in files : if os . path . exists ( f ) : bot . verbose3 ( "Cleaning up %s" % f ) os . remove ( f )
def push ( self , path , name , tag = None ) : path = os . path . abspath ( path ) image = os . path . basename ( path ) bot . debug ( "PUSH %s" % path ) if not os . path . exists ( path ) : bot . error ( '%s does not exist.' % path ) sys . exit ( 1 ) # Extract the metadata names = parse image name ( remove uri ( name ) , tag = tag ) image size = os . path . getsize ( path ) >> 20 # Create extra metadata, this is how we identify the image later # *important* bug in boto3 will return these capitalized metadata = { 'sizemb' : "%s" % image size , 'client' : 'sregistry' } self . bucket . upload file ( path , names [ 'storage uri' ] , { "Metadata" : metadata } )
def get collection ( self , name ) : from sregistry . database . models import Collection return Collection . query . filter ( Collection . name == name ) . first ( )
def get container ( self , name , collection id , tag = "latest" , version = None ) : from sregistry . database . models import Container if version is None : container = Container . query . filter by ( collection id = collection id , name = name , tag = tag ) . first ( ) else : container = Container . query . filter by ( collection id = collection id , name = name , tag = tag , version = version ) . first ( ) return container
def rmi ( self , image name ) : container = self . rm ( image name , delete = True ) if container is not None : bot . info ( "[rmi] %s" % container )
def push ( self , path , name , tag = None ) : path = os . path . abspath ( path ) image = os . path . basename ( path ) bot . debug ( "PUSH %s" % path ) if not os . path . exists ( path ) : bot . error ( '%s does not exist.' % path ) sys . exit ( 1 ) # Interaction with a registry requires secrets self . require secrets ( ) # Extract the metadata names = parse image name ( remove uri ( name ) , tag = tag ) image size = os . path . getsize ( path ) >> 20 # COLLECTION ################################################################### # If the registry is provided in the uri, use it if names [ 'registry' ] == None : names [ 'registry' ] = self . base # If the base doesn't start with http or https, add it names = self . add https ( names ) # Prepare push request, this will return a collection ID if permission url = '%s/push/' % names [ 'registry' ] auth url = '%s/upload/chunked upload' % names [ 'registry' ] SREGISTRY EVENT = self . authorize ( request type = "push" , names = names ) # Data fields for collection fields = { 'collection' : names [ 'collection' ] , 'name' : names [ 'image' ] , 'tag' : names [ 'tag' ] } headers = { 'Authorization' : SREGISTRY EVENT } r = requests . post ( auth url , json = fields , headers = headers ) # Always tell the user what's going on! message = self . read response ( r ) print ( '\n[1. Collection return status {0} {1}]' . format ( r . status code , message ) ) # Get the collection id, if created, and continue with upload if r . status code != 200 : sys . exit ( 1 ) # UPLOAD ####################################################################### url = '%s/upload' % names [ 'registry' ] . replace ( '/api' , '' ) bot . debug ( 'Seting upload URL to {0}' . format ( url ) ) cid = r . json ( ) [ 'cid' ] upload to = os . path . basename ( names [ 'storage' ] ) SREGISTRY EVENT = self . authorize ( request type = "upload" , names = names ) encoder = Multipart Encoder ( fields = { 'SREGISTRY EVENT' : SREGISTRY EVENT , 'name' : names [ 'image' ] , 'collection' : str ( cid ) , 'tag' : names [ 'tag' ] , 'file1' : ( upload to , open ( path , 'rb' ) , 'text/plain' ) } ) progress callback = create callback ( encoder , self . quiet ) monitor = Multipart Encoder Monitor ( encoder , progress callback ) headers = { 'Content-Type' : monitor . content type , 'Authorization' : SREGISTRY EVENT } try : r = requests . post ( url , data = monitor , headers = headers ) r . raise for status ( ) message = r . json ( ) [ 'message' ] print ( '\n[Return status {0} {1}]' . format ( r . status code , message ) ) except requests . HTTP Error as e : print ( '\n Upload failed: {0}.' . format ( e ) ) except Keyboard Interrupt : print ( '\n Upload cancelled.' ) except Exception as e : print ( e )
def run build ( self , config , bucket , names ) : project = self . get project ( ) #          prefix,    message, color bot . custom ( 'PROJECT' , project , "CYAN" ) bot . custom ( 'BUILD  ' , config [ 'steps' ] [ 0 ] [ 'name' ] , "CYAN" ) response = self . build service . projects ( ) . builds ( ) . create ( body = config , project Id = project ) . execute ( ) build id = response [ 'metadata' ] [ 'build' ] [ 'id' ] status = response [ 'metadata' ] [ 'build' ] [ 'status' ] bot . log ( "build %s: %s" % ( build id , status ) ) start = time . time ( ) while status not in [ 'COMPLETE' , 'FAILURE' , 'SUCCESS' ] : time . sleep ( 15 ) response = self . build service . projects ( ) . builds ( ) . get ( id = build id , project Id = project ) . execute ( ) build id = response [ 'id' ] status = response [ 'status' ] bot . log ( "build %s: %s" % ( build id , status ) ) end = time . time ( ) bot . log ( 'Total build time: %s seconds' % ( round ( end - start , 2 ) ) ) # If successful, update blob metadata and visibility if status == 'SUCCESS' : # Does the user want to keep the container private? env = 'SREGISTRY GOOGLE STORAGE PRIVATE' blob = bucket . blob ( response [ 'artifacts' ] [ 'objects' ] [ 'paths' ] [ 0 ] ) # Make Public, if desired if self . get and update setting ( env ) == None : blob . make public ( ) response [ 'public url' ] = blob . public url # Add the metadata directly to the object update blob metadata ( blob , response , config , bucket , names ) response [ 'media link' ] = blob . media link response [ 'size' ] = blob . size response [ 'file hash' ] = blob . md5 hash return response
def search all ( self ) : # This should be your apis url for a search url = '...' # paginte get is what it sounds like, and what you want for multiple # pages of results results = self . paginate get ( url ) if len ( results ) == 0 : bot . info ( "No container collections found." ) sys . exit ( 1 ) bot . info ( "Collections" ) # Here is how to create a simple table. You of course must parse your # custom result and form the fields in the table to be what you think # are important! rows = [ ] for result in results : if "containers" in result : for c in result [ 'containers' ] : rows . append ( [ c [ 'uri' ] , c [ 'detail' ] ] ) bot . table ( rows ) return rows
def get manifest ( self , repo name , tag ) : image = None repo = self . aws . describe images ( repository Name = repo name ) if 'image Details' in repo : for contender in repo . get ( 'image Details' ) : if tag in contender [ 'image Tags' ] : image = contender break # if the image isn't found, we need to exit if image is None : bot . exit ( 'Cannot find %s:%s, is the uri correct?' % ( repo name , digest ) ) digest = image [ 'image Digest' ] digests = self . aws . batch get image ( repository Name = repo name , image Ids = [ { "image Digest" : digest , "image Tag" : tag } ] ) self . manifest = json . loads ( digests [ 'images' ] [ 0 ] [ 'image Manifest' ] ) return self . manifest
def s3errors ( path ) : try : yield except Client Error as error : error = error . response . get ( "Error" , { } ) error code = error . get ( "Code" , None ) response meta = error . response . get ( "Response Metadata" , { } ) http status = response meta . get ( "HTTP Status Code" , 200 ) error msg = error . get ( "Message" , None ) if error code == "No Such Bucket" : raise errors . Resource Error ( path , exc = error , msg = error msg ) if http status == 404 : raise errors . Resource Not Found ( path ) elif http status == 403 : raise errors . Permission Denied ( path = path , msg = error msg ) else : raise errors . Operation Failed ( path = path , exc = error ) except SSL Error as error : raise errors . Operation Failed ( path , exc = error ) except Endpoint Connection Error as error : raise errors . Remote Connection Error ( path , exc = error , msg = "{}" . format ( error ) )
def factory ( cls , filename , mode , on close ) : temp file = tempfile . Temporary File ( ) proxy = cls ( temp file , filename , mode , on close = on close ) return proxy
def gravatar url ( user or email , size = GRAVATAR DEFAULT SIZE ) : if hasattr ( user or email , 'email' ) : email = user or email . email else : email = user or email try : return escape ( get gravatar url ( email = email , size = size ) ) except : return ''
def has gravatar ( email ) : # Request a 404 response if the gravatar does not exist url = get gravatar url ( email , default = GRAVATAR DEFAULT IMAGE 404 ) # Verify an OK response was received try : request = Request ( url ) request . get method = lambda : 'HEAD' return 200 == urlopen ( request ) . code except ( HTTP Error , URL Error ) : return False
def chimera blocks ( M = 16 , N = 16 , L = 4 ) : for x in xrange ( M ) : for y in xrange ( N ) : for u in ( 0 , 1 ) : yield tuple ( ( x , y , u , k ) for k in xrange ( L ) )
def main ( ) : # Root options parser = Molvs Parser ( epilog = 'use "molvs <command> -h" to show help for a specific command' ) subparsers = parser . add subparsers ( title = 'Available commands' ) # Options common to all commands common parser = Molvs Parser ( add help = False ) common parser . add argument ( 'infile' , nargs = '?' , help = 'input filename' , type = argparse . File Type ( 'r' ) , default = sys . stdin ) common parser . add argument ( '-i' , '--intype' , help = 'input filetype' , choices = FILETYPES ) common parser . add argument ( '-:' , '--smiles' , help = 'input SMILES instead of file' , metavar = '<smiles>' ) common parser . add argument ( '-O' , '--outfile' , help = 'output filename' , type = argparse . File Type ( 'w' ) , default = sys . stdout , metavar = '<outfile>' ) # Standardize options standardize parser = subparsers . add parser ( 'standardize' , help = 'standardize a molecule' , parents = [ common parser ] ) standardize parser . add argument ( '-o' , '--outtype' , help = 'output filetype' , choices = FILETYPES ) standardize parser . set defaults ( func = standardize main ) # Validate options validate parser = subparsers . add parser ( 'validate' , help = 'validate a molecule' , parents = [ common parser ] ) validate parser . set defaults ( func = validate main ) args = parser . parse args ( ) try : args . func ( args ) except Exception as e : sys . stderr . write ( 'Error: %s\n\n' . encode ( ) % e . message ) parser . print help ( ) sys . exit ( 2 )
def integrate ivp ( u0 = 1.0 , v0 = 0.0 , mu = 1.0 , tend = 10.0 , dt0 = 1e-8 , nt = 0 , nsteps = 600 , t0 = 0.0 , atol = 1e-8 , rtol = 1e-8 , plot = False , savefig = 'None' , method = 'bdf' , dpi = 100 , verbose = False ) : f , j = get f and j ( mu ) if nt > 1 : tout = np . linspace ( t0 , tend , nt ) yout , nfo = integrate predefined ( f , j , [ u0 , v0 ] , tout , dt0 , atol , rtol , nsteps = nsteps , check indexing = False , method = method ) else : tout , yout , nfo = integrate adaptive ( f , j , [ u0 , v0 ] , t0 , tend , dt0 , atol , rtol , nsteps = nsteps , check indexing = False , method = method ) # dfdt[:] also for len == 1 if verbose : print ( nfo ) if plot : import matplotlib . pyplot as plt plt . plot ( tout , yout [ : , 1 ] , 'g--' ) plt . plot ( tout , yout [ : , 0 ] , 'k-' , linewidth = 2 ) if savefig == 'None' : plt . show ( ) else : plt . savefig ( savefig , dpi = dpi )
def get mems of org ( self ) : print 'Getting members.' counter = 0 for member in self . org retrieved . iter members ( ) : self . members json [ member . id ] = member . to json ( ) counter += 1 return counter
def get teams of org ( self ) : print 'Getting teams.' counter = 0 for team in self . org retrieved . iter teams ( ) : self . teams json [ team . id ] = team . to json ( ) counter += 1 return counter
def repos ( self , repo type = 'public' , organization = 'llnl' ) : print 'Getting repos.' for repo in self . org retrieved . iter repos ( type = repo type ) : #JSON json = repo . to json ( ) self . repos json [ repo . name ] = json #CSV temp repo = my repo . My Repo ( ) temp repo . name = repo . full name self . total repos += 1 temp repo . contributors = my github . get total contributors ( repo ) self . total contributors += temp repo . contributors temp repo . forks = repo . forks count self . total forks += temp repo . forks temp repo . stargazers = repo . stargazers self . total stars += temp repo . stargazers temp repo . pull requests open , temp repo . pull requests closed = my github . get pull reqs ( repo ) temp repo . pull requests = ( temp repo . pull requests open + temp repo . pull requests closed ) self . total pull reqs += temp repo . pull requests open self . total pull reqs += temp repo . pull requests closed self . total pull reqs open += temp repo . pull requests open self . total pull reqs closed += temp repo . pull requests closed temp repo . open issues = repo . open issues count self . total open issues += temp repo . open issues temp repo . closed issues = my github . get issues ( repo , organization = organization ) temp repo . issues = temp repo . closed issues + temp repo . open issues self . total closed issues += temp repo . closed issues self . total issues += temp repo . issues my github . get languages ( repo , temp repo ) temp repo . readme = my github . get readme ( repo ) #temp repo.license = my github.get license(repo) temp repo . commits = self . get commits ( repo = repo , organization = organization ) self . total commits += temp repo . commits self . all repos . append ( temp repo )
def get pull reqs ( self , repo ) : pull reqs open = 0 pull reqs closed = 0 for pull request in repo . iter pulls ( state = 'all' ) : self . pull requests json [ repo . name ] . append ( pull request . to json ( ) ) if pull request . closed at is not None : pull reqs closed += 1 else : pull reqs open += 1 return pull reqs open , pull reqs closed
def get issues ( self , repo , organization = 'llnl' ) : #JSON path = ( '../github-data/' + organization + '/' + repo . name + '/issues' ) is only today = False if not os . path . exists ( path ) : #no previous path, get all issues all issues = repo . iter issues ( state = 'all' ) is only today = True else : files = os . listdir ( path ) date = str ( files [ - 1 ] [ : - 5 ] ) if date == str ( datetime . date . today ( ) ) : #most recent date is actually today, get previous most recent date if len ( files ) > 2 : date = str ( files [ - 2 ] [ : - 5 ] ) else : #This means there is only one file, today. Retrieve every issue all issues = repo . iter issues ( state = 'all' ) is only today = True if not is only today : #there's a previous saved JSON that's not today all issues = repo . iter issues ( since = date , state = 'all' ) for issue in all issues : self . issues json [ repo . name ] . append ( issue . to json ( ) ) #CSV closed issues = 0 for issue in repo . iter issues ( state = 'closed' ) : if issue is not None : closed issues += 1 return closed issues
def get license ( self , repo ) : if self . search limit >= 28 : print 'Hit search limit. Sleeping for 60 sec.' time . sleep ( 60 ) self . search limit = 0 self . search limit += 1 search results = self . logged in gh . search code ( 'license' + 'in:path repo:' + repo . full name ) try : for result in search results : path = result . path [ 1 : ] if '/' not in path and 'license' in path . lower ( ) : self . total licenses += 1 return path return 'MISS' except ( Stop Iteration ) as e : return 'MISS'
def write org json ( self , date = ( datetime . date . today ( ) ) , organization = 'llnl' , dict to write = { } , path ending type = '' , is list = False ) : path = ( '../github-data/' + organization + '-org/' + path ending type + '/' + str ( date ) + '.json' ) self . check Dir ( path ) with open ( path , 'w' ) as out clear : #clear old data out clear . close ( ) with open ( path , 'a' ) as out : if is list : #used for list of items out . write ( '[' ) for item in dict to write : out . write ( json . dumps ( dict to write [ item ] , sort keys = True , indent = 4 , separators = ( ',' , ': ' ) ) + ',' ) out . seek ( - 1 , os . SEEK END ) #kill last comma out . truncate ( ) if is list : out . write ( ']' ) out . close ( )
def write repo json ( self , date = ( datetime . date . today ( ) ) , organization = 'llnl' , dict to write = { } , path ending type = '' , is list = False , is dict = False ) : for repo in dict to write : path = ( '../github-data/' + organization + '/' + repo + '/' + path ending type + '/' + str ( date ) + '.json' ) self . check Dir ( path ) with open ( path , 'w' ) as out : if is list : out . write ( '[' ) for value in dict to write [ repo ] : if is dict : for inner dict in value : out . write ( json . dumps ( inner dict , sort keys = True , indent = 4 , separators = ( ',' , ': ' ) ) + ',' ) else : out . write ( json . dumps ( value , sort keys = True , indent = 4 , separators = ( ',' , ': ' ) ) + ',' ) out . seek ( - 1 , os . SEEK END ) #kill last comma out . truncate ( ) out . write ( ']' ) else : out . write ( json . dumps ( dict to write [ repo ] , sort keys = True , indent = 4 , separators = ( ',' , ': ' ) ) ) out . close ( )
def write totals ( self , file path = '' , date = str ( datetime . date . today ( ) ) , organization = 'N/A' , members = 0 , teams = 0 ) : total exists = os . path . isfile ( file path ) with open ( file path , 'a' ) as out total : if not total exists : out total . write ( 'date,organization,repos,members,teams,' + 'unique contributors,total contributors,forks,' + 'stargazers,pull requests,open issues,has readme,' + 'has license,pull requests open,pull requests closed,' + 'commits,id,closed issues,issues\n' ) self . delete last line ( date = date , file path = file path ) out total . close ( ) with open ( file path , 'r' ) as file read : row count = sum ( 1 for row in file read ) - 1 file read . close ( ) with open ( file path , 'a' ) as out total : out total . write ( date + ',' + organization + ',' + str ( self . total repos ) + ',' + str ( members ) + ',' + str ( teams ) + ',' + str ( len ( self . unique contributors ) ) + ',' + str ( self . total contributors ) + ',' + str ( self . total forks ) + ',' + str ( self . total stars ) + ',' + str ( self . total pull reqs ) + ',' + str ( self . total open issues ) + ',' + str ( self . total readmes ) + ',' + str ( self . total licenses ) + ',' + str ( self . total pull reqs open ) + ',' + str ( self . total pull reqs closed ) + ',' + str ( self . total commits ) + ',' + str ( row count ) + ',' + str ( self . total closed issues ) + ',' + str ( self . total issues ) + '\n' ) out total . close ( )
def write languages ( self , file path = '' , date = str ( datetime . date . today ( ) ) ) : self . remove date ( file path = file path , date = date ) languages exists = os . path . isfile ( file path ) with open ( file path , 'a' ) as out languages : if not languages exists : out languages . write ( 'date,language,count,size,size log\n' ) languages sorted = sorted ( self . languages size ) #self.delete last line(date=date, file path=file path) for language in languages sorted : try : out languages . write ( date + ',' + language + ',' + str ( self . languages [ language ] ) + ',' + str ( self . languages size [ language ] ) + ',' + str ( math . log10 ( int ( self . languages size [ language ] ) ) ) + '\n' ) except ( Type Error , Key Error ) as e : out languages . write ( date + ',' + language + ',' + str ( 0 ) + ',' + str ( self . languages size [ language ] ) + ',' + str ( math . log10 ( int ( self . languages size [ language ] ) ) ) + '\n' )
def connect ( url = , token = None ) : gh session = None if url == : gh session = create session ( token ) else : gh session = create enterprise session ( url , token ) if gh session is None : msg = 'Unable to connect to (%s) with provided token.' raise Runtime Error ( msg , url ) logger . info ( 'Connected to: %s' , url ) return gh session
def write to file ( self , file path = '' , date = ( datetime . date . today ( ) ) , organization = 'llnl' ) : with open ( file path , 'w+' ) as out : out . write ( 'date,organization,stargazers\n' ) sorted stargazers = sorted ( self . stargazers ) #sort based on lowercase for star in sorted stargazers : out . write ( star + ',' + str ( self . stargazers [ star ] ) + '\n' ) out . close ( )
def from github3 ( klass , repository , labor hours = True ) : if not isinstance ( repository , github3 . repos . repo . Repository ) : raise Type Error ( 'Repository must be a github3 Repository object' ) logger . info ( 'Processing: %s' , repository . full name ) project = klass ( ) logger . debug ( 'Git Hub3: repository=%s' , repository ) # -- REQUIRED FIELDS -- project [ 'name' ] = repository . name project [ 'repository URL' ] = repository . git url project [ 'description' ] = repository . description try : repo license = repository . license ( ) except github3 . exceptions . Not Found Error : logger . debug ( 'no license found for repo=%s' , repository ) repo license = None if repo license : license = repo license . license if license : logger . debug ( 'license spdx=%s; url=%s' , license . spdx id , license . url ) if license . url is None : project [ 'permissions' ] [ 'licenses' ] = [ { "name" : license . spdx id } ] else : project [ 'permissions' ] [ 'licenses' ] = [ { "URL" : license . url , "name" : license . spdx id } ] else : project [ 'permissions' ] [ 'licenses' ] = None public server = repository . html url . startswith ( ) if not repository . private and public server : project [ 'permissions' ] [ 'usage Type' ] = 'open Source' elif date parse ( repository . created at ) < POLICY START DATE : project [ 'permissions' ] [ 'usage Type' ] = 'exempt By Policy Date' if labor hours : project [ 'labor Hours' ] = labor hours from url ( project [ 'repository URL' ] ) else : project [ 'labor Hours' ] = 0 project [ 'tags' ] = [ 'github' ] old accept = repository . session . headers [ 'Accept' ] repository . session . headers [ 'Accept' ] = 'application/vnd.github.mercy-preview+json' topics = repository . get ( repository . url + '/topics' ) . json ( ) project [ 'tags' ] . extend ( topics . get ( 'names' , [ ] ) ) repository . session . headers [ 'Accept' ] = old accept # Hacky way to get an Organization object back with Git Hub3.py >= 1.2.0 owner url = repository . owner . url owner api response = repository . get ( owner url ) organization = repository . json ( owner api response , 200 ) project [ 'contact' ] [ 'email' ] = organization [ 'email' ] project [ 'contact' ] [ 'URL' ] = organization [ 'html url' ] # -- OPTIONAL FIELDS -- # project['version'] = '' project [ 'organization' ] = organization [ 'name' ] project [ 'status' ] = 'Development' project [ 'vcs' ] = 'git' project [ 'homepage URL' ] = repository . html url project [ 'download URL' ] = repository . downloads url project [ 'languages' ] = [ l for l , in repository . languages ( ) ] # project['partners'] = [] # project['related Code'] = [] # project['reused Code'] = [] # date: [object] A date object describing the release. #   created: [string] The date the release was originally created, in YYYY-MM-DD or ISO 8601 format. #   last Modified: [string] The date the release was modified, in YYYY-MM-DD or ISO 8601 format. #   metadata Last Updated: [string] The date the metadata of the release was last updated, in YYYY-MM-DD or ISO 8601 format. try : created at = repository . created at . date ( ) except Attribute Error : created at = date parse ( repository . created at ) . date ( ) try : updated at = repository . updated at . date ( ) except Attribute Error : updated at = date parse ( repository . updated at ) . date ( ) project [ 'date' ] = { 'created' : created at . isoformat ( ) , 'last Modified' : updated at . isoformat ( ) , 'metadata Last Updated' : '' , } prune dict null str ( project ) return project
def from gitlab ( klass , repository , labor hours = True ) : if not isinstance ( repository , gitlab . v4 . objects . Project ) : raise Type Error ( 'Repository must be a gitlab Repository object' ) project = klass ( ) logger . debug ( 'Git Lab: repository id=%d path with namespace=%s' , repository . id , repository . path with namespace , ) # -- REQUIRED FIELDS -- project [ 'name' ] = repository . name project [ 'repository URL' ] = repository . http url to repo project [ 'description' ] = repository . description # TODO: Update licenses from Git Lab API project [ 'permissions' ] [ 'licenses' ] = None web url = repository . web url public server = web url . startswith ( 'https://gitlab.com' ) if repository . visibility in ( 'public' ) and public server : project [ 'permissions' ] [ 'usage Type' ] = 'open Source' elif date parse ( repository . created at ) < POLICY START DATE : project [ 'permissions' ] [ 'usage Type' ] = 'exempt By Policy Date' if labor hours : project [ 'labor Hours' ] = labor hours from url ( project [ 'repository URL' ] ) else : project [ 'labor Hours' ] = 0 project [ 'tags' ] = [ 'gitlab' ] + repository . tag list project [ 'contact' ] = { 'email' : '' , 'URL' : web url , } # -- OPTIONAL FIELDS -- # project['version'] = '' project [ 'organization' ] = repository . namespace [ 'name' ] project [ 'status' ] = 'Development' project [ 'vcs' ] = 'git' project [ 'homepage URL' ] = repository . web url api url = repository . manager . gitlab . url archive suffix = '/projects/%s/repository/archive' % repository . get id ( ) project [ 'download URL' ] = api url + archive suffix # project['languages'] = [l for l,   in repository.languages()] # project['partners'] = [] # project['related Code'] = [] # project['reused Code'] = [] project [ 'date' ] = { 'created' : date parse ( repository . created at ) . date ( ) . isoformat ( ) , 'last Modified' : date parse ( repository . last activity at ) . date ( ) . isoformat ( ) , 'metadata Last Updated' : '' , } prune dict null str ( project ) return project
def from stashy ( klass , repository , labor hours = True ) : # if not isinstance(repository, stashy.repos.Repository): #     raise Type Error('Repository must be a stashy Repository object') if not isinstance ( repository , dict ) : raise Type Error ( 'Repository must be a dict' ) project = klass ( ) logger . debug ( 'Stashy: project key=%s repository slug=%s' , repository [ 'name' ] , repository [ 'project' ] [ 'key' ] , ) # -- REQUIRED FIELDS -- project [ 'name' ] = repository [ 'name' ] clone urls = [ clone [ 'href' ] for clone in repository [ 'links' ] [ 'clone' ] ] for url in clone urls : # Only rely on SSH Urls for repository urls if url . startswith ( 'ssh://' ) : project [ 'repository URL' ] = url break description = repository [ 'project' ] . get ( 'description' , '' ) if description : project [ 'description' ] = 'Project description: %s' % description project [ 'permissions' ] [ 'licenses' ] = None web url = repository [ 'links' ] [ 'self' ] [ 0 ] [ 'href' ] public server = web url . startswith ( 'https://bitbucket.org' ) if repository [ 'public' ] and public server : project [ 'permissions' ] [ 'usage Type' ] = 'open Source' if labor hours : project [ 'labor Hours' ] = labor hours from url ( project [ 'repository URL' ] ) else : project [ 'labor Hours' ] = 0 project [ 'tags' ] = [ 'bitbucket' ] project [ 'contact' ] [ 'email' ] = '' project [ 'contact' ] [ 'URL' ] = repository [ 'links' ] [ 'self' ] [ 0 ] [ 'href' ] # -- OPTIONAL FIELDS -- # project['version'] = '' # project['organization'] = organization.name project [ 'status' ] = 'Development' project [ 'vcs' ] = repository [ 'scm Id' ] project [ 'homepage URL' ] = repository [ 'links' ] [ 'self' ] [ 0 ] [ 'href' ] # project['download URL'] = # project['languages'] = # project['partners'] = [] # project['related Code'] = [] # project['reused Code'] = [] # date: [object] A date object describing the release. #   created: [string] The date the release was originally created, in YYYY-MM-DD or ISO 8601 format. #   last Modified: [string] The date the release was modified, in YYYY-MM-DD or ISO 8601 format. #   metadata Last Updated: [string] The date the metadata of the release was last updated, in YYYY-MM-DD or ISO 8601 format. # project['date'] = { #     'created': repository.pushed at.isoformat(), #     'last Modified': repository.updated at.isoformat(), #     'metadata Last Updated': '', # } prune dict null str ( project ) return project
def force attributes ( metadata , config ) : organization = config . get ( 'organization' , '' ) logger . debug ( 'Organization: %s' , organization ) contact email = config . get ( 'contact email' ) logger . debug ( 'Contact Email: %s' , contact email ) permissions = config . get ( 'permissions' , { } ) default usage = permissions . get ( 'usage Type' , '' ) default exemption text = permissions . get ( 'exemption Text' , '' ) logger . debug ( 'Default usage Type: %s' , default usage ) logger . debug ( 'Default exemption Text: %s' , default exemption text ) # Force certain fields if organization : logger . debug ( 'Forcing Organization to: %s' , organization ) if contact email : logger . debug ( 'Forcing Contact Email to: %s' , contact email ) for release in metadata [ 'releases' ] : if organization : release [ 'organization' ] = organization if contact email : release [ 'contact' ] [ 'email' ] = contact email if 'licenses' not in release [ 'permissions' ] : release [ 'permissions' ] [ 'licenses' ] = None if 'description' not in release : release [ 'description' ] = 'No description available...' if 'usage Type' not in release [ 'permissions' ] : release [ 'permissions' ] [ 'usage Type' ] = default usage release [ 'permissions' ] [ 'exemption Text' ] = default exemption text return metadata
def get traffic ( self ) : print 'Getting traffic.' #Uses the developer API. Note this could change. headers = { 'Accept' : 'application/vnd.github.spiderman-preview' , 'Authorization' : 'token ' + self . token } headers release = { 'Authorization' : 'token ' + self . token } for repo in self . org retrieved . iter repos ( type = 'public' ) : url = ( 'https://api.github.com/repos/' + self . organization name + '/' + repo . name ) self . get referrers ( url = url , headers = headers , repo name = repo . name ) self . get paths ( url = url , headers = headers ) self . get data ( url = url , headers = headers , dict to store = self . views , type = 'views' , repo name = repo . name ) self . get data ( url = url , headers = headers , dict to store = self . clones , type = 'clones' , repo name = repo . name ) self . get releases ( url = url , headers = headers release , repo name = repo . name )
def get releases ( self , url = '' , headers = { } , repo name = '' ) : url releases = ( url + '/releases' ) r = requests . get ( url releases , headers = headers ) self . releases json [ repo name ] = r . json ( )
def write json ( self , date = ( datetime . date . today ( ) ) , organization = 'llnl' , dict to write = { } , path ending type = '' ) : for repo in dict to write : if len ( dict to write [ repo ] ) != 0 : #don't need to write out empty lists path = ( '../github-data/' + organization + '/' + repo + '/' + path ending type + '/' + str ( date ) + '.json' ) self . check Dir ( path ) with open ( path , 'w' ) as out : out . write ( json . dumps ( dict to write [ repo ] , sort keys = True , indent = 4 , separators = ( ',' , ': ' ) ) ) out . close ( )
def write to file ( self , referrers file path = '' , views file path = '' , clones file path = '' , date = ( datetime . date . today ( ) ) , organization = 'llnl' , views row count = 0 , clones row count = 0 ) : self . write referrers to file ( file path = referrers file path ) self . write data to file ( file path = views file path , dict to write = self . views , name = 'views' , row count = views row count ) self . write data to file ( file path = clones file path , dict to write = self . clones , name = 'clones' , row count = clones row count )
def write data to file ( self , file path = '' , date = str ( datetime . date . today ( ) ) , organization = 'llnl' , dict to write = { } , name = '' , row count = 0 ) : exists = os . path . isfile ( file path ) with open ( file path , 'a' ) as out : if not exists : out . write ( 'date,organization,' + name + ',unique ' + name + ',id\n' ) sorted dict = sorted ( dict to write ) for day in sorted dict : day formatted = datetime . datetime . utcfromtimestamp ( day ) . strftime ( '%Y-%m-%d' ) out . write ( day formatted + ',' + organization + ',' + str ( dict to write [ day ] [ 0 ] ) + ',' + str ( dict to write [ day ] [ 1 ] ) + ',' + str ( row count ) + '\n' ) row count += 1
def write referrers to file ( self , file path = '' , date = str ( datetime . date . today ( ) ) , organization = 'llnl' ) : self . remove date ( file path = file path , date = date ) referrers exists = os . path . isfile ( file path ) with open ( file path , 'a' ) as out : if not referrers exists : out . write ( 'date,organization,referrer,count,count log,uniques,' + 'uniques logged\n' ) sorted referrers = sorted ( self . referrers lower ) #sort based on lowercase for referrer in sorted referrers : ref name = self . referrers lower [ referrer ] #grab real name from count = self . referrers [ ref name ] [ 0 ] uniques = self . referrers [ ref name ] [ 1 ] if count == 1 : #so we don't display 0 for count of 1 count = 1.5 if uniques == 1 : uniques = 1.5 count logged = math . log ( count ) uniques logged = math . log ( uniques ) out . write ( date + ',' + organization + ',' + ref name + ',' + str ( count ) + ',' + str ( count logged ) + ',' + str ( uniques ) + ',' + str ( uniques logged ) + '\n' ) out . close ( )
def write to file ( self , file path = '' ) : with open ( file path , 'w+' ) as out : out . write ( 'user, email\n' ) sorted names = sorted ( self . logins lower ) #sort based on lowercase for login in sorted names : out . write ( self . logins lower [ login ] + ',' + self . emails [ self . logins lower [ login ] ] + '\n' ) out . close ( )
def connect ( url , username , password ) : bb session = stashy . connect ( url , username , password ) logger . info ( 'Connected to: %s as %s' , url , username ) return bb session
def query repos ( gl session , repos = None ) : if repos is None : repos = [ ] for repo in repos : yield gl session . projects . get ( repo ) if not repos : for project in gl session . projects . list ( as list = False ) : yield project
def prune dict null str ( dictionary ) : for key , value in list ( dictionary . items ( ) ) : if value is None or str ( value ) == '' : del dictionary [ key ] if isinstance ( value , dict ) : dictionary [ key ] = prune dict null str ( dictionary [ key ] ) return dictionary
def create tfs connection ( url , token ) : if token is None : token = os . environ . get ( 'TFS API TOKEN' , None ) tfs credentials = Basic Authentication ( '' , token ) tfs connection = Vss Connection ( base url = url , creds = tfs credentials ) return tfs connection
def create tfs git client ( url , token = None ) : if token is None : token = os . environ . get ( 'TFS API TOKEN' , None ) tfs connection = create tfs connection ( url , token ) tfs git client = tfs connection . get client ( 'vsts.git.v4 1.git client.Git Client' ) if tfs git client is None : msg = 'Unable to create TFS Git Client, failed to connect to TFS Enterprise (%s) with provided token.' raise Runtime Error ( msg , url ) return tfs git client
def create tfs tfvc client ( url , token = None ) : if token is None : token = os . environ . get ( 'TFS API TOKEN' , None ) tfs connection = create tfs connection ( url , token ) tfs tfvc client = tfs connection . get client ( 'vsts.tfvc.v4 1.tfvc client.Tfvc Client' ) if tfs tfvc client is None : msg = 'Unable to create TFS Git Client, failed to connect to TFS Enterprise (%s) with provided token.' raise Runtime Error ( msg , url ) return tfs tfvc client
def get git repos ( url , token , collection , project ) : git client = create tfs git client ( '{url}/{collection name}' . format ( url = url , collection name = collection . name ) , token ) logger . debug ( 'Retrieving Git Repos for Project: {project name}' . format ( project name = project . name ) ) return git client . get repositories ( project . id )
def get tfvc repos ( url , token , collection , project ) : branch list = [ ] tfvc client = create tfs tfvc client ( '{url}/{collection name}' . format ( url = url , collection name = collection . name ) , token ) logger . debug ( 'Retrieving Tfvc Branches for Project: {project name}' . format ( project name = project . name ) ) branches = tfvc client . get branches ( project . id , True , True , False , True ) if branches : branch list . extend ( branches ) else : logger . debug ( 'No Tfvcc Branches in Project: {project name}' . format ( project name = project . name ) ) return branch list
def write to file ( self ) : with open ( '../github stats output/last year commits.csv' , 'w+' ) as output : output . write ( 'date,organization,repos,members,teams,' + 'unique contributors,total contributors,forks,' + 'stargazers,pull requests,open issues,has readme,' + 'has license,pull requests open,pull requests closed,' + 'commits\n' ) #no reverse this time to print oldest first previous commits = 0 for week in self . sorted weeks : if str ( self . commits [ week ] ) != previous commits : #delete dups week formatted = datetime . datetime . utcfromtimestamp ( week ) . strftime ( '%Y-%m-%d' ) output . write ( week formatted + ',llnl,0,0,0,0,0,0,0,0,0,0,0,0,0,' + str ( self . commits [ week ] ) + '\n' ) previous commits = str ( self . commits [ week ] )
def incr ( self , stat , value = 1 , tags = None ) : self . client . incr ( stat = stat , count = value )
def timing ( self , stat , value , tags = None ) : self . client . timing ( stat = stat , delta = value )
def incr ( self , stat , value = 1 , tags = None ) : self . client . increment ( metric = stat , value = value , tags = tags )
def gauge ( self , stat , value , tags = None ) : self . client . gauge ( metric = stat , value = value , tags = tags )
def timing ( self , stat , value , tags = None ) : self . client . timing ( metric = stat , value = value , tags = tags )
def histogram ( self , stat , value , tags = None ) : self . client . histogram ( metric = stat , value = value , tags = tags )
def incr ( self , stat , value = 1 , tags = None ) : self . log ( 'incr' , stat , value , tags )
def gauge ( self , stat , value , tags = None ) : self . log ( 'gauge' , stat , value , tags )
def timing ( self , stat , value , tags = None ) : self . log ( 'timing' , stat , value , tags )
def histogram ( self , stat , value , tags = None ) : self . log ( 'histogram' , stat , value , tags )
def rollup ( self ) : now = time . time ( ) if now < self . next rollup : return self . next rollup = now + self . flush interval for key , values in sorted ( self . incr stats . items ( ) ) : self . logger . info ( '%s INCR %s: count:%d|rate:%d/%d' , self . leader , key , len ( values ) , sum ( values ) , self . flush interval ) self . incr stats [ key ] = [ ] for key , values in sorted ( self . gauge stats . items ( ) ) : if values : self . logger . info ( '%s GAUGE %s: count:%d|current:%s|min:%s|max:%s' , self . leader , key , len ( values ) , values [ - 1 ] , min ( values ) , max ( values ) , ) else : self . logger . info ( '%s (gauge) %s: no data' , self . leader , key ) self . gauge stats [ key ] = [ ] for key , values in sorted ( self . histogram stats . items ( ) ) : if values : self . logger . info ( ( '%s HISTOGRAM %s: ' 'count:%d|min:%.2f|avg:%.2f|median:%.2f|ninety-five:%.2f|max:%.2f' ) , self . leader , key , len ( values ) , min ( values ) , statistics . mean ( values ) , statistics . median ( values ) , values [ int ( len ( values ) * 95 / 100 ) ] , max ( values ) ) else : self . logger . info ( '%s (histogram) %s: no data' , self . leader , key ) self . histogram stats [ key ] = [ ]
def incr ( self , stat , value = 1 , tags = None ) : self . rollup ( ) # FIXME(willkg): what to do with tags? self . incr stats . setdefault ( stat , [ ] ) . append ( value )
def gauge ( self , stat , value , tags = None ) : self . rollup ( ) # FIXME(willkg): what to do with tags? self . gauge stats . setdefault ( stat , [ ] ) . append ( value )
def histogram ( self , stat , value , tags = None ) : self . rollup ( ) # FIXME(willkg): what to do with tags? self . histogram stats . setdefault ( stat , [ ] ) . append ( value )
def from db value ( self , value , expression , connection , context ) : if value is None : return value return self . enum [ value ]
def to python ( self , value ) : if value is None : return value if isinstance ( value , self . enum ) : return value return self . enum [ value ]
def get prep value ( self , value ) : if value is None : return None if isinstance ( value , self . enum ) : return value . name raise Value Error ( "Unknown value {value:r} of type {cls}" . format ( value = value , cls = type ( value ) ) )
def t parse ( self , s ) : # self.root = None # self.path = s with self . lock : try : return self . parser . parse ( s , lexer = self . lexer , debug = False ) except Cannot Parse as e : e . s = s raise e
def parse ( self , s ) : with self . lock : try : return self . parser . parse ( s , lexer = self . lexer ) except Invalid IEML Object Argument as e : raise Cannot Parse ( s , str ( e ) ) except Cannot Parse as e : e . s = s raise e
def resolve path ( obj , path ) : if obj . class not in path . context . accept : result = set ( ) for ctx in path . context . accept : result |= { e for u in obj [ ctx ] for e in resolve path ( u , path ) } return result if isinstance ( obj , Text ) : if path . index is not None : return { obj . children [ path . index ] } return set ( obj . children ) if isinstance ( obj , ( Fact , Theory ) ) : return resolve path tree graph ( obj . tree graph , path ) if isinstance ( obj , Topic ) : if path . kind == 'r' : if path . index is not None : return { obj . root [ path . index ] } return set ( obj . root ) else : if path . index is not None : return { obj . flexing [ path . index ] } return set ( obj . flexing )
def mean ( self ) : if self . counter . value > 0 : return self . sum . value / self . counter . value return 0.0
def mean rate ( self ) : if self . counter . value == 0 : return 0.0 else : elapsed = time ( ) - self . start time return self . counter . value / elapsed
def send metric ( self , name , metric ) : config = SERIALIZER CONFIG [ class name ( metric ) ] mmap ( self . buffered send metric , self . serialize metric ( metric , name , config [ 'keys' ] , config [ 'serialized type' ] ) ) if hasattr ( metric , 'snapshot' ) and config . get ( 'snapshot keys' ) : mmap ( self . buffered send metric , self . serialize metric ( metric . snapshot , name , config [ 'snapshot keys' ] , config [ 'serialized type' ] ) )
def serialize metric ( self , metric , m name , keys , m type ) : return [ self . format metric string ( m name , getattr ( metric , key ) , m type ) for key in keys ]
def format metric string ( self , name , value , m type ) : # NOTE(romcheg): This serialized metric template is based on #                statsd's documentation. template = '{name}:{value}|{m type}\n' if self . prefix : name = "{prefix}.{m name}" . format ( prefix = self . prefix , m name = name ) return template . format ( name = name , value = value , m type = m type )
def buffered send metric ( self , metric str ) : self . batch count += 1 self . batch buffer += metric str # NOTE(romcheg): Send metrics if the number of metrics in the buffer #                has reached the threshold for sending. if self . batch count >= self . batch size : self . send ( )
def json safe ( data ) : if not hasattr ( data , 'encode' ) : try : data = data . decode ( 'utf-8' ) except Unicode Decode Error : raise Value Error ( 'Expected valid UTF8 for JSON data, got %r' % ( data , ) ) return data
def solve ( grid ) : clauses = sudoku clauses ( ) for i in range ( 1 , 10 ) : for j in range ( 1 , 10 ) : d = grid [ i - 1 ] [ j - 1 ] # For each digit already known, a clause (with one literal). # Note: #     We could also remove all variables for the known cells #     altogether (which would be more efficient).  However, for #     the sake of simplicity, we decided not to do that. if d : clauses . append ( [ v ( i , j , d ) ] ) # solve the SAT problem sol = set ( pycosat . solve ( clauses ) ) def read cell ( i , j ) : # return the digit of cell i, j according to the solution for d in range ( 1 , 10 ) : if v ( i , j , d ) in sol : return d for i in range ( 1 , 10 ) : for j in range ( 1 , 10 ) : grid [ i - 1 ] [ j - 1 ] = read cell ( i , j )
def view ( injector ) : handler = create handler ( View , injector ) apply http methods ( handler , injector ) return injector . let ( as view = handler . as view )
def form view ( injector ) : handler = create handler ( Form View , injector ) apply form methods ( handler , injector ) return injector . let ( as view = handler . as view )
def method view ( injector ) : handler = create handler ( Method View ) apply http methods ( handler , injector ) return injector . let ( as view = handler . as view )
def api view ( injector ) : handler = create handler ( API View , injector ) apply http methods ( handler , injector ) apply api view methods ( handler , injector ) return injector . let ( as view = handler . as view )
def generic api view ( injector ) : handler = create handler ( Generic API View , injector ) apply http methods ( handler , injector ) apply api view methods ( handler , injector ) apply generic api view methods ( handler , injector ) return injector . let ( as view = handler . as view )
def model view set ( injector ) : handler = create handler ( Model View Set , injector ) apply api view methods ( handler , injector ) apply generic api view methods ( handler , injector ) apply model view set methods ( handler , injector ) return injector . let ( as viewset = lambda : handler )
def stream from fd ( fd , loop ) : reader = asyncio . Stream Reader ( loop = loop ) protocol = asyncio . Stream Reader Protocol ( reader , loop = loop ) waiter = asyncio . futures . Future ( loop = loop ) transport = Unix File Descriptor Transport ( loop = loop , fileno = fd , protocol = protocol , waiter = waiter , ) try : yield from waiter except Exception : transport . close ( ) if loop . get debug ( ) : logger . debug ( "Read fd %r connected: (%r, %r)" , fd , transport , protocol ) return reader , transport
def read ready ( self ) : try : data = os . read ( self . fileno , self . max size ) except Interrupted Error : # No worries ;) pass except OS Error as exc : # Some OS-level problem, crash. self . fatal error ( exc , "Fatal read error on file descriptor read" ) else : if data : self . protocol . data received ( data ) else : # We reached end-of-file. if self . loop . get debug ( ) : logger . info ( "%r was closed by the kernel" , self ) self . closing = False self . pause reading ( ) self . loop . call soon ( self . protocol . eof received ) self . loop . call soon ( self . call connection lost , None )
def close ( self , error = None ) : self . closing = True self . pause reading ( ) self . loop . call soon ( self . call connection lost , error )
def watch ( self , path , flags , * , alias = None ) : if alias is None : alias = path if alias in self . requests : raise Value Error ( "A watch request is already scheduled for alias %s" % alias ) self . requests [ alias ] = ( path , flags ) if self . fd is not None : # We've started, register the watch immediately. self . setup watch ( alias , path , flags )
def unwatch ( self , alias ) : if alias not in self . descriptors : raise Value Error ( "Unknown watch alias %s; current set is %r" % ( alias , list ( self . descriptors . keys ( ) ) ) ) wd = self . descriptors [ alias ] errno = Lib C . inotify rm watch ( self . fd , wd ) if errno != 0 : raise IO Error ( "Failed to close watcher %d: errno=%d" % ( wd , errno ) ) del self . descriptors [ alias ] del self . requests [ alias ] del self . aliases [ wd ]
def setup watch ( self , alias , path , flags ) : assert alias not in self . descriptors , "Registering alias %s twice!" % alias wd = Lib C . inotify add watch ( self . fd , path , flags ) if wd < 0 : raise IO Error ( "Error setting up watch on %s with flags %s: wd=%s" % ( path , flags , wd ) ) self . descriptors [ alias ] = wd self . aliases [ wd ] = alias
def setup ( self , loop ) : self . loop = loop self . fd = Lib C . inotify init ( ) for alias , ( path , flags ) in self . requests . items ( ) : self . setup watch ( alias , path , flags ) # We pass ownership of the fd to the transport; it will close it. self . stream , self . transport = yield from aioutils . stream from fd ( self . fd , loop )
def touch ( self ) : assert not self . has responded self . trigger ( event . TOUCH , message = self )
def success ( self ) : if self . interval == 0.0 : return self . short interval -= self . short unit self . long interval -= self . long unit self . short interval = max ( self . short interval , Decimal ( 0 ) ) self . long interval = max ( self . long interval , Decimal ( 0 ) ) self . update interval ( )
def failure ( self ) : self . short interval += self . short unit self . long interval += self . long unit self . short interval = min ( self . short interval , self . max short timer ) self . long interval = min ( self . long interval , self . max long timer ) self . update interval ( )
def close ( self ) : for conn in self . conns . values ( ) : conn . close ( ) self . redist periodic . stop ( ) if self . query periodic is not None : self . query periodic . stop ( )
def set max in flight ( self , max in flight ) : assert isinstance ( max in flight , int ) self . max in flight = max in flight if max in flight == 0 : # set RDY 0 to all connections for conn in itervalues ( self . conns ) : if conn . rdy > 0 : logger . debug ( '[%s:%s] rdy: %d -> 0' , conn . id , self . name , conn . rdy ) self . send rdy ( conn , 0 ) self . total rdy = 0 else : self . need rdy redistributed = True self . redistribute rdy state ( )
def score function ( self , x , W ) : # need refector if ( self . svm kernel == 'polynomial kernel' or self . svm kernel == 'gaussian kernel' or self . svm kernel == 'soft polynomial kernel' or self . svm kernel == 'soft gaussian kernel' ) : x = x [ 1 : ] score = np . sign ( np . sum ( self . sv alpha * self . sv Y * utility . Kernel . kernel matrix x X ( self , x , self . sv X ) ) + self . sv avg b ) else : score = np . sign ( np . inner ( x , W ) ) return score
def score function ( self , x , W ) : score = super ( Binary Classifier , self ) . score function ( x , W ) if score >= 0.5 : score = 1.0 else : score = - 1.0 return score
def score function ( self , x , W ) : # need refector score = self . sign * np . sign ( x [ self . feature index ] - self . theta ) return score
def set feature transform ( self , mode = 'polynomial' , degree = 1 ) : if self . status != 'load train data' : print ( "Please load train data first." ) return self . train X self . feature transform mode = mode self . feature transform degree = degree self . train X = self . train X [ : , 1 : ] self . train X = utility . Dataset Loader . feature transform ( self . train X , self . feature transform mode , self . feature transform degree ) return self . train X
def score function ( self , x , W ) : # need refector score = self . theta ( np . inner ( x , W ) ) return score
def clean up ( fastq pairs , clear ) : # Find unpaired fastq files unpaired fastq = [ f for f in os . listdir ( "." ) if f . endswith ( " U.fastq.gz" ) ] # Remove unpaired fastq files, if any for fpath in unpaired fastq : os . remove ( fpath ) # Expected output to assess whether it is safe to remove temporary input expected out = [ f for f in os . listdir ( "." ) if f . endswith ( " trim.fastq.gz" ) ] if clear == "true" and len ( expected out ) == 2 : for fq in fastq pairs : # Get real path of fastq files, following symlinks rp = os . path . realpath ( fq ) logger . debug ( "Removing temporary fastq file path: {}" . format ( rp ) ) if re . match ( ".*/work/.{2}/.{30}/.*" , rp ) : os . remove ( rp )
def check required files ( self ) : if not os . path . exists ( self . trace file ) : raise eh . Inspection Error ( "The provided trace file could not be " "opened: {}" . format ( self . trace file ) ) if not os . path . exists ( self . log file ) : raise eh . Inspection Error ( "The .nextflow.log files could not be " "opened. Are you sure you are in a " "nextflow project directory?" )
def clear inspect ( self ) : self . trace info = defaultdict ( list ) self . process tags = { } self . process stats = { } self . samples = [ ] self . stored ids = [ ] self . stored log ids = [ ] self . time start = None self . time stop = None self . execution command = None self . nextflow version = None self . abort cause = None self . c = 0 # Clean up of tag running status for p in self . processes . values ( ) : p [ "barrier" ] = "W" for i in [ "submitted" , "finished" , "failed" , "retry" ] : p [ i ] = set ( )
def update barrier status ( self ) : with open ( self . log file ) as fh : for line in fh : # Exit barrier update after session abort signal if "Session aborted" in line : return if "<<< barrier arrive" in line : # Retrieve process name from string process m = re . match ( ".*process: (.*)\)" , line ) if process m : process = process m . group ( 1 ) # Updates process channel to complete if process in self . processes : self . processes [ process ] [ "barrier" ] = "C"
def display overview ( self ) : stay alive = True self . screen = curses . initscr ( ) self . screen . keypad ( True ) self . screen . nodelay ( - 1 ) curses . cbreak ( ) curses . noecho ( ) curses . start color ( ) self . screen lines = self . screen . getmaxyx ( ) [ 0 ] # self.screen width = self.screen.getmaxyx()[1] try : while stay alive : # Provide functionality to certain keybindings self . curses keybindings ( ) # Updates main inspector attributes self . update inspection ( ) # Display curses interface self . flush overview ( ) sleep ( self . refresh rate ) except File Not Found Error : sys . stderr . write ( colored print ( "ERROR: nextflow log and/or trace files are no longer " "reachable!" , "red bold" ) ) except Exception as e : sys . stderr . write ( str ( e ) ) finally : curses . nocbreak ( ) self . screen . keypad ( 0 ) curses . echo ( ) curses . endwin ( )
def updown ( self , direction ) : if direction == "up" and self . top line != 0 : self . top line -= 1 elif direction == "down" and self . screen . getmaxyx ( ) [ 0 ] + self . top line <= self . content lines + 3 : self . top line += 1
def rightleft ( self , direction ) : if direction == "left" and self . padding != 0 : self . padding -= 1 if direction == "right" and self . screen . getmaxyx ( ) [ 1 ] + self . padding < self . max width : self . padding += 1
def get run hash ( self ) : # Get name and path of the pipeline from the log file pipeline path = get nextflow filepath ( self . log file ) # Get hash from the entire pipeline file pipeline hash = hashlib . md5 ( ) with open ( pipeline path , "rb" ) as fh : for chunk in iter ( lambda : fh . read ( 4096 ) , b"" ) : pipeline hash . update ( chunk ) # Get hash from the current working dir and hostname workdir = self . workdir . encode ( "utf8" ) hostname = socket . gethostname ( ) . encode ( "utf8" ) hardware addr = str ( uuid . getnode ( ) ) . encode ( "utf8" ) dir hash = hashlib . md5 ( workdir + hostname + hardware addr ) return pipeline hash . hexdigest ( ) + dir hash . hexdigest ( )
def write report data ( self ) : json plot = self . get plot data ( ) json table = self . get table data ( ) json dic = { * * json plot , * * json table } with open ( ".report.json" , "w" ) as json report : json report . write ( json . dumps ( json dic , separators = ( "," , ":" ) ) )
def build header ( self ) : logger . debug ( "===============" ) logger . debug ( "Building header" ) logger . debug ( "===============" ) self . template += hs . header
def build footer ( self ) : logger . debug ( "===============" ) logger . debug ( "Building header" ) logger . debug ( "===============" ) self . template += fs . footer
def set status channels ( self ) : status inst = pc . Status Compiler ( template = "status compiler" ) report inst = pc . Report Compiler ( template = "report compiler" ) # Compile status channels from pipeline process status channels = [ ] for p in [ p for p in self . processes ] : if not any ( [ isinstance ( p , x ) for x in self . skip class ] ) : status channels . extend ( p . status strs ) if not status channels : logger . debug ( "No status channels found. Skipping status compiler" "process" ) return logger . debug ( "Setting status channels: {}" . format ( status channels ) ) # Check for duplicate channels. Raise exception if found. if len ( status channels ) != len ( set ( status channels ) ) : raise eh . Process Error ( "Duplicate status channels detected. Please ensure that " "the 'status channels' attributes of each process are " "unique. Here are the status channels:\n\n{}" . format ( ", " . join ( status channels ) ) ) status inst . set compiler channels ( status channels ) report channels = [ "REPORT {}" . format ( x . lstrip ( "STATUS " ) ) for x in status channels ] report inst . set compiler channels ( report channels ) self . processes . extend ( [ status inst , report inst ] )
def export directives ( self ) : directives json = { } # Skip first init process for p in self . processes [ 1 : ] : directives json [ p . template ] = p . directives # Flush params json to stdout sys . stdout . write ( json . dumps ( directives json ) )
def get report id ( self ) : if self . watch : # Searches for the first occurence of the nextflow pipeline # file name in the .nextflow.log file pipeline path = get nextflow filepath ( self . log file ) # Get hash from the entire pipeline file pipeline hash = hashlib . md5 ( ) with open ( pipeline path , "rb" ) as fh : for chunk in iter ( lambda : fh . read ( 4096 ) , b"" ) : pipeline hash . update ( chunk ) # Get hash from the current working dir and hostname workdir = os . getcwd ( ) . encode ( "utf8" ) hostname = socket . gethostname ( ) . encode ( "utf8" ) hardware addr = str ( uuid . getnode ( ) ) . encode ( "utf8" ) dir hash = hashlib . md5 ( workdir + hostname + hardware addr ) return pipeline hash . hexdigest ( ) + dir hash . hexdigest ( ) else : with open ( self . report file ) as fh : report json = json . loads ( fh . read ( ) ) metadata = report json [ "data" ] [ "results" ] [ 0 ] [ "nf Metadata" ] try : report id = metadata [ "script Id" ] + metadata [ "session Id" ] except Key Error : raise eh . Report Error ( "Incomplete or corrupt report JSON file " "missing the 'script Id' and/or 'session Id' " "metadata information" ) return report id
def update log watch ( self ) : # Check the size stamp of the tracefile. Only proceed with the parsing # if it changed from the previous size. size stamp = os . path . getsize ( self . log file ) self . trace retry = 0 if size stamp and size stamp == self . log sizestamp : return else : logger . debug ( "Updating log size stamp to: {}" . format ( size stamp ) ) self . log sizestamp = size stamp self . update pipeline status ( )
def map w to data ( self ) : # assign W to the next best data sample self . Wmapped index = vq ( self . data , self . W ) self . Wmapped = np . zeros ( self . W . shape ) # do not directly assign, i.e. Wdist = self.data[:,sel] # as self might be unsorted (in non ascending order) # -> sorting sel would screw the matching to W if # self.data is stored as a hdf5 table (see h5py) for i , s in enumerate ( self . Wmapped index ) : self . Wmapped [ : , i ] = self . data [ : , s ]
def median filter ( X , M = 8 ) : for i in range ( X . shape [ 1 ] ) : X [ : , i ] = filters . median filter ( X [ : , i ] , size = M ) return X
def compute gaussian krnl ( M ) : g = signal . gaussian ( M , M // 3. , sym = True ) G = np . dot ( g . reshape ( - 1 , 1 ) , g . reshape ( 1 , - 1 ) ) G [ M // 2 : , : M // 2 ] = - G [ M // 2 : , : M // 2 ] G [ : M // 2 , M // 2 : ] = - G [ : M // 2 , M // 2 : ] return G
def compute ssm ( X , metric = "seuclidean" ) : D = distance . pdist ( X , metric = metric ) D = distance . squareform ( D ) D /= D . max ( ) return 1 - D
def pick peaks ( nc , L = 16 ) : offset = nc . mean ( ) / 20. nc = filters . gaussian filter1d ( nc , sigma = 4 ) # Smooth out nc th = filters . median filter ( nc , size = L ) + offset #th = filters.gaussian filter(nc, sigma=L/2., mode="nearest") + offset peaks = [ ] for i in range ( 1 , nc . shape [ 0 ] - 1 ) : # is it a peak? if nc [ i - 1 ] < nc [ i ] and nc [ i ] > nc [ i + 1 ] : # is it above the threshold? if nc [ i ] > th [ i ] : peaks . append ( i ) #plt.plot(nc) #plt.plot(th) #for peak in peaks: #plt.axvline(peak) #plt.show() return peaks
def gaussian filter ( X , M = 8 , axis = 0 ) : for i in range ( X . shape [ axis ] ) : if axis == 1 : X [ : , i ] = filters . gaussian filter ( X [ : , i ] , sigma = M / 2. ) elif axis == 0 : X [ i , : ] = filters . gaussian filter ( X [ i , : ] , sigma = M / 2. ) return X
def compute nc ( X ) : N = X . shape [ 0 ] # nc = np.sum(np.diff(X, axis=0), axis=1) # Difference between SF's nc = np . zeros ( N ) for i in range ( N - 1 ) : nc [ i ] = distance . euclidean ( X [ i , : ] , X [ i + 1 , : ] ) # Normalize nc += np . abs ( nc . min ( ) ) nc /= float ( nc . max ( ) ) return nc
def pick peaks ( nc , L = 16 , offset denom = 0.1 ) : offset = nc . mean ( ) * float ( offset denom ) th = filters . median filter ( nc , size = L ) + offset #th = filters.gaussian filter(nc, sigma=L/2., mode="nearest") + offset #import pylab as plt #plt.plot(nc) #plt.plot(th) #plt.show() # th = np.ones(nc.shape[0]) * nc.mean() - 0.08 peaks = [ ] for i in range ( 1 , nc . shape [ 0 ] - 1 ) : # is it a peak? if nc [ i - 1 ] < nc [ i ] and nc [ i ] > nc [ i + 1 ] : # is it above the threshold? if nc [ i ] > th [ i ] : peaks . append ( i ) return peaks
def embedded space ( X , m , tau = 1 ) : N = X . shape [ 0 ] - int ( np . ceil ( m ) ) Y = np . zeros ( ( N , int ( np . ceil ( X . shape [ 1 ] * m ) ) ) ) for i in range ( N ) : # print X[i:i+m,:].flatten().shape, w, X.shape # print Y[i,:].shape rem = int ( ( m % 1 ) * X . shape [ 1 ] ) # Reminder for float m Y [ i , : ] = np . concatenate ( ( X [ i : i + int ( m ) , : ] . flatten ( ) , X [ i + int ( m ) , : rem ] ) ) return Y
def plot one track ( file struct , est times , est labels , boundaries id , labels id , title = None ) : import matplotlib . pyplot as plt # Set up the boundaries id bid lid = boundaries id if labels id is not None : bid lid += " + " + labels id try : # Read file jam = jams . load ( file struct . ref file ) ann = jam . search ( namespace = 'segment .*' ) [ 0 ] ref inters , ref labels = ann . to interval values ( ) # To times ref times = utils . intervals to times ( ref inters ) all boundaries = [ ref times , est times ] all labels = [ ref labels , est labels ] algo ids = [ "GT" , bid lid ] except : logging . warning ( "No references found in %s. Not plotting groundtruth" % file struct . ref file ) all boundaries = [ est times ] all labels = [ est labels ] algo ids = [ bid lid ] N = len ( all boundaries ) # Index the labels to normalize them for i , labels in enumerate ( all labels ) : all labels [ i ] = mir eval . util . index labels ( labels ) [ 0 ] # Get color map cm = plt . get cmap ( 'gist rainbow' ) max label = max ( max ( labels ) for labels in all labels ) figsize = ( 8 , 4 ) plt . figure ( 1 , figsize = figsize , dpi = 120 , facecolor = 'w' , edgecolor = 'k' ) for i , boundaries in enumerate ( all boundaries ) : color = "b" if i == 0 : color = "g" for b in boundaries : plt . axvline ( b , i / float ( N ) , ( i + 1 ) / float ( N ) , color = color ) if labels id is not None : labels = all labels [ i ] inters = utils . times to intervals ( boundaries ) for label , inter in zip ( labels , inters ) : plt . axvspan ( inter [ 0 ] , inter [ 1 ] , ymin = i / float ( N ) , ymax = ( i + 1 ) / float ( N ) , alpha = 0.6 , color = cm ( label / float ( max label ) ) ) plt . axhline ( i / float ( N ) , color = "k" , linewidth = 1 ) # Format plot plot formatting ( title , os . path . basename ( file struct . audio file ) , algo ids , all boundaries [ 0 ] [ - 1 ] , N , None )
def get dataset files ( in path ) : # Get audio files audio files = [ ] for ext in ds config . audio exts : audio files += glob . glob ( os . path . join ( in path , ds config . audio dir , "*" + ext ) ) # Make sure directories exist utils . ensure dir ( os . path . join ( in path , ds config . features dir ) ) utils . ensure dir ( os . path . join ( in path , ds config . estimations dir ) ) utils . ensure dir ( os . path . join ( in path , ds config . references dir ) ) # Get the file structs file structs = [ ] for audio file in audio files : file structs . append ( File Struct ( audio file ) ) # Sort by audio file name file structs = sorted ( file structs , key = lambda file struct : file struct . audio file ) return file structs
def get dataset file ( self , dir , ext ) : audio file ext = "." + self . audio file . split ( "." ) [ - 1 ] base file = os . path . basename ( self . audio file ) . replace ( audio file ext , ext ) return os . path . join ( self . ds path , dir , base file )
def write features ( self ) : out json = collections . Ordered Dict ( ) try : # Only save the necessary information self . read features ( ) except ( Wrong Features Format Error , Features Not Found , No Features File Error ) : # We need to create the file or overwite it # Metadata out json = collections . Ordered Dict ( { "metadata" : { "versions" : { "librosa" : librosa . version , "msaf" : msaf . version , "numpy" : np . version } , "timestamp" : datetime . datetime . today ( ) . strftime ( "%Y/%m/%d %H:%M:%S" ) } } ) # Global parameters out json [ "globals" ] = { "dur" : self . dur , "sample rate" : self . sr , "hop length" : self . hop length , "audio file" : self . file struct . audio file } # Beats out json [ "est beats" ] = self . est beats times . tolist ( ) out json [ "est beatsync times" ] = self . est beatsync times . tolist ( ) if self . ann beats times is not None : out json [ "ann beats" ] = self . ann beats times . tolist ( ) out json [ "ann beatsync times" ] = self . ann beatsync times . tolist ( ) except Feature Params Error : # We have other features in the file, simply add these ones with open ( self . file struct . features file ) as f : out json = json . load ( f ) finally : # Specific parameters of the current features out json [ self . get id ( ) ] = { } out json [ self . get id ( ) ] [ "params" ] = { } for param name in self . get param names ( ) : value = getattr ( self , param name ) # Check for special case of functions if hasattr ( value , ' call ' ) : value = value . name else : value = str ( value ) out json [ self . get id ( ) ] [ "params" ] [ param name ] = value # Actual features out json [ self . get id ( ) ] [ "framesync" ] = self . framesync features . tolist ( ) out json [ self . get id ( ) ] [ "est beatsync" ] = self . est beatsync features . tolist ( ) if self . ann beatsync features is not None : out json [ self . get id ( ) ] [ "ann beatsync" ] = self . ann beatsync features . tolist ( ) # Save it with open ( self . file struct . features file , "w" ) as f : json . dump ( out json , f , indent = 2 )
def compute framesync times ( self ) : self . framesync times = librosa . core . frames to time ( np . arange ( self . framesync features . shape [ 0 ] ) , self . sr , self . hop length )
def preprocess ( self , valid features = [ "pcp" , "tonnetz" , "mfcc" , "cqt" , "tempogram" ] ) : # Use specific feature if self . feature str not in valid features : raise Runtime Error ( "Feature %s in not valid for algorithm: %s " "(valid features are %s)." % ( self . feature str , name , valid features ) ) else : try : F = self . features . features except Key Error : raise Runtime Error ( "Feature %s in not supported by MSAF" % ( self . feature str ) ) return F
def process ( in path , annot beats = False , feature = "mfcc" , framesync = False , boundaries id = "gt" , labels id = None , n jobs = 4 , config = None ) : results file = "results sweep bounds E%s labels E%s.csv" % ( boundaries id , labels id ) if labels id == "cnmf3" or boundaries id == "cnmf3" : config = io . get configuration ( feature , annot beats , framesync , boundaries id , labels id ) hh = range ( 15 , 33 ) RR = range ( 15 , 40 ) ranks = range ( 3 , 6 ) RR labels = range ( 11 , 12 ) ranks labels = range ( 6 , 7 ) all results = pd . Data Frame ( ) for rank in ranks : for h in hh : for R in RR : for rank labels in ranks labels : for R labels in RR labels : config [ "h" ] = h config [ "R" ] = R config [ "rank" ] = rank config [ "rank labels" ] = rank labels config [ "R labels" ] = R labels config [ "features" ] = None # Run process msaf . run . process ( in path , n jobs = n jobs , boundaries id = boundaries id , labels id = labels id , config = config ) # Compute evaluations results = msaf . eval . process ( in path , boundaries id , labels id , save = True , n jobs = n jobs , config = config ) # Save avg results new columns = { "config h" : h , "config R" : R , "config rank" : rank , "config R labels" : R labels , "config rank labels" : rank labels } results = results . append ( [ new columns ] , ignore index = True ) all results = all results . append ( results . mean ( ) , ignore index = True ) all results . to csv ( results file ) elif labels id is None and boundaries id == "sf" : config = io . get configuration ( feature , annot beats , framesync , boundaries id , labels id ) MM = range ( 20 , 32 ) mm = range ( 3 , 4 ) kk = np . arange ( 0.03 , 0.1 , 0.01 ) Mpp = range ( 16 , 32 ) ott = np . arange ( 0.02 , 0.1 , 0.01 ) all results = pd . Data Frame ( ) for k in kk : for ot in ott : for m in mm : for M in MM : for Mp in Mpp : config [ "M gaussian" ] = M config [ "m embedded" ] = m config [ "k nearest" ] = k config [ "Mp adaptive" ] = Mp config [ "offset thres" ] = ot config [ "features" ] = None # Run process msaf . run . process ( in path , n jobs = n jobs , boundaries id = boundaries id , labels id = labels id , config = config ) # Compute evaluations results = msaf . eval . process ( in path , boundaries id , labels id , save = True , n jobs = n jobs , config = config ) # Save avg results new columns = { "config M" : M , "config m" : m , "config k" : k , "config Mp" : Mp , "config ot" : ot } results = results . append ( [ new columns ] , ignore index = True ) all results = all results . append ( results . mean ( ) , ignore index = True ) all results . to csv ( results file ) else : logging . error ( "Can't sweep parameters for %s algorithm. " "Implement me! :D" )
def main ( ) : parser = argparse . Argument Parser ( description = "Runs the speficied algorithm(s) on the MSAF " "formatted dataset." , formatter class = argparse . Argument Defaults Help Formatter ) parser . add argument ( "in path" , action = "store" , help = "Input dataset" ) parser . add argument ( "-f" , action = "store" , dest = "feature" , default = "pcp" , type = str , help = "Type of features" , choices = [ "pcp" , "tonnetz" , "mfcc" , "cqt" , "tempogram" ] ) parser . add argument ( "-b" , action = "store true" , dest = "annot beats" , help = "Use annotated beats" , default = False ) parser . add argument ( "-fs" , action = "store true" , dest = "framesync" , help = "Use frame-synchronous features" , default = False ) parser . add argument ( "-bid" , action = "store" , help = "Boundary algorithm identifier" , dest = "boundaries id" , default = "gt" , choices = [ "gt" ] + io . get all boundary algorithms ( ) ) parser . add argument ( "-lid" , action = "store" , help = "Label algorithm identifier" , dest = "labels id" , default = None , choices = io . get all label algorithms ( ) ) parser . add argument ( "-j" , action = "store" , dest = "n jobs" , default = 4 , type = int , help = "The number of threads to use" ) args = parser . parse args ( ) start time = time . time ( ) # Run the algorithm(s) process ( args . in path , annot beats = args . annot beats , feature = args . feature , framesync = args . framesync , boundaries id = args . boundaries id , labels id = args . labels id , n jobs = args . n jobs ) # Done! logging . info ( "Done! Took %.2f seconds." % ( time . time ( ) - start time ) )
def main ( ) : parser = argparse . Argument Parser ( description = "Runs the speficied algorithm(s) on the input file and " "the results using the MIREX format." , formatter class = argparse . Argument Defaults Help Formatter ) parser . add argument ( "-bid" , action = "store" , help = "Boundary algorithm identifier" , dest = "boundaries id" , default = msaf . config . default bound id , choices = [ "gt" ] + msaf . io . get all boundary algorithms ( ) ) parser . add argument ( "-lid" , action = "store" , help = "Label algorithm identifier" , dest = "labels id" , default = msaf . config . default label id , choices = msaf . io . get all label algorithms ( ) ) parser . add argument ( "-i" , action = "store" , dest = "in file" , help = "Input audio file" ) parser . add argument ( "-o" , action = "store" , dest = "out file" , help = "Output file with the results" , default = "out.txt" ) args = parser . parse args ( ) start time = time . time ( ) # Setup the logger logging . basic Config ( format = '%(asctime)s: %(levelname)s: %(message)s' , level = logging . INFO ) # Run MSAF params = { "annot beats" : False , "feature" : "cqt" , "framesync" : False , "boundaries id" : args . boundaries id , "labels id" : args . labels id , "n jobs" : 1 , "hier" : False , "sonify bounds" : False , "plot" : False } res = msaf . run . process ( args . in file , * * params ) msaf . io . write mirex ( res [ 0 ] , res [ 1 ] , args . out file ) # Done! logging . info ( "Done! Took %.2f seconds." % ( time . time ( ) - start time ) )
def compute all features ( file struct , framesync ) : for feature id in msaf . features registry : logging . info ( "Computing %s for file %s" % ( feature id , file struct . audio file ) ) feats = Features . select features ( feature id , file struct , False , framesync ) feats . features
def process ( in path , out file , n jobs , framesync ) : if os . path . isfile ( in path ) : # Single file mode # Get (if they exitst) or compute features file struct = msaf . io . File Struct ( in path ) file struct . features file = out file compute all features ( file struct , framesync ) else : # Collection mode file structs = msaf . io . get dataset files ( in path ) # Call in parallel return Parallel ( n jobs = n jobs ) ( delayed ( compute all features ) ( file struct , framesync ) for file struct in file structs )
def main ( ) : parser = argparse . Argument Parser ( description = "Extracts a set of features from a given dataset " "or audio file and saves them into the 'features' folder of " "the dataset or the specified single file." , formatter class = argparse . Argument Defaults Help Formatter ) parser . add argument ( "in path" , action = "store" , help = "Input dataset dir or audio file" ) parser . add argument ( "-j" , action = "store" , dest = "n jobs" , type = int , help = "Number of jobs (only for collection mode)" , default = 4 ) parser . add argument ( "-o" , action = "store" , dest = "out file" , type = str , help = "Output file (only for single file mode)" , default = "out.json" ) parser . add argument ( "-d" , action = "store" , dest = "ds name" , default = "*" , help = "The prefix of the dataset to use " "(e.g. Isophonics, SALAMI)" ) parser . add argument ( "-fs" , action = "store true" , dest = "framesync" , help = "Use frame-synchronous features" , default = False ) args = parser . parse args ( ) start time = time . time ( ) # Setup the logger logging . basic Config ( format = '%(asctime)s: %(levelname)s: %(message)s' , level = logging . INFO ) # Run the main process process ( args . in path , out file = args . out file , n jobs = args . n jobs , framesync = args . framesync ) # Done! logging . info ( "Done! Took %.2f seconds." % ( time . time ( ) - start time ) )
def gaussian cost ( X ) : d , n = X . shape if n < 2 : return 0 sigma = np . var ( X , axis = 1 , ddof = 1 ) cost = - 0.5 * d * n * np . log ( 2. * np . pi ) - 0.5 * ( n - 1. ) * np . sum ( sigma ) return cost
def lognormalize ( F , floor = 0.1 , min db = - 80 ) : assert min db < 0 F = min max normalize ( F , floor = floor ) F = np . abs ( min db ) * np . log10 ( F ) # Normalize from min db to 0 return F
def min max normalize ( F , floor = 0.001 ) : F += - F . min ( ) + floor F = F / F . max ( axis = 0 ) return F
def get time frames ( dur , anal ) : n frames = get num frames ( dur , anal ) return np . linspace ( 0 , dur , num = n frames )
def remove empty segments ( times , labels ) : assert len ( times ) - 1 == len ( labels ) inters = times to intervals ( times ) new inters = [ ] new labels = [ ] for inter , label in zip ( inters , labels ) : if inter [ 0 ] < inter [ 1 ] : new inters . append ( inter ) new labels . append ( label ) return intervals to times ( np . asarray ( new inters ) ) , new labels
def distance ( self , idx ) : if scipy . sparse . issparse ( self . data ) : step = self . data . shape [ 1 ] else : step = 50000 d = np . zeros ( ( self . data . shape [ 1 ] ) ) if idx == - 1 : # set vec to origin if idx=-1 vec = np . zeros ( ( self . data . shape [ 0 ] , 1 ) ) if scipy . sparse . issparse ( self . data ) : vec = scipy . sparse . csc matrix ( vec ) else : vec = self . data [ : , idx : idx + 1 ] self . logger . info ( 'compute distance to node ' + str ( idx ) ) # slice data into smaller chunks for idx start in range ( 0 , self . data . shape [ 1 ] , step ) : if idx start + step > self . data . shape [ 1 ] : idx end = self . data . shape [ 1 ] else : idx end = idx start + step d [ idx start : idx end ] = self . distfunc ( self . data [ : , idx start : idx end ] , vec ) self . logger . info ( 'completed:' + str ( idx end / ( self . data . shape [ 1 ] / 100.0 ) ) + "%" ) return d
def run kmeans ( self , X , K ) : w X = vq . whiten ( X ) means , dist = vq . kmeans ( w X , K , iter = 100 ) labels , dist = vq . vq ( w X , means ) return means , labels
def compute bic ( self , D , means , labels , K , R ) : D = vq . whiten ( D ) Rn = D . shape [ 0 ] M = D . shape [ 1 ] if R == K : return 1 # Maximum likelihood estimate (MLE) mle var = 0 for k in range ( len ( means ) ) : X = D [ np . argwhere ( labels == k ) ] X = X . reshape ( ( X . shape [ 0 ] , X . shape [ - 1 ] ) ) for x in X : mle var += distance . euclidean ( x , means [ k ] ) #print x, means[k], mle var mle var /= float ( R - K ) # Log-likelihood of the data l D = - Rn / 2. * np . log ( 2 * np . pi ) - ( Rn * M ) / 2. * np . log ( mle var ) - ( Rn - K ) / 2. + Rn * np . log ( Rn ) - Rn * np . log ( R ) # Params of BIC p = ( K - 1 ) + M * K + mle var #print "BIC:", l D, p, R, K # Return the bic return l D - p / 2. * np . log ( R )
def magnitude ( X ) : r = np . real ( X ) i = np . imag ( X ) return np . sqrt ( r * r + i * i )
def compute ffmc2d ( X ) : # 2d-fft fft2 = scipy . fftpack . fft2 ( X ) # Magnitude fft2m = magnitude ( fft2 ) # FF Tshift and flatten fftshift = scipy . fftpack . fftshift ( fft2m ) . flatten ( ) #cmap = plt.cm.get cmap('hot') #plt.imshow(np.log1p(scipy.fftpack.fftshift(fft2m)).T, interpolation="nearest", #    aspect="auto", cmap=cmap) #plt.show() # Take out redundant components return fftshift [ : fftshift . shape [ 0 ] // 2 + 1 ]
def compute labels ( X , rank , R , bound idxs , niter = 300 ) : try : F , G = cnmf ( X , rank , niter = niter , hull = False ) except : return [ 1 ] label frames = filter activation matrix ( G . T , R ) label frames = np . asarray ( label frames , dtype = int ) #labels = [label frames[0]] labels = [ ] bound inters = zip ( bound idxs [ : - 1 ] , bound idxs [ 1 : ] ) for bound inter in bound inters : if bound inter [ 1 ] - bound inter [ 0 ] <= 0 : labels . append ( np . max ( label frames ) + 1 ) else : labels . append ( most frequent ( label frames [ bound inter [ 0 ] : bound inter [ 1 ] ] ) ) #print bound inter, labels[-1] #labels.append(label frames[-1]) return labels
def filter activation matrix ( G , R ) : #import pylab as plt #plt.imshow(G, interpolation="nearest", aspect="auto") #plt.show() idx = np . argmax ( G , axis = 1 ) max idx = np . arange ( G . shape [ 0 ] ) max idx = ( max idx , idx . flatten ( ) ) G [ : , : ] = 0 G [ max idx ] = idx + 1 # TODO: Order matters? G = np . sum ( G , axis = 1 ) G = median filter ( G [ : , np . newaxis ] , R ) return G . flatten ( )
def main ( ) : args = command line ( ) translate = partial ( translator , args . source , args . dest , version = ' ' . join ( [ version , build ] ) ) return source ( spool ( set task ( translate , translit = args . translit ) ) , args . text )
def print table ( language ) : table = translation table ( language ) for code , name in sorted ( table . items ( ) , key = operator . itemgetter ( 0 ) ) : print ( u'{language:<8} {name:\u3000<20}' . format ( name = name , language = code ) ) return None
def disable ( self ) : if not self . active : return None # Disable current mock engine self . mock engine . disable ( ) # Disable engine state self . active = False
def get setting ( connection , key ) : if key in connection . settings dict : return connection . settings dict [ key ] else : return getattr ( settings , key )
def as sql ( self , compiler , connection ) : sql , params = super ( Decrypted Col , self ) . as sql ( compiler , connection ) sql = self . target . get decrypt sql ( connection ) % ( sql , self . target . get cast sql ( ) ) return sql , params
def pre save ( self , model instance , add ) : if self . original : original value = getattr ( model instance , self . original ) setattr ( model instance , self . attname , original value ) return super ( Hash Mixin , self ) . pre save ( model instance , add )
def get col ( self , alias , output field = None ) : if output field is None : output field = self if alias != self . model . meta . db table or output field != self : return Decrypted Col ( alias , self , output field ) else : return self . cached col
def get placeholder ( self , value = None , compiler = None , connection = None ) : return self . encrypt sql . format ( get setting ( connection , 'PUBLIC PGP KEY' ) )
def attach to tree ( self ) : for clade in self . tree . find clades ( ) : if clade . up is not None : clade . branch length interpolator . merger cost = self . cost
def optimize Tc ( self ) : from scipy . optimize import minimize scalar initial Tc = self . Tc def cost ( Tc ) : self . set Tc ( Tc ) return - self . total LH ( ) sol = minimize scalar ( cost , bounds = [ ttconf . TINY NUMBER , 10.0 ] ) if "success" in sol and sol [ "success" ] : self . set Tc ( sol [ 'x' ] ) else : self . logger ( "merger models:optimze Tc: optimization of coalescent time scale failed: " + str ( sol ) , 0 , warn = True ) self . set Tc ( initial Tc . y , T = initial Tc . x )
def prepare nodes ( self ) : self . tree . root . up = None self . tree . root . bad branch = self . tree . root . bad branch if hasattr ( self . tree . root , 'bad branch' ) else False internal node count = 0 for clade in self . tree . get nonterminals ( order = 'preorder' ) : # parents first internal node count += 1 if clade . name is None : clade . name = "NODE " + format ( self . internal node count , '07d' ) self . internal node count += 1 for c in clade . clades : if c . is terminal ( ) : c . bad branch = c . bad branch if hasattr ( c , 'bad branch' ) else False c . up = clade for clade in self . tree . get nonterminals ( order = 'postorder' ) : # parents first clade . bad branch = all ( [ c . bad branch for c in clade ] ) self . calc dist2root ( ) self . internal node count = max ( internal node count , self . internal node count )
def create gtr ( params ) : model = params . gtr gtr params = params . gtr params if model == 'infer' : gtr = GTR . standard ( 'jc' , alphabet = 'aa' if params . aa else 'nuc' ) else : try : kwargs = { } if gtr params is not None : for param in gtr params : keyval = param . split ( '=' ) if len ( keyval ) != 2 : continue if keyval [ 0 ] in [ 'pis' , 'pi' , 'Pi' , 'Pis' ] : keyval [ 0 ] = 'pi' keyval [ 1 ] = list ( map ( float , keyval [ 1 ] . split ( ',' ) ) ) elif keyval [ 0 ] not in [ 'alphabet' ] : keyval [ 1 ] = float ( keyval [ 1 ] ) kwargs [ keyval [ 0 ] ] = keyval [ 1 ] else : print ( "GTR params are not specified. Creating GTR model with default parameters" ) gtr = GTR . standard ( model , * * kwargs ) infer gtr = False except : print ( "Could not create GTR model from input arguments. Using default (Jukes-Cantor 1969)" ) gtr = GTR . standard ( 'jc' , alphabet = 'aa' if params . aa else 'nuc' ) infer gtr = False return gtr
def read if vcf ( params ) : ref = None aln = params . aln fixed pi = None if hasattr ( params , 'aln' ) and params . aln is not None : if any ( [ params . aln . lower ( ) . endswith ( x ) for x in [ '.vcf' , '.vcf.gz' ] ] ) : if not params . vcf reference : print ( "ERROR: a reference Fasta is required with VCF-format alignments" ) return - 1 compress seq = read vcf ( params . aln , params . vcf reference ) sequences = compress seq [ 'sequences' ] ref = compress seq [ 'reference' ] aln = sequences if not hasattr ( params , 'gtr' ) or params . gtr == "infer" : #if not specified, set it: alpha = alphabets [ 'aa' ] if params . aa else alphabets [ 'nuc' ] fixed pi = [ ref . count ( base ) / len ( ref ) for base in alpha ] if fixed pi [ - 1 ] == 0 : fixed pi [ - 1 ] = 0.05 fixed pi = [ v - 0.01 for v in fixed pi ] return aln , ref , fixed pi
def scan homoplasies ( params ) : if assure tree ( params , tmp dir = 'homoplasy tmp' ) : return 1 gtr = create gtr ( params ) ########################################################################### ### READ IN VCF ########################################################################### #sets ref and fixed pi to None if not VCF aln , ref , fixed pi = read if vcf ( params ) is vcf = True if ref is not None else False ########################################################################### ### ANCESTRAL RECONSTRUCTION ########################################################################### treeanc = Tree Anc ( params . tree , aln = aln , ref = ref , gtr = gtr , verbose = 1 , fill overhangs = True ) if treeanc . aln is None : # if alignment didn't load, exit return 1 if is vcf : L = len ( ref ) + params . const else : L = treeanc . aln . get alignment length ( ) + params . const N seq = len ( treeanc . aln ) N tree = treeanc . tree . count terminals ( ) if params . rescale != 1.0 : for n in treeanc . tree . find clades ( ) : n . branch length *= params . rescale n . mutation length = n . branch length print ( "read alignment from file %s with %d sequences of length %d" % ( params . aln , N seq , L ) ) print ( "read tree from file %s with %d leaves" % ( params . tree , N tree ) ) print ( "\ninferring ancestral sequences..." ) ndiff = treeanc . infer ancestral sequences ( 'ml' , infer gtr = params . gtr == 'infer' , marginal = False , fixed pi = fixed pi ) print ( "...done." ) if ndiff == ttconf . ERROR : # if reconstruction failed, exit print ( "Something went wrong during ancestral reconstruction, please check your input files." , file = sys . stderr ) return 1 else : print ( "...done." ) if is vcf : treeanc . recover var ambigs ( ) ########################################################################### ### analysis of reconstruction ########################################################################### from collections import defaultdict from scipy . stats import poisson offset = 0 if params . zero based else 1 if params . drms : DRM info = read in DR Ms ( params . drms , offset ) drms = DRM info [ 'DR Ms' ] # construct dictionaries gathering mutations and positions mutations = defaultdict ( list ) positions = defaultdict ( list ) terminal mutations = defaultdict ( list ) for n in treeanc . tree . find clades ( ) : if n . up is None : continue if len ( n . mutations ) : for ( a , pos , d ) in n . mutations : if '-' not in [ a , d ] and 'N' not in [ a , d ] : mutations [ ( a , pos + offset , d ) ] . append ( n ) positions [ pos + offset ] . append ( n ) if n . is terminal ( ) : for ( a , pos , d ) in n . mutations : if '-' not in [ a , d ] and 'N' not in [ a , d ] : terminal mutations [ ( a , pos + offset , d ) ] . append ( n ) # gather homoplasic mutations by strain mutation by strain = defaultdict ( list ) for n in treeanc . tree . get terminals ( ) : for a , pos , d in n . mutations : if pos + offset in positions and len ( positions [ pos + offset ] ) > 1 : if '-' not in [ a , d ] and 'N' not in [ a , d ] : mutation by strain [ n . name ] . append ( [ ( a , pos + offset , d ) , len ( positions [ pos ] ) ] ) # total branch length is the expected number of substitutions # corrected branch length is the expected number of observable substitutions # (probability of an odd number of substitutions at a particular site) total branch length = treeanc . tree . total branch length ( ) corrected branch length = np . sum ( [ np . exp ( - x . branch length ) * np . sinh ( x . branch length ) for x in treeanc . tree . find clades ( ) ] ) corrected terminal branch length = np . sum ( [ np . exp ( - x . branch length ) * np . sinh ( x . branch length ) for x in treeanc . tree . get terminals ( ) ] ) expected mutations = L * corrected branch length expected terminal mutations = L * corrected terminal branch length # make histograms and sum mutations in different categories multiplicities = np . bincount ( [ len ( x ) for x in mutations . values ( ) ] ) total mutations = np . sum ( [ len ( x ) for x in mutations . values ( ) ] ) multiplicities terminal = np . bincount ( [ len ( x ) for x in terminal mutations . values ( ) ] ) terminal mutation count = np . sum ( [ len ( x ) for x in terminal mutations . values ( ) ] ) multiplicities positions = np . bincount ( [ len ( x ) for x in positions . values ( ) ] ) multiplicities positions [ 0 ] = L - np . sum ( multiplicities positions ) ########################################################################### ### Output the distribution of times particular mutations are observed ########################################################################### print ( "\n The TOTAL tree length is %1.3e and %d mutations were observed." % ( total branch length , total mutations ) ) print ( "Of these %d mutations," % total mutations + "" . join ( [ '\n\t - %d occur %d times' % ( n , mi ) for mi , n in enumerate ( multiplicities ) if n ] ) ) # additional optional output this for terminal mutations only if params . detailed : print ( "\n The TERMINAL branch length is %1.3e and %d mutations were observed." % ( corrected terminal branch length , terminal mutation count ) ) print ( "Of these %d mutations," % terminal mutation count + "" . join ( [ '\n\t - %d occur %d times' % ( n , mi ) for mi , n in enumerate ( multiplicities terminal ) if n ] ) ) ########################################################################### ### Output the distribution of times mutations at particular positions are observed ########################################################################### print ( "\n Of the %d positions in the genome," % L + "" . join ( [ '\n\t - %d were hit %d times (expected %1.2f)' % ( n , mi , L * poisson . pmf ( mi , 1.0 * total mutations / L ) ) for mi , n in enumerate ( multiplicities positions ) if n ] ) ) # compare that distribution to a Poisson distribution with the same mean p = poisson . pmf ( np . arange ( 10 * multiplicities positions . max ( ) ) , 1.0 * total mutations / L ) print ( "\nlog-likelihood difference to Poisson distribution with same mean: %1.3e" % ( - L * np . sum ( p * np . log ( p + 1e-100 ) ) + np . sum ( multiplicities positions * np . log ( p [ : len ( multiplicities positions ) ] + 1e-100 ) ) ) ) ########################################################################### ### Output the mutations that are observed most often ########################################################################### if params . drms : print ( "\n\n The ten most homoplasic mutations are:\n\tmut\tmultiplicity\t DRM details (gene drug A Amut)" ) mutations sorted = sorted ( mutations . items ( ) , key = lambda x : len ( x [ 1 ] ) - 0.1 * x [ 0 ] [ 1 ] / L , reverse = True ) for mut , val in mutations sorted [ : params . n ] : if len ( val ) > 1 : print ( "\t%s%d%s\t%d\t%s" % ( mut [ 0 ] , mut [ 1 ] , mut [ 2 ] , len ( val ) , " " . join ( [ drms [ mut [ 1 ] ] [ 'gene' ] , drms [ mut [ 1 ] ] [ 'drug' ] , drms [ mut [ 1 ] ] [ 'alt base' ] [ mut [ 2 ] ] ] ) if mut [ 1 ] in drms else "" ) ) else : break else : print ( "\n\n The ten most homoplasic mutations are:\n\tmut\tmultiplicity" ) mutations sorted = sorted ( mutations . items ( ) , key = lambda x : len ( x [ 1 ] ) - 0.1 * x [ 0 ] [ 1 ] / L , reverse = True ) for mut , val in mutations sorted [ : params . n ] : if len ( val ) > 1 : print ( "\t%s%d%s\t%d" % ( mut [ 0 ] , mut [ 1 ] , mut [ 2 ] , len ( val ) ) ) else : break # optional output specifically for mutations on terminal branches if params . detailed : if params . drms : print ( "\n\n The ten most homoplasic mutation on terminal branches are:\n\tmut\tmultiplicity\t DRM details (gene drug A Amut)" ) terminal mutations sorted = sorted ( terminal mutations . items ( ) , key = lambda x : len ( x [ 1 ] ) - 0.1 * x [ 0 ] [ 1 ] / L , reverse = True ) for mut , val in terminal mutations sorted [ : params . n ] : if len ( val ) > 1 : print ( "\t%s%d%s\t%d\t%s" % ( mut [ 0 ] , mut [ 1 ] , mut [ 2 ] , len ( val ) , " " . join ( [ drms [ mut [ 1 ] ] [ 'gene' ] , drms [ mut [ 1 ] ] [ 'drug' ] , drms [ mut [ 1 ] ] [ 'alt base' ] [ mut [ 2 ] ] ] ) if mut [ 1 ] in drms else "" ) ) else : break else : print ( "\n\n The ten most homoplasic mutation on terminal branches are:\n\tmut\tmultiplicity" ) terminal mutations sorted = sorted ( terminal mutations . items ( ) , key = lambda x : len ( x [ 1 ] ) - 0.1 * x [ 0 ] [ 1 ] / L , reverse = True ) for mut , val in terminal mutations sorted [ : params . n ] : if len ( val ) > 1 : print ( "\t%s%d%s\t%d" % ( mut [ 0 ] , mut [ 1 ] , mut [ 2 ] , len ( val ) ) ) else : break ########################################################################### ### Output strains that have many homoplasic mutations ########################################################################### # TODO: add statistical criterion if params . detailed : if params . drms : print ( "\n\n Taxons that carry positions that mutated elsewhere in the tree:\n\ttaxon name\t#of homoplasic mutations\t# DRM" ) mutation by strain sorted = sorted ( mutation by strain . items ( ) , key = lambda x : len ( x [ 1 ] ) , reverse = True ) for name , val in mutation by strain sorted [ : params . n ] : if len ( val ) : print ( "\t%s\t%d\t%d" % ( name , len ( val ) , len ( [ mut for mut , l in val if mut [ 1 ] in drms ] ) ) ) else : print ( "\n\n Taxons that carry positions that mutated elsewhere in the tree:\n\ttaxon name\t#of homoplasic mutations" ) mutation by strain sorted = sorted ( mutation by strain . items ( ) , key = lambda x : len ( x [ 1 ] ) , reverse = True ) for name , val in mutation by strain sorted [ : params . n ] : if len ( val ) : print ( "\t%s\t%d" % ( name , len ( val ) ) ) return 0
def delta function ( cls , x pos , weight = 1. , min width = MIN INTEGRATION PEAK ) : distribution = cls ( x pos , 0. , is log = True , min width = min width ) distribution . weight = weight return distribution
def multiply ( dists ) : if not all ( [ isinstance ( k , Distribution ) for k in dists ] ) : raise Not Implemented Error ( "Can only multiply Distribution objects" ) n delta = np . sum ( [ k . is delta for k in dists ] ) min width = np . max ( [ k . min width for k in dists ] ) if n delta > 1 : raise Arithmetic Error ( "Cannot multiply more than one delta functions!" ) elif n delta == 1 : delta dist ii = np . where ( [ k . is delta for k in dists ] ) [ 0 ] [ 0 ] delta dist = dists [ delta dist ii ] new xpos = delta dist . peak pos new weight = np . prod ( [ k . prob ( new xpos ) for k in dists if k != delta dist ii ] ) * delta dist . weight res = Distribution . delta function ( new xpos , weight = new weight , min width = min width ) else : new xmin = np . max ( [ k . xmin for k in dists ] ) new xmax = np . min ( [ k . xmax for k in dists ] ) x vals = np . unique ( np . concatenate ( [ k . x for k in dists ] ) ) x vals = x vals [ ( x vals > new xmin - TINY NUMBER ) & ( x vals < new xmax + TINY NUMBER ) ] y vals = np . sum ( [ k . call ( x vals ) for k in dists ] , axis = 0 ) peak = y vals . min ( ) ind = ( y vals - peak ) < BIG NUMBER / 1000 n points = ind . sum ( ) if n points == 0 : print ( "ERROR in distribution multiplication: Distributions do not overlap" ) x vals = [ 0 , 1 ] y vals = [ BIG NUMBER , BIG NUMBER ] res = Distribution ( x vals , y vals , is log = True , min width = min width , kind = 'linear' ) elif n points == 1 : res = Distribution . delta function ( x vals [ 0 ] ) else : res = Distribution ( x vals [ ind ] , y vals [ ind ] , is log = True , min width = min width , kind = 'linear' , assume sorted = True ) return res
def timetree likelihood ( self ) : LH = 0 for node in self . tree . find clades ( order = 'preorder' ) : # sum the likelihood contributions of all branches if node . up is None : # root node continue LH -= node . branch length interpolator ( node . branch length ) # add the root sequence LH and return if self . aln : LH += self . gtr . sequence log LH ( self . tree . root . cseq , pattern multiplicity = self . multiplicity ) return LH
def min interp ( interp object ) : try : return interp object . x [ interp object ( interp object . x ) . argmin ( ) ] except Exception as e : s = "Cannot find minimum of the interpolation object" + str ( interp object . x ) + "Minimal x: " + str ( interp object . x . min ( ) ) + "Maximal x: " + str ( interp object . x . max ( ) ) raise e
def median interp ( interp object ) : new grid = np . sort ( np . concatenate ( [ interp object . x [ : - 1 ] + 0.1 * ii * np . diff ( interp object . x ) for ii in range ( 10 ) ] ) . flatten ( ) ) tmp prop = np . exp ( - ( interp object ( new grid ) - interp object . y . min ( ) ) ) tmp cumsum = np . cumsum ( 0.5 * ( tmp prop [ 1 : ] + tmp prop [ : - 1 ] ) * np . diff ( new grid ) ) median index = min ( len ( tmp cumsum ) - 3 , max ( 2 , np . searchsorted ( tmp cumsum , tmp cumsum [ - 1 ] * 0.5 ) + 1 ) ) return new grid [ median index ]
def close ( self ) : self . client . close ( ) self . client = None self . connected = False self . logger . debug ( 'Connection closed.' )
def receive ( self ) : start = 0 while True : idx = self . buffer . find ( INST TERM . encode ( ) , start ) if idx != - 1 : # instruction was fully received! line = self . buffer [ : idx + 1 ] . decode ( ) self . buffer = self . buffer [ idx + 1 : ] self . logger . debug ( 'Received instruction: %s' % line ) return line else : start = len ( self . buffer ) # we are still waiting for instruction termination buf = self . client . recv ( BUF LEN ) if not buf : # No data recieved, connection lost?! self . close ( ) self . logger . debug ( 'Failed to receive instruction. Closing.' ) return None self . buffer . extend ( buf )
def send ( self , data ) : self . logger . debug ( 'Sending data: %s' % data ) self . client . sendall ( data . encode ( ) )
def send instruction ( self , instruction ) : self . logger . debug ( 'Sending instruction: %s' % str ( instruction ) ) return self . send ( instruction . encode ( ) )
def handshake ( self , protocol = 'vnc' , width = 1024 , height = 768 , dpi = 96 , audio = None , video = None , image = None , * * kwargs ) : if protocol not in PROTOCOLS : self . logger . debug ( 'Invalid protocol: %s' % protocol ) raise Guacamole Error ( 'Cannot start Handshake. Missing protocol.' ) if audio is None : audio = list ( ) if video is None : video = list ( ) if image is None : image = list ( ) # 1. Send 'select' instruction self . logger . debug ( 'Send `select` instruction.' ) self . send instruction ( Instruction ( 'select' , protocol ) ) # 2. Receive `args` instruction instruction = self . read instruction ( ) self . logger . debug ( 'Expecting `args` instruction, received: %s' % str ( instruction ) ) if not instruction : self . close ( ) raise Guacamole Error ( 'Cannot establish Handshake. Connection Lost!' ) if instruction . opcode != 'args' : self . close ( ) raise Guacamole Error ( 'Cannot establish Handshake. Expected opcode `args`, ' 'received `%s` instead.' % instruction . opcode ) # 3. Respond with size, audio & video support self . logger . debug ( 'Send `size` instruction (%s, %s, %s)' % ( width , height , dpi ) ) self . send instruction ( Instruction ( 'size' , width , height , dpi ) ) self . logger . debug ( 'Send `audio` instruction (%s)' % audio ) self . send instruction ( Instruction ( 'audio' , * audio ) ) self . logger . debug ( 'Send `video` instruction (%s)' % video ) self . send instruction ( Instruction ( 'video' , * video ) ) self . logger . debug ( 'Send `image` instruction (%s)' % image ) self . send instruction ( Instruction ( 'image' , * image ) ) # 4. Send `connect` instruction with proper values connection args = [ kwargs . get ( arg . replace ( '-' , ' ' ) , '' ) for arg in instruction . args ] self . logger . debug ( 'Send `connect` instruction (%s)' % connection args ) self . send instruction ( Instruction ( 'connect' , * connection args ) ) # 5. Receive ``ready`` instruction, with client ID. instruction = self . read instruction ( ) self . logger . debug ( 'Expecting `ready` instruction, received: %s' % str ( instruction ) ) if instruction . opcode != 'ready' : self . logger . warning ( 'Expected `ready` instruction, received: %s instead' ) if instruction . args : self . id = instruction . args [ 0 ] self . logger . debug ( 'Established connection with client id: %s' % self . id ) self . logger . debug ( 'Handshake completed.' ) self . connected = True
def class url ( cls ) : base = 'v{0}' . format ( getattr ( cls , 'RESOURCE VERSION' , '1' ) ) return "/{0}/{1}" . format ( base , class to api name ( cls . class name ( ) ) )
def instance url ( self ) : id = self . get ( self . ID ATTR ) base = self . class url ( ) if id : return '/' . join ( [ base , six . text type ( id ) ] ) else : raise Exception ( 'Could not determine which URL to request: %s instance ' 'has invalid ID: %r' % ( type ( self ) . name , id ) , self . ID ATTR )
def parent object ( self ) : from . import types parent klass = types . get ( self . parent job model . split ( '.' ) [ 1 ] ) return parent klass . retrieve ( self . parent job id , client = self . client )
def ask for credentials ( ) : print msg ( 'Please enter your Solve Bio credentials' ) domain = raw input ( 'Domain (e.g. <domain>.solvebio.com): ' ) # Check to see if this domain supports password authentication try : account = client . request ( 'get' , '/p/accounts/{}' . format ( domain ) ) auth = account [ 'authentication' ] except : raise Solve Error ( 'Invalid domain: {}' . format ( domain ) ) # Account must support password-based login if auth . get ( 'login' ) or auth . get ( 'SAML' , { } ) . get ( 'simple login' ) : email = raw input ( 'Email: ' ) password = getpass . getpass ( 'Password (typing will be hidden): ' ) return ( domain , email , password ) else : print msg ( 'Your domain uses Single Sign-On (SSO). ' 'Please visit https://{}.solvebio.com/settings/security ' 'for instructions on how to log in.' . format ( domain ) ) sys . exit ( 1 )
def print user ( user ) : email = user [ 'email' ] domain = user [ 'account' ] [ 'domain' ] role = user [ 'role' ] print ( 'You are logged-in to the "{0}" domain ' 'as {1} with role {2}.' . format ( domain , email , role ) )
def range ( self , chromosome , start , stop , exact = False ) : return self . clone ( filters = [ Genomic Filter ( chromosome , start , stop , exact ) ] )
def position ( self , chromosome , position , exact = False ) : return self . clone ( filters = [ Genomic Filter ( chromosome , position , exact = exact ) ] )
def main ( argv = sys . argv [ 1 : ] ) : parser = Solve Argument Parser ( ) args = parser . parse solvebio args ( argv ) if args . api host : solvebio . api host = args . api host if args . api key : solvebio . api key = args . api key if not solvebio . api key : # If nothing is set (via command line or environment) # look in local credentials try : from . credentials import get credentials solvebio . api key = get credentials ( ) except : pass # Update the client host and token client . set host ( ) client . set token ( ) return args . func ( args )
def construct from ( cls , values , * * kwargs ) : instance = cls ( values . get ( cls . ID ATTR ) , * * kwargs ) instance . refresh from ( values ) return instance
def logout ( self ) : if self . oauth client secret : try : oauth token = flask . request . cookies [ self . TOKEN COOKIE NAME ] # Revoke the token requests . post ( urljoin ( self . api host , self . OAUTH2 REVOKE TOKEN PATH ) , data = { 'client id' : self . oauth client id , 'client secret' : self . oauth client secret , 'token' : oauth token } ) except : pass response = flask . redirect ( '/' ) self . clear cookies ( response ) return response
def child object ( self ) : from . import types child klass = types . get ( self . task type . split ( '.' ) [ 1 ] ) return child klass . retrieve ( self . task id , client = self . client )
def row to dict ( self , row , allele , alternate alleles ) : def variant sbid ( * * kwargs ) : """Generates a Solve Bio variant ID (SBID).""" return '{build}-{chromosome}-{start}-{stop}-{allele}' . format ( * * kwargs ) . upper ( ) if allele == '.' : # Try to use the ref, if '.' is supplied for alt. allele = row . REF or allele genomic coordinates = { 'build' : self . genome build , 'chromosome' : row . CHROM , 'start' : row . POS , 'stop' : row . POS + len ( row . REF ) - 1 } # Solve Bio standard variant format variant sbid = variant sbid ( allele = allele , * * genomic coordinates ) return { 'genomic coordinates' : genomic coordinates , 'variant' : variant sbid , 'allele' : allele , 'row id' : row . ID , 'reference allele' : row . REF , 'alternate alleles' : alternate alleles , 'info' : self . parse info ( row . INFO ) , 'qual' : row . QUAL , 'filter' : row . FILTER }
def save ( self , path ) : rep = "" for host in self . hosts . keys ( ) : attrs = self . hosts [ host ] rep = rep + "machine " + host + "\n\tlogin " + six . text type ( attrs [ 0 ] ) + "\n" if attrs [ 1 ] : rep = rep + "account " + six . text type ( attrs [ 1 ] ) rep = rep + "\tpassword " + six . text type ( attrs [ 2 ] ) + "\n" for macro in self . macros . keys ( ) : rep = rep + "macdef " + macro + "\n" for line in self . macros [ macro ] : rep = rep + line rep = rep + "\n" f = open ( path , 'w' ) f . write ( rep ) f . close ( )
def build row ( cells , padding , begin , sep , end ) : pad = " " * padding padded cells = [ pad + cell + pad for cell in cells ] # Solve Bio: we're only displaying Key-Value tuples (dimension of 2). #  enforce that we don't wrap lines by setting a max #  limit on row width which is equal to TTY COLS (see printing) rendered cells = ( begin + sep . join ( padded cells ) + end ) . rstrip ( ) if len ( rendered cells ) > TTY COLS : if not cells [ - 1 ] . endswith ( " " ) and not cells [ - 1 ] . endswith ( "-" ) : terminating str = " ... " else : terminating str = "" rendered cells = "{0}{1}{2}" . format ( rendered cells [ : TTY COLS - len ( terminating str ) - 1 ] , terminating str , end ) return rendered cells
def build line ( colwidths , padding , begin , fill , sep , end ) : cells = [ fill * ( w + 2 * padding ) for w in colwidths ] return build row ( cells , 0 , begin , sep , end )
def mediawiki cell attrs ( row , colaligns ) : alignment = { "left" : '' , "right" : 'align="right"| ' , "center" : 'align="center"| ' , "decimal" : 'align="right"| ' } row2 = [ alignment [ a ] + c for c , a in zip ( row , colaligns ) ] return row2
def format table ( fmt , headers , rows , colwidths , colaligns ) : lines = [ ] hidden = fmt . with header hide if headers else fmt . without header hide pad = fmt . padding headerrow = fmt . headerrow if fmt . headerrow else fmt . datarow if fmt . lineabove and "lineabove" not in hidden : lines . append ( build line ( colwidths , pad , * fmt . lineabove ) ) if headers : lines . append ( build row ( headers , pad , * headerrow ) ) if fmt . linebelowheader and "linebelowheader" not in hidden : begin , fill , sep , end = fmt . linebelowheader if fmt . usecolons : segs = [ line segment with colons ( fmt . linebelowheader , a , w + 2 * pad ) for w , a in zip ( colwidths , colaligns ) ] lines . append ( build row ( segs , 0 , begin , sep , end ) ) else : lines . append ( build line ( colwidths , pad , * fmt . linebelowheader ) ) if rows and fmt . linebetweenrows and "linebetweenrows" not in hidden : # initial rows with a line below for row in rows [ : - 1 ] : lines . append ( build row ( row , pad , * fmt . datarow ) ) lines . append ( build line ( colwidths , pad , * fmt . linebetweenrows ) ) # the last row without a line below lines . append ( build row ( rows [ - 1 ] , pad , * fmt . datarow ) ) else : for row in rows : lines . append ( build row ( row , pad , * fmt . datarow ) ) if fmt . linebelow and "linebelow" not in hidden : lines . append ( build line ( colwidths , pad , * fmt . linebelow ) ) return "\n" . join ( lines )
def evaluate ( self , data = None , data type = 'string' , is list = False ) : payload = { 'data' : data , 'expression' : self . expr , 'data type' : data type , 'is list' : is list } res = self . client . post ( '/v1/evaluate' , payload ) return res [ 'result' ]
def get column types ( self , data ) : columns = list ( zip longest ( * data ) ) return [ self . get column type ( column ) for column in columns ]
def get column type ( self , column ) : type values = [ TYPES [ self . get type ( v ) ] for v in column ] inverse types = { v : k for k , v in TYPES . items ( ) } return inverse types [ max ( type values ) ]
def get type ( self , value ) : if value is None : return type ( None ) elif type ( value ) in int types : return int elif type ( value ) in float types : return float elif isinstance ( value , binary type ) : return binary type else : return text type
def adapter ( data , headers , table format = None , preserve whitespace = False , * * kwargs ) : keys = ( 'floatfmt' , 'numalign' , 'stralign' , 'showindex' , 'disable numparse' ) tkwargs = { 'tablefmt' : table format } tkwargs . update ( filter dict by key ( kwargs , keys ) ) if table format in supported markup formats : tkwargs . update ( numalign = None , stralign = None ) tabulate . PRESERVE WHITESPACE = preserve whitespace return iter ( tabulate . tabulate ( data , headers , * * tkwargs ) . split ( '\n' ) )
def user config file ( self ) : return os . path . join ( get user config dir ( self . app name , self . app author ) , self . filename )
def system config files ( self ) : return [ os . path . join ( f , self . filename ) for f in get system config dirs ( self . app name , self . app author ) ]
def additional files ( self ) : return [ os . path . join ( f , self . filename ) for f in self . additional dirs ]
def truncate string ( value , max width = None ) : if isinstance ( value , text type ) and max width is not None and len ( value ) > max width : return value [ : max width ] return value
def filter dict by key ( d , keys ) : return { k : v for k , v in d . items ( ) if k in keys }
def replace ( s , replace ) : for r in replace : s = s . replace ( * r ) return s
def adapter ( data , headers , * * kwargs ) : for row in chain ( ( headers , ) , data ) : yield "\t" . join ( ( replace ( r , ( ( '\n' , r'\n' ) , ( '\t' , r'\t' ) ) ) for r in row ) )
def call and exit ( self , cmd , shell = True ) : sys . exit ( subprocess . call ( cmd , shell = shell ) )
def call in sequence ( self , cmds , shell = True ) : for cmd in cmds : if subprocess . call ( cmd , shell = shell ) == 1 : sys . exit ( 1 )
def apply options ( self , cmd , options = ( ) ) : for option in ( self . default cmd options + options ) : cmd = self . apply option ( cmd , option , active = getattr ( self , option , False ) ) return cmd
def apply option ( self , cmd , option , active = True ) : return re . sub ( r'{{{}\:(?P<option>[^}}]*)}}' . format ( option ) , '\g<option>' if active else '' , cmd )
def initialize options ( self ) : self . branch = 'master' self . fix = False super ( lint , self ) . initialize options ( )
def run ( self ) : cmd = 'pep8radius {branch} {{fix: --in-place}}{{verbose: -vv}}' cmd = cmd . format ( branch = self . branch ) self . call and exit ( self . apply options ( cmd , ( 'fix' , ) ) )
def run ( self ) : cmds = ( self . clean docs cmd , self . html docs cmd , self . view docs cmd ) self . call in sequence ( cmds )
def get separator ( num , sep title , sep character , sep length ) : left divider length = right divider length = sep length if isinstance ( sep length , tuple ) : left divider length , right divider length = sep length left divider = sep character * left divider length right divider = sep character * right divider length title = sep title . format ( n = num + 1 ) return "{left divider}[ {title} ]{right divider}\n" . format ( left divider = left divider , right divider = right divider , title = title )
def format row ( headers , row ) : formatted row = [ ' | ' . join ( field ) for field in zip ( headers , row ) ] return '\n' . join ( formatted row )
def adapter ( data , headers , * * kwargs ) : keys = ( 'sep title' , 'sep character' , 'sep length' ) return vertical table ( data , headers , * * filter dict by key ( kwargs , keys ) )
def adapter ( data , headers , table format = 'csv' , * * kwargs ) : keys = ( 'dialect' , 'delimiter' , 'doublequote' , 'escapechar' , 'quotechar' , 'quoting' , 'skipinitialspace' , 'strict' ) if table format == 'csv' : delimiter = ',' elif table format == 'csv-tab' : delimiter = '\t' else : raise Value Error ( 'Invalid table format specified.' ) ckwargs = { 'delimiter' : delimiter , 'lineterminator' : '' } ckwargs . update ( filter dict by key ( kwargs , keys ) ) l = linewriter ( ) writer = csv . writer ( l , * * ckwargs ) writer . writerow ( headers ) yield l . line for row in data : l . reset ( ) writer . writerow ( row ) yield l . line
def adapter ( data , headers , table format = None , * * kwargs ) : keys = ( 'title' , ) table = table format handler [ table format ] t = table ( [ headers ] + list ( data ) , * * filter dict by key ( kwargs , keys ) ) dimensions = terminaltables . width and alignment . max dimensions ( t . table data , t . padding left , t . padding right ) [ : 3 ] for r in t . gen table ( * dimensions ) : yield u'' . join ( r )
def to dict ( self ) : # all the attibutes defined by PKCS#11 all attributes = Py KCS11 . CKA . keys ( ) # only use the integer values and not the strings like 'CKM RSA PKCS' all attributes = [ attr for attr in all attributes if isinstance ( attr , int ) ] # all the attributes of the object attributes = self . session . get Attribute Value ( self , all attributes ) dico = dict ( ) for key , attr in zip ( all attributes , attributes ) : if attr is None : continue if key == CKA CLASS : dico [ Py KCS11 . CKA [ key ] ] = Py KCS11 . CKO [ attr ] elif key == CKA CERTIFICATE TYPE : dico [ Py KCS11 . CKA [ key ] ] = Py KCS11 . CKC [ attr ] elif key == CKA KEY TYPE : dico [ Py KCS11 . CKA [ key ] ] = Py KCS11 . CKK [ attr ] else : dico [ Py KCS11 . CKA [ key ] ] = attr return dico
def to dict ( self ) : dico = dict ( ) for field in self . fields . keys ( ) : if field == "flags" : dico [ field ] = self . flags2text ( ) elif field == "state" : dico [ field ] = self . state2text ( ) else : dico [ field ] = eval ( "self." + field ) return dico
def insert img ( qr img , icon img = None , factor = 4 , icon box = None , static dir = None ) : img w , img h = qr img . size size w = int ( img w ) / int ( factor ) size h = int ( img h ) / int ( factor ) try : # load icon from current dir icon fp = os . path . join ( icon img ) if static dir : # load icon from app's static dir icon fp = os . path . join ( static dir , icon img ) if icon img . split ( "://" ) [ 0 ] in [ "http" , "https" , "ftp" ] : icon fp = Bytes IO ( urlopen ( icon img ) . read ( ) ) # download icon icon = Image . open ( icon fp ) except : return qr img icon w , icon h = icon . size icon w = size w if icon w > size w else icon w icon h = size h if icon h > size h else icon h icon = icon . resize ( ( int ( icon w ) , int ( icon h ) ) , Image . ANTIALIAS ) icon = icon . convert ( "RGBA" ) left = int ( ( img w - icon w ) / 2 ) top = int ( ( img h - icon h ) / 2 ) icon box = ( int ( icon box [ 0 ] ) , int ( icon box [ 1 ] ) ) if icon box else ( left , top ) qr img . paste ( im = icon , box = icon box , mask = icon ) return qr img
def biweekly helper ( self ) : self . num = 14 mycount = self . repeat biweekly ( ) if mycount : if self . event . is chunk ( ) and min ( mycount ) not in xrange ( 1 , 8 ) : mycount = chunk fill out first week ( self . year , self . month , mycount , self . event , diff = self . event . start end diff , ) for k , v in mycount . items ( ) : for item in v : self . count [ k ] . append ( item )
def user ( context , user id , update role , add institute , remove admin , remove institute ) : adapter = context . obj [ 'adapter' ] user obj = adapter . user ( user id ) if not user obj : LOG . warning ( "User %s could not be found" , user id ) context . abort ( ) existing roles = set ( user obj . get ( 'roles' , [ ] ) ) if update role : if not update role in user obj [ 'roles' ] : existing roles = set ( user obj [ 'roles' ] ) existing roles . add ( update role ) LOG . info ( "Adding role %s to user" , update role ) else : LOG . warning ( "User already have role %s" , update role ) if remove admin : try : existing roles . remove ( 'admin' ) LOG . info ( "Removing admin rights from user %s" , user id ) except Key Error as err : LOG . info ( "User %s does not have admin rights" , user id ) user obj [ 'roles' ] = list ( existing roles ) existing institutes = set ( user obj . get ( 'institutes' , [ ] ) ) for institute id in add institute : institute obj = adapter . institute ( institute id ) if not institute obj : LOG . warning ( "Institute %s could not be found" , institute id ) else : existing institutes . add ( institute id ) LOG . info ( "Adding institute %s to user" , institute id ) for institute id in remove institute : try : existing institutes . remove ( institute id ) LOG . info ( "Removing institute %s from user" , institute id ) except Key Error as err : LOG . info ( "User does not have access to institute %s" , institute id ) user obj [ 'institutes' ] = list ( existing institutes ) updated user = adapter . update user ( user obj )
def variants ( institute id , case name ) : page = int ( request . form . get ( 'page' , 1 ) ) institute obj , case obj = institute and case ( store , institute id , case name ) variant type = request . args . get ( 'variant type' , 'clinical' ) # Update filter settings if Clinical Filter was requested default panels = [ ] for panel in case obj [ 'panels' ] : if panel . get ( 'is default' ) : default panels . append ( panel [ 'panel name' ] ) request . form . get ( 'gene panels' ) if bool ( request . form . get ( 'clinical filter' ) ) : clinical filter = Multi Dict ( { 'variant type' : 'clinical' , 'region annotations' : [ 'exonic' , 'splicing' ] , 'functional annotations' : SEVERE SO TERMS , 'clinsig' : [ 4 , 5 ] , 'clinsig confident always returned' : True , 'gnomad frequency' : str ( institute obj [ 'frequency cutoff' ] ) , 'variant type' : 'clinical' , 'gene panels' : default panels } ) if ( request . method == "POST" ) : if bool ( request . form . get ( 'clinical filter' ) ) : form = Filters Form ( clinical filter ) form . csrf token = request . args . get ( 'csrf token' ) else : form = Filters Form ( request . form ) else : form = Filters Form ( request . args ) # populate available panel choices available panels = case obj . get ( 'panels' , [ ] ) + [ { 'panel name' : 'hpo' , 'display name' : 'HPO' } ] panel choices = [ ( panel [ 'panel name' ] , panel [ 'display name' ] ) for panel in available panels ] form . gene panels . choices = panel choices # upload gene panel if symbol file exists if ( request . files ) : file = request . files [ form . symbol file . name ] if request . files and file and file . filename != '' : log . debug ( "Upload file request files: {0}" . format ( request . files . to dict ( ) ) ) try : stream = io . String IO ( file . stream . read ( ) . decode ( 'utf-8' ) , newline = None ) except Unicode Decode Error as error : flash ( "Only text files are supported!" , 'warning' ) return redirect ( request . referrer ) hgnc symbols set = set ( form . hgnc symbols . data ) log . debug ( "Symbols prior to upload: {0}" . format ( hgnc symbols set ) ) new hgnc symbols = controllers . upload panel ( store , institute id , case name , stream ) hgnc symbols set . update ( new hgnc symbols ) form . hgnc symbols . data = hgnc symbols set # reset gene panels form . gene panels . data = '' # update status of case if vistited for the first time if case obj [ 'status' ] == 'inactive' and not current user . is admin : flash ( 'You just activated this case!' , 'info' ) user obj = store . user ( current user . email ) case link = url for ( 'cases.case' , institute id = institute obj [ ' id' ] , case name = case obj [ 'display name' ] ) store . update status ( institute obj , case obj , user obj , 'active' , case link ) # check if supplied gene symbols exist hgnc symbols = [ ] non clinical symbols = [ ] not found symbols = [ ] not found ids = [ ] if ( form . hgnc symbols . data ) and len ( form . hgnc symbols . data ) > 0 : is clinical = form . data . get ( 'variant type' , 'clinical' ) == 'clinical' clinical symbols = store . clinical symbols ( case obj ) if is clinical else None for hgnc symbol in form . hgnc symbols . data : if hgnc symbol . isdigit ( ) : hgnc gene = store . hgnc gene ( int ( hgnc symbol ) ) if hgnc gene is None : not found ids . append ( hgnc symbol ) else : hgnc symbols . append ( hgnc gene [ 'hgnc symbol' ] ) elif store . hgnc genes ( hgnc symbol ) . count ( ) == 0 : not found symbols . append ( hgnc symbol ) elif is clinical and ( hgnc symbol not in clinical symbols ) : non clinical symbols . append ( hgnc symbol ) else : hgnc symbols . append ( hgnc symbol ) if ( not found ids ) : flash ( "HGNC id not found: {}" . format ( ", " . join ( not found ids ) ) , 'warning' ) if ( not found symbols ) : flash ( "HGNC symbol not found: {}" . format ( ", " . join ( not found symbols ) ) , 'warning' ) if ( non clinical symbols ) : flash ( "Gene not included in clinical list: {}" . format ( ", " . join ( non clinical symbols ) ) , 'warning' ) form . hgnc symbols . data = hgnc symbols # handle HPO gene list separately if form . data [ 'gene panels' ] == [ 'hpo' ] : hpo symbols = list ( set ( term obj [ 'hgnc symbol' ] for term obj in case obj [ 'dynamic gene list' ] ) ) form . hgnc symbols . data = hpo symbols variants query = store . variants ( case obj [ ' id' ] , query = form . data ) data = { } if request . form . get ( 'export' ) : document header = controllers . variants export header ( case obj ) export lines = [ ] if form . data [ 'chrom' ] == 'MT' : # Return all MT variants export lines = controllers . variant export lines ( store , case obj , variants query ) else : # Return max 500 variants export lines = controllers . variant export lines ( store , case obj , variants query . limit ( 500 ) ) def generate ( header , lines ) : yield header + '\n' for line in lines : yield line + '\n' headers = Headers ( ) headers . add ( 'Content-Disposition' , 'attachment' , filename = str ( case obj [ 'display name' ] ) + '-filtered variants.csv' ) # return a csv with the exported variants return Response ( generate ( "," . join ( document header ) , export lines ) , mimetype = 'text/csv' , headers = headers ) data = controllers . variants ( store , institute obj , case obj , variants query , page ) return dict ( institute = institute obj , case = case obj , form = form , severe so terms = SEVERE SO TERMS , page = page , * * data )
def variant ( institute id , case name , variant id ) : institute obj , case obj = institute and case ( store , institute id , case name ) log . debug ( "Variants view requesting data for variant {}" . format ( variant id ) ) data = controllers . variant ( store , institute obj , case obj , variant id = variant id ) if data is None : log . warning ( "An error occurred: variants view requesting data for variant {}" . format ( variant id ) ) flash ( 'An error occurred while retrieving variant object' , 'danger' ) return redirect ( request . referrer ) if current app . config . get ( 'LOQUSDB SETTINGS' ) : data [ 'observations' ] = controllers . observations ( store , loqusdb , case obj , data [ 'variant' ] ) data [ 'cancer' ] = request . args . get ( 'cancer' ) == 'yes' return dict ( institute = institute obj , case = case obj , * * data )
def str variants ( institute id , case name ) : page = int ( request . args . get ( 'page' , 1 ) ) variant type = request . args . get ( 'variant type' , 'clinical' ) form = Str Filters Form ( request . args ) institute obj , case obj = institute and case ( store , institute id , case name ) query = form . data query [ 'variant type' ] = variant type variants query = store . variants ( case obj [ ' id' ] , category = 'str' , query = query ) data = controllers . str variants ( store , institute obj , case obj , variants query , page ) return dict ( institute = institute obj , case = case obj , variant type = variant type , form = form , page = page , * * data )
def sv variants ( institute id , case name ) : page = int ( request . form . get ( 'page' , 1 ) ) variant type = request . args . get ( 'variant type' , 'clinical' ) institute obj , case obj = institute and case ( store , institute id , case name ) form = Sv Filters Form ( request . form ) default panels = [ ] for panel in case obj [ 'panels' ] : if panel [ 'is default' ] : default panels . append ( panel [ 'panel name' ] ) request . form . get ( 'gene panels' ) if bool ( request . form . get ( 'clinical filter' ) ) : clinical filter = Multi Dict ( { 'variant type' : 'clinical' , 'region annotations' : [ 'exonic' , 'splicing' ] , 'functional annotations' : SEVERE SO TERMS , 'thousand genomes frequency' : str ( institute obj [ 'frequency cutoff' ] ) , 'variant type' : 'clinical' , 'clingen ngi' : 10 , 'swegen' : 10 , 'size' : 100 , 'gene panels' : default panels } ) if ( request . method == "POST" ) : if bool ( request . form . get ( 'clinical filter' ) ) : form = Sv Filters Form ( clinical filter ) form . csrf token = request . args . get ( 'csrf token' ) else : form = Sv Filters Form ( request . form ) else : form = Sv Filters Form ( request . args ) available panels = case obj . get ( 'panels' , [ ] ) + [ { 'panel name' : 'hpo' , 'display name' : 'HPO' } ] panel choices = [ ( panel [ 'panel name' ] , panel [ 'display name' ] ) for panel in available panels ] form . gene panels . choices = panel choices if form . data [ 'gene panels' ] == [ 'hpo' ] : hpo symbols = list ( set ( term obj [ 'hgnc symbol' ] for term obj in case obj [ 'dynamic gene list' ] ) ) form . hgnc symbols . data = hpo symbols # update status of case if vistited for the first time if case obj [ 'status' ] == 'inactive' and not current user . is admin : flash ( 'You just activated this case!' , 'info' ) user obj = store . user ( current user . email ) case link = url for ( 'cases.case' , institute id = institute obj [ ' id' ] , case name = case obj [ 'display name' ] ) store . update status ( institute obj , case obj , user obj , 'active' , case link ) variants query = store . variants ( case obj [ ' id' ] , category = 'sv' , query = form . data ) data = { } # if variants should be exported if request . form . get ( 'export' ) : document header = controllers . variants export header ( case obj ) export lines = [ ] # Return max 500 variants export lines = controllers . variant export lines ( store , case obj , variants query . limit ( 500 ) ) def generate ( header , lines ) : yield header + '\n' for line in lines : yield line + '\n' headers = Headers ( ) headers . add ( 'Content-Disposition' , 'attachment' , filename = str ( case obj [ 'display name' ] ) + '-filtered sv-variants.csv' ) return Response ( generate ( "," . join ( document header ) , export lines ) , mimetype = 'text/csv' , headers = headers ) # return a csv with the exported variants else : data = controllers . sv variants ( store , institute obj , case obj , variants query , page ) return dict ( institute = institute obj , case = case obj , variant type = variant type , form = form , severe so terms = SEVERE SO TERMS , page = page , * * data )
def sv variant ( institute id , case name , variant id ) : data = controllers . sv variant ( store , institute id , case name , variant id ) return data
def str variant ( institute id , case name , variant id ) : data = controllers . str variant ( store , institute id , case name , variant id ) return data
def verify ( institute id , case name , variant id , variant category , order ) : institute obj , case obj = institute and case ( store , institute id , case name ) variant obj = store . variant ( variant id ) user obj = store . user ( current user . email ) comment = request . form . get ( 'verification comment' ) try : controllers . variant verification ( store = store , mail = mail , institute obj = institute obj , case obj = case obj , user obj = user obj , comment = comment , variant obj = variant obj , sender = current app . config [ 'MAIL USERNAME' ] , variant url = request . referrer , order = order , url builder = url for ) except controllers . Missing Verification Recipient Error : flash ( 'No verification recipients added to institute.' , 'danger' ) return redirect ( request . referrer )
def clinvar ( institute id , case name , variant id ) : data = controllers . clinvar export ( store , institute id , case name , variant id ) if request . method == 'GET' : return data else : #POST form dict = request . form . to dict ( ) submission objects = set submission objects ( form dict ) # A tuple of submission objects (variants and casedata objects) # Add submission data to an open clinvar submission object, # or create a new if no open submission is found in database open submission = store . get open clinvar submission ( current user . email , institute id ) updated submission = store . add to submission ( open submission [ ' id' ] , submission objects ) # Redirect to clinvar submissions handling page, and pass it the updated submission object return redirect ( url for ( 'cases.clinvar submissions' , institute id = institute id ) )
def cancer variants ( institute id , case name ) : data = controllers . cancer variants ( store , request . args , institute id , case name ) return data
def variant acmg ( institute id , case name , variant id ) : if request . method == 'GET' : data = controllers . variant acmg ( store , institute id , case name , variant id ) return data else : criteria = [ ] criteria terms = request . form . getlist ( 'criteria' ) for term in criteria terms : criteria . append ( dict ( term = term , comment = request . form . get ( "comment-{}" . format ( term ) ) , links = [ request . form . get ( "link-{}" . format ( term ) ) ] , ) ) acmg = controllers . variant acmg post ( store , institute id , case name , variant id , current user . email , criteria ) flash ( "classified as: {}" . format ( acmg ) , 'info' ) return redirect ( url for ( '.variant' , institute id = institute id , case name = case name , variant id = variant id ) )
def evaluation ( evaluation id ) : evaluation obj = store . get evaluation ( evaluation id ) controllers . evaluation ( store , evaluation obj ) if request . method == 'POST' : link = url for ( '.variant' , institute id = evaluation obj [ 'institute' ] [ ' id' ] , case name = evaluation obj [ 'case' ] [ 'display name' ] , variant id = evaluation obj [ 'variant specific' ] ) store . delete evaluation ( evaluation obj ) return redirect ( link ) return dict ( evaluation = evaluation obj , institute = evaluation obj [ 'institute' ] , case = evaluation obj [ 'case' ] , variant = evaluation obj [ 'variant' ] , CRITERIA = ACMG CRITERIA )
def acmg ( ) : criteria = request . args . getlist ( 'criterion' ) classification = get acmg ( criteria ) return jsonify ( dict ( classification = classification ) )
def upload panel ( institute id , case name ) : file = form . symbol file . data if file . filename == '' : flash ( 'No selected file' , 'warning' ) return redirect ( request . referrer ) try : stream = io . String IO ( file . stream . read ( ) . decode ( 'utf-8' ) , newline = None ) except Unicode Decode Error as error : flash ( "Only text files are supported!" , 'warning' ) return redirect ( request . referrer ) category = request . args . get ( 'category' ) if ( category == 'sv' ) : form = Sv Filters Form ( request . args ) else : form = Filters Form ( request . args ) hgnc symbols = set ( form . hgnc symbols . data ) new hgnc symbols = controllers . upload panel ( store , institute id , case name , stream ) hgnc symbols . update ( new hgnc symbols ) form . hgnc symbols . data = ',' . join ( hgnc symbols ) # reset gene panels form . gene panels . data = '' # HTTP redirect code 307 asks that the browser preserves the method of request (POST). if ( category == 'sv' ) : return redirect ( url for ( '.sv variants' , institute id = institute id , case name = case name , * * form . data ) , code = 307 ) else : return redirect ( url for ( '.variants' , institute id = institute id , case name = case name , * * form . data ) , code = 307 )
def download verified ( ) : user obj = store . user ( current user . email ) user institutes = user obj . get ( 'institutes' ) temp excel dir = os . path . join ( variants bp . static folder , 'verified folder' ) os . makedirs ( temp excel dir , exist ok = True ) written files = controllers . verified excel file ( store , user institutes , temp excel dir ) if written files : today = datetime . datetime . now ( ) . strftime ( '%Y-%m-%d' ) # zip the files on the fly and serve the archive to the user data = io . Bytes IO ( ) with zipfile . Zip File ( data , mode = 'w' ) as z : for f name in pathlib . Path ( temp excel dir ) . iterdir ( ) : zipfile . Zip File z . write ( f name , os . path . basename ( f name ) ) data . seek ( 0 ) # remove temp folder with excel files in it shutil . rmtree ( temp excel dir ) return send file ( data , mimetype = 'application/zip' , as attachment = True , attachment filename = ' ' . join ( [ 'scout' , 'verified variants' , today ] ) + '.zip' ) else : flash ( "No verified variants could be exported for user's institutes" , 'warning' ) return redirect ( request . referrer )
def add incomplete penetrance ( genes , alias genes , hpo lines ) : LOG . info ( "Add incomplete penetrance info" ) for hgnc symbol in get incomplete penetrance genes ( hpo lines ) : for hgnc id in get correct ids ( hgnc symbol , alias genes ) : genes [ hgnc id ] [ 'incomplete penetrance' ] = True
def panels ( ) : if request . method == 'POST' : # update an existing panel csv file = request . files [ 'csv file' ] content = csv file . stream . read ( ) lines = None try : if b'\n' in content : lines = content . decode ( 'utf-8' , 'ignore' ) . split ( '\n' ) else : lines = content . decode ( 'windows-1252' ) . split ( '\r' ) except Exception as err : flash ( 'Something went wrong while parsing the panel CSV file! ({})' . format ( err ) , 'danger' ) return redirect ( request . referrer ) new panel name = request . form . get ( 'new panel name' ) if new panel name : #create a new panel new panel id = controllers . new panel ( store = store , institute id = request . form [ 'institute' ] , panel name = new panel name , display name = request . form [ 'display name' ] , csv lines = lines , ) if new panel id is None : flash ( 'Something went wrong and the panel list was not updated!' , 'warning' ) return redirect ( request . referrer ) else : flash ( "new gene panel added, {}!" . format ( new panel name ) , 'success' ) return redirect ( url for ( 'panels.panel' , panel id = new panel id ) ) else : # modify an existing panel update option = request . form [ 'modify option' ] panel obj = controllers . update panel ( store = store , panel name = request . form [ 'panel name' ] , csv lines = lines , option = update option ) if panel obj is None : return abort ( 404 , "gene panel not found: {}" . format ( request . form [ 'panel name' ] ) ) else : return redirect ( url for ( 'panels.panel' , panel id = panel obj [ ' id' ] ) ) institutes = list ( user institutes ( store , current user ) ) panel names = [ name for institute in institutes for name in store . gene panels ( institute id = institute [ ' id' ] ) . distinct ( 'panel name' ) ] panel versions = { } for name in panel names : panel versions [ name ] = store . gene panels ( panel id = name ) panel groups = [ ] for institute obj in institutes : institute panels = store . latest panels ( institute obj [ ' id' ] ) panel groups . append ( ( institute obj , institute panels ) ) return dict ( panel groups = panel groups , panel names = panel names , panel versions = panel versions , institutes = institutes )
def panel update ( panel id ) : panel obj = store . panel ( panel id ) update version = request . form . get ( 'version' , None ) new panel id = store . apply pending ( panel obj , update version ) return redirect ( url for ( 'panels.panel' , panel id = new panel id ) )
def panel export ( panel id ) : panel obj = store . panel ( panel id ) data = controllers . panel export ( store , panel obj ) data [ 'report created at' ] = datetime . datetime . now ( ) . strftime ( "%Y-%m-%d" ) html report = render template ( 'panels/panel pdf simple.html' , * * data ) return render pdf ( HTML ( string = html report ) , download filename = data [ 'panel' ] [ 'panel name' ] + ' ' + str ( data [ 'panel' ] [ 'version' ] ) + ' ' + datetime . datetime . now ( ) . strftime ( "%Y-%m-%d" ) + ' scout.pdf' )
def gene edit ( panel id , hgnc id ) : panel obj = store . panel ( panel id ) hgnc gene = store . hgnc gene ( hgnc id ) panel gene = controllers . existing gene ( store , panel obj , hgnc id ) form = Panel Gene Form ( ) transcript choices = [ ] for transcript in hgnc gene [ 'transcripts' ] : if transcript . get ( 'refseq id' ) : refseq id = transcript . get ( 'refseq id' ) transcript choices . append ( ( refseq id , refseq id ) ) form . disease associated transcripts . choices = transcript choices if form . validate on submit ( ) : action = 'edit' if panel gene else 'add' info data = form . data . copy ( ) if 'csrf token' in info data : del info data [ 'csrf token' ] store . add pending ( panel obj , hgnc gene , action = action , info = info data ) return redirect ( url for ( '.panel' , panel id = panel id ) ) if panel gene : for field key in [ 'disease associated transcripts' , 'reduced penetrance' , 'mosaicism' , 'inheritance models' , 'database entry version' , 'comment' ] : form field = getattr ( form , field key ) if not form field . data : panel value = panel gene . get ( field key ) if panel value is not None : form field . process data ( panel value ) return dict ( panel = panel obj , form = form , gene = hgnc gene , panel gene = panel gene )
def delivery report ( context , case id , report path , update ) : adapter = context . obj [ 'adapter' ] try : load delivery report ( adapter = adapter , case id = case id , report path = report path , update = update ) LOG . info ( "saved report to case!" ) except Exception as e : LOG . error ( e ) context . abort ( )
def whitelist ( context ) : LOG . info ( "Running scout view users" ) adapter = context . obj [ 'adapter' ] ## TODO add a User interface to the adapter for whitelist obj in adapter . whitelist collection . find ( ) : click . echo ( whitelist obj [ ' id' ] )
def gene ( store , hgnc id ) : res = { 'builds' : { '37' : None , '38' : None } , 'symbol' : None , 'description' : None , 'ensembl id' : None , 'record' : None } for build in res [ 'builds' ] : record = store . hgnc gene ( hgnc id , build = build ) if record : record [ 'position' ] = "{this[chromosome]}:{this[start]}-{this[end]}" . format ( this = record ) res [ 'aliases' ] = record [ 'aliases' ] res [ 'hgnc id' ] = record [ 'hgnc id' ] res [ 'description' ] = record [ 'description' ] res [ 'builds' ] [ build ] = record res [ 'symbol' ] = record [ 'hgnc symbol' ] res [ 'description' ] = record [ 'description' ] res [ 'entrez id' ] = record . get ( 'entrez id' ) res [ 'pli score' ] = record . get ( 'pli score' ) add gene links ( record , int ( build ) ) res [ 'omim id' ] = record . get ( 'omim id' ) res [ 'incomplete penetrance' ] = record . get ( 'incomplete penetrance' , False ) res [ 'inheritance models' ] = record . get ( 'inheritance models' , [ ] ) for transcript in record [ 'transcripts' ] : transcript [ 'position' ] = ( "{this[chrom]}:{this[start]}-{this[end]}" . format ( this = transcript ) ) add tx links ( transcript , build ) for phenotype in record . get ( 'phenotypes' , [ ] ) : phenotype [ 'omim link' ] = omim ( phenotype . get ( 'mim number' ) ) if not res [ 'record' ] : res [ 'record' ] = record # If none of the genes where found if not any ( res . values ( ) ) : raise Value Error return res
def genes to json ( store , query ) : gene query = store . hgnc genes ( query , search = True ) json terms = [ { 'name' : "{} | {} ({})" . format ( gene [ 'hgnc id' ] , gene [ 'hgnc symbol' ] , ', ' . join ( gene [ 'aliases' ] ) ) , 'id' : gene [ 'hgnc id' ] } for gene in gene query ] return json terms
def index ( ) : accessible institutes = current user . institutes if not 'admin' in current user . roles : accessible institutes = current user . institutes if not accessible institutes : flash ( 'Not allowed to see information - please visit the dashboard later!' ) return redirect ( url for ( 'cases.dahboard general.html' ) ) LOG . debug ( 'User accessible institutes: {}' . format ( accessible institutes ) ) institutes = [ inst for inst in store . institutes ( accessible institutes ) ] # Insert a entry that displays all institutes in the beginning of the array institutes . insert ( 0 , { ' id' : None , 'display name' : 'All institutes' } ) institute id = None slice query = None panel = 1 if request . method == 'POST' : institute id = request . form . get ( 'institute' ) slice query = request . form . get ( 'query' ) panel = request . form . get ( 'pane id' ) elif request . method == 'GET' : institute id = request . args . get ( 'institute' ) slice query = request . args . get ( 'query' ) # User should be restricted to their own institute if: #1) Their default institute when the page is first loaded #2) if they ask for an institute that they don't belong to #3) if they want perform a query on all institutes if not institute id : institute id = accessible institutes [ 0 ] elif ( not current user . is admin ) and ( slice query and institute id == 'None' ) : institute id = accessible institutes [ 0 ] elif ( not institute id in accessible institutes ) and not ( institute id == 'None' ) : institute id = accessible institutes [ 0 ] LOG . info ( "Fetch all cases with institute: %s" , institute id ) data = get dashboard info ( store , institute id , slice query ) data [ 'institutes' ] = institutes data [ 'choice' ] = institute id total cases = data [ 'total cases' ] LOG . info ( "Found %s cases" , total cases ) if total cases == 0 : flash ( 'no cases found for institute {} (with that query) - please visit the dashboard later!' . format ( institute id ) , 'info' ) #        return redirect(url for('cases.index')) return render template ( 'dashboard/dashboard general.html' , institute = institute id , query = slice query , panel = panel , * * data )
def transcripts ( context , build , hgnc id , json ) : LOG . info ( "Running scout view transcripts" ) adapter = context . obj [ 'adapter' ] if not json : click . echo ( "Chromosome\tstart\tend\ttranscript id\thgnc id\trefseq\tis primary" ) for tx obj in adapter . transcripts ( build = build , hgnc id = hgnc id ) : if json : pp ( tx obj ) continue click . echo ( "{0}\t{1}\t{2}\t{3}\t{4}\t{5}\t{6}" . format ( tx obj [ 'chrom' ] , tx obj [ 'start' ] , tx obj [ 'end' ] , tx obj [ 'ensembl transcript id' ] , tx obj [ 'hgnc id' ] , tx obj . get ( 'refseq id' , '' ) , tx obj . get ( 'is primary' ) or '' , ) )
def variants ( store , institute obj , case obj , variants query , page = 1 , per page = 50 ) : variant count = variants query . count ( ) skip count = per page * max ( page - 1 , 0 ) more variants = True if variant count > ( skip count + per page ) else False variant res = variants query . skip ( skip count ) . limit ( per page ) genome build = case obj . get ( 'genome build' , '37' ) if genome build not in [ '37' , '38' ] : genome build = '37' variants = [ ] for variant obj in variant res : overlapping svs = [ sv for sv in store . overlapping ( variant obj ) ] variant obj [ 'overlapping' ] = overlapping svs or None variants . append ( parse variant ( store , institute obj , case obj , variant obj , update = True , genome build = genome build ) ) return { 'variants' : variants , 'more variants' : more variants , }
def sv variants ( store , institute obj , case obj , variants query , page = 1 , per page = 50 ) : skip count = ( per page * max ( page - 1 , 0 ) ) more variants = True if variants query . count ( ) > ( skip count + per page ) else False genome build = case obj . get ( 'genome build' , '37' ) if genome build not in [ '37' , '38' ] : genome build = '37' return { 'variants' : ( parse variant ( store , institute obj , case obj , variant , genome build = genome build ) for variant in variants query . skip ( skip count ) . limit ( per page ) ) , 'more variants' : more variants , }
def str variants ( store , institute obj , case obj , variants query , page = 1 , per page = 50 ) : # Nothing unique to ST Rs on this level. Inheritance? return variants ( store , institute obj , case obj , variants query , page , per page )
def get predictions ( genes ) : data = { 'sift predictions' : [ ] , 'polyphen predictions' : [ ] , 'region annotations' : [ ] , 'functional annotations' : [ ] } for gene obj in genes : for pred key in data : gene key = pred key [ : - 1 ] if len ( genes ) == 1 : value = gene obj . get ( gene key , '-' ) else : gene id = gene obj . get ( 'hgnc symbol' ) or str ( gene obj [ 'hgnc id' ] ) value = ':' . join ( [ gene id , gene obj . get ( gene key , '-' ) ] ) data [ pred key ] . append ( value ) return data
def find bai file ( bam file ) : bai file = bam file . replace ( '.bam' , '.bai' ) if not os . path . exists ( bai file ) : # try the other convention bai file = "{}.bai" . format ( bam file ) return bai file
def observations ( store , loqusdb , case obj , variant obj ) : composite id = ( "{this[chromosome]} {this[position]} {this[reference]} " "{this[alternative]}" . format ( this = variant obj ) ) obs data = loqusdb . get variant ( { ' id' : composite id } ) or { } obs data [ 'total' ] = loqusdb . case count ( ) obs data [ 'cases' ] = [ ] institute id = variant obj [ 'institute' ] for case id in obs data . get ( 'families' , [ ] ) : if case id != variant obj [ 'case id' ] and case id . startswith ( institute id ) : other variant = store . variant ( variant obj [ 'variant id' ] , case id = case id ) other case = store . case ( case id ) obs data [ 'cases' ] . append ( dict ( case = other case , variant = other variant ) ) return obs data
def parse gene ( gene obj , build = None ) : build = build or 37 if gene obj . get ( 'common' ) : add gene links ( gene obj , build ) refseq transcripts = [ ] for tx obj in gene obj [ 'transcripts' ] : parse transcript ( gene obj , tx obj , build ) # select refseq transcripts as "primary" if not tx obj . get ( 'refseq id' ) : continue refseq transcripts . append ( tx obj ) gene obj [ 'primary transcripts' ] = ( refseq transcripts if refseq transcripts else [ ] )
def transcript str ( transcript obj , gene name = None ) : if transcript obj . get ( 'exon' ) : gene part , part count raw = 'exon' , transcript obj [ 'exon' ] elif transcript obj . get ( 'intron' ) : gene part , part count raw = 'intron' , transcript obj [ 'intron' ] else : # variant between genes gene part , part count raw = 'intergenic' , '0' part count = part count raw . rpartition ( '/' ) [ 0 ] change str = "{}:{}{}:{}:{}" . format ( transcript obj . get ( 'refseq id' , '' ) , gene part , part count , transcript obj . get ( 'coding sequence name' , 'NA' ) , transcript obj . get ( 'protein sequence name' , 'NA' ) , ) if gene name : change str = "{}:" . format ( gene name ) + change str return change str
def end position ( variant obj ) : alt bases = len ( variant obj [ 'alternative' ] ) num bases = max ( len ( variant obj [ 'reference' ] ) , alt bases ) return variant obj [ 'position' ] + ( num bases - 1 )
def clinsig human ( variant obj ) : for clinsig obj in variant obj [ 'clnsig' ] : # The clinsig objects allways have a accession if isinstance ( clinsig obj [ 'accession' ] , int ) : # New version link = "https://www.ncbi.nlm.nih.gov/clinvar/variation/{}" else : # Old version link = "https://www.ncbi.nlm.nih.gov/clinvar/{}" human str = 'not provided' if clinsig obj . get ( 'value' ) : try : # Old version int ( clinsig obj [ 'value' ] ) human str = CLINSIG MAP . get ( clinsig obj [ 'value' ] , 'not provided' ) except Value Error : # New version human str = clinsig obj [ 'value' ] clinsig obj [ 'human' ] = human str clinsig obj [ 'link' ] = link . format ( clinsig obj [ 'accession' ] ) yield clinsig obj
def thousandg link ( variant obj , build = None ) : dbsnp id = variant obj . get ( 'dbsnp id' ) build = build or 37 if not dbsnp id : return None if build == 37 : url template = ( "http://grch37.ensembl.org/Homo sapiens/Variation/Explore" "?v={};vdb=variation" ) else : url template = ( "http://www.ensembl.org/Homo sapiens/Variation/Explore" "?v={};vdb=variation" ) return url template . format ( dbsnp id )
def beacon link ( variant obj , build = None ) : build = build or 37 url template = ( "https://beacon-network.org/#/search?pos={this[position]}&" "chrom={this[chromosome]}&allele={this[alternative]}&" "ref={this[reference]}&rs=GR Ch37" ) # beacon does not support build 38 at the moment # if build == '38': #     url template = ("https://beacon-network.org/#/search?pos={this[position]}&" #                     "chrom={this[chromosome]}&allele={this[alternative]}&" #                     "ref={this[reference]}&rs=GR Ch38") return url template . format ( this = variant obj )
def ucsc link ( variant obj , build = None ) : build = build or 37 url template = ( "http://genome.ucsc.edu/cgi-bin/hg Tracks?db=hg19&" "position=chr{this[chromosome]}:{this[position]}" "-{this[position]}&dgv=pack&known Gene=pack&omim Gene=pack" ) if build == 38 : url template = ( "http://genome.ucsc.edu/cgi-bin/hg Tracks?db=hg20&" "position=chr{this[chromosome]}:{this[position]}" "-{this[position]}&dgv=pack&known Gene=pack&omim Gene=pack" ) return url template . format ( this = variant obj )
def spidex human ( variant obj ) : if variant obj . get ( 'spidex' ) is None : return 'not reported' elif abs ( variant obj [ 'spidex' ] ) < SPIDEX HUMAN [ 'low' ] [ 'pos' ] [ 1 ] : return 'low' elif abs ( variant obj [ 'spidex' ] ) < SPIDEX HUMAN [ 'medium' ] [ 'pos' ] [ 1 ] : return 'medium' else : return 'high'
def expected inheritance ( variant obj ) : manual models = set ( ) for gene in variant obj . get ( 'genes' , [ ] ) : manual models . update ( gene . get ( 'manual inheritance' , [ ] ) ) return list ( manual models )
def callers ( variant obj , category = 'snv' ) : calls = set ( ) for caller in CALLERS [ category ] : if variant obj . get ( caller [ 'id' ] ) : calls . add ( ( caller [ 'name' ] , variant obj [ caller [ 'id' ] ] ) ) return list ( calls )
def cancer variants ( store , request args , institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) form = Cancer Filters Form ( request args ) variants query = store . variants ( case obj [ ' id' ] , category = 'cancer' , query = form . data ) . limit ( 50 ) data = dict ( institute = institute obj , case = case obj , variants = ( parse variant ( store , institute obj , case obj , variant , update = True ) for variant in variants query ) , form = form , variant type = request args . get ( 'variant type' , 'clinical' ) , ) return data
def variant acmg ( store , institute id , case name , variant id ) : institute obj , case obj = institute and case ( store , institute id , case name ) variant obj = store . variant ( variant id ) return dict ( institute = institute obj , case = case obj , variant = variant obj , CRITERIA = ACMG CRITERIA , ACMG OPTIONS = ACMG OPTIONS )
def variant acmg post ( store , institute id , case name , variant id , user email , criteria ) : institute obj , case obj = institute and case ( store , institute id , case name ) variant obj = store . variant ( variant id ) user obj = store . user ( user email ) variant link = url for ( 'variants.variant' , institute id = institute id , case name = case name , variant id = variant id ) classification = store . submit evaluation ( institute obj = institute obj , case obj = case obj , variant obj = variant obj , user obj = user obj , link = variant link , criteria = criteria , ) return classification
def evaluation ( store , evaluation obj ) : evaluation obj [ 'institute' ] = store . institute ( evaluation obj [ 'institute id' ] ) evaluation obj [ 'case' ] = store . case ( evaluation obj [ 'case id' ] ) evaluation obj [ 'variant' ] = store . variant ( evaluation obj [ 'variant specific' ] ) evaluation obj [ 'criteria' ] = { criterion [ 'term' ] : criterion for criterion in evaluation obj [ 'criteria' ] } evaluation obj [ 'classification' ] = ACMG COMPLETE MAP [ evaluation obj [ 'classification' ] ] return evaluation obj
def upload panel ( store , institute id , case name , stream ) : institute obj , case obj = institute and case ( store , institute id , case name ) raw symbols = [ line . strip ( ) . split ( '\t' ) [ 0 ] for line in stream if line and not line . startswith ( '#' ) ] # check if supplied gene symbols exist hgnc symbols = [ ] for raw symbol in raw symbols : if store . hgnc genes ( raw symbol ) . count ( ) == 0 : flash ( "HGNC symbol not found: {}" . format ( raw symbol ) , 'warning' ) else : hgnc symbols . append ( raw symbol ) return hgnc symbols
def export genes ( adapter , build = '37' ) : LOG . info ( "Exporting all genes to .bed format" ) for gene obj in adapter . all genes ( build = build ) : yield gene obj
def index ( context , collection name ) : LOG . info ( "Running scout view index" ) adapter = context . obj [ 'adapter' ] i = 0 click . echo ( "collection\tindex" ) for collection name in adapter . collections ( ) : for index in adapter . indexes ( collection name ) : click . echo ( "{0}\t{1}" . format ( collection name , index ) ) i += 1 if i == 0 : LOG . info ( "No indexes found" )
def genes ( context , build , json ) : LOG . info ( "Running scout export genes" ) adapter = context . obj [ 'adapter' ] result = adapter . all genes ( build = build ) if json : click . echo ( dumps ( result ) ) return gene string = ( "{0}\t{1}\t{2}\t{3}\t{4}" ) click . echo ( "#Chromosom\t Start\t End\t Hgnc id\t Hgnc symbol" ) for gene obj in result : click . echo ( gene string . format ( gene obj [ 'chromosome' ] , gene obj [ 'start' ] , gene obj [ 'end' ] , gene obj [ 'hgnc id' ] , gene obj [ 'hgnc symbol' ] , ) )
def case ( institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) if case obj is None : return abort ( 404 ) return Response ( json util . dumps ( case obj ) , mimetype = 'application/json' )
def variant ( institute id , case name , variant id ) : institute obj , case obj = institute and case ( store , institute id , case name ) variant obj = store . variant ( variant id ) return Response ( json util . dumps ( variant obj ) , mimetype = 'application/json' )
def collections ( context ) : LOG . info ( "Running scout view collections" ) adapter = context . obj [ 'adapter' ] for collection name in adapter . collections ( ) : click . echo ( collection name )
def institute ( ctx , internal id , display name , sanger recipients ) : adapter = ctx . obj [ 'adapter' ] if not internal id : logger . warning ( "A institute has to have an internal id" ) ctx . abort ( ) if not display name : display name = internal id if sanger recipients : sanger recipients = list ( sanger recipients ) try : load institute ( adapter = adapter , internal id = internal id , display name = display name , sanger recipients = sanger recipients ) except Exception as e : logger . warning ( e ) ctx . abort ( )
def get file handle ( file path ) : if file path . endswith ( '.gz' ) : file handle = getreader ( 'utf-8' ) ( gzip . open ( file path , 'r' ) , errors = 'replace' ) else : file handle = open ( file path , 'r' , encoding = 'utf-8' ) return file handle
def get net ( req ) : try : nxt , prev = map ( int , ( req . GET . get ( 'cal next' , 0 ) , req . GET . get ( 'cal prev' , 0 ) ) ) net = nxt - prev except Exception : net = 0 return net
def get next and prev ( net ) : if net == 0 : nxt = prev = 1 elif net > 0 : nxt = net + 1 prev = - ( net - 1 ) else : nxt = net + 1 prev = abs ( net ) + 1 return nxt , prev
def check year ( year , month , error , error msg ) : if year not in xrange ( ( now . year - 50 ) , ( now . year + 51 ) ) : year = now . year month = now . month error = error msg return year , month , error
def add peddy information ( config data ) : ped info = { } ped check = { } sex check = { } relations = [ ] if config data . get ( 'peddy ped' ) : file handle = open ( config data [ 'peddy ped' ] , 'r' ) for ind info in parse peddy ped ( file handle ) : ped info [ ind info [ 'sample id' ] ] = ind info if config data . get ( 'peddy ped check' ) : file handle = open ( config data [ 'peddy ped check' ] , 'r' ) for pair info in parse peddy ped check ( file handle ) : ped check [ ( pair info [ 'sample a' ] , pair info [ 'sample b' ] ) ] = pair info if config data . get ( 'peddy sex check' ) : file handle = open ( config data [ 'peddy sex check' ] , 'r' ) for ind info in parse peddy sex check ( file handle ) : sex check [ ind info [ 'sample id' ] ] = ind info if not ped info : return analysis inds = { } for ind in config data [ 'samples' ] : ind id = ind [ 'sample id' ] analysis inds [ ind id ] = ind for ind id in analysis inds : ind = analysis inds [ ind id ] # Check if peddy has inferred the ancestry if ind id in ped info : ind [ 'predicted ancestry' ] = ped info [ ind id ] . get ( 'ancestry-prediction' , 'UNKNOWN' ) # Check if peddy has inferred the sex if ind id in sex check : if sex check [ ind id ] [ 'error' ] : ind [ 'confirmed sex' ] = False else : ind [ 'confirmed sex' ] = True # Check if peddy har confirmed parental relations for parent in [ 'mother' , 'father' ] : # If we are looking at individual with parents if ind [ parent ] != '0' : # Check if the child/parent pair is in peddy data for pair in ped check : if ( ind id in pair and ind [ parent ] in pair ) : # If there is a parent error we mark that if ped check [ pair ] [ 'parent error' ] : analysis inds [ ind [ parent ] ] [ 'confirmed parent' ] = False else : # Else if parent confirmation has not been done if 'confirmed parent' not in analysis inds [ ind [ parent ] ] : # Set confirmatio to True analysis inds [ ind [ parent ] ] [ 'confirmed parent' ] = True
def panel ( context , path , date , display name , version , panel type , panel id , institute , omim , api key , panel app ) : adapter = context . obj [ 'adapter' ] institute = institute or 'cust000' if omim : api key = api key or context . obj . get ( 'omim api key' ) if not api key : LOG . warning ( "Please provide a omim api key to load the omim gene panel" ) context . abort ( ) #Check if OMIM-AUTO exists if adapter . gene panel ( panel id = 'OMIM-AUTO' ) : LOG . warning ( "OMIM-AUTO already exists in database" ) LOG . info ( "To create a new version use scout update omim" ) return # Here we know that there is no panel loaded try : adapter . load omim panel ( api key , institute = institute ) except Exception as err : LOG . error ( err ) context . abort ( ) if panel app : # try: load panel app ( adapter , panel id , institute = institute ) # except Exception as err: #     LOG.warning(err) #     context.abort() if ( omim or panel app ) : return if path is None : LOG . info ( "Please provide a panel" ) return try : load panel ( path , adapter , date , display name , version , panel type , panel id , institute ) except Exception as err : LOG . warning ( err ) context . abort ( )
def panel ( context , panel id , version ) : LOG . info ( "Running scout delete panel" ) adapter = context . obj [ 'adapter' ] panel objs = adapter . gene panels ( panel id = panel id , version = version ) if panel objs . count ( ) == 0 : LOG . info ( "No panels found" ) for panel obj in panel objs : adapter . delete panel ( panel obj )
def index ( context ) : LOG . info ( "Running scout delete index" ) adapter = context . obj [ 'adapter' ] for collection in adapter . db . collection names ( ) : adapter . db [ collection ] . drop indexes ( ) LOG . info ( "All indexes deleted" )
def user ( context , mail ) : LOG . info ( "Running scout delete user" ) adapter = context . obj [ 'adapter' ] user obj = adapter . user ( mail ) if not user obj : LOG . warning ( "User {0} could not be found in database" . format ( mail ) ) else : adapter . delete user ( mail )
def genes ( context , build ) : LOG . info ( "Running scout delete genes" ) adapter = context . obj [ 'adapter' ] if build : LOG . info ( "Dropping genes collection for build: %s" , build ) else : LOG . info ( "Dropping genes collection" ) adapter . drop genes ( )
def exons ( context , build ) : LOG . info ( "Running scout delete exons" ) adapter = context . obj [ 'adapter' ] adapter . drop exons ( build )
def case ( context , institute , case id , display name ) : adapter = context . obj [ 'adapter' ] if not ( case id or display name ) : click . echo ( "Please specify what case to delete" ) context . abort ( ) if display name : if not institute : click . echo ( "Please specify the owner of the case that should be " "deleted with flag '-i/--institute'." ) context . abort ( ) case id = "{0}-{1}" . format ( institute , display name ) LOG . info ( "Running deleting case {0}" . format ( case id ) ) case = adapter . delete case ( case id = case id , institute id = institute , display name = display name ) if case . deleted count == 1 : adapter . delete variants ( case id = case id , variant type = 'clinical' ) adapter . delete variants ( case id = case id , variant type = 'research' ) else : LOG . warning ( "Case does not exist in database" ) context . abort ( )
def individuals ( context , institute , causatives , case id ) : LOG . info ( "Running scout view individuals" ) adapter = context . obj [ 'adapter' ] individuals = [ ] if case id : case = adapter . case ( case id = case id ) if case : cases = [ case ] else : LOG . info ( "Could not find case %s" , case id ) return else : cases = [ case obj for case obj in adapter . cases ( collaborator = institute , has causatives = causatives ) ] if len ( cases ) == 0 : LOG . info ( "Could not find cases that match criteria" ) return individuals = ( ind obj for case obj in cases for ind obj in case obj [ 'individuals' ] ) click . echo ( "#case id\tind id\tdisplay name\tsex\tphenotype\tmother\tfather" ) for case in cases : for ind obj in case [ 'individuals' ] : ind info = [ case [ ' id' ] , ind obj [ 'individual id' ] , ind obj [ 'display name' ] , SEX MAP [ int ( ind obj [ 'sex' ] ) ] , PHENOTYPE MAP [ ind obj [ 'phenotype' ] ] , ind obj [ 'mother' ] , ind obj [ 'father' ] ] click . echo ( '\t' . join ( ind info ) )
def cases ( context , institute , display name , case id , nr variants , variants treshold ) : LOG . info ( "Running scout view institutes" ) adapter = context . obj [ 'adapter' ] models = [ ] if case id : case obj = adapter . case ( case id = case id ) if case obj : models . append ( case obj ) else : models = adapter . cases ( collaborator = institute , name query = display name ) models = [ case obj for case obj in models ] if not models : LOG . info ( "No cases could be found" ) return header = [ 'case id' , 'display name' , 'institute' ] if variants treshold : LOG . info ( "Only show cases with more than %s variants" , variants treshold ) nr variants = True if nr variants : LOG . info ( "Displaying number of variants for each case" ) header . append ( 'clinical' ) header . append ( 'research' ) click . echo ( "#" + '\t' . join ( header ) ) for model in models : output str = "{:<12}\t{:<12}\t{:<12}" output values = [ model [ ' id' ] , model [ 'display name' ] , model [ 'owner' ] ] if nr variants : output str += "\t{:<12}\t{:<12}" nr clinical = 0 nr research = 0 variants = adapter . variant collection . find ( { 'case id' : model [ ' id' ] } ) i = 0 for i , var in enumerate ( variants , 1 ) : if var [ 'variant type' ] == 'clinical' : nr clinical += 1 else : nr research += 1 output values . extend ( [ nr clinical , nr research ] ) if variants treshold and i < variants treshold : LOG . debug ( "Case %s had to few variants, skipping" , model [ ' id' ] ) continue click . echo ( output str . format ( * output values ) )
def load user ( user email ) : user obj = store . user ( user email ) user inst = Login User ( user obj ) if user obj else None return user inst
def login ( ) : # store potential next param URL in the session if 'next' in request . args : session [ 'next url' ] = request . args [ 'next' ] if current app . config . get ( 'GOOGLE' ) : callback url = url for ( '.authorized' , external = True ) return google . authorize ( callback = callback url ) user email = request . args . get ( 'email' ) user obj = store . user ( user email ) if user obj is None : flash ( "email not whitelisted: {}" . format ( user email ) , 'warning' ) return redirect ( url for ( 'public.index' ) ) return perform login ( user obj )
def user events ( self , user obj = None ) : query = dict ( user id = user obj [ ' id' ] ) if user obj else dict ( ) return self . event collection . find ( query )
def hpo terms ( ) : if request . method == 'GET' : data = controllers . hpo terms ( store = store , limit = 100 ) return data else : # POST. user is searching for a specific term or phenotype search term = request . form . get ( 'hpo term' ) limit = request . form . get ( 'limit' ) data = controllers . hpo terms ( store = store , query = search term , limit = limit ) return dict ( data , query = search term , limit = limit )
def transcripts ( context , build ) : LOG . info ( "Running scout export transcripts" ) adapter = context . obj [ 'adapter' ] header = [ "#Chrom\t Start\t End\t Transcript\t Ref Seq\t Hgnc ID" ] for line in header : click . echo ( line ) transcript string = ( "{0}\t{1}\t{2}\t{3}\t{4}\t{5}" ) for tx obj in export transcripts ( adapter ) : click . echo ( transcript string . format ( tx obj [ 'chrom' ] , tx obj [ 'start' ] , tx obj [ 'end' ] , tx obj [ 'ensembl transcript id' ] , tx obj . get ( 'refseq id' , '' ) , tx obj [ 'hgnc id' ] , ) )
def exons ( context , build ) : adapter = context . obj [ 'adapter' ] start = datetime . now ( ) # Test if there are any exons loaded nr exons = adapter . exons ( build = build ) . count ( ) if nr exons : LOG . warning ( "Dropping all exons " ) adapter . drop exons ( build = build ) LOG . info ( "Exons dropped" ) # Load the exons ensembl exons = fetch ensembl exons ( build = build ) load exons ( adapter , ensembl exons , build ) adapter . update indexes ( ) LOG . info ( "Time to load exons: {0}" . format ( datetime . now ( ) - start ) )
def intervals ( context , build ) : LOG . info ( "Running scout view index" ) adapter = context . obj [ 'adapter' ] intervals = adapter . get coding intervals ( build ) nr intervals = 0 longest = 0 for chrom in CHROMOSOMES : for iv in intervals [ chrom ] : iv len = iv . end - iv . begin if iv len > longest : longest = iv len int nr = len ( intervals . get ( chrom , [ ] ) ) click . echo ( "{0}\t{1}" . format ( chrom , int nr ) ) nr intervals += int nr LOG . info ( "Total nr intervals:%s" , nr intervals ) LOG . info ( "Total nr genes:%s" , adapter . all genes ( build ) . count ( ) ) LOG . info ( "Longest interval:%s" , longest )
def region ( context , hgnc id , case id , chromosome , start , end ) : adapter = context . obj [ 'adapter' ] load region ( adapter = adapter , case id = case id , hgnc id = hgnc id , chrom = chromosome , start = start , end = end )
def parse reqs ( req path = './requirements.txt' ) : install requires = [ ] with io . open ( os . path . join ( here , 'requirements.txt' ) , encoding = 'utf-8' ) as handle : # remove comments and empty lines lines = ( line . strip ( ) for line in handle if line . strip ( ) and not line . startswith ( '#' ) ) for line in lines : # check for nested requirements files if line . startswith ( '-r' ) : # recursively call this function install requires += parse reqs ( req path = line [ 3 : ] ) else : # add the line as a new requirement install requires . append ( line ) return install requires
def existing gene ( store , panel obj , hgnc id ) : existing genes = { gene [ 'hgnc id' ] : gene for gene in panel obj [ 'genes' ] } return existing genes . get ( hgnc id )
def panel export ( store , panel obj ) : panel obj [ 'institute' ] = store . institute ( panel obj [ 'institute' ] ) full name = "{}({})" . format ( panel obj [ 'display name' ] , panel obj [ 'version' ] ) panel obj [ 'name and version' ] = full name return dict ( panel = panel obj )
def archive info ( database : Database , archive case : dict ) -> dict : data = { 'collaborators' : archive case [ 'collaborators' ] , 'synopsis' : archive case . get ( 'synopsis' ) , 'assignees' : [ ] , 'suspects' : [ ] , 'causatives' : [ ] , 'phenotype terms' : [ ] , 'phenotype groups' : [ ] , } if archive case . get ( 'assignee' ) : archive user = database . user . find one ( { ' id' : archive case [ 'assignee' ] } ) data [ 'assignee' ] . append ( archive user [ 'email' ] ) for key in [ 'suspects' , 'causatives' ] : for variant id in archive case . get ( key , [ ] ) : archive variant = database . variant . find one ( { ' id' : variant id } ) data [ key ] . append ( { 'chromosome' : archive variant [ 'chromosome' ] , 'position' : archive variant [ 'position' ] , 'reference' : archive variant [ 'reference' ] , 'alternative' : archive variant [ 'alternative' ] , 'variant type' : archive variant [ 'variant type' ] , } ) for key in [ 'phenotype terms' , 'phenotype groups' ] : for archive term in archive case . get ( key , [ ] ) : data [ key ] . append ( { 'phenotype id' : archive term [ 'phenotype id' ] , 'feature' : archive term [ 'feature' ] , } ) return data
def migrate case ( adapter : Mongo Adapter , scout case : dict , archive data : dict ) : # update collaborators collaborators = list ( set ( scout case [ 'collaborators' ] + archive data [ 'collaborators' ] ) ) if collaborators != scout case [ 'collaborators' ] : LOG . info ( f"set collaborators: {', '.join(collaborators)}" ) scout case [ 'collaborators' ] = collaborators # update assignees if len ( scout case . get ( 'assignees' , [ ] ) ) == 0 : scout user = adapter . user ( archive data [ 'assignee' ] ) if scout user : scout case [ 'assignees' ] = [ archive data [ 'assignee' ] ] else : LOG . warning ( f"{archive data['assignee']}: unable to find assigned user" ) # add/update suspected/causative variants for key in [ 'suspects' , 'causatives' ] : scout case [ key ] = scout case . get ( key , [ ] ) for archive variant in archive data [ key ] : variant id = get variantid ( archive variant , scout case [ ' id' ] ) scout variant = adapter . variant ( variant id ) if scout variant : if scout variant [ ' id' ] in scout case [ key ] : LOG . info ( f"{scout variant[' id']}: variant already in {key}" ) else : LOG . info ( f"{scout variant[' id']}: add to {key}" ) scout variant [ key ] . append ( scout variant [ ' id' ] ) else : LOG . warning ( f"{scout variant[' id']}: unable to find variant ({key})" ) scout variant [ key ] . append ( variant id ) if not scout case . get ( 'synopsis' ) : # update synopsis scout case [ 'synopsis' ] = archive data [ 'synopsis' ] scout case [ 'is migrated' ] = True adapter . case collection . find one and replace ( { ' id' : scout case [ ' id' ] } , scout case , ) # add/update phenotype groups/terms scout institute = adapter . institute ( scout case [ 'owner' ] ) scout user = adapter . user ( 'mans.magnusson@scilifelab.se' ) for key in [ 'phenotype terms' , 'phenotype groups' ] : for archive term in archive data [ key ] : adapter . add phenotype ( institute = scout institute , case = scout case , user = scout user , link = f"/{scout case['owner']}/{scout case['display name']}" , hpo term = archive term [ 'phenotype id' ] , is group = key == 'phenotype groups' , )
def migrate ( uri : str , archive uri : str , case id : str , dry : bool , force : bool ) : scout client = Mongo Client ( uri ) scout database = scout client [ uri . rsplit ( '/' , 1 ) [ - 1 ] ] scout adapter = Mongo Adapter ( database = scout database ) scout case = scout adapter . case ( case id ) if not force and scout case . get ( 'is migrated' ) : print ( "case already migrated" ) return archive client = Mongo Client ( archive uri ) archive database = archive client [ archive uri . rsplit ( '/' , 1 ) [ - 1 ] ] archive case = archive database . case . find one ( { 'owner' : scout case [ 'owner' ] , 'display name' : scout case [ 'display name' ] } ) archive data = archive info ( archive database , archive case ) if dry : print ( ruamel . yaml . safe dump ( archive data ) ) else : #migrate case(scout adapter, scout case, archive data) pass
def hpo ( context , term , description ) : LOG . info ( "Running scout view hpo" ) adapter = context . obj [ 'adapter' ] if term : term = term . upper ( ) if not term . startswith ( 'HP:' ) : while len ( term ) < 7 : term = '0' + term term = 'HP:' + term LOG . info ( "Searching for term %s" , term ) hpo terms = adapter . hpo terms ( hpo term = term ) elif description : sorted terms = sorted ( adapter . hpo terms ( query = description ) , key = itemgetter ( 'hpo number' ) ) for term in sorted terms : term . pop ( 'genes' ) print ( "name: {} | {} | {}" . format ( term [ ' id' ] , term [ 'description' ] , term [ 'hpo number' ] ) ) # pp(hpo terms) context . abort ( ) else : hpo terms = adapter . hpo terms ( ) if hpo terms . count ( ) == 0 : LOG . warning ( "No matching terms found" ) return click . echo ( "hpo id\tdescription\tnr genes" ) for hpo obj in hpo terms : click . echo ( "{0}\t{1}\t{2}" . format ( hpo obj [ 'hpo id' ] , hpo obj [ 'description' ] , len ( hpo obj . get ( 'genes' , [ ] ) ) ) )
def create app ( config file = None , config = None ) : app = Flask ( name ) app . config . from pyfile ( 'config.py' ) app . jinja env . add extension ( 'jinja2.ext.do' ) if config : app . config . update ( config ) if config file : app . config . from pyfile ( config file ) # If there is a Match Maker Exchange server # collect the connected external nodes app . mme nodes = mme nodes ( app . config . get ( 'MME URL' ) , app . config . get ( 'MME TOKEN' ) ) app . config [ "JSON SORT KEYS" ] = False current log level = logger . get Effective Level ( ) coloredlogs . install ( level = 'DEBUG' if app . debug else current log level ) configure extensions ( app ) register blueprints ( app ) register filters ( app ) if not ( app . debug or app . testing ) and app . config . get ( 'MAIL USERNAME' ) : # setup email logging of errors configure email logging ( app ) @ app . before request def check user ( ) : if not app . config . get ( 'LOGIN DISABLED' ) and request . endpoint : # check if the endpoint requires authentication static endpoint = 'static' in request . endpoint or 'report' in request . endpoint public endpoint = getattr ( app . view functions [ request . endpoint ] , 'is public' , False ) relevant endpoint = not ( static endpoint or public endpoint ) # if endpoint requires auth, check if user is authenticated if relevant endpoint and not current user . is authenticated : # combine visited URL (convert byte string query string to unicode!) next url = "{}?{}" . format ( request . path , request . query string . decode ( ) ) login url = url for ( 'login.login' , next = next url ) return redirect ( login url ) return app
def configure extensions ( app ) : extensions . toolbar . init app ( app ) extensions . bootstrap . init app ( app ) extensions . mongo . init app ( app ) extensions . store . init app ( app ) extensions . login manager . init app ( app ) extensions . oauth . init app ( app ) extensions . mail . init app ( app ) Markdown ( app ) if app . config . get ( 'SQLALCHEMY DATABASE URI' ) : configure coverage ( app ) if app . config . get ( 'LOQUSDB SETTINGS' ) : # setup Loqus DB extensions . loqusdb . init app ( app )
def register blueprints ( app ) : app . register blueprint ( public . public bp ) app . register blueprint ( genes . genes bp ) app . register blueprint ( cases . cases bp ) app . register blueprint ( login . login bp ) app . register blueprint ( variants . variants bp ) app . register blueprint ( panels . panels bp ) app . register blueprint ( dashboard . dashboard bp ) app . register blueprint ( api . api bp ) app . register blueprint ( alignviewers . alignviewers bp ) app . register blueprint ( phenotypes . hpo bp ) app . register blueprint ( institutes . overview )
def configure coverage ( app ) : # setup chanjo report app . config [ 'SQLALCHEMY TRACK MODIFICATIONS' ] = True if app . debug else False if chanjo api : chanjo api . init app ( app ) configure template filters ( app ) # register chanjo report blueprint app . register blueprint ( report bp , url prefix = '/reports' ) babel = Babel ( app ) @ babel . localeselector def get locale ( ) : """Determine locale to use for translations.""" accept languages = current app . config . get ( 'ACCEPT LANGUAGES' , [ 'en' ] ) # first check request args session language = request . args . get ( 'lang' ) if session language in accept languages : current app . logger . info ( "using session language: %s" , session language ) return session language # language can be forced in config user language = current app . config . get ( 'REPORT LANGUAGE' ) if user language : return user language # try to guess the language from the user accept header that # the browser transmits.  We support de/fr/en in this example. # The best match wins. return request . accept languages . best match ( accept languages )
def aliases ( context , build , symbol ) : LOG . info ( "Running scout view aliases" ) adapter = context . obj [ 'adapter' ] if symbol : alias genes = { } res = adapter . gene by alias ( symbol , build = build ) for gene obj in res : hgnc id = gene obj [ 'hgnc id' ] # Collect the true symbol given by hgnc hgnc symbol = gene obj [ 'hgnc symbol' ] # Loop aver all aliases for alias in gene obj [ 'aliases' ] : true id = None # If the alias is the same as hgnc symbol we know the true id if alias == hgnc symbol : true id = hgnc id # If the alias is already in the list we add the id if alias in alias genes : alias genes [ alias ] [ 'ids' ] . add ( hgnc id ) if true id : alias genes [ alias ] [ 'true' ] = hgnc id else : alias genes [ alias ] = { 'true' : hgnc id , 'ids' : set ( [ hgnc id ] ) } else : alias genes = adapter . genes by alias ( build = build ) if len ( alias genes ) == 0 : LOG . info ( "No gene found for build %s" , build ) return click . echo ( "#hgnc symbol\ttrue id\thgnc ids" ) for alias symbol in alias genes : info = alias genes [ alias symbol ] # pp(info) click . echo ( "{0}\t{1}\t{2}\t" . format ( alias symbol , ( alias genes [ alias symbol ] [ 'true' ] or 'None' ) , ', ' . join ( [ str ( gene id ) for gene id in alias genes [ alias symbol ] [ 'ids' ] ] ) ) )
def variants ( context , collaborator , document id , case id , json ) : LOG . info ( "Running scout export variants" ) adapter = context . obj [ 'adapter' ] collaborator = collaborator or 'cust000' variants = export variants ( adapter , collaborator , document id = document id , case id = case id ) if json : click . echo ( dumps ( [ var for var in variants ] ) ) return vcf header = VCF HEADER #If case id is given, print more complete vcf entries, with INFO, #and genotypes if case id : vcf header [ - 1 ] = vcf header [ - 1 ] + "\t FORMAT" case obj = adapter . case ( case id = case id ) for individual in case obj [ 'individuals' ] : vcf header [ - 1 ] = vcf header [ - 1 ] + "\t" + individual [ 'individual id' ] #print header for line in vcf header : click . echo ( line ) for variant obj in variants : variant string = get vcf entry ( variant obj , case id = case id ) click . echo ( variant string )
def serve ( context , config , host , port , debug , livereload ) : pymongo config = dict ( MONGO HOST = context . obj [ 'host' ] , MONGO PORT = context . obj [ 'port' ] , MONGO DBNAME = context . obj [ 'mongodb' ] , MONGO USERNAME = context . obj [ 'username' ] , MONGO PASSWORD = context . obj [ 'password' ] , ) valid connection = check connection ( host = pymongo config [ 'MONGO HOST' ] , port = pymongo config [ 'MONGO PORT' ] , username = pymongo config [ 'MONGO USERNAME' ] , password = pymongo config [ 'MONGO PASSWORD' ] , authdb = context . obj [ 'authdb' ] , ) log . info ( "Test if mongod is running" ) if not valid connection : log . warning ( "Connection could not be established" ) log . info ( "Is mongod running?" ) context . abort ( ) config = os . path . abspath ( config ) if config else None app = create app ( config = pymongo config , config file = config ) if livereload : server = Server ( app . wsgi app ) server . serve ( host = host , port = port , debug = debug ) else : app . run ( host = host , port = port , debug = debug )
def init app ( self , app ) : host = app . config . get ( 'MONGO HOST' , 'localhost' ) port = app . config . get ( 'MONGO PORT' , 27017 ) dbname = app . config [ 'MONGO DBNAME' ] log . info ( "connecting to database: %s:%s/%s" , host , port , dbname ) self . setup ( app . config [ 'MONGO DATABASE' ] )
def setup ( self , database ) : self . db = database self . hgnc collection = database . hgnc gene self . user collection = database . user self . whitelist collection = database . whitelist self . institute collection = database . institute self . event collection = database . event self . case collection = database . case self . panel collection = database . gene panel self . hpo term collection = database . hpo term self . disease term collection = database . disease term self . variant collection = database . variant self . acmg collection = database . acmg self . clinvar collection = database . clinvar self . clinvar submission collection = database . clinvar submission self . exon collection = database . exon self . transcript collection = database . transcript
def index ( context , update ) : LOG . info ( "Running scout index" ) adapter = context . obj [ 'adapter' ] if update : adapter . update indexes ( ) else : adapter . load indexes ( )
def database ( context , institute name , user name , user mail , api key ) : LOG . info ( "Running scout setup database" ) # Fetch the omim information api key = api key or context . obj . get ( 'omim api key' ) if not api key : LOG . warning ( "Please provide a omim api key with --api-key" ) context . abort ( ) institute name = institute name or context . obj [ 'institute name' ] user name = user name or context . obj [ 'user name' ] user mail = user mail or context . obj [ 'user mail' ] adapter = context . obj [ 'adapter' ] LOG . info ( "Setting up database %s" , context . obj [ 'mongodb' ] ) setup scout ( adapter = adapter , institute id = institute name , user name = user name , user mail = user mail , api key = api key )
def setup ( context , institute , user mail , user name ) : context . obj [ 'institute name' ] = institute context . obj [ 'user name' ] = user name context . obj [ 'user mail' ] = user mail if context . invoked subcommand == 'demo' : # Update context.obj settings here LOG . debug ( "Change database name to scout-demo" ) context . obj [ 'mongodb' ] = 'scout-demo' LOG . info ( "Setting database name to %s" , context . obj [ 'mongodb' ] ) LOG . debug ( "Setting host to %s" , context . obj [ 'host' ] ) LOG . debug ( "Setting port to %s" , context . obj [ 'port' ] ) try : client = get connection ( host = context . obj [ 'host' ] , port = context . obj [ 'port' ] , username = context . obj [ 'username' ] , password = context . obj [ 'password' ] , mongodb = context . obj [ 'mongodb' ] ) except Connection Failure : context . abort ( ) LOG . info ( "connecting to database %s" , context . obj [ 'mongodb' ] ) database = client [ context . obj [ 'mongodb' ] ] LOG . info ( "Test if mongod is running" ) try : LOG . info ( "Test if mongod is running" ) database . test . find one ( ) except Server Selection Timeout Error as err : LOG . warning ( "Connection could not be established" ) LOG . warning ( "Please check if mongod is running" ) context . abort ( ) LOG . info ( "Setting up a mongo adapter" ) mongo adapter = Mongo Adapter ( database ) context . obj [ 'adapter' ] = mongo adapter
def institutes ( context , institute id , json ) : LOG . info ( "Running scout view institutes" ) adapter = context . obj [ 'adapter' ] if institute id : institute objs = [ ] institute obj = adapter . institute ( institute id ) if not institute obj : LOG . info ( "Institute %s does not exost" , institute id ) return institute objs . append ( institute obj ) else : institute objs = [ ins obj for ins obj in adapter . institutes ( ) ] if len ( institute objs ) == 0 : click . echo ( "No institutes found" ) context . abort ( ) header = '' if not json : for key in institute objs [ 0 ] . keys ( ) : header = header + "{0}\t" . format ( key ) click . echo ( header ) for institute obj in institute objs : if json : click . echo ( institute obj ) continue row = '' for value in institute obj . values ( ) : row = row + "{0}\t" . format ( value ) click . echo ( row )
def panels ( context , institute ) : LOG . info ( "Running scout view panels" ) adapter = context . obj [ 'adapter' ] panel objs = adapter . gene panels ( institute id = institute ) if panel objs . count ( ) == 0 : LOG . info ( "No panels found" ) context . abort ( ) click . echo ( "#panel name\tversion\tnr genes\tdate" ) for panel obj in panel objs : click . echo ( "{0}\t{1}\t{2}\t{3}" . format ( panel obj [ 'panel name' ] , str ( panel obj [ 'version' ] ) , len ( panel obj [ 'genes' ] ) , str ( panel obj [ 'date' ] . strftime ( '%Y-%m-%d' ) ) ) )
def hpo genes ( context , hpo term ) : LOG . info ( "Running scout export hpo genes" ) adapter = context . obj [ 'adapter' ] header = [ "#Gene id\t Count" ] if not hpo term : LOG . warning ( "Please use at least one hpo term" ) context . abort ( ) for line in header : click . echo ( line ) for term in adapter . generate hpo gene list ( * hpo term ) : click . echo ( "{0}\t{1}" . format ( term [ 0 ] , term [ 1 ] ) )
def user ( context , institute id , user name , user mail , admin ) : adapter = context . obj [ 'adapter' ] institutes = [ ] for institute in institute id : institute obj = adapter . institute ( institute id = institute ) if not institute obj : LOG . warning ( "Institute % does not exist" , institute ) context . abort ( ) institutes . append ( institute ) roles = [ ] if admin : LOG . info ( "User is admin" ) roles . append ( 'admin' ) user info = dict ( email = user mail . lower ( ) , name = user name , roles = roles , institutes = institutes ) user obj = build user ( user info ) try : adapter . add user ( user obj ) except Exception as err : LOG . warning ( err ) context . abort ( )
def institutes ( ) : institute objs = user institutes ( store , current user ) institutes = [ ] for ins obj in institute objs : sanger recipients = [ ] for user mail in ins obj . get ( 'sanger recipients' , [ ] ) : user obj = store . user ( user mail ) if not user obj : continue sanger recipients . append ( user obj [ 'name' ] ) institutes . append ( { 'display name' : ins obj [ 'display name' ] , 'internal id' : ins obj [ ' id' ] , 'coverage cutoff' : ins obj . get ( 'coverage cutoff' , 'None' ) , 'sanger recipients' : sanger recipients , 'frequency cutoff' : ins obj . get ( 'frequency cutoff' , 'None' ) , 'phenotype groups' : ins obj . get ( 'phenotype groups' , PHENOTYPE GROUPS ) } ) data = dict ( institutes = institutes ) return render template ( 'overview/institutes.html' , * * data )
def remote static ( ) : file path = request . args . get ( 'file' ) range header = request . headers . get ( 'Range' , None ) if not range header and file path . endswith ( '.bam' ) : return abort ( 500 ) new resp = send file partial ( file path ) return new resp
def pileup ( ) : vcf file = request . args . get ( 'vcf' ) bam files = request . args . getlist ( 'bam' ) bai files = request . args . getlist ( 'bai' ) samples = request . args . getlist ( 'sample' ) alignments = [ { 'bam' : bam , 'bai' : bai , 'sample' : sample } for bam , bai , sample in zip ( bam files , bai files , samples ) ] position = { 'contig' : request . args [ 'contig' ] , 'start' : request . args [ 'start' ] , 'stop' : request . args [ 'stop' ] } genome = current app . config . get ( 'PILEUP GENOME' ) if genome : if not os . path . isfile ( genome ) : flash ( "The pilup genome path ({}) provided does not exist" . format ( genome ) ) genome = None LOG . debug ( "Use pileup genome %s" , genome ) exons = current app . config . get ( 'PILEUP EXONS' ) if exons : if not os . path . isfile ( exons ) : flash ( "The pilup exons path ({}) provided does not exist" . format ( exons ) ) genome = None LOG . debug ( "Use pileup exons %s" , exons ) LOG . debug ( "View alignment for positions Chrom:{0}, Start:{1}, End: {2}" . format ( position [ 'contig' ] , position [ 'start' ] , position [ 'stop' ] ) ) LOG . debug ( "Use alignment files {}" . format ( alignments ) ) return render template ( 'alignviewers/pileup.html' , alignments = alignments , position = position , vcf file = vcf file , genome = genome , exons = exons )
def compounds ( context , case id ) : adapter = context . obj [ 'adapter' ] LOG . info ( "Running scout update compounds" ) # Check if the case exists case obj = adapter . case ( case id ) if not case obj : LOG . warning ( "Case %s could not be found" , case id ) context . abort ( ) try : adapter . update case compounds ( case obj ) except Exception as err : LOG . warning ( err ) context . abort ( )
def hgnc ( ctx , hgnc symbol , hgnc id , build ) : adapter = ctx . obj [ 'adapter' ] if not ( hgnc symbol or hgnc id ) : log . warning ( "Please provide a hgnc symbol or hgnc id" ) ctx . abort ( ) if hgnc id : result = adapter . hgnc gene ( hgnc id , build = build ) if result : hgnc symbol = result [ 'hgnc symbol' ] else : log . warning ( "Gene with id %s could not be found" , hgnc id ) ctx . abort ( ) result = adapter . hgnc genes ( hgnc symbol , build = build ) if result . count ( ) == 0 : log . info ( "No results found" ) else : click . echo ( "#hgnc id\thgnc symbol\taliases\ttranscripts" ) for gene in result : click . echo ( "{0}\t{1}\t{2}\t{3}" . format ( gene [ 'hgnc id' ] , gene [ 'hgnc symbol' ] , ', ' . join ( gene [ 'aliases' ] ) , ', ' . join ( tx [ 'ensembl transcript id' ] for tx in gene [ 'transcripts' ] ) , ) )
def parse hpo obo ( hpo lines ) : term = { } for line in hpo lines : if len ( line ) == 0 : continue line = line . rstrip ( ) # New term starts with [Term] if line == '[Term]' : if term : yield term term = { } elif line . startswith ( 'id' ) : term [ 'hpo id' ] = line [ 4 : ] elif line . startswith ( 'name' ) : term [ 'description' ] = line [ 6 : ] elif line . startswith ( 'alt id' ) : if 'aliases' not in term : term [ 'aliases' ] = [ ] term [ 'aliases' ] . append ( line [ 8 : ] ) elif line . startswith ( 'is a' ) : if 'ancestors' not in term : term [ 'ancestors' ] = [ ] term [ 'ancestors' ] . append ( line [ 6 : 16 ] ) if term : yield term
def genes ( ) : query = request . args . get ( 'query' , '' ) if '|' in query : hgnc id = int ( query . split ( ' | ' , 1 ) [ 0 ] ) return redirect ( url for ( '.gene' , hgnc id = hgnc id ) ) gene q = store . all genes ( ) . limit ( 20 ) return dict ( genes = gene q )
def gene ( hgnc id = None , hgnc symbol = None ) : if hgnc symbol : query = store . hgnc genes ( hgnc symbol ) if query . count ( ) == 1 : hgnc id = query . first ( ) [ 'hgnc id' ] else : return redirect ( url for ( '.genes' , query = hgnc symbol ) ) try : genes = controllers . gene ( store , hgnc id ) except Value Error as error : return abort ( 404 ) return genes
def api genes ( ) : query = request . args . get ( 'query' ) json out = controllers . genes to json ( store , query ) return jsonify ( json out )
def institute and case ( store , institute id , case name = None ) : institute obj = store . institute ( institute id ) if institute obj is None and institute id != 'favicon.ico' : flash ( "Can't find institute: {}" . format ( institute id ) , 'warning' ) return abort ( 404 ) if case name : if case name : case obj = store . case ( institute id = institute id , display name = case name ) if case obj is None : return abort ( 404 ) # validate that user has access to the institute if not current user . is admin : if institute id not in current user . institutes : if not case name or not any ( inst id in case obj [ 'collaborators' ] for inst id in current user . institutes ) : # you don't have access!! flash ( "You don't have acccess to: {}" . format ( institute id ) , 'danger' ) return abort ( 403 ) # you have access! if case name : return institute obj , case obj else : return institute obj
def user institutes ( store , login user ) : if login user . is admin : institutes = store . institutes ( ) else : institutes = [ store . institute ( inst id ) for inst id in login user . institutes ] return institutes
def panel ( context , panel , version , update date , update version ) : adapter = context . obj [ 'adapter' ] # Check that the panel exists panel obj = adapter . gene panel ( panel , version = version ) if not panel obj : LOG . warning ( "Panel %s (version %s) could not be found" % ( panel , version ) ) context . abort ( ) date obj = None if update date : try : date obj = get date ( update date ) except Exception as err : LOG . warning ( err ) context . abort ( ) update panel ( adapter , panel , panel version = panel obj [ 'version' ] , new version = update version , new date = date obj )
def diseases ( context , api key ) : adapter = context . obj [ 'adapter' ] # Fetch the omim information api key = api key or context . obj . get ( 'omim api key' ) if not api key : LOG . warning ( "Please provide a omim api key to load the omim gene panel" ) context . abort ( ) try : mim files = fetch mim files ( api key , genemap2 = True ) except Exception as err : LOG . warning ( err ) context . abort ( ) LOG . info ( "Dropping Disease Terms" ) adapter . disease term collection . drop ( ) LOG . debug ( "Disease Terms dropped" ) load disease terms ( adapter = adapter , genemap lines = mim files [ 'genemap2' ] , ) LOG . info ( "Successfully loaded all disease terms" )
def users ( context ) : LOG . info ( "Running scout view users" ) adapter = context . obj [ 'adapter' ] user objs = adapter . users ( ) if user objs . count ( ) == 0 : LOG . info ( "No users found" ) context . abort ( ) click . echo ( "#name\temail\troles\tinstitutes" ) for user obj in user objs : click . echo ( "{0}\t{1}\t{2}\t{3}\t" . format ( user obj [ 'name' ] , user obj . get ( 'mail' , user obj [ ' id' ] ) , ', ' . join ( user obj . get ( 'roles' , [ ] ) ) , ', ' . join ( user obj . get ( 'institutes' , [ ] ) ) , ) )
def load omim panel ( self , api key , institute = None ) : existing panel = self . gene panel ( panel id = 'OMIM-AUTO' ) if not existing panel : LOG . warning ( "OMIM-AUTO does not exists in database" ) LOG . info ( 'Creating a first version' ) version = 1.0 if existing panel : version = float ( math . floor ( existing panel [ 'version' ] ) + 1 ) LOG . info ( "Setting version to %s" , version ) try : mim files = fetch mim files ( api key = api key , genemap2 = True , mim2genes = True ) except Exception as err : raise err date string = None # Get the correct date when omim files where released for line in mim files [ 'genemap2' ] : if 'Generated' in line : date string = line . split ( ':' ) [ - 1 ] . lstrip ( ) . rstrip ( ) date obj = get date ( date string ) if existing panel : if existing panel [ 'date' ] == date obj : LOG . warning ( "There is no new version of OMIM" ) return panel data = { } panel data [ 'path' ] = None panel data [ 'type' ] = 'clinical' panel data [ 'date' ] = date obj panel data [ 'panel id' ] = 'OMIM-AUTO' panel data [ 'institute' ] = institute or 'cust002' panel data [ 'version' ] = version panel data [ 'display name' ] = 'OMIM-AUTO' panel data [ 'genes' ] = [ ] alias genes = self . genes by alias ( ) genes = get omim panel genes ( genemap2 lines = mim files [ 'genemap2' ] , mim2gene lines = mim files [ 'mim2genes' ] , alias genes = alias genes , ) for gene in genes : panel data [ 'genes' ] . append ( gene ) panel obj = build panel ( panel data , self ) if existing panel : new genes = self . compare mim panels ( existing panel , panel obj ) if new genes : self . update mim version ( new genes , panel obj , old version = existing panel [ 'version' ] ) else : LOG . info ( "The new version of omim does not differ from the old one" ) LOG . info ( "No update is added" ) return self . add gene panel ( panel obj )
def clinical symbols ( self , case obj ) : panel ids = [ panel [ 'panel id' ] for panel in case obj [ 'panels' ] ] query = self . panel collection . aggregate ( [ { '$match' : { ' id' : { '$in' : panel ids } } } , { '$unwind' : '$genes' } , { '$group' : { ' id' : '$genes.symbol' } } ] ) return set ( item [ ' id' ] for item in query )
def cases ( context , case id , institute , reruns , finished , causatives , research requested , is research , status , json ) : adapter = context . obj [ 'adapter' ] models = [ ] if case id : case obj = adapter . case ( case id = case id ) if case obj : models . append ( case obj ) else : LOG . info ( "No case with id {}" . format ( case id ) ) else : models = adapter . cases ( collaborator = institute , reruns = reruns , finished = finished , has causatives = causatives , research requested = research requested , is research = is research , status = status ) models = [ case obj for case obj in models ] if len ( models ) == 0 : LOG . info ( "No cases could be found" ) if json : click . echo ( dumps ( models ) ) return for model in models : pp ( model )
def drop indexes ( self ) : LOG . warning ( "Dropping all indexe" ) for collection name in INDEXES : LOG . warning ( "Dropping all indexes for collection name %s" , collection name ) self . db [ collection name ] . drop indexes ( )
def wipe ( ctx ) : LOG . info ( "Running scout wipe" ) db name = ctx . obj [ 'mongodb' ] LOG . info ( "Dropping database %s" , db name ) try : ctx . obj [ 'client' ] . drop database ( db name ) except Exception as err : LOG . warning ( err ) ctx . abort ( ) LOG . info ( "Dropped whole database" )
def parse panel ( csv stream ) : reader = csv . Dict Reader ( csv stream , delimiter = ';' , quoting = csv . QUOTE NONE ) genes = [ ] for gene row in reader : if not gene row [ 'HGNC I Dnumber' ] . strip ( ) . isdigit ( ) : continue transcripts raw = gene row . get ( 'Disease associated transcript' ) if transcripts raw : transcripts list = [ tx . split ( ':' , 1 ) [ - 1 ] . strip ( ) for tx in transcripts raw . split ( ',' ) ] else : transcripts list = [ ] models raw = gene row . get ( 'Genetic disease model' ) models list = [ model . strip ( ) for model in models raw . split ( ',' ) ] if models raw else [ ] panel gene = dict ( symbol = gene row [ 'HGNC symbol' ] . strip ( ) if gene row . get ( 'HGNC symbol' ) else None , hgnc id = int ( gene row [ 'HGNC I Dnumber' ] . strip ( ) ) , disease associated transcripts = transcripts list , reduced penetrance = True if gene row . get ( 'Reduced penetrance' ) else None , mosaicism = True if gene row . get ( 'Mosaicism' ) else None , inheritance models = models list , database entry version = gene row . get ( 'Database entry version' ) , ) genes . append ( panel gene ) return genes
def drop genes ( self , build = None ) : if build : LOG . info ( "Dropping the hgnc gene collection, build %s" , build ) self . hgnc collection . delete many ( { 'build' : build } ) else : LOG . info ( "Dropping the hgnc gene collection" ) self . hgnc collection . drop ( )
def drop transcripts ( self , build = None ) : if build : LOG . info ( "Dropping the transcripts collection, build %s" , build ) self . transcript collection . delete many ( { 'build' : build } ) else : LOG . info ( "Dropping the transcripts collection" ) self . transcript collection . drop ( )
def drop exons ( self , build = None ) : if build : LOG . info ( "Dropping the exons collection, build %s" , build ) self . exon collection . delete many ( { 'build' : build } ) else : LOG . info ( "Dropping the exons collection" ) self . exon collection . drop ( )
def omim ( context , api key , institute ) : LOG . info ( "Running scout update omim" ) adapter = context . obj [ 'adapter' ] api key = api key or context . obj . get ( 'omim api key' ) if not api key : LOG . warning ( "Please provide a omim api key to load the omim gene panel" ) context . abort ( ) institute obj = adapter . institute ( institute ) if not institute obj : LOG . info ( "Institute %s could not be found in database" , institute ) LOG . warning ( "Please specify an existing institute" ) context . abort ( ) try : adapter . load omim panel ( api key , institute = institute ) except Exception as err : LOG . error ( err ) context . abort ( )
def index ( ) : institute objs = user institutes ( store , current user ) institutes count = ( ( institute obj , store . cases ( collaborator = institute obj [ ' id' ] ) . count ( ) ) for institute obj in institute objs if institute obj ) return dict ( institutes = institutes count )
def cases ( institute id ) : institute obj = institute and case ( store , institute id ) query = request . args . get ( 'query' ) limit = 100 if request . args . get ( 'limit' ) : limit = int ( request . args . get ( 'limit' ) ) skip assigned = request . args . get ( 'skip assigned' ) is research = request . args . get ( 'is research' ) all cases = store . cases ( collaborator = institute id , name query = query , skip assigned = skip assigned , is research = is research ) data = controllers . cases ( store , all cases , limit ) sanger unevaluated = controllers . get sanger unevaluated ( store , institute id , current user . email ) if len ( sanger unevaluated ) > 0 : data [ 'sanger unevaluated' ] = sanger unevaluated return dict ( institute = institute obj , skip assigned = skip assigned , is research = is research , query = query , * * data )
def case ( institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) data = controllers . case ( store , institute obj , case obj ) return dict ( institute = institute obj , case = case obj , * * data )
def matchmaker matches ( institute id , case name ) : # check that only authorized users can access MME patients matches user obj = store . user ( current user . email ) if 'mme submitter' not in user obj [ 'roles' ] : flash ( 'unauthorized request' , 'warning' ) return redirect ( request . referrer ) # Required params for getting matches from MME server: mme base url = current app . config . get ( 'MME URL' ) mme token = current app . config . get ( 'MME TOKEN' ) if not mme base url or not mme token : flash ( 'An error occurred reading matchmaker connection parameters. Please check config file!' , 'danger' ) return redirect ( request . referrer ) institute obj , case obj = institute and case ( store , institute id , case name ) data = controllers . mme matches ( case obj , institute obj , mme base url , mme token ) if data and data . get ( 'server errors' ) : flash ( 'Match Maker server returned error:{}' . format ( data [ 'server errors' ] ) , 'danger' ) return redirect ( request . referrer ) elif not data : data = { 'institute' : institute obj , 'case' : case obj } return data
def matchmaker match ( institute id , case name , target ) : institute obj , case obj = institute and case ( store , institute id , case name ) # check that only authorized users can run matches user obj = store . user ( current user . email ) if 'mme submitter' not in user obj [ 'roles' ] : flash ( 'unauthorized request' , 'warning' ) return redirect ( request . referrer ) # Required params for sending an add request to MME: mme base url = current app . config . get ( 'MME URL' ) mme accepts = current app . config . get ( 'MME ACCEPTS' ) mme token = current app . config . get ( 'MME TOKEN' ) nodes = current app . mme nodes if not mme base url or not mme token or not mme accepts : flash ( 'An error occurred reading matchmaker connection parameters. Please check config file!' , 'danger' ) return redirect ( request . referrer ) match results = controllers . mme match ( case obj , target , mme base url , mme token , nodes , mme accepts ) ok responses = 0 for match results in match results : match results [ 'status code' ] == 200 ok responses += 1 if ok responses : flash ( "Match request sent. Look for eventual matches in 'Matches' page." , 'info' ) else : flash ( 'An error occurred while sending match request.' , 'danger' ) return redirect ( request . referrer )
def matchmaker add ( institute id , case name ) : # check that only authorized users can add patients to MME user obj = store . user ( current user . email ) if 'mme submitter' not in user obj [ 'roles' ] : flash ( 'unauthorized request' , 'warning' ) return redirect ( request . referrer ) institute obj , case obj = institute and case ( store , institute id , case name ) causatives = False features = False if case obj . get ( 'suspects' ) and len ( case obj . get ( 'suspects' ) ) > 3 : flash ( 'At the moment it is not possible to save to Match Maker more than 3 pinned variants' , 'warning' ) return redirect ( request . referrer ) elif case obj . get ( 'suspects' ) : causatives = True if case obj . get ( 'phenotype terms' ) : features = True mme save options = [ 'sex' , 'features' , 'disorders' ] for index , item in enumerate ( mme save options ) : if item in request . form : log . info ( 'item {} is in request form' . format ( item ) ) mme save options [ index ] = True else : mme save options [ index ] = False genomic features = request . form . get ( 'genomicfeatures' ) genes only = True # upload to matchmaker only gene names if genomic features == 'variants' : genes only = False # upload to matchmaker both variants and gene names # If there are no genomic features nor HPO terms to share for this case, abort if ( not case obj . get ( 'suspects' ) and not mme save options [ 1 ] ) or ( causatives is False and features is False ) : flash ( 'In order to upload a case to Match Maker you need to pin a variant or at least assign a phenotype (HPO term)' , 'danger' ) return redirect ( request . referrer ) user obj = store . user ( current user . email ) # Required params for sending an add request to MME: mme base url = current app . config . get ( 'MME URL' ) mme accepts = current app . config . get ( 'MME ACCEPTS' ) mme token = current app . config . get ( 'MME TOKEN' ) if not mme base url or not mme accepts or not mme token : flash ( 'An error occurred reading matchmaker connection parameters. Please check config file!' , 'danger' ) return redirect ( request . referrer ) add result = controllers . mme add ( store = store , user obj = user obj , case obj = case obj , add gender = mme save options [ 0 ] , add features = mme save options [ 1 ] , add disorders = mme save options [ 2 ] , genes only = genes only , mme base url = mme base url , mme accepts = mme accepts , mme token = mme token ) # flash MME responses (one for each patient posted) n succes response = 0 n inserted = 0 n updated = 0 category = 'warning' for resp in add result [ 'server responses' ] : message = resp . get ( 'message' ) if resp . get ( 'status code' ) == 200 : n succes response += 1 else : flash ( 'an error occurred while adding patient to matchmaker: {}' . format ( message ) , 'warning' ) if message == 'Patient was successfully updated.' : n updated += 1 elif message == 'Patient was successfully inserted into database.' : n inserted += 1 # if at least one patient was inserted or updated into matchmaker, save submission at the case level: if n inserted or n updated : category = 'success' store . case mme update ( case obj = case obj , user obj = user obj , mme subm obj = add result ) flash ( 'Number of new patients in matchmaker:{0}, number of updated records:{1}, number of failed requests:{2}' . format ( n inserted , n updated , len ( add result . get ( 'server responses' ) ) - n succes response ) , category ) return redirect ( request . referrer )
def matchmaker delete ( institute id , case name ) : # check that only authorized users can delete patients from MME user obj = store . user ( current user . email ) if 'mme submitter' not in user obj [ 'roles' ] : flash ( 'unauthorized request' , 'warning' ) return redirect ( request . referrer ) institute obj , case obj = institute and case ( store , institute id , case name ) # Required params for sending a delete request to MME: mme base url = current app . config . get ( 'MME URL' ) mme token = current app . config . get ( 'MME TOKEN' ) if not mme base url or not mme token : flash ( 'An error occurred reading matchmaker connection parameters. Please check config file!' , 'danger' ) return redirect ( request . referrer ) delete result = controllers . mme delete ( case obj , mme base url , mme token ) n deleted = 0 category = 'warning' for resp in delete result : if resp [ 'status code' ] == 200 : n deleted += 1 else : flash ( resp [ 'message' ] , category ) if n deleted : category = 'success' # update case by removing mme submission # and create events for patients deletion from MME user obj = store . user ( current user . email ) store . case mme delete ( case obj = case obj , user obj = user obj ) flash ( 'Number of patients deleted from Matchmaker: {} out of {}' . format ( n deleted , len ( delete result ) ) , category ) return redirect ( request . referrer )
def gene variants ( institute id ) : page = int ( request . form . get ( 'page' , 1 ) ) institute obj = institute and case ( store , institute id ) # populate form, conditional on request method if ( request . method == "POST" ) : form = Gene Variant Filters Form ( request . form ) else : form = Gene Variant Filters Form ( request . args ) variant type = form . data . get ( 'variant type' , 'clinical' ) # check if supplied gene symbols exist hgnc symbols = [ ] non clinical symbols = [ ] not found symbols = [ ] not found ids = [ ] data = { } if ( form . hgnc symbols . data ) and len ( form . hgnc symbols . data ) > 0 : is clinical = form . data . get ( 'variant type' , 'clinical' ) == 'clinical' clinical symbols = store . clinical symbols ( case obj ) if is clinical else None for hgnc symbol in form . hgnc symbols . data : if hgnc symbol . isdigit ( ) : hgnc gene = store . hgnc gene ( int ( hgnc symbol ) ) if hgnc gene is None : not found ids . append ( hgnc symbol ) else : hgnc symbols . append ( hgnc gene [ 'hgnc symbol' ] ) elif store . hgnc genes ( hgnc symbol ) . count ( ) == 0 : not found symbols . append ( hgnc symbol ) elif is clinical and ( hgnc symbol not in clinical symbols ) : non clinical symbols . append ( hgnc symbol ) else : hgnc symbols . append ( hgnc symbol ) if ( not found ids ) : flash ( "HGNC id not found: {}" . format ( ", " . join ( not found ids ) ) , 'warning' ) if ( not found symbols ) : flash ( "HGNC symbol not found: {}" . format ( ", " . join ( not found symbols ) ) , 'warning' ) if ( non clinical symbols ) : flash ( "Gene not included in clinical list: {}" . format ( ", " . join ( non clinical symbols ) ) , 'warning' ) form . hgnc symbols . data = hgnc symbols log . debug ( "query {}" . format ( form . data ) ) variants query = store . gene variants ( query = form . data , category = 'snv' , variant type = variant type ) data = controllers . gene variants ( store , variants query , page ) return dict ( institute = institute obj , form = form , page = page , * * data )
def pdf case report ( institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) data = controllers . case report content ( store , institute obj , case obj ) # add coverage report on the bottom of this report if current app . config . get ( 'SQLALCHEMY DATABASE URI' ) : data [ 'coverage report' ] = controllers . coverage report contents ( store , institute obj , case obj , request . url root ) # workaround to be able to print the case pedigree to pdf if case obj . get ( 'madeline info' ) is not None : with open ( os . path . join ( cases bp . static folder , 'madeline.svg' ) , 'w' ) as temp madeline : temp madeline . write ( case obj [ 'madeline info' ] ) html report = render template ( 'cases/case report.html' , institute = institute obj , case = case obj , format = 'pdf' , * * data ) return render pdf ( HTML ( string = html report ) , download filename = case obj [ 'display name' ] + ' ' + datetime . datetime . now ( ) . strftime ( "%Y-%m-%d" ) + ' scout.pdf' )
def case diagnosis ( institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) user obj = store . user ( current user . email ) link = url for ( '.case' , institute id = institute id , case name = case name ) level = 'phenotype' if 'phenotype' in request . form else 'gene' omim id = request . form [ 'omim id' ] remove = True if request . args . get ( 'remove' ) == 'yes' else False store . diagnose ( institute obj , case obj , user obj , link , level = level , omim id = omim id , remove = remove ) return redirect ( request . referrer )
def phenotypes actions ( institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) case url = url for ( '.case' , institute id = institute id , case name = case name ) action = request . form [ 'action' ] hpo ids = request . form . getlist ( 'hpo id' ) user obj = store . user ( current user . email ) if action == 'DELETE' : for hpo id in hpo ids : # DELETE a phenotype from the list store . remove phenotype ( institute obj , case obj , user obj , case url , hpo id ) elif action == 'PHENOMIZER' : if len ( hpo ids ) == 0 : hpo ids = [ term [ 'phenotype id' ] for term in case obj . get ( 'phenotype terms' , [ ] ) ] username = current app . config [ 'PHENOMIZER USERNAME' ] password = current app . config [ 'PHENOMIZER PASSWORD' ] diseases = controllers . hpo diseases ( username , password , hpo ids ) return render template ( 'cases/diseases.html' , diseases = diseases , institute = institute obj , case = case obj ) elif action == 'GENES' : hgnc symbols = set ( ) for raw symbols in request . form . getlist ( 'genes' ) : # avoid empty lists if raw symbols : hgnc symbols . update ( raw symbol . split ( ' ' , 1 ) [ 0 ] for raw symbol in raw symbols . split ( '|' ) ) store . update dynamic gene list ( case obj , hgnc symbols = hgnc symbols ) elif action == 'GENERATE' : if len ( hpo ids ) == 0 : hpo ids = [ term [ 'phenotype id' ] for term in case obj . get ( 'phenotype terms' , [ ] ) ] results = store . generate hpo gene list ( * hpo ids ) # determine how many HPO terms each gene must match hpo count = int ( request . form . get ( 'min match' ) or 1 ) hgnc ids = [ result [ 0 ] for result in results if result [ 1 ] >= hpo count ] store . update dynamic gene list ( case obj , hgnc ids = hgnc ids , phenotype ids = hpo ids ) return redirect ( case url )
def status ( institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) user obj = store . user ( current user . email ) status = request . form . get ( 'status' , case obj [ 'status' ] ) link = url for ( '.case' , institute id = institute id , case name = case name ) if status == 'archive' : store . archive case ( institute obj , case obj , user obj , status , link ) else : store . update status ( institute obj , case obj , user obj , status , link ) return redirect ( request . referrer )
def assign ( institute id , case name , user id = None ) : institute obj , case obj = institute and case ( store , institute id , case name ) link = url for ( '.case' , institute id = institute id , case name = case name ) if user id : user obj = store . user ( user id ) else : user obj = store . user ( current user . email ) if request . form . get ( 'action' ) == 'DELETE' : store . unassign ( institute obj , case obj , user obj , link ) else : store . assign ( institute obj , case obj , user obj , link ) return redirect ( request . referrer )
def hpoterms ( ) : query = request . args . get ( 'query' ) if query is None : return abort ( 500 ) terms = sorted ( store . hpo terms ( query = query ) , key = itemgetter ( 'hpo number' ) ) json terms = [ { 'name' : '{} | {}' . format ( term [ ' id' ] , term [ 'description' ] ) , 'id' : term [ ' id' ] } for term in terms [ : 7 ] ] return jsonify ( json terms )
def mark validation ( institute id , case name , variant id ) : institute obj , case obj = institute and case ( store , institute id , case name ) variant obj = store . variant ( variant id ) user obj = store . user ( current user . email ) validate type = request . form [ 'type' ] or None link = url for ( 'variants.variant' , institute id = institute id , case name = case name , variant id = variant id ) store . validate ( institute obj , case obj , user obj , link , variant obj , validate type ) return redirect ( request . referrer or link )
def mark causative ( institute id , case name , variant id ) : institute obj , case obj = institute and case ( store , institute id , case name ) variant obj = store . variant ( variant id ) user obj = store . user ( current user . email ) link = url for ( 'variants.variant' , institute id = institute id , case name = case name , variant id = variant id ) if request . form [ 'action' ] == 'ADD' : store . mark causative ( institute obj , case obj , user obj , link , variant obj ) elif request . form [ 'action' ] == 'DELETE' : store . unmark causative ( institute obj , case obj , user obj , link , variant obj ) # send the user back to the case that was marked as solved case url = url for ( '.case' , institute id = institute id , case name = case name ) return redirect ( case url )
def delivery report ( institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) if case obj . get ( 'delivery report' ) is None : return abort ( 404 ) date str = request . args . get ( 'date' ) if date str : delivery report = None analysis date = parse date ( date str ) for analysis data in case obj [ 'analyses' ] : if analysis data [ 'date' ] == analysis date : delivery report = analysis data [ 'delivery report' ] if delivery report is None : return abort ( 404 ) else : delivery report = case obj [ 'delivery report' ] out dir = os . path . dirname ( delivery report ) filename = os . path . basename ( delivery report ) return send from directory ( out dir , filename )
def share ( institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) user obj = store . user ( current user . email ) collaborator id = request . form [ 'collaborator' ] revoke access = 'revoke' in request . form link = url for ( '.case' , institute id = institute id , case name = case name ) if revoke access : store . unshare ( institute obj , case obj , collaborator id , user obj , link ) else : store . share ( institute obj , case obj , collaborator id , user obj , link ) return redirect ( request . referrer )
def rerun ( institute id , case name ) : sender = current app . config [ 'MAIL USERNAME' ] recipient = current app . config [ 'TICKET SYSTEM EMAIL' ] controllers . rerun ( store , mail , current user , institute id , case name , sender , recipient ) return redirect ( request . referrer )
def research ( institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) user obj = store . user ( current user . email ) link = url for ( '.case' , institute id = institute id , case name = case name ) store . open research ( institute obj , case obj , user obj , link ) return redirect ( request . referrer )
def default panels ( institute id , case name ) : panel ids = request . form . getlist ( 'panel ids' ) controllers . update default panels ( store , current user , institute id , case name , panel ids ) return redirect ( request . referrer )
def vcf2cytosure ( institute id , case name , individual id ) : ( display name , vcf2cytosure ) = controllers . vcf2cytosure ( store , institute id , case name , individual id ) outdir = os . path . abspath ( os . path . dirname ( vcf2cytosure ) ) filename = os . path . basename ( vcf2cytosure ) log . debug ( "Attempt to deliver file {0} from dir {1}" . format ( filename , outdir ) ) attachment filename = display name + ".vcf2cytosure.cgh" return send from directory ( outdir , filename , attachment filename = attachment filename , as attachment = True )
def multiqc ( institute id , case name ) : data = controllers . multiqc ( store , institute id , case name ) if data [ 'case' ] . get ( 'multiqc' ) is None : return abort ( 404 ) out dir = os . path . abspath ( os . path . dirname ( data [ 'case' ] [ 'multiqc' ] ) ) filename = os . path . basename ( data [ 'case' ] [ 'multiqc' ] ) return send from directory ( out dir , filename )
def clinvar submissions ( store , user id , institute id ) : submissions = list ( store . clinvar submissions ( user id , institute id ) ) return submissions
def rerun ( store , mail , current user , institute id , case name , sender , recipient ) : institute obj , case obj = institute and case ( store , institute id , case name ) user obj = store . user ( current user . email ) link = url for ( 'cases.case' , institute id = institute id , case name = case name ) store . request rerun ( institute obj , case obj , user obj , link ) # this should send a JSON document to the Su Sy API in the future html = . format ( institute = institute obj [ 'display name' ] , case = case obj [ 'display name' ] , case id = case obj [ ' id' ] , name = user obj [ 'name' ] . encode ( ) ) # compose and send the email message msg = Message ( subject = ( "SCOUT: request RERUN for {}" . format ( case obj [ 'display name' ] ) ) , html = html , sender = sender , recipients = [ recipient ] , # cc the sender of the email for confirmation cc = [ user obj [ 'email' ] ] ) mail . send ( msg )
def update default panels ( store , current user , institute id , case name , panel ids ) : institute obj , case obj = institute and case ( store , institute id , case name ) user obj = store . user ( current user . email ) link = url for ( 'cases.case' , institute id = institute id , case name = case name ) panel objs = [ store . panel ( panel id ) for panel id in panel ids ] store . update default panels ( institute obj , case obj , user obj , link , panel objs )
def vcf2cytosure ( store , institute id , case name , individual id ) : institute obj , case obj = institute and case ( store , institute id , case name ) for individual in case obj [ 'individuals' ] : if individual [ 'individual id' ] == individual id : individual obj = individual return ( individual obj [ 'display name' ] , individual obj [ 'vcf2cytosure' ] )
def gene variants ( store , variants query , page = 1 , per page = 50 ) : variant count = variants query . count ( ) skip count = per page * max ( page - 1 , 0 ) more variants = True if variant count > ( skip count + per page ) else False variant res = variants query . skip ( skip count ) . limit ( per page ) my institutes = list ( inst [ ' id' ] for inst in user institutes ( store , current user ) ) variants = [ ] for variant obj in variant res : # hide other institutes for now if variant obj [ 'institute' ] not in my institutes : LOG . warning ( "Institute {} not allowed." . format ( variant obj [ 'institute' ] ) ) continue # Populate variant case display name variant case obj = store . case ( case id = variant obj [ 'case id' ] ) if not variant case obj : # A variant with missing case was encountered continue case display name = variant case obj . get ( 'display name' ) variant obj [ 'case display name' ] = case display name genome build = variant case obj . get ( 'genome build' , '37' ) if genome build not in [ '37' , '38' ] : genome build = '37' # Update the HGNC symbols if they are not set variant genes = variant obj . get ( 'genes' ) if variant genes is not None : for gene obj in variant genes : # If there is no hgnc id there is nothin we can do if not gene obj [ 'hgnc id' ] : continue # Else we collect the gene object and check the id if gene obj . get ( 'hgnc symbol' ) is None or gene obj . get ( 'description' ) is None : hgnc gene = store . hgnc gene ( gene obj [ 'hgnc id' ] , build = genome build ) if not hgnc gene : continue gene obj [ 'hgnc symbol' ] = hgnc gene [ 'hgnc symbol' ] gene obj [ 'description' ] = hgnc gene [ 'description' ] # Populate variant HGVS and predictions gene ids = [ ] gene symbols = [ ] hgvs c = [ ] hgvs p = [ ] variant genes = variant obj . get ( 'genes' ) if variant genes is not None : functional annotation = '' for gene obj in variant genes : hgnc id = gene obj [ 'hgnc id' ] gene symbol = gene ( store , hgnc id ) [ 'symbol' ] gene ids . append ( hgnc id ) gene symbols . append ( gene symbol ) hgvs nucleotide = '-' # gather HGVS info from gene transcripts transcripts list = gene obj . get ( 'transcripts' ) for transcript obj in transcripts list : if transcript obj . get ( 'is canonical' ) and transcript obj . get ( 'is canonical' ) is True : hgvs nucleotide = str ( transcript obj . get ( 'coding sequence name' ) ) hgvs protein = str ( transcript obj . get ( 'protein sequence name' ) ) hgvs c . append ( hgvs nucleotide ) hgvs p . append ( hgvs protein ) if len ( gene symbols ) == 1 : if ( hgvs p [ 0 ] != "None" ) : hgvs = hgvs p [ 0 ] elif ( hgvs c [ 0 ] != "None" ) : hgvs = hgvs c [ 0 ] else : hgvs = "-" variant obj [ 'hgvs' ] = hgvs # populate variant predictions for display variant obj . update ( get predictions ( variant genes ) ) variants . append ( variant obj ) return { 'variants' : variants , 'more variants' : more variants , }
def multiqc ( store , institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) return dict ( institute = institute obj , case = case obj , )
def genes ( context , build , api key ) : LOG . info ( "Running scout update genes" ) adapter = context . obj [ 'adapter' ] # Fetch the omim information api key = api key or context . obj . get ( 'omim api key' ) if not api key : LOG . warning ( "Please provide a omim api key to load the omim gene panel" ) context . abort ( ) try : mim files = fetch mim files ( api key , mim2genes = True , morbidmap = True , genemap2 = True ) except Exception as err : LOG . warning ( err ) context . abort ( ) LOG . warning ( "Dropping all gene information" ) adapter . drop genes ( build ) LOG . info ( "Genes dropped" ) LOG . warning ( "Dropping all transcript information" ) adapter . drop transcripts ( build ) LOG . info ( "transcripts dropped" ) hpo genes = fetch hpo genes ( ) if build : builds = [ build ] else : builds = [ '37' , '38' ] hgnc lines = fetch hgnc ( ) exac lines = fetch exac constraint ( ) for build in builds : ensembl genes = fetch ensembl genes ( build = build ) # load the genes hgnc genes = load hgnc genes ( adapter = adapter , ensembl lines = ensembl genes , hgnc lines = hgnc lines , exac lines = exac lines , mim2gene lines = mim files [ 'mim2genes' ] , genemap lines = mim files [ 'genemap2' ] , hpo lines = hpo genes , build = build , ) ensembl genes = { } for gene obj in hgnc genes : ensembl id = gene obj [ 'ensembl id' ] ensembl genes [ ensembl id ] = gene obj # Fetch the transcripts from ensembl ensembl transcripts = fetch ensembl transcripts ( build = build ) transcripts = load transcripts ( adapter , ensembl transcripts , build , ensembl genes ) adapter . update indexes ( ) LOG . info ( "Genes, transcripts and Exons loaded" )
def parse cadd ( variant , transcripts ) : cadd = 0 cadd keys = [ 'CADD' , 'CADD PHRED' ] for key in cadd keys : cadd = variant . INFO . get ( key , 0 ) if cadd : return float ( cadd ) for transcript in transcripts : cadd entry = transcript . get ( 'cadd' ) if ( cadd entry and cadd entry > cadd ) : cadd = cadd entry return cadd
def convert ( context , panel ) : adapter = context . obj [ 'adapter' ] new header = [ "hgnc id" , "hgnc symbol" , "disease associated transcripts" , "reduced penetrance" , "genetic disease models" , "mosaicism" , "database entry version" ] genes = parse genes ( panel ) adapter . add hgnc id ( genes ) click . echo ( "#{0}" . format ( '\t' . join ( new header ) ) ) for gene in genes : if gene . get ( 'hgnc id' ) : print info = [ ] for head in new header : print info . append ( str ( gene [ head ] ) if gene . get ( head ) else '' ) click . echo ( '\t' . join ( print info ) )
def cli ( context , morbid , genemap , mim2gene , mim titles , phenotypes ) : # if not (morbid and genemap and mim2gene, mim titles): #     print("Please provide all files") #     context.abort() from scout . utils . handle import get file handle from pprint import pprint as pp print ( "Morbid file: %s" % morbid ) print ( "Genemap file: %s" % genemap ) print ( "mim2gene file: %s" % mim2gene ) print ( "Mim Titles file: %s" % mim titles ) if morbid : morbid handle = get file handle ( morbid ) if genemap : genemap handle = get file handle ( genemap ) if mim2gene : mim2gene handle = get file handle ( mim2gene ) if mim titles : mimtitles handle = get file handle ( mim titles ) mim genes = get mim genes ( genemap handle , mim2gene handle ) for entry in mim genes : if entry == 'C10orf11' : pp ( mim genes [ entry ] ) context . abort ( ) if phenotypes : if not genemap : click . echo ( "Please provide the genemap file" ) context . abort ( ) phenotypes = get mim phenotypes ( genemap handle ) for i , mim term in enumerate ( phenotypes ) : # pp(phenotypes[mim term]) pass print ( "Number of phenotypes found: %s" % i ) context . abort ( ) # hgnc genes = get mim genes(genemap handle, mim2gene handle) # for hgnc symbol in hgnc genes: #     pp(hgnc genes[hgnc symbol]) # phenotypes = get mim phenotypes(genemap handle, mim2gene handle, mimtitles handle) # for mim nr in phenotypes: #     pp(phenotypes[mim nr]) genes = get mim genes ( genemap handle , mim2gene handle ) for hgnc symbol in genes : if hgnc symbol == 'OPA1' : print ( genes [ hgnc symbol ] )
def case ( context , case id , case name , institute , collaborator , vcf , vcf sv , vcf cancer , vcf research , vcf sv research , vcf cancer research , peddy ped , reupload sv , rankscore treshold , rankmodel version ) : adapter = context . obj [ 'adapter' ] if not case id : if not ( case name and institute ) : LOG . info ( "Please specify which case to update." ) context . abort case id = "{0}-{1}" . format ( institute , case name ) # Check if the case exists case obj = adapter . case ( case id ) if not case obj : LOG . warning ( "Case %s could not be found" , case id ) context . abort ( ) case changed = False if collaborator : if not adapter . institute ( collaborator ) : LOG . warning ( "Institute %s could not be found" , collaborator ) context . abort ( ) if not collaborator in case obj [ 'collaborators' ] : case changed = True case obj [ 'collaborators' ] . append ( collaborator ) LOG . info ( "Adding collaborator %s" , collaborator ) if vcf : LOG . info ( "Updating 'vcf snv' to %s" , vcf ) case obj [ 'vcf files' ] [ 'vcf snv' ] = vcf case changed = True if vcf sv : LOG . info ( "Updating 'vcf sv' to %s" , vcf sv ) case obj [ 'vcf files' ] [ 'vcf sv' ] = vcf sv case changed = True if vcf cancer : LOG . info ( "Updating 'vcf cancer' to %s" , vcf cancer ) case obj [ 'vcf files' ] [ 'vcf cancer' ] = vcf cancer case changed = True if vcf research : LOG . info ( "Updating 'vcf research' to %s" , vcf research ) case obj [ 'vcf files' ] [ 'vcf research' ] = vcf research case changed = True if vcf sv research : LOG . info ( "Updating 'vcf sv research' to %s" , vcf sv research ) case obj [ 'vcf files' ] [ 'vcf sv research' ] = vcf sv research case changed = True if vcf cancer research : LOG . info ( "Updating 'vcf cancer research' to %s" , vcf cancer research ) case obj [ 'vcf files' ] [ 'vcf cancer research' ] = vcf cancer research case changed = True if case changed : adapter . update case ( case obj ) if reupload sv : LOG . info ( "Set needs check to True for case %s" , case id ) updates = { 'needs check' : True } if rankscore treshold : updates [ 'sv rank model version' ] = rankmodel version if vcf sv : updates [ 'vcf files.vcf sv' ] = vcf sv if vcf sv : updates [ 'vcf files.vcf sv research' ] = vcf sv research updated case = adapter . case collection . find one and update ( { ' id' : case id } , { '$set' : updates } , return document = pymongo . Return Document . AFTER ) rankscore treshold = rankscore treshold or updated case . get ( "rank score threshold" , 5 ) # Delete and reload the clinical SV variants if updated case [ 'vcf files' ] . get ( 'vcf sv' ) : adapter . delete variants ( case id , variant type = 'clinical' , category = 'sv' ) adapter . load variants ( updated case , variant type = 'clinical' , category = 'sv' , rank threshold = rankscore treshold ) # Delete and reload research SV variants if updated case [ 'vcf files' ] . get ( 'vcf sv research' ) : adapter . delete variants ( case id , variant type = 'research' , category = 'sv' ) if updated case . get ( 'is research' ) : adapter . load variants ( updated case , variant type = 'research' , category = 'sv' , rank threshold = rankscore treshold )
def formatmonth ( self , theyear , themonth , withyear = True , net = None , qs = None , template = 'happenings/partials/calendar/month table.html' ) : context = self . get context ( ) context [ 'month start date' ] = date ( self . yr , self . mo , 1 ) context [ 'week rows' ] = [ ] for week in self . monthdays2calendar ( theyear , themonth ) : week row = [ ] for day , weekday in week : week row . append ( self . formatday ( day , weekday ) ) context [ 'week rows' ] . append ( week row ) nxt , prev = get next and prev ( net ) extra qs = ( '&' + '&' . join ( qs ) ) if qs else '' context [ 'prev qs' ] = mark safe ( '?cal prev=%d%s' % ( prev , extra qs ) ) context [ 'next qs' ] = mark safe ( '?cal next=%d%s' % ( nxt , extra qs ) ) context [ 'withyear' ] = withyear return render to string ( template , context )
def formatday ( self , day , weekday , day template = 'happenings/partials/calendar/day cell.html' , noday template = 'happenings/partials/calendar/day noday cell.html' , popover template = 'happenings/partials/calendar/popover.html' , ) : super ( Event Calendar , self ) . formatday ( day , weekday ) now = get now ( ) context = self . get context ( ) context [ 'events' ] = [ ] context [ 'day' ] = day context [ 'day url' ] = self . get day url ( day ) context [ 'month start date' ] = date ( self . yr , self . mo , 1 ) context [ 'weekday' ] = weekday context [ 'cssclass' ] = self . cssclasses [ weekday ] context [ 'popover template' ] = popover template context [ 'num events' ] = len ( self . count . get ( day , [ ] ) ) , try : processed date = date ( self . yr , self . mo , day ) except Value Error : # day is out of range for month processed date = None context [ 'month start date' ] = date ( self . yr , self . mo , 1 ) if day == 0 : template = noday template else : template = day template if now . date ( ) == processed date : context [ 'is current day' ] = True if processed date and ( day in self . count ) : for item in self . count [ day ] : self . pk = item [ 1 ] self . title = item [ 0 ] for event in self . events : if event . pk == self . pk : event . check if cancelled ( processed date ) # allow to use event.last check if cancelled and populate event.title.extra context [ 'events' ] . append ( event ) return render to string ( template , context )
def formatday ( self , day , weekday ) : return super ( Mini Event Calendar , self ) . formatday ( day , weekday , day template = 'happenings/partials/calendar/mini day cell.html' , popover template = 'happenings/partials/calendar/mini popover.html' , )
def formatday ( self , day , weekday ) : self . wkday not today = '<td class="%s"><div class="td-inner">' % ( self . cssclasses [ weekday ] ) self . wkday today = ( '<td class="%s calendar-today"><div class="td-inner">' % ( self . cssclasses [ weekday ] ) ) if URLS NAMESPACE : url name = '%s:day list' % ( URLS NAMESPACE ) else : url name = 'day list' self . day url = reverse ( url name , args = ( self . yr , self . mo , day ) ) self . day = day self . anch = '<a href="%s">%d</a>' % ( self . day url , day ) self . end = '</div></td>'
def popover helper ( self ) : # when display month = month name [ self . mo ] if isinstance ( display month , six . binary type ) and self . encoding : display month = display month . decode ( 'utf-8' ) self . when = ( '<p><b>When:</b> ' + display month + ' ' + str ( self . day ) + ', ' + self . event . l start date . strftime ( LEGACY CALENDAR TIME FORMAT ) . lstrip ( '0' ) + ' - ' + self . event . l end date . strftime ( LEGACY CALENDAR TIME FORMAT ) . lstrip ( '0' ) + '</p>' ) if self . event . location . exists ( ) : # where self . where = '<p><b>Where:</b> ' for l in self . event . location . all ( ) : self . where += l . name self . where += '</p>' else : self . where = '' # description self . desc = '<p><b>Description:</b> ' + self . event . description [ : 100 ] self . desc += ( '...</p>' if len ( self . event . description ) > 100 else '</p>' ) self . event url = self . event . get absolute url ( ) # url t = LEGACY CALENDAR TIME FORMAT if self . event . l start date . minute else LEGACY CALENDAR HOUR FORMAT self . title2 = ( self . event . l start date . strftime ( t ) . lstrip ( '0' ) + ' ' + self . title )
def formatday ( self , day , weekday ) : super ( Event Calendar , self ) . formatday ( day , weekday ) now = get now ( ) self . day = day out = '' if day == 0 : return '<td class="noday">&nbsp;</td>' # day outside month elif now . month == self . mo and now . year == self . yr and day == now . day : if day in self . count : # don't return just yet out = self . wkday today + self . anch else : return self . wkday today + self . anch + self . end elif day in self . count : # don't return just yet out = self . wkday not today + self . anch else : return self . wkday not today + self . anch + self . end detail = "%s%s%s<br><a href='%s'>View details</a>" extras = ( '<div title="%s" data-content="%s" data-container="body"' ' data-toggle="popover" class="calendar-event"%s>' ) common = ' style=background:%s;color:%s;' # inject style and extras into calendar html for item in self . count [ day ] : self . pk = item [ 1 ] self . title = item [ 0 ] for event in self . events : if event . pk == self . pk : self . event = event self . check if cancelled ( ) # self.add occurrence self . popover helper ( ) bg , fnt = self . event . get colors ( ) out += ( '<a class="event-anch" href="' + self . event url + '">' + extras % ( self . title , detail % ( self . when , self . where , self . desc , self . event url ) , common % ( bg , fnt ) ) + self . title2 + '</div></a>' ) return out + self . end
def formatday ( self , day , weekday ) : super ( Mini Event Calendar , self ) . formatday ( day , weekday ) now = get now ( ) self . day = day if day == 0 : return '<td class="noday">&nbsp;</td>' # day outside month elif now . month == self . mo and now . year == self . yr and day == now . day : if day in self . count : self . popover helper ( ) return self . wkday today + self . anch + self . cal event + self . end else : return self . wkday today + self . anch + self . end elif day in self . count : self . popover helper ( ) return self . wkday not today + self . anch + self . cal event + self . end else : return self . wkday not today + self . anch + self . end
def diseases ( context ) : LOG . info ( "Running scout view diseases" ) adapter = context . obj [ 'adapter' ] disease objs = adapter . disease terms ( ) nr diseases = disease objs . count ( ) if nr diseases == 0 : click . echo ( "No diseases found" ) else : click . echo ( "Disease" ) for disease obj in adapter . disease terms ( ) : click . echo ( "{0}" . format ( disease obj [ ' id' ] ) ) LOG . info ( "{0} diseases found" . format ( nr diseases ) )
def hpo ( context ) : LOG . info ( "Running scout update hpo" ) adapter = context . obj [ 'adapter' ] LOG . info ( "Dropping HPO terms" ) adapter . hpo term collection . drop ( ) LOG . debug ( "HPO terms dropped" ) load hpo terms ( adapter )
def users ( store ) : user objs = list ( store . users ( ) ) total events = store . user events ( ) . count ( ) for user obj in user objs : if user obj . get ( 'institutes' ) : user obj [ 'institutes' ] = [ store . institute ( inst id ) for inst id in user obj . get ( 'institutes' ) ] else : user obj [ 'institutes' ] = [ ] user obj [ 'events' ] = store . user events ( user obj ) . count ( ) user obj [ 'events rank' ] = event rank ( user obj [ 'events' ] ) return dict ( users = sorted ( user objs , key = lambda user : - user [ 'events' ] ) , total events = total events , )
def render to json response ( self , context , * * kwargs ) : return Http Response ( self . convert context to json ( context ) , content type = 'application/json' , * * kwargs )
def check for cancelled events ( self , d ) : for event in self . events : for cn in event . cancellations . all ( ) : if cn . date == d : event . title += ' (CANCELLED)'
def cmd tool ( args = None ) : from argparse import Argument Parser parser = Argument Parser ( description = "Command line utility for reading and plotting filterbank files." ) parser . add argument ( '-p' , action = 'store' , default = 'ank' , dest = 'what to plot' , type = str , help = ) parser . add argument ( 'filename' , type = str , help = 'Name of file to read' ) parser . add argument ( '-b' , action = 'store' , default = None , dest = 'f start' , type = float , help = 'Start frequency (begin), in M Hz' ) parser . add argument ( '-e' , action = 'store' , default = None , dest = 'f stop' , type = float , help = 'Stop frequency (end), in M Hz' ) parser . add argument ( '-B' , action = 'store' , default = None , dest = 't start' , type = int , help = 'Start integration (begin) ID' ) parser . add argument ( '-E' , action = 'store' , default = None , dest = 't stop' , type = int , help = 'Stop integration (end) ID' ) parser . add argument ( '-i' , action = 'store true' , default = False , dest = 'info only' , help = 'Show info only' ) parser . add argument ( '-a' , action = 'store true' , default = False , dest = 'average' , help = 'average along time axis (plot spectrum only)' ) parser . add argument ( '-s' , action = 'store' , default = '' , dest = 'plt filename' , type = str , help = 'save plot graphic to file (give filename as argument)' ) parser . add argument ( '-S' , action = 'store true' , default = False , dest = 'save only' , help = 'Turn off plotting of data and only save to file.' ) parser . add argument ( '-D' , action = 'store false' , default = True , dest = 'blank dc' , help = 'Use to not blank DC bin.' ) parser . add argument ( '-c' , action = 'store true' , default = False , dest = 'calibrate band pass' , help = 'Calibrate band pass.' ) args = parser . parse args ( ) # Open blimpy data filename = args . filename load data = not args . info only # only load one integration if looking at spectrum wtp = args . what to plot if not wtp or 's' in wtp : if args . t start == None : t start = 0 else : t start = args . t start t stop = t start + 1 if args . average : t start = None t stop = None else : t start = args . t start t stop = args . t stop if args . info only : args . blank dc = False args . calibrate band pass = False fil = Filterbank ( filename , f start = args . f start , f stop = args . f stop , t start = t start , t stop = t stop , load data = load data , blank dc = args . blank dc , cal band pass = args . calibrate band pass ) fil . info ( ) # And if we want to plot data, then plot data. if not args . info only : # check start & stop frequencies make sense #try: #    if args.f start: #        print "Start freq: %2.2f" % args.f start #        assert args.f start >= fil.freqs[0] or np.isclose(args.f start, fil.freqs[0]) # #    if args.f stop: #        print "Stop freq: %2.2f" % args.f stop #        assert args.f stop <= fil.freqs[-1] or np.isclose(args.f stop, fil.freqs[-1]) #except Assertion Error: #    print "Error: Start and stop frequencies must lie inside file's frequency range." #    print "i.e. between %2.2f-%2.2f M Hz." % (fil.freqs[0], fil.freqs[-1]) #    exit() if args . what to plot == "w" : plt . figure ( "waterfall" , figsize = ( 8 , 6 ) ) fil . plot waterfall ( f start = args . f start , f stop = args . f stop ) elif args . what to plot == "s" : plt . figure ( "Spectrum" , figsize = ( 8 , 6 ) ) fil . plot spectrum ( logged = True , f start = args . f start , f stop = args . f stop , t = 'all' ) elif args . what to plot == "mm" : plt . figure ( "min max" , figsize = ( 8 , 6 ) ) fil . plot spectrum min max ( logged = True , f start = args . f start , f stop = args . f stop , t = 'all' ) elif args . what to plot == "k" : plt . figure ( "kurtosis" , figsize = ( 8 , 6 ) ) fil . plot kurtosis ( f start = args . f start , f stop = args . f stop ) elif args . what to plot == "t" : plt . figure ( "Time Series" , figsize = ( 8 , 6 ) ) fil . plot time series ( f start = args . f start , f stop = args . f stop , orientation = 'h' ) elif args . what to plot == "a" : plt . figure ( "Multiple diagnostic plots" , figsize = ( 12 , 9 ) , facecolor = 'white' ) fil . plot all ( logged = True , f start = args . f start , f stop = args . f stop , t = 'all' ) elif args . what to plot == "ank" : plt . figure ( "Multiple diagnostic plots" , figsize = ( 12 , 9 ) , facecolor = 'white' ) fil . plot all ( logged = True , f start = args . f start , f stop = args . f stop , t = 'all' , kurtosis = False ) if args . plt filename != '' : plt . savefig ( args . plt filename ) if not args . save only : if 'DISPLAY' in os . environ . keys ( ) : plt . show ( ) else : print ( "No $DISPLAY available." )
def setup time axis ( self , t start = None , t stop = None ) : # now check to see how many integrations requested ii start , ii stop = 0 , self . n ints in file if t start : ii start = t start if t stop : ii stop = t stop n ints = ii stop - ii start ## Setup time axis t0 = self . header [ b'tstart' ] t delt = self . header [ b'tsamp' ] self . timestamps = np . arange ( 0 , n ints ) * t delt / 24. / 60. / 60 + t0 return ii start , ii stop , n ints
def compute lst ( self ) : if self . header [ b'telescope id' ] == 6 : self . coords = gbt coords elif self . header [ b'telescope id' ] == 4 : self . coords = parkes coords else : raise Runtime Error ( "Currently only Parkes and GBT supported" ) if HAS SLALIB : # dut1 = (0.2 /3600.0) * np.pi/12.0 dut1 = 0.0 mjd = self . header [ b'tstart' ] tellong = np . deg2rad ( self . coords [ 1 ] ) last = s . sla gmst ( mjd ) - tellong + s . sla eqeqx ( mjd ) + dut1 # lmst = s.sla gmst(mjd) - tellong if last < 0.0 : last = last + 2.0 * np . pi return last else : raise Runtime Error ( "This method requires py SLALIB" )
def calc extent ( self , plot f = None , plot t = None , MJD time = False ) : plot f begin = plot f [ 0 ] plot f end = plot f [ - 1 ] + ( plot f [ 1 ] - plot f [ 0 ] ) plot t begin = self . timestamps [ 0 ] plot t end = self . timestamps [ - 1 ] + ( self . timestamps [ 1 ] - self . timestamps [ 0 ] ) if MJD time : extent = ( plot f begin , plot f begin end , plot t begin , plot t end ) else : extent = ( plot f begin , plot f end , 0.0 , ( plot t end - plot t begin ) * 24. * 60. * 60 ) return extent
def closest ( xarr , val ) : idx closest = np . argmin ( np . abs ( np . array ( xarr ) - val ) ) return idx closest
def get diff ( dio cross , feedtype , * * kwargs ) : #Get Stokes parameters, frequencies, and time sample length obs = Waterfall ( dio cross , max load = 150 ) freqs = obs . populate freqs ( ) tsamp = obs . header [ 'tsamp' ] data = obs . data obs = None I , Q , U , V = get stokes ( data , feedtype ) #Fold noise diode data I OFF , I ON = foldcal ( I , tsamp , * * kwargs ) Q OFF , Q ON = foldcal ( Q , tsamp , * * kwargs ) U OFF , U ON = foldcal ( U , tsamp , * * kwargs ) V OFF , V ON = foldcal ( V , tsamp , * * kwargs ) #Do ON-OFF subtraction Idiff = I ON - I OFF Qdiff = Q ON - Q OFF Udiff = U ON - U OFF Vdiff = V ON - V OFF return Idiff , Qdiff , Udiff , Vdiff , freqs
def cmd tool ( ) : parser = argparse . Argument Parser ( description = 'Dices hdf5 or fil files and writes to hdf5 or fil.' ) parser . add argument ( '-f' , '--input filename' , action = 'store' , default = None , dest = 'in fname' , type = str , help = 'Name of file to write from (HDF5 or FIL)' ) parser . add argument ( '-b' , action = 'store' , default = None , dest = 'f start' , type = float , help = 'Start frequency in M Hz' ) parser . add argument ( '-e' , action = 'store' , default = None , dest = 'f stop' , type = float , help = 'Stop frequency in M Hz' ) parser . add argument ( '-x' , '--output file' , action = 'store' , default = None , dest = 'out format' , type = str , help = 'Output file format [.h5 or .fil].' ) parser . add argument ( '-o' , '--output filename' , action = 'store' , default = None , dest = 'out fname' , type = str , help = 'Ouput file name to write (to HDF5 or FIL).' ) parser . add argument ( '-l' , action = 'store' , default = None , dest = 'max load' , type = float , help = 'Maximum data limit to load. Default:1GB' ) args = parser . parse args ( ) if len ( sys . argv ) == 1 : logger . error ( 'Indicate file name and start and stop frequencies' ) sys . exit ( ) if args . in fname == None : logger . error ( 'Need to indicate input file name' ) sys . exit ( ) if args . out fname == None : if ( args . out format == None ) or ( args . out format == 'h5' ) : if args . in fname [ len ( args . in fname ) - 4 : ] == '.fil' : args . out fname = args . in fname args . out fname = args . out fname . replace ( '.fil' , ' diced.h5' ) elif args . in fname [ len ( args . in fname ) - 3 : ] == '.h5' : args . out fname = args . in fname args . out fname = args . out fname . replace ( '.h5' , ' diced.h5' ) else : logger . error ( 'Input file not recognized' ) sys . exit ( ) elif args . out format == 'fil' : if args . in fname [ len ( args . in fname ) - 4 : ] == '.fil' : args . out fname = args . in fname args . out fname = args . out fname . replace ( '.fil' , ' diced.fil' ) elif args . in fname [ len ( args . in fname ) - 3 : ] == '.h5' : args . out fname = args . in fname args . out fname = args . out fname . replace ( '.h5' , ' diced.fil' ) else : logger . error ( 'input file not recognized.' ) sys . exit ( ) else : logger . error ( 'Must indicate either output file name or valid output file extension.' ) sys . exit ( ) elif ( args . out fname [ len ( args . out fname ) - 4 : ] == '.fil' ) and ( args . out format == 'h5' ) : logger . error ( 'Output file extension does not match output file name' ) sys . exit ( ) elif ( args . out fname [ len ( args . out fname ) - 3 : ] == '.h5' ) and ( args . out format == 'fil' ) : logger . error ( 'Output file extension does not match output file name.' ) sys . exit ( ) if ( args . out fname [ len ( args . out fname ) - 3 : ] != '.h5' ) and ( args . out fname [ len ( args . out fname ) - 4 : ] != '.fil' ) : logger . error ( 'Indicate output file name with extension, or simply output file extension.' ) sys . exit ( ) if args . f start == None and args . f stop == None : logger . error ( 'Please give either start and/or end frequencies. Otherwise use fil2h5 or h52fil functions.' ) sys . exit ( ) if args . f start == None : logger . warning ( 'Lower frequency not given, setting to ' + str ( f min file ) + ' M Hz to match file.' ) if args . f stop == None : logger . warning ( 'Higher frequency not given, setting to ' + str ( f max file ) + ' M Hz to match file.' ) file big = Waterfall ( args . in fname , max load = args . max load ) f min file = file big . header [ 'fch1' ] f max file = file big . header [ 'fch1' ] + file big . header [ 'nchans' ] * file big . header [ 'foff' ] if f max file < f min file : f max file , f min file = f min file , f max file Freq BW File = f max file - f min file std DF = Freq BW File / float ( file big . calc n coarse chan ( ) ) if args . f stop < args . f start : args . f stop , args . f start = args . f start , args . f stop if args . f start < f max file and args . f start > f min file and args . f stop > f max file : args . f stop = f max file logger . warning ( 'Higher frequency set to ' + str ( f max file ) + ' M Hz to match file.' ) if args . f stop < f max file and args . f stop > f min file and args . f start < f min file : args . f start = f min file logger . warning ( 'Lower frequency set to ' + str ( f min file ) + ' M Hz to match file.' ) if args . f start < f min file and args . f stop > f max file : args . f start = f min file args . f stop = f max file logger . warning ( 'Lower frequency set to ' + str ( f min file ) + ' M Hz and higher frequency set to ' + str ( f max file ) + ' M Hz to match file.' ) if min ( args . f start , args . f stop ) < f min file or max ( args . f start , args . f stop ) > f max file : logger . error ( 'Bandwidth to extract must be within ' + str ( f min file ) + ' M Hz and ' + str ( f max file ) + ' M Hz.' ) sys . exit ( ) f start real = math . floor ( ( min ( args . f start , args . f stop ) - f min file ) / std DF ) * std DF + f min file f stop real = f max file - math . floor ( ( f max file - max ( args . f start , args . f stop ) ) / std DF ) * std DF logger . info ( 'Writing to ' + args . out fname ) logger . info ( 'Extacting from ' + str ( f start real ) + ' M Hz to ' + str ( f stop real ) + ' M Hz.' ) file small = Waterfall ( args . in fname , f start = f start real , f stop = f stop real , max load = args . max load ) if args . out fname [ len ( args . out fname ) - 4 : ] == '.fil' : file small . write to fil ( args . out fname ) elif args . out fname [ len ( args . out fname ) - 3 : ] == '.h5' : file small . write to hdf5 ( args . out fname ) else : logger . error ( 'Error in output file creation : verify output file name and extension.' ) sys . exit ( )
def cmd tool ( args = None ) : from argparse import Argument Parser parser = Argument Parser ( description = "Command line utility for creating HDF5 Filterbank files." ) parser . add argument ( 'dirname' , type = str , help = 'Name of directory to read' ) args = parser . parse args ( ) if not HAS BITSHUFFLE : print ( "Error: the bitshuffle library is required to run this script." ) exit ( ) filelist = glob . glob ( os . path . join ( args . dirname , '*.fil' ) ) for filename in filelist : if not os . path . exists ( filename + '.h5' ) : t0 = time . time ( ) print ( "\n Reading %s header..." % filename ) fb = Filterbank ( filename , load data = False ) data shape = ( fb . n ints in file , fb . header [ 'nifs' ] , fb . header [ 'nchans' ] ) data dtype = fb . data . dtype print ( data dtype ) print ( "Creating new dataset, %s" % str ( data shape ) ) block size = 0 h5 = h5py . File ( filename + '.h5' , 'w' ) h5 . attrs [ 'CLASS' ] = 'FILTERBANK' dset = h5 . create dataset ( 'data' , shape = data shape , compression = bitshuffle . h5 . H5FILTER , compression opts = ( block size , bitshuffle . h5 . H5 COMPRESS LZ4 ) , dtype = data dtype ) dset mask = h5 . create dataset ( 'mask' , shape = data shape , compression = bitshuffle . h5 . H5FILTER , compression opts = ( block size , bitshuffle . h5 . H5 COMPRESS LZ4 ) , dtype = 'uint8' ) dset . dims [ 0 ] . label = "frequency" dset . dims [ 1 ] . label = "feed id" dset . dims [ 2 ] . label = "time" dset mask . dims [ 0 ] . label = "frequency" dset mask . dims [ 1 ] . label = "feed id" dset mask . dims [ 2 ] . label = "time" # Copy over header information as attributes for key , value in fb . header . items ( ) : dset . attrs [ key ] = value filesize = os . path . getsize ( filename ) if filesize >= MAX SIZE : n int per read = int ( filesize / MAX SIZE / 2 ) print ( "Filling in with data over %i reads..." % n int per read ) for ii in range ( 0 , n int per read ) : print ( "Reading %i of %i" % ( ii + 1 , n int per read ) ) #print  ii*n int per read, (ii+1)*n int per read fb = Filterbank ( filename , t start = ii * n int per read , t stop = ( ii + 1 ) * n int per read ) dset [ ii * n int per read : ( ii + 1 ) * n int per read ] = fb . data [ : ] else : fb = Filterbank ( filename ) print ( dset . shape , " -> " , fb . data . shape ) dset [ : ] = fb . data [ : ] h5 . close ( ) t1 = time . time ( ) print ( "Conversion time: %2.2fs" % ( t1 - t0 ) )
def calc selection size ( self ) : #Check to see how many integrations requested n ints = self . t stop - self . t start #Check to see how many frequency channels requested n chan = ( self . f stop - self . f start ) / abs ( self . header [ b'foff' ] ) n bytes = self . n bytes selection size = int ( n ints * n chan * n bytes ) return selection size
def calc selection shape ( self ) : #Check how many integrations requested n ints = int ( self . t stop - self . t start ) #Check how many frequency channels requested n chan = int ( np . round ( ( self . f stop - self . f start ) / abs ( self . header [ b'foff' ] ) ) ) selection shape = ( n ints , int ( self . header [ b'nifs' ] ) , n chan ) return selection shape
def setup freqs ( self ) : if self . header [ b'foff' ] > 0 : self . f start = self . f begin + self . chan start idx * abs ( self . header [ b'foff' ] ) self . f stop = self . f begin + self . chan stop idx * abs ( self . header [ b'foff' ] ) else : self . f start = self . f end - self . chan stop idx * abs ( self . header [ b'foff' ] ) self . f stop = self . f end - self . chan start idx * abs ( self . header [ b'foff' ] )
def calc n blobs ( self , blob dim ) : n blobs = int ( np . ceil ( 1.0 * np . prod ( self . selection shape ) / np . prod ( blob dim ) ) ) return n blobs
def isheavy ( self ) : selection size bytes = self . calc selection size ( ) if selection size bytes > self . MAX DATA ARRAY SIZE : return True else : return False
def find blob start ( self , blob dim , n blob ) : #Convert input frequencies into what their corresponding channel number would be. self . setup chans ( ) #Check which is the blob time offset blob time start = self . t start + blob dim [ self . time axis ] * n blob #Check which is the blob frequency offset (in channels) blob freq start = self . chan start idx + ( blob dim [ self . freq axis ] * n blob ) % self . selection shape [ self . freq axis ] blob start = np . array ( [ blob time start , 0 , blob freq start ] ) return blob start
def read blob ( self , blob dim , n blob = 0 ) : n blobs = self . calc n blobs ( blob dim ) if n blob > n blobs or n blob < 0 : raise Value Error ( 'Please provide correct n blob value. Given %i, but max values is %i' % ( n blob , n blobs ) ) #This prevents issues when the last blob is smaller than the others in time if blob dim [ self . time axis ] * ( n blob + 1 ) > self . selection shape [ self . time axis ] : updated blob dim = ( self . selection shape [ self . time axis ] - blob dim [ self . time axis ] * n blob , 1 , blob dim [ self . freq axis ] ) else : updated blob dim = [ int ( i ) for i in blob dim ] blob start = self . find blob start ( blob dim , n blob ) blob end = blob start + np . array ( updated blob dim ) blob = self . h5 [ "data" ] [ int ( blob start [ self . time axis ] ) : int ( blob end [ self . time axis ] ) , : , int ( blob start [ self . freq axis ] ) : int ( blob end [ self . freq axis ] ) ] #         if self.header[b'foff'] < 0: #             blob = blob[:,:,::-1] return blob
def find blob start ( self ) : # Convert input frequencies into what their corresponding channel number would be. self . setup chans ( ) # Check which is the blob time offset blob time start = self . t start # Check which is the blob frequency offset (in channels) blob freq start = self . chan start idx blob start = blob time start * self . n channels in file + blob freq start return blob start
def read blob ( self , blob dim , n blob = 0 ) : n blobs = self . calc n blobs ( blob dim ) if n blob > n blobs or n blob < 0 : raise Value Error ( 'Please provide correct n blob value. Given %i, but max values is %i' % ( n blob , n blobs ) ) # This prevents issues when the last blob is smaller than the others in time. if blob dim [ self . time axis ] * ( n blob + 1 ) > self . selection shape [ self . time axis ] : updated blob dim = ( int ( self . selection shape [ self . time axis ] - blob dim [ self . time axis ] * n blob ) , 1 , int ( blob dim [ self . freq axis ] ) ) else : updated blob dim = [ int ( i ) for i in blob dim ] blob start = self . find blob start ( ) blob = np . zeros ( updated blob dim , dtype = self . d type ) # EE: For now; also assuming one polarization and one beam. # Assuming the blob will loop over the whole frequency range. if self . f start == self . f begin and self . f stop == self . f end : blob flat size = np . prod ( blob dim ) updated blob flat size = np . prod ( updated blob dim ) # Load binary data with open ( self . filename , 'rb' ) as f : f . seek ( int ( self . idx data + self . n bytes * ( blob start + n blob * blob flat size ) ) ) dd = np . fromfile ( f , count = updated blob flat size , dtype = self . d type ) if dd . shape [ 0 ] == updated blob flat size : blob = dd . reshape ( updated blob dim ) else : logger . info ( 'DD shape != blob shape.' ) blob = dd . reshape ( ( int ( dd . shape [ 0 ] / blob dim [ self . freq axis ] ) , blob dim [ self . beam axis ] , blob dim [ self . freq axis ] ) ) else : for blobt in range ( updated blob dim [ self . time axis ] ) : #Load binary data with open ( self . filename , 'rb' ) as f : f . seek ( int ( self . idx data + self . n bytes * ( blob start + n blob * blob dim [ self . time axis ] * self . n channels in file + blobt * self . n channels in file ) ) ) dd = np . fromfile ( f , count = blob dim [ self . freq axis ] , dtype = self . d type ) blob [ blobt ] = dd #         if self.header[b'foff'] < 0: #             blob = blob[:,:,::-1] return blob
def cmd tool ( args = None ) : from argparse import Argument Parser parser = Argument Parser ( description = "Command line utility for reading and plotting blimpy files." ) parser . add argument ( 'filename' , type = str , help = 'Name of file to read' ) parser . add argument ( '-p' , action = 'store' , default = 'a' , dest = 'what to plot' , type = str , help = ) parser . add argument ( '-b' , action = 'store' , default = None , dest = 'f start' , type = float , help = 'Start frequency (begin), in M Hz' ) parser . add argument ( '-e' , action = 'store' , default = None , dest = 'f stop' , type = float , help = 'Stop frequency (end), in M Hz' ) parser . add argument ( '-B' , action = 'store' , default = None , dest = 't start' , type = int , help = 'Start integration (begin, inclusive) ID ' ) parser . add argument ( '-E' , action = 'store' , default = None , dest = 't stop' , type = int , help = 'Stop integration (end, exclusive) ID' ) parser . add argument ( '-i' , action = 'store true' , default = False , dest = 'info only' , help = 'Show info only' ) parser . add argument ( '-a' , action = 'store true' , default = False , dest = 'average' , help = 'average along time axis (plot spectrum only)' ) parser . add argument ( '-s' , action = 'store' , default = '' , dest = 'plt filename' , type = str , help = 'save plot graphic to file (give filename as argument)' ) parser . add argument ( '-S' , action = 'store true' , default = False , dest = 'save only' , help = 'Turn off plotting of data and only save to file.' ) parser . add argument ( '-D' , action = 'store false' , default = True , dest = 'blank dc' , help = 'Use to not blank DC bin.' ) parser . add argument ( '-H' , action = 'store true' , default = False , dest = 'to hdf5' , help = 'Write file to hdf5 format.' ) parser . add argument ( '-F' , action = 'store true' , default = False , dest = 'to fil' , help = 'Write file to .fil format.' ) parser . add argument ( '-o' , action = 'store' , default = None , dest = 'filename out' , type = str , help = 'Filename output (if not probided, the name will be the same but with apropiate extension).' ) parser . add argument ( '-l' , action = 'store' , default = None , dest = 'max load' , type = float , help = 'Maximum data limit to load. Default:1GB' ) if args is None : args = sys . argv [ 1 : ] parse args = parser . parse args ( args ) # Open blimpy data filename = parse args . filename load data = not parse args . info only info only = parse args . info only filename out = parse args . filename out fil = Waterfall ( filename , f start = parse args . f start , f stop = parse args . f stop , t start = parse args . t start , t stop = parse args . t stop , load data = load data , max load = parse args . max load ) fil . info ( ) #Check the size of selection. if fil . container . isheavy ( ) or parse args . to hdf5 or parse args . to fil : info only = True # And if we want to plot data, then plot data. if not info only : print ( '' ) if parse args . blank dc : logger . info ( "Blanking DC bin" ) n coarse chan = fil . calc n coarse chan ( ) fil . blank dc ( n coarse chan ) if parse args . what to plot == "w" : plt . figure ( "waterfall" , figsize = ( 8 , 6 ) ) fil . plot waterfall ( f start = parse args . f start , f stop = parse args . f stop ) elif parse args . what to plot == "s" : plt . figure ( "Spectrum" , figsize = ( 8 , 6 ) ) fil . plot spectrum ( logged = True , f start = parse args . f start , f stop = parse args . f stop , t = 'all' ) elif parse args . what to plot == "mm" : plt . figure ( "min max" , figsize = ( 8 , 6 ) ) fil . plot spectrum min max ( logged = True , f start = parse args . f start , f stop = parse args . f stop , t = 'all' ) elif parse args . what to plot == "k" : plt . figure ( "kurtosis" , figsize = ( 8 , 6 ) ) fil . plot kurtosis ( f start = parse args . f start , f stop = parse args . f stop ) elif parse args . what to plot == "t" : plt . figure ( "Time Series" , figsize = ( 8 , 6 ) ) fil . plot time series ( f start = parse args . f start , f stop = parse args . f stop , orientation = 'h' ) elif parse args . what to plot == "a" : plt . figure ( "Multiple diagnostic plots" , figsize = ( 12 , 9 ) , facecolor = 'white' ) fil . plot all ( logged = True , f start = parse args . f start , f stop = parse args . f stop , t = 'all' ) elif parse args . what to plot == "ank" : plt . figure ( "Multiple diagnostic plots" , figsize = ( 12 , 9 ) , facecolor = 'white' ) fil . plot all ( logged = True , f start = parse args . f start , f stop = parse args . f stop , t = 'all' , kutosis = False ) if parse args . plt filename != '' : plt . savefig ( parse args . plt filename ) if not parse args . save only : if 'DISPLAY' in os . environ . keys ( ) : plt . show ( ) else : logger . warning ( "No $DISPLAY available." ) else : if parse args . to hdf5 and parse args . to fil : raise Warning ( 'Either provide to hdf5 or to fil, but not both.' ) if parse args . to hdf5 : if not filename out : filename out = filename . replace ( '.fil' , '.h5' ) elif '.h5' not in filename out : filename out = filename out . replace ( '.fil' , '' ) + '.h5' logger . info ( 'Writing file : %s' % ( filename out ) ) fil . write to hdf5 ( filename out ) logger . info ( 'File written.' ) if parse args . to fil : if not filename out : filename out = filename . replace ( '.h5' , '.fil' ) elif '.fil' not in filename out : filename out = filename out . replace ( '.h5' , '' ) + '.fil' logger . info ( 'Writing file : %s' % ( filename out ) ) fil . write to fil ( filename out ) logger . info ( 'File written.' )
def read data ( self , f start = None , f stop = None , t start = None , t stop = None ) : self . container . read data ( f start = f start , f stop = f stop , t start = t start , t stop = t stop ) self . load data ( )
def update header ( self ) : #Updating frequency of first channel from selection if self . header [ b'foff' ] < 0 : self . header [ b'fch1' ] = self . container . f stop else : self . header [ b'fch1' ] = self . container . f start #Updating number of coarse channels. self . header [ b'nchans' ] = self . container . selection shape [ self . freq axis ] #Updating time stamp for first time bin from selection self . header [ b'tstart' ] = self . container . populate timestamps ( update header = True )
def info ( self ) : print ( "\n--- File Info ---" ) for key , val in self . file header . items ( ) : if key == 'src raj' : val = val . to string ( unit = u . hour , sep = ':' ) if key == 'src dej' : val = val . to string ( unit = u . deg , sep = ':' ) print ( "%16s : %32s" % ( key , val ) ) print ( "\n%16s : %32s" % ( "Num ints in file" , self . n ints in file ) ) print ( "%16s : %32s" % ( "File shape" , self . file shape ) ) print ( "--- Selection Info ---" ) print ( "%16s : %32s" % ( "Data selection shape" , self . selection shape ) ) print ( "%16s : %32s" % ( "Minimum freq (M Hz)" , self . container . f start ) ) print ( "%16s : %32s" % ( "Maximum freq (M Hz)" , self . container . f stop ) )
def get chunk dimensions ( self ) : #Usually '.0000.' is in self.filename if np . abs ( self . header [ b'foff' ] ) < 1e-5 : logger . info ( 'Detecting high frequency resolution data.' ) chunk dim = ( 1 , 1 , 1048576 ) #1048576 is the number of channels in a coarse channel. return chunk dim #Usually '.0001.' is in self.filename elif np . abs ( self . header [ b'tsamp' ] ) < 1e-3 : logger . info ( 'Detecting high time resolution data.' ) chunk dim = ( 2048 , 1 , 512 ) #512 is the total number of channels per single band (ie. blc00) return chunk dim #Usually '.0002.' is in self.filename elif np . abs ( self . header [ b'foff' ] ) < 1e-2 and np . abs ( self . header [ b'foff' ] ) >= 1e-5 : logger . info ( 'Detecting intermediate frequency and time resolution data.' ) chunk dim = ( 10 , 1 , 65536 ) #65536 is the total number of channels per single band (ie. blc00) #            chunk dim = (1,1,65536/4) return chunk dim else : logger . warning ( 'File format not known. Will use minimum chunking. NOT OPTIMAL.' ) chunk dim = ( 1 , 1 , 512 ) return chunk dim
def cmd tool ( args = None ) : from argparse import Argument Parser parser = Argument Parser ( description = "Command line utility for creating spectra from Guppi Raw files." ) parser . add argument ( 'filename' , type = str , help = 'Name of file to read' ) parser . add argument ( '-o' , dest = 'outdir' , type = str , default = './' , help = 'output directory for PNG files' ) args = parser . parse args ( ) r = Guppi Raw ( args . filename ) r . print stats ( ) bname = os . path . splitext ( os . path . basename ( args . filename ) ) [ 0 ] bname = os . path . join ( args . outdir , bname ) r . plot histogram ( filename = "%s hist.png" % bname ) r . plot spectrum ( filename = "%s spec.png" % bname )
def print stats ( self ) : header , data = self . read next data block ( ) data = data . view ( 'float32' ) print ( "AVG: %2.3f" % data . mean ( ) ) print ( "STD: %2.3f" % data . std ( ) ) print ( "MAX: %2.3f" % data . max ( ) ) print ( "MIN: %2.3f" % data . min ( ) ) import pylab as plt
def plot histogram ( self , filename = None ) : header , data = self . read next data block ( ) data = data . view ( 'float32' ) plt . figure ( "Histogram" ) plt . hist ( data . flatten ( ) , 65 , facecolor = '#cc0000' ) if filename : plt . savefig ( filename ) plt . show ( )
def generate filterbank header ( self , nchans = 1 , ) : gp head = self . read first header ( ) fb head = { } telescope str = gp head . get ( "TELESCOP" , "unknown" ) if telescope str in ( 'GBT' , 'GREENBANK' ) : fb head [ "telescope id" ] = 6 elif telescope str in ( 'PKS' , 'PARKES' ) : fb head [ "telescop id" ] = 7 else : fb head [ "telescop id" ] = 0 # Using .get() method allows us to fill in default values if not present fb head [ "source name" ] = gp head . get ( "SRC NAME" , "unknown" ) fb head [ "az start" ] = gp head . get ( "AZ" , 0 ) fb head [ "za start" ] = gp head . get ( "ZA" , 0 ) fb head [ "src raj" ] = Angle ( str ( gp head . get ( "RA" , 0.0 ) ) + "hr" ) fb head [ "src dej" ] = Angle ( str ( gp head . get ( "DEC" , 0.0 ) ) + "deg" ) fb head [ "rawdatafile" ] = self . filename # hardcoded fb head [ "machine id" ] = 20 fb head [ "data type" ] = 1 # blio datatype fb head [ "barycentric" ] = 0 fb head [ "pulsarcentric" ] = 0 fb head [ "nbits" ] = 32 # TODO - compute these values. Need to figure out the correct calcs fb head [ "tstart" ] = 0.0 fb head [ "tsamp" ] = 1.0 fb head [ "fch1" ] = 0.0 fb head [ "foff" ] = 187.5 / nchans # Need to be updated based on output specs fb head [ "nchans" ] = nchans fb head [ "nifs" ] = 1 fb head [ "nbeams" ] = 1 return fb head
def find header size ( filename ) : # open datafile filfile = open ( filename , 'rb' ) # go to the start of the file filfile . seek ( 0 ) #read some region larger than the header. round1 = filfile . read ( 1000 ) headersize = round1 . find ( 'HEADER END' ) + len ( 'HEADER END' ) return headersize
def cmd tool ( args = None ) : if 'bl' in local host : header loc = '/usr/local/sigproc/bin/header' #Current location of header command in GBT. else : raise IO Error ( 'Script only able to run in BL systems.' ) p = Option Parser ( ) p . set usage ( 'matchfils <FIL FILE1> <FIL FILE2>' ) opts , args = p . parse args ( sys . argv [ 1 : ] ) file1 = args [ 0 ] file2 = args [ 1 ] #------------------------------------ #Create batch script make batch script ( ) #------------------------------------ #First checksum headersize1 = find header size ( file1 ) file size1 = os . path . getsize ( file1 ) #Strip header from file, and calculate the md5sum of the rest. #command=['tail','-c',str(file size1-headersize1),file1,'|','md5sum'] command = [ './tail sum.sh' , file1 , str ( file size1 - headersize1 ) ] print ( '[matchfils] ' + ' ' . join ( command ) ) proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) ( out , err ) = proc . communicate ( ) check sum1 = out . split ( ) [ 0 ] print ( '[matchfils] Checksum is:' , check sum1 ) if err : raise Error ( 'There is an error.' ) #--- out , err = reset outs ( ) command = [ header loc , file1 ] print ( '[matchfils] Header information:' ) proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) ( out , err ) = proc . communicate ( ) header1 = out print ( header1 ) #------------------------------------ #Second checksum out , err = reset outs ( ) headersize2 = find header size ( file2 ) file size2 = os . path . getsize ( file2 ) #Strip header from file, and calculate the md5sum of the rest. command = [ './tail sum.sh' , file2 , str ( file size2 - headersize2 ) ] print ( '[matchfils] ' + ' ' . join ( command ) ) proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) ( out , err ) = proc . communicate ( ) check sum2 = out . split ( ) [ 0 ] print ( '[matchfils] Checksum is:' , check sum2 ) if err : raise Error ( 'There is an error.' ) #--- out , err = reset outs ( ) command = [ header loc , file2 ] print ( '[matchfils] Header information:' ) proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) ( out , err ) = proc . communicate ( ) header2 = out print ( header2 ) #------------------------------------ #check the checksums if check sum1 != check sum2 : print ( '[matchfils] Booo! Checksum does not match between files.' ) else : print ( '[matchfils] Hooray! Checksum matches between files.' ) #------------------------------------ #Remove batch script os . remove ( 'tail sum.sh' )
def cmd tool ( args = None ) : from argparse import Argument Parser if not HAS BITSHUFFLE : print ( "Error: the bitshuffle library is required to run this script." ) exit ( ) parser = Argument Parser ( description = "Command line utility for creating HDF5 Raw files." ) parser . add argument ( 'filename' , type = str , help = 'Name of filename to read' ) args = parser . parse args ( ) fileroot = args . filename . split ( '.0000.raw' ) [ 0 ] filelist = glob . glob ( fileroot + '*.raw' ) filelist = sorted ( filelist ) # Read first file r = Guppi Raw ( filelist [ 0 ] ) header , data = r . read next data block ( ) dshape = data . shape #r.read next data block shape() print ( dshape ) n blocks total = 0 for filename in filelist : print ( filename ) r = Guppi Raw ( filename ) n blocks total += r . n blocks print ( n blocks total ) full dshape = np . concatenate ( ( ( n blocks total , ) , dshape ) ) # Create h5py file h5 = h5py . File ( fileroot + '.h5' , 'w' ) h5 . attrs [ 'CLASS' ] = 'GUPPIRAW' block size = 0 # This is chunk block size dset = h5 . create dataset ( 'data' , shape = full dshape , #compression=bitshuffle.h5.H5FILTER, #compression opts=(block size, bitshuffle.h5.H5 COMPRESS LZ4), dtype = data . dtype ) h5 idx = 0 for filename in filelist : print ( "\n Reading %s header..." % filename ) r = Guppi Raw ( filename ) h5 = h5py . File ( filename + '.h5' , 'w' ) header , data = r . read next data block ( ) for ii in range ( 0 , r . n blocks ) : t0 = time . time ( ) print ( "Reading block %i of %i" % ( h5 idx + 1 , full dshape [ 0 ] ) ) header , data = r . read next data block ( ) t1 = time . time ( ) t2 = time . time ( ) print ( "Writing block %i of %i" % ( h5 idx + 1 , full dshape [ 0 ] ) ) dset [ h5 idx , : ] = data t3 = time . time ( ) print ( "Read: %2.2fs, Write %2.2fs" % ( ( t1 - t0 ) , ( t3 - t2 ) ) ) h5 idx += 1 # Copy over header information as attributes for key , value in header . items ( ) : dset . attrs [ key ] = value h5 . close ( ) t1 = time . time ( ) print ( "Conversion time: %2.2fs" % ( t1 - t0 ) )
def is filterbank ( filename ) : with open ( filename , 'rb' ) as fh : is fil = True # Check this is a blimpy file try : keyword , value , idx = read next header keyword ( fh ) try : assert keyword == b'HEADER START' except Assertion Error : is fil = False except Key Error : is fil = False return is fil
def to sigproc angle ( angle val ) : x = str ( angle val ) if '.' in x : if 'h' in x : d , m , s , ss = int ( x [ 0 : x . index ( 'h' ) ] ) , int ( x [ x . index ( 'h' ) + 1 : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + 1 : x . index ( '.' ) ] ) , float ( x [ x . index ( '.' ) : x . index ( 's' ) ] ) if 'd' in x : d , m , s , ss = int ( x [ 0 : x . index ( 'd' ) ] ) , int ( x [ x . index ( 'd' ) + 1 : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + 1 : x . index ( '.' ) ] ) , float ( x [ x . index ( '.' ) : x . index ( 's' ) ] ) else : if 'h' in x : d , m , s = int ( x [ 0 : x . index ( 'h' ) ] ) , int ( x [ x . index ( 'h' ) + 1 : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + 1 : x . index ( 's' ) ] ) if 'd' in x : d , m , s = int ( x [ 0 : x . index ( 'd' ) ] ) , int ( x [ x . index ( 'd' ) + 1 : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + 1 : x . index ( 's' ) ] ) ss = 0 num = str ( d ) . zfill ( 2 ) + str ( m ) . zfill ( 2 ) + str ( s ) . zfill ( 2 ) + '.' + str ( ss ) . split ( "." ) [ - 1 ] return np . float64 ( num ) . tostring ( )
def calc n ints in file ( filename ) : # Load binary data h = read header ( filename ) n bytes = int ( h [ b'nbits' ] / 8 ) n chans = h [ b'nchans' ] n ifs = h [ b'nifs' ] idx data = len header ( filename ) f = open ( filename , 'rb' ) f . seek ( idx data ) filesize = os . path . getsize ( filename ) n bytes data = filesize - idx data if h [ b'nbits' ] == 2 : n ints = int ( 4 * n bytes data / ( n chans * n ifs ) ) else : n ints = int ( n bytes data / ( n bytes * n chans * n ifs ) ) return n ints
def to dict ( self ) : if self . tb next is None : tb next = None else : tb next = self . tb next . to dict ( ) code = { 'co filename' : self . tb frame . f code . co filename , 'co name' : self . tb frame . f code . co name , } frame = { 'f globals' : self . tb frame . f globals , 'f code' : code , } return { 'tb frame' : frame , 'tb lineno' : self . tb lineno , 'tb next' : tb next , }
def make rr subparser ( subparsers , rec type , args and types ) : sp = subparsers . add parser ( rec type ) sp . add argument ( "name" , type = str ) sp . add argument ( "ttl" , type = int , nargs = '?' ) sp . add argument ( rec type , type = str ) for my spec in args and types : ( argname , argtype ) = my spec [ : 2 ] if len ( my spec ) > 2 : nargs = my spec [ 2 ] sp . add argument ( argname , type = argtype , nargs = nargs ) else : sp . add argument ( argname , type = argtype ) return sp
def make parser ( ) : line parser = Zonefile Line Parser ( ) subparsers = line parser . add subparsers ( ) # parse $ORIGIN sp = subparsers . add parser ( "$ORIGIN" ) sp . add argument ( "$ORIGIN" , type = str ) # parse $TTL sp = subparsers . add parser ( "$TTL" ) sp . add argument ( "$TTL" , type = int ) # parse each RR args and types = [ ( "mname" , str ) , ( "rname" , str ) , ( "serial" , int ) , ( "refresh" , int ) , ( "retry" , int ) , ( "expire" , int ) , ( "minimum" , int ) ] make rr subparser ( subparsers , "SOA" , args and types ) make rr subparser ( subparsers , "NS" , [ ( "host" , str ) ] ) make rr subparser ( subparsers , "A" , [ ( "ip" , str ) ] ) make rr subparser ( subparsers , "AAAA" , [ ( "ip" , str ) ] ) make rr subparser ( subparsers , "CNAME" , [ ( "alias" , str ) ] ) make rr subparser ( subparsers , "ALIAS" , [ ( "host" , str ) ] ) make rr subparser ( subparsers , "MX" , [ ( "preference" , str ) , ( "host" , str ) ] ) make txt subparser ( subparsers ) make rr subparser ( subparsers , "PTR" , [ ( "host" , str ) ] ) make rr subparser ( subparsers , "SRV" , [ ( "priority" , int ) , ( "weight" , int ) , ( "port" , int ) , ( "target" , str ) ] ) make rr subparser ( subparsers , "SPF" , [ ( "data" , str ) ] ) make rr subparser ( subparsers , "URI" , [ ( "priority" , int ) , ( "weight" , int ) , ( "target" , str ) ] ) return line parser
def remove comments ( text ) : ret = [ ] lines = text . split ( "\n" ) for line in lines : if len ( line ) == 0 : continue line = serialize ( tokenize line ( line ) ) ret . append ( line ) return "\n" . join ( ret )
def parse zone file ( text , ignore invalid = False ) : text = remove comments ( text ) text = flatten ( text ) text = remove class ( text ) text = add default name ( text ) json zone file = parse lines ( text , ignore invalid = ignore invalid ) return json zone file
def process origin ( data , template ) : record = "" if data is not None : record += "$ORIGIN %s" % data return template . replace ( "{$origin}" , record )
def process ttl ( data , template ) : record = "" if data is not None : record += "$TTL %s" % data return template . replace ( "{$ttl}" , record )
def process soa ( data , template ) : record = template [ : ] if data is not None : assert len ( data ) == 1 , "Only support one SOA RR at this time" data = data [ 0 ] soadat = [ ] domain fields = [ 'mname' , 'rname' ] param fields = [ 'serial' , 'refresh' , 'retry' , 'expire' , 'minimum' ] for f in domain fields + param fields : assert f in data . keys ( ) , "Missing '%s' (%s)" % ( f , data ) data name = str ( data . get ( 'name' , '@' ) ) soadat . append ( data name ) if data . get ( 'ttl' ) is not None : soadat . append ( str ( data [ 'ttl' ] ) ) soadat . append ( "IN" ) soadat . append ( "SOA" ) for key in domain fields : value = str ( data [ key ] ) soadat . append ( value ) soadat . append ( "(" ) for key in param fields : value = str ( data [ key ] ) soadat . append ( value ) soadat . append ( ")" ) soa txt = " " . join ( soadat ) record = record . replace ( "{soa}" , soa txt ) else : # clear all SOA fields  record = record . replace ( "{soa}" , "" ) return record
def process txt ( data , template ) : if data is None : to process = None else : # quote txt to process = copy . deepcopy ( data ) for datum in to process : if isinstance ( datum [ "txt" ] , list ) : datum [ "txt" ] = " " . join ( [ '"%s"' % entry . replace ( ";" , "\;" ) for entry in datum [ "txt" ] ] ) else : datum [ "txt" ] = '"%s"' % datum [ "txt" ] . replace ( ";" , "\;" ) return process rr ( to process , "TXT" , "txt" , "{txt}" , template )
def parse schema string ( schema string ) : if isinstance ( schema string , str ) : schema string = schema string . decode ( "utf8" ) schema struct = json . loads ( schema string ) return Avro Schema Parser ( ) . parse schema struct ( schema struct )
def to json compatible ( record ) : d = { } for fname , f in record . fields . iteritems ( ) : val = getattr ( record , fname ) if val is not None : d [ fname ] = f . dump ( val ) return d
def from json compatible ( schema , dct ) : kwargs = { } for key in dct : field type = schema . fields . get ( key ) if field type is None : raise Parse Error ( "Unexpected field encountered in line for record %s: %s" % ( schema . name , key ) ) kwargs [ key ] = field type . load ( dct [ key ] ) return schema ( * * kwargs )
def from json compatible ( schema , dct ) : kwargs = { } for key in dct : field type = schema . fields . get ( key ) if field type is None : warnings . warn ( "Unexpected field encountered in line for record %s: %r" % ( schema . name , key ) ) continue kwargs [ key ] = field type . avro load ( dct [ key ] ) return schema ( * * kwargs )
def all include attributes ( self , attributes ) : self . reload ( expand = True , attributes = attributes ) entities = [ Entity ( self , r , attributes = attributes ) for r in self . resources ] self . reload ( ) return entities
def give another quote ( q ) : for qc in QUOTES : if qc != q : return qc else : raise Value Error ( u'Could not find a different quote for {}' . format ( q ) )
def parse Command Line Arguments ( ) : parser = argparse . Argument Parser ( description = "Plot predicted Gaia sky averaged proper motion errors as a function of V" ) parser . add argument ( "-p" , action = "store true" , dest = "pdf Output" , help = "Make PDF plot" ) parser . add argument ( "-b" , action = "store true" , dest = "png Output" , help = "Make PNG plot" ) parser . add argument ( "-g" , action = "store true" , dest = "gmag Abscissa" , help = "Plot performance vs G instead of V" ) args = vars ( parser . parse args ( ) ) return args
def parse Command Line Arguments ( ) : parser = argparse . Argument Parser ( description = "Calculate parallax error for given G and (V-I)" ) parser . add argument ( "gmag" , help = "G-band magnitude of source" , type = float ) parser . add argument ( "vmini" , help = "(V-I) colour of source" , type = float ) args = vars ( parser . parse args ( ) ) return args
def uniquote ( value ) : if isinstance ( value , six . binary type ) : try : value = value . decode ( 'utf-8' ) except Unicode Decode Error : # Not utf-8. Show the repr value = six . text type ( dequote ( repr ( value ) ) ) # trim quotes result = six . text type ( value ) if isinstance ( value , six . text type ) : result = "'%s'" % result return result
def serach path ( ) : operating system = get os ( ) # 1st choice: in ~/.kerncraft/iaca-{} # 2nd choice: in package directory / iaca-{} return [ os . path . expanduser ( "~/.kerncraft/iaca/{}/" . format ( operating system ) ) , os . path . abspath ( os . path . dirname ( os . path . realpath ( file ) ) ) + '/iaca/{}/' . format ( operating system ) ]
def build minimal runs ( events ) : # Eliminate multiples events = [ e for i , e in enumerate ( events ) if events . index ( e ) == i ] # Build list of runs per register group scheduled runs = { } scheduled events = [ ] cur run = 0 while len ( scheduled events ) != len ( events ) : for event tpl in events : event , registers , parameters = event tpl # Skip allready scheduled events if event tpl in scheduled events : continue # Compile explicit list of possible register locations for possible reg in register options ( registers ) : # Schedule in current run, if register is not yet in use s = scheduled runs . setdefault ( cur run , { } ) if possible reg not in s : s [ possible reg ] = ( event , possible reg , parameters ) # ban from further scheduling attempts scheduled events . append ( event tpl ) break cur run += 1 # Collaps all register dicts to single runs runs = [ list ( v . values ( ) ) for v in scheduled runs . values ( ) ] return runs
def calculate cache access ( self ) : self . results = { 'misses' : self . predictor . get misses ( ) , 'hits' : self . predictor . get hits ( ) , 'evicts' : self . predictor . get evicts ( ) , 'verbose infos' : self . predictor . get infos ( ) , # only for verbose outputs 'bottleneck level' : 0 , 'mem bottlenecks' : [ ] } element size = self . kernel . datatypes size [ self . kernel . datatype ] cacheline size = float ( self . machine [ 'cacheline size' ] ) elements per cacheline = int ( cacheline size // element size ) total flops = sum ( self . kernel . flops . values ( ) ) * elements per cacheline # TODO let user choose threads per core: threads per core = 1 # Compile relevant information # CPU-L1 stats (in bytes!) # We compile CPU-L1 stats on our own, because cacheprediction only works on cache lines read offsets , write offsets = zip ( * list ( self . kernel . compile global offsets ( iteration = range ( 0 , elements per cacheline ) ) ) ) read offsets = set ( [ item for sublist in read offsets if sublist is not None for item in sublist ] ) write offsets = set ( [ item for sublist in write offsets if sublist is not None for item in sublist ] ) write streams = len ( write offsets ) read streams = len ( read offsets ) + write streams # write-allocate total loads = read streams * element size # total evicts = write streams * element size bw , measurement kernel = self . machine . get bandwidth ( 0 , read streams - write streams , # no write-allocate in L1 write streams , threads per core , cores = self . cores ) # Calculate performance (arithmetic intensity * bandwidth with # arithmetic intensity = flops / bytes loaded ) if total loads == 0 : # This happens in case of full-caching arith intens = None performance = None else : arith intens = float ( total flops ) / total loads performance = Prefixed Unit ( arith intens * float ( bw ) , 'FLOP/s' ) self . results [ 'mem bottlenecks' ] . append ( { 'performance' : self . conv perf ( Prefixed Unit ( performance , 'FLOP/s' ) ) , 'level' : self . machine [ 'memory hierarchy' ] [ 0 ] [ 'level' ] , 'arithmetic intensity' : arith intens , 'bw kernel' : measurement kernel , 'bandwidth' : bw , 'bytes transfered' : total loads } ) self . results [ 'bottleneck level' ] = len ( self . results [ 'mem bottlenecks' ] ) - 1 self . results [ 'min performance' ] = self . conv perf ( performance ) # for other cache and memory levels: for cache level , cache info in list ( enumerate ( self . machine [ 'memory hierarchy' ] ) ) [ : - 1 ] : # Compiling stats (in bytes!) total misses = self . results [ 'misses' ] [ cache level ] * cacheline size total evicts = self . results [ 'evicts' ] [ cache level ] * cacheline size # choose bw according to cache level and problem # first, compile stream counts at current cache level # write-allocate is allready resolved above read streams = self . results [ 'misses' ] [ cache level ] write streams = self . results [ 'evicts' ] [ cache level ] # second, try to find best fitting kernel (closest to stream seen stream counts): bw , measurement kernel = self . machine . get bandwidth ( cache level + 1 , read streams , write streams , threads per core , cores = self . cores ) # Calculate performance (arithmetic intensity * bandwidth with # arithmetic intensity = flops / bytes transfered) bytes transfered = total misses + total evicts if bytes transfered == 0 : # This happens in case of full-caching arith intens = float ( 'inf' ) performance = Prefixed Unit ( float ( 'inf' ) , 'FLOP/s' ) else : arith intens = float ( total flops ) / bytes transfered performance = Prefixed Unit ( arith intens * float ( bw ) , 'FLOP/s' ) self . results [ 'mem bottlenecks' ] . append ( { 'performance' : self . conv perf ( performance ) , 'level' : ( self . machine [ 'memory hierarchy' ] [ cache level + 1 ] [ 'level' ] ) , 'arithmetic intensity' : arith intens , 'bw kernel' : measurement kernel , 'bandwidth' : bw , 'bytes transfered' : bytes transfered } ) if performance < self . results . get ( 'min performance' , { 'FLOP/s' : performance } ) [ 'FLOP/s' ] : self . results [ 'bottleneck level' ] = len ( self . results [ 'mem bottlenecks' ] ) - 1 self . results [ 'min performance' ] = self . conv perf ( performance ) return self . results
def report ( self , output file = sys . stdout ) : max perf = self . results [ 'max perf' ] if self . args and self . args . verbose >= 3 : print ( '{}' . format ( pformat ( self . results ) ) , file = output file ) if self . args and self . args . verbose >= 1 : print ( '{}' . format ( pformat ( self . results [ 'verbose infos' ] ) ) , file = output file ) print ( 'Bottlenecks:' , file = output file ) print ( '  level | a. intensity |   performance   |   peak bandwidth  | peak bandwidth kernel' , file = output file ) print ( '--------+--------------+-----------------+-------------------+----------------------' , file = output file ) print ( '    CPU |              | {!s:>15} |                   |' . format ( max perf [ self . args . unit ] ) , file = output file ) for b in self . results [ 'mem bottlenecks' ] : print ( '{level:>7} | {arithmetic intensity:>5.2} FLOP/B | {0!s:>15} |' ' {bandwidth!s:>17} | {bw kernel:<8}' . format ( b [ 'performance' ] [ self . args . unit ] , * * b ) , file = output file ) print ( '' , file = output file ) if self . results [ 'min performance' ] [ 'FLOP/s' ] > max perf [ 'FLOP/s' ] : # CPU bound print ( 'CPU bound. {!s} due to CPU max. FLOP/s' . format ( max perf ) , file = output file ) else : # Cache or mem bound print ( 'Cache or mem bound.' , file = output file ) bottleneck = self . results [ 'mem bottlenecks' ] [ self . results [ 'bottleneck level' ] ] print ( '{!s} due to {} transfer bottleneck (with bw from {} benchmark)' . format ( bottleneck [ 'performance' ] [ self . args . unit ] , bottleneck [ 'level' ] , bottleneck [ 'bw kernel' ] ) , file = output file ) print ( 'Arithmetic Intensity: {:.2f} FLOP/B' . format ( bottleneck [ 'arithmetic intensity' ] ) , file = output file )
def analyze ( self ) : self . results = self . calculate cache access ( ) try : iaca analysis , asm block = self . kernel . iaca analysis ( micro architecture = self . machine [ 'micro-architecture' ] , asm block = self . asm block , pointer increment = self . pointer increment , verbose = self . verbose > 2 ) except Runtime Error as e : print ( "IACA analysis failed: " + str ( e ) ) sys . exit ( 1 ) block throughput = iaca analysis [ 'throughput' ] uops = iaca analysis [ 'uops' ] iaca output = iaca analysis [ 'output' ] port cycles = iaca analysis [ 'port cycles' ] # Normalize to cycles per cacheline elements per block = abs ( asm block [ 'pointer increment' ] / self . kernel . datatypes size [ self . kernel . datatype ] ) block size = elements per block * self . kernel . datatypes size [ self . kernel . datatype ] try : block to cl ratio = float ( self . machine [ 'cacheline size' ] ) / block size except Zero Division Error as e : print ( "Too small block size / pointer increment:" , e , file = sys . stderr ) sys . exit ( 1 ) port cycles = dict ( [ ( i [ 0 ] , i [ 1 ] * block to cl ratio ) for i in list ( port cycles . items ( ) ) ] ) uops = uops * block to cl ratio cl throughput = block throughput * block to cl ratio flops per element = sum ( self . kernel . flops . values ( ) ) # Overwrite CPU-L1 stats, because they are covered by IACA self . results [ 'mem bottlenecks' ] [ 0 ] = None # Reevaluate mem bottleneck self . results [ 'min performance' ] = self . conv perf ( Prefixed Unit ( float ( 'inf' ) , 'FLOP/s' ) ) self . results [ 'bottleneck level' ] = None for level , bottleneck in enumerate ( self . results [ 'mem bottlenecks' ] ) : if level == 0 : # ignoring CPU-L1 continue if bottleneck [ 'performance' ] [ 'FLOP/s' ] < self . results [ 'min performance' ] [ 'FLOP/s' ] : self . results [ 'bottleneck level' ] = level self . results [ 'min performance' ] = bottleneck [ 'performance' ] # Create result dictionary self . results . update ( { 'cpu bottleneck' : { 'port cycles' : port cycles , 'cl throughput' : cl throughput , 'uops' : uops , 'performance throughput' : self . conv perf ( Prefixed Unit ( self . machine [ 'clock' ] / block throughput * elements per block * flops per element * self . cores , "FLOP/s" ) ) , 'IACA output' : iaca output } } )
def report ( self , output file = sys . stdout ) : cpu perf = self . results [ 'cpu bottleneck' ] [ 'performance throughput' ] if self . verbose >= 3 : print ( '{}' . format ( pformat ( self . results ) ) , file = output file ) if self . verbose >= 1 : print ( 'Bottlenecks:' , file = output file ) print ( '  level | a. intensity |   performance   |   peak bandwidth  | peak bandwidth kernel' , file = output file ) print ( '--------+--------------+-----------------+-------------------+----------------------' , file = output file ) print ( '    CPU |              | {!s:>15} |                   |' . format ( cpu perf [ self . args . unit ] ) , file = output file ) for b in self . results [ 'mem bottlenecks' ] : # Skip CPU-L1 from Roofline model if b is None : continue print ( '{level:>7} | {arithmetic intensity:>5.2} FLOP/B | {0!s:>15} |' ' {bandwidth!s:>17} | {bw kernel:<8}' . format ( b [ 'performance' ] [ self . args . unit ] , * * b ) , file = output file ) print ( '' , file = output file ) print ( 'IACA analisys:' , file = output file ) print ( '{!s}' . format ( { k : v for k , v in list ( self . results [ 'cpu bottleneck' ] . items ( ) ) if k not in [ 'IACA output' ] } ) , file = output file ) if self . results [ 'min performance' ] [ 'FLOP/s' ] > cpu perf [ 'FLOP/s' ] : # CPU bound print ( 'CPU bound. {!s} due to CPU bottleneck' . format ( cpu perf [ self . args . unit ] ) , file = output file ) else : # Cache or mem bound print ( 'Cache or mem bound.' , file = output file ) bottleneck = self . results [ 'mem bottlenecks' ] [ self . results [ 'bottleneck level' ] ] print ( '{!s} due to {} transfer bottleneck (with bw from {} benchmark)' . format ( bottleneck [ 'performance' ] [ self . args . unit ] , bottleneck [ 'level' ] , bottleneck [ 'bw kernel' ] ) , file = output file ) print ( 'Arithmetic Intensity: {:.2f} FLOP/B' . format ( bottleneck [ 'arithmetic intensity' ] ) , file = output file )
def calculate cache access ( self ) : # FIXME handle multiple datatypes element size = self . kernel . datatypes size [ self . kernel . datatype ] results = { 'dimensions' : { } } def sympy compare ( a , b ) : c = 0 for i in range ( min ( len ( a ) , len ( b ) ) ) : s = a [ i ] - b [ i ] if sympy . simplify ( s > 0 ) : c = - 1 elif sympy . simplify ( s == 0 ) : c = 0 else : c = 1 if c != 0 : break return c accesses = defaultdict ( list ) sympy accesses = defaultdict ( list ) for var name in self . kernel . variables : for r in self . kernel . sources . get ( var name , [ ] ) : if r is None : continue accesses [ var name ] . append ( r ) sympy accesses [ var name ] . append ( self . kernel . access to sympy ( var name , r ) ) for w in self . kernel . destinations . get ( var name , [ ] ) : if w is None : continue accesses [ var name ] . append ( w ) sympy accesses [ var name ] . append ( self . kernel . access to sympy ( var name , w ) ) # order accesses by increasing order accesses [ var name ] . sort ( key = cmp to key ( sympy compare ) , reverse = True ) results [ 'accesses' ] = accesses results [ 'sympy accesses' ] = sympy accesses # For each dimension (1D, 2D, 3D ... n D) for dimension in range ( 1 , len ( list ( self . kernel . get loop stack ( ) ) ) + 1 ) : results [ 'dimensions' ] [ dimension ] = { } slices = defaultdict ( list ) slices accesses = defaultdict ( list ) for var name in accesses : for a in accesses [ var name ] : # slices are identified by the tuple of indices of higher dimensions slice id = tuple ( [ var name , tuple ( a [ : - dimension ] ) ] ) slices [ slice id ] . append ( a ) slices accesses [ slice id ] . append ( self . kernel . access to sympy ( var name , a ) ) results [ 'dimensions' ] [ dimension ] [ 'slices' ] = slices results [ 'dimensions' ] [ dimension ] [ 'slices accesses' ] = slices accesses slices distances = defaultdict ( list ) for k , v in slices accesses . items ( ) : for i in range ( 1 , len ( v ) ) : slices distances [ k ] . append ( ( v [ i ] - v [ i - 1 ] ) . simplify ( ) ) results [ 'dimensions' ] [ dimension ] [ 'slices distances' ] = slices distances # Check that distances contain only free symbols based on constants for dist in chain ( * slices distances . values ( ) ) : if any ( [ s not in self . kernel . constants . keys ( ) for s in dist . free symbols ] ) : raise Value Error ( "Some distances are not based on constants: " + str ( dist ) ) # Sum of lengths between relative distances slices sum = sum ( [ sum ( dists ) for dists in slices distances . values ( ) ] ) results [ 'dimensions' ] [ dimension ] [ 'slices sum' ] = slices sum # Max of lengths between relative distances # Work-around, the arguments with the most symbols get to stay # FIXME, may not be correct in all cases. e.g., N+M vs. N*M def Fucked Up Max ( * args ) : if len ( args ) == 1 : return args [ 0 ] # expand all expressions: args = [ a . expand ( ) for a in args ] # Filter expressions with less than the maximum number of symbols max symbols = max ( [ len ( a . free symbols ) for a in args ] ) args = list ( filter ( lambda a : len ( a . free symbols ) == max symbols , args ) ) if max symbols == 0 : return sympy . Max ( * args ) # Filter symbols with lower exponent max coeffs = 0 for a in args : for s in a . free symbols : max coeffs = max ( max coeffs , len ( sympy . Poly ( a , s ) . all coeffs ( ) ) ) def coeff filter ( a ) : return max ( 0 , 0 , * [ len ( sympy . Poly ( a , s ) . all coeffs ( ) ) for s in a . free symbols ] ) == max coeffs args = list ( filter ( coeff filter , args ) ) m = sympy . Max ( * args ) # if m.is Function: #     raise Value Error("Could not resolve {} to maximum.".format(m)) return m slices max = Fucked Up Max ( sympy . Integer ( 0 ) , * [ Fucked Up Max ( * dists ) for dists in slices distances . values ( ) ] ) results [ 'dimensions' ] [ dimension ] [ 'slices max' ] = slices max # Nmber of slices slices count = len ( slices accesses ) results [ 'dimensions' ] [ dimension ] [ 'slices count' ] = slices count # Cache requirement expression cache requirement bytes = ( slices sum + slices max * slices count ) * element size results [ 'dimensions' ] [ dimension ] [ 'cache requirement bytes' ] = cache requirement bytes # Apply to all cache sizes csim = self . machine . get cachesim ( self . args . cores ) results [ 'dimensions' ] [ dimension ] [ 'caches' ] = { } for cl in csim . levels ( with mem = False ) : cache equation = sympy . Eq ( cache requirement bytes , cl . size ( ) ) if len ( self . kernel . constants . keys ( ) ) <= 1 : inequality = sympy . solve ( sympy . Less Than ( cache requirement bytes , cl . size ( ) ) , * self . kernel . constants . keys ( ) ) else : # Sympy does not solve for multiple constants inequality = sympy . Less Than ( cache requirement bytes , cl . size ( ) ) try : eq = sympy . solve ( inequality , * self . kernel . constants . keys ( ) , dict = True ) except Not Implemented Error : eq = None results [ 'dimensions' ] [ dimension ] [ 'caches' ] [ cl . name ] = { 'cache size' : cl . size ( ) , 'equation' : cache equation , 'lt' : inequality , 'eq' : eq } return results
def analyze ( self ) : # check that layer conditions can be applied on this kernel: # 1. All iterations may only have a step width of 1 loop stack = list ( self . kernel . get loop stack ( ) ) if any ( [ l [ 'increment' ] != 1 for l in loop stack ] ) : raise Value Error ( "Can not apply layer condition, since not all loops are of step " "length 1." ) # 2. The order of iterations must be reflected in the order of indices in all array #    references containing the inner loop index. If the inner loop index is not part of the #    reference, the reference is simply ignored # TODO support flattend array indexes for aref in list ( self . kernel . index order ( ) ) : # Strip left most empty sets (refereces without index) while aref and len ( aref [ 0 ] ) == 0 : aref . pop ( 0 ) for i , idx names in enumerate ( aref ) : # 1. Check for that there are enough loops to handle access dimensions # 2. Check that offset index matches loop index (in same order) if i >= len ( loop stack ) or any ( [ loop stack [ i ] [ 'index' ] != idx . name for idx in idx names ] ) : raise Value Error ( "Can not apply layer condition, order of indices in array " "does not follow order of loop indices. Single-dimension is " "currently not supported." ) # 3. Indices may only increase with one # TODO use a public interface, not self.kernel. * for arefs in chain ( chain ( * self . kernel . sources . values ( ) ) , chain ( * self . kernel . destinations . values ( ) ) ) : if not arefs : continue # Strip left most constant offsets (refereces without index) to support things like: # a[0][i+1][j][k-1] with an i, j and k loop-nest while arefs and not arefs [ 0 ] . free symbols : arefs = arefs [ 1 : ] # Check that remaining indices are in orde for i , expr in enumerate ( arefs ) : diff = sympy . diff ( expr , sympy . Symbol ( loop stack [ i ] [ 'index' ] ) ) if diff != 0 and diff != 1 : # TODO support -1 aswell raise Value Error ( "Can not apply layer condition, array references may not " "increment more then one per iteration." ) self . results = self . calculate cache access ( )
def report ( self , output file = sys . stdout ) : if self . args and self . args . verbose > 2 : pprint ( self . results ) for dimension , lc info in self . results [ 'dimensions' ] . items ( ) : print ( "{}D layer condition:" . format ( dimension ) , file = output file ) for cache , lc solution in sorted ( lc info [ 'caches' ] . items ( ) ) : print ( cache + ": " , end = '' , file = output file ) if lc solution [ 'lt' ] is sympy . true : print ( "unconditionally fulfilled" , file = output file ) else : if lc solution [ 'eq' ] is None : print ( "{}" . format ( lc solution [ 'lt' ] ) , file = output file ) elif type ( lc solution [ 'eq' ] ) is not list : print ( "{}" . format ( lc solution [ 'eq' ] ) , file = output file ) else : for solu in lc solution [ 'eq' ] : for s , v in solu . items ( ) : print ( "{} <= {}" . format ( s , v ) , file = output file )
def round to next ( x , base ) : # Based on: http://stackoverflow.com/a/2272174 return int ( base * math . ceil ( float ( x ) / base ) )
def calculate cache access ( self ) : self . results . update ( { 'cycles' : [ ] , # will be filled by caclculate cycles() 'misses' : self . predictor . get misses ( ) , 'hits' : self . predictor . get hits ( ) , 'evicts' : self . predictor . get evicts ( ) , 'verbose infos' : self . predictor . get infos ( ) } )
def analyze ( self ) : self . calculate cache access ( ) self . calculate cycles ( ) self . results [ 'flops per iteration' ] = sum ( self . kernel . flops . values ( ) ) return self . results
def report ( self , output file = sys . stdout ) : if self . verbose > 1 : print ( '{}' . format ( pprint . pformat ( self . results [ 'verbose infos' ] ) ) , file = output file ) for level , cycles in self . results [ 'cycles' ] : print ( '{} = {}' . format ( level , self . conv cy ( cycles ) [ self . args . unit ] ) , file = output file ) if self . verbose > 1 : if 'memory bandwidth kernel' in self . results : print ( 'memory cycles based on {} kernel with {}' . format ( self . results [ 'memory bandwidth kernel' ] , self . results [ 'memory bandwidth' ] ) , file = output file ) if self . verbose > 1 : print ( file = output file ) print ( self . report data transfers ( ) , file = output file )
def analyze ( self ) : try : incore analysis , asm block = self . kernel . iaca analysis ( micro architecture = self . machine [ 'micro-architecture' ] , asm block = self . asm block , pointer increment = self . pointer increment , verbose = self . verbose > 2 ) except Runtime Error as e : print ( "IACA analysis failed: " + str ( e ) ) sys . exit ( 1 ) block throughput = incore analysis [ 'throughput' ] port cycles = incore analysis [ 'port cycles' ] uops = incore analysis [ 'uops' ] # Normalize to cycles per cacheline elements per block = abs ( asm block [ 'pointer increment' ] // self . kernel . datatypes size [ self . kernel . datatype ] ) block size = elements per block * self . kernel . datatypes size [ self . kernel . datatype ] try : block to cl ratio = float ( self . machine [ 'cacheline size' ] ) / block size except Zero Division Error as e : print ( "Too small block size / pointer increment:" , e , file = sys . stderr ) sys . exit ( 1 ) port cycles = dict ( [ ( i [ 0 ] , i [ 1 ] * block to cl ratio ) for i in list ( port cycles . items ( ) ) ] ) uops = uops * block to cl ratio cl throughput = block throughput * block to cl ratio # Compile most relevant information T OL = max ( [ v for k , v in list ( port cycles . items ( ) ) if k in self . machine [ 'overlapping model' ] [ 'ports' ] ] ) T n OL = max ( [ v for k , v in list ( port cycles . items ( ) ) if k in self . machine [ 'non-overlapping model' ] [ 'ports' ] ] ) # Use IACA throughput prediction if it is slower then T n OL if T n OL < cl throughput : T OL = cl throughput # Create result dictionary self . results = { 'port cycles' : port cycles , 'cl throughput' : self . conv cy ( cl throughput ) , 'uops' : uops , 'T n OL' : T n OL , 'T OL' : T OL , 'IACA output' : incore analysis [ 'output' ] , 'elements per block' : elements per block , 'pointer increment' : asm block [ 'pointer increment' ] , 'flops per iteration' : sum ( self . kernel . flops . values ( ) ) } return self . results
def report ( self , output file = sys . stdout ) : if self . verbose > 2 : print ( "IACA Output:" , file = output file ) print ( self . results [ 'IACA output' ] , file = output file ) print ( '' , file = output file ) if self . verbose > 1 : print ( 'Detected pointer increment: {}' . format ( self . results [ 'pointer increment' ] ) , file = output file ) print ( 'Derived elements stored to per asm block iteration: {}' . format ( self . results [ 'elements per block' ] ) , file = output file ) print ( 'Ports and cycles:' , str ( self . results [ 'port cycles' ] ) , file = output file ) print ( 'Uops:' , str ( self . results [ 'uops' ] ) , file = output file ) print ( 'Throughput: {}' . format ( self . results [ 'cl throughput' ] [ self . args . unit ] ) , file = output file ) print ( 'T n OL = {:.1f} cy/CL' . format ( self . results [ 'T n OL' ] ) , file = output file ) print ( 'T OL = {:.1f} cy/CL' . format ( self . results [ 'T OL' ] ) , file = output file )
def analyze ( self ) : self . CPU . analyze ( ) self . data . analyze ( ) self . results = copy . deepcopy ( self . CPU . results ) self . results . update ( copy . deepcopy ( self . data . results ) ) cores per numa domain = self . machine [ 'cores per NUMA domain' ] # Compile total single-core prediction self . results [ 'total cycles' ] = self . CPU . conv cy ( max ( self . results [ 'T OL' ] , sum ( [ self . results [ 'T n OL' ] ] + [ i [ 1 ] for i in self . results [ 'cycles' ] ] ) ) ) T ECM = float ( self . results [ 'total cycles' ] [ 'cy/CL' ] ) # T MEM is the cycles accounted to memory transfers T MEM = self . results [ 'cycles' ] [ - 1 ] [ 1 ] # Simple scaling prediction: # Assumptions are: #  - bottleneck is always LLC-MEM #  - all caches scale with number of cores (bw AND size(WRONG!)) # Full caching in higher cache level self . results [ 'scaling cores' ] = float ( 'inf' ) # Not full caching: if self . results [ 'cycles' ] [ - 1 ] [ 1 ] != 0.0 : # Considering memory bus utilization utilization = [ 0 ] self . results [ 'scaling cores' ] = float ( 'inf' ) for c in range ( 1 , cores per numa domain + 1 ) : if c * T MEM > ( T ECM + utilization [ c - 1 ] * ( c - 1 ) * T MEM / 2 ) : utilization . append ( 1.0 ) self . results [ 'scaling cores' ] = min ( self . results [ 'scaling cores' ] , c ) else : utilization . append ( c * T MEM / ( T ECM + utilization [ c - 1 ] * ( c - 1 ) * T MEM / 2 ) ) utilization = utilization [ 1 : ] # Old scaling code #self.results['scaling cores'] = ( #    max(self.results['T OL'], #        self.results['T n OL'] + sum([c[1] for c in self.results['cycles']])) / #    self.results['cycles'][-1][1]) scaling predictions = [ ] for cores in range ( 1 , self . machine [ 'cores per socket' ] + 1 ) : scaling = { 'cores' : cores , 'notes' : [ ] , 'performance' : None , 'in-NUMA performance' : None } # Detailed scaling: if cores <= self . results [ 'scaling cores' ] : # Is it purely in-cache? innuma rectp = Prefixed Unit ( max ( sum ( [ c [ 1 ] for c in self . results [ 'cycles' ] ] ) + self . results [ 'T n OL' ] , self . results [ 'T OL' ] ) / ( T ECM / T MEM ) , "cy/CL" ) scaling [ 'notes' ] . append ( "memory-interface not saturated" ) else : innuma rectp = Prefixed Unit ( self . results [ 'cycles' ] [ - 1 ] [ 1 ] , 'cy/CL' ) scaling [ 'notes' ] . append ( "memory-interface saturated on first NUMA domain" ) # Include NUMA-local performance in results dict scaling [ 'in-NUMA performance' ] = innuma rectp if 0 < cores <= cores per numa domain : # only in-numa scaling to consider scaling [ 'performance' ] = self . CPU . conv cy ( innuma rectp / utilization [ cores - 1 ] ) scaling [ 'notes' ] . append ( "in-NUMA-domain scaling" ) elif cores <= self . machine [ 'cores per socket' ] * self . machine [ 'sockets' ] : # out-of-numa scaling behavior scaling [ 'performance' ] = self . CPU . conv cy ( innuma rectp * cores per numa domain / cores ) scaling [ 'notes' ] . append ( "out-of-NUMA-domain scaling" ) else : raise Value Error ( "Number of cores must be greater than zero and upto the max. " "number of cores defined by cores per socket and sockets in" "machine file." ) scaling predictions . append ( scaling ) else : # pure in-cache performace (perfect scaling) scaling predictions = [ { 'cores' : cores , 'notes' : [ 'pure in-cache' ] , 'performance' : self . CPU . conv cy ( T ECM / cores ) , 'in-NUMA performance' : self . CPU . conv cy ( T ECM / cores per numa domain ) } for cores in range ( 1 , self . machine [ 'cores per socket' ] + 1 ) ] # Also include prediction for all in-NUMA core counts in results self . results [ 'scaling prediction' ] = scaling predictions if self . args . cores : self . results [ 'multi-core' ] = scaling predictions [ self . args . cores - 1 ] else : self . results [ 'multi-core' ] = None
def report ( self , output file = sys . stdout ) : report = '' if self . verbose > 1 : self . CPU . report ( ) self . data . report ( ) report += '{{ {:.1f} || {:.1f} | {} }} cy/CL' . format ( self . results [ 'T OL' ] , self . results [ 'T n OL' ] , ' | ' . join ( [ '{:.1f}' . format ( i [ 1 ] ) for i in self . results [ 'cycles' ] ] ) ) if self . args . cores > 1 : report += " (single core)" report += ' = {}' . format ( self . results [ 'total cycles' ] [ self . args . unit ] ) report += '\n{{ {:.1f} \ {} }} cy/CL' . format ( max ( self . results [ 'T OL' ] , self . results [ 'T n OL' ] ) , ' \ ' . join ( [ '{:.1f}' . format ( max ( sum ( [ x [ 1 ] for x in self . results [ 'cycles' ] [ : i + 1 ] ] ) + self . results [ 'T n OL' ] , self . results [ 'T OL' ] ) ) for i in range ( len ( self . results [ 'cycles' ] ) ) ] ) ) if self . args . cores > 1 : report += " (single core)" report += '\nsaturating at {:.0f} cores' . format ( self . results [ 'scaling cores' ] ) if self . results [ 'multi-core' ] : report += "\nprediction for {} cores," . format ( self . results [ 'multi-core' ] [ 'cores' ] ) + " assuming static scheduling: " report += "{} ({})\n" . format ( self . results [ 'multi-core' ] [ 'performance' ] [ self . args . unit ] , ', ' . join ( self . results [ 'multi-core' ] [ 'notes' ] ) ) if self . results [ 'scaling prediction' ] : report += "\n Scaling prediction, considering memory bus utilization penalty and " "assuming all scalable caches:\n" if self . machine [ 'cores per socket' ] > self . machine [ 'cores per NUMA domain' ] : report += "1st NUMA dom." + ( len ( self . args . unit ) - 4 ) * ' ' + '||' + '--------' * ( self . machine [ 'cores per NUMA domain' ] - 1 ) + '-------|\n' report += "cores " + ( len ( self . args . unit ) + 2 ) * ' ' + " || " + ' | ' . join ( [ '{:<5}' . format ( s [ 'cores' ] ) for s in self . results [ 'scaling prediction' ] ] ) + '\n' report += "perf. ({}) || " . format ( self . args . unit ) + ' | ' . join ( [ '{:<5.1f}' . format ( float ( s [ 'performance' ] [ self . args . unit ] ) ) for s in self . results [ 'scaling prediction' ] ] ) + '\n' print ( report , file = output file ) if self . args and self . args . ecm plot : assert plot support , "matplotlib couldn't be imported. Plotting is not supported." fig = plt . figure ( frameon = False ) self . plot ( fig )
def plot ( self , fig = None ) : if not fig : fig = plt . gcf ( ) fig . subplots adjust ( left = 0.1 , right = 0.9 , top = 0.9 , bottom = 0.15 ) ax = fig . add subplot ( 1 , 1 , 1 ) sorted overlapping ports = sorted ( [ ( p , self . results [ 'port cycles' ] [ p ] ) for p in self . machine [ 'overlapping ports' ] ] , key = lambda x : x [ 1 ] ) yticks labels = [ ] yticks = [ ] xticks labels = [ ] xticks = [ ] # Plot configuration height = 0.9 i = 0 # T OL colors = ( [ ( 254. / 255 , 177. / 255. , 178. / 255. ) ] + [ ( 255. / 255. , 255. / 255. , 255. / 255. ) ] * ( len ( sorted overlapping ports ) - 1 ) ) for p , c in sorted overlapping ports : ax . barh ( i , c , height , align = 'center' , color = colors . pop ( ) , edgecolor = ( 0.5 , 0.5 , 0.5 ) , linestyle = 'dashed' ) if i == len ( sorted overlapping ports ) - 1 : ax . text ( c / 2.0 , i , '$T \mathrm{OL}$' , ha = 'center' , va = 'center' ) yticks labels . append ( p ) yticks . append ( i ) i += 1 xticks . append ( sorted overlapping ports [ - 1 ] [ 1 ] ) xticks labels . append ( '{:.1f}' . format ( sorted overlapping ports [ - 1 ] [ 1 ] ) ) # T n OL + memory transfers y = 0 colors = [ ( 187. / 255. , 255 / 255. , 188. / 255. ) ] * ( len ( self . results [ 'cycles' ] ) ) + [ ( 119. / 255 , 194. / 255. , 255. / 255. ) ] for k , v in [ ( 'n OL' , self . results [ 'T n OL' ] ) ] + self . results [ 'cycles' ] : ax . barh ( i , v , height , y , align = 'center' , color = colors . pop ( ) ) ax . text ( y + v / 2.0 , i , '$T \mathrm{' + k + '}$' , ha = 'center' , va = 'center' ) xticks . append ( y + v ) xticks labels . append ( '{:.1f}' . format ( y + v ) ) y += v yticks labels . append ( 'LD' ) yticks . append ( i ) ax . tick params ( axis = 'y' , which = 'both' , left = 'off' , right = 'off' ) ax . tick params ( axis = 'x' , which = 'both' , top = 'off' ) ax . set xlabel ( 't [cy]' ) ax . set ylabel ( 'execution port' ) ax . set yticks ( yticks ) ax . set yticklabels ( yticks labels ) ax . set xticks ( xticks ) ax . set xticklabels ( xticks labels , rotation = 'vertical' ) ax . xaxis . grid ( alpha = 0.7 , linestyle = '--' ) fig . savefig ( self . args . ecm plot )
def strip and uncomment ( asm lines ) : asm stripped = [ ] for line in asm lines : # Strip comments and whitespaces asm stripped . append ( line . split ( '#' ) [ 0 ] . strip ( ) ) return asm stripped
def strip unreferenced labels ( asm lines ) : asm stripped = [ ] for line in asm lines : if re . match ( r'^\S+:' , line ) : # Found label label = line [ 0 : line . find ( ':' ) ] # Search for references to current label if not any ( [ re . match ( r'^[^#]*\s' + re . escape ( label ) + '[\s,]?.*$' , l ) for l in asm lines ] ) : # Skip labels without seen reference line = '' asm stripped . append ( line ) return asm stripped
def find asm blocks ( asm lines ) : blocks = [ ] last labels = Ordered Dict ( ) packed ctr = 0 avx ctr = 0 xmm references = [ ] ymm references = [ ] zmm references = [ ] gp references = [ ] mem references = [ ] increments = { } for i , line in enumerate ( asm lines ) : # Register access counts zmm references += re . findall ( '%zmm[0-9]+' , line ) ymm references += re . findall ( '%ymm[0-9]+' , line ) xmm references += re . findall ( '%xmm[0-9]+' , line ) gp references += re . findall ( '%r[a-z0-9]+' , line ) if re . search ( r'\d*\(%\w+(,%\w+)?(,\d)?\)' , line ) : m = re . search ( r'(?P<off>[-]?\d*)\(%(?P<basep>\w+)(,%(?P<idx>\w+))?(?:,(?P<scale>\d))?\)' r'(?P<eol>$)?' , line ) mem references . append ( ( int ( m . group ( 'off' ) ) if m . group ( 'off' ) else 0 , m . group ( 'basep' ) , m . group ( 'idx' ) , int ( m . group ( 'scale' ) ) if m . group ( 'scale' ) else 1 , 'load' if m . group ( 'eol' ) is None else 'store' ) ) if re . match ( r"^[v]?(mul|add|sub|div|fmadd(132|213|231)?)[h]?p[ds]" , line ) : if line . startswith ( 'v' ) : avx ctr += 1 packed ctr += 1 elif re . match ( r'^\S+:' , line ) : # last labels[label name] = line number last labels [ line [ 0 : line . find ( ':' ) ] ] = i # Reset counters packed ctr = 0 avx ctr = 0 xmm references = [ ] ymm references = [ ] zmm references = [ ] gp references = [ ] mem references = [ ] increments = { } elif re . match ( r'^inc[bwlq]?\s+%[a-z0-9]+' , line ) : reg start = line . find ( '%' ) + 1 increments [ line [ reg start : ] ] = 1 elif re . match ( r'^add[bwlq]?\s+\$[0-9]+,\s*%[a-z0-9]+' , line ) : const start = line . find ( '$' ) + 1 const end = line [ const start + 1 : ] . find ( ',' ) + const start + 1 reg start = line . find ( '%' ) + 1 increments [ line [ reg start : ] ] = int ( line [ const start : const end ] ) elif re . match ( r'^dec[bwlq]?' , line ) : reg start = line . find ( '%' ) + 1 increments [ line [ reg start : ] ] = - 1 elif re . match ( r'^sub[bwlq]?\s+\$[0-9]+,' , line ) : const start = line . find ( '$' ) + 1 const end = line [ const start + 1 : ] . find ( ',' ) + const start + 1 reg start = line . find ( '%' ) + 1 increments [ line [ reg start : ] ] = - int ( line [ const start : const end ] ) elif last labels and re . match ( r'^j[a-z]+\s+\S+\s*' , line ) : # End of block(s) due to jump # Check if jump target matches any previously recoded label last label = None last label line = - 1 for label name , label line in last labels . items ( ) : if re . match ( r'^j[a-z]+\s+' + re . escape ( label name ) + r'\s*' , line ) : # matched last label = label name last label line = label line labels = list ( last labels . keys ( ) ) if last label : # deduce loop increment from memory index register pointer increment = None # default -> can not decide, let user choose possible idx regs = None if mem references : # we found memory references to work with # If store accesses exist, consider only those store references = [ mref for mref in mem references if mref [ 4 ] == 'store' ] refs = store references or mem references possible idx regs = list ( set ( increments . keys ( ) ) . intersection ( set ( [ r [ 1 ] for r in refs if r [ 1 ] is not None ] + [ r [ 2 ] for r in refs if r [ 2 ] is not None ] ) ) ) for mref in refs : for reg in list ( possible idx regs ) : # Only consider references with two registers, where one could be an index if None not in mref [ 1 : 3 ] : # One needs to mach, other registers will be excluded if not ( reg == mref [ 1 ] or reg == mref [ 2 ] ) : # reg can not be it possible idx regs . remove ( reg ) idx reg = None if len ( possible idx regs ) == 1 : # good, exactly one register was found idx reg = possible idx regs [ 0 ] elif possible idx regs and items Equal ( [ increments [ pidxreg ] for pidxreg in possible idx regs ] ) : # multiple were option found, but all have the same increment # use first match: idx reg = possible idx regs [ 0 ] if idx reg : mem scales = [ mref [ 3 ] for mref in refs if idx reg == mref [ 2 ] or idx reg == mref [ 1 ] ] if items Equal ( mem scales ) : # good, all scales are equal try : pointer increment = mem scales [ 0 ] * increments [ idx reg ] except : print ( "labels" , pformat ( labels [ labels . index ( last label ) : ] ) ) print ( "lines" , pformat ( asm lines [ last label line : i + 1 ] ) ) print ( "increments" , increments ) print ( "mem references" , pformat ( mem references ) ) print ( "idx reg" , idx reg ) print ( "mem scales" , mem scales ) raise blocks . append ( { 'first line' : last label line , 'last line' : i , 'ops' : i - last label line , 'labels' : labels [ labels . index ( last label ) : ] , 'packed instr' : packed ctr , 'avx instr' : avx ctr , 'XMM' : ( len ( xmm references ) , len ( set ( xmm references ) ) ) , 'YMM' : ( len ( ymm references ) , len ( set ( ymm references ) ) ) , 'ZMM' : ( len ( zmm references ) , len ( set ( zmm references ) ) ) , 'GP' : ( len ( gp references ) , len ( set ( gp references ) ) ) , 'regs' : ( len ( xmm references ) + len ( ymm references ) + len ( zmm references ) + len ( gp references ) , len ( set ( xmm references ) ) + len ( set ( ymm references ) ) + len ( set ( zmm references ) ) + len ( set ( gp references ) ) ) , 'pointer increment' : pointer increment , 'lines' : asm lines [ last label line : i + 1 ] , 'possible idx regs' : possible idx regs , 'mem references' : mem references , 'increments' : increments , } ) # Reset counters packed ctr = 0 avx ctr = 0 xmm references = [ ] ymm references = [ ] zmm references = [ ] gp references = [ ] mem references = [ ] increments = { } last labels = Ordered Dict ( ) return list ( enumerate ( blocks ) )
def select best block ( blocks ) : # TODO make this cleverer with more stats if not blocks : raise Value Error ( "No suitable blocks were found in assembly." ) best block = max ( blocks , key = lambda b : b [ 1 ] [ 'packed instr' ] ) if best block [ 1 ] [ 'packed instr' ] == 0 : best block = max ( blocks , key = lambda b : ( b [ 1 ] [ 'ops' ] + b [ 1 ] [ 'packed instr' ] + b [ 1 ] [ 'avx instr' ] , b [ 1 ] [ 'ZMM' ] , b [ 1 ] [ 'YMM' ] , b [ 1 ] [ 'XMM' ] ) ) return best block [ 0 ]
def userselect increment ( block ) : print ( "Selected block:" ) print ( '\n    ' + ( '\n    ' . join ( block [ 'lines' ] ) ) ) print ( ) increment = None while increment is None : increment = input ( "Choose store pointer increment (number of bytes): " ) try : increment = int ( increment ) except Value Error : increment = None block [ 'pointer increment' ] = increment return increment
def userselect block ( blocks , default = None , debug = False ) : print ( "Blocks found in assembly file:" ) print ( "      block     | O Ps | pck. | AVX || Registers |    ZMM   |    YMM   |    XMM   |    GP   ||ptr.inc|\n" "----------------+-----+------+-----++-----------+----------+----------+----------+---------++-------|" ) for idx , b in blocks : print ( '{:>2} {b[labels]!r:>12} | {b[ops]:>3} | {b[packed instr]:>4} | {b[avx instr]:>3} |' '| {b[regs][0]:>3} ({b[regs][1]:>3}) | {b[ZMM][0]:>3} ({b[ZMM][1]:>2}) | ' '{b[YMM][0]:>3} ({b[YMM][1]:>2}) | ' '{b[XMM][0]:>3} ({b[XMM][1]:>2}) | {b[GP][0]:>2} ({b[GP][1]:>2}) || ' '{b[pointer increment]!s:>5} |' . format ( idx , b = b ) ) if debug : ln = b [ 'first line' ] print ( ' ' * 4 + 'Code:' ) for l in b [ 'lines' ] : print ( ' ' * 8 + '{:>5} | {}' . format ( ln , l ) ) ln += 1 print ( ' ' * 4 + 'Metadata:' ) print ( textwrap . indent ( pformat ( { k : v for k , v in b . items ( ) if k not in [ 'lines' ] } ) , ' ' * 8 ) ) # Let user select block: block idx = - 1 while not ( 0 <= block idx < len ( blocks ) ) : block idx = input ( "Choose block to be marked [" + str ( default ) + "]: " ) or default try : block idx = int ( block idx ) except Value Error : block idx = - 1 # block = blocks[block idx][1] return block idx
def insert markers ( asm lines , start line , end line ) : asm lines = ( asm lines [ : start line ] + START MARKER + asm lines [ start line : end line + 1 ] + END MARKER + asm lines [ end line + 1 : ] ) return asm lines
def main ( ) : parser = argparse . Argument Parser ( description = 'Find and analyze basic loop blocks and mark for IACA.' , epilog = '/RRZE-HPC/kerncraft\n License: AGP Lv3' ) parser . add argument ( '--version' , action = 'version' , version = '%(prog)s {}' . format ( version ) ) parser . add argument ( 'source' , type = argparse . File Type ( ) , nargs = '?' , default = sys . stdin , help = 'assembly file to analyze (default: stdin)' ) parser . add argument ( '--outfile' , '-o' , type = argparse . File Type ( 'w' ) , nargs = '?' , default = sys . stdout , help = 'output file location (default: stdout)' ) parser . add argument ( '--debug' , action = 'store true' , help = 'Output nternal analysis information for debugging.' ) args = parser . parse args ( ) # pointer increment is given, since it makes no difference on the command lien and requires # less user input iaca instrumentation ( input file = args . source , output file = args . outfile , block selection = 'manual' , pointer increment = 1 , debug = args . debug )
def simulate ( kernel , model , define dict , blocking constant , blocking length ) : kernel . clear state ( ) # Add constants from define arguments for k , v in define dict . items ( ) : kernel . set constant ( k , v ) kernel . set constant ( blocking constant , blocking length ) model . analyze ( ) return sum ( [ cy for dscr , cy in model . results [ 'cycles' ] ] )
def get last modified datetime ( dir path = os . path . dirname ( file ) ) : max mtime = 0 for root , dirs , files in os . walk ( dir path ) : for f in files : p = os . path . join ( root , f ) try : max mtime = max ( max mtime , os . stat ( p ) . st mtime ) except File Not Found Error : pass return datetime . utcfromtimestamp ( max mtime )
def create parser ( ) : parser = argparse . Argument Parser ( description = 'Analytical performance modelling and benchmarking toolkit.' , epilog = '/RRZE-HPC/kerncraft\n License: AGP Lv3' ) parser . add argument ( '--version' , action = 'version' , version = '%(prog)s {}' . format ( version ) ) parser . add argument ( '--machine' , '-m' , type = argparse . File Type ( 'r' ) , required = True , help = 'Path to machine description yaml file.' ) parser . add argument ( '--pmodel' , '-p' , choices = models . all , required = True , action = 'append' , default = [ ] , help = 'Performance model to apply' ) parser . add argument ( '-D' , '--define' , nargs = 2 , metavar = ( 'KEY' , 'VALUE' ) , default = [ ] , action = Append String Range , help = 'Define constant to be used in C code. Values must be integer or ' 'match start-stop[:num[log[base]]]. If range is given, all ' 'permutation s will be tested. Overwrites constants from testcase ' 'file.' ) parser . add argument ( '--verbose' , '-v' , action = 'count' , default = 0 , help = 'Increases verbosity level.' ) parser . add argument ( 'code file' , metavar = 'FILE' , type = argparse . File Type ( ) , help = 'File with loop kernel C code' ) parser . add argument ( '--asm-block' , metavar = 'BLOCK' , default = 'auto' , help = 'Number of ASM block to mark for IACA, "auto" for automatic ' 'selection or "manual" for interactiv selection.' ) parser . add argument ( '--pointer-increment' , metavar = 'INCR' , default = 'auto' , type = int or str , help = 'Increment of store pointer within one ASM block in bytes. If "auto": ' 'automatic detection, error on failure to detect, if ' '"auto with manual fallback": fallback to manual input, or if ' '"manual": always prompt user.' ) parser . add argument ( '--store' , metavar = 'PICKLE' , type = argparse . File Type ( 'a+b' ) , help = 'Addes results to PICKLE file for later processing.' ) parser . add argument ( '--unit' , '-u' , choices = [ 'cy/CL' , 'cy/It' , 'It/s' , 'FLOP/s' ] , help = 'Select the output unit, defaults to model specific if not given.' ) parser . add argument ( '--cores' , '-c' , metavar = 'CORES' , type = int , default = 1 , help = 'Number of cores to be used in parallel. (default: 1) ' 'ECM model will consider the scaling of the last level cache and ' 'predict the overall performance in addition to single-core behavior. ' 'The benchmark mode will run the code with Open MP on as many physical ' 'cores.' ) parser . add argument ( '--kernel-description' , action = 'store true' , help = 'Use kernel description instead of analyzing the kernel code.' ) parser . add argument ( '--clean-intermediates' , action = 'store true' , help = 'If set, will delete all intermediate files after completion.' ) # Needed for ECM, ECM Data and Roofline model: parser . add argument ( '--cache-predictor' , '-P' , choices = [ 'LC' , 'SIM' ] , default = 'SIM' , help = 'Change cache predictor to use, options are LC (layer conditions) and ' 'SIM (cache simulation with pycachesim), default is SIM.' ) # Needed for ECM, Roofline IACA and Benchmark model: parser . add argument ( '--compiler' , '-C' , type = str , default = None , help = 'Compiler to use, default is first in machine description file.' ) parser . add argument ( '--compiler-flags' , type = str , default = None , help = 'Compiler flags to use. If not set, flags are taken from machine ' 'description file (-std=c99 is always added).' ) for m in models . all : ag = parser . add argument group ( 'arguments for ' + m + ' model' , getattr ( models , m ) . name ) getattr ( models , m ) . configure arggroup ( ag ) return parser
def check arguments ( args , parser ) : if args . asm block not in [ 'auto' , 'manual' ] : try : args . asm block = int ( args . asm block ) except Value Error : parser . error ( '--asm-block can only be "auto", "manual" or an integer' ) # Set default unit depending on performance model requested if not args . unit : if 'Roofline' in args . pmodel or 'Roofline IACA' in args . pmodel : args . unit = 'FLOP/s' else : args . unit = 'cy/CL'
def run ( parser , args , output file = sys . stdout ) : # Try loading results file (if requested) result storage = { } if args . store : args . store . seek ( 0 ) try : result storage = pickle . load ( args . store ) except EOF Error : pass args . store . close ( ) # machine information # Read machine description machine = Machine Model ( args . machine . name , args = args ) # process kernel if not args . kernel description : code = str ( args . code file . read ( ) ) code = clean code ( code ) kernel = Kernel Code ( code , filename = args . code file . name , machine = machine , keep intermediates = not args . clean intermediates ) else : description = str ( args . code file . read ( ) ) kernel = Kernel Description ( yaml . load ( description , Loader = yaml . Loader ) , machine = machine ) # if no defines were given, guess suitable defines in-mem # TODO support in-cache # TODO broaden cases to n-dimensions # TODO make configurable (no hardcoded 512MB/1GB/min. 3 iteration ...) # works only for up to 3 dimensions required consts = [ v [ 1 ] for v in kernel . variables . values ( ) if v [ 1 ] is not None ] required consts += [ [ l [ 'start' ] , l [ 'stop' ] ] for l in kernel . get loop stack ( ) ] # split into individual consts required consts = [ i for l in required consts for i in l ] required consts = set ( [ i for l in required consts for i in l . free symbols ] ) if len ( required consts ) > 0 : # build defines permutations define dict = { } for name , values in args . define : if name not in define dict : define dict [ name ] = [ [ name , v ] for v in values ] continue for v in values : if v not in define dict [ name ] : define dict [ name ] . append ( [ name , v ] ) define product = list ( itertools . product ( * list ( define dict . values ( ) ) ) ) # Check that all consts have been defined if set ( required consts ) . difference ( set ( [ symbol pos int ( k ) for k in define dict . keys ( ) ] ) ) : raise Value Error ( "Not all constants have been defined. Required are: {}" . format ( required consts ) ) else : define product = [ { } ] for define in define product : # Reset state of kernel kernel . clear state ( ) # Add constants from define arguments for k , v in define : kernel . set constant ( k , v ) for model name in uniquify ( args . pmodel ) : # print header print ( '{:^80}' . format ( ' kerncraft ' ) , file = output file ) print ( '{:<40}{:>40}' . format ( args . code file . name , '-m ' + args . machine . name ) , file = output file ) print ( ' ' . join ( [ '-D {} {}' . format ( k , v ) for k , v in define ] ) , file = output file ) print ( '{:-^80}' . format ( ' ' + model name + ' ' ) , file = output file ) if args . verbose > 1 : if not args . kernel description : kernel . print kernel code ( output file = output file ) print ( '' , file = output file ) kernel . print variables info ( output file = output file ) kernel . print kernel info ( output file = output file ) if args . verbose > 0 : kernel . print constants info ( output file = output file ) model = getattr ( models , model name ) ( kernel , machine , args , parser ) model . analyze ( ) model . report ( output file = output file ) # Add results to storage kernel name = os . path . split ( args . code file . name ) [ 1 ] if kernel name not in result storage : result storage [ kernel name ] = { } if tuple ( kernel . constants . items ( ) ) not in result storage [ kernel name ] : result storage [ kernel name ] [ tuple ( kernel . constants . items ( ) ) ] = { } result storage [ kernel name ] [ tuple ( kernel . constants . items ( ) ) ] [ model name ] = model . results print ( '' , file = output file ) # Save storage to file (if requested) if args . store : temp name = args . store . name + '.tmp' with open ( temp name , 'wb+' ) as f : pickle . dump ( result storage , f ) shutil . move ( temp name , args . store . name )
def main ( ) : # Create and populate parser parser = create parser ( ) # Parse given arguments args = parser . parse args ( ) # Checking arguments check arguments ( args , parser ) # BUSINESS LOGIC IS FOLLOWING run ( parser , args )
def main ( ) : parser = argparse . Argument Parser ( description = 'Recursively merges two or more pickle files. Only supports pickles consisting ' 'of a single dictionary object.' ) parser . add argument ( 'destination' , type = argparse . File Type ( 'r+b' ) , help = 'File to write to and include in resulting pickle. (WILL BE CHANGED)' ) parser . add argument ( 'source' , type = argparse . File Type ( 'rb' ) , nargs = '+' , help = 'File to include in resulting pickle.' ) args = parser . parse args ( ) result = pickle . load ( args . destination ) assert isinstance ( result , collections . Mapping ) , "only Mapping types can be handled." for s in args . source : data = pickle . load ( s ) assert isinstance ( data , collections . Mapping ) , "only Mapping types can be handled." update ( result , data ) args . destination . seek ( 0 ) args . destination . truncate ( ) pickle . dump ( result , args . destination )
def symbol pos int ( * args , * * kwargs ) : kwargs . update ( { 'positive' : True , 'integer' : True } ) return sympy . Symbol ( * args , * * kwargs )
def find node type ( ast , node type ) : if type ( ast ) is node type : return [ ast ] elif type ( ast ) is list : return reduce ( operator . add , list ( map ( lambda a : find node type ( a , node type ) , ast ) ) , [ ] ) elif ast is None : return [ ] else : return reduce ( operator . add , [ find node type ( o [ 1 ] , node type ) for o in ast . children ( ) ] , [ ] )
def force iterable ( f ) : def wrapper ( * args , * * kwargs ) : r = f ( * args , * * kwargs ) if hasattr ( r , ' iter ' ) : return r else : return [ r ] return wrapper
def check ( self ) : datatypes = [ v [ 0 ] for v in self . variables . values ( ) ] assert len ( set ( datatypes ) ) <= 1 , 'mixing of datatypes within a kernel is not supported.'
def subs consts ( self , expr ) : if isinstance ( expr , numbers . Number ) : return expr else : return expr . subs ( self . constants )
def remove duplicate accesses ( self ) : self . destinations = { var name : set ( acs ) for var name , acs in self . destinations . items ( ) } self . sources = { var name : set ( acs ) for var name , acs in self . sources . items ( ) }
def get loop stack ( self , subs consts = False ) : for l in self . loop stack : if subs consts : yield { 'index' : l [ 0 ] , 'start' : self . subs consts ( l [ 1 ] ) , 'stop' : self . subs consts ( l [ 2 ] ) , 'increment' : self . subs consts ( l [ 3 ] ) } else : yield { 'index' : l [ 0 ] , 'start' : l [ 1 ] , 'stop' : l [ 2 ] , 'increment' : l [ 3 ] }
def global iterator ( self ) : global iterator = sympy . Integer ( 0 ) total length = sympy . Integer ( 1 ) for var name , start , end , incr in reversed ( self . loop stack ) : loop var = symbol pos int ( var name ) length = end - start # FIXME is incr handled correct here? global iterator += ( loop var - start ) * total length total length *= length return global iterator
def max global iteration ( self ) : return self . indices to global iterator ( { symbol pos int ( var name ) : end - 1 for var name , start , end , incr in self . loop stack } )
def print kernel info ( self , output file = sys . stdout ) : table = ( '     idx |        min        max       step\n' + '---------+---------------------------------\n' ) for l in self . loop stack : table += '{:>8} | {!r:>10} {!r:>10} {!r:>10}\n' . format ( * l ) print ( prefix indent ( 'loop stack:        ' , table ) , file = output file ) table = ( '    name |  offsets   ...\n' + '---------+------------...\n' ) for name , offsets in list ( self . sources . items ( ) ) : prefix = '{:>8} | ' . format ( name ) right side = '\n' . join ( [ '{!r:}' . format ( o ) for o in offsets ] ) table += prefix indent ( prefix , right side , later prefix = '         | ' ) print ( prefix indent ( 'data sources:      ' , table ) , file = output file ) table = ( '    name |  offsets   ...\n' + '---------+------------...\n' ) for name , offsets in list ( self . destinations . items ( ) ) : prefix = '{:>8} | ' . format ( name ) right side = '\n' . join ( [ '{!r:}' . format ( o ) for o in offsets ] ) table += prefix indent ( prefix , right side , later prefix = '         | ' ) print ( prefix indent ( 'data destinations: ' , table ) , file = output file ) table = ( ' op | count \n' + '----+-------\n' ) for op , count in list ( self . flops . items ( ) ) : table += '{:>3} | {:>4}\n' . format ( op , count ) table += '     =======\n' table += '      {:>4}' . format ( sum ( self . flops . values ( ) ) ) print ( prefix indent ( 'FLO Ps:     ' , table ) , file = output file )
def print variables info ( self , output file = sys . stdout ) : table = ( '    name |   type size             \n' + '---------+-------------------------\n' ) for name , var info in list ( self . variables . items ( ) ) : table += '{:>8} | {:>6} {!s:<10}\n' . format ( name , var info [ 0 ] , var info [ 1 ] ) print ( prefix indent ( 'variables: ' , table ) , file = output file )
def print constants info ( self , output file = sys . stdout ) : table = ( '    name | value     \n' + '---------+-----------\n' ) for name , value in list ( self . constants . items ( ) ) : table += '{!s:>8} | {:<10}\n' . format ( name , value ) print ( prefix indent ( 'constants: ' , table ) , file = output file )
def print kernel code ( self , output file = sys . stdout ) : print ( self . kernel code , file = output file )
def get array declarations ( self ) : return [ d for d in self . kernel ast . block items if type ( d ) is c ast . Decl and type ( d . type ) is c ast . Array Decl ]
def get kernel loop nest ( self ) : loop nest = [ s for s in self . kernel ast . block items if type ( s ) in [ c ast . For , c ast . Pragma , c ast . Func Call ] ] assert len ( loop nest ) >= 1 , "Found to few for statements in kernel" return loop nest
def find inner most loop ( self , loop nest ) : r = None for s in loop nest : if type ( s ) is c ast . For : return self . find inner most loop ( s ) or s else : r = r or self . find inner most loop ( s ) return r
def build kernel function declaration ( self , name = 'kernel' ) : array declarations , array dimensions = self . build array declarations ( with init = False ) scalar declarations = self . build scalar declarations ( with init = False ) const declarations = self . build const declartions ( with init = False ) return c ast . Func Decl ( args = c ast . Param List ( params = array declarations + scalar declarations + const declarations ) , type = c ast . Type Decl ( declname = name , quals = [ ] , type = c ast . Identifier Type ( names = [ 'void' ] ) ) )
def build scalar declarations ( self , with init = True ) : # copy scalar declarations from from kernel ast scalar declarations = [ deepcopy ( d ) for d in self . kernel ast . block items if type ( d ) is c ast . Decl and type ( d . type ) is c ast . Type Decl ] # add init values to declarations if with init : random . seed ( 2342 ) # we want reproducible random numbers for d in scalar declarations : if d . type . type . names [ 0 ] in [ 'double' , 'float' ] : d . init = c ast . Constant ( 'float' , str ( random . uniform ( 1.0 , 0.1 ) ) ) elif d . type . type . names [ 0 ] in [ 'int' , 'long' , 'long long' , 'unsigned int' , 'unsigned long' , 'unsigned long long' ] : d . init = c ast . Constant ( 'int' , 2 ) return scalar declarations
def build kernel call ( self , name = 'kernel' ) : return c ast . Func Call ( name = c ast . ID ( name = name ) , args = c ast . Expr List ( exprs = [ c ast . ID ( name = d . name ) for d in ( self . build array declarations ( ) [ 0 ] + self . build scalar declarations ( ) + self . build const declartions ( ) ) ] ) )
def get main code ( self , as filename = False , kernel function name = 'kernel' ) : # TODO produce nicer code, including help text and other "comfort features". assert self . kernel ast is not None , "AST does not exist, this could be due to running " "based on a kernel description rather than code." fp , already available = self . get intermediate file ( 'main.c' , machine and compiler dependent = False ) # Use already cached version if already available : code = fp . read ( ) else : parser = C Parser ( ) template code = self . CODE TEMPLATE template ast = parser . parse ( clean code ( template code , macros = True , comments = True , pragmas = False ) ) ast = deepcopy ( template ast ) # Define and replace DECLARE CONSTS replace id ( ast , "DECLARE CONSTS" , self . build const declartions ( with init = True ) ) # Define and replace DECLARE ARRAYS array declarations , array dimensions = self . build array declarations ( ) replace id ( ast , "DECLARE ARRAYS" , array declarations ) # Define and replace DECLARE INIT SCALARS replace id ( ast , "DECLARE INIT SCALARS" , self . build scalar declarations ( ) ) # Define and replace DUMMY CALLS replace id ( ast , "DUMMY CALLS" , self . build dummy calls ( ) ) # Define and replace KERNEL DECL ast . ext . insert ( 0 , self . build kernel function declaration ( name = kernel function name ) ) # Define and replace KERNEL CALL replace id ( ast , "KERNEL CALL" , self . build kernel call ( ) ) # Define and replace INIT ARRAYS based on previously generated kernel replace id ( ast , "INIT ARRAYS" , self . build array initializations ( array dimensions ) ) # Generate code code = C Generator ( ) . visit ( ast ) # Insert missing #includes from template to top of code code = '\n' . join ( [ l for l in template code . split ( '\n' ) if l . startswith ( "#include" ) ] ) + '\n\n' + code # Store to file fp . write ( code ) fp . close ( ) if as filename : return fp . name else : return code
def build executable ( self , lflags = None , verbose = False , openmp = False ) : compiler , compiler args = self . machine . get compiler ( ) kernel obj filename = self . compile kernel ( openmp = openmp , verbose = verbose ) out filename , already exists = self . get intermediate file ( os . path . splitext ( os . path . basename ( kernel obj filename ) ) [ 0 ] , binary = True , fp = False ) if not already exists : main source filename = self . get main code ( as filename = True ) if not ( ( 'LIKWID INCLUDE' in os . environ or 'LIKWID INC' in os . environ ) and 'LIKWID LIB' in os . environ ) : print ( 'Could not find LIKWID INCLUDE (e.g., "-I/app/likwid/4.1.2/include") and ' 'LIKWID LIB (e.g., "-L/apps/likwid/4.1.2/lib") environment variables' , file = sys . stderr ) sys . exit ( 1 ) compiler args += [ '-std=c99' , '-I' + reduce path ( os . path . abspath ( os . path . dirname ( os . path . realpath ( file ) ) ) + '/headers/' ) , os . environ . get ( 'LIKWID INCLUDE' , '' ) , os . environ . get ( 'LIKWID INC' , '' ) , '-llikwid' ] # This is a special case for unittesting if os . environ . get ( 'LIKWID LIB' ) == '' : compiler args = compiler args [ : - 1 ] if lflags is None : lflags = [ ] lflags += os . environ [ 'LIKWID LIB' ] . split ( ' ' ) + [ '-pthread' ] compiler args += os . environ [ 'LIKWID LIB' ] . split ( ' ' ) + [ '-pthread' ] infiles = [ reduce path ( os . path . abspath ( os . path . dirname ( os . path . realpath ( file ) ) ) + '/headers/dummy.c' ) , kernel obj filename , main source filename ] cmd = [ compiler ] + infiles + compiler args + [ '-o' , out filename ] # remove empty arguments cmd = list ( filter ( bool , cmd ) ) if verbose : print ( 'Executing (build executable): ' , ' ' . join ( cmd ) ) try : subprocess . check output ( cmd ) except subprocess . Called Process Error as e : print ( "Build failed:" , e , file = sys . stderr ) sys . exit ( 1 ) else : if verbose : print ( 'Executing (build executable): ' , 'using cached' , out filename ) return out filename
def string to sympy ( cls , s ) : if isinstance ( s , int ) : return sympy . Integer ( s ) elif isinstance ( s , list ) : return tuple ( [ cls . string to sympy ( e ) for e in s ] ) elif s is None : return None else : # Step 1 build expression with the whole alphabet redefined: local dict = { c : symbol pos int ( c ) for c in s if c in string . ascii letters } # TODO find nicer solution for N and other pre-mapped letters preliminary expr = parse expr ( s , local dict = local dict ) # Replace all free symbols with positive integer versions: local dict . update ( { s . name : symbol pos int ( s . name ) for s in preliminary expr . free symbols } ) return parse expr ( s , local dict = local dict )
def get identifier ( self ) : if self . path : return os . path . basename ( self . path ) else : return hashlib . sha256 ( hashlib . sha256 ( repr ( self . data ) . encode ( ) ) ) . hexdigest ( )
def get last modified datetime ( self ) : if self . path : statbuf = os . stat ( self . path ) return datetime . utcfromtimestamp ( statbuf . st mtime ) else : return datetime . now ( )
def enforce no overlap ( self , start at = 0 ) : i = start at while i + 1 < len ( self . data ) : if self . data [ i ] [ 1 ] >= self . data [ i + 1 ] [ 0 ] : # beginning of i+1-th range is contained in i-th range if self . data [ i ] [ 1 ] < self . data [ i + 1 ] [ 1 ] : # i+1-th range is longer, thus enlarge i-th range self . data [ i ] [ 1 ] = self . data [ i + 1 ] [ 1 ] # removed contained range del self . data [ i + 1 ] i += 1
def get header path ( ) -> str : import os return os . path . abspath ( os . path . dirname ( os . path . realpath ( file ) ) ) + '/headers/'
def align iteration with cl boundary ( self , iteration , subtract = True ) : # FIXME handle multiple datatypes element size = self . kernel . datatypes size [ self . kernel . datatype ] cacheline size = self . machine [ 'cacheline size' ] elements per cacheline = int ( cacheline size // element size ) # Gathering some loop information: inner loop = list ( self . kernel . get loop stack ( subs consts = True ) ) [ - 1 ] inner increment = inner loop [ 'increment' ] # do this by aligning either writes (preferred) or reads # Assumption: writes (and reads) increase linearly o = self . kernel . compile global offsets ( iteration = iteration ) [ 0 ] if len ( o [ 1 ] ) : # we have a write to work with: first offset = min ( o [ 1 ] ) else : # we use reads first offset = min ( o [ 0 ] ) diff = first offset - ( int ( first offset ) >> self . csim . first level . cl bits << self . csim . first level . cl bits ) if diff == 0 : return iteration elif subtract : return iteration - ( diff // element size ) // inner increment else : return iteration + ( elements per cacheline - diff // element size ) // inner increment
def get loads ( self ) : return [ self . stats [ cache level ] [ 'LOAD count' ] / self . first dim factor for cache level in range ( len ( self . machine [ 'memory hierarchy' ] ) ) ]
def get hits ( self ) : return [ self . stats [ cache level ] [ 'HIT count' ] / self . first dim factor for cache level in range ( len ( self . machine [ 'memory hierarchy' ] ) ) ]
def get misses ( self ) : return [ self . stats [ cache level ] [ 'MISS count' ] / self . first dim factor for cache level in range ( len ( self . machine [ 'memory hierarchy' ] ) ) ]
def get stores ( self ) : return [ self . stats [ cache level ] [ 'STORE count' ] / self . first dim factor for cache level in range ( len ( self . machine [ 'memory hierarchy' ] ) ) ]
def get evicts ( self ) : return [ self . stats [ cache level ] [ 'EVICT count' ] / self . first dim factor for cache level in range ( len ( self . machine [ 'memory hierarchy' ] ) ) ]
def get infos ( self ) : first dim factor = self . first dim factor infos = { 'memory hierarchy' : [ ] , 'cache stats' : self . stats , 'cachelines in stats' : first dim factor } for cache level , cache info in list ( enumerate ( self . machine [ 'memory hierarchy' ] ) ) : infos [ 'memory hierarchy' ] . append ( { 'index' : len ( infos [ 'memory hierarchy' ] ) , 'level' : '{}' . format ( cache info [ 'level' ] ) , 'total loads' : self . stats [ cache level ] [ 'LOAD byte' ] / first dim factor , 'total misses' : self . stats [ cache level ] [ 'MISS byte' ] / first dim factor , 'total hits' : self . stats [ cache level ] [ 'HIT byte' ] / first dim factor , 'total stores' : self . stats [ cache level ] [ 'STORE byte' ] / first dim factor , 'total evicts' : self . stats [ cache level ] [ 'EVICT byte' ] / first dim factor , 'total lines load' : self . stats [ cache level ] [ 'LOAD count' ] / first dim factor , 'total lines misses' : self . stats [ cache level ] [ 'MISS count' ] / first dim factor , 'total lines hits' : self . stats [ cache level ] [ 'HIT count' ] / first dim factor , 'total lines stores' : self . stats [ cache level ] [ 'STORE count' ] / first dim factor , 'total lines evicts' : self . stats [ cache level ] [ 'EVICT count' ] / first dim factor , 'cycles' : None } ) return infos
def measure bw ( type , total size , threads per core , max threads per core , cores per socket , sockets ) : groups = [ ] for s in range ( sockets ) : groups += [ '-w' , 'S' + str ( s ) + ':' + str ( total size ) + 'k B:' + str ( threads per core * cores per socket ) + ':1:' + str ( int ( max threads per core / threads per core ) ) ] # for older likwid versions add ['-g', str(sockets), '-i', str(iterations)] to cmd cmd = [ 'likwid-bench' , '-t' , type ] + groups sys . stderr . write ( ' ' . join ( cmd ) ) output = subprocess . Popen ( cmd , stdout = subprocess . PIPE ) . communicate ( ) [ 0 ] . decode ( 'utf-8' ) if not output : print ( ' ' . join ( cmd ) + ' returned no output, possibly wrong version installed ' '(requires 4.0 or later)' , file = sys . stderr ) sys . exit ( 1 ) bw = float ( get match or break ( r'^M Byte/s:\s+([0-9]+(?:\.[0-9]+)?)\s*$' , output ) [ 0 ] ) print ( ' ' , Prefixed Unit ( bw , 'MB/s' ) , file = sys . stderr ) return Prefixed Unit ( bw , 'MB/s' )
def fix env variable ( name , value ) : orig = os . environ . get ( name , None ) if value is not None : # Set if value is not None os . environ [ name ] = value elif name in os . environ : # Unset if value is None del os . environ [ name ] try : yield finally : if orig is not None : # Restore original value os . environ [ name ] = orig elif name in os . environ : # Unset del os . environ [ name ]
def configure arggroup ( cls , parser ) : parser . add argument ( '--no-phenoecm' , action = 'store true' , help = 'Disables the phenomenological ECM model building.' ) parser . add argument ( '--iterations' , type = int , default = 10 , help = 'Number of outer-loop iterations (e.g. time loop) during benchmarking. ' 'Default is 10, but actual number will be adapted to at least 0.2s runtime.' ) parser . add argument ( '--ignore-warnings' , action = 'store true' , help = 'Ignore warnings about missmatched CPU model and frequency.' )
def report ( self , output file = sys . stdout ) : if self . verbose > 1 : with pprint nosort ( ) : pprint . pprint ( self . results ) if self . verbose > 0 : print ( 'Runtime (per repetition): {:.2g} s' . format ( self . results [ 'Runtime (per repetition) [s]' ] ) , file = output file ) if self . verbose > 0 : print ( 'Iterations per repetition: {!s}' . format ( self . results [ 'Iterations per repetition' ] ) , file = output file ) print ( 'Runtime (per cacheline update): {:.2f} cy/CL' . format ( self . results [ 'Runtime (per cacheline update) [cy/CL]' ] ) , file = output file ) print ( 'MEM volume (per repetition): {:.0f} Byte' . format ( self . results [ 'MEM volume (per repetition) [B]' ] ) , file = output file ) print ( 'Performance: {:.2f} MFLOP/s' . format ( self . results [ 'Performance [MFLOP/s]' ] ) , file = output file ) print ( 'Performance: {:.2f} MLUP/s' . format ( self . results [ 'Performance [MLUP/s]' ] ) , file = output file ) print ( 'Performance: {:.2f} It/s' . format ( self . results [ 'Performance [M It/s]' ] ) , file = output file ) if self . verbose > 0 : print ( 'MEM bandwidth: {:.2f} M Byte/s' . format ( self . results [ 'MEM BW [M Byte/s]' ] ) , file = output file ) print ( '' , file = output file ) if not self . no phenoecm : print ( "Data Transfers:" ) print ( "{:^8} |" . format ( "cache" ) , end = '' ) for metrics in self . results [ 'data transfers' ] . values ( ) : for metric name in sorted ( metrics ) : print ( " {:^14}" . format ( metric name ) , end = '' ) print ( ) break for cache , metrics in sorted ( self . results [ 'data transfers' ] . items ( ) ) : print ( "{!s:^8} |" . format ( cache ) , end = '' ) for k , v in sorted ( metrics . items ( ) ) : print ( " {!s:^14}" . format ( v ) , end = '' ) print ( ) print ( ) print ( 'Phenomenological ECM model: {{ {T OL:.1f} || {T n OL:.1f} | {T L1L2:.1f} | ' '{T L2L3:.1f} | {T L3MEM:.1f} }} cy/CL' . format ( * * { k : float ( v ) for k , v in self . results [ 'ECM' ] . items ( ) } ) , file = output file ) print ( 'T OL assumes that two loads per cycle may be retiered, which is true for ' '128bit SSE/half-AVX loads on SNB and IVY, and 256bit full-AVX loads on HSW, ' 'BDW, SKL and SKX, but it also depends on AGU availability.' , file = output file )
def build purchase item ( course id , course url , cost in cents , mode , course data , sku ) : # build item description item = { 'id' : "{}-{}" . format ( course id , mode ) , 'url' : course url , 'price' : cost in cents , 'qty' : 1 , } # get title from course info if we don't already have it from Sailthru if 'title' in course data : item [ 'title' ] = course data [ 'title' ] else : # can't find, just invent title item [ 'title' ] = 'Course {} mode: {}' . format ( course id , mode ) if 'tags' in course data : item [ 'tags' ] = course data [ 'tags' ] # add vars to item item [ 'vars' ] = dict ( course data . get ( 'vars' , { } ) , mode = mode , course run id = course id ) item [ 'vars' ] [ 'purchase sku' ] = sku return item
def send offer assignment notification email ( config , user email , subject , email body , site code , task ) : try : sailthru client = get sailthru client ( site code ) except Sailthru Error : logger . exception ( '[Offer Assignment] A client error occurred while attempting to send a offer assignment notification.' ' Message: {message}' . format ( message = email body ) ) return None email vars = { 'subject' : subject , 'email body' : email body , } try : response = sailthru client . send ( template = config [ 'templates' ] [ 'assignment email' ] , email = user email , vars = email vars ) except Sailthru Client Error : logger . exception ( '[Offer Assignment] A client error occurred while attempting to send a offer assignment notification.' ' Message: {message}' . format ( message = email body ) ) return None if not response . is ok ( ) : error = response . get error ( ) logger . error ( '[Offer Assignment] A {token error code} - {token error message} error occurred' ' while attempting to send a offer assignment notification.' ' Message: {message}' . format ( message = email body , token error code = error . get error code ( ) , token error message = error . get message ( ) ) ) if can retry sailthru request ( error ) : logger . info ( '[Offer Assignment] An attempt will be made to resend the offer assignment notification.' ' Message: {message}' . format ( message = email body ) ) schedule retry ( task , config ) else : logger . warning ( '[Offer Assignment] No further attempts will be made to send the offer assignment notification.' ' Failed Message: {message}' . format ( message = email body ) ) return response
def file refs ( self ) : if self . prepared file refs is None : self . prepared file refs = { FILE REFS . idf : File Info ( constructor = lambda path : self . epm cls . from idf ( path , idd or buffer or path = self . idd ) , get path = lambda : get input file path ( self . dir path , FILE REFS . idf ) ) , FILE REFS . epw : File Info ( constructor = lambda path : self . weather data cls . from epw ( path ) , get path = lambda : get input file path ( self . dir path , FILE REFS . epw ) ) , FILE REFS . eio : File Info ( constructor = lambda path : self . eio cls ( path ) , get path = lambda : get output file path ( self . dir path , FILE REFS . eio ) ) , FILE REFS . eso : File Info ( constructor = lambda path : self . standard output cls ( path ) , get path = lambda : get output file path ( self . dir path , FILE REFS . eso ) ) , FILE REFS . mtr : File Info ( constructor = lambda path : self . standard output cls ( path ) , get path = lambda : get output file path ( self . dir path , FILE REFS . mtr ) ) , FILE REFS . mtd : File Info ( constructor = lambda path : self . mtd cls ( path ) , get path = lambda : get output file path ( self . dir path , FILE REFS . mtd ) ) , FILE REFS . mdd : File Info ( constructor = lambda path : open ( path ) . read ( ) , get path = lambda : get output file path ( self . dir path , FILE REFS . mdd ) ) , FILE REFS . err : File Info ( constructor = lambda path : self . err cls ( path ) , get path = lambda : get output file path ( self . dir path , FILE REFS . err ) ) , FILE REFS . summary table : File Info ( constructor = lambda path : self . summary table cls ( path ) , get path = lambda : get output file path ( self . dir path , FILE REFS . summary table ) ) } return self . prepared file refs
def dev populate from json data ( self , json data ) : # workflow # -------- # (methods belonging to create/update/delete framework: #     epm. dev populate from json data, table.batch add, record.update, queryset.delete, record.delete) # 1. add inert #     * data is checked #     * old links are unregistered #     * record is stored in table (=> pk uniqueness is checked) # 2. activate: hooks, links, external files # manage comment if any comment = json data . pop ( " comment" , None ) if comment is not None : self . comment = comment # populate external files external files data = json data . pop ( " external files" , dict ( ) ) self . dev external files manager . populate from json data ( external files data ) # manage records added records = [ ] for table ref , json data records in json data . items ( ) : # find table table = getattr ( self , table ref ) # create record (inert) records = table . dev add inert ( json data records ) # add records (inert) added records . extend ( records ) # activate hooks for r in added records : r . dev activate hooks ( ) # activate links and external files for r in added records : r . dev activate links ( ) r . dev activate external files ( )
def get external files ( self ) : external files = [ ] for table in self . tables . values ( ) : for r in table : external files . extend ( [ ef for ef in r . get external files ( ) ] ) return external files
def set defaults ( self ) : for table in self . tables . values ( ) : for r in table : r . set defaults ( )
def prepare extensible ( self ) : # see if extensible and store cycle len for k in self . tags : if "extensible" in k : cycle len = int ( k . split ( ":" ) [ 1 ] ) break else : # not extensible return # find cycle start and prepare patterns cycle start = None cycle patterns = [ ] for i , field descriptor in enumerate ( self . field descriptors ) : # quit if finished if ( cycle start is not None ) and ( i >= ( cycle start + cycle len ) ) : break # set cycle start if not set yet if ( cycle start is None ) and ( "begin-extensible" in field descriptor . tags ) : cycle start = i # leave if cycle start not reached yet if cycle start is None : continue # store pattern cycle patterns . append ( field descriptor . ref . replace ( "1" , r"(\d+)" ) ) else : raise Runtime Error ( "cycle start not found" ) # detach unnecessary field descriptors self . field descriptors = self . field descriptors [ : cycle start + cycle len ] # store cycle info self . extensible info = ( cycle start , cycle len , tuple ( cycle patterns ) ) # set field descriptor cycle start index (for error messages while serialization) for i , fd in enumerate ( self . field descriptors [ cycle start : ] ) : fd . set extensible info ( cycle start , cycle len , cycle patterns [ i ] )
def get value ( self , column name or i , filter column name or i , filter criterion ) : # find column indexes column i = self . get column index ( column name or i ) filter column i = self . get column index ( filter column name or i ) filter fct = { float : lambda x : float ( x ) == filter criterion , int : lambda x : int ( x ) == filter criterion , str : lambda x : x . lower ( ) == filter criterion . lower ( ) } [ type ( filter criterion ) ] for row i , row in enumerate ( self . data ) : if filter fct ( row [ filter column i ] ) : break else : raise Value Error ( "Filter did not return any values." ) return self . data [ row i ] [ column i ]
def update value inert ( self , index , value ) : # get field descriptor field descriptor = self . table . dev descriptor . get field descriptor ( index ) # prepare value value = field descriptor . deserialize ( value , index ) # unregister previous link if relevant if isinstance ( value , Link ) : # de-activate current link if any current link = self . data . get ( index ) if current link is not None : current link . unregister ( ) # unregister previous hook if relevant if isinstance ( value , Record Hook ) : current record hook = self . data . get ( index ) if current record hook is not None : current record hook . unregister ( ) # unregister previous external file if relevant if isinstance ( value , External File ) : current external file = self . data . get ( index ) if current external file is not None : current external file . dev unregister ( ) # if None remove and leave if value in ( None , NONE RECORD HOOK , NONE LINK , NONE EXTERNAL FILE ) : # we don't check required, because this method is called by  update inert which does the job self . dev set none without unregistering ( index , check not required = False ) return # if relevant, store current pk to signal table old hook = None if index == 0 and not self . table . dev auto pk : old hook = self . data . get ( 0 ) # we use get, because record may not have a pk yet if it is being created # set value self . data [ index ] = value # signal pk update if relevant if old hook is not None : self . table . dev record pk was updated ( old hook . target value )
def set defaults ( self ) : defaults = { } for i in range ( len ( self ) ) : if i in self . data : continue default = self . get field descriptor ( i ) . tags . get ( "default" , [ None ] ) [ 0 ] if default is not None : defaults [ i ] = default self . update ( defaults )
def delete ( self ) : # workflow # -------- # (methods belonging to create/update/delete framework: #     epm. dev populate from json data, table.batch add, record.update, queryset.delete, record.delete) # 1. unregister: links, hooks and external files # 3. remove from table without unregistering # unregister links self . unregister links ( ) # unregister hooks self . unregister hooks ( ) # unregister external files self . unregister external files ( ) # tell table to remove without unregistering self . get table ( ) . dev remove record without unregistering ( self ) # make stale self . table = None self . data = None
def register record hook ( self , hook ) : for key in hook . keys : if key in self . record hooks : field descriptor = hook . target record . get field descriptor ( hook . target index ) raise Field Validation Error ( f"Reference key already exists, can't create: {key}. " f"{field descriptor.get error location message(hook.target value, hook.target index)}" ) self . record hooks [ key ] = hook
def register link ( self , link ) : keys = tuple ( ( ref , link . initial hook value ) for ref in link . hook references ) # look for a record hook for k in keys : if k in self . record hooks : # set link target link . set target ( target record = self . record hooks [ k ] . target record ) break else : # look for a table hook for k in keys : if k in self . table hooks : # set link target link . set target ( target table = self . table hooks [ k ] ) break else : field descriptor = link . source record . get field descriptor ( link . source index ) raise Field Validation Error ( f"No object found with any of given references : {keys}. " f"{field descriptor.get error location message(link.initial hook value)}" ) # store by source if link . source record not in self . links by source : self . links by source [ link . source record ] = set ( ) self . links by source [ link . source record ] . add ( link ) # store by target if link . target not in self . links by target : self . links by target [ link . target ] = set ( ) self . links by target [ link . target ] . add ( link )
def create regex ( self , line , intent name ) : try : return re . compile ( self . create intent pattern ( line , intent name ) , re . IGNORECASE ) except sre constants . error as e : LOG . warning ( 'Failed to parse the line "{}" ' 'for {}' . format ( line , intent name ) ) return None
def remaining duration ( self , time ) : return max ( 0 , self . end - max ( self . start , time ) )
def http request ( url , post data = None ) : logger . debug ( 'Requesting URL: %s' % url ) buf = bio ( ) curl = pycurl . Curl ( ) curl . setopt ( curl . URL , url . encode ( 'ascii' , 'ignore' ) ) # Disable HTTPS verification methods if insecure is set if config ( ) [ 'server' ] [ 'insecure' ] : curl . setopt ( curl . SSL VERIFYPEER , 0 ) curl . setopt ( curl . SSL VERIFYHOST , 0 ) if config ( ) [ 'server' ] [ 'certificate' ] : # Make sure verification methods are turned on curl . setopt ( curl . SSL VERIFYPEER , 1 ) curl . setopt ( curl . SSL VERIFYHOST , 2 ) # Import your certificates curl . setopt ( pycurl . CAINFO , config ( ) [ 'server' ] [ 'certificate' ] ) if post data : curl . setopt ( curl . HTTPPOST , post data ) curl . setopt ( curl . WRITEFUNCTION , buf . write ) curl . setopt ( pycurl . HTTPAUTH , pycurl . HTTPAUTH DIGEST ) curl . setopt ( pycurl . USERPWD , "%s:%s" % ( config ( ) [ 'server' ] [ 'username' ] , config ( ) [ 'server' ] [ 'password' ] ) ) curl . setopt ( curl . HTTPHEADER , [ 'X-Requested-Auth: Digest' ] ) curl . setopt ( curl . FAILONERROR , True ) curl . setopt ( curl . FOLLOWLOCATION , True ) curl . perform ( ) curl . close ( ) result = buf . getvalue ( ) buf . close ( ) return result
def try mkdir ( directory ) : try : os . mkdir ( directory ) except OS Error as err : if err . errno != errno . EEXIST : raise err
def update event status ( event , status ) : dbs = db . get session ( ) dbs . query ( db . Recorded Event ) . filter ( db . Recorded Event . start == event . start ) . update ( { 'status' : status } ) event . status = status dbs . commit ( )
def set service status ( service , status ) : srv = db . Service States ( ) srv . type = service srv . status = status dbs = db . get session ( ) dbs . merge ( srv ) dbs . commit ( ) dbs . close ( )
def get service status ( service ) : dbs = db . get session ( ) srvs = dbs . query ( db . Service States ) . filter ( db . Service States . type == service ) if srvs . count ( ) : return srvs [ 0 ] . status return db . Service Status . STOPPED
def update agent state ( ) : configure service ( 'capture.admin' ) status = 'idle' # Determine reported agent state with priority list if get service status ( db . Service . SCHEDULE ) == db . Service Status . STOPPED : status = 'offline' elif get service status ( db . Service . CAPTURE ) == db . Service Status . BUSY : status = 'capturing' elif get service status ( db . Service . INGEST ) == db . Service Status . BUSY : status = 'uploading' register ca ( status = status )
def configuration file ( cfgfile ) : if cfgfile is not None : return cfgfile # If no file is explicitely specified, probe for the configuration file # location. cfg = './etc/pyca.conf' if not os . path . isfile ( cfg ) : return '/etc/pyca.conf' return cfg
def check ( ) : if config ( 'server' ) [ 'insecure' ] : logger . warning ( 'HTTPS CHECKS ARE TURNED OFF. A SECURE CONNECTION IS ' 'NOT GUARANTEED' ) if config ( 'server' ) [ 'certificate' ] : # Ensure certificate exists and is readable open ( config ( 'server' ) [ 'certificate' ] , 'rb' ) . close ( ) if config ( 'agent' ) [ 'backup mode' ] : logger . info ( 'Agent runs in backup mode. No data will be sent to ' 'Opencast' )
def logger init ( ) : handlers = [ ] logconf = config ( 'logging' ) if logconf [ 'syslog' ] : handlers . append ( logging . handlers . Sys Log Handler ( address = '/dev/log' ) ) if logconf [ 'stderr' ] : handlers . append ( logging . Stream Handler ( sys . stderr ) ) if logconf [ 'file' ] : handlers . append ( logging . handlers . Watched File Handler ( logconf [ 'file' ] ) ) for handler in handlers : handler . set Formatter ( logging . Formatter ( logconf [ 'format' ] ) ) logging . root . add Handler ( handler ) logging . root . set Level ( logconf [ 'level' ] . upper ( ) ) logger . info ( 'Log level set to %s' % logconf [ 'level' ] )
def home ( ) : # Get I Ds of existing preview images preview = config ( ) [ 'capture' ] [ 'preview' ] previewdir = config ( ) [ 'capture' ] [ 'preview dir' ] preview = [ p . replace ( '{{previewdir}}' , previewdir ) for p in preview ] preview = zip ( preview , range ( len ( preview ) ) ) preview = [ p [ 1 ] for p in preview if os . path . isfile ( p [ 0 ] ) ] # Get limits for recording table try : limit upcoming = int ( request . args . get ( 'limit upcoming' , 5 ) ) limit processed = int ( request . args . get ( 'limit processed' , 15 ) ) except Value Error : limit upcoming = 5 limit processed = 15 db = get session ( ) upcoming events = db . query ( Upcoming Event ) . order by ( Upcoming Event . start ) . limit ( limit upcoming ) recorded events = db . query ( Recorded Event ) . order by ( Recorded Event . start . desc ( ) ) . limit ( limit processed ) recording = get service status ( Service . CAPTURE ) == Service Status . BUSY uploading = get service status ( Service . INGEST ) == Service Status . BUSY processed = db . query ( Recorded Event ) . count ( ) upcoming = db . query ( Upcoming Event ) . count ( ) return render template ( 'home.html' , preview = preview , config = config ( ) , recorded events = recorded events , upcoming events = upcoming events , recording = recording , uploading = uploading , processed = processed , upcoming = upcoming , limit upcoming = limit upcoming , limit processed = limit processed , dtfmt = dtfmt )
def serve image ( image id ) : try : preview dir = config ( ) [ 'capture' ] [ 'preview dir' ] filepath = config ( ) [ 'capture' ] [ 'preview' ] [ image id ] filepath = filepath . replace ( '{{previewdir}}' , preview dir ) filepath = os . path . abspath ( filepath ) if os . path . isfile ( filepath ) : directory , filename = filepath . rsplit ( '/' , 1 ) return send from directory ( directory , filename ) except ( Index Error , Key Error ) : pass return '' , 404
def sigterm handler ( signum , frame ) : sigint handler ( signum , frame ) for process in multiprocessing . active children ( ) : process . terminate ( ) sys . exit ( 0 )
def run all ( * modules ) : processes = [ multiprocessing . Process ( target = mod . run ) for mod in modules ] for p in processes : p . start ( ) for p in processes : p . join ( )
def parse ical ( vcal ) : vcal = vcal . replace ( '\r\n ' , '' ) . replace ( '\r\n\r\n' , '\r\n' ) vevents = vcal . split ( '\r\n BEGIN:VEVENT\r\n' ) del ( vevents [ 0 ] ) events = [ ] for vevent in vevents : event = { } for line in vevent . split ( '\r\n' ) : line = line . split ( ':' , 1 ) key = line [ 0 ] . lower ( ) if len ( line ) <= 1 or key == 'end' : continue if key . startswith ( 'dt' ) : event [ key ] = unix ts ( dateutil . parser . parse ( line [ 1 ] ) ) continue if not key . startswith ( 'attach' ) : event [ key ] = line [ 1 ] continue # finally handle attachments event [ 'attach' ] = event . get ( 'attach' , [ ] ) attachment = { } for x in [ x . split ( '=' ) for x in line [ 0 ] . split ( ';' ) ] : if x [ 0 ] . lower ( ) in [ 'fmttype' , 'x-apple-filename' ] : attachment [ x [ 0 ] . lower ( ) ] = x [ 1 ] attachment [ 'data' ] = b64decode ( line [ 1 ] ) . decode ( 'utf-8' ) event [ 'attach' ] . append ( attachment ) events . append ( event ) return events
def control loop ( ) : set service status ( Service . SCHEDULE , Service Status . BUSY ) notify . notify ( 'READY=1' ) while not terminate ( ) : notify . notify ( 'WATCHDOG=1' ) # Try getting an updated schedule get schedule ( ) session = get session ( ) next event = session . query ( Upcoming Event ) . filter ( Upcoming Event . end > timestamp ( ) ) . order by ( Upcoming Event . start ) . first ( ) if next event : logger . info ( 'Next scheduled recording: %s' , datetime . fromtimestamp ( next event . start ) ) notify . notify ( 'STATUS=Next scheduled recording: %s' % datetime . fromtimestamp ( next event . start ) ) else : logger . info ( 'No scheduled recording' ) notify . notify ( 'STATUS=No scheduled recording' ) session . close ( ) next update = timestamp ( ) + config ( ) [ 'agent' ] [ 'update frequency' ] while not terminate ( ) and timestamp ( ) < next update : time . sleep ( 0.1 ) logger . info ( 'Shutting down schedule service' ) set service status ( Service . SCHEDULE , Service Status . STOPPED )
def control loop ( ) : set service status ( Service . AGENTSTATE , Service Status . BUSY ) notify . notify ( 'READY=1' ) notify . notify ( 'STATUS=Running' ) while not terminate ( ) : notify . notify ( 'WATCHDOG=1' ) update agent state ( ) next update = timestamp ( ) + config ( ) [ 'agent' ] [ 'update frequency' ] while not terminate ( ) and timestamp ( ) < next update : time . sleep ( 0.1 ) logger . info ( 'Shutting down agentstate service' ) set service status ( Service . AGENTSTATE , Service Status . STOPPED )
def make error response ( error , status = 500 ) : content = { 'errors' : [ { 'status' : status , 'title' : error } ] } return make response ( jsonify ( content ) , status )
def make data response ( data , status = 200 ) : content = { 'data' : ensurelist ( data ) } return make response ( jsonify ( content ) , status )
def internal state ( ) : data = { 'services' : { 'capture' : Service Status . str ( get service status ( Service . CAPTURE ) ) , 'ingest' : Service Status . str ( get service status ( Service . INGEST ) ) , 'schedule' : Service Status . str ( get service status ( Service . SCHEDULE ) ) , 'agentstate' : Service Status . str ( get service status ( Service . AGENTSTATE ) ) } } return make response ( jsonify ( { 'meta' : data } ) )
def events ( ) : db = get session ( ) upcoming events = db . query ( Upcoming Event ) . order by ( Upcoming Event . start ) recorded events = db . query ( Recorded Event ) . order by ( Recorded Event . start . desc ( ) ) result = [ event . serialize ( ) for event in upcoming events ] result += [ event . serialize ( ) for event in recorded events ] return make data response ( result )
def event ( uid ) : db = get session ( ) event = db . query ( Recorded Event ) . filter ( Recorded Event . uid == uid ) . first ( ) or db . query ( Upcoming Event ) . filter ( Upcoming Event . uid == uid ) . first ( ) if event : return make data response ( event . serialize ( ) ) return make error response ( 'No event with specified uid' , 404 )
def ingest ( event ) : # Update status set service status ( Service . INGEST , Service Status . BUSY ) notify . notify ( 'STATUS=Uploading' ) recording state ( event . uid , 'uploading' ) update event status ( event , Status . UPLOADING ) # Select ingest service # The ingest service to use is selected at random from the available # ingest services to ensure that not every capture agent uses the same # service at the same time service = config ( 'service-ingest' ) service = service [ randrange ( 0 , len ( service ) ) ] logger . info ( 'Selecting ingest service to use: ' + service ) # create mediapackage logger . info ( 'Creating new mediapackage' ) mediapackage = http request ( service + '/create Media Package' ) # extract workflow def, workflow config and add DC catalogs prop = 'org.opencastproject.capture.agent.properties' dcns = 'http://www.opencastproject.org/xsd/1.0/dublincore/' for attachment in event . get data ( ) . get ( 'attach' ) : data = attachment . get ( 'data' ) if attachment . get ( 'x-apple-filename' ) == prop : workflow def , workflow config = get config params ( data ) # Check for dublincore catalogs elif attachment . get ( 'fmttype' ) == 'application/xml' and dcns in data : name = attachment . get ( 'x-apple-filename' , '' ) . rsplit ( '.' , 1 ) [ 0 ] logger . info ( 'Adding %s DC catalog' % name ) fields = [ ( 'media Package' , mediapackage ) , ( 'flavor' , 'dublincore/%s' % name ) , ( 'dublin Core' , data . encode ( 'utf-8' ) ) ] mediapackage = http request ( service + '/add DC Catalog' , fields ) # add track for ( flavor , track ) in event . get tracks ( ) : logger . info ( 'Adding track ({0} -> {1})' . format ( flavor , track ) ) track = track . encode ( 'ascii' , 'ignore' ) fields = [ ( 'media Package' , mediapackage ) , ( 'flavor' , flavor ) , ( 'BODY1' , ( pycurl . FORM FILE , track ) ) ] mediapackage = http request ( service + '/add Track' , fields ) # ingest logger . info ( 'Ingest recording' ) fields = [ ( 'media Package' , mediapackage ) ] if workflow def : fields . append ( ( 'workflow Definition Id' , workflow def ) ) if event . uid : fields . append ( ( 'workflow Instance Id' , event . uid . encode ( 'ascii' , 'ignore' ) ) ) fields += workflow config mediapackage = http request ( service + '/ingest' , fields ) # Update status recording state ( event . uid , 'upload finished' ) update event status ( event , Status . FINISHED UPLOADING ) notify . notify ( 'STATUS=Running' ) set service status immediate ( Service . INGEST , Service Status . IDLE ) logger . info ( 'Finished ingest' )
def sigterm handler ( signum , frame ) : if captureproc and captureproc . poll ( ) is None : captureproc . terminate ( ) terminate ( True ) sys . exit ( 0 )
def render to fragment ( self , request , * * kwargs ) : fragment = Fragment ( TEST HTML ) fragment . add javascript ( TEST JS ) fragment . add css ( TEST CSS ) return fragment
def resources ( self ) : seen = set ( ) # seen.add always returns None, so 'not seen.add(x)' is always True, # but will only be called if the value is not already in seen (because # 'and' short-circuits) return [ x for x in self . resources if x not in seen and not seen . add ( x ) ]
def to dict ( self ) : return { 'content' : self . content , 'resources' : [ r . asdict ( ) for r in self . resources ] , # pylint: disable=W0212 'js init fn' : self . js init fn , 'js init version' : self . js init version , 'json init args' : self . json init args }
def from dict ( cls , pods ) : frag = cls ( ) frag . content = pods [ 'content' ] frag . resources = [ Fragment Resource ( * * d ) for d in pods [ 'resources' ] ] # pylint: disable=protected-access frag . js init fn = pods [ 'js init fn' ] frag . js init version = pods [ 'js init version' ] frag . json init args = pods [ 'json init args' ] return frag
def resource to html ( resource ) : if resource . mimetype == "text/css" : if resource . kind == "text" : return u"<style type='text/css'>\n%s\n</style>" % resource . data elif resource . kind == "url" : return u"<link rel='stylesheet' href='%s' type='text/css'>" % resource . data else : raise Exception ( "Unrecognized resource kind %r" % resource . kind ) elif resource . mimetype == "application/javascript" : if resource . kind == "text" : return u"<script>\n%s\n</script>" % resource . data elif resource . kind == "url" : return u"<script src='%s' type='application/javascript'></script>" % resource . data else : raise Exception ( "Unrecognized resource kind %r" % resource . kind ) elif resource . mimetype == "text/html" : assert resource . kind == "text" return resource . data else : raise Exception ( "Unrecognized mimetype %r" % resource . mimetype )
def get ( self , request , * args , * * kwargs ) : fragment = self . render to fragment ( request , * * kwargs ) response format = request . GET . get ( 'format' ) or request . POST . get ( 'format' ) or 'html' if response format == 'json' or WEB FRAGMENT RESPONSE TYPE in request . META . get ( 'HTTP ACCEPT' , 'text/html' ) : return Json Response ( fragment . to dict ( ) ) else : return self . render standalone response ( request , fragment , * * kwargs )
def render standalone response ( self , request , fragment , * * kwargs ) : # pylint: disable=unused-argument if fragment is None : return Http Response ( status = 204 ) html = self . render to standalone html ( request , fragment , * * kwargs ) return Http Response ( html )
def render to standalone html ( self , request , fragment , * * kwargs ) : # pylint: disable=unused-argument template = get template ( STANDALONE TEMPLATE NAME ) context = { 'head html' : fragment . head html ( ) , 'body html' : fragment . body html ( ) , 'foot html' : fragment . foot html ( ) , } return template . render ( context )
def calc ( pvalues , lamb ) : m = len ( pvalues ) pi0 = ( pvalues > lamb ) . sum ( ) / ( ( 1 - lamb ) * m ) p FDR = np . ones ( m ) print ( "p FDR    y        Pr     fast Pow" ) for i in range ( m ) : y = pvalues [ i ] Pr = max ( 1 , m - i ) / float ( m ) p FDR [ i ] = ( pi0 * y ) / ( Pr * ( 1 - math . pow ( 1 - y , m ) ) ) print ( i , p FDR [ i ] , y , Pr , 1.0 - math . pow ( 1 - y , m ) ) num null = pi0 * m num alt = m - num null num negs = np . array ( range ( m ) ) num pos = m - num negs pp = num pos / float ( m ) qvalues = np . ones ( m ) qvalues [ 0 ] = p FDR [ 0 ] for i in range ( m - 1 ) : qvalues [ i + 1 ] = min ( qvalues [ i ] , p FDR [ i + 1 ] ) sens = ( ( 1.0 - qvalues ) * num pos ) / num alt sens [ sens > 1.0 ] = 1.0 df = pd . Data Frame ( dict ( pvalue = pvalues , qvalue = qvalues , FDR = p FDR , percentile positive = pp , sens = sens ) ) df [ "svalue" ] = df . sens [ : : - 1 ] . cummax ( ) [ : : - 1 ] return df , num null , m
def to one dim array ( values , as type = None ) : if isinstance ( values , ( list , tuple ) ) : values = np . array ( values , dtype = np . float32 ) elif isinstance ( values , pd . Series ) : values = values . values values = values . flatten ( ) assert values . ndim == 1 , "values has wrong dimension" if as type is not None : return values . astype ( as type ) return values
def lookup values from error table ( scores , err df ) : ix = find nearest matches ( np . float32 ( err df . cutoff . values ) , np . float32 ( scores ) ) return err df . pvalue . iloc [ ix ] . values , err df . svalue . iloc [ ix ] . values , err df . pep . iloc [ ix ] . values , err df . qvalue . iloc [ ix ] . values
def summary err table ( df , qvalues = [ 0 , 0.01 , 0.02 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ] ) : qvalues = to one dim array ( qvalues ) # find best matching fows in df for given qvalues: ix = find nearest matches ( np . float32 ( df . qvalue . values ) , qvalues ) # extract sub table df sub = df . iloc [ ix ] . copy ( ) # remove duplicate hits, mark them with None / NAN: for i sub , ( i0 , i1 ) in enumerate ( zip ( ix , ix [ 1 : ] ) ) : if i1 == i0 : df sub . iloc [ i sub + 1 , : ] = None # attach q values column df sub . qvalue = qvalues # remove old index from original df: df sub . reset index ( inplace = True , drop = True ) return df sub [ [ 'qvalue' , 'pvalue' , 'svalue' , 'pep' , 'fdr' , 'fnr' , 'fpr' , 'tp' , 'tn' , 'fp' , 'fn' , 'cutoff' ] ]
def error statistics ( target scores , decoy scores , parametric , pfdr , pi0 lambda , pi0 method = "smoother" , pi0 smooth df = 3 , pi0 smooth log pi0 = False , compute lfdr = False , lfdr trunc = True , lfdr monotone = True , lfdr transf = "probit" , lfdr adj = 1.5 , lfdr eps = np . power ( 10.0 , - 8 ) ) : target scores = to one dim array ( target scores ) target scores = np . sort ( target scores [ ~ np . isnan ( target scores ) ] ) decoy scores = to one dim array ( decoy scores ) decoy scores = np . sort ( decoy scores [ ~ np . isnan ( decoy scores ) ] ) # compute p-values using decoy scores if parametric : # parametric target pvalues = pnorm ( target scores , decoy scores ) else : # non-parametric target pvalues = pemp ( target scores , decoy scores ) # estimate pi0 pi0 = pi0est ( target pvalues , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 ) # compute q-value target qvalues = qvalue ( target pvalues , pi0 [ 'pi0' ] , pfdr ) # compute other metrics metrics = stat metrics ( target pvalues , pi0 [ 'pi0' ] , pfdr ) # generate main statistics table error stat = pd . Data Frame ( { 'cutoff' : target scores , 'pvalue' : target pvalues , 'qvalue' : target qvalues , 'svalue' : metrics [ 'svalue' ] , 'tp' : metrics [ 'tp' ] , 'fp' : metrics [ 'fp' ] , 'tn' : metrics [ 'tn' ] , 'fn' : metrics [ 'fn' ] , 'fpr' : metrics [ 'fpr' ] , 'fdr' : metrics [ 'fdr' ] , 'fnr' : metrics [ 'fnr' ] } ) # compute lfdr / PEP if compute lfdr : error stat [ 'pep' ] = lfdr ( target pvalues , pi0 [ 'pi0' ] , lfdr trunc , lfdr monotone , lfdr transf , lfdr adj , lfdr eps ) return error stat , pi0
def find cutoff ( tt scores , td scores , cutoff fdr , parametric , pfdr , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 ) : error stat , pi0 = error statistics ( tt scores , td scores , parametric , pfdr , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 , False ) if not len ( error stat ) : raise click . Click Exception ( "Too little data for calculating error statistcs." ) i0 = ( error stat . qvalue - cutoff fdr ) . abs ( ) . idxmin ( ) cutoff = error stat . iloc [ i0 ] [ "cutoff" ] return cutoff
def score ( infile , outfile , classifier , xgb autotune , apply weights , xeval fraction , xeval num iter , ss initial fdr , ss iteration fdr , ss num iter , ss main score , group id , parametric , pfdr , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 , lfdr truncate , lfdr monotone , lfdr transformation , lfdr adj , lfdr eps , level , ipf max peakgroup rank , ipf max peakgroup pep , ipf max transition isotope overlap , ipf min transition sn , tric chromprob , threads , test ) : if outfile is None : outfile = infile else : outfile = outfile # Prepare XG Boost-specific parameters xgb hyperparams = { 'autotune' : xgb autotune , 'autotune num rounds' : 10 , 'num boost round' : 100 , 'early stopping rounds' : 10 , 'test size' : 0.33 } xgb params = { 'eta' : 0.3 , 'gamma' : 0 , 'max depth' : 6 , 'min child weight' : 1 , 'subsample' : 1 , 'colsample bytree' : 1 , 'colsample bylevel' : 1 , 'colsample bynode' : 1 , 'lambda' : 1 , 'alpha' : 0 , 'scale pos weight' : 1 , 'silent' : 1 , 'objective' : 'binary:logitraw' , 'nthread' : 1 , 'eval metric' : 'auc' } xgb params space = { 'eta' : hp . uniform ( 'eta' , 0.0 , 0.3 ) , 'gamma' : hp . uniform ( 'gamma' , 0.0 , 0.5 ) , 'max depth' : hp . quniform ( 'max depth' , 2 , 8 , 1 ) , 'min child weight' : hp . quniform ( 'min child weight' , 1 , 5 , 1 ) , 'subsample' : 1 , 'colsample bytree' : 1 , 'colsample bylevel' : 1 , 'colsample bynode' : 1 , 'lambda' : hp . uniform ( 'lambda' , 0.0 , 1.0 ) , 'alpha' : hp . uniform ( 'alpha' , 0.0 , 1.0 ) , 'scale pos weight' : 1.0 , 'silent' : 1 , 'objective' : 'binary:logitraw' , 'nthread' : 1 , 'eval metric' : 'auc' } if not apply weights : Py Prophet Learner ( infile , outfile , classifier , xgb hyperparams , xgb params , xgb params space , xeval fraction , xeval num iter , ss initial fdr , ss iteration fdr , ss num iter , ss main score , group id , parametric , pfdr , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 , lfdr truncate , lfdr monotone , lfdr transformation , lfdr adj , lfdr eps , level , ipf max peakgroup rank , ipf max peakgroup pep , ipf max transition isotope overlap , ipf min transition sn , tric chromprob , threads , test ) . run ( ) else : Py Prophet Weight Applier ( infile , outfile , classifier , xgb hyperparams , xgb params , xgb params space , xeval fraction , xeval num iter , ss initial fdr , ss iteration fdr , ss num iter , ss main score , group id , parametric , pfdr , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 , lfdr truncate , lfdr monotone , lfdr transformation , lfdr adj , lfdr eps , level , ipf max peakgroup rank , ipf max peakgroup pep , ipf max transition isotope overlap , ipf min transition sn , tric chromprob , threads , test , apply weights ) . run ( )
def ipf ( infile , outfile , ipf ms1 scoring , ipf ms2 scoring , ipf h0 , ipf grouped fdr , ipf max precursor pep , ipf max peakgroup pep , ipf max precursor peakgroup pep , ipf max transition pep ) : if outfile is None : outfile = infile else : outfile = outfile infer peptidoforms ( infile , outfile , ipf ms1 scoring , ipf ms2 scoring , ipf h0 , ipf grouped fdr , ipf max precursor pep , ipf max peakgroup pep , ipf max precursor peakgroup pep , ipf max transition pep )
def peptide ( infile , outfile , context , parametric , pfdr , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 , lfdr truncate , lfdr monotone , lfdr transformation , lfdr adj , lfdr eps ) : if outfile is None : outfile = infile else : outfile = outfile infer peptides ( infile , outfile , context , parametric , pfdr , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 , lfdr truncate , lfdr monotone , lfdr transformation , lfdr adj , lfdr eps )
def protein ( infile , outfile , context , parametric , pfdr , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 , lfdr truncate , lfdr monotone , lfdr transformation , lfdr adj , lfdr eps ) : if outfile is None : outfile = infile else : outfile = outfile infer proteins ( infile , outfile , context , parametric , pfdr , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 , lfdr truncate , lfdr monotone , lfdr transformation , lfdr adj , lfdr eps )
def subsample ( infile , outfile , subsample ratio , test ) : if outfile is None : outfile = infile else : outfile = outfile subsample osw ( infile , outfile , subsample ratio , test )
def reduce ( infile , outfile ) : if outfile is None : outfile = infile else : outfile = outfile reduce osw ( infile , outfile )
def backpropagate ( infile , outfile , apply scores ) : if outfile is None : outfile = infile else : outfile = outfile backpropagate oswr ( infile , outfile , apply scores )
def create group ( self , group ) : self . valid group id ( group . id ) body = { "data" : group . json data ( ) } url = "{}/group/{}" . format ( self . API , group . name ) data = self . put resource ( url , headers = { } , body = body ) return self . group from json ( data . get ( "data" ) )
def delete group ( self , group id ) : self . valid group id ( group id ) url = "{}/group/{}" . format ( self . API , group id ) self . delete resource ( url ) return True
def is effective member ( self , group id , netid ) : self . valid group id ( group id ) # GWS doesn't accept EPP Ns on effective member checks, for UW users netid = re . sub ( '@washington.edu' , '' , netid ) url = "{}/group/{}/effective member/{}" . format ( self . API , group id , netid ) try : data = self . get resource ( url ) return True # 200 except Data Failure Exception as ex : if ex . status == 404 : return False else : raise
def parse version ( ) : from os . path import dirname , join import ast modname = setupkw [ 'name' ] init fpath = join ( dirname ( file ) , modname , ' init .py' ) with open ( init fpath ) as file : sourcecode = file . read ( ) pt = ast . parse ( sourcecode ) class Version Visitor ( ast . Node Visitor ) : def visit Assign ( self , node ) : for target in node . targets : if target . id == ' version ' : self . version = node . value . s visitor = Version Visitor ( ) visitor . visit ( pt ) return visitor . version
def create container ( context , path , l mtime , size ) : new context = context . copy ( ) new context . input = None new context . headers = None new context . query = None container = path . split ( '/' , 1 ) [ 0 ] + ' segments' cli put container ( new context , container ) prefix = container + '/' + path . split ( '/' , 1 ) [ 1 ] prefix = '%s/%s/%s/' % ( prefix , l mtime , size ) return prefix
def is empty ( self ) : something = self . read ( 1 ) if something : if self . buf : self . buf = something + self . buf else : self . buf = something return False else : return True
def forwards ( self , orm ) : # Note: Remember to use orm['appname.Model Name'] rather than "from appname.models..." for title in orm [ 'hero slider.Slider Item Title' ] . objects . all ( ) : title . is published = True title . save ( )
def get slider items ( context , amount = None ) : req = context . get ( 'request' ) qs = Slider Item . objects . published ( req ) . order by ( 'position' ) if amount : qs = qs [ : amount ] return qs
def render hero slider ( context ) : req = context . get ( 'request' ) qs = Slider Item . objects . published ( req ) . order by ( 'position' ) return { 'slider items' : qs , }
def reader acquire ( self ) : self . order mutex . acquire ( ) self . readers mutex . acquire ( ) if self . readers == 0 : self . access mutex . acquire ( ) self . readers += 1 self . order mutex . release ( ) self . readers mutex . release ( )
def reader release ( self ) : self . readers mutex . acquire ( ) self . readers -= 1 if self . readers == 0 : self . access mutex . release ( ) self . readers mutex . release ( )
def writer acquire ( self ) : self . order mutex . acquire ( ) self . access mutex . acquire ( ) self . order mutex . release ( )
def tasks ( self ) : self . rwlock . reader acquire ( ) tl = [ v for v in self . tasks . values ( ) ] tl . sort ( key = lambda x : x . task id ) self . rwlock . reader release ( ) return tl
def to dict ( self ) : properties = find class properties ( self . class ) config = { name : self . getattribute ( name ) for name , in properties } return config
def create index ( idx url , clean = False ) : try : r = requests . get ( idx url ) except requests . exceptions . Connection Error : cause = "Error connecting to Elastic Search (index: %s)" % idx url raise Elastic Search Error ( cause = cause ) if r . status code != 200 : # The index does not exist r = requests . put ( idx url ) if r . status code != 200 : logger . info ( "Can't create index %s (%s)" , idx url , r . status code ) cause = "Error creating Elastic Search index %s" % idx url raise Elastic Search Error ( cause = cause ) logger . info ( "Index %s created" , idx url ) return True elif r . status code == 200 and clean : requests . delete ( idx url ) requests . put ( idx url ) logger . info ( "Index deleted and created (index: %s)" , idx url ) return True return False
def json encoder ( * args , * * kwargs ) : obj = cherrypy . serving . request . json inner handler ( * args , * * kwargs ) for chunk in JSON Encoder ( ) . iterencode ( obj ) : yield chunk . encode ( 'utf-8' )
def items ( self ) : # Get and remove queued items in an atomic transaction pipe = self . conn . pipeline ( ) pipe . lrange ( Q STORAGE ITEMS , 0 , - 1 ) pipe . ltrim ( Q STORAGE ITEMS , 1 , 0 ) items = pipe . execute ( ) [ 0 ] for item in items : item = pickle . loads ( item ) yield item
def validate args ( task id , backend , category , backend args ) : if not task id or task id . strip ( ) == "" : msg = "Missing task id for task" raise Value Error ( msg ) if not backend or backend . strip ( ) == "" : msg = "Missing backend for task '%s'" % task id raise Value Error ( msg ) if backend args and not isinstance ( backend args , dict ) : msg = "Backend args is not a dict, task '%s'" % task id raise Value Error ( msg ) if not category or category . strip ( ) == "" : msg = "Missing category for task '%s'" % task id raise Value Error ( msg )
def parse archive args ( self , archive args ) : if not archive args : return None archiving args = copy . deepcopy ( archive args ) if self . archive path : archiving args [ 'archive path' ] = self . archive path else : archiving args [ 'archive path' ] = os . path . expanduser ( ARCHIVES DEFAULT PATH ) return Archiving Task Config . from dict ( archiving args )
def schedule job task ( self , queue id , task id , job args , delay = 0 ) : self . rwlock . writer acquire ( ) job id = self . generate job id ( task id ) event = self . scheduler . enter ( delay , 1 , self . enqueue job , argument = ( queue id , job id , job args , ) ) self . jobs [ job id ] = event self . tasks [ task id ] = job id self . rwlock . writer release ( ) logging . debug ( "Job #%s (task: %s) scheduled on %s (wait: %s)" , job id , task id , queue id , delay ) return job id
def cancel job task ( self , task id ) : try : self . rwlock . writer acquire ( ) job id = self . tasks . get ( task id , None ) if job id : self . cancel job ( job id ) else : logger . warning ( "Task %s set to be removed was not found" , task id ) finally : self . rwlock . writer release ( )
def run ( self ) : try : self . listen ( ) except Exception as e : logger . critical ( "Job Listener instence crashed. Error: %s" , str ( e ) ) logger . critical ( traceback . format exc ( ) )
def listen ( self ) : pubsub = self . conn . pubsub ( ) pubsub . subscribe ( self . pubsub channel ) logger . debug ( "Listening on channel %s" , self . pubsub channel ) for msg in pubsub . listen ( ) : logger . debug ( "New message received of type %s" , str ( msg [ 'type' ] ) ) if msg [ 'type' ] != 'message' : logger . debug ( "Ignoring job message" ) continue data = pickle . loads ( msg [ 'data' ] ) job id = data [ 'job id' ] job = rq . job . Job . fetch ( job id , connection = self . conn ) if data [ 'status' ] == 'finished' : logging . debug ( "Job #%s completed" , job id ) handler = self . result handler elif data [ 'status' ] == 'failed' : logging . debug ( "Job #%s failed" , job id ) handler = self . result handler err else : continue if handler : logging . debug ( "Calling handler for job #%s" , job id ) handler ( job )
def schedule ( self ) : if self . async mode : self . scheduler . start ( ) self . listener . start ( ) else : self . scheduler . schedule ( )
def build job arguments ( task ) : job args = { } job args [ 'qitems' ] = Q STORAGE ITEMS job args [ 'task id' ] = task . task id # Backend parameters job args [ 'backend' ] = task . backend backend args = copy . deepcopy ( task . backend args ) if 'next from date' in backend args : backend args [ 'from date' ] = backend args . pop ( 'next from date' ) if 'next offset' in backend args : backend args [ 'offset' ] = backend args . pop ( 'next offset' ) job args [ 'backend args' ] = backend args # Category job args [ 'category' ] = task . category # Archiving parameters archiving cfg = task . archiving cfg job args [ 'archive args' ] = archiving cfg . to dict ( ) if archiving cfg else None # Scheduler parameters sched cfg = task . scheduling cfg job args [ 'max retries' ] = sched cfg . max retries if sched cfg else MAX JOB RETRIES return job args
def reverse action ( self , url name , * args , * * kwargs ) : if self . request and not self . request . version : return reverse ( self . get url name ( url name ) , * args , * * kwargs ) return super ( ) . reverse action ( url name , * args , * * kwargs )
def get version ( version = None ) : if version is None : version = VERSION assert len ( version ) == 5 assert version [ 3 ] in ( "alpha" , "beta" , "rc" , "final" ) # Now build the two parts of the version number: # main = X.Y[.Z] # sub = .dev N - for pre-alpha releases #     | {a|b|c}N - for alpha, beta and rc releases parts = 2 if version [ 2 ] == 0 else 3 main = "." . join ( str ( x ) for x in version [ : parts ] ) sub = "" if version [ 3 ] != "final" : mapping = { "alpha" : "a" , "beta" : "b" , "rc" : "c" } sub = mapping [ version [ 3 ] ] + str ( version [ 4 ] ) return main + sub
def create ( self , request ) : # TODO: Decorate api with sensitive post parameters as Django admin do? # from django.utils.decorators import method decorator # from django.views.decorators.debug import sensitive post parameters # sensitive post parameters m = method decorator(sensitive post parameters()) login form = Authentication Form ( request , data = request . data ) if not login form . is valid ( ) : raise serializers . Validation Error ( login form . errors ) auth login ( request , login form . get user ( ) ) serializer = User Serializer ( request . user ) return Response ( serializer . data , status = status . HTTP 200 OK )
def list ( self , request ) : serializer = self . get serializer ( request . user ) return Response ( serializer . data , status = status . HTTP 200 OK )
def create ( self , request ) : # TODO: Decorate api with sensitive post parameters as Django admin do? password form = Password Change Form ( request . user , data = request . data ) if not password form . is valid ( ) : raise serializers . Validation Error ( password form . errors ) password form . save ( ) update session auth hash ( request , password form . user ) return Response ( status = status . HTTP 204 NO CONTENT )
def create field ( field info ) : field type = field info . get ( 'type' ) if field type not in FIELDS NAME MAP : raise Value Error ( ( 'not support this field: {}' ) . format ( field type ) ) field class = FIELDS NAME MAP . get ( field type ) params = dict ( field info ) params . pop ( 'type' ) return field class . from dict ( params )
def change logging kwargs ( kwargs ) : log levels = kwargs . pop ( 'log level' , None ) log folder = kwargs . pop ( 'log folder' , 'logs' ) logger names = kwargs . pop ( 'logger names' , '' ) if log levels is None : log levels = kwargs . pop ( 'log levels' , logging . INFO ) log multiproc = kwargs . pop ( 'log multiproc' , True ) if not isinstance ( logger names , ( tuple , list ) ) : logger names = [ logger names ] if not isinstance ( log levels , ( tuple , list ) ) : log levels = [ log levels ] if len ( log levels ) == 1 : log levels = [ log levels [ 0 ] for in logger names ] # We don't want to manipulate the original dictionary dictionary = copy . deepcopy ( LOGGING DICT ) prefixes = [ '' ] if not log multiproc : for key in list ( dictionary . keys ( ) ) : if key . startswith ( 'multiproc ' ) : del dictionary [ key ] else : prefixes . append ( 'multiproc ' ) # Add all handlers to all loggers for prefix in prefixes : for handler dict in dictionary [ prefix + 'handlers' ] . values ( ) : if 'filename' in handler dict : filename = os . path . join ( log folder , handler dict [ 'filename' ] ) filename = os . path . normpath ( filename ) handler dict [ 'filename' ] = filename dictionary [ prefix + 'loggers' ] = { } logger dict = dictionary [ prefix + 'loggers' ] for idx , logger name in enumerate ( logger names ) : logger dict [ logger name ] = { 'level' : log levels [ idx ] , 'handlers' : list ( dictionary [ prefix + 'handlers' ] . keys ( ) ) } kwargs [ 'log config' ] = dictionary
def get strings ( args ) : string list = [ ] for elem in ast . walk ( ast . parse ( args ) ) : if isinstance ( elem , ast . Str ) : string list . append ( elem . s ) return string list
def extract replacements ( self , trajectory ) : self . env name = trajectory . v environment name self . traj name = trajectory . v name self . set name = trajectory . f wildcard ( '$set' ) self . run name = trajectory . f wildcard ( '$' )
def parser to string io ( parser ) : memory file = String IO ( ) parser . write ( memory file ) memory file . flush ( ) memory file . seek ( 0 ) return memory file
def make logging handlers and tools ( self , multiproc = False ) : log stdout = self . log stdout if sys . stdout is self . stdout to logger : # If we already redirected stdout we don't neet to redo it again log stdout = False if self . log config : if multiproc : proc log config = self . mp config else : proc log config = self . sp config if proc log config : if isinstance ( proc log config , dict ) : new dict = self . handle dict config ( proc log config ) dict Config ( new dict ) else : parser = self . handle config parsing ( proc log config ) memory file = self . parser to string io ( parser ) file Config ( memory file , disable existing loggers = False ) if log stdout : #  Create a logging mock for stdout std name , std level = self . log stdout stdout = Stdout To Logger ( std name , log level = std level ) stdout . start ( ) self . tools . append ( stdout )
def finalize ( self , remove all handlers = True ) : for tool in self . tools : tool . finalize ( ) self . tools = [ ] self . stdout to logger = None for config in ( self . sp config , self . mp config ) : if hasattr ( config , 'close' ) : config . close ( ) self . sp config = None self . mp config = None if remove all handlers : self . tabula rasa ( )
def start ( self ) : if sys . stdout is not self : self . original steam = sys . stdout sys . stdout = self self . redirection = True if self . redirection : print ( 'Established redirection of `stdout`.' )
def write ( self , buf ) : if not self . recursion : self . recursion = True try : for line in buf . rstrip ( ) . splitlines ( ) : self . logger . log ( self . log level , line . rstrip ( ) ) finally : self . recursion = False else : # If stderr is redirected to stdout we can avoid further recursion by sys . stderr . write ( 'ERROR: Recursion in Stream redirection!' )
def get all attributes ( instance ) : try : result dict = instance . dict . copy ( ) except Attribute Error : result dict = { } if hasattr ( instance , ' all slots ' ) : all slots = instance . all slots else : all slots = slots . get all slots ( instance . class ) for slot in all slots : result dict [ slot ] = getattr ( instance , slot ) result dict . pop ( ' dict ' , None ) result dict . pop ( ' weakref ' , None ) return result dict
def kwargs mutual exclusive ( param1 name , param2 name , map2to1 = None ) : def wrapper ( func ) : @ functools . wraps ( func ) def new func ( * args , * * kwargs ) : if param2 name in kwargs : if param1 name in kwargs : raise Value Error ( 'You cannot specify `%s` and `%s` at the same time, ' 'they are mutually exclusive.' % ( param1 name , param2 name ) ) param2 = kwargs . pop ( param2 name ) if map2to1 is not None : param1 = map2to1 ( param2 ) else : param1 = param2 kwargs [ param1 name ] = param1 return func ( * args , * * kwargs ) return new func return wrapper
def not in run ( func ) : doc = func . doc na string = '''\n ATTENTION: This function is not available during a single run!\n''' if doc is not None : func . doc = '\n' . join ( [ doc , na string ] ) func . not in run = True @ functools . wraps ( func ) def new func ( self , * args , * * kwargs ) : if self . is run : raise Type Error ( 'Function `%s` is not available during a single run.' % func . name ) return func ( self , * args , * * kwargs ) return new func
def with open store ( func ) : doc = func . doc na string = '''\n ATTENTION: This function can only be used if the store is open!\n''' if doc is not None : func . doc = '\n' . join ( [ doc , na string ] ) func . with open store = True @ functools . wraps ( func ) def new func ( self , * args , * * kwargs ) : if not self . traj . v storage service . is open : raise Type Error ( 'Function `%s` is only available if the storage is open.' % func . name ) return func ( self , * args , * * kwargs ) return new func
def prefix naming ( cls ) : if hasattr ( cls , ' getattr ' ) : raise Type Error ( ' getattr  already defined' ) cls . getattr = prfx getattr cls . setattr = prfx setattr return cls
def add params ( traj ) : # We set the Brian Parameter to be the standard parameter traj . v standard parameter = Brian2Parameter traj . v fast access = True # Add parameters we need for our network traj . f add parameter ( 'Net.C' , 281 * p F ) traj . f add parameter ( 'Net.g L' , 30 * n S ) traj . f add parameter ( 'Net.EL' , - 70.6 * m V ) traj . f add parameter ( 'Net.VT' , - 50.4 * m V ) traj . f add parameter ( 'Net.Delta T' , 2 * m V ) traj . f add parameter ( 'Net.tauw' , 40 * ms ) traj . f add parameter ( 'Net.a' , 4 * n S ) traj . f add parameter ( 'Net.b' , 0.08 * n A ) traj . f add parameter ( 'Net.I' , .8 * n A ) traj . f add parameter ( 'Net.Vcut' , 'vm > 0*m V' ) # practical threshold condition traj . f add parameter ( 'Net.N' , 50 ) eqs = traj . f add parameter ( 'Net.eqs' , eqs ) traj . f add parameter ( 'reset' , 'vm=Vr;w+=b' )
def run net ( traj ) : eqs = traj . eqs # Create a namespace dictionairy namespace = traj . Net . f to dict ( short names = True , fast access = True ) # Create the Neuron Group neuron = Neuron Group ( traj . N , model = eqs , threshold = traj . Vcut , reset = traj . reset , namespace = namespace ) neuron . vm = traj . EL neuron . w = traj . a * ( neuron . vm - traj . EL ) neuron . Vr = linspace ( - 48.3 * m V , - 47.7 * m V , traj . N ) # bifurcation parameter # Run the network initially for 100 milliseconds print ( 'Initial Run' ) net = Network ( neuron ) net . run ( 100 * ms , report = 'text' ) # we discard the first spikes # Create a Spike Monitor M Spike = Spike Monitor ( neuron ) net . add ( M Spike ) # Create a State Monitor for the membrane voltage, record from neurons 1-3 M State V = State Monitor ( neuron , variables = [ 'vm' ] , record = [ 1 , 2 , 3 ] ) net . add ( M State V ) # Now record for 500 milliseconds print ( 'Measurement run' ) net . run ( 500 * ms , report = 'text' ) # Add the BRAIN monitors traj . v standard result = Brian2Monitor Result traj . f add result ( 'Spike Monitor' , M Spike ) traj . f add result ( 'State Monitor V' , M State V )
def add parameters ( traj ) : traj . f add parameter ( 'steps' , 10000 , comment = 'Number of time steps to simulate' ) traj . f add parameter ( 'dt' , 0.01 , comment = 'Step size' ) # Here we want to add the initial conditions as an array parameter. We will simulate # a 3-D differential equation, the Lorenz attractor. traj . f add parameter ( Array Parameter , 'initial conditions' , np . array ( [ 0.0 , 0.0 , 0.0 ] ) , comment = 'Our initial conditions, as default we will start from' ' origin!' ) # We will group all parameters of the Lorenz differential equation into the group 'func params' traj . f add parameter ( 'func params.sigma' , 10.0 ) traj . f add parameter ( 'func params.beta' , 8.0 / 3.0 ) traj . f add parameter ( 'func params.rho' , 28.0 ) #For the fun of it we will annotate the  group traj . func params . v annotations . info = 'This group contains as default the original values chosen ' 'by Edward Lorenz in 1963. Check it out on wikipedia ' '(https://en.wikipedia.org/wiki/Lorenz attractor)!'
def create storage ( storage service , trajectory = None , * * kwargs ) : kwargs copy = kwargs . copy ( ) kwargs copy [ 'trajectory' ] = trajectory matching kwargs = get matching kwargs ( storage service , kwargs copy ) storage service = storage service ( * * matching kwargs ) unused kwargs = set ( kwargs . keys ( ) ) - set ( matching kwargs . keys ( ) ) return storage service , unused kwargs
def add parameters ( traj ) : assert ( isinstance ( traj , Trajectory ) ) scale = traj . simulation . scale traj . v standard parameter = Brian2Parameter model eqs = conn eqs = traj . f add parameter ( 'model.eqs' , model eqs , comment = 'The differential equation for the neuron model' ) traj . f add parameter ( 'model.synaptic.eqs' , conn eqs , comment = 'The differential equation for the synapses. ' 'PRE will be replaced by `i` or `e` depending ' 'on the source population' ) traj . f add parameter ( 'model.synaptic.tau1' , 1 * ms , comment = 'The decay time' ) traj . f add parameter ( 'model.synaptic.tau2 e' , 3 * ms , comment = 'The rise time, excitatory' ) traj . f add parameter ( 'model.synaptic.tau2 i' , 2 * ms , comment = 'The rise time, inhibitory' ) traj . f add parameter ( 'model.V th' , 'V >= 1.0' , comment = "Threshold value" ) traj . f add parameter ( 'model.reset func' , 'V=0.0' , comment = "String representation of reset function" ) traj . f add parameter ( 'model.refractory' , 5 * ms , comment = "Absolute refractory period" ) traj . f add parameter ( 'model.N e' , int ( 2000 * scale ) , comment = "Amount of excitatory neurons" ) traj . f add parameter ( 'model.N i' , int ( 500 * scale ) , comment = "Amount of inhibitory neurons" ) traj . f add parameter ( 'model.tau e' , 15 * ms , comment = "Membrane time constant, excitatory" ) traj . f add parameter ( 'model.tau i' , 10 * ms , comment = "Membrane time constant, inhibitory" ) traj . f add parameter ( 'model.mu e min' , 1.1 , comment = "Lower bound for bias, excitatory" ) traj . f add parameter ( 'model.mu e max' , 1.2 , comment = "Upper bound for bias, excitatory" ) traj . f add parameter ( 'model.mu i min' , 1.0 , comment = "Lower bound for bias, inhibitory" ) traj . f add parameter ( 'model.mu i max' , 1.05 , comment = "Upper bound for bias, inhibitory" )
def add parameters ( traj ) : assert ( isinstance ( traj , Trajectory ) ) traj . v standard parameter = Brian2Parameter scale = traj . simulation . scale traj . f add parameter ( 'connections.R ee' , 1.0 , comment = 'Scaling factor for clustering' ) traj . f add parameter ( 'connections.clustersize e' , 100 , comment = 'Size of a cluster' ) traj . f add parameter ( 'connections.strength factor' , 2.5 , comment = 'Factor for scaling cluster weights' ) traj . f add parameter ( 'connections.p ii' , 0.25 , comment = 'Connection probability from inhibitory to inhibitory' ) traj . f add parameter ( 'connections.p ei' , 0.25 , comment = 'Connection probability from inhibitory to excitatory' ) traj . f add parameter ( 'connections.p ie' , 0.25 , comment = 'Connection probability from excitatory to inhibitory' ) traj . f add parameter ( 'connections.p ee' , 0.1 , comment = 'Connection probability from excitatory to excitatory' ) traj . f add parameter ( 'connections.J ii' , 0.027 / np . sqrt ( scale ) , comment = 'Connection strength from inhibitory to inhibitory' ) traj . f add parameter ( 'connections.J ei' , 0.032 / np . sqrt ( scale ) , comment = 'Connection strength from inhibitory to excitatroy' ) traj . f add parameter ( 'connections.J ie' , 0.009 / np . sqrt ( scale ) , comment = 'Connection strength from excitatory to inhibitory' ) traj . f add parameter ( 'connections.J ee' , 0.012 / np . sqrt ( scale ) , comment = 'Connection strength from excitatory to excitatory' )
def add parameters ( self , traj ) : par = traj . f add parameter ( Brian2Parameter , 'simulation.durations.initial run' , 500 * ms , comment = 'Initialisation run for more realistic ' 'measurement conditions.' ) par . v annotations . order = 0 par = traj . f add parameter ( Brian2Parameter , 'simulation.durations.measurement run' , 1500 * ms , comment = 'Measurement run that is considered for ' 'statistical evaluation' ) par . v annotations . order = 1
def add monitors ( self , traj , network , network dict ) : neurons e = network dict [ 'neurons e' ] monitor list = [ ] # Spiketimes self . spike monitor = Spike Monitor ( neurons e ) monitor list . append ( self . spike monitor ) # Membrane Potential self . V monitor = State Monitor ( neurons e , 'V' , record = list ( traj . neuron records ) ) monitor list . append ( self . V monitor ) # Exc. syn .Current self . I syn e monitor = State Monitor ( neurons e , 'I syn e' , record = list ( traj . neuron records ) ) monitor list . append ( self . I syn e monitor ) # Inh. syn. Current self . I syn i monitor = State Monitor ( neurons e , 'I syn i' , record = list ( traj . neuron records ) ) monitor list . append ( self . I syn i monitor ) # Add monitors to network and dictionary network . add ( * monitor list ) network dict [ 'monitors' ] = monitor list
def plot result ( self , traj , result name ) : result = traj . f get ( result name ) varname = result . record variables [ 0 ] values = result [ varname ] times = result . t record = result . record for idx , celia neuron in enumerate ( record ) : plt . subplot ( len ( record ) , 1 , idx + 1 ) plt . plot ( times , values [ idx , : ] ) if idx == 0 : plt . title ( '%s' % varname ) if idx == 1 : plt . ylabel ( '%s' % ( varname ) ) if idx == len ( record ) - 1 : plt . xlabel ( 't' )
def print graphs ( self , traj ) : print folder = self . make folder ( traj ) # If we use BRIAN's own raster plot functionality we # need to sue the Spike Monitor directly plt . figure ( ) plt . scatter ( self . spike monitor . t , self . spike monitor . i , s = 1 ) plt . xlabel ( 't' ) plt . ylabel ( 'Exc. Neurons' ) plt . title ( 'Spike Raster Plot' ) filename = os . path . join ( print folder , 'spike.png' ) print ( 'Current plot: %s ' % filename ) plt . savefig ( filename ) plt . close ( ) fig = plt . figure ( ) self . plot result ( traj , 'monitors.V' ) filename = os . path . join ( print folder , 'V.png' ) print ( 'Current plot: %s ' % filename ) fig . savefig ( filename ) plt . close ( ) plt . figure ( ) self . plot result ( traj , 'monitors.I syn e' ) filename = os . path . join ( print folder , 'I syn e.png' ) print ( 'Current plot: %s ' % filename ) plt . savefig ( filename ) plt . close ( ) plt . figure ( ) self . plot result ( traj , 'monitors.I syn i' ) filename = os . path . join ( print folder , 'I syn i.png' ) print ( 'Current plot: %s ' % filename ) plt . savefig ( filename ) plt . close ( ) if not traj . analysis . show plots : plt . close ( 'all' ) else : plt . show ( )
def get batch ( ) : optlist , args = getopt . getopt ( sys . argv [ 1 : ] , '' , longopts = 'batch=' ) batch = 0 for o , a in optlist : if o == '--batch' : batch = int ( a ) print ( 'Found batch %d' % batch ) return batch
def explore batch ( traj , batch ) : explore dict = { } explore dict [ 'sigma' ] = np . arange ( 10.0 * batch , 10.0 * ( batch + 1 ) , 1.0 ) . tolist ( ) # for batch = 0 explores sigma in [0.0, 1.0, 2.0, ..., 9.0], # for batch = 1 explores sigma in [10.0, 11.0, 12.0, ..., 19.0] # and so on traj . f explore ( explore dict )
def vars ( self ) : if self . vars is None : self . vars = NN Tree Node Vars ( self ) return self . vars
def func ( self ) : if self . func is None : self . func = NN Tree Node Func ( self ) return self . func
def rename ( self , full name ) : self . full name = full name if full name : self . name = full name . rsplit ( '.' , 1 ) [ - 1 ]
def set details ( self , depth , branch , run branch ) : self . depth = depth self . branch = branch self . run branch = run branch
def determine types ( start node , first name , add leaf , add link ) : if start node . v is root : where = first name else : where = start node . branch if where in SUBTREE MAPPING : type tuple = SUBTREE MAPPING [ where ] else : type tuple = ( GROUP , LEAF ) if add link : return type tuple [ 0 ] , LINK if add leaf : return type tuple else : return type tuple [ 0 ] , type tuple [ 0 ]
def create link ( self , act node , name , instance ) : act node . links [ name ] = instance act node . children [ name ] = instance full name = instance . v full name if full name not in self . root instance . linked by : self . root instance . linked by [ full name ] = { } linking = self . root instance . linked by [ full name ] if act node . v full name not in linking : linking [ act node . v full name ] = ( act node , set ( ) ) linking [ act node . v full name ] [ 1 ] . add ( name ) if name not in self . links count : self . links count [ name ] = 0 self . links count [ name ] = self . links count [ name ] + 1 self . logger . debug ( 'Added link `%s` under `%s` pointing ' 'to `%s`.' % ( name , act node . v full name , instance . v full name ) ) return instance
def create any group ( self , parent node , name , type name , instance = None , constructor = None , args = None , kwargs = None ) : if args is None : args = [ ] if kwargs is None : kwargs = { } full name = self . make full name ( parent node . v full name , name ) if instance is None : if constructor is None : if type name == RESULT GROUP : constructor = Result Group elif type name == PARAMETER GROUP : constructor = Parameter Group elif type name == CONFIG GROUP : constructor = Config Group elif type name == DERIVED PARAMETER GROUP : constructor = Derived Parameter Group elif type name == GROUP : constructor = NN Group Node else : raise Runtime Error ( 'You shall not pass!' ) instance = self . root instance . construct instance ( constructor , full name , * args , * * kwargs ) else : instance . rename ( full name ) # Check if someone tries to add a particular standard group to a branch where # it does not belong: if type name == RESULT GROUP : if type ( instance ) in ( NN Group Node , Parameter Group , Config Group , Derived Parameter Group ) : raise Type Error ( 'You cannot add a `%s` type of group under results' % str ( type ( instance ) ) ) elif type name == PARAMETER GROUP : if type ( instance ) in ( NN Group Node , Result Group , Config Group , Derived Parameter Group ) : raise Type Error ( 'You cannot add a `%s` type of group under parameters' % str ( type ( instance ) ) ) elif type name == CONFIG GROUP : if type ( instance ) in ( NN Group Node , Parameter Group , Result Group , Derived Parameter Group ) : raise Type Error ( 'You cannot add a `%s` type of group under config' % str ( type ( instance ) ) ) elif type name == DERIVED PARAMETER GROUP : if type ( instance ) in ( NN Group Node , Parameter Group , Config Group , Result Group ) : raise Type Error ( 'You cannot add a `%s` type of group under derived ' 'parameters' % str ( type ( instance ) ) ) elif type name == GROUP : if type ( instance ) in ( Result Group , Parameter Group , Config Group , Derived Parameter Group ) : raise Type Error ( 'You cannot add a `%s` type of group under other data' % str ( type ( instance ) ) ) else : raise Runtime Error ( 'You shall not pass!' ) self . set details tree node ( parent node , name , instance ) instance . nn interface = self self . root instance . all groups [ instance . v full name ] = instance self . add to nodes and leaves ( instance ) parent node . children [ name ] = instance parent node . groups [ name ] = instance return instance
def add group from storage ( self , args , kwargs ) : return self . nn interface . add generic ( self , type name = GROUP , group type name = GROUP , args = args , kwargs = kwargs , add prefix = False , check naming = False )
def add leaf from storage ( self , args , kwargs ) : return self . nn interface . add generic ( self , type name = LEAF , group type name = GROUP , args = args , kwargs = kwargs , add prefix = False , check naming = False )
def f dir data ( self ) : if ( self . nn interface is not None and self . nn interface . root instance is not None and self . v root . v auto load ) : try : if self . v is root : self . f load ( recursive = True , max depth = 1 , load data = pypetconstants . LOAD SKELETON , with meta data = False , with run information = False ) else : self . f load ( recursive = True , max depth = 1 , load data = pypetconstants . LOAD SKELETON ) except Exception as exc : pass return list ( self . children . keys ( ) )
def unit from expression ( expr ) : if expr == '1' : return get unit fast ( 1 ) elif isinstance ( expr , str ) : mod = ast . parse ( expr , mode = 'eval' ) expr = mod . body return unit from expression ( expr ) elif expr . class is ast . Name : return ALLUNITS [ expr . id ] elif expr . class is ast . Num : return expr . n elif expr . class is ast . Unary Op : op = expr . op . class . name operand = unit from expression ( expr . operand ) if op == 'U Sub' : return - operand else : raise Syntax Error ( "Unsupported operation " + op ) elif expr . class is ast . Bin Op : op = expr . op . class . name left = unit from expression ( expr . left ) right = unit from expression ( expr . right ) if op == 'Add' : u = left + right elif op == 'Sub' : u = left - right elif op == 'Mult' : u = left * right elif op == 'Div' : u = left / right elif op == 'Pow' : n = unit from expression ( expr . right ) u = left ** n elif op == 'Mod' : u = left % right else : raise Syntax Error ( "Unsupported operation " + op ) return u else : raise Runtime Error ( 'You shall not pass' )
def f supports ( self , data ) : if isinstance ( data , Quantity ) : return True elif super ( Brian2Parameter , self ) . f supports ( data ) : return True return False
def supports ( self , data ) : if isinstance ( data , Quantity ) : return True elif super ( Brian2Result , self ) . supports ( data ) : return True return False
def add commit variables ( traj , commit ) : git time value = time . strftime ( '%Y %m %d %Hh%Mm%Ss' , time . localtime ( commit . committed date ) ) git short name = str ( commit . hexsha [ 0 : 7 ] ) git commit name = 'commit %s ' % git short name git commit name = 'git.' + git commit name + git time value if not traj . f contains ( 'config.' + git commit name , shortcuts = False ) : git commit name += '.' # Add the hexsha traj . f add config ( git commit name + 'hexsha' , commit . hexsha , comment = 'SHA-1 hash of commit' ) # Add the description string traj . f add config ( git commit name + 'name rev' , commit . name rev , comment = 'String describing the commits hex sha based on ' 'the closest Reference' ) # Add unix epoch traj . f add config ( git commit name + 'committed date' , commit . committed date , comment = 'Date of commit as unix epoch seconds' ) # Add commit message traj . f add config ( git commit name + 'message' , str ( commit . message ) , comment = 'The commit message' )
def get argspec ( func ) : if inspect . isclass ( func ) : func = func . init if not inspect . isfunction ( func ) : # Init function not existing return [ ] , False parameters = inspect . signature ( func ) . parameters args = [ ] uses starstar = False for par in parameters . values ( ) : if ( par . kind == inspect . Parameter . POSITIONAL OR KEYWORD or par . kind == inspect . Parameter . KEYWORD ONLY ) : args . append ( par . name ) elif par . kind == inspect . Parameter . VAR KEYWORD : uses starstar = True return args , uses starstar
def get matching kwargs ( func , kwargs ) : args , uses startstar = get argspec ( func ) if uses startstar : return kwargs . copy ( ) else : matching kwargs = dict ( ( k , kwargs [ k ] ) for k in args if k in kwargs ) return matching kwargs
def format time ( timestamp ) : format string = '%Y %m %d %Hh%Mm%Ss' formatted time = datetime . datetime . fromtimestamp ( timestamp ) . strftime ( format string ) return formatted time
def port to tcp ( port = None ) : #address = 'tcp://' + socket.gethostbyname(socket.getfqdn()) domain name = socket . getfqdn ( ) try : addr list = socket . getaddrinfo ( domain name , None ) except Exception : addr list = socket . getaddrinfo ( '127.0.0.1' , None ) family , socktype , proto , canonname , sockaddr = addr list [ 0 ] host = convert ipv6 ( sockaddr [ 0 ] ) address = 'tcp://' + host if port is None : port = ( ) if not isinstance ( port , int ) : # determine port automatically context = zmq . Context ( ) try : socket = context . socket ( zmq . REP ) socket . ipv6 = is ipv6 ( address ) port = socket . bind to random port ( address , * port ) except Exception : print ( 'Could not connect to {} using {}' . format ( address , addr list ) ) pypet root logger = logging . get Logger ( 'pypet' ) pypet root logger . exception ( 'Could not connect to {}' . format ( address ) ) raise socket . close ( ) context . term ( ) return address + ':' + str ( port )
def racedirs ( path ) : if os . path . isfile ( path ) : raise IO Error ( 'Path `%s` is already a file not a directory' ) while True : try : if os . path . isdir ( path ) : # only break if full path has been created or exists break os . makedirs ( path ) except Environment Error as exc : # Part of the directory path already exist if exc . errno != 17 : # This error won't be any good raise
def reset ( self , index , total , percentage step , length ) : self . start time = datetime . datetime . now ( ) self . start index = index self . current index = index self . percentage step = percentage step self . total = float ( total ) self . total minus one = total - 1 self . length = length self . norm factor = total * percentage step / 100.0 self . current interval = int ( ( index + 1.0 ) / self . norm factor )
def get remaining ( self , index ) : try : current time = datetime . datetime . now ( ) time delta = current time - self . start time try : total seconds = time delta . total seconds ( ) except Attribute Error : # for backwards-compatibility # Python 2.6 does not support `total seconds` total seconds = ( ( time delta . microseconds + ( time delta . seconds + time delta . days * 24 * 3600 ) * 10 ** 6 ) / 10.0 ** 6 ) remaining seconds = int ( ( self . total - self . start index - 1.0 ) * total seconds / float ( index - self . start index ) - total seconds ) remaining delta = datetime . timedelta ( seconds = remaining seconds ) remaining str = ', remaining: ' + str ( remaining delta ) except Zero Division Error : remaining str = '' return remaining str
def f remove ( self , key ) : key = self . translate key ( key ) try : del self . dict [ key ] except Key Error : raise Attribute Error ( 'Your annotations do not contain %s' % key )
def f ann to str ( self ) : resstr = '' for key in sorted ( self . dict . keys ( ) ) : resstr += '%s=%s; ' % ( key , str ( self . dict [ key ] ) ) return resstr [ : - 2 ]
def supports ( self , item ) : result = super ( Shared Result , self ) . supports ( item ) result = result or type ( item ) in Shared Result . SUPPORTED DATA return result
def create shared data ( self , name = None , * * kwargs ) : if name is None : item = self . f get ( ) else : item = self . f get ( name ) return item . create shared data ( * * kwargs )
def send done ( self ) : self . start ( test connection = False ) self . logger . debug ( 'Sending shutdown signal' ) self . req rep ( ZMQ Server . DONE )
def req rep retry ( self , request ) : retries left = self . RETRIES while retries left : self . logger . log ( 1 , 'Sending REQ `%s`' , request ) self . send request ( request ) socks = dict ( self . poll . poll ( self . TIMEOUT ) ) if socks . get ( self . socket ) == zmq . POLLIN : response = self . receive response ( ) self . logger . log ( 1 , 'Received REP `%s`' , response ) return response , self . RETRIES - retries left else : self . logger . debug ( 'No response from server (%d retries left)' % retries left ) self . close socket ( confused = True ) retries left -= 1 if retries left == 0 : raise Runtime Error ( 'Server seems to be offline!' ) time . sleep ( self . SLEEP ) self . start socket ( )
def put on queue ( self , to put ) : old = self . pickle queue self . pickle queue = False try : self . queue . put ( to put , block = True ) finally : self . pickle queue = old
def put on pipe ( self , to put ) : self . acquire lock ( ) self . send chunks ( to put ) self . release lock ( )
def handle data ( self , msg , args , kwargs ) : stop = False try : if msg == 'DONE' : stop = True elif msg == 'STORE' : if 'msg' in kwargs : store msg = kwargs . pop ( 'msg' ) else : store msg = args [ 0 ] args = args [ 1 : ] if 'stuff to store' in kwargs : stuff to store = kwargs . pop ( 'stuff to store' ) else : stuff to store = args [ 0 ] args = args [ 1 : ] trajectory name = kwargs [ 'trajectory name' ] if self . trajectory name != trajectory name : if self . storage service . is open : self . close file ( ) self . trajectory name = trajectory name self . open file ( ) self . storage service . store ( store msg , stuff to store , * args , * * kwargs ) self . storage service . store ( pypetconstants . FLUSH , None ) self . check and collect garbage ( ) else : raise Runtime Error ( 'You queued something that was not ' 'intended to be queued. I did not understand message ' '`%s`.' % msg ) except Exception : self . logger . exception ( 'ERROR occurred during storing!' ) time . sleep ( 0.01 ) pass # We don't want to kill the queue process in case of an error return stop
def run ( self ) : try : while True : msg , args , kwargs = self . receive data ( ) stop = self . handle data ( msg , args , kwargs ) if stop : break finally : if self . storage service . is open : self . close file ( ) self . trajectory name = ''
def receive data ( self ) : result = self . queue . get ( block = True ) if hasattr ( self . queue , 'task done' ) : self . queue . task done ( ) return result
def receive data ( self ) : while True : while len ( self . buffer ) < self . max size and self . conn . poll ( ) : data = self . read chunks ( ) if data is not None : self . buffer . append ( data ) if len ( self . buffer ) > 0 : return self . buffer . popleft ( )
def store ( self , * args , * * kwargs ) : try : self . acquire lock ( ) return self . storage service . store ( * args , * * kwargs ) finally : if self . lock is not None : try : self . release lock ( ) except Runtime Error : self . logger . error ( 'Could not release lock `%s`!' % str ( self . lock ) )
def store ( self , msg , stuff to store , * args , * * kwargs ) : trajectory name = kwargs [ 'trajectory name' ] if trajectory name not in self . references : self . references [ trajectory name ] = [ ] self . references [ trajectory name ] . append ( ( msg , cp . copy ( stuff to store ) , args , kwargs ) )
def store references ( self , references ) : for trajectory name in references : self . storage service . store ( pypetconstants . LIST , references [ trajectory name ] , trajectory name = trajectory name ) self . check and collect garbage ( )
def parse config ( init func ) : @ functools . wraps ( init func ) def new func ( env , * args , * * kwargs ) : config interpreter = Config Interpreter ( kwargs ) # Pass the config data to the kwargs new kwargs = config interpreter . interpret ( ) init func ( env , * args , * * new kwargs ) # Add parameters and config data from the `.ini` file config interpreter . add parameters ( env . traj ) return new func
def collect section ( self , section ) : kwargs = { } try : if self . parser . has section ( section ) : options = self . parser . options ( section ) for option in options : str val = self . parser . get ( section , option ) val = ast . literal eval ( str val ) kwargs [ option ] = val return kwargs except : raise
def collect config ( self ) : kwargs = { } sections = ( 'storage service' , 'trajectory' , 'environment' ) for section in sections : kwargs . update ( self . collect section ( section ) ) return kwargs
def interpret ( self ) : if self . config file : new kwargs = self . collect config ( ) for key in new kwargs : # Already specified kwargs take precedence over the ini file if key not in self . kwargs : self . kwargs [ key ] = new kwargs [ key ] if not use simple logging ( self . kwargs ) and 'log config' not in self . kwargs : self . kwargs [ 'log config' ] = self . config file return self . kwargs
def add parameters ( self , traj ) : if self . config file : parameters = self . collect section ( 'parameters' ) for name in parameters : value = parameters [ name ] if not isinstance ( value , tuple ) : value = ( value , ) traj . f add parameter ( name , * value ) config = self . collect section ( 'config' ) for name in config : value = config [ name ] if not isinstance ( value , tuple ) : value = ( value , ) traj . f add config ( name , * value )
def overview group ( self ) : if self . overview group is None : self . overview group = self . all create or get groups ( 'overview' ) [ 0 ] return self . overview group
def trj load exploration ( self , traj ) : if hasattr ( self . overview group , 'explorations' ) : explorations table = self . overview group . f get child ( 'explorations' ) for row in explorations table . iterrows ( ) : param name = row [ 'explorations' ] . decode ( 'utf-8' ) if param name not in traj . explored parameters : traj . explored parameters [ param name ] = None else : # This is for backwards compatibility for what in ( 'parameters' , 'derived parameters' ) : if hasattr ( self . trajectory group , what ) : parameters = self . trajectory group . f get child ( what ) for group in parameters . f walk groups ( ) : if self . all get from attrs ( group , HDF5Storage Service . LENGTH ) : group location = group . v pathname full name = '.' . join ( group location . split ( '/' ) [ 2 : ] ) traj . explored parameters [ full name ] = None
def trj store explorations ( self , traj ) : nexplored = len ( traj . explored parameters ) if nexplored > 0 : if hasattr ( self . overview group , 'explorations' ) : explorations table = self . overview group . f get child ( 'explorations' ) if len ( explorations table ) != nexplored : self . hdf5file . remove node ( where = self . overview group , name = 'explorations' ) if not hasattr ( self . overview group , 'explorations' ) : explored list = list ( traj . explored parameters . keys ( ) ) if explored list : string col = self . all get table col ( 'explorations' , explored list , 'overview.explorations' ) else : string col = pt . String Col ( 1 ) description = { 'explorations' : string col } explorations table = self . hdf5file . create table ( where = self . overview group , name = 'explorations' , description = description ) rows = [ ( x . encode ( 'utf-8' ) , ) for x in explored list ] if rows : explorations table . append ( rows ) explorations table . flush ( )
def srvc make overview tables ( self , tables to make , traj = None ) : for table name in tables to make : # Prepare the tables desciptions, depending on which overview table we create # we need different columns paramdescriptiondict = { } expectedrows = 0 # Every overview table has a name and location column paramdescriptiondict [ 'location' ] = pt . String Col ( pypetconstants . HDF5 STRCOL MAX LOCATION LENGTH , pos = 0 ) paramdescriptiondict [ 'name' ] = pt . String Col ( pypetconstants . HDF5 STRCOL MAX NAME LENGTH , pos = 1 ) paramdescriptiondict [ 'comment' ] = pt . String Col ( pypetconstants . HDF5 STRCOL MAX COMMENT LENGTH ) paramdescriptiondict [ 'value' ] = pt . String Col ( pypetconstants . HDF5 STRCOL MAX VALUE LENGTH , pos = 2 ) if table name == 'config overview' : if traj is not None : expectedrows = len ( traj . config ) if table name == 'parameters overview' : if traj is not None : expectedrows = len ( traj . parameters ) if table name == 'explored parameters overview' : paramdescriptiondict [ 'range' ] = pt . String Col ( pypetconstants . HDF5 STRCOL MAX RANGE LENGTH ) paramdescriptiondict [ 'length' ] = pt . Int Col ( ) if traj is not None : expectedrows = len ( traj . explored parameters ) if table name . endswith ( 'summary' ) : paramdescriptiondict [ 'hexdigest' ] = pt . String Col ( 64 , pos = 10 ) # Check if the user provided an estimate of the amount of results per run # This can help to speed up storing if table name == 'derived parameters overview' : expectedrows = self . derived parameters per run if traj is not None : expectedrows *= len ( traj ) expectedrows += len ( traj . derived parameters ) if table name == 'results overview' : expectedrows = self . results per run if traj is not None : expectedrows *= len ( traj ) expectedrows += len ( traj . results ) if expectedrows > 0 : paramtable = self . all get or create table ( where = self . overview group , tablename = table name , description = paramdescriptiondict , expectedrows = expectedrows ) else : paramtable = self . all get or create table ( where = self . overview group , tablename = table name , description = paramdescriptiondict ) paramtable . flush ( )
def all get or create table ( self , where , tablename , description , expectedrows = None ) : where node = self . hdf5file . get node ( where ) if not tablename in where node : if not expectedrows is None : table = self . hdf5file . create table ( where = where node , name = tablename , description = description , title = tablename , expectedrows = expectedrows , filters = self . all get filters ( ) ) else : table = self . hdf5file . create table ( where = where node , name = tablename , description = description , title = tablename , filters = self . all get filters ( ) ) else : table = where node . f get child ( tablename ) return table
def all get node by name ( self , name ) : path name = name . replace ( '.' , '/' ) where = '/%s/%s' % ( self . trajectory name , path name ) return self . hdf5file . get node ( where = where )
def all insert into row ( self , row , insert dict ) : for key , val in insert dict . items ( ) : try : row [ key ] = val except Key Error as ke : self . logger . warning ( 'Could not write `%s` into a table, ' % key + repr ( ke ) )
def all create or get group ( self , name , parent hdf5 group = None ) : if not name in parent hdf5 group : new hdf5 group = self . hdf5file . create group ( where = parent hdf5 group , name = name , title = name , filters = self . all get filters ( ) ) return new hdf5 group , True else : new hdf5 group = parent hdf5 group . f get child ( name ) return new hdf5 group , False
def ann store annotations ( self , item with annotations , node , overwrite = False ) : # If we overwrite delete all annotations first if overwrite is True or overwrite == 'v annotations' : annotated = self . all get from attrs ( node , HDF5Storage Service . ANNOTATED ) if annotated : current attrs = node . v attrs for attr name in current attrs . v attrnames : if attr name . startswith ( HDF5Storage Service . ANNOTATION PREFIX ) : delattr ( current attrs , attr name ) delattr ( current attrs , HDF5Storage Service . ANNOTATED ) self . hdf5file . flush ( ) # Only store annotations if the item has some if not item with annotations . v annotations . f is empty ( ) : anno dict = item with annotations . v annotations . dict current attrs = node . v attrs changed = False for field name in anno dict : val = anno dict [ field name ] field name with prefix = HDF5Storage Service . ANNOTATION PREFIX + field name if field name with prefix not in current attrs : # Only store *new* annotations, if they already exist on disk, skip storage setattr ( current attrs , field name with prefix , val ) changed = True if changed : setattr ( current attrs , HDF5Storage Service . ANNOTATED , True ) self . hdf5file . flush ( )
def ann load annotations ( self , item with annotations , node ) : annotated = self . all get from attrs ( node , HDF5Storage Service . ANNOTATED ) if annotated : annotations = item with annotations . v annotations # You can only load into non-empty annotations, to prevent overwriting data in RAM if not annotations . f is empty ( ) : raise Type Error ( 'Loading into non-empty annotations!' ) current attrs = node . v attrs for attr name in current attrs . v attrnames : if attr name . startswith ( HDF5Storage Service . ANNOTATION PREFIX ) : key = attr name key = key . replace ( HDF5Storage Service . ANNOTATION PREFIX , '' ) data = getattr ( current attrs , attr name ) setattr ( annotations , key , data )
def grp load group ( self , traj group , load data = pypetconstants . LOAD DATA , with links = True , recursive = False , max depth = None , traj = None , as new = False , hdf5 group = None ) : if hdf5 group is None : hdf5 group = self . all get node by name ( traj group . v full name ) traj = traj group . v root if recursive : parent traj node = traj group . f get parent ( ) self . tree load nodes dfs ( parent traj node , load data = load data , with links = with links , recursive = recursive , max depth = max depth , current depth = 0 , trajectory = traj , as new = as new , hdf5 group = hdf5 group ) else : if load data == pypetconstants . LOAD NOTHING : return elif load data == pypetconstants . OVERWRITE DATA : traj group . v annotations . f empty ( ) traj group . v comment = '' self . all load skeleton ( traj group , hdf5 group ) traj group . stored = not as new # Signal completed node loading self . node processing timer . signal update ( )
def all load skeleton ( self , traj node , hdf5 group ) : if traj node . v annotations . f is empty ( ) : self . ann load annotations ( traj node , hdf5 group ) if traj node . v comment == '' : comment = self . all get from attrs ( hdf5 group , HDF5Storage Service . COMMENT ) if comment is None : comment = '' traj node . v comment = comment
def prm write shared array ( self , key , data , hdf5 group , full name , flag , * * kwargs ) : if flag == HDF5Storage Service . ARRAY : self . prm write into array ( key , data , hdf5 group , full name , * * kwargs ) elif flag in ( HDF5Storage Service . CARRAY , HDF5Storage Service . EARRAY , HDF5Storage Service . VLARRAY ) : self . prm write into other array ( key , data , hdf5 group , full name , flag = flag , * * kwargs ) else : raise Runtime Error ( 'Flag `%s` of hdf5 data `%s` of `%s` not understood' % ( flag , key , full name ) ) self . hdf5file . flush ( )
def prm write shared table ( self , key , hdf5 group , fullname , * * kwargs ) : first row = None description = None if 'first row' in kwargs : first row = kwargs . pop ( 'first row' ) if not 'description' in kwargs : description = { } for colname in first row : data = first row [ colname ] column = self . all get table col ( key , [ data ] , fullname ) description [ colname ] = column if 'description' in kwargs : description = kwargs . pop ( 'description' ) if 'filters' in kwargs : filters = kwargs . pop ( 'filters' ) else : filters = self . all get filters ( kwargs ) table = self . hdf5file . create table ( where = hdf5 group , name = key , description = description , filters = filters , * * kwargs ) table . flush ( ) if first row is not None : row = table . row for key in description : row [ key ] = first row [ key ] row . append ( ) table . flush ( )
def lnk delete link ( self , link name ) : translated name = '/' + self . trajectory name + '/' + link name . replace ( '.' , '/' ) link = self . hdf5file . get node ( where = translated name ) link . f remove ( )
def prm make description ( self , data , fullname ) : def convert lists and tuples ( series of data ) : """Converts lists and tuples to numpy arrays""" if isinstance ( series of data [ 0 ] , ( list , tuple ) ) : # and not isinstance(series of data[0], np.ndarray): # If the first data item is a list, the rest must be as well, since # data has to be homogeneous for idx , item in enumerate ( series of data ) : series of data [ idx ] = np . array ( item ) descriptiondict = { } # dictionary containing the description to build a pytables table original data type dict = { } # dictionary containing the original data types for key in data : val = data [ key ] # remember the original data types self . all set attributes to recall natives ( val [ 0 ] , PT Item Mock ( original data type dict ) , HDF5Storage Service . FORMATTED COLUMN PREFIX % key ) convert lists and tuples ( val ) # get a pytables column from the data col = self . all get table col ( key , val , fullname ) descriptiondict [ key ] = col return descriptiondict , original data type dict
def prm get longest stringsize ( string list ) : maxlength = 1 for stringar in string list : if isinstance ( stringar , np . ndarray ) : if stringar . ndim > 0 : for string in stringar . ravel ( ) : maxlength = max ( len ( string ) , maxlength ) else : maxlength = max ( len ( stringar . tolist ( ) ) , maxlength ) else : maxlength = max ( len ( stringar ) , maxlength ) # Make the string Col longer than needed in order to allow later on slightly larger strings return int ( maxlength * 1.5 )
def make set name ( idx ) : GROUPSIZE = 1000 set idx = idx // GROUPSIZE if set idx >= 0 : return pypetconstants . FORMATTED SET NAME % set idx else : return pypetconstants . SET NAME DUMMY
def preset ( self , name , args , kwargs ) : if self . f contains ( name , shortcuts = False ) : raise Value Error ( 'Parameter `%s` is already part of your trajectory, use the normal' 'accessing routine to change config.' % name ) else : self . changed default parameters [ name ] = ( args , kwargs )
def remove exploration ( self ) : for param in self . explored parameters . values ( ) : if param . stored : try : self . f delete item ( param ) except Exception : self . logger . exception ( 'Could not delete expanded parameter `%s` ' 'from disk.' % param . v full name )
def update run information ( self , run information dict ) : idx = run information dict [ 'idx' ] name = run information dict [ 'name' ] self . run information [ name ] = run information dict self . updated run information . add ( idx )
def add run info ( self , idx , name = '' , timestamp = 42.0 , finish timestamp = 1.337 , runtime = 'forever and ever' , time = '>>Maybe time`s gone on strike' , completed = 0 , parameter summary = 'Not yet my friend!' , short environment hexsha = 'N/A' ) : if idx in self . single run ids : # Delete old entries, they might be replaced by a new name old name = self . single run ids [ idx ] del self . single run ids [ old name ] del self . single run ids [ idx ] del self . run information [ old name ] if name == '' : name = self . f wildcard ( '$' , idx ) # The ` single run ids` dict is bidirectional and maps indices to run names and vice versa self . single run ids [ name ] = idx self . single run ids [ idx ] = name info dict = { 'idx' : idx , 'timestamp' : timestamp , 'finish timestamp' : finish timestamp , 'runtime' : runtime , 'time' : time , 'completed' : completed , 'name' : name , 'parameter summary' : parameter summary , 'short environment hexsha' : short environment hexsha } self . run information [ name ] = info dict self . length = len ( self . run information )
def f lock parameters ( self ) : for par in self . parameters . values ( ) : if not par . f is empty ( ) : par . f lock ( )
def f lock derived parameters ( self ) : for par in self . derived parameters . values ( ) : if not par . f is empty ( ) : par . f lock ( )
def check if both have same parameters ( self , other trajectory , ignore data , consecutive merge ) : if not isinstance ( other trajectory , Trajectory ) : raise Type Error ( 'Can only merge trajectories, the other trajectory' ' is of type `%s`.' % str ( type ( other trajectory ) ) ) if self . stored and not consecutive merge : self . f load skeleton ( ) if other trajectory . stored : other trajectory . f load skeleton ( ) # Check wildcard set other wildcard set = set ( x [ 1 ] for x in other trajectory . wildcard functions . keys ( ) ) wildcard set = set ( x [ 1 ] for x in self . wildcard functions . keys ( ) ) diff = wildcard set . symmetric difference ( other wildcard set ) if diff : raise Type Error ( 'The wildcard sets are not matching. `%s` != `%s`' % ( str ( wildcard set ) , str ( other wildcard set ) ) ) # Load all parameters of the current and the other trajectory if self . stored : # To suppress warnings if nothing needs to be loaded with self . nn interface . disable logging : self . f load items ( self . parameters . keys ( ) , only empties = True ) if other trajectory . stored : with self . nn interface . disable logging : other trajectory . f load items ( other trajectory . parameters . keys ( ) , only empties = True ) self . f restore default ( ) other trajectory . f restore default ( ) allmyparams = self . parameters . copy ( ) allotherparams = other trajectory . parameters . copy ( ) # If not ignored, add also the trajectory derived parameters to check for merging if 'derived parameters' in self : my traj dpars = self . derived parameters if self . stored : with self . nn interface . disable logging : self . f load items ( my traj dpars . keys ( ) , only empties = True ) allmyparams . update ( my traj dpars ) other traj dpars = other trajectory . derived parameters if other trajectory . stored : with self . nn interface . disable logging : other trajectory . f load items ( other traj dpars . keys ( ) , only empties = True ) allotherparams . update ( other traj dpars ) # Check if the trajectories have the same parameters: my keyset = set ( allmyparams . keys ( ) ) other keyset = set ( allotherparams . keys ( ) ) diff = my keyset . symmetric difference ( other keyset ) - ignore data run dummys = ( self . f wildcard ( '$' , - 1 ) , other trajectory . f wildcard ( '$' , - 1 ) ) if diff : run difference can be resolved = True for full name in diff : split name = full name . split ( '.' ) if not any ( x in self . run information or x in other trajectory . run information or x in run dummys for x in split name ) : run difference can be resolved = False break elif full name in allotherparams : del allotherparams [ full name ] if not run difference can be resolved : raise Type Error ( 'Cannot merge trajectories, ' 'they do not live in the same space,the ' 'set of parameters `%s` is only ' 'found in one trajectory.' % str ( diff ) ) # Check if corresponding parameters in both trajectories are of the same type for key , other param in allotherparams . items ( ) : if key in ignore data : continue my param = self . f get ( key ) split key = key . split ( '.' ) if any ( x in self . run information or x in other trajectory . run information for x in split key ) : pass else : if not my param . values of same type ( my param . f get ( ) , other param . f get ( ) ) : raise Type Error ( 'Cannot merge trajectories, values of parameters `%s` are not ' 'of the same type. Types are %s (current) and %s (other).' % ( key , str ( type ( my param . f get ( ) ) ) , str ( type ( other param . f get ( ) ) ) ) )
def make reversed wildcards ( self , old length = - 1 ) : if len ( self . reversed wildcards ) > 0 : # We already created reversed wildcards, so we don't need to do all of them # again start = old length else : start = - 1 for wildcards , func in self . wildcard functions . items ( ) : for irun in range ( start , len ( self ) ) : translated name = func ( irun ) if not translated name in self . reversed wildcards : self . reversed wildcards [ translated name ] = ( [ ] , wildcards ) self . reversed wildcards [ translated name ] [ 0 ] . append ( irun )
def merge single runs ( self , other trajectory , used runs ) : count = len ( self ) # Variable to count the increasing new run indices and create # new run names run indices = range ( len ( other trajectory ) ) run name dict = Ordered Dict ( ) to store groups with annotations = [ ] for idx in run indices : # Iterate through all used runs and store annotated groups and mark results and # derived parameters for merging if idx in used runs : # Update the run information dict of the current trajectory other info dict = other trajectory . f get run information ( idx ) time = other info dict [ 'time' ] timestamp = other info dict [ 'timestamp' ] completed = other info dict [ 'completed' ] short environment hexsha = other info dict [ 'short environment hexsha' ] finish timestamp = other info dict [ 'finish timestamp' ] runtime = other info dict [ 'runtime' ] new idx = used runs [ idx ] new runname = self . f wildcard ( '$' , new idx ) run name dict [ idx ] = new runname info dict = dict ( idx = new idx , time = time , timestamp = timestamp , completed = completed , short environment hexsha = short environment hexsha , finish timestamp = finish timestamp , runtime = runtime ) self . add run info ( * * info dict )
def rename full name ( self , full name , other trajectory , used runs = None , new run idx = None ) : split name = full name . split ( '.' ) for idx , name in enumerate ( split name ) : if name in other trajectory . reversed wildcards : run indices , wildcards = other trajectory . reversed wildcards [ name ] if new run idx is None : # We can safely take the first index of the index list that matches run idx = None for run jdx in run indices : if run jdx in used runs : run idx = used runs [ run jdx ] break elif run jdx == - 1 : run idx = - 1 break if run idx is None : raise Runtime Error ( 'You shall not pass!' ) else : run idx = new run idx new name = self . f wildcard ( wildcards [ 0 ] , run idx ) split name [ idx ] = new name full name = '.' . join ( split name ) return full name
def make single run ( self ) : self . is run = False # to be able to use f set crun self . new nodes = Ordered Dict ( ) self . new links = Ordered Dict ( ) self . is run = True return self
def set start ( self ) : init time = time . time ( ) formatted time = datetime . datetime . fromtimestamp ( init time ) . strftime ( '%Y %m %d %Hh%Mm%Ss' ) run info dict = self . run information [ self . v crun ] run info dict [ 'timestamp' ] = init time run info dict [ 'time' ] = formatted time if self . environment hexsha is not None : run info dict [ 'short environment hexsha' ] = self . environment hexsha [ 0 : 7 ]
def set finish ( self ) : run info dict = self . run information [ self . v crun ] timestamp run = run info dict [ 'timestamp' ] run summary = self . summarize explored parameters ( ) finish timestamp run = time . time ( ) findatetime = datetime . datetime . fromtimestamp ( finish timestamp run ) startdatetime = datetime . datetime . fromtimestamp ( timestamp run ) runtime run = str ( findatetime - startdatetime ) run info dict [ 'parameter summary' ] = run summary run info dict [ 'completed' ] = 1 run info dict [ 'finish timestamp' ] = finish timestamp run run info dict [ 'runtime' ] = runtime run
def pool single run ( kwargs ) : wrap mode = kwargs [ 'wrap mode' ] traj = kwargs [ 'traj' ] traj . v storage service = pool single run . storage service if wrap mode == pypetconstants . WRAP MODE LOCAL : # Free references from previous runs traj . v storage service . free references ( ) return sigint handling single run ( kwargs )
def frozen pool single run ( kwargs ) : idx = kwargs . pop ( 'idx' ) frozen kwargs = frozen pool single run . kwargs frozen kwargs . update ( kwargs ) # in case of `run map` # we need to update job's args and kwargs traj = frozen kwargs [ 'traj' ] traj . f set crun ( idx ) return sigint handling single run ( frozen kwargs )
def configure pool ( kwargs ) : pool single run . storage service = kwargs [ 'storage service' ] configure niceness ( kwargs ) configure logging ( kwargs , extract = False )
def configure frozen pool ( kwargs ) : frozen pool single run . kwargs = kwargs configure niceness ( kwargs ) configure logging ( kwargs , extract = False ) # Reset full copy to it's old value traj = kwargs [ 'traj' ] traj . v full copy = kwargs [ 'full copy' ]
def process single run ( kwargs ) : configure niceness ( kwargs ) configure logging ( kwargs ) result queue = kwargs [ 'result queue' ] result = sigint handling single run ( kwargs ) result queue . put ( result ) result queue . close ( )
def scoop single run ( kwargs ) : try : try : is origin = scoop . IS ORIGIN except Attribute Error : # scoop is not properly started, i.e. with `python -m scoop...` # in this case scoop uses default `map` function, i.e. # the main process is origin = True if not is origin : # configure logging and niceness if not the main process: configure niceness ( kwargs ) configure logging ( kwargs ) return single run ( kwargs ) except Exception : scoop . logger . exception ( 'ERROR occurred during a single run!' ) raise
def configure niceness ( kwargs ) : niceness = kwargs [ 'niceness' ] if niceness is not None : try : try : current = os . nice ( 0 ) if niceness - current > 0 : # Under Linux you cannot decrement niceness if set elsewhere os . nice ( niceness - current ) except Attribute Error : # Fall back on psutil under Windows psutil . Process ( ) . nice ( niceness ) except Exception as exc : sys . stderr . write ( 'Could not configure niceness because of: %s' % repr ( exc ) ) traceback . print exc ( )
def sigint handling single run ( kwargs ) : try : graceful exit = kwargs [ 'graceful exit' ] if graceful exit : sigint handling . start ( ) if sigint handling . hit : result = ( sigint handling . SIGINT , None ) else : result = single run ( kwargs ) if sigint handling . hit : result = ( sigint handling . SIGINT , result ) return result return single run ( kwargs ) except : # Log traceback of exception pypet root logger = logging . get Logger ( 'pypet' ) pypet root logger . exception ( 'ERROR occurred during a single run! ' ) raise
def wrap handling ( kwargs ) : configure logging ( kwargs , extract = False ) # Main job, make the listener to the queue start receiving message for writing to disk. handler = kwargs [ 'handler' ] graceful exit = kwargs [ 'graceful exit' ] # import c Profile as profile # profiler = profile.Profile() # profiler.enable() if graceful exit : sigint handling . start ( ) handler . run ( )
def f supports ( self , data ) : dtype = type ( data ) if dtype is tuple or dtype is list : # Parameters cannot handle empty tuples if len ( data ) == 0 : return False old type = None # Check if the data in the tuple is homogeneous for item in data : if not type ( item ) in pypetconstants . PARAMETER SUPPORTED DATA : return False if not old type is None and old type != type ( item ) : return False old type = type ( item ) return True elif dtype is np . ndarray or dtype is np . matrix : if data . size == 0 : return False # Parameters cannot handle empty arrays and matrices # Numpy has many string types that depend on the length of the string, # We allow all of them dtype = data . dtype if np . issubdtype ( dtype , np . str ) : dtype = np . str return dtype in pypetconstants . PARAMETER SUPPORTED DATA
def f supports ( self , data ) : dtype = type ( data ) if dtype is tuple or dtype is list and len ( data ) == 0 : return True #  Array Parameter does support empty tuples elif dtype is np . ndarray and data . size == 0 and data . ndim == 1 : return True #  Array Parameter supports empty numpy arrays else : return super ( Array Parameter , self ) . f supports ( data )
def equal values ( self , val1 , val2 ) : if self . is supported matrix ( val1 ) : if self . is supported matrix ( val2 ) : , , hash tuple 1 = self . serialize matrix ( val1 ) , , hash tuple 2 = self . serialize matrix ( val2 ) return hash ( hash tuple 1 ) == hash ( hash tuple 2 ) else : return False else : return super ( Sparse Parameter , self ) . equal values ( val1 , val2 )
def is supported matrix ( data ) : return ( spsp . isspmatrix csc ( data ) or spsp . isspmatrix csr ( data ) or spsp . isspmatrix bsr ( data ) or spsp . isspmatrix dia ( data ) )
def f translate key ( self , key ) : if isinstance ( key , int ) : if key == 0 : key = self . v name else : key = self . v name + ' %d' % key return key
def f remove ( self , * args ) : for arg in args : arg = self . f translate key ( arg ) if arg in self . data : del self . data [ arg ] else : raise Attribute Error ( 'Your result `%s` does not contain %s.' % ( self . name , arg ) )
def supports ( self , item ) : if Sparse Parameter . is supported matrix ( item ) : return True else : return super ( Sparse Result , self ) . supports ( item )
def store ( self ) : store dict = { } for key , val in self . data . items ( ) : store dict [ key ] = pickle . dumps ( val , protocol = self . v protocol ) store dict [ Pickle Result . PROTOCOL ] = self . v protocol return store dict
def main ( ) : folder = os . getcwd ( ) print ( 'Merging all files' ) merge all in folder ( folder , delete other files = True , # We will only keep one trajectory dynamic imports = Function Parameter , backup = False ) print ( 'Done' )
def create session ( ) : ctx = saga . Context ( "User Pass" ) ctx . user id = USER ctx . user pass = PASSWORD session = saga . Session ( ) session . add context ( ctx ) return session
def merge trajectories ( session ) : jd = saga . job . Description ( ) jd . executable = 'python' jd . arguments = [ 'merge trajs.py' ] jd . output = "mysagajob merge.stdout" jd . error = "mysagajob merge.stderr" jd . working directory = WORKING DIR js = saga . job . Service ( 'ssh://' + ADDRESS , session = session ) myjob = js . create job ( jd ) print ( "\n...starting job...\n" ) # Now we can start our job. myjob . run ( ) print ( "Job ID    : %s" % ( myjob . id ) ) print ( "Job State : %s" % ( myjob . state ) ) print ( "\n...waiting for job...\n" ) # wait for the job to either finish or fail myjob . wait ( ) print ( "Job State : %s" % ( myjob . state ) ) print ( "Exitcode  : %s" % ( myjob . exit code ) )
def start jobs ( session ) : js = saga . job . Service ( 'ssh://' + ADDRESS , session = session ) batches = range ( 3 ) jobs = [ ] for batch in batches : print ( 'Starting batch %d' % batch ) jd = saga . job . Description ( ) jd . executable = 'python' jd . arguments = [ 'the task.py --batch=' + str ( batch ) ] jd . output = "mysagajob.stdout" + str ( batch ) jd . error = "mysagajob.stderr" + str ( batch ) jd . working directory = WORKING DIR myjob = js . create job ( jd ) print ( "Job ID    : %s" % ( myjob . id ) ) print ( "Job State : %s" % ( myjob . state ) ) print ( "\n...starting job...\n" ) myjob . run ( ) jobs . append ( myjob ) for myjob in jobs : print ( "Job ID    : %s" % ( myjob . id ) ) print ( "Job State : %s" % ( myjob . state ) ) print ( "\n...waiting for job...\n" ) # wait for the job to either finish or fail myjob . wait ( ) print ( "Job State : %s" % ( myjob . state ) ) print ( "Exitcode  : %s" % ( myjob . exit code ) )
def multiply ( traj ) : z = traj . x * traj . y traj . f add result ( 'z' , z = z , comment = 'I am the product of two reals!' )
def add parameters ( traj ) : print ( 'Adding Parameters' ) traj . f add parameter ( 'neuron.V init' , 0.0 , comment = 'The initial condition for the ' 'membrane potential' ) traj . f add parameter ( 'neuron.I' , 0.0 , comment = 'The externally applied current.' ) traj . f add parameter ( 'neuron.tau V' , 10.0 , comment = 'The membrane time constant in milliseconds' ) traj . f add parameter ( 'neuron.tau ref' , 5.0 , comment = 'The refractory period in milliseconds ' 'where the membrane potnetial ' 'is clamped.' ) traj . f add parameter ( 'simulation.duration' , 1000.0 , comment = 'The duration of the experiment in ' 'milliseconds.' ) traj . f add parameter ( 'simulation.dt' , 0.1 , comment = 'The step size of an Euler integration step.' )
def add exploration ( traj ) : print ( 'Adding exploration of I and tau ref' ) explore dict = { 'neuron.I' : np . arange ( 0 , 1.01 , 0.01 ) . tolist ( ) , 'neuron.tau ref' : [ 5.0 , 7.5 , 10.0 ] } explore dict = cartesian product ( explore dict , ( 'neuron.tau ref' , 'neuron.I' ) ) # The second argument, the tuple, specifies the order of the cartesian product, # The variable on the right most side changes fastest and defines the # 'inner for-loop' of the cartesian product traj . f explore ( explore dict )
def make filename ( traj ) : explored parameters = traj . f get explored parameters ( ) filename = '' for param in explored parameters . values ( ) : short name = param . v name val = param . f get ( ) filename += '%s %s ' % ( short name , str ( val ) ) return filename [ : - 2 ] + '.png'
def main ( ) : # Now let's make use of logging logger = logging . get Logger ( ) # Create folders for data and plots folder = os . path . join ( os . getcwd ( ) , 'experiments' , 'ca patterns pypet' ) if not os . path . isdir ( folder ) : os . makedirs ( folder ) filename = os . path . join ( folder , 'all patterns.hdf5' ) # Create an environment env = Environment ( trajectory = 'cellular automata' , multiproc = True , ncores = 4 , wrap mode = 'QUEUE' , filename = filename , overwrite file = True ) # extract the trajectory traj = env . traj traj . par . ncells = Parameter ( 'ncells' , 400 , 'Number of cells' ) traj . par . steps = Parameter ( 'steps' , 250 , 'Number of timesteps' ) traj . par . rule number = Parameter ( 'rule number' , 30 , 'The ca rule' ) traj . par . initial name = Parameter ( 'initial name' , 'random' , 'The type of initial state' ) traj . par . seed = Parameter ( 'seed' , 100042 , 'RNG Seed' ) # Explore exp dict = { 'rule number' : [ 10 , 30 , 90 , 110 , 184 ] , 'initial name' : [ 'single' , 'random' ] , } # # You can uncomment the ``exp dict`` below to see that changing the # # exploration scheme is now really easy: # exp dict = {'rule number' : [10, 30, 90, 110, 184], #             'ncells' : [100, 200, 300], #             'seed': [333444555, 123456]} exp dict = cartesian product ( exp dict ) traj . f explore ( exp dict ) # Run the simulation logger . info ( 'Starting Simulation' ) env . run ( wrap automaton ) # Load all data traj . f load ( load data = 2 ) logger . info ( 'Printing data' ) for idx , run name in enumerate ( traj . f iter runs ( ) ) : # Plot all patterns filename = os . path . join ( folder , make filename ( traj ) ) plot pattern ( traj . crun . pattern , traj . rule number , filename ) progressbar ( idx , len ( traj ) , logger = logger ) # Finally disable logging and close all log-files env . disable logging ( )
def config from file ( filename , config = None ) : if config : # We're writing configuration try : with open ( filename , 'w' ) as fdesc : fdesc . write ( json . dumps ( config ) ) except IO Error as error : logger . exception ( error ) return False return True else : # We're reading config if os . path . isfile ( filename ) : try : with open ( filename , 'r' ) as fdesc : return json . loads ( fdesc . read ( ) ) except IO Error as error : return False else : return { }
def request pin ( self ) : url = 'https://api.ecobee.com/authorize' params = { 'response type' : 'ecobee Pin' , 'client id' : self . api key , 'scope' : 'smart Write' } try : request = requests . get ( url , params = params ) except Request Exception : logger . warn ( "Error connecting to Ecobee.  Possible connectivity outage." "Could not request pin." ) return self . authorization code = request . json ( ) [ 'code' ] self . pin = request . json ( ) [ 'ecobee Pin' ] logger . error ( 'Please authorize your ecobee developer app with PIN code ' + self . pin + '\n Goto https://www.ecobee.com/consumerportal' '/index.html, click\n My Apps, Add application, Enter Pin' ' and click Authorize.\n After authorizing, call request ' 'tokens() method.' )
def request tokens ( self ) : url = 'https://api.ecobee.com/token' params = { 'grant type' : 'ecobee Pin' , 'code' : self . authorization code , 'client id' : self . api key } try : request = requests . post ( url , params = params ) except Request Exception : logger . warn ( "Error connecting to Ecobee.  Possible connectivity outage." "Could not request token." ) return if request . status code == requests . codes . ok : self . access token = request . json ( ) [ 'access token' ] self . refresh token = request . json ( ) [ 'refresh token' ] self . write tokens to file ( ) self . pin = None else : logger . warn ( 'Error while requesting tokens from ecobee.com.' ' Status code: ' + str ( request . status code ) ) return
def refresh tokens ( self ) : url = 'https://api.ecobee.com/token' params = { 'grant type' : 'refresh token' , 'refresh token' : self . refresh token , 'client id' : self . api key } request = requests . post ( url , params = params ) if request . status code == requests . codes . ok : self . access token = request . json ( ) [ 'access token' ] self . refresh token = request . json ( ) [ 'refresh token' ] self . write tokens to file ( ) return True else : self . request pin ( )
def get thermostats ( self ) : url = 'https://api.ecobee.com/1/thermostat' header = { 'Content-Type' : 'application/json;charset=UTF-8' , 'Authorization' : 'Bearer ' + self . access token } params = { 'json' : ( '{"selection":{"selection Type":"registered",' '"include Runtime":"true",' '"include Sensors":"true",' '"include Program":"true",' '"include Equipment Status":"true",' '"include Events":"true",' '"include Weather":"true",' '"include Settings":"true"}}' ) } try : request = requests . get ( url , headers = header , params = params ) except Request Exception : logger . warn ( "Error connecting to Ecobee.  Possible connectivity outage." ) return None if request . status code == requests . codes . ok : self . authenticated = True self . thermostats = request . json ( ) [ 'thermostat List' ] return self . thermostats else : self . authenticated = False logger . info ( "Error connecting to Ecobee while attempting to get " "thermostat data.  Refreshing tokens and trying again." ) if self . refresh tokens ( ) : return self . get thermostats ( ) else : return None
def write tokens to file ( self ) : config = dict ( ) config [ 'API KEY' ] = self . api key config [ 'ACCESS TOKEN' ] = self . access token config [ 'REFRESH TOKEN' ] = self . refresh token config [ 'AUTHORIZATION CODE' ] = self . authorization code if self . file based config : config from file ( self . config filename , config ) else : self . config = config
def set hvac mode ( self , index , hvac mode ) : body = { "selection" : { "selection Type" : "thermostats" , "selection Match" : self . thermostats [ index ] [ 'identifier' ] } , "thermostat" : { "settings" : { "hvac Mode" : hvac mode } } } log msg action = "set HVAC mode" return self . make request ( body , log msg action )
def set fan min on time ( self , index , fan min on time ) : body = { "selection" : { "selection Type" : "thermostats" , "selection Match" : self . thermostats [ index ] [ 'identifier' ] } , "thermostat" : { "settings" : { "fan Min On Time" : fan min on time } } } log msg action = "set fan minimum on time." return self . make request ( body , log msg action )
def set climate hold ( self , index , climate , hold type = "next Transition" ) : body = { "selection" : { "selection Type" : "thermostats" , "selection Match" : self . thermostats [ index ] [ 'identifier' ] } , "functions" : [ { "type" : "set Hold" , "params" : { "hold Type" : hold type , "hold Climate Ref" : climate } } ] } log msg action = "set climate hold" return self . make request ( body , log msg action )
def delete vacation ( self , index , vacation ) : body = { "selection" : { "selection Type" : "thermostats" , "selection Match" : self . thermostats [ index ] [ 'identifier' ] } , "functions" : [ { "type" : "delete Vacation" , "params" : { "name" : vacation } } ] } log msg action = "delete a vacation" return self . make request ( body , log msg action )
def resume program ( self , index , resume all = False ) : body = { "selection" : { "selection Type" : "thermostats" , "selection Match" : self . thermostats [ index ] [ 'identifier' ] } , "functions" : [ { "type" : "resume Program" , "params" : { "resume All" : resume all } } ] } log msg action = "resume program" return self . make request ( body , log msg action )
def send message ( self , index , message = "Hello from python-ecobee!" ) : body = { "selection" : { "selection Type" : "thermostats" , "selection Match" : self . thermostats [ index ] [ 'identifier' ] } , "functions" : [ { "type" : "send Message" , "params" : { "text" : message [ 0 : 500 ] } } ] } log msg action = "send message" return self . make request ( body , log msg action )
def dict self ( self ) : return { k : v for k , v in self . dict . items ( ) if k in FSM ATTRS }
def reset ( self , iface = None , client mac = None , xid = None , scriptfile = None ) : logger . debug ( 'Reseting attributes.' ) if iface is None : iface = conf . iface if client mac is None : # scapy for python 3 returns byte, not tuple tempmac = get if raw hwaddr ( iface ) if isinstance ( tempmac , tuple ) and len ( tempmac ) == 2 : mac = tempmac [ 1 ] else : mac = tempmac client mac = str2mac ( mac ) self . client = DHCPCAP ( iface = iface , client mac = client mac , xid = xid ) if scriptfile is not None : self . script = Client Script ( scriptfile ) else : self . script = None self . time sent request = None self . discover attempts = 0 self . request attempts = 0 self . current state = STATE PREINIT self . offers = list ( )
def get timeout ( self , state , function ) : state = STATES2NAMES [ state ] for timeout fn t in self . timeout [ state ] : # access the function name if timeout fn t [ 1 ] is not None and timeout fn t [ 1 ] . atmt condname == function . atmt condname : logger . debug ( 'Timeout for state %s, function %s, is %s' , state , function . atmt condname , timeout fn t [ 0 ] ) return timeout fn t [ 0 ] return None
def set timers ( self ) : logger . debug ( 'setting timeouts' ) self . set timeout ( self . current state , self . renewing time expires , self . client . lease . renewal time ) self . set timeout ( self . current state , self . rebinding time expires , self . client . lease . rebinding time )
def process received nak ( self , pkt ) : if isnak ( pkt ) : logger . info ( 'DHCPNAK of %s from %s' , self . client . client ip , self . client . server ip ) return True return False
def receive offer ( self , pkt ) : logger . debug ( "C2. Received OFFER?, in SELECTING state." ) if isoffer ( pkt ) : logger . debug ( "C2: T, OFFER received" ) self . offers . append ( pkt ) if len ( self . offers ) >= MAX OFFERS COLLECTED : logger . debug ( "C2.5: T, raise REQUESTING." ) self . select offer ( ) raise self . REQUESTING ( ) logger . debug ( "C2.5: F, raise SELECTING." ) raise self . SELECTING ( )
def receive ack requesting ( self , pkt ) : logger . debug ( "C3. Received ACK?, in REQUESTING state." ) if self . process received ack ( pkt ) : logger . debug ( "C3: T. Received ACK, in REQUESTING state, " "raise BOUND." ) raise self . BOUND ( )
def receive nak requesting ( self , pkt ) : logger . debug ( "C3.1. Received NAK?, in REQUESTING state." ) if self . process received nak ( pkt ) : logger . debug ( "C3.1: T. Received NAK, in REQUESTING state, " "raise INIT." ) raise self . INIT ( )
def receive ack renewing ( self , pkt ) : logger . debug ( "C3. Received ACK?, in RENEWING state." ) if self . process received ack ( pkt ) : logger . debug ( "C3: T. Received ACK, in RENEWING state, " "raise BOUND." ) raise self . BOUND ( )
def receive nak renewing ( self , pkt ) : logger . debug ( "C3.1. Received NAK?, in RENEWING state." ) if self . process received nak ( pkt ) : logger . debug ( "C3.1: T. Received NAK, in RENEWING state, " " raise INIT." ) raise self . INIT ( )
def receive ack rebinding ( self , pkt ) : logger . debug ( "C3. Received ACK?, in REBINDING state." ) if self . process received ack ( pkt ) : logger . debug ( "C3: T. Received ACK, in REBINDING state, " "raise BOUND." ) raise self . BOUND ( )
def receive nak rebinding ( self , pkt ) : logger . debug ( "C3.1. Received NAK?, in RENEWING state." ) if self . process received nak ( pkt ) : logger . debug ( "C3.1: T. Received NAK, in RENEWING state, " "raise INIT." ) raise self . INIT ( )
def set ( self , name , value ) : clone = self . clone ( ) if django . VERSION [ 0 ] <= 1 and django . VERSION [ 1 ] <= 4 : value = value or None clone . qsl = [ ( q , v ) for ( q , v ) in self . qsl if q != name ] if value is not None : clone . qsl . append ( ( name , value ) ) return clone
def add ( self , name , value ) : clone = self . clone ( ) clone . qsl = [ p for p in self . qsl if not ( p [ 0 ] == name and p [ 1 ] == value ) ] clone . qsl . append ( ( name , value , ) ) return clone
def remove ( self , name , value ) : clone = self . clone ( ) clone . qsl = [ qb for qb in self . qsl if qb != ( name , str ( value ) ) ] return clone
def read tdms ( tdms file ) : tdms file = nptdms . Tdms File ( tdms file ) ch names = [ ] ch data = [ ] for o in tdms file . objects . values ( ) : if o . data is not None and len ( o . data ) : chn = o . path . split ( '/' ) [ - 1 ] . strip ( "'" ) if "unit string" in o . properties : unit = o . properties [ "unit string" ] ch names . append ( "{} [{}]" . format ( chn , unit ) ) else : ch names . append ( chn ) ch data . append ( o . data ) return ch names , ch data
def tdms2fcs ( tdms file ) : fcs file = tdms file [ : - 4 ] + "fcs" chn names , data = read tdms ( tdms file ) chn names , data = add deformation ( chn names , data ) fcswrite . write fcs ( filename = fcs file , chn names = chn names , data = np . array ( data ) . transpose ( ) )
def equal ( self , cwd ) : cmd = [ "diff" ] cmd . append ( "-q" ) cmd . append ( self . left . get name ( ) ) cmd . append ( self . right . get name ( ) ) try : Process ( cmd ) . run ( cwd = cwd , suppress output = True ) except Subprocess Error as e : if e . get returncode ( ) == 1 : return False else : raise e return True
def backup file ( self , file , patch ) : dest dir = self . quilt pc + patch . get name ( ) file dir = file . get directory ( ) if file dir : #TODO get relative path dest dir = dest dir + file dir backup = Backup ( ) backup . backup file ( file , dest dir , copy empty = True )
def link ( self , link ) : if isinstance ( link , File ) : link = link . filename os . link ( self . filename , link )
def copy ( self , dest ) : if isinstance ( dest , File ) : dest dir = dest . get directory ( ) dest dir . create ( ) dest = dest . filename elif isinstance ( dest , Directory ) : dest = dest . dirname shutil . copy2 ( self . filename , dest )
def apply patch ( self , patch name , force = False , quiet = False ) : self . check ( ) patch = Patch ( patch name ) patches = self . series . patches until ( patch ) [ : ] applied = self . db . applied patches ( ) for patch in applied : if patch in patches : patches . remove ( patch ) if not patches : raise All Patches Applied ( self . series , self . db . top patch ( ) ) self . applying ( patch ) try : for cur patch in patches : self . apply patch ( cur patch , force , quiet ) finally : self . db . save ( ) self . applied ( self . db . top patch ( ) )
def apply next patch ( self , force = False , quiet = False ) : self . check ( ) top = self . db . top patch ( ) if not top : patch = self . series . first patch ( ) else : patch = self . series . patch after ( top ) if not patch : raise All Patches Applied ( self . series , top ) self . applying ( patch ) self . apply patch ( patch , force , quiet ) self . db . save ( ) self . applied ( self . db . top patch ( ) )
def apply all ( self , force = False , quiet = False ) : self . check ( ) top = self . db . top patch ( ) if top : patches = self . series . patches after ( top ) else : patches = self . series . patches ( ) if not patches : raise All Patches Applied ( self . series , top ) try : for patch in patches : self . applying ( patch ) self . apply patch ( patch , force , quiet ) finally : self . db . save ( ) self . applied ( self . db . top patch ( ) )
def read ( self ) : self . patchlines = [ ] self . patch2line = dict ( ) if self . exists ( ) : with open ( self . series file , "r" ) as f : for line in f : self . add patch ( line )
def save ( self ) : with open ( self . series file , "wb" ) as f : for patchline in self . patchlines : f . write ( encode str ( str ( patchline ) ) ) f . write ( b"\n" )
def add patch ( self , patch ) : patchline = Patch Line ( patch ) patch = patchline . get patch ( ) if patch : self . patch2line [ patch ] = patchline self . patchlines . append ( patchline )
def insert patches ( self , patches ) : patchlines = [ ] for patch name in patches : patchline = Patch Line ( patch name ) patch = patchline . get patch ( ) if patch : self . patch2line [ patch ] = patchline patchlines . append ( patchline ) patchlines . extend ( self . patchlines ) self . patchlines = patchlines
def add patches ( self , patches , after = None ) : if after is None : self . insert patches ( patches ) else : self . check patch ( after ) patchlines = self . patchlines before ( after ) patchlines . append ( self . patch2line [ after ] ) for patch in patches : patchline = Patch Line ( patch ) patchlines . append ( patchline ) self . patch2line [ patchline . get patch ( ) ] = patchline patchlines . extend ( self . patchlines after ( after ) ) self . patchlines = patchlines
def remove patch ( self , patch ) : self . check patch ( patch ) patchline = self . patch2line [ patch ] del self . patch2line [ patch ] self . patchlines . remove ( patchline )
def patches after ( self , patch ) : return [ line . get patch ( ) for line in self . patchlines after ( patch ) if line . get patch ( ) ]
def patches before ( self , patch ) : return [ line . get patch ( ) for line in self . patchlines before ( patch ) if line . get patch ( ) ]
def create ( self ) : if not os . path . exists ( self . dirname ) : os . makedirs ( self . dirname ) self . create version ( self . version file )
def import patches ( self , patches ) : dest dir = self . quilt patches patch names = [ ] for patch in patches : patch name = os . path . basename ( patch ) patch file = File ( patch ) dest file = dest dir + File ( patch name ) patch file . copy ( dest file ) patch names . append ( patch name ) self . import patches ( patch names )
def way ( self , w ) : if w . id not in self . way ids : return way points = [ ] for n in w . nodes : try : way points . append ( Point ( n . location . lon , n . location . lat ) ) except o . Invalid Location Error : logging . debug ( 'Invalid Location Error at way %s node %s' , w . id , n . ref ) self . ways [ w . id ] = Way ( w . id , way points )
def missing node ids ( self ) : present node ids = self . nodes . keys ( ) for nid in self . node ids : if nid not in present node ids : yield nid
def node ( self , n ) : if n . id not in self . node ids : return try : self . nodes [ n . id ] = Node ( n . id , n . location . lon , n . location . lat , { t . k : t . v for t in n . tags } ) except o . Invalid Location Error : logging . debug ( 'Invalid Location Error at node %s' , n . id )
def build route ( relation ) : if relation . tags . get ( 'type' ) != 'route' : # Build route only for relations of type `route` return short name = create route short name ( relation ) color = relation . tags . get ( 'color' ) return Route ( relation . id , short name , create route long name ( relation , short name ) , map osm route type to gtfs ( relation . tags . get ( 'route' ) ) , 'https://www.openstreetmap.org/relation/{}' . format ( relation . id ) , color . strip ( '#' ) if color else '' , get agency id ( relation ) )
def create route long name ( relation , short name ) : if relation . tags . get ( 'from' ) and relation . tags . get ( 'to' ) : return "{0}-to-{1}" . format ( relation . tags . get ( 'from' ) , relation . tags . get ( 'to' ) ) name = relation . tags . get ( 'name' ) or relation . tags . get ( 'alt name' ) or "OSM Route No. {}" . format ( relation . id ) # Drop route short name from this one if it contains it if short name and name . startswith ( short name ) : # Drop it return name [ len ( short name ) : ] return name
def get agency id ( relation ) : op = relation . tags . get ( 'operator' ) if op : return int ( hashlib . sha256 ( op . encode ( 'utf-8' ) ) . hexdigest ( ) , 16 ) % 10 ** 8 return - 1
def process ( self ) : # Extract relations self . rh = Relation Handler ( ) self . rh . apply file ( self . filename ) logging . debug ( 'Found %d public transport relations.' , len ( self . rh . relations ) ) # Collect ids of interest node ids , stop node ids , way ids , reverse map = self . collect ids ( ) # Extract nodes self . nh = Node Handler ( node ids ) self . nh . apply file ( self . filename , locations = True ) count = 0 for idx , missing node id in enumerate ( self . nh . missing node ids ) : count += 1 logging . warning ( '[no data] missing stop node. rel: https://osm.org/relation/%s node: https://osm.org/node/%s.' , reverse map [ missing node id ] , missing node id ) if count : logging . warning ( '%d nodes that appear in relations are missing.' , count ) else : logging . debug ( 'Lucky you! All relation member nodes were found.' ) # Extract ways self . wh = Way Handler ( way ids ) self . wh . apply file ( self . filename , locations = True )
def relation ( self , rel ) : rel type = rel . tags . get ( 'type' ) if any ( [ rel . deleted , not rel . visible , not self . is new version ( rel ) , rel type not in [ 'route' , 'public transport' ] ] ) : return route tag = rel . tags . get ( 'route' ) if rel type == 'route' and route tag not in self . transit route types : return public transport = rel . tags . get ( 'public transport' ) if rel type == 'public transport' and public transport != 'stop area' : return self . relations [ rel . id ] = Relation ( rel . id , { 'type' : rel type , 'public transport' : public transport , 'route' : route tag , 'operator' : rel . tags . get ( 'operator' ) , 'color' : rel . tags . get ( 'color' ) , 'ref' : rel . tags . get ( 'ref' ) , 'from' : rel . tags . get ( 'from' ) , 'to' : rel . tags . get ( 'to' ) , 'name' : rel . tags . get ( 'name' ) , 'alt name' : rel . tags . get ( 'alt name' ) , 'url' : rel . tags . get ( 'url' ) , 'contact website' : rel . tags . get ( 'contact:website' ) } , [ ( member . type , member . ref , member . role ) for member in rel . members ] ) self . versions [ rel . id ] = rel . version
def patch agencies ( agencies ) : # First return the unknown agency entry yield Agency ( - 1 , 'http://hiposfer.com' , 'Unknown agency' , 'Europe/Berlin' ) # Then return the rest. for agency id , agency url , agency name , agency timezone in agencies : if not agency url : agency url = 'http://hiposfer.com' if not agency timezone : # Set everything to one time zone to get rid of transitfeeds error. agency timezone = 'Europe/Berlin' yield Agency ( agency id , agency url , agency name , agency timezone )
def create dummy trip stoptimes ( trip id , stops , first service time ) : waiting = datetime . timedelta ( seconds = 30 ) arrival = first service time last departure = first service time last departure hour = ( arrival + waiting ) . hour last stop = None departure hour = None arrival hour = None for stop sequence , stop in enumerate ( stops ) : # Avoid time travels arrival = last departure + get time from last stop ( last stop , stop ) departure = arrival + waiting # Cover the case when the arrival time falls into the next day if arrival . hour < last departure hour : diff = last departure hour arrival hour = arrival . hour + diff departure hour = departure . hour + diff last departure hour = departure . hour + diff else : arrival hour = arrival . hour departure hour = departure . hour last departure hour = departure . hour # Cover the case when adding waiting time to the arrival time # falls into the next day if departure . hour < arrival . hour : diff = last departure hour departure hour = departure . hour + diff last departure hour = departure . hour + diff yield { 'trip id' : trip id , 'arrival time' : '{:02}:{}' . format ( arrival hour , arrival . strftime ( '%M:%S' ) ) , 'departure time' : '{:02}:{}' . format ( departure hour , departure . strftime ( '%M:%S' ) ) , 'stop id' : stop . stop id , 'stop sequence' : stop sequence } last stop = stop last departure = departure
def write zipped ( self , filepath ) : with zipfile . Zip File ( filepath , mode = 'w' , compression = zipfile . ZIP DEFLATED ) as zfile : for name , buffer in self . buffers . items ( ) : encoded values = io . Bytes IO ( buffer . getvalue ( ) . encode ( 'utf-8' ) ) zfile . writestr ( '{}.txt' . format ( name ) , encoded values . getbuffer ( ) ) for name , path in self . files . items ( ) : zfile . write ( path , arcname = name )
def write unzipped ( self , destination ) : for name , buffer in self . buffers . items ( ) : with open ( os . path . join ( destination , '{}.txt' . format ( name ) ) , 'w' , encoding = 'utf-8' ) as file : file . write ( buffer . getvalue ( ) ) for name , path in self . files . items ( ) : shutil . copy ( path , os . path . join ( destination , name ) )
def build agency ( relation , nodes ) : # TODO: find out the operator for routes without operator tag. # See: http://wiki.openstreetmap.org/wiki/Key:operator # Quote from the above link: # #    If the vast majority of a certain object in an area is operated by a certain #    organization and only very few by others then it may be sufficient to only tag the #    exceptions. For example, when nearly all roads in an area are managed by a local #    authority then it would be sufficient to only tag those that are not with an operator #    tag. op = relation . tags . get ( 'operator' ) agency url = relation . tags . get ( 'url' ) or relation . tags . get ( 'contact website' ) if not op : return agency id = int ( hashlib . sha256 ( op . encode ( 'utf8' ) ) . hexdigest ( ) , 16 ) % 10 ** 8 return Agency ( agency id , agency url , op , '' )
def extract stops ( relation , nodes , visited stop ids , stop to station map ) : # member role: stop, halt, platform, terminal, etc. for member type , member id , member role in relation . member info : if member id not in visited stop ids and member id in nodes and member role in ( 'stop' , 'halt' ) : location type = '' visited stop ids . add ( member id ) yield Stop ( member id , nodes [ member id ] . tags . get ( 'name' ) or "Unnamed {} stop." . format ( relation . tags . get ( 'route' ) ) , nodes [ member id ] . lon if member id in nodes else '' , nodes [ member id ] . lat if member id in nodes else '' , relation . id , map wheelchair ( nodes [ member id ] . tags . get ( 'wheelchair' ) ) , location type , stop to station map . get ( member id , '' ) )
def build shape ( relation , nodes , ways ) : sequence index = 0 for member type , member id , member role in relation . member info : if member id in nodes : yield Shape ( relation . id , nodes [ member id ] . lat , nodes [ member id ] . lon , sequence index ) sequence index += 1 # Do we need to consider ways too? It dramatically increases the number of shapes. elif member id in ways : continue #     for point in ways[member id].points: #         shape = Shape( #             relation.id, #             point.lat, #             point.lon, #             sequence index) #         sequence index += 1 else : # Ignore excessive logging for now. pass
def get supported versions ( self ) : if not hasattr ( self , ' versions' ) : try : self . versions = [ self . send apdu ( INS GET VERSION ) . decode ( ) ] except exc . APDU Error as e : # v0 didn't support the instruction. self . versions = [ 'v0' ] if e . code == 0x6d00 else [ ] return self . versions
def send apdu ( self , ins , p1 = 0 , p2 = 0 , data = b'' ) : if data is None : data = b'' elif isinstance ( data , int ) : data = int2byte ( data ) size = len ( data ) l0 = size >> 16 & 0xff l1 = size >> 8 & 0xff l2 = size & 0xff apdu data = struct . pack ( 'B B B B B B B %is B B' % size , 0 , ins , p1 , p2 , l0 , l1 , l2 , data , 0x00 , 0x00 ) try : resp = self . do send apdu ( apdu data ) except Exception as e : # TODO Use six.reraise if/when Six becomes an agreed dependency. raise exc . Device Error ( e ) status = struct . unpack ( '>H' , resp [ - 2 : ] ) [ 0 ] data = resp [ : - 2 ] if status != APDU OK : raise exc . APDU Error ( status ) return data
def register ( devices , params , facet ) : for device in devices [ : ] : try : device . open ( ) except : devices . remove ( device ) sys . stderr . write ( '\n Touch the U2F device you wish to register...\n' ) try : while devices : removed = [ ] for device in devices : try : return u2f . register ( device , params , facet ) except exc . APDU Error as e : if e . code == APDU USE NOT SATISFIED : pass else : removed . append ( device ) except exc . Device Error : removed . append ( device ) devices = [ d for d in devices if d not in removed ] for d in removed : d . close ( ) time . sleep ( 0.25 ) finally : for device in devices : device . close ( ) sys . stderr . write ( '\n Unable to register with any U2F device.\n' ) sys . exit ( 1 )
def u2str ( data ) : if isinstance ( data , dict ) : return { u2str ( k ) : u2str ( v ) for k , v in data . items ( ) } elif isinstance ( data , list ) : return [ u2str ( x ) for x in data ] elif isinstance ( data , text type ) : return data . encode ( 'utf-8' ) else : return data
def wrap function ( func = None , error threshold = None , reraise exception = True , save current stack trace = True ) : # This if/else allows wrap function to behave like a normal decorator when # used like: #         @wrap function #         def some func(): # # However, it also allows wrap function to also be passed keyword arguments # like the following: #         @wrap function(error threshold=3, reraise exception=False) #         def some func(): if func : return flawless . client . client . wrap function with error decorator ( func = func , error threshold = error threshold , reraise exception = reraise exception , save current stack trace = save current stack trace ) else : return functools . partial ( flawless . client . client . wrap function with error decorator , error threshold = error threshold , reraise exception = reraise exception , save current stack trace = save current stack trace )
def get entry ( self , entry , entry tree ) : for e in entry tree [ entry . filename ] : if entry == e : return e
def markdown to re ST ( text ) : # Convert parameters to italics and prepend a newline text = re . sub ( pattern = r"\n       (\w+) - (.+)\n" , repl = r"\n\n       *\g<1>* - \g<2>\n" , string = text ) # Parse [http://url](text), and just leave the url text = re . sub ( pattern = r"\[([^\]]+)\]\([^)]+\)" , repl = r"\g<1>" , string = text ) # Disable formatting of numbered lists text = re . sub ( pattern = r"\n(\d+). " , repl = r"\n\\\g<1>. " , string = text ) return text
def record error ( hostname , exc info , preceding stack = None , error threshold = None , additional info = None ) : stack = [ ] exc type , exc value , sys traceback = exc info while sys traceback is not None : stack . append ( sys traceback ) sys traceback = sys traceback . tb next stack lines = [ ] for row in preceding stack or [ ] : stack lines . append ( api ttypes . Stack Line ( filename = os . path . abspath ( row [ 0 ] ) , line number = row [ 1 ] , function name = row [ 2 ] , text = row [ 3 ] ) ) for index , tb in enumerate ( stack ) : filename = tb . tb frame . f code . co filename func name = tb . tb frame . f code . co name lineno = tb . tb lineno line = linecache . getline ( filename , lineno , tb . tb frame . f globals ) frame locals = None if index >= ( len ( stack ) - NUM FRAMES TO SAVE ) : # Include some limits on max string length & number of variables to keep things from getting # out of hand frame locals = dict ( ( k , myrepr ( k , v ) ) for k , v in list ( tb . tb frame . f locals . items ( ) ) [ : MAX LOCALS ] if k != "self" ) if "self" in tb . tb frame . f locals and hasattr ( tb . tb frame . f locals [ "self" ] , " dict " ) : frame locals . update ( dict ( ( "self." + k , myrepr ( k , v ) ) for k , v in list ( tb . tb frame . f locals [ "self" ] . dict . items ( ) ) [ : MAX LOCALS ] if k != "self" ) ) stack lines . append ( api ttypes . Stack Line ( filename = os . path . abspath ( filename ) , line number = lineno , function name = func name , text = line , frame locals = frame locals ) ) # Check LRU cache & potentially do not send error report if this client has already reported this error # several times. key = Cached Error Info . get hash key ( stack lines ) info = ERROR CACHE . get ( key ) or Cached Error Info ( ) info . increment ( ) ERROR CACHE [ key ] = info if info . should report ( ) : error count = info . mark reported ( ) send request ( api ttypes . Record Error Request ( traceback = stack lines , exception message = repr ( exc value ) , exception type = exc type . module + "." + exc type . name , hostname = hostname , error threshold = error threshold , additional info = additional info , error count = error count , ) )
def url to image ( url ) : r = requests . get ( url ) image = String IO ( r . content ) return image
def string to image ( image string ) : image filelike = String IO ( image string ) image = Image . open ( image filelike ) return image
def is big enough ( image , size ) : if ( size [ 0 ] > image . size [ 0 ] ) and ( size [ 1 ] > image . size [ 1 ] ) : raise Image Size Error ( image . size , size )
def width is big enough ( image , width ) : if width > image . size [ 0 ] : raise Image Size Error ( image . size [ 0 ] , width )
def height is big enough ( image , height ) : if height > image . size [ 1 ] : raise Image Size Error ( image . size [ 1 ] , height )
def parse category ( self , item , field name , source name ) : # Get and checks for the corresponding slug slug = category map . get ( self . get value ( item , source name ) , None ) if not slug : return None # Load the category instance try : return Category . objects . get ( slug = slug ) except Category . Does Not Exist : pass
def parse totals ( self , item , field name , source name ) : val = self . get value ( item , source name ) try : return int ( val ) except : return 0
def get items ( self ) : # Use `iterparse`, it's more efficient, specially for big files for event , item in Element Tree . iterparse ( self . source ) : if item . tag == self . item tag name : yield item # Releases the item from memory item . clear ( )
def save error ( self , data , exception info ) : # TODO: what to do with errors? Let it flow? Write to a log file? self . errors . append ( { 'data' : data , 'exception' : '' . join ( format exception ( * exception info ) ) , } )
def parse ( self ) : # Checks if the source is loaded if not self . loaded : self . load ( self . source ) for item in self . get items ( ) : # Parse the fields from the source into a dict data = self . parse item ( item ) # Get the instance from the DB, or a new one instance = self . get instance ( data ) # Feed instance with data self . feed instance ( data , instance ) # Try to save the instance or keep the error try : self . save item ( item , data , instance ) except Exception as e : self . save error ( data , sys . exc info ( ) ) # Unload the source self . unload ( )
def parse item ( self , item ) : # Create a dictionary from values for each field parsed data = { } for field name in self . fields : # A field-name may be mapped to another identifier on the source, # it could be a XML path or a CSV column name / position. # Defaults to the field-name itself. source name = self . field map . get ( field name , field name ) # Uses a custom method "parse %(field name)" # or get the value from the item parse = getattr ( self , 'parse %s' % field name , None ) if parse : value = parse ( item , field name , source name ) else : value = self . get value ( item , source name ) # Add the value to the parsed data parsed data [ field name ] = value return parsed data
def get instance ( self , data ) : # Get unique fields unique fields = self . unique fields # If there are no unique fields option, all items are new if not unique fields : return self . model ( ) # Build the filter filter = dict ( [ ( f , data [ f ] ) for f in unique fields ] ) # Get the instance from the DB or use a new instance try : instance = self . model . default manager . get ( * * filter ) except self . model . Does Not Exist : return self . model ( ) return instance
def save item ( self , item , data , instance , commit = True ) : if commit : instance . save ( ) return instance
def load ( self , source ) : self . source = open ( self . source , 'rb' ) self . loaded = True
def get items ( self ) : # Get the csv reader reader = csv . reader ( self . source ) # Get the headers from the first line headers = reader . next ( ) # Read each line yielding a dictionary mapping # the column headers to the row values for row in reader : # Skip empty rows if not row : continue yield dict ( zip ( headers , row ) )
def allow network access ( self , value : bool ) : if self . is running : raise Value Error ( "Cannot change network access settings on a running sandbox" ) self . allow network access = value
def get enrollments for course by sis id ( self , sis course id , params = { } ) : return self . get enrollments for course ( self . sis id ( sis course id , sis field = "course" ) , params )
def get enrollments for section by sis id ( self , sis section id , params = { } ) : return self . get enrollments for section ( self . sis id ( sis section id , sis field = "section" ) , params )
def get roles by account sis id ( self , account sis id , params = { } ) : return self . get roles in account ( self . sis id ( account sis id , sis field = "account" ) , params )
def get role by account sis id ( self , account sis id , role id ) : return self . get role ( self . sis id ( account sis id , sis field = "account" ) , role id )
def get course by sis id ( self , sis course id , params = { } ) : return self . get course ( self . sis id ( sis course id , sis field = "course" ) , params )
def get courses in account by sis id ( self , sis account id , params = { } ) : return self . get courses in account ( self . sis id ( sis account id , sis field = "account" ) , params )
def get published courses in account ( self , account id , params = { } ) : params [ "published" ] = True return self . get courses in account ( account id , params )
def get published courses in account by sis id ( self , sis account id , params = { } ) : return self . get published courses in account ( self . sis id ( sis account id , sis field = "account" ) , params )
def get users for course ( self , course id , params = { } ) : url = COURSES API . format ( course id ) + "/users" data = self . get paged resource ( url , params = params ) users = [ ] for datum in data : users . append ( Canvas User ( data = datum ) ) return users
def get users for sis course id ( self , sis course id , params = { } ) : return self . get users for course ( self . sis id ( sis course id , sis field = "course" ) , params )
def next page ( self , response ) : for link in response . getheader ( "link" , "" ) . split ( "," ) : try : ( url , rel ) = link . split ( ";" ) if "next" in rel : return url . lstrip ( "<" ) . rstrip ( ">" ) except Exception : return
def get resource ( self , url , params = None , data key = None ) : if not params : params = { } self . set as user ( params ) full url = url + self . params ( params ) return self . get resource url ( full url , True , data key )
def put resource ( self , url , body ) : params = { } self . set as user ( params ) headers = { 'Content-Type' : 'application/json' , 'Accept' : 'application/json' , 'Connection' : 'keep-alive' } url = url + self . params ( params ) response = DAO . put URL ( url , headers , json . dumps ( body ) ) if not ( response . status == 200 or response . status == 201 or response . status == 204 ) : raise Data Failure Exception ( url , response . status , response . data ) return json . loads ( response . data )
def post resource ( self , url , body ) : params = { } self . set as user ( params ) headers = { 'Content-Type' : 'application/json' , 'Accept' : 'application/json' , 'Connection' : 'keep-alive' } url = url + self . params ( params ) response = DAO . post URL ( url , headers , json . dumps ( body ) ) if not ( response . status == 200 or response . status == 204 ) : raise Data Failure Exception ( url , response . status , response . data ) return json . loads ( response . data )
def delete resource ( self , url ) : params = { } self . set as user ( params ) headers = { 'Accept' : 'application/json' , 'Connection' : 'keep-alive' } url = url + self . params ( params ) response = DAO . delete URL ( url , headers ) if not ( response . status == 200 or response . status == 204 ) : raise Data Failure Exception ( url , response . status , response . data ) return response
def create admin by sis id ( self , sis account id , user id , role ) : return self . create admin ( self . sis id ( sis account id ) , user id , role )
def delete admin by sis id ( self , sis account id , user id , role ) : return self . delete admin ( self . sis id ( sis account id ) , user id , role )
def get section by sis id ( self , sis section id , params = { } ) : return self . get section ( self . sis id ( sis section id , sis field = "section" ) , params )
def get sections in course by sis id ( self , sis course id , params = { } ) : return self . get sections in course ( self . sis id ( sis course id , sis field = "course" ) , params )
def get sections with students in course ( self , course id , params = { } ) : include = params . get ( "include" , [ ] ) if "students" not in include : include . append ( "students" ) params [ "include" ] = include return self . get sections in course ( course id , params )
def get sections with students in course by sis id ( self , sis course id , params = { } ) : return self . get sections with students in course ( self . sis id ( sis course id , sis field = "course" ) , params )
def get term by sis id ( self , sis term id ) : for term in self . get all terms ( ) : if term . sis term id == sis term id : return term
def build archive ( self , dir path ) : zip path = os . path . join ( dir path , "import.zip" ) archive = zipfile . Zip File ( zip path , "w" ) for filename in CSV FILES : filepath = os . path . join ( dir path , filename ) if os . path . exists ( filepath ) : archive . write ( filepath , filename , zipfile . ZIP DEFLATED ) archive . close ( ) with open ( zip path , "rb" ) as f : body = f . read ( ) return body
def get report data ( self , report ) : if report . report id is None or report . status is None : raise Report Failure Exception ( report ) interval = getattr ( settings , 'CANVAS REPORT POLLING INTERVAL' , 5 ) while report . status != "complete" : if report . status == "error" : raise Report Failure Exception ( report ) sleep ( interval ) report = self . get report status ( report ) if report . attachment is None or report . attachment . url is None : return data = self . get report file ( report . attachment . url ) return data . split ( "\n" )
def empty value ( self ) : edit empty value = self . config . get ( 'edit empty value' , False ) if edit empty value : return edit empty value else : return unicode ( inplace settings . INPLACEEDIT EDIT EMPTY VALUE )
def create metrics ( self , metric configs : Iterable [ Metric Config ] ) -> Dict [ str , Metric ] : return self . registry . create metrics ( metric configs )
def configure registry ( self , include process stats : bool = False ) : if include process stats : self . registry . register additional collector ( Process Collector ( registry = None ) )
def create metrics ( self , configs : Iterable [ Metric Config ] ) -> Dict [ str , Metric ] : metrics : Dict [ str , Metric ] = { config . name : self . register metric ( config ) for config in configs } self . metrics . update ( metrics ) return metrics
def get metric ( self , name : str , labels : Union [ Dict [ str , str ] , None ] = None ) -> Metric : metric = self . metrics [ name ] if labels : return metric . labels ( * * labels ) return metric
async def handle home ( self , request : Request ) -> Response : if self . description : title = f'{self.name} - {self.description}' else : title = self . name text = dedent ( ) return Response ( content type = 'text/html' , text = text )
async def handle metrics ( self , request : Request ) -> Response : if self . update handler : await self . update handler ( self . registry . get metrics ( ) ) response = Response ( body = self . registry . generate metrics ( ) ) response . content type = CONTENT TYPE LATEST return response
def info ( self ) : return itertools . chain ( self . pods , self . assumptions , self . warnings )
def results ( self ) : return ( pod for pod in self . pods if pod . primary or pod . title == 'Result' )
def vector ( members : Iterable [ T ] , meta : Optional [ I Persistent Map ] = None ) -> Vector [ T ] : return Vector ( pvector ( members ) , meta = meta )
def v ( * members : T , meta : Optional [ I Persistent Map ] = None ) -> Vector [ T ] : return Vector ( pvector ( members ) , meta = meta )
def eval file ( filename : str , ctx : compiler . Compiler Context , module : types . Module Type ) : last = None for form in reader . read file ( filename , resolver = runtime . resolve alias ) : last = compiler . compile and exec form ( form , ctx , module ) return last
def eval stream ( stream , ctx : compiler . Compiler Context , module : types . Module Type ) : last = None for form in reader . read ( stream , resolver = runtime . resolve alias ) : last = compiler . compile and exec form ( form , ctx , module ) return last
def eval str ( s : str , ctx : compiler . Compiler Context , module : types . Module Type , eof : Any ) : last = eof for form in reader . read str ( s , resolver = runtime . resolve alias , eof = eof ) : last = compiler . compile and exec form ( form , ctx , module ) return last
def run ( # pylint: disable=too-many-arguments file or code , code , in ns , use var indirection , warn on shadowed name , warn on shadowed var , warn on var indirection , ) : basilisp . init ( ) ctx = compiler . Compiler Context ( filename = CLI INPUT FILE PATH if code else ( STDIN INPUT FILE PATH if file or code == STDIN FILE NAME else file or code ) , opts = { compiler . WARN ON SHADOWED NAME : warn on shadowed name , compiler . WARN ON SHADOWED VAR : warn on shadowed var , compiler . USE VAR INDIRECTION : use var indirection , compiler . WARN ON VAR INDIRECTION : warn on var indirection , } , ) eof = object ( ) with runtime . ns bindings ( in ns ) as ns : if code : print ( runtime . lrepr ( eval str ( file or code , ctx , ns . module , eof ) ) ) elif file or code == STDIN FILE NAME : print ( runtime . lrepr ( eval stream ( click . get text stream ( "stdin" ) , ctx , ns . module ) ) ) else : print ( runtime . lrepr ( eval file ( file or code , ctx , ns . module ) ) )
def multifn ( dispatch : Dispatch Function , default = None ) -> Multi Function [ T ] : name = sym . symbol ( dispatch . qualname , ns = dispatch . module ) return Multi Function ( name , dispatch , default )
def add method ( m : lmap . Map , key : T , method : Method ) -> lmap . Map : return m . assoc ( key , method )
def remove method ( m : lmap . Map , key : T ) -> lmap . Map : return m . dissoc ( key )
def remove method ( self , key : T ) -> Optional [ Method ] : method = self . methods . entry ( key , None ) if method : self . methods . swap ( Multi Function . remove method , key ) return method
def is macro ( v : Var ) -> bool : return ( Maybe ( v . meta ) . map ( lambda m : m . entry ( SYM MACRO META KEY , None ) ) # type: ignore . or else get ( False ) )
def clean meta ( meta : Optional [ lmap . Map ] ) -> Optional [ lmap . Map ] : if meta is None : return None else : new meta = meta . dissoc ( reader . READER LINE KW , reader . READER COL KW ) return None if len ( new meta ) == 0 else new meta
def deftype impls ( # pylint: disable=too-many-branches ctx : Parser Context , form : I Seq ) -> Tuple [ List [ Def Type Base ] , List [ Method ] ] : current interface sym : Optional [ sym . Symbol ] = None current interface : Optional [ Def Type Base ] = None interfaces = [ ] methods : List [ Method ] = [ ] interface methods : Mutable Mapping [ sym . Symbol , List [ Method ] ] = { } for elem in form : if isinstance ( elem , sym . Symbol ) : if current interface is not None : if current interface sym in interface methods : raise Parser Exception ( f"deftype* forms may only implement an interface once" , form = elem , ) assert ( current interface sym is not None ) , "Symbol must be defined with interface" interface methods [ current interface sym ] = methods current interface sym = elem current interface = parse ast ( ctx , elem ) methods = [ ] if not isinstance ( current interface , ( Maybe Class , Maybe Host Form , Var Ref ) ) : raise Parser Exception ( f"deftype* interface implementation must be an existing interface" , form = elem , ) interfaces . append ( current interface ) elif isinstance ( elem , I Seq ) : if current interface is None : raise Parser Exception ( f"deftype* method cannot be declared without interface" , form = elem ) methods . append ( deftype method ( ctx , elem , current interface ) ) else : raise Parser Exception ( f"deftype* must consist of interface or protocol names and methods" , form = elem , ) if current interface is not None : if len ( methods ) > 0 : if current interface sym in interface methods : raise Parser Exception ( f"deftype* forms may only implement an interface once" , form = current interface sym , ) assert ( current interface sym is not None ) , "Symbol must be defined with interface" interface methods [ current interface sym ] = methods else : raise Parser Exception ( f"deftype* may not declare interface without at least one method" , form = current interface sym , ) return interfaces , list ( chain . from iterable ( interface methods . values ( ) ) )
def resolve namespaced symbol ( # pylint: disable=too-many-branches ctx : Parser Context , form : sym . Symbol ) -> Union [ Maybe Class , Maybe Host Form , Var Ref ] : assert form . ns is not None if form . ns == ctx . current ns . name : v = ctx . current ns . find ( sym . symbol ( form . name ) ) if v is not None : return Var Ref ( form = form , var = v , env = ctx . get node env ( ) ) elif form . ns == BUILTINS NS : class = munge ( form . name , allow builtins = True ) target = getattr ( builtins , class , None ) if target is None : raise Parser Exception ( f"cannot resolve builtin function '{class }'" , form = form ) return Maybe Class ( form = form , class = class , target = target , env = ctx . get node env ( ) ) if "." in form . name : raise Parser Exception ( "symbol names may not contain the '.' operator" , form = form ) ns sym = sym . symbol ( form . ns ) if ns sym in ctx . current ns . imports or ns sym in ctx . current ns . import aliases : # We still import Basilisp code, so we'll want to make sure # that the symbol isn't referring to a Basilisp Var first v = Var . find ( form ) if v is not None : return Var Ref ( form = form , var = v , env = ctx . get node env ( ) ) # Fetch the full namespace name for the aliased namespace/module. # We don't need this for actually generating the link later, but # we  do  need it for fetching a reference to the module to check # for membership. if ns sym in ctx . current ns . import aliases : ns = ctx . current ns . import aliases [ ns sym ] assert ns is not None ns name = ns . name else : ns name = ns sym . name safe module name = munge ( ns name ) assert ( safe module name in sys . modules ) , f"Module '{safe module name}' is not imported" ns module = sys . modules [ safe module name ] safe name = munge ( form . name ) # Try without allowing builtins first if safe name in vars ( ns module ) : return Maybe Host Form ( form = form , class = munge ( ns sym . name ) , field = safe name , target = vars ( ns module ) [ safe name ] , env = ctx . get node env ( ) , ) # Then allow builtins safe name = munge ( form . name , allow builtins = True ) if safe name not in vars ( ns module ) : raise Parser Exception ( "can't identify aliased form" , form = form ) # Aliased imports generate code which uses the import alias, so we # don't need to care if this is an import or an alias. return Maybe Host Form ( form = form , class = munge ( ns sym . name ) , field = safe name , target = vars ( ns module ) [ safe name ] , env = ctx . get node env ( ) , ) elif ns sym in ctx . current ns . aliases : aliased ns : runtime . Namespace = ctx . current ns . aliases [ ns sym ] v = Var . find ( sym . symbol ( form . name , ns = aliased ns . name ) ) if v is None : raise Parser Exception ( f"unable to resolve symbol '{sym.symbol(form.name, ns sym.name)}' in this context" , form = form , ) return Var Ref ( form = form , var = v , env = ctx . get node env ( ) ) else : raise Parser Exception ( f"unable to resolve symbol '{form}' in this context" , form = form )
def resolve sym ( ctx : Parser Context , form : sym . Symbol ) -> Union [ Maybe Class , Maybe Host Form , Var Ref ] : # Support special class-name syntax to instantiate new classes #   (Classname. *args) #   (aliased.Classname. *args) #   (fully.qualified.Classname. *args) if form . ns is None and form . name . endswith ( "." ) : try : ns , name = form . name [ : - 1 ] . rsplit ( "." , maxsplit = 1 ) form = sym . symbol ( name , ns = ns ) except Value Error : form = sym . symbol ( form . name [ : - 1 ] ) if form . ns is not None : return resolve namespaced symbol ( ctx , form ) else : return resolve bare symbol ( ctx , form )
def bootstrap module ( gctx : Generator Context , optimizer : Python AST Optimizer , mod : types . Module Type , collect bytecode : Optional [ Bytecode Collector ] = None , ) -> None : incremental compile module ( optimizer , py module preamble ( gctx ) , mod , source filename = gctx . filename , collect bytecode = collect bytecode , ) mod . basilisp bootstrapped = True
def sequence ( s : Iterable ) -> I Seq [ Any ] : try : i = iter ( s ) return Sequence ( i , next ( i ) ) except Stop Iteration : return EMPTY
def fraction ( numerator : int , denominator : int ) -> Fraction : return Fraction ( numerator = numerator , denominator = denominator )
def get handler ( level : str , fmt : str ) -> logging . Handler : handler : logging . Handler = logging . Null Handler ( ) if os . getenv ( "BASILISP USE DEV LOGGER" ) == "true" : handler = logging . Stream Handler ( ) handler . set Formatter ( logging . Formatter ( fmt ) ) handler . set Level ( level ) return handler
def map ( kvs : Mapping [ K , V ] , meta = None ) -> Map [ K , V ] : # pylint:disable=redefined-builtin return Map ( pmap ( initial = kvs ) , meta = meta )
def partition ( coll , n : int ) : assert n > 0 start = 0 stop = n while stop <= len ( coll ) : yield tuple ( e for e in coll [ start : stop ] ) start += n stop += n if start < len ( coll ) < stop : stop = len ( coll ) yield tuple ( e for e in coll [ start : stop ] )
def read namespaced ( ctx : Reader Context , allowed suffix : Optional [ str ] = None ) -> Tuple [ Optional [ str ] , str ] : ns : List [ str ] = [ ] name : List [ str ] = [ ] reader = ctx . reader has ns = False while True : token = reader . peek ( ) if token == "/" : reader . next token ( ) if has ns : raise Syntax Error ( "Found '/'; expected word character" ) elif len ( name ) == 0 : name . append ( "/" ) else : if "/" in name : raise Syntax Error ( "Found '/' after '/'" ) has ns = True ns = name name = [ ] elif ns name chars . match ( token ) : reader . next token ( ) name . append ( token ) elif allowed suffix is not None and token == allowed suffix : reader . next token ( ) name . append ( token ) else : break ns str = None if not has ns else "" . join ( ns ) name str = "" . join ( name ) # A small exception for the symbol '/ used for division if ns str is None : if "/" in name str and name str != "/" : raise Syntax Error ( "'/' character disallowed in names" ) assert ns str is None or len ( ns str ) > 0 return ns str , name str
def read list ( ctx : Reader Context ) -> llist . List : start = ctx . reader . advance ( ) assert start == "(" return read coll ( ctx , llist . list , ")" , "list" )
def read vector ( ctx : Reader Context ) -> vector . Vector : start = ctx . reader . advance ( ) assert start == "[" return read coll ( ctx , vector . vector , "]" , "vector" )
def read set ( ctx : Reader Context ) -> lset . Set : start = ctx . reader . advance ( ) assert start == "{" def set if valid ( s : Collection ) -> lset . Set : if len ( s ) != len ( set ( s ) ) : raise Syntax Error ( "Duplicated values in set" ) return lset . set ( s ) return read coll ( ctx , set if valid , "}" , "set" )
def read map ( ctx : Reader Context ) -> lmap . Map : reader = ctx . reader start = reader . advance ( ) assert start == "{" d : Mutable Mapping [ Any , Any ] = { } while True : if reader . peek ( ) == "}" : reader . next token ( ) break k = read next ( ctx ) if k is COMMENT : continue while True : if reader . peek ( ) == "}" : raise Syntax Error ( "Unexpected token '}'; expected map value" ) v = read next ( ctx ) if v is COMMENT : continue if k in d : raise Syntax Error ( f"Duplicate key '{k}' in map literal" ) break d [ k ] = v return lmap . map ( d )
def read kw ( ctx : Reader Context ) -> keyword . Keyword : start = ctx . reader . advance ( ) assert start == ":" ns , name = read namespaced ( ctx ) if "." in name : raise Syntax Error ( "Found '.' in keyword name" ) return keyword . keyword ( name , ns = ns )
def read function ( ctx : Reader Context ) -> llist . List : if ctx . is in anon fn : raise Syntax Error ( f"Nested #() definitions not allowed" ) with ctx . in anon fn ( ) : form = read list ( ctx ) arg set = set ( ) def arg suffix ( arg num ) : if arg num is None : return "1" elif arg num == "&" : return "rest" else : return arg num def sym replacement ( arg num ) : suffix = arg suffix ( arg num ) return symbol . symbol ( f"arg-{suffix}" ) def identify and replace ( f ) : if isinstance ( f , symbol . Symbol ) : if f . ns is None : match = fn macro args . match ( f . name ) if match is not None : arg num = match . group ( 2 ) suffix = arg suffix ( arg num ) arg set . add ( suffix ) return sym replacement ( arg num ) return f body = walk . postwalk ( identify and replace , form ) if len ( form ) > 0 else None arg list : List [ symbol . Symbol ] = [ ] numbered args = sorted ( map ( int , filter ( lambda k : k != "rest" , arg set ) ) ) if len ( numbered args ) > 0 : max arg = max ( numbered args ) arg list = [ sym replacement ( str ( i ) ) for i in range ( 1 , max arg + 1 ) ] if "rest" in arg set : arg list . append ( AMPERSAND ) arg list . append ( sym replacement ( "rest" ) ) return llist . l ( FN , vector . vector ( arg list ) , body )
def read quoted ( ctx : Reader Context ) -> llist . List : start = ctx . reader . advance ( ) assert start == "'" next form = read next consuming comment ( ctx ) return llist . l ( QUOTE , next form )
def read syntax quoted ( ctx : Reader Context ) -> Reader Form : start = ctx . reader . advance ( ) assert start == "`" with ctx . syntax quoted ( ) : return process syntax quoted form ( ctx , read next consuming comment ( ctx ) )
def read deref ( ctx : Reader Context ) -> Lisp Form : start = ctx . reader . advance ( ) assert start == "@" next form = read next consuming comment ( ctx ) return llist . l ( DEREF , next form )
def read regex ( ctx : Reader Context ) -> Pattern : s = read str ( ctx , allow arbitrary escapes = True ) try : return langutil . regex from str ( s ) except re . error : raise Syntax Error ( f"Unrecognized regex pattern syntax: {s}" )
def read next ( ctx : Reader Context ) -> Lisp Reader Form : # noqa: C901 reader = ctx . reader token = reader . peek ( ) if token == "(" : return read list ( ctx ) elif token == "[" : return read vector ( ctx ) elif token == "{" : return read map ( ctx ) elif begin num chars . match ( token ) : return read num ( ctx ) elif whitespace chars . match ( token ) : reader . next token ( ) return read next ( ctx ) elif token == ":" : return read kw ( ctx ) elif token == '"' : return read str ( ctx ) elif token == "'" : return read quoted ( ctx ) elif token == "\\" : return read character ( ctx ) elif ns name chars . match ( token ) : return read sym ( ctx ) elif token == "#" : return read reader macro ( ctx ) elif token == "^" : return read meta ( ctx ) # type: ignore elif token == ";" : return read comment ( ctx ) elif token == "`" : return read syntax quoted ( ctx ) elif token == "~" : return read unquote ( ctx ) elif token == "@" : return read deref ( ctx ) elif token == "" : return ctx . eof else : raise Syntax Error ( "Unexpected token '{token}'" . format ( token = token ) )
def basilisp bytecode ( mtime : int , source size : int , code : List [ types . Code Type ] ) -> bytes : data = bytearray ( MAGIC NUMBER ) data . extend ( w long ( mtime ) ) data . extend ( w long ( source size ) ) data . extend ( marshal . dumps ( code ) ) # type: ignore return data
def exec cached module ( self , fullname : str , loader state : Mapping [ str , str ] , path stats : Mapping [ str , int ] , module : types . Module Type , ) : filename = loader state [ "filename" ] cache filename = loader state [ "cache filename" ] with timed ( lambda duration : logger . debug ( f"Loaded cached Basilisp module '{fullname}' in {duration / 1000000}ms" ) ) : logger . debug ( f"Checking for cached Basilisp module '{fullname}''" ) cache data = self . get data ( cache filename ) cached code = get basilisp bytecode ( fullname , path stats [ "mtime" ] , path stats [ "size" ] , cache data ) compiler . compile bytecode ( cached code , compiler . Generator Context ( filename = filename ) , compiler . Python AST Optimizer ( ) , module , )
def exec module ( self , fullname : str , loader state : Mapping [ str , str ] , path stats : Mapping [ str , int ] , module : types . Module Type , ) : filename = loader state [ "filename" ] cache filename = loader state [ "cache filename" ] with timed ( lambda duration : logger . debug ( f"Loaded Basilisp module '{fullname}' in {duration / 1000000}ms" ) ) : # During compilation, bytecode objects are added to the list via the closure # add bytecode below, which is passed to the compiler. The collected bytecodes # will be used to generate an .lpyc file for caching the compiled file. all bytecode = [ ] def add bytecode ( bytecode : types . Code Type ) : all bytecode . append ( bytecode ) logger . debug ( f"Reading and compiling Basilisp module '{fullname}'" ) forms = reader . read file ( filename , resolver = runtime . resolve alias ) compiler . compile module ( # pylint: disable=unexpected-keyword-arg forms , compiler . Compiler Context ( filename = filename ) , module , collect bytecode = add bytecode , ) # Cache the bytecode that was collected through the compilation run. cache file bytes = basilisp bytecode ( path stats [ "mtime" ] , path stats [ "size" ] , all bytecode ) self . cache bytecode ( filename , cache filename , cache file bytes )
def symbol ( name : str , ns : Optional [ str ] = None , meta = None ) -> Symbol : return Symbol ( name , ns = ns , meta = meta )
def complete ( text : str , kw cache : atom . Atom [ "P Map[int, Keyword]" ] = INTERN ) -> Iterable [ str ] : assert text . startswith ( ":" ) interns = kw cache . deref ( ) text = text [ 1 : ] if "/" in text : prefix , suffix = text . split ( "/" , maxsplit = 1 ) results = filter ( lambda kw : ( kw . ns is not None and kw . ns == prefix ) and kw . name . startswith ( suffix ) , interns . itervalues ( ) , ) else : results = filter ( lambda kw : kw . name . startswith ( text ) or ( kw . ns is not None and kw . ns . startswith ( text ) ) , interns . itervalues ( ) , ) return map ( str , results )
def keyword ( name : str , ns : Optional [ str ] = None , kw cache : atom . Atom [ "P Map[int, Keyword]" ] = INTERN , ) -> Keyword : h = hash ( ( name , ns ) ) return kw cache . swap ( get or create , h , name , ns ) [ h ]
def chain py ast ( * genned : Generated Py AST , ) -> Tuple [ Py AST Stream , Py AST Stream ] : deps = chain . from iterable ( map ( lambda n : n . dependencies , genned ) ) nodes = map ( lambda n : n . node , genned ) return deps , nodes
def simple ast generator ( gen ast ) : @ wraps ( gen ast ) def wrapped ast generator ( ctx : Generator Context , form : Lisp Form ) -> Generated Py AST : return Generated Py AST ( node = gen ast ( ctx , form ) ) return wrapped ast generator
def collection ast ( ctx : Generator Context , form : Iterable [ Node ] ) -> Tuple [ Py AST Stream , Py AST Stream ] : return chain py ast ( * map ( partial ( gen py ast , ctx ) , form ) )
def clean meta ( form : I Meta ) -> Lisp Form : assert form . meta is not None , "Form must have non-null 'meta' attribute" meta = form . meta . dissoc ( reader . READER LINE KW , reader . READER COL KW ) if len ( meta ) == 0 : return None return cast ( lmap . Map , meta )
def is redefable ( v : Var ) -> bool : return ( Maybe ( v . meta ) . map ( lambda m : m . get ( SYM REDEF META KEY , None ) ) # type: ignore . or else get ( False ) )
def should warn on redef ( ctx : Generator Context , defsym : sym . Symbol , safe name : str , def meta : lmap . Map ) -> bool : no warn on redef = def meta . entry ( SYM NO WARN ON REDEF META KEY , False ) if no warn on redef : return False elif safe name in ctx . current ns . module . dict : return True elif defsym in ctx . current ns . interns : var = ctx . current ns . find ( defsym ) assert var is not None , f"Var {defsym} cannot be none here" if var . meta is not None and var . meta . entry ( SYM REDEF META KEY ) : return False elif var . is bound : return True else : return False else : return False
def def to py ast ( # pylint: disable=too-many-branches ctx : Generator Context , node : Def ) -> Generated Py AST : assert node . op == Node Op . DEF defsym = node . name is defn = False if node . init is not None : # Since Python function definitions always take the form `def name(...):`, # it is redundant to assign them to the their final name after they have # been defined under a private alias. This codepath generates `defn` # declarations by directly generating the Python `def` with the correct # function name and short-circuiting the default double-declaration. if node . init . op == Node Op . FN : assert isinstance ( node . init , Fn ) def ast = fn to py ast ( ctx , node . init , def name = defsym . name ) is defn = True elif ( node . init . op == Node Op . WITH META and isinstance ( node . init , With Meta ) and node . init . expr . op == Node Op . FN ) : assert isinstance ( node . init , With Meta ) def ast = with meta to py ast ( ctx , node . init , def name = defsym . name ) is defn = True else : def ast = gen py ast ( ctx , node . init ) else : def ast = Generated Py AST ( node = ast . Name Constant ( None ) ) ns name = ast . Call ( func = NEW SYM FN NAME , args = [ NS VAR NAME ] , keywords = [ ] ) def name = ast . Call ( func = NEW SYM FN NAME , args = [ ast . Str ( defsym . name ) ] , keywords = [ ] ) safe name = munge ( defsym . name ) assert node . meta is not None , "Meta should always be attached to Def nodes" def meta = node . meta . form assert isinstance ( def meta , lmap . Map ) , "Meta should always be a map" # If the Var is marked as dynamic, we need to generate a keyword argument # for the generated Python code to set the Var as dynamic is dynamic = def meta . entry ( SYM DYNAMIC META KEY , False ) dynamic kwarg = ( [ ast . keyword ( arg = "dynamic" , value = ast . Name Constant ( is dynamic ) ) ] if is dynamic else [ ] ) # Warn if this symbol is potentially being redefined if should warn on redef ( ctx , defsym , safe name , def meta ) : logger . warning ( f"redefining local Python name '{safe name}' in module '{ctx.current ns.module. name }'" ) meta ast = gen py ast ( ctx , node . meta ) # For defn style def generation, we specifically need to generate the # global declaration prior to emitting the Python `def` otherwise the # Python compiler will throw an exception during compilation # complaining that we assign the value prior to global declaration. if is defn : def dependencies = list ( chain ( [ ] if node . top level else [ ast . Global ( names = [ safe name ] ) ] , def ast . dependencies , [ ] if meta ast is None else meta ast . dependencies , ) ) else : def dependencies = list ( chain ( def ast . dependencies , [ ] if node . top level else [ ast . Global ( names = [ safe name ] ) ] , [ ast . Assign ( targets = [ ast . Name ( id = safe name , ctx = ast . Store ( ) ) ] , value = def ast . node , ) ] , [ ] if meta ast is None else meta ast . dependencies , ) ) return Generated Py AST ( node = ast . Call ( func = INTERN VAR FN NAME , args = [ ns name , def name , ast . Name ( id = safe name , ctx = ast . Load ( ) ) ] , keywords = list ( chain ( dynamic kwarg , [ ] if meta ast is None else [ ast . keyword ( arg = "meta" , value = meta ast . node ) ] , ) ) , ) , dependencies = def dependencies , )
def deftype to py ast ( # pylint: disable=too-many-branches ctx : Generator Context , node : Def Type ) -> Generated Py AST : assert node . op == Node Op . DEFTYPE type name = munge ( node . name ) ctx . symbol table . new symbol ( sym . symbol ( node . name ) , type name , Local Type . DEFTYPE ) bases = [ ] for base in node . interfaces : base node = gen py ast ( ctx , base ) assert ( count ( base node . dependencies ) == 0 ) , "Class and host form nodes do not have dependencies" bases . append ( base node . node ) decorator = ast . Call ( func = ATTR CLASS DECORATOR NAME , args = [ ] , keywords = [ ast . keyword ( arg = "cmp" , value = ast . Name Constant ( False ) ) , ast . keyword ( arg = "frozen" , value = ast . Name Constant ( node . is frozen ) ) , ast . keyword ( arg = "slots" , value = ast . Name Constant ( True ) ) , ] , ) with ctx . new symbol table ( node . name ) : type nodes = [ ] for field in node . fields : safe field = munge ( field . name ) type nodes . append ( ast . Assign ( targets = [ ast . Name ( id = safe field , ctx = ast . Store ( ) ) ] , value = ast . Call ( func = ATTRIB FIELD FN NAME , args = [ ] , keywords = [ ] ) , ) ) ctx . symbol table . new symbol ( sym . symbol ( field . name ) , safe field , field . local ) type deps : List [ ast . AST ] = [ ] for method in node . methods : type ast = deftype method to py ast ( ctx , method ) type nodes . append ( type ast . node ) type deps . extend ( type ast . dependencies ) return Generated Py AST ( node = ast . Name ( id = type name , ctx = ast . Load ( ) ) , dependencies = list ( chain ( type deps , [ ast . Class Def ( name = type name , bases = bases , keywords = [ ] , body = type nodes , decorator list = [ decorator ] , ) ] , ) ) , )
def do to py ast ( ctx : Generator Context , node : Do ) -> Generated Py AST : assert node . op == Node Op . DO assert not node . is body body ast = Generated Py AST . reduce ( * map ( partial ( gen py ast , ctx ) , chain ( node . statements , [ node . ret ] ) ) ) fn body ast : List [ ast . AST ] = [ ] do result name = genname ( DO PREFIX ) fn body ast . extend ( map ( statementize , body ast . dependencies ) ) fn body ast . append ( ast . Assign ( targets = [ ast . Name ( id = do result name , ctx = ast . Store ( ) ) ] , value = body ast . node ) ) return Generated Py AST ( node = ast . Name ( id = do result name , ctx = ast . Load ( ) ) , dependencies = fn body ast )
def fn args to py ast ( ctx : Generator Context , params : Iterable [ Binding ] , body : Do ) -> Tuple [ List [ ast . arg ] , Optional [ ast . arg ] , List [ ast . AST ] ] : fn args , varg = [ ] , None fn body ast : List [ ast . AST ] = [ ] for binding in params : assert binding . init is None , ":fn nodes cannot have bindint :inits" assert varg is None , "Must have at most one variadic arg" arg name = genname ( munge ( binding . name ) ) if not binding . is variadic : fn args . append ( ast . arg ( arg = arg name , annotation = None ) ) ctx . symbol table . new symbol ( sym . symbol ( binding . name ) , arg name , Local Type . ARG ) else : varg = ast . arg ( arg = arg name , annotation = None ) safe local = genname ( munge ( binding . name ) ) fn body ast . append ( ast . Assign ( targets = [ ast . Name ( id = safe local , ctx = ast . Store ( ) ) ] , value = ast . Call ( func = COLLECT ARGS FN NAME , args = [ ast . Name ( id = arg name , ctx = ast . Load ( ) ) ] , keywords = [ ] , ) , ) ) ctx . symbol table . new symbol ( sym . symbol ( binding . name ) , safe local , Local Type . ARG ) body ast = synthetic do to py ast ( ctx , body ) fn body ast . extend ( map ( statementize , body ast . dependencies ) ) fn body ast . append ( ast . Return ( value = body ast . node ) ) return fn args , varg , fn body ast
def single arity fn to py ast ( ctx : Generator Context , node : Fn , method : Fn Method , def name : Optional [ str ] = None , meta node : Optional [ Meta Node ] = None , ) -> Generated Py AST : assert node . op == Node Op . FN assert method . op == Node Op . FN METHOD lisp fn name = node . local . name if node . local is not None else None py fn name = fn name ( lisp fn name ) if def name is None else munge ( def name ) py fn node = ast . Async Function Def if node . is async else ast . Function Def with ctx . new symbol table ( py fn name ) , ctx . new recur point ( method . loop id , Recur Type . FN , is variadic = node . is variadic ) : # Allow named anonymous functions to recursively call themselves if lisp fn name is not None : ctx . symbol table . new symbol ( sym . symbol ( lisp fn name ) , py fn name , Local Type . FN ) fn args , varg , fn body ast = fn args to py ast ( ctx , method . params , method . body ) meta deps , meta decorators = fn meta ( ctx , meta node ) return Generated Py AST ( node = ast . Name ( id = py fn name , ctx = ast . Load ( ) ) , dependencies = list ( chain ( meta deps , [ py fn node ( name = py fn name , args = ast . arguments ( args = fn args , kwarg = None , vararg = varg , kwonlyargs = [ ] , defaults = [ ] , kw defaults = [ ] , ) , body = fn body ast , decorator list = list ( chain ( meta decorators , [ BASILISP FN FN NAME ] , [ TRAMPOLINE FN NAME ] if ctx . recur point . has recur else [ ] , ) ) , returns = None , ) ] , ) ) , )
def multi arity fn to py ast ( # pylint: disable=too-many-locals ctx : Generator Context , node : Fn , methods : Collection [ Fn Method ] , def name : Optional [ str ] = None , meta node : Optional [ Meta Node ] = None , ) -> Generated Py AST : assert node . op == Node Op . FN assert all ( [ method . op == Node Op . FN METHOD for method in methods ] ) lisp fn name = node . local . name if node . local is not None else None py fn name = fn name ( lisp fn name ) if def name is None else munge ( def name ) py fn node = ast . Async Function Def if node . is async else ast . Function Def arity to name = { } rest arity name : Optional [ str ] = None fn defs = [ ] for method in methods : arity name = f"{py fn name} arity{' rest' if method.is variadic else method.fixed arity}" if method . is variadic : rest arity name = arity name else : arity to name [ method . fixed arity ] = arity name with ctx . new symbol table ( arity name ) , ctx . new recur point ( method . loop id , Recur Type . FN , is variadic = node . is variadic ) : # Allow named anonymous functions to recursively call themselves if lisp fn name is not None : ctx . symbol table . new symbol ( sym . symbol ( lisp fn name ) , py fn name , Local Type . FN ) fn args , varg , fn body ast = fn args to py ast ( ctx , method . params , method . body ) fn defs . append ( py fn node ( name = arity name , args = ast . arguments ( args = fn args , kwarg = None , vararg = varg , kwonlyargs = [ ] , defaults = [ ] , kw defaults = [ ] , ) , body = fn body ast , decorator list = [ TRAMPOLINE FN NAME ] if ctx . recur point . has recur else [ ] , returns = None , ) ) dispatch fn ast = multi arity dispatch fn ( ctx , py fn name , arity to name , default name = rest arity name , max fixed arity = node . max fixed arity , meta node = meta node , is async = node . is async , ) return Generated Py AST ( node = dispatch fn ast . node , dependencies = list ( chain ( fn defs , dispatch fn ast . dependencies ) ) , )
def fn to py ast ( ctx : Generator Context , node : Fn , def name : Optional [ str ] = None , meta node : Optional [ Meta Node ] = None , ) -> Generated Py AST : assert node . op == Node Op . FN if len ( node . methods ) == 1 : return single arity fn to py ast ( ctx , node , next ( iter ( node . methods ) ) , def name = def name , meta node = meta node ) else : return multi arity fn to py ast ( ctx , node , node . methods , def name = def name , meta node = meta node )
def import to py ast ( ctx : Generator Context , node : Import ) -> Generated Py AST : assert node . op == Node Op . IMPORT last = None deps : List [ ast . AST ] = [ ] for alias in node . aliases : safe name = munge ( alias . name ) try : module = importlib . import module ( safe name ) if alias . alias is not None : ctx . add import ( sym . symbol ( alias . name ) , module , sym . symbol ( alias . alias ) ) else : ctx . add import ( sym . symbol ( alias . name ) , module ) except Module Not Found Error as e : raise Import Error ( f"Python module '{alias.name}' not found" , node . form , node ) from e py import alias = ( munge ( alias . alias ) if alias . alias is not None else safe name . split ( "." , maxsplit = 1 ) [ 0 ] ) deps . append ( ast . Assign ( targets = [ ast . Name ( id = py import alias , ctx = ast . Store ( ) ) ] , value = ast . Call ( func = load attr ( "builtins. import " ) , args = [ ast . Str ( safe name ) ] , keywords = [ ] , ) , ) ) last = ast . Name ( id = py import alias , ctx = ast . Load ( ) ) # Note that we add this import to the live running system in the above # calls to `ctx.add import`, however, since we compile and cache Python # bytecode, we need to generate calls to `add import` for the running # namespace so when this code is reloaded from the cache, the runtime # is correctly configured. deps . append ( ast . Call ( func = load attr ( f"{ NS VAR VALUE}.add import" ) , args = [ ast . Call ( func = NEW SYM FN NAME , args = [ ast . Str ( safe name ) ] , keywords = [ ] ) , last , ] , keywords = [ ] , ) ) assert last is not None , "import* node must have at least one import" return Generated Py AST ( node = last , dependencies = deps )
def invoke to py ast ( ctx : Generator Context , node : Invoke ) -> Generated Py AST : assert node . op == Node Op . INVOKE fn ast = gen py ast ( ctx , node . fn ) args deps , args nodes = collection ast ( ctx , node . args ) return Generated Py AST ( node = ast . Call ( func = fn ast . node , args = list ( args nodes ) , keywords = [ ] ) , dependencies = list ( chain ( fn ast . dependencies , args deps ) ) , )
def let to py ast ( ctx : Generator Context , node : Let ) -> Generated Py AST : assert node . op == Node Op . LET with ctx . new symbol table ( "let" ) : let body ast : List [ ast . AST ] = [ ] for binding in node . bindings : init node = binding . init assert init node is not None init ast = gen py ast ( ctx , init node ) binding name = genname ( munge ( binding . name ) ) let body ast . extend ( init ast . dependencies ) let body ast . append ( ast . Assign ( targets = [ ast . Name ( id = binding name , ctx = ast . Store ( ) ) ] , value = init ast . node , ) ) ctx . symbol table . new symbol ( sym . symbol ( binding . name ) , binding name , Local Type . LET ) let result name = genname ( "let result" ) body ast = synthetic do to py ast ( ctx , node . body ) let body ast . extend ( map ( statementize , body ast . dependencies ) ) let body ast . append ( ast . Assign ( targets = [ ast . Name ( id = let result name , ctx = ast . Store ( ) ) ] , value = body ast . node , ) ) return Generated Py AST ( node = ast . Name ( id = let result name , ctx = ast . Load ( ) ) , dependencies = let body ast )
def loop to py ast ( ctx : Generator Context , node : Loop ) -> Generated Py AST : assert node . op == Node Op . LOOP with ctx . new symbol table ( "loop" ) : binding names = [ ] init bindings : List [ ast . AST ] = [ ] for binding in node . bindings : init node = binding . init assert init node is not None init ast = gen py ast ( ctx , init node ) init bindings . extend ( init ast . dependencies ) binding name = genname ( munge ( binding . name ) ) binding names . append ( binding name ) init bindings . append ( ast . Assign ( targets = [ ast . Name ( id = binding name , ctx = ast . Store ( ) ) ] , value = init ast . node , ) ) ctx . symbol table . new symbol ( sym . symbol ( binding . name ) , binding name , Local Type . LOOP ) loop result name = genname ( "loop" ) with ctx . new recur point ( node . loop id , Recur Type . LOOP , binding names = binding names ) : loop body ast : List [ ast . AST ] = [ ] body ast = synthetic do to py ast ( ctx , node . body ) loop body ast . extend ( body ast . dependencies ) loop body ast . append ( ast . Assign ( targets = [ ast . Name ( id = loop result name , ctx = ast . Store ( ) ) ] , value = body ast . node , ) ) loop body ast . append ( ast . Break ( ) ) return Generated Py AST ( node = load attr ( loop result name ) , dependencies = list ( chain ( [ ast . Assign ( targets = [ ast . Name ( id = loop result name , ctx = ast . Store ( ) ) ] , value = ast . Name Constant ( None ) , ) ] , init bindings , [ ast . While ( test = ast . Name Constant ( True ) , body = loop body ast , orelse = [ ] , ) ] , ) ) , )
def quote to py ast ( ctx : Generator Context , node : Quote ) -> Generated Py AST : assert node . op == Node Op . QUOTE return const node to py ast ( ctx , node . expr )
def fn recur to py ast ( ctx : Generator Context , node : Recur ) -> Generated Py AST : assert node . op == Node Op . RECUR assert ctx . recur point . is variadic is not None recur nodes : List [ ast . AST ] = [ ] recur deps : List [ ast . AST ] = [ ] for expr in node . exprs : expr ast = gen py ast ( ctx , expr ) recur nodes . append ( expr ast . node ) recur deps . extend ( expr ast . dependencies ) return Generated Py AST ( node = ast . Call ( func = TRAMPOLINE ARGS FN NAME , args = list ( chain ( [ ast . Name Constant ( ctx . recur point . is variadic ) ] , recur nodes ) ) , keywords = [ ] , ) , dependencies = recur deps , )
def deftype method recur to py ast ( ctx : Generator Context , node : Recur ) -> Generated Py AST : assert node . op == Node Op . RECUR recur nodes : List [ ast . AST ] = [ ] recur deps : List [ ast . AST ] = [ ] for expr in node . exprs : expr ast = gen py ast ( ctx , expr ) recur nodes . append ( expr ast . node ) recur deps . extend ( expr ast . dependencies ) this entry = ctx . symbol table . find symbol ( ctx . current this ) assert this entry is not None , "Field type local must have this" return Generated Py AST ( node = ast . Call ( func = TRAMPOLINE ARGS FN NAME , args = list ( chain ( [ ast . Name Constant ( ctx . recur point . is variadic ) , ast . Name ( id = this entry . munged , ctx = ast . Load ( ) ) , ] , recur nodes , ) ) , keywords = [ ] , ) , dependencies = recur deps , )
def loop recur to py ast ( ctx : Generator Context , node : Recur ) -> Generated Py AST : assert node . op == Node Op . RECUR recur deps : List [ ast . AST ] = [ ] recur targets : List [ ast . Name ] = [ ] recur exprs : List [ ast . AST ] = [ ] for name , expr in zip ( ctx . recur point . binding names , node . exprs ) : expr ast = gen py ast ( ctx , expr ) recur deps . extend ( expr ast . dependencies ) recur targets . append ( ast . Name ( id = name , ctx = ast . Store ( ) ) ) recur exprs . append ( expr ast . node ) if len ( recur targets ) == 1 : assert len ( recur exprs ) == 1 recur deps . append ( ast . Assign ( targets = recur targets , value = recur exprs [ 0 ] ) ) else : recur deps . append ( ast . Assign ( targets = [ ast . Tuple ( elts = recur targets , ctx = ast . Store ( ) ) ] , value = ast . Tuple ( elts = recur exprs , ctx = ast . Load ( ) ) , ) ) recur deps . append ( ast . Continue ( ) ) return Generated Py AST ( node = ast . Name Constant ( None ) , dependencies = recur deps )
def set bang to py ast ( ctx : Generator Context , node : Set Bang ) -> Generated Py AST : assert node . op == Node Op . SET BANG val temp name = genname ( "set bang val" ) val ast = gen py ast ( ctx , node . val ) target = node . target assert isinstance ( target , ( Host Field , Local , Var Ref ) ) , f"invalid set! target type {type(target)}" if isinstance ( target , Host Field ) : target ast = interop prop to py ast ( ctx , target , is assigning = True ) elif isinstance ( target , Var Ref ) : target ast = var sym to py ast ( ctx , target , is assigning = True ) elif isinstance ( target , Local ) : target ast = local sym to py ast ( ctx , target , is assigning = True ) else : # pragma: no cover raise Generator Exception ( f"invalid set! target type {type(target)}" , lisp ast = target ) return Generated Py AST ( node = ast . Name ( id = val temp name , ctx = ast . Load ( ) ) , dependencies = list ( chain ( val ast . dependencies , [ ast . Assign ( targets = [ ast . Name ( id = val temp name , ctx = ast . Store ( ) ) ] , value = val ast . node , ) ] , target ast . dependencies , [ ast . Assign ( targets = [ target ast . node ] , value = val ast . node ) ] , ) ) , )
def throw to py ast ( ctx : Generator Context , node : Throw ) -> Generated Py AST : assert node . op == Node Op . THROW throw fn = genname ( THROW PREFIX ) exc ast = gen py ast ( ctx , node . exception ) raise body = ast . Raise ( exc = exc ast . node , cause = None ) return Generated Py AST ( node = ast . Call ( func = ast . Name ( id = throw fn , ctx = ast . Load ( ) ) , args = [ ] , keywords = [ ] ) , dependencies = [ ast . Function Def ( name = throw fn , args = ast . arguments ( args = [ ] , kwarg = None , vararg = None , kwonlyargs = [ ] , defaults = [ ] , kw defaults = [ ] , ) , body = list ( chain ( exc ast . dependencies , [ raise body ] ) ) , decorator list = [ ] , returns = None , ) ] , )
def try to py ast ( ctx : Generator Context , node : Try ) -> Generated Py AST : assert node . op == Node Op . TRY try expr name = genname ( "try expr" ) body ast = synthetic do to py ast ( ctx , node . body ) catch handlers = list ( map ( partial ( catch to py ast , ctx , try expr name = try expr name ) , node . catches ) ) finallys : List [ ast . AST ] = [ ] if node . finally is not None : finally ast = synthetic do to py ast ( ctx , node . finally ) finallys . extend ( map ( statementize , finally ast . dependencies ) ) finallys . append ( statementize ( finally ast . node ) ) return Generated Py AST ( node = ast . Name ( id = try expr name , ctx = ast . Load ( ) ) , dependencies = [ ast . Try ( body = list ( chain ( body ast . dependencies , [ ast . Assign ( targets = [ ast . Name ( id = try expr name , ctx = ast . Store ( ) ) ] , value = body ast . node , ) ] , ) ) , handlers = catch handlers , orelse = [ ] , finalbody = finallys , ) ] , )
def local sym to py ast ( ctx : Generator Context , node : Local , is assigning : bool = False ) -> Generated Py AST : assert node . op == Node Op . LOCAL sym entry = ctx . symbol table . find symbol ( sym . symbol ( node . name ) ) assert sym entry is not None if node . local == Local Type . FIELD : this entry = ctx . symbol table . find symbol ( ctx . current this ) assert this entry is not None , "Field type local must have this" return Generated Py AST ( node = load attr ( f"{this entry.munged}.{sym entry.munged}" , ctx = ast . Store ( ) if is assigning else ast . Load ( ) , ) ) else : return Generated Py AST ( node = ast . Name ( id = sym entry . munged , ctx = ast . Store ( ) if is assigning else ast . Load ( ) ) )
def var find to py ast ( var name : str , ns name : str , py var ctx : ast . AST ) -> Generated Py AST : return Generated Py AST ( node = ast . Attribute ( value = ast . Call ( func = FIND VAR FN NAME , args = [ ast . Call ( func = NEW SYM FN NAME , args = [ ast . Str ( var name ) ] , keywords = [ ast . keyword ( arg = "ns" , value = ast . Str ( ns name ) ) ] , ) ] , keywords = [ ] , ) , attr = "value" , ctx = py var ctx , ) )
def interop call to py ast ( ctx : Generator Context , node : Host Call ) -> Generated Py AST : assert node . op == Node Op . HOST CALL target ast = gen py ast ( ctx , node . target ) args deps , args nodes = collection ast ( ctx , node . args ) return Generated Py AST ( node = ast . Call ( func = ast . Attribute ( value = target ast . node , attr = munge ( node . method , allow builtins = True ) , ctx = ast . Load ( ) , ) , args = list ( args nodes ) , keywords = [ ] , ) , dependencies = list ( chain ( target ast . dependencies , args deps ) ) , )
def interop prop to py ast ( ctx : Generator Context , node : Host Field , is assigning : bool = False ) -> Generated Py AST : assert node . op == Node Op . HOST FIELD target ast = gen py ast ( ctx , node . target ) return Generated Py AST ( node = ast . Attribute ( value = target ast . node , attr = munge ( node . field ) , ctx = ast . Store ( ) if is assigning else ast . Load ( ) , ) , dependencies = target ast . dependencies , )
def with meta to py ast ( ctx : Generator Context , node : With Meta , * * kwargs ) -> Generated Py AST : assert node . op == Node Op . WITH META handle expr = WITH META EXPR HANDLER . get ( node . expr . op ) assert ( handle expr is not None ) , "No expression handler for with-meta child node type" return handle expr ( ctx , node . expr , meta node = node . meta , * * kwargs )
def py module preamble ( ctx : Generator Context , ) -> Generated Py AST : preamble : List [ ast . AST ] = [ ] preamble . extend ( module imports ( ctx ) ) preamble . append ( from module import ( ) ) preamble . append ( ns var ( ) ) return Generated Py AST ( node = ast . Name Constant ( None ) , dependencies = preamble )
def set ( members : Iterable [ T ] , meta = None ) -> Set [ T ] : # pylint:disable=redefined-builtin return Set ( pset ( members ) , meta = meta )
def s ( * members : T , meta = None ) -> Set [ T ] : return Set ( pset ( members ) , meta = meta )
def visit Except Handler ( self , node : ast . Except Handler ) -> Optional [ ast . AST ] : new node = self . generic visit ( node ) assert isinstance ( new node , ast . Except Handler ) return ast . copy location ( ast . Except Handler ( type = new node . type , name = new node . name , body = filter dead code ( new node . body ) , ) , new node , )
def visit Function Def ( self , node : ast . Function Def ) -> Optional [ ast . AST ] : new node = self . generic visit ( node ) assert isinstance ( new node , ast . Function Def ) return ast . copy location ( ast . Function Def ( name = new node . name , args = new node . args , body = filter dead code ( new node . body ) , decorator list = new node . decorator list , returns = new node . returns , ) , new node , )
def visit While ( self , node : ast . While ) -> Optional [ ast . AST ] : new node = self . generic visit ( node ) assert isinstance ( new node , ast . While ) return ast . copy location ( ast . While ( test = new node . test , body = filter dead code ( new node . body ) , orelse = filter dead code ( new node . orelse ) , ) , new node , )
def visit Try ( self , node : ast . Try ) -> Optional [ ast . AST ] : new node = self . generic visit ( node ) assert isinstance ( new node , ast . Try ) return ast . copy location ( ast . Try ( body = filter dead code ( new node . body ) , handlers = new node . handlers , orelse = filter dead code ( new node . orelse ) , finalbody = filter dead code ( new node . finalbody ) , ) , new node , )
def nthrest ( coll , i : int ) : while True : if coll is None : return None if i == 0 : return coll i -= 1 coll = rest ( coll )
def nthnext ( coll , i : int ) -> Optional [ I Seq ] : while True : if coll is None : return None if i == 0 : return to seq ( coll ) i -= 1 coll = next ( coll )
def to seq ( o ) -> Optional [ I Seq ] : if o is None : return None if isinstance ( o , I Seq ) : return seq or nil ( o ) if isinstance ( o , I Seqable ) : return seq or nil ( o . seq ( ) ) return seq or nil ( lseq . sequence ( o ) )
def concat ( * seqs ) -> I Seq : allseqs = lseq . sequence ( itertools . chain ( * filter ( None , map ( to seq , seqs ) ) ) ) if allseqs is None : return lseq . EMPTY return allseqs
def partial ( f , * args ) : @ functools . wraps ( f ) def partial f ( * inner args ) : return f ( * itertools . chain ( args , inner args ) ) return partial f
def contains ( coll , k ) : if isinstance ( coll , I Associative ) : return coll . contains ( k ) return k in coll
def get ( m , k , default = None ) : if isinstance ( m , I Associative ) : return m . entry ( k , default = default ) try : return m [ k ] except ( Key Error , Index Error , Type Error ) as e : logger . debug ( "Ignored %s: %s" , type ( e ) . name , e ) return default
def to lisp ( o , keywordize keys : bool = True ) : if not isinstance ( o , ( dict , frozenset , list , set , tuple ) ) : return o else : # pragma: no cover return to lisp backup ( o , keywordize keys = keywordize keys )
def to py ( o , keyword fn : Callable [ [ kw . Keyword ] , Any ] = kw name ) : if isinstance ( o , I Seq ) : return to py list ( o , keyword fn = keyword fn ) elif not isinstance ( o , ( I Persistent List , I Persistent Map , I Persistent Set , I Persistent Vector ) ) : return o else : # pragma: no cover return to py backup ( o , keyword fn = keyword fn )
def collect args ( args ) -> I Seq : if isinstance ( args , tuple ) : return llist . list ( args ) raise Type Error ( "Python variadic arguments should always be a tuple" )
def init ns var ( which ns : str = CORE NS , ns var name : str = NS VAR NAME ) -> Var : core sym = sym . Symbol ( which ns ) core ns = Namespace . get or create ( core sym ) ns var = Var . intern ( core sym , sym . Symbol ( ns var name ) , core ns , dynamic = True ) logger . debug ( f"Created namespace variable {sym.symbol(ns var name, ns=which ns)}" ) return ns var
def set current ns ( ns name : str , module : types . Module Type = None , ns var name : str = NS VAR NAME , ns var ns : str = NS VAR NS , ) -> Var : symbol = sym . Symbol ( ns name ) ns = Namespace . get or create ( symbol , module = module ) ns var sym = sym . Symbol ( ns var name , ns = ns var ns ) ns var = Maybe ( Var . find ( ns var sym ) ) . or else raise ( lambda : Runtime Exception ( f"Dynamic Var {sym.Symbol(ns var name, ns=ns var ns)} not bound!" ) ) ns var . push bindings ( ns ) logger . debug ( f"Setting {ns var sym} to {ns}" ) return ns var
def get current ns ( ns var name : str = NS VAR NAME , ns var ns : str = NS VAR NS ) -> Namespace : ns sym = sym . Symbol ( ns var name , ns = ns var ns ) ns : Namespace = Maybe ( Var . find ( ns sym ) ) . map ( lambda v : v . value ) . or else raise ( lambda : Runtime Exception ( f"Dynamic Var {ns sym} not bound!" ) ) return ns
def resolve alias ( s : sym . Symbol , ns : Optional [ Namespace ] = None ) -> sym . Symbol : if s in SPECIAL FORMS : return s ns = Maybe ( ns ) . or else ( get current ns ) if s . ns is not None : aliased ns = ns . get alias ( sym . symbol ( s . ns ) ) if aliased ns is not None : return sym . symbol ( s . name , aliased ns . name ) else : return s else : which var = ns . find ( sym . symbol ( s . name ) ) if which var is not None : return sym . symbol ( which var . name . name , which var . ns . name ) else : return sym . symbol ( s . name , ns = ns . name )
def add generated python ( generated python : str , var name : str = GENERATED PYTHON VAR NAME , which ns : Optional [ str ] = None , ) -> None : if which ns is None : which ns = get current ns ( ) . name ns sym = sym . Symbol ( var name , ns = which ns ) v = Maybe ( Var . find ( ns sym ) ) . or else ( lambda : Var . intern ( sym . symbol ( which ns ) , # type: ignore sym . symbol ( var name ) , "" , dynamic = True , meta = lmap . map ( { PRIVATE META KEY : True } ) , ) ) v . value = v . value + generated python
def print generated python ( var name : str = PRINT GENERATED PY VAR NAME , core ns name : str = CORE NS ) -> bool : ns sym = sym . Symbol ( var name , ns = core ns name ) return ( Maybe ( Var . find ( ns sym ) ) . map ( lambda v : v . value ) . or else raise ( lambda : Runtime Exception ( f"Dynamic Var {ns sym} not bound!" ) ) )
def intern ( ns : sym . Symbol , name : sym . Symbol , val , dynamic : bool = False , meta = None ) -> "Var" : var ns = Namespace . get or create ( ns ) var = var ns . intern ( name , Var ( var ns , name , dynamic = dynamic , meta = meta ) ) var . root = val return var
def intern unbound ( ns : sym . Symbol , name : sym . Symbol , dynamic : bool = False , meta = None ) -> "Var" : var ns = Namespace . get or create ( ns ) return var ns . intern ( name , Var ( var ns , name , dynamic = dynamic , meta = meta ) )
def add alias ( self , alias : sym . Symbol , namespace : "Namespace" ) -> None : self . aliases . swap ( lambda m : m . assoc ( alias , namespace ) )
def add refer ( self , sym : sym . Symbol , var : Var ) -> None : if not var . is private : self . refers . swap ( lambda s : s . assoc ( sym , var ) )
def get refer ( self , sym : sym . Symbol ) -> Optional [ Var ] : return self . refers . entry ( sym , None )
def refer all ( cls , refers : lmap . Map , other ns interns : lmap . Map ) -> lmap . Map : final refers = refers for entry in other ns interns : s : sym . Symbol = entry . key var : Var = entry . value if not var . is private : final refers = final refers . assoc ( s , var ) return final refers
def refer all ( self , other ns : "Namespace" ) : self . refers . swap ( Namespace . refer all , other ns . interns )
def list ( members , meta = None ) -> List : # pylint:disable=redefined-builtin return List ( # pylint: disable=abstract-class-instantiated plist ( iterable = members ) , meta = meta )
def l ( * members , meta = None ) -> List : return List ( # pylint: disable=abstract-class-instantiated plist ( iterable = members ) , meta = meta )
def change style ( style , representer ) : def new representer ( dumper , data ) : scalar = representer ( dumper , data ) scalar . style = style return scalar return new representer
def delete ( self , * args ) : cache = get cache ( ) key = self . get cache key ( * args ) if key in cache : del cache [ key ]
async def connect ( self ) : self . reader , self . writer = await asyncio . open connection ( self . host , self . port , loop = self . loop ) self . welcome msg = await self . reader . read ( self . buffer size )
async def receive ( self ) : try : incomming = await self . reader . read ( self . buffer size ) except OS Error : return [ ] return parse receive ( incomming )
def dump ( ndb model , fp , * * kwargs ) : for chunk in Ndb Encoder ( * * kwargs ) . iterencode ( ndb model ) : fp . write ( chunk )
def object hook handler ( self , val ) : return { k : self . decode date ( v ) for k , v in val . iteritems ( ) }
def decode date ( self , val ) : if isinstance ( val , basestring ) and val . count ( '-' ) == 2 and len ( val ) > 9 : try : dt = dateutil . parser . parse ( val ) # Check for UTC. if val . endswith ( ( '+00:00' , '-00:00' , 'Z' ) ) : # Then remove tzinfo for gae, which is offset-naive. dt = dt . replace ( tzinfo = None ) return dt except ( Type Error , Value Error ) : pass return val
def decode ( self , val ) : # First try the date decoder. new val = self . decode date ( val ) if val != new val : return new val # Fall back to the default decoder. return json . JSON Decoder . decode ( self , val )
def default ( self , obj ) : obj type = type ( obj ) # NDB Models return a repr to calls from type(). if obj type not in self . ndb type encoding : if hasattr ( obj , ' metaclass ' ) : obj type = obj . metaclass else : # Try to encode subclasses of types for ndb type in NDB TYPES : if isinstance ( obj , ndb type ) : obj type = ndb type break fn = self . ndb type encoding . get ( obj type ) if fn : return fn ( obj ) return json . JSON Encoder . default ( self , obj )
def validate version ( ) : import leicacam version string = leicacam . version versions = version string . split ( '.' , 3 ) try : for ver in versions : int ( ver ) except Value Error : print ( 'Only integers are allowed in release version, ' 'please adjust current version {}' . format ( version string ) ) return None return version string
def robust topological sort ( graph : Graph ) -> list : assert check argument types ( ) components = strongly connected components ( graph ) node component = { } for component in components : for node in component : node component [ node ] = component component graph = { } for component in components : component graph [ component ] = [ ] for node in graph : node c = node component [ node ] for successor in graph [ node ] : successor c = node component [ successor ] if node c != successor c : component graph [ node c ] . append ( successor c ) return topological sort ( component graph )
def logger ( function ) : @ functools . wraps ( function ) def wrapper ( * args , * * kwargs ) : """Wrap function.""" sep = kwargs . get ( 'sep' , ' ' ) end = kwargs . get ( 'end' , '' ) # do not add newline by default out = sep . join ( [ repr ( x ) for x in args ] ) out = out + end LOGGER . debug ( out ) return function ( * args , * * kwargs ) return wrapper
def connect ( self ) : self . socket = socket . socket ( ) self . socket . connect ( ( self . host , self . port ) ) self . socket . settimeout ( False ) # non-blocking sleep ( self . delay ) # wait for response self . welcome msg = self . socket . recv ( self . buffer size )
def flush ( self ) : debug ( 'flushing incomming socket messages' ) try : while True : msg = self . socket . recv ( self . buffer size ) debug ( b'< ' + msg ) except socket . error : pass
def receive ( self ) : try : incomming = self . socket . recv ( self . buffer size ) except socket . error : return [ ] return parse receive ( incomming )
def enable ( self , slide = 0 , wellx = 1 , welly = 1 , fieldx = 1 , fieldy = 1 ) : # pylint: disable=too-many-arguments cmd = [ ( 'cmd' , 'enable' ) , ( 'slide' , str ( slide ) ) , ( 'wellx' , str ( wellx ) ) , ( 'welly' , str ( welly ) ) , ( 'fieldx' , str ( fieldx ) ) , ( 'fieldy' , str ( fieldy ) ) , ( 'value' , 'true' ) ] self . send ( cmd ) return self . wait for ( * cmd [ 0 ] )
def save template ( self , filename = "{Scanning Template}leicacam.xml" ) : cmd = [ ( 'sys' , '0' ) , ( 'cmd' , 'save' ) , ( 'fil' , str ( filename ) ) ] self . send ( cmd ) return self . wait for ( * cmd [ 0 ] )
def get information ( self , about = 'stage' ) : cmd = [ ( 'cmd' , 'getinfo' ) , ( 'dev' , str ( about ) ) ] self . send ( cmd ) return self . wait for ( * cmd [ 1 ] )
def locate package json ( ) : directory = settings . SYSTEMJS PACKAGE JSON DIR if not directory : raise Improperly Configured ( "Could not locate 'package.json'. Set SYSTEMJS PACKAGE JSON DIR " "to the directory that holds 'package.json'." ) path = os . path . join ( directory , 'package.json' ) if not os . path . isfile ( path ) : raise Improperly Configured ( "'package.json' does not exist, tried looking in %s" % path ) return path
def parse package json ( ) : with open ( locate package json ( ) ) as pjson : data = json . loads ( pjson . read ( ) ) return data
def validate yourls response ( response , data ) : try : response . raise for status ( ) except HTTP Error as http exc : # Collect full HTTP Error information so we can reraise later if required. http error info = sys . exc info ( ) # We will reraise outside of try..except block to prevent exception # chaining showing wrong traceback when we try and parse JSON response. reraise = False try : jsondata = response . json ( ) except Value Error : reraise = True else : logger . debug ( 'Received error {response} with JSON {json}' , response = response , json = jsondata ) handle api error with json ( http exc , jsondata , response ) if reraise : six . reraise ( * http error info ) else : # We have a valid HTTP response, but we need to check what the API says # about the request. jsondata = response . json ( ) logger . debug ( 'Received {response} with JSON {json}' , response = response , json = jsondata ) if { 'status' , 'code' , 'message' } <= set ( jsondata . keys ( ) ) : status = jsondata [ 'status' ] code = jsondata [ 'code' ] message = jsondata [ 'message' ] if status == 'fail' : if code == 'error:keyword' : raise YOURLS Keyword Exists Error ( message , keyword = data [ 'keyword' ] ) elif code == 'error:url' : url = json to shortened url ( jsondata [ 'url' ] , jsondata [ 'shorturl' ] ) raise YOURLSURL Exists Error ( message , url = url ) else : raise YOURLSAPI Error ( message ) else : return jsondata else : # Without status, nothing special needs to be handled. return jsondata
def interp dep vector ( wave , indep vector ) : dep vector is int = wave . dep vector . dtype . name . startswith ( "int" ) dep vector is complex = wave . dep vector . dtype . name . startswith ( "complex" ) if ( wave . interp , wave . indep scale ) == ( "CONTINUOUS" , "LOG" ) : wave interp func = scipy . interpolate . interp1d ( np . log10 ( wave . indep vector ) , wave . dep vector ) ret = wave interp func ( np . log10 ( indep vector ) ) elif ( wave . interp , wave . indep scale ) == ( "CONTINUOUS" , "LINEAR" ) : dep vector = ( wave . dep vector . astype ( np . float64 ) if not dep vector is complex else wave . dep vector ) wave interp func = scipy . interpolate . interp1d ( wave . indep vector , dep vector ) ret = wave interp func ( indep vector ) else : # wave.interp == 'STAIRCASE' wave interp func = scipy . interpolate . interp1d ( wave . indep vector , wave . dep vector , kind = "zero" ) # Interpolator does not return the right value for the last # data point, it gives the previous "stair" value ret = wave interp func ( indep vector ) eq comp = np . all ( np . isclose ( wave . indep vector [ - 1 ] , indep vector [ - 1 ] , FP RTOL , FP ATOL ) ) if eq comp : ret [ - 1 ] = wave . dep vector [ - 1 ] round ret = np . round ( ret , 0 ) return ( round ret . astype ( "int" ) if ( dep vector is int and np . all ( np . isclose ( round ret , ret , FP RTOL , FP ATOL ) ) ) else ret )
def get indep vector ( wave a , wave b ) : exobj = pexdoc . exh . addex ( Runtime Error , "Independent variable ranges do not overlap" ) min bound = max ( np . min ( wave a . indep vector ) , np . min ( wave b . indep vector ) ) max bound = min ( np . max ( wave a . indep vector ) , np . max ( wave b . indep vector ) ) exobj ( bool ( min bound > max bound ) ) raw range = np . unique ( np . concatenate ( ( wave a . indep vector , wave b . indep vector ) ) ) return raw range [ np . logical and ( min bound <= raw range , raw range <= max bound ) ]
def verify compatibility ( wave a , wave b , check dep units = True ) : exobj = pexdoc . exh . addex ( Runtime Error , "Waveforms are not compatible" ) ctuple = ( bool ( wave a . indep scale != wave b . indep scale ) , bool ( wave a . dep scale != wave b . dep scale ) , bool ( wave a . indep units != wave b . indep units ) , ( bool ( wave a . dep units != wave b . dep units ) if check dep units else False ) , bool ( wave a . interp != wave b . interp ) , ) exobj ( any ( ctuple ) )
def trace pars ( mname ) : pickle fname = os . path . join ( os . path . dirname ( file ) , "{0}.pkl" . format ( mname ) ) ddir = os . path . dirname ( os . path . dirname ( file ) ) moddb fname = os . path . join ( ddir , "moddb.json" ) in callables fname = moddb fname if os . path . exists ( moddb fname ) else None out callables fname = os . path . join ( ddir , "{0}.json" . format ( mname ) ) noption = os . environ . get ( "NOPTION" , None ) exclude = [ " pytest" , "execnet" ] partuple = collections . namedtuple ( "Par Tuple" , [ "pickle fname" , "in callables fname" , "out callables fname" , "noption" , "exclude" , ] , ) return partuple ( pickle fname , in callables fname , out callables fname , noption , exclude )
def run trace ( mname , fname , module prefix , callable names , no print , module exclude = None , callable exclude = None , debug = False , ) : # pylint: disable=R0913 module exclude = [ ] if module exclude is None else module exclude callable exclude = [ ] if callable exclude is None else callable exclude par = trace pars ( mname ) start time = datetime . datetime . now ( ) with pexdoc . exdoc . Ex Doc Cxt ( exclude = par . exclude + module exclude , pickle fname = par . pickle fname , in callables fname = par . in callables fname , out callables fname = par . out callables fname , no print = no print , ) as exdoc obj : fname = os . path . realpath ( os . path . join ( os . path . dirname ( file ) , ".." , ".." , "tests" , "test {0}.py" . format ( fname ) , ) ) test cmd = ( [ "--color=yes" ] + ( [ "-s" , "-vv" ] if debug else [ "-q" , "-q" , "-q" ] ) + [ "--disable-warnings" ] + [ "-x" ] + ( [ par . noption ] if par . noption else [ ] ) + [ "-m " + mname ] + [ fname ] ) with warnings . catch warnings ( ) : warnings . filterwarnings ( "ignore" , category = Pytest Warning ) if pytest . main ( test cmd ) : raise Runtime Error ( "Tracing did not complete successfully" ) stop time = datetime . datetime . now ( ) if not no print : print ( "Auto-generation of exceptions documentation time: {0}" . format ( pmisc . elapsed time string ( start time , stop time ) ) ) for callable name in callable names : callable name = module prefix + callable name print ( "\n Callable: {0}" . format ( callable name ) ) print ( exdoc obj . get sphinx doc ( callable name , exclude = callable exclude ) ) print ( "\n" ) return copy . copy ( exdoc obj )
def flatten ( iterable , map2iter = None ) : if map2iter and isinstance ( iterable ) : iterable = map2iter ( iterable ) for item in iterable : if isinstance ( item , str ) or not isinstance ( item , abc . Iterable ) : yield item else : yield from flatten ( item , map2iter )
def printtsv ( table , sep = "\t" , file = sys . stdout ) : for record in table : print ( * record , sep = sep , file = file )
def mkdummy ( name , * * attrs ) : return type ( name , ( ) , dict ( repr = ( lambda self : "<%s>" % name ) , * * attrs ) ) ( )
def from str ( cls , human readable str , decimal = False , bits = False ) : divisor = 1000 if decimal else 1024 num = [ ] c = "" for c in human readable str : if c not in cls . digits : break num . append ( c ) num = "" . join ( num ) try : num = int ( num ) except Value Error : num = float ( num ) if bits : num /= 8 return cls ( round ( num * divisor ** cls . key [ c . lower ( ) ] ) )
def trace module ( no print = True ) : mname = "wave core" fname = "peng" module prefix = "peng.{0}.Waveform." . format ( mname ) callable names = ( " init " , ) return docs . support . trace support . run trace ( mname , fname , module prefix , callable names , no print )
def def links ( mobj ) : fdict = json load ( os . path . join ( "data" , "requirements.json" ) ) sdeps = sorted ( fdict . keys ( ) ) olines = [ ] for item in sdeps : olines . append ( "..  {name}: {url}\n" . format ( name = fdict [ item ] [ "name" ] , url = fdict [ item ] [ "url" ] ) ) ret = [ ] for line in olines : wobj = textwrap . wrap ( line , width = LINE WIDTH , subsequent indent = "   " ) ret . append ( "\n" . join ( [ item for item in wobj ] ) ) mobj . out ( "\n" . join ( ret ) )
def make common entry ( plist , pyver , suffix , req ver ) : prefix = "Python {pyver}.x{suffix}" . format ( pyver = pyver , suffix = suffix ) plist . append ( "{prefix}{ver}" . format ( prefix = prefix , ver = ops to words ( req ver ) ) )
def make multi entry ( plist , pkg pyvers , ver dict ) : for pyver in pkg pyvers : pver = pyver [ 2 ] + "." + pyver [ 3 : ] plist . append ( "Python {0}: {1}" . format ( pver , ops to words ( ver dict [ pyver ] ) ) )
def ops to words ( item ) : unsupp ops = [ "~=" , "===" ] # Ordered for  "pleasant" word specification supp ops = [ ">=" , ">" , "==" , "<=" , "<" , "!=" ] tokens = sorted ( item . split ( "," ) , reverse = True ) actual tokens = [ ] for req in tokens : for op in unsupp ops : if req . startswith ( op ) : raise Runtime Error ( "Unsupported version specification: {0}" . format ( op ) ) for op in supp ops : if req . startswith ( op ) : actual tokens . append ( op ) break else : raise Runtime Error ( "Illegal comparison operator: {0}" . format ( op ) ) if len ( list ( set ( actual tokens ) ) ) != len ( actual tokens ) : raise Runtime Error ( "Multiple comparison operators of the same type" ) if "!=" in actual tokens : return ( " and " . join ( [ op to words ( token ) for token in tokens [ : - 1 ] ] ) + " " + op to words ( tokens [ - 1 ] ) ) return " and " . join ( [ op to words ( token ) for token in tokens ] )
def proc requirements ( mobj ) : pyvers = [ "py{0}" . format ( item . replace ( "." , "" ) ) for item in get supported interps ( ) ] py2vers = sorted ( [ item for item in pyvers if item . startswith ( "py2" ) ] ) py3vers = sorted ( [ item for item in pyvers if item . startswith ( "py3" ) ] ) fdict = json load ( os . path . join ( "data" , "requirements.json" ) ) olines = [ "" ] sdict = dict ( [ ( item [ "name" ] , item ) for item in fdict . values ( ) ] ) for real name in sorted ( sdict . keys ( ) ) : pkg dict = sdict [ real name ] if pkg dict [ "cat" ] == [ "rtd" ] : continue plist = [ ] if not pkg dict [ "optional" ] else [ "optional" ] # Convert instances that have a single version for all Python # interpreters into a full dictionary of Python interpreter and # package versions # so as to apply the same algorithm in all cases if isinstance ( pkg dict [ "ver" ] , str ) : pkg dict [ "ver" ] = dict ( [ ( pyver , pkg dict [ "ver" ] ) for pyver in pyvers ] ) pkg pyvers = sorted ( pkg dict [ "ver" ] . keys ( ) ) pkg py2vers = sorted ( [ item for item in pkg dict [ "ver" ] . keys ( ) if item . startswith ( "py2" ) ] ) req vers = list ( set ( pkg dict [ "ver" ] . values ( ) ) ) req py2vers = list ( set ( [ pkg dict [ "ver" ] [ item ] for item in py2vers if item in pkg dict [ "ver" ] ] ) ) req py3vers = list ( set ( [ pkg dict [ "ver" ] [ item ] for item in py3vers if item in pkg dict [ "ver" ] ] ) ) if ( len ( req vers ) == 1 ) and ( pkg pyvers == pyvers ) : plist . append ( ops to words ( req vers [ 0 ] ) ) elif ( ( pkg pyvers == pyvers ) and ( len ( req py2vers ) == 1 ) and ( len ( req py3vers ) == 1 ) ) : make common entry ( plist , "2" , ": " , req py2vers [ 0 ] ) make common entry ( plist , "3" , ": " , req py3vers [ 0 ] ) elif ( ( pkg pyvers == pyvers ) and ( len ( req py2vers ) == len ( py2vers ) ) and ( len ( req py3vers ) == 1 ) and ( pkg dict [ "ver" ] [ pkg py2vers [ - 1 ] ] == req py3vers [ 0 ] ) ) : py2dict = dict ( [ ( key , value ) for key , value in pkg dict [ "ver" ] . items ( ) if key . startswith ( "py2" ) and ( key != pkg py2vers [ - 1 ] ) ] ) make multi entry ( plist , py2vers [ : - 1 ] , py2dict ) pver = pkg py2vers [ - 1 ] [ 2 ] + "." + pkg py2vers [ - 1 ] [ 3 : ] plist . append ( "Python {pyver} or newer: {ver}" . format ( pyver = pver , ver = ops to words ( req py3vers [ 0 ] ) ) ) elif ( ( pkg pyvers == pyvers ) and ( len ( req py2vers ) == len ( py2vers ) ) and ( len ( req py3vers ) == 1 ) ) : py2dict = dict ( [ ( key , value ) for key , value in pkg dict [ "ver" ] . items ( ) if key . startswith ( "py2" ) ] ) make multi entry ( plist , py2vers , py2dict ) make common entry ( plist , "3" , ": " , req py3vers [ 0 ] ) elif ( ( pkg pyvers == pyvers ) and ( len ( req py3vers ) == len ( py3vers ) ) and ( len ( req py2vers ) == 1 ) ) : py3dict = dict ( [ ( key , value ) for key , value in pkg dict [ "ver" ] . items ( ) if key . startswith ( "py3" ) ] ) make common entry ( plist , "2" , ": " , req py2vers [ 0 ] ) make multi entry ( plist , py3vers , py3dict ) elif ( len ( req vers ) == 1 ) and ( pkg pyvers == py2vers ) : make common entry ( plist , "2" , " only, " , req vers [ 0 ] ) elif ( len ( req vers ) == 1 ) and ( pkg pyvers == py3vers ) : make common entry ( plist , "3" , " only, " , req vers [ 0 ] ) else : make multi entry ( plist , pkg pyvers , pkg dict [ "ver" ] ) olines . append ( "    * `{name}`  ({par})" . format ( name = pkg dict [ "name" ] , par = ", " . join ( plist ) ) ) ret = [ ] for line in olines : wobj = textwrap . wrap ( line , width = LINE WIDTH , subsequent indent = "      " ) ret . append ( "\n" . join ( [ item for item in wobj ] ) ) mobj . out ( "\n\n" . join ( ret ) + "\n\n" )
def chunk noise ( noise ) : data = zip ( noise [ "freq" ] , noise [ "nf" ] , np . abs ( noise [ "rc" ] ) , np . angle ( noise [ "rc" ] ) , noise [ "res" ] , ) for freq , nf , rcmag , rcangle , res in data : yield freq , nf , rcmag , rcangle , res
def chunk pars ( freq vector , data matrix , pformat ) : pformat = pformat . upper ( ) length = 4 for freq , data in zip ( freq vector , data matrix ) : data = data . flatten ( ) for index in range ( 0 , data . size , length ) : fpoint = [ freq ] if not index else [ None ] cdata = data [ index : index + length ] if pformat == "MA" : vector1 = np . abs ( cdata ) vector2 = np . rad2deg ( np . angle ( cdata ) ) elif pformat == "RI" : vector1 = np . real ( cdata ) vector2 = np . imag ( cdata ) else : # elif pformat == 'DB': vector1 = 20.0 * np . log10 ( np . abs ( cdata ) ) vector2 = np . rad2deg ( np . angle ( cdata ) ) sep data = np . array ( [ ] ) for item1 , item2 in zip ( vector1 , vector2 ) : sep data = np . concatenate ( ( sep data , np . array ( [ item1 , item2 ] ) ) ) ret = np . concatenate ( ( np . array ( fpoint ) , sep data ) ) yield ret
def bound waveform ( wave , indep min , indep max ) : indep min , indep max = validate min max ( wave , indep min , indep max ) indep vector = copy . copy ( wave . indep vector ) if ( isinstance ( indep min , float ) or isinstance ( indep max , float ) ) and indep vector . dtype . name . startswith ( "int" ) : indep vector = indep vector . astype ( float ) min pos = np . searchsorted ( indep vector , indep min ) if not np . isclose ( indep min , indep vector [ min pos ] , FP RTOL , FP ATOL ) : indep vector = np . insert ( indep vector , min pos , indep min ) max pos = np . searchsorted ( indep vector , indep max ) if not np . isclose ( indep max , indep vector [ max pos ] , FP RTOL , FP ATOL ) : indep vector = np . insert ( indep vector , max pos , indep max ) dep vector = interp dep vector ( wave , indep vector ) wave . indep vector = indep vector [ min pos : max pos + 1 ] wave . dep vector = dep vector [ min pos : max pos + 1 ]
def build units ( indep units , dep units , op ) : if ( not dep units ) and ( not indep units ) : return "" if dep units and ( not indep units ) : return dep units if ( not dep units ) and indep units : return ( remove extra delims ( "1{0}({1})" . format ( op , indep units ) ) if op == "/" else remove extra delims ( "({0})" . format ( indep units ) ) ) return remove extra delims ( "({0}){1}({2})" . format ( dep units , op , indep units ) )
def operation ( wave , desc , units , fpointer ) : ret = copy . copy ( wave ) ret . dep units = units ret . dep name = "{0}({1})" . format ( desc , ret . dep name ) ret . dep vector = fpointer ( ret . dep vector ) return ret
def running area ( indep vector , dep vector ) : rect height = np . minimum ( dep vector [ : - 1 ] , dep vector [ 1 : ] ) rect base = np . diff ( indep vector ) rect area = np . multiply ( rect height , rect base ) triang height = np . abs ( np . diff ( dep vector ) ) triang area = 0.5 * np . multiply ( triang height , rect base ) return np . cumsum ( np . concatenate ( ( np . array ( [ 0.0 ] ) , triang area + rect area ) ) )
def validate min max ( wave , indep min , indep max ) : imin , imax = False , False if indep min is None : indep min = wave . indep vector [ 0 ] imin = True if indep max is None : indep max = wave . indep vector [ - 1 ] imax = True if imin and imax : return indep min , indep max exminmax = pexdoc . exh . addex ( Runtime Error , "Incongruent `indep min` and `indep max` arguments" ) exmin = pexdoc . exh . addai ( "indep min" ) exmax = pexdoc . exh . addai ( "indep max" ) exminmax ( bool ( indep min >= indep max ) ) exmin ( bool ( ( indep min < wave . indep vector [ 0 ] ) and ( not np . isclose ( indep min , wave . indep vector [ 0 ] , FP RTOL , FP ATOL ) ) ) ) exmax ( bool ( ( indep max > wave . indep vector [ - 1 ] ) and ( not np . isclose ( indep max , wave . indep vector [ - 1 ] , FP RTOL , FP ATOL ) ) ) ) return indep min , indep max
def get short desc ( long desc ) : found = False olines = [ ] for line in [ item . rstrip ( ) for item in long desc . split ( "\n" ) ] : if found and ( ( ( not line ) and ( not olines ) ) or ( line and olines ) ) : olines . append ( line ) elif found and olines and ( not line ) : return ( " " . join ( olines ) . split ( "." ) [ 0 ] ) . strip ( ) found = line == ".. [[[end]]]" if not found else found return ""
def render ( self , context ) : module path = self . path . resolve ( context ) if not settings . SYSTEMJS ENABLED : if settings . SYSTEMJS DEFAULT JS EXTENSIONS : name , ext = posixpath . splitext ( module path ) if not ext : module path = '{}.js' . format ( module path ) if settings . SYSTEMJS SERVER URL : tpl = """<script src="{url}{app}" type="text/javascript"></script>""" else : tpl = """<script type="text/javascript">System.import('{app}');</script>""" return tpl . format ( app = module path , url = settings . SYSTEMJS SERVER URL ) # else: create a bundle rel path = System . get bundle path ( module path ) url = staticfiles storage . url ( rel path ) tag attrs = { 'type' : 'text/javascript' } for key , value in self . tag attrs . items ( ) : if not isinstance ( value , bool ) : value = value . resolve ( context ) tag attrs [ key ] = value return """<script{attrs} src="{url}"></script>""" . format ( url = url , attrs = flatatt ( tag attrs ) )
def build expr ( tokens , higher oplevel = - 1 , ldelim = "(" , rdelim = ")" ) : # Numbers if isinstance ( tokens , str ) : return tokens # Unary operators if len ( tokens ) == 2 : return "" . join ( tokens ) # Multi-term operators oplevel = get op level ( tokens [ 1 ] ) stoken = "" for num , item in enumerate ( tokens ) : if num % 2 == 0 : stoken += build expr ( item , oplevel , ldelim = ldelim , rdelim = rdelim ) else : stoken += item if ( oplevel < higher oplevel ) or ( ( oplevel == higher oplevel ) and ( oplevel in OP PREC PAR ) ) : stoken = ldelim + stoken + rdelim return stoken
def next rdelim ( items , pos ) : for num , item in enumerate ( items ) : if item > pos : break else : raise Runtime Error ( "Mismatched delimiters" ) del items [ num ] return item
def get functions ( expr , ldelim = "(" , rdelim = ")" ) : tpars = pair delims ( expr , ldelim = ldelim , rdelim = rdelim ) alphas = "abcdefghijklmnopqrstuvwxyz" "ABCDEFGHIJKLMNOPQRSTUVWXYZ" fchars = "abcdefghijklmnopqrstuvwxyz" "ABCDEFGHIJKLMNOPQRSTUVWXYZ" "0123456789" " " tfuncs = [ ] for lnum , rnum in tpars : if lnum and expr [ lnum - 1 ] in fchars : for cnum , char in enumerate ( reversed ( expr [ : lnum ] ) ) : if char not in fchars : break else : cnum = lnum tfuncs . append ( { "fname" : expr [ lnum - cnum : lnum ] , "expr" : expr [ lnum + 1 : rnum ] , "start" : lnum - cnum , "stop" : rnum , } ) if expr [ lnum - cnum ] not in alphas : raise Runtime Error ( "Function name `{0}` is not valid" . format ( expr [ lnum - cnum : lnum ] ) ) return tfuncs
def parse expr ( text , ldelim = "(" , rdelim = ")" ) : var = pyparsing . Word ( pyparsing . alphas + " " , pyparsing . alphanums + " " ) point = pyparsing . Literal ( "." ) exp = pyparsing . Caseless Literal ( "E" ) number = pyparsing . Combine ( pyparsing . Word ( "+-" + pyparsing . nums , pyparsing . nums ) + pyparsing . Optional ( point + pyparsing . Optional ( pyparsing . Word ( pyparsing . nums ) ) ) + pyparsing . Optional ( exp + pyparsing . Word ( "+-" + pyparsing . nums , pyparsing . nums ) ) ) atom = var | number oplist = [ ( pyparsing . Literal ( "**" ) , 2 , pyparsing . op Assoc . RIGHT ) , ( pyparsing . one Of ( "+ - ~" ) , 1 , pyparsing . op Assoc . RIGHT ) , ( pyparsing . one Of ( "* / // %" ) , 2 , pyparsing . op Assoc . LEFT ) , ( pyparsing . one Of ( "+ -" ) , 2 , pyparsing . op Assoc . LEFT ) , ( pyparsing . one Of ( "<< >>" ) , 2 , pyparsing . op Assoc . LEFT ) , ( pyparsing . Literal ( "&" ) , 2 , pyparsing . op Assoc . LEFT ) , ( pyparsing . Literal ( "^" ) , 2 , pyparsing . op Assoc . LEFT ) , ( pyparsing . Literal ( "|" ) , 2 , pyparsing . op Assoc . LEFT ) , ] # Get functions expr = pyparsing . infix Notation ( atom , oplist , lpar = pyparsing . Suppress ( ldelim ) , rpar = pyparsing . Suppress ( rdelim ) ) return expr . parse String ( text ) [ 0 ]
def remove consecutive delims ( expr , ldelim = "(" , rdelim = ")" ) : tpars = pair delims ( expr , ldelim = ldelim , rdelim = rdelim ) # Flag superfluous delimiters ddelim = [ ] for ctuple , ntuple in zip ( tpars , tpars [ 1 : ] ) : if ctuple == ( ntuple [ 0 ] - 1 , ntuple [ 1 ] + 1 ) : ddelim . extend ( ntuple ) ddelim . sort ( ) # Actually remove delimiters from expression for num , item in enumerate ( ddelim ) : expr = expr [ : item - num ] + expr [ item - num + 1 : ] # Get functions return expr
def needs ext ( self ) : if settings . SYSTEMJS DEFAULT JS EXTENSIONS : name , ext = posixpath . splitext ( self . app ) if not ext : return True return False
def bundle ( self ) : outfile , rel path = self . get paths ( ) options = self . opts if self . system . has jspm log ( ) : self . command += ' --log {log}' options . setdefault ( 'log' , 'err' ) if options . get ( 'minify' ) : self . command += ' --minify' if options . get ( 'skip source maps' ) : self . command += ' --skip-source-maps' try : cmd = self . command . format ( app = self . app , outfile = outfile , * * options ) proc = subprocess . Popen ( cmd , shell = True , cwd = self . system . cwd , stdout = self . stdout , stdin = self . stdin , stderr = self . stderr ) result , err = proc . communicate ( ) # block until it's done if err and self . system . has jspm log ( ) : fmt = 'Could not bundle \'%s\': \n%s' logger . warn ( fmt , self . app , err ) raise Bundle Error ( fmt % ( self . app , err ) ) if result . strip ( ) : logger . info ( result ) except ( IO Error , OS Error ) as e : if isinstance ( e , Bundle Error ) : raise raise Bundle Error ( 'Unable to apply %s (%r): %s' % ( self . class . name , cmd , e ) ) else : if not options . get ( 'sfx' ) : # add the import statement, which is missing for non-sfx bundles sourcemap = find sourcemap comment ( outfile ) with open ( outfile , 'a' ) as of : of . write ( "\n System.import('{app}{ext}');\n{sourcemap}" . format ( app = self . app , ext = '.js' if self . needs ext ( ) else '' , sourcemap = sourcemap if sourcemap else '' , ) ) return rel path
def parse docstring ( doc ) : doc = inspect . cleandoc ( doc ) lines = doc . split ( '\n' ) section = None section indent = None params = { } returns = None for line in lines : line = line . rstrip ( ) if len ( line ) == 0 : continue elif str ( line ) == 'Args:' : section = 'args' section indent = None continue elif str ( line ) == 'Returns:' : section = 'return' section indent = None continue if section is not None : stripped = line . lstrip ( ) margin = len ( line ) - len ( stripped ) if section indent is None : section indent = margin if margin != section indent : continue # These are all the param lines in the docstring that are # not continuations of the previous line if section == 'args' : param name , type info = parse param ( stripped ) params [ param name ] = type info elif section == 'return' : returns = parse return ( stripped ) return params , returns
def split line ( self , line ) : parts = shlex . split ( line , posix = self . posix lex ) if not self . posix lex : parts = [ self . remove quotes ( x ) for x in parts ] return parts
def builtin help ( self , args ) : if len ( args ) == 0 : return self . list dir ( self . contexts [ - 1 ] ) if len ( args ) == 1 : func = self . find function ( self . contexts [ - 1 ] , args [ 0 ] ) return annotate . get help ( func ) help text = "Too many arguments: " + str ( args ) + "\n" help text += "Usage: help [function]" return help text
def extract arg value ( cls , arg name , arg type , remaining ) : next arg = None should consume = False if len ( remaining ) > 0 : next arg = remaining [ 0 ] should consume = True if next arg == '--' : next arg = None # Generally we just return the next argument, however if the type # is bool we allow not specifying anything to mean true if there # is no ambiguity if arg type == "bool" : if next arg is None or next arg . startswith ( '-' ) : next arg = True should consume = False else : if next arg is None : raise Argument Error ( "Could not find value for keyword argument" , argument = arg name ) if should consume : remaining . pop ( 0 ) return next arg
def parse param ( param , include desc = False ) : param def , colon , desc = param . partition ( ':' ) if not include desc : desc = None else : desc = desc . lstrip ( ) if colon == "" : raise Validation Error ( "Invalid parameter declaration in docstring, missing colon" , declaration = param ) param name , space , param type = param def . partition ( ' ' ) if len ( param type ) < 2 or param type [ 0 ] != '(' or param type [ - 1 ] != ')' : raise Validation Error ( "Invalid parameter type string not enclosed in ( ) characters" , param string = param def , type string = param type ) param type = param type [ 1 : - 1 ] return param name , Parameter Info ( param type , [ ] , desc )
def classify section ( cls , section ) : name = section . lower ( ) if name in frozenset ( [ 'args' , 'arguments' , "params" , "parameters" ] ) : return cls . ARGS SECTION if name in frozenset ( [ 'returns' , 'return' ] ) : return cls . RETURN SECTION if name in frozenset ( [ 'main' ] ) : return cls . MAIN SECTION return None
def classify line ( cls , line ) : line = line . rstrip ( ) if len ( line ) == 0 : return Blank Line ( '' ) if ' ' not in line and line . endswith ( ':' ) : name = line [ : - 1 ] return Section Header ( name ) if line . startswith ( '  ' ) : return Continuation Line ( line . lstrip ( ) ) if line . startswith ( ' - ' ) : return List Item ( '-' , line [ 3 : ] . lstrip ( ) ) if line . startswith ( '- ' ) : return List Item ( '-' , line [ 2 : ] . lstrip ( ) ) return Line ( line )
def join paragraphs ( cls , lines , use indent = False , leading blanks = False , trailing blanks = False ) : curr para = [ ] paragraphs = [ ] for line in lines : if use indent : if line . startswith ( ' ' ) : curr para . append ( line . lstrip ( ) ) continue elif line == '' : continue else : if len ( curr para ) > 0 : paragraphs . append ( cls . join paragraph ( curr para , leading blanks , trailing blanks ) ) curr para = [ line . lstrip ( ) ] else : if len ( line ) != 0 : curr para . append ( line ) else : paragraphs . append ( cls . join paragraph ( curr para , leading blanks , trailing blanks ) ) curr para = [ ] # Finish the last paragraph if there is one if len ( curr para ) > 0 : paragraphs . append ( cls . join paragraph ( curr para , leading blanks , trailing blanks ) ) return paragraphs
def split type ( self , typename ) : name = self . canonicalize type ( typename ) if '(' not in name : return name , False , [ ] base , sub = name . split ( '(' ) if len ( sub ) == 0 or sub [ - 1 ] != ')' : raise Argument Error ( "syntax error in complex type, no matching ) found" , passed type = typename , basetype = base , subtype string = sub ) sub = sub [ : - 1 ] subs = sub . split ( ',' ) return base , True , subs
def instantiate type ( self , typename , base , subtypes ) : if base not in self . type factories : raise Argument Error ( "unknown complex base type specified" , passed type = typename , base type = base ) base type = self . type factories [ base ] #Make sure all of the subtypes are valid for sub type in subtypes : try : self . get type ( sub type ) except Key Value Exception as exc : raise Argument Error ( "could not instantiate subtype for complex type" , passed type = typename , sub type = sub type , error = exc ) typeobj = base type . Build ( * subtypes , type system = self ) self . inject type ( typename , typeobj )
def short description ( func ) : doc = inspect . getdoc ( func ) if doc is not None : doc = inspect . cleandoc ( doc ) lines = doc . splitlines ( ) return lines [ 0 ] return ""
def load ( ) : autodiscover modules ( 'cron' ) if PROJECT MODULE : if '.' in PROJECT MODULE . name : try : import module ( '%s.cron' % '.' . join ( PROJECT MODULE . name . split ( '.' ) [ 0 : - 1 ] ) ) except Import Error as e : if 'No module named' not in str ( e ) : print ( e ) # load django tasks for cmd , app in get commands ( ) . items ( ) : try : load command class ( app , cmd ) except django . core . exceptions . Improperly Configured : pass
def install ( ) : load ( ) tab = crontab . Cron Tab ( user = True ) for task in registry : tab . new ( task . command , KRONOS BREADCRUMB ) . setall ( task . schedule ) tab . write ( ) return len ( registry )
def uninstall ( ) : tab = crontab . Cron Tab ( user = True ) count = len ( list ( tab . find comment ( KRONOS BREADCRUMB ) ) ) tab . remove all ( comment = KRONOS BREADCRUMB ) tab . write ( ) return count
def kind ( self ) : optics = [ Equality , Isomorphism , Prism , Review , Lens , Traversal , Getter , Setter , Fold , ] for optic in optics : if self . is kind ( optic ) : return optic
def play ( ) : ai = { 'X' : player move , 'O' : random move } board = Board ( ) while not board . winner : x , y = ai [ board . player ] ( board ) board = board . make move ( x , y ) print ( board , end = '\n\n' ) print ( board . winner )
def winner ( self ) : for potential win in self . potential wins ( ) : if potential win == tuple ( 'XXX' ) : return Outcome . win for crosses elif potential win == tuple ( 'OOO' ) : return Outcome . win for naughts if self . count ( ' ' ) == 0 : return Outcome . draw return Outcome . ongoing
def open spider ( self , spider ) : # Store timestamp to replace {time} in S3PIPELINE URL self . ts = datetime . utcnow ( ) . replace ( microsecond = 0 ) . isoformat ( ) . replace ( ':' , '-' )
def upload chunk ( self , spider ) : if not self . items : return # Do nothing when items is empty. f = self . make fileobj ( ) # Build object key by replacing variables in object key template. object key = self . object key template . format ( * * self . get uri params ( spider ) ) try : self . s3 . upload fileobj ( f , self . bucket name , object key ) except Client Error : self . stats . inc value ( 'pipeline/s3/fail' ) raise else : self . stats . inc value ( 'pipeline/s3/success' ) finally : # Prepare for the next chunk self . chunk number += len ( self . items ) self . items = [ ]
def make fileobj ( self ) : bio = Bytes IO ( ) f = gzip . Gzip File ( mode = 'wb' , fileobj = bio ) if self . use gzip else bio # Build file object using Item Exporter exporter = Json Lines Item Exporter ( f ) exporter . start exporting ( ) for item in self . items : exporter . export item ( item ) exporter . finish exporting ( ) if f is not bio : f . close ( ) # Close the file if Gzip File # Seek to the top of file to be read later bio . seek ( 0 ) return bio
def call ( self , method , params = None , request id = None ) : params = params or [ ] # Determines which 'id' value to use and increment the counter associated with the current # client instance if applicable. rid = request id or self . id counter if request id is None : self . id counter += 1 # Prepares the payload and the headers that will be used to forge the request. payload = { 'jsonrpc' : '2.0' , 'method' : method , 'params' : params , 'id' : rid } headers = { 'Content-Type' : 'application/json' } scheme = 'https' if self . tls else 'http' url = '{}://{}:{}' . format ( scheme , self . host , self . port ) # Calls the JSON-RPC endpoint! try : response = self . session . post ( url , headers = headers , data = json . dumps ( payload ) ) response . raise for status ( ) except HTTP Error : raise Transport Error ( 'Got unsuccessful response from server (status code: {})' . format ( response . status code ) , response = response ) # Ensures the response body can be deserialized to JSON. try : response data = response . json ( ) except Value Error as e : raise Protocol Error ( 'Unable to deserialize response body: {}' . format ( e ) , response = response ) # Properly handles potential errors. if response data . get ( 'error' ) : code = response data [ 'error' ] . get ( 'code' , '' ) message = response data [ 'error' ] . get ( 'message' , '' ) raise Protocol Error ( 'Error[{}] {}' . format ( code , message ) , response = response , data = response data ) elif 'result' not in response data : raise Protocol Error ( 'Response is empty (result field is missing)' , response = response , data = response data ) return response data [ 'result' ]
def is hash256 ( s ) : if not s or not isinstance ( s , str ) : return False return re . match ( '^[0-9A-F]{64}$' , s . strip ( ) , re . IGNORECASE )
def is hash160 ( s ) : if not s or not isinstance ( s , str ) : return False if not len ( s ) == 40 : return False for c in s : if ( c < '0' or c > '9' ) and ( c < 'A' or c > 'F' ) and ( c < 'a' or c > 'f' ) : return False return True
def encode invocation params ( params ) : final params = [ ] for p in params : if isinstance ( p , bool ) : final params . append ( { 'type' : Contract Parameter Types . BOOLEAN . value , 'value' : p } ) elif isinstance ( p , int ) : final params . append ( { 'type' : Contract Parameter Types . INTEGER . value , 'value' : p } ) elif is hash256 ( p ) : final params . append ( { 'type' : Contract Parameter Types . HASH256 . value , 'value' : p } ) elif is hash160 ( p ) : final params . append ( { 'type' : Contract Parameter Types . HASH160 . value , 'value' : p } ) elif isinstance ( p , bytearray ) : final params . append ( { 'type' : Contract Parameter Types . BYTE ARRAY . value , 'value' : p } ) elif isinstance ( p , str ) : final params . append ( { 'type' : Contract Parameter Types . STRING . value , 'value' : p } ) elif isinstance ( p , list ) : innerp = encode invocation params ( p ) final params . append ( { 'type' : Contract Parameter Types . ARRAY . value , 'value' : innerp } ) return final params
def decode invocation result ( result ) : if 'stack' not in result : return result result = copy . deepcopy ( result ) result [ 'stack' ] = decode invocation result stack ( result [ 'stack' ] ) return result
def connect ( cls , settings ) : server = serializer ( 'json' ) . loads ( settings [ 'kvs.perlsess' ] ) server . setdefault ( 'key prefix' , 'perlsess::' ) server . setdefault ( 'codec' , 'storable' ) cls . cookie name = server . pop ( 'cookie name' , 'session id' ) cls . client = KVS ( * * server )
def basic ( username , password ) : none ( ) config . username = username config . password = password
def api key ( api key ) : none ( ) config . api key prefix [ "Authorization" ] = "api-key" config . api key [ "Authorization" ] = "key=" + b64encode ( api key . encode ( ) ) . decode ( )
def get json content from folder ( folder ) : for dirpath , dirnames , filenames in os . walk ( folder ) : for filename in filenames : if filename . lower ( ) . endswith ( ".json" ) : filepath = os . path . join ( dirpath , filename ) with open ( filepath , "rb" ) as file : yield json . loads ( file . read ( ) . decode ( "UTF-8" ) )
def get schema ( self ) : path = os . path . join ( self . get schema folder ( ) , self . name + ".json" ) with open ( path , "rb" ) as file : schema = json . loads ( file . read ( ) . decode ( "UTF-8" ) ) return schema
def get valid examples ( self ) : path = os . path . join ( self . get schema folder ( ) , "examples" , "valid" ) return list ( get json content from folder ( path ) )
def get invalid examples ( self ) : path = os . path . join ( self . get schema folder ( ) , "examples" , "invalid" ) return list ( get json content from folder ( path ) )
def auth user get url ( self , scope = None ) : if not self . client id : raise Auth Missing Error ( 'No client id specified' ) return '{}?{}' . format ( self . auth url user , urllib . urlencode ( dict ( client id = self . client id , scope = ' ' . join ( scope or self . auth scope ) , response type = 'code' , redirect uri = self . auth redirect uri ) ) )
def auth user process url ( self , url ) : url = urlparse . urlparse ( url ) url qs = dict ( it . chain . from iterable ( urlparse . parse qsl ( v ) for v in [ url . query , url . fragment ] ) ) if url qs . get ( 'error' ) : raise API Auth Error ( '{} :: {}' . format ( url qs [ 'error' ] , url qs . get ( 'error description' ) ) ) self . auth code = url qs [ 'code' ] return self . auth code
def auth get token ( self , check scope = True ) : res = self . auth access data raw = self . auth token request ( ) return self . auth token process ( res , check scope = check scope )
def get user id ( self ) : if self . user id is None : self . user id = self . get user data ( ) [ 'id' ] return self . user id
def listdir ( self , folder id = 'me/skydrive' , limit = None , offset = None ) : return self ( self . api url join ( folder id , 'files' ) , dict ( limit = limit , offset = offset ) )
def comment add ( self , obj id , message ) : return self ( self . api url join ( obj id , 'comments' ) , method = 'post' , data = dict ( message = message ) , auth header = True )
def decode obj ( obj , force = False ) : if isinstance ( obj , unicode ) : return obj elif isinstance ( obj , bytes ) : if force encoding is not None : return obj . decode ( force encoding ) if chardet : enc guess = chardet . detect ( obj ) if enc guess [ 'confidence' ] > 0.7 : return obj . decode ( enc guess [ 'encoding' ] ) return obj . decode ( 'utf-8' ) else : return obj if not force else repr ( obj )
def set drop target ( obj , root , designer , inspector ) : if obj . meta . container : dt = Tool Box Drop Target ( obj , root , designer = designer , inspector = inspector ) obj . drop target = dt for child in obj : set drop target ( child , root , designer , inspector )
def start drag opperation ( self , evt ) : # get the control ctrl = self . menu ctrl map [ evt . Get Tool Id ( ) ] # create our own data format and use it in a custom data object ldata = wx . Custom Data Object ( "gui" ) ldata . Set Data ( ctrl . meta . name ) # only strings are allowed! # Also create a Bitmap version of the drawing bmp = ctrl . image . Get Bitmap ( ) # Now make a data object for the bitmap and also a composite # data object holding both of the others. bdata = wx . Bitmap Data Object ( bmp ) data = wx . Data Object Composite ( ) data . Add ( ldata ) data . Add ( bdata ) # And finally, create the drop source and begin the drag # and drop opperation drop Source = wx . Drop Source ( self ) drop Source . Set Data ( data ) if DEBUG : print ( "Begining Drag Drop\n" ) result = drop Source . Do Drag Drop ( wx . Drag Allow Move ) if DEBUG : print ( "Drag Drop completed: %d\n" % result ) if result == wx . Drag Move : if DEBUG : print "dragmove!" self . Refresh ( )
def set default tlw ( self , tlw , designer , inspector ) : self . designer = designer self . inspector = inspector
def inspect ( obj ) : from gui . tools . inspector import Inspector Tool inspector = Inspector Tool ( ) inspector . show ( obj ) return inspector
def migrate window ( bg ) : ret = { } for k , v in bg . items ( ) : if k == 'type' : v = WIN MAP [ v ] . meta . name elif k == 'menubar' : menus = v [ 'menus' ] v = [ migrate control ( menu ) for menu in menus ] elif k == 'components' : v = [ migrate control ( comp ) for comp in v ] else : k = SPEC MAP [ 'Widget' ] . get ( k , k ) ret [ k ] = v return ret
def migrate control ( comp ) : ret = { } for k , v in comp . items ( ) : if k == 'type' : v = CTRL MAP [ v ] . meta . name elif k == 'menubar' : pass elif k == 'components' : v = [ migrate control ( comp ) for comp in v ] else : k = SPEC MAP [ 'Widget' ] . get ( k , k ) if comp [ 'type' ] in SPEC MAP : k = SPEC MAP [ comp [ 'type' ] ] . get ( k , k ) if k == 'font' : v = migrate font ( v ) ret [ k ] = v return ret
def migrate font ( font ) : if 'face Name' in font : font [ 'face' ] = font . pop ( 'face Name' ) if 'family' in font and font [ 'family' ] == 'sans Serif' : font [ 'family' ] = 'sans serif' return font
def load page ( self , location ) : if not location : self . wx obj . Set Page ( "" ) else : self . wx obj . Load Page ( location )
def Get Param ( tag , param , default = SENTINEL ) : if tag . Has Param ( param ) : return tag . Get Param ( param ) else : if default == SENTINEL : raise Key Error else : return default
def send ( evt ) : # get the text written by the user (input textbox control) msg = ctrl input . value # send the message (replace with socket/queue/etc.) gui . alert ( msg , "Message" ) # record the message (update the UI) log ( msg ) ctrl input . value = "" ctrl input . set focus ( )
def wellcome tip ( wx obj ) : msg = ( "Close the main window to exit & save.\n" "Drag & Drop / Click the controls from the Tool Box to create new ones.\n" "Left click on the created controls to select them.\n" "Double click to edit the default property.\n" "Right click to pop-up the context menu.\n" ) # create a super tool tip manager and set some styles stt = STT . Super Tool Tip ( msg ) stt . Set Header ( "Welcome to gui2py designer!" ) stt . Set Draw Header Line ( True ) stt . Apply Style ( "Office 2007 Blue" ) stt . Set Drop Shadow ( True ) stt . Set Header Bitmap ( images . designer . Get Bitmap ( ) ) stt . Set End Delay ( 15000 ) # hide in 15 s # create a independent tip window, show/hide manually (avoid binding wx obj) tip = Custom Tool Tip Window ( wx obj , stt ) tip . Calculate Best Size ( ) tip . Calculate Best Position ( wx obj ) tip . Drop Shadow ( stt . Get Drop Shadow ( ) ) if stt . Get Use Fade ( ) : show = lambda : tip . Start Alpha ( True ) else : show = lambda : tip . Show ( ) wx . Call Later ( 1000 , show ) # show the tip in 1 s wx . Call Later ( 30000 , tip . Destroy )
def mouse down ( self , evt ) : if DEBUG : print "down!" if ( not evt . Control Down ( ) and not evt . Shift Down ( ) ) or evt . Alt Down ( ) : for obj in self . selection : # clear marker if obj . sel marker : obj . sel marker . show ( False ) obj . sel marker . destroy ( ) obj . sel marker = None self . selection = [ ] # clear previous selection wx obj = evt . Get Event Object ( ) if wx obj . Parent is None or evt . Alt Down ( ) : if not evt . Alt Down ( ) : evt . Skip ( ) # start the rubberband effect (multiple selection using the mouse)  self . current = wx obj self . overlay = wx . Overlay ( ) self . pos = evt . Get Position ( ) self . parent . wx obj . Capture Mouse ( ) #if self.inspector and hasattr(wx obj, "obj"): #    self.inspector.inspect(wx obj.obj)  # inspect top level window #self.dclick = False else : # create the selection marker and assign it to the control obj = wx obj . obj self . overlay = None if DEBUG : print wx obj sx , sy = wx obj . Screen To Client ( wx obj . Get Position Tuple ( ) ) dx , dy = wx obj . Screen To Client ( wx . Get Mouse Position ( ) ) self . pos = wx obj . Screen To Client ( wx . Get Mouse Position ( ) ) self . start = ( sx - dx , sy - dy ) self . current = wx obj if DEBUG : print "capture..." # do not capture on Text Ctrl, it will fail (blocking) at least in gtk # do not capture on wx.Notebook to allow selecting the tabs if not isinstance ( wx obj , wx . Notebook ) : self . parent . wx obj . Capture Mouse ( ) self . select ( obj , keep selection = True )
def mouse move ( self , evt ) : if DEBUG : print "move!" if self . current and not self . overlay : wx obj = self . current sx , sy = self . start x , y = wx . Get Mouse Position ( ) # calculate the new position (this will overwrite relative dimensions): x , y = ( x + sx , y + sy ) if evt . Shift Down ( ) : # snap to grid: x = x / GRID SIZE [ 0 ] * GRID SIZE [ 0 ] y = y / GRID SIZE [ 1 ] * GRID SIZE [ 1 ] # calculate the diff to use in the rest of the selected objects: ox , oy = wx obj . obj . pos dx , dy = ( x - ox ) , ( y - oy ) # move all selected objects: for obj in self . selection : x , y = obj . pos x = x + dx y = y + dy obj . pos = ( wx . Point ( x , y ) ) elif self . overlay : wx obj = self . current pos = evt . Get Position ( ) # convert to relative client coordinates of the containter: if evt . Get Event Object ( ) != wx obj : pos = evt . Get Event Object ( ) . Client To Screen ( pos ) # frame pos = wx obj . Screen To Client ( pos ) # panel rect = wx . Rect PP ( self . pos , pos ) # Draw the rubber-band rectangle using an overlay so it  # will manage keeping the rectangle and the former window  # contents separate.  dc = wx . Client DC ( wx obj ) odc = wx . DC Overlay ( self . overlay , dc ) odc . Clear ( ) dc . Set Pen ( wx . Pen ( "blue" , 2 ) ) if 'wx Mac' in wx . Platform Info : dc . Set Brush ( wx . Brush ( wx . Colour ( 0x C0 , 0x C0 , 0x C0 , 0x80 ) ) ) else : dc . Set Brush ( wx . TRANSPARENT BRUSH ) dc . Draw Rectangle Rect ( rect ) del odc
def key press ( self , event ) : key = event . Get Key Code ( ) if key in ( wx . WXK LEFT , wx . WXK UP , wx . WXK RIGHT , wx . WXK DOWN ) : for obj in self . selection : x , y = obj . pos if event . Shift Down ( ) : # snap to grid:t  # for now I'm only going to align to grid # in the direction of the cursor movement  if key == wx . WXK LEFT : x = ( x - GRID SIZE [ 0 ] ) / GRID SIZE [ 0 ] * GRID SIZE [ 0 ] elif key == wx . WXK RIGHT : x = ( x + GRID SIZE [ 0 ] ) / GRID SIZE [ 0 ] * GRID SIZE [ 0 ] elif key == wx . WXK UP : y = ( y - GRID SIZE [ 1 ] ) / GRID SIZE [ 1 ] * GRID SIZE [ 1 ] elif key == wx . WXK DOWN : y = ( y + GRID SIZE [ 1 ] ) / GRID SIZE [ 1 ] * GRID SIZE [ 1 ] else : if key == wx . WXK LEFT : x = x - 1 elif key == wx . WXK RIGHT : x = x + 1 elif key == wx . WXK UP : y = y - 1 elif key == wx . WXK DOWN : y = y + 1 obj . pos = ( x , y ) elif key == wx . WXK DELETE : self . delete ( event ) elif key == wx . WXK INSERT : self . duplicate ( event ) else : if DEBUG : print "KEY:" , key
def delete ( self , event ) : # get the selected objects (if any) for obj in self . selection : if obj : if DEBUG : print "deleting" , obj . name obj . destroy ( ) self . selection = [ ] # clean selection self . inspector . load object ( )
def duplicate ( self , event ) : # duplicate the selected objects (if any) new selection = [ ] for obj in self . selection : if obj : if DEBUG : print "duplicating" , obj . name obj . sel marker . destroy ( ) obj . sel marker = None obj2 = obj . duplicate ( ) obj2 . sel marker = Selection Marker ( obj2 ) obj2 . sel marker . show ( True ) new selection . append ( obj2 ) self . selection = new selection # update with new obj's self . inspector . load object ( )
def refresh ( self ) : self . bmp = self . obj . snapshot ( ) # change z-order to overlap controls (windows) and show the image: self . Raise ( ) self . Show ( ) self . Refresh ( )
def Calculate Best Position ( self , widget ) : if isinstance ( widget , wx . Frame ) : screen = wx . Client Display Rect ( ) [ 2 : ] left , top = widget . Client To Screen XY ( 0 , 0 ) right , bottom = widget . Client To Screen XY ( * widget . Get Client Rect ( ) [ 2 : ] ) size = self . Get Size ( ) xpos = right ypos = bottom - size [ 1 ] self . Set Position ( ( xpos , ypos ) ) else : STT . Tool Tip Window . Calculate Best Position ( self , widget )
def Get Py Data ( self , item ) : wx data = self . Get Item Data ( item ) py data = self . py data map . get ( wx data ) return py data
def Set Py Data ( self , item , py data ) : wx data = wx . New Id ( ) self . Set Item Data ( item , wx data ) self . py data map [ wx data ] = py data self . wx data map [ py data ] = wx data return wx data
def Find Py Data ( self , start , py data ) : wx data = self . wx data map [ py data ] if wx . VERSION < ( 3 , 0 , 0 ) or 'classic' in wx . version ( ) : data = self . Find Item Data ( start , wx data ) else : data = self . Find Item ( start , wx data ) return data
def Delete Item ( self , item ) : wx data = self . Get Item Data ( item ) py data = self . py data map [ wx data ] del self . py data map [ wx data ] del self . wx data map [ py data ] wx . List Ctrl . Delete Item ( self , item )
def Delete All Items ( self ) : self . py data map . clear ( ) self . wx data map . clear ( ) wx . List Ctrl . Delete All Items ( self )
def delete ( self , a position ) : key = self . wx obj . Get Py Data ( a position ) del self . items [ key ]
def clear all ( self ) : self . clear ( ) for ch in reversed ( self . columns ) : del self [ ch . name ]
def clear ( self ) : dict . clear ( self ) self . key = 0 if hasattr ( self . list view , "wx obj" ) : self . list view . wx obj . Delete All Items ( )
def set selection ( self , index , dummy = False ) : if index is None : self . wx obj . Set Selection ( - 1 ) if hasattr ( self . wx obj , "Set Value" ) : self . wx obj . Set Value ( "" ) else : self . wx obj . Set Selection ( index ) wx event = Item Container Control Select Event ( self . commandtype , index , self . wx obj ) if hasattr ( self , "onchange" ) and self . onchange : event = Form Event ( name = "change" , wx event = wx event ) self . onchange ( event )
def get string selection ( self ) : if self . multiselect : return [ self . wx obj . Get String ( i ) for i in self . wx obj . Get Selections ( ) ] else : return self . wx obj . Get String Selection ( )
def set data ( self , n , data ) : self . wx obj . Set Client Data ( n , data ) self . items dict [ data ] = self . get string ( n )
def append ( self , a string , data = None ) : self . wx obj . Append ( a string , data ) self . items dict [ data ] = a string
def delete ( self , a position ) : self . wx obj . Delete ( a position ) data = self . get data ( ) if data in self . items dict : del self . items dict [ data ]
def represent ( obj , prefix , parent = "" , indent = 0 , context = False , max cols = 80 ) : try : name = getattr ( obj , "name" , "" ) class name = "%s.%s" % ( prefix , obj . class . name ) padding = len ( class name ) + 1 + indent * 4 + ( 5 if context else 0 ) params = [ ] for ( k , spec ) in sorted ( obj . meta . specs . items ( ) , key = get sort key ) : if k == "index" : continue if k == "parent" and parent != "" : v = parent else : v = getattr ( obj , k , "" ) if ( not isinstance ( spec , Internal Spec ) and v != spec . default and ( k != 'id' or v > 0 ) and isinstance ( v , ( basestring , int , long , float , bool , dict , list , decimal . Decimal , datetime . datetime , datetime . date , datetime . time , Font , Color ) ) and repr ( v ) != 'None' ) : v = repr ( v ) else : v = None if v is not None : params . append ( "%s=%s" % ( k , v ) ) param lines = [ ] line = "" for param in params : if len ( line + param ) + 3 > max cols - padding : param lines . append ( line ) line = "" line += param + ", " param lines . append ( line ) param str = ( "\n%s" % ( " " * padding ) ) . join ( param lines ) return "%s(%s)" % ( class name , param str ) except : raise return object . repr ( obj )
def get ( obj name , init = False ) : wx parent = None if isinstance ( obj name , basestring ) : obj parent = COMPONENTS . get ( obj name ) if not obj parent : wx parent = wx . Find Window By Name ( obj name ) if wx parent : obj parent = getattr ( wx parent , "obj" ) else : for obj in COMPONENTS . values ( ) : if obj . name == obj name : obj parent = obj else : obj parent = obj name return obj parent or wx parent
def duplicate ( self , new parent = None ) : kwargs = { } for spec name , spec in self . meta . specs . items ( ) : value = getattr ( self , spec name ) if isinstance ( value , Color ) : print "COLOR" , value , value . default if value . default : value = None if value is not None : kwargs [ spec name ] = value del kwargs [ 'parent' ] new id = wx . New Id ( ) kwargs [ 'id' ] = new id kwargs [ 'name' ] = "%s %s" % ( kwargs [ 'name' ] , new id ) new obj = self . class ( new parent or self . get parent ( ) , * * kwargs ) for child in self : child . duplicate ( new obj ) return new obj
def sizer add ( self , child ) : if self . sizer : if DEBUG : print "adding to sizer:" , child . name border = None if not border : border = child . sizer border flags = child . sizer flags if child . sizer align : flags |= child . sizer align if child . sizer expand : flags |= wx . EXPAND if 'grid' in self . sizer : self . sizer . Add ( child . wx obj , flag = flags , border = border , pos = ( child . sizer row , child . sizer col ) , span = ( child . sizer rowspan , child . sizer colspan ) ) else : self . sizer . Add ( child . wx obj , 0 , flags , border )
def set parent ( self , new parent , init = False ) : Component . set parent ( self , new parent , init ) if not init : if DEBUG : print "reparenting" , ctrl . name if hasattr ( self . wx obj , "Reparent" ) : self . wx obj . Reparent ( self . parent . wx obj )
def resize ( self , evt = None ) : if DEBUG : print "RESIZE!" , self . name , self . width , self . height if not isinstance ( self . wx obj , wx . Top Level Window ) : if self . left and self . left [ - 1 ] == "%" or self . top and self . top [ - 1 ] == "%" : if DEBUG : print "MOVING" , self . name , self . width self . set pos ( ( self . left , self . top ) ) if self . width and self . width [ - 1 ] == "%" or self . height and self . height [ - 1 ] == "%" : if DEBUG : print "RESIZING" , self . name , self . width , self . height self . set size ( ( self . width , self . height ) ) for child in self : if isinstance ( child , Control ) : child . resize ( evt ) if evt : evt . Skip ( )
def tile background ( self , dc ) : sz = self . wx obj . Get Client Size ( ) bmp = self . bitmap . get bits ( ) w = bmp . Get Width ( ) h = bmp . Get Height ( ) if isinstance ( self , wx . Scrolled Window ) : spx , spy = self . wx obj . Get Scroll Pixels Per Unit ( ) vsx , vsy = self . wx obj . Get View Start ( ) dx , dy = ( spx * vsx ) % w , ( spy * vsy ) % h else : dx , dy = ( w , h ) x = - dx while x < sz . width : y = - dy while y < sz . height : dc . Draw Bitmap ( bmp , x , y ) y = y + h x = x + w
def on erase background ( self , evt ) : if self . bitmap : dc = evt . Get DC ( ) if not dc : dc = wx . Client DC ( self ) r = self . wx obj . Get Update Region ( ) . Get Box ( ) dc . Set Clipping Region ( r . x , r . y , r . width , r . height ) if self . background tiling : self . tile background ( dc ) else : dc . Draw Bitmap Point ( self . bitmap . get bits ( ) , ( 0 , 0 ) )
def on paint ( self , event ) : dc = wx . GCDC ( wx . Paint DC ( self . wx obj ) ) dc . Set Font ( self . wx obj . Get Font ( ) ) dc . Set Text Foreground ( self . wx obj . Get Foreground Colour ( ) ) dc . Draw Text ( self . wx obj . Get Label ( ) , 0 , 0 )
def get column headings ( self ) : headers = [ ctrl for ctrl in self if isinstance ( ctrl , Grid Column ) ] return sorted ( headers , key = lambda ch : ch . index )
def Reset View ( self , grid ) : grid . Begin Batch ( ) for current , new , delmsg , addmsg in [ ( self . rows , self . Get Number Rows ( ) , gridlib . GRIDTABLE NOTIFY ROWS DELETED , gridlib . GRIDTABLE NOTIFY ROWS APPENDED ) , ( self . cols , self . Get Number Cols ( ) , gridlib . GRIDTABLE NOTIFY COLS DELETED , gridlib . GRIDTABLE NOTIFY COLS APPENDED ) , ] : if new < current : msg = gridlib . Grid Table Message ( self , delmsg , new , current - new ) grid . Process Table Message ( msg ) elif new > current : msg = gridlib . Grid Table Message ( self , addmsg , new - current ) grid . Process Table Message ( msg ) self . Update Values ( grid ) grid . End Batch ( ) self . rows = self . Get Number Rows ( ) self . cols = self . Get Number Cols ( ) self . update Col Attrs ( grid ) grid . Adjust Scrollbars ( ) grid . Force Refresh ( )
def Update Values ( self , grid ) : msg = gridlib . Grid Table Message ( self , gridlib . GRIDTABLE REQUEST VIEW GET VALUES ) grid . Process Table Message ( msg )
def update Col Attrs ( self , grid ) : col = 0 for column in self . columns : attr = gridlib . Grid Cell Attr ( ) if False : attr . Set Read Only ( ) if False : attr . Set Renderer ( renderer ) grid . Set Col Size ( col , column . width ) grid . Set Col Attr ( col , attr ) col += 1
def clear ( self ) : for i in range ( len ( self ) - 1 , - 1 , - 1 ) : del self [ i ] self . key = 0 if hasattr ( self . grid view , "wx obj" ) : self . grid view . wx obj . Clear Grid ( )
def Create ( self , parent , id , evt Handler ) : self . tc = wx . Combo Box ( parent , id , "" , ( 100 , 50 ) ) self . Set Control ( self . tc ) self . tc . Push Event Handler ( wx . Evt Handler ( ) ) self . tc . Bind ( wx . EVT COMBOBOX , self . On Change )
def Begin Edit ( self , row , col , grid ) : self . start Value = grid . Get Table ( ) . Get Value ( row , col ) choices = grid . Get Table ( ) . columns [ col ] . choices self . tc . Clear ( ) self . tc . Append Items ( choices ) self . tc . Set String Selection ( self . start Value ) self . tc . Set Focus ( )
def End Edit ( self , row , col , grid , val = None ) : changed = False val = self . tc . Get String Selection ( ) print "val" , val , row , col , self . start Value if val != self . start Value : changed = True grid . Get Table ( ) . Set Value ( row , col , val ) self . start Value = '' self . tc . Set String Selection ( '' ) return changed
def Is Accepted Key ( self , evt ) : return ( not ( evt . Control Down ( ) or evt . Alt Down ( ) ) and evt . Get Key Code ( ) != wx . WXK SHIFT )
def Starting Key ( self , evt ) : key = evt . Get Key Code ( ) ch = None if key in [ wx . WXK NUMPAD0 , wx . WXK NUMPAD1 , wx . WXK NUMPAD2 , wx . WXK NUMPAD3 , wx . WXK NUMPAD4 , wx . WXK NUMPAD5 , wx . WXK NUMPAD6 , wx . WXK NUMPAD7 , wx . WXK NUMPAD8 , wx . WXK NUMPAD9 ] : ch = ch = chr ( ord ( '0' ) + key - wx . WXK NUMPAD0 ) elif key < 256 and key >= 0 and chr ( key ) in string . printable : ch = chr ( key ) if not evt . Shift Down ( ) : ch = ch . lower ( ) if ch is not None : self . tc . Set String Selection ( ch ) else : evt . Skip ( )
def Enable ( self , value ) : for i in range ( self . Get Menu Item Count ( ) ) : it = self . Find Item By Position ( i ) it . Enable ( value )
def Is Enabled ( self , * args , * * kwargs ) : for i in range ( self . Get Menu Item Count ( ) ) : it = self . Find Item By Position ( i ) if not it . Is Enabled ( ) : return False return True
def Enable ( self , value ) : for i in range ( self . Get Menu Count ( ) ) : self . Enable Top ( i , value )
def Is Enabled ( self , * args , * * kwargs ) : for i in range ( self . Get Menu Count ( ) ) : if not self . Is Enabled Top ( i ) : return False return True
def Remove Item ( self , menu ) : menus = self . Get Menus ( ) menus = [ submenu for submenu in menus if submenu [ 0 ] != menu ] self . Set Menus ( menus )
def set Object Tag ( self , object , tag ) : object . attributes = { } object . name = tag . Get Name ( ) . lower ( ) for name in self . attributes : object . attributes [ " %s" % name ] = tag . Get Param ( name ) if object . attributes [ " %s" % name ] == "" : object . attributes [ " %s" % name ] = None
def autosummary table visit html ( self , node ) : try : tbody = node [ 0 ] [ 0 ] [ - 1 ] for row in tbody : col1 entry = row [ 0 ] par = col1 entry [ 0 ] for j , subnode in enumerate ( list ( par ) ) : if isinstance ( subnode , nodes . Text ) : new text = unicode ( subnode . astext ( ) ) new text = new text . replace ( u" " , u"\u00a0" ) par [ j ] = nodes . Text ( new text ) except Index Error : pass
def mangle signature ( sig , max chars = 30 ) : s = re . sub ( r"^\((.*)\)$" , r"\1" , sig ) . strip ( ) # Strip strings (which can contain things that confuse the code below) s = re . sub ( r"\\\\" , "" , s ) s = re . sub ( r"\\'" , "" , s ) s = re . sub ( r"'[^']*'" , "" , s ) # Parse the signature to arguments + options args = [ ] opts = [ ] opt re = re . compile ( r"^(.*, |)([a-z A-Z0-9 *]+)=" ) while s : m = opt re . search ( s ) if not m : # The rest are arguments args = s . split ( ', ' ) break opts . insert ( 0 , m . group ( 2 ) ) s = m . group ( 1 ) [ : - 2 ] # Produce a more compact signature sig = limited join ( ", " , args , max chars = max chars - 2 ) if opts : if not sig : sig = "[%s]" % limited join ( ", " , opts , max chars = max chars - 4 ) elif len ( sig ) < max chars - 4 - 2 - 3 : sig += "[, %s]" % limited join ( ", " , opts , max chars = max chars - len ( sig ) - 4 - 2 ) return u"(%s)" % sig
def import by name ( name ) : try : name parts = name . split ( '.' ) # try first interpret `name` as MODNAME.OBJ modname = '.' . join ( name parts [ : - 1 ] ) if modname : try : import ( modname ) mod = sys . modules [ modname ] return getattr ( mod , name parts [ - 1 ] ) , mod except ( Import Error , Index Error , Attribute Error ) : pass # ... then as MODNAME, MODNAME.OBJ1, MODNAME.OBJ1.OBJ2, ... last j = 0 modname = None for j in reversed ( range ( 1 , len ( name parts ) + 1 ) ) : last j = j modname = '.' . join ( name parts [ : j ] ) try : import ( modname ) except : # Import Error: continue if modname in sys . modules : break if last j < len ( name parts ) : parent = None obj = sys . modules [ modname ] for obj name in name parts [ last j : ] : parent = obj obj = getattr ( obj , obj name ) return obj , parent else : return sys . modules [ modname ] , None except ( Value Error , Import Error , Attribute Error , Key Error ) , e : raise Import Error ( * e . args )
def alert ( message , title = "" , parent = None , scrolled = False , icon = "exclamation" ) : if not scrolled : icons = { 'exclamation' : wx . ICON EXCLAMATION , 'error' : wx . ICON ERROR , 'question' : wx . ICON QUESTION , 'info' : wx . ICON INFORMATION } style = wx . OK | icons [ icon ] result = dialogs . message Dialog ( parent , message , title , style ) else : result = dialogs . scrolled Message Dialog ( parent , message , title )
def prompt ( message = "" , title = "" , default = "" , multiline = False , password = None , parent = None ) : if password : style = wx . TE PASSWORD | wx . OK | wx . CANCEL result = dialogs . text Entry Dialog ( parent , message , title , default , style ) elif multiline : style = wx . TE MULTILINE | wx . OK | wx . CANCEL result = dialogs . text Entry Dialog ( parent , message , title , default , style ) result . text = '\n' . join ( result . text . splitlines ( ) ) else : result = dialogs . text Entry Dialog ( parent , message , title , default ) if result . accepted : return result . text
def select font ( message = "" , title = "" , font = None , parent = None ) : if font is not None : wx font = font . get wx font ( ) else : wx font = None font = Font ( ) result = dialogs . font Dialog ( parent , font = wx font ) if result . accepted : font data = result . font Data result . color = result . font Data . Get Colour ( ) . Get ( ) wx font = result . font Data . Get Chosen Font ( ) font . set wx font ( wx font ) wx font = None return font
def select color ( message = "" , title = "" , color = None , parent = None ) : result = dialogs . color Dialog ( parent , color = color ) return result . accepted and result . color
def choose directory ( message = 'Choose a directory' , path = "" , parent = None ) : result = dialogs . directory Dialog ( parent , message , path ) return result . path
def find ( default = '' , whole words = 0 , case sensitive = 0 , parent = None ) : result = dialogs . find Dialog ( parent , default , whole words , case sensitive ) return { 'text' : result . search Text , 'whole words' : result . whole Words Only , 'case sensitive' : result . case Sensitive }
def clear ( self ) : dict . clear ( self ) self . key = 0 if hasattr ( self . tree view , "wx obj" ) : self . tree view . wx obj . Delete All Items ( )
def set has children ( self , has children = True ) : self . tree model . tree view . wx obj . Set Item Has Children ( self . wx item , has children )
def set icon ( self , icon = None ) : if icon is not None : try : wx icon = wx . Icon ( icon , wx . BITMAP TYPE ICO ) self . wx obj . Set Icon ( wx icon ) except : pass
def show ( self , value = True , modal = None ) : self . wx obj . Show ( value ) if modal : disabler = wx . Window Disabler ( self . wx obj ) eventloop = wx . Event Loop ( ) def on close modal ( evt ) : evt . Skip ( ) eventloop . Exit ( ) self . wx obj . Bind ( wx . EVT CLOSE , on close modal ) eventloop . Run ( ) del disabler
def resize ( self , evt = None ) : for child in self : if isinstance ( child , Control ) : child . resize ( evt ) if evt : evt . Skip ( )
def parse ( filename = "" ) : # use the provided resource file: s = open ( filename ) . read ( ) ##s.decode("latin1").encode("utf8") import datetime , decimal rsrc = eval ( s ) return rsrc
def save ( filename , rsrc ) : s = pprint . pformat ( rsrc ) ## s = s.encode("utf8") open ( filename , "w" ) . write ( s )
def build window ( res ) : # windows specs (parameters) kwargs = dict ( res . items ( ) ) wintype = kwargs . pop ( 'type' ) menubar = kwargs . pop ( 'menubar' , None ) components = kwargs . pop ( 'components' ) panel = kwargs . pop ( 'panel' , { } ) from gui import registry import gui winclass = registry . WINDOWS [ wintype ] win = winclass ( * * kwargs ) # add an implicit panel by default (as pythoncard had) if False and panel is not None : panel [ 'name' ] = 'panel' p = gui . Panel ( win , * * panel ) else : p = win if components : for comp in components : build component ( comp , parent = p ) if menubar : mb = gui . Menu Bar ( name = "menu" , parent = win ) for menu in menubar : build component ( menu , parent = mb ) return win
def build component ( res , parent = None ) : # control specs (parameters) kwargs = dict ( res . items ( ) ) comtype = kwargs . pop ( 'type' ) if 'components' in res : components = kwargs . pop ( 'components' ) elif comtype == 'Menu' and 'items' in res : components = kwargs . pop ( 'items' ) else : components = [ ] from gui import registry if comtype in registry . CONTROLS : comclass = registry . CONTROLS [ comtype ] elif comtype in registry . MENU : comclass = registry . MENU [ comtype ] elif comtype in registry . MISC : comclass = registry . MISC [ comtype ] else : raise Runtime Error ( "%s not in registry" % comtype ) # Instantiate the GUI object com = comclass ( parent = parent , * * kwargs ) for comp in components : build component ( comp , parent = com ) return com
def convert ( self , name ) : new name = PYTHONCARD PROPERTY MAP . get ( name ) if new name : print "WARNING: property %s should be %s (%s)" % ( name , new name , self . obj . name ) return new name else : return name
def set data ( data ) : try : if wx . The Clipboard . Open ( ) : if isinstance ( data , ( str , unicode ) ) : do = wx . Text Data Object ( ) do . Set Text ( data ) wx . The Clipboard . Set Data ( do ) elif isinstance ( data , wx . Bitmap ) : do = wx . Bitmap Data Object ( ) do . Set Bitmap ( data ) wx . The Clipboard . Set Data ( do ) wx . The Clipboard . Close ( ) except : pass
def load object ( self , obj = None ) : # if not obj is given, do a full reload using the current root if obj : self . root obj = obj else : obj = self . root obj self . tree . Delete All Items ( ) self . root = self . tree . Add Root ( "application" ) self . tree . Set Item Text ( self . root , "App" , 1 ) self . tree . Set Item Text ( self . root , "col 2 root" , 2 ) #self.tree.Set Item Image(self.root, fldridx, which = wx.Tree Item Icon Normal) #self.tree.Set Item Image(self.root, fldropenidx, which = wx.Tree Item Icon Expanded) self . build tree ( self . root , obj ) self . tree . Expand ( self . root )
def inspect ( self , obj , context menu = False , edit prop = False , mouse pos = None ) : child = self . tree . Find Item ( self . root , obj . name ) if DEBUG : print "inspect child" , child if child : self . tree . Scroll To ( child ) self . tree . Set Current Item ( child ) self . tree . Select Item ( child ) child . Selected = True self . activate item ( child , edit prop ) if context menu : self . show context menu ( child , mouse pos )
def activate item ( self , child , edit prop = False , select = False ) : d = self . tree . Get Item Data ( child ) if d : o = d . Get Data ( ) self . selected obj = o callback = lambda o = o , * * kwargs : self . update ( o , * * kwargs ) self . propeditor . load object ( o , callback ) if edit prop : wx . Call After ( self . propeditor . edit ) if select and self . designer : self . designer . select ( o ) else : self . selected obj = None
def update ( self , obj , * * kwargs ) : # search for the old name: child = self . tree . Find Item ( self . root , kwargs [ 'name' ] ) if DEBUG : print "update child" , child , kwargs if child : self . tree . Scroll To ( child ) self . tree . Set Current Item ( child ) self . tree . Select Item ( child ) child . Selected = True # update the new name self . tree . Set Item Text ( child , obj . name , 0 )
def show context menu ( self , item , mouse pos = None ) : if item : d = self . tree . Get Item Data ( item ) if d : obj = d . Get Data ( ) if obj : # highligh and store the selected object: self . highlight ( obj . wx obj ) self . obj = obj # make the context menu menu = wx . Menu ( ) id del , id dup , id raise , id lower = [ wx . New Id ( ) for i in range ( 4 ) ] menu . Append ( id del , "Delete" ) menu . Append ( id dup , "Duplicate" ) menu . Append ( id raise , "Bring to Front" ) menu . Append ( id lower , "Send to Back" ) # make submenu! sm = wx . Menu ( ) for ctrl in sorted ( obj . meta . valid children , key = lambda c : registry . ALL . index ( c . meta . name ) ) : new id = wx . New Id ( ) sm . Append ( new id , ctrl . meta . name ) self . Bind ( wx . EVT MENU , lambda evt , ctrl = ctrl : self . add child ( ctrl , mouse pos ) , id = new id ) menu . Append Menu ( wx . New Id ( ) , "Add child" , sm ) self . Bind ( wx . EVT MENU , self . delete , id = id del ) self . Bind ( wx . EVT MENU , self . duplicate , id = id dup ) self . Bind ( wx . EVT MENU , self . bring to front , id = id raise ) self . Bind ( wx . EVT MENU , self . send to back , id = id lower ) self . Popup Menu ( menu ) menu . Destroy ( ) self . load object ( self . root obj )
def select option ( self ) : if self . disabled : warn ( "Attempt to select disabled option: {}" . format ( self . value or self . text ) ) self . base . select option ( )
def raise server error ( self ) : if self . server and self . server . error : try : if capybara . raise server errors : raise self . server . error finally : self . server . reset error ( )
def traceback ( self ) -> str : if not self . log traceback : return "" exc info = sys . exc info ( ) stack = traceback . extract stack ( ) exc tb = traceback . extract tb ( exc info [ 2 ] ) full tb = stack [ : 1 ] + exc tb # cut decorator and build full traceback exc line : typing . List [ str ] = traceback . format exception only ( * exc info [ : 2 ] ) # Make standard traceback string tb text = "\n Traceback (most recent call last):\n" + "" . join ( traceback . format list ( full tb ) ) + "" . join ( exc line ) return tb text
def get obj source ( self , instance : typing . Any , owner : typing . Optional [ type ] = None ) -> str : if self . log object repr : return f"{instance!r}" return f"<{owner. name  if owner is not None else instance. class . name }() at 0x{id(instance):X}>"
def logger ( self , logger : typing . Union [ logging . Logger , str , None ] ) -> None : if logger is None or isinstance ( logger , logging . Logger ) : self . logger = logger else : self . logger = logging . get Logger ( logger )
def channels ( self ) : if not self . channels : self . channels = self . call api ( 'channels.list' ) [ 'channels' ] return self . channels
def users ( self ) : if not self . users : self . users = self . call api ( 'users.list' ) [ 'members' ] return self . users
def channel from name ( self , name ) : try : channel = [ channel for channel in self . channels if channel [ 'name' ] == name ] [ 0 ] except Index Error : raise Value Error ( 'Unknown channel for name: "{}"' . format ( name ) ) return channel
def translate ( self , message ) : # translate user try : user id = message . pop ( 'user' ) user = self . slack . user from id ( user id ) message [ u'user' ] = user [ 'name' ] except ( Key Error , Index Error , Value Error ) : pass # translate channel try : if type ( message [ 'channel' ] ) == str : channel id = message . pop ( 'channel' ) else : channel id = message . pop ( 'channel' ) [ 'id' ] self . slack . reload channels ( ) channel = self . slack . channel from id ( channel id ) message [ u'channel' ] = channel [ 'name' ] except ( Key Error , Index Error , Value Error ) : pass return message
def send Slack ( self , message ) : channel = message . get ( 'channel' , 'general' ) self . send Message ( self . make message ( message [ 'text' ] , channel ) )
def read channel ( self ) : channel , message = self . protocol . channel layer . receive many ( [ u'slack.send' ] , block = False ) delay = 0.1 if channel : self . protocols [ 0 ] . send Slack ( message ) reactor . call Later ( delay , self . read channel )
def run ( self , args ) : args = self . parser . parse args ( args ) if not args . token : raise Value Error ( 'Supply the slack token through --token or setting DJANGOBOT TOKEN' ) # Import the channel layer sys . path . insert ( 0 , "." ) module path , object path = args . channel layer . split ( ':' , 1 ) channel layer = importlib . import module ( module path ) for part in object path . split ( '.' ) : channel layer = getattr ( channel layer , part ) # Boot up the client Client ( channel layer = channel layer , token = args . token , ) . run ( )
def dict diff ( prv , nxt ) : keys = set ( prv . keys ( ) + nxt . keys ( ) ) result = { } for k in keys : if prv . get ( k ) != nxt . get ( k ) : result [ k ] = ( prv . get ( k ) , nxt . get ( k ) ) return result
def colorize ( msg , color ) : if DONT COLORIZE : return msg else : return "{}{}{}" . format ( COLORS [ color ] , msg , COLORS [ "endc" ] )
def v2 playbook on task start ( self , task , * * kwargs ) : self . last task name = task . get name ( ) self . printed last task = False
def v2 runner on ok ( self , result , * * kwargs ) : failed = "failed" in result . result unreachable = "unreachable" in result . result if ( "print action" in result . task . tags or failed or unreachable or self . display . verbosity > 1 ) : self . print task ( ) self . last skipped = False msg = unicode ( result . result . get ( "msg" , "" ) ) or unicode ( result . result . get ( "reason" , "" ) ) or unicode ( result . result . get ( "message" , "" ) ) stderr = [ result . result . get ( "exception" , None ) , result . result . get ( "module stderr" , None ) , ] stderr = "\n" . join ( [ e for e in stderr if e ] ) . strip ( ) self . print host or item ( result . host , result . result . get ( "changed" , False ) , msg , result . result . get ( "diff" , None ) , is host = True , error = failed or unreachable , stdout = result . result . get ( "module stdout" , None ) , stderr = stderr . strip ( ) , ) if "results" in result . result : for r in result . result [ "results" ] : failed = "failed" in r stderr = [ r . get ( "exception" , None ) , r . get ( "module stderr" , None ) ] stderr = "\n" . join ( [ e for e in stderr if e ] ) . strip ( ) self . print host or item ( r [ "item" ] , r . get ( "changed" , False ) , unicode ( r . get ( "msg" , "" ) ) , r . get ( "diff" , None ) , is host = False , error = failed , stdout = r . get ( "module stdout" , None ) , stderr = stderr . strip ( ) , ) else : self . last skipped = True print ( "." , end = "" )
def v2 playbook on stats ( self , stats ) : print ( ) self . printed last task = False self . print task ( "STATS" ) hosts = sorted ( stats . processed . keys ( ) ) for host in hosts : s = stats . summarize ( host ) if s [ "failures" ] or s [ "unreachable" ] : color = "failed" elif s [ "changed" ] : color = "changed" else : color = "ok" msg = "{}    : ok={}\tchanged={}\tfailed={}\tunreachable={}" . format ( host , s [ "ok" ] , s [ "changed" ] , s [ "failures" ] , s [ "unreachable" ] ) print ( colorize ( msg , color ) )
def v2 runner on skipped ( self , result , * * kwargs ) : if self . display . verbosity > 1 : self . print task ( ) self . last skipped = False line length = 120 spaces = " " * ( 31 - len ( result . host . name ) - 4 ) line = "  * {}{}- {}" . format ( colorize ( result . host . name , "not so bold" ) , spaces , colorize ( "skipped" , "skipped" ) , ) reason = result . result . get ( "skipped reason" , "" ) or result . result . get ( "skip reason" , "" ) if len ( reason ) < 50 : line += " -- {}" . format ( reason ) print ( "{} {}---------" . format ( line , "-" * ( line length - len ( line ) ) ) ) else : print ( "{} {}" . format ( line , "-" * ( line length - len ( line ) ) ) ) print ( self . indent text ( reason , 8 ) ) print ( reason )
def load filters ( ) : all filters = { } for m in JINJA FILTERS : if hasattr ( m , "filters" ) : all filters . update ( m . filters ( ) ) return all filters
def get authorization ( self ) : auth = self . authorization class ( ) header = self . get authorization header ( ) if not header or not header . split : return auth header = header . split ( ) if len ( header ) > 1 and header [ 0 ] == 'Bearer' : auth . is oauth = True access token = header [ 1 ] self . validate access token ( access token , auth ) if not auth . is valid : auth . error = 'access denied' return auth
def open ( self , bus ) : # Close the device if it's already open. if self . device is not None : self . close ( ) # Try to open the file for the specified bus.  Must turn off buffering # or else Python 3 fails (see: https://bugs.python.org/issue20074) self . device = open ( '/dev/i2c-{0}' . format ( bus ) , 'r+b' , buffering = 0 )
def read byte ( self , addr ) : assert self . device is not None , 'Bus must be opened before operations are made against it!' self . select device ( addr ) return ord ( self . device . read ( 1 ) )
def read bytes ( self , addr , number ) : assert self . device is not None , 'Bus must be opened before operations are made against it!' self . select device ( addr ) return self . device . read ( number )
def read byte data ( self , addr , cmd ) : assert self . device is not None , 'Bus must be opened before operations are made against it!' # Build ctypes values to marshall between ioctl and Python. reg = c uint8 ( cmd ) result = c uint8 ( ) # Build ioctl request. request = make i2c rdwr data ( [ ( addr , 0 , 1 , pointer ( reg ) ) , # Write cmd register. ( addr , I2C M RD , 1 , pointer ( result ) ) # Read 1 byte as result. ] ) # Make ioctl call and return result data. ioctl ( self . device . fileno ( ) , I2C RDWR , request ) return result . value
def write quick ( self , addr ) : # What a strange function, from the python-smbus source this appears to # just write a single byte that initiates a write to the specified device # address (but writes no data!).  The functionality is duplicated below # but the actual use case for this is unknown. assert self . device is not None , 'Bus must be opened before operations are made against it!' # Build ioctl request. request = make i2c rdwr data ( [ ( addr , 0 , 0 , None ) , # Write with no data. ] ) # Make ioctl call and return result data. ioctl ( self . device . fileno ( ) , I2C RDWR , request )
def write byte ( self , addr , val ) : assert self . device is not None , 'Bus must be opened before operations are made against it!' self . select device ( addr ) data = bytearray ( 1 ) data [ 0 ] = val & 0x FF self . device . write ( data )
def write bytes ( self , addr , buf ) : assert self . device is not None , 'Bus must be opened before operations are made against it!' self . select device ( addr ) self . device . write ( buf )
def write byte data ( self , addr , cmd , val ) : assert self . device is not None , 'Bus must be opened before operations are made against it!' # Construct a string of data to send with the command register and byte value. data = bytearray ( 2 ) data [ 0 ] = cmd & 0x FF data [ 1 ] = val & 0x FF # Send the data to the device. self . select device ( addr ) self . device . write ( data )
def write i2c block data ( self , addr , cmd , vals ) : assert self . device is not None , 'Bus must be opened before operations are made against it!' # Construct a string of data to send, including room for the command register. data = bytearray ( len ( vals ) + 1 ) data [ 0 ] = cmd & 0x FF # Command register at the start. data [ 1 : ] = vals [ 0 : ] # Copy in the block data (ugly but necessary to ensure # the entire write happens in one transaction). # Send the data to the device. self . select device ( addr ) self . device . write ( data )
def datetime created ( self ) : if self . info ( ) . get ( 'datetime created' ) : return dateutil . parser . parse ( self . info ( ) [ 'datetime created' ] )
def construct from ( cls , group info ) : group = cls ( group info [ 'id' ] ) group . info cache = group info return group
def base opration ( self , method ) : uuids = self . uuids ( ) while True : chunk = list ( islice ( uuids , 0 , self . chunk size ) ) if not chunk : return rest request ( method , self . storage url , chunk )
def uuids ( self ) : for f in self . seq : if isinstance ( f , File ) : yield f . uuid elif isinstance ( f , six . string types ) : yield f else : raise Value Error ( 'Invalid type for sequence item: {0}' . format ( type ( f ) ) )
def list ( api list class , arg namespace , * * extra ) : if arg namespace . starting point : ordering field = ( arg namespace . ordering or '' ) . lstrip ( '-' ) if ordering field in ( '' , 'datetime uploaded' , 'datetime created' ) : arg namespace . starting point = parser . parse ( arg namespace . starting point ) items = api list class ( starting point = arg namespace . starting point , ordering = arg namespace . ordering , limit = arg namespace . limit , request limit = arg namespace . request limit , * * extra ) items . constructor = lambda x : x try : pprint ( list ( items ) ) except Value Error as e : print ( e )
def bar ( iter content , parts , title = '' ) : parts = max ( float ( parts ) , 1.0 ) cells = 10 progress = 0 step = cells / parts draw = lambda progress : sys . stdout . write ( '\r[{0:10}] {1:.2f}% {2}' . format ( '#' * int ( progress ) , progress * cells , title ) ) for chunk in iter content : yield chunk progress += step draw ( progress ) sys . stdout . flush ( ) draw ( cells ) print ( '' )
def home mode set state ( self , state , * * kwargs ) : # It appears that surveillance station needs lowercase text # true/false for the on switch if state not in ( HOME MODE ON , HOME MODE OFF ) : raise Value Error ( 'Invalid home mode state' ) api = self . api info [ 'home mode' ] payload = dict ( { 'api' : api [ 'name' ] , 'method' : 'Switch' , 'version' : api [ 'version' ] , 'on' : state , ' sid' : self . sid , } , * * kwargs ) response = self . get json with retry ( api [ 'url' ] , payload ) if response [ 'success' ] : return True return False
def home mode status ( self , * * kwargs ) : api = self . api info [ 'home mode' ] payload = dict ( { 'api' : api [ 'name' ] , 'method' : 'Get Info' , 'version' : api [ 'version' ] , ' sid' : self . sid } , * * kwargs ) response = self . get json with retry ( api [ 'url' ] , payload ) return response [ 'data' ] [ 'on' ]
def camera list ( self , * * kwargs ) : api = self . api info [ 'camera' ] payload = dict ( { ' sid' : self . sid , 'api' : api [ 'name' ] , 'method' : 'List' , 'version' : api [ 'version' ] , } , * * kwargs ) response = self . get json with retry ( api [ 'url' ] , payload ) cameras = [ ] for data in response [ 'data' ] [ 'cameras' ] : cameras . append ( Camera ( data , self . video stream url ) ) return cameras
def camera info ( self , camera ids , * * kwargs ) : api = self . api info [ 'camera' ] payload = dict ( { ' sid' : self . sid , 'api' : api [ 'name' ] , 'method' : 'Get Info' , 'version' : api [ 'version' ] , 'camera Ids' : ', ' . join ( str ( id ) for id in camera ids ) , } , * * kwargs ) response = self . get json with retry ( api [ 'url' ] , payload ) cameras = [ ] for data in response [ 'data' ] [ 'cameras' ] : cameras . append ( Camera ( data , self . video stream url ) ) return cameras
def camera snapshot ( self , camera id , * * kwargs ) : api = self . api info [ 'camera' ] payload = dict ( { ' sid' : self . sid , 'api' : api [ 'name' ] , 'method' : 'Get Snapshot' , 'version' : api [ 'version' ] , 'camera Id' : camera id , } , * * kwargs ) response = self . get ( api [ 'url' ] , payload ) return response . content
def camera event motion enum ( self , camera id , * * kwargs ) : api = self . api info [ 'camera event' ] payload = dict ( { ' sid' : self . sid , 'api' : api [ 'name' ] , 'method' : 'Motion Enum' , 'version' : api [ 'version' ] , 'cam Id' : camera id , } , * * kwargs ) response = self . get json with retry ( api [ 'url' ] , payload ) return Motion Setting ( camera id , response [ 'data' ] [ 'MD Param' ] )
def camera event md param save ( self , camera id , * * kwargs ) : api = self . api info [ 'camera event' ] payload = dict ( { ' sid' : self . sid , 'api' : api [ 'name' ] , 'method' : 'MD Param Save' , 'version' : api [ 'version' ] , 'cam Id' : camera id , } , * * kwargs ) response = self . get json with retry ( api [ 'url' ] , payload ) return response [ 'data' ] [ 'cam Id' ]
def update ( self ) : cameras = self . api . camera list ( ) self . cameras by id = { v . camera id : v for i , v in enumerate ( cameras ) } motion settings = [ ] for camera id in self . cameras by id . keys ( ) : motion setting = self . api . camera event motion enum ( camera id ) motion settings . append ( motion setting ) self . motion settings by id = { v . camera id : v for i , v in enumerate ( motion settings ) }
def set home mode ( self , state ) : state parameter = HOME MODE OFF if state : state parameter = HOME MODE ON return self . api . home mode set state ( state parameter )
def is last li ( li , meta data , current num Id ) : if not is li ( li , meta data ) : return False w namespace = get namespace ( li , 'w' ) next el = li while True : # If we run out of element this must be the last list item if next el is None : return True next el = next el . getnext ( ) # Ignore elements that are not a list item if not is li ( next el , meta data ) : continue new num Id = get num Id ( next el , w namespace ) if current num Id != new num Id : return True # If we have gotten here then we have found another list item in the # current list, so ``li`` is not the last li in the list. return False
def get single list nodes data ( li , meta data ) : yield li w namespace = get namespace ( li , 'w' ) current num Id = get num Id ( li , w namespace ) starting ilvl = get ilvl ( li , w namespace ) el = li while True : el = el . getnext ( ) if el is None : break # If the tag has no content ignore it. if not has text ( el ) : continue # Stop the lists if you come across a list item that should be a # heading. if is top level upper roman ( el , meta data ) : break if ( is li ( el , meta data ) and ( starting ilvl > get ilvl ( el , w namespace ) ) ) : break new num Id = get num Id ( el , w namespace ) if new num Id is None or new num Id == - 1 : # Not a p tag or a list item yield el continue # If the list id of the next tag is different that the previous that # means a new list being made (not nested) if current num Id != new num Id : # Not a subsequent list. break if is last li ( el , meta data , current num Id ) : yield el break yield el
def is bold ( r ) : w namespace = get namespace ( r , 'w' ) rpr = r . find ( '%sr Pr' % w namespace ) if rpr is None : return False bold = rpr . find ( '%sb' % w namespace ) return style is false ( bold )
def build list ( li nodes , meta data ) : # Need to keep track of all incomplete nested lists. ol dict = { } # Need to keep track of the current indentation level. current ilvl = - 1 # Need to keep track of the current list id. current num Id = - 1 # Need to keep track of list that new li tags should be added too. current ol = None # Store the first list created (the root list) for the return value. root ol = None visited nodes = [ ] list contents = [ ] def build li ( list contents ) : data = '<br />' . join ( t for t in list contents if t is not None ) return etree . XML ( '<li>%s</li>' % data ) def build non li content ( el , meta data ) : w namespace = get namespace ( el , 'w' ) if el . tag == '%stbl' % w namespace : new el , visited nodes = build table ( el , meta data ) return etree . tostring ( new el ) , visited nodes elif el . tag == '%sp' % w namespace : return get element content ( el , meta data ) , [ el ] if has text ( el ) : raise Unintended Tag ( 'Did not expect %s' % el . tag ) def merge lists ( ilvl , current ilvl , ol dict , current ol ) : for i in reversed ( range ( ilvl , current ilvl ) ) : # Any list that is more indented that ilvl needs to # be merged to the list before it. if i not in ol dict : continue if ol dict [ i ] is not current ol : if ol dict [ i ] is current ol : continue ol dict [ i ] [ - 1 ] . append ( current ol ) current ol = ol dict [ i ] # Clean up finished nested lists. for key in list ( ol dict ) : if key > ilvl : del ol dict [ key ] return current ol for li node in li nodes : w namespace = get namespace ( li node , 'w' ) if not is li ( li node , meta data ) : # Get the content and visited nodes new el , el visited nodes = build non li content ( li node , meta data , ) list contents . append ( new el ) visited nodes . extend ( el visited nodes ) continue if list contents : li el = build li ( list contents ) list contents = [ ] current ol . append ( li el ) # Get the data needed to build the current list item list contents . append ( get element content ( li node , meta data , ) ) ilvl = get ilvl ( li node , w namespace ) num Id = get num Id ( li node , w namespace ) list type = get ordered list type ( meta data , num Id , ilvl ) # If the ilvl is greater than the current ilvl or the list id is # changing then we have the first li tag in a nested list. We need to # create a new list object and update all of our variables for keeping # track. if ( ilvl > current ilvl ) or ( num Id != current num Id ) : # Only create a new list ol dict [ ilvl ] = create list ( list type ) current ol = ol dict [ ilvl ] current ilvl = ilvl current num Id = num Id # Both cases above are not True then we need to close all lists greater # than ilvl and then remove them from the ol dict else : # Merge any nested lists that need to be merged. current ol = merge lists ( ilvl = ilvl , current ilvl = current ilvl , ol dict = ol dict , current ol = current ol , ) # Set the root list after the first list is created. if root ol is None : root ol = current ol # Set the current list. if ilvl in ol dict : current ol = ol dict [ ilvl ] else : # In some instances the ilvl is not in the ol dict, if that is the # case, create it here (not sure how this happens but it has # before.) Only do this if the current ol is not the root ol, # otherwise etree will crash. if current ol is not root ol : # Merge the current ol into the root ol.  merge lists is not # equipped to handle this situation since the only way to get # into this block of code is to have mangled ilvls. root ol [ - 1 ] . append ( current ol ) # Reset the current ol current ol = create list ( list type ) # Create the li element. visited nodes . extend ( list ( li node . iter ( ) ) ) # If a list item is the last thing in a document, then you will need to add # it here. Should probably figure out how to get the above logic to deal # with it. if list contents : li el = build li ( list contents ) list contents = [ ] current ol . append ( li el ) # Merge up any nested lists that have not been merged. current ol = merge lists ( ilvl = 0 , current ilvl = current ilvl , ol dict = ol dict , current ol = current ol , ) return root ol , visited nodes
def build tr ( tr , meta data , row spans ) : # Create a blank tr element. tr el = etree . Element ( 'tr' ) w namespace = get namespace ( tr , 'w' ) visited nodes = [ ] for el in tr : if el in visited nodes : continue visited nodes . append ( el ) # Find the table cells. if el . tag == '%stc' % w namespace : v merge = get v merge ( el ) # If there is a v merge and it is not restart then this cell can be # ignored. if ( v merge is not None and v merge . get ( '%sval' % w namespace ) != 'restart' ) : continue # Loop through each and build a list of all the content. texts = [ ] for td content in el : # Since we are doing look-a-heads in this loop we need to check # again to see if we have already visited the node. if td content in visited nodes : continue # Check to see if it is a list or a regular paragraph. if is li ( td content , meta data ) : # If it is a list, create the list and update # visited nodes. li nodes = get single list nodes data ( td content , meta data , ) list el , list visited nodes = build list ( li nodes , meta data , ) visited nodes . extend ( list visited nodes ) texts . append ( etree . tostring ( list el ) ) elif td content . tag == '%stbl' % w namespace : table el , table visited nodes = build table ( td content , meta data , ) visited nodes . extend ( table visited nodes ) texts . append ( etree . tostring ( table el ) ) elif td content . tag == '%stc Pr' % w namespace : # Do nothing visited nodes . append ( td content ) continue else : text = get element content ( td content , meta data , is td = True , ) texts . append ( text ) data = '<br />' . join ( t for t in texts if t is not None ) td el = etree . XML ( '<td>%s</td>' % data ) # if there is a colspan then set it here. colspan = get grid span ( el ) if colspan > 1 : td el . set ( 'colspan' , '%d' % colspan ) v merge = get v merge ( el ) # If this td has a v merge and it is restart then set the rowspan # here. if ( v merge is not None and v merge . get ( '%sval' % w namespace ) == 'restart' ) : rowspan = next ( row spans ) td el . set ( 'rowspan' , '%d' % rowspan ) tr el . append ( td el ) return tr el
def build table ( table , meta data ) : # Create a blank table element. table el = etree . Element ( 'table' ) w namespace = get namespace ( table , 'w' ) # Get the rowspan values for cells that have a rowspan. row spans = get rowspan data ( table ) for el in table : if el . tag == '%str' % w namespace : # Create the tr element. tr el = build tr ( el , meta data , row spans , ) # And append it to the table. table el . append ( tr el ) visited nodes = list ( table . iter ( ) ) return table el , visited nodes
def get t tag content ( t , parent , remove bold , remove italics , meta data ) : if t is None or t . text is None : return '' # Need to escape the text so that we do not accidentally put in text # that is not valid XML. # cgi will replace things like & < > with &amp; &lt; &gt; text = cgi . escape ( t . text ) # Wrap the text with any modifiers it might have (bold, italics or # underline) el is bold = not remove bold and ( is bold ( parent ) or is underlined ( parent ) ) el is italics = not remove italics and is italics ( parent ) if el is bold : text = '<strong>%s</strong>' % text if el is italics : text = '<em>%s</em>' % text return text
def strip tag ( tree , tag ) : for el in tree . iter ( ) : if el . tag == tag : el . getparent ( ) . remove ( el )
def find ( dataset , url ) : fn = os . path . join ( DATASETS , dataset ) dn = os . path . dirname ( fn ) if not os . path . exists ( dn ) : print ( 'creating dataset directory: %s' , dn ) os . makedirs ( dn ) if not os . path . exists ( fn ) : if sys . version info < ( 3 , ) : urllib . urlretrieve ( url , fn ) else : urllib . request . urlretrieve ( url , fn ) return fn
def load mnist ( flatten = True , labels = False ) : fn = find ( 'mnist.pkl.gz' , 'http://deeplearning.net/data/mnist/mnist.pkl.gz' ) h = gzip . open ( fn , 'rb' ) if sys . version info < ( 3 , ) : ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) = pickle . load ( h ) else : ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) = pickle . load ( h , encoding = 'bytes' ) h . close ( ) if not flatten : timg = timg . reshape ( ( - 1 , 28 , 28 , 1 ) ) vimg = vimg . reshape ( ( - 1 , 28 , 28 , 1 ) ) simg = simg . reshape ( ( - 1 , 28 , 28 , 1 ) ) if labels : return ( ( timg , tlab . astype ( 'i' ) ) , ( vimg , vlab . astype ( 'i' ) ) , ( simg , slab . astype ( 'i' ) ) ) return ( timg , ) , ( vimg , ) , ( simg , )
def load cifar ( flatten = True , labels = False ) : def extract ( name ) : print ( 'extracting data from {}' . format ( name ) ) h = tar . extractfile ( name ) if sys . version info < ( 3 , ) : d = pickle . load ( h ) else : d = pickle . load ( h , encoding = 'bytes' ) for k in list ( d ) : d [ k . decode ( 'utf8' ) ] = d [ k ] h . close ( ) img = d [ 'data' ] . reshape ( ( - 1 , 3 , 32 , 32 ) ) . transpose ( ( 0 , 2 , 3 , 1 ) ) . astype ( 'f' ) / 128 - 1 if flatten : img = img . reshape ( ( - 1 , 32 * 32 * 3 ) ) d [ 'data' ] = img return d fn = find ( 'cifar10.tar.gz' , 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz' ) tar = tarfile . open ( fn ) imgs = [ ] labs = [ ] for i in range ( 1 , 6 ) : d = extract ( 'cifar-10-batches-py/data batch {}' . format ( i ) ) imgs . extend ( d [ 'data' ] ) labs . extend ( d [ 'labels' ] ) timg = np . asarray ( imgs [ : 40000 ] ) tlab = np . asarray ( labs [ : 40000 ] , 'i' ) vimg = np . asarray ( imgs [ 40000 : ] ) vlab = np . asarray ( labs [ 40000 : ] , 'i' ) d = extract ( 'cifar-10-batches-py/test batch' ) simg = d [ 'data' ] slab = d [ 'labels' ] tar . close ( ) if labels : return ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) return ( timg , ) , ( vimg , ) , ( simg , )
def plot layers ( weights , tied weights = False , channels = 1 ) : if hasattr ( weights [ 0 ] , 'get value' ) : weights = [ w . get value ( ) for w in weights ] k = min ( len ( weights ) , 9 ) imgs = np . eye ( weights [ 0 ] . shape [ 0 ] ) for i , weight in enumerate ( weights [ : - 1 ] ) : imgs = np . dot ( weight . T , imgs ) plot images ( imgs , 100 + 10 * k + i + 1 , channels = channels , title = 'Layer {}' . format ( i + 1 ) ) weight = weights [ - 1 ] n = weight . shape [ 1 ] / channels if int ( np . sqrt ( n ) ) ** 2 != n : return if tied weights : imgs = np . dot ( weight . T , imgs ) plot images ( imgs , 100 + 10 * k + k , channels = channels , title = 'Layer {}' . format ( k ) ) else : plot images ( weight , 100 + 10 * k + k , channels = channels , title = 'Decoding weights' )
def plot filters ( filters ) : imgs = filters . get value ( ) N , channels , x , y = imgs . shape n = int ( np . sqrt ( N ) ) assert n * n == N , 'filters must contain a square number of rows!' assert channels == 1 or channels == 3 , 'can only plot grayscale or rgb filters!' img = np . zeros ( ( ( y + 1 ) * n - 1 , ( x + 1 ) * n - 1 , channels ) , dtype = imgs [ 0 ] . dtype ) for i , pix in enumerate ( imgs ) : r , c = divmod ( i , n ) img [ r * ( y + 1 ) : ( r + 1 ) * ( y + 1 ) - 1 , c * ( x + 1 ) : ( c + 1 ) * ( x + 1 ) - 1 ] = pix . transpose ( ( 1 , 2 , 0 ) ) img -= img . min ( ) img /= img . max ( ) ax = plt . gcf ( ) . add subplot ( 111 ) ax . xaxis . set visible ( False ) ax . yaxis . set visible ( False ) ax . set frame on ( False ) ax . imshow ( img . squeeze ( ) , cmap = plt . cm . gray )
def batches ( dataset ) : seq lengths = dataset . variables [ 'seq Lengths' ] . data seq begins = np . concatenate ( ( [ 0 ] , np . cumsum ( seq lengths ) [ : - 1 ] ) ) def sample ( ) : chosen = np . random . choice ( list ( range ( len ( seq lengths ) ) ) , BATCH SIZE , replace = False ) return batch at ( dataset . variables [ 'inputs' ] . data , dataset . variables [ 'target Classes' ] . data , seq begins [ chosen ] , seq lengths [ chosen ] ) return sample
def variables ( self ) : result = [ self . target ] if self . weights is not None : result . append ( self . weights ) return result
def reservoir ( xs , n , rng ) : pool = [ ] for i , x in enumerate ( xs ) : if len ( pool ) < n : pool . append ( x / np . linalg . norm ( x ) ) continue j = rng . randint ( i + 1 ) if j < n : pool [ j ] = x / np . linalg . norm ( x ) # if the pool still has fewer than n items, pad with distorted random # duplicates from the source data. L = len ( pool ) S = np . std ( pool , axis = 0 ) while len ( pool ) < n : x = pool [ rng . randint ( L ) ] pool . append ( x + S * rng . randn ( * x . shape ) ) return np . array ( pool , dtype = pool [ 0 ] . dtype )
def inputs ( self ) : return [ l . input for l in self . layers if isinstance ( l , layers . Input ) ]
def variables ( self ) : result = self . inputs seen = set ( i . name for i in result ) for loss in self . losses : for v in loss . variables : if v . name not in seen : result . append ( v ) seen . add ( v . name ) return result
def output size ( self ) : shape = self . output shape if shape is None : raise util . Configuration Error ( 'undefined output size for layer "{}"' . format ( self . name ) ) return shape [ - 1 ]
def resolve outputs ( self ) : input shape = None for i , shape in enumerate ( self . input shapes . values ( ) ) : if i == 0 : input shape = shape if len ( input shape ) != len ( shape ) or any ( a is not None and b is not None and a != b for a , b in zip ( input shape [ : - 1 ] , shape [ : - 1 ] ) ) : raise util . Configuration Error ( 'layer "{}" incompatible input shapes {}' . format ( self . name , self . input shapes ) ) size = self . kwargs . get ( 'size' ) shape = self . kwargs . get ( 'shape' ) if shape is not None : pass elif size is not None : shape = tuple ( input shape [ : - 1 ] ) + ( size , ) else : raise util . Configuration Error ( 'layer "{}" does not specify a size' . format ( self . name ) ) self . output shapes [ 'out' ] = shape
def log ( self ) : inputs = ', ' . join ( '"{0}" {1}' . format ( * ns ) for ns in self . input shapes . items ( ) ) util . log ( 'layer {0. class . name } "{0.name}" {0.output shape} {1} from {2}' , self , getattr ( self . activate , 'name' , self . activate ) , inputs ) util . log ( 'learnable parameters: {}' , self . log params ( ) )
def log params ( self ) : total = 0 for p in self . params : shape = p . get value ( ) . shape util . log ( 'parameter "{}" {}' , p . name , shape ) total += np . prod ( shape ) return total
def fmt ( self , string ) : if '{' not in string : string = '{}.' + string return string . format ( self . name )
def get all intervals ( self ) : ints = sorted ( self . get intervals ( True ) ) if self . tier type == 'Interval Tier' : if not ints : ints . append ( ( self . xmin , self . xmax , '' ) ) else : if ints [ 0 ] [ 0 ] > self . xmin : ints . insert ( 0 , ( self . xmin , ints [ 0 ] [ 0 ] , '' ) ) if ints [ - 1 ] [ 1 ] < self . xmax : ints . append ( ( ints [ - 1 ] [ 1 ] , self . xmax , '' ) ) p = ints [ - 1 ] for index , i in reversed ( list ( enumerate ( ints [ : - 1 ] , 1 ) ) ) : if p [ 0 ] - i [ 1 ] != 0 : ints . insert ( index , ( i [ 1 ] , p [ 0 ] , '' ) ) p = i return ints
def main ( ) : import optparse import sys import codecs import locale import six from . algorithm import get display parser = optparse . Option Parser ( ) parser . add option ( '-e' , '--encoding' , dest = 'encoding' , default = 'utf-8' , type = 'string' , help = 'Text encoding (default: utf-8)' ) parser . add option ( '-u' , '--upper-is-rtl' , dest = 'upper is rtl' , default = False , action = 'store true' , help = "Treat upper case chars as strong 'R' " 'for debugging (default: False).' ) parser . add option ( '-d' , '--debug' , dest = 'debug' , default = False , action = 'store true' , help = "Output to stderr steps taken with the algorithm" ) parser . add option ( '-b' , '--base-dir' , dest = 'base dir' , default = None , type = 'string' , help = "Override base direction [L|R]" ) options , rest = parser . parse args ( ) if options . base dir and options . base dir not in 'LR' : parser . error ( 'option -b can be L or R' ) # allow unicode in sys.stdout.write if six . PY2 : sys . stdout = codecs . getwriter ( locale . getpreferredencoding ( ) ) ( sys . stdout ) if rest : lines = rest else : lines = sys . stdin for line in lines : display = get display ( line , options . encoding , options . upper is rtl , options . base dir , options . debug ) # adjust the encoding as unicode, to match the output encoding if not isinstance ( display , six . text type ) : display = display . decode ( options . encoding ) six . print ( display , end = '' )
def debug storage ( storage , base info = False , chars = True , runs = False ) : import codecs import locale import sys if six . PY2 : stderr = codecs . getwriter ( locale . getpreferredencoding ( ) ) ( sys . stderr ) else : stderr = sys . stderr caller = inspect . stack ( ) [ 1 ] [ 3 ] stderr . write ( 'in %s\n' % caller ) if base info : stderr . write ( u'  base level  : %d\n' % storage [ 'base level' ] ) stderr . write ( u'  base dir    : %s\n' % storage [ 'base dir' ] ) if runs : stderr . write ( u'  runs        : %s\n' % list ( storage [ 'runs' ] ) ) if chars : output = u'  Chars       : ' for ch in storage [ 'chars' ] : if ch != '\n' : output += ch [ 'ch' ] else : output += 'C' stderr . write ( output + u'\n' ) output = u'  Res. levels : %s\n' % u'' . join ( [ six . text type ( ch [ 'level' ] ) for ch in storage [ 'chars' ] ] ) stderr . write ( output ) types = [ ch [ 'type' ] . ljust ( 3 ) for ch in storage [ 'chars' ] ] for i in range ( 3 ) : if i : output = u'                %s\n' else : output = u'  Res. types  : %s\n' stderr . write ( output % u'' . join ( [ t [ i ] for t in types ] ) )
def reorder resolved levels ( storage , debug ) : # Applies L1. should reset = True chars = storage [ 'chars' ] for ch in chars [ : : - 1 ] : # L1. On each line, reset the embedding level of the following # characters to the paragraph embedding level: if ch [ 'orig' ] in ( 'B' , 'S' ) : # 1. Segment separators, # 2. Paragraph separators, ch [ 'level' ] = storage [ 'base level' ] should reset = True elif should reset and ch [ 'orig' ] in ( 'BN' , 'WS' ) : # 3. Any sequence of whitespace characters preceding a segment # separator or paragraph separator # 4. Any sequence of white space characters at the end of the # line. ch [ 'level' ] = storage [ 'base level' ] else : should reset = False max len = len ( chars ) # L2 should be per line # Calculates highest level and loweset odd level on the fly. line start = line end = 0 highest level = 0 lowest odd level = EXPLICIT LEVEL LIMIT for idx in range ( max len ) : ch = chars [ idx ] # calc the levels char level = ch [ 'level' ] if char level > highest level : highest level = char level if char level % 2 and char level < lowest odd level : lowest odd level = char level if ch [ 'orig' ] == 'B' or idx == max len - 1 : line end = idx # omit line breaks if ch [ 'orig' ] == 'B' : line end -= 1 reverse contiguous sequence ( chars , line start , line end , highest level , lowest odd level ) # reset for next line run line start = idx + 1 highest level = 0 lowest odd level = EXPLICIT LEVEL LIMIT if debug : debug storage ( storage )
def process ( self , context ) : import os from maya import cmds current file = cmds . file ( scene Name = True , query = True ) # Maya returns forward-slashes by default normalised = os . path . normpath ( current file ) context . set data ( 'current File' , value = normalised ) # For backwards compatibility context . set data ( 'current file' , value = normalised )
def add ( object , name , value ) : self . added . append ( name ) setattr ( object , name , value )
def cli ( args ) : import argparse parser = argparse . Argument Parser ( ) parser . add argument ( "--convert" , help = "Path to compiled Python module, e.g. my ui.py" ) parser . add argument ( "--compile" , help = "Accept raw .ui file and compile with native " "Py Side2 compiler." ) parser . add argument ( "--stdout" , help = "Write to stdout instead of file" , action = "store true" ) parser . add argument ( "--stdin" , help = "Read from stdin instead of file" , action = "store true" ) args = parser . parse args ( args ) if args . stdout : raise Not Implemented Error ( "--stdout" ) if args . stdin : raise Not Implemented Error ( "--stdin" ) if args . compile : raise Not Implemented Error ( "--compile" ) if args . convert : sys . stdout . write ( "#\n" "# WARNING: --convert is an ALPHA feature.\n#\n" "# for details.\n" "#\n" ) # # ------> Read # with open ( args . convert ) as f : lines = convert ( f . readlines ( ) ) backup = "%s backup%s" % os . path . splitext ( args . convert ) sys . stdout . write ( "Creating \"%s\"..\n" % backup ) shutil . copy ( args . convert , backup ) # # <------ Write # with open ( args . convert , "w" ) as f : f . write ( "" . join ( lines ) ) sys . stdout . write ( "Successfully converted \"%s\"\n" % args . convert )
def discover gui ( ) : # Prefer last registered guis = reversed ( pyblish . api . registered guis ( ) ) for gui in guis : try : gui = import ( gui ) . show except ( Import Error , Attribute Error ) : continue else : return gui
def get single axis values ( self , axis , dataset ) : data index = getattr ( self , '%s data index' % axis ) return [ p [ data index ] for p in dataset [ 'data' ] ]
def draw constant line ( self , value label style ) : value , label , style = value label style start = self . transform output coordinates ( ( 0 , value ) ) [ 1 ] stop = self . graph width path = etree . Sub Element ( self . graph , 'path' , { 'd' : 'M 0 %(start)s h%(stop)s' % locals ( ) , 'class' : 'constant Line' } ) if style : path . set ( 'style' , style ) text = etree . Sub Element ( self . graph , 'text' , { 'x' : str ( 2 ) , 'y' : str ( start - 2 ) , 'class' : 'constant Line' } ) text . text = label
def load transform parameters ( self ) : x min , x max , x div = self . x range ( ) y min , y max , y div = self . y range ( ) x step = ( float ( self . graph width ) - self . font size * 2 ) / ( x max - x min ) y step = ( float ( self . graph height ) - self . font size * 2 ) / ( y max - y min ) self . transform parameters = dict ( locals ( ) ) del self . transform parameters [ 'self' ]
def add popup ( self , x , y , label ) : txt width = len ( label ) * self . font size * 0.6 + 10 tx = x + [ 5 , - 5 ] [ int ( x + txt width > self . width ) ] anchor = [ 'start' , 'end' ] [ x + txt width > self . width ] style = 'fill: #000; text-anchor: %s;' % anchor id = 'label-%s' % self . w3c name ( label ) attrs = { 'x' : str ( tx ) , 'y' : str ( y - self . font size ) , 'visibility' : 'hidden' , 'style' : style , 'text' : label , 'id' : id , } etree . Sub Element ( self . foreground , 'text' , attrs ) # add the circle element to the foreground vis tmpl = ( "document.get Element By Id('{id}').set Attribute('visibility', {val})" ) attrs = { 'cx' : str ( x ) , 'cy' : str ( y ) , 'r' : str ( 10 ) , 'style' : 'opacity: 0;' , 'onmouseover' : vis tmpl . format ( val = 'visible' , id = id ) , 'onmouseout' : vis tmpl . format ( val = 'hidden' , id = id ) , } etree . Sub Element ( self . foreground , 'circle' , attrs )
def make datapoint text ( self , x , y , value , style = None ) : if not self . show data values : # do nothing return # first lay down the text in a wide white stroke to #  differentiate it from the background e = etree . Sub Element ( self . foreground , 'text' , { 'x' : str ( x ) , 'y' : str ( y ) , 'class' : 'data Point Label' , 'style' : '%(style)s stroke: #fff; stroke-width: 2;' % vars ( ) , } ) e . text = str ( value ) # then lay down the text in the specified style e = etree . Sub Element ( self . foreground , 'text' , { 'x' : str ( x ) , 'y' : str ( y ) , 'class' : 'data Point Label' } ) e . text = str ( value ) if style : e . set ( 'style' , style )
def draw x labels ( self ) : if self . show x labels : labels = self . get x labels ( ) count = len ( labels ) labels = enumerate ( iter ( labels ) ) start = int ( not self . step include first x label ) labels = itertools . islice ( labels , start , None , self . step x labels ) list ( map ( self . draw x label , labels ) ) self . draw x guidelines ( self . field width ( ) , count )
def draw y labels ( self ) : if not self . show y labels : # do nothing return labels = self . get y labels ( ) count = len ( labels ) labels = enumerate ( iter ( labels ) ) start = int ( not self . step include first y label ) labels = itertools . islice ( labels , start , None , self . step y labels ) list ( map ( self . draw y label , labels ) ) self . draw y guidelines ( self . field height ( ) , count )
def draw x guidelines ( self , label height , count ) : if not self . show x guidelines : return # skip the first one for count in range ( 1 , count ) : move = 'M {start} 0 v{stop}' . format ( start = label height * count , stop = self . graph height , ) path = { 'd' : move , 'class' : 'guide Lines' } etree . Sub Element ( self . graph , 'path' , path )
def draw y guidelines ( self , label height , count ) : if not self . show y guidelines : return for count in range ( 1 , count ) : move = 'M 0 {start} h{stop}' . format ( start = self . graph height - label height * count , stop = self . graph width , ) path = { 'd' : move , 'class' : 'guide Lines' } etree . Sub Element ( self . graph , 'path' , path )
def draw titles ( self ) : if self . show graph title : self . draw graph title ( ) if self . show graph subtitle : self . draw graph subtitle ( ) if self . show x title : self . draw x title ( ) if self . show y title : self . draw y title ( )
def render inline styles ( self ) : if not self . css inline : # do nothing return styles = self . parse css ( ) for node in self . root . xpath ( '//*[@class]' ) : cl = '.' + node . attrib [ 'class' ] if cl not in styles : continue style = styles [ cl ] if 'style' in node . attrib : style += node . attrib [ 'style' ] node . attrib [ 'style' ] = style
def start svg ( self ) : SVG NAMESPACE = 'http://www.w3.org/2000/svg' SVG = '{%s}' % SVG NAMESPACE NSMAP = { None : SVG NAMESPACE , 'xlink' : 'http://www.w3.org/1999/xlink' , 'a3' : 'http://ns.adobe.com/Adobe SVG Viewer Extensions/3.0/' , } root attrs = self . get root attributes ( ) self . root = etree . Element ( SVG + "svg" , attrib = root attrs , nsmap = NSMAP ) if hasattr ( self , 'style sheet href' ) : pi = etree . Processing Instruction ( 'xml-stylesheet' , 'href="%s" type="text/css"' % self . style sheet href ) self . root . addprevious ( pi ) comment strings = ( ' Created with SVG.Graph ' , ' SVG.Graph by Jason R. Coombs ' , ' Based on SVG::Graph by Sean E. Russel ' , ' Based on Perl SVG:TT:Graph by Leo Lapworth & Stephan Morgan ' , ' ' + '/' * 66 , ) list ( map ( self . root . append , map ( etree . Comment , comment strings ) ) ) defs = etree . Sub Element ( self . root , 'defs' ) self . add defs ( defs ) if not hasattr ( self , 'style sheet href' ) and not self . css inline : self . root . append ( etree . Comment ( ' include default stylesheet if none specified ' ) ) style = etree . Sub Element ( defs , 'style' , type = 'text/css' ) # TODO: the text was previously escaped in a CDATA declaration... how #  to do that with etree? style . text = self . get stylesheet ( ) . css Text self . root . append ( etree . Comment ( 'SVG Background' ) ) etree . Sub Element ( self . root , 'rect' , { 'width' : str ( self . width ) , 'height' : str ( self . height ) , 'x' : '0' , 'y' : '0' , 'class' : 'svg Background' } )
def get stylesheet resources ( self ) : # allow css to include class variables class vars = class dict ( self ) loader = functools . partial ( self . load resource stylesheet , subs = class vars ) sheets = list ( map ( loader , self . stylesheet names ) ) return sheets
def send validation email ( self ) : if self . email verified : raise Value Error ( ( 'Cannot validate already active user.' ) ) site = Site . objects . get current ( ) self . validation notification ( user = self , site = site ) . notify ( )
def send password reset ( self ) : site = Site . objects . get current ( ) self . password reset notification ( user = self , site = site ) . notify ( )
def allow request ( self , request , view ) : if request . method != 'POST' : return True return super ( Post Request Throttle Mixin , self ) . allow request ( request , view )
def client ( self ) : cls = self . class if cls . client is None : kwargs = { } if self . tls config : kwargs [ 'tls' ] = docker . tls . TLS Config ( * * self . tls config ) kwargs . update ( kwargs from env ( ) ) client = docker . API Client ( version = 'auto' , * * kwargs ) cls . client = client return cls . client
def poll ( self ) : service = yield self . get service ( ) if not service : self . log . warn ( "Docker service not found" ) return 0 task filter = { 'service' : service [ 'Spec' ] [ 'Name' ] } tasks = yield self . docker ( 'tasks' , task filter ) running task = None for task in tasks : task state = task [ 'Status' ] [ 'State' ] self . log . debug ( "Task %s of Docker service %s status: %s" , task [ 'ID' ] [ : 7 ] , self . service id [ : 7 ] , pformat ( task state ) , ) if task state == 'running' : # there should be at most one running task running task = task if running task is not None : return None else : return 1
def filter queryset ( self , value , queryset ) : return super ( Unique Email Validator , self ) . filter queryset ( value . lower ( ) , queryset , )
def update ( self , instance , validated data ) : if not instance . check password ( validated data [ 'old password' ] ) : msg = ( 'Invalid password.' ) raise serializers . Validation Error ( { 'old password' : msg } ) instance . set password ( validated data [ 'new password' ] ) instance . save ( ) return instance
def update ( self , instance , validated data ) : instance . set password ( validated data [ 'new password' ] ) instance . save ( ) return instance
def delete ( self , request , * args , * * kwargs ) : # Logic repeated from DRF because one cannot easily reuse it auth = get authorization header ( request ) . split ( ) if not auth or auth [ 0 ] . lower ( ) != b'token' : return response . Response ( status = status . HTTP 400 BAD REQUEST ) if len ( auth ) == 1 : msg = 'Invalid token header. No credentials provided.' return response . Response ( msg , status = status . HTTP 400 BAD REQUEST ) elif len ( auth ) > 2 : msg = 'Invalid token header. Token string should not contain spaces.' return response . Response ( msg , status = status . HTTP 400 BAD REQUEST ) try : token = self . model . objects . get ( key = auth [ 1 ] ) except self . model . Does Not Exist : pass else : token . delete ( ) signals . user logged out . send ( type ( self ) , user = token . user , request = request , ) return response . Response ( status = status . HTTP 204 NO CONTENT )
def initial ( self , request , * args , * * kwargs ) : email = request . data . get ( 'email' ) if request . user . is authenticated ( ) and email != request . user . email : raise Permission Denied ( ) return super ( Resend Confirmation Email , self ) . initial ( request , * args , * * kwargs )
def post ( self , request , * args , * * kwargs ) : serializer = self . serializer class ( data = request . data ) if not serializer . is valid ( ) : return response . Response ( serializer . errors , status = status . HTTP 400 BAD REQUEST , ) serializer . user . send validation email ( ) msg = ( 'Email confirmation sent.' ) return response . Response ( msg , status = status . HTTP 204 NO CONTENT )
def update expiry ( self , commit = True ) : self . expires = update expiry ( self . created ) if commit : self . save ( )
def password reset email context ( notification ) : return { 'protocol' : 'https' , 'uid' : notification . user . generate uid ( ) , 'token' : notification . user . generate token ( ) , 'site' : notification . site , }
def email handler ( notification , email context ) : incuna mail . send ( to = notification . user . email , subject = notification . email subject , template name = notification . text email template , html template name = notification . html email template , context = email context ( notification ) , headers = getattr ( notification , 'headers' , { } ) , )
def password reset email handler ( notification ) : base subject = ( '{domain} password reset' ) . format ( domain = notification . site . domain ) subject = getattr ( settings , 'DUM PASSWORD RESET SUBJECT' , base subject ) notification . email subject = subject email handler ( notification , password reset email context )
def validation email handler ( notification ) : base subject = ( '{domain} account validate' ) . format ( domain = notification . site . domain ) subject = getattr ( settings , 'DUM VALIDATE EMAIL SUBJECT' , base subject ) notification . email subject = subject email handler ( notification , validation email context )
def authenticate credentials ( self , key ) : user , token = super ( Token Authentication , self ) . authenticate credentials ( key ) if token . expires < timezone . now ( ) : msg = ( 'Token has expired.' ) raise exceptions . Authentication Failed ( msg ) # Update the token's expiration date token . update expiry ( ) return ( user , token )
def notebook show ( obj , doc , comm ) : target = obj . ref [ 'id' ] load mime = 'application/vnd.holoviews load.v0+json' exec mime = 'application/vnd.holoviews exec.v0+json' # Publish plot HTML bokeh script , bokeh div , = bokeh . embed . notebook . notebook content ( obj , comm . id ) publish display data ( data = { 'text/html' : encode utf8 ( bokeh div ) } ) # Publish comm manager JS = '\n' . join ( [ PYVIZ PROXY , Jupyter Comm Manager . js manager ] ) publish display data ( data = { load mime : JS , 'application/javascript' : JS } ) # Publish bokeh plot JS msg handler = bokeh msg handler . format ( plot id = target ) comm js = comm . js template . format ( plot id = target , comm id = comm . id , msg handler = msg handler ) bokeh js = '\n' . join ( [ comm js , bokeh script ] ) # Note: extension should be altered so text/html is not required publish display data ( data = { exec mime : '' , 'text/html' : '' , 'application/javascript' : bokeh js } , metadata = { exec mime : { 'id' : target } } )
def process hv plots ( widgets , plots ) : bokeh plots = [ ] for plot in plots : if hasattr ( plot , ' update callbacks' ) : for subplot in plot . traverse ( lambda x : x ) : subplot . comm = widgets . server comm for cb in subplot . callbacks : for c in cb . callbacks : c . code = c . code . replace ( plot . id , widgets . plot id ) plot = plot . state bokeh plots . append ( plot ) return bokeh plots
def widget ( self , param name ) : if param name not in self . widgets : self . widgets [ param name ] = self . make widget ( param name ) return self . widgets [ param name ]
def render function ( obj , view ) : try : import holoviews as hv except : hv = None if hv and isinstance ( obj , hv . core . Dimensioned ) : renderer = hv . renderer ( 'bokeh' ) if not view . notebook : renderer = renderer . instance ( mode = 'server' ) plot = renderer . get plot ( obj , doc = view . document ) if view . notebook : plot . comm = view . comm plot . document = view . document return plot . state return obj
def Text Widget ( * args , * * kw ) : kw [ 'value' ] = str ( kw [ 'value' ] ) kw . pop ( 'options' , None ) return Text Input ( * args , * * kw )
def ping ( self , params = None ) : try : self . transport . perform request ( 'HEAD' , '/' , params = params ) except Transport Error : raise gen . Return ( False ) raise gen . Return ( True )
def bytes to readable ( num ) : if num < 512 : return "0 Kb" elif num < 1024 : return "1 Kb" for unit in [ '' , 'Kb' , 'Mb' , 'Gb' , 'Tb' , 'Pb' , 'Eb' , 'Zb' ] : if abs ( num ) < 1024.0 : return "%3.1f%s" % ( num , unit ) num /= 1024.0 return "%.1f%s" % ( num , 'Yb' )
def cpu total load ( self ) : system load = self . cpu system load user load = self . cpu user load other load = self . cpu other load if system load is not None and user load is not None and other load is not None : return system load + user load + other load
def memory size ( self , human readable = True ) : if self . data is not None : return data = int ( self . data [ "memory" ] [ "memory size" ] ) * 1024 if human readable : return Syno Format Helper . bytes to readable ( return data ) else : return return data
def network up ( self , human readable = True ) : network = self . get network ( "total" ) if network is not None : return data = int ( network [ "tx" ] ) if human readable : return Syno Format Helper . bytes to readable ( return data ) else : return return data
def volumes ( self ) : if self . data is not None : volumes = [ ] for volume in self . data [ "volumes" ] : volumes . append ( volume [ "id" ] ) return volumes
def get volume ( self , volume id ) : if self . data is not None : for volume in self . data [ "volumes" ] : if volume [ "id" ] == volume id : return volume
def volume size total ( self , volume , human readable = True ) : volume = self . get volume ( volume ) if volume is not None : return data = int ( volume [ "size" ] [ "total" ] ) if human readable : return Syno Format Helper . bytes to readable ( return data ) else : return return data
def volume percentage used ( self , volume ) : volume = self . get volume ( volume ) if volume is not None : total = int ( volume [ "size" ] [ "total" ] ) used = int ( volume [ "size" ] [ "used" ] ) if used is not None and used > 0 and total is not None and total > 0 : return round ( ( float ( used ) / float ( total ) ) * 100.0 , 1 )
def volume disk temp avg ( self , volume ) : volume = self . get volume ( volume ) if volume is not None : vol disks = volume [ "disks" ] if vol disks is not None : total temp = 0 total disks = 0 for vol disk in vol disks : disk temp = self . disk temp ( vol disk ) if disk temp is not None : total disks += 1 total temp += disk temp if total temp > 0 and total disks > 0 : return round ( total temp / total disks , 0 )
def volume disk temp max ( self , volume ) : volume = self . get volume ( volume ) if volume is not None : vol disks = volume [ "disks" ] if vol disks is not None : max temp = 0 for vol disk in vol disks : disk temp = self . disk temp ( vol disk ) if disk temp is not None and disk temp > max temp : max temp = disk temp return max temp
def get disk ( self , disk id ) : if self . data is not None : for disk in self . data [ "disks" ] : if disk [ "id" ] == disk id : return disk
def login ( self ) : api path = "%s/auth.cgi?api=SYNO.API.Auth&version=2" % ( self . base url , ) login path = "method=login&%s" % ( self . encode credentials ( ) ) url = "%s&%s&session=Core&format=cookie" % ( api path , login path ) result = self . execute get url ( url , False ) if result is not None : self . access token = result [ "data" ] [ "sid" ] self . debuglog ( "Authentication Succesfull, token: " + str ( self . access token ) ) return True else : self . debuglog ( "Authentication Failed" ) return False
def get url ( self , url , retry on error = True ) : if self . access token is None or self . session is None or self . session error : self . access token = None self . session error = False if self . session is not None : self . session = None self . debuglog ( "Creating New Session" ) self . session = requests . Session ( ) if self . use https : self . session . verify = False if self . login ( ) is False : self . session error = True self . debuglog ( "Login Failed, unable to process request" ) return response = self . execute get url ( url ) if ( self . session error or response is None ) and retry on error : self . debuglog ( "Error occured, retrying..." ) self . get url ( url , False ) return response
def execute get url ( self , request url , append sid = True ) : self . debuglog ( "Requesting URL: '" + request url + "'" ) if append sid : self . debuglog ( "Appending access token (SID: " + self . access token + ") to url" ) request url = "%s& sid=%s" % ( request url , self . access token ) try : resp = self . session . get ( request url ) self . debuglog ( "Request executed: " + str ( resp . status code ) ) if resp . status code == 200 : json data = json . loads ( resp . text ) if json data [ "success" ] : self . debuglog ( "Succesfull returning data" ) self . debuglog ( str ( json data ) ) return json data else : if json data [ "error" ] [ "code" ] in { 105 , 106 , 107 , 119 } : self . debuglog ( "Session error: " + str ( json data [ "error" ] [ "code" ] ) ) self . session error = True else : self . debuglog ( "Failed: " + resp . text ) else : return None except : return None
def update ( self ) : if self . utilisation is not None : api = "SYNO.Core.System.Utilization" url = "%s/entry.cgi?api=%s&version=1&method=get& sid=%s" % ( self . base url , api , self . access token ) self . utilisation . update ( self . get url ( url ) ) if self . storage is not None : api = "SYNO.Storage.CGI.Storage" url = "%s/entry.cgi?api=%s&version=1&method=load info& sid=%s" % ( self . base url , api , self . access token ) self . storage . update ( self . get url ( url ) )
def utilisation ( self ) : if self . utilisation is None : api = "SYNO.Core.System.Utilization" url = "%s/entry.cgi?api=%s&version=1&method=get" % ( self . base url , api ) self . utilisation = Syno Utilization ( self . get url ( url ) ) return self . utilisation
def storage ( self ) : if self . storage is None : api = "SYNO.Storage.CGI.Storage" url = "%s/entry.cgi?api=%s&version=1&method=load info" % ( self . base url , api ) self . storage = Syno Storage ( self . get url ( url ) ) return self . storage
def for request ( request , body = None ) : tenant , jwt data = Tenant . objects . for request ( request , body ) webhook sender id = jwt data . get ( 'sub' ) sender data = None if body and 'item' in body : if 'sender' in body [ 'item' ] : sender data = body [ 'item' ] [ 'sender' ] elif 'message' in body [ 'item' ] and 'from' in body [ 'item' ] [ 'message' ] : sender data = body [ 'item' ] [ 'message' ] [ 'from' ] if sender data is None : if webhook sender id is None : raise Bad Tenant Error ( 'Cannot identify sender in tenant' ) sender data = { 'id' : webhook sender id } return Context ( tenant = tenant , sender = Hipchat User ( id = sender data . get ( 'id' ) , name = sender data . get ( 'name' ) , mention name = sender data . get ( 'mention name' ) , ) , signed request = request . GET . get ( 'signed request' ) , context = jwt data . get ( 'context' ) or { } , )
def tenant token ( self ) : rv = getattr ( self , ' tenant token' , None ) if rv is None : rv = self . tenant token = self . tenant . get token ( ) return rv
def build attrs ( self , extra attrs = None , * * kwargs ) : self . attrs = self . widget . build attrs ( extra attrs = None , * * kwargs ) return self . attrs
def get global settings ( self ) : return dict ( ( key , getattr ( global settings , key ) ) for key in dir ( global settings ) if key . isupper ( ) )
def do GET ( self ) : parsed url = urlparse ( self . path ) if parsed url [ 2 ] == "/" + SERVER REDIRECT PATH : # 2 = Path parsed query = parse qs ( parsed url [ 4 ] ) # 4 = Query if "code" not in parsed query : self . send response ( 200 ) self . send header ( "Content-Type" , "text/plain" ) self . end headers ( ) self . wfile . write ( "No code found, try again!" . encode ( "utf-8" ) ) return self . server . response code = parsed query [ "code" ] [ 0 ] self . send response ( 200 ) self . send header ( "Content-Type" , "text/plain" ) self . end headers ( ) self . wfile . write ( "Thank you for using O Auth2Util. The authorization was successful, " "you can now close this window." . encode ( "utf-8" ) ) elif parsed url [ 2 ] == "/" + SERVER LINK PATH : # 2 = Path self . send response ( 200 ) self . send header ( "Content-Type" , "text/html" ) self . end headers ( ) self . wfile . write ( "<html><body>Hey there!<br/>Click <a href=\"{0}\">here</a> to claim your prize.</body></html>" . format ( self . server . authorize url ) . encode ( "utf-8" ) ) else : self . send response ( 404 ) self . send header ( "Content-Type" , "text/plain" ) self . end headers ( ) self . wfile . write ( "404 not found" . encode ( "utf-8" ) )
def get value ( self , key , func = None , split val = None , as boolean = False , exception default = None ) : try : if as boolean : return self . config . getboolean ( key [ 0 ] , key [ 1 ] ) value = self . config . get ( key [ 0 ] , key [ 1 ] ) if split val is not None : value = value . split ( split val ) if func is not None : return func ( value ) return value except ( Key Error , configparser . No Section Error , configparser . No Option Error ) as e : if exception default is not None : return exception default raise Key Error ( e )
def change value ( self , key , value ) : if not self . config . has section ( key [ 0 ] ) : self . config . add section ( key [ 0 ] ) self . config . set ( key [ 0 ] , key [ 1 ] , str ( value ) ) with open ( self . configfile , "w" ) as f : self . config . write ( f )
def migrate config ( self , oldname = DEFAULT CONFIG , newname = DEFAULT CONFIG ) : self . log ( "Your O Auth2Util config file is in an old format and needs " "to be changed. I tried as best as I could to migrate it." , logging . WARNING ) with open ( oldname , "r" ) as old : with open ( newname , "w" ) as new : new . write ( "[app]\n" ) new . write ( old . read ( ) )
def start webserver ( self , authorize url = None ) : server address = ( SERVER URL , SERVER PORT ) self . server = HTTP Server ( server address , O Auth2Util Request Handler ) self . server . response code = None self . server . authorize url = authorize url t = Thread ( target = self . server . serve forever ) t . daemon = True t . start ( )
def wait for response ( self ) : while not self . server . response code : time . sleep ( 2 ) time . sleep ( 5 ) self . server . shutdown ( )
def get new access information ( self ) : if not self . r . has oauth app info : self . log ( 'Cannot obtain authorize url from PRAW. Please check your configuration.' , logging . ERROR ) raise Attribute Error ( 'Reddit Session invalid, please check your designated config file.' ) url = self . r . get authorize url ( 'Using O Auth2Util' , self . get value ( CONFIGKEY SCOPE , set , split val = ',' ) , self . get value ( CONFIGKEY REFRESHABLE , as boolean = True ) ) self . start webserver ( url ) if not self . get value ( CONFIGKEY SERVER MODE , as boolean = True ) : webbrowser . open ( url ) else : print ( "Webserver is waiting for you :D. Please open {0}:{1}/{2} " "in your browser" . format ( SERVER URL , SERVER PORT , SERVER LINK PATH ) ) self . wait for response ( ) try : access information = self . r . get access information ( self . server . response code ) except praw . errors . O Auth Exception : self . log ( "Can not authenticate, maybe the app infos (e.g. secret) are wrong." , logging . ERROR ) raise self . change value ( CONFIGKEY TOKEN , access information [ "access token" ] ) self . change value ( CONFIGKEY REFRESH TOKEN , access information [ "refresh token" ] ) self . change value ( CONFIGKEY VALID UNTIL , time . time ( ) + TOKEN VALID DURATION )
def check token present ( self ) : try : self . get value ( CONFIGKEY TOKEN ) self . get value ( CONFIGKEY REFRESH TOKEN ) self . get value ( CONFIGKEY REFRESHABLE ) except Key Error : self . log ( "Request new Token (CTP)" ) self . get new access information ( )
def set access credentials ( self , retry = 0 ) : if retry >= 5 : raise Connection Aborted Error ( 'Reddit is not accessible right now, cannot refresh O Auth2 tokens.' ) self . check token present ( ) try : self . r . set access credentials ( self . get value ( CONFIGKEY SCOPE , set , split val = "," ) , self . get value ( CONFIGKEY TOKEN ) , self . get value ( CONFIGKEY REFRESH TOKEN ) ) except ( praw . errors . O Auth Invalid Token , praw . errors . HTTP Exception ) as e : # todo check e status code # self. log('Retrying in 5s.') # time.sleep(5) # self.set access credentials( retry= retry + 1) self . log ( "Request new Token (SAC)" ) self . get new access information ( )
def fix schema ( prefix , schema ) : schema dict = extract schema ( schema ) snake case organization = schema dict [ 'vendor' ] . replace ( '.' , ' ' ) . lower ( ) snake case name = re . sub ( '([^A-Z ])([A-Z])' , '\g<1> \g<2>' , schema dict [ 'name' ] ) . lower ( ) model = schema dict [ 'version' ] . split ( '-' ) [ 0 ] return "{} {} {} {}" . format ( prefix , snake case organization , snake case name , model )
def transform ( line , known fields = ENRICHED EVENT FIELD TYPES , add geolocation data = True ) : return jsonify good event ( line . split ( '\t' ) , known fields , add geolocation data )
def jsonify good event ( event , known fields = ENRICHED EVENT FIELD TYPES , add geolocation data = True ) : if len ( event ) != len ( known fields ) : raise Snowplow Event Transformation Exception ( [ "Expected {} fields, received {} fields." . format ( len ( known fields ) , len ( event ) ) ] ) else : output = { } errors = [ ] if add geolocation data and event [ LATITUDE INDEX ] != '' and event [ LONGITUDE INDEX ] != '' : output [ 'geo location' ] = event [ LATITUDE INDEX ] + ',' + event [ LONGITUDE INDEX ] for i in range ( len ( event ) ) : key = known fields [ i ] [ 0 ] if event [ i ] != '' : try : kvpairs = known fields [ i ] [ 1 ] ( key , event [ i ] ) for kvpair in kvpairs : output [ kvpair [ 0 ] ] = kvpair [ 1 ] except Snowplow Event Transformation Exception as sete : errors += sete . error messages except Exception as e : errors += [ "Unexpected exception parsing field with key {} and value {}: {}" . format ( known fields [ i ] [ 0 ] , event [ i ] , repr ( e ) ) ] if errors : raise Snowplow Event Transformation Exception ( errors ) else : return output
def print context ( self , context ) : text = [ CONTEXT TITLE ] for i , context scope in enumerate ( context ) : dump1 = linebreaksbr ( pformat django context html ( context scope ) ) dump2 = pformat dict summary html ( context scope ) # Collapse long objects by default (e.g. request, LANGUAGES and sql queries) if len ( context scope ) <= 3 and dump1 . count ( '<br />' ) > 20 : ( dump1 , dump2 ) = ( dump2 , dump1 ) text . append ( CONTEXT BLOCK . format ( style = PRE STYLE , num = i , dump1 = dump1 , dump2 = dump2 ) ) return u'' . join ( text )
def print variables ( self , context ) : text = [ ] for name , expr in self . variables : # Some extended resolving, to handle unknown variables data = '' try : if isinstance ( expr . var , Variable ) : data = expr . var . resolve ( context ) else : data = expr . resolve ( context ) # could return TEMPLATE STRING IF INVALID except Variable Does Not Exist as e : # Failed to resolve, display exception inline keys = [ ] for scope in context : keys += scope . keys ( ) keys = sorted ( set ( keys ) ) # Remove duplicates, e.g. csrf token return ERROR TYPE BLOCK . format ( style = PRE ALERT STYLE , error = escape ( u"Variable '{0}' not found!  Available context variables are:\n\n{1}" . format ( expr , u', ' . join ( keys ) ) ) ) else : # Regular format textdata = linebreaksbr ( pformat django context html ( data ) ) # At top level, prefix class name if it's a longer result if isinstance ( data , SHORT NAME TYPES ) : text . append ( BASIC TYPE BLOCK . format ( style = PRE STYLE , name = name , value = textdata ) ) else : text . append ( OBJECT TYPE BLOCK . format ( style = PRE STYLE , name = name , type = data . class . name , value = textdata ) ) return u'' . join ( text )
def pformat sql html ( sql ) : sql = escape ( sql ) sql = RE SQL NL . sub ( u'<br>\n\\1' , sql ) sql = RE SQL . sub ( u'<strong>\\1</strong>' , sql ) return sql
def pformat dict summary html ( dict ) : if not dict : return '   {}' html = [ ] for key , value in sorted ( six . iteritems ( dict ) ) : if not isinstance ( value , DICT EXPANDED TYPES ) : value = '...' html . append ( format dict item ( key , value ) ) return mark safe ( u'<br/>' . join ( html ) )
def format ( self , object , stream , indent , allowance , context , level ) : try : Pretty Printer . format ( self , object , stream , indent , allowance , context , level ) except Exception as e : stream . write ( format exception ( e ) )
def get organisation information ( self , query params = None ) : return self . fetch json ( uri path = self . base uri , query params = query params or { } )
def get list information ( self , query params = None ) : return self . fetch json ( uri path = self . base uri , query params = query params or { } )
def add card ( self , query params = None ) : card json = self . fetch json ( uri path = self . base uri + '/cards' , http method = 'POST' , query params = query params or { } ) return self . create card ( card json )
def get label information ( self , query params = None ) : return self . fetch json ( uri path = self . base uri , query params = query params or { } )
def update label name ( self , name ) : label json = self . fetch json ( uri path = self . base uri , http method = 'PUT' , query params = { 'name' : name } ) return self . create label ( label json )
def update label dict ( self , query params = { } ) : label json = self . fetch json ( uri path = self . base uri , http method = 'PUT' , query params = query params ) return self . create label ( label json )
def get card information ( self , query params = None ) : return self . fetch json ( uri path = self . base uri , query params = query params or { } )
def add comment ( self , comment text ) : return self . fetch json ( uri path = self . base uri + '/actions/comments' , http method = 'POST' , query params = { 'text' : comment text } )
def add attachment ( self , filename , open file ) : fields = { 'api key' : self . client . api key , 'token' : self . client . user auth token } content type , body = self . encode multipart formdata ( fields = fields , filename = filename , file values = open file ) return self . fetch json ( uri path = self . base uri + '/attachments' , http method = 'POST' , body = body , headers = { 'Content-Type' : content type } , )
def add checklist ( self , query params = None ) : checklist json = self . fetch json ( uri path = self . base uri + '/checklists' , http method = 'POST' , query params = query params or { } ) return self . create checklist ( checklist json )
def add label from dict ( self , query params = None ) : return self . fetch json ( uri path = self . base uri + '/labels' , http method = 'POST' , query params = query params or { } )
def add label from class ( self , label = None ) : return self . fetch json ( uri path = self . base uri + '/id Labels' , http method = 'POST' , query params = { 'value' : label . id } )
def add member ( self , member id ) : members = self . fetch json ( uri path = self . base uri + '/id Members' , http method = 'POST' , query params = { 'value' : member id } ) members list = [ ] for member json in members : members list . append ( self . create member ( member json ) ) return members list
def create checklist item ( self , card id , checklist id , checklistitem json , * * kwargs ) : return self . client . create checklist item ( card id , checklist id , checklistitem json , * * kwargs )
def get board information ( self , query params = None ) : return self . fetch json ( uri path = '/boards/' + self . id , query params = query params or { } )
def get checklists ( self ) : checklists = self . get Checklists Json ( self . base uri ) checklists list = [ ] for checklist json in checklists : checklists list . append ( self . create Checklist ( checklist json ) ) return checklists list
def update board ( self , query params = None ) : board json = self . fetch json ( uri path = self . base uri , http method = 'PUT' , query params = query params or { } ) return self . create board ( board json )
def add list ( self , query params = None ) : list json = self . fetch json ( uri path = self . base uri + '/lists' , http method = 'POST' , query params = query params or { } ) return self . create list ( list json )
def add label ( self , query params = None ) : list json = self . fetch json ( uri path = self . base uri + '/labels' , http method = 'POST' , query params = query params or { } ) return self . create label ( list json )
def get checklist information ( self , query params = None ) : # We don't use trelloobject.Trello Object.get checklist json, because # that is meant to return lists of checklists. return self . fetch json ( uri path = self . base uri , query params = query params or { } )
def get card ( self ) : card id = self . get checklist information ( ) . get ( 'id Card' , None ) if card id : return self . client . get card ( card id )
def get item objects ( self , query params = None ) : card = self . get card ( ) checklistitems list = [ ] for checklistitem json in self . get items ( query params ) : checklistitems list . append ( self . create checklist item ( card . id , self . id , checklistitem json ) ) return checklistitems list
def update checklist ( self , name ) : checklist json = self . fetch json ( uri path = self . base uri , http method = 'PUT' , query params = { 'name' : name } ) return self . create checklist ( checklist json )
def remove item ( self , item id ) : return self . fetch json ( uri path = self . base uri + '/check Items/' + item id , http method = 'DELETE' )
def update name ( self , name ) : checklistitem json = self . fetch json ( uri path = self . base uri + '/name' , http method = 'PUT' , query params = { 'value' : name } ) return self . create checklist item ( self . id Card , self . id Checklist , checklistitem json )
def update state ( self , state ) : checklistitem json = self . fetch json ( uri path = self . base uri + '/state' , http method = 'PUT' , query params = { 'value' : 'complete' if state else 'incomplete' } ) return self . create checklist item ( self . id Card , self . id Checklist , checklistitem json )
def add authorisation ( self , query params ) : query params [ 'key' ] = self . api key if self . user auth token : query params [ 'token' ] = self . user auth token return query params
def check errors ( self , uri , response ) : if response . status == 401 : raise trolly . Unauthorised ( uri , response ) if response . status != 200 : raise trolly . Resource Unavailable ( uri , response )
def build uri ( self , path , query params ) : url = 'https://api.trello.com/1' + self . clean path ( path ) url += '?' + urlencode ( query params ) return url
def create checklist item ( self , card id , checklist id , checklistitem json ) : return trolly . checklist . Checklist Item ( trello client = self , card id = card id , checklist id = checklist id , checklistitem id = checklistitem json [ 'id' ] . encode ( 'utf-8' ) , name = checklistitem json [ 'name' ] . encode ( 'utf-8' ) , state = checklistitem json [ 'state' ] . encode ( 'utf-8' ) )
def set password ( self , service , username , password ) : assoc = self . generate assoc ( service , username ) # encrypt the password password encrypted = self . encrypt ( password . encode ( 'utf-8' ) , assoc ) # encode with base64 and add line break to untangle config file password base64 = '\n' + encodebytes ( password encrypted ) . decode ( ) self . write config value ( service , username , password base64 )
def main ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] cli = Command Line Tool ( ) try : return cli . run ( argv ) except Keyboard Interrupt : print ( 'Canceled' ) return 3
def create cipher ( self , password , salt , nonce = None ) : from argon2 . low level import hash secret raw , Type from Crypto . Cipher import AES aesmode = self . get mode ( self . aesmode ) if aesmode is None : # pragma: no cover raise Value Error ( 'invalid AES mode: %s' % self . aesmode ) key = hash secret raw ( secret = password . encode ( self . password encoding ) , salt = salt , time cost = self . time cost , memory cost = self . memory cost , parallelism = self . parallelism , hash len = 16 , type = Type . ID ) return AES . new ( key , aesmode , nonce )
def get mode ( mode = None ) : from Crypto . Cipher import AES AES Mode Map = { 'CCM' : AES . MODE CCM , 'EAX' : AES . MODE EAX , 'GCM' : AES . MODE GCM , 'OCB' : AES . MODE OCB , } if mode is None : return AES Mode Map . keys ( ) return AES Mode Map . get ( mode )
def connect To Broker ( self , protocol ) : self . protocol = protocol self . protocol . on Publish = self . on Publish self . protocol . on Disconnection = self . on Disconnection self . protocol . set Window Size ( 3 ) try : yield self . protocol . connect ( "Twisted MQTT-subs" , keepalive = 60 ) yield self . subscribe ( ) except Exception as e : log . error ( "Connecting to {broker} raised {excp!s}" , broker = BROKER , excp = e ) else : log . info ( "Connected and subscribed to {broker}" , broker = BROKER )
def on Publish ( self , topic , payload , qos , dup , retain , msg Id ) : log . debug ( "msg={payload}" , payload = payload )
def connect To Broker ( self , protocol ) : self . protocol = protocol self . protocol . on Publish = self . on Publish self . protocol . on Disconnection = self . on Disconnection self . protocol . set Window Size ( 3 ) self . task = task . Looping Call ( self . publish ) self . task . start ( 5.0 , now = False ) try : yield self . protocol . connect ( "Twisted MQTT-pubsubs" , keepalive = 60 ) yield self . subscribe ( ) except Exception as e : log . error ( "Connecting to {broker} raised {excp!s}" , broker = BROKER , excp = e ) else : log . info ( "Connected and subscribed to {broker}" , broker = BROKER )
def make Id ( self ) : self . id = ( self . id + 1 ) % 65536 self . id = self . id or 1 # avoid id 0 return self . id
def connect ( self , request ) : state = self . class . name return defer . fail ( MQTT State Error ( "Unexpected connect() operation" , state ) )
def handle CONNACK ( self , response ) : state = self . class . name log . error ( "Unexpected {packet:7} packet received in {log source}" , packet = "CONNACK" )
def encode ( self ) : header = bytearray ( 2 ) header [ 0 ] = 0x E0 self . encoded = header return str ( header ) if PY2 else bytes ( header )
def decode ( self , packet ) : self . encoded = packet # Strip the fixed header plus variable length field len Len = 1 while packet [ len Len ] & 0x80 : len Len += 1 packet remaining = packet [ len Len + 1 : ] # Variable Header version str , packet remaining = decode String ( packet remaining ) version id = int ( packet remaining [ 0 ] ) if version id == v31 [ 'level' ] : self . version = v31 else : self . version = v311 flags = packet remaining [ 1 ] self . clean Start = ( flags & 0x02 ) != 0 will Flag = ( flags & 0x04 ) != 0 will Qo S = ( flags >> 3 ) & 0x03 will Retain = ( flags & 0x20 ) != 0 user Flag = ( flags & 0x80 ) != 0 pass Flag = ( flags & 0x40 ) != 0 packet remaining = packet remaining [ 2 : ] self . keepalive = decode16Int ( packet remaining ) # Payload packet remaining = packet remaining [ 2 : ] self . client Id , packet remaining = decode String ( packet remaining ) if will Flag : self . will Retain = will Retain self . will Qo S = will Qo S self . will Topic , packet remaining = decode String ( packet remaining ) self . will Message , packet remaining = decode String ( packet remaining ) if user Flag : self . username , packet remaining = decode String ( packet remaining ) if pass Flag : l = decode16Int ( packet remaining ) self . password = packet remaining [ 2 : 2 + l ]
def encode ( self ) : header = bytearray ( 1 ) var Header = bytearray ( 2 ) header [ 0 ] = 0x20 var Header [ 0 ] = self . session var Header [ 1 ] = self . result Code header . extend ( encode Length ( len ( var Header ) ) ) header . extend ( var Header ) self . encoded = header return str ( header ) if PY2 else bytes ( header )
def decode ( self , packet ) : self . encoded = packet # Strip the fixed header plus variable length field len Len = 1 while packet [ len Len ] & 0x80 : len Len += 1 packet remaining = packet [ len Len + 1 : ] self . session = ( packet remaining [ 0 ] & 0x01 ) == 0x01 self . result Code = int ( packet remaining [ 1 ] )
def decode ( self , packet ) : self . encoded = packet len Len = 1 while packet [ len Len ] & 0x80 : len Len += 1 packet remaining = packet [ len Len + 1 : ] self . msg Id = decode16Int ( packet remaining [ 0 : 2 ] ) self . topics = [ ] packet remaining = packet remaining [ 2 : ] while len ( packet remaining ) : topic , packet remaining = decode String ( packet remaining ) qos = int ( packet remaining [ 0 ] ) & 0x03 self . topics . append ( ( topic , qos ) ) packet remaining = packet remaining [ 1 : ]
def encode ( self ) : header = bytearray ( 1 ) payload = bytearray ( ) var Header = encode16Int ( self . msg Id ) header [ 0 ] = 0x90 for code in self . granted : payload . append ( code [ 0 ] | ( 0x80 if code [ 1 ] == True else 0x00 ) ) header . extend ( encode Length ( len ( var Header ) + len ( payload ) ) ) header . extend ( var Header ) header . extend ( payload ) self . encoded = header return str ( header ) if PY2 else bytes ( header )
def decode ( self , packet ) : self . encoded = packet len Len = 1 while packet [ len Len ] & 0x80 : len Len += 1 packet remaining = packet [ len Len + 1 : ] self . msg Id = decode16Int ( packet remaining [ 0 : 2 ] ) self . topics = [ ] packet remaining = packet remaining [ 2 : ] while len ( packet remaining ) : l = decode16Int ( packet remaining [ 0 : 2 ] ) topic = packet remaining [ 2 : 2 + l ] . decode ( encoding = 'utf-8' ) self . topics . append ( topic ) packet remaining = packet remaining [ 2 + l : ]
def encode ( self ) : header = bytearray ( 1 ) var Header = encode16Int ( self . msg Id ) header [ 0 ] = 0x B0 header . extend ( encode Length ( len ( var Header ) ) ) header . extend ( var Header ) self . encoded = header return str ( header ) if PY2 else bytes ( header )
def decode ( self , packet ) : self . encoded = packet len Len = 1 while packet [ len Len ] & 0x80 : len Len += 1 packet remaining = packet [ len Len + 1 : ] self . dup = ( packet [ 0 ] & 0x08 ) == 0x08 self . qos = ( packet [ 0 ] & 0x06 ) >> 1 self . retain = ( packet [ 0 ] & 0x01 ) == 0x01 self . topic , = decode String ( packet remaining ) topic Len = decode16Int ( packet remaining ) if self . qos : self . msg Id = decode16Int ( packet remaining [ topic Len + 2 : topic Len + 4 ] ) self . payload = packet remaining [ topic Len + 4 : ] else : self . msg Id = None self . payload = packet remaining [ topic Len + 2 : ]
def decode ( self , packet ) : self . encoded = packet len Len = 1 while packet [ len Len ] & 0x80 : len Len += 1 packet remaining = packet [ len Len + 1 : ] self . msg Id = decode16Int ( packet remaining ) self . dup = ( packet [ 0 ] & 0x08 ) == 0x08
def refresh ( self ) : if self . comm . rank == 0 : self . blocks = self . list blocks ( ) else : self . blocks = None self . blocks = self . comm . bcast ( self . blocks )
def get total time span ( d ) : tmax = 0 for di in d . values ( ) : if di . u Time . max ( ) > tmax : tmax = di . u Time . max ( ) return tmax
def get defined srms ( srm file ) : srms = read table ( srm file ) return np . asanyarray ( srms . index . unique ( ) )
def read configuration ( config = 'DEFAULT' ) : # read configuration file , conf = read latoolscfg ( ) # if 'DEFAULT', check which is the default configuration if config == 'DEFAULT' : config = conf [ 'DEFAULT' ] [ 'config' ] # grab the chosen configuration conf = dict ( conf [ config ] ) # update config name with chosen conf [ 'config' ] = config return conf
def print all ( ) : # read configuration file , conf = read latoolscfg ( ) default = conf [ 'DEFAULT' ] [ 'config' ] pstr = '\n Currently defined L Atools configurations:\n\n' for s in conf . sections ( ) : if s == default : pstr += s + ' [DEFAULT]\n' elif s == 'REPRODUCE' : pstr += s + ' [DO NOT ALTER]\n' else : pstr += s + '\n' for k , v in conf [ s ] . items ( ) : if k != 'config' : if v [ : 9 ] == 'resources' : v = pkgrs . resource filename ( 'latools' , v ) pstr += '   ' + k + ': ' + v + '\n' pstr += '\n' print ( pstr ) return
def change default ( config ) : config file , cf = read latoolscfg ( ) if config not in cf . sections ( ) : raise Value Error ( "\n'{:s}' is not a defined configuration." . format ( config ) ) if config == 'REPRODUCE' : pstr = ( 'Are you SURE you want to set REPRODUCE as your default configuration?\n' + '     ... this is an odd thing to be doing.' ) else : pstr = ( 'Are you sure you want to change the default configuration from {:s}' . format ( cf [ 'DEFAULT' ] [ 'config' ] ) + 'to {:s}?' . format ( config ) ) response = input ( pstr + '\n> [N/y]: ' ) if response . lower ( ) == 'y' : cf . set ( 'DEFAULT' , 'config' , config ) with open ( config file , 'w' ) as f : cf . write ( f ) print ( '  Default changed!' ) else : print ( '  Done nothing.' )
def autorange plot ( self , analyte = 'total counts' , gwin = 7 , swin = None , win = 20 , on mult = [ 1.5 , 1. ] , off mult = [ 1. , 1.5 ] , transform = 'log' ) : if analyte is None : # sig = self.focus[self.internal standard] sig = self . data [ 'total counts' ] elif analyte == 'total counts' : sig = self . data [ 'total counts' ] elif analyte in self . analytes : sig = self . focus [ analyte ] else : raise Value Error ( 'Invalid analyte.' ) if transform == 'log' : sig = np . log10 ( sig ) fig , axs = plot . autorange plot ( t = self . Time , sig = sig , gwin = gwin , swin = swin , win = win , on mult = on mult , off mult = off mult ) return fig , axs
def rangecalc ( x , y = None , pad = 0.05 ) : mn = np . nanmin ( [ np . nanmin ( x ) , np . nanmin ( y ) ] ) mx = np . nanmax ( [ np . nanmax ( x ) , np . nanmax ( y ) ] ) rn = mx - mn return ( mn - pad * rn , mx + pad * rn )
def rangecalcx ( x , pad = 0.05 ) : mn = np . nanmin ( x ) mx = np . nanmax ( x ) rn = mx - mn return ( mn - pad * rn , mx + pad * rn )
def gen keywords ( * args : Union [ ANSI Colors , ANSI Styles ] , * * kwargs : Union [ ANSI Colors , ANSI Styles ] ) -> tuple : fields : tuple = tuple ( ) values : tuple = tuple ( ) for tpl in args : fields += tpl . fields values += tpl for prefix , tpl in kwargs . items ( ) : fields += tuple ( map ( lambda x : ' ' . join ( [ prefix , x ] ) , tpl . fields ) ) values += tpl return namedtuple ( 'ANSI Sequences' , fields ) ( * values )
def dedup ( stack : tuple ) -> tuple : # Initializes with an accumulator and then reduces the stack with first match # deduplication. reducer = lambda x , y : x if y in x else x + ( y , ) return reduce ( reducer , stack , tuple ( ) )
def stderr ( a ) : return np . nanstd ( a ) / np . sqrt ( sum ( np . isfinite ( a ) ) )
def filter nremoved ( self , filt = True , quiet = False ) : rminfo = { } for n in self . subsets [ 'All Samples' ] : s = self . data [ n ] rminfo [ n ] = s . filt nremoved ( filt ) if not quiet : max L = max ( [ len ( s ) for s in rminfo . keys ( ) ] ) print ( '{string:{number}s}' . format ( string = 'Sample ' , number = max L + 3 ) + '{total:4s}' . format ( total = 'tot' ) + '{removed:4s}' . format ( removed = 'flt' ) + '{percent:4s}' . format ( percent = '%rm' ) ) for k , ( ntot , nfilt , pcrm ) in rminfo . items ( ) : print ( '{string:{number}s}' . format ( string = k , number = max L + 3 ) + '{total:4.0f}' . format ( total = ntot ) + '{removed:4.0f}' . format ( removed = nfilt ) + '{percent:4.0f}' . format ( percent = pcrm ) ) return rminfo
def getstats ( self , save = True , filename = None , samples = None , subset = None , ablation time = False ) : slst = [ ] if samples is not None : subset = self . make subset ( samples ) samples = self . get samples ( subset ) for s in self . stats calced : for nm in [ n for n in samples if self . srm identifier not in n ] : if self . stats [ nm ] [ s ] . ndim == 2 : # make multi - index reps = np . arange ( self . stats [ nm ] [ s ] . shape [ - 1 ] ) ss = np . array ( [ s ] * reps . size ) nms = np . array ( [ nm ] * reps . size ) # make sub - dataframe stdf = pd . Data Frame ( self . stats [ nm ] [ s ] . T , columns = self . stats [ nm ] [ 'analytes' ] , index = [ ss , nms , reps ] ) stdf . index . set names ( [ 'statistic' , 'sample' , 'rep' ] , inplace = True ) else : stdf = pd . Data Frame ( self . stats [ nm ] [ s ] , index = self . stats [ nm ] [ 'analytes' ] , columns = [ [ s ] , [ nm ] ] ) . T stdf . index . set names ( [ 'statistic' , 'sample' ] , inplace = True ) slst . append ( stdf ) out = pd . concat ( slst ) if ablation time : ats = self . ablation times ( samples = samples , subset = subset ) ats [ 'statistic' ] = 'nanmean' ats . set index ( 'statistic' , append = True , inplace = True ) ats = ats . reorder levels ( [ 'statistic' , 'sample' , 'rep' ] ) out = out . join ( ats ) out . drop ( self . internal standard , 1 , inplace = True ) if save : if filename is None : filename = 'stat export.csv' out . to csv ( self . export dir + '/' + filename ) self . stats df = out return out
def minimal export traces ( self , outdir = None , analytes = None , samples = None , subset = 'All Analyses' ) : if analytes is None : analytes = self . analytes elif isinstance ( analytes , str ) : analytes = [ analytes ] if samples is not None : subset = self . make subset ( samples ) samples = self . get samples ( subset ) focus stage = 'rawdata' # ud = 'counts' if not os . path . isdir ( outdir ) : os . mkdir ( outdir ) for s in samples : d = self . data [ s ] . data [ focus stage ] out = Bunch ( ) for a in analytes : out [ a ] = d [ a ] out = pd . Data Frame ( out , index = self . data [ s ] . Time ) out . index . name = 'Time' d = dateutil . parser . parse ( self . data [ s ] . meta [ 'date' ] ) header = [ '# Minimal Reproduction Dataset Exported from LATOOLS on %s' % ( time . strftime ( '%Y:%m:%d %H:%M:%S' ) ) , "# Analysis described in '../analysis.lalog'" , '# Run latools.reproduce to import analysis.' , '#' , '# Sample: %s' % ( s ) , '# Analysis Time: ' + d . strftime ( '%Y-%m-%d %H:%M:%S' ) ] header = '\n' . join ( header ) + '\n' csv = out . to csv ( ) with open ( '%s/%s.csv' % ( outdir , s ) , 'w' ) as f : f . write ( header ) f . write ( csv ) return
def save log ( self , directory = None , logname = None , header = None ) : if directory is None : directory = self . export dir if not os . path . isdir ( directory ) : directory = os . path . dirname ( directory ) if logname is None : logname = 'analysis.lalog' if header is None : header = self . log header ( ) loc = logging . write logfile ( self . log , header , os . path . join ( directory , logname ) ) return loc
def pca plot ( pca , dt , xlabs = None , mode = 'scatter' , lognorm = True ) : nc = pca . n components f = np . arange ( pca . n features ) cs = list ( itertools . combinations ( range ( nc ) , 2 ) ) ind = ~ np . apply along axis ( any , 1 , np . isnan ( dt ) ) cylim = ( pca . components . min ( ) , pca . components . max ( ) ) yd = cylim [ 1 ] - cylim [ 0 ] # Make figure fig , axs = plt . subplots ( nc , nc , figsize = [ 3 * nc , nc * 3 ] , tight layout = True ) for x , y in zip ( * np . triu indices ( nc ) ) : if x == y : tax = axs [ x , y ] tax . bar ( f , pca . components [ x ] , 0.8 ) tax . set xticks ( [ ] ) tax . axhline ( 0 , zorder = - 1 , c = ( 0 , 0 , 0 , 0.6 ) ) # labels             tax . set ylim ( cylim [ 0 ] - 0.2 * yd , cylim [ 1 ] + 0.2 * yd ) for xi , yi , lab in zip ( f , pca . components [ x ] , xlabs ) : if yi > 0 : yo = yd * 0.03 va = 'bottom' else : yo = yd * - 0.02 va = 'top' tax . text ( xi , yi + yo , lab , ha = 'center' , va = va , rotation = 90 , fontsize = 8 ) else : xv = dt [ ind , x ] yv = dt [ ind , y ] if mode == 'scatter' : axs [ x , y ] . scatter ( xv , yv , alpha = 0.2 ) axs [ y , x ] . scatter ( yv , xv , alpha = 0.2 ) if mode == 'hist2d' : if lognorm : norm = mpl . colors . Log Norm ( ) else : norm = None axs [ x , y ] . hist2d ( xv , yv , 50 , cmap = plt . cm . Blues , norm = norm ) axs [ y , x ] . hist2d ( yv , xv , 50 , cmap = plt . cm . Blues , norm = norm ) if x == 0 : axs [ y , x ] . set ylabel ( 'PC{:.0f}' . format ( y + 1 ) ) if y == nc - 1 : axs [ y , x ] . set xlabel ( 'PC{:.0f}' . format ( x + 1 ) ) return fig , axs , xv , yv
def calc windows ( fn , s , min points ) : max points = np . sum ( ~ np . isnan ( s ) ) n points = max points - min points out = np . full ( ( n points , s . size ) , np . nan ) # skip nans, for speed ind = ~ np . isnan ( s ) s = s [ ind ] for i , w in enumerate ( range ( min points , s . size ) ) : r = rolling window ( s , w , pad = np . nan ) out [ i , ind ] = np . apply along axis ( fn , 1 , r ) return out
def calc window mean std ( s , min points , ind = None ) : max points = np . sum ( ~ np . isnan ( s ) ) n points = max points - min points mean = np . full ( ( n points , s . size ) , np . nan ) std = np . full ( ( n points , s . size ) , np . nan ) # skip nans, for speed if ind is None : ind = ~ np . isnan ( s ) else : ind = ind & ~ np . isnan ( s ) s = s [ ind ] for i , w in enumerate ( range ( min points , s . size ) ) : r = rolling window ( s , w , pad = np . nan ) mean [ i , ind ] = r . sum ( 1 ) / w std [ i , ind ] = ( ( ( r - mean [ i , ind ] [ : , np . newaxis ] ) ** 2 ) . sum ( 1 ) / ( w - 1 ) ) ** 0.5 # mean[i, ind] = np.apply along axis(np.nanmean, 1, r) # std[i, ind] = np.apply along axis(np.nanstd, 1, r) return mean , std
def bayes scale ( s ) : if sum ( ~ np . isnan ( s ) ) > 1 : bm , bv , bs = bayes mvs ( s [ ~ np . isnan ( s ) ] ) return ( s - bm . statistic ) / bs . statistic else : return np . full ( s . shape , np . nan )
def median scaler ( s ) : if sum ( ~ np . isnan ( s ) ) > 2 : ss = s [ ~ np . isnan ( s ) ] median = np . median ( ss ) IQR = np . diff ( np . percentile ( ss , [ 25 , 75 ] ) ) return ( s - median ) / IQR else : return np . full ( s . shape , np . nan )
def clear ( self ) : self . components = { } self . info = { } self . params = { } self . switches = { } self . keys = { } self . index = { } self . sets = { } self . maxset = - 1 self . n = 0 for a in self . analytes : self . switches [ a ] = { } return
def clean ( self ) : for f in sorted ( self . components . keys ( ) ) : unused = not any ( self . switches [ a ] [ f ] for a in self . analytes ) if unused : self . remove ( f )
def get info ( self ) : out = '' for k in sorted ( self . components . keys ( ) ) : out += '{:s}: {:s}' . format ( k , self . info [ k ] ) + '\n' return ( out )
def log ( func ) : @ wraps ( func ) def wrapper ( self , * args , * * kwargs ) : a = func ( self , * args , * * kwargs ) self . log . append ( func . name + ' :: args={} kwargs={}' . format ( args , kwargs ) ) return a return wrapper
def autologin ( function , timeout = TIMEOUT ) : @ wraps ( function ) async def wrapper ( self , * args , * * kwargs ) : """Wrap a function with timeout.""" try : async with async timeout . timeout ( timeout ) : return await function ( self , * args , * * kwargs ) except ( asyncio . Timeout Error , Client Error , Error ) : pass LOGGER . debug ( "autologin" ) try : async with async timeout . timeout ( timeout ) : await self . login ( ) return await function ( self , * args , * * kwargs ) except ( asyncio . Timeout Error , Client Error , Error ) : raise Error ( str ( function ) ) return wrapper
async def get information ( ) : jar = aiohttp . Cookie Jar ( unsafe = True ) websession = aiohttp . Client Session ( cookie jar = jar ) modem = eternalegypt . Modem ( hostname = sys . argv [ 1 ] , websession = websession ) await modem . login ( password = sys . argv [ 2 ] ) result = await modem . information ( ) for sms in result . sms : pprint . pprint ( sms ) await modem . logout ( ) await websession . close ( )
async def send message ( ) : jar = aiohttp . Cookie Jar ( unsafe = True ) websession = aiohttp . Client Session ( cookie jar = jar ) modem = eternalegypt . Modem ( hostname = sys . argv [ 1 ] , websession = websession ) await modem . login ( password = sys . argv [ 2 ] ) await modem . sms ( phone = sys . argv [ 3 ] , message = sys . argv [ 4 ] ) await modem . logout ( ) await websession . close ( )
async def get information ( ) : jar = aiohttp . Cookie Jar ( unsafe = True ) websession = aiohttp . Client Session ( cookie jar = jar ) try : modem = eternalegypt . Modem ( hostname = sys . argv [ 1 ] , websession = websession ) await modem . login ( password = sys . argv [ 2 ] ) result = await modem . information ( ) print ( "upstream: {}" . format ( result . upstream ) ) print ( "serial number: {}" . format ( result . serial number ) ) print ( "wire connected: {}" . format ( result . wire connected ) ) print ( "mobile connected: {}" . format ( result . mobile connected ) ) print ( "connection text: {}" . format ( result . connection text ) ) print ( "connection type: {}" . format ( result . connection type ) ) print ( "current nw service type: {}" . format ( result . current nw service type ) ) print ( "current ps service type: {}" . format ( result . current ps service type ) ) print ( "register network display: {}" . format ( result . register network display ) ) print ( "roaming: {}" . format ( result . roaming ) ) print ( "radio quality: {}" . format ( result . radio quality ) ) print ( "rx level: {}" . format ( result . rx level ) ) print ( "tx level: {}" . format ( result . tx level ) ) print ( "current band: {}" . format ( result . current band ) ) print ( "cell id: {}" . format ( result . cell id ) ) await modem . logout ( ) except eternalegypt . Error : print ( "Could not login" ) await websession . close ( )
async def set failover mode ( mode ) : jar = aiohttp . Cookie Jar ( unsafe = True ) websession = aiohttp . Client Session ( cookie jar = jar ) try : modem = eternalegypt . Modem ( hostname = sys . argv [ 1 ] , websession = websession ) await modem . login ( password = sys . argv [ 2 ] ) await modem . set failover mode ( mode ) await modem . logout ( ) except eternalegypt . Error : print ( "Could not login" ) await websession . close ( )
def nbviewer link ( url ) : if six . PY2 : from urlparse import urlparse as urlsplit else : from urllib . parse import urlsplit info = urlsplit ( url ) domain = info . netloc url type = 'github' if domain == 'github.com' else 'url' return 'https://nbviewer.jupyter.org/%s%s' % ( url type , info . path )
def thumbnail div ( self ) : return self . THUMBNAIL TEMPLATE . format ( snippet = self . get description ( ) [ 1 ] , thumbnail = self . thumb file , ref name = self . reference )
def code div ( self ) : code example = self . code example if code example is None : return None return self . CODE TEMPLATE . format ( snippet = self . get description ( ) [ 1 ] , code = code example , ref name = self . reference )
def code example ( self ) : if self . code example is not None : return self . code example return getattr ( self . nb . metadata , 'code example' , None )
def supplementary files ( self ) : if self . supplementary files is not None : return self . supplementary files return getattr ( self . nb . metadata , 'supplementary files' , None )
def other supplementary files ( self ) : if self . other supplementary files is not None : return self . other supplementary files return getattr ( self . nb . metadata , 'other supplementary files' , None )
def url ( self ) : if self . url is not None : url = self . url else : url = getattr ( self . nb . metadata , 'url' , None ) if url is not None : return nbviewer link ( url )
def get out file ( self , ending = 'rst' ) : return os . path . splitext ( self . outfile ) [ 0 ] + os . path . extsep + ending
def create rst ( self , nb , in dir , odir ) : raw rst , resources = nbconvert . export by name ( 'rst' , nb ) # remove ipython magics rst content = '' i0 = 0 m = None # HACK: we insert the bokeh style sheets here as well, since for some # themes (e.g. the sphinx rtd theme) it is not sufficient to include # the style sheets only via app.add stylesheet bokeh str = '' if 'bokeh' in raw rst and self . insert bokeh : bokeh str += self . BOKEH TEMPLATE . format ( version = self . insert bokeh ) if 'bokeh' in raw rst and self . insert bokeh widgets : bokeh str += self . BOKEH WIDGETS TEMPLATE . format ( version = self . insert bokeh widgets ) for m in code blocks . finditer ( raw rst ) : lines = m . group ( ) . splitlines ( True ) header , content = lines [ 0 ] , '' . join ( lines [ 1 : ] ) no magics = magic patt . sub ( '\g<1>' , content ) # if the code cell only contained magic commands, we skip it if no magics . strip ( ) : rst content += ( raw rst [ i0 : m . start ( ) ] + bokeh str + header + no magics ) bokeh str = '' i0 = m . end ( ) else : rst content += raw rst [ i0 : m . start ( ) ] i0 = m . end ( ) if m is not None : rst content += bokeh str + raw rst [ m . end ( ) : ] else : rst content = raw rst rst content = '..  %s:\n\n' % self . reference + rst content url = self . url if url is not None : rst content += self . CODE DOWNLOAD NBVIEWER . format ( pyfile = os . path . basename ( self . py file ) , nbfile = os . path . basename ( self . outfile ) , url = url ) else : rst content += self . CODE DOWNLOAD . format ( pyfile = os . path . basename ( self . py file ) , nbfile = os . path . basename ( self . outfile ) ) supplementary files = self . supplementary files other supplementary files = self . other supplementary files if supplementary files or other supplementary files : for f in ( supplementary files or [ ] ) + ( other supplementary files or [ ] ) : if not os . path . exists ( os . path . join ( odir , f ) ) : copyfile ( os . path . join ( in dir , f ) , os . path . join ( odir , f ) ) if supplementary files : rst content += self . data download ( supplementary files ) rst file = self . get out file ( ) outputs = sorted ( resources [ 'outputs' ] , key = rst content . find ) base = os . path . join ( 'images' , os . path . splitext ( os . path . basename ( self . infile ) ) [ 0 ] + ' %i.png' ) out map = { os . path . basename ( original ) : base % i for i , original in enumerate ( outputs ) } for original , final in six . iteritems ( out map ) : rst content = rst content . replace ( original , final ) with open ( rst file , 'w' ) as f : f . write ( rst content . rstrip ( ) + '\n' ) pictures = [ ] for original in outputs : fname = os . path . join ( odir , out map [ os . path . basename ( original ) ] ) pictures . append ( fname ) if six . PY3 : f = open ( fname , 'w+b' ) else : f = open ( fname , 'w' ) f . write ( resources [ 'outputs' ] [ original ] ) f . close ( ) self . pictures = pictures
def create py ( self , nb , force = False ) : # Although we would love to simply use ``nbconvert.export python(nb)`` # this causes troubles in other cells processed by the ipython # directive. Instead of getting something like ``Out [5]:``, we get # some weird like '[0;31m Out[[1;31m5[0;31m]: [0m' which look like # color information if we allow the call of nbconvert.export python if list ( map ( int , re . findall ( '\d+' , nbconvert . version ) ) ) >= [ 4 , 2 ] : py file = os . path . basename ( self . py file ) else : py file = self . py file try : level = logger . logger . level except Attribute Error : level = logger . level spr . call ( [ 'jupyter' , 'nbconvert' , '--to=python' , '--output=' + py file , '--log-level=%s' % level , self . outfile ] ) with open ( self . py file ) as f : py content = f . read ( ) # comment out ipython magics py content = re . sub ( '^\s*get ipython\(\).magic.*' , '# \g<0>' , py content , flags = re . MULTILINE ) with open ( self . py file , 'w' ) as f : f . write ( py content )
def data download ( self , files ) : if len ( files ) > 1 : return self . DATA DOWNLOAD % ( ( '\n\n' + ' ' * 8 ) + ( '\n' + ' ' * 8 ) . join ( '* :download:`%s`' % f for f in files ) ) return self . DATA DOWNLOAD % ':download:`%s`' % files [ 0 ]
def create thumb ( self ) : thumbnail figure = self . copy thumbnail figure ( ) if thumbnail figure is not None : if isinstance ( thumbnail figure , six . string types ) : pic = thumbnail figure else : pic = self . pictures [ thumbnail figure ] self . save thumbnail ( pic ) else : for pic in self . pictures [ : : - 1 ] : if pic . endswith ( 'png' ) : self . save thumbnail ( pic ) return
def get description ( self ) : def split header ( s , get header = True ) : s = s . lstrip ( ) . rstrip ( ) parts = s . splitlines ( ) if parts [ 0 ] . startswith ( '#' ) : if get header : header = re . sub ( '#+\s*' , '' , parts . pop ( 0 ) ) if not parts : return header , '' else : header = '' rest = '\n' . join ( parts ) . lstrip ( ) . split ( '\n\n' ) desc = rest [ 0 ] . replace ( '\n' , ' ' ) return header , desc else : if get header : if parts [ 0 ] . startswith ( ( '=' , '-' ) ) : parts = parts [ 1 : ] header = parts . pop ( 0 ) if parts and parts [ 0 ] . startswith ( ( '=' , '-' ) ) : parts . pop ( 0 ) if not parts : return header , '' else : header = '' rest = '\n' . join ( parts ) . lstrip ( ) . split ( '\n\n' ) desc = rest [ 0 ] . replace ( '\n' , ' ' ) return header , desc first cell = self . nb [ 'cells' ] [ 0 ] if not first cell [ 'cell type' ] == 'markdown' : return '' , '' header , desc = split header ( first cell [ 'source' ] ) if not desc and len ( self . nb [ 'cells' ] ) > 1 : second cell = self . nb [ 'cells' ] [ 1 ] if second cell [ 'cell type' ] == 'markdown' : , desc = split header ( second cell [ 'source' ] , False ) return header , desc
def save thumbnail ( self , image path ) : thumb dir = os . path . join ( os . path . dirname ( image path ) , 'thumb' ) create dirs ( thumb dir ) thumb file = os . path . join ( thumb dir , '%s thumb.png' % self . reference ) if os . path . exists ( image path ) : logger . info ( 'Scaling %s to thumbnail %s' , image path , thumb file ) self . scale image ( image path , thumb file , 400 , 280 ) self . thumb file = thumb file
def copy thumbnail figure ( self ) : ret = None if self . thumbnail figure is not None : if not isstring ( self . thumbnail figure ) : ret = self . thumbnail figure else : ret = osp . join ( osp . dirname ( self . outfile ) , osp . basename ( self . thumbnail figure ) ) copyfile ( self . thumbnail figure , ret ) return ret elif hasattr ( self . nb . metadata , 'thumbnail figure' ) : if not isstring ( self . nb . metadata . thumbnail figure ) : ret = self . nb . metadata . thumbnail figure else : ret = osp . join ( osp . dirname ( self . outfile ) , 'images' , osp . basename ( self . nb . metadata . thumbnail figure ) ) copyfile ( osp . join ( osp . dirname ( self . infile ) , self . nb . metadata . thumbnail figure ) , ret ) return ret
def get db change languages ( self , field name , db table fields ) : for lang code , lang name in get languages ( ) : if get real fieldname ( field name , lang code ) not in db table fields : yield lang code for db table field in db table fields : pattern = re . compile ( '^%s (?P<lang>\w{2})$' % field name ) m = pattern . match ( db table field ) if not m : continue lang = m . group ( 'lang' ) yield lang
def get sync sql ( self , field name , db change langs , model , db table fields ) : qn = connection . ops . quote name style = no style ( ) sql output = [ ] db table = model . meta . db table was translatable before = self . was translatable before ( field name , db table fields ) default f = self . get default field ( field name , model ) default f required = default f and self . get field required in db ( db table , default f . name , value not implemented = False ) for lang in db change langs : new field = get real fieldname ( field name , lang ) try : f = model . meta . get field ( new field ) col type = self . get type of db field ( field name , model ) field column = f . column except Field Does Not Exist : # columns in db, removed the settings.LANGUGES field column = new field col type = self . get type of db field ( field name , model ) field sql = [ style . SQL FIELD ( qn ( field column ) ) , style . SQL COLTYPE ( col type ) ] alter colum set = 'ALTER COLUMN %s SET' % qn ( field column ) if default f : alter colum drop = 'ALTER COLUMN %s DROP' % qn ( field column ) not null = style . SQL KEYWORD ( 'NOT NULL' ) if 'mysql' in backend . name : alter colum set = 'MODIFY %s %s' % ( qn ( field column ) , col type ) not null = style . SQL KEYWORD ( 'NULL' ) if default f : alter colum drop = 'MODIFY %s %s' % ( qn ( field column ) , col type ) # column creation if not new field in db table fields : sql output . append ( "ALTER TABLE %s ADD COLUMN %s" % ( qn ( db table ) , ' ' . join ( field sql ) ) ) if lang == self . default lang and not was translatable before : # data copy from old field (only for default language) sql output . append ( "UPDATE %s SET %s = %s" % ( qn ( db table ) , qn ( field column ) , qn ( field name ) ) ) if not f . null : # changing to NOT NULL after having data copied sql output . append ( "ALTER TABLE %s %s %s" % ( qn ( db table ) , alter colum set , style . SQL KEYWORD ( 'NOT NULL' ) ) ) elif default f and not default f . null : if lang == self . default lang : f required = self . get field required in db ( db table , field column , value not implemented = False ) if default f . name == new field and default f required : continue if not f required : # data copy from old field (only for default language) sql output . append ( ( "UPDATE %(db table)s SET %(f colum)s = '%(value default)s' " "WHERE %(f colum)s is %(null)s or %(f colum)s = '' " % { 'db table' : qn ( db table ) , 'f colum' : qn ( field column ) , 'value default' : self . get value default ( ) , 'null' : style . SQL KEYWORD ( 'NULL' ) , } ) ) # changing to NOT NULL after having data copied sql output . append ( "ALTER TABLE %s %s %s" % ( qn ( db table ) , alter colum set , style . SQL KEYWORD ( 'NOT NULL' ) ) ) else : f required = self . get field required in db ( db table , field column , value not implemented = True ) if f required : sql output . append ( ( "ALTER TABLE %s %s %s" % ( qn ( db table ) , alter colum drop , not null ) ) ) if not was translatable before : # we drop field only if field was no translatable before sql output . append ( "ALTER TABLE %s DROP COLUMN %s" % ( qn ( db table ) , qn ( field name ) ) ) return sql output
def pre save ( self , model instance , add ) : file = getattr ( model instance , self . attname ) if file and not file . committed : image file = file if self . resize source to : file . seek ( 0 ) image file = processors . process ( file , self . resize source to ) image file = post processors . process ( image file , self . resize source to ) filename = str ( shortuuid . uuid ( ) ) + os . path . splitext ( file . name ) [ 1 ] file . save ( filename , image file , save = False ) return file
def refresh cache ( self ) : self . thumbnails = { } metadatas = self . metadata backend . get thumbnails ( self . source image . name ) for metadata in metadatas : self . thumbnails [ metadata . size ] = Thumbnail ( metadata = metadata , storage = self . storage )
def all ( self ) : if self . thumbnails is not None : return self . thumbnails self . refresh cache ( ) return self . thumbnails
def create ( self , size ) : thumbnail = images . create ( self . source image . name , size , self . metadata backend , self . storage ) return thumbnail
def delete ( self , size ) : images . delete ( self . source image . name , size , self . metadata backend , self . storage ) del ( self . thumbnails [ size ] )
def get ( source name , size , metadata backend = None , storage backend = None ) : if storage backend is None : storage backend = backends . storage . get backend ( ) if metadata backend is None : metadata backend = backends . metadata . get backend ( ) metadata = metadata backend . get thumbnail ( source name , size ) if metadata is None : return None else : return Thumbnail ( metadata = metadata , storage = storage backend )
def delete ( source name , size , metadata backend = None , storage backend = None ) : if storage backend is None : storage backend = backends . storage . get backend ( ) if metadata backend is None : metadata backend = backends . metadata . get backend ( ) storage backend . delete ( get thumbnail name ( source name , size ) ) metadata backend . delete thumbnail ( source name , size )
def jsonex api ( f ) : @ wraps ( f ) def wrapper ( * args , * * kwargs ) : # Call, catch exceptions try : code , res = 200 , f ( * args , * * kwargs ) except HTTP Exception as e : code , res = e . code , { 'error' : e } except Exception as e : code , res = 500 , { 'error' : e } logger . exception ( 'Method error' ) # Response response = make response ( jsonex dumps ( res ) , code ) response . headers [ 'Content-Type' ] = 'application/json' return response return wrapper
def estimate tx gas with web3 ( self , safe address : str , to : str , value : int , data : bytes ) -> int : return self . ethereum client . estimate gas ( safe address , to , value , data , block identifier = 'pending' )
def has bad headers ( self , default from = None ) : sender = self . sender or default from reply to = self . reply to or '' for val in [ self . subject , sender , reply to ] + self . recipients : for c in '\r\n' : if c in val : return True return False
def from module ( module name ) : d = importlib . import module ( module name ) config = { } for key in dir ( d ) : if key . isupper ( ) : config [ key ] = getattr ( d , key ) return Config ( config )
def register resources ( self , * * resources ) : for key , resource in resources . items ( ) : if key in self . resources : raise Already Exists Exception ( 'A Service for {} is already registered.' . format ( key ) ) self . init resource ( key , resource )
def require ( self , key ) : value = self . get ( key ) if not value : raise Value Error ( '"{}" is empty.' . format ( key ) ) return value
def exit ( self , obj , type , value , traceback ) : if type is None : # No in-context exception occurred try : obj . next ( ) except Stop Iteration : # Resource closed as expected return else : raise Runtime Error ( '{} yielded more than once.' . format ( obj ) ) else : # In-context exception occurred try : obj . throw ( type , value , traceback ) raise Runtime Error ( '{} did not close after throw()' . format ( obj ) ) except Stop Iteration as exc : # Suppress the exception *unless* it's the same exception that # was passed to throw().  This prevents a Stop Iteration # raised inside the "with" statement from being suppressed return exc is not value except : # only re-raise if it's *not* the exception that was # passed to throw(), because  exit () must not raise # an exception unless  exit () itself failed.  But # resource.throw() will raise the exception to signal propagation, # so this fixes the impedance mismatch between the throw() protocol # and the  exit () protocol. # # Middleware or Resources that throw exceptions before yielding # will just rethrow the same exception here which is expected. They # won't have a chance to do anything about the exception though which # seems OK since they never got to the point of being ready anyway. if sys . exc info ( ) [ 1 ] is not value : raise
def get resource ( self , resource name , name ) : # pylint: disable=too-many-locals, too-many-nested-blocks try : logger . info ( "Trying to get %s: '%s'" , resource name , name ) services list = False if resource name == 'host' and '/' in name : splitted name = name . split ( '/' ) services list = True name = splitted name [ 0 ] params = { 'where' : json . dumps ( { 'name' : name } ) } if resource name in [ 'host' , 'service' , 'user' ] : params = { 'where' : json . dumps ( { 'name' : name , ' is template' : self . model } ) } if resource name == 'service' and '/' in name : splitted name = name . split ( '/' ) # new name = splitted name[0] + ' ' + splitted name[1] # name = splitted name[1] # Get host from name response2 = self . backend . get ( 'host' , params = { 'where' : json . dumps ( { 'name' : splitted name [ 0 ] } ) } ) if response2 [ ' items' ] : host = response2 [ ' items' ] [ 0 ] logger . info ( "Got host '%s' for the service '%s'" , splitted name [ 0 ] , splitted name [ 1 ] ) else : logger . warning ( "Not found host '%s'!" , splitted name [ 0 ] ) return False params = { 'where' : json . dumps ( { 'name' : splitted name [ 1 ] , 'host' : host [ ' id' ] , ' is template' : self . model } ) } if self . embedded and resource name in self . embedded resources : params . update ( { 'embedded' : json . dumps ( self . embedded resources [ resource name ] ) } ) response = self . backend . get ( resource name , params = params ) if response [ ' items' ] : response = response [ ' items' ] [ 0 ] logger . info ( "-> found %s '%s': %s" , resource name , name , response [ ' id' ] ) if services list : # Get services for the host params = { 'where' : json . dumps ( { 'host' : response [ ' id' ] } ) } if self . embedded and 'service' in self . embedded resources : params . update ( { 'embedded' : json . dumps ( self . embedded resources [ 'service' ] ) } ) response2 = self . backend . get ( 'service' , params = params ) if response2 [ ' items' ] : response [ ' services' ] = response2 [ ' items' ] logger . info ( "Got %d services for host '%s'" , len ( response2 [ ' items' ] ) , splitted name [ 0 ] ) else : logger . warning ( "Not found host '%s'!" , splitted name [ 0 ] ) return False # Exists in the backend, we got the element if not self . dry run : logger . info ( "-> dumping %s: %s" , resource name , name ) # Filter fields prefixed with an   (internal backend fields) for field in list ( response ) : if field in [ ' created' , ' updated' , ' etag' , ' links' , ' status' ] : response . pop ( field ) continue # Filter fields prefixed with an   in embedded items if self . embedded and resource name in self . embedded resources and field in self . embedded resources [ resource name ] : logger . info ( "-> embedded %s" , field ) # Embedded items may be a list or a simple dictionary, # always make it a list embedded items = response [ field ] if not isinstance ( response [ field ] , list ) : embedded items = [ response [ field ] ] # Filter fields in each embedded item for embedded item in embedded items : if not embedded item : continue for embedded field in list ( embedded item ) : if embedded field . startswith ( ' ' ) : embedded item . pop ( embedded field ) dump = json . dumps ( response , indent = 4 , separators = ( ',' , ': ' ) , sort keys = True ) if not self . quiet : print ( dump ) if resource name == 'service' and '/' in name : name = splitted name [ 0 ] + ' ' + splitted name [ 1 ] filename = self . file dump ( response , 'alignak-object-dump-%s-%s.json' % ( resource name , name ) ) if filename : logger . info ( "-> dumped %s '%s' to %s" , resource name , name , filename ) logger . info ( "-> dumped %s: %s" , resource name , name ) else : if resource name == 'service' and '/' in name : name = splitted name [ 0 ] + ' ' + splitted name [ 1 ] logger . info ( "Dry-run mode: should have dumped an %s '%s'" , resource name , name ) return True else : logger . warning ( "-> %s '%s' not found" , resource name , name ) return False except Backend Exception as exp : # pragma: no cover, should never happen logger . exception ( "Exception: %s" , exp ) logger . error ( "Response: %s" , exp . response ) print ( "Get error for  '%s' : %s" % ( resource name , name ) ) print ( "~~~~~~~~~~~~~~~~~~~~~~~~~~" ) print ( "Exiting with error code: 5" ) return False
def delete resource ( self , resource name , name ) : try : logger . info ( "Trying to get %s: '%s'" , resource name , name ) if name is None : # No name is defined, delete all the resources... if not self . dry run : headers = { 'Content-Type' : 'application/json' } logger . info ( "-> deleting all %s" , resource name ) self . backend . delete ( resource name , headers ) logger . info ( "-> deleted all %s" , resource name ) else : response = { ' id' : ' fake' , ' etag' : ' fake' } logger . info ( "Dry-run mode: should have deleted all %s" , resource name ) else : params = { 'where' : json . dumps ( { 'name' : name } ) } if resource name in [ 'host' , 'service' , 'user' ] : params = { 'where' : json . dumps ( { 'name' : name , ' is template' : self . model } ) } if resource name == 'service' and '/' in name : splitted name = name . split ( '/' ) name = splitted name [ 0 ] + ' ' + splitted name [ 1 ] # Get host from name response2 = self . backend . get ( 'host' , params = { 'where' : json . dumps ( { 'name' : splitted name [ 0 ] } ) } ) if response2 [ ' items' ] : host = response2 [ ' items' ] [ 0 ] logger . info ( "Got host '%s' for the service '%s'" , splitted name [ 0 ] , splitted name [ 1 ] ) else : logger . warning ( "Not found host '%s'!" , splitted name [ 0 ] ) return False if splitted name [ 1 ] == '*' : params = { 'where' : json . dumps ( { 'host' : host [ ' id' ] } ) } else : params = { 'where' : json . dumps ( { 'name' : splitted name [ 1 ] , 'host' : host [ ' id' ] } ) } response = self . backend . get all ( resource name , params = params ) if response [ ' items' ] : logger . info ( "-> found %d matching %s" , len ( response [ ' items' ] ) , resource name ) for item in response [ ' items' ] : logger . info ( "-> found %s '%s': %s" , resource name , name , item [ 'name' ] ) # Exists in the backend, we must delete the element... if not self . dry run : headers = { 'Content-Type' : 'application/json' , 'If-Match' : item [ ' etag' ] } logger . info ( "-> deleting %s: %s" , resource name , item [ 'name' ] ) self . backend . delete ( resource name + '/' + item [ ' id' ] , headers ) logger . info ( "-> deleted %s: %s" , resource name , item [ 'name' ] ) else : response = { ' id' : ' fake' , ' etag' : ' fake' } logger . info ( "Dry-run mode: should have deleted an %s '%s'" , resource name , name ) logger . info ( "-> deleted: '%s': %s" , resource name , item [ ' id' ] ) else : logger . warning ( "-> %s item '%s' not found" , resource name , name ) return False except Backend Exception as exp : # pragma: no cover, should never happen logger . exception ( "Exception: %s" , exp ) logger . error ( "Response: %s" , exp . response ) print ( "Deletion error for  '%s' : %s" % ( resource name , name ) ) print ( "~~~~~~~~~~~~~~~~~~~~~~~~~~" ) print ( "Exiting with error code: 5" ) return False return True
def samefile ( path1 , path2 ) : # Check if both are on the same volume and have the same file ID info1 = fs . getfileinfo ( path1 ) info2 = fs . getfileinfo ( path2 ) return ( info1 . dw Volume Serial Number == info2 . dw Volume Serial Number and info1 . n File Index High == info2 . n File Index High and info1 . n File Index Low == info2 . n File Index Low )
def create ( source , link name ) : success = False if not os . path . isdir ( source ) : raise Exception ( "%s is not a directory" % source ) if os . path . exists ( link name ) : raise Exception ( "%s: junction link name already exists" % link name ) link name = os . path . abspath ( link name ) os . mkdir ( link name ) # Get a handle to the directory hlink = Create File ( link name , fs . GENERIC WRITE , fs . FILE SHARE READ | fs . FILE SHARE WRITE , None , fs . OPEN EXISTING , fs . FILE FLAG OPEN REPARSE POINT | fs . FILE FLAG BACKUP SEMANTICS , None ) try : if hlink == fs . INVALID HANDLE VALUE : raise Win Error ( ) srcvolpath = unparsed convert ( source ) ( junctioninfo , infolen ) = new junction reparse buffer ( srcvolpath ) dummy = DWORD ( 0 ) res = Device Io Control ( hlink , FSCTL SET REPARSE POINT , byref ( junctioninfo ) , infolen , None , 0 , byref ( dummy ) , None ) if res == 0 : raise Win Error ( ) success = True finally : if hlink != fs . INVALID HANDLE VALUE : Close Handle ( hlink ) if not success : os . rmdir ( link name )
def initialize logger ( args ) : global log filename log filename = os . path . join ( os . getcwd ( ) , "jacquard.log" ) if args . log file : validate log file ( args . log file ) log filename = args . log file logging . basic Config ( format = FILE LOG FORMAT , level = "DEBUG" , datefmt = DATE FORMAT , filename = log filename ) global verbose if args . verbose : verbose = args . verbose start time = datetime . now ( ) . strftime ( DATE FORMAT ) global logging dict logging dict = { 'user' : getpass . getuser ( ) , 'host' : socket . gethostname ( ) , 'start time' : start time , 'tool' : args . subparser name }
def error ( self , message ) : message = self . remessage invalid subparser ( message ) raise utils . Usage Error ( message )
def read ( * paths ) : with open ( os . path . join ( * paths ) , 'r' ) as filename : return filename . read ( )
def prefix line terminator ( self , data ) : for t in self . LINE TERMINATORS : if data . startswith ( t ) : return t return None
def suffix line terminator ( self , data ) : for t in self . LINE TERMINATORS : if data . endswith ( t ) : return t return None
def tail ( self , lines = 10 ) : self . file . seek ( 0 , SEEK END ) for i in range ( lines ) : if self . seek previous line ( ) == - 1 : break data = self . file . read ( ) for t in self . LINE TERMINATORS : if data . endswith ( t ) : # Only terminators  between  lines should be preserved. # Otherwise terminator of the last line will be treated as separtaing line and empty line. data = data [ : - len ( t ) ] break if data : return self . splitlines ( data ) else : return [ ]
def head ( self , lines = 10 ) : self . file . seek ( 0 ) for i in range ( lines ) : if self . seek next line ( ) == - 1 : break end pos = self . file . tell ( ) self . file . seek ( 0 ) data = self . file . read ( end pos ) for t in self . LINE TERMINATORS : if data . endswith ( t ) : # Only terminators  between  lines should be preserved. # Otherwise terminator of the last line will be treated as separtaing line and empty line. data = data [ : - len ( t ) ] break if data : return self . splitlines ( data ) else : return [ ]
def format tags ( self ) : tags = Vcf Record . EMPTY SET if self . sample tag values : first sample = list ( self . sample tag values . keys ( ) ) [ 0 ] tags = set ( self . sample tag values [ first sample ] . keys ( ) ) return tags
def join info fields ( self ) : if self . info dict : info fields = [ ] if len ( self . info dict ) > 1 : self . info dict . pop ( "." , None ) for field , value in self . info dict . items ( ) : if field == value : info fields . append ( value ) else : info fields . append ( "=" . join ( [ field , value ] ) ) self . info = ";" . join ( info fields ) else : self . info = "."
def format field ( self ) : format field = "." if self . sample tag values : first sample = list ( self . sample tag values . keys ( ) ) [ 0 ] tag names = self . sample tag values [ first sample ] . keys ( ) if tag names : format field = ":" . join ( tag names ) return format field
def text ( self ) : stringifier = [ self . chrom , self . pos , self . vcf id , self . ref , self . alt , self . qual , self . filter , self . info , self . format field ( ) ] for sample in self . sample tag values : stringifier . append ( self . sample field ( sample ) ) return "\t" . join ( stringifier ) + "\n"
def add or replace filter ( self , new filter ) : if self . filter . lower ( ) in self . FILTERS TO REPLACE : self . filter = new filter elif new filter not in self . filter . split ( ";" ) : self . filter = ";" . join ( [ self . filter , new filter ] )
def add product error ( self , product , error ) : self . add error ( self . field name ( product ) , error )
def model fields form factory ( model ) : fields = model . meta . get fields ( ) choices = [ ] for field in fields : if hasattr ( field , "verbose name" ) : choices . append ( ( field . name , field . verbose name ) ) class Model Fields Form ( forms . Form ) : fields = forms . Multiple Choice Field ( choices = choices , required = False , ) return Model Fields Form
def items pending or purchased ( self ) : status = [ commerce . Cart . STATUS PAID , commerce . Cart . STATUS ACTIVE ] return self . items ( status )
def iter osm notes ( feed limit = 25 , interval = 60 , parse timestamps = True ) : last seen guid = None while True : u = urllib2 . urlopen ( 'https://www.openstreetmap.org/api/0.6/notes/feed?limit=%d' % feed limit ) tree = etree . parse ( u ) new notes = [ ] for note item in tree . xpath ( '/rss/channel/item' ) : title = note item . xpath ( 'title' ) [ 0 ] . text if title . startswith ( 'new note (' ) : action = 'create' elif title . startswith ( 'new comment (' ) : action = 'comment' elif title . startswith ( 'closed note (' ) : action = 'close' # Note that (at least for now) the link and guid are the same in the feed. guid = note item . xpath ( 'link' ) [ 0 ] . text if last seen guid == guid : break elif last seen guid == None : # The first time through we want the first item to be the "last seen" # because the RSS feed is newest-to-oldest last seen guid = guid else : note id = int ( guid . split ( '/' ) [ - 1 ] . split ( '#c' ) [ 0 ] ) new notes . append ( ( action , get note ( note id , parse timestamps ) ) ) # We yield the reversed list because we want to yield in change order # (i.e. "oldest to most current") for note in reversed ( new notes ) : yield note yield model . Finished ( None , None ) time . sleep ( interval )
def passes filter ( self , user ) : cls = type ( self . condition ) qs = cls . objects . filter ( pk = self . condition . id ) return self . condition in self . pre filter ( qs , user )
def apply voucher ( self , voucher code ) : # Try and find the voucher voucher = inventory . Voucher . objects . get ( code = voucher code . upper ( ) ) # Re-applying vouchers should be idempotent if voucher in self . cart . vouchers . all ( ) : return self . test voucher ( voucher ) # If successful... self . cart . vouchers . add ( voucher )
def recalculate discounts ( self ) : # Delete the existing entries. commerce . Discount Item . objects . filter ( cart = self . cart ) . delete ( ) # Order the products such that the most expensive ones are # processed first. product items = self . cart . productitem set . all ( ) . select related ( "product" , "product category" ) . order by ( "-product price" ) products = [ i . product for i in product items ] discounts = Discount Controller . available discounts ( self . cart . user , [ ] , products , ) # The highest-value discounts will apply to the highest-value # products first, because of the order by clause for item in product items : self . add discount ( item . product , item . quantity , discounts )
def rows ( self , content type ) : for row in self . data : yield [ self . cell text ( content type , i , cell ) for i , cell in enumerate ( row ) ]
def get form ( self , request ) : # Create a form instance if self . form type is not None : form = self . form type ( request . GET ) # Pre-validate it form . is valid ( ) else : form = None return form
def reports list ( request ) : reports = [ ] for report in get all reports ( ) : reports . append ( { "name" : report . name , "url" : reverse ( report ) , "description" : report . doc , } ) reports . sort ( key = lambda report : report [ "name" ] ) ctx = { "reports" : reports , } return render ( request , "registrasion/reports list.html" , ctx )
def sales payment summary ( ) : def value or zero ( aggregate , key ) : return aggregate [ key ] or 0 def sum amount ( payment set ) : a = payment set . values ( "amount" ) . aggregate ( total = Sum ( "amount" ) ) return value or zero ( a , "total" ) headings = [ "Category" , "Total" ] data = [ ] # Summarise all sales made (= income.) sales = commerce . Line Item . objects . filter ( invoice status = commerce . Invoice . STATUS PAID , ) . values ( "price" , "quantity" ) . aggregate ( total = Sum ( F ( "price" ) * F ( "quantity" ) , output field = CURRENCY ( ) ) , ) sales = value or zero ( sales , "total" ) all payments = sum amount ( commerce . Payment Base . objects . all ( ) ) # Manual payments # Credit notes generated (total) # Payments made by credit note # Claimed credit notes all credit notes = 0 - sum amount ( commerce . Credit Note . objects . all ( ) ) unclaimed credit notes = 0 - sum amount ( commerce . Credit Note . unclaimed ( ) ) claimed credit notes = sum amount ( commerce . Credit Note Application . objects . all ( ) ) refunded credit notes = 0 - sum amount ( commerce . Credit Note . refunded ( ) ) data . append ( [ "Items on paid invoices" , sales ] ) data . append ( [ "All payments" , all payments ] ) data . append ( [ "Sales - Payments " , sales - all payments ] ) data . append ( [ "All credit notes" , all credit notes ] ) data . append ( [ "Credit notes paid on invoices" , claimed credit notes ] ) data . append ( [ "Credit notes refunded" , refunded credit notes ] ) data . append ( [ "Unclaimed credit notes" , unclaimed credit notes ] ) data . append ( [ "Credit notes - (claimed credit notes + unclaimed credit notes)" , all credit notes - claimed credit notes - refunded credit notes - unclaimed credit notes ] ) return List Report ( "Sales and Payments Summary" , headings , data )
def payments ( ) : payments = commerce . Payment Base . objects . all ( ) return Queryset Report ( "Payments" , [ "invoice id" , "id" , "reference" , "amount" ] , payments , link view = views . invoice , )
def credit note refunds ( ) : notes refunded = commerce . Credit Note . refunded ( ) return Queryset Report ( "Credit note refunds" , [ "id" , "creditnoterefund reference" , "amount" ] , notes refunded , link view = views . credit note , )
def discount status ( request , form ) : discounts = form . cleaned data [ "discount" ] items = commerce . Discount Item . objects . filter ( Q ( discount in = discounts ) , ) . select related ( "cart" , "product" , "product category" ) items = group by cart status ( items , [ "discount" ] , [ "discount" , "discount description" ] , ) headings = [ "Discount" , "Paid" , "Reserved" , "Unreserved" , "Refunded" , ] data = [ ] for item in items : data . append ( [ item [ "discount description" ] , item [ "total paid" ] , item [ "total reserved" ] , item [ "total unreserved" ] , item [ "total refunded" ] , ] ) return List Report ( "Usage by item" , headings , data )
def credit notes ( request , form ) : notes = commerce . Credit Note . objects . all ( ) . select related ( "creditnoterefund" , "creditnoteapplication" , "invoice" , "invoice user attendee attendeeprofilebase" , ) return Queryset Report ( "Credit Notes" , [ "id" , "invoice user attendee attendeeprofilebase invoice recipient" , "status" , "value" ] , notes , headings = [ "id" , "Owner" , "Status" , "Value" ] , link view = views . credit note , )
def invoices ( request , form ) : invoices = commerce . Invoice . objects . all ( ) . order by ( "status" , "id" ) return Queryset Report ( "Invoices" , [ "id" , "recipient" , "value" , "get status display" ] , invoices , headings = [ "id" , "Recipient" , "Value" , "Status" ] , link view = views . invoice , )
def attendee list ( request ) : attendees = people . Attendee . objects . select related ( "attendeeprofilebase" , "user" , ) profiles = Attendee Profile . objects . filter ( attendee in = attendees ) . select related ( "attendee" , "attendee user" , ) profiles by attendee = dict ( ( i . attendee , i ) for i in profiles ) attendees = attendees . annotate ( has registered = Count ( Q ( user invoice status = commerce . Invoice . STATUS PAID ) ) , ) headings = [ "User ID" , "Name" , "Email" , "Has registered" , ] data = [ ] for a in attendees : data . append ( [ a . user . id , ( profiles by attendee [ a ] . attendee name ( ) if a in profiles by attendee else "" ) , a . user . email , a . has registered > 0 , ] ) # Sort by whether they've registered, then ID. data . sort ( key = lambda a : ( - a [ 3 ] , a [ 0 ] ) ) return Attendee List Report ( "Attendees" , headings , data , link view = attendee )
def speaker registrations ( request , form ) : kinds = form . cleaned data [ "kind" ] presentations = schedule models . Presentation . objects . filter ( proposal base kind in = kinds , ) . exclude ( cancelled = True , ) users = User . objects . filter ( Q ( speaker profile presentations in = presentations ) | Q ( speaker profile copresentations in = presentations ) ) paid carts = commerce . Cart . objects . filter ( status = commerce . Cart . STATUS PAID ) paid carts = Case ( When ( cart in = paid carts , then = Value ( 1 ) ) , default = Value ( 0 ) , output field = models . Integer Field ( ) , ) users = users . annotate ( paid carts = Sum ( paid carts ) ) users = users . order by ( "paid carts" ) return Queryset Report ( "Speaker Registration Status" , [ "id" , "speaker profile name" , "email" , "paid carts" ] , users , link view = attendee , ) return [ ]
def missing categories ( context ) : user = user for context ( context ) categories available = set ( Category Controller . available categories ( user ) ) items = Item Controller ( user ) . items pending or purchased ( ) categories held = set ( ) for product , quantity in items : categories held . add ( product . category ) return categories available - categories held
def voucher code ( request ) : VOUCHERS FORM PREFIX = "vouchers" # Handle the voucher form *before* listing products. # Products can change as vouchers are entered. v = handle voucher ( request , VOUCHERS FORM PREFIX ) voucher form , voucher handled = v if voucher handled : messages . success ( request , "Your voucher code was accepted." ) return redirect ( "dashboard" ) data = { "voucher form" : voucher form , } return render ( request , "registrasion/voucher code.html" , data )
def amend registration ( request , user id ) : user = User . objects . get ( id = int ( user id ) ) current cart = Cart Controller . for user ( user ) items = commerce . Product Item . objects . filter ( cart = current cart . cart , ) . select related ( "product" ) initial = [ { "product" : i . product , "quantity" : i . quantity } for i in items ] Staff Products Form Set = forms . staff products formset factory ( user ) formset = Staff Products Form Set ( request . POST or None , initial = initial , prefix = "products" , ) for item , form in zip ( items , formset ) : queryset = inventory . Product . objects . filter ( id = item . product . id ) form . fields [ "product" ] . queryset = queryset voucher form = forms . Voucher Form ( request . POST or None , prefix = "voucher" , ) if request . POST and formset . is valid ( ) : pq = [ ( f . cleaned data [ "product" ] , f . cleaned data [ "quantity" ] ) for f in formset if "product" in f . cleaned data and f . cleaned data [ "product" ] is not None ] try : current cart . set quantities ( pq ) return redirect ( amend registration , user id ) except Validation Error as ve : for ve field in ve . error list : product , message = ve field . message for form in formset : if "product" not in form . cleaned data : # This is the empty form. continue if form . cleaned data [ "product" ] == product : form . add error ( "quantity" , message ) if request . POST and voucher form . has changed ( ) and voucher form . is valid ( ) : try : current cart . apply voucher ( voucher form . cleaned data [ "voucher" ] ) return redirect ( amend registration , user id ) except Validation Error as ve : voucher form . add error ( None , ve ) ic = Item Controller ( user ) data = { "user" : user , "paid" : ic . items purchased ( ) , "cancelled" : ic . items released ( ) , "form" : formset , "voucher form" : voucher form , } return render ( request , "registrasion/amend registration.html" , data )
def extend reservation ( request , user id , days = 7 ) : user = User . objects . get ( id = int ( user id ) ) cart = Cart Controller . for user ( user ) cart . extend reservation ( datetime . timedelta ( days = days ) ) return redirect ( request . META [ "HTTP REFERER" ] )
def invoice mailout ( request ) : category = request . GET . getlist ( "category" , [ ] ) product = request . GET . getlist ( "product" , [ ] ) status = request . GET . get ( "status" ) form = forms . Invoice Email Form ( request . POST or None , category = category , product = product , status = status , ) emails = [ ] if form . is valid ( ) : emails = [ ] for invoice in form . cleaned data [ "invoice" ] : # datatuple = (subject, message, from email, recipient list) from email = form . cleaned data [ "from email" ] subject = form . cleaned data [ "subject" ] body = Template ( form . cleaned data [ "body" ] ) . render ( Context ( { "invoice" : invoice , "user" : invoice . user , } ) ) recipient list = [ invoice . user . email ] emails . append ( Email ( subject , body , from email , recipient list ) ) if form . cleaned data [ "action" ] == forms . Invoice Email Form . ACTION SEND : # Send e-mails *ONLY* if we're sending. send mass mail ( emails ) messages . info ( request , "The e-mails have been sent." ) data = { "form" : form , "emails" : emails , } return render ( request , "registrasion/invoice mailout.html" , data )
def render badge ( user ) : data = { "user" : user , } t = loader . get template ( 'registrasion/badge.svg' ) return t . render ( data )
def generate from cart ( cls , cart ) : cart . refresh from db ( ) # Generate the line items from the cart. product items = commerce . Product Item . objects . filter ( cart = cart ) product items = product items . select related ( "product" , "product category" , ) product items = product items . order by ( "product category order" , "product order" ) if len ( product items ) == 0 : raise Validation Error ( "Your cart is empty." ) discount items = commerce . Discount Item . objects . filter ( cart = cart ) discount items = discount items . select related ( "discount" , "product" , "product category" , ) def format product ( product ) : return "%s - %s" % ( product . category . name , product . name ) def format discount ( discount , product ) : description = discount . description return "%s (%s)" % ( description , format product ( product ) ) line items = [ ] for item in product items : product = item . product line item = commerce . Line Item ( description = format product ( product ) , quantity = item . quantity , price = product . price , product = product , ) line items . append ( line item ) for item in discount items : line item = commerce . Line Item ( description = format discount ( item . discount , item . product ) , quantity = item . quantity , price = cls . resolve discount value ( item ) * - 1 , product = item . product , ) line items . append ( line item ) # Generate the invoice min due time = cart . reservation duration + cart . time last updated return cls . generate ( cart . user , cart , min due time , line items )
def apply credit notes ( cls , invoice ) : # We only automatically apply credit notes if this is the *only* # unpaid invoice for this user. invoices = commerce . Invoice . objects . filter ( user = invoice . user , status = commerce . Invoice . STATUS UNPAID , ) if invoices . count ( ) > 1 : return notes = commerce . Credit Note . unclaimed ( ) . filter ( invoice user = invoice . user ) for note in notes : try : Credit Note Controller ( note ) . apply to invoice ( invoice ) except Validation Error : # Validation Error will get raised once we're overpaying. break invoice . refresh from db ( )
def refresh ( self ) : self . invoice . refresh from db ( ) if self . invoice . cart : self . invoice . cart . refresh from db ( )
def void ( self ) : if self . invoice . total payments ( ) > 0 : raise Validation Error ( "Invoices with payments must be refunded." ) elif self . invoice . is refunded : raise Validation Error ( "Refunded invoices may not be voided." ) if self . invoice . is paid : self . release cart ( ) self . mark void ( )
def update ( self , data ) : fields = [ 'id' , 'status' , 'type' , 'persistence' , 'date start' , 'date finish' , 'date created' , 'date modified' , 'checksum' , 'processor name' , 'input' , 'input schema' , 'output' , 'output schema' , 'static' , 'static schema' , 'var' , 'var template' , ] self . annotation = { } for f in fields : setattr ( self , f , data [ f ] ) self . name = data [ 'static' ] [ 'name' ] if 'name' in data [ 'static' ] else '' self . annotation . update ( self . flatten field ( data [ 'input' ] , data [ 'input schema' ] , 'input' ) ) self . annotation . update ( self . flatten field ( data [ 'output' ] , data [ 'output schema' ] , 'output' ) ) self . annotation . update ( self . flatten field ( data [ 'static' ] , data [ 'static schema' ] , 'static' ) ) self . annotation . update ( self . flatten field ( data [ 'var' ] , data [ 'var template' ] , 'var' ) )
def flatten field ( self , field , schema , path ) : flat = { } for field schema , fields , path in iterate schema ( field , schema , path ) : name = field schema [ 'name' ] typ = field schema [ 'type' ] label = field schema [ 'label' ] value = fields [ name ] if name in fields else None flat [ path ] = { 'name' : name , 'value' : value , 'type' : typ , 'label' : label } return flat
def print downloads ( self ) : for path , ann in self . annotation . items ( ) : if path . startswith ( 'output' ) and ann [ 'type' ] == 'basic:file:' : print ( "{}: {}" . format ( path , ann [ 'value' ] [ 'file' ] ) )
def data ( self , * * query ) : objects = self . cache [ 'objects' ] data = self . api . data . get ( * * query ) [ 'objects' ] data objects = [ ] for d in data : id = d [ 'id' ] if id in objects : # Update existing object objects [ id ] . update ( d ) else : # Insert new object objects [ id ] = Gen Data ( d , self ) data objects . append ( objects [ id ] ) # Hydrate reference fields for d in data objects : count += 1 while True : ref annotation = { } remove annotation = [ ] for path , ann in d . annotation . items ( ) : if ann [ 'type' ] . startswith ( 'data:' ) : # Referenced data object found # Copy annotation id = ann [ 'value' ] if id not in objects : try : d tmp = self . api . data ( id ) . get ( ) except slumber . exceptions . Http Client Error as ex : if ex . response . status code == 404 : continue else : raise ex objects [ id ] = Gen Data ( d tmp , self ) annotation = objects [ id ] . annotation ref annotation . update ( { path + '.' + k : v for k , v in annotation . items ( ) } ) remove annotation . append ( path ) if ref annotation : d . annotation . update ( ref annotation ) for path in remove annotation : del d . annotation [ path ] else : break return data objects
def rundata ( self , strjson ) : d = json . loads ( strjson ) return self . api . data . post ( d )
def get subclasses ( c ) : subclasses = c . subclasses ( ) for d in list ( subclasses ) : subclasses . extend ( get subclasses ( d ) ) return subclasses
def get repo and project ( self ) : app = self . app # Get repo repo = app . data . apply ( 'github-repo' , app . args . github repo , app . prompt repo , on load = app . github . get repo , on save = lambda r : r . id ) assert repo , "repository not found." # Get project project = app . data . apply ( 'asana-project' , app . args . asana project , app . prompt project , on load = app . asana . projects . find by id , on save = lambda p : p [ 'id' ] ) assert project , "project not found." # Set first issue first issue = app . data . apply ( 'first-issue' , app . args . first issue , "set the first issue to sync with [1 for new repos]" , on save = int ) assert first issue assert first issue >= 0 , "issue must be positive" app . sync data ( ) return repo , project
def get variant phenotypes with suggested changes ( variant id list ) : variants = civic . get variants by ids ( variant id list ) evidence = list ( ) for variant in variants : evidence . extend ( variant . evidence ) for e in evidence : suggested changes url = f'https://civicdb.org/api/evidence items/{e.id}/suggested changes' resp = requests . get ( suggested changes url ) resp . raise for status ( ) suggested changes = dict ( ) for suggested change in resp . json ( ) : pheno changes = suggested change [ 'suggested changes' ] . get ( 'phenotype ids' , None ) if pheno changes is None : continue a , b = pheno changes added = set ( b ) - set ( a ) deleted = set ( a ) - set ( b ) rid = suggested change [ 'id' ] suggested changes [ rid ] = { 'added' : added , 'deleted' : deleted } yield e , { 'suggested changes' : suggested changes , 'current' : set ( [ x . id for x in e . phenotypes ] ) }
def get variant phenotypes with suggested changes merged ( variant id list ) : for evidence , phenotype status in get variant phenotypes with suggested changes ( variant id list ) : final = phenotype status [ 'current' ] for rid in sorted ( phenotype status [ 'suggested changes' ] ) : changes = phenotype status [ 'suggested changes' ] [ rid ] final = final - changes [ 'deleted' ] final = final | changes [ 'added' ] if final : yield evidence , final
def update ( self , allow partial = True , force = False , * * kwargs ) : if kwargs : self . init ( partial = allow partial , force = force , * * kwargs ) return not self . partial if not force and CACHE . get ( hash ( self ) ) : cached = CACHE [ hash ( self ) ] for field in self . SIMPLE FIELDS | self . COMPLEX FIELDS : v = getattr ( cached , field ) setattr ( self , field , v ) self . partial = False logging . info ( f'Loading {str(self)} from cache' ) return True resp dict = element lookup by id ( self . type , self . id ) self . init ( partial = False , * * resp dict ) return True
def uniqify ( cls , seq ) : seen = set ( ) seen add = seen . add return [ x for x in seq if x not in seen and not seen add ( x ) ]
def authenticate ( self ) : if self . oauth : return False # Save asana. self . settings . apply ( 'api-asana' , self . args . asana api , "enter asana api key" ) # Save github.com self . settings . apply ( 'api-github' , self . args . github api , "enter github.com token" ) logging . debug ( "authenticating asana api." ) self . asana = Client . basic auth ( self . settings [ 'api-asana' ] ) self . asana errors = asana errors self . asana me = self . asana . users . me ( ) logging . debug ( "authenticating github api" ) self . github = Github ( self . settings [ 'api-github' ] ) self . github user = self . github . get user ( ) self . oauth = True
def list select ( cls , lst , prompt , offset = 0 ) : inp = raw input ( "select %s: " % prompt ) assert inp , "value required." try : return lst [ int ( inp ) + offset ] except Value Error : return inp except Index Error : assert False , "bad value."
def move saved issue data ( self , issue , ns , other ns ) : if isinstance ( issue , int ) : issue number = str ( issue ) elif isinstance ( issue , basestring ) : issue number = issue else : issue number = issue . number issue data key = self . issue data key ( ns ) other issue data key = self . issue data key ( other ns ) issue data = self . data . get ( issue data key , { } ) other issue data = self . data . get ( other issue data key , { } ) id = issue data . pop ( issue number , None ) if id : other issue data [ issue number ] = id self . data [ other issue data key ] = other issue data self . data [ issue data key ] = issue data
def get asana task ( self , asana task id ) : try : return self . asana . tasks . find by id ( asana task id ) except asana errors . Not Found Error : return None except asana errors . Forbidden Error : return None
def transport task ( func ) : def wrapped func ( * args , * * kwargs ) : tries = 0 while True : try : try : return func ( * args , * * kwargs ) except ( asana errors . Invalid Request Error , asana errors . Not Found Error ) , exc : logging . warn ( "warning: invalid request: %r" , exc ) except asana errors . Forbidden Error , exc : logging . warn ( "forbidden error: %r" , exc ) except asana errors . Not Found Error , exc : logging . warn ( "not found error: %r" , exc ) return None except asana errors . Retryable Asana Error , retry exc : tries += 1 logging . warn ( "retry exception %r on try %d" , retry exc , tries ) if tries >= 3 : raise except Exception , exc : logging . exception ( "Exception in transport." ) return return wrapped func
def flush ( callback = None ) : while True : if shutdown event . is set ( ) : return if callable ( callback ) : callback ( ) try : item = queue . get ( timeout = 1 ) queue . put ( item ) # put it back, we're just peeking. except Queue . Empty : return
def format task numbers with links ( tasks ) : project id = data . get ( 'asana-project' , None ) def task format ( task id ) : if project id : asana url = tool . Tool App . make asana url ( project id , task id ) return "[#%d](%s)" % ( task id , asana url ) else : return "#%d" % task id return "\n" . join ( [ task format ( tid ) for tid in tasks ] )
def create missing task ( self , asana workspace id , name , assignee , projects , completed , issue number , issue html url , issue state , issue body , tasks , labels , label tag map ) : task = self . asana . tasks . create in workspace ( asana workspace id , { 'name' : name , 'notes' : issue body , 'assignee' : assignee , 'projects' : projects , 'completed' : completed , } ) # Announce task git issue task id = task [ 'id' ] put ( "create story" , task id = task id , text = "Git Issue #%d: \n" "%s" % ( issue number , issue html url , ) ) put ( "apply tasks to issue" , tasks = [ task id ] , issue number = issue number , issue body = issue body , ) # Save task to drive put setting ( "save issue data task" , issue = issue number , task id = task id , namespace = issue state ) tasks . append ( task id ) # Sync tags/labels put ( "sync tags" , tasks = tasks , labels = labels , label tag map = label tag map )
def apply tasks to issue ( self , tasks , issue number , issue body ) : issue body = issue body task numbers = format task numbers with links ( tasks ) if task numbers : new body = ASANA SECTION RE . sub ( '' , issue body ) new body = new body + "\n## Asana Tasks:\n\n%s" % task numbers put ( "issue edit" , issue number = issue number , body = new body ) return new body return issue body
def data types ( self ) : data = self . gencloud . project data ( self . id ) return sorted ( set ( d . type for d in data ) )
def data ( self , * * query ) : data = self . gencloud . project data ( self . id ) query [ 'case ids contains' ] = self . id ids = set ( d [ 'id' ] for d in self . gencloud . api . dataid . get ( * * query ) [ 'objects' ] ) return [ d for d in data if d . id in ids ]
def init Port ( self ) : try : self . m ser = serial . Serial ( port = self . m ttyport , baudrate = self . m baudrate , timeout = 0 , parity = serial . PARITY EVEN , stopbits = serial . STOPBITS ONE , bytesize = serial . SEVENBITS , rtscts = False ) ekm log ( "Pyserial version = " + serial . VERSION ) ekm log ( "Port = " + self . m ttyport ) ekm log ( "Rate = " + str ( self . m baudrate ) ) time . sleep ( self . m init wait ) return True except : ekm log ( traceback . format exc ( sys . exc info ( ) ) ) return False
def combine AB ( self ) : v4definition meter = V4Meter ( ) v4definition meter . make AB ( ) defv4 = v4definition meter . get Read Buffer ( ) v3definition meter = V3Meter ( ) v3definition meter . make Return Format ( ) defv3 = v3definition meter . get Read Buffer ( ) for fld in defv3 : if fld not in self . m all fields : compare fld = fld . upper ( ) if not "RESERVED" in compare fld and not "CRC" in compare fld : self . m all fields [ fld ] = defv3 [ fld ] for fld in defv4 : if fld not in self . m all fields : compare fld = fld . upper ( ) if not "RESERVED" in compare fld and not "CRC" in compare fld : self . m all fields [ fld ] = defv4 [ fld ] pass
def update Observers ( self ) : for observer in self . m observers : try : observer . update ( self . m req ) except : ekm log ( traceback . format exc ( sys . exc info ( ) ) )
def init Lcd Lookup ( self ) : self . m lcd lookup [ "k Wh Tot" ] = LCD Items . k Wh Tot self . m lcd lookup [ "Rev k Wh Tot" ] = LCD Items . Rev k Wh Tot self . m lcd lookup [ "RMS Volts Ln 1" ] = LCD Items . RMS Volts Ln 1 self . m lcd lookup [ "RMS Volts Ln 2" ] = LCD Items . RMS Volts Ln 2 self . m lcd lookup [ "RMS Volts Ln 3" ] = LCD Items . RMS Volts Ln 3 self . m lcd lookup [ "Amps Ln 1" ] = LCD Items . Amps Ln 1 self . m lcd lookup [ "Amps Ln 2" ] = LCD Items . Amps Ln 2 self . m lcd lookup [ "Amps Ln 3" ] = LCD Items . Amps Ln 3 self . m lcd lookup [ "RMS Watts Ln 1" ] = LCD Items . RMS Watts Ln 1 self . m lcd lookup [ "RMS Watts Ln 2" ] = LCD Items . RMS Watts Ln 2 self . m lcd lookup [ "RMS Watts Ln 3" ] = LCD Items . RMS Watts Ln 3 self . m lcd lookup [ "RMS Watts Tot" ] = LCD Items . RMS Watts Tot self . m lcd lookup [ "Power Factor Ln 1" ] = LCD Items . Power Factor Ln 1 self . m lcd lookup [ "Power Factor Ln 2" ] = LCD Items . Power Factor Ln 2 self . m lcd lookup [ "Power Factor Ln 3" ] = LCD Items . Power Factor Ln 3 self . m lcd lookup [ "k Wh Tariff 1" ] = LCD Items . k Wh Tariff 1 self . m lcd lookup [ "k Wh Tariff 2" ] = LCD Items . k Wh Tariff 2 self . m lcd lookup [ "k Wh Tariff 3" ] = LCD Items . k Wh Tariff 3 self . m lcd lookup [ "k Wh Tariff 4" ] = LCD Items . k Wh Tariff 4 self . m lcd lookup [ "Rev k Wh Tariff 1" ] = LCD Items . Rev k Wh Tariff 1 self . m lcd lookup [ "Rev k Wh Tariff 2" ] = LCD Items . Rev k Wh Tariff 2 self . m lcd lookup [ "Rev k Wh Tariff 3" ] = LCD Items . Rev k Wh Tariff 3 self . m lcd lookup [ "Rev k Wh Tariff 4" ] = LCD Items . Rev k Wh Tariff 4 self . m lcd lookup [ "Reactive Pwr Ln 1" ] = LCD Items . Reactive Pwr Ln 1 self . m lcd lookup [ "Reactive Pwr Ln 2" ] = LCD Items . Reactive Pwr Ln 2 self . m lcd lookup [ "Reactive Pwr Ln 3" ] = LCD Items . Reactive Pwr Ln 3 self . m lcd lookup [ "Reactive Pwr Tot" ] = LCD Items . Reactive Pwr Tot self . m lcd lookup [ "Line Freq" ] = LCD Items . Line Freq self . m lcd lookup [ "Pulse Cnt 1" ] = LCD Items . Pulse Cnt 1 self . m lcd lookup [ "Pulse Cnt 2" ] = LCD Items . Pulse Cnt 2 self . m lcd lookup [ "Pulse Cnt 3" ] = LCD Items . Pulse Cnt 3 self . m lcd lookup [ "k Wh Ln 1" ] = LCD Items . k Wh Ln 1 self . m lcd lookup [ "Rev k Wh Ln 1" ] = LCD Items . Rev k Wh Ln 1 self . m lcd lookup [ "k Wh Ln 2" ] = LCD Items . k Wh Ln 2 self . m lcd lookup [ "Rev k Wh Ln 2" ] = LCD Items . Rev k Wh Ln 2 self . m lcd lookup [ "k Wh Ln 3" ] = LCD Items . k Wh Ln 3 self . m lcd lookup [ "Rev k Wh Ln 3" ] = LCD Items . Rev k Wh Ln 3 self . m lcd lookup [ "Reactive Energy Tot" ] = LCD Items . Reactive Energy Tot self . m lcd lookup [ "Max Demand Rst" ] = LCD Items . Max Demand Rst self . m lcd lookup [ "Rev k Wh Rst" ] = LCD Items . Rev k Wh Rst self . m lcd lookup [ "State Inputs" ] = LCD Items . State Inputs self . m lcd lookup [ "Max Demand" ] = LCD Items . Max Demand
def make AB ( self ) : for fld in self . m blk a : compare fld = fld . upper ( ) if not "RESERVED" in compare fld and not "CRC" in compare fld : self . m req [ fld ] = self . m blk a [ fld ] for fld in self . m blk b : compare fld = fld . upper ( ) if not "RESERVED" in compare fld and not "CRC" in compare fld : self . m req [ fld ] = self . m blk b [ fld ] pass
def calculate Fields ( self ) : pf1 = self . m blk b [ Field . Cos Theta Ln 1 ] [ Meter Data . String Value ] pf2 = self . m blk b [ Field . Cos Theta Ln 2 ] [ Meter Data . String Value ] pf3 = self . m blk b [ Field . Cos Theta Ln 3 ] [ Meter Data . String Value ] pf1 int = self . calc PF ( pf1 ) pf2 int = self . calc PF ( pf2 ) pf3 int = self . calc PF ( pf3 ) self . m blk b [ Field . Power Factor Ln 1 ] [ Meter Data . String Value ] = str ( pf1 int ) self . m blk b [ Field . Power Factor Ln 2 ] [ Meter Data . String Value ] = str ( pf2 int ) self . m blk b [ Field . Power Factor Ln 3 ] [ Meter Data . String Value ] = str ( pf3 int ) self . m blk b [ Field . Power Factor Ln 1 ] [ Meter Data . Native Value ] = pf1 int self . m blk b [ Field . Power Factor Ln 2 ] [ Meter Data . Native Value ] = pf2 int self . m blk b [ Field . Power Factor Ln 3 ] [ Meter Data . Native Value ] = pf2 int rms watts 1 = self . m blk b [ Field . RMS Watts Ln 1 ] [ Meter Data . Native Value ] rms watts 2 = self . m blk b [ Field . RMS Watts Ln 2 ] [ Meter Data . Native Value ] rms watts 3 = self . m blk b [ Field . RMS Watts Ln 3 ] [ Meter Data . Native Value ] sign rms watts 1 = 1 sign rms watts 2 = 1 sign rms watts 3 = 1 direction byte = self . m blk a [ Field . State Watts Dir ] [ Meter Data . Native Value ] if direction byte == Direction Flag . Forward Forward Forward : # all good pass if direction byte == Direction Flag . Forward Forward Reverse : sign rms watts 3 = - 1 pass if direction byte == Direction Flag . Forward Reverse Forward : sign rms watts 2 = - 1 pass if direction byte == Direction Flag . Reverse Forward Forward : sign rms watts 1 = - 1 pass if direction byte == Direction Flag . Forward Reverse Reverse : sign rms watts 2 = - 1 sign rms watts 3 = - 1 pass if direction byte == Direction Flag . Reverse Forward Reverse : sign rms watts 1 = - 1 sign rms watts 3 = - 1 pass if direction byte == Direction Flag . Reverse Reverse Forward : sign rms watts 1 = - 1 sign rms watts 2 = - 1 pass if direction byte == Direction Flag . Reverse Reverse Reverse : sign rms watts 1 = - 1 sign rms watts 2 = - 1 sign rms watts 3 = - 1 pass net watts 1 = rms watts 1 * sign rms watts 1 net watts 2 = rms watts 2 * sign rms watts 2 net watts 3 = rms watts 3 * sign rms watts 3 net watts tot = net watts 1 + net watts 2 + net watts 3 self . m blk b [ Field . Net Calc Watts Ln 1 ] [ Meter Data . Native Value ] = net watts 1 self . m blk b [ Field . Net Calc Watts Ln 2 ] [ Meter Data . Native Value ] = net watts 2 self . m blk b [ Field . Net Calc Watts Ln 3 ] [ Meter Data . Native Value ] = net watts 3 self . m blk b [ Field . Net Calc Watts Tot ] [ Meter Data . Native Value ] = net watts tot self . m blk b [ Field . Net Calc Watts Ln 1 ] [ Meter Data . String Value ] = str ( net watts 1 ) self . m blk b [ Field . Net Calc Watts Ln 2 ] [ Meter Data . String Value ] = str ( net watts 2 ) self . m blk b [ Field . Net Calc Watts Ln 3 ] [ Meter Data . String Value ] = str ( net watts 3 ) self . m blk b [ Field . Net Calc Watts Tot ] [ Meter Data . String Value ] = str ( net watts tot ) pass
def serial Post End ( self ) : ekm log ( "Termination string sent (" + self . m context + ")" ) try : self . m serial port . write ( "0142300375" . decode ( "hex" ) ) except : ekm log ( traceback . format exc ( sys . exc info ( ) ) ) pass
def apply tasks to issue ( self , issue , tasks , issue body = None ) : issue body = issue body or issue . body task numbers = transport . format task numbers with links ( tasks ) if task numbers : new body = transport . ASANA SECTION RE . sub ( '' , issue body ) new body = new body + "\n## Asana Tasks:\n\n%s" % task numbers transport . issue edit ( issue , body = new body ) return new body return issue body
def statistics ( self , elapsed , result ) : return "\n" . join ( ( self . timing ( elapsed ) , self . result summary ( result ) ) )
def color ( self , color , text ) : return "{escape}{text}{reset}" . format ( escape = self . ANSI [ color ] , text = text , reset = self . ANSI [ "reset" ] , )
def show ( self , text ) : self . stream . write ( text ) self . stream . flush ( )
def result summary ( self , result ) : return "{} examples, {} errors, {} failures\n" . format ( result . tests Run , len ( result . errors ) , len ( result . failures ) , )
def parse ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] # Evade http://bugs.python.org/issue9253 if not argv or argv [ 0 ] not in { "run" , "transform" } : argv = [ "run" ] + argv arguments = clean ( parser . parse args ( argv ) ) return arguments
def setup ( config ) : formatter = config . Formatter ( ) if config . verbose : formatter = result . Verbose ( formatter ) if config . color : formatter = result . Colored ( formatter ) current result = result . Example Result ( formatter ) ivoire . current result = ivoire . manager . result = current result
def run ( config ) : setup ( config ) if config . exitfirst : ivoire . current result . failfast = True ivoire . current result . start Test Run ( ) for spec in config . specs : try : load by name ( spec ) except Exception : ivoire . current result . add Error ( Example Not Running ( ) , sys . exc info ( ) ) ivoire . current result . stop Test Run ( ) sys . exit ( not ivoire . current result . was Successful ( ) )
def transform ( config ) : if transform possible : Example Loader . register ( ) args , sys . argv [ 1 : ] = sys . argv [ 1 : ] , config . args try : return runpy . run path ( config . runner , run name = " main " ) finally : sys . argv [ 1 : ] = args
def takes only self ( self ) : return ast . arguments ( args = [ ast . arg ( arg = "self" ) ] , defaults = [ ] , kw defaults = [ ] , kwonlyargs = [ ] , )
def register ( cls ) : cls . finder = File Finder . path hook ( ( cls , [ cls . suffix ] ) ) sys . path hooks . append ( cls . finder )
def source to code ( self , source bytes , source path ) : node = ast . parse ( source bytes ) transformed = Example Transformer ( ) . transform ( node ) return compile ( transformed , source path , "exec" , dont inherit = True )
def apply argument parser ( arguments Parser , options = None ) : if options is not None : args = arguments Parser . parse args ( options ) else : args = arguments Parser . parse args ( ) return args
def load by name ( name ) : if os . path . exists ( name ) : load from path ( name ) else : import ( name )
def load from path ( path ) : if os . path . isdir ( path ) : paths = discover ( path ) else : paths = [ path ] for path in paths : name = os . path . basename ( os . path . splitext ( path ) [ 0 ] ) imp . load source ( name , path )
def delimit ( values , delimiter = ', ' ) : toks = [ ] if not values : return toks if not isinstance ( delimiter , ( list , tuple ) ) : delimiter = [ delimiter ] last = len ( values ) - 1 for i , value in enumerate ( values ) : toks . append ( value ) if i < last : toks . extend ( delimiter ) return toks
def exists ( value ) : if not isinstance ( value , Token ) : raise Type Error ( 'value must be a token' ) if not hasattr ( value , 'identifier' ) : raise Type Error ( 'value must support an identifier' ) if not value . identifier : value = value . class ( * * value . dict ) value . identifier = 'v' ident = Identifier ( value . identifier ) return Query ( [ Optional Match ( value ) , Return ( Predicate ( ident , 'IS NOT NULL' ) ) , Limit ( 1 ) , ] )
def get ( value ) : if not isinstance ( value , Token ) : raise Type Error ( 'value must be a token' ) if not hasattr ( value , 'identifier' ) : raise Type Error ( 'value must support an identifier' ) if not value . identifier : value = value . class ( * * value . dict ) value . identifier = 'v' ident = Identifier ( value . identifier ) return Query ( [ Match ( value ) , Return ( ident ) ] )
def check ( self ) : if self . closed : raise Value Error ( "Cannot check a closed state" ) self . maybe Reset ( ) if self . url is None : return False return self . maybe Check ( )
def wrap Heart ( service ) : master = taservice . Multi Service ( ) service . set Service Parent ( master ) maybe Add Heart ( master ) return master
def freeze from checkpoint ( input checkpoint , output file path , output node names ) : check input checkpoint ( input checkpoint ) output node names = output node names string as list ( output node names ) with tf . Session ( ) as sess : restore from checkpoint ( sess , input checkpoint ) freeze graph . freeze graph with def protos ( input graph def = sess . graph def , input saver def = None , input checkpoint = input checkpoint , output node names = ',' . join ( output node names ) , restore op name = 'save/restore all' , filename tensor name = 'save/Const:0' , output graph = output file path , clear devices = True , initializer nodes = '' )
def freeze ( sess , output file path , output node names ) : with Temporary Directory ( ) as temp dir name : checkpoint path = os . path . join ( temp dir name , 'model.ckpt' ) tf . train . Saver ( ) . save ( sess , checkpoint path ) freeze from checkpoint ( checkpoint path , output file path , output node names )
def save graph only ( sess , output file path , output node names , as text = False ) : for node in sess . graph def . node : node . device = '' graph def = graph util . extract sub graph ( sess . graph def , output node names ) output dir , output filename = os . path . split ( output file path ) graph io . write graph ( graph def , output dir , output filename , as text = as text )
def save graph only from checkpoint ( input checkpoint , output file path , output node names , as text = False ) : check input checkpoint ( input checkpoint ) output node names = output node names string as list ( output node names ) with tf . Session ( ) as sess : restore from checkpoint ( sess , input checkpoint ) save graph only ( sess , output file path , output node names , as text = as text )
def save weights ( sess , output path , conv var names = None , conv transpose var names = None ) : if not conv var names : conv var names = [ ] if not conv transpose var names : conv transpose var names = [ ] for var in tf . trainable variables ( ) : filename = '{}-{}' . format ( output path , var . name . replace ( ':' , '-' ) . replace ( '/' , '-' ) ) if var . name in conv var names : var = tf . transpose ( var , perm = [ 3 , 0 , 1 , 2 ] ) elif var . name in conv transpose var names : var = tf . transpose ( var , perm = [ 3 , 1 , 0 , 2 ] ) value = sess . run ( var ) # noinspection Py Type Checker with open ( filename , 'w' ) as file : value . tofile ( file )
def save weights from checkpoint ( input checkpoint , output path , conv var names = None , conv transpose var names = None ) : check input checkpoint ( input checkpoint ) with tf . Session ( ) as sess : restore from checkpoint ( sess , input checkpoint ) save weights ( sess , output path , conv var names = conv var names , conv transpose var names = conv transpose var names )
def restore from checkpoint ( sess , input checkpoint ) : saver = tf . train . import meta graph ( '{}.meta' . format ( input checkpoint ) ) saver . restore ( sess , input checkpoint ) return saver
def render tag ( self , context , * tag args , * * tag kwargs ) : raise Not Implemented Error ( "{0}.render tag() is not implemented!" . format ( self . class . name ) )
def validate args ( cls , tag name , * args , * * kwargs ) : if cls . min args is not None and len ( args ) < cls . min args : if cls . min args == 1 : raise Template Syntax Error ( "'{0}' tag requires at least {1} argument" . format ( tag name , cls . min args ) ) else : raise Template Syntax Error ( "'{0}' tag requires at least {1} arguments" . format ( tag name , cls . min args ) ) if cls . max args is not None and len ( args ) > cls . max args : if cls . max args == 0 : if cls . allowed kwargs : raise Template Syntax Error ( "'{0}' tag only allows keywords arguments, for example {1}=\"...\"." . format ( tag name , cls . allowed kwargs [ 0 ] ) ) else : raise Template Syntax Error ( "'{0}' tag doesn't support any arguments" . format ( tag name ) ) elif cls . max args == 1 : raise Template Syntax Error ( "'{0}' tag only allows {1} argument." . format ( tag name , cls . max args ) ) else : raise Template Syntax Error ( "'{0}' tag only allows {1} arguments." . format ( tag name , cls . max args ) )
def get context data ( self , parent context , * tag args , * * tag kwargs ) : raise Not Implemented Error ( "{0}.get context data() is not implemented." . format ( self . class . name ) )
def render tag ( self , context , * tag args , * * tag kwargs ) : if self . as var : # Assign the value in the parent context context [ self . as var ] = self . get value ( context , * tag args , * * tag kwargs ) return u''
def parse ( cls , parser , token ) : bits , as var = parse as var ( parser , token ) tag name , args , kwargs = parse token kwargs ( parser , bits , ( 'template' , ) + cls . allowed kwargs , compile args = cls . compile args , compile kwargs = cls . compile kwargs ) # Pass through standard chain cls . validate args ( tag name , * args ) return cls ( tag name , as var , * args , * * kwargs )
def render tag ( self , context , * tag args , * * tag kwargs ) : # Be very explicit about which base functionality is used: # Using super() for mixin support will not work nicely anyway here. if self . as var : # Assign the value in the parent context return Base Assignment Node . render tag ( self , context , * tag args , * * tag kwargs ) else : # Render the output using the Base Inclusion Node features return Base Inclusion Node . render tag ( self , context , * tag args , * * tag kwargs )
def caffe to tensorflow session ( caffe def path , caffemodel path , inputs , graph name = 'Graph' , conversion out dir path = None , use padding same = False ) : try : # noinspection Py Unresolved References from caffeflow import convert except Import Error : raise Exception ( "caffeflow package needs to be installed to freeze Caffe models. Check out the README file." ) with ( dummy context mgr ( conversion out dir path ) or util . Temporary Directory ( ) ) as dir path : params values output path = os . path . join ( dir path , 'params values.npy' ) network output path = os . path . join ( dir path , 'network.py' ) convert . convert ( caffe def path , caffemodel path , params values output path , network output path , False , use padding same = use padding same ) network module = imp . load source ( 'module.name' , network output path ) network class = getattr ( network module , graph name ) network = network class ( inputs ) sess = tf . Session ( ) network . load ( params values output path , sess ) return sess
def freeze ( caffe def path , caffemodel path , inputs , output file path , output node names , graph name = 'Graph' , conversion out dir path = None , checkpoint out path = None , use padding same = False ) : with caffe to tensorflow session ( caffe def path , caffemodel path , inputs , graph name = graph name , conversion out dir path = conversion out dir path , use padding same = use padding same ) as sess : saver = tf . train . Saver ( ) with ( dummy context mgr ( checkpoint out path ) or util . Temporary Directory ( ) ) as temp dir path : checkpoint path = checkpoint out path or os . path . join ( temp dir path , 'pose.ckpt' ) saver . save ( sess , checkpoint path ) output node names = util . output node names string as list ( output node names ) tf freeze . freeze from checkpoint ( checkpoint path , output file path , output node names )
def save graph only ( caffe def path , caffemodel path , inputs , output file path , output node names , graph name = 'Graph' , use padding same = False ) : with caffe to tensorflow session ( caffe def path , caffemodel path , inputs , graph name = graph name , use padding same = use padding same ) as sess : tf freeze . save graph only ( sess , output file path , output node names )
def save weights ( caffe def path , caffemodel path , inputs , output path , graph name = 'Graph' , conv var names = None , conv transpose var names = None , use padding same = False ) : with caffe to tensorflow session ( caffe def path , caffemodel path , inputs , graph name = graph name , use padding same = use padding same ) as sess : tf freeze . save weights ( sess , output path , conv var names = conv var names , conv transpose var names = conv transpose var names )
def descendant ( self , chain path ) : public child = self . hdkeychain chain step bytes = 4 max bits per step = 2 ** 31 chain steps = [ int ( chain path [ i : i + chain step bytes * 2 ] , 16 ) % max bits per step for i in range ( 0 , len ( chain path ) , chain step bytes * 2 ) ] for step in chain steps : public child = public child . get child ( step ) return Public Keychain ( public child )
def object iter ( obj , parent = None , parent key = None , idx = None , siblings = None ) : obj node = Node ( value = obj , parent = parent , parent key = parent key , siblings = siblings , idx = idx ) if isinstance ( obj , list ) : siblings = len ( obj ) for i , elem in enumerate ( obj ) : for node in object iter ( elem , obj node , None , i + 1 , siblings ) : yield node elif isinstance ( obj , collections . Mapping ) : for key in obj : for node in object iter ( obj [ key ] , obj node , key ) : yield node yield obj node
def parse ( self , selector ) : log . debug ( self . obj ) tokens = lex ( selector ) if self . peek ( tokens , 'operator' ) == '*' : self . match ( tokens , 'operator' ) results = list ( object iter ( self . obj ) ) else : results = self . selector production ( tokens ) results = [ node . value for node in results ] # single results should be returned as a primitive if len ( results ) == 1 : return results [ 0 ] elif not len ( results ) : return None return results
def selector production ( self , tokens ) : validators = [ ] # the following productions should return predicate functions. if self . peek ( tokens , 'type' ) : type = self . match ( tokens , 'type' ) validators . append ( self . type production ( type ) ) if self . peek ( tokens , 'identifier' ) : key = self . match ( tokens , 'identifier' ) validators . append ( self . key production ( key ) ) if self . peek ( tokens , 'pclass' ) : pclass = self . match ( tokens , 'pclass' ) validators . append ( self . pclass production ( pclass ) ) if self . peek ( tokens , 'nth func' ) : nth func = self . match ( tokens , 'nth func' ) validators . append ( self . nth child production ( nth func , tokens ) ) if self . peek ( tokens , 'pclass func' ) : pclass func = self . match ( tokens , 'pclass func' ) validators . append ( self . pclass func production ( pclass func , tokens ) ) if not len ( validators ) : raise Selector Syntax Error ( 'no selector recognized.' ) # apply validators from a selector expression to self.obj results = self . match nodes ( validators , self . obj ) if self . peek ( tokens , 'operator' ) : operator = self . match ( tokens , 'operator' ) rvals = self . selector production ( tokens ) if operator == ',' : results . extend ( rvals ) elif operator == '>' : results = self . parents ( results , rvals ) elif operator == '~' : results = self . siblings ( results , rvals ) elif operator == ' ' : results = self . ancestors ( results , rvals ) else : raise Selector Syntax Error ( "unrecognized operator '%s'" % operator ) else : if len ( tokens ) : rvals = self . selector production ( tokens ) results = self . ancestors ( results , rvals ) return results
def parents ( self , lhs , rhs ) : return [ node for node in rhs if node . parent in lhs ]
def ancestors ( self , lhs , rhs ) : def search ( node ) : if node in lhs : return True if not node . parent : return False return search ( node . parent ) return [ node for node in rhs if search ( node ) ]
def siblings ( self , lhs , rhs ) : parents = [ node . parent for node in lhs ] return [ node for node in rhs if node . parent in parents ]
def nth child production ( self , lexeme , tokens ) : args = self . match ( tokens , 'expr' ) pat = self . nth child pat . match ( args ) if pat . group ( 5 ) : a = 2 b = 1 if pat . group ( 5 ) == 'odd' else 0 elif pat . group ( 6 ) : a = 0 b = int ( pat . group ( 6 ) ) else : sign = pat . group ( 1 ) if pat . group ( 1 ) else '+' coef = pat . group ( 2 ) if pat . group ( 2 ) else '1' a = eval ( sign + coef ) b = eval ( pat . group ( 3 ) + pat . group ( 4 ) ) if pat . group ( 3 ) else 0 reverse = False if lexeme == 'nth-last-child' : reverse = True def validate ( node ) : """This crazy function taken from jsonselect.js:444.""" if not node . siblings : return False idx = node . idx - 1 tot = node . siblings if reverse : idx = tot - idx else : idx += 1 if a == 0 : m = b == idx else : mod = ( idx - b ) % a m = not mod and ( idx * a + b ) >= 0 return m return validate
def get Body ( self , url , method = 'GET' , headers = { } , data = None , socket = None ) : if not 'User-Agent' in headers : headers [ 'User-Agent' ] = [ 'Tensor HTTP checker' ] return self . request ( url , method , headers , data , socket )
def expire ( self , age ) : now = time . time ( ) cache = self . acquire cache ( ) expired = [ k for k , v in cache . items ( ) if ( now - v [ 0 ] ) > age ] for k in expired : if k in cache : del cache [ k ] if k in self . store : del self . store [ k ] self . write cache ( cache )
def set ( self , k , v ) : self . store [ k ] = ( time . time ( ) , v ) self . persist ( )
def get ( self , k ) : if self . changed ( ) : self . read ( ) if k in self . store : return tuple ( self . store [ k ] ) else : return None
def contains ( self , k ) : if self . changed ( ) : self . read ( ) return k in self . store . keys ( )
def rendered content ( self ) : template = self . resolve template ( self . template name ) if django . VERSION [ 1 ] < 8 : if template . name . endswith ( '.min' ) : return super ( Minified Js Template Response , self ) . rendered content else : if template . template . name . endswith ( '.min' ) : return super ( Minified Js Template Response , self ) . rendered content # if no minified template exists, minify the response content = super ( Minified Js Template Response , self ) . rendered content content = jsmin . jsmin ( content ) return content
def get ( self , max lines = None ) : rows = [ ] self . get fn ( lambda row : rows . append ( row ) , max lines = max lines ) return rows
def engine ( self ) : if not hasattr ( self , ' engine' ) : from cryptography . fernet import Fernet from cryptography . hazmat . backends import default backend from cryptography . hazmat . primitives import hashes digest = hashes . Hash ( hashes . SHA256 ( ) , backend = default backend ( ) ) digest . update ( current app . config [ 'SECRET KEY' ] . encode ( 'utf8' ) ) fernet key = urlsafe b64encode ( digest . finalize ( ) ) self . engine = Fernet ( fernet key ) return self . engine
def create token ( self , obj id , extra data ) : return self . engine . encrypt ( super ( Encrypted Token Mix In , self ) . create token ( obj id , extra data ) )
def compat validate token ( cls , * args , * * kwargs ) : data = None for algorithm in SUPPORTED DIGEST ALGORITHMS : data = cls ( algorithm name = algorithm ) . validate token ( * args , * * kwargs ) if not data : # move to next algorithm continue return data
def create token ( cls , obj id , data , expires at = None ) : if expires at : s = Timed Secret Link Serializer ( expires at = expires at ) else : s = Secret Link Serializer ( ) return s . create token ( obj id , data )
def Counter32 ( a , b , delta ) : if b < a : c = 4294967295 - a return ( c + b ) / float ( delta ) return ( b - a ) / float ( delta )
def Counter64 ( a , b , delta ) : if b < a : c = 18446744073709551615 - a return ( c + b ) / float ( delta ) return ( b - a ) / float ( delta )
def average duration ( total duration , visits ) : if not visits : seconds = 0 else : seconds = int ( round ( total duration / Decimal ( visits ) ) ) duration = timedelta ( seconds = seconds ) return str ( duration )
def setup Sources ( self , config ) : sources = config . get ( 'sources' , [ ] ) for source in sources : src = self . create Source ( source ) self . setup Triggers ( source , src ) self . sources . append ( src )
def validate expires at ( form , field ) : if form . accept . data : if not field . data or datetime . utcnow ( ) . date ( ) >= field . data : raise validators . Stop Validation ( ( "Please provide a future date." ) ) if not field . data or datetime . utcnow ( ) . date ( ) + timedelta ( days = 365 ) < field . data : raise validators . Stop Validation ( ( "Please provide a date no more than 1 year into the future." ) )
def validate accept ( form , field ) : if field . data and form . reject . data : raise validators . Validation Error ( ( "Both reject and accept cannot be set at the same time." ) )
def validate reject ( form , field ) : if field . data and form . accept . data : raise validators . Validation Error ( ( "Both reject and accept cannot be set at the same time." ) )
def verify token ( ) : try : from . models import Secret Link token = request . args [ 'token' ] # if the token is valid if token and Secret Link . validate token ( token , { } ) : # then save in session the token session [ 'accessrequests-secret-token' ] = token except Key Error : pass
def init app ( self , app ) : app . before request ( verify token ) self . init config ( app ) state = App State ( app = app ) app . extensions [ 'zenodo-accessrequests' ] = state
def name ( self ) : if ( self . device type and self . device type . code in ( Device Type . MOBILE , Device Type . TABLET ) ) : return self . device else : return self . browser
def warn node ( self , msg , * args , * * kwargs ) : if not msg . startswith ( 'nonlocal image URI found:' ) : warn node old ( self , msg , * args , * * kwargs )
def connect receivers ( ) : request created . connect ( send email validation ) request confirmed . connect ( send confirmed notifications ) request rejected . connect ( send reject notification ) # Order is important: request accepted . connect ( create secret link ) request accepted . connect ( send accept notification )
def create secret link ( request , message = None , expires at = None ) : pid , record = get record ( request . recid ) if not record : raise Record Not Found ( request . recid ) description = render template ( "zenodo accessrequests/link description.tpl" , request = request , record = record , pid = pid , expires at = expires at , message = message , ) request . create secret link ( record [ "title" ] , description = description , expires at = expires at )
def send accept notification ( request , message = None , expires at = None ) : pid , record = get record ( request . recid ) send notification ( request . sender email , ( "Access request accepted" ) , "zenodo accessrequests/emails/accepted.tpl" , request = request , record = record , pid = pid , record link = request . link . get absolute url ( 'invenio records ui.recid' ) , message = message , expires at = expires at , )
def send confirmed notifications ( request ) : pid , record = get record ( request . recid ) if record is None : current app . logger . error ( "Cannot retrieve record %s. Emails not sent" % request . recid ) return title = ( "Access request: %(record)s" , record = record [ "title" ] ) send notification ( request . receiver . email , title , "zenodo accessrequests/emails/new request.tpl" , request = request , record = record , pid = pid , ) send notification ( request . sender email , title , "zenodo accessrequests/emails/confirmation.tpl" , request = request , record = record , pid = pid , )
def send email validation ( request ) : token = Email Confirmation Serializer ( ) . create token ( request . id , dict ( email = request . sender email ) ) pid , record = get record ( request . recid ) send notification ( request . sender email , ( "Access request verification" ) , "zenodo accessrequests/emails/validate email.tpl" , request = request , record = record , pid = pid , days = timedelta ( seconds = current app . config [ "ACCESSREQUESTS CONFIRMLINK EXPIRES IN" ] ) . days , confirm link = url for ( "invenio records ui.recid access request email confirm" , pid value = request . recid , token = token , external = True , ) )
def send reject notification ( request , message = None ) : pid , record = get record ( request . recid ) send notification ( request . sender email , ( "Access request rejected" ) , "zenodo accessrequests/emails/rejected.tpl" , request = request , record = record , pid = pid , message = message , )
def send notification ( to , subject , template , * * ctx ) : msg = Message ( subject , sender = current app . config . get ( 'SUPPORT EMAIL' ) , recipients = [ to ] ) msg . body = render template ( template , * * ctx ) send email . delay ( msg . dict )
def create ( cls , title , owner , extra data , description = "" , expires at = None ) : if isinstance ( expires at , date ) : expires at = datetime . combine ( expires at , datetime . min . time ( ) ) with db . session . begin nested ( ) : obj = cls ( owner = owner , title = title , description = description , expires at = expires at , token = '' , ) db . session . add ( obj ) with db . session . begin nested ( ) : # Create token (dependent on obj.id and recid) obj . token = Secret Link Factory . create token ( obj . id , extra data , expires at = expires at ) . decode ( 'utf8' ) link created . send ( obj ) return obj
def revoke ( self ) : if self . revoked at is None : with db . session . begin nested ( ) : self . revoked at = datetime . utcnow ( ) link revoked . send ( self ) return True return False
def get by receiver ( cls , request id , user ) : return cls . query . filter by ( id = request id , receiver user id = user . id ) . first ( )
def confirm email ( self ) : with db . session . begin nested ( ) : if self . status != Request Status . EMAIL VALIDATION : raise Invalid Request State Error ( Request Status . EMAIL VALIDATION ) self . status = Request Status . PENDING request confirmed . send ( self )
def create secret link ( self , title , description = None , expires at = None ) : self . link = Secret Link . create ( title , self . receiver , extra data = dict ( recid = self . recid ) , description = description , expires at = expires at , ) return self . link
def is embargoed ( record ) : return record . get ( 'access right' ) == 'embargoed' and record . get ( 'embargo date' ) and record . get ( 'embargo date' ) > datetime . utcnow ( ) . date ( )
def access request ( pid , record , template , * * kwargs ) : recid = int ( pid . pid value ) datastore = Local Proxy ( lambda : current app . extensions [ 'security' ] . datastore ) # Record must be in restricted access mode. if record . get ( 'access right' ) != 'restricted' or not record . get ( 'access conditions' ) : abort ( 404 ) # Record must have an owner and owner must still exists. owners = record . get ( 'owners' , [ ] ) record owners = [ datastore . find user ( id = owner id ) for owner id in owners ] if not record owners : abort ( 404 ) sender = None initialdata = dict ( ) # Prepare initial form data if current user . is authenticated : sender = current user initialdata [ 'email' ] = current user . email if current user . profile : initialdata [ 'full name' ] = current user . profile . full name # Normal form validation form = Access Request Form ( formdata = request . form , * * initialdata ) if form . validate on submit ( ) : accreq = Access Request . create ( recid = recid , receiver = record owners [ 0 ] , sender full name = form . data [ 'full name' ] , sender email = form . data [ 'email' ] , justification = form . data [ 'justification' ] , sender = sender ) db . session . commit ( ) if accreq . status == Request Status . EMAIL VALIDATION : flash ( ( "Email confirmation needed: We have sent you an email to " "verify your address. Please check the email and follow the " "instructions to complete the access request." ) , category = 'info' ) else : flash ( ( "Access request submitted." ) , category = 'info' ) return redirect ( url for ( 'invenio records ui.recid' , pid value = recid ) ) return render template ( template , pid = pid , record = record , form = form , owners = record owners , )
def confirm ( pid , record , template , * * kwargs ) : recid = int ( pid . pid value ) token = request . view args [ 'token' ] # Validate token data = Email Confirmation Serializer . compat validate token ( token ) if data is None : flash ( ( "Invalid confirmation link." ) , category = 'danger' ) return redirect ( url for ( "invenio records ui.recid" , pid value = recid ) ) # Validate request exists. r = Access Request . query . get ( data [ 'id' ] ) if not r : abort ( 404 ) # Confirm email address. if r . status != Request Status . EMAIL VALIDATION : abort ( 404 ) r . confirm email ( ) db . session . commit ( ) flash ( ( "Email validated and access request submitted." ) , category = 'info' ) return redirect ( url for ( "invenio records ui.recid" , pid value = recid ) )
def get endpoint ( self ) : return SSH Command Client Endpoint . new Connection ( reactor , b'/bin/cat' , self . username , self . hostname , port = self . port , keys = self . keys , password = self . password , known Hosts = self . known Hosts )
def reverse ( self , col ) : if col in self . options : if self . is selected ( col ) : return col if not self . asc else '-{0}' . format ( col ) else : return col return None
def selected ( self ) : if self . selected : return self . selected if self . asc else "-{0}" . format ( self . selected ) return None
def items ( self ) : if self . asc is not None : if self . selected and self . asc : return self . query . order by ( self . selected ) elif self . selected and not self . asc : return self . query . order by ( desc ( self . selected ) ) return self . query
def records ( ) : import uuid from invenio records . api import Record from invenio pidstore . models import Persistent Identifier , PID Status create test user ( ) indexer = Record Indexer ( ) # Record 1 - Live record with db . session . begin nested ( ) : rec uuid = uuid . uuid4 ( ) pid1 = Persistent Identifier . create ( 'recid' , '1' , object type = 'rec' , object uuid = rec uuid , status = PID Status . REGISTERED ) Record . create ( { 'title' : 'Registered' , 'description' : 'This is an awesome description' , 'control number' : '1' , 'access right' : 'restricted' , 'access conditions' : 'fuu' , 'owners' : [ 1 , 2 ] , 'recid' : 1 } , id = rec uuid ) indexer . index by id ( pid1 . object uuid ) db . session . commit ( ) sleep ( 3 )
def init ssh ( self ) : self . ssh host = self . config . get ( 'ssh host' , self . hostname ) self . known hosts = self . config . get ( 'ssh knownhosts file' , self . tensor . config . get ( 'ssh knownhosts file' , None ) ) self . ssh keyfile = self . config . get ( 'ssh keyfile' , self . tensor . config . get ( 'ssh keyfile' , None ) ) self . ssh key = self . config . get ( 'ssh key' , self . tensor . config . get ( 'ssh key' , None ) ) # Not sure why you'd bother but maybe you've got a weird policy self . ssh keypass = self . config . get ( 'ssh keypass' , self . tensor . config . get ( 'ssh keypass' , None ) ) self . ssh user = self . config . get ( 'ssh username' , self . tensor . config . get ( 'ssh username' , None ) ) self . ssh password = self . config . get ( 'ssh password' , self . tensor . config . get ( 'ssh password' , None ) ) self . ssh port = self . config . get ( 'ssh port' , self . tensor . config . get ( 'ssh port' , 22 ) ) # Verify config to see if we're good to go if not ( self . ssh key or self . ssh keyfile or self . ssh password ) : raise Exception ( "To use SSH you must specify *one* of ssh key," " ssh keyfile or ssh password for this source" " check or globally" ) if not self . ssh user : raise Exception ( "ssh username must be set" ) self . ssh keydb = [ ] c Hash = hashlib . sha1 ( ':' . join ( ( self . ssh host , self . ssh user , str ( self . ssh port ) , str ( self . ssh password ) , str ( self . ssh key ) , str ( self . ssh keyfile ) ) ) . encode ( ) ) . hexdigest ( ) if c Hash in self . tensor . host Connector Cache : self . ssh client = self . tensor . host Connector Cache . get ( c Hash ) self . ssh connector = False else : self . ssh connector = True self . ssh client = ssh . SSH Client ( self . ssh host , self . ssh user , self . ssh port , password = self . ssh password , knownhosts = self . known hosts ) if self . ssh keyfile : self . ssh client . add Key File ( self . ssh keyfile , self . ssh keypass ) if self . ssh key : self . ssh client . add Key String ( self . ssh key , self . ssh keypass ) self . tensor . host Connector Cache [ c Hash ] = self . ssh client
def start Timer ( self ) : self . td = self . t . start ( self . inter ) if self . use ssh and self . ssh connector : self . ssh client . connect ( )
def create Event ( self , state , description , metric , prefix = None , hostname = None , aggregation = None , evtime = None ) : if prefix : service name = self . service + "." + prefix else : service name = self . service return Event ( state , service name , description , metric , self . ttl , hostname = hostname or self . hostname , aggregation = aggregation , evtime = evtime , tags = self . tags , attributes = self . attributes )
def create Log ( self , type , data , evtime = None , hostname = None ) : return Event ( None , type , data , 0 , self . ttl , hostname = hostname or self . hostname , evtime = evtime , tags = self . tags , type = 'log' )
def index ( ) : query = request . args . get ( 'query' , '' ) order = request . args . get ( 'sort' , '-created' ) try : page = int ( request . args . get ( 'page' , 1 ) ) per page = int ( request . args . get ( 'per page' , 20 ) ) except ( Type Error , Value Error ) : abort ( 404 ) # Delete form form = Delete Form ( request . form ) if form . validate on submit ( ) : link = Secret Link . query by owner ( current user ) . filter by ( id = form . link . data ) . first ( ) if link . revoke ( ) : flash ( ( "Shared link revoked." ) , category = 'success' ) db . session . commit ( ) # Links links = Secret Link . query by owner ( current user ) . filter ( Secret Link . revoked at . is ( None ) ) # Querying if query : lquery = "%{0}%" . format ( query ) links = links . filter ( Secret Link . title . like ( lquery ) | Secret Link . description . like ( lquery ) ) # Ordering ordering = Query Ordering ( links , [ 'title' , 'created' , 'expires at' ] , order ) links = ordering . items ( ) # Pending access requests requests = Access Request . query by receiver ( current user ) . filter by ( status = Request Status . PENDING ) . order by ( 'created' ) return render template ( "zenodo accessrequests/settings/index.html" , links pagination = links . paginate ( page , per page = per page ) , requests = requests , query = query , order = ordering , get record = get record , form = Delete Form ( ) , )
def create Client ( self ) : server = self . config . get ( 'server' , 'localhost' ) port = self . config . get ( 'port' , 5555 ) failover = self . config . get ( 'failover' , False ) self . factory = riemann . Riemann Client Factory ( server , failover = failover ) if failover : initial = random . choice ( server ) else : initial = server log . msg ( 'Connecting to Riemann on %s:%s' % ( initial , port ) ) if self . tls : if SSL : self . connector = reactor . connect SSL ( initial , port , self . factory , Client TLS Context ( self . key , self . cert ) ) else : log . msg ( '[FATAL] SSL support not available!' ' Please install Py Open SSL. Exiting now' ) reactor . stop ( ) else : self . connector = reactor . connect TCP ( initial , port , self . factory ) d = defer . Deferred ( ) def cb ( ) : # Wait until we have a useful proto object if hasattr ( self . factory , 'proto' ) and self . factory . proto : self . t . start ( self . inter ) d . callback ( None ) else : reactor . call Later ( 0.01 , cb ) cb ( ) return d
def stop ( self ) : self . t . stop ( ) self . factory . stop Trying ( ) self . connector . disconnect ( )
def tick ( self ) : if self . factory . proto : # Check backpressure if ( self . pressure < 0 ) or ( self . factory . proto . pressure <= self . pressure ) : self . empty Queue ( ) elif self . expire : # Check queue age and expire stale events for i , e in enumerate ( self . events ) : if ( time . time ( ) - e . time ) > e . ttl : self . events . pop ( i )
def empty Queue ( self ) : if self . events : if self . queue Depth and ( len ( self . events ) > self . queue Depth ) : # Remove maximum of self.queue Depth items from queue events = self . events [ : self . queue Depth ] self . events = self . events [ self . queue Depth : ] else : events = self . events self . events = [ ] if self . allow nan : self . factory . proto . send Events ( events ) else : self . factory . proto . send Events ( [ e for e in events if e . metric is not None ] )
def create Client ( self ) : server = self . config . get ( 'server' , '127.0.0.1' ) port = self . config . get ( 'port' , 5555 ) def connect ( ip ) : self . protocol = riemann . Riemann UDP ( ip , port ) self . endpoint = reactor . listen UDP ( 0 , self . protocol ) d = reactor . resolve ( server ) d . add Callback ( connect ) return d
def create Client ( self ) : server = self . config . get ( 'server' , 'localhost' ) port = int ( self . config . get ( 'port' , 9200 ) ) self . client = elasticsearch . Elastic Search ( self . url , self . user , self . password , self . index ) self . t . start ( self . inter )
def tick ( self ) : if self . events : if self . queue Depth and ( len ( self . events ) > self . queue Depth ) : # Remove maximum of self.queue Depth items from queue events = self . events [ : self . queue Depth ] self . events = self . events [ self . queue Depth : ] else : events = self . events self . events = [ ] try : result = yield self . send Events ( events ) if result . get ( 'errors' , False ) : log . msg ( repr ( result ) ) self . events . extend ( events ) except Exception as e : log . msg ( 'Could not connect to elasticsearch ' + str ( e ) ) self . events . extend ( events )
def encode Event ( self , event ) : pbevent = proto pb2 . Event ( time = int ( event . time ) , state = event . state , service = event . service , host = event . hostname , description = event . description , tags = event . tags , ttl = event . ttl , ) if event . metric is not None : # I have no idea what I'm doing if isinstance ( event . metric , int ) : pbevent . metric sint64 = event . metric pbevent . metric f = float ( event . metric ) else : pbevent . metric d = float ( event . metric ) pbevent . metric f = float ( event . metric ) if event . attributes is not None : for key , value in event . attributes . items ( ) : attribute = pbevent . attributes . add ( ) attribute . key , attribute . value = key , value return pbevent
def encode Message ( self , events ) : message = proto pb2 . Msg ( events = [ self . encode Event ( e ) for e in events if e . type == 'riemann' ] ) return message . Serialize To String ( )
def decode Message ( self , data ) : message = proto pb2 . Msg ( ) message . Parse From String ( data ) return message
def send Events ( self , events ) : self . pressure += 1 self . send String ( self . encode Message ( events ) )
def generate ( ctx , url , * args , * * kwargs ) : file previews = ctx . obj [ 'file previews' ] options = { } metadata = kwargs [ 'metadata' ] width = kwargs [ 'width' ] height = kwargs [ 'height' ] output format = kwargs [ 'format' ] if metadata : options [ 'metadata' ] = metadata . split ( ',' ) if width : options . setdefault ( 'size' , { } ) options [ 'size' ] [ 'width' ] = width if height : options . setdefault ( 'size' , { } ) options [ 'size' ] [ 'height' ] = height if output format : options [ 'format' ] = output format results = file previews . generate ( url , * * options ) click . echo ( results )
def retrieve ( ctx , preview id , * args , * * kwargs ) : file previews = ctx . obj [ 'file previews' ] results = file previews . retrieve ( preview id ) click . echo ( results )
def message loop ( self , t q , r q ) : t msg = { } while t msg . get ( "state" , "" ) != " DIE " : try : t msg = t q . get ( True , self . cycle sleep ) # Poll blocking self . task = t msg . get ( "task" , "" ) #  DIE  has no task if self . task != "" : self . task . task start = time . time ( ) # Start the timer # Send ACK to the controller who requested work on this task self . r q send ( { "w id" : self . w id , "task" : self . task , "state" : " ACK " } ) # Update the sleep time with latest recommendations self . cycle sleep = self . task . worker loop delay # Assign the result of task.run() to task.result self . task . result = self . task . run ( ) self . task . task stop = time . time ( ) # Seconds since epoch self . r q send ( { "w id" : self . w id , "task" : self . task , "state" : " FINISHED " } ) # Ack work finished self . task = None except Empty : pass except Full : time . sleep ( 0.1 ) ## Disable extraneous error handling... except : if self . task is not None : self . task . task stop = time . time ( ) # Seconds since epoch # Handle all other errors here... tb str = "" . join ( tb . format exception ( * ( sys . exc info ( ) ) ) ) self . r q send ( { "w id" : self . w id , "task" : self . task , "error" : tb str , "state" : " ERROR " , } ) return
def log time ( self ) : if self . hot loop and self . time delta >= self . log interval : return True return False
def log message ( self ) : time delta = deepcopy ( self . time delta ) total work time = self . worker count * time delta time worked = sum ( self . exec times ) pct busy = time worked / total work time * 100.0 min task time = min ( self . exec times ) avg task time = sum ( self . exec times ) / len ( self . exec times ) max task time = max ( self . exec times ) min queue time = min ( self . queue times ) avg queue time = sum ( self . queue times ) / len ( self . queue times ) max queue time = max ( self . queue times ) time delta = self . time delta total tasks = len ( self . exec times ) avg task rate = total tasks / time delta self . reset ( ) task msg = """Ran {0} tasks, {1} tasks/s; {2} workers {3}% busy""" . format ( total tasks , round ( avg task rate , 1 ) , self . worker count , round ( pct busy , 1 ) ) task mam = """     Task run times: {0}/{1}/{2} (min/avg/max)""" . format ( round ( min task time , 3 ) , round ( avg task time , 3 ) , round ( max task time , 3 ) ) queue mam = """     Time in queue: {0}/{1}/{2} (min/avg/max)""" . format ( round ( min queue time , 6 ) , round ( avg queue time , 6 ) , round ( max queue time , 6 ) ) return """{0}\n{1}\n{2}""" . format ( task msg , task mam , queue mam )
def post Construction ( self ) : self . set Window Title ( 'Filesystem Browser' ) self . filesystem Widget . sort By Column ( 0 , Qt Core . Qt . Ascending Order ) # TODO: Remove once bookmarks widget implemented. self . bookmarks Widget . hide ( ) self . accept Button . set Default ( True ) self . accept Button . set Disabled ( True ) self . accept Button . clicked . connect ( self . accept ) self . cancel Button . clicked . connect ( self . reject ) self . configure Shortcuts ( ) self . set Location ( self . root ) self . filesystem Widget . horizontal Header ( ) . set Resize Mode ( Qt Gui . Q Header View . Resize To Contents ) self . filesystem Widget . horizontal Header ( ) . set Resize Mode ( 0 , Qt Gui . Q Header View . Stretch ) self . up Button . clicked . connect ( self . on Navigate Up Button Clicked ) self . location Widget . current Index Changed . connect ( self . on Navigate ) self . filesystem Widget . activated . connect ( self . on Activate Item ) selection Model = self . filesystem Widget . selection Model ( ) selection Model . current Row Changed . connect ( self . on Select Item )
def configure Shortcuts ( self ) : self . up Shortcut = Qt Gui . Q Shortcut ( Qt Gui . Q Key Sequence ( 'Backspace' ) , self ) self . up Shortcut . set Auto Repeat ( False ) self . up Shortcut . activated . connect ( self . on Navigate Up Button Clicked )
def on Activate Item ( self , index ) : item = self . filesystem Widget . model ( ) . item ( index ) if not isinstance ( item , riffle . model . File ) : self . accept Button . set Disabled ( True ) self . set Location ( item . path , interactive = True )
def on Select Item ( self , selection , previous Selection ) : self . accept Button . set Enabled ( True ) del self . selected [ : ] item = self . filesystem Widget . model ( ) . item ( selection ) self . selected . append ( item . path )
def on Navigate ( self , index ) : if index > 0 : self . set Location ( self . location Widget . item Data ( index ) , interactive = True )
def segment Path ( self , path ) : parts = [ ] model = self . filesystem Widget . model ( ) # Separate root path from remainder. remainder = path while True : if remainder == model . root . path : break if remainder : parts . append ( remainder ) head , tail = os . path . split ( remainder ) if head == remainder : break remainder = head parts . append ( model . root . path ) return parts
def finalize options ( self ) : self . resource source path = os . path . join ( RESOURCE PATH , 'resource.qrc' ) self . resource target path = RESOURCE TARGET PATH
def add Child ( self , item ) : if item . parent and item . parent != self : item . parent . remove Child ( item ) self . children . append ( item ) item . parent = self
def fetch Children ( self ) : children = [ ] for entry in Q Dir . drives ( ) : path = os . path . normpath ( entry . canonical File Path ( ) ) children . append ( Mount ( path ) ) return children
def fetch Children ( self ) : children = [ ] # List paths under this directory. paths = [ ] for name in os . listdir ( self . path ) : paths . append ( os . path . normpath ( os . path . join ( self . path , name ) ) ) # Handle collections. collections , remainder = clique . assemble ( paths , [ clique . PATTERNS [ 'frames' ] ] ) for path in remainder : try : child = Item Factory ( path ) except Value Error : pass else : children . append ( child ) for collection in collections : children . append ( Collection ( collection ) ) return children
def fetch Children ( self ) : children = [ ] for path in self . collection : try : child = Item Factory ( path ) except Value Error : pass else : children . append ( child ) return children
def row Count ( self , parent ) : if parent . column ( ) > 0 : return 0 if parent . is Valid ( ) : item = parent . internal Pointer ( ) else : item = self . root return len ( item . children )
def index ( self , row , column , parent ) : if not self . has Index ( row , column , parent ) : return Q Model Index ( ) if not parent . is Valid ( ) : item = self . root else : item = parent . internal Pointer ( ) try : child = item . children [ row ] except Index Error : return Q Model Index ( ) else : return self . create Index ( row , column , child )
def path Index ( self , path ) : if path == self . root . path : return Q Model Index ( ) if not path . startswith ( self . root . path ) : return Q Model Index ( ) parts = [ ] while True : if path == self . root . path : break head , tail = os . path . split ( path ) if head == path : if path : parts . append ( path ) break parts . append ( tail ) path = head parts . reverse ( ) if parts : item = self . root count = 0 for count , part in enumerate ( parts ) : matched = False for child in item . children : if child . name == part : item = child matched = True break if not matched : break if count + 1 == len ( parts ) : return self . create Index ( item . row , 0 , item ) return Q Model Index ( )
def parent ( self , index ) : if not index . is Valid ( ) : return Q Model Index ( ) item = index . internal Pointer ( ) if not item : return Q Model Index ( ) parent = item . parent if not parent or parent == self . root : return Q Model Index ( ) return self . create Index ( parent . row , 0 , parent )
def data ( self , index , role ) : if not index . is Valid ( ) : return None column = index . column ( ) item = index . internal Pointer ( ) if role == self . ITEM ROLE : return item elif role == Qt . Display Role : if column == 0 : return item . name elif column == 1 : if item . size : return item . size elif column == 2 : return item . type elif column == 3 : if item . modified is not None : return item . modified . strftime ( '%c' ) elif role == Qt . Decoration Role : if column == 0 : return self . icon Factory . icon ( item ) elif role == Qt . Text Alignment Role : if column == 1 : return Qt . Align Right else : return Qt . Align Left return None
def header Data ( self , section , orientation , role ) : if orientation == Qt . Horizontal : if section < len ( self . columns ) : column = self . columns [ section ] if role == Qt . Display Role : return column return None
def can Fetch More ( self , index ) : if not index . is Valid ( ) : item = self . root else : item = index . internal Pointer ( ) return item . can Fetch More ( )
def fetch More ( self , index ) : if not index . is Valid ( ) : item = self . root else : item = index . internal Pointer ( ) if item . can Fetch More ( ) : start Index = len ( item . children ) additional Children = item . fetch Children ( ) end Index = start Index + len ( additional Children ) - 1 if end Index >= start Index : self . begin Insert Rows ( index , start Index , end Index ) for new Child in additional Children : item . add Child ( new Child ) self . end Insert Rows ( )
def less Than ( self , left , right ) : source Model = self . source Model ( ) if source Model : left Item = source Model . item ( left ) right Item = source Model . item ( right ) if ( isinstance ( left Item , Directory ) and not isinstance ( right Item , Directory ) ) : return self . sort Order ( ) == Qt . Ascending Order elif ( not isinstance ( left Item , Directory ) and isinstance ( right Item , Directory ) ) : return self . sort Order ( ) == Qt . Descending Order return super ( Filesystem Sort Proxy , self ) . less Than ( left , right )
def path Index ( self , path ) : source Model = self . source Model ( ) if not source Model : return Q Model Index ( ) return self . map From Source ( source Model . path Index ( path ) )
def item ( self , index ) : source Model = self . source Model ( ) if not source Model : return None return source Model . item ( self . map To Source ( index ) )
def icon ( self , index ) : source Model = self . source Model ( ) if not source Model : return None return source Model . icon ( self . map To Source ( index ) )
def has Children ( self , index ) : source Model = self . source Model ( ) if not source Model : return False return source Model . has Children ( self . map To Source ( index ) )
def can Fetch More ( self , index ) : source Model = self . source Model ( ) if not source Model : return False return source Model . can Fetch More ( self . map To Source ( index ) )
def fetch More ( self , index ) : source Model = self . source Model ( ) if not source Model : return False return source Model . fetch More ( self . map To Source ( index ) )
def type ( self , item ) : icon Type = Icon Type . Unknown if isinstance ( item , riffle . model . Computer ) : icon Type = Icon Type . Computer elif isinstance ( item , riffle . model . Mount ) : icon Type = Icon Type . Mount elif isinstance ( item , riffle . model . Directory ) : icon Type = Icon Type . Directory elif isinstance ( item , riffle . model . File ) : icon Type = Icon Type . File elif isinstance ( item , riffle . model . Collection ) : icon Type = Icon Type . Collection return icon Type
def get max fd ( self ) : limits = resource . getrlimit ( resource . RLIMIT NOFILE ) result = limits [ 1 ] if result == resource . RLIM INFINITY : result = maxfd return result
def close fd ( self , fd ) : try : os . close ( fd ) except OS Error , exc : if exc . errno != errno . EBADF : msg = "Failed to close file descriptor {}: {}" . format ( fd , exc ) raise Error ( msg )
def close open fds ( self ) : maxfd = self . get max fd ( ) for fd in reversed ( range ( maxfd ) ) : if fd not in self . exclude fds : self . close fd ( fd )
def redirect ( self , stream , target ) : if target is None : target fd = os . open ( os . devnull , os . O RDWR ) else : target fd = target . fileno ( ) os . dup2 ( target fd , stream . fileno ( ) )
def is valid s3 url ( url ) : # Skip if the url start with source: (gbdxtools syntax) if url . startswith ( 'source:' ) : return True scheme , netloc , path , , , = urlparse ( url ) port except = Remote Port Validation Error ( 'Port value %s is not a valid s3 location' % url ) if len ( scheme ) < 2 : raise port except if 's3' in scheme or 's3' in netloc or 's3' in path : return True else : raise port except
def get template abs path ( filename ) : if os . path . isabs ( filename ) and os . path . isfile ( filename ) : return filename else : return os . path . join ( os . getcwd ( ) , filename )
def list ( self , s3 folder = '' , full key data = False ) : if not s3 folder . startswith ( '/' ) : s3 folder = '/' + s3 folder s3 prefix = self . prefix + s3 folder bucket data = self . client . list objects ( Bucket = self . bucket , Prefix = s3 prefix ) if full key data : return bucket data [ 'Contents' ] else : return [ k [ 'Key' ] for k in bucket data [ 'Contents' ] ]
def build worklfow json ( self ) : wf json = { 'tasks' : [ ] , 'name' : 'cloud-harness %s' % str ( uuid . uuid4 ( ) ) } task def = json . loads ( self . task template . json ( ) ) d = { "name" : task def [ 'name' ] , "outputs" : [ ] , "inputs" : [ ] , "task Type" : task def [ 'task Type' ] } # Add input ports for port in self . task template . input ports : port value = port . value if port value is False : port value = 'false' if port value is True : port value = 'true' d [ 'inputs' ] . append ( { "name" : port . name , "value" : port value } ) # Add output ports for port in self . task template . output ports : d [ 'outputs' ] . append ( { "name" : port . name } ) # Add task to workflow wf json [ 'tasks' ] . append ( d ) # Add port to be saved for port in self . task template . output ports : # Add save data locations if hasattr ( port , 'stage To S3' ) and port . stage To S3 : save location = '{customer storage}/{run name}/{port}' . format ( customer storage = self . storage . location , run name = self . task template . run name , port = port . name ) new task = dict ( * * self . STAGE TO S3 ) new task [ 'inputs' ] = [ { 'name' : 'data' , 'source' : '%s:%s' % ( task def [ 'name' ] , port . name ) } , { 'name' : 'destination' , 'value' : save location } ] wf json [ 'tasks' ] . append ( new task ) return wf json
def execute ( self , override wf json = None ) : r = self . gbdx . post ( self . URL , json = self . json if override wf json is None else override wf json ) try : r . raise for status ( ) except : print ( "GBDX API Status Code: %s" % r . status code ) print ( "GBDX API Response: %s" % r . text ) self . id = None return self . id = r . json ( ) [ 'id' ] self . refresh status ( )
def archive ( folder , dry run = False ) : # error handling on archive dir already done in main() for f in folder : if not os . path . exists ( f ) : bail ( 'folder does not exist: ' + f ) archive safe ( folder , PROJ ARCHIVE , dry run = dry run )
def mkdir ( p ) : isdir = os . path . isdir stack = [ os . path . abspath ( p ) ] while not isdir ( stack [ - 1 ] ) : parent dir = os . path . dirname ( stack [ - 1 ] ) stack . append ( parent dir ) while stack : p = stack . pop ( ) if not isdir ( p ) : os . mkdir ( p )
def list ( pattern = ( ) ) : # strategy: pick the intersection of all the patterns the user provides globs = [ '*{0}*' . format ( p ) for p in pattern ] + [ '*' ] matches = [ ] offset = len ( PROJ ARCHIVE ) + 1 for suffix in globs : glob pattern = os . path . join ( PROJ ARCHIVE , '*' , '*' , suffix ) matches . append ( set ( f [ offset : ] for f in glob . glob ( glob pattern ) ) ) matches = reduce ( lambda x , y : x . intersection ( y ) , matches ) for m in sorted ( matches ) : print ( m )
def restore ( folder ) : if os . path . isdir ( folder ) : bail ( 'a folder of the same name already exists!' ) pattern = os . path . join ( PROJ ARCHIVE , '*' , '*' , folder ) matches = glob . glob ( pattern ) if not matches : bail ( 'no project matches: ' + folder ) if len ( matches ) > 1 : print ( 'Warning: multiple matches, picking the most recent' , file = sys . stderr ) source = sorted ( matches ) [ - 1 ] print ( source , '-->' , folder ) shutil . move ( source , '.' )
def validate storage path ( cls , path , projects allowed = True ) : if not path or not isinstance ( path , str ) or path [ 0 ] != '/' or path == '/' : raise Storage Argument Exception ( 'The path must be a string, start with a slash (/), and be longer' ' than 1 character.' ) if not projects allowed and len ( [ elem for elem in path . split ( '/' ) if elem ] ) == 1 : raise Storage Argument Exception ( 'This method does not accept projects in the path.' )
def new ( cls , access token , environment = 'prod' ) : return cls ( storage client = Storage Client . new ( access token , environment = environment ) )
def emit ( self , record ) : msg = self . format ( record ) if not isinstance ( msg , dict ) : msg = json . loads ( msg ) self . collection . insert ( msg )
def sort ( self , f = lambda d : d [ "t" ] ) : list . sort ( self , key = f ) return self
def sum ( self ) : raw = self . raw ( ) s = 0 for i in range ( len ( raw ) ) : s += raw [ i ] [ "d" ] return s
def rfxcom ( device ) : # If the device isn't passed in, look for it in the config. if device is None : device = app . config . get ( 'DEVICE' ) # If the device is *still* none, error. if device is None : print ( "The serial device needs to be passed in as --device or " "set in the config as DEVICE." ) return rfxcom collect ( device )
def create user ( username ) : password = prompt pass ( "Enter password" ) user = User ( username = username , password = password ) db . session . add ( user ) db . session . commit ( )
def refresh ( self ) : self . metadata = self . db . read ( self . path ) . json ( )
def streams ( self ) : result = self . db . read ( self . path , { "q" : "ls" } ) if result is None or result . json ( ) is None : return [ ] streams = [ ] for s in result . json ( ) : strm = self [ s [ "name" ] ] strm . metadata = s streams . append ( strm ) return streams
def users ( self ) : result = self . db . read ( "" , { "q" : "ls" } ) if result is None or result . json ( ) is None : return [ ] users = [ ] for u in result . json ( ) : usr = self ( u [ "name" ] ) usr . metadata = u users . append ( usr ) return users
def connectordb ( self ) : if self . cdb is None : logging . debug ( "Logger: Connecting to " + self . serverurl ) self . cdb = Connector DB ( self . apikey , url = self . serverurl ) return self . cdb
def sync ( self ) : logging . debug ( "Logger: Syncing..." ) failed = False try : # Get the connectordb object cdb = self . connectordb # Ping the database - most connection errors will happen here cdb . ping ( ) with self . synclock : c = self . database . cursor ( ) for stream in self . streams : s = cdb [ stream ] c . execute ( "SELECT * FROM cache WHERE stream=? ORDER BY timestamp ASC;" , ( stream , ) ) datapoint Array = [ ] for dp in c . fetchall ( ) : datapoint Array . append ( { "t" : dp [ 1 ] , "d" : json . loads ( dp [ 2 ] ) } ) # First, check if the data already inserted has newer timestamps, # and in that case, assume that there was an error, and remove the datapoints # with an older timestamp, so that we don't have an error when syncing if len ( s ) > 0 : newtime = s [ - 1 ] [ "t" ] while ( len ( datapoint Array ) > 0 and datapoint Array [ 0 ] [ "t" ] < newtime ) : logging . debug ( "Datapoint exists with older timestamp. Removing the datapoint." ) datapoint Array = datapoint Array [ 1 : ] if len ( datapoint Array ) > 0 : logging . debug ( "%s: syncing %i datapoints" % ( stream , len ( datapoint Array ) ) ) while ( len ( datapoint Array ) > DATAPOINT INSERT LIMIT ) : # We insert datapoints in chunks of a couple # thousand so that they fit in the insert size # limit of Connector DB s . insert array ( datapoint Array [ : DATAPOINT INSERT LIMIT ] ) # Clear the written datapoints datapoint Array = datapoint Array [ DATAPOINT INSERT LIMIT : ] # If there was no error inserting, delete the # datapoints from the cache c . execute ( "DELETE FROM cache WHERE stream=? AND timestamp <?" , ( stream , datapoint Array [ 0 ] [ "t" ] ) ) s . insert array ( datapoint Array ) # If there was no error inserting, delete the # datapoints from the cache c . execute ( "DELETE FROM cache WHERE stream=? AND timestamp <=?" , ( stream , datapoint Array [ - 1 ] [ "t" ] ) ) self . lastsynctime = time . time ( ) if self . onsync is not None : self . onsync ( ) except Exception as e : # Handle the sync failure callback falied = True reraise = self . syncraise if self . onsyncfail is not None : reraise = self . onsyncfail ( e ) if reraise : raise
def stop ( self ) : with self . synclock : if self . syncthread is not None : self . syncthread . cancel ( ) self . syncthread = None
def read ( * paths ) : filename = os . path . join ( * paths ) with codecs . open ( filename , mode = 'r' , encoding = 'utf-8' ) as handle : return handle . read ( )
def download url job ( job , url , name = None , s3 key path = None , cghub key path = None ) : work dir = job . file Store . get Local Temp Dir ( ) fpath = download url ( job = job , url = url , work dir = work dir , name = name , s3 key path = s3 key path , cghub key path = cghub key path ) return job . file Store . write Global File ( fpath )
def s3am upload job ( job , file id , file name , s3 dir , s3 key path = None ) : work dir = job . file Store . get Local Temp Dir ( ) fpath = job . file Store . read Global File ( file id , os . path . join ( work dir , file name ) ) s3am upload ( job = job , fpath = fpath , s3 dir = s3 dir , num cores = job . cores , s3 key path = s3 key path )
def labels ( ontology , output , ols base ) : for label in get labels ( ontology = ontology , ols base = ols base ) : click . echo ( label , file = output )
def tree ( ontology , output , ols base ) : for parent , child in get hierarchy ( ontology = ontology , ols base = ols base ) : click . echo ( '{}\t{}' . format ( parent , child ) , file = output )
def get mean insert size ( work dir , bam name ) : cmd = "docker run --log-driver=none --rm -v {}:/data quay.io/ucsc cgl/samtools " "view -f66 {}" . format ( work dir , os . path . join ( work dir , bam name ) ) process = subprocess . Popen ( args = cmd , shell = True , stdout = subprocess . PIPE ) b sum = 0.0 b count = 0.0 while True : line = process . stdout . readline ( ) if not line : break tmp = line . split ( "\t" ) if abs ( long ( tmp [ 8 ] ) ) < 10000 : b sum += abs ( long ( tmp [ 8 ] ) ) b count += 1 process . wait ( ) try : mean = b sum / b count except Zero Division Error : mean = 150 print "Using insert size: %d" % mean return int ( mean )
def device ( self ) : splitted path = self . path . split ( "/" ) return Device ( self . db , splitted path [ 0 ] + "/" + splitted path [ 1 ] )
def get empty config ( self ) : self . generate config ( ) path = self . get config path ( ) with open ( path , 'r' ) as readable : contents = readable . read ( ) os . remove ( path ) return contents
def create pipeline command ( self , args , workdir path , config path ) : return ( [ self . name , 'run' , os . path . join ( workdir path , 'job Store' ) , '--config' , config path , '--work Dir' , workdir path , '--retry Count' , '1' ] + ( [ '--restart' ] if args . restart else [ ] ) )
def delete ( self , path ) : return self . handleresult ( self . r . delete ( urljoin ( self . url + CRUD PATH , path ) ) )
def subscribe ( self , stream , callback , transform = "" ) : return self . ws . subscribe ( stream , callback , transform )
def devices ( self ) : result = self . db . read ( self . path , { "q" : "ls" } ) if result is None or result . json ( ) is None : return [ ] devices = [ ] for d in result . json ( ) : dev = self [ d [ "name" ] ] dev . metadata = d devices . append ( dev ) return devices
def send ( self , cmd ) : with self . ws sendlock : self . ws . send ( json . dumps ( cmd ) )
def subscribe ( self , stream , callback , transform = "" ) : if self . status == "disconnected" or self . status == "disconnecting" or self . status == "connecting" : self . connect ( ) if self . status is not "connected" : return False logging . debug ( "Subscribing to %s" , stream ) self . send ( { "cmd" : "subscribe" , "arg" : stream , "transform" : transform } ) with self . subscription lock : self . subscriptions [ stream + ":" + transform ] = callback return True
def reconnect ( self ) : self . status = "reconnecting" # Reset the disconnect time after 15 minutes if self . disconnected time - self . connected time > 15 * 60 : self . reconnect time = self . reconnect time starting seconds else : self . reconnect time *= self . reconnect time backoff multiplier if self . reconnect time > self . reconnect time max seconds : self . reconnect time = self . reconnect time max seconds # We want to add some randomness to the reconnect rate - necessary so that we don't pound the server # if it goes down self . reconnect time *= 1 + random . uniform ( - 0.2 , 0.2 ) if self . reconnect time < self . reconnect time starting seconds : self . reconnect time = self . reconnect time starting seconds logging . warn ( "Connector DB:WS: Attempting to reconnect in %fs" , self . reconnect time ) self . reconnector = threading . Timer ( self . reconnect time , self . reconnect fnc ) self . reconnector . daemon = True self . reconnector . start ( )
def on open ( self , ws ) : logging . debug ( "Connector DB: Websocket opened" ) # Connection success - decrease the wait time for next connection self . reconnect time /= self . reconnect time backoff multiplier self . status = "connected" self . lastpingtime = time . time ( ) self . ensure ping ( ) self . connected time = time . time ( ) # Release the lock that connect called self . ws openlock . release ( )
def on close ( self , ws ) : if self . status == "disconnected" : return # This can be double-called on disconnect logging . debug ( "Connector DB:WS: Websocket closed" ) # Turn off the ping timer if self . pingtimer is not None : self . pingtimer . cancel ( ) self . disconnected time = time . time ( ) if self . status == "disconnecting" : self . status = "disconnected" elif self . status == "connected" : self . reconnect ( )
def on error ( self , ws , err ) : logging . debug ( "Connector DB:WS: Connection Error" ) if self . status == "connecting" : self . status = "errored" self . ws openlock . release ( )
def on message ( self , ws , msg ) : msg = json . loads ( msg ) logging . debug ( "Connector DB:WS: Msg '%s'" , msg [ "stream" ] ) # Build the subcription key stream key = msg [ "stream" ] + ":" if "transform" in msg : stream key += msg [ "transform" ] self . subscription lock . acquire ( ) if stream key in self . subscriptions : subscription function = self . subscriptions [ stream key ] self . subscription lock . release ( ) fresult = subscription function ( msg [ "stream" ] , msg [ "data" ] ) if fresult is True : # This is a special result - if the subscription function of a downlink returns True, # then the datapoint is acknowledged automatically (ie, reinserted in non-downlink stream) fresult = msg [ "data" ] if fresult is not False and fresult is not None and msg [ "stream" ] . endswith ( "/downlink" ) and msg [ "stream" ] . count ( "/" ) == 3 : # If the above conditions are true, it means that the datapoints were from a downlink, # and the subscriber function chooses to acknowledge them, so we reinsert them. self . insert ( msg [ "stream" ] [ : - 9 ] , fresult ) else : self . subscription lock . release ( ) logging . warn ( "Connector DB:WS: Msg '%s' not subscribed! Subscriptions: %s" , msg [ "stream" ] , list ( self . subscriptions . keys ( ) ) )
def write config ( configuration ) : with open ( CONFIG PATH , 'w' ) as f : json . dump ( configuration , f , indent = 2 , sort keys = True )
def check ( self ) : status = check Container Status ( self . spark Container ID , self . hdfs Container ID , spark Noun = 'worker' , hdfs Noun = 'datanode' ) return status
def base tokenizer ( fp ) : if isinstance ( fp , String IO ) : template file = fp size = template file . len else : #empty file check if os . fstat ( fp . fileno ( ) ) . st size == 0 : yield TOKEN EOF , 'EOF' , 0 , 0 return template file = mmap . mmap ( fp . fileno ( ) , 0 , access = mmap . ACCESS READ ) size = template file . size ( ) lineno = 0 while 1 : lineno += 1 pos = 1 # end of file if template file . tell ( ) == size : yield TOKEN EOF , 'EOF' , lineno , 0 break # now we tokinize line by line line = template file . readline ( ) . decode ( 'utf-8' ) line = line . replace ( '\r\n' , '' ) line = line . replace ( '\n' , '' ) # ignoring non XML comments if re comment . match ( line ) : continue last text = deque ( ) while line : line len = len ( line ) for token in tokens : m = token . regex . match ( line ) if m : if last text : yield TOKEN TEXT , '' . join ( last text ) , lineno , pos pos += len ( last text ) last text . clear ( ) offset , value = m . end ( ) , m . group ( ) line = line [ offset : ] yield token , value , lineno , pos pos += offset break # we did not get right in tokens list, so next char is text if line len == len ( line ) : last text . append ( line [ 0 ] ) line = line [ 1 : ] if last text : yield TOKEN TEXT , '' . join ( last text ) , lineno , pos pos += len ( last text ) last text . clear ( ) yield TOKEN NEWLINE , '\n' , lineno , pos # all work is done template file . close ( )
def fitness ( self ) : if len ( self . members ) != 0 : if self . num processes > 1 : members = [ m . get ( ) for m in self . members ] else : members = self . members return sum ( m . fitness score for m in members ) / len ( members ) else : return None
def ave cost fn val ( self ) : if len ( self . members ) != 0 : if self . num processes > 1 : members = [ m . get ( ) for m in self . members ] else : members = self . members return sum ( m . cost fn val for m in members ) / len ( members ) else : return None
def med cost fn val ( self ) : if len ( self . members ) != 0 : if self . num processes > 1 : members = [ m . get ( ) for m in self . members ] else : members = self . members return median ( [ m . cost fn val for m in members ] ) else : return None
def parameters ( self ) : if len ( self . members ) != 0 : if self . num processes > 1 : members = [ m . get ( ) for m in self . members ] else : members = self . members params = { } for p in self . parameters : params [ p . name ] = sum ( m . parameters [ p . name ] for m in members ) / len ( members ) return params else : return None
def members ( self ) : if self . num processes > 1 : return [ m . get ( ) for m in self . members ] else : return self . members
def get environ vars ( self ) : for key , val in os . environ . items ( ) : if environ prefix re . search ( key ) : yield ( environ prefix re . sub ( "" , key ) . lower ( ) , val )
def transform result ( typ , result ) : if issubclass ( typ , bytes ) : return tostring ( result , encoding = 'utf-8' ) elif issubclass ( typ , unicode ) : return tostring ( result , encoding = 'unicode' ) else : return result
def is single class ( ) : ret = False counts = get counts ( ) if counts [ "classes" ] < 1 and counts [ "modules" ] < 1 : ret = counts [ "tests" ] > 0 else : ret = counts [ "classes" ] <= 1 and counts [ "modules" ] <= 1 return ret
def is single module ( ) : ret = False counts = get counts ( ) if counts [ "modules" ] == 1 : ret = True elif counts [ "modules" ] < 1 : ret = is single class ( ) return ret
def validate params ( request ) : if 'params' in request : correct params = isinstance ( request [ 'params' ] , ( list , dict ) ) error = 'Incorrect parameter values' assert correct params , error
def validate id ( request ) : if 'id' in request : correct id = isinstance ( request [ 'id' ] , ( string types , int , None ) , ) error = 'Incorrect identifier' assert correct id , error
def escape argspec ( obj , iterable , escape ) : for key , value in iterable : if hasattr ( value , ' html ' ) or isinstance ( value , string types ) : obj [ key ] = escape ( value ) return obj
def sub symbols ( pattern , code , symbol ) : return pattern . replace ( '¤¤',  c de). r e place(' ¤ ', s y bol)
def amount converter ( obj ) : if isinstance ( obj , Decimal ) : return obj elif isinstance ( obj , ( str , int , float ) ) : return Decimal ( str ( obj ) ) else : raise Value Error ( 'do not know how to convert: {}' . format ( type ( obj ) ) )
def exception ( self ) : buf = traceback . format exception only ( self . exc type , self . exc value ) rv = '' . join ( buf ) . strip ( ) return rv . decode ( 'utf-8' , 'replace' ) if PY2 else rv
def render summary ( self , include title = True ) : title = '' frames = [ ] classes = [ 'traceback' ] if not self . frames : classes . append ( 'noframe-traceback' ) if include title : if self . is syntax error : title = u'Syntax Error' else : title = u'Traceback <em>(most recent call last)</em>:' for frame in self . frames : frames . append ( u'<li%s>%s' % ( frame . info and u' title="%s"' % escape ( frame . info ) or u'' , frame . render ( ) ) ) if self . is syntax error : description wrapper = u'<pre class=syntaxerror>%s</pre>' else : description wrapper = u'<blockquote>%s</blockquote>' return SUMMARY HTML % { 'classes' : u' ' . join ( classes ) , 'title' : title and u'<h3>%s</h3>' % title or u'' , 'frames' : u'\n' . join ( frames ) , 'description' : description wrapper % escape ( self . exception ) }
def generate plaintext traceback ( self ) : yield u'Traceback (most recent call last):' for frame in self . frames : yield u'  File "%s", line %s, in %s' % ( frame . filename , frame . lineno , frame . function name ) yield u'    ' + frame . current line . strip ( ) yield self . exception
def get annotated lines ( self ) : lines = [ Line ( idx + 1 , x ) for idx , x in enumerate ( self . sourcelines ) ] # find function definition and mark lines if hasattr ( self . code , 'co firstlineno' ) : lineno = self . code . co firstlineno - 1 while lineno > 0 : if funcdef re . match ( lines [ lineno ] . code ) : break lineno -= 1 try : offset = len ( inspect . getblock ( [ x . code + '\n' for x in lines [ lineno : ] ] ) ) except Token Error : offset = 0 for line in lines [ lineno : lineno + offset ] : line . in frame = True # mark current line try : lines [ self . lineno - 1 ] . current = True except Index Error : pass return lines
def render source ( self ) : return SOURCE TABLE HTML % u'\n' . join ( line . render ( ) for line in self . get annotated lines ( ) )
def link package versions ( self , link , search ) : platform = get platform ( ) version = None if link . egg fragment : egg info = link . egg fragment ext = link . ext else : egg info , ext = link . splitext ( ) if not ext : self . log skipped link ( link , 'not a file' ) return if ext not in SUPPORTED EXTENSIONS : self . log skipped link ( link , 'unsupported archive format: %s' % ext ) return if "binary" not in search . formats and ext == wheel ext : self . log skipped link ( link , 'No binaries permitted for %s' % search . supplied ) return if "macosx10" in link . path and ext == '.zip' : self . log skipped link ( link , 'macosx10 one' ) return if ext == wheel ext : try : wheel = Wheel ( link . filename ) except Invalid Wheel Filename : self . log skipped link ( link , 'invalid wheel filename' ) return if ( pkg resources . safe name ( wheel . name ) . lower ( ) != search . canonical ) : self . log skipped link ( link , 'wrong project name (not %s)' % search . supplied ) return if not wheel . supported ( ) : self . log skipped link ( link , 'it is not compatible with this Python' ) return # This is a dirty hack to prevent installing Binary Wheels from # Py PI unless it is a Windows or Mac Binary Wheel. This is # paired with a change to Py PI disabling uploads for the # same. Once we have a mechanism for enabling support for # binary wheels on linux that deals with the inherent problems # of binary distribution this can be removed. comes from = getattr ( link , "comes from" , None ) if ( ( not platform . startswith ( 'win' ) and not platform . startswith ( 'macosx' ) and not platform == 'cli' ) and comes from is not None and urllib parse . urlparse ( comes from . url ) . netloc . endswith ( Py PI . netloc ) ) : if not wheel . supported ( tags = supported tags noarch ) : self . log skipped link ( link , "it is a pypi-hosted binary " "Wheel on an unsupported platform" , ) return version = wheel . version # This should be up by the search.ok binary check, but see issue 2700. if "source" not in search . formats and ext != wheel ext : self . log skipped link ( link , 'No sources permitted for %s' % search . supplied ) return if not version : version = egg info matches ( egg info , search . supplied , link ) if version is None : self . log skipped link ( link , 'wrong project name (not %s)' % search . supplied ) return if ( link . internal is not None and not link . internal and not normalize name ( search . supplied ) . lower ( ) in self . allow external and not self . allow all external ) : # We have a link that we are sure is external, so we should skip #   it unless we are allowing externals self . log skipped link ( link , 'it is externally hosted' ) self . need warn external = True return if ( link . verifiable is not None and not link . verifiable and not ( normalize name ( search . supplied ) . lower ( ) in self . allow unverified ) ) : # We have a link that we are sure we cannot verify its integrity, #   so we should skip it unless we are allowing unsafe installs #   for this requirement. self . log skipped link ( link , 'it is an insecure and unverifiable file' ) self . need warn unverified = True return match = self . py version re . search ( version ) if match : version = version [ : match . start ( ) ] py version = match . group ( 1 ) if py version != sys . version [ : 3 ] : self . log skipped link ( link , 'Python version is incorrect' ) return logger . debug ( 'Found link %s, version: %s' , link , version ) return Installation Candidate ( search . supplied , version , link )
def get content type ( url , session ) : scheme , netloc , path , query , fragment = urllib parse . urlsplit ( url ) if scheme not in ( 'http' , 'https' ) : # FIXME: some warning or something? # assertion error? return '' resp = session . head ( url , allow redirects = True ) resp . raise for status ( ) return resp . headers . get ( "Content-Type" , "" )
def links ( self ) : for anchor in self . parsed . findall ( ".//a" ) : if anchor . get ( "href" ) : href = anchor . get ( "href" ) url = self . clean link ( urllib parse . urljoin ( self . base url , href ) ) # Determine if this link is internal. If that distinction #   doesn't make sense in this context, then we don't make #   any distinction. internal = None if self . api version and self . api version >= 2 : # Only api versions >= 2 have a distinction between #   external and internal links internal = bool ( anchor . get ( "rel" ) and "internal" in anchor . get ( "rel" ) . split ( ) ) yield Link ( url , self , internal = internal )
def find data files ( self , package , src dir ) : globs = ( self . package data . get ( '' , [ ] ) + self . package data . get ( package , [ ] ) ) files = self . manifest files . get ( package , [ ] ) [ : ] for pattern in globs : # Each pattern has to be converted to a platform-specific path files . extend ( glob ( os . path . join ( src dir , convert path ( pattern ) ) ) ) return self . exclude data files ( package , src dir , files )
def check package ( self , package , package dir ) : try : return self . packages checked [ package ] except Key Error : pass init py = orig . build py . check package ( self , package , package dir ) self . packages checked [ package ] = init py if not init py or not self . distribution . namespace packages : return init py for pkg in self . distribution . namespace packages : if pkg == package or pkg . startswith ( package + '.' ) : break else : return init py f = open ( init py , 'rb U' ) if 'declare namespace' . encode ( ) not in f . read ( ) : from distutils . errors import Distutils Error raise Distutils Error ( "Namespace package problem: %s is a namespace package, but " "its\n init .py does not call declare namespace()! Please " 'fix it.\n(See the setuptools manual under ' '"Namespace Packages" for details.)\n"' % ( package , ) ) f . close ( ) return init py
def exclude data files ( self , package , src dir , files ) : globs = ( self . exclude package data . get ( '' , [ ] ) + self . exclude package data . get ( package , [ ] ) ) bad = [ ] for pattern in globs : bad . extend ( fnmatch . filter ( files , os . path . join ( src dir , convert path ( pattern ) ) ) ) bad = dict . fromkeys ( bad ) seen = { } return [ f for f in files if f not in bad and f not in seen and seen . setdefault ( f , 1 ) # ditch dupes ]
def ignore comments ( iterator ) : for line in iterator : line = COMMENT RE . sub ( '' , line ) line = line . strip ( ) if line : yield line
def skip regex ( lines , options ) : skip regex = options . skip requirements regex if options else None if skip regex : lines = filterfalse ( re . compile ( skip regex ) . search , lines ) return lines
def compile ( marker ) : try : return cache [ marker ] except Key Error : pass if not marker . strip ( ) : def marker fn ( environment = None , override = None ) : """""" return True else : compiled marker = compile marker ( parse marker ( marker ) ) def marker fn ( environment = None , override = None ) : """override updates environment""" if override is None : override = { } if environment is None : environment = default environment ( ) environment . update ( override ) return eval ( compiled marker , environment ) marker fn . doc = marker cache [ marker ] = marker fn return cache [ marker ]
def visit ( self , node ) : if not isinstance ( node , self . ALLOWED ) : raise Syntax Error ( 'Not allowed in environment markers.\n%s\n%s' % ( self . statement , ( ' ' * node . col offset ) + '^' ) ) return ast . Node Transformer . visit ( self , node )
def visit Attribute ( self , node ) : new node = ast . Name ( "%s.%s" % ( node . value . id , node . attr ) , node . ctx ) return ast . copy location ( new node , node )
def push ( self ) : self . refcnt += 1 app ctx stack . push ( self ) appcontext pushed . send ( self . app )
def pop ( self , exc = None ) : self . refcnt -= 1 if self . refcnt <= 0 : if exc is None : exc = sys . exc info ( ) [ 1 ] self . app . do teardown appcontext ( exc ) rv = app ctx stack . pop ( ) assert rv is self , 'Popped wrong app context.  (%r instead of %r)' % ( rv , self ) appcontext popped . send ( self . app )
def push ( self ) : # If an exception occurs in debug mode or if context preservation is # activated under exception situations exactly one context stays # on the stack.  The rationale is that you want to access that # information under debug situations.  However if someone forgets to # pop that context again we want to make sure that on the next push # it's invalidated, otherwise we run at risk that something leaks # memory.  This is usually only a problem in testsuite since this # functionality is not active in production environments. top = request ctx stack . top if top is not None and top . preserved : top . pop ( top . preserved exc ) # Before we push the request context we have to ensure that there # is an application context. app ctx = app ctx stack . top if app ctx is None or app ctx . app != self . app : app ctx = self . app . app context ( ) app ctx . push ( ) self . implicit app ctx stack . append ( app ctx ) else : self . implicit app ctx stack . append ( None ) request ctx stack . push ( self ) # Open the session at the moment that the request context is # available. This allows a custom open session method to use the # request context (e.g. code that access database information # stored on `g` instead of the appcontext). self . session = self . app . open session ( self . request ) if self . session is None : self . session = self . app . make null session ( )
def dist in usersite ( dist ) : norm path = normalize path ( dist location ( dist ) ) return norm path . startswith ( normalize path ( user site ) )
def dist is editable ( dist ) : # TODO: factor out determining editableness out of Frozen Requirement from pip import Frozen Requirement req = Frozen Requirement . from dist ( dist , [ ] ) return req . editable
def run ( self , options , args ) : shells = COMPLETION SCRIPTS . keys ( ) shell options = [ '--' + shell for shell in sorted ( shells ) ] if options . shell in shells : script = COMPLETION SCRIPTS . get ( options . shell , '' ) print ( BASE COMPLETION % { 'script' : script , 'shell' : options . shell } ) else : sys . stderr . write ( 'ERROR: You must pass %s\n' % ' or ' . join ( shell options ) )
def root is purelib ( name , wheeldir ) : name folded = name . replace ( "-" , " " ) for item in os . listdir ( wheeldir ) : match = dist info re . match ( item ) if match and match . group ( 'name' ) == name folded : with open ( os . path . join ( wheeldir , item , 'WHEEL' ) ) as wheel : for line in wheel : line = line . lower ( ) . rstrip ( ) if line == "root-is-purelib: true" : return True return False
def iter symbols ( code ) : for name in code . co names : yield name for const in code . co consts : if isinstance ( const , basestring ) : yield const elif isinstance ( const , Code Type ) : for name in iter symbols ( const ) : yield name
def ensure fresh rates ( func ) : def wrapper ( self , * args , * * kwargs ) : if self . last updated + timedelta ( minutes = 5 ) < zulu . now ( ) : self . refresh ( ) return func ( self , * args , * * kwargs ) return wrapper
def write delete marker file ( directory ) : filepath = os . path . join ( directory , PIP DELETE MARKER FILENAME ) with open ( filepath , 'w' ) as marker fp : marker fp . write ( DELETE MARKER MESSAGE )
def running under virtualenv ( ) : if hasattr ( sys , 'real prefix' ) : return True elif sys . prefix != getattr ( sys , "base prefix" , sys . prefix ) : return True return False
def get username ( ) : if WINDOWS : return getpass . getuser ( ) import pwd return pwd . getpwuid ( os . geteuid ( ) ) . pw name
def distutils scheme ( dist name , user = False , home = None , root = None , isolated = False ) : from distutils . dist import Distribution scheme = { } if isolated : extra dist args = { "script args" : [ "--no-user-cfg" ] } else : extra dist args = { } dist args = { 'name' : dist name } dist args . update ( extra dist args ) d = Distribution ( dist args ) d . parse config files ( ) i = d . get command obj ( 'install' , create = True ) # NOTE: setting user or home has the side-effect of creating the home dir # or user base for installations during finalize options() # ideally, we'd prefer a scheme class that has no side-effects. i . user = user or i . user i . home = home or i . home i . root = root or i . root i . finalize options ( ) for key in SCHEME KEYS : scheme [ key ] = getattr ( i , 'install ' + key ) if i . install lib is not None : # install lib takes precedence over purelib and platlib scheme . update ( dict ( purelib = i . install lib , platlib = i . install lib ) ) if running under virtualenv ( ) : scheme [ 'headers' ] = os . path . join ( sys . prefix , 'include' , 'site' , 'python' + sys . version [ : 3 ] , dist name , ) if root is not None : scheme [ "headers" ] = os . path . join ( root , os . path . abspath ( scheme [ "headers" ] ) [ 1 : ] , ) return scheme
def install script ( self , dist , script name , script text , dev path = None ) : spec = str ( dist . as requirement ( ) ) is script = is python script ( script text , script name ) if is script : script text = ( Script Writer . get header ( script text ) + self . load template ( dev path ) % locals ( ) ) self . write script ( script name , to ascii ( script text ) , 'b' )
def install site py ( self ) : if self . sitepy installed : return # already did it, or don't need to sitepy = os . path . join ( self . install dir , "site.py" ) source = resource string ( "setuptools" , "site-patch.py" ) current = "" if os . path . exists ( sitepy ) : log . debug ( "Checking existing site.py in %s" , self . install dir ) f = open ( sitepy , 'rb' ) current = f . read ( ) # we want str, not bytes if PY3 : current = current . decode ( ) f . close ( ) if not current . startswith ( 'def  boot():' ) : raise Distutils Error ( "%s is not a setuptools-generated site.py; please" " remove it." % sitepy ) if current != source : log . info ( "Creating %s" , sitepy ) if not self . dry run : ensure directory ( sitepy ) f = open ( sitepy , 'wb' ) f . write ( source ) f . close ( ) self . byte compile ( [ sitepy ] ) self . sitepy installed = True
def save ( self ) : if not self . dirty : return data = '\n' . join ( map ( self . make relative , self . paths ) ) if data : log . debug ( "Saving %s" , self . filename ) data = ( "import sys; sys. plen = len(sys.path)\n" "%s\n" "import sys; new=sys.path[sys. plen:];" " del sys.path[sys. plen:];" " p=getattr(sys,' egginsert',0); sys.path[p:p]=new;" " sys. egginsert = p+len(new)\n" ) % data if os . path . islink ( self . filename ) : os . unlink ( self . filename ) f = open ( self . filename , 'wt' ) f . write ( data ) f . close ( ) elif os . path . exists ( self . filename ) : log . debug ( "Deleting empty %s" , self . filename ) os . unlink ( self . filename ) self . dirty = False
def add filters ( self , filterer , filters ) : for f in filters : try : filterer . add Filter ( self . config [ 'filters' ] [ f ] ) except Standard Error as e : raise Value Error ( 'Unable to add filter %r: %s' % ( f , e ) )
def configure handler ( self , config ) : formatter = config . pop ( 'formatter' , None ) if formatter : try : formatter = self . config [ 'formatters' ] [ formatter ] except Standard Error as e : raise Value Error ( 'Unable to set formatter ' '%r: %s' % ( formatter , e ) ) level = config . pop ( 'level' , None ) filters = config . pop ( 'filters' , None ) if '()' in config : c = config . pop ( '()' ) if not hasattr ( c , ' call ' ) and hasattr ( types , 'Class Type' ) and type ( c ) != types . Class Type : c = self . resolve ( c ) factory = c else : klass = self . resolve ( config . pop ( 'class' ) ) # Special case for handler which refers to another handler if issubclass ( klass , logging . handlers . Memory Handler ) and 'target' in config : try : config [ 'target' ] = self . config [ 'handlers' ] [ config [ 'target' ] ] except Standard Error as e : raise Value Error ( 'Unable to set target handler ' '%r: %s' % ( config [ 'target' ] , e ) ) elif issubclass ( klass , logging . handlers . SMTP Handler ) and 'mailhost' in config : config [ 'mailhost' ] = self . as tuple ( config [ 'mailhost' ] ) elif issubclass ( klass , logging . handlers . Sys Log Handler ) and 'address' in config : config [ 'address' ] = self . as tuple ( config [ 'address' ] ) factory = klass kwargs = dict ( ( k , config [ k ] ) for k in config if valid ident ( k ) ) try : result = factory ( * * kwargs ) except Type Error as te : if "'stream'" not in str ( te ) : raise # The argument name changed from strm to stream # Retry with old name. # This is so that code can be used with older Python versions #(e.g. by Django) kwargs [ 'strm' ] = kwargs . pop ( 'stream' ) result = factory ( * * kwargs ) if formatter : result . set Formatter ( formatter ) if level is not None : result . set Level ( check Level ( level ) ) if filters : self . add filters ( result , filters ) return result
def add handlers ( self , logger , handlers ) : for h in handlers : try : logger . add Handler ( self . config [ 'handlers' ] [ h ] ) except Standard Error as e : raise Value Error ( 'Unable to add handler %r: %s' % ( h , e ) )
def common logger config ( self , logger , config , incremental = False ) : level = config . get ( 'level' , None ) if level is not None : logger . set Level ( check Level ( level ) ) if not incremental : # Remove any existing handlers for h in logger . handlers [ : ] : logger . remove Handler ( h ) handlers = config . get ( 'handlers' , None ) if handlers : self . add handlers ( logger , handlers ) filters = config . get ( 'filters' , None ) if filters : self . add filters ( logger , filters )
def execfile ( filename , globals , locals = None ) : mode = 'rb' with open ( filename , mode ) as stream : script = stream . read ( ) # compile() function in Python 2.6 and 3.1 requires LF line endings. if sys . version info [ : 2 ] < ( 2 , 7 ) or sys . version info [ : 2 ] >= ( 3 , 0 ) and sys . version info [ : 2 ] < ( 3 , 2 ) : script = script . replace ( b'\r\n' , b'\n' ) script = script . replace ( b'\r' , b'\n' ) if locals is None : locals = globals code = compile ( script , filename , 'exec' ) exec ( code , globals , locals )
def override temp ( replacement ) : if not os . path . isdir ( replacement ) : os . makedirs ( replacement ) saved = tempfile . tempdir tempfile . tempdir = replacement try : yield finally : tempfile . tempdir = saved
def run setup ( setup script , args ) : setup dir = os . path . abspath ( os . path . dirname ( setup script ) ) with setup context ( setup dir ) : try : sys . argv [ : ] = [ setup script ] + list ( args ) sys . path . insert ( 0 , setup dir ) # reset to include setup dir, w/clean callback list working set . init ( ) working set . callbacks . append ( lambda dist : dist . activate ( ) ) def runner ( ) : ns = dict ( file = setup script , name = ' main ' ) execfile ( setup script , ns ) Directory Sandbox ( setup dir ) . run ( runner ) except System Exit as v : if v . args and v . args [ 0 ] : raise
def getitem ( self , obj , argument ) : try : return obj [ argument ] except ( Type Error , Lookup Error ) : if isinstance ( argument , string types ) : try : attr = str ( argument ) except Exception : pass else : try : return getattr ( obj , attr ) except Attribute Error : pass return self . undefined ( obj = obj , name = argument )
def find eggs in zip ( importer , path item , only = False ) : if importer . archive . endswith ( '.whl' ) : # wheels are not supported with this finder # they don't have PKG-INFO metadata, and won't ever contain eggs return metadata = Egg Metadata ( importer ) if metadata . has metadata ( 'PKG-INFO' ) : yield Distribution . from filename ( path item , metadata = metadata ) if only : # don't yield nested distros return for subitem in metadata . resource listdir ( '/' ) : if subitem . endswith ( '.egg' ) : subpath = os . path . join ( path item , subitem ) for dist in find eggs in zip ( zipimport . zipimporter ( subpath ) , subpath ) : yield dist
def find on path ( importer , path item , only = False ) : path item = normalize cached ( path item ) if os . path . isdir ( path item ) and os . access ( path item , os . R OK ) : if path item . lower ( ) . endswith ( '.egg' ) : # unpacked egg yield Distribution . from filename ( path item , metadata = Path Metadata ( path item , os . path . join ( path item , 'EGG-INFO' ) ) ) else : # scan for .egg and .egg-info in directory for entry in os . listdir ( path item ) : lower = entry . lower ( ) if lower . endswith ( '.egg-info' ) or lower . endswith ( '.dist-info' ) : fullpath = os . path . join ( path item , entry ) if os . path . isdir ( fullpath ) : # egg-info directory, allow getting metadata metadata = Path Metadata ( path item , fullpath ) else : metadata = File Metadata ( fullpath ) yield Distribution . from location ( path item , entry , metadata , precedence = DEVELOP DIST ) elif not only and lower . endswith ( '.egg' ) : dists = find distributions ( os . path . join ( path item , entry ) ) for dist in dists : yield dist elif not only and lower . endswith ( '.egg-link' ) : with open ( os . path . join ( path item , entry ) ) as entry file : entry lines = entry file . readlines ( ) for line in entry lines : if not line . strip ( ) : continue path = os . path . join ( path item , line . rstrip ( ) ) dists = find distributions ( path ) for item in dists : yield item break
def declare namespace ( package Name ) : imp . acquire lock ( ) try : if package Name in namespace packages : return path , parent = sys . path , None if '.' in package Name : parent = '.' . join ( package Name . split ( '.' ) [ : - 1 ] ) declare namespace ( parent ) if parent not in namespace packages : import ( parent ) try : path = sys . modules [ parent ] . path except Attribute Error : raise Type Error ( "Not a package:" , parent ) # Track what packages are namespaces, so when new path items are added, # they can be updated namespace packages . setdefault ( parent , [ ] ) . append ( package Name ) namespace packages . setdefault ( package Name , [ ] ) for path item in path : # Ensure all the parent's path items are reflected in the child, # if they apply handle ns ( package Name , path item ) finally : imp . release lock ( )
def get mro ( cls ) : if not isinstance ( cls , type ) : class cls ( cls , object ) : pass return cls . mro [ 1 : ] return cls . mro
def find adapter ( registry , ob ) : for t in get mro ( getattr ( ob , ' class ' , type ( ob ) ) ) : if t in registry : return registry [ t ]
def ensure directory ( path ) : dirname = os . path . dirname ( path ) if not os . path . isdir ( dirname ) : os . makedirs ( dirname )
def insert on ( self , path , loc = None ) : loc = loc or self . location if not loc : return nloc = normalize cached ( loc ) bdir = os . path . dirname ( nloc ) npath = [ ( p and normalize cached ( p ) or p ) for p in path ] for p , item in enumerate ( npath ) : if item == nloc : break elif item == bdir and self . precedence == EGG DIST : # if it's an .egg, give it precedence over its directory if path is sys . path : self . check version conflict ( ) path . insert ( p , loc ) npath . insert ( p , nloc ) break else : if path is sys . path : self . check version conflict ( ) path . append ( loc ) return # p is the spot where we found or inserted loc; now remove duplicates while True : try : np = npath . index ( nloc , p + 1 ) except Value Error : break else : del npath [ np ] , path [ np ] # ha! p = np return
def parse pattern ( pattern ) : if isinstance ( pattern , Number Pattern ) : return pattern def match number ( pattern ) : rv = number re . search ( pattern ) if rv is None : raise Value Error ( 'Invalid number pattern %r' % pattern ) return rv . groups ( ) pos pattern = pattern # Do we have a negative subpattern? if ';' in pattern : pos pattern , neg pattern = pattern . split ( ';' , 1 ) pos prefix , number , pos suffix = match number ( pos pattern ) neg prefix , , neg suffix = match number ( neg pattern ) else : pos prefix , number , pos suffix = match number ( pos pattern ) neg prefix = '-' + pos prefix neg suffix = pos suffix if 'E' in number : number , exp = number . split ( 'E' , 1 ) else : exp = None if '@' in number : if '.' in number and '0' in number : raise Value Error ( 'Significant digit patterns can not contain ' '"@" or "0"' ) if '.' in number : integer , fraction = number . rsplit ( '.' , 1 ) else : integer = number fraction = '' def parse precision ( p ) : """Calculate the min and max allowed digits""" min = max = 0 for c in p : if c in '@0' : min += 1 max += 1 elif c == '#' : max += 1 elif c == ',' : continue else : break return min , max int prec = parse precision ( integer ) frac prec = parse precision ( fraction ) if exp : exp plus = exp . startswith ( '+' ) exp = exp . lstrip ( '+' ) exp prec = parse precision ( exp ) else : exp plus = None exp prec = None grouping = babel . numbers . parse grouping ( integer ) return Number Pattern ( pattern , ( pos prefix , neg prefix ) , ( pos suffix , neg suffix ) , grouping , int prec , frac prec , exp prec , exp plus )
def get decimal quantum ( precision ) : assert isinstance ( precision , ( int , decimal . Decimal ) ) return decimal . Decimal ( 10 ) ** ( - precision )
def scientific notation elements ( self , value , locale ) : # Normalize value to only have one lead digit. exp = value . adjusted ( ) value = value * get decimal quantum ( exp ) assert value . adjusted ( ) == 0 # Shift exponent and value by the minimum number of leading digits # imposed by the rendering pattern. And always make that number # greater or equal to 1. lead shift = max ( [ 1 , min ( self . int prec ) ] ) - 1 exp = exp - lead shift value = value * get decimal quantum ( - lead shift ) # Get exponent sign symbol. exp sign = '' if exp < 0 : exp sign = babel . numbers . get minus sign symbol ( locale ) elif self . exp plus : exp sign = babel . numbers . get plus sign symbol ( locale ) # Normalize exponent value now that we have the sign. exp = abs ( exp ) return value , exp , exp sign
def total seconds ( td ) : if hasattr ( td , 'total seconds' ) : return td . total seconds ( ) ms = td . microseconds secs = ( td . seconds + td . days * 24 * 3600 ) return ( ms + secs * 10 ** 6 ) / 10 ** 6
def check extras ( dist , attr , value ) : try : for k , v in value . items ( ) : if ':' in k : k , m = k . split ( ':' , 1 ) if pkg resources . invalid marker ( m ) : raise Distutils Setup Error ( "Invalid environment marker: " + m ) list ( pkg resources . parse requirements ( v ) ) except ( Type Error , Value Error , Attribute Error ) : raise Distutils Setup Error ( "'extras require' must be a dictionary whose values are " "strings or lists of strings containing valid project/version " "requirement specifiers." )
def check requirements ( dist , attr , value ) : try : list ( pkg resources . parse requirements ( value ) ) except ( Type Error , Value Error ) as error : tmpl = ( "{attr!r} must be a string or list of strings " "containing valid project/version requirement specifiers; {error}" ) raise Distutils Setup Error ( tmpl . format ( attr = attr , error = error ) )
def fetch build egg ( self , req ) : try : cmd = self . egg fetcher cmd . package index . to scan = [ ] except Attribute Error : from setuptools . command . easy install import easy install dist = self . class ( { 'script args' : [ 'easy install' ] } ) dist . parse config files ( ) opts = dist . get option dict ( 'easy install' ) keep = ( 'find links' , 'site dirs' , 'index url' , 'optimize' , 'site dirs' , 'allow hosts' ) for key in list ( opts ) : if key not in keep : del opts [ key ] # don't use any other settings if self . dependency links : links = self . dependency links [ : ] if 'find links' in opts : links = opts [ 'find links' ] [ 1 ] . split ( ) + links opts [ 'find links' ] = ( 'setup' , links ) install dir = self . get egg cache dir ( ) cmd = easy install ( dist , args = [ "x" ] , install dir = install dir , exclude scripts = True , always copy = False , build directory = None , editable = False , upgrade = False , multi version = True , no report = True , user = False ) cmd . ensure finalized ( ) self . egg fetcher = cmd return cmd . easy install ( req )
def do dice roll ( ) : options = get options ( ) dice = Dice ( options . sides ) rolls = [ dice . roll ( ) for n in range ( options . number ) ] for roll in rolls : print ( 'rolled' , roll ) if options . number > 1 : print ( 'total' , sum ( rolls ) )
def price converter ( obj ) : if isinstance ( obj , str ) : obj = Price Class . parse ( obj ) return obj
def get method ( self , args ) : try : method = self . app [ args [ 'method' ] ] except Key Error : method not found ( args [ 'id' ] ) else : return method
def apply ( self , method , args ) : try : params = args [ 'params' ] if isinstance ( params , dict ) : result = method ( * * params ) else : result = method ( * params ) except Exception as error : server error ( args [ 'id' ] , error ) else : return result
def blueprint ( self ) : if self . url rule and '.' in self . url rule . endpoint : return self . url rule . endpoint . rsplit ( '.' , 1 ) [ 0 ]
def cleanup files ( self ) : logger . debug ( 'Cleaning up...' ) with indent log ( ) : for req in self . reqs to cleanup : req . remove temporary source ( )
def get all ns packages ( self ) : nsp = set ( ) for pkg in self . distribution . namespace packages or [ ] : pkg = pkg . split ( '.' ) while pkg : nsp . add ( '.' . join ( pkg ) ) pkg . pop ( ) return sorted ( nsp )
def default ( self , obj ) : if isinstance ( obj , models . Model ) : return self . encode ( model to dict ( obj ) ) elif isinstance ( obj , models . query . Query Set ) : return serializers . serialize ( 'json' , obj ) else : return super ( Json Response Encoder , self ) . default ( obj )
def tokenize annotated ( doc , annotation ) : tokens = tokenize ( doc , include hrefs = False ) for tok in tokens : tok . annotation = annotation return tokens
def copy annotations ( src , dest ) : assert len ( src ) == len ( dest ) for src tok , dest tok in zip ( src , dest ) : dest tok . annotation = src tok . annotation
def fixup chunks ( chunks ) : tag accum = [ ] cur word = None result = [ ] for chunk in chunks : if isinstance ( chunk , tuple ) : if chunk [ 0 ] == 'img' : src = chunk [ 1 ] tag , trailing whitespace = split trailing whitespace ( chunk [ 2 ] ) cur word = tag token ( 'img' , src , html repr = tag , pre tags = tag accum , trailing whitespace = trailing whitespace ) tag accum = [ ] result . append ( cur word ) elif chunk [ 0 ] == 'href' : href = chunk [ 1 ] cur word = href token ( href , pre tags = tag accum , trailing whitespace = " " ) tag accum = [ ] result . append ( cur word ) continue if is word ( chunk ) : chunk , trailing whitespace = split trailing whitespace ( chunk ) cur word = token ( chunk , pre tags = tag accum , trailing whitespace = trailing whitespace ) tag accum = [ ] result . append ( cur word ) elif is start tag ( chunk ) : tag accum . append ( chunk ) elif is end tag ( chunk ) : if tag accum : tag accum . append ( chunk ) else : assert cur word , ( "Weird state, cur word=%r, result=%r, chunks=%r of %r" % ( cur word , result , chunk , chunks ) ) cur word . post tags . append ( chunk ) else : assert ( 0 ) if not result : return [ token ( '' , pre tags = tag accum ) ] else : result [ - 1 ] . post tags . extend ( tag accum ) return result
def start tag ( el ) : return '<%s%s>' % ( el . tag , '' . join ( [ ' %s="%s"' % ( name , html escape ( value , True ) ) for name , value in el . attrib . items ( ) ] ) )
def fixup ins del tags ( doc ) : for tag in [ 'ins' , 'del' ] : for el in doc . xpath ( 'descendant-or-self::%s' % tag ) : if not contains block level tag ( el ) : continue move el inside block ( el , tag = tag ) el . drop tag ( )
def cache url ( self , * * kwargs ) : query = { 'Operation' : self . Operation , 'Service' : "AWSE Commerce Service" , 'Version' : self . Version , } query . update ( kwargs ) service domain = SERVICE DOMAINS [ self . Region ] [ 0 ] return "http://" + service domain + "/onca/xml?" + quote query ( query )
def document fromstring ( html , guess charset = True , parser = None ) : if not isinstance ( html , strings ) : raise Type Error ( 'string required' ) if parser is None : parser = html parser return parser . parse ( html , use Chardet = guess charset ) . getroot ( )
def export ( self , location ) : url , rev = self . get url rev ( ) rev options = get rev options ( url , rev ) logger . info ( 'Exporting svn repository %s to %s' , url , location ) with indent log ( ) : if os . path . exists ( location ) : # Subversion doesn't like to check out over an existing # directory --force fixes this, but was only added in svn 1.5 rmtree ( location ) self . run command ( [ 'export' ] + rev options + [ url , location ] , show stdout = False )
def get revision ( self , location ) : # Note: taken from setuptools.command.egg info revision = 0 for base , dirs , files in os . walk ( location ) : if self . dirname not in dirs : dirs [ : ] = [ ] continue # no sense walking uncontrolled subdirs dirs . remove ( self . dirname ) entries fn = os . path . join ( base , self . dirname , 'entries' ) if not os . path . exists ( entries fn ) : # FIXME: should we warn? continue dirurl , localrev = self . get svn url rev ( base ) if base == location : base url = dirurl + '/' # save the root url elif not dirurl or not dirurl . startswith ( base url ) : dirs [ : ] = [ ] continue # not part of the same svn tree, skip it revision = max ( revision , localrev ) return revision
def unique ( iterable ) : seen = set ( ) for value in iterable : if not value in seen : seen . add ( value ) yield value
def handle requires ( metadata , pkg info , key ) : may requires = defaultdict ( list ) for value in pkg info . get all ( key ) : extra match = EXTRA RE . search ( value ) if extra match : groupdict = extra match . groupdict ( ) condition = groupdict [ 'condition' ] extra = groupdict [ 'extra' ] package = groupdict [ 'package' ] if condition . endswith ( ' and ' ) : condition = condition [ : - 5 ] else : condition , extra = None , None package = value key = May Requires Key ( condition , extra ) may requires [ key ] . append ( package ) if may requires : metadata [ 'run requires' ] = [ ] for key , value in may requires . items ( ) : may requirement = { 'requires' : value } if key . extra : may requirement [ 'extra' ] = key . extra if key . condition : may requirement [ 'environment' ] = key . condition metadata [ 'run requires' ] . append ( may requirement ) if not 'extras' in metadata : metadata [ 'extras' ] = [ ] metadata [ 'extras' ] . extend ( [ key . extra for key in may requires . keys ( ) if key . extra ] )
def requires to requires dist ( requirement ) : requires dist = [ ] for op , ver in requirement . specs : requires dist . append ( op + ver ) if not requires dist : return '' return " (%s)" % ',' . join ( requires dist )
def modules ( self ) : # since the module has to be importable we go ahead and put the # basepath as the very first path to check as that should minimize # namespace collisions, this is what unittest does also sys . path . insert ( 0 , self . basedir ) for p in self . paths ( ) : # http://stackoverflow.com/questions/67631/ try : module name = self . module path ( p ) logger . debug ( "Importing {} from path {}" . format ( module name , p ) ) m = importlib . import module ( module name ) yield m except Exception as e : logger . warning ( 'Caught exception while importing {}: {}' . format ( p , e ) ) logger . warning ( e , exc info = True ) error info = getattr ( self , 'error info' , None ) if not error info : exc info = sys . exc info ( ) #raise e. class , e, exc info[2] #self.error info = (e, exc info) self . error info = exc info continue sys . path . pop ( 0 )
def classes ( self ) : for module in self . modules ( ) : cs = inspect . getmembers ( module , inspect . isclass ) class name = getattr ( self , 'class name' , '' ) class regex = '' if class name : if class name . startswith ( "*" ) : class name = class name . strip ( "*" ) class regex = re . compile ( r'.*?{}' . format ( class name ) , re . I ) else : class regex = re . compile ( r'^{}' . format ( class name ) , re . I ) for c name , c in cs : can yield = True if class regex and not class regex . match ( c name ) : #if class name and class name not in c name: can yield = False if can yield and issubclass ( c , unittest . Test Case ) : if c is not unittest . Test Case : # ignore actual Test Case class logger . debug ( 'class: {} matches {}' . format ( c name , class name ) ) yield c
def method names ( self ) : for c in self . classes ( ) : #ms = inspect.getmembers(c, inspect.ismethod) # http://stackoverflow.com/questions/17019949/ ms = inspect . getmembers ( c , lambda f : inspect . ismethod ( f ) or inspect . isfunction ( f ) ) method name = getattr ( self , 'method name' , '' ) method regex = '' if method name : if method name . startswith ( self . method prefix ) : method regex = re . compile ( r'^{}' . format ( method name ) , flags = re . I ) else : if method name . startswith ( "*" ) : method name = method name . strip ( "*" ) method regex = re . compile ( r'^{}[ ]{{0,1}}.*?{}' . format ( self . method prefix , method name ) , flags = re . I ) else : method regex = re . compile ( r'^{}[ ]{{0,1}}{}' . format ( self . method prefix , method name ) , flags = re . I ) for m name , m in ms : if not m name . startswith ( self . method prefix ) : continue can yield = True if method regex and not method regex . match ( m name ) : can yield = False if can yield : logger . debug ( 'method: {} matches {}' . format ( m name , method name ) ) yield c , m name
def dump arg defaults ( kwargs ) : if current app : kwargs . setdefault ( 'cls' , current app . json encoder ) if not current app . config [ 'JSON AS ASCII' ] : kwargs . setdefault ( 'ensure ascii' , False ) kwargs . setdefault ( 'sort keys' , current app . config [ 'JSON SORT KEYS' ] ) else : kwargs . setdefault ( 'sort keys' , True ) kwargs . setdefault ( 'cls' , JSON Encoder )
def load arg defaults ( kwargs ) : if current app : kwargs . setdefault ( 'cls' , current app . json decoder ) else : kwargs . setdefault ( 'cls' , JSON Decoder )
def get dist ( self ) : egg info = self . egg info path ( '' ) . rstrip ( '/' ) base dir = os . path . dirname ( egg info ) metadata = pkg resources . Path Metadata ( base dir , egg info ) dist name = os . path . splitext ( os . path . basename ( egg info ) ) [ 0 ] return pkg resources . Distribution ( os . path . dirname ( egg info ) , project name = dist name , metadata = metadata )
def to text ( s , blank if none = True ) : if s is None : if blank if none : return "" else : return None elif isinstance ( s , text type ) : return s else : return text type ( s )
def find ca bundle ( ) : if os . name == 'nt' : return get win certfile ( ) else : for cert path in cert paths : if os . path . isfile ( cert path ) : return cert path try : return pkg resources . resource filename ( 'certifi' , 'cacert.pem' ) except ( Import Error , Resolution Error , Extraction Error ) : return None
def parse ( doc , treebuilder = "etree" , encoding = None , namespace HTML Elements = True ) : tb = treebuilders . get Tree Builder ( treebuilder ) p = HTML Parser ( tb , namespace HTML Elements = namespace HTML Elements ) return p . parse ( doc , encoding = encoding )
def bind ( self ) : HTTP Server . init ( self , ( self . host , self . port ) , HTTP Request Handler ) self . port = self . server port
def report ( self ) : print ( self . report message . format ( service = self . service , host = self . host , port = self . port , ) ) sys . stdout . flush ( )
def load bytecode ( self , f ) : # make sure the magic header is correct magic = f . read ( len ( bc magic ) ) if magic != bc magic : self . reset ( ) return # the source code of the file changed, we need to reload checksum = pickle . load ( f ) if self . checksum != checksum : self . reset ( ) return self . code = marshal load ( f )
def get impl ver ( ) : impl ver = sysconfig . get config var ( "py version nodot" ) if not impl ver : impl ver = '' . join ( map ( str , sys . version info [ : 2 ] ) ) return impl ver
def distros for location ( location , basename , metadata = None ) : if basename . endswith ( '.egg.zip' ) : basename = basename [ : - 4 ] # strip the .zip if basename . endswith ( '.egg' ) and '-' in basename : # only one, unambiguous interpretation return [ Distribution . from location ( location , basename , metadata ) ] if basename . endswith ( '.exe' ) : win base , py ver , platform = parse bdist wininst ( basename ) if win base is not None : return interpret distro name ( location , win base , metadata , py ver , BINARY DIST , platform ) # Try source distro extensions (.zip, .tgz, etc.) # for ext in EXTENSIONS : if basename . endswith ( ext ) : basename = basename [ : - len ( ext ) ] return interpret distro name ( location , basename , metadata ) return [ ]
def find external links ( url , page ) : for match in REL . finditer ( page ) : tag , rel = match . groups ( ) rels = set ( map ( str . strip , rel . lower ( ) . split ( ',' ) ) ) if 'homepage' in rels or 'download' in rels : for match in HREF . finditer ( tag ) : yield urljoin ( url , htmldecode ( match . group ( 1 ) ) ) for tag in ( "<th>Home Page" , "<th>Download URL" ) : pos = page . find ( tag ) if pos != - 1 : match = HREF . search ( page , pos ) if match : yield urljoin ( url , htmldecode ( match . group ( 1 ) ) )
def local open ( url ) : scheme , server , path , param , query , frag = urlparse ( url ) filename = url2pathname ( path ) if os . path . isfile ( filename ) : return urllib2 . urlopen ( url ) elif path . endswith ( '/' ) and os . path . isdir ( filename ) : files = [ ] for f in os . listdir ( filename ) : if f == 'index.html' : with open ( os . path . join ( filename , f ) , 'r' ) as fp : body = fp . read ( ) break elif os . path . isdir ( os . path . join ( filename , f ) ) : f += '/' files . append ( "<a href=%r>%s</a>" % ( f , f ) ) else : body = ( "<html><head><title>%s</title>" % url ) + "</head><body>%s</body></html>" % '\n' . join ( files ) status , message = 200 , "OK" else : status , message , body = 404 , "Path not found" , "Not found" headers = { 'content-type' : 'text/html' } return HTTP Error ( url , status , message , headers , String IO ( body ) )
def process url ( self , url , retrieve = False ) : if url in self . scanned urls and not retrieve : return self . scanned urls [ url ] = True if not URL SCHEME ( url ) : self . process filename ( url ) return else : dists = list ( distros for url ( url ) ) if dists : if not self . url ok ( url ) : return self . debug ( "Found link: %s" , url ) if dists or not retrieve or url in self . fetched urls : list ( map ( self . add , dists ) ) return # don't need the actual page if not self . url ok ( url ) : self . fetched urls [ url ] = True return self . info ( "Reading %s" , url ) self . fetched urls [ url ] = True # prevent multiple fetch attempts f = self . open url ( url , "Download error on %s: %%s -- Some packages may not be found!" % url ) if f is None : return self . fetched urls [ f . url ] = True if 'html' not in f . headers . get ( 'content-type' , '' ) . lower ( ) : f . close ( ) # not html, we can't process it return base = f . url # handle redirects page = f . read ( ) if not isinstance ( page , str ) : # We are in Python 3 and got bytes. We want str. if isinstance ( f , HTTP Error ) : # Errors have no charset, assume latin1: charset = 'latin-1' else : charset = f . headers . get param ( 'charset' ) or 'latin-1' page = page . decode ( charset , "ignore" ) f . close ( ) for match in HREF . finditer ( page ) : link = urljoin ( base , htmldecode ( match . group ( 1 ) ) ) self . process url ( link ) if url . startswith ( self . index url ) and getattr ( f , 'code' , None ) != 404 : page = self . process index ( url , page )
def init pathinfo ( ) : d = set ( ) for dir in sys . path : try : if os . path . isdir ( dir ) : dir , dircase = makepath ( dir ) d . add ( dircase ) except Type Error : continue return d
def setcopyright ( ) : builtins . copyright = Printer ( "copyright" , sys . copyright ) if is jython : builtins . credits = Printer ( "credits" , "Jython is maintained by the Jython developers (www.jython.org)." ) elif is pypy : builtins . credits = Printer ( "credits" , "Py Py is maintained by the Py Py developers: http://pypy.org/" ) else : builtins . credits = Printer ( "credits" , ) here = os . path . dirname ( os . file ) builtins . license = Printer ( "license" , "See http://www.python.org/%.3s/license.html" % sys . version , [ "LICENSE.txt" , "LICENSE" ] , [ os . path . join ( here , os . pardir ) , here , os . curdir ] )
def have pyrex ( ) : pyrex impls = 'Cython.Distutils.build ext' , 'Pyrex.Distutils.build ext' for pyrex impl in pyrex impls : try : # from (pyrex impl) import build ext import ( pyrex impl , fromlist = [ 'build ext' ] ) . build ext return True except Exception : pass return False
def debug application ( self , environ , start response ) : app iter = None try : app iter = self . app ( environ , start response ) for item in app iter : yield item if hasattr ( app iter , 'close' ) : app iter . close ( ) except Exception : if hasattr ( app iter , 'close' ) : app iter . close ( ) traceback = get current traceback ( skip = 1 , show hidden frames = self . show hidden frames , ignore system exceptions = True ) for frame in traceback . frames : self . frames [ frame . id ] = frame self . tracebacks [ traceback . id ] = traceback try : start response ( '500 INTERNAL SERVER ERROR' , [ ( 'Content-Type' , 'text/html; charset=utf-8' ) , # Disable Chrome's XSS protection, the debug # output can cause false-positives. ( 'X-XSS-Protection' , '0' ) , ] ) except Exception : # if we end up here there has been output but an error # occurred.  in that situation we can do nothing fancy any # more, better log something into the error log and fall # back gracefully. environ [ 'wsgi.errors' ] . write ( 'Debugging middleware caught exception in streamed ' 'response at a point where response headers were already ' 'sent.\n' ) else : yield traceback . render full ( evalex = self . evalex , secret = self . secret ) . encode ( 'utf-8' , 'replace' ) traceback . log ( environ [ 'wsgi.errors' ] )
def get resource ( self , request , filename ) : filename = join ( dirname ( file ) , 'shared' , basename ( filename ) ) if isfile ( filename ) : mimetype = mimetypes . guess type ( filename ) [ 0 ] or 'application/octet-stream' f = open ( filename , 'rb' ) try : return Response ( f . read ( ) , mimetype = mimetype ) finally : f . close ( ) return Response ( 'Not Found' , status = 404 )
def user agent ( ) : data = { "installer" : { "name" : "pip" , "version" : pip . version } , "python" : platform . python version ( ) , "implementation" : { "name" : platform . python implementation ( ) , } , } if data [ "implementation" ] [ "name" ] == 'C Python' : data [ "implementation" ] [ "version" ] = platform . python version ( ) elif data [ "implementation" ] [ "name" ] == 'Py Py' : if sys . pypy version info . releaselevel == 'final' : pypy version info = sys . pypy version info [ : 3 ] else : pypy version info = sys . pypy version info data [ "implementation" ] [ "version" ] = "." . join ( [ str ( x ) for x in pypy version info ] ) elif data [ "implementation" ] [ "name" ] == 'Jython' : # Complete Guess data [ "implementation" ] [ "version" ] = platform . python version ( ) elif data [ "implementation" ] [ "name" ] == 'Iron Python' : # Complete Guess data [ "implementation" ] [ "version" ] = platform . python version ( ) if sys . platform . startswith ( "linux" ) : distro = dict ( filter ( lambda x : x [ 1 ] , zip ( [ "name" , "version" , "id" ] , platform . linux distribution ( ) ) , ) ) libc = dict ( filter ( lambda x : x [ 1 ] , zip ( [ "lib" , "version" ] , platform . libc ver ( ) ) , ) ) if libc : distro [ "libc" ] = libc if distro : data [ "distro" ] = distro if sys . platform . startswith ( "darwin" ) and platform . mac ver ( ) [ 0 ] : data [ "distro" ] = { "name" : "OS X" , "version" : platform . mac ver ( ) [ 0 ] } if platform . system ( ) : data . setdefault ( "system" , { } ) [ "name" ] = platform . system ( ) if platform . release ( ) : data . setdefault ( "system" , { } ) [ "release" ] = platform . release ( ) if platform . machine ( ) : data [ "cpu" ] = platform . machine ( ) return "{data[installer][name]}/{data[installer][version]} {json}" . format ( data = data , json = json . dumps ( data , separators = ( "," , ":" ) , sort keys = True ) , )
def is url ( name ) : if ':' not in name : return False scheme = name . split ( ':' , 1 ) [ 0 ] . lower ( ) return scheme in [ 'http' , 'https' , 'file' , 'ftp' ] + vcs . all schemes
def download http url ( link , session , temp dir ) : target url = link . url . split ( '#' , 1 ) [ 0 ] try : resp = session . get ( target url , # We use Accept-Encoding: identity here because requests # defaults to accepting compressed responses. This breaks in # a variety of ways depending on how the server is configured. # - Some servers will notice that the file isn't a compressible #   file and will leave the file alone and with an empty #   Content-Encoding # - Some servers will notice that the file is already #   compressed and will leave the file alone and will add a #   Content-Encoding: gzip header # - Some servers won't notice anything at all and will take #   a file that's already been compressed and compress it again #   and set the Content-Encoding: gzip header # By setting this to request only the identity encoding We're # hoping to eliminate the third case. Hopefully there does not # exist a server which when given a file will notice it is # already compressed and that you're not asking for a # compressed file and will then decompress it before sending # because if that's the case I don't think it'll ever be # possible to make this work. headers = { "Accept-Encoding" : "identity" } , stream = True , ) resp . raise for status ( ) except requests . HTTP Error as exc : logger . critical ( "HTTP error %s while getting %s" , exc . response . status code , link , ) raise content type = resp . headers . get ( 'content-type' , '' ) filename = link . filename # fallback # Have a look at the Content-Disposition header for a better guess content disposition = resp . headers . get ( 'content-disposition' ) if content disposition : type , params = cgi . parse header ( content disposition ) # We use ``or`` here because we don't want to use an "empty" value # from the filename param. filename = params . get ( 'filename' ) or filename ext = splitext ( filename ) [ 1 ] if not ext : ext = mimetypes . guess extension ( content type ) if ext : filename += ext if not ext and link . url != resp . url : ext = os . path . splitext ( resp . url ) [ 1 ] if ext : filename += ext file path = os . path . join ( temp dir , filename ) with open ( file path , 'wb' ) as content file : download url ( resp , link , content file ) return file path , content type
def currency Format ( context , code , symbol , format , currency digits = True , decimal quantization = True , name = '' ) : context . action ( discriminator = ( 'currency' , name , code ) , callable = register currency , args = ( name , code , symbol , format , currency digits , decimal quantization ) )
def exchange ( context , component , backend , base , name = '' ) : context . action ( discriminator = ( 'currency' , 'exchange' , component ) , callable = register exchange , args = ( name , component , backend , base ) )
def print results ( distributions , list all files ) : results printed = False for dist in distributions : results printed = True logger . info ( "---" ) logger . info ( "Metadata-Version: %s" % dist . get ( 'metadata-version' ) ) logger . info ( "Name: %s" % dist [ 'name' ] ) logger . info ( "Version: %s" % dist [ 'version' ] ) logger . info ( "Summary: %s" % dist . get ( 'summary' ) ) logger . info ( "Home-page: %s" % dist . get ( 'home-page' ) ) logger . info ( "Author: %s" % dist . get ( 'author' ) ) logger . info ( "Author-email: %s" % dist . get ( 'author-email' ) ) logger . info ( "License: %s" % dist . get ( 'license' ) ) logger . info ( "Location: %s" % dist [ 'location' ] ) logger . info ( "Requires: %s" % ', ' . join ( dist [ 'requires' ] ) ) if list all files : logger . info ( "Files:" ) if dist [ 'files' ] is not None : for line in dist [ 'files' ] : logger . info ( "  %s" % line . strip ( ) ) else : logger . info ( "Cannot locate installed-files.txt" ) if 'entry points' in dist : logger . info ( "Entry-points:" ) for line in dist [ 'entry points' ] : logger . info ( "  %s" % line . strip ( ) ) return results printed
def decode ( self , data , decode content , flush decoder ) : try : if decode content and self . decoder : data = self . decoder . decompress ( data ) except ( IO Error , zlib . error ) as e : content encoding = self . headers . get ( 'content-encoding' , '' ) . lower ( ) raise Decode Error ( "Received response with content-encoding: %s, but " "failed to decode it." % content encoding , e ) if flush decoder and decode content and self . decoder : buf = self . decoder . decompress ( binary type ( ) ) data += buf + self . decoder . flush ( ) return data
def render ( template , context , app ) : rv = template . render ( context ) template rendered . send ( app , template = template , context = context ) return rv
def parse version ( version ) : global parse version try : from pkg resources import parse version except Import Error : from distutils . version import Loose Version as parse version return parse version ( version )
def install ( self , force = False , overrides = { } ) : # Utility to get the target directory for a particular key def get path ( key ) : return overrides . get ( key ) or self . install paths [ key ] # The base target location is either purelib or platlib if self . parsed wheel info [ 'Root-Is-Purelib' ] == 'true' : root = get path ( 'purelib' ) else : root = get path ( 'platlib' ) # Parse all the names in the archive name trans = { } for info in self . zipfile . infolist ( ) : name = info . filename # Zip files can contain entries representing directories. # These end in a '/'. # We ignore these, as we create directories on demand. if name . endswith ( '/' ) : continue # Pathnames in a zipfile namelist are always /-separated. # In theory, paths could start with ./ or have other oddities # but this won't happen in practical cases of well-formed wheels. # We'll cover the simple case of an initial './' as it's both easy # to do and more common than most other oddities. if name . startswith ( './' ) : name = name [ 2 : ] # Split off the base directory to identify files that are to be # installed in non-root locations basedir , sep , filename = name . partition ( '/' ) if sep and basedir == self . datadir name : # Data file. Target destination is elsewhere key , sep , filename = filename . partition ( '/' ) if not sep : raise Value Error ( "Invalid filename in wheel: {0}" . format ( name ) ) target = get path ( key ) else : # Normal file. Target destination is root key = '' target = root filename = name # Map the actual filename from the zipfile to its intended target # directory and the pathname relative to that directory. dest = os . path . normpath ( os . path . join ( target , filename ) ) name trans [ info ] = ( key , target , filename , dest ) # We're now ready to start processing the actual install. The process # is as follows: #   1. Prechecks - is the wheel valid, is its declared architecture #      OK, etc. [[Responsibility of the caller]] #   2. Overwrite check - do any of the files to be installed already #      exist? #   3. Actual install - put the files in their target locations. #   4. Update RECORD - write a suitably modified RECORD file to #      reflect the actual installed paths. if not force : for info , v in name trans . items ( ) : k = info . filename key , target , filename , dest = v if os . path . exists ( dest ) : raise Value Error ( "Wheel file {0} would overwrite {1}. Use force if this is intended" . format ( k , dest ) ) # Get the name of our executable, for use when replacing script # wrapper hashbang lines. # We encode it using getfilesystemencoding, as that is "the name of # the encoding used to convert Unicode filenames into system file # names". exename = sys . executable . encode ( sys . getfilesystemencoding ( ) ) record data = [ ] record name = self . distinfo name + '/RECORD' for info , ( key , target , filename , dest ) in name trans . items ( ) : name = info . filename source = self . zipfile . open ( info ) # Skip the RECORD file if name == record name : continue ddir = os . path . dirname ( dest ) if not os . path . isdir ( ddir ) : os . makedirs ( ddir ) destination = Hashing File ( open ( dest , 'wb' ) ) if key == 'scripts' : hashbang = source . readline ( ) if hashbang . startswith ( b'#!python' ) : hashbang = b'#!' + exename + binary ( os . linesep ) destination . write ( hashbang ) shutil . copyfileobj ( source , destination ) reldest = os . path . relpath ( dest , root ) reldest . replace ( os . sep , '/' ) record data . append ( ( reldest , destination . digest ( ) , destination . length ) ) destination . close ( ) source . close ( ) # preserve attributes (especially +x bit for scripts) attrs = info . external attr >> 16 if attrs : # tends to be 0 if Windows. os . chmod ( dest , info . external attr >> 16 ) record name = os . path . join ( root , self . record name ) writer = csv . writer ( open for csv ( record name , 'w+' ) ) for reldest , digest , length in sorted ( record data ) : writer . writerow ( ( reldest , digest , length ) ) writer . writerow ( ( self . record name , '' , '' ) )
def is declared ( self , name ) : if name in self . declared locally or name in self . declared parameter : return True return name in self . declared
def visit Name ( self , node ) : if node . ctx == 'store' : self . identifiers . declared locally . add ( node . name ) elif node . ctx == 'param' : self . identifiers . declared parameter . add ( node . name ) elif node . ctx == 'load' and not self . identifiers . is declared ( node . name ) : self . identifiers . undeclared . add ( node . name )
def visit From Import ( self , node , frame ) : self . newline ( node ) self . write ( 'included template = environment.get template(' ) self . visit ( node . template , frame ) self . write ( ', %r).' % self . name ) if node . with context : self . write ( 'make module(context.parent, True)' ) else : self . write ( 'module' ) var names = [ ] discarded names = [ ] for name in node . names : if isinstance ( name , tuple ) : name , alias = name else : alias = name self . writeline ( 'l %s = getattr(included template, ' '%r, missing)' % ( alias , name ) ) self . writeline ( 'if l %s is missing:' % alias ) self . indent ( ) self . writeline ( 'l %s = environment.undefined(%r %% ' 'included template. name , ' 'name=%r)' % ( alias , 'the template %%r (imported on %s) does ' 'not export the requested name %s' % ( self . position ( node ) , repr ( name ) ) , name ) ) self . outdent ( ) if frame . toplevel : var names . append ( alias ) if not alias . startswith ( ' ' ) : discarded names . append ( alias ) frame . assigned names . add ( alias ) if var names : if len ( var names ) == 1 : name = var names [ 0 ] self . writeline ( 'context.vars[%r] = l %s' % ( name , name ) ) else : self . writeline ( 'context.vars.update({%s})' % ', ' . join ( '%r: l %s' % ( name , name ) for name in var names ) ) if discarded names : if len ( discarded names ) == 1 : self . writeline ( 'context.exported vars.discard(%r)' % discarded names [ 0 ] ) else : self . writeline ( 'context.exported vars.difference ' 'update((%s))' % ', ' . join ( imap ( repr , discarded names ) ) )
def populate requirement set ( requirement set , args , options , finder , session , name , wheel cache ) : for req in args : requirement set . add requirement ( Install Requirement . from line ( req , None , isolated = options . isolated mode , wheel cache = wheel cache ) ) for req in options . editables : requirement set . add requirement ( Install Requirement . from editable ( req , default vcs = options . default vcs , isolated = options . isolated mode , wheel cache = wheel cache ) ) found req in file = False for filename in options . requirements : for req in parse requirements ( filename , finder = finder , options = options , session = session , wheel cache = wheel cache ) : found req in file = True requirement set . add requirement ( req ) if not ( args or options . editables or found req in file ) : opts = { 'name' : name } if options . find links : msg = ( 'You must give at least one requirement to ' '%(name)s (maybe you meant "pip %(name)s ' '%(links)s"?)' % dict ( opts , links = ' ' . join ( options . find links ) ) ) else : msg = ( 'You must give at least one requirement ' 'to %(name)s (see "pip help %(name)s")' % opts ) logger . warning ( msg )
def export ( self , location ) : temp dir = tempfile . mkdtemp ( '-export' , 'pip-' ) self . unpack ( temp dir ) if os . path . exists ( location ) : # Remove the location to make sure Bazaar can export it correctly rmtree ( location ) try : self . run command ( [ 'export' , location ] , cwd = temp dir , show stdout = False ) finally : rmtree ( temp dir )
def verify signature ( self , key , value , sig ) : return constant time compare ( sig , self . get signature ( key , value ) )
def get signature ( self , value ) : value = want bytes ( value ) key = self . derive key ( ) sig = self . algorithm . get signature ( key , value ) return base64 encode ( sig )
def sign ( self , value ) : return value + want bytes ( self . sep ) + self . get signature ( value )
def verify signature ( self , value , sig ) : key = self . derive key ( ) try : sig = base64 decode ( sig ) except Exception : return False return self . algorithm . verify signature ( key , value , sig )
def unsign ( self , signed value ) : signed value = want bytes ( signed value ) sep = want bytes ( self . sep ) if sep not in signed value : raise Bad Signature ( 'No %r found in value' % self . sep ) value , sig = signed value . rsplit ( sep , 1 ) if self . verify signature ( value , sig ) : return value raise Bad Signature ( 'Signature %r does not match' % sig , payload = value )
def sign ( self , value ) : value = want bytes ( value ) timestamp = base64 encode ( int to bytes ( self . get timestamp ( ) ) ) sep = want bytes ( self . sep ) value = value + sep + timestamp return value + sep + self . get signature ( value )
def all dirs ( base path ) : for root , dirs , files in os . walk ( base path , followlinks = True ) : for dir in dirs : yield os . path . relpath ( os . path . join ( root , dir ) , base path )
def install scripts ( distributions ) : try : from setuptools . command import easy install import pkg resources except Import Error : raise Runtime Error ( "'wheel install scripts' needs setuptools." ) for dist in distributions : pkg resources dist = pkg resources . get distribution ( dist ) install = wheel . paths . get install command ( dist ) command = easy install . easy install ( install . distribution ) command . args = [ 'wheel' ] # dummy argument command . finalize options ( ) command . install egg scripts ( pkg resources dist )
def get node ( self , ID ) : node = super ( Graph , self ) . get node ( ID ) if node is not None : return node for graph in self . all graphs : for each node in graph . nodes : if each node . ID == ID : return each node else : return None
def directed changed ( self , new ) : if new : conn = "->" else : conn = "--" for edge in [ e for g in self . all graphs for e in g . edges ] : edge . conn = conn
def on edges ( self , object , name , old , new ) : if name == "edges items" : edges = new . added elif name == "edges" : edges = new else : edges = [ ] all nodes = [ n for g in self . all graphs for n in g . nodes ] for each edge in edges : # Ensure the edge's nodes exist in the graph. if each edge . tail node not in all nodes : object . nodes . append ( each edge . tail node ) if each edge . head node not in all nodes : object . nodes . append ( each edge . head node ) # Initialise the edge's list of available nodes. each edge . nodes = all nodes
def component changed ( self , old , new ) : canvas = self . canvas if old is not None : canvas . remove ( old ) if new is not None : canvas . add ( new )
def diagram canvas changed ( self , new ) : logger . debug ( "Diagram canvas changed!" ) canvas = self . diagram canvas for tool in self . tools : if canvas is not None : print "Adding tool: %s" % tool canvas . tools . append ( tool ( canvas ) )
def clear canvas ( self ) : logger . debug ( "Clearing the diagram canvas!" ) old canvas = self . diagram canvas #        logger.debug("Canvas components: %s" % canvas.components) #        for component in canvas.components: #            canvas.remove(component) #        logger.debug("Canvas components: %s" % canvas.components) #        for component in canvas.components: #            canvas.remove(component) #        logger.debug("Canvas components: %s" % canvas.components) #        canvas.request redraw() new canvas = Canvas ( ) new canvas . copy traits ( old canvas , [ "bgcolor" , "draw axes" ] ) self . diagram canvas = new canvas self . viewport . component = new canvas self . viewport . request redraw ( ) return
def domain model changed for diagram ( self , obj , name , old , new ) : if old is not None : self . unmap model ( old ) if new is not None : self . map model ( new )
def map model ( self , new ) : logger . debug ( "Mapping the domain model!" ) dot = Dot ( ) self . diagram . clear canvas ( ) for node mapping in self . nodes : ct = node mapping . containment trait logger . debug ( "Mapping elements contained by the '%s' trait" % ct ) if hasattr ( new , ct ) : elements = getattr ( new , ct ) logger . debug ( "%d element(s) found" % len ( elements ) ) for element in elements : pydot node = Node ( str ( id ( element ) ) ) dot attrs = node mapping . dot node if dot attrs is not None : self . style node ( pydot node , dot attrs ) dot . add node ( pydot node ) new . on trait change ( self . map element , ct + " items" ) logger . debug ( "Retrieving xdot data and forming pydot graph!" ) xdot = graph from dot data ( dot . create ( self . program , "xdot" ) ) parser = X Dot Parser ( ) for node in xdot . get node list ( ) : diagram node = parser . parse node ( node ) logger . debug ( "Parsed node [%s] and received diagram node [%s]" % ( node , diagram node ) ) if diagram node is not None : for node mapping in self . nodes : # FIXME: slow ct = node mapping . containment trait for element in getattr ( new , ct ) : if str ( id ( element ) ) == diagram node . dot node . get name ( ) : logger . debug ( "Referencing element [%s] from diagram node [%s]" % ( element , diagram node ) ) diagram node . element = element break # Tools if isinstance ( diagram node . element , node mapping . element ) : for tool in node mapping . tools : logger . debug ( "Adding tool [%s] to diagram node [%s]" % ( tool , diagram node ) ) diagram node . tools . append ( tool ( diagram node ) ) else : if diagram node . element is None : logger . warning ( "Diagram node not referenced to element" ) self . diagram . diagram canvas . add ( diagram node ) del parser
def unmap model ( self , old ) : for node mapping in self . nodes : ct = node mapping . containment trait if hasattr ( old , ct ) : old elements = getattr ( old , ct ) for old element in old elements : old . on trait change ( self . map element , ct + " items" , remove = True )
def map element ( self , obj , name , event ) : canvas = self . diagram . diagram canvas parser = X Dot Parser ( ) for element in event . added : logger . debug ( "Mapping new element [%s] to diagram node" % element ) for node mapping in self . nodes : ct = name [ : - 6 ] #strip ' items' if node mapping . containment trait == ct : dot attrs = node mapping . dot node dot = Dot ( ) graph node = Node ( str ( id ( element ) ) ) self . style node ( graph node , dot attrs ) dot . add node ( graph node ) xdot = graph from dot data ( dot . create ( self . program , "xdot" ) ) diagram nodes = parser . parse nodes ( xdot ) #.get node list()) for dn in diagram nodes : if dn is not None : dn . element = element # Tools for tool in node mapping . tools : dn . tools . append ( tool ( dn ) ) canvas . add ( dn ) canvas . request redraw ( ) for element in event . removed : logger . debug ( "Unmapping element [%s] from diagram" % element ) for component in canvas . components : if element == component . element : canvas . remove ( component ) canvas . request redraw ( ) break
def parse xdot data ( self , data ) : parser = self . parser #        if pyparsing version >= "1.2": #            parser.parse With Tabs() if data : return parser . parse String ( data ) else : return [ ]
def proc font ( self , tokens ) : size = int ( tokens [ "s" ] ) self . pen . font = "%s %d" % ( tokens [ "b" ] , size ) return [ ]
def proc ellipse ( self , tokens , filled ) : component = Ellipse ( pen = self . pen , x origin = tokens [ "x0" ] , y origin = tokens [ "y0" ] , e width = tokens [ "w" ] , e height = tokens [ "h" ] , filled = filled ) return component
def proc polygon ( self , tokens , filled ) : pts = [ ( p [ "x" ] , p [ "y" ] ) for p in tokens [ "points" ] ] component = Polygon ( pen = self . pen , points = pts , filled = filled ) return component
def proc polyline ( self , tokens ) : pts = [ ( p [ "x" ] , p [ "y" ] ) for p in tokens [ "points" ] ] component = Polyline ( pen = self . pen , points = pts ) return component
def proc text ( self , tokens ) : component = Text ( pen = self . pen , text x = tokens [ "x" ] , text y = tokens [ "y" ] , justify = tokens [ "j" ] , text w = tokens [ "w" ] , text = tokens [ "b" ] ) return component
def proc image ( self , tokens ) : print "IMAGE:" , tokens , tokens . as List ( ) , tokens . keys ( ) raise Not Implemented Error
def render grid file ( context , f ) : f . seek ( 0 ) # Ensure we are reading from the beginning. response = context . response # Frequently accessed, so made local.  Useless optimization on Pypy. if debug : # We add some useful diagnostic information in development, omitting from production due to sec. response . headers [ 'Grid-ID' ] = str ( f . id ) # The Grid FS file ID. log . debug ( "Serving Grid FS file." , extra = dict ( identifier = str ( f . id ) , filename = f . filename , length = f . length , mimetype = f . content type ) ) response . conditional response = True response . accept ranges = 'bytes' # We allow returns of partial content, if requested. response . content type = f . content type # Direct transfer of Grid FS-stored MIME type. response . content length = f . length # The length was pre-computed when the file was uploaded. response . content md5 = response . etag = f . md5 # As was the MD5, used for simple integrity testing. response . last modified = f . metadata . get ( 'modified' , None ) # Optional additional metadata. response . content disposition = 'attachment; filename=' + f . name # Preserve the filename through to the client. # Being asked for a range or not determines the streaming style used. if context . request . if range . match response ( response ) : response . body file = f # Support seek + limited read. else : response . app iter = iter ( f ) # Assign the body as a streaming, chunked iterator. return True
def save ( self , obj ) : fd = None try : fd = open ( self . dot file . absolute path , "wb" ) obj . save dot ( fd ) finally : if fd is not None : fd . close ( ) #        self.m time = getmtime(self.adaptee.absolute path) return
def load ( self ) : fd = None try : obj = parse dot file ( self . dot file . absolute path ) finally : if fd is not None : fd . close ( ) return obj
def is in ( self , point x , point y ) : x = self . x origin y = self . y origin a = self . e width #/2 # FIXME: Why divide by two b = self . e height #/2 return ( ( point x - x ) ** 2 / ( a ** 2 ) ) + ( ( point y - y ) ** 2 / ( b ** 2 ) ) < 1.0
def draw bounds ( self , gc ) : dx , dy = self . bounds x , y = self . position gc . rect ( x , y , dx , dy ) gc . stroke path ( )
def perform ( self , event ) : wizard = New Dot Graph Wizard ( parent = self . window . control , window = self . window , title = "New Graph" ) # Open the wizard if wizard . open ( ) == OK : wizard . finished = True
def start ( self , context ) : if debug : log . info ( "Connecting SQL Alchemy database layer." , extra = dict ( uri = redact uri ( self . uri ) , config = self . config , alias = self . alias , ) ) # Construct the engine. engine = self . engine = create engine ( self . uri , * * self . config ) # Construct the session factory. self . Session = scoped session ( sessionmaker ( bind = engine ) ) # Test the connection. engine . connect ( ) . close ( ) # Assign the engine to our database alias. context . db [ self . alias ] = engine
def parse dot code fired ( self ) : parser = Godot Data Parser ( ) graph = parser . parse dot data ( self . dot code ) if graph is not None : self . model = graph
def new model ( self , info ) : if info . initialized : retval = confirm ( parent = info . ui . control , message = "Replace existing graph?" , title = "New Graph" , default = YES ) if retval == YES : self . model = Graph ( )
def open file ( self , info ) : if not info . initialized : return # Escape. #        retval = self.edit traits(parent=info.ui.control, view="file view") dlg = File Dialog ( action = "open" , wildcard = "Graphviz Files (*.dot, *.xdot, *.txt)|" "*.dot;*.xdot;*.txt|Dot Files (*.dot)|*.dot|" "All Files (*.*)|*.*|" ) if dlg . open ( ) == OK : parser = Godot Data Parser ( ) model = parser . parse dot file ( dlg . path ) if model is not None : self . model = model else : print "error parsing: %s" % dlg . path self . save file = dlg . path del dlg
def save ( self , info ) : save file = self . save file if not isfile ( save file ) : self . save as ( info ) else : fd = None try : fd = open ( save file , "wb" ) dot code = str ( self . model ) fd . write ( dot code ) finally : if fd is not None : fd . close ( )
def save as ( self , info ) : if not info . initialized : return #        retval = self.edit traits(parent=info.ui.control, view="file view") dlg = File Dialog ( action = "save as" , wildcard = "Graphviz Files (*.dot, *.xdot, *.txt)|" "*.dot;*.xdot;*.txt|Dot Files (*.dot)|*.dot|" "All Files (*.*)|*.*|" ) if dlg . open ( ) == OK : fd = None try : fd = open ( dlg . path , "wb" ) dot code = str ( self . model ) fd . write ( dot code ) self . save file = dlg . path except : error ( parent = info . ui . control , title = "Save Error" , message = "An error was encountered when saving\nto %s" % self . file ) finally : if fd is not None : fd . close ( ) del dlg
def configure graph ( self , info ) : if info . initialized : self . model . edit traits ( parent = info . ui . control , kind = "live" , view = attr view )
def configure nodes ( self , info ) : if info . initialized : self . model . edit traits ( parent = info . ui . control , kind = "live" , view = nodes view )
def configure edges ( self , info ) : if info . initialized : self . model . edit traits ( parent = info . ui . control , kind = "live" , view = edges view )
def about godot ( self , info ) : if info . initialized : self . edit traits ( parent = info . ui . control , kind = "livemodal" , view = about view )
def add node ( self , info ) : if not info . initialized : return graph = self . request graph ( info . ui . control ) if graph is None : return I Ds = [ v . ID for v in graph . nodes ] node = Node ( ID = make unique name ( "node" , I Ds ) ) graph . nodes . append ( node ) retval = node . edit traits ( parent = info . ui . control , kind = "livemodal" ) if not retval . result : graph . nodes . remove ( node )
def add edge ( self , info ) : if not info . initialized : return graph = self . request graph ( info . ui . control ) if graph is None : return n nodes = len ( graph . nodes ) I Ds = [ v . ID for v in graph . nodes ] if n nodes == 0 : tail node = Node ( ID = make unique name ( "node" , I Ds ) ) head name = make unique name ( "node" , I Ds + [ tail node . ID ] ) head node = Node ( ID = head name ) elif n nodes == 1 : tail node = graph . nodes [ 0 ] head node = Node ( ID = make unique name ( "node" , I Ds ) ) else : tail node = graph . nodes [ 0 ] head node = graph . nodes [ 1 ] edge = Edge ( tail node , head node , nodes = graph . nodes ) retval = edge . edit traits ( parent = info . ui . control , kind = "livemodal" ) if retval . result : graph . edges . append ( edge )
def add subgraph ( self , info ) : if not info . initialized : return graph = self . request graph ( info . ui . control ) if graph is not None : subgraph = Subgraph ( ) #root=graph, parent=graph) retval = subgraph . edit traits ( parent = info . ui . control , kind = "livemodal" ) if retval . result : graph . subgraphs . append ( subgraph )
def add cluster ( self , info ) : if not info . initialized : return graph = self . request graph ( info . ui . control ) if graph is not None : cluster = Cluster ( ) #root=graph, parent=graph) retval = cluster . edit traits ( parent = info . ui . control , kind = "livemodal" ) if retval . result : graph . clusters . append ( cluster )
def godot options ( self , info ) : if info . initialized : self . edit traits ( parent = info . ui . control , kind = "livemodal" , view = "options view" )
def configure dot code ( self , info ) : if not info . initialized : return self . dot code = str ( self . model ) retval = self . edit traits ( parent = info . ui . control , kind = "livemodal" , view = "dot code view" )
def on exit ( self , info ) : if self . prompt on exit : # and (not is ok): retval = confirm ( parent = info . ui . control , message = "Exit Godot?" , title = "Confirm exit" , default = YES ) if retval == YES : self . on close ( info ) else : self . on close ( info )
def save to file like ( self , flo , format = None , * * kwargs ) : format = self . format if format is None else format save = getattr ( self , "save %s" % format , None ) if save is None : raise Value Error ( "Unknown format '%s'." % format ) save ( flo , * * kwargs )
def save to file ( self , filename , format = None , * * kwargs ) : if format is None : # try to derive protocol from file extension format = format from extension ( filename ) with file ( filename , 'wb' ) as fp : self . save to file like ( fp , format , * * kwargs )
def add node ( self , node or ID , * * kwds ) : if not isinstance ( node or ID , Node ) : node ID = str ( node or ID ) if node ID in self . nodes : node = self . nodes [ self . nodes . index ( node ID ) ] else : if self . default node is not None : node = self . default node . clone traits ( copy = "deep" ) node . ID = node ID else : node = Node ( node ID ) self . nodes . append ( node ) else : node = node or ID if node in self . nodes : node = self . nodes [ self . nodes . index ( node or ID ) ] else : self . nodes . append ( node ) node . set ( * * kwds ) return node
def delete node ( self , node or ID ) : if isinstance ( node or ID , Node ) : #            name = node or ID.ID node = node or ID else : #            name = node or ID node = self . get node ( node or ID ) if node is None : raise Value Error ( "Node %s does not exists" % node or ID ) #        try: #            del self.nodes[name] #        except: #            raise Value Error("Node %s does not exists" % name) #        self.nodes = [n for n in self.nodes if n.ID != name] #        idx = self.nodes.index(name) #        return self.nodes.pop(idx) self . nodes . remove ( node )
def get node ( self , ID ) : for node in self . nodes : if node . ID == str ( ID ) : return node return None
def delete edge ( self , tail node or ID , head node or ID ) : if isinstance ( tail node or ID , Node ) : tail node = tail node or ID else : tail node = self . get node ( tail node or ID ) if isinstance ( head node or ID , Node ) : head node = head node or ID else : head node = self . get node ( head node or ID ) if ( tail node is None ) or ( head node is None ) : return None for i , edge in enumerate ( self . edges ) : if ( edge . tail node == tail node ) and ( edge . head node == head node ) : edge = self . edges . pop ( i ) return edge return None
def add edge ( self , tail node or ID , head node or ID , * * kwds ) : tail node = self . add node ( tail node or ID ) head node = self . add node ( head node or ID ) # Only top level graphs are directed and/or strict. if "directed" in self . trait names ( ) : directed = self . directed else : directed = False if self . default edge is not None : edge = self . default edge . clone traits ( copy = "deep" ) edge . tail node = tail node edge . head node = head node edge . conn = "->" if directed else "--" edge . set ( * * kwds ) else : edge = Edge ( tail node , head node , directed , * * kwds ) if "strict" in self . trait names ( ) : if not self . strict : self . edges . append ( edge ) else : self . edges . append ( edge ) # FIXME: Implement strict graphs. #                raise Not Implemented Error else : self . edges . append ( edge )
def add subgraph ( self , subgraph or ID ) : if not isinstance ( subgraph or ID , ( godot . subgraph . Subgraph , godot . cluster . Cluster ) ) : subgraph ID = str ( subgraph or ID ) if subgraph or ID . startswith ( "cluster" ) : subgraph = godot . cluster . Cluster ( ID = subgraph ID ) else : subgraph = godot . subgraph . Subgraph ( ID = subgraph ID ) else : subgraph = subgraph or ID subgraph . default node = self . default node subgraph . default edge = self . default edge #        subgraph.level = self.level + 1 #        subgraph.padding += self.padding if isinstance ( subgraph , godot . subgraph . Subgraph ) : self . subgraphs . append ( subgraph ) elif isinstance ( subgraph , godot . cluster . Cluster ) : self . clusters . append ( subgraph ) else : raise return subgraph
def program changed ( self , new ) : progs = self . progs if not progs . has key ( prog ) : logger . warning ( 'Graph Viz\'s executable "%s" not found' % prog ) if not os . path . exists ( progs [ prog ] ) or not os . path . isfile ( progs [ prog ] ) : logger . warning ( "Graph Viz's executable '%s' is not a " "file or doesn't exist" % progs [ prog ] )
def set node lists ( self , new ) : for edge in self . edges : edge . nodes = self . nodes
def parse dot file ( filename ) : parser = Godot Data Parser ( ) graph = parser . parse dot file ( filename ) del parser return graph
def parse dot file ( self , file or filename ) : if isinstance ( file or filename , basestring ) : file = None try : file = open ( file or filename , "rb" ) data = file . read ( ) except : print "Could not open %s." % file or filename return None finally : if file is not None : file . close ( ) else : file = file or filename data = file . read ( ) return self . parse dot data ( data )
def build top graph ( self , tokens ) : # Get basic graph information. strict = tokens [ 0 ] == 'strict' graphtype = tokens [ 1 ] directed = graphtype == 'digraph' graphname = tokens [ 2 ] # Build the graph graph = Graph ( ID = graphname , strict = strict , directed = directed ) self . graph = self . build graph ( graph , tokens [ 3 ] )
def build graph ( self , graph , tokens ) : subgraph = None for element in tokens : cmd = element [ 0 ] if cmd == ADD NODE : cmd , nodename , opts = element graph . add node ( nodename , * * opts ) elif cmd == ADD EDGE : cmd , src , dest , opts = element srcport = destport = "" if isinstance ( src , tuple ) : srcport = src [ 1 ] src = src [ 0 ] if isinstance ( dest , tuple ) : destport = dest [ 1 ] dest = dest [ 0 ] graph . add edge ( src , dest , tailport = srcport , headport = destport , * * opts ) elif cmd in [ ADD GRAPH TO NODE EDGE , ADD GRAPH TO GRAPH EDGE , ADD NODE TO GRAPH EDGE ] : cmd , src , dest , opts = element srcport = destport = "" if isinstance ( src , tuple ) : srcport = src [ 1 ] if isinstance ( dest , tuple ) : destport = dest [ 1 ] if not ( cmd == ADD NODE TO GRAPH EDGE ) : if cmd == ADD GRAPH TO NODE EDGE : src = subgraph else : src = prev subgraph dest = subgraph else : dest = subgraph src is graph = isinstance ( src , ( Subgraph , Cluster ) ) dst is graph = isinstance ( dst , ( Subgraph , Cluster ) ) if src is graph : src nodes = src . nodes else : src nodes = [ src ] if dst is graph : dst nodes = dst . nodes else : dst nodes = [ dst ] for src node in src nodes : for dst node in dst nodes : graph . add edge ( from node = src node , to node = dst node , tailport = srcport , headport = destport , * * kwds ) elif cmd == SET GRAPH ATTR : graph . set ( * * element [ 1 ] ) elif cmd == SET DEF NODE ATTR : graph . default node . set ( * * element [ 1 ] ) elif cmd == SET DEF EDGE ATTR : graph . default edge . set ( * * element [ 1 ] ) elif cmd == SET DEF GRAPH ATTR : graph . default graph . set ( * * element [ 1 ] ) elif cmd == ADD SUBGRAPH : cmd , name , elements = element if subgraph : prev subgraph = subgraph if name . startswith ( "cluster" ) : cluster = Cluster ( ID = name ) cluster = self . build graph ( cluster , elements ) graph . add cluster ( cluster ) else : subgraph = Subgraph ( ID = name ) subgraph = self . build graph ( subgraph , elements ) graph . add subgraph ( subgraph ) return graph
def format duration ( seconds ) : units , divider = get time units and multiplier ( seconds ) seconds *= divider return "%.3f %s" % ( seconds , units )
def on path ( self , new ) : self . name = basename ( new ) self . graph = self . editor input . load ( )
def get children ( self , object ) : children = [ ] children . extend ( object . subgraphs ) children . extend ( object . clusters ) children . extend ( object . nodes ) children . extend ( object . edges ) return children
def append child ( self , object , child ) : if isinstance ( child , Subgraph ) : object . subgraphs . append ( child ) elif isinstance ( child , Cluster ) : object . clusters . append ( child ) elif isinstance ( child , Node ) : object . nodes . append ( child ) elif isinstance ( child , Edge ) : object . edges . append ( child ) else : pass
def insert child ( self , object , index , child ) : if isinstance ( child , Subgraph ) : object . subgraphs . insert ( index , child ) elif isinstance ( child , Cluster ) : object . clusters . insert ( index , child ) elif isinstance ( child , Node ) : object . nodes . insert ( index , child ) elif isinstance ( child , Edge ) : object . edges . insert ( index , child ) else : pass
def delete child ( self , object , index ) : if isinstance ( child , Subgraph ) : object . subgraphs . pop ( index ) elif isinstance ( child , Cluster ) : object . clusters . pop ( index ) elif isinstance ( child , Node ) : object . nodes . pop ( index ) elif isinstance ( child , Edge ) : object . edges . pop ( index ) else : pass
def get label ( self , object ) : label = self . label if label [ : 1 ] == '=' : return label [ 1 : ] label = xgetattr ( object , label , '' ) if self . formatter is None : return label return self . formatter ( object , label )
def set label ( self , object , label ) : label name = self . label if label name [ : 1 ] != '=' : xsetattr ( object , label name , label )
def add listeners ( self ) : object = self . value canvas = self . factory . canvas if canvas is not None : for name in canvas . node children : object . on trait change ( self . nodes replaced , name ) object . on trait change ( self . nodes changed , name + " items" ) for name in canvas . edge children : object . on trait change ( self . edges replaced , name ) object . on trait change ( self . edges changed , name + " items" ) else : raise Value Error ( "Graph canvas not set for graph editor." )
def nodes replaced ( self , object , name , old , new ) : self . delete nodes ( old ) self . add nodes ( new )
def nodes changed ( self , object , name , undefined , event ) : self . delete nodes ( event . removed ) self . add nodes ( event . added )
def delete nodes ( self , features ) : graph = self . graph if graph is not None : for feature in features : graph . delete node ( id ( feature ) ) graph . arrange all ( )
def edges replaced ( self , object , name , old , new ) : self . delete edges ( old ) self . add edges ( new )
def edges changed ( self , object , name , undefined , event ) : self . delete edges ( event . removed ) self . add edges ( event . added )
def delete edges ( self , features ) : graph = self . graph if graph is not None : for feature in features : for graph edge in self . factory . edges : if feature . class in graph edge . edge for : tail feature = getattr ( feature , graph edge . tail name ) head feature = getattr ( feature , graph edge . head name ) graph . delete edge ( id ( tail feature ) , id ( head feature ) ) graph . arrange all ( )
def arrange all ( self ) : # FIXME: Circular reference avoidance. import godot . dot data parser import godot . graph graph = godot . graph . Graph ( ID = "g" , directed = True ) self . conn = "->" graph . edges . append ( self ) xdot data = graph . create ( format = "xdot" ) #        print "XDOT DATA:", xdot data parser = godot . dot data parser . Godot Data Parser ( ) ndata = xdot data . replace ( '\\\n' , '' ) tokens = parser . dotparser . parse String ( ndata ) [ 0 ] for element in tokens [ 3 ] : cmd = element [ 0 ] if cmd == "add edge" : cmd , src , dest , opts = element self . set ( * * opts )
def parse xdot directive ( self , name , new ) : parser = Xdot Attr Parser ( ) components = parser . parse xdot data ( new ) # The absolute coordinate of the drawing container wrt graph origin. x1 = min ( [ c . x for c in components ] ) y1 = min ( [ c . y for c in components ] ) print "X1/Y1:" , name , x1 , y1 # Components are positioned relative to their container. This # function positions the bottom-left corner of the components at # their origin rather than relative to the graph. #        move to origin( components ) for c in components : if isinstance ( c , Ellipse ) : component . x origin -= x1 component . y origin -= y1 #                c.position = [ c.x - x1, c.y - y1 ] elif isinstance ( c , ( Polygon , B Spline ) ) : print "Points:" , c . points c . points = [ ( t [ 0 ] - x1 , t [ 1 ] - y1 ) for t in c . points ] print "Points:" , c . points elif isinstance ( c , Text ) : #                font = str to font( str(c.pen.font) ) c . text x , c . text y = c . x - x1 , c . y - y1 container = Container ( auto size = True , position = [ x1 , y1 ] , bgcolor = "yellow" ) container . add ( * components ) if name == " draw " : self . drawing = container elif name == " hdraw " : self . arrowhead drawing = container else : raise
def on drawing ( self , object , name , old , new ) : attrs = [ "drawing" , "arrowhead drawing" ] others = [ getattr ( self , a ) for a in attrs if ( a != name ) and ( getattr ( self , a ) is not None ) ] x , y = self . component . position print "POS:" , x , y , self . component . position abs x = [ d . x + x for d in others ] abs y = [ d . y + y for d in others ] print "ABS:" , abs x , abs y # Assume that he new drawing is positioned relative to graph origin. x1 = min ( abs x + [ new . x ] ) y1 = min ( abs y + [ new . y ] ) print "DRAW:" , new . position new . position = [ new . x - x1 , new . y - y1 ] print "DRAW:" , new . position #        for i, b in enumerate( others ): #            self.drawing.position = [100, 100] #            self.drawing.request redraw() #            print "OTHER:", b.position, abs x[i] - x1 #            b.position = [ abs x[i] - x1, abs y[i] - y1 ] #            b.x = 50 #            b.y = 50 #            print "OTHER:", b.position, abs x[i], x1 #        for attr in attrs: #            if attr != name: #                if getattr(self, attr) is not None: #                    drawing = getattr(self, attr) #                    drawing.position = [50, 50] if old is not None : self . component . remove ( old ) if new is not None : self . component . add ( new ) print "POS NEW:" , self . component . position self . component . position = [ x1 , y1 ] print "POS NEW:" , self . component . position self . component . request redraw ( ) print "POS NEW:" , self . component . position
def node factory ( * * row factory kw ) : if " table editor " in row factory kw : graph = row factory kw [ " table editor " ] . object ID = make unique name ( "n" , [ node . ID for node in graph . nodes ] ) del row factory kw [ " table editor " ] return godot . node . Node ( ID ) else : return godot . node . Node ( uuid . uuid4 ( ) . hex [ : 6 ] )
def edge factory ( * * row factory kw ) : if " table editor " in row factory kw : table editor = row factory kw [ " table editor " ] graph = table editor . object ID = make unique name ( "node" , [ node . ID for node in graph . nodes ] ) n nodes = len ( graph . nodes ) I Ds = [ v . ID for v in graph . nodes ] if n nodes == 0 : tail node = godot . Node ( ID = make unique name ( "n" , I Ds ) ) head node = godot . Node ( ID = make unique name ( "n" , I Ds ) ) elif n nodes == 1 : tail node = graph . nodes [ 0 ] head node = godot . Node ( ID = make unique name ( "n" , I Ds ) ) else : tail node = graph . nodes [ 0 ] head node = graph . nodes [ 1 ] return godot . edge . Edge ( tail node , head node , nodes = graph . nodes ) else : return None
def start ( self , context ) : self . config [ 'alias' ] = self . alias safe config = dict ( self . config ) del safe config [ 'host' ] log . info ( "Connecting Mongo Engine database layer." , extra = dict ( uri = redact uri ( self . config [ 'host' ] ) , config = self . config , ) ) self . connection = connect ( * * self . config )
def prepare ( self , context ) : context . db [ self . alias ] = Mongo Engine Proxy ( self . connection )
def arrange all ( self ) : # FIXME: Circular reference avoidance. import godot . dot data parser import godot . graph graph = godot . graph . Graph ( ID = "g" ) graph . add node ( self ) print "GRAPH DOT:\n" , str ( graph ) xdot data = graph . create ( format = "xdot" ) print "XDOT DATA:\n" , xdot data parser = godot . dot data parser . Godot Data Parser ( ) #        parser.parse dot data(xdot data) flat data = xdot data . replace ( '\\\n' , '' ) tokens = parser . dotparser . parse String ( flat data ) [ 0 ] for element in tokens [ 3 ] : print "TOK:" , element cmd = element [ 0 ] if cmd == 'add node' : cmd , nodename , opts = element assert nodename == self . ID print "OPTIONS:" , opts self . set ( * * opts )
def parse xdot drawing directive ( self , new ) : components = Xdot Attr Parser ( ) . parse xdot data ( new ) max x = max ( [ c . bounds [ 0 ] for c in components ] + [ 1 ] ) max y = max ( [ c . bounds [ 1 ] for c in components ] + [ 1 ] ) pos x = min ( [ c . x for c in components ] ) pos y = min ( [ c . y for c in components ] ) move to origin ( components ) container = Container ( auto size = True , position = [ pos x - self . pos [ 0 ] , pos y - self . pos [ 1 ] ] , bgcolor = "blue" ) #        self.bounds = bounds=[max x, max y] #        container = Container(fit window=False, auto size=True, bgcolor="blue") container . add ( * components ) self . drawing = container
def drawing changed ( self , old , new ) : if old is not None : self . component . remove ( old ) if new is not None : #            new.bgcolor="pink" self . component . add ( new ) w , h = self . component . bounds self . component . position = [ self . pos [ 0 ] - ( w / 2 ) , self . pos [ 1 ] - ( h / 2 ) ] #        self.component.position = [ self.pos[0], self.pos[1] ] self . component . request redraw ( )
def on position change ( self , new ) : w , h = self . component . bounds self . pos = tuple ( [ new [ 0 ] + ( w / 2 ) , new [ 1 ] + ( h / 2 ) ] )
def pos changed ( self , new ) : w , h = self . component . bounds self . component . position = [ new [ 0 ] - ( w / 2 ) , new [ 1 ] - ( h / 2 ) ] #        self.component.position = list( new ) self . component . request redraw ( )
def highlight info ( ctx , style ) : click . secho ( "The following styles are available to choose from:" , fg = "green" ) click . echo ( list ( pygments . styles . get all styles ( ) ) ) click . echo ( ) click . secho ( f'The following CSS for the "{style}" style can be customized:' , fg = "green" ) click . echo ( pygments . formatters . Html Formatter ( style = style ) . get style defs ( ) )
def draw mainlayer ( self , gc , view bounds = None , mode = "default" ) : gc . save state ( ) try : #            self. draw bounds(gc) if len ( self . points ) >= 2 : # Set the drawing parameters. gc . set fill color ( self . pen . fill color ) gc . set stroke color ( self . pen . color ) gc . set line width ( self . pen . line width ) # Draw the path. gc . begin path ( ) #                x0 = self.points[0][0] - self.x #                y0 = self.points[0][1] + self.y #                gc.move to(x0, y0) #                offset points = [(x-self.x, y+self.y) for x, y in self.points] gc . lines ( self . points ) gc . close path ( ) if self . filled : gc . draw path ( self . inside rule ) else : gc . stroke path ( ) finally : gc . restore state ( )
def is in ( self , point x , point y ) : point array = array ( ( ( point x , point y ) , ) ) vertices = array ( self . points ) winding = self . inside rule == "winding" result = points in polygon ( point array , vertices , winding ) return result [ 0 ]
def draw mainlayer ( self , gc , view bounds = None , mode = "default" ) : if not self . points : return gc . save state ( ) try : gc . set fill color ( self . pen . fill color ) gc . set line width ( self . pen . line width ) gc . set stroke color ( self . pen . color ) gc . begin path ( ) start x , start y = self . points [ 0 ] gc . move to ( start x , start y ) for triple in nsplit ( self . points [ 1 : ] , 3 ) : x1 , y1 = triple [ 0 ] x2 , y2 = triple [ 1 ] end x , end y = triple [ 2 ] gc . curve to ( x1 , y1 , x2 , y2 , end x , end y ) # One point overlap gc . move to ( end x , end y ) gc . stroke path ( ) finally : gc . restore state ( )
def connect ( self , context ) : if debug : log . info ( "Connecting " + self . engine . partition ( ':' ) [ 0 ] + " database layer." , extra = dict ( uri = redact uri ( self . uri , self . protect ) , config = self . config , alias = self . alias , ) ) self . connection = context . db [ self . alias ] = self . connector ( self . uri , * * self . config )
def handle event ( self , event , * args , * * kw ) : for engine in self . engines . values ( ) : if hasattr ( engine , event ) : getattr ( engine , event ) ( * args , * * kw )
def get full page url ( self , page number , scheme = None ) : args = dict ( request . view args , external = True , ) if scheme is not None : args [ ' scheme' ] = scheme if page number != 1 : args [ 'page' ] = page number return url for ( request . endpoint , * * args )
def render prev next links ( self , scheme = None ) : output = '' if self . has prev : output += '<link rel="prev" href="{}" />\n' . format ( self . get full page url ( self . prev , scheme = scheme ) ) if self . has next : output += '<link rel="next" href="{}" />\n' . format ( self . get full page url ( self . next , scheme = scheme ) ) return Markup ( output )
def render seo links ( self , scheme = None ) : out = self . render prev next links ( scheme = scheme ) if self . total pages == 1 : out += self . render canonical link ( scheme = scheme ) return out
def content type matches ( candidate , pattern ) : def wildcard compare ( type spec , type pattern ) : return type pattern == '*' or type spec == type pattern return ( wildcard compare ( candidate . content type , pattern . content type ) and wildcard compare ( candidate . content subtype , pattern . content subtype ) )
def ensure dir ( path ) : try : log . info ( 'Ensuring directory exists: %s' % path ) os . makedirs ( path ) except OS Error : if not os . path . isdir ( path ) : raise
def list dataset uris ( cls , base uri , config path ) : storage account name = generous parse uri ( base uri ) . netloc blobservice = get blob service ( storage account name , config path ) containers = blobservice . list containers ( include metadata = True ) uri list = [ ] for c in containers : admin metadata = c . metadata uri = cls . generate uri ( admin metadata [ 'name' ] , admin metadata [ 'uuid' ] , base uri ) uri list . append ( uri ) return uri list
def list overlay names ( self ) : overlay names = [ ] for blob in self . blobservice . list blobs ( self . uuid , prefix = self . overlays key prefix ) : overlay file = blob . name . rsplit ( '/' , 1 ) [ - 1 ] overlay name , ext = overlay file . split ( '.' ) overlay names . append ( overlay name ) return overlay names
def iter item handles ( self ) : blob generator = self . blobservice . list blobs ( self . uuid , include = 'metadata' ) for blob in blob generator : if 'type' in blob . metadata : if blob . metadata [ 'type' ] == 'item' : handle = blob . metadata [ 'relpath' ] yield handle
def luhn check ( card number ) : sum = 0 num digits = len ( card number ) oddeven = num digits & 1 for count in range ( 0 , num digits ) : digit = int ( card number [ count ] ) if not ( ( count & 1 ) ^ oddeven ) : digit *= 2 if digit > 9 : digit -= 9 sum += digit return ( sum % 10 ) == 0
def remove namespaces ( root ) : for elem in root . getiterator ( ) : if not hasattr ( elem . tag , 'find' ) : continue i = elem . tag . find ( '}' ) if i >= 0 : elem . tag = elem . tag [ i + 1 : ] objectify . deannotate ( root , cleanup namespaces = True )
def merge ( self , new dict ) : actions = new dict . pop ( "actions" ) for action in actions : self . add action ( action ) self . dict . update ( new dict )
def execute actions ( self , cwd ) : self . execute globals ( cwd ) for action in self . actions : logger . info ( "executing {}" . format ( action ) ) p = subprocess . Popen ( action , shell = True , cwd = cwd ) p . wait ( )
def add details ( self , message ) : msg = message # Try to append Flask request details try : from flask import request url = request . url method = request . method endpoint = request . endpoint # Obscure password field and prettify a little bit form dict = dict ( request . form ) for key in form dict : if key . lower ( ) in error reporting obscured fields : form dict [ key ] = '******' elif len ( form dict [ key ] ) == 1 : form dict [ key ] = form dict [ key ] [ 0 ] form = pprint . pformat ( form dict ) . replace ( '\n' , '\n          ' ) msg = '%s\n Request:\n\nurl:      %s\nmethod:   %s\nendpoint: %s\nform:     %s\n' % ( msg , url , method , endpoint , form ) except Exception : traceback . print exc ( ) # Try to append the session try : from flask import session from flask . json import JSON Encoder session str = json . dumps ( dict ( * * session ) , indent = 2 , cls = JSON Encoder ) msg = '%s\n Session:\n\n%s\n' % ( msg , session str ) except Exception : traceback . print exc ( ) return msg
def get context ( self , value ) : context = super ( Rendition Aware Struct Block , self ) . get context ( value ) context [ 'image rendition' ] = self . rendition . image rendition or 'original' return context
def set ( self , k , v ) : k = k . lstrip ( '/' ) url = '{}/{}' . format ( self . endpoint , k ) r = requests . put ( url , data = str ( v ) ) if r . status code != 200 or r . json ( ) is not True : raise KV Store Error ( 'PUT returned {}' . format ( r . status code ) )
def get ( self , k , wait = False , wait index = False , timeout = '5m' ) : k = k . lstrip ( '/' ) url = '{}/{}' . format ( self . endpoint , k ) params = { } if wait : params [ 'index' ] = wait index params [ 'wait' ] = timeout r = requests . get ( url , params = params ) if r . status code == 404 : raise Key Does Not Exist ( "Key " + k + " does not exist" ) if r . status code != 200 : raise KV Store Error ( 'GET returned {}' . format ( r . status code ) ) try : return base64 . b64decode ( r . json ( ) [ 0 ] [ 'Value' ] ) except Type Error as e : # Value was empty and wild None appeared return ""
def recurse ( self , k , wait = False , wait index = None , timeout = '5m' ) : k = k . lstrip ( '/' ) url = '{}/{}' . format ( self . endpoint , k ) params = { } params [ 'recurse' ] = 'true' if wait : params [ 'wait' ] = timeout if not wait index : params [ 'index' ] = self . index ( k , recursive = True ) else : params [ 'index' ] = wait index r = requests . get ( url , params = params ) if r . status code == 404 : raise Key Does Not Exist ( "Key " + k + " does not exist" ) if r . status code != 200 : raise KV Store Error ( 'GET returned {}' . format ( r . status code ) ) entries = { } for e in r . json ( ) : if e [ 'Value' ] : entries [ e [ 'Key' ] ] = base64 . b64decode ( e [ 'Value' ] ) else : entries [ e [ 'Key' ] ] = '' return entries
def delete ( self , k , recursive = False ) : k = k . lstrip ( '/' ) url = '{}/{}' . format ( self . endpoint , k ) params = { } if recursive : params [ 'recurse' ] = '' r = requests . delete ( url , params = params ) if r . status code != 200 : raise KV Store Error ( 'DELETE returned {}' . format ( r . status code ) )
def add months ( months , timestamp = datetime . datetime . utcnow ( ) ) : month = timestamp . month new month = month + months years = 0 while new month < 1 : new month += 12 years -= 1 while new month > 12 : new month -= 12 years += 1 # month = timestamp.month year = timestamp . year + years try : return datetime . datetime ( year , new month , timestamp . day , timestamp . hour , timestamp . minute , timestamp . second ) except Value Error : # This means that the day exceeds the last day of the month, i.e. it is 30th March, and we are finding the day # 1 month ago, and it is trying to return 30th February if months > 0 : # We are adding, so use the first day of the next month new month += 1 if new month > 12 : new month -= 12 year += 1 return datetime . datetime ( year , new month , 1 , timestamp . hour , timestamp . minute , timestamp . second ) else : # We are subtracting - use the last day of the same month new day = calendar . monthrange ( year , new month ) [ 1 ] return datetime . datetime ( year , new month , new day , timestamp . hour , timestamp . minute , timestamp . second )
def add months to date ( months , date ) : month = date . month new month = month + months years = 0 while new month < 1 : new month += 12 years -= 1 while new month > 12 : new month -= 12 years += 1 # month = timestamp.month year = date . year + years try : return datetime . date ( year , new month , date . day ) except Value Error : # This means that the day exceeds the last day of the month, i.e. it is 30th March, and we are finding the day # 1 month ago, and it is trying to return 30th February if months > 0 : # We are adding, so use the first day of the next month new month += 1 if new month > 12 : new month -= 12 year += 1 return datetime . datetime ( year , new month , 1 ) else : # We are subtracting - use the last day of the same month new day = calendar . monthrange ( year , new month ) [ 1 ] return datetime . datetime ( year , new month , new day )
def is christmas period ( ) : now = datetime . date . today ( ) if now . month != 12 : return False if now . day < 15 : return False if now . day > 27 : return False return True
def from csv ( self , label column = 'labels' ) : df = pd . read csv ( self . path , header = 0 ) X = df . loc [ : , df . columns != label column ] . to dict ( 'records' ) X = map dict list ( X , if func = lambda k , v : v and math . isfinite ( v ) ) y = list ( df [ label column ] . values ) return X , y
def from json ( self ) : with gzip . open ( '%s.gz' % self . path , 'rt' ) if self . gz else open ( self . path ) as file : return list ( map ( list , zip ( * json . load ( file ) ) ) ) [ : : - 1 ]
def restore data ( self , data dict ) : session [ self . base key ] = data dict self . data dict = session [ self . base key ]
def verify block ( self , block type , block ) : if block type in self . registry : raise Already Registered ( "A block has already been registered to the {} `block type` " "in the registry. Either unregister that block before trying " "to register this block under a different `block type`" . format ( block type ) ) if not isinstance ( block , Block ) : raise Invalid Block ( "The block you tried register to {} is invalid. Only " "instances of `wagtail.wagtailcore.blocks.Block` may be " "registered with the the block registry." . format ( block type ) )
def register block ( self , block type , block ) : self . verify block ( block type , block ) self . registry [ block type ] = block
def connect ( self ) : SCOPES = 'https://www.googleapis.com/auth/drive' store = file . Storage ( 'drive credentials.json' ) creds = store . get ( ) if not creds or creds . invalid : try : flow = client . flow from clientsecrets ( 'client secret.json' , SCOPES ) except Invalid Client Secrets Error : log . error ( 'ERROR: Could not find client secret.json in current directory, please obtain it from the API console.' ) return creds = tools . run flow ( flow , store ) self . connection = build ( 'drive' , 'v3' , http = creds . authorize ( Http ( ) ) ) response = self . connection . files ( ) . list ( q = "name='Music' and mime Type='application/vnd.google-apps.folder' and trashed=false" ) . execute ( ) try : folder id = response . get ( 'files' , [ ] ) [ 0 ] [ 'id' ] except Index Error : log . warning ( 'Music folder is missing. Creating it.' ) folder metadata = { 'name' : 'Music' , 'mime Type' : 'application/vnd.google-apps.folder' } folder = self . connection . files ( ) . create ( body = folder metadata , fields = 'id' ) . execute ( )
def connect ( self ) : if self . music folder is None : music folder = os . path . join ( os . path . expanduser ( '~' ) , 'Music' ) if not os . path . exists ( music folder ) : os . makedirs ( music folder ) self . music folder = music folder
def write sky params to file ( self ) : inp file = self . sky file + ' params.txt' lg . info ( 'Writing Inputs to file : ' + inp file ) f = open ( inp file , 'w' ) f . write ( 'verbose= ' + str ( self . verbose ) + '\n' ) f . write ( 'band count= ' + str ( self . num bands ) + '\n' ) f . write ( 'band centres data= ' ) f . write ( "," . join ( [ str ( wave ) for wave in self . wavelengths ] ) + '\n' ) f . write ( 'partition= ' + self . partition + '\n' ) f . write ( 'vn= ' + str ( self . vn ) + '\n' ) f . write ( 'hn= ' + str ( self . hn ) + '\n' ) f . write ( 'rdif= ' + str ( self . sky r dif ) + '\n' ) f . write ( 'theta points= ' ) f . write ( "," . join ( [ str ( theta ) for theta in self . theta points ] ) + '\n' ) f . write ( 'type= ' + self . sky type + '\n' ) f . write ( 'azimuth= ' + str ( self . sky azimuth ) + '\n' ) f . write ( 'zenith= ' + str ( self . sky zenith ) + '\n' ) f . write ( 'sky save fp= ' + inp file . strip ( ' params.txt' ) + '\n' ) f . write ( 'sky image save fp= ' + self . sky file + '.ppm' + '\n' ) f . write ( 'sky image size= 256' + '\n' ) if self . sky type == 'hlideal' : f . write ( 'C= ' + str ( self . sky c ) + '\n' ) f . write ( 'rdif= ' + str ( self . sky r dif ) + '\n' ) f . flush ( ) f . close ( )
def write surf params to file ( self ) : inp file = self . water surface file + ' params.txt' lg . info ( 'Writing Inputs to file : ' + inp file ) if self . surf state == 'flat' : # this is the only one that currently works. lg . info ( 'Surface Type is :: flat' ) f = open ( inp file , 'w' ) f . write ( 'verbose= ' + str ( self . verbose ) + '\n' ) f . write ( 'band count= ' + str ( self . num bands ) + '\n' ) f . write ( 'band centres data= ' ) f . write ( "," . join ( [ str ( wave ) for wave in self . wavelengths ] ) + '\n' ) f . write ( 'partition= ' + self . partition + '\n' ) f . write ( 'vn= ' + str ( self . vn ) + '\n' ) f . write ( 'hn= ' + str ( self . hn ) + '\n' ) f . write ( 'theta points= ' ) f . write ( "," . join ( [ str ( theta ) for theta in self . theta points ] ) + '\n' ) f . write ( 'type= ' + self . iface type + '\n' ) f . write ( 'refrac index 0= ' + str ( self . iface 0 ri ) + '\n' ) f . write ( 'refrac index 1= ' + str ( self . iface 1 ri ) + '\n' ) f . write ( 'wind speed= ' + str ( self . wind speed ) + '\n' ) f . write ( 'wind direc= ' + str ( self . wind direc ) + '\n' ) f . write ( 'crosswind vertices= ' + str ( self . crosswind vertices ) + '\n' ) f . write ( 'upwind vertices= ' + str ( self . upwind vertices ) + '\n' ) f . write ( 'surface size= ' + str ( self . surface size ) + '\n' ) f . write ( 'surface radius=' + str ( self . surface radius ) + '\n' ) f . write ( 'target size= ' + str ( self . target size ) + '\n' ) f . write ( 'rays per quad= ' + str ( self . rays per quad ) + '\n' ) f . write ( 'surface count= ' + str ( self . surface count ) + '\n' ) f . write ( 'azimuthally average= ' + str ( self . azimuthally average ) + '\n' ) f . write ( 'surface save fp= ' + inp file . strip ( ' params.txt' ) + '\n' ) f . flush ( ) f . close ( )
def write phase params to file ( self ) : inp file = os . path . join ( os . path . join ( self . input path , 'phase files' ) , self . phase function file ) + ' params.txt' lg . info ( 'Writing Inputs to file : ' + inp file ) if self . iop type == 'isotropic' or 'isotropic integ' or 'petzold' or 'pure water ' : lg . info ( 'Iop type is :: ' + self . iop type ) f = open ( inp file , 'w' ) f . write ( 'verbose = ' + str ( self . verbose ) + '\n' ) f . write ( 'band count = ' + str ( self . num bands ) + '\n' ) f . write ( 'band centres data = ' ) f . write ( "," . join ( [ str ( wave ) for wave in self . wavelengths ] ) + '\n' ) f . write ( 'partition = ' + self . partition + '\n' ) f . write ( 'vn = ' + str ( self . vn ) + '\n' ) f . write ( 'hn = ' + str ( self . hn ) + '\n' ) f . write ( 'theta points = ' ) f . write ( "," . join ( [ str ( theta ) for theta in self . theta points ] ) + '\n' ) f . write ( 'type = ' + self . iop type + '\n' ) f . write ( 'phase func save fp = ' + inp file . strip ( ' params.txt' ) + '\n' ) f . flush ( ) f . close ( )
def update filenames ( self ) : self . sky file = os . path . abspath ( os . path . join ( os . path . join ( self . input path , 'sky files' ) , 'sky ' + self . sky state + ' z' + str ( self . sky zenith ) + ' a' + str ( self . sky azimuth ) + ' ' + str ( self . num bands ) + ' ' + self . ds code ) )
def string to float list ( string var ) : try : return [ float ( s ) for s in string var . strip ( '[' ) . strip ( ']' ) . split ( ', ' ) ] except : return [ float ( s ) for s in string var . strip ( '[' ) . strip ( ']' ) . split ( ',' ) ]
def set handler ( self , signals , handler = signal . SIG DFL ) : for sig in signals : self . log . debug ( "Creating handler for signal: {0}" . format ( sig ) ) signal . signal ( sig , handler )
def pseudo handler ( self , signum , frame ) : self . log . warn ( "Received sigal {0} but system is already busy processing a previous signal, current frame: {1}" . format ( signum , str ( frame ) ) )
def default handler ( self , signum , frame ) : self . log . debug ( "Signal handler called with signal: {0}" . format ( signum ) ) # 1. If signal is HUP restart the python process # 2. If signal is TERM, INT or QUIT we try to cleanup then exit with -1 # 3. If signal is STOP or TSTP we pause # 4. If signal is CONT or USR1 we continue # 5. If signal is INFO we print status # 6. If signal is USR2 we we abort and then exit with -1 if signum in self . restart signals : self . set handler ( self . handled signals , self . pseudo handler ) self . cleanup ( ) os . execl ( 'python' , 'python' , * sys . argv ) elif signum in self . abort signals : self . abort ( signum ) elif signum in self . pause signals : self . pause ( signum ) elif signum in self . resume signals : self . resume ( signum ) elif signum in self . status signals : self . status ( signum ) elif signum in self . error signals : self . log . error ( 'Signal handler received error signal from an external process, aborting' ) self . abort ( signum ) else : self . log . error ( "Unhandled signal received: {0}" . format ( signum ) ) raise
def status ( self , signum ) : self . log . debug ( 'Signal handler got status signal' ) new status callbacks = [ ] for status call in self . status callbacks : # If callback is non persistent we remove it try : self . log . debug ( "Calling {0}({1},{2})" . format ( status call [ 'function' ] . name , status call [ 'args' ] , status call [ 'kwargs' ] ) ) except Attribute Error : self . log . debug ( "Calling unbound function/method {0}" . format ( str ( status call ) ) ) apply ( status call [ 'function' ] , status call [ 'args' ] , status call [ 'kwargs' ] ) if status call [ 'persistent' ] : new status callbacks . append ( status call ) self . status callbacks = new status callbacks self . resume ( signum )
def unreg event ( self , event list , event ) : try : self . log . debug ( "Removing event {0}({1},{2})" . format ( event [ 'function' ] . name , event [ 'args' ] , event [ 'kwargs' ] ) ) except Attribute Error : self . log . debug ( "Removing event {0}" . format ( str ( event ) ) ) try : event list . remove ( event ) except Value Error : try : self . log . warn ( "Unable to remove event {0}({1},{2}) , not found in list: {3}" . format ( event [ 'function' ] . name , event [ 'args' ] , event [ 'kwargs' ] , event list ) ) except Attribute Error : self . log . debug ( "Unable to remove event {0}" . format ( str ( event ) ) ) raise Key Error ( 'Unable to unregister the specified event from the signals specified' )
def sig from partial ( self , inst ) : self . pargl = list ( inst . pargl ) self . kargl = list ( inst . kargl ) self . def argv = inst . def argv . copy ( ) self . var pargs = inst . var pargs self . var kargs = inst . var kargs
def vlq2int ( data ) : # The VLQ is little-endian. byte = ord ( data . read ( 1 ) ) value = byte & 0x7F shift = 1 while byte & 0x80 != 0 : byte = ord ( data . read ( 1 ) ) value = ( ( byte & 0x7F ) << shift * 7 ) | value shift += 1 return value
def parse header ( self ) : header = Ordered Dict ( ) user data header = self . archive . header [ 'user data header' ] [ 'content' ] if re . search ( r'Star Craft II replay' , user data header ) : user data header = String IO . String IO ( user data header ) user data header . seek ( 30 ) # Just skip the beginning. header . update ( read table ( user data header , [ 'release flag' , 'major version' , 'minor version' , 'maintenance version' , 'build number' , 'unknown' , 'unknown' , 'duration' ] ) ) # Some post processing is required. header [ 'version' ] = '%s.%s.%s.%s' % ( header [ 'major version' ] , header [ 'minor version' ] , header [ 'maintenance version' ] , header [ 'build number' ] ) if not header [ 'release flag' ] : header [ 'version' ] += ' (dev)' # Duration is actually stored as 1/16th of a seconds. Go figure. header [ 'duration' ] /= 16 else : raise Value Error ( "The given file is not a Star Craft II replay." ) return header
def get duration ( self , seconds ) : duration = "" minutes , seconds = divmod ( seconds , 60 ) if minutes >= 60 : hours , minutes = divmod ( minutes , 60 ) duration = "%sh " % hours duration += "%sm %ss" % ( minutes , seconds ) return duration
def print details ( self ) : print 'Map      ' , self . map print 'Duration ' , self . duration print 'Version  ' , self . version print 'Team  Player       Race       Color' print '-----------------------------------' for player in self . players : print '{team:<5} {name:12} {race:10} {color}' . format ( * * player )
def data ( self ) : self . batch name value = self . ui . batch name value . text ( ) self . saa values = self . ui . saa values . text ( ) self . sza values = self . ui . sza values . text ( ) self . p values = self . ui . p values . text ( ) self . x value = self . ui . x value . text ( ) self . y value = self . ui . y value . text ( ) self . g value = self . ui . g value . text ( ) self . s value = self . ui . s value . text ( ) self . z value = self . ui . z value . text ( ) self . wavelength values = self . ui . wavelength values . text ( ) self . verbose value = self . ui . verbose value . text ( ) self . phytoplankton path = self . ui . phyto path . text ( ) self . bottom path = self . ui . bottom path . text ( ) self . executive path = self . ui . exec path . text ( ) self . nb cpu = self . ui . nb cpu . current Text ( ) self . report parameter value = str ( self . ui . report parameter value . text ( ) )
def search file result ( self ) : if self . ui . tab Widget . current Index ( ) == Tab Widget . NORMAL MODE : self . result file = self . file dialog . get Open File Name ( caption = str ( "Open Report File" ) , directory = "./outputs" ) if not self . result file == '' : self . ui . show all curves . set Disabled ( False ) self . ui . show grid . set Disabled ( False ) self . data processing ( ) self . display the graphic ( self . num line , self . wavelength , self . data wanted , self . information ) self . authorized display = True
def write to file ( self ) : bt = Batch File ( self . batch name value , self . p values , self . x value , self . y value , self . g value , self . s value , self . z value , self . wavelength values , self . verbose value , self . phytoplankton path , self . bottom path , self . nb cpu , self . executive path , self . saa values , self . sza values , self . report parameter value ) # bt.write batch to file(str(self.batch name value + " batch.txt")) bt . write batch to file ( str ( self . batch name value + " batch.txt" ) )
def data processing ( self ) : the file name = str ( self . result file ) the file = open ( the file name , 'r' ) lines = the file . readlines ( ) # We put all lines in an array and we put each cell of the line in a column. lines array = [ ] for line in lines : line = line . split ( ',' ) # Each time there is a tabulation, there is a new cell lines array . append ( line ) labels line = lines array [ 0 ] cell labels line = 0 # Iterator on each cell of the line labels line. flag = True # Become FALSE when we find the word which separate data from wavelength values. try : while flag : # While it is TRUE, so if the word doesn't match, it's an infinite loop, if "wave length (nm)" in labels line [ cell labels line ] : index = labels line . index ( labels line [ cell labels line ] ) # Find the index of the string searched. flag = False else : cell labels line += 1 except Index Error : # In case of an infinite loop. raise sys . exit ( "Warning : There is no value named 'wavelength' in the file used to plot curves. " "So, I can't separate data to plot curves and data about tests linking with these curves." ) self . information = [ ] # This array will contain the data displayed under the curves. data wavelength = [ ] # This array will contain the data to plot curves. self . num line = 0 # Iterator on each line of lines array, # The array containing data about information and wavelength. for line in lines array : cell line = 0 # Iterator on each cell of the line. self . information . append ( [ ] ) data wavelength . append ( [ ] ) while cell line < len ( line ) : if cell line < index : self . information [ self . num line ] . append ( line [ cell line ] ) elif cell line > index : data wavelength [ self . num line ] . append ( line [ cell line ] ) cell line += 1 self . num line += 1 # We transform wavelengths from strings to floats. line wavelength = 0 # Iterator on each line of data wavelength for row data wavelength in data wavelength : row data wavelength = [ float ( item . strip ( '\n' ) . strip ( '\"' ) ) for item in row data wavelength ] data wavelength [ line wavelength ] = row data wavelength line wavelength += 1 self . wavelength = data wavelength [ 0 ] # The first line contains wavelength self . data wanted = data wavelength [ 1 : ] # The others contain data useful to plot curves. the file . close ( )
def display error message ( self ) : self . ui . error label . set Scaled Contents ( True ) # Warning image shown. self . ui . error text label . show ( ) # Warning message shown. self . ui . error text label . set Style Sheet ( 'color: red' )
def hide error message ( self ) : self . ui . error label . set Scaled Contents ( False ) # Warning image hiden. self . ui . error text label . hide ( )
def run ( self ) : print ( 'Executing planarrad' ) # If we are not in the reverse mode : if self . ui . tab Widget . current Index ( ) == Tab Widget . NORMAL MODE : self . data ( ) self . check values ( ) if self . without error == False : self . display error message ( ) elif self . without error == True : self . is running = True self . hide error message ( ) self . write to file ( ) os . chdir ( './' ) self . progress bar ( ) this dir = os . path . dirname ( os . path . realpath ( file ) ) . rstrip ( 'gui/' ) batch file = os . path . join ( this dir , "inputs/batch files/" + str ( self . batch name value ) + " batch.txt" ) print ( batch file ) self . p = subprocess . Popen ( [ "./planarrad.py -i " + batch file ] , shell = True ) if self . ui . progress Bar . value ( ) == 100 : self . display the graphic ( self . num line , self . wavelength , self . data wanted , self . information )
def cancel planarrad ( self ) : if ( self . is running == True ) & ( self . ui . tab Widget . current Index ( ) == Tab Widget . NORMAL MODE ) : cancel = Qt Gui . Q Message Box . question ( self . ui . cancel , 'Cancel Planar Rad' , "Are you sure to cancel ?" , Qt Gui . Q Message Box . Yes , Qt Gui . Q Message Box . No ) if cancel == Qt Gui . Q Message Box . Yes : self . is running = False os . kill ( self . p . pid , signal . SIGTERM ) print ( "Necessary to check if cancel planarrad works well !" ) self . ui . progress Bar . reset ( ) else : pass
def quit ( self ) : if self . is running == True : warning planarrad running = Qt Gui . Q Message Box . warning ( self . ui . quit , 'Warning !' , "Planar Rad is running. Stop it before quit !" , Qt Gui . Q Message Box . Ok ) else : quit = Qt Gui . Q Message Box . question ( self . ui . quit , 'Quit Planar Rad' , "Are you sure to quit ?" , Qt Gui . Q Message Box . Yes , Qt Gui . Q Message Box . No ) if quit == Qt Gui . Q Message Box . Yes : Qt Gui . q App . quit ( )
def open log file ( self ) : # webbrowser.open('https://marrabld.github.io/planarradpy/') f = open ( os . path . expanduser ( '~/.planarradpy/log/libplanarradpy.log' ) ) # self.ui Log.text Edit.set Text(str(f.readlines())) self . ui Log . text Edit . set Plain Text ( str ( f . read ( ) ) ) self . log window . show ( )
def open documentation ( self ) : # webbrowser.open('https://marrabld.github.io/planarradpy/') window = Window ( ) html = Qt Core . Q Url . from Local File ( os . path . join ( os . getcwd ( ) , './docs/ build/html/index.html' ) ) #open('./docs/ build/html/index.html').read() #window.show() window . view . load ( html ) window . show ( ) window . exec ( )
def prerequisite actions ( self ) : self . hide error message ( ) self . ui . show all curves . set Disabled ( True ) self . ui . sens . set Disabled ( True ) self . ui . show grid . set Disabled ( True ) pathname = os . path . dirname ( sys . argv [ 0 ] ) path = os . path . abspath ( pathname ) # self.phytoplankton path = self.ui.phyto path.set Text(path.replace('gui', 'inputs/iop files')) # self.bottom path = self.ui.bottom path.set Text(path.replace('gui', 'inputs/bottom files')) # self.executive path = self.ui.exec path.set Text("Decide where will be 'jude2 install/bin'") self . verbose value = self . ui . verbose value . set Text ( "6" ) self . report parameter value = self . ui . report parameter value . set Text ( "Rrs" ) self . ui . progress Bar . reset ( )
def click ( self , event ) : if event . button == 3 : if self . ui . tab Widget . current Index ( ) == Tab Widget . NORMAL MODE : self . pos = Qt Gui . Q Cursor ( ) . pos ( ) self . graphic context menu ( self . pos )
def mouse move ( self , event ) : if ( self . ui . tab Widget . current Index ( ) == Tab Widget . NORMAL MODE ) : self . pos X = event . xdata self . pos Y = event . ydata self . graphic target ( self . pos X , self . pos Y )
def graphic target ( self , x , y ) : if self . authorized display == True : try : self . display the graphic ( self . num line , self . wavelength , self . data wanted , self . information ) self . ui . mouse coordinate . set Text ( "(%0.3f, %0.3f)" % ( x , y ) ) except : pass
def sign ( self , privkey ) : if self . v : raise Invalid Signature ( "already signed" ) if privkey in ( 0 , '' , '\x00' * 32 ) : raise Invalid Signature ( "Zero privkey cannot sign" ) rawhash = sha3 ( rlp . encode ( self , self . class . exclude ( [ 'v' , 'r' , 's' ] ) ) ) if len ( privkey ) == 64 : privkey = encode privkey ( privkey , 'bin' ) pk = Private Key ( privkey , raw = True ) signature = pk . ecdsa recoverable serialize ( pk . ecdsa sign recoverable ( rawhash , raw = True ) ) signature = signature [ 0 ] + chr ( signature [ 1 ] ) self . v = ord ( signature [ 64 ] ) + 27 self . r = big endian to int ( signature [ 0 : 32 ] ) self . s = big endian to int ( signature [ 32 : 64 ] ) self . sender = None return self
def hash ( self ) : if self . sender is None : raise Missing Signature Error ( ) class Hash Serializable ( rlp . Serializable ) : fields = [ ( field , sedes ) for field , sedes in self . fields if field not in ( 'v' , 'r' , 's' ) ] + [ ( ' sender' , binary ) ] sedes = None return sha3 ( rlp . encode ( self , Hash Serializable ) )
def check ( self ) : if not self . is valid : return True test = ( self . has quorum , self . has quorum possible , self . has noquorum ) assert 1 == len ( [ x for x in test if x is not None ] ) return True
def validate votes ( self , validators H , validators prev H ) : assert self . sender def check ( lockset , validators ) : if not lockset . num eligible votes == len ( validators ) : raise Invalid Proposal Error ( 'lockset num eligible votes mismatch' ) for v in lockset : if v . sender not in validators : raise Invalid Proposal Error ( 'invalid signer' ) if self . round lockset : check ( self . round lockset , validators H ) check ( self . signing lockset , validators prev H ) return True
def validate votes ( self , validators H ) : assert self . sender if not self . round lockset . num eligible votes == len ( validators H ) : raise Invalid Proposal Error ( 'round lockset num eligible votes mismatch' ) for v in self . round lockset : if v . sender not in validators H : raise Invalid Proposal Error ( 'invalid signer' )
def issue funds ( ctx , amount = 'uint256' , rtgs hash = 'bytes32' , returns = STATUS ) : # allocate new issue as result of a new cash entry ctx . accounts [ ctx . msg sender ] += amount ctx . issued amounts [ ctx . msg sender ] += amount # Store hash(rtgs) ctx . Issuance ( ctx . msg sender , rtgs hash , amount ) return OK
def last lock ( self ) : rs = list ( self . rounds ) assert len ( rs ) < 2 or rs [ 0 ] > rs [ 1 ] # FIXME REMOVE for r in self . rounds : # is sorted highest to lowest if self . rounds [ r ] . lock is not None : return self . rounds [ r ] . lock
def last voted blockproposal ( self ) : for r in self . rounds : if isinstance ( self . rounds [ r ] . proposal , Block Proposal ) : assert isinstance ( self . rounds [ r ] . lock , Vote ) if self . rounds [ r ] . proposal . blockhash == self . rounds [ r ] . lock . blockhash : return self . rounds [ r ] . proposal
def last valid lockset ( self ) : for r in self . rounds : ls = self . rounds [ r ] . lockset if ls . is valid : return ls return None
def get timeout ( self ) : if self . timeout time is not None or self . proposal : return now = self . cm . chainservice . now round timeout = Consensus Manager . round timeout round timeout factor = Consensus Manager . round timeout factor delay = round timeout * round timeout factor ** self . round self . timeout time = now + delay return delay
def on proposal ( self , proposal , proto ) : assert isinstance ( proto , HDC Protocol ) assert isinstance ( proposal , Proposal ) if proposal . height >= self . cm . height : assert proposal . lockset . is valid self . last active protocol = proto
def mk privkeys ( num ) : privkeys = [ ] assert num <= num colors for i in range ( num ) : j = 0 while True : k = sha3 ( str ( j ) ) a = privtoaddr ( k ) an = big endian to int ( a ) if an % num colors == i : break j += 1 privkeys . append ( k ) return privkeys
def delay ( self , sender , receiver , packet , add delay = 0 ) : bw = min ( sender . ul bandwidth , receiver . dl bandwidth ) delay = sender . base latency + receiver . base latency delay += len ( packet ) / bw delay += add delay return delay
def deliver ( self , sender , receiver , packet ) : to = Consensus Manager . round timeout assert to > 0 print "in slow transport deliver" super ( Slow Transport , self ) . deliver ( sender , receiver , packet , add delay = to )
def chain nac proxy ( chain , sender , contract address , value = 0 ) : klass = registry [ contract address ] . im self assert issubclass ( klass , Native ABI Contract ) def mk method ( method ) : def m ( s , * args ) : data = abi encode args ( method , args ) block = chain . head candidate output = test call ( block , sender , contract address , data ) if output is not None : return abi decode return vals ( method , output ) return m class cproxy ( object ) : pass for m in klass . abi methods ( ) : setattr ( cproxy , m . func . func name , mk method ( m ) ) return cproxy ( )
def address to native contract class ( self , address ) : assert isinstance ( address , bytes ) and len ( address ) == 20 assert self . is instance address ( address ) nca = self . native contract address prefix + address [ - 4 : ] return self . native contracts [ nca ]
def update ( self , data ) : if data not in self . filter : self . filter . append ( data ) if len ( self . filter ) > self . max items : self . filter . pop ( 0 ) return True else : self . filter . append ( self . filter . pop ( 0 ) ) return False
def on receive transactions ( self , proto , transactions ) : log . debug ( '----------------------------------' ) log . debug ( 'remote transactions received' , count = len ( transactions ) , remote id = proto ) def add txs ( ) : for tx in transactions : self . add transaction ( tx , origin = proto ) gevent . spawn ( add txs )
def img from vgg ( x ) : x = x . transpose ( ( 1 , 2 , 0 ) ) x [ : , : , 0 ] += 103.939 x [ : , : , 1 ] += 116.779 x [ : , : , 2 ] += 123.68 x = x [ : , : , : : - 1 ] # to RGB return x
def img to vgg ( x ) : x = x [ : , : , : : - 1 ] # to BGR x [ : , : , 0 ] -= 103.939 x [ : , : , 1 ] -= 116.779 x [ : , : , 2 ] -= 123.68 x = x . transpose ( ( 2 , 0 , 1 ) ) return x
def get f layer ( self , layer name ) : inputs = [ self . net input ] if self . learning phase is not None : inputs . append ( K . learning phase ( ) ) return K . function ( inputs , [ self . get layer output ( layer name ) ] )
def get layer output ( self , name ) : if not name in self . f layer outputs : layer = self . net . get layer ( name ) self . f layer outputs [ name ] = layer . output return self . f layer outputs [ name ]
def get features ( self , x , layers ) : if not layers : return None inputs = [ self . net . input ] if self . learning phase is not None : inputs . append ( self . learning phase ) f = K . function ( inputs , [ self . get layer output ( layer name ) for layer name in layers ] ) feature outputs = f ( [ x ] ) features = dict ( zip ( layers , feature outputs ) ) return features
def fix compile ( remove flags ) : import distutils . ccompiler def fix compile ( self , sources , output dir = None , macros = None , include dirs = None , debug = 0 , extra preargs = None , extra postargs = None , depends = None ) : for flag in remove flags : if flag in self . compiler so : self . compiler so . remove ( flag ) macros , objects , extra postargs , pp opts , build = self . setup compile ( output dir , macros , include dirs , sources , depends , extra postargs ) cc args = self . get cc args ( pp opts , debug , extra preargs ) for obj in objects : try : src , ext = build [ obj ] except Key Error : continue self . compile ( obj , src , ext , cc args , extra postargs , pp opts ) return objects distutils . ccompiler . C Compiler . compile = fix compile
def do table ( self , line ) : if len ( line ) > 0 : if line . strip ( ) . lower ( ) == "on" : log . write ( "Table ON" ) self . table output = True return elif line . strip ( ) . lower ( ) == "off" : log . write ( "Table OFF" ) self . table output = False return log . write ( "Table output: {}" . format ( "ON" if self . table output else "OFF" ) )
def float with multiplier ( string ) : match = re float with multiplier . search ( string ) if not match or not match . group ( 'num' ) : raise Value Error ( 'String "{}" is not numeric!' . format ( string ) ) num = float ( match . group ( 'num' ) ) multi = match . group ( 'multi' ) if multi : try : num *= multipliers [ multi ] except Key Error : raise Value Error ( 'Unknown multiplier: {}' . format ( multi ) ) return num
def specific gains ( string ) : if not string : return { } gains = { } for gain in string . split ( ',' ) : amp name , value = gain . split ( '=' ) gains [ amp name . strip ( ) ] = float ( value . strip ( ) ) return gains
def device settings ( string ) : if not string : return { } settings = { } for setting in string . split ( ',' ) : setting name , value = setting . split ( '=' ) settings [ setting name . strip ( ) ] = value . strip ( ) return settings
def wrap ( text , indent = '    ' ) : wrapper = textwrap . Text Wrapper ( width = int ( os . environ . get ( 'COLUMNS' , 80 ) ) , initial indent = indent , subsequent indent = indent ) return '\n' . join ( wrapper . wrap ( text ) )
def detect devices ( soapy args = '' ) : devices = simplesoapy . detect devices ( soapy args , as string = True ) text = [ ] text . append ( 'Detected Soapy SDR devices:' ) if devices : for i , d in enumerate ( devices ) : text . append ( '  {}' . format ( d ) ) else : text . append ( '  No devices found!' ) return ( devices , '\n' . join ( text ) )
def device info ( soapy args = '' ) : text = [ ] try : device = simplesoapy . Soapy Device ( soapy args ) text . append ( 'Selected device: {}' . format ( device . hardware ) ) text . append ( '  Available RX channels:' ) text . append ( '    {}' . format ( ', ' . join ( str ( x ) for x in device . list channels ( ) ) ) ) text . append ( '  Available antennas:' ) text . append ( '    {}' . format ( ', ' . join ( device . list antennas ( ) ) ) ) text . append ( '  Available tunable elements:' ) text . append ( '    {}' . format ( ', ' . join ( device . list frequencies ( ) ) ) ) text . append ( '  Available amplification elements:' ) text . append ( '    {}' . format ( ', ' . join ( device . list gains ( ) ) ) ) text . append ( '  Available device settings:' ) for key , s in device . list settings ( ) . items ( ) : text . append ( wrap ( '{} ... {} - {} (default: {})' . format ( key , s [ 'name' ] , s [ 'description' ] , s [ 'value' ] ) ) ) text . append ( '  Available stream arguments:' ) for key , s in device . list stream args ( ) . items ( ) : text . append ( wrap ( '{} ... {} - {} (default: {})' . format ( key , s [ 'name' ] , s [ 'description' ] , s [ 'value' ] ) ) ) text . append ( '  Allowed gain range [d B]:' ) text . append ( '    {:.2f} - {:.2f}' . format ( * device . get gain range ( ) ) ) text . append ( '  Allowed frequency range [M Hz]:' ) text . append ( '    {:.2f} - {:.2f}' . format ( * [ x / 1e6 for x in device . get frequency range ( ) ] ) ) text . append ( '  Allowed sample rates [M Hz]:' ) rates = [ ] for r in device . list sample rates ( ) : if r [ 0 ] == r [ 1 ] : rates . append ( '{:.2f}' . format ( r [ 0 ] / 1e6 ) ) else : rates . append ( '{:.2f} - {:.2f}' . format ( r [ 0 ] / 1e6 , r [ 1 ] / 1e6 ) ) text . append ( wrap ( ', ' . join ( rates ) ) ) text . append ( '  Allowed bandwidths [M Hz]:' ) bandwidths = [ ] for b in device . list bandwidths ( ) : if b [ 0 ] == b [ 1 ] : bandwidths . append ( '{:.2f}' . format ( b [ 0 ] / 1e6 ) ) else : bandwidths . append ( '{:.2f} - {:.2f}' . format ( b [ 0 ] / 1e6 , b [ 1 ] / 1e6 ) ) if bandwidths : text . append ( wrap ( ', ' . join ( bandwidths ) ) ) else : text . append ( '    N/A' ) except Runtime Error : device = None text . append ( 'No devices found!' ) return ( device , '\n' . join ( text ) )
def setup argument parser ( ) : # Fix help formatter width if 'COLUMNS' not in os . environ : os . environ [ 'COLUMNS' ] = str ( shutil . get terminal size ( ) . columns ) parser = argparse . Argument Parser ( prog = 'soapy power' , formatter class = argparse . Raw Description Help Formatter , description = 'Obtain a power spectrum from Soapy SDR devices' , add help = False ) # Fix recognition of optional argements of type float with multiplier parser . negative number matcher = re float with multiplier negative main title = parser . add argument group ( 'Main options' ) main title . add argument ( '-h' , '--help' , action = 'help' , help = 'show this help message and exit' ) main title . add argument ( '-f' , '--freq' , metavar = 'Hz|Hz:Hz' , type = freq or freq range , default = '1420405752' , help = 'center frequency or frequency range to scan, number ' 'can be followed by a k, M or G multiplier (default: %(default)s)' ) output group = main title . add mutually exclusive group ( ) output group . add argument ( '-O' , '--output' , metavar = 'FILE' , type = argparse . File Type ( 'w' ) , default = sys . stdout , help = 'output to file (incompatible with --output-fd, default is stdout)' ) output group . add argument ( '--output-fd' , metavar = 'NUM' , type = int , default = None , help = 'output to existing file descriptor (incompatible with -O)' ) main title . add argument ( '-F' , '--format' , choices = sorted ( writer . formats . keys ( ) ) , default = 'rtl power' , help = 'output format (default: %(default)s)' ) main title . add argument ( '-q' , '--quiet' , action = 'store true' , help = 'limit verbosity' ) main title . add argument ( '--debug' , action = 'store true' , help = 'detailed debugging messages' ) main title . add argument ( '--detect' , action = 'store true' , help = 'detect connected Soapy SDR devices and exit' ) main title . add argument ( '--info' , action = 'store true' , help = 'show info about selected Soapy SDR device and exit' ) main title . add argument ( '--version' , action = 'version' , version = '%(prog)s {}' . format ( version ) ) bins title = parser . add argument group ( 'FFT bins' ) bins group = bins title . add mutually exclusive group ( ) bins group . add argument ( '-b' , '--bins' , type = int , default = 512 , help = 'number of FFT bins (incompatible with -B, default: %(default)s)' ) bins group . add argument ( '-B' , '--bin-size' , metavar = 'Hz' , type = float with multiplier , help = 'bin size in Hz (incompatible with -b)' ) spectra title = parser . add argument group ( 'Averaging' ) spectra group = spectra title . add mutually exclusive group ( ) spectra group . add argument ( '-n' , '--repeats' , type = int , default = 1600 , help = 'number of spectra to average (incompatible with -t and -T, default: %(default)s)' ) spectra group . add argument ( '-t' , '--time' , metavar = 'SECONDS' , type = float , help = 'integration time (incompatible with -T and -n)' ) spectra group . add argument ( '-T' , '--total-time' , metavar = 'SECONDS' , type = float , help = 'total integration time of all hops (incompatible with -t and -n)' ) runs title = parser . add argument group ( 'Measurements' ) runs group = runs title . add mutually exclusive group ( ) runs group . add argument ( '-c' , '--continue' , dest = 'endless' , action = 'store true' , help = 'repeat the measurement endlessly (incompatible with -u and -e)' ) runs group . add argument ( '-u' , '--runs' , type = int , default = 1 , help = 'number of measurements (incompatible with -c and -e, default: %(default)s)' ) runs group . add argument ( '-e' , '--elapsed' , metavar = 'SECONDS' , type = float , help = 'scan session duration (time limit in seconds, incompatible with -c and -u)' ) device title = parser . add argument group ( 'Device settings' ) device title . add argument ( '-d' , '--device' , default = '' , help = 'Soapy SDR device to use' ) device title . add argument ( '-C' , '--channel' , type = int , default = 0 , help = 'Soapy SDR RX channel (default: %(default)s)' ) device title . add argument ( '-A' , '--antenna' , default = '' , help = 'Soapy SDR selected antenna' ) device title . add argument ( '-r' , '--rate' , metavar = 'Hz' , type = float with multiplier , default = 2e6 , help = 'sample rate (default: %(default)s)' ) device title . add argument ( '-w' , '--bandwidth' , metavar = 'Hz' , type = float with multiplier , default = 0 , help = 'filter bandwidth (default: %(default)s)' ) device title . add argument ( '-p' , '--ppm' , type = int , default = 0 , help = 'frequency correction in ppm' ) gain group = device title . add mutually exclusive group ( ) gain group . add argument ( '-g' , '--gain' , metavar = 'd B' , type = float , default = 37.2 , help = 'total gain (incompatible with -G and -a, default: %(default)s)' ) gain group . add argument ( '-G' , '--specific-gains' , metavar = 'STRING' , type = specific gains , default = '' , help = 'specific gains of individual amplification elements ' '(incompatible with -g and -a, example: LNA=28,VGA=12,AMP=0' ) gain group . add argument ( '-a' , '--agc' , action = 'store true' , help = 'enable Automatic Gain Control (incompatible with -g and -G)' ) device title . add argument ( '--lnb-lo' , metavar = 'Hz' , type = float with multiplier , default = 0 , help = 'LNB LO frequency, negative for upconverters (default: %(default)s)' ) device title . add argument ( '--device-settings' , metavar = 'STRING' , type = device settings , default = '' , help = 'Soapy SDR device settings (example: biastee=true)' ) device title . add argument ( '--force-rate' , action = 'store true' , help = 'ignore list of sample rates provided by device and allow any value' ) device title . add argument ( '--force-bandwidth' , action = 'store true' , help = 'ignore list of filter bandwidths provided by device and allow any value' ) device title . add argument ( '--tune-delay' , metavar = 'SECONDS' , type = float , default = 0 , help = 'time to delay measurement after changing frequency (to avoid artifacts)' ) device title . add argument ( '--reset-stream' , action = 'store true' , help = 'reset streaming after changing frequency (to avoid artifacts)' ) crop title = parser . add argument group ( 'Crop' ) crop group = crop title . add mutually exclusive group ( ) crop group . add argument ( '-o' , '--overlap' , metavar = 'PERCENT' , type = float , default = 0 , help = 'percent of overlap when frequency hopping (incompatible with -k)' ) crop group . add argument ( '-k' , '--crop' , metavar = 'PERCENT' , type = float , default = 0 , help = 'percent of crop when frequency hopping (incompatible with -o)' ) perf title = parser . add argument group ( 'Performance options' ) perf title . add argument ( '-s' , '--buffer-size' , type = int , default = 0 , help = 'base buffer size (number of samples, 0 = auto, default: %(default)s)' ) perf title . add argument ( '-S' , '--max-buffer-size' , type = int , default = 0 , help = 'maximum buffer size (number of samples, -1 = unlimited, 0 = auto, default: %(default)s)' ) fft rules group = perf title . add mutually exclusive group ( ) fft rules group . add argument ( '--even' , action = 'store true' , help = 'use only even numbers of FFT bins' ) fft rules group . add argument ( '--pow2' , action = 'store true' , help = 'use only powers of 2 as number of FFT bins' ) perf title . add argument ( '--max-threads' , metavar = 'NUM' , type = int , default = 0 , help = 'maximum number of PSD threads (0 = auto, default: %(default)s)' ) perf title . add argument ( '--max-queue-size' , metavar = 'NUM' , type = int , default = 0 , help = 'maximum size of PSD work queue (-1 = unlimited, 0 = auto, default: %(default)s)' ) perf title . add argument ( '--no-pyfftw' , action = 'store true' , help = 'don\'t use pyfftw library even if it is available (use scipy.fftpack or numpy.fft)' ) other title = parser . add argument group ( 'Other options' ) other title . add argument ( '-l' , '--linear' , action = 'store true' , help = 'linear power values instead of logarithmic' ) other title . add argument ( '-R' , '--remove-dc' , action = 'store true' , help = 'interpolate central point to cancel DC bias (useful only with boxcar window)' ) other title . add argument ( '-D' , '--detrend' , choices = [ 'none' , 'constant' ] , default = 'none' , help = 'remove mean value from data to cancel DC bias (default: %(default)s)' ) other title . add argument ( '--fft-window' , choices = [ 'boxcar' , 'hann' , 'hamming' , 'blackman' , 'bartlett' , 'kaiser' , 'tukey' ] , default = 'hann' , help = 'Welch\'s method window function (default: %(default)s)' ) other title . add argument ( '--fft-window-param' , metavar = 'FLOAT' , type = float , default = None , help = 'shape parameter of window function (required for kaiser and tukey windows)' ) other title . add argument ( '--fft-overlap' , metavar = 'PERCENT' , type = float , default = 50 , help = 'Welch\'s method overlap between segments (default: %(default)s)' ) return parser
def set center freq ( self , center freq ) : psd state = { 'repeats' : 0 , 'freq array' : self . base freq array + self . lnb lo + center freq , 'pwr array' : None , 'update lock' : threading . Lock ( ) , 'futures' : [ ] , } return psd state
def result ( self , psd state ) : freq array = numpy . fft . fftshift ( psd state [ 'freq array' ] ) pwr array = numpy . fft . fftshift ( psd state [ 'pwr array' ] ) if self . crop factor : crop bins half = round ( ( self . crop factor * self . bins ) / 2 ) freq array = freq array [ crop bins half : - crop bins half ] pwr array = pwr array [ crop bins half : - crop bins half ] if psd state [ 'repeats' ] > 1 : pwr array = pwr array / psd state [ 'repeats' ] if self . log scale : pwr array = 10 * numpy . log10 ( pwr array ) return ( freq array , pwr array )
def wait for result ( self , psd state ) : if len ( psd state [ 'futures' ] ) > 1 : concurrent . futures . wait ( psd state [ 'futures' ] ) elif psd state [ 'futures' ] : psd state [ 'futures' ] [ 0 ] . result ( ) return self . result ( psd state )
def update ( self , psd state , samples array ) : freq array , pwr array = simplespectral . welch ( samples array , self . sample rate , nperseg = self . bins , window = self . fft window , noverlap = self . fft overlap bins , detrend = self . detrend ) if self . remove dc : pwr array [ 0 ] = ( pwr array [ 1 ] + pwr array [ - 1 ] ) / 2 with psd state [ 'update lock' ] : psd state [ 'repeats' ] += 1 if psd state [ 'pwr array' ] is None : psd state [ 'pwr array' ] = pwr array else : psd state [ 'pwr array' ] += pwr array
def read ( self , f ) : magic = f . read ( len ( self . magic ) ) if not magic : return None if magic != self . magic : raise Value Error ( 'Magic bytes not found! Read data: {}' . format ( magic ) ) header = self . header . make ( self . header struct . unpack ( f . read ( self . header struct . size ) ) ) pwr array = numpy . fromstring ( f . read ( header . size ) , dtype = 'float32' ) return ( header , pwr array )
def write ( self , f , time start , time stop , start , stop , step , samples , pwr array ) : f . write ( self . magic ) f . write ( self . header struct . pack ( self . version , time start , time stop , start , stop , step , samples , pwr array . nbytes ) ) #pwr array.tofile(f) f . write ( pwr array . tobytes ( ) ) f . flush ( )
def write ( self , psd data or future , time start , time stop , samples ) : try : # Wait for result of future f array , pwr array = psd data or future . result ( ) except Attribute Error : f array , pwr array = psd data or future try : step = f array [ 1 ] - f array [ 0 ] self . formatter . write ( self . output , time start . timestamp ( ) , time stop . timestamp ( ) , f array [ 0 ] , f array [ - 1 ] + step , step , samples , pwr array ) except Exception as e : logging . exception ( 'Error writing to output file: {}' . format ( e ) )
def write ( self , psd data or future , time start , time stop , samples ) : try : # Wait for result of future f array , pwr array = psd data or future . result ( ) except Attribute Error : f array , pwr array = psd data or future self . output . write ( '# soapy power output\n' ) self . output . write ( '# Acquisition start: {}\n' . format ( time start ) ) self . output . write ( '# Acquisition end: {}\n' . format ( time stop ) ) self . output . write ( '#\n' ) self . output . write ( '# frequency [Hz] power spectral density [d B/Hz]\n' ) for f , pwr in zip ( f array , pwr array ) : self . output . write ( '{} {}\n' . format ( f , pwr ) ) self . output . write ( '\n' ) self . output . flush ( )
def write ( self , psd data or future , time start , time stop , samples ) : try : # Wait for result of future f array , pwr array = psd data or future . result ( ) except Attribute Error : f array , pwr array = psd data or future try : step = f array [ 1 ] - f array [ 0 ] row = [ time stop . strftime ( '%Y-%m-%d' ) , time stop . strftime ( '%H:%M:%S' ) , f array [ 0 ] , f array [ - 1 ] + step , step , samples ] row += list ( pwr array ) self . output . write ( '{}\n' . format ( ', ' . join ( str ( x ) for x in row ) ) ) self . output . flush ( ) except Exception as e : logging . exception ( 'Error writing to output file:' )
def time to repeats ( self , bins , integration time ) : return math . ceil ( ( self . device . sample rate * integration time ) / bins )
def freq plan ( self , min freq , max freq , bins , overlap = 0 , quiet = False ) : bin size = self . bins to bin size ( bins ) bins crop = round ( ( 1 - overlap ) * bins ) sample rate crop = ( 1 - overlap ) * self . device . sample rate freq range = max freq - min freq hopping = True if freq range >= sample rate crop else False hop size = self . nearest freq ( sample rate crop , bin size ) hops = math . ceil ( freq range / hop size ) if hopping else 1 min center freq = min freq + ( hop size / 2 ) if hopping else min freq + ( freq range / 2 ) max center freq = min center freq + ( ( hops - 1 ) * hop size ) freq list = [ min center freq + ( i * hop size ) for i in range ( hops ) ] if not quiet : logger . info ( 'overlap: {:.5f}' . format ( overlap ) ) logger . info ( 'bin size: {:.2f} Hz' . format ( bin size ) ) logger . info ( 'bins: {}' . format ( bins ) ) logger . info ( 'bins (after crop): {}' . format ( bins crop ) ) logger . info ( 'sample rate: {:.3f} M Hz' . format ( self . device . sample rate / 1e6 ) ) logger . info ( 'sample rate (after crop): {:.3f} M Hz' . format ( sample rate crop / 1e6 ) ) logger . info ( 'freq range: {:.3f} M Hz' . format ( freq range / 1e6 ) ) logger . info ( 'hopping: {}' . format ( 'YES' if hopping else 'NO' ) ) logger . info ( 'hop size: {:.3f} M Hz' . format ( hop size / 1e6 ) ) logger . info ( 'hops: {}' . format ( hops ) ) logger . info ( 'min center freq: {:.3f} M Hz' . format ( min center freq / 1e6 ) ) logger . info ( 'max center freq: {:.3f} M Hz' . format ( max center freq / 1e6 ) ) logger . info ( 'min freq (after crop): {:.3f} M Hz' . format ( ( min center freq - ( hop size / 2 ) ) / 1e6 ) ) logger . info ( 'max freq (after crop): {:.3f} M Hz' . format ( ( max center freq + ( hop size / 2 ) ) / 1e6 ) ) logger . debug ( 'Frequency hops table:' ) logger . debug ( '  {:8s}      {:8s}      {:8s}' . format ( 'Min:' , 'Center:' , 'Max:' ) ) for f in freq list : logger . debug ( '  {:8.3f} M Hz  {:8.3f} M Hz  {:8.3f} M Hz' . format ( ( f - ( self . device . sample rate / 2 ) ) / 1e6 , f / 1e6 , ( f + ( self . device . sample rate / 2 ) ) / 1e6 , ) ) return freq list
def create buffer ( self , bins , repeats , base buffer size , max buffer size = 0 ) : samples = bins * repeats buffer repeats = 1 buffer size = math . ceil ( samples / base buffer size ) * base buffer size if not max buffer size : # Max buffer size about 100 MB max buffer size = ( 100 * 1024 ** 2 ) / 8 if max buffer size > 0 : max buffer size = math . ceil ( max buffer size / base buffer size ) * base buffer size if buffer size > max buffer size : logger . warning ( 'Required buffer size ({}) will be shrinked to max buffer size ({})!' . format ( buffer size , max buffer size ) ) buffer repeats = math . ceil ( buffer size / max buffer size ) buffer size = max buffer size logger . info ( 'repeats: {}' . format ( repeats ) ) logger . info ( 'samples: {} (time: {:.5f} s)' . format ( samples , samples / self . device . sample rate ) ) if max buffer size > 0 : logger . info ( 'max buffer size (samples): {} (repeats: {:.2f}, time: {:.5f} s)' . format ( max buffer size , max buffer size / bins , max buffer size / self . device . sample rate ) ) else : logger . info ( 'max buffer size (samples): UNLIMITED' ) logger . info ( 'buffer size (samples): {} (repeats: {:.2f}, time: {:.5f} s)' . format ( buffer size , buffer size / bins , buffer size / self . device . sample rate ) ) logger . info ( 'buffer repeats: {}' . format ( buffer repeats ) ) return ( buffer repeats , zeros ( buffer size , numpy . complex64 ) )
def setup ( self , bins , repeats , base buffer size = 0 , max buffer size = 0 , fft window = 'hann' , fft overlap = 0.5 , crop factor = 0 , log scale = True , remove dc = False , detrend = None , lnb lo = 0 , tune delay = 0 , reset stream = False , max threads = 0 , max queue size = 0 ) : if self . device . is streaming : self . device . stop stream ( ) base buffer = self . device . start stream ( buffer size = base buffer size ) self . bins = bins self . repeats = repeats self . base buffer size = len ( base buffer ) self . max buffer size = max buffer size self . buffer repeats , self . buffer = self . create buffer ( bins , repeats , self . base buffer size , self . max buffer size ) self . tune delay = tune delay self . reset stream = reset stream self . psd = psd . PSD ( bins , self . device . sample rate , fft window = fft window , fft overlap = fft overlap , crop factor = crop factor , log scale = log scale , remove dc = remove dc , detrend = detrend , lnb lo = lnb lo , max threads = max threads , max queue size = max queue size ) self . writer = writer . formats [ self . output format ] ( self . output )
def stop ( self ) : if not self . device . is streaming : return self . device . stop stream ( ) self . writer . close ( ) self . bins = None self . repeats = None self . base buffer size = None self . max buffer size = None self . buffer repeats = None self . buffer = None self . tune delay = None self . reset stream = None self . psd = None self . writer = None
def psd ( self , freq ) : if not self . device . is streaming : raise Runtime Error ( 'Streaming is not initialized, you must run setup() first!' ) # Tune to new frequency in main thread logger . debug ( '  Frequency hop: {:.2f} Hz' . format ( freq ) ) t freq = time . time ( ) if self . device . freq != freq : # Deactivate streaming before tuning if self . reset stream : self . device . device . deactivate Stream ( self . device . stream ) # Actually tune to new center frequency self . device . freq = freq # Reactivate straming after tuning if self . reset stream : self . device . device . activate Stream ( self . device . stream ) # Delay reading samples after tuning if self . tune delay : t delay = time . time ( ) while True : self . device . read stream ( ) t delay end = time . time ( ) if t delay end - t delay >= self . tune delay : break logger . debug ( '    Tune delay: {:.3f} s' . format ( t delay end - t delay ) ) else : logger . debug ( '    Same frequency as before, tuning skipped' ) psd state = self . psd . set center freq ( freq ) t freq end = time . time ( ) logger . debug ( '    Tune time: {:.3f} s' . format ( t freq end - t freq ) ) for repeat in range ( self . buffer repeats ) : logger . debug ( '    Repeat: {}' . format ( repeat + 1 ) ) # Read samples from SDR in main thread t acq = time . time ( ) acq time start = datetime . datetime . utcnow ( ) self . device . read stream into buffer ( self . buffer ) acq time stop = datetime . datetime . utcnow ( ) t acq end = time . time ( ) logger . debug ( '      Acquisition time: {:.3f} s' . format ( t acq end - t acq ) ) # Start FFT computation in another thread self . psd . update async ( psd state , numpy . copy ( self . buffer ) ) t final = time . time ( ) if shutdown : break psd future = self . psd . result async ( psd state ) logger . debug ( '    Total hop time: {:.3f} s' . format ( t final - t freq ) ) return ( psd future , acq time start , acq time stop )
def sweep ( self , min freq , max freq , bins , repeats , runs = 0 , time limit = 0 , overlap = 0 , fft window = 'hann' , fft overlap = 0.5 , crop = False , log scale = True , remove dc = False , detrend = None , lnb lo = 0 , tune delay = 0 , reset stream = False , base buffer size = 0 , max buffer size = 0 , max threads = 0 , max queue size = 0 ) : self . setup ( bins , repeats , base buffer size , max buffer size , fft window = fft window , fft overlap = fft overlap , crop factor = overlap if crop else 0 , log scale = log scale , remove dc = remove dc , detrend = detrend , lnb lo = lnb lo , tune delay = tune delay , reset stream = reset stream , max threads = max threads , max queue size = max queue size ) try : freq list = self . freq plan ( min freq - lnb lo , max freq - lnb lo , bins , overlap ) t start = time . time ( ) run = 0 while not shutdown and ( runs == 0 or run < runs ) : run += 1 t run start = time . time ( ) logger . debug ( 'Run: {}' . format ( run ) ) for freq in freq list : # Tune to new frequency, acquire samples and compute Power Spectral Density psd future , acq time start , acq time stop = self . psd ( freq ) # Write PSD to stdout (in another thread) self . writer . write async ( psd future , acq time start , acq time stop , len ( self . buffer ) * self . buffer repeats ) if shutdown : break # Write end of measurement marker (in another thread) write next future = self . writer . write next async ( ) t run = time . time ( ) logger . debug ( '  Total run time: {:.3f} s' . format ( t run - t run start ) ) # End measurement if time limit is exceeded if time limit and ( time . time ( ) - t start ) >= time limit : logger . info ( 'Time limit of {} s exceeded, completed {} runs' . format ( time limit , run ) ) break # Wait for last write to be finished write next future . result ( ) # Debug thread pool queues logging . debug ( 'Number of USB buffer overflow errors: {}' . format ( self . device . buffer overflow count ) ) logging . debug ( 'PSD worker threads: {}' . format ( self . psd . executor . max workers ) ) logging . debug ( 'Max. PSD queue size: {} / {}' . format ( self . psd . executor . max queue size reached , self . psd . executor . max queue size ) ) logging . debug ( 'Writer worker threads: {}' . format ( self . writer . executor . max workers ) ) logging . debug ( 'Max. Writer queue size: {} / {}' . format ( self . writer . executor . max queue size reached , self . writer . executor . max queue size ) ) finally : # Shutdown SDR self . stop ( ) t stop = time . time ( ) logger . info ( 'Total time: {:.3f} s' . format ( t stop - t start ) )
def run cmake ( arg = "" ) : if ds . find executable ( 'cmake' ) is None : print "C Make  is required to build zql" print "Please install cmake version >= 2.8 and re-run setup" sys . exit ( - 1 ) print "Configuring zql build with C Make.... " cmake args = arg try : build dir = op . join ( op . split ( file ) [ 0 ] , 'build' ) dd . mkpath ( build dir ) os . chdir ( "build" ) ds . spawn ( [ 'cmake' , '..' ] + cmake args . split ( ) ) ds . spawn ( [ 'make' , 'clean' ] ) ds . spawn ( [ 'make' ] ) os . chdir ( ".." ) except ds . Distutils Exec Error : print "Error while running cmake" print "run 'setup.py build --help' for build options" print "You may also try editing the settings in C Make Lists.txt file and re-running setup" sys . exit ( - 1 )
def bring gpio interrupt into userspace ( ) : # activate gpio interrupt try : # is it already there? with open ( GPIO INTERRUPT DEVICE VALUE ) : return except IO Error : # no, bring it into userspace with open ( GPIO EXPORT FILE , 'w' ) as export file : export file . write ( str ( GPIO INTERRUPT PIN ) ) wait until file exists ( GPIO INTERRUPT DEVICE VALUE )
def gpio interrupts enable ( self ) : try : bring gpio interrupt into userspace ( ) set gpio interrupt edge ( ) except Timeout as e : raise Interrupt Enable Exception ( "There was an error bringing gpio%d into userspace. %s" % ( GPIO INTERRUPT PIN , e . message ) )
def has errors ( self , form ) : return any ( [ fieldname error for fieldname error in form . errors . keys ( ) if fieldname error in self ] )
def get form kwargs ( self ) : kwargs = super ( Form Containers Mixin , self ) . get form kwargs ( ) kwargs . update ( { 'pack' : "foundation-{}" . format ( self . kwargs . get ( 'foundation version' ) ) } ) return kwargs
def publish ( self ) : return self . publish ( self . args , self . server , self . URI )
def get ( data ) : crc = 0 for byte in array ( 'B' , data ) : crc = ( V Pro CRC . CRC TABLE [ ( crc >> 8 ) ^ byte ] ^ ( ( crc & 0x FF ) << 8 ) ) return crc
def unpack storm date ( date ) : year = ( date & 0x7f ) + 2000 # 7 bits day = ( date >> 7 ) & 0x01f # 5 bits month = ( date >> 12 ) & 0x0f # 4 bits return "%s-%s-%s" % ( year , month , day )
def use rev b archive ( self , records , offset ) : # if pre-determined, return result if type ( self . ARCHIVE REV B ) is bool : return self . ARCHIVE REV B # assume, B and check 'Rec Type' field data = Archive B Struct . unpack from ( records , offset ) if data [ 'Rec Type' ] == 0 : log . info ( 'detected archive rev. B' ) self . ARCHIVE REV B = True else : log . info ( 'detected archive rev. A' ) self . ARCHIVE REV B = False return self . ARCHIVE REV B
def wakeup ( self ) : log . info ( "send: WAKEUP" ) for i in xrange ( 3 ) : self . port . write ( '\n' ) # wakeup device ack = self . port . read ( len ( self . WAKE ACK ) ) # read wakeup string log raw ( 'read' , ack ) if ack == self . WAKE ACK : return raise No Device Exception ( 'Can not access weather station' )
def dmpaft cmd ( self , time fields ) : records = [ ] # convert time stamp fields to buffer tbuf = struct . pack ( '2H' , * time fields ) # 1. send 'DMPAFT' cmd self . cmd ( 'DMPAFT' ) # 2. send time stamp + crc crc = V Pro CRC . get ( tbuf ) crc = struct . pack ( '>H' , crc ) # crc in big-endian format log raw ( 'send' , tbuf + crc ) self . port . write ( tbuf + crc ) # send time stamp + crc ack = self . port . read ( len ( self . ACK ) ) # read ACK log raw ( 'read' , ack ) if ack != self . ACK : return # if bad ack, return # 3. read pre-amble data raw = self . port . read ( Dmp Struct . size ) log raw ( 'read' , raw ) if not V Pro CRC . verify ( raw ) : # check CRC value log raw ( 'send ESC' , self . ESC ) self . port . write ( self . ESC ) # if bad, escape and abort return log raw ( 'send ACK' , self . ACK ) self . port . write ( self . ACK ) # send ACK # 4. loop through all page records dmp = Dmp Struct . unpack ( raw ) log . info ( 'reading %d pages, start offset %d' % ( dmp [ 'Pages' ] , dmp [ 'Offset' ] ) ) for i in xrange ( dmp [ 'Pages' ] ) : # 5. read page data raw = self . port . read ( Dmp Page Struct . size ) log raw ( 'read' , raw ) if not V Pro CRC . verify ( raw ) : # check CRC value log raw ( 'send ESC' , self . ESC ) self . port . write ( self . ESC ) # if bad, escape and abort return log raw ( 'send ACK' , self . ACK ) self . port . write ( self . ACK ) # send ACK # 6. loop through archive records page = Dmp Page Struct . unpack ( raw ) offset = 0 # assume offset at 0 if i == 0 : offset = dmp [ 'Offset' ] * Archive A Struct . size while offset < Archive A Struct . size * 5 : log . info ( 'page %d, reading record at offset %d' % ( page [ 'Index' ] , offset ) ) if self . use rev b archive ( page [ 'Records' ] , offset ) : a = Archive B Struct . unpack from ( page [ 'Records' ] , offset ) else : a = Archive A Struct . unpack from ( page [ 'Records' ] , offset ) # 7. verify that record has valid data, and store if a [ 'Date Stamp' ] != 0xffff and a [ 'Time Stamp' ] != 0xffff : records . append ( a ) offset += Archive A Struct . size log . info ( 'read all pages' ) return records
def weather update ( station , pub sites , interval ) : station . parse ( ) # read weather data # santity check weather data if station . fields [ 'Temp Out' ] > 200 : raise No Sensor Exception ( 'Out of range temperature value: %.1f, check sensors' % ( station . fields [ 'Temp Out' ] , ) ) gust , gust dir = Wind Gust . get ( station , interval ) # upload data in the following order: for ps in pub sites : try : # try block necessary to attempt every publisher ps . set ( pressure = station . fields [ 'Pressure' ] , dewpoint = station . fields [ 'Dew Point' ] , humidity = station . fields [ 'Hum Out' ] , tempf = station . fields [ 'Temp Out' ] , rainin = station . fields [ 'Rain Rate' ] , rainday = station . fields [ 'Rain Day' ] , dateutc = station . fields [ 'Date Stamp Utc' ] , windspeed = station . fields [ 'Wind Speed10Min' ] , winddir = station . fields [ 'Wind Dir' ] , windgust = gust , windgustdir = gust dir , ) ps . publish ( ) except ( Exception ) as e : log . warn ( 'publisher %s: %s' % ( ps . class . name , e ) )
def init log ( quiet , debug ) : from logging . handlers import Sys Log Handler fmt = logging . Formatter ( os . path . basename ( sys . argv [ 0 ] ) + ".%(name)s %(levelname)s - %(message)s" ) facility = Sys Log Handler . LOG DAEMON syslog = Sys Log Handler ( address = '/dev/log' , facility = facility ) syslog . set Formatter ( fmt ) log . add Handler ( syslog ) if not quiet : console = logging . Stream Handler ( ) console . set Formatter ( fmt ) log . add Handler ( console ) log . set Level ( logging . INFO ) if debug : log . set Level ( logging . DEBUG )
def get pub services ( opts ) : sites = [ ] for p key in vars ( opts ) . keys ( ) : args = getattr ( opts , p key ) if p key in PUB SERVICES and args : if isinstance ( args , tuple ) : ps = PUB SERVICES [ p key ] ( * args ) else : ps = PUB SERVICES [ p key ] ( args ) sites . append ( ps ) return sites
def get options ( parser ) : # station services # publication services pub g = optparse . Option Group ( parser , "Publication Services" , , ) pub g . add option ( '-w' , '--wundergound' , nargs = 2 , type = 'string' , dest = 'wug' , help = 'Weather Underground service; WUG=[SID(station ID), PASSWORD]' ) pub g . add option ( '-p' , '--pws' , nargs = 2 , type = 'string' , dest = 'pws' , help = 'PWS service; PWS=[SID(station ID), PASSWORD]' ) pub g . add option ( '-f' , '--file' , nargs = 1 , type = 'string' , dest = 'file' , help = 'Local file; FILE=[FILE NAME]' ) parser . add option group ( pub g ) parser . add option ( '-d' , '--debug' , dest = 'debug' , action = "store true" , default = False , help = 'enable verbose debug logging' ) parser . add option ( '-q' , '--quiet' , dest = 'quiet' , action = "store true" , default = False , help = 'disable all console logging' ) parser . add option ( '-t' , '--tty' , dest = 'tty' , default = '/dev/tty S0' , help = 'set serial port device [/dev/tty S0]' ) parser . add option ( '-n' , '--interval' , dest = 'interval' , default = 60 , type = 'int' , help = 'polling/update interval in seconds [60]' ) return parser . parse args ( )
def set ( self , * * kw ) : self . args = kw log . debug ( self . args )
def publish ( self ) : with open ( self . file name , 'w' ) as fh : for k , v in self . args . iteritems ( ) : buf = String IO . String IO ( ) buf . write ( k ) self . append vals ( buf , v ) fh . write ( buf . getvalue ( ) + '\n' ) buf . close ( )
def init app ( self , app ) : if not hasattr ( app , "extensions" ) : # pragma: no cover app . extensions = { } app . extensions [ "allows" ] = self @ app . before request def start context ( * a , * * k ) : self . overrides . push ( Override ( ) ) self . additional . push ( Additional ( ) ) @ app . after request def cleanup ( response ) : self . clear all overrides ( ) self . clear all additional ( ) return response
def unduplicate field names ( field names ) : res = [ ] for k in field names : if k in res : i = 1 while k + ' ' + str ( i ) in res : i += 1 k += ' ' + str ( i ) res . append ( k ) return res
def get dataframe ( self ) : if pd is None : raise Import Error ( "Try installing Pandas first." ) frame = pd . Data Frame ( self [ : ] , columns = ( self and self . keys ) or [ ] ) return frame
def get widgets sorted ( self ) : result = [ ] for widget name , widget in self . get widgets ( ) . items ( ) : result . append ( ( widget name , widget , widget . position ) ) result . sort ( key = lambda x : x [ 2 ] ) return result
def unregister widget ( self , widget cls ) : if widget cls . name in self . widgets : del self . widgets [ widget cls ( ) . get name ( ) ]
def get last update ( self ) : instance , created = models . Dashboard Widget Last Update . objects . get or create ( widget name = self . get name ( ) ) return instance
def save setting ( self , setting name , value ) : setting = self . get setting ( setting name ) if setting is None : setting = models . Dashboard Widget Settings . objects . create ( widget name = self . get name ( ) , setting name = setting name , value = value ) setting . value = value setting . save ( ) return setting
def format axes ( axes , shape ) : if isinstance ( axes , int ) : axes = ( axes , ) elif isinstance ( axes , list ) or hasattr ( axes , ' iter ' ) : axes = tuple ( axes ) if not isinstance ( axes , tuple ) : raise Value Error ( "axes argument %s in the constructor not specified correctly" % str ( axes ) ) if min ( axes ) < 0 or max ( axes ) > len ( shape ) - 1 : raise Value Error ( "invalid key axes %s given shape %s" % ( str ( axes ) , str ( shape ) ) ) return axes
def wrap ( func , shape , context = None , axis = ( 0 , ) , dtype = None , npartitions = None ) : if isinstance ( shape , int ) : shape = ( shape , ) key shape , value shape = get kv shape ( shape , Construct Spark . format axes ( axis , shape ) ) split = len ( key shape ) # make the keys rdd = context . parallelize ( list ( product ( * [ arange ( x ) for x in key shape ] ) ) , npartitions ) # use a map to make the arrays in parallel rdd = rdd . map ( lambda x : ( x , func ( value shape , dtype , order = 'C' ) ) ) return Bolt Array Spark ( rdd , shape = shape , split = split , dtype = dtype )
def first ( self ) : from bolt . local . array import Bolt Array Local rdd = self . rdd if self . ordered else self . rdd . sort By Key ( ) return Bolt Array Local ( rdd . values ( ) . first ( ) )
def zip with index ( rdd ) : starts = [ 0 ] if rdd . get Num Partitions ( ) > 1 : nums = rdd . map Partitions ( lambda it : [ sum ( 1 for in it ) ] ) . collect ( ) count = sum ( nums ) for i in range ( len ( nums ) - 1 ) : starts . append ( starts [ - 1 ] + nums [ i ] ) else : count = rdd . count ( ) def func ( k , it ) : for i , v in enumerate ( it , starts [ k ] ) : yield v , i return count , rdd . map Partitions With Index ( func )
def wrapped ( f ) : import inspect def extract ( func ) : append = "" args = inspect . getargspec ( func ) for i , a in enumerate ( args . args ) : if i < ( len ( args ) - len ( args . defaults ) ) : append += str ( a ) + ", " else : default = args . defaults [ i - len ( args . defaults ) ] if hasattr ( default , " name " ) : default = default . name else : default = str ( default ) append += str ( a ) + "=" + default + ", " append = append [ : - 2 ] + ")" return append doc = f . doc + "\n" doc += "    local -> array(" + extract ( getattr ( Construct Local , f . name ) ) + "\n" doc += "    spark -> array(" + extract ( getattr ( Construct Spark , f . name ) ) + "\n" f . doc = doc return f
def plotcdf ( x , xmin , alpha ) : x = sort ( x ) n = len ( x ) xcdf = arange ( n , 0 , - 1 , dtype = 'float' ) / float ( n ) q = x [ x >= xmin ] fcdf = ( q / xmin ) ** ( 1 - alpha ) nc = xcdf [ argmax ( x >= xmin ) ] fcdf norm = nc * fcdf loglog ( x , xcdf ) loglog ( q , fcdf norm )
def plotpdf ( x , xmin , alpha , nbins = 30 , dolog = False ) : x = sort ( x ) n = len ( x ) if dolog : hb = hist ( x , bins = logspace ( log10 ( min ( x ) ) , log10 ( max ( x ) ) , nbins ) , log = True ) alpha += 1 else : hb = hist ( x , bins = linspace ( ( min ( x ) ) , ( max ( x ) ) , nbins ) ) h , b = hb [ 0 ] , hb [ 1 ] b = b [ 1 : ] q = x [ x >= xmin ] px = ( alpha - 1 ) / xmin * ( q / xmin ) ** ( - alpha ) arg = argmin ( abs ( b - xmin ) ) norm = mean ( h [ b > xmin ] / ( ( alpha - 1 ) / xmin * ( b [ b > xmin ] / xmin ) ** ( - alpha ) ) ) px = px * norm loglog ( q , px ) gca ( ) . set xlim ( min ( x ) , max ( x ) )
def discrete max likelihood arg ( data , xmin , alpharange = ( 1.5 , 3.5 ) , n alpha = 201 ) : likelihoods = discrete likelihood vector ( data , xmin , alpharange = alpharange , n alpha = n alpha ) Largmax = np . argmax ( likelihoods ) return Largmax
def discrete max likelihood ( data , xmin , alpharange = ( 1.5 , 3.5 ) , n alpha = 201 ) : likelihoods = discrete likelihood vector ( data , xmin , alpharange = alpharange , n alpha = n alpha ) Lmax = np . max ( likelihoods ) return Lmax
def most likely alpha ( data , xmin , alpharange = ( 1.5 , 3.5 ) , n alpha = 201 ) : alpha vector = np . linspace ( alpharange [ 0 ] , alpharange [ 1 ] , n alpha ) return alpha vector [ discrete max likelihood arg ( data , xmin , alpharange = alpharange , n alpha = n alpha ) ]
def plotcdf ( self , x = None , xmin = None , alpha = None , pointcolor = 'k' , dolog = True , zoom = True , pointmarker = '+' , * * kwargs ) : if x is None : x = self . data if xmin is None : xmin = self . xmin if alpha is None : alpha = self . alpha x = np . sort ( x ) n = len ( x ) xcdf = np . arange ( n , 0 , - 1 , dtype = 'float' ) / float ( n ) q = x [ x >= xmin ] fcdf = ( q / xmin ) ** ( 1 - alpha ) nc = xcdf [ argmax ( x >= xmin ) ] fcdf norm = nc * fcdf D location = argmax ( xcdf [ x >= xmin ] - fcdf norm ) pylab . vlines ( q [ D location ] , xcdf [ x >= xmin ] [ D location ] , fcdf norm [ D location ] , color = 'm' , linewidth = 2 , zorder = 2 ) pylab . plot ( [ q [ D location ] ] * 2 , [ xcdf [ x >= xmin ] [ D location ] , fcdf norm [ D location ] ] , color = 'm' , marker = 's' , zorder = 3 ) #plotx = pylab.linspace(q.min(),q.max(),1000) #ploty = (plotx/xmin)**(1-alpha) * nc if dolog : pylab . loglog ( x , xcdf , marker = pointmarker , color = pointcolor , * * kwargs ) pylab . loglog ( q , fcdf norm , 'r' , * * kwargs ) else : pylab . semilogx ( x , xcdf , marker = pointmarker , color = pointcolor , * * kwargs ) pylab . semilogx ( q , fcdf norm , 'r' , * * kwargs ) if zoom : pylab . axis ( [ xmin , x . max ( ) , xcdf . min ( ) , nc ] )
def plot lognormal pdf ( self , * * kwargs ) : if not hasattr ( self , 'lognormal dist' ) : return normalized pdf = self . lognormal dist . pdf ( self . data ) / self . lognormal dist . pdf ( self . data ) . max ( ) min Y , max Y = pylab . gca ( ) . get ylim ( ) pylab . plot ( self . data , normalized pdf * max Y , '.' , * * kwargs )
def plot lognormal cdf ( self , * * kwargs ) : if not hasattr ( self , 'lognormal dist' ) : return x = np . sort ( self . data ) n = len ( x ) xcdf = np . arange ( n , 0 , - 1 , dtype = 'float' ) / float ( n ) lcdf = self . lognormal dist . sf ( x ) D location = argmax ( xcdf - lcdf ) pylab . vlines ( x [ D location ] , xcdf [ D location ] , lcdf [ D location ] , color = 'm' , linewidth = 2 ) pylab . plot ( x , lcdf , ',' , * * kwargs )
def hash sha256 ( self ) : fp plain = hashlib . sha256 ( self . decoded key ) . digest ( ) return ( b"SHA256:" + base64 . b64encode ( fp plain ) . replace ( b"=" , b"" ) ) . decode ( "utf-8" )
def hash sha512 ( self ) : fp plain = hashlib . sha512 ( self . decoded key ) . digest ( ) return ( b"SHA512:" + base64 . b64encode ( fp plain ) . replace ( b"=" , b"" ) ) . decode ( "utf-8" )
def parse long ( cls , data ) : if sys . version < '3' : # this does not exist in python 3 - undefined-variable disabled to make pylint happier. ret = long ( 0 ) # pylint:disable=undefined-variable for byte in data : ret = ( ret << 8 ) + ord ( byte ) else : ret = 0 for byte in data : ret = ( ret << 8 ) + byte return ret
def decode key ( cls , pubkey content ) : try : decoded key = base64 . b64decode ( pubkey content . encode ( "ascii" ) ) except ( Type Error , binascii . Error ) : raise Malformed Data Error ( "Unable to decode the key" ) return decoded key
def parse options ( self , options ) : quote open = False parsed options = { } def parse add single option ( opt ) : """Parses and validates a single option, and adds it to parsed options field.""" if "=" in opt : opt name , opt value = opt . split ( "=" , 1 ) opt value = opt value . replace ( '"' , '' ) else : opt name = opt opt value = True if " " in opt name or not self . OPTION NAME RE . match ( opt name ) : raise Invalid Option Name Error ( "%s is not valid option name." % opt name ) if self . strict mode : for valid opt name , value required in self . OPTIONS SPEC : if opt name . lower ( ) == valid opt name : if value required and opt value is True : raise Missing Mandatory Option Value Error ( "%s is missing mandatory value." % opt name ) break else : raise Unknown Option Name Error ( "%s is unrecognized option name." % opt name ) if opt name not in parsed options : parsed options [ opt name ] = [ ] parsed options [ opt name ] . append ( opt value ) start of current opt = 0 i = 1 # Need to be set for empty options strings for i , character in enumerate ( options ) : if character == '"' : # only double quotes are allowed, no need to care about single quotes quote open = not quote open if quote open : continue if character == "," : opt = options [ start of current opt : i ] parse add single option ( opt ) start of current opt = i + 1 # Data begins after the first space if start of current opt + 1 != i : opt = options [ start of current opt : ] parse add single option ( opt ) if quote open : raise Invalid Options Error ( "Unbalanced quotes." ) return parsed options
def process ssh rsa ( self , data ) : current position , raw e = self . unpack by int ( data , 0 ) current position , raw n = self . unpack by int ( data , current position ) unpacked e = self . parse long ( raw e ) unpacked n = self . parse long ( raw n ) self . rsa = RSA Public Numbers ( unpacked e , unpacked n ) . public key ( default backend ( ) ) self . bits = self . rsa . key size if self . strict mode : min length = self . RSA MIN LENGTH STRICT max length = self . RSA MAX LENGTH STRICT else : min length = self . RSA MIN LENGTH LOOSE max length = self . RSA MAX LENGTH LOOSE if self . bits < min length : raise Too Short Key Error ( "%s key data can not be shorter than %s bits (was %s)" % ( self . key type , min length , self . bits ) ) if self . bits > max length : raise Too Long Key Error ( "%s key data can not be longer than %s bits (was %s)" % ( self . key type , max length , self . bits ) ) return current position
def process ssh dss ( self , data ) : data fields = { } current position = 0 for item in ( "p" , "q" , "g" , "y" ) : current position , value = self . unpack by int ( data , current position ) data fields [ item ] = self . parse long ( value ) q bits = self . bits in number ( data fields [ "q" ] ) p bits = self . bits in number ( data fields [ "p" ] ) if q bits != self . DSA N LENGTH : raise Invalid Key Error ( "Incorrect DSA key parameters: bits(p)=%s, q=%s" % ( self . bits , q bits ) ) if self . strict mode : min length = self . DSA MIN LENGTH STRICT max length = self . DSA MAX LENGTH STRICT else : min length = self . DSA MIN LENGTH LOOSE max length = self . DSA MAX LENGTH LOOSE if p bits < min length : raise Too Short Key Error ( "%s key can not be shorter than %s bits (was %s)" % ( self . key type , min length , p bits ) ) if p bits > max length : raise Too Long Key Error ( "%s key data can not be longer than %s bits (was %s)" % ( self . key type , max length , p bits ) ) dsa parameters = DSA Parameter Numbers ( data fields [ "p" ] , data fields [ "q" ] , data fields [ "g" ] ) self . dsa = DSA Public Numbers ( data fields [ "y" ] , dsa parameters ) . public key ( default backend ( ) ) self . bits = self . dsa . key size return current position
def process ecdsa sha ( self , data ) : current position , curve information = self . unpack by int ( data , 0 ) if curve information not in self . ECDSA CURVE DATA : raise Not Implemented Error ( "Invalid curve type: %s" % curve information ) curve , hash algorithm = self . ECDSA CURVE DATA [ curve information ] current position , key data = self . unpack by int ( data , current position ) try : # data starts with \x04, which should be discarded. ecdsa key = ecdsa . Verifying Key . from string ( key data [ 1 : ] , curve , hash algorithm ) except Assertion Error : raise Invalid Key Error ( "Invalid ecdsa key" ) self . bits = int ( curve information . replace ( b"nistp" , b"" ) ) self . ecdsa = ecdsa key return current position
def main ( properties = properties , options = options , * * custom options ) : return init ( * * dict ( options , * * custom options ) ) ( * * properties )
def create file ( ) : f = wave . open ( 'audio.wav' , mode = 'wb' ) f . setnchannels ( 2 ) p = pyaudio . Py Audio ( ) f . setsampwidth ( p . get sample size ( pyaudio . pa Int16 ) ) f . setframerate ( p . get default input device info ( ) [ 'default Sample Rate' ] ) try : yield f finally : f . close ( )
def djfrontend jquery datatables css ( version = None ) : if version is None : if not getattr ( settings , 'DJFRONTEND JQUERY DATATABLES CSS' , False ) : version = getattr ( settings , 'DJFRONTEND JQUERY DATATABLES VERSION' , DJFRONTEND JQUERY DATATABLES VERSION DEFAULT ) else : version = getattr ( settings , 'DJFRONTEND JQUERY DATATABLES CSS' , DJFRONTEND JQUERY DATATABLES VERSION DEFAULT ) return format html ( '<link rel="stylesheet" href="{static}djfrontend/css/jquery/jquery.data Tables/{v}/jquery.data Tables{min}.css">' , static = static url , v = version , min = min )
def djfrontend jquery datatables themeroller ( version = None ) : if version is None : if not getattr ( settings , 'DJFRONTEND JQUERY DATATABLES THEMEROLLER' , False ) : version = getattr ( settings , 'DJFRONTEND JQUERY DATATABLES VERSION' , DJFRONTEND JQUERY DATATABLES VERSION DEFAULT ) else : version = getattr ( settings , 'DJFRONTEND JQUERY DATATABLES THEMEROLLER' , DJFRONTEND JQUERY DATATABLES VERSION DEFAULT ) return format html ( '<link rel="stylesheet" href="href="{static}djfrontend/css/jquery/jquery.data Tables/{v}/jquery.data Tables themeroller.min.css">' , static = static url , v = version )
def calc expiry time ( minutes valid ) : return ( timezone . now ( ) + datetime . timedelta ( minutes = minutes valid + 1 ) ) . replace ( second = 0 , microsecond = 0 )
def get user token ( user , purpose , minutes valid ) : token = '' . join ( dumps ( [ user . get username ( ) , get auth hash ( user , purpose ) , ] ) . encode ( 'base64' ) . split ( '\n' ) ) return { 'id' : get meteor id ( user ) , 'token' : token , 'token Expires' : calc expiry time ( minutes valid ) , }
def serialize ( self , obj , * args , * * kwargs ) : # use default serialization, then modify to suit our needs. data = super ( Users , self ) . serialize ( obj , * args , * * kwargs ) # everything that isn't handled explicitly ends up in `profile` profile = data . pop ( 'fields' ) profile . setdefault ( 'name' , obj . get full name ( ) ) fields = data [ 'fields' ] = { 'username' : obj . get username ( ) , 'emails' : [ ] , 'profile' : profile , 'permissions' : sorted ( self . model . get all permissions ( obj ) ) , } # clear out sensitive data for sensitive in [ 'password' , 'user permissions ids' , 'is active' , 'is staff' , 'is superuser' , 'groups ids' , ] : profile . pop ( sensitive , None ) # created At (default is django.contrib.auth.models.User.date joined) try : fields [ 'created At' ] = profile . pop ( 'date joined' ) except Key Error : date joined = getattr ( obj , 'get date joined' , lambda : getattr ( obj , 'date joined' , None ) ) ( ) if date joined : fields [ 'created At' ] = date joined # email (default is django.contrib.auth.models.User.email) try : email = profile . pop ( 'email' ) except Key Error : email = getattr ( obj , 'get email' , lambda : getattr ( obj , 'email' , None ) ) ( ) if email : fields [ 'emails' ] . append ( { 'address' : email , 'verified' : True } ) return data
def deserialize profile ( profile , key prefix = '' , pop = False ) : result = { } if pop : getter = profile . pop else : getter = profile . get def prefixed ( name ) : """Return name prefixed by `key prefix`.""" return '%s%s' % ( key prefix , name ) for key in profile . keys ( ) : val = getter ( key ) if key == prefixed ( 'name' ) : result [ 'full name' ] = val else : raise Meteor Error ( 400 , 'Bad profile key: %r' % key ) return result
def update ( self , selector , update , options = None ) : # we're ignoring the `options` argument at this time del options user = get object ( self . model , selector [ ' id' ] , pk = this . user id , ) profile update = self . deserialize profile ( update [ '$set' ] , key prefix = 'profile.' , pop = True , ) if len ( update [ '$set' ] ) != 0 : raise Meteor Error ( 400 , 'Invalid update fields: %r' ) for key , val in profile update . items ( ) : setattr ( user , key , val ) user . save ( )
def auth failed ( * * credentials ) : if credentials : user login failed . send robust ( sender = name , credentials = auth . clean credentials ( credentials ) , ) raise Meteor Error ( 403 , 'Authentication failed.' )
def validated user ( cls , token , purpose , minutes valid ) : try : username , auth hash = loads ( token . decode ( 'base64' ) ) except ( Value Error , Error ) : cls . auth failed ( token = token ) try : user = cls . user model . objects . get ( * * { cls . user model . USERNAME FIELD : username , 'is active' : True , } ) user . backend = 'django.contrib.auth.backends.Model Backend' except cls . user model . Does Not Exist : cls . auth failed ( username = username , token = token ) if auth hash not in iter auth hashes ( user , purpose , minutes valid ) : cls . auth failed ( username = username , token = token ) return user
def check secure ( ) : if this . request . is secure ( ) : return True # using SSL elif this . request . META [ 'REMOTE ADDR' ] in [ 'localhost' , '127.0.0.1' , ] : return True # localhost raise Meteor Error ( 403 , 'Authentication refused without SSL.' )
def get username ( self , user ) : if isinstance ( user , basestring ) : return user elif isinstance ( user , dict ) and len ( user ) == 1 : [ ( key , val ) ] = user . items ( ) if key == 'username' or ( key == self . user model . USERNAME FIELD ) : # username provided directly return val elif key in ( 'email' , 'emails.address' ) : email field = getattr ( self . user model , 'EMAIL FIELD' , 'email' ) if self . user model . USERNAME FIELD == email field : return val # email is username # find username by email return self . user model . objects . values list ( self . user model . USERNAME FIELD , flat = True , ) . get ( * * { email field : val } ) elif key in ( 'id' , 'pk' ) : # find username by primary key (ID) return self . user model . objects . values list ( self . user model . USERNAME FIELD , flat = True , ) . get ( pk = val , ) else : raise Meteor Error ( 400 , 'Invalid user lookup: %r' % key ) else : raise Meteor Error ( 400 , 'Invalid user expression: %r' % user )
def create user ( self , params ) : receivers = create user . send ( sender = name , request = this . request , params = params , ) if len ( receivers ) == 0 : raise Not Implemented Error ( 'Handler for `create user` not registered.' ) user = receivers [ 0 ] [ 1 ] user = auth . authenticate ( username = user . get username ( ) , password = params [ 'password' ] , ) self . do login ( user ) return get user token ( user = user , purpose = Hash Purpose . RESUME LOGIN , minutes valid = HASH MINUTES VALID [ Hash Purpose . RESUME LOGIN ] , )
def do login ( self , user ) : this . user id = user . pk this . user ddp id = get meteor id ( user ) # silent subscription (sans sub/nosub msg) to Logged In User pub this . user sub id = meteor random id ( ) API . do sub ( this . user sub id , 'Logged In User' , silent = True ) self . update subs ( user . pk ) user logged in . send ( sender = user . class , request = this . request , user = user , )
def do logout ( self ) : # silent unsubscription (sans sub/nosub msg) from Logged In User pub API . do unsub ( this . user sub id , silent = True ) del this . user sub id self . update subs ( None ) user logged out . send ( sender = self . user model , request = this . request , user = this . user , ) this . user id = None this . user ddp id = None
def login ( self , params ) : if 'password' in params : return self . login with password ( params ) elif 'resume' in params : return self . login with resume token ( params ) else : self . auth failed ( * * params )
def login with password ( self , params ) : # never allow insecure login self . check secure ( ) username = self . get username ( params [ 'user' ] ) password = self . get password ( params [ 'password' ] ) user = auth . authenticate ( username = username , password = password ) if user is not None : # the password verified for the user if user . is active : self . do login ( user ) return get user token ( user = user , purpose = Hash Purpose . RESUME LOGIN , minutes valid = HASH MINUTES VALID [ Hash Purpose . RESUME LOGIN ] , ) # Call to `authenticate` couldn't verify the username and password. # It will have sent the `user login failed` signal, no need to pass the # `username` argument to auth failed(). self . auth failed ( )
def forgot password ( self , params ) : username = self . get username ( params ) try : user = self . user model . objects . get ( * * { self . user model . USERNAME FIELD : username , } ) except self . user model . Does Not Exist : self . auth failed ( ) minutes valid = HASH MINUTES VALID [ Hash Purpose . PASSWORD RESET ] token = get user token ( user = user , purpose = Hash Purpose . PASSWORD RESET , minutes valid = minutes valid , ) forgot password . send ( sender = name , user = user , token = token , request = this . request , expiry date = calc expiry time ( minutes valid ) , )
def reset password ( self , token , new password ) : user = self . validated user ( token , purpose = Hash Purpose . PASSWORD RESET , minutes valid = HASH MINUTES VALID [ Hash Purpose . PASSWORD RESET ] , ) user . set password ( new password ) user . save ( ) self . do login ( user ) return { "user Id" : this . user ddp id }
def read ( path , default = None , encoding = 'utf8' ) : if not path : return default try : with io . open ( path , mode = 'r' , encoding = encoding ) as contents : return contents . read ( ) except IO Error : if default is not None : return default raise
def get meteor id ( obj or model , obj pk = None ) : if obj or model is None : return None # Django model. meta is now public API -> pylint: disable=W0212 meta = obj or model . meta model = meta . model if model is Object Mapping : # this doesn't make sense - raise Type Error raise Type Error ( "Can't map Object Mapping instances through self." ) # try getting value of Alea Id Field straight from instance if possible if isinstance ( obj or model , model ) : # obj or model is an instance, not a model. if isinstance ( meta . pk , Alea Id Field ) : return obj or model . pk if obj pk is None : # fall back to primary key, but coerce as string type for lookup. obj pk = str ( obj or model . pk ) alea unique fields = [ field for field in meta . local fields if isinstance ( field , Alea Id Field ) and field . unique ] if len ( alea unique fields ) == 1 : # found an Alea Id Field with unique=True, assume it's got the value. aid = alea unique fields [ 0 ] . attname if isinstance ( obj or model , model ) : val = getattr ( obj or model , aid ) elif obj pk is None : val = None else : val = model . objects . values list ( aid , flat = True ) . get ( pk = obj pk , ) if val : return val if obj pk is None : # bail out if args are (model, pk) but pk is None. return None # fallback to using Alea Id Field from Object Mapping model. content type = Content Type . objects . get for model ( model ) try : return Object Mapping . objects . values list ( 'meteor id' , flat = True , ) . get ( content type = content type , object id = obj pk , ) except Object Does Not Exist : return Object Mapping . objects . create ( content type = content type , object id = obj pk , meteor id = meteor random id ( '/collection/%s' % meta ) , ) . meteor id
def get meteor ids ( model , object ids ) : # Django model. meta is now public API -> pylint: disable=W0212 meta = model . meta result = collections . Ordered Dict ( ( str ( obj pk ) , None ) for obj pk in object ids ) if isinstance ( meta . pk , Alea Id Field ) : # primary key is an Alea Id Field, use it. return collections . Ordered Dict ( ( obj pk , obj pk ) for obj pk in object ids ) alea unique fields = [ field for field in meta . local fields if isinstance ( field , Alea Id Field ) and field . unique and not field . null ] if len ( alea unique fields ) == 1 : aid = alea unique fields [ 0 ] . name query = model . objects . filter ( pk in = object ids , ) . values list ( 'pk' , aid ) else : content type = Content Type . objects . get for model ( model ) query = Object Mapping . objects . filter ( content type = content type , object id in = list ( result ) ) . values list ( 'object id' , 'meteor id' ) for obj pk , meteor id in query : result [ str ( obj pk ) ] = meteor id for obj pk , meteor id in result . items ( ) : if meteor id is None : result [ obj pk ] = get meteor id ( model , obj pk ) return result
def get object id ( model , meteor id ) : if meteor id is None : return None # Django model. meta is now public API -> pylint: disable=W0212 meta = model . meta if model is Object Mapping : # this doesn't make sense - raise Type Error raise Type Error ( "Can't map Object Mapping instances through self." ) if isinstance ( meta . pk , Alea Id Field ) : # meteor id is the primary key return meteor id alea unique fields = [ field for field in meta . local fields if isinstance ( field , Alea Id Field ) and field . unique ] if len ( alea unique fields ) == 1 : # found an Alea Id Field with unique=True, assume it's got the value. val = model . objects . values list ( 'pk' , flat = True , ) . get ( * * { alea unique fields [ 0 ] . attname : meteor id , } ) if val : return val content type = Content Type . objects . get for model ( model ) return Object Mapping . objects . filter ( content type = content type , meteor id = meteor id , ) . values list ( 'object id' , flat = True ) . get ( )
def get object ids ( model , meteor ids ) : if model is Object Mapping : # this doesn't make sense - raise Type Error raise Type Error ( "Can't map Object Mapping instances through self." ) # Django model. meta is now public API -> pylint: disable=W0212 meta = model . meta alea unique fields = [ field for field in meta . local fields if isinstance ( field , Alea Id Field ) and field . unique and not field . null ] result = collections . Ordered Dict ( ( str ( meteor id ) , None ) for meteor id in meteor ids ) if len ( alea unique fields ) == 1 : aid = alea unique fields [ 0 ] . name query = model . objects . filter ( * * { '%s in' % aid : meteor ids , } ) . values list ( aid , 'pk' ) else : content type = Content Type . objects . get for model ( model ) query = Object Mapping . objects . filter ( content type = content type , meteor id in = meteor ids , ) . values list ( 'meteor id' , 'object id' ) for meteor id , object id in query : result [ meteor id ] = object id return result
def get object ( model , meteor id , * args , * * kwargs ) : # Django model. meta is now public API -> pylint: disable=W0212 meta = model . meta if isinstance ( meta . pk , Alea Id Field ) : # meteor id is the primary key return model . objects . filter ( * args , * * kwargs ) . get ( pk = meteor id ) alea unique fields = [ field for field in meta . local fields if isinstance ( field , Alea Id Field ) and field . unique and not field . null ] if len ( alea unique fields ) == 1 : return model . objects . filter ( * args , * * kwargs ) . get ( * * { alea unique fields [ 0 ] . name : meteor id , } ) return model . objects . filter ( * args , * * kwargs ) . get ( pk = get object id ( model , meteor id ) , )
def get pk value on save ( self , instance ) : value = super ( Alea Id Field , self ) . get pk value on save ( instance ) if not value : value = self . get seeded value ( instance ) return value
def pre save ( self , model instance , add ) : value = super ( Alea Id Field , self ) . pre save ( model instance , add ) if ( not value ) and self . default in ( meteor random id , NOT PROVIDED ) : value = self . get seeded value ( model instance ) setattr ( model instance , self . attname , value ) return value
def set default forwards ( app name , operation , apps , schema editor ) : model = apps . get model ( app name , operation . model name ) for obj pk in model . objects . values list ( 'pk' , flat = True ) : model . objects . filter ( pk = obj pk ) . update ( * * { operation . name : get meteor id ( model , obj pk ) , } )
def set default reverse ( app name , operation , apps , schema editor ) : model = apps . get model ( app name , operation . model name ) for obj pk in model . objects . values list ( 'pk' , flat = True ) : get meteor id ( model , obj pk )
def database forwards ( self , app label , schema editor , from state , to state ) : self . truncate ( app label , schema editor , self . truncate forwards )
def database backwards ( self , app label , schema editor , from state , to state ) : self . truncate ( app label , schema editor , self . truncate backwards )
def initialize options ( self ) : setuptools . command . build py . build py . initialize options ( self ) self . meteor = 'meteor' self . meteor debug = False self . build lib = None self . package dir = None self . meteor builds = [ ] self . no prune npm = None self . inplace = True
def finalize options ( self ) : # Get all the information we need to install pure Python modules # from the umbrella 'install' command -- build (source) directory, # install (target) directory, and whether to compile .py files. self . set undefined options ( 'build' , ( 'build lib' , 'build lib' ) , ) self . set undefined options ( 'build py' , ( 'package dir' , 'package dir' ) , ) setuptools . command . build py . build py . finalize options ( self )
def path to dir ( * path args ) : return os . path . join ( * list ( path args [ : - 1 ] ) + path args [ - 1 ] . split ( posixpath . sep ) )
def seed ( self , values ) : if not values : # Meteor uses epoch seconds as the seed if no args supplied, we use # a much more secure seed by default to avoid hash collisions. seed ids = [ int , str , random , self , values , self . class ] random . shuffle ( seed ids ) values = list ( map ( id , seed ids ) ) + [ time . time ( ) , os . urandom ( 512 ) ] mash = Mash ( ) self . c = 1 self . s0 = mash ( ' ' ) self . s1 = mash ( ' ' ) self . s2 = mash ( ' ' ) for val in values : self . s0 -= mash ( val ) if self . s0 < 0 : self . s0 += 1 self . s1 -= mash ( val ) if self . s1 < 0 : self . s1 += 1 self . s2 -= mash ( val ) if self . s2 < 0 : self . s2 += 1
def state ( self ) : return { 'c' : self . c , 's0' : self . s0 , 's1' : self . s1 , 's2' : self . s2 }
def random string ( self , length , alphabet ) : return '' . join ( self . choice ( alphabet ) for n in range ( length ) )
def api endpoints ( obj ) : for name in dir ( obj ) : attr = getattr ( obj , name ) api path = getattr ( attr , 'api path' , None ) if api path : yield ( '%s%s' % ( obj . api path prefix , api path ) , attr , ) for api provider in obj . api providers : for api path , attr in api endpoints ( api provider ) : yield ( api path , attr )
def clear api path map cache ( self ) : self . api path cache = None for api provider in self . api providers : if six . get method self ( api provider . clear api path map cache , ) is not None : api provider . clear api path map cache ( )
def dprint ( name , val ) : from pprint import pformat print ( '% 5s: %s' % ( name , '\n       ' . join ( pformat ( val , indent = 4 , width = 75 , ) . split ( '\n' ) ) , ) , )
def validate kwargs ( func , kwargs ) : func name = func . name argspec = inspect . getargspec ( func ) all args = argspec . args [ : ] defaults = list ( argspec . defaults or [ ] ) # ignore implicit 'self' argument if inspect . ismethod ( func ) and all args [ : 1 ] == [ 'self' ] : all args [ : 1 ] = [ ] # don't require arguments that have defaults if defaults : required = all args [ : - len ( defaults ) ] else : required = all args [ : ] # translate 'foo ' to avoid reserved names like 'id' trans = { arg : arg . endswith ( ' ' ) and arg [ : - 1 ] or arg for arg in all args } for key in list ( kwargs ) : key adj = '%s ' % key if key adj in all args : kwargs [ key adj ] = kwargs . pop ( key ) # figure out what we're missing supplied = sorted ( kwargs ) missing = [ trans . get ( arg , arg ) for arg in required if arg not in supplied ] if missing : raise Meteor Error ( 400 , func . err , 'Missing required arguments to %s: %s' % ( func name , ' ' . join ( missing ) , ) , ) # figure out what is extra extra = [ arg for arg in supplied if arg not in all args ] if extra : raise Meteor Error ( 400 , func . err , 'Unknown arguments to %s: %s' % ( func name , ' ' . join ( extra ) ) , )
def on open ( self ) : this . request = WSGI Request ( self . ws . environ ) this . ws = self this . send = self . send this . reply = self . reply self . logger = self . ws . logger self . remote ids = collections . defaultdict ( set ) # ` tx buffer` collects outgoing messages which must be sent in order self . tx buffer = { } # track the head of the queue (buffer) and the next msg to be sent self . tx buffer id gen = itertools . cycle ( irange ( sys . maxint ) ) self . tx next id gen = itertools . cycle ( irange ( sys . maxint ) ) # start by waiting for the very first message self . tx next id = next ( self . tx next id gen ) this . remote addr = self . remote addr = '{0[REMOTE ADDR]}:{0[REMOTE PORT]}' . format ( self . ws . environ , ) this . subs = { } safe call ( self . logger . info , '+ %s OPEN' , self ) self . send ( 'o' ) self . send ( 'a["{\\"server id\\":\\"0\\"}"]' )
def on close ( self , * args , * * kwargs ) : if self . connection is not None : del self . pgworker . connections [ self . connection . pk ] self . connection . delete ( ) self . connection = None signals . request finished . send ( sender = self . class ) safe call ( self . logger . info , '- %s %s' , self , args or 'CLOSE' )
def on message ( self , message ) : if self . ws . closed : return None try : safe call ( self . logger . debug , '< %s %r' , self , message ) # process individual messages for data in self . ddp frames from message ( message ) : self . process ddp ( data ) # emit request finished signal to close DB connections signals . request finished . send ( sender = self . class ) except geventwebsocket . Web Socket Error : self . ws . close ( )
def ddp frames from message ( self , message ) : # parse message set try : msgs = ejson . loads ( message ) except Value Error : self . reply ( 'error' , error = 400 , reason = 'Data is not valid EJSON' , ) raise Stop Iteration if not isinstance ( msgs , list ) : self . reply ( 'error' , error = 400 , reason = 'Invalid EJSON messages' , ) raise Stop Iteration # process individual messages while msgs : # pop raw message from the list raw = msgs . pop ( 0 ) # parse message payload try : data = ejson . loads ( raw ) except ( Type Error , Value Error ) : data = None if not isinstance ( data , dict ) : self . reply ( 'error' , error = 400 , reason = 'Invalid Sock JS DDP payload' , offending Message = raw , ) yield data if msgs : # yield to other greenlets before processing next msg gevent . sleep ( )
def process ddp ( self , data ) : msg id = data . get ( 'id' , None ) try : msg = data . pop ( 'msg' ) except Key Error : self . reply ( 'error' , reason = 'Bad request' , offending Message = data , ) return try : # dispatch message self . dispatch ( msg , data ) except Exception as err : # pylint: disable=broad-except # This should be the only protocol exception handler kwargs = { 'msg' : { 'method' : 'result' } . get ( msg , 'error' ) , } if msg id is not None : kwargs [ 'id' ] = msg id if isinstance ( err , Meteor Error ) : error = err . as dict ( ) else : error = { 'error' : 500 , 'reason' : 'Internal server error' , } if kwargs [ 'msg' ] == 'error' : kwargs . update ( error ) else : kwargs [ 'error' ] = error if not isinstance ( err , Meteor Error ) : # not a client error, should always be logged. stack , = safe call ( self . logger . error , '%r %r' , msg , data , exc info = 1 , ) if stack is not None : # something went wrong while logging the error, revert to # writing a stack trace to stderr. traceback . print exc ( file = sys . stderr ) sys . stderr . write ( 'Additionally, while handling the above error the ' 'following error was encountered:\n' ) sys . stderr . write ( stack ) elif settings . DEBUG : print ( 'ERROR: %s' % err ) dprint ( 'msg' , msg ) dprint ( 'data' , data ) error . setdefault ( 'details' , traceback . format exc ( ) ) # print stack trace for client errors when DEBUG is True. print ( error [ 'details' ] ) self . reply ( * * kwargs ) if msg id and msg == 'method' : self . reply ( 'updated' , methods = [ msg id ] )
def dispatch ( self , msg , kwargs ) : # enforce calling 'connect' first if self . connection is None and msg != 'connect' : self . reply ( 'error' , reason = 'Must connect first' ) return if msg == 'method' : if ( 'method' not in kwargs ) or ( 'id' not in kwargs ) : self . reply ( 'error' , error = 400 , reason = 'Malformed method invocation' , ) return # lookup method handler try : handler = getattr ( self , 'recv %s' % msg ) except ( Attribute Error , Unicode Encode Error ) : raise Meteor Error ( 404 , 'Method not found' ) # validate handler arguments validate kwargs ( handler , kwargs ) # dispatch to handler handler ( * * kwargs )
def recv connect ( self , version = None , support = None , session = None ) : del session # Meteor doesn't even use this! if self . connection is not None : raise Meteor Error ( 400 , 'Session already established.' , self . connection . connection id , ) elif None in ( version , support ) or version not in self . versions : self . reply ( 'failed' , version = self . versions [ 0 ] ) elif version not in support : raise Meteor Error ( 400 , 'Client version/support mismatch.' ) else : from dddp . models import Connection cur = connection . cursor ( ) cur . execute ( 'SELECT pg backend pid()' ) ( backend pid , ) = cur . fetchone ( ) this . version = version this . support = support self . connection = Connection . objects . create ( server addr = '%d:%s' % ( backend pid , self . ws . handler . socket . getsockname ( ) , ) , remote addr = self . remote addr , version = version , ) self . pgworker . connections [ self . connection . pk ] = self atexit . register ( self . on close , 'Shutting down.' ) self . reply ( 'connected' , session = self . connection . connection id )
def recv ping ( self , id = None ) : if id is None : self . reply ( 'pong' ) else : self . reply ( 'pong' , id = id )
def recv sub ( self , id , name , params ) : self . api . sub ( id , name , * params )
def recv unsub ( self , id = None ) : if id : self . api . unsub ( id ) else : self . reply ( 'nosub' )
def recv method ( self , method , params , id , random Seed = None ) : if random Seed is not None : this . random streams . random seed = random Seed this . alea random = alea . Alea ( random Seed ) self . api . method ( method , params , id ) self . reply ( 'updated' , methods = [ id ] )
def ddpp sockjs info ( environ , start response ) : import random import ejson start response ( '200 OK' , [ ( 'Content-Type' , 'application/json; charset=UTF-8' ) , ] + common headers ( environ ) , ) yield ejson . dumps ( collections . Ordered Dict ( [ ( 'websocket' , True ) , ( 'origins' , [ '*:*' , ] ) , ( 'cookie needed' , False ) , ( 'entropy' , random . getrandbits ( 32 ) ) , ] ) )
def serve ( listen , verbosity = 1 , debug port = 0 , * * ssl args ) : launcher = DDP Launcher ( debug = verbosity == 3 , verbosity = verbosity ) if debug port : launcher . servers . append ( launcher . get backdoor server ( 'localhost:%d' % debug port ) ) launcher . add web servers ( listen , * * ssl args ) # die gracefully with SIGINT or SIGQUIT sigmap = { val : name for name , val in vars ( signal ) . items ( ) if name . startswith ( 'SIG' ) } def sighandler ( signum = None , frame = None ) : """Signal handler""" launcher . logger . info ( 'Received signal %s in frame %r' , sigmap . get ( signum , signum ) , frame , ) launcher . stop ( ) for signum in [ signal . SIGINT , signal . SIGQUIT ] : gevent . signal ( signum , sighandler ) launcher . run ( )
def main ( ) : parser = argparse . Argument Parser ( description = doc ) django = parser . add argument group ( 'Django Options' ) django . add argument ( '--verbosity' , '-v' , metavar = 'VERBOSITY' , dest = 'verbosity' , type = int , default = 1 , ) django . add argument ( '--debug-port' , metavar = 'DEBUG PORT' , dest = 'debug port' , type = int , default = 0 , ) django . add argument ( '--settings' , metavar = 'SETTINGS' , dest = 'settings' , help = "The Python path to a settings module, e.g. " "\"myproject.settings.main\". If this isn't provided, the " "DJANGO SETTINGS MODULE environment variable will be used." , ) http = parser . add argument group ( 'HTTP Options' ) http . add argument ( 'listen' , metavar = 'address[:port]' , nargs = '*' , type = addr , help = 'Listening address for HTTP(s) server.' , ) ssl = parser . add argument group ( 'SSL Options' ) ssl . add argument ( '--ssl-version' , metavar = 'SSL VERSION' , dest = 'ssl version' , help = "SSL version to use (see stdlib ssl module's) [3]" , choices = [ '1' , '2' , '3' ] , default = '3' ) ssl . add argument ( '--certfile' , metavar = 'FILE' , dest = 'certfile' , help = "SSL certificate file [None]" ) ssl . add argument ( '--ciphers' , metavar = 'CIPHERS' , dest = 'ciphers' , help = "Ciphers to use (see stdlib ssl module's) [TL Sv1]" ) ssl . add argument ( '--ca-certs' , metavar = 'FILE' , dest = 'ca certs' , help = "CA certificates file [None]" ) ssl . add argument ( '--keyfile' , metavar = 'FILE' , dest = 'keyfile' , help = "SSL key file [None]" ) namespace = parser . parse args ( ) if namespace . settings : os . environ [ 'DJANGO SETTINGS MODULE' ] = namespace . settings serve ( namespace . listen or [ Addr ( 'localhost' , 8000 ) ] , debug port = namespace . debug port , keyfile = namespace . keyfile , certfile = namespace . certfile , verbosity = namespace . verbosity , )
def print ( self , msg , * args , * * kwargs ) : if self . verbosity >= 1 : print ( msg , * args , * * kwargs )
def stop ( self ) : self . logger . debug ( 'Postgres Greenlet stop' ) self . stop event . set ( ) # ask all threads to stop. for server in self . servers + [ DDP Launcher . pgworker ] : self . logger . debug ( 'Stopping %s' , server ) server . stop ( ) # wait for all threads to stop. gevent . joinall ( self . threads + [ DDP Launcher . pgworker ] ) self . threads = [ ]
def run ( self ) : self . logger . debug ( 'Postgres Greenlet run' ) self . start ( ) self . stop event . wait ( ) # wait for all threads to stop. gevent . joinall ( self . threads + [ DDP Launcher . pgworker ] ) self . threads = [ ]
def run ( self ) : # pylint: disable=method-hidden conn params = self . connection . get connection params ( ) # See http://initd.org/psycopg/docs/module.html#psycopg2.connect and # http://www.postgresql.org/docs/current/static/libpq-connect.html # section 31.1.2 (Parameter Key Words) for details on available params. conn params . update ( async = True , application name = '{} pid={} django-ddp' . format ( socket . gethostname ( ) , # hostname os . getpid ( ) , # PID ) [ : 64 ] , # 64 characters for default Postgre SQL build config ) conn = None while conn is None : try : conn = psycopg2 . connect ( * * conn params ) except psycopg2 . Operational Error as err : # Some variants of the psycopg2 driver for Django add extra # params that aren't meant to be passed directly to # `psycopg2.connect()` -- issue a warning and try again. msg = ( '%s' % err ) . strip ( ) msg prefix = 'invalid connection option "' if not msg . startswith ( msg prefix ) : # *waves hand* this is not the errror you are looking for. raise key = msg [ len ( msg prefix ) : - 1 ] self . logger . warning ( 'Ignoring unknown settings.DATABASES[%r] option: %s=%r' , self . connection . alias , key , conn params . pop ( key ) , ) self . poll ( conn ) # wait for conneciton to start import logging logging . get Logger ( 'dddp' ) . info ( '=> Started Postgres Greenlet.' ) cur = conn . cursor ( ) cur . execute ( 'LISTEN "ddp";' ) while not self . stop event . is set ( ) : try : self . select greenlet = gevent . spawn ( gevent . select . select , [ conn ] , [ ] , [ ] , timeout = None , ) self . select greenlet . get ( ) except gevent . Greenlet Exit : self . stop event . set ( ) finally : self . select greenlet = None self . poll ( conn ) self . poll ( conn ) cur . close ( ) self . poll ( conn ) conn . close ( )
def poll ( self , conn ) : while 1 : state = conn . poll ( ) if state == psycopg2 . extensions . POLL OK : while conn . notifies : notify = conn . notifies . pop ( ) self . logger . info ( "Got NOTIFY (pid=%d, payload=%r)" , notify . pid , notify . payload , ) # read the header and check seq/fin. hdr , chunk = notify . payload . split ( '|' , 1 ) # print('RECEIVE: %s' % hdr) header = ejson . loads ( hdr ) uuid = header [ 'uuid' ] size , chunks = self . chunks . setdefault ( uuid , [ 0 , { } ] ) if header [ 'fin' ] : size = self . chunks [ uuid ] [ 0 ] = header [ 'seq' ] # stash the chunk chunks [ header [ 'seq' ] ] = chunk if len ( chunks ) != size : # haven't got all the chunks yet continue # process next NOTIFY in loop # got the last chunk -> process it. data = '' . join ( chunk for , chunk in sorted ( chunks . items ( ) ) ) del self . chunks [ uuid ] # don't forget to cleanup! data = ejson . loads ( data ) sender = data . pop ( ' sender' , None ) tx id = data . pop ( ' tx id' , None ) for connection id in data . pop ( ' connection ids' ) : try : websocket = self . connections [ connection id ] except Key Error : continue # connection not in this process if connection id == sender : websocket . send ( data , tx id = tx id ) else : websocket . send ( data ) break elif state == psycopg2 . extensions . POLL WRITE : gevent . select . select ( [ ] , [ conn . fileno ( ) ] , [ ] ) elif state == psycopg2 . extensions . POLL READ : gevent . select . select ( [ conn . fileno ( ) ] , [ ] , [ ] ) else : self . logger . warn ( 'POLL ERR: %s' , state )
def greenify ( ) : # don't greenify twice. if GREEN : return GREEN [ True ] = True from gevent . monkey import patch all , saved if ( 'threading' in sys . modules ) and ( 'threading' not in saved ) : import warnings warnings . warn ( 'threading module loaded before patching!' ) patch all ( ) try : # Use psycopg2 by default import psycopg2 del psycopg2 except Import Error : # Fallback to psycopg2cffi if required (eg: pypy) from psycopg2cffi import compat compat . register ( ) from psycogreen . gevent import patch psycopg patch psycopg ( )
def meteor random id ( name = None , length = 17 ) : if name is None : stream = THREAD LOCAL . alea random else : stream = THREAD LOCAL . random streams [ name ] return stream . random string ( length , METEOR ID CHARS )
def autodiscover ( ) : from django . utils . module loading import autodiscover modules from dddp . api import API autodiscover modules ( 'ddp' , register to = API ) return API
def as dict ( self , * * kwargs ) : error , reason , details , err kwargs = self . args result = { key : val for key , val in { 'error' : error , 'reason' : reason , 'details' : details , } . items ( ) if val is not None } result . update ( err kwargs ) result . update ( kwargs ) return result
def get ( self , name , factory , * factory args , * * factory kwargs ) : update thread local = getattr ( factory , 'update thread local' , True ) if ( not update thread local ) or ( name not in self . dict ) : obj = factory ( * factory args , * * factory kwargs ) if update thread local : setattr ( self , name , obj ) return obj return getattr ( self , name )
def emit ( self , record ) : if getattr ( this , 'subs' , { } ) . get ( LOGS NAME , False ) : self . format ( record ) this . send ( { 'msg' : ADDED , 'collection' : LOGS NAME , 'id' : meteor random id ( '/collection/%s' % LOGS NAME ) , 'fields' : { attr : { # typecasting methods for specific attributes 'args' : lambda args : [ repr ( arg ) for arg in args ] , 'created' : datetime . datetime . fromtimestamp , 'exc info' : stacklines or none , } . get ( attr , lambda val : val # default typecasting method ) ( getattr ( record , attr , None ) ) for attr in ( 'args' , 'asctime' , 'created' , 'exc info' , 'filename' , 'func Name' , 'levelname' , 'levelno' , 'lineno' , 'module' , 'msecs' , 'message' , 'name' , 'pathname' , 'process' , 'process Name' , 'relative Created' , 'thread' , 'thread Name' , ) } , } )
def send message ( self , message , * * kwargs ) : from . . libs . gcm import gcm send message data = kwargs . pop ( "extra" , { } ) if message is not None : data [ "message" ] = message return gcm send message ( registration id = self . registration id , data = data , * * kwargs )
def gcm send message ( registration id , data , encoding = 'utf-8' , * * kwargs ) : messenger = GCM Messenger ( registration id , data , encoding = encoding , * * kwargs ) return messenger . send plain ( )
def gcm send bulk message ( registration ids , data , encoding = 'utf-8' , * * kwargs ) : messenger = GCM Messenger ( registration ids , data , encoding = encoding , * * kwargs ) return messenger . send bulk ( )
def send json ( self , ids = None ) : items = ids or self . registration id values = { "registration ids" : items } if self . data is not None : values [ "data" ] = self . data for key , val in self . kwargs . items ( ) : if val : values [ key ] = val data = json . dumps ( values , separators = ( "," , ":" ) , sort keys = True ) . encode ( self . encoding ) result = json . loads ( self . send ( data , "application/json" ) ) if ( "failure" in result ) and ( result [ "failure" ] ) : unregistered = [ ] throw error = False for index , error in enumerate ( result . get ( "results" , [ ] ) ) : error = error . get ( "error" , "" ) if error in ( "Not Registered" , "Invalid Registration" ) : unregistered . append ( items [ index ] ) elif error != "" : throw error = True self . deactivate unregistered devices ( unregistered ) if throw error : raise GCM Push Error ( result ) return result
def send ( self , data , content type ) : headers = { "Content-Type" : content type , "Authorization" : "key=%s" % ( self . api key ) , "Content-Length" : str ( len ( data ) ) } request = Request ( self . api url , data , headers ) return urlopen ( request ) . read ( ) . decode ( self . encoding )
def get model ( module location ) : if not isinstance ( module location , ( str , unicode ) ) : raise Value Error ( "The value provided should either be a string or " "unicode instance. The value '%s' provided was %s " "rather." % ( module location , type ( module location ) ) ) try : name split = module location . split ( "." ) class name = name split . pop ( - 1 ) if not len ( name split ) : raise Value Error ( "The value should provide the module location " "joined by '.' e.g. for model named 'test' in " "/app/module.py, The value should be 'app.module.test'" ) module location = "." . join ( name split ) module = importlib . import module ( module location ) cls = getattr ( module , class name ) return cls except Attribute Error : pass
def fetch ( self , endpoint name , * * params ) : params [ 'api key' ] = self . api key resp = requests . get ( self . endpoint ( endpoint name ) , params = params ) resp . raise for status ( ) data = resp . json ( ) self . check or raise ( data . get ( 'meta' , { } ) ) return data
def video ( request , video id ) : # Check video availability # Available states are: processing api = Api ( ) api . authenticate ( ) availability = api . check upload status ( video id ) if availability is not True : # Video is not available video = Video . objects . filter ( video id = video id ) . get ( ) state = availability [ "upload state" ] # Add additional states here. I'm not sure what states are available if state == "failed" or state == "rejected" : return render to response ( "django youtube/video failed.html" , { "video" : video , "video id" : video id , "message" : ( "Invalid video." ) , "availability" : availability } , context instance = Request Context ( request ) ) else : return render to response ( "django youtube/video unavailable.html" , { "video" : video , "video id" : video id , "message" : ( "This video is currently being processed" ) , "availability" : availability } , context instance = Request Context ( request ) ) video params = video params ( request , video id ) return render to response ( "django youtube/video.html" , video params , context instance = Request Context ( request ) )
def newick ( self ) : label = self . name or '' if self . length : label += ':' + self . length descendants = ',' . join ( [ n . newick for n in self . descendants ] ) if descendants : descendants = '(' + descendants + ')' return descendants + label
def remove internal names ( self ) : self . visit ( lambda n : setattr ( n , 'name' , None ) , lambda n : not n . is leaf )
def remove leaf names ( self ) : self . visit ( lambda n : setattr ( n , 'name' , None ) , lambda n : n . is leaf )
def auth required ( realm , auth func ) : def auth decorator ( func ) : def inner ( self , * args , * * kw ) : if self . get authenticated user ( auth func , realm ) : return func ( self , * args , * * kw ) return inner return auth decorator
def require setting ( self , name , feature = "this feature" ) : if name not in self . settings : raise Exception ( "You must define the '%s' setting in your " "application to use %s" % ( name , feature ) )
def get cookie ( self , name , default = None ) : assert self . cookie monster , 'Cookie Monster not set' return self . cookie monster . get cookie ( name , default )
def clear cookie ( self , name , path = "/" , domain = None ) : assert self . cookie monster , 'Cookie Monster not set' #, path=path, domain=domain) self . cookie monster . delete cookie ( name )
def get authenticated user ( self , callback ) : # Look to see if we are doing combined Open ID/O Auth oauth ns = "" for name , values in self . request . arguments . iteritems ( ) : if name . startswith ( "openid.ns." ) and values [ - 1 ] == u"http://specs.openid.net/extensions/oauth/1.0" : oauth ns = name [ 10 : ] break token = self . get argument ( "openid." + oauth ns + ".request token" , "" ) if token : http = httpclient . Async HTTP Client ( ) token = dict ( key = token , secret = "" ) http . fetch ( self . oauth access token url ( token ) , self . async callback ( self . on access token , callback ) ) else : Open Id Mixin . get authenticated user ( self , callback )
def add ( self , name , value ) : norm name = HTTP Headers . normalize name ( name ) self . last key = norm name if norm name in self : # bypass our override of  setitem  since it modifies  as list dict . setitem ( self , norm name , self [ norm name ] + ',' + value ) self . as list [ norm name ] . append ( value ) else : self [ norm name ] = value
def get list ( self , name ) : norm name = HTTP Headers . normalize name ( name ) return self . as list . get ( norm name , [ ] )
def select Policy ( self , origin , request method = None ) : ret origin = None policyname = None if self . matchstrategy in ( "firstmatch" , "verbmatch" ) : for pol in self . activepolicies : policy = self . policies [ pol ] ret origin = None policyname = policy . name if policyname == "deny" : break if self . matchstrategy == "verbmatch" : if policy . methods != "*" and not CORS . matchlist ( request method , policy . methods , case sensitive = True ) : continue if origin and policy . match : if CORS . matchlist ( origin , policy . match ) : ret origin = origin elif policy . origin == "copy" : ret origin = origin elif policy . origin : ret origin = policy . origin if ret origin : break return policyname , ret origin
def get data from user ( msg type ) : data = { } for k , v in CONFIG [ msg type ] [ "settings" ] . items ( ) : data [ k ] = input ( v + ": " ) return data
def get auth from user ( msg type ) : auth = [ ] for k , v in CONFIG [ msg type ] [ "auth" ] . items ( ) : auth . append ( ( k , getpass ( v + ": " ) ) ) return Ordered Dict ( auth )
def construct message ( self ) : self . message [ "text" ] = "" if self . from : self . message [ "text" ] += "From: " + self . from + "\n" if self . subject : self . message [ "text" ] += "Subject: " + self . subject + "\n" self . message [ "text" ] += self . body self . add attachments ( )
def send ( self , encoding = "json" ) : self . construct message ( ) if self . verbose : print ( "Debugging info" "\n--------------" "\n{} Message created." . format ( timestamp ( ) ) ) if encoding == "json" : resp = requests . post ( self . url , json = self . message ) elif encoding == "url" : resp = requests . post ( self . url , data = self . message ) try : resp . raise for status ( ) if resp . history and resp . history [ 0 ] . status code >= 300 : raise Message Send Error ( "HTTP Redirect: Possibly Invalid authentication" ) elif "invalid auth" in resp . text : raise Message Send Error ( "Invalid Auth: Possibly Bad Auth Token" ) except ( requests . exceptions . HTTP Error , Message Send Error ) as e : raise Message Send Error ( e ) if self . verbose : print ( timestamp ( ) , type ( self ) . name , " info:" , self . str ( indentation = "\n * " ) , "\n * HTTP status code:" , resp . status code , ) print ( "Message sent." )
def validate input ( msg type , attr , value ) : try : valid = { "Email" : validate email , "Twilio" : validate twilio , "Slack Webhook" : validate slackwebhook , "Slack Post" : validate slackpost , "Telegram Bot" : validate telegrambot , "Whats App" : validate whatsapp , } [ msg type ] ( attr , value ) except Key Error : return 1 else : return 0
def validate twilio ( attr , value ) : if attr in ( "from " , "to" ) : check valid ( "Twilio" , attr , value , validus . isphone , "phone number" ) elif attr in ( "attachments" ) : check valid ( "Twilio" , attr , value , validus . isurl , "url" )
def validate slackpost ( attr , value ) : if attr in ( "channel" , "credentials" ) : if not isinstance ( value , str ) : raise Invalid Message Input Error ( "Slack Post" , attr , value , "string" ) elif attr in ( "attachments" ) : check valid ( "Slack Post" , attr , value , validus . isurl , "url" )
def validate whatsapp ( attr , value ) : if attr in ( "from " , "to" ) : if value is not None and "whatsapp:" in value : value = value . split ( "whatsapp:+" ) [ - 1 ] check valid ( "Whats App" , attr , value , validus . isint , "phone number starting with the '+' symbol" , ) elif attr in ( "attachments" ) : check valid ( "Whats App" , attr , value , validus . isurl , "url" )
def add message ( self , msg ) : try : self . coro . send ( msg ) except Attribute Error : raise Unsupported Message Type Error ( msg . class . name )
def get body from file ( kwds ) : if kwds [ "file" ] and os . path . isfile ( kwds [ "file" ] ) : kwds [ "body" ] = open ( kwds [ "file" ] , "r" ) . read ( ) kwds [ "file" ] = None
def trim args ( kwds ) : reject key = ( "type" , "types" , "configure" ) reject val = ( None , ( ) ) kwargs = { k : v for k , v in kwds . items ( ) if k not in reject key and v not in reject val } for k , v in kwargs . items ( ) : if k in ( "to" , "cc" , "bcc" , "attachments" ) : kwargs [ k ] = list ( kwargs [ k ] ) return kwargs
def send message ( msg type , kwds ) : if kwds [ "file" ] : get body from file ( kwds ) kwargs = trim args ( kwds ) send ( msg type , send async = False , * * kwargs )
def get chat id ( self , username ) : if username is not None : chats = requests . get ( self . base url + "/get Updates" ) . json ( ) user = username . split ( "@" ) [ - 1 ] for chat in chats [ "result" ] : if chat [ "message" ] [ "from" ] [ "username" ] == user : return chat [ "message" ] [ "from" ] [ "id" ]
def construct message ( self ) : self . message [ "chat id" ] = self . chat id self . message [ "text" ] = "" if self . from : self . message [ "text" ] += "From: " + self . from + "\n" if self . subject : self . message [ "text" ] += "Subject: " + self . subject + "\n" self . message [ "text" ] += self . body self . message . update ( self . params )
def send content ( self , method = "/send Message" ) : url = self . base url + method try : resp = requests . post ( url , json = self . message ) resp . raise for status ( ) except requests . exceptions . HTTP Error as e : raise Message Send Error ( e ) if self . verbose : if method == "/send Message" : content type = "Message body" elif method == "/send Document" : content type = "Attachment: " + self . message [ "document" ] print ( timestamp ( ) , content type , "sent." )
def send ( self ) : self . construct message ( ) if self . verbose : print ( "Debugging info" "\n--------------" "\n{} Message created." . format ( timestamp ( ) ) ) self . send content ( "/send Message" ) if self . attachments : if isinstance ( self . attachments , str ) : self . attachments = [ self . attachments ] for a in self . attachments : self . message [ "document" ] = a self . send content ( method = "/send Document" ) if self . verbose : print ( timestamp ( ) , type ( self ) . name + " info:" , self . str ( indentation = "\n * " ) , ) print ( "Message sent." )
def get server ( address = None ) : if address : domain = address . split ( "@" ) [ 1 ] try : return SMTP SERVERS [ domain ] except Key Error : return ( "smtp." + domain , 465 ) return ( None , None )
def generate email ( self ) : self . message = MIME Multipart ( ) self . add header ( ) self . add body ( ) self . add attachments ( )
def add header ( self ) : self . message [ "From" ] = self . from self . message [ "Subject" ] = self . subject if self . to : self . message [ "To" ] = self . list to string ( self . to ) if self . cc : self . message [ "Cc" ] = self . list to string ( self . cc ) if self . bcc : self . message [ "Bcc" ] = self . list to string ( self . bcc )
def add body ( self ) : if self . body : b = MIME Text ( "text" , "plain" ) b . set payload ( self . body ) self . message . attach ( b )
def add attachments ( self ) : num attached = 0 if self . attachments : if isinstance ( self . attachments , str ) : self . attachments = [ self . attachments ] for item in self . attachments : doc = MIME Application ( open ( item , "rb" ) . read ( ) ) doc . add header ( "Content-Disposition" , "attachment" , filename = item ) self . message . attach ( doc ) num attached += 1 return num attached
def get session ( self ) : if self . port in ( 465 , "465" ) : session = self . get ssl ( ) elif self . port in ( 587 , "587" ) : session = self . get tls ( ) try : session . login ( self . from , self . auth ) except SMTP Response Exception as e : raise Message Send Error ( e . smtp error . decode ( "unicode escape" ) ) return session
def get ssl ( self ) : return smtplib . SMTP SSL ( self . server , self . port , context = ssl . create default context ( ) )
def get tls ( self ) : session = smtplib . SMTP ( self . server , self . port ) session . ehlo ( ) session . starttls ( context = ssl . create default context ( ) ) session . ehlo ( ) return session
def delete ( self , filename = None ) : if self . tags is not None : if filename is None : filename = self . filename else : warnings . warn ( "delete(filename=...) is deprecated, reload the file" , Deprecation Warning ) return self . tags . delete ( filename )
def save ( self , filename = None , * * kwargs ) : if filename is None : filename = self . filename else : warnings . warn ( "save(filename=...) is deprecated, reload the file" , Deprecation Warning ) if self . tags is not None : return self . tags . save ( filename , * * kwargs ) else : raise Value Error ( "no tags in file" )
def unload ( self ) : if self . handle != - 1 : lib . Unload Image ( self . handle ) self . handle = - 1
def clear ( self ) : for i in list ( self . internal ) : self . internal . remove ( i )
def read ( self ) : self . fileobj . seek ( self . data offset ) self . data = self . fileobj . read ( self . data size )
def delete ( self ) : delete bytes ( self . fileobj , self . size , self . offset ) if self . parent chunk is not None : self . parent chunk . resize ( self . parent chunk . data size - self . size )
def resize ( self , data size ) : self . fileobj . seek ( self . offset + 4 ) self . fileobj . write ( pack ( '>I' , data size ) ) if self . parent chunk is not None : size diff = self . data size - data size self . parent chunk . resize ( self . parent chunk . data size - size diff ) self . data size = data size self . size = data size + self . HEADER SIZE
def insert chunk ( self , id ) : if not isinstance ( id , text type ) : id = id . decode ( 'ascii' ) if not is valid chunk id ( id ) : raise Key Error ( "AIFF key must be four ASCII characters." ) self . fileobj . seek ( self . next offset ) self . fileobj . write ( pack ( '>4si' , id . ljust ( 4 ) . encode ( 'ascii' ) , 0 ) ) self . fileobj . seek ( self . next offset ) chunk = IFF Chunk ( self . fileobj , self [ u'FORM' ] ) self [ u'FORM' ] . resize ( self [ u'FORM' ] . data size + chunk . size ) self . chunks [ id ] = chunk self . next offset = chunk . offset + chunk . size
def save ( self , filename = None , v2 version = 4 , v23 sep = '/' ) : framedata = self . prepare framedata ( v2 version , v23 sep ) framesize = len ( framedata ) if filename is None : filename = self . filename # Unlike the parent ID3.save method, we won't save to a blank file # since we would have to construct a empty AIFF file fileobj = open ( filename , 'rb+' ) iff file = IFF File ( fileobj ) try : if u'ID3' not in iff file : iff file . insert chunk ( u'ID3' ) chunk = iff file [ u'ID3' ] fileobj . seek ( chunk . data offset ) header = fileobj . read ( 10 ) header = self . prepare id3 header ( header , framesize , v2 version ) header , new size , = header data = header + framedata + ( b'\x00' * ( new size - framesize ) ) # Include ID3 header size in 'new size' calculation new size += 10 # Expand the chunk if necessary, including pad byte if new size > chunk . size : insert at = chunk . offset + chunk . size insert size = new size - chunk . size + new size % 2 insert bytes ( fileobj , insert size , insert at ) chunk . resize ( new size ) fileobj . seek ( chunk . data offset ) fileobj . write ( data ) finally : fileobj . close ( )
def delete ( self , filename = None ) : if filename is None : filename = self . filename delete ( filename ) self . clear ( )
def load ( self , filename , * * kwargs ) : self . filename = filename try : self . tags = IFFID3 ( filename , * * kwargs ) except ID3Error : self . tags = None try : fileobj = open ( filename , "rb" ) self . info = AIFF Info ( fileobj ) finally : fileobj . close ( )
def parse file ( self , filename ) : self . reset ( ) self . filename = filename fileinput . close ( ) self . format = None self . lineno = 0 self . lines = [ ] for line in fileinput . input ( filename ) : # strip trailing newlines, important on Windows machines! if line [ - 1 ] == '\012' : line = line [ 0 : - 1 ] if self . format == None : self . process normal line ( line ) else : if self . format . end . match ( line ) : # that's a normal block end, add it to 'lines' and # create a new block self . lines . append ( line ) self . add block lines ( ) elif self . format . column . match ( line ) : # that's a normal column line, add it to 'lines' self . lines . append ( line ) else : # humm.. this is an unexpected block end, # create a new block, but don't process the line self . add block lines ( ) # we need to process the line again self . process normal line ( line ) # record the last lines self . add block lines ( )
def process normal line ( self , line ) : for f in re source block formats : if f . start . match ( line ) : self . add block lines ( ) self . format = f self . lineno = fileinput . filelineno ( ) self . lines . append ( line )
def add block lines ( self ) : if self . lines != [ ] : block = Source Block ( self , self . filename , self . lineno , self . lines ) self . blocks . append ( block ) self . format = None self . lines = [ ]
def make html words ( self , words ) : line = "" if words : line = html quote ( words [ 0 ] ) for w in words [ 1 : ] : line = line + " " + html quote ( w ) return line
def make html word ( self , word ) : # look for cross-references m = re crossref . match ( word ) if m : try : name = m . group ( 1 ) rest = m . group ( 2 ) block = self . identifiers [ name ] url = self . make block url ( block ) return '<a href="' + url + '">' + name + '</a>' + rest except : # we detected a cross-reference to an unknown item sys . stderr . write ( "WARNING: undefined cross reference '" + name + "'.\n" ) return '?' + name + '?' + rest # look for italics and bolds m = re italic . match ( word ) if m : name = m . group ( 1 ) rest = m . group ( 3 ) return '<i>' + name + '</i>' + rest m = re bold . match ( word ) if m : name = m . group ( 1 ) rest = m . group ( 3 ) return '<b>' + name + '</b>' + rest return html quote ( word )
def make html para ( self , words ) : line = "" if words : line = self . make html word ( words [ 0 ] ) for word in words [ 1 : ] : line = line + " " + self . make html word ( word ) # convert `...' quotations into real left and right single quotes line = re . sub ( r"(^|\W)`(.*?)'(\W|$)" , r'\1&lsquo;\2&rsquo;\3' , line ) # convert tilde into non-breakable space line = string . replace ( line , "~" , "&nbsp;" ) return para header + line + para footer
def make html code ( self , lines ) : line = code header + '\n' for l in lines : line = line + html quote ( l ) + '\n' return line + code footer
def make html items ( self , items ) : lines = [ ] for item in items : if item . lines : lines . append ( self . make html code ( item . lines ) ) else : lines . append ( self . make html para ( item . words ) ) return string . join ( lines , '\n' )
def save ( self , filename ) : values = [ ] items = sorted ( self . items ( ) , key = MP4Tags . get sort stats ) for key , value in items : info = self . atoms . get ( key [ : 4 ] , ( None , type ( self ) . render text ) ) try : values . append ( info [ 1 ] ( self , key , value , * info [ 2 : ] ) ) except ( Type Error , Value Error ) as s : reraise ( MP4Metadata Value Error , s , sys . exc info ( ) [ 2 ] ) data = Atom . render ( b"ilst" , b"" . join ( values ) ) # Find the old atoms. fileobj = open ( filename , "rb+" ) try : atoms = Atoms ( fileobj ) try : path = atoms . path ( b"moov" , b"udta" , b"meta" , b"ilst" ) except Key Error : self . save new ( fileobj , atoms , data ) else : self . save existing ( fileobj , atoms , path , data ) finally : fileobj . close ( )
def update parents ( self , fileobj , path , delta ) : for atom in path : fileobj . seek ( atom . offset ) size = cdata . uint be ( fileobj . read ( 4 ) ) if size == 1 : # 64bit # skip name (4B) and read size (8B) size = cdata . ulonglong be ( fileobj . read ( 12 ) [ 4 : ] ) fileobj . seek ( atom . offset + 8 ) fileobj . write ( cdata . to ulonglong be ( size + delta ) ) else : # 32bit fileobj . seek ( atom . offset ) fileobj . write ( cdata . to uint be ( size + delta ) )
def load ( self , filename ) : self . filename = filename fileobj = open ( filename , "rb" ) try : data = AP Ev2Data ( fileobj ) finally : fileobj . close ( ) if data . tag : self . clear ( ) self . casemap . clear ( ) self . parse tag ( data . tag , data . items ) else : raise APE No Header Error ( "No APE tag found" )
def delete ( self , filename = None ) : filename = filename or self . filename fileobj = open ( filename , "r+b" ) try : data = AP Ev2Data ( fileobj ) if data . start is not None and data . size is not None : delete bytes ( fileobj , data . end - data . start , data . start ) finally : fileobj . close ( ) self . clear ( )
def size ( self ) : header size = 27 # Initial header size for datum in self . packets : quot , rem = divmod ( len ( datum ) , 255 ) header size += quot + 1 if not self . complete and rem == 0 : # Packet contains a multiple of 255 bytes and is not # terminated, so we don't have a \x00 at the end. header size -= 1 header size += sum ( map ( len , self . packets ) ) return header size
def load ( self , filename ) : self . filename = filename fileobj = open ( filename , "rb" ) try : try : self . info = self . Info ( fileobj ) self . tags = self . Tags ( fileobj , self . info ) self . info . post tags ( fileobj ) except error as e : reraise ( self . Error , e , sys . exc info ( ) [ 2 ] ) except EOF Error : raise self . Error ( "no appropriate stream found" ) finally : fileobj . close ( )
def set section ( self , section name ) : if not self . sections . has key ( section name ) : section = Doc Section ( section name ) self . sections [ section name ] = section self . section = section else : self . section = self . sections [ section name ]
def add markup ( self ) : if self . markup and self . markup lines : # get rid of last line of markup if it's empty marks = self . markup lines if len ( marks ) > 0 and not string . strip ( marks [ - 1 ] ) : self . markup lines = marks [ : - 1 ] m = Doc Markup ( self . markup , self . markup lines ) self . markups . append ( m ) self . markup = None self . markup lines = [ ]
def get markup ( self , tag name ) : for m in self . markups : if m . tag == string . lower ( tag name ) : return m return None
def utf8 ( data ) : if isinstance ( data , bytes ) : return data . decode ( "utf-8" , "replace" ) . encode ( "utf-8" ) elif isinstance ( data , text type ) : return data . encode ( "utf-8" ) else : raise Type Error ( "only unicode/bytes types can be converted to UTF-8" )
def delete ( self ) : cset = Change Set ( connection = self . connection , hosted zone id = self . zone id ) cset . add change ( 'DELETE' , self ) return self . connection . change resource record sets ( cset )
def save ( self ) : cset = Change Set ( connection = self . connection , hosted zone id = self . zone id ) # Record sets can't actually be modified. You have to delete the # existing one and create a new one. Since this happens within a single # change set, it appears that the values were modified, when instead # the whole thing is replaced. cset . add change ( 'DELETE' , self ) cset . add change ( 'CREATE' , self ) retval = self . connection . change resource record sets ( cset ) # Now copy the current attribute values on this instance to # the initial vals dict. This will re-set the modification tracking. for key , val in self . initial vals . items ( ) : self . initial vals [ key ] = getattr ( self , key ) return retval
def Parse ID3v1 ( data ) : try : data = data [ data . index ( b'TAG' ) : ] except Value Error : return None if 128 < len ( data ) or len ( data ) < 124 : return None # Issue #69 - Previous versions of Mutagen, when encountering # out-of-spec TDRC and TYER frames of less than four characters, # wrote only the characters available - e.g. "1" or "" - into the # year field. To parse those, reduce the size of the year field. # Amazingly, "0s" works as a struct format string. unpack fmt = "3s30s30s30s%ds29s BB" % ( len ( data ) - 124 ) try : tag , title , artist , album , year , comment , track , genre = unpack ( unpack fmt , data ) except Struct Error : return None if tag != b"TAG" : return None def fix ( data ) : return data . split ( b'\x00' ) [ 0 ] . strip ( ) . decode ( 'latin1' ) title , artist , album , year , comment = map ( fix , [ title , artist , album , year , comment ] ) frames = { } if title : frames [ 'TIT2' ] = TIT2 ( encoding = 0 , text = title ) if artist : frames [ 'TPE1' ] = TPE1 ( encoding = 0 , text = [ artist ] ) if album : frames [ 'TALB' ] = TALB ( encoding = 0 , text = album ) if year : frames [ 'TDRC' ] = TDRC ( encoding = 0 , text = year ) if comment : frames [ 'COMM' ] = COMM ( encoding = 0 , lang = 'eng' , desc = "ID3v1 Comment" , text = comment ) # Don't read a track number if it looks like the comment was # padded with spaces instead of nulls (thanks, Win Amp). if track and ( ( track != 32 ) or ( data [ - 3 ] == b'\x00' [ 0 ] ) ) : frames [ 'TRCK' ] = TRCK ( encoding = 0 , text = str ( track ) ) if genre != 255 : frames [ 'TCON' ] = TCON ( encoding = 0 , text = str ( genre ) ) return frames
def Make ID3v1 ( id3 ) : v1 = { } for v2id , name in { "TIT2" : "title" , "TPE1" : "artist" , "TALB" : "album" } . items ( ) : if v2id in id3 : text = id3 [ v2id ] . text [ 0 ] . encode ( 'latin1' , 'replace' ) [ : 30 ] else : text = b'' v1 [ name ] = text + ( b'\x00' * ( 30 - len ( text ) ) ) if "COMM" in id3 : cmnt = id3 [ "COMM" ] . text [ 0 ] . encode ( 'latin1' , 'replace' ) [ : 28 ] else : cmnt = b'' v1 [ 'comment' ] = cmnt + ( b'\x00' * ( 29 - len ( cmnt ) ) ) if "TRCK" in id3 : try : v1 [ "track" ] = chr ( + id3 [ "TRCK" ] ) except Value Error : v1 [ "track" ] = b'\x00' else : v1 [ "track" ] = b'\x00' if "TCON" in id3 : try : genre = id3 [ "TCON" ] . genres [ 0 ] except Index Error : pass else : if genre in TCON . GENRES : v1 [ "genre" ] = chr ( TCON . GENRES . index ( genre ) ) if "genre" not in v1 : v1 [ "genre" ] = b"\xff" if "TDRC" in id3 : year = text type ( id3 [ "TDRC" ] ) . encode ( 'latin1' , 'replace' ) elif "TYER" in id3 : year = text type ( id3 [ "TYER" ] ) . encode ( 'latin1' , 'replace' ) else : year = b'' v1 [ 'year' ] = ( year + b'\x00\x00\x00\x00' ) [ : 4 ] return ( b'TAG' + v1 [ 'title' ] + v1 [ 'artist' ] + v1 [ 'album' ] + v1 [ 'year' ] + v1 [ 'comment' ] + v1 [ 'track' ] + v1 [ 'genre' ] )
def fullread ( self , size ) : try : if size < 0 : raise Value Error ( 'Requested bytes (%s) less than zero' % size ) if size > self . filesize : raise EOF Error ( 'Requested %#x of %#x (%s)' % ( int ( size ) , int ( self . filesize ) , self . filename ) ) except Attribute Error : pass data = self . fileobj . read ( size ) if len ( data ) != size : raise EOF Error self . readbytes += size return data
def delall ( self , key ) : if key in self : del ( self [ key ] ) else : key = key + ":" for k in self . keys ( ) : if k . startswith ( key ) : del ( self [ k ] )
def loaded frame ( self , tag ) : # turn 2.2 into 2.3/2.4 tags if len ( type ( tag ) . name ) == 3 : tag = type ( tag ) . base ( tag ) self [ tag . Hash Key ] = tag
def update common ( self ) : if "TCON" in self : # Get rid of "(xx)Foobr" format. self [ "TCON" ] . genres = self [ "TCON" ] . genres if self . version < self . V23 : # ID3v2.2 PIC frames are slightly different. pics = self . getall ( "APIC" ) mimes = { "PNG" : "image/png" , "JPG" : "image/jpeg" } self . delall ( "APIC" ) for pic in pics : newpic = APIC ( encoding = pic . encoding , mime = mimes . get ( pic . mime , pic . mime ) , type = pic . type , desc = pic . desc , data = pic . data ) self . add ( newpic ) # ID3v2.2 LNK frames are just way too different to upgrade. self . delall ( "LINK" )
def unload ( self ) : if self . handle != - 1 : lib . Unload Sound ( self . handle ) self . handle = - 1
def adobe glyph values ( ) : lines = string . split ( adobe glyph list , '\n' ) glyphs = [ ] values = [ ] for line in lines : if line : fields = string . split ( line , ';' ) #     print fields[1] + ' - ' + fields[0] subfields = string . split ( fields [ 1 ] , ' ' ) if len ( subfields ) == 1 : glyphs . append ( fields [ 0 ] ) values . append ( fields [ 1 ] ) return glyphs , values
def filter glyph names ( alist , filter ) : count = 0 extras = [ ] for name in alist : try : filtered index = filter . index ( name ) except : extras . append ( name ) return extras
def dump encoding ( file , encoding name , encoding list ) : write = file . write write ( "  /* the following are indices into the SID name table */\n" ) write ( "  static const unsigned short  " + encoding name + "[" + repr ( len ( encoding list ) ) + "] =\n" ) write ( "  {\n" ) line = "    " comma = "" col = 0 for value in encoding list : line += comma line += "%3d" % value comma = "," col += 1 if col == 16 : col = 0 comma = ",\n    " write ( line + "\n  };\n\n\n" )
def dump array ( the array , write , array name ) : write ( "  static const unsigned char  " + array name + "[" + repr ( len ( the array ) ) + "L] =\n" ) write ( "  {\n" ) line = "" comma = "    " col = 0 for value in the array : line += comma line += "%3d" % ord ( value ) comma = "," col += 1 if col == 16 : col = 0 comma = ",\n    " if len ( line ) > 1024 : write ( line ) line = "" write ( line + "\n  };\n\n\n" )
def file exists ( pathname ) : result = 1 try : file = open ( pathname , "r" ) file . close ( ) except : result = None sys . stderr . write ( pathname + " couldn't be accessed\n" ) return result
def make file list ( args = None ) : file list = [ ] # sys.stderr.write( repr( sys.argv[1 :] ) + '\n' ) if not args : args = sys . argv [ 1 : ] for pathname in args : if string . find ( pathname , '*' ) >= 0 : newpath = glob . glob ( pathname ) newpath . sort ( ) # sort files -- this is important because # of the order of files else : newpath = [ pathname ] file list . extend ( newpath ) if len ( file list ) == 0 : file list = None else : # now filter the file list to remove non-existing ones file list = filter ( file exists , file list ) return file list
def writeblocks ( blocks ) : data = [ ] codes = [ [ block . code , block . write ( ) ] for block in blocks ] codes [ - 1 ] [ 0 ] |= 128 for code , datum in codes : byte = chr ( code ) if len ( datum ) > 2 ** 24 : raise error ( "block is too long to write" ) length = struct . pack ( ">I" , len ( datum ) ) [ - 3 : ] data . append ( byte + length + datum ) return b"" . join ( data )
def load ( self , filename ) : self . metadata blocks = [ ] self . tags = None self . cuesheet = None self . seektable = None self . filename = filename fileobj = Strict File Object ( open ( filename , "rb" ) ) try : self . check header ( fileobj ) while self . read metadata block ( fileobj ) : pass finally : fileobj . close ( ) try : self . metadata blocks [ 0 ] . length except ( Attribute Error , Index Error ) : raise FLAC No Header Error ( "Stream info block not found" )
def init logs ( ) : start time = dt . fromtimestamp ( time . time ( ) ) . strftime ( '%Y%m%d %H%M' ) logname = os . path . join ( os . path . expanduser ( "~" ) + "/nano GUI " + start time + ".log" ) handlers = [ logging . File Handler ( logname ) ] logging . basic Config ( format = '%(asctime)s %(message)s' , handlers = handlers , level = logging . INFO ) logging . info ( 'Nano GUI {} started with Nano Plot {}' . format ( version , nanoplot . version ) ) logging . info ( 'Python version is: {}' . format ( sys . version . replace ( '\n' , ' ' ) ) ) return logname
def alias item ( self , alias ) : ident = self . alias [ alias ] return self . items [ ident ]
def initialize bars ( self , sender = None , * * kwargs ) : for bar in self . bars . values ( ) : for initializer in bar . initializers : initializer ( self )
def bind bar ( self , sender = None , * * kwargs ) : bar = kwargs . pop ( 'bar' ) self . bars [ bar . name ] = bar
def validate ( metric class ) : if not hasattr ( metric class , 'label' ) : raise Improperly Configured ( "No 'label' attribute found for metric %s." % metric class . name ) if not hasattr ( metric class , 'widget' ) : raise Improperly Configured ( "No 'widget' attribute found for metric %s." % metric class . name )
def calculate statistics ( stat , frequencies ) : stats = ensure list ( stat ) frequencies = ensure list ( frequencies ) for stat in stats : for f in frequencies : print "Calculating %s (%s)..." % ( stat . name , settings . STATISTIC FREQUENCY DICT [ f ] ) stat . calculate ( f )
def calculate ( cls , frequency = settings . STATISTIC FREQUENCY DAILY , verbose = settings . STATISTIC CALCULATION VERBOSE ) : if verbose : print ( "Calculating statistics for %(class)s..." ) % { 'class' : cls . get label ( ) } start datetime = None end datetime = None # get the latest statistic latest stat = cls . get latest ( frequency ) # work out today's date, truncated to midnight today = datetime . strptime ( datetime . now ( ) . strftime ( "%Y %m %d" ) , "%Y %m %d" ) now = datetime . now ( ) # if this statistic only has cumulative stats available if cls . cumulative : if frequency == settings . STATISTIC FREQUENCY HOURLY : # truncate to the nearest hour start datetime = datetime . strptime ( now . strftime ( "%Y %m %d %H:00:00" ) , "%Y %m %d %H:%M:%S" ) elif frequency == settings . STATISTIC FREQUENCY DAILY : start datetime = today elif frequency == settings . STATISTIC FREQUENCY WEEKLY : # truncate today to the start of this week start datetime = datetime . strptime ( today . strftime ( "%Y %W 0" ) , "%Y %W %w" ) elif frequency == settings . STATISTIC FREQUENCY MONTHLY : # truncate today to the start of this month start datetime = datetime . strptime ( today . strftime ( "%Y %m 1" ) , "%Y %m %d" ) stat , created = cls . objects . get or create ( date time = start datetime , frequency = frequency ) stat . cumulative count = cls . get cumulative ( ) stat . count = ( stat . cumulative count - latest stat . cumulative count ) if latest stat else stat . cumulative count else : # get the date/time at which we should start calculating start datetime = cls . get start datetime ( ) if latest stat is None else latest stat . date time # truncate the start date/time to the appropriate frequency if frequency == settings . STATISTIC FREQUENCY HOURLY : start datetime = datetime . strptime ( start datetime . strftime ( "%Y %m %d %H:00:00" ) , "%Y %m %d %H:%M:%S" ) end datetime = start datetime + timedelta ( hours = 1 ) elif frequency == settings . STATISTIC FREQUENCY DAILY : start datetime = datetime . strptime ( start datetime . strftime ( "%Y %m %d" ) , "%Y %m %d" ) end datetime = start datetime + timedelta ( days = 1 ) elif frequency == settings . STATISTIC FREQUENCY WEEKLY : # start at the beginning of the week of the latest stat start datetime = datetime . strptime ( start datetime . strftime ( "%Y %W 0" ) , "%Y %W %w" ) - timedelta ( days = 7 ) end datetime = start datetime + timedelta ( days = 7 ) elif frequency == settings . STATISTIC FREQUENCY MONTHLY : # start at the beginning of the month of the latest stat start datetime = datetime . strptime ( start datetime . strftime ( "%Y %m 1" ) , "%Y %m %d" ) end datetime = datetime . strptime ( ( start datetime + timedelta ( days = 33 ) ) . strftime ( "%Y %m 1" ) , "%Y %m %d" ) # if we're doing the normal count while start datetime < now : count = cls . get count ( start datetime , end datetime ) cumulative count = 0 if isinstance ( count , tuple ) : cumulative count = count [ 1 ] count = count [ 0 ] else : cumulative count = ( latest stat . cumulative count + count ) if latest stat else count stat , created = cls . objects . get or create ( date time = start datetime , frequency = frequency ) stat . count = count stat . cumulative count = cumulative count stat . save ( ) latest stat = stat # update the dates/time window start datetime = end datetime if frequency == settings . STATISTIC FREQUENCY HOURLY : end datetime += timedelta ( hours = 1 ) elif frequency == settings . STATISTIC FREQUENCY DAILY : end datetime += timedelta ( days = 1 ) elif frequency == settings . STATISTIC FREQUENCY WEEKLY : end datetime += timedelta ( days = 7 ) elif frequency == settings . STATISTIC FREQUENCY MONTHLY : end datetime = datetime . strptime ( ( start datetime + timedelta ( days = 33 ) ) . strftime ( "%Y %m 1" ) , "%Y %m %d" )
def handle ( self , * args , * * kwargs ) : frequency = kwargs [ 'frequency' ] frequencies = settings . STATISTIC FREQUENCY ALL if frequency == 'a' else ( frequency . split ( ',' ) if ',' in frequency else [ frequency ] ) if kwargs [ 'list' ] : maintenance . list statistics ( ) # if we're supposed to calculate the latest statistics elif kwargs [ 'calculate' ] : maintenance . calculate statistics ( maintenance . get statistic by name ( kwargs [ 'calculate' ] ) , frequencies ) # pure reset of statistic(s) elif kwargs [ 'reset' ] : maintenance . reset statistics ( maintenance . get statistic by name ( kwargs [ 'reset' ] ) , frequencies , kwargs [ 'reset cumulative' ] ) # recalculation of statistic(s) elif kwargs [ 'recalculate' ] : maintenance . reset statistics ( maintenance . get statistic by name ( kwargs [ 'recalculate' ] ) , frequencies , kwargs [ 'reset cumulative' ] , True )
def get GET array ( request , var name , fail silently = True ) : vals = request . GET . getlist ( var name ) if not vals : if fail silently : return [ ] else : raise Exception , ( "No array called '%(varname)s' in GET variables" ) % { 'varname' : var name } return vals
def get GET bool ( request , var name , default = True ) : val = request . GET . get ( var name , default ) if isinstance ( val , str ) or isinstance ( val , unicode ) : val = True if val [ 0 ] == 't' else False return val
def get next colour ( ) : colour = settings . GECKOBOARD COLOURS [ get next colour . cur colour ] get next colour . cur colour += 1 if get next colour . cur colour >= len ( settings . GECKOBOARD COLOURS ) : get next colour . cur colour = 0 return colour
def geckoboard number widget ( request ) : params = get gecko params ( request , days back = 7 ) metric = Metric . objects . get ( uid = params [ 'uid' ] ) try : latest stat = metric . statistics . filter ( frequency = params [ 'frequency' ] ) . order by ( '-date time' ) [ 0 ] except Index Error : return ( 0 , 0 ) try : prev stat = metric . statistics . filter ( frequency = params [ 'frequency' ] , date time lte = latest stat . date time - timedelta ( days = params [ 'days back' ] ) ) . order by ( '-date time' ) [ 0 ] except Index Error : # if there is no previous stat return ( latest stat . cumulative count , 0 ) if params [ 'cumulative' ] else ( latest stat . count , 0 ) return ( latest stat . cumulative count , prev stat . cumulative count ) if params [ 'cumulative' ] else ( latest stat . count , prev stat . count )
def geckoboard line chart ( request ) : params = get gecko params ( request , cumulative = False , days back = 7 ) metric = Metric . objects . get ( uid = params [ 'uid' ] ) start date = datetime . now ( ) - timedelta ( days = params [ 'days back' ] ) stats = [ s for s in metric . statistics . filter ( frequency = params [ 'frequency' ] , date time gte = start date ) . order by ( 'date time' ) ] if len ( stats ) == 0 : raise Exception , ( "No statistics for metric %(metric)s." ) % { 'metric' : params [ 'uid' ] } dates = [ stats [ 0 ] . date time ] # get up to 3 dates from the stats if len ( stats ) >= 3 : mid = len ( stats ) / 2 if not mid : mid = 1 dates . extend ( [ stats [ mid ] . date time , stats [ - 1 ] . date time ] ) elif len ( stats ) == 2 : dates . extend ( [ stats [ - 1 ] . date time ] ) return ( [ s . count for s in stats ] , dates , metric . title , )
def geckoboard geckometer ( request ) : params = get gecko params ( request , cumulative = True ) metric = Metric . objects . get ( uid = params [ 'uid' ] ) return ( metric . latest count ( frequency = params [ 'frequency' ] , count = not params [ 'cumulative' ] , cumulative = params [ 'cumulative' ] ) , params [ 'min' ] , params [ 'max' ] )
def geckoboard funnel ( request , frequency = settings . STATISTIC FREQUENCY DAILY ) : # get all the parameters for this function params = get gecko params ( request , cumulative = True ) metrics = Metric . objects . filter ( uid in = params [ 'uids' ] ) items = [ ( metric . latest count ( frequency = params [ 'frequency' ] , count = not params [ 'cumulative' ] , cumulative = params [ 'cumulative' ] ) , metric . title ) for metric in metrics ] return { 'items' : items , 'type' : params [ 'type' ] , 'percentage' : params [ 'percentage' ] , 'sort' : params [ 'sort' ] , }
def get active stats ( self ) : stats = [ ] for gadget in self . registry . values ( ) : for s in gadget . stats : if s not in stats : stats . append ( s ) return stats
def get context data ( self , * * kwargs ) : #max columns, max rows = self.get max dimension() context = { 'gadgets' : self . registry , 'columns' : self . columns , 'rows' : self . rows , 'column ratio' : 100 - self . columns * 2 , 'row ratio' : 100 - self . rows * 2 , } context . update ( kwargs ) return context
def error ( self , message , code = 1 ) : print >> sys . stderr , message sys . exit ( code )
def valid ( schema = None ) : def dec ( fun ) : @ wraps ( fun ) def d func ( self , ctx , data , * a , * * kw ) : try : validate ( data [ 'params' ] , schema ) except Validation Error as err : raise Invalid Params ( err ) except Schema Error as err : raise Internal Error ( err ) return fun ( self , ctx , data [ 'params' ] , * a , * * kw ) return d func return dec
def long input ( prompt = 'Multi-line input\n' + 'Enter EOF on a blank line to end ' + '(ctrl-D in *nix, ctrl-Z in windows)' , maxlines = None , maxlength = None ) : lines = [ ] print ( prompt ) lnum = 1 try : while True : if maxlines : if lnum > maxlines : break else : if maxlength : lines . append ( string input ( '' ) [ : maxlength ] ) else : lines . append ( string input ( '' ) ) lnum += 1 else : if maxlength : lines . append ( string input ( '' ) [ : maxlength ] ) else : lines . append ( string input ( '' ) ) except EOF Error : pass finally : return '\n' . join ( lines )
def list input ( prompt = 'List input - enter each item on a seperate line\n' + 'Enter EOF on a blank line to end ' + '(ctrl-D in *nix, ctrl-Z in windows)' , maxitems = None , maxlength = None ) : lines = [ ] print ( prompt ) inum = 1 try : while True : if maxitems : if inum > maxitems : break else : if maxlength : lines . append ( string input ( '' ) [ : maxlength ] ) else : lines . append ( string input ( '' ) ) inum += 1 else : if maxlength : lines . append ( string input ( '' ) [ : maxlength ] ) else : lines . append ( string input ( '' ) ) except EOF Error : pass finally : return lines
def outfile input ( extension = None ) : fileok = False while not fileok : filename = string input ( 'File name? ' ) if extension : if not filename . endswith ( extension ) : if extension . startswith ( '.' ) : filename = filename + extension else : filename = filename + '.' + extension if os . path . isfile ( filename ) : choice = choice input ( prompt = filename + ' already exists. Overwrite?' , options = [ 'y' , 'n' ] ) if choice == 'y' : try : nowtime = time . time ( ) with open ( filename , 'a' ) as f : os . utime ( filename , ( nowtime , nowtime ) ) fileok = True except IO Error : print ( 'Write permission denied on ' + filename + '. Try again.' ) except Permission Error : print ( 'Write permission denied on ' + filename + '. Try again.' ) except File Not Found Error : print ( filename + ': directory not found. Try again.' ) else : choice = choice input ( prompt = filename + ' does not exist. Create it?' , options = [ 'y' , 'n' ] ) if choice == 'y' : try : nowtime = time . time ( ) with open ( filename , 'w' ) as f : os . utime ( filename , ( nowtime , nowtime ) ) fileok = True except IO Error : print ( 'Write permission denied on ' + filename + '. Try again.' ) except Permission Error : print ( 'Write permission denied on ' + filename + '. Try again.' ) except File Not Found Error : print ( filename + ': directory not found. Try again.' ) return filename
def winner ( self ) : hm Score = self . home score ( ) aw Score = self . away score ( ) if hm Score > aw Score : return self . home ( ) elif hm Score < aw Score : return self . away ( ) else : return None
def standings ( self ) : doc = self . get sub doc ( 'standings' ) east table = doc ( 'table#divs standings E' ) east df = pd . Data Frame ( sportsref . utils . parse table ( east table ) ) east df . sort values ( 'wins' , ascending = False , inplace = True ) east df [ 'seed' ] = range ( 1 , len ( east df ) + 1 ) east df [ 'conference' ] = 'E' west table = doc ( 'table#divs standings W' ) west df = sportsref . utils . parse table ( west table ) west df . sort values ( 'wins' , ascending = False , inplace = True ) west df [ 'seed' ] = range ( 1 , len ( west df ) + 1 ) west df [ 'conference' ] = 'W' full df = pd . concat ( [ east df , west df ] , axis = 0 ) . reset index ( drop = True ) full df [ 'team id' ] = full df . team id . str . extract ( r'(\w+)\W*\(\d+\)' , expand = False ) full df [ 'gb' ] = [ gb if isinstance ( gb , int ) or isinstance ( gb , float ) else 0 for gb in full df [ 'gb' ] ] full df = full df . drop ( 'has class full table' , axis = 1 ) expanded table = doc ( 'table#expanded standings' ) expanded df = sportsref . utils . parse table ( expanded table ) full df = pd . merge ( full df , expanded df , on = 'team id' ) return full df
def roy voting ( self ) : url = '{}/awards/awards {}.html' . format ( sportsref . nba . BASE URL , self . yr ) doc = pq ( sportsref . utils . get html ( url ) ) table = doc ( 'table#roy' ) df = sportsref . utils . parse table ( table ) return df
def linescore ( self ) : doc = self . get main doc ( ) table = doc ( 'table#line score' ) columns = [ th . text ( ) for th in table ( 'tr.thead' ) . items ( 'th' ) ] columns [ 0 ] = 'team id' data = [ [ sportsref . utils . flatten links ( td ) for td in tr ( 'td' ) . items ( ) ] for tr in table ( 'tr.thead' ) . next all ( 'tr' ) . items ( ) ] return pd . Data Frame ( data , index = [ 'away' , 'home' ] , columns = columns , dtype = 'float' )
def get class instance key ( cls , args , kwargs ) : l = [ id ( cls ) ] for arg in args : l . append ( id ( arg ) ) l . extend ( ( k , id ( v ) ) for k , v in kwargs . items ( ) ) return tuple ( sorted ( l ) )
def stats per game ( self , kind = 'R' , summary = False ) : return self . get stats table ( 'per game' , kind = kind , summary = summary )
def stats totals ( self , kind = 'R' , summary = False ) : return self . get stats table ( 'totals' , kind = kind , summary = summary )
def stats per36 ( self , kind = 'R' , summary = False ) : return self . get stats table ( 'per minute' , kind = kind , summary = summary )
def stats per100 ( self , kind = 'R' , summary = False ) : return self . get stats table ( 'per poss' , kind = kind , summary = summary )
def stats advanced ( self , kind = 'R' , summary = False ) : return self . get stats table ( 'advanced' , kind = kind , summary = summary )
def stats shooting ( self , kind = 'R' , summary = False ) : return self . get stats table ( 'shooting' , kind = kind , summary = summary )
def stats pbp ( self , kind = 'R' , summary = False ) : return self . get stats table ( 'advanced pbp' , kind = kind , summary = summary )
def Game Play Finder ( * * kwargs ) : querystring = kwargs to qs ( * * kwargs ) url = '{}?{}' . format ( GPF URL , querystring ) # if verbose, print url if kwargs . get ( 'verbose' , False ) : print ( url ) html = utils . get html ( url ) doc = pq ( html ) # parse table = doc ( 'table#all plays' ) plays = utils . parse table ( table ) # parse score column if 'score' in plays . columns : o Score , d Score = zip ( * plays . score . apply ( lambda s : s . split ( '-' ) ) ) plays [ 'team Score' ] = o Score plays [ 'opp Score' ] = d Score # add parsed pbp info if 'description' in plays . columns : plays = pbp . expand details ( plays , detail Col = 'description' ) return plays
def get ( self ) : self . write ( "Memory Session Object Demo:" ) if "sv" in self . session : current value = self . session [ "sv" ] self . write ( "current sv value is %s, and system will delete this value.<br/>" % self . session [ "sv" ] ) self . session . delete ( "sv" ) if "sv" not in self . session : self . write ( "current sv value is empty" ) else : self . write ( "Session data not found" )
def Player Season Finder ( * * kwargs ) : if 'offset' not in kwargs : kwargs [ 'offset' ] = 0 player Seasons = [ ] while True : querystring = kwargs to qs ( * * kwargs ) url = '{}?{}' . format ( PSF URL , querystring ) if kwargs . get ( 'verbose' , False ) : print ( url ) html = utils . get html ( url ) doc = pq ( html ) table = doc ( 'table#results' ) df = utils . parse table ( table ) if df . empty : break this Season = list ( zip ( df . player id , df . year ) ) player Seasons . extend ( this Season ) if doc ( '*:contains("Next Page")' ) : kwargs [ 'offset' ] += 100 else : break return player Seasons
def wait ( self ) : with self . cvar : self . count . value += 1 self . cvar . notify all ( ) while self . count . value < self . n procs : self . cvar . wait ( )
def wait ( self ) : self . barrier A . wait ( ) # The current barrier (barrier A) is switched with the reserve barrier. # This is because the current barrier cannot be safely reset until the reserve barrier has been passed. self . barrier A , self . barrier B = self . barrier B , self . barrier A self . barrier A . reset ( )
def close ( self ) : self . read queue . put ( Queue Closed ) self . write queue . put ( Queue Closed )
def read varint ( self ) : buff = self . fd . read ( 1 ) if buff == b'' : return 0 while ( bytearray ( buff ) [ - 1 ] & 0x80 ) >> 7 == 1 : # while the MSB is 1 new byte = self . fd . read ( 1 ) if new byte == b'' : raise EOF Error ( 'unexpected EOF.' ) buff += new byte varint , = decode Varint ( buff , 0 ) return varint
def close ( self ) : self . flush ( ) if self . myfd is not None : self . myfd . close ( ) self . myfd = None
def flush ( self ) : if not self . is output ( ) : return count = len ( self . write buff ) if count == 0 : return encode Varint ( self . fd . write , count , True ) for obj in self . write buff : obj str = obj . Serialize To String ( ) encode Varint ( self . fd . write , len ( obj str ) , True ) self . fd . write ( obj str ) self . write buff = [ ]
def get game dir ( self , username = False ) : if not self . common and not username : raise Runtime Error ( "Can't determine this game's directory without username" ) if self . common : subdir = "common" else : subdir = "username" subsubdir = self . dir if WIN32 or CYGWIN : subsubdir = subsubdir . lower ( ) return os . path . join ( subdir , subsubdir )
def with ignored exceptions ( self , * ignored exceptions ) : for exception in ignored exceptions : self . ignored exceptions = self . ignored exceptions + ( exception , ) return self
def send ( self , message , read reply = False ) : sock = None for tries in range ( 0 , 3 ) : try : sock = socket . socket ( socket . AF INET , socket . SOCK STREAM ) sock . connect ( ( self . host , self . PORT ) ) break except ( Connection Error , Broken Pipe Error ) : if tries == 3 : print ( "socket connect failed." ) return sleep ( 0.1 ) sock . send ( codecs . decode ( message , 'hex codec' ) ) if read reply : sleep ( 0.1 ) reply = '' tries = 0 max tries = 20 while len ( reply ) < len ( message ) and tries < max tries : try : reply += codecs . encode ( sock . recv ( self . BUFFERSIZE ) , 'hex' ) . decode ( "utf-8" ) except ( Connection Error , Broken Pipe Error ) : pass tries += 1 sock . close ( ) if tries >= max tries : return return reply sock . close ( )
def power off ( self ) : status = self . status ( ) if status [ 'power' ] : # Setting power off when it is already off can cause hangs self . send ( self . CMD POWERSAVE + self . CMD OFF )
def power on ( self ) : status = self . status ( ) if not status [ 'power' ] : self . send ( self . CMD ON , read reply = True ) sleep ( 0.5 )
def set volume ( self , volume ) : if 0 <= volume <= 200 : volume = format ( volume , "02x" ) # Convert to hex self . send ( self . CMD VOLUME + volume )
def select source ( self , source ) : status = self . status ( ) if status [ 'power' ] : # Changing source when off may hang NAD7050 if status [ 'source' ] != source : # Setting the source to the current source will hang the NAD7050 if source in self . SOURCES : self . send ( self . CMD SOURCE + self . SOURCES [ source ] , read reply = True )
def exec command ( self , domain , function , operator , value = None ) : if operator in CMDS [ domain ] [ function ] [ 'supported operators' ] : if operator is '=' and value is None : raise Value Error ( 'No value provided' ) if value is None : cmd = '' . join ( [ CMDS [ domain ] [ function ] [ 'cmd' ] , operator ] ) else : cmd = '' . join ( [ CMDS [ domain ] [ function ] [ 'cmd' ] , operator , str ( value ) ] ) else : raise Value Error ( 'Invalid operator provided %s' % operator ) if self . open connection ( ) : # For telnet the first \r / \n is recommended only self . telnet . write ( ( '' . join ( [ '\r' , cmd , '\n' ] ) . encode ( ) ) ) # Could raise eg. socket.error, Unicode Error, let the client handle it # Test 3 x buffer is completely empty # With the default timeout that means a delay at # about 3+ seconds loop = 3 while loop : msg = self . telnet . read until ( '\n' . encode ( ) , self . timeout ) # Could raise eg. EOF Error, Unicode Error, let the client handle it if msg == "" : # Nothing in buffer loop -= 1 continue msg = msg . decode ( ) . strip ( '\r\n' ) # Could raise eg. Unicode Error, let the client handle it #print("NAD reponded with '%s'" % msg) # Wait for the response that equals the requested domain.function if msg . strip ( ) . split ( '=' ) [ 0 ] . lower ( ) == '.' . join ( [ domain , function ] ) . lower ( ) : # b'Main.Volume=-12\r will return -12 return msg . strip ( ) . split ( '=' ) [ 1 ] raise Runtime Error ( 'Failed to read response' ) raise Runtime Error ( 'Failed to open connection' )
def crc ( plaintext ) : if not isinstance ( plaintext , six . binary type ) : plaintext = six . b ( plaintext ) return ( zlib . crc32 ( plaintext ) % 2147483647 ) & 0xffffffff
def missing schema ( self , html , song name ) : #html=self.get html response(url) soup = Beautiful Soup ( html ) name = ' ' . join ( song name ) print '%s not found' % name print "But you can download any of the following songs :" a list = soup . find All ( 'a' , 'touch' ) for x in xrange ( len ( a list ) - 1 ) : r = a list [ x ] p = str ( r ) q = re . sub ( r'<a.*/>|<span.*">|</span>|</a>|<a.*html">|<font.*">|</font>' , '' , p ) print q
def list of all href ( self , html ) : soup = Beautiful Soup ( html ) links = [ ] a list = soup . find All ( 'a' , 'touch' ) for x in xrange ( len ( a list ) - 1 ) : link = a list [ x ] . get ( 'href' ) name = a list [ x ] name = str ( name ) name = re . sub ( r'<a.*/>|<span.*">|</span>|</a>|<a.*html">|<font.*">|</font>' , '' , name ) name = re . sub ( r'^[0-9]+\.' , '' , name ) links . append ( [ link , name ] ) #quit() return links
def check if song name ( self , html ) : soup = Beautiful Soup ( html ) a list = soup . find All ( 'a' , 'touch' ) #print a list text = [ str ( x ) for x in a list ] text = '' . join ( text ) text = text . lower ( ) string1 = 'download in 48 kbps' string2 = 'download in 128 kbps' string3 = 'download in 320 kbps' href = '' if string3 in text : #print 'Downloading in 320 kbps' href = a list [ 2 ] . get ( 'href' ) elif string2 in text : #print 'Downloading in 128 kbps' href = a list [ 1 ] . get ( 'href' ) elif string1 in text : #print 'Downloading in 48 kbps'	 href = a list [ 0 ] . get ( 'href' ) else : return ( True , 'nothing' ) return ( False , href )
def google url ( self , song name , website ) : name = '+' . join ( song name ) prefix = 'https://www.google.co.in/search?q=' website = website . split ( " " ) suffix = '+' . join ( website ) url = prefix + name + suffix #print url return url
def get html response ( self , url ) : print "Downloading page %s .." % url try : response = requests . get ( url , timeout = 50 ) except requests . exceptions . SSL Error : try : response = requests . get ( url , verify = False , timeout = 50 ) except requests . exceptions . Request Exception as e : print e quit ( ) except requests . exceptions . Request Exception as e : print e quit ( ) return response . content
def file download using requests ( self , url ) : file name = url . split ( '/' ) [ - 1 ] if os . path . exists ( os . path . join ( os . getcwd ( ) , file name ) ) : print 'File already exists' return #print 'Downloading file %s '%file name #print 'Downloading from %s'%url try : r = requests . get ( url , stream = True , timeout = 200 ) except requests . exceptions . SSL Error : try : response = requests . get ( url , stream = True , verify = False , timeout = 200 ) except requests . exceptions . Request Exception as e : print e quit ( ) except requests . exceptions . Request Exception as e : print e quit ( ) chunk size = 1024 total size = int ( r . headers [ 'Content-Length' ] ) total chunks = total size / chunk size file iterable = r . iter content ( chunk size = chunk size ) tqdm iter = tqdm ( iterable = file iterable , total = total chunks , unit = 'KB' , leave = False ) with open ( file name , 'wb' ) as f : for data in tqdm iter : f . write ( data ) #total size=float(r.headers['Content-Length'])/(1024*1024) print 'Downloaded file %s ' % file name
def file download using wget ( self , url ) : file name = url . split ( '/' ) [ - 1 ] print 'Downloading file %s ' % file name command = 'wget -c --read-timeout=50 --tries=3 -q --show-progress --no-check-certificate ' url = '"' + url + '"' command = command + url os . system ( command )
def main ( ) : #print VERSION from commands . download import Download options = docopt ( doc , version = VERSION ) #print "You reached here" #print options print "working." p = Download ( options ) p . run ( )
def find Station Codes By City ( city name , token ) : req = requests . get ( API ENDPOINT SEARCH , params = { 'token' : token , 'keyword' : city name } ) if req . status code == 200 and req . json ( ) [ "status" ] == "ok" : return [ result [ "uid" ] for result in req . json ( ) [ "data" ] ] else : return [ ]
def get location observation ( lat , lng , token ) : req = requests . get ( API ENDPOINT GEO % ( lat , lng ) , params = { 'token' : token } ) if req . status code == 200 and req . json ( ) [ "status" ] == "ok" : return parse observation response ( req . json ( ) [ "data" ] ) return { }
def parse observation response ( json ) : logging . debug ( json ) iaqi = json [ 'iaqi' ] result = { 'idx' : json [ 'idx' ] , 'city' : json . get ( 'city' , '' ) , 'aqi' : json [ 'aqi' ] , 'dominentpol' : json . get ( "dominentpol" , '' ) , 'time' : json [ 'time' ] [ 's' ] , 'iaqi' : [ { 'p' : item , 'v' : iaqi [ item ] [ 'v' ] } for item in iaqi ] } return result
def compilers ( self ) : return [ self . environment . compilers . get ( e ) for e in self . compiler extensions ]
def mimetype ( self ) : return ( self . environment . mimetypes . get ( self . format extension ) or self . compiler mimetype or 'application/octet-stream' )
def compiler mimetype ( self ) : for compiler in reversed ( self . compilers ) : if compiler . result mimetype : return compiler . result mimetype return None
def compiler format extension ( self ) : for extension , mimetype in self . environment . mimetypes . items ( ) : if mimetype == self . compiler mimetype : return extension return None
def register ( self , mimetype , processor ) : if mimetype not in self or processor not in self [ mimetype ] : self . setdefault ( mimetype , [ ] ) . append ( processor )
def register defaults ( self ) : self . mimetypes . register defaults ( ) self . preprocessors . register defaults ( ) self . postprocessors . register defaults ( )
def table ( name , auth = None , eager = True ) : auth = auth or [ ] dynamodb = boto . connect dynamodb ( * auth ) table = dynamodb . get table ( name ) return Table ( table = table , eager = eager )
def tables ( auth = None , eager = True ) : auth = auth or [ ] dynamodb = boto . connect dynamodb ( * auth ) return [ table ( t , auth , eager = eager ) for t in dynamodb . list tables ( ) ]
def metadata id ( item ) : if Crates . metadata category ( item ) == CATEGORY CRATES : return str ( item [ 'id' ] ) else : ts = item [ 'fetched on' ] ts = str to datetime ( ts ) return str ( ts . timestamp ( ) )
def fetch crate owner team ( self , crate id ) : raw owner team = self . client . crate attribute ( crate id , 'owner team' ) owner team = json . loads ( raw owner team ) return owner team
def fetch crate owner user ( self , crate id ) : raw owner user = self . client . crate attribute ( crate id , 'owner user' ) owner user = json . loads ( raw owner user ) return owner user
def fetch crate versions ( self , crate id ) : raw versions = self . client . crate attribute ( crate id , "versions" ) version downloads = json . loads ( raw versions ) return version downloads
def fetch crate version downloads ( self , crate id ) : raw version downloads = self . client . crate attribute ( crate id , "downloads" ) version downloads = json . loads ( raw version downloads ) return version downloads
def summary ( self ) : path = urijoin ( CRATES API URL , CATEGORY SUMMARY ) raw content = self . fetch ( path ) return raw content
def crates ( self , from page = 1 ) : path = urijoin ( CRATES API URL , CATEGORY CRATES ) raw crates = self . fetch items ( path , from page ) return raw crates
def crate ( self , crate id ) : path = urijoin ( CRATES API URL , CATEGORY CRATES , crate id ) raw crate = self . fetch ( path ) return raw crate
def fetch items ( self , path , page = 1 ) : fetch data = True parsed crates = 0 total crates = 0 while fetch data : logger . debug ( "Fetching page: %i" , page ) try : payload = { 'sort' : 'alphabetical' , 'page' : page } raw content = self . fetch ( path , payload = payload ) content = json . loads ( raw content ) parsed crates += len ( content [ 'crates' ] ) if not total crates : total crates = content [ 'meta' ] [ 'total' ] except requests . exceptions . HTTP Error as e : logger . error ( "HTTP exception raised - %s" , e . response . text ) raise e yield raw content page += 1 if parsed crates >= total crates : fetch data = False
def fetch ( self , url , payload = None ) : response = super ( ) . fetch ( url , payload = payload ) return response . text
def get questions ( self , offset = None ) : page = Kitsune Client . FIRST PAGE if offset : page += int ( offset / Kitsune Client . ITEMS PER PAGE ) while True : api questions url = urijoin ( self . base url , '/question' ) + '/' params = { "page" : page , "ordering" : "updated" } questions = self . fetch ( api questions url , params ) yield questions questions json = json . loads ( questions ) next uri = questions json [ 'next' ] if not next uri : break page += 1
def fetch ( self , url , params ) : logger . debug ( "Kitsune client calls API: %s params: %s" , url , str ( params ) ) response = super ( ) . fetch ( url , payload = params ) return response . text
def get items ( self , category = CATEGORY EVENT , offset = REMO DEFAULT OFFSET ) : more = True # There are more items to be processed next uri = None # URI for the next items page query page = Re Mo Client . FIRST PAGE page += int ( offset / Re Mo Client . ITEMS PER PAGE ) if category == CATEGORY EVENT : api = self . api events url elif category == CATEGORY ACTIVITY : api = self . api activities url elif category == CATEGORY USER : api = self . api users url else : raise Value Error ( category + ' not supported in Re Mo' ) while more : params = { "page" : page , "orderby" : "ASC" } logger . debug ( "Re Mo client calls AP Iv2: %s params: %s" , api , str ( params ) ) raw items = self . fetch ( api , payload = params ) yield raw items items data = json . loads ( raw items ) next uri = items data [ 'next' ] if not next uri : more = False else : # https://reps.mozilla.org/remo/api/remo/v1/events/?orderby=ASC&page=269 parsed uri = urllib . parse . urlparse ( next uri ) parsed params = urllib . parse . parse qs ( parsed uri . query ) page = parsed params [ 'page' ] [ 0 ]
def io priority ( self ) : return ( self . iocb . aio reqprio if self . iocb . u . c . flags & libaio . IOCB FLAG IOPRIO else None )
def get cells ( self ) : logger . info ( "Retrieving all cells spreadsheet data ..." ) logger . debug ( "Mozilla Club client calls API: %s" , self . base url ) raw cells = self . fetch ( self . base url ) return raw cells . text
def parse ( self ) : nevents wrong = 0 feed json = json . loads ( self . feed ) if 'entry' not in feed json [ 'feed' ] : return self . cells = feed json [ 'feed' ] [ 'entry' ] self . ncell = 0 event fields = self . get event fields ( ) # Process all events reading the rows according to the event template # The only way to detect the end of row is looking to the # number of column. When the max number is reached (cell cols) the next # cell is from the next row. while self . ncell < len ( self . cells ) : # Process the next row (event) getting all cols to build the event event = self . get next event ( event fields ) if event [ 'Date of Event' ] is None or event [ 'Club Name' ] is None : logger . warning ( "Wrong event data: %s" , event ) nevents wrong += 1 continue yield event logger . info ( "Total number of wrong events: %i" , nevents wrong )
def get data files ( dirname ) : flist = [ ] for dirpath , dirnames , filenames in os . walk ( dirname ) : for fname in filenames : flist . append ( osp . join ( dirpath , fname ) ) return flist
def export formats ( self , pid type ) : if pid type not in self . export formats : fmts = self . app . config . get ( 'RECORDS UI EXPORT FORMATS' , { } ) . get ( pid type , { } ) self . export formats [ pid type ] = sorted ( [ ( k , v ) for k , v in fmts . items ( ) if v ] , key = lambda x : x [ 1 ] [ 'order' ] , ) return self . export formats [ pid type ]
def permission factory ( self ) : if self . permission factory is None : imp = self . app . config [ 'RECORDS UI DEFAULT PERMISSION FACTORY' ] self . permission factory = obj or import string ( imp ) return self . permission factory
def records ( ) : import uuid from invenio records . api import Record from invenio pidstore . models import Persistent Identifier , PID Status # Record 1 - Live record with db . session . begin nested ( ) : pid1 = Persistent Identifier . create ( 'recid' , '1' , object type = 'rec' , object uuid = rec1 uuid , status = PID Status . REGISTERED ) Record . create ( { 'title' : 'Registered ' , 'authors' : [ { 'name' : 'Ellis Jonathan' } , { 'name' : 'Higgs Peter' } , ] , 'access' : 'open' , 'keywords' : [ 'CERN' , 'higgs' ] , } , id = rec1 uuid ) Persistent Identifier . create ( 'recid' , '2' , object type = 'rec' , object uuid = rec2 uuid , status = PID Status . REGISTERED ) Record . create ( { 'title' : 'Registered ' , 'authors' : [ { 'name' : 'Ellis Jonathan' } , { 'name' : 'Higgs Peter' } , ] , 'access' : 'closed' , 'keywords' : [ 'CERN' , 'higgs' ] , } , id = rec2 uuid ) # Record 3 - Deleted PID with record rec3 uuid = uuid . uuid4 ( ) pid = Persistent Identifier . create ( 'recid' , '3' , object type = 'rec' , object uuid = rec3 uuid , status = PID Status . REGISTERED ) pid . delete ( ) Record . create ( { 'title' : 'Live ' } , id = rec3 uuid ) # Record 4 - Deleted PID without a record Persistent Identifier . create ( 'recid' , '4' , status = PID Status . DELETED ) # Record 5 - Registered PID without a record Persistent Identifier . create ( 'recid' , '5' , status = PID Status . REGISTERED ) # Record 6 - Redirected PID pid = Persistent Identifier . create ( 'recid' , '6' , status = PID Status . REGISTERED ) pid . redirect ( pid1 ) # Record 7 - Redirected non existing endpoint doi = Persistent Identifier . create ( 'doi' , '10.1234/foo' , status = PID Status . REGISTERED ) pid = Persistent Identifier . create ( 'recid' , '7' , status = PID Status . REGISTERED ) pid . redirect ( doi ) # Record 8 - Unregistered PID Persistent Identifier . create ( 'recid' , '8' , status = PID Status . RESERVED ) db . session . commit ( )
def time callable ( self , name , target , rate = None , args = ( ) , kwargs = { } ) : # type: (str, Callable, float, Tuple, Dict) -> Chronometer assert callable ( target ) if rate is None : rate = self . rate else : assert sample rate ( rate ) start time = time ( ) # type: float result = target ( * args , * * kwargs ) self . since ( name , start time , rate ) return result
def increment ( self , name , count = 1 , rate = 1 ) : # type: (str, int, float) -> None if self . should send metric ( name , rate ) : self . request ( Counter ( self . create metric name for request ( name ) , int ( count ) , rate ) . to request ( ) )
def timing ( self , name , milliseconds , rate = 1 ) : # type: (str, float, float) -> None if self . should send metric ( name , rate ) : milliseconds = int ( milliseconds ) self . request ( Timer ( self . create metric name for request ( name ) , milliseconds , rate ) . to request ( ) )
def timing since ( self , name , start time , rate = 1 ) : # type: (str, Union[float, datetime], float) -> None duration = 0 # type: float if isinstance ( start time , datetime ) : duration = ( datetime . now ( start time . tzinfo ) - start time ) . total seconds ( ) * 1000 elif is numeric ( start time ) : assert start time > 0 duration = ( time ( ) - start time ) * 1000 else : raise Value Error ( "start time should be a timestamp or a datetime" ) self . timing ( name , duration , rate )
def gauge ( self , name , value , rate = 1 ) : # type: (str, float, float) -> None if self . should send metric ( name , rate ) : if not is numeric ( value ) : value = float ( value ) self . request ( Gauge ( self . create metric name for request ( name ) , value , rate ) . to request ( ) )
def gauge delta ( self , name , delta , rate = 1 ) : # type: (str, float, float) -> None if self . should send metric ( name , rate ) : if not is numeric ( delta ) : delta = float ( delta ) self . request ( Gauge Delta ( self . create metric name for request ( name ) , delta , rate ) . to request ( ) )
def set ( self , name , value , rate = 1 ) : # type: (str, str, float) -> None if self . should send metric ( name , rate ) : value = str ( value ) self . request ( Set ( self . create metric name for request ( name ) , value , rate ) . to request ( ) )
def request ( self , data ) : # type: (str) -> None data = bytearray ( "{}\n" . format ( data ) . encode ( ) ) self . prepare batches for storage ( len ( data ) ) self . batches [ - 1 ] . extend ( data )
def batch client ( self , size = 512 ) : # type: (int) -> Batch Client batch client = Batch Client ( self . host , self . port , self . prefix , size ) self . configure client ( batch client ) return batch client
def unit client ( self ) : # type: () -> Client client = Client ( self . host , self . port , self . prefix ) self . configure client ( client ) return client
def flush ( self ) : # type: () -> Batch Client address = self . remote address while len ( self . batches ) > 0 : self . socket . sendto ( self . batches [ 0 ] , address ) self . batches . popleft ( ) return self
def my permission factory ( record , * args , * * kwargs ) : def can ( self ) : rec = Record . get record ( record . id ) return rec . get ( 'access' , '' ) == 'open' return type ( 'My Permission Checker' , ( ) , { 'can' : can } ) ( )
def batch client ( self , size = 512 ) : # type: (int) -> TCP Batch Client batch client = TCP Batch Client ( self . host , self . port , self . prefix , size ) self . configure client ( batch client ) return batch client
def flush ( self ) : # type: () -> TCP Batch Client while len ( self . batches ) > 0 : self . socket . sendall ( self . batches [ 0 ] ) self . batches . popleft ( ) return self
def unit client ( self ) : # type: () -> TCP Client client = TCP Client ( self . host , self . port , self . prefix ) self . configure client ( client ) return client
def convert As Open Math ( term , converter ) : # if we already have openmath, or have some of our magic helpers, use interpret As Open Math if hasattr ( term , " ishelper" ) and term . ishelper or isinstance ( term , om . OM Any ) : return interpret As Open Math ( term ) # next try to convert using the converter if converter is not None : try : converted = converter . to openmath ( term ) except Exception as e : converted = None if isinstance ( converted , om . OM Any ) : return converted # fallback to the openmath helper return interpret As Open Math ( term )
def to python ( self , omobj ) : # general overrides if omobj . class in self . omclass to py : return self . omclass to py [ omobj . class ] ( omobj ) # oms elif isinstance ( omobj , om . OM Symbol ) : return self . lookup to python ( omobj . cdbase , omobj . cd , omobj . name ) # oma elif isinstance ( omobj , om . OM Application ) : elem = self . to python ( omobj . elem ) arguments = [ self . to python ( x ) for x in omobj . arguments ] return elem ( * arguments ) raise Value Error ( 'Cannot convert object of class %s to Python.' % omobj . class . name )
def to openmath ( self , obj ) : for cl , conv in reversed ( self . conv to om ) : if cl is None or isinstance ( obj , cl ) : try : return conv ( obj ) except Cannot Convert Error : continue if hasattr ( obj , ' openmath ' ) : return obj . openmath ( ) raise Value Error ( 'Cannot convert %r to Open Math.' % obj )
def init app ( self , app ) : app . config . setdefault ( 'REDIS URLS' , { 'main' : 'redis://localhost:6379/0' , 'admin' : 'redis://localhost:6379/1' , } ) app . before request ( self . before request ) self . app = app
def valid choices ( choices ) : for key , value in choices : if isinstance ( value , ( list , tuple ) ) : for key , in value : yield key else : yield key
def split model kwargs ( kw ) : from collections import defaultdict model fields = { } fields agrs = defaultdict ( lambda : { } ) for key in kw . keys ( ) : if ' ' in key : field , , subfield = key . partition ( ' ' ) fields agrs [ field ] [ subfield ] = kw [ key ] else : model fields [ key ] = kw [ key ] return model fields , fields agrs
def any form default ( form cls , * * kwargs ) : form data = { } form files = { } form fields , fields args = split model kwargs ( kwargs ) for name , field in form cls . base fields . iteritems ( ) : if name in form fields : form data [ name ] = kwargs [ name ] else : form data [ name ] = any form field ( field , * * fields args [ name ] ) return form data , form files
def field choices attibute ( function ) : def wrapper ( field , * * kwargs ) : if hasattr ( field . widget , 'choices' ) : return random . choice ( list ( valid choices ( field . widget . choices ) ) ) return function ( field , * * kwargs ) return wrapper
def model choice field data ( field , * * kwargs ) : data = list ( field . queryset [ : 10 ] ) if data : return random . choice ( data ) else : raise Type Error ( 'No %s available in queryset' % field . queryset . model )
def tag ( version = version ) : build = local ( "git tag {0}" . format ( version ) ) if build . succeeded : local ( "git push --tags" )
def any field blank ( function ) : def wrapper ( field , * * kwargs ) : if kwargs . get ( 'isnull' , False ) : return None if field . blank and random . random < 0.1 : return None return function ( field , * * kwargs ) return wrapper
def any file field ( field , * * kwargs ) : def get some file ( path ) : subdirs , files = field . storage . listdir ( path ) if files : result file = random . choice ( files ) instance = field . storage . open ( "%s/%s" % ( path , result file ) ) . file return Field File ( instance , field , result file ) for subdir in subdirs : result = get some file ( "%s/%s" % ( path , subdir ) ) if result : return result result = get some file ( field . upload to ) if result is None and not field . null : raise Type Error ( "Can't found file in %s for non nullable File Field" % field . upload to ) return result
def any filepath field ( field , * * kwargs ) : def get some file ( path ) : subdirs , files = [ ] , [ ] for entry in os . listdir ( path ) : entry path = os . path . join ( path , entry ) if os . path . isdir ( entry path ) : subdirs . append ( entry path ) else : if not field . match or re . match ( field . match , entry ) : files . append ( entry path ) if files : return random . choice ( files ) if field . recursive : for subdir in subdirs : result = get some file ( subdir ) if result : return result result = get some file ( field . path ) if result is None and not field . null : raise Type Error ( "Can't found file in %s for non nullable File Path Field" % field . path ) return result
def decode ( data ) : data = bytearray ( data ) # <- python 2/3 compatibility fix result = bytearray ( ) pos = 0 while pos < len ( data ) : header byte = data [ pos ] if header byte > 127 : header byte -= 256 pos += 1 if 0 <= header byte <= 127 : result . extend ( data [ pos : pos + header byte + 1 ] ) pos += header byte + 1 elif header byte == - 128 : pass else : result . extend ( [ data [ pos ] ] * ( 1 - header byte ) ) pos += 1 return bytes ( result )
def encode ( data ) : if len ( data ) == 0 : return data if len ( data ) == 1 : return b'\x00' + data data = bytearray ( data ) result = bytearray ( ) buf = bytearray ( ) pos = 0 repeat count = 0 MAX LENGTH = 127 # we can safely start with RAW as empty RAW sequences # are handled by finish raw() state = 'RAW' def finish raw ( ) : if len ( buf ) == 0 : return result . append ( len ( buf ) - 1 ) result . extend ( buf ) buf [ : ] = bytearray ( ) def finish rle ( ) : result . append ( 256 - ( repeat count - 1 ) ) result . append ( data [ pos ] ) while pos < len ( data ) - 1 : current byte = data [ pos ] if data [ pos ] == data [ pos + 1 ] : if state == 'RAW' : # end of RAW data finish raw ( ) state = 'RLE' repeat count = 1 elif state == 'RLE' : if repeat count == MAX LENGTH : # restart the encoding finish rle ( ) repeat count = 0 # move to next byte repeat count += 1 else : if state == 'RLE' : repeat count += 1 finish rle ( ) state = 'RAW' repeat count = 0 elif state == 'RAW' : if len ( buf ) == MAX LENGTH : # restart the encoding finish raw ( ) buf . append ( current byte ) pos += 1 if state == 'RAW' : buf . append ( data [ pos ] ) finish raw ( ) else : repeat count += 1 finish rle ( ) return bytes ( result )
def add ( self , name , path ) : if not ( os . path . exists ( path ) ) : raise Value Error ( "Workspace path `%s` doesn't exists." % path ) if ( self . exists ( name ) ) : raise Value Error ( "Workspace `%s` already exists." % name ) self . config [ "workspaces" ] [ name ] = { "path" : path , "repositories" : { } } self . config . write ( )
def remove ( self , name ) : if not ( self . exists ( name ) ) : raise Value Error ( "Workspace `%s` doesn't exists." % name ) self . config [ "workspaces" ] . pop ( name , 0 ) self . config . write ( )
def list ( self ) : ws list = { } for key , value in self . config [ "workspaces" ] . items ( ) : ws list [ key ] = dict ( { "name" : key } , * * value ) return ws list
def repository exists ( self , workspace , repo ) : if not self . exists ( workspace ) : return False workspaces = self . list ( ) return repo in workspaces [ workspace ] [ "repositories" ]
def sync ( self , ws name ) : path = self . config [ "workspaces" ] [ ws name ] [ "path" ] repositories = self . config [ "workspaces" ] [ ws name ] [ "repositories" ] logger = logging . get Logger ( name ) color = Color ( ) for r in os . listdir ( path ) : try : repo = Repository ( os . path . join ( path , r ) ) except Repository Error : continue else : repositories [ r ] = repo . path for repo name , path in repositories . items ( ) : logger . info ( color . colored ( " - %s" % repo name , "blue" ) ) self . config [ "workspaces" ] [ ws name ] [ "repositories" ] self . config . write ( )
def clone ( url , path ) : adapter = None if url [ : 4 ] == "git@" or url [ - 4 : ] == ".git" : adapter = Git ( path ) if url [ : 6 ] == "svn://" : adapter = Svn ( path ) if url [ : 6 ] == "bzr://" : adapter = Bzr ( path ) if url [ : 9 ] == "ssh://hg@" : adapter = Hg ( path ) if adapter is None : raise Repository Adapter Not Found ( "Can't find adapter for `%s` repository url" % url ) return adapter . clone ( url )
def check version ( ) : import requests r = requests . get ( 'https://pypi.python.org/pypi/ndio/json' ) . json ( ) r = r [ 'info' ] [ 'version' ] if r != version : print ( "A newer version of ndio is available. " + "'pip install -U ndio' to update." ) return r
def execute ( self , args ) : if args . name is not None : self . print workspace ( args . name ) elif args . all is not None : self . print all ( )
def print update ( self , repo name , repo path ) : color = Color ( ) self . logger . info ( color . colored ( "=> [%s] %s" % ( repo name , repo path ) , "green" ) ) try : repo = Repository ( repo path ) repo . update ( ) except Repository Error as e : self . logger . error ( e ) pass print ( "\n" )
def set console handler ( self , debug = False ) : console = logging . Stream Handler ( ) console . set Formatter ( Formatter ( LFORMAT ) ) if not debug : console . set Level ( logging . INFO ) self . add Handler ( console )
def execute ( self , command , path = None ) : logger = logging . get Logger ( name ) self . check executable ( ) logger . debug ( "Executing command `%s` (cwd: %s)" % ( command , path ) ) process = subprocess . Popen ( command , shell = True , cwd = path , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) stdout , stderr = process . communicate ( ) exit code = process . wait ( ) if stdout : logger . info ( stdout . decode ( "utf-8" ) ) if stderr : if exit code != 0 : logger . error ( stderr . decode ( "utf-8" ) ) else : logger . info ( stderr . decode ( "utf-8" ) ) return process
def print workspace ( self , name ) : path list = find path ( name , self . config ) if len ( path list ) == 0 : self . logger . error ( "No matches for `%s`" % name ) return False for name , path in path list . items ( ) : self . print status ( name , path )
def print status ( self , repo name , repo path ) : color = Color ( ) self . logger . info ( color . colored ( "=> [%s] %s" % ( repo name , repo path ) , "green" ) ) try : repo = Repository ( repo path ) repo . status ( ) except Repository Error as e : self . logger . error ( e ) pass print ( "\n" )
def post cutout no chunking blosc ( self , token , channel , x start , y start , z start , data , resolution ) : data = numpy . expand dims ( data , axis = 0 ) blosc data = blosc . pack array ( data ) url = self . url ( "{}/{}/blosc/{}/{},{}/{},{}/{},{}/0,0/" . format ( token , channel , resolution , x start , x start + data . shape [ 3 ] , y start , y start + data . shape [ 2 ] , z start , z start + data . shape [ 1 ] ) ) req = self . remote utils . post url ( url , data = blosc data , headers = { 'Content-Type' : 'application/octet-stream' } ) if req . status code is not 200 : raise Remote Data Upload Error ( req . text ) else : return True
def clone ( self , url ) : return self . execute ( "%s branch %s %s" % ( self . executable , url , self . path ) )
def get version ( ) : requirement = pkg resources . Requirement . parse ( "yoda" ) provider = pkg resources . get provider ( requirement ) return provider . version
def mix and match ( name , greeting = 'Hello' , yell = False ) : say = '%s, %s' % ( greeting , name ) if yell : print '%s!' % say . upper ( ) else : print '%s.' % say
def option decorator ( name , greeting , yell ) : # Use the @option decorator when you need more control over the # command line options. say = '%s, %s' % ( greeting , name ) if yell : print '%s!' % say . upper ( ) else : print '%s.' % say
def parse ( self ) : parser = self . subparser . add parser ( "show" , help = "Show workspace details" , description = "Show workspace details." ) group = parser . add mutually exclusive group ( required = True ) group . add argument ( '--all' , action = 'store true' , help = "All workspaces" ) group . add argument ( 'name' , type = str , help = "Workspace name" , nargs = '?' )
def execute ( self , args ) : if args . name is not None : self . show workspace ( slashes2dash ( args . name ) ) elif args . all is not None : self . show all ( )
def show workspace ( self , name ) : if not self . workspace . exists ( name ) : raise Value Error ( "Workspace `%s` doesn't exists." % name ) color = Color ( ) workspaces = self . workspace . list ( ) self . logger . info ( "<== %s workspace ==>" % color . colored ( name , "green" ) ) self . logger . info ( "\t Path: %s" % workspaces [ name ] [ "path" ] ) self . logger . info ( "\t Number of repositories: %s" % color . colored ( len ( workspaces [ name ] [ "repositories" ] ) , "yellow" ) ) repo colored = color . colored ( "Repositories" , "blue" ) path colored = color . colored ( "Path" , "blue" ) trepositories = Pretty Table ( [ repo colored , path colored , color . colored ( "+" , "blue" ) ] ) trepositories . align [ repo colored ] = "l" trepositories . align [ path colored ] = "l" for repo name in workspaces [ name ] [ "repositories" ] : fullname = "%s/%s" % ( name , repo name ) fullpath = find path ( fullname , self . config ) [ fullname ] try : repo = Repository ( fullpath ) repo scm = repo . get scm ( ) except Repository Adapter Not Found : repo scm = None trepositories . add row ( [ color . colored ( repo name , "cyan" ) , fullpath , repo scm ] ) self . logger . info ( trepositories )
def show all ( self ) : for ws in self . workspace . list ( ) . keys ( ) : self . show workspace ( ws ) print ( "\n\n" )
def RAMON ( typ ) : if six . PY2 : lookup = [ str , unicode ] elif six . PY3 : lookup = [ str ] if type ( typ ) is int : return ramon types [ typ ] elif type ( typ ) in lookup : return ramon types [ types [ typ ] ]
def nd json ( self , dataset , project , channel list , metadata ) : nd dict = { } nd dict [ 'dataset' ] = self . dataset dict ( * dataset ) nd dict [ 'project' ] = self . project dict ( * project ) nd dict [ 'metadata' ] = metadata nd dict [ 'channels' ] = { } for channel name , value in channel list . items ( ) : nd dict [ 'channels' ] [ channel name ] = self . channel dict ( * value ) return json . dumps ( nd dict , sort keys = True , indent = 4 )
def dataset dict ( self , dataset name , imagesize , voxelres , offset , timerange , scalinglevels , scaling ) : dataset dict = { } dataset dict [ 'dataset name' ] = dataset name dataset dict [ 'imagesize' ] = imagesize dataset dict [ 'voxelres' ] = voxelres if offset is not None : dataset dict [ 'offset' ] = offset if timerange is not None : dataset dict [ 'timerange' ] = timerange if scalinglevels is not None : dataset dict [ 'scalinglevels' ] = scalinglevels if scaling is not None : dataset dict [ 'scaling' ] = scaling return dataset dict
def channel dict ( self , channel name , datatype , channel type , data url , file format , file type , exceptions , resolution , windowrange , readonly ) : channel dict = { } channel dict [ 'channel name' ] = channel name channel dict [ 'datatype' ] = datatype channel dict [ 'channel type' ] = channel type if exceptions is not None : channel dict [ 'exceptions' ] = exceptions if resolution is not None : channel dict [ 'resolution' ] = resolution if windowrange is not None : channel dict [ 'windowrange' ] = windowrange if readonly is not None : channel dict [ 'readonly' ] = readonly channel dict [ 'data url' ] = data url channel dict [ 'file format' ] = file format channel dict [ 'file type' ] = file type return channel dict
def project dict ( self , project name , token name , public ) : project dict = { } project dict [ 'project name' ] = project name if token name is not None : if token name == '' : project dict [ 'token name' ] = project name else : project dict [ 'token name' ] = token name else : project dict [ 'token name' ] = project name if public is not None : project dict [ 'public' ] = public return project dict
def identify imagesize ( self , image type , image path = '/tmp/img.' ) : dims = ( ) try : if ( image type . lower ( ) == 'png' ) : dims = np . shape ( ndpng . load ( '{}{}' . format ( image path , image type ) ) ) elif ( image type . lower ( ) == 'tif' or image type . lower ( ) == 'tiff' ) : dims = np . shape ( ndtiff . load ( '{}{}' . format ( image path , image type ) ) ) else : raise Value Error ( "Unsupported image type." ) except : raise OS Error ( 'The file was not accessible at {}{}' . format ( image path , image type ) ) return dims [ : : - 1 ]
def verify path ( self , data , verifytype ) : # Insert try and catch blocks try : token name = data [ "project" ] [ "token name" ] except : token name = data [ "project" ] [ "project name" ] channel names = list ( data [ "channels" ] . copy ( ) . keys ( ) ) imgsz = data [ 'dataset' ] [ 'imagesize' ] for i in range ( 0 , len ( channel names ) ) : channel type = data [ "channels" ] [ channel names [ i ] ] [ "channel type" ] path = data [ "channels" ] [ channel names [ i ] ] [ "data url" ] aws pattern = re . compile ( "^(http:\/\/)(.+)(\.s3\.amazonaws\.com)" ) file type = data [ "channels" ] [ channel names [ i ] ] [ "file type" ] if "offset" in data [ "dataset" ] : offset = data [ "dataset" ] [ "offset" ] [ 0 ] else : offset = 0 if ( aws pattern . match ( path ) ) : verifytype = VERIFY BY SLICE if ( channel type == "timeseries" ) : timerange = data [ "dataset" ] [ "timerange" ] try : assert ( timerange [ 0 ] != timerange [ 1 ] ) except Assertion Error : raise Value Error ( ) for j in range ( timerange [ 0 ] , timerange [ 1 ] + 1 ) : # Test for tifs or such? Currently test for just not # empty if ( verifytype == VERIFY BY FOLDER ) : work path = "{}/{}/{}/time{}/" . format ( path , token name , channel names [ i ] , ( "%04d" % j ) ) elif ( verifytype == VERIFY BY SLICE ) : work path = "{}/{}/{}/time{}/{}.{}" . format ( path , token name , channel names [ i ] , ( "%04d" % j ) , ( "%04d" % offset ) , file type ) else : raise Type Error ( 'Incorrect verify method' ) # Check for accessibility try : if ( verifytype == VERIFY BY FOLDER ) : resp = requests . head ( work path ) assert ( resp . status code == 200 ) elif ( verifytype == VERIFY BY SLICE ) : resp = requests . get ( work path , stream = True , verify = False ) with open ( '/tmp/img.{}' . format ( file type ) , 'wb' ) as out file : shutil . copyfileobj ( resp . raw , out file ) out file . close ( ) assert ( resp . status code == 200 ) resp . close ( ) except Assertion Error : raise OS Error ( . format ( resp . status code , work path ) ) # Attempt to Verify imagesize here try : if ( verifytype == VERIFY BY SLICE ) : assert ( list ( self . identify imagesize ( file type ) ) == imgsz [ 0 : 2 ] ) except : raise Value Error ( ) else : # Test for tifs or such? Currently test for just not empty if ( verifytype == VERIFY BY FOLDER ) : work path = "{}/{}/{}/" . format ( path , token name , channel names [ i ] ) elif ( verifytype == VERIFY BY SLICE ) : work path = "{}/{}/{}/{}.{}" . format ( path , token name , channel names [ i ] , ( "%04d" % offset ) , file type ) else : raise Type Error ( 'Incorrect verify method' ) # Check for accessibility if ( verifytype == VERIFY BY FOLDER ) : resp = requests . head ( work path ) elif ( verifytype == VERIFY BY SLICE ) : resp = requests . get ( work path , stream = True , verify = False ) with open ( '/tmp/img.{}' . format ( file type ) , 'wb' ) as out file : shutil . copyfileobj ( resp . raw , out file ) out file . close ( ) resp . close ( ) if ( resp . status code >= 300 ) : raise OS Error ( . format ( resp . status code , work path ) ) # Attempt to Verify imagesize here try : if ( verifytype == VERIFY BY SLICE ) : assert ( list ( self . identify imagesize ( file type ) ) == imgsz [ 0 : 2 ] ) except : raise Value Error ( )
def put data ( self , data ) : URL Path = self . oo . url ( "auto Ingest/" ) # URL Path = 'https://{}/ca/auto Ingest/'.format(self.oo.site host) try : response = requests . post ( URL Path , data = json . dumps ( data ) , verify = False ) assert ( response . status code == 200 ) print ( "From ndio: {}" . format ( response . content ) ) except : raise OS Error ( . format ( response . status code ) )
def find path ( name , config , wsonly = False ) : workspace = Workspace ( config ) config = config [ "workspaces" ] path list = { } if name . find ( '/' ) != - 1 : wsonly = False try : ws , repo = name . split ( '/' ) except Value Error : raise Value Error ( "There is too many / in `name` argument. " "Argument syntax: `workspace/repository`." ) if ( workspace . exists ( ws ) ) : if ( repo in config [ ws ] [ "repositories" ] ) : path name = "%s/%s" % ( ws , repo ) path list [ path name ] = config [ ws ] [ "repositories" ] [ repo ] for ws name , ws in sorted ( config . items ( ) ) : if ( name == ws name ) : if wsonly is True : return { ws name : ws [ "path" ] } repositories = sorted ( config [ ws name ] [ "repositories" ] . items ( ) ) for name , path in repositories : path list [ "%s/%s" % ( ws name , name ) ] = path break for repo name , repo path in sorted ( ws [ "repositories" ] . items ( ) ) : if ( repo name == name ) : path list [ "%s/%s" % ( ws name , repo name ) ] = repo path return path list
def nvim io recover ( self , io : Nvim IO Recover [ A ] ) -> Nvim IO [ B ] : return eval step ( self . vim ) ( io . map ( lambda a : a ) )
def read codeml output ( filename , df , altall cutoff = 0.2 , ) : # Read paml output. with open ( filename , 'r' ) as f : data = f . read ( ) # Rip all trees out of the codeml output. regex = re . compile ( '\([()\w\:. ,]+;' ) trees = regex . findall ( data ) anc tree = trees [ 2 ] # First tree in codeml file is the original input tree tip tree = dendropy . Tree . get ( data = trees [ 0 ] , schema = 'newick' ) # Third tree in codeml fule is ancestor tree. anc tree = dendropy . Tree . get ( data = trees [ 2 ] , schema = 'newick' ) # Main tree to return tree = tip tree # Map ancestors onto main tree object ancestors = anc tree . internal nodes ( ) for i , node in enumerate ( tree . internal nodes ( ) ) : node . label = ancestors [ i ] . label # Map nodes onto dataframe. df [ 'reconstruct label' ] = None for node in tree . postorder node iter ( ) : # Ignore parent node if node . parent node is None : pass elif node . is leaf ( ) : node label = node . taxon . label parent label = node . parent node . label # Set node label. df . loc [ df . uid == node label , 'reconstruct label' ] = node label # Set parent label. parent id = df . loc [ df . uid == node label , 'parent' ] . values [ 0 ] df . loc [ df . id == parent id , 'reconstruct label' ] = node . parent node . label elif node . is internal ( ) : label = node . label parent id = df . loc [ df . reconstruct label == label , 'parent' ] . values [ 0 ] df . loc [ df . id == parent id , 'reconstruct label' ] = node . parent node . label # Compile a regular expression to find blocks of data for internal nodes node regex = re . compile ( """Prob distribution at node [0-9]+, by site[-\w():.\s]+\n""" ) # Strip the node number from this block of data. node num regex = re . compile ( "[0-9]+" ) # Get dataframes for all ancestors. df [ 'ml sequence' ] = None df [ 'ml posterior' ] = None df [ 'alt sequence' ] = None df [ 'alt posterior' ] = None for node in node regex . findall ( data ) : # Get node label node label = node num regex . search ( node ) . group ( 0 ) # Compile regex for matching site data site regex = re . compile ( "(?:\w\(\w.\w{3}\) )+" ) # Iterate through each match for site data. ml sequence , ml posterior , alt sequence , alt posterior = [ ] , [ ] , [ ] , [ ] for site in site regex . findall ( node ) : # Iterate through residues scores = [ float ( site [ i + 2 : i + 7 ] ) for i in range ( 0 , len ( site ) , 9 ) ] residues = [ site [ i ] for i in range ( 0 , len ( site ) , 9 ) ] # Get the indices of sorted scores sorted score index = [ i [ 0 ] for i in sorted ( enumerate ( scores ) , key = lambda x : x [ 1 ] , reverse = True ) ] ml idx = sorted score index [ 0 ] alt idx = sorted score index [ 1 ] # Should we keep alterative site. ml sequence . append ( residues [ ml idx ] ) ml posterior . append ( scores [ ml idx ] ) if scores [ alt idx ] < altall cutoff : alt idx = ml idx alt sequence . append ( residues [ alt idx ] ) alt posterior . append ( scores [ alt idx ] ) keys = [ "ml sequence" , "ml posterior" , "alt sequence" , "alt posterior" ] vals = [ "" . join ( ml sequence ) , sum ( ml posterior ) / len ( ml posterior ) , "" . join ( alt sequence ) , sum ( alt posterior ) / len ( alt posterior ) , ] df . loc [ df . reconstruct label == node label , keys ] = vals return df
def ugettext ( message , context = None ) : stripped = strip whitespace ( message ) message = add context ( context , stripped ) if context else stripped ret = django ugettext ( message ) # If the context isn't found, we need to return the string without it return stripped if ret == message else ret
def ungettext ( singular , plural , number , context = None ) : singular stripped = strip whitespace ( singular ) plural stripped = strip whitespace ( plural ) if context : singular = add context ( context , singular stripped ) plural = add context ( context , plural stripped ) else : singular = singular stripped plural = plural stripped ret = django nugettext ( singular , plural , number ) # If the context isn't found, the string is returned as it came if ret == singular : return singular stripped elif ret == plural : return plural stripped return ret
def install jinja translations ( ) : class Translation ( object ) : ugettext = staticmethod ( ugettext ) ungettext = staticmethod ( ungettext ) import jingo jingo . env . install gettext translations ( Translation )
def exclusive ns ( guard : State Guard [ A ] , desc : str , thunk : Callable [ ... , NS [ A , B ] ] , * a : Any ) -> Do : yield guard . acquire ( ) log . debug2 ( lambda : f'exclusive: {desc}' ) state , response = yield N . ensure failure ( thunk ( * a ) . run ( guard . state ) , guard . release ) yield N . delay ( lambda v : unsafe update state ( guard , state ) ) yield guard . release ( ) log . debug2 ( lambda : f'release: {desc}' ) yield N . pure ( response )
def percent ( data , part , total ) : try : return round ( 100 * float ( data [ part ] ) / float ( data [ total ] ) , 1 ) except Zero Division Error : return 0
def get cache stats ( server name = None ) : server info = { } for svr in mc client . get stats ( ) : svr info = svr [ 0 ] . split ( ' ' ) svr name = svr info [ 0 ] svr stats = svr [ 1 ] svr stats [ 'bytes percent' ] = percent ( svr stats , 'bytes' , 'limit maxbytes' ) svr stats [ 'get hit rate' ] = percent ( svr stats , 'get hits' , 'cmd get' ) svr stats [ 'get miss rate' ] = percent ( svr stats , 'get misses' , 'cmd get' ) if server name and server name == svr name : return svr stats server info [ svr name ] = svr stats return server info
def get cache slabs ( server name = None ) : server info = { } for svr in mc client . get slabs ( ) : svr info = svr [ 0 ] . split ( ' ' ) svr name = svr info [ 0 ] if server name and server name == svr name : return svr [ 1 ] server info [ svr name ] = svr [ 1 ] return server info
def context data ( data , request = None ) : try : return dict ( site . each context ( request ) . items ( ) + data . items ( ) ) except Attribute Error : return data
def server status ( request ) : data = { 'cache stats' : get cache stats ( ) , 'can get slabs' : hasattr ( mc client , 'get slabs' ) , } return render to response ( 'memcache admin/server status.html' , data , Request Context ( request ) )
def dashboard ( request ) : # mc client will be a dict if memcached is not configured if not isinstance ( mc client , dict ) : cache stats = get cache stats ( ) else : cache stats = None if cache stats : data = context data ( { 'title' : ( 'Memcache Dashboard' ) , 'cache stats' : cache stats , 'can get slabs' : hasattr ( mc client , 'get slabs' ) , 'REFRESH RATE' : SETTINGS [ 'REFRESH RATE' ] , } , request ) template = 'memcache admin/dashboard.html' else : data = context data ( { 'title' : ( 'Memcache Dashboard - Error' ) , 'error message' : ( 'Unable to connect to a memcache server.' ) , } , request ) template = 'memcache admin/dashboard error.html' return render to response ( template , data , Request Context ( request ) )
def stats ( request , server name ) : server name = server name . strip ( '/' ) data = context data ( { 'title' : ( 'Memcache Statistics for %s' ) % server name , 'cache stats' : get cache stats ( server name ) , } , request ) return render to response ( 'memcache admin/stats.html' , data , Request Context ( request ) )
def slabs ( request , server name ) : data = context data ( { 'title' : ( 'Memcache Slabs for %s' ) % server name , 'cache slabs' : get cache slabs ( server name ) , } , request ) return render to response ( 'memcache admin/slabs.html' , data , Request Context ( request ) )
def human bytes ( value ) : value = float ( value ) if value >= 1073741824 : gigabytes = value / 1073741824 size = '%.2f GB' % gigabytes elif value >= 1048576 : megabytes = value / 1048576 size = '%.2f MB' % megabytes elif value >= 1024 : kilobytes = value / 1024 size = '%.2f KB' % kilobytes else : size = '%.2f B' % value return size
def add ( self , * * kwargs ) : for key in kwargs : if type ( kwargs [ key ] ) == str : self . children [ key ] = Directory ( kwargs [ key ] ) else : self . children [ key ] = kwargs [ key ] self . children [ key ] . env = self self . children [ key ] . apply config ( Config Applicator ( self . config ) ) self . children [ key ] . prepare ( )
def apply config ( self , applicator ) : if type ( self . fpath ) == str : self . fpath = applicator . apply ( self . fpath )
def path ( self ) : if self . parent : return os . path . join ( self . parent . path , self . fpath ) else : return self . fpath
def read ( self ) : with open ( self . path ) as f : d = f . read ( ) return d
def configure ( self ) : # build a file handler for this file handler = logging . File Handler ( self . path , delay = True ) # if we got a format string, create a formatter with it if self . format : handler . set Formatter ( logging . Formatter ( self . format ) ) # if we got a string for the formatter, assume it's the name of a # formatter in the environment's config if type ( self . formatter ) == str : if self . env and self . env . config . logging . dict config . formatters [ self . formatter ] : d = self . env . config . logging . dict config . formatters [ self . formatter ] . to dict ( ) handler . set Formatter ( logging . Formatter ( * * d ) ) elif type ( self . formatter ) == dict : # if it's a dict it must be the actual formatter params handler . set Formatter ( logging . Formatter ( * * self . formatter ) ) # add the file handler to whatever loggers were specified if len ( self . loggers ) : for name in self . loggers : logging . get Logger ( name ) . add Handler ( handler ) else : # none specified, just add it to the root logger logging . get Logger ( ) . add Handler ( handler )
def apply config ( self , applicator ) : if type ( self . path ) == str : self . path = applicator . apply ( self . path ) for key in self . children : self . children [ key ] . apply config ( applicator )
def path ( self ) : p = '' if self . parent and self . parent . path : p = os . path . join ( p , self . parent . path ) if self . base : p = os . path . join ( p , self . base ) if self . path : p = os . path . join ( p , self . path ) return p
def remove ( self , recursive = True , ignore error = True ) : try : if recursive or self . cleanup == 'recursive' : shutil . rmtree ( self . path ) else : os . rmdir ( self . path ) except Exception as e : if not ignore error : raise e
def path to ( self , path ) : return os . path . join ( self . path , str ( path ) )
def list ( self ) : return [ File ( f , parent = self ) for f in os . listdir ( self . path ) ]
def write ( self , filename , data , mode = 'w' ) : with open ( self . path to ( str ( filename ) ) , mode ) as f : f . write ( data )
def read ( self , filename ) : with open ( self . path to ( str ( filename ) ) ) as f : d = f . read ( ) return d
def add ( self , * args , * * kwargs ) : for key in kwargs : if isinstance ( kwargs [ key ] , str ) : self . children [ key ] = File ( kwargs [ key ] ) else : self . children [ key ] = kwargs [ key ] self . children [ key ] . parent = self self . children [ key ] . env = self . env added = [ ] for arg in args : if isinstance ( arg , File ) : self . children [ arg . name ] = arg self . children [ arg . name ] . parent = self self . children [ arg . name ] . env = self . env elif isinstance ( arg , str ) : f = File ( arg ) added . append ( f ) self . children [ arg ] = f self . children [ arg ] . parent = self self . children [ arg ] . env = self . env else : raise Type Error ( type ( arg ) ) # if we were passed a single file/filename, return the File object for convenience if len ( added ) == 1 : return added [ 0 ] if len ( args ) == 1 : return args [ 0 ]
def save ( self ) : with open ( self . path , 'w' ) as f : f . write ( yaml . dump ( dict ( self . d ) ) )
def load ( self ) : if os . path . exists ( self . path ) : with open ( self . path , 'r' ) as f : self . d = yaml . safe load ( f . read ( ) . replace ( '\t' , ' ' * 4 ) )
def cleanup ( self ) : if os . path . exists ( self . path ) : os . remove ( self . path )
def get value ( self ) : if self . path : try : container , last = self . resolve path ( ) return container [ last ] except Key Error : return None except Index Error : return None else : return self . data
def load ( self , reload = False ) : if reload or not self . loaded : # load defaults if self . defaults file and type ( self . defaults file ) == str : self . defaults file = File ( self . defaults file , parent = self . parent ) defaults = { } if self . defaults file : defaults = yaml . safe load ( self . defaults file . read ( ) . replace ( '\t' , '    ' ) ) # load data data = { } if self . exists : data = yaml . safe load ( self . read ( ) . replace ( '\t' , '    ' ) ) # initialise with the loaded data self . defaults = defaults self . data = copy . deepcopy ( self . defaults ) self . update ( data = data ) # if specified, apply environment variables if self . apply env : self . update ( Config Env ( self . env prefix ) ) self . loaded = True return self
def apply to str ( self , obj ) : toks = re . split ( '({config:|})' , obj ) newtoks = [ ] try : while len ( toks ) : tok = toks . pop ( 0 ) if tok == '{config:' : # pop the config variable, look it up var = toks . pop ( 0 ) val = self . config [ var ] # if we got an empty node, then it didn't exist if type ( val ) == Config Node and val == None : raise Key Error ( "No such config variable '{}'" . format ( var ) ) # add the value to the list newtoks . append ( str ( val ) ) # pop the '}' toks . pop ( 0 ) else : # not the start of a config block, just append it to the list newtoks . append ( tok ) return '' . join ( newtoks ) except Index Error : pass return obj
def process input ( self ) : try : pyngus . read socket input ( self . connection , self . socket ) except Exception as e : LOG . error ( "Exception on socket read: %s" , str ( e ) ) self . connection . close input ( ) self . connection . close ( ) self . connection . process ( time . time ( ) )
def send output ( self ) : try : pyngus . write socket output ( self . connection , self . socket ) except Exception as e : LOG . error ( "Exception on socket write: %s" , str ( e ) ) self . connection . close output ( ) self . connection . close ( ) self . connection . process ( time . time ( ) )
def send request ( self ) : msg = Message ( ) msg . subject = "An RPC call!" msg . address = self . to msg . reply to = self . reply to msg . body = self . method msg . correlation id = 5 # whatever... print ( "sending RPC call request: %s" % str ( self . method ) ) # @todo send timeout self. sender.send(msg, self, None, time.time() + # 10) self . sender . send ( msg , self )
def configure ( self , target address , source address , handler , properties ) : self . handler = handler self . properties = properties dynamic props = None if properties : dynamic props = properties . get ( "dynamic-node-properties" ) mode = dist modes . get ( properties . get ( "distribution-mode" ) ) if mode is not None : self . pn link . source . distribution mode = mode mode = snd settle modes . get ( properties . get ( "snd-settle-mode" ) ) if mode is not None : self . pn link . snd settle mode = mode mode = rcv settle modes . get ( properties . get ( "rcv-settle-mode" ) ) if mode is not None : self . pn link . rcv settle mode = mode if target address is None : if not self . pn link . is sender : raise Exception ( "Dynamic target not allowed" ) self . pn link . target . dynamic = True if dynamic props : self . pn link . target . properties . clear ( ) self . pn link . target . properties . put dict ( dynamic props ) elif target address : self . pn link . target . address = target address if source address is None : if not self . pn link . is receiver : raise Exception ( "Dynamic source not allowed" ) self . pn link . source . dynamic = True if dynamic props : self . pn link . source . properties . clear ( ) self . pn link . source . properties . put dict ( dynamic props ) elif source address : self . pn link . source . address = source address
def source address ( self ) : # If link is a sender, source is determined by the local # value, else use the remote. if self . pn link . is sender : return self . pn link . source . address else : return self . pn link . remote source . address
def target address ( self ) : # If link is a receiver, target is determined by the local # value, else use the remote. if self . pn link . is receiver : return self . pn link . target . address else : return self . pn link . remote target . address
def session closed ( self ) : # if link not already closed: if self . endpoint state & proton . Endpoint . REMOTE ACTIVE : # simulate close received self . process remote state ( ) elif self . endpoint state & proton . Endpoint . REMOTE UNINIT : # locally created link, will never come up self . failed = True self . link failed ( "Parent session closed." )
def reject ( self , pn condition = None ) : self . pn link . source . type = proton . Terminus . UNSPECIFIED super ( Sender Link , self ) . reject ( pn condition )
def process delivery ( self , pn delivery ) : if pn delivery . tag in self . send requests : if pn delivery . settled or pn delivery . remote state : # remote has reached a 'terminal state' outcome = pn delivery . remote state state = Sender Link . DISPOSITION STATE MAP . get ( outcome , self . UNKNOWN ) pn disposition = pn delivery . remote info = { } if state == Sender Link . REJECTED : if pn disposition . condition : info [ "condition" ] = pn disposition . condition elif state == Sender Link . MODIFIED : info [ "delivery-failed" ] = pn disposition . failed info [ "undeliverable-here" ] = pn disposition . undeliverable annotations = pn disposition . annotations if annotations : info [ "message-annotations" ] = annotations send req = self . send requests . pop ( pn delivery . tag ) send req . destroy ( state , info ) pn delivery . settle ( ) elif pn delivery . writable : # we can now send on this delivery if self . pending sends : tag = self . pending sends . popleft ( ) send req = self . send requests [ tag ] self . write msg ( pn delivery , send req ) else : # tag no longer valid, expired or canceled send? LOG . debug ( "Delivery ignored, tag=%s" , str ( pn delivery . tag ) ) pn delivery . settle ( )
def reject ( self , pn condition = None ) : self . pn link . target . type = proton . Terminus . UNSPECIFIED super ( Receiver Link , self ) . reject ( pn condition )
def process delivery ( self , pn delivery ) : if pn delivery . readable and not pn delivery . partial : data = self . pn link . recv ( pn delivery . pending ) msg = proton . Message ( ) msg . decode ( data ) self . pn link . advance ( ) if self . handler : handle = "rmsg-%s:%x" % ( self . name , self . next handle ) self . next handle += 1 self . unsettled deliveries [ handle ] = pn delivery with self . callback lock : self . handler . message received ( self , msg , handle ) else : # TODO(kgiusti): is it ok to assume Delivery.REJECTED? pn delivery . settle ( )
def new sender ( self , name ) : pn link = self . pn session . sender ( name ) return self . request sender ( pn link )
def request sender ( self , pn link ) : sl = Sender Link ( self . connection , pn link ) self . links . add ( sl ) return sl
def new receiver ( self , name ) : pn link = self . pn session . receiver ( name ) return self . request receiver ( pn link )
def request receiver ( self , pn link ) : rl = Receiver Link ( self . connection , pn link ) self . links . add ( rl ) return rl
def link destroyed ( self , link ) : self . links . discard ( link ) if not self . links : # no more links LOG . debug ( "destroying unneeded session" ) self . pn session . close ( ) self . pn session . free ( ) self . pn session = None self . connection = None
def ep need close ( self ) : LOG . debug ( "Session %s close requested - closing..." , self . name ) links = self . links . copy ( ) # may modify  links for link in links : link . session closed ( )
def extend Markdown ( self , md , md globals ) : mark tag = Simple Tag Pattern ( MARK RE , 'mark' ) md . inline Patterns . add ( 'mark' , mark tag , ' begin' )
def receiver remote closed ( self , receiver link , pn condition ) : LOG . debug ( "receiver remote closed condition=%s" , pn condition ) receiver link . close ( ) self . done = True
def receiver failed ( self , receiver link , error ) : LOG . warn ( "receiver failed error=%s" , error ) receiver link . close ( ) self . done = True
def get host port ( server address ) : regex = re . compile ( r"^amqp://([a-z A-Z0-9.]+)(:([\d]+))?$" ) x = regex . match ( server address ) if not x : raise Exception ( "Bad address syntax: %s" % server address ) matches = x . groups ( ) host = matches [ 0 ] port = int ( matches [ 2 ] ) if matches [ 2 ] else None return host , port
def connect socket ( host , port , blocking = True ) : addr = socket . getaddrinfo ( host , port , socket . AF INET , socket . SOCK STREAM ) if not addr : raise Exception ( "Could not translate address '%s:%s'" % ( host , str ( port ) ) ) my socket = socket . socket ( addr [ 0 ] [ 0 ] , addr [ 0 ] [ 1 ] , addr [ 0 ] [ 2 ] ) if not blocking : my socket . setblocking ( 0 ) try : my socket . connect ( addr [ 0 ] [ 4 ] ) except socket . error as e : if e . errno != errno . EINPROGRESS : raise return my socket
def server socket ( host , port , backlog = 10 ) : addr = socket . getaddrinfo ( host , port , socket . AF INET , socket . SOCK STREAM ) if not addr : raise Exception ( "Could not translate address '%s:%s'" % ( host , str ( port ) ) ) my socket = socket . socket ( addr [ 0 ] [ 0 ] , addr [ 0 ] [ 1 ] , addr [ 0 ] [ 2 ] ) my socket . setblocking ( 0 ) # 0=non-blocking try : my socket . bind ( addr [ 0 ] [ 4 ] ) my socket . listen ( backlog ) except socket . error as e : if e . errno != errno . EINPROGRESS : raise return my socket
def process ( self , now ) : if self . pn connection is None : LOG . error ( "Connection.process() called on destroyed connection!" ) return 0 # do nothing until the connection has been opened if self . pn connection . state & proton . Endpoint . LOCAL UNINIT : return 0 if self . pn sasl and not self . sasl done : # wait until SASL has authenticated if ( PROTON VERSION < ( 0 , 10 ) ) : if self . pn sasl . state not in ( proton . SASL . STATE PASS , proton . SASL . STATE FAIL ) : LOG . debug ( "SASL in progress. State=%s" , str ( self . pn sasl . state ) ) if self . handler : with self . callback lock : self . handler . sasl step ( self , self . pn sasl ) return self . next deadline self . sasl done = True if self . handler : with self . callback lock : self . handler . sasl done ( self , self . pn sasl , self . pn sasl . outcome ) else : if self . pn sasl . outcome is not None : self . sasl done = True if self . handler : with self . callback lock : self . handler . sasl done ( self , self . pn sasl , self . pn sasl . outcome ) # process timer events: timer deadline = self . expire timers ( now ) transport deadline = self . pn transport . tick ( now ) if timer deadline and transport deadline : self . next deadline = min ( timer deadline , transport deadline ) else : self . next deadline = timer deadline or transport deadline # process events from proton: pn event = self . pn collector . peek ( ) while pn event : # LOG.debug("pn event: %s received", pn event.type) if Link . handle proton event ( pn event , self ) : pass elif self . handle proton event ( pn event ) : pass elif Session Proxy . handle proton event ( pn event , self ) : pass self . pn collector . pop ( ) pn event = self . pn collector . peek ( ) # check for connection failure after processing all pending # engine events: if self . error : if self . handler : # nag application until connection is destroyed self . next deadline = now with self . callback lock : self . handler . connection failed ( self , self . error ) elif ( self . endpoint state == self . CLOSED and self . read done and self . write done ) : # invoke closed callback after endpoint has fully closed and # all pending I/O has completed: if self . handler : with self . callback lock : self . handler . connection closed ( self ) return self . next deadline
def output data ( self ) : c = self . has output if c <= 0 : return None try : buf = self . pn transport . peek ( c ) except Exception as e : self . connection failed ( str ( e ) ) return None return buf
def create sender ( self , source address , target address = None , event handler = None , name = None , properties = None ) : ident = name or str ( source address ) if ident in self . sender links : raise Key Error ( "Sender %s already exists!" % ident ) session = Session Proxy ( "session-%s" % ident , self ) session . open ( ) sl = session . new sender ( ident ) sl . configure ( target address , source address , event handler , properties ) self . sender links [ ident ] = sl return sl
def reject sender ( self , link handle , pn condition = None ) : link = self . sender links . get ( link handle ) if not link : raise Exception ( "Invalid link handle: %s" % link handle ) link . reject ( pn condition ) # note: normally, link.destroy() cannot be called from a callback, # but this link was never made available to the application so this # link is only referenced by the connection link . destroy ( )
def create receiver ( self , target address , source address = None , event handler = None , name = None , properties = None ) : ident = name or str ( target address ) if ident in self . receiver links : raise Key Error ( "Receiver %s already exists!" % ident ) session = Session Proxy ( "session-%s" % ident , self ) session . open ( ) rl = session . new receiver ( ident ) rl . configure ( target address , source address , event handler , properties ) self . receiver links [ ident ] = rl return rl
def connection failed ( self , error = "Error not specified!" ) : if not self . error : LOG . error ( "Connection failed: %s" , str ( error ) ) self . error = error
def ep active ( self ) : LOG . debug ( "Connection is up" ) if self . handler : with self . callback lock : self . handler . connection active ( self )
def ep need close ( self ) : LOG . debug ( "Connection remotely closed" ) if self . handler : cond = self . pn connection . remote condition with self . callback lock : self . handler . connection remote closed ( self , cond )
def ep error ( self , error ) : super ( Connection , self ) . ep error ( error ) self . connection failed ( "Protocol error occurred." )
def get color string ( self ) : s = '' if self . color type == 'd' : if self . name is "black" : s = '%.3f G' % 0 else : s = '%.3f %.3f %.3f RG' % ( self . red / 255.0 , self . green / 255.0 , self . blue / 255.0 ) elif self . color type == 'f' or self . color type == 't' : if self . name is "black" : s = '%.3f g' % 0 else : s = '%.3f %.3f %.3f rg' % ( self . red / 255.0 , self . green / 255.0 , self . blue / 255.0 ) return s
def set font ( self , family = None , style = None , size = None ) : if style is not None : if 'B' in style : family += ' bold' if 'I' in style : family += ' italic' self . set family ( family ) self . get diffs ( ) self . set style ( style ) self . set metrics ( ) self . set size ( size ) self . set font key ( )
def string width ( self , s ) : s = str ( s ) w = 0 for char in s : char = ord ( char ) w += self . character widths [ char ] return w * self . font size / 1000.0
def get ttf ( self ) : font dict = { } families = [ ] rootdirlist = string . split ( self . search path , os . pathsep ) #for rootdir in rootdirlist: #    rootdir = os.path.expanduser(rootdir) for dir Name , subdir List , filelist in itertools . chain . from iterable ( os . walk ( path ) for path in rootdirlist ) : for item in filelist : root , ext = os . path . splitext ( item ) if ext == '.ttf' : if root [ 0 ] . lower ( ) in english : source = os . path . join ( dir Name , item ) name = root . lower ( ) . replace ( ' ' , ' ' ) if ' bold' in name : name = name . replace ( ' bold' , ' bold' ) if ' italic' in name : name = name . replace ( ' italic' , ' italic' ) elif 'bold' in name : name = name . replace ( 'bold' , ' bold' ) if 'italic' in name : name = name . replace ( 'italic' , ' italic' ) elif ' italic' in name : name = name . replace ( ' italic' , ' italic' ) elif 'italic' in name : name = name . replace ( 'italic' , ' italic' ) elif 'oblique' in name : name = name . replace ( 'oblique' , ' italic' ) else : families . append ( name ) font dict [ name ] = source else : source = os . path . join ( dir Name , item ) name = root . lower ( ) . replace ( ' ' , ' ' ) font dict [ name ] = source families . append ( name ) self . font dict = font dict self . families = families
def put stream ( self , stream ) : self . out ( 'stream' ) self . out ( stream ) self . out ( 'endstream' )
def set font size ( self , size ) : if self . font . font size == size : pass else : self . font . set size ( size )
def add pie chart ( self , data , cursor , width , height , title = None , data type = "raw" , fill colors = None , labels = False , background = None , legend = None ) : save draw color = self . draw color save fill color = self . fill color chart = PDF Pie Chart ( self . session , self . page , data , cursor , width , height , title , data type , fill colors , labels , background , legend ) self . set draw color ( save draw color ) self . set fill color ( save fill color )
def output ( self ) : self . session . out ( '<</Type /X Object' ) self . session . out ( '/Subtype /Image' ) self . session . out ( '/Width %s' % self . width ) self . session . out ( '/Height %s' % self . height ) if self . colorspace is 'Indexed' : self . session . out ( '/Color Space [/Indexed /Device RGB %s %s 0 R' % ( self . pal , self . number + 1 ) ) else : self . session . out ( '/Color Space /%s' % self . colorspace ) if self . colorspace is 'Device CMYK' : self . session . out ( '/Decode [1 0 1 0 1 0 1 0]' ) self . session . out ( '/Bits Per Component %s' % self . bits per component ) if self . filter : self . session . out ( '/Filter /%s' % self . filter ) if self . decode : self . session . out ( '/Decode Parms << %s >>' % self . decode ) if self . transparent : self . session . out ( '/Mask [%s]' % self . transparent string ) if self . soft mask : self . session . out ( '/S Mask %s 0 R' % ( self . number + 1 ) ) self . session . out ( '/Length %s >>' % self . size ) self . session . put stream ( self . image data ) self . session . out ( 'endobj' ) if self . colorspace is 'Indexed' : self . session . out ( '<<%s /Length %s >>' % ( self . palette filter , self . palette length ) ) self . session . put stream ( self . palette ) self . session . out ( 'endobj' ) if isinstance ( self . soft mask , PDF Image ) : obj = self . session . add object ( ) self . soft mask . set number ( obj . id ) self . soft mask . output ( )
def absolute position ( self , x , y ) : ( a , b , c , d , e , f ) = self . current Matrix xp = a * x + c * y + e yp = b * x + d * y + f return xp , yp
def set font ( self , family = None , style = None , size = None ) : self . set family ( family ) self . set style ( style ) self . set size ( size ) self . set font key ( ) self . set name ( ) self . set character widths ( )
def string width ( self , s ) : s = str ( s ) w = 0 for i in s : w += self . character widths [ i ] return w * self . font size / 1000.0
def set display mode ( self , zoom = 'fullpage' , layout = 'continuous' ) : self . zoom options = [ "fullpage" , "fullwidth" , "real" , "default" ] self . layout options = [ "single" , "continuous" , "two" , "default" ] if zoom in self . zoom options or ( isinstance ( zoom , int ) and 0 < zoom <= 100 ) : self . zoom mode = zoom else : raise Exception ( 'Incorrect zoom display mode: ' + zoom ) if layout in self . layout options : self . layout mode = layout else : raise Exception ( 'Incorrect layout display mode: ' + layout )
def close ( self ) : self . document . set page numbers ( ) self . put header ( ) self . put pages ( ) self . put resources ( ) self . put information ( ) self . put catalog ( ) self . put trailer ( ) if hasattr ( self . destination , "write" ) : output = self . output to io ( ) elif self . destination == 'string' : output = self . output to string ( ) else : self . output to file ( ) output = None return output
def put header ( self ) : self . session . out ( '%%PDF-%s' % self . pdf version ) if self . session . compression : self . session . buffer += '%' + chr ( 235 ) + chr ( 236 ) + chr ( 237 ) + chr ( 238 ) + "\n"
def put resource dict ( self ) : self . session . add object ( 2 ) self . session . out ( '<<' ) self . session . out ( '/Proc Set [/PDF /Text /Image B /Image C /Image I]' ) self . session . out ( '/Font <<' ) for font in self . document . fonts : self . session . out ( '/F%s %s 0 R' % ( font . index , font . number ) ) self . session . out ( '>>' ) if self . document . images : self . session . out ( '/X Object <<' ) for image in self . document . images : self . session . out ( '/I%s %s 0 R' % ( image . index , image . number ) ) self . session . out ( '>>' ) self . session . out ( '>>' ) self . session . out ( 'endobj' )
def put information ( self ) : self . session . add object ( ) self . session . out ( '<<' ) self . session . out ( '/Producer ' + self . text to string ( ) ) if self . title : self . session . out ( '/Title ' + self . text to string ( self . title ) ) if self . subject : self . session . out ( '/Subject ' + self . text to string ( self . subject ) ) if self . author : self . session . out ( '/Author ' + self . text to string ( self . author ) ) if self . keywords : self . session . out ( '/Keywords ' + self . text to string ( self . keywords ) ) if self . creator : self . session . out ( '/Creator ' + self . text to string ( self . creator ) ) self . session . out ( '/Creation Date ' + self . text to string ( 'D:' + datetime . now ( ) . strftime ( '%Y%m%d%H%M%S' ) ) ) self . session . out ( '>>' ) self . session . out ( 'endobj' )
def x fit ( self , test length ) : if ( self . x + test length ) >= self . xmax : return False else : return True
def y fit ( self , test length ) : if ( self . y + test length ) >= self . ymax : return False else : return True
def x is greater than ( self , test ordinate ) : self . is coordinate ( test ordinate ) if self . x > test ordinate . x : return True else : return False
def y is greater than ( self , test ordinate ) : self . is coordinate ( test ordinate ) if self . y > test ordinate . y : return True else : return False
def copy ( self ) : new cursor = self . class ( self . x , self . y ) new cursor . set bounds ( self . xmin , self . ymin , self . xmax , self . ymax , self . ymaxmax ) new cursor . set deltas ( self . dx , self . dy ) return new cursor
def x plus ( self , dx = None ) : if dx is None : self . x += self . dx else : self . x = self . x + dx
def y plus ( self , dy = None ) : if dy is None : self . y += self . dy else : self . y = self . y + dy
def draw ( self ) : self . compile ( ) self . rows [ 0 ] . advance first row ( ) self . set borders ( ) self . draw fill ( ) self . draw borders ( ) self . draw text ( ) self . set final cursor ( )
def setup ( app ) : app . setup extension ( 'sphinx.ext.todo' ) app . setup extension ( 'sphinx.ext.mathjax' ) app . setup extension ( "sphinx.ext.intersphinx" ) app . config . intersphinx mapping . update ( { 'https://docs.python.org/' : None } ) app . config . intersphinx mapping . update ( { sage doc url + doc + "/" : None for doc in sage documents } ) app . config . intersphinx mapping . update ( { sage doc url + "reference/" + module : None for module in sage modules } ) app . setup extension ( "sphinx.ext.extlinks" ) app . config . extlinks . update ( { 'python' : ( 'https://docs.python.org/release/' + pythonversion + '/%s' , '' ) , # Sage trac ticket shortcuts. For example, :trac:`7549` . 'trac' : ( 'https://trac.sagemath.org/%s' , 'trac ticket #' ) , 'wikipedia' : ( 'https://en.wikipedia.org/wiki/%s' , 'Wikipedia article ' ) , 'arxiv' : ( 'http://arxiv.org/abs/%s' , 'Arxiv ' ) , 'oeis' : ( 'https://oeis.org/%s' , 'OEIS sequence ' ) , 'doi' : ( 'https://dx.doi.org/%s' , 'doi:' ) , 'pari' : ( 'http://pari.math.u-bordeaux.fr/dochtml/help/%s' , 'pari:' ) , 'mathscinet' : ( 'http://www.ams.org/mathscinet-getitem?mr=%s' , 'Math Sci Net ' ) } ) app . config . html theme = 'sage'
def duration ( self ) : ecc = self . ecc if not np . isnan ( self . ecc ) else np . sqrt ( self . ecw ** 2 + self . esw ** 2 ) esw = self . esw if not np . isnan ( self . esw ) else ecc * np . sin ( self . w ) a Rs = ( ( G * self . rhos * ( 1. + self . Mp Ms ) * ( self . per * DAYSEC ) ** 2. ) / ( 3. * np . pi ) ) ** ( 1. / 3. ) inc = np . arccos ( self . bcirc / a Rs ) becc = self . bcirc * ( 1 - ecc ** 2 ) / ( 1 - esw ) tdur = self . per / 2. / np . pi * np . arcsin ( ( ( 1. + self . Rp Rs ) ** 2 - becc ** 2 ) ** 0.5 / ( np . sin ( inc ) * a Rs ) ) tdur *= np . sqrt ( 1. - ecc ** 2. ) / ( 1. - esw ) return tdur
def update ( self , * * kwargs ) : if kwargs . get ( 'verify kwargs' , True ) : valid = [ y [ 0 ] for x in [ TRANSIT , LIMBDARK , SETTINGS ] for y in x . fields ] # List of valid kwargs valid += [ 'b' , 'times' ] # These are special! for k in kwargs . keys ( ) : if k not in valid : raise Exception ( "Invalid kwarg '%s'." % k ) if ( 'q1' in kwargs . keys ( ) ) and ( 'q2' in kwargs . keys ( ) ) : kwargs . update ( { 'ldmodel' : KIPPING } ) elif ( 'c1' in kwargs . keys ( ) ) and ( 'c2' in kwargs . keys ( ) ) and ( 'c3' in kwargs . keys ( ) ) and ( 'c4' in kwargs . keys ( ) ) : kwargs . update ( { 'ldmodel' : NONLINEAR } ) self . limbdark . update ( * * kwargs ) self . transit . update ( * * kwargs ) self . settings . update ( * * kwargs )
def Compute ( self ) : err = Compute ( self . transit , self . limbdark , self . settings , self . arrays ) if err != ERR NONE : Raise Error ( err )
def Bin ( self ) : err = Bin ( self . transit , self . limbdark , self . settings , self . arrays ) if err != ERR NONE : Raise Error ( err )
def Free ( self ) : if self . arrays . calloc : dbl free ( self . arrays . time ) dbl free ( self . arrays . flux ) dbl free ( self . arrays . bflx ) dbl free ( self . arrays . M ) dbl free ( self . arrays . E ) dbl free ( self . arrays . f ) dbl free ( self . arrays . r ) dbl free ( self . arrays . x ) dbl free ( self . arrays . y ) dbl free ( self . arrays . z ) self . arrays . calloc = 0 if self . arrays . balloc : dbl free ( self . arrays . b ) self . arrays . balloc = 0 if self . arrays . ialloc : dbl free ( self . arrays . iarr ) self . arrays . ialloc = 0
def list extensions gen ( self ) : code , message = self . command ( "LIST EXTENSIONS" ) if code != 202 : raise NNTP Reply Error ( code , message ) for line in self . info gen ( code , message ) : yield line . strip ( )
def xpat gen ( self , header , msgid range , * pattern ) : args = " " . join ( [ header , utils . unparse msgid range ( msgid range ) ] + list ( pattern ) ) code , message = self . command ( "XPAT" , args ) if code != 221 : raise NNTP Reply Error ( code , message ) for line in self . info gen ( code , message ) : yield line . strip ( )
def xfeature compress gzip ( self , terminator = False ) : args = "TERMINATOR" if terminator else None code , message = self . command ( "XFEATURE COMPRESS GZIP" , args ) if code != 290 : raise NNTP Reply Error ( code , message ) return True
def api post ( self , url , * * kwargs ) : response = self . session . post ( url = url , headers = self . get api headers ( ) , * * kwargs ) if not response . ok : raise Server Exception ( '{0}: {1}' . format ( response . status code , response . text or response . reason ) ) return response . json ( )
def api delete ( self , url , * * kwargs ) : response = self . session . delete ( url = url , headers = self . get api headers ( ) , * * kwargs ) if not response . ok : raise Server Exception ( '{0}: {1}' . format ( response . status code , response . text or response . reason ) ) return response
def api get ( self , url , * * kwargs ) : response = self . session . get ( url = url , headers = self . get api headers ( ) , * * kwargs ) if not response . ok : raise Server Exception ( '{0}: {1}' . format ( response . status code , response . text or response . reason ) ) return response . json ( )
def create scheduled query ( self , query , change , scope unit , scope count ) : query data = { 'scheduled query' : { 'name' : 'For Anomaly Report' , 'query' : query , 'threshold type' : '%' , 'threshold value' : change , 'time period' : scope unit . title ( ) , 'time value' : scope count , } } query url = 'https://logentries.com/rest/{account id}/api/scheduled queries' return self . api post ( url = query url . format ( account id = self . account id ) , data = json . dumps ( query data , sort keys = True ) )
def do POST ( self ) : self . send response ( urllib2 . httplib . OK ) self . end headers ( ) content length = int ( self . headers [ 'Content-Length' ] ) body = self . rfile . read ( content length ) print ( "Client: {0}" . format ( str ( self . client address ) ) ) print ( "headers: {0}" . format ( self . headers ) ) print ( "path: {0}" . format ( self . path ) ) print ( "body: {0}" . format ( body ) )
def defaults docstring ( defaults , header = None , indent = None , footer = None ) : if indent is None : indent = '' if header is None : header = '' if footer is None : footer = '' width = 60 #hbar = indent + width * '=' + '\n'  # horizontal bar hbar = '\n' s = hbar + ( header ) + hbar for key , value , desc in defaults : if isinstance ( value , basestring ) : value = "'" + value + "'" if hasattr ( value , ' call ' ) : value = "<" + value . name + ">" s += indent + '%-12s\n' % ( "%s :" % key ) s += indent + indent + ( indent + 23 * ' ' ) . join ( desc . split ( '\n' ) ) s += ' [%s]\n\n' % str ( value ) s += hbar s += footer return s
def defaults decorator ( defaults ) : def decorator ( func ) : kwargs = dict ( header = 'Keyword arguments\n-----------------\n' , indent = '  ' , footer = '\n' ) doc = defaults docstring ( defaults , * * kwargs ) if func . doc is None : func . doc = '' func . doc += doc return func return decorator
def load ( self , * * kwargs ) : defaults = dict ( [ ( d [ 0 ] , d [ 1 ] ) for d in self . defaults ] ) # Require kwargs are in defaults for k in kwargs : if k not in defaults : msg = "Unrecognized attribute of %s: %s" % ( self . class . name , k ) raise Attribute Error ( msg ) defaults . update ( kwargs ) # This doesn't overwrite the properties self . dict . update ( defaults ) # This should now be set self . check type ( self . dict [ 'default' ] ) # This sets the underlying property values (i.e.,  value ) self . set ( * * defaults )
def defaults docstring ( cls , header = None , indent = None , footer = None ) : return defaults docstring ( cls . defaults , header = header , indent = indent , footer = footer )
def set errors ( self , errors ) : if errors is None : self . errors = None return self . errors = [ asscalar ( e ) for e in errors ]
def load and parse ( self ) : f = open ( self . file path , "r" ) metrics json = f . read ( ) self . metrics = json . loads ( metrics json )
def extract dictionary ( self , metrics ) : new metrics = { } for m in metrics : metric = self . extract fields ( m ) new metrics [ m [ 'name' ] ] = metric return new metrics
def filter ( self ) : if self . filter expression is not None : new metrics = [ ] metrics = self . metrics [ 'result' ] for m in metrics : if self . filter expression . search ( m [ 'name' ] ) : new metrics . append ( m ) else : new metrics = self . metrics [ 'result' ] self . metrics = self . extract dictionary ( new metrics )
def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . host Group Id is not None : self . host Group Id = self . args . host Group Id self . path = "v1/hostgroup/{0}" . format ( str ( self . host Group Id ) )
def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . tenant id is not None : self . tenant id = self . args . tenant id if self . args . fingerprint fields is not None : self . fingerprint fields = self . args . fingerprint fields if self . args . title is not None : self . title = self . args . title if self . args . source is not None : self . source = self . args . source if self . args . severity is not None : self . severity = self . args . severity if self . args . message is not None : self . message = self . args . message event = { } if self . title is not None : event [ 'title' ] = self . title if self . severity is not None : event [ 'severity' ] = self . severity if self . message is not None : event [ 'message' ] = self . message if self . source is not None : if 'source' not in event : event [ 'source' ] = { } if len ( self . source ) >= 1 : event [ 'source' ] [ 'ref' ] = self . source [ 0 ] if len ( self . source ) >= 2 : event [ 'source' ] [ 'type' ] = self . source [ 1 ] self . process properties ( self . args . properties ) if self . properties is not None : event [ 'properties' ] = self . properties if self . fingerprint fields is not None : event [ 'fingerprint Fields' ] = self . fingerprint fields self . data = json . dumps ( event , sort keys = True ) self . headers = { 'Content-Type' : 'application/json' }
def call api ( self ) : # Allocate a socket and connect to the meter sockobj = socket ( AF INET , SOCK STREAM ) sockobj . connect ( ( self . rpc host , self . rpc port ) ) self . get json ( ) message = [ self . rpc message . encode ( 'utf-8' ) ] for line in message : sockobj . send ( line ) data = sockobj . recv ( self . MAX LINE ) print ( data ) self . rpc data . append ( data ) sockobj . close ( )
def get arguments ( self ) : Hostgroup Modify . get arguments ( self ) if self . args . host group id is not None : self . host group id = self . args . host group id self . path = "v1/hostgroup/" + str ( self . host group id )
def identifier ( self , text ) : self . attempting ( text ) return concatenation ( [ alternation ( [ self . alpha character , " " ] ) , zero or more ( alternation ( [ self . alpha character , " " , self . digit ] ) ) ] , ignore whitespace = False ) ( text ) . compressed ( Token Type . identifier )
def operator ( self , text ) : self . attempting ( text ) return alternation ( [ "|" , "." , "," , "-" ] ) ( text ) . retyped ( Token Type . operator )
def op mult ( self , text ) : self . attempting ( text ) return terminal ( "*" ) ( text ) . retyped ( Token Type . op mult )
def op add ( self , text ) : self . attempting ( text ) return terminal ( "+" ) ( text ) . retyped ( Token Type . op add )
def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . plugin Name is not None : self . plugin Name = self . args . plugin Name self . path = "v1/plugins/{0}/components" . format ( self . plugin Name )
def get environment ( self ) : if 'TSP EMAIL' in os . environ : self . email = os . environ [ 'TSP EMAIL' ] if 'TSP API TOKEN' in os . environ : self . api token = os . environ [ 'TSP API TOKEN' ] if 'TSP API HOST' in os . environ : self . api host = os . environ [ 'TSP API HOST' ] else : self . api host = 'api.truesight.bmc.com'
def call api ( self ) : self . url = self . form url ( ) if self . headers is not None : logging . debug ( self . headers ) if self . data is not None : logging . debug ( self . data ) if len ( self . get url parameters ( ) ) > 0 : logging . debug ( self . get url parameters ( ) ) result = self . methods [ self . method ] ( ) if not self . good response ( result . status code ) : logging . error ( self . url ) logging . error ( self . method ) if self . data is not None : logging . error ( self . data ) logging . error ( result ) self . api result = result
def get arguments ( self ) : # Api Cli.get arguments(self) if self . args . file name is not None : self . file name = self . args . file name
def execute ( self ) : # self. get environment() self . add arguments ( ) self . parse args ( ) self . get arguments ( ) if self . validate arguments ( ) : self . plot data ( ) else : print ( self . message )
def get remote file size ( self , url ) : try : req = urllib . request . urlopen ( url ) return int ( req . getheader ( 'Content-Length' ) . strip ( ) ) except urllib . error . HTTP Error as error : logger . error ( 'Error retrieving size of the remote file %s' % error ) print ( 'Error retrieving size of the remote file %s' % error ) self . connect earthexplorer ( ) self . get remote file size ( url )
def download ( self , bands = None , download dir = None , metadata = False ) : if not download dir : download dir = DOWNLOAD DIR if bands is None : bands = list ( range ( 1 , 12 ) ) + [ 'BQA' ] else : self . validate bands ( bands ) pattern = re . compile ( '^[^\s]+ (.+)\.tiff?' , re . I ) band list = [ 'B%i' % ( i , ) if isinstance ( i , int ) else i for i in bands ] image list = [ ] # Connect Earth explore self . connect earthexplorer ( ) # tgz name tgzname = self . scene Info . name + '.tgz' dest dir = check create folder ( join ( download dir , self . scene Info . name ) ) # Download File downloaded = self . download file ( self . url , dest dir , tgzname ) # Log logger . debug ( 'Status downloaded %s' % downloaded ) print ( '\n Status downloaded %s' % downloaded ) if downloaded [ 'sucess' ] : # Log print ( '\n Downloaded sucess' ) logger . debug ( 'Downloaded sucess of scene: %s' % self . scene Info . name ) try : tar = tarfile . open ( downloaded [ 'file path' ] , 'r' ) folder path = join ( download dir , self . scene Info . name ) tar . extractall ( folder path ) remove ( downloaded [ 'file path' ] ) images path = listdir ( folder path ) for image path in images path : matched = pattern . match ( image path ) file path = join ( folder path , image path ) if matched and matched . group ( 1 ) in band list : image list . append ( [ file path , getsize ( file path ) ] ) elif matched : remove ( file path ) except tarfile . Read Error as error : print ( '\n Error when extracting files. %s' % error ) logger . error ( 'Error when extracting files. %s' % error ) return image list else : logger . debug ( 'Info downloaded: %s' % downloaded ) print ( '\n Info downloaded: %s' % downloaded ) return downloaded
def validate bands ( bands ) : if not isinstance ( bands , list ) : raise Type Error ( 'Parameter bands must be a "list"' ) valid bands = list ( range ( 1 , 12 ) ) + [ 'BQA' ] for band in bands : if band not in valid bands : raise Invalid Band Error ( '%s is not a valid band' % band )
def connect earthexplorer ( self ) : logger . info ( "Establishing connection to Earthexplorer" ) print ( "\n Establishing connection to Earthexplorer" ) try : opener = urllib . request . build opener ( urllib . request . HTTP Cookie Processor ( ) ) urllib . request . install opener ( opener ) params = urllib . parse . urlencode ( dict ( username = self . user , password = self . password ) ) params = params . encode ( 'utf-8' ) f = opener . open ( "https://ers.cr.usgs.gov/login" , params ) data = f . read ( ) . decode ( 'utf-8' ) f . close ( ) if data . find ( 'You must sign in as a registered user to download data or place orders for USGS EROS products' ) > 0 : print ( "\n Authentification failed" ) logger . error ( "Authentification failed" ) raise Autentication USGS Failed ( 'Authentification USGS failed' ) print ( 'User %s connected with USGS' % self . user ) logger . debug ( 'User %s connected with USGS' % self . user ) return except Exception as e : print ( '\n Error when trying to connect USGS: %s' % e ) raise logger . error ( 'Error when trying to connect USGS: %s' % e )
def download file ( self , url , download dir , scene Name ) : try : # Log logger . info ( '\n Starting download..' ) print ( '\n Starting download..\n' ) # Request req = urllib . request . urlopen ( url ) try : if req . info ( ) . get content type ( ) == 'text/html' : logger . error ( "error : the file format is html" ) lines = req . read ( ) if lines . find ( 'Download Not Found' ) > 0 : raise Type Error ( 'Download USGS not found for scene: %s' % self . scene Info . name ) else : print ( lines ) print ( sys . exit ( - 1 ) ) except Exception as e : logger . error ( 'Erro in USGS download for scene %s error: %s' % ( self . scene Info . name , e ) ) raise Credentials Usgs Error ( 'User or Password invalid ! ' ) total size = int ( req . getheader ( 'Content-Length' ) . strip ( ) ) if total size < 50000 : logger . error ( "Error: The file is too small to be a Landsat Image for scene %s" % self . scene Info . name ) raise Small Landsat Image Error ( "Error: The file is too small to be a Landsat Image" ) total size fmt = sizeof fmt ( total size ) downloaded = 0 CHUNK = 1024 * 1024 * 8 with open ( download dir + '/' + scene Name , 'wb' ) as fp : start = time . clock ( ) logger . debug ( 'Downloading {0} ({1}):' . format ( self . scene Info . name , total size fmt ) ) print ( 'Downloading {0} ({1}):' . format ( self . scene Info . name , total size fmt ) ) while True : chunk = req . read ( CHUNK ) downloaded += len ( chunk ) done = int ( 50 * downloaded / total size ) print ( '\r[{1}{2}]{0:3.0f}% {3}ps' . format ( floor ( ( float ( downloaded ) / total size ) * 100 ) , '-' * done , ' ' * ( 50 - done ) , sizeof fmt ( ( downloaded // ( time . clock ( ) - start ) ) / 8 ) ) ) if not chunk : logger . debug ( 'Download {0} completed({1}):' . format ( self . scene Info . name , total size fmt ) ) break fp . write ( chunk ) except urllib . error . HTTP Error as e : if e . code == 500 : logger . error ( "File doesn't exist" ) print ( "\n File doesn't exist: %s " % e ) raise Remote File Doesnt Exist ( "File doesn't exist" ) elif e . code == 403 : # Log celery logger . error ( "HTTP Error:" , e . code , url ) logger . debug ( '\n trying to download it again scene: %s' % self . scene Info . name ) # Log shell print ( "\n HTTP Error:" , e . code , url ) print ( '\n trying to download it again scene: %s' % self . scene Info . name ) self . connect earthexplorer ( ) self . download file ( url , download dir , scene Name ) else : logger . error ( "HTTP Error:" , e ) print ( "HTTP Error:" , e . code , url ) raise e except urllib . error . URL Error as e : print ( "URL Error:" , e . reason , url ) logger . error ( "URL Error: %s in %s" % ( e , url ) ) raise e except Connection Reset Error as e : print ( 'Error Connection Reset Error: %s' % e ) logger . error ( 'Error Connection Reset Error: %s' % e ) print ( '\n trying to download it again scene: %s' % self . scene Info . name ) logger . debug ( 'trying to download it again scene: %s' % self . scene Info . name ) self . download file ( url , download dir , scene Name ) except urllib . error . HTTP Error as e : print ( '\n Http Error: %s' % e ) print ( '\n trying to download it again scene: %s' % self . scene Info . name ) logger . error ( 'Http Error: %s' % e ) logger . debug ( 'trying to download it again scene: %s' % self . scene Info . name ) self . download file ( url , download dir , scene Name ) except Exception as error : logger . error ( 'Error unknown %s in download %s at scene: %s' % ( error , url , self . scene Info . name ) ) print ( 'Error unknown %s in download % at scene: %s' % ( error , url , self . scene Info . name ) ) logger . debug ( 'trying to download it again scene: %s' % self . scene Info . name ) self . download file ( url , download dir , scene Name ) percent = floor ( ( float ( downloaded ) / total size ) * 100 ) or 0 if percent != 100 : logger . debug ( 'trying to download it again scene: %s' % self . scene Info . name ) logger . error ( 'Download interrupted in %s%%, trying to download it again scene: %s' % ( percent , self . scene Info . name ) ) print ( '\n Download interrupted in %s%%, trying to download it again scene: %s' % ( percent , self . scene Info . name ) ) self . download file ( url , download dir , scene Name ) path item = download dir + '/' + scene Name info = { 'total size' : total size fmt , 'scene' : self . scene Info . name , 'sucess' : verify sucess ( total size , path item ) , 'file path' : path item } return info
def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . metric name is not None : self . metric name = self . args . metric name self . path = "v1/metrics/{0}" . format ( self . metric name )
def normalize ( self , dt , is dst = False ) : if dt . tzinfo is None : raise Value Error ( 'Naive time - no tzinfo set' ) return dt . replace ( tzinfo = self )
def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . host Group Id is not None : self . host Group Id = self . args . host Group Id if self . args . force is not None : self . force = self . args . force if self . force : self . url parameters = { "force Remove" : True } self . path = "v1/hostgroup/{0}" . format ( str ( self . host Group Id ) )
def get arguments ( self ) : Api Cli . get arguments ( self ) self . actions = self . args . actions if self . args . actions is not None else None self . alarm name = self . args . alarm name if self . args . alarm name is not None else None self . metric = self . args . metric if self . args . metric is not None else None self . aggregate = self . args . aggregate if self . args . aggregate is not None else None self . operation = self . args . operation if self . args . operation is not None else None self . threshold = self . args . threshold if self . args . threshold is not None else None self . trigger interval = self . args . trigger interval if self . args . trigger interval is not None else None self . host group id = self . args . host group id if self . args . host group id is not None else None self . note = self . args . note if self . args . note is not None else None self . per host notify = self . args . per host notify if self . args . per host notify is not None else None self . is disabled = self . args . is disabled if self . args . is disabled is not None else None self . notify clear = self . args . notify clear if self . args . notify clear is not None else None self . notify set = self . args . notify set if self . args . notify set is not None else None self . timeout interval = self . args . timeout interval if self . args . timeout interval is not None else None
def dump text ( self ) : results = self . relay output [ 'result' ] for l in results : dt = time . strftime ( "%Y-%m-%d T%H:%M:%SZ" , time . gmtime ( int ( l [ 1 ] [ 'ts' ] ) ) ) print ( "{0} {1} {2} {3}" . format ( l [ 0 ] , dt , l [ 1 ] [ 'type' ] , l [ 1 ] [ 'msg' ] ) )
def handle results ( self ) : # Only process if we get HTTP result of 200 if self . api result . status code == requests . codes . ok : self . relay output = json . loads ( self . api result . text ) if self . raw : self . dump json ( ) else : self . dump text ( )
def get arguments ( self ) : Plugin Base . get arguments ( self ) if self . args . organization Name is not None : self . organization Name = self . args . organization Name if self . args . repository Name is not None : self . repository Name = self . args . repository Name self . path = "v1/plugins/private/{0}/{1}/{2}" . format ( self . plugin Name , self . organization Name , self . repository Name )
def get arguments ( self ) : Alarm Modify . get arguments ( self ) self . alarm id = self . args . alarm id if self . args . alarm id is not None else None self . get api parameters ( )
def filter ( self ) : if self . metrics or self . control or self . plugins : relays = self . relays [ 'result' ] [ 'relays' ] for relay in relays : if self . metrics : del relays [ relay ] [ 'metrics' ] if self . control : del relays [ relay ] [ 'control' ] if self . plugins : if 'plugins' in relays [ relay ] : del relays [ relay ] [ 'plugins' ]
def handle results ( self ) : # Only process if we get HTTP result of 200 if self . api result . status code == requests . codes . ok : self . relays = json . loads ( self . api result . text ) self . filter ( ) self . dump json ( )
def fromlist ( cls , files , equal = False , offensive = False , lang = None ) : self = cls . new ( cls ) self . files = fortunes = [ ] count = 0 for file in files : fortune = load fortune ( file , offensive = offensive , lang = lang ) if fortune is None : logger . warn ( "Can't load: %s" , file ) continue count += 1 if equal else fortune . size fortunes . append ( ( fortune , count ) ) if not fortunes : raise Value Error ( 'All fortune files specified are invalid' ) self . count = count self . keys = [ i [ 1 ] for i in self . files ] return self
def set chance ( cls , files , equal = False , offensive = False , lang = None ) : # where files are (name, chance) self = cls . new ( cls ) total = 0. file = [ ] leftover = [ ] for name , chance in files : if total >= 1 : break fortune = load fortune ( name , offensive = offensive , lang = lang ) if fortune is None or not fortune . size : continue if chance : file . append ( ( fortune , chance ) ) total += chance else : leftover . append ( fortune ) if leftover and total < 1 : left = 1 - total if equal : perfile = left / len ( leftover ) for fortune in leftover : file . append ( ( fortune , perfile ) ) else : entries = sum ( map ( attrgetter ( 'size' ) , leftover ) ) logger . debug ( '%d entries left' , entries ) for fortune in leftover : chance = left * fortune . size / entries file . append ( ( fortune , chance ) ) # Arbitrary limit to calculate upper bound with, nice round number self . count = count = 65536 bound = 0 self . files = fortunes = [ ] for file , chance in file : bound += int ( chance * count ) fortunes . append ( ( file , bound ) ) self . keys = [ i [ 1 ] for i in self . files ] return self
def grammar ( self , text ) : self . attempting ( text ) return concatenation ( [ zero or more ( self . comment , ignore whitespace = True ) , self . rule , zero or more ( alternation ( [ self . comment , self . rule , ] ) , ignore whitespace = True ) , ] , ignore whitespace = True ) ( text ) . retyped ( Token Type . grammar )
def rule ( self , text ) : self . attempting ( text ) return concatenation ( [ self . identifier , "=" , self . expression , ";" , ] , ignore whitespace = True ) ( text ) . retyped ( Token Type . rule )
def special handling ( self , text ) : self . attempting ( text ) return concatenation ( [ "?" , self . identifier , "?" , ] , ignore whitespace = True ) ( text ) . retyped ( Token Type . special handling )
def number ( self , text ) : self . attempting ( text ) return concatenation ( [ exclusion ( self . digit , "0" ) , zero or more ( self . digit , ignore whitespace = False ) , ] , ignore whitespace = False ) ( text ) . compressed ( Token Type . number )
def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . metric Name is not None : self . metric Name = self . args . metric Name if self . args . measurement is not None : self . measurement = self . args . measurement if self . args . source is not None : self . source = self . args . source else : self . source = socket . gethostname ( ) if self . args . timestamp is not None : self . timestamp = int ( self . args . timestamp ) m = { 'metric' : self . metric Name , 'measure' : self . measurement } if self . source is not None : m [ 'source' ] = self . source if self . timestamp is not None : m [ 'timestamp' ] = int ( self . timestamp ) self . process properties ( ) if self . properties is not None : m [ 'metadata' ] = self . properties self . data = json . dumps ( m , sort keys = True ) self . headers = { 'Content-Type' : 'application/json' , "Accept" : "application/json" }
def handle results ( self ) : # Only process if we get HTTP result of 200 if self . api result . status code == requests . codes . ok : payload = json . loads ( self . api result . text ) out = json . dumps ( payload , sort keys = True , indent = 4 , separators = ( ',' , ': ' ) ) print ( self . colorize json ( out ) )
def grammar ( self ) : if self . grammar is None : self . parser = Parser ( ) grammar = self . parser . parse ( self . input source ) self . grammar = grammar . trimmed ( ) . flattened ( ) . flattened ( self . flatten ) return self . grammar
def rules ( self ) : if self . rules is None : self . rules = [ ] for child in self . grammar . children : if child . is type ( Token Type . rule ) : name , expression = child . children self . rules . append ( Rule ( name . value , self . expression to asn ( expression ) , name . position , child . consumed ) ) return self . rules
def comments ( self ) : if self . comments is None : self . comments = [ c for c in self . grammar . children if c . is type ( Token Type . comment ) ] return self . comments
def directives ( self ) : if self . directives is None : self . directives = [ ] for comment in self . comments : self . directives . extend ( self . directives from comment ( comment ) ) return self . directives
def output source ( self ) : if self . output source is None : self . output source = self . compile ( ) return self . output source
def compile ( self ) : fmt = fmt = self . clean fmt ( fmt ) return fmt . format ( date = datetime . utcnow ( ) . isoformat ( ) , imports = self . get imports ( ) , token type enum = self . get token type enum ( ) , class definition = self . get class definition ( ) )
def get imports ( self ) : import directives = [ d for d in self . directives if d . name == "import" ] if import directives : return "\n" + "\n" . join ( d . args [ "value" ] for d in import directives ) else : return ""
def get token type enum ( self ) : fmt = "class Token Type(Enum):\n" "{indent}\"\"\"The token types for parse nodes generated by the Parser.\"\"\"\n" "{indent}" + "\n{indent}" . join ( "{1} = {0}" . format ( num + 1 , r . name ) for num , r in enumerate ( self . rules ) ) return fmt . format ( indent = self . indent )
def get class definition ( self ) : fmt = fmt = self . clean fmt ( fmt ) return fmt . format ( parser base = self . get parser base ( ) , indent = self . indent , entry point = self . get entry point ( ) , rule definitions = "\n" . join ( self . get rule definitions ( ) ) )
def get entry point ( self ) : ep = self . find directive ( "entry point" ) if ep : return ep . args [ "value" ] else : return self . rules [ 0 ] . name
def get rule definition ( self , rule ) : fmt = fmt = self . clean fmt ( fmt ) source = self . indent ( self . ast to code ( rule . expression ) , skip first line = True ) # All the primitives will accept a string x in place of terminal(x). This is terminal shorthand. # However, if a rule is only a wrapper around a single terminal, we have to actually make a # terminal call. This handles that situation. if self . use terminal shorthand and len ( source ) == 1 and source [ 0 ] . startswith ( ( "'" , '"' ) ) : source = [ "terminal({})" . format ( source [ 0 ] ) ] rule source = fmt . format ( rule fxn name = self . get rule fxn name ( rule . name ) , indent = self . indent , rule source = self . get rule source ( rule ) , rule definition = "\n" . join ( source ) , transform = self . get rule transform ( rule ) ) return self . indent ( rule source , 1 )
def get rule source ( self , rule ) : p = len ( self . input source ) + rule . position source = self . input source [ p : p + rule . consumed ] . rstrip ( ) return self . indent ( source , depth = self . indent + "   " , skip first line = True )
def expression to asn ( self , expression ) : new children = [ self . node to asn ( c ) for c in expression . children ] return self . remove grouping groups ( infix to optree ( new children ) )
def node to asn ( self , node ) : if node . is type ( Token Type . identifier ) : return Identifier ( node . svalue ) elif node . is type ( Token Type . terminal ) : return Terminal ( node . svalue ) elif node . is type ( Token Type . option group ) : expr = node . children [ 0 ] return Option Group ( self . expression to asn ( expr ) ) elif node . is type ( Token Type . repetition group ) : expr = node . children [ 0 ] return Repetition Group ( self . expression to asn ( expr ) ) elif node . is type ( Token Type . grouping group ) : expr = node . children [ 0 ] return Grouping Group ( self . expression to asn ( expr ) ) elif node . is type ( Token Type . special handling ) : ident = node . children [ 0 ] return Special Handling ( ident ) elif node . is type ( Token Type . number ) : return Number ( node . svalue ) elif node . is type ( ( Token Type . operator , Token Type . op mult , Token Type . op add ) ) : return Operator Node ( OPERATOR INDEX [ node . svalue ] , node . position ) else : raise Exception ( "Unhandled parse tree node: {0}" . format ( node ) )
def ast to code ( self , node , * * kwargs ) : if isinstance ( node , Optree Node ) : return self . ast optree node to code ( node , * * kwargs ) elif isinstance ( node , Identifier ) : return self . ast identifier to code ( node , * * kwargs ) elif isinstance ( node , Terminal ) : return self . ast terminal to code ( node , * * kwargs ) elif isinstance ( node , Option Group ) : return self . ast option group to code ( node , * * kwargs ) elif isinstance ( node , Repetition Group ) : return self . ast repetition group to code ( node , * * kwargs ) elif isinstance ( node , Special Handling ) : return self . ast special handling to code ( node , * * kwargs ) elif isinstance ( node , Number ) : return self . ast number to code ( node , * * kwargs ) else : raise Exception ( "Unhandled ast node: {0}" . format ( node ) )
def ast optree node to code ( self , node , * * kwargs ) : opnode = node . opnode if opnode is None : return self . ast to code ( node . operands [ 0 ] ) else : operator = opnode . operator if operator is OP ALTERNATE : return self . ast op alternate to code ( node , * * kwargs ) elif operator is OP WS CONCAT : kwargs [ "ignore whitespace" ] = False return self . ast op concat to code ( node , * * kwargs ) elif operator is OP CONCAT : kwargs [ "ignore whitespace" ] = True return self . ast op concat to code ( node , * * kwargs ) elif operator is OP EXCLUDE : return self . ast op exclude to code ( node , * * kwargs ) elif operator is OP MULTIPLY : return self . ast op multiply to code ( node , * * kwargs ) elif operator is OP REPEAT : return self . ast op repeat to code ( node , * * kwargs ) else : raise Exception ( "Unhandled optree node: {0}" . format ( node ) )
def ast terminal to code ( self , terminal , * * kwargs ) : value = replace ( terminal . value ) if self . use terminal shorthand : return [ value ] else : return [ "terminal({})" . format ( value ) ]
def ast option group to code ( self , option group , * * kwargs ) : lines = [ "option(" ] lines . extend ( self . indent ( self . ast to code ( option group . expression ) ) ) lines . append ( ")" ) return lines
def ast repetition group to code ( self , repetition group , ignore whitespace = False , * * kwargs ) : lines = [ "zero or more(" ] lines . extend ( self . indent ( self . ast to code ( repetition group . expression ) ) ) lines [ - 1 ] += "," lines . append ( self . indent ( "ignore whitespace={}" . format ( bool ( ignore whitespace ) ) ) ) lines . append ( ")" ) return lines
def ast special handling to code ( self , special handling , * * kwargs ) : ident = special handling . value . svalue if ident in PB SPECIAL HANDLING : return [ "PB.{0}" . format ( ident ) ] else : return [ "self.{0}" . format ( ident ) ]
def ast op alternate to code ( self , opr , * * kwargs ) : hoist target = OP ALTERNATE operands = self . hoist operands ( opr . operands , lambda t : isinstance ( t , Optree Node ) and t . opnode . operator is hoist target ) lines = [ "alternation([" ] for op in operands : lines . extend ( self . indent ( self . ast to code ( op ) ) ) lines [ - 1 ] += "," lines . append ( "])" ) return lines
def ast op concat to code ( self , opr , * , ignore whitespace , * * kwargs ) : hoist target = OP CONCAT if ignore whitespace else OP WS CONCAT operands = self . hoist operands ( opr . operands , lambda t : isinstance ( t , Optree Node ) and t . opnode . operator is hoist target ) lines = [ "concatenation([" ] for op in operands : lines . extend ( self . indent ( self . ast to code ( op , ignore whitespace = ignore whitespace ) ) ) lines [ - 1 ] += "," lines . append ( "], ignore whitespace={})" . format ( bool ( ignore whitespace ) ) ) return lines
def ast op exclude to code ( self , opr , * * kwargs ) : opl , opr = opr . operands lines = [ "exclusion(" ] lines . extend ( self . indent ( self . ast to code ( opl ) ) ) lines [ - 1 ] += "," lines . extend ( self . indent ( self . ast to code ( opr ) ) ) lines . append ( ")" ) return lines
def ast op multiply to code ( self , opr , ignore whitespace = False , * * kwargs ) : opl , opr = opr . operands if isinstance ( opl , Number ) : times = opl . value subject = self . ast to code ( opr ) else : times = opr . value subject = self . ast to code ( opl ) lines = [ "repeated(" ] lines . extend ( self . indent ( subject ) ) lines [ - 1 ] += "," lines . append ( "{0}times={1}," . format ( self . indent , times ) ) lines . append ( "{0}ignore whitespace={1}" . format ( self . indent , bool ( ignore whitespace ) ) ) lines . append ( ")" ) return lines
def ast op repeat to code ( self , opr , ignore whitespace = False , * * kwargs ) : lines = [ "one or more(" ] lines . extend ( self . indent ( self . ast to code ( opr . operands [ 0 ] ) ) ) lines [ - 1 ] += "," lines . append ( self . indent ( "ignore whitespace={}" . format ( bool ( ignore whitespace ) ) ) ) lines . append ( ")" ) return lines
def find directives ( self , pred ) : if isinstance ( pred , str ) : return [ d for d in self . directives if d . name == pred ] else : return [ d for d in self . directives if pred ( d ) ]
def flatten ( child , parent ) : return parent . is type ( Token Type . expression ) and child . node type == parent . node type
def directives from comment ( cls , comment ) : comment contents = comment . value [ 2 : - 2 ] . strip ( ) comment lines = ( l . strip ( ) for l in comment contents . split ( "\n" ) ) directives = ( l [ 1 : ] . strip ( ) for l in comment lines if l . startswith ( "!" ) ) for directive def in directives : yield cls . parse directive def ( directive def )
def parse directive def ( cls , directive def ) : name , * kwargs = esc split ( directive def , ignore empty = True ) return Directive ( name , { key : value for key , value in ( esc split ( arg , "=" ) for arg in kwargs ) } )
def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . host Group Name is not None : self . url parameters = { "name" : self . args . host Group Name }
def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . plugin name is not None : self . plugin name = self . args . plugin name self . path = "v1/plugins/{0}" . format ( self . plugin name )
def get arguments ( self ) : Api Cli . get arguments ( self ) self . alarm id = self . args . alarm id if self . args . alarm id is not None else None
def handle results ( self ) : # Only process if we get HTTP return code other 200. if self . api result . status code != requests . codes . ok : print ( self . colorize json ( self . api result . text ) )
def get id ( id ) : if id == None : id = wx . New Id ( ) logger . debug ( 'Generated new ID %s.' , id ) else : logger . debug ( 'Using provided id %s.' , id ) return id
def add arguments ( self ) : self . add logging argument ( ) self . parser . add argument ( '-a' , '--api-host' , dest = 'api host' , action = 'store' , metavar = "api host" , help = '{0} API host endpoint' . format ( self . product name ) ) self . parser . add argument ( '-e' , '--email' , dest = 'email' , action = 'store' , metavar = "e mail" , help = 'e-mail that has access to the {0} account' . format ( self . product name ) ) self . parser . add argument ( '-t' , '--api-token' , dest = 'api token' , required = False , action = 'store' , metavar = "api token" , help = 'API token for given e-mail that has access to the {0} account' . format ( self . product name ) ) self . parser . add argument ( '-z' , '--curl' , dest = 'curl' , required = False , action = 'store true' , default = False , help = 'Output the corresponding curl command line and exit' )
def configure logging ( self ) : if self . args . log Level is not None : logging . basic Config ( level = self . levels [ self . args . log Level ] ) logging . info ( "Set logging level to {0}" . format ( self . args . log Level ) )
def execute ( self ) : # Set default arguments from environment variables self . get environment ( ) # Call our member function to add command line arguments, child classes that override need # to call the Api Cli version first to add standard arguments self . add arguments ( ) # Parse the command line arguments self . parse args ( ) # Arguments are parsed call back to the instance so that it can extract the command line # arguments for its use self . get arguments ( ) self . get api parameters ( ) if self . validate arguments ( ) : if self . curl : self . curl output ( ) else : self . call api ( ) self . handle results ( ) else : print ( self . message )
def postfix to optree ( nodes ) : while len ( nodes ) > 1 : nodes = reduce ( nodes ) if len ( nodes ) == 0 : raise Operator Error ( "Empty node list" ) node = nodes [ 0 ] if isinstance ( node , Operator Node ) : raise Operator Error ( "Operator without operands" ) if isinstance ( node , Optree Node ) : return node return Optree Node ( None , ( node , ) )
def pprint ( root , depth = 0 , space unit = "    " ) : spacing = space unit * depth if isinstance ( root , Optree Node ) : print ( "{0}Operator ({1})" . format ( spacing , root . opnode . operator . symbol if root . opnode else "None -> IDENTITY" ) ) for operand in root . operands : pprint ( operand , depth + 1 ) else : print ( "{0}• {1}".f o rmat(s p acing,  r ot))
def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . plugin Name is not None : self . plugin Name = self . args . plugin Name
def add arguments ( self ) : Metric Common . add arguments ( self ) self . parser . add argument ( '-n' , '--metric-name' , dest = 'metric Name' , action = 'store' , required = True , metavar = 'metric name' , help = 'Metric identifier' ) self . parser . add argument ( '-d' , '--display-name' , dest = 'display Name' , action = 'store' , required = True , metavar = 'display name' , help = 'Metric display name' ) self . parser . add argument ( '-s' , '--display-name-short' , dest = 'display Name Short' , action = 'store' , required = True , metavar = 'display short name' , help = 'Metric short display name' ) self . parser . add argument ( '-i' , '--description' , dest = 'description' , action = 'store' , required = not self . update , metavar = 'description' , help = 'Metric description' ) self . parser . add argument ( '-g' , '--aggregate' , dest = 'aggregate' , action = 'store' , required = True , choices = [ 'avg' , 'max' , 'min' , 'sum' ] , help = 'Metric default aggregate' ) self . parser . add argument ( '-u' , '--unit' , dest = 'unit' , action = 'store' , required = False , choices = [ 'percent' , 'number' , 'bytecount' , 'duration' ] , help = 'Metric unit' ) self . parser . add argument ( '-r' , '--resolution' , dest = 'resolution' , action = 'store' , metavar = 'resolution' , required = False , help = 'Metric default resolution' ) self . parser . add argument ( '-y' , '--type' , dest = 'type' , action = 'store' , default = None , required = False , metavar = 'type' , help = 'Sets the type metadata field' ) self . parser . add argument ( '-x' , '--is-disabled' , dest = 'is Disabled' , action = 'store' , default = None , required = False , choices = [ 'true' , 'false' ] , help = 'Enable or disable the metric definition' )
def get arguments ( self ) : Metric Common . get arguments ( self ) if self . args . metric Name is not None : self . metric Name = self . args . metric Name if self . args . display Name is not None : self . display Name = self . args . display Name if self . args . display Name Short is not None : self . display Name Short = self . args . display Name Short if self . args . description is not None : self . description = self . args . description if self . args . aggregate is not None : self . aggregate = self . args . aggregate if self . args . unit is not None : self . unit = self . args . unit if self . args . resolution is not None : self . resolution = self . args . resolution if self . args . is Disabled is not None : self . is Disabled = self . args . is Disabled if self . args . type is not None : self . type = self . args . type data = { } if self . metric Name is not None : data [ 'name' ] = self . metric Name if self . display Name is not None : data [ 'display Name' ] = self . display Name if self . display Name Short is not None : data [ 'display Name Short' ] = self . display Name Short if self . description is not None : data [ 'description' ] = self . description if self . aggregate is not None : data [ 'default Aggregate' ] = self . aggregate if self . unit is not None : data [ 'unit' ] = self . unit if self . resolution is not None : data [ 'default Resolution MS' ] = self . resolution if self . is Disabled is not None : data [ 'is Disabled' ] = True if self . is Disabled == 'yes' else False if self . type is not None : data [ 'type' ] = self . type self . path = "v1/metrics/{0}" . format ( self . metric Name ) self . data = json . dumps ( data , sort keys = True ) self . headers = { 'Content-Type' : 'application/json' , "Accept" : "application/json" }
def get arguments ( self ) : Api Cli . get arguments ( self ) self . alarm name = self . args . alarm name if self . args . alarm name is not None else None
def read ( self ) : f = open ( self . path , "r" ) self . manifest json = f . read ( )
def load ( self ) : manifest = Plugin Manifest ( self . file path ) manifest . get ( ) self . manifest = manifest . get manifest ( )
def get Metric Definition ( self , name ) : metric = None for m in self . metric definitions : if m [ 'name' ] == name : metric = m break return metric
def print Metrics Header ( self , m , d ) : mstr = "Metric Name" dstr = "Description" print ( '|{0}{1}|{2}{3}|' . format ( mstr , ' ' * ( m - len ( mstr ) ) , dstr , ' ' * ( d - len ( dstr ) ) ) ) print ( '|:{0}|:{1}|' . format ( '-' * ( m - 1 ) , '-' * ( d - 1 ) ) )
def get Fields Column Lengths ( self ) : name Len = 0 desc Len = 0 for f in self . fields : name Len = max ( name Len , len ( f [ 'title' ] ) ) desc Len = max ( desc Len , len ( f [ 'description' ] ) ) return ( name Len , desc Len )
def get Metrics Column Lengths ( self ) : display Len = 0 desc Len = 0 for m in self . metrics : display Len = max ( display Len , len ( m [ 'display Name' ] ) ) desc Len = max ( desc Len , len ( m [ 'description' ] ) ) return ( display Len , desc Len )
def escape Underscores ( self ) : new metrics = [ ] for m in self . metrics : m [ 'name' ] = m [ 'name' ] . replace ( " " , "\ " ) new metrics . append ( m ) self . metrics = new metrics
def print Fields Header ( self , f , d ) : fstr = "Field Name" dstr = "Description" f = max ( f , len ( fstr ) ) d = max ( d , len ( dstr ) ) print ( '|{0}{1}|{2}{3}|' . format ( fstr , ' ' * ( f - len ( fstr ) ) , dstr , ' ' * ( d - len ( dstr ) ) ) ) print ( '|:{0}|:{1}|' . format ( '-' * ( f - 1 ) , '-' * ( d - 1 ) ) ) return ( f , d )
def print Metrics ( self , m , d ) : for metric in self . metrics : mstr = metric [ 'display Name' ] dstr = metric [ 'description' ] mlen = m - len ( mstr ) dlen = d - len ( dstr ) print ( "|{0}{1}|{2}{3}|" . format ( mstr , ' ' * mlen , dstr , ' ' * dlen ) )
def print Fields ( self , f , d ) : for field in self . fields : fstr = field [ "title" ] dstr = field [ "description" ] flen = f - len ( fstr ) dlen = d - len ( dstr ) print ( "|{0}{1}|{2}{3}|" . format ( fstr , ' ' * flen , dstr , ' ' * dlen ) )
def output Field Markdown ( self ) : f , d = self . get Fields Column Lengths ( ) fc , dc = self . print Fields Header ( f , d ) f = max ( fc , f ) d = max ( dc , d ) self . print Fields ( f , d )
def output Metric Markdown ( self ) : self . escape Underscores ( ) m , d = self . get Metrics Column Lengths ( ) self . print Metrics Header ( m , d ) self . print Metrics ( m , d )
def generate Markdown ( self ) : self . generate Metric Definitions ( ) self . generate Field Definitions ( ) self . generate Dashboard Definitions ( ) self . output Markdown ( )
def parse ( self , text ) : self . original text = text try : return getattr ( self , self . entry point ) ( text ) except ( Dead End ) as exc : raise Parser Error ( self . most consumed , "Failed to parse input" ) from exc return tree
def attempting ( self , text ) : consumed = len ( self . original text ) - len ( text ) self . most consumed = max ( consumed , self . most consumed )
def add arguments ( self ) : # Call our parent to add the default arguments Api Cli . add arguments ( self ) # Command specific arguments self . parser . add argument ( '-f' , '--format' , dest = 'format' , action = 'store' , required = False , choices = [ 'csv' , 'json' , 'raw' , 'xml' ] , help = 'Output format. Default is raw' ) self . parser . add argument ( '-n' , '--name' , dest = 'metric name' , action = 'store' , required = True , metavar = "metric name" , help = 'Metric identifier' ) self . parser . add argument ( '-g' , '--aggregate' , dest = 'aggregate' , action = 'store' , required = False , choices = [ 'sum' , 'avg' , 'max' , 'min' ] , help = 'Metric default aggregate' ) self . parser . add argument ( '-r' , '--sample' , dest = 'sample' , action = 'store' , type = int , metavar = "sample" , help = 'Down sample rate sample in seconds' ) self . parser . add argument ( '-s' , '--source' , dest = 'source' , action = 'store' , metavar = "source" , required = True , help = 'Source of measurement' ) self . parser . add argument ( '-b' , '--start' , dest = 'start' , action = 'store' , required = True , metavar = "start" , help = 'Start of time range as ISO 8601 string or epoch seconds' ) self . parser . add argument ( '-d' , '--end' , dest = 'end' , action = 'store' , metavar = "end" , required = False , help = 'End of time range as ISO 8601 string or epoch seconds' ) self . parser . add argument ( '-o' , '--date-format' , dest = 'date format' , action = 'store' , metavar = "format" , required = False , help = 'For CSV, JSON, and XML output formats dates (see Python date.strftime). ' + 'Default format is %%s' )
def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . metric name is not None : self . metric name = self . args . metric name if self . args . sample is not None : self . sample = self . args . sample if self . args . source is not None : self . source = self . args . source else : self . source = None if self . args . aggregate is not None : self . aggregate = self . args . aggregate else : self . aggregate = "avg" if self . args . format is not None : self . format = self . args . format else : self . format = "json" if self . args . date format is not None : self . date format = self . args . date format start time = int ( self . parse time date ( self . args . start ) . strftime ( "%s" ) ) # If the end time is not specified then # default to the current time if self . args . end is None : stop time = int ( self . now . strftime ( "%s" ) ) else : stop time = int ( self . parse time date ( self . args . end ) . strftime ( "%s" ) ) # Convert to epoch time in milli-seconds start time *= 1000 stop time *= 1000 self . path = "v1/measurements/{0}" . format ( self . metric name ) url parameters = { "start" : str ( start time ) , "end" : str ( stop time ) , "sample" : str ( self . sample ) , "agg" : self . aggregate } if self . source is not None : url parameters [ 'source' ] = self . source self . url parameters = url parameters
def output csv ( self , text ) : payload = json . loads ( text ) # Print CSV header print ( "{0},{1},{2},{3},{4}" . format ( 'timestamp' , 'metric' , 'aggregate' , 'source' , 'value' ) ) metric name = self . metric name # Loop through the aggregates one row per timestamp, and 1 or more source/value pairs for r in payload [ 'result' ] [ 'aggregates' ] [ 'key' ] : timestamp = self . format timestamp ( r [ 0 ] [ 0 ] ) # timestamp = string.strip(timestamp, ' ') # timestamp = string.strip(timestamp, "'") for s in r [ 1 ] : print ( '{0},"{1}","{2}","{3}",{4}' . format ( timestamp , metric name , self . aggregate , s [ 0 ] , s [ 1 ] ) )
def output json ( self , text ) : payload = json . loads ( text ) data = [ ] metric name = self . metric name for r in payload [ 'result' ] [ 'aggregates' ] [ 'key' ] : timestamp = self . format timestamp ( r [ 0 ] [ 0 ] ) for s in r [ 1 ] : data . append ( { "timestamp" : timestamp , "metric" : metric name , "aggregate" : self . aggregate , "source" : s [ 0 ] , "value" : s [ 1 ] , } ) payload = { "data" : data } out = json . dumps ( payload , indent = self . indent , separators = ( ',' , ': ' ) ) print ( self . colorize json ( out ) )
def output raw ( self , text ) : payload = json . loads ( text ) out = json . dumps ( payload , sort keys = True , indent = self . indent , separators = ( ',' , ': ' ) ) print ( self . colorize json ( out ) )
def output xml ( self , text ) : # Create the main document nodes document = Element ( 'results' ) comment = Comment ( 'Generated by True Sight Pulse measurement-get CLI' ) document . append ( comment ) aggregates = Sub Element ( document , 'aggregates' ) aggregate = Sub Element ( aggregates , 'aggregate' ) measurements = Sub Element ( aggregate , 'measurements' ) # Parse the JSON result so we can translate to XML payload = json . loads ( text ) # Current only support a single metric, if we move to the batch API then # we can handle multiple metric name = self . metric name # Loop through the aggregates one row per timestamp, and 1 or more source/value pairs for r in payload [ 'result' ] [ 'aggregates' ] [ 'key' ] : timestamp = self . format timestamp ( r [ 0 ] [ 0 ] ) for s in r [ 1 ] : # Each timestamp, metric, source, values is placed in a measure tag measure node = Sub Element ( measurements , 'measure' ) source = s [ 0 ] value = str ( s [ 1 ] ) ts node = Sub Element ( measure node , 'timestamp' ) ts node . text = str ( timestamp ) metric node = Sub Element ( measure node , 'metric' ) metric node . text = metric name metric node = Sub Element ( measure node , 'aggregate' ) metric node . text = self . aggregate source node = Sub Element ( measure node , 'source' ) source node . text = source value node = Sub Element ( measure node , 'value' ) value node . text = value rough string = Element Tree . tostring ( document , 'utf-8' ) reparse = minidom . parse String ( rough string ) output = reparse . toprettyxml ( indent = " " ) print ( self . colorize xml ( output ) )
def handle results ( self ) : # Only process if we get HTTP result of 200 if self . api result . status code == requests . codes . ok : if self . format == "json" : self . output json ( self . api result . text ) elif self . format == "csv" : self . output csv ( self . api result . text ) elif self . format == "raw" : self . output raw ( self . api result . text ) elif self . format == "xml" : self . output xml ( self . api result . text ) else : pass
def trimmed pred default ( node , parent ) : return isinstance ( node , Parse Node ) and ( node . is empty or node . is type ( Parse Node Type . terminal ) )
def pprint ( root , depth = 0 , space unit = "    " , * , source len = 0 , file = None ) : spacing = space unit * depth if isinstance ( root , str ) : print ( "{0}terminal@(?): {1}" . format ( spacing , root ) , file = file ) else : if root . position is None : position = - 1 elif root . position < 0 : position = source len + root . position else : position = root . position if root . is value : print ( "{0}{1}@({2}:{3}):\t{4}" . format ( spacing , root . node type , position , root . consumed , root . svalue ) , file = file ) else : print ( "{0}{1}@({2}:{3}):" . format ( spacing , root . node type , position , root . consumed ) , file = file ) for child in root . children : pprint ( child , depth + 1 , source len = source len , file = file )
def repetition ( extractor , bounds , * , ignore whitespace = False ) : return partial ( get repetition , extractor , bounds = bounds , ignore whitespace = ignore whitespace )
def count leading whitespace ( text ) : idx = 0 for idx , char in enumerate ( text ) : if not char . isspace ( ) : return idx return idx + 1
def retyped ( self , new type ) : return Parse Node ( new type , children = list ( self . children ) , consumed = self . consumed , position = self . position , ignored = self . ignored )
def get arguments ( self ) : Api Cli . get arguments ( self ) # Get the host group name if self . args . host group name is not None : self . host group name = self . args . host group name # Get the list of sources separated by commas if self . args . sources is not None : self . sources = self . args . sources payload = { } if self . host group name is not None : payload [ 'name' ] = self . host group name if self . sources is not None : source list = str . split ( self . sources , ',' ) if 'hostnames' not in payload : payload [ 'hostnames' ] = [ ] for s in source list : payload [ 'hostnames' ] . append ( s ) self . data = json . dumps ( payload , sort keys = True ) self . headers = { 'Content-Type' : 'application/json' , "Accept" : "application/json" }
def get scope list ( self ) -> list : # by default only return scoped name lstparent = [ self ] p = self . get parent ( ) while p is not None : lstparent . append ( p ) p = p . get parent ( ) return lstparent
def get scope names ( self ) -> list : # allow global scope to have an None string instance lscope = [ ] for scope in reversed ( self . get scope list ( ) ) : if scope . name is not None : # handle fun/block scope decoration lscope . append ( scope . name ) return lscope
def position ( self ) -> Position : return Position ( self . index , self . lineno , self . col offset )
def max readed position ( self ) -> Position : return Position ( self . maxindex , self . maxline , self . maxcol )
def step next char ( self ) : self . index += 1 self . col offset += 1 if self . index > self . maxindex : self . maxindex = self . index self . maxcol = self . col offset self . maxline = self . lineno
def step next line ( self ) : self . eol . append ( self . position ) self . lineno += 1 self . col offset = 0
def step prev line ( self ) : #TODO(bps): raise explicit error for unregistered eol #assert self. eol[-1].index == self. index if len ( self . eol ) > 0 : self . position = self . eol . pop ( )
def last readed line ( self ) -> str : mpos = self . cursor . max readed position mindex = mpos . index # search last \n prevline = mindex - 1 if mindex == self . eos index else mindex while prevline >= 0 and self . content [ prevline ] != '\n' : prevline -= 1 # search next \n nextline = mindex while nextline < self . eos index and self . content [ nextline ] != '\n' : nextline += 1 last line = self . content [ prevline + 1 : nextline ] return last line
def incpos ( self , length : int = 1 ) -> int : if length < 0 : raise Value Error ( "length must be positive" ) i = 0 while ( i < length ) : if self . cursor . index < self . len : if self . peek char == '\n' : self . cursor . step next line ( ) self . cursor . step next char ( ) i += 1 return self . cursor . index
def save context ( self ) -> bool : self . contexts . append ( self . cursor . position ) return True
def restore context ( self ) -> bool : self . cursor . position = self . contexts . pop ( ) return False
def to fmt ( self ) -> fmt . indentable : qual = "scope" txt = fmt . sep ( " " , [ qual ] ) name = self . show name ( ) if name != "" : txt . lsdata . append ( name ) if len ( self . hsig ) > 0 or len ( self . map Type Translate ) > 0 : lsb = [ ] if len ( self . map Type Translate ) > 0 : lsb . append ( "translate:\n" ) lsb . append ( fmt . end ( "\n" , self . map Type Translate . to fmt ( ) ) ) for k in sorted ( self . hsig . keys ( ) ) : s = self . hsig [ k ] lsb . append ( fmt . end ( "\n" , [ s . to fmt ( ) ] ) ) block = fmt . block ( ":\n" , "" , fmt . tab ( lsb ) ) txt . lsdata . append ( block ) return txt
def to fmt ( self ) : qual = "evalctx" lseval = [ ] block = fmt . block ( ":\n" , "" , fmt . tab ( lseval ) ) txt = fmt . sep ( " " , [ qual , block ] ) lseval . append ( self . sig . to fmt ( ) ) if len ( self . resolution ) > 0 : lsb = [ ] for k in sorted ( self . resolution . keys ( ) ) : s = self . resolution [ k ] if s is not None : lsb . append ( fmt . end ( "\n" , [ "'%s': %s (%s)" % ( k , s , s ( ) . show name ( ) ) ] ) ) else : lsb . append ( fmt . end ( "\n" , [ "'%s': Unresolved" % ( k ) ] ) ) if self . translate to is not None : lsb . append ( "use translator:" ) lsb . append ( self . translate to . to fmt ( ) ) if self . variadic types is not None : lsb . append ( "variadic types:\n" ) arity = self . sig . arity for t in self . variadic types : lsb . append ( "[%d] : %s\n" % ( arity , t ) ) arity += 1 lseval . append ( fmt . block ( "\nresolution :\n" , "" , fmt . tab ( lsb ) ) ) return txt
def to fmt ( self , with from = False ) -> fmt . indentable : txt = fmt . sep ( "\n" , [ fmt . sep ( " " , [ self . type source , "to" , self . type target , '=' , self . fun . to fmt ( ) ] ) , self . notify . get content ( with from ) ] ) return txt
def to fmt ( self ) : params = "" txt = fmt . sep ( " " , [ 'val' ] ) name = self . show name ( ) if name != "" : txt . lsdata . append ( name ) txt . lsdata . append ( '(%s)' % self . value ) txt . lsdata . append ( ': ' + self . tret ) return txt
def to fmt ( self ) : params = "" txt = fmt . sep ( " " , [ 'fun' ] ) name = self . show name ( ) if name != "" : txt . lsdata . append ( name ) tparams = [ ] if self . tparams is not None : tparams = list ( self . tparams ) if self . variadic : tparams . append ( '...' ) params = '(' + ", " . join ( tparams ) + ')' txt . lsdata . append ( ': ' + params ) txt . lsdata . append ( '-> ' + self . tret ) return txt
def set name ( self , name : str ) : self . name = name # update internal names lsig = self . hsig . values ( ) self . hsig = { } for s in lsig : self . hsig [ s . internal name ( ) ] = s
def count vars ( self ) -> int : n = 0 for s in self . hsig . values ( ) : if hasattr ( s , 'is var' ) and s . is var : n += 1 return n
def count funs ( self ) -> int : n = 0 for s in self . hsig . values ( ) : if hasattr ( s , 'is fun' ) and s . is fun : n += 1 return n
def update ( self , sig : list or Scope ) -> Scope : values = sig if hasattr ( sig , 'values' ) : values = sig . values ( ) for s in values : if self . is namespace : s . set parent ( self ) if isinstance ( s , Scope ) : s . state = State Scope . EMBEDDED self . hsig [ s . internal name ( ) ] = s self . update count ( ) return self
def union ( self , sig : Scope ) -> Scope : new = Scope ( sig = self . hsig . values ( ) , state = self . state ) new |= sig return new
def intersection update ( self , oset : Scope ) -> Scope : keys = list ( self . hsig . keys ( ) ) for k in keys : if k not in oset : del self . hsig [ k ] else : self . hsig [ k ] = oset . get ( k ) return self
def intersection ( self , sig : Scope ) -> Scope : new = Scope ( sig = self . hsig . values ( ) , state = self . state ) new &= sig return new
def difference update ( self , oset : Scope ) -> Scope : keys = list ( self . hsig . keys ( ) ) for k in keys : if k in oset : del self . hsig [ k ] return self
def difference ( self , sig : Scope ) -> Scope : new = Scope ( sig = self . hsig . values ( ) , state = self . state ) new -= sig return new
def symmetric difference ( self , sig : Scope ) -> Scope : new = Scope ( sig = self . hsig . values ( ) , state = self . state ) new ^= sig return new
def add ( self , it : Signature ) -> bool : if isinstance ( it , Scope ) : it . state = State Scope . EMBEDDED txt = it . internal name ( ) it . set parent ( self ) if self . is namespace : txt = it . internal name ( ) if txt == "" : txt = ' ' + str ( len ( self . hsig ) ) if txt in self . hsig : raise Key Error ( "Already exists %s" % txt ) self . hsig [ txt ] = it self . update count ( ) return True
def remove ( self , it : Signature ) -> bool : txt = it . internal name ( ) if txt not in self . hsig : raise Key Error ( it . show name ( ) + ' not in Set' ) sig = self . hsig [ txt ] if isinstance ( sig , Scope ) : sig . state = State Scope . LINKED del self . hsig [ txt ] return True
def discard ( self , it : Signature ) -> bool : txt = it . internal name ( ) if txt in self . hsig : sig = self . hsig [ txt ] if isinstance ( sig , Scope ) : sig . state = State Scope . LINKED del self . hsig [ txt ] return True return False
def first ( self ) -> Signature : k = sorted ( self . hsig . keys ( ) ) return self . hsig [ k [ 0 ] ]
def last ( self ) -> Signature : k = sorted ( self . hsig . keys ( ) ) return self . hsig [ k [ - 1 ] ]
def get ( self , key : str , default = None ) -> Signature : item = default if key in self . hsig : item = self . hsig [ key ] return item
def get by symbol name ( self , name : str ) -> Scope : lst = [ ] for s in self . values ( ) : if s . name == name : # create an Eval Ctx only when necessary lst . append ( Eval Ctx . from sig ( s ) ) # include parent # TODO: see all case of local redefinition for #       global overloads # possible algos... take all with different internal name if len ( lst ) == 0 : p = self . get parent ( ) if p is not None : return p . get by symbol name ( name ) rscope = Scope ( sig = lst , state = State Scope . LINKED , is namespace = False ) # inherit type/translation from parent rscope . set parent ( self ) return rscope
def call Injector ( self , old : Node , trans : Translator ) -> Node : if self . ast Translator Injector is None : if self . parent is not None : # TODO: think if we forward for all State Scope # forward to parent scope return self . parent ( ) . call Injector ( old , trans ) else : raise Type Error ( "Must define an Translator Injector" ) return self . ast Translator Injector ( old , trans )
def set ( self , othernode ) : self . class = othernode . class self . clean ( ) if len ( othernode ) > 0 : for k , v in othernode . items ( ) : self [ k ] = v for k , v in vars ( othernode ) . items ( ) : setattr ( self , k , v )
def hit ok ( hit , min hit charge , max hit charge ) : # Omit hits with charge < min hit charge if hit [ 'charge' ] < min hit charge : return False # Omit hits with charge > max hit charge if max hit charge != 0 and hit [ 'charge' ] > max hit charge : return False return True
def cluster hits ( hits , clusters , assigned hit array , cluster hit indices , column cluster distance , row cluster distance , frame cluster distance , min hit charge , max hit charge , ignore same hits , noisy pixels , disabled pixels ) : total hits = hits . shape [ 0 ] if total hits == 0 : return 0 # total clusters max cluster hits = cluster hit indices . shape [ 0 ] if total hits != clusters . shape [ 0 ] : raise Value Error ( "hits and clusters must be the same size" ) if total hits != assigned hit array . shape [ 0 ] : raise Value Error ( "hits and assigned hit array must be the same size" ) # Correction for charge weighting # Some chips have non-zero charge for a charge value of zero, charge needs to be corrected to calculate cluster center correctly if min hit charge == 0 : charge correction = 1 else : charge correction = 0 # Temporary variables that are reset for each cluster or event start event hit index = 0 start event cluster index = 0 cluster size = 0 event number = hits [ 0 ] [ 'event number' ] event cluster index = 0 # Outer loop over all hits in the array (referred to as actual hit) for i in range ( total hits ) : # Check for new event and reset event variables if new event ( hits [ i ] [ 'event number' ] , event number ) : finish event ( hits = hits , clusters = clusters , start event hit index = start event hit index , stop event hit index = i , start event cluster index = start event cluster index , stop event cluster index = start event cluster index + event cluster index ) start event hit index = i start event cluster index = start event cluster index + event cluster index event number = hits [ i ] [ 'event number' ] event cluster index = 0 if assigned hit array [ i ] > 0 : # Hit was already assigned to a cluster in the inner loop, thus skip actual hit continue if not hit ok ( hit = hits [ i ] , min hit charge = min hit charge , max hit charge = max hit charge ) or ( disabled pixels . shape [ 0 ] != 0 and pixel masked ( hits [ i ] , disabled pixels ) ) : set hit invalid ( hit = hits [ i ] , cluster id = - 1 ) assigned hit array [ i ] = 1 continue # Set/reset cluster variables for new cluster # Reset temp array with hit indices of actual cluster for the next cluster set 1d array ( cluster hit indices , - 1 , cluster size ) cluster hit indices [ 0 ] = i assigned hit array [ i ] = 1 cluster size = 1 # actual cluster has one hit so far for j in cluster hit indices : # Loop over all hits of the actual cluster; cluster hit indices is updated within the loop if new hit are found if j < 0 : # There are no more cluster hits found break for k in range ( cluster hit indices [ 0 ] + 1 , total hits ) : # Stop event hits loop if new event is reached if new event ( hits [ k ] [ 'event number' ] , event number ) : break # Hit is already assigned to a cluster, thus skip actual hit if assigned hit array [ k ] > 0 : continue if not hit ok ( hit = hits [ k ] , min hit charge = min hit charge , max hit charge = max hit charge ) or ( disabled pixels . shape [ 0 ] != 0 and pixel masked ( hits [ k ] , disabled pixels ) ) : set hit invalid ( hit = hits [ k ] , cluster id = - 1 ) assigned hit array [ k ] = 1 continue # Check if event hit belongs to actual hit and thus to the actual cluster if is in max difference ( hits [ j ] [ 'column' ] , hits [ k ] [ 'column' ] , column cluster distance ) and is in max difference ( hits [ j ] [ 'row' ] , hits [ k ] [ 'row' ] , row cluster distance ) and is in max difference ( hits [ j ] [ 'frame' ] , hits [ k ] [ 'frame' ] , frame cluster distance ) : if not ignore same hits or hits [ j ] [ 'column' ] != hits [ k ] [ 'column' ] or hits [ j ] [ 'row' ] != hits [ k ] [ 'row' ] : cluster size += 1 if cluster size > max cluster hits : raise Index Error ( 'cluster hit indices is too small to contain all cluster hits' ) cluster hit indices [ cluster size - 1 ] = k assigned hit array [ k ] = 1 else : set hit invalid ( hit = hits [ k ] , cluster id = - 2 ) assigned hit array [ k ] = 1 # check for valid cluster and add it to the array if cluster size == 1 and noisy pixels . shape [ 0 ] != 0 and pixel masked ( hits [ cluster hit indices [ 0 ] ] , noisy pixels ) : set hit invalid ( hit = hits [ cluster hit indices [ 0 ] ] , cluster id = - 1 ) else : finish cluster ( hits = hits , clusters = clusters , cluster size = cluster size , cluster hit indices = cluster hit indices , cluster index = start event cluster index + event cluster index , cluster id = event cluster index , charge correction = charge correction , noisy pixels = noisy pixels , disabled pixels = disabled pixels ) event cluster index += 1 # Last event is assumed to be finished at the end of the hit array, thus add info finish event ( hits = hits , clusters = clusters , start event hit index = start event hit index , stop event hit index = total hits , start event cluster index = start event cluster index , stop event cluster index = start event cluster index + event cluster index ) total clusters = start event cluster index + event cluster index return total clusters
def resolve ( self ) : # collect types for resolution t2resolv = [ ] if hasattr ( self . sig , 'tret' ) : t2resolv . append ( self . sig . tret ) if hasattr ( self . sig , 'tparams' ) and self . sig . tparams is not None : for p in self . sig . tparams : t2resolv . append ( p ) if self . translate to is not None : t2resolv . append ( self . translate to . target ) if self . variadic types is not None : for t in self . variadic types : t2resolv . append ( t ) for t in t2resolv : for c in t . components : if c not in self . resolution or self . resolution [ c ] is None : # try to find what is c parent = self . get parent ( ) if parent is not None : sc = parent . get by symbol name ( c ) if len ( sc ) == 1 : sc = list ( sc . values ( ) ) [ 0 ] # unwrap Eval Ctx around Type if isinstance ( sc , Eval Ctx ) : sc = sc . sig rtyp = weakref . ref ( sc ) self . resolution [ c ] = rtyp continue # unresolved self . resolution [ c ] = None
def set resolved name ( self , ref : dict , type name2solve : Type Name , type name ref : Type Name ) : if self . resolution [ type name2solve . value ] is None : self . resolution [ type name2solve . value ] = ref [ type name ref . value ]
def to fmt ( self ) -> fmt . indentable : lsb = [ ] if len ( self . lsig ) > 0 : for s in self . lsig : lsb . append ( s . to fmt ( ) ) block = fmt . block ( "(" , ")" , fmt . sep ( ', ' , lsb ) ) qual = "tuple" txt = fmt . sep ( "" , [ qual , block ] ) return txt
def internal name ( self ) : unq = super ( ) . internal name ( ) if self . tret is not None : unq += " " + self . tret return unq
def delete local ( self , filename ) : if os . path . exists ( filename ) : os . remove ( filename )
def delete s3 ( self , filename , bucket name ) : conn = S3Connection ( self . access key id , self . access key secret ) bucket = conn . get bucket ( bucket name ) if type ( filename ) . name == 'Key' : filename = '/' + filename . name path = self . get s3 path ( filename ) k = Key ( bucket ) k . key = path try : bucket . delete key ( k ) except S3Response Error : pass
def delete ( self , filename , storage type = None , bucket name = None ) : if not ( storage type and bucket name ) : self . delete local ( filename ) else : if storage type != 's3' : raise Value Error ( 'Storage type "%s" is invalid, the only supported storage type (apart from default local storage) is s3.' % storage type ) self . delete s3 ( filename , bucket name )
def save local ( self , temp file , filename , obj ) : path = self . get path ( filename ) if not os . path . exists ( os . path . dirname ( path ) ) : os . makedirs ( os . path . dirname ( path ) , self . permission | 0o111 ) fd = open ( path , 'wb' ) # Thanks to: # http://stackoverflow.com/a/3253276/2066849 temp file . seek ( 0 ) t = temp file . read ( 1048576 ) while t : fd . write ( t ) t = temp file . read ( 1048576 ) fd . close ( ) if self . filesize field : setattr ( obj , self . filesize field , os . path . getsize ( path ) ) return filename
def save s3 ( self , temp file , filename , obj ) : conn = S3Connection ( self . access key id , self . access key secret ) bucket = conn . get bucket ( self . bucket name ) path = self . get s3 path ( filename ) k = bucket . new key ( path ) k . set contents from string ( temp file . getvalue ( ) ) k . set acl ( self . acl ) if self . filesize field : setattr ( obj , self . filesize field , k . size ) return filename
def save ( self , temp file , filename , obj ) : if not ( self . storage type and self . bucket name ) : ret = self . save local ( temp file , filename , obj ) else : if self . storage type != 's3' : raise Value Error ( 'Storage type "%s" is invalid, the only supported storage type (apart from default local storage) is s3.' % self . storage type ) ret = self . save s3 ( temp file , filename , obj ) if self . field name : setattr ( obj , self . field name , ret ) if self . storage type == 's3' : if self . storage type field : setattr ( obj , self . storage type field , self . storage type ) if self . bucket name field : setattr ( obj , self . bucket name field , self . bucket name ) else : if self . storage type field : setattr ( obj , self . storage type field , '' ) if self . bucket name field : setattr ( obj , self . bucket name field , '' ) return ret
def find by path s3 ( self , path , bucket name ) : conn = S3Connection ( self . access key id , self . access key secret ) bucket = conn . get bucket ( bucket name ) s3 path = self . get s3 path ( path ) return bucket . list ( prefix = s3 path )
def enum ( * sequential , * * named ) : #: build enums from parameter enums = dict ( zip ( sequential , range ( len ( sequential ) ) ) , * * named ) enums [ 'map' ] = copy . copy ( enums ) #: build reverse mapping enums [ 'rmap' ] = { } for key , value in enums . items ( ) : if type ( value ) is int : enums [ 'rmap' ] [ value ] = key return type ( 'Enum' , ( ) , enums )
def checktypes ( func ) : sig = inspect . signature ( func ) types = { } for param in sig . parameters . values ( ) : # Iterate through function's parameters and build the list of # arguments types param type = param . annotation if param type is param . empty or not inspect . isclass ( param type ) : # Missing annotation or not a type, skip it continue types [ param . name ] = param type # If the argument has a type specified, let's check that its # default value (if present) conforms with the type. if ( param . default is not param . empty and not isinstance ( param . default , param type ) ) : raise Value Error ( "{func}: wrong type of a default value for {arg!r}" . format ( func = func . qualname , arg = param . name ) ) def check type ( sig , arg name , arg type , arg value ) : # Internal function that encapsulates arguments type checking if not isinstance ( arg value , arg type ) : raise Value Error ( "{func}: wrong type of {arg!r} argument, " "{exp!r} expected, got {got!r}" . format ( func = func . qualname , arg = arg name , exp = arg type . name , got = type ( arg value ) . name ) ) @ functools . wraps ( func ) def wrapper ( * args , * * kwargs ) : # Let's bind the arguments ba = sig . bind ( * args , * * kwargs ) for arg name , arg in ba . arguments . items ( ) : # And iterate through the bound arguments try : type = types [ arg name ] except Key Error : continue else : # OK, we have a type for the argument, lets get the # corresponding parameter description from the signature object param = sig . parameters [ arg name ] if param . kind == param . VAR POSITIONAL : # If this parameter is a variable-argument parameter, # then we need to check each of its values for value in arg : check type ( sig , arg name , type , value ) elif param . kind == param . VAR KEYWORD : # If this parameter is a variable-keyword-argument # parameter: for subname , value in arg . items ( ) : check type ( sig , arg name + ':' + subname , type , value ) else : # And, finally, if this parameter a regular one: check type ( sig , arg name , type , arg ) result = func ( * ba . args , * * ba . kwargs ) # The last bit - let's check that the result is correct return type = sig . return annotation if ( return type is not sig . empty and isinstance ( return type , type ) and not isinstance ( result , return type ) ) : raise Value Error ( '{func}: wrong return type, {exp} expected, got {got}' . format ( func = func . qualname , exp = return type . name , got = type ( result ) . name ) ) return result return wrapper
def add method ( cls ) : def wrapper ( f ) : #if hasattr(cls, f. name ): #    raise Attribute Error("{} already has a '{}' attribute".format( #        cls. name , f. name )) setattr ( cls , f . name , f ) return f return wrapper
def read eol ( self ) -> bool : if self . read eof ( ) : return False self . stream . save context ( ) self . read char ( '\r' ) if self . read char ( '\n' ) : return self . stream . validate context ( ) return self . stream . restore context ( )
def push rule nodes ( self ) -> bool : if self . rule nodes is None : self . rule nodes = collections . Chain Map ( ) self . tag cache = collections . Chain Map ( ) self . id cache = collections . Chain Map ( ) else : self . rule nodes = self . rule nodes . new child ( ) self . tag cache = self . tag cache . new child ( ) self . id cache = self . id cache . new child ( ) return True
def pop rule nodes ( self ) -> bool : self . rule nodes = self . rule nodes . parents self . tag cache = self . tag cache . parents self . id cache = self . id cache . parents return True
def value ( self , n : Node ) -> str : id n = id ( n ) idcache = self . id cache if id n not in idcache : return "" name = idcache [ id n ] tag cache = self . tag cache if name not in tag cache : raise Exception ( "Incoherent tag cache" ) tag = tag cache [ name ] k = "%d:%d" % ( tag . begin , tag . end ) valcache = self . streams [ - 1 ] . value cache if k not in valcache : valcache [ k ] = str ( tag ) return valcache [ k ]
def begin tag ( self , name : str ) -> Node : # Check if we could attach tag cache to current rule nodes scope self . tag cache [ name ] = Tag ( self . stream , self . stream . index ) return True
def end tag ( self , name : str ) -> Node : self . tag cache [ name ] . set end ( self . stream . index ) return True
def set rules ( cls , rules : dict ) -> bool : cls . rules = cls . rules . new child ( ) for rule name , rule pt in rules . items ( ) : if '.' not in rule name : rule name = cls . module + '.' + cls . name + '.' + rule name meta . set one ( cls . rules , rule name , rule pt ) return True
def set hooks ( cls , hooks : dict ) -> bool : cls . hooks = cls . hooks . new child ( ) for hook name , hook pt in hooks . items ( ) : if '.' not in hook name : hook name = cls . module + '.' + cls . name + '.' + hook name meta . set one ( cls . hooks , hook name , hook pt ) return True
def eval rule ( self , name : str ) -> Node : # context created by caller n = Node ( ) id n = id ( n ) self . rule nodes [ ' ' ] = n self . id cache [ id n ] = ' ' # TODO: other behavior for  empty rules? if name not in self . class . rules : self . diagnostic . notify ( error . Severity . ERROR , "Unknown rule : %s" % name , error . Location Info . from stream ( self . stream , is error = True ) ) raise self . diagnostic self . last Rule = name rule to eval = self . class . rules [ name ] # TODO: add packrat cache here, same rule - same pos == same res res = rule to eval ( self ) if res : res = self . rule nodes [ ' ' ] return res
def eval hook ( self , name : str , ctx : list ) -> Node : if name not in self . class . hooks : # TODO: don't always throw error, could have return True by default self . diagnostic . notify ( error . Severity . ERROR , "Unknown hook : %s" % name , error . Location Info . from stream ( self . stream , is error = True ) ) raise self . diagnostic self . last Rule = '#' + name res = self . class . hooks [ name ] ( self , * ctx ) if type ( res ) is not bool : raise Type Error ( "Your hook %r didn't return a bool value" % name ) return res
def peek text ( self , text : str ) -> bool : start = self . stream . index stop = start + len ( text ) if stop > self . stream . eos index : return False return self . stream [ self . stream . index : stop ] == text
def one char ( self ) -> bool : if self . read eof ( ) : return False self . stream . incpos ( ) return True
def read until eof ( self ) -> bool : if self . read eof ( ) : return True # TODO: read ALL self . stream . save context ( ) while not self . read eof ( ) : self . stream . incpos ( ) return self . stream . validate context ( )
def ignore blanks ( self ) -> bool : self . stream . save context ( ) if not self . read eof ( ) and self . stream . peek char in " \t\v\f\r\n" : while ( not self . read eof ( ) and self . stream . peek char in " \t\v\f\r\n" ) : self . stream . incpos ( ) return self . stream . validate context ( ) return self . stream . validate context ( )
def internal name ( self ) : unq = 'f ' + super ( ) . internal name ( ) if self . tparams is not None : unq += " " + " " . join ( self . tparams ) if self . tret is not None : unq += " " + self . tret return unq
def check struct compatibility ( self , hits ) : for key , in self . cluster hits descr : if key in self . hit fields mapping inverse : mapped key = self . hit fields mapping inverse [ key ] else : mapped key = key # Only check hit fields that contain hit information if mapped key in [ 'cluster ID' , 'is seed' , 'cluster size' , 'n cluster' ] : continue if key not in hits . dtype . names : raise Type Error ( 'Required hit field "%s" not found.' % key ) if self . cluster hits . dtype [ mapped key ] != hits . dtype [ key ] : raise Type Error ( 'The dtype for hit data field "%s" does not match. Got/expected: %s/%s.' % ( key , hits . dtype [ key ] , self . cluster hits . dtype [ mapped key ] ) ) additional hit fields = set ( hits . dtype . names ) - set ( [ key for key , val in self . cluster hits descr ] ) if additional hit fields : logging . warning ( 'Found additional hit fields: %s' % ", " . join ( additional hit fields ) )
def add mod ( self , seq , mod ) : modstr = self . value ( mod ) if modstr == '~' : seq . parser tree = parsing . Complement ( seq . parser tree ) elif modstr == '!!' : seq . parser tree = parsing . Look Ahead ( seq . parser tree ) elif modstr == '!' : seq . parser tree = parsing . Neg ( seq . parser tree ) elif modstr == '->' : seq . parser tree = parsing . Until ( seq . parser tree ) return True
def add ruleclause name ( self , ns name , rid ) -> bool : ns name . parser tree = parsing . Rule ( self . value ( rid ) ) return True
def add rules ( self , bnf , r ) -> bool : bnf [ r . rulename ] = r . parser tree return True
def add rule ( self , rule , rn , alts ) -> bool : rule . rulename = self . value ( rn ) rule . parser tree = alts . parser tree return True
def add sequences ( self , sequences , cla ) -> bool : if not hasattr ( sequences , 'parser tree' ) : # forward sublevel of sequence as is sequences . parser tree = cla . parser tree else : oldnode = sequences if isinstance ( oldnode . parser tree , parsing . Seq ) : oldpt = list ( oldnode . parser tree . ptlist ) else : oldpt = [ oldnode . parser tree ] oldpt . append ( cla . parser tree ) sequences . parser tree = parsing . Seq ( * tuple ( oldpt ) ) return True
def add alt ( self , alternatives , alt ) -> bool : if not hasattr ( alternatives , 'parser tree' ) : # forward sublevel of alt as is if hasattr ( alt , 'parser tree' ) : alternatives . parser tree = alt . parser tree else : alternatives . parser tree = alt else : oldnode = alternatives if isinstance ( oldnode . parser tree , parsing . Alt ) : oldpt = list ( oldnode . parser tree . ptlist ) else : oldpt = [ oldnode . parser tree ] oldpt . append ( alt . parser tree ) alternatives . parser tree = parsing . Alt ( * tuple ( oldpt ) ) return True
def add range ( self , sequence , begin , end ) : sequence . parser tree = parsing . Range ( self . value ( begin ) . strip ( "'" ) , self . value ( end ) . strip ( "'" ) ) return True
def add rpt ( self , sequence , mod , pt ) : modstr = self . value ( mod ) if modstr == '!!' : # cursor on the REPEATER self . stream . restore context ( ) # log the error self . diagnostic . notify ( error . Severity . ERROR , "Cannot repeat a lookahead rule" , error . Location Info . from stream ( self . stream , is error = True ) ) raise self . diagnostic if modstr == '!' : # cursor on the REPEATER self . stream . restore context ( ) # log the error self . diagnostic . notify ( error . Severity . ERROR , "Cannot repeat a negated rule" , error . Location Info . from stream ( self . stream , is error = True ) ) raise self . diagnostic oldnode = sequence sequence . parser tree = pt . functor ( oldnode . parser tree ) return True
def add capture ( self , sequence , cpt ) : cpt value = self . value ( cpt ) sequence . parser tree = parsing . Capture ( cpt value , sequence . parser tree ) return True
def add bind ( self , sequence , cpt ) : cpt value = self . value ( cpt ) sequence . parser tree = parsing . Bind ( cpt value , sequence . parser tree ) return True
def add hook ( self , sequence , h ) : sequence . parser tree = parsing . Hook ( h . name , h . listparam ) return True
def param num ( self , param , n ) : param . pair = ( int ( self . value ( n ) ) , int ) return True
def param str ( self , param , s ) : param . pair = ( self . value ( s ) . strip ( '"' ) , str ) return True
def param char ( self , param , c ) : param . pair = ( self . value ( c ) . strip ( "'" ) , str ) return True
def param id ( self , param , i ) : param . pair = ( self . value ( i ) , parsing . Node ) return True
def hook name ( self , hook , n ) : hook . name = self . value ( n ) hook . listparam = [ ] return True
def hook param ( self , hook , p ) : hook . listparam . append ( p . pair ) return True
def add directive2 ( self , sequence , d , s ) : sequence . parser tree = parsing . Directive2 ( d . name , d . listparam , s . parser tree ) return True
def add directive ( self , sequence , d , s ) : if d . name in meta . directives : the class = meta . directives [ d . name ] sequence . parser tree = parsing . Directive ( the class ( ) , d . listparam , s . parser tree ) elif d . name in meta . decorators : the class = meta . decorators [ d . name ] sequence . parser tree = parsing . Decorator ( the class , d . listparam , s . parser tree ) else : raise Type Error ( "Unkown directive or decorator %s" % d . name ) return True
def ignore cxx ( self ) -> bool : self . stream . save context ( ) while not self . read eof ( ) : idxref = self . stream . index if self . stream . peek char in " \t\v\f\r\n" : while ( not self . read eof ( ) and self . stream . peek char in " \t\v\f\r\n" ) : self . stream . incpos ( ) if self . peek text ( "//" ) : while not self . read eof ( ) and not self . peek char ( "\n" ) : self . stream . incpos ( ) if not self . read char ( "\n" ) and self . read eof ( ) : return self . stream . validate context ( ) if self . peek text ( "/*" ) : while not self . read eof ( ) and not self . peek text ( "*/" ) : self . stream . incpos ( ) if not self . read text ( "*/" ) and self . read eof ( ) : return self . stream . restore context ( ) if idxref == self . stream . index : break return self . stream . validate context ( )
def add state ( self , s : State ) : ids = id ( s ) uid = len ( self . states ) if ids not in self . states : self . states [ ids ] = ( uid , s )
def to dot ( self ) -> str : txt = "" txt += "digraph S%d {\n" % id ( self ) if self . label is not None : txt += '\tlabel="%s";\n' % ( self . label + '\l' ) . replace ( '\n' , '\l' ) txt += "\trankdir=LR;\n" #txt += '\tlabelloc="t";\n' txt += '\tgraph [labeljust=l, labelloc=t, nojustify=true];\n' txt += "\tesep=1;\n" txt += '\tranksep="equally";\n' txt += "\tnode [shape = circle];\n" txt += "\tsplines = ortho;\n" for s in self . states . values ( ) : txt += s [ 1 ] . to dot ( ) txt += "}\n" return txt
def to dot file ( self , fname : str ) : with open ( fname , 'w' ) as f : f . write ( self . to dot ( ) )
def to png file ( self , fname : str ) : cmd = pipes . Template ( ) cmd . append ( 'dot -Tpng > %s' % fname , '-.' ) with cmd . open ( 'pipefile' , 'w' ) as f : f . write ( self . to dot ( ) )
def to fmt ( self ) -> str : infos = fmt . end ( ";\n" , [ ] ) s = fmt . sep ( ', ' , [ ] ) for ids in sorted ( self . states . keys ( ) ) : s . lsdata . append ( str ( ids ) ) infos . lsdata . append ( fmt . block ( '(' , ')' , [ s ] ) ) infos . lsdata . append ( "events:" + repr ( self . events ) ) infos . lsdata . append ( "named events:" + repr ( list ( self . named events . keys ( ) ) ) ) infos . lsdata . append ( "uid events:" + repr ( list ( self . uid events . keys ( ) ) ) ) return infos
def nextstate ( self , newstate , treenode = None , user data = None ) : if newstate is None : return self if isinstance ( newstate , State ) and id ( newstate ) != id ( self ) : return newstate elif isinstance ( newstate , State Event ) : self . state register . named events [ newstate . name ] = True return newstate . st elif isinstance ( newstate , State Precond ) : return newstate . st elif isinstance ( newstate , State Hook ) : # final API using PSL newstate . call ( treenode , user data ) return newstate . st return self
def reset Living State ( self ) : # TODO: add some test to control number of instanciation of Living State # clean all living state on S0 must delete = [ ] l = len ( self . ls ) for idx , ls in zip ( range ( l ) , self . ls ) : # TODO: alive by default on False, change to True on the first match ids = id ( ls [ 1 ] . thestate ( ) ) if ids == id ( ls [ 0 ] ) and ( ls [ 1 ] . have finish or not ls [ 1 ] . alive ) : must delete . append ( idx ) elif ls [ 1 ] . alive : ls [ 1 ] . alive = False for delete in reversed ( must delete ) : self . ls . pop ( delete ) self . init all ( )
def infer block ( self , body , diagnostic = None ) : # Root Block Stmt has his own .infer node (created via infer type) for e in body : e . infer node = Infer Node ( parent = self . infer node ) e . infer type ( diagnostic = diagnostic )
def infer subexpr ( self , expr , diagnostic = None ) : expr . infer node = Infer Node ( parent = self . infer node ) expr . infer type ( diagnostic = diagnostic )
def list dataset uris ( cls , base uri , config path ) : uri list = [ ] parse result = generous parse uri ( base uri ) bucket name = parse result . netloc bucket = boto3 . resource ( 's3' ) . Bucket ( bucket name ) for obj in bucket . objects . filter ( Prefix = 'dtool' ) . all ( ) : uuid = obj . key . split ( '-' , 1 ) [ 1 ] uri = cls . generate uri ( None , uuid , base uri ) storage broker = cls ( uri , config path ) if storage broker . has admin metadata ( ) : uri list . append ( uri ) return uri list
def list overlay names ( self ) : bucket = self . s3resource . Bucket ( self . bucket ) overlay names = [ ] for obj in bucket . objects . filter ( Prefix = self . overlays key prefix ) . all ( ) : overlay file = obj . key . rsplit ( '/' , 1 ) [ - 1 ] overlay name , ext = overlay file . split ( '.' ) overlay names . append ( overlay name ) return overlay names
def iter item handles ( self ) : bucket = self . s3resource . Bucket ( self . bucket ) for obj in bucket . objects . filter ( Prefix = self . data key prefix ) . all ( ) : relpath = obj . get ( ) [ 'Metadata' ] [ 'handle' ] yield relpath
def list set indent ( lst : list , indent : int = 1 ) : for i in lst : if isinstance ( i , indentable ) : i . set indent ( indent ) if isinstance ( i , list ) : list set indent ( i , indent )
def list to str ( lst : list , content : str , indent : int = 1 ) : for i in lst : if isinstance ( i , indentable ) : content = i . to str ( content , indent ) elif isinstance ( i , list ) : content = list to str ( i , content , indent ) elif isinstance ( i , str ) : content = catend ( content , i , indent ) return content
def populate from sequence ( seq : list , r : ref ( Edge ) , sr : state . State Register ) : base state = r # we need to detect the last state of the sequence idxlast = len ( seq ) - 1 idx = 0 for m in seq : # alternatives are represented by builtin list if isinstance ( m , list ) : # so recursively connect all states of each alternative sequences. for item in m : populate from sequence ( item , r , sr ) elif isinstance ( m , Match Expr ) : # from the current state, have we a existing edge for this event? e X = r ( ) . get next edge ( m ) if e X is None : s X = None if idx != idxlast : s X = state . State ( sr ) s X . match Default ( base state ( ) . s ) else : # last state of sequence return to the base s X = base state ( ) . s e X = Edge ( s X ) r ( ) . next edge [ id ( s X ) ] = e X m . attach ( r ( ) . s , s X , sr ) r = ref ( e X ) idx += 1
def from string ( bnf : str , entry = None , * optional inherit ) -> Grammar : inherit = [ Grammar ] + list ( optional inherit ) scope = { 'grammar' : bnf , 'entry' : entry } return build grammar ( tuple ( inherit ) , scope )
def from file ( fn : str , entry = None , * optional inherit ) -> Grammar : import os . path if os . path . exists ( fn ) : f = open ( fn , 'r' ) bnf = f . read ( ) f . close ( ) inherit = [ Grammar ] + list ( optional inherit ) scope = { 'grammar' : bnf , 'entry' : entry , 'source' : fn } return build grammar ( tuple ( inherit ) , scope ) raise Exception ( "File not Found!" )
def parse ( self , source : str = None , entry : str = None ) -> parsing . Node : self . from string = True if source is not None : self . parsed stream ( source ) if entry is None : entry = self . entry if entry is None : raise Value Error ( "No entry rule name defined for {}" . format ( self . class . name ) ) return self . do parse ( entry )
def parse file ( self , filename : str , entry : str = None ) -> parsing . Node : self . from string = False import os . path with open ( filename , 'r' ) as f : self . parsed stream ( f . read ( ) , os . path . abspath ( filename ) ) if entry is None : entry = self . entry if entry is None : raise Value Error ( "No entry rule name defined for {}" . format ( self . class . name ) ) return self . do parse ( entry )
def default serializer ( o ) : defs = ( ( ( datetime . date , datetime . time ) , lambda x : x . isoformat ( ) , ) , ( ( datetime . datetime , ) , lambda x : dt2utc timestamp ( x ) , ) , ) for types , fun in defs : if isinstance ( o , types ) : return fun ( o )
def dump ( deposition , from date , with json = True , latest only = False , * * kwargs ) : # Serialize the  getstate  and fall back to default serializer dep json = json . dumps ( deposition . getstate ( ) , default = default serializer ) dep dict = json . loads ( dep json ) dep dict [ ' p' ] = { } dep dict [ ' p' ] [ 'id' ] = deposition . id dep dict [ ' p' ] [ 'created' ] = dt2utc timestamp ( deposition . created ) dep dict [ ' p' ] [ 'modified' ] = dt2utc timestamp ( deposition . modified ) dep dict [ ' p' ] [ 'user id' ] = deposition . user id dep dict [ ' p' ] [ 'state' ] = deposition . state dep dict [ ' p' ] [ 'has sip' ] = deposition . has sip ( ) dep dict [ ' p' ] [ 'submitted' ] = deposition . submitted return dep dict
def get recids invenio12 ( from date ) : from invenio . dbquery import run sql return ( id [ 0 ] for id in run sql ( 'select id bibrec from ' 'bibrec bibdoc as r join bibdoc as d on r.id bibdoc=d.id ' 'where d.modification date >=%s' , ( from date , ) , run on slave = True ) )
def get recids invenio2 ( from date ) : from invenio . legacy . dbquery import run sql return ( id [ 0 ] for id in run sql ( 'select id bibrec from ' 'bibrec bibdoc as r join bibdoc as d on r.id bibdoc=d.id ' 'where d.modification date >=%s' , ( from date , ) , run on slave = True ) )
def get check ( ) : try : from invenio . dbquery import run sql except Import Error : from invenio . legacy . dbquery import run sql return ( run sql ( 'select count(id) from bibdoc' , run on slave = True ) [ 0 ] [ 0 ] , [ id [ 0 ] for id in run sql ( 'select id from bibdoc' , run on slave = True ) ] , )
def dump ( obj , from date , with json = True , latest only = False , * * kwargs ) : return dict ( id = obj . id , client id = obj . client id , user id = obj . user id , token type = obj . token type , access token = obj . access token , refresh token = obj . refresh token , expires = dt2iso or empty ( obj . expires ) , scopes = obj . scopes , is personal = obj . is personal , is internal = obj . is internal )
def get ( * args , * * kwargs ) : try : from invenio . modules . accounts . models import User EXT except Import Error : from invenio accounts . models import User EXT q = User EXT . query return q . count ( ) , q . all ( )
def get modified recids invenio12 ( from date ) : from invenio . search engine import search pattern from invenio . dbquery import run sql return set ( ( id [ 0 ] for id in run sql ( 'select id from bibrec where modification date >= %s' , ( from date , ) , run on slave = True ) ) ) , search pattern
def get modified recids invenio2 ( from date ) : from invenio . legacy . search engine import search pattern from invenio . modules . records . models import Record date = datetime . datetime . strptime ( from date , '%Y-%m-%d %H:%M:%S' ) return set ( ( x [ 0 ] for x in Record . query . filter ( Record . modification date >= date ) . values ( Record . id ) ) ) , search pattern
def get collection restrictions ( collection ) : try : from invenio . dbquery import run sql from invenio . access control firerole import compile role definition except Import Error : from invenio . modules . access . firerole import compile role definition from invenio . legacy . dbquery import run sql res = run sql ( 'SELECT r.firerole def src, email ' 'FROM acc ROLE as r ' 'JOIN acc ROLE acc ACTION acc ARGUMENT ON r.id=id acc ROLE ' 'JOIN acc ARGUMENT AS a ON a.id=id acc ARGUMENT ' 'JOIN user acc ROLE AS u ON r.id=u.id acc ROLE ' 'JOIN user ON user.id=u.id user ' 'WHERE a.keyword="collection" AND ' 'a.value=%s AND ' 'id acc ACTION=(select id from acc ACTION where name="viewrestrcoll")' , ( collection , ) , run on slave = True ) fireroles = set ( ) users = set ( ) for f , u in res : fireroles . add ( compile role definition ( f ) ) users . add ( u ) return { 'fireroles' : list ( fireroles ) , 'users' : users }
def get record revisions ( recid , from date ) : try : from invenio . dbquery import run sql except Import Error : from invenio . legacy . dbquery import run sql return run sql ( 'SELECT job date, marcxml ' 'FROM hst RECORD WHERE id bibrec = %s AND job date >= %s ' 'ORDER BY job date ASC' , ( recid , from date ) , run on slave = True )
def get record collections ( recid ) : try : from invenio . search engine import ( get all collections of a record , get restricted collections for recid ) except Import Error : from invenio . legacy . search engine import ( get all collections of a record , get restricted collections for recid ) collections = { 'all' : get all collections of a record ( recid , recreate cache if needed = False ) , } collections [ 'restricted' ] = dict ( ( coll , get collection restrictions ( coll ) ) for coll in get restricted collections for recid ( recid , recreate cache if needed = False ) ) return collections
def dump record json ( marcxml ) : try : from invenio . modules . records . api import Record d = Record . create ( marcxml , 'marc' ) return d . dumps ( clean = True ) except Import Error : from invenio . bibfield import create record d = create record ( marcxml , master format = 'marc' ) return d . dumps ( )
def get ( query , from date , * * kwargs ) : recids , search pattern = get modified recids ( from date ) recids = recids . union ( get modified bibdoc recids ( from date ) ) if query : recids = recids . intersection ( set ( search pattern ( p = query . encode ( 'utf-8' ) ) ) ) return len ( recids ) , recids
def load common ( model cls , data ) : obj = model cls ( * * data ) db . session . add ( obj ) db . session . commit ( )
def collect things entry points ( ) : things = dict ( ) for entry point in iter entry points ( group = 'invenio migrator.things' ) : things [ entry point . name ] = entry point . load ( ) return things
def init app context ( ) : try : from invenio . base . factory import create app app = create app ( ) app . test request context ( '/' ) . push ( ) app . preprocess request ( ) except Import Error : pass
def memoize ( func ) : cache = { } @ wraps ( func ) def wrap ( * args , * * kwargs ) : key = '{0}{1}' . format ( args , kwargs ) if key not in cache : cache [ key ] = func ( * args , * * kwargs ) return cache [ key ] return wrap
def get connected roles ( action id ) : try : from invenio . access control admin import compile role definition except Import Error : from invenio . modules . access . firerole import compile role definition run sql = get run sql ( ) roles = { } res = run sql ( 'select r.id, r.name, r.description, r.firerole def src, ' 'a.keyword, a.value, email from acc ROLE as r ' 'join acc ROLE acc ACTION acc ARGUMENT on r.id=id acc ROLE ' 'join acc ARGUMENT as a on  a.id=id acc ARGUMENT ' 'join user acc ROLE as u on r.id=u.id acc ROLE ' 'join user on user.id=u.id user ' 'where id acc ACTION=%s' , ( action id , ) ) for r in res : role = roles . setdefault ( r [ 0 ] , { 'id' : r [ 0 ] , 'name' : r [ 1 ] , 'description' : r [ 2 ] , 'firerole def' : r [ 3 ] , 'compiled firerole def' : compile role definition ( r [ 3 ] ) , 'users' : set ( ) , 'parameters' : { } } ) param = role [ 'parameters' ] . setdefault ( r [ 4 ] , set ( ) ) param . add ( r [ 5 ] ) role [ 'users' ] . add ( r [ 6 ] ) return six . itervalues ( roles )
def get ( query , * args , * * kwargs ) : run sql = get run sql ( ) actions = [ dict ( id = row [ 0 ] , name = row [ 1 ] , allowedkeywords = row [ 2 ] , optional = row [ 3 ] ) for action in query . split ( ',' ) for row in run sql ( 'select id, name, description, allowedkeywords, optional ' 'from acc ACTION where name like %s' , ( action , ) , run on slave = True ) ] return len ( actions ) , actions
def load token ( data ) : from invenio oauth2server . models import Token data [ 'expires' ] = iso2dt or none ( data [ 'expires' ] ) load common ( Token , data )
def config imp or default ( app , config var imp , default ) : imp = app . config . get ( config var imp ) return import string ( imp ) if imp else default
def init app ( self , app ) : self . init config ( app . config ) state = Invenio Migrator State ( app ) app . extensions [ 'invenio-migrator' ] = state app . cli . add command ( dumps ) return state
def dump ( obj , from date , with json = True , latest only = False , * * kwargs ) : return dict ( name = obj . name , description = obj . description , website = obj . website , user id = obj . user id , client id = obj . client id , client secret = obj . client secret , is confidential = obj . is confidential , is internal = obj . is internal , redirect uris = obj . redirect uris , default scopes = obj . default scopes )
def get users invenio12 ( * args , * * kwargs ) : from invenio . dbquery import run sql , deserialize via marshal User = namedtuple ( 'User' , [ 'id' , 'email' , 'password' , 'password salt' , 'note' , 'full name' , 'settings' , 'nickname' , 'last login' ] ) users = run sql ( 'SELECT id, email, password, note, settings, nickname, last login' ' FROM user' , run on slave = True ) return len ( users ) , [ User ( id = user [ 0 ] , email = user [ 1 ] , password = user [ 2 ] . decode ( 'latin1' ) , password salt = user [ 1 ] , note = user [ 3 ] , full name = user [ 5 ] , settings = deserialize via marshal ( user [ 4 ] ) if user [ 4 ] else { } , # we don't have proper nicknames on Invenio v1 nickname = 'id {0}' . format ( user [ 0 ] ) , last login = user [ 6 ] ) for user in users ]
def get users invenio2 ( * args , * * kwargs ) : from invenio . modules . accounts . models import User q = User . query return q . count ( ) , q . all ( )
def create files and sip ( deposit , dep pid ) : from invenio pidstore . errors import PID Does Not Exist Error from invenio pidstore . models import Persistent Identifier , PID Status , Record Identifier from invenio sipstore . errors import SIP User Does Not Exist from invenio sipstore . models import SIP , Record SIP , SIP File from invenio files rest . models import Bucket , File Instance , Object Version from invenio records files . models import Records Buckets from invenio db import db buc = Bucket . create ( ) recbuc = Records Buckets ( record id = deposit . id , bucket id = buc . id ) db . session . add ( recbuc ) deposit . setdefault ( ' deposit' , dict ( ) ) deposit . setdefault ( ' buckets' , dict ( deposit = str ( buc . id ) ) ) deposit . setdefault ( ' files' , list ( ) ) files = deposit . get ( 'files' , [ ] ) sips = deposit . get ( 'sips' , [ ] ) # Look for prereserved DOI (and recid) if 'drafts' in deposit : drafts = list ( deposit [ 'drafts' ] . items ( ) ) if len ( drafts ) != 1 : logger . exception ( 'Deposit {dep pid} has multiple drafts' . format ( dep pid = dep pid ) ) if len ( drafts ) == 1 : draft type , draft = drafts [ 0 ] draft v = draft [ 'values' ] if 'prereserve doi' in draft v : pre recid = str ( draft v [ 'prereserve doi' ] [ 'recid' ] ) pre doi = str ( draft v [ 'prereserve doi' ] [ 'doi' ] ) # If pre-reserve info available, try to reserve 'recid' try : pid = Persistent Identifier . get ( pid type = 'recid' , pid value = str ( pre recid ) ) except PID Does Not Exist Error : # Reserve recid pid = Persistent Identifier . create ( pid type = 'recid' , pid value = str ( pre recid ) , object type = 'rec' , status = PID Status . RESERVED ) # If pre-reserve info available, try to reserve 'doi' try : pid = Persistent Identifier . get ( pid type = 'doi' , pid value = str ( pre doi ) ) except PID Does Not Exist Error : # Reserve DOI pid = Persistent Identifier . create ( pid type = 'doi' , pid value = str ( pre doi ) , object type = 'rec' , status = PID Status . RESERVED ) if Record Identifier . query . get ( int ( pre recid ) ) is None : Record Identifier . insert ( int ( pre recid ) ) # Store the path -> File Instance mappings for SIP File creation later dep file instances = list ( ) for file in files : size = file [ 'size' ] key = file [ 'name' ] # Warning: Assumes all checksums are MD5! checksum = 'md5:{0}' . format ( file [ 'checksum' ] ) fi = File Instance . create ( ) fi . set uri ( file [ 'path' ] , size , checksum ) ov = Object Version . create ( buc , key , file id = fi . id ) ext = splitext ( ov . key ) [ 1 ] . lower ( ) if ext . startswith ( '.' ) : ext = ext [ 1 : ] file meta = dict ( bucket = str ( ov . bucket . id ) , key = ov . key , checksum = ov . file . checksum , size = ov . file . size , version id = str ( ov . version id ) , type = ext , ) deposit [ ' files' ] . append ( file meta ) dep file instances . append ( ( file [ 'path' ] , fi ) ) # Get a recid from SIP information recid = None if sips : recids = [ int ( sip [ 'metadata' ] [ 'recid' ] ) for sip in sips ] if len ( set ( recids ) ) > 1 : logger . error ( 'Multiple recids ({recids}) found in deposit {depid}' ' does not exists.' . format ( recids = recids , depid = dep pid . pid value ) ) raise Deposit Multiple Recids ( dep pid . pid value , list ( set ( recids ) ) ) elif recids : # If only one recid recid = recids [ 0 ] for idx , sip in enumerate ( sips ) : agent = None user id = None if sip [ 'agents' ] : agent = dict ( ip address = empty str if none ( sip [ 'agents' ] [ 0 ] . get ( 'ip address' , "" ) ) , email = empty str if none ( sip [ 'agents' ] [ 0 ] . get ( 'email address' , "" ) ) , ) user id = sip [ 'agents' ] [ 0 ] [ 'user id' ] if user id == 0 : user id = None content = sip [ 'package' ] sip format = 'marcxml' try : sip = SIP . create ( sip format , content , user id = user id , agent = agent ) except SIP User Does Not Exist : logger . exception ( 'User ID {user id} referred in deposit {depid} ' 'does not exists.' . format ( user id = user id , depid = dep pid . pid value ) ) sip = SIP . create ( sip format , content , agent = agent ) # Attach recid to SIP if recid : try : pid = Persistent Identifier . get ( pid type = 'recid' , pid value = str ( recid ) ) record sip = Record SIP ( sip id = sip . id , pid id = pid . id ) db . session . add ( record sip ) except PID Does Not Exist Error : logger . exception ( 'Record {recid} referred in ' 'Deposit {depid} does not exists.' . format ( recid = recid , depid = dep pid . pid value ) ) if deposit [ ' p' ] [ 'submitted' ] is True : logger . exception ( 'Pair {recid}/{depid} was submitted,' ' (should it be unpublished?).' . format ( recid = recid , depid = dep pid . pid value ) ) else : msg = 'Pair {recid}/{depid} was not submitted.' . format ( recid = recid , depid = dep pid . pid value ) logger . exception ( msg ) # Reserve recid pid = Persistent Identifier . create ( pid type = 'recid' , pid value = str ( recid ) , object type = 'rec' , status = PID Status . RESERVED ) if Record Identifier . query . get ( int ( recid ) ) is None : Record Identifier . insert ( int ( recid ) ) if idx == 0 : for fp , fi in dep file instances : sipf = SIP File ( sip id = sip . id , filepath = fp , file id = fi . id ) db . session . add ( sipf ) deposit . commit ( ) return deposit
def loadrecords ( sources , source type , recid ) : # Load all record dumps up-front and find the specific JSON if recid is not None : for source in sources : records = json . load ( source ) for item in records : if str ( item [ 'recid' ] ) == str ( recid ) : loadrecord ( item , source type , eager = True ) click . echo ( "Record '{recid}' loaded." . format ( recid = recid ) ) return click . echo ( "Record '{recid}' not found." . format ( recid = recid ) ) else : for idx , source in enumerate ( sources , 1 ) : click . echo ( 'Loading dump {0} of {1} ({2})' . format ( idx , len ( sources ) , source . name ) ) data = json . load ( source ) with click . progressbar ( data ) as records : for item in records : loadrecord ( item , source type )
def inspectrecords ( sources , recid , entity = None ) : for idx , source in enumerate ( sources , 1 ) : click . echo ( 'Loading dump {0} of {1} ({2})' . format ( idx , len ( sources ) , source . name ) ) data = json . load ( source ) # Just print record identifiers if none are selected. if not recid : click . secho ( 'Record identifiers' , fg = 'green' ) total = 0 for r in ( d [ 'recid' ] for d in data ) : click . echo ( r ) total += 1 click . echo ( '{0} records found in dump.' . format ( total ) ) return data = list ( filter ( lambda d : d [ 'recid' ] == recid , data ) ) if not data : click . secho ( "Record not found." , fg = 'yellow' ) return for record in data : if entity is None : click . echo ( json . dumps ( record , indent = 2 ) ) if entity == 'files' : click . secho ( 'Files' , fg = 'green' ) click . echo ( json . dumps ( record [ 'files' ] , indent = 2 ) ) if entity == 'json' : click . secho ( 'Records (JSON)' , fg = 'green' ) for revision in record [ 'record' ] : click . secho ( 'Revision {0}' . format ( revision [ 'modification datetime' ] ) , fg = 'yellow' ) click . echo ( json . dumps ( revision [ 'json' ] , indent = 2 ) ) if entity == 'marcxml' : click . secho ( 'Records (MARCXML)' , fg = 'green' ) for revision in record [ 'record' ] : click . secho ( 'Revision {0}' . format ( revision [ 'marcxml' ] ) , fg = 'yellow' ) click . echo ( revision )
def main ( port = 8888 ) : import tornado . ioloop routes = [ ] + Tornado Profiler ( ) . get routes ( ) app = tornado . web . Application ( routes ) app . listen ( port ) tornado . ioloop . IO Loop . current ( ) . start ( )
def get ( self ) : sort = self . get argument ( 'sort' , 'cum time' ) count = self . get argument ( 'count' , 20 ) strip dirs = self . get argument ( 'strip dirs' , True ) error = '' sorts = ( 'num calls' , 'cum time' , 'total time' , 'cum time per call' , 'total time per call' ) if sort not in sorts : error += "Invalid `sort` '%s', must be in %s." % ( sort , sorts ) try : count = int ( count ) except ( Value Error , Type Error ) : error += "Can't cast `count` '%s' to int." % count if count <= 0 : count = None strip dirs = str ( strip dirs ) . lower ( ) not in ( 'false' , 'no' , 'none' , 'null' , '0' , '' ) if error : self . write ( { 'error' : error } ) self . set status ( 400 ) self . finish ( ) return try : statistics = get profiler statistics ( sort , count , strip dirs ) self . write ( { 'statistics' : statistics } ) self . set status ( 200 ) except Type Error : logger . exception ( 'Error while retrieving profiler statistics' ) self . write ( { 'error' : 'No stats available. Start and stop the profiler before trying to retrieve stats.' } ) self . set status ( 404 ) self . finish ( )
def post ( self ) : if is profiler running ( ) : self . set status ( 201 ) self . finish ( ) return start profiling ( ) self . set status ( 201 ) self . finish ( )
def post ( self ) : filename = self . get argument ( 'filename' , 'dump.prof' ) C Profile Wrapper . profiler . dump stats ( filename ) self . finish ( )
def get ( self ) : C Profile Wrapper . profiler . print stats ( ) s = String IO . String IO ( ) sortby = 'cumulative' ps = pstats . Stats ( C Profile Wrapper . profiler , stream = s ) . sort stats ( sortby ) ps . print stats ( ) self . set status ( 200 ) self . write ( s . getvalue ( ) ) self . finish ( )
def delete ( self ) : C Profile Wrapper . profiler . create stats ( ) self . enable ( ) self . set status ( 204 ) self . finish ( )
def post ( self ) : C Profile Wrapper . profiler = c Profile . Profile ( ) C Profile Wrapper . profiler . enable ( ) self . running = True self . set status ( 201 ) self . finish ( )
def delete ( self ) : C Profile Wrapper . profiler . disable ( ) self . running = False self . set status ( 204 ) self . finish ( )
def get ( self ) : self . write ( { "running" : self . running } ) self . set status ( 200 ) self . finish ( )
def disable timestamp ( method ) : @ wraps ( method ) def wrapper ( * args , * * kwargs ) : result = None with correct date ( ) : result = method ( * args , * * kwargs ) return result return wrapper
def add ones dim ( arr ) : arr = arr [ ... , np . newaxis ] return np . concatenate ( ( arr , np . ones like ( arr ) ) , axis = - 1 )
def create ( cls , dump ) : # If 'record' is not present, just create the PID if not dump . data . get ( 'record' ) : try : Persistent Identifier . get ( pid type = 'recid' , pid value = dump . recid ) except PID Does Not Exist Error : Persistent Identifier . create ( 'recid' , dump . recid , status = PID Status . RESERVED ) db . session . commit ( ) return None dump . prepare revisions ( ) dump . prepare pids ( ) dump . prepare files ( ) # Create or update? existing files = [ ] if dump . record : existing files = dump . record . get ( ' files' , [ ] ) record = cls . update record ( revisions = dump . revisions , created = dump . created , record = dump . record ) pids = dump . missing pids else : record = cls . create record ( dump ) pids = dump . pids if pids : cls . create pids ( record . id , pids ) if dump . files : cls . create files ( record , dump . files , existing files ) # Update files. if dump . is deleted ( record ) : cls . delete record ( record ) return record
def create record ( cls , dump ) : # Reserve record identifier, create record and recid pid in one # operation. timestamp , data = dump . latest record = Record . create ( data ) record . model . created = dump . created . replace ( tzinfo = None ) record . model . updated = timestamp . replace ( tzinfo = None ) Record Identifier . insert ( dump . recid ) Persistent Identifier . create ( pid type = 'recid' , pid value = str ( dump . recid ) , object type = 'rec' , object uuid = str ( record . id ) , status = PID Status . REGISTERED ) db . session . commit ( ) return cls . update record ( revisions = dump . rest , record = record , created = dump . created )
def update record ( cls , revisions , created , record ) : for timestamp , revision in revisions : record . model . json = revision record . model . created = created . replace ( tzinfo = None ) record . model . updated = timestamp . replace ( tzinfo = None ) db . session . commit ( ) return Record ( record . model . json , model = record . model )
def create pids ( cls , record uuid , pids ) : for p in pids : Persistent Identifier . create ( pid type = p . pid type , pid value = p . pid value , pid provider = p . provider . pid provider if p . provider else None , object type = 'rec' , object uuid = record uuid , status = PID Status . REGISTERED , ) db . session . commit ( )
def delete record ( cls , record ) : record . delete ( ) Persistent Identifier . query . filter by ( object type = 'rec' , object uuid = record . id , ) . update ( { Persistent Identifier . status : PID Status . DELETED } ) cls . delete buckets ( record ) db . session . commit ( )
def create file ( self , bucket , key , file versions ) : objs = [ ] for file ver in file versions : f = File Instance . create ( ) . set uri ( file ver [ 'full path' ] , file ver [ 'size' ] , 'md5:{0}' . format ( file ver [ 'checksum' ] ) , ) obj = Object Version . create ( bucket , key ) . set file ( f ) obj . created = arrow . get ( file ver [ 'creation date' ] ) . datetime . replace ( tzinfo = None ) objs . append ( obj ) # Set head version db . session . commit ( ) return objs [ - 1 ]
def delete buckets ( cls , record ) : files = record . get ( ' files' , [ ] ) buckets = set ( ) for f in files : buckets . add ( f . get ( 'bucket' ) ) for b id in buckets : b = Bucket . get ( b id ) b . deleted = True
def missing pids ( self ) : missing = [ ] for p in self . pids : try : Persistent Identifier . get ( p . pid type , p . pid value ) except PID Does Not Exist Error : missing . append ( p ) return missing
def prepare files ( self ) : # Prepare files files = { } for f in self . data [ 'files' ] : k = f [ 'full name' ] if k not in files : files [ k ] = [ ] files [ k ] . append ( f ) # Sort versions for k in files . keys ( ) : files [ k ] . sort ( key = lambda x : x [ 'version' ] ) self . files = files
def prepare pids ( self ) : self . pids = [ ] for fetcher in self . pid fetchers : val = fetcher ( None , self . revisions [ - 1 ] [ 1 ] ) if val : self . pids . append ( val )
def is deleted ( self , record = None ) : record = record or self . revisions [ - 1 ] [ 1 ] return any ( col == 'deleted' for col in record . get ( 'collections' , [ ] ) )
def dump ( thing , query , from date , file prefix , chunk size , limit , thing flags ) : init app context ( ) file prefix = file prefix if file prefix else '{0} dump' . format ( thing ) kwargs = dict ( ( f . strip ( '-' ) . replace ( '-' , ' ' ) , True ) for f in thing flags ) try : thing func = collect things entry points ( ) [ thing ] except Key Error : click . Abort ( '{0} is not in the list of available things to migrate: ' '{1}' . format ( thing , collect things entry points ( ) ) ) click . echo ( "Querying {0}..." . format ( thing ) ) count , items = thing func . get ( query , from date , limit = limit , * * kwargs ) progress i = 0 # Progress bar counter click . echo ( "Dumping {0}..." . format ( thing ) ) with click . progressbar ( length = count ) as bar : for i , chunk ids in enumerate ( grouper ( items , chunk size ) ) : with open ( '{0} {1}.json' . format ( file prefix , i ) , 'w' ) as fp : fp . write ( "[\n" ) for id in chunk ids : try : json . dump ( thing func . dump ( id , from date , * * kwargs ) , fp , default = set serializer ) fp . write ( "," ) except Exception as e : click . secho ( "Failed dump {0} {1} ({2})" . format ( thing , id , e . message ) , fg = 'red' ) progress i += 1 bar . update ( progress i ) # Strip trailing comma. fp . seek ( fp . tell ( ) - 1 ) fp . write ( "\n]" )
def check ( thing ) : init app context ( ) try : thing func = collect things entry points ( ) [ thing ] except Key Error : click . Abort ( '{0} is not in the list of available things to migrate: ' '{1}' . format ( thing , collect things entry points ( ) ) ) click . echo ( "Querying {0}..." . format ( thing ) ) count , items = thing func . get check ( ) i = 0 click . echo ( "Checking {0}..." . format ( thing ) ) with click . progressbar ( length = count ) as bar : for id in items : thing func . check ( id ) i += 1 bar . update ( i )
def write reports ( self , relative path , suite name , reports , package name = None ) : dest path = self . reserve file ( relative path ) with open ( dest path , 'wb' ) as outf : outf . write ( toxml ( reports , suite name , package name = package name ) ) return dest path
def toxml ( test reports , suite name , hostname = gethostname ( ) , package name = "tests" ) : testsuites = et . Element ( "testsuites" ) testsuite = et . Sub Element ( testsuites , "testsuite" ) test count = len ( test reports ) if test count < 1 : raise Value Error ( 'there must be at least one test report' ) assert test count > 0 , 'expecting at least one test' error count = len ( [ r for r in test reports if r . errors ] ) failure count = len ( [ r for r in test reports if r . failures ] ) ts = test reports [ 0 ] . start ts start timestamp = datetime . fromtimestamp ( ts ) . isoformat ( ) total duration = test reports [ - 1 ] . end ts - test reports [ 0 ] . start ts def quote attribute ( value ) : return value if value is not None else "(null)" testsuite . attrib = dict ( id = "0" , errors = str ( error count ) , failures = str ( failure count ) , tests = str ( test count ) , hostname = quote attribute ( hostname ) , timestamp = quote attribute ( start timestamp ) , time = "%f" % total duration , name = quote attribute ( suite name ) , package = quote attribute ( package name ) , ) for r in test reports : test name = r . name test duration = r . end ts - r . start ts class name = r . src location testcase = et . Sub Element ( testsuite , "testcase" ) testcase . attrib = dict ( name = test name , classname = quote attribute ( class name ) , time = "%f" % test duration , ) if r . errors or r . failures : if r . failures : failure = et . Sub Element ( testcase , "failure" ) failure . attrib = dict ( type = "exception" , message = quote attribute ( '\n' . join ( [ '%s' % e for e in r . failures ] ) ) , ) else : error = et . Sub Element ( testcase , "error" ) error . attrib = dict ( type = "exception" , message = quote attribute ( '\n' . join ( [ '%s' % e for e in r . errors ] ) ) , ) return et . tostring ( testsuites , encoding = "utf-8" )
def add Menu ( self , menu ) : # If there is no menu selected currently, this menu will automatically be made active. # Add the line above to the docstring if fixed self . menus [ menu . name ] = menu self . peng . send Event ( "peng3d:window.menu.add" , { "peng" : self . peng , "window" : self , "menu" : menu } )
def redraw label ( self ) : # Convenience variables sx , sy = self . size x , y = self . pos # Label position self . label . anchor x = "left" self . label . x = x + sx / 2. + sx self . label . y = y + sy / 2. + sy * .15 self . label . update ( )
def render3d ( self , view = None ) : super ( Static World , self ) . render3d ( view ) self . batch3d . draw ( )
def on redraw ( self ) : x , y = self . pos sx , sy = self . size self . bg vlist . vertices = [ x , y , x + sx , y , x + sx , y + sy , x , y + sy ] self . stencil vlist . vertices = [ x , y , x + sx , y , x + sx , y + sy , x , y + sy ] if isinstance ( self . bg , Background ) : if not self . bg . initialized : self . bg . init bg ( ) self . bg . initialized = True self . bg . redraw bg ( )
def do Action ( self , action ) : if not hasattr ( self , "actions" ) : return for f , args , kwargs in self . actions . get ( action , [ ] ) : f ( * args , * * kwargs )
def get Size ( self ) : return self . widget . size [ 0 ] - self . border [ 0 ] * 2 , self . widget . size [ 1 ] - self . border [ 1 ] * 2
def read inp ( path ) : def line Info ( line ) : out = { "type" : "data" } if line [ 0 ] == "*" : if line [ 1 ] == "*" : out [ "type" ] = "comment" out [ "text" ] = line [ 2 : ] else : out [ "type" ] = "command" words = line [ 1 : ] . split ( "," ) out [ "value" ] = words [ 0 ] . strip ( ) out [ "options" ] = { } for word in words [ 1 : ] : key , value = [ s . strip ( ) for s in word . split ( "=" ) ] out [ "options" ] [ key ] = value return out def element Mapper ( inpeltype ) : if inpeltype == "t3d2" : return "Line2" if inpeltype [ : 3 ] in [ "cps" , "cpe" , "cax" ] : if inpeltype [ 3 ] == "3" : return "tri3" if inpeltype [ 3 ] == "4" : return "quad4" if inpeltype [ : 3 ] in [ "c3d" ] : if inpeltype [ 3 ] == "4" : return "tetra4" if inpeltype [ 3 ] == "5" : return "pyra5" if inpeltype [ 3 ] == "6" : return "prism6" if inpeltype [ 3 ] == "8" : return "hexa8" nlabels = [ ] coords = [ ] nsets = { } elabels = [ ] etypes = [ ] connectivity = [ ] esets = { } surfaces = { } # File preprocessing lines = np . array ( [ l . strip ( ) . lower ( ) for l in open ( path ) . readlines ( ) ] ) lines = [ line for line in lines if len ( line ) != 0 ] # Data processing env , setlabel = None , None for line in lines : d = line Info ( line ) if d [ "type" ] == "command" : env = d [ "value" ] # Nodes if env == "node" : opt = d [ "options" ] currentset = None if "nset" in opt . keys ( ) : currentset = opt [ "nset" ] nsets [ currentset ] = [ ] # Elements if env == "element" : opt = d [ "options" ] eltype = element Mapper ( opt [ "type" ] ) currentset = None if "elset" in opt . keys ( ) : currentset = opt [ "elset" ] esets [ currentset ] = [ ] # Nsets if env == "nset" : opt = d [ "options" ] currentset = opt [ "nset" ] nsets [ currentset ] = [ ] # Elsets      if env == "elset" : opt = d [ "options" ] currentset = opt [ "elset" ] esets [ currentset ] = [ ] # Surfaces if env == "surface" : opt = d [ "options" ] currentsurface = opt [ "name" ] if opt [ "type" ] == "element" : surfaces [ currentsurface ] = [ ] if d [ "type" ] == "data" : words = line . strip ( ) . split ( "," ) if env == "node" : label = int ( words [ 0 ] ) nlabels . append ( label ) coords . append ( np . array ( [ np . float64 ( w ) for w in words [ 1 : 4 ] ] ) ) if currentset != None : nsets [ currentset ] . append ( label ) if env == "element" : label = int ( words [ 0 ] ) elabels . append ( label ) connectivity . append ( np . array ( [ np . int32 ( w ) for w in words [ 1 : ] if len ( w ) != 0 ] ) ) etypes . append ( eltype ) if currentset != None : esets [ currentset ] . append ( label ) if env == "nset" : nsets [ currentset ] += [ int ( w ) for w in words if len ( w ) != 0 ] if env == "elset" : esets [ currentset ] += [ int ( w ) for w in words if len ( w ) != 0 ] if env == "surface" : if opt [ "type" ] == "element" : surfaces [ currentsurface ] . append ( [ w . strip ( ) for w in words ] ) surfaces2 = { } for tag , surface in surfaces . items ( ) : surfaces2 [ tag ] = [ ] for sdata in surface : labels = esets [ sdata [ 0 ] ] face = int ( sdata [ 1 ] . split ( "s" ) [ 1 ] . strip ( ) ) - 1 for label in labels : surfaces2 [ tag ] . append ( ( label , face ) ) return Mesh ( nlabels = nlabels , coords = coords , nsets = nsets , elabels = elabels , etypes = etypes , connectivity = connectivity , esets = esets , )
def write xdmf ( mesh , path , dataformat = "XML" ) : pattern = Template ( open ( MODPATH + "/templates/mesh/xdmf.xdmf" ) . read ( ) ) attribute pattern = Template ( open ( MODPATH + "/templates/mesh/xdmf attribute.xdmf" ) . read ( ) ) # MAPPINGS cell map = { "tri3" : 4 , "quad4" : 5 , "tetra4" : 6 , "pyra5" : 7 , "prism6" : 8 , "hexa8" : 9 } # REFERENCES nodes , elements = mesh . nodes . data , mesh . elements . data fields = mesh . fields # NUMBERS Ne , Nn = len ( elements ) , len ( nodes ) # NODES nodes map = np . arange ( nodes . index . max ( ) + 1 ) nodes map [ nodes . index ] = np . arange ( len ( nodes . index ) ) nodes map [ 0 ] = - 1 # ELEMENTS cols = [ "n{0}" . format ( i ) for i in range ( elements . shape [ 1 ] - 1 ) ] connectivities = mesh . elements . data [ cols ] . as matrix ( ) connectivities [ np . isnan ( connectivities ) ] = 0 connectivities = connectivities . astype ( np . int32 ) connectivities = nodes map [ connectivities ] labels = np . array ( elements . index ) etypes = np . array ( [ cell map [ t ] for t in elements . etype ] ) lconn = Ne + ( connectivities != - 1 ) . sum ( ) # FIELDS fields string = "" field data = { } for tag , field in fields . items ( ) : field data [ tag ] = { } field . data . sort index ( inplace = True ) fshape = field . data . shape [ 1 ] if fshape == 1 : ftype = "Scalar" elif fshape == 3 : ftype = "Vector" elif fshape == 2 : ftype = "Vector" # UGLY HACK... field = copy . copy ( field ) field . data [ "v3" ] = np . zeros like ( field . data . index ) fields [ tag ] = field # BACK TO NORMAL   elif fshape == 6 : ftype = "Tensor6" elif fshape == 4 : ftype = "Tensor6" # UGLY HACK... field = copy . copy ( field ) field . data [ "v13" ] = np . zeros like ( field . data . index ) field . data [ "v23" ] = np . zeros like ( field . data . index ) fields [ tag ] = field # BACK TO NORMAL   if field . metadata . position == "Nodal" : position = "Node" if field . metadata . position == "Element" : position = "Cell" field data [ tag ] [ "TAG" ] = tag field data [ tag ] [ "ATTRIBUTETYPE" ] = ftype field data [ tag ] [ "FORMAT" ] = dataformat field data [ tag ] [ "FIELD DIMENSION" ] = " " . join ( [ str ( l ) for l in field . data . shape ] ) field data [ tag ] [ "POSITION" ] = position if dataformat == "XML" : #NODES nodes string = "\n" . join ( [ 11 * " " + "{0} {1} {2}" . format ( n . x , n . y , n . z ) for i , n in nodes . iterrows ( ) ] ) # ELEMENTS elements string = "" for i in range ( Ne ) : elements string += 11 * " " + str ( etypes [ i ] ) + " " c = connectivities [ i ] c = c [ np . where ( c != - 1 ) ] elements string += " " . join ( [ str ( i ) for i in c ] ) + "\n" elements strings = elements string [ : - 1 ] # FIELDS for tag , field in fields . items ( ) : fdata = field . data . to csv ( sep = " " , index = False , header = False ) . split ( "\n" ) fdata = [ 11 * " " + l for l in fdata ] fdata = "\n" . join ( fdata ) field data [ tag ] [ "DATA" ] = fdata fields string += attribute pattern . substitute ( * * field data [ tag ] ) elif dataformat == "HDF" : hdf = pd . HDF Store ( path + ".h5" ) hdf . put ( "COORDS" , mesh . nodes . data [ list ( "xyz" ) ] ) flatconn = np . zeros ( lconn , dtype = np . int32 ) pos = 0 for i in range ( Ne ) : c = connectivities [ i ] c = c [ np . where ( c != - 1 ) ] lc = len ( c ) flatconn [ pos ] = etypes [ i ] flatconn [ pos + 1 + np . arange ( lc ) ] = c pos += 1 + lc hdf . put ( "CONNECTIVITY" , pd . Data Frame ( flatconn ) ) nodes string = 11 * " " + "{0}.h5:/COORDS/block0 values" . format ( path ) elements string = 11 * " " + "{0}.h5:/CONNECTIVITY/block0 values" . format ( path ) for tag , field in fields . items ( ) : fstrings [ tag ] = fstrings [ tag ] . replace ( "#DATA" , 11 * " " + "{0}.h5:/FIELDS/{1}/block0 values" . format ( path , tag ) ) fields string += fstrings [ tag ] hdf . put ( "FIELDS/{0}" . format ( tag ) , fields . data ) hdf . close ( ) fields string = "\n" . join ( [ attribute pattern . substitute ( * * value ) for key , value in field data . items ( ) ] ) pattern = pattern . substitute ( ELEMENT NUMBER = str ( Ne ) , CONN DIMENSION = str ( lconn ) , CONN PATH = elements string , NODE NUMBER = str ( Nn ) , NODE PATH = nodes string , DATAFORMAT = dataformat , ATTRIBUTES = fields string ) open ( path + ".xdmf" , "wb" ) . write ( pattern )
def write inp ( mesh , path = None , maxwidth = 40 , sections = "solid" ) : def set to inp ( sets , keyword ) : ss = "" for sk in sets . keys ( ) : labels = sets [ sk ] . loc [ sets [ sk ] ] . index . values labels = list ( labels ) labels . sort ( ) if len ( labels ) != 0 : ss += "*{0}, {0}={1}\n" . format ( keyword , sk ) ss += argiope . utils . list to string ( labels ) + "\n" return ss . strip ( ) # DATA mesh = mesh . copy ( ) # NODES nodes output = ( mesh . nodes . coords . to csv ( header = False ) . split ( ) ) nodes output = ( "\n" . join ( [ "  " + s . replace ( "," , ", " ) for s in nodes output ] ) ) # NODE SETS if "sets" in mesh . nodes . columns . levels [ 0 ] : nsets = set to inp ( mesh . nodes . sets , "NSET" ) else : nsets = "**" # SURFACES  surf output = [ ] if "surfaces" in mesh . elements . keys ( ) : sk = mesh . elements . surfaces . keys ( ) for sindex in np . unique ( sk . labels [ 0 ] ) : slabel = sk . levels [ 0 ] [ sindex ] surface = mesh . elements . surfaces [ slabel ] if surface . values . sum ( ) != 0 : mesh . surface to element sets ( slabel ) surf output . append ( "*SURFACE, TYPE=ELEMENT, NAME={0}" . format ( slabel ) ) for findex in surface . keys ( ) : if surface [ findex ] . sum ( ) != 0 : surf output . append ( "   SURF {0} FACE{1}, S{1}" . format ( slabel , findex [ 1 : ] ) ) else : surf output . append ( "**" ) # ELEMENTS elements output = "" for etype , group in mesh . elements . groupby ( ( ( "type" , "solver" , "" ) , ) ) : els = group . conn . replace ( 0 , np . nan ) . to csv ( header = False , float format = '%.0f' ) . split ( ) elements output += "*ELEMENT, TYPE={0}\n" . format ( etype ) elements output += ( "\n" . join ( [ "  " + s . strip ( ) . strip ( "," ) . replace ( "," , ", " ) for s in els ] ) ) elements output += "\n" elements output = elements output . strip ( ) el sets = { } # MATERIALS section output = "" for material , group in mesh . elements . groupby ( "materials" ) : slabel = " MAT {0}" . format ( material ) section output += "*ELSET, ELSET= MAT {0}\n{1}\n" . format ( material , argiope . utils . list to string ( group . index . values ) ) #mesh.elements[("sets", slabel, "")] = False #mesh.elements.loc[group.index, ("sets", slabel, "")] = True if sections == "solid" : section output += "*SOLID SECTION, ELSET= MAT {0}, MATERIAL={0}\n" . format ( material ) # ELEMENTS SETS if "sets" in mesh . elements . columns . levels [ 0 ] : esets = set to inp ( mesh . elements . sets . swaplevel ( 1 , 0 , axis = 1 ) [ "" ] , "ELSET" ) else : esets = "**" # PATTERN pattern = Template ( open ( MODPATH + "/templates/mesh/inp.inp" ) . read ( ) ) pattern = pattern . substitute ( NODES = nodes output , NODE SETS = nsets , ELEMENTS = elements output , ELEMENT SETS = esets , ELEMENT SURFACES = "\n" . join ( surf output ) , SECTIONS = section output . strip ( ) ) pattern = pattern . strip ( ) if path == None : return pattern else : open ( path , "w" ) . write ( pattern )
def make conn ( shape ) : shape = np . array ( shape ) Ne = shape . prod ( ) if len ( shape ) == 2 : nx , ny = np . array ( shape ) + 1 conn = np . zeros ( ( Ne , 4 ) , dtype = np . int32 ) counter = 0 pattern = np . array ( [ 0 , 1 , 1 + nx , nx ] ) for j in range ( shape [ 1 ] ) : for i in range ( shape [ 0 ] ) : conn [ counter ] = pattern + 1 + i + j * nx counter += 1 if len ( shape ) == 3 : nx , ny , nz = np . array ( shape ) + 1 conn = np . zeros ( ( Ne , 8 ) , dtype = np . int32 ) counter = 0 pattern = np . array ( [ 0 , 1 , 1 + nx , nx , nx * ny , 1 + nx * ny , 1 + ( nx + 1 ) * ny , ( nx + 1 ) * ny ] ) for k in range ( shape [ 2 ] ) : for j in range ( shape [ 1 ] ) : for i in range ( shape [ 0 ] ) : conn [ counter ] = pattern + 1 + i + j * nx + k * nx * ny counter += 1 return conn
def set fields ( self , fields = None , * * kwargs ) : self . fields = [ ] if fields != None : for field in fields : self . fields . append ( field )
def add fields ( self , fields = None , * * kwargs ) : if fields != None : for field in fields : self . fields . append ( field )
def check elements ( self ) : # ELEMENT TYPE CHECKING existing types = set ( self . elements . type . argiope . values . flatten ( ) ) allowed types = set ( ELEMENTS . keys ( ) ) if ( existing types <= allowed types ) == False : raise Value Error ( "Element types {0} not in know elements {1}" . format ( existing types - allowed types , allowed types ) ) print ( "<Elements: OK>" )
def space ( self ) : return self . elements . type . argiope . map ( lambda t : ELEMENTS [ t ] . space )
def centroids and volumes ( self , sort index = True ) : elements = self . elements out = [ ] for etype , group in self . elements . groupby ( [ ( "type" , "argiope" , "" ) ] ) : etype info = ELEMENTS [ etype ] simplices info = etype info . simplices index = group . index simplices data = self . split ( into = "simplices" , loc = index , at = "coords" ) simplices = simplices data . values . reshape ( index . size , simplices info . shape [ 0 ] , simplices info . shape [ 1 ] , 3 ) edges = simplices [ : , : , 1 : ] - simplices [ : , : , : 1 ] simplices centroids = simplices . mean ( axis = 2 ) if etype info . space == 2 : simplices volumes = np . linalg . norm ( np . cross ( edges [ : , : , 0 ] , edges [ : , : , 1 ] , axis = 2 ) , axis = 2 ) / 2. elif etype info . space == 3 : simplices volumes = ( np . cross ( edges [ : , : , 0 ] , edges [ : , : , 1 ] , axis = 2 ) * edges [ : , : , 2 ] ) . sum ( axis = 2 ) / 6. elements volumes = simplices volumes . sum ( axis = 1 ) elements centroids = ( ( simplices volumes . reshape ( * simplices volumes . shape , 1 ) * simplices centroids ) . sum ( axis = 1 ) / elements volumes . reshape ( * elements volumes . shape , 1 ) ) volumes df = pd . Data Frame ( index = index , data = elements volumes , columns = pd . Multi Index . from product ( [ [ "volume" ] , [ "" ] ] ) ) centroids df = pd . Data Frame ( index = index , data = elements centroids , columns = pd . Multi Index . from product ( [ [ "centroid" ] , [ "x" , "y" , "z" ] ] ) ) out . append ( pd . concat ( [ volumes df , centroids df ] , axis = 1 ) ) out = pd . concat ( out ) if sort index : out . sort index ( inplace = True ) return out . sort index ( axis = 1 )
def angles ( self , zfill = 3 ) : elements = self . elements . sort index ( axis = 1 ) etypes = elements [ ( "type" , "argiope" ) ] . unique ( ) out = [ ] for etype in etypes : etype info = ELEMENTS [ etype ] angles info = etype info . angles loc = elements [ ( "type" , "argiope" , "" ) ] == etype index = elements . loc [ loc ] . index angles data = self . split ( into = "angles" , loc = loc , at = "coords" ) data = angles data . values . reshape ( index . size , angles info . shape [ 0 ] , angles info . shape [ 1 ] , 3 ) edges = data [ : , : , [ 0 , 2 ] , : ] - data [ : , : , 1 : 2 , : ] edges /= np . linalg . norm ( edges , axis = 3 ) . reshape ( index . size , angles info . shape [ 0 ] , 2 , 1 ) angles = np . degrees ( np . arccos ( ( edges [ : , : , 0 ] * edges [ : , : , 1 ] ) . sum ( axis = 2 ) ) ) deviation = angles - etype info . optimal angles angles df = pd . Data Frame ( index = index , data = angles , columns = pd . Multi Index . from product ( [ [ "angles" ] , [ "a" + "{0}" . format ( s ) . zfill ( zfill ) for s in range ( angles info . shape [ 0 ] ) ] ] ) ) deviation df = pd . Data Frame ( index = index , data = deviation , columns = pd . Multi Index . from product ( [ [ "deviation" ] , [ "d" + "{0}" . format ( s ) . zfill ( zfill ) for s in range ( angles info . shape [ 0 ] ) ] ] ) ) df = pd . concat ( [ angles df , deviation df ] , axis = 1 ) . sort index ( axis = 1 ) df [ "stats" , "max angle" ] = df . angles . max ( axis = 1 ) df [ "stats" , "min angle" ] = df . angles . min ( axis = 1 ) df [ "stats" , "max angular deviation" ] = df . deviation . max ( axis = 1 ) df [ "stats" , "min angular deviation" ] = df . deviation . min ( axis = 1 ) df [ "stats" , "max abs angular deviation" ] = abs ( df . deviation ) . max ( axis = 1 ) df = df . sort index ( axis = 1 ) out . append ( df ) out = pd . concat ( out ) . sort index ( axis = 1 ) return out
def edges ( self , zfill = 3 ) : edges = self . split ( "edges" , at = "coords" ) . unstack ( ) edges [ "lx" ] = edges . x [ 1 ] - edges . x [ 0 ] edges [ "ly" ] = edges . y [ 1 ] - edges . y [ 0 ] edges [ "lz" ] = edges . z [ 1 ] - edges . z [ 0 ] edges [ "l" ] = np . linalg . norm ( edges [ [ "lx" , "ly" , "lz" ] ] , axis = 1 ) edges = ( edges . l ) . unstack ( ) edges . columns = pd . Multi Index . from product ( [ [ "length" ] , [ "e" + "{0}" . format ( s ) . zfill ( zfill ) for s in np . arange ( edges . shape [ 1 ] ) ] ] ) edges [ ( "stats" , "lmax" ) ] = edges . length . max ( axis = 1 ) edges [ ( "stats" , "lmin" ) ] = edges . length . min ( axis = 1 ) edges [ ( "stats" , "aspect ratio" ) ] = edges . stats . lmax / edges . stats . lmin return edges . sort index ( axis = 1 )
def stats ( self ) : cv = self . centroids and volumes ( ) angles = self . angles ( ) edges = self . edges ( ) return pd . concat ( [ cv , angles [ [ "stats" ] ] , edges [ [ "stats" ] ] ] , axis = 1 ) . sort index ( axis = 1 )
def element set to node set ( self , tag ) : nodes , elements = self . nodes , self . elements loc = ( elements . conn [ elements [ ( "sets" , tag , "" ) ] ] . stack ( ) . stack ( ) . unique ( ) ) loc = loc [ loc != 0 ] nodes [ ( "sets" , tag ) ] = False nodes . loc [ loc , ( "sets" , tag ) ] = True
def node set to surface ( self , tag ) : # Create a dummy node with label 0 nodes = self . nodes . copy ( ) dummy = nodes . iloc [ 0 ] . copy ( ) dummy [ "coords" ] *= np . nan dummy [ "sets" ] = True nodes . loc [ 0 ] = dummy # Getting element surfaces element surfaces = self . split ( "surfaces" ) . unstack ( ) # killer hack ! surf = pd . Data Frame ( nodes . sets [ tag ] . loc [ element surfaces . values . flatten ( ) ] . values . reshape ( element surfaces . shape ) . prod ( axis = 1 ) . astype ( np . bool ) , index = element surfaces . index ) . unstack ( ) . fillna ( False ) for k in surf . keys ( ) : self . elements [ "surfaces" , tag , "f{0}" . format ( k [ 1 ] + 1 ) ] = surf . loc [ : , k ]
def surface to element sets ( self , tag ) : surface = self . elements . surfaces [ tag ] for findex in surface . keys ( ) : if surface [ findex ] . sum ( ) != 0 : self . elements [ ( "sets" , " SURF {0} FACE{1}" . format ( tag , findex [ 1 : ] ) , "" ) ] = surface [ findex ]
def fields metadata ( self ) : return ( pd . concat ( [ f . metadata ( ) for f in self . fields ] , axis = 1 ) . transpose ( ) . sort values ( [ "step num" , "frame" , "label" , "position" ] ) )
def metadata ( self ) : return pd . Series ( { "part" : self . part , "step num" : self . step num , "step label" : self . step label , "frame" : self . frame , "frame value" : self . frame value , "label" : self . label , "position" : self . position , } )
def make directories ( self ) : if os . path . isdir ( self . workdir ) == False : os . mkdir ( self . workdir )
def run postproc ( self ) : t0 = time . time ( ) if self . verbose : print ( '#### POST-PROCESSING "{0}" USING POST-PROCESSOR "{1}"'. f ormat( s elf. l abel,   self . solver . upper ( ) ) ) if self . solver == "abaqus" : command = '{0} viewer no GUI={1} abqpp.py' . format ( self . solver path , self . label ) process = subprocess . Popen ( command , cwd = self . workdir , shell = True , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) for line in iter ( process . stdout . readline , b'' ) : line = line . rstrip ( ) . decode ( 'utf8' ) print ( "    " , line ) t1 = time . time ( ) if self . verbose : print ( '  => POST-PROCESSED {0}: DURATION = {1:.2f}s >' . format ( self . label , t1 - t0 ) )
def run gmsh ( self ) : argiope . utils . run gmsh ( gmsh path = self . gmsh path , gmsh space = self . gmsh space , gmsh options = self . gmsh options , name = self . file name + ".geo" , workdir = self . workdir ) self . mesh = argiope . mesh . read msh ( self . workdir + self . file name + ".msh" )
def read history report ( path , steps , x name = None ) : data = pd . read csv ( path , delim whitespace = True ) if x name != None : data [ x name ] = data . X del data [ "X" ] data [ "step" ] = 0 t = 0. for i in range ( len ( steps ) ) : dt = steps [ i ] . duration loc = data [ data . t == t ] . index if len ( loc ) == 2 : data . loc [ loc [ 1 ] : , "step" ] = i t += dt return data
def read field report ( path , data flag = "*DATA" , meta data flag = "*METADATA" ) : text = open ( path ) . read ( ) mdpos = text . find ( meta data flag ) dpos = text . find ( data flag ) mdata = io . String IO ( "\n" . join ( text [ mdpos : dpos ] . split ( "\n" ) [ 1 : ] ) ) data = io . String IO ( "\n" . join ( text [ dpos : ] . split ( "\n" ) [ 1 : ] ) ) data = pd . read csv ( data , index col = 0 ) data = data . groupby ( data . index ) . mean ( ) mdata = pd . read csv ( mdata , sep = "=" , header = None , index col = 0 ) [ 1 ] mdata = mdata . to dict ( ) out = { } out [ "step num" ] = int ( mdata [ "step num" ] ) out [ "step label" ] = mdata [ "step label" ] out [ "frame" ] = int ( mdata [ "frame" ] ) out [ "frame value" ] = float ( mdata [ "frame value" ] ) out [ "part" ] = mdata [ "instance" ] position map = { "NODAL" : "node" , "ELEMENT CENTROID" : "element" , "WHOLE ELEMENT" : "element" } out [ "position" ] = position map [ mdata [ "position" ] ] out [ "label" ] = mdata [ "label" ] out [ "data" ] = data field class = getattr ( argiope . mesh , mdata [ "argiope class" ] ) return field class ( * * out )
def list to string ( l = range ( 200 ) , width = 40 , indent = "  " ) : l = [ str ( v ) + "," for v in l ] counter = 0 out = "" + indent for w in l : s = len ( w ) if counter + s > width : out += "\n" + indent counter = 0 out += w counter += s return out . strip ( "," )
def equation ( nodes = ( 1 , 2 ) , dofs = ( 1 , 1 ) , coefficients = ( 1. , 1. ) , comment = None ) : N = len ( nodes ) if comment == None : out = "" else : out = "**EQUATION: {0}\n" . format ( comment ) out += "*EQUATION\n  {0}\n  " . format ( N ) out += "\n  " . join ( [ "," . join ( [ str ( nodes [ i ] ) , str ( int ( dofs [ i ] ) ) , str ( coefficients [ i ] ) ] ) for i in range ( N ) ] ) return out
def unsorted set ( df , label , * * kwargs ) : out = "*NSET, NSET={0}, UNSORTED\n" . format ( label ) labels = df . index . values return out + argiope . utils . list to string ( labels , * * kwargs )
def write inp ( self ) : template = self . get template ( ) return template . substitute ( { "class" : self . class . name , "label" : self . label } ) . strip ( )
def write inp ( self ) : template = self . get template ( ) plastic table = self . get plastic table ( ) return template . substitute ( { "class" : self . class . name , "label" : self . label , "young modulus" : self . young modulus , "poisson ratio" : self . poisson ratio , "plastic table" : ( self . get plastic table ( ) [ [ "stress" , "plastic strain" ] ] . to csv ( header = False , index = False , sep = "," ) . strip ( ) ) } ) . strip ( )
def get plastic table ( self ) : E = self . young modulus sy = self . yield stress n = self . hardening exponent eps max = self . max strain Np = self . strain data points ey = sy / E s = 10. ** np . linspace ( 0. , np . log10 ( eps max / ey ) , Np ) strain = ey * s stress = sy * s ** n plastic strain = strain - stress / E return pd . Data Frame ( { "strain" : strain , "stress" : stress , "plastic strain" : plastic strain } )
def get plastic table ( self ) : K = self . consistency sy = self . yield stress n = self . hardening exponent eps max = self . max strain Np = self . strain data points plastic strain = np . linspace ( 0. , eps max , Np ) stress = sy + K * plastic strain ** n return pd . Data Frame ( { "stress" : stress , "plastic strain" : plastic strain } )
def write xy report ( odb , path , tags , columns , steps ) : xy Data = [ session . XY Data From History ( name = columns [ i ] , odb = odb , output Variable Name = tags [ i ] , steps = steps ) for i in xrange ( len ( tags ) ) ] session . xy Report Options . set Values ( num Digits = 8 , number Format = SCIENTIFIC ) session . write XY Report ( file Name = path , append Mode = OFF , xy Data = xy Data )
def write field report ( odb , path , label , argiope class , variable , instance , output position , step = - 1 , frame = - 1 , sort Item = 'Node Label' ) : step Keys = get steps ( odb ) step = xrange ( len ( step Keys ) ) [ step ] frame = xrange ( get frames ( odb , step Keys [ step ] ) ) [ frame ] nf = Number Format ( num Digits = 9 , precision = 0 , format = SCIENTIFIC ) session . field Report Options . set Values ( print Total = OFF , print Min Max = OFF , number Format = nf ) leaf = dgo . Leaf From Part Instance ( part Instance Name = instance ) session . viewports [ 'Viewport: 1' ] . odb Display . display Group . replace ( leaf = leaf ) session . write Field Report ( file Name = path , append = OFF , sort Item = sort Item , odb = odb , step = step , frame = frame , output Position = output position , variable = variable ) lines = [ line . strip ( ) for line in open ( path ) . readlines ( ) ] isdata = - 1 data = [ ] for line in lines : if isdata == 1 : if len ( line ) == 0 : isdata -= 1 else : data . append ( line ) elif isdata < 1 : if line . startswith ( "--" ) : isdata += 1 data = "\n" . join ( [ "," . join ( line . split ( ) ) for line in data if len ( line ) != 0 ] ) # HEADER header = str ( output position ) . lower ( ) + "," header += "," . join ( [ v [ 1 ] for v in variable [ 0 ] [ 2 ] ] ) + "\n" # METADATA metadata = ( ( "label" , label ) , ( "argiope class" , argiope class ) , ( "odb" , odb . path ) , ( "instance" , instance ) , ( "position" , output position ) , ( "step num" , step ) , ( "step label" , step Keys [ step ] ) , ( "frame" , frame ) , ( "frame value" , odb . steps [ step Keys [ step ] ] . frames [ frame ] . frame Value ) ) out = "*METADATA\n{0}\n*DATA\n{1}" . format ( "\n" . join ( [ "{0}={1}" . format ( k , v ) for k , v in metadata ] ) , header + data ) open ( path , "w" ) . write ( out )
def list ( component type ) : config loader = initialise component loader ( ) component types = sorted ( { "displays" : lambda : config loader . load by type ( Component Type . DISPLAY ) , "datafeeds" : lambda : config loader . load by type ( Component Type . DATA FEED ) , "filters" : lambda : config loader . load by type ( Component Type . FILTER ) , "notifications" : lambda : config loader . load by type ( Component Type . NOTIFICATION ) } . items ( ) , key = lambda t : t [ 0 ] ) def print ids ( creators ) : ids = { c . id key value [ 1 ] if hasattr ( c , "id key value" ) else c . get id ( ) for c in creators } for i in sorted ( ids ) : click . echo ( " - %s" % i ) for k , v in component types : if component type == k or component type == "all" : click . echo ( "Available %s:" % k ) print ids ( v ( ) ) if component type == "all" : click . echo ( "" )
def set data ( self ) : if getattr ( self , 'data' , False ) and not getattr ( self , ' x' , False ) and not getattr ( self , ' y' , False ) : x = X Variable ( ) y = Y Variable ( ) x . contribute to class ( self , 'X' , self . data ) y . contribute to class ( self , 'Y' , self . data ) self [ 'data' ] = zip ( self . x . points , self . y . points ) else : for axis in ( ' x' , ' y' ) : axis obj = getattr ( self , axis , False ) if not axis obj : raise exception . Missing Axis Exception ( "%s missing" % axis ) if not getattr ( axis obj , 'points' , False ) : raise exception . Missing Data Exception ( ) self [ 'data' ] = zip ( self . x . points , self . y . points )
def get axis mode ( self , axis ) : if all ( [ isinstance ( getattr ( s , axis ) , Time Variable ) for s in self . series ] ) : return 'time' return None
def set options ( self ) : # this is aweful # FIXME: Axis options should be passed completly by a Graph Option if 'xaxis' in self . options . keys ( ) : self . options [ 'xaxis' ] . update ( { 'mode' : self . get axis mode ( X Axis . var name ) } ) if 'yaxis' in self . options . keys ( ) : self . options [ 'yaxis' ] . update ( { 'mode' : self . get axis mode ( Y Axis . var name ) } )
def create setter ( func , attrs ) : def set ( self , instance , value , name = None ) : args = [ getattr ( self , attr ) for attr in attrs ] if not func ( value , * args ) : raise Value Error ( self . err msg ( instance , value ) ) return set
def make class ( clsname , func , attrs ) : clsdict = { " set " : create setter ( func , attrs ) } if len ( attrs ) > 0 : clsdict [ " init " ] = create init ( attrs ) clsobj = type ( str ( clsname ) , ( Descriptor , ) , clsdict ) clsobj . doc = docstrings . get ( clsname ) return clsobj
def cycle ( self ) : messages = self . poll datafeeds ( ) notifications = self . process notifications ( messages ) self . draw notifications ( notifications )
def plot ( parser , token ) : tokens = token . split contents ( ) tokens . pop ( 0 ) graph = tokens . pop ( 0 ) attrs = dict ( [ token . split ( "=" ) for token in tokens ] ) if 'id' not in attrs . keys ( ) : attrs [ 'id' ] = '' . join ( [ chr ( choice ( range ( 65 , 90 ) ) ) for i in range ( 0 , 5 ) ] ) else : attrs [ 'id' ] = attrs [ 'id' ] [ 1 : len ( attrs [ 'id' ] ) - 1 ] attr string = '' . join ( [ " %s=%s" % ( k , v ) for k , v in attrs . iteritems ( ) ] ) return Graph Renderer ( graph , attr string , attrs [ 'id' ] )
def read varint ( self ) : buf = self . read ( 8 ) ( n , l ) = Decode Varint ( buf , 0 ) self . unread ( buf [ l : ] ) return n
def working directory ( path ) : prev dir = os . getcwd ( ) os . chdir ( str ( path ) ) try : yield finally : os . chdir ( prev dir )
def exit ( self ) : if self . server is not None : self . server . shutdown ( ) self . server . server close ( ) self . server = None
def get error page callback ( self ) : if self . response . status in self . error handlers : return self . error handlers [ self . response . status ] elif None in self . error handlers : return self . error handlers [ None ] else : # Rudimentary error handler if no error handler was found self . response . media type = 'text/plain' return lambda : self . response . status line
def attempt fetch ( work unit , fpath ) : url = 'http://s3.amazonaws.com/aws-publicdatasets/' + work unit . key . strip ( ) ## cheapest way to iterate over the corpus is a few stages of ## streamed child processes.  Note that stderr needs to go ## separately to a file so that reading the stdin doesn't get ## blocked: cmd = '(wget -O - %s | gpg --no-permission-warning --trust-model always --output - --decrypt - | xz --decompress) 2> %s-err' % ( url , fpath ) print cmd child = Popen ( cmd , stdout = PIPE , shell = True ) print 'child launched' sys . stdout . flush ( ) si count = 0 serif count = 0 exc = '' stream ids = list ( ) clean visible bytes = 0 clean visible count = 0 try : for si in Chunk ( file obj = child . stdout ) : print si . stream id , si . abs url if si . body . language : lang = si . body . language . code else : lang = '' stream ids . append ( ( lang , si . stream id ) ) if si . body . clean visible : clean visible count += 1 clean visible bytes += len ( si . body . clean visible ) si count += 1 if 'serif' in si . body . sentences : serif count += 1 except Exception , exc : exc = re . sub ( '\s+' , ' ' , str ( exc ) ) . strip ( ) child . terminate ( ) child . wait ( ) child . stdout . close ( ) return exc , si count , serif count , clean visible bytes , clean visible count , stream ids
def get file lines ( file name ) : file path = path . join ( path . dirname ( path . abspath ( file ) ) , file name ) with open ( file path ) as file obj : return [ line for line in file obj . read ( ) . splitlines ( ) if line ]
def random adjspecies pair ( ) : describer , desc position = random describer ( ) if desc position == 'prefix' : return ( describer , random species ( ) ) elif desc position == 'suffix' : return ( random species ( ) , describer )
def morph ( ctx , app id , sentence file , json flag , sentence , info filter , pos filter , request id ) : # type: (Context, unicode, Optional[IO], bool, unicode, unicode, unicode, unicode) -> None  # NOQA app id = clean app id ( app id ) sentence = clean sentence ( sentence , sentence file ) if info filter : info filter = info filter . replace ( ',' , '|' ) if pos filter : pos filter = pos filter . replace ( ',' , '|' ) api = Goolabs API ( app id ) ret = api . morph ( sentence = sentence , info filter = info filter , pos filter = pos filter , request id = request id , ) if json flag : click . echo ( format json ( api . response . json ( ) ) ) return for words in ret [ 'word list' ] : for word in words : click . echo ( ',' . join ( word ) )
def similarity ( ctx , app id , json flag , query pair , request id ) : # type: (Context, unicode, bool, List[unicode], unicode) -> None app id = clean app id ( app id ) api = Goolabs API ( app id ) ret = api . similarity ( query pair = query pair , request id = request id ) if json flag : click . echo ( format json ( api . response . json ( ) ) ) return click . echo ( '{0:.16f}' . format ( ret [ 'score' ] ) )
def hiragana ( ctx , app id , sentence file , json flag , sentence , output type , request id ) : # type: (Context, unicode, Optional[IO], bool, unicode, unicode, unicode) -> None # NOQA app id = clean app id ( app id ) sentence = clean sentence ( sentence , sentence file ) api = Goolabs API ( app id ) ret = api . hiragana ( sentence = sentence , output type = output type , request id = request id ) if json flag : click . echo ( format json ( api . response . json ( ) ) ) return click . echo ( ret [ 'converted' ] )
def entity ( ctx , app id , sentence file , json flag , sentence , class filter , request id ) : # type: (Context, unicode, Optional[IO], bool, unicode, unicode, unicode) -> None # NOQA app id = clean app id ( app id ) sentence = clean sentence ( sentence , sentence file ) if class filter : class filter = class filter . replace ( ',' , '|' ) api = Goolabs API ( app id ) ret = api . entity ( sentence = sentence , class filter = class filter , request id = request id ) if json flag : click . echo ( format json ( api . response . json ( ) ) ) return for ne in ret [ 'ne list' ] : click . echo ( ',' . join ( ne ) )
def shortsum ( ctx , app id , review file , json flag , review , length , request id ) : # type: (Context, unicode, Optional[IO], bool, unicode, unicode, unicode) -> None # NOQA app id = clean app id ( app id ) review list = clean review ( review , review file ) length int = clean length ( length ) # type: Optional[int] api = Goolabs API ( app id ) ret = api . shortsum ( review list = review list , length = length int , request id = request id , ) if json flag : click . echo ( format json ( api . response . json ( ) ) ) return click . echo ( ret [ 'summary' ] )
def keyword ( ctx , app id , body file , json flag , title , body , max num , forcus , request id ) : # type: (Context, unicode, Optional[IO], bool, unicode, unicode, int, unicode, unicode) -> None # NOQA app id = clean app id ( app id ) body = clean body ( body , body file ) api = Goolabs API ( app id ) ret = api . keyword ( title = title , body = body , max num = max num , forcus = forcus , request id = request id , ) if json flag : click . echo ( format json ( api . response . json ( ) ) ) return for k in ret [ 'keywords' ] : k = dict ( ( key . encode ( 'utf-8' ) , k [ key ] ) for key in k . keys ( ) ) for keyword , score in six . iteritems ( k ) : click . echo ( u'{0},{1}' . format ( text ( keyword ) , score ) )
def chrono ( ctx , app id , sentence file , json flag , sentence , doc time , request id ) : # type: (Context, unicode, Optional[IO], bool, unicode, unicode, unicode) -> None  # NOQA app id = clean app id ( app id ) sentence = clean sentence ( sentence , sentence file ) api = Goolabs API ( app id ) ret = api . chrono ( sentence = sentence , doc time = doc time , request id = request id , ) if json flag : click . echo ( format json ( api . response . json ( ) ) ) return for pair in ret [ 'datetime list' ] : click . echo ( u'{0}: {1}' . format ( text ( pair [ 0 ] ) , pair [ 1 ] ) )
def make app ( ) : env = Environment ( ) # STDIN is ignored because HTT Pony runs a server that doesn't care. # Additionally, it is needed or else pytest blows up. args = parser . parse args ( args = [ '/' , '--ignore-stdin' ] , env = env ) args . output options = 'HB' # Output only requests. server = 'HTT Pony/{0}' . format ( version ) def application ( environ , start response ) : # The WSGI server puts content length and type in the environment # even when not provided with the request. Drop them if they are empty. if environ . get ( 'CONTENT LENGTH' ) == '' : del environ [ 'CONTENT LENGTH' ] if environ . get ( 'CONTENT TYPE' ) == '' : del environ [ 'CONTENT TYPE' ] wrequest = Werkzeug Request ( environ ) data = wrequest . get data ( ) request = Request ( method = wrequest . method , url = wrequest . url , headers = wrequest . headers , data = data , ) prepared = request . prepare ( ) stream = streams . build output stream ( args , env , prepared , response = None , output options = args . output options ) streams . write stream ( stream , env . stdout , env . stdout isatty ) # When there is data in the request, give the next one breathing room. if data : print ( "\n" , file = env . stdout ) # Make dreams come true. response = Response ( headers = { 'Server' : server } ) return response ( environ , start response ) return application
def make ner file ( self , clean visible path , ner xml path ) : if self . template is None : raise exceptions . Not Implemented Error ( ) tagger config = dict ( tagger root path = self . config [ 'tagger root path' ] , clean visible path = clean visible path , ner xml path = ner xml path ) ## get a java heap size or default to 1GB tagger config [ 'java heap size' ] = self . config . get ( 'java heap size' , '' ) cmd = self . template % tagger config start time = time . time ( ) ## make sure we are using as little memory as possible gc . collect ( ) try : self . child = subprocess . Popen ( cmd , stderr = subprocess . PIPE , shell = True ) except OS Error , exc : msg = traceback . format exc ( exc ) msg += make memory info msg ( clean visible path , ner xml path ) raise Pipeline Out Of Memory ( msg ) s out , errors = self . child . communicate ( ) if not self . child . returncode == 0 : if 'java.lang.Out Of Memory Error' in errors : msg = errors + make memory info msg ( clean visible path , ner xml path ) raise Pipeline Out Of Memory ( msg ) elif self . child . returncode == 137 : msg = 'tagger returncode = 137\n' + errors msg += make memory info msg ( clean visible path , ner xml path ) # maybe get a tail of /var/log/messages raise Pipeline Out Of Memory ( msg ) elif 'Exception' in errors : raise Pipeline Base Exception ( errors ) else : raise Pipeline Base Exception ( 'tagger exited with %r' % self . child . returncode ) elapsed = time . time ( ) - start time logger . info ( 'finished tagging in %.1f seconds' % elapsed ) return elapsed
def align chunk with ner ( self , ner xml path , i chunk , o chunk ) : ## prepare to iterate over the input chunk input iter = i chunk . iter ( ) all ner = xml . dom . minidom . parse ( open ( ner xml path ) ) ## this converts our UTF-8 data into unicode strings, so when ## we want to compute byte offsets or construct tokens, we ## must .encode('utf8') for ner dom in all ner . get Elements By Tag Name ( 'FILENAME' ) : #for stream id, raw ner in files(open(ner xml path).read().decode('utf8')): stream item = input iter . next ( ) ## get stream id out of the XML stream id = ner dom . attributes . get ( 'stream id' ) . value if stream item . stream id is None : assert not stream id , 'out of sync: None != %r' % stream id logger . critical ( 'si.stream id is None... ignoring' ) continue assert stream id and stream id == stream item . stream id , '%s != %s' % ( stream id , stream item . stream id ) if not stream item . body : ## the XML better have had an empty clean visible too... #assert not ner dom....something continue tagging = Tagging ( ) tagging . tagger id = self . tagger id # pylint: disable=E1101 #tagging.raw tagging = tagged doc tagging . generation time = streamcorpus . make stream time ( ) stream item . body . taggings [ self . tagger id ] = tagging # pylint: disable=E1101 ## could consume lots of memory here by instantiating everything sentences , relations , attributes = self . get sentences ( ner dom ) stream item . body . sentences [ self . tagger id ] = sentences # pylint: disable=E1101 stream item . body . relations [ self . tagger id ] = relations # pylint: disable=E1101 stream item . body . attributes [ self . tagger id ] = attributes # pylint: disable=E1101 logger . debug ( 'finished aligning tokens %s' % stream item . stream id ) if 'align labels by' in self . config and self . config [ 'align labels by' ] : assert 'aligner data' in self . config , 'config missing "aligner data"' aligner = Alignment Strategies [ self . config [ 'align labels by' ] ] aligner ( stream item , self . config [ 'aligner data' ] ) ## forcibly collect dereferenced objects gc . collect ( ) try : o chunk . add ( stream item ) except Memory Error , exc : msg = traceback . format exc ( exc ) msg += make memory info msg ( ) logger . critical ( msg ) raise Pipeline Out Of Memory ( msg ) ## all done, so close the o chunk try : o chunk . close ( ) logger . info ( 'finished chunk for %r' % ner xml path ) except Memory Error , exc : msg = traceback . format exc ( exc ) msg += make memory info msg ( ) logger . critical ( msg ) raise Pipeline Out Of Memory ( msg )
def shutdown ( self ) : if self . child : try : self . child . terminate ( ) except OS Error , exc : if exc . errno == 3 : ## child is already gone, possibly because it ran ## out of memory and caused us to shutdown pass
def mult ( p , n ) : np = P ( ) while n >= 1 : if n % 2 : np = np + p p = p + p n = n // 2 return np
def fix emails ( text ) : emails = bracket emails . findall ( text ) keys = [ ] for email in emails : email = email . replace ( "<" , "&lt;" ) . replace ( ">" , "&gt;" ) text = text . replace ( email , email ) return text
def sentences ( self , clean visible ) : previous end = 0 clean visible = clean visible . decode ( 'utf8' ) for start , end in self . sentence tokenizer . span tokenize ( clean visible ) : # no need to check start, because the first byte of text # is always first byte of first sentence, and we will # have already made the previous sentence longer on the # end if there was an overlap. if start < previous end : start = previous end if start > end : # skip this sentence... because it was eaten by # an earlier sentence with a label continue try : label = self . label index . find le ( end ) except Value Error : label = None if label : ## avoid splitting a label off = label . offsets [ Offset Type . CHARS ] end = max ( off . first + off . length , end ) previous end = end sent str = clean visible [ start : end ] yield start , end , sent str
def make label index ( self , stream item ) : labels = stream item . body . labels . get ( self . annotator id ) if not labels : labels = [ ] self . label index = Sorted Collection ( [ l for l in labels if Offset Type . CHARS in l . offsets ] , key = lambda label : label . offsets [ Offset Type . CHARS ] . first )
def make sentences ( self , stream item ) : self . make label index ( stream item ) sentences = [ ] token num = 0 new mention id = 0 for sent start , sent end , sent str in self . sentences ( stream item . body . clean visible ) : assert isinstance ( sent str , unicode ) sent = Sentence ( ) sentence pos = 0 for start , end in self . word tokenizer . span tokenize ( sent str ) : token str = sent str [ start : end ] . encode ( 'utf8' ) tok = Token ( token num = token num , token = token str , sentence pos = sentence pos , ) tok . offsets [ Offset Type . CHARS ] = Offset ( type = Offset Type . CHARS , first = sent start + start , length = end - start , ) # whitespace tokenizer will never get a token # boundary in the middle of an 'author' label try : label = self . label index . find le ( sent start + start ) except Value Error : label = None if label : off = label . offsets [ Offset Type . CHARS ] if off . first + off . length > sent start + start : streamcorpus . add annotation ( tok , label ) logger . debug ( 'adding label to tok: %r has %r' , tok . token , label . target . target id ) if label in self . label to mention id : mention id = self . label to mention id [ label ] else : mention id = new mention id new mention id += 1 self . label to mention id [ label ] = mention id tok . mention id = mention id token num += 1 sentence pos += 1 sent . tokens . append ( tok ) sentences . append ( sent ) return sentences
def make cleansed file ( i chunk , tmp cleansed path ) : tmp cleansed = open ( tmp cleansed path , 'wb' ) for idx , si in enumerate ( i chunk ) : tmp cleansed . write ( '<FILENAME docid="%s">\n' % si . stream id ) tmp cleansed . write ( si . body . cleansed ) ## how to deal with other content? tmp cleansed . write ( '</FILENAME>\n' ) tmp cleansed . close ( ) ## replace this with log.info() print 'created %s' % tmp cleansed path
def make ner file ( tagger id , tmp cleansed path , tmp ner path , pipeline root ) : params = dict ( INPUT FILE = tmp cleansed path , #RAW OUTPUT FILE=tmp ner raw path, OUTPUT FILE = tmp ner path , PIPELINE ROOT = pipeline root ) pipeline cmd = pipeline cmd templates [ tagger id ] % params print pipeline cmd ## replace this with log.info() print 'creating %s' % tmp ner path start time = time . time ( ) gpg child = subprocess . Popen ( pipeline cmd , stderr = subprocess . PIPE , shell = True ) s out , errors = gpg child . communicate ( ) assert gpg child . returncode == 0 and 'Exception' not in errors , errors elapsed = time . time ( ) - start time ## replace this with log.info() print 'created %s in %.1f sec' % ( tmp ner path , elapsed )
def make clean visible file ( i chunk , clean visible path ) : clean = open ( clean visible path , 'wb' ) clean . write ( '<?xml version="1.0" encoding="UTF-8"?>' ) clean . write ( '<root>' ) for idx , si in enumerate ( i chunk ) : if si . stream id is None : # create the FILENAME element anyway, so the ordering # remains the same as the i chunk and can be aligned. stream id = '' else : stream id = si . stream id doc = lxml . etree . Element ( "FILENAME" , stream id = stream id ) if si . body and si . body . clean visible : try : # is UTF-8, and etree wants .text to be unicode doc . text = si . body . clean visible . decode ( 'utf8' ) except Value Error : doc . text = drop invalid and upper utf8 chars ( si . body . clean visible . decode ( 'utf8' ) ) except Exception , exc : # this should never ever fail, because if it does, # then it means that clean visible (or more likely # clean html) is not what it is supposed to be. # Therefore, do not take it lightly: logger . critical ( traceback . format exc ( exc ) ) logger . critical ( 'failed on stream id=%s to follow:' , si . stream id ) logger . critical ( repr ( si . body . clean visible ) ) logger . critical ( 'above was stream id=%s' , si . stream id ) # [I don't know who calls this, but note that this # will *always* fail if clean visible isn't valid UTF-8.] raise else : doc . text = '' clean . write ( lxml . etree . tostring ( doc , encoding = 'UTF-8' ) ) clean . write ( '</root>' ) clean . close ( ) logger . info ( clean visible path )
def main ( ) : import argparse import sys parser = argparse . Argument Parser ( ) parser . add argument ( 'path' ) args = parser . parse args ( ) html = open ( args . path ) . read ( ) html = html . decode ( 'utf8' ) cursor = 0 for s in non tag chars from raw ( html ) : for c in s : if c != ' ' and c != html [ cursor ] : import pdb pdb . set trace ( ) sys . stdout . write ( c . encode ( 'utf8' ) ) sys . stdout . flush ( ) cursor += 1
def paths ( input dir ) : for root , dirs , fnames in os . walk ( input dir ) : for i fname in fnames : i path = os . path . join ( root , i fname ) yield i path
def tasks ( self , key prefix = '' ) : for row in self . tasks . get range ( ) : logger . debug ( row ) if not row [ 0 ] . startswith ( key prefix ) : continue data = json . loads ( row [ 1 ] [ 'task data' ] ) data [ 'task key' ] = row [ 0 ] yield data
def get random available ( self , max iter = 10000 ) : c = 1 keeper = None ## note the Consistency Level here.  If we do not do this, and ## get all slick with things like column count=0 and filter ## empty False, then we can get keys that were recently ## deleted... EVEN if the default consistency would seem to ## rule that out! ## note the random start key, so that we do not always hit the ## same place in the key range with all workers #random key = hashlib.md5(str(random.random())).hexdigest() #random key = '0' * 32 #logger.debug('available.get range(%r)' % random key) ## scratch that idea: turns out that using a random start key ## OR using row count=1 can cause get range to hang for hours ## why we need Consistency Level.ALL on a single node is not ## clear, but experience indicates it is needed. ## note that putting a finite row count is problematic in two ## ways: # 1) if there are more workers than max iter, some will not # get tasks # # 2) if there are more than max iter records, then all workers # have to wade through all of these just to get a task!  What # we really want is a "pick random row" function, and that is # probably best implemented using CQL3 token function via the # cql python module instead of pycassa... for row in self . available . get range ( row count = max iter , read consistency level = pycassa . Consistency Level . ALL ) : #for row in self. available.get range(row count=100): logger . debug ( 'considering %r' % ( row , ) ) if random . random ( ) < 1 / c : keeper = row [ 0 ] if c == max iter : break c += 1 return keeper
def tokens ( self , sentence dom ) : ## keep track of sentence position, which is reset for each ## sentence, and used above in  make token self . sent pos = 0 ## keep track of mention id, so we can distinguish adjacent ## multi-token mentions within the same coref chain mention id = 0 while len ( sentence dom . child Nodes ) > 0 : ## shrink the sentence dom's child nodes.  In v0 2 0 this ## was required to cope with Hit Maxi16.  Now it is just to ## save memory. node = sentence dom . child Nodes . pop ( 0 ) if node . node Type == node . TEXT NODE : ## process portion before an ENAMEX tag for line in node . data . splitlines ( True ) : self . input string = line for start , end in self . word tokenizer . span tokenize ( line ) : tok = self . make token ( start , end ) if tok : yield tok if line . endswith ( '\n' ) : ## maintain the index to the current line self . line idx += 1 ## increment index pasat the 'before' portion self . byte idx += len ( line . encode ( 'utf-8' ) ) else : ## process text inside an ENAMEX tag assert node . node Name == 'ENAMEX' , node . node Name chain id = node . attributes . get ( 'ID' ) . value entity type = node . attributes . get ( 'TYPE' ) . value for node in node . child Nodes : assert node . node Type == node . TEXT NODE , node . node Type for line in node . data . splitlines ( True ) : self . input string = line for start , end in self . word tokenizer . span tokenize ( line ) : tok = self . make token ( start , end ) if tok : if entity type in PRONOUNS : tok . mention type = Mention Type . PRO tok . entity type = ENTITY TYPES [ entity type ] ## create an attribute attr = Attribute ( attribute type = Attribute Type . PER GENDER , value = str ( PRONOUNS [ entity type ] ) ) self . attributes . append ( attr ) else : ## regular entity type tok . mention type = Mention Type . NAME tok . entity type = ENTITY TYPES [ entity type ] tok . equiv id = int ( chain id ) tok . mention id = mention id yield tok if line . endswith ( '\n' ) : ## maintain the index to the current line self . line idx += 1 ## increment index pasat the 'before' portion self . byte idx += len ( line . encode ( 'utf-8' ) ) ## increment mention id within this sentence mention id += 1
def get sentences ( self , ner dom ) : lp parser = Ling Pipe Parser ( self . config ) lp parser . set ( ner dom ) sentences = list ( lp parser . sentences ( ) ) return sentences , lp parser . relations , lp parser . attributes
def verify md5 ( md5 expected , data , other errors = None ) : # O o ? md5 recv = hashlib . md5 ( data ) . hexdigest ( ) if md5 expected != md5 recv : if other errors is not None : logger . critical ( '\n' . join ( other errors ) ) raise Failed Verification ( 'original md5 = %r != %r = received md5' % ( md5 expected , md5 recv ) ) return True
def main ( argv = sys . argv ) : args = parse ( argv ) hostname = args . listen port = args . port print ( "Making all your dreams for a pony come true on http://{0}:{1}.\n" "Press Ctrl+C to quit.\n" . format ( hostname , port ) ) # Hush, werkzeug. logging . get Logger ( 'werkzeug' ) . set Level ( logging . CRITICAL ) plugin manager . load installed plugins ( ) app = make app ( ) run simple ( hostname , port , app )
def build parser ( ) : description = ( 'HTT Pony (pronounced aych-tee-tee-pony) is a simple HTTP ' 'server that pretty prints HTTP requests to a terminal. It ' 'is a useful aide for developing clients that send HTTP ' 'requests. HTT Pony acts as a sink for a client so that a ' 'developer can understand what the client is sending.' ) parser = argparse . Argument Parser ( description = description ) parser . add argument ( '-l' , '--listen' , help = 'set the IP address or hostname' , default = 'localhost' ) parser . add argument ( '-p' , '--port' , help = 'set the port' , default = 8000 , type = int ) return parser
def sentences to char tokens ( si sentences ) : for sentence in si sentences : for token in sentence . tokens : if Offset Type . CHARS in token . offsets : yield token
def char tokens to char offsets ( si tokens ) : for token in si tokens : offset = token . offsets [ Offset Type . CHARS ] yield offset . first , offset . first + offset . length
def text index ( self ) : # This is the number of text nodes we've seen so far. # If we are currently in a text node, great; if not then add # one for the text node that's about to begin. i = self . tags . get ( Text Element , 0 ) if self . last tag is not Text Element : i += 1 return i
def descendants ( elem ) : for child in elem . xml children : if isinstance ( child , element ) : yield child yield from descendants ( child )
def following siblings ( elem ) : it = itertools . dropwhile ( lambda x : x != elem , elem . xml parent . xml children ) next ( it ) #Skip the element itself return it
def svg2pdf ( svg file path , pdf file path , dpi = 150 , command binpath = None , support unicode = False ) : if support unicode : return rsvg export ( svg file path , pdf file path , dpi = dpi , rsvg binpath = command binpath ) return inkscape export ( svg file path , pdf file path , export flag = "-A" , dpi = dpi , inkscape binpath = command binpath )
def svg2png ( svg file path , png file path , dpi = 150 , inkscape binpath = None ) : return inkscape export ( svg file path , png file path , export flag = "-e" , dpi = dpi , inkscape binpath = inkscape binpath )
def strval ( node , outermost = True ) : if not isinstance ( node , element ) : return node . xml value if outermost else [ node . xml value ] accumulator = [ ] for child in node . xml children : if isinstance ( child , text ) : accumulator . append ( child . xml value ) elif isinstance ( child , element ) : accumulator . extend ( strval ( child , outermost = False ) ) if outermost : accumulator = '' . join ( accumulator ) return accumulator
def parse options ( ) : # build options and help version = "%%prog {version}" . format ( version = version ) parser = Option Parser ( version = version ) parser . add option ( "-u" , "--username" , action = "store" , dest = "username" , type = "string" , default = "" , metavar = "RECIPIENT" , help = "user" ) parser . add option ( "-C" , "--calendar" , metavar = "CALENDAR" , action = "store" , type = "string" , dest = "calendar" , default = "" , help = "google calendar ID" ) parser . add option ( "-t" , "--timezone" , metavar = "TIMEZONE" , action = "store" , type = "string" , dest = "timezone" , default = "" , help = "user timezone" ) parser . add option ( "-m" , "--message" , metavar = "MESSAGE" , action = "store" , type = "string" , dest = "message" , default = "" , help = "message text" ) parser . add option ( "-c" , "--config" , metavar = "CONFIG" , action = "store" , type = "string" , dest = "config" , help = "path to config file" , default = "/etc/nagios/notification google calendar.ini" ) parser . add option ( "-q" , "--quiet" , metavar = "QUIET" , action = "store true" , default = False , dest = "quiet" , help = "be quiet" ) parser . add option ( "-g" , "--get-google-credentials" , metavar = "GET-GOOGLE-CREDENTIALS" , action = "store true" , default = False , dest = "get google credentials" , help = "get google API credentials for user" ) options = parser . parse args ( sys . argv ) [ 0 ] mandatories = [ "username" , ] # check mandatory command line options supplied if not options . get google credentials : mandatories . append ( "calendar" ) # set calendar option required when sending message mandatories . append ( "message" ) # set message option required when sending message mandatories . append ( "timezone" ) # set timezone option required when sending message if not all ( options . dict [ mandatory ] for mandatory in mandatories ) : parser . error ( "Required command line option missing\n" ) return options
def parse config ( options ) : if os . path . exists ( options . config ) : config = Config Parser . Config Parser ( ) try : config . read ( options . config ) except Exception , err : if not options . quiet : sys . stderr . write ( "ERROR: Config file read {config} error. {err}" . format ( config = options . config , err = err ) ) sys . exit ( - 1 ) try : configdata = { "secrets" : config . get ( "GOOGLE" , "secrets" ) , "credentials" : config . get ( "nagios-notification-google-calendar" , "credentials" ) , "start" : config . get ( "nagios-notification-google-calendar" , "start" ) , "end" : config . get ( "nagios-notification-google-calendar" , "end" ) , "message" : config . get ( "nagios-notification-google-calendar" , "message" ) , } except Config Parser . No Option Error , err : if not options . quiet : sys . stderr . write ( "ERROR: Config file missing option error. {err}\n" . format ( err = err ) ) sys . exit ( - 1 ) # check mandatory config options supplied mandatories = [ "secrets" , "credentials" , "start" , "end" , "message" , ] if not all ( configdata [ mandatory ] for mandatory in mandatories ) : if not options . quiet : sys . stdout . write ( "Mandatory config option missing\n" ) sys . exit ( 0 ) return configdata else : if not options . quiet : sys . stderr . write ( "ERROR: Config file {config} does not exist\n" . format ( config = options . config ) ) sys . exit ( 0 )
def get google credentials ( options , config ) : try : if options . get google credentials : flow = flow from clientsecrets ( config [ "secrets" ] , scope = SCOPE , redirect uri = "oob" ) sys . stdout . write ( "Follow this URL: {url} and grant access to calendar.\n" . format ( url = flow . step1 get authorize url ( ) ) ) token = raw input ( "Enter token:" ) credentials = flow . step2 exchange ( token ) storage = Storage ( os . path . join ( config [ "credentials" ] , "{username}.json" . format ( username = options . username ) ) ) storage . put ( credentials ) credentials . set store ( storage ) else : storage = Storage ( os . path . join ( config [ "credentials" ] , "{username}.json" . format ( username = options . username ) ) ) credentials = storage . get ( ) except Exception , err : if not options . quiet : sys . stderr . write ( "ERROR: Getting google API credentials error. {err}\n" . format ( err = err ) ) sys . exit ( - 1 ) return credentials
def create event datetimes ( options , config ) : now = datetime . datetime . now ( ) return { "start" : { "date Time" : ( now + datetime . timedelta ( minutes = int ( config [ "start" ] ) ) ) . strftime ( DT FORMAT ) , "time Zone" : options . timezone , } , "end" : { "date Time" : ( now + datetime . timedelta ( minutes = int ( config [ "end" ] ) ) ) . strftime ( DT FORMAT ) , "time Zone" : options . timezone , } , }
def create event ( options , config , credentials ) : try : http = credentials . authorize ( httplib2 . Http ( ) ) service = build ( "calendar" , "v3" , http = http ) event = { "summary" : options . message , "location" : "" , "reminders" : { "use Default" : False , "overrides" : [ { "method" : "sms" , "minutes" : config [ "message" ] , } , ] , } } event . update ( create event datetimes ( options , config ) ) service . events ( ) . insert ( calendar Id = options . calendar , send Notifications = True , body = event ) . execute ( ) except Exception , err : if not options . quiet : sys . stderr . write ( "ERROR: Creating google calendar event error. {err}\n" . format ( err = err ) ) sys . exit ( - 1 )
def main ( ) : # getting info for creating event options = parse options ( ) config = parse config ( options ) credentials = get google credentials ( options , config ) if not options . get google credentials : create event ( options , config , credentials )
def parse ( self ) : for tag in self . soup . find All ( 'span' ) : self . create italic ( tag ) self . create strong ( tag ) self . create underline ( tag ) self . unwrap span ( tag ) for tag in self . soup . find All ( 'a' ) : self . remove comments ( tag ) self . check next ( tag ) if self . soup . body : for tag in self . soup . body . find All ( ) : self . remove empty ( tag ) self . remove inline comment ( tag ) self . parse attrs ( tag ) for token , target in self . tokens : self . find token ( tag , token , target ) self . remove blacklisted tags ( tag )
def check next ( self , tag ) : if ( type ( tag . next sibling ) == element . Tag and tag . next sibling . name == 'a' ) : next tag = tag . next sibling if tag . get ( 'href' ) and next tag . get ( 'href' ) : href = self . parse href ( tag . get ( 'href' ) ) next href = self . parse href ( next tag . get ( 'href' ) ) if href == next href : next text = next tag . get text ( ) tag . append ( next text ) self . tags blacklist . append ( next tag )
def create italic ( self , tag ) : style = tag . get ( 'style' ) if style and 'font-style:italic' in style : tag . wrap ( self . soup . new tag ( 'em' ) )
def create strong ( self , tag ) : style = tag . get ( 'style' ) if ( style and ( 'font-weight:bold' in style or 'font-weight:700' in style ) ) : tag . wrap ( self . soup . new tag ( 'strong' ) )
def create underline ( self , tag ) : style = tag . get ( 'style' ) if style and 'text-decoration:underline' in style : tag . wrap ( self . soup . new tag ( 'u' ) )
def parse attrs ( self , tag ) : if tag . name in ATTR WHITELIST . keys ( ) : attrs = copy ( tag . attrs ) for attr , value in attrs . items ( ) : if attr in ATTR WHITELIST [ tag . name ] : tag . attrs [ attr ] = self . parse attr ( tag . name , attr , value ) else : del tag . attrs [ attr ] else : tag . attrs = { }
def remove empty ( self , tag ) : has children = len ( tag . contents ) has text = len ( list ( tag . stripped strings ) ) if not has children and not has text and not tag . is empty element : tag . extract ( )
def boolean arg ( ctx , obj ) : if hasattr ( obj , 'compute' ) : obj = next ( obj . compute ( ctx ) , False ) return to boolean ( obj )
def number arg ( ctx , obj ) : if hasattr ( obj , 'compute' ) : obj = next ( obj . compute ( ctx ) , False ) return to number ( obj )
def string arg ( ctx , obj ) : if hasattr ( obj , 'compute' ) : obj = next ( obj . compute ( ctx ) , False ) return to string ( obj )
def concat ( ctx , * strings ) : strings = flatten ( [ ( s . compute ( ctx ) if callable ( s ) else s ) for s in strings ] ) strings = ( next ( string arg ( ctx , s ) , '' ) for s in strings ) #assert(all(map(lambda x: isinstance(x, str), strings))) #FIXME: Check arg types yield '' . join ( strings )
def starts with ( ctx , full , part ) : full = next ( string arg ( ctx , full ) , '' ) part = next ( string arg ( ctx , part ) , '' ) yield full . startswith ( part )
def contains ( ctx , full , part ) : full = next ( string arg ( ctx , full ) , '' ) part = next ( string arg ( ctx , part ) , '' ) yield part in full
def check inputs ( self ) : try : = self . inputs [ 0 ] except Type Error : raise Runtime Error ( "inputs should be iterable but found type='{0}', value=" "'{1}'" . format ( type ( self . inputs ) , str ( self . inputs ) ) ) from melody . inputs import Input for check input in self . inputs : if not isinstance ( check input , Input ) : raise Runtime Error ( "input should be a subclass of the Input class but " "found type='{0}', value='{1}'" . format ( type ( check input ) , str ( check input ) ) )
def check function ( self ) : # note, callable is valid for Python 2 and Python 3.2 onwards but # not inbetween if not callable ( self . function ) : raise Runtime Error ( "provided function '{0}' is not callable" . format ( str ( self . function ) ) ) from inspect import getargspec arg info = getargspec ( self . function ) if len ( arg info . args ) != 1 : print str ( arg info ) raise Runtime Error ( "provided function should have one argument but found " "{0}" . format ( len ( arg info . args ) ) )
def recurse ( self , inputs , output , depth , max depth ) : if depth < max depth : for index , option in enumerate ( inputs ) : my output = list ( output ) my output . append ( option ) self . recurse ( inputs [ index + 1 : ] , my output , depth + 1 , max depth ) else : self . options . append ( output )
def to string ( obj ) : if isinstance ( obj , Literal Wrapper ) : val = obj . obj elif isinstance ( obj , Iterable ) and not isinstance ( obj , str ) : val = next ( obj , None ) else : val = obj if val is None : yield '' elif isinstance ( val , str ) : yield val elif isinstance ( val , node ) : yield strval ( val ) elif isinstance ( val , int ) or isinstance ( val , float ) : yield str ( val ) elif isinstance ( item , bool ) : yield 'true' if item else 'false' else : raise Runtime Error ( 'Unknown type for string conversion: {}' . format ( val ) )
def to number ( obj ) : if isinstance ( obj , Literal Wrapper ) : val = obj . obj elif isinstance ( obj , Iterable ) and not isinstance ( obj , str ) : val = next ( obj , None ) else : val = obj if val is None : #FIXME: Should be Na N, not 0 yield 0 elif isinstance ( val , str ) : yield float ( val ) elif isinstance ( val , node ) : yield float ( strval ( val ) ) elif isinstance ( val , int ) or isinstance ( val , float ) : yield val else : raise Runtime Error ( 'Unknown type for number conversion: {}' . format ( val ) )
def to boolean ( obj ) : #if hasattr(obj, ' iter '): if isinstance ( obj , Literal Wrapper ) : val = obj . obj elif isinstance ( obj , Iterable ) and not isinstance ( obj , str ) : val = next ( obj , None ) else : val = obj if val is None : yield False elif isinstance ( val , bool ) : yield val elif isinstance ( val , str ) : yield bool ( str ) elif isinstance ( val , node ) : yield True elif isinstance ( val , float ) or isinstance ( val , int ) : yield bool ( val ) else : raise Runtime Error ( 'Unknown type for boolean conversion: {}' . format ( val ) )
def intersect ( self , other ) : inter = Envelope ( tuple ( self ) ) if inter . intersects ( other ) : mid = len ( other ) // 2 inter . ll = map ( max , inter . ll , other [ : mid ] ) inter . ur = map ( min , inter . ur , other [ mid : ] ) else : inter . ll = ( 0 , 0 ) inter . ur = ( 0 , 0 ) return inter
def polygon ( self ) : ring = ogr . Geometry ( ogr . wkb Linear Ring ) for coord in self . ll , self . lr , self . ur , self . ul , self . ll : ring . Add Point 2D ( * coord ) polyg = ogr . Geometry ( ogr . wkb Polygon ) polyg . Add Geometry Directly ( ring ) return polyg
def from name ( cls , name ) : filename = os . path . join ( package dir , 'data' , name + '.txt' ) return cls . from file ( filename , name )
def from file ( cls , filename , name = '' ) : df = pd . read csv ( filename , header = 0 , delim whitespace = True , index col = [ 0 , 1 ] ) [ 'M' ] df . name = name return cls ( df = df , name = name )
def odd even ( self ) : return self . select ( lambda Z , N : ( Z % 2 ) and not ( N % 2 ) , name = self . name )
def even odd ( self ) : return self . select ( lambda Z , N : not ( Z % 2 ) and ( N % 2 ) , name = self . name )
def even even ( self ) : return self . select ( lambda Z , N : not ( Z % 2 ) and not ( N % 2 ) , name = self . name )
def binding energy ( self ) : M P = 938.2723 # Me V M E = 0.5110 # Me V M N = 939.5656 # Me V AMU = 931.494028 # Me V df = self . Z * ( M P + M E ) + ( self . A - self . Z ) * M N - ( self . df + self . A * AMU ) return Table ( df = df , name = 'BE' + '(' + self . name + ')' )
def s2n ( self ) : M N = 8.0713171 # neutron mass excess in Me V f = lambda parent , daugther : - parent + daugther + 2 * M N return self . derived ( 's2n' , ( 0 , - 2 ) , f )
def s1n ( self ) : M N = 8.0713171 # neutron mass excess in Me V f = lambda parent , daugther : - parent + daugther + M N return self . derived ( 's1n' , ( 0 , - 1 ) , f )
def s2p ( self ) : M P = 7.28897050 # proton mass excess in Me V f = lambda parent , daugther : - parent + daugther + 2 * M P return self . derived ( 's2p' , ( - 2 , 0 ) , f )
def s1p ( self ) : M P = 7.28897050 # proton mass excess in Me V f = lambda parent , daugther : - parent + daugther + M P return self . derived ( 's1p' , ( - 1 , 0 ) , f )
def derived ( self , name , relative coords , formula ) : rel Z , rel N = relative coords daughter idx = [ ( x [ 0 ] + rel Z , x [ 1 ] + rel N ) for x in self . df . index ] values = formula ( self . df . values , self . df . loc [ daughter idx ] . values ) return Table ( df = pd . Series ( values , index = self . df . index , name = name + '(' + self . name + ')' ) )
def derive key ( self , master password ) : encoder = encoding . Encoder ( self . charset ) bytes = ( '%s:%s' % ( master password , self . name ) ) . encode ( 'utf8' ) start time = time . clock ( ) # we fix the scrypt parameters in case the defaults change digest = scrypt . hash ( bytes , self . salt , N = 1 << 14 , r = 8 , p = 1 ) key = encoder . encode ( digest , self . key length ) derivation time in s = time . clock ( ) - start time logger . debug ( 'Key derivation took %.2fms' , derivation time in s * 1000 ) return key
def search ( self , query ) : results = self . session . query ( Domain ) . filter ( Domain . name . ilike ( '%%%s%%' % query ) ) . all ( ) return results
def srid ( self ) : epsg id = ( self . Get Authority Code ( 'PROJCS' ) or self . Get Authority Code ( 'GEOGCS' ) ) try : return int ( epsg id ) except Type Error : return
def main ( ) : args = get args ( ) ret code = args . target ( args ) logger . debug ( 'Exiting with code %d' , ret code ) sys . exit ( ret code )
def update file ( url , filename ) : resp = urlopen ( url ) if resp . code != 200 : raise Exception ( 'GET {} failed.' . format ( url ) ) with open ( get package path ( filename ) , 'w' ) as fp : for l in resp : if not l . startswith ( b'#' ) : fp . write ( l . decode ( 'utf8' ) ) print ( 'Updated {}' . format ( filename ) )
def driver ( self ) : if self . driver is None : self . driver = Image Driver ( self . ds . Get Driver ( ) ) return self . driver
def lookup alphabet ( charset ) : if charset in PRESETS : return PRESETS [ charset ] if len ( charset ) < 16 : logger . warning ( 'very small alphabet in use, possibly a failed lookup?' ) return charset
def chunk to long ( self , chunk ) : return sum ( [ 256 ** ( self . chunklen [ 0 ] - 1 - i ) * ord byte ( chunk [ i ] ) for i in range ( self . chunklen [ 0 ] ) ] )
def get chunk ( self , data , index ) : return data [ index * self . chunklen [ 0 ] : ( index + 1 ) * self . chunklen [ 0 ] ]
def memoize ( func ) : cache = { } @ wraps ( func ) def inner ( filename ) : if filename not in cache : cache [ filename ] = func ( filename ) return cache [ filename ] return inner
def regexp ( filename ) : lines = get resource content ( filename ) . decode ( 'utf-8' ) . splitlines ( ) return re . compile ( '|' . join ( lines ) )
def detect timezone ( ) : default timezone = 'America/New York' locale code = locale . getdefaultlocale ( ) return default timezone if not locale code [ 0 ] else str ( pytz . country timezones [ locale code [ 0 ] [ - 2 : ] ] [ 0 ] )
def to dict ( self ) : result = { } for attr , in iteritems ( self . swagger types ) : value = getattr ( self , attr ) if isinstance ( value , list ) : result [ attr ] = list ( map ( lambda x : x . to dict ( ) if hasattr ( x , "to dict" ) else x , value ) ) elif hasattr ( value , "to dict" ) : result [ attr ] = value . to dict ( ) else : result [ attr ] = value return result
def activate pdb hook ( ) : def debug exception ( type exception , value , tb ) : import pdb pdb . post mortem ( tb ) import sys sys . excepthook = debug exception
def worker main ( job handler , host , port ) : loop = asyncio . new event loop ( ) asyncio . set event loop ( None ) loop . run until complete ( handle jobs ( job handler , host , port , loop = loop ) ) loop . close ( )
def send message ( self , msg ) : LW Link . the queue . put nowait ( msg ) if LW Link . thread is None or not LW Link . thread . is Alive ( ) : LW Link . thread = Thread ( target = self . send queue ) LW Link . thread . start ( )
def turn on light ( self , device id , name ) : msg = "!%s Fd P32|Turn On|%s" % ( device id , name ) self . send message ( msg )
def turn on switch ( self , device id , name ) : msg = "!%s F1|Turn On|%s" % ( device id , name ) self . send message ( msg )
def turn on with brightness ( self , device id , name , brightness ) : brightness value = round ( ( brightness * 31 ) / 255 ) + 1 # F1 = Light on and F0 = light off. Fd P[0..32] is brightness. 32 is # full. We want that when turning the light on. msg = "!%s Fd P%d|Lights %d|%s" % ( device id , brightness value , brightness value , name ) self . send message ( msg )
def turn off ( self , device id , name ) : msg = "!%s F0|Turn Off|%s" % ( device id , name ) self . send message ( msg )
def send queue ( self ) : while not LW Link . the queue . empty ( ) : self . send reliable message ( LW Link . the queue . get nowait ( ) )
def send reliable message ( self , msg ) : result = False max retries = 15 trans id = next ( LW Link . transaction id ) msg = "%d,%s" % ( trans id , msg ) try : with socket . socket ( socket . AF INET , socket . SOCK DGRAM ) as write sock , socket . socket ( socket . AF INET , socket . SOCK DGRAM ) as read sock : write sock . setsockopt ( socket . SOL SOCKET , socket . SO REUSEADDR , 1 ) read sock . setsockopt ( socket . SOL SOCKET , socket . SO BROADCAST , 1 ) read sock . settimeout ( self . SOCKET TIMEOUT ) read sock . bind ( ( '0.0.0.0' , self . RX PORT ) ) while max retries : max retries -= 1 write sock . sendto ( msg . encode ( 'UTF-8' ) , ( LW Link . link ip , self . TX PORT ) ) result = False while True : response , dummy = read sock . recvfrom ( 1024 ) response = response . decode ( 'UTF-8' ) if "Not yet registered." in response : LOGGER . error ( "Not yet registered" ) self . register ( ) result = True break if response . startswith ( "%d,OK" % trans id ) : result = True break if response . startswith ( "%d,ERR" % trans id ) : LOGGER . error ( response ) break LOGGER . info ( response ) if result : break time . sleep ( 0.25 ) except socket . timeout : LOGGER . error ( "LW broker timeout!" ) return result except Exception as ex : LOGGER . error ( ex ) raise if result : LOGGER . info ( "LW broker OK!" ) else : LOGGER . error ( "LW broker fail!" ) return result
def reset ( self ) : for opt , meta in self . defaults ( ) : self [ opt ] = meta . default
def names ( section , option ) : meta = section . def [ option ] action = meta . cmd kwargs . get ( 'action' ) if action is internal . Switch : names = [ '-{}' . format ( option ) , '+{}' . format ( option ) ] if meta . shortname is not None : names . append ( '-{}' . format ( meta . shortname ) ) names . append ( '+{}' . format ( meta . shortname ) ) else : names = [ '--{}' . format ( option ) ] if meta . shortname is not None : names . append ( '-{}' . format ( meta . shortname ) ) return names
def cmd opts solver ( self , cmd name ) : sections = self . sections list ( cmd name ) cmd dict = self . opt cmds [ cmd name ] if cmd name else self . opt bare for sct in reversed ( sections ) : for opt , opt meta in self . conf [ sct ] . def . items ( ) : if not opt meta . cmd arg : continue if opt not in cmd dict : cmd dict [ opt ] = sct else : warnings . warn ( 'Command <{0}>: {1}.{2} shadowed by {3}.{2}' . format ( cmd name , sct , opt , cmd dict [ opt ] ) , error . Loam Warning , stacklevel = 4 )
def add options to parser ( self , opts dict , parser ) : store bool = ( 'store true' , 'store false' ) for opt , sct in opts dict . items ( ) : meta = self . conf [ sct ] . def [ opt ] kwargs = copy . deepcopy ( meta . cmd kwargs ) action = kwargs . get ( 'action' ) if action is internal . Switch : kwargs . update ( nargs = 0 ) elif meta . default is not None and action not in store bool : kwargs . setdefault ( 'type' , type ( meta . default ) ) kwargs . update ( help = meta . help ) kwargs . setdefault ( 'default' , self . conf [ sct ] [ opt ] ) parser . add argument ( * names ( self . conf [ sct ] , opt ) , * * kwargs )
async def start master ( host = "" , port = 48484 , * , loop = None ) : loop = loop if loop is not None else asyncio . get event loop ( ) manager = jobs . Job Manager ( loop = loop ) workers = set ( ) server = await loop . create server ( lambda : Worker Protocol ( manager , workers ) , host , port ) return Master ( server , manager , workers , loop = loop )
def run ( self , job list ) : if self . closed : raise Runtime Error ( "master is closed" ) return self . manager . add job set ( job list )
def add ( self , result ) : assert not self . complete self . results . append ( result ) self . change ( )
def done ( self ) : self . results . complete ( ) waiters = self . waiters for waiter in waiters : waiter . set result ( None ) self . manager . job set done ( self )
def get job ( self , callback ) : assert not self . closed if self . active js is None or not self . active js . job available ( ) : self . ready callbacks . append ( callback ) else : job = self . active js . get job ( ) self . job sources [ job ] = self . active js callback ( job )
def return job ( self , job ) : if self . closed : return js = self . job sources [ job ] if len ( self . ready callbacks ) > 0 : callback = self . ready callbacks . popleft ( ) callback ( job ) else : del self . job sources [ job ] js . return job ( job )
def uniquify ( list ) : seen = set ( ) result = [ ] for x in list : if x not in seen : result . append ( x ) seen . add ( x ) return result
def get region ( ) : global REGION if REGION is None : region name = os . getenv ( "AWS DEFAULT REGION" ) or "us-east-1" region dict = { r . name : r for r in boto . regioninfo . get regions ( "ec2" ) } if region name not in region dict : raise Value Error ( "No such EC2 region: {}. Check AWS DEFAULT REGION " "environment variable" . format ( region name ) ) REGION = region dict [ region name ] return REGION
def sort by ( cls , entries , attribute ) : def key ( entry ) : return entry . get attrib ( attribute , convert to str = True ) return sorted ( entries , key = key )
def add timestamp ( logger class , log method , event dict ) : event dict [ 'timestamp' ] = calendar . timegm ( time . gmtime ( ) ) return event dict
def logger ( name = name , output = None , uuid = False , timestamp = False ) : processors = [ ] if output == 'json' : processors . append ( structlog . processors . JSON Renderer ( ) ) if uuid : processors . append ( add unique id ) if uuid : processors . append ( add timestamp ) return structlog . wrap logger ( logbook . Logger ( name ) , processors = processors )
def setup ( title , output = 'json' , timezone = None ) : timezone = timezone or dna . time utils . detect timezone ( ) broker url = 'redis://{}:{}/{}' . format ( os . environ . get ( 'BROKER HOST' , 'localhost' ) , os . environ . get ( 'BROKER PORT' , 6379 ) , 0 ) app = Celery ( title , broker = broker url ) app . conf . update ( CELERY TASK SERIALIZER = output , CELERY ACCEPT CONTENT = [ output ] , # Ignore other content CELERY RESULT SERIALIZER = output , CELERY RESULT BACKEND = broker url , CELERY TIMEZONE = timezone , CELERYD FORCE EXECV = True , CELERY ENABLE UTC = True , CELERY IGNORE RESULT = False ) return app
def delete ( self , worker id ) : code = 200 if worker id in self . jobs : # NOTE pop it if done ? self . jobs [ worker id ] [ 'worker' ] . revoke ( terminate = True ) report = { 'id' : worker id , 'revoked' : True # FIXME Unable to serialize self.jobs[worker id] # 'session': self.jobs.pop(worker id) } self . jobs . pop ( worker id ) else : report = { 'error' : 'job {} unknown' . format ( worker id ) } code = 404 return flask . jsonify ( report ) , code
def color ( number ) : if supports 256 ( ) : template = "\033[38;5;{number}m{text}\033[0m" else : template = "\033[{number}m{text}\033[0m" def color ( text ) : if not all ( [ sys . stdout . isatty ( ) , sys . stderr . isatty ( ) ] ) : return text else : return template . format ( number = number , text = text ) return color
def get color hash ( string , min = MIN COLOR BRIGHT , max = MAX COLOR BRIGHT ) : hash num = int ( hashlib . sha1 ( string . encode ( 'utf-8' ) ) . hexdigest ( ) [ : 6 ] , 16 ) range = max - min num in range = hash num % range return color ( min + num in range )
def random color ( min = MIN COLOR , max = MAX COLOR ) : return color ( random . randint ( min , max ) )
def requires token auth ( resource ) : @ functools . wraps ( resource ) def decorated ( * args , * * kwargs ) : ''' Check provided token ''' token = flask . request . headers . get ( 'Authorization' ) user = check token ( token ) if not token or user is None : log . warn ( 'authentification failed' , token = token ) return auth failed ( ) flask . g . user = user log . info ( 'authentification succeeded' , token = token , user = flask . g . user ) return resource ( * args , * * kwargs ) return decorated
def is running ( process ) : try : pgrep = sh . Command ( '/usr/bin/pgrep' ) pgrep ( process ) flag = True except sh . Error Return Code 1 : flag = False return flag
def dynamic import ( mod path , obj name = None ) : try : module = import ( mod path , fromlist = [ 'whatever' ] ) except Import Error , error : raise errors . Dynamic Import Failed ( module = '.' . join ( [ mod path , obj name ] ) , reason = error ) # Make sure we're up-to-date reload ( module ) if obj name is None : obj = module elif hasattr ( module , obj name ) : obj = getattr ( module , obj name ) else : raise errors . Dynamic Import Failed ( module = '.' . join ( [ mod path , obj name ] ) , reason = 'module {} has no attribute {}' . format ( module . name , obj name ) ) return None return obj
def self ip ( public = False ) : try : if public : data = str ( urlopen ( 'http://checkip.dyndns.com/' ) . read ( ) ) ip addr = re . compile ( r'Address: (\d+\.\d+\.\d+\.\d+)' ) . search ( data ) . group ( 1 ) else : sock = socket . socket ( socket . AF INET , socket . SOCK DGRAM ) sock . connect ( ( 'google.com' , 0 ) ) ip addr = sock . getsockname ( ) [ 0 ] except Exception , error : print ( 'Online test failed : {}' . format ( error ) ) raise return ip addr
def request ( self , method , url , query params = None , headers = None , post params = None , body = None ) : if method == "GET" : return self . rest client . GET ( url , query params = query params , headers = headers ) elif method == "HEAD" : return self . rest client . HEAD ( url , query params = query params , headers = headers ) elif method == "OPTIONS" : return self . rest client . OPTIONS ( url , query params = query params , headers = headers , post params = post params , body = body ) elif method == "POST" : return self . rest client . POST ( url , query params = query params , headers = headers , post params = post params , body = body ) elif method == "PUT" : return self . rest client . PUT ( url , query params = query params , headers = headers , post params = post params , body = body ) elif method == "PATCH" : return self . rest client . PATCH ( url , query params = query params , headers = headers , post params = post params , body = body ) elif method == "DELETE" : return self . rest client . DELETE ( url , query params = query params , headers = headers ) else : raise Value Error ( "http method must be `GET`, `HEAD`," " `POST`, `PATCH`, `PUT` or `DELETE`." )
def serve ( self , app docopt = DEFAULT DOC , description = '' ) : exit status = 0 if isinstance ( app docopt , str ) : args = docopt ( app docopt , version = description ) elif isinstance ( app docopt , dict ) : args = app docopt else : raise Value Error ( 'unknown configuration object ({})' . format ( type ( app docopt ) ) ) log level = args . get ( '--log' , 'debug' ) is debug = args . get ( '--debug' , False ) # TODO More serious default log output = 'stdout' if is debug else 'apy.log' safe bind = args . get ( '--bind' , '127.0.0.1' ) safe port = int ( args . get ( '--port' , 5000 ) ) log setup = dna . logging . setup ( level = log level , output = log output ) with log setup . applicationbound ( ) : try : log . info ( 'server ready' , version = description , log = log level , debug = is debug , bind = '{}:{}' . format ( safe bind , safe port ) ) self . app . run ( host = safe bind , port = safe port , debug = is debug ) except Exception as error : if is debug : raise log . error ( '{}: {}' . format ( type ( error ) . name , str ( error ) ) ) exit status = 1 finally : log . info ( 'session ended with status {}' . format ( exit status ) ) return exit status
def render ( self , name , value , attrs = None ) : context = attrs or { } context . update ( { 'name' : name , 'value' : value , } ) return render to string ( self . template name , context )
def networkdays ( from date , to date , locale = 'en-US' ) : holidays = locales [ locale ] return workdays . networkdays ( from date , to date , holidays )
def get path ( cmd ) : if cmd in PATHS : return PATHS [ cmd ] out = subprocess . check output ( 'which {}' . format ( cmd ) , shell = True ) PATHS [ cmd ] = out . decode ( "utf-8" ) . strip ( ) return PATHS [ cmd ]
def build ssh command ( hostname , username , idfile , ssh command , tunnel ) : command = [ get path ( 'ssh' ) , '-o' , 'Strict Host Key Checking=no' , '-o' , 'Connect Timeout=5' ] if idfile is not None : command . extend ( [ '-i' , idfile ] ) if tunnel is not None : # If there's a tunnel, run the ssh command on the tunneled host. command . extend ( [ '-A' , '-t' , tunnel , 'ssh' , '-A' , '-t' ] ) if username is not None : command . append ( '{}@{}' . format ( username , hostname ) ) else : command . append ( hostname ) if ssh command is not None : command . append ( repr ( ssh command ) ) return ( ' ' . join ( command ) )
def get args ( ) : parser = argparse . Argument Parser ( description = 'List EC2 instances' ) parser . add argument ( '-l' , '--latest' , action = 'store true' , default = False , help = 'Query AWS for latest instances' ) parser . add argument ( '--version' , action = 'store true' , default = False , help = 'Print version and exit' ) parser . add argument ( '--refresh-only' , action = 'store true' , default = False , help = 'Refresh cache and exit' ) parser . add argument ( '--host' , help = 'Specific host to list' , default = None ) parser . add argument ( '-s' , '--ssh' , action = 'store true' , help = 'SSH to instance' , default = False ) parser . add argument ( '-i' , '--identity-file' , help = 'SSH identify file' , default = None ) parser . add argument ( '-u' , '--username' , default = None , help = 'Log in as this user' ) parser . add argument ( 'filters' , nargs = '*' , help = 'Text filters for output lines' ) parser . add argument ( '-v' , '--exclude' , nargs = '+' , help = 'Exclude results that match these' ) parser . add argument ( '-c' , '--command' , type = str , help = 'Command to run on matching instance(s)' ) parser . add argument ( '-y' , '--no-prompt' , action = 'store true' , default = False , help = "Don't ask for confirmation before running a " "command" ) parser . add argument ( '-p' , '--profile' , type = str , help = 'Profile to use (defined in ~/.lsi)' ) parser . add argument ( '--show' , nargs = '+' , default = None , help = 'Instance attributes to show' ) parser . add argument ( '--only' , nargs = '+' , default = None , help = 'Show ONLY these instance attributes' ) parser . add argument ( '--sep' , type = str , default = None , help = 'Simple output with given separator' ) parser . add argument ( '--sort-by' , type = str , default = "name" , help = 'What to sort list by' ) parser . add argument ( '-L' , '--limit' , type = int , default = None , help = 'Show at most this many entries' ) parser . add argument ( '--attributes' , action = 'store true' , help = 'Show all available attributes' ) parser . add argument ( '--get' , nargs = 2 , default = None , help = 'Get files from matching instances, must be ' 'followed by the source and destination filenames' ) parser . add argument ( '--put' , nargs = 2 , default = None , help = 'Put a local file on matching instances, must be ' 'followed by the source and destination filenames' ) parser . add argument ( '-t' , '--tunnel' , default = None , help = 'Connect via the tunneled host.' ) parser . add argument ( "-r" , "--random" , action = "store true" , default = False , help = "Choose a random instance from within results." ) parser . add argument ( "-n" , "--num" , type = int , default = None , help = "Choose the given number from within results." ) args = parser . parse args ( ) if args . exclude is None : args . exclude = [ ] # Presumably, if someone is sorting by something, they want to show that # thing... if args . sort by is not None : args . show = ( args . show or [ ] ) + [ args . sort by ] return args
def load ( cls , profile name = None ) : lsi location = os . path . expanduser ( '~/.lsi' ) if not os . path . exists ( lsi location ) : return Lsi Profile ( ) cfg parser = Config Parser ( ) cfg parser . read ( lsi location ) if profile name is None : # Load the default profile if one exists; otherwise return empty. if cfg parser . has section ( 'default' ) : profile name = 'default' else : return cls ( ) elif not cfg parser . has section ( profile name ) : raise cls . Load Error ( 'No such profile {}' . format ( profile name ) ) def get ( option , alt = None ) : """Gets an option if it exists; else returns `alt`.""" if cfg parser . has option ( profile name , option ) : return cfg parser . get ( profile name , option ) else : return alt if cfg parser . has option ( profile name , 'inherit' ) : profile = cls . load ( cfg parser . get ( profile name , 'inherit' ) ) else : profile = cls ( ) profile . override ( 'username' , get ( 'username' ) ) profile . override ( 'identity file' , get ( 'identity file' ) ) profile . override ( 'command' , get ( 'command' ) ) filters = [ s for s in get ( 'filters' , '' ) . split ( ',' ) if len ( s ) > 0 ] exclude = [ s for s in get ( 'exclude' , '' ) . split ( ',' ) if len ( s ) > 0 ] profile . filters . extend ( filters ) profile . exclude . extend ( exclude ) return profile
def from args ( args ) : # If the args specify a username explicitly, don't load from file. if args . username is not None or args . identity file is not None : profile = Lsi Profile ( ) else : profile = Lsi Profile . load ( args . profile ) profile . override ( 'username' , args . username ) profile . override ( 'identity file' , args . identity file ) profile . override ( 'command' , args . command ) profile . no prompt = args . no prompt profile . filters . extend ( args . filters ) profile . exclude . extend ( args . exclude ) if profile . identity file is not None : profile . identity file = os . path . expanduser ( profile . identity file ) return profile
def relate ( self , part , id = None ) : assert part . name . startswith ( self . base ) name = part . name [ len ( self . base ) : ] . lstrip ( '/' ) rel = Relationship ( self , name , part . rel type , id = id ) self . relationships . add ( rel ) return rel
def related ( self , reltype ) : parts = [ ] package = getattr ( self , 'package' , None ) or self for rel in self . relationships . types . get ( reltype , [ ] ) : parts . append ( package [ posixpath . join ( self . base , rel . target ) ] ) return parts
def load rels ( self , source ) : # don't get confused here - the original source is string data; #  the parameter source below is a Part object self . relationships . load ( source = self , data = source )
def load part ( self , rel type , name , data ) : if self . content types . find for ( name ) is None : log . warning ( 'no content type found for part %(name)s' % vars ( ) ) return cls = Part . classes by rel type [ rel type ] part = cls ( self , name ) part . load ( data ) self [ name ] = part return part
def find for ( self , name ) : map = self . items # first search the overrides (by name) # then fall back to the defaults (by extension) # finally, return None if unmatched return map . get ( name , None ) or map . get ( get ext ( name ) or None , None )
def from element ( cls , element ) : # disambiguate the subclass ns , class name = parse tag ( element . tag ) class = getattr ( Content Type , class name ) if not class : msg = 'Invalid Types child element: %(class name)s' % vars ( ) raise Value Error ( msg ) # construct the subclass key = element . get ( class . key name ) name = element . get ( 'Content Type' ) return class ( name , key )
def as stream ( self ) : stream = io . Bytes IO ( ) self . store ( stream ) stream . seek ( 0 ) return stream
def loud ( self , lang = 'englist' ) : lang method = getattr ( self , lang , None ) if lang method : return lang method ( ) . upper ( ) else : return self . english ( ) . upper ( )
def upload ( ctx , product , git ref , dirname , aws id , aws secret , ci env , on travis push , on travis pr , on travis api , on travis cron , skip upload ) : logger = logging . get Logger ( name ) if skip upload : click . echo ( 'Skipping ltd upload.' ) sys . exit ( 0 ) logger . debug ( 'CI environment: %s' , ci env ) logger . debug ( 'Travis events settings. ' 'On Push: %r, PR: %r, API: %r, Cron: %r' , on travis push , on travis pr , on travis api , on travis cron ) # Abort upload on Travis CI under certain events if ci env == 'travis' and should skip travis event ( on travis push , on travis pr , on travis api , on travis cron ) : sys . exit ( 0 ) # Authenticate to LTD Keeper host ensure login ( ctx ) # Detect git refs git refs = get git refs ( ci env , git ref ) build resource = register build ( ctx . obj [ 'keeper hostname' ] , ctx . obj [ 'token' ] , product , git refs ) logger . debug ( 'Created build resource %r' , build resource ) # Do the upload. # This cache control is appropriate for builds since they're immutable. # The LTD Keeper server changes the cache settings when copying the build # over to be a mutable edition. upload dir ( build resource [ 'bucket name' ] , build resource [ 'bucket root dir' ] , dirname , aws access key id = aws id , aws secret access key = aws secret , surrogate key = build resource [ 'surrogate key' ] , cache control = 'max-age=31536000' , surrogate control = None , upload dir redirect objects = True ) logger . debug ( 'Upload complete for %r' , build resource [ 'self url' ] ) # Confirm upload confirm build ( build resource [ 'self url' ] , ctx . obj [ 'token' ] ) logger . debug ( 'Build %r complete' , build resource [ 'self url' ] )
def part edit cmd ( ) : parser = argparse . Argument Parser ( description = inspect . getdoc ( part edit cmd ) ) parser . add argument ( 'path' , help = 'Path to part (including path to zip file, i.e. ./file.zipx/part)' , ) parser . add argument ( '--reformat-xml' , action = 'store true' , help = ( 'run the content through an XML pretty-printer ' 'first for improved editability' ) , ) args = parser . parse args ( ) part edit ( args . path , args . reformat xml )
def pack dir cmd ( ) : parser = argparse . Argument Parser ( description = inspect . getdoc ( part edit cmd ) ) parser . add argument ( 'path' , help = ( 'Path to list (including path to zip file, ' 'i.e. ./file.zipx or ./file.zipx/subdir)' ) , ) args = parser . parse args ( ) for item , is file in sorted ( list contents ( args . path ) ) : prefix = 'd ' if not is file else '  ' msg = prefix + item print ( msg )
def process module ( self , node ) : if self . config . file header : if sys . version info [ 0 ] < 3 : pattern = re . compile ( '\A' + self . config . file header , re . LOCALE | re . MULTILINE ) else : # The use of re.LOCALE is discouraged in python 3 pattern = re . compile ( '\A' + self . config . file header , re . MULTILINE ) content = None with node . stream ( ) as stream : # Explicit decoding required by python 3 content = stream . read ( ) . decode ( 'utf-8' ) matches = pattern . findall ( content ) if len ( matches ) != 1 : self . add message ( 'invalid-file-header' , 1 , args = self . config . file header )
def html ( self , slug , name , chart obj , filepath = None , html before = "" , html after = "" ) : try : html = "" if name : html = "<h3>" + name + "</h3>" json data = chart obj . to json ( ) json data = self . patch json ( json data ) html = html before + html + self . json to html ( slug , json data ) + html after except Exception as e : tr . new ( e ) tr . check ( ) # generate file if filepath is not None : self . write file ( slug , filepath , html ) return None else : return html
def patch json ( self , json data ) : json data = json . loads ( json data ) # add schema json data [ "$schema" ] = "https://vega.github.io/schema/vega-lite/2.0.0-beta.15.json" # add top level width and height json data [ "width" ] = json data [ "config" ] [ "cell" ] [ "width" ] json data [ "height" ] = json data [ "config" ] [ "cell" ] [ "height" ] del ( json data [ "config" ] [ "cell" ] ) return json . dumps ( json data )
def json to html ( self , slug , json data ) : html = '<div id="chart-' + slug + '"></div>' html += '<script>' html += 'var s' + slug + ' = ' + json data + ';' html += 'vega.embed("#chart-' + slug + '", s' + slug + ');' #html += 'console.log(JSON.stringify(s{id}, null, 2));' html += '</script>' return html
def dict to df ( self , dictobj , xfield , yfield ) : x = [ ] y = [ ] for datapoint in dictobj : x . append ( datapoint ) y . append ( dictobj [ datapoint ] ) df = pd . Data Frame ( { xfield [ 0 ] : x , yfield [ 0 ] : y } ) return df
def write file ( self , slug , folderpath , html ) : # check directories if not os . path . isdir ( folderpath ) : try : os . makedirs ( folderpath ) except Exception as e : tr . err ( e ) # construct file path filepath = folderpath + "/" + slug + ".html" #~ write the file try : filex = open ( filepath , "w" ) filex . write ( html ) filex . close ( ) except Exception as e : tr . err ( e )
def chart class ( self , df , chart type , * * kwargs ) : if chart type == "bar" : return Chart ( df ) . mark bar ( * * kwargs ) elif chart type == "circle" : return Chart ( df ) . mark circle ( * * kwargs ) elif chart type == "line" : return Chart ( df ) . mark line ( * * kwargs ) elif chart type == "point" : return Chart ( df ) . mark point ( * * kwargs ) elif chart type == "area" : return Chart ( df ) . mark area ( * * kwargs ) elif chart type == "tick" : return Chart ( df ) . mark tick ( * * kwargs ) elif chart type == "text" : return Chart ( df ) . mark text ( * * kwargs ) elif chart type == "square" : return Chart ( df ) . mark square ( * * kwargs ) elif chart type == "rule" : return Chart ( df ) . mark rule ( * * kwargs ) return None
def encode fields ( self , xfield , yfield , time unit = None , scale = Scale ( zero = False ) ) : if scale is None : scale = Scale ( ) xfieldtype = xfield [ 1 ] yfieldtype = yfield [ 1 ] x options = None if len ( xfield ) > 2 : x options = xfield [ 2 ] y options = None if len ( yfield ) > 2 : y options = yfield [ 2 ] if time unit is not None : if x options is None : xencode = X ( xfieldtype , time Unit = time unit ) else : xencode = X ( xfieldtype , axis = Axis ( * * x options ) , time Unit = time unit , scale = scale ) else : if x options is None : xencode = X ( xfieldtype ) else : xencode = X ( xfieldtype , axis = Axis ( * * x options ) , scale = scale ) if y options is None : yencode = Y ( yfieldtype , scale = scale ) else : yencode = Y ( yfieldtype , axis = Axis ( * * y options ) , scale = scale ) return xencode , yencode
def infer tarball url ( ) : try : with click . open file ( 'app.json' , 'r' ) as f : contents = f . read ( ) app json = json . loads ( contents ) except IO Error : return None repository = app json . get ( 'repository' ) if not repository : return None else : return app json . get ( 'repository' ) + '/tarball/master/'
def up ( tarball url , auth token , env , app name ) : tarball url = tarball url or infer tarball url ( ) if not tarball url : click . echo ( 'No tarball URL found.' ) sys . exit ( 1 ) if env : # Split ["KEY=value", ...] into {"KEY": "value", ...} env = { arg . split ( '=' ) [ 0 ] : arg . split ( '=' ) [ 1 ] for arg in env } happy = Happy ( auth token = auth token ) click . echo ( 'Creating app... ' , nl = False ) build id , app name = happy . create ( tarball url = tarball url , env = env , app name = app name , ) click . echo ( app name ) click . echo ( 'Building... ' , nl = False ) happy . wait ( build id ) write app name ( app name ) click . echo ( 'done' ) click . echo ( "It's up! :) https://%s.herokuapp.com" % app name )
def down ( auth token , force , app name ) : if not app name : click . echo ( 'WARNING: Inferring the app name when deleting is deprecated. ' 'Starting with happy 2.0, the app name parameter will be required.' ) app name = app name or read app name ( ) if not app name : click . echo ( 'No app name given.' ) sys . exit ( 1 ) if not force : click . confirm ( 'Are you sure you want to delete %s?' % app name , abort = True , ) happy = Happy ( auth token = auth token ) click . echo ( 'Destroying app %s... ' % app name , nl = False ) happy . delete ( app name = app name ) delete app name file ( ) click . echo ( 'done' ) click . echo ( "It's down. :(" )
def date ( start , end ) : stime = date to timestamp ( start ) etime = date to timestamp ( end ) ptime = stime + random . random ( ) * ( etime - stime ) return datetime . date . fromtimestamp ( ptime )
def get session ( self ) : session = Session ( ) session . headers = { 'Content-type' : 'application/json' , 'Accept' : 'application/vnd.heroku+json; version=3' , } if self . auth token : session . trust env = False # Effectively disable netrc auth session . headers [ 'Authorization' ] = 'Bearer %s' % self . auth token return session
def get root argparser ( self ) : return self . arg parse class ( description = self . get help ( ) , formatter class = self . get formatter class ( ) )
def get description ( self ) : if self . description : return self . description elif self . doc and self . doc . strip ( ) : return self . doc . strip ( ) . split ( '.' ) [ 0 ] + '.' else : return ''
def get help ( self ) : if self . help : return self . help elif self . doc and self . doc . strip ( ) : return self . doc . strip ( ) else : return ''
def get version ( ) : with open ( os . path . join ( os . path . dirname ( file ) , 'argparsetree' , ' init .py' ) ) as init py : return re . search ( ' version  = [\'"]([^\'"]+)[\'"]' , init py . read ( ) ) . group ( 1 )
def url with auth ( regex , view , kwargs = None , name = None , prefix = '' ) : from djapiauth . auth import api auth if isinstance ( view , six . string types ) : # view is a string, must be full path return url ( regex , api auth ( import by path ( prefix + "." + view if prefix else view ) ) ) elif isinstance ( view , ( list , tuple ) ) : # include return url ( regex , view , name , prefix , * * kwargs ) else : # view is an object return url ( regex , api auth ( view ) )
def render ( self ) : for opt , values in self . data . items ( ) : if opt == 'ticks' : self [ 'chxtc' ] = '|' . join ( values ) else : self [ 'chx%s' % opt [ 0 ] ] = '|' . join ( values ) return self
def dataset ( self , data , series = '' ) : self . dataset = data self . series = series return self
def render ( self ) : self . update ( self . axes . render ( ) ) encoder = Encoder ( self . encoding , None , self . series ) if not 'chs' in self : self [ 'chs' ] = '300x150' else : size = self [ 'chs' ] . split ( 'x' ) assert len ( size ) == 2 , 'Invalid size, must be in the format Wx H' self . check size ( * map ( int , size ) ) assert 'cht' in self , 'No chart type defined, use type method' self [ 'cht' ] = self . check type ( self [ 'cht' ] ) if ( 'any' in dir ( self . dataset ) and self . dataset . any ( ) ) or self . dataset : self [ 'chd' ] = encoder . encode ( self . dataset ) elif not 'choe' in self : assert 'chd' in self , 'You must have a dataset, or use chd' if self . scale : assert self [ 'chd' ] . startswith ( 't' ) , 'You must use text encoding with chds' self [ 'chds' ] = ',' . join ( self . scale ) if self . geo and self . ld : self [ 'chtm' ] = self . geo self [ 'chld' ] = self . ld if self . lines : self [ 'chls' ] = '|' . join ( self . lines ) if self . markers : self [ 'chm' ] = '|' . join ( self . markers ) if self . fills : self [ 'chf' ] = '|' . join ( self . fills )
def url ( self ) : self . render ( ) return self . apiurl + '&' . join ( self . parts ( ) ) . replace ( ' ' , '+' )
def urlopen ( self ) : req = Request ( str ( self ) ) try : return urlopen ( req ) except HTTP Error : print ( 'The server couldn\'t fulfill the request.' ) except URL Error : print ( 'We failed to reach a server.' )
def parse args ( ) : usage = "Usage: create concordance <infile> [<outfile>]" description = "Simple Concordance Generator" argparser = argparse . Argument Parser ( usage = usage , description = description ) argparser . add argument ( 'infile' , type = argparse . File Type ( 'r' ) , help = "File read in to create concordance" ) argparser . add argument ( 'outfile' , nargs = '?' , type = argparse . File Type ( 'w' ) , default = sys . stdout , help = "File to write concordance to.  " "Default is stdout" ) argparser . add argument ( '--word' , nargs = "?" , const = str , help = "Display a word in concordance" ) args = argparser . parse args ( ) return args
def add Command Line Args ( arg parser ) : arg parser . register ( "action" , "log levels" , Log Level Action ) arg parser . register ( "action" , "log files" , Log File Action ) arg parser . register ( "action" , "log help" , Log Help Action ) group = arg parser . add argument group ( "Logging options" ) group . add argument ( "-l" , "--log-level" , dest = "log levels" , action = "log levels" , metavar = "LOGGER:LEVEL" , default = [ ] , help = "Set log levels for individual loggers. See --help-logging for " "complete details." ) group . add argument ( "-L" , "--log-file" , dest = "log files" , action = "log files" , metavar = "LOGGER:FILE" , default = [ ] , help = "Set log the output file for individual loggers. " " See --help-logging for complete details." ) group . add argument ( "--help-logging" , action = "log help" , help = argparse . SUPPRESS )
def validate page number ( number ) : try : number = int ( number ) except ( Type Error , Value Error ) : raise Page Not An Integer ( 'That page number is not an integer' ) if number < 1 : raise Empty Page ( 'That page number is less than 1' ) return number
def chmod ( path , mode , recursive = True ) : if recursive : cmd = 'chmod -R %s %s' % ( mode , path ) else : cmd = 'chmod %s %s' % ( mode , path ) return sh ( cmd )
def create bundle ( self , data ) : kwargs = { } filters = None if isinstance ( data , dict ) : kwargs . update ( filters = data . get ( 'filters' , None ) , output = data . get ( 'output' , None ) , debug = data . get ( 'debug' , None ) , extra = data . get ( 'extra' , { } ) , config = data . get ( 'config' , { } ) , depends = data . get ( 'depends' , None ) ) bundle = Bundle ( * list ( self . yield bundle contents ( data ) ) , * * kwargs ) return self . auto filter bundle ( bundle )
def urls for ( self , asset type , * args , * * kwargs ) : return self . urls for depends ( asset type , * args , * * kwargs ) + self . urls for self ( asset type , * args , * * kwargs )
def html tags for ( self , asset type , * args , * * kwargs ) : html = [ ] for ref in self . depends : html . append ( self . ref ( ref ) . html tags for ( asset type , * args , * * kwargs ) ) if asset type in self . typed bundles : html . append ( render asset html tags ( asset type , self . urls for self ( asset type , * args , * * kwargs ) ) ) return "\n" . join ( html )
def html tags ( self , * args , * * kwargs ) : html = [ ] for asset type in list asset types ( ) : html . append ( self . html tags for ( asset type . name , * args , * * kwargs ) ) return "\n" . join ( html )
def find version ( filename ) : with io . open ( filename , encoding = "utf-8" ) as version file : version match = re . search ( r'^ version  = [\'"]([^\'"]*)[\'"]' , version file . read ( ) , re . M ) if version match : return version match . group ( 1 ) return "0.0-version-unknown"
def import modules ( self ) : modules = self . get modules ( ) log . info ( "import service modules: " + str ( modules ) ) try : for module in modules : import ( module ) except Import Error as error : raise Import Modules Error ( error . msg )
def send ( self , peer , typename , data ) : def attempt to send ( ) : if peer not in self . connections : d = self . connect ( peer ) d . add Callback ( attempt to send ) return d else : conn = self . connections [ peer ] [ 0 ] conn . send packet ( typename , data ) return defer . succeed ( None ) d = attempt to send ( None ) self . ongoing sends . add ( d ) def send completed ( result ) : if d in self . ongoing sends : self . ongoing sends . remove ( d ) return result d . add Both ( send completed ) return d
def receive Data ( self , connection , data ) : try : protocol = self . protocols [ connection ] except Key Error : raise No Such Connection ( ) protocol . data Received ( data ) return { }
def disconnect ( self , connection ) : proto = self . protocols . pop ( connection ) proto . transport = None return { }
def send Data ( self , data ) : d = self . call Remote ( Transmit , connection = self . connection , data = data ) d . add Errback ( log . err )
def get Local Protocol ( self , connection Identifier ) : for factory in self . local Factories : try : return factory . protocols [ connection Identifier ] except Key Error : continue raise No Such Connection ( )
def disconnect ( self , connection ) : proto = self . get Local Protocol ( connection ) proto . transport . lose Connection ( ) return { }
def centered ( mystring , linewidth = None , fill = " " ) : if linewidth is None : linewidth = get terminal size ( ) . columns - 1 sides = ( linewidth - length no ansi ( mystring ) ) // 2 extra = ( linewidth - length no ansi ( mystring ) ) % 2 fill = fill [ : 1 ] sidestring = fill * sides extrastring = fill * extra newstring = sidestring + mystring + sidestring + extrastring return newstring
def clock on right ( mystring ) : taken = length no ansi ( mystring ) padding = ( get terminal size ( ) . columns - 1 ) - taken - 5 clock = time . strftime ( "%I:%M" , time . localtime ( ) ) print ( mystring + " " * padding + clock )
def main ( arguments = None ) : if not arguments : arguments = sys . argv [ 1 : ] wordlist , sowpods , by length , start , end = argument parser ( arguments ) for word in wordlist : pretty print ( word , anagrams in word ( word , sowpods , start , end ) , by length , )
def ping ( self , peerid , callid ) : if not ( peerid , callid ) in self . remote to local : logger . warn ( "No remote call %s from %s. Might just be unfoutunate timing." % ( callid , peerid ) )
def cmd Regex ( self , cmd grp = None ) : cmd grp = cmd grp or "cmd" help opts = ( "-h" , "--help" ) cmd = self . name ( ) names = "|" . join ( [ re . escape ( cmd ) ] + [ re . escape ( a ) for a in self . aliases ( ) ] ) opts = [ ] for action in self . parser . actions : opts += [ a for a in action . option strings if a not in help opts ] opts re = "|" . join ( [ re . escape ( o ) for o in opts ] ) if opts re : opts re = rf"(\s+(?P<{cmd grp} opts>{opts re}))*" help re = "|" . join ( [ re . escape ( o ) for o in help opts ] ) help re = rf"(\s+(?P<HELP OPTS>{help re}))*" completers = { } if opts re : completers [ f"{cmd grp} opts" ] = Word Completer ( opts ) # Singe Help completer added elsewhere return tuple ( [ rf"""(?P<{cmd grp}>{names}){opts re}{help re}""" , completers ] )
def from String Proto ( self , in String , proto ) : value , = amp . Amp List . from String Proto ( self , in String , proto ) return value
def to String Proto ( self , in Object , proto ) : return amp . Amp List . to String Proto ( self , [ in Object ] , proto )
def connection ( username = None , password = None , host = None , port = None , db = None ) : c opts = { } if username : c opts [ 'user' ] = username if password : c opts [ 'password' ] = password if host : c opts [ 'host' ] = host if port : c opts [ 'port' ] = port if db : c opts [ 'database' ] = db dbc = psycopg2 . connect ( * * c opts ) dbc . autocommit = True return dbc
def db list ( username = None , password = None , host = None , port = None , maintain db = 'postgres' ) : conn = connection ( username = username , password = password , host = host , port = port , db = maintain db ) cur = conn . cursor ( ) cur . execute ( 'SELECT DATNAME from pg database' ) rows = cur . fetchall ( ) conn . close ( ) result = [ ] for row in rows : result . append ( row [ 0 ] ) return result
def get local files ( self , path ) : if not path : raise Value Error ( "No path specified" ) files = defaultdict ( lambda : None ) path len = len ( path ) + 1 for root , dirs , filenames in os . walk ( path ) : for name in filenames : full path = join ( root , name ) files [ full path [ path len : ] ] = compute md5 ( full path ) return files
def tokens required ( service list ) : def decorator ( func ) : @ wraps ( func ) def inner ( request , * args , * * kwargs ) : for service in service list : if service not in request . session [ "user tokens" ] : return redirect ( 'denied' ) return func ( request , * args , * * kwargs ) return inner return decorator
def login ( request , template name = 'ci/login.html' , redirect field name = REDIRECT FIELD NAME , authentication form = Authentication Form ) : redirect to = request . POST . get ( redirect field name , request . GET . get ( redirect field name , '' ) ) if request . method == "POST" : form = authentication form ( request , data = request . POST ) if form . is valid ( ) : # Ensure the user-originating redirection url is safe. if not is safe url ( url = redirect to , host = request . get host ( ) ) : redirect to = resolve url ( settings . LOGIN REDIRECT URL ) # Okay, security check complete. Get the user object from auth api. user = form . get user ( ) request . session [ 'user token' ] = user [ "token" ] request . session [ 'user email' ] = user [ "email" ] request . session [ 'user permissions' ] = user [ "permissions" ] request . session [ 'user id' ] = user [ "id" ] request . session [ 'user list' ] = user [ "user list" ] if not settings . HIDE DASHBOARDS : # Set user dashboards because they are slow to change dashboards = ci Api . get user dashboards ( user [ "id" ] ) dashboard list = list ( dashboards [ 'results' ] ) if len ( dashboard list ) > 0 : request . session [ 'user dashboards' ] = dashboard list [ 0 ] [ "dashboards" ] request . session [ 'user default dashboard' ] = dashboard list [ 0 ] [ "default dashboard" ] [ "id" ] else : request . session [ 'user dashboards' ] = [ ] request . session [ 'user default dashboard' ] = None # Get the user access tokens too and format for easy access tokens = ci Api . get user service tokens ( params = { "user id" : user [ "id" ] } ) token list = list ( tokens [ 'results' ] ) user tokens = { } if len ( token list ) > 0 : for token in token list : user tokens [ token [ "service" ] [ "name" ] ] = { "token" : token [ "token" ] , "url" : token [ "service" ] [ "url" ] + "/api/v1" } request . session [ 'user tokens' ] = user tokens return Http Response Redirect ( redirect to ) else : form = authentication form ( request ) current site = get current site ( request ) context = { 'form' : form , redirect field name : redirect to , 'site' : current site , 'site name' : current site . name , } return Template Response ( request , template name , context )
def build ( cli , path , package ) : for , name , ispkg in iter modules ( path ) : module = import module ( f'.{name}' , package ) if ispkg : build ( cli . group ( name ) ( module . group ) , module . path , module . package ) else : cli . command ( name ) ( module . command )
def descovery ( testdir ) : from os . path import join , exists , isdir , splitext , basename , sep if not testdir or not exists ( testdir ) or not isdir ( testdir ) : return None from os import walk import fnmatch import imp for root , , filenames in walk ( testdir ) : for filename in fnmatch . filter ( filenames , '*.py' ) : path = join ( root , filename ) modulepath = splitext ( root ) [ 0 ] . replace ( sep , '.' ) imp . load source ( modulepath , path )
def main ( clargs = None ) : from argparse import Argument Parser from librarian . library import Library import sys parser = Argument Parser ( description = "A test runner for each card in a librarian library." ) parser . add argument ( "library" , help = "Library database" ) parser . add argument ( "-t" , "--tests" , default = "test/" , help = "Test directory" ) args = parser . parse args ( clargs ) descovery ( args . tests ) library = Library ( args . library ) cardcount , passes , failures = execute tests ( library ) print ( RESULTS . format ( len ( SINGLES ) , len ( TESTS ) , cardcount , passes , failures ) ) sys . exit ( failures )
def write Response ( self , response ) : encoded = dumps ( response , default = default ) self . transport . write ( encoded )
def connection Lost ( self , reason ) : self . remote . box Receiver . stop Receiving Boxes ( reason ) return basic . Netstring Receiver . connection Lost ( self , reason )
def build Protocol ( self , addr ) : proto = self . factory . build Protocol ( addr ) return JSONAMP Dialect Receiver ( proto )
def pout ( msg , log = None ) : print ( msg , sys . stdout , log func = log . info if log else None )
def perr ( msg , log = None ) : print ( msg , sys . stderr , log func = log . error if log else None )
def register ( Command Sub Class ) : name = Command Sub Class . name ( ) if name in Command . all commands : raise Value Error ( "Command already exists: " + name ) Command . all commands [ name ] = Command Sub Class return Command Sub Class
def register ( Class , Command Sub Class ) : for name in [ Command Sub Class . name ( ) ] + Command Sub Class . aliases ( ) : if name in Class . registered commands [ Class ] : raise Value Error ( "Command already exists: " + name ) Class . registered commands [ Class ] [ name ] = Command Sub Class return Command Sub Class
def init mq ( self ) : mq = self . init connection ( ) self . init consumer ( mq ) return mq . connection
def init modules ( self ) : if not self . config : raise Value Error ( "please read your config file." ) log . debug ( "begin to import customer's service modules." ) modules = Service Modules ( self . config ) modules . import modules ( ) log . debug ( "end to import customer's service modules." )
def music info ( songid ) : if isinstance ( songid , list ) : songid = ',' . join ( songid ) data = { "hq" : 1 , "song Ids" : songid } res = requests . post ( MUSIC INFO URL , data = data ) info = res . json ( ) music data = info [ "data" ] songs = [ ] for song in music data [ "song List" ] : song link , size = song link ( song , music data [ "xcode" ] ) songs . append ( { "name" : song [ "song Name" ] , "singer" : song [ "artist Name" ] , "lrc link" : song [ "lrc Link" ] , "song link" : song link , "size" : size } ) return songs
def download music ( song , thread num = 4 ) : filename = "{}.mp3" . format ( song [ "name" ] ) if os . path . exists ( filename ) : os . remove ( filename ) part = int ( song [ "size" ] / thread num ) if part <= 1024 : thread num = 1 id = uuid . uuid4 ( ) . hex logger . info ( "downloading '{}'..." . format ( song [ "name" ] ) ) threads = [ ] for i in range ( thread num ) : if i == thread num - 1 : end = '' else : end = ( i + 1 ) * part - 1 thread = Worker ( ( i * part , end ) , song , id ) thread . start ( ) threads . append ( thread ) for t in threads : t . join ( ) file Parts = glob . glob ( "part-{}-*" . format ( id ) ) file Parts . sort ( key = lambda e : e . split ( '-' ) [ - 1 ] ) logger . info ( "'{}' combine parts..." . format ( song [ "name" ] ) ) with open ( filename , "ab" ) as f : for part in file Parts : with open ( part , "rb" ) as d : shutil . copyfileobj ( d , f ) os . remove ( part ) logger . info ( "'{}' finished" . format ( song [ "name" ] ) )
def load name ( self , name ) : if name in self . globals : return self . globals [ name ] b = self . globals [ ' builtins ' ] if isinstance ( b , dict ) : return b [ name ] else : return getattr ( b , name )
def pop ( self , n ) : poped = self . stack [ len ( self . stack ) - n : ] del self . stack [ len ( self . stack ) - n : ] return poped
def connection ( username = None , password = None , host = None , port = None ) : c opts = { } if username : c opts [ 'user' ] = username if password : c opts [ 'passwd' ] = password if host : c opts [ 'host' ] = host if port : c opts [ 'port' ] = port dbc = My SQ Ldb . connect ( * * c opts ) dbc . autocommit ( True ) return dbc
def render ditaa ( self , code , options , prefix = 'ditaa' ) : hashkey = code . encode ( 'utf-8' ) + str ( options ) + str ( self . builder . config . ditaa ) + str ( self . builder . config . ditaa args ) infname = '%s-%s.%s' % ( prefix , sha ( hashkey ) . hexdigest ( ) , "ditaa" ) outfname = '%s-%s.%s' % ( prefix , sha ( hashkey ) . hexdigest ( ) , "png" ) inrelfn = posixpath . join ( self . builder . imgpath , infname ) infullfn = path . join ( self . builder . outdir , ' images' , infname ) outrelfn = posixpath . join ( self . builder . imgpath , outfname ) outfullfn = path . join ( self . builder . outdir , ' images' , outfname ) if path . isfile ( outfullfn ) : return outrelfn , outfullfn ensuredir ( path . dirname ( outfullfn ) ) # ditaa expects UTF-8 by default if isinstance ( code , unicode ) : code = code . encode ( 'utf-8' ) ditaa args = [ self . builder . config . ditaa ] ditaa args . extend ( self . builder . config . ditaa args ) ditaa args . extend ( options ) ditaa args . extend ( [ infullfn ] ) ditaa args . extend ( [ outfullfn ] ) f = open ( infullfn , 'w' ) f . write ( code ) f . close ( ) try : self . builder . warn ( ditaa args ) p = Popen ( ditaa args , stdout = PIPE , stdin = PIPE , stderr = PIPE ) except OS Error , err : if err . errno != ENOENT : # No such file or directory raise self . builder . warn ( 'ditaa command %r cannot be run (needed for ditaa ' 'output), check the ditaa setting' % self . builder . config . ditaa ) self . builder . ditaa warned dot = True return None , None went Wrong = False try : # Ditaa may close standard input when an error occurs, # resulting in a broken pipe on communicate() stdout , stderr = p . communicate ( code ) except OS Error , err : if err . errno != EPIPE : raise went Wrong = True except IO Error , err : if err . errno != EINVAL : raise went Wrong = True if went Wrong : # in this case, read the standard output and standard error streams # directly, to get the error message(s) stdout , stderr = p . stdout . read ( ) , p . stderr . read ( ) p . wait ( ) if p . returncode != 0 : raise Ditaa Error ( 'ditaa exited with error:\n[stderr]\n%s\n' '[stdout]\n%s' % ( stderr , stdout ) ) return outrelfn , outfullfn
def atexit ( self ) : self . log . debug ( "Application. atexit" ) if self . atexit func : self . atexit func ( self )
def run ( self , args list = None ) : self . log . debug ( "Application.run: {args list}" . format ( * * locals ( ) ) ) retval = None try : retval = self . run ( args list = args list ) except Keyboard Interrupt : self . log . verbose ( "Interrupted" ) # pragma: nocover except System Exit as exit : self . log . verbose ( "Exited" ) retval = exit . code except Exception : print ( "Uncaught exception" , file = sys . stderr ) traceback . print exc ( ) if "debug pdb" in self . args and self . args . debug pdb : debugger ( ) retval = Application . UNCAUGHT EXCEPTION EXIT raise finally : try : self . atexit ( ) finally : sys . stderr . flush ( ) sys . stdout . flush ( ) sys . exit ( retval )
def main ( ) : parser = optparse . Option Parser ( usage = "%prog [options] <model path> [another model path...]" , formatter = optparse . Titled Help Formatter ( ) ) parser . set description ( doc . strip ( ) ) parser . add option ( "-f" , "--function" , dest = "function" , metavar = "NAME" , help = "append integrity checking actions to functions named NAME (required)" , action = "store" , default = None ) parser . add option ( "-o" , "--output" , dest = 'output' , metavar = "PATH" , help = "save sql model instances to PATH (required)" , action = "store" , default = None ) parser . add option ( "-v" , "--verbosity" , dest = 'verbosity' , action = "count" , help = "increase debug logging level" , default = 2 ) ( opts , args ) = parser . parse args ( ) if len ( args ) == 0 or None in [ opts . output , opts . function ] : parser . print help ( ) sys . exit ( 1 ) levels = { 0 : logging . ERROR , 1 : logging . WARNING , 2 : logging . INFO , 3 : logging . DEBUG , } logging . basic Config ( level = levels . get ( opts . verbosity , logging . DEBUG ) ) m = ooaofooa . load metamodel ( args ) for c c in m . select many ( 'C C' ) : filt = lambda sel : ooaofooa . is contained in ( sel , c c ) and sel . Name == opts . function s sync = m . select any ( 'S SYNC' , filt ) if not s sync : s sync = m . new ( 'S SYNC' , Name = opts . function ) pe pe = m . new ( 'PE PE' ) s dt = m . select any ( 'S DT' , where ( Name = 'boolean' ) ) relate ( pe pe , s sync , 8001 ) relate ( s dt , s sync , 25 ) generate actions ( m , c c , s sync ) xtuml . persist instances ( m , opts . output )
def scrape ( ctx , url ) : data = load feed ( url ) feed = data [ 'feed' ] entries = data [ 'entries' ] # THIS IS SPECIFIC TO # http://konfery.cz/rss/ type = 'community' country = 'Czech Republic' # title, title detail, links, link, published, summary, tags # unused: summary detail, guidislink, published parsed for entry in entries : id = sluggify ( entry [ 'id' ] ) city = entry [ 'tags' ] [ 0 ] [ 'term' ] landing = entry [ 'link' ] start time = dt normalize ( entry [ 'published parsed' ] , local tz = True ) title = entry [ 'title' ] summary = entry [ 'summary' ] link = entry [ 'link' ] ipdb . set trace ( )
def fancy tag compiler ( params , defaults , takes var args , takes var kwargs , takes context , name , node class , parser , token ) : bits = token . split contents ( ) [ 1 : ] if takes context : if 'context' in params [ : 1 ] : params = params [ 1 : ] else : raise Template Syntax Error ( "Any tag function decorated with takes context=True " "must have a first argument of 'context'" ) # Split args and kwargs args = [ ] kwargs = { } kwarg found = False unhandled params = list ( params ) handled params = [ ] if len ( bits ) > 1 and bits [ - 2 ] == 'as' : output var = bits [ - 1 ] if len ( set ( output var ) - set ( ALLOWED VARIABLE CHARS ) ) > 0 : raise Template Syntax Error ( "%s got output var name with forbidden chars: '%s'" % ( name , output var ) ) bits = bits [ : - 2 ] else : output var = None for bit in bits : kwarg match = kwarg re . match ( bit ) if kwarg match : kw , var = kwarg match . groups ( ) if kw not in params and not takes var kwargs : raise Template Syntax Error ( "%s got unknown keyword argument '%s'" % ( name , kw ) ) elif kw in handled params : raise Template Syntax Error ( "%s got multiple values for keyword argument '%s'" % ( name , kw ) ) else : kwargs [ str ( kw ) ] = var kwarg found = True handled params . append ( kw ) else : if kwarg found : raise Template Syntax Error ( "%s got non-keyword arg after keyword arg" % name ) else : args . append ( bit ) try : handled params . append ( unhandled params . pop ( 0 ) ) except Index Error : if not takes var args : raise Template Syntax Error ( "%s got too many arguments" % name ) # Consider the last n params handled, where n is the number of defaults. if defaults is not None : unhandled params = unhandled params [ : - len ( defaults ) ] if len ( unhandled params ) == 1 : raise Template Syntax Error ( "%s didn't get a value for argument '%s'" % ( name , unhandled params [ 0 ] ) ) elif len ( unhandled params ) > 1 : raise Template Syntax Error ( "%s didn't get values for arguments: %s" % ( name , ', ' . join ( [ "'%s'" % p for p in unhandled params ] ) ) ) return node class ( args , kwargs , output var , takes context )
def get defining component ( pe pe ) : if pe pe is None : return None if pe pe . class . name != 'PE PE' : pe pe = xtuml . navigate one ( pe pe ) . PE PE [ 8001 ] ( ) ep pkg = xtuml . navigate one ( pe pe ) . EP PKG [ 8000 ] ( ) if ep pkg : return get defining component ( ep pkg ) return xtuml . navigate one ( pe pe ) . C C [ 8003 ] ( )
def main ( ) : parser = optparse . Option Parser ( usage = "%prog [options] <model path> [another model path..]" , version = xtuml . version . complete string , formatter = optparse . Titled Help Formatter ( ) ) parser . add option ( "-v" , "--verbosity" , dest = 'verbosity' , action = "count" , help = "increase debug logging level" , default = 1 ) parser . add option ( "-o" , "--output" , dest = "output" , metavar = "PATH" , help = "set output to PATH" , action = "store" , default = None ) ( opts , args ) = parser . parse args ( ) if len ( args ) == 0 or opts . output is None : parser . print help ( ) sys . exit ( 1 ) levels = { 0 : logging . ERROR , 1 : logging . WARNING , 2 : logging . INFO , 3 : logging . DEBUG , } logging . basic Config ( level = levels . get ( opts . verbosity , logging . DEBUG ) ) m = ooaofooa . load metamodel ( args ) prebuild model ( m ) xtuml . persist instances ( m , opts . output )
def find symbol ( self , name = None , kind = None ) : for s in reversed ( self . stack ) : for symbol name , handle in s . symbols . items ( ) : symbol kind = handle . class . name if name == symbol name and kind == symbol kind : return handle elif name is None and kind == handle . class . name : return handle elif name == symbol name and kind is None : return handle if name is None and kind == s . handle . class . name : return s . handle
def is contained in ( pe pe , root ) : if not pe pe : return False if type ( pe pe ) . name != 'PE PE' : pe pe = one ( pe pe ) . PE PE [ 8001 ] ( ) ep pkg = one ( pe pe ) . EP PKG [ 8000 ] ( ) c c = one ( pe pe ) . C C [ 8003 ] ( ) if root in [ ep pkg , c c ] : return True elif is contained in ( ep pkg , root ) : return True elif is contained in ( c c , root ) : return True else : return False
def is global ( pe pe ) : if type ( pe pe ) . name != 'PE PE' : pe pe = one ( pe pe ) . PE PE [ 8001 ] ( ) if one ( pe pe ) . C C [ 8003 ] ( ) : return False pe pe = one ( pe pe ) . EP PKG [ 8000 ] . PE PE [ 8001 ] ( ) if not pe pe : return True return is global ( pe pe )
def get data type name ( s dt ) : s cdt = one ( s dt ) . S CDT [ 17 ] ( ) if s cdt and s cdt . Core Typ in range ( 1 , 6 ) : return s dt . Name . upper ( ) if one ( s dt ) . S EDT [ 17 ] ( ) : return 'INTEGER' s dt = one ( s dt ) . S UDT [ 17 ] . S DT [ 18 ] ( ) if s dt : return get data type name ( s dt )
def get related attributes ( r rgo , r rto ) : l1 = list ( ) l2 = list ( ) ref filter = lambda ref : ref . OIR ID == r rgo . OIR ID for o ref in many ( r rto ) . O RTIDA [ 110 ] . O REF [ 111 ] ( ref filter ) : o attr = one ( o ref ) . O RATTR [ 108 ] . O ATTR [ 106 ] ( ) l1 . append ( o attr . Name ) o attr = one ( o ref ) . O RTIDA [ 111 ] . O OIDA [ 110 ] . O ATTR [ 105 ] ( ) l2 . append ( o attr . Name ) return l1 , l2
def mk enum ( s edt ) : s dt = one ( s edt ) . S DT [ 17 ] ( ) enums = list ( ) kwlist = [ 'False' , 'None' , 'True' ] + keyword . kwlist for enum in many ( s edt ) . S ENUM [ 27 ] ( ) : if enum . Name in kwlist : enums . append ( enum . Name + ' ' ) else : enums . append ( enum . Name ) Enum = collections . namedtuple ( s dt . Name , enums ) return Enum ( * range ( len ( enums ) ) )
def mk bridge ( metamodel , s brg ) : action = s brg . Action Semantics internal label = s brg . Name return lambda * * kwargs : interpret . run function ( metamodel , label , action , kwargs )
def mk function ( metamodel , s sync ) : action = s sync . Action Semantics internal label = s sync . Name return lambda * * kwargs : interpret . run function ( metamodel , label , action , kwargs )
def mk constant ( cnst syc ) : s dt = one ( cnst syc ) . S DT [ 1500 ] ( ) cnst lsc = one ( cnst syc ) . CNST LFSC [ 1502 ] . CNST LSC [ 1503 ] ( ) if s dt . Name == 'boolean' : return cnst lsc . Value . lower ( ) == 'true' if s dt . Name == 'integer' : return int ( cnst lsc . Value ) if s dt . Name == 'real' : return float ( cnst lsc . Value ) if s dt . Name == 'string' : return str ( cnst lsc . Value )
def mk class ( m , o obj , derived attributes = False ) : first filter = lambda selected : not one ( selected ) . O ATTR [ 103 , 'succeeds' ] ( ) o attr = one ( o obj ) . O ATTR [ 102 ] ( first filter ) attributes = list ( ) while o attr : s dt = get attribute type ( o attr ) ty = get data type name ( s dt ) if not derived attributes and one ( o attr ) . O BATTR [ 106 ] . O DBATTR [ 107 ] ( ) : pass #            logger.warning('Omitting derived attribute %s.%s ' % #                           (o obj.Key Lett, o attr.Name)) elif not ty : logger . warning ( 'Omitting unsupported attribute %s.%s ' % ( o obj . Key Lett , o attr . Name ) ) else : attributes . append ( ( o attr . Name , ty ) ) o attr = one ( o attr ) . O ATTR [ 103 , 'precedes' ] ( ) metaclass = m . define class ( o obj . Key Lett , list ( attributes ) , o obj . Descrip ) for o id in many ( o obj ) . O ID [ 104 ] ( ) : o oida = many ( o id ) . O OIDA [ 105 ] ( ) o attrs = many ( o oida ) . O ATTR [ 105 ] ( ) if not derived attributes and one ( o attrs ) . O BATTR [ 106 ] . O DBATTR [ 107 ] ( ) : logger . warning ( 'Omitting unique identifier %s.I%d' % ( o obj . Key Lett , o id . Oid ID + 1 ) ) continue names = [ o attr . Name for o attr in o attrs ] m . define unique identifier ( o obj . Key Lett , o id . Oid ID + 1 , * names ) for o tfr in many ( o obj ) . O TFR [ 115 ] ( ) : fn = mk operation ( metaclass , o tfr ) setattr ( metaclass . clazz , o tfr . Name , fn ) for o dbattr in many ( o obj ) . O ATTR [ 102 ] . O BATTR [ 106 ] . O DBATTR [ 107 ] ( ) : o attr = one ( o dbattr ) . O BATTR [ 107 ] . O ATTR [ 106 ] ( ) fn = mk derived attribute ( metaclass , o dbattr ) setattr ( metaclass . clazz , o attr . Name , fn ) return metaclass
def mk simple association ( m , r simp ) : r rel = one ( r simp ) . R REL [ 206 ] ( ) r form = one ( r simp ) . R FORM [ 208 ] ( ) r part = one ( r simp ) . R PART [ 207 ] ( ) r rgo = one ( r form ) . R RGO [ 205 ] ( ) r rto = one ( r part ) . R RTO [ 204 ] ( ) if not r form : logger . info ( 'unformalized association R%s' % ( r rel . Numb ) ) r form = one ( r simp ) . R PART [ 207 ] ( lambda sel : sel != r part ) r rgo = one ( r form ) . R RTO [ 204 ] ( ) source o obj = one ( r rgo ) . R OIR [ 203 ] . O OBJ [ 201 ] ( ) target o obj = one ( r rto ) . R OIR [ 203 ] . O OBJ [ 201 ] ( ) source ids , target ids = get related attributes ( r rgo , r rto ) if source o obj . Obj ID != target o obj . Obj ID : source phrase = target phrase = '' else : source phrase = r part . Txt Phrs target phrase = r form . Txt Phrs m . define association ( rel id = r rel . Numb , source kind = source o obj . Key Lett , target kind = target o obj . Key Lett , source keys = source ids , target keys = target ids , source conditional = r form . Cond , target conditional = r part . Cond , source phrase = source phrase , target phrase = target phrase , source many = r form . Mult , target many = r part . Mult )
def mk linked association ( m , r assoc ) : r rel = one ( r assoc ) . R REL [ 206 ] ( ) r rgo = one ( r assoc ) . R ASSR [ 211 ] . R RGO [ 205 ] ( ) source o obj = one ( r rgo ) . R OIR [ 203 ] . O OBJ [ 201 ] ( ) def mk assoc ( side1 , side2 ) : r rto = one ( side1 ) . R RTO [ 204 ] ( ) target o obj = one ( r rto ) . R OIR [ 203 ] . O OBJ [ 201 ] ( ) source ids , target ids = get related attributes ( r rgo , r rto ) if side1 . Obj ID != side2 . Obj ID : source phrase = target phrase = '' else : source phrase = side1 . Txt Phrs target phrase = side2 . Txt Phrs m . define association ( rel id = r rel . Numb , source kind = source o obj . Key Lett , target kind = target o obj . Key Lett , source keys = source ids , target keys = target ids , source conditional = side2 . Cond , target conditional = False , source phrase = source phrase , target phrase = target phrase , source many = side2 . Mult , target many = False ) r aone = one ( r assoc ) . R AONE [ 209 ] ( ) r aoth = one ( r assoc ) . R AOTH [ 210 ] ( ) mk assoc ( r aone , r aoth ) mk assoc ( r aoth , r aone )
def mk association ( m , r rel ) : handler = { 'R SIMP' : mk simple association , 'R ASSOC' : mk linked association , 'R SUBSUP' : mk subsuper association , 'R COMP' : mk derived association , } inst = subtype ( r rel , 206 ) fn = handler . get ( type ( inst ) . name ) return fn ( m , inst )
def delete globals ( m , disconnect = False ) : filt = lambda sel : ( 247728914420827907967735776184937480192 <= sel . DT ID <= 247728914420827907967735776184937480208 ) for s dt in m . select many ( 'S DT' , filt ) : xtuml . delete ( one ( s dt ) . PE PE [ 8001 ] ( ) , disconnect ) xtuml . delete ( subtype ( s dt , 17 ) , disconnect ) xtuml . delete ( s dt , disconnect )
def accept ( self , reply socket , channel ) : info = self . info or b'' self . send raw ( reply socket , ACCEPT , info , * channel )
def reject ( self , reply socket , call id , topics = ( ) ) : info = self . info or b'' self . send raw ( reply socket , REJECT , info , call id , b'' , topics )
def raise ( self , reply socket , channel , exc info = None ) : if not reply socket : return if exc info is None : exc info = sys . exc info ( ) exc type , exc , tb = exc info while tb . tb next is not None : tb = tb . tb next if issubclass ( exc type , Remote Exception ) : exc type = exc type . exc type filename , lineno = tb . tb frame . f code . co filename , tb . tb lineno val = ( exc type , str ( exc ) , filename , lineno ) try : state = exc . getstate ( ) except Attribute Error : pass else : val += ( state , ) self . send reply ( reply socket , RAISE , val , * channel )
def call wait ( self , hints , name , args , kwargs , topics = ( ) , raw = False , limit = None , retry = False , max retries = None ) : col = self . collector if not col . is running ( ) : col . start ( ) call id = uuid4 bytes ( ) reply to = ( DUPLEX if self . socket is col . socket else col . topic ) # Normal tuple is faster than namedtuple. header = self . make header ( name , call id , reply to , hints ) payload = self . pack ( args , kwargs , raw ) # Use short names. def send call ( ) : try : safe ( send , self . socket , header , payload , topics , zmq . NOBLOCK ) except zmq . Again : raise Undelivered ( 'emission was not delivered' ) col . prepare ( call id , self , name , args , kwargs ) send call ( ) return col . establish ( call id , self . timeout , limit , send call if retry else None , max retries = max retries )
def dispatch reply ( self , reply , value ) : method = reply . method call id = reply . call id task id = reply . task id if method & ACK : try : result queue = self . result queues [ call id ] except Key Error : raise Key Error ( 'already established or unprepared call' ) if method == ACCEPT : worker info = value result = Remote Result ( self , call id , task id , worker info ) self . results [ call id ] [ task id ] = result result queue . put nowait ( result ) elif method == REJECT : result queue . put nowait ( None ) else : result = self . results [ call id ] [ task id ] result . set reply ( reply . method , value )
def guess type name ( value ) : value = str ( value ) if value . upper ( ) in [ 'TRUE' , 'FALSE' ] : return 'BOOLEAN' elif re . match ( r'(-)?(\d+)(\.\d+)' , value ) : return 'REAL' elif re . match ( r'(-)?(\d+)' , value ) : return 'INTEGER' elif re . match ( r'\'((\'\')|[^\'])*\'' , value ) : return 'STRING' elif re . match ( r'\"([^\\\n]|(\\.))*?\"' , value ) : return 'UNIQUE ID'
def deserialize value ( ty , value ) : uty = ty . upper ( ) if uty == 'BOOLEAN' : if value . isdigit ( ) : return bool ( int ( value ) ) elif value . upper ( ) == 'FALSE' : return False elif value . upper ( ) == 'TRUE' : return True else : return None elif uty == 'INTEGER' : if '"' in value : return uuid . UUID ( value [ 1 : - 1 ] ) . int else : return int ( value ) elif uty == 'REAL' : return float ( value ) elif uty == 'STRING' : return value [ 1 : - 1 ] . replace ( "''" , "'" ) elif uty == 'UNIQUE ID' : if '"' in value : return uuid . UUID ( value [ 1 : - 1 ] ) . int else : return int ( value )
def populate classes ( self , metamodel ) : for stmt in self . statements : if isinstance ( stmt , Create Class Stmt ) : metamodel . define class ( stmt . kind , stmt . attributes )
def populate connections ( self , metamodel ) : storage = dict ( ) for ass in metamodel . associations : source class = ass . source link . to metaclass target class = ass . target link . to metaclass if target class not in storage : storage [ target class ] = dict ( ) link key = frozenset ( ass . source link . key map . values ( ) ) if link key not in storage [ target class ] : storage [ target class ] [ link key ] = dict ( ) for other inst in target class . storage : inst key = ass . source link . compute index key ( other inst ) if inst key is None : continue if inst key not in storage [ target class ] [ link key ] : storage [ target class ] [ link key ] [ inst key ] = xtuml . Ordered Set ( ) storage [ target class ] [ link key ] [ inst key ] . add ( other inst ) for inst in source class . storage : inst key = ass . source link . compute lookup key ( inst ) if inst key is None : continue if inst key not in storage [ target class ] [ link key ] : continue for other inst in storage [ target class ] [ link key ] [ inst key ] : ass . source link . connect ( other inst , inst , check = False ) ass . target link . connect ( inst , other inst , check = False ) for inst in metamodel . instances : metaclass = xtuml . get metaclass ( inst ) for attr in metaclass . referential attributes : if attr in inst . dict : delattr ( inst , attr )
def populate ( self , metamodel ) : self . populate classes ( metamodel ) self . populate unique identifiers ( metamodel ) self . populate associations ( metamodel ) self . populate instances ( metamodel ) self . populate connections ( metamodel )
def build metamodel ( self , id generator = None ) : m = xtuml . Meta Model ( id generator ) self . populate ( m ) return m
def source ( self , feature names ) : if feature names is None : return True elif isinstance ( feature names , bool ) : return feature names else : return map ( lambda n : 'fc.' + n , feature names )
def range filters ( self , * key ranges ) : filters = [ ] for s , e in key ranges : if isinstance ( s , basestring ) : s = eid ( s ) if isinstance ( e , basestring ) : # Make the range inclusive. # We need a valid codepoint, so use the max. e += u'\U0010FFFF' e = eid ( e ) if s == ( ) and e == ( ) : filters . append ( { 'match all' : { } } ) elif e == ( ) : filters . append ( { 'range' : { ' id' : { 'gte' : s } } } ) elif s == ( ) : filters . append ( { 'range' : { ' id' : { 'lte' : e } } } ) else : filters . append ( { 'range' : { ' id' : { 'gte' : s , 'lte' : e } } } ) if len ( filters ) == 0 : return [ { 'match all' : { } } ] else : return filters
def create mappings ( self ) : self . conn . indices . put mapping ( index = self . index , doc type = self . type , timeout = 60 , request timeout = 60 , body = { self . type : { 'dynamic templates' : [ { 'default no analyze fc' : { 'match' : 'fc.*' , 'mapping' : { 'index' : 'no' } , } , } ] , ' all' : { 'enabled' : False , } , ' id' : { 'index' : 'not analyzed' , # allows range queries } , 'properties' : self . get index mappings ( ) , } , } ) # It is possible to create an index and quickly launch a request # that will fail because the index hasn't been set up yet. Usually, # you'll get a "no active shards available" error. # # Since index creation is a very rare operation (it only happens # when the index doesn't already exist), we sit and wait for the # cluster to become healthy. self . conn . cluster . health ( index = self . index , wait for status = 'yellow' )
def get index mappings ( self ) : maps = { } for fname in self . indexed features : config = self . indexes . get ( fname , { } ) print ( fname , config ) maps [ fname to idx name ( fname ) ] = { 'type' : config . get ( 'es index type' , 'integer' ) , 'store' : False , 'index' : 'not analyzed' , } for fname in self . fulltext indexed features : maps [ fname to full idx name ( fname ) ] = { 'type' : 'string' , 'store' : False , 'index' : 'analyzed' , } return maps
def get field types ( self ) : mapping = self . conn . indices . get mapping ( index = self . index , doc type = self . type ) return mapping [ self . index ] [ 'mappings' ] [ self . type ] [ 'properties' ]
def fc index disjunction from query ( self , query fc , fname ) : if len ( query fc . get ( fname , [ ] ) ) == 0 : return [ ] terms = query fc [ fname ] . keys ( ) disj = [ ] for fname in self . indexes [ fname ] [ 'feature names' ] : disj . append ( { 'terms' : { fname to idx name ( fname ) : terms } } ) return disj
def fc bytes ( self , fc dict ) : num bytes = 0 for , feat in fc dict . iteritems ( ) : num bytes += len ( feat ) return num bytes
def pretty string ( fc ) : s = [ ] for fname , feature in sorted ( fc . items ( ) ) : if isinstance ( feature , String Counter ) : feature = [ u'%s: %d' % ( k , v ) for ( k , v ) in feature . most common ( ) ] feature = u'\n\t' + u'\n\t' . join ( feature ) s . append ( fname + u': ' + feature ) return u'\n' . join ( s )
def process docopts ( ) : # type: ()->None arguments = docopt ( doc , version = "Find Known Secrets {0}" . format ( version ) ) logger . debug ( arguments ) # print(arguments) if arguments [ "here" ] : # all default go ( ) else : # user config files = arguments [ "--secrets" ] searcher = Searcher ( source = arguments [ "--source" ] , files = files ) searcher . go ( )
def default formatter ( error ) : quoted = formencode . htmlfill . escape formatter ( error ) return u'<span class="error-message">{0}</span>' . format ( quoted )
def pretty to link ( inst , link ) : values = '' prefix = '' metaclass = xtuml . get metaclass ( inst ) for name , ty in metaclass . attributes : if name in link . key map : value = getattr ( inst , name ) value = xtuml . serialize value ( value , ty ) name = link . key map [ name ] values += '%s%s=%s' % ( prefix , name , value ) prefix = ', ' return '%s(%s)' % ( link . kind , values )
def pretty unique identifier ( inst , identifier ) : values = '' prefix = '' metaclass = xtuml . get metaclass ( inst ) for name , ty in metaclass . attributes : if name in metaclass . identifying attributes : value = getattr ( inst , name ) value = xtuml . serialize value ( value , ty ) values += '%s%s=%s' % ( prefix , name , value ) prefix = ', ' return '%s(%s)' % ( identifier , values )
def check uniqueness constraint ( m , kind = None ) : if kind is None : metaclasses = m . metaclasses . values ( ) else : metaclasses = [ m . find metaclass ( kind ) ] res = 0 for metaclass in metaclasses : id map = dict ( ) for identifier in metaclass . indices : id map [ identifier ] = dict ( ) for inst in metaclass . select many ( ) : # Check for null-values for name , ty in metaclass . attributes : if name not in metaclass . identifying attributes : continue value = getattr ( inst , name ) isnull = value is None isnull |= ( ty == 'UNIQUE ID' and not value ) if isnull : res += 1 logger . warning ( '%s.%s is part of an identifier and is null' % ( metaclass . kind , name ) ) # Check uniqueness for identifier in metaclass . indices : kwargs = dict ( ) for name in metaclass . indices [ identifier ] : kwargs [ name ] = getattr ( inst , name ) index key = frozenset ( kwargs . items ( ) ) if index key in id map [ identifier ] : res += 1 id string = pretty unique identifier ( inst , identifier ) logger . warning ( 'uniqueness constraint violation in %s, %s' % ( metaclass . kind , id string ) ) id map [ identifier ] [ index key ] = inst return res
def check link integrity ( m , link ) : res = 0 for inst in link . from metaclass . select many ( ) : q set = list ( link . navigate ( inst ) ) if ( len ( q set ) < 1 and not link . conditional ) or ( ( len ( q set ) > 1 and not link . many ) ) : res += 1 logger . warning ( 'integrity violation in ' '%s --(%s)--> %s' % ( pretty from link ( inst , link ) , link . rel id , pretty to link ( inst , link ) ) ) return res
def check subtype integrity ( m , super kind , rel id ) : if isinstance ( rel id , int ) : rel id = 'R%d' % rel id res = 0 for inst in m . select many ( super kind ) : if not xtuml . navigate subtype ( inst , rel id ) : res += 1 logger . warning ( 'integrity violation across ' '%s[%s]' % ( super kind , rel id ) ) return res
def basic transform ( val ) : if isinstance ( val , int ) : return struct . pack ( '>i' , val ) else : return safe lower utf8 ( val )
def get type name ( s dt ) : s cdt = nav one ( s dt ) . S CDT [ 17 ] ( ) if s cdt and s cdt . Core Typ in range ( 1 , 6 ) : return s dt . Name s edt = nav one ( s dt ) . S EDT [ 17 ] ( ) if s edt : return s dt . Name s udt = nav one ( s dt ) . S UDT [ 17 ] ( ) if s udt : return s dt . Name
def get refered attribute ( o attr ) : o attr ref = nav one ( o attr ) . O RATTR [ 106 ] . O BATTR [ 113 ] . O ATTR [ 106 ] ( ) if o attr ref : return get refered attribute ( o attr ref ) else : return o attr
def build core type ( s cdt ) : s dt = nav one ( s cdt ) . S DT [ 17 ] ( ) if s dt . name == 'void' : type name = None elif s dt . name == 'boolean' : type name = 'xs:boolean' elif s dt . name == 'integer' : type name = 'xs:integer' elif s dt . name == 'real' : type name = 'xs:decimal' elif s dt . name == 'string' : type name = 'xs:string' elif s dt . name == 'unique id' : type name = 'xs:integer' else : type name = None if type name : mapped type = ET . Element ( 'xs:simple Type' , name = s dt . name ) ET . Sub Element ( mapped type , 'xs:restriction' , base = type name ) return mapped type
def build enum type ( s edt ) : s dt = nav one ( s edt ) . S DT [ 17 ] ( ) enum = ET . Element ( 'xs:simple Type' , name = s dt . name ) enum list = ET . Sub Element ( enum , 'xs:restriction' , base = 'xs:string' ) first filter = lambda selected : not nav one ( selected ) . S ENUM [ 56 , 'succeeds' ] ( ) s enum = nav any ( s edt ) . S ENUM [ 27 ] ( first filter ) while s enum : ET . Sub Element ( enum list , 'xs:enumeration' , value = s enum . name ) s enum = nav one ( s enum ) . S ENUM [ 56 , 'precedes' ] ( ) return enum
def build struct type ( s sdt ) : s dt = nav one ( s sdt ) . S DT [ 17 ] ( ) struct = ET . Element ( 'xs:complex Type' , name = s dt . name ) first filter = lambda selected : not nav one ( selected ) . S MBR [ 46 , 'succeeds' ] ( ) s mbr = nav any ( s sdt ) . S MBR [ 44 ] ( first filter ) while s mbr : s dt = nav one ( s mbr ) . S DT [ 45 ] ( ) type name = get type name ( s dt ) ET . Sub Element ( struct , 'xs:attribute' , name = s mbr . name , type = type name ) s mbr = nav one ( s mbr ) . S MBR [ 46 , 'precedes' ] ( ) return struct
def build user type ( s udt ) : s dt user = nav one ( s udt ) . S DT [ 17 ] ( ) s dt base = nav one ( s udt ) . S DT [ 18 ] ( ) base name = get type name ( s dt base ) if base name : user = ET . Element ( 'xs:simple Type' , name = s dt user . name ) ET . Sub Element ( user , 'xs:restriction' , base = base name ) return user
def build type ( s dt ) : s cdt = nav one ( s dt ) . S CDT [ 17 ] ( ) if s cdt : return build core type ( s cdt ) s edt = nav one ( s dt ) . S EDT [ 17 ] ( ) if s edt : return build enum type ( s edt ) s udt = nav one ( s dt ) . S UDT [ 17 ] ( ) if s udt : return build user type ( s udt )
def build class ( o obj ) : cls = ET . Element ( 'xs:element' , name = o obj . key lett , min Occurs = '0' , max Occurs = 'unbounded' ) attributes = ET . Sub Element ( cls , 'xs:complex Type' ) for o attr in nav many ( o obj ) . O ATTR [ 102 ] ( ) : o attr ref = get refered attribute ( o attr ) s dt = nav one ( o attr ref ) . S DT [ 114 ] ( ) while nav one ( s dt ) . S UDT [ 17 ] ( ) : s dt = nav one ( s dt ) . S UDT [ 17 ] . S DT [ 18 ] ( ) type name = get type name ( s dt ) if type name and not nav one ( o attr ) . O BATTR [ 106 ] . O DBATTR [ 107 ] ( ) : ET . Sub Element ( attributes , 'xs:attribute' , name = o attr . name , type = type name ) else : logger . warning ( 'Omitting %s.%s' % ( o obj . key lett , o attr . Name ) ) return cls
def build component ( m , c c ) : component = ET . Element ( 'xs:element' , name = c c . name ) classes = ET . Sub Element ( component , 'xs:complex Type' ) classes = ET . Sub Element ( classes , 'xs:sequence' ) scope filter = lambda selected : ooaofooa . is contained in ( selected , c c ) for o obj in m . select many ( 'O OBJ' , scope filter ) : cls = build class ( o obj ) classes . append ( cls ) return component
def build schema ( m , c c ) : schema = ET . Element ( 'xs:schema' ) schema . set ( 'xmlns:xs' , 'http://www.w3.org/2001/XML Schema' ) global filter = lambda selected : ooaofooa . is global ( selected ) for s dt in m . select many ( 'S DT' , global filter ) : datatype = build type ( s dt ) if datatype is not None : schema . append ( datatype ) scope filter = lambda selected : ooaofooa . is contained in ( selected , c c ) for s dt in m . select many ( 'S DT' , scope filter ) : datatype = build type ( s dt ) if datatype is not None : schema . append ( datatype ) component = build component ( m , c c ) schema . append ( component ) return schema
def prettify ( xml string ) : reparsed = xml . dom . minidom . parse String ( xml string ) return reparsed . toprettyxml ( indent = "    " )
def set positional info ( node , p ) : node . position = Position ( ) node . position . label = p . lexer . label node . position . start stream = p . lexpos ( 1 ) node . position . start line = p . lineno ( 1 ) node . position . start column = find column ( p . lexer . lexdata , node . position . start stream ) , node . position . end stream = p . lexspan ( len ( p ) - 1 ) , node . position . end line = p . linespan ( len ( p ) - 1 ) node . position . end column = find column ( p . lexer . lexdata , node . position . end stream ) - 1 node . character stream = p . lexer . lexdata [ node . position . start stream : node . position . end stream ]
def track production ( f ) : @ wraps ( f ) def wrapper ( self , p ) : r = f ( self , p ) node = p [ 0 ] if isinstance ( node , Node ) and len ( p ) > 1 : set positional info ( node , p ) return r return wrapper
def create msg ( self , to , subject , msg Html , msg Plain , attachments = None ) : sender = self . sender if attachments and isinstance ( attachments , str ) : attachments = [ attachments ] else : attachments = list ( attachments or [ ] ) msg = MIME Multipart ( 'alternative' ) msg [ 'Subject' ] = subject msg [ 'From' ] = sender msg [ 'To' ] = to msg . attach ( MIME Text ( msg Plain , 'plain' ) ) msg . attach ( MIME Text ( msg Html , 'html' ) ) # append attachments if any for path in attachments : attachment = self . prep attachment ( path ) msg . attach ( attachment ) raw = base64 . urlsafe b64encode ( msg . as bytes ( ) ) . decode ( ) #raw = raw.decode() body = { 'raw' : raw } return body
def read ( self ) : # Only download the image if it has changed if self . connection . has changed ( ) : image path = self . connection . download image ( ) image = Image . open ( image path ) self . text cache = pytesseract . image to string ( image ) image . close ( ) return self . text cache
def main ( ) : parser = optparse . Option Parser ( usage = "%prog [options] <model path> [another model path..]" , version = xtuml . version . complete string , formatter = optparse . Titled Help Formatter ( ) ) parser . add option ( "-v" , "--verbosity" , dest = 'verbosity' , action = "count" , default = 1 , help = "increase debug logging level" ) parser . add option ( "-f" , "--function" , dest = 'function' , action = "store" , help = "invoke function named NAME" , metavar = 'NAME' ) parser . add option ( "-c" , "--component" , dest = 'component' , action = "store" , help = "look for the function in a component named NAME" , metavar = 'NAME' , default = None ) ( opts , args ) = parser . parse args ( ) if len ( args ) == 0 or not opts . function : parser . print help ( ) sys . exit ( 1 ) levels = { 0 : logging . ERROR , 1 : logging . WARNING , 2 : logging . INFO , 3 : logging . DEBUG , } logging . basic Config ( level = levels . get ( opts . verbosity , logging . DEBUG ) ) from bridgepoint import ooaofooa mm = ooaofooa . load metamodel ( args ) c c = mm . select any ( 'C C' , where ( Name = opts . component ) ) domain = ooaofooa . mk component ( mm , c c , derived attributes = False ) func = domain . find symbol ( opts . function ) return func ( )
def main ( ) : parser = optparse . Option Parser ( usage = "%prog [options] <model path> [another model path...]" , version = xtuml . version . complete string , formatter = optparse . Titled Help Formatter ( ) ) parser . set description ( doc . strip ( ) ) parser . add option ( "-c" , "--component" , dest = "component" , metavar = "NAME" , help = "export sql schema for the component named NAME" , action = "store" , default = None ) parser . add option ( "-d" , "--derived-attributes" , dest = "derived" , help = "include derived attributes in the schema" , action = "store true" , default = False ) parser . add option ( "-o" , "--output" , dest = 'output' , metavar = "PATH" , help = "save sql schema to PATH (required)" , action = "store" , default = None ) parser . add option ( "-v" , "--verbosity" , dest = 'verbosity' , action = "count" , help = "increase debug logging level" , default = 2 ) ( opts , args ) = parser . parse args ( ) if len ( args ) == 0 or opts . output is None : parser . print help ( ) sys . exit ( 1 ) levels = { 0 : logging . ERROR , 1 : logging . WARNING , 2 : logging . INFO , 3 : logging . DEBUG , } logging . basic Config ( level = levels . get ( opts . verbosity , logging . DEBUG ) ) loader = ooaofooa . Loader ( ) for filename in args : loader . filename input ( filename ) c = loader . build component ( opts . component , opts . derived ) xtuml . persist database ( c , opts . output )
def serialize value ( value , ty ) : ty = ty . upper ( ) null value = { 'BOOLEAN' : False , 'INTEGER' : 0 , 'REAL' : 0.0 , 'STRING' : '' , 'UNIQUE ID' : 0 } transfer fn = { 'BOOLEAN' : lambda v : '%d' % int ( v ) , 'INTEGER' : lambda v : '%d' % v , 'REAL' : lambda v : '%f' % v , 'STRING' : lambda v : "'%s'" % v . replace ( "'" , "''" ) , 'UNIQUE ID' : lambda v : '"%s"' % uuid . UUID ( int = v ) } if value is None : value = null value [ ty ] return transfer fn [ ty ] ( value )
def serialize instance ( instance ) : attr count = 0 metaclass = xtuml . get metaclass ( instance ) s = 'INSERT INTO %s VALUES (' % metaclass . kind for name , ty in metaclass . attributes : value = getattr ( instance , name ) s += '\n    ' s += serialize value ( value , ty ) attr count += 1 if attr count < len ( metaclass . attributes ) : s += ', -- %s : %s' % ( name , ty ) else : s += ' -- %s : %s' % ( name , ty ) s += '\n);\n' return s
def serialize instances ( metamodel ) : s = '' for inst in metamodel . instances : s += serialize instance ( inst ) return s
def serialize association ( ass ) : s1 = '%s %s (%s)' % ( ass . source link . cardinality , ass . source link . to metaclass . kind , ', ' . join ( ass . source keys ) ) if ass . target link . phrase : s1 += " PHRASE '%s'" % ass . target link . phrase s2 = '%s %s (%s)' % ( ass . target link . cardinality , ass . target link . to metaclass . kind , ', ' . join ( ass . target keys ) ) if ass . source link . phrase : s2 += " PHRASE '%s'" % ass . source link . phrase return 'CREATE ROP REF ID %s FROM %s TO %s;\n' % ( ass . rel id , s1 , s2 )
def serialize class ( Cls ) : metaclass = xtuml . get metaclass ( Cls ) attributes = [ '%s %s' % ( name , ty . upper ( ) ) for name , ty in metaclass . attributes ] s = 'CREATE TABLE %s (\n    ' % metaclass . kind s += ',\n    ' . join ( attributes ) s += '\n);\n' return s
def serialize schema ( metamodel ) : s = '' for kind in sorted ( metamodel . metaclasses . keys ( ) ) : s += serialize class ( metamodel . metaclasses [ kind ] . clazz ) for ass in sorted ( metamodel . associations , key = lambda x : x . rel id ) : s += serialize association ( ass ) return s
def serialize ( resource ) : if isinstance ( resource , xtuml . Meta Model ) : return serialize database ( resource ) elif isinstance ( resource , type ) and issubclass ( resource , xtuml . Class ) : return serialize class ( resource ) elif isinstance ( resource , xtuml . Association ) : return serialize association ( resource ) elif isinstance ( resource , xtuml . Class ) : return serialize instance ( resource )
def main ( ) : parser = Argument Parser ( description = "search files using n-grams" ) parser . add argument ( '--path' , dest = 'path' , help = "where to search" , nargs = 1 , action = "store" , default = getcwd ( ) ) parser . add argument ( '--update' , dest = 'update' , help = "update the index" , action = 'store true' , default = True ) parser . add argument ( '--filetype' , dest = 'filetype' , help = "any, images, documents, code, audio, video" , nargs = 1 , action = "store" , default = [ "any" ] ) parser . add argument ( '--verbose' , dest = 'verbose' , help = "extended output" , action = 'store true' , default = False ) parser . add argument ( '--results' , dest = 'results' , help = "number of results to display" , action = "store" , default = 10 ) parser . add argument ( 'query' , nargs = '+' , help = "what to search" , action = "store" ) args = parser . parse args ( ) if args . verbose : verbose = 2 pprint ( args ) else : verbose = 0 query = args . query [ 0 ] for arg in args . query [ 1 : ] : query = query + " " + arg slb = min ( [ len ( w ) for w in query . split ( " " ) ] ) files = Files ( path = args . path , filetype = args . filetype [ 0 ] , exclude = [ ] , update = args . update , verbose = verbose ) index = Index ( files , slb = slb , verbose = verbose ) results = index . search ( query , verbose = verbose ) Handler ( results , results number = int ( args . results ) )
def partition ( condition , collection ) -> Tuple [ List , List ] : succeed , fail = [ ] , [ ] for x in collection : if condition ( x ) : succeed . append ( x ) else : fail . append ( x ) return succeed , fail
def find link ( inst1 , inst2 , rel id , phrase ) : metaclass1 = get metaclass ( inst1 ) metaclass2 = get metaclass ( inst2 ) if isinstance ( rel id , int ) : rel id = 'R%d' % rel id for ass in metaclass1 . metamodel . associations : if ass . rel id != rel id : continue if ( ass . source link . from metaclass . kind == metaclass1 . kind and ass . source link . to metaclass . kind == metaclass2 . kind and ass . source link . phrase == phrase ) : return inst1 , inst2 , ass if ( ass . target link . from metaclass . kind == metaclass1 . kind and ass . target link . to metaclass . kind == metaclass2 . kind and ass . target link . phrase == phrase ) : return inst2 , inst1 , ass raise Unknown Link Exception ( metaclass1 . kind , metaclass2 . kind , rel id , phrase )
def get metaclass ( class or instance ) : if isinstance ( class or instance , Class ) : return class or instance . metaclass elif issubclass ( class or instance , Class ) : return class or instance . metaclass raise Meta Exception ( "the provided argument is not an xtuml class or instance" )
def disconnect ( self , instance , another instance ) : if instance not in self : return False if another instance not in self [ instance ] : return False self [ instance ] . remove ( another instance ) return True
def attribute type ( self , attribute name ) : attribute name = attribute name . upper ( ) for name , ty in self . attributes : if name . upper ( ) == attribute name : return ty
def add link ( self , metaclass , rel id , phrase , conditional , many ) : link = Link ( self , rel id , metaclass , phrase , conditional , many ) key = ( metaclass . kind . upper ( ) , rel id , phrase ) self . links [ key ] = link return link
def delete attribute ( self , name ) : for idx , attr in enumerate ( self . attributes ) : attr name , = attr if attr name == name : del self . attributes [ idx ] return
def default value ( self , type name ) : uname = type name . upper ( ) if uname == 'BOOLEAN' : return False elif uname == 'INTEGER' : return 0 elif uname == 'REAL' : return 0.0 elif uname == 'STRING' : return '' elif uname == 'UNIQUE ID' : if self . metamodel : return next ( self . metamodel . id generator ) else : return None else : raise Meta Exception ( "Unknown type named '%s'" % type name )
def new ( self , * args , * * kwargs ) : inst = self . clazz ( ) self . storage . append ( inst ) # set all attributes with an initial default value referential attributes = dict ( ) for name , ty in self . attributes : if name not in self . referential attributes : value = self . default value ( ty ) setattr ( inst , name , value ) # set all positional arguments for attr , value in zip ( self . attributes , args ) : name , ty = attr if name not in self . referential attributes : setattr ( inst , name , value ) else : referential attributes [ name ] = value # set all named arguments for name , value in kwargs . items ( ) : if name not in self . referential attributes : setattr ( inst , name , value ) else : referential attributes [ name ] = value if not referential attributes : return inst # batch relate referential attributes  for link in self . links . values ( ) : if set ( link . key map . values ( ) ) - set ( referential attributes . keys ( ) ) : continue kwargs = dict ( ) for key , value in link . key map . items ( ) : kwargs [ key ] = referential attributes [ value ] if not kwargs : continue for other inst in link . to metaclass . query ( kwargs ) : relate ( other inst , inst , link . rel id , link . phrase ) for name , value in referential attributes . items ( ) : if getattr ( inst , name ) != value : logger . warning ( 'unable to assign %s to %s' , name , inst ) return inst
def instances ( self ) : for metaclass in self . metaclasses . values ( ) : for inst in metaclass . storage : yield inst
def define class ( self , kind , attributes , doc = '' ) : ukind = kind . upper ( ) if ukind in self . metaclasses : raise Meta Model Exception ( 'A class with the name %s is already defined' % kind ) metaclass = Meta Class ( kind , self ) for name , ty in attributes : metaclass . append attribute ( name , ty ) self . metaclasses [ ukind ] = metaclass return metaclass
def find metaclass ( self , kind ) : ukind = kind . upper ( ) if ukind in self . metaclasses : return self . metaclasses [ ukind ] else : raise Unknown Class Exception ( kind )
def dead code ( ) : with safe cd ( SRC ) : if IS TRAVIS : command = "{0} vulture {1}" . format ( PYTHON , PROJECT NAME ) . strip ( ) . split ( ) else : command = "{0} vulture {1}" . format ( PIPENV , PROJECT NAME ) . strip ( ) . split ( ) output file name = "dead code.txt" with open ( output file name , "w" ) as outfile : env = config pythonpath ( ) subprocess . call ( command , stdout = outfile , env = env ) cutoff = 20 num lines = sum ( 1 for line in open ( output file name ) if line ) if num lines > cutoff : print ( "Too many lines of dead code : {0}, max {1}" . format ( num lines , cutoff ) ) exit ( - 1 )
def parse emails ( values ) : emails = [ ] if isinstance ( values , str ) : values = [ values ] # now we know we have a list of strings for value in values : matches = re emails . findall ( value ) emails . extend ( [ match [ 2 ] for match in matches ] ) return emails
def rpc ( f = None , * * kwargs ) : if f is not None : if isinstance ( f , six . string types ) : if 'name' in kwargs : raise Value Error ( 'name option duplicated' ) kwargs [ 'name' ] = f else : return rpc ( * * kwargs ) ( f ) return functools . partial ( rpc , * * kwargs )
def rpc spec table ( app ) : table = { } for attr , value in inspect . getmembers ( app ) : rpc spec = get rpc spec ( value , default = None ) if rpc spec is None : continue table [ rpc spec . name ] = ( value , rpc spec ) return table
async def normalize postcode middleware ( request , handler ) : postcode : Optional [ str ] = request . match info . get ( 'postcode' , None ) if postcode is None or postcode == "random" : return await handler ( request ) elif not is uk postcode ( postcode ) : raise web . HTTP Not Found ( text = "Invalid Postcode" ) postcode processed = postcode . upper ( ) . replace ( " " , "" ) if postcode processed == postcode : return await handler ( request ) else : url name = request . match info . route . name url = request . app . router [ url name ] params = dict ( request . match info ) params [ 'postcode' ] = postcode processed raise web . HTTP Moved Permanently ( str ( url . url for ( * * params ) ) )
def next ( self ) : val = self . current self . current = self . readfunc ( ) return val
def accept S SYS ( self , inst ) : for child in many ( inst ) . EP PKG [ 1401 ] ( ) : self . accept ( child )
def accept C C ( self , inst ) : for child in many ( inst ) . PE PE [ 8003 ] ( ) : self . accept ( child )
def accept EP PKG ( self , inst ) : for child in many ( inst ) . PE PE [ 8000 ] ( ) : self . accept ( child )
def get brightness ( self ) : # Only download the image if it has changed if not self . connection . has changed ( ) : return self . image brightness image path = self . connection . download image ( ) converted image = Image . open ( image path ) . convert ( 'L' ) statistics = Image Stat . Stat ( converted image ) self . image brightness = statistics . mean [ 0 ] return self . image brightness
def selection for character ( self , position ) : selection = Qt Gui . Q Text Edit . Extra Selection ( ) cursor = self . text edit . text Cursor ( ) cursor . set Position ( position ) cursor . move Position ( Qt Gui . Q Text Cursor . Next Character , Qt Gui . Q Text Cursor . Keep Anchor ) selection . cursor = cursor selection . format = self . format return selection
def cursor position changed ( self ) : # Clear out the old formatting. self . text edit . set Extra Selections ( [ ] ) # Attempt to match a bracket for the new cursor position. cursor = self . text edit . text Cursor ( ) if not cursor . has Selection ( ) : position = cursor . position ( ) - 1 match position = self . find match ( position ) if match position != - 1 : extra selections = [ self . selection for character ( pos ) for pos in ( position , match position ) ] self . text edit . set Extra Selections ( extra selections )
def exc info ( self ) : e = self . exc info ( ) if sys . platform == 'cli' : if isinstance ( e [ 0 ] , String Exception ) : # Iron Python throws these String Exceptions, but # traceback checks type(etype) == str. Make a real # string here. e = ( str ( e [ 0 ] ) , e [ 1 ] , e [ 2 ] ) return e
def run ( self , result ) : # proxy the result for myself log . debug ( "suite %s (%s) run called, tests: %s" , id ( self ) , self , self . tests ) #import pdb #pdb.set trace() if self . result Proxy : result , orig = self . result Proxy ( result , self ) , result else : result , orig = result , result try : self . set Up ( ) except Keyboard Interrupt : raise except : self . error context = 'setup' result . add Error ( self , self . exc info ( ) ) return try : for test in self . tests : if result . should Stop : log . debug ( "stopping" ) break # each nose.case.Test will create its own result proxy # so the cases need the original result, to avoid proxy # chains test ( orig ) finally : self . has run = True try : self . tear Down ( ) except Keyboard Interrupt : raise except : self . error context = 'teardown' result . add Error ( self , self . exc info ( ) )
def options ( self , parser , env ) : parser . add option ( '--collect-only' , action = 'store true' , dest = self . enable Opt , default = env . get ( 'NOSE COLLECT ONLY' ) , help = "Enable collect-only: %s [COLLECT ONLY]" % ( self . help ( ) ) )
def execute ( self , source = None , hidden = False , interactive = False ) : if not hidden : history = self . input buffer if source is None else source executed = super ( History Console Widget , self ) . execute ( source , hidden , interactive ) if executed and not hidden : # Save the command unless it was an empty string or was identical # to the previous command. history = history . rstrip ( ) if history and ( not self . history or self . history [ - 1 ] != history ) : self . history . append ( history ) # Emulate readline: reset all history edits. self . history edits = { } # Move the history index to the most recent item. self . history index = len ( self . history ) return executed
def handle execute reply ( self , msg ) : msg id = msg [ 'parent header' ] [ 'msg id' ] info = self . request info [ 'execute' ] . pop ( msg id , None ) if info and info . kind == 'save magic' and not self . hidden : content = msg [ 'content' ] status = content [ 'status' ] if status == 'ok' : self . max session history = ( int ( content [ 'user expressions' ] [ 'hlen' ] ) )
def history locked ( self ) : return ( self . history lock and ( self . get edited history ( self . history index ) != self . input buffer ) and ( self . get prompt cursor ( ) . block Number ( ) != self . get end cursor ( ) . block Number ( ) ) )
def get edited history ( self , index ) : if index in self . history edits : return self . history edits [ index ] elif index == len ( self . history ) : return unicode ( ) return self . history [ index ]
def set history ( self , history ) : self . history = list ( history ) self . history edits = { } self . history index = len ( self . history )
def store edits ( self ) : current = self . input buffer if self . history index == len ( self . history ) or self . history [ self . history index ] != current : self . history edits [ self . history index ] = current
def On Time To Close ( self , evt ) : print ( "See ya later!" ) sys . stdout . flush ( ) self . cleanup consoles ( evt ) self . Close ( ) # Not sure why, but our I Python kernel seems to prevent normal WX # shutdown, so an explicit exit() call is needed. sys . exit ( )
def cleanup files ( self ) : logger . debug ( 'Cleaning up...' ) with indent log ( ) : for req in self . reqs to cleanup : req . remove temporary source ( ) if self . pip has created build dir ( ) : logger . debug ( 'Removing temporary dir %s...' , self . build dir ) rmtree ( self . build dir )
def subscribe ( self ) : self . stream . setsockopt ( zmq . UNSUBSCRIBE , '' ) if '' in self . topics : self . log . debug ( "Subscribing to: everything" ) self . stream . setsockopt ( zmq . SUBSCRIBE , '' ) else : for topic in self . topics : self . log . debug ( "Subscribing to: %r" % ( topic ) ) self . stream . setsockopt ( zmq . SUBSCRIBE , topic )
def log message ( self , raw ) : if len ( raw ) != 2 or '.' not in raw [ 0 ] : self . log . error ( "Invalid log message: %s" % raw ) return else : topic , msg = raw # don't newline, since log messages always newline: topic , level name = topic . rsplit ( '.' , 1 ) level , topic = self . extract level ( topic ) if msg [ - 1 ] == '\n' : msg = msg [ : - 1 ] self . log . log ( level , "[%s] %s" % ( topic , msg ) )
def remote iterator ( view , name ) : view . execute ( 'it%s=iter(%s)' % ( name , name ) , block = True ) while True : try : result = view . apply sync ( lambda x : x . next ( ) , Reference ( 'it' + name ) ) # This causes the Stop Iteration exception to be raised. except Remote Error as e : if e . ename == 'Stop Iteration' : raise Stop Iteration else : raise e else : yield result
def String IO ( * args , * * kw ) : global String IO try : from c String IO import String IO except Import Error : from String IO import String IO return String IO ( * args , * * kw )
def insert on ( self , path , loc = None ) : loc = loc or self . location if self . project name == 'setuptools' : try : version = self . version except Value Error : version = '' if '0.7' in version : raise Value Error ( "A 0.7-series setuptools cannot be installed " "with distribute. Found one at %s" % str ( self . location ) ) if not loc : return if path is sys . path : self . check version conflict ( ) nloc = normalize cached ( loc ) bdir = os . path . dirname ( nloc ) npath = map ( normalize cached , path ) bp = None for p , item in enumerate ( npath ) : if item == nloc : break elif item == bdir and self . precedence == EGG DIST : # if it's an .egg, give it precedence over its directory path . insert ( p , loc ) npath . insert ( p , nloc ) break else : path . append ( loc ) return # p is the spot where we found or inserted loc; now remove duplicates while 1 : try : np = npath . index ( nloc , p + 1 ) except Value Error : break else : del npath [ np ] , path [ np ] p = np # ha! return
def parsed pkg info ( self ) : try : return self . pkg info except Attribute Error : from email . parser import Parser self . pkg info = Parser ( ) . parsestr ( self . get metadata ( self . PKG INFO ) ) return self . pkg info
def compute dependencies ( self ) : from markerlib import compile as compile marker dm = self . dep map = { None : [ ] } reqs = [ ] # Including any condition expressions for req in self . parsed pkg info . get all ( 'Requires-Dist' ) or [ ] : distvers , mark = self . preparse requirement ( req ) parsed = parse requirements ( distvers ) . next ( ) parsed . marker fn = compile marker ( mark ) reqs . append ( parsed ) def reqs for extra ( extra ) : for req in reqs : if req . marker fn ( override = { 'extra' : extra } ) : yield req common = frozenset ( reqs for extra ( None ) ) dm [ None ] . extend ( common ) for extra in self . parsed pkg info . get all ( 'Provides-Extra' ) or [ ] : extra = safe extra ( extra . strip ( ) ) dm [ extra ] = list ( frozenset ( reqs for extra ( extra ) ) - common ) return dm
def collapse leading ws ( header , txt ) : if header . lower ( ) == 'description' : # preserve newlines return '\n' . join ( [ x [ 8 : ] if x . startswith ( ' ' * 8 ) else x for x in txt . strip ( ) . splitlines ( ) ] ) else : return ' ' . join ( [ x . strip ( ) for x in txt . splitlines ( ) ] )
def hide Event ( self , event ) : super ( Completion Widget , self ) . hide Event ( event ) self . text edit . cursor Position Changed . disconnect ( self . update current ) self . text edit . remove Event Filter ( self )
def show Event ( self , event ) : super ( Completion Widget , self ) . show Event ( event ) self . text edit . cursor Position Changed . connect ( self . update current ) self . text edit . install Event Filter ( self )
def complete current ( self ) : self . current text cursor ( ) . insert Text ( self . current Item ( ) . text ( ) ) self . hide ( )
def update current ( self ) : prefix = self . current text cursor ( ) . selection ( ) . to Plain Text ( ) if prefix : items = self . find Items ( prefix , ( Qt Core . Qt . Match Starts With | Qt Core . Qt . Match Case Sensitive ) ) if items : self . set Current Item ( items [ 0 ] ) else : self . hide ( ) else : self . hide ( )
def register Admin Site ( app Name , exclude Models = [ ] ) : for model in apps . get app config ( app Name ) . get models ( ) : if model not in exclude Models : admin . site . register ( model )
def virtual memory ( ) : mem = psutil mswindows . get virtual mem ( ) totphys , availphys , totpagef , availpagef , totvirt , freevirt = mem # total = totphys avail = availphys free = availphys used = total - avail percent = usage percent ( ( total - avail ) , total , round = 1 ) return nt virtmem info ( total , avail , percent , used , free )
def get disk usage ( path ) : try : total , free = psutil mswindows . get disk usage ( path ) except Windows Error : err = sys . exc info ( ) [ 1 ] if not os . path . exists ( path ) : raise OS Error ( errno . ENOENT , "No such file or directory: '%s'" % path ) raise used = total - free percent = usage percent ( used , total , round = 1 ) return nt diskinfo ( total , used , free , percent )
def disk partitions ( all ) : rawlist = psutil mswindows . get disk partitions ( all ) return [ nt partition ( * x ) for x in rawlist ]
def get system cpu times ( ) : user , system , idle = 0 , 0 , 0 # computes system global times summing each processor value for cpu time in psutil mswindows . get system cpu times ( ) : user += cpu time [ 0 ] system += cpu time [ 1 ] idle += cpu time [ 2 ] return cputimes ntuple ( user , system , idle )
def get system per cpu times ( ) : ret = [ ] for cpu t in psutil mswindows . get system cpu times ( ) : user , system , idle = cpu t item = cputimes ntuple ( user , system , idle ) ret . append ( item ) return ret
def get system users ( ) : retlist = [ ] rawlist = psutil mswindows . get system users ( ) for item in rawlist : user , hostname , tstamp = item nt = nt user ( user , None , hostname , tstamp ) retlist . append ( nt ) return retlist
def stdin raw nonblock ( self ) : # WARNING: This is experimental, and produces inconsistent results. #          It's possible for the handle not to be appropriate for use #          with Wait For Single Object, among other things. handle = msvcrt . get osfhandle ( sys . stdin . fileno ( ) ) result = Wait For Single Object ( handle , 100 ) if result == WAIT FAILED : raise ctypes . Win Error ( ) elif result == WAIT TIMEOUT : print ( "." , end = '' ) return None else : data = ctypes . create string buffer ( 256 ) bytes Read = DWORD ( 0 ) print ( '?' , end = '' ) if not Read File ( handle , data , 256 , ctypes . byref ( bytes Read ) , None ) : raise ctypes . Win Error ( ) # This ensures the non-blocking works with an actual console # Not checking the error, so the processing will still work with # other handle types Flush Console Input Buffer ( handle ) data = data . value data = data . replace ( '\r\n' , '\n' ) data = data . replace ( '\r' , '\n' ) print ( repr ( data ) + " " , end = '' ) return data
def stdin raw block ( self ) : # The big problem with the blocking read is that it doesn't # exit when it's supposed to in all contexts. An extra # key-press may be required to trigger the exit. try : data = sys . stdin . read ( 1 ) data = data . replace ( '\r' , '\n' ) return data except Windows Error as we : if we . winerror == ERROR NO DATA : # This error occurs when the pipe is closed return None else : # Otherwise let the error propagate raise we
def stdout raw ( self , s ) : print ( s , end = '' , file = sys . stdout ) sys . stdout . flush ( )
def stderr raw ( self , s ) : print ( s , end = '' , file = sys . stderr ) sys . stderr . flush ( )
def create tab with current kernel ( self ) : current widget = self . tab widget . current Widget ( ) current widget index = self . tab widget . index Of ( current widget ) current widget name = self . tab widget . tab Text ( current widget index ) widget = self . slave frontend factory ( current widget ) if 'slave' in current widget name : # don't keep stacking slaves name = current widget name else : name = '(%s) slave' % current widget name self . add tab with frontend ( widget , name = name )
def add tab with frontend ( self , frontend , name = None ) : if not name : name = 'kernel %i' % self . next kernel id self . tab widget . add Tab ( frontend , name ) self . update tab bar visibility ( ) self . make frontend visible ( frontend ) frontend . exit requested . connect ( self . close tab )
def close Event ( self , event ) : if self . tab widget . count ( ) == 0 : # no tabs, just close event . accept ( ) return # Do Not loop on the widget count as it change while closing title = self . window ( ) . window Title ( ) cancel = Qt Gui . Q Message Box . Cancel okay = Qt Gui . Q Message Box . Ok if self . confirm exit : if self . tab widget . count ( ) > 1 : msg = "Close all tabs, stop all kernels, and Quit?" else : msg = "Close console, stop kernel, and Quit?" info = "Kernels not started here (e.g. notebooks) will be left alone." closeall = Qt Gui . Q Push Button ( "&Quit" , self ) closeall . set Shortcut ( 'Q' ) box = Qt Gui . Q Message Box ( Qt Gui . Q Message Box . Question , title , msg ) box . set Informative Text ( info ) box . add Button ( cancel ) box . add Button ( closeall , Qt Gui . Q Message Box . Yes Role ) box . set Default Button ( closeall ) box . set Escape Button ( cancel ) pixmap = Qt Gui . Q Pixmap ( self . app . icon . pixmap ( Qt Core . Q Size ( 64 , 64 ) ) ) box . set Icon Pixmap ( pixmap ) reply = box . exec ( ) else : reply = okay if reply == cancel : event . ignore ( ) return if reply == okay : while self . tab widget . count ( ) >= 1 : # prevent further confirmations: widget = self . active frontend widget . confirm exit = False self . close tab ( widget ) event . accept ( )
def toggle boolean ( self , request ) : try : item id = int ( request . POST . get ( 'item id' , None ) ) attr = str ( request . POST . get ( 'attr' , None ) ) except : return Http Response Bad Request ( "Malformed request" ) if not request . user . is staff : logging . warning ( "Denied AJAX request by non-staff %s to toggle boolean %s for object #%s" , request . user , attr , item id ) return Http Response Forbidden ( "You do not have permission to access this object" ) self . collect editable booleans ( ) if not self . ajax editable booleans . has key ( attr ) : return Http Response Bad Request ( "not a valid attribute %s" % attr ) try : obj = self . model . default manager . get ( pk = item id ) except self . model . Does Not Exist : return Http Response Not Found ( "Object does not exist" ) can change = False if hasattr ( obj , "user can" ) and obj . user can ( request . user , change page = True ) : # Was added in c7f04dfb5d, but I've no idea what user can is about. can change = True else : can change = self . has change permission ( request , obj = obj ) if not can change : logging . warning ( "Denied AJAX request by %s to toggle boolean %s for object %s" , request . user , attr , item id ) return Http Response Forbidden ( "You do not have permission to access this object" ) logging . info ( "Processing request by %s to toggle %s on %s" , request . user , attr , obj ) try : before data = self . ajax editable booleans [ attr ] ( self , obj ) setattr ( obj , attr , not getattr ( obj , attr ) ) obj . save ( ) self . refresh changelist caches ( ) # ???: Perhaps better a post save signal? # Construct html snippets to send back to client for status update data = self . ajax editable booleans [ attr ] ( self , obj ) except Exception : #, e: logging . exception ( "Unhandled exception while toggling %s on %s" , attr , obj ) return Http Response Server Error ( "Unable to toggle %s on %s" % ( attr , obj ) ) # Weed out unchanged cells to keep the updates small. This assumes # that the order a possible get descendents() returns does not change # before and after toggling this attribute. Unlikely, but still... d = [ ] for a , b in zip ( before data , data ) : if a != b : d . append ( b ) # TODO: Shorter: [ y for x,y in zip(a,b) if x!=y ] return Http Response ( json . dumps ( d ) , mimetype = "application/json" )
def add children ( G , parent , level , n = 2 ) : if level == 0 : return for i in range ( n ) : child = parent + str ( i ) G . add node ( child ) G . add edge ( parent , child ) add children ( G , child , level - 1 , n )
def make bintree ( levels ) : G = nx . Di Graph ( ) root = '0' G . add node ( root ) add children ( G , root , levels , 2 ) return G
def submit jobs ( view , G , jobs ) : results = { } for node in nx . topological sort ( G ) : with view . temp flags ( after = [ results [ n ] for n in G . predecessors ( node ) ] ) : results [ node ] = view . apply ( jobs [ node ] ) return results
def validate tree ( G , results ) : for node in G : started = results [ node ] . metadata . started for parent in G . predecessors ( node ) : finished = results [ parent ] . metadata . completed assert started > finished , "%s should have happened after %s" % ( node , parent )
def copy ( self , name = None ) : if name is None : name = self . name return Color Scheme ( name , self . colors . dict ( ) )
def add scheme ( self , new scheme ) : if not isinstance ( new scheme , Color Scheme ) : raise Value Error , 'Color Scheme Table only accepts Color Scheme instances' self [ new scheme . name ] = new scheme
def home lib ( home ) : if hasattr ( sys , 'pypy version info' ) : lib = 'site-packages' else : lib = os . path . join ( 'lib' , 'python' ) return os . path . join ( home , lib )
def handle stdin request ( self , timeout = 0.1 ) : msg rep = self . km . stdin channel . get msg ( timeout = timeout ) # in case any iopub came while we were waiting: self . handle iopub ( ) if self . session id == msg rep [ "parent header" ] . get ( "session" ) : # wrap SIGINT handler real handler = signal . getsignal ( signal . SIGINT ) def double int ( sig , frame ) : # call real handler (forwards sigint to kernel), # then raise local interrupt, stopping local raw input real handler ( sig , frame ) raise Keyboard Interrupt signal . signal ( signal . SIGINT , double int ) try : raw data = raw input ( msg rep [ "content" ] [ "prompt" ] ) except EOF Error : # turn EOF Error into EOF character raw data = '\x04' except Keyboard Interrupt : sys . stdout . write ( '\n' ) return finally : # restore SIGINT handler signal . signal ( signal . SIGINT , real handler ) # only send stdin reply if there *was not* another request # or execution finished while we were reading. if not ( self . km . stdin channel . msg ready ( ) or self . km . shell channel . msg ready ( ) ) : self . km . stdin channel . input ( raw data )
def wait for kernel ( self , timeout = None ) : tic = time . time ( ) self . km . hb channel . unpause ( ) while True : self . run cell ( '1' , False ) if self . km . hb channel . is beating ( ) : # heart failure was not the reason this returned break else : # heart failed if timeout is not None and ( time . time ( ) - tic ) > timeout : return False return True
def interact ( self , display banner = None ) : # batch run -> do not interact if self . exit now : return if display banner is None : display banner = self . display banner if isinstance ( display banner , basestring ) : self . show banner ( display banner ) elif display banner : self . show banner ( ) more = False # run a non-empty no-op, so that we don't get a prompt until # we know the kernel is ready. This keeps the connection # message above the first prompt. if not self . wait for kernel ( 3 ) : error ( "Kernel did not respond\n" ) return if self . has readline : self . readline startup hook ( self . pre readline ) hlen b4 cell = self . readline . get current history length ( ) else : hlen b4 cell = 0 # exit now is set by a call to %Exit or %Quit, through the # ask exit callback. while not self . exit now : if not self . km . is alive : # kernel died, prompt for action or exit action = "restart" if self . km . has kernel else "wait for restart" ans = self . ask yes no ( "kernel died, %s ([y]/n)?" % action , default = 'y' ) if ans : if self . km . has kernel : self . km . restart kernel ( True ) self . wait for kernel ( 3 ) else : self . exit now = True continue try : # protect prompt block from Keyboard Interrupt # when sitting on ctrl-C self . hooks . pre prompt hook ( ) if more : try : prompt = self . prompt manager . render ( 'in2' ) except Exception : self . showtraceback ( ) if self . autoindent : self . rl do indent = True else : try : prompt = self . separate in + self . prompt manager . render ( 'in' ) except Exception : self . showtraceback ( ) line = self . raw input ( prompt ) if self . exit now : # quick exit on sys.std[in|out] close break if self . autoindent : self . rl do indent = False except Keyboard Interrupt : #double-guard against keyboardinterrupts during kbdint handling try : self . write ( '\n Keyboard Interrupt\n' ) source raw = self . input splitter . source raw reset ( ) [ 1 ] hlen b4 cell = self . replace rlhist multiline ( source raw , hlen b4 cell ) more = False except Keyboard Interrupt : pass except EOF Error : if self . autoindent : self . rl do indent = False if self . has readline : self . readline startup hook ( None ) self . write ( '\n' ) self . exit ( ) except bdb . Bdb Quit : warn ( 'The Python debugger has exited with a Bdb Quit exception.\n' 'Because of how pdb handles the stack, it is impossible\n' 'for I Python to properly format this particular exception.\n' 'I Python will resume normal operation.' ) except : # exceptions here are VERY RARE, but they can be triggered # asynchronously by signal handlers, for example. self . showtraceback ( ) else : self . input splitter . push ( line ) more = self . input splitter . push accepts more ( ) if ( self . Syntax TB . last syntax error and self . autoedit syntax ) : self . edit syntax error ( ) if not more : source raw = self . input splitter . source raw reset ( ) [ 1 ] hlen b4 cell = self . replace rlhist multiline ( source raw , hlen b4 cell ) self . run cell ( source raw ) # Turn off the exit flag, so the mainloop can be restarted if desired self . exit now = False
def set style ( self , style ) : if isinstance ( style , basestring ) : style = get style by name ( style ) self . style = style self . clear caches ( )
def get format ( self , token ) : if token in self . formats : return self . formats [ token ] if self . style is None : result = self . get format from document ( token , self . document ) else : result = self . get format from style ( token , self . style ) self . formats [ token ] = result return result
def get format from document ( self , token , document ) : code , html = self . formatter . format lines ( [ ( token , u'dummy' ) ] ) . next ( ) self . document . set Html ( html ) return Qt Gui . Q Text Cursor ( self . document ) . char Format ( )
def get format from style ( self , token , style ) : result = Qt Gui . Q Text Char Format ( ) for key , value in style . style for token ( token ) . items ( ) : if value : if key == 'color' : result . set Foreground ( self . get brush ( value ) ) elif key == 'bgcolor' : result . set Background ( self . get brush ( value ) ) elif key == 'bold' : result . set Font Weight ( Qt Gui . Q Font . Bold ) elif key == 'italic' : result . set Font Italic ( True ) elif key == 'underline' : result . set Underline Style ( Qt Gui . Q Text Char Format . Single Underline ) elif key == 'sans' : result . set Font Style Hint ( Qt Gui . Q Font . Sans Serif ) elif key == 'roman' : result . set Font Style Hint ( Qt Gui . Q Font . Times ) elif key == 'mono' : result . set Font Style Hint ( Qt Gui . Q Font . Type Writer ) return result
def find command ( cmd , paths = None , pathext = None ) : if paths is None : paths = os . environ . get ( 'PATH' , '' ) . split ( os . pathsep ) if isinstance ( paths , six . string types ) : paths = [ paths ] # check if there are funny path extensions for executables, e.g. Windows if pathext is None : pathext = get pathext ( ) pathext = [ ext for ext in pathext . lower ( ) . split ( os . pathsep ) if len ( ext ) ] # don't use extensions if the command ends with one of them if os . path . splitext ( cmd ) [ 1 ] . lower ( ) in pathext : pathext = [ '' ] # check if we find the command on PATH for path in paths : # try without extension first cmd path = os . path . join ( path , cmd ) for ext in pathext : # then including the extension cmd path ext = cmd path + ext if os . path . isfile ( cmd path ext ) : return cmd path ext if os . path . isfile ( cmd path ) : return cmd path raise Bad Command ( 'Cannot find command %r' % cmd )
def normalize path ( path ) : return os . path . normcase ( os . path . realpath ( os . path . expanduser ( path ) ) )
def check nsp ( dist , attr , value ) : assert string list ( dist , attr , value ) for nsp in value : if not dist . has contents for ( nsp ) : raise Distutils Setup Error ( "Distribution contains no modules or packages for " + "namespace package %r" % nsp ) if '.' in nsp : parent = '.' . join ( nsp . split ( '.' ) [ : - 1 ] ) if parent not in value : distutils . log . warn ( "%r is declared as a package namespace, but %r is not:" " please correct this in setup.py" , nsp , parent )
def check extras ( dist , attr , value ) : try : for k , v in value . items ( ) : list ( pkg resources . parse requirements ( v ) ) except ( Type Error , Value Error , Attribute Error ) : raise Distutils Setup Error ( "'extras require' must be a dictionary whose values are " "strings or lists of strings containing valid project/version " "requirement specifiers." )
def check entry points ( dist , attr , value ) : try : pkg resources . Entry Point . parse map ( value ) except Value Error , e : raise Distutils Setup Error ( e )
def transform assign system ( line ) : m = assign system re . match ( line ) if m is not None : cmd = m . group ( 'cmd' ) lhs = m . group ( 'lhs' ) new line = '%s = get ipython().getoutput(%r)' % ( lhs , cmd ) return new line return line
def transform assign magic ( line ) : m = assign magic re . match ( line ) if m is not None : cmd = m . group ( 'cmd' ) lhs = m . group ( 'lhs' ) new line = '%s = get ipython().magic(%r)' % ( lhs , cmd ) return new line return line
def transform ipy prompt ( line ) : if not line or line . isspace ( ) : return line #print 'LINE:  %r' % line # dbg m = ipy prompt re . match ( line ) if m : #print 'MATCH! %r -> %r' % (line, line[len(m.group(0)):]) # dbg return line [ len ( m . group ( 0 ) ) : ] else : return line
def reset ( self ) : self . indent spaces = 0 self . buffer [ : ] = [ ] self . source = '' self . code = None self . is complete = False self . full dedent = False
def reset ( self ) : super ( I Python Input Splitter , self ) . reset ( ) self . buffer raw [ : ] = [ ] self . source raw = '' self . cell magic parts = [ ] self . processing cell magic = False
def source raw reset ( self ) : out = self . source out r = self . source raw self . reset ( ) return out , out r
def handle cell magic ( self , lines ) : self . processing cell magic = True first , , body = lines . partition ( '\n' ) magic name , , line = first . partition ( ' ' ) magic name = magic name . lstrip ( ESC MAGIC ) # We store the body of the cell and create a call to a method that # will use this stored value. This is ugly, but it's a first cut to # get it all working, as right now changing the return API of our # methods would require major refactoring. self . cell magic parts = [ body ] tpl = 'get ipython(). run cached cell magic(%r, %r)' tlines = tpl % ( magic name , line ) self . store ( tlines ) self . store ( lines , self . buffer raw , 'source raw' ) # We can actually choose whether to allow for single blank lines here # during input for clients that use cell mode to decide when to stop # pushing input (currently only the Qt console). # My first implementation did that, and then I realized it wasn't # consistent with the terminal behavior, so I've reverted it to one # line.  But I'm leaving it here so we can easily test both behaviors, # I kind of liked having full blank lines allowed in the cell magics... #self. is complete = last two blanks(lines) self . is complete = last blank ( lines ) return self . is complete
def line mode cell append ( self , lines ) : # Only store the raw input.  Lines beyond the first one are only only # stored for history purposes; for execution the caller will grab the # magic pieces from cell magic parts and will assemble the cell body self . store ( lines , self . buffer raw , 'source raw' ) self . cell magic parts . append ( lines ) # Find out if the last stored block has a whitespace line as its # last line and also this line is whitespace, case in which we're # done (two contiguous blank lines signal termination).  Note that # the storage logic *enforces* that every stored block is # newline-terminated, so we grab everything but the last character # so we can have the body of the block alone. last block = self . cell magic parts [ - 1 ] self . is complete = last blank ( last block ) and lines . isspace ( ) return self . is complete
def transform cell ( self , cell ) : self . reset ( ) self . push ( cell ) return self . source reset ( )
def observers for notification ( self , ntype , sender ) : keys = ( ( ntype , sender ) , ( ntype , None ) , ( None , sender ) , ( None , None ) ) obs = set ( ) for k in keys : obs . update ( self . observers . get ( k , set ( ) ) ) return obs
def status ( self , verbose = 0 ) : self . update status ( ) self . group report ( self . running , 'Running' ) self . group report ( self . completed , 'Completed' ) self . group report ( self . dead , 'Dead' ) # Also flush the report queues self . comp report [ : ] = [ ] self . dead report [ : ] = [ ]
def init ( self ) : for attr in [ 'call' , 'strform' ] : assert hasattr ( self , attr ) , "Missing attribute <%s>" % attr # The num tag can be set by an external job manager self . num = None self . status = Background Job Base . stat created self . stat code = Background Job Base . stat created c self . finished = False self . result = '<Background Job has not completed>' # reuse the ipython traceback handler if we can get to it, otherwise # make a new one try : make tb = get ipython ( ) . Interactive TB . text except : make tb = Auto Formatted TB ( mode = 'Context' , color scheme = 'No Color' , tb offset = 1 ) . text # Note that the actual API for text() requires the three args to be # passed in, so we wrap it in a simple lambda. self . make tb = lambda : make tb ( None , None , None ) # Hold a formatted traceback if one is generated. self . tb = None threading . Thread . init ( self )
def energy ( self , state = None ) : state = self . state if state is None else state route = state e = 0 if self . distance matrix : for i in range ( len ( route ) ) : e += self . distance matrix [ "{},{}" . format ( route [ i - 1 ] , route [ i ] ) ] else : for i in range ( len ( route ) ) : e += distance ( self . cities [ route [ i - 1 ] ] , self . cities [ route [ i ] ] ) return e
def defaults ( self , keys = None ) : d = { } keys = self . keys if keys is None else keys for key in keys : d [ key ] = None return d
def init db ( self ) : # register adapters sqlite3 . register adapter ( dict , adapt dict ) sqlite3 . register converter ( 'dict' , convert dict ) sqlite3 . register adapter ( list , adapt bufs ) sqlite3 . register converter ( 'bufs' , convert bufs ) # connect to the db dbfile = os . path . join ( self . location , self . filename ) self . db = sqlite3 . connect ( dbfile , detect types = sqlite3 . PARSE DECLTYPES , # isolation level = None)#, cached statements = 64 ) # print dir(self. db) first table = previous table = self . table i = 0 while not self . check table ( ) : i += 1 self . table = first table + ' %i' % i self . log . warn ( "Table %s exists and doesn't match db format, trying %s" % ( previous table , self . table ) ) previous table = self . table self . db . execute ( % self . table ) self . db . commit ( )
def render expression ( self , check ) : expressions = [ ] args = [ ] skeys = set ( check . keys ( ) ) skeys . difference update ( set ( self . keys ) ) skeys . difference update ( set ( [ 'buffers' , 'result buffers' ] ) ) if skeys : raise Key Error ( "Illegal testing key(s): %s" % skeys ) for name , sub check in check . iteritems ( ) : if isinstance ( sub check , dict ) : for test , value in sub check . iteritems ( ) : try : op = operators [ test ] except Key Error : raise Key Error ( "Unsupported operator: %r" % test ) if isinstance ( op , tuple ) : op , join = op if value is None and op in null operators : expr = "%s %s" % ( name , null operators [ op ] ) else : expr = "%s %s ?" % ( name , op ) if isinstance ( value , ( tuple , list ) ) : if op in null operators and any ( [ v is None for v in value ] ) : # equality tests don't work with NULL raise Value Error ( "Cannot use %r test with NULL values on SQ Lite backend" % test ) expr = '( %s )' % ( join . join ( [ expr ] * len ( value ) ) ) args . extend ( value ) else : args . append ( value ) expressions . append ( expr ) else : # it's an equality check if sub check is None : expressions . append ( "%s IS NULL" % name ) else : expressions . append ( "%s = ?" % name ) args . append ( sub check ) expr = " AND " . join ( expressions ) return expr , args
def add record ( self , msg id , rec ) : d = self . defaults ( ) d . update ( rec ) d [ 'msg id' ] = msg id line = self . dict to list ( d ) tups = '(%s)' % ( ',' . join ( [ '?' ] * len ( line ) ) ) self . db . execute ( "INSERT INTO %s VALUES %s" % ( self . table , tups ) , line )
def get record ( self , msg id ) : cursor = self . db . execute ( """SELECT * FROM %s WHERE msg id==?""" % self . table , ( msg id , ) ) line = cursor . fetchone ( ) if line is None : raise Key Error ( "No such msg: %r" % msg id ) return self . list to dict ( line )
def update record ( self , msg id , rec ) : query = "UPDATE %s SET " % self . table sets = [ ] keys = sorted ( rec . keys ( ) ) values = [ ] for key in keys : sets . append ( '%s = ?' % key ) values . append ( rec [ key ] ) query += ', ' . join ( sets ) query += ' WHERE msg id == ?' values . append ( msg id ) self . db . execute ( query , values )
def drop matching records ( self , check ) : expr , args = self . render expression ( check ) query = "DELETE FROM %s WHERE %s" % ( self . table , expr ) self . db . execute ( query , args )
def get history ( self ) : query = """SELECT msg id FROM %s ORDER by submitted ASC""" % self . table cursor = self . db . execute ( query ) # will be a list of length 1 tuples return [ tup [ 0 ] for tup in cursor . fetchall ( ) ]
def table ( rows ) : output = '<table>' for row in rows : output += '<tr>' for column in row : output += '<td>{s}</td>' . format ( s = column ) output += '</tr>' output += '</table>' return output
def link ( url , text = '' , classes = '' , target = '' , get = "" , * * kwargs ) : if not ( url . startswith ( 'http' ) or url . startswith ( '/' ) ) : # Handle additional reverse args. urlargs = { } for arg , val in kwargs . items ( ) : if arg [ : 4 ] == "url " : urlargs [ arg [ 4 : ] ] = val url = reverse ( url , kwargs = urlargs ) if get : url += '?' + get return html . tag ( 'a' , text or url , { 'class' : classes , 'target' : target , 'href' : url } )
def jsfile ( url ) : if not url . startswith ( 'http://' ) and not url [ : 1 ] == '/' : #add media url for relative paths url = settings . STATIC URL + url return '<script type="text/javascript" src="{src}"></script>' . format ( src = url )
def cssfile ( url ) : if not url . startswith ( 'http://' ) and not url [ : 1 ] == '/' : #add media url for relative paths url = settings . STATIC URL + url return '<link href="{src}" rel="stylesheet">' . format ( src = url )
def img ( url , alt = '' , classes = '' , style = '' ) : if not url . startswith ( 'http://' ) and not url [ : 1 ] == '/' : #add media url for relative paths url = settings . STATIC URL + url attr = { 'class' : classes , 'alt' : alt , 'style' : style , 'src' : url } return html . tag ( 'img' , '' , attr )
def sub ( value , arg ) : try : return valid numeric ( value ) - valid numeric ( arg ) except ( Value Error , Type Error ) : try : return value - arg except Exception : return ''
def mul ( value , arg ) : try : return valid numeric ( value ) * valid numeric ( arg ) except ( Value Error , Type Error ) : try : return value * arg except Exception : return ''
def div ( value , arg ) : try : return valid numeric ( value ) / valid numeric ( arg ) except ( Value Error , Type Error ) : try : return value / arg except Exception : return ''
def mod ( value , arg ) : try : return valid numeric ( value ) % valid numeric ( arg ) except ( Value Error , Type Error ) : try : return value % arg except Exception : return ''
def run ( ) : config option help = "'show' - displays configured options, 'set [section] [name] [value]' - sets config under a section,'set [name] [value]' - sets configuration globally" parser = Option Parser ( ) parser . add option ( "-a" , "--add" , action = "store" , type = "string" , dest = "addfile" , help = "adds a notes" ) parser . add option ( "-c" , "--config" , action = "store" , type = "string" , dest = "config" , help = config option help ) parser . add option ( "-e" , "--edit" , action = "store" , type = "string" , dest = "editfile" , help = "edits a notes" ) parser . add option ( "-o" , "--open" , action = "store" , type = "string" , dest = "openfile" , help = "opens a notes" ) parser . add option ( "-r" , "--remove" , action = "store" , type = "string" , dest = "remove" , help = "removes a notes" ) options , args = parser . parse args ( ) if options . config : if options . config == "show" : config option list = '' config sections = config . sections ( ) for section in config sections : config option list = config option list + section + "\n" section items = config . items ( section ) for item in section items : config option list = config option list + "    " + item [ 0 ] + "    " + item [ 1 ] + "\n" print config option list quit ( ) def add notes ( note name , existing tags ) : call ( [ editor , environ [ "HOME" ] + "/.mypy/myhelp/notes/" + note name + ".note" ] ) definedtags = raw input ( "Define Tags (separated by spaces): " ) . split ( " " ) definedtags . append ( note name ) print definedtags print existing tags definedtags = list ( set ( definedtags ) - set ( existing tags ) ) print definedtags if len ( definedtags ) > 0 : modify tags xml ( note name , definedtags , files , rootfiles , tags , roottags , tree , TAGS XML DIR ) def get tags from file ( note name ) : fil = get file from files ( note name ) filetags = fil . iter ( 'tag' ) filetaglist = [ ] for tag in filetags : filetaglist . append ( tag . text ) return filetaglist if options . addfile : existing tags = [ ] if is File ( options . addfile , files ) : existing tags = get tags from file ( options . addfile ) raw input ( "Note exists with tags - " + " " . join ( existing tags ) + "\n Do you want to edit the notes ? [Press enter to continue]\n" ) add notes ( options . addfile , existing tags ) quit ( ) if options . editfile : if is File ( options . editfile , files ) : add notes ( note name , [ ] ) else : raw input ( "Note doesn't exist.\n Do you want add note ? [Press enter to continue]" ) add notes ( note name ) if options . remove : pass if len ( args ) != 1 : print "Please use a search term\n example : myhelp <some tag word> " quit ( ) key File = "Note" key Results = "    Results                                     " table = { key Results : [ ] } for tag in tags : if tag . attrib [ "value" ] == args [ 0 ] : fileelements = tag . iter ( "file" ) for fileelement in fileelements : f = open ( environ [ "HOME" ] + "/.mypy/myhelp/notes/" + fileelement . text + ".note" , "r" ) table [ key Results ] . append ( f . read ( ) + "\r\n\tfile: ~/.mypy/myhelp/notes/" + fileelement . text + ".note" ) f . close ( ) print tabulate ( table , headers = [ ] , tablefmt = "rst" )
def options ( self , parser , env ) : parser . add option ( "--processes" , action = "store" , default = env . get ( 'NOSE PROCESSES' , 0 ) , dest = "multiprocess workers" , metavar = "NUM" , help = "Spread test run among this many processes. " "Set a number equal to the number of processors " "or cores in your machine for best results. " "[NOSE PROCESSES]" ) parser . add option ( "--process-timeout" , action = "store" , default = env . get ( 'NOSE PROCESS TIMEOUT' , 10 ) , dest = "multiprocess timeout" , metavar = "SECONDS" , help = "Set timeout for return of results from each " "test runner process. [NOSE PROCESS TIMEOUT]" ) parser . add option ( "--process-restartworker" , action = "store true" , default = env . get ( 'NOSE PROCESS RESTARTWORKER' , False ) , dest = "multiprocess restartworker" , help = "If set, will restart each worker process once" " their tests are done, this helps control memory " "leaks from killing the system. " "[NOSE PROCESS RESTARTWORKER]" )
def run ( self , result ) : # proxy the result for myself log . debug ( "suite %s (%s) run called, tests: %s" , id ( self ) , self , self . tests ) if self . result Proxy : result , orig = self . result Proxy ( result , self ) , result else : result , orig = result , result try : #log.debug('set Up for %s', id(self)); self . set Up ( ) except Keyboard Interrupt : raise except : self . error context = 'setup' result . add Error ( self , self . exc info ( ) ) return try : for test in self . tests : if ( isinstance ( test , nose . case . Test ) and self . arg is not None ) : test . test . arg = self . arg else : test . arg = self . arg test . test Queue = self . test Queue test . tasks = self . tasks if result . should Stop : log . debug ( "stopping" ) break # each nose.case.Test will create its own result proxy # so the cases need the original result, to avoid proxy # chains #log.debug('running test %s in suite %s', test, self); try : test ( orig ) except Keyboard Interrupt , e : timeout = isinstance ( e , Timed Out Exception ) if timeout : msg = 'Timeout when running test %s in suite %s' else : msg = 'Keyboard Interrupt when running test %s in suite %s' log . debug ( msg , test , self ) err = ( Timed Out Exception , Timed Out Exception ( str ( test ) ) , sys . exc info ( ) [ 2 ] ) test . config . plugins . add Error ( test , err ) orig . add Error ( test , err ) if not timeout : raise finally : self . has run = True try : #log.debug('tear Down for %s', id(self)); self . tear Down ( ) except Keyboard Interrupt : raise except : self . error context = 'teardown' result . add Error ( self , self . exc info ( ) )
def add builtin ( self , key , value ) : bdict = builtin . dict orig = bdict . get ( key , Builtin Undefined ) if value is Hide Builtin : if orig is not Builtin Undefined : #same as 'key in bdict' self . orig builtins [ key ] = orig del bdict [ key ] else : self . orig builtins [ key ] = orig bdict [ key ] = value
def remove builtin ( self , key , orig ) : if orig is Builtin Undefined : del builtin . dict [ key ] else : builtin . dict [ key ] = orig
def activate ( self ) : add builtin = self . add builtin for name , func in self . auto builtins . iteritems ( ) : add builtin ( name , func )
def explicit rel links ( self , rels = ( 'homepage' , 'download' ) ) : rels = set ( rels ) for anchor in self . parsed . findall ( ".//a" ) : if anchor . get ( "rel" ) and anchor . get ( "href" ) : found rels = set ( anchor . get ( "rel" ) . split ( ) ) # Determine the intersection between what rels were found and #   what rels were being looked for if found rels & rels : href = anchor . get ( "href" ) url = self . clean link ( urllib parse . urljoin ( self . base url , href ) ) yield Link ( url , self , trusted = False )
def unshell list ( s ) : if not s : return None if sys . platform == 'win32' : # When running coverage as coverage.exe, some of the behavior # of the shell is emulated: wildcards are expanded into a list of # filenames.  So you have to single-quote patterns on the command # line, but (not) helpfully, the single quotes are included in the # argument, so we have to strip them off here. s = s . strip ( "'" ) return s . split ( ',' )
def add action ( self , dash , dashdash , action code ) : option = self . add option ( dash , dashdash , action = 'callback' , callback = self . append action ) option . action code = action code
def append action ( self , option , opt unused , value unused , parser ) : parser . values . actions . append ( option . action code )
def help ( self , error = None , topic = None , parser = None ) : assert error or topic or parser if error : print ( error ) print ( "Use 'coverage help' for help." ) elif parser : print ( parser . format help ( ) . strip ( ) ) else : help msg = HELP TOPICS . get ( topic , '' ) . strip ( ) if help msg : print ( help msg % self . covpkg . dict ) else : print ( "Don't know topic %r" % topic )
def do execute ( self , options , args ) : # Set the first path element properly. old path0 = sys . path [ 0 ] # Run the script. self . coverage . start ( ) code ran = True try : try : if options . module : sys . path [ 0 ] = '' self . run python module ( args [ 0 ] , args ) else : filename = args [ 0 ] sys . path [ 0 ] = os . path . abspath ( os . path . dirname ( filename ) ) self . run python file ( filename , args ) except No Source : code ran = False raise finally : self . coverage . stop ( ) if code ran : self . coverage . save ( ) # Restore the old path sys . path [ 0 ] = old path0
def do debug ( self , args ) : if not args : self . help fn ( "What information would you like: data, sys?" ) return ERR for info in args : if info == 'sys' : print ( "-- sys ----------------------------------------" ) for line in info formatter ( self . coverage . sysinfo ( ) ) : print ( " %s" % line ) elif info == 'data' : print ( "-- data ---------------------------------------" ) self . coverage . load ( ) print ( "path: %s" % self . coverage . data . filename ) print ( "has arcs: %r" % self . coverage . data . has arcs ( ) ) summary = self . coverage . data . summary ( fullpath = True ) if summary : filenames = sorted ( summary . keys ( ) ) print ( "\n%d files:" % len ( filenames ) ) for f in filenames : print ( "%s: %d lines" % ( f , summary [ f ] ) ) else : print ( "No data collected" ) else : self . help fn ( "Don't know what you mean by %r" % info ) return ERR return OK
def unserialize object ( bufs ) : bufs = list ( bufs ) sobj = pickle . loads ( bufs . pop ( 0 ) ) if isinstance ( sobj , ( list , tuple ) ) : for s in sobj : if s . data is None : s . data = bufs . pop ( 0 ) return uncan Sequence ( map ( unserialize , sobj ) ) , bufs elif isinstance ( sobj , dict ) : newobj = { } for k in sorted ( sobj . iterkeys ( ) ) : s = sobj [ k ] if s . data is None : s . data = bufs . pop ( 0 ) newobj [ k ] = uncan ( unserialize ( s ) ) return newobj , bufs else : if sobj . data is None : sobj . data = bufs . pop ( 0 ) return uncan ( unserialize ( sobj ) ) , bufs
def set ( self ) : if sys . displayhook is not self . hook : self . old hook = sys . displayhook sys . displayhook = self . hook
def is url ( url ) : if '://' not in url : return False proto , addr = url . split ( '://' , 1 ) if proto . lower ( ) not in [ 'tcp' , 'pgm' , 'epgm' , 'ipc' , 'inproc' ] : return False return True
def validate url ( url ) : if not isinstance ( url , basestring ) : raise Type Error ( "url must be a string, not %r" % type ( url ) ) url = url . lower ( ) proto addr = url . split ( '://' ) assert len ( proto addr ) == 2 , 'Invalid url: %r' % url proto , addr = proto addr assert proto in [ 'tcp' , 'pgm' , 'epgm' , 'ipc' , 'inproc' ] , "Invalid protocol: %r" % proto # domain pattern adapted from http://www.regexlib.com/RE Details.aspx?regexp id=391 # author: Remi Sabourin pat = re . compile ( r'^([\w\d]([\w\d\-]{0,61}[\w\d])?\.)*[\w\d]([\w\d\-]{0,61}[\w\d])?$' ) if proto == 'tcp' : lis = addr . split ( ':' ) assert len ( lis ) == 2 , 'Invalid url: %r' % url addr , s port = lis try : port = int ( s port ) except Value Error : raise Assertion Error ( "Invalid port %r in url: %r" % ( port , url ) ) assert addr == '*' or pat . match ( addr ) is not None , 'Invalid url: %r' % url else : # only validate tcp urls currently pass return True
def validate url container ( container ) : if isinstance ( container , basestring ) : url = container return validate url ( url ) elif isinstance ( container , dict ) : container = container . itervalues ( ) for element in container : validate url container ( element )
def pull ( keys ) : user ns = globals ( ) if isinstance ( keys , ( list , tuple , set ) ) : for key in keys : if not user ns . has key ( key ) : raise Name Error ( "name '%s' is not defined" % key ) return map ( user ns . get , keys ) else : if not user ns . has key ( keys ) : raise Name Error ( "name '%s' is not defined" % keys ) return user ns . get ( keys )
def select random ports ( n ) : ports = [ ] for i in xrange ( n ) : sock = socket . socket ( ) sock . bind ( ( '' , 0 ) ) while sock . getsockname ( ) [ 1 ] in random ports : sock . close ( ) sock = socket . socket ( ) sock . bind ( ( '' , 0 ) ) ports . append ( sock ) for i , sock in enumerate ( ports ) : port = sock . getsockname ( ) [ 1 ] sock . close ( ) ports [ i ] = port random ports . add ( port ) return ports
def get readline tail ( self , n = 10 ) : end = self . shell . readline . get current history length ( ) + 1 start = max ( end - n , 1 ) ghi = self . shell . readline . get history item return [ ghi ( x ) for x in range ( start , end ) ]
def init logstart ( self ) : if self . logappend : self . magic ( 'logstart %s append' % self . logappend ) elif self . logfile : self . magic ( 'logstart %s' % self . logfile ) elif self . logstart : self . magic ( 'logstart' )
def restore sys module state ( self ) : try : for k , v in self . orig sys module state . iteritems ( ) : setattr ( sys , k , v ) except Attribute Error : pass # Reset what what done in self.init sys modules if self . orig sys modules main mod is not None : sys . modules [ self . orig sys modules main name ] = self . orig sys modules main mod
def register post execute ( self , func ) : if not callable ( func ) : raise Value Error ( 'argument %s must be callable' % func ) self . post execute [ func ] = True
def new main mod ( self , ns = None ) : main mod = self . user main module init fakemod dict ( main mod , ns ) return main mod
def ofind property ( self , oname , info ) : if info . found : # Get the docstring of the class property if it exists. path = oname . split ( '.' ) root = '.' . join ( path [ : - 1 ] ) if info . parent is not None : try : target = getattr ( info . parent , ' class ' ) # The object belongs to a class instance. try : target = getattr ( target , path [ - 1 ] ) # The class defines the object. if isinstance ( target , property ) : oname = root + '. class .' + path [ - 1 ] info = Struct ( self . ofind ( oname ) ) except Attribute Error : pass except Attribute Error : pass # We return either the new info or the unmodified input if the object # hadn't been found return info
def object find ( self , oname , namespaces = None ) : inf = Struct ( self . ofind ( oname , namespaces ) ) return Struct ( self . ofind property ( oname , inf ) )
def init history ( self ) : self . history manager = History Manager ( shell = self , config = self . config ) self . configurables . append ( self . history manager )
def set completer frame ( self , frame = None ) : if frame : self . Completer . namespace = frame . f locals self . Completer . global namespace = frame . f globals else : self . Completer . namespace = self . user ns self . Completer . global namespace = self . user global ns
def ex ( self , cmd ) : with self . builtin trap : exec cmd in self . user global ns , self . user ns
def run cached cell magic ( self , magic name , line ) : cell = self . current cell magic body self . current cell magic body = None return self . run cell magic ( magic name , line , cell )
def broadcast ( client , sender , msg name , dest name = None , block = None ) : dest name = msg name if dest name is None else dest name client [ sender ] . execute ( 'com.publish(%s)' % msg name , block = None ) targets = client . ids targets . remove ( sender ) return client [ targets ] . execute ( '%s=com.consume()' % dest name , block = None )
def send ( client , sender , targets , msg name , dest name = None , block = None ) : dest name = msg name if dest name is None else dest name def send ( targets , m name ) : msg = globals ( ) [ m name ] return com . send ( targets , msg ) client [ sender ] . apply async ( send , targets , msg name ) return client [ targets ] . execute ( '%s=com.recv()' % dest name , block = None )
def list profiles in ( path ) : files = os . listdir ( path ) profiles = [ ] for f in files : full path = os . path . join ( path , f ) if os . path . isdir ( full path ) and f . startswith ( 'profile ' ) : profiles . append ( f . split ( ' ' , 1 ) [ - 1 ] ) return profiles
def list bundled profiles ( ) : path = os . path . join ( get ipython package dir ( ) , u'config' , u'profile' ) files = os . listdir ( path ) profiles = [ ] for profile in files : full path = os . path . join ( path , profile ) if os . path . isdir ( full path ) and profile != " pycache " : profiles . append ( profile ) return profiles
def next ( self ) : # File-like object. result = self . readline ( ) if result == self . empty buffer : raise Stop Iteration return result
def prepare regex pattern ( self , p ) : if isinstance ( p . pattern , unicode ) : p = re . compile ( p . pattern . encode ( 'utf-8' ) , p . flags & ~ re . UNICODE ) return p
def prepare regex pattern ( self , p ) : if isinstance ( p . pattern , bytes ) : p = re . compile ( p . pattern . decode ( self . encoding ) , p . flags ) return p
def finish displayhook ( self ) : sys . stdout . flush ( ) sys . stderr . flush ( ) self . session . send ( self . pub socket , self . msg , ident = self . topic ) self . msg = None
def log listener ( log : logging . Logger = None , level = logging . INFO ) : if log is None : log = logging . get Logger ( "Progress Monitor" ) def listen ( monitor ) : name = "{}: " . format ( monitor . name ) if monitor . name is not None else "" perc = int ( monitor . progress * 100 ) msg = "[{name}{perc:3d}%] {monitor.message}" . format ( * * locals ( ) ) log . log ( level , msg ) return listen
def last error ( self ) : if not len ( self . log ) : raise Runtime Error ( 'Nothing executed' ) try : errs = [ l for l in self . log if l [ 1 ] != 0 ] return errs [ - 1 ] [ 2 ] except Index Error : # odd case where there were no errors #TODO return 'no last error'
def check output ( self , cmd ) : ret , output = self . exec ( cmd ) if not ret == 0 : raise Command Error ( self ) return output
def arcs executed ( self ) : executed = self . coverage . data . executed arcs ( self . filename ) m2fl = self . parser . first line executed = [ ( m2fl ( l1 ) , m2fl ( l2 ) ) for ( l1 , l2 ) in executed ] return sorted ( executed )
def arcs missing ( self ) : possible = self . arc possibilities ( ) executed = self . arcs executed ( ) missing = [ p for p in possible if p not in executed and p [ 0 ] not in self . no branch ] return sorted ( missing )
def arcs unpredicted ( self ) : possible = self . arc possibilities ( ) executed = self . arcs executed ( ) # Exclude arcs here which connect a line to itself.  They can occur # in executed data in some cases.  This is where they can cause # trouble, and here is where it's the least burden to remove them. unpredicted = [ e for e in executed if e not in possible and e [ 0 ] != e [ 1 ] ] return sorted ( unpredicted )
def branch lines ( self ) : exit counts = self . parser . exit counts ( ) return [ l1 for l1 , count in iitems ( exit counts ) if count > 1 ]
def total branches ( self ) : exit counts = self . parser . exit counts ( ) return sum ( [ count for count in exit counts . values ( ) if count > 1 ] )
def set precision ( cls , precision ) : assert 0 <= precision < 10 cls . precision = precision cls . near0 = 1.0 / 10 ** precision cls . near100 = 100.0 - cls . near0
def get pc covered ( self ) : if self . n statements > 0 : pc cov = ( 100.0 * ( self . n executed + self . n executed branches ) / ( self . n statements + self . n branches ) ) else : pc cov = 100.0 return pc cov
def highlight text ( needles , haystack , cls name = 'highlighted' , words = False , case = False ) : if not needles : return haystack if not haystack : return '' if words : pattern = r"(%s)" % "|" . join ( [ '\\b{}\\b' . format ( re . escape ( n ) ) for n in needles ] ) else : pattern = r"(%s)" % "|" . join ( [ re . escape ( n ) for n in needles ] ) if case : regex = re . compile ( pattern ) else : regex = re . compile ( pattern , re . I ) i , out = 0 , "" for m in regex . finditer ( haystack ) : out += "" . join ( [ haystack [ i : m . start ( ) ] , '<span class="%s">' % cls name , haystack [ m . start ( ) : m . end ( ) ] , "</span>" ] ) i = m . end ( ) return mark safe ( out + haystack [ i : ] )
def highlight ( string , keywords , cls name = 'highlighted' ) : if not keywords : return string if not string : return '' include , exclude = get text tokenizer ( keywords ) highlighted = highlight text ( include , string , cls name ) return highlighted
def highlight words ( string , keywords , cls name = 'highlighted' ) : if not keywords : return string if not string : return '' include , exclude = get text tokenizer ( keywords ) highlighted = highlight text ( include , string , cls name , words = True ) return highlighted
def run setup ( setup script , args ) : old dir = os . getcwd ( ) save argv = sys . argv [ : ] save path = sys . path [ : ] setup dir = os . path . abspath ( os . path . dirname ( setup script ) ) temp dir = os . path . join ( setup dir , 'temp' ) if not os . path . isdir ( temp dir ) : os . makedirs ( temp dir ) save tmp = tempfile . tempdir save modules = sys . modules . copy ( ) pr state = pkg resources . getstate ( ) try : tempfile . tempdir = temp dir os . chdir ( setup dir ) try : sys . argv [ : ] = [ setup script ] + list ( args ) sys . path . insert ( 0 , setup dir ) Directory Sandbox ( setup dir ) . run ( lambda : execfile ( "setup.py" , { ' file ' : setup script , ' name ' : ' main ' } ) ) except System Exit , v : if v . args and v . args [ 0 ] : raise # Normal exit, just return finally : pkg resources . setstate ( pr state ) sys . modules . update ( save modules ) # remove any modules imported within the sandbox del modules = [ mod name for mod name in sys . modules if mod name not in save modules # exclude any encodings modules. See #285 and not mod name . startswith ( 'encodings.' ) ] map ( sys . modules . delitem , del modules ) os . chdir ( old dir ) sys . path [ : ] = save path sys . argv [ : ] = save argv tempfile . tempdir = save tmp
def run ( self , func ) : try : self . copy ( self ) if file : builtin . file = self . file builtin . open = self . open self . active = True return func ( ) finally : self . active = False if file : builtin . file = file builtin . open = open self . copy ( os )
def unquote ends ( istr ) : if not istr : return istr if ( istr [ 0 ] == "'" and istr [ - 1 ] == "'" ) or ( istr [ 0 ] == '"' and istr [ - 1 ] == '"' ) : return istr [ 1 : - 1 ] else : return istr
def find optimal ( rlist , separator size = 2 , displaywidth = 80 ) : for nrow in range ( 1 , len ( rlist ) + 1 ) : chk = map ( max , chunks ( rlist , nrow ) ) sumlength = sum ( chk ) ncols = len ( chk ) if sumlength + separator size * ( ncols - 1 ) <= displaywidth : break return { 'columns numbers' : ncols , 'optimal separator width' : ( displaywidth - sumlength ) / ( ncols - 1 ) if ( ncols - 1 ) else 0 , 'rows numbers' : nrow , 'columns width' : chk }
def get or default ( mylist , i , default = None ) : if i >= len ( mylist ) : return default else : return mylist [ i ]
def build kernel argv ( self , argv = None ) : if argv is None : argv = sys . argv [ 1 : ] self . kernel argv = swallow argv ( argv , self . frontend aliases , self . frontend flags ) # kernel should inherit default config file from frontend self . kernel argv . append ( "--Kernel App.parent appname='%s'" % self . name )
def init ssh ( self ) : if not self . sshserver and not self . sshkey : return if self . sshkey and not self . sshserver : # specifying just the key implies that we are connecting directly self . sshserver = self . ip self . ip = LOCALHOST # build connection dict for tunnels: info = dict ( ip = self . ip , shell port = self . shell port , iopub port = self . iopub port , stdin port = self . stdin port , hb port = self . hb port ) self . log . info ( "Forwarding connections to %s via %s" % ( self . ip , self . sshserver ) ) # tunnels return a new set of ports, which will be on localhost: self . ip = LOCALHOST try : newports = tunnel to kernel ( info , self . sshserver , self . sshkey ) except : # even catch Keyboard Interrupt self . log . error ( "Could not setup tunnels" , exc info = True ) self . exit ( 1 ) self . shell port , self . iopub port , self . stdin port , self . hb port = newports cf = self . connection file base , ext = os . path . splitext ( cf ) base = os . path . basename ( base ) self . connection file = os . path . basename ( base ) + '-ssh' + ext self . log . critical ( "To connect another client via this tunnel, use:" ) self . log . critical ( "--existing %s" % self . connection file )
def pretty ( obj , verbose = False , max width = 79 , newline = '\n' ) : stream = String IO ( ) printer = Representation Printer ( stream , verbose , max width , newline ) printer . pretty ( obj ) printer . flush ( ) return stream . getvalue ( )
def pprint ( obj , verbose = False , max width = 79 , newline = '\n' ) : printer = Representation Printer ( sys . stdout , verbose , max width , newline ) printer . pretty ( obj ) printer . flush ( ) sys . stdout . write ( newline ) sys . stdout . flush ( )
def super pprint ( obj , p , cycle ) : p . begin group ( 8 , '<super: ' ) p . pretty ( obj . self class ) p . text ( ',' ) p . breakable ( ) p . pretty ( obj . self ) p . end group ( 8 , '>' )
def re pattern pprint ( obj , p , cycle ) : p . text ( 're.compile(' ) pattern = repr ( obj . pattern ) if pattern [ : 1 ] in 'u U' : pattern = pattern [ 1 : ] prefix = 'ur' else : prefix = 'r' pattern = prefix + pattern . replace ( '\\\\' , '\\' ) p . text ( pattern ) if obj . flags : p . text ( ',' ) p . breakable ( ) done one = False for flag in ( 'TEMPLATE' , 'IGNORECASE' , 'LOCALE' , 'MULTILINE' , 'DOTALL' , 'UNICODE' , 'VERBOSE' , 'DEBUG' ) : if obj . flags & getattr ( re , flag ) : if done one : p . text ( '|' ) p . text ( 're.' + flag ) done one = True p . text ( ')' )
def type pprint ( obj , p , cycle ) : if obj . module in ( ' builtin ' , 'exceptions' ) : name = obj . name else : name = obj . module + '.' + obj . name p . text ( name )
def function pprint ( obj , p , cycle ) : if obj . module in ( ' builtin ' , 'exceptions' ) or not obj . module : name = obj . name else : name = obj . module + '.' + obj . name p . text ( '<function %s>' % name )
def exception pprint ( obj , p , cycle ) : if obj . class . module in ( 'exceptions' , 'builtins' ) : name = obj . class . name else : name = '%s.%s' % ( obj . class . module , obj . class . name ) step = len ( name ) + 1 p . begin group ( step , name + '(' ) for idx , arg in enumerate ( getattr ( obj , 'args' , ( ) ) ) : if idx : p . text ( ',' ) p . breakable ( ) p . pretty ( arg ) p . end group ( step , ')' )
def for type ( typ , func ) : oldfunc = type pprinters . get ( typ , None ) if func is not None : # To support easy restoration of old pprinters, we need to ignore Nones. type pprinters [ typ ] = func return oldfunc
def text ( self , obj ) : width = len ( obj ) if self . buffer : text = self . buffer [ - 1 ] if not isinstance ( text , Text ) : text = Text ( ) self . buffer . append ( text ) text . add ( obj , width ) self . buffer width += width self . break outer groups ( ) else : self . output . write ( obj ) self . output width += width
def end group ( self , dedent = 0 , close = '' ) : self . indentation -= dedent group = self . group stack . pop ( ) if not group . breakables : self . group queue . remove ( group ) if close : self . text ( close )
def flush ( self ) : for data in self . buffer : self . output width += data . output ( self . output , self . output width ) self . buffer . clear ( ) self . buffer width = 0
def pretty ( self , obj ) : obj id = id ( obj ) cycle = obj id in self . stack self . stack . append ( obj id ) self . begin group ( ) try : obj class = getattr ( obj , ' class ' , None ) or type ( obj ) # First try to find registered singleton printers for the type. try : printer = self . singleton pprinters [ obj id ] except ( Type Error , Key Error ) : pass else : return printer ( obj , self , cycle ) # Next walk the mro and check for either: #   1) a registered printer #   2) a  repr pretty  method for cls in get mro ( obj class ) : if cls in self . type pprinters : # printer registered in self.type pprinters return self . type pprinters [ cls ] ( obj , self , cycle ) else : # deferred printer printer = self . in deferred types ( cls ) if printer is not None : return printer ( obj , self , cycle ) else : # Finally look for special method names. # Some objects automatically create any requested # attribute. Try to ignore most of them by checking for # callability. if ' repr pretty ' in obj class . dict : meth = obj class . repr pretty if callable ( meth ) : return meth ( obj , self , cycle ) return default pprint ( obj , self , cycle ) finally : self . end group ( ) self . stack . pop ( )
def write row into ods ( ods , sheet no , row no , row ) : ods . content . get Sheet ( sheet no ) for j , col in enumerate ( row ) : cell = ods . content . get Cell ( j , row no + 1 ) cell . string Value ( escape apostrophe ( col ) ) if j % 2 == 1 : cell . set Cell Color ( settings . EVEN COLUMN BG COLOR ) else : cell . set Cell Color ( settings . ODD COLUMN BG COLOR )
def osx clipboard get ( ) : p = subprocess . Popen ( [ 'pbpaste' , '-Prefer' , 'ascii' ] , stdout = subprocess . PIPE ) text , stderr = p . communicate ( ) # Text comes in with old Mac \r line endings. Change them to \n. text = text . replace ( '\r' , '\n' ) return text
def get build prefix ( ) : path = os . path . join ( tempfile . gettempdir ( ) , 'pip build %s' % get username ( ) . replace ( ' ' , ' ' ) ) if WINDOWS : """ on windows(tested on 7) temp dirs are isolated """ return path try : os . mkdir ( path ) write delete marker file ( path ) except OS Error : file uid = None try : # raises OS Error for symlinks file uid = get path uid ( path ) except OS Error : file uid = None if file uid != os . geteuid ( ) : msg = ( "The temporary folder for building (%s) is either not owned by" " you, or is a symlink." % path ) print ( msg ) print ( "pip will not work until the temporary folder is either " "deleted or is a real directory owned by your user account." ) raise exceptions . Installation Error ( msg ) return path
def prepare communication ( self ) : Rect Partitioner . prepare communication ( self ) if self . lower neighbors [ 0 ] >= 0 : self . in lower buffers = [ zeros ( 1 , float ) ] self . out lower buffers = [ zeros ( 1 , float ) ] if self . upper neighbors [ 0 ] >= 0 : self . in upper buffers = [ zeros ( 1 , float ) ] self . out upper buffers = [ zeros ( 1 , float ) ]
def prepare communication ( self ) : Rect Partitioner . prepare communication ( self ) self . in lower buffers = [ [ ] , [ ] ] self . out lower buffers = [ [ ] , [ ] ] self . in upper buffers = [ [ ] , [ ] ] self . out upper buffers = [ [ ] , [ ] ] size1 = self . subd hi ix [ 1 ] - self . subd lo ix [ 1 ] + 1 if self . lower neighbors [ 0 ] >= 0 : self . in lower buffers [ 0 ] = zeros ( size1 , float ) self . out lower buffers [ 0 ] = zeros ( size1 , float ) if self . upper neighbors [ 0 ] >= 0 : self . in upper buffers [ 0 ] = zeros ( size1 , float ) self . out upper buffers [ 0 ] = zeros ( size1 , float ) size0 = self . subd hi ix [ 0 ] - self . subd lo ix [ 0 ] + 1 if self . lower neighbors [ 1 ] >= 0 : self . in lower buffers [ 1 ] = zeros ( size0 , float ) self . out lower buffers [ 1 ] = zeros ( size0 , float ) if self . upper neighbors [ 1 ] >= 0 : self . in upper buffers [ 1 ] = zeros ( size0 , float ) self . out upper buffers [ 1 ] = zeros ( size0 , float )
def extract dates ( obj ) : if isinstance ( obj , dict ) : obj = dict ( obj ) # don't clobber for k , v in obj . iteritems ( ) : obj [ k ] = extract dates ( v ) elif isinstance ( obj , ( list , tuple ) ) : obj = [ extract dates ( o ) for o in obj ] elif isinstance ( obj , basestring ) : if ISO8601 PAT . match ( obj ) : obj = datetime . strptime ( obj , ISO8601 ) return obj
def squash dates ( obj ) : if isinstance ( obj , dict ) : obj = dict ( obj ) # don't clobber for k , v in obj . iteritems ( ) : obj [ k ] = squash dates ( v ) elif isinstance ( obj , ( list , tuple ) ) : obj = [ squash dates ( o ) for o in obj ] elif isinstance ( obj , datetime ) : obj = obj . strftime ( ISO8601 ) return obj
def date default ( obj ) : if isinstance ( obj , datetime ) : return obj . strftime ( ISO8601 ) else : raise Type Error ( "%r is not JSON serializable" % obj )
def check site dir ( self ) : instdir = normalize path ( self . install dir ) pth file = os . path . join ( instdir , 'easy-install.pth' ) # Is it a configured, PYTHONPATH, implicit, or explicit site dir? is site dir = instdir in self . all site dirs if not is site dir and not self . multi version : # No?  Then directly test whether it does .pth file processing is site dir = self . check pth processing ( ) else : # make sure we can write to target dir testfile = self . pseudo tempname ( ) + '.write-test' test exists = os . path . exists ( testfile ) try : if test exists : os . unlink ( testfile ) open ( testfile , 'w' ) . close ( ) os . unlink ( testfile ) except ( OS Error , IO Error ) : self . cant write to target ( ) if not is site dir and not self . multi version : # Can't install non-multi to non-site dir raise Distutils Error ( self . no default version msg ( ) ) if is site dir : if self . pth file is None : self . pth file = Pth Distributions ( pth file , self . all site dirs ) else : self . pth file = None PYTHONPATH = os . environ . get ( 'PYTHONPATH' , '' ) . split ( os . pathsep ) if instdir not in map ( normalize path , [ f for f in PYTHONPATH if f ] ) : # only PYTHONPATH dirs need a site.py, so pretend it's there self . sitepy installed = True elif self . multi version and not os . path . exists ( pth file ) : self . sitepy installed = True # don't need site.py in this case self . pth file = None # and don't create a .pth file self . install dir = instdir
def write script ( self , script name , contents , mode = "t" , * ignored ) : from setuptools . command . easy install import chmod , current umask log . info ( "Installing %s script to %s" , script name , self . install dir ) target = os . path . join ( self . install dir , script name ) self . outfiles . append ( target ) mask = current umask ( ) if not self . dry run : ensure directory ( target ) f = open ( target , "w" + mode ) f . write ( contents ) f . close ( ) chmod ( target , 0777 - mask )
def sleep here ( count , t ) : import time , sys print ( "hi from engine %i" % id ) sys . stdout . flush ( ) time . sleep ( t ) return count , t
def convert pyx sources to c ( self ) : def pyx to c ( source ) : if source . endswith ( '.pyx' ) : source = source [ : - 4 ] + '.c' return source self . sources = map ( pyx to c , self . sources )
def main ( connection file ) : ctx = zmq . Context . instance ( ) with open ( connection file ) as f : cfg = json . loads ( f . read ( ) ) location = cfg [ 'location' ] reg url = cfg [ 'url' ] session = Session ( key = str to bytes ( cfg [ 'exec key' ] ) ) query = ctx . socket ( zmq . DEALER ) query . connect ( disambiguate url ( cfg [ 'url' ] , location ) ) session . send ( query , "connection request" ) idents , msg = session . recv ( query , mode = 0 ) c = msg [ 'content' ] iopub url = disambiguate url ( c [ 'iopub' ] , location ) sub = ctx . socket ( zmq . SUB ) # This will subscribe to all messages: sub . setsockopt ( zmq . SUBSCRIBE , b'' ) # replace with b'' with b'engine.1.stdout' to subscribe only to engine 1's stdout # 0MQ subscriptions are simple 'foo*' matches, so 'engine.1.' subscribes # to everything from engine 1, but there is no way to subscribe to # just stdout from everyone. # multiple calls to subscribe will add subscriptions, e.g. to subscribe to # engine 1's stderr and engine 2's stdout: # sub.setsockopt(zmq.SUBSCRIBE, b'engine.1.stderr') # sub.setsockopt(zmq.SUBSCRIBE, b'engine.2.stdout') sub . connect ( iopub url ) while True : try : idents , msg = session . recv ( sub , mode = 0 ) except Keyboard Interrupt : return # ident always length 1 here topic = idents [ 0 ] if msg [ 'msg type' ] == 'stream' : # stdout/stderr # stream names are in msg['content']['name'], if you want to handle # them differently print ( "%s: %s" % ( topic , msg [ 'content' ] [ 'data' ] ) ) elif msg [ 'msg type' ] == 'pyerr' : # Python traceback c = msg [ 'content' ] print ( topic + ':' ) for line in c [ 'traceback' ] : # indent lines print ( '    ' + line )
def log level changed ( self , name , old , new ) : if isinstance ( new , basestring ) : new = getattr ( logging , new ) self . log level = new self . log . set Level ( new )
def flags changed ( self , name , old , new ) : for key , value in new . iteritems ( ) : assert len ( value ) == 2 , "Bad flag: %r:%s" % ( key , value ) assert isinstance ( value [ 0 ] , ( dict , Config ) ) , "Bad flag: %r:%s" % ( key , value ) assert isinstance ( value [ 1 ] , basestring ) , "Bad flag: %r:%s" % ( key , value )
def print alias help ( self ) : if not self . aliases : return lines = [ ] classdict = { } for cls in self . classes : # include all parents (up to, but excluding Configurable) in available names for c in cls . mro ( ) [ : - 3 ] : classdict [ c . name ] = c for alias , longname in self . aliases . iteritems ( ) : classname , traitname = longname . split ( '.' , 1 ) cls = classdict [ classname ] trait = cls . class traits ( config = True ) [ traitname ] help = cls . class get trait help ( trait ) . splitlines ( ) # reformat first line help [ 0 ] = help [ 0 ] . replace ( longname , alias ) + ' (%s)' % longname if len ( alias ) == 1 : help [ 0 ] = help [ 0 ] . replace ( '--%s=' % alias , '-%s ' % alias ) lines . extend ( help ) # lines.append('') print os . linesep . join ( lines )
def print flag help ( self ) : if not self . flags : return lines = [ ] for m , ( cfg , help ) in self . flags . iteritems ( ) : prefix = '--' if len ( m ) > 1 else '-' lines . append ( prefix + m ) lines . append ( indent ( dedent ( help . strip ( ) ) ) ) # lines.append('') print os . linesep . join ( lines )
def print subcommands ( self ) : if not self . subcommands : return lines = [ "Subcommands" ] lines . append ( '-' * len ( lines [ 0 ] ) ) lines . append ( '' ) for p in wrap paragraphs ( self . subcommand description ) : lines . append ( p ) lines . append ( '' ) for subc , ( cls , help ) in self . subcommands . iteritems ( ) : lines . append ( subc ) if help : lines . append ( indent ( dedent ( help . strip ( ) ) ) ) lines . append ( '' ) print os . linesep . join ( lines )
def update config ( self , config ) : # Save a copy of the current config. newconfig = deepcopy ( self . config ) # Merge the new config into the current one. newconfig . merge ( config ) # Save the combined config as self.config, which triggers the traits # events. self . config = newconfig
def initialize subcommand ( self , subc , argv = None ) : subapp , help = self . subcommands . get ( subc ) if isinstance ( subapp , basestring ) : subapp = import item ( subapp ) # clear existing instances self . class . clear instance ( ) # instantiate self . subapp = subapp . instance ( ) # and initialize subapp self . subapp . initialize ( argv )
def parse command line ( self , argv = None ) : argv = sys . argv [ 1 : ] if argv is None else argv if argv and argv [ 0 ] == 'help' : # turn `ipython help notebook` into `ipython notebook -h` argv = argv [ 1 : ] + [ '-h' ] if self . subcommands and len ( argv ) > 0 : # we have subcommands, and one may have been specified subc , subargv = argv [ 0 ] , argv [ 1 : ] if re . match ( r'^\w(\-?\w)*$' , subc ) and subc in self . subcommands : # it's a subcommand, and *not* a flag or class parameter return self . initialize subcommand ( subc , subargv ) if '-h' in argv or '--help' in argv or '--help-all' in argv : self . print description ( ) self . print help ( '--help-all' in argv ) self . print examples ( ) self . exit ( 0 ) if '--version' in argv or '-V' in argv : self . print version ( ) self . exit ( 0 ) # flatten flags&aliases, so cl-args get appropriate priority: flags , aliases = self . flatten flags ( ) loader = KV Arg Parse Config Loader ( argv = argv , aliases = aliases , flags = flags ) config = loader . load config ( ) self . update config ( config ) # store unparsed args in extra args self . extra args = loader . extra args
def load config file ( self , filename , path = None ) : loader = Py File Config Loader ( filename , path = path ) try : config = loader . load config ( ) except Config File Not Found : # problem finding the file, raise raise except Exception : # try to get the full filename, but it will be empty in the # unlikely event that the error raised before filefind finished filename = loader . full filename or filename # problem while running the file self . log . error ( "Exception while loading config file %s" , filename , exc info = True ) else : self . log . debug ( "Loaded config file: %s" , loader . full filename ) self . update config ( config )
def generate config file ( self ) : lines = [ "# Configuration file for %s." % self . name ] lines . append ( '' ) lines . append ( 'c = get config()' ) lines . append ( '' ) for cls in self . classes : lines . append ( cls . class config section ( ) ) return '\n' . join ( lines )
def downsample ( array , k ) : length = array . shape [ 0 ] indices = random . sample ( xrange ( length ) , k ) return array [ indices ]
def write ( self , msg ) : if self . should ( 'pid' ) : msg = "pid %5d: %s" % ( os . getpid ( ) , msg ) self . output . write ( msg + "\n" ) self . output . flush ( )
def class config section ( cls ) : def c ( s ) : """return a commented, wrapped block.""" s = '\n\n' . join ( wrap paragraphs ( s , 78 ) ) return '# ' + s . replace ( '\n' , '\n# ' ) # section header breaker = '#' + '-' * 78 s = "# %s configuration" % cls . name lines = [ breaker , s , breaker , '' ] # get the description trait desc = cls . class traits ( ) . get ( 'description' ) if desc : desc = desc . default value else : # no description trait, use  doc desc = getattr ( cls , ' doc ' , '' ) if desc : lines . append ( c ( desc ) ) lines . append ( '' ) parents = [ ] for parent in cls . mro ( ) : # only include parents that are not base classes # and are not the class itself # and have some configurable traits to inherit if parent is not cls and issubclass ( parent , Configurable ) and parent . class traits ( config = True ) : parents . append ( parent ) if parents : pstr = ', ' . join ( [ p . name for p in parents ] ) lines . append ( c ( '%s will inherit config from: %s' % ( cls . name , pstr ) ) ) lines . append ( '' ) for name , trait in cls . class traits ( config = True ) . iteritems ( ) : help = trait . get metadata ( 'help' ) or '' lines . append ( c ( help ) ) lines . append ( '# c.%s.%s = %r' % ( cls . name , name , trait . get default value ( ) ) ) lines . append ( '' ) return '\n' . join ( lines )
def clear instance ( cls ) : if not cls . initialized ( ) : return for subclass in cls . walk mro ( ) : if isinstance ( subclass . instance , cls ) : # only clear instances that are instances # of the calling class subclass . instance = None
def format Failure ( self , test , err ) : ec , ev , tb = err tbinfo = inspect traceback ( tb ) test . tbinfo = tbinfo return ( ec , '\n' . join ( [ str ( ev ) , tbinfo ] ) , tb )
def crash handler lite ( etype , evalue , tb ) : traceback . print exception ( etype , evalue , tb ) from I Python . core . interactiveshell import Interactive Shell if Interactive Shell . initialized ( ) : # we are in a Shell environment, give %magic example config = "%config " else : # we are not in a shell, show generic config config = "c." print >> sys . stderr , lite message template . format ( email = author email , config = config )
def make report ( self , traceback ) : sec sep = self . section sep report = [ '*' * 75 + '\n\n' + 'I Python post-mortem report\n\n' ] rpt add = report . append rpt add ( sys info ( ) ) try : config = pformat ( self . app . config ) rpt add ( sec sep ) rpt add ( 'Application name: %s\n\n' % self . app name ) rpt add ( 'Current user configuration structure:\n\n' ) rpt add ( config ) except : pass rpt add ( sec sep + 'Crash traceback:\n\n' + traceback ) return '' . join ( report )
def call handlers ( self , msg ) : # Emit the generic signal. self . message received . emit ( msg ) # Emit signals for specialized message types. msg type = msg [ 'header' ] [ 'msg type' ] signal = getattr ( self , msg type , None ) if signal : signal . emit ( msg ) if not self . handlers called : self . first reply . emit ( ) self . handlers called = True
def call handlers ( self , msg ) : # Emit the generic signal. self . message received . emit ( msg ) # Emit signals for specialized message types. msg type = msg [ 'header' ] [ 'msg type' ] signal = getattr ( self , msg type + ' received' , None ) if signal : signal . emit ( msg ) elif msg type in ( 'stdout' , 'stderr' ) : self . stream received . emit ( msg )
def flush ( self ) : super ( Qt Sub Socket Channel , self ) . flush ( ) Qt Core . Q Core Application . instance ( ) . process Events ( )
def call handlers ( self , msg ) : # Emit the generic signal. self . message received . emit ( msg ) # Emit signals for specialized message types. msg type = msg [ 'header' ] [ 'msg type' ] if msg type == 'input request' : self . input requested . emit ( msg )
def start kernel ( self , * args , * * kw ) : if self . shell channel is not None : self . shell channel . reset first reply ( ) super ( Qt Kernel Manager , self ) . start kernel ( * args , * * kw ) self . started kernel . emit ( )
def start channels ( self , * args , * * kw ) : super ( Qt Kernel Manager , self ) . start channels ( * args , * * kw ) self . started channels . emit ( )
def shell channel ( self ) : if self . shell channel is None : self . shell channel = super ( Qt Kernel Manager , self ) . shell channel self . shell channel . first reply . connect ( self . first reply ) return self . shell channel
def read ( self , fp , * * kwargs ) : nbs = fp . read ( ) if not py3compat . PY3 and not isinstance ( nbs , unicode ) : nbs = py3compat . str to unicode ( nbs ) return self . reads ( nbs , * * kwargs )
def write ( self , nb , fp , * * kwargs ) : nbs = self . writes ( nb , * * kwargs ) if not py3compat . PY3 and not isinstance ( nbs , unicode ) : # this branch is likely only taken for JSON on Python 2 nbs = py3compat . str to unicode ( nbs ) return fp . write ( nbs )
def method magic marker ( magic kind ) : validate type ( magic kind ) # This is a closure to capture the magic kind.  We could also use a class, # but it's overkill for just that one bit of state. def magic deco ( arg ) : call = lambda f , * a , * * k : f ( * a , * * k ) if callable ( arg ) : # "Naked" decorator call (just @foo, no args) func = arg name = func . func name retval = decorator ( call , func ) record magic ( magics , magic kind , name , name ) elif isinstance ( arg , basestring ) : # Decorator called with arguments (@foo('bar')) name = arg def mark ( func , * a , * * kw ) : record magic ( magics , magic kind , name , func . func name ) return decorator ( call , func ) retval = mark else : raise Type Error ( "Decorator can only be called with " "string or function" ) return retval # Ensure the resulting decorator has a usable docstring magic deco . doc = docstring template . format ( 'method' , magic kind ) return magic deco
def function magic marker ( magic kind ) : validate type ( magic kind ) # This is a closure to capture the magic kind.  We could also use a class, # but it's overkill for just that one bit of state. def magic deco ( arg ) : call = lambda f , * a , * * k : f ( * a , * * k ) # Find get ipython() in the caller's namespace caller = sys . getframe ( 1 ) for ns in [ 'f locals' , 'f globals' , 'f builtins' ] : get ipython = getattr ( caller , ns ) . get ( 'get ipython' ) if get ipython is not None : break else : raise Name Error ( 'Decorator can only run in context where ' '`get ipython` exists' ) ip = get ipython ( ) if callable ( arg ) : # "Naked" decorator call (just @foo, no args) func = arg name = func . func name ip . register magic function ( func , magic kind , name ) retval = decorator ( call , func ) elif isinstance ( arg , basestring ) : # Decorator called with arguments (@foo('bar')) name = arg def mark ( func , * a , * * kw ) : ip . register magic function ( func , magic kind , name ) return decorator ( call , func ) retval = mark else : raise Type Error ( "Decorator can only be called with " "string or function" ) return retval # Ensure the resulting decorator has a usable docstring ds = docstring template . format ( 'function' , magic kind ) ds += dedent ( ) magic deco . doc = ds return magic deco
def format latex ( self , strng ) : # Characters that need to be escaped for latex: escape re = re . compile ( r'(%| |\$|#|&)' , re . MULTILINE ) # Magic command names as headers: cmd name re = re . compile ( r'^(%s.*?):' % ESC MAGIC , re . MULTILINE ) # Magic commands cmd re = re . compile ( r'(?P<cmd>%s.+?\b)(?!\}\}:)' % ESC MAGIC , re . MULTILINE ) # Paragraph continue par re = re . compile ( r'\\$' , re . MULTILINE ) # The "\n" symbol newline re = re . compile ( r'\\n' ) # Now build the string for output: #strng = cmd name re.sub(r'\n\\texttt{\\textsl{\\large \1}}:',strng) strng = cmd name re . sub ( r'\n\\bigskip\n\\texttt{\\textbf{ \1}}:' , strng ) strng = cmd re . sub ( r'\\texttt{\g<cmd>}' , strng ) strng = par re . sub ( r'\\\\' , strng ) strng = escape re . sub ( r'\\\1' , strng ) strng = newline re . sub ( r'\\textbackslash{}n' , strng ) return strng
def default option ( self , fn , optstr ) : if fn not in self . lsmagic ( ) : error ( "%s is not a magic function" % fn ) self . options table [ fn ] = optstr
def page guiref ( arg s = None ) : from I Python . core import page page . page ( gui reference , auto html = True )
def task with callable ( the callable , label = None , schedule = DEFAULT SCHEDULE , userdata = None , pk override = None ) : task = Task ( ) if isinstance ( the callable , str ) : if pk override is not None : components = the callable . split ( '.' ) info = dict ( func type = 'instancemethod' , module name = '.' . join ( components [ : - 2 ] ) , class name = components [ - 2 ] , class path = '.' . join ( components [ : - 1 ] ) , model pk = pk override , func name = components [ - 1 ] , func path = the callable , ) task . funcinfo = info else : task . funcinfo = get func info ( func from string ( the callable ) ) else : task . funcinfo = get func info ( the callable ) if label is None : task . label = task . funcinfo [ 'func path' ] else : task . label = label task . schedule = schedule if not croniter . is valid ( task . schedule ) : raise Value Error ( f"Cron schedule {task.schedule} is not valid" ) if userdata is None : task . userdata = dict ( ) else : if isinstance ( userdata , dict ) : task . userdata = userdata else : raise Value Error ( "Userdata must be a dictionary of JSON-serializable data" ) return task
def func from info ( self ) : info = self . funcinfo functype = info [ 'func type' ] if functype in [ 'instancemethod' , 'classmethod' , 'staticmethod' ] : the modelclass = get module member by dottedpath ( info [ 'class path' ] ) if functype == 'instancemethod' : the modelobject = the modelclass . objects . get ( pk = info [ 'model pk' ] ) the callable = get member ( the modelobject , info [ 'func name' ] ) else : the callable = get member ( the modelclass , info [ 'func name' ] ) return the callable elif functype == 'function' : mod = import module ( info [ 'module name' ] ) the callable = get member ( mod , info [ 'func name' ] ) return the callable else : raise Value Error ( f"Unknown functype '{functype} in task {self.pk} ({self.label})" )
def calc next run ( self ) : base time = self . last run if self . last run == HAS NOT RUN : if self . wait for schedule is False : self . next run = timezone . now ( ) self . wait for schedule = False # reset so we don't run on every clock tick self . save ( ) return else : base time = timezone . now ( ) self . next run = croniter ( self . schedule , base time ) . get next ( datetime ) self . save ( )
def run ( self , message ) : the callable = self . func from info ( ) try : task message = dict ( task = self , channel message = message , ) the callable ( task message ) finally : if self . end running < self . next run : self . enabled = False Channel ( KILL TASK CHANNEL ) . send ( { 'id' : self . pk } ) return if self . iterations == 0 : return else : self . iterations -= 1 if self . iterations == 0 : self . enabled = False Channel ( KILL TASK CHANNEL ) . send ( { 'id' : self . pk } ) self . save ( )
def run asap ( self ) : now = timezone . now ( ) self . last run = now self . calc next run ( ) self . save ( ) self . submit ( now )
def run iterations ( cls , the callable , iterations = 1 , label = None , schedule = '* * * * * *' , userdata = None , run immediately = False , delay until = None ) : task = task with callable ( the callable , label = label , schedule = schedule , userdata = userdata ) task . iterations = iterations if delay until is not None : if isinstance ( delay until , datetime ) : if delay until > timezone . now ( ) : task . start running = delay until else : raise Value Error ( "Task cannot start running in the past" ) else : raise Value Error ( "delay until must be a datetime.datetime instance" ) if run immediately : task . next run = timezone . now ( ) else : task . calc next run ( ) task . save ( )
def run once ( cls , the callable , userdata = None , delay until = None ) : cls . run iterations ( the callable , userdata = userdata , run immediately = True , delay until = delay until )
def bind kernel ( self , * * kwargs ) : if self . kernel app is not None : return self . log . info ( "Opening ports for direct connections as an I Python kernel" ) kernel = self . kernel kwargs . setdefault ( 'config' , self . config ) kwargs . setdefault ( 'log' , self . log ) kwargs . setdefault ( 'profile dir' , self . profile dir ) kwargs . setdefault ( 'session' , self . engine . session ) app = self . kernel app = IP Kernel App ( * * kwargs ) # allow IP Kernel App.instance(): IP Kernel App . instance = app app . init connection file ( ) # relevant contents of init sockets: app . shell port = app . bind socket ( kernel . shell streams [ 0 ] , app . shell port ) app . log . debug ( "shell ROUTER Channel on port: %i" , app . shell port ) app . iopub port = app . bind socket ( kernel . iopub socket , app . iopub port ) app . log . debug ( "iopub PUB Channel on port: %i" , app . iopub port ) kernel . stdin socket = self . engine . context . socket ( zmq . ROUTER ) app . stdin port = app . bind socket ( kernel . stdin socket , app . stdin port ) app . log . debug ( "stdin ROUTER Channel on port: %i" , app . stdin port ) # start the heartbeat, and log connection info: app . init heartbeat ( ) app . log connection info ( ) app . write connection file ( )
def pid exists ( pid ) : if not isinstance ( pid , int ) : raise Type Error ( 'an integer is required' ) if pid < 0 : return False try : os . kill ( pid , 0 ) except OS Error : e = sys . exc info ( ) [ 1 ] return e . errno == errno . EPERM else : return True
def get disk usage ( path ) : st = os . statvfs ( path ) free = ( st . f bavail * st . f frsize ) total = ( st . f blocks * st . f frsize ) used = ( st . f blocks - st . f bfree ) * st . f frsize percent = usage percent ( used , total , round = 1 ) # NB: the percentage is -5% than what shown by df due to # reserved blocks that we are currently not considering: # http://goo.gl/s W Gb H return nt diskinfo ( total , used , free , percent )
def run ( self ) : try : from winapi import WAIT OBJECT 0 , INFINITE except Import Error : from subprocess import WAIT OBJECT 0 , INFINITE # Build the list of handle to listen on. handles = [ ] if self . interrupt handle : handles . append ( self . interrupt handle ) if self . parent handle : handles . append ( self . parent handle ) arch = platform . architecture ( ) [ 0 ] c int = ctypes . c int64 if arch . startswith ( '64' ) else ctypes . c int # Listen forever. while True : result = ctypes . windll . kernel32 . Wait For Multiple Objects ( len ( handles ) , # n Count ( c int * len ( handles ) ) ( * handles ) , # lp Handles False , # b Wait All INFINITE ) # dw Milliseconds if WAIT OBJECT 0 <= result < len ( handles ) : handle = handles [ result - WAIT OBJECT 0 ] if handle == self . interrupt handle : interrupt main ( ) elif handle == self . parent handle : os . exit ( 1 ) elif result < 0 : # wait failed, just give up and stop polling. warn ( ) return
def filter ns ( ns , name pattern = "*" , type pattern = "all" , ignore case = True , show all = True ) : pattern = name pattern . replace ( "*" , ".*" ) . replace ( "?" , "." ) if ignore case : reg = re . compile ( pattern + "$" , re . I ) else : reg = re . compile ( pattern + "$" ) # Check each one matches regex; shouldn't be hidden; of correct type. return dict ( ( key , obj ) for key , obj in ns . iteritems ( ) if reg . match ( key ) and show hidden ( key , show all ) and is type ( obj , type pattern ) )
def draw if interactive ( ) : # signal that the current active figure should be sent at the end of # execution.  Also sets the  draw called flag, signaling that there will be # something to send.  At the end of the code execution, a separate call to # flush figures() will act upon these values fig = Gcf . get active ( ) . canvas . figure # Hack: matplotlib Figure Manager objects in interacive backends (at least # in some of them) monkeypatch the figure object and add a .show() method # to it.  This applies the same monkeypatch in order to support user code # that might expect `.show()` to be part of the official API of figure # objects. # For further reference: if not hasattr ( fig , 'show' ) : # Queue up `fig` for display fig . show = lambda * a : send figure ( fig ) # If matplotlib was manually set to non-interactive mode, this function # should be a no-op (otherwise we'll generate duplicate plots, since a user # who set ioff() manually expects to make separate draw/show calls). if not matplotlib . is interactive ( ) : return # ensure current figure will be drawn, and each subsequent call # of draw if interactive() moves the active figure to ensure it is # drawn last try : show . to draw . remove ( fig ) except Value Error : # ensure it only appears in the draw list once pass # Queue up the figure for drawing in next show() call show . to draw . append ( fig ) show . draw called = True
def send figure ( fig ) : fmt = Inline Backend . instance ( ) . figure format data = print figure ( fig , fmt ) # print figure will return None if there's nothing to draw: if data is None : return mimetypes = { 'png' : 'image/png' , 'svg' : 'image/svg+xml' } mime = mimetypes [ fmt ] # flush text streams before sending figures, helps a little with output # synchronization in the console (though it's a bandaid, not a real sln) sys . stdout . flush ( ) sys . stderr . flush ( ) publish display data ( 'I Python.zmq.pylab.backend inline.send figure' , { mime : data } )
def handle sigint ( self , sig , frame ) : # register more forceful signal handler for ^C^C case signal . signal ( signal . SIGINT , self . signal stop ) # request confirmation dialog in bg thread, to avoid # blocking the App thread = threading . Thread ( target = self . confirm exit ) thread . daemon = True thread . start ( )
def render ( self , name , color = True , * * kwargs ) : if name == 'rewrite' : return self . render rewrite ( color = color ) if color : scheme = self . color scheme table . active colors if name == 'out' : colors = color lists [ 'normal' ] colors . number , colors . prompt , colors . normal = scheme . out number , scheme . out prompt , scheme . normal else : colors = color lists [ 'inp' ] colors . number , colors . prompt , colors . normal = scheme . in number , scheme . in prompt , scheme . in normal if name == 'in2' : colors . prompt = scheme . in prompt2 else : # No color colors = color lists [ 'nocolor' ] colors . number , colors . prompt , colors . normal = '' , '' , '' count = self . shell . execution count # Shorthand # Build the dictionary to be passed to string formatting fmtargs = dict ( color = colors , count = count , dots = "." * len ( str ( count ) ) , width = self . width , txtwidth = self . txtwidth ) fmtargs . update ( self . lazy evaluate fields ) fmtargs . update ( kwargs ) # Prepare the prompt prompt = colors . prompt + self . templates [ name ] + colors . normal # Fill in required fields return self . formatter . format ( prompt , * * fmtargs )
def mappable ( obj ) : if isinstance ( obj , ( tuple , list ) ) : return True for m in array Modules : if isinstance ( obj , m [ 'type' ] ) : return True return False
def get Partition ( self , seq , p , q ) : # Test for error conditions here if p < 0 or p >= q : print "No partition exists." return remainder = len ( seq ) % q basesize = len ( seq ) // q hi = [ ] lo = [ ] for n in range ( q ) : if n < remainder : lo . append ( n * ( basesize + 1 ) ) hi . append ( lo [ - 1 ] + basesize + 1 ) else : lo . append ( n * basesize + remainder ) hi . append ( lo [ - 1 ] + basesize ) try : result = seq [ lo [ p ] : hi [ p ] ] except Type Error : # some objects (iterators) can't be sliced, # use islice: result = list ( islice ( seq , lo [ p ] , hi [ p ] ) ) return result
def main ( ) : parser = optparse . Option Parser ( usage = MAIN USAGE ) newopt = parser . add option newopt ( '--ipython' , action = 'store const' , dest = 'mode' , const = 'ipython' , help = 'I Python interactive runner (default).' ) newopt ( '--python' , action = 'store const' , dest = 'mode' , const = 'python' , help = 'Python interactive runner.' ) newopt ( '--sage' , action = 'store const' , dest = 'mode' , const = 'sage' , help = 'SAGE interactive runner.' ) opts , args = parser . parse args ( ) runners = dict ( ipython = I Python Runner , python = Python Runner , sage = SAGE Runner ) try : ext = os . path . splitext ( args [ 0 ] ) [ - 1 ] except Index Error : ext = '' modes = { '.ipy' : 'ipython' , '.py' : 'python' , '.sage' : 'sage' } mode = modes . get ( ext , "ipython" ) if opts . mode : mode = opts . mode runners [ mode ] ( ) . main ( args )
def main ( self , argv = None ) : parser = optparse . Option Parser ( usage = USAGE % self . class . name ) newopt = parser . add option newopt ( '-i' , '--interact' , action = 'store true' , default = False , help = 'Interact with the program after the script is run.' ) opts , args = parser . parse args ( argv ) if len ( args ) != 1 : print >> sys . stderr , "You must supply exactly one file to run." sys . exit ( 1 ) self . run file ( args [ 0 ] , opts . interact )
def xml file ( self , cu , analysis ) : # Create the 'lines' and 'package' XML elements, which # are populated later.  Note that a package == a directory. package name = rpartition ( cu . name , "." ) [ 0 ] class Name = cu . name package = self . packages . setdefault ( package name , [ { } , 0 , 0 , 0 , 0 ] ) xclass = self . xml out . create Element ( "class" ) xclass . append Child ( self . xml out . create Element ( "methods" ) ) xlines = self . xml out . create Element ( "lines" ) xclass . append Child ( xlines ) xclass . set Attribute ( "name" , class Name ) filename = cu . file locator . relative filename ( cu . filename ) xclass . set Attribute ( "filename" , filename . replace ( "\\" , "/" ) ) xclass . set Attribute ( "complexity" , "0" ) branch stats = analysis . branch stats ( ) # For each statement, create an XML 'line' element. for line in sorted ( analysis . statements ) : xline = self . xml out . create Element ( "line" ) xline . set Attribute ( "number" , str ( line ) ) # Q: can we get info about the number of times a statement is # executed?  If so, that should be recorded here. xline . set Attribute ( "hits" , str ( int ( line not in analysis . missing ) ) ) if self . arcs : if line in branch stats : total , taken = branch stats [ line ] xline . set Attribute ( "branch" , "true" ) xline . set Attribute ( "condition-coverage" , "%d%% (%d/%d)" % ( 100 * taken / total , taken , total ) ) xlines . append Child ( xline ) class lines = len ( analysis . statements ) class hits = class lines - len ( analysis . missing ) if self . arcs : class branches = sum ( [ t for t , k in branch stats . values ( ) ] ) missing branches = sum ( [ t - k for t , k in branch stats . values ( ) ] ) class br hits = class branches - missing branches else : class branches = 0.0 class br hits = 0.0 # Finalize the statistics that are collected in the XML DOM. xclass . set Attribute ( "line-rate" , rate ( class hits , class lines ) ) xclass . set Attribute ( "branch-rate" , rate ( class br hits , class branches ) ) package [ 0 ] [ class Name ] = xclass package [ 1 ] += class hits package [ 2 ] += class lines package [ 3 ] += class br hits package [ 4 ] += class branches
def reduce freqs ( freqlist ) : allfreqs = np . zeros like ( freqlist [ 0 ] ) for f in freqlist : allfreqs += f return allfreqs
def compute n digit freqs ( filename , n ) : d = txt file to digits ( filename ) freqs = n digit freqs ( d , n ) return freqs
def txt file to digits ( filename , the type = str ) : with open ( filename , 'r' ) as f : for line in f . readlines ( ) : for c in line : if c != '\n' and c != ' ' : yield the type ( c )
def one digit freqs ( digits , normalize = False ) : freqs = np . zeros ( 10 , dtype = 'i4' ) for d in digits : freqs [ int ( d ) ] += 1 if normalize : freqs = freqs / freqs . sum ( ) return freqs
def two digit freqs ( digits , normalize = False ) : freqs = np . zeros ( 100 , dtype = 'i4' ) last = digits . next ( ) this = digits . next ( ) for d in digits : index = int ( last + this ) freqs [ index ] += 1 last = this this = d if normalize : freqs = freqs / freqs . sum ( ) return freqs
def plot two digit freqs ( f2 ) : f2 copy = f2 . copy ( ) f2 copy . shape = ( 10 , 10 ) ax = plt . matshow ( f2 copy ) plt . colorbar ( ) for i in range ( 10 ) : for j in range ( 10 ) : plt . text ( i - 0.2 , j + 0.2 , str ( j ) + str ( i ) ) plt . ylabel ( 'First digit' ) plt . xlabel ( 'Second digit' ) return ax
def plot one digit freqs ( f1 ) : ax = plt . plot ( f1 , 'bo-' ) plt . title ( 'Single digit counts in pi' ) plt . xlabel ( 'Digit' ) plt . ylabel ( 'Count' ) return ax
def debug src ( src , pm = False , globs = None ) : testsrc = script from examples ( src ) debug script ( testsrc , pm , globs )
def debug script ( src , pm = False , globs = None ) : import pdb # Note that tempfile.Name Temporary File() cannot be used.  As the # docs say, a file so created cannot be opened by name a second time # on modern Windows boxes, and execfile() needs to open it. srcfilename = tempfile . mktemp ( ".py" , "doctestdebug" ) f = open ( srcfilename , 'w' ) f . write ( src ) f . close ( ) try : if globs : globs = globs . copy ( ) else : globs = { } if pm : try : execfile ( srcfilename , globs , globs ) except : print sys . exc info ( ) [ 1 ] pdb . post mortem ( sys . exc info ( ) [ 2 ] ) else : # Note that %r is vital here.  '%s' instead can, e.g., cause # backslashes to get treated as metacharacters on Windows. pdb . run ( "execfile(%r)" % srcfilename , globs , globs ) finally : os . remove ( srcfilename )
def hdict ( self , hashroot ) : hfiles = self . keys ( hashroot + "/*" ) hfiles . sort ( ) last = len ( hfiles ) and hfiles [ - 1 ] or '' if last . endswith ( 'xx' ) : # print "using xx" hfiles = [ last ] + hfiles [ : - 1 ] all = { } for f in hfiles : # print "using",f try : all . update ( self [ f ] ) except Key Error : print "Corrupt" , f , "deleted - hset is not threadsafe!" del self [ f ] self . uncache ( f ) return all
def keys ( self , globpat = None ) : if globpat is None : files = self . root . walkfiles ( ) else : files = [ Path ( p ) for p in glob . glob ( self . root / globpat ) ] return [ self . normalized ( p ) for p in files if p . isfile ( ) ]
def allow ( self , record ) : if not self : # nothing to filter return True return self . allow ( record ) and not self . deny ( record )
def options ( self , parser , env ) : parser . add option ( "--nologcapture" , action = "store false" , default = not env . get ( self . env opt ) , dest = "logcapture" , help = "Disable logging capture plugin. " "Logging configurtion will be left intact." " [NOSE NOLOGCAPTURE]" ) parser . add option ( "--logging-format" , action = "store" , dest = "logcapture format" , default = env . get ( 'NOSE LOGFORMAT' ) or self . logformat , metavar = "FORMAT" , help = "Specify custom format to print statements. " "Uses the same format as used by standard logging handlers." " [NOSE LOGFORMAT]" ) parser . add option ( "--logging-datefmt" , action = "store" , dest = "logcapture datefmt" , default = env . get ( 'NOSE LOGDATEFMT' ) or self . logdatefmt , metavar = "FORMAT" , help = "Specify custom date/time format to print statements. " "Uses the same format as used by standard logging handlers." " [NOSE LOGDATEFMT]" ) parser . add option ( "--logging-filter" , action = "store" , dest = "logcapture filters" , default = env . get ( 'NOSE LOGFILTER' ) , metavar = "FILTER" , help = "Specify which statements to filter in/out. " "By default, everything is captured. If the output is too" " verbose,\nuse this option to filter out needless output.\n" "Example: filter=foo will capture statements issued ONLY to\n" " foo or foo.what.ever.sub but not foobar or other logger.\n" "Specify multiple loggers with comma: filter=foo,bar,baz.\n" "If any logger name is prefixed with a minus, eg filter=-foo,\n" "it will be excluded rather than included. Default: " "exclude logging messages from nose itself (-nose)." " [NOSE LOGFILTER]\n" ) parser . add option ( "--logging-clear-handlers" , action = "store true" , default = False , dest = "logcapture clear" , help = "Clear all other logging handlers" ) parser . add option ( "--logging-level" , action = "store" , default = 'NOTSET' , dest = "logcapture level" , help = "Set the log level to capture" )
def format Error ( self , test , err ) : # logic flow copied from Capture.format Error test . captured Logging = records = self . format Log Records ( ) if not records : return err ec , ev , tb = err return ( ec , self . add Capture To Err ( ev , records ) , tb )
def get new csv writers ( trans title , meta title , trans csv path , meta csv path ) : trans writer = Unicode Writer ( trans csv path ) trans writer . writerow ( trans title ) meta writer = Unicode Writer ( meta csv path ) meta writer . writerow ( meta title ) return trans writer , meta writer
def subscribe user ( self , user ) : url = self . root url + "subscribe user" values = { } values [ "username" ] = user return self . query ( url , values )
def init parser ( ) : usage = parser = Option Parser ( usage , version = "%prog " + notifo . version ) parser . add option ( "-u" , "--user" , action = "store" , dest = "user" , help = "your notifo username" ) parser . add option ( "-s" , "--secret" , action = "store" , dest = "secret" , help = "your notifo API secret" ) parser . add option ( "-n" , "--name" , action = "store" , dest = "name" , help = "recipient for the notification" ) parser . add option ( "-l" , "--label" , action = "store" , dest = "label" , help = "label for the notification" ) parser . add option ( "-t" , "--title" , action = "store" , dest = "title" , help = "title of the notification" ) parser . add option ( "-c" , "--callback" , action = "store" , dest = "callback" , help = "callback URL to call" ) parser . add option ( "-m" , "--message" , action = "store true" , dest = "message" , default = False , help = "send message instead of notification" ) ( options , args ) = parser . parse args ( ) return ( parser , options , args )
def make code from py ( filename ) : # Open the source file. try : source file = open source ( filename ) except IO Error : raise No Source ( "No file to run: %r" % filename ) try : source = source file . read ( ) finally : source file . close ( ) # We have the source.  `compile` still needs the last line to be clean, # so make sure it is, then compile a code object from it. if not source or source [ - 1 ] != '\n' : source += '\n' code = compile ( source , filename , "exec" ) return code
def make code from pyc ( filename ) : try : fpyc = open ( filename , "rb" ) except IO Error : raise No Code ( "No file to run: %r" % filename ) try : # First four bytes are a version-specific magic number.  It has to # match or we won't run the file. magic = fpyc . read ( 4 ) if magic != imp . get magic ( ) : raise No Code ( "Bad magic number in .pyc file" ) # Skip the junk in the header that we don't need. fpyc . read ( 4 ) # Skip the moddate. if sys . version info >= ( 3 , 3 ) : # 3.3 added another long to the header (size), skip it. fpyc . read ( 4 ) # The rest of the file is the code object we want. code = marshal . load ( fpyc ) finally : fpyc . close ( ) return code
def html tableify ( item matrix , select = None , header = None , footer = None ) : if not item matrix : return '' html cols = [ ] tds = lambda text : u'<td>' + text + u'  </td>' trs = lambda text : u'<tr>' + text + u'</tr>' tds items = [ map ( tds , row ) for row in item matrix ] if select : row , col = select tds items [ row ] [ col ] = u'<td class="inverted">' + item matrix [ row ] [ col ] + u'  </td>' #select the right item html cols = map ( trs , ( u'' . join ( row ) for row in tds items ) ) head = '' foot = '' if header : head = ( u'<tr>' + '' . join ( ( u'<td>' + header + u'</td>' ) * len ( item matrix [ 0 ] ) ) + '</tr>' ) if footer : foot = ( u'<tr>' + '' . join ( ( u'<td>' + footer + u'</td>' ) * len ( item matrix [ 0 ] ) ) + '</tr>' ) html = ( u'<table class="completion" style="white-space:pre">' + head + ( u'' . join ( html cols ) ) + foot + u'</table>' ) return html
def current ( self , value ) : current = min ( max ( self . min , value ) , self . max ) self . current = current if current > self . stop : self . stop = current self . start = current - self . width elif current < self . start : self . start = current self . stop = current + self . width if abs ( self . start - self . min ) <= self . sticky lenght : self . start = self . min if abs ( self . stop - self . max ) <= self . sticky lenght : self . stop = self . max
def update list ( self , hilight = True ) : self . sliding interval . current = self . index [ 0 ] head = None foot = None if self . sliding interval . start > 0 : head = '...' if self . sliding interval . stop < self . sliding interval . max : foot = '...' items m = self . justified items [ self . sliding interval . start : self . sliding interval . stop + 1 ] self . console widget . clear temporary buffer ( ) if ( hilight ) : sel = ( self . sliding interval . nth , self . index [ 1 ] ) else : sel = None strng = html tableify ( items m , select = sel , header = head , footer = foot ) self . console widget . fill temporary buffer ( self . old cursor , strng , html = True )
def complete current ( self ) : i = self . index item = self . items [ i [ 0 ] ] [ i [ 1 ] ] item = item . strip ( ) if item : self . current text cursor ( ) . insert Text ( item ) self . cancel completion ( )
def wordfreq ( text , is filename = False ) : if is filename : with open ( text ) as f : text = f . read ( ) freqs = { } for word in text . split ( ) : lword = word . lower ( ) freqs [ lword ] = freqs . get ( lword , 0 ) + 1 return freqs
def print wordfreq ( freqs , n = 10 ) : words , counts = freqs . keys ( ) , freqs . values ( ) items = zip ( counts , words ) items . sort ( reverse = True ) for ( count , word ) in items [ : n ] : print ( word , count )
def tostring ( self ) : root = self . as element ( ) indent ( root ) txt = ET . tostring ( root , encoding = "utf-8" ) # Now remove the tokens used to order the attributes. txt = re . sub ( r' [A-Z] ' , '' , txt ) txt = '<?xml version="1.0" encoding="utf-8"?>\n' + txt return txt
def write ( self , filename ) : txt = self . tostring ( ) with open ( filename , 'w' ) as f : f . write ( txt )
def begin ( self , total : int , name = None , message = None ) : self . total = total message = message or name or "Working..." self . name = name or "Progress Monitor" self . update ( 0 , message )
def task ( self , total : int , name = None , message = None ) : self . begin ( total , name , message ) try : yield self finally : self . done ( )
def subtask ( self , units : int ) : sm = self . submonitor ( units ) try : yield sm finally : if sm . total is None : # begin was never called, so the subtask cannot be closed self . update ( units ) else : sm . done ( )
def update ( self , units : int = 1 , message : str = None ) : if self . total is None : raise Exception ( "Cannot call progressmonitor.update before calling begin" ) self . worked = min ( self . total , self . worked + units ) if message : self . message = message for listener in self . listeners : listener ( self )
def load config ( self ) : self . clear ( ) try : self . find file ( ) except IO Error as e : raise Config File Not Found ( str ( e ) ) self . read file as dict ( ) self . convert to config ( ) return self . config
def read file as dict ( self ) : # This closure is made available in the namespace that is used # to exec the config file.  It allows users to call # load subconfig('myconfig.py') to load config files recursively. # It needs to be a closure because it has references to self.path # and self.config.  The sub-config is loaded with the same path # as the parent, but it uses an empty config which is then merged # with the parents. # If a profile is specified, the config file will be loaded # from that profile def load subconfig ( fname , profile = None ) : # import here to prevent circular imports from I Python . core . profiledir import Profile Dir , Profile Dir Error if profile is not None : try : profile dir = Profile Dir . find profile dir by name ( get ipython dir ( ) , profile , ) except Profile Dir Error : return path = profile dir . location else : path = self . path loader = Py File Config Loader ( fname , path ) try : sub config = loader . load config ( ) except Config File Not Found : # Pass silently if the sub config is not there. This happens # when a user s using a profile, but not the default config. pass else : self . config . merge ( sub config ) # Again, this needs to be a closure and should be used in config # files to get the config being loaded. def get config ( ) : return self . config namespace = dict ( load subconfig = load subconfig , get config = get config ) fs encoding = sys . getfilesystemencoding ( ) or 'ascii' conf filename = self . full filename . encode ( fs encoding ) py3compat . execfile ( conf filename , namespace )
def load flag ( self , cfg ) : if isinstance ( cfg , ( dict , Config ) ) : # don't clobber whole config sections, update # each section from config: for sec , c in cfg . iteritems ( ) : self . config [ sec ] . update ( c ) else : raise Type Error ( "Invalid flag: %r" % cfg )
def decode argv ( self , argv , enc = None ) : uargv = [ ] if enc is None : enc = DEFAULT ENCODING for arg in argv : if not isinstance ( arg , unicode ) : # only decode if not already decoded arg = arg . decode ( enc ) uargv . append ( arg ) return uargv
def interrupt then kill ( self , delay = 2.0 ) : try : self . signal ( SIGINT ) except Exception : self . log . debug ( "interrupt failed" ) pass self . killer = ioloop . Delayed Callback ( lambda : self . signal ( SIGKILL ) , delay * 1000 , self . loop ) self . killer . start ( )
def start ( self , n ) : dlist = [ ] for i in range ( n ) : if i > 0 : time . sleep ( self . delay ) el = self . launcher class ( work dir = self . work dir , config = self . config , log = self . log , profile dir = self . profile dir , cluster id = self . cluster id , ) # Copy the engine args over to each engine launcher. el . engine cmd = copy . deepcopy ( self . engine cmd ) el . engine args = copy . deepcopy ( self . engine args ) el . on stop ( self . notice engine stopped ) d = el . start ( ) self . launchers [ i ] = el dlist . append ( d ) self . notify start ( dlist ) return dlist
def find args ( self ) : return self . mpi cmd + [ '-n' , str ( self . n ) ] + self . mpi args + self . program + self . program args
def start ( self , n ) : self . n = n return super ( MPI Launcher , self ) . start ( )
def start ( self , n ) : self . n = n return super ( MPI Engine Set Launcher , self ) . start ( n )
def send file ( self , local , remote ) : remote = "%s:%s" % ( self . location , remote ) for i in range ( 10 ) : if not os . path . exists ( local ) : self . log . debug ( "waiting for %s" % local ) time . sleep ( 1 ) else : break self . log . info ( "sending %s to %s" , local , remote ) check output ( self . scp cmd + [ local , remote ] )
def fetch file ( self , remote , local ) : full remote = "%s:%s" % ( self . location , remote ) self . log . info ( "fetching %s from %s" , local , full remote ) for i in range ( 10 ) : # wait up to 10s for remote file to exist check = check output ( self . ssh cmd + self . ssh args + [ self . location , 'test -e' , remote , "&& echo 'yes' || echo 'no'" ] ) check = check . strip ( ) if check == 'no' : time . sleep ( 1 ) elif check == 'yes' : break check output ( self . scp cmd + [ full remote , local ] )
def engine count ( self ) : count = 0 for n in self . engines . itervalues ( ) : if isinstance ( n , ( tuple , list ) ) : n , args = n count += n return count
def start ( self , n ) : self . write job file ( n ) args = [ 'submit' , '/jobfile:%s' % self . job file , '/scheduler:%s' % self . scheduler ] self . log . debug ( "Starting Win HPC Job: %s" % ( self . job cmd + ' ' + ' ' . join ( args ) , ) ) output = check output ( [ self . job cmd ] + args , env = os . environ , cwd = self . work dir , stderr = STDOUT ) job id = self . parse job id ( output ) self . notify start ( job id ) return job id
def parse job id ( self , output ) : m = self . job id regexp . search ( output ) if m is not None : job id = m . group ( ) else : raise Launcher Error ( "Job id couldn't be determined: %s" % output ) self . job id = job id self . log . info ( 'Job submitted with job id: %r' , job id ) return job id
def write batch script ( self , n ) : self . n = n # first priority is batch template if set if self . batch template file and not self . batch template : # second priority is batch template file with open ( self . batch template file ) as f : self . batch template = f . read ( ) if not self . batch template : # third (last) priority is default template self . batch template = self . default template # add jobarray or queue lines to user-specified template # note that this is *only* when user did not specify a template. # print self.job array regexp.search(self.batch template) if not self . job array regexp . search ( self . batch template ) : self . log . debug ( "adding job array settings to batch script" ) firstline , rest = self . batch template . split ( '\n' , 1 ) self . batch template = u'\n' . join ( [ firstline , self . job array template , rest ] ) # print self.queue regexp.search(self.batch template) if self . queue and not self . queue regexp . search ( self . batch template ) : self . log . debug ( "adding PBS queue settings to batch script" ) firstline , rest = self . batch template . split ( '\n' , 1 ) self . batch template = u'\n' . join ( [ firstline , self . queue template , rest ] ) script as string = self . formatter . format ( self . batch template , * * self . context ) self . log . debug ( 'Writing batch script: %s' , self . batch file ) with open ( self . batch file , 'w' ) as f : f . write ( script as string ) os . chmod ( self . batch file , stat . S IRUSR | stat . S IWUSR | stat . S IXUSR )
def start ( self , n ) : self . log . debug ( "Starting %s: %r" , self . class . name , self . args ) # Here we save profile dir in the context so they # can be used in the batch script template as {profile dir} self . write batch script ( n ) output = check output ( self . args , env = os . environ ) job id = self . parse job id ( output ) self . notify start ( job id ) return job id
def context menu make ( self , pos ) : format = self . control . cursor For Position ( pos ) . char Format ( ) name = format . string Property ( Qt Gui . Q Text Format . Image Name ) if name : menu = Qt Gui . Q Menu ( ) menu . add Action ( 'Copy Image' , lambda : self . copy image ( name ) ) menu . add Action ( 'Save Image As...' , lambda : self . save image ( name ) ) menu . add Separator ( ) svg = self . name to svg map . get ( name , None ) if svg is not None : menu . add Separator ( ) menu . add Action ( 'Copy SVG' , lambda : svg to clipboard ( svg ) ) menu . add Action ( 'Save SVG As...' , lambda : save svg ( svg , self . control ) ) else : menu = super ( Rich I Python Widget , self ) . context menu make ( pos ) return menu
def handle pyout ( self , msg ) : if not self . hidden and self . is from this session ( msg ) : content = msg [ 'content' ] prompt number = content . get ( 'execution count' , 0 ) data = content [ 'data' ] if data . has key ( 'image/svg+xml' ) : self . pre image append ( msg , prompt number ) self . append svg ( data [ 'image/svg+xml' ] , True ) self . append html ( self . output sep2 , True ) elif data . has key ( 'image/png' ) : self . pre image append ( msg , prompt number ) self . append png ( decodestring ( data [ 'image/png' ] . encode ( 'ascii' ) ) , True ) self . append html ( self . output sep2 , True ) elif data . has key ( 'image/jpeg' ) and self . jpg supported : self . pre image append ( msg , prompt number ) self . append jpg ( decodestring ( data [ 'image/jpeg' ] . encode ( 'ascii' ) ) , True ) self . append html ( self . output sep2 , True ) else : # Default back to the plain text representation. return super ( Rich I Python Widget , self ) . handle pyout ( msg )
def handle display data ( self , msg ) : if not self . hidden and self . is from this session ( msg ) : source = msg [ 'content' ] [ 'source' ] data = msg [ 'content' ] [ 'data' ] metadata = msg [ 'content' ] [ 'metadata' ] # Try to use the svg or html representations. # FIXME: Is this the right ordering of things to try? if data . has key ( 'image/svg+xml' ) : self . log . debug ( "display: %s" , msg . get ( 'content' , '' ) ) svg = data [ 'image/svg+xml' ] self . append svg ( svg , True ) elif data . has key ( 'image/png' ) : self . log . debug ( "display: %s" , msg . get ( 'content' , '' ) ) # PNG data is base64 encoded as it passes over the network # in a JSON structure so we decode it. png = decodestring ( data [ 'image/png' ] . encode ( 'ascii' ) ) self . append png ( png , True ) elif data . has key ( 'image/jpeg' ) and self . jpg supported : self . log . debug ( "display: %s" , msg . get ( 'content' , '' ) ) jpg = decodestring ( data [ 'image/jpeg' ] . encode ( 'ascii' ) ) self . append jpg ( jpg , True ) else : # Default back to the plain text representation. return super ( Rich I Python Widget , self ) . handle display data ( msg )
def append jpg ( self , jpg , before prompt = False ) : self . append custom ( self . insert jpg , jpg , before prompt )
def append png ( self , png , before prompt = False ) : self . append custom ( self . insert png , png , before prompt )
def append svg ( self , svg , before prompt = False ) : self . append custom ( self . insert svg , svg , before prompt )
def copy image ( self , name ) : image = self . get image ( name ) Qt Gui . Q Application . clipboard ( ) . set Image ( image )
def get image ( self , name ) : document = self . control . document ( ) image = document . resource ( Qt Gui . Q Text Document . Image Resource , Qt Core . Q Url ( name ) ) return image
def insert img ( self , cursor , img , fmt ) : try : image = Qt Gui . Q Image ( ) image . load From Data ( img , fmt . upper ( ) ) except Value Error : self . insert plain text ( cursor , 'Received invalid %s data.' % fmt ) else : format = self . add image ( image ) cursor . insert Block ( ) cursor . insert Image ( format ) cursor . insert Block ( )
def insert svg ( self , cursor , svg ) : try : image = svg to image ( svg ) except Value Error : self . insert plain text ( cursor , 'Received invalid SVG data.' ) else : format = self . add image ( image ) self . name to svg map [ format . name ( ) ] = svg cursor . insert Block ( ) cursor . insert Image ( format ) cursor . insert Block ( )
def save image ( self , name , format = 'PNG' ) : dialog = Qt Gui . Q File Dialog ( self . control , 'Save Image' ) dialog . set Accept Mode ( Qt Gui . Q File Dialog . Accept Save ) dialog . set Default Suffix ( format . lower ( ) ) dialog . set Name Filter ( '%s file (*.%s)' % ( format , format . lower ( ) ) ) if dialog . exec ( ) : filename = dialog . selected Files ( ) [ 0 ] image = self . get image ( name ) image . save ( filename , format )
def exit now changed ( self , name , old , new ) : if new : loop = ioloop . IO Loop . instance ( ) loop . add timeout ( time . time ( ) + 0.1 , loop . stop )
def init environment ( self ) : env = os . environ # These two ensure 'ls' produces nice coloring on BSD-derived systems env [ 'TERM' ] = 'xterm-color' env [ 'CLICOLOR' ] = '1' # Since normal pagers don't work at all (over pexpect we don't have # single-key control of the subprocess), try to disable paging in # subprocesses as much as possible. env [ 'PAGER' ] = 'cat' env [ 'GIT PAGER' ] = 'cat' # And install the payload version of page. install payload page ( )
def ask exit ( self ) : self . exit now = True payload = dict ( source = 'I Python.zmq.zmqshell.ZMQ Interactive Shell.ask exit' , exit = True , keepkernel = self . keepkernel on exit , ) self . payload manager . write payload ( payload )
def read ( self , filename ) : kwargs = { } if sys . version info >= ( 3 , 2 ) : kwargs [ 'encoding' ] = "utf-8" return configparser . Raw Config Parser . read ( self , filename , * * kwargs )
def from environment ( self , env var ) : # Timidity: for nose users, read an environment variable.  This is a # cheap hack, since the rest of the command line arguments aren't # recognized, but it solves some users' problems. env = os . environ . get ( env var , '' ) if env : self . timid = ( '--timid' in env )
def from args ( self , * * kwargs ) : for k , v in iitems ( kwargs ) : if v is not None : if k in self . MUST BE LIST and isinstance ( v , string class ) : v = [ v ] setattr ( self , k , v )
def set attr from config option ( self , cp , attr , where , type = '' ) : section , option = where . split ( ":" ) if cp . has option ( section , option ) : method = getattr ( cp , 'get' + type ) setattr ( self , attr , method ( section , option ) )
def delims ( self , delims ) : expr = '[' + '' . join ( '\\' + c for c in delims ) + ']' self . delim re = re . compile ( expr ) self . delims = delims self . delim expr = expr
def split line ( self , line , cursor pos = None ) : l = line if cursor pos is None else line [ : cursor pos ] return self . delim re . split ( l ) [ - 1 ]
def greedy changed ( self , name , old , new ) : if new : self . splitter . delims = GREEDY DELIMS else : self . splitter . delims = DELIMS if self . readline : self . readline . set completer delims ( self . splitter . delims )
def alias matches ( self , text ) : #print 'Completer->alias matches:',text,'lb',self.text until cursor # dbg # if we are not in the first 'item', alias matching # doesn't make sense - unless we are starting with 'sudo' command. main text = self . text until cursor . lstrip ( ) if ' ' in main text and not main text . startswith ( 'sudo' ) : return [ ] text = os . path . expanduser ( text ) aliases = self . alias table . keys ( ) if text == '' : return aliases else : return [ a for a in aliases if a . startswith ( text ) ]
def python matches ( self , text ) : #io.rprint('Completer->python matches, txt=%r' % text) # dbg if "." in text : try : matches = self . attr matches ( text ) if text . endswith ( '.' ) and self . omit names : if self . omit names == 1 : # true if txt is  not  a   name, false otherwise: no name = ( lambda txt : re . match ( r'.*\. .*? ' , txt ) is None ) else : # true if txt is  not  a   name, false otherwise: no name = ( lambda txt : re . match ( r'.*\. .*?' , txt ) is None ) matches = filter ( no name , matches ) except Name Error : # catches <undefined attributes>.<tab> matches = [ ] else : matches = self . global matches ( text ) return matches
def match one ( self , rec , tests ) : for key , test in tests . iteritems ( ) : if not test ( rec . get ( key , None ) ) : return False return True
def match ( self , check ) : matches = [ ] tests = { } for k , v in check . iteritems ( ) : if isinstance ( v , dict ) : tests [ k ] = Composite Filter ( v ) else : tests [ k ] = lambda o : o == v for rec in self . records . itervalues ( ) : if self . match one ( rec , tests ) : matches . append ( copy ( rec ) ) return matches
def extract subdict ( self , rec , keys ) : d = { } d [ 'msg id' ] = rec [ 'msg id' ] for key in keys : d [ key ] = rec [ key ] return copy ( d )
def add record ( self , msg id , rec ) : if self . records . has key ( msg id ) : raise Key Error ( "Already have msg id %r" % ( msg id ) ) self . records [ msg id ] = rec
def get record ( self , msg id ) : if not msg id in self . records : raise Key Error ( "No such msg id %r" % ( msg id ) ) return copy ( self . records [ msg id ] )
def drop matching records ( self , check ) : matches = self . match ( check ) for m in matches : del self . records [ m [ 'msg id' ] ]
def get history ( self ) : msg ids = self . records . keys ( ) return sorted ( msg ids , key = lambda m : self . records [ m ] [ 'submitted' ] )
def quiet ( self ) : # do not print output if input ends in ';' try : cell = self . shell . history manager . input hist parsed [ self . prompt count ] if cell . rstrip ( ) . endswith ( ';' ) : return True except Index Error : # some uses of ipshellembed may fail here pass return False
def update user ns ( self , result ) : # Avoid recursive reference when displaying  oh/Out if result is not self . shell . user ns [ ' oh' ] : if len ( self . shell . user ns [ ' oh' ] ) >= self . cache size and self . do full cache : warn ( 'Output cache limit (currently ' + `self.cache size` + ' entries) hit.\n' 'Flushing cache and resetting history counter...\n' 'The only history variables available will be  , ,  and  1\n' 'with the current result.' ) self . flush ( ) # Don't overwrite ' ' and friends if ' ' is in  builtin  (otherwise # we cause buggy behavior for things like gettext). if ' ' not in builtin . dict : self . = self . self . = self . self . = result self . shell . push ( { ' ' : self . , ' ' : self . , ' ' : self . } , interactive = False ) # hackish access to top-level  namespace to create  1, 2... dynamically to main = { } if self . do full cache : new result = ' ' + `self.prompt count` to main [ new result ] = result self . shell . push ( to main , interactive = False ) self . shell . user ns [ ' oh' ] [ self . prompt count ] = result
def log output ( self , format dict ) : if self . shell . logger . log output : self . shell . logger . log write ( format dict [ 'text/plain' ] , 'output' ) self . shell . history manager . output hist reprs [ self . prompt count ] = format dict [ 'text/plain' ]
def finish displayhook ( self ) : io . stdout . write ( self . shell . separate out2 ) io . stdout . flush ( )
def load ipython extension ( ip ) : global loaded if not loaded : plugin = Store Magic ( shell = ip , config = ip . config ) ip . plugin manager . register plugin ( 'storemagic' , plugin ) loaded = True
def raise if freezed ( self ) : if self . is freezed : name = type ( self ) . name raise Invalid Operation Exception ( 'obj {name} is freezed.' . format ( name = name ) )
def mysql timestamp converter ( s ) : # My SQL>4.1 returns TIMESTAMP in the same format as DATETIME if s [ 4 ] == '-' : return Date Time or None ( s ) s = s + "0" * ( 14 - len ( s ) ) # padding parts = map ( int , filter ( None , ( s [ : 4 ] , s [ 4 : 6 ] , s [ 6 : 8 ] , s [ 8 : 10 ] , s [ 10 : 12 ] , s [ 12 : 14 ] ) ) ) try : return Timestamp ( * parts ) except ( System Exit , Keyboard Interrupt ) : raise except : return None
def eventloop changed ( self , name , old , new ) : loop = ioloop . IO Loop . instance ( ) loop . add timeout ( time . time ( ) + 0.1 , self . enter eventloop )
def start ( self ) : self . shell . exit now = False if self . control stream : self . control stream . on recv ( self . dispatch control , copy = False ) def make dispatcher ( stream ) : def dispatcher ( msg ) : return self . dispatch shell ( stream , msg ) return dispatcher for s in self . shell streams : s . on recv ( make dispatcher ( s ) , copy = False )
def do one iteration ( self ) : if self . control stream : self . control stream . flush ( ) for stream in self . shell streams : # handle at most one request per iteration stream . flush ( zmq . POLLIN , 1 ) stream . flush ( zmq . POLLOUT )
def publish pyin ( self , code , parent , execution count ) : self . session . send ( self . iopub socket , u'pyin' , { u'code' : code , u'execution count' : execution count } , parent = parent , ident = self . topic ( 'pyin' ) )
def abort request ( self , stream , ident , parent ) : msg ids = parent [ 'content' ] . get ( 'msg ids' , None ) if isinstance ( msg ids , basestring ) : msg ids = [ msg ids ] if not msg ids : self . abort queues ( ) for mid in msg ids : self . aborted . add ( str ( mid ) ) content = dict ( status = 'ok' ) reply msg = self . session . send ( stream , 'abort reply' , content = content , parent = parent , ident = ident ) self . log . debug ( "%s" , reply msg )
def clear request ( self , stream , idents , parent ) : self . shell . reset ( False ) msg = self . session . send ( stream , 'clear reply' , ident = idents , parent = parent , content = dict ( status = 'ok' ) )
def topic ( self , topic ) : if self . int id >= 0 : base = "engine.%i" % self . int id else : base = "kernel.%s" % self . ident return py3compat . cast bytes ( "%s.%s" % ( base , topic ) )
def at shutdown ( self ) : # io.rprint("Kernel at shutdown") # dbg if self . shutdown message is not None : self . session . send ( self . iopub socket , self . shutdown message , ident = self . topic ( 'shutdown' ) ) self . log . debug ( "%s" , self . shutdown message ) [ s . flush ( zmq . POLLOUT ) for s in self . shell streams ]
def init gui pylab ( self ) : # Provide a wrapper for :meth:`Interactive Shell App.init gui pylab` # to ensure that any exception is printed straight to stderr. # Normally  showtraceback associates the reply with an execution, # which means frontends will never draw it, as this exception # is not associated with any execute request. shell = self . shell showtraceback = shell . showtraceback try : # replace pyerr-sending traceback with stderr def print tb ( etype , evalue , stb ) : print ( "GUI event loop or pylab initialization failed" , file = io . stderr ) print ( shell . Interactive TB . stb2text ( stb ) , file = io . stderr ) shell . showtraceback = print tb Interactive Shell App . init gui pylab ( self ) finally : shell . showtraceback = showtraceback
def before Context ( self ) : mods = sys . modules . copy ( ) self . mod stack . append ( mods )
def virtual memory ( ) : total , active , inactive , wired , free = psutil osx . get virtual mem ( ) avail = inactive + free used = active + inactive + wired percent = usage percent ( ( total - avail ) , total , round = 1 ) return nt virtmem info ( total , avail , percent , used , free , active , inactive , wired )
def get system cpu times ( ) : user , nice , system , idle = psutil osx . get system cpu times ( ) return cputimes ntuple ( user , nice , system , idle )
def get system per cpu times ( ) : ret = [ ] for cpu t in psutil osx . get system per cpu times ( ) : user , nice , system , idle = cpu t item = cputimes ntuple ( user , nice , system , idle ) ret . append ( item ) return ret
def get process cmdline ( self ) : if not pid exists ( self . pid ) : raise No Such Process ( self . pid , self . process name ) return psutil osx . get process cmdline ( self . pid )
def get memory info ( self ) : rss , vms = psutil osx . get process memory info ( self . pid ) [ : 2 ] return nt meminfo ( rss , vms )
def get ext memory info ( self ) : rss , vms , pfaults , pageins = psutil osx . get process memory info ( self . pid ) return self . nt ext mem ( rss , vms , pfaults * PAGESIZE , pageins * PAGESIZE )
def get open files ( self ) : if self . pid == 0 : return [ ] files = [ ] rawlist = psutil osx . get process open files ( self . pid ) for path , fd in rawlist : if isfile strict ( path ) : ntuple = nt openfile ( path , fd ) files . append ( ntuple ) return files
def usage percent ( used , total , round = None ) : try : ret = ( used / total ) * 100 except Zero Division Error : ret = 0 if round is not None : return round ( ret , round ) else : return ret
def memoize ( f ) : cache = { } def memf ( * x ) : if x not in cache : cache [ x ] = f ( * x ) return cache [ x ] return memf
def deprecated ( replacement = None ) : def outer ( fun ) : msg = "psutil.%s is deprecated" % fun . name if replacement is not None : msg += "; use %s instead" % replacement if fun . doc is None : fun . doc = msg @ wraps ( fun ) def inner ( * args , * * kwargs ) : warnings . warn ( msg , category = Deprecation Warning , stacklevel = 2 ) return fun ( * args , * * kwargs ) return inner return outer
def login ( self ) : try : self . gd client = gdata . docs . client . Docs Client ( ) self . gd client . Client Login ( self . email , self . password , self . source ) except Request Error as e : raise PO Docs Error ( e )
def get gdocs key ( self ) : try : args = urlparse . parse qs ( urlparse . urlparse ( self . url ) . query ) self . key = args [ 'key' ] [ 0 ] except Key Error as e : raise PO Docs Error ( e )
def ensure temp path exists ( self ) : try : if not os . path . exists ( self . temp path ) : os . mkdir ( self . temp path ) except OS Error as e : raise PO Docs Error ( e )
def download ( self ) : trans csv path = os . path . realpath ( os . path . join ( self . temp path , GDOCS TRANS CSV ) ) meta csv path = os . path . realpath ( os . path . join ( self . temp path , GDOCS META CSV ) ) self . download csv from gdocs ( trans csv path , meta csv path ) try : csv to po ( trans csv path , meta csv path , self . locale root , self . po files path , header = self . header ) except IO Error as e : raise PO Docs Error ( e ) self . clear temp ( )
def clear ( self ) : empty file path = os . path . join ( self . temp path , 'empty.csv' ) try : empty file = open ( empty file path , 'w' ) empty file . write ( ',' ) empty file . close ( ) except IO Error as e : raise PO Docs Error ( e ) self . upload file to gdoc ( empty file path , content type = 'text/csv' ) os . remove ( empty file path )
def new qt console ( self , evt = None ) : return connect qtconsole ( self . ipkernel . connection file , profile = self . ipkernel . profile )
def url has contents ( url , contents , case sensitive = False , timeout = 10 ) : try : req = urllib2 . urlopen ( url , timeout = timeout ) except Exception , : False else : rep = req . read ( ) if ( not case sensitive and rep . lower ( ) . find ( contents . lower ( ) ) >= 0 ) or ( case sensitive and rep . find ( contents ) >= 0 ) : return True else : return False
def get response code ( url , timeout = 10 ) : try : req = urllib2 . urlopen ( url , timeout = timeout ) except HTTP Error , e : return e . getcode ( ) except Exception , : fail ( "Couldn't reach the URL '%s'" % url ) else : return req . getcode ( )
def clear output ( self , stdout = True , stderr = True , other = True ) : if stdout : print ( '\033[2K\r' , file = io . stdout , end = '' ) io . stdout . flush ( ) if stderr : print ( '\033[2K\r' , file = io . stderr , end = '' ) io . stderr . flush ( )
def source file ( self ) : if os . path . exists ( self . filename ) : # A regular text file: open it. return open source ( self . filename ) # Maybe it's in a zip file? source = self . file locator . get zip data ( self . filename ) if source is not None : return String IO ( source ) # Couldn't find source. raise Coverage Exception ( "No source for code '%s'." % self . filename )
def total seconds ( td ) : try : # Python >= 2.7 return td . total seconds ( ) except Attribute Error : # Python 2.6 return 1e-6 * ( td . microseconds + ( td . seconds + td . days * 24 * 3600 ) * 10 ** 6 )
def abort ( self ) : assert not self . ready ( ) , "Can't abort, I am already done!" return self . client . abort ( self . msg ids , targets = self . targets , block = True )
def elapsed ( self ) : if self . ready ( ) : return self . wall time now = submitted = datetime . now ( ) for msg id in self . msg ids : if msg id in self . client . metadata : stamp = self . client . metadata [ msg id ] [ 'submitted' ] if stamp and stamp < submitted : submitted = stamp return total seconds ( now - submitted )
def wait interactive ( self , interval = 1. , timeout = None ) : N = len ( self ) tic = time . time ( ) while not self . ready ( ) and ( timeout is None or time . time ( ) - tic <= timeout ) : self . wait ( interval ) clear output ( ) print ( "%4i/%i tasks finished after %4i s" % ( self . progress , N , self . elapsed ) , end = "" ) sys . stdout . flush ( ) print ( ) print ( "done" )
def republish displaypub ( self , content , eid ) : try : ip = get ipython ( ) except Name Error : # displaypub is meaningless outside I Python return md = content [ 'metadata' ] or { } md [ 'engine' ] = eid ip . display pub . publish ( content [ 'source' ] , content [ 'data' ] , md )
def wait for outputs ( self , timeout = - 1 ) : if not self . success : # don't wait on errors return tic = time . time ( ) while not all ( md [ 'outputs ready' ] for md in self . metadata ) : time . sleep ( 0.01 ) self . client . flush iopub ( self . client . iopub socket ) if timeout >= 0 and time . time ( ) > tic + timeout : break
def unordered iter ( self ) : try : rlist = self . get ( 0 ) except error . Timeout Error : pending = set ( self . msg ids ) while pending : try : self . client . wait ( pending , 1e-3 ) except error . Timeout Error : # ignore timeout error, because that only means # *some* jobs are outstanding pass # update ready set with those no longer outstanding: ready = pending . difference ( self . client . outstanding ) # update pending to exclude those that are finished pending = pending . difference ( ready ) while ready : msg id = ready . pop ( ) ar = Async Result ( self . client , msg id , self . fname ) rlist = ar . get ( ) try : for r in rlist : yield r except Type Error : # flattened, not a list # this could get broken by flattened data that returns iterables # but most calls to map do not expose the `flatten` argument yield rlist else : # already done for r in rlist : yield r
def wait ( self , timeout = - 1 ) : start = time . time ( ) if self . ready : return local ids = filter ( lambda msg id : msg id in self . client . outstanding , self . msg ids ) local ready = self . client . wait ( local ids , timeout ) if local ready : remote ids = filter ( lambda msg id : msg id not in self . client . results , self . msg ids ) if not remote ids : self . ready = True else : rdict = self . client . result status ( remote ids , status only = False ) pending = rdict [ 'pending' ] while pending and ( timeout < 0 or time . time ( ) < start + timeout ) : rdict = self . client . result status ( remote ids , status only = False ) pending = rdict [ 'pending' ] if pending : time . sleep ( 0.1 ) if not pending : self . ready = True if self . ready : try : results = map ( self . client . results . get , self . msg ids ) self . result = results if self . single result : r = results [ 0 ] if isinstance ( r , Exception ) : raise r else : results = error . collect exceptions ( results , self . fname ) self . result = self . reconstruct result ( results ) except Exception , e : self . exception = e self . success = False else : self . success = True finally : self . metadata = map ( self . client . metadata . get , self . msg ids )
def abs file ( filename ) : path = os . path . expandvars ( os . path . expanduser ( filename ) ) path = os . path . abspath ( os . path . realpath ( path ) ) path = actual path ( path ) return path
def sep ( s ) : sep match = re . search ( r"[\\/]" , s ) if sep match : the sep = sep match . group ( 0 ) else : the sep = os . sep return the sep
def match ( self , fpath ) : for d in self . dirs : if fpath . startswith ( d ) : if fpath == d : # This is the same file! return True if fpath [ len ( d ) ] == os . sep : # This is a file in the directory return True return False
def match ( self , fpath ) : for pat in self . pats : if fnmatch . fnmatch ( fpath , pat ) : return True return False
def loop qt4 ( kernel ) : from I Python . external . qt for kernel import Qt Core from I Python . lib . guisupport import get app qt4 , start event loop qt4 kernel . app = get app qt4 ( [ " " ] ) kernel . app . set Quit On Last Window Closed ( False ) kernel . timer = Qt Core . Q Timer ( ) kernel . timer . timeout . connect ( kernel . do one iteration ) # Units for the timer are in milliseconds kernel . timer . start ( 1000 * kernel . poll interval ) start event loop qt4 ( kernel . app )
def loop wx ( kernel ) : import wx from I Python . lib . guisupport import start event loop wx doi = kernel . do one iteration # Wx uses milliseconds poll interval = int ( 1000 * kernel . poll interval ) # We have to put the wx.Timer in a wx.Frame for it to fire properly. # We make the Frame hidden when we create it in the main app below. class Timer Frame ( wx . Frame ) : def init ( self , func ) : wx . Frame . init ( self , None , - 1 ) self . timer = wx . Timer ( self ) # Units for the timer are in milliseconds self . timer . Start ( poll interval ) self . Bind ( wx . EVT TIMER , self . on timer ) self . func = func def on timer ( self , event ) : self . func ( ) # We need a custom wx.App to create our Frame subclass that has the # wx.Timer to drive the ZMQ event loop. class IP Wx App ( wx . App ) : def On Init ( self ) : self . frame = Timer Frame ( doi ) self . frame . Show ( False ) return True # The redirect=False here makes sure that wx doesn't replace # sys.stdout/stderr with its own classes. kernel . app = IP Wx App ( redirect = False ) # The import of wx on Linux sets the handler for signal.SIGINT # to 0.  This is a bug in wx or gtk.  We fix by just setting it # back to the Python default. import signal if not callable ( signal . getsignal ( signal . SIGINT ) ) : signal . signal ( signal . SIGINT , signal . default int handler ) start event loop wx ( kernel . app )
def loop tk ( kernel ) : import Tkinter doi = kernel . do one iteration # Tk uses milliseconds poll interval = int ( 1000 * kernel . poll interval ) # For Tkinter, we create a Tk object and call its withdraw method. class Timer ( object ) : def init ( self , func ) : self . app = Tkinter . Tk ( ) self . app . withdraw ( ) self . func = func def on timer ( self ) : self . func ( ) self . app . after ( poll interval , self . on timer ) def start ( self ) : self . on timer ( ) # Call it once to get things going. self . app . mainloop ( ) kernel . timer = Timer ( doi ) kernel . timer . start ( )
def loop gtk ( kernel ) : from . gui . gtkembed import GTK Embed gtk kernel = GTK Embed ( kernel ) gtk kernel . start ( )
def enable gui ( gui , kernel = None ) : if gui not in loop map : raise Value Error ( "GUI %r not supported" % gui ) if kernel is None : if Application . initialized ( ) : kernel = getattr ( Application . instance ( ) , 'kernel' , None ) if kernel is None : raise Runtime Error ( "You didn't specify a kernel," " and no I Python Application with a kernel appears to be running." ) loop = loop map [ gui ] if kernel . eventloop is not None and kernel . eventloop is not loop : raise Runtime Error ( "Cannot activate multiple GUI eventloops" ) kernel . eventloop = loop
def GOE ( N ) : m = ra . standard normal ( ( N , N ) ) m += m . T return m / 2
def center eigenvalue diff ( mat ) : N = len ( mat ) evals = np . sort ( la . eigvals ( mat ) ) diff = np . abs ( evals [ N / 2 ] - evals [ N / 2 - 1 ] ) return diff
def ensemble diffs ( num , N ) : diffs = np . empty ( num ) for i in xrange ( num ) : mat = GOE ( N ) diffs [ i ] = center eigenvalue diff ( mat ) return diffs
def init crash handler ( self ) : self . crash handler = self . crash handler class ( self ) sys . excepthook = self . excepthook def unset crashhandler ( ) : sys . excepthook = sys . excepthook atexit . register ( unset crashhandler )
def init profile dir ( self ) : try : # location explicitly specified: location = self . config . Profile Dir . location except Attribute Error : # location not specified, find by profile name try : p = Profile Dir . find profile dir by name ( self . ipython dir , self . profile , self . config ) except Profile Dir Error : # not found, maybe create it (always create default profile) if self . auto create or self . profile == 'default' : try : p = Profile Dir . create profile dir by name ( self . ipython dir , self . profile , self . config ) except Profile Dir Error : self . log . fatal ( "Could not create profile: %r" % self . profile ) self . exit ( 1 ) else : self . log . info ( "Created profile dir: %r" % p . location ) else : self . log . fatal ( "Profile %r not found." % self . profile ) self . exit ( 1 ) else : self . log . info ( "Using existing profile dir: %r" % p . location ) else : # location is fully specified try : p = Profile Dir . find profile dir ( location , self . config ) except Profile Dir Error : # not found, maybe create it if self . auto create : try : p = Profile Dir . create profile dir ( location , self . config ) except Profile Dir Error : self . log . fatal ( "Could not create profile directory: %r" % location ) self . exit ( 1 ) else : self . log . info ( "Creating new profile dir: %r" % location ) else : self . log . fatal ( "Profile directory %r not found." % location ) self . exit ( 1 ) else : self . log . info ( "Using existing profile dir: %r" % location ) self . profile dir = p self . config file paths . append ( p . location )
def stage default config file ( self ) : s = self . generate config file ( ) fname = os . path . join ( self . profile dir . location , self . config file name ) if self . overwrite or not os . path . exists ( fname ) : self . log . warn ( "Generating default config file: %r" % ( fname ) ) with open ( fname , 'w' ) as f : f . write ( s )
def erase ( self ) : if self . use file : if self . filename : file be gone ( self . filename ) self . lines = { } self . arcs = { }
def line data ( self ) : return dict ( [ ( f , sorted ( lmap . keys ( ) ) ) for f , lmap in iitems ( self . lines ) ] )
def arc data ( self ) : return dict ( [ ( f , sorted ( amap . keys ( ) ) ) for f , amap in iitems ( self . arcs ) ] )
def write file ( self , filename ) : # Create the file data. data = { } data [ 'lines' ] = self . line data ( ) arcs = self . arc data ( ) if arcs : data [ 'arcs' ] = arcs if self . collector : data [ 'collector' ] = self . collector if self . debug and self . debug . should ( 'dataio' ) : self . debug . write ( "Writing data to %r" % ( filename , ) ) # Write the pickle to the file. fdata = open ( filename , 'wb' ) try : pickle . dump ( data , fdata , 2 ) finally : fdata . close ( )
def read file ( self , filename ) : self . lines , self . arcs = self . read file ( filename )
def raw data ( self , filename ) : if self . debug and self . debug . should ( 'dataio' ) : self . debug . write ( "Reading data from %r" % ( filename , ) ) fdata = open ( filename , 'rb' ) try : data = pickle . load ( fdata ) finally : fdata . close ( ) return data
def add to hash ( self , filename , hasher ) : hasher . update ( self . executed lines ( filename ) ) hasher . update ( self . executed arcs ( filename ) )
def get pasted lines ( sentinel , l input = py3compat . input ) : print "Pasting code; enter '%s' alone on the line to stop or use Ctrl-D." % sentinel while True : try : l = l input ( ':' ) if l == sentinel : return else : yield l except EOF Error : print '<EOF>' return
def replace rlhist multiline ( self , source raw , hlen before cell ) : # do nothing without readline or disabled multiline if not self . has readline or not self . multiline history : return hlen before cell # windows rl has no remove history item if not hasattr ( self . readline , "remove history item" ) : return hlen before cell # skip empty cells if not source raw . rstrip ( ) : return hlen before cell # nothing changed do nothing, e.g. when rl removes consecutive dups hlen = self . readline . get current history length ( ) if hlen == hlen before cell : return hlen before cell for i in range ( hlen - hlen before cell ) : self . readline . remove history item ( hlen - i - 1 ) stdin encoding = get stream enc ( sys . stdin , 'utf-8' ) self . readline . add history ( py3compat . unicode to str ( source raw . rstrip ( ) , stdin encoding ) ) return self . readline . get current history length ( )
def interact ( self , display banner = None ) : # batch run -> do not interact if self . exit now : return if display banner is None : display banner = self . display banner if isinstance ( display banner , basestring ) : self . show banner ( display banner ) elif display banner : self . show banner ( ) more = False if self . has readline : self . readline startup hook ( self . pre readline ) hlen b4 cell = self . readline . get current history length ( ) else : hlen b4 cell = 0 # exit now is set by a call to %Exit or %Quit, through the # ask exit callback. while not self . exit now : self . hooks . pre prompt hook ( ) if more : try : prompt = self . prompt manager . render ( 'in2' ) except : self . showtraceback ( ) if self . autoindent : self . rl do indent = True else : try : prompt = self . separate in + self . prompt manager . render ( 'in' ) except : self . showtraceback ( ) try : line = self . raw input ( prompt ) if self . exit now : # quick exit on sys.std[in|out] close break if self . autoindent : self . rl do indent = False except Keyboard Interrupt : #double-guard against keyboardinterrupts during kbdint handling try : self . write ( '\n Keyboard Interrupt\n' ) source raw = self . input splitter . source raw reset ( ) [ 1 ] hlen b4 cell = self . replace rlhist multiline ( source raw , hlen b4 cell ) more = False except Keyboard Interrupt : pass except EOF Error : if self . autoindent : self . rl do indent = False if self . has readline : self . readline startup hook ( None ) self . write ( '\n' ) self . exit ( ) except bdb . Bdb Quit : warn ( 'The Python debugger has exited with a Bdb Quit exception.\n' 'Because of how pdb handles the stack, it is impossible\n' 'for I Python to properly format this particular exception.\n' 'I Python will resume normal operation.' ) except : # exceptions here are VERY RARE, but they can be triggered # asynchronously by signal handlers, for example. self . showtraceback ( ) else : self . input splitter . push ( line ) more = self . input splitter . push accepts more ( ) if ( self . Syntax TB . last syntax error and self . autoedit syntax ) : self . edit syntax error ( ) if not more : source raw = self . input splitter . source raw reset ( ) [ 1 ] self . run cell ( source raw , store history = True ) hlen b4 cell = self . replace rlhist multiline ( source raw , hlen b4 cell ) # Turn off the exit flag, so the mainloop can be restarted if desired self . exit now = False
def should recompile ( self , e ) : if e . filename in ( '<ipython console>' , '<input>' , '<string>' , '<console>' , '<Background Job compilation>' , None ) : return False try : if ( self . autoedit syntax and not self . ask yes no ( 'Return to editor to correct syntax error? ' '[Y/n] ' , 'y' ) ) : return False except EOF Error : return False def int0 ( x ) : try : return int ( x ) except Type Error : return 0 # always pass integer line and offset values to editor hook try : self . hooks . fix error editor ( e . filename , int0 ( e . lineno ) , int0 ( e . offset ) , e . msg ) except Try Next : warn ( 'Could not open editor' ) return False return True
def new frontend master ( self ) : ip = self . ip if self . ip in LOCAL IPS else LOCALHOST kernel manager = self . kernel manager class ( ip = ip , connection file = self . new connection file ( ) , config = self . config , ) # start the kernel kwargs = dict ( ) kwargs [ 'extra arguments' ] = self . kernel argv kernel manager . start kernel ( * * kwargs ) kernel manager . start channels ( ) widget = self . widget factory ( config = self . config , local kernel = True ) self . init colors ( widget ) widget . kernel manager = kernel manager widget . existing = False widget . may close = True widget . confirm exit = self . confirm exit return widget
def init colors ( self , widget ) : # Note: This will be dramatically simplified when colors # are removed from the backend. # parse the colors arg down to current known labels try : colors = self . config . ZMQ Interactive Shell . colors except Attribute Error : colors = None try : style = self . config . I Python Widget . syntax style except Attribute Error : style = None try : sheet = self . config . I Python Widget . style sheet except Attribute Error : sheet = None # find the value for colors: if colors : colors = colors . lower ( ) if colors in ( 'lightbg' , 'light' ) : colors = 'lightbg' elif colors in ( 'dark' , 'linux' ) : colors = 'linux' else : colors = 'nocolor' elif style : if style == 'bw' : colors = 'nocolor' elif styles . dark style ( style ) : colors = 'linux' else : colors = 'lightbg' else : colors = None # Configure the style if style : widget . style sheet = styles . sheet from template ( style , colors ) widget . syntax style = style widget . syntax style changed ( ) widget . style sheet changed ( ) elif colors : # use a default dark/light/bw style widget . set default style ( colors = colors ) if self . stylesheet : # we got an explicit stylesheet if os . path . isfile ( self . stylesheet ) : with open ( self . stylesheet ) as f : sheet = f . read ( ) else : raise IO Error ( "Stylesheet %r not found." % self . stylesheet ) if sheet : widget . style sheet = sheet widget . style sheet changed ( )
def info ( self ) : return ( self . identity , self . url , self . pub url , self . location )
def set colors ( self , * args , * * kw ) : # Set own color table self . color scheme table . set active scheme ( * args , * * kw ) # for convenience, set Colors to the active scheme self . Colors = self . color scheme table . active colors # Also set colors of debugger if hasattr ( self , 'pdb' ) and self . pdb is not None : self . pdb . set colors ( * args , * * kw )
def color toggle ( self ) : if self . color scheme table . active scheme name == 'No Color' : self . color scheme table . set active scheme ( self . old scheme ) self . Colors = self . color scheme table . active colors else : self . old scheme = self . color scheme table . active scheme name self . color scheme table . set active scheme ( 'No Color' ) self . Colors = self . color scheme table . active colors
def structured traceback ( self , etype , evalue , etb , tb offset = None , context = 5 ) : tb offset = self . tb offset if tb offset is None else tb offset # some locals try : etype = etype . name except Attribute Error : pass Colors = self . Colors # just a shorthand + quicker name lookup Colors Normal = Colors . Normal # used a lot col scheme = self . color scheme table . active scheme name indent = ' ' * INDENT SIZE em normal = '%s\n%s%s' % ( Colors . val Em , indent , Colors Normal ) undefined = '%sundefined%s' % ( Colors . em , Colors Normal ) exc = '%s%s%s' % ( Colors . exc Name , etype , Colors Normal ) # some internal-use functions def text repr ( value ) : """Hopefully pretty robust repr equivalent.""" # this is pretty horrible but should always return *something* try : return pydoc . text . repr ( value ) except Keyboard Interrupt : raise except : try : return repr ( value ) except Keyboard Interrupt : raise except : try : # all still in an except block so we catch # getattr raising name = getattr ( value , ' name ' , None ) if name : # ick, recursion return text repr ( name ) klass = getattr ( value , ' class ' , None ) if klass : return '%s instance' % text repr ( klass ) except Keyboard Interrupt : raise except : return 'UNRECOVERABLE REPR FAILURE' def eqrepr ( value , repr = text repr ) : return '=%s' % repr ( value ) def nullrepr ( value , repr = text repr ) : return '' # meat of the code begins try : etype = etype . name except Attribute Error : pass if self . long header : # Header with the exception type, python version, and date pyver = 'Python ' + sys . version . split ( ) [ 0 ] + ': ' + sys . executable date = time . ctime ( time . time ( ) ) head = '%s%s%s\n%s%s%s\n%s' % ( Colors . topline , '-' * 75 , Colors Normal , exc , ' ' * ( 75 - len ( str ( etype ) ) - len ( pyver ) ) , pyver , date . rjust ( 75 ) ) head += "\n A problem occured executing Python code.  Here is the sequence of function" "\ncalls leading up to the error, with the most recent (innermost) call last." else : # Simplified header head = '%s%s%s\n%s%s' % ( Colors . topline , '-' * 75 , Colors Normal , exc , 'Traceback (most recent call last)' . rjust ( 75 - len ( str ( etype ) ) ) ) frames = [ ] # Flush cache before calling inspect.  This helps alleviate some of the # problems with python 2.3's inspect.py. ##self.check cache() # Drop topmost frames if requested try : # Try the default getinnerframes and Alex's: Alex's fixes some # problems, but it generates empty tracebacks for console errors # (5 blanks lines) where none should be returned. #records = inspect.getinnerframes(etb, context)[tb offset:] #print 'python records:', records # dbg records = fixed getinnerframes ( etb , context , tb offset ) #print 'alex   records:', records # dbg except : # FIXME: I've been getting many crash reports from python 2.3 # users, traceable to inspect.py.  If I can find a small test-case # to reproduce this, I should either write a better workaround or # file a bug report against inspect (if that's the real problem). # So far, I haven't been able to find an isolated example to # reproduce the problem. inspect error ( ) traceback . print exc ( file = self . ostream ) info ( '\n Unfortunately, your original traceback can not be constructed.\n' ) return '' # build some color string templates outside these nested loops tpl link = '%s%%s%s' % ( Colors . filename Em , Colors Normal ) tpl call = 'in %s%%s%s%%s%s' % ( Colors . v Name , Colors . val Em , Colors Normal ) tpl call fail = 'in %s%%s%s(***failed resolving arguments***)%s' % ( Colors . v Name , Colors . val Em , Colors Normal ) tpl local var = '%s%%s%s' % ( Colors . v Name , Colors Normal ) tpl global var = '%sglobal%s %s%%s%s' % ( Colors . em , Colors Normal , Colors . v Name , Colors Normal ) tpl name val = '%%s %s= %%s%s' % ( Colors . val Em , Colors Normal ) tpl line = '%s%%s%s %%s' % ( Colors . lineno , Colors Normal ) tpl line em = '%s%%s%s %%s%s' % ( Colors . lineno Em , Colors . line , Colors Normal ) # now, loop over all records printing context and info abspath = os . path . abspath for frame , file , lnum , func , lines , index in records : #print '*** record:',file,lnum,func,lines,index  # dbg if not file : file = '?' elif not ( file . startswith ( "<" ) and file . endswith ( ">" ) ) : # Guess that filenames like <string> aren't real filenames, so # don't call abspath on them.                     try : file = abspath ( file ) except OS Error : # Not sure if this can still happen: abspath now works with # file names like <string> pass link = tpl link % file args , varargs , varkw , locals = inspect . getargvalues ( frame ) if func == '?' : call = '' else : # Decide whether to include variable details or not var repr = self . include vars and eqrepr or nullrepr try : call = tpl call % ( func , inspect . formatargvalues ( args , varargs , varkw , locals , formatvalue = var repr ) ) except Key Error : # This happens in situations like errors inside generator # expressions, where local variables are listed in the # line, but can't be extracted from the frame.  I'm not # 100% sure this isn't actually a bug in inspect itself, # but since there's no info for us to compute with, the # best we can do is report the failure and move on.  Here # we must *not* call any traceback construction again, # because that would mess up use of %debug later on.  So we # simply report the failure and move on.  The only # limitation will be that this frame won't have locals # listed in the call signature.  Quite subtle problem... # I can't think of a good way to validate this in a unit # test, but running a script consisting of: #  dict( (k,v.strip()) for (k,v) in range(10) ) # will illustrate the error, if this exception catch is # disabled. call = tpl call fail % func # Don't attempt to tokenize binary files. if file . endswith ( ( '.so' , '.pyd' , '.dll' ) ) : frames . append ( '%s %s\n' % ( link , call ) ) continue elif file . endswith ( ( '.pyc' , '.pyo' ) ) : # Look up the corresponding source file. file = pyfile . source from cache ( file ) def linereader ( file = file , lnum = [ lnum ] , getline = linecache . getline ) : line = getline ( file , lnum [ 0 ] ) lnum [ 0 ] += 1 return line # Build the list of names on this line of code where the exception # occurred. try : names = [ ] name cont = False for token type , token , start , end , line in generate tokens ( linereader ) : # build composite names if token type == tokenize . NAME and token not in keyword . kwlist : if name cont : # Continuation of a dotted name try : names [ - 1 ] . append ( token ) except Index Error : names . append ( [ token ] ) name cont = False else : # Regular new names.  We append everything, the caller # will be responsible for pruning the list later.  It's # very tricky to try to prune as we go, b/c composite # names can fool us.  The pruning at the end is easy # to do (or the caller can print a list with repeated # names if so desired. names . append ( [ token ] ) elif token == '.' : name cont = True elif token type == tokenize . NEWLINE : break except ( Index Error , Unicode Decode Error ) : # signals exit of tokenizer pass except tokenize . Token Error , msg : m = ( "An unexpected error occurred while tokenizing input\n" "The following traceback may be corrupted or invalid\n" "The error message is: %s\n" % msg ) error ( m ) # Join composite names (e.g. "dict.fromkeys") names = [ '.' . join ( n ) for n in names ] # prune names list of duplicates, but keep the right order unique names = uniq stable ( names ) # Start loop over vars lvals = [ ] if self . include vars : for name full in unique names : name base = name full . split ( '.' , 1 ) [ 0 ] if name base in frame . f code . co varnames : if locals . has key ( name base ) : try : value = repr ( eval ( name full , locals ) ) except : value = undefined else : value = undefined name = tpl local var % name full else : if frame . f globals . has key ( name base ) : try : value = repr ( eval ( name full , frame . f globals ) ) except : value = undefined else : value = undefined name = tpl global var % name full lvals . append ( tpl name val % ( name , value ) ) if lvals : lvals = '%s%s' % ( indent , em normal . join ( lvals ) ) else : lvals = '' level = '%s %s\n' % ( link , call ) if index is None : frames . append ( level ) else : frames . append ( '%s%s' % ( level , '' . join ( format traceback lines ( lnum , index , lines , Colors , lvals , col scheme ) ) ) ) # Get (safely) a string form of the exception info try : etype str , evalue str = map ( str , ( etype , evalue ) ) except : # User exception is improperly defined. etype , evalue = str , sys . exc info ( ) [ : 2 ] etype str , evalue str = map ( str , ( etype , evalue ) ) # ... and format it exception = [ '%s%s%s: %s' % ( Colors . exc Name , etype str , Colors Normal , evalue str ) ] if ( not py3compat . PY3 ) and type ( evalue ) is types . Instance Type : try : names = [ w for w in dir ( evalue ) if isinstance ( w , basestring ) ] except : # Every now and then, an object with funny inernals blows up # when dir() is called on it.  We do the best we can to report # the problem and continue m = '%s Exception reporting error (object with broken dir())%s:' exception . append ( m % ( Colors . exc Name , Colors Normal ) ) etype str , evalue str = map ( str , sys . exc info ( ) [ : 2 ] ) exception . append ( '%s%s%s: %s' % ( Colors . exc Name , etype str , Colors Normal , evalue str ) ) names = [ ] for name in names : value = text repr ( getattr ( evalue , name ) ) exception . append ( '\n%s%s = %s' % ( indent , name , value ) ) # vds: >> if records : filepath , lnum = records [ - 1 ] [ 1 : 3 ] #print "file:", str(file), "linenb", str(lnum) # dbg filepath = os . path . abspath ( filepath ) ipinst = ipapi . get ( ) if ipinst is not None : ipinst . hooks . synchronize with editor ( filepath , lnum , 0 ) # vds: << # return all our info assembled as a single string # return '%s\n\n%s\n%s' % (head,'\n'.join(frames),''.join(exception[0]) ) return [ head ] + frames + [ '' . join ( exception [ 0 ] ) ]
def group required ( group , login url = None , redirect field name = REDIRECT FIELD NAME , skip superuser = True ) : def decorator ( view func ) : @ login required ( redirect field name = redirect field name , login url = login url ) def wrapped view ( request , * args , * * kwargs ) : if not ( request . user . is superuser and skip superuser ) : if request . user . groups . filter ( name = group ) . count ( ) == 0 : raise Permission Denied return view func ( request , * args , * * kwargs ) return wrapped view return decorator
def add submodule ( mod , submod , fullname , subname ) : if mod is None : return #Nothing to do here. if submod is None : submod = sys . modules [ fullname ] setattr ( mod , subname , submod ) return
def ensure fromlist ( mod , fromlist , buf , recursive ) : if not hasattr ( mod , ' path ' ) : return for item in fromlist : if not hasattr ( item , 'rindex' ) : raise Type Error ( "Item in ``from list'' not a string" ) if item == '*' : if recursive : continue # avoid endless recursion try : all = mod . all except Attribute Error : pass else : ret = ensure fromlist ( mod , all , buf , 1 ) if not ret : return 0 elif not hasattr ( mod , item ) : import submodule ( mod , item , buf + '.' + item )
def add section ( self ) : sect = Code Builder ( self . indent amount ) self . code . append ( sect ) return sect
def get function ( self , fn name ) : assert self . indent amount == 0 g = { } code text = str ( self ) exec ( code text , g ) return g [ fn name ]
def expr code ( self , expr ) : if "|" in expr : pipes = expr . split ( "|" ) code = self . expr code ( pipes [ 0 ] ) for func in pipes [ 1 : ] : self . all vars . add ( func ) code = "c %s(%s)" % ( func , code ) elif "." in expr : dots = expr . split ( "." ) code = self . expr code ( dots [ 0 ] ) args = [ repr ( d ) for d in dots [ 1 : ] ] code = "dot(%s, %s)" % ( code , ", " . join ( args ) ) else : self . all vars . add ( expr ) code = "c %s" % expr return code
def do dots ( self , value , * dots ) : for dot in dots : try : value = getattr ( value , dot ) except Attribute Error : value = value [ dot ] if hasattr ( value , ' call ' ) : value = value ( ) return value
def formatters default ( self ) : formatter classes = [ Plain Text Formatter , HTML Formatter , SVG Formatter , PNG Formatter , JPEG Formatter , Latex Formatter , JSON Formatter , Javascript Formatter ] d = { } for cls in formatter classes : f = cls ( config = self . config ) d [ f . format type ] = f return d
def user config files ( ) : return filter ( os . path . exists , map ( os . path . expanduser , config files ) )
def configure Where ( self , where ) : from nose . importer import add path self . working Dir = None where = tolist ( where ) warned = False for path in where : if not self . working Dir : abs path = absdir ( path ) if abs path is None : raise Value Error ( "Working directory %s not found, or " "not a directory" % path ) log . info ( "Set working dir to %s" , abs path ) self . working Dir = abs path if self . add Paths and os . path . exists ( os . path . join ( abs path , ' init .py' ) ) : log . info ( "Working directory %s is a package; " "adding to sys.path" % abs path ) add path ( abs path ) continue if not warned : warn ( "Use of multiple -w arguments is deprecated and " "support may be removed in a future release. You can " "get the same behavior by passing directories without " "the -w argument on the command line, or by using the " "--tests argument in a configuration file." , Deprecation Warning ) self . test Names . append ( path )
def get Parser ( self , doc = None ) : if self . parser : return self . parser env = self . env parser = self . parser Class ( doc ) parser . add option ( "-V" , "--version" , action = "store true" , dest = "version" , default = False , help = "Output nose version and exit" ) parser . add option ( "-p" , "--plugins" , action = "store true" , dest = "show Plugins" , default = False , help = "Output list of available plugins and exit. Combine with " "higher verbosity for greater detail" ) parser . add option ( "-v" , "--verbose" , action = "count" , dest = "verbosity" , default = self . verbosity , help = "Be more verbose. [NOSE VERBOSE]" ) parser . add option ( "--verbosity" , action = "store" , dest = "verbosity" , metavar = 'VERBOSITY' , type = "int" , help = "Set verbosity; --verbosity=2 is " "the same as -v" ) parser . add option ( "-q" , "--quiet" , action = "store const" , const = 0 , dest = "verbosity" , help = "Be less verbose" ) parser . add option ( "-c" , "--config" , action = "append" , dest = "files" , metavar = "FILES" , help = "Load configuration from config file(s). May be specified " "multiple times; in that case, all config files will be " "loaded and combined" ) parser . add option ( "-w" , "--where" , action = "append" , dest = "where" , metavar = "WHERE" , help = "Look for tests in this directory. " "May be specified multiple times. The first directory passed " "will be used as the working directory, in place of the current " "working directory, which is the default. Others will be added " "to the list of tests to execute. [NOSE WHERE]" ) parser . add option ( "--py3where" , action = "append" , dest = "py3where" , metavar = "PY3WHERE" , help = "Look for tests in this directory under Python 3.x. " "Functions the same as 'where', but only applies if running under " "Python 3.x or above.  Note that, if present under 3.x, this " "option completely replaces any directories specified with " "'where', so the 'where' option becomes ineffective. " "[NOSE PY3WHERE]" ) parser . add option ( "-m" , "--match" , "--testmatch" , action = "store" , dest = "test Match" , metavar = "REGEX" , help = "Files, directories, function names, and class names " "that match this regular expression are considered tests.  " "Default: %s [NOSE TESTMATCH]" % self . test Match Pat , default = self . test Match Pat ) parser . add option ( "--tests" , action = "store" , dest = "test Names" , default = None , metavar = 'NAMES' , help = "Run these tests (comma-separated list). This argument is " "useful mainly from configuration files; on the command line, " "just pass the tests to run as additional arguments with no " "switch." ) parser . add option ( "-l" , "--debug" , action = "store" , dest = "debug" , default = self . debug , help = "Activate debug logging for one or more systems. " "Available debug loggers: nose, nose.importer, " "nose.inspector, nose.plugins, nose.result and " "nose.selector. Separate multiple names with a comma." ) parser . add option ( "--debug-log" , dest = "debug Log" , action = "store" , default = self . debug Log , metavar = "FILE" , help = "Log debug messages to this file " "(default: sys.stderr)" ) parser . add option ( "--logging-config" , "--log-config" , dest = "logging Config" , action = "store" , default = self . logging Config , metavar = "FILE" , help = "Load logging config from this file -- bypasses all other" " logging config settings." ) parser . add option ( "-I" , "--ignore-files" , action = "append" , dest = "ignore Files" , metavar = "REGEX" , help = "Completely ignore any file that matches this regular " "expression. Takes precedence over any other settings or " "plugins. " "Specifying this option will replace the default setting. " "Specify this option multiple times " "to add more regular expressions [NOSE IGNORE FILES]" ) parser . add option ( "-e" , "--exclude" , action = "append" , dest = "exclude" , metavar = "REGEX" , help = "Don't run tests that match regular " "expression [NOSE EXCLUDE]" ) parser . add option ( "-i" , "--include" , action = "append" , dest = "include" , metavar = "REGEX" , help = "This regular expression will be applied to files, " "directories, function names, and class names for a chance " "to include additional tests that do not match TESTMATCH.  " "Specify this option multiple times " "to add more regular expressions [NOSE INCLUDE]" ) parser . add option ( "-x" , "--stop" , action = "store true" , dest = "stop On Error" , default = self . stop On Error , help = "Stop running tests after the first error or failure" ) parser . add option ( "-P" , "--no-path-adjustment" , action = "store false" , dest = "add Paths" , default = self . add Paths , help = "Don't make any changes to sys.path when " "loading tests [NOSE NOPATH]" ) parser . add option ( "--exe" , action = "store true" , dest = "include Exe" , default = self . include Exe , help = "Look for tests in python modules that are " "executable. Normal behavior is to exclude executable " "modules, since they may not be import-safe " "[NOSE INCLUDE EXE]" ) parser . add option ( "--noexe" , action = "store false" , dest = "include Exe" , help = "DO NOT look for tests in python modules that are " "executable. (The default on the windows platform is to " "do so.)" ) parser . add option ( "--traverse-namespace" , action = "store true" , default = self . traverse Namespace , dest = "traverse Namespace" , help = "Traverse through all path entries of a namespace package" ) parser . add option ( "--first-package-wins" , "--first-pkg-wins" , "--1st-pkg-wins" , action = "store true" , default = False , dest = "first Package Wins" , help = "nose's importer will normally evict a package from sys." "modules if it sees a package with the same name in a different " "location. Set this option to disable that behavior." ) self . plugins . load Plugins ( ) self . plugin Opts ( parser ) self . parser = parser return parser
def page file ( fname , start = 0 , pager cmd = None ) : pager cmd = get pager cmd ( pager cmd ) pager cmd += ' ' + get pager start ( pager cmd , start ) try : if os . environ [ 'TERM' ] in [ 'emacs' , 'dumb' ] : raise Environment Error system ( pager cmd + ' ' + fname ) except : try : if start > 0 : start -= 1 page ( open ( fname ) . read ( ) , start ) except : print 'Unable to show file' , `fname`
def print basic unicode ( o , p , cycle ) : if cycle : return p . text ( 'Basic(...)' ) out = pretty ( o , use unicode = True ) if '\n' in out : p . text ( u'\n' ) p . text ( out )
def print png ( o ) : s = latex ( o , mode = 'inline' ) # mathtext does not understand certain latex flags, so we try to replace # them with suitable subs. s = s . replace ( '\\operatorname' , '' ) s = s . replace ( '\\overline' , '\\bar' ) png = latex to png ( s ) return png
def print display png ( o ) : s = latex ( o , mode = 'plain' ) s = s . strip ( '$' ) # As matplotlib does not support display style, dvipng backend is # used here. png = latex to png ( '$$%s$$' % s , backend = 'dvipng' ) return png
def load ipython extension ( ip ) : import sympy # sympyprinting extension has been moved to Sym Py as of 0.7.2, if it # exists there, warn the user and import it try : import sympy . interactive . ipythonprinting except Import Error : pass else : warnings . warn ( "The sympyprinting extension in I Python is deprecated, " "use sympy.interactive.ipythonprinting" ) ip . extension manager . load extension ( 'sympy.interactive.ipythonprinting' ) return global loaded if not loaded : plaintext formatter = ip . display formatter . formatters [ 'text/plain' ] for cls in ( object , str ) : plaintext formatter . for type ( cls , print basic unicode ) printable containers = [ list , tuple ] # set and frozen set were broken with Sym Py's latex() function, but # was fixed in the 0.7.1-git development version. See # http://code.google.com/p/sympy/issues/detail?id=3062. if sympy . version > '0.7.1' : printable containers += [ set , frozenset ] else : plaintext formatter . for type ( cls , print basic unicode ) plaintext formatter . for type by name ( 'sympy.core.basic' , 'Basic' , print basic unicode ) plaintext formatter . for type by name ( 'sympy.matrices.matrices' , 'Matrix' , print basic unicode ) png formatter = ip . display formatter . formatters [ 'image/png' ] png formatter . for type by name ( 'sympy.core.basic' , 'Basic' , print png ) png formatter . for type by name ( 'sympy.matrices.matrices' , 'Matrix' , print display png ) for cls in [ dict , int , long , float ] + printable containers : png formatter . for type ( cls , print png ) latex formatter = ip . display formatter . formatters [ 'text/latex' ] latex formatter . for type by name ( 'sympy.core.basic' , 'Basic' , print latex ) latex formatter . for type by name ( 'sympy.matrices.matrices' , 'Matrix' , print latex ) for cls in printable containers : # Use La Te X only if every element is printable by latex latex formatter . for type ( cls , print latex ) loaded = True
def run loop ( self ) : while True : try : self . ioloop . start ( ) except ZMQ Error as e : if e . errno == errno . EINTR : continue else : raise except Exception : if self . exiting : break else : raise else : break
def input ( self , string ) : content = dict ( value = string ) msg = self . session . msg ( 'input reply' , content ) self . queue send ( msg )
def stop channels ( self ) : if self . shell channel . is alive ( ) : self . shell channel . stop ( ) if self . sub channel . is alive ( ) : self . sub channel . stop ( ) if self . stdin channel . is alive ( ) : self . stdin channel . stop ( ) if self . hb channel . is alive ( ) : self . hb channel . stop ( )
def channels running ( self ) : return ( self . shell channel . is alive ( ) or self . sub channel . is alive ( ) or self . stdin channel . is alive ( ) or self . hb channel . is alive ( ) )
def load connection file ( self ) : with open ( self . connection file ) as f : cfg = json . loads ( f . read ( ) ) self . ip = cfg [ 'ip' ] self . shell port = cfg [ 'shell port' ] self . stdin port = cfg [ 'stdin port' ] self . iopub port = cfg [ 'iopub port' ] self . hb port = cfg [ 'hb port' ] self . session . key = str to bytes ( cfg [ 'key' ] )
def write connection file ( self ) : if self . connection file written : return self . connection file , cfg = write connection file ( self . connection file , ip = self . ip , key = self . session . key , stdin port = self . stdin port , iopub port = self . iopub port , shell port = self . shell port , hb port = self . hb port ) # write connection file also sets default ports: self . shell port = cfg [ 'shell port' ] self . stdin port = cfg [ 'stdin port' ] self . iopub port = cfg [ 'iopub port' ] self . hb port = cfg [ 'hb port' ] self . connection file written = True
def kill kernel ( self ) : if self . has kernel : # Pause the heart beat channel if it exists. if self . hb channel is not None : self . hb channel . pause ( ) # Attempt to kill the kernel. try : self . kernel . kill ( ) except OS Error , e : # In Windows, we will get an Access Denied error if the process # has already terminated. Ignore it. if sys . platform == 'win32' : if e . winerror != 5 : raise # On Unix, we may get an ESRCH error if the process has already # terminated. Ignore it. else : from errno import ESRCH if e . errno != ESRCH : raise self . kernel = None else : raise Runtime Error ( "Cannot kill kernel. No kernel is running!" )
def is alive ( self ) : if self . has kernel : if self . kernel . poll ( ) is None : return True else : return False elif self . hb channel is not None : # We didn't start the kernel with this Kernel Manager so we # use the heartbeat. return self . hb channel . is beating ( ) else : # no heartbeat and not local, we can't tell if it's running, # so naively return True return True
def shell channel ( self ) : if self . shell channel is None : self . shell channel = self . shell channel class ( self . context , self . session , ( self . ip , self . shell port ) ) return self . shell channel
def sub channel ( self ) : if self . sub channel is None : self . sub channel = self . sub channel class ( self . context , self . session , ( self . ip , self . iopub port ) ) return self . sub channel
def walk egg ( egg dir ) : walker = os . walk ( egg dir ) base , dirs , files = walker . next ( ) if 'EGG-INFO' in dirs : dirs . remove ( 'EGG-INFO' ) yield base , dirs , files for bdf in walker : yield bdf
def scan module ( egg dir , base , name , stubs ) : filename = os . path . join ( base , name ) if filename [ : - 1 ] in stubs : return True # Extension module pkg = base [ len ( egg dir ) + 1 : ] . replace ( os . sep , '.' ) module = pkg + ( pkg and '.' or '' ) + os . path . splitext ( name ) [ 0 ] if sys . version info < ( 3 , 3 ) : skip = 8 # skip magic & date else : skip = 12 # skip magic & date & file size f = open ( filename , 'rb' ) f . read ( skip ) code = marshal . load ( f ) f . close ( ) safe = True symbols = dict . fromkeys ( iter symbols ( code ) ) for bad in [ ' file ' , ' path ' ] : if bad in symbols : log . warn ( "%s: module references %s" , module , bad ) safe = False if 'inspect' in symbols : for bad in [ 'getsource' , 'getabsfile' , 'getsourcefile' , 'getfile' 'getsourcelines' , 'findsource' , 'getcomments' , 'getframeinfo' , 'getinnerframes' , 'getouterframes' , 'stack' , 'trace' ] : if bad in symbols : log . warn ( "%s: module MAY be using inspect.%s" , module , bad ) safe = False if ' name ' in symbols and ' main ' in symbols and '.' not in module : if sys . version [ : 3 ] == "2.4" : # -m works w/zipfiles in 2.5 log . warn ( "%s: top-level module may be 'python -m' script" , module ) safe = False return safe
def make init files ( self ) : init files = [ ] for base , dirs , files in walk egg ( self . bdist dir ) : if base == self . bdist dir : # don't put an  init  in the root continue for name in files : if name . endswith ( '.py' ) : if ' init .py' not in files : pkg = base [ len ( self . bdist dir ) + 1 : ] . replace ( os . sep , '.' ) if self . distribution . has contents for ( pkg ) : log . warn ( "Creating missing  init .py for %s" , pkg ) filename = os . path . join ( base , ' init .py' ) if not self . dry run : f = open ( filename , 'w' ) f . write ( NS PKG STUB ) f . close ( ) init files . append ( filename ) break else : # not a package, don't traverse to subdirectories dirs [ : ] = [ ] return init files
def launch new instance ( ) : if sys . platform == 'win32' : # make sure we don't get called from a multiprocessing subprocess # this can result in infinite Controllers being started on Windows # which doesn't have a proper fork, so multiprocessing is wonky # this only comes up when I Python has been installed using vanilla # setuptools, and *not* distribute. import multiprocessing p = multiprocessing . current process ( ) # the main process has name 'Main Process' # subprocesses will have names like 'Process-1' if p . name != 'Main Process' : # we are a subprocess, don't start another Controller! return app = IP Controller App . instance ( ) app . initialize ( ) app . start ( )
def save connection dict ( self , fname , cdict ) : c = self . config url = cdict [ 'url' ] location = cdict [ 'location' ] if not location : try : proto , ip , port = split url ( url ) except Assertion Error : pass else : try : location = socket . gethostbyname ex ( socket . gethostname ( ) ) [ 2 ] [ - 1 ] except ( socket . gaierror , Index Error ) : self . log . warn ( "Could not identify this machine's IP, assuming 127.0.0.1." " You may need to specify '--location=<external ip address>' to help" " I Python decide when to connect via loopback." ) location = '127.0.0.1' cdict [ 'location' ] = location fname = os . path . join ( self . profile dir . security dir , fname ) self . log . info ( "writing connection info to %s" , fname ) with open ( fname , 'w' ) as f : f . write ( json . dumps ( cdict , indent = 2 ) ) os . chmod ( fname , stat . S IRUSR | stat . S IWUSR )
def load config from json ( self ) : c = self . config self . log . debug ( "loading config from JSON" ) # load from engine config fname = os . path . join ( self . profile dir . security dir , self . engine json file ) self . log . info ( "loading connection info from %s" , fname ) with open ( fname ) as f : cfg = json . loads ( f . read ( ) ) key = cfg [ 'exec key' ] # json gives unicode, Session.key wants bytes c . Session . key = key . encode ( 'ascii' ) xport , addr = cfg [ 'url' ] . split ( '://' ) c . Hub Factory . engine transport = xport ip , ports = addr . split ( ':' ) c . Hub Factory . engine ip = ip c . Hub Factory . regport = int ( ports ) self . location = cfg [ 'location' ] if not self . engine ssh server : self . engine ssh server = cfg [ 'ssh' ] # load client config fname = os . path . join ( self . profile dir . security dir , self . client json file ) self . log . info ( "loading connection info from %s" , fname ) with open ( fname ) as f : cfg = json . loads ( f . read ( ) ) assert key == cfg [ 'exec key' ] , "exec key mismatch between engine and client keys" xport , addr = cfg [ 'url' ] . split ( '://' ) c . Hub Factory . client transport = xport ip , ports = addr . split ( ':' ) c . Hub Factory . client ip = ip if not self . ssh server : self . ssh server = cfg [ 'ssh' ] assert int ( ports ) == c . Hub Factory . regport , "regport mismatch"
def load secondary config ( self ) : if self . reuse files : try : self . load config from json ( ) except ( Assertion Error , IO Error ) as e : self . log . error ( "Could not load config from JSON: %s" % e ) else : # successfully loaded config from JSON, and reuse=True # no need to wite back the same file self . write connection files = False # switch Session.key default to secure default secure ( self . config ) self . log . debug ( "Config changed" ) self . log . debug ( repr ( self . config ) )
def script args ( f ) : args = [ magic arguments . argument ( '--out' , type = str , help = ) , magic arguments . argument ( '--err' , type = str , help = ) , magic arguments . argument ( '--bg' , action = "store true" , help = ) , magic arguments . argument ( '--proc' , type = str , help = ) , ] for arg in args : f = arg ( f ) return f
def parallel execute ( self , cell , block = None , groupby = 'type' , save name = None ) : # defaults: block = self . view . block if block is None else block base = "Parallel" if block else "Async parallel" targets = self . view . targets if isinstance ( targets , list ) and len ( targets ) > 10 : str targets = str ( targets [ : 4 ] ) [ : - 1 ] + ', ..., ' + str ( targets [ - 4 : ] ) [ 1 : ] else : str targets = str ( targets ) if self . verbose : print base + " execution on engine(s): %s" % str targets result = self . view . execute ( cell , silent = False , block = False ) self . last result = result if save name : self . shell . user ns [ save name ] = result if block : result . get ( ) result . display outputs ( groupby ) else : # return Async Result only on non-blocking submission return result
def disable autopx ( self ) : if self . autopx : self . shell . run cell = self . original run cell self . autopx = False print "%autopx disabled"
def run heartbeat ( message ) : then = arrow . get ( message [ 'time' ] ) now = arrow . get ( ) if ( now - then ) > timezone . timedelta ( seconds = ( TICK FREQ + 1 ) ) : pass # discard old ticks else : Task . run tasks ( )
def run task ( message ) : task = Task . objects . get ( pk = message [ 'id' ] ) if task . allow overlap : task . run ( message ) else : if not task . running : task . running = True task . save ( ) try : task . run ( message ) finally : task . running = False task . save ( )
def remove task ( message ) : task = Task . objects . get ( pk = message [ 'id' ] ) task . delete ( )
def patch if missing ( obj , name , method ) : setattr ( obj , name , getattr ( obj , name , method ) )
def accept connection ( self ) : assert self . pending , "Connection is not pending." self . server protocol = self . server . server factory . build Protocol ( None ) self . accept d . callback ( Fake Server Protocol Wrapper ( self , self . server protocol ) ) return self . await connected ( )
def reject connection ( self , reason = None ) : assert self . pending , "Connection is not pending." if reason is None : reason = Connection Refused Error ( ) self . accept d . errback ( reason )
def get agent ( self , reactor = None , context Factory = None ) : return Proxy Agent With Context ( self . endpoint , reactor = reactor , context Factory = context Factory )
def form valid ( self , form ) : self . object = form . save ( commit = False ) # Invoke pre save hook, and allow it to abort the saving # process and do a redirect. response = self . pre save ( self . object ) if response : return response self . object . save ( ) form . save m2m ( ) self . post save ( self . object ) return Http Response Redirect ( self . get success url ( ) )
def delete ( self , request , * args , * * kwargs ) : self . object = self . get object ( ) success url = self . get success url ( ) self . pre delete ( self . object ) self . object . delete ( ) self . post delete ( self . object ) return Http Response Redirect ( success url )
def pre save ( self , instance ) : super ( User View Mixin , self ) . pre save ( instance ) if self . request . user . is authenticated ( ) : for field in self . user field : setattr ( instance , field , self . request . user )
def check ( self , check all = False ) : if not self . enabled and not check all : return if check all or self . check all : modules = sys . modules . keys ( ) else : modules = self . modules . keys ( ) for modname in modules : m = sys . modules . get ( modname , None ) if modname in self . skip modules : continue if not hasattr ( m , ' file ' ) : continue if m . name == ' main ' : # we cannot reload( main ) continue filename = m . file path , ext = os . path . splitext ( filename ) if ext . lower ( ) == '.py' : ext = PY COMPILED EXT pyc filename = pyfile . cache from source ( filename ) py filename = filename else : pyc filename = filename try : py filename = pyfile . source from cache ( filename ) except Value Error : continue try : pymtime = os . stat ( py filename ) . st mtime if pymtime <= os . stat ( pyc filename ) . st mtime : continue if self . failed . get ( py filename , None ) == pymtime : continue except OS Error : continue try : superreload ( m , reload , self . old objects ) if py filename in self . failed : del self . failed [ py filename ] except : print >> sys . stderr , "[autoreload of %s failed: %s]" % ( modname , traceback . format exc ( 1 ) ) self . failed [ py filename ] = pymtime
def clipboard get ( self ) : from I Python . lib . clipboard import ( osx clipboard get , tkinter clipboard get , win32 clipboard get ) if sys . platform == 'win32' : chain = [ win32 clipboard get , tkinter clipboard get ] elif sys . platform == 'darwin' : chain = [ osx clipboard get , tkinter clipboard get ] else : chain = [ tkinter clipboard get ] dispatcher = Command Chain Dispatcher ( ) for func in chain : dispatcher . add ( func ) text = dispatcher ( ) return text
def add ( self , func , priority = 0 ) : self . chain . append ( ( priority , func ) ) self . chain . sort ( key = lambda x : x [ 0 ] )
def configure ( self , options , conf ) : self . conf = conf self . enabled = options . debug Errors or options . debug Failures self . enabled for errors = options . debug Errors self . enabled for failures = options . debug Failures
def import item ( name ) : package = '.' . join ( name . split ( '.' ) [ 0 : - 1 ] ) obj = name . split ( '.' ) [ - 1 ] # Note: the original code for this was the following.  We've left it # visible for now in case the new implementation shows any problems down # the road, to make it easier on anyone looking for a problem.  This code # should be removed once we're comfortable we didn't break anything. ## exec String = 'from %s import %s' % (package, obj) ## try: ##     exec exec String ## except Syntax Error: ##     raise Import Error("Invalid class specification: %s" % name) ## exec 'temp = %s' % obj ## return temp if package : module = import ( package , fromlist = [ obj ] ) try : pak = module . dict [ obj ] except Key Error : raise Import Error ( 'No module named %s' % obj ) return pak else : return import ( obj )
def try passwordless openssh ( server , keyfile ) : if pexpect is None : raise Import Error ( "pexpect unavailable, use paramiko" ) cmd = 'ssh -f ' + server if keyfile : cmd += ' -i ' + keyfile cmd += ' exit' p = pexpect . spawn ( cmd ) while True : try : p . expect ( '[Pp]assword:' , timeout = .1 ) except pexpect . TIMEOUT : continue except pexpect . EOF : return True else : return False
def try passwordless paramiko ( server , keyfile ) : if paramiko is None : msg = "Paramiko unavaliable, " if sys . platform == 'win32' : msg += "Paramiko is required for ssh tunneled connections on Windows." else : msg += "use Open SSH." raise Import Error ( msg ) username , server , port = split server ( server ) client = paramiko . SSH Client ( ) client . load system host keys ( ) client . set missing host key policy ( paramiko . Warning Policy ( ) ) try : client . connect ( server , port , username = username , key filename = keyfile , look for keys = True ) except paramiko . Authentication Exception : return False else : client . close ( ) return True
def unwrap exception ( self , content ) : e = error . unwrap exception ( content ) # print e.traceback if e . engine info : e uuid = e . engine info [ 'engine uuid' ] eid = self . engines [ e uuid ] e . engine info [ 'engine id' ] = eid return e
def register engine ( self , msg ) : content = msg [ 'content' ] eid = content [ 'id' ] d = { eid : content [ 'queue' ] } self . update engines ( d )
def unregister engine ( self , msg ) : content = msg [ 'content' ] eid = int ( content [ 'id' ] ) if eid in self . ids : self . ids . remove ( eid ) uuid = self . engines . pop ( eid ) self . handle stranded msgs ( eid , uuid ) if self . task socket and self . task scheme == 'pure' : self . stop scheduling tasks ( )
def flush results ( self , sock ) : idents , msg = self . session . recv ( sock , mode = zmq . NOBLOCK ) while msg is not None : if self . debug : pprint ( msg ) msg type = msg [ 'header' ] [ 'msg type' ] handler = self . queue handlers . get ( msg type , None ) if handler is None : raise Exception ( "Unhandled message type: %s" % msg . msg type ) else : handler ( msg ) idents , msg = self . session . recv ( sock , mode = zmq . NOBLOCK )
def flush ignored control ( self ) : while self . ignored control replies > 0 : self . session . recv ( self . control socket ) self . ignored control replies -= 1
def spin every ( self , interval = 1 ) : while True : if self . stop spinning . is set ( ) : return time . sleep ( interval ) self . spin ( )
def stop spin thread ( self ) : if self . spin thread is not None : self . stop spinning . set ( ) self . spin thread . join ( ) self . spin thread = None
def send execute request ( self , socket , code , silent = True , subheader = None , ident = None ) : if self . closed : raise Runtime Error ( "Client cannot be used after its sockets have been closed" ) # defaults: subheader = subheader if subheader is not None else { } # validate arguments if not isinstance ( code , basestring ) : raise Type Error ( "code must be text, not %s" % type ( code ) ) if not isinstance ( subheader , dict ) : raise Type Error ( "subheader must be dict, not %s" % type ( subheader ) ) content = dict ( code = code , silent = bool ( silent ) , user variables = [ ] , user expressions = { } ) msg = self . session . send ( socket , "execute request" , content = content , ident = ident , subheader = subheader ) msg id = msg [ 'header' ] [ 'msg id' ] self . outstanding . add ( msg id ) if ident : # possibly routed to a specific engine if isinstance ( ident , list ) : ident = ident [ - 1 ] if ident in self . engines . values ( ) : # save for later, in case of engine death self . outstanding dict [ ident ] . add ( msg id ) self . history . append ( msg id ) self . metadata [ msg id ] [ 'submitted' ] = datetime . now ( ) return msg
def opcode set ( * names ) : s = set ( ) for name in names : try : s . add ( opcode ( name ) ) except Key Error : pass return s
def get byte parser ( self ) : if not self . byte parser : self . byte parser = Byte Parser ( text = self . text , filename = self . filename ) return self . byte parser
def first line ( self , line ) : rng = self . multiline . get ( line ) if rng : first line = rng [ 0 ] else : first line = line return first line
def block stack repr ( self , block stack ) : blocks = ", " . join ( [ "(%s, %r)" % ( dis . opname [ b [ 0 ] ] , b [ 1 ] ) for b in block stack ] ) return "[" + blocks + "]"
def validate chunks ( self , chunks ) : # starts is the entrances to the chunks starts = set ( [ ch . byte for ch in chunks ] ) for ch in chunks : assert all ( [ ( ex in starts or ex < 0 ) for ex in ch . exits ] )
def options ( self , parser , env ) : super ( Coverage , self ) . options ( parser , env ) parser . add option ( "--cover-package" , action = "append" , default = env . get ( 'NOSE COVER PACKAGE' ) , metavar = "PACKAGE" , dest = "cover packages" , help = "Restrict coverage output to selected packages " "[NOSE COVER PACKAGE]" ) parser . add option ( "--cover-erase" , action = "store true" , default = env . get ( 'NOSE COVER ERASE' ) , dest = "cover erase" , help = "Erase previously collected coverage " "statistics before run" ) parser . add option ( "--cover-tests" , action = "store true" , dest = "cover tests" , default = env . get ( 'NOSE COVER TESTS' ) , help = "Include test modules in coverage report " "[NOSE COVER TESTS]" ) parser . add option ( "--cover-min-percentage" , action = "store" , dest = "cover min percentage" , default = env . get ( 'NOSE COVER MIN PERCENTAGE' ) , help = "Minimum percentage of coverage for tests" "to pass [NOSE COVER MIN PERCENTAGE]" ) parser . add option ( "--cover-inclusive" , action = "store true" , dest = "cover inclusive" , default = env . get ( 'NOSE COVER INCLUSIVE' ) , help = "Include all python files under working " "directory in coverage report.  Useful for " "discovering holes in test coverage if not all " "files are imported by the test suite. " "[NOSE COVER INCLUSIVE]" ) parser . add option ( "--cover-html" , action = "store true" , default = env . get ( 'NOSE COVER HTML' ) , dest = 'cover html' , help = "Produce HTML coverage information" ) parser . add option ( '--cover-html-dir' , action = 'store' , default = env . get ( 'NOSE COVER HTML DIR' , 'cover' ) , dest = 'cover html dir' , metavar = 'DIR' , help = 'Produce HTML coverage information in dir' ) parser . add option ( "--cover-branches" , action = "store true" , default = env . get ( 'NOSE COVER BRANCHES' ) , dest = "cover branches" , help = "Include branch coverage in coverage report " "[NOSE COVER BRANCHES]" ) parser . add option ( "--cover-xml" , action = "store true" , default = env . get ( 'NOSE COVER XML' ) , dest = "cover xml" , help = "Produce XML coverage information" ) parser . add option ( "--cover-xml-file" , action = "store" , default = env . get ( 'NOSE COVER XML FILE' , 'coverage.xml' ) , dest = "cover xml file" , metavar = "FILE" , help = "Produce XML coverage information in file" )
def begin ( self ) : log . debug ( "Coverage begin" ) self . skip Modules = sys . modules . keys ( ) [ : ] if self . cover Erase : log . debug ( "Clearing previously collected coverage statistics" ) self . cover Instance . combine ( ) self . cover Instance . erase ( ) self . cover Instance . exclude ( '#pragma[: ]+[n N][o O] [c C][o O][v V][e E][r R]' ) self . cover Instance . load ( ) self . cover Instance . start ( )
def report ( self , stream ) : log . debug ( "Coverage report" ) self . cover Instance . stop ( ) self . cover Instance . combine ( ) self . cover Instance . save ( ) modules = [ module for name , module in sys . modules . items ( ) if self . want Module Coverage ( name , module ) ] log . debug ( "Coverage report will cover modules: %s" , modules ) self . cover Instance . report ( modules , file = stream ) if self . cover Html Dir : log . debug ( "Generating HTML coverage report" ) self . cover Instance . html report ( modules , self . cover Html Dir ) if self . cover Xml File : log . debug ( "Generating XML coverage report" ) self . cover Instance . xml report ( modules , self . cover Xml File ) # make sure we have minimum required coverage if self . cover Min Percentage : f = String IO . String IO ( ) self . cover Instance . report ( modules , file = f ) m = re . search ( r'-------\s\w+\s+\d+\s+\d+\s+(\d+)%\s+\d*\s{0,1}$' , f . getvalue ( ) ) if m : percentage = int ( m . groups ( ) [ 0 ] ) if percentage < self . cover Min Percentage : log . error ( 'TOTAL Coverage did not reach minimum ' 'required: %d%%' % self . cover Min Percentage ) sys . exit ( 1 ) else : log . error ( "No total percentage was found in coverage output, " "something went wrong." )
def open with auth ( url ) : scheme , netloc , path , params , query , frag = urlparse . urlparse ( url ) # Double scheme does not raise on Mac OS X as revealed by a # failing test. We would expect "nonnumeric port". Refs #20. if netloc . endswith ( ':' ) : raise httplib . Invalid URL ( "nonnumeric port: ''" ) if scheme in ( 'http' , 'https' ) : auth , host = urllib2 . splituser ( netloc ) else : auth = None if auth : auth = "Basic " + encode auth ( auth ) new url = urlparse . urlunparse ( ( scheme , host , path , params , query , frag ) ) request = urllib2 . Request ( new url ) request . add header ( "Authorization" , auth ) else : request = urllib2 . Request ( url ) request . add header ( 'User-Agent' , user agent ) fp = urllib2 . urlopen ( request ) if auth : # Put authentication info back into request URL if same host, # so that links found on the page will work s2 , h2 , path2 , param2 , query2 , frag2 = urlparse . urlparse ( fp . url ) if s2 == scheme and h2 == host : fp . url = urlparse . urlunparse ( ( s2 , netloc , path2 , param2 , query2 , frag2 ) ) return fp
def get parent ( obj ) : names = obj . qualname . split ( '.' ) [ : - 1 ] if '<locals>' in names : # locals function raise Value Error ( 'cannot get parent from locals object.' ) module = sys . modules [ obj . module ] parent = module while names : parent = getattr ( parent , names . pop ( 0 ) ) return parent
def render template ( content , context ) : rendered = Template ( content ) . render ( Context ( context ) ) return rendered
def configure ( self , options , conf ) : self . conf = conf if not options . capture : self . enabled = False
def format Error ( self , test , err ) : test . captured Output = output = self . buffer self . buf = None if not output : # Don't return None as that will prevent other # formatters from formatting and remove earlier formatters # formats, instead return the err we got return err ec , ev , tb = err return ( ec , self . add Capture To Err ( ev , output ) , tb )
def split By ( data , num ) : return [ data [ i : i + num ] for i in range ( 0 , len ( data ) , num ) ]
def hex to rgb ( color ) : if color . startswith ( '#' ) : color = color [ 1 : ] if len ( color ) == 3 : color = '' . join ( [ c * 2 for c in color ] ) if len ( color ) != 6 : return False try : r = int ( color [ : 2 ] , 16 ) g = int ( color [ 2 : 4 ] , 16 ) b = int ( color [ 4 : ] , 16 ) except Value Error : return False else : return r , g , b
def handle complete reply ( self , rep ) : self . log . debug ( "complete: %s" , rep . get ( 'content' , '' ) ) cursor = self . get cursor ( ) info = self . request info . get ( 'complete' ) if info and info . id == rep [ 'parent header' ] [ 'msg id' ] and info . pos == cursor . position ( ) : matches = rep [ 'content' ] [ 'matches' ] text = rep [ 'content' ] [ 'matched text' ] offset = len ( text ) # Clean up matches with period and path separators if the matched # text has not been transformed. This is done by truncating all # but the last component and then suitably decreasing the offset # between the current cursor position and the start of completion. if len ( matches ) > 1 and matches [ 0 ] [ : offset ] == text : parts = re . split ( r'[./\\]' , text ) sep count = len ( parts ) - 1 if sep count : chop length = sum ( map ( len , parts [ : sep count ] ) ) + sep count matches = [ match [ chop length : ] for match in matches ] offset -= chop length # Move the cursor to the start of the match and complete. cursor . move Position ( Qt Gui . Q Text Cursor . Left , n = offset ) self . complete with items ( cursor , matches )
def handle execute reply ( self , msg ) : msg id = msg [ 'parent header' ] . get ( 'msg id' ) info = self . request info [ 'execute' ] . get ( msg id ) if info and info . kind == 'prompt' : number = msg [ 'content' ] [ 'execution count' ] + 1 self . show interpreter prompt ( number ) self . request info [ 'execute' ] . pop ( msg id ) else : super ( I Python Widget , self ) . handle execute reply ( msg )
def handle pyout ( self , msg ) : self . log . debug ( "pyout: %s" , msg . get ( 'content' , '' ) ) if not self . hidden and self . is from this session ( msg ) : content = msg [ 'content' ] prompt number = content . get ( 'execution count' , 0 ) data = content [ 'data' ] if data . has key ( 'text/html' ) : self . append plain text ( self . output sep , True ) self . append html ( self . make out prompt ( prompt number ) , True ) html = data [ 'text/html' ] self . append plain text ( '\n' , True ) self . append html ( html + self . output sep2 , True ) elif data . has key ( 'text/plain' ) : self . append plain text ( self . output sep , True ) self . append html ( self . make out prompt ( prompt number ) , True ) text = data [ 'text/plain' ] # If the repr is multiline, make sure we start on a new line, # so that its lines are aligned. if "\n" in text and not self . output sep . endswith ( "\n" ) : self . append plain text ( '\n' , True ) self . append plain text ( text + self . output sep2 , True )
def handle display data ( self , msg ) : self . log . debug ( "display: %s" , msg . get ( 'content' , '' ) ) # For now, we don't display data from other frontends, but we # eventually will as this allows all frontends to monitor the display # data. But we need to figure out how to handle this in the GUI. if not self . hidden and self . is from this session ( msg ) : source = msg [ 'content' ] [ 'source' ] data = msg [ 'content' ] [ 'data' ] metadata = msg [ 'content' ] [ 'metadata' ] # In the regular I Python Widget, we simply print the plain text # representation. if data . has key ( 'text/html' ) : html = data [ 'text/html' ] self . append html ( html , True ) elif data . has key ( 'text/plain' ) : text = data [ 'text/plain' ] self . append plain text ( text , True ) # This newline seems to be needed for text and html output. self . append plain text ( u'\n' , True )
def started channels ( self ) : super ( I Python Widget , self ) . started channels ( ) self . load guiref magic ( ) self . kernel manager . shell channel . history ( hist access type = 'tail' , n = 1000 )
def execute file ( self , path , hidden = False ) : # Use forward slashes on Windows to avoid escaping each separator. if sys . platform == 'win32' : path = os . path . normpath ( path ) . replace ( '\\' , '/' ) # Perhaps we should not be using %run directly, but while we # are, it is necessary to quote or escape filenames containing spaces  # or quotes.  # In earlier code here, to minimize escaping, we sometimes quoted the  # filename with single quotes. But to do this, this code must be # platform-aware, because run uses shlex rather than python string # parsing, so that: # * In Win: single quotes can be used in the filename without quoting, #   and we cannot use single quotes to quote the filename. # * In *nix: we can escape double quotes in a double quoted filename, #   but can't escape single quotes in a single quoted filename. # So to keep this code non-platform-specific and simple, we now only # use double quotes to quote filenames, and escape when needed: if ' ' in path or "'" in path or '"' in path : path = '"%s"' % path . replace ( '"' , '\\"' ) self . execute ( '%%run %s' % path , hidden = hidden )
def complete ( self ) : # We let the kernel split the input line, so we *always* send an empty # text field. Readline-based frontends do get a real text field which # they can use. text = '' # Send the completion request to the kernel msg id = self . kernel manager . shell channel . complete ( text , # text self . get input buffer cursor line ( ) , # line self . get input buffer cursor column ( ) , # cursor pos self . input buffer ) # block pos = self . get cursor ( ) . position ( ) info = self . Completion Request ( msg id , pos ) self . request info [ 'complete' ] = info
def process execute error ( self , msg ) : content = msg [ 'content' ] traceback = '\n' . join ( content [ 'traceback' ] ) + '\n' if False : # FIXME: For now, tracebacks come as plain text, so we can't use # the html renderer yet.  Once we refactor ultratb to produce # properly styled tracebacks, this branch should be the default traceback = traceback . replace ( ' ' , '&nbsp;' ) traceback = traceback . replace ( '\n' , '<br/>' ) ename = content [ 'ename' ] ename styled = '<span class="error">%s</span>' % ename traceback = traceback . replace ( ename , ename styled ) self . append html ( traceback ) else : # This is the fallback for now, using plain text with ansi escapes self . append plain text ( traceback )
def process execute payload ( self , item ) : handler = self . payload handlers . get ( item [ 'source' ] ) if handler is None : # We have no handler for this type of payload, simply ignore it return False else : handler ( item ) return True
def show interpreter prompt ( self , number = None ) : # If a number was not specified, make a prompt number request. if number is None : msg id = self . kernel manager . shell channel . execute ( '' , silent = True ) info = self . Execution Request ( msg id , 'prompt' ) self . request info [ 'execute' ] [ msg id ] = info return # Show a new prompt and save information about it so that it can be # updated later if the prompt number turns out to be wrong. self . prompt sep = self . input sep self . show prompt ( self . make in prompt ( number ) , html = True ) block = self . control . document ( ) . last Block ( ) length = len ( self . prompt ) self . previous prompt obj = self . Prompt Block ( block , length , number ) # Update continuation prompt to reflect (possibly) new prompt length. self . set continuation prompt ( self . make continuation prompt ( self . prompt ) , html = True )
def show interpreter prompt for reply ( self , msg ) : # Update the old prompt number if necessary. content = msg [ 'content' ] # abort replies do not have any keys: if content [ 'status' ] == 'aborted' : if self . previous prompt obj : previous prompt number = self . previous prompt obj . number else : previous prompt number = 0 else : previous prompt number = content [ 'execution count' ] if self . previous prompt obj and self . previous prompt obj . number != previous prompt number : block = self . previous prompt obj . block # Make sure the prompt block has not been erased. if block . is Valid ( ) and block . text ( ) : # Remove the old prompt and insert a new prompt. cursor = Qt Gui . Q Text Cursor ( block ) cursor . move Position ( Qt Gui . Q Text Cursor . Right , Qt Gui . Q Text Cursor . Keep Anchor , self . previous prompt obj . length ) prompt = self . make in prompt ( previous prompt number ) self . prompt = self . insert html fetching plain text ( cursor , prompt ) # When the HTML is inserted, Qt blows away the syntax # highlighting for the line, so we need to rehighlight it. self . highlighter . rehighlight Block ( cursor . block ( ) ) self . previous prompt obj = None # Show a new prompt with the kernel's estimated prompt number. self . show interpreter prompt ( previous prompt number + 1 )
def make in prompt ( self , number ) : try : body = self . in prompt % number except Type Error : # allow in prompt to leave out number, e.g. '>>> ' body = self . in prompt return '<span class="in-prompt">%s</span>' % body
def style sheet changed ( self ) : self . set Style Sheet ( self . style sheet ) if self . control is not None : self . control . document ( ) . set Default Style Sheet ( self . style sheet ) bg color = self . control . palette ( ) . window ( ) . color ( ) self . ansi processor . set background color ( bg color ) if self . page control is not None : self . page control . document ( ) . set Default Style Sheet ( self . style sheet )
def syntax style changed ( self ) : if self . highlighter is None : # ignore premature calls return if self . syntax style : self . highlighter . set style ( self . syntax style ) else : self . highlighter . set style sheet ( self . style sheet )
def virtual memory ( ) : mem = psutil bsd . get virtual mem ( ) total , free , active , inactive , wired , cached , buffers , shared = mem avail = inactive + cached + free used = active + wired + cached percent = usage percent ( ( total - avail ) , total , round = 1 ) return nt virtmem info ( total , avail , percent , used , free , active , inactive , buffers , cached , shared , wired )
def get system cpu times ( ) : user , nice , system , idle , irq = psutil bsd . get system cpu times ( ) return cputimes ntuple ( user , nice , system , idle , irq )
def get system per cpu times ( ) : ret = [ ] for cpu t in psutil bsd . get system per cpu times ( ) : user , nice , system , idle , irq = cpu t item = cputimes ntuple ( user , nice , system , idle , irq ) ret . append ( item ) return ret
def get process uids ( self ) : real , effective , saved = psutil bsd . get process uids ( self . pid ) return nt uids ( real , effective , saved )
def get process gids ( self ) : real , effective , saved = psutil bsd . get process gids ( self . pid ) return nt gids ( real , effective , saved )
def get memory info ( self ) : rss , vms = psutil bsd . get process memory info ( self . pid ) [ : 2 ] return nt meminfo ( rss , vms )
def get process threads ( self ) : rawlist = psutil bsd . get process threads ( self . pid ) retlist = [ ] for thread id , utime , stime in rawlist : ntuple = nt thread ( thread id , utime , stime ) retlist . append ( ntuple ) return retlist
def get open files ( self ) : # XXX - C implementation available on Free BSD >= 8 only # else fallback on lsof parser if hasattr ( psutil bsd , "get process open files" ) : rawlist = psutil bsd . get process open files ( self . pid ) return [ nt openfile ( path , fd ) for path , fd in rawlist ] else : lsof = psposix . Lsof Parser ( self . pid , self . process name ) return lsof . get process open files ( )
def num cpus darwin ( ) : p = subprocess . Popen ( [ 'sysctl' , '-n' , 'hw.ncpu' ] , stdout = subprocess . PIPE ) return p . stdout . read ( )
def fetchone ( self ) : self . check executed ( ) r = self . fetch row ( 1 ) if not r : self . warning check ( ) return None self . rownumber = self . rownumber + 1 return r [ 0 ]
def fetchall ( self ) : self . check executed ( ) r = self . fetch row ( 0 ) self . rownumber = self . rownumber + len ( r ) self . warning check ( ) return r
def connect ( com , peers , tree , pub url , root id ) : com . connect ( peers , tree , pub url , root id )
def reads json ( s , * * kwargs ) : nbf , minor , d = parse json ( s , * * kwargs ) if nbf == 1 : nb = v1 . to notebook json ( d , * * kwargs ) nb = v3 . convert to this nbformat ( nb , orig version = 1 ) elif nbf == 2 : nb = v2 . to notebook json ( d , * * kwargs ) nb = v3 . convert to this nbformat ( nb , orig version = 2 ) elif nbf == 3 : nb = v3 . to notebook json ( d , * * kwargs ) nb = v3 . convert to this nbformat ( nb , orig version = 3 , orig minor = minor ) else : raise NB Format Error ( 'Unsupported JSON nbformat version: %i' % nbf ) return nb
def reads py ( s , * * kwargs ) : nbf , nbm , s = parse py ( s , * * kwargs ) if nbf == 2 : nb = v2 . to notebook py ( s , * * kwargs ) elif nbf == 3 : nb = v3 . to notebook py ( s , * * kwargs ) else : raise NB Format Error ( 'Unsupported PY nbformat version: %i' % nbf ) return nb
def convert to metadata ( ) : import glob for fname in glob . glob ( '*.ipynb' ) : print ( 'Converting file:' , fname ) with open ( fname , 'r' ) as f : nb = read ( f , u'json' ) md = new metadata ( ) if u'name' in nb : md . name = nb . name del nb [ u'name' ] nb . metadata = md with open ( fname , 'w' ) as f : write ( nb , f , u'json' )
def want Function ( self , function ) : try : if hasattr ( function , 'compat func name' ) : funcname = function . compat func name else : funcname = function . name except Attribute Error : # not a function return False declared = getattr ( function , ' test ' , None ) if declared is not None : wanted = declared else : wanted = not funcname . startswith ( ' ' ) and self . matches ( funcname ) plug wants = self . plugins . want Function ( function ) if plug wants is not None : wanted = plug wants log . debug ( "want Function %s? %s" , function , wanted ) return wanted
def want Method ( self , method ) : try : method name = method . name except Attribute Error : # not a method return False if method name . startswith ( ' ' ) : # never collect 'private' methods return False declared = getattr ( method , ' test ' , None ) if declared is not None : wanted = declared else : wanted = self . matches ( method name ) plug wants = self . plugins . want Method ( method ) if plug wants is not None : wanted = plug wants log . debug ( "want Method %s? %s" , method , wanted ) return wanted
def list command pydb ( self , arg ) : filename , first , last = Old Pdb . parse list cmd ( self , arg ) if filename is not None : self . print list lines ( filename , first , last )
def do pdef ( self , arg ) : namespaces = [ ( 'Locals' , self . curframe . f locals ) , ( 'Globals' , self . curframe . f globals ) ] self . shell . find line magic ( 'pdef' ) ( arg , namespaces = namespaces )
def conversion factor ( from symbol , to symbol , date ) : from currency = Currency . objects . get ( symbol = from symbol ) try : from currency price = Currency Price . objects . get ( currency = from currency , date = date ) . mid price except Currency Price . Does Not Exist : print "Cannot fetch prices for %s on %s" % ( str ( from currency ) , str ( date ) ) return None to currency = Currency . objects . get ( symbol = to symbol ) try : to currency price = Currency Price . objects . get ( currency = to currency , date = date ) . mid price except Currency Price . Does Not Exist : print "Cannot fetch prices for %s on %s" % ( str ( to currency ) , str ( date ) ) return None return to currency price / from currency price
def convert currency ( from symbol , to symbol , value , date ) : if from symbol == to symbol : return value factor = conversion factor ( from symbol , to symbol , date ) if type ( value ) == float : output = value * float ( factor ) elif type ( value ) == Decimal : output = Decimal ( format ( value * factor , '.%sf' % str ( PRICE PRECISION ) ) ) elif type ( value ) in [ np . float16 , np . float32 , np . float64 , np . float128 , np . float ] : output = float ( value ) * float ( factor ) else : output = None return output
def compute return ( self , start date , end date , rate = "MID" ) : if rate not in [ "MID" , "ASK" , "BID" ] : raise Value Error ( "Unknown rate type (%s)- must be 'MID', 'ASK' or 'BID'" % str ( rate ) ) if end date <= start date : raise Value Error ( "End date must be on or after start date" ) df = self . generate dataframe ( start date = start date , end date = end date ) start price = df . ix [ start date ] [ rate ] end price = df . ix [ end date ] [ rate ] currency return = ( end price / start price ) - 1.0 return currency return
def write connection file ( self ) : if os . path . basename ( self . connection file ) == self . connection file : cf = os . path . join ( self . profile dir . security dir , self . connection file ) else : cf = self . connection file write connection file ( cf , ip = self . ip , key = self . session . key , shell port = self . shell port , stdin port = self . stdin port , hb port = self . hb port , iopub port = self . iopub port ) self . full connection file = cf
def init heartbeat ( self ) : # heartbeat doesn't share context, because it mustn't be blocked # by the GIL, which is accessed by libzmq when freeing zero-copy messages hb ctx = zmq . Context ( ) self . heartbeat = Heartbeat ( hb ctx , ( self . ip , self . hb port ) ) self . hb port = self . heartbeat . port self . log . debug ( "Heartbeat REP Channel on port: %i" % self . hb port ) self . heartbeat . start ( ) # Helper to make it easier to connect to an existing kernel. # set log-level to critical, to make sure it is output self . log . critical ( "To connect another client to this kernel, use:" )
def log connection info ( self ) : basename = os . path . basename ( self . connection file ) if basename == self . connection file or os . path . dirname ( self . connection file ) == self . profile dir . security dir : # use shortname tail = basename if self . profile != 'default' : tail += " --profile %s" % self . profile else : tail = self . connection file self . log . critical ( "--existing %s" , tail ) self . ports = dict ( shell = self . shell port , iopub = self . iopub port , stdin = self . stdin port , hb = self . hb port )
def init session ( self ) : default secure ( self . config ) self . session = Session ( config = self . config , username = u'kernel' )
def init io ( self ) : if self . outstream class : outstream factory = import item ( str ( self . outstream class ) ) sys . stdout = outstream factory ( self . session , self . iopub socket , u'stdout' ) sys . stderr = outstream factory ( self . session , self . iopub socket , u'stderr' ) if self . displayhook class : displayhook factory = import item ( str ( self . displayhook class ) ) sys . displayhook = displayhook factory ( self . session , self . iopub socket )
def init kernel ( self ) : kernel factory = import item ( str ( self . kernel class ) ) self . kernel = kernel factory ( config = self . config , session = self . session , shell socket = self . shell socket , iopub socket = self . iopub socket , stdin socket = self . stdin socket , log = self . log ) self . kernel . record ports ( self . ports )
def init connector ( self ) : self . using ssh = bool ( self . sshkey or self . sshserver ) if self . sshkey and not self . sshserver : # We are using ssh directly to the controller, tunneling localhost to localhost self . sshserver = self . url . split ( '://' ) [ 1 ] . split ( ':' ) [ 0 ] if self . using ssh : if tunnel . try passwordless ssh ( self . sshserver , self . sshkey , self . paramiko ) : password = False else : password = getpass ( "SSH Password for %s: " % self . sshserver ) else : password = False def connect ( s , url ) : url = disambiguate url ( url , self . location ) if self . using ssh : self . log . debug ( "Tunneling connection to %s via %s" % ( url , self . sshserver ) ) return tunnel . tunnel connection ( s , url , self . sshserver , keyfile = self . sshkey , paramiko = self . paramiko , password = password , ) else : return s . connect ( url ) def maybe tunnel ( url ) : """like connect, but don't complete the connection (for use by heartbeat)""" url = disambiguate url ( url , self . location ) if self . using ssh : self . log . debug ( "Tunneling connection to %s via %s" % ( url , self . sshserver ) ) url , tunnelobj = tunnel . open tunnel ( url , self . sshserver , keyfile = self . sshkey , paramiko = self . paramiko , password = password , ) return url return connect , maybe tunnel
def html to text ( content ) : text = None h2t = html2text . HTML2Text ( ) h2t . ignore links = False text = h2t . handle ( content ) return text
def md to text ( content ) : text = None html = markdown . markdown ( content ) if html : text = html to text ( content ) return text
def domain to fqdn ( domain , proto = None ) : from . generic import get site proto proto = proto or get site proto ( ) fdqn = '{proto}://{domain}' . format ( proto = proto , domain = domain ) return fdqn
def options ( self , parser , env = os . environ ) : super ( Nose Exclude , self ) . options ( parser , env ) env dirs = [ ] if 'NOSE EXCLUDE DIRS' in env : exclude dirs = env . get ( 'NOSE EXCLUDE DIRS' , '' ) env dirs . extend ( exclude dirs . split ( ';' ) ) parser . add option ( "--exclude-dir" , action = "append" , dest = "exclude dirs" , default = env dirs , help = ) parser . add option ( "--exclude-dir-file" , type = "string" , dest = "exclude dir file" , default = env . get ( 'NOSE EXCLUDE DIRS FILE' , False ) , help = )
def configure ( self , options , conf ) : super ( Nose Exclude , self ) . configure ( options , conf ) self . exclude dirs = { } # preload directories from file if options . exclude dir file : if not options . exclude dirs : options . exclude dirs = [ ] new dirs = self . load from file ( options . exclude dir file ) options . exclude dirs . extend ( new dirs ) if not options . exclude dirs : self . enabled = False return self . enabled = True root = os . getcwd ( ) log . debug ( 'cwd: %s' % root ) # Normalize excluded directory names for lookup for exclude param in options . exclude dirs : # when using setup.cfg, you can specify only one 'exclude-dir' # separated by some character (new line is good enough) for d in exclude param . split ( '\n' ) : d = d . strip ( ) abs d = self . force to abspath ( d ) if abs d : self . exclude dirs [ abs d ] = True exclude str = "excluding dirs: %s" % "," . join ( self . exclude dirs . keys ( ) ) log . debug ( exclude str )
def want Directory ( self , dirname ) : if dirname in self . exclude dirs : log . debug ( "excluded: %s" % dirname ) return False else : return None
def links to dynamic ( self , ext ) : # XXX this should check to ensure the lib is actually being built # XXX as dynamic, and not just using a locally-found version or a # XXX static-compiled version libnames = dict . fromkeys ( [ lib . full name for lib in self . shlibs ] ) pkg = '.' . join ( ext . full name . split ( '.' ) [ : - 1 ] + [ '' ] ) for libname in ext . libraries : if pkg + libname in libnames : return True return False
def append func ( self , func , * args , * * kwargs ) : wraped func = partial ( func , * args , * * kwargs ) self . append ( wraped func )
def insert func ( self , index , func , * args , * * kwargs ) : wraped func = partial ( func , * args , * * kwargs ) self . insert ( index , wraped func )
def construct parser ( magic func ) : kwds = getattr ( magic func , 'argcmd kwds' , { } ) if 'description' not in kwds : kwds [ 'description' ] = getattr ( magic func , ' doc ' , None ) arg name = real name ( magic func ) parser = Magic Argument Parser ( arg name , * * kwds ) # Reverse the list of decorators in order to apply them in the # order in which they appear in the source. group = None for deco in magic func . decorators [ : : - 1 ] : result = deco . add to parser ( parser , group ) if result is not None : group = result # Replace the starting 'usage: ' with I Python's %. help text = parser . format help ( ) if help text . startswith ( 'usage: ' ) : help text = help text . replace ( 'usage: ' , '%' , 1 ) else : help text = '%' + help text # Replace the magic function's docstring with the full help text. magic func . doc = help text return parser
def real name ( magic func ) : magic name = magic func . name if magic name . startswith ( 'magic ' ) : magic name = magic name [ len ( 'magic ' ) : ] return getattr ( magic func , 'argcmd name' , magic name )
def add to parser ( self , parser , group ) : if group is not None : parser = group parser . add argument ( * self . args , * * self . kwds ) return None
def add to parser ( self , parser , group ) : return parser . add argument group ( * self . args , * * self . kwds )
def highlight Block ( self , string ) : if not self . highlighting on : return # The input to this function is a unicode string that may contain # paragraph break characters, non-breaking spaces, etc. Here we acquire # the string as plain text so we can compare it. current block = self . current Block ( ) string = self . frontend . get block plain text ( current block ) # Decide whether to check for the regular or continuation prompt. if current block . contains ( self . frontend . prompt pos ) : prompt = self . frontend . prompt else : prompt = self . frontend . continuation prompt # Only highlight if we can identify a prompt, but make sure not to # highlight the prompt. if string . startswith ( prompt ) : self . current offset = len ( prompt ) string = string [ len ( prompt ) : ] super ( Frontend Highlighter , self ) . highlight Block ( string )
def rehighlight Block ( self , block ) : old = self . highlighting on self . highlighting on = True super ( Frontend Highlighter , self ) . rehighlight Block ( block ) self . highlighting on = old
def set Format ( self , start , count , format ) : start += self . current offset super ( Frontend Highlighter , self ) . set Format ( start , count , format )
def copy ( self ) : if self . page control is not None and self . page control . has Focus ( ) : self . page control . copy ( ) elif self . control . has Focus ( ) : text = self . control . text Cursor ( ) . selection ( ) . to Plain Text ( ) if text : lines = map ( self . transform prompt , text . splitlines ( ) ) text = '\n' . join ( lines ) Qt Gui . Q Application . clipboard ( ) . set Text ( text ) else : self . log . debug ( "frontend widget : unknown copy target" )
def context menu make ( self , pos ) : menu = super ( Frontend Widget , self ) . context menu make ( pos ) for before action in menu . actions ( ) : if before action . shortcut ( ) . matches ( Qt Gui . Q Key Sequence . Paste ) == Qt Gui . Q Key Sequence . Exact Match : menu . insert Action ( before action , self . copy raw action ) break return menu
def event filter console keypress ( self , event ) : key = event . key ( ) if self . control key down ( event . modifiers ( ) , include command = False ) : if key == Qt Core . Qt . Key C and self . executing : self . request interrupt kernel ( ) return True elif key == Qt Core . Qt . Key Period : self . request restart kernel ( ) return True elif not event . modifiers ( ) & Qt Core . Qt . Alt Modifier : # Smart backspace: remove four characters in one backspace if: # 1) everything left of the cursor is whitespace # 2) the four characters immediately left of the cursor are spaces if key == Qt Core . Qt . Key Backspace : col = self . get input buffer cursor column ( ) cursor = self . control . text Cursor ( ) if col > 3 and not cursor . has Selection ( ) : text = self . get input buffer cursor line ( ) [ : col ] if text . endswith ( '    ' ) and not text . strip ( ) : cursor . move Position ( Qt Gui . Q Text Cursor . Left , Qt Gui . Q Text Cursor . Keep Anchor , 4 ) cursor . remove Selected Text ( ) return True return super ( Frontend Widget , self ) . event filter console keypress ( event )
def insert continuation prompt ( self , cursor ) : super ( Frontend Widget , self ) . insert continuation prompt ( cursor ) cursor . insert Text ( ' ' * self . input splitter . indent spaces )
def handle complete reply ( self , rep ) : self . log . debug ( "complete: %s" , rep . get ( 'content' , '' ) ) cursor = self . get cursor ( ) info = self . request info . get ( 'complete' ) if info and info . id == rep [ 'parent header' ] [ 'msg id' ] and info . pos == cursor . position ( ) : text = '.' . join ( self . get context ( ) ) cursor . move Position ( Qt Gui . Q Text Cursor . Left , n = len ( text ) ) self . complete with items ( cursor , rep [ 'content' ] [ 'matches' ] )
def handle execute reply ( self , msg ) : self . log . debug ( "execute: %s" , msg . get ( 'content' , '' ) ) msg id = msg [ 'parent header' ] [ 'msg id' ] info = self . request info [ 'execute' ] . get ( msg id ) # unset reading flag, because if execute finished, raw input can't # still be pending. self . reading = False if info and info . kind == 'user' and not self . hidden : # Make sure that all output from the SUB channel has been processed # before writing a new prompt. self . kernel manager . sub channel . flush ( ) # Reset the ANSI style information to prevent bad text in stdout # from messing up our colors. We're not a true terminal so we're # allowed to do this. if self . ansi codes : self . ansi processor . reset sgr ( ) content = msg [ 'content' ] status = content [ 'status' ] if status == 'ok' : self . process execute ok ( msg ) elif status == 'error' : self . process execute error ( msg ) elif status == 'aborted' : self . process execute abort ( msg ) self . show interpreter prompt for reply ( msg ) self . executed . emit ( msg ) self . request info [ 'execute' ] . pop ( msg id ) elif info and info . kind == 'silent exec callback' and not self . hidden : self . handle exec callback ( msg ) self . request info [ 'execute' ] . pop ( msg id ) else : super ( Frontend Widget , self ) . handle execute reply ( msg )
def handle input request ( self , msg ) : self . log . debug ( "input: %s" , msg . get ( 'content' , '' ) ) if self . hidden : raise Runtime Error ( 'Request for raw input during hidden execution.' ) # Make sure that all output from the SUB channel has been processed # before entering readline mode. self . kernel manager . sub channel . flush ( ) def callback ( line ) : self . kernel manager . stdin channel . input ( line ) if self . reading : self . log . debug ( "Got second input request, assuming first was interrupted." ) self . reading = False self . readline ( msg [ 'content' ] [ 'prompt' ] , callback = callback )
def handle kernel died ( self , since last heartbeat ) : self . log . debug ( "kernel died: %s" , since last heartbeat ) if self . custom restart : self . custom restart kernel died . emit ( since last heartbeat ) else : message = 'The kernel heartbeat has been inactive for %.2f ' 'seconds. Do you want to restart the kernel? You may ' 'first want to check the network connection.' % since last heartbeat self . restart kernel ( message , now = True )
def handle object info reply ( self , rep ) : self . log . debug ( "oinfo: %s" , rep . get ( 'content' , '' ) ) cursor = self . get cursor ( ) info = self . request info . get ( 'call tip' ) if info and info . id == rep [ 'parent header' ] [ 'msg id' ] and info . pos == cursor . position ( ) : # Get the information for a call tip.  For now we format the call # line as string, later we can pass False to format call and # syntax-highlight it ourselves for nicer formatting in the # calltip. content = rep [ 'content' ] # if this is from pykernel, 'docstring' will be the only key if content . get ( 'ismagic' , False ) : # Don't generate a call-tip for magics. Ideally, we should # generate a tooltip, but not on ( like we do for actual # callables. call info , doc = None , None else : call info , doc = call tip ( content , format call = True ) if call info or doc : self . call tip widget . show call info ( call info , doc )
def handle pyout ( self , msg ) : self . log . debug ( "pyout: %s" , msg . get ( 'content' , '' ) ) if not self . hidden and self . is from this session ( msg ) : text = msg [ 'content' ] [ 'data' ] self . append plain text ( text + '\n' , before prompt = True )
def handle stream ( self , msg ) : self . log . debug ( "stream: %s" , msg . get ( 'content' , '' ) ) if not self . hidden and self . is from this session ( msg ) : # Most consoles treat tabs as being 8 space characters. Convert tabs # to spaces so that output looks as expected regardless of this # widget's tab width. text = msg [ 'content' ] [ 'data' ] . expandtabs ( 8 ) self . append plain text ( text , before prompt = True ) self . control . move Cursor ( Qt Gui . Q Text Cursor . End )
def handle shutdown reply ( self , msg ) : self . log . debug ( "shutdown: %s" , msg . get ( 'content' , '' ) ) if not self . hidden and not self . is from this session ( msg ) : if self . local kernel : if not msg [ 'content' ] [ 'restart' ] : self . exit requested . emit ( self ) else : # we just got notified of a restart! time . sleep ( 0.25 ) # wait 1/4 sec to reset # lest the request for a new prompt # goes to the old kernel self . reset ( ) else : # remote kernel, prompt on Kernel shutdown/reset title = self . window ( ) . window Title ( ) if not msg [ 'content' ] [ 'restart' ] : reply = Qt Gui . Q Message Box . question ( self , title , "Kernel has been shutdown permanently. " "Close the Console?" , Qt Gui . Q Message Box . Yes , Qt Gui . Q Message Box . No ) if reply == Qt Gui . Q Message Box . Yes : self . exit requested . emit ( self ) else : # XXX: remove message box in favor of using the # clear on kernel restart setting? reply = Qt Gui . Q Message Box . question ( self , title , "Kernel has been reset. Clear the Console?" , Qt Gui . Q Message Box . Yes , Qt Gui . Q Message Box . No ) if reply == Qt Gui . Q Message Box . Yes : time . sleep ( 0.25 ) # wait 1/4 sec to reset # lest the request for a new prompt # goes to the old kernel self . reset ( )
def restart kernel ( self , message , now = False ) : # FIXME: now should be configurable via a checkbox in the dialog.  Right # now at least the heartbeat path sets it to True and the manual restart # to False.  But those should just be the pre-selected states of a # checkbox that the user could override if so desired.  But I don't know # enough Qt to go implementing the checkbox now. if self . custom restart : self . custom restart requested . emit ( ) elif self . kernel manager . has kernel : # Pause the heart beat channel to prevent further warnings. self . kernel manager . hb channel . pause ( ) # Prompt the user to restart the kernel. Un-pause the heartbeat if # they decline. (If they accept, the heartbeat will be un-paused # automatically when the kernel is restarted.) if self . confirm restart : buttons = Qt Gui . Q Message Box . Yes | Qt Gui . Q Message Box . No result = Qt Gui . Q Message Box . question ( self , 'Restart kernel?' , message , buttons ) do restart = result == Qt Gui . Q Message Box . Yes else : # confirm restart is False, so we don't need to ask user # anything, just do the restart do restart = True if do restart : try : self . kernel manager . restart kernel ( now = now ) except Runtime Error : self . append plain text ( 'Kernel started externally. ' 'Cannot restart.\n' , before prompt = True ) else : self . reset ( ) else : self . kernel manager . hb channel . unpause ( ) else : self . append plain text ( 'Kernel process is either remote or ' 'unspecified. Cannot restart.\n' , before prompt = True )
def call tip ( self ) : # Decide if it makes sense to show a call tip if not self . enable calltips : return False cursor = self . get cursor ( ) cursor . move Position ( Qt Gui . Q Text Cursor . Left ) if cursor . document ( ) . character At ( cursor . position ( ) ) != '(' : return False context = self . get context ( cursor ) if not context : return False # Send the metadata request to the kernel name = '.' . join ( context ) msg id = self . kernel manager . shell channel . object info ( name ) pos = self . get cursor ( ) . position ( ) self . request info [ 'call tip' ] = self . Call Tip Request ( msg id , pos ) return True
def complete ( self ) : context = self . get context ( ) if context : # Send the completion request to the kernel msg id = self . kernel manager . shell channel . complete ( '.' . join ( context ) , # text self . get input buffer cursor line ( ) , # line self . get input buffer cursor column ( ) , # cursor pos self . input buffer ) # block pos = self . get cursor ( ) . position ( ) info = self . Completion Request ( msg id , pos ) self . request info [ 'complete' ] = info
def process execute error ( self , msg ) : content = msg [ 'content' ] # If a System Exit is passed along, this means exit() was called - also # all the ipython %exit magic syntax of '-k' to be used to keep # the kernel running if content [ 'ename' ] == 'System Exit' : keepkernel = content [ 'evalue' ] == '-k' or content [ 'evalue' ] == 'True' self . keep kernel on exit = keepkernel self . exit requested . emit ( self ) else : traceback = '' . join ( content [ 'traceback' ] ) self . append plain text ( traceback )
def process execute ok ( self , msg ) : payload = msg [ 'content' ] [ 'payload' ] for item in payload : if not self . process execute payload ( item ) : warning = 'Warning: received unknown payload of type %s' print ( warning % repr ( item [ 'source' ] ) )
def generate ( self , * arg , * * kw ) : for p , meth in self . plugins : result = None try : result = meth ( * arg , * * kw ) if result is not None : for r in result : yield r except ( Keyboard Interrupt , System Exit ) : raise except : exc = sys . exc info ( ) yield Failure ( * exc ) continue
def simple ( self , * arg , * * kw ) : for p , meth in self . plugins : result = meth ( * arg , * * kw ) if result is not None : return result
def load Plugins ( self ) : from pkg resources import iter entry points loaded = { } for entry point , adapt in self . entry points : for ep in iter entry points ( entry point ) : if ep . name in loaded : continue loaded [ ep . name ] = True log . debug ( '%s load plugin %s' , self . class . name , ep ) try : plugcls = ep . load ( ) except Keyboard Interrupt : raise except Exception , e : # never want a plugin load to kill the test run # but we can't log here because the logger is not yet # configured warn ( "Unable to load plugin %s: %s" % ( ep , e ) , Runtime Warning ) continue if adapt : plug = adapt ( plugcls ( ) ) else : plug = plugcls ( ) self . add Plugin ( plug ) super ( Entry Point Plugin Manager , self ) . load Plugins ( )
def load Plugins ( self ) : from nose . plugins import builtin for plug in builtin . plugins : self . add Plugin ( plug ( ) ) super ( Builtin Plugin Manager , self ) . load Plugins ( )
def cleanup files ( self , bundle = False ) : logger . notify ( 'Cleaning up...' ) logger . indent += 2 for req in self . reqs to cleanup : req . remove temporary source ( ) remove dir = [ ] if self . pip has created build dir ( ) : remove dir . append ( self . build dir ) # The source dir of a bundle can always be removed. # FIXME: not if it pre-existed the bundle! if bundle : remove dir . append ( self . src dir ) for dir in remove dir : if os . path . exists ( dir ) : logger . info ( 'Removing temporary dir %s...' % dir ) rmtree ( dir ) logger . indent -= 2
def name ( self ) : name = self . platform impl . get process name ( ) if os . name == 'posix' : # On UNIX the name gets truncated to the first 15 characters. # If it matches the first part of the cmdline we return that # one instead because it's usually more explicative. # Examples are "gnome-keyring-d" vs. "gnome-keyring-daemon". try : cmdline = self . cmdline except Access Denied : pass else : if cmdline : extended name = os . path . basename ( cmdline [ 0 ] ) if extended name . startswith ( name ) : name = extended name # XXX - perhaps needs refactoring self . platform impl . process name = name return name
def exe ( self ) : def guess it ( fallback ) : # try to guess exe from cmdline[0] in absence of a native # exe representation cmdline = self . cmdline if cmdline and hasattr ( os , 'access' ) and hasattr ( os , 'X OK' ) : exe = cmdline [ 0 ] # the possible exe rexe = os . path . realpath ( exe ) # ...in case it's a symlink if os . path . isabs ( rexe ) and os . path . isfile ( rexe ) and os . access ( rexe , os . X OK ) : return exe if isinstance ( fallback , Access Denied ) : raise fallback return fallback try : exe = self . platform impl . get process exe ( ) except Access Denied : err = sys . exc info ( ) [ 1 ] return guess it ( fallback = err ) else : if not exe : # underlying implementation can legitimately return an # empty string; if that's the case we don't want to # raise AD while guessing from the cmdline try : exe = guess it ( fallback = exe ) except Access Denied : pass return exe
def is running ( self ) : if self . gone : return False try : # Checking if pid is alive is not enough as the pid might # have been reused by another process. # pid + creation time, on the other hand, is supposed to # identify a process univocally. return self . create time == self . platform impl . get process create time ( ) except No Such Process : self . gone = True return False
def suspend ( self ) : # safety measure in case the current process has been killed in # meantime and the kernel reused its PID if not self . is running ( ) : name = self . platform impl . process name raise No Such Process ( self . pid , name ) # windows if hasattr ( self . platform impl , "suspend process" ) : self . platform impl . suspend process ( ) else : # posix self . send signal ( signal . SIGSTOP )
def resume ( self ) : # safety measure in case the current process has been killed in # meantime and the kernel reused its PID if not self . is running ( ) : name = self . platform impl . process name raise No Such Process ( self . pid , name ) # windows if hasattr ( self . platform impl , "resume process" ) : self . platform impl . resume process ( ) else : # posix self . send signal ( signal . SIGCONT )
def kill ( self ) : # safety measure in case the current process has been killed in # meantime and the kernel reused its PID if not self . is running ( ) : name = self . platform impl . process name raise No Such Process ( self . pid , name ) if os . name == 'posix' : self . send signal ( signal . SIGKILL ) else : self . platform impl . kill process ( )
def init transformers ( self ) : self . transformers = [ ] for transformer cls in default transformers : transformer cls ( shell = self . shell , prefilter manager = self , config = self . config )
def register transformer ( self , transformer ) : if transformer not in self . transformers : self . transformers . append ( transformer ) self . sort transformers ( )
def unregister transformer ( self , transformer ) : if transformer in self . transformers : self . transformers . remove ( transformer )
def init checkers ( self ) : self . checkers = [ ] for checker in default checkers : checker ( shell = self . shell , prefilter manager = self , config = self . config )
def register checker ( self , checker ) : if checker not in self . checkers : self . checkers . append ( checker ) self . sort checkers ( )
def unregister checker ( self , checker ) : if checker in self . checkers : self . checkers . remove ( checker )
def init handlers ( self ) : self . handlers = { } self . esc handlers = { } for handler in default handlers : handler ( shell = self . shell , prefilter manager = self , config = self . config )
def register handler ( self , name , handler , esc strings ) : self . handlers [ name ] = handler for esc str in esc strings : self . esc handlers [ esc str ] = handler
def unregister handler ( self , name , handler , esc strings ) : try : del self . handlers [ name ] except Key Error : pass for esc str in esc strings : h = self . esc handlers . get ( esc str ) if h is handler : del self . esc handlers [ esc str ]
def find handler ( self , line info ) : for checker in self . checkers : if checker . enabled : handler = checker . check ( line info ) if handler : return handler return self . get handler by name ( 'normal' )
def transform line ( self , line , continue prompt ) : for transformer in self . transformers : if transformer . enabled : line = transformer . transform ( line , continue prompt ) return line
def check ( self , line info ) : obj = self . shell . user ns . get ( line info . ifun , None ) if isinstance ( obj , I Py Autocall ) : obj . set ip ( self . shell ) return self . prefilter manager . get handler by name ( 'auto' ) else : return None
def check ( self , line info ) : # Note that this one of the only places we check the first character of # ifun and *not* the pre char.  Also note that the below test matches # both ! and !!. if line info . continue prompt and self . prefilter manager . multi line specials : if line info . esc == ESC MAGIC : return self . prefilter manager . get handler by name ( 'magic' ) else : return None
def check ( self , line info ) : # Note: aliases can not contain '.' head = line info . ifun . split ( '.' , 1 ) [ 0 ] if line info . ifun not in self . shell . alias manager or head not in self . shell . alias manager or is shadowed ( head , self . shell ) : return None return self . prefilter manager . get handler by name ( 'alias' )
def handle ( self , line info ) : # print "normal: ", line info # With autoindent on, we need some way to exit the input loop, and I # don't want to force the user to have to backspace all the way to # clear the line.  The rule will be in this case, that either two # lines of pure whitespace in a row, or a line of pure whitespace but # of a size different to the indent level, will exit the input loop. line = line info . line continue prompt = line info . continue prompt if ( continue prompt and self . shell . autoindent and line . isspace ( ) and 0 < abs ( len ( line ) - self . shell . indent current nsp ) <= 2 ) : line = '' return line
def handle ( self , line info ) : transformed = self . shell . alias manager . expand aliases ( line info . ifun , line info . the rest ) # pre is needed, because it carries the leading whitespace.  Otherwise # aliases won't work in indented sections. line out = '%sget ipython().system(%r)' % ( line info . pre whitespace , transformed ) return line out
def handle ( self , line info ) : magic handler = self . prefilter manager . get handler by name ( 'magic' ) line = line info . line if line . lstrip ( ) . startswith ( ESC SH CAP ) : # rewrite Line Info's line, ifun and the rest to properly hold the # call to %sx and the actual command to be executed, so # handle magic can work correctly.  Note that this works even if # the line is indented, so it handles multi line specials # properly. new rest = line . lstrip ( ) [ 2 : ] line info . line = '%ssx %s' % ( ESC MAGIC , new rest ) line info . ifun = 'sx' line info . the rest = new rest return magic handler . handle ( line info ) else : cmd = line . lstrip ( ) . lstrip ( ESC SHELL ) line out = '%sget ipython().system(%r)' % ( line info . pre whitespace , cmd ) return line out
def handle ( self , line info ) : ifun = line info . ifun the rest = line info . the rest cmd = '%sget ipython().magic(%r)' % ( line info . pre whitespace , ( ifun + " " + the rest ) ) return cmd
def handle ( self , line info ) : line = line info . line ifun = line info . ifun the rest = line info . the rest pre = line info . pre esc = line info . esc continue prompt = line info . continue prompt obj = line info . ofind ( self . shell ) [ 'obj' ] #print 'pre <%s> ifun <%s> rest <%s>' % (pre,ifun,the rest)  # dbg # This should only be active for single-line input! if continue prompt : return line force auto = isinstance ( obj , I Py Autocall ) # User objects sometimes raise exceptions on attribute access other # than Attribute Error (we've seen it in the past), so it's safest to be # ultra-conservative here and catch all. try : auto rewrite = obj . rewrite except Exception : auto rewrite = True if esc == ESC QUOTE : # Auto-quote splitting on whitespace newcmd = '%s("%s")' % ( ifun , '", "' . join ( the rest . split ( ) ) ) elif esc == ESC QUOTE2 : # Auto-quote whole string newcmd = '%s("%s")' % ( ifun , the rest ) elif esc == ESC PAREN : newcmd = '%s(%s)' % ( ifun , "," . join ( the rest . split ( ) ) ) else : # Auto-paren.        if force auto : # Don't rewrite if it is already a call. do rewrite = not the rest . startswith ( '(' ) else : if not the rest : # We only apply it to argument-less calls if the autocall # parameter is set to 2. do rewrite = ( self . shell . autocall >= 2 ) elif the rest . startswith ( '[' ) and hasattr ( obj , ' getitem ' ) : # Don't autocall in this case: item access for an object # which is BOTH callable and implements  getitem . do rewrite = False else : do rewrite = True # Figure out the rewritten command if do rewrite : if the rest . endswith ( ';' ) : newcmd = '%s(%s);' % ( ifun . rstrip ( ) , the rest [ : - 1 ] ) else : newcmd = '%s(%s)' % ( ifun . rstrip ( ) , the rest ) else : normal handler = self . prefilter manager . get handler by name ( 'normal' ) return normal handler . handle ( line info ) # Display the rewritten call if auto rewrite : self . shell . auto rewrite input ( newcmd ) return newcmd
def enter Event ( self , event ) : super ( Call Tip Widget , self ) . enter Event ( event ) self . hide timer . stop ( )
def paint Event ( self , event ) : painter = Qt Gui . Q Style Painter ( self ) option = Qt Gui . Q Style Option Frame ( ) option . init From ( self ) painter . draw Primitive ( Qt Gui . Q Style . PE Panel Tip Label , option ) painter . end ( ) super ( Call Tip Widget , self ) . paint Event ( event )
def show tip ( self , tip ) : # Attempt to find the cursor position at which to show the call tip. text edit = self . text edit document = text edit . document ( ) cursor = text edit . text Cursor ( ) search pos = cursor . position ( ) - 1 self . start position , = self . find parenthesis ( search pos , forward = False ) if self . start position == - 1 : return False # Set the text and resize the widget accordingly. self . set Text ( tip ) self . resize ( self . size Hint ( ) ) # Locate and show the widget. Place the tip below the current line # unless it would be off the screen.  In that case, decide the best # location based trying to minimize the  area that goes off-screen. padding = 3 # Distance in pixels between cursor bounds and tip box. cursor rect = text edit . cursor Rect ( cursor ) screen rect = Qt Gui . q App . desktop ( ) . screen Geometry ( text edit ) point = text edit . map To Global ( cursor rect . bottom Right ( ) ) point . set Y ( point . y ( ) + padding ) tip height = self . size ( ) . height ( ) tip width = self . size ( ) . width ( ) vertical = 'bottom' horizontal = 'Right' if point . y ( ) + tip height > screen rect . height ( ) : point = text edit . map To Global ( cursor rect . top Right ( ) ) # If tip is still off screen, check if point is in top or bottom # half of screen. if point . y ( ) - tip height < padding : # If point is in upper half of screen, show tip below it. # otherwise above it. if 2 * point . y ( ) < screen rect . height ( ) : vertical = 'bottom' else : vertical = 'top' else : vertical = 'top' if point . x ( ) + tip width > screen rect . width ( ) : point = text edit . map To Global ( cursor rect . top Right ( ) ) # If tip is still off-screen, check if point is in the right or # left half of the screen. if point . x ( ) - tip width < padding : if 2 * point . x ( ) < screen rect . width ( ) : horizontal = 'Right' else : horizontal = 'Left' else : horizontal = 'Left' pos = getattr ( cursor rect , '%s%s' % ( vertical , horizontal ) ) point = text edit . map To Global ( pos ( ) ) if vertical == 'top' : point . set Y ( point . y ( ) - tip height - padding ) if horizontal == 'Left' : point . set X ( point . x ( ) - tip width - padding ) self . move ( point ) self . show ( ) return True
def cursor position changed ( self ) : cursor = self . text edit . text Cursor ( ) if cursor . position ( ) <= self . start position : self . hide ( ) else : position , commas = self . find parenthesis ( self . start position + 1 ) if position != - 1 : self . hide ( )
def read ( * paths ) : with open ( os . path . join ( * paths ) , 'r' ) as file handler : return file handler . read ( )
def virtualenv no global ( ) : #this mirrors the logic in virtualenv.py for locating the no-global-site-packages.txt file site mod dir = os . path . dirname ( os . path . abspath ( site . file ) ) no global file = os . path . join ( site mod dir , 'no-global-site-packages.txt' ) if running under virtualenv ( ) and os . path . isfile ( no global file ) : return True
def default aliases ( ) : # Note: the aliases defined here should be safe to use on a kernel # regardless of what frontend it is attached to.  Frontends that use a # kernel in-process can define additional aliases that will only work in # their case.  For example, things like 'less' or 'clear' that manipulate # the terminal should NOT be declared here, as they will only work if the # kernel is running inside a true terminal, and not over the network. if os . name == 'posix' : default aliases = [ ( 'mkdir' , 'mkdir' ) , ( 'rmdir' , 'rmdir' ) , ( 'mv' , 'mv -i' ) , ( 'rm' , 'rm -i' ) , ( 'cp' , 'cp -i' ) , ( 'cat' , 'cat' ) , ] # Useful set of ls aliases.  The GNU and BSD options are a little # different, so we make aliases that provide as similar as possible # behavior in ipython, by passing the right flags for each platform if sys . platform . startswith ( 'linux' ) : ls aliases = [ ( 'ls' , 'ls -F --color' ) , # long ls ( 'll' , 'ls -F -o --color' ) , # ls normal files only ( 'lf' , 'ls -F -o --color %l | grep ^-' ) , # ls symbolic links ( 'lk' , 'ls -F -o --color %l | grep ^l' ) , # directories or links to directories, ( 'ldir' , 'ls -F -o --color %l | grep /$' ) , # things which are executable ( 'lx' , 'ls -F -o --color %l | grep ^-..x' ) , ] else : # BSD, OSX, etc. ls aliases = [ ( 'ls' , 'ls -F' ) , # long ls ( 'll' , 'ls -F -l' ) , # ls normal files only ( 'lf' , 'ls -F -l %l | grep ^-' ) , # ls symbolic links ( 'lk' , 'ls -F -l %l | grep ^l' ) , # directories or links to directories, ( 'ldir' , 'ls -F -l %l | grep /$' ) , # things which are executable ( 'lx' , 'ls -F -l %l | grep ^-..x' ) , ] default aliases = default aliases + ls aliases elif os . name in [ 'nt' , 'dos' ] : default aliases = [ ( 'ls' , 'dir /on' ) , ( 'ddir' , 'dir /ad /on' ) , ( 'ldir' , 'dir /ad /on' ) , ( 'mkdir' , 'mkdir' ) , ( 'rmdir' , 'rmdir' ) , ( 'echo' , 'echo' ) , ( 'ren' , 'ren' ) , ( 'copy' , 'copy' ) , ] else : default aliases = [ ] return default aliases
def soft define alias ( self , name , cmd ) : try : self . define alias ( name , cmd ) except Alias Error , e : error ( "Invalid alias: %s" % e )
def validate alias ( self , name , cmd ) : if name in self . no alias : raise Invalid Alias Error ( "The name %s can't be aliased " "because it is a keyword or builtin." % name ) if not ( isinstance ( cmd , basestring ) ) : raise Invalid Alias Error ( "An alias command must be a string, " "got: %r" % cmd ) nargs = cmd . count ( '%s' ) if nargs > 0 and cmd . find ( '%l' ) >= 0 : raise Invalid Alias Error ( 'The %s and %l specifiers are mutually ' 'exclusive in alias definitions.' ) return nargs
def call alias ( self , alias , rest = '' ) : cmd = self . transform alias ( alias , rest ) try : self . shell . system ( cmd ) except : self . shell . showtraceback ( )
def transform alias ( self , alias , rest = '' ) : nargs , cmd = self . alias table [ alias ] if ' ' in cmd and os . path . isfile ( cmd ) : cmd = '"%s"' % cmd # Expand the %l special to be the user's input line if cmd . find ( '%l' ) >= 0 : cmd = cmd . replace ( '%l' , rest ) rest = '' if nargs == 0 : # Simple, argument-less aliases cmd = '%s %s' % ( cmd , rest ) else : # Handle aliases with positional arguments args = rest . split ( None , nargs ) if len ( args ) < nargs : raise Alias Error ( 'Alias <%s> requires %s arguments, %s given.' % ( alias , nargs , len ( args ) ) ) cmd = '%s %s' % ( cmd % tuple ( args [ : nargs ] ) , ' ' . join ( args [ nargs : ] ) ) return cmd
def autohelp directive ( dirname , arguments , options , content , lineno , content offset , block text , state , state machine ) : config = Config ( parser Class = Opt Bucket , plugins = Builtin Plugin Manager ( ) ) parser = config . get Parser ( Test Program . usage ( ) ) rst = View List ( ) for line in parser . format help ( ) . split ( '\n' ) : rst . append ( line , '<autodoc>' ) rst . append ( 'Options' , '<autodoc>' ) rst . append ( '-------' , '<autodoc>' ) rst . append ( '' , '<autodoc>' ) for opt in parser : rst . append ( opt . options ( ) , '<autodoc>' ) rst . append ( '   \n' , '<autodoc>' ) rst . append ( '   ' + opt . help + '\n' , '<autodoc>' ) rst . append ( '\n' , '<autodoc>' ) node = nodes . section ( ) node . document = state . document surrounding title styles = state . memo . title styles surrounding section level = state . memo . section level state . memo . title styles = [ ] state . memo . section level = 0 state . nested parse ( rst , 0 , node , match titles = 1 ) state . memo . title styles = surrounding title styles state . memo . section level = surrounding section level return node . children
def reset sgr ( self ) : self . intensity = 0 self . italic = False self . bold = False self . underline = False self . foreground color = None self . background color = None
def split string ( self , string ) : self . actions = [ ] start = 0 # strings ending with \r are assumed to be ending in \r\n since # \n is appended to output strings automatically.  Accounting # for that, here. last char = '\n' if len ( string ) > 0 and string [ - 1 ] == '\n' else None string = string [ : - 1 ] if last char is not None else string for match in ANSI OR SPECIAL PATTERN . finditer ( string ) : raw = string [ start : match . start ( ) ] substring = SPECIAL PATTERN . sub ( self . replace special , raw ) if substring or self . actions : yield substring self . actions = [ ] start = match . end ( ) groups = filter ( lambda x : x is not None , match . groups ( ) ) g0 = groups [ 0 ] if g0 == '\a' : self . actions . append ( Beep Action ( 'beep' ) ) yield None self . actions = [ ] elif g0 == '\r' : self . actions . append ( Carriage Return Action ( 'carriage-return' ) ) yield None self . actions = [ ] elif g0 == '\b' : self . actions . append ( Back Space Action ( 'backspace' ) ) yield None self . actions = [ ] elif g0 == '\n' or g0 == '\r\n' : self . actions . append ( New Line Action ( 'newline' ) ) yield g0 self . actions = [ ] else : params = [ param for param in groups [ 1 ] . split ( ';' ) if param ] if g0 . startswith ( '[' ) : # Case 1: CSI code. try : params = map ( int , params ) except Value Error : # Silently discard badly formed codes. pass else : self . set csi code ( groups [ 2 ] , params ) elif g0 . startswith ( ']' ) : # Case 2: OSC code. self . set osc code ( params ) raw = string [ start : ] substring = SPECIAL PATTERN . sub ( self . replace special , raw ) if substring or self . actions : yield substring if last char is not None : self . actions . append ( New Line Action ( 'newline' ) ) yield last char
def get format ( self ) : format = Qt Gui . Q Text Char Format ( ) # Set foreground color qcolor = self . get color ( self . foreground color , self . intensity ) if qcolor is not None : format . set Foreground ( qcolor ) # Set background color qcolor = self . get color ( self . background color , self . intensity ) if qcolor is not None : format . set Background ( qcolor ) # Set font weight/style options if self . bold : format . set Font Weight ( Qt Gui . Q Font . Bold ) else : format . set Font Weight ( Qt Gui . Q Font . Normal ) format . set Font Italic ( self . italic ) format . set Font Underline ( self . underline ) return format
def generate ( secret , age , * * payload ) : jti = str ( uuid . uuid1 ( ) ) # random id if not payload : payload = { } payload [ 'exp' ] = int ( time . time ( ) + age ) payload [ 'jti' ] = jti return jwt . encode ( payload , decode secret ( secret ) )
def mutex ( func ) : def wrapper ( * args , * * kwargs ) : """Decorator Wrapper""" lock = args [ 0 ] . lock lock . acquire ( True ) try : return func ( * args , * * kwargs ) except : raise finally : lock . release ( ) return wrapper
def clean ( self ) : now = time . time ( ) for jwt in self . jwts . keys ( ) : if ( now - self . jwts [ jwt ] ) > ( self . age * 2 ) : del self . jwts [ jwt ]
def already used ( self , tok ) : if tok in self . jwts : return True self . jwts [ tok ] = time . time ( ) return False
def valid ( self , token ) : now = time . time ( ) if 'Bearer ' in token : token = token [ 7 : ] data = None for secret in self . secrets : try : data = jwt . decode ( token , secret ) break except jwt . Decode Error : continue except jwt . Expired Signature Error : raise Jwt Failed ( "Jwt expired" ) if not data : raise Jwt Failed ( "Jwt cannot be decoded" ) exp = data . get ( 'exp' ) if not exp : raise Jwt Failed ( "Jwt missing expiration (exp)" ) if now - exp > self . age : raise Jwt Failed ( "Jwt bad expiration - greater than I want to accept" ) jti = data . get ( 'jti' ) if not jti : raise Jwt Failed ( "Jwt missing one-time id (jti)" ) if self . already used ( jti ) : raise Jwt Failed ( "Jwt re-use disallowed (jti={})" . format ( jti ) ) return data
def write ( self , nb , fp , * * kwargs ) : return fp . write ( self . writes ( nb , * * kwargs ) )
def can cut ( self ) : cursor = self . control . text Cursor ( ) return ( cursor . has Selection ( ) and self . in buffer ( cursor . anchor ( ) ) and self . in buffer ( cursor . position ( ) ) )
def can paste ( self ) : if self . control . text Interaction Flags ( ) & Qt Core . Qt . Text Editable : return bool ( Qt Gui . Q Application . clipboard ( ) . text ( ) ) return False
def set font ( self , font ) : font metrics = Qt Gui . Q Font Metrics ( font ) self . control . set Tab Stop Width ( self . tab width * font metrics . width ( ' ' ) ) self . completion widget . set Font ( font ) self . control . document ( ) . set Default Font ( font ) if self . page control : self . page control . document ( ) . set Default Font ( font ) self . font changed . emit ( font )
def print ( self , printer = None ) : if ( not printer ) : printer = Qt Gui . Q Printer ( ) if ( Qt Gui . Q Print Dialog ( printer ) . exec ( ) != Qt Gui . Q Dialog . Accepted ) : return self . control . print ( printer )
def prompt to top ( self ) : if not self . executing : prompt cursor = self . get prompt cursor ( ) if self . get cursor ( ) . block Number ( ) < prompt cursor . block Number ( ) : self . set cursor ( prompt cursor ) self . set top cursor ( prompt cursor )
def reset font ( self ) : if sys . platform == 'win32' : # Consolas ships with Vista/Win7, fallback to Courier if needed fallback = 'Courier' elif sys . platform == 'darwin' : # OSX always has Monaco fallback = 'Monaco' else : # Monospace should always exist fallback = 'Monospace' font = get font ( self . font family , fallback ) if self . font size : font . set Point Size ( self . font size ) else : font . set Point Size ( Qt Gui . q App . font ( ) . point Size ( ) ) font . set Style Hint ( Qt Gui . Q Font . Type Writer ) self . set font ( font )
def append html ( self , html , before prompt = False ) : self . append custom ( self . insert html , html , before prompt )
def append html fetching plain text ( self , html , before prompt = False ) : return self . append custom ( self . insert html fetching plain text , html , before prompt )
def append plain text ( self , text , before prompt = False ) : self . append custom ( self . insert plain text , text , before prompt )
def complete with items ( self , cursor , items ) : self . cancel completion ( ) if len ( items ) == 1 : cursor . set Position ( self . control . text Cursor ( ) . position ( ) , Qt Gui . Q Text Cursor . Keep Anchor ) cursor . insert Text ( items [ 0 ] ) elif len ( items ) > 1 : current pos = self . control . text Cursor ( ) . position ( ) prefix = commonprefix ( items ) if prefix : cursor . set Position ( current pos , Qt Gui . Q Text Cursor . Keep Anchor ) cursor . insert Text ( prefix ) current pos = cursor . position ( ) cursor . move Position ( Qt Gui . Q Text Cursor . Left , n = len ( prefix ) ) self . completion widget . show items ( cursor , items )
def fill temporary buffer ( self , cursor , text , html = False ) : current pos = self . control . text Cursor ( ) . position ( ) cursor . begin Edit Block ( ) self . append plain text ( '\n' ) self . page ( text , html = html ) cursor . end Edit Block ( ) cursor . set Position ( current pos ) self . control . move Cursor ( Qt Gui . Q Text Cursor . End ) self . control . set Text Cursor ( cursor ) self . temp buffer filled = True
def create control ( self ) : # Create the underlying control. if self . custom control : control = self . custom control ( ) elif self . kind == 'plain' : control = Qt Gui . Q Plain Text Edit ( ) elif self . kind == 'rich' : control = Qt Gui . Q Text Edit ( ) control . set Accept Rich Text ( False ) # Install event filters. The filter on the viewport is needed for # mouse events and drag events. control . install Event Filter ( self ) control . viewport ( ) . install Event Filter ( self ) # Connect signals. control . custom Context Menu Requested . connect ( self . custom context menu requested ) control . copy Available . connect ( self . copy available ) control . redo Available . connect ( self . redo available ) control . undo Available . connect ( self . undo available ) # Hijack the document size change signal to prevent Qt from adjusting # the viewport's scrollbar. We are relying on an implementation detail # of Q(Plain)Text Edit here, which is potentially dangerous, but without # this functionality we cannot create a nice terminal interface. layout = control . document ( ) . document Layout ( ) layout . document Size Changed . disconnect ( ) layout . document Size Changed . connect ( self . adjust scrollbars ) # Configure the control. control . set Attribute ( Qt Core . Qt . WA Input Method Enabled , True ) control . set Context Menu Policy ( Qt Core . Qt . Custom Context Menu ) control . set Read Only ( True ) control . set Undo Redo Enabled ( False ) control . set Vertical Scroll Bar Policy ( Qt Core . Qt . Scroll Bar Always On ) return control
def create page control ( self ) : if self . custom page control : control = self . custom page control ( ) elif self . kind == 'plain' : control = Qt Gui . Q Plain Text Edit ( ) elif self . kind == 'rich' : control = Qt Gui . Q Text Edit ( ) control . install Event Filter ( self ) viewport = control . viewport ( ) viewport . install Event Filter ( self ) control . set Read Only ( True ) control . set Undo Redo Enabled ( False ) control . set Vertical Scroll Bar Policy ( Qt Core . Qt . Scroll Bar Always On ) return control
def get block plain text ( self , block ) : cursor = Qt Gui . Q Text Cursor ( block ) cursor . move Position ( Qt Gui . Q Text Cursor . Start Of Block ) cursor . move Position ( Qt Gui . Q Text Cursor . End Of Block , Qt Gui . Q Text Cursor . Keep Anchor ) return cursor . selection ( ) . to Plain Text ( )
def get end cursor ( self ) : cursor = self . control . text Cursor ( ) cursor . move Position ( Qt Gui . Q Text Cursor . End ) return cursor
def get prompt cursor ( self ) : cursor = self . control . text Cursor ( ) cursor . set Position ( self . prompt pos ) return cursor
def insert continuation prompt ( self , cursor ) : if self . continuation prompt html is None : self . insert plain text ( cursor , self . continuation prompt ) else : self . continuation prompt = self . insert html fetching plain text ( cursor , self . continuation prompt html )
def keyboard quit ( self ) : if self . temp buffer filled : self . cancel completion ( ) self . clear temporary buffer ( ) else : self . input buffer = ''
def prompt started ( self ) : # Temporarily disable the maximum block count to permit undo/redo and # to ensure that the prompt position does not change due to truncation. self . control . document ( ) . set Maximum Block Count ( 0 ) self . control . set Undo Redo Enabled ( True ) # Work around bug in Q Plain Text Edit: input method is not re-enabled # when read-only is disabled. self . control . set Read Only ( False ) self . control . set Attribute ( Qt Core . Qt . WA Input Method Enabled , True ) if not self . reading : self . executing = False self . prompt started hook ( ) # If the input buffer has changed while executing, load it. if self . input buffer pending : self . input buffer = self . input buffer pending self . input buffer pending = '' self . control . move Cursor ( Qt Gui . Q Text Cursor . End )
def set top cursor ( self , cursor ) : scrollbar = self . control . vertical Scroll Bar ( ) scrollbar . set Value ( scrollbar . maximum ( ) ) original cursor = self . control . text Cursor ( ) self . control . set Text Cursor ( cursor ) self . control . ensure Cursor Visible ( ) self . control . set Text Cursor ( original cursor )
def adjust scrollbars ( self ) : # This code is adapted from  q adjust Scrollbars in qplaintextedit.cpp # and qtextedit.cpp. document = self . control . document ( ) scrollbar = self . control . vertical Scroll Bar ( ) viewport height = self . control . viewport ( ) . height ( ) if isinstance ( self . control , Qt Gui . Q Plain Text Edit ) : maximum = max ( 0 , document . line Count ( ) - 1 ) step = viewport height / self . control . font Metrics ( ) . line Spacing ( ) else : # Q Text Edit does not do line-based layout and blocks will not in # general have the same height. Therefore it does not make sense to # attempt to scroll in line height increments. maximum = document . size ( ) . height ( ) step = viewport height diff = maximum - scrollbar . maximum ( ) scrollbar . set Range ( 0 , maximum ) scrollbar . set Page Step ( step ) # Compensate for undesirable scrolling that occurs automatically due to # maximum Block Count() text truncation. if diff < 0 and document . block Count ( ) == document . maximum Block Count ( ) : scrollbar . set Value ( scrollbar . value ( ) + diff )
def dist in usersite ( dist ) : if user site : return normalize path ( dist location ( dist ) ) . startswith ( normalize path ( user site ) ) else : return False
def print results ( distributions , list all files ) : results printed = False for dist in distributions : results printed = True logger . info ( "---" ) logger . info ( "Name: %s" % dist [ 'name' ] ) logger . info ( "Version: %s" % dist [ 'version' ] ) logger . info ( "Location: %s" % dist [ 'location' ] ) logger . info ( "Requires: %s" % ', ' . join ( dist [ 'requires' ] ) ) if list all files : logger . info ( "Files:" ) if dist [ 'files' ] is not None : for line in dist [ 'files' ] : logger . info ( "  %s" % line . strip ( ) ) else : logger . info ( "Cannot locate installed-files.txt" ) return results printed
def main ( args = None ) : options , paths = parse options ( args ) format = getattr ( options , 'output' , 'simple' ) formatter = FORMATTERS [ format ] ( options ) for path in paths : meta = get metadata ( path , options . metadata version ) if meta is None : continue if options . download url prefix : if meta . download url is None : filename = os . path . basename ( path ) meta . download url = '%s/%s' % ( options . download url prefix , filename ) formatter ( meta ) formatter . finish ( )
def cmp to key ( mycmp ) : class Key ( object ) : def init ( self , obj ) : self . obj = obj def lt ( self , other ) : return mycmp ( self . obj , other . obj ) < 0 def gt ( self , other ) : return mycmp ( self . obj , other . obj ) > 0 def eq ( self , other ) : return mycmp ( self . obj , other . obj ) == 0 return Key
def file read ( filename ) : fobj = open ( filename , 'r' ) source = fobj . read ( ) fobj . close ( ) return source
def close ( self ) : self . flush ( ) setattr ( sys , self . channel , self . ostream ) self . file . close ( ) self . closed = True
def write ( self , data ) : self . file . write ( data ) self . ostream . write ( data ) self . ostream . flush ( )
def add new heart handler ( self , handler ) : self . log . debug ( "heartbeat::new heart handler: %s" , handler ) self . new handlers . add ( handler )
def add heart failure handler ( self , handler ) : self . log . debug ( "heartbeat::new heart failure handler: %s" , handler ) self . failure handlers . add ( handler )
def handle pong ( self , msg ) : current = str to bytes ( str ( self . lifetime ) ) last = str to bytes ( str ( self . last ping ) ) if msg [ 1 ] == current : delta = time . time ( ) - self . tic # self.log.debug("heartbeat::heart %r took %.2f ms to respond"%(msg[0], 1000*delta)) self . responses . add ( msg [ 0 ] ) elif msg [ 1 ] == last : delta = time . time ( ) - self . tic + ( self . lifetime - self . last ping ) self . log . warn ( "heartbeat::heart %r missed a beat, and took %.2f ms to respond" , msg [ 0 ] , 1000 * delta ) self . responses . add ( msg [ 0 ] ) else : self . log . warn ( "heartbeat::got bad heartbeat (possibly old?): %s (current=%.3f)" , msg [ 1 ] , self . lifetime )
def display All ( elapsed , display amt , est end , n Loops , count , num Prints ) : if num Prints > n Loops : display amt = 1 else : display amt = round ( n Loops / num Prints ) if count % display amt == 0 : avg = elapsed / count est end = round ( avg * n Loops ) ( disp elapsed , disp avg , disp est ) = time Unit ( int ( round ( elapsed ) ) , int ( round ( avg ) ) , int ( round ( est end ) ) ) print "%s%%" % str ( round ( count / float ( n Loops ) * 100 ) ) , "@" + str ( count ) , total Time = disp est [ 0 ] unit = disp est [ 1 ] if str ( unit ) == "secs" : remain = total Time - round ( elapsed ) remain Unit = "secs" elif str ( unit ) == "mins" : remain = total Time - round ( elapsed ) / 60 remain Unit = "mins" elif str ( unit ) == "hr" : remain = total Time - round ( elapsed ) / 3600 remain Unit = "hr" print "ETA: %s %s" % ( str ( remain ) , remain Unit ) print return
def time Unit ( elapsed , avg , est end ) : minute = 60 hr = 3600 day = 86400 if elapsed <= 3 * minute : unit elapsed = ( elapsed , "secs" ) if elapsed > 3 * minute : unit elapsed = ( ( elapsed / 60 ) , "mins" ) if elapsed > 3 * hr : unit elapsed = ( ( elapsed / 3600 ) , "hr" ) if avg <= 3 * minute : unit avg = ( avg , "secs" ) if avg > 3 * minute : unit avg = ( ( avg / 60 ) , "mins" ) if avg > 3 * hr : unit avg = ( ( avg / 3600 ) , "hr" ) if est end <= 3 * minute : unit est End = ( est end , "secs" ) if est end > 3 * minute : unit est End = ( ( est end / 60 ) , "mins" ) if est end > 3 * hr : unit est End = ( ( est end / 3600 ) , "hr" ) return [ unit elapsed , unit avg , unit est End ]
def uncache zipdir ( path ) : from zipimport import zip directory cache as zdc uncache ( path , zdc ) uncache ( path , sys . path importer cache )
def nt quote arg ( arg ) : result = [ ] needquote = False nb = 0 needquote = ( " " in arg ) or ( "\t" in arg ) if needquote : result . append ( '"' ) for c in arg : if c == '\\' : nb += 1 elif c == '"' : # double preceding backslashes, then add a \" result . append ( '\\' * ( nb * 2 ) + '\\"' ) nb = 0 else : if nb : result . append ( '\\' * nb ) nb = 0 result . append ( c ) if nb : result . append ( '\\' * nb ) if needquote : result . append ( '\\' * nb ) # double the trailing backslashes result . append ( '"' ) return '' . join ( result )
def install script ( self , dist , script name , script text , dev path = None ) : spec = str ( dist . as requirement ( ) ) is script = is python script ( script text , script name ) def get template ( filename ) : raw bytes = resource string ( 'setuptools' , template name ) template str = raw bytes . decode ( 'utf-8' ) clean template = template str . replace ( '"""' , '' ) return clean template if is script : template name = 'script template.py' if dev path : template name = template name . replace ( '.py' , ' (dev).py' ) script text = ( get script header ( script text ) + get template ( template name ) % locals ( ) ) self . write script ( script name , to ascii ( script text ) , 'b' )
def check conflicts ( self , dist ) : return dist # XXX temporarily disable until new strategy is stable from imp import find module , get suffixes from glob import glob blockers = [ ] names = dict . fromkeys ( dist . get metadata ( 'top level.txt' ) ) # XXX private attr exts = { '.pyc' : 1 , '.pyo' : 1 } # get suffixes() might leave one out for ext , mode , typ in get suffixes ( ) : exts [ ext ] = 1 for path , files in expand paths ( [ self . install dir ] + self . all site dirs ) : for filename in files : base , ext = os . path . splitext ( filename ) if base in names : if not ext : # no extension, check for package try : f , filename , descr = find module ( base , [ path ] ) except Import Error : continue else : if f : f . close ( ) if filename not in blockers : blockers . append ( filename ) elif ext in exts and base != 'site' : # XXX ugh blockers . append ( os . path . join ( path , filename ) ) if blockers : self . found conflicts ( dist , blockers ) return dist
def create home path ( self ) : if not self . user : return home = convert path ( os . path . expanduser ( "~" ) ) for name , path in self . config vars . iteritems ( ) : if path . startswith ( home ) and not os . path . isdir ( path ) : self . debug print ( "os.makedirs('%s', 0700)" % path ) os . makedirs ( path , 0700 )
def is archive file ( name ) : archives = ( '.zip' , '.tar.gz' , '.tar.bz2' , '.tgz' , '.tar' , '.whl' ) ext = splitext ( name ) [ 1 ] . lower ( ) if ext in archives : return True return False
def new output ( output type = None , output text = None , output png = None , output html = None , output svg = None , output latex = None , output json = None , output javascript = None , output jpeg = None , prompt number = None , etype = None , evalue = None , traceback = None ) : output = Notebook Node ( ) if output type is not None : output . output type = unicode ( output type ) if output type != 'pyerr' : if output text is not None : output . text = unicode ( output text ) if output png is not None : output . png = bytes ( output png ) if output jpeg is not None : output . jpeg = bytes ( output jpeg ) if output html is not None : output . html = unicode ( output html ) if output svg is not None : output . svg = unicode ( output svg ) if output latex is not None : output . latex = unicode ( output latex ) if output json is not None : output . json = unicode ( output json ) if output javascript is not None : output . javascript = unicode ( output javascript ) if output type == u'pyout' : if prompt number is not None : output . prompt number = int ( prompt number ) if output type == u'pyerr' : if etype is not None : output . etype = unicode ( etype ) if evalue is not None : output . evalue = unicode ( evalue ) if traceback is not None : output . traceback = [ unicode ( frame ) for frame in list ( traceback ) ] return output
def new code cell ( input = None , prompt number = None , outputs = None , language = u'python' , collapsed = False , metadata = None ) : cell = Notebook Node ( ) cell . cell type = u'code' if language is not None : cell . language = unicode ( language ) if input is not None : cell . input = unicode ( input ) if prompt number is not None : cell . prompt number = int ( prompt number ) if outputs is None : cell . outputs = [ ] else : cell . outputs = outputs if collapsed is not None : cell . collapsed = bool ( collapsed ) cell . metadata = Notebook Node ( metadata or { } ) return cell
def new text cell ( cell type , source = None , rendered = None , metadata = None ) : cell = Notebook Node ( ) # VERSIONHACK: plaintext -> raw # handle never-released plaintext name for raw cells if cell type == 'plaintext' : cell type = 'raw' if source is not None : cell . source = unicode ( source ) if rendered is not None : cell . rendered = unicode ( rendered ) cell . metadata = Notebook Node ( metadata or { } ) cell . cell type = cell type return cell
def new heading cell ( source = None , rendered = None , level = 1 , metadata = None ) : cell = Notebook Node ( ) cell . cell type = u'heading' if source is not None : cell . source = unicode ( source ) if rendered is not None : cell . rendered = unicode ( rendered ) cell . level = int ( level ) cell . metadata = Notebook Node ( metadata or { } ) return cell
def new notebook ( name = None , metadata = None , worksheets = None ) : nb = Notebook Node ( ) nb . nbformat = nbformat nb . nbformat minor = nbformat minor if worksheets is None : nb . worksheets = [ ] else : nb . worksheets = list ( worksheets ) if metadata is None : nb . metadata = new metadata ( ) else : nb . metadata = Notebook Node ( metadata ) if name is not None : nb . metadata . name = unicode ( name ) return nb
def new metadata ( name = None , authors = None , license = None , created = None , modified = None , gistid = None ) : metadata = Notebook Node ( ) if name is not None : metadata . name = unicode ( name ) if authors is not None : metadata . authors = list ( authors ) if created is not None : metadata . created = unicode ( created ) if modified is not None : metadata . modified = unicode ( modified ) if license is not None : metadata . license = unicode ( license ) if gistid is not None : metadata . gistid = unicode ( gistid ) return metadata
def new author ( name = None , email = None , affiliation = None , url = None ) : author = Notebook Node ( ) if name is not None : author . name = unicode ( name ) if email is not None : author . email = unicode ( email ) if affiliation is not None : author . affiliation = unicode ( affiliation ) if url is not None : author . url = unicode ( url ) return author
def writable dir ( path ) : return os . path . isdir ( path ) and os . access ( path , os . W OK )
def unquote filename ( name , win32 = ( sys . platform == 'win32' ) ) : if win32 : if name . startswith ( ( "'" , '"' ) ) and name . endswith ( ( "'" , '"' ) ) : name = name [ 1 : - 1 ] return name
def get ipython package dir ( ) : ipdir = os . path . dirname ( I Python . file ) return py3compat . cast unicode ( ipdir , fs encoding )
def update suggestions dictionary ( request , object ) : if request . user . is authenticated ( ) : user = request . user content type = Content Type . objects . get for model ( type ( object ) ) try : # Check if the user has visited this page before Object View . objects . get ( user = user , object id = object . id , content type = content type ) except : Object View . objects . create ( user = user , content object = object ) # Get a list of all the objects a user has visited viewed = Object View . objects . filter ( user = user ) else : update dict for guests ( request , object , content type ) return if viewed : for obj in viewed : if content type == obj . content type : if not exists in dictionary ( request , object , content type , obj , True ) : # Create an entry if it's non existent if object . id != obj . object id : Object View Dictionary . objects . create ( current object = object , visited before object = obj . content object ) if not exists in dictionary ( request , obj , obj . content type , object , False ) : Object View Dictionary . objects . create ( current object = obj . content object , visited before object = object ) return
def get suggestions with size ( object , size ) : content type = Content Type . objects . get for model ( type ( object ) ) try : return Object View Dictionary . objects . filter ( current object id = object . id , current content type = content type ) . extra ( order by = [ '-visits' ] ) [ : size ] except : return Object View Dictionary . objects . filter ( current object id = object . id , current content type = content type ) . extra ( order by = [ '-visits' ] )
def get suggestions ( object ) : content type = Content Type . objects . get for model ( type ( object ) ) return Object View Dictionary . objects . filter ( current object id = object . id , current content type = content type ) . extra ( order by = [ '-visits' ] )
def options ( self , parser , env ) : if not self . available ( ) : return Plugin . options ( self , parser , env ) parser . add option ( '--profile-sort' , action = 'store' , dest = 'profile sort' , default = env . get ( 'NOSE PROFILE SORT' , 'cumulative' ) , metavar = "SORT" , help = "Set sort order for profiler output" ) parser . add option ( '--profile-stats-file' , action = 'store' , dest = 'profile stats file' , metavar = "FILE" , default = env . get ( 'NOSE PROFILE STATS FILE' ) , help = 'Profiler stats file; default is a new ' 'temp file on each run' ) parser . add option ( '--profile-restrict' , action = 'append' , dest = 'profile restrict' , metavar = "RESTRICT" , default = env . get ( 'NOSE PROFILE RESTRICT' ) , help = "Restrict profiler output. See help for " "pstats.Stats for details" )
def begin ( self ) : if not self . available ( ) : return self . create pfile ( ) self . prof = hotshot . Profile ( self . pfile )
def report ( self , stream ) : log . debug ( 'printing profiler report' ) self . prof . close ( ) prof stats = stats . load ( self . pfile ) prof stats . sort stats ( self . sort ) # 2.5 has completely different stream handling from 2.4 and earlier. # Before 2.5, stats objects have no stream attribute; in 2.5 and later # a reference sys.stdout is stored before we can tweak it. compat 25 = hasattr ( prof stats , 'stream' ) if compat 25 : tmp = prof stats . stream prof stats . stream = stream else : tmp = sys . stdout sys . stdout = stream try : if self . restrict : log . debug ( 'setting profiler restriction to %s' , self . restrict ) prof stats . print stats ( * self . restrict ) else : prof stats . print stats ( ) finally : if compat 25 : prof stats . stream = tmp else : sys . stdout = tmp
def finalize ( self , result ) : if not self . available ( ) : return try : self . prof . close ( ) except Attribute Error : # TODO: is this trying to catch just the case where not # hasattr(self.prof, "close")?  If so, the function call should be # moved out of the try: suite. pass if self . clean stats file : if self . fileno : try : os . close ( self . fileno ) except OS Error : pass try : os . unlink ( self . pfile ) except OS Error : pass return None
def setup partitioner ( index , num procs , gnum cells , parts ) : global partitioner p = MPI Rect Partitioner2D ( my id = index , num procs = num procs ) p . redim ( global num cells = gnum cells , num parts = parts ) p . prepare communication ( ) # put the partitioner into the global namespace: partitioner = p
def wave saver ( u , x , y , t ) : global u hist global t hist t hist . append ( t ) u hist . append ( 1.0 * u )
def init db ( self ) : # use detect types so that timestamps return datetime objects self . db = sqlite3 . connect ( self . hist file , detect types = sqlite3 . PARSE DECLTYPES | sqlite3 . PARSE COLNAMES ) self . db . execute ( ) self . db . execute ( ) # Output history is optional, but ensure the table's there so it can be # enabled later. self . db . execute ( ) self . db . commit ( )
def new session ( self , conn = None ) : if conn is None : conn = self . db with conn : cur = conn . execute ( , ( datetime . datetime . now ( ) , ) ) self . session number = cur . lastrowid
def end session ( self ) : self . writeout cache ( ) with self . db : self . db . execute ( , ( datetime . datetime . now ( ) , len ( self . input hist parsed ) - 1 , self . session number ) ) self . session number = 0
def name session ( self , name ) : with self . db : self . db . execute ( "UPDATE sessions SET remark=? WHERE session==?" , ( name , self . session number ) )
def writeout cache ( self , conn = None ) : if conn is None : conn = self . db with self . db input cache lock : try : self . writeout input cache ( conn ) except sqlite3 . Integrity Error : self . new session ( conn ) print ( "ERROR! Session/line number was not unique in" , "database. History logging moved to new session" , self . session number ) try : # Try writing to the new session. If this fails, don't # recurse self . writeout input cache ( conn ) except sqlite3 . Integrity Error : pass finally : self . db input cache = [ ] with self . db output cache lock : try : self . writeout output cache ( conn ) except sqlite3 . Integrity Error : print ( "!! Session/line number for output was not unique" , "in database. Output will not be stored." ) finally : self . db output cache = [ ]
def get num cpus ( ) : # we try to determine num CP Us by using different approaches. # SC NPROCESSORS ONLN seems to be the safer and it is also # used by multiprocessing module try : return os . sysconf ( "SC NPROCESSORS ONLN" ) except Value Error : # as a second fallback we try to parse /proc/cpuinfo num = 0 f = open ( '/proc/cpuinfo' , 'r' ) try : lines = f . readlines ( ) finally : f . close ( ) for line in lines : if line . lower ( ) . startswith ( 'processor' ) : num += 1 # unknown format (e.g. amrel/sparc architectures), see: # http://code.google.com/p/psutil/issues/detail?id=200 # try to parse /proc/stat as a last resort if num == 0 : f = open ( '/proc/stat' , 'r' ) try : lines = f . readlines ( ) finally : f . close ( ) search = re . compile ( 'cpu\d' ) for line in lines : line = line . split ( ' ' ) [ 0 ] if search . match ( line ) : num += 1 if num == 0 : raise Runtime Error ( "can't determine number of CP Us" ) return num
def disk partitions ( all = False ) : phydevs = [ ] f = open ( "/proc/filesystems" , "r" ) try : for line in f : if not line . startswith ( "nodev" ) : phydevs . append ( line . strip ( ) ) finally : f . close ( ) retlist = [ ] partitions = psutil linux . get disk partitions ( ) for partition in partitions : device , mountpoint , fstype , opts = partition if device == 'none' : device = '' if not all : if device == '' or fstype not in phydevs : continue ntuple = nt partition ( device , mountpoint , fstype , opts ) retlist . append ( ntuple ) return retlist
def get system users ( ) : retlist = [ ] rawlist = psutil linux . get system users ( ) for item in rawlist : user , tty , hostname , tstamp , user process = item # XXX the underlying C function includes entries about # system boot, run level and others.  We might want # to use them in the future. if not user process : continue if hostname == ':0.0' : hostname = 'localhost' nt = nt user ( user , tty or None , hostname , tstamp ) retlist . append ( nt ) return retlist
def get pid list ( ) : pids = [ int ( x ) for x in os . listdir ( '/proc' ) if x . isdigit ( ) ] return pids
def short stack ( ) : stack = inspect . stack ( ) [ : 0 : - 1 ] return "\n" . join ( [ "%30s : %s @%d" % ( t [ 3 ] , t [ 1 ] , t [ 2 ] ) for t in stack ] )
def join regex ( regexes ) : if len ( regexes ) > 1 : return "|" . join ( [ "(%s)" % r for r in regexes ] ) elif regexes : return regexes [ 0 ] else : return ""
def file be gone ( path ) : try : os . remove ( path ) except OS Error : , e , = sys . exc info ( ) if e . errno != errno . ENOENT : raise
def update ( self , v ) : self . md5 . update ( to bytes ( str ( type ( v ) ) ) ) if isinstance ( v , string class ) : self . md5 . update ( to bytes ( v ) ) elif v is None : pass elif isinstance ( v , ( int , float ) ) : self . md5 . update ( to bytes ( str ( v ) ) ) elif isinstance ( v , ( tuple , list ) ) : for e in v : self . update ( e ) elif isinstance ( v , dict ) : keys = v . keys ( ) for k in sorted ( keys ) : self . update ( k ) self . update ( v [ k ] ) else : for k in dir ( v ) : if k . startswith ( ' ' ) : continue a = getattr ( v , k ) if inspect . isroutine ( a ) : continue self . update ( k ) self . update ( a )
def update profiles ( self ) : for path in [ get ipython dir ( ) , os . getcwdu ( ) ] : for profile in list profiles in ( path ) : pd = self . get profile dir ( profile , path ) if profile not in self . profiles : self . log . debug ( "Adding cluster profile '%s'" % profile ) self . profiles [ profile ] = { 'profile' : profile , 'profile dir' : pd , 'status' : 'stopped' }
def start cluster ( self , profile , n = None ) : self . check profile ( profile ) data = self . profiles [ profile ] if data [ 'status' ] == 'running' : raise web . HTTP Error ( 409 , u'cluster already running' ) cl , esl , default n = self . build launchers ( data [ 'profile dir' ] ) n = n if n is not None else default n def clean data ( ) : data . pop ( 'controller launcher' , None ) data . pop ( 'engine set launcher' , None ) data . pop ( 'n' , None ) data [ 'status' ] = 'stopped' def engines stopped ( r ) : self . log . debug ( 'Engines stopped' ) if cl . running : cl . stop ( ) clean data ( ) esl . on stop ( engines stopped ) def controller stopped ( r ) : self . log . debug ( 'Controller stopped' ) if esl . running : esl . stop ( ) clean data ( ) cl . on stop ( controller stopped ) dc = ioloop . Delayed Callback ( lambda : cl . start ( ) , 0 , self . loop ) dc . start ( ) dc = ioloop . Delayed Callback ( lambda : esl . start ( n ) , 1000 * self . delay , self . loop ) dc . start ( ) self . log . debug ( 'Cluster started' ) data [ 'controller launcher' ] = cl data [ 'engine set launcher' ] = esl data [ 'n' ] = n data [ 'status' ] = 'running' return self . profile info ( profile )
def stop cluster ( self , profile ) : self . check profile ( profile ) data = self . profiles [ profile ] if data [ 'status' ] == 'stopped' : raise web . HTTP Error ( 409 , u'cluster not running' ) data = self . profiles [ profile ] cl = data [ 'controller launcher' ] esl = data [ 'engine set launcher' ] if cl . running : cl . stop ( ) if esl . running : esl . stop ( ) # Return a temp info dict, the real one is updated in the on stop # logic above. result = { 'profile' : data [ 'profile' ] , 'profile dir' : data [ 'profile dir' ] , 'status' : 'stopped' } return result
def find cmd ( cmd ) : try : from win32api import Search Path except Import Error : raise Import Error ( 'you need to have pywin32 installed for this to work' ) else : PATH = os . environ [ 'PATH' ] extensions = [ '.exe' , '.com' , '.bat' , '.py' ] path = None for ext in extensions : try : path = Search Path ( PATH , cmd + ext ) [ 0 ] except : pass if path is None : raise OS Error ( "command %r not found" % cmd ) else : return path
def system body ( p ) : enc = DEFAULT ENCODING for line in read no interrupt ( p . stdout ) . splitlines ( ) : line = line . decode ( enc , 'replace' ) print ( line , file = sys . stdout ) for line in read no interrupt ( p . stderr ) . splitlines ( ) : line = line . decode ( enc , 'replace' ) print ( line , file = sys . stderr ) # Wait to finish for returncode return p . wait ( )
def setup partitioner ( comm , addrs , index , num procs , gnum cells , parts ) : global partitioner p = ZMQ Rect Partitioner2D ( comm , addrs , my id = index , num procs = num procs ) p . redim ( global num cells = gnum cells , num parts = parts ) p . prepare communication ( ) # put the partitioner into the global namespace: partitioner = p
def init gui pylab ( self ) : if self . gui or self . pylab : shell = self . shell try : if self . pylab : gui , backend = pylabtools . find gui and backend ( self . pylab ) self . log . info ( "Enabling GUI event loop integration, " "toolkit=%s, pylab=%s" % ( gui , self . pylab ) ) shell . enable pylab ( gui , import all = self . pylab import all ) else : self . log . info ( "Enabling GUI event loop integration, " "toolkit=%s" % self . gui ) shell . enable gui ( self . gui ) except Exception : self . log . warn ( "GUI event loop or pylab initialization failed" ) self . shell . showtraceback ( )
def init code ( self ) : self . run startup files ( ) self . run exec lines ( ) self . run exec files ( ) self . run cmd line code ( ) self . run module ( ) # flush output, so itwon't be attached to the first cell sys . stdout . flush ( ) sys . stderr . flush ( ) # Hide variables defined here from %who etc. self . shell . user ns hidden . update ( self . shell . user ns )
def run exec lines ( self ) : if not self . exec lines : return try : self . log . debug ( "Running code from I Python App.exec lines..." ) for line in self . exec lines : try : self . log . info ( "Running code in user namespace: %s" % line ) self . shell . run cell ( line , store history = False ) except : self . log . warn ( "Error in executing line in user " "namespace: %s" % line ) self . shell . showtraceback ( ) except : self . log . warn ( "Unknown error in handling I Python App.exec lines:" ) self . shell . showtraceback ( )
def run startup files ( self ) : startup dir = self . profile dir . startup dir startup files = glob . glob ( os . path . join ( startup dir , '*.py' ) ) startup files += glob . glob ( os . path . join ( startup dir , '*.ipy' ) ) if not startup files : return self . log . debug ( "Running startup files from %s..." , startup dir ) try : for fname in sorted ( startup files ) : self . exec file ( fname ) except : self . log . warn ( "Unknown error in handling startup files:" ) self . shell . showtraceback ( )
def run exec files ( self ) : if not self . exec files : return self . log . debug ( "Running files in I Python App.exec files..." ) try : for fname in self . exec files : self . exec file ( fname ) except : self . log . warn ( "Unknown error in handling I Python App.exec files:" ) self . shell . showtraceback ( )
def run cmd line code ( self ) : if self . code to run : line = self . code to run try : self . log . info ( "Running code given at command line (c=): %s" % line ) self . shell . run cell ( line , store history = False ) except : self . log . warn ( "Error in executing line in user namespace: %s" % line ) self . shell . showtraceback ( ) # Like Python itself, ignore the second if the first of these is present elif self . file to run : fname = self . file to run try : self . exec file ( fname ) except : self . log . warn ( "Error in executing file in user namespace: %s" % fname ) self . shell . showtraceback ( )
def run module ( self ) : if self . module to run : # Make sure that the module gets a proper sys.argv as if it were # run using `python -m`. save argv = sys . argv sys . argv = [ sys . executable ] + self . extra args try : self . shell . safe run module ( self . module to run , self . shell . user ns ) finally : sys . argv = save argv
def generic ( func ) : sentinel = object ( ) def by class ( * args , * * kw ) : cls = args [ 0 ] . class for t in type ( cls . name , ( cls , object ) , { } ) . mro : f = gbt ( t , sentinel ) if f is not sentinel : return f ( * args , * * kw ) else : return func ( * args , * * kw ) by type = { object : func } try : by type [ Instance Type ] = by class except Name Error : # Python 3 pass gbt = by type . get def when type ( * types ) : """Decorator to add a method that will be called for the given types""" for t in types : if not isinstance ( t , classtypes ) : raise Type Error ( "%r is not a type or class" % ( t , ) ) def decorate ( f ) : for t in types : if by type . setdefault ( t , f ) is not f : raise Type Error ( "%r already has method for type %r" % ( func , t ) ) return f return decorate by object = { } gbo = by object . get def when object ( * obs ) : """Decorator to add a method to be called for the given object(s)""" def decorate ( f ) : for o in obs : if by object . setdefault ( id ( o ) , ( o , f ) ) [ 1 ] is not f : raise Type Error ( "%r already has method for object %r" % ( func , o ) ) return f return decorate def dispatch ( * args , * * kw ) : f = gbo ( id ( args [ 0 ] ) , sentinel ) if f is sentinel : for t in type ( args [ 0 ] ) . mro : f = gbt ( t , sentinel ) if f is not sentinel : return f ( * args , * * kw ) else : return func ( * args , * * kw ) else : return f [ 1 ] ( * args , * * kw ) dispatch . name = func . name dispatch . dict = func . dict . copy ( ) dispatch . doc = func . doc dispatch . module = func . module dispatch . when type = when type dispatch . when object = when object dispatch . default = func dispatch . has object = lambda o : id ( o ) in by object dispatch . has type = lambda t : t in by type return dispatch
def data ( fname ) : data file = open ( data filename ( fname ) ) try : return data file . read ( ) finally : data file . close ( )
def escape ( t ) : return ( t # Convert HTML special chars into HTML entities. . replace ( "&" , "&amp;" ) . replace ( "<" , "&lt;" ) . replace ( ">" , "&gt;" ) . replace ( "'" , "&#39;" ) . replace ( '"' , "&quot;" ) # Convert runs of spaces: "......" -> "&nbsp;.&nbsp;.&nbsp;." . replace ( "  " , "&nbsp; " ) # To deal with odd-length runs, convert the final pair of spaces # so that "....." -> "&nbsp;.&nbsp;&nbsp;." . replace ( "  " , "&nbsp; " ) )
def make local static report files ( self ) : # The files we provide must always be copied. for static , pkgdir in self . STATIC FILES : shutil . copyfile ( data filename ( static , pkgdir ) , os . path . join ( self . directory , static ) ) # The user may have extra CSS they want copied. if self . extra css : shutil . copyfile ( self . config . extra css , os . path . join ( self . directory , self . extra css ) )
def write html ( self , fname , html ) : fout = open ( fname , "wb" ) try : fout . write ( html . encode ( 'ascii' , 'xmlcharrefreplace' ) ) finally : fout . close ( )
def file hash ( self , source , cu ) : m = Hasher ( ) m . update ( source ) self . coverage . data . add to hash ( cu . filename , m ) return m . digest ( )
def html file ( self , cu , analysis ) : source file = cu . source file ( ) try : source = source file . read ( ) finally : source file . close ( ) # Find out if the file on disk is already correct. flat rootname = cu . flat rootname ( ) this hash = self . file hash ( source , cu ) that hash = self . status . file hash ( flat rootname ) if this hash == that hash : # Nothing has changed to require the file to be reported again. self . files . append ( self . status . index info ( flat rootname ) ) return self . status . set file hash ( flat rootname , this hash ) # If need be, determine the encoding of the source file. We use it # later to properly write the HTML. if sys . version info < ( 3 , 0 ) : encoding = source encoding ( source ) # Some UTF8 files have the dreaded UTF8 BOM. If so, junk it. if encoding . startswith ( "utf-8" ) and source [ : 3 ] == "\xef\xbb\xbf" : source = source [ 3 : ] encoding = "utf-8" # Get the numbers for this file. nums = analysis . numbers if self . arcs : missing branch arcs = analysis . missing branch arcs ( ) # These classes determine which lines are highlighted by default. c run = "run hide run" c exc = "exc" c mis = "mis" c par = "par " + c run lines = [ ] for lineno , line in enumerate ( source token lines ( source ) ) : lineno += 1 # 1-based line numbers. # Figure out how to mark this line. line class = [ ] annotate html = "" annotate title = "" if lineno in analysis . statements : line class . append ( "stm" ) if lineno in analysis . excluded : line class . append ( c exc ) elif lineno in analysis . missing : line class . append ( c mis ) elif self . arcs and lineno in missing branch arcs : line class . append ( c par ) annlines = [ ] for b in missing branch arcs [ lineno ] : if b < 0 : annlines . append ( "exit" ) else : annlines . append ( str ( b ) ) annotate html = "&nbsp;&nbsp; " . join ( annlines ) if len ( annlines ) > 1 : annotate title = "no jumps to these line numbers" elif len ( annlines ) == 1 : annotate title = "no jump to this line number" elif lineno in analysis . statements : line class . append ( c run ) # Build the HTML for the line html = [ ] for tok type , tok text in line : if tok type == "ws" : html . append ( escape ( tok text ) ) else : tok html = escape ( tok text ) or '&nbsp;' html . append ( "<span class='%s'>%s</span>" % ( tok type , tok html ) ) lines . append ( { 'html' : '' . join ( html ) , 'number' : lineno , 'class' : ' ' . join ( line class ) or "pln" , 'annotate' : annotate html , 'annotate title' : annotate title , } ) # Write the HTML page for this file. html = spaceless ( self . source tmpl . render ( { 'c exc' : c exc , 'c mis' : c mis , 'c par' : c par , 'c run' : c run , 'arcs' : self . arcs , 'extra css' : self . extra css , 'cu' : cu , 'nums' : nums , 'lines' : lines , } ) ) if sys . version info < ( 3 , 0 ) : html = html . decode ( encoding ) html filename = flat rootname + ".html" html path = os . path . join ( self . directory , html filename ) self . write html ( html path , html ) # Save this file's information for the index file. index info = { 'nums' : nums , 'html filename' : html filename , 'name' : cu . name , } self . files . append ( index info ) self . status . set index info ( flat rootname , index info )
def index file ( self ) : index tmpl = Templite ( data ( "index.html" ) , self . template globals ) self . totals = sum ( [ f [ 'nums' ] for f in self . files ] ) html = index tmpl . render ( { 'arcs' : self . arcs , 'extra css' : self . extra css , 'files' : self . files , 'totals' : self . totals , } ) if sys . version info < ( 3 , 0 ) : html = html . decode ( "utf-8" ) self . write html ( os . path . join ( self . directory , "index.html" ) , html ) # Write the latest hashes for next time. self . status . write ( self . directory )
def read ( self , directory ) : usable = False try : status file = os . path . join ( directory , self . STATUS FILE ) fstatus = open ( status file , "rb" ) try : status = pickle . load ( fstatus ) finally : fstatus . close ( ) except ( IO Error , Value Error ) : usable = False else : usable = True if status [ 'format' ] != self . STATUS FORMAT : usable = False elif status [ 'version' ] != coverage . version : usable = False if usable : self . files = status [ 'files' ] self . settings = status [ 'settings' ] else : self . reset ( )
def write ( self , directory ) : status file = os . path . join ( directory , self . STATUS FILE ) status = { 'format' : self . STATUS FORMAT , 'version' : coverage . version , 'settings' : self . settings , 'files' : self . files , } fout = open ( status file , "wb" ) try : pickle . dump ( status , fout ) finally : fout . close ( )
def get slice ( seq , start = 0 , stop = None , step = 1 ) : if stop == None : stop = len ( seq ) item = lambda i : seq [ i ] return map ( item , xrange ( start , stop , step ) )
def chop ( seq , size ) : chunk = lambda i : seq [ i : i + size ] return map ( chunk , xrange ( 0 , len ( seq ) , size ) )
def read config ( ) : # XXX modifies global state, which is kind of evil config = Config Parser . Config Parser ( ) config . read ( [ 'setup.cfg' ] ) if not config . has section ( 'check-manifest' ) : return if ( config . has option ( 'check-manifest' , 'ignore-default-rules' ) and config . getboolean ( 'check-manifest' , 'ignore-default-rules' ) ) : del IGNORE [ : ] if config . has option ( 'check-manifest' , 'ignore' ) : patterns = [ p . strip ( ) for p in config . get ( 'check-manifest' , 'ignore' ) . splitlines ( ) ] IGNORE . extend ( p for p in patterns if p )
def file matches ( filename , patterns ) : return any ( fnmatch . fnmatch ( filename , pat ) for pat in patterns )
def get versioned files ( ) : # Git for Windows uses UTF-8 instead of the locale encoding. # Regular Git on sane POSIX systems uses the locale encoding encoding = 'UTF-8' if sys . platform == 'win32' else None output = run ( [ 'git' , 'ls-files' , '-z' ] , encoding = encoding ) return add directories ( output . split ( '\0' ) [ : - 1 ] )
def start kernel ( self , * * kwargs ) : kernel id = unicode ( uuid . uuid4 ( ) ) # use base Kernel Manager for each Kernel km = self . kernel manager factory ( connection file = os . path . join ( self . connection dir , "kernel-%s.json" % kernel id ) , config = self . config , ) km . start kernel ( * * kwargs ) # start just the shell channel, needed for graceful restart km . start channels ( shell = True , sub = False , stdin = False , hb = False ) self . kernels [ kernel id ] = km return kernel id
def notebook for kernel ( self , kernel id ) : notebook ids = [ k for k , v in self . notebook mapping . iteritems ( ) if v == kernel id ] if len ( notebook ids ) == 1 : return notebook ids [ 0 ] else : return None
def shutdown kernel ( self , kernel id ) : self . check kernel id ( kernel id ) super ( Mapping Kernel Manager , self ) . shutdown kernel ( kernel id ) self . delete mapping for kernel ( kernel id ) self . log . info ( "Kernel shutdown: %s" % kernel id )
def interrupt kernel ( self , kernel id ) : self . check kernel id ( kernel id ) super ( Mapping Kernel Manager , self ) . interrupt kernel ( kernel id ) self . log . info ( "Kernel interrupted: %s" % kernel id )
def restart kernel ( self , kernel id ) : self . check kernel id ( kernel id ) km = self . get kernel ( kernel id ) km . restart kernel ( ) self . log . info ( "Kernel restarted: %s" % kernel id ) return kernel id # the following remains, in case the KM restart machinery is # somehow unacceptable # Get the notebook id to preserve the kernel/notebook association. notebook id = self . notebook for kernel ( kernel id ) # Create the new kernel first so we can move the clients over. new kernel id = self . start kernel ( ) # Now kill the old kernel. self . kill kernel ( kernel id ) # Now save the new kernel/notebook association. We have to save it # after the old kernel is killed as that will delete the mapping. self . set kernel for notebook ( notebook id , new kernel id ) self . log . info ( "Kernel restarted: %s" % new kernel id ) return new kernel id
def create iopub stream ( self , kernel id ) : self . check kernel id ( kernel id ) return super ( Mapping Kernel Manager , self ) . create iopub stream ( kernel id )
def create shell stream ( self , kernel id ) : self . check kernel id ( kernel id ) return super ( Mapping Kernel Manager , self ) . create shell stream ( kernel id )
def create hb stream ( self , kernel id ) : self . check kernel id ( kernel id ) return super ( Mapping Kernel Manager , self ) . create hb stream ( kernel id )
def reset ( self ) : instdict = self . dict classdict = self . class . dict # To reset them, we simply remove them from the instance dict.  At that # point, it's as if they had never been computed.  On the next access, # the accessor function from the parent class will be called, simply # because that's how the python descriptor protocol works. for mname , mval in classdict . items ( ) : if mname in instdict and isinstance ( mval , One Time Property ) : delattr ( self , mname )
def ensure utf8 ( image tag ) : if py3compat . PY3 : # nothing to do on Python 3 return image tag def utf8 image tag ( * args , * * kwargs ) : s = image tag ( * args , * * kwargs ) if isinstance ( s , unicode ) : s = s . encode ( 'utf8' ) return s return utf8 image tag
def get unique or none ( klass , * args , * * kwargs ) : try : return klass . objects . get ( * args , * * kwargs ) except klass . Does Not Exist : return None except klass . Multiple Objects Returned : return None return None
def get query includes ( tokenized terms , search fields ) : query = None for term in tokenized terms : or query = None for field name in search fields : q = Q ( * * { "%s icontains" % field name : term } ) if or query is None : or query = q else : or query = or query | q if query is None : query = or query else : query = query & or query return query
def get text query ( query string , search fields ) : include terms , exclude terms = get text tokenizer ( query string ) include q = get query includes ( include terms , search fields ) exclude q = get query excludes ( exclude terms , search fields ) query = None if include q and exclude q : query = include q & ~ exclude q elif not exclude q : query = include q else : query = ~ exclude q return query
def get date greater query ( days , date field ) : query = None days = get integer ( days ) if days : past = get days ago ( days ) query = Q ( * * { "%s gte" % date field : past . isoformat ( ) } ) return query
def get date less query ( days , date field ) : query = None days = get integer ( days ) if days : future = get days from now ( days ) query = Q ( * * { "%s lte" % date field : future . isoformat ( ) } ) return query
def get null or blank query ( field = None ) : if not field : return field null q = get null query ( field ) blank q = get blank query ( field ) return ( null q | blank q )
def case insensitive ( self , fields dict ) : if hasattr ( self . model , 'CASE INSENSITIVE FIELDS' ) : for field in self . model . CASE INSENSITIVE FIELDS : if field in fields dict : fields dict [ field + ' iexact' ] = fields dict [ field ] del fields dict [ field ]
def options ( self , parser , env ) : parser . add option ( "-a" , "--attr" , dest = "attr" , action = "append" , default = env . get ( 'NOSE ATTR' ) , metavar = "ATTR" , help = "Run only tests that have attributes " "specified by ATTR [NOSE ATTR]" ) # disable in < 2.4: eval can't take needed args if compat 24 : parser . add option ( "-A" , "--eval-attr" , dest = "eval attr" , metavar = "EXPR" , action = "append" , default = env . get ( 'NOSE EVAL ATTR' ) , help = "Run only tests for whose attributes " "the Python expression EXPR evaluates " "to True [NOSE EVAL ATTR]" )
def want Method ( self , method ) : try : cls = method . im class except Attribute Error : return False return self . validate Attrib ( method , cls )
def rotate ( self ) : if self . prev yank : text = self . ring . rotate ( ) if text : self . skip cursor = True cursor = self . text edit . text Cursor ( ) cursor . move Position ( Qt Gui . Q Text Cursor . Left , Qt Gui . Q Text Cursor . Keep Anchor , n = len ( self . prev yank ) ) cursor . insert Text ( text ) self . prev yank = text
def start hb ( self , callback ) : if not self . beating : self . kernel alive = True def ping or dead ( ) : self . hb stream . flush ( ) if self . kernel alive : self . kernel alive = False self . hb stream . send ( b'ping' ) # flush stream to force immediate socket send self . hb stream . flush ( ) else : try : callback ( ) except : pass finally : self . stop hb ( ) def beat received ( msg ) : self . kernel alive = True self . hb stream . on recv ( beat received ) loop = ioloop . IO Loop . instance ( ) self . hb periodic callback = ioloop . Periodic Callback ( ping or dead , self . time to dead * 1000 , loop ) loop . add timeout ( time . time ( ) + self . first beat , self . really start hb ) self . beating = True
def stop hb ( self ) : if self . beating : self . beating = False self . hb periodic callback . stop ( ) if not self . hb stream . closed ( ) : self . hb stream . on recv ( None )
def fload ( self ) : # read data and parse into blocks if hasattr ( self , 'fobj' ) and self . fobj is not None : self . fobj . close ( ) if hasattr ( self . src , "read" ) : # It seems to be a file or a file-like object self . fobj = self . src else : # Assume it's a string or something that can be converted to one self . fobj = open ( self . fname )
def reload ( self ) : self . fload ( ) self . src = self . fobj . read ( ) src b = [ b . strip ( ) for b in self . re stop . split ( self . src ) if b ] self . silent = [ bool ( self . re silent . findall ( b ) ) for b in src b ] self . auto = [ bool ( self . re auto . findall ( b ) ) for b in src b ] # if auto all is not given (def. None), we read it from the file if self . auto all is None : self . auto all = bool ( self . re auto all . findall ( src b [ 0 ] ) ) else : self . auto all = bool ( self . auto all ) # Clean the sources from all markup so it doesn't get displayed when # running the demo src blocks = [ ] auto strip = lambda s : self . re auto . sub ( '' , s ) for i , b in enumerate ( src b ) : if self . auto [ i ] : src blocks . append ( auto strip ( b ) ) else : src blocks . append ( b ) # remove the auto all marker src blocks [ 0 ] = self . re auto all . sub ( '' , src blocks [ 0 ] ) self . nblocks = len ( src blocks ) self . src blocks = src blocks # also build syntax-highlighted source self . src blocks colored = map ( self . ip colorize , self . src blocks ) # ensure clean namespace and seek offset self . reset ( )
def show ( self , index = None ) : index = self . get index ( index ) if index is None : return print >> io . stdout , self . marquee ( '<%s> block # %s (%s remaining)' % ( self . title , index , self . nblocks - index - 1 ) ) print >> io . stdout , ( self . src blocks colored [ index ] ) sys . stdout . flush ( )
def show all ( self ) : fname = self . title title = self . title nblocks = self . nblocks silent = self . silent marquee = self . marquee for index , block in enumerate ( self . src blocks colored ) : if silent [ index ] : print >> io . stdout , marquee ( '<%s> SILENT block # %s (%s remaining)' % ( title , index , nblocks - index - 1 ) ) else : print >> io . stdout , marquee ( '<%s> block # %s (%s remaining)' % ( title , index , nblocks - index - 1 ) ) print >> io . stdout , block , sys . stdout . flush ( )
def reload ( self ) : # read data and parse into blocks self . fload ( ) lines = self . fobj . readlines ( ) src b = [ l for l in lines if l . strip ( ) ] nblocks = len ( src b ) self . src = '' . join ( lines ) self . silent = [ False ] * nblocks self . auto = [ True ] * nblocks self . auto all = True self . nblocks = nblocks self . src blocks = src b # also build syntax-highlighted source self . src blocks colored = map ( self . ip colorize , self . src blocks ) # ensure clean namespace and seek offset self . reset ( )
def thread ( function , sequence , cores = None , run Series = False , quiet = False ) : # Make the Pool of workes if cores is None : pool = Thread Pool ( ) else : pool = Thread Pool ( cores ) # Operate on the list of subjects with the requested function # in the split threads tic = time . time ( ) if run Series is False : try : results = pool . map ( function , sequence ) # close the pool and wiat for teh work to finish pool . close ( ) pool . join ( ) except : print 'thread Failed... running in series :-(' results = series ( sequence , function ) else : results = series ( sequence , function ) toc = time . time ( ) elapsed = toc - tic if quiet is False : if cores is None : print "Elapsed time: %s  :-)\n" % str ( elapsed ) else : print "Elapsed time: %s  on %s threads :-)\n" % ( str ( elapsed ) , str ( cores ) ) # Noes: # import functools # abc = map(functools.partial(sb.dist, dist Name = 'weibull'), wbldf List) return results
def with objattrs ( * names ) : def wrap ( func ) : @ functools . wraps ( func ) def wrapper ( self , * args , * * kwargs ) : with contextlib . Exit Stack ( ) as stack : for name in names : stack . enter context ( getattr ( self , name ) ) return func ( self , * args , * * kwargs ) return wrapper return wrap
def countdown ( name , date , description = '' , id = '' , granularity = 'sec' , start = None , progressbar = False , progressbar inversed = False , showpct = False ) : end date = dateparse . parse datetime ( date ) end = dateformat . format ( end date , 'U' ) content = '<div class="name">' + name + '</div>' content += '<div class="description">' + description + '</div>' if progressbar : if not end : raise Exception ( 'For progressbar, start date is requried.' ) parsed date = datetime . datetime . combine ( dateparse . parse date ( start ) , datetime . time ( ) ) start date = dateparse . parse datetime ( start ) or parsed date now = datetime . datetime . now ( ) pct = ( now - start date ) . total seconds ( ) / ( end date - start date ) . total seconds ( ) pct = int ( pct * 100 ) if progressbar inversed : pct = 100 - pct # Note: the output is for bootstrap! bar = '<div class="progress progress-striped active">' bar += '<div class="progress-bar"  role="progressbar" aria-valuenow="{pct}" aria-valuemin="0" aria-valuemax="100" style="width: {pct}%">' bar += '<span class="sr-only">{pct}% Complete</span>' bar += '</div>' bar += '</div>' if showpct : bar += '<div class="percentage">{pct}%</div>' bar = bar . format ( pct = pct ) content += bar content += '<div class="counter"></div>' attr = { 'class' : 'countdownbox' , 'data-datetime' : end , 'data-granularity' : granularity } if id : attr [ 'id' ] = id return html . tag ( 'div' , content , attr )
def cleanup ( controller , engines ) : import signal , time print ( 'Starting cleanup' ) print ( 'Stopping engines...' ) for e in engines : e . send signal ( signal . SIGINT ) print ( 'Stopping controller...' ) # so it can shut down its queues controller . send signal ( signal . SIGINT ) time . sleep ( 0.1 ) print ( 'Killing controller...' ) controller . kill ( ) print ( 'Cleanup done' )
def save ids ( f , self , * args , * * kwargs ) : n previous = len ( self . client . history ) try : ret = f ( self , * args , * * kwargs ) finally : nmsgs = len ( self . client . history ) - n previous msg ids = self . client . history [ - nmsgs : ] self . history . extend ( msg ids ) map ( self . outstanding . add , msg ids ) return ret
def sync results ( f , self , * args , * * kwargs ) : ret = f ( self , * args , * * kwargs ) delta = self . outstanding . difference ( self . client . outstanding ) completed = self . outstanding . intersection ( delta ) self . outstanding = self . outstanding . difference ( completed ) return ret
def spin after ( f , self , * args , * * kwargs ) : ret = f ( self , * args , * * kwargs ) self . spin ( ) return ret
def add record ( self , msg id , rec ) : # print rec rec = self . binary buffers ( rec ) self . records . insert ( rec )
def get record ( self , msg id ) : r = self . records . find one ( { 'msg id' : msg id } ) if not r : # r will be '' if nothing is found raise Key Error ( msg id ) return r
def update record ( self , msg id , rec ) : rec = self . binary buffers ( rec ) self . records . update ( { 'msg id' : msg id } , { '$set' : rec } )
def get history ( self ) : cursor = self . records . find ( { } , { 'msg id' : 1 } ) . sort ( 'submitted' ) return [ rec [ 'msg id' ] for rec in cursor ]
def get msgs ( self ) : msgs = [ ] while True : try : msgs . append ( self . get msg ( block = False ) ) except Empty : break return msgs
def get msg ( self , block = True , timeout = None ) : return self . in queue . get ( block , timeout )
def parse ( url ) : config = { } if not isinstance ( url , six . string types ) : url = '' url = urlparse . urlparse ( url ) # Remove query strings. path = url . path [ 1 : ] path = path . split ( '?' , 2 ) [ 0 ] # Update with environment configuration. config . update ( { 'NAME' : path , 'USER' : url . username , 'PASSWORD' : url . password , 'HOST' : url . hostname , 'PORT' : url . port , } ) if url . scheme in SCHEMES : config [ 'ENGINE' ] = SCHEMES [ url . scheme ] return config
def magic run completer ( self , event ) : comps = arg split ( event . line , strict = False ) relpath = ( len ( comps ) > 1 and comps [ - 1 ] or '' ) . strip ( "'\"" ) #print("\nev=", event)  # dbg #print("rp=", relpath)  # dbg #print('comps=', comps)  # dbg lglob = glob . glob isdir = os . path . isdir relpath , tilde expand , tilde val = expand user ( relpath ) dirs = [ f . replace ( '\\' , '/' ) + "/" for f in lglob ( relpath + '*' ) if isdir ( f ) ] # Find if the user has already typed the first filename, after which we # should complete on all files, since after the first one other files may # be arguments to the input script. if filter ( magic run re . match , comps ) : pys = [ f . replace ( '\\' , '/' ) for f in lglob ( '*' ) ] else : pys = [ f . replace ( '\\' , '/' ) for f in lglob ( relpath + '*.py' ) + lglob ( relpath + '*.ipy' ) + lglob ( relpath + '*.pyw' ) ] #print('run comp:', dirs+pys) # dbg return [ compress user ( p , tilde expand , tilde val ) for p in dirs + pys ]
def cd completer ( self , event ) : ip = get ipython ( ) relpath = event . symbol #print(event) # dbg if event . line . endswith ( '-b' ) or ' -b ' in event . line : # return only bookmark completions bkms = self . db . get ( 'bookmarks' , None ) if bkms : return bkms . keys ( ) else : return [ ] if event . symbol == '-' : width dh = str ( len ( str ( len ( ip . user ns [ ' dh' ] ) + 1 ) ) ) # jump in directory history by number fmt = '-%0' + width dh + 'd [%s]' ents = [ fmt % ( i , s ) for i , s in enumerate ( ip . user ns [ ' dh' ] ) ] if len ( ents ) > 1 : return ents return [ ] if event . symbol . startswith ( '--' ) : return [ "--" + os . path . basename ( d ) for d in ip . user ns [ ' dh' ] ] # Expand ~ in path and normalize directory separators. relpath , tilde expand , tilde val = expand user ( relpath ) relpath = relpath . replace ( '\\' , '/' ) found = [ ] for d in [ f . replace ( '\\' , '/' ) + '/' for f in glob . glob ( relpath + '*' ) if os . path . isdir ( f ) ] : if ' ' in d : # we don't want to deal with any of that, complex code # for this is elsewhere raise Try Next found . append ( d ) if not found : if os . path . isdir ( relpath ) : return [ compress user ( relpath , tilde expand , tilde val ) ] # if no completions so far, try bookmarks bks = self . db . get ( 'bookmarks' , { } ) . iterkeys ( ) bkmatches = [ s for s in bks if s . startswith ( event . symbol ) ] if bkmatches : return bkmatches raise Try Next return [ compress user ( p , tilde expand , tilde val ) for p in found ]
def quoteattr ( self , attr ) : attr = xml safe ( attr ) if isinstance ( attr , unicode ) and not UNICODE STRINGS : attr = attr . encode ( self . encoding ) return saxutils . quoteattr ( attr )
def configure ( self , options , config ) : Plugin . configure ( self , options , config ) self . config = config if self . enabled : self . stats = { 'errors' : 0 , 'failures' : 0 , 'passes' : 0 , 'skipped' : 0 } self . errorlist = [ ] self . error report file = codecs . open ( options . xunit file , 'w' , self . encoding , 'replace' )
def add Error ( self , test , err , capt = None ) : taken = self . time Taken ( ) if issubclass ( err [ 0 ] , Skip Test ) : type = 'skipped' self . stats [ 'skipped' ] += 1 else : type = 'error' self . stats [ 'errors' ] += 1 tb = '' . join ( traceback . format exception ( * err ) ) id = test . id ( ) self . errorlist . append ( '<testcase classname=%(cls)s name=%(name)s time="%(taken).3f">' '<%(type)s type=%(errtype)s message=%(message)s><![CDATA[%(tb)s]]>' '</%(type)s></testcase>' % { 'cls' : self . quoteattr ( id split ( id ) [ 0 ] ) , 'name' : self . quoteattr ( id split ( id ) [ - 1 ] ) , 'taken' : taken , 'type' : type , 'errtype' : self . quoteattr ( nice classname ( err [ 0 ] ) ) , 'message' : self . quoteattr ( exc message ( err ) ) , 'tb' : escape cdata ( tb ) , } )
def add Failure ( self , test , err , capt = None , tb info = None ) : taken = self . time Taken ( ) tb = '' . join ( traceback . format exception ( * err ) ) self . stats [ 'failures' ] += 1 id = test . id ( ) self . errorlist . append ( '<testcase classname=%(cls)s name=%(name)s time="%(taken).3f">' '<failure type=%(errtype)s message=%(message)s><![CDATA[%(tb)s]]>' '</failure></testcase>' % { 'cls' : self . quoteattr ( id split ( id ) [ 0 ] ) , 'name' : self . quoteattr ( id split ( id ) [ - 1 ] ) , 'taken' : taken , 'errtype' : self . quoteattr ( nice classname ( err [ 0 ] ) ) , 'message' : self . quoteattr ( exc message ( err ) ) , 'tb' : escape cdata ( tb ) , } )
def add Success ( self , test , capt = None ) : taken = self . time Taken ( ) self . stats [ 'passes' ] += 1 id = test . id ( ) self . errorlist . append ( '<testcase classname=%(cls)s name=%(name)s ' 'time="%(taken).3f" />' % { 'cls' : self . quoteattr ( id split ( id ) [ 0 ] ) , 'name' : self . quoteattr ( id split ( id ) [ - 1 ] ) , 'taken' : taken , } )
def register engine ( self , uid ) : # head of the line: self . targets . insert ( 0 , uid ) self . loads . insert ( 0 , 0 ) # initialize sets self . completed [ uid ] = set ( ) self . failed [ uid ] = set ( ) self . pending [ uid ] = { } # rescan the graph: self . update graph ( None )
def unregister engine ( self , uid ) : if len ( self . targets ) == 1 : # this was our only engine pass # handle any potentially finished tasks: self . engine stream . flush ( ) # don't pop destinations, because they might be used later # map(self.destinations.pop, self.completed.pop(uid)) # map(self.destinations.pop, self.failed.pop(uid)) # prevent this engine from receiving work idx = self . targets . index ( uid ) self . targets . pop ( idx ) self . loads . pop ( idx ) # wait 5 seconds before cleaning up pending jobs, since the results might # still be incoming if self . pending [ uid ] : dc = ioloop . Delayed Callback ( lambda : self . handle stranded tasks ( uid ) , 5000 , self . loop ) dc . start ( ) else : self . completed . pop ( uid ) self . failed . pop ( uid )
def handle stranded tasks ( self , engine ) : lost = self . pending [ engine ] for msg id in lost . keys ( ) : if msg id not in self . pending [ engine ] : # prevent double-handling of messages continue raw msg = lost [ msg id ] . raw msg idents , msg = self . session . feed identities ( raw msg , copy = False ) parent = self . session . unpack ( msg [ 1 ] . bytes ) idents = [ engine , idents [ 0 ] ] # build fake error reply try : raise error . Engine Error ( "Engine %r died while running task %r" % ( engine , msg id ) ) except : content = error . wrap exception ( ) # build fake header header = dict ( status = 'error' , engine = engine , date = datetime . now ( ) , ) msg = self . session . msg ( 'apply reply' , content , parent = parent , subheader = header ) raw reply = map ( zmq . Message , self . session . serialize ( msg , ident = idents ) ) # and dispatch it self . dispatch result ( raw reply ) # finally scrub completed/failed lists self . completed . pop ( engine ) self . failed . pop ( engine )
def dispatch submission ( self , raw msg ) : # ensure targets up to date: self . notifier stream . flush ( ) try : idents , msg = self . session . feed identities ( raw msg , copy = False ) msg = self . session . unserialize ( msg , content = False , copy = False ) except Exception : self . log . error ( "task::Invaid task msg: %r" % raw msg , exc info = True ) return # send to monitor self . mon stream . send multipart ( [ b'intask' ] + raw msg , copy = False ) header = msg [ 'header' ] msg id = header [ 'msg id' ] self . all ids . add ( msg id ) # get targets as a set of bytes objects # from a list of unicode objects targets = header . get ( 'targets' , [ ] ) targets = map ( cast bytes , targets ) targets = set ( targets ) retries = header . get ( 'retries' , 0 ) self . retries [ msg id ] = retries # time dependencies after = header . get ( 'after' , None ) if after : after = Dependency ( after ) if after . all : if after . success : after = Dependency ( after . difference ( self . all completed ) , success = after . success , failure = after . failure , all = after . all , ) if after . failure : after = Dependency ( after . difference ( self . all failed ) , success = after . success , failure = after . failure , all = after . all , ) if after . check ( self . all completed , self . all failed ) : # recast as empty set, if `after` already met, # to prevent unnecessary set comparisons after = MET else : after = MET # location dependencies follow = Dependency ( header . get ( 'follow' , [ ] ) ) # turn timeouts into datetime objects: timeout = header . get ( 'timeout' , None ) if timeout : # cast to float, because jsonlib returns floats as decimal.Decimal, # which timedelta does not accept timeout = datetime . now ( ) + timedelta ( 0 , float ( timeout ) , 0 ) job = Job ( msg id = msg id , raw msg = raw msg , idents = idents , msg = msg , header = header , targets = targets , after = after , follow = follow , timeout = timeout , ) # validate and reduce dependencies: for dep in after , follow : if not dep : # empty dependency continue # check valid: if msg id in dep or dep . difference ( self . all ids ) : self . depending [ msg id ] = job return self . fail unreachable ( msg id , error . Invalid Dependency ) # check if unreachable: if dep . unreachable ( self . all completed , self . all failed ) : self . depending [ msg id ] = job return self . fail unreachable ( msg id ) if after . check ( self . all completed , self . all failed ) : # time deps already met, try to run if not self . maybe run ( job ) : # can't run yet if msg id not in self . all failed : # could have failed as unreachable self . save unmet ( job ) else : self . save unmet ( job )
def audit timeouts ( self ) : now = datetime . now ( ) for msg id in self . depending . keys ( ) : # must recheck, in case one failure cascaded to another: if msg id in self . depending : job = self . depending [ msg id ] if job . timeout and job . timeout < now : self . fail unreachable ( msg id , error . Task Timeout )
def maybe run ( self , job ) : msg id = job . msg id self . log . debug ( "Attempting to assign task %s" , msg id ) if not self . targets : # no engines, definitely can't run return False if job . follow or job . targets or job . blacklist or self . hwm : # we need a can run filter def can run ( idx ) : # check hwm if self . hwm and self . loads [ idx ] == self . hwm : return False target = self . targets [ idx ] # check blacklist if target in job . blacklist : return False # check targets if job . targets and target not in job . targets : return False # check follow return job . follow . check ( self . completed [ target ] , self . failed [ target ] ) indices = filter ( can run , range ( len ( self . targets ) ) ) if not indices : # couldn't run if job . follow . all : # check follow for impossibility dests = set ( ) relevant = set ( ) if job . follow . success : relevant = self . all completed if job . follow . failure : relevant = relevant . union ( self . all failed ) for m in job . follow . intersection ( relevant ) : dests . add ( self . destinations [ m ] ) if len ( dests ) > 1 : self . depending [ msg id ] = job self . fail unreachable ( msg id ) return False if job . targets : # check blacklist+targets for impossibility job . targets . difference update ( job . blacklist ) if not job . targets or not job . targets . intersection ( self . targets ) : self . depending [ msg id ] = job self . fail unreachable ( msg id ) return False return False else : indices = None self . submit task ( job , indices ) return True
def save unmet ( self , job ) : msg id = job . msg id self . depending [ msg id ] = job # track the ids in follow or after, but not those already finished for dep id in job . after . union ( job . follow ) . difference ( self . all done ) : if dep id not in self . graph : self . graph [ dep id ] = set ( ) self . graph [ dep id ] . add ( msg id )
def submit task ( self , job , indices = None ) : if indices : loads = [ self . loads [ i ] for i in indices ] else : loads = self . loads idx = self . scheme ( loads ) if indices : idx = indices [ idx ] target = self . targets [ idx ] # print (target, map(str, msg[:3])) # send job to the engine self . engine stream . send ( target , flags = zmq . SNDMORE , copy = False ) self . engine stream . send multipart ( job . raw msg , copy = False ) # update load self . add job ( idx ) self . pending [ target ] [ job . msg id ] = job # notify Hub content = dict ( msg id = job . msg id , engine id = target . decode ( 'ascii' ) ) self . session . send ( self . mon stream , 'task destination' , content = content , ident = [ b'tracktask' , self . ident ] )
def dispatch result ( self , raw msg ) : try : idents , msg = self . session . feed identities ( raw msg , copy = False ) msg = self . session . unserialize ( msg , content = False , copy = False ) engine = idents [ 0 ] try : idx = self . targets . index ( engine ) except Value Error : pass # skip load-update for dead engines else : self . finish job ( idx ) except Exception : self . log . error ( "task::Invaid result: %r" , raw msg , exc info = True ) return header = msg [ 'header' ] parent = msg [ 'parent header' ] if header . get ( 'dependencies met' , True ) : success = ( header [ 'status' ] == 'ok' ) msg id = parent [ 'msg id' ] retries = self . retries [ msg id ] if not success and retries > 0 : # failed self . retries [ msg id ] = retries - 1 self . handle unmet dependency ( idents , parent ) else : del self . retries [ msg id ] # relay to client and update graph self . handle result ( idents , parent , raw msg , success ) # send to Hub monitor self . mon stream . send multipart ( [ b'outtask' ] + raw msg , copy = False ) else : self . handle unmet dependency ( idents , parent )
def handle result ( self , idents , parent , raw msg , success = True ) : # first, relay result to client engine = idents [ 0 ] client = idents [ 1 ] # swap ids for ROUTER-ROUTER mirror raw msg [ : 2 ] = [ client , engine ] # print (map(str, raw msg[:4])) self . client stream . send multipart ( raw msg , copy = False ) # now, update our data structures msg id = parent [ 'msg id' ] self . pending [ engine ] . pop ( msg id ) if success : self . completed [ engine ] . add ( msg id ) self . all completed . add ( msg id ) else : self . failed [ engine ] . add ( msg id ) self . all failed . add ( msg id ) self . all done . add ( msg id ) self . destinations [ msg id ] = engine self . update graph ( msg id , success )
def handle unmet dependency ( self , idents , parent ) : engine = idents [ 0 ] msg id = parent [ 'msg id' ] job = self . pending [ engine ] . pop ( msg id ) job . blacklist . add ( engine ) if job . blacklist == job . targets : self . depending [ msg id ] = job self . fail unreachable ( msg id ) elif not self . maybe run ( job ) : # resubmit failed if msg id not in self . all failed : # put it back in our dependency tree self . save unmet ( job ) if self . hwm : try : idx = self . targets . index ( engine ) except Value Error : pass # skip load-update for dead engines else : if self . loads [ idx ] == self . hwm - 1 : self . update graph ( None )
def logstate ( self ) : if self . logfile is None : print 'Logging has not been activated.' else : state = self . log active and 'active' or 'temporarily suspended' print 'Filename       :' , self . logfname print 'Mode           :' , self . logmode print 'Output logging :' , self . log output print 'Raw input log  :' , self . log raw input print 'Timestamping   :' , self . timestamp print 'State          :' , state
def log write ( self , data , kind = 'input' ) : #print 'data: %r' % data # dbg if self . log active and data : write = self . logfile . write if kind == 'input' : if self . timestamp : write ( str to unicode ( time . strftime ( '# %a, %d %b %Y %H:%M:%S\n' , time . localtime ( ) ) ) ) write ( data ) elif kind == 'output' and self . log output : odata = u'\n' . join ( [ u'#[Out]# %s' % s for s in data . splitlines ( ) ] ) write ( u'%s\n' % odata ) self . logfile . flush ( )
def new worksheet ( name = None , cells = None ) : ws = Notebook Node ( ) if name is not None : ws . name = unicode ( name ) if cells is None : ws . cells = [ ] else : ws . cells = list ( cells ) return ws
def new notebook ( metadata = None , worksheets = None ) : nb = Notebook Node ( ) nb . nbformat = 2 if worksheets is None : nb . worksheets = [ ] else : nb . worksheets = list ( worksheets ) if metadata is None : nb . metadata = new metadata ( ) else : nb . metadata = Notebook Node ( metadata ) return nb
def add s ( self , s , obj , priority = 0 ) : chain = self . strs . get ( s , Command Chain Dispatcher ( ) ) chain . add ( obj , priority ) self . strs [ s ] = chain
def add re ( self , regex , obj , priority = 0 ) : chain = self . regexs . get ( regex , Command Chain Dispatcher ( ) ) chain . add ( obj , priority ) self . regexs [ regex ] = chain
def dispatch ( self , key ) : if key in self . strs : yield self . strs [ key ] for r , obj in self . regexs . items ( ) : if re . match ( r , key ) : yield obj else : #print "nomatch",key  # dbg pass
def flat matches ( self , key ) : for val in self . dispatch ( key ) : for el in val : yield el [ 1 ] # only value, no priority return
def notebook dir changed ( self , name , old , new ) : if os . path . exists ( new ) and not os . path . isdir ( new ) : raise Trait Error ( "notebook dir %r is not a directory" % new ) if not os . path . exists ( new ) : self . log . info ( "Creating notebook dir %s" , new ) try : os . mkdir ( new ) except : raise Trait Error ( "Couldn't create notebook dir %r" % new )
def new notebook id ( self , name ) : # TODO: the following will give stable urls for notebooks, but unless # the notebooks are immediately redirected to their new urls when their # filemname changes, nasty inconsistencies result.  So for now it's # disabled and instead we use a random uuid4() call.  But we leave the # logic here so that we can later reactivate it, whhen the necessary # url redirection code is written. #notebook id = unicode(uuid.uuid5(uuid.NAMESPACE URL, #                 'file://'+self.get path by name(name).encode('utf-8'))) notebook id = unicode ( uuid . uuid4 ( ) ) self . mapping [ notebook id ] = name self . rev mapping [ name ] = notebook id return notebook id
def delete notebook id ( self , notebook id ) : name = self . mapping [ notebook id ] del self . mapping [ notebook id ] del self . rev mapping [ name ]
def notebook exists ( self , notebook id ) : if notebook id not in self . mapping : return False path = self . get path by name ( self . mapping [ notebook id ] ) return os . path . isfile ( path )
def find path ( self , notebook id ) : try : name = self . mapping [ notebook id ] except Key Error : raise web . HTTP Error ( 404 , u'Notebook does not exist: %s' % notebook id ) return self . get path by name ( name )
def get path by name ( self , name ) : filename = name + self . filename ext path = os . path . join ( self . notebook dir , filename ) return path
def get notebook ( self , notebook id , format = u'json' ) : format = unicode ( format ) if format not in self . allowed formats : raise web . HTTP Error ( 415 , u'Invalid notebook format: %s' % format ) last modified , nb = self . get notebook object ( notebook id ) kwargs = { } if format == 'json' : # don't split lines for sending over the wire, because it # should match the Python in-memory format. kwargs [ 'split lines' ] = False data = current . writes ( nb , format , * * kwargs ) name = nb . metadata . get ( 'name' , 'notebook' ) return last modified , name , data
def get notebook object ( self , notebook id ) : path = self . find path ( notebook id ) if not os . path . isfile ( path ) : raise web . HTTP Error ( 404 , u'Notebook does not exist: %s' % notebook id ) info = os . stat ( path ) last modified = datetime . datetime . utcfromtimestamp ( info . st mtime ) with open ( path , 'r' ) as f : s = f . read ( ) try : # v1 and v2 and json in the .ipynb files. nb = current . reads ( s , u'json' ) except : raise web . HTTP Error ( 500 , u'Unreadable JSON notebook.' ) # Always use the filename as the notebook name. nb . metadata . name = os . path . splitext ( os . path . basename ( path ) ) [ 0 ] return last modified , nb
def save notebook ( self , notebook id , data , name = None , format = u'json' ) : if format not in self . allowed formats : raise web . HTTP Error ( 415 , u'Invalid notebook format: %s' % format ) try : nb = current . reads ( data . decode ( 'utf-8' ) , format ) except : raise web . HTTP Error ( 400 , u'Invalid JSON data' ) if name is not None : nb . metadata . name = name self . save notebook object ( notebook id , nb )
def save notebook object ( self , notebook id , nb ) : if notebook id not in self . mapping : raise web . HTTP Error ( 404 , u'Notebook does not exist: %s' % notebook id ) old name = self . mapping [ notebook id ] try : new name = nb . metadata . name except Attribute Error : raise web . HTTP Error ( 400 , u'Missing notebook name' ) path = self . get path by name ( new name ) try : with open ( path , 'w' ) as f : current . write ( nb , f , u'json' ) except Exception as e : raise web . HTTP Error ( 400 , u'Unexpected error while saving notebook: %s' % e ) # save .py script as well if self . save script : pypath = os . path . splitext ( path ) [ 0 ] + '.py' try : with io . open ( pypath , 'w' , encoding = 'utf-8' ) as f : current . write ( nb , f , u'py' ) except Exception as e : raise web . HTTP Error ( 400 , u'Unexpected error while saving notebook as script: %s' % e ) if old name != new name : old path = self . get path by name ( old name ) if os . path . isfile ( old path ) : os . unlink ( old path ) if self . save script : old pypath = os . path . splitext ( old path ) [ 0 ] + '.py' if os . path . isfile ( old pypath ) : os . unlink ( old pypath ) self . mapping [ notebook id ] = new name self . rev mapping [ new name ] = notebook id del self . rev mapping [ old name ]
def delete notebook ( self , notebook id ) : path = self . find path ( notebook id ) if not os . path . isfile ( path ) : raise web . HTTP Error ( 404 , u'Notebook does not exist: %s' % notebook id ) os . unlink ( path ) self . delete notebook id ( notebook id )
def new notebook ( self ) : path , name = self . increment filename ( 'Untitled' ) notebook id = self . new notebook id ( name ) metadata = current . new metadata ( name = name ) nb = current . new notebook ( metadata = metadata ) with open ( path , 'w' ) as f : current . write ( nb , f , u'json' ) return notebook id
def copy notebook ( self , notebook id ) : last mod , nb = self . get notebook object ( notebook id ) name = nb . metadata . name + '-Copy' path , name = self . increment filename ( name ) nb . metadata . name = name notebook id = self . new notebook id ( name ) self . save notebook object ( notebook id , nb ) return notebook id
def make report ( self , traceback ) : sec sep = self . section sep # Start with parent report report = [ super ( IP App Crash Handler , self ) . make report ( traceback ) ] # Add interactive-specific info we may have rpt add = report . append try : rpt add ( sec sep + "History of session input:" ) for line in self . app . shell . user ns [ ' ih' ] : rpt add ( line ) rpt add ( '\n*** Last line of input (may not be in above history):\n' ) rpt add ( self . app . shell . last input line + '\n' ) except : pass return '' . join ( report )
def classes default ( self ) : return [ Interactive Shell App , # Shell App comes before Terminal App, because self . class , # it will also affect subclasses (e.g. Qt Console) Terminal Interactive Shell , Prompt Manager , History Manager , Profile Dir , Plain Text Formatter , IP Completer , Script Magics , ]
def parse command line ( self , argv = None ) : argv = sys . argv [ 1 : ] if argv is None else argv if '-pylab' in argv : # deprecated `-pylab` given, # warn and transform into current syntax argv = argv [ : ] # copy, don't clobber idx = argv . index ( '-pylab' ) warn . warn ( "`-pylab` flag has been deprecated.\n" "    Use `--pylab` instead, or `--pylab=foo` to specify a backend." ) sub = '--pylab' if len ( argv ) > idx + 1 : # check for gui arg, as in '-pylab qt' gui = argv [ idx + 1 ] if gui in ( 'wx' , 'qt' , 'qt4' , 'gtk' , 'auto' ) : sub = '--pylab=' + gui argv . pop ( idx + 1 ) argv [ idx ] = sub return super ( Terminal I Python App , self ) . parse command line ( argv )
def initialize ( self , argv = None ) : super ( Terminal I Python App , self ) . initialize ( argv ) if self . subapp is not None : # don't bother initializing further, starting subapp return if not self . ignore old config : check for old config ( self . ipython dir ) # print self.extra args if self . extra args and not self . something to run : self . file to run = self . extra args [ 0 ] self . init path ( ) # create the shell self . init shell ( ) # and draw the banner self . init banner ( ) # Now a variety of things that happen after the banner is printed. self . init gui pylab ( ) self . init extensions ( ) self . init code ( )
def init shell ( self ) : # Create an Interactive Shell instance. # shell.display banner should always be False for the terminal # based app, because we call shell.show banner() by hand below # so the banner shows *before* all extension loading stuff. self . shell = Terminal Interactive Shell . instance ( config = self . config , display banner = False , profile dir = self . profile dir , ipython dir = self . ipython dir ) self . shell . configurables . append ( self )
def init banner ( self ) : if self . display banner and self . interact : self . shell . show banner ( ) # Make sure there is a space below the banner. if self . log level <= logging . INFO : print
def pylab changed ( self , name , old , new ) : if new == 'inline' : warn . warn ( "'inline' not available as pylab backend, " "using 'auto' instead.\n" ) self . pylab = 'auto'
def trait metadata ( self , traitname , key ) : try : trait = getattr ( self . class , traitname ) except Attribute Error : raise Trait Error ( "Class %s does not have a trait named %s" % ( self . class . name , traitname ) ) else : return trait . get metadata ( key )
def validate ( self , obj , value ) : try : if issubclass ( value , self . klass ) : return value except : if ( value is None ) and ( self . allow none ) : return value self . error ( obj , value )
def info ( self ) : if isinstance ( self . klass , basestring ) : klass = self . klass else : klass = self . klass . name result = 'a subclass of ' + klass if self . allow none : return result + ' or None' return result
def info ( self ) : result = 'any of ' + repr ( self . values ) if self . allow none : return result + ' or None' return result
def check ( self , completed , failed = None ) : if len ( self ) == 0 : return True against = set ( ) if self . success : against = completed if failed is not None and self . failure : against = against . union ( failed ) if self . all : return self . issubset ( against ) else : return not self . isdisjoint ( against )
def unreachable ( self , completed , failed = None ) : if len ( self ) == 0 : return False against = set ( ) if not self . success : against = completed if failed is not None and not self . failure : against = against . union ( failed ) if self . all : return not self . isdisjoint ( against ) else : return self . issubset ( against )
def as dict ( self ) : return dict ( dependencies = list ( self ) , all = self . all , success = self . success , failure = self . failure )
def Ainv ( self ) : if not hasattr ( self , ' Ainv' ) : self . Ainv = self . Solver ( self . A ) return self . Ainv
def Ainv ( self ) : if getattr ( self , ' Ainv' , None ) is None : self . Ainv = self . Solver ( self . A , 13 ) self . Ainv . run pardiso ( 12 ) return self . Ainv
def depth ( n , tree ) : d = 0 parent = tree [ n ] while parent is not None : d += 1 parent = tree [ parent ] return d
def print bintree ( tree , indent = '  ' ) : for n in sorted ( tree . keys ( ) ) : print "%s%s" % ( indent * depth ( n , tree ) , n )
def disambiguate dns url ( url , location ) : if not ip pat . match ( location ) : location = socket . gethostbyname ( location ) return disambiguate url ( url , location )
def allreduce ( self , f , value , flat = True ) : return self . reduce ( f , value , flat = flat , all = True )
def validate targets ( self , targets ) : if targets is None : # default to all return self . ids if isinstance ( targets , ( int , str , unicode ) ) : # only one target specified targets = [ targets ] targets = [ ] for t in targets : # map raw identities to ids if isinstance ( t , ( str , unicode ) ) : t = self . by ident . get ( cast bytes ( t ) , t ) targets . append ( t ) targets = targets bad targets = [ t for t in targets if t not in self . ids ] if bad targets : raise Index Error ( "No Such Engine: %r" % bad targets ) if not targets : raise Index Error ( "No Engines Registered" ) return targets
def dispatch query ( self , msg ) : try : idents , msg = self . session . feed identities ( msg ) except Value Error : idents = [ ] if not idents : self . log . error ( "Bad Query Message: %r" , msg ) return client id = idents [ 0 ] try : msg = self . session . unserialize ( msg , content = True ) except Exception : content = error . wrap exception ( ) self . log . error ( "Bad Query Message: %r" , msg , exc info = True ) self . session . send ( self . query , "hub error" , ident = client id , content = content ) return # print client id, header, parent, content #switch on message type: msg type = msg [ 'header' ] [ 'msg type' ] self . log . info ( "client::client %r requested %r" , client id , msg type ) handler = self . query handlers . get ( msg type , None ) try : assert handler is not None , "Bad Message Type: %r" % msg type except : content = error . wrap exception ( ) self . log . error ( "Bad Message Type: %r" , msg type , exc info = True ) self . session . send ( self . query , "hub error" , ident = client id , content = content ) return else : handler ( idents , msg )
def save task request ( self , idents , msg ) : client id = idents [ 0 ] try : msg = self . session . unserialize ( msg ) except Exception : self . log . error ( "task::client %r sent invalid task message: %r" , client id , msg , exc info = True ) return record = init record ( msg ) record [ 'client uuid' ] = client id . decode ( 'ascii' ) record [ 'queue' ] = 'task' header = msg [ 'header' ] msg id = header [ 'msg id' ] self . pending . add ( msg id ) self . unassigned . add ( msg id ) try : # it's posible iopub arrived first: existing = self . db . get record ( msg id ) if existing [ 'resubmitted' ] : for key in ( 'submitted' , 'client uuid' , 'buffers' ) : # don't clobber these keys on resubmit # submitted and client uuid should be different # and buffers might be big, and shouldn't have changed record . pop ( key ) # still check content,header which should not change # but are not expensive to compare as buffers for key , evalue in existing . iteritems ( ) : if key . endswith ( 'buffers' ) : # don't compare buffers continue rvalue = record . get ( key , None ) if evalue and rvalue and evalue != rvalue : self . log . warn ( "conflicting initial state for record: %r:%r <%r> %r" , msg id , rvalue , key , evalue ) elif evalue and not rvalue : record [ key ] = evalue try : self . db . update record ( msg id , record ) except Exception : self . log . error ( "DB Error updating record %r" , msg id , exc info = True ) except Key Error : try : self . db . add record ( msg id , record ) except Exception : self . log . error ( "DB Error adding record %r" , msg id , exc info = True ) except Exception : self . log . error ( "DB Error saving task request %r" , msg id , exc info = True )
def save task result ( self , idents , msg ) : client id = idents [ 0 ] try : msg = self . session . unserialize ( msg ) except Exception : self . log . error ( "task::invalid task result message send to %r: %r" , client id , msg , exc info = True ) return parent = msg [ 'parent header' ] if not parent : # print msg self . log . warn ( "Task %r had no parent!" , msg ) return msg id = parent [ 'msg id' ] if msg id in self . unassigned : self . unassigned . remove ( msg id ) header = msg [ 'header' ] engine uuid = header . get ( 'engine' , u'' ) eid = self . by ident . get ( cast bytes ( engine uuid ) , None ) status = header . get ( 'status' , None ) if msg id in self . pending : self . log . info ( "task::task %r finished on %s" , msg id , eid ) self . pending . remove ( msg id ) self . all completed . add ( msg id ) if eid is not None : if status != 'aborted' : self . completed [ eid ] . append ( msg id ) if msg id in self . tasks [ eid ] : self . tasks [ eid ] . remove ( msg id ) completed = header [ 'date' ] started = header . get ( 'started' , None ) result = { 'result header' : header , 'result content' : msg [ 'content' ] , 'started' : started , 'completed' : completed , 'received' : datetime . now ( ) , 'engine uuid' : engine uuid , } result [ 'result buffers' ] = msg [ 'buffers' ] try : self . db . update record ( msg id , result ) except Exception : self . log . error ( "DB Error saving task request %r" , msg id , exc info = True ) else : self . log . debug ( "task::unknown task %r finished" , msg id )
def save iopub message ( self , topics , msg ) : # print (topics) try : msg = self . session . unserialize ( msg , content = True ) except Exception : self . log . error ( "iopub::invalid IO Pub message" , exc info = True ) return parent = msg [ 'parent header' ] if not parent : self . log . warn ( "iopub::IO Pub message lacks parent: %r" , msg ) return msg id = parent [ 'msg id' ] msg type = msg [ 'header' ] [ 'msg type' ] content = msg [ 'content' ] # ensure msg id is in db try : rec = self . db . get record ( msg id ) except Key Error : rec = empty record ( ) rec [ 'msg id' ] = msg id self . db . add record ( msg id , rec ) # stream d = { } if msg type == 'stream' : name = content [ 'name' ] s = rec [ name ] or '' d [ name ] = s + content [ 'data' ] elif msg type == 'pyerr' : d [ 'pyerr' ] = content elif msg type == 'pyin' : d [ 'pyin' ] = content [ 'code' ] elif msg type in ( 'display data' , 'pyout' ) : d [ msg type ] = content elif msg type == 'status' : pass else : self . log . warn ( "unhandled iopub msg type: %r" , msg type ) if not d : return try : self . db . update record ( msg id , d ) except Exception : self . log . error ( "DB Error saving iopub message %r" , msg id , exc info = True )
def connection request ( self , client id , msg ) : self . log . info ( "client::client %r connected" , client id ) content = dict ( status = 'ok' ) content . update ( self . client info ) jsonable = { } for k , v in self . keytable . iteritems ( ) : if v not in self . dead engines : jsonable [ str ( k ) ] = v . decode ( 'ascii' ) content [ 'engines' ] = jsonable self . session . send ( self . query , 'connection reply' , content , parent = msg , ident = client id )
def register engine ( self , reg , msg ) : content = msg [ 'content' ] try : queue = cast bytes ( content [ 'queue' ] ) except Key Error : self . log . error ( "registration::queue not specified" , exc info = True ) return heart = content . get ( 'heartbeat' , None ) if heart : heart = cast bytes ( heart ) """register a new engine, and create the socket(s) necessary""" eid = self . next id # print (eid, queue, reg, heart) self . log . debug ( "registration::register engine(%i, %r, %r, %r)" , eid , queue , reg , heart ) content = dict ( id = eid , status = 'ok' ) content . update ( self . engine info ) # check if requesting available I Ds: if queue in self . by ident : try : raise Key Error ( "queue id %r in use" % queue ) except : content = error . wrap exception ( ) self . log . error ( "queue id %r in use" , queue , exc info = True ) elif heart in self . hearts : # need to check unique hearts? try : raise Key Error ( "heart id %r in use" % heart ) except : self . log . error ( "heart id %r in use" , heart , exc info = True ) content = error . wrap exception ( ) else : for h , pack in self . incoming registrations . iteritems ( ) : if heart == h : try : raise Key Error ( "heart id %r in use" % heart ) except : self . log . error ( "heart id %r in use" , heart , exc info = True ) content = error . wrap exception ( ) break elif queue == pack [ 1 ] : try : raise Key Error ( "queue id %r in use" % queue ) except : self . log . error ( "queue id %r in use" , queue , exc info = True ) content = error . wrap exception ( ) break msg = self . session . send ( self . query , "registration reply" , content = content , ident = reg ) if content [ 'status' ] == 'ok' : if heart in self . heartmonitor . hearts : # already beating self . incoming registrations [ heart ] = ( eid , queue , reg [ 0 ] , None ) self . finish registration ( heart ) else : purge = lambda : self . purge stalled registration ( heart ) dc = ioloop . Delayed Callback ( purge , self . registration timeout , self . loop ) dc . start ( ) self . incoming registrations [ heart ] = ( eid , queue , reg [ 0 ] , dc ) else : self . log . error ( "registration::registration %i failed: %r" , eid , content [ 'evalue' ] ) return eid
def unregister engine ( self , ident , msg ) : try : eid = msg [ 'content' ] [ 'id' ] except : self . log . error ( "registration::bad engine id for unregistration: %r" , ident , exc info = True ) return self . log . info ( "registration::unregister engine(%r)" , eid ) # print (eid) uuid = self . keytable [ eid ] content = dict ( id = eid , queue = uuid . decode ( 'ascii' ) ) self . dead engines . add ( uuid ) # self.ids.remove(eid) # uuid = self.keytable.pop(eid) # # ec = self.engines.pop(eid) # self.hearts.pop(ec.heartbeat) # self.by ident.pop(ec.queue) # self.completed.pop(eid) handleit = lambda : self . handle stranded msgs ( eid , uuid ) dc = ioloop . Delayed Callback ( handleit , self . registration timeout , self . loop ) dc . start ( ) ############## TODO: HANDLE IT ################ if self . notifier : self . session . send ( self . notifier , "unregistration notification" , content = content )
def shutdown request ( self , client id , msg ) : self . session . send ( self . query , 'shutdown reply' , content = { 'status' : 'ok' } , ident = client id ) # also notify other clients of shutdown self . session . send ( self . notifier , 'shutdown notice' , content = { 'status' : 'ok' } ) dc = ioloop . Delayed Callback ( lambda : self . shutdown ( ) , 1000 , self . loop ) dc . start ( )
def resubmit task ( self , client id , msg ) : def finish ( reply ) : self . session . send ( self . query , 'resubmit reply' , content = reply , ident = client id ) content = msg [ 'content' ] msg ids = content [ 'msg ids' ] reply = dict ( status = 'ok' ) try : records = self . db . find records ( { 'msg id' : { '$in' : msg ids } } , keys = [ 'header' , 'content' , 'buffers' ] ) except Exception : self . log . error ( 'db::db error finding tasks to resubmit' , exc info = True ) return finish ( error . wrap exception ( ) ) # validate msg ids found ids = [ rec [ 'msg id' ] for rec in records ] pending ids = [ msg id for msg id in found ids if msg id in self . pending ] if len ( records ) > len ( msg ids ) : try : raise Runtime Error ( "DB appears to be in an inconsistent state." "More matching records were found than should exist" ) except Exception : return finish ( error . wrap exception ( ) ) elif len ( records ) < len ( msg ids ) : missing = [ m for m in msg ids if m not in found ids ] try : raise Key Error ( "No such msg(s): %r" % missing ) except Key Error : return finish ( error . wrap exception ( ) ) elif pending ids : pass # no need to raise on resubmit of pending task, now that we # resubmit under new ID, but do we want to raise anyway? # msg id = invalid ids[0] # try: #     raise Value Error("Task(s) %r appears to be inflight" % ) # except Exception: #     return finish(error.wrap exception()) # mapping of original I Ds to resubmitted I Ds resubmitted = { } # send the messages for rec in records : header = rec [ 'header' ] msg = self . session . msg ( header [ 'msg type' ] , parent = header ) msg id = msg [ 'msg id' ] msg [ 'content' ] = rec [ 'content' ] # use the old header, but update msg id and timestamp fresh = msg [ 'header' ] header [ 'msg id' ] = fresh [ 'msg id' ] header [ 'date' ] = fresh [ 'date' ] msg [ 'header' ] = header self . session . send ( self . resubmit , msg , buffers = rec [ 'buffers' ] ) resubmitted [ rec [ 'msg id' ] ] = msg id self . pending . add ( msg id ) msg [ 'buffers' ] = rec [ 'buffers' ] try : self . db . add record ( msg id , init record ( msg ) ) except Exception : self . log . error ( "db::DB Error updating record: %s" , msg id , exc info = True ) finish ( dict ( status = 'ok' , resubmitted = resubmitted ) ) # store the new I Ds in the Task DB for msg id , resubmit id in resubmitted . iteritems ( ) : try : self . db . update record ( msg id , { 'resubmitted' : resubmit id } ) except Exception : self . log . error ( "db::DB Error updating record: %s" , msg id , exc info = True )
def extract record ( self , rec ) : io dict = { } for key in ( 'pyin' , 'pyout' , 'pyerr' , 'stdout' , 'stderr' ) : io dict [ key ] = rec [ key ] content = { 'result content' : rec [ 'result content' ] , 'header' : rec [ 'header' ] , 'result header' : rec [ 'result header' ] , 'received' : rec [ 'received' ] , 'io' : io dict , } if rec [ 'result buffers' ] : buffers = map ( bytes , rec [ 'result buffers' ] ) else : buffers = [ ] return content , buffers
def get results ( self , client id , msg ) : content = msg [ 'content' ] msg ids = sorted ( set ( content [ 'msg ids' ] ) ) statusonly = content . get ( 'status only' , False ) pending = [ ] completed = [ ] content = dict ( status = 'ok' ) content [ 'pending' ] = pending content [ 'completed' ] = completed buffers = [ ] if not statusonly : try : matches = self . db . find records ( dict ( msg id = { '$in' : msg ids } ) ) # turn match list into dict, for faster lookup records = { } for rec in matches : records [ rec [ 'msg id' ] ] = rec except Exception : content = error . wrap exception ( ) self . session . send ( self . query , "result reply" , content = content , parent = msg , ident = client id ) return else : records = { } for msg id in msg ids : if msg id in self . pending : pending . append ( msg id ) elif msg id in self . all completed : completed . append ( msg id ) if not statusonly : c , bufs = self . extract record ( records [ msg id ] ) content [ msg id ] = c buffers . extend ( bufs ) elif msg id in records : if rec [ 'completed' ] : completed . append ( msg id ) c , bufs = self . extract record ( records [ msg id ] ) content [ msg id ] = c buffers . extend ( bufs ) else : pending . append ( msg id ) else : try : raise Key Error ( 'No such message: ' + msg id ) except : content = error . wrap exception ( ) break self . session . send ( self . query , "result reply" , content = content , parent = msg , ident = client id , buffers = buffers )
def get history ( self , client id , msg ) : try : msg ids = self . db . get history ( ) except Exception as e : content = error . wrap exception ( ) else : content = dict ( status = 'ok' , history = msg ids ) self . session . send ( self . query , "history reply" , content = content , parent = msg , ident = client id )
def db query ( self , client id , msg ) : content = msg [ 'content' ] query = content . get ( 'query' , { } ) keys = content . get ( 'keys' , None ) buffers = [ ] empty = list ( ) try : records = self . db . find records ( query , keys ) except Exception as e : content = error . wrap exception ( ) else : # extract buffers from reply content: if keys is not None : buffer lens = [ ] if 'buffers' in keys else None result buffer lens = [ ] if 'result buffers' in keys else None else : buffer lens = None result buffer lens = None for rec in records : # buffers may be None, so double check b = rec . pop ( 'buffers' , empty ) or empty if buffer lens is not None : buffer lens . append ( len ( b ) ) buffers . extend ( b ) rb = rec . pop ( 'result buffers' , empty ) or empty if result buffer lens is not None : result buffer lens . append ( len ( rb ) ) buffers . extend ( rb ) content = dict ( status = 'ok' , records = records , buffer lens = buffer lens , result buffer lens = result buffer lens ) # self.log.debug (content) self . session . send ( self . query , "db reply" , content = content , parent = msg , ident = client id , buffers = buffers )
def cd ( self , newdir ) : prevdir = os . getcwd ( ) os . chdir ( newdir ) try : yield finally : os . chdir ( prevdir )
def decode cmd out ( self , completed cmd ) : try : stdout = completed cmd . stdout . encode ( 'utf-8' ) . decode ( ) except Attribute Error : try : stdout = str ( bytes ( completed cmd . stdout ) , 'big5' ) . strip ( ) except Attribute Error : stdout = str ( bytes ( completed cmd . stdout ) . decode ( 'utf-8' ) ) . strip ( ) try : stderr = completed cmd . stderr . encode ( 'utf-8' ) . decode ( ) except Attribute Error : try : stderr = str ( bytes ( completed cmd . stderr ) , 'big5' ) . strip ( ) except Attribute Error : stderr = str ( bytes ( completed cmd . stderr ) . decode ( 'utf-8' ) ) . strip ( ) return Parsed Completed Command ( completed cmd . returncode , completed cmd . args , stdout , stderr )
def run command under r root ( self , cmd , catched = True ) : RPATH = self . path with self . cd ( newdir = RPATH ) : if catched : process = sp . run ( cmd , stdout = sp . PIPE , stderr = sp . PIPE ) else : process = sp . run ( cmd ) return process
def get installed version ( name ) : pattern = re . compile ( r'''Installed:\s+(?P<version>.*)''' ) cmd = 'apt-cache policy %s' % name args = shlex . split ( cmd ) try : output = subprocess . check output ( args ) if not output : return None except Called Process Error : return None # check output match = pattern . search ( output ) if match : version = match . groupdict ( ) [ 'version' ] if version == '(none)' : return None else : return version
def squash unicode ( obj ) : if isinstance ( obj , dict ) : for key in obj . keys ( ) : obj [ key ] = squash unicode ( obj [ key ] ) if isinstance ( key , unicode ) : obj [ squash unicode ( key ) ] = obj . pop ( key ) elif isinstance ( obj , list ) : for i , v in enumerate ( obj ) : obj [ i ] = squash unicode ( v ) elif isinstance ( obj , unicode ) : obj = obj . encode ( 'utf8' ) return obj
def extract header ( msg or header ) : if not msg or header : return { } try : # See if msg or header is the entire message. h = msg or header [ 'header' ] except Key Error : try : # See if msg or header is just the header h = msg or header [ 'msg id' ] except Key Error : raise else : h = msg or header if not isinstance ( h , dict ) : h = dict ( h ) return h
def check packers ( self ) : pack = self . pack unpack = self . unpack # check simple serialization msg = dict ( a = [ 1 , 'hi' ] ) try : packed = pack ( msg ) except Exception : raise Value Error ( "packer could not serialize a simple message" ) # ensure packed message is bytes if not isinstance ( packed , bytes ) : raise Value Error ( "message packed to %r, but bytes are required" % type ( packed ) ) # check that unpack is pack's inverse try : unpacked = unpack ( packed ) except Exception : raise Value Error ( "unpacker could not handle the packer's output" ) # check datetime support msg = dict ( t = datetime . now ( ) ) try : unpacked = unpack ( pack ( msg ) ) except Exception : self . pack = lambda o : pack ( squash dates ( o ) ) self . unpack = lambda s : extract dates ( unpack ( s ) )
def object info ( * * kw ) : infodict = dict ( izip longest ( info fields , [ None ] ) ) infodict . update ( kw ) return infodict
def head ( self , h ) : return '%s%s%s' % ( self . color table . active colors . header , h , self . color table . active colors . normal )
def noinfo ( self , msg , oname ) : print 'No %s found' % msg , if oname : print 'for %s' % oname else : print
def psource ( self , obj , oname = '' ) : # Flush the source cache because inspect can return out-of-date source linecache . checkcache ( ) try : src = getsource ( obj ) except : self . noinfo ( 'source' , oname ) else : page . page ( self . format ( py3compat . unicode to str ( src ) ) )
def pfile ( self , obj , oname = '' ) : lineno = find source lines ( obj ) if lineno is None : self . noinfo ( 'file' , oname ) return ofile = find file ( obj ) # run contents of file through pager starting at line where the object # is defined, as long as the file isn't binary and is actually on the # filesystem. if ofile . endswith ( ( '.so' , '.dll' , '.pyd' ) ) : print 'File %r is binary, not printing.' % ofile elif not os . path . isfile ( ofile ) : print 'File %r does not exist, not printing.' % ofile else : # Print only text files, not extension binaries.  Note that # getsourcelines returns lineno with 1-offset and page() uses # 0-offset, so we must adjust. page . page ( self . format ( open ( ofile ) . read ( ) ) , lineno - 1 )
def print figure ( fig , fmt = 'png' ) : # When there's an empty figure, we shouldn't return anything, otherwise we # get big blank areas in the qt console. if not fig . axes and not fig . lines : return fc = fig . get facecolor ( ) ec = fig . get edgecolor ( ) fig . set facecolor ( 'white' ) fig . set edgecolor ( 'white' ) try : bytes io = Bytes IO ( ) fig . canvas . print figure ( bytes io , format = fmt , bbox inches = 'tight' ) data = bytes io . getvalue ( ) finally : fig . set facecolor ( fc ) fig . set edgecolor ( ec ) return data
def activate matplotlib ( backend ) : import matplotlib if backend . startswith ( 'module://' ) : # Work around bug in matplotlib: matplotlib.use converts the # backend id to lowercase even if a module name is specified! matplotlib . rc Params [ 'backend' ] = backend else : matplotlib . use ( backend ) matplotlib . interactive ( True ) # This must be imported last in the matplotlib series, after # backend/interactivity choices have been made import matplotlib . pylab as pylab # XXX For now leave this commented out, but depending on discussions with # mpl-dev, we may be able to allow interactive switching... #import matplotlib.pyplot #matplotlib.pyplot.switch backend(backend) pylab . show . needmain = False # We need to detect at runtime whether show() is called by the user. # For this, we wrap it into a decorator which adds a 'called' flag. pylab . draw if interactive = flag calls ( pylab . draw if interactive )
def import pylab ( user ns , import all = True ) : # Import numpy as np/pyplot as plt are conventions we're trying to # somewhat standardize on.  Making them available to users by default # will greatly help this. s = ( "import numpy\n" "import matplotlib\n" "from matplotlib import pylab, mlab, pyplot\n" "np = numpy\n" "plt = pyplot\n" ) exec s in user ns if import all : s = ( "from matplotlib.pylab import *\n" "from numpy import *\n" ) exec s in user ns
def trace ( self , frame , event , arg unused ) : if self . stopped : return if 0 : sys . stderr . write ( "trace event: %s %r @%d\n" % ( event , frame . f code . co filename , frame . f lineno ) ) if self . last exc back : if frame == self . last exc back : # Someone forgot a return event. if self . arcs and self . cur file data : pair = ( self . last line , - self . last exc firstlineno ) self . cur file data [ pair ] = None self . cur file data , self . last line = self . data stack . pop ( ) self . last exc back = None if event == 'call' : # Entering a new function context.  Decide if we should trace # in this file. self . data stack . append ( ( self . cur file data , self . last line ) ) filename = frame . f code . co filename if filename not in self . should trace cache : tracename = self . should trace ( filename , frame ) self . should trace cache [ filename ] = tracename else : tracename = self . should trace cache [ filename ] #print("called, stack is %d deep, tracename is %r" % ( #               len(self.data stack), tracename)) if tracename : if tracename not in self . data : self . data [ tracename ] = { } self . cur file data = self . data [ tracename ] else : self . cur file data = None # Set the last line to -1 because the next arc will be entering a # code block, indicated by (-1, n). self . last line = - 1 elif event == 'line' : # Record an executed line. if self . cur file data is not None : if self . arcs : #print("lin", self.last line, frame.f lineno) self . cur file data [ ( self . last line , frame . f lineno ) ] = None else : #print("lin", frame.f lineno) self . cur file data [ frame . f lineno ] = None self . last line = frame . f lineno elif event == 'return' : if self . arcs and self . cur file data : first = frame . f code . co firstlineno self . cur file data [ ( self . last line , - first ) ] = None # Leaving this function, pop the filename stack. self . cur file data , self . last line = self . data stack . pop ( ) #print("returned, stack is %d deep" % (len(self.data stack))) elif event == 'exception' : #print("exc", self.last line, frame.f lineno) self . last exc back = frame . f back self . last exc firstlineno = frame . f code . co firstlineno return self . trace
def stop ( self ) : self . stopped = True if self . thread != threading . current Thread ( ) : # Called on a different thread than started us: we can't unhook # ourseves, but we've set the flag that we should stop, so we won't # do any more tracing. return if hasattr ( sys , "gettrace" ) and self . warn : if sys . gettrace ( ) != self . trace : msg = "Trace function changed, measurement is likely wrong: %r" self . warn ( msg % ( sys . gettrace ( ) , ) ) #print("Stopping tracer on %s" % threading.current thread().ident) sys . settrace ( None )
def start tracer ( self ) : tracer = self . trace class ( ) tracer . data = self . data tracer . arcs = self . branch tracer . should trace = self . should trace tracer . should trace cache = self . should trace cache tracer . warn = self . warn fn = tracer . start ( ) self . tracers . append ( tracer ) return fn
def installation trace ( self , frame unused , event unused , arg unused ) : # Remove ourselves as the trace function sys . settrace ( None ) # Install the real tracer. fn = self . start tracer ( ) # Invoke the real trace function with the current event, to be sure # not to lose an event. if fn : fn = fn ( frame unused , event unused , arg unused ) # Return the new trace function to continue tracing in this scope. return fn
def start ( self ) : if self . collectors : self . collectors [ - 1 ] . pause ( ) self . collectors . append ( self ) #print("Started: %r" % self. collectors, file=sys.stderr) # Check to see whether we had a fullcoverage tracer installed. traces0 = [ ] if hasattr ( sys , "gettrace" ) : fn0 = sys . gettrace ( ) if fn0 : tracer0 = getattr ( fn0 , ' self ' , None ) if tracer0 : traces0 = getattr ( tracer0 , 'traces' , [ ] ) # Install the tracer on this thread. fn = self . start tracer ( ) for args in traces0 : ( frame , event , arg ) , lineno = args try : fn ( frame , event , arg , lineno = lineno ) except Type Error : raise Exception ( "fullcoverage must be run with the C trace function." ) # Install our installation tracer in threading, to jump start other # threads. threading . settrace ( self . installation trace )
def stop ( self ) : #print >>sys.stderr, "Stopping: %r" % self. collectors assert self . collectors assert self . collectors [ - 1 ] is self self . pause ( ) self . tracers = [ ] # Remove this Collector from the stack, and resume the one underneath # (if any). self . collectors . pop ( ) if self . collectors : self . collectors [ - 1 ] . resume ( )
def pause ( self ) : for tracer in self . tracers : tracer . stop ( ) stats = tracer . get stats ( ) if stats : print ( "\n Coverage.py tracer stats:" ) for k in sorted ( stats . keys ( ) ) : print ( "%16s: %s" % ( k , stats [ k ] ) ) threading . settrace ( None )
def resume ( self ) : for tracer in self . tracers : tracer . start ( ) threading . settrace ( self . installation trace )
def new code cell ( code = None , prompt number = None ) : cell = Notebook Node ( ) cell . cell type = u'code' if code is not None : cell . code = unicode ( code ) if prompt number is not None : cell . prompt number = int ( prompt number ) return cell
def new text cell ( text = None ) : cell = Notebook Node ( ) if text is not None : cell . text = unicode ( text ) cell . cell type = u'text' return cell
def new notebook ( cells = None ) : nb = Notebook Node ( ) if cells is not None : nb . cells = cells else : nb . cells = [ ] return nb
def render traceback ( self , excid = None ) : lines = [ ] if excid is None : for ( en , ev , etb , ei ) in self . elist : lines . append ( self . get engine str ( ei ) ) lines . extend ( ( etb or 'No traceback available' ) . splitlines ( ) ) lines . append ( '' ) else : try : en , ev , etb , ei = self . elist [ excid ] except : raise Index Error ( "an exception with index %i does not exist" % excid ) else : lines . append ( self . get engine str ( ei ) ) lines . extend ( ( etb or 'No traceback available' ) . splitlines ( ) ) return lines
def canonical dir ( self , morf ) : return os . path . split ( Code Unit ( morf , self . file locator ) . filename ) [ 0 ]
def source for file ( self , filename ) : if not filename . endswith ( ".py" ) : if filename [ - 4 : - 1 ] == ".py" : filename = filename [ : - 1 ] elif filename . endswith ( "$py.class" ) : # jython filename = filename [ : - 9 ] + ".py" return filename
def warn ( self , msg ) : self . warnings . append ( msg ) sys . stderr . write ( "Coverage.py warning: %s\n" % msg )
def check for packages ( self ) : # Our self.source pkgs attribute is a list of package names we want to # measure.  Each time through here, we see if we've imported any of # them yet.  If so, we add its file to source match, and we don't have # to look for that package any more. if self . source pkgs : found = [ ] for pkg in self . source pkgs : try : mod = sys . modules [ pkg ] except Key Error : continue found . append ( pkg ) try : pkg file = mod . file except Attribute Error : pkg file = None else : d , f = os . path . split ( pkg file ) if f . startswith ( ' init ' ) : # This is actually a package, return the directory. pkg file = d else : pkg file = self . source for file ( pkg file ) pkg file = self . file locator . canonical filename ( pkg file ) if not os . path . exists ( pkg file ) : pkg file = None if pkg file : self . source . append ( pkg file ) self . source match . add ( pkg file ) else : self . warn ( "Module %s has no Python source." % pkg ) for pkg in found : self . source pkgs . remove ( pkg )
def atexit ( self ) : if self . started : self . stop ( ) if self . auto data : self . save ( )
def exclude regex ( self , which ) : if which not in self . exclude re : excl list = getattr ( self . config , which + " list" ) self . exclude re [ which ] = join regex ( excl list ) return self . exclude re [ which ]
def save ( self ) : data suffix = self . data suffix if data suffix is True : # If data suffix was a simple true value, then make a suffix with # plenty of distinguishing information.  We do this here in # `save()` at the last minute so that the pid will be correct even # if the process forks. extra = "" if TEST NAME FILE : f = open ( TEST NAME FILE ) test name = f . read ( ) f . close ( ) extra = "." + test name data suffix = "%s%s.%s.%06d" % ( socket . gethostname ( ) , extra , os . getpid ( ) , random . randint ( 0 , 999999 ) ) self . harvest data ( ) self . data . write ( suffix = data suffix )
def analysis ( self , morf ) : f , s , , m , mf = self . analysis2 ( morf ) return f , s , m , mf
def reload ( self ) : if self . filename is not None : with open ( self . filename , self . read flags ) as f : self . data = f . read ( ) elif self . url is not None : try : import urllib2 response = urllib2 . urlopen ( self . url ) self . data = response . read ( ) # extract encoding from header, if there is one: encoding = None for sub in response . headers [ 'content-type' ] . split ( ';' ) : sub = sub . strip ( ) if sub . startswith ( 'charset' ) : encoding = sub . split ( '=' ) [ - 1 ] . strip ( ) break # decode data, if an encoding was specified if encoding : self . data = self . data . decode ( encoding , 'replace' ) except : self . data = None
def find cmd ( cmd ) : path = sp . Popen ( [ '/usr/bin/env' , 'which' , cmd ] , stdout = sp . PIPE , stderr = sp . PIPE ) . communicate ( ) [ 0 ] return py3compat . bytes to str ( path )
def run ( self ) : line = self . fd . readline ( ) # allow for files opened in unicode mode if isinstance ( line , unicode ) : send = self . sock . send unicode else : send = self . sock . send while line : send ( line ) line = self . fd . readline ( ) # line == '' means EOF self . fd . close ( ) self . sock . close ( )
def start ( self ) : try : pid = self . get pid from file ( ) except PID File Error : self . log . critical ( 'Could not read pid file, cluster is probably not running.' ) # Here I exit with a unusual exit status that other processes # can watch for to learn how I existed. self . remove pid file ( ) self . exit ( ALREADY STOPPED ) if not self . check pid ( pid ) : self . log . critical ( 'Cluster [pid=%r] is not running.' % pid ) self . remove pid file ( ) # Here I exit with a unusual exit status that other processes # can watch for to learn how I existed. self . exit ( ALREADY STOPPED ) elif os . name == 'posix' : sig = self . signal self . log . info ( "Stopping cluster [pid=%r] with [signal=%r]" % ( pid , sig ) ) try : os . kill ( pid , sig ) except OS Error : self . log . error ( "Stopping cluster failed, assuming already dead." , exc info = True ) self . remove pid file ( ) elif os . name == 'nt' : try : # kill the whole tree p = check call ( [ 'taskkill' , '-pid' , str ( pid ) , '-t' , '-f' ] , stdout = PIPE , stderr = PIPE ) except ( Called Process Error , OS Error ) : self . log . error ( "Stopping cluster failed, assuming already dead." , exc info = True ) self . remove pid file ( )
def build launcher ( self , clsname , kind = None ) : try : klass = find launcher class ( clsname , kind ) except ( Import Error , Key Error ) : self . log . fatal ( "Could not import launcher class: %r" % clsname ) self . exit ( 1 ) launcher = klass ( work dir = u'.' , config = self . config , log = self . log , profile dir = self . profile dir . location , cluster id = self . cluster id , ) return launcher
def start ( self ) : self . log . info ( "I Python cluster: started" ) # First see if the cluster is already running # Now log and daemonize self . log . info ( 'Starting engines with [daemon=%r]' % self . daemonize ) # TODO: Get daemonize working on Windows or as a Windows Server. if self . daemonize : if os . name == 'posix' : daemonize ( ) dc = ioloop . Delayed Callback ( self . start engines , 0 , self . loop ) dc . start ( ) # Now write the new pid file AFTER our new forked pid is active. # self.write pid file() try : self . loop . start ( ) except Keyboard Interrupt : pass except zmq . ZMQ Error as e : if e . errno == errno . EINTR : pass else : raise
def start ( self ) : # First see if the cluster is already running try : pid = self . get pid from file ( ) except PID File Error : pass else : if self . check pid ( pid ) : self . log . critical ( 'Cluster is already running with [pid=%s]. ' 'use "ipcluster stop" to stop the cluster.' % pid ) # Here I exit with a unusual exit status that other processes # can watch for to learn how I existed. self . exit ( ALREADY STARTED ) else : self . remove pid file ( ) # Now log and daemonize self . log . info ( 'Starting ipcluster with [daemon=%r]' % self . daemonize ) # TODO: Get daemonize working on Windows or as a Windows Server. if self . daemonize : if os . name == 'posix' : daemonize ( ) dc = ioloop . Delayed Callback ( self . start controller , 0 , self . loop ) dc . start ( ) dc = ioloop . Delayed Callback ( self . start engines , 1000 * self . delay , self . loop ) dc . start ( ) # Now write the new pid file AFTER our new forked pid is active. self . write pid file ( ) try : self . loop . start ( ) except Keyboard Interrupt : pass except zmq . ZMQ Error as e : if e . errno == errno . EINTR : pass else : raise finally : self . remove pid file ( )
def get app wx ( * args , * * kwargs ) : import wx app = wx . Get App ( ) if app is None : if not kwargs . has key ( 'redirect' ) : kwargs [ 'redirect' ] = False app = wx . Py Simple App ( * args , * * kwargs ) return app
def is event loop running wx ( app = None ) : if app is None : app = get app wx ( ) if hasattr ( app , ' in event loop' ) : return app . in event loop else : return app . Is Main Loop Running ( )
def start event loop wx ( app = None ) : if app is None : app = get app wx ( ) if not is event loop running wx ( app ) : app . in event loop = True app . Main Loop ( ) app . in event loop = False else : app . in event loop = True
def get app qt4 ( * args , * * kwargs ) : from I Python . external . qt for kernel import Qt Gui app = Qt Gui . Q Application . instance ( ) if app is None : if not args : args = ( [ '' ] , ) app = Qt Gui . Q Application ( * args , * * kwargs ) return app
def is event loop running qt4 ( app = None ) : if app is None : app = get app qt4 ( [ '' ] ) if hasattr ( app , ' in event loop' ) : return app . in event loop else : # Does qt4 provide a other way to detect this? return False
def start event loop qt4 ( app = None ) : if app is None : app = get app qt4 ( [ '' ] ) if not is event loop running qt4 ( app ) : app . in event loop = True app . exec ( ) app . in event loop = False else : app . in event loop = True
def check package ( self , package , package dir ) : try : return self . packages checked [ package ] except Key Error : pass init py = build py . check package ( self , package , package dir ) self . packages checked [ package ] = init py if not init py or not self . distribution . namespace packages : return init py for pkg in self . distribution . namespace packages : if pkg == package or pkg . startswith ( package + '.' ) : break else : return init py f = open ( init py , 'rb U' ) if 'declare namespace' . encode ( ) not in f . read ( ) : from distutils import log log . warn ( "WARNING: %s is a namespace package, but its  init .py does\n" "not declare namespace(); setuptools 0.7 will REQUIRE this!\n" '(See the setuptools manual under "Namespace Packages" for ' "details.)\n" , package ) f . close ( ) return init py
def get unique key from get ( get dict ) : site = Site . objects . get current ( ) key = get dict to encoded url ( get dict ) cache key = '{} {}' . format ( site . domain , key ) return hashlib . md5 ( cache key ) . hexdigest ( )
def get domain ( url ) : if 'http' not in url . lower ( ) : url = 'http://{}' . format ( url ) return urllib . parse . urlparse ( url ) . hostname
def get url args ( url ) : url data = urllib . parse . urlparse ( url ) arg dict = urllib . parse . parse qs ( url data . query ) return arg dict
